{
  "title": "Collectively encoding protein properties enriches protein language models",
  "url": "https://openalex.org/W4308589354",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2756607880",
      "name": "Jingmin An",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Northeast Agricultural University",
        "Institute of Zoology"
      ]
    },
    {
      "id": "https://openalex.org/A2097535659",
      "name": "Xiaogang Weng",
      "affiliations": [
        "Northeast Agricultural University"
      ]
    },
    {
      "id": "https://openalex.org/A2756607880",
      "name": "Jingmin An",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097535659",
      "name": "Xiaogang Weng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6600195515",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W3166142427",
    "https://openalex.org/W3165163830",
    "https://openalex.org/W2951433247",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W3164453494",
    "https://openalex.org/W3040739508",
    "https://openalex.org/W3126639583",
    "https://openalex.org/W3129155125",
    "https://openalex.org/W3207838037",
    "https://openalex.org/W6600704668",
    "https://openalex.org/W2156269526",
    "https://openalex.org/W2772848669",
    "https://openalex.org/W2983113805",
    "https://openalex.org/W2920837834",
    "https://openalex.org/W2993794083",
    "https://openalex.org/W2041495858",
    "https://openalex.org/W2085277871",
    "https://openalex.org/W2108067237",
    "https://openalex.org/W2984761660",
    "https://openalex.org/W4225891318",
    "https://openalex.org/W4255421341",
    "https://openalex.org/W2941112903",
    "https://openalex.org/W3163390259",
    "https://openalex.org/W2804822363",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3037888463",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2789876780",
    "https://openalex.org/W2120615054",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W4245888765",
    "https://openalex.org/W3141797743",
    "https://openalex.org/W4210836354",
    "https://openalex.org/W4234552385",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2549976854",
    "https://openalex.org/W2156798505",
    "https://openalex.org/W2059136964",
    "https://openalex.org/W2029476353",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W2950374603",
    "https://openalex.org/W2104972430",
    "https://openalex.org/W2137965757",
    "https://openalex.org/W2104467962",
    "https://openalex.org/W2951621517",
    "https://openalex.org/W2913820882",
    "https://openalex.org/W3198971594",
    "https://openalex.org/W2550969987",
    "https://openalex.org/W2058833671",
    "https://openalex.org/W2889498145",
    "https://openalex.org/W2161072217",
    "https://openalex.org/W2130479394",
    "https://openalex.org/W4254491045",
    "https://openalex.org/W3022629704",
    "https://openalex.org/W2789251334",
    "https://openalex.org/W4200158771",
    "https://openalex.org/W2808950571",
    "https://openalex.org/W2898402099",
    "https://openalex.org/W2472351724",
    "https://openalex.org/W4283278367",
    "https://openalex.org/W4297243376",
    "https://openalex.org/W2971227267",
    "https://openalex.org/W2963457143",
    "https://openalex.org/W2911349016",
    "https://openalex.org/W4205773061",
    "https://openalex.org/W2949159188"
  ],
  "abstract": "Abstract Pre-trained natural language processing models on a large natural language corpus can naturally transfer learned knowledge to protein domains by fine-tuning specific in-domain tasks. However, few studies focused on enriching such protein language models by jointly learning protein properties from strongly-correlated protein tasks. Here we elaborately designed a multi-task learning (MTL) architecture, aiming to decipher implicit structural and evolutionary information from three sequence-level classification tasks for protein family, superfamily and fold. Considering the co-existing contextual relevance between human words and protein language, we employed BERT, pre-trained on a large natural language corpus, as our backbone to handle protein sequences. More importantly, the encoded knowledge obtained in the MTL stage can be well transferred to more fine-grained downstream tasks of TAPE. Experiments on structure- or evolution-related applications demonstrate that our approach outperforms many state-of-the-art Transformer-based protein models, especially in remote homology detection.",
  "full_text": "Collectively encoding protein properties \nenriches protein language models\nJingmin An1,2 and Xiaogang Weng1* \nIntroduction\nNatural language is inherently context-dependent. This fact becomes particularly \nprominent when two strongly-related words are far separated. From a linguistics per -\nspective, context plays a significant role in deciphering the actual meaning of a word \n[1]. Likewise, correctly encoding contextual information is essential for natural lan -\nguage processing (NLP). Much prior research employed deep learning methods, such \nas convolutional neural network (CNN) [2], Recurrent Neural Network (RNN) [3] and \nword embedding [4], to acquire such inter-word dependencies. Recent advanced atten -\ntion-based models possess equally powerful representation abilities to capture these \ncontextual relationships through the self-attention mechanism [5]. Similar to natural \nlanguage, protein sequences also hold strong contextual information, implicitly denot -\ning structural, evolutionary or functional characteristics [5]. Appropriately capturing \nthese inter-residue relationships from the sequence is of great interest to computational \nbiologists. Considering the co-existing contextual relevance between natural and protein \nlanguage, a sophisticated NLP model can likewise learn contexts in protein language. \nAbstract \nPre-trained natural language processing models on a large natural language corpus \ncan naturally transfer learned knowledge to protein domains by fine-tuning specific \nin-domain tasks. However, few studies focused on enriching such protein language \nmodels by jointly learning protein properties from strongly-correlated protein tasks. \nHere we elaborately designed a multi-task learning (MTL) architecture, aiming to \ndecipher implicit structural and evolutionary information from three sequence-level \nclassification tasks for protein family, superfamily and fold. Considering the co-existing \ncontextual relevance between human words and protein language, we employed \nBERT, pre-trained on a large natural language corpus, as our backbone to handle \nprotein sequences. More importantly, the encoded knowledge obtained in the MTL \nstage can be well transferred to more fine-grained downstream tasks of TAPE. Experi-\nments on structure- or evolution-related applications demonstrate that our approach \noutperforms many state-of-the-art Transformer-based protein models, especially in \nremote homology detection.\nKeywords: Protein language modeling, Multi-task learning, Transfer learning\nOpen Access\n© The Author(s) 2022. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// \ncreat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco mmons. org/ publi \ncdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nRESEARCH\nAn and Weng  BMC Bioinformatics          (2022) 23:467  \nhttps://doi.org/10.1186/s12859-022-05031-z\nBMC Bioinformatics\n*Correspondence:   \nwengxg@neau.edu.cn\n1 School of Life Sciences, \nNortheast Agricultural University, \nHarbin 150030, China\n2 State Key Laboratory \nof Membrane Biology, Institute \nof Zoology, Chinese Academy \nof Sciences, Beijing 100101, \nChina\nPage 2 of 18An and Weng  BMC Bioinformatics          (2022) 23:467 \nMany researchers have studied this aspect, and BERT [6] is one of the most popular NLP \narchitectures. These studies can be classified into two types, depending on whether they \nintroduce protein knowledge in the pre-training stage. The first type aims to construct \na protein language model by pre-training on a large protein corpus [5, 7–11]. The other \ntype directly transfers knowledge in human words to decode protein language [12–14], \ndemonstrating slightly poor performance compared to that of the models pre-trained on \nprotein language. Indeed, the pre-training stage is imperative for improving results on \ndownstream tasks [8]. However, it is still unclear whether the performance of costly pre-\ntraining on a large in-domain corpus certainly will outperform that when transferring \nlearned knowledge from natural language into domain-specific tasks. In addition, most \nsecond-type research simply transfers natural language embeddings to learn protein rep-\nresentations by fine-tuning specific tasks without following protein in-domain re-train -\ning. Admittedly, the abundant contextual representations encoded by human language \nmodels can naturally be used to capture such context in proteins. Most importantly, \ndelicately enriching protein knowledge by in-domain protein tasks is greatly helpful for \ndeciphering useful protein properties. Therefore, with the help of BERT pre-trained on \nlarge natural language corpus, together with encoding protein properties from closely-\nrelated protein tasks, such protein language models is expected to get promising down -\nstream results.\nMulti-task learning (MTL), which is able to leverage useful information of related tasks \nto achieve simultaneous strong performance on multiple associated tasks [15], has led to \ngreat success in many machine learning applications like NLP [15, 16]. As for the protein \nsequence domain, MTL has been widely applied for functional studies, like protein–pro-\ntein interaction and protein targets [17–20]. A notable work [21] fused self-supervised \nlanguage modeling and four supervised tasks in a model, realizing an end-to-end MTL \narchitecture. Specifically, they employed two residue-level (secondary structure predic -\ntion in 3- and 8-states) and two protein-level (subcellular localization prediction and the \nclassification membrane-vs-soluble proteins) tasks, which enables the model to jointly \ndecipher protein properties and transfer knowledge between these different tasks. How -\never, the multiple supervised tasks they adopted are not highly dependent, and they did \nnot test the model performance on downstream tasks either. There are many correlated \ntasks in the protein domain, such as structural similarity and contact prediction, con -\ntact prediction and remote homology detection. However, little research focused on the \ninterrelated protein tasks to facilitate the survey of protein structure or evolution. It is \nworth mentioning that Charuvaka et al. [22] employed every hierarchical category from \nthe Structural Classification of Proteins (SCOP) [23] and CATH [24] databases as a sin -\ngle classification task to predict the structural type of protein sequences. SCOP [25] is \na popular protein database that hierarchically classifies protein domains into four cat -\negories, listed from the bottom to the top: family (the proteins that share the exact evo -\nlutionary origin), superfamily (the proteins that evolved from the same ancestor but are \ndistantly related), fold (the proteins that hold the same global structural features), class \n(the proteins gathered from fold and superfamily that have specific secondary structural \ncontent). To be precise, family explicitly denotes the evolutionary relations between pro-\nteins while superfamily gathers proteins with similar structure but less sequence similar-\nity. Fold groups superfamilies based on the global structural features shared by most of \nPage 3 of 18\nAn and Weng  BMC Bioinformatics          (2022) 23:467 \n \ntheir members and the constituted families can evolve distinct structures. Accordingly, \nwe can clearly realize the intrinsically-related evolutionary and structural properties \namong family, superfamily and fold categories. Villegas-Morcillo et al. [26] adopted pair-\nwise fold recognition (PFR) and direct fold recognition (DFC) tasks to identify protein \nfold category. However, they only focused on the classification performance toward fold \nlabel without collectively employing abundant information behind these three labels. \nComprehensively considering the information behind the three categories is expected \nto encode important evolutionary and structure in prior knowledge, which could further \nbe transferred to related downstream tasks. In this paper, therefore, based on the three \nclosely-related classification tasks, we designed a MTL architecture to capture such \nstructural and evolutionary relationships.\nTransfer learning means transferring the knowledge from a related task that has \nalready been learned to a new task [27]. Rives et al. [9] pointed out that learning intrin -\nsic biological properties directly from protein sequences can further be transferred to \nprediction and generation. Likewise, Bepler et al. [5] showed that transfer learning could \npotentially improve downstream applications in certain scenarios. Through learning two \nsupervised structural tasks, they found that the performance of their protein language \nmodel on two function tasks had been improved. It is worth noticing that these learn -\ning procedures are based entirely on protein sequences. Different from their work, we \nboth introduced in prior knowledge of the natural language and protein sequences in \nthe pre-training and multi-task learning stages, respectively. Another noteworthy work \n[28] used three types of fine-tuning protein tasks, including sequence classification, \nsequence-pair classification and token-level classification, ultimately improving several \ndownstream performances and demonstrating the effectiveness of transfer learning \nfor protein in-domain tasks. Tasks assessing protein embeddings (TAPE) [8] provides \nstandardized benchmarks to evaluate the performance of learned protein embeddings. \nIt contains five biologically-relevant tasks with regard to structure prediction, evolution-\nary understanding and protein engineering domains. Among these benchmark tests, we \nchose secondary structure prediction, contact prediction and remote homology detec -\ntion as the downstream tasks to verify the transfer ability of our MTL models.\nIn this work, to sum up, we proposed a multi-task learning framework using three \nBERT-based backbones, which employed abundant contextual representations obtained \nin natural language and jointly learned knowledge on interrelated protein tasks. Three \nstructural- or evolutionary-relevant downstream tasks, well-defined in TAPE, were used \nto evaluate whether our MTL architectures properly capture the structural and evolu -\ntional relationships. The overall workflow is shown in Fig. 1.\nMaterials and methods\nOverall, we elaborately designed three MTL backbones with different intrinsic architec -\ntures, namely MT-BERT, MT-BCNN, and MT-BLSTM. To jointly learn protein structure \nand evolution properties, we assigned these models a multi-task classification pipe -\nline with respect to protein family, superfamily and fold categories. Finally, the learned \nknowledge was transferred to decode fine-grained applications well-defined in TAPE. \nSpecifically, we adopted three structural- or evolutionary-related downstream tasks, \nPage 4 of 18An and Weng  BMC Bioinformatics          (2022) 23:467 \nincluding secondary structure prediction, contact prediction, and remote homology \ndetection, to evaluate the transfer learning performance of our proposed MTL models.\nMTL models\nMTL datasets\nThe training and test datasets we used for the MTL pipeline are all derived from SCOP 2 \n[25], a widely-applied database that aims to encode structural and evolutionary relation -\nships between proteins. We first extracted the label information directly downloaded \nfrom this database, and then located the corresponding sequence as per the superfam -\nily-level domain identification. Until May 1st 2022, the SCOP 2 dataset provides 36,534 \nwell-labeled amino acid sequences. Moreover, to avoid information leakage in the MTL \ntraining stage and downstream test phase, we eliminated 367 overlapped sequences \nbetween TAPE remote homology detection test set and our whole MTL dataset. After \nthat, we split the cleaned dataset (a total number of 36,167 sequences) into training and \ntest sets on a scale of 7:3. The statistics [25] of our reconstructed dataset for these labels \nare in Table 1, and we can see that a great many types are included in each label. Addi -\ntionally, the number of protein sequences for each type is quite unbalanced (e.g., only \nseveral proteins are classified into a specific type). Therefore, we ensured that every type \nin each label includes at least one training sequence to avoid a test protein belonging to \nan unknown type.\nMTL backbones\nFor all NLP models, pre-training on a large natural language corpus is essential for learn-\ning universal language representations [32]. As a pioneering and representative work, \nFig. 1 Workflow of our method. The colored rectangles represent three interrelated protein labels defined \nin SCOP 2 [25]. The predicted secondary structure sequence, contact map and 3D structure are illustrated by \nPSIPRED [29], ProteinTools [30] and Swiss-model [31], respectively (PDBid: 3H8D)\nTable 1 Statistics of family, superfamily and fold categories in our MTL dataset\nStatistics Family Superfamily Fold\nNumber 5842 2750 1577\nPage 5 of 18\nAn and Weng  BMC Bioinformatics          (2022) 23:467 \n \nBERT [6], a variance of Transformer [33], demonstrates powerful transfer learning abil -\nity with pre-training bidirectional representations from amounts of unlabeled natural \nlanguage data. It has been shown that the prior natural language knowledge encoded \nin NLP models can be well transferred to handle biological sequences [12–14]. Addi -\ntionally, BERT attention captures the folding structure of proteins, targets binding sites \nand focuses on progressively more complex biophysical properties with increasing layer \ndepth [34]. Accordingly, we employed BERT [35] pre-trained on 3300 M human words \nas part of our MTL backbones, and the one that did not follow any other sequence anal -\nysis networks acts as our MT-BERT architecture.\nThe powerful feature extraction ability of BERT with substantial parameters may over-\nfit the classification task, especially with limited training data [36]. CNN has demon -\nstrated its great potential in the application of image data [37]. Besides, it has achieved \nequally strong performance on text classification [2] and natural language modeling \n[38], even though it is not as frequently used as in images. In such a structure, neu -\nrons between different layers are partially connected, which can well reduce intrin -\nsic noises in protein sequences that hinder language models from deciphering protein \nproperties.  Therefore, introducing CNN to BERT can avoid overfitting to a certain \nextent.  Additionally, Long Short-Term Memory (LSTM) [39], a variant of RNN, first \ncame out to address the difficulty of storing long-range sequence information. The fact \nof linearly encoding of input sequence enables LSTM to retain relative intra-sequence \ndependencies better. Accordingly, LSTM is especially suitable for encoding distantly-\nrelated and order-depended structural and evolutionary relevance in protein sequences. \nTherefore, based on the above considerations, we respectively added CNN and  LSTM \nlayers to the final BERT encoder, becoming the backbones of the so-called MT-BCNN \nand MT-BLSTM.\nCompared to single-task learning, which learns only one specific representation for \nonce, multi-task learning enables the knowledge learned in one task to benefit other \ntasks [40]. As mentioned in “Introduction ” section, there is a strong correlation among \nthe pre-labeled protein family, superfamily and fold categories. Hence, we adopted three \nclassification tasks for these labels to enable our MTL models to encode implicit evolu -\ntionary and structural information.\nDetails of MTL architecture\nDeep learning, which is inherently specialized in learning complex non-linear feature \nrepresentations [41, 42], has been widely applied in MTL domains. Zhang et  al. [42] \nclassified deep MTL models into two main types: learning common feature mappings \nby sharing the first several layers or introducing adversarial learning; learning different \nfeature mappings with a cross-stitch network. They also pointed out that only sharing \nhidden layers is quite powerful when all the tasks are correlated. Since we defined three \nclosely-related classification tasks of the protein family, superfamily and fold, our MTL \nmodels also share previous layers to learn common structural and evolutionary repre -\nsentations. Inspired by Liu et al. [40], we proposed an improved deep MTL architecture \nspecially designed for modeling protein language (see Fig. 2).\nEvery input protein sequence would first be tokenized into separate amino acids rep -\nresented by specific alphabets, then embedded in a maximum of 8096 vector spaces \nPage 6 of 18An and Weng  BMC Bioinformatics          (2022) 23:467 \naccording to the length of the sequence. Note that the [CLS] token is used for sequence-\nlevel classification tasks. After that, these embeddings would all go through BERT Trans-\nformer Encoder layers for extracting contextual information. The difference between our \nthree MTL models lies in the shared layers. MT-BCNN and MT-BLSTM concat the con-\ntext embeddings from the last BERT Encoder with six CNN layers and two bidirectional \nLSTM layers, respectively. The output representations of the shared layers would finally \nfeed into three task-specific fully connected layers for classifying protein family, superfam-\nily and fold.\nGenerally, an MTL model can be trained by linearly combining loss functions from dif-\nferent tasks into a single total loss function [15]. In this way, the model can learn a shared \nrepresentation for all tasks by stochastic gradient descent (SGD) with back-propagation \n[15, 43]. Ordinarily, assuming that there are M tasks in all, the global loss function can be \ndefined as\nwhere L i represents task-specific loss function, and w i denotes weights assigned for each \nL i.\nIt is worth noticing that the performance of MTL models strongly depends on the relative \nweighting between the loss of each task [44]. It has been reported that many researchers set \nthese weights according to experience or through costly grid search [15]. Following the pre-\nvious work of Kendall et al. [44], we adopted homoscedastic tasks uncertainty to optimize \nthe loss weights w i . Moreover, we used cross-entropy loss function for each classification \ntask:\n(1)Ltotal=\nM\ni\nw iLi\nFig. 2 The detailed structure of the proposed MTL framework with three kinds of backbones\nPage 7 of 18\nAn and Weng  BMC Bioinformatics          (2022) 23:467 \n \nwhere X denotes the input protein sequence, Id (X , c) is a binary identification (0 or 1) \nindicating whether the label c is the correct category of X , and p(X |c) represents the pre-\ndicted probability that X is classified to label c.\nHere we demonstrate how to train our MTL models in Algorithm 1. The same MTL \ndataset, derived from SCOP 2, was used to jointly learn how to classify proteins into \nfamily, superfamily and fold. We set a batch size of 32, a dropout rate of 0.1 for BERT \nand 0.4 for LSTM. We defined a larger learning rate of 1e-2 for CNN and LSTM con -\nnected after BERT, while the pre-trained BERT held a relatively small learning rate of \n1e-5. All the above hyperparameters were fine-tuned through Bayesian Optimization \n[45]. We also employed SGD to update model parameters step by step. The training pro-\ncedure was implemented with PyTorch [46] on NVIDIA Quadro \nGP100.\nDealing with long protein sequences\nAmong the three proposed MTL models, the protein sequences are always first fed into \nBERT Encoder (see Figs.  1, 2). The maximum input of the pre-trained BERT model on \nnatural language is set to 512, while the length of amino acid sequences can sometimes \nexceed it. Sun et al. [32] proposed three ways to deal with long natural language articles: \nhead-only, tail-only and head–tail. However, these novel solutions cannot readily handle \nprotein sequences since every residue may represent unique structural and evolution -\nary information. Thus, instead of cutting up residues, we must keep the whole sequence \nas the input for the following embedding  process. We first re-initialized the length of \nthe max positional embedding dictionary to 8096, the same size as that in TAPE [8]. \nThen we replaced the randomly initialized first 512 tokens in the whole 8096 tokens with \nthe previously-encoded position embeddings in pre-trained BERT. In doing so, we not \nonly retain the encoded representations obtained in natural language pre-training, but \ncan further embed the rest 7584 vectors in the following MTL and downstream protein \ntasks.\n(2)Li =− ∑\nc\nId (X ,c) log p(X |c)\nPage 8 of 18An and Weng  BMC Bioinformatics          (2022) 23:467 \nMTL model evaluation metrics\nThe performance of our models in the MTL stage can not only partly influence that on \ndownstream scenarios, but also validate whether abundant natural language knowledge \nis well transferred to encode protein properties. Therefore, we report four evaluation \nmetrics to estimate the sequence-level classification performance: Precision (Pre), Accu-\nracy (Acc), Recall (Rec) and F1-score (F1). These indexes are frequently used to assess \nthe generalization of machine learning models from distinct perspectives. The detailed \ndefinitions can be seen below.\nDownstream tasks\nMTL aims to help improve the generalization of all tasks [42]. In this study, however, we \nwant to investigate how the jointly-learned protein knowledge could facilitate relevant \ndownstream tasks. In other words, we would like to see whether the encoded structural \nand evolutionary information can transfer to decode more fine-grained assignments. \nTo test the transfer ability of our MTL models, we therefore employ two structure pre -\ndiction and one evolutionary understanding tasks in TAPE [8]. Furthermore, all data -\nsets and metrics used to evaluate our models are identical to those in TAPE to ensure \ncomparability.\nSecondary structure prediction\nSecondary structure prediction (SS prediction) is a sequence-to-sequence classification \nassignment. Assuming that there is a protein sequence, this task is dedicated to label -\nling every input amino acid with a secondary structure position (see Fig.  3). The labels \ncan further be categorized into 3-state secondary structure (i.e., alpha-helix (H), beta-\nstrand (E) and coil region (C)) and 8-state secondary structure (i.e., helix (G), α-helix \n(H), π-helix (I), β-stand (E), bridge (B), turn (T), bend (S), and others (C)) [47–49]. We \noften evaluate the performance of SS prediction by Q3 or Q8 accuracy, which meas -\nures how many residues for which 3-state or 8-state secondary structure is correctly pre-\ndicted [50]. Accurate SS prediction facilitates the study of protein structure and function \n[47], including fold-recognition, homology modeling, ab initio and constraint-based ter -\ntiary structure prediction, as well as identification of functional domains [51].\nAs in TAPE, the training and validation datasets for secondary structure prediction are \nfrom [53], and Q3 accuracy is reported on the test set CB513 [54].\n(3)Precision= TP\nTP +FP\n(4)Accuracy= TP\nTP +TN +FP+FN\n(5)Recall= TP\nTP +FN\n(6)F 1 − score= 2∗Precision∗Recall\nPrecision+Recall\nPage 9 of 18\nAn and Weng  BMC Bioinformatics          (2022) 23:467 \n \nContact prediction\nContact prediction is a pairwise classification assignment. Given a protein sequence, the \ngoal of this task is to predict whether each pair of residues from this sequence are “in \ncontact” (typically, it is defined as the distance in folded structure < 8 Å [8, 49]) or not \n(see Fig.  4). The contacts can be subdivided into short-, medium- and long- ranges cor -\nresponding to the sequence separation equal to 6–11, 12–24 and > 24 respectively [55]. \nCorrectly-predicted contacts capture powerful global structural and folding information \n[8, 56], facilitating 3D structure modeling, especially de novo protein structure predic -\ntion [57].\nAs in TAPE, the dataset for contact prediction is from ProteinNet [58]. The precision \nof the L/5 most likely contacts for medium- and long-range contacts, where L is the \nlength of protein sequence, are reported on the ProteinNet CASP 12 test set [59].\nRemote homology detection\nRemote homology detection is a conventional sequence-level classification assign -\nment. Since distantly related proteins may share similar structures and functions [60], \nthis task targets to predict which fold structure the input protein sequence belongs \nto (see Fig.  5). This fold structure is the exact fold label clearly defined in SCOP [25]. \nProtein remote homology is critical for studying protein structures and functions \n[55] and drug design [61]. It identifies proteins from different families and therefore \nFig. 3 Illustration of secondary structure prediction, where the 3D structure is established by Alphafold [52] \n(PDB id: 1J1Q)\nFig. 4 Illustration of contact prediction, where the 3D structure is established by Alphafold [52] (PDB id: \n1J1Q)\nPage 10 of 18An and Weng  BMC Bioinformatics          (2022) 23:467 \nis suitable for predicting the structure and functions of specific proteins [62]. Note \nthat this assignment is similar to the fold classification task implemented in the MTL \nstage, and significantly improved performance is reported compared to other SOTA \nwork in  “Result ” section.\nAs in TAPE, the dataset of remote homology detection comes from [63], which \noriginates from SCOP 1.75 database and Protein Data Bank [64, 65]. The overall clas -\nsification accuracy is reported on the fold-level heldout test set from [63]. It is worth \nmentioning that the dataset we used in the MTL stage derives from SCOP 2 (a succes -\nsor to SCOP 1.75); thus, the test set in TAPE may contaminate our training set. There -\nfore, we screened the overlapped proteins in our constructed MTL dataset for accuracy \nconcerns.\nResults\nMTL model evaluation\nUsing the four evaluation metrics reported in  “MTL model evaluation metrics” section, \nwe first estimated the model performance on family, superfamily and fold classification \ntasks in the MTL phase. The reported results are averaged over the three classifications \nbased on tenfold cross-validation (see Table 2).\nThese results validate that the knowledge in natural language can indeed transfer to \nhandle sequence-level protein classifications. Furthermore, compared to MT-BERT \nsolely employed BERT, the introduced CNN and LSTM layers in MT-BCNN and MT-\nBLSTM have improved the overall classification performance. Specifically, we can see \nthat MT-BLSTM gets the best results among all the three MTL models.\nFig. 5 Illustration of remote homology detection, where the 3D structure is established by Alphafold [52] \n(PDB id: 1J1Q)\nTable 2 Averaged results of the tenfold cross-validation on our MTL training set\nModel Pre Acc Rec F1\nMT-BERT 0.636 0.643 0.705 0.662\nMT-BCNN 0.705 0.767 0.742 0.723\nMT-BLSTM 0.731 0.771 0.756 0.743\nPage 11 of 18\nAn and Weng  BMC Bioinformatics          (2022) 23:467 \n \nThe effectiveness of MTL\nAs mentioned above, we considered three of the four protein categories, namely family, \nsuperfamily and fold, and the information behind the class label has not been employed. \nClass category gathers folds and intrinsically-unstructured proteins from superfamily, \nwhich indicates a solid structural concept and the correlation with evolution. For this \ntime, this label can be used to verify the effectiveness of our MTL models. We compared \nthe learned features between the original pre-trained BERT and our MTL models to \ncheck whether these multiple tasks encode useful structural and evolutionary informa -\ntion. Generally, an MTL model would encode a given sequence into high-dimensional \nvector embeddings. However, it is possible to map the whole semantic space by pooling \nthem into fixed-size embeddings by reduction [5]. Moreover, introducing clustering and \nmanifold embedding to visualize large protein datasets can reveal structural and evolu -\ntionary relationships between sequences [5]. Thus, we compared the embedding results \nof pre-trained BERT without the MTL process with those of our MTL models. Figure  6 \nshows the visualized proteins in our whole MTL dataset after embedding and dimen -\nsionality reduction by Multidimensional Scaling [66–68]. The pre-trained BERT on nat -\nural language, without MTL protein-domain tasks, demonstrates inadequate structural \nclassification ability, and the embedding spaces are significantly sparse and mixed. Three \njointly-learned interrelated protein tasks are allocated, making the boundaries between \ndistinct class labels clearer. Overall, it can be noticed from those embedded proteins that \nthe MTL process improves the clustering performance. Furthermore, to statistically ana-\nlyze the embedding differences of different models in Fig. 6, we evaluate the classification \nFig. 6 Comparison of manifold embedding of MTL dataset proteins. Different colours represent distinct \nlabels in the Class Category\nPage 12 of 18An and Weng  BMC Bioinformatics          (2022) 23:467 \nperformance toward the class label on our whole MTL dataset. Table  3 reports detailed \nresults using the same classification metrics as Table 2.\nWe can see from Table 3 that the classification performance of our MTL models signif-\nicantly outperforms the original BERT model that did not implement the MTL process, \nwhich is consistent with the manifold embeddings in Fig. 6.\nOverall, such abstract clustering representations and statistical results proved that our \nMTL models captured useful structural and evolutionary information that could further \nfacilitate related downstream tasks.\nMTL model performance on downstream tasks\nWe evaluated the MTL model performance on three downstream applications: sec -\nondary structure (SS) prediction, contact prediction and remote homology detection. \nDepending on the task, we reported the accuracy or precision described in  “Down -\nstream tasks” section as done in TAPE [8]. Each metric has a maximum value of 1.0, and \nhigher represents the better. Note that the evaluation metrics remain the same in the \nfollowing experiments.\nTable 4 compares our three MTL models with the TAPE BERT [8], ProteinBERT [7], \nBERT medium [69], String2Seq [28] and ProtBert [11]. Except the secondary structure \nprediction dataset used in the BERT medium was from [70], all the results in Table  4 are \nreported on the same datasets described in “Downstream tasks” section. We can notice \nthat MT-BLSTM and MT-BCNN obtained the best results under these three structural \nor evolutionary tasks. Notably, the performance of remote homology detection had been \nsignificantly improved compared with other SOTA models. This phenomenon can partly \nbe attributed to the close relationship between fold label classification in the MTL stage \nTable 3 Class label classification performance on our whole MTL dataset\nModel Pre Acc Rec F1\nBERT 0.232 0.186 0.219 0.225\nMT-BERT 0.565 0.557 0.593 0.579\nMT-BCNN 0.716 0.736 0.667 0.691\nMT-BLSTM 0.745 0.726 0.717 0.731\nTable 4 Comparison of TAPE benchmark results on three structure- or evolution-related tasks\nModel Structure Evolution\nSS Prediction Contact\nPrediction\nRemote\nHomology\nTAPE BERT 0.73 0.36 0.21\nProtein-\nBERT\n0.74 – 0.22\nBERT medium 0.74 – –\nString2Seq – – 0.25\nProtBert 0.80 – –\nMT-BERT 0.75 0.39 0.32\nMT-BCNN 0.82 0.43 0.39\nMT-BLSTM 0.77 0.45 0.42\nPage 13 of 18\nAn and Weng  BMC Bioinformatics          (2022) 23:467 \n \nand the essence of remote homology detection, both of which view fold category as a \nclassification task. In conclusion, our MTL models effectively deciphered underlying \nprotein properties and obtained well downstream performance.\nComparison of pre‑training on natural language and protein language\nCompared to other Transformer-based models that acquire in prior knowledge of pro -\nteins, we can see that the BERT pre-trained on natural language in our MTL models can \nget good transfer learning performance. Furthermore, these results can verify the impor-\ntance of introducing in-domain knowledge to natural language pre-training models. \nIn other words, appropriately encoding protein property information can significantly \nboost the performance of downstream applications. More than that, it is essential to val -\nidate if pre-training on protein sequences outperforms pre-training on natural language \ncorpus. To do this, we employed two BERT-based models TAPE BERT [8] and Protein -\nBERT [7], pre-trained on 31 M protein domains from Pfam [71] and ~ 106 M proteins \nfrom UniProtKB/UniRef90 [72] respectively. Moreover, the structure and parameters of \nTAPE BERT are almost identical to our MT-BERT, and the major difference between \nthem lies in the pre-training corpus. Therefore, the performance of TAPE BERT can \napproximately reflect that of our MT-BERT if it is pre-trained on protein sequences. The \nProteinBERT and TAPE BERT underwent the same MTL process as we did on our MTL \nmodels. Table 5 reports the downstream results on the same TAPE benchmarks.\nCompared to our basic model MT-BERT, we can see that pre-training on protein \ndata significantly improves transferred performance. However, the best results are \ncomparable with our MT-BCNN and MT-BLSTM models that rely on human words \npre-training in Table  4. Furthermore, the increased results compared with the origi -\nnal TAPE BERT and ProteinBERT (see Tables  4 and 5 ) demonstrate the necessity of \nour MTL process for downstream tasks. In general, as Sun et al. [32] said, within-task \nand in-domain pre-training can largely boost the performance of BERT. However, the \ndelicately-designed MTL models like MT-BCNN and MT-BLSTM can largely narrow \nthe gap. In other words, pre-training on in-domain protein language deserves to per -\nform better, but this is not the main point to be focused on. The MTL process indeed \nenriches protein properties, and the most predominant increase exists in remote \nhomology detection. Therefore, the most important thing is how to subtly bring in \nstrong biological priors, such as structure- or evolution-related information.\nTable 5 Comparison of TAPE benchmark results of protein sequences and natural language pre-\ntraining models after the MTL process\nTAPE BERT(MT) and ProteinBERT(MT) are used to distinguish the original ones that did not implement the MTL process\nModel Structure Evolution\nSS prediction Contact prediction Remote homology\nTAPE BERT(MT) 0.79 0.42 0.32\nProtein-BERT(MT) 0.77 0.34 0.30\nMT-BERT 0.75 0.39 0.32\nPage 14 of 18An and Weng  BMC Bioinformatics          (2022) 23:467 \nAblation study employing two classification tasks\nMoreover, exploring which two of the three tasks provide relative critical informa -\ntion is equally meaningful. Since the former three supervised tasks are closely related, \nwe thereby tested how these well-designed MTL models perform if one of the tasks is \nmissed (see Tables 6, 7, 8).\nAfter removing one specific task, we can see an overall degraded performance with \nvarying degrees. The combination of superfamily and fold tasks gets the best overall out -\ncome in the ablation study. As described in SCOP 2 [25], the family and fold label explic-\nitly denote the ancestor and space structure of proteins respectively, while the proteins \nin the superfamily usually share a similar structure. The results of this ablation study are \nbasically consistent with the characteristics of proteins in different categories.\nOverall, the learned representations by two related tasks can still be well transferred \nto downstream scenarios. However, the best results in these applications occur when all \nthree highly-dependent classification tasks are considered.\nAblation study employing single classification task\nFinally, to validate if the reduced complexity of single-task learning could influence the \nmodel performance, we solely adopt one of the three classification tasks to enrich our \nmodels. Note that ST-BERT, ST-BCNN and ST-BLSTM denote a single-task learn -\ning version compared with three MTL models, in which the whole model architectures \nremain the same.\nTable 6 Comparison of TAPE benchmark results based solely on family and superfamily \nclassification task\nModel Structure Evolution\nSS prediction Contact prediction Remote homology\nMT-BERT 0.71 0.34 0.21\nMT-BCNN 0.75 0.35 0.23\nMT-BLSTM 0.72 0.38 0.25\nTable 7 Comparison of TAPE benchmark results based solely on family and fold classification task\nModel Structure Evolution\nSS prediction Contact prediction Remote homology\nMT-BERT 0.73 0.33 0.28\nMT-BCNN 0.77 0.37 0.32\nMT-BLSTM 0.75 0.37 0.35\nTable 8 Comparison of TAPE benchmark results based solely on fold and superfamily classification \ntask\nModel Structure Evolution\nSS prediction Contact prediction Remote homology\nMT-BERT 0.73 0.35 0.29\nMT-BCNN 0.74 0.39 0.34\nMT-BLSTM 0.75 0.41 0.37\nPage 15 of 18\nAn and Weng  BMC Bioinformatics          (2022) 23:467 \n \nTable 9 shows the model performance based on single-task learning. Overall, these \nresults are not competitive enough compared to those of MTL models when multiple \ntasks are involved. Notably, the single fold classification task significantly improved the \nperformance of remote homology detection. Moreover, this task also enabled the ST-\nBCNN model to obtain the best contact prediction result. Additionally, the superfamily \ncategory information may better specialize in predicting secondary structure.\nDiscussion\nThe advanced NLP models, pre-trained on abundant natural language corpus, can be \nwell transferred to decode biological sequences. Combined with the supervised train -\ning on multiple interrelated in-domain tasks, we demonstrate that these powerful NLP \nmodels can even outperform those fully modeling on protein language. Additionally, our \napproach further validates that transfer learning indeed improves downstream applica -\ntions [5]. Furthermore, it enlightens us that costly pre-training on in-domain language \ncorpus may not be indispensable, since our MTL  models transferred knowledge from \nnatural language and obtained competitive results in protein tasks (see “ Comparison of \npre-training on natural language and protein language” section). Conversely, the most \nfundamental part lies in how the in-domain knowledge can be subtly introduced. On the \none hand, pre-training on a large natural language corpus enriches advanced NLP mod -\nels abundant in prior knowledge, which can be well utilized to transfer to other domains. \nOn the other hand, the way of in-domain re-training plays a leading role in improving \nmodel performance. It is generally accepted that jointly learning interrelated tasks can \nleverage important information, thus outperforming sing-task learning. [15]. Consid -\nering the many interrelated tasks in the protein domain, we can then comprehensively \nemploy these tasks together. In this study, we adopted three classification tasks towards \nfamily, superfamily and fold categories hierarchically classified in SCOP 2, in order to \nencode implicit structural and evolutionary information from protein sequences.\nFurthermore, we elaborately designed an MTL architecture. It contains three kinds \nof backbones: MT-BERT, MT-BCNN and MT-BLSTM. MT-BERT simply employs pre-\ntrained BERT, while MT-BCNN and MT-BLSTM added CNN or LSTM layers to the top \nof BERT, aiming to avoid overfitting or better capture sequential invariance. Adequate \nexperiments show that these models capture proper structural and evolutionary rela -\ntionships by collectively learning  from three  correlated  sequence-level classifications. \nBesides, the most critical part depends on the transfer learning ability. Among three \nchallenging structure- or evolution-related tasks, the performance on remote homology \ndetection has been significantly improved compared to other SOTA Transformer-based \nTable 9 Comparison of TAPE benchmark results based solely on one classification task\nSS, Contact and Remote denote SS prediction, contact prediction and remote homology detection, respectively\nModel Task: family Task: superfamily Task: fold\nSS Contact Remote SS Contact Remote SS Contact Remote\nST-BERT 0.68 0.30 0.15 0.71 0.29 0.14 0.69 0.28 0.20\nST-BCNN 0.72 0.32 0.17 0.73 0.29 0.18 0.70 0.33 0.23\nST-BLSTM 0.70 0.31 0.18 0.71 0.28 0.20 0.72 0.30 0.26\nPage 16 of 18An and Weng  BMC Bioinformatics          (2022) 23:467 \nModels. Moreover, we can see the effectiveness of added CNN and LSTM layers in MT-\nBCNN and MT-BLSTM, which obtained better performance than MT-BERT.\nOverall, we believe that our proposed methodology can facilitate the study of how \nto draw on sophisticated tools in natural language to learn protein language, as well as \nthe way to encode strong biological priors into protein language models [5]. Further \nresearch can be focused on the MTL architecture itself. Since protein sequences differ \nfrom human sentences in structure and grammar, the most powerful MTL approach in \nNLP may not be the best protein language encoder. Moreover, employing other strongly-\ncorrelated tasks involving more fine-grained protein properties is expected to obtain \npromising downstream results as well.\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. org/ 10. 1186/ s12859- 022- 05031-z.\nAdditional file 1. Constructed protein dataset derived and cleaned from SCOP 2. Except for the protein sequence, \neach row of data contains specific SCOP domain identifications including FA-DOMID, SF-DOMID, CL, CF, SF, and FA, \nwhose descriptions can be found at https:// scop. mrc- lmb. cam. ac. uk/ downl oad.\nAcknowledgements\nSpecial thanks for the support of Professor Qi Gu and Dr Shuyu Zhang from the Institute of Zoology, Chinese Academy of \nSciences.\nAuthor contributions\nJA—Writing original manuscript & Data preparation & Experiments. XW—Revising manuscript.\nFunding\nThis work was supported by Innovative Research Team of Northeast Agricultural University (No. 54941012).\nAvailability of data and materials\nConstructed protein dataset derived and cleaned from SCOP 2 is included into the Additional file 1. Except for the pro-\ntein sequence, each row of data contains specific SCOP domain identifications including FA-DOMID, SF-DOMID, CL, CF, \nSF, and FA, whose descriptions can be found at https:// scop. mrc- lmb. cam. ac. uk/ downl oad.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nNo competing interests could have appeared to influence the work reported in this paper.\nReceived: 14 July 2022   Accepted: 31 October 2022\nReferences\n 1. Dash NS. Context and contextual word meaning. SKASE J Theor Linguist. 2008.\n 2. Zhang Y, Wallace B. A sensitivity analysis of (and practitioners’ guide to) convolutional neural networks for sentence \nclassification. arXiv preprint arXiv: 1510. 03820, 2015.\n 3. Elman JL. Finding structure in time. Cogn Sci. 1990;14(2):179–211.\n 4. Mikolov T, et al. Efficient estimation of word representations in vector space. arXiv preprint arXiv: 1301. 3781, 2013.\n 5. Bepler T, Berger B. Learning the protein language: evolution, structure, and function. Cell Syst. 2021;12(6):654–69.\n 6. Devlin J, et al. Bert: pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv: \n1810. 04805, 2018.\n 7. Brandes N, et al. ProteinBERT: a universal deep-learning model of protein sequence and function. bioRxiv, 2021.\n 8. Rao R, et al. Evaluating protein transfer learning with TAPE. Adv Neural Inform Process Syst. 2019. 32.\n 9. Rives A, et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein \nsequences. Proc Natl Acad Sci. 2021;118:15.\n 10. Zhang Y, et al. A novel antibacterial peptide recognition algorithm based on BERT. Brief Bioinform. 2021;22(6):200.\nPage 17 of 18\nAn and Weng  BMC Bioinformatics          (2022) 23:467 \n \n 11. Elnaggar A, et al. ProtTrans: towards cracking the language of Life’s code through self-supervised deep learning and \nhigh performance computing. arXiv preprint arXiv: 2007. 06225, 2020.\n 12. Shah SMA, et al. GT-finder: classify the family of glucose transporters with pre-trained BERT language models. Com-\nput Biol Med. 2021;131: 104259.\n 13. Ho Q-T, Le NQK, Ou Y-Y. FAD-BERT: improved prediction of FAD binding sites using pre-training of deep bidirectional \ntransformers. Comput Biol Med. 2021;131: 104258.\n 14. Qiao Y, Zhu X, Gong H. BERT-Kcr: prediction of lysine crotonylation sites by a transfer learning method with pre-\ntrained BERT models. Bioinformatics. 2022;38(3):648–54.\n 15. Chen S, Zhang Y, Yang Q, Multi-task learning in natural language processing: an overview. arXiv preprint arXiv: 2109. \n09138, 2021.\n 16. Ruder S. An overview of multi-task learning in deep neural networks. arXiv preprint arXiv: 1706. 05098, 2017.\n 17. Qi Y, et al. Semi-supervised multi-task learning for predicting interactions between HIV-1 and human proteins. \nBioinformatics. 2010;26(18):i645–52.\n 18. Yang M, et al. Linking drug target and pathway activation for effective therapy using multi-task learning. Sci Rep. \n2018;8(1):1–10.\n 19. Sadawi N, et al. Multi-task learning with a natural metric for quantitative structure activity relationship learning. J \nCheminform. 2019;11(1):1–13.\n 20. Gilvary C, Dry JR, Elemento O. Multi-task learning predicts drug combination synergy in cells and in the clinic. \nBioRxiv, 2019: p. 576017.\n 21. Elnaggar A, et al. End-to-end multitask learning, from protein language to protein features without alignments. \nbioRxiv, 2020: p. 864405.\n 22. Charuvaka A, Rangwala H. Classifying protein sequences using regularized multi-task learning. IEEE/ACM Trans \nComput Biol Bioinf. 2014;11(6):1087–98.\n 23. Murzin AG, et al. SCOP: a structural classification of proteins database for the investigation of sequences and struc-\ntures. J Mol Biol. 1995;247(4):536–40.\n 24. Orengo CA, et al. CATH—a hierarchic classification of protein domain structures. Structure. 1997;5(8):1093–109.\n 25. Andreeva A, et al. The SCOP database in 2020: expanded classification of representative family and superfamily \ndomains of known protein structures. Nucleic Acids Res. 2019;48(D1):D376–82.\n 26. Villegas-Morcillo A, Gomez AM, Sanchez V. An analysis of protein language model embeddings for fold prediction. \nBrief Bioinform. 2022;23(3):142.\n 27. Torrey L, Shavlik J. Transfer learning. In: Handbook of research on machine learning applications and trends: algo-\nrithms, methods, and techniques. 2010, IGI Global. pp. 242–264.\n 28. Filipavicius M, et al. Pre-training protein language models with label-agnostic binding pairs enhances performance \nin downstream tasks. arXiv preprint arXiv: 2012. 03084, 2020.\n 29. Buchan DW, Jones DT. The PSIPRED protein analysis workbench: 20 years on. Nucleic Acids Res. \n2019;47(W1):W402–7.\n 30. Ferruz N, Schmidt S, Höcker B. ProteinTools: a toolkit to analyze protein structures. Nucleic Acids Res. \n2021;49(W1):W559–66.\n 31. Waterhouse A, et al. SWISS-MODEL: homology modelling of protein structures and complexes. Nucleic Acids Res. \n2018;46(W1):W296–303.\n 32. Sun C, et al. How to fine-tune bert for text classification? In: China national conference on Chinese computational \nlinguistics. 2019. Springer.\n 33. Vaswani A, et al. Attention is all you need. In: Advances in neural information processing systems. 2017.\n 34. Vig J, et al. Bertology meets biology: interpreting attention in protein language models. arXiv preprint arXiv: 2006. \n15222, 2020.\n 35. Wolf T, et al. Transformers: state-of-the-art natural language processing. In: Proceedings of the 2020 conference on \nempirical methods in natural language processing: system demonstrations. 2020.\n 36. Ezen-Can A. A comparison of LSTM and BERT for small corpus. arXiv preprint arXiv: 2009. 05451, 2020.\n 37. Albawi S, Mohammed TA, Al-Zawi S. Understanding of a convolutional neural network. In: 2017 International confer-\nence on engineering and technology (ICET). 2017. IEEE.\n 38. Kalchbrenner N, Grefenstette E, Blunsom P . A convolutional neural network for modelling sentences. arXiv preprint \narXiv: 1404. 2188, 2014.\n 39. Hochreiter S, Schmidhuber J. Long short-term memory. Neural Comput. 1997;9(8):1735–80.\n 40. Liu X, et al. Multi-task deep neural networks for natural language understanding. In: Proceedings of the 57th annual \nmeeting of the association for computational linguistics. 2019.\n 41. Kelleher JD. Deep learning. MIT Press; 2019.\n 42. Zhang Y, Yang Q. A survey on multi-task learning. IEEE Trans Knowl Data Eng. 2021.\n 43. Capel H, Feenstra KA, Abeln S. Multi-task learning to leverage partially annotated data for PPI interface prediction. \n2022.\n 44. Kendall A, Gal Y, Cipolla R. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. \nIn: Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.\n 45. Nogueira F. Bayesian optimization: open source constrained global optimization tool for Python. https:// github. \ncom/ fmfn/ Bayes ianOp timiz ation. 2014.\n 46. Paszke A, et al. Pytorch: an imperative style, high-performance deep learning library. Adv Neural Inform Process Syst. \n2019. 32.\n 47. Wang S, et al. Protein secondary structure prediction using deep convolutional neural fields. Sci Rep. 2016;6(1):1–11.\n 48. Pollastri G, et al. Improving the prediction of protein secondary structure in three and eight classes using recurrent \nneural networks and profiles. Proteins Struct Funct Bioinform. 2002;47(2):228–35.\n 49. Xiao Y, et al. Modeling protein using large-scale pretrain language model. arXiv preprint arXiv: 2108. 07435, 2021.\n 50. Spencer M, Eickholt J, Cheng J. A deep learning network approach to ab initio protein secondary structure predic-\ntion. IEEE/ACM Trans Comput Biol Bioinf. 2014;12(1):103–12.\nPage 18 of 18An and Weng  BMC Bioinformatics          (2022) 23:467 \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \n 51. Drozdetskiy A, et al. JPred4: a protein secondary structure prediction server. Nucleic Acids Res. \n2015;43(W1):W389–94.\n 52. Jumper J, et al. Highly accurate protein structure prediction with AlphaFold. Nature. 2021;596(7873):583–9.\n 53. Klausen MS, et al. NetSurfP-2.0: improved prediction of protein structural features by integrated deep learning. \nProteins Struct Funct Bioinform. 2019;87(6):520–7.\n 54. Cuff JA, Barton GJ. Evaluation and improvement of multiple sequence methods for protein secondary structure \nprediction. Proteins Struct Funct Bioinform. 1999;34(4):508–19.\n 55. Wu S, Zhang Y. A comprehensive assessment of sequence-based and template-based methods for protein contact \nprediction. Bioinformatics. 2008;24(7):924–31.\n 56. Ma J, et al. Protein contact prediction by integrating joint evolutionary coupling analysis and supervised learning. \nBioinformatics. 2015;31(21):3506–13.\n 57. Adhikari B, Hou J, Cheng J. DNCON2: improved protein contact prediction using two-level deep convolutional \nneural networks. Bioinformatics. 2018;34(9):1466–72.\n 58. AlQuraishi M. ProteinNet: a standardized data set for machine learning of protein structure. BMC Bioinform. \n2019;20(1):1–10.\n 59. Moult J, et al. Critical assessment of methods of protein structure prediction (CASP)—round XII. Proteins Struct \nFunct Bioinform. 2018;86:7–15.\n 60. Chen J, et al. A comprehensive review and comparison of different computational methods for protein remote \nhomology detection. Brief Bioinform. 2018;19(2):231–44.\n 61. Liu B, Jiang S, Zou Q. HITS-PR-HHblits: protein remote homology detection by combining PageRank and hyperlink-\ninduced topic search. Brief Bioinform. 2020;21(1):298–308.\n 62. Liu B et al. Using distances between Top-n-gram and residue pairs for protein remote homology detection. In: BMC \nBioinformatics. 2014. Springer.\n 63. Hou J, Adhikari B, Cheng J. DeepSF: deep convolutional neural network for mapping protein sequences to folds. \nBioinformatics. 2018;34(8):1295–303.\n 64. Fox NK, Brenner SE, Chandonia J-M. SCOPe: structural classification of proteins—extended, integrating SCOP and \nASTRAL data and classification of new structures. Nucleic Acids Res. 2014;42(D1):D304–9.\n 65. Berman HM, et al. The protein data bank. Nucleic Acids Res. 2000;28(1):235–42.\n 66. Cox MA, Cox TF. Multidimensional scaling. In: Handbook of data visualization. Springer; 2008. p. 315–47.\n 67. Carroll JD, Arabie P . Multidimensional scaling. Measurement, judgment and decision making; 1998. pp. 179–250.\n 68. Hout MC, Papesh MH, Goldinger SD. Multidimensional scaling. Wiley Interdiscip Rev Cognit Sci. 2013;4(1):93–103.\n 69. Capel H et al. ProteinGLUE: a multi-task benchmark suite for self-supervised protein modeling. bioRxiv; 2021.\n 70. Hanson J, et al. Accurate prediction of protein contact maps by coupling residual two-dimensional bidirectional \nlong short-term memory with convolutional neural networks. Bioinformatics. 2018;34(23):4039–45.\n 71. El-Gebali S, et al. The Pfam protein families database in 2019. Nucleic Acids Res. 2019;47(D1):D427–32.\n 72. Boutet E, et al. UniProtKB/Swiss-Prot, the manually annotated section of the UniProt knowledge base: how to use \nthe entry view. In: Plant Bioinformatics. Springer; 2016. p. 23–54.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.741518497467041
    },
    {
      "name": "Natural language processing",
      "score": 0.5692559480667114
    },
    {
      "name": "DECIPHER",
      "score": 0.5084108710289001
    },
    {
      "name": "Encoding (memory)",
      "score": 0.5053189396858215
    },
    {
      "name": "Transfer of learning",
      "score": 0.5024900436401367
    },
    {
      "name": "Natural language",
      "score": 0.4906710386276245
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4838908612728119
    },
    {
      "name": "Transformer",
      "score": 0.4830915927886963
    },
    {
      "name": "Relevance (law)",
      "score": 0.46187615394592285
    },
    {
      "name": "Bioinformatics",
      "score": 0.190883606672287
    },
    {
      "name": "Biology",
      "score": 0.1686442494392395
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I169572211",
      "name": "Northeast Agricultural University",
      "country": "CN"
    }
  ]
}