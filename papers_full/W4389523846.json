{
  "title": "UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning",
  "url": "https://openalex.org/W4389523846",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3111181451",
      "name": "Ahmed Masry",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3184326272",
      "name": "Parsa Kavehzadeh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4316065853",
      "name": "Do Long",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2146337708",
      "name": "Enamul Hoque",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1208744602",
      "name": "Shafiq Joty",
      "affiliations": [
        "Nanyang Technological University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3014611590",
    "https://openalex.org/W3184735396",
    "https://openalex.org/W4304192731",
    "https://openalex.org/W4312233877",
    "https://openalex.org/W4297435087",
    "https://openalex.org/W3174476431",
    "https://openalex.org/W884650706",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3104953317",
    "https://openalex.org/W4312879041",
    "https://openalex.org/W1895577753",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2766732270",
    "https://openalex.org/W1763968285",
    "https://openalex.org/W2967615747",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W3171730885",
    "https://openalex.org/W4385571411",
    "https://openalex.org/W4313122483",
    "https://openalex.org/W3174519801",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W3035140194",
    "https://openalex.org/W3034837210",
    "https://openalex.org/W3193402170",
    "https://openalex.org/W4226149412",
    "https://openalex.org/W2783459461",
    "https://openalex.org/W2152922709",
    "https://openalex.org/W4389519254",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4296938090",
    "https://openalex.org/W3209274285",
    "https://openalex.org/W4386065837",
    "https://openalex.org/W4226020328",
    "https://openalex.org/W3092234597",
    "https://openalex.org/W4385573930",
    "https://openalex.org/W4385569882",
    "https://openalex.org/W3090149233",
    "https://openalex.org/W4226182655",
    "https://openalex.org/W136732505",
    "https://openalex.org/W3009518609",
    "https://openalex.org/W2971034336",
    "https://openalex.org/W4385565485",
    "https://openalex.org/W4225666925",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4288329833",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W4304013646",
    "https://openalex.org/W3167118264",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W3176851559",
    "https://openalex.org/W4294619406",
    "https://openalex.org/W4385571140",
    "https://openalex.org/W639708223",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3088290493",
    "https://openalex.org/W2959442326",
    "https://openalex.org/W3184784418",
    "https://openalex.org/W4361230777",
    "https://openalex.org/W2307512708",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W4385570934",
    "https://openalex.org/W4319793767",
    "https://openalex.org/W4283799331",
    "https://openalex.org/W4309591663",
    "https://openalex.org/W4362679631",
    "https://openalex.org/W3201174429",
    "https://openalex.org/W4280596375",
    "https://openalex.org/W4285255856",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W4225683910",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4389523957",
    "https://openalex.org/W2552929535",
    "https://openalex.org/W3116465435",
    "https://openalex.org/W2135415614",
    "https://openalex.org/W3118703676",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4244888246",
    "https://openalex.org/W2963420691"
  ],
  "abstract": "Charts are widely used for data analysis, providing visual representations and insights into complex data. To facilitate chart-based data analysis using natural language, several downstream tasks have been introduced recently such as chart question answering and chart summarization. However, existing methods for these tasks often rely on pretraining on language or vision-language tasks, neglecting the explicit modeling of chart structures (e.g., how chart elements are related to each other). To address this, we first build a large corpus of charts covering diverse topics and visual styles. We then present UniChart, a pretrained model for chart comprehension and reasoning. UniChart encodes the relevant text, data, and visual elements of charts and then uses a chart-grounded text decoder for text generation. We propose several chart-specific pretraining tasks that include: (i) low-level tasks to extract the visual elements (e.g., bars, lines) and data from charts, and (ii) high-level tasks to acquire chart understanding and reasoning skills. Our experiments demonstrate that pretraining UniChart on a large corpus with chart-specific objectives, followed by fine-tuning, yields state-of-the-art performance on four downstream tasks. Moreover, our model exhibits superior generalizability to unseen chart corpus, surpassing previous approaches that lack chart-specific objectives and utilize limited chart resources.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 14662–14684\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nUniChart: A Universal Vision-language Pretrained Model for\nChart Comprehension and Reasoning\nAhmed Masry♣∗, Parsa Kavehzadeh ♣∗, Xuan Long Do ♠†, Enamul Hoque ♣, Shafiq Joty ♠♦\n♣York University, Canada\n♠Nanyang Technological University, Singapore,♦Salesforce AI\n{parsaka, enamulh}@yorku.ca\nsrjoty@ntu.edu.sg\nahmed.elmasry24653@gmail.com, xuanlong.do@u.nus.edu\nAbstract\nCharts are widely used for data analysis, pro-\nviding visual representations and insights into\ncomplex data. To facilitate chart-based data\nanalysis using natural language, several down-\nstream tasks have been introduced recently\nsuch as chart question answering and chart\nsummarization. However, existing methods for\nthese tasks often rely on pretraining on lan-\nguage or vision-language tasks, neglecting the\nexplicit modeling of chart structures (e.g., how\nchart elements are related to each other). To ad-\ndress this, we first build a large corpus of charts\ncovering diverse topics and visual styles. We\nthen present UniChart, a pretrained model for\nchart comprehension and reasoning. UniChart\nencodes the relevant text, data, and visual ele-\nments of charts and then uses a chart-grounded\ntext decoder for text generation. We propose\nseveral chart-specific pretraining tasks that in-\nclude: (i) low-level tasks to extract the visual\nelements (e.g., bars, lines) and data from charts,\nand (ii) high-level tasks to acquire chart un-\nderstanding and reasoning skills. Our exper-\niments demonstrate that pretraining UniChart\non a large corpus with chart-specific objectives,\nfollowed by fine-tuning, yields state-of-the-art\nperformance on four downstream tasks. More-\nover, our model exhibits superior generalizabil-\nity to unseen chart corpus, surpassing previous\napproaches that lack chart-specific objectives\nand utilize limited chart resources.\n1 Introduction\nInformation visualizations such as bar charts and\nline charts are commonly used for analyzing data,\ninferring key insights and making informed deci-\nsions (Hoque et al., 2022). However, understand-\ning important patterns and trends from charts and\nanswering complex questions about them can be\ncognitively taxing. Thus, to facilitate users in ana-\nlyzing charts, several downstream NLP tasks over\n∗Equal contribution.\n† Now affiliated with NUS.\ncharts have been proposed recently, including chart\nquestion answering (Masry et al., 2022; Kantharaj\net al., 2022; Lee et al., 2022), natural language gen-\neration for visualizations (Obeid and Hoque, 2020;\nShankar et al., 2022) and automatic data story gen-\neration (Shi et al., 2020).\nA dominant strategy to tackle these downstream\ntasks is to utilize pretrained models (Su et al., 2020;\nLi et al., 2020b; Kim et al., 2021; Cho et al., 2021)\ntrained on langauge and vision tasks (Du et al.,\n2022). However, although effective, such mod-\nels may not be optimal for chart-specific tasks be-\ncause they are trained on large text corpus and/or\nimage-text pairs without any specific focus on chart\ncomprehension. In reality, charts differ from natu-\nral images in that they visually communicate the\ndata using graphical marks (e.g., bars, lines) and\ntext (e.g., titles, labels, legends). Readers can dis-\ncover important patterns, trends, and outliers from\nsuch visual representation (Munzner, 2014). Exist-\ning pretrained models do not consider such unique\nstructures and communicative goals of charts. For\ninstance, Pix2Struct (Lee et al., 2022) is a pre-\ntrained image-to-text model designed for situated\nlanguage understanding. Its pretraining objective\nfocuses on screenshot parsing based on HTML\ncodes of webpages, with a primary emphasis on\nlayout understanding rather than reasoning over\nthe visual elements. MatCha (Liu et al., 2022b)\nextends Pix2Struct by incorporating math reason-\ning and chart data extraction tasks, but it still lacks\ntraining objectives for text generation from charts\nand it was trained on a limited number of charts.\nIn this work, we present UniChart, a pretrained\nmodel designed specifically for chart comprehen-\nsion and reasoning. UniChart is pretrained on a\nlarge corpus of charts and it aims to serve as a Uni-\nversal model for various chart-related downstream\ntasks (Fig. 1). Inspired by the model architecture\nfrom Kim et al. (2022), UniChart consists of two\nmodules: (1) a chart encoder, which takes the chart\n14662\nChart Image\n Encoder\n<data table generation> </answer>\n<chart summarization> </answer>\n<numerical visual reasoning> Sum two leftmost values\nof gray line </answer>\n<open-ended question answering> What is the birth rate in\nthe U.S. from 2005 to 2019? </answer>\n90.0\nThe line chart shows the birth rate in the United States from 2005 to 2019, by\npoverty status. The numbers represent the number of births per 1,000\nwomen. In 2019, the birth rate for women below the poverty level was 74, for\nwomen with an income of 100 to 199 percent of the poverty level it was 61,\nand for women with an income of 200 percent or more of the poverty level it\nwas 44. The trend over the years shows a decline in birth rates for all income\nlevels, with the highest birth rate in 2005 for women below the poverty level\nat 95, and the lowest birth rate in 2019 for women with an income of 200\npercent or more of the poverty level at 44.\nCharacteristic | Income below poverty level | Income 100 to 199 percent of\npoverty level | Income 200 percent of poverty level or more & 2019 | 74 | 61 |\n44 & 2018 | 77 | 60 | 45 & 2017 | 76 | 62 | 45 & 2016 | 78 | 61 | 44 & 2015 |\n76 | 61 | 44 & 2014 | 77 | 60 | 43 & 2013 | 76 | 60 | 43 & 2012 | 82 | 63 | 45 &\n2011 | 81 | 62 | 45 & 2010 | 85 | 64 | 45 & 2009 | 92 | 69 | 46 & 2008 | 96 | 72\n| 48 & 2007 | 92 | 69 | 44 & 2006 | 91 | 68 | 45 & 2005 | 95 | 70 | 45\nThe trend over the years shows a decline in birth rates for all income levels,\nwith the highest birth rate in 2005 for women below the poverty level at 95,\nand the lowest birth rate in 2019 for women with an income of 200 percent or\nmore of the poverty level at 44.\nText\n Decoder\nInput Image and Prompts OutputUniChart\nFigure 1: Our UniChart model with different pretraining objectives. The model consists of two main modules: Chart Image\nEncoder, and Text Decoder. Four different pretraining objectives are specified in different colors;data table generation, chart\nsummarization, numerical and visual reasoning, and open-ended question answering.\nimage as input, and (2) a text decoder, trained to\ndecode the expected output based on the encoded\nimage and the text input fed in the decoder as task\nprompt. We performed pretraining on a diverse\nset of 611K charts that we collected from multiple\nreal-world sources. Our pretraining objectives in-\nclude both low-level tasks focused on extracting\nvisual elements and data from chart images, as well\nas high-level tasks, intended to align more closely\nwith downstream applications. One key challenge\nfor pretraining was that most charts in the corpus do\nnot come with informative summaries, which are\ncritical for various downstream tasks. To address\nthis challenge, we used knowledge distillation tech-\nniques to leverage large language models (LLMs)\nfor opportunistically collecting chart summaries,\nwhich were then used during pretraining.\nWe conducted extensive experiments and analy-\nsis on various chart-specific downstream tasks to\nevaluate the effectiveness of our approach. Specifi-\ncally, we evaluatedUniChart on two chart question\nanswering datasets, ChartQA (Masry et al., 2022)\nand OpenCQA (Kantharaj et al., 2022), and found\nthat it outperformed the state-of-the-art models in\nboth cases. For chart summarization, UniChart\nachieves superior performance in both human and\nautomatic evaluation measures such as BLEU (Post,\n2018) and ratings from ChatGPT (OpenAI, 2022).\nMoreover, UniChart achieved state-of-the-art re-\nsults in the Chart-to-Table downstream task. Fi-\nnally, our model showed improved time and mem-\nory efficiency compared to the previous state-of-\nthe-art model, MatCha, being more than 11 times\nfaster with 28% fewer parameters.\nOur primary contributions are: (i) A pretrained\nmodel for chart comprehension with unique low-\nlevel and high-level pretraining objectives specific\nto charts; (ii) a large-scale chart corpus for pretrain-\ning, covering a diverse range of visual styles and\ntopics; (iii) extensive automatic and human evalu-\nations that demonstrate the state-of-the-art perfor-\nmance of UniChart across various chart-specific\nbenchmark task while optimizing time and mem-\nory efficiency. We have made our code and chart\ncorpus publicly available at https://github.com/vis-\nnlp/UniChart.\n2 Related Work\n2.1 Vision-language Pretraining\nPretrained models have dominated in many vision\nand language tasks (Du et al., 2022). Building a pre-\ntrained vision-language model typically involves\nthree steps. First, textual input is usually encoded\nusing BERT-based encoder (Lu et al., 2019; Rad-\nford et al., 2021; Li et al., 2021, 2022). Second,\nfor the input image, some prior studies utilize Fast-\nRCNN (Ren et al., 2015) to encode the sequence\nof object regions as the image features (Li et al.,\n2019; Lu et al., 2019; Chen et al., 2020). However,\nthis method may neglect some crucial regions in an\nimage. Recent approaches favor encoding the im-\nage as a whole (Huang et al., 2020, 2021; Li et al.,\n2021, 2022) by using ResNet (He et al., 2016) or\nViT (Dosovitskiy et al., 2021). Third, to fuse the\ntextual and visual features, prior work mostly either\ndesigns a fusion encoder (Tan and Bansal, 2019;\nSu et al., 2020; Cho et al., 2021; Kim et al., 2021)\nor a dual encoder (Radford et al., 2021; Jia et al.,\n2021; Li et al., 2022). Finally, multiple common\ncross-modal pretraining tasks have been designed\nsuch as image-text matching (Chen et al., 2020;\nLi et al., 2020a), cross-modal contrastive learning\n(Radford et al., 2021; Jia et al., 2021) and genera-\ntion tasks such as visual question answering (Cho\net al., 2021; Wang et al., 2021).\n14663\nOur work is also related to multimodal docu-\nment understanding tasks that involve analyzing\nthe textual content, layout, and visual elements\nof documents (Xu et al., 2020b,a; Wang et al.,\n2022; Huang et al., 2022; Kim et al., 2022; Tang\net al., 2022). These tasks can be addressed using\nencoder-only and encoder-decoder architectures.\nEncoder-only models rely on OCR engines to ex-\ntract text from document images and use BERT-\nlike encoders augmented with specialized embed-\ndings to encode layout and visual features (Xu\net al., 2020b,a; Wang et al., 2022; Huang et al.,\n2022). In contrast, encoder-decoder architectures\ncombine transformer-based encoders with autore-\ngressive text decoders for text generation tasks re-\nlated to documents (Tang et al., 2022; Kim et al.,\n2022; Lee et al., 2022). While Tang et al. (2022)\nincorporates an OCR tool to supplement the vision\nencoder, Kim et al. (2022) and Lee et al. (2022)\noperate in an end-to-end manner without external\nOCR engines. In line with the latter approach, our\nmodel adopts an end-to-end encoder-decoder archi-\ntecture (Kim et al., 2022).\nIn general, the above work focuses on training on\nlarge image-text pairs or text corpus, lacking focus\non chart understanding. One exception is MatCha\n(Liu et al., 2022b), a pretrained chart model based\non Pix2Struct (Lee et al., 2022), which achieved\nSoTA on chart question answering and summariza-\ntion tasks. However, MatCha’s pretraining tasks\nmainly focus on data table generation without fo-\ncusing on text generation tasks. The model is also\npretrained with reasoning tasks using the textual\ndatasets which might limit its visual reasoning abil-\nity. Our model is trained on a larger corpus with\nchart-specific pretraining objectives, including vi-\nsual reasoning and text generation, making it more\nversatile for various chart-related tasks.\n2.2 Chart-related Downstream Tasks\nThere has been growing interest in solving var-\nious chart-related tasks. Chart question answer-\ning (ChartQA) tackles questions about charts, with\nbenchmarks like (Methani et al., 2020) and (Masry\net al., 2022) targeting factoid questions involv-\ning visual and arithmetic reasoning. Open-ended\nquestion answering (OpenCQA) task requires an\nexplanatory answer by reasoning with the chart\ncontent (Kantharaj et al., 2022). Finally, Chart-to-\nText generates natural language summaries from\ninput charts (Shankar et al., 2022), while Chart-to-\nTable generates underlying data tables (Choi et al.,\n2019). We evaluate our model on these four chart-\nrelated tasks, as they involve the interaction be-\ntween language and vision and have publicly avail-\nable datasets. There are a few other tasks such as in-\nfographics understanding (Mathew et al., 2022) and\nquestion answering with science diagram (Kemb-\nhavi et al., 2016), however, in this work, we only\nfocus on chart-related tasks.\n3 Chart Pretraining Corpus\nTo build a large and diverse corpus with various\nstyles, topics, and storage formats, we crawled\ncharts from various online sources. Additionally,\nwe utilized publicly available chart datasets suit-\nable for pretraining. The collected charts can be\ncategorized into two types: charts with underlying\ndata tables and charts without data tables.\n3.1 Charts with Data Tables\nCharts with an underlying data table are collected\nin three ways: (i) utilize existing datasets, (ii) ex-\ntract SVG charts, and (iii) data augmentation.\n• Utilize Existing Datasets Our goal was to\ntrain the model based on real-world data, thus, we\ndid not consider the ones that are generated from\nsynthetic data (Kafle et al., 2018; Kahou et al.,\n2018). In particular, we used the following five\nchart datasets for which the underlying data tables\nwere available: (i) Statista (statista.com) (Shankar\net al., 2022), (ii) Our World In Data or OWID (our-\nworldindata.org) (Masry et al., 2022), (iii) Organ-\nisation for Economic Co-operation and Develop-\nment or OECD (oecd.org) (Masry et al., 2022),\n(iv) PlotQA (Methani et al., 2020), and (v) a subset\nof the ChartInfo (ChartInfo, 2022) dataset that pro-\nvides bounding box annotations for data encoding\nmarks (e.g., bars in a bar chart).\n• Extract SVG Charts: We extracted charts\nin SVG format from the Chartblocks and Plotly\ndatasets of the Beagle corpus (Battle et al., 2018).\nThese charts do not come with data tables, but the\ndata can be extracted accurately from the SVG ele-\nments. The steps for preparing these charts are: (1)\nidentify axis labels and legends using specific class\nnames of HTML attribute, (2) extract bounding\nboxes of chart elements (e.g., bars, line) using SVG\nattribute properties (e.g., size and location of\n<rect>), (3) construct the underlying data table\nby iterating through each of the <g> elements to\nfind data values of each data attribute. When data\nlabels are absent, we utilize the scale information\nbased on the axis labels and tick marks of the chart\n14664\nand the bounding box information of data encoding\nmarks to recover the data values.\n• Data Augmentation We further augmented\nthe corpus by creating charts from publicly avail-\nable data tables. We used the The Web Data Com-\nmons (WDC) (WDC, 2022), which used Common\nCrawl1 to collect a large amount of structured data.\nThe charts are created in the following steps:\n(i) Data pre-processing: Since many tables in\nWDC contain more than three columns, we decom-\nposed so that tables are suitable for creating desired\nchart types (e.g., bars, lines, and pie charts). In par-\nticular, we automatically analyze the data type of\neach column (e.g, numeric vs. categorical) and\nthen randomly choose one column with numeric\ndata values and one/two column(s) with categorical\ndata. We also limit the maximum number of rows\nof the table to 8 so that the corresponding chart can\nfit within reasonable screen space.\n(ii) Chart generation: To generate visually diverse\ncharts, we used the D3 (Bostock et al., 2011) library\nthat provides great flexibility in terms of creating\ndiverse visualization styles. We also employed\nVega-Lite (Satyanarayan et al., 2016) which creates\ncharts based on declarative JSON syntax. We used\nsimple heuristics for determining chart types from\nthe data table (Mackinlay et al., 2007). We created\nfour types of charts: (1) vertical simple bar charts\nwith one numeric data column, (2) vertical grouped\nbar charts, (3) pie charts, and (4) line charts (both\nsingle series and multi-series).\n(iii) Visual diversification: To create visually di-\nverse charts resembling real-world variations, we\nmanipulated the following visual style properties:\n(1) Colors and shapes: Color schemes from Color-\nBrewer2 and Tableau3 were chosen for categorical\ndata attributes. We also varied shape properties\nsuch as bar thickness, line types (e.g., continuous\nvs dotted), and legend shape types (e.g., rect, cir-\ncle). (2) Position and distance: We also varied\nbar positions and distances with respect to axis la-\nbels. (3) Guides: Charts may contain additional\nguides such as grids, so we generate charts with\nand without grids to diversify styles.\nFig. 2 depicts a visually diverse set of charts\ncreated using this augmentation process. In total,\nwe created a total of 189,839 charts (Table 4).\n1https://commoncrawl.org/\n2https://colorbrewer2.org/\n3tableau.com\n3.2 Charts without Data Tables\nMany online charts are available only as images,\nwithout corresponding data tables. However, they\ncan still be valuable for large-scale pretraining as\nwe can extract chart elements and rich textual con-\ntents (e.g., titles, surrounding texts, captions) us-\ning object detection and optical character recog-\nnition (OCR) techniques. We collected image\nchart datasets such as LineCap (Mahinpei et al.,\n2022) and Neural Caption Generation (Spreafico\nand Carenini, 2020) since they provide high-quality\nsummaries. We also used the Pew dataset from\n(Shankar et al., 2022) and further augmented it\nby an crawling additional 1K charts. Finally, we\nused the ExcelChart400K dataset (Luo et al., 2021)\nwhich only provides bounding boxes without under-\nlying data tables. We also considered other existing\nimage chart datasets such as Vis30K (Chen et al.,\n2021) and VisImage (Deng et al., 2020), but they\nare not suitable as they usually have poor resolution\nand lack meaningful textual content (e.g., titles).\n3.3 Augmentation by Knowledge Distillation\nfor Chart-to-text Generation Tasks\nChart-related downstream tasks such as chart sum-\nmarization (Shankar et al., 2022) and open-ended\nquestion answering (Kantharaj et al., 2022) require\ngenerating informative and relevant texts. How-\never, for most of the charts in the pretraining cor-\npus, there are either no associated summaries or\nthe summaries that are collected opportunistically\nsuch as the Statista dataset (Shankar et al., 2022)\nlack quality (e.g., too short and not very infor-\nmative). Training on such substandard “ground-\ntruth” summaries can negatively affect the overall\nmodel performance as shown in text summariza-\ntion (Kryscinski et al., 2019; Clark et al., 2021).\nIndeed, Goyal et al. (2022) and Liu et al. (2023b)\nhave recently shown that human raters prefer sum-\nmaries generated by LLMs, especially the ones that\nare instruction-tuned such as InstructGPT (Ouyang\net al., 2022), compared to the reference summaries\nin various text summarization datasets. Conse-\nquently, the instruction-tuned LLMs have been\nsuccessfully used as a annotator in several recent\nstudies (DING et al., 2023; Qin et al., 2023).\nInspired by these findings, we leveraged Instruct-\nGPT to generate coherent and relevant text. Specifi-\ncally, we promptedtext-davinci-003 by pro-\nviding the underlying data table as input and one\nexemplar (i.e., 1-shot in-context learning). Since\n14665\ngenerating summaries for thousands of charts by\ncalling OpenAI API is quite costly, we devised\na knowledge distillation approach. We first used\ntext-davinci-003 to create a small dataset of\n3700 summaries for different chart types. Next, we\nfinetuned Flan-T5 XL (Chung et al., 2022) on this\ndataset. Finally, we utilized the finetuned Flan-T5\nmodel to generate summaries for charts that do not\nhave an associated summary. More details about\nthis approach can be found in Appendix A.2.\n3.4 Datasets Analysis\nOur chart pretraining corpus has over 611K charts\ncovering a diverse range of bar charts, line charts,\nand pie charts (Table 4). Data tables of Simple\ncharts have two columns (simple bar charts or\nsingle-series line charts), whereas Complex charts\ninvolve at least three columns (e.g., stacked or\ngroup bar charts, line charts with multiple lines).\nThe first two chart groups in Table 4 come with\nan underlying data table which cover over 80%\nof the corpus. The bottom group contains five\ndatasets which only provide charts in image for-\nmat without a data table4 and cover about 20% of\nthe corpus. Bar charts make up the majority portion\n(58.51%), followed by line charts (32.94%) and pie\ncharts (9.39%). About 60% of the charts have mul-\ntiple columns in their data tables, while 40% of\nthe charts have only two columns. 5 The corpus\nalso covers a diverse range of topics including tech-\nnology, economy, politics, health, and society. To\nensure a fair evaluation, we excluded charts found\nin the validation and test sets of the downstream\ntasks from our pretraining corpus. Details about\nthe linguistics of the corpus textual elements can\nbe found in Appendix A.3.\n4 Method\nWe propose UniChart, a unified pretrained model\nfor chart comprehension and reasoning. This sec-\ntion first introduces the UniChart architecture fol-\nlowed by its pretraining objectives.\n4.1 Model Architecture\nUniChart consists of two main modules: a chart im-\nage encoder and a text decoder as shown in Fig. 1.\n4The ExcelChart400K dataset only provides bounding\nbox annotations of chart elements and we used this dataset for\ndata value estimation task during pretraining.\n5Since we do not have access to the chart types of Pew\ndataset, we manually tagged random 200 samples from each\nof these datasets to estimate the chart type distribution.\n• Chart Image Encoder In order to effectively\nencode a chart image, an encoder needs to identify\nand interpret three different types of chart compo-\nnents: (1) textual elements (axis labels and leg-\nends), (2) visual elements (e.g., bars, lines), and\n(3) the layout that arranges textual and visual el-\nements within a chart. Since this has a similarity\nwith document image (e.g., receipts) understanding,\nour chart image encoder builds upon the encoder of\none of the recent state-of-the-art document image\nunderstanding models, Donut (Kim et al., 2022).\nDonut offers an OCR-free architecture. The\nmodel is pretrained using an OCR-pseudo task,\nwhere it sequentially generates the encoded text in\na document image, following the order from the\ntop-left corner to the bottom-right corner of the\nimage. As a result, we did not have to run an exter-\nnal OCR module like CRAFT (Baek et al., 2019)\nand Parseq (Bautista and Atienza, 2022), which\nimproved time and memory efficiency throughout\nour training pipeline. Donut employs Swin Trans-\nformer (Liu et al., 2021) architecture as the im-\nage encoder. To encode the chart image features,\nthe images are split into non-overlapping patches,\nwhich are then processed using shifted window-\nbased multi-headed self-attention and MLP layers\nto produce the image embeddings.\n• Text Decoder Similar to Donut (Kim et al.,\n2022), we use the BART (Lewis et al., 2019) de-\ncoder for generating the output. The textual (task-\nspecific) prompts are fed to the decoder and the\ndecoder has to generate the output by conditioning\non the prompted context (see Fig. 1).\n4.2 Pretraining Objectives\nOur pretraining objectives include low-level tasks\nthat are more focused on retrieving the underlying\ndata from the chart images and high-level tasks that\nalign closely with the downstream tasks.\n• Data Table Generation A chart creates a vi-\nsual representation of a data table by mapping each\ndata attribute (e.g., ‘country’, ‘population’) to cor-\nresponding visual attributes (e.g., x-positions,\nheight) of graphical marks (e.g, bars). An ef-\nfective chart comprehension and reasoning model\nshould be able to deconstruct the structured under-\nlying data table by recovering such mappings. To\nthis end, we propose the data table generation task\nin which we ask the model to generate the flattened\ndata table given a chart image.\nA vast amount of charts available online are\n14666\nDataset Data TableGenerationNumerical &Visual ReasoningOpen-endedQuestion AnsweringChartSummarization\nPew 0 0 5,295 5,295Statista, OECD, OWID 144,147 679,420 126,009 126,009PlotQA 155,082 2,414,359 157,070 157,070LineCap 0 0 2,821 2,821Neural Caption 0 0 100 306Beagle 3,972 51 0 0ChartInfo 1,796 21,949 0 0Data Aug. 189,792 2,218,468 189,802 189,802ExcelChart 106,897 0 0 0\nTotal 601,686 5,334,247 481,097 481,303\nTable 1: Number of examples for each task in pretraining.\nstored as bitmap images without access to the un-\nderlying data. It is important to learn how to re-\ncover data values when the chart data is not avail-\nable. Therefore, we also introduce the data value\nestimation task, in which the model is asked to gen-\nerate the scale of the graphical marks (e.g., bars,\nline points) as a percentage of the chart plot area.\nWe obtain these scales by dividing the bars or line\npoints heights (bounding boxes) by the height of\nthe chart plot area and rounding the result to two\ndecimal places. At the final stage, we use charts for\nwhich both data tables and object bounding boxes\nare available as well as charts for which at least\nthe bounding box annotations are available, e.g.,\nExcelCharts from (Luo et al., 2021).\n• Numerical & Visual Reasoning Many down-\nstream applications over charts may involve numer-\nical and visual reasoning with the chart elements\nsuch as chart QA and summarization. For example,\nthe model may need to apply a series of mathe-\nmatical and logical operations such as addition,\nsubtraction and comparisons to answer a question.\nTo inject such reasoning skills into the model, we\ndesign template-based numerical reasoning tasks\nwhere the model is trained to execute/perform the\nmost common mathematical operations over the\nchart data values. We manually analyzed the exist-\ning task datasets (e.g., ChartQA) to find the most\ncommon operations (e.g., sum, average, difference,\netc.) and constructed 90 templates that we utilize to\ngenerate synthetic question and answer pairs. All\nthe templates are provided in Appendix A.8.\n• Open-ended Question Answering It is very\ncommon for users to ask open-ended questions\nover charts (Kantharaj et al., 2022). Such questions\noften ask for answers that require high-level reason-\ning and explanations. To improve the capability of\nthe model in answering open-ended questions, we\nfollow previous work (Shi et al., 2022) to generate\nsynthetic open-ended QA pairs. Specifically, a T5\nmodel (Raffel et al., 2020) pretrained on SQuAD\n(Rajpurkar et al., 2016) is employed to generate an\nopen-ended question for each summary. The sen-\ntence containing the answer in the summary then\nserves as the answer to its generated question.\n• Chart Summarization Image captioning is a\nfundamental problem in AI in which the machines\nneed to summarize the main content of the image\nin the textual form. This task has been studied ex-\ntensively (Vinyals et al., 2015; Herdade et al., 2019;\nHu et al., 2021; Li et al., 2022). We follow previ-\nous work (Vinyals et al., 2015; Xia et al., 2021)\nto pretrain our model on this task to further en-\nhance the model’s capability in generating textual\ndescriptions from the chart image. As discussed\nin §3.3, we used mostly the summaries generated\nfrom GPT models provided by OpenAI either di-\nrectly or through a knowledge distillation step.\n4.3 Downstream Tasks\nIn addition to zero-shot evaluation, we also adapt\nUniChart by finetuning it on a downstream task.\nWe consider four downstream tasks: (1) Fac-\ntoid Chart Question Answering: we use ChartQA\n(Masry et al., 2022), which is a benchmark con-\nsisting of factoid question-answer pairs for charts\nwith a particular focus on visual and logical rea-\nsoning questions; (2) Complex Chart Question An-\nswering: we consider OpenCQA (Kantharaj et al.,\n2022), another QA benchmark in which answers\nare explanatory descriptions; (3) Chart Summariza-\ntion: we use Chart-to-Text (Shankar et al., 2022),\na large-scale benchmark for chart summarization;\n(4) Chart-to-Table: we use ChartQA for both fine-\ntuning and evaluation. Moreover, we evaluate the\npretrained model in a zero-shot setup on the We-\nbCharts dataset (Choi et al., 2019), a collection of\n300 charts obtained from the web.\n4.4 Experiments Setup\nTo minimize the computational resource require-\nments, we initialize our model from the base Donut\nweights (Kim et al., 2022). Our pretraining process\nconsists of two stages. In the first stage, we set\nthe input image resolution to 512x512 and pretrain\nfor 300K steps. In the second stage, we increase\nthe input image resolution to 960x960 and pretrain\nfor an additional 100K steps. Table 6 shows the\nhyperparameters we used in pretraining and fine-\ntuning our model on each downstream task. All\nour experiments were carried out using one 4-A100\n(40GB), one 4-A100 (80GB), and one 4-V100 (32\nGB) GPU machines.\n14667\nChartQA OpenCQA Chart-to-Text Chart-to-Table\n(RA) ( BLEU) ( BLEU) ( RNSS| RMSF1)\nModel #Paramsaug. human avg. OpenCQA Pew Statista ChartQA WebCharts\nVisionTaPas (Masry et al., 2022)- 61.44 29.60 45.52 - - - - -\nT5 (Masry et al., 2022) 222M 56.96 25.12 41.04 9.28 10.49 35.29 - -\nVL-T5 (Masry et al., 2022) - 56.88 26.24 41.56 14.73 - - - -\nPix2Struct (Lee et al., 2022)282M 81.6 30.5 56.0 - 10.3 38.0 - -\nMatCha (Liu et al., 2022b)282M 90.2 38.2 64.2 - 12.2 39.4 85.21 | 83.49 44.37 | 17.94\nUniChart 201M 88.56 43.92 66.24 14.88 12.48 38.21 94.01| 91.10 60.73| 43.21\nTable 2: Evaluation results on four public benchmarks: ChartQA, Chart-to-Text, OpenCQA, and Chart-to-Table. All the results\nare calculated after finetuning UniChart pretrained checkpoint except for WebCharts (zero-shot).\n5 Evaluation\n5.1 Baselines & Evaluation Metrics\nWe compare our model against five baselines: (1)\nT5 (Raffel et al., 2020), a unified seq2seq Trans-\nformer model that achieved state-of-the-art (SoTA)\nresults on various text-to-text tasks, including ques-\ntion answering and summarization; (2) VL-T5 (Cho\net al., 2021), a T5-based model that unifies Vision-\nLanguage (VL) tasks as text generation conditioned\non multimodal inputs and achieved SoTA results on\nOpenCQA (Kantharaj et al., 2022); (3)VisionTapas\n(Masry et al., 2022), an extension of TaPas (Herzig\net al., 2020), a SoTA table encoder, adapted for\nQA over charts; (4) Pix2Struct (Lee et al., 2022), a\npretrained image-to-text model for visual language\nunderstanding and achieved SoTA results on doc-\nument understanding tasks; and (5) MatCha (Liu\net al., 2022b), an adapted version of Pix2Struct for\ncharts that is further pretrained on math reasoning\nand chart data extraction tasks, achieving SoTA\nresults on Chart-to-Text (Shankar et al., 2022) and\nChartQA (Masry et al., 2022).\nTo evaluate our approach, we follow previous\nworks (Lee et al., 2022; Shankar et al., 2022;\nMasry et al., 2022; Kantharaj et al., 2022; Liu\net al., 2022b) and utilize Relaxed Accuracy (RA)\nfor ChartQA and BLEU (Post, 2018) for text-\ngeneration tasks (Chart-to-Text and OpenCQA).\nHowever, the BLEU score has limitations as it\nprimarily focuses on n-gram matching between\nthe generated and reference texts, overlooking im-\nportant factors such as semantic similarity, infor-\nmativeness, and factual correctness (Goyal et al.,\n2022). Therefore, we conduct a human evaluation\nand ChatGPT-driven study to assess and compare\nthese crucial aspects in the outputs of different mod-\nels (§5.3). Finally, we use Relative Number Set\nSimilarity (RNSS) (Masry et al., 2022) and Rela-\ntive Mapping Similarity (RMS) (Liu et al., 2022a)\nSummary Human ChatGPT p-value\nUniChart ZeroShot 3.97 3.18 1.70e-10\nUniChart Finetuned 2.86 2.37 2.32e-8\nMatCha (Liu et al., 2022b)2.50 2.18 0.0020\nGold (Shankar et al., 2022)3.19 2.73 2.13e-6\nTable 3: Average Informativeness scores from Human and\nChatGPT-based evaluation.\nmetrics to evaluate the Chart-to-Table task.\n5.2 Main Results\nAs shown in Table 2, UniChart outperforms previ-\nous state-of-the-art models, MatCha and VL-T5,\non the ChartQA and Chart-to-Text (Pew) datasets,\nalthough it shows slightly lower performance on\nChart-to-Text (Statista). The performance gap is\nmore prominent on the challenging human-written\nquestions in the ChartQA benchmark (Masry et al.,\n2022), where our model’s pretraining objectives\ntailored to visual and numerical reasoning give it\na significant advantage. UniChart also achieved a\nhigher BLUE score compared to the SoTA VL-T5\nmodel on OpenCQA benchmark, which demon-\nstrates our model’s capability in generating ex-\nplanatory answers for questions about charts. Fi-\nnally, UniChart surpasses MatCha’s performance\non two datasets, demonstrating its generalizability\nacross diverse visual styles, even in a zero-shot\nsetup on unseen charts (WebCharts). Overall, these\nresults establish UniChart as the SoTA model for\nchart comprehension and reasoning tasks.\nTo further assess the impact of our different pre-\ntraining objectives on our model’s performance,\nwe conducted ablation studies. We observe that re-\nmoving various pertaining objectives led to a slight\ndecrease in performance (Table 8). The decrease\nin performance is particularly noticeable when the\nNumerical Reasoning pretaining task is removed,\nhighlighting the importance of this task in imbuing\nnumerical abilities into our model. More details of\nthis experiment can be found in Appendix A.4.\n14668\n5.3 Human and ChatGPT Evaluation\nAs discussed in §5.1, reference-based metrics like\nBLEU have relatively low correlations with hu-\nman judgments (Belz and Reiter, 2006; Tan et al.,\n2015; Liu et al., 2023a), and generated texts with\nvery high such scores can be of a very poor quality\n(Smith et al., 2016). Therefore, we decided to con-\nduct a human evaluation to measure the quality of\nsummaries generated by different models. We fo-\ncus on following criteria in the chart summarization\ntask:(1) Informativeness; (2) Factual Correctness;\nand(3) Semantic Levels that characterize the con-\ntent of the summary. More details about the criteria\ncan be found in Appendix A.5.\nWe randomly picked 150 sample charts from\nChart2text Statista test split and asked 3 human\nannotators to rate four summaries for each chart\nbased on informativeness out of 1 to 5. The order\nof exposure of summaries to the annotator was ran-\ndomized to avoid any potential bias. Summaries\nfor each chart were rated by one annotator except\nfor the first 100 charts for which we had two an-\nnotators to measure the agreement. We computed\nKrippendorff’s alpha (Krippendorff, 2011) to mea-\nsure inter-annotator agreement and found a mod-\nerate level of agreement with an alpha coefficient\nof 0.54. We further utilize ChatGPT for evaluating\nthe same 150 samples, as LLMs have demonstrated\ntheir effectiveness as evaluators for text generation\ntasks (Luo et al., 2023; Liu et al., 2023a; Gao et al.,\n2023; Fu et al., 2023). We define the informative-\nness criteria and rating scheme to ChatGPT and\nthen employ ChatGPT to generate evaluation steps.\nWe then send these evaluation steps along with the\ndata table of the chart and the summary to ChatGPT\nto obtain ratings (see Appendix A.5 for details).\nTable 3 shows the result of human evaluation on\nchart summarization based on informativeness cri-\nteria. We notice that annotators preferred ZeroShot\nversion of our model which generates summaries\nthat are more similar to those generated by GPT,\nrather than gold summaries. The finetuned ver-\nsion of UniChart was also rated higher compared\nto SoTA MatCha (Liu et al., 2022b). The finetuned\nUniChart model also produces fewer factual er-\nrors compared to Matcha and the ZeroShot version\n(Appendix A.5 and Table 7). We observe that the\nratings provided by ChatGPT are roughly consis-\ntent with the human annotators’ scores in terms of\ninformativeness criteria. Moreover, we conducted\na statistical test (p-value) for ratings from humans\nand ChatGPT, with the null hypothesis that the rat-\nings are two independent samples. The p-values\nin each row in Table 3 demonstrate that it is very\ninfrequent that two rating samples are independent\nbased on the observed ratings. Also in terms of\ndifferent semantic contents, the ZeroShot model\ntends to contain more sentences with high-level\nvisual patterns and trends. A previous study finds\nthat such high-level insights lead to more reader\ntakeaways compared to the text describing low-\nlevel visual encodings like axes and colors (Stokes\net al., 2022). Overall, the results above suggest that\nUniChart model’s summaries are more informative\nwith high-level insights and factually accurate than\nthe SoTA (MatCha).\n5.4 Time and Memory Efficiency\nUniChart exhibits significant time efficiency com-\npared to MatCha, as shown in Fig. 4. The gap\nin speed is more evident on tasks that require the\ngeneration of long output sequences (e.g., Chart-to-\nText). This difference in speed can be attributed to\nMatCha’s use of a long input sequence (4K) with a\nquadratic increase in complexity while UniChart’s\nvision encoder relies on sliding windows with a\nlocal attention mechanism that scales linearly with\nthe input image size. Moreover, UniChart boasts\na smaller parameter count (201M) compared to\nMatCha (282M), further contributing to its effi-\nciency. As a result, UniChart is highly suitable for\nreal-world applications that prioritize fast inference\nspeeds. More details are provided in Appendix A.7.\n5.5 Error Analysis and Challenges\nWe conducted a manual analysis of our model’s\noutputs to identify key challenges faced by existing\nmodels.\n• Densely populated charts: Our model struggles\nwith extracting insights from chart images that con-\ntain numerous data elements densely packed in a\nlimited area. This is evident in Figure Fig. 9 (Q3)\nwhere our model generates a hallucinated summary\ndue to the complexity of the chart. Increasing\nmodel parameters and input image resolution could\npotentially improve performance in these cases.\n• Numerical reasoning: Despite efforts to incor-\nporate mathematical skills, our model still encoun-\nters difficulties with complex arithmetic calcula-\ntions (Q2 in Fig. 9). Addressing this challenge\ninvolves decoupling arithmetic calculations and rea-\nsoning steps by employing external program execu-\ntors that perform the calculations using the equa-\n14669\ntions generated by our model (Gao et al., 2022).\n• Factual correctness in generated summaries:\nFactual correctness still poses a challenge for au-\ntoregressive language models (Lin et al., 2022; Ope-\nnAI, 2022; Zhao et al., 2023). Although our fine-\ntuned UniChart model produced fewer factual er-\nrors compared to MatCha (see Table 7), it still gen-\nerates some incorrect statements (see Q4 in Fig. 9).\nThis issue can be attributed to factual errors in the\npretraining captions generated by ChatGPT.\n6 Conclusion\nWe present UniChart, a general purpose pretrained\nmodel designed for a broad range of chart-related\ntasks. Our model incorporates chart-specific pre-\ntraining tasks and is trained on a large and diverse\ncollection of charts and corresponding summaries\ncollected opportunistically using LLMs. We con-\nducted both human and ChatGPT evaluations to\nshow the superiority of our method. While our\nmodel sets the state-of-the-art record on four differ-\nent downstream tasks and showed improved time\nand memory efficiency, the evaluation also reveals\nopportunities for improvement. We believe that\nour model and pretraining data will be valuable\nresources for future research and encourage further\nexploration in this relatively new area.\nLimitations\nWhile UniChart exhibits state-of-the-art perfor-\nmance on several benchmarks, it suffers from sev-\neral limitations. Despite the remarkable abilities\non the ChartQA dataset, the model still struggles to\nanswer questions that involve compositional mathe-\nmatical operations. Moreover, we have noticed that\nthe model may hallucinate and produce factually\nincorrect statements on the text generation tasks\nsuch as Chart-to-Text and OpenCQA.\nDespite the generalizability of our model on un-\nseen chart image styles (WebCharts), there’s still\na noticeable drop in performance compared to the\nperformance on the tasks on which the model is\nfinetuned (e.g., ChartQA). Hence, there’s still a\nneed for better generalizable chart models for the\ndiverse charts on the Web. One direction is to en-\nlarge our pretraining datasets by crawling millions\nof chart images from the Web. Since most charts on\nthe Web do not provide high-quality captions or the\nunderlying data table, self-supervised pretraining\nobjectives are needed to benefit from these charts.\nDue to the limited computing resources, we did\nnot investigate the effect hyperparameter tuning\nmight have on the performance on the different\ndownstream tasks. Also, although we have noticed\nthe convergence of UniChart at the end of the sec-\nond stage pretraining, we can not confirm whether\nfurther pretraining may improve the performance\nof our model.\nEthics Statement\nDuring the dataset collection process, we made\nsure to comply with the terms and conditions of\nthe different websites we used to crawl our data.\nStatista6 provide a permissive license to use their\npublicly available data for scientific purposes. Pew\nResearch Centre 7 also provide a permissive license\nto use their data with the condition that we attribute\nit to the Centre. OECD8 allows the users to down-\nload and publish their data as long as they give ap-\npropriate credit to the OECD website. For OWID9,\nall their data are provided under the Creative Com-\nmons BY license which gives the permission for\ndownloading and publication. Web Data Com-\nmons10, which we used in the data augmentation\nprocess, allows the usage of their data under the\nconditions of the Apache License Software which\ngives the right to download and publish. Finally, all\nthe remaining datasets (PlotQA, Beagle, ChartInfo,\nExcelChart400K, LineCap, and Neural Captions)\nare publicly available datasets which were released\nin earlier scientific publications.\nDue to the generative nature of our models, they\nmay be abused for misinforming the public by gen-\nerating factually incorrect responses. Moreover,\nwe can not guarantee that our models may not pro-\nduce texts that may contain hate speech or harmful\ncontent.\nAcknowledgement\nThe authors would like to thank the anonymous re-\nviewers for their helpful comments. This research\nwas supported by the Natural Sciences & Engi-\nneering Research Council (NSERC) of Canada and\nCanada Foundation for Innovation (CFI).\n6https://www.statista.com/getting-started/publishing-\nstatista-content-terms-of-use-and-publication-rights\n7https://www.pewresearch.org/about/terms-and-\nconditions/\n8https://www.oecd.org/termsandconditions/\n9https://ourworldindata.org/faqs#can-i-use-or-\nreproduce-your-data\n10https://webdatacommons.org/\n14670\nReferences\nYoungmin Baek, Bado Lee, Dongyoon Han, Sangdoo\nYun, and Hwalsuk Lee. 2019. Character region\nawareness for text detection. CoRR, abs/1904.01941.\nLeilani Battle, Peitong Duan, Zachery Miranda, Dana\nMukusheva, Remco Chang, and Michael Stonebraker.\n2018. Beagle: Automated extraction and interpreta-\ntion of visualizations from the web. In Proceedings\nof the 2018 CHI Conference on Human Factors in\nComputing Systems, pages 1–8.\nDarwin Bautista and Rowel Atienza. 2022. Scene text\nrecognition with permuted autoregressive sequence\nmodels. In European Conference on Computer Vi-\nsion, pages 178–196, Cham. Springer Nature Switzer-\nland.\nAnja Belz and Ehud Reiter. 2006. Comparing auto-\nmatic and human evaluation of NLG systems. In\n11th Conference of the European Chapter of the As-\nsociation for Computational Linguistics, pages 313–\n320, Trento, Italy. Association for Computational\nLinguistics.\nMichael Bostock, Vadim Ogievetsky, and Jeffrey Heer.\n2011. D3: Data-driven documents. IEEE Transac-\ntions on Visualization & Computer Graphics (Proc.\nInfoVis).\nChartInfo. 2022. Competition on harvesting raw tables\nfrom infographics.\nJian Chen, Meng Ling, Rui Li, Petra Isenberg, Tobias\nIsenberg, Michael Sedlmair, Torsten Moller, Robert S\nLaramee, Han-Wei Shen, Katharina Wunsche, et al.\n2021. Vis30k: A collection of figures and tables from\nieee visualization conference publications. IEEE\nTransactions on Visualization and Computer Graph-\nics.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. Uniter: Universal image-text\nrepresentation learning. In European conference on\ncomputer vision, pages 104–120. Springer.\nJaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021.\nUnifying vision-and-language tasks via text genera-\ntion. In ICML.\nJ. Choi, Sanghun Jung, Deok Gun Park, J. Choo, and\nN. Elmqvist. 2019. Visualizing for the non-visual:\nEnabling the visually impaired to use visualization.\nComputer Graphics Forum, 38.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nElizabeth Clark, Tal August, Sofia Serrano, Nikita\nHaduong, Suchin Gururangan, and Noah A. Smith.\n2021. All that’s ‘human’ is not gold: Evaluating\nhuman evaluation of generated text. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 7282–7296, Online.\nAssociation for Computational Linguistics.\nDazhen Deng, Yihong Wu, Xinhuan Shu, Jiang Wu,\nMengye Xu, Siwei Fu, Weiwei Cui, and Yingcai Wu.\n2020. Visimages: a corpus of visualizations in the\nimages of visualization publications. arXiv preprint\narXiv:2007.04584.\nBOSHENG DING, Chengwei Qin, Linlin Liu, Yew Ken\nChia, Lidong Bing, Boyang Li, and Shafiq Joty. 2023.\nIs gpt-3 a good data annotator? In Proceedings of the\n61st Annual Meeting of the Association for Computa-\ntional Linguistics, ACL’23, Toronto, Canada. ACL.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2021. An image\nis worth 16x16 words: Transformers for image\nrecognition at scale. In International Conference on\nLearning Representations.\nYifan Du, Zikang Liu, Junyi Li, and Wayne Xin Zhao.\n2022. A survey of vision-language pre-trained mod-\nels. In Proceedings of the Thirty-First International\nJoint Conference on Artificial Intelligence, IJCAI-22,\npages 5436–5443. International Joint Conferences on\nArtificial Intelligence Organization. Survey Track.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2022. Pal: Program-aided language\nmodels. arXiv preprint arXiv:2211.10435.\nMingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Ship-\ning Yang, and Xiaojun Wan. 2023. Human-like sum-\nmarization evaluation with chatgpt.\nTanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022.\nNews summarization and evaluation in the era of\ngpt-3.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nSimao Herdade, Armin Kappeler, Kofi Boakye, and\nJoao Soares. 2019. Image captioning: Transforming\nobjects into words. Advances in Neural Information\nProcessing Systems, 32.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Eisenschlos.\n2020. TaPas: Weakly supervised table parsing via\n14671\npre-training. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4320–4333, Online. Association for Computa-\ntional Linguistics.\nEnamul Hoque, Parsa Kavehzadeh, and Ahmed Masry.\n2022. Chart question answering: State of the art\nand future directions. Journal of Computer Graphics\nForum (Proc. EuroVis), pages 555–572.\nXiaowei Hu, Xi Yin, Kevin Lin, Lei Zhang, Jianfeng\nGao, Lijuan Wang, and Zicheng Liu. 2021. Vivo:\nVisual vocabulary pre-training for novel object cap-\ntioning. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 35, pages 1575–1583.\nYupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and\nFuru Wei. 2022. Layoutlmv3: Pre-training for doc-\nument ai with unified text and image masking. In\nProceedings of the 30th ACM International Confer-\nence on Multimedia, MM ’22, page 4083–4091, New\nYork, NY , USA. Association for Computing Machin-\nery.\nZhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei\nLiu, Dongmei Fu, and Jianlong Fu. 2021. Seeing\nout of the box: End-to-end pre-training for vision-\nlanguage representation learning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 12976–12985.\nZhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu,\nand Jianlong Fu. 2020. Pixel-bert: Aligning image\npixels with text by deep multi-modal transformers.\narXiv preprint arXiv:2004.00849.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen\nLi, and Tom Duerig. 2021. Scaling up visual and\nvision-language representation learning with noisy\ntext supervision. In International Conference on\nMachine Learning, pages 4904–4916. PMLR.\nKushal Kafle, Brian Price, Scott Cohen, and Christopher\nKanan. 2018. Dvqa: Understanding data visualiza-\ntions via question answering. Proceedings of the\nIEEE Computer Society Conference on Computer\nVision and Pattern Recognition, pages 5648–5656.\nSamira Ebrahimi Kahou, Vincent Michalski, Adam\nAtkinson, Ákos Kádár, Adam Trischler, and Yoshua\nBengio. 2018. Figureqa: An annotated figure dataset\nfor visual reasoning. 6th International Conference\non Learning Representations, ICLR 2018 - Workshop\nTrack Proceedings, pages 1–20.\nShankar Kantharaj, Xuan Long Do, Rixie Tiffany Ko\nLeong, Jia Qing Tan, Enamul Hoque, and Shafiq Joty.\n2022. Opencqa: Open-ended question answering\nwith charts. In Proceedings of EMNLP (to appear).\nAniruddha Kembhavi, Mike Salvato, Eric Kolve, Min-\njoon Seo, Hannaneh Hajishirzi, and Ali Farhadi.\n2016. A diagram is worth a dozen images. In Euro-\npean conference on computer vision, pages 235–251.\nSpringer.\nGeewook Kim, Teakgyu Hong, Moonbin Yim,\nJeongYeon Nam, Jinyoung Park, Jinyeong Yim, Won-\nseok Hwang, Sangdoo Yun, Dongyoon Han, and\nSeunghyun Park. 2022. Ocr-free document under-\nstanding transformer. In European Conference on\nComputer Vision, pages 498–517. Springer.\nWonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt:\nVision-and-language transformer without convolu-\ntion or region supervision. In Proceedings of the\n38th International Conference on Machine Learning,\nvolume 139 of Proceedings of Machine Learning\nResearch, pages 5583–5594. PMLR.\nKlaus Krippendorff. 2011. Computing krippendorff’s\nalpha-reliability.\nWojciech Kryscinski, Nitish Shirish Keskar, Bryan Mc-\nCann, Caiming Xiong, and Richard Socher. 2019.\nNeural text summarization: A critical evaluation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 540–551, Hong\nKong, China. Association for Computational Linguis-\ntics.\nKenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu,\nFangyu Liu, Julian Eisenschlos, Urvashi Khandel-\nwal, Peter Shaw, Ming-Wei Chang, and Kristina\nToutanova. 2022. Pix2struct: Screenshot parsing as\npretraining for visual language understanding. arXiv\npreprint arXiv:2210.03347.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2019.\nBART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. CoRR, abs/1910.13461.\nGen Li, Nan Duan, Yuejian Fang, Ming Gong, and\nDaxin Jiang. 2020a. Unicoder-vl: A universal en-\ncoder for vision and language by cross-modal pre-\ntraining. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 34, pages 11336–\n11344.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven\nHoi. 2022. Blip: Bootstrapping language-image pre-\ntraining for unified vision-language understanding\nand generation. In ICML.\nJunnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak\nGotmare, Shafiq Joty, Caiming Xiong, and Steven\nHoi. 2021. Align before fuse: Vision and language\nrepresentation learning with momentum distillation.\nIn NeurIPS.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019. Visualbert: A\nsimple and performant baseline for vision and lan-\nguage.\n14672\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2020b. What does BERT\nwith vision look at? In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 5265–5275, Online. Association\nfor Computational Linguistics.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\nTruthfulQA: Measuring how models mimic human\nfalsehoods. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 3214–3252, Dublin,\nIreland. Association for Computational Linguistics.\nFangyu Liu, Julian Martin Eisenschlos, Francesco Pic-\ncinno, Syrine Krichene, Chenxi Pang, Kenton Lee,\nMandar Joshi, Wenhu Chen, Nigel Collier, and\nYasemin Altun. 2022a. Deplot: One-shot visual lan-\nguage reasoning by plot-to-table translation. arXiv\npreprint arXiv:2212.10505.\nFangyu Liu, Francesco Piccinno, Syrine Krichene,\nChenxi Pang, Kenton Lee, Mandar Joshi, Yasemin\nAltun, Nigel Collier, and Julian Martin Eisenschlos.\n2022b. Matcha: Enhancing visual language pretrain-\ning with math reasoning and chart derendering.arXiv\npreprint arXiv:2212.09662.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023a. G-eval:\nNlg evaluation using gpt-4 with better human align-\nment.\nYixin Liu, Alex Fabbri, Pengfei Liu, Yilun Zhao,\nLinyong Nan, Ruilin Han, Simeng Han, Shafiq\nJoty, Chien-Sheng Jason Wu, Caiming Xiong, and\nDragomir Radev. 2023b. Revisiting the gold stan-\ndard: Grounding summarization evaluation with ro-\nbust human evaluation. In Proceedings of the 61st\nAnnual Meeting of the Association for Computational\nLinguistics, ACL’23, Toronto, Canada. ACL.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. 2021.\nSwin transformer: Hierarchical vision transformer\nusing shifted windows. In Proceedings of the\nIEEE/CVF international conference on computer vi-\nsion, pages 10012–10022.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.\n2019. Vilbert: Pretraining task-agnostic visiolinguis-\ntic representations for vision-and-language tasks. Ad-\nvances in neural information processing systems, 32.\nAlan Lundgard and Arvind Satyanarayan. 2021. Ac-\ncessible visualization via natural language descrip-\ntions: A four-level model of semantic content. IEEE\ntransactions on visualization and computer graphics,\n28(1):1073–1083.\nJunyu Luo, Zekun Li, Jinpeng Wang, and Chin-Yew Lin.\n2021. Chartocr: Data extraction from charts images\nvia a deep hybrid framework. 2021 IEEE Winter Con-\nference on Applications of Computer Vision (WACV),\npages 1916–1924.\nZheheng Luo, Qianqian Xie, and Sophia Ananiadou.\n2023. Chatgpt as a factual inconsistency evaluator\nfor text summarization.\nJock Mackinlay, Pat Hanrahan, and Chris Stolte. 2007.\nShow me: Automatic presentation for visual analysis.\nIEEE transactions on visualization and computer\ngraphics, 13(6):1137–1144.\nAnita Mahinpei, Zona Kostic, and Chris Tanner. 2022.\nLinecap: Line charts for data visualization caption-\ning models. In 2022 IEEE Visualization and Visual\nAnalytics (VIS), pages 35–39. IEEE.\nAhmed Masry, Do Long, Jia Qing Tan, Shafiq Joty,\nand Enamul Hoque. 2022. ChartQA: A benchmark\nfor question answering about charts with visual and\nlogical reasoning. In Findings of the Association for\nComputational Linguistics: ACL 2022, pages 2263–\n2279, Dublin, Ireland. Association for Computational\nLinguistics.\nMinesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthe-\nnis Karatzas, Ernest Valveny, and CV Jawahar. 2022.\nInfographicvqa. In Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vi-\nsion, pages 1697–1706.\nNitesh Methani, Pritha Ganguly, Mitesh M. Khapra,\nand Pratyush Kumar. 2020. Plotqa: Reasoning over\nscientific plots. In Proceedings of the IEEE/CVF Win-\nter Conference on Applications of Computer Vision\n(WACV).\nTamara Munzner. 2014. Visualization Analysis and\nDesign. CRC Press.\nJason Obeid and Enamul Hoque. 2020. Chart-to-text:\nGenerating natural language descriptions for charts\nby adapting the transformer model. In Proceedings\nof the 13th International Conference on Natural Lan-\nguage Generation, pages 138–147, Dublin, Ireland.\nAssociation for Computational Linguistics.\nOpenAI. 2022. Chatgpt: Optimizing language models\nfor dialogue.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao\nChen, Michihiro Yasunaga, and Diyi Yang. 2023. Is\nchatgpt a general-purpose natural language process-\ning task solver?\n14673\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models\nfrom natural language supervision. In International\nConference on Machine Learning, pages 8748–8763.\nPMLR.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,\nand Percy Liang. 2016. Squad: 100, 000+ ques-\ntions for machine comprehension of text. CoRR,\nabs/1606.05250.\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian\nSun. 2015. Faster r-cnn: Towards real-time object\ndetection with region proposal networks. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 28. Curran Associates, Inc.\nArvind Satyanarayan, Dominik Moritz, Kanit Wong-\nsuphasawat, and Jeffrey Heer. 2016. Vega-lite: A\ngrammar of interactive graphics. IEEE transactions\non visualization and computer graphics, 23(1):341–\n350.\nKantharaj Shankar, Leong Rixie Tiffany Ko, Lin Xi-\nang, Masry Ahmed, Thakkar Megh, Hoque Enamul,\nand Joty Shafiq. 2022. Chart-to-text: A large-scale\nbenchmark for chart summarization. In In Proceed-\nings of the Annual Meeting of the Association for\nComputational Linguistics (ACL), 2022.\nDanqing Shi, Xinyue Xu, Fuling Sun, Yang Shi, and\nNan Cao. 2020. Calliope: Automatic visual data\nstory generation from a spreadsheet. IEEE Trans-\nactions on Visualization and Computer Graphics ,\n27(2):453–463.\nPeng Shi, Patrick Ng, Feng Nan, Henghui Zhu, Jun\nWang, Jiarong Jiang, Alexander Hanbo Li, Rishav\nChakravarti, Donald Weidner, Bing Xiang, and\nZhiguo Wang. 2022. Generation-focused table-based\nintermediate pre-training for free-form question an-\nswering. Proceedings of the AAAI Conference on\nArtificial Intelligence, 36(10):11312–11320.\nAaron Smith, Christian Hardmeier, and Joerg Tiede-\nmann. 2016. Climbing mont BLEU: The strange\nworld of reachable high-BLEU translations. In Pro-\nceedings of the 19th Annual Conference of the Eu-\nropean Association for Machine Translation, pages\n269–281.\nAndrea Spreafico and Giuseppe Carenini. 2020. Neural\ndata-driven captioning of time-series line charts. In\nProceedings of the International Conference on Ad-\nvanced Visual Interfaces, A VI ’20, New York, NY ,\nUSA. Association for Computing Machinery.\nChase Stokes, Vidya Setlur, Bridget Cogley, Arvind\nSatyanarayan, and Marti A Hearst. 2022. Striking\na balance: Reader takeaways and preferences when\nintegrating text and charts. IEEE Transactions on\nVisualization and Computer Graphics, 29(1):1233–\n1243.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2020. Vl-bert: Pre-training\nof generic visual-linguistic representations. In Inter-\nnational Conference on Learning Representations.\nHao Tan and Mohit Bansal. 2019. Lxmert: Learning\ncross-modality encoder representations from trans-\nformers. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing.\nLiling Tan, Jon Dehdari, and Josef van Genabith. 2015.\nAn awkward disparity between BLEU / RIBES scores\nand human judgements in machine translation. In\nProceedings of the 2nd Workshop on Asian Transla-\ntion (WAT2015), pages 74–81, Kyoto, Japan. Work-\nshop on Asian Translation.\nZineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang,\nYang Liu, Chenguang Zhu, Michael Zeng, Cha\nZhang, and Mohit Bansal. 2022. Unifying vision,\ntext, and layout for universal document processing.\narXiv preprint arXiv:2212.02623.\nOriol Vinyals, Alexander Toshev, Samy Bengio, and\nDumitru Erhan. 2015. Show and tell: A neural image\ncaption generator. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition,\npages 3156–3164.\nJiapeng Wang, Lianwen Jin, and Kai Ding. 2022. Lilt:\nA simple yet effective language-independent layout\ntransformer for structured document understanding.\narXiv preprint arXiv:2202.13669.\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yu-\nlia Tsvetkov, and Yuan Cao. 2021. Simvlm: Simple\nvisual language model pretraining with weak super-\nvision. arXiv preprint arXiv:2108.10904.\nWDC. 2022. Web data commons, extracting structured\ndata from the common crawl.\nQiaolin Xia, Haoyang Huang, Nan Duan, Dongdong\nZhang, Lei Ji, Zhifang Sui, Edward Cui, Taroon\nBharti, and Ming Zhou. 2021. Xgpt: Cross-modal\ngenerative pre-training for image captioning. In\nCCF International Conference on Natural Language\nProcessing and Chinese Computing, pages 786–797.\nSpringer.\nYang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu\nWei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha\nZhang, Wanxiang Che, et al. 2020a. Layoutlmv2:\nMulti-modal pre-training for visually-rich document\nunderstanding. arXiv preprint arXiv:2012.14740.\nYiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu\nWei, and Ming Zhou. 2020b. Layoutlm: Pre-training\nof text and layout for document image understanding.\n14674\nProceedings of the ACM SIGKDD International Con-\nference on Knowledge Discovery and Data Mining,\npages 1192–1200.\nRuochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei\nQin, and Lidong Bing. 2023. Verify-and-edit: A\nknowledge-enhanced chain-of-thought framework.\narXiv preprint arXiv:2305.03268.\nA Appendices\nA.1 Data Augmentation\nDuring the data augmentation process, we mainly\nutilized two of the most popular visualization li-\nbraries: D3 (Bostock et al., 2011) and Vegalite\n(Satyanarayan et al., 2016). Moreover, we have\nintroduced a range of visual variations in terms of\ncolor scheme, elements dimensions, shapes, back-\nground, .etc (see Fig. 2). This makes our generated\nchart images closely resemble the real-world charts\nfound on the Web.\nA.2 Data Augmentation by Knowledge\nDistillation\nWe select the small dataset (3700 charts) from\nPlotQA and the augmented charts from WDC,\nsince these datasets are accompanied by the un-\nderlying data tables which serve as suitable chart\nrepresentation for LLMs. Also, they cover a wide\nrange of topics which contributes to the diversity\nin the generated summaries (§3.1). Fig. 3 shows\nour process for generating summaries for the charts\nthat have underlying data tables using InstructGPT\nmodel (Ouyang et al., 2022). The input mainly\nconsists of one demonstration (table-caption pair)\nfollowed by the desired chart data table. The output\nis the generated summary. Using this mechanism,\nwe generated a small dataset of 3,700 samples. We\nthen finetuned Flan-T5 XL (Chung et al., 2022) on\nthis dataset. To our knowledge, Flan-T5 was the\nSoTA open-sourced instruction-tuned model during\nthe development of our dataset. After finetuning\non our task, we (qualitatively) observed similar per-\nformance as text-davinci-003. At the final\nstep, we used the finetuned Flan-T5 model to gen-\nerate summaries for all the charts that do not have\nan associated summary (e.g., PlotQA, augmented\ncharts, OWID and OECD charts). In this process,\nwe added around 470K summaries for charts in our\npretraining corpus. Fig. 5 shows some examples\ngenerated by the finetuned Flan-T5.\nTo benefit more from the capability of GPT mod-\nels in generating high-quality summaries, we fur-\nther prompt ChatGPT (gpt-3.5-turbo) (Ope-\nnAI, 2022) to generate summaries for the charts\nfrom Statista and Pew Research and put these in\nour pretraining corpus instead of the original sum-\nmaries in the Chart-to-Text benchmark (Shankar\net al., 2022). In most cases, we found the sum-\nmaries from ChatGPT to be more elaborate and of\nbetter writing style. For the Pew Research Centre\n14675\nChart Type Linguistic Statistics\nDatasets Bar Line Pie #Charts #V ocab Avg. Character Avg. Token\nTwo-Col Multi-Col Two-Col Multi-Col\nStatista 71.9% 15.3% 8.3% 2% 2% 19,143 24,392 111.37 21.88\nOWID 51.9% 0.0% 9% 38.9% 0.0% 60,624 3,721 85.89 16.96\nOECD 49.1% 0.0% 3.1% 47.7% 0.0% 64,380 1,606 65.47 14.67\nPlotQA 11.2% 55.6% 6.7% 26.2% 0.0% 157,070 2,230 155.32 33.21\nBeagle 29.8% 27.3% 24.7% 17.9% 0.0% 3,972 11,361 78.76 20.55\nChartInfo 31.7% 51.0% 8.6% 8.6% 0.0% 1,796 13,329 120.75 26.11\nData Augmentation 13.3% 49.3% 11.7% 11.1% 14.3% 189,836117,244 85.62 21.16\nExcelChart400K 11.5% 32.7% 12.0% 22.3% 27.7% 106,897515,922 138.68 27.72\nPewResearch 11.4% 55.5% 4.4% 21.9% 6.5% 5,295 38,165 477.33 98.08\nLineCap 0.0% 0.0% 15.9% 84.0% 0.0% 2,821 16,570 102.11 24.62\nNeural Caption 0.0% 0.0% 100% 0.0% 0.0% 100 389 117.56 27.43\nTotal 21.95% 36.56% 9.23% 23.71% 9.39% 611,934 888,522 114.70 25.01\nTable 4: Chart type distribution and linguistic statistics of the chart pertaining corpus. The charts in the last group\n(magenta) do not come with an underlying data table. The charts generated by the data augmentation process are\nshown in blue.\nDatasets #Vocab Avg. Char Avg. Token Avg. Sentence\nStatista 72,725 450.28 106.68 4.46\nOWID 58,212 463.48 105.99 4.47\nOECD 24,752 414.97 95.08 4.78\nPlotQA 112,394 666.09 149.84 5.20\nData Aug. 162,239 468.41 113.46 4.49\nPewResearch13,449 604.04 133.01 4.66\nLineCap 2018 110.82 26.24 1.87\nNeural Caption1338 262.84 53.28 3.58\nTable 5: Statistics about the captions of the datasets used in\nLM pretraining.\ncharts, the underlying data tables are not provided.\nHowever, we have observed that the underlying\ndata values are written on the visual elements in\nmost of these charts. Hence, we decided to use\nan OCR tool to extract the layout-preserving texts\nfrom the chart images, and then feed it into Chat-\nGPT to generate the summaries as shown in Fig. 6.\nWe realized that ChatGPT is capable of understand-\ning a chart from the OCR data.\nA.3 Dataset Analysis\nThe linguistic characteristics of the textual ele-\nments vary across different datasets, with charts\nfrom PlotQA and PewResearch often having longer\ntext elements (e.g., axis labels, legends, titles),\nwhile augmented data and Beagle datasets contain\nshorter text (Table 4, right). In Table 5, we further\nprovide linguistic statistics for the summaries of\nthe datasets used in the summary generation task\nat pretraining.\nA.4 Ablation study\nTo further assess the impact of our different pre-\ntraining objectives on our model’s performance,\nwe conducted ablation studies. Due to computa-\ntional limitations, we focused on pretraining the\nmodel only on the lower image size (512x512) and\ncompared it against the corresponding main model\n(512x512). From Table 8, we observe that remov-\ning the Chart Summarization or Open-ended Ques-\ntion Answering objectives led to a slight decrease\nin performance on ChartQA. We attributed this to\nthe abundance of numerical reasoning examples\nin pretraining. However, removing the Numerical\nReasoning pretaining task led to a substantial de-\ncrease in performance on ChartQA, highlighting\nthe importance of this task in imbuing numerical\nabilities into our model. Pretraining the model with-\nout the Data Table Generation objective resulted\nin a relatively weak performance in the ChartQA\nbenchmark, underscoring the importance of under-\nstanding underlying data tables of charts in answer-\ning reasoning questions.\nA.5 Human and ChatGPT Evaluation\nAs discussed in section §5.3, we evaluate the fol-\nlowing three criteria in the human evaluation study:\n(1) Informativeness which measures how much in-\nformation from the chart the summary covers. Ide-\nally, an informative summary should contain high-\nlevel insights from the chart, such as important\npatterns, trends, and outliers in data; (2) Factual\n14676\nExperiment # Epochs/Steps Learning Rate Batch Size GPUs Saving Mechanism\nPretraining\nFirst-stage/ablations 300K steps 1e-4 160 4xA100 82GB each 50K steps\nSecond-stage 100K steps 1e-4 80 4xA100 82GB each 50K steps\nFinetuning (main 960x960 model)\nChartQA 20 epochs 5e-5 24 4xV100 32GB each 1 epoch\nChart-to-text Pew 200 epochs 5e-5 48 4xA100 40GB each 5 epochs\nChart-to-text Statista 100 epochs 5e-5 48 4xA100 40GB each 5 epochs\nOpenCQA 200 epochs 5e-5 24 4xV100 32GB each 5 epochs\nTable 6: Training details for pretraining and finetuning experiments.\nCriteria ZeroShot Finetuned MatCha Gold\nFactually incorrect sents13.45% 9.63% 21.97%3.59%\nElemental/encoded sents19.42% 26.06%29.61%21.07%\nStatistical/relational sents57.41%33.42% 34.07% 34.70%\nPerceptual/cognitive sents6.98% 1.41% 0.31% 5.39%\nContextual/domain-specific sents1.36% 14.44% 7.32%20.56%\nTable 7: Human evaluation on summaries for 150 ran-\ndom samples from Chat2text Statista test split.\nCorrectness which considers how accurate the sum-\nmary is. A factually correct summary should only\ncontain information (e.g. numbers, events, enti-\nties) that is true and/or supported by the chart; (3)\nSemantic Levels defined by (Lundgard and Satya-\nnarayan, 2021) which categorize the content of\nsummaries across four levels: visual encoding (e.g.,\naxis, legends, color), statistical/relational (e.g., min,\nmax, avg.), perceptual/cognitive (e.g., describing\noverall trends, complex patterns, or outliers), and\ncontext/domain specific information. Our process\nfor evaluating the informativeness is explained in\n§5.3. For factual correctness and semantic level\nmeasures, the annotator goes through each sen-\ntence of the summary to determine whether the\nsentence contains any factual error and what levels\nof semantic content are present for that sentence.\nTable 7 shows the results of our human evaluation\nstudy on factual correctness, and different semantic\nlevels.\nFig. 7 shows an overview of the paradigm we use\nin our ChatGPT-driven evaluation study. Fig. 8 de-\npicts the interface we used in our human evaluation\nstudy.\nA.6 Error Analysis\nFig. 9 shows the models performance on challeng-\ning samples. Q1 and Q2 examples are two visual\nnumerical reasoning questions about charts which\nChartQA\nModel aug. human avg.\nUniChart (512x512) 85.84 43.60 64.72\nNo Chart Summarization 84.96 42.72 63.84\nNo Open-ended Question Answering85.52 42.96 64.24\nNo Numerical & Visual Reasoning84.08 35.44 59.76\nNo Data Table Generation 83.84 42.24 63.04\nTable 8: UniChart ablations on ChartQA benchmark.\nlook challenging for SoTA models. Q3 is an exam-\nple of an overpopulated chart with so many data\nelements which confuses the model to generate in-\nsightful summary. Finally, Q4 shows a factual error\nin a generated summary from finetuned UniChart.\nA.7 Time and Memory Efficiency\nTo compare the time efficiency, we measure the av-\nerage inference time of the models on three bench-\nmarks: ChartQA, Chart-to-Text (Pew), and Chart-\nto-Text (Statista) using 10 random samples from\neach benchmark. The experiments were conducted\non Google’s Colab platform with cpu type. Over-\nall, UniChart shows much faster inference times\ncompared to MatCha as shown in Fig. 4.\nA.8 Templates for Numerical and Visual\nReasoning Question Generation\nTable 9 is the list of the templates we used to gen-\nerate numerical and visual reasoning questions.\n14677\n(a)\n (b)\n(c)\n (d)\n(e)\n (f)\n(g)\n (h)\nFigure 2: Visually diverse charts generated by D3 and Vegalite for WDC corpus (Fig. 2a, Fig. 2b, Fig. 2c, Fig. 2d,\nFig. 2g and Fig. 2h from D3-WDC, Fig. 2e and Fig. 2f from Vegalite-WDC. Visual factors like color scheme, width\nof bars, and existence of grids and axis labels are different among the samples.\n14678\nFigure 3: An example of the performance of InstructGPT in generation summaries for data tables. On the left side,\nthe red text is a full example of a demonstration and its summary followed by the demonstration for the target chart.\nThe paragraph in green shows the summary generated by the model.\nFigure 4: Average inference time for 10 random samples from three major benchmarks in chart understanding\ndomain for UniChart and MatCha models\n14679\nThe line chart shows the amount of agricultural\nraw materials, food, and fuel exports of Norway\nfrom 1990 to 1995. Agricultural raw materials\nexports decreased from 2.18% in 1990 to 1.32%\nin 1993. Food exports increased from 6.94% in\n1990 to 8.85% in 1994. Fuel exports increased\nfrom 47.78% in 1990 to 51.26% in 1993.\nOverall, the trend shows a decrease in the\namount of agricultural raw materials, food, and\nfuel exports of Norway from 1990 to 1995.\nThe line chart displays the number of domestic\ntakeoffs and takeoffs abroad of air carriers\nregistered in Israel over a six-year period from 1998\nto 2003. The number of flight takeoffs started at\n50,100 in 1998, increased to 51,010 in 1999, then\ndecreased to 56,427 in 2000, decreased further to\n47,648 in 2001, increased to 49,404 in 2002, and\ndecreased again to 46,334 in 2003. Therefore, the\nchart depicts a fluctuating trend of the number of\nflight takeoffs of air carriers registered in Israel\nduring the six-year period, with an overall\ndecreasing trend from 1998 to 2003.\nThe line chart shows the statistical performance of\nindividual players from Delaware Valley and\nWidener in a football game on November 10, 2012.\nThe categories measured are number of carries\n(NO), yards gained (YDS), average yards per\ncarry (AVG), longest gain (LG), and touchdowns\nscored (TD). Among the players, Lewis Vincent\nhad the most carries (5.09) and yards gained\n(64.09), with an impressive average of 13.09 yards\nper carry. Chris Ruiz also had a strong\nperformance, with an average of 8.42 yards per\ncarry and 0.42 touchdowns scored. Meanwhile,\nMike Anusky had the fewest carries (1.42) and\nyards gained (6.09), but still managed to score a\ntouchdown. Overall, the chart shows that the\nplayers from both teams had varying levels of\nperformance in the game.\nThe bar chart compares the performance of two\nhigh school girls soccer teams, Fleetwood and\nTrinity, in a game. Both teams had a goal\ndifference of 1.0, with Fleetwood scoring 1.0 goal\nand Trinity scoring 0.0. Both teams had 0.0 in\nshots on goal, saves, points, and corner kicks.\nHowever, Fleetwood had 4.0 shots on goal, 7.0\nsaves, 2.0 points, and 6.0 corner kicks, while\nTrinity had 8.0 shots on goal, 3.0 saves, 2.0 points,\nand 3.0 corner kicks. Overall, the chart shows\nthat both teams had similar performances in the\ngame, with Fleetwood scoring 1.0 goal and\nTrinity scoring 0.0.\nThe bar chart shows the results of the\nVital Leeds poll in chronological order for\nseven teams: Huddersfield, Hull City,\nIpswich, Leeds United, Middlesbrough,\nMK Dons, and Preston. All teams had a\nscore of 0.0 in the poll, indicating that they\nhad not been voted on yet. This suggests\nthat the team was not yet considered to be\na clear favorite in the poll, and that the\nresults of the poll may be skewed by the\noutcome of the next few games.\nThe pie chart shows the results of the\nHungarian Grand Prix motor race in 2020,\ncategorized by the winner of the race. The\nwinner of the race was Sebastian Vettel, who\naccounted for 17% of the total results, followed\nby Daniil Kvyat at 33%, and Daniel Ricciardo\nat 50%. This chart indicates that the race was\nclosely contested, with all three drivers finishing\nwithin a close margin of each other.\nThe line chart displays the number of non-fatal\ninjuries from terrorist attacks over a nine-year\nperiod from 1970 to 1982. The x-axis shows the\nnumber of non-fatal injuries, while the y-axis\nshows the number of fatalities. The number of\nnon-fatal injuries increased from 4.23 in 1970 to\n271.09 in 1980, before decreasing to 42.18 in\n1981. Overall, the chart shows an increasing\ntrend in the number of non-fatal injuries from\nterrorist attacks during the nine-year period,\nwith a slight decrease in 1978.\nThe bar chart shows the number of one-year-olds\nwho did not receive the vaccine against diphtheria,\npertussis, and tetanus (DTP3) in 2000 for five\ncountries: Lebanon, Eritrea, Iran, Comoros, and Sri\nLanka. Lebanon had the highest number of one-\nyear-olds who did not receive the vaccine with\n13,958 while Eritrea had 12,089 and Iran had\n11,401. Comoros had 5,467 and Sri Lanka had 3,291.\nThe data suggests that there is a significant\nvariation in the number of one-year-olds who did\nnot receive the vaccine against diphtheria, pertussis,\nand tetanus across these countries.\nThe bar chart compares the number of patents on\nenvironment technologies between Canada and\nMexico. Canada has 6.8 patents on environment\ntechnologies, while Mexico has 26.3. This indicates\nthat Mexico has more than double the number of\npatents on environment technologies compared to\nCanada. This suggests that Mexico is more active in\nthe field of environment technologies.\nFigure 5: Examples of summaries generated by Flan-T5 XL model after fine-tuning.\n14680\nThe chart shows the percentage of U.S. teens, ages\n13 to 17, who feel overwhelmed by drama, excluded\nby friends, pressured to post content for likes, or\nworse about their life due to social media. Girls are\nmore likely than boys to feel overwhelmed by drama,\nexcluded by friends, and worse about their life. Around\n45% of girls report feeling overwhelmed by drama,\nwhile only 32% of boys feel the same. Similarly, 37%\nof girls report feeling excluded by their friends\ncompared to 24% of boys. Regarding pressure to post\ncontent for likes, 32% of boys and 27% of girls feel\nthis pressure. Finally, 28% of girls and 18% of boys\nreport feeling worse about their own life due to social\nmedia. Overall, the chart suggests that social media\nhas a greater negative impact on the mental health of\nteen girls than teen boys.\nAnalyze the following chart in one paragraph.\nTeen girls more likely than teen boysto say social\nmedia has made them feel overwhelmed by drama,\nexcluded by friends or worse about their life\n%ofU.S. teens whosay thatin general, what theyseeonsocial media makes\nthemfeela lot or little...\n                                                                             U.S.\n                                                                             teens\n                                             Boys     Girls\n     Overwhelmed because ofall               320      045                     38\n                    the drama\n     Liketheir friends are leaving        240 -037                            31\n               them outofthings\nPressure to post content thatwill\n                                           270032                             29\n    getlots of comments or likes\n       Worse about their own life - 18       28                               23\n                                     0                  50             100\nNote: Teensare thoseages 13to 17.Thosewho did notgivean\nanswerorwhogaveother\nresponses are not shown.\nSource: Survey conducted April 14-May4,2022.\n\"Connection, Creativityand Drama: Teen Life on Social Media in 2022\"\n   PEW RESEARCH CENTER\nChatGPT\nGenerated SummaryOCR text extracted from chart\nFigure 6: An example of the layout-preserved OCR-extracted text for a PewResearch chart image where the\nunderlying data table is not available. The extracted text is then given to ChatGPT to generate a summary. ChatGPT\ncan still extract and comprehend important information and insights from the layout-preserving text of the chart\nimage.\n1. Read the summary carefully and ensure understanding of the\ninformation presented in the chart data table.\n2. Evaluate the informativeness of the summary by assessing how\nmuch information from the data table is covered.\n3. Rate the informativeness of the summary on a scale of 1 to 5,\nwith 1 being not informative at all and 5 being highly informative\nand containing all key insights.\n4. Consider whether the summary effectively conveys the key\ninformation from the chart data table and is factually accurate.\n5. Provide specific feedback on areas for improvement, if necessary.\nChatGPT \nYou will be given one summary written for a chart. Your task is to rate\nthe summary on one metric and provide appropriate feedback.\nEvaluation Criteria: Informativeness (1-5) - It measures how\ninformative the generated summary is. In other words,\ninformativeness measures how much information from the data table\nof the chart the summary covers. Ideally, an informative summary\nshould contain high-level insights from the chart, such as important\npatterns, trends and outliers in data.\nEvaluation steps:\nThis statistic shows the economic contribution of the travel\nand tourism industry to the United Kingdom ( UK ) in 2017, by\nsector. The direct contribution to GDP is approximately 93.5\nbillion U.S. dollars.\nChatGPT \nInitial Instruction Detailed Steps Generated By ChatGPT\nEvaluation:\nInformativeness: 2/5\nFeedback: The summary only covers one data point\nfrom the table, which is the direct contribution to GDP. It\ndoes not provide any information on the total\ncontribution to GDP, visitor exports, domestic spending,\nleisure spending, business spending, or capital\ninvestment. The summary is not informative enough as\nit does not contain any key insights from the chart data\ntable. The summary needs to include more information\nfrom the table in order to be more informative.\nFeedback Generated By ChatGPT\nChart Sample\nSummary Sample\nFigure 7: The pipeline designed for the ChatGPT Evaluation Experiment. First, we feed the task description followed by our\ndesired criteria into ChatGPT in order to get detailed grading instructions. Then, the chart (underlying data table representation)\nand a sample summary are appended to the prompt which is fed again into ChatGPT to receive the feedback.\n14681\n(a)\n(b)\nFigure 8: Human evaluation interface.\n14682\nQ1: What is the largest value of red bar?\nAnswer: 25       UniChart : 25         MatCha: 40\nQ3: Summarize the chart.\nUniChart ZeroShot:  The line chart shows the year-on-year change in air\noperations due to the outbreak of COVID-19 in Poland from March to\nNovember 2020, broken down by major occupations. The occupations are\nmeasured in air operations (excluding Fight operations), in wholes\n(excluding Fight operations), and in wholes (excluding Fight\noperations).The occupations are measured in air operations (excluding\nFight operations), in wholes (excluding Fight operations), and in wholes\n(excluding Fight operations). The occupations are measured in air\noperations (excluding Fight operations), in wholes (excluding Fight\noperations), and in wholes (excluding Fight operations). The occupations\nare measured in air operations (excluding Fight operations), in wholes\n(excluding Fight operations), ................................................................. \nrevehivehivehivehivehivehivehivehivehivehivehivehivehivehivehivehiveh.\nQ2: What's the average of all the values in the green\nbars (round to one decimal)?\nAnswer: 21.6        UniChart : 0.4         MatCha:0.51\nQ4: Summarize the chart.\nUniChart Finetuned: This statistic illustrates the\nannual consumption of fluid cow milk worldwide in\n2019, by country. In that year, India's fluid cow milk\namounted to 77.7 million metric tons, making it the\nworld's second largest consumer of milk.\nFigure 9: Some challenging examples from the ChartQA and Chart-to-text benchmarks.\n14683\nTemplate-based Questions\n1) First bar from the top/left in the second group from the top/left2) First bar from the bottom/right in the second group from the bottom/right3) Second bar from the bottom/right in the first group from the bottom/right4) Second bar from the right/bottom in the first group from the left/top5) Topmost/Leftmost bar6) Bottommost/Rightmost bar7) Second bar from the top/left8) Second bar from the right/bottom9) Leftmost topmost bar10) Leftmost bottommost bar11) Rightmost topmost bar12) Rightmost bottommost bar13) Leftmost<color>data14) Rightmost<color>data15) Second from the left<color>data16) Second from the right<color>data17) Which legend represented by<color>?18) What is the color of<legend>?19) Which one is greater,<x1>or<x2>?20) Divide the sum of largest and lowest values by<n>21) When did line<legend - label>peak?22) What is the difference between maximum and minimum of<legend - label>?23) Sum pie segments above<value>24) What is the sum of top three values?25) What is the median/mode of<legend - label>?26) What is the negative peak of<legend - label>?27) What is the largest/smallest value of<legend - label>?28) Which two x-axis labels of<legend - label>sums up to<value>?29) What is the sum of the second highest and second lowest value of<legend - label>?30) Which x-axis label is second highest for<legend - label>?31) What is the sum of two middle values of<legend - label>?32) Which two x-axis labels of<legend - label>have a difference of<value>?33) What is the average of<legend - label>from<x - label - 1>to<x - label - 2>?34) What is the average of the highest and lowest value of<legend - label - l>?35) What is the sum of the average of<legend - label - 1>and average of<legend - label - 2>?36) What is the sum/difference of the maximum of<legend - label - 1>and minimum of<legend - label - 2>?37) Which x-axis label has the maximum/minimum difference between<legend - label - 1>and minimum of<legend - label - 2>?38) Which x-axis label witnessed the smallest value of<legend - label>?39) Which label contains largest/smallest values across all labels?40) Sum up the medians of all the data series in this chart41) What is the average of all values above<value>?42) What is the sum of the largest and smallest difference between<legend - label - 1>and<legend - label - 2>?43) What is the maximum/minimum difference between<legend - label - 1>and<legend - label - 2>?44) What is the ratio of the largest to the smallest pie segment?45) What is the ratio of the two largest/smallest segments?46) What is the difference between the leftmost and rightmost bars?47) What is the sum of the bars in the second group from the left?48) What is the sum of the bars in the first group from the right?49) What is the ratio between the two leftmost bars?50) What is the difference between the rightmost<color - 1>bar and leftmost<color - 2>bar?51) What is the average of<color>bars values?52) How many<color>bars are larger than<N>?53) What is the average of the bars in the second group from the right?54) How many bars in the leftmost group have a value over<N>?55) What does the<color>represent?56) What is the median value of the<color>bars/line?57) What is the average of the<color - 1>sum and<color - 2>sum?58) What is the average of the<color - 1>median and<color - 2>median?59) What is the least difference between the<color - 1>and<color - 2>bars/line?60) What is the ratio between the leftmost and rightmost bar in the first group from the left?61) What is the maximum value in the<color>bars/line?62) What is the minimum value in the<color>bars/line?63) What is the sum of<color>bars/line?64) What is the difference between the maximum values of the two leftmost bar groups?65) Sum of the first<color - 1>and last<color - 2>bars/line points66) Difference between the two lowest<color>bars67) Add largest and smallest<color>line/bar values and divide by 268) What is the value of<color>line/bars in<x - axis - label>?69) Sum/Average of<color - 1>and<color - 2>values in<x - axis - label>?70) Sum of highest points in<color - 1>and<color - 2>lines/bars71) Which color has the highest/smallest values?72) How many values are equal in<color - 1>line/bar?73) Sum two rightmost values of<color>graph74) Product of two smallest values in the graph75) Sum of lowest and median values of<color>graph/bars76) When did<color>line reached the peak?77) What is the average of the rightmost three points of<color>line?78) How many<color>data points are above<value>?79) What’s the ratio of the largest and the third/second-largest<color>bar?80) Is the sum of lowest value of<color - 1>and<color - 2>bar greater than largest value of<color - 3>bar?81) Is the median value of<color - 1>bars greater than the median value of<color - 2>bars?82) Is the median of all the<color - 1>bars greater than the largest value of<color - 2>bar?83) What’s the product of<color>bars in India and Japan?84) Is the sum of the two middle bars greater than the sum of top and bottom bars?85) What’s the ratio of the<x - axis - label - 1> <color - 1>bar and the<x - axis - 2> <color - 2>bar?86) Is the total of all<color - 1>bars greater than the total of all<color - 2>bars?87) Take the sum of the two smallest<color - 1>bars and smallest<color - 2>bars, deduct the smaller value from the larger value, what’s the result?88) What is the sum/average of two smallest/largest<color>bars?89) What is the ratio of<color - 1>and<color - 2>segments?90) What segment is represented by<color>?\nTable 9: Numerical & Visual reasoning templates.\n14684",
  "topic": "Chart",
  "concepts": [
    {
      "name": "Chart",
      "score": 0.7996324896812439
    },
    {
      "name": "Computer science",
      "score": 0.7995728850364685
    },
    {
      "name": "Automatic summarization",
      "score": 0.7024620771408081
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6359469890594482
    },
    {
      "name": "Natural language processing",
      "score": 0.6189059615135193
    },
    {
      "name": "Generalizability theory",
      "score": 0.5139164924621582
    },
    {
      "name": "Question answering",
      "score": 0.4130038022994995
    },
    {
      "name": "Machine learning",
      "score": 0.3556992709636688
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ]
}