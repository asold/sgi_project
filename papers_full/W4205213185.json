{
    "title": "RTNet: Relation Transformer Network for Diabetic Retinopathy Multi-Lesion Segmentation",
    "url": "https://openalex.org/W4205213185",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2159848521",
            "name": "Shiqi Huang",
            "affiliations": [
                "Beijing Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2112578002",
            "name": "Jianan Li",
            "affiliations": [
                "Beijing Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2596995140",
            "name": "Yuze Xiao",
            "affiliations": [
                "Beijing Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2026926520",
            "name": "Ning Shen",
            "affiliations": [
                "Beijing Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2313302185",
            "name": "Tingfa Xu",
            "affiliations": [
                "Beijing Institute of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2985982097",
        "https://openalex.org/W2016795705",
        "https://openalex.org/W2298121077",
        "https://openalex.org/W2803487511",
        "https://openalex.org/W2234307896",
        "https://openalex.org/W2939250925",
        "https://openalex.org/W6637373629",
        "https://openalex.org/W2978017498",
        "https://openalex.org/W3009238059",
        "https://openalex.org/W2598680349",
        "https://openalex.org/W2501861162",
        "https://openalex.org/W2906258544",
        "https://openalex.org/W2792618444",
        "https://openalex.org/W2978935251",
        "https://openalex.org/W2060749088",
        "https://openalex.org/W1982761740",
        "https://openalex.org/W2081630708",
        "https://openalex.org/W6684191040",
        "https://openalex.org/W3085574514",
        "https://openalex.org/W2983395335",
        "https://openalex.org/W2921406441",
        "https://openalex.org/W2980702601",
        "https://openalex.org/W2626629100",
        "https://openalex.org/W3087738047",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3001467249",
        "https://openalex.org/W3161200432",
        "https://openalex.org/W3158315073",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W2828862258",
        "https://openalex.org/W2977650145",
        "https://openalex.org/W2948685905",
        "https://openalex.org/W2952722001",
        "https://openalex.org/W845365781",
        "https://openalex.org/W2787091153",
        "https://openalex.org/W2953129827",
        "https://openalex.org/W2150769593",
        "https://openalex.org/W2145305441",
        "https://openalex.org/W2963446712",
        "https://openalex.org/W6675354045",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2531409750",
        "https://openalex.org/W3142788400",
        "https://openalex.org/W3174708193",
        "https://openalex.org/W2752782242",
        "https://openalex.org/W2884585870",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2101234009",
        "https://openalex.org/W1686810756"
    ],
    "abstract": "Automatic diabetic retinopathy (DR) lesions segmentation makes great sense of assisting ophthalmologists in diagnosis. Although many researches have been conducted on this task, most prior works paid too much attention to the designs of networks instead of considering the pathological association for lesions. Through investigating the pathogenic causes of DR lesions in advance, we found that certain lesions are closed to specific vessels and present relative patterns to each other. Motivated by the observation, we propose a relation transformer block (RTB) to incorporate attention mechanisms at two main levels: a self-attention transformer exploits global dependencies among lesion features, while a cross-attention transformer allows interactions between lesion and vessel features by integrating valuable vascular information to alleviate ambiguity in lesion detection caused by complex fundus structures. In addition, to capture the small lesion patterns first, we propose a global transformer block (GTB) which preserves detailed information in deep network. By integrating the above blocks of dual-branches, our network segments the four kinds of lesions simultaneously. Comprehensive experiments on IDRiD and DDR datasets well demonstrate the superiority of our approach, which achieves competitive performance compared to state-of-the-arts.",
    "full_text": "IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2022 1\nRTNet: Relation Transformer Network for\nDiabetic Retinopathy Multi-lesion Segmentation\nShiqi Huang, Jianan Li, Yuze Xiao, Ning Shen and Tingfa Xu\nAbstract— Automatic diabetic retinopathy (DR) lesions\nsegmentation makes great sense of assisting ophthalmol-\nogists in diagnosis. Although many researches have been\nconducted on this task, most prior works paid too much\nattention to the designs of networks instead of considering\nthe pathological association for lesions. Through investi-\ngating the pathogenic causes of DR lesions in advance,\nwe found that certain lesions are closed to speciﬁc vessels\nand present relative patterns to each other. Motivated by the\nobservation, we propose a relation transformer block (RTB)\nto incorporate attention mechanisms at two main levels:\na self-attention transformer exploits global dependencies\namong lesion features, while a cross-attention transformer\nallows interactions between lesion and vessel features by\nintegrating valuable vascular information to alleviate ambi-\nguity in lesion detection caused by complex fundus struc-\ntures. In addition, to capture the small lesion patterns ﬁrst,\nwe propose a global transformer block (GTB) which pre-\nserves detailed information in deep network. By integrating\nthe above blocks of dual-branches, our network segments\nthe four kinds of lesions simultaneously. Comprehensive\nexperiments on IDRiD and DDR datasets well demonstrate\nthe superiority of our approach, which achieves competi-\ntive performance compared to state-of-the-arts.\nIndex Terms— Diabetic retinopathy, Fundus image, Se-\nmantic segmentation, Transformer, Deep learning\nI. I NTRODUCTION\nD\nIABETIC retinopathy (DR) has become a worldwide\nmajor medical concern for the large population of di-\nabetic patients and has been the leading cause of blindness in\nthe working-age population today [1]–[3]. DR lesions often\npresent as microaneurysms (MAs), hemorrhages (HEs), soft\nexudates (SEs), and hard exudates (EXs) which can be ob-\nserved in colorful fundus images and are the basis of diagnosis\nfor ophthalmologists.\nHowever, until now there has been no valid treatment to cure\nthis disease completely. The most recognized treatment is the\nearly diagnosis and intervention to controll the progression of\nthe disease and to avoid eventual loss of vision [4]. Thus, many\nnational health institutions are promoting DR screening, which\nhas been proven effective in reducing the rate of blindness\ncaused by DR [2], [5]. However, screening is a heavy burden\nThis work was supported by the Key Laboratory Foundation under\nGrant TCGZ2020C004 and Grant 202020429036.\nShiqi Huang, Jianan Li, Yuze Xiao, Ning Shen and Tingfa Xu are\nwith Beijing Institute of Technology, China. Tingfa Xu is also with\nChongqing Innovation Center, Beijing Institute of Technology, China (e-\nmail: huangsq, ciom xtf1, lijianan@bit.edu.cn).\nCorresponding authors: Tingfa Xu and Jianan Li.\nFig. 1. Illustration of fundus image with characteristics of DR lesion. SE:\nsoft exudate; HE: hemorrhage; EX: hard exudate; MA: microaneurysm.\n(a) Original image with different clusters of lesions denoted by red\nand blue bounding boxes; (b) magniﬁed lesion regions where green,\npurple, red and blue area represent SE, HE, EX and MA, respectively;\n(c) location statistics of certain lesions on IDRiD dataset in pixels.\nSpeciﬁcally, from left to right are the distances from SE center to nearest\nHE center, from EX cluster center to nearest MA center, from SE center\nto the nearest vascular tree midline, and from MA center to the nearest\nvascular tree midline.\nfor primary care systems during the promotion, since the\nophthalmologists are in very short supply and have already\nengaged in post-DR treatment. For this reason, automatic\nsegmentation technology for DR lesions has become a trend\ntowards assisting ophthalmologists in diagnosis.\nRecent research efforts have been directed towards auto-\nmatic DR segmentation based on deep learning (DL). Guo\net al. [6] adopted a pretrained Vgg16 [7] as backbone with\nmulti-scale feature fusion block for DR lesion segmentation.\nSpeciﬁcally, they extracted side features from each convo-\nlution layer in Vgg16 and fused them in a weighted way.\nFurther, a multi-channel bin loss was proposed to alleviate\nclass-imbalance and loss-imbalance problems. Zhou et al. [8]\ndesigned a collaborative learning network to jointly improve\nthe performance of DR grading and DR lesion segmentation\nwith attention mechanism. The attention mechanism allowed\nfeatures with image-level annotations to be reﬁned by class-\nspeciﬁc information, and generated pixel-level pseudo-masks\nfor the training of segmentation model. However, previously\npublished studies paid too much attention to the designs of\nnetworks and many of the researches up to now have only\nachieved the segmentation of just one or two lesions [9]–[13].\narXiv:2201.11037v1  [eess.IV]  26 Jan 2022\n2 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2022\nIt should be noted that as a complex medical lesion segmen-\ntation task, the pathological connections have not received\nenough attention.\nAfter comprehensive investigation in possible causes of DR\nlesions, we found two interesting presentation phenomena\nshown in Fig.1: 1) lesions are usually closed to speciﬁc veins\nand arteries. For example, SEs are generally distributed at the\nmargins of the main trunk of the upper and lower arteries, and\nMAs are generally distributed at the margins of the capillaries;\n2) most lesions have certain spatial interactions with each\nother. Speciﬁcally, SEs commonly appear at the edge of HEs,\nwhile EXs are usually arranged in a circular pattern around\none or several MAs, which is consistent with the occurrence\nof pathology.\nMotivated by the above observations, we propose a relation\ntransformer block (RTB), comprised of a cross-attention and\na self-attention head, to explore dependencies among lesions\nand other fundus tissues. For speciﬁc, the cross-attention head\nis designed to capture implicit relations between lesions and\nvessels. We design a dual-branch network to employ both\nlesion and vascular information, and cross-attention head is\nintegrated between the two branches to make effective use\nof vascular information in lesion segmentation branch. As we\nknow, the fundus tissues, e.g., vessels, optic disc, nerves and\nother lesions, are complex and easily confused with DR lesions\nof interest, but considering that vascular information describes\ncertain distribution patterns of the above tissues, the cross-\nattention head is able to locate more lesions through the layout\nprovided and eliminate the false positives far from certain\nvessels. The self-attention head is employed to investigate the\nrelationships of multi-lesion themselves. Since some of the\nlesions look similar, e.g., HE and MA (both red-lesion), SE and\nEX (both exduate-lesion), misclassiﬁcations between lesions\noccur frequently. Through impactful information emphasized\nby self-attention head, the distinction and connection of lesions\nplay roles in reducing confusion in segmentation. Note that\nbefore DL was utilized in medical imaging, vessels were easily\nmistaken for red lesions, and vessel detection was regarded\nas a routine step in the segmentation with detected vessels\nbeing removed straight away. However, as no fundus dataset\nis annotated with both vessels and DR lesions, the DL-based\nlesion segmentation task no longer extracts vessel features\nseparately. To our best knowledge, this is the ﬁrst trial that\nutilizes vascular information for deep based fundus lesion\nsegmentation.\nIn addition, some special lesion patterns, such as MA\nwith small size and SE with blurred border, are hard to be\nsituated accurately due to the lack of ﬁne-grained details in\nhigh-level features. To alleviate this, we propose a Global\nTransformer Block (GTB) inspired by GCNet [14] to further\nextract detail information, which can preserve the detailed\nlesion information and suppress the less useful information of\nchannels in each position. In our network, GTB is also adopted\nto generate dual branches. Speciﬁcally, after the backbone, the\nshared fundus features are obtain as input of the two GTBs,\nand then GTBs generate more speciﬁc features of vessels and\nlesions respectively, which would be further investigated at the\npathological connection level by RTB.\nWe have evaluated our network on two publicly available\ndatasets - IDRiD and DDR. Experimental results show that our\nnetwork outperforms the state-of-the-art DR lesion segmenta-\ntion reports and achieves the best performance in EX, MA and\nSE. Furthermore, we also implement ablative experiments on\nIDRiD dataset and validate the effectiveness of RTB and GTB\nin improving DR lesion segmentation outcomes.\nIn summary, our contributions are as follows:\n• We propose a dual-branch architecture to obtain vascular\ninformation, which contributes to locate the position of\nDR lesions. For effective use of vascular information in\nmulti-lesion segmentation, we design a relation trans-\nformer block (RTB) based on transformer mechanism.\nTo our best knowledge, this is the ﬁrst work to employ\nmulti-heads transformer structure in lesion segmentation\nin fundus medical images.\n• We present global transformer block (GTB) and relation\ntransformer block (RTB) to detect the special medical\npatterns with small size or blurred border. The design\nexplores the internal relationship between DR lesions\nwhich improves the performance in capture the details\nof interest.\n• Experiments on the IDRiD dataset show that our method\nachieves a front row ﬁnish on DR multi-lesion seg-\nmentation. Speciﬁcally, our method achieves the best\nperformance in exduates segmentation and ranks second\nin HE lesion segmentation. Experiments on the DDR\ndataset show that our method outperforms other methods\non EX, MA and SE segmentation task and ranks second\non HE segmentation task.\nII. R ELATED WORK\nA. Pathological Analysis of the DR Lesions\nDR lesions segmentation is a complex topic due to large\nintra-class variance. Furthermore, DR lesions vary with differ-\nent stages of disease as well, which also brings challenges to\nsegmentation. However, instead of discovering lesions directly,\nwe notice that there are pathological associations between\nthese lesions, which can be depicted in the spatial distribution.\nWe ﬁrst investigate the possible pathological causes of DR\nlesions. Brieﬂy, MA is the earliest lesion of DR observed as\nspherical lateralized swelling which is produced by vascular\natresia; EX looks like yellowish-white well-deﬁned waxy\npatch, generally thought to be lipid produced by the rupture\nof the retinal nerve tissues, incidentally, which is also resulted\nfrom the vascular atresia. Additionally, when the rupture of\nvessels happens after the vascular atresia, bloods leak out\nfrom the vessels, which leads to the lipoproteins in vessels\nleaking into the retina as well. The leaking bloods form the\nHE patterns and the leaking lipoproteins form the SE patterns\nwith poorly deﬁned borders. In summary, as shown in Fig.1(b),\nmost of the EXs are observed arranged in a circular pattern\naround one or several MAs (the bottom line) and most of the\nSEs appear at the edge of HEs (the upper line).\nIn addition to the intra-class dependencies among lesions,\nthe inter-class relations between DR lesions and vessels also\nmake great sense. As mentioned above, vascular abnormalities\nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING 3\nare the direct or indirect causes of DR lesions, speciﬁcally, we\nfound the fact that SEs are often distributed near the trunk of\nupper and lower arteries, and MAs are generally distributed\namong the capillaries. Furthermore, intricate fundus tissues\noften confuse the identiﬁcation of lesions, but we notice that\nthere are certain pattern rules of fundus, especially in the\ndistribution of various veins and arteries, which would provide\nvaluable prior information.\nTo conﬁrm the above pathological analysis, we count the\ndistances in pixel between different fundus tissues on the\nIDRiD dataset. Fig.1 (c) illustrates the distance between cluster\ncenter of EX and the nearest neighbor MA, SE and the nearest\nneighbor HE, MA and the closet capillaries, SE and closest\nupper and lower arteries, respectively. The statistical results\nverify that there is an exploitable pattern in the distribution of\nDR lesions.\nB. Deep Neural Networks in DR Lesion Segmentations\nDR lesion segmentation task based on traditional image\nprocessing techniques [15]–[17] is facing two main challenges:\nthe great morphological differences of the same lesions in\ndifferent disease stages and the confusions of DR lesions and\nsimilar structures in the fundus. These two problems have not\nbeen effectively solved until deep neural networks (DNNs)\nexploded in the ﬁeld of computer vision (CV) [18] and have\nalso been widely applied to DR lesion segmentations [19]–\n[21].\nHowever, DNNs also raise new difﬁculties. For instance,\nthe detailed information is easily lost by deep networks but\nmost of the DR lesions are very small and even just one\nor two pixels. Besides, considering the balance of different\ncharacteristics of red and exudate lesions, the accuracy of\nmulti-task model is limited.\nTo deal with the above issues, researchers have proposed\nmany improvements, which can be summarised as two direc-\ntions:\nFirstly, some researchers focus on the designs of attention\nmodels fusing low-level and high-level features together to\navoid details lost in the deep network. Zhang et al. [22] fused\nmultiple features with distinct target features in each layer\nbased on attention mechanism and achieved preliminary MA\ndetection. Wang et al. [23] designed a dual-branch attention\nnetwork, with one producing a 5-graded score map and the\nother producing an attention gate map combined to the score\nmap to highlight suspicious regions. Zhou et al. [8] applied\nlow-level and high-level guidances to different lesion features\nand obtained the reﬁned multi-lesion attention maps, which\nwere further employed as pseudo-masks to train the segmen-\ntation model.\nSecondly, the task of segmenting DR lesions is divided into\nsegmenting red lesions and exudate lesions separately, which\nevades the balanced cost of inter-class disparities and enables\nfully learning of the same type of lesions. Mo et al. [13]\ndesigned a fully convolutional residual network incorporating\nmulti-level hierarchical information to segment the exudates\nwithout taking the red lesions into account. Xie et al. [24]\nbuilt a general framework to predict the errors generated by\nexisting models and then correct them, which performed well\nin MA segmentation.\nHowever, the ﬁrst direction pays much attention to the\nnetwork designs, with seldom considering the pathological\nconnections of DR lesions, while the second direction leads to\ntime consuming and large memory requirements. In order to\ntake advantages of the pathological connections and improve\nthe efﬁciency of the multi-task model, we propose a RTB\nconsisting of self-attention and cross-attention head to segment\nDR lesions simultaneously.\nC. Transformer in Medical Images\nTransformer network has been one of the fundamental\narchitecture for natural language processing (NLP) since 2017\ndue to the efﬁcient and effective self-attention mechanism [25].\nIt improves the performance on many NLP tasks, such as text\nclassiﬁcation, general language understanding and question\nanswering. Compared with recurrent networks, transformer\nnetwork achieves parallel computation and reduces the com-\nputational complexity. In a basic transformer attention block,\nQuery (Q), Key (K), Value (V) are the three typical inputs to a\nattention operation. At ﬁrst, Qand Kare computed in the form\nof pairwise function to obtain the corresponding attention of\nall points on K for each point on Q. The pairwise function can\noptionally be Gaussian, Embedding Guassian, Dot-Product,\nConcatenation and etc.. Then, the product is multiplied by V\nand passes through a column-wise softmax operator to ensure\nevery column sum to 1. Every position on the output contains\nthe recoded global information by attention mechanism. In\nself-attention operator, Q = K = V, so that the output has\nthe same shape with input.\nInspired by the success in the domain of NLP, a standard\ntransformer was applying to CV in 2020 [26] with the fewest\nmodiﬁcations, called as Vision Transformer (ViT). The input\nto a ViT is a sequence of cropping images which are linearly\nencoded by aliquoting the original image. The image patches\nare treated the same way as tokens (words) in an NLP\napplication.\nRecently, the transformer architecture has also been applied\nto the ﬁeld of medical image processing. Liu et al. [27]\nproposed a global pixel transformer (GPT) to predict several\ntarget ﬂuorescent labels in microscopy images. The GPT is\nsimilar to a three-headed transformer with different sizes of\nquery inputs, which allows it to adequately capture features at\ndifferent scales. Guo et al. [28] applied the ViT to anisotropic\n3D medical image segmentation, with the self-attention model\narranged at the bottom of the Unet architecture. Song et al. [29]\nbuilt a Deep Relation Transformer (DRT) to combine OCT\nand VF information for glaucoma diagnosis. They modiﬁed\nthe standard transformer to an interactive transformer that\nutilizes a relationship map of VF features interacting with OCT\nfeatures.\nIII. M ETHODOLOGY\nIn this section, we ﬁrst give a brief overview of our\nproposed network, and then elaborate on the key network\ncomponents, i.e., global transformer block (GTB) and relation\n4 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2022\ntransformer block (RTB). Finally, designed loss function is\nfurther provided.\nA. Overview\nGiven an input fundus image, the proposed network is\ndesigned to output one vascular mask and four lesion masks\nin parallel. Fig.2 depicts its overall architecture, which is\ncomprised of four key components: backbone, global trans-\nformer block (GTB), relation transformer block (RTB), and\nsegmentation head. A dual-branch architecture is employed\nupon the backbone to explore vascular and pathological fea-\ntures separately, where the transformers based on GTB and\nRTB are incorporated to reason about interactions among both\nfeatures.\nTo be speciﬁc, the fundus image ﬁrst passes through a\nbackbone to obtain an abstracted feature map F, with a spatial\nresolution of W ×H and C number of channels. Then,\ntwo parallel branches comprised of global transformer block\n(GTB) are incorporated to exploit long-range dependencies\namong pixels in F, resulting in speciﬁc vessel features Fv\nand primary lesion features Fl fueled with global contextual\ninformation, respectively. Upon the branch providing lesion\nfeatures, we further integrate a relation transformer block\n(RTB) to model spatial relations between vessels and lesions\ndue to their inherent pathological connections using a self-\nattention and a cross-attention head: the self-attention head\ninputs only the lesion features Fl, and exploits long-range\ncontextual information to generate self-attentive features Fs\nthrough a self-attention mechanism; the cross-attention head\ninputs both the lesion and vessel features Fl, Fv, and in-\ncorporates beneﬁcial ﬁne-grained vessel structural information\ninto Fv, producing cross-attentive features Fc. The resulting\nFs and Fc are concatenated together to form the output of\nthe RTB. Finally, two sibling heads, each of which contains\na Norm layer and a 1 ×1 convolution, are used to predict\nvascular and pathology masks based on the vessel features\nand concatenated lesion features, respectively.\nGTB contains one head while RTB contains two heads.\nAlthough the basic heads of GTB and RTB are based on the\ntransformer structure that generates query, key and value for\nrelation reasoning, they are structurally different in our work.\nThe query of head in GTB is similar to a channel-wise weights\nand the one in RTB has the same size in spatial dimension with\nthe input. In a training process, GTB is employed to generate\nspeciﬁc multi-lesion and vessel features independently which\nmaintain more details of interest, and RTB further exploits\nthe inherent pathogenic relationships between multi-lesion\nand vessels, which eliminate noise and imply the location\ninformation.\nB. Global Transformer Block\nThe Global Transformer Block (GTB) contains two parallel\nbranches of the same architecture to extract features for lesions\nand vessels separately. Such a dual-branch design owns to\nthe fact that lesions and vessels generally have dramatically\ndifferent visual patterns. To be concrete, lesions are discrete\npatterns, with nearly random spatial distribution. While vessels\nare topological connected structures, and the layout of vascular\ntrunks, containing central retinal artery, ciliary artery and etc,\ngenerally follows some common rules. It is hence necessary\nto use specialized branches to learn speciﬁc characteristics of\ndifferent objects of interest.\nFig.3 presents the detailed structure of each GTB branch.\nIt takes an input F ∈R C×W×H generated from the back-\nbone, and outputs attentively-reﬁned feature maps Fi ∈\nR C×W×H,i ∈ {l,v} of lesions and vessels respectively.\nSpeciﬁcally, GTB follows the typical framework of trans-\nformer networks. Three generators, denoted as Q, Kand V\nare ﬁrst employed to transform the input F into query, key and\nvalue, respectively. In GTB, the generator Qis implemented\nwith a 3×3 convolution followed by a global average pooling,\nand outputs a query vector Q(F) ∈R C\n′\n×1 with the channel\nnumber designed as C′= C/8; the generator Kand Vhave\nthe same architecture as Qexpect for replacing the global\naveraging pooling with a reshape operation, leading to the key\nand value K(F),V(F) ∈R C\n′\n×HW .\nWe deﬁne the pairwise function of query and key as a matrix\nmultiplication:\nF(F) =K(F)T Q(F), (1)\nwhere the superscript T denotes a transpose operator for ma-\ntrix. Note that the query of GTB acts as a channel-wise query\ninstead of the position query as NLNet [30]. To be speciﬁc, the\nquery vector is considered as a feature selector for channels\nof key matrix. Subsequently, the product F(F) ∈R HW ×1\nalso acts as a feature selector for spatial positions of value\nmatrix. In summary, the GTB can be roughly described as a\nattention mechanism which fuses channel-wise ﬁrst and then\nspatial-wise weighted features together with input information.\nNext, we consider the global transform operation deﬁned\nas:\nG(F) =V(F)softmax(F(F)) ∈R C\n′\n×1, (2)\nwhere softmax is a softmax function to normalize the F(F).\nThen, We take the obtained attentive features G(F) with a\nlinear embedding as a residual term to the input F, and get\nthe ﬁnal output through a residual connection:\nFi = WG(F) +F, i∈{l,v}, (3)\nwhere the + operation denotes broadcasting element-wise sum\noperation; the W is a linear embedding, implemented as 1×1\nconvolution to convert the channel number of intermediate\nfeature map from C\n′\nback to C. As a result, the output\nfeatures are received with the same format as the input, but\nhave been enriched with specialized vessel and lesion features,\nrespectively.\nThe GTB structure is inspired by the GCNet [14]. They both\nfollow the idea of transformer mechanism but generate the\nper-channel weights. The weight vectors in GCNet and GTB\nboth obtained by a matrix multiplication, but different from\nGCNet, GTB attains the two multipliers with three generator\nto further highlight the useful channels in each position,\nrealizing both channel-wise and spatial-wise attention. Due\nto the fundus lesion features, especially the small discrete\nones, are easily confused with artifacts or idiosyncratic tissues,\nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING 5\nFig. 2. Pipeline of the proposed method. The input image passes through a backbone to obtain the shared feature F. Then the shared feature\nF takes two branches to achieve vessels and multi-lesion segmentation respectively. Two Global Transformer Blocks (GTB) are applied to both\nbranches to generate speciﬁc features, and a Relation Transformer Block (RTB) is incorporated after GTB to explore the inherent pathological\nconnections among multi-lesion and between multi-lesion and vessels.\nFig. 3. The overall structure of Global Transformer Block (GTB).\nFig. 4. The details of Relation Transformer Block (RTB).\nuseful information tends to exist in only a few pixels of certain\nchannels. It is an improved method aimed at the feasibility of\nsmall discrete pattern segmentation in fundus images.\nC. Relation Transformer Block\nRelation Transformer Block (RTB) consists of a self-\nattention and a cross-attention head, used to capture intra-class\ndependencies among lesions and inter-class relations between\nlesions and vessels, respectively, as shown in Fig.4. In each\nhead, three trainable linear embeddings, implemented with\na 3 ×3 convolution followed by a reshape operation, are\nemployed as the query, key and value generator Gi,Ki,Vi,i ∈\n{s,c}, respectively. The pairwise computations of query and\nkey in self-attention head and cross-attention head are de-\nscribed as:\nFs(Fl) =Ks(Fl)T Qs(Fl)\nFc(Fl,Fv) =Kc(Fv)T Qc(Fl), (4)\nwhere the subscripts s and c denote the self-attention and the\ncross-attention head, respectively. It is important to emphasize\nthat different from the self-attention head that derives the\nquery, key all from input lesion features Fl, the cross-attention\nhead generates the key from the vessel features Fv instead to\nintegrate vascular information.\nNext, the individual attentive features of the two heads are\ncomputed respectively as:\nGs(Fl) =Vs(Fl)softmax(Fs(Fl))\nGc(Fl,Fv) =Vc(Fv)softmax(Fc(Fl,Fv)). (5)\nWe adopt residual learning to each head as well and get the\noutputs:\nFi = WiGi(Fl,Fv) ⊕Fl\ni∈{s,c}, (6)\nwhere the Wi is a linear embedding implemented as 1 ×1\nconvolution, and the ⊕operation is performed by a residual\nconnection of element-wise addition.\nAs such, the self-attention head computes the response in\na position as a weighted sum of the features in all positions,\nand thus well captures long-range dependencies. Given the fact\nthat DR lesions are usually dispersed over a broad range, the\nself-attention can exchange message among multiple lesions,\nregardless of their positional distance, and thus allows the\nmodeling of intra-class pairwise relations of lesions. The head\nis supposed to distinguish the mixtures of more than two\nlesions and further reﬁne the edges of large patterns in lesion\nsegmentation.\nThe cross-attention head queries global vascular structures\nfrom the vessel features, thus incorporating interactions be-\ntween lesions and vessels. Considering that lesions and ves-\nsels have strong inherent pathogenic connections, the cross\nattention help to better locate MA and SE, and meanwhile\n6 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2022\neliminate false positives of EX caused by vessel reﬂection and\nMA caused by capillary confusion.\nWe concatenate the resulting features Fs from the self-\nattention head and that Fc from the cross-attention head,\nleading to the ﬁnal RTB output:\nFout = [Fs; Fc], (7)\nwhere the [·; ·] denotes the concatenation at channel dimen-\nsion.\nD. Loss Function\nWe employ two loss functions, i.e., Llesion and Lvessel for\nthe multi-lesion and vessel segmentation branches respectively,\nand the total loss of our network is deﬁned as:\nL= Llesion + λLvessel, (8)\nwhere the Llesion denotes the 5-class weighted cross-entropy\nloss for multi-lesion segmentation and the Lvessel is a binary\nweighted cross-entropy loss to learn vascular features; the λ\nis set as the weight in the loss function. When λ = 0.0, the\nnetwork is optimized by the multi-lesion features only, and\nas λ grows, vascular information plays an increasing role in\noptimization.\nIV. E XPERIMENTS AND RESULTS\nA. Datasets\nIDRiD Datasetis available for the segmentation and grad-\ning of retinal image challenge 2018 [31], [32]. The segmenta-\ntion part of the dataset contains 81 4288 ×2848 sized fundus\nimages, accompanied by four pixel-level annotations, i.e., EX,\nHE, MA and SE if the image has this type of lesion. In\ntotal, there are 81 EX annotations, 81 MA annotations, 80 HE\nannotations, and 40 SE annotations. The partition of training\nset and testing set is provided on IDRiD already, with 54\nimages for training and the rest 27 images for testing.\nDDR Dataset is provided by Ocular Disease Intelligent\nRecognition (ODIR-2019) for lesion segmentation and lesion\ndetection [33]. This dataset consists 13,673 fundus images\nfrom 147 hospitals, covering 23 provinces in China. For\nsegmentation task, 757 fundus images are provided with pixel-\nlevel annotation for EX, HE, MA and SE if the image has\nthis type of lesion. In total, there are 486 EX annotations, 570\nMA annotations, 601 HE annotations, and 239 SE annotations.\nThe partition of training set, validation set and testing set is\nprovided on DDR already, with 383 images for training, 149\nimages for validation and the rest 225 images for testing.\nB. Implementation Details\n1) Data Preparation: To prepare more trainable data, some\noperations are performed on the original images. First, the\nimages are input into the a segmentation model pretrained on\nDRIVE [39] and STARE [40] dataset with vessel annotations\nand the pseudo vascular masks are obtained. Next, limited by\nthe memory received, the large images are random resized\nand cropped into small pieces of 512 ×512 size, additionally\nwe apply random horizontal ﬂips, vertical ﬂips, and random\nrotation as forms of data augmentation to reduce overﬁtting.\nThen, in order to enhance image contrast while preserving\nlocal details, we process Contrast Limited Adaptive Histogram\nEqualization (CLAHE) on all input images with ClipLimit=2\nand GridSize=8 by setting. The CLAHE is proven to be\neffective due to the anomalousness distinguished from the\nbackground in diabetic fundus, and the quantitative results are\npresented in Table V.\n2) Model Settings: The typical UNet architecture [38] is a\npopular method for medical images segmentation, constructed\nby an encoder and a decoder with skip connections in channel-\nwise concatenation manner. In this paper, we apply DenseNet-\n161 [41] pretrained on ImageNet dataset as the backbone of\nUNet encoder [37] to achieve better performance. The channel\nnumber C of output of UNet is set to 32.\n3) Experiment settings: Our framework is implemented us-\ning pytorch backend and performed on NVIDIA GeForce RTX\n3090 GPU with 24GB of memory. During the training, the\nbatch-size is set to 16. The initial learning rate is set to 0.001\nand is decay in a step-wise manner to 0.1 times of the previous\nevery 120 epochs. All models are trained for 250 epochs with\nthe SGD optimizer with momentum 0.9 and weight decay\n0.0005.\nThe loss function settings are as follows: a) the return loss\nratio λ is set to 0.1; b) the weights of Llesion are set as\n0.001, 0.1, 0.1, 1.0, 0.1 for background, EX, HE, MA and SE\nrespectively; c) The coefﬁcients of background and vessels in\nLvessel are set as 0.01 and 1.0.\nC. Evaluation Metrics\nTo evaluate the performance of the proposed method, we\nemploy the area-under-the-curve (AUC) of both the precision\nand recall (PR) curve and receiving operating characteristic\n(ROC) curve [42], which are also recognized as metrics\nof fundus image segmentation in previous competitions and\nresearches. The former is more concerned with the accuracy\nof the true data in prediction, while the latter reﬂects the\nperformance of the data predicted positively. There is more\nemphasis on recall metric in medical images, which indicates\nthe performance of true samples being successfully predicted,\ni.e., the AUC PR drawn by recall metric shows more practical\nvalue, and the AUC ROC also characterizes the effectiveness\nof the model.\nD. Comparisons on Other State-of-the-art Methods\nWe compare our method with previous works reported\non the IDRiD dataset (Table I). Our method ranks ﬁrst in\nAUC ROC of EX, MA and SE and AUC PR of EX and\nSE, ranks second in AUC ROC and AUC PR of HE. Note\nthat the ﬁrst ﬁve methods of Table I all employed individual\nmodels segmenting the four lesions separately: according to\nthe conference reports of the top 3 IDRiD competition teams,\nfour models were developed for segmentation of four lesions\nrespectively; DRUNet [34] and SESV [35] proposed a speciﬁc\nnetwork to segment MA only. Although SESV achieved the\nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING 7\nTABLE I\nPERFORMANCE COMPARISON WITH THE STATE -OF-THE -ART WORKS REPORTED ON THE IDR ID DATASET, WHERE THE SEPARATE AND SAME\nINDICATES THE WAY IN WHICH THE METHOD SEGMENTS THE LESIONS SEPARATELY BY DIFFERENT MODELS OR AT THE SAME TIME BY ONE MODEL\nMethod sepa-\nrate same Hard Exudates Haemorrhages Microaneurysms Soft Exudates\nAUC PR AUC ROC AUC PR AUC ROC AUC PR AUC ROC AUC PR AUC ROC\nVRT(1st) ✓ 0.7127 - 0.6804 - 0.4951 - 0.6995 -\nPATech(2nd) ✓ 0.8850 - 0.6490 - 0.4740 - - -\niFLYTEK-MIG(3rd) ✓ 0.8741 - 0.5588 - 0.5017 - 0.6588 -\nDRUNet [34] ✓ - - - - - 0.9820 - -\nSESV [24] ✓ - - - - 0.5099 - - -\nL-Seg [6] ✓ 0.7945 - 0.6374 - 0.4627 - 0.7113 -\nSSCL [8] ✓ 0.8872 0.9935 0.6936 0.9779 0.4960 0.9828 0.7407 0.9936\nRTN(Ours) ✓ 0.9024 0.9980 0.6880 0.9731 0.4897 0.9952 0.7502 0.9938\nTABLE II\nPERFORMANCE COMPARISON WITH STATE -OF-THE -ART SEGMENTATION METHODS ON THE DDR DATASET, WHERE * DENOTES THE RESULTS ARE\nREPRODUCED BY OURSELVES\nMethod Hard Exudates Haemorrhages Microaneurysms Soft Exudates\nAUC PR AUC ROC AUC PR AUC ROC AUC PR AUC ROC AUC PR AUC ROC\nHED [35] 0.4252 0.9612 0.2014 0.8878 0.0652 0.9299 0.1301 0.8215\nDeepLab v3+ [36] 0.5405 0.9641 0.3789 0.9308 0.0316 0.9245 0.2185 0.8642\nUNet [37], [38] 0.5505 0.9741 0.3899 0.9387 0.0334 0.9366 0.2455 0.8778\nL-seg* [6] 0.5645 0.9726 0.3588 0.9298 0.1174 0.9423 0.2654 0.8795\nRTN(Ours) 0.5671 0.9751 0.3656 0.9321 0.1176 0.9452 0.2943 0.8845\nbest AUC PR of MA, such individual designs require modify-\ning a large number of hyper-parameters in training stage and\nlead to time consuming in inference. The rest methods of Table\nI: L-Seg [8], SSCL [8] and our network, all propose one model\nto segment the four lesions at the same time. SSCL performs\nbetter than ours in AUC PR of MA segmentation but worse in\nAUC ROC, that might result from the fact that although RTB\nreduces false positives of MA away from the vessels, it also\nintroduces the false negatives of MA.\nWe also verify the effectiveness of the proposed method\non DDR dataset. Compared with the IDRiD dataset, the DDR\ndataset is an updated dataset with few results reported on it,\nso we apply some state-of-the-art segmentation methods on\nthe DDR dataset to make comparisons. As shown in Table\nII, in the comparison with other state-of-the-arts, our method\nachieves the best performance in EX, MA and SE and ranks\nsecond in HE. It is obvious that our method is not doing\nthe best job in HE segmentation both in IDRiD and DDR\ndataset, which may be explained by the fact that the large\nHE patterns are formed by blood irregularly haloing on the\nretina, and the speciﬁc bleeding points have been blurred.\nIn this case, RTB, an approach that is more concerned with\ntheoretical correlations, does not contribute useful information\nin the segmentation of large HEs. As mentioned in the\ndataset section, considering there are many low quality images\nwith uneven illumination, underexposure, overexposure, image\nblurring, retinal artifacts and other disturbing lesion tissues in\nDDR dataset, DDR dataset is more challenging than IDRiD\ndataset, which results in the performance of the former lagging\nbehind that of the latter.\nE. Ablation Studies on IDRiD Dataset\nWe conduct ablation studies to better understand the impact\nof each component of our network. First, results of several\nencoding architectures are available to select the proper one as\nTABLE III\nPERFORMANCE COMPARISON OF THE DIFFERENT BACKBONES WITH\nOUR NETWORK\nFramework Encoder AUCPR\nEX HE MA SE\nUNet [37]\nResNet-34 [43] 0.8778 0.6764 0.4659 0.7407\nResNet-50 [43] 0.8858 0.6874 0.4692 0.7475\nXception [44] 0.8924 0.6806 0.4851 0.7424\nVgg19-bn [7] 0.8821 0.6832 0.4789 0.7423\nDenseNet-161 [41] 0.9024 0.6880 0.4897 0.7502\nbackbone. Then, considering the topology of vessels, regular-\nization terms of loss function are discussed. Next, we analyze\nthe effect of GTB based on the baseline, which is deﬁned\nas the complete workﬂow without GTB and RTB. In order\nto identify whether vascular information contributes to lesion\nsegmentation in advanced, we apply concatenation operation\nto the two outputs of GTBs directly. Finally, the roles of\nboth self-attention head and cross-attention head in RTB are\ndiscussed thoroughly. Since AUC PR is considered as the most\nimportant clinical evaluation metric for lesion segmentation in\nfundus images, we simplify the metrics to AUC PR only for\nablative comparisons. The loss and PR curves obtained from\ndifferent experiments are set out in Fig.5 and detailed AUC PR\nvalues can be compared in Table VI.\n1) Analysis on the Backbone:To compare the effectiveness\nof backbone models, we perform experiments to select proper\nencoding architecture. ResNet-34 [43], ResNet-50 [43], Xcep-\ntion [44], Vgg19-bn [7] and DenseNet-161 [41] integrated with\nUNet are implemented with the segmentation heads of our\nnetwork. As can be seen from the Table III, the DenseNet-\n161 integrated with UNet achieves the best performance in all\nlesions and is utilized as the following backbone.\n2) Analysis on the Regularization Term: Considering the\ntopology of vessels, an extension of loss function regular-\nization terms has been conducted. In the result of vessel\nsegmentation, there are two common cases, the neglected ends\n8 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2022\nFig. 5. Loss and PR curves for segmentation over four DR lesions. Ablation studies are compared to explore the effectiveness of the baseline itself\nand stacked by GTB, self-attention head (sah), cross-attention head (rah) and RTB one by one.\nFig. 6. Visualization of (a) Original images with annotations; (b) spatial\nattention features with Convolutional Block Attention Module (CBAM)\nand (c) query attention features F with Global Transformer Block (GTB)\nfor three images from IDRiD dataset. The GTB picks more discrete and\nsmall lesions up and ﬁne-grains the patterns of interest.\nand the truncated trunks. Based on the above cases, we propose\ntwo regularization terms Rthin and Rcl. The former inspired\nby [45] applies focal-loss function speciﬁcally on peripheral\nvessels, and the latter utilizes the center-line idea of [46]\nto ensure the connectivity. Table IV indicates that the Rthin\nimproves the performance of MA segmentation and the Rcl\nachieves the best grades in AUC PR of HE and MA. However,\nTABLE IV\nPERFORMANCE COMPARISON OF DIFFERENT REGULARIZATION TERMS\nON IDR ID DATASET\nRthin Rcl\nAUC PR\nEX HE MA SE\n0.9024 0.6880 0.4897 0.7502\n✓ 0.8975 0.6845 0.4899 0.7432\n✓ 0.8912 0.6891 0.4901 0.7435\n✓ ✓ 0.8923 0.6808 0.4895 0.7426\nthe alterations are unremarkable, probably due to the fact that\nthe groundtruths of vessels are pseudo-masks generated by\nsemi-supervision, rather than manually annotated masks. The\nupper bound on the performance of the vessel segmentation\nrestricts the improvement of the regularization terms on the\nﬁnal results.\n3) Analyze the Effect of GTB: Table VI shows that GTB\nimproves the performance on the basis of baseline, which\nindicates the weights assigned to channels in each position\nby GTB beneﬁt the segmentation of all four lesions. To\nhighlight the performance of GTB further, we compare it\nwith other popular attention blocks under the same model\nparameters. Table V illustrates that the performance of GTB on\nIDRiD dataset is better than others. Different from the pooling\noperation as other attention blocks do, channel-wise weights\nof GTB are speciﬁc in each pixel, so that different channels\nare enlightened in different pixels.\nAs shown in Fig.6, the snapshots of attentive features after\nsoftmax function are taken in color. In the comparison of the\nspatial attention features of CBAM [48] and the attention fea-\ntures Fof GTB, many discrete and small patterns overlooked\nby the former are noticed by the latter.\n4) Analyze the Effect of RTB:Table VI lists the ablation\nresults. We ﬁrst reafﬁrm the idea that vascular information\nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING 9\nFig. 7. Visualization of the query position (red points) of different lesions and their two query-speciﬁc attention maps with Relation Transformer\nBlock (RTB). The red borders denote the self-attention maps, and the blue denote the cross-attention maps. The attention of different query positions\nin EX, HE, MA and SE varies.\nTABLE V\nPERFORMANCE COMPARISON OF DIFFERENT ATTENTION BLOCKS ON THE IDR ID DATASET\nlesion Hard Exudates Haemorrhages Microaneurysms Soft Exudates\nmethod AUC PR AUC ROC AUC PR AUC ROC AUC PR AUC ROC AUC PR AUC ROC\nbaseline(without CLAHE) 0.8025 0.9912 0.6031 0.9498 0.3912 0.9803 0.5478 0.9120\nbaseline 0.8593 0.9919 0.6284 0.9553 0.4279 0.9830 0.5766 0.9171\nbaseline+SENet [47] 0.8653 0.9931 0.6408 0.9497 0.3861 0.9869 0.5683 0.9299\nbaseline+CBAM [48] 0.8606 0.9918 0.6470 0.9480 0.3979 0.9871 0.5525 0.9373\nbaseline+GC [14] 0.8633 0.9929 0.6406 0.9488 0.4031 0.9809 0.5540 0.9251\nbaseline+GTB(Ours) 0.8659 0.9933 0.6570 0.9534 0.4071 0.9879 0.5968 0.9458\nTABLE VI\nPERFORMANCE COMPARISON OF DIFFERENT COMPONENTS OF OUR NETWORK ON THE IDR ID DATASET, WHERE CAT DENOTES A SIMPLE\nCONCATENATE OF MULTI -LESION AND VESSEL FEATURES IN CHANNEL -WISE , CAH AND SAH IS ABBREVIATIONS FOR CROSS -ATTENTION HEAD AND\nSELF -ATTENTION HEAD RESPECTIVELY\nFramework GTB cat RTB AUC PR\ncah sah EX HE MA SE\nbaseline\n0.8593 0.6284 0.4071 0.5766\n✓ 0.8659 0.6570 0.4279 0.5968\n✓ ✓ 0.8672 0.6756 0.4294 0.6663\n✓ ✓ 0.8682 0.6818 0.4847 0.7463\n✓ ✓ 0.8862 0.6846 0.4663 0.7422\n✓ ✓ ✓ 0.9024 0.6880 0.4897 0.7502\ncontributes to multi-lesion segmentation by concatenating the\nvascular and multi-lesion features in channel-wise. In the\ncomparison with and without concatenations, the greatest\nperformance gains are obtained for HE and SE, which is\nconsistent with the previous analysis that the HE and SE have\nstrong relations with vessels. In order to make full use of\nvascular information, RTB is applied instead of simple con-\ncatenation. Compared with the simple concatenation, cross-\nattention head incorporating multi-lesion and vascular features\nin a transformer way enhances the scores of all four lesions\nfurther. Likewise, with the integration of self-attention head,\nthe scores get a huge improvement as well.\nAs visualized in Fig.7, query-speciﬁc attention maps focus\non specialized tissues. Take the query pixel on MA as an\nexample, the self-attention tends to smaller patterns, and the\ncross-attention specializes the vascular tributaries, which are\nsupposed to assist in reducing false negatives mistaken for\ntributaries and eliminating false alarms far from tributaries.\nBoth self-attention head and cross-attention head play a role\nin improving the network performance, evincing the fact that\nexploring the internal relationships of multi-lesion and vessels\nmakes sense.\nFinally both GTB and RTB are incorporated and the net-\nwork achieves the highest results. As shown in the bottom half\nof the Fig.5, the curve corresponding to the complete network\nwraps almost entirely around the others.\nF . Generalization Studies on DDR and IDRiD Dataset\nFor medical images, it is challenging but meaningful to re-\nalize the generalization over different domains under different\nimaging conditions. In purpose to validate the generalization\ncapability, models are trained with the images from the train\nset of DDR dataset and tested on test set of IDRiD dataset\nwhich is captured from another source. Table VII compares\nthe results obtained from the preliminary analysis of general-\nization. From the chart, it can be seen that our method achieves\nthe best performance by narrowing down the gap between\nimages under different conditions.\nG. Qualitative Results\nTo better illustrate the effect of GTB and RTB, we visualize\nthe results of certain images. Fig.8 compares the segmentation\n10 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2022\nFig. 8. Visualization of segmentation results for multi-lesion segmentation on IDRiD dataset. The different columns represent the original images,\nthe segmentation results generated by baseline, baseline+GTB, baseline+GTB+RTB and groundtruths respectively. The yellow boxes denote the\nimprovements over baseline brought about by GTB, in the form of pickups of missing detections, while the green boxes denote the improvements\nover baseline+GTB brought about by RTB, mainly in the form of fewer false alarms.\nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING 11\nTABLE VII\nPERFORMANCE COMPARISON OF DIFFERENT METHODS ON THE\nGENERALIZATION FROM DDR DATASET TO IDR ID DATASET\nFramework AUC PR\nEX HE MA SE\nHED [35] 0.5420 0.2104 0.1245 0.1278\nDeepLab v3+ [36] 0.6480 0.4472 0.1823 0.2926\nUNet [37], [38] 0.6472 0.4452 0.1965 0.2845\nLseg [6] 0.6501 0.4405 0.1986 0.3059\nRTN(Ours) 0.6799 0.4504 0.2114 0.3401\nresults with corresponding original images and groundtruths.\nWe take the segmentation maps of baseline, baseline with GTB\nand baseline with GTB and RTB to present the improvements\nof different components of our network. The yellow boxes\npresent the improvements of GTB, where the missing de-\ntections are discovered. The green boxes are steps up from\nthe yellow boxes by RTB, which picks up missing detections\nfurther and reduces false alarms. Additionally, the edges of the\nlarge lesion patterns are more precisely ﬁne-tuned by RTB,\nespecially for the SE with blurred edges.\nV. C ONCLUSION AND DISCUSSION\nIn this paper, we present a novel network that employs\na dual-branch architecture with GTB and RTB to segment\nthe four DR lesions simultaneously. Outstanding experiment\nresults of our network can be attributed to GTB and RTB,\nwhich investigate the intra-class dependencies among multi-\nlesion and inter-class relations of multi-lesion and vessels.\nHowever, limited to the considerable cost of expertise pixel-\nlevel annotations, the vessel pseudo masks provided by semi-\nsupervised learning are inevitably coarse-grained and lead to\nthe inadequacy of our network. Therefore, in our future work,\nwe will further modify the vascular semi-supervised learning\nstrategy and keep improving the transformer structures to\nachieve better performance in DR multi-lesion segmentation\nwith less memory requirement.\nREFERENCES\n[1] R. Thomas, S. Halim, S. Gurudas, S. Sivaprasad, and D. Owens, “Idf\ndiabetes atlas: A review of studies utilising retinal photography on the\nglobal prevalence of diabetes related retinopathy between 2015 and\n2018,” Diabetes research and clinical practice, vol. 157, p. 107840,\n2019.\n[2] T. A. Ciulla, A. G. Amador, and B. Zinman, “Diabetic retinopathy\nand diabetic macular edema: pathophysiology, screening, and novel\ntherapies,” Diabetes care, vol. 26, no. 9, pp. 2653–2664, 2003.\n[3] R. Raman, L. Gella, S. Srinivasan, and T. Sharma, “Diabetic retinopa-\nthy: An epidemic at home and around the world,” Indian journal of\nophthalmology, vol. 64, no. 1, p. 69, 2016.\n[4] T. Y . Wong, J. Sun, R. Kawasaki, P. Ruamviboonsuk, N. Gupta,\nV . C. Lansingh, M. Maia, W. Mathenge, S. Moreker, M. M. Muqit\net al., “Guidelines on diabetic eye care: the international council of\nophthalmology recommendations for screening, follow-up, referral, and\ntreatment based on resource settings,” Ophthalmology, vol. 125, no. 10,\npp. 1608–1622, 2018.\n[5] D. S. W. Ting, G. C. M. Cheung, and T. Y . Wong, “Diabetic retinopathy:\nglobal prevalence, major risk factors, screening practices and public\nhealth challenges: a review,” Clinical & experimental ophthalmology,\nvol. 44, no. 4, pp. 260–277, 2016.\n[6] S. Guo, T. Li, H. Kang, N. Li, Y . Zhang, and K. Wang, “L-seg: An\nend-to-end uniﬁed framework for multi-lesion segmentation of fundus\nimages,” Neurocomputing, vol. 349, pp. 52–63, 2019.\n[7] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” arXiv:1409.1556, 2014.\n[8] Y . Zhou, X. He, L. Huang, L. Liu, F. Zhu, S. Cui, and L. Shao, “Col-\nlaborative learning of semi-supervised segmentation and classiﬁcation\nfor medical images,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 2019, pp. 2079–\n2088.\n[9] M. Tavakoli, S. Jazani, and M. Nazar, “Automated detection of microa-\nneurysms in color fundus images using deep learning with different pre-\nprocessing approaches,” in Medical Imaging 2020: Imaging Informatics\nfor Healthcare, Research, and Applications, vol. 11318. International\nSociety for Optics and Photonics, 2020, p. 113180E.\n[10] R. T. Mamilla, V . K. R. Ede, and P. R. Bhima, “Extraction of mi-\ncroaneurysms and hemorrhages from digital retinal images,” Journal of\nMedical and Biological Engineering, vol. 37, no. 3, pp. 395–408, 2017.\n[11] B. Wu, W. Zhu, F. Shi, S. Zhu, and X. Chen, “Automatic detection\nof microaneurysms in retinal fundus images,” Computerized Medical\nImaging and Graphics, vol. 55, pp. 106–112, 2017.\n[12] P. Khojasteh, B. Aliahmad, and D. K. Kumar, “A novel color space\nof fundus images for automatic exudates detection,” Biomedical Signal\nProcessing and Control, vol. 49, pp. 240–249, 2019.\n[13] J. Mo, L. Zhang, and Y . Feng, “Exudate-based diabetic macular edema\nrecognition in retinal images using cascaded deep residual networks,”\nNeurocomputing, vol. 290, pp. 161–171, 2018.\n[14] J. Ni, J. Wu, J. Tong, Z. Chen, and J. Zhao, “Gc-net: Global context\nnetwork for medical image segmentation,” Computer Methods and\nPrograms in Biomedicine, vol. 190, p. 105121, 2019.\n[15] T. Walter, P. Massin, A. Erginay, R. Ordonez, C. Jeulin, and J.-C.\nKlein, “Automatic detection of microaneurysms in color fundus images,”\nMedical Image Analysis, vol. 11, no. 6, pp. 555–566, 2007.\n[16] M. Niemeijer, B. van Ginneken, J. Staal, M. Suttorp-Schulten, and\nM. Abramoff, “Automatic detection of red lesions in digital color fundus\nphotographs,” IEEE Transactions on Medical Imaging, vol. 24, no. 5,\npp. 584–592, 2005.\n[17] S. H. M. Alipour, H. Rabbani, M. Akhlaghi, A. M. Dehnavi, and S. H.\nJavanmard, “Analysis of foveal avascular zone for grading of diabetic\nretinopathy severity based on curvelet transform,” Graefe’s Archive for\nClinical and Experimental Ophthalmology, vol. 250, no. 11, pp. 1607–\n1614, 2012.\n[18] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\nwith deep convolutional neural networks,” Advances in Neural Informa-\ntion Processing Systems, vol. 25, pp. 1097–1105, 2012.\n[19] A. He, T. Li, N. Li, K. Wang, and H. Fu, “CABNet: Category\nAttention Block for Imbalanced Diabetic Retinopathy Grading,” IEEE\nTransactions on Medical Imaging, vol. 40, no. 1, pp. 143–153, 2021.\n[20] X. Li, X. Hu, L. Yu, L. Zhu, C.-W. Fu, and P.-A. Heng, “CANet: Cross-\nDisease Attention Network for Joint Diabetic Retinopathy and Diabetic\nMacular Edema Grading,” IEEE Transactions on Medical Imaging ,\nvol. 39, no. 5, pp. 1483–1493, 2020.\n[21] Z. Gu, J. Cheng, H. Fu, K. Zhou, H. Hao, Y . Zhao, T. Zhang, S. Gao,\nand J. Liu, “CE-Net: Context Encoder Network for 2D Medical Image\nSegmentation,” IEEE Transactions on Medical Imaging, vol. 38, no. 10,\npp. 2281–2292, 2019.\n[22] L. Zhang, S. Feng, G. Duan, Y . Li, and G. Liu, “Detection of microa-\nneurysms in fundus images based on an attention mechanism,” Genes,\nvol. 10, no. 10, p. 817, 2019.\n[23] Z. Wang, Y . Yin, J. Shi, W. Fang, H. Li, and X. Wang, “Zoom-\nin-net: Deep mining lesions for diabetic retinopathy detection,” in\nInternational Conference on Medical Image Computing and Computer-\nAssisted Intervention (MICCAI). Springer, 2017, pp. 267–275.\n[24] Y . Xie, J. Zhang, H. Lu, C. Shen, and Y . Xia, “Sesv: Accurate\nmedical image segmentation by predicting and correcting errors,” IEEE\nTransactions on Medical Imaging, vol. 40, no. 1, pp. 286–296, 2020.\n[25] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”\narXiv:1706.03762, 2017.\n[26] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n“An image is worth 16x16 words: Transformers for image recognition\nat scale,” arXiv:2010.11929, 2020.\n[27] Y . Liu, H. Yuan, Z. Wang, and S. Ji, “Global pixel transformers for\nvirtual staining of microscopy images,” IEEE Transactions on Medical\nImaging, vol. 39, no. 6, pp. 2256–2266, 2020.\n[28] D. Guo and D. Terzopoulos, “A transformer-based network for\nanisotropic 3d medical image segmentation,” in Proceedings of Inter-\nnational Conference on Patern Recognition (ICPR). IEEE, 2021, pp.\n8857–8861.\n12 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2022\n[29] D. Song, B. Fu, F. Li, J. Xiong, J. He, X. Zhang, and Y . Qiao, “Deep\nrelation transformer for diagnosing glaucoma with optical coherence\ntomography and visual ﬁeld function,” IEEE Transactions on Medical\nImaging, 2021.\n[30] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural net-\nworks,” in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2018, pp. 7794–7803.\n[31] P. Porwal, S. Pachade, R. Kamble, M. Kokare, G. Deshmukh, V . Sa-\nhasrabuddhe, and F. Meriaudeau, “Indian diabetic retinopathy image\ndataset (idrid): a database for diabetic retinopathy screening research,”\nData, vol. 3, no. 3, p. 25, 2018.\n[32] P. Porwal, S. Pachade, M. Kokare, G. Deshmukh, J. Son, W. Bae, L. Liu,\nJ. Wang, X. Liu, L. Gao et al., “Idrid: Diabetic retinopathy–segmentation\nand grading challenge,” Medical Image Analysis, vol. 59, p. 101561,\n2020.\n[33] T. Li, Y . Gao, K. Wang, S. Guo, H. Liu, and H. Kang, “Diagnostic as-\nsessment of deep learning algorithms for diabetic retinopathy screening,”\nInformation Sciences, vol. 501, pp. 511 – 522, 2019. [Online]. Available:\nhttp://www.sciencedirect.com/science/article/pii/S0020025519305377\n[34] C. Kou, W. Li, W. Liang, Z. Yu, and J. Hao, “Microaneurysms seg-\nmentation with a u-net based on recurrent residual convolutional neural\nnetwork,” Journal of Medical Imaging, vol. 6, no. 2, p. 025008, 2019.\n[35] S. Xie and Z. Tu, “Holistically-nested edge detection,” in Proceedings of\nthe IEEE International Conference on Computer Vision (ICCV), 2015,\npp. 1395–1403.\n[36] L.-C. Chen, Y . Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-\ndecoder with atrous separable convolution for semantic image segmen-\ntation,” in Proceedings of the European Conference on Computer Vision\n(ECCV), 2018, pp. 801–818.\n[37] S. Guan, A. A. Khan, S. Sikdar, and P. V . Chitnis, “Fully dense unet for\n2-d sparse photoacoustic tomography artifact removal,” IEEE journal of\nbiomedical and health informatics, vol. 24, no. 2, pp. 568–576, 2019.\n[38] P. Yakubovskiy, “Segmentation models pytorch,” 2020. [Online].\nAvailable: https://github.com/qubvel/segmentation models.pytorch\n[39] J. Staal, M. D. Abr `amoff, M. Niemeijer, M. A. Viergever, and\nB. Van Ginneken, “Ridge-based vessel segmentation in color images\nof the retina,” IEEE Transactions on Medical Imaging, vol. 23, no. 4,\npp. 501–509, 2004.\n[40] A. Hoover, V . Kouznetsova, and M. Goldbaum, “Locating blood vessels\nin retinal images by piecewise threshold probing of a matched ﬁlter\nresponse,” IEEE Transactions on Medical Imaging, vol. 19, no. 3, pp.\n203–210, 2000.\n[41] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely\nconnected convolutional networks,” in Proceedings of the IEEE con-\nference on computer vision and pattern recognition (CVPR), 2017, pp.\n4700–4708.\n[42] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion,\nO. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg, J. Vander-\nplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay, “Scikit-learn: Machine learning in Python,” Journal of Machine\nLearning Research, vol. 12, pp. 2825–2830, 2011.\n[43] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition (CVPR), 2016, pp. 770–778.\n[44] F. Chollet, “Xception: Deep learning with depthwise separable convolu-\ntions,” in Proceedings of the IEEE conference on computer vision and\npattern recognition (CVPR), 2017, pp. 1251–1258.\n[45] L. Yang, H. Wang, Q. Zeng, Y . Liu, and G. Bian, “A hybrid deep\nsegmentation network for fundus vessels via deep-learning framework,”\nNeurocomputing, vol. 448, pp. 168–178, 2021.\n[46] S. Shit, J. C. Paetzold, A. Sekuboyina, I. Ezhov, A. Unger, A. Zhylka,\nJ. P. Pluim, U. Bauer, and B. H. Menze, “cldice-a novel topology-\npreserving loss function for tubular structure segmentation,” in Proceed-\nings of the IEEE international conference on computer vision (CVPR),\n2021, pp. 16 560–16 569.\n[47] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2018, pp. 7132–7141.\n[48] S. Woo, J. Park, J.-Y . Lee, and I. S. Kweon, “Cbam: Convolutional\nblock attention module,” in Proceedings of the European Conference on\nComputer Vision (ECCV), 2018, pp. 3–19."
}