{
  "title": "Sign Language Transformers: Joint End-to-End Sign Language Recognition and Translation",
  "url": "https://openalex.org/W3015075688",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2219775466",
      "name": "Necati Cihan Camgoz",
      "affiliations": [
        "University of Surrey"
      ]
    },
    {
      "id": "https://openalex.org/A2054195923",
      "name": "Oscar Koller",
      "affiliations": [
        "Microsoft (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2156010532",
      "name": "Simon Hadfield",
      "affiliations": [
        "University of Surrey"
      ]
    },
    {
      "id": "https://openalex.org/A2145160160",
      "name": "Richard Bowden",
      "affiliations": [
        "University of Surrey"
      ]
    },
    {
      "id": "https://openalex.org/A2219775466",
      "name": "Necati Cihan Camgoz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2054195923",
      "name": "Oscar Koller",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2156010532",
      "name": "Simon Hadfield",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2145160160",
      "name": "Richard Bowden",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2346219687",
    "https://openalex.org/W2159787221",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W6762422539",
    "https://openalex.org/W2519061812",
    "https://openalex.org/W2954798773",
    "https://openalex.org/W6638667902",
    "https://openalex.org/W6747721094",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2121326299",
    "https://openalex.org/W2903314716",
    "https://openalex.org/W2943845043",
    "https://openalex.org/W2997573805",
    "https://openalex.org/W4234518198",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W6694071422",
    "https://openalex.org/W6631943919",
    "https://openalex.org/W6694260854",
    "https://openalex.org/W6737778391",
    "https://openalex.org/W2066601700",
    "https://openalex.org/W6762718338",
    "https://openalex.org/W2937532208",
    "https://openalex.org/W2964051877",
    "https://openalex.org/W2192975138",
    "https://openalex.org/W6754392867",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2250636693",
    "https://openalex.org/W6747563306",
    "https://openalex.org/W6691399679",
    "https://openalex.org/W2250689755",
    "https://openalex.org/W6756040250",
    "https://openalex.org/W2075788335",
    "https://openalex.org/W2161919149",
    "https://openalex.org/W2034386547",
    "https://openalex.org/W2146221819",
    "https://openalex.org/W2766531849",
    "https://openalex.org/W6741933031",
    "https://openalex.org/W6601910725",
    "https://openalex.org/W2013148829",
    "https://openalex.org/W2522165970",
    "https://openalex.org/W6732160175",
    "https://openalex.org/W2188882108",
    "https://openalex.org/W2534456876",
    "https://openalex.org/W2127334685",
    "https://openalex.org/W2096976709",
    "https://openalex.org/W6771831205",
    "https://openalex.org/W2746301562",
    "https://openalex.org/W2908497602",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2939671996",
    "https://openalex.org/W6679434410",
    "https://openalex.org/W2981531407",
    "https://openalex.org/W2759302818",
    "https://openalex.org/W6603854914",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2799020610",
    "https://openalex.org/W6632172967",
    "https://openalex.org/W6637242042",
    "https://openalex.org/W6733484633",
    "https://openalex.org/W2755802490",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2970756316",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2996574577",
    "https://openalex.org/W2402144811",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2587277634",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2941870244",
    "https://openalex.org/W2898167222",
    "https://openalex.org/W2091636093",
    "https://openalex.org/W1867976179",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W2270585835",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W1538839500",
    "https://openalex.org/W1510952022",
    "https://openalex.org/W2250670799",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2739151452",
    "https://openalex.org/W2964253156",
    "https://openalex.org/W46193368",
    "https://openalex.org/W1665214252",
    "https://openalex.org/W2577700712",
    "https://openalex.org/W94699809",
    "https://openalex.org/W2781902187",
    "https://openalex.org/W2890952074",
    "https://openalex.org/W3022360907",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2964350391",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W1591242602",
    "https://openalex.org/W2888163892",
    "https://openalex.org/W2899771611"
  ],
  "abstract": "Prior work on Sign Language Translation has shown that having a mid-level sign gloss representation (effectively recognizing the individual signs) improves the translation performance drastically. In fact, the current state-of-the-art in translation requires gloss level tokenization in order to work. We introduce a novel transformer based architecture that jointly learns Continuous Sign Language Recognition and Translation while being trainable in an end-to-end manner. This is achieved by using a Connectionist Temporal Classification (CTC) loss to bind the recognition and translation problems into a single unified architecture. This joint approach does not require any ground-truth timing information, simultaneously solving two co-dependant sequence-to-sequence learning problems and leads to significant performance gains. We evaluate the recognition and translation performances of our approaches on the challenging RWTH-PHOENIX-Weather-2014T (PHOENIX14T) dataset. We report state-of-the-art sign language recognition and translation results achieved by our Sign Language Transformers. Our translation networks outperform both sign video to spoken language and gloss to spoken language translation models, in some cases more than doubling the performance (9.58 vs. 21.80 BLEU-4 Score). We also share new baseline translation results using transformer networks for several other text-to-text sign language translation tasks.",
  "full_text": "Sign Language Transformers:\nJoint End-to-end Sign Language Recognition and Translation\nNecati Cihan Camg¬®oz/_388, Oscar Koller/windows, Simon HadÔ¨Åeld /_388and Richard Bowden/_388\n/_388CVSSP, University of Surrey, Guildford, UK,/windowsMicrosoft, Munich, Germany\n{n.camgoz, s.hadfield, r.bowden}@surrey.ac.uk, oscar.koller@microsoft.com\nAbstract\nPrior work on Sign Language Translation has shown\nthat having a mid-level sign gloss representation (effectively\nrecognizing the individual signs) improves the translation\nperformance drastically. In fact, the current state-of-the-\nart in translation requires gloss level tokenization in order\nto work. We introduce a novel transformer based architec-\nture that jointly learns Continuous Sign Language Recogni-\ntion and Translation while being trainable in an end-to-end\nmanner. This is achieved by using a Connectionist Temporal\nClassiÔ¨Åcation (CTC) loss to bind the recognition and trans-\nlation problems into a single uniÔ¨Åed architecture. This joint\napproach does not require any ground-truth timing informa-\ntion, simultaneously solving two co-dependant sequence-to-\nsequence learning problems and leads to signiÔ¨Åcant perfor-\nmance gains.\nWe evaluate the recognition and translation perfor-\nmances of our approaches on the challenging RWTH-\nPHOENIX-Weather-2014T (PHOENIX14T) dataset. We re-\nport state-of-the-art sign language recognition and trans-\nlation results achieved by our Sign Language Transform-\ners. Our translation networks outperform both sign video to\nspoken language and gloss to spoken language translation\nmodels, in some cases more than doubling the performance\n(9.58 vs. 21.80 BLEU-4 Score). We also share new baseline\ntranslation results using transformer networks for several\nother text-to-text sign language translation tasks.\n1. Introduction\nSign Languages are the native languages of the Deaf and\ntheir main medium of communication. As visual languages,\nthey utilize multiple complementary channels1 to convey in-\nformation [62]. This includes manual features, such as hand\nshape, movement and pose as well as non-manuals features,\nsuch as facial expression, mouth and movement of the head,\nshoulders and torso [5].\nThe goal of sign language translation is to either convert\nwritten language into a video of sign (production) [59, 60]\nor to extract an equivalent spoken language sentence from\na video of someone performing continuous sign [9]. How-\never, in the Ô¨Åeld of computer vision, much of this latter work\n1Linguists refer to these channels as articulators.\nSpatial Embedding Word Embedding\nShifted Spoken Language Words\nSIGN LANGUAGE GLOSSES\nSpoken Language Sentence\nConnectionist Temporal Classification\nTransformer Encoder\nTransformer Encoder Transformer Decoder\nTransformer Decoder\nSLRT SLTT\nFigure 1: An overview of our end-to-end Sign Language\nRecognition and Translation approach using transformers.\nhas focused on recognising the sequence of sign glosses 2\n(Continuous Sign Language Recognition (CSLR)) rather\nthan the full translation to a spoken language equivalent\n(Sign Language Translation (SLT)). This distinction is im-\nportant as the grammar of sign and spoken languages are\nvery different. These differences include (to name a few):\ndifferent word ordering, multiple channels used to convey\nconcurrent information and the use of direction and space\nto convey the relationships between objects. Put simply, the\nmapping between speech and sign is complex and there is\nno simple word-to-sign mapping.\nGenerating spoken language sentences given sign lan-\nguage videos is therefore a spatio-temporal machine trans-\nlation task [9]. Such a translation system requires us to ac-\ncomplish several sub-tasks, which are currently unsolved:\nSign Segmentation: Firstly, the system needs to detect\nsign sentences, which are commonly formed using topic-\ncomment structures [62], from continuous sign language\nvideos. This is trivial to achieve for text based machine\ntranslation tasks [48], where the models can use punctua-\ntion marks to separate sentences. Speech-based recogni-\ntion and translation systems, on the other hand, look for\npauses, e.g. silent regions, between phonemes to segment\nspoken language utterances [69, 76]. There have been stud-\nies in the literature addressing automatic sign segmentation\n[36, 52, 55, 4, 13]. However to the best of the authors‚Äô\nknowledge, there is no study which utilizes sign segmenta-\ntion for realizing continuous sign language translation.\n2Sign glosses are spoken language words that match the meaning of\nsigns and, linguistically, manifest as minimal lexical items.\narXiv:2003.13830v1  [cs.CV]  30 Mar 2020\nSign Language Recognition and Understanding:Fol-\nlowing successful segmentation, the system needs to under-\nstand what information is being conveyed within a sign sen-\ntence. Current approaches tackle this by recognizing sign\nglosses and other linguistic components. Such methods can\nbe grouped under the banner of CSLR [40, 8]. From a com-\nputer vision perspective, this is the most challenging task.\nConsidering the input of the system is high dimensional\nspatio-temporal data, i.e. sign videos, models are required\nthat understand what a signer looks like and how they in-\nteract and move within their 3D signing space. Moreover,\nthe model needs to comprehend what these aspects mean in\ncombination. This complex modelling problem is exacer-\nbated by the asynchronous multi-articulatory nature of sign\nlanguages [51, 58]. Although there have been promising re-\nsults towards CSLR, the state-of-the-art [39] can only rec-\nognize sign glosses and operate within a limited domain of\ndiscourse, namely weather forecasts [26].\nSign Language Translation:Once the information em-\nbedded in the sign sentences is understood by the system,\nthe Ô¨Ånal step is to generate spoken language sentences. As\nwith any other natural language, sign languages have their\nown unique linguistic and grammatical structures, which\noften do not have a one-to-one mapping to their spoken\nlanguage counterparts. As such, this problem truly repre-\nsents a machine translation task. Initial studies conducted\nby computational linguists have used text-to-text statistical\nmachine translation models to learn the mapping between\nsign glosses and their spoken language translations [45].\nHowever, glosses are simpliÔ¨Åed representations of sign lan-\nguages and linguists are yet to come to a consensus on how\nsign languages should be annotated.\nThere have been few contributions towards video based\ncontinuous SLT, mainly due to the lack of suitable\ndatasets to train such models. More recently, Camgoz\net al . [9] released the Ô¨Årst publicly available sign lan-\nguage video to spoken language translation dataset, namely\nPHOENIX14T. In their work, the authors proposed ap-\nproaching SLT as a Neural Machine Translation (NMT)\nproblem. Using attention-based NMT models [44, 3], they\ndeÔ¨Åne several SLT tasks and realized the Ô¨Årst end-to-end\nsign language video to spoken language sentence transla-\ntion model, namely Sign2Text.\nOne of the main Ô¨Åndings of [9] was that using gloss\nbased mid-level representations improved the SLT per-\nformance drastically when compared to an end-to-end\nSign2Text approach. The resulting Sign2Gloss2Text model\nÔ¨Årst recognized glosses from continuous sign videos using\na state-of-the-art CSLR method [41], which worked as a\ntokenization layer. The recognized sign glosses were then\npassed to a text-to-text attention-based NMT network [44]\nto generate spoken language sentences.\nWe hypothesize that there are two main reasons why\nSign2Gloss2Text performs better than Sign2Text (18.13 vs\n9.58 BLEU-4 scores). Firstly, the number of sign glosses\nis much lower than the number of frames in the videos\nthey represent. By using gloss representations instead of\nthe spatial embeddings extracted from the video frames,\nSign2Gloss2Text avoids the long-term dependency issues,\nwhich Sign2Text suffers from.\nWe think the second and more critical reason is the\nlack of direct guidance for understanding sign sentences\nin Sign2Text training. Given the aforementioned complex-\nity of the task, it might be too difÔ¨Åcult for current Neu-\nral Sign Language Translation architectures to comprehend\nsign without any explicit intermediate supervision. In this\npaper we propose a novel Sign Language Transformer ap-\nproach, which addresses this issue while avoiding the need\nfor a two-step pipeline, where translation is solely depen-\ndent on recognition accuracy. This is achieved by jointly\nlearning sign language recognition and translation from\nspatial-representations of sign language videos in an end-to-\nend manner. Exploiting the encoder-decoder based archi-\ntecture of transformer networks [70], we propose a multi-\ntask formalization of the joint continuous sign language\nrecognition and translation problem.\nTo help our translation networks with sign language un-\nderstanding and to achieve CSLR, we introduce a Sign Lan-\nguage Recognition Transformer (SLRT), an encoder trans-\nformer model trained using a CTC loss [2], to predict sign\ngloss sequences. SLRT takes spatial embeddings extracted\nfrom sign videos and learns spatio-temporal representa-\ntions. These representations are then fed to the Sign Lan-\nguage Translation Transformer (SLTT), an autoregressive\ntransformer decoder model, which is trained to predict one\nword at a time to generate the corresponding spoken lan-\nguage sentence. An overview of the approach can be seen\nin Figure 1.\nThe contributions of this paper can be summarized as:\n‚Ä¢A novel multi-task formalization of CSLR and SLT\nwhich exploits the supervision power of glosses, with-\nout limiting the translation to spoken language.\n‚Ä¢The Ô¨Årst successful application of transformers for\nCSLR and SLT which achieves state-of-the-art results\nin both recognition and translation accuracy, vastly\noutperforming all comparable previous approaches.\n‚Ä¢A broad range of new baseline results to guide future\nresearch in this Ô¨Åeld.\nThe rest of this paper is organized as follows: In Sec-\ntion 2, we survey the previous studies on SLT and the state-\nof-the-art in the Ô¨Åeld of NMT. In Section 3, we introduce\nSign Language Transformers, a novel joint sign language\nrecognition and translation approach which can be trained\nin an end-to-end manner. We share our experimental setup\nin Section 4. We then report quantitative results of the Sign\nLanguage Transformers in Section 5 and present new base-\nline results for the previously deÔ¨Åned text-to-text translation\ntasks [9]. In Section 6, we share translation examples gen-\nerated by our network to give the reader further qualitative\ninsight of how our approach performs. We conclude the\npaper in Section 7 by discussing our Ô¨Åndings and possible\nfuture work.\n2. Related Work\nSign languages have been studied by the computer\nvision community for the last three decades [65, 56]. The\nend goal of computational sign language research is to\nbuild translation and production systems [16], that are\ncapable of translating sign language videos to spoken\nlanguage sentences and vice versa, to ease the daily lives\nof the Deaf [15, 6]. However, most of the research to date\nhas mainly focused on Isolated Sign Language Recognition\n[35, 75, 72, 10, 63, 67], working on application speciÔ¨Åc\ndatasets [11, 71, 23], thus limiting the applicability of such\ntechnologies. More recent work has tackled continuous\ndata [42, 32, 17, 18], but the move from recognition to\ntranslation is still in its infancy [9].\nThere have been earlier attempts to realize SLT by com-\nputational linguists. However, existing work has solely fo-\ncused on the text-to-text translation problem and has been\nvery limited in size, averaging around 3000 total words\n[46, 57, 54]. Using statistical machine translation meth-\nods, Stein et al. [57] proposed a weather broadcast trans-\nlation system from spoken German into German Sign Lan-\nguage - Deutsche Gebrdensprache (DGS) and vice versa,\nusing the RWTH-PHOENIX-Weather-2012 (PHOENIX12)\n[25] dataset. Another method translated air travel infor-\nmation from spoken English to Irish Sign Language (ISL),\nspoken German to ISL, spoken English to DGS, and spo-\nken German to DGS [45]. Ebling [22] developed an ap-\nproach to translate written German train announcements\ninto Swiss German Sign Language - Deutschschweizer\nGeb¬®ardensprache (DSGS). While non-manual information\nhas not been included in most previous systems, Ebling &\nHuenerfauth [24] proposed a sequence classiÔ¨Åcation based\nmodel to schedule the automatic generation of non-manual\nfeatures after the core machine translation step.\nConceptual video based SLT systems were introduced in\nthe early 2000s [7]. There have been studies, such as [12],\nwhich propose recognizing signs in isolation and then con-\nstructing sentences using a language model. However, end-\nto-end SLT from video has not been realized until recently.\nThe most important obstacle to vision based SLT re-\nsearch has been the availability of suitable datasets. Cu-\nrating and annotating continuous sign language videos with\nspoken language translations is a laborious task. There\nare datasets available from linguistic sources [53, 31] and\nsign language interpretations from broadcasts [14]. How-\never, the available annotations are either weak (subtitles) or\ntoo few to build models which would work on a large do-\nmain of discourse. In addition, such datasets lack the hu-\nman pose information which legacy Sign Language Recog-\nnition (SLR) methods heavily relied on.\nThe relationship between sign sentences and their spo-\nken language translations are non-monotonic, as they have\ndifferent ordering. Also, sign glosses and linguistic con-\nstructs do not necessarily have a one-to-one mapping with\ntheir spoken language counterparts. This made the use of\navailable CSLR methods [42, 41] (that were designed to\nlearn from weakly annotated data) infeasible, as they are\nbuild on the assumption that sign language videos and cor-\nresponding annotations share the same temporal order.\nTo address these issues, Camgoz et al . [9] released\nthe Ô¨Årst publicly available SLT dataset, PHOENIX14 T,\nwhich is an extension of the popular RWTH-PHOENIX-\nWeather-2014 (PHOENIX14) CSLR dataset. The authors\napproached the task as a spatio-temporal neural machine\ntranslation problem, which they term ‚ÄòNeural Sign Lan-\nguage Translation‚Äô. They proposed a system using Con-\nvolutional Neural Networks (CNNs) in combination with\nattention-based NMT methods [44, 3] to realize the Ô¨Årst\nend-to-end SLT models. Following this, Ko et al . pro-\nposed a similar approach but used body key-point coordi-\nnates as input for their translation networks, and evaluated\ntheir method on a Korean Sign Language dataset [38].\nConcurrently, there have been several advancements in\nthe Ô¨Åeld of NMT, one of the most important being the intro-\nduction of transformer networks [70]. Transformers dras-\ntically improved the translation performance over legacy\nattention based encoder-decoder approaches. Also due to\nthe fully-connected nature of the architecture, transformers\nare fast and easy to parallelize, which has enabled them\nto become the new go to architecture for many machine\ntranslation tasks. In addition to NMT, transformers have\nachieved success in various other challenging tasks, such as\nlanguage modelling [19, 77], learning sentence representa-\ntions [21], multi-modal language understanding [68], activ-\nity [73] and speech recognition [34]. Inspired by their re-\ncent wide-spread success, in this work we propose a novel\narchitecture where multiple co-dependent transformer net-\nworks are simultaneously trained to jointly solve related\ntasks. We then apply this architecture to the problem of si-\nmultaneous recognition and translation where joint training\nprovides signiÔ¨Åcant beneÔ¨Åts.\n3. Sign Language Transformers\nIn this section we introduce Sign Language Transform-\ners which jointly learn to recognize and translate sign video\nsequences into sign glosses and spoken language sentences\nin an end-to-end manner. Our objective is to learn the con-\nditional probabilities p(G|V) and p(S|V) of generating a\nsign gloss sequence G= (g1,...,g N) with N glosses and a\nspoken language sentence S= (w1,...,w U) with U words\ngiven a sign video V= (I1,...,I T) with T frames.\nModelling these conditional probabilities is a sequence-\nto-sequence task, and poses several challenges. In both\ncases, the number of tokens in the source domain is much\nlarger than the corresponding target sequence lengths ( i.e.\nT ‚â´N and T ‚â´U). Furthermore, the mapping between\nsign language videos, V, and spoken language sentences,S,\nis non-monotonic, as both languages have different vocabu-\nlaries, grammatical rules and orderings.\nPrevious sequence-to-sequence based literature on SLT\ncan be categorized into two groups: The Ô¨Årst group break\ndown the problem in two stages. They consider CSLR as an\nConnectionist Temporal Classification\nLinear Linear Linear\nCNNCNNCNN\nSelf-Attention\nFF FF FF\nAdd & Normalize\nLinearLinearLinear\n(Masked) Self-Attention\nAdd & Normalize\nFF FF FF\nEncoder-Decoder Attention\nAdd & Normalize\nSoftmax Softmax Softmax\nAdd & Normalize\nLinear Linear Linear\nSoftmax Softmax Softmax\nAdd & Normalize\nSE WE\nSLRT\nSLTT\nùêºùëáùêºùë°ùêº1\nùëìùëáùëìùë°\nùëù(ùëîùëõ) ùëù(ùëîùëÅ)ùëù(ùëî1)\n·àòùëìùë° ·àòùëìùëá\nùëì1\n·àòùëì1\nPE(1) PE(ùë°) PE(ùëá) PE(1) PE(ùë¢) PE(ùëà)\nùëßùë° ùëßùëáùëß1\nùëù(ùë§ùë¢+1) ùëù(< ùëíùëúùë† >)ùëù(ùë§1)\n‚Ñéùë¢ ‚Ñéùëà‚Ñé0\nùë§ùëàùë§ùë¢< ùëèùëúùë† >\nùëöùë¢ ùëöùëàùëö0\n‡∑ùùëöùë¢ ‡∑ùùëöùëà‡∑ùùëö0\nFigure 2: A detailed overview of a single layered Sign Language Transformer.\n(SE: Spatial Embedding, WE: Word Embedding , PE: Positional Encoding, FF: Feed Forward)\ninitial process and then try to solve the problem as a text-to-\ntext translation task [12, 9]. Camgoz et al. utilized a state-\nof-the-art CSLR method [41] to obtain sign glosses, and\nthen used an attention-based text-to-text NMT model [44] to\nlearn the sign gloss to spoken language sentence translation,\np(S|G) [9]. However, in doing so, this approach introduces\nan information bottleneck in the mid-level gloss represen-\ntation. This limits the network‚Äôs ability to understand sign\nlanguage as the translation model can only be as good as the\nsign gloss annotations it was trained from. There is also an\ninherent loss of information as a sign gloss is an incomplete\nannotation intended only for linguistic study and it therefore\nneglects many crucial details and information present in the\noriginal sign language video.\nThe second group of methods focus on translation from\nthe sign video representations to spoken language with no\nintermediate representation [9, 38]. These approaches at-\ntempt to learn p(S|V) directly. Given enough data and a\nsufÔ¨Åciently sophisticated network architecture, such mod-\nels could theoretically realize end-to-end SLT with no need\nfor a human-interpretable information that act as a bottle-\nneck. However, due to the lack of direct supervision guid-\ning sign language understanding, such methods have signif-\nicantly lower performance than their counterparts on cur-\nrently available datasets [9].\nTo address this, we propose to jointly learn p(G|V) and\np(S|V), in an end-to-end manner. We build upon trans-\nformer networks [70] to create a uniÔ¨Åed model, which\nwe call Sign Language Transformers (See Figure 2). We\ntrain our networks to generate spoken language sentences\nfrom sign language video representations. During training,\nwe inject intermediate gloss supervision in the form of a\nCTC loss into the Sign Language Recognition Transformer\n(SLRT) encoder. This helps our networks learn more mean-\ningful spatio-temporal representations of the sign without\nlimiting the information passed to the decoder. We em-\nploy an autoregressive Sign Language Translation Trans-\nformer (SLTT) decoder which predicts one word at a time\nto generate the spoken language sentence translation.\n3.1. Spatial and Word Embeddings\nFollowing the classic NMT pipeline, we start by embed-\nding our source and target tokens, namely sign language\nvideo frames and spoken language words. As word embed-\nding we use a linear layer, which is initialized from scratch\nduring training, to project a one-hot-vector representation of\nthe words into a denser space. To embed video frames, we\nuse the SpatialEmbedding approach [9], and propagate each\nimage through CNNs. We formulate these operations as:\nmu = WordEmbedding(wu)\nft = SpatialEmbedding(It) (1)\nwhere mu is the embedded representation of the spoken\nlanguage word wu and ft corresponds to the non-linear\nframe level spatial representation obtained from a CNN.\nUnlike other sequence-to-sequence models [61, 27],\ntransformer networks do not employ recurrence or convo-\nlutions, thus lacking the positional information within se-\nquences. To address this issue we follow the positional en-\ncoding method proposed in [70] and add temporal ordering\ninformation to our embedded representations as:\nÀÜft = ft + PositionalEncoding(t)\nÀÜmu = mu + PositionalEncoding(u)\nwhere PositionalEncoding is a predeÔ¨Åned function which\nproduces a unique vector in the form of a phase shifted sine\nwave for each time step.\n3.2. Sign Language Recognition Transformers\nThe aim of SLRT is to recognize glosses from continu-\nous sign language videos while learning meaningful spatio-\ntemporal representations for the end goal of sign language\ntranslation. Using the positionally encoded spatial embed-\ndings, ÀÜf1:T, we train a transformer encoder model [70].\nThe inputs to SLRT are Ô¨Årst modelled by a Self-\nAttention layer which learns the contextual relationship be-\ntween the frame representations of a video. Outputs of the\nself-attention are then passed through a non-linear point-\nwise feed forward layer. All the operations are followed by\nresidual connections and normalization to help training. We\nformulate this encoding process as:\nzt = SLRT(ÀÜft|ÀÜf1:T) (2)\nwhere zt denotes the spatio-temporal representation of the\nframe It, which is generated by SLRT at time step t, given\nthe spatial representations of all of the video frames, ÀÜf1:T.\nWe inject intermediate supervision to help our networks\nunderstand sign and to guide them to learn a meaningful\nsign representation which helps with the main task of trans-\nlation. We train the SLRT to modelp(G|V) and predict sign\nglosses. Due to the spatio-temporal nature of the signs,\nglosses have a one-to-many mapping to video frames but\nshare the same ordering.\nOne way to train the SLRT would be using cross-entropy\nloss [29] with frame level annotations. However, sign\ngloss annotations with such precision are rare. An alter-\nnative form of weaker supervision is to use a sequence-to-\nsequence learning loss functions, such as CTC [30].\nGiven spatio-temporal representations, z1:T, we obtain\nframe level gloss probabilities, p(gt|V), using a linear pro-\njection layer followed by a softmax activation. We then use\nCTC to compute p(G|V) by marginalizing over all possible\nVto Galignments as:\np(G|V) =\n‚àë\nœÄ‚ààB\np(œÄ|V) (3)\nwhere œÄ is a path and Bare the set of all viable paths that\ncorrespond to G. We then use the p(G|V) to calculate the\nCSLR loss as:\nLR = 1‚àíp(G‚àó|V) (4)\nwhere G‚àóis the ground truth gloss sequence.\n3.3. Sign Language Translation Transformers\nThe end goal of our approach is to generate spoken lan-\nguage sentences from sign video representations. We pro-\npose training an autoregressive transformer decoder model,\nnamed SLTT, which exploits the spatio-temporal represen-\ntations learned by the SLRT. We start by preÔ¨Åxing the target\nspoken language sentence Swith the special beginning of\nsentence token, <bos >. We then extract the positionally\nencoded word embeddings. These embeddings are passed\nto a masked self-attention layer. Although the main idea be-\nhind self-attention is the same as in SLRT, the SLTT utilizes\na mask over the self-attention layer inputs. This ensures that\neach token may only use its predecessors while extracting\ncontextual information. This masking operation is neces-\nsary, as at inference time the SLTT won‚Äôt have access to the\noutput tokens which would follow the token currently being\ndecoded.\nRepresentations extracted from both SLRT and SLTT\nself-attention layers are combined and given to an encoder-\ndecoder attention module which learns the mapping\nbetween source and target sequences. Outputs of the\nencoder-decoder attention are then passed through a\nnon-linear point-wise feed forward layer. Similar to SLRT,\nall the operations are followed by residual connections and\nnormalization. We formulate this decoding process as:\nhu+1 = SLTT( ÀÜmu|ÀÜm1:u‚àí1,z1:T). (5)\nSLTT learns to generate one word at a time until it produces\nthe special end of sentence token, <eos >. It is trained\nby decomposing the sequence level conditional probability\np(S|V) into ordered conditional probabilities\np(S|V) =\nU‚àè\nu=1\np(wu|hu) (6)\nwhich are used to calculate the cross-entropy loss for each\nword as:\nLT = 1‚àí\nU‚àè\nu=1\nD‚àë\nd=1\np( ÀÜwd\nu)p(wd\nu|hu) (7)\nwhere p( ÀÜwd\nu) represents the ground truth probability of\nword wd at decoding step u and D is the target language\nvocabulary size.\nWe train our networks by minimizing the joint loss term\nL, which is a weighted sum of the recognition loss LR and\nthe translation loss LT as:\nL= ŒªRLR + ŒªTLT (8)\nwhere ŒªR and ŒªT are hyper parameters which decides the\nimportance of each loss function during training and are\nevaluated in Section 5.\n4. Dataset and Translation Protocols\nWe evaluate our approach on the recently released\nPHOENIX14T dataset [9], which is a large vocabulary, con-\ntinuous SLT corpus. PHOENIX14T is a translation focused\nextension of the PHOENIX14 corpus, which has become\nthe primary benchmark for CSLR in recent years.\nPHOENIX14T contains parallel sign language videos,\ngloss annotations and their translations, which makes it the\nonly available dataset suitable for training and evaluating\njoint SLR and SLT techniques. The corpus includes uncon-\nstrained continuous sign language from 9 different signers\nwith a vocabulary of 1066 different signs. Translations for\nthese videos are provided in German spoken language with\na vocabulary of 2887 different words.\nThe evaluation protocols on the PHOENIX14 T dataset,\nas laid down by [9], are as follows:\nSign2Text is the end goal of SLT, where the objective\nis to translate directly from continuous sign videos to spo-\nken language sentences without going via any intermediary\nrepresentation, such as glosses.\nGloss2Text is a text-to-text translation problem, where\nthe objective is to translate ground truth sign gloss se-\nquences to German spoken language sentences. The re-\nsults of these experiments act as a virtual upper bound for\nthe available NMT translation technology. This assump-\ntion is based on the fact that perfect sign language recog-\nnition/understanding is simulated by using the ground truth\ngloss annotation. However, as mentioned earlier, one needs\nto bear in mind that gloss representations are imprecise.\nAs glosses are textual representations of multi-channel tem-\nporal signals, they represent an information bottleneck for\nany translation system. This means that under ideal con-\nditions, a Sign2Text system could and should outperform\nGloss2Text. However, more sophisticated network architec-\ntures and data are needed to achieve this and hence such a\ngoal remains a longer term objective beyond the scope of\nthis manuscript.\nSign2Gloss2Text is the current state-of-the-art in SLT.\nThis approach utilizes CSLR models to extract gloss se-\nquences from sign language videos which are then used to\nsolve the translation task as a text-to-text problem by train-\ning a Gloss2Text network using the CSLR predictions.\nSign2Gloss‚ÜíGloss2Text is similar to Sign2Gloss2Text\nand also uses CSLR models to extract gloss sequences.\nHowever, instead of training text-to-text translation net-\nworks from scratch, Sign2Gloss‚ÜíGloss2Text models use\nthe best performing Gloss2Text network, which has been\ntrained with ground truth gloss annotations, to generate spo-\nken language sentences from intermediate sign gloss se-\nquences from the output of the CSLR models.\nIn addition to evaluating our networks in the context\nof the above protocols, we additionally introduce two\nnew protocols which follow the same naming convention.\nSign2Gloss is a protocol which essentially performs CSLR,\nwhile Sign2(Gloss+Text) requires joint learning of contin-\nuous sign language recognition and translation.\n5. Quantitative Results\nIn this section we share our sign language recognition\nand translation experimental setups and report quantitative\nresults. We Ô¨Årst go over the implementation details and\nintroduce the evaluation metrics we will be using to mea-\nsure the performance of our models. We start our exper-\niments by applying transformer networks to the text-to-\ntext based SLT tasks, namely Gloss2Text, Sign2Gloss2Text,\nSign2Gloss‚ÜíGloss2Text and report improved performance\nover using Recurrent Neural Network (RNN) based models.\nWe share our Sign2Gloss experiments, in which we explore\nthe effects of different types of spatial embeddings and net-\nwork structures on the performance of CSLR. We then train\nSign2Textand Sign2(Gloss+Text)models using the best per-\nforming Sign2Gloss conÔ¨Åguration and investigate the effect\nof different recognition loss weights on the joint recogni-\ntion and translation performance. Finally, we compare our\nbest performing models against other approaches and report\nstate-of-the-art results.\n5.1. Implementation and Evaluation Details\nFramework: We used a modiÔ¨Åed version of JoeyNMT\n[43] to implement our Sign Language Transformers 3. All\ncomponents of our network were built using the PyTorch\nframework [50], except the CTC beam search decoding, for\nwhich we utilized the TensorFlow implementation [1].\nNetwork Details: Our transformers are built using 512\nhidden units and 8 heads in each layer. We use Xavier ini-\ntialization [28] and train all of our networks from scratch.\nWe also utilize dropout with 0.1 drop rate on transformer\nlayers and word embeddings to mitigate over-Ô¨Åtting.\nPerformance Metrics:We use Word Error Rate (WER)\nfor assessing our recognition models, as it is the prevalent\nmetric for evaluating CSLR performance [40]. To measure\nthe translation performance of our networks, we utilized\nBLEU [49] score (n-grams ranging from 1 to 4), which is\nthe most common metric for machine translation.\nTraining: We used the Adam [37] optimizer to train our\nnetworks using a batch size of 32 with a learning rate of\n10‚àí3 (Œ≤1=0.9, Œ≤2=0.998) and a weight decay of 10‚àí3. We\nutilize plateau learning rate scheduling which tracks the de-\nvelopment set performance. We evaluate our network every\n100 iterations. If the development score does not decrease\nfor 8 evaluation steps, we decrease the learning rate by a\nfactor of 0.7. This continues until the learning rate drops\nbelow 10‚àí6.\nDecoding: During the training and validation steps we\nemploy a greedy search to decode both gloss sequences and\nspoken language sentences. At inference time, we utilize\nbeam search decoding with widths ranging from0 to 10. We\nalso implement a length penalty [74] with Œ±values ranging\nfrom 0 to 2. We Ô¨Ånd the best performing combination of\nbeam width and Œ± on the development set and use these\nvalues for the test set evaluation.\n5.2. Text-to-Text Sign Language Translation\nIn our Ô¨Årst set of experiments, we adapt the trans-\nformer backbone of our technique, for text-to-text sign lan-\nguage translation. We then evaluate the performance gain\nachieved over the RNN-based attention architectures.\nAs can be seen in Table 1, utilizing transformers for\ntext-to-text sign language translation improved the perfor-\nmance across all tasks, reaching an impressive 25.35/24.54\nBLEU-4 score on the development and test sets. We believe\nthis performance gain is due to the more sophisticated at-\ntention architectures, namely self-attention modules, which\nlearn the contextual information within both source and tar-\nget sequences.\n3https://github.com/neccam/slt\nDEV TEST\nText-to-Text Tasks (RNNs vs Transformers)WER BLEU-1 BLEU-2 BLEU-3 BLEU-4WER BLEU-1 BLEU-2 BLEU-3 BLEU-4\nGloss2Text [9] - 44.40 31.83 24.61 20.16 - 44.13 31.47 23.89 19.26\nOur Gloss2Text- 50.69 38.16 30.53 25.35 - 48.90 36.88 29.45 24.54\nSign2Gloss2Text [9]- 42.88 30.30 23.02 18.40 - 43.29 30.39 22.82 18.13\nOur Sign2Gloss2Text- 47.73 34.82 27.11 22.11 - 48.47 35.35 27.57 22.45\nSign2Gloss‚ÜíGloss2Text [9] - 41.08 29.10 22.16 17.86 - 41.54 29.52 22.24 17.79\nOur Sign2Gloss‚ÜíGloss2Text - 47.84 34.65 26.88 21.84 - 47.74 34.37 26.55 21.59\nVideo-to-Text TasksWER BLEU-1 BLEU-2 BLEU-3 BLEU-4WER BLEU-1 BLEU-2 BLEU-3 BLEU-4\nCNN+LSTM+HMM [39]24.50 - - - - 26.50 - - - -\nOur Sign2Gloss24.88 - - - - 24.59 - - - -\nSign2Text [9] - 31.87 19.11 13.16 9.94 - 32.24 19.03 12.83 9.58\nOur Sign2Text- 45.54 32.60 25.30 20.69 - 45.34 32.31 24.83 20.17\nOur Best Recog. Sign2(Gloss+Text)24.61 46.56 34.03 26.83 22.12 24.49 47.20 34.46 26.75 21.80\nOur Best Trans. Sign2(Gloss+Text)24.98 47.26 34.40 27.05 22.38 26.16 46.61 33.73 26.19 21.32\nTable 1: (Top) New baseline results for text-to-text tasks on Phoenix2014T [9] using transformer networks and\n(Bottom) Our best performing Sign Language Transformers compared against the state-of-the-art.\n5.3. Sign2Gloss\nTo tackle the Sign2Gloss task, we utilize our SLRT net-\nworks. Any CNN architecture can be used as spatial em-\nbedding layers to learn the sign language video frame rep-\nresentation while training SLRT in an end-to-end manner.\nHowever, due to hardware limitations (graphics card mem-\nory) we utilize pretrained CNNs as our spatial embeddings.\nWe extract frame level representations from sign videos and\ntrain our sign language transformers to learn CSLR and SLT\njointly in an end-to-end manner.\nIn our Ô¨Årst set of experiments, we investigate which CNN\nwe should be using to represent our sign videos. We utilize\nstate-of-the-art EfÔ¨ÅcientNets [66], namely B0, B4 and B7,\nwhich were trained on ImageNet [20]. We also use an In-\nception [64] network which was pretrained for learning sign\nlanguage recognition in a CNN+LSTM+HMM setup [39].\nIn this set of experiments we employed a two layered trans-\nformer encoder model.\nTable 2 shows that as the spatial embedding layer be-\ncomes more advanced, i.e. B0 vs B7, the recognition per-\nformance increases. However, our networks beneÔ¨Åted more\nwhen we used pretrained features, as these networks had\nseen sign videos before and learned kernels which can em-\nbed more meaningful representations in the latent space.\nWe then tried utilizing Batch Normalization [33] followed\nby a ReLU [47] to normalize our inputs and allow our\nnetworks to learn more abstract non-linear representations.\nThis improved our results drastically, giving us a boost of\nnearly 7% and 6% of absolute WER reduction on the devel-\nopment and test sets, respectively. Considering these Ô¨Ånd-\nDEV TEST\nSpatial Embedding del / ins WER del / ins WER\nEfÔ¨ÅcientNet-B0 47.22 / 1.59 57.06 46.09 / 1.75 56.29\nEfÔ¨ÅcientNet-B4 40.73 / 2.45 51.26 38.34 / 2.80 50.09\nEfÔ¨ÅcientNet-B7 39.29 / 2.84 50.18 37.05 / 2.76 47.96\nPretrained CNN 21.51 / 6.10 33.90 20.29 / 5.35 33.39\n+ BN & ReLU13.54 / 5.74 26.7013.85 / 6.43 27.62\nTable 2: Impact of the Spatial Embedding Layer variants.\nings, the rest of our experiments used the batch normalized\npretrained CNN features of [39] followed by ReLU.\nNext, we investigated the effects of having different\nnumbers of transformer layers. Although having a larger\nnumber of layers would allow our networks to learn more\nabstract representations, it also makes them prone to over-\nÔ¨Åtting. To this end, we built our SLRT networks using one\nto six layers and evaluate their CSLR performance.\nOur recognition performance initially improves with ad-\nditional layers (See Table 3). However, as we continue\nadding more layers, our networks started to over-Ô¨Åt on the\ntraining data, causing performance degradation. In the light\nof this, for the rest of our experiments, we constructed our\nsign language transformers using three layers.\n5.4. Sign2Text and Sign2(Gloss+Text)\nIn our next set of experiments we examine the per-\nformance gain achieved by unifying the recognition and\ntranslation tasks into a single model. As a baseline, we\ntrained a Sign2Text network by setting our recognition loss\nweight ŒªR to zero. We then jointly train our sign language\ntransformers, for recognition and translation, with various\nweightings between the losses.\nAs can be seen in Table 4, jointly learning recogni-\ntion and translation with equal weighting (ŒªR=ŒªT=1.0) im-\nproves the translation performance, while degrading the\nrecognition performance compared to task speciÔ¨Åc net-\nworks. We believe this is due to scale differences of the\nCTC and word-level cross entropy losses. Increasing the\nrecognition loss weight improved both the recognition and\nDEV TEST\n# Layers del/ins WER del/ins WER\n1 11.72 / 9.02 28.08 11.20 / 10.57 29.90\n2 13.54 / 5.74 26.70 13.85 / 6.43 27.62\n3 11.68 / 6.48 24.88 11.16 / 6.09 24.59\n4 12.55 / 5.87 24.97 13.48 / 6.02 26.87\n5 11.94 / 6.12 25.23 11.81 / 6.12 25.51\n6 15.01 / 6.11 27.46 14.30 / 6.28 27.78\nTable 3: Impact of different numbers of layers\nLoss Weights DEV TEST\nŒªR ŒªT WER BLEU-4 WER BLEU-4\n1.0 0.0 24.88 - 24.59 -\n0.0 1.0 - 20.69 - 20.17\n1.0 1.0 35.13 21.73 33.75 21.22\n2.5 1.0 26.99 22.11 27.55 21.37\n5.0 1.0 24.61 22.12 24.49 21.80\n10.0 1.0 24.98 22.38 26.16 21.32\n20.0 1.0 25.87 20.90 25.73 20.93\nTable 4: Training Sign Language Transformers to jointly\nlearn recognition and translation with different weight on\nrecognition loss.\nthe translation performance, demonstrating the value of\nsharing training between these related tasks.\nCompared to previously published methods, our Sign\nLanguage Transformers surpass both their recognition and\ntranslation performance (See Table 1). We report a decrease\nof 2% WER over [39] on the test set in both Sign2Gloss\nand Sign2(Gloss+Text)setups. More impressively, both our\nSign2Textand Sign2(Gloss+Text)networks doubled the pre-\nvious state-of-the-art translation results (9.58 vs. 20.17 and\n21.32 BLEU-4, respectively). Furthermore, our best per-\nforming translation Sign2(Gloss+Text) outperforms Cam-\ngoz et al .‚Äôs text-to-text based Gloss2Text translation per-\nformance (19.26 vs 21.32 BLEU-4), which was previously\nproposed as a pseudo upper bound on performance in [9].\nThis supports our claim that given more sophisticated net-\nwork architectures, one would and should achieve better\nperformance translating directly from video representations\nrather than doing text-to-text translation through a limited\ngloss representation.\n6. Qualitative Results\nIn this section we report our qualitative results. We\nshare the spoken language translations generated by our\nbest performing Sign2(Gloss+Text) model given sign video\nrepresentations (See Table 5) 4. As the annotations in the\nPHOENIX14T dataset are in German, we share both the\nproduced sentences and their translations in English.\nOverall, the quality of the translations is good, and even\nwhere the exact wording differs, it conveys the same infor-\nmation. The most difÔ¨Åcult translations seem to be named\nentities like locations which occur in limited contexts in the\ntraining data. SpeciÔ¨Åc numbers are also challenging as there\nis no grammatical context to distinguish one from another.\nDespite this, the sentences produced follow standard gram-\nmar with surprisingly few exceptions.\n7. Conclusion and Future Work\nSign language recognition and understanding is an es-\nsential part of the sign language translation task. Previous\ntranslation approaches relied heavily on recognition as the\ninitial step of their system. In this paper we proposed Sign\nLanguage Transformers, a novel transformer based archi-\ntecture to jointly learn sign language recognition and trans-\n4Visit our code repository for further qualitative examples.\nReference: im sden schwacher wind .( in the south gentle wind . )Ours: der wind weht im sden schwach .( the wind blows gentle in the south . )Reference: hnliches wetter dann auch am donnerstag .( similar weather then also on thursday . )Ours: hnliches wetter auch am donnerstag .( similar weather also on thursday . )Reference: ganz hnliche temperaturen wie heute zwischen sechs und elf grad .( quite similar temperatures as today between six and eleven degrees . )Ours: hnlich wie heute nacht das sechs bis elf grad .( similar as today at night that six to eleven degrees . )Reference: heute nacht neunzehn bis fnfzehn grad im sdosten bis zwlf grad .( tonight nineteen till Ô¨Åfteen degrees in the southeast till twelve degrees . )Ours: heute nacht werte zwischen neun und fnfzehn grad im sdosten bis zwlf grad .( tonight values between nine and Ô¨Åfteen degrees in the southeast till twelve degrees . )Reference: am sonntag im norden und in der mitte schauer dabei ist es im norden strmisch .( on sunday in the north and center shower while it is stormy in the north . )Ours: am sonntag im norden und in der mitte niederschlge im norden ist es weiter strmisch .( on sunday in the north and center rainfall in the north it is continuously stormy . )Reference: im sden und sdwesten gebietsweise regen sonst recht freundlich .( in the south and southwest partly rain otherwise quite friendly . )Ours: im sdwesten regnet es zum teil krftig .( in the southwest partly heavy rain . )Reference: in der nacht sinken die temperaturen auf vierzehn bis sieben grad .( at night the temperatures lower till fourteen to seven degrees . )Ours: heute nacht werte zwischen sieben und sieben grad .( tonight values between seven and seven degrees . )Reference: heute nacht ist es meist stark bewlkt rtlich regnet oder nieselt es etwas .( tonight it is mostly cloudy locally rain or drizzle . )Ours: heute nacht ist es verbreitet wolkenverhangen gebietsweise regnet es krftig .(tonight it is widespread covered with clouds partly strong rain . )Reference: an der saar heute nacht milde sechzehn an der elbe teilweise nur acht grad .( at the saar river tonight mild sixteen at the elbe river partly only eight degrees . )Ours: im rhein und sdwesten macht sich morgen nur knapp ber null grad .( in the rhine river and south west becomes just above zero degrees . )\nTable 5: Generated spoken language translations by our\nSign Language Transformers.\nlation in an end-to-end manner. We utilized CTC loss to\ninject gloss level supervision into the transformer encoder,\ntraining it to do sign language recognition while learning\nmeaningful representations for the end goal of sign lan-\nguage translation, without having an explicit gloss repre-\nsentation as an information bottleneck.\nWe evaluated our approach on the challenging\nPHOENIX14T dataset and report state-of-the-art sign\nlanguage recognition and translation results, in some\ncases doubling the performance of previous translation\napproaches. Our Ô¨Årst set of experiments have shown\nthat using features which were pretrained on sign data\noutperformed using generic ImageNet based spatial rep-\nresentations. Furthermore, we have shown that jointly\nlearning recognition and translation improved the per-\nformance across both tasks. More importantly, we have\nsurpassed the text-to-text translation results, which was set\nas a virtual upper-bound, by directly translating spoken\nlanguage sentences from video representations.\nAs future work, we would like to expand our approach to\nmodel multiple sign articulators, namely faces, hands and\nbody, individually to encourage our networks to learn the\nlinguistic relationship between them.\n8. Acknowledgements\nThis work received funding from the SNSF Sinergia project\n‚ÄòSMILE‚Äô (CRSII2 160811), the European Union‚Äôs Horizon2020\nresearch and innovation programme under grant agreement\nno. 762021 ‚ÄòContent4All‚Äô and the EPSRC project ‚ÄòExTOL‚Äô\n(EP/R03298X/1). This work reÔ¨Çects only the authors view and the\nCommission is not responsible for any use that may be made of\nthe information it contains. We would also like to thank NVIDIA\nCorporation for their GPU grant.\nReferences\n[1] Mart ¬¥ƒ±n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen,\nAndy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghe-\nmawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur,\nJosh Levenberg, Rajat Monga, Sherry Moore, Derek G. Mur-\nray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete War-\nden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Ten-\nsorÔ¨Çow: A System for Large-scale Machine Learning. In\nProceedings of the 12th Symposium on Operating Systems\nDesign and Implementation, 2016.\n[2] Triantafyllos Afouras, Joon Son Chung, Andrew Senior,\nOriol Vinyals, and Andrew Zisserman. Deep Audio-visual\nSpeech Recognition. IEEE Transactions on Pattern Analysis\nand Machine Intelligence (TPAMI), 2018.\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.\nNeural Machine Translation by Jointly Learning to Align and\nTranslate. In Proceedings of the International Conference on\nLearning Representations (ICLR), 2015.\n[4] Mark Borg and Kenneth P Camilleri. Sign Language De-\ntection in the Wild with Recurrent Neural Networks. In Pro-\nceedings of the IEEE International Conference on Acoustics,\nSpeech, and Signal Processing (ICASSP), 2019.\n[5] Penny Boyes-Braem and Rachel Sutton-Spence. The Hands\nare the Head of the Mouth: The Mouth as Articulator in Sign\nLanguages. Gallaudet University Press, 2001.\n[6] Danielle Bragg, Oscar Koller, Mary Bellard, Larwan Berke,\nPatrick Boudrealt, Annelies Braffort, Naomi Caselli, Matt\nHuenerfauth, Hernisa Kacorri, Tessa Verhoef, Christian\nV ogler, and Meredith Ringel Morris. Sign Language Recog-\nnition, Generation, and Translation: An Interdisciplinary\nPerspective. In Proceedings of the International ACM\nSIGACCESS Conference on Computers and Accessibility\n(ASSETS), 2019.\n[7] Jan Bungeroth and Hermann Ney. Statistical Sign Language\nTranslation. In Proceedings of the Workshop on Represen-\ntation and Processing of Sign Languages at International\nConference on Language Resources and Evaluation (LREC),\n2004.\n[8] Necati Cihan Camgoz, Simon HadÔ¨Åeld, Oscar Koller, and\nRichard Bowden. SubUNets: End-to-end Hand Shape and\nContinuous Sign Language Recognition. In Proceedings\nof the IEEE International Conference on Computer Vision\n(ICCV), 2017.\n[9] Necati Cihan Camgoz, Simon HadÔ¨Åeld, Oscar Koller, Her-\nmann Ney, and Richard Bowden. Neural Sign Language\nTranslation. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2018.\n[10] Necati Cihan Camgoz, Ahmet Alp Kindiroglu, and Lale\nAkarun. Sign Language Recognition for Assisting the Deaf\nin Hospitals. In Proceedings of the International Workshop\non Human Behavior Understanding (HBU), 2016.\n[11] Necati Cihan Camgoz, Ahmet Alp Kindiroglu, Serpil\nKarabuklu, Meltem Kelepir, Ayse Sumru Ozsoy, and Lale\nAkarun. BosphorusSign: A Turkish Sign Language Recog-\nnition Corpus in Health and Finance Domains. In Proceed-\nings of the International Conference on Language Resources\nand Evaluation (LREC), 2016.\n[12] Xiujuan Chai, Guang Li, Yushun Lin, Zhihao Xu, Yili Tang,\nXilin Chen, and Ming Zhou. Sign Language Recognition\nand Translation with Kinect. In Proceedings of the Interna-\ntional Conference on Automatic Face and Gesture Recogni-\ntion (FG), 2013.\n[13] Neva Cherniavsky, Richard E Ladner, and Eve A Riskin. Ac-\ntivity Detection in Conversational Sign Language Video for\nMobile Telecommunication. In Proceedings of the Interna-\ntional Conference on Automatic Face and Gesture Recogni-\ntion (FG), 2008.\n[14] Helen Cooper and Richard Bowden. Learning Signs from\nSubtitles: A Weakly Supervised Approach to Sign Language\nRecognition. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2009.\n[15] Helen Cooper, Brian Holt, and Richard Bowden. Sign Lan-\nguage Recognition. In Visual Analysis of Humans. Springer,\n2011.\n[16] Kearsy Cormier, Neil Fox, Bencie Woll, Andrew Zisserman,\nNecati Cihan Camgoz, and Richard Bowden. ExTOL: Au-\ntomatic recognition of British Sign Language using the BSL\nCorpus. In Proceedings of the Sign Language Translation\nand Avatar Technology (SLTAT), 2019.\n[17] Runpeng Cui, Hu Liu, and Changshui Zhang. Recurrent\nConvolutional Neural Networks for Continuous Sign Lan-\nguage Recognition by Staged Optimization. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2017.\n[18] Runpeng Cui, Hu Liu, and Changshui Zhang. A Deep Neural\nFramework for Continuous Sign Language Recognition by\nIterative Training. IEEE Transactions on Multimedia, 2019.\n[19] Zihang Dai, Zhilin Yang, Yiming Yang, William W Co-\nhen, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdi-\nnov. Transformer-XL: Attentive Language Models beyond a\nFixed-length Context. In Proceedings of the Annual Meet-\ning of the Association for Computational Linguistics (ACL),\n2019.\n[20] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and\nFei-Fei Li. ImageNet: A Large-scale Hierarchical Image\nDatabase. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2009.\n[21] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of Deep Bidirectional Trans-\nformers for Language Understanding. In Proceedings of the\nConference of the North American Chapter of the Associa-\ntion for Computational Linguistics (ACL): Human Language\nTechnologies, 2019.\n[22] Sarah Ebling. Automatic Translation from German to Syn-\nthesized Swiss German Sign Language. PhD thesis, Univer-\nsity of Zurich, 2016.\n[23] Sarah Ebling, Necati Cihan Camgoz, Penny Boyes Braem,\nKatja Tissi, Sandra Sidler-Miserez, Stephanie Stoll, Simon\nHadÔ¨Åeld, Tobias Haug, Richard Bowden, Sandrine Tornay,\nMarzieh Razavi, and Mathew Magimai-Doss. SMILE Swiss\nGerman Sign Language Dataset. In Proceedings of the Inter-\nnational Conference on Language Resources and Evaluation\n(LREC), 2018.\n[24] Sarah Ebling and Matt Huenerfauth. Bridging the Gap be-\ntween Sign Language Machine Translation and Sign Lan-\nguage Animation using Sequence ClassiÔ¨Åcation. InProceed-\nings of the 6th Workshop on Speech and Language Process-\ning for Assistive Technologies (SPLAT), 2015.\n[25] Jens Forster, Christoph Schmidt, Thomas Hoyoux, Oscar\nKoller, Uwe Zelle, Justus H Piater, and Hermann Ney.\nRWTH-PHOENIX-Weather: A Large V ocabulary Sign Lan-\nguage Recognition and Translation Corpus. In Proceedings\nof the International Conference on Language Resources and\nEvaluation (LREC), 2012.\n[26] Jens Forster, Christoph Schmidt, Oscar Koller, Martin Bell-\ngardt, and Hermann Ney. Extensions of the Sign Language\nRecognition and Translation Corpus RWTH-PHOENIX-\nWeather. In Proceedings of the International Conference on\nLanguage Resources and Evaluation (LREC), 2014.\n[27] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats,\nand Yann N Dauphin. Convolutional Sequence to Sequence\nLearning. In Proceedings of the ACM International Confer-\nence on Machine Learning (ICML), 2017.\n[28] Xavier Glorot and Yoshua Bengio. Understanding the Dif-\nÔ¨Åculty of Training Deep Feedforward Neural Networks. In\nProceedings of the International Conference on ArtiÔ¨Åcial In-\ntelligence and Statistics, 2010.\n[29] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep\nLearning. MIT press, 2016.\n[30] Alex Graves, Santiago Fern ¬¥andez, Faustino Gomez, and\nJ¬®urgen Schmidhuber. Connectionist Temporal ClassiÔ¨Åcation:\nLabelling Unsegmented Sequence Data with Recurrent Neu-\nral Networks. In Proceedings of the ACM International Con-\nference on Machine Learning (ICML), 2006.\n[31] Thomas Hanke, Lutz K ¬®onig, Sven Wagner, and Silke\nMatthes. DGS Corpus & Dicta-Sign: The Hamburg Studio\nSetup. In Proceedings of the Representation and Processing\nof Sign Languages: Corpora and Sign Language Technolo-\ngies, 2010.\n[32] Jie Huang, Wengang Zhou, Qilin Zhang, Houqiang Li, and\nWeiping Li. Video-based Sign Language Recognition with-\nout Temporal Segmentation. In Proceedings of the AAAI\nConference on ArtiÔ¨Åcial Intelligence, 2018.\n[33] Sergey Ioffe and Christian Szegedy. Batch Normalization:\nAccelerating Deep Network Training by Reducing Internal\nCovariate Shift. In Proceedings of the International Confer-\nence on Machine Learning (ICML), 2015.\n[34] Kazuki Irie, Albert Zeyer, Ralf Schl ¬®uter, and Hermann Ney.\nLanguage Modeling with Deep Transformers. In 20th An-\nnual Conference of the International Speech Communication\nAssociation (INTERSPEECH), 2019.\n[35] Hamid Reza Vaezi Joze and Oscar Koller. MS-ASL: A\nLarge-Scale Data Set and Benchmark for Understanding\nAmerican Sign Language. In Proceedings of the British Ma-\nchine Vision Conference (BMVC), 2019.\n[36] Shujjat Khan, Donald G Bailey, and Gourab Sen Gupta.\nPause detection in continuous sign language. International\nJournal of Computer Applications in Technology, 50, 2014.\n[37] Diederik P. Kingma and Jimmy Ba. Adam: A Method for\nStochastic Optimization. In Proceedings of the International\nConference on Learning Representations (ICLR), 2014.\n[38] Sang-Ki Ko, Chang Jo Kim, Hyedong Jung, and Choongsang\nCho. Neural Sign Language Translation based on Human\nKeypoint Estimation. Applied Sciences, 9(13), 2019.\n[39] Oscar Koller, Necati Cihan Camgoz, Richard Bowden, and\nHermann Ney. Weakly Supervised Learning with Multi-\nStream CNN-LSTM-HMMs to Discover Sequential Paral-\nlelism in Sign Language Videos. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence (TPAMI), 2019.\n[40] Oscar Koller, Jens Forster, and Hermann Ney. Continu-\nous Sign Language Recognition: Towards Large V ocabulary\nStatistical Recognition Systems Handling Multiple Signers.\nComputer Vision and Image Understanding (CVIU) , 141,\n2015.\n[41] Oscar Koller, Sepehr Zargaran, and Hermann Ney. Re-Sign:\nRe-Aligned End-to-End Sequence Modelling with Deep Re-\ncurrent CNN-HMMs. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR) ,\n2017.\n[42] Oscar Koller, Sepehr Zargaran, Hermann Ney, and Richard\nBowden. Deep Sign: Hybrid CNN-HMM for Continuous\nSign Language Recognition. In Proceedings of the British\nMachine Vision Conference (BMVC), 2016.\n[43] Julia Kreutzer, Joost Bastings, and Stefan Riezler. Joey\nNMT: A minimalist NMT toolkit for novices. In Pro-\nceedings of the Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP): System Demonstra-\ntions, 2019.\n[44] Minh-Thang Luong, Hieu Pham, and Christopher D Man-\nning. Effective Approaches to Attention-based Neural Ma-\nchine Translation. In Proceedings of the Conference on Em-\npirical Methods in Natural Language Processing (EMNLP),\n2015.\n[45] Sara Morrissey. Data-driven Machine Translation for Sign\nLanguages. PhD thesis, Dublin City University, 2008.\n[46] Sara Morrissey, Harold Somers, Robert Smith, Shane\nGilchrist, and Sandipan Dandapat. Building a Sign Language\nCorpus for Use in Machine Translation. In Proceedings of\nthe Representation and Processing of Sign Languages: Cor-\npora and Sign Language Technologies, 2010.\n[47] Vinod Nair and Geoffrey E Hinton. RectiÔ¨Åed Linear Units\nImprove Restricted Boltzmann Machines. In Proceedings of\nthe International Conference on Machine Learning (ICML),\n2010.\n[48] Graham Neubig. Neural Machine Translation and Sequence-\nto-Sequence Models: A Tutorial. arXiv:1703.01619, 2017.\n[49] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing\nZhu. BLEU: A Method for Automatic Evaluation of Ma-\nchine Translation. In Proceedings of the Annual Meeting of\nthe Association for Computational Linguistics (ACL), 2002.\n[50] Adam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-\nban Desmaison, Luca Antiga, and Adam Lerer. Auto-\nmatic Differentiation in PyTorch. In Proceedings of the Ad-\nvances in Neural Information Processing Systems Workshops\n(NIPSW), 2017.\n[51] Wendy Sandler. Sign Language and Modularity. Lingua,\n89(4), 1993.\n[52] Pinar Santemiz, Oya Aran, Murat Saraclar, and Lale Akarun.\nAutomatic Sign Segmentation from Continuous Signing via\nMultiple Sequence Alignment. In Proceedings of the IEEE\nInternational Conference on Computer Vision Workshops\n(ICCVW), 2009.\n[53] Adam Schembri, Jordan Fenlon, Ramas Rentelis, Sally\nReynolds, and Kearsy Cormier. Building the British Sign\nLanguage Corpus. Language Documentation & Conserva-\ntion (LD&C), 7, 2013.\n[54] Christoph Schmidt, Oscar Koller, Hermann Ney, Thomas\nHoyoux, and Justus Piater. Using Viseme Recognition to Im-\nprove a Sign Language Translation System. In Proceedings\nof the International Workshop on Spoken Language Transla-\ntion, 2013.\n[55] Frank M Shipman, Satyakiran Duggina, Caio DD Monteiro,\nand Ricardo Gutierrez-Osuna. Speed-Accuracy Tradeoffs for\nDetecting Sign Language Content in Video Sharing Sites. In\nProceedings of the International ACM SIGACCESS Confer-\nence on Computers and Accessibility (ASSETS), 2017.\n[56] Thad Starner, Joshua Weaver, and Alex Pentland. Real-time\nAmerican Sign Language Recognition using Desk and Wear-\nable Computer Based Video. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence (TPAMI), 20(12), 1998.\n[57] Daniel Stein, Christoph Schmidt, and Hermann Ney. Analy-\nsis, Preparation, and Optimization of Statistical Sign Lan-\nguage Machine Translation. Machine Translation, 26(4),\n2012.\n[58] William C Stokoe. Sign Language Structure. Annual Review\nof Anthropology, 9(1), 1980.\n[59] Stephanie Stoll, Necati Cihan Camgoz, Simon HadÔ¨Åeld, and\nRichard Bowden. Sign Language Production using Neural\nMachine Translation and Generative Adversarial Networks.\nIn Proceedings of the British Machine Vision Conference\n(BMVC), 2018.\n[60] Stephanie Stoll, Necati Cihan Camgoz, Simon HadÔ¨Åeld, and\nRichard Bowden. Text2Sign: Towards Sign Language Pro-\nduction using Neural Machine Translation and Generative\nAdversarial Networks. International Journal of Computer\nVision (IJCV), 2020.\n[61] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to\nSequence Learning with Neural Networks. In Proceedings\nof the Advances in Neural Information Processing Systems\n(NIPS), 2014.\n[62] Rachel Sutton-Spence and Bencie Woll. The Linguistics of\nBritish Sign Language: An Introduction. Cambridge Univer-\nsity Press, 1999.\n[63] Muhammed Mirac Suzgun, Hilal Ozdemir, Necati Cihan\nCamgoz, Ahmet Kindiroglu, Dogac Basaran, Cengiz Togay,\nand Lale Akarun. Hospisign: An Interactive Sign Language\nPlatform for Hearing Impaired. In Proceedings of the Inter-\nnational Conference on Computer Graphics, Animation and\nGaming Technologies (Eurasia Graphics), 2015.\n[64] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and\nAlexander A Alemi. Inception-v4, Inception-ResNet and the\nImpact of Residual Connections on Learning. InProceedings\nof the AAAI Conference on ArtiÔ¨Åcial Intelligence., 2017.\n[65] Shinichi Tamura and Shingo Kawasaki. Recognition of Sign\nLanguage Motion Images. Pattern Recognition, 21(4), 1988.\n[66] Mingxing Tan and Quoc V Le. EfÔ¨ÅcientNet: Rethinking\nModel Scaling for Convolutional Neural Networks. In Pro-\nceedings of the International Conference on Machine Learn-\ning (ICML), 2019.\n[67] Sandrine Tornay, Marzieh Razavi, Necati Cihan Camgoz,\nRichard Bowden, and Mathew Magimai-Doss. HMM-\nbased Approaches to Model Multichannel Information in\nSign Language Inspired from Articulatory Features-based\nSpeech Processing. In Proceedings of the IEEE Interna-\ntional Conference on Acoustics, Speech, and Signal Process-\ning (ICASSP), 2019.\n[68] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, , J. Zico\nKolter, Louis-Philippe Morency, and Ruslan Salakhutdinov.\nMultimodal Transformer for Unaligned Multimodal Lan-\nguage Sequences. In Proceedings of the Annual Meeting of\nthe Association for Computational Linguistics (ACL), 2019.\n[69] Jan P van Hemert. Automatic Segmentation of Speech. IEEE\nTransactions on Signal Processing, 39(4), 1991.\n[70] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia\nPolosukhin. Attention is All You Need. InProceedings of the\nAdvances in Neural Information Processing Systems (NIPS),\n2017.\n[71] Hanjie Wang, Xiujuan Chai, and Xilin Chen. Sparse Ob-\nservation (SO) Alignment for Sign Language Recognition.\nNeurocomputing, 175, 2016.\n[72] Hanjie Wang, Xiujuan Chai, Xiaopeng Hong, Guoying Zhao,\nand Xilin Chen. Isolated Sign Language Recognition with\nGrassmann Covariance Matrices. ACM Transactions on Ac-\ncessible Computing, 8(4), 2016.\n[73] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 7794‚Äì7803, 2018.\n[74] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V . Le,\nMohammad Norouzi, Wolfgang Macherey, Maxim Krikun,\nYuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva\nShah, Melvin Johnson, Xiaobing Liu, ukasz Kaiser, Stephan\nGouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa,\nKeith Stevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rudnick,\nOriol Vinyals, Greg Corrado, Macduff Hughes, and Jef-\nfrey Dean. Google‚Äôs Neural Machine Translation System:\nBridging the Gap Between Human and Machine Translation.\narXiv:1609.08144, 2016.\n[75] Fang Yin, Xiujuan Chai, and Xilin Chen. Iterative Refer-\nence Driven Metric Learning for Signer Independent Isolated\nSign Language Recognition. In Proceedings of the European\nConference on Computer Vision (ECCV), 2016.\n[76] Norimasa Yoshida. Automatic Utterance Segmentation in\nSpontaneous Speech. PhD thesis, Massachusetts Institute of\nTechnology, 2002.\n[77] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong\nSun, and Qun Liu. ERNIE: Enhanced Language Represen-\ntation with Informative Entities. In Proceedings of the An-\nnual Meeting of the Association for Computational Linguis-\ntics (ACL), 2019.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.817797064781189
    },
    {
      "name": "Machine translation",
      "score": 0.7561509609222412
    },
    {
      "name": "Sign language",
      "score": 0.696444034576416
    },
    {
      "name": "Transformer",
      "score": 0.6336601972579956
    },
    {
      "name": "Natural language processing",
      "score": 0.5378453135490417
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5252675414085388
    },
    {
      "name": "Speech recognition",
      "score": 0.4958556592464447
    },
    {
      "name": "Linguistics",
      "score": 0.15943482518196106
    },
    {
      "name": "Engineering",
      "score": 0.09404176473617554
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}