{
  "title": "CLUECorpus2020: A Large-scale Chinese Corpus for Pre-training Language Model",
  "url": "https://openalex.org/W3010108619",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2039357331",
      "name": "Xu Liang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2303041400",
      "name": "Zhang Xuanwei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2076836149",
      "name": "Dong, Qianqian",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2965373594",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2116343275",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2607892599",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3210120707",
    "https://openalex.org/W2996035354"
  ],
  "abstract": "In this paper, we introduce the Chinese corpus from CLUE organization, CLUECorpus2020, a large-scale corpus that can be used directly for self-supervised learning such as pre-training of a language model, or language generation. It has 100G raw corpus with 35 billion Chinese characters, which is retrieved from Common Crawl. To better understand this corpus, we conduct language understanding experiments on both small and large scale, and results show that the models trained on this corpus can achieve excellent performance on Chinese. We release a new Chinese vocabulary with a size of 8K, which is only one-third of the vocabulary size used in Chinese Bert released by Google. It saves computational cost and memory while works as good as original vocabulary. We also release both large and tiny versions of the pre-trained model on this corpus. The former achieves the state-of-the-art result, and the latter retains most precision while accelerating training and prediction speed for eight times compared to Bert-base. To facilitate future work on self-supervised learning on Chinese, we release our dataset, new vocabulary, codes, and pre-trained models on Github.",
  "full_text": "CLUECorpus2020: A Large-scale Chinese Corpus for Pre-training\nLanguage Model\nLiang Xu, Xuanwei Zhang, Qianqian Dong\nCLUE Organization\nCLUEbenchmark@163.com\nAbstract\nIn this paper, we introduce the Chinese corpus\nfrom CLUE organization, CLUECorpus2020,\na large-scale corpus that can be used di-\nrectly for self-supervised learning such as pre-\ntraining of a language model, or language gen-\neration. It has 100G raw corpus with 35 billion\nChinese characters, which is retrieved from\nCommon Crawl 1. To better understand this\ncorpus, we conduct language understanding\nexperiments on both small and large scale, and\nresults show that the models trained on this cor-\npus can achieve excellent performance on Chi-\nnese. We release a new Chinese vocabulary\n(vocab clue) with a size of 8K, which is only\none-third of the vocabulary size used in Chi-\nnese Bert released by Google. It saves com-\nputational cost and memory while works as\ngood as original vocabulary. We also release\nboth large and tiny versions of the pre-trained\nmodel on this corpus. The former achieves\nthe state-of-the-art result, and the latter retains\nmost precision while accelerating training and\nprediction speed for eight times compared to\nBert-base. To facilitate future work on self-\nsupervised learning on Chinese, we release\nour dataset, new vocabulary, codes, and pre-\ntrained models on Github2.\n1 Introduction\nTransfer learning in natural language processing\n(NLP), which ﬁrstly pre-training a large model on\nthe raw text, then ﬁne-tuning on downstream tasks,\nnow becomes the mainstream paradigm. It lever-\nages large-scale raw text, which is abundant on\nthe internet and achieves excellent performance.\nFor example, T5 (Raffel et al., 2019) treats all\nNLP problems as “text-to-text” problem, trained\non Colossal Clean Crawled Corpus (C4) with 750\n1http://commoncrawl.org\n2https://github.com/CLUEbenchmark/\nCLUECorpus2020/\nGB raw corpus, and achieves the state-of-the-art\nperformance on GLUE (Wang et al., 2018) and\nSQuAD (Rajpurkar et al., 2016).\nBehind the rapid development of NLP recently,\nnew and better models become available, and large\nscale raw corpus become more and more critical.\nThere are several large-scale pre-training datasets\npublicly available in English. However, there is\nstill a lack of open source large-scale Chinese cor-\npus that can serve for the pre-training of language\nmodel. Therefore, we release CLUECorpus2020.\nFor the convenience of reference, we also name it\nas C5, which stands for Colossal Clean Crawled\nCorpus for Chinese.\nIt contains 100 GB Chinese raw corpus, which\nis retrieved from Common Crawl. It is a\nwell-deﬁned dataset that can be used directly\nfor pre-training without requiring additional pre-\nprocessing. CLUECorpus2020 contains around\n29k separate ﬁles with each ﬁle following the pre-\ntraining format for the training set. And it has some\namount of ﬁles for the development and test set,\nbut each set is smaller. And several experiments on\nthis dataset have been conducted to test the quality\nof this dataset.\nTo summarize, this paper makes the following\ncontributions:\n•A large-scale Chinese raw corpus that can be\nused for pre-training, language generation or\nlearning word representation, such as word\nembedding.\n•Through our experiments, we show that the\nmodel trained on a small percentage of our\ncorpus can achieve better performance than\nthe model trained on Chinese Wiki, which in-\ndicates the excellent quality and big potential\nof the dataset. With the whole dataset, we\nare able to match the state-of-the-art result on\nChinese.\n1\narXiv:2003.01355v2  [cs.CL]  5 Mar 2020\n•A compact vocabulary( vocab clue) that can\nbe used for NLP tasks in Chinese with only\n8k vocabulary size, which is one-third of the\nvocabulary size of Chinese Bert( vocab bert).\nModels trained on vocab clue and vocab bert\nachieve comparable performance, while our\nvocabulary is smaller and better suit for Chi-\nnese, and can be faster for training machine\nlearning models.\n•We also release large and tiny versions of\nour pre-trained models trained on this dataset.\nThe large version achieves the state-of-the-art\nperformance, while the tiny version can be\nused to accelerate experiments and real appli-\ncations.\n2 Related work\nFor English, there are a large number of open-\nsource unlabeled corpora. For example, 1) Toronto\nBooks Corpus (Zhu et al., 2015), a 4 GB dataset\ncontains text extracted from eBooks, which rep-\nresents a dierent domain of natural language.\n2) WebText-like (Radford et al., 2019), a 17 GB\nWebText-like English dataset, only uses content\nfrom web pages that were submitted to the content\naggregation website Reddit and received a score of\nat least. 3) English Wikipedia, a 16 GB English\nWikipedia text data which consists of millions of\nencyclopedia articles written collaboratively and\ncan be found in TensorFlow Datasets3, which omits\nall markup and reference sections from the articles.\n4) C4 (Raffel et al., 2019) dataset, a 750 GB En-\nglish dataset consists of hundreds of gigabytes of\nclean English text scraped from the web.\nBut for Chinese, similar corpus collections are\nstill relatively rare and have a small size. For ex-\nample, 1) THUCTC (Sum et al., 2016), a 2.19 GB\ndataset contains 740,000 news documents. 2) Chi-\nnese Wikipedia4, a 1.1 GB dataset contains Chinese\nWikipedia text data. As we all know, the size of the\nexisting Chinese dataset is relatively small. In this\npaper, to solve the problem of lacking large-scale\nunlabeled corpus in Chinese, we leverage Common\nCrawl which is crawled from the whole internet\nand pre-process this dataset in detail. Finally, we\nprovide a bigger and higher-quality all-inclusive\ncorpus.\nDataset Token (B) Sentences (M) Size (GB)\nTrain 34.7 106 99.0\nDev 0.18 3.9 0.5\nTest 0.18 3.9 0.5\nTable 1: Statistical information of CLUECorpus2020.\n“B”: billion; “M”: million. Dev and Test set were\ndrawn from the same distribution of training set, which\ncan be used to check the generalization ability of a\nmodel. e.g., check mask Language Model(LM) accu-\nracy of a model during the training stage, or perplexity\nof a language model after training.\n3 Dataset Description\nBefore we release this corpus, there is few large-\nscale high-quality Chinese dataset designed for pre-\ntraining language model in Chinese. This corpus\nis around 100 GB and comes from different web-\nsites 5. We use the ratio of 99 : 0.5 : 0.5 to split\nthe data into the training set, development set and\ntest set randomly. As we seen from samples, it\ncovers all sorts of topics, like news, entertainment,\nsports, health, international affairs, movies, celebri-\nties, and so on. We follow pre-training format to\norganize the ﬁles of our dataset: one sentence per\nline and add an empty line at the end of a document.\nThe overall statistics of this corpus is described in\nTable 1. We will elaborate on the data construction\nprocess in the next section.\n4 Dataset Construction\nUnlabeled large scale datasets for unsupervised\nlearning play an increasingly important role for\nChinese NLP tasks. We believe that higher-quality\ndata will have a better impact on Chinese NLP\ntasks. In this paper, we select and provide high-\nquality unlabeled dataset. To generate datasets that\nsatisfy our requirements, we leverage Common\nCrawl as a source of text scraped from the web.\nCommon Crawl is an organization that crawls\nthe web and freely provides its archives and\ndatasets to the public. Common Crawl usually\ncrawls internet web content once a month. Com-\nmon Crawl’s web archives consist of petabytes of\ndata collected since 2011. First, we extract text\ncontent from the scraped HTML ﬁles according\nto the detailed rules. Unfortunately, the majority\nof text content contains gibberish like dirty text or\n3https://www.tensorﬂow.org/datasets/catalog/wikipedia/\n4https://github.com/brightmart/nlp chinese corpus/\n5http://commoncrawl.org/the-data/get-started/\n2\nsource code that we think useless to NLP tasks in\nChinese. Furthermore, the scraped text includes a\nlot of duplicate content. To solve these problems,\nwe do further ﬁltering and extraction using the fol-\nlowing heuristics rules, by which we have special\ntreatment for Chinese, in addition to referring to\nthe ﬁltering method of C4:\n•Since we focus on Chinese tasks, we select\nsentences whose language type is Chinese, if\na language is mentioned.\n•To avoid incomplete sentences, we remove\ncharacters from the end of the text, until we\nﬁnd a Chinese terminal punctuation mark (i.e.,\na period, question mark, or the end of double\nquotation mark).\n•Since Chinese words that contain “List\nof Dirty, Naughty, Obscene or Other Bad\nWords.6 have a bad effect on building a\nhealthy and civilized internet environment, so\nwe remove all sentences that contain them.\n•Warnings state that Javascript should be en-\nabled unlikely to be helpful for NLP tasks, so\nwe remove any line with the word Javascript\nor JavaScript.\n•To deduplicate the dataset, we discard all but\none of any four-sentence span occurring more\nthan once in the dataset.\n•We replace consecutive blank characters (i.e.,\ntabs, spaces, invisible characters, etc.) that\nare generally meaningless in sentences with\nspace.\n•Since the curly bracket “ {appears in many\nprogramming languages (such as Javascript,\nwidely used on the web) but not in the natural\ntext, we remove any sentence that contains a\ncurly bracket.\n•To generate pre-trained format data, we use\npyltp (Che et al., 2010) to separate text con-\ntent into sentences, one complete sentence per\nline.\n•Since too short sentences that may be problem-\natic or incomplete sentences are not suitable\nfor language model training, we only retain\nsentences longer than 5.\nWe download the corpus from July to December\n2019 from Common Crawl. After the aforemen-\ntioned ltering method, we extract the corpus of 100\nGB. The corpus is much larger than the previous\nmost datasets used for pre-training (about 100 GB)\nand is clean and natural Chinese text.\n6https://github.com/LDNOOBW/\nList-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/\n5 Creation of CLUE Vocab\nToken Type Google CLUE\nSimpliﬁed Chinese 11378 5689\nTraditional Chinese 3264 \u0017\nEnglish 3529 1320\nJapanese 573 \u0017\nKorean 84 \u0017\nEmoji 56 \u0017\nNumbers 1179 140\nSpecial Tokens 106 106\nOther Tokens 959 766\nTotal 21128 8021\nTable 2: Statistical information of the two versions of\nthe dictionaries. “Special Tokens” include “[PAD]”,\n“[UNK]”, “[CLS]”, “[SEP]”, “[MASK]”, “⟨S⟩”, “⟨T⟩”\nand 99 unused tokens.\nThe original BERT model uses character-based\ntokenization for Chinese. But there are many redun-\ndant tokens in the original vocabulary. Therefore,\nwe have compiled a reﬁned vocabulary through\nautomated scripts and manual review. A detailed\ncomparison of the two versions of the dictionaries\ncan be seen in Table 2. We remove many unneces-\nsary tokens, which will not be used in most cases\nof Chinese NLP tasks, such as traditional Chinese,\nJapanese, Korean and emojis. For English, we re-\nmove the most preﬁx tokens except for the single\ncharacter and retain the most sufﬁx tokens to guar-\nantee the tokenization for English words. Similarly,\nfor tokens standing for numbers, we only keep sep-\narate numeric symbols and the more commonly\nused words that represent the year. In addition, we\nalso remove tokens composed of more than two\nspecial symbols.\nAs a result, the vocabulary size is only one-third\nof the original size of Chinese BERT. We call it\n“vocab clue”.\n6 Experiments\n6.1 Pre-training with CLUECorpus2020 and\nWiki\nIn this section, we want to compare our new dataset\nwith Wiki using the same model. We choose\nBERT (Devlin et al., 2018) as our baseline model.\nWe pre-train the BERT-base model with Wiki data\nand C5 data, respectively. Due to the limited com-\nputing resources, we have designed a comparison\n3\nIndex Model V ocab Data Steps AFQMC TNEWS IFLYTEK CMNLI A VG\n1 BERT-base Google Wiki (1 GB) 125K 69.93 54.77 57.54 75.64 64.47\n2 BERT-base Google C5 (1 GB) 125K 69.63 55.72 58.87 75.75 64.99\n3 BERT-base CLUE C5 (1 GB) 125K 69.00 55.04 59.07 75.84 64.74\n4 BERT-base† Google C5 (1 GB) 125K 69.57 55.17 59.69 75.86 65.07\nTable 3: Performance of Bert Models on CLUE benchmark (http://www.cluebenchmark.com) . BERT-base†stand\nfor BERT-base mm. For each experiment, we select the best model using dev set after training stage, then submit to\nCLUE benchmark and get score. Performance comparison for different corpus through Index 1 and 2. Performance\ncomparison of two vocabularies through Index 2 and 3. Comparison of attention mechanism through Index 2 and\n4. BERT-base mm, stand for BERT-base with minus and element-wise multiplication.\nIndex Model V ocab Data Steps AFQMC TNEWS IFLYTEK CMNLI A VG\n5 BERT-base Google C5 (1 GB) 375K 69.85 55.97 59.62 76.41 65.46\n6 BERT-base CLUE C5 (1 GB) 375K 69.93 56.38 59.35 76.58 65.56\n7 BERT-base Google C5 (3 GB) 375K 70.22 56.41 59.58 76.70 65.73\n8 BERT-base CLUE C5 (3 GB) 375K 69.49 55.97 60.12 77.66 65.81\nTable 4: The effects of more training data of C5 and steps. With three times steps(3* 125k), BERT-base model\ntrained on C5, gain 0.47 to 0.82 point compare to same model trained with 125k using different vocabularies. With\ntree times training data(3* 1GB), BERT-base model gain 0.74 to 1.07 point compare to same model trained with\n1GB data and 125K, 0.25 to 0.27 point compare to same steps on 1GB. And the model trained with CLUE vocab\nalways better than with Google vocab.\nIndex Model V ocab Training Data Steps ACC of Masked LM Loss of Masked LM\n1 BERT-base Google Wiki(1 GB) 125K 72.24% 1.2321\n2 BERT-base Google C5 (1 GB) 125K 77.94% 0.9702\n3 BERT-base CLUE C5 (1 GB) 125K 76.47% 1.0691\n4 BERT-base mm Google C5 (1 GB) 125K 78.02% 0.9816\nTable 5: Training metrics of Bert Models. Accuracy and loss of masked LM is on training set.\nTask Length Batch Size Learning Rate Epoch Save Steps\nAFQMC 128 16 2e-5 3 300\nTNEWS 128 16 2e-5 3 300\nIFLYTEK 128 32 2e-5 3 300\nCMNLI 128 64 3e-5 2 300\nTable 6: Hyper-parameters of ﬁne-tuning on CLUE tasks. We keep all hyper-parameters the same throughout all\nthe experiments.\non a small-scale corpus. The performance on large-\nscale corpus will be added in the next version. To\nmake a fair comparison, we keep the parameters\nof both models the same. Meanwhile, the size of\nWiki and the selected part of C5 data are both 1\nGB. We release both of this corpus to make our\nresults reproducible. As the length of most classiﬁ-\ncation tasks is less than 128, we set the maximum\nsequence length of the pre-training to 128. It also\nimproves the speed of pre-training. We pre-train\nBERT using masked language model (LM) pre-\ndiction task without the next sentence prediction\n(NSP) task, as NSP task makes the performance\nof models worse observed in some recent papers,\nsuch as RoBERTa (Liu et al., 2019).\nClassiﬁcation board of CLUE benchmark com-\nprises 6 tasks meant to test general language under-\nstanding ability. We use the following four tasks to\ntest the performance of our models as the sequence\nlength of these tasks is 128 or less. During ﬁne-\n4\nModel V ocabulary V ocabulary Size Parameters Training Device Training Speed\nBert-base google vocab 21128 102M TPU V3-8 1000steps/404s\nBert-base clue vocab 8021 (↓ 62.04%) 92M (↓ 9.80%) TPU V3-8 1000steps/350s (↑ 15.43%)\nRoBERTa-tiny-clue clue vocab 8021 (↓ 62.04%) 7.5M (↓ 92.6%) TPU V3-8 1000steps/50s (↑ 708.0%)\nTable 7: Detailed speed comparison of “google vocab” and “clue vocab”, and RoBERTa-tiny-clue with Bert-base.\nCLUE vocabulary is one-third of Google vocabulary, with ten percentage or more speedup. RoBERTa-tiny-clue is\n7 to 8 times faster than Bert-base\ntuning CLUE benchmark tasks, we also maintain\nthe same parameters, as can be seen in Table 6.\nHere are four tasks we used, including different\nkinds of tasks.\n•Sentence Pair Similarity:\nAFQMC7\n•Sentiment Analysis:\nTNEWS8\n•Multi-label Classiﬁcation:\nIFLYTEK (IFLYTEK CO., 2019)\n•Natural Language Inference:\nCMNLI (Conneau et al., 2018; Williams et al.,\n2017)\nAs we can see in Table 3, the performance of\nthe model pre-trained on C5 is 0.52% higher than\nthe model pre-trained on Wiki. It suggests that our\ndata quality is similar or even better than Wiki. As\nwe only use one percent of the whole dataset, there\nis a big potential to have a good performance using\nthe whole dataset.\n6.2 Comparison of Attention Mechanisms\nwith C5\nThe backbone of pre-trained models, typically\nlike BERT and its variants, is Transformer\nmodel (Vaswani et al., 2017). The key compo-\nnent is the self-attention mechanism. With our new\ndataset, we are able to explore some variants of\nthis mechanism. A self-attention module takes in\nn inputs and returns n outputs. The self-attention\nmechanism allows the inputs to interact with each\nother (“self) and ﬁnd out what they should pay\nmore attention to (“attention). The outputs are ag-\ngregates of these interactions and attention scores.9\n7https://dc.cloud.alipay.com/index#/topic/intro?id=3\n8https://github.com/fatecbf/\ntoutiao-text-classﬁcation-dataset/\n9https://towardsdatascience.com/\nillustrated-self-attention-2d627e33b20a\nWe believe there is some room for improvement\nof attention mechanism, especially that current\nheavily used self-attention may still too simple and\nnaive to represent the importance of information\nfor the input sequence. So we try a variant of self-\nattention mechanism, as follows:\n•BERT-base mm (Minus and element-wise\nMultiplication): Given two vector, we want\nto use a simple and inexpensive computation\nmethod to compute the similarity of these two\nvectors, such as using the absolute minus and\nelement-wise multiplication operation. Then\nwe transform the result with a dense layer and\nadd them to the attention score.\nAs we can see from Table 3 and Table 5, our\nvariant has similar or slightly better performance\nas/than baseline. These results indicate that pos-\nsible improvement of attention mechanisms may\nimprove performance on downstream tasks. With\nour new dataset, researchers of NLP can explore\ntheir ideas regarding this area.\n6.3 Performance Comparison of Two\nVocabularies\nIn order to verify the rationality of our vocabulary,\nwe train the BERT-base model using the original\nvocabulary published by Google and our reﬁned\nnew vocabulary. We use the same model, and keep\nall hyper-parameters the same, and the only dif-\nference is the vocabulary. Similar to the previous\nsection, we used multiple downstream tasks, four\ntasks, to conﬁrm the performance of the vocabu-\nlary. As can be seen from Table 3, for Index 2\nand 3, the performance is similar, with only 0.25\npoint difference. We believe that our new vocabu-\nlary, “vocab clue” can be used in downstream NLP\ntasks in Chinese in the future, especially for those\nsituations with limited resource and computation\npower.\nIn table 7, we make a detailed comparison. As\nwe can see, the size of clue vocab is 62.04% less\n5\nIndex Model V ocab Data AFQMC TNEWS IFLYTEK CMNLI A VG\n9 BERT-base (Devlin et al., 2018) Google / 73.70 56.58 60.29 79.69 67.57\n10 ALBERT-tiny Google (30GB) 69.92 53.35 48.71 70.61 60.65\n11 ELECTRA-joint-generator-tiny (Clark et al., 2019) Google / 69.90 54.63 52.31 73.17 62.50\n12 RoBERTa-tiny-clue (Cui et al., 2019) CLUE C5 (100 GB) 69.52 54.57 57.31 73.1 63.60\nTable 8: Performance of the tiny version of pre-training models. Our tiny version model, RoBERTa-tiny-clue,\nretains most of precision compared to BERT-base, only 4 points lower than BERT-base, while performance is\nmuch better than ALBERT-tiny. All scores were reported by submitting to CLUE benchmark.\nthan origin vocabulary and has about 9.84% fewer\nparameters compared to BERT-base. We ﬁnd the\ntraining speed is 15.43% faster than the original\nBERT-base, which pre-trained both on TPU V3-8.\n6.4 More training data of C5 and steps\nAs we can see from Tabel 4 , under the same steps,\nthe performance of BERT-base which use 3 GB of\ndata is 0.27 point higher than the performance of\nBERT-base which use 1 GB of data through Index\n5 and 7. Meanwhile, the performance of BERT-\nbase which use clue vocab is 0.1 point higher than\nthe performance of BERT-base which use google\nvocabulary through Index 5 and 6. It can be con-\ncluded that the increase of the corpus can improve\nthe performance of the model, and our clue vocab\nis better than google vocabulary while BERT-base\nmodel is trained enough steps.\n6.5 Performance of Large Version\nWe generate our training data the same as\nRoBERTa (Liu et al., 2019), and remove the Next\nSentence Prediction(NSP) task. To compare with\nRoBERTa-wwm-large10, which is currently the\nbest Chinese model, we also use whole word mask\nas our mask strategy.\nTo speed up pre-training in our experiments, sim-\nilar to BERT, we ﬁrst train 500k steps on 128 se-\nquence length with batch size 8k. We then train\n600k on 512 sequence length with batch size 4k,\nand make the model more suitable for tasks with\nlonger sequence lengths.\nAs we can see from Table 9, As the number of\ntraining steps increases, the performance of the\nmodel is gradually getting better. Finally, we can\nsee our performance is better than the original\nroberta-wwm-large.\n6.6 Performance of Tiny version\nState-of-the-art models, like BERT, can achieve\nvery good performance compared to other models.\n10https://github.com/ymcui/Chinese-BERT-wwm\nHowever, as these models are very big and deep,\nwith hundreds of millions of parameters, they are\nusually very slow during the prediction stage. To\nease this problem, we release a small version of\nthe pre-trained model. We want to keep it as small\nand fast as possible but retain the most precision.\nFor those tasks that are not too difﬁcult, like clas-\nsiﬁcation with few labels or sentence-pair, we rec-\nommend using this small version model to replace\nthose big and slow BERT models.\nWe name it RoBERTa-tiny-clue, as it bases on\nthe model of RoBERTa, and trained with corpus\nand vocabulary from CLUE. We ﬁrst train it on\nsequence length 128 for 500k steps with batch size\n8k using 100G corpus, then we train it for addi-\ntional 200k steps with the same batch size using an\nadditional 30G corpus. Together, it trains with 5.6\nbillion training instances.\nThe conﬁguration of hyper-parameters is keep\nsame as ALBERT-tiny11, with hidden size 312 for\n4 layers. It is around ten times fast for training\nand prediction compare to BERT-base. It gains\nan additional ten percentage speed accelerate even\ncompare to ALBERT-tiny as the vocabulary used,\nvocab-clue, is only one-third of the vocabulary of\nBERT. Most importantly, it’s performance is much\nbetter than ALBERT-tiny. Check table 8 for perfor-\nmance comparison with Bert-base and ALBERT-\ntiny.\n6.7 Transfer Learning Among Similar Tasks\nusing Pre-trained Models\nPre-trained models are powerful, but it is still dif-\nﬁcult for them to learn tasks without enough train-\ning data. We observe that the robust performance\nmodel RoBERTa-large-clue can not learn well on\ntask AFQMC, a sentence-pair task. The CMNLI\ntask, which is also a sentence pair-task, has a lot\nof training data( around 390k). Therefore, We ﬁrst\ntrain CMNLI using our pre-trained model, then we\n11https://github.com/brightmart/albert zh\n6\nIndex Model V ocab Data Steps Init AFQMC TNEWS IFLYTEK CMNLI A VG\n13 RoBERTa-wwm-large’ Google / / \u0017 74.44 58.41 62.77 82.2 69.46\n14 RoBERTa-wwm-large’ Google / / CMNLI 75.19 58.41 62.77 82.2 69.64\n15 RoBERTa-large-clue CLUE C5 (100 GB) 100K \u0017 69.9 56.95 62.08 80.48 67.35\n16 RoBERTa-large-clue CLUE C5 (100 GB) 200K \u0017 69.98 58.66 62.50 81.33 68.12\n17 RoBERTa-large-clue CLUE C5 (100 GB) 500K \u0017 74.00 58.70 62.31 82.04 69.26\n18 RoBERTa-large-clue CLUE C5 (100 GB) 500K CMNLI 74.41 58.70 62.31 82.04 69.37\n19 RoBERTa-large-clue CLUE C5 (100 GB) 650K \u0017 70.01 58.52 62.54 82.68 68.44\n20 RoBERTa-large-clue CLUE C5 (100 GB) 650K CMNLI 74.41 58.52 62.54 82.68 69.54\n21 RoBERTa-large-clue CLUE C5 (130 GB) 800K CMNLI 74.41 58.38 63.58 82.36 69.68\nTable 9: Performance of RoBERTa-large using 100G corpus and vocabulary from CLUE. Our model RoBERTa-\nlarge-clue trained with 100g achieve same performance as RoBERTa-wwm-large’ (Cui et al., 2019), slightly better\nperformance in two tasks, IFLYTEK and CMNLI. Init with CMNLI means when training task AFQMC, the model\nis initialized from a model that trained on CMNLI. All scores were reported by submitting to CLUE benchmark.\nuse this trained model to initialize AFQMC. As a\nresult, it is around 0.8 to 4 points of performance\nboost compare to initializing from the pre-trained\nmodel. See ﬁeld of init with CMNLI on table 9. We\nbelieve this is a kind of transfer earning, which uses\nknowledge learned from one task and applies it to\nanother similar task. We name the model trained\nwith CMNIL as RoBERTa-pair, and release it on\nour repository. We believe for many other sentence-\npair tasks, with the help of this model, people can\nalso achieve better performance than initializing\nfrom general pre-trained models.\n7 Conclusion\nIn this paper, we introduce CLUECorpus2020, a\nlarge-scale corpus that can be used directly for\nthe pre-training language model in Chinese. It is\nthe ﬁrst well-deﬁned large-scale public available\ndataset that serves the purpose of the pre-training\nlanguage model in Chinese. We conduct experi-\nments on a small portion of this new dataset and\nChinese Wiki. The results prove that our dataset\nhas good quality and huge potential. In addition,\nwe conduct experiments on the full dataset for a\nfull-network pre-trained model. We also release a\nnew vocabulary which size is small but work well\nfor Chinese tasks. With our corpus and vocabulary,\nour model is able to match state-of-the-art perfor-\nmance in Chinese. We also observe that transfer\nlearning is useful among similar tasks, and it can\nboost performance. We release our dataset, vocab-\nulary, pre-trained models, and codes on Github.\nIn this work, we focus on pre-training, especially\nfor language understanding. However, this dataset\ncan also be used for language generation and other\nNLP tasks. We leave these for further study.\n8 Acknowledgements\nOur research is supported by Cloud TPUs from\nGoogle’s TensorFlow Research Cloud (TFRC). We\nthank Zhe Zhao, Junyi Li, Shaomian Zheng, Zhen-\nzhong Lan , and Peng Li for the sharing fee of the\nexperiments.\nReferences\nWanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp:\nA chinese language technology platform. In Pro-\nceedings of the 23rd International Conference on\nComputational Linguistics: Demonstrations , pages\n13–16. Association for Computational Linguistics.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2019. Electra: Pre-training\ntext encoders as discriminators rather than genera-\ntors. In International Conference on Learning Rep-\nresentations.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. Xnli: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing. Association for Computa-\ntional Linguistics.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin,\nZiqing Yang, Shijin Wang, and Guoping Hu. 2019.\nPre-training with whole word masking for chinese\nbert. arXiv preprint arXiv:1906.08101.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\n7\nLTD. IFLYTEK CO. 2019. Iﬂytek: a multiple cate-\ngories chinese text classiﬁer. competition ofﬁcial\nwebsite, http://challenge.xfyun.cn/2019/gamelist.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv e-prints.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nM Sum, J Li, Z Guo, Y Zhao, Y Zheng, X Si, and\nZ Liu. 2016. Thuctc: an efﬁcient chinese text classi-\nﬁer. GitHub Repository.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2017. A broad-coverage challenge corpus for\nsentence understanding through inference. arXiv\npreprint arXiv:1704.05426.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE inter-\nnational conference on computer vision , pages 19–\n27.\n8",
  "topic": "Vocabulary",
  "concepts": [
    {
      "name": "Vocabulary",
      "score": 0.8322212100028992
    },
    {
      "name": "Computer science",
      "score": 0.8233956694602966
    },
    {
      "name": "Natural language processing",
      "score": 0.6716583967208862
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6541717052459717
    },
    {
      "name": "Language model",
      "score": 0.6460352540016174
    },
    {
      "name": "Scale (ratio)",
      "score": 0.5028414130210876
    },
    {
      "name": "Text corpus",
      "score": 0.4529378414154053
    },
    {
      "name": "Training (meteorology)",
      "score": 0.43794065713882446
    },
    {
      "name": "Speech recognition",
      "score": 0.37105539441108704
    },
    {
      "name": "Linguistics",
      "score": 0.16919168829917908
    },
    {
      "name": "Meteorology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}