{
  "title": "Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization",
  "url": "https://openalex.org/W3176647794",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A1908610349",
      "name": "Chen Liang",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2903413907",
      "name": "Simiao Zuo",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2574818019",
      "name": "Minshuo Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2472586093",
      "name": "Haoming Jiang",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2101917160",
      "name": "Xiaodong Liu",
      "affiliations": [
        "Georgia Institute of Technology",
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2118516085",
      "name": "Pengcheng He",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2234658305",
      "name": "Tuo Zhao",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2108390110",
      "name": "Wei‐Zhu Chen",
      "affiliations": [
        "Microsoft (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3105249545",
    "https://openalex.org/W2951569836",
    "https://openalex.org/W2963813662",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2896409484",
    "https://openalex.org/W2707890836",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2948635472",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2995816250",
    "https://openalex.org/W2948130861",
    "https://openalex.org/W3098985395",
    "https://openalex.org/W2943283198",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3034230673",
    "https://openalex.org/W3103754749",
    "https://openalex.org/W2995107071",
    "https://openalex.org/W3022969335",
    "https://openalex.org/W2396767181",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2998103904",
    "https://openalex.org/W3168545914",
    "https://openalex.org/W2976872728",
    "https://openalex.org/W3007728469",
    "https://openalex.org/W3104216863",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W4313908941",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W3111265704",
    "https://openalex.org/W3037624666",
    "https://openalex.org/W2948253213",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2996309822",
    "https://openalex.org/W3102446692",
    "https://openalex.org/W2915106038",
    "https://openalex.org/W3087835661",
    "https://openalex.org/W3104263050",
    "https://openalex.org/W2984218712",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W4287864853",
    "https://openalex.org/W2788496822",
    "https://openalex.org/W2975381464",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2996159613",
    "https://openalex.org/W3176010265",
    "https://openalex.org/W2805003733",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2995197005",
    "https://openalex.org/W3098405901",
    "https://openalex.org/W2970277060",
    "https://openalex.org/W3015233032",
    "https://openalex.org/W1480376833"
  ],
  "abstract": "Chen Liang, Simiao Zuo, Minshuo Chen, Haoming Jiang, Xiaodong Liu, Pengcheng He, Tuo Zhao, Weizhu Chen. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 6524–6538\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6524\nSuper Tickets in Pre-Trained Language Models: From Model\nCompression to Improving Generalization\nChen Liang∗1, Simiao Zuo1, Minshuo Chen1, Haoming Jiang1,\nXiaodong Liu2, Pengcheng He3, Tuo Zhao1, Weizhu Chen3\n1 Georgia Institute of Technology,2 Microsoft Research, 3 Microsoft Azure AI\n{cliang73, simiaozuo, mchen393, jianghm, tourzhao}@gatech.edu\n{xiaodl,penhe,wzchen}@microsoft.com\nAbstract\nThe Lottery Ticket Hypothesis suggests that an\nover-parametrized network consists of “lottery tick-\nets”, and training a certain collection of them (i.e.,\na subnetwork) can match the performance of the\nfull model. In this paper, we study such a collec-\ntion of tickets, which is referred to as “winning\ntickets”, in extremely over-parametrized models,\ne.g., pre-trained language models. We observe that\nat certain compression ratios, the generalization\nperformance of the winning tickets can not only\nmatch but also exceed that of the full model. In par-\nticular, we observe a phase transition phenomenon:\nAs the compression ratio increases, generalization\nperformance of the winning tickets ﬁrst improves\nthen deteriorates after a certain threshold. We refer\nto the tickets on the threshold as “super tickets”.\nWe further show that the phase transition is task\nand model dependent — as the model size becomes\nlarger and the training data set becomes smaller, the\ntransition becomes more pronounced. Our experi-\nments on the GLUE benchmark show that the super\ntickets improve single task ﬁne-tuning by0.9 points\non BERT-base and 1.0 points on BERT-large, in\nterms of task-average score. We also demonstrate\nthat adaptively sharing the super tickets across tasks\nbeneﬁts multi-task learning1.\n1 Introduction\nThe Lottery Ticket Hypothesis (LTH, Frankle and\nCarbin (2018)) suggests that an over-parameterized\nnetwork consists of “lottery tickets”, and training a\ncertain collection of them (i.e., a subnetwork) can\n1) match the performance of the full model; and 2)\n∗Work was done at Microsoft Azure AI.\n1Our codes are available at\nhttps://github.com/cliang1453/\nsuper-structured-lottery-tickets .\noutperform randomly sampled subnetworks of the\nsame size (i.e., “random tickets”). The existence\nof such a collection of tickets, which is usually\nreferred to as “winning tickets”, indicates the po-\ntential of training a smaller network to achieve the\nfull model’s performance. LTH has been widely\nexplored in across various ﬁelds of deep learning\n(Frankle et al., 2019; Zhou et al., 2019; You et al.,\n2019; Brix et al., 2020; Movva and Zhao, 2020;\nGirish et al., 2020).\nAside from training from scratch, such winning\ntickets have demonstrated their abilities to transfer\nacross tasks and datasets (Morcos et al., 2019; Yu\net al., 2019; Desai et al., 2019; Chen et al., 2020a).\nIn natural language processing, Chen et al. (2020b);\nPrasanna et al. (2020) have shown existence of\nthe winning tickets in pre-trained language models.\nThese tickets can be identiﬁed when ﬁne-tuning\nthe pre-trained models on downstream tasks. As\nthe pre-trained models are usually extremely over-\nparameterized (e.g., BERT Devlin et al. (2019),\nGPT-3 Brown et al. (2020), T5 Raffel et al. (2019)),\nprevious works mainly focus on searching for a\nhighly compressed subnetwork that matches the\nperformance of the full model. However, behav-\nior of the winning tickets in lightly compressed\nsubnetworks is largely overlooked.\nIn this paper, we study the behavior of the win-\nning tickets in pre-trained language models, with\na particular focus on lightly compressed subnet-\nworks. We observe that generalization performance\nof the winning tickets selected at appropriate com-\npression ratios can not only match, but also exceed\nthat of the full model. In particular, we observe\na phase transition phenomenon (Figure 1): The\ntest accuracy improves as the compression ratio\ngrows until a certain threshold (Phase I); Passing\nthe threshold, the accuracy deteriorates, yet is still\nbetter than that of the random tickets (Phase II). In\nPhase III, where the model is highly compressed,\n6525\nBiasVariance\nPercent of Weight Remaining\nPhase I Phase II Phase III\nBiasVariance\nPercent of Weight Remaining\nPhase I Phase II Phase III\n0.8 0.6 0.41.0\nPercent of Weight Remaining\nPhase I Phase II Phase III\nWinning \nRandom \nAccuracy\n0.8 0.6 0.41.0\nBias\nVariance\nPercent of Weight Remaining\nPhase I Phase II Phase III\nPercent of Weight Remaining\nPhase I Phase II Phase III\nWinning\nRandom\nAccuracy\n0.8 0.6 0.41.00.8 0.6 0.41.0\nPercent of Weight Remaining\nPhase I Phase II Phase III\nFigure 1: Illustrations of the phase transition phenomenon. Left: Generalization performance of the ﬁne-tuned\nsubnetworks (the same as Figure 4 in Section 5). Middle and Right: An interpretation of bias-variance trade-off.\ntraining collapses. We refer to the set of winning\ntickets selected on that threshold as “super tickets”.\nWe interpret the phase transition in the context of\ntrade-offs between model bias and variance (Fried-\nman et al., 2001, Chapter 7). It is well understood\nthat an expressive model induces a small bias, and\na large model induces a large variance. We classify\nthe tickets into three categories: non-expressive\ntickets, lightly expressive tickets, and highly ex-\npressive tickets. The full model has a strong expres-\nsive power due to over-parameterization, so that\nits bias is small. Yet its variance is relatively large.\nIn Phase I, by removing non-expressive tickets,\nvariance of the selected subnetwork reduces, while\nmodel bias remains unchanged and the expressive\npower sustains. Accordingly, generalization per-\nformance improves. We enter Phase II by further\nincreasing the compression ratio. Here lightly ex-\npressive tickets are pruned. Consequently, model\nvariance continues to decrease. However, model\nbias increases and overturns the beneﬁt of the re-\nduced variance. Lastly for Phase III, in the highly\ncompressed region, model bias becomes notori-\nously large and reduction of the variance pales. As\na result, training breaks down and generalization\nperformance drops signiﬁcantly.\nWe conduct systematic experiments and analyses\nto understand the phase transition. Our experiments\non multiple natural language understanding (NLU)\ntasks in the GLUE (Wang et al., 2018) benchmark\nshow that the super tickets can be used to improve\nsingle task ﬁne-tuning by 0.9 points over BERT-\nbase (Devlin et al., 2019) and1.0 points over BERT-\nlarge, in terms of task-average score. Moreover,\nour experiments show that the phase transition phe-\nnomenon is task and model dependent. It becomes\nmore pronounced as a larger model is used to ﬁt a\ntask with less training data. In such a case, the set\nof super tickets forms a compressed network that\nexhibits a large performance gain.\nThe existence of super tickets suggests poten-\ntial beneﬁts to applications, such as Multi-task\nLearning (MTL). In MTL, different tasks require\ndifferent capacities to achieve a balance between\nmodel bias and variance. However, existing meth-\nods do not speciﬁcally balance the bias and vari-\nance to accommodate each task. In fact, the ﬁne-\ntuning performance on tasks with a small dataset\nis very sensitive to randomness. This suggests that\nmodel variance in these tasks are high due to over-\nparameterization. To reduce such variance, we\npropose a tickets sharing strategy. Speciﬁcally, for\neach task, we select a set of super tickets during\nsingle task ﬁne-tuning. Then, we adaptively share\nthese super tickets across tasks.\nOur experiments show that tickets sharing im-\nproves MTL by 0.9 points over MT-DNNBASE (Liu\net al., 2019) and1.0 points over MT-DNNLARGE, in\nterms of task-average score. Tickets sharing further\nbeneﬁts downstream ﬁne-tuning of the multi-task\nmodel, and achieves a gain of 1.0 task-average\nscore. In addition, the multi-task model obtained\nby such a sharing strategy exhibits lower sensitiv-\nity to randomness in downstream ﬁne-tuning tasks,\nsuggesting a reduction in variance.\nWe summarize our contributions as follows:\n•Our result is the ﬁrst to identify the phase\ntransition phenomenon in pruning large neural lan-\nguage models.\n•Our result is the ﬁrst to show that pruning can\nimprove the generalization when the models are\nlightly compressed, which has been overlooked\nby previous works. Our analysis paves the way\nfor understanding the connection between model\ncompression and generalization.\n•Motivated by our observed phase transition,\n6526\nwe further propose a new pruning approach for\nmulti-task ﬁne-tuning of neural language models.\n2 Background\nWe brieﬂy introduce the Transformer architecture\nand the Lottery Ticket Hypothesis.\n2.1 Transformer Architecture\nThe Transformer (Vaswani et al., 2017) encoder\nis composed of a stack of identical Transformer\nlayers. Each layer consists of a multi-head attention\nmodule (MHA) followed by a feed-forward module\n(FFN), with a residual connection around each. The\nvanilla single-head attention operates as\nAtt(Q,K,V ) = Softmax\n(QK⊤\n√\nd\n)\nV,\nwhere Q,K,V ∈Rl×d are d-dimensional vector\nrepresentations of lwords in sequences of queries,\nkeys and values. In MHA, the h-th attention head\nis parameterized by WQ\nh ,WK\nh ,WV\nh ∈Rd×dh as\nHh(q,x,W{Q,K,V}\nh ) = Att(qWQ\nh ,xWK\nh ,xWV\nh ),\nwhere q ∈Rl×d and x ∈Rl×d are the query and\nkey/value vectors. In MHA, H independently pa-\nrameterized attention heads are applied in parallel,\nand the outputs are aggregated by WO\nh ∈Rdh×d:\nMHA(q,x)=\nH∑\nh\nHh(q,x,W{Q,K,V}\nh )WO\nh .\nEach FFN module contains a two-layer fully con-\nnected network. Given the input embedding z, we\nlet FFN(z) denote the output of a FFN module.\n2.2 Structured and Unstructured LTHs\nLTH (Frankle and Carbin, 2018) has been widely\nexplored in various applications of deep learning\n(Brix et al., 2020; Movva and Zhao, 2020; Girish\net al., 2020). Most of existing results focus on\nﬁnding unstructured winning tickets via iterative\nmagnitude pruning and rewinding in randomly ini-\ntialized networks (Frankle et al., 2019; Renda et al.,\n2020), where each ticket is a single parameter. Re-\ncent works further investigate learning dynamics of\nthe tickets (Zhou et al., 2019; Frankle et al., 2020)\nand efﬁcient methods to identify them (You et al.,\n2019; Savarese et al., 2020). Besides training from\nscratch, researchers also explore the existence of\nwinning tickets under transfer learning regimes for\nover-parametrized pre-trained models across var-\nious tasks and datasets (Morcos et al., 2019; Yu\net al., 2019; Desai et al., 2019; Chen et al., 2020a).\nFor example, Chen et al. (2020b); Prasanna et al.\n(2020) have shown the existence of winning tickets\nwhen ﬁne-tuning BERT on downstream tasks.\nThere is also a surge of research exploring\nwhether certain structures, e.g., channels in convo-\nlutional layers and attention heads in Transformers,\nexhibit properties of the lottery tickets. Compared\nto unstructured tickets, training with structured tick-\nets is memory efﬁcient (Cao et al., 2019). Liu et al.\n(2018); Prasanna et al. (2020) suggest that there is\nno clear evidence that structured winning tickets\nexist in randomly initialized or pre-trained weights.\nPrasanna et al. (2020) observe that, in highly com-\npressed BERT (e.g., the percent of weight remain-\ning is around 50%), all tickets perform equally\nwell. However, Prasanna et al. (2020) have not\ninvestigated the cases where the percent of weight\nremaining is over 50%.\n3 Finding Super Tickets\nWe identify winning tickets in BERT through struc-\ntured pruning of attention heads and feed-forward\nlayers. Speciﬁcally, in each Transformer layer, we\nassociate mask variables ξh to each attention head\nand νto the FFN (Prasanna et al., 2020):\nMHA(Q,x) =\nH∑\nh\nξhHh(Q,x,W{Q,K,V}\nh )WO\nh ,\nFFN(z) = νFFN(z).\nHere, we set ξh,ν ∈{0,1}, and a 0 value indicates\nthat the corresponding structure is pruned.\nWe adopt importance score (Michel et al., 2019)\nas a gauge for pruning. In particular, the impor-\ntance score is deﬁned as the expected sensitivity\nof the model outputs with respect to the mask vari-\nables. Speciﬁcally, in each Transformer layer,\nIh\nMHA = E\nx∼Dx\n⏐⏐⏐⏐\n∂L(x)\n∂ξh\n⏐⏐⏐⏐,\nIFFN = E\nx∼Dx\n⏐⏐⏐⏐\n∂L(x)\n∂ν\n⏐⏐⏐⏐,\nwhere Lis a loss function and Dx is the data distri-\nbution. In practice, we compute the average over\nthe training set. We apply a layer-wise ℓ2 normal-\nization on the importance scores of the attention\nheads (Molchanov et al., 2016; Michel et al., 2019).\n6527\nThe importance score is closely tied to expres-\nsive power. A low importance score indicates that\nthe corresponding structure only has a small con-\ntribution towards the output. Such a structure has\nlow expressive power. On the contrary, a large\nimportance score implies high expressive power.\nWe compute the importance scores for all the\nmask variables in a single backward pass at the end\nof ﬁne-tuning. We perform one-shot pruning of the\nsame percent of heads and feed-forward layers with\nthe lowest importance scores. We conduct pruning\nmultiple times to obtain subnetworks, or winning\ntickets, at different compression ratios.\nWe adopt the weight rewinding technique in\nRenda et al. (2020): We reset the parameters of\nthe winning tickets to their values in the pre-trained\nweights, and subsequently ﬁne-tune the subnetwork\nwith the original learning rate schedule. The super\ntickets are selected as the winning tickets with the\nbest rewinding validation performance.\n4 Multi-task Learning with Tickets\nSharing\nIn multi-task learning, the shared model is highly\nover-parameterized to ensure a sufﬁcient capacity\nfor ﬁtting individual tasks. Thus, the multi-task\nmodel inevitably exhibits task-dependent redun-\ndancy when being adapted to individual tasks. Such\nredundancy induces a large model variance.\nWe propose to mitigate the aforementioned\nmodel redundancy by identifying task-speciﬁc\nsuper tickets to accommodate each task’s need.\nSpeciﬁcally, when viewing an individual task in\nisolation, the super tickets can tailor the multi-task\nmodel to strike an appealing balance between the\nmodel bias and variance (recall from Section 3 that\nsuper tickets retain sufﬁcient expressive power, yet\nkeep the model variance low). Therefore, we ex-\npect that deploying super tickets can effectively\ntame the model redundancy for individual tasks.\nGiven the super tickets identiﬁed by each task,\nwe exploit the multi-task information to reinforce\nﬁne-tuning. Speciﬁcally, we propose a tickets shar-\ning algorithm to update the parameters of the multi-\ntask model: For a certain network structure (e.g., an\nattention head), if it is identiﬁed as super tickets by\nmultiple tasks, then its weights are jointly updated\nby these tasks; if it is only selected by one speciﬁc\ntask, then its weights are updated by that task only;\notherwise, its weights are completely pruned. See\nFigure 2 for an illustration.\nFigure 2: Illustration of tickets sharing.\nIn more detail, we denote the weight parameters\nin the multi-task model as θ. Suppose there are N\ntasks. For each task i ∈{1,...,N }, we denote\nΩi = {ξi\nh,ℓ}H,L\nh=1,ℓ=1\n⋃{νi\nℓ}L\nℓ=1 as the collection of\nthe mask variables, where ℓis the layer index and\nh is the head index. Then the parameters to be\nupdated in task i are denoted as θi = M(θ,Ωi),\nwhere M(·,Ωi) masks the pruned parameters ac-\ncording to Ωi. We use stochastic gradient descent-\ntype algorithms to update θi. Note that the task-\nshared and task-speciﬁc parameters are encoded\nby the mask variable Ωi. The detailed algorithm is\ngiven in Algorithm 1.\nTickets sharing has two major difference com-\npared to Sparse Sharing (Sun et al., 2020): 1) Sun\net al. (2020) share winning tickets, while our strat-\negy focuses on super tickets, which can better gen-\neralize and strike a sensible balance between model\nbias and variance. 2) In tickets sharing, tickets are\nstructured and chosen from pre-trained weight pa-\nrameters. It does not require Multi-task Warmup,\nwhich is indispensable in Sun et al. (2020) to stabi-\nlize the sharing among unstructured tickets selected\nfrom randomly initialized weight parameters.\n5 Single Task Experiments\n5.1 Data\nGeneral Language Understanding Evaluation\n(GLUE, Wang et al. (2018)) is a standard bench-\nmark for evaluating model generalization perfor-\nmance. It contains nine NLU tasks, including ques-\ntion answering, sentiment analysis, text similarity\n6528\nAlgorithm 1 Tickets Sharing\nInput: Pre-trained base model parametersθ. Num-\nber of tasks N. Mask variables {Ωi}N\ni=1. Loss\nfunctions {Li}N\ni=1. Dataset D = ⋃N\ni=1 Di.\nNumber of epochs Tmax.\n1: for iin N do\n2: Initialize the super tickets for task i: θi =\nM(θ,Ωi).\n3: end for\n4: for epoch in 1,...,T max do\n5: Shufﬂe dataset D.\n6: for a minibatch bi of task iin Ddo\n7: Compute Loss Li(θi).\n8: Compute gradient ∇θLi(θi).\n9: Update θi using SGD-type algorithm.\n10: end for\n11: end for\nand textual entailment. Details about the bench-\nmark are deferred to Appendix A.1.1.\n5.2 Models & Training\nWe ﬁne-tune a pre-trained BERT model with task-\nspeciﬁc data to obtain a single task model. We ap-\npend a task-speciﬁc fully-connected layer to BERT\nas in Devlin et al. (2019).\n•ST-DNNBASE/LARGE is initialized with BERT-\nbase/large followed by a task-speciﬁc layer.\n•SuperTBASE/LARGE is initialized with the chosen\nset of super tickets in BERT-base/large followed\nby a task-speciﬁc layer. Speciﬁcally, we prune\nBERT-base/large in unit of 10% heads and 10%\nfeed-forward layers (FFN) at 8 different sparsity\nlevels (10% heads and 10% FFN, 20% heads and\n20% FFN, etc). Among them, the one with the best\nrewinding validation result is chosen as the set of\nsuper tickets. We randomly sample 10% GLUE\ndevelopment set for tickets selection.\nOur implementation is based on the MT-DNN\ncode base 3. We use Adamax (Kingma and Ba,\n2014) as our optimizer. We tune the learning rate\nin {5×10−5,1×10−4,2×10−4}and batch size in\n{8,16,32}. We train for a maximum of 6 epochs\nwith early-stopping. All training details are sum-\nmarized in Appendix A.1.2.\n5.3 Generalization of the Super Tickets\nWe conduct 5 trails of pruning and rewinding ex-\nperiments using different random seeds. Table 1\n3https://github.com/namisan/mt-dnn\nand 2 show the averaged evaluation results on the\nGLUE development and test sets, respectively. We\nremark that the gain of SuperTBASE/LARGE over ST-\nDNNBASE/LARGE is statistically signiﬁcant. All the\nresults4 have passed a paired student t-test with\np-values less than 0.05. More validation statistics\nare summarized in Appendix A.1.3.\nOur results can be summarized as follows.\n1) In all the tasks, SuperT consistently achieves\nbetter generalization than ST-DNN. The task-\naveraged improvement is around 0.9 over ST-\nDNNBASE and 1.0 over ST-DNNLARGE.\n0\n1\n2\n3\n4\nPerformance Gain\nSuperT-Large \nSuperT-Base\nRTE\n 2.49k\nMRPC\n 3.67k\nSTS-B\n 5.75k\nCoLA\n 8.55k\nSST-2\n 67.3k\nQNLI\n 108k\nQQP\n 364k\nMNLI\n 393k\nTask (Size)\n0.7\n0.8\n0.9\n1.0% of Weight Remaining\n0.82\n0.66\n0.84\n0.77 0.79\n0.89\n0.84\n0.92\n0.83 0.86 0.89 0.86 0.86\n0.93 0.93 0.93\nFigure 3: Single task ﬁne-tuning validation results\nin different GLUE tasks. Upper: Performance Gain.\nLower: Percent of weight remaining.\n2) Performance gain of the super tickets is more\nsigniﬁcant in small tasks. For example, in Ta-\nble 1, we obtain 3.3 points gain on RTE (2.5k\ndata), but only 0.4/0.3 on QQP (364k data) in the\nSuperTBASE experiments. Furthermore, from Fig-\nure 3, note that the super tickets are more heavily\ncompressed in small tasks, e.g., for SuperT BASE,\n83% weights remaining for RTE, but 93% for\nQQP. These observations suggest that for small\ntasks, model variance is large, and removing non-\nexpressive tickets reduces variance and improves\ngeneralization. For large tasks, model variance is\nlow, and all tickets are expressive to some extent.\n3) Performance of the super tickets is related\nto model size. Switching from SuperT BASE to\nSuperTLARGE, the percent of weights remaining\nshrinks uniformly across tasks, yet the generaliza-\ntion gains persist (Figure 3). This suggests that in\nlarge models, more non-expressive tickets can be\npruned without performance degradation.\n4Except for STS-B (SuperT BASE, Table 1), where the p-\nvalue is 0.37.\n6529\nRTE MRPC CoLA SST STS-B QNLI QQP MNLI-m/mm Average Average\nAcc Acc/F1 Mcc Acc P/S Corr Acc Acc/F1 Acc Score Compression\nST-DNNBASE 69.2 86.2/90.4 57.8 92.9 89.7/89.2 91.2 90.9/88.0 84.5/84.4 82.8 100%\nSuperTBASE 72.5 87.5/91.1 58.8 93.4 89.8/89.4 91.3 91.3/88.3 84.5/84.5 83.7 86.8%\nST-DNNLARGE 72.1 85.2/89.5 62.1 93.3 89.9/89.6 92.2 91.3/88.4 86.2/86.1 84.1 100%\nSuperTLARGE 74.1 88.0/91.4 64.3 93.9 89.9/89.7 92.4 91.4/88.5 86.5/86.2 85.1 81.7%\nTable 1: Single task ﬁne-tuning evaluation results on the GLUE development set. ST-DNN and SuperT results are\nthe averaged score over 5 trails with different random seeds.\nRTE MRPC CoLA SST STS-B QNLI QQP MNLI-m/mm Average Average\nAcc F1 Mcc Acc S Corr Acc F1 Acc Score Compression\nST-DNNBASE 66.4 88.9 52.1 93.5 85.8 90.5 71.2 84.6/83.4 79.6 100%\nSuperTBASE 69.6 89.4 54.3 94.1 86.2 90.5 71.3 84.6/83.8 80.4 86.8%\nTable 2: Single task ﬁne-tuning test set results scored using the GLUE evaluation server2. Results of ST-DNNBASE\nare from Devlin et al. (2019).\nFigure 4: Single task ﬁne-tuning evaluation results of\nthe winning (blue), the random (orange), and the losing\n(green) tickets on the GLUE development set under var-\nious sparsity levels.\nFigure 5: Phase transition under different randomly\nsampled training subsets. Note that the settings are the\nsame as Figure 4 (bottom left), except the data size.\n5.4 Phase Transition\nPhase transitions are shown in Figure 4. We plot\nthe evaluation results of the winning, the random,\nand the losing tickets under 8 sparsity levels us-\ning BERT-base and BERT-large. The winning\ntickets contain structures with the highest impor-\ntance scores. The losing tickets are selected re-\nversely, i.e., the structures with the lowest im-\nportance scores are selected, and high-importance\nstructures are pruned. The random tickets are sam-\npled uniformly across the network. We plot the\naveraged scores over 5 trails using different ran-\ndom seeds5. Phase transitions of all the GLUE\ntasks are in Appendix A.5.\nWe summarize our observations:\n1) The winning tickets are indeed the “winners”.\nIn Phase I and early Phase II, the winning tickets\nperform better than the full model and the random\ntickets. This demonstrates the existence of struc-\n5Except for MNLI, where we plot 3 trails as the there are\nless variance among trails.\n6530\nRTE MRPC CoLA SST STS-B QNLI QQP MNLI-m/mm Average Average\nAcc Acc/F1 Mcc Acc P/S Corr Acc Acc/F1 Acc Score Compression\nMT-DNNBASE 79.0 80.6/86.2 54.0 92.2 86.2/86.4 90.5 90.6/87.4 84.6/84.2 82.4 100%\n+ ST Fine-tuning 79.1 86.8/89.2 59.5 93.6 90.6/90.4 91.0 91.6/88.6 85.3/85.0 84.6 100%\nTicket-ShareBASE 81.2 87.0/90.5 52.0 92.7 87.7/87.5 91.0 90.7/87.5 84.5/84.1 83.3 92.9%\n+ ST Fine-tuning 83.0 89.2/91.6 59.7 93.5 91.1/91.0 91.9 91.6/88.7 85.0/85.0 85.6 92.9%\nMT-DNNLARGE 83.0 85.2/89.4 56.2 93.5 87.2/86.9 92.2 91.2/88.1 86.5/86.0 84.4 100%\n+ ST Fine-tuning 83.4 87.5/91.0 63.5 94.3 90.7/90.6 92.9 91.9/89.2 87.1/86.7 86.4 100%\nTicket-ShareLARGE 80.5 88.4/91.5 61.8 93.2 89.2/89.1 92.1 91.3/88.4 86.7/86.0 85.4 83.3%\n+ ST Fine-tuning 84.5 90.2/92.9 65.0 94.1 91.3/91.1 93.0 91.9/89.1 87.0/86.8 87.1 83.3%\nTable 3: Multi-task Learning evaluation results on the GLUE development set. Results ofMT-DNNBASE/LARGE with\nand without ST Fine-tuning are from Liu et al. (2020).\ntured winning tickets in lightly compressed BERT\nmodels, which Prasanna et al. (2020) overlook.\n2) Phase transition is pronounced over different\ntasks and models. Accuracy of the winning tickets\nincreases up till a certain compression ratio (Phase\nI); Passing the threshold, the accuracy decreases\n(Phase II), until its value intersects with that of\nthe random tickets (Phase III). Note that Phase\nIII agrees with the observations in Prasanna et al.\n(2020). Accuracy of the random tickets decreases\nin each phase. This suggests that model bias in-\ncreases steadily, since tickets with both low and\nhigh expressive power are discarded. Accuracy of\nthe losing tickets drops signiﬁcantly even in Phase\nI, suggesting that model bias increases drastically\nas highly expressive tickets are pruned.\n3) Phase transition is more pronounced in large\nmodels and small tasks. For example, in Figure 4,\nthe phase transition is more noticeable in BERT-\nlarge than in BERT-base, and is more pronounced\nin RTE (2.5k) and MRPC (3.7k) than in SST (67k)\nand MNLI (393k). The phenomenon becomes\nmore signiﬁcant for the same task when we only\nuse a part of the data, e.g., Figure 5 vs. Figure 4\n(bottom left).\n6 Multi-task Learning Experiments\n6.1 Model & Training\nWe adopt the MT-DNN architecture proposed in\nLiu et al. (2020). The MT-DNN model consists\nof a set of task-shared layers followed by a set of\ntask-speciﬁc layers. The task-shared layers take in\nthe input sequence embedding, and generate shared\nsemantic representations by optimizing multi-task\nobjectives. Our implementation is based on the\nMT-DNN code base. We follow the same training\nsettings in Liu et al. (2020) for multi-task learn-\ning, and in Section 5.2 for downstream ﬁne-tuning.\nMore details are summarized in Appendix A.2.\n•MT-DNNBASE/LARGE. An MT-DNN model re-\nﬁned through multi-task learning, with task-shared\nlayers initialized by pre-trained BERT-base/large.\n•MT-DNNBASE/LARGE + ST Fine-tuning. A sin-\ngle task model obtained by further ﬁne-tuning MT-\nDNN on an individual downstream task.\n•Ticket-ShareBASE/LARGE. An MT-DNN model\nreﬁned through the ticket sharing strategy, with\ntask-shared layers initialized by the union of the\nsuper tickets in pre-trained BERT-base/large.\n•Ticket-ShareBASE/LARGE + ST Fine-tuning. A\nﬁne-tuned single-task Ticket-Share model.\n6.2 Experimental Results\nTable 3 summarizes experimental results. The ﬁne-\ntuning results are averaged over5 trails using differ-\nent random seeds. We have several observations:\n1) Ticket-Share BASE and Ticket-Share LARGE\nachieve 0.9 and 1.0 gain in task-average score over\nMT-DNNBASE and MT-DNNLARGE, respectively.\nIn some small tasks (RTE, MRPC), Ticket-Share\nachieves better or on par results compared to MT-\nDNN+Fine-tuning. This suggests that by balancing\nthe bias and variance for different tasks, the multi-\ntask model’s variance is reduced. In large tasks\n(QQP, QNLI and MNLI), Ticket-Share behaves\nequally well with the full model. This is because\ntask-shared information is kept during pruning and\nstill beneﬁts multi-task learning.\n2) Ticket-Share BASE+Fine-tuning and Ticket-\nShareLARGE+Fine-tuning achieve 1.0 and 0.7 gains\nin task-average score over MT-DNN BASE+Fine-\ntuning and MT-DNNLARGE+Fine-tuning, respec-\ntively. This suggests that reducing the variance\nin the multi-task model beneﬁts ﬁne-tuning down-\nstream tasks.\n6531\nFigure 6: Illustration of tickets importance across tasks. Each ticket is represented by a pie chart. The size of a\npie indicates the Ticket Importance, where a larger pie suggests the ticket exhibits higher expressivity. Each task\nis represented by a color. The share of a color indicates the Task Share, where a even share suggests the ticket\nexhibits equal expressivity in all tasks.\nModel 0.1% 1% 10% 100%\nSNLI (Dev Acc%)\n# Training Data 549 5493 54k 549k\nMNLI-ST-DNNBASE 82.1 85.1 88.4 90.7\nMNLI-SuperTBASE 82.9 85.5 88.8 91.4\nMT-DNNBASE 82.1 85.2 88.4 91.1\nTicket-ShareBASE 83.3 85.8 88.9 91.5\nSciTail (Dev Acc%)\n# Training Data 23 235 23k 235k\nMNLI-ST-DNNBASE 80.6 88.8 92.0 95.7\nMNLI-SuperTBASE 82.9 89.8 92.8 96.2\nMT-DNNBASE 81.9 88.3 91.1 95.7\nTicket-ShareBASE 83.1 90.1 93.5 96.5\nTable 4: Domain adaptation evaluation results on SNLI\nand SciTail development set. Results of MT-DNNBASE\nare from Liu et al. (2020).\n7 Domain Adaptation\nTo demonstrate that super tickets can quickly gen-\neralize to new tasks/domains, we conduct few-shot\ndomain adaptation on out-of-domain NLI datasets.\n7.1 Data & Training\nWe brieﬂy introduce the target domain datasets.\nThe data and training details are summarized in\nAppendix A.3.1 and A.3.2, respectively.\nSNLI.The Stanford Natural Language Inference\ndataset (Bowman et al., 2015) is one of the most\nwidely used entailment dataset for NLI. It contains\n570k sentence pairs, where the premises are drawn\nfrom the captions of the Flickr30 corpus and hy-\npotheses are manually annotated.\nSciTail is a textual entailment dataset derived from\na science question answering (SciQ) dataset (Khot\net al., 2018). The hypotheses are created from\nscience questions, rendering SciTail challenging.\n7.2 Experimental Results\nWe consider domain adaptation on both single\ntask and multi-task super tickets. Speciﬁcally,\nwe adapt SuperTBASE and ST-DNNBASE from\nMNLI to SNLI/SciTail, and adapt the shared em-\nbeddings generated by Ticket-ShareBASE and by\nMT-DNNBASE to SNLI/SciTail. We adapt these\nmodels to 0.1%,1%,10% and 100% SNLI/SciTail\ntraining sets6, and evaluate the transferred models\non SNLI/SciTail development sets. Table 4 shows\nthe domain adaptation evaluation results. As we\ncan see, SuperT and Ticket-Share can better adapt\nto SNLI/SciTail than ST-DNN and MT-DNN, espe-\ncially under the few shot setting.\n6We use the subsets released in MT-DNN code base.\n6532\n8 Analysis\nSensitivity to Random Seed. To better demon-\nstrate that training with super tickets effectively\nreduces model variance, we evaluate models’ sen-\nsitivity to changes in random seeds during single\ntask ﬁne-tuning and multi-task downstream ﬁne-\ntuning. In particular, we investigate ﬁtting small\ntasks with highly over-parametrized models (vari-\nance is often large in these models, see Section 5\nand 6). As shown in Table 5, SuperT LARGE and\nTicket-ShareLARGE induce much smaller standard\ndeviation in validation results. Experimental details\nand further analyses are deferred to Appendix A.4.\nRTE MRPC CoLA STS-B SST-2\nST-DNNLARGE 1.17 0.61 1.32 0.16 0.17\nSuperTLARGE 0.72 0.20 0.97 0.07 0.16\nMT-DNNLARGE 1.43 0.78 1.14 0.15 0.18\nTicket ShareLARGE 0.99 0.67 0.81 0.08 0.16\nTable 5: Standard deviation of tasks in GLUE (dev)\nover 5 different random seeds.\nTickets Importance Across Tasks. We analyze\nthe importance score of each ticket computed in\ndifferent GLUE tasks. For each ticket, we compute\nthe importance score averaged over tasks as the\nTicket Importance, and the proportion of the task-\nspeciﬁc importance score out of the sum of all tasks’\nscores as the Task Share, as illustrated in Figure 6.\nWe observe that many tickets exhibit almost\nequal Task Shares for over 5 out of 8 tasks (Fig-\nure 6(a)(b)). While these tickets contribute to the\nknowledge sharing in the majority of tasks, they\nare considered non-expressive for tasks such as\nSST-2 (see Figure 6(a)(c)(d)). This explains why\nSST-2 beneﬁts little from tickets sharing. Further-\nmore, a small number of tickets are dominated by\na single task, e.g., CoLA (Figure 6(c)), or domi-\nnated jointly by two tasks, e.g., CoLA and STS-B\n(Figure 6(d)). This suggests that some tickets only\nlearn task-speciﬁc knowledge, and the two tasks\nmay share certain task-speciﬁc knowledge.\n9 Discussion\nStructured Lottery Tickets. LTH hypothesizes\nthat a subset of unstructured parameters can be\ntrained to match the full model’s performance. In-\nstead, we question whether a subset of structured\nweight matrices, e.g., FFN layers and attention\nheads, can also be trained to match the full model’s\nperformance. This question is more practically\nimportant than the unstructured one: training and\ninference on structured matrices are better opti-\nmized for hardware acceleration. Our results give\na positive answer to this question, while previous\nworks show that the structured tickets do not ex-\nist in highly compressed models (Prasanna et al.,\n2020).\nSearching Better Generalized Super Tickets.\nWe select winning tickets according to the sensitiv-\nity of the model outputs with respect to the mask\nvariables of each structure (Michel et al., 2019;\nPrasanna et al., 2020), as this measure is closely\ntied to the structure’s expressive power (Section 3).\nIn addition, we conduct an one-shot pruning for\ncomputational simplicity. We leave other impor-\ntance measures and pruning schedules, which may\nhelp identifying better generalized super tickets,\nfor future works (V oita et al., 2019; Behnke and\nHeaﬁeld, 2020; Wang et al., 2019; Fan et al., 2019;\nZhou et al., 2020; Sajjad et al., 2020).\nSearching Super Tickets Efﬁciently. Determin-\ning the compression ratio of the super tickets re-\nquires rewinding models at multiple sparsity levels.\nTo leverage super tickets in practice, a potential di-\nrection of research is to ﬁnd heuristics to determine\nthis ratio prior or early-on in training. We leave\nthis for future works.\n10 Conclusion\nWe study the behaviors of the structured lottery\ntickets in pre-trained BERT. We observe that the\ngeneralization performance of the winning tickets\nexhibits a phase transition phenomenon, suggesting\npruning can improve generalization when models\nare lightly compressed. Based on the observation,\nwe further propose a tickets sharing strategy to\nimprove multi-task ﬁne-tuning. Our analysis paves\nthe way for understanding the connection between\nmodel compression and generalization.\nBroader Impact\nThis paper studies the behavior of the structured\nlottery tickets in pre-trained language models. Our\ninvestigation neither introduces any social/ethical\nbias to the model nor ampliﬁes any bias in the data.\nWe do not foresee any direct social consequences or\nethical issues. Furthermore, our proposed method\nimproves performance through model compression,\nrendering it energy efﬁcient.\n6533\nReferences\nRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, and\nDanilo Giampiccolo. 2006. The second PASCAL\nrecognising textual entailment challenge. In Pro-\nceedings of the Second PASCAL Challenges Work-\nshop on Recognising Textual Entailment.\nMaximiliana Behnke and Kenneth Heaﬁeld. 2020. Los-\ning heads in the lottery: Pruning transformer atten-\ntion in neural machine translation. InProceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 2664–\n2674, Online. Association for Computational Lin-\nguistics.\nLuisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo\nGiampiccolo, and Bernardo Magnini. 2009. The\nﬁfth pascal recognizing textual entailment challenge.\nIn In Proc Text Analysis Conference (TAC’09).\nSamuel R Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\narXiv preprint arXiv:1508.05326.\nChristopher Brix, Parnia Bahar, and Hermann Ney.\n2020. Successfully applying the stabilized lottery\nticket hypothesis to the transformer architecture. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 3909–\n3915.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nShijie Cao, Chen Zhang, Zhuliang Yao, Wencong Xiao,\nLanshun Nie, Dechen Zhan, Yunxin Liu, Ming Wu,\nand Lintao Zhang. 2019. Efﬁcient and effective\nsparse lstm on fpga with bank-balanced sparsity.\nIn Proceedings of the 2019 ACM/SIGDA Interna-\ntional Symposium on Field-Programmable Gate Ar-\nrays, pages 63–72.\nDaniel Cer, Mona Diab, Eneko Agirre, I ˜nigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings of\nthe 11th International Workshop on Semantic Evalu-\nation (SemEval-2017), pages 1–14.\nTianlong Chen, Jonathan Frankle, Shiyu Chang,\nSijia Liu, Yang Zhang, Michael Carbin, and\nZhangyang Wang. 2020a. The lottery tickets hy-\npothesis for supervised and self-supervised pre-\ntraining in computer vision models. arXiv preprint\narXiv:2012.06908.\nTianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia\nLiu, Yang Zhang, Zhangyang Wang, and Michael\nCarbin. 2020b. The lottery ticket hypothesis\nfor pre-trained bert networks. arXiv preprint\narXiv:2007.12223.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2006. The pascal recognising textual entailment\nchallenge. In Proceedings of the First Inter-\nnational Conference on Machine Learning Chal-\nlenges: Evaluating Predictive Uncertainty Visual\nObject Classiﬁcation, and Recognizing Textual En-\ntailment, MLCW’05, pages 177–190, Berlin, Hei-\ndelberg. Springer-Verlag.\nShrey Desai, Hongyuan Zhan, and Ahmed Aly. 2019.\nEvaluating lottery tickets under distributional shifts.\nIn Proceedings of the 2nd Workshop on Deep Learn-\ning Approaches for Low-Resource NLP (DeepLo\n2019), pages 153–162.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nAngela Fan, Edouard Grave, and Armand Joulin. 2019.\nReducing transformer depth on demand with struc-\ntured dropout. arXiv preprint arXiv:1909.11556.\nJonathan Frankle and Michael Carbin. 2018. The lot-\ntery ticket hypothesis: Finding sparse, trainable neu-\nral networks. arXiv preprint arXiv:1803.03635.\nJonathan Frankle, Gintare Karolina Dziugaite,\nDaniel M Roy, and Michael Carbin. 2019. Stabi-\nlizing the lottery ticket hypothesis. arXiv preprint\narXiv:1903.01611.\nJonathan Frankle, David J Schwab, and Ari S Morcos.\n2020. The early phase of neural network training.\narXiv preprint arXiv:2002.10365.\nJerome Friedman, Trevor Hastie, Robert Tibshirani,\net al. 2001. The elements of statistical learning, vol-\nume 1. Springer series in statistics New York.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand Bill Dolan. 2007. The third PASCAL recogniz-\ning textual entailment challenge. In Proceedings of\nthe ACL-PASCAL Workshop on Textual Entailment\nand Paraphrasing, pages 1–9, Prague. Association\nfor Computational Linguistics.\nSharath Girish, Shishira R Maiya, Kamal Gupta, Hao\nChen, Larry Davis, and Abhinav Shrivastava. 2020.\nThe lottery ticket hypothesis for object recognition.\narXiv preprint arXiv:2012.04643.\nTushar Khot, Ashish Sabharwal, and Peter Clark. 2018.\nScitail: A textual entailment dataset from science\nquestion answering. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, volume 32.\n6534\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019. Multi-task deep neural networks\nfor natural language understanding. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4487–4496.\nXiaodong Liu, Yu Wang, Jianshu Ji, Hao Cheng,\nXueyun Zhu, Emmanuel Awa, Pengcheng He,\nWeizhu Chen, Hoifung Poon, Guihong Cao, et al.\n2020. The microsoft toolkit of multi-task deep\nneural networks for natural language understanding.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics: System\nDemonstrations, pages 118–126.\nZhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang,\nand Trevor Darrell. 2018. Rethinking the value of\nnetwork pruning. arXiv preprint arXiv:1810.05270.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? In Ad-\nvances in Neural Information Processing Systems ,\npages 14014–14024.\nPavlo Molchanov, Stephen Tyree, Tero Karras, Timo\nAila, and Jan Kautz. 2016. Pruning convolutional\nneural networks for resource efﬁcient inference.\narXiv preprint arXiv:1611.06440.\nAri S Morcos, Haonan Yu, Michela Paganini, and Yuan-\ndong Tian. 2019. One ticket to win them all: gen-\neralizing lottery ticket initializations across datasets\nand optimizers. arXiv preprint arXiv:1906.02773.\nRajiv Movva and Jason Y Zhao. 2020. Dissecting lot-\ntery ticket transformers: Structural and behavioral\nstudy of sparse neural machine translation. arXiv\npreprint arXiv:2009.13270.\nSai Prasanna, Anna Rogers, and Anna Rumshisky.\n2020. When bert plays the lottery, all tickets are\nwinning. arXiv preprint arXiv:2005.00561.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nAlex Renda, Jonathan Frankle, and Michael Carbin.\n2020. Comparing rewinding and ﬁne-tuning\nin neural network pruning. arXiv preprint\narXiv:2003.02389.\nHassan Sajjad, Fahim Dalvi, Nadir Durrani, and\nPreslav Nakov. 2020. Poor man’s bert: Smaller\nand faster transformer models. arXiv preprint\narXiv:2004.03844.\nPedro Savarese, Hugo Silva, and Michael Maire. 2020.\nWinning the lottery with continuous sparsiﬁcation.\nAdvances in Neural Information Processing Systems,\n33.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing ,\npages 1631–1642.\nTianxiang Sun, Yunfan Shao, Xiaonan Li, Pengfei Liu,\nHang Yan, Xipeng Qiu, and Xuanjing Huang. 2020.\nLearning sparse sharing architectures for multiple\ntasks. In Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, volume 34, pages 8936–8943.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. arXiv preprint arXiv:1706.03762.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-\nhead self-attention: Specialized heads do the heavy\nlifting, the rest can be pruned. arXiv preprint\narXiv:1905.09418.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nZiheng Wang, Jeremy Wohlwend, and Tao Lei. 2019.\nStructured pruning of large language models. arXiv\npreprint arXiv:1910.04732.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122. Association for\nComputational Linguistics.\nHaoran You, Chaojian Li, Pengfei Xu, Yonggan Fu,\nYue Wang, Xiaohan Chen, Richard G Baraniuk,\nZhangyang Wang, and Yingyan Lin. 2019. Drawing\nearly-bird tickets: Towards more efﬁcient training of\ndeep networks. arXiv preprint arXiv:1909.11957.\n6535\nHaonan Yu, Sergey Edunov, Yuandong Tian, and Ari S\nMorcos. 2019. Playing the lottery with rewards\nand multiple languages: lottery tickets in rl and nlp.\narXiv preprint arXiv:1906.02768.\nHattie Zhou, Janice Lan, Rosanne Liu, and Jason\nYosinski. 2019. Deconstructing lottery tickets: Ze-\nros, signs, and the supermask. arXiv preprint\narXiv:1905.01067.\nWangchunshu Zhou, Tao Ge, Ke Xu, Furu Wei, and\nMing Zhou. 2020. Scheduled drophead: A regu-\nlarization method for transformer models. arXiv\npreprint arXiv:2004.13342.\n6536\nA Appendix\nA.1 Single Task Experiments\nA.1.1 Data\nGLUE. GLUE is a collection of nine NLU tasks.\nThe benchmark includes question answering (Ra-\njpurkar et al., 2016), linguistic acceptability (CoLA,\nWarstadt et al. 2019), sentiment analysis (SST,\nSocher et al. 2013), text similarity (STS-B, Cer\net al. 2017), paraphrase detection (MRPC, Dolan\nand Brockett 2005), and natural language inference\n(RTE & MNLI, Dagan et al. 2006; Bar-Haim et al.\n2006; Giampiccolo et al. 2007; Bentivogli et al.\n2009; Williams et al. 2018) tasks. Details of the\nGLUE benchmark, including tasks, statistics, and\nevaluation metrics, are summarized in Table 9.\nA.1.2 Training\nWe use Adamax as the optimizer. A linear learning\nrate decay schedule with warm-up over 0.1 is used.\nWe apply a gradient norm clipping of 1. We set\nthe dropout rate of all task speciﬁc layers as 0.1,\nexcept 0.3 for MNLI and 0.05 for CoLA. All the\ntexts were tokenized using wordpieces, and were\nchopped to spans no longer than 512 tokens. All\nexperiments are conducted on Nvidia V100 GPUs.\nA.1.3 Evaluation Results Statistics\nWe conduct 5 sets of experiments on different ran-\ndom seeds. Each set of experiment consists of\nﬁne-tuning, pruning, and rewinding at 8 sparsity\nlevels. For results on GLUE dev set (Table 1), we\nreport the average score of super tickets rewinding\nresults over 5 sets of experiments. The standard\ndeviation of the results is shown in Table 6. The\nstatistics of the percent of weight remaining in the\nselected super tickets are shown in Table 7.\nFor results on GLUE test set (Table 2), as the\nevaluation server sets an limit on submission times,\nwe only evaluate the test prediction under a sin-\ngle random seed that gives the best task-average\nvalidation results.\nA.2 Multi-task Learning Experiments\nA.2.1 Multi-task Model Training\nWe adopt the MT-DNN code base and adopt the\nexact optimization settings in Liu et al. (2020). We\nuse Adamax as our optimizer with a learning rate\nof 5 ×10−5 and a batch size of 32. We train for a\nmaximum number of epochs of 5 with early stop-\nping. A linear learning rate decay schedule with\nwarm-up over 0.1 was used. The dropout rate of all\nthe task speciﬁc layers is set to be 0.1, except 0.3\nfor MNLI and 0.05 for CoLa. We clipped the gra-\ndient norm within 1. All the texts were tokenized\nusing wordpieces, and were chopped to spans no\nlonger than 512 tokens.\nWorth mentioning, the task-speciﬁc super tickets\nused in Ticket Share are all selected during the case\nwhere a matched learning rate (i.e., 5 ×10−5) is\nused in single task ﬁne-tuning. We empirically ﬁnd\nthat, rewinding the super tickets selected under a\nmatched optimization settings usually outperforms\nthose selected under a mismatched settings (i.e. us-\ning two different learning rates in single-task ﬁne-\ntuning and rewinding/multi-task learning). This\nagrees with previous observation in literature of\nLottery Ticket Hypothesis, which shows that un-\nstructured winning tickets are not only related to its\nweight initialization, but also model optimization\npath.\nA.2.2 Multi-task Model Downstream\nFine-tuning\nWe follow the exact optimization setting as in Sec-\ntion 5.2 and in Section A.1.2, except we choose\nlearning rate in {1×10−5,2×10−5,5×10−5,1×\n10−4,2×10−4}, and choose the dropout rate of all\ntask speciﬁc layers in {0.05,0.1,0.2,0.3}.\nA.3 Domain Adaptation Experiments\nA.3.1 Data\nSNLI. is one of the most widely used entailment\ndataset for NLI.\nSciTail involves assessing whether a given premise\nentails a given hypothesis. In contrast to other en-\ntailment datasets, the hypotheses in SciTail is cre-\nated from science questions. These sentences are\nlinguistically challenging. The corresponding an-\nswer candidates and premises come from relevant\nweb sentences. The lexical similarity of premise\nand hypothesis is often high, making SciTail partic-\nularly challenging.\nDetails of the SNLI and SciTail, including tasks,\nstatistics, and evaluation metrics, are summarized\nin Table 9.\nA.3.2 Training\nFor single task model domain adaptation from\nMNLI to SNLI/SciTail, we follow the exact op-\ntimization setting as in Section 5.2 and in Sec-\ntion A.1.2, except we choose the learning rate in\n{5 ×10−5,1 ×10−4,5 ×10−4}.\n6537\nRTE MRPC CoLA STS-B SST-2 QNLI QQP MNLI\nSuperTBASE 0.91 0.74 1.51 0.49 0.50 0.10 0.08 0.04\nSuperTLARGE 0.72 0.20 0.97 0.07 0.16 0.07 0.11 0.02\nTable 6: Standard deviation of the evaluation results on GLUE development set over5 different random seeds.\nRTE MRPC CoLA STS-B SST-2 QNLI QQP MNLI\nSuperTBASE (Mean) 0.83 0.86 0.89 0.86 0.93 0.93 0.93 0.93\nSuperTBASE (Std Dev) 0.07 0.08 0.04 0.06 0.07 0.00 0.00 0.00\nSuperTLARGE (Mean) 0.82 0.66 0.84 0.77 0.79 0.90 0.84 0.92\nSuperTLARGE (Std Dev) 0.04 0.04 0.00 0.10 0.05 0.03 0.00 0.00\nTable 7: Statistics of the percent of weight remaining of the selected super tickets over 5 different random seeds.\nA.4 Sensitivity Analysis\nA.4.1 Randomness Analysis\nFor single task experiments in Table 5, we vary\nthe random seeds only and keep all other hyper-\nparameters ﬁxed. We present the standard deviation\nof the validation results over 5 trails rewinding ex-\nperiments. For multi-task downstream ﬁne-tuning\nexperiments, we present the standard deviation of\nthe validation results over 5 trails, each result aver-\naged over learning rates in{5×10−5,1×10−4,2×\n10−4}. This is because the downstream ﬁne-tuning\nperformance is more sensitive to hyper-parameters.\nA.4.2 Hyper-parameter Analysis\nWe further analyze the sensitivity of Ticket\nShareLARGE model to changes in hyper-parameters\nin downstream ﬁne-tuning in some GLUE tasks.\nWe vary the learning rate in {5 ×10−5,1 ×\n10−4,2×10−4}and keep all other hyper-parameter\nﬁxed. Table 8 shows the standard deviation of the\nvalidation results over different learning rates, each\nresult averaged over 5 different random seeds. As\ncan be seen, Task ShareLARGE exhibits stronger ro-\nbustness to changes in learning rate in downstream\nﬁne-tuning.\nRTE MRPC CoLA STS-B SST-2\nMT-DNNLARGE 1.26 0.86 1.05 0.42 0.26\nTicket ShareLARGE 0.44 0.58 0.61 0.36 0.25\nTable 8: Standard deviation of some tasks in GLUE\n(dev) over 3 different learning rates.\nA.5 Phase Transition on GLUE Tasks\nFigure 7 shows the phase transition plots on win-\nning tickets on GLUE tasks absent from Figure 4.\nAll experimental settings conform to Figure 4.\nBERT-base 94 BERT-large \n92 \n92 \n>,90 u 90 � ro \nCl !3 88 08 88 \n< 86\n86 \n84 \n84 1.0 0.8 0.6 0.4 1.0 0.8 0.6 0.4 \n92 92 \n>,90 u 90 ,..... ro \n� 8 88\n88 Cl u \n< 86 86 \n84 \n84 \n1.0 0.8 0.6 0.4 1.0 0.8 0.6 0.4 \n70 \n70 \n60 >,u 60 < ro \n� !3 50 0 U \nu u 50 < 40 \n40 \n30 1.0 0.8 0.6 0.4 1.0 0.8 0.6 0.4 \n90.0 90.0 \n>, \na::l � 87.5 87.5 \nI \n!3rn u\n� < 85.0 85.0 \n82.5 82.5 \n80.0 80.0 1.0 0.8 0.6 0.4 1.0 0.8 0.6 0.4 \nPercent of Weight Remaining Percent of Weight Remaining \nFigure 7: Single task ﬁne-tuning evaluation results of\nthe winning tickets on the GLUE development set un-\nder various sparsity levels.\n6538\nCorpus Task #Train #Dev #Test #Label Metrics\nSingle-Sentence Classiﬁcation (GLUE)\nCoLA Acceptability 8.5k 1k 1k 2 Matthews corr\nSST Sentiment 67k 872 1.8k 2 Accuracy\nPairwise Text Classiﬁcation (GLUE)\nMNLI NLI 393k 20k 20k 3 Accuracy\nRTE NLI 2.5k 276 3k 2 Accuracy\nQQP Paraphrase 364k 40k 391k 2 Accuracy/F1\nMRPC Paraphrase 3.7k 408 1.7k 2 Accuracy/F1\nQNLI QA/NLI 108k 5.7k 5.7k 2 Accuracy\nText Similarity (GLUE)\nSTS-B Similarity 7k 1.5k 1.4k 1 Pearson/Spearman corr\nPairwise Text Classiﬁcation\nSNLI NLI 549k 9.8k 9.8k 3 Accuracy\nSciTail NLI 23.5k 1.3k 2.1k 2 Accuracy\nTable 9: Summary of the GLUE benchmark, SNLI and SciTail.",
  "topic": "Chen",
  "concepts": [
    {
      "name": "Chen",
      "score": 0.8688160181045532
    },
    {
      "name": "Generalization",
      "score": 0.6911629438400269
    },
    {
      "name": "Computer science",
      "score": 0.6136225461959839
    },
    {
      "name": "Natural language processing",
      "score": 0.5734332799911499
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.5019123554229736
    },
    {
      "name": "Computational linguistics",
      "score": 0.49012264609336853
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46611693501472473
    },
    {
      "name": "Linguistics",
      "score": 0.4446139931678772
    },
    {
      "name": "Joint (building)",
      "score": 0.4334469437599182
    },
    {
      "name": "Speech recognition",
      "score": 0.37180662155151367
    },
    {
      "name": "Mathematics",
      "score": 0.16604822874069214
    },
    {
      "name": "Engineering",
      "score": 0.15582934021949768
    },
    {
      "name": "Philosophy",
      "score": 0.09177151322364807
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}