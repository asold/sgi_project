{
  "title": "Synergistic Integration of Large Language Models and Cognitive Architectures for Robust AI: An Exploratory Analysis",
  "url": "https://openalex.org/W4391116688",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2502684515",
      "name": "Oscar J. Romero",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2145936579",
      "name": "John Zimmerman",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A932414507",
      "name": "Aaron Steinfeld",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A1966690917",
      "name": "Anthony Tomasic",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6637441467",
    "https://openalex.org/W6839366901",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W6646483767",
    "https://openalex.org/W2883445328",
    "https://openalex.org/W6614489337",
    "https://openalex.org/W2779578326",
    "https://openalex.org/W4221159132",
    "https://openalex.org/W2613869211",
    "https://openalex.org/W3005742798",
    "https://openalex.org/W6638782989",
    "https://openalex.org/W4391116610",
    "https://openalex.org/W6852454441",
    "https://openalex.org/W3184152240",
    "https://openalex.org/W1966109336",
    "https://openalex.org/W2505184925",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W6839112312",
    "https://openalex.org/W419646410",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4362703956",
    "https://openalex.org/W4378505261",
    "https://openalex.org/W4321855256",
    "https://openalex.org/W4377130677",
    "https://openalex.org/W4304194220",
    "https://openalex.org/W4379347837",
    "https://openalex.org/W4319452268",
    "https://openalex.org/W4283170666",
    "https://openalex.org/W1679907412",
    "https://openalex.org/W4385571421",
    "https://openalex.org/W4366850747",
    "https://openalex.org/W4321177655",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2166958329",
    "https://openalex.org/W4378509427",
    "https://openalex.org/W4283263983",
    "https://openalex.org/W4378711660",
    "https://openalex.org/W1984917351",
    "https://openalex.org/W4309591663",
    "https://openalex.org/W4385573164",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W2968297680",
    "https://openalex.org/W4377371478"
  ],
  "abstract": "This paper explores the integration of two AI subdisciplines employed in the development of artificial agents that exhibit intelligent behavior: Large Language Models (LLMs) and Cognitive Architectures (CAs). We present three integration approaches, each grounded in theoretical models and supported by preliminary empirical evidence. The modular approach, which introduces four models with varying degrees of integration, makes use of chain-of-thought prompting, and draws inspiration from augmented LLMs, the Common Model of Cognition, and the simulation theory of cognition. The agency approach, motivated by the Society of Mind theory and the LIDA cognitive architecture, proposes the formation of agent collections that interact at micro and macro cognitive levels, driven by either LLMs or symbolic components. The neuro-symbolic approach, which takes inspiration from the CLARION cognitive architecture, proposes a model where bottom-up learning extracts symbolic representations from an LLM layer and top-down guidance utilizes symbolic representations to direct prompt engineering in the LLM layer. These approaches aim to harness the strengths of both LLMs and CAs, while mitigating their weaknesses, thereby advancing the development of more robust AI systems. We discuss the tradeoffs and challenges associated with each approach.",
  "full_text": "Synergistic Integration of Large Language Models and Cognitive Architectures\nfor Robust AI: An Exploratory Analysis\nOscar J. Romero, John Zimmerman, Aaron Steinfeld, Anthony Tomasic\nCarnegie Mellon University\noscarr@andrew.cmu.edu, johnz@andrew.cmu.edu, steinfeld@cmu.edu, tomasic@andrew.cmu.edu\nAbstract\nThis paper explores the integration of two AI subdisciplines\nemployed in the development of artificial agents that ex-\nhibit intelligent behavior: Large Language Models (LLMs)\nand Cognitive Architectures (CAs). We present three inte-\ngration approaches, each grounded in theoretical models and\nsupported by preliminary empirical evidence. The modular\napproach, which introduces four models with varying de-\ngrees of integration, makes use of chain-of-thought prompt-\ning, and draws inspiration from augmented LLMs, the Com-\nmon Model of Cognition, and the simulation theory of cogni-\ntion. The agency approach, motivated by the Society of Mind\ntheory and the LIDA cognitive architecture, proposes the for-\nmation of agent collections that interact at micro and macro\ncognitive levels, driven by either LLMs or symbolic compo-\nnents. The neuro-symbolic approach, which takes inspiration\nfrom the CLARION cognitive architecture, proposes a model\nwhere bottom-up learning extracts symbolic representations\nfrom an LLM layer and top-down guidance utilizes sym-\nbolic representations to direct prompt engineering in the LLM\nlayer. These approaches aim to harness the strengths of both\nLLMs and CAs, while mitigating their weaknesses, thereby\nadvancing the development of more robust AI systems. We\ndiscuss the tradeoffs and challenges of each approach.\nIntroduction\nPre-trained Large Language Models (LLMs) like ChatGPT,\nGPT-4, and PaLM 2 are generative models that excel in a\nvariety of natural language tasks (Brown et al. 2020; Devlin\net al. 2019) and even show promise in interactive decision-\nmaking (Li et al. 2022), reasoning (Diao et al. 2023; Xie\net al. 2023; Yao et al. 2023b), and modeling aspects of ar-\ntificial general intelligence (AGI) (Kosinski 2023; Bubeck\net al. 2023). However, LLMs face interpretability, consis-\ntency, and scalability issues (Mialon et al. 2023), partly\ndue to limitations in context window size and sensitivity to\nprompt structure as they often rely on precise and carefully\nengineered instructions (Wei et al. 2022). They’re criticized\nfor being stochastic parrots and lacking detailed reasoning\nexplanations (Bender et al. 2021). Hallucinations (Welleck\net al. 2019; Qian et al. 2022; Wei et al. 2022) and biases\n(Weidinger et al. 2022; Venkit, Srinath, and Wilson 2022)\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nare further concerns, affecting trustworthiness and ethical\naspects (Huang et al. 2023). The dependence on larger mod-\nels for better performance raises resource challenges (Mi-\nalon et al. 2023), and scalable LLMs incorporating continual\nlearning are still an open question (Scialom et al. 2022).\nIn contrast, Cognitive Architectures (CAs) propose hy-\npotheses about the fixed structures governing the operation\nof minds, whether in natural or artificial systems, facilitat-\ning intelligent behavior in complex environments (Laird,\nLebiere, and Rosenbloom 2017). CAs like ACT-R (Ander-\nson and Lebiere 2014), SOAR (Laird 2019), CLARION\n(Sun 2016), and LIDA (Franklin and Patterson 2006) model\nvarious human cognitive aspects: memory, learning, reason-\ning, perceptual-motor interaction, theory of mind, and AGI\n(Kotseruba and Tsotsos 2020). CAs prioritize bounded ratio-\nnality, striving for satisfactory decisions under resource con-\nstraints, diverging from LLMs’ pursuit of optimality. How-\never, CAs face challenges in knowledge representation and\nscalability, that is, the knowledge processed by a cognitive\nagent1 is typically tailored for specific domains and tasks\n(Lieto, Lebiere, and Oltramari 2018).\nUnlike humans, CAs struggle with complex knowledge\nand their actions are confined to manually curated proce-\ndural knowledge (Park et al. 2023). According to (Mar-\ncus 2020), LLMs struggle to derive cognitive models from\ndiscourse and lack capabilities to reason over those cogni-\ntive models2. Hence, CAs could play a pivotal role in ei-\nther augmenting or leveraging LLMs by contributing to the\ncreation and dynamic updating of cognitive models. Like-\nwise, cognitive models could be leveraged to better interpret\nLLMs’ black-box learning algorithms and decision-making\nprocesses (Binz and Schulz 2023). Both LLMs and CAs\nhave made valuable and sound contributions to the construc-\ntion of complex autonomous AI agents; however, each ap-\nproach has its strengths and weaknesses (as summarized on\nTable 1). Thus, the main contribution of this work lies in\ncharacterizing plausible approaches to integrating CAs and\nLLMs, viewing them through a hybrid and synergetic lens.\n1Hereafter, consider a cognitive agent as an artificial agent con-\nstructed on a particular CA.\n2A cognitive model should at least include information about\nthe entities in the external world, their properties, and their relation-\nships with other entities, as well as the modeling of the cognitive\nprocesses that operate over those entities (Marcus 2020).\nAAAI Fall Symposium Series (FSS-23)\n396\nFeature LLMs CAs\nLanguage processing ++ -+\nWorld knowledge ++ -+\nReasoning -+ ++\nSymbolic processing -+ ++\nConnectionist processing ++ -+\nKnowledge scalability +- -+\nPlanning -+ +-\nLearning – +-\nMemory management – ++\nConsistency (no hallucinations) -+ ++\nTable 1: Feature comparison between LLMs and CAs. (++)\nFully supported. (+-) Almost always supported. (-+) Some-\ntimes supported. (–) Rarely (or not) supported.\nRelevant Work\nChain-of-thought prompting (CoT):CoT prompting (Mi-\nalon et al. 2023; Diao et al. 2023) enhances LLM reasoning,\nleading to improved performance in various reasoning and\nnatural language processing tasks. CoT breaks down multi-\nstep problems into intermediate steps, enabling the model\nto address reasoning problems. ReAct (Yao et al. 2023b)\ncombines both reasoning (CoT prompts) and action (ac-\ntion plan generation). It organizes a workflow that decom-\nposes task goals, injects task-relevant knowledge, extracts\nimportant observation components, and refines action plans\nbased on feedback. Auto-CoT (Zhang et al. 2022) proposes\na model that samples questions with diversity and automat-\nically generates demonstrations to correct mistakes in rea-\nsoning chains. The approaches we propose in this paper as-\nsume using CoT for problem decomposition, allowing a CA\nto inject its output into each reasoning step.\nAugmented Language Models: it combines enhanced\nreasoning skills of an LLM with tools like APIs, DBs, and\ncode interpreters for improved knowledge retrieval, reason-\ning, and action execution (Mialon et al. 2023). Program-\nAided Language model (PAL) (Gao et al. 2023) reads natu-\nral language problems, generates intermediate programs for\nreasoning, and delegates the solution step to a Python inter-\npreter. Toolformer (Schick et al. 2023) is a model trained to\ndecide which APIs to call, when to call them, what argu-\nments to pass, and how to best incorporate the results into\nfuture token prediction. Our modular approach extends the\nidea of augmenting an LLM with cognitive processing and\nassumes the usage of external APIs.\nCAs and LLMs: Generative Agents (Park et al. 2023)\nis a model that uses a cognitive architecture and an LLM\nto generate realistic behavior. It defines components for\nmemory, self-reflection, and planning. This approach dif-\nfers from ours in that components do not manipulate sym-\nbol representations but simply use unstructured natural lan-\nguage. OlaGPT (Xie et al. 2023) is an LLM cognition frame-\nwork aiming to solve reasoning problems with human-like\nproblem-solving abilities by leveraging CoT. OlaGPT pro-\nposes to approximate cognitive modules, such as attention,\nmemory, learning, reasoning, action selection, and decision-\nmaking. The first case of our modular approach resembles\nOlaGPT to some extent.\nIn (Mitsopoulos et al. 2023), an agent-driven approach is\nproposed to simulate human behavior in an epidemiologi-\ncal network, which integrates LLMs to enhance a cognitive\narchitecture with human-like linguistic outputs such as at-\ntitudes, beliefs, sentiments, and stances. These factors influ-\nence how individuals behave in the real world, ultimately en-\nriching the creation of agents simulating human roles, as we\ndiscussed later in the macro-level of our agency approach.\nOpen-source experimental applications like Auto-GPT 3\nand BabyAGI 4 aim to advance AGI. Auto-GPT manages\nlong-term and short-term memory, language generation, and\nsummarization. BabyAGI uses LLM chains to perform tasks\nbased on goals. These approaches hold significant poten-\ntial and are likely to integrate further with human cognition\nmodeling. Although with not a strict commitment to model\na cognitive architecture, V oyager (Wang et al. 2023) facil-\nitates continual learning through an evolving code library\nfor complex behaviors. An iterative prompting mechanism\nincorporates feedback, errors, and self-verification for pro-\ngram improvement. (LeCun 2022) outlines the considera-\ntions for crafting a cognitive architecture using energy min-\nimization mechanisms, enabling reasoning, prediction, and\nmulti-scale planning. They emphasize that while determin-\nistic generative architectures withstand energy distribution\nissues, non-deterministic structures like auto-encoders and\njoint embeddings are susceptible to collapse.\nIntegration Approaches\nIn this section, we propose and discuss the tradeoffs of three\ndifferent approaches for the integration of CAs and LLMs:\nthe modular approach, the agency approach, and the neuro-\nsymbolic approach. To illustrate the practical implementa-\ntion of each approach, we base our examples on a scenario\ninvolving a cognitive agent designed to assist people with\nvisual impairments in everyday tasks such as navigation and\nexploration of indoor environments, effective use of public\ntransportation, etc. The agent operates on a smartphone de-\nvice, utilizing sensor data processing, computer vision for\nobject detection, and speech recognition to perceive its en-\nvironment. Its actions encompass language generation and\ninvocation of external APIs. The agent engages in conver-\nsation with its user, reasons about their needs and requests,\nconstructs shared mental models to achieve goals effectively,\nand makes decisions that unfold in the short and long term.\nFor the remainder of this paper, let us consider that the\ninputs of an LLM can be multimodal, involving text and\nimages, while the outputs are exclusively text-based. Con-\nversely, for the sake of simplicity, CAs’ inputs and outputs\nare limited to formatted text, although, in practice, various\nCAs can process diverse modalities. As a reference frame-\nwork for CAs’ structure, our approach adopts the Common\nModel of Cognition (CMC) (Laird, Lebiere, and Rosen-\nbloom 2017), which captures a consensus regarding the\nstructures and processes that resemble those found in human\ncognition. CMC defines five high-level modules, including\n3https://github.com/Significant-Gravitas/Auto-GPT\n4https://github.com/yoheinakajima/babyagi\n397\nperception, motor, working memory, declarative long-term\nmemory, and procedural long-term memory, each of which\ncan be further decomposed into multiple sub-modules. Be-\nhavior in the CMC is organized around a cognitive cycle\ndriven by procedural memory, with complex behavior (e.g.,\nreasoning, planning, etc.) emerging as sequences of such cy-\ncles. In each cognitive cycle, the system senses the current\nsituation, interprets it with respect to ongoing goals, and then\nselects an internal or external action in response. Both the\nagency and the neuro-symbolic approaches use different ref-\nerence frames, which will be discussed later.\nModular Approach A straightforward way to integrate\nLLMs and CAs is using a modular approach where either (1)\nLLMs partially enhance the performance of certain modules\nand components of a CA, or (2) a CA augments an LLM by\ninjecting reasoning traces and contents from memories into\nthe prompting process. Figure 1 depicts 4 different cases of\nmodular integration. This integration allows modules to be\neasily replaced by LLMs or their CA module counterparts.\nCase (a) assumes a recursive prompting scenario (Mialon\net al. 2023) where an LLM decomposes a complex problem\ninto subproblems, and the intermediate outputs are aggre-\ngated to generate a final output. In this case, a CA could\nbe used to prime every intermediate step at the LLM with\nreasoning traces from procedural knowledge as well as rel-\nevant content from memories. The mechanism would be as\nfollows: given an initial inputi0 (e.g., a user’s request, exter-\nnal signals, etc.), the LLM generates an intermediate output\no0 (e.g., the first step towards the solution of the user’s re-\nquest) and a set of equivalent symbolic structures for both\nthe input, si0 (e.g., intents, entities, and properties recog-\nnized from the input), and the output, so0 (e.g., symbolic\nrepresentation of LLM’s actions and reasoning steps)5. The\nCA uses those symbolic structures as inputs and executes\none or several cognitive cycles, after which, the contents of\nthe working memory (w0), including fired productions, rel-\nevant information from declarative memories, and actions,\nare injected as cues into the next intermediate step of the\nLLM. The process repeats until a final output is generated.\nConsider this streamlined example: A bus rider employs\nthe term “downtown” ambiguously as the destination. De-\npending on the day of the week, the user may refer to two\nspecific places in the downtown area, namely the work-\nplace or the gym. The cognitive agent uses a combination of\ndeclarative and procedural knowledge to discern the user’s\ntransit patterns and effectively disambiguate the term down-\ntown. The triggered productions and relevant contents of the\nworking memory are subsequently integrated into the next\nLLM’s recursive step, thereby guiding and enhancing its rea-\nsoning process with new information.\n[i0] User: \"when is my next bus to downtown coming?\"\n[o0] search bus schedule with destination\ndowntown\n[si0] (intent, (inform, destination, downtown))\n[os0] (step, (search, orig, X, destination, downtown))\n[w0] (semantic, (is_a (downtown, place)))\n5Empirical work demonstrates that LLMs can be trained and\nfine-tuned to learn to encode key aspects of traditional symbolic\nstructures (Pavlick 2023; Romero et al. 2021; Tomasic et al. 2021).\n(semantic, (today (saturday)))\n(episodic, (destination=downtown, place=workplace,\në days=[monday... friday]))\n(episodic, (destination=downtown, place=gym,\në days=[saturday, sunday]))\n(procedural, (if destination == x and today == y\në then destination=place))\n(semantic, (destination (gym)))\n[o1] search bus schedule with destination gym\n...\nNow, case (b) proposes an integration scenario where a\nCA leverages multimodal LLMs to support both the per-\nception and motor modules of the system. At the perception\nlevel, the LLM converts external signals (e.g., text and im-\nages) into symbolic representations and their relations, along\nwith relevant metadata, which is then stored in the corre-\nsponding working memory buffers. For instance, we used a\ndescription of an image generated by GPT-46, then we pro-\nvided an in-context learning example to ChatGPT to convert\nthe description into symbolic structures:\n[prompt] your job is to transform that natural language\ndescription into symbolic representations. Example,\nInput: \"this is an office with a desk, a computer on\ntop of it, and a chair. There is also a printer on\nthe left of the computer and some pencils on the\nright\". Output: has(office, desk). has(office,\ncomputer). has(office, chair). onTopOf(computer,\ndesk). toTheRightOf(printer, computer). toTheLeftOf(\ncomputer, printer). toTheLeftOf(computer, pencils)..\n[prompt] Input: \"a busy city street with a clock tower\nin the background. The street is lined with shops\nand restaurants on both sides. There are several\nmotorcycles parked on the side of the road, and\npeople are walking down the street. The clock tower\nhas a large clock face with roman numerals and a\nsmall spire on top...\".\nOutput:\n[ChatGPT] has(city_street, clock_tower). has(clock_tower\n, clock_face). has(clock_face, roman_numerals). has(\nclock_face, small_spire). has(city_street, shops).\nhas(city_street, restaurants). has(city_street,\nmotorcycles). toTheLeftOf(clock_tower, city_street).\ntoTheRightOf(city_street, clock_tower). toTheRightOf\n(shops, city_street)... onTopOf(roman_numerals,\nclock_face). onTopOf(small_spire, clock_face).\nonTopOf(clock_face, clock_tower)...\nAs observed in the example, though not entirely accurate,\nLLMs demonstrate the capability to extract high-level com-\npositional and spatial relationships between entities from a\ngiven image/text and then re-express them using symbolic\nrepresentations. After generating and storing these symbolic\nstructures in the working memory, other modules of the CA\ncan access them and perform diverse kinds of cognitive pro-\ncesses. Considering our initial example, it is expected that\nthis symbolic representation of perceived images will en-\nable both the visually impaired user and the cognitive agent\nto collaboratively construct shared mental models for navi-\ngation, thereby enhancing spatial cognition and situational\n6At the time of writing this paper, OpenAI is holding back GPT-\n4 image processing features, so we used a natural language descrip-\ntion generated with GPT-4 and reported in (Zhu et al. 2023).\n398\nLLM LLM\nWM\nPM DM\nLLM\ni0\na. Cognitively-augmented LLM\no0 on-1 on\nsi0\nso0\nw0 wn-1\nCA\nsin-1\nson-1\ni\nWM\nPM\nDM\nP\nLLM\nM\nLLM\no\nCA\nb. Perception and Motor powered by LLM\nsi\nw\ni\nWM\nPM\nDM\nP\nLLM\nM\nLLM\no\nCA\nc. CA powered by LLM\nsi\nw\nLLM\nLLM\nP\nM A\nP\nM\nP\nM\nio\nA\no0\no1\ni1\nin\non\nd. Internal simulation for anticipation and planning \n…\nb\ni1 in\n…\nb b\nb\nb b\nWM\nWM\nWM\nCA\nCA\nCA\nFigure 1: Modular approach. (a) Chain-of-Thought or recursive reasoning that augments an LLM with content generated by a\nCA. (b) Perception and Motor modules of a CA that leverages the power of LLMs. (c) Multiple modules of a CA that use LLMs\nto process and/or retrieve data. (d) A CA that leverages LLMs to predict/anticipate future states of the environment in order to\nperform reasoning and planning (some modules are not shown for the sake of legibility). Red-colored boxes denote LLMs and\nblue-colored ones denote CAs modules. Perception (P), motor (M), working memory (WM), long-term procedural memory\n(PM), long-term declarative memory (DM), and Anticipation (A) correspond to modules of a CA. i and o correspond to the\ninput and output of the system, respectively.si and so are symbolic representations of the input i and the output o, respectively.\nw corresponds to the contents of the working memory.b are module-specific working memory buffers. Solid arrows denote the\nflow of information and dotted arrows denote predictions of the next input.\nawareness of the user. Conversely, the LLM-based motor\nmodule converts the symbol structures that have been stored\nin the working memory buffers into external actions (e.g.,\nnatural language generation, motor control, etc.)\nUnlike case (b), which loosely integrates LLMs and CAs,\ncase (c) proposes an integration where not only the percep-\ntion/motor modules are driven by LLMs, but also the pro-\ncedural and declarative (semantic and episodic) memories.\nPrior research (Park et al. 2023) suggested using LLMs to\nretain episodic knowledge as lists of observations (depicting\nagents’ behaviors in natural language). These can be synthe-\nsized into high-level observations using LLMs’ summariza-\ntion abilities, enabling agents to reflect on their experiences\nacross different time spans. From another perspective, we\nenvision the possibility of converting these natural language\ndescriptions into symbolic structures using a proper method\nfor fine-tuning or prompt-engineering an LLM.\nSimilarly, the large amount of factual knowledge directly\naccessible through LLMs can be harnessed to automatically\nextract knowledge and populate a semantic memory (e.g.,\nan ontology) of a CA, which would otherwise require labo-\nrious manual knowledge curation and acquisition. Prelimi-\nnary work in this area aims to populate arbitrary complex\nnested semantic knowledge schemas (Caufield et al. 2023).\nRegarding the procedural module, previous work (Park\net al. 2023) has shown that LLMs can be leveraged to cre-\nate detailed step-by-step plans by translating the agent’s ob-\nservations of the environment expressed in natural language\ninto high-level action plans, which are then recursively de-\ncomposed into detailed behaviors for action and reaction.\nAlternatively, we anticipate that, with the proper in-context\nprompting or fine-tuning, an LLM can translate natural lan-\n399\nguage descriptions into symbolic structures such as produc-\ntion rules. For instance, consider the example below where\nwe prompted ChatGPT to generate an ACT-R production\nrule for filtering out transit searches based on a natural\nlanguage description and a single example (borrowed from\n(ACT-R Website. 2015)) used to prime the LLM7:\n[prompt]\nInstructions: ... your job is to generate ACT-R\nproductions based on a natural language description\nthat I will provide to you... For instance:\nDescription: generate an ACT-R production that\ncounts until a given number. Production:\n(p example-counting\n=goal>\n....\n[prompt]\nDescription: generate an ACT-R production such\nthat if I need to take a bus, check first the\nweather and the traffic patterns. If it is snowing\nand the traffic is heavy, then search only for bus\nroutes without transfers and minimum walking\ndistance.\nProduction:\n[ChatGPT]\n(p check-weather-traffic-snow-heavy\n=goal>\nisa take-bus\nstate check-weather-traffic\n=retrieval>\nisa weather-traffic\nweather snowing\ntraffic heavy\n==>\n=goal>\nisa take-bus\nstate search-routes\ntransfer none\nwalking min-distance\n+retrieval>\nisa bus-routes\ntransfer none\nwalking min-distance)\nThe condition part of the production checks both that\nthe state of the goal is check-weather-traffic and that\nthe retrieved declarative chunk has the values snowing\nand heavy for the slots weather and traffic, respec-\ntively. If so, the action will change the goal’s state to\nsearch-routes and retrieve a declarative chunk for bus\nroutes with no transfers and minimum walking distance.\nAlthough the generated production captures correctly the\nintention of the natural language description, it contains re-\ndundant slots fortransfer and walking on the goal buffer\nof the action part (in italics). This type of transformation\nfrom natural language descriptions to symbolic productions\ncan allow users to instruct and teach explicit procedural\nknowledge to their agents via conversation and, more inter-\nestingly, transform symbolic productions back into natural\nlanguage for explainability purposes. However, it is not clear\nhow an LLM could keep consistency between learned pro-\nductions for a large knowledge base. Additionally, at least at\nits current state, LLMs by themselves cannot compile cer-\ntain operations over the procedural memory such as conflict\n7The complete log is available here: https://shareg.pt/nO1zssm.\nresolution and execution, so an LLM would still require an\nexternal interaction with a CA’s procedural engine.\nFinally, case (d) presents a streamlined approach to the\nsimulation theory of cognition, which states that cognitive\nfunctions like planning and anticipation stem from inter-\nnally simulated interactions with the environment (Shana-\nhan 2006; Hesslow 2012). By inputting appropriate contex-\ntual information (such as working memory contents, sen-\nsory input, motor responses, and past experiences), we pos-\ntulate that LLMs have the potential to forecast likely rep-\nresentations of the world’s states resulting from the current\nstate. That is, upon receiving an initial sensory input (i0), the\nCA progresses through its standard perception-action path-\nway. Subsequently, rather than executing the resulting ac-\ntion (O0) in the real world, the action O0, along with the\nworking memory contents, are used as inputs of the LLM.\nThe LLM then generates a prediction for the next world\nstate (i1), which serves as a simulated input. Next, a sim-\nulated output o1 is generated, and then the process cycles\nuntil a certain condition is met. By chaining these sequences\nof perception-action-prediction, the system could anticipate\nthe outcomes of its actions across multiple temporal scales.\nThese simulated sequences may take on either linear\nconfigurations, predicting only one world state per step,\nor branching tree-like structures, predicting multiple likely\nworld states per step (tangentially related work has been\nproposed by (Yao et al. 2023a)). In the latter case, a plan-\nning mechanism could explore different branches of the tree\nby assessing their likelihood of occurrence and performing\nbacktracking when necessary. Similar to the approach pro-\nposed by (Park et al. 2023), an LLM can be prompted to\n“guesstimate” a probability for each node of the tree. Be-\nlow is a simplified example where the inputs to the LLM\nare the contents of the working memory (green), simulated\nuser actions (red), and simulated system actions (blue). For\nsimplicity, both user and system actions are natural language\nand do not involve symbols:\n[prompt] You are an intelligent agent that assists\na person who is blind in retrieving information\nfrom public transportation. Today is snowing and\ntherefore the traffic is heavy . Predict 3 things\nthat the user will ask you to do and assign a\nprobability to occur to each one.\n[ChatGPT] Check current bus/train delay (0.6),\nsuggest alternative routes (0.3),\nprovide weather-related alerts (0.1)\n[prompt] The user asks you to provide weather-related\nalerts , and you provide those alerts . What do you\npredict the user will ask next?\n[ChatGPT] Ask for specific service disruptions (0.4),\nrequest tips for navigating in snowy conditions\n(0.3), inquire about expected clearing times (0.2)\nNeuro-Symbolic Approach\nWe present a neuro-symbolic approach inspired by the\nCLARION cognitive architecture, focusing primarily on\nthe action-centered sub-system (ACS), while acknowledg-\ning the existence of three additional sub-systems within the\n400\nPerception (LLM)\nWM\nUser: How do I \nget X store?\nSymbols\nNatural \nlanguage\n(intent, \ntake_bus)\n(dest,\nX store) \nConnectionist (Bottom) Level – LLM\nHow do I get \nX store?\nSymbolic (Top) Level\nUser: If it is \nsnowing then \ndiscard bus \ntransfers and \nuse min walk\n-\ning distance\n(intent, \ntake_bus)\n(weather,\nsnowing)\n(transfers, \nnone)…\nWeather cond: \nsnowing\n(weather,   \nsnowing)\nIf (intent,\ntake_bus)\n(weather,\nsnowing)\nThen \n(transfers, none)\n(walk_dist, min)\nIRL RER\nIf (intent, take_bus) and (weather, snowing)… then (transfers, fewer )\nIf … and (weather, [snowing, raining]) then (transfers, fewer )\nIf … and (weather, snowing) and (traffic, heavy) then (transfers, fewer )\nMotor\nPrompt: You are an intelligent agent that assists a blind user ... It is snowing and she asks you  \"how do I get X store?\" Which \nfilter you would apply to the bus search? Output: opt for routes with fewer transfers, prioritize routes with higher frequencies…\nUtter: \n“Your next \nbus is..”\nG\nS\nTop-down \nlearning\nBottom-up \nlearning\nCall bus\nAPI\nFeedback\nFigure 2: Neuro-symbolic approach. WM: Working Memory. IRL: Independent Rule Learning. RER: Rule Extraction Refine-\nment. G: Generalization scenario. S: Specialization scenario. Orange-colored boxes illustrate the IRL case while the blue-\ncolored boxes illustrate the RER case. Highlighted text represents entities and keywords present at the bottom level that are\nfurther extracted and translated into symbols at the top level.\narchitecture. The ACS operates across two distinct levels:\nthe top level (symbolic), responsible for encoding explicit\nknowledge, and the bottom level (connectionist), tasked with\nencoding implicit knowledge. Consequently, the architec-\nture exhibits a degree of redundancy in knowledge repre-\nsentation. These two levels synergistically engage in action\nselection, reasoning, and learning processes. Our focus is\nto explore the incorporation of LLMs at the bottom level,\nenhancing the knowledge extraction and integration process\nwhile exhibiting potential scalability towards novel scenar-\nios. Further details on the mathematical model underpinning\nthe cognitive processes can be found in (Sun 2016).\nCLARION defines three types of symbolic rules at the top\nlevel. The fixed rules (FR) are rules that have been hard-\nwired by an expert and cannot be deleted; Independent-\nRule-Learning (IRL) rules are independently generated at\nthe top level, with little involvement (or no involvement at\nall) of the bottom level, which can be refined or deleted as\nneeded; and Rule-Extraction-Refinement (RER) rules which\nare extracted from the bottom level. Figure 2 illustrates\nthe process wherein a human provides a natural language\ninstruction to create a new rule and the LLM-based per-\nception module extracts symbolic structures that are fur-\nther stored in the working memory. Through a template-\nmatching mechanism, the contents of the working mem-\nory are expressed as an IRL rule where both its condition\nand action parts are chunks composed of dimension-value\npairs8, e.g., chunki((intent, take\nbus), (weather, snowing))\nÑ chunkj((transfers, none), (walk distance, min)).\nOn the other hand, if an action determined at the bot-\ntom level proves successful (according to a certain crite-\nrion), an RER rule is formulated and subsequently incorpo-\nrated into the top level, e.g., given the output generated by\nthe LLM at the bottom level 9 on Figure 2, the correspond-\ning RER rule is chunki((intent, take\nbus), (weather, snow-\n8Each dimension may have one or multiple values associated.\n9See full output log here: https://sharegpt.com/c/LYIz9in\ning)) Ñ chunkj((transfers, fewer)). During subsequent in-\nteractions with the environment, the rule is refined based on\nthe outcomes of its application: if the result is deemed suc-\ncessful, the rule’s conditions may be generalized to make it\nmore universal by adding new values to dimensions (e.g.,\nchunki((intent, take\nbus), (weather, [snowing, raining])) Ñ\nchunkj((transfers, fewer))). Conversely, if the outcome does\nnot yield success, the rule should be specialized by remov-\ning values from dimensions or by adding new dimension-\nvalue pairs (e.g., chunki((intent, take\nbus), (weather, snow-\ning), (traffic, heavy)) Ñ chunkj((transfers, fewer))).\nRule selection in IRL is determined by an information\ngain function, while RER uses a Boltzmann distribution\nbased on rule’s utility function and a base-level activation.\nThe integration of both levels can be achieved through var-\nious mechanisms. Stochastic selection involves choosing a\nlevel (top or bottom) and a group of rules if the top level is\nchosen (e.g., FR, RER, or IRL). These selections are based\non probabilities assigned by a metacognitive module to each\nlevel/group. Integration through bottom-up rectification oc-\ncurs when the top level rectifies and incorporates outcomes\nfrom the bottom level (e.g., the LLM may discover addi-\ntional dimension-value pairs not specified by the top level\nlike “prioritize routes with higher frequencies”). Alterna-\ntively, top-down guidance involves the bottom level utiliz-\ning outcomes from the top level, combined with its own\nknowledge, to make action decisions. This top-down guid-\nance can be achieved by using prompt engineering tech-\nniques to prime the LLM with either FR or IRL rules.\nBottom-up learning is facilitated by the rule extraction\nmechanism, whereas top-down learning can be realized by\nusing both FR and IRL rules as exemples to fine-tune the\nLLM at the bottom level. Determining whether an outcome\nfrom the bottom level is successful requires feedback, often\nin the form of rewards or reinforcement, which might not be\nreadily available. To address this challenge, we propose two\napproaches: the incorporation of human-in-the-loop inter-\nactions, where feedback ensures the coherence of extracted\n401\nrules, and the utilization of an additional LLM for self-play\ninteractions emulating human feedback. Overall, both the\nbottom-up and the top-down learning mechanisms support\nexplainability of decision-making and reasoning processes\nperformed by the LLM at the bottom level.\nHarnessing LLMs at the bottom level of a CLARION-like\narchitecture can contribute remarkably to enhancing the sys-\ntem’s flexibility and scalability. First, unlike backpropaga-\ntion neural networks used in CLARION, LLMs are not re-\nstricted to a fixed number of features and labels. Also, the\nLLMs-based variation we propose do not require to pre-\ndefine dimension-value pairs as CLARION does. Conse-\nquently, the utilization of LLMs at the bottom level can\nenable enhanced representational flexibility, with cascad-\ning benefits reaching the top level. Secondly, the conver-\nsion from unstructured natural language to symbols and vice\nversa can be executed seamlessly by an LLM-based bottom\nlevel. Lastly, leveraging an LLM with such broad knowledge\nof the world, coupled with cross-level learning dynamics and\nhuman feedback, can foster continuous learning loops where\nknowledge is constructed and refined over time.\nAgency Approach\nThe Agency approach operates on two levels - micro and\nmacro (see Figure 3). Inspired by the Society of Mind theory\n(Minsky 1988) and LIDA cognitive architecture (Franklin\nand Patterson 2006), micro-level agency occurs within the\ncognitive architecture itself. Specialized agents process in-\nformation in parallel, competing for resources like atten-\ntion and memory. They collaborate by forming coalitions for\ndecision-making and problem-solving. In contrast, macro-\nlevel agency involves cognitive agents interacting with other\nagents and humans to collaboratively achieve goals.\nConsider the case of our cognitive agent designed to aid\nblind users in indoor navigation. At a micro-level, each agent\noperates through either a fine-tuned LLM or a symbolic pro-\ncessor. Cognitive processing unfolds as follows: sensory in-\nputs are processed by the perception module, yielding ab-\nstract entities like objects, categories, actions, events, etc.,\nforwarded to the working memory. Then, the working mem-\nory cues declarative memories to establish local associa-\ntions, e.g., user navigation preferences, place familiarity,\nand more. Specialized agents at the agency observe work-\ning memory contents and form coalitions.\nFor instance, object detection and semantic localization\nconstitute one coalition, while natural language understand-\ning and semantic grounding form another. These coalitions\nare transferred to the Global Workspace, where a competi-\ntive process selects the most relevant coalition. If a user ap-\nproaches a staircase lacking a handrail, the coalition involv-\ning object detection and semantic localization takes prece-\ndence, globally transmitting its contents (e.g., staircase prox-\nimity and orientation) to other agents. In subsequent cog-\nnitive cycles, the coalition for natural language generation\nwould be chosen to provide timely warnings to the user.\nWhile not a novel architectural approach, its potential lies\nin the diverse roles agents can assume within coalitions.\nFor instance, an LLM agent engages in pair work, process-\ning text or images to produce symbols, while a symbolic\nagent infers insights from these symbols. Another scenario\ninvolves one LLM agent fine-tuned to convert symbol struc-\ntures into natural language text and another serving a super-\nvisory role, pinpointing errors in the first agent’s output.\nNow, to better understand macro-level interactions, let’s\nconsider two users (Aand B) alongside their cognitive agents\n(a and b). Agents a and b collaborate to exchange knowledge\nand intentions (e.g., a shares spatial insights with b of previ-\nous A’s exploration of a building, thus aidingB’s future nav-\nigation), negotiate (e.g., a and b helping teammates A and\nB reach an agreement when having conflicting goals), de-\nbate (e.g., a and b debating about their reasoning processes\nto approach a problem while reaching a consensus (Du et al.\n2023)), among others. All these kinds of interactions among\nagents could use natural language in order to foster trans-\nparency and interpretability, from the user’s point of view,\nof the reasoning processes and conciliated actions, although\nthe necessity of symbolic counterparts remains unclear.\nDiscussion\nAmong the three approaches discussed so far, there are some\ncommonalities that we highlight next. First, the working\nmemory, along with the perception module, plays an impor-\ntant role in retaining the most pertinent information while\nfiltering out irrelevant stimuli. This contrasts with the idea of\na context window in LLMs, where truncation strategies arbi-\ntrarily delete the oldest tokens observed when the length of\nthe window reaches a maximum, potentially discarding crit-\nical parts of the context. The contents of the working mem-\nory are selectively and intentionally stored and recalled from\nlong-term memories, allowing the agent to continuously in-\nteract with the environment without losing track of events.\nA second common aspect among all three approaches is the\nutilization of LLMs to accurately translate unstructured nat-\nural language to symbols and vice versa, as well as to extract\nfactual knowledge about the world. This breakthrough opens\nup a realm of new possibilities, allowing for the seamless\nscaling of CAs to tackle complex real-world problems.\nThird, the three approaches can benefit from multi-modal\nmulti-turn interaction. In cases where cognitive agents col-\nlaborate with humans, there is an opportunity to incremen-\ntally refine shared mental models of a task through con-\ntinuous conversational interaction and scene understanding.\nFourth, since all the approaches depend, in one way or an-\nother, on LLMs, they are susceptible to the stochastic nature\nof LLMs. This stochastic nature leads to variations (some-\ntimes remarkable) in the outputs, even when the model is\nprompted with exactly the same input. And fifth, all three\napproaches contribute, to a greater or lesser extent, to the\ncontinuous construction of cognitive models about the enti-\nties in the world, their relationships, and the distinct cogni-\ntive processes that operate over them.\nRegarding the Modular approach, the main difference\namong the four cases presented is the degree of integra-\ntion between an LLM and a CA. The first case, the cogni-\ntively augmented LLM, aligns with the current trend of aug-\nmenting LLMs with external tools and interpreters and rep-\nresents the most loosely integrated model among the four.\nIn this case, the LLM retains control of execution, and the\n402\na. Agency at the micro-level b. Agency at the macro-level\nHuman Cognitive agent (CA + LLM)\nWM\nDM\nP\nM\nCA module Competing/receiving Agent\nGlobal Workspace\nAgency\nReceiving agents\nCompeting input agents\nActive Agent\nCognitive Agent\nFigure 3: Agency approach. a) Agents at the micro-level compete for resources and cooperate in decision-making. b) Agents at\nthe macro-level interact with other agents and humans to cooperate in task resolution. P (Perception), M (Motor), WM (working\nmemory), and DM (declarative memory) are modules of a CA.\noutputs of the CA are solely utilized for in-context learn-\ning purposes. The strength of this approach is that recursive\nLLMs receive gradual guidance during the chain-of-thought\nreasoning process. However, a notable disadvantage is that,\ndue to the lack of overall control, the CA components can\nonly contribute to reactive (System 1) responses rather than\ndeliberative, high-order (System 2) ones.\nThe second case of the modular approach presents a mod-\nerately integrated model where only the perception and mo-\ntor modules of a CA are powered with LLMs. The main\nstrength of this model is that it aligns with the evident ben-\nefits obtained from multi-modal LLMs, which notably en-\nhance text and image understanding, avoiding the need for\ntask-specific and laborious labeling and training of machine\nlearning models. Another advantage of this case is that it as-\nsumes a straightforward transformation from sensory inputs\nto symbolic percepts, which facilitates further processing.\nHowever, one main disadvantage is that the other modules\nof the CA still do not fully leverage the power of LLMs.\nThe third case presents a tightly integrated model that\nleverages the synergistic interaction between LLMs and\nsymbolic components of a CA. LLMs extract factual knowl-\nedge from the world, automatically populating ontologies.\nThese semantic representations then facilitate the creation\nof world models, addressing a limitation of LLMs. Further-\nmore, proper LLM’s prompt engineering techniques would\nproduce syntactically and semantically correct CA produc-\ntions, which can be later compiled by a symbolic engine.\nHowever, a drawback of this integrated system is its heavy\nreliance on LLM outputs, rendering it susceptible to cascad-\ning failures, including hallucinations and biases.\nThe fourth case represents the most tightly integrated\nmodel. It involves a module designed for simulating the out-\ncomes of future events. The primary advantage of this case is\nits capability to anticipate and plan by traversing and back-\ntracking a tree-like structure of possible events. However,\nsimilar to the third case, this system heavily relies on the\noutputs of the LLM, which might occasionally be inconsis-\ntent. This inconsistency could lead to erroneous predictions\nin the early stages of internal simulation, resulting in cascad-\ning errors in the planning process.\nUnlike the Modular approach, which can suffer from\noverall failures and inconsistencies if individual modules\nare poorly designed, the Agency approach at the micro-\nlevel offers greater robustness from two key angles. First,\nagents may encode redundant knowledge, resulting in mul-\ntiple agents capable of achieving the same competence. This\nredundancy enhances system resilience as individual agents\nmay fail, yet the system can still yield satisfactory outcomes.\nSecond, agent role-playing strategies enable the system to\nself-reflect and promptly rectify potential deviations in rea-\nsoning processes. At the macro-level, the Agency approach\nstands out as the only one among the three approaches that\nconsiders inter-agent interactions, with a primary focus on\ncollaborative interactions between agents and humans. How-\never, aspects such as communication, coordination, hierar-\nchies, etc. between agents still remain open questions.\nThe Neuro-symbolic approach is arguably the most\ntightly integrated model. It leverages the capabilities of\nLLMs to seamlessly translate unstructured natural language\ninto structured symbolic representations and vice versa. This\napproach plays a crucial role in extracting rules from the\nconnectionist level and subsequently generalizing and spe-\ncializing those extracted rules over time. The interactions\nbetween the symbolic and connectionist levels enable the\ncontinuous construction of explainable models for decision-\nmaking and procedural processing based on black-boxed\nLLMs. However, a potential weakness of this approach lies\nin its heavy reliance on the LLM layer.\nAcknowledgements\nThe contents of this paper were developed under grants\nfrom the National Institute on Disability, Independent Liv-\ning, and Rehabilitation Research (NIDILRR grant numbers\n90DPGE0003 and 90REGE0007)\nReferences\nACT-R Website. 2015. Unit 1: Understanding Production\n403\nSystems. http://act-r.psy.cmu.edu/wordpress/wp-content/\nthemes/ACT-R/tutorials/unit1.htm. Accessed: 2023-08-03.\nAnderson, J. R.; and Lebiere, C. J. 2014. The atomic com-\nponents of thought. Psychology Press.\nBender, E. M.; Gebru, T.; McMillan-Major, A.; and\nShmitchell, S. 2021. On the Dangers of Stochastic Par-\nrots: Can Language Models Be Too Big? In Proceed-\nings of the 2021 ACM Conference on Fairness, Account-\nability, and Transparency, FAccT ’21, 610–623. New York,\nNY , USA: Association for Computing Machinery. ISBN\n9781450383097.\nBinz, M.; and Schulz, E. 2023. Using cognitive psychology\nto understand GPT-3. Proceedings of the National Academy\nof Sciences, 120(6): e2218523120.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language Mod-\nels are Few-Shot Learners. In Larochelle, H.; Ranzato, M.;\nHadsell, R.; Balcan, M.; and Lin, H., eds.,Advances in Neu-\nral Information Processing Systems, volume 33, 1877–1901.\nCurran Associates, Inc.\nBubeck, S.; Chandrasekaran, V .; Eldan, R.; Gehrke, J.;\nHorvitz, E.; Kamar, E.; Lee, P.; Lee, Y . T.; Li, Y .; Lundberg,\nS.; Nori, H.; Palangi, H.; Ribeiro, M. T.; and Zhang, Y . 2023.\nSparks of Artificial General Intelligence: Early experiments\nwith GPT-4. arXiv:2303.12712.\nCaufield, J. H.; Hegde, H.; Emonet, V .; Harris, N. L.;\nJoachimiak, M. P.; Matentzoglu, N.; Kim, H.; Moxon, S. A.;\nReese, J. T.; Haendel, M. A.; et al. 2023. Structured\nprompt interrogation and recursive extraction of semantics\n(SPIRES): A method for populating knowledge bases using\nzero-shot learning. arXiv preprint arXiv:2304.02711.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), 4171–4186. Min-\nneapolis, Minnesota: Association for Computational Lin-\nguistics.\nDiao, S.; Wang, P.; Lin, Y .; and Zhang, T. 2023. Ac-\ntive Prompting with Chain-of-Thought for Large Language\nModels. arXiv:2302.12246.\nDu, Y .; Li, S.; Torralba, A.; Tenenbaum, J. B.; and Mor-\ndatch, I. 2023. Improving Factuality and Reasoning in Lan-\nguage Models through Multiagent Debate. arXiv preprint\narXiv:2305.14325.\nFranklin, S.; and Patterson, F. 2006. The LIDA architec-\nture: Adding new modes of learning to an intelligent, au-\ntonomous, software agent. pat, 703: 764–1004.\nGao, L.; Madaan, A.; Zhou, S.; Alon, U.; Liu, P.; Yang, Y .;\nCallan, J.; and Neubig, G. 2023. PAL: Program-aided Lan-\nguage Models. arXiv:2211.10435.\nHesslow, G. 2012. The current status of the simulation the-\nory of cognition. Brain research, 1428: 71–79.\nHuang, X.; Ruan, W.; Huang, W.; Jin, G.; Dong, Y .; Wu,\nC.; Bensalem, S.; Mu, R.; Qi, Y .; Zhao, X.; Cai, K.; Zhang,\nY .; Wu, S.; Xu, P.; Wu, D.; Freitas, A.; and Mustafa, M. A.\n2023. A Survey of Safety and Trustworthiness of Large Lan-\nguage Models through the Lens of Verification and Valida-\ntion. arXiv:2305.11391.\nKosinski, M. 2023. Theory of Mind May Have\nSpontaneously Emerged in Large Language Models.\narXiv:2302.02083.\nKotseruba, I.; and Tsotsos, J. K. 2020. 40 years of cognitive\narchitectures: core cognitive abilities and practical applica-\ntions. Artificial Intelligence Review, 53(1): 17–94.\nLaird, J. E. 2019. The Soar cognitive architecture. MIT\npress.\nLaird, J. E.; Lebiere, C.; and Rosenbloom, P. S. 2017. A\nStandard Model of the Mind: Toward a Common Compu-\ntational Framework across Artificial Intelligence, Cognitive\nScience, Neuroscience, and Robotics. AI Magazine, 38(4):\n13–26.\nLeCun, Y . 2022. A path towards autonomous machine intel-\nligence version 0.9. 2, 2022-06-27. Open Review, 62.\nLi, S.; Puig, X.; Paxton, C.; Du, Y .; Wang, C.; Fan, L.; Chen,\nT.; Huang, D.; Aky ¨urek, E.; Anandkumar, A.; Andreas, J.;\nMordatch, I.; Torralba, A.; and Zhu, Y . 2022. Pre-Trained\nLanguage Models for Interactive Decision-Making. CoRR,\nabs/2202.01771.\nLieto, A.; Lebiere, C.; and Oltramari, A. 2018. The knowl-\nedge level in cognitive architectures: Current limitations and\npossible developments. Cognitive Systems Research, 48:\n39–55. Cognitive Architectures for Artificial Minds.\nMarcus, G. 2020. The Next Decade in AI: Four\nSteps Towards Robust Artificial Intelligence. CoRR,\nabs/2002.06177.\nMialon, G.; Dess `ı, R.; Lomeli, M.; Nalmpantis, C.; Pa-\nsunuru, R.; Raileanu, R.; Rozi `ere, B.; Schick, T.; Dwivedi-\nYu, J.; Celikyilmaz, A.; Grave, E.; LeCun, Y .; and Scialom,\nT. 2023. Augmented Language Models: a Survey.\narXiv:2302.07842.\nMinsky, M. 1988. Society of mind. Simon and Schuster.\nMitsopoulos, K.; Bose, R.; Mather, B.; Bhatia, A.;\nGluck, K.; Dorr, B.; Lebiere, C.; and Pirolli, P. 2023.\nPsychologically-Valid Generative Agents: A Novel Ap-\nproach to Agent-Based Modeling in Social Sciences. In\nProceedings of the 2023 AAAI Fall Symposium on Integrat-\ning Cognitive Architectures and Generative Models. AAAI\nPress.\nPark, J. S.; O’Brien, J. C.; Cai, C. J.; Morris, M. R.; Liang,\nP.; and Bernstein, M. S. 2023. Generative Agents: Interac-\ntive Simulacra of Human Behavior. arXiv:2304.03442.\nPavlick, E. 2023. Symbols and grounding in large language\nmodels. Philosophical Transactions of the Royal Society A,\n381(2251): 20220041.\n404\nQian, J.; Wang, H.; Li, Z.; Li, S.; and Yan, X. 2022. Limita-\ntions of language models in arithmetic and symbolic induc-\ntion. arXiv preprint arXiv:2208.05051.\nRomero, O. J.; Wang, A.; Zimmerman, J.; Steinfeld, A.; and\nTomasic, A. 2021. A Task-Oriented Dialogue Architecture\nvia Transformer Neural Language Models and Symbolic In-\njection. In Proceedings of the 22nd Annual Meeting of the\nSpecial Interest Group on Discourse and Dialogue, 438–\n444. Singapore and Online: Association for Computational\nLinguistics.\nSchick, T.; Dwivedi-Yu, J.; Dess`ı, R.; Raileanu, R.; Lomeli,\nM.; Zettlemoyer, L.; Cancedda, N.; and Scialom, T. 2023.\nToolformer: Language Models Can Teach Themselves to\nUse Tools. arXiv:2302.04761.\nScialom et al., T. 2022. Fine-tuned language models are con-\ntinual learners. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing, 6107–\n6122.\nShanahan, M. 2006. A cognitive architecture that combines\ninternal simulation with a global workspace. Consciousness\nand cognition, 15(2): 433–449.\nSun, R. 2016. Anatomy of the mind: exploring psycholog-\nical mechanisms and processes with the Clarion cognitive\narchitecture. Oxford University Press.\nTomasic, A.; Romero, O. J.; Zimmerman, J.; and Steinfeld,\nA. 2021. Propositional Reasoning via Neural Transformer\nLanguage Models. Int. Workshop on Neural-Symbolic\nLearning and Reasoning (NESY).\nVenkit, P. N.; Srinath, M.; and Wilson, S. 2022. A Study of\nImplicit Bias in Pretrained Language Models against Peo-\nple with Disabilities. In Proceedings of the 29th Inter-\nnational Conference on Computational Linguistics, 1324–\n1332. Gyeongju, Republic of Korea: International Commit-\ntee on Computational Linguistics.\nWang, G.; Xie, Y .; Jiang, Y .; Mandlekar, A.; Xiao, C.; Zhu,\nY .; Fan, L.; and Anandkumar, A. 2023. V oyager: An open-\nended embodied agent with large language models. arXiv\npreprint arXiv:2305.16291.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Chi, E. H.;\nLe, Q.; and Zhou, D. 2022. Chain of Thought Prompt-\ning Elicits Reasoning in Large Language Models. CoRR,\nabs/2201.11903.\nWeidinger, L.; Uesato, J.; Rauh, M.; Griffin, C.; Huang, P.-\nS.; Mellor, J.; Glaese, A.; Cheng, M.; Balle, B.; Kasirzadeh,\nA.; Biles, C.; Brown, S.; Kenton, Z.; Hawkins, W.; Steple-\nton, T.; Birhane, A.; Hendricks, L. A.; Rimell, L.; Isaac, W.;\nHaas, J.; Legassick, S.; Irving, G.; and Gabriel, I. 2022. Tax-\nonomy of Risks Posed by Language Models. In Proceed-\nings of the 2022 ACM Conference on Fairness, Account-\nability, and Transparency, FAccT ’22, 214–229. New York,\nNY , USA: Association for Computing Machinery. ISBN\n9781450393522.\nWelleck, S.; Kulikov, I.; Roller, S.; Dinan, E.; Cho, K.; and\nWeston, J. 2019. Neural text generation with unlikelihood\ntraining. arXiv preprint arXiv:1908.04319.\nXie, Y .; Xie, T.; Lin, M.; Wei, W.; Li, C.; Kong, B.; Chen,\nL.; Zhuo, C.; Hu, B.; and Li, Z. 2023. OlaGPT: Empow-\nering LLMs With Human-like Problem-Solving Abilities.\narXiv:2305.16334.\nYao, S.; Yu, D.; Zhao, J.; Shafran, I.; Griffiths, T. L.; Cao,\nY .; and Narasimhan, K. 2023a. Tree of Thoughts: De-\nliberate Problem Solving with Large Language Models.\narXiv:2305.10601.\nYao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan,\nK.; and Cao, Y . 2023b. ReAct: Synergizing Reasoning and\nActing in Language Models. arXiv:2210.03629.\nZhang, Z.; Zhang, A.; Li, M.; and Smola, A. 2022. Auto-\nmatic Chain of Thought Prompting in Large Language Mod-\nels. arXiv:2210.03493.\nZhu, D.; Chen, J.; Shen, X.; Li, X.; and Elhoseiny, M.\n2023. Minigpt-4: Enhancing vision-language understand-\ning with advanced large language models. arXiv preprint\narXiv:2304.10592.\n405",
  "topic": "Cognitive architecture",
  "concepts": [
    {
      "name": "Cognitive architecture",
      "score": 0.666142463684082
    },
    {
      "name": "Cognition",
      "score": 0.6126174926757812
    },
    {
      "name": "Modular design",
      "score": 0.5780909061431885
    },
    {
      "name": "Cognitive science",
      "score": 0.5649197697639465
    },
    {
      "name": "Computer science",
      "score": 0.5465651154518127
    },
    {
      "name": "CLARION",
      "score": 0.485113263130188
    },
    {
      "name": "Cognitive model",
      "score": 0.4846387505531311
    },
    {
      "name": "Causal chain",
      "score": 0.4377542734146118
    },
    {
      "name": "Embodied cognition",
      "score": 0.41149234771728516
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35221222043037415
    },
    {
      "name": "Psychology",
      "score": 0.23589631915092468
    },
    {
      "name": "Epistemology",
      "score": 0.10779106616973877
    },
    {
      "name": "Programming language",
      "score": 0.10113051533699036
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    }
  ]
}