{
    "title": "Time Series Forecasting Based on Convolution Transformer",
    "url": "https://openalex.org/W4367555917",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A1229069860",
            "name": "Na Wang",
            "affiliations": [
                "Nanjing Audit University",
                "Nanjing University of Aeronautics and Astronautics"
            ]
        },
        {
            "id": "https://openalex.org/A2095996199",
            "name": "Xianglian Zhao",
            "affiliations": [
                "Nanjing University of Aeronautics and Astronautics"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2786228682",
        "https://openalex.org/W3034301699",
        "https://openalex.org/W2604847698",
        "https://openalex.org/W2765932895",
        "https://openalex.org/W2980994438",
        "https://openalex.org/W2954731415",
        "https://openalex.org/W3111507638",
        "https://openalex.org/W3172345956",
        "https://openalex.org/W2012079387",
        "https://openalex.org/W2797846142",
        "https://openalex.org/W2613328025",
        "https://openalex.org/W2997705255",
        "https://openalex.org/W3171884590",
        "https://openalex.org/W3173539742",
        "https://openalex.org/W3198794504",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2946948417",
        "https://openalex.org/W3042011474",
        "https://openalex.org/W2570343428"
    ],
    "abstract": "For many fields in real life, time series forecasting is essential. Recent studies have shown that Transformer has certain advantages when dealing with such problems, especially when dealing with long sequence time input and long sequence time forecasting problems. In order to improve the efficiency and local stability of Transformer, these studies combine Transformer and CNN with different structures. However, previous time series forecasting network models based on Transformer cannot make full use of CNN, and they have not been used in a better combination of both. In response to this problem in time series forecasting, we propose the time series forecasting algorithm based on convolution Transformer. (1) ES attention mechanism: Combine external attention with traditional self-attention mechanism through the two-branch network, the computational cost of self-attention mechanism is reduced, and the higher forecasting accuracy is obtained. (2) Frequency enhanced block: A Frequency Enhanced Block is added in front of the ESAttention module, which can capture important structures in time series through frequency domain mapping. (3) Causal dilated convolution: The self-attention mechanism module is connected by replacing the traditional standard convolution layer with a causal dilated convolution layer, so that it obtains the receptive field of exponentially growth without increasing the calculation consumption. (4) Multi-layer feature fusion: The outputs of different self-attention mechanism modules are extracted, and the convolutional layers are used to adjust the size of the feature map for the fusion. The more fine-grained feature information is obtained at negligible computational cost. Experiments on real world datasets show that the time series network forecasting model structure proposed in this paper can greatly improve the real-time forecasting performance of the current state-of-the-art Transformer model, and the calculation and memory costs are significantly lower. Compared with previous algorithms, the proposed algorithm has achieved a greater performance improvement in both effectiveness and forecasting accuracy.",
    "full_text": "976\nIEICE TRANS. INF. & SYST., VOL.E106–D, NO.5 MAY 2023\nPAPER\nTime Series Forecasting Based on Convolution Transformer\nNa WANG†,††a) and Xianglian ZHAO†, Nonmembers\nSUMMARY For many ﬁelds in real life, time series forecasting is es-\nsential. Recent studies have shown that Transformer has certain advantages\nwhen dealing with such problems, especially when dealing with long se-\nquence time input and long sequence time forecasting problems. In order\nto improve the e ﬃciency and local stability of Transformer, these studies\ncombine Transformer and CNN with di ﬀerent structures. However, previ-\nous time series forecasting network models based on Transformer cannot\nmake full use of CNN, and they have not been used in a better combination\nof both. In response to this problem in time series forecasting, we propose\nthe time series forecasting algorithm based on convolution Transformer.\n(1) ES attention mechanism: Combine external attention with traditional\nself-attention mechanism through the two-branch network, the computa-\ntional cost of self-attention mechanism is reduced, and the higher forecast-\ning accuracy is obtained. (2) Frequency enhanced block: A Frequency\nEnhanced Block is added in front of the ESAttention module, which can\ncapture important structures in time series through frequency domain map-\nping. (3) Causal dilated convolution: The self-attention mechanism module\nis connected by replacing the traditional standard convolution layer with\na causal dilated convolution layer, so that it obtains the receptive ﬁeld\nof exponentially growth without increasing the calculation consumption.\n(4) Multi-layer feature fusion: The outputs of di ﬀerent self-attention mech-\nanism modules are extracted, and the convolutional layers are used to adjust\nthe size of the feature map for the fusion. The more ﬁne-grained feature in-\nformation is obtained at negligible computational cost. Experiments on real\nworld datasets show that the time series network forecasting model struc-\nture proposed in this paper can greatly improve the real-time forecasting\nperformance of the current state-of-the-art Transformer model, and the cal-\nculation and memory costs are signiﬁcantly lower. Compared with previ-\nous algorithms, the proposed algorithm has achieved a greater performance\nimprovement in both e ﬀectiveness and forecasting accuracy.\nkey words: time series forecasting, transformer, CNN\n1. Introduction\nMost industry scenarios in today’s world, especially the In-\nternet and quantitative industries, produce a large amount of\ndata every day. Such as the trend of stock prices over time\nin the ﬁnancial ﬁeld, daily sales of e-commerce industry, the\nprices of air tickets and hotel in the tourism industry change\nwith the holiday cycle. This kind of data received at di ﬀer-\nent times and describing one or more characteristics chang-\ning with time is usually called time series data. Time series\nforecasting is to predict future data according to historical\ndata through the internal time correlation characteristics of\nManuscript received July 27, 2022.\nManuscript revised January 5, 2023.\nManuscript publicized February 15, 2023.\n†The authors are with College of Economics and Management,\nNanjing University of Aeronautics and Astronautics, China.\n††The author is also with Department of Accounting and Audit,\nNanjing Audit University Jinshen College, China.\na) E-mail: wna1982@163.com\nDOI: 10.1587/transinf.2022EDP7136\ndata.\nTime series forecasting has di ﬀerent categories from\ndiﬀerent angles. It can be divided into traditional statis-\ntics and machine learning (also divided into deep learning\nand non-deep learning) from the method and principle dis-\ntinguish. According to the forecasting step size, it can be\ndivided into single-step forecasting and multi-step forecast-\ning. In short, it is to predict one time unit or multiple time\nunits in the future. According to the input variables, it can be\ndivided into autoregressive forecasting and using covariate\nforecasting. The di ﬀerence lies in whether the dimension\ncontains covariates. For example, when predicting future\nsales, if only time and historical sales data are accepted, it is\nautoregressive forecasting. If other relevant variables such\nas weather, economic index and policy event classiﬁcation\nare accepted, it is called using covariate forecasting. Ac-\ncording to the output results, it can be divided into point\nforecasting and probability forecasting. In most scenarios,\nprobability forecasting is closer to the actual situation. For\nfuture forecasting, it is a probability distribution.\nIn recent years, the forecasting model based on deep\nneural network has shown good performance, especially the\nnetwork model based on Transformer [1]. Compared with\nthe time series forecasting model based on CNN [2]–[4] and\nRNN [5]–[7], the self-attention mechanism of Transformer\nhelps to model each part of the input sequences, so that the\nforecasting model can recover remote information and cap-\nture long distance correlation. However, the self-attention\nmechanism of Transformer also brings about the secondary\ncalculation cost of the model and the increasing memory\nconsumption with the increase of input length. In addi-\ntion, because the point multiplication mechanism in the self-\nattention mechanism is too sensitive to local context infor-\nmation, Transformer model will become unstable when re-\nceiving abnormal data.\nIn order to improve the e ﬃciency and enhance the ap-\nplicability of Transformer model, many researchers have\nproposed the Transformer like model, combined with CNN\nwhich can extract local features to solve various tasks, in-\ncluding time series forecasting. However, almost all ex-\nisting related models only use general CNN layers, such\nas standard convolutional layers and max-pooling layers.\nInformer [9] uses the classic convolution layer and maxi-\nmum pooling layer to connect the self-attention mechanism.\nTransCNN network [10] applies the pooling layer to the self-\nattention mechanism and connects the self-attention mecha-\nnism module with the deep convolution layer and the max-\nCopyright c⃝2023 The Institute of Electronics, Information and Communication Engineers\nWANG and ZHAO: TIME SERIES FORECASTING BASED ON CONVOLUTION TRANSFORMER\n977\nimum pooling layer. The application of these algorithms\nfor CNN in self-attention mechanism is still limited to com-\nmon pooling layers. Undoubtedly, this can improve the per-\nformance of Transformer model to some extent. However,\nonly the speciﬁc CNN structure is applied in Transformer,\nso that Transformer and CNN are closely integrated, so as to\ngive full play to their advantages. Although great progress\nhas been made in Transformer-based methods for time se-\nries forecasting, they tend to fail in capturing the overall\ncharacteristics of time series in some cases. To address this\nproblem, we combine Fourier analysis with the Transformer\nbased method. Instead of applying Transformer to the time\ndomain, we apply it to the frequency domain which helps\nTransformer better capture global properties of time series.\nTherefore, the work of this paper is dedicated to combin-\ning Transformer and CNN with frequency domain analysis,\nso as to improve the e ﬃciency of the forecasting model,\nstrengthen its learning ability and provide more accurate\nforecasting results. The contributions of this paper can be\nsummarized as follows.\n1. A di ﬀerent convolution Transformer model struc-\nture is proposed, which adopts three encoder layers and two\ndecoder layers. The improved forecasting model structure\nnot only enhances the local and global self-attention mech-\nanism, but also reduces the calculation cost and memory\noverhead.\n2. A two branch attention mechanism combined with\nCNN module—ESAttention is proposed, which reduces\nthe memory occupation and time complexity of traditional\nTransformer structure, and achieves better forecasting accu-\nracy.\n3. The frequency enhanced block is used before the\nESAttention, it allows us to learn time series features in\nthe frequency domain through Discrete Fourier Transforms,\nwhich can be used to predict future data more accurately.\n4. The causal dilated convolution layer is used be-\nhind the self-attention mechanism module, which helps the\nforecasting model obtain the receptive ﬁeld of exponential\ngrowth without increasing the computational cost, so as to\nstrengthen the learning ability of Transformer.\n5. A multi-layer feature fusion module is proposed to\nfuse the feature maps of di ﬀerent encoder layers with dif-\nferent scales, so as to obtain more ﬁne-grained information.\nThis extends the feature maps to obtain better results, which\ncan improve the forecasting performance of Transformer\nmodel.\n2. Literature Review\n2.1 Time Series Forecasting\nTime series forecasting plays an important role in many\nﬁelds of life, such as stock forecasting [11], human ﬂow\nanalysis, weather forecasting and so on. With the develop-\nment of deep learning and the collection of large data sets,\ntime series prediction models need longer prediction length\nand better processing methods to deal with a large amount\nof information in the past.\nTime series forecasting has been studied for decades.\nThe traditional time series forecasting algorithm is mainly\nbased on statistics, which has theoretical guarantee and\nstrong interpretability. The method based on deep learning\nmainly uses RNN recurrent neural network to process and\npredict multivariable sequences. Classical time series fore-\ncasting algorithms, such as ARIMA [12], it can only learn\nthe linear relationship between di ﬀerent time steps, which\nhas inherent defects in ﬁtting many highly nonlinear real\ntime series data. With the development of deep learning\nmodel, a lot of work in this ﬁeld has achieved the impres-\nsive results recently. For example, Qin et al. [13] use LSTM\nto capture nonlinear dynamics and long-term correlation in\ntime series data. However, as Zhao et al. [14] point out,\nthe memory ability of LSTM is still limited. To solve this\nproblem, Tang et al. [15] create an external memory to ex-\nplicitly store some representative patterns, which can e ﬀec-\ntively guide the forecasting when similar patterns appear.\nLai et al. [5] use jump connections to enable information to\nbe transmitted from distant history. Attention mechanism is\nanother option to deal with the vanishing memory.\nMost of the above work and traditional algorithms can\ndeal with multivariable time series forecasting problems,\nwhich is still not good enough in dealing with long time se-\nries output or long time series forecasting problems. Among\nthese methods, Transformer is a representative architecture,\nit only includes attention operations, which can establish the\nlong-term correlation between input and output.\n2.2 Transformer\nTransformer was a model proposed by Google in 2017 for\nnatural language processing tasks. In recent years, Trans-\nformer has made important breakthroughs in the ﬁelds of\nimage processing and natural language processing, it has\nbeen proved to be beneﬁcial to improving the performance\nof the model. Of course, some scholars have proposed sev-\neral Transformer based model structures to solve the prob-\nlems encountered in time series forecasting. It can pay at-\ntention to the long-term dependence information of the se-\nquences and support parallel computing, but itself does not\nhave the concept of sequences. It needs to add location cod-\ning to make the network learn the location information of\nthe sequences. These are developed from Transformer, and\nTransformer network structure [16] has almost no change in\nessence.\nThe application of Transformers in time series fore-\ncasting has also made great progress in the past two years.\nLogTrans[8], Informer [9] and TCCT [18] are the most rel-\nevant works with this paper. They try to combine CNN with\nTransformer. Informer [9] uses convolution layer and maxi-\nmum pool layer to connect self-attention block. TCCT [18]\nuses transmission mechanism to design tightly coupled\nconvolutional Transformer. Autoformer [17] designs the\nAuto-Correlation mechanism based on the series periodic-\nity. However, these network models do not combine con-\n978\nIEICE TRANS. INF. & SYST., VOL.E106–D, NO.5 MAY 2023\nvolution with Transformer structure well, there is room for\nfurther improvement of forecasting accuracy.\n2.3 Convolutional Neural Network\nThe relevant CNN convolutional neural networks [19]–[22]\nplay a leading role in dealing with computer vision prob-\nlems. Among many excellent research results, we focus\non two CNN network architectures related to the work in\nthis paper. They are the object detection network struc-\ntures CSPNet [23] and YOLO [24]. Although computer vi-\nsion and time series forecasting are two completely di ﬀerent\ntasks, some excellent ideas from CNN network can also be\napplied to time series forecasting. Cross Stage Partial Net-\nwork (CSPNet) [23] is committed to reducing the computa-\ntional bottleneck of complex CNN network architecture for\nhandling a large number of computer vision tasks. CSPNet\ndivides the input feature maps into two parts, and then com-\nbines the features through a cross stage hierarchical struc-\nture. In this way, the calculation amount can be greatly re-\nduced and the reasoning speed and accuracy of the network\ncan be improved. Similar concepts can also be applied to\nthe time series forecasting task of self-attention mechanism.\nThe convolutional neural network of YOLO [24] series is a\nvery famous network architecture for real-time object detec-\ntion. Even now, the most advanced object detector based on\nFig. 1 The overall network framework of the algorithm proposed in this paper.\nYOLO [24] is still being proposed. The transmission mech-\nanism ﬁrst proposed in YOLO9000 [24] has also been im-\nproved and applied to the Transformer structure to transmit\nand integrate features obtained from multiple encoder out-\nputs.\nIn addition to the ﬁeld of computer vision, CNN also\nhas good research results in time series forecasting, such as\nWaveNet[3], TCN [2], Seq-U-net [4]. These networks are\nthe use of causal convolution and its derived forms. This pa-\nper borrows the concept of causal dilated convolution from\nthe TCN to connect the self-attention mechanism module\nfor obtaining the growth of receptive ﬁeld.\n3. Network Framework\nThe network framework proposed in this paper consists\nof encoder and decoder. The complete network structure\nframework is shown in Fig. 1. The encoder contains three\nTransformer attention mechanism modules. Each attention\nmechanism module is followed by a convolution pooling\nlayer to adjust the size of the feature map, which is used\nto connect the attention modules of upper and lower layers.\nThe decoder includes a self-attention module and a cross\nattention module. The input of the decoder is the fusion fea-\nture of the encoder.\nWANG and ZHAO: TIME SERIES FORECASTING BASED ON CONVOLUTION TRANSFORMER\n979\n3.1 ESAttention\nThe structure of ESAttention module proposed in this pa-\nper is shown in Fig. 2. ESAttention adopts a dual branch\nstructure, the input feature is divided into two parts and goes\nthrough Self-Attention and External-Attention respectively,\nand then the features are combined through cross phase ad-\ndition. Self-Attention can e ﬀectively establish the internal\nfeatures of samples, but it requires a lot of computation and\nonly uses the information in its own samples. External-\nAttention can establish the relationship between di ﬀerent\nsamples and reduce the amount of computation by intro-\nducing two smaller external matrices. In the Fig. 2, input\nX ∈R\nL×d, where L is the length of the input sequence and\nd is the dimension of the input sequence. The input se-\nquence is divided into two parts according to dimensions:\nX =[XL×d1\n1 ,XL×d2\n2 ]. X1 is the input of Part 1, X2 is the in-\nput of Part 2, X1 passes through a Self-Attention module\nand X2 passes through an External-Attention module. The\noutput results of two parts are spliced to get the ﬁnal re-\nsult. Self-Attention in Part 1 can enhance the features by\ncalculating their similarity and then updating the features\naccording to the similarity. Speciﬁcally, for the input fea-\nture F ∈N ×d, Q, K, V are a linear transformation of F,\nand the Self-Attention mechanism can be written as follows:\nA = (α)\ni,j = softmax(QKT ), Fout = AV. Part 2 passes\nthrough External-Attention, and its structure is shown in\nFig. 3. It consists of only two linear layers and two nor-\nmalization layers, and has linear computational complexity.\nThe formula is: A =Norm(FM\nT\nK), Fout =AMV .F o r t h e\nFig. 2 The network structure of ESAttention.\nFig. 3 The network structure of External-Attention.\nSelf-Attention, it has two disadvantages: one is the com-\nputational complexity of square order, and the other is to\nonly consider the value of single sample for attention. Com-\npared with Self-Attention, External-Attention uses an exter-\nnal matrix M to model the similarities between the i-th pixel\nand the j-th row, and M is learnable and variable in size.\nAt the same time, External-Attention can implicitly learn\nthe characteristics of the entire dataset through the external\nmatrix M, thus it can model the relationship between dif-\nferent samples in the entire dataset. In the ESAttention,\nwe adopt a dual branch strategy and introduce External-\nAttention, which can not only enable the network learn more\nrich features, but also e ﬀectively reduce the amount of cal-\nculation and memory consumption, and improve the reason-\ning speed and accuracy of the network.\n3.2 Frequency Enhanced Block\nTransformer based forecasting algorithms signiﬁcantly im-\nprove the latest results of long-term series forecasting, but\nthey cannot capture the overall characteristics and global\nview of time series in some cases. To solve these problems,\nwe propose to combine the Transformer with the Frequency\nEnhanced Block. Frequency Enhanced Block can capture\nthe global contour of the time series, while Transformer can\ncapture the more detailed structure.\nThe proposed Frequency Enhanced Block is imple-\nmented through Discrete Fourier Transform. Given a se-\nquence of real numbers x\nn in time domain, where n =\n1,2,..., N. Discrete Fourier Transform is deﬁned as Xl =∑N−1\nn=0 xne−iωln, where i is the imaginary unit and Xl, l =\n1,2,..., L is a sequence of complex numbers in the fre-\nquency domain. Similarly, inverse Discrete Fourier Trans-\nform is deﬁned as x\nn =∑L−1\nl=0 Xleiωln. The input ( x ∈RN×D)\nof Frequency Enhanced Block is ﬁrst linearly projected with\nω∈R\nD×D, thus q =x ·ω. Then q is converted from time\ndomain to frequency domain. The Fourier transform of q is\ndenoted as Q ∈CN×D. In the frequency domain, only the\nrandomly selected M modes are kept, thus we use a select\noperator as:\n˜Q =Select(Q) =Select(F(q)) (1)\nwhere ˜Q ∈CM×D and M ≪N. Here, M and N refer to the\nlength of the sequence, and D refers to the dimension of the\nfeature. Frequency Enhanced Block is deﬁned as:\nFEB(q) =F−1(Padding( ˜Q ⊙R)) (2)\nwhere R ∈ CD×D×M, is a parameterized kernel initialized\nrandomly. Let Y =Q ⊙C, with Y ∈CM×D. The production\noperator ⊙is deﬁned as: Ym,do =∑D\ndi=0 Qm,di ·Rdi,do,m, where\ndi =1,2,..., D is the input channel and do =1,2,..., D is\nthe output channel. The result of Q ⊙R is then zero-padded\nto CN×D before performing inverse Fourier transform back\nto the time domain. The structure is shown in Fig. 4.\n3.3 Causal Dilated Convolution\nUtilizing multiple self-attention blocks is beneﬁcial to ex-\n980\nIEICE TRANS. INF. & SYST., VOL.E106–D, NO.5 MAY 2023\nFig. 4 Frequency Enhanced Block with Fourier transform structure.\nFig. 5 Schematic diagram of causal dilated convolution model.\ntract deeper feature maps, however, it brings more computa-\ntional complexity in time and space. To further reduce mem-\nory consumption, Informer uses sparse attention operations.\nAnd it uses convolution layer and maximum pooling layer\nbetween each two self-attention blocks to trim the length of\nthe input sequences. After the ﬁrst attention module, a con-\nvolution layer with a step size of 1 and a convolution ker-\nnel size of 3 is used to make the features focus on the local\ncontext information. The maximum pooling layer with step\nsize of 2 and kernel size of 3 is connected behind the con-\nvolution layer to give priority to local dominant features and\nprovide less but more focused feature mapping for the latter\nself-attention module.\nHowever, the classical convolution layer has two main\ndisadvantages in time series forecasting. First, it can only\ntrace back the historical sequences of linear size as the depth\nof network growth. Thus even though Informer also intends\nto deal with the forecasting of long time series, it is not\nstrong enough in dealing with ultra-long series. Stacking\nthe self-attention mechanism module with the classical con-\nvolution layer will not bring more beneﬁts with the increase\nof computing cost. On the contrary, it may lead to repeated\nand meaningless computing because of the limited recep-\ntive ﬁeld. In addition, the classical convolution layer has no\ntime concept in the time series forecasting problem, which\nwill inevitably lead to the leakage of future information.\nInspired by the TCN algorithm model, the causal di-\nlated convolution is used to replace the traditional convo-\nlution layer to obtain the exponential growth of receptive\nﬁeld. The schematic diagram of causal dilated convolution\nis shown in Fig. 5. For the i-th convolution layer after the\ni-th self-attention mechanism module, the number of jump\nelements between each two adjacent ﬁlter taps of causal di-\nlated convolution is: (2\ni−1 −1). In addition, each element\nx of the sequence in time t is convoluted only with the el-\nements before time t. Because of the nature of causality, it\nis only ﬁlled at the front end of the timeline to ensure that\nthere will be no leakage of future information.\nIn the proposed algorithm model, even if there are only\ntwo causal dilated convolution layers, the receptive ﬁeld that\nour network can use is signiﬁcantly larger than the tradi-\ntional convolution layer. When more self-attention mecha-\nnism modules are stacked with causal expansion convolu-\ntion, the gap between receptive ﬁelds will be larger and the\nperformance of time series forecasting network will be bet-\nter. In addition, the application of causal dilated convolu-\ntion only brings a small amount of computational cost and\nalmost negligible memory use.\nWhen used in combination with ESAttention module,\nthe causal dilated convolution layer can also be used as a\ntransition layer to fuse the feature maps from the two parts\nof the previous self-attention module.\n3.4 Multi-Layer Feature Fusion\nFeature transmission fusion mechanism is usually used in\nthe ﬁeld of computer vision to extract features from convo-\nlutional neural network. Similar concepts can also be ap-\nplied to Transformer based networks. The feature pyramid\nstructure proposed from the ﬁeld of target detection can ob-\ntain multiple feature maps of di ﬀerent scales from the fea-\nture extraction network, and weight and fuse them to obtain\nthe ﬁnal feature map, which can extract more ﬁne-grained\ninformation.\nThe feature maps with di ﬀerent scales are used in\nTransformer based networks. In this paper, three self-\nattention mechanism modules are set in the encoder, and\neach self-attention mechanism module will produce a fea-\nture map. The length of the k-th feature map is L/2\nk−1 and\nthe dimension size is d. In order to extract three feature\nmaps with the same size for fusion, the three feature maps\nobtained from the attention mechanism module are cropped\nﬁrstly, and the k-th feature map is equivalently divided into\n2\nn−k feature maps with length L/2n−1. Then the dimen-\nsions of each feature map is adjusted by three di ﬀerent one-\ndimensional convolution to obtain three feature maps with\nthe same size. They are simply added, and the ﬁnal feature\nmap is sent to the decoder.\nThe function of multi-layer feature fusion is di ﬀerent\nfrom the full distillation operation of Informer. The full dis-\ntillation operation needs as many encoders as the modules\nof self-attention mechanism, while only one encoder can be\nrequired in our multi-layer feature fusion mechanism. Al-\nthough the input length of the encoder of Informer with full\ndistillation operation gradually decreases, it makes the net-\nwork model pay more attention to the later time series data.\nThe multi-layer feature fusion module used in this paper\nis also very di ﬀerent from the transmission mechanism in\nTCCT. Firstly, the feature maps of the three encoders are ex-\nWANG and ZHAO: TIME SERIES FORECASTING BASED ON CONVOLUTION TRANSFORMER\n981\ntracted, then the size of these three feature maps is adjusted\nto the same through the clipping operation, next the channel\nnumber of each feature is consistent through the three con-\nvolution layers, and ﬁnally the feature fusion is completed\nthrough the feature addition operation. Compared with the\nfeature stitching in TCCT, the fusion method used here can\nreduce a certain amount of parameters, and it can better fuse\nthe features of each layer in the network. More importantly,\nthe multi-layer feature fusion module will hardly bring addi-\ntional calculation cost, while Informer with full distillation\noperation will incur considerable additional calculation cost\ndue to its multi-encoder architecture.\n4. Experimental Results and Analysis\nThe experimental results of the proposed algorithm on the\nETT public dataset are given in this section, it is compared\nwith some advanced time series forecasting algorithms, and\nthe ablation experiments are conducted to evaluate the con-\ntribution of each component in the model.\n4.1 Experimental Details\nBaseline network. The baseline of this algorithm is In-\nformer which is the best paper of AAAI-2021. Informer\nis also a network model derived from the traditional Trans-\nformer framework for time series forecasting. Informer\nhas made some improvements on Transformer structure to\nenhance the forecasting ability of Transformer framework\nmodel in Long Series Time Forecasting (LSTF). Because of\nits advanced performance, it is used as the baseline in this\npaper.\nExperimental stage. Multiple groups of experiments\nare carried out to evaluate the accuracy and e ﬀect of the pro-\nposed network model architecture in time series forecasting.\nMSE is selected as the loss function. All methods are op-\ntimized by Adam optimizer. The learning rate starts from\n1e-4 and each epoch is reduced by twice. The initial token\nlength of decoder is consistent with the input length of en-\ncoder. The total number of epoch is 6, and it will be stopped\nin advance appropriately. The batch size is set to 32. Each\nindividual experiment is repeated ten times and the average\nresult is taken. All experiments are conducted on a single\nNvidia GTX 1080Ti 12 GB GPU.\nEvaluation datasets and indicators.The experiment\nis conducted on the public real world dataset ETT (Electric-\nity Transformer Temperature), which is composed of ETT\ndata lasting for nearly two years. ETT dataset consists of\nfour subsets: {ETTh1 and ETTh2 }are one hour datasets\nfrom two counties in China, {ETTm1 and ETTtm2 }are ﬁf-\nteen minute datasets from the same source in {ETTh1 and\nETTh2}. Each data point is composed of the target value “oil\ntemperature” and six other power load features. The train-\ning set, veriﬁcation set and testing set are 12 /4/4 months by\ndefault.\n4.2 Experimental Results and Analysis\nOn the ETTh1 and ETTm1 datasets, the input sequence\nlength of encoder is set to 384, and the length of the fore-\ncasting sequence is {48, 96, 192, 384 }. While on the ETTh2\nand ETTm2 datasets, the input sequence length of the en-\ncoder is set to 96, and the length of the forecasting se-\nquence is {96, 192, 336, 720 }, and the forecasting sequence\nlength gradually increases. The evaluation indicators are:\nMSE =\n1\nn\nn∑\ni=1\n(y−y∧)2 and MAE = 1\nn\nn∑\ni=1\n|y−y∧|, where n\nis the length of the forecasting window. The experiment\nin this section is conducted on the ETTh1, ETTm1, ETTh2\nand ETTm2 four datasets, and it is divided into univariate\nforecasting and multivariate forecasting. The experimental\nresults are shown in Tables 1–8. Experimental results show\nthat the forecasting accuracy of the algorithm proposed in\nthis paper is generally better than the existing forecasting\nmodels. ESAttention module designed in this paper can\nTable 1 Univariate forecasting results on ETTh1 dataset.\nTable 2 Multivariate forecasting results on ETTh1 dataset.\nTable 3 Univariate forecasting results on ETTh2 dataset.\nTable 4 Multivariate forecasting results on ETTh2 dataset.\nTable 5 Univariate forecasting results on ETTm1 dataset.\n982\nIEICE TRANS. INF. & SYST., VOL.E106–D, NO.5 MAY 2023\nTable 6 Multivariate forecasting results on ETTm1 dataset.\nTable 7 Univariate forecasting results on ETTm2 dataset.\nTable 8 Multivariate forecasting results on ETTm2 dataset.\nTable 9 Comparisons of time and memory consumption.\neﬀectively reduce parameters and improve forecasting ac-\ncuracy by combining self-attention and external attention\nmechanism in the form of double branches. Frequency En-\nhanced Block can obtain important information in the time\nseries through frequency domain mapping, so as to predict\nfuture data more accurately. Causal Dilated Convolution is\nalso contributed to the forecasting of future data by greatly\nincreasing the form of receptive ﬁeld. Through multi-layer\nfeature fusion mechanism, the outputs of multiple encoder\nlayers are cleverly fused to obtain more ﬁne-grained feature\nrepresentation.\nIn the experiment of univariate forecasting, the algo-\nrithm in this paper performs well. The forecasting results of\nthe proposed algorithm are better than the latest algorithms\nInformer, TCCT and Autoformer with the increase of the\nforecasting sequence length. In the multivariable forecast-\ning experiment, the e ﬀect of this algorithm in forecasting\nshort sequences is generally better than the latest algorithm.\nIn multivariable forecasting, the correlation between vari-\nables will weaken with the increase of forecasting sequence\nlength, which will a ﬀect the relationship between each vari-\nable modeled by self-attention and external attention mecha-\nnism in ESAttention module. Thus the forecasting accuracy\nwill decline with the increase of the forecasting sequence\nlength in multivariate forecasting.\nIn Table 9, we also show the comparison of time and\nmemory consumption of several di ﬀerent algorithms. With\nthe integration of various innovation parts, the parameter\nquantity of our algorithm has reached 3.643M. Compared\nTable 10 Ablation experiments on ETTh1 dataset.\nwith the 3.864M parameter quantity of Autoformer, the pa-\nrameter quantity of our algorithm is reduced by 5.7%, and\ncompared with the 3.827M parameter quantity of TCCT, our\nalgorithm is reduced by 4.8%. It shows that the innovation\nproposed in this paper can make the whole network more\nlightweight. In terms of training speed, Autoformer takes\naverage of 65.2 s to train each epoch, TCCT takes an aver-\nage of 64.3 s to train each epoch, while our algorithm takes\nan average of 62.5 s to train each epoch, which is 4.3% faster\nthan Autoformer and 2.9% faster than TCCT. The experi-\nmental results show that the time series forecasting algo-\nrithm proposed in this paper has more advantages than the\nlatest advanced algorithms (TCCT and Autoformer) on the\nkey contribution of time and memory consumption. All ex-\nperiments are performed on the ETTh1 dataset.\nIn addition, we extensively evaluate our proposed\nmethod on ﬁve other benchmarks, covering ﬁve mainstream\ntime series forecasting applications: energy, economics,\ntraﬃc, weather and disease. Here, Electricity dataset con-\ntains the hourly electricity consumption of 321 customers\nfrom 2012 to 2014. Exchange records the daily exchange\nrates of eight di ﬀerent countries ranging from 1990 to 2016.\nTraﬃc is a collection of hourly data from California De-\npartment of Transportation, which describes the road occu-\npancy rates measured by di ﬀerent sensors on San Francisco\nBay area freeways. Weather is recorded every 10 minutes\nfor 2020 whole year, which contains 21 meteorological in-\ndicators, such as air temperature, humidity, etc. ILI includes\nthe weekly recorded inﬂuenza-like illness patient data from\nCenters for Disease Control and Prevention of the United\nStates between 2002 and 2021, which describes the ratio of\npatients seen with ILI and the total number of the patients.\nThe experimental results are shown in Table 11. Experimen-\ntal results further show that the forecasting accuracy of the\nalgorithm proposed in this paper is better than the existing\nadvanced forecasting models.\n4.3 Ablation Experiment and Analysis\nFour ablation experiments are designed on the ETTh1\nWANG and ZHAO: TIME SERIES FORECASTING BASED ON CONVOLUTION TRANSFORMER\n983\nTable 11 Multivariate forecasting results on four datasets, we set the input length as 36 for ILI and\n96 for the others.\ndataset, and the univariate forecasting results are given.\nESAttention is replaced with the traditional self-attention\nmechanism module in ablation experiment I. The causal\ndilated convolution is replaced with causal convolution in\nablation experiment II. The multi-layer feature transmission\nmechanism is removed, only the output of the last encoder\nis sent to the decoder in ablation experiment III. The fre-\nquency enhanced block is removed in ablation experiment\nIV. ESAtention is replaced with CSPattention of TCCT\nin ablation experiment V. Multi-layer feature fusion is re-\nplaced with the passthrough mechanism of TCCT in abla-\ntion experiment VI. The ablation experimental results are\nshown in Table 10.\nThe results show that the improvement measures pro-\nposed in this paper are e ﬀective. In ablation experi-\nment I, after replacing ESAttention with the traditional self-\nattention mechanism module, the forecasting accuracy is de-\ncreased signiﬁcantly, it is indicated that the designed ES-\nAttention module is e ﬀective in time series forecasting task,\nand it plays a signiﬁcant role in improving the forecasting\naccuracy. In ablation experiment II, the causal dilated con-\nvolution is replaced by causal convolution. When the length\nof the forecasting sequence is 48, the e ﬀect decline is not\nsigniﬁcantly. However, with the increase of the length of\nthe forecasting sequence, the receptive ﬁeld of the input se-\nquence obtained by the network in this paper becomes larger\nand larger, and the causal dilated convolution module can\nalso play a good e ﬀect. In ablation experiment III, after re-\nmoving the multi-layer feature fusion module, the forecast-\ning accuracy is signiﬁcantly reduced, which indicates that\nthe multi-layer feature fusion mechanism proposed in this\npaper plays a positive role in improving the forecasting ac-\ncuracy. In ablation experiment IV , after removing frequency\nenhanced block before the ESAttention module, the results\nof the prediction accuracy are not particularly ideal, which\nindicates the e ﬀectiveness of frequency enhanced block.\nThe introduction of frequency enhanced block can signif-\nicantly improve the prediction performance of the model.\nThis is because that frequency enhanced block can not only\ncombine Transformer with frequency analysis, but also en-\nable the network model to capture important structure infor-\nmation in time series through frequency domain mapping,\nwhich is more e ﬀective for long-term time series predic-\ntion. In ablation experiment V , after replacing ESAttention\nwith CSPAttention in TCCT, the forecasting accuracy is de-\ncreased, it indicates that ESAttention proposed in this pa-\nper has a better forecasting e ﬀect than CSPAttention. This\nis because that in the double branch structure of CSPAtten-\ntion, one branch passes through a 1 ∗1 convolution layer to\nadjust the dimension, and the other branch passes through\na Self-Attention module. While in the ESAttention, one\nbranch passes through the additional attention module, and\nthe other branch passes through the Self-Attention module.\nThis structure can not only e ﬀectively reduce the memory\noccupation and time complexity of the traditional attention\nstructure, but also can model the relations between di ﬀer-\nent samples to obtain better prediction accuracy. In abla-\ntion experiment VI, after replacing multi-layer feature fu-\nsion with the passthrough mechanism in TCCT, the fore-\ncasting accuracy is decreased signiﬁcantly, it indicates that\nthe multi-layer feature fusion has better forecasting e ﬀect\nthan passthrough mechanism. This is because that we ﬁrstly\nuse the clipping operation to adjust the three feature maps to\nthe same size, then adjust the number of channels through\nthe three convolution layers, and ﬁnally directly add the fea-\nture maps. This fusion method can reduce certain parame-\nters compared with feature stitching in TCCT transmission\nmechanism, and can better fuse the features obtained in each\nencoder.\n5. Conclusion\nA time series forecasting network based on convolutional\nneural network and Transformer being closely combined\n984\nIEICE TRANS. INF. & SYST., VOL.E106–D, NO.5 MAY 2023\nis proposed in this paper. ESAttention attention mecha-\nnism module is designed, which adopts the idea of double\nbranches, and the use of the external attention and self-\nattention modules can e ﬀectively reduce the memory con-\nsumption and improve the forecasting accuracy. Before the\nattention mechanism module, the frequency enhanced block\nis used to capture the global features of the sequence in\nthe frequency domain. In addition, replacing the traditional\nclassical convolution layer with the causal dilated convolu-\ntion can make Transformer module obtain larger receptive\nﬁeld without increasing memory and calculation consump-\ntion. More importantly, the idea of transmission mechanism\nin target detection is adopted in this paper, a multi-layer fea-\nture fusion module is proposed, it extracts the output fea-\ntures of multiple transformer encoders, and uses the convo-\nlution layer to adjust the fusion, which helps the decoder to\nobtain more ﬁne-grained feature representation. Su ﬃcient\nexperiments on several public datasets show that the algo-\nrithm proposed in this paper is reliable and e ﬀective.\nAcknowledgments\nThis work was supported by University Philosophy and\nSocial Science Research Project of Jiangsu province,\n2019SJA2034; Key Projects of Special Topics for Finan-\ncial Development of High-quality Social Science Applica-\ntion Research Project of Jiangsu province, 17SCA-06.\nReferences\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Ad-\nvances in Neural Information Processing Systems, vol.30, pp.5998–\n6008, 2017.\n[2] S. Bai, J.Z. Kolter, and V . Koltun, “Convolutional sequence mod-\neling revisited,” International Conference on Learning Representa-\ntions, 2018.\n[3] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals,\nA. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu,\n“WaveNet: A generative model for raw audio,” arXiv preprint arXiv:\n1609.03499, 2016.\n[4] D. Stoller, M. Tian, S. Ewert, and S. Dixon, “Seq-U-Net: A one-\ndimensional causal U-net for e ﬃcient sequence modelling,” arXiv\npreprint arXiv: 1911.06393, 2019.\n[5] G. Lai, W.-C. Chang, Y . Yang, and H. Liu, “Modeling long- and\nshort-term temporal patterns with deep neural networks,” The 41st\nInternational ACM SIGIR Conference on Research & Development\nin Information Retrieval, pp.95–104, 2018.\n[6] R. Yu, S. Zheng, A. Anandkumar, and Y . Yue, “Long-term fore-\ncasting using tensor-train RNNs,” arXiv preprint arXiv: 1711.00073,\n2017.\n[7] D. Salinas, V . Flunkert, J. Gasthaus, and T. Januschowski,\n“DeepAR: Probabilistic forecasting with autoregressive recurrent\nnetworks,” International Journal of Forecasting, vol.36, no.3,\npp.1181–1191, 2020.\n[8] S. Li, X. Jin, Y . Xuan, X. Zhou, W. Chen, Y .-X. Wang, and X.\nYan, “Enhancing the locality and breaking the memory bottleneck\nof transformer on time series forecasting,” arXiv preprint arXiv:\n1907.00235, 2019.\n[9] H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, and W.\nZhang, “Informer: Beyond e ﬃcient transformer for long sequence\ntime-series forecasting,” arXiv preprint arXiv: 2012.07436, 2020.\n[10] Y . Liu, G. Sun, Y . Qiu, L. Zhang, A. Chhatkuli, L.V . Gool, “Trans-\nformer in Convolutional Neural Networks,” arXiv preprint arXiv:\n2106.03180, 2021.\n[11] K.-J. Kim, “Financial time series forecasting using support vector\nmachines,” Neurocomputing, vol.55, no.1-2, pp.307–319, 2003.\n[12] G.E.P. Box and G.M. Jenkins, “Some recent advances in forecast-\ning and control,” Journal of the Royal Statistical Society: Series C\n(Applied Statistics), vol.17, no.2, pp.91–109, 1968.\n[13] Y . Qin, D. Song, H. Chen, W. Cheng, G. Jiang, and G.W. Cottrell, “A\ndual-stage attention-based recurrent neural network for time series\nprediction,” Proc. 26th International Joint Conference on Artiﬁcial\nIntelligence, pp.2627–2633, 2017.\n[14] J. Zhao, F. Huang, J. Lv, Y . Duan, Z. Qin, G. Li, and G. Tian, “Do\nRNN and LSTM have long memory?” International Conference on\nMachine Learning, pp.11365–11375, 2020.\n[15] X. Tang, H. Yao, Y . Sun, C. Aggarwal, P. Mitra, and S. Wang, “Joint\nmodeling of local and global temporal dynamics for multivariate\ntime series forecasting with missing values,” AAAI, vol.34, no.4,\npp.5956–5963, 2020.\n[16] B. Lim, S. ¨O. Arık, N. Loe ﬀ, and T. Pﬁster, “Temporal fusion\ntransformers for interpretable multi-horizon time series forecasting,”\nInternational Journal of Forecasting, vol.37, no.4, pp.1748–1764,\n2021.\n[17] H. Wu, J. Xu, J. Wang, and M. Long, “Autoformer: Decompo-\nsition transformers with auto-correlation for long-term series fore-\ncasting,” Proc. Advancesin Neural Information Processing Systems\n(NeurIPS), pp.101–112, 2021.\n[18] L. Shen and Y . Wang, “TCCT: Tightly-coupled convolutional\ntransformer on time series forecasting,” arXiv preprint arXiv:\n2108.12784, 2021.\n[19] A. Krizhevsky, I. Sutskever, and G.E. Hinton, “ImageNet classiﬁca-\ntion with deep convolutional neural networks,” Advances in Neural\nInformation Processing Systems, vol.25, pp.1097–1105, 2012.\n[20] K. Simonyan and A. Zisserman, “Very deep convolutional networks\nfor large-scale image recognition,” arXiv preprint arXiv: 1409.1556,\n2014.\n[21] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” Proc. IEEE Conference on Computer Vision and\nPattern Recognition, pp.770–778, 2016.\n[22] M. Tan and Q. Le, “E ﬃcientNet: Rethinking model scaling for con-\nvolutional neural networks,” International Conference on Machine\nLearning, pp.6105–6114, 2019.\n[23] C.-Y . Wang, H.-Y .M. Liao, Y .-H. Wu, P.-Y . Chen, J.-W. Hsieh, and\nI.-H. Yeh, “CSPNet: A new backbone that can enhance learning ca-\npability of CNN,” Proc. IEEE /CVF Conference on Computer Vision\nand Pattern Recognition Workshops, pp.1571–1580, 2020.\n[24] J. Redmon and A. Farhadi, “YOLO9000: Better, faster, stronger,”\nProc. IEEE Conference on Computer Vision and Pattern Recogni-\ntion, pp.6517–6525, 2017.\nWANG and ZHAO: TIME SERIES FORECASTING BASED ON CONVOLUTION TRANSFORMER\n985\nNa Wang received the B.S. degree from\nShandong University of Technology in 2004,\nand M.S. degree from Nanjing University of\nAeronautics and Astronautics, in 2008. Since\n2015, she studied for a doctorate in the school\nof economics and management of Nanjing Uni-\nversity of Aeronautics and Astronautics. Since\n2021, she has been with Nanjing Audit Univer-\nsity Jinshen College, Nanjing, China, where she\nis currently an associate professor in the Depart-\nment of Accounting and Audit. Her current re-\nsearch interests include machine learning and time series forecasting.\nXianglian Zhao received the Ph.D. degree\nfrom Nanjing University of Science and Tech-\nnology, in 2005. Since 2005, she has been with\nNanjing University of Aeronautics and Astro-\nnautics, Nanjing, China, where she is currently a\nprofessor in the College of Economics and Man-\nagement. Her current research interests include\nmachine learning and time series forecasting."
}