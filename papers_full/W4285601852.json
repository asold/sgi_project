{
  "title": "CATrans: Context and Affinity Transformer for Few-Shot Segmentation",
  "url": "https://openalex.org/W4285601852",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5100325428",
      "name": "Shan Zhang",
      "affiliations": [
        "Australian National University",
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5101956448",
      "name": "Tianyi Wu",
      "affiliations": [
        "Baidu (China)",
        "National Engineering Laboratory of Deep Learning Technology and Application"
      ]
    },
    {
      "id": "https://openalex.org/A5016573203",
      "name": "Sitong Wu",
      "affiliations": [
        "Baidu (China)",
        "National Engineering Laboratory of Deep Learning Technology and Application"
      ]
    },
    {
      "id": "https://openalex.org/A5085022758",
      "name": "Guodong Guo",
      "affiliations": [
        "Baidu (China)",
        "National Engineering Laboratory of Deep Learning Technology and Application"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3030520226",
    "https://openalex.org/W3169596906",
    "https://openalex.org/W6754568377",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2144794286",
    "https://openalex.org/W3140935014",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2563705555",
    "https://openalex.org/W3042143320",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W6864487941",
    "https://openalex.org/W3191241195",
    "https://openalex.org/W2976499615",
    "https://openalex.org/W2754560798",
    "https://openalex.org/W3046971235",
    "https://openalex.org/W6864014924",
    "https://openalex.org/W2969082793",
    "https://openalex.org/W3108187451",
    "https://openalex.org/W3048290571",
    "https://openalex.org/W6791858558",
    "https://openalex.org/W2897139960",
    "https://openalex.org/W3166448862",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3106906018",
    "https://openalex.org/W2893918048",
    "https://openalex.org/W3214418393",
    "https://openalex.org/W2983850069",
    "https://openalex.org/W2990230185",
    "https://openalex.org/W2963078159",
    "https://openalex.org/W3033502887",
    "https://openalex.org/W3108189450",
    "https://openalex.org/W4308909683",
    "https://openalex.org/W2981787211",
    "https://openalex.org/W3204077273",
    "https://openalex.org/W2963599420"
  ],
  "abstract": "Few-shot segmentation (FSS) aims to segment novel categories given scarce annotated support images. The crux of FSS is how to aggregate dense correlations between support and query images for query segmentation while being robust to the large variations in appearance and context. To this end, previous Transformer-based methods explore global consensus either on context similarity or affinity map between support-query pairs. In this work, we effectively integrate the context and affinity information via the proposed novel Context and Affinity Transformer (CATrans) in a hierarchical architecture. Specifically, the Relation-guided Context Transformer (RCT) propagates context information from support to query images conditioned on more informative support features. Based on the observation that a huge feature distinction between support and query pairs brings barriers for context knowledge transfer, the Relation-guided Affinity Transformer (RAT) measures attention-aware affinity as auxiliary information for FSS, in which the self-affinity is responsible for more reliable cross-affinity. We conduct experiments to demonstrate the effectiveness of the proposed model, outperforming the state-of-the-art methods.",
  "full_text": "CATrans: Context and Affinity Transformer for Few-Shot Segmentation\nShan Zhang 1‚àó\n, Tianyi Wu2,3 , Sitong Wu2,3 , Guodong Guo2,3 ‚Ä†\n1Australian National University, Canberra, Australia\n2Institute of Deep Learning, Baidu Research, Beijing, China\n3National Engineering Laboratory for Deep Learning Technology and Application, Beijing, China\nShan.Zhang@anu.edu.au, wusitong98@gmail.com, {wutianyi01, guoguodong01}@baidu.com\nAbstract\nFew-shot segmentation (FSS) aims to segment\nnovel categories given scarce annotated support\nimages. The crux of FSS is how to aggregate\ndense correlations between support and query im-\nages for query segmentation while being robust to\nthe large variations in appearance and context. To\nthis end, previous Transformer-based methods ex-\nplore global consensus either on context similar-\nity or affinity map between support-query pairs. In\nthis work, we effectively integrate the context and\naffinity information via the proposed novel Context\nand Affinity Transformer (CATrans) in a hierarchi-\ncal architecture. Specifically, the Relation-guided\nContext Transformer (RCT) propagates context in-\nformation from support to query images condi-\ntioned on more informative support features. Based\non the observation that a huge feature distinction\nbetween support and query pairs brings barriers\nfor context knowledge transfer, the Relation-guided\nAffinity Transformer (RAT) measures attention-\naware affinity as auxiliary information for FSS,\nin which the self-affinity is responsible for more\nreliable cross-affinity. We conduct experiments\nto demonstrate the effectiveness of the proposed\nmodel, outperforming the state-of-the-art methods.\n1 Introduction\nFully supervised semantic segmentation has made tremen-\ndous progress in recent years [Long et al., 2015; Lin et\nal., 2017 ]. However, these methods rely heavily on a\nlarge amount of pixel-wise annotations which requires in-\ntensive manual labor, and are incapable of generalizing to\nnew classes with a handful of annotations. In contrast, hu-\nmans can recognize a new category even with little guid-\nance. Inspired by this, Few-shot segmentation (FSS) has\nrecently received a growing interest in the computer vi-\nsion community [Wang et al., 2019; Zhang et al., 2019b;\nZhang et al., 2020].\n‚àóInterns at the Institute of Deep Learning, Baidu Research\n‚Ä†Corresponding author\n‚®Ç\n‚®Ç ùê¥##\n‚®Ç\n‚®Ç\nRCT\nRAT\nTransformer BlockDecoder Classifier\n(c)\n(a)\n(b)\n‚®Ç Matrix Multiplication\n(d)\nùëÄ#\nùêπ#\nùêπ&\nùêπ'\nùê¥#&\nùê¥&&ùêπ#\nùêπ&\nùíû##\t\nùíû#&\t\nùíû&&\t\n‚äö\nùê¥#&\nùêπ#\nùêπ&\nùíû#&\t\nùêπ#\nùêπ&\nclassifier\nweights\n‚®Ç\n‚äö Concatenation\nùíû&&\t\nFigure 1: Comparisons with different Transformer-based few-shot\nsegmentation methods. The red and blue square shadow denote\ncontext (C) and affinity ( A) aggregation, respectively. (a) Classi-\nfier Weight Transformer [Lu et al., 2021 ]. (b) Cycle-Consistent\nTransformer [Zhang et al., 2021]. (c) Cost Aggregation Transformer\n[Cho et al., 2021]. (d) Our Context and Affinity Transformer with\nRelation-guided Context Transformer (RCT) and Relation-guided\nAffinity Transformer (RAT).\nThe goal of FSS is to segment the novel class in the query\nimage, conditioned on the given support set which contains\nonly a few support images and the corresponding ground-\ntruth masks. A fundamental challenge is the large intra-class\nappearance and geometric variations between support-query\npairs, so the key issue is how to effectively reason the rela-\ntionships of paired samples.\nMost FSS methods [Cho et al., 2021; Zhang et al., 2021;\nLu et al., 2021] follow a learning-to-learn paradigm. Specifi-\ncally, features are extracted from both query and support im-\nages, and then passed through a feature matching procedure\nto transfer the support mask to the query image. The con-\nvolutional neural network (CNN)-based approaches [Wang\net al., 2019; Yang et al., 2020; Liu et al., 2020 ] condense\nthe masked object features in the support image into a sin-\ngle or few prototypes. Recently, some approaches intro-\nduce Transformer-based architecture to establish pixel-wise\nmatching scores between support-query pairs, containing two\nmain technical routes, ie., the context and affinity.\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n1658\nAs the dense context information is beneficial to FSS task,\nespecially when the large intra-class variances exist in the\nsupport and query set, [Lu et al., 2021] propose the Clas-\nsifier Weight Transformer (CWT) to dynamically adapt the\nclassifier‚Äôs weights trained on support set to each query im-\nage as shown in Figure 1 (a). [Zhang et al., 2021] aggregates\nthe context information within query images and between\nsupport-query pairs via transformer blocks for self- and cross-\nalignment, as shown in Figure 1 (b). This method, however,\nsuffers from unrepresentative of support feature which stimu-\nlates us to propose the Relation-guided Context Transformer\n(RCT), in which the global context information of support can\nbe considered with a mask encoder [Johnander et al., 2021]\nand a transformer block. The RCT takes as input discrimi-\nnative self-feature to build more accurate cross-correlation of\ncontext, as briefly shown in Figure 1 (d).\nThe attention-aware affinity is globally constructed be-\ntween support and query features as another guidance for\nquery segmentation. The Cost Aggregation with Transform-\ners (CATs) [Cho et al., 2021] builds the cross-affinity be-\ntween support and query features followed by transformer\nblocks, as shown in Figure 1 (c). However, this method does\nnot incorporate individual self-affinity for support object or\nquery image to disambiguate noisy correlations, which mea-\nsures pixel-wise correspondences within itself, enabling each\nspatial fiber to match itself and other tokens. We, thus, design\na Relation-guided Affinity Transformer (RAT) for generating\na reliable cross-affinity map inherited from the self-affinity.\nThe illustration is schematically depicted in the Figure 1 (d).\nAdditionally, we explore how to utilize both context and\naffinity guidance simultaneously. To be specific, we develop\na hierarchical CATrans: Context and Affinity Transformer,\nwhere we leverage a stack of multi-level correlation maps re-\nlated to both context and affinity. Moreover, following by\n[Johnander et al., 2021], we also concatenate the query em-\nbedding with high resolution with those low-resolution corre-\nlation maps to guide the decoder.\nOverall, our contributions are : i) we design a Relation-\nguided Context Transformer (RCT) with the enhanced sup-\nport features to propagate informative semantic information\nfrom support to query images. ii) we develop a Relation-\nguided Affinity Transformer (RAT) to measure the reli-\nable cross correspondences by considering the auxiliary self-\naffinity of both support object and query images. iii) we pro-\npose Context and Affinity Transformer, dubbed as CATrans,\nin a hierarchical architecture to aggregate the context and\naffinity together, resulting in discriminative representations\nfrom support to query mask, enhancing robustness to intra-\nclass variations between support and query images. Our\nCATrans outperforms the state-of-the-art methods on two\nbenchmarks, Pascal-5i and COCO-20i.\n2 Related Work\n2.1 Semantic Segmentation\nSemantic segmentation is a fundamental problem in computer\nvision, which aims to classify each pixel of an image into\npredefined categories. Most existing semantic segmentation\nmethods are based on fully convolutional networks (FCNs)\n[Long et al., 2015], that replaces the fully connected layer\nwith fully convolutional ones for pixel-level prediction. Re-\ncent breakthroughs in semantic segmentation have mainly\ncome from multi-scale feature aggregation or attention mech-\nanisms. However, the traditional fully supervised segmenta-\ntion methods require large amounts of image-mask pairs for\ntraining, which are very expensive and time consuming. Ad-\nditionally, it cannot extend model‚Äôs generalizability to unseen\nclasses with only a few well-labeled samples.\n2.2 Few-shot Segmentation\nFew-shot semantic segmentation has attracted lots of research\nattentions after that [Shaban et al., 2017], which first dealt\nwith this issue by proposing to adapt the classifier for each\nclass, conditioned on the support set. Recent approaches for-\nmulate few-shot segmentation from the view of metric learn-\ning. [Dong and Xing, 2018 ] learned prototypes for differ-\nent classes and the segmentation results are made by cosine\nsimilarity between the features and the prototypes. [Wang et\nal., 2019 ] developed an efficient prototype learning frame-\nwork to build consistent prototypes. PFENet [Tian et al.,\n2020] made progress by further designing an effective fea-\nture pyramid module and leveraged a prior map to achieve a\nbetter segmentation performance. Recently, [Liu et al., 2020;\nYang et al., 2020 ] found that it is insufficient to repre-\nsent a category with a single support prototype. Therefore,\nthey used multiple prototypes to represent the support ob-\njects via the EM algorithm or K-means clustering. However,\nthese methods disregard the pixel-wise relationships of spa-\ntial structure in feature maps.\nRecent works [Zhang et al., 2019a; Zhang et al., 2021;\nCho et al., 2021; Lu et al., 2021] attempted to fully utilize\na correlation map to leverage the pixel-wise relationships be-\ntween support and query features. Specially, [Zhang et al.,\n2019a] used graph attention networks to propagate informa-\ntion from the support image to query images, and [Zhang\net al., 2021] utilized cycle-consistent transformer to aggre-\ngate the pixel-wise support features into the query. [Lu et\nal., 2021] proposed a classifier weight transformer where the\ntransformer is applied to adapt the classifier solely by freezing\nthe encoder and decoder. However, all these dense matching\nmethods focus on either context correspondences or affinity\nmaps only. This is no study about whether these two mea-\nsures are complementary, and can be integrate to achieve a\nbetter performance.\n2.3 Transformers in Vision\nRecently, transformers, first introduced in natural language\nprocessing [Vaswani et al., 2017], and are receiving increas-\ning interests in the computer community. Since the pioneer\nworks such as ViT [Dosovitskiy et al., 2021], it demonstrates\nthe pure transformer architecture can achieve the state-of-the-\nart for image recognition. On the other hand, DETR [Car-\nion et al., 2020] built up an end-to-end framework with a\ntransformer encoder-decoder on top of backbone networks\nfor object segmentation. And its deformable variants [Zhu et\nal., 2021] improved the performance and training efficiency.\nHowever, there are few studies that compute both context and\naffinity.\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n1659\nFigure 2: (a) The overall framework of our Context and Affinity Transformer (CATrans). The detailed architecture of our Relation-guided\nContext Transformer (RCT) and Relation-guided Affinity Transformer (RAT) are shown in (b) and (c), respectively.\n3 Preliminaries\n3.1 Problem Formulation\nThe goal of few-shot segmentation is to segment novel ob-\njects with very few annotated samples. Specifically, all\nclasses are divided into two disjointed class set Ctrain and\nCtest. To mitigate the overfitting caused by insufficient train-\ning data, we follow the common protocol called episodic\ntraining. Under K-shot setting, each episode is composed\nof a support set S = {(Is, Ms)}K, where Is, Ms are sup-\nport image and its corresponding mask, and a query sam-\nple Q = (Iq, Mq), where Iq, Mq are the query image and\nmask, respectively. In particular, given dataset Dtrain =\n{S, Q}Ntrain and Dtest = {S, Q}Ntest with category set\nCtrain and Ctest, respectively, where Ntrain and Ntest is the\nnumber of episodes for training and test sets. During train-\ning, our model takes a sampled episode from both support\nmasks Ms and query masks Mq, and only use support masks\nfor predicting the query segmentation map ÀÜMq during testing.\n3.2 Revisiting of Transformer\nTransformer block [Vaswani et al., 2017] consists of multi-\nhead attention (MHA) and multi-layer perception (MLP) with\ninputs of a set of Query (Q), Key (K) and Value (V) ele-\nments. In addition, LayerNorm (LN) and residual connection\nare available at the end of each block. Specially, an attention\nlayer is formulated as Architecture Overview:\nAtten(Q, K, V) =softmax\n\u0010Q ¬∑ KT\n‚àö\nd\n\u0011\n¬∑ V, (1)\nwhere [Q;K; V ] = [WqFq; WkFk; WvFv], in which Fq is the\ninput query sequence, Fk/Fv is the input key/value sequence,\nWq, Wk, Wv ‚àà Rc√óc are learnable weights, c is the channel\ndimension of the input sequences.\nThe multi-head attention layer is an enhancement of atten-\ntion layer, where h attention units are applied and then con-\ncatenated together. Concretely, this operation splits input se-\nquences along the channel dimension c into h groups:\nMHA(Q, K, V) = [head1, . . . , headh], (2)\nwhere headm = Atten(Qm, Km, Vm) and the inputs\n[Qm, Km, Vm] are the mth group from [Q, K, V] with di-\nmension c/h.\n4 Methodology\nBelow we present the overall architecture of CATrans fol-\nlowed by a description of its individual components.\n4.1 Architecture Overview\nThe overall architecture of our Context and Affinity Trans-\nformer (CATrans) is illustrated in Figure 2, which consists\nof Image Encoder, Mask Encoder, Relation-guided Context\nTransformer (RCT), Relation-guided Affinity Transformer\n(RAT) and the Decoder. Specifically, the input support-\nquery images {Is, Iq} are passed through the Image Encoder\nto extract multi-scale features {Fl\ns, Fl\nq ‚àà RHl√óWl√óCl }4\nl= 1,\nwhere l denotes the scale-level, and pyramid mask features\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n1660\n{Fl\nm ‚àà RHl\nm√óWl\nm√óCl\nm }4\nl= 1are extracted via the Mask En-\ncoder with the input of support binary mask Ms. The re-\nsulting triple {Fl\ns, Fl\nq, Fl\nm}4\nl= 3is passed into the RCT and\nRAT, respectively. In practice, the pixel-wise context Cl ‚àà\nRHl√óWl√óCl\nm provided by the RCT and dense affinity map\nA ‚ààRHlWl√óHlWl generated by RAT are concatenated for\ninformation aggregation. In the decoder, the fused represen-\ntations associate with the high-resolution features of query\nimage {Fl\ns, Fl\nq}2\nl= 1for predicting the query mask ÀÜMq.\n4.2 Relation-guided Context Transformer\nInspirations: Current CNN-based prototypical learning\napproaches, ie. [Wang et al., 2019], condense the support\nfeatures into a single or few context-wise prototypes. How-\never, prototypical features inevitably drop spatial informa-\ntion and fail to discriminate incorrect matches due to lim-\nited receptive fields of CNNs. So recent research applied the\nTransformer-based architecture, such as [Zhang et al., 2021],\nto establish long-range and pixel-wise context relationships\nbetween paired support-query samples, outperforming previ-\nous CNN-based methods by a large margin. We conjecture\nthat the representative features within the individual support\nand query image would help to aggregate more precise cross-\nrelationships, being robust to large intra-class differences of\npaired support-query samples.\nTo this end, the RCT is designed and shown in Figure 2\n(b). We flatten the given triple features (Fl\ns, Fl\nq, Fl\nm) into 1D\ntokens as inputs for the following items.\nSelf-context. Self-context is separately employed for sup-\nport objects and query features by aggregating its relevant\ncontext information, leading to more informative support and\nquery features to be connected via the cross-context. The re-\nsulting contexts Cl\nss and Cl\nqq are designed as:\nCl\nss = MLP(LN(MHA(Fl\ns, Fl\ns, Fl\nm))), (3)\nCl\nqq = MLP(LN(MHA(Fl\nq, Fl\nq, Fl\nq))), (4)\nwhere MHA (¬∑), LN(¬∑) and MLP(¬∑) are operations introduced\nin the section 3.\nCross-context. Considering that Cl\nss, guided by mask fea-\ntures Fl\nm, mainly focuses on the foreground, but background\nsupport pixels are beneficial for building the semantic rela-\ntionships, we collaborate the enhanced self-context features\nof support and query withFl\ns to establish the pixel-wise cross-\ncontext. The process of cross-context is formed as:\nCl\nsq = MLP(LN(MHA(Cl\nqq, Fl\ns, Cl\nss))), (5)\nwhere Cl\nsq ‚àà RHlWl√óCl\nm is spatially rearranged to the shape\nof Hl √ó Wl √ó Cl\nm.\n4.3 Relation-guided Affinity Transformer\nA huge feature distinction between the support and query im-\nages brings barriers for context knowledge transfer, which\ncripples the segmentation performance. We explore several\nattention-aware affinity maps that measure pixel-wise corre-\nspondences to facilitate the FSS task, as shown in Figure 2\n(c). Overall, this module provides affinity guidance stemming\nfrom attention-aware features instead of semantics.\nWhy needs self-affinity? The training samples belonging\nto the same class always have features with large variations\nin appearance as these objects are taken in unconstrained set-\ntings. Take aeroplane as an example, all aeroplanes are made\nby metal and have wings. These features can be seen as intrin-\nsic features. As the differences of shooting angle and lighting\nconditions, the shape and color of aeroplanes can be different.\nIn few-shot segmentation, we need to enable each pixel-wise\nfeature belonging to itself to match pixel-wise feature at the\nsame position, making it robust to large variations in object\nappearance between the support and query images.\nWhat is Self-affinity? For a support image, we utilize high-\ndimension support mask features Fl\nm concatenated with sup-\nport image features Fl\ns for estimating its affinity map by\nscaled-dot product followed by softmax function. The pro-\ncess for l-th features can be defined as:\nAl\nss = softmax\n\u0010(fl\nm||fl\ns)Wq ¬∑ ((fl\nm||fl\ns)Wk)T\np\nCl + Clm\n\u0011\n, (6)\nwhere {¬∑||¬∑} denotes the concatenation operation, softmax (¬∑)\nis a row-wise softmax function for attention normalization\nand two individual fc layers are applied to learn discrimi-\nnative features by learnable parameters. Analogy to support\nfeature, we formulate the self-affinity for query as:\nAl\nqq = softmax\n\u0010fl\nqWq ¬∑ (fl\nqWk)T\n‚àöCl\n\u0011\n. (7)\nCross-affinity. The cross-affinity between support and\nquery features Fl\nca is computed by:\nAl\nsq = softmax\n\u0010fl\nqWq ¬∑ (fl\ns Wk)T\n‚àöCl\n\u0011\n. (8)\nHowever, solely relying on the cross-affinity between features\noften suffers from the challenges caused by large intra-class\nvariations. We then embed the self-affinity of query Al\nqq into\ncross-affinity. The final cross-affinity is formulated as:\nMLP(LN(MHA(Al\nsq, Al\nss, Al\nsq) +Al\nqq)). (9)\nSupport set\n RAT\n RCT\n CATrans\n GT\nFigure 3: Visualize the results predicted by the CATrans and its vari-\nants on PASCAL-5i, 1-shot setting. GT means Ground Truth of the\nquery image.\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n1661\nMethod V\nenue Backbone\n1-shot 5-shot\n50 51 52 53 Mean 50 51 52 53 Mean\nCANet [Zhang et al.\n, 2019b] CVPR19\nResNet-50\n52.5 65.9\n51.3 51.9 55.4 55.5 67.8\n51.9 53.2 57.1\nPGNet [Zhang et al., 2019a] ICCV19 56.0 66.9\n50.6 50.4 56.0 57.7 68.7\n52.9 54.6 58.5\nRPMMs [Yang et al., 2020] ECCV20 55.2 66.9\n52.6 50.7 56.3 56.3 67.3\n54.5 51.0 57.3\nPPNet [Liu et al., 2020] ECCV20 47.8 58.8\n53.8 45.6 51.5 58.4 67.8\n64.9 56.7 62.0\nPFENet [Tian et al., 2020] TPAMI20 61.7 69.5\n55.4 56.3 60.8 63.1 70.7\n55.8 57.9 61.9\nCWT [Lu et al., 2021] ICCV21 56.3 62.0\n59.9 47.2 56.4 61.3 68.5\n68.5 56.6 63.7\nCyCTR [Zhang et al., 2021] NeurIPS21 67.8 72.8 58.0\n58.0 64.2 71.1 73.2\n60.5 57.5 65.6\nDGPNet [Johnander et al., 2021] arXiv21 63.5 71.1\n58.2 61.2 63.5 72.4 76.9\n73.2 71.7 73.5\nCAT\nrans Ours 67.6 73.2 61.3 63.2 66.3 75.1 78.5 75.1 72.5 75.3\nFWB [Nguyen and\nTodorovic, 2019] ICCV19\nResNet-101\n51.3 64.5\n56.7 52.2 56.2 54.9 67.4\n62.2 55.3 59.9\nDAN [Wang et al., 2020] ECCV20 54.7 68.6\n57.8 51.6 58.2 57.9 69.0\n60.1 54.9 60.5\nPFENet [Tian et al., 2020] TPAMI20 60.5 69.4\n54.4 55.9 60.1 62.8 70.4\n54.9 57.6 61.4\nCWT [Lu et al., 2021] ICCV21 56.9 65.2\n61.2 48.8 58.0 62.6 70.2\n68.8 57.2 64.7\nCyCTR [Zhang et al., 2021] NeurIPS21 69.3 72.7 56.5\n58.6 64.3 73.5 74.0\n58.6 60.2 66.6\nDGPNet [Johnander et al., 2021] arXiv21 63.9 71.0\n63.0 61.4 64.8 74.1 77.4\n76.7 73.4 75.4\nCAT\nrans Ours 67.8 73.2 64.7 63.2 67.2 75.2 78.4 77.7 74.8 76.5\nCAT\nrans Ours Swin-T 68.0 73.5 64.9 63.7 67.6 75.9 79.1 78.3 75.6 77.3\nTable 1: Comparison with the state-of-the-art in 1-shot and 5-shot segmentation on PASCAL-5 i dataset using the mIoU (%) as evaluation\nmetric. Best results in bold.\nRAT RCT RCT ‚ó¶ PASCAL-5i COCO-20i\n1-shot 5-shot 1-shot 5-shot\n58.3 62.7 33.4 42.6\n‚úì 65.1 70.6 41.3 54.9\n‚úì 66.0 72.4 43.9 56.4\n‚úì 65.3 71.3 42.1 55.3\n‚úì ‚úì 66.3 75.3 46.6 58.2\n(a)\n#l PASCAL-5i COCO-20i\n1 2 3 4 1-shot 5-shot 1-shot 5-shot\n‚úì 62.4 72.5 42.4 55.8‚úì 64.8 73.3 44.8 57.9‚úì 65.1 74.2 45.3 58.1\n‚úì ‚úì 64.9 74.1 44.8 56.5‚úì 61.2 71.7 40.6 53.3‚úì ‚úì 66.3 75.3 46.6 58.2\n(b)\n#h PASCAL-5i COCO-20i\n1-shot 5-shot 1-shot 5-shot\n1 66.3 75.3 46.6 58.2\n2 65.0 73.9 45.1 56.8\n3 65.1 73.6 45.4 56.5\n(c)\nTable 2: Ablation studies on the effectiveness of RCT and RAT in (a), multi-level context and affinity utilization in (b), and the head number\nof attention in (c). Best results are shown in bold.\n5 Experiments\nIn this section, we conduct extensive experiments for our\nCATrans on two widely-used few-shot segmentation bench-\nmarks, PASCAL-5i and COCO-20i, to demonstrate the ef-\nfectiveness of our method.\n5.1 Datasets\nPASCAL-5i [Shaban et al., 2017] is composed of PASCAL\nVOC 2012 with additional SBD [Hariharan et al., 2011] an-\nnotations, which contains 20 categories split into 4 folds (15/5\ncategories as base/novel classes).\nCOCO-20i [Lin et al., 2014 ] is created from MS COCO\nwhere the 80 object categories are divided into four splits\n(60/20 categories as base/novel classes).\n5.2 Implementation Details\nWe conduct all experiments on 1 NVIDIA V100 GPU. The\nmodels are trained for 20k and 40k iterations on PASCAL-5i\nand COCO-20i, respectively, with AdamW as the optimizer.\nThe initial learning rate is set to 5e-5 and decays at 10k it-\neration with a factor of 0.1. During training, we first resize\nthe input images to 384 √ó384 and 512 √ó512 for PASCAL-5i\nand COCO-20i, respectively, and then perform the horizon-\ntal flip operation randomly. We simply use cross-entropy loss\nwith a weight of 1 and 4 for background and foreground pix-\nels, respectively. The BN layers of image encoder are frozen.\nFor a fair comparision, we employ the widely-used ResNet-\n50, ResNet-101 and Swin-Transformer as the image encoder.\nThe mask encoder includes four light-weight layers, each of\nwhich is composed of 3 √ó 3 convolution, BatchNorm and\nReLU.\nDuring evaluation, the results are averaged on the ran-\ndomly sampled 5k and 20k episodes for each fold and 5 runs\nwith different seeds. we report the mean-IoU (mIoU) un-\nder both 1-shot (given a single support example) and 5-shot\n(given five support examples).\n5.3 Comparisons with the State-of-the-art\nResults on PASCAL-5i. As shown in Table 1, our CATrans\noutperforms the previous best DGPNet by +2.8/+1.8% and\n+2.4/+1.1% for 1-shot/5-shot mIoU using ResNet-50 and\nResNet-101 as the backbone, repectively. Rendering the\nSwin-T as the image encoder, our CATrans further achieves\n67.6% and 77.3% mIoU for 1-shot and 5-shot segmentation.\nResults on COCO-20i. Table 3 reports the comparisons on\nthe more challenging COCO-20 i dataset. Compared with\nthe previous best DGPNet, our CATrans surpasses it by\n+1.6/+2.0% 1-shot/5-shot mIoU using the ResNet-50. When\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n1662\nMethod Backbone\n1-shot 5-shot\n200 201 202 203 Mean 200 201 202 203 Mean\nPANet [W\nang et al., 2019] ICCV19\nResNet-50\n31.5 22.6\n21.5 16.2 23.0 45.9 29.2\n30.6 29.6 33.8\nRPMMs [Yang et al., 2020] ECCV20 29.5 36.8\n29.0 27.0 30.6 33.8 42.0\n33.0 33.3 35.5\nPPNet [Liu et al., 2020] ECCV20 34.5 25.4\n24.3 18.6 25.7 48.3 30.9\n35.7 30.2 36.2\nCWT [Lu et al., 2021] ICCV21 32.2 36.0\n31.6 31.6 32.9 40.1 43.8\n39.0 42.4 41.3\nCyCTR [Zhang et al., 2021] NeurIPS21 38.9 43.0\n39.6 39.8 40.3 41.1 48.9\n45.2 47.0 45.6\nDGPNet [Johnander et al., 2021] arXiv21 43.6 47.8\n44.5 44.2 45.0 54.7 59.1\n56.8 54.4 56.2\nCAT\nrans Ours 46.5 49.3 45.6 45.1 46.6 56.3 60.7 59.2 56.3 58.2\nFWB [Nguyen and\nTodorovic, 2019] ICCV19\nResNet-101\n19.9 18.0\n21.0 28.9 21.2 19.1 21.5\n23.9 30.1 23.7\nPFENet [Tian et al., 2020] TPAMI20 34.3 33.0\n32.3 30.1 32.4 38.5 38.6\n38.2 34.3 37.4\nCWT [Lu et al., 2021] ICCV21 30.3 36.6\n30.5 32.2 32.4 38.5 46.7\n39.4 43.2 42.0\nDGPNet [Johnander et al., 2021] arXiv21 45.1 49.5\n46.6 45.6 46.7 56.8 60.4\n58.4 55.9 57.9\nCAT\nrans Ours 47.2 51.7 48.6 47.8 48.8 58.5 63.4 59.6 57.2 59.7\nCAT\nrans Ours Swin-T 47.9 52.3 49.2 48.0 49.4 59.3 64.1 59.6 57.3 60.1\nTable 3: Comparison with the state-of-the-art in 1-shot and 5-shot segmentation on COCO-20 i dataset using the mIoU (%) as evaluation\nmetric. Best results in bold.\nusing ResNet-101, our CATrans is +2.1% and +1.8% higher\nthan DGPNet for 1-shot and 5-shot protocals, repectively.\nEquipped with the Swin-T, our CATrans achieves 49.4/60.1%\n1-shot/5-shot mIoU.\n5.4 Ablation Study\nHere, we conduct extensive ablation studies with ResNet-50\non PASCAL-5i and COCO-20i to analyze the effect of the\nkey components in our CATrans.\nEffectiveness of RAT and RCT. We ablate our CATrans to\nobserve the effectiveness of RAT and RCT modules, as shown\nin Table 2a. We define the baseline, without RAT and RCT\nmodules, that is simply concatenating the support and query\nfeatures along the channel mode. The variant with either RAT\nor RCT boosts baseline by up to +6.8/+7.7 on 1-shot proto-\ncol. Moreover, it can be seen that the support self-context\nbranch of RCT provides additional ‚àº1% mIoU on 1-shot set-\nting (RCT vs. RCT‚ó¶), endowing RCT with a powerful context\naggregator. Considering that the cross-affinity between sup-\nport and query images can serve as the additional guidance\nfor FSS task, when the large feature distinctions impede con-\ntext knowledge transfer, we verify how much benefit comes\nfrom the RAT. Table 2a shows the model equips with both\nRCT and RAT has ‚àº5% increase for FSS results.\nMulti-scale Representations. We first verify the influence\nof fusing the high-resolution features of query image (l=1/2)\non the top panel of Table 2b. Then on the bottom panel of\nTable 2b, it shows a comparison experiment between single-\nscale and multi-scale representations of CATrans (l = 3/4).\nThe more levels of information used, the better the perfor-\nmance. The performance reaches the highest when these\ntwo levels are both leveraged, especially in the 1-shot setting\nwhere multi-scale guidance can extract more guiding infor-\nmation for query segmentation with 1.4%/1.8% mIoU gain\non PASCAL-5i/COCO-20i.\nEffect of Model Capacity. We stack more numbers of\nheads of attention layer to increase capacity of our CATrans\nand validate its effectiveness. It shows that our model per-\nformance is stable across different choices, particularly for\n#h ‚â•2.\nMemory and Run-time. CATrans, with trivial compu-\ntational overhead, performs the best over closely re-\nlated transformer-based methods. CyCTR and CATs\nstack two successive transformer blocks whereas CATrans\nconsists of RCT and RAT, each with one transformer\nblock. The memory and run-time comparisons are below:\nMemory (GB) Run-time (ms)\nCyCTR CA\nTrans CATs CyCTR CA\nTrans CATs\n1.78 1.85\n1.90 31.7 33.2\n34.5\nQualitative results. To show the performance of CATrans\nintuitively, we visualize some final prediction masks pro-\nduced by our method and its variants in Figure 3. The first\ncolumn is support image and its ground truth, and the next\nthree columns are query mask produced by RAT, RCT and\nCATrans, respectively. The last column is ground truth of\nquery image. On the top row of Figure 3, as the large intra-\nclass appearance variations between support and query im-\nages hidden context knowledge transferring, RCT fails to pre-\ncisely segment one of its aerofoils, where RCT performs bet-\nter by use of the self- and cross- affinity for query segmenta-\ntion. On the bottom of Figure 3, the RAT mistakenly regards\nsome plates as part of the target cup because of the similarity\nin the shape and color, measured by the attention-aware affin-\nity. But RAT reduces the area of wrong segmentation signif-\nicantly by successfully aggregating the context information.\nOverall, adopting both RAT and RCT performs the best.\n6 Conclusion\nWe have proposed the novel Context and Affinity Trans-\nformer (CATrans) with RCT and RAT in a hierarchical archi-\ntecture to deal with the large intra-class appearance and geo-\nmetric variations in few-shot segmentation, so that CATrans\ncan effectively incorporate both context similarity and affinity\nmap for query segmentation. In addition, we consider pixel-\nwise correspondences for individual support and query fea-\ntures to disambiguate noisy correlations.\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n1663\nReferences\n[Carion et al., 2020] Nicolas Carion, Francisco Massa,\nGabriel Synnaeve, Nicolas Usunier, Alexander Kirillov,\nand Sergey Zagoruyko. End-to-end object detection with\ntransformers. In ECCV 2020, volume 12346 of Lecture\nNotes in Computer Science, pages 213‚Äì229. Springer,\n2020.\n[Cho et al., 2021] Seokju Cho, Sunghwan Hong, Sangryul\nJeon, Yunsung Lee, Kwanghoon Sohn, and Seungryong\nKim. Semantic correspondence with transformers. CoRR,\nabs/2106.02520, 2021.\n[Dong and Xing, 2018] Nanqing Dong and Eric P. Xing.\nFew-shot semantic segmentation with prototype learning.\nIn BMVC 2018, page 79. BMV A Press, 2018.\n[Dosovitskiy et al., 2021] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Min-\nderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and\nNeil Houlsby. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In ICLR 2021.\nOpenReview.net, 2021.\n[Hariharan et al., 2011] Bharath Hariharan, Pablo Arbelaez,\nLubomir D. Bourdev, Subhransu Maji, and Jitendra Malik.\nSemantic contours from inverse detectors. In ICCV 2011,\npages 991‚Äì998. IEEE Computer Society, 2011.\n[Johnander et al., 2021] Joakim Johnander, Johan Edstedt,\nMichael Felsberg, Fahad Shahbaz Khan, and Martin\nDanelljan. Dense gaussian processes for few-shot segmen-\ntation. CoRR, abs/2110.03674, 2021.\n[Lin et al., 2014] Tsung-Yi Lin, Michael Maire, Serge J. Be-\nlongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll¬¥ar, and C. Lawrence Zitnick. Microsoft COCO: com-\nmon objects in context. In ECCV 2014, volume 8693\nof Lecture Notes in Computer Science, pages 740‚Äì755.\nSpringer, 2014.\n[Lin et al., 2017] Guosheng Lin, Anton Milan, Chunhua\nShen, and Ian D. Reid. Refinenet: Multi-path refinement\nnetworks for high-resolution semantic segmentation. In\nCVPR 2017, pages 5168‚Äì5177. IEEE Computer Society,\n2017.\n[Liu et al., 2020] Yongfei Liu, Xiangyi Zhang, Songyang\nZhang, and Xuming He. Part-aware prototype network for\nfew-shot semantic segmentation. In ECCV 2020, volume\n12354 of Lecture Notes in Computer Science, pages 142‚Äì\n158. Springer, 2020.\n[Long et al., 2015] Jonathan Long, Evan Shelhamer, and\nTrevor Darrell. Fully convolutional networks for seman-\ntic segmentation. In CVPR 2015, pages 3431‚Äì3440. IEEE\nComputer Society, 2015.\n[Lu et al., 2021] Zhihe Lu, Sen He, Xiatian Zhu, Li Zhang,\nYi-Zhe Song, and Tao Xiang. Simpler is better: Few-shot\nsemantic segmentation with classifier weight transformer.\nCoRR, abs/2108.03032, 2021.\n[Nguyen and Todorovic, 2019] Khoi Nguyen and Sinisa\nTodorovic. Feature weighting and boosting for few-shot\nsegmentation. In ICCV 2019, pages 622‚Äì631. IEEE, 2019.\n[Shaban et al., 2017] Amirreza Shaban, Shray Bansal, Zhen\nLiu, Irfan Essa, and Byron Boots. One-shot learning for\nsemantic segmentation. In BMVC 2017. BMV A Press,\n2017.\n[Tian et al., 2020] Zhuotao Tian, Hengshuang Zhao,\nMichelle Shu, Zhicheng Yang, Ruiyu Li, and Jiaya Jia.\nPrior guided feature enrichment network for few-shot\nsegmentation. CoRR, abs/2008.01449, 2020.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In Advances in Neural Information Processing Sys-\ntems 30: December 4-9, 2017, pages 5998‚Äì6008, 2017.\n[Wang et al., 2019] Kaixin Wang, Jun Hao Liew, Yingtian\nZou, Daquan Zhou, and Jiashi Feng. Panet: Few-shot im-\nage semantic segmentation with prototype alignment. In\nICCV 2019, pages 9196‚Äì9205. IEEE, 2019.\n[Wang et al., 2020] Haochen Wang, Xudong Zhang, Yutao\nHu, Yandan Yang, Xianbin Cao, and Xiantong Zhen. Few-\nshot semantic segmentation with democratic attention net-\nworks. In ECCV 2020, volume 12358 ofLecture Notes in\nComputer Science, pages 730‚Äì746. Springer, 2020.\n[Yang et al., 2020] Boyu Yang, Chang Liu, Bohao Li, Jian-\nbin Jiao, and Qixiang Ye. Prototype mixture models for\nfew-shot semantic segmentation. In ECCV 2020, volume\n12353 of Lecture Notes in Computer Science, pages 763‚Äì\n778. Springer, 2020.\n[Zhang et al., 2019a] Chi Zhang, Guosheng Lin, Fayao Liu,\nJiushuang Guo, Qingyao Wu, and Rui Yao. Pyramid graph\nnetworks with connection attentions for region-based one-\nshot semantic segmentation. In ICCV 2019, pages 9586‚Äì\n9594. IEEE, 2019.\n[Zhang et al., 2019b] Chi Zhang, Guosheng Lin, Fayao Liu,\nRui Yao, and Chunhua Shen. Canet: Class-agnostic seg-\nmentation networks with iterative refinement and atten-\ntive few-shot learning. In CVPR 2019, pages 5217‚Äì5226.\nComputer Vision Foundation / IEEE, 2019.\n[Zhang et al., 2020] Xiaolin Zhang, Yunchao Wei, Yi Yang,\nand Thomas S. Huang. Sg-one: Similarity guidance net-\nwork for one-shot semantic segmentation. IEEE Trans.\nCybern., 50(9):3855‚Äì3865, 2020.\n[Zhang et al., 2021] Gengwei Zhang, Guoliang Kang, Yun-\nchao Wei, and Yi Yang. Few-shot segmentation via cycle-\nconsistent transformer. CoRR, abs/2106.02320, 2021.\n[Zhu et al., 2021] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li,\nXiaogang Wang, and Jifeng Dai. Deformable DETR: de-\nformable transformers for end-to-end object detection. In\nICLR 2021. OpenReview.net, 2021.\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n1664",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7042993903160095
    },
    {
      "name": "Segmentation",
      "score": 0.6131924390792847
    },
    {
      "name": "Transformer",
      "score": 0.5878878831863403
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4720626175403595
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3925669491291046
    },
    {
      "name": "Engineering",
      "score": 0.09441253542900085
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I118347636",
      "name": "Australian National University",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I98301712",
      "name": "Baidu (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210129579",
      "name": "National Engineering Laboratory of Deep Learning Technology and Application",
      "country": "CN"
    }
  ],
  "cited_by": 19
}