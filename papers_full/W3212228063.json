{
    "title": "Restormer: Efficient Transformer for High-Resolution Image Restoration",
    "url": "https://openalex.org/W3212228063",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4225782226",
            "name": "Zamir, Syed Waqas",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2746663768",
            "name": "Arora, Aditya",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2483644475",
            "name": "Khan, Salman",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222938126",
            "name": "Hayat, Munawar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4202207810",
            "name": "khan, Fahad Shahbaz",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4200861175",
            "name": "Yang, Ming-Hsuan",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2740982616",
        "https://openalex.org/W2613155248",
        "https://openalex.org/W3166368936",
        "https://openalex.org/W2963494934",
        "https://openalex.org/W2988641380",
        "https://openalex.org/W2969717429",
        "https://openalex.org/W2056370875",
        "https://openalex.org/W2767829160",
        "https://openalex.org/W2963762505",
        "https://openalex.org/W2980047233",
        "https://openalex.org/W3106758205",
        "https://openalex.org/W3099686304",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W3202362072",
        "https://openalex.org/W2970318705",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W3202040256",
        "https://openalex.org/W3040726448",
        "https://openalex.org/W3167568784",
        "https://openalex.org/W3085824656",
        "https://openalex.org/W2167191464",
        "https://openalex.org/W3109494165",
        "https://openalex.org/W2963085671",
        "https://openalex.org/W3204896399",
        "https://openalex.org/W3000775737",
        "https://openalex.org/W3202637858",
        "https://openalex.org/W2799192307",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W3170697543",
        "https://openalex.org/W2866634454",
        "https://openalex.org/W2476548250",
        "https://openalex.org/W2995057678",
        "https://openalex.org/W3034504121",
        "https://openalex.org/W2508457857",
        "https://openalex.org/W3047011367",
        "https://openalex.org/W1990592195",
        "https://openalex.org/W2963312584",
        "https://openalex.org/W2964030969",
        "https://openalex.org/W2884068670",
        "https://openalex.org/W2963263347",
        "https://openalex.org/W3035383808",
        "https://openalex.org/W2912435603",
        "https://openalex.org/W2961218591",
        "https://openalex.org/W2963970792",
        "https://openalex.org/W2969254886",
        "https://openalex.org/W2963878020",
        "https://openalex.org/W3131149871",
        "https://openalex.org/W2962767526",
        "https://openalex.org/W3092462694",
        "https://openalex.org/W3203606893",
        "https://openalex.org/W2983315964",
        "https://openalex.org/W2560533888",
        "https://openalex.org/W2971719842",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2912268344",
        "https://openalex.org/W3174300208",
        "https://openalex.org/W2963017889",
        "https://openalex.org/W3035022492",
        "https://openalex.org/W3104725225",
        "https://openalex.org/W2914992179",
        "https://openalex.org/W3035326127",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W2962935103",
        "https://openalex.org/W3207918547",
        "https://openalex.org/W3034789174",
        "https://openalex.org/W2967584026",
        "https://openalex.org/W3009428327",
        "https://openalex.org/W2167307343",
        "https://openalex.org/W2121927366",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2150081556",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W3211490618",
        "https://openalex.org/W2962737939",
        "https://openalex.org/W3119866685",
        "https://openalex.org/W3174970555",
        "https://openalex.org/W2965217508",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3083579885",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2081418206",
        "https://openalex.org/W2559264300",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W3171125843",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W1912194039",
        "https://openalex.org/W2963372104",
        "https://openalex.org/W1916731006",
        "https://openalex.org/W3107405705",
        "https://openalex.org/W2985577924",
        "https://openalex.org/W2509784253",
        "https://openalex.org/W2804078698",
        "https://openalex.org/W2967997213",
        "https://openalex.org/W2964212750",
        "https://openalex.org/W3175634226",
        "https://openalex.org/W2798735168",
        "https://openalex.org/W2963725279",
        "https://openalex.org/W3035484352"
    ],
    "abstract": "Since convolutional neural networks (CNNs) perform well at learning generalizable image priors from large-scale data, these models have been extensively applied to image restoration and related tasks. Recently, another class of neural architectures, Transformers, have shown significant performance gains on natural language and high-level vision tasks. While the Transformer model mitigates the shortcomings of CNNs (i.e., limited receptive field and inadaptability to input content), its computational complexity grows quadratically with the spatial resolution, therefore making it infeasible to apply to most image restoration tasks involving high-resolution images. In this work, we propose an efficient Transformer model by making several key designs in the building blocks (multi-head attention and feed-forward network) such that it can capture long-range pixel interactions, while still remaining applicable to large images. Our model, named Restoration Transformer (Restormer), achieves state-of-the-art results on several image restoration tasks, including image deraining, single-image motion deblurring, defocus deblurring (single-image and dual-pixel data), and image denoising (Gaussian grayscale/color denoising, and real image denoising). The source code and pre-trained models are available at https://github.com/swz30/Restormer.",
    "full_text": "Restormer: Efﬁcient Transformer for High-Resolution Image Restoration\nSyed Waqas Zamir1 Aditya Arora1 Salman Khan2 Munawar Hayat3\nFahad Shahbaz Khan2 Ming-Hsuan Yang4,5,6\n1Inception Institute of AI 2Mohamed bin Zayed University of AI 3Monash University\n4University of California, Merced 5Yonsei University 6Google Research\nAbstract\nSince convolutional neural networks (CNNs) perform\nwell at learning generalizable image priors from large-\nscale data, these models have been extensively applied to\nimage restoration and related tasks. Recently, another class\nof neural architectures, Transformers, have shown signiﬁ-\ncant performance gains on natural language and high-level\nvision tasks. While the Transformer model mitigates the\nshortcomings of CNNs ( i.e., limited receptive ﬁeld and in-\nadaptability to input content), its computational complex-\nity grows quadratically with the spatial resolution, there-\nfore making it infeasible to apply to most image restora-\ntion tasks involving high-resolution images. In this work,\nwe propose an efﬁcient Transformer model by making sev-\neral key designs in the building blocks (multi-head atten-\ntion and feed-forward network) such that it can capture\nlong-range pixel interactions, while still remaining appli-\ncable to large images. Our model, named Restoration\nTransformer (Restormer), achieves state-of-the-art results\non several image restoration tasks, including image derain-\ning, single-image motion deblurring, defocus deblurring\n(single-image and dual-pixel data), and image denoising\n(Gaussian grayscale/color denoising, and real image de-\nnoising). The source code and pre-trained models are avail-\nable at https://github.com/swz30/Restormer.\n1. Introduction\nImage restoration is the task of reconstructing a high-\nquality image by removing degradations ( e.g., noise, blur,\nrain drops) from a degraded input. Due to the ill-posed na-\nture, it is a highly challenging problem that usually requires\nstrong image priors for effective restoration. Since con-\nvolutional neural networks (CNNs) perform well at learn-\ning generalizable priors from large-scale data, they have\nemerged as a preferable choice compared to conventional\nrestoration approaches.\nThe basic operation in CNNs is the ‘ convolution’ that\nprovides local connectivity and translation equivariance.\nWhile these properties bring efﬁciency and generalization\nto CNNs, they also cause two main issues. (a) The convo-\n140 240 340 440 540 640 740\nNumber of Flops (G)\n31.0\n31.3\n31.6\n31.9\n32.2\n32.5\n32.8\n33.1PSNR (dB)\nRestormer\n(Ours)\nIPT\n(CVPR21)\nMTRNN\n(ECCV20)\nMPRNet\n(CVPR21)\nMimoUNet+\n(ICCV21)\nDMPHN\n(CVPR19)\nDBGAN\n(CVPR20)\n[94]\n[58]\n[14]\n[13]\n[93]\n[100]\n30 130 230 330 430 530\nNumber of Flops (G)\n28.5\n29.5\n30.5\n31.5\n32.5\n33.5PSNR (dB)\nRestormer\n(Ours)\nMSPFN\n(CVPR20)\nPreNet\n(CVPR19)\nRESCAN\n(ECCV18)\nMPRNet\n(CVPR21)\n[93]\n[64]\n[43]\n[32]\n(a) Deblurring (Tab. 2) (b) Deraining (Tab. 1)\n140 240 340 440 540 640 740\nNumber of Flops (G)\n27.5\n27.6\n27.7\n27.8\n27.9\n28.0\n28.1\n28.2\n28.3\n28.4PSNR (dB)\nRestormer\n(Ours)\nDAGL\n(ICCV21)\nSwinIR\n(ICCVW21)\nDeamNet\n(CVPR21)\nDRUNet\n(TPAMI21)\nRNAN\n(ICLR19)\n[63]\n[99]\n[55] [44]\n[106]\n40 140 240 340 440 540 640 740\nNumber of Flops (G)\n38.8\n39.0\n39.2\n39.4\n39.6\n39.8\n40.0\n40.2PSNR (dB)\nRestormer\n(Ours)\nUformer\n(ArXiv21)\nMPRNet\n(CVPR21)\nDAGL\n(ICCV21)\nMIRNet\n(ECCV20)\nDeamNet\n(CVPR21)\n[63]\n[55]\n[80]\n[93]\n[92]\n(c) Gaussian Denoising (Tab. 4) (d) Real Denoising (Tab. 6)\nFigure 1. Our Restormer achieves the state-of-the-art performance\non image restoration tasks while being computationally efﬁcient.\nlution operator has a limited receptive ﬁeld, thus prevent-\ning it from modeling long-range pixel dependencies. (b)\nThe convolution ﬁlters have static weights at inference, and\nthereby cannot ﬂexibly adapt to the input content. To deal\nwith the above-mentioned shortcomings, a more powerful\nand dynamic alternative is the self-attention (SA) mecha-\nnism [17,77,79,95] that calculates response at a given pixel\nby a weighted sum of all other positions.\nSelf-attention is a core component in Transformer mod-\nels [34, 77] but with a unique implementation, i.e., multi-\nhead SA that is optimized for parallelization and effective\nrepresentation learning. Transformers have shown state-of-\nthe-art performance on natural language tasks [10,19,49,62]\nand on high-level vision problems [11,17,76,78]. Although\nSA is highly effective in capturing long-range pixel inter-\nactions, its complexity grows quadratically with the spatial\nresolution, therefore making it infeasible to apply to high-\nresolution images (a frequent case in image restoration).\nRecently, few efforts have been made to tailor Transformers\nfor image restoration tasks [13, 44, 80]. To reduce the com-\nputational loads, these methods either apply SA on small\nspatial windows of size 8×8 around each pixel [44, 80], or\n1\narXiv:2111.09881v2  [cs.CV]  11 Mar 2022\ndivide the input image into non-overlapping patches of size\n48×48 and compute SA on each patch independently [13].\nHowever, restricting the spatial extent of SA is contradic-\ntory to the goal of capturing the true long-range pixel rela-\ntionships, especially on high-resolution images.\nIn this paper, we propose an efﬁcient Transformer for\nimage restoration that is capable of modeling global con-\nnectivity and is still applicable to large images. Speciﬁ-\ncally, we introduce a multi-Dconv head ‘transposed’ atten-\ntion (MDTA) block (Sec. 3.1) in place of vanilla multi-head\nSA [77], that has linear complexity. It applies SA across\nfeature dimension rather than the spatial dimension, i.e.,\ninstead of explicitly modeling pairwise pixel interactions,\nMDTA computes cross-covariance across feature channels\nto obtain attention map from the ( key and query projected)\ninput features. An important feature of our MDTA block is\nthe local context mixing before feature covariance compu-\ntation. This is achieved via pixel-wise aggregation of cross-\nchannel context using 1×1 convolution and channel-wise\naggregation of local context using efﬁcient depth-wise con-\nvolutions. This strategy provides two key advantages. First,\nit emphasizes on the spatially local context and brings in the\ncomplimentary strength of convolution operation within our\npipeline. Second, it ensures that the contextualized global\nrelationships between pixels are implicitly modeled while\ncomputing covariance-based attention maps.\nA feed-forward network (FN) is the other building block\nof the Transformer model [77], which consists of two fully\nconnected layers with a non-linearity in between. In this\nwork, we reformulate the ﬁrst linear transformation layer\nof the regular FN [77] with a gating mechanism [16] to im-\nprove the information ﬂow through the network. This gating\nlayer is designed as the element-wise product of two linear\nprojection layers, one of which is activated with the GELU\nnon-linearity [27]. Our gated-Dconv FN (GDFN) (Sec. 3.2)\nis also based on local content mixing similar to the MDTA\nmodule to equally emphasize on the spatial context. The\ngating mechanism in GDFN controls which complementary\nfeatures should ﬂow forward and allows subsequent layers\nin the network hierarchy to speciﬁcally focus on more re-\nﬁned image attributes, thus leading to high-quality outputs.\nApart from the above architectural novelties, we show\nthe effectiveness of our progressive learning strategy for\nRestormer (Sec. 3.3). In this process, the network is trained\non small patches and large batches in early epochs, and on\ngradually large image patches and small batches in later\nepochs. This training strategy helps Restormer to learn con-\ntext from large images, and subsequently provides qual-\nity performance improvements at test time. We conduct\ncomprehensive experiments and demonstrate state-of-the-\nart performance of our Restormer on16 benchmark datasets\nfor several image restoration tasks, including image derain-\ning, single-image motion deblurring, defocus deblurring\n(on single-image and dual pixel data), and image denois-\ning (on synthetic and real data); See Fig. 1. Furthermore,\nwe provide extensive ablations to show the effectiveness of\narchitectural designs and experimental choices.\nThe main contributions of this work are summarized below:\n• We propose Restormer, an encoder-decoder Transformer\nfor multi-scale local-global representation learning on\nhigh-resolution images without disintegrating them into\nlocal windows, thereby exploiting distant image context.\n• We propose a multi-Dconv head transposed attention\n(MDTA) module that is capable of aggregating local and\nnon-local pixel interactions, and is efﬁcient enough to\nprocess high-resolution images.\n• A new gated-Dconv feed-forward network (GDFN) that\nperforms controlled feature transformation,i.e., suppress-\ning less informative features, and allowing only the useful\ninformation to pass further through the network hierarchy.\n2. Background\nImage Restoration. In recent years, data-driven CNN ar-\nchitectures [7, 18, 92, 93, 105, 107] have been shown to out-\nperform conventional restoration approaches [26,36,53,75].\nAmong convolutional designs, encoder-decoder based U-\nNet architectures [3, 14, 39, 80, 90, 93, 99] have been pre-\ndominantly studied for restoration due to their hierarchical\nmulti-scale representation while remaining computationally\nefﬁcient. Similarly, skip connection based approaches have\nbeen shown to be effective for restoration due to speciﬁc\nfocus on learning residual signals [24, 48, 92, 106]. Spatial\nand channel attention modules have also been incorporated\nto selectively attend to relevant information [43,92,93]. We\nrefer the reader to NTIRE challenge reports [2,5,30,57] and\nrecent literature reviews [8,42,73], which summarize major\ndesign choices for image restoration.\nVision Transformers. The Transformer model is ﬁrst de-\nveloped for sequence processing in natural language tasks\n[77]. It has been adapted in numerous vision tasks such as\nimage recognition [17, 76, 88], segmentation [78, 83, 108],\nobject detection [11, 50, 109]. The Vision Transformers\n[17, 76] decompose an image into a sequence of patches\n(local windows) and learn their mutual relationships. The\ndistinguishing feature of these models is the strong ca-\npability to learn long-range dependencies between image\npatch sequences and adaptability to given input content\n[34]. Due to these characteristics, Transformer models have\nalso been studied for the low-level vision problems such as\nsuper-resolution [44,85], image colorization [37], denoising\n[13, 80], and deraining [80]. However, the computational\ncomplexity of SA in Transformers can increase quadrati-\ncally with the number of image patches, therby prohibiting\nits application to high-resolution images. Therefore, in low-\nlevel image processing applications, where high-resolution\n2\n3x3\nR\nSoftmax\nTransformer\nBlock\nNorm\n1x11x11x1\nDconv\n1x1\nHxWxC\nc\n1x1\n1x1\nc\nc\nGDFN\nMDTA\n3x3\nDconv\n3x3\nDconv\n3x3 Norm\n1x1\nDconv\n3x3\nDconv\n3x3\n3x3\nR\nR R\nNorm\nDconv\n3x3\nc\nR\nDownsample\nUpsample\nElement-wise Addition\nConcatenation\nReshape\nGELU Activation\nLayer Normalization\nDepth-wise Convolution\nMatrix Multiplication\nElement-wise Multiplication\nMDTA Multi-Dconv Head\nTransposed Attention\nGDFN Gated Dconv\nFeed-Forward Network\nSkip Connections\nHxWx2C\nHxWx2C HxWx3\nReﬁnementHxWxC\nHxWxC\nH  W\n4   4   x    x4C\nHxWxC\nTransposed-Attention\nMap (    )\nH  W\n8   8   x    x8C\nH  W\n4   4   x    x4C\nH  W\n4   4   x    x4C\nH  W\n8   8   x    x8C\nH  W\n2   2   x    x2C\nH  W\n2   2   x    x2C\nH  W\n2   2   x    x2C\nHxWxC\nHWxC\nCxHW\nCxC\nHWxC HxWxCHWxC\nHxWxC\nHxWx   C\nHxWxC\nDegraded (  )\nRestored (  )\nTransformer\nBlock\nTransformer\nBlock\nTransformer\nBlock\nTransformer\nBlock\nTransformer\nBlock\nTransformer\nBlock\nHxWx2C\nLx r\n1x1\n1x1\nLx 1Lx 1\nLx 2\nLx 3\nLx 4\nLx 3\n(a)\n(b)\nFigure 2. Architecture of Restormer for high-resolution image restoration. Our Restormer consists of multiscale hierarchical design incor-\nporating efﬁcient Transformer blocks. The core modules of Transformer block are:(a) multi-Dconv head transposed attention (MDTA) that\nperforms (spatially enriched) query-key feature interaction across channels rather the spatial dimension, and(b) Gated-Dconv feed-forward\nnetwork (GDFN) that performs controlled feature transformation, i.e., to allow useful information to propagate further.\noutputs need to be generated, recent methods generally em-\nploy different strategies to reduce complexity. One potential\nremedy is to apply self-attention within local image regions\n[44, 80] using the Swin Transformer design [44]. However,\nthis design choice restricts the context aggregation within\nlocal neighbourhood, defying the main motivation of using\nself-attention over convolutions, thus not ideally suited for\nimage-restoration tasks. In contrast, we present a Trans-\nformer model that can learn long-range dependencies while\nremaining computationally efﬁcient.\n3. Method\nOur main goal is to develop an efﬁcient Transformer\nmodel that can handle high-resolution images for restora-\ntion tasks. To alleviate the computational bottleneck, we in-\ntroduce key designs to the multi-head SA layer and a multi-\nscale hierarchical module that has lesser computing require-\nments than a single-scale network [44]. We ﬁrst present the\noverall pipeline of our Restormer architecture (see Fig. 2).\nThen we describe the core components of the proposed\nTransformer block: (a) multi-Dconv head transposed atten-\ntion (MDTA) and (b) gated-Dconv feed-forward network\n(GDFN). Finally, we provide details on the progressive\ntraining scheme for effectively learning image statistics.\nOverall Pipeline. Given a degraded image I ∈RH×W×3,\nRestormer ﬁrst applies a convolution to obtain low-level\nfeature embeddings F0 ∈RH×W×C; where H×W denotes\nthe spatial dimension and C is the number of channels.\nNext, these shallow featuresF0 pass through a 4-level sym-\nmetric encoder-decoder and transformed into deep features\nFd ∈RH×W×2C. Each level of encoder-decoder contains\nmultiple Transformer blocks, where the number of blocks\nare gradually increased from the top to bottom levels to\nmaintain efﬁciency. Starting from the high-resolution input,\nthe encoder hierarchically reduces spatial size, while ex-\npanding channel capacity. The decoder takes low-resolution\nlatent features Fl ∈R\nH\n8 ×W\n8 ×8C as input and progressively\nrecovers the high-resolution representations. For feature\ndownsampling and upsampling, we apply pixel-unshufﬂe\nand pixel-shufﬂe operations [69], respectively. To assist\nthe recovery process, the encoder features are concatenated\nwith the decoder features via skip connections [66]. The\nconcatenation operation is followed by a 1×1 convolution\nto reduce channels (by half) at all levels, except the top\none. At level-1, we let Transformer blocks to aggregate the\nlow-level image features of the encoder with the high-level\nfeatures of the decoder. It is beneﬁcial in preserving the\nﬁne structural and textural details in the restored images.\nNext, the deep features Fd are further enriched in the re-\nﬁnement stage operating at high spatial resolution. These\ndesign choices yield quality improvements as we shall see\nin the experiment section (Sec. 4). Finally, a convolution\nlayer is applied to the reﬁned features to generate residual\nimage R ∈RH×W×3 to which degraded image is added to\nobtain the restored image: ˆI = I + R. Next, we present the\nmodules of the Transformer block.\n3.1. Multi-Dconv Head Transposed Attention\nThe major computational overhead in Transformers\ncomes from the self-attention layer. In conventional SA [17,\n77], the time and memory complexity of the key-query dot-\nproduct interaction grows quadratically with the spatial res-\nolution of input, i.e., O(W2H2) for images of W×H pix-\nels. Therefore, it is infeasible to apply SA on most im-\nage restoration tasks that often involve high-resolution im-\n3\nages. To alleviate this issue, we propose MDTA, shown\nin Fig. 2(a), that has linear complexity. The key ingredient\nis to apply SA across channels rather than the spatial di-\nmension, i.e., to compute cross-covariance across channels\nto generate an attention map encoding the global context\nimplicitly. As another essential component in MDTA, we\nintroduce depth-wise convolutions to emphasize on the lo-\ncal context before computing feature covariance to produce\nthe global attention map.\nFrom a layer normalized tensor Y ∈R ˆH× ˆW× ˆC, our\nMDTA ﬁrst generates query (Q), key (K) and value (V)\nprojections, enriched with local context. It is achieved\nby applying 1×1 convolutions to aggregate pixel-wise\ncross-channel context followed by 3×3 depth-wise con-\nvolutions to encode channel-wise spatial context, yielding\nQ=WQ\nd WQ\np Y, K=WK\nd WK\np Y and V=WV\nd WV\np Y. Where\nW(·)\np is the 1×1 point-wise convolution andW(·)\nd is the 3×3\ndepth-wise convolution. We use bias-free convolutional\nlayers in the network. Next, we reshape query and key\nprojections such that their dot-product interaction gener-\nates a transposed-attention map A of size RˆC× ˆC, instead of\nthe huge regular attention map of size R ˆH ˆW× ˆH ˆW [17, 77].\nOverall, the MDTA process is deﬁned as:\nˆX = Wp Attention\n(\nˆQ,ˆK,ˆV\n)\n+ X,\nAttention\n(\nˆQ,ˆK,ˆV\n)\n= ˆV ·Softmax\n(\nˆK ·ˆQ/α\n)\n,\n(1)\nwhere X and ˆX are the input and output feature maps;\nˆQ ∈ R ˆH ˆW× ˆC; ˆK ∈ RˆC× ˆH ˆW ; and ˆV ∈ R ˆH ˆW× ˆC ma-\ntrices are obtained after reshaping tensors from the original\nsize R ˆH× ˆW× ˆC. Here, αis a learnable scaling parameter to\ncontrol the magnitude of the dot product of ˆK and ˆQ before\napplying the softmax function. Similar to the conventional\nmulti-head SA [17], we divide the number of channels into\n‘heads’ and learn separate attention maps in parallel.\n3.2. Gated-Dconv Feed-Forward Network\nTo transform features, the regular feed-forward network\n(FN) [17,77] operates on each pixel location separately and\nidentically. It uses two 1×1 convolutions, one to expand\nthe feature channels (usually by factor γ=4) and second to\nreduce channels back to the original input dimension. A\nnon-linearity is applied in the hidden layer. In this work,\nwe propose two fundamental modiﬁcations in FN to im-\nprove representation learning: (1) gating mechanism, and\n(2) depthwise convolutions. The architecture of our GDFN\nis shown in Fig. 2(b). The gating mechanism is formulated\nas the element-wise product of two parallel paths of linear\ntransformation layers, one of which is activated with the\nGELU non-linearity [27]. As in MDTA, we also include\ndepth-wise convolutions in GDFN to encode information\nfrom spatially neighboring pixel positions, useful for learn-\ning local image structure for effective restoration. Given an\ninput tensor X ∈R ˆH× ˆW× ˆC, GDFN is formulated as:\nˆX = W0\np Gating (X) +X,\nGating(X) =φ(W1\nd W1\np (LN(X)))⊙W2\nd W2\np (LN(X)),\n(2)\nwhere ⊙denotes element-wise multiplication, φrepresents\nthe GELU non-linearity, and LN is the layer normaliza-\ntion [9]. Overall, the GDFN controls the information ﬂow\nthrough the respective hierarchical levels in our pipeline,\nthereby allowing each level to focus on the ﬁne details\ncomplimentary to the other levels. That is, GDFN offers\na distinct role compared to MDTA (focused on enriching\nfeatures with contextual information). Since the proposed\nGDFN performs more operations as compared to the regu-\nlar FN [17], we reduce the expansion ratio γ so as to have\nsimilar parameters and compute burden.\n3.3. Progressive Learning\nCNN-based restoration models are usually trained on\nﬁxed-size image patches. However, training a Transformer\nmodel on small cropped patches may not encode the global\nimage statistics, thereby providing suboptimal performance\non full-resolution images at test time. To this end, we per-\nform progressive learning where the network is trained on\nsmaller image patches in the early epochs and on grad-\nually larger patches in the later training epochs. The\nmodel trained on mixed-size patches via progressive learn-\ning shows enhanced performance at test time where images\ncan be of different resolutions (a common case in image\nrestoration). The progressive learning strategy behaves in\na similar fashion to the curriculum learning process where\nthe network starts with a simpler task and gradually moves\nto learning a more complex one (where the preservation of\nﬁne image structure/textures is required). Since training on\nlarge patches comes at the cost of longer time, we reduce the\nbatch size as the patch size increases to maintain a similar\ntime per optimization step as of the ﬁxed patch training.\n4. Experiments and Analysis\nWe evaluate the proposed Restormer on benchmark\ndatasets and experimental settings for four image process-\ning tasks: (a) image deraining, (b) single-image motion de-\nblurring, (c) defocus deblurring (on single-image, and dual-\npixel data), and (d) image denoising (on synthetic and real\ndata). More details on datasets, training protocols, and ad-\nditional visual results are presented in the supplementary\nmaterial. In tables, the best and second-best quality scores\nof the evaluated methods are highlighted and underlined.\nImplementation Details. We train separate models for dif-\nferent image restoration tasks. In all experiments, we use\nthe following training parameters, unless mentioned other-\nwise. Our Restormer employs a 4-level encoder-decoder.\n4\nTable 1. Image deraining results. When averaged across all ﬁve datasets, our Restormer advances state-of-the-art by 1.05 dB.\nTest100[97] Rain100H[86] Rain100L[86] Test2800[22] Test1200[96] Average\nMethod PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑\nDerainNet [21] 22.77 0.810 14.92 0.592 27.03 0.884 24.31 0.861 23.38 0.835 22.48 0.796\nSEMI [81] 22.35 0.788 16.56 0.486 25.03 0.842 24.43 0.782 26.05 0.822 22.88 0.744\nDIDMDN [96] 22.56 0.818 17.35 0.524 25.23 0.741 28.13 0.867 29.65 0.901 24.58 0.770\nUMRL [87] 24.41 0.829 26.01 0.832 29.18 0.923 29.97 0.905 30.55 0.910 28.02 0.880\nRESCAN [43] 25.00 0.835 26.36 0.786 29.80 0.881 31.29 0.904 30.51 0.882 28.59 0.857\nPreNet [64] 24.81 0.851 26.77 0.858 32.44 0.950 31.75 0.916 31.36 0.911 29.42 0.897\nMSPFN [32] 27.50 0.876 28.66 0.860 32.40 0.933 32.82 0.930 32.39 0.916 30.75 0.903\nMPRNet [93] 30.27 0.897 30.41 0.890 36.40 0.965 33.64 0.938 32.91 0.916 32.73 0.921\nSPAIR [61] 30.35 0.909 30.95 0.892 36.93 0.969 33.34 0.936 33.04 0.922 32.91 0.926\nRestormer 32.00 0.923 31.46 0.904 38.99 0.978 34.18 0.944 33.19 0.926 33.96 0.935\nPSNR 18.76 dB 20.23 dB 23.66 dB 25.52 dB\nReference Rainy DerainNet [21] SEMI [81] UMRL [87]\n18.76 dB 26.88 dB 27.16 dB 29.86 dB 32.15 dB 33.97 dB\nRainy Image RESCAN [43] PreNet [64] MSPFN [32] MPRNet [93] Restormer\nFigure 3. Image deraining example. Our Restormer generates rain-free image with structural ﬁdelity and without artifacts.\nTable 2. Single-image motion deblurring results. Our Restormer\nis trained only on the GoPro dataset [56] and directly applied to the\nHIDE [67] and RealBlur [65] benchmark datasets.\nGoPro[56] HIDE[67] RealBlur-R[65]RealBlur-J[65]\nMethod PSNRSSIMPSNRSSIM PSNRSSIM PSNRSSIM\nXuet al. [84] 21.00 0.741 - 34.460.937 27.140.830\nDeblurGAN [38] 28.700.85824.510.871 33.790.903 27.970.834\nNahet al. [56] 29.08 0.91425.730.874 32.510.841 27.870.827\nZhanget al. [98] 29.190.931 - 35.480.947 27.800.847\nDeblurGAN-v2 [39]29.550.93426.610.875 35.260.944 28.700.866\nSRN [72] 30.26 0.93428.360.915 35.660.947 28.560.867\nShenet al. [67] - 28.890.930 - -\nGaoet al. [23] 30.90 0.93529.110.913 - -\nDBGAN [100] 31.100.94228.940.915 33.780.909 24.930.745\nMT-RNN [58] 31.150.94529.150.918 35.790.951 28.440.862\nDMPHN [94] 31.200.94029.090.924 35.700.948 28.420.860\nSuinet al. [71] 31.85 0.94829.980.930 - -\nSPAIR [61] 32.06 0.95330.290.931 - 28.810.875\nMIMO-UNet+ [14] 32.450.95729.990.930 35.540.947 27.630.837\nIPT [13] 32.52 - - - -\nMPRNet [93] 32.660.95930.960.939 35.990.952 28.700.873\nRestormer 32.92 0.96131.220.942 36.190.957 28.960.879\nFrom level-1 to level-4, the number of Transformer blocks\nare [4,6,6,8], attention heads in MDTA are [1,2,4,8], and\nnumber of channels are [48,96,192,384]. The reﬁnement\nstage contains 4 blocks. The channel expansion factor in\nGDFN is γ=2.66. We train models with AdamW optimizer\n(β1=0.9, β2=0.999, weight decay 1e−4) and L 1 loss for\n300K iterations with the initial learning rate3e−4 gradually\nreduced to 1e−6 with the cosine annealing [51]. For pro-\ngressive learning, we start training with patch size128×128\nand batch size 64. The patch size and batch size pairs\nare updated to [( 1602,40), ( 1922,32), ( 2562,16), ( 3202,8),\n(3842,8)] at iterations [92K, 156K, 204K, 240K, 276K]. For\ndata augmentation, we use horizontal and vertical ﬂips.\n4.1. Image Deraining Results\nWe compute PSNR/SSIM scores using the Y channel\nin YCbCr color space in a way similar to existing meth-\nods [32, 61, 93]. Table 1 shows that our Restormer achieves\nconsistent and signiﬁcant performance gains over existing\napproaches on all ﬁve datasets. Compared to the recent\nbest method SPAIR [61], Restormer achieves 1.05 dB im-\nprovement when averaged across all datasets. On indi-\nvidual datasets, the gain can be as large as 2.06 dB, e.g.,\nRain100L. Figure 3 shows a challenging visual example.\nOur Restormer reproduces a raindrop-free image while ef-\nfectively preserving the structural content.\n4.2. Single-image Motion Deblurring Results\nWe evaluate deblurring methods both on the synthetic\ndatasets (GoPro [56], HIDE [67]) and the real-world\ndatasets (RealBlur-R [65], RealBlur-J [65]). Table 2 shows\nthat our Restormer outperforms other approaches on all four\nbenchmark datasets. When averaged across all datasets, our\nmethod obtains a performance boost of 0.47 dB over the\nrecent algorithm MIMO-UNet+ [14] and 0.26 dB over the\nprevious best method MPRNet [93]. Compared to MPR-\nNet [93], Restormer has 81% fewer FLOPs (See Fig. 1).\n5\nPSNR 19.45 dB 23.85 dB 23.56 dB 23.86 dB\nReference Blurry Gao et al. [23] DBGAN [100] MTRNN [58]\n19.45 dB 24.85 dB 25.20 dB 25.67 dB 24.33 dB 26.96 dB\nBlurry Image DMPHN [94] Suin et al. [71] MPRNet [93] MIMO-UNet+ [14] Restormer\nFigure 4. Single image motion deblurring on GoPro [56]. Restormer generates sharper and visually-faithful result.\nTable 3. Defocus deblurring comparisons on the DPDD testset [3] (containing 37 indoor and 39 outdoor scenes).S: single-image defocus\ndeblurring. D: dual-pixel defocus deblurring. Restormer sets new state-of-the-art for both single-image and dual pixel defocus deblurring.\nIndoor Scenes Outdoor Scenes Combined\nMethod PSNR↑ SSIM↑ MAE↓ LPIPS↓ PSNR↑ SSIM↑ MAE↓ LPIPS↓ PSNR↑ SSIM↑ MAE↓ LPIPS↓\nEBDBS[33] 25.77 0.772 0.040 0.297 21.25 0.599 0.058 0.373 23.45 0.683 0.049 0.336\nDMENetS[40] 25.50 0.788 0.038 0.298 21.43 0.644 0.063 0.397 23.41 0.714 0.051 0.349\nJNBS[68] 26.73 0.828 0.031 0.273 21.10 0.608 0.064 0.355 23.84 0.715 0.048 0.315\nDPDNetS[3] 26.54 0.816 0.031 0.239 22.25 0.682 0.056 0.313 24.34 0.747 0.044 0.277\nKPACS[70] 27.97 0.852 0.026 0.182 22.62 0.701 0.053 0.269 25.22 0.774 0.040 0.227\nIFANS[41] 28.11 0.861 0.026 0.179 22.76 0.720 0.052 0.254 25.37 0.789 0.039 0.217\nRestormerS 28.87 0.882 0.025 0.145 23.24 0.743 0.050 0.209 25.98 0.811 0.038 0.178\nDPDNetD[3] 27.48 0.849 0.029 0.189 22.90 0.726 0.052 0.255 25.13 0.786 0.041 0.223\nRDPDD[4] 28.10 0.843 0.027 0.210 22.82 0.704 0.053 0.298 25.39 0.772 0.040 0.255\nUformerD[80] 28.23 0.860 0.026 0.199 23.10 0.728 0.051 0.285 25.65 0.795 0.039 0.243\nIFAND[41] 28.66 0.868 0.025 0.172 23.46 0.743 0.049 0.240 25.99 0.804 0.037 0.207\nRestormerD 29.48 0.895 0.023 0.134 23.97 0.773 0.047 0.175 26.66 0.833 0.035 0.155\nPSNR 27.19 dB 27.44 dB 28.67 dB\nReference Blurry DMENet [40] DPDNet [3]\n27.19 dB 29.01 dB 28.35 dB 29.12 dB 30.45 dB\nBlurry Image RDPD [4] IFAN [41] Uformer [80] Restormer\nFigure 5. Dual-pixel defocus deblurring comparison on the DPDD dataset [3]. Compared to the other approaches, our Restormer more\neffectively removes blur while preserving the ﬁne image details.\nMoreover, our method shows 0.4 dB improvement over the\nTransformer model IPT [13], while having 4.4×fewer pa-\nrameters and runs 29×faster. Notably, our Restormer is\ntrained only on the GoPro [56] dataset, yet it demonstrates\nstrong generalization to other datasets by setting new state-\nof-the-art. Fig. 4 shows that the image produced by our\nmethod is more sharper and visually closer to the ground-\ntruth than those of the other algorithms.\n4.3. Defocus Deblurring Results\nTable 3 shows image ﬁdelity scores of the conventional\ndefocus deblurring methods (EBDB [33] and JNB [68]) as\nwell as learning based approaches on the DPDD dataset [3].\nOur Restormer signiﬁcantly outperforms the state-of-the-art\nschemes for the single-image and dual-pixel defocus de-\nblurring tasks on all scene categories. Particularly on the\ncombined scene category, Restormer yields ∼0.6 dB im-\nprovements over the previous best method IFAN [41]. Com-\npared to the Transformer model Uformer [80], our method\nprovides a substantial gain of 1.01 dB PSNR. Figure 5 il-\nlustrates that our method is more effective in removing spa-\ntially varying defocus blur than other approaches.\n4.4. Image Denoising Results\nWe perform denoising experiments on synthetic bench-\nmark datasets generated with additive white Gaussian noise\n(Set12 [101], BSD68 [52], Urban100 [29], Kodak24 [20]\nand McMaster [104]) as well as on real-world datasets\n6\nTable 4. Gaussian grayscale image denoising comparisons\nfor two categories of methods. Top super row: learning a\nsingle model to handle various noise levels. Bottom super\nrow: training a separate model for each noise level.\nSet12[101] BSD68[52] Urban100[29]\nMethod σ=15σ=25σ=50σ=15σ=25σ=50σ=15σ=25σ=50\nDnCNN [101]32.67 30.35 27.1831.62 29.16 26.2332.28 29.80 26.35\nFFDNet [103]32.75 30.43 27.3231.63 29.19 26.2932.40 29.90 26.50\nIRCNN [102]32.76 30.37 27.1231.63 29.15 26.1932.46 29.80 26.22\nDRUNet [99]33.2530.9427.9031.9129.4826.5933.4431.1127.96\nRestormer33.35 31.04 28.0131.95 29.51 26.6233.67 31.39 28.33\nFOCNet [31]33.07 30.73 27.6831.83 29.38 26.5033.15 30.64 27.40\nMWCNN [47]33.15 30.79 27.7431.86 29.41 26.5333.17 30.66 27.42\nNLRN [46]33.16 30.80 27.6431.88 29.41 26.4733.45 30.94 27.49\nRNAN [106]- - 27.70 - - 26.48 - - 27.65\nDeamNet [63]33.19 30.81 27.7431.91 29.44 26.5433.37 30.85 27.53\nDAGL [55]33.28 30.93 27.8131.93 29.46 26.5133.79 31.39 27.97\nSwinIR [44]33.3631.0127.9131.9729.5026.5833.7031.3027.98\nRestormer33.42 31.08 28.0031.9629.52 26.6233.79 31.46 28.29\nTable 5. Gaussian color image denoising. Our Restormer demonstrates\nfavorable performance among both categories of methods. On Urban\ndataset [29] for noise level 50, Restormer yields 0.41 dB gain over CNN-\nbased DRUNet [99], and 0.2 dB over Transformer model SwinIR [44].\nCBSD68[52] Kodak24[20] McMaster[104] Urban100[29]\nMethod σ=15σ=25σ=50σ=15σ=25σ=50σ=15σ=25σ=50σ=15σ=25σ=50\nIRCNN [102]33.86 31.16 27.8634.69 32.18 28.9334.58 32.18 28.9133.78 31.20 27.70\nFFDNet [103]33.87 31.21 27.9634.63 32.13 28.9834.66 32.35 29.1833.83 31.40 28.05\nDnCNN [101]33.90 31.24 27.9534.60 32.14 28.9533.45 31.52 28.6232.98 30.81 27.59\nDSNet [59]33.91 31.28 28.0534.63 32.16 29.0534.67 32.40 29.28- - -\nDRUNet [99]34.3031.6928.5135.3132.8929.8635.4033.1430.0834.8132.6029.61\nRestormer34.39 31.78 28.5935.44 33.02 30.0035.55 33.31 30.2935.06 32.91 30.02\nRPCNN [82]- 31.24 28.06 - 32.34 29.25 - 32.33 29.33 - 31.81 28.62\nBRDNet [74]34.10 31.43 28.1634.88 32.41 29.2235.08 32.75 29.5234.42 31.99 28.56\nRNAN [106]- - 28.27 - - 29.58 - - 29.72 - - 29.08\nRDN [107] - - 28.31 - - 29.66 - - - - - 29.38\nIPT [13] - - 28.39 - - 29.64 - - 29.98 - - 29.71\nSwinIR [44]34.4231.7828.5635.3432.8929.7935.6133.2030.2235.1332.9029.82\nRestormer34.4031.79 28.6035.47 33.04 30.0135.61 33.34 30.3035.13 32.96 30.02\nTable 6. Real image denoising on SIDD [1] and DND [60] datasets. ∗ denotes methods using additional training data. Our Restormer is\ntrained only on the SIDD images and directly tested on DND. Among competing approaches, only Restormer surpasses 40 dB PSNR.\nMethodDnCNN BM3D CBDNet* RIDNet* AINDNet* VDN SADNet* DANet+* CycleISP* MIRNet DeamNet* MPRNet DAGL UformerRestormer\nDataset [101] [15] [25] [6] [35] [89] [12] [90] [91] [92] [63] [93] [55] [80] (Ours)\nSIDDPSNR↑ 23.66 25.65 30.78 38.71 39.08 39.28 39.46 39.47 39.52 39.72 39.47 39.71 38.94 39.77 40.02\n[1] SSIM↑ 0.583 0.685 0.801 0.951 0.954 0.956 0.957 0.957 0.957 0.959 0.957 0.958 0.953 0.959 0.960\nDND PSNR↑ 32.43 34.51 38.06 39.26 39.37 39.38 39.59 39.58 39.56 39.88 39.63 39.80 39.77 39.96 40.03\n[60] SSIM↑ 0.790 0.851 0.942 0.953 0.951 0.952 0.952 0.955 0.956 0.956 0.953 0.954 0.956 0.956 0.956\nNoisy 14.92 dB PSNR 27.61 dB 31.83 dB 30.12 dB 31.74 dB 32.83 dB\nImage Noisy Reference DnCNN [101] DRUNet [99] DeamNet [63] SwinIR [44] Restormer\nNoisy 14.81 dB PSNR 33.83 dB 35.09 dB 34.86 dB 35.20 dB 35.63 dB\nImage Noisy Reference FFDNet [103] DRUNet [99] IPT [13] SwinIR [44] Restormer\nNoisy 18.16 dB PSNR 31.36 dB 30.25 dB 31.17 dB 31.15 dB 31.57 dB\nImage Noisy Reference MIRNet [92] DeamNet [63] MPRNet [93] Uformer [80] Restormer\nFigure 6. Visual results on Image denoising. Top row: Gaussian grayscale denoising. Middle row: Gaussian color denoising. Bottom\nrow: real image denoising. The image reproduction quality of our Restormer is more faithful to the ground-truth than other methods.\n(SIDD [1] and DND [60]). Following [54, 93, 99], we use\nbias-free Restormer for denoising.\nGaussian denoising. Table 4 and Table 5 show PSNR\nscores of different approaches on several benchmark\ndatasets for grayscale and color image denoising, respec-\ntively. Consistent with existing methods [44, 99], we in-\nclude noise levels 15, 25 and 50 in testing. The evalu-\nated methods are divided into two experimental categories:\n(1) learning a single model to handle various noise lev-\nels, and (2) learning a separate model for each noise level.\nOur Restormer achieves state-of-the-art performance un-\nder both experimental settings on different datasets and\nnoise levels. Speciﬁcally, for the challenging noise level\n50 on high-resolution Urban100 dataset [29], Restormer\nachieves 0.37 dB gain over the previous best CNN-based\nmethod DRUNet [99], and 0.31 dB boost over the recent\n7\nTable 7. Ablation experiments for the Transformer block.\nPSNR is computed on a high-resolution Urban100 dataset [29].\nFLOPsParams PSNR\nNetwork Component (B) (M) σ=15σ=25σ=50\nBaseline(a)UNet with Resblocks [45]83.4 24.5334.42 32.18 29.11\nMulti-head(b) MTA + FN [77] 83.7 24.8434.66 32.39 29.28\nattention(c) MDTA + FN [77] 85.3 25.0234.72 32.48 29.43\nFeed-forward(d) MTA + GFN 83.5 24.7934.70 32.43 32.40\nnetwork (e) MTA + DFN 85.8 25.0834.68 32.45 29.42\n(f) MTA + GDFN 86.2 25.1234.77 32.56 29.54\nOverall (g) MDTA + GDFN 87.7 25.3134.82 32.61 29.62\ntransformer-based network SwinIR [44], as shown in Ta-\nble 4. Similar performance gains can be observed for the\nGaussian color denoising in Table 5. It is worth mentioning\nthat DRUNet [99] requires the noise level map as an addi-\ntional input, whereas our method only takes the noisy im-\nage. Furthermore, compared to SwinIR [44], our Restormer\nhas 3.14×fewer FLOPs and runs 13×faster. Figure 6\npresents denoised results by different methods for grayscale\ndenoising (top row) and color denoising (middle row). Our\nRestormer restores clean and crisp images.\nReal image denoising. Table 6 shows that our method is the\nonly one surpassing 40 dB PSNR on both datasets. Notably,\non the SIDD dataset our Restormer obtains PSNR gains of\n0.3 dB and 0.25 dB over the previous best CNN method\nMIRNet [92] and Transformer model Uformer [80], respec-\ntively. Fig. 6 (bottom row) shows that our Restormer gener-\nates clean image without compromising ﬁne texture.\n4.5. Ablation Studies\nFor ablation experiments, we train Gaussian color de-\nnoising models on image patches of size128×128 for 100K\niterations only. Testing is performed on Urban100 [29], and\nanalysis is provided for a challenging noise level σ=50.\nFLOPs and inference time are computed on image size\n256×256. Table 7-10 show that our contributions yield\nquality performance improvements. Next, we describe the\ninﬂuence of each component individually.\nImprovements in multi-head attention. Table 7c demon-\nstrates that our MDTA provides favorable gain of 0.32 dB\nover the baseline (Table 7a). Furthermore, bringing locality\nto MDTA via depth-wise convolution improves robustness\nas removing it results in PSNR drop (see Table 7b).\nImprovements in feed-forward network (FN). Table 7d\nshows that the gating mechanism in FN to control informa-\ntion ﬂow yields 0.12 dB gain over the conventional FN [77].\nAs in multi-head attention, introducing local mechanism to\nFN also brings performance advantages (see Table 7e). We\nfurther strengthen the FN by incorporating gated depth-wise\nconvolutions. Our GDFN (Table 7f) achieves PSNR gain of\n0.26 dB over the standard FN [77] for the noise level 50.\nOverall, our Transformer block contributions lead to a sig-\nniﬁcant gain of 0.51 dB over the baseline.\nDesign choices for decoder at level-1. To aggregate en-\ncoder features with the decoder at level-1, we do not em-\nploy 1×1 convolution (that reduces channels by half) af-\nter concatenation operation. It is helpful in preserving ﬁne\ntextural details coming from the encoder, as shown in Ta-\nble 8. These results further demonstrate the effectiveness of\nadding Transformer blocks in the reﬁnement stage.\nImpact of progressive learning. Table 9 shows that the\nprogressive learning provides better results than the ﬁxed\npatch training, while having similar training time.\nDeeper or wider Restormer? Table 10 shows that, un-\nder similar parameters/FLOPs budget, a deep-narrow model\nperforms more accurately than its wide-shallow counter-\npart. However, the wider model runs faster due to paral-\nlelization. In this paper we use deep-narrow Restormer.\n5. Conclusion\nWe present an image restoration Transformer model,\nRestormer, that is computationally efﬁcient to handle high-\nresolution images. We introduce key designs to the core\ncomponents of the Transformer block for improved feature\naggregation and transformation. Speciﬁcally, our multi-\nDconv head transposed attention (MDTA) module implic-\nitly models global context by applying self-attention across\nchannels rather than the spatial dimension, thus having lin-\near complexity rather than quadratic. Furthermore, the\nproposed gated-Dconv feed-forward network (GDFN) in-\ntroduces a gating mechanism to perform controlled fea-\nture transformation. To incorporate the strength of CNNs\ninto the Transformer model, both MDTA and GDFN mod-\nules include depth-wise convolutions for encoding spatially\nlocal context. Extensive experiments on 16 benchmark\ndatasets demonstrate that Restormer achieves the state-of-\nthe-art performance for numerous image restoration tasks.\nTable 8. Inﬂuence of concat (w/o\n1x1 conv) and reﬁnement stage at\ndecoder (level-1). We add the com-\nponents to experiment Table 7(g).\nPSNR(σ=50) FLOPsParams\nTable 7(g) 29.62 87.7 25.31\n+Concat 29.66 110 25.65\n+Reﬁnement 29.71 141 26.12\nTable 9. Results of training Restormer on ﬁxed\npatch size and progressively large patch sizes. In\nprogressive learning [28], we reduce batch size\n(as patch size increases) to have similar time per\noptimization step as of ﬁxed patch training.\nPatch Size PSNR(σ=50) Train Time (h)\nFixed (1282) 29.71 22.5\nProgressive (1282to3842) 29.78 23.1\nTable 10. Deeper vs wider model. We ad-\njust # of transformer blocks to keep ﬂops and\nparams constant. Deep narrow model is more\naccurate, while wide shallow model is faster.\nDimPSNR(σ=50) Train Time (h)Test Time (s)\n48 29.71 22.5 0.115\n64 29.63 15.5 0.080\n80 29.56 13.5 0.069\n8\nAcknowledgements. Ming-Hsuan Yang is supported by\nthe NSF CAREER grant 1149783. Munawar Hayat is\nsupported by the ARC DECRA Fellowship DE200101100.\nSpecial thanks to Abdullah Abuolaim and Zhendong Wang\nfor providing the results.\nReferences\n[1] Abdelrahman Abdelhamed, Stephen Lin, and Michael S\nBrown. A high-quality denoising dataset for smartphone\ncameras. In CVPR, 2018. 7\n[2] Abdelrahman Abdelhamed, Radu Timofte, and Michael S\nBrown. NTIRE 2019 challenge on real image denoising:\nMethods and results. In CVPR Workshops, 2019. 2\n[3] Abdullah Abuolaim and Michael S Brown. Defocus deblur-\nring using dual-pixel data. In ECCV, 2020. 2, 6\n[4] Abdullah Abuolaim, Mauricio Delbracio, Damien Kelly,\nMichael S. Brown, and Peyman Milanfar. Learning to re-\nduce defocus blur by realistically modeling dual-pixel data.\nIn ICCV, 2021. 6\n[5] Abdullah Abuolaim, Radu Timofte, and Michael S Brown.\nNTIRE 2021 challenge for defocus deblurring using dual-\npixel images: Methods and results. In CVPR Workshops,\n2021. 2\n[6] Saeed Anwar and Nick Barnes. Real image denoising with\nfeature attention. ICCV, 2019. 7\n[7] Saeed Anwar and Nick Barnes. Densely residual laplacian\nsuper-resolution. TPAMI, 2020. 2\n[8] Saeed Anwar, Salman Khan, and Nick Barnes. A deep jour-\nney into super-resolution: A survey. ACM Computing Sur-\nveys, 2019. 2\n[9] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\nLayer normalization. arXiv:1607.06450, 2016. 4\n[10] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners.arXiv:2005.14165,\n2020. 1\n[11] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nico-\nlas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\nEnd-to-end object detection with transformers. In ECCV,\n2020. 1, 2\n[12] Meng Chang, Qi Li, Huajun Feng, and Zhihai Xu. Spatial-\nadaptive network for single image denoising. In ECCV,\n2020. 7\n[13] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yip-\ning Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu,\nand Wen Gao. Pre-trained image processing transformer. In\nCVPR, 2021. 1, 2, 5, 6, 7\n[14] Sung-Jin Cho, Seo-Won Ji, Jun-Pyo Hong, Seung-Won\nJung, and Sung-Jea Ko. Rethinking coarse-to-ﬁne approach\nin single image deblurring. In ICCV, 2021. 1, 2, 5, 6\n[15] Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik,\nand Karen Egiazarian. Image denoising by sparse 3-D\ntransform-domain collaborative ﬁltering. TIP, 2007. 7\n[16] Yann N Dauphin, Angela Fan, Michael Auli, and David\nGrangier. Language modeling with gated convolutional net-\nworks. In ICML, 2017. 2\n[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In ICLR, 2021. 1,\n2, 3, 4\n[18] Akshay Dudhane, Syed Waqas Zamir, Salman Khan, Fahad\nKhan, and Ming-Hsuan Yang. Burst image restoration and\nenhancement. In CVPR, 2022. 2\n[19] William Fedus, Barret Zoph, and Noam Shazeer. Switch\ntransformers: Scaling to trillion parameter models with\nsimple and efﬁcient sparsity. arXiv:2101.03961, 2021. 1\n[20] Rich Franzen. Kodak lossless true color image suite.\nhttp://r0k.us/graphics/kodak/ , 1999. Online\naccessed 24 Oct 2021. 6, 7\n[21] Xueyang Fu, Jiabin Huang, Xinghao Ding, Yinghao Liao,\nand John Paisley. Clearing the skies: A deep network ar-\nchitecture for single-image rain removal. TIP, 2017. 5\n[22] Xueyang Fu, Jiabin Huang, Delu Zeng, Yue Huang, Xing-\nhao Ding, and John Paisley. Removing rain from single\nimages via a deep detail network. In CVPR, 2017. 5\n[23] Hongyun Gao, Xin Tao, Xiaoyong Shen, and Jiaya Jia. Dy-\nnamic scene deblurring with parameter selective sharing\nand nested skip connections. In CVPR, 2019. 5, 6\n[24] Shuhang Gu, Yawei Li, Luc Van Gool, and Radu Timofte.\nSelf-guided network for fast image denoising. In ICCV,\n2019. 2\n[25] Shi Guo, Zifei Yan, Kai Zhang, Wangmeng Zuo, and Lei\nZhang. Toward convolutional blind denoising of real pho-\ntographs. In CVPR, 2019. 7\n[26] Kaiming He, Jian Sun, and Xiaoou Tang. Single image haze\nremoval using dark channel prior. TPAMI, 2010. 2\n[27] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\nunits (GELUs). arXiv:1606.08415, 2016. 2, 4\n[28] Elad Hoffer, Berry Weinstein, Itay Hubara, Tal Ben-Nun,\nTorsten Hoeﬂer, and Daniel Soudry. Mix & match: training\nconvnets with mixed image sizes for improved accuracy,\nspeed and scale resiliency. arXiv:1908.08986, 2019. 8\n[29] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja.\nSingle image super-resolution from transformed self-\nexemplars. In CVPR, 2015. 6, 7, 8\n[30] Andrey Ignatov and Radu Timofte. NTIRE 2019 challenge\non image enhancement: Methods and results. In CVPR\nWorkshops, 2019. 2\n[31] Xixi Jia, Sanyang Liu, Xiangchu Feng, and Lei Zhang. Foc-\nnet: A fractional optimal control network for image denois-\ning. In CVPR, 2019. 7\n[32] Kui Jiang, Zhongyuan Wang, Peng Yi, Baojin Huang,\nYimin Luo, Jiayi Ma, and Junjun Jiang. Multi-scale pro-\ngressive fusion network for single image deraining. In\nCVPR, 2020. 1, 5\n[33] Ali Karaali and Claudio Rosito Jung. Edge-based defocus\nblur estimation with adaptive scale selection. TIP, 2017. 6\n[34] Salman Khan, Muzammal Naseer, Munawar Hayat,\nSyed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak\nShah. Transformers in vision: A survey.arXiv:2101.01169,\n2021. 1, 2\n9\n[35] Yoonsik Kim, Jae Woong Soh, Gu Yong Park, and Nam Ik\nCho. Transfer learning from synthetic to real-noise denois-\ning with adaptive instance normalization. In CVPR, 2020.\n7\n[36] Johannes Kopf, Boris Neubert, Billy Chen, Michael Cohen,\nDaniel Cohen-Or, Oliver Deussen, Matt Uyttendaele, and\nDani Lischinski. Deep photo: Model-based photograph en-\nhancement and viewing. ACM TOG, 2008. 2\n[37] Manoj Kumar, Dirk Weissenborn, and Nal Kalchbrenner.\nColorization transformer. In ICLR, 2021. 2\n[38] Orest Kupyn, V olodymyr Budzan, Mykola Mykhailych,\nDmytro Mishkin, and Ji ˇr´ı Matas. DeblurGAN: Blind mo-\ntion deblurring using conditional adversarial networks. In\nCVPR, 2018. 5\n[39] Orest Kupyn, Tetiana Martyniuk, Junru Wu, and\nZhangyang Wang. DeblurGAN-v2: Deblurring (orders-of-\nmagnitude) faster and better. In ICCV, 2019. 2, 5\n[40] Junyong Lee, Sungkil Lee, Sunghyun Cho, and Seungyong\nLee. Deep defocus map estimation using domain adapta-\ntion. In CVPR, 2019. 6\n[41] Junyong Lee, Hyeongseok Son, Jaesung Rim, Sunghyun\nCho, and Seungyong Lee. Iterative ﬁlter adaptive network\nfor single image defocus deblurring. In CVPR, 2021. 6\n[42] Siyuan Li, Iago Breno Araujo, Wenqi Ren, Zhangyang\nWang, Eric K Tokuda, Roberto Hirata Junior, Roberto\nCesar-Junior, Jiawan Zhang, Xiaojie Guo, and Xiaochun\nCao. Single image deraining: A comprehensive benchmark\nanalysis. In CVPR, 2019. 2\n[43] Xia Li, Jianlong Wu, Zhouchen Lin, Hong Liu, and Hong-\nbin Zha. Recurrent squeeze-and-excitation context aggre-\ngation net for single image deraining. In ECCV, 2018. 1, 2,\n5\n[44] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc\nVan Gool, and Radu Timofte. SwinIR: Image restoration\nusing swin transformer. In ICCV Workshops, 2021. 1, 2, 3,\n7, 8\n[45] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and\nKyoung Mu Lee. Enhanced deep residual networks for sin-\ngle image super-resolution. In CVPR Workshops, 2017. 8\n[46] Ding Liu, Bihan Wen, Yuchen Fan, Chen Change Loy, and\nThomas S Huang. Non-local recurrent network for image\nrestoration. In NeurIPS, 2018. 7\n[47] Pengju Liu, Hongzhi Zhang, Kai Zhang, Liang Lin, and\nWangmeng Zuo. Multi-level wavelet-cnn for image restora-\ntion. In CVPR Workshops, 2018. 7\n[48] Xing Liu, Masanori Suganuma, Zhun Sun, and Takayuki\nOkatani. Dual residual networks leveraging the potential of\npaired operations for image restoration. In CVPR, 2019. 2\n[49] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-\nmoyer, and Veselin Stoyanov. RoBERTa: A robustly opti-\nmized bert pretraining approach. arXiv:1907.11692, 2019.\n1\n[50] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted win-\ndows. arXiv:2103.14030, 2021. 2\n[51] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gra-\ndient descent with warm restarts. In ICLR, 2017. 5\n[52] David Martin, Charless Fowlkes, Doron Tal, and Jitendra\nMalik. A database of human segmented natural images and\nits application to evaluating segmentation algorithms and\nmeasuring ecological statistics. In ICCV, 2001. 6, 7\n[53] Tomer Michaeli and Michal Irani. Nonparametric blind\nsuper-resolution. In ICCV, 2013. 2\n[54] Sreyas Mohan, Zahra Kadkhodaie, Eero P Simoncelli, and\nCarlos Fernandez-Granda. Robust and interpretable blind\nimage denoising via bias-free convolutional neural net-\nworks. In ICLR, 2019. 7\n[55] Chong Mou, Jian Zhang, and Zhuoyuan Wu. Dynamic at-\ntentive graph learning for image restoration. InICCV, 2021.\n1, 7\n[56] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep\nmulti-scale convolutional neural network for dynamic scene\ndeblurring. In CVPR, 2017. 5, 6\n[57] Seungjun Nah, Sanghyun Son, Suyoung Lee, Radu Timo-\nfte, and Kyoung Mu Lee. Ntire 2021 challenge on image\ndeblurring. In CVPR Workshops, 2021. 2\n[58] Dongwon Park, Dong Un Kang, Jisoo Kim, and Se Young\nChun. Multi-temporal recurrent neural networks for pro-\ngressive non-uniform single image deblurring with incre-\nmental temporal training. In ECCV, 2020. 1, 5, 6\n[59] Yali Peng, Lu Zhang, Shigang Liu, Xiaojun Wu, Yu Zhang,\nand Xili Wang. Dilated residual networks with symmet-\nric skip connection for image denoising. Neurocomputing,\n2019. 7\n[60] Tobias Plotz and Stefan Roth. Benchmarking denoising al-\ngorithms with real photographs. In CVPR, 2017. 7\n[61] Kuldeep Purohit, Maitreya Suin, AN Rajagopalan, and\nVishnu Naresh Boddeti. Spatially-adaptive image restora-\ntion using distortion-guided networks. In ICCV, 2021. 5\n[62] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. Improving language understanding by genera-\ntive pre-training. Technical report, OpenAI, 2018. 1\n[63] Chao Ren, Xiaohai He, Chuncheng Wang, and Zhibo Zhao.\nAdaptive consistency prior based deep network for image\ndenoising. In CVPR, 2021. 1, 7\n[64] Dongwei Ren, Wangmeng Zuo, Qinghua Hu, Pengfei Zhu,\nand Deyu Meng. Progressive image deraining networks: A\nbetter and simpler baseline. In CVPR, 2019. 1, 5\n[65] Jaesung Rim, Haeyun Lee, Jucheol Won, and Sunghyun\nCho. Real-world blur dataset for learning and benchmark-\ning deblurring algorithms. In ECCV, 2020. 5\n[66] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nNet: convolutional networks for biomedical image segmen-\ntation. In MICCAI, 2015. 3\n[67] Ziyi Shen, Wenguan Wang, Xiankai Lu, Jianbing Shen,\nHaibin Ling, Tingfa Xu, and Ling Shao. Human-aware mo-\ntion deblurring. In ICCV, 2019. 5\n[68] Jianping Shi, Li Xu, and Jiaya Jia. Just noticeable defocus\nblur detection and estimation. In CVPR, 2015. 6\n[69] Wenzhe Shi, Jose Caballero, Ferenc Husz ´ar, Johannes Totz,\nAndrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan\nWang. Real-time single image and video super-resolution\n10\nusing an efﬁcient sub-pixel convolutional neural network.\nIn CVPR, 2016. 3\n[70] Hyeongseok Son, Junyong Lee, Sunghyun Cho, and Se-\nungyong Lee. Single image defocus deblurring using\nkernel-sharing parallel atrous convolutions. In ICCV, 2021.\n6\n[71] Maitreya Suin, Kuldeep Purohit, and A. N. Rajagopalan.\nSpatially-attentive patch-hierarchical network for adaptive\nmotion deblurring. In CVPR, 2020. 5, 6\n[72] Xin Tao, Hongyun Gao, Xiaoyong Shen, Jue Wang, and Ji-\naya Jia. Scale-recurrent network for deep image deblurring.\nIn CVPR, 2018. 5\n[73] Chunwei Tian, Lunke Fei, Wenxian Zheng, Yong Xu,\nWangmeng Zuo, and Chia-Wen Lin. Deep learning on im-\nage denoising: An overview. Neural Networks, 2020. 2\n[74] Chunwei Tian, Yong Xu, and Wangmeng Zuo. Image de-\nnoising using deep cnn with batch renormalization. Neural\nNetworks, 2020. 7\n[75] Radu Timofte, Vincent De Smet, and Luc Van Gool. An-\nchored neighborhood regression for fast example-based\nsuper-resolution. In ICCV, 2013. 2\n[76] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J ´egou. Train-\ning data-efﬁcient image transformers & distillation through\nattention. In ICML, 2021. 1, 2\n[77] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In NeurIPS,\n2017. 1, 2, 3, 4, 8\n[78] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\nPyramid vision transformer: A versatile backbone for dense\nprediction without convolutions. In ICCV, 2021. 1, 2\n[79] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In CVPR, 2018. 1\n[80] Zhendong Wang, Xiaodong Cun, Jianmin Bao, and\nJianzhuang Liu. Uformer: A general u-shaped transformer\nfor image restoration. arXiv:2106.03106, 2021. 1, 2, 3, 6,\n7, 8\n[81] Wei Wei, Deyu Meng, Qian Zhao, Zongben Xu, and Ying\nWu. Semi-supervised transfer learning for image rain re-\nmoval. In CVPR, 2019. 5\n[82] Zhihao Xia and Ayan Chakrabarti. Identifying recurring\npatterns with deep neural networks for natural image de-\nnoising. In WACV, 2020. 7\n[83] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,\nJose M Alvarez, and Ping Luo. Segformer: Simple and ef-\nﬁcient design for semantic segmentation with transformers.\narXiv:2105.15203, 2021. 2\n[84] Li Xu, Shicheng Zheng, and Jiaya Jia. Unnatural l0 sparse\nrepresentation for natural image deblurring. In CVPR,\n2013. 5\n[85] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and\nBaining Guo. Learning texture transformer network for im-\nage super-resolution. In CVPR, 2020. 2\n[86] Wenhan Yang, Robby T Tan, Jiashi Feng, Jiaying Liu,\nZongming Guo, and Shuicheng Yan. Deep joint rain de-\ntection and removal from a single image. In CVPR, 2017.\n5\n[87] Rajeev Yasarla and Vishal M Patel. Uncertainty guided\nmulti-scale residual learning-using a cycle spinning cnn for\nsingle image de-raining. In CVPR, 2019. 5\n[88] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nZihang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-token vit: Training vision transformers\nfrom scratch on imagenet. arXiv:2101.11986, 2021. 2\n[89] Zongsheng Yue, Hongwei Yong, Qian Zhao, Deyu Meng,\nand Lei Zhang. Variational denoising network: Toward\nblind noise modeling and removal. In NeurIPS, 2019. 7\n[90] Zongsheng Yue, Qian Zhao, Lei Zhang, and Deyu Meng.\nDual adversarial network: Toward real-world noise removal\nand noise generation. In ECCV, 2020. 2, 7\n[91] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar\nHayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling\nShao. CycleISP: Real image restoration via improved data\nsynthesis. In CVPR, 2020. 7\n[92] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar\nHayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling\nShao. Learning enriched features for real image restoration\nand enhancement. In ECCV, 2020. 1, 2, 7, 8\n[93] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar\nHayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling\nShao. Multi-stage progressive image restoration. In CVPR,\n2021. 1, 2, 5, 6, 7\n[94] Hongguang Zhang, Yuchao Dai, Hongdong Li, and Piotr\nKoniusz. Deep stacked hierarchical multi-patch network\nfor image deblurring. In CVPR, 2019. 1, 5, 6\n[95] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augus-\ntus Odena. Self-attention generative adversarial networks.\nIn ICML, 2019. 1\n[96] He Zhang and Vishal M Patel. Density-aware single image\nde-raining using a multi-stream dense network. In CVPR,\n2018. 5\n[97] He Zhang, Vishwanath Sindagi, and Vishal M Patel. Image\nde-raining using a conditional generative adversarial net-\nwork. TCSVT, 2019. 5\n[98] Jiawei Zhang, Jinshan Pan, Jimmy Ren, Yibing Song, Lin-\nchao Bao, Rynson WH Lau, and Ming-Hsuan Yang. Dy-\nnamic scene deblurring using spatially variant recurrent\nneural networks. In CVPR, 2018. 5\n[99] Kai Zhang, Yawei Li, Wangmeng Zuo, Lei Zhang, Luc\nVan Gool, and Radu Timofte. Plug-and-play image restora-\ntion with deep denoiser prior. TPAMI, 2021. 1, 2, 7, 8\n[100] Kaihao Zhang, Wenhan Luo, Yiran Zhong, Lin Ma, Bjorn\nStenger, Wei Liu, and Hongdong Li. Deblurring by realistic\nblurring. In CVPR, 2020. 1, 5, 6\n[101] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and\nLei Zhang. Beyond a gaussian denoiser: Residual learning\nof deep cnn for image denoising. TIP, 2017. 6, 7\n[102] Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang.\nLearning deep cnn denoiser prior for image restoration. In\nCVPR, 2017. 7\n[103] Kai Zhang, Wangmeng Zuo, and Lei Zhang. FFDNet: To-\nward a fast and ﬂexible solution for CNN-based image de-\nnoising. TIP, 2018. 7\n11\n[104] Lei Zhang, Xiaolin Wu, Antoni Buades, and Xin Li. Color\ndemosaicking by local directional interpolation and nonlo-\ncal adaptive thresholding. JEI, 2011. 6, 7\n[105] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng\nZhong, and Yun Fu. Image super-resolution using very deep\nresidual channel attention networks. In ECCV, 2018. 2\n[106] Yulun Zhang, Kunpeng Li, Kai Li, Bineng Zhong, and\nYun Fu. Residual non-local attention networks for image\nrestoration. In ICLR, 2019. 1, 2, 7\n[107] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and\nYun Fu. Residual dense network for image restoration.\nTPAMI, 2020. 2, 7\n[108] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao\nXiang, Philip HS Torr, et al. Rethinking semantic segmen-\ntation from a sequence-to-sequence perspective with trans-\nformers. In CVPR, 2021. 2\n[109] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. Deformable DETR: Deformable trans-\nformers for end-to-end object detection.arXiv:2010.04159,\n2020. 2\n12"
}