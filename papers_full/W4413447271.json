{
  "title": "Towards generalist foundation model for radiology by leveraging web-scale 2D&amp;3D medical data",
  "url": "https://openalex.org/W4413447271",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2290108068",
      "name": "Chaoyi Wu",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2107527362",
      "name": "Xiaoman Zhang",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2100362878",
      "name": "Ya Zhang",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Shanghai Jiao Tong University",
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2104299286",
      "name": "Hui Hui",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2098837876",
      "name": "Yanfeng Wang",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "Beijing Academy of Artificial Intelligence",
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2413558386",
      "name": "Weidi Xie",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Shanghai Jiao Tong University",
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2290108068",
      "name": "Chaoyi Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2107527362",
      "name": "Xiaoman Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100362878",
      "name": "Ya Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104299286",
      "name": "Hui Hui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098837876",
      "name": "Yanfeng Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2413558386",
      "name": "Weidi Xie",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6800751262",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4379259189",
    "https://openalex.org/W4365143687",
    "https://openalex.org/W6854691855",
    "https://openalex.org/W4392044798",
    "https://openalex.org/W4405673884",
    "https://openalex.org/W4296027312",
    "https://openalex.org/W4385347692",
    "https://openalex.org/W3024545783",
    "https://openalex.org/W4385546024",
    "https://openalex.org/W4385970122",
    "https://openalex.org/W4385572845",
    "https://openalex.org/W4403990404",
    "https://openalex.org/W4389471380",
    "https://openalex.org/W6772383348",
    "https://openalex.org/W6852800892",
    "https://openalex.org/W4225323055",
    "https://openalex.org/W4406152279",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4389116614",
    "https://openalex.org/W4366197770",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W4306820534",
    "https://openalex.org/W4387211014",
    "https://openalex.org/W2901125500",
    "https://openalex.org/W3165058054",
    "https://openalex.org/W4401397240",
    "https://openalex.org/W4391159111",
    "https://openalex.org/W4404782407",
    "https://openalex.org/W4395443743",
    "https://openalex.org/W4393248026",
    "https://openalex.org/W4392678661",
    "https://openalex.org/W4403853618",
    "https://openalex.org/W6854054124",
    "https://openalex.org/W2159583324",
    "https://openalex.org/W3134144764",
    "https://openalex.org/W4386768656",
    "https://openalex.org/W2912664121",
    "https://openalex.org/W4376631151",
    "https://openalex.org/W2611650229",
    "https://openalex.org/W4315628888",
    "https://openalex.org/W1986649315",
    "https://openalex.org/W3026637813",
    "https://openalex.org/W2122328291",
    "https://openalex.org/W2580596898",
    "https://openalex.org/W4385245495",
    "https://openalex.org/W2152772232",
    "https://openalex.org/W3151410070",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W4328120750",
    "https://openalex.org/W4387211791",
    "https://openalex.org/W4220700457",
    "https://openalex.org/W3101156210",
    "https://openalex.org/W4389894040",
    "https://openalex.org/W2995225687",
    "https://openalex.org/W2901466771"
  ],
  "abstract": null,
  "full_text": "Article https://doi.org/10.1038/s41467-025-62385-7\nTowards generalist foundation model for\nradiology by leveraging web-scale 2D&3D\nmedical data\nChaoyi Wu1,2,3, Xiaoman Zhang 1,2,3, Ya Zhang 1,2,H u iH u i1,\nYanfeng Wang 1,2 &W e i d iX i e1,2\nIn this study, as a proof-of-concept, we aim to initiate the development of\nRadiology Foundation Model, termed asRadFM. We consider three per-\nspectives: dataset construction, model design, and thorough evaluation,\nconcluded as follows: (i), we contribute 4 multimodal datasets with 13M 2D\nimages and 615K 3D scans. When combined with a vast collection of existing\ndatasets, this forms our training dataset, termed asMedical Multi-modal\nDataset,MedMD.( ii), we propose an architecture that enables to integrate text\ninput with 2D or 3D medical scans, and generates responses for diverse radi-\nologic tasks, including diagnosis, visual question answering, report genera-\ntion, and rationale diagnosis; (iii), beyond evaluation on 9 existing datasets, we\npropose a new benchmark,RadBench, comprising three tasks aiming to\nassess foundation models comprehensively. We conduct both automatic and\nhuman evaluations on RadBench. RadFM outperforms former accessible\nmulti-modal foundation models, including GPT-4V. Additionally, we adapt\nRadFM for diverse public benchmarks, surpassing various existing SOTAs.\nGeneralist foundation models1, the latest generation of artiﬁcial intel-\nligence models pretrained on large-scale dataset, have demonstrated\nremarkable success in various domains, e.g., natural language pro-\ncessing, computer vision\n2,3. Their ability to address diverse and chal-\nlenging tasks has also attracted tremendous attention among\nresearchers in theﬁeld of AI for Medicine (AI4Medicine)4–8. Despite the\npromising clinical usage, developing medical foundation models has\nbeen fundamentally hindered by three challenges:\n Lack of multimodal datasets for training: medicine by its nature,\nrequires understanding multimodal data, spanning text (electro-\nnic health record, medical reports), 1D signals (ECG), 2D images\n(ultrasound, X-ray), 3D images (CT or MRI scans), genomics, and\nmore. To support the training of the medical generalist founda-\ntion model, a large-scale, diverse, multimodal dataset is despe-\nrately required;\n Lack of general architecture formulation: in the literature of\nAI4Medicine, various clinical tasks have largely been tackled by\nfollowing a divide-and-conquer paradigm, i.e., different architec-\ntures are designed for the problem of interest, like diagnosis\n9,10 or\nreport generation11,12. In contrast, developing a medical founda-\ntion model requires one general architecture that is capable of\ntackling a wide spectrum of clinical tasks, by fusing information\nfrom a mixture of different modalities;\n Lack of effective benchmark to monitor progress: benchmarking\nthe models’ clinical knowledge predominantly relies on task-\nspeciﬁc datasets with a limited number of testing cases. An high-\nquality benchmark is yet to be established, to comprehensively\nmeasure the progress of the development on medical foundation\nmodel across a wide range of clinical tasks.\nConsidering the abovementioned challenges, in this paper, we\ntake a preliminary, yet realistic step toward developing a generalist\nmedical foundation model for radiology, which has shown to play a\nvital role in clinical scenarios, for example, disease diagnosis,\nReceived: 12 April 2024\nAccepted: 17 July 2025\nCheck for updates\n1Shanghai Jiao Tong University, Shanghai, China.2Shanghai Artiﬁcial Intelligence Laboratory, Shanghai, China.3These authors contributed equally: Chaoyi\nWu, Xiaoman Zhang. e-mail: wangyanfeng622@sjtu.edu.cn; weidi@sjtu.edu.cn\nNature Communications|         (2025) 16:7866 1\n1234567890():,;\n1234567890():,;\ntreatment planning, and monitoring patient progression. Speciﬁcally,\nwe present our progress towards building aRadiology Foundation\nModel (RadFM), that aims to tackle a wide spectrum of clinical radi-\nology tasks, by learning from medical scans (X-ray, CT, MRI, PET, etc.)\nand corresponding text descriptions/reports.\nTo achieve this, as shown in Fig.1, we start by constructing four\nnovel medical multimodal datasets, by exploiting the highly specia-\nlised, high-quality radiological images on the Internet, where the\ndiagnosis labels have been extensively reviewed by a panel of experi-\nenced clinicians, namely, PMC-Inline, RP3D, PMC-CaseReport and\nMPx, consisting of 13M 2D and 615K 3D radiology scans. Additionally,\nwe combine a vast collection of existing datasets with our collections,\nresulting in a large-scaleMedical MultimodalDataset, namedMedMD,\nwith totally 16M 2D and 3D radiology scans, accompanied with high-\nquality textual descriptions, for example, radiology reports, visual-\nlanguage instruction, or crucial disease diagnosis labels. MedMD\nencompasses a wide range of radiological modalities, covering 17\nmedical systems, e.g., breast, cardiac, central nervous system, chest,\ngastrointestinal, gynecology, hematology, head and neck, hepatobili-\nary, musculoskeletal, obstetrics, oncology, pediatrics, spine, trauma,\nurogenital and vascular featuring over 5000 diseases, thus potentially\nserving as the cornerstone for developing foundation models in\nradiology.\nArchitecturally, RadFM refers to a visually conditioned auto-\nregressive text generation model, that enables seamless integration of\nnatural language with 2D or 3D medical scans, and address a wide\nrange of medical tasks with natural language as output. The proposed\nmodel is initially pretrained on the largeMedMD dataset, and subse-\nquently trained with domain-speciﬁc visual instruction tuning on a\nﬁltered radiology subset, comprising 3M meticulously curated multi-\nmodal samples with only radiologic cases, termed asRadMD,e n s u r i n g\na high-quality and reliable dataset.\nTo monitor the developmental progress of the foundation model\nfor radiology, in addition to using the existing benchmarks, we also\nestablish a comprehensive evaluation benchmark, spanning various\nclinical task types, termed asRadBench, covering a variety of clinical\ntasks, for example, report generation, and visual question-answering\non radiologic modalities and anatomical regions. All samples in Rad-\nBench have undergone meticulous manual veriﬁcation to ensure data\nquality. We conduct both automatic and human evaluation on\nI\u0014\n/TV[Z <OY[GR YIGTY\u0003\u000e\u0019* UX \u0018*\u000f OTZKXRKG\\OTM ]OZN ZK^ZY\n<OY[GR\n+TIUJKX\n6KXIKO\\KX\n:K^Z 5[ZV[Z\n2GXMK 2GTM[GMK\u00033UJKR\n\u0017\u0013YZ :XGTYLUXSKX *KIUJKX (RUIQ\nT\u0013ZN :XGTYLUXSKX *KIUJKX (RUIQ\nȜ\n63)\u0013<7'\n3/3/)\u0013)>83KJ3* \f 8GJ3*\n63)\u0013/TROTK63)\u0013)GYK8KVUXZ\n86\u0019*36^\n5ZNKX +^OYZOTM *GZGYKZY\n63)\u00135'\n4/. )NKYZ>XG_\u0017\u001a\nG\u0014 H\u0014\n3KJ3*\n6XK\u0013ZXGOTOTM\n\u000e\u0017\u001c3\u000f8GJ3*\n*USGOT\u0013YVKIOLOI\u0003\n/TYZX[IZOUT :[TOTM\n\u000e\u00193\u000f\n0\n1\n2\n3\n4\n5\nMedical VQA Report Generation Rationale Diagnosis Average\nScore\nHuman Rating\nOpenFlamingo\nMedVInT\nMedFlamingo\nGPT-4V\nRadFM\nJ\u0014\nFig. 1 | The mainﬁgure demonstrates our contributions. aDataset demonstra-\ntion. The colored datasets are constructed by us in this paper.b Our training\nprocedure. For better radiologic performance, weﬁrst pre-train our model on the\nwhole medical domain with 16M scans (MedMD), thenﬁnetune on a cleaned\ndataset with 3M radiologic scans (RadMD).c RadFM architecture. Our architecture\nenables multi-image input interleaving with texts 2D or 3D images.d The human\nrating comparison ofﬁve foundation models under three open-ended task types\n(medical VQA, report generation, and rationale diagnosis), adding GPT-4V14 into\ncomparison. All evaluations have shown the superiority of RadFM.\nArticle https://doi.org/10.1038/s41467-025-62385-7\nNature Communications|         (2025) 16:7866 2\nRadBench with powerful models that are publicly accessible, for\nexample, Open-ﬂamingo13,M e d V I n T8, LLaVA-Med4,M e d F l a m i n g o6,\nand GPT-4V14, and observe signiﬁcant beneﬁts across all considered\ntasks. In addition, we perform task-speciﬁc ﬁnetuning of RadFM on\nseveral public benchmarks, demonstrating its strong ability for\ntransfer.\nOverall, in this work, we demonstrate a preliminary attempt for\nbuilding a generalist foundation model for radiology, by making con-\ntributions from three key aspects: four new large-scale medical mul-\ntimodal datasets, covering both 2D and 3D medical images, namely,\nPMC-Inline, RP3D, PMC-CaseReport, and MPx, a superior radiology\nfoundation model (RadFM), and a comprehensive benchmark for\nradiology (RadBench).\nResults\nIn this section, we start by presenting evaluation results on nine public\ndatasets, comparing them to the existing medical multi-modal foun-\ndation models, together with ablation results on our training proce-\ndure and our newly collected dataset from the Internet. Considering\nexisting medical datasets cannot comprehensively cover all medical\ntasks, we further report the results on our proposed RadBench, with\nthree medical tasks, namely, medical VQA, report generation, and\nrationale diagnosis, as demonstrated in Fig.2.\nWe start by comparing the zero-shot prompting results for RadFM\nwith other foundation model baselines (both zero-shot and few-shot\nsettings). Following that,w ep e r f o r mt a s k - s p e c iﬁc ﬁnetuning experi-\nments to thoroughly evaluate the performance of our model with dif-\nferent state-of-the-art task-speciﬁc models. Additionally, to evaluate the\nmodel’s generalization ability, we employed a zero-shot evaluation on\nthe unseen classes in the PadChest dataset. It is worth noting that the\nresults on PadChest have not undergone any task-speciﬁc ﬁnetuning.\nResults on existing benchmarks\nWe compare our model with other foundation models on nine existing\nbenchmarks, e.g., VinDr-Mammo, VinDr-SpineXr, VinDr-PCXR, CXR-\nMix, RadChest-CT, PMC-VQA, VQA-RAD, SLAKE, and MIMIC-CXR,\ncovering tasks like diagnosis, medical VQA, and report generation, as\nshown in Table1. In detail, we compare with the Open-ﬂamingo\n13,\nMedVInT8, LLaVA-Med4, and MedFlamingo6.F o rt h eﬂamingo-series,\nwe adopt the few-shot (three-shot) prompting setting, as the models\nare supposed to demonstrate better performance under a few-shot\nscenario, while for MedVInT, LLaVA-Med, and our RadFM, we adopt a\nzero-shot prompting strategy, as both the two are trained to follow\nsemantic instructions rather than few-shot samples. The zero-shot\nresults for ﬂamingo-series are also included in the Supplementary\nTable 2.\nComparison with other foundation models. As shown by the results\nin Table 1,o u rﬁnal model shows superior results on nine publicly\navailable datasets. First, for disease diagnosis, existing foundation\nmodels perform poorly, with an accuracy score (ACC) of nearly 50%.\nConsidering that we prompt the problem with a judgment format, i.e.,\n“Does the patient have disease?”, this score is nearly random. In con-\ntrast, our proposed RadFM, exhibits evaluation results, with 59.96,\n68.82, 56.32, 83.62, and 72.95% ACC scores on theﬁve diagnosis\ndatasets, respectively. Second, for other long sentence generation\ntasks, i.e., medical VQA and report generation, RadFM also surpasses\nother models signiﬁcantly on most metrics.\nAblation studies.I nT a b l e1, we also carry out ablation studies on our\nmethods. First, we dismiss the domain-speciﬁc instruction tuning on\nRadMD. Similar to the observation in language domain\n15– 17,w eﬁnd that\ndomain-speciﬁc instruction tuning is a critical step for building up a\nFig. 2 | Examples of inputs and outputs of three different evaluation tasks obtained from RadFM.The ﬁgure shows input prompts and corresponding RadFM outputs\nfor Medical VQA (top), Radiology Report Generation (middle), and Rationale Diagnosis (bottom).\nArticle https://doi.org/10.1038/s41467-025-62385-7\nNature Communications|         (2025) 16:7866 3\nTable 1 | Comparison of our proposed RadFM with other foundation models on nine existing datasets, together with ablation studies\nDataset Metric OpenFlamingo (Few-shot) MedVInT LLaVA-Med MedFla mingo (Few-shot) RadFM (w/o Ins-tuning) RadFM (w/o Our Data) RadFM\nDisease diagnosis\nVinDr-Mammo ACC 49.92 (48.20, 51.65) 50.06 (48.52, 51.59) 50.27 (49.20 , 51.52) 49.80 (48.33, 51.42) 49.80 (48.15,51.53) 55.35 (54.35,559.47) 59.96 (58.41, 61.59)\nF1 57.01 (54.64, 60.08) 66.56 (65.2, 67.93) 56.48 (55.45, 57.73) 64.92 (63.52, 66.32) 60.32 (58.25 ,62.31) 60.57 (58.75,62.58) 62.11 (60.09, 63.75)\nVinDr-SpineXr ACC 50.33 (47.13, 53.53) 49.93 (46.99, 52.86) 49.85 (47.95, 52.23) 49.61 (46.05, 53.16) 52.19 (49.18,55.17) 64.43 (61.76,67.22) 68.82 (65.92, 71.47)\nF1 31.79 (26.99, 36.58) 62.32 (59.38, 65.25) 54.83 (51.88, 57.45) 63.23 (59.74, 66.74) 34.19 (30.17,38.09) 65.86 (62.62,68.81) 67.69 (64.5, 70.98)\nVinDr-PCXR ACC 49.85 (45.40, 54.31) 50.29 (45.88, 54.69) 49.62 (45.79, 53.64) 49.37 (44.44, 54.31) 50.12 (45.21, 54.60) 51.82 (46.46, 57.09) 56.32 (51.82, 61.21)\nF1 41.44 (33.77, 49.10) 66.29 (62.36, 70.23) 47.81 (42.33, 53.42) 66.94 (62.57, 71.32) 43.33 (40.37, 40.88) 49.14 (43.66, 56.18) 37.53 (28.88, 43.67)\nCXR-Mix ACC 50.63 (50.07, 51.03) 49.2 (48.53, 49.88) 53.26 (52.72, 53 .91) 50.00 (49.50, 50.51) 77.71 (77. 25, 77.95) 78.63 (78.51, 79.10) 83.62 (83.23, 83.97)\nF1 24.83 (24.11, 25.54) 67.22 (66.62, 67.82) 22.63 (22.70, 24.53) 66.11 (65.72, 66.61) 74.42 (73.98, 75.01) 78.35 (77.85, 78.93) 82.99 (82.58, 83.49)\nRadChest-CT ACC 50.93 (49.13, 52.72) 50.07 (47.6 8, 52.45) 51.09 (50.05, 52.63) 50.39 (48.34, 52.43) 51.97 (50.05, 53.31) 69.72 (67.44, 71.53) 72.95 (71.06, 74.78)\nF1 43.49 (41.18, 45.99) 66.57 (64.45, 68.69) 44.42 (42.00, 46.55) 63.31 (61.39, 65.23) 38.67 (36.37, 41.46) 67.84 (65.64, 70.11) 71.86 (69.42, 83.49)\nMedical VQA\nPMC-VQA BLEU 11.10 (8.93, 13.41) 23.73 (21.03, 26.73) 13.66 (11.68, 15.52) 11.03 (9.27, 13.49) 5.23 (3.23, 8.84) 14.01 (10.92, 17.25) 17.99 (14.80, 20.83)\nROUGE 13.03 (10.63, 15.46) 27.24 (24.04, 30.91) 18.14 (16.46, 20.20) 13.06 (10.93, 15.66) 5.82 (2.03, 10.09) 14.23 (11.20, 17.66) 19.43 (16.56, 23.55)\nUMLS_Precision 7.60 (5.41, 10.83) 19 .64 (16.2, 23.59) 16.38 (12.67, 2 0.25) 6.45 (4.05, 8.97) 18.63 (14.84, 20.76) 13.24 (9.90, 17.02) 20.74 (17.39, 24.71)\nUMLS_Recall 7.56 (5.40, 10.51) 18.88 (15.51, 22.68) 13.34 (10.59, 16.07) 6.10 (4.04, 8.97) 15.03 (12.07, 18.34) 12.94 (9.39, 15.86) 14.14 (11.19, 17.37)\nBERT-Sim 52.08 (50.43, 54.07) 57.81 (55.49, 59.76) 42.46 (41.50, 43.44) 51.37 (49.57, 53.01) 47.85 (44.20, 49.37) 57.57 (55.85, 60.19) 63.85 (62.04, 65.94)\nVQA-RAD BLEU 33.98 (26.75, 41.85) 35.1 (28.44, 41.55) 31.55 (24.89, 38.35) 35.97 (29.14, 45.45) 22.03 (15.67, 30.38) 43.98 (36.58, 50.51) 52.24 (44.97, 59.43)\nROUGE 35.26 (28.21, 43.91) 39.2 (31.36, 46.33) 37.47 (30.83, 44.47) 38.64 (31.42, 48.23) 22.67 (14.92, 28.57) 44.70 (38.35, 50.81) 52.74 (45.39, 61.05)\nUMLS_Precision 14.72 (6.86, 24.22) 16 .46 (7.83, 25.93) 13.30 (12.14, 14. 50) 18.70 (8.76, 29.61) 60.30 (50 .88, 67.07) 61.52 (53.65, 69.51) 62.12 (54.01, 71.12)\nUMLS_Recall 14.52 (7.63, 23.33) 15.94 (7.72, 25.48) 12.16 (10.09, 1 3.93) 17.46 (8.76, 27.85) 39.43 (32.59, 47.12) 41.14 (34.49, 48.76) 42.82 (32.31, 51.54)\nBERT-Sim 71.49 (67.63, 74.96) 71.39 (66.94, 75.46) 68.28 (64.07, 72.00) 73.40 (69.62, 77. 32) 58.88 (56.74, 61.08) 80.64 (77.55, 83.89) 81.52 (77.41, 85.17)\nS L A K E B L E U 2 7 . 1 6( 2 2 . 0 1 ,3 2 . 5 6 ) 2 4 . 8 1( 2 0 . 2 3 ,3 0 . 5 2 ) 2 1 . 4 3( 1 7 . 0 7 ,2 5 . 3 5 ) 2 3 . 6 2( 1 8 . 0 6 ,2 8 . 2 6 ) 2 4 . 3 9( 1 5 . 8 1 ,3 0 . 7 4 ) 6 7 . 4 4( 6 3 . 7 4 ,7 1 . 6 8 )78.56 (72.2, 83.28)\nROUGE 29.36 (24.23, 34.73) 29.08 (24.06, 34.8) 29.92 (25.31, 34.09) 24.86 (19.47, 29.94) 24.81 (16.93, 30.59) 67.90 (63.58, 74.28) 79.42 (75.15, 84.05)\nUMLS_Precision 23.02 (17.52, 30.73) 23.32 (18.08, 29.42) 23.14 (18.29, 28.86) 18.28 (13.23, 23.38) 68.87 ( 64.43, 73.27) 76.09 (71.63, 80.21) 81.5 (76.81, 86.87)\nUMLS_Recall 22.71 (17.48, 29.53) 23.74 (18, 30.08) 23.31 (18.29, 27 .98) 19.21 (13.38, 24.37) 57.38 (52.49, 63.66) 72.04 (67.59, 76.36) 74.42 (66.7, 81.19)\nBERT-Sim 69.42 (66.09, 72.04) 67.7 (64.94, 70.69) 69.14 (66.53, 70.92) 66.93 (63.98, 70.32) 62.35 (61.15, 63.66) 90.93 (89.46, 92.30) 93.30 (90.99, 95.60)\nReport generation\nMIMIC-CXR BLEU 23.79 (22.62, 24.86) 0.04 (0.01, 0.08) 11.29 (9.92, 12.86) 22.65 (20.93, 24.06) 11.06 (8.36, 14.43) 20.63 (17.16, 25.43) 19.43 (16.12, 23.25)\nROUGE 35.83 (33.7, 37.96) 2.69 (2.26, 3.15) 13.91 (12.63, 15.29) 27.29 (25.63, 29.04) 15.05 (12.72, 19.54) 25.42 (21.89, 29.47) 26.18 (23.07, 29.86)\nUMLS_Precision 16.75 (15.74, 17.88) 26.67 (11.1 9, 42.12) 10.50 (8.42, 12.88) 22.36 (20.13, 24. 33) 21.80 (19.26, 24.29) 43.64 (36.96, 49.45) 45.51 (40.47, 52.77)\nUMLS_Recall 24.93 (22.86, 27.38) 0.52 (0.2, 0.88) 10.71 (8.37, 13.85) 19.64 (17.89, 21.43) 15.97 (12.92, 18.48) 22.73 (19.64, 26.57) 23.39 (20.18, 27.53)\nBERT-Sim 65.91 (65.20, 66.70) 34.48 (32.69, 36.02) 49.20 (48.22, 50 .35) 66.03 (65.37, 66.83) 63.13 (61.31, 64.87) 64.22 (61.74, 65.97) 66.77 (64.87, 68.58)\nWe adopt a few-shot prompting setting forﬂamingo-liked models, while we adopt a zero-shot instruction prompting strategy for MedVInT, LLaVA-Med and RadFM.“W/o Ins-tuning” denotes training without the domain-speciﬁc instruction tuning, and“w/o Our\nData” denotes training without all our newly collected data, i.e., using the combination of existing datasets only. ACC, F1, BLEU, ROUGE, UMLS_Precision,UMLS_Recall and BERT-Sim are reported based on task types, and the metrics refer to the average score on all\ntest samples. Numbers within parentheses indicate 95% CI. Percentage (%) signs have been omitted in the table.\nArticle https://doi.org/10.1038/s41467-025-62385-7\nNature Communications|         (2025) 16:7866 4\nmulti-modal foundation model that can rule various medical tasks with\nproper instruction prompting. As shown by the results, without\ndomain-speciﬁc instruction tuning on RadMD, the model can hardly\nrespond correctly to diverse task instructions. Moreover, we evaluate\nthe effectiveness of our newly collected data, i.e., PMC-Inline, RP3D,\nPMC-CaseReport, and MPx, by comparing the model trained with and\nwithout them for auto-regressive pretraining and domain-speciﬁc\ninstruction tuning. As shown by the results, adding our new-collected\ndata can signiﬁcantly improve theﬁnal results regardless of task types,\nunderscoring that, our newly collected datasets, though sourced from\nInternet, are effective for improving model performance on existing\nclinical datasets.\nBeyond the data-centric ablation studies, we also conducted an\nadditional series of experiments to demonstrate the effectiveness of\nour training design, as shown in Supplementary Table 3. Given the\ncomputational cost of testing these settings across the entire training\ndataset, we randomly sampled 10% of the RadMD dataset (using a 0.1\nsample ratio) for these experiments. We used the default setup of our\nmethod— speciﬁcally, a 3D ViT model with a patch size of 32 and the\nprompting strategy described in the Methods section— and system-\natically varied the following factors: the image-encoder architecture,\nthe patch size, and the prompting strategy.\nSpeciﬁcally, we vary the following factors: (i) to investigate the\nimpact of the image-encoder architecture, we experimented with\nmore recent models, such as those proposed in ref.18 and ref.19, (ii)\nwe test the effects of increasing the patch size beyond 32, (iii) we\nexperiment with more complex prompts. For example, we tested\nprompts with added role-play elements, such as“Assuming you are an\nexperienced radiologist reading modality images, please interpret the\nfollowing images and answer the related questions. This is a serious\nclinical case, so please be as careful and disciplined as possible ques-\ntion,” as well as incorporating varied, synonymous prompts generated\nby GPT-4 and veriﬁed by human reviewers.\nThe results, summarized in Supplementary Table 3, show that\nmodifying the image-encoder architecture produced some slight gains\non certain datasets, but these improvements were not statistically\nsigniﬁcant. Given that the focus of our work is not on architectural\ndesign, we chose to retain the classic 3D ViT architecture, which offers\na solid baseline. When we tested the impact of increasing the patch\nsize, we observed that larger patch sizes beyond 32 actually led to a\ndegradation in performance. Notably, here, using smaller patch sizes\nmight be better but they will bring unacceptable computational cost\ndue to the increment of token number in ViT, this conﬁrms our deci-\nsion to use a patch size of 32. Lastly, variations in prompt complexity,\nincluding the use of role-playing or diverse synonymous prompts, had\nlittle to no effect on the model’s performance.\nThese additional ablation studies provide further validation for\nthe choices we made in model architecture, patch size, and prompt\ndesign, supporting the robustness of our method.\nResults on RadBench\nIn this section, we further assess the long sentence generation ability\nfor different models on our proposed benchmarks, which compensate\nfor three medical tasks, i.e., medical VQA, report generation, and\nrationale diagnosis.\nMedical visual question answering (VQA). Medical VQA denotes a\ncomprehensive and versatile challenge in theﬁeld of medical image\nanalysis. In a clinical setting, patients and radiologists may pose a wide\nvariety of questions related to medical images, ranging from simple\ninquiries about image modality to more complex reasoning queries. In\ncontrast to the aforementioned existing medical VQA datasets, on\nRadBench, the image input is more close to a clinical scenario with 3D\nscan input.\nA ss h o w ni nT a b l e2, RadFM generally demonstrates superior\nperformance. Compared to the second best model, MedVInT, which\nwas speciﬁcally trained on visual question answering, despite achiev-\ning better results on its in-domain PMC-VQA test set, its generalization\nto real 3D scans is relatively poor, even though the task is still medical\nvisual question answering. MedVInT struggles with real 3D medical\nscans, which require a model capturing the information from an extra\nimage dimension. In contrast, our RadFM model shows a substantial\nimprovement in UMLS_Precision from 20.12 to 31.77% and UMLS_Re-\ncall from 15.82 to 24.93% across the whole test set, demonstrating its\nproﬁciency to comprehensively understand the given textual infor-\nmation andﬂexible adaptation to various complex clinical scenarios.\nReport generation. Report generation is a crucial and prominent use\ncase for generative medical foundational models. Unlike Medical VQA,\nthis application generally requires the model to emphasize clinically\nsigniﬁcant observations based on the image. Considering that, current\nreport generation benchmarks are all concentrated on X-ray; in Rad-\nBench, we focus more on testing the report generation ability for other\nimaging modalities. As shown in Table2,R a d F Ms h o w ss i g n iﬁcant\nimprovement over existing models, across various metrics, particu-\nlarly in relation to medical-speciﬁc terminology. For instance, RadFM\nimproves UMLS_Precision from 9.61 to 22.49%, and UMLS_Recall from\n3.66 to 12.07% in the zero-shot setting.\nRationale diagnosis. In addition to basic diagnosis, the ability to\nscrutinize diagnostic prediction outcomes is crucial, particularly in\nlight of the stringent demands for precision and interpretability within\nmedical contexts. Thus, on RadBench, we further evaluate the ability to\ngenerate diagnosis rationale sentences for different models. Much like\nreport generation, this task also requires proﬁciency in generat-\ning supplementary paragraphs and a comprehensive understanding\non medical knowledge.\nAs indicated in Table2, RadFM is the only model that can effec-\ntively respond on this task, outperforming other models on BLEU and\nROUGE scores by 16.50 and 12.87%, respectively, even comparing with\nthe few-shot case. Moreover, it exhibits signiﬁcant improvements in\nUMLS_Precision and UMLS_Recall scores, showcasing advancements\nof 21.91 and 16.18%, respectively.\nHuman rating. In Fig.3b, we show the human rating results on the\nt h r e eg e n e r a t i v et a s k sf o ra l lm o d e l s .W ec h o o s eO p e n F l a m i n g ot o\ndenote the performance of the general-domain multimodal founda-\ntion models, MedVInT for zero-shot-based medical multimodal foun-\ndation models, MedFlamingo for few-shot-ones, and GPT-4V for best\nclose-sources multimodal foundation models. As shown on the left of\nthe ﬁgure, RadFM achieves higher scores on all three generative-based\ntasks compared with existing open-source models, only falling behind\nwith GPT-4V in rationale diagnosis. On the right, we further show the\nrelative comparison between RadFM and a certain model. In most\ncases, results from RadFM are preferred by human clinicians. It is\nworth highlighting that we also show the comparison between RadFM\nand GPT-4V(ison), which has been widely considered as the strongest\nfoundation model. As GPT-4V can only input up to four 2D pictures per\nquery, we thus ask the radiologists to pick out the most informative\nslices based on the reference’s answer from 3D volumes. With human\nprior, answering questions becomes easier than directly inputting\noriginal 3D volumes, which is used as the evaluation style for our\nmodel. Despite this, RadFM still surpasses GPT-4V in average scores.\nResults for task-speciﬁc ﬁnetunine\nIn Table3, we treat RadFM as a pretrained model and task-speciﬁc\nﬁnetune it on various downstream datasets. For diagnosis, we use the\nimage-encoder weights as initialization for both 2D and 3D imaging\nArticle https://doi.org/10.1038/s41467-025-62385-7\nNature Communications|         (2025) 16:7866 5\nTable 2 | Comparison of proposed RadFM with foundation model baselines on RadBench\nMetric OpenFlamingo 13 MedVInT8 LLaVA-Med4 Med-Flamingo6 OpenFlamingo13 (Few-shot) Med-Flamingo 6 (Few-shot) RadFM\nMedical VQA\nBLEU 6.42 (6.10, 6.67) 1.56 (1.31, 1.97) 21.23 ((20.65, 21.80) 15.55 (14.71, 16.44) 19.93 (18.73, 21.07) 18.68 (17.77, 19.78) 23.23 (22.16, 24.26)\nROUGE 28.97 (27.79, 30.12) 3.95 (3.54, 4.51) 25.19 (24.51, 25.95) 20.56 (19.63, 21.66) 26.27 (24.83, 27.55) 24.86 (23.86, 26.15) 30.88 (29.84, 32.16)\nUMLS_Precision 17.4 (16.2, 18.61) 8.62 (6.80, 10.33) 19.57 (18.84, 20 .34) 21.93 (20.48, 23.66) 22.28 (20. 42, 24.19) 19.42 (17.75, 21.03) 22.89 (21.02, 24.48)\nUMLS_Recall 19.78 (18.54, 20.92) 1.95 (1.47, 2.46) 18.07 (15.84, 20.36) 12.98 (11.86, 14.11) 17.19 (15.73, 18.62) 14.19 (12.84, 15.55) 17.80 (16.43, 19.08)\nBERT-Sim 46.17 (45.55, 46.66) 71.39 (66.94, 75.46) 60.12 (58.83, 61.26) 52.12 (51.37, 52.81) 58.24 (57.59, 58.97) 57.34 (56.64, 58.09) 72.13 (70.36, 74.23)\nReport generation\nBLEU 3.25 (2.24, 4.23) 1.73 (1.20,2.30) 8.89 (8.39, 10.37) 9.91 (9.40, 10.37) 1.94 (1.3, 2.71) 4.97 (4.53, 5.40) 10.21 (9.48, 11.03)\nROUGE 7.17 (5.26, 9.24) 4.72 (4.21,5.27) 14.21 (13.71, 14.83) 15.62 (14.96, 16.17) 3.59 (1.44, 5.47) 6.96 (6.32, 7.44) 15.51 (18.12, 19.98)\nUMLS_Precision 1.13 (0.19, 3.3) 9.61 (7.3 3,11.84) 6.86 (6.62, 7.14 ) 2.57 (2.14, 3.06) 0.77 (0 , 2.26) 2.00 (1.58, 2.51) 18.97 (18.12, 19.98)\nUMLS_Recall 1.35 (0.19, 3.3) 1.45 (0.95,1.95) 6.00 (5.39 , 5.78) 2.03 (1.63, 2.40) 0.71 (0, 2.10) 1.15 (0.87, 1.41) 9.32 (8.81, 9.89)\nBERT-Sim 37.18 (35.76, 38.45) 38.89 (38.18,39.58) 47.51 (47.12, 47 .99) 47.98 (47.6, 48.37) 36.01 (34.42, 37.10) 44.98 (44.26, 45.70) 56.78 (56.40, 57.22)\nRationale diagnosis\nBLEU 4.12 (3.63,4.88) 0.08 (0.02,0.19) 10.82 (10.29, 11.36) 7.65 (7.00,8.37) 18.10 (17.52,18.86) 17.15 (16.4,17.81) 34.60 (31.69,37.74)\nROUGE 4.56 (4.10,4.98) 0.67 (0.52,0.83) 14.32 (13.85, 14.82) 7.38 (6.69,7.90) 28.40 (26.88,29.63) 29.02 (27.60,30.28) 41.89 (39.20,44.77)\nUMLS_Precision 11.58 (8.49,14.90) 7.73 (1.69, 15.24) 7.73 (1.69,15.24) 6.01 (5.67, 6.34) 1 9.26 (18.14,20.51) 2 1.04 (19.73,22.19) 42.95 (39.59,46.22)\nUMLS_Recall 0.96 (0.66,1.29) 0.06 (0.01,0.20) 10.22 (8.82, 11.24) 2.17 (1.78,2.66) 16.68 (15.55,17.86) 16.89 (15.71,18.24) 33.07 (30.93,36.17)\nBERT-Sim 39.20 (38.55,40.02) 29.14 (28.48,29.81) 51.14 (50.90, 51.38) 44.72 (43.97,45.65) 54.11 (53.61,54.57) 54.38 (53.94,54.89) 68.47 (66.85,70.05)\nThe benchmark includes three generative-based tasks, medical visual question answering, report generation, and rationale diagnosis. ACC, F1, BLEU, ROUGE, BERT-Sim UMLS_Precision, and UMLS_Recall are reported, and the metrics refer to the average score on\nall test samples. Numbers within parentheses indicate 95% CI. Percentage (%) signs have been omitted in the table.\nArticle https://doi.org/10.1038/s41467-025-62385-7\nNature Communications|         (2025) 16:7866 6\nmodalities, for VQA and report generation, the whole model is further\nﬁnetuned on the speciﬁc dataset, as shown in the Table3, our model\nimproves both diagnosis results and text generation quality, based on\nthe automatic metrics. In general, the representation learned in RadFM\nbeneﬁts various clinical tasks across diverse medical imaging mod-\nalities, like CT, MRI, X-ray, or even rarely seen PET-CT, regardless of\nwhether they are presented in 2D slices or 3D scans. Consistent\nimprovement can be observed, across various task types, like diag-\nnosis, VQA, and report generation.\nGeneralization to unseen classes of PadChest\nIn Fig. 4, we show the results of zero-shot evaluation of RadFM on\nunseen classes from the PadChest dataset. We modify the task as an\ninduction task, for each disease, we randomly select a prompt sen-\ntence like “Is {disease} shown in this image” as input, the network\noutputs whether the case has this disease. Note that we balance the\nratio of“yes” or “no” in the test set, and all the disease classes never\nappeared in the training set. The prompting utilized here are essen-\ntially equivalent to traditional multi-label disease classiﬁcation meth-\nods, with each disease’s performance being evaluated independently,\nleading to a holistic outcome through sequential assessment of all\nlisted diseases. Following this same rationale, in our context, we can\nprovide comprehensive diagnosis results by iteratively employing the\n“yes/no” prompting approach.\nQualitative results\nIn this section, we show the qualitative results for different free-form\ntext generation tasks.\nFor medical VQA, qualitatively, as shown in Fig. 5,R a d F M\ndemonstrates the ability to comprehend the questions and provide\nanswers in a consistent format, accurately addressing the questions.\nHowever, in some challenging cases, such as theﬁrst example, where\nthe question pertains to the type of abnormality, the model faces\ndifﬁculty predicting“ectopic ACTH-producing tumor” and mistakenly\nidentiﬁes it as“primary lung neoplasm”,w h i c hr e q u i r e sﬁne-grained\ndiscrimination within tumor types.\nIn Fig.6, we provide qualitative examples of the radiology reports\ngeneration task by RadFM. It can be observed that the model is capable\nof identifying the underlying diseases and, in some cases, performs\nexceptionally well. However, the report generated by RadFM may lack\nOpenFlamingo MedVInT Med-Flamingo RadFM\n0\n20\n40\n60\n80\n100\nVQA-RAD\nAVG\n0\n10\n20\n30\n40\n50\nRP3D-VQA\nAVG\n0\n20\n40\n60\n80\n100\nCXR-Mix\nACC\n0\n20\n40\n60\n80\n100\nVinDr-Mammo\nACC\n0\n20\n40\n60\n80\n100\nSLAKE\nAVG\n0\n5\n10\n15\n20\n25\nRP3D-Report\nAVG\n0\n20\n40\n60\n80\n100\nRadChest-CT\nACC\n0\n20\n40\n60\n80\n100\nVinDr-SpineXr\nACC\n0\n10\n20\n30\n40\n50\nMIMIC-CXR\nAVG\n0\n10\n20\n30\n40\n50\nRP3D-Rationale\nAVG\n0\n10\n20\n30\n40\n50\nPMC-VQA\nAVG\n0\n20\n40\n60\n80\n100\nVinDr-PCXR\nACC\nMedical\nVQA\nReport\nGeneration\nRationale\nDiagnosis Average\nOpenFlamingo 2.21 0.89 1.09 1.40\nMedVInT 1.44 1.16 0.83 1.14\nMedFlamingo 1.74 0.86 1.03 1.21\nGPT-4V 2.13 1.64 2.22 1.99\nRadFM 2.87 1.88 1.76 2.17\n0\n1\n2\n3\n4\n5\nRating Score\nWin Tie Lose\n214\n235\n236\n150\n140\n140\n36\n25\n24\n02 0 0 4 0 0\nRationale Diagnosis\nReport Generation\nMedical VQA\nRadFM vs Med-Flamingo\n187\n224\n184\n191\n166\n145\n22\n10\n71\n02 0 0 4 0 0\nRationale Diagnosis\nReport Generation\nMedical VQA\nRadFM vs OpenFlamingo\n214\n189\n257\n176\n187\n116\n10\n24\n27\n02 0 0 4 0 0\nRadFM vs MedVInT\n80\n133\n200\n128\n175\n84\n190\n82\n106\n02 0 0 4 0 0\nRadFM vs GPT-4V\nG\u0014\nH\u0014\nRadBench\n(n=4229)\n(n=1468)\n(n=1000)\nRP3D-Caption\n(n=268)(n=1167)(n=1044)\n(n=2688)\n(n=9946) (n=91204)\n(n=7554)\n(n=374)\n(n=595)\nFig. 3 | The comparison of RadFM with other foundation models on machine\nand human rating. aComparison of RadFM with various foundation models on\ndifferent subsets under zero-shot evaluation with machine rating scores. The\ndetailed sample sizes for testing are marked as“(n=… )” in the left corner of each bar\nplot. For the dataset involving diagnosis, like VinDr-Mammo, VinDr-SpineX, VinDr-\nPCXR, CXR-Mix, RadChest-CT,”ACC“ scores are plotted in theﬁgure. For the left\ndatasets, “AVG” scores, denoting the average of the four word-overlap-based eva-\nluation metrics, i.e., BLEU, ROUGE, UMLS_Precision, and UMLS_Recall, are plotted.\nFor each bar plot, the error bars represent the 95% conﬁdence interval (CI) via 1000\ntechnical replications, with the center of the error bars indicating the average score\nacross all test cases within each dataset. The exact number for each bar plot can be\nfound in Supplementary Tab. 2.b Comparison of RadFM with other methods on\nhuman rating scores. On the left, we show the absolute human rating scores of\ndifferent methods on the three generative tasks. i.e., VQA, report generation and\nrationale diagnosis. On the right, we show the relative comparison. Each sub-ﬁgure\nin right shows the number of RadFM win/tie/lose cases when comparing against a\ncertain model. Note that, considering GPT-4V may refuse to answer medical\nquestions for safety, we dismiss such cases when calculating the scores or com-\nparisons relating to GPT-4V. In detail, for 1200 testing cases, 22 cases were dis-\nmissed for GPT-4V due to safety.\nArticle https://doi.org/10.1038/s41467-025-62385-7\nNature Communications|         (2025) 16:7866 7\nspeciﬁc location information, such as the‘left’or “right” of an anato-\nmical region.\nAt last, Fig.6 shows two rationale diagnosis cases. Theﬁrst case is\na patient with pulmonary embolism and the latter is with subarachnoid\nhaemorrhage. On both cases, RadFM can make accurate diagnosis in\nfree form and give further related radiologic reasoning. However, the\nlimitation can also be observed that the reasoning results are still\ngeneral and more like background medical knowledge, yet not speciﬁc\nto the input case.\nDiscussion\nRadFM tries to develop a medical foundation model that enables\nthe processing of both 2D and 3D radiologic images with inter-\nleaved texts.I nt h eﬁeld of radiologic images, one signiﬁcant chal-\nlenge in developing a foundation model lies in the disparity of image\ndimensions, i.e., medical scans are either 2D or 3D, posing challenges\nin integrating real 3D MRI or CT images alongside 2D images like X-rays\nor ultrasounds. As a consequence, the development of foundational\nmodels has been signiﬁcantly impeded, with most current models only\naccommodating 2D images. To overcome these limitations, we pro-\npose a new training structure that uniﬁes 2D and 3D images, allowing\nto process various clinical images. By unifying the training procedure,\nour model beneﬁts from a more comprehensive understanding of the\ndiverse clinical images, leading to improved performance and versa-\ntility. Additionally, to facilitate research and foster collaboration in the\nﬁeld, we collect four medical multimodal datasets, namely, PMC-Inline,\nRP3D, PMC-CaseReport, and MPx, consisting of 13M 2D and 615K 3D\nradiology scans with text descriptions or labels.\nRadFM uniﬁes the medical tasks with a generative model.\nWhile developing AI for medicine, traditional approaches consider a\nTable 3 | Comparison of RadFM with SOTA models on disease diagnosis, medical visual question answering, report generation\nDataset Modality Metric SOTA RadFM\nDisease diagnosis\nVinDr-Mammo Mammography 2D AUC 64.5 50 64.76 (64.23, 65.88)\nF1 N/A 39.42 (39.37, 39.59)\nCXR14 X-ray 2D AUC 80.1 52 81.13 (81.07, 81.18)\nF1 N/A 30.20 (30.17, 30.22)\nLDCT CT 3D AUC 82.1 50 83.23 (81.97, 85.85)\nF1 N/A 58.34 (57.38, 61.23)\nMosMedData CT 3D AUC 77.47 54 78.33 (76.37, 80.84)\nF1 50.70 52.35 (49.26, 55.17)\nCOVID-CT CT 2D AUC 76.00 † 81.37 (78.00, 82.49)\nF1 73.35 † 76.11 (74.30, 77.06)\nBraTs2019 MRI 3D AUC 88.06 68 90.61 (85.66, 92.13)\nF1 90.36 68 92.21 (91.01, 93.21)\nADNI MRI 3D AUC 79.34 58 80.39 (78.26, 82.44)\nF1 N/A 69.88 (68.43, 71.10)\nBTM-17 MRI 2D AUC 92.80 † 94.47 (92.60, 96.98)\nF1 70.35 † 74.19 (72.45, 76.31)\nLung-PET-CT-Dx PET-CT 3D AUC 53.44 † 54.57 (51.31, 57.69)\nF1 36.07 † 37.24 (34.41, 41.53)\nMedical VQA\nMedDiffVQA X-ray 2D Comparison Bleu 62.80\n61 63.89 (62.27, 64.39)\nRogue N/A 65.90 (64.48, 63.39)\nF1 N/A 59.19 (57.88, 60.43)\nVQA-RAD Radiology 2D Bleu 71.03 69 73.44 (66.04, 82.18)\nRogue N/A 73.81 (67.80, 80.04)\nF1 N/A 78.09 (73.54, 81.90)\nSLAKE Radiology 2D Bleu 78.6 70 83.16 (79.68, 87.10)\nRogue N/A 83.65 (80.39, 87.10)\nF1 78.1 70 84.37 (81.60, 86.78)\nPMC-VQA Radiology 2D Bleu 23.69 (20.70, 26.93) 8 24.13 (21.01, 27.91)\nRogue 27.20 (24.09, 31.13)8 25.64 (22.73, 29.29)\nF1 43.93 (41.16, 46.43) 8 48.50 (46.19, 51.00)\nReport Generation\nIU-X-ray X-ray 2D Bleu-1 38.7\n63 37.88 (35.96, 39.32)\nBleu-2 24.5 63 24.62 (22.73, 26.94)\nBleu-3 16.6 63 17.72 (15.77, 19.69)\nBleu-4 11.163 10.28 (8.89, 11.64)\nRogue-L 28.9 63 29.51 (28.09, 30.61)\nAll models wereﬁnetuned and evaluated on the same train/test set. AUC, F1, BLEU, and ROUGE are reported, and the metrics refer to the average score on all test samples. For multiple class tasks, the\nmacro-average on classes of the used metrics is adopted. Numbers within parentheses indicate 95% CI.\n†These datasets are not considered a lot as classiﬁcation tasks. Thus, these scores are obtained by training from scratch with the same architecture as ours to show the effectiveness of RadFM as a\npretrained foundation model.\nArticle https://doi.org/10.1038/s41467-025-62385-7\nNature Communications|         (2025) 16:7866 8\ndivide-and-conquer idea, that tackles a myriad of speciﬁc tasks\nindividually, such as diagnosis, report generation, and medical visual\nquestion answering, resulting in separated approaches with limited\ngeneralization ability and efﬁciency. Here, we formulate diverse\nmedical tasks as multi-modal question-answering and develop a\ngenerative visual-language model, RadFM, that can answer arbitrary\nquestions or follow instructions. In contrast to existing works with\nthe use of exemplars in prompts, we use zero-shot prompts for all\ntasks, allowing users to interact with the model without providing\nany exemplar images, questions, and answers. Training models\nsupport zero-shot prompts is certainly more challenging, however,\nconsidering the user might be patients without no clinical back-\nground, or examplar images, zero-shot prompt would also be indis-\npensable for real-world applications. By unifying the tasks, RadFM\nachieves promising performance across a wide spectrum of clinical\ntasks. On the medical VQA task, RadFM surpasses the performance of\nMedVINT, a pretrained model trained solely on a single Medical VQA\ndataset.\nRadFM supports multiple images as input. Till now, existing\nmulti-modal foundation models in the medical community have been\nlimited to supporting only a single image input per interaction. How-\never, such a design poses critical challenges in medical scenarios where\ndiagnosis and treatment decisions often necessitate longitudinal clin-\nical follow-ups, that comprise a series of radiologic images. To over-\ncome this limitation and pave the way for more comprehensive\nmedical image analysis, our proposed RadFM supports multi-image\ninput. To support the training, our constructed dataset is largely\ncomposed of multi-image input data, and our innovative trainingﬂow\nFig. 4 | Zero-shot evaluation of RadFM on the unseen classes in the PadChest dataset.We evaluate the model on the human-annotated subset of the PadChest dataset,\nand ACC scores are shown for the radiographicﬁndings or diagnosis. The top 100 classes in the test dataset are shown in theﬁgure.\nArticle https://doi.org/10.1038/s41467-025-62385-7\nNature Communications|         (2025) 16:7866 9\nseamlessly accommodates this unique medical scenery, fostering\nadvancements in medical image analysis.\nA general evaluation benchmark for radiology foundation\nmodels. Evaluating the performance of medical foundation models is\nchallenging, due to the specialized nature of medical tasks. In the\npursuit of advancing radiology foundation models, we propose Rad-\nBench, a novel benchmark that encompasses a diverse range of med-\nical scenarios. By incorporating both 2D and 3D images, RadBench\noffers a more comprehensive and realistic evaluation platform com-\npared to existing benchmarks. Combining with the existing medical\nbenchmarks, we comprehensively evaluate models for four medical\ntasks, namely plain diagnosis, visual question answering, report gen-\neration, and rationale diagnosis, covering multiple imaging modalities.\nAdditionally, as existing evaluation metrics are primarily designed for\ngeneral natural language tasks, which may not adequately capture the\nintricacies and nuances speciﬁc to medical image analysis, thus may\nnot reﬂect the model’s true capabilities in real-world clinical scenarios.\nTo address this limitation, we propose two new evaluation metrics,\nnamely UMLS_Precision and UMLS_Recall. Unlike conventional\nmetrics, UMLS Precision and Recall are tailored to measure the model’s\nperformance in medical tasks. By leveraging the Uniﬁed Medical Lan-\nguage System (UMLS), a comprehensive medical knowledge resource,\nthese metrics provide a more tailored evaluation, ensuring that the\nmodel’s outputs align with medical domain expertise.\nThe superiority of RadFM.A ss h o w ni nT a b l e s1, 2 and Fig.1,\nwhile evaluating on our proposed comprehensive benchmark for\nFig. 5 | Qualitative examples of medical visual question answering (VQA).We present several examples with answers generated by RadFM along with the target ground\ntruth. The green color highlights accurate keywords, while the red color indicates prediction errors.\nArticle https://doi.org/10.1038/s41467-025-62385-7\nNature Communications|         (2025) 16:7866 10\nFig. 6 | Qualitative examples of report generation and rationale diagnosis.We\npresent several examples with reports generated by RadFM and reference reports.\nThe green color highlights accurate keywords, while the red color indicates\nprediction errors. Additionally, the blue color denotes instances where the model\nmissed this information that has been mentioned in the reference reports.\nArticle https://doi.org/10.1038/s41467-025-62385-7\nNature Communications|         (2025) 16:7866 11\nradiology, namely, RadBench, and nine existing radiological datasets,\nRadFM outperforms previous methods by a signi ﬁcant margin\nacross allﬁve tasks, showcasing its exceptional capabilities. Notably,\nRadFM excels in particularly challenging tasks such as medical VQA,\nreport generation, and rationale diagnosis, which demand a pro-\nfound understanding of both textual information and images. The\naverage human evaluation score for RadFM in these tasks surpasses\nthat of GPT-4V, especially in the medical VQA task, where RadFM\nachieves a score of 2.87 compared to GPT-4V’s score of 2.13. In\nmedical VQA, the questions can be drastically varying, from simple\nqueries like“What modality is the given image?” to more complex\nand context-rich questions, such as“Based on the provided images,\npatient data (age, gender, and medical history), can you identify the\ndisease that is commonly associated with such radiological mani-\nfestations?” The complexity of questions makes medical VQA a\ncomprehensive and versatile task. By integrating visual and textual\ninformation, RadFM can handle these varying question types, deli-\nvering accurate and meaningful answers. Similarly, in report gen-\neration, RadFM showcases signiﬁcant improvement. The model’s\nability to discern relevant information from the provided images\nand weave it cohesively with textual prompts leads to highly infor-\nmative and contextually rich reports, setting it apart from traditional\nmethods. Overall, the performance of RadFM across these diverse\ntasks conﬁrms its versatility and transformative potential in radi-\nology image analysis.\nClinical impact. In contrast to all existing medical models, RadFM\nis the ﬁrst attempt towards developing foundation models that\nsimultaneously satisﬁes three important criteria in clinical practice: (i)\nin support of both 2D and 3D data, for example, 2D chest-X-ray, 3D CT\nor MRIs; (ii) be able to process multiple scans from various imaging\nmodalities; (iii) to support interleaved data format, for example,\nallowing the user to freely input additional background information in\ntext form, along with radiology scans, the model enables to accom-\npolish complex clinical decision-making tasks. Overall, RadFM allows\nusers to input 3D multiple scans interleaved with texts per query,\nwhich can greatly beneﬁt its clinical usage.\nLimitations. Despite our efforts in developing a foundation\nmodel for radiology and surpassing former medical foundation mod-\nels signiﬁcantly, RadFM is still a proof-of-concept model design\ntowards medical generalist AI (GMAI) and need more efforts for real\nclinical usage. In detail, it may exhibit the following limitations:\nFirst, the capacity to generate meaningful and accurate long\nsentences remains underdeveloped, causing the foundation models to\nbe still far from clinically useful. As demonstrated in Supplementary\nTab. 2, for rationale diagnosis and report generation, the quantitative\nresults surpass previous works but are still far from practically satis-\nfactory. In human rating, similar results are also observed. As shown in\nFig. 3, none of the models gets over score 3 which represents mod-\nerately accurate, showing that there is still a long way to go for\ndeveloping generalist medical foundation models. For real clinical\nusage, we suggest future improvements, including scaling model sizes,\nincreasing image resolutions, and expanding clinical data, as outlined\nby the scaling laws\n20.\nSecond, the proportion of actual 3D images in the data remains\nlimited. As illustrated in Fig.7, although we attempt to compensate for\nthe lack of 3D images, 2D images remain to be dominating.\nThird, the automatic evaluation metrics fall short of expectations.\nCompared to general contexts where the emphasis is placed on the\noverall coherence andﬂuency of sentences, medical texts prioritize\nprecision in key statements and contain many synonyms, like“MRI”\nand “magnetic resonance imaging”, overlooking minor syntax errors.\nAlthough we employ UMLS_Precision and UMLS_Recall to mitigate this\nissue, they do not fully reﬂect true performance. On the other hand,\nthough human evaluation isﬂexible and accurate, it is costly and\ncannot be carried out on a large scale. A robust automatic evaluation\nmetric is essential to guide the construction of reliable and robust\nmedical foundation models.\nFourth, as the 3D images in our dataset are downloaded from the\ninternet, some metadata is missing, for example, the imaging spacing.\nSuch a lack of precise distance measurement makes it impossible to\nmake certain statements, such as“The tumor is 3-cm large”.T h i s\nspeciﬁcation is crucial for report writing, particularly for tasks requir-\ning precise spatial information, such as tumor size estimation, cur-\nrently, we acknowledge that our model cannot be used in this way. We\npropose several potential solutions to address this limitation in future\nwork, for instance, one way is to explore incorporating registration or\nspacing prediction models to generate the pseudo-spatial information,\nallowing the model to make more accurate numerical predictions\nrelated to physical measurements (e.g., tumor size). Another promis-\ning direction is to enhance the model’s ability to interact with external\ntools or agents. For example, the model could use segmentation\nmodels alongside coding functions to calculate physical spacing\ndirectly, providing precise numeric feedback to assist in tasks like\nreport generation or anomaly detection.\nLastly, due to the scale of the dataset (16M image-text pairs),\nmodel size (14B parameters), investigating the effects of different\ncomponents becomes increasingly challenging and prohibitively\nexpensive in terms of both time and computational resources. As\nfuture work, we will further break down the problem and investigate\neach component, ultimately enhancing our understanding and reﬁn-\ning the model’s performance. This includes, but is not limited to,\nimproved 2D and 3D uniﬁed encoding methods, more effective choi-\nces for LLM base models, and enhanced training pipelines that address\ndata imbalance arising from data combination.\nRelated works. With the success of generative language founda-\ntion models such as GPT-4\n16 and PaLM-221, there has been a surge of\ninterest in multi-modal foundation models. Signiﬁcant strides have\nbeen made in the realm of natural scenery, as evidenced by BLIP-23 and\nFlamingo22. Though in the context of the medical language-only\nmodels, great steps have been made, like Med-PALM series23,24,P M C -\nLLaMA25, Meditron-70b26, the development of multimodal medical\nartiﬁcial intelligence is still in its nascent stages5. The relevant research\ncan be bifurcated into two primary areas, namely, dataset construction\nand model training.\n Dataset Construction. Contrary to the natural scenery domain,\nwhich boasts numerous large-scale multi-modal datasets such as\nMMC4\n27,V i s u a lG e n o m e28,a n dL I O N - 5 B29,t h em e d i c a ld o m a i ni s\nsomewhat lacking. The most widely utilized medical multi-modal\ndataset is MIMIC-CXR\n30, which only contains chest X-ray images\nwith caption reports, and its quantity (224K) is relatively small. In\nPMC-OA\n31, the authors have compiled a dataset containing 1.6M\nimage-caption pairs. Although it encompasses various image\nmodalities, many 3D medical scans are presented as 2D slices\nsince the images are extracted from papers. There are also some\nmedical VQA datasets, such as VQA-RAD\n32, SLAKE33,a n dP M C -\nVQA8, but they are also limited to 2D images. In Med-Flamingo6,\nthey have collected a dataset, MTB, consisting of approximately\n0.8M images interleaved with texts while it is not open-source.\nConsequently, due to the limitations on data availability, existing\nmedical foundation models have concentrated on a narrow range\nof data modalities. For example, LLaVA-Med\n4 and MedVInT8\nutilize image captions in PubMed Central, which exists a\nsigniﬁcant domain gap between real-world clinical data. In Med-\nPaLM M\n7, the authors amalgamate existing medical images or\nmulti-modal datasets, but the majority of images are X-rays, which\nare not sufﬁciently accurate for clinical practice.\n Model Training. To date, several works have focused on building\nmedical foundation models, yet most of these works are limited to\nsupport for 2D images\n4,6–8. An ideal foundation model should\nexhibit a comprehensive set of capabilities: it should support both\nArticle https://doi.org/10.1038/s41467-025-62385-7\nNature Communications|         (2025) 16:7866 12\nFig. 7 | The overview of our proposed datasets and benchmarks. aOverview of\nMedical Multimodal Dataset (MedMD). Our collected data covers the majority of\nradiologic modalities and anatomical regions of the human body, such as the brain,\nhead and neck, thorax, spine, abdomen, upper limb, lower limb, and pelvis, etc. The\ndataset mixes two types of datasets, i.e., interleaved datasets and visual instruction\ndatasets.T refers to the text of interleaved data,I refers to the instruction input\ntext, andR refers to the response text.b The data statistics of RadMD and Rad-\nBench. The left image shows the distribution of different modalities of RadMD, and\nthe center image shows the distribution of 2D and 3D sample pairs of RadMD. The\nright image shows the distribution of the anatomy of the samples in the RadBench.\nArticle https://doi.org/10.1038/s41467-025-62385-7\nNature Communications|         (2025) 16:7866 13\n2D and 3D image inputs, multi-image input per case, and images\ninterleaved with text inputs. Currently, there exists no model that\ncan simultaneously support this series of heterogeneous input\nforms. This paper aims to address these gaps, aligning more\nclosely with clinical practice and we are very glad to see many\ncurrent works\n34–37 have been inspired based on our efforts.\nIn conclusion, this paper serves as a proof-of-concept initializa-\ntion for building medical generative vision-language foundation\nmodels. Speciﬁcally, our work demonstrates:\n Data collection: Highlighting the importance of leveraging and\ncombining web-scale radiology data sources to ensure robust and\ncomprehensive model training.\n Architecture formulation:P r o p o s i n gt h eu n iﬁcation of all radi-\nology tasks within a generative architecture formulation, carefully\ndesigned with task-speciﬁc instructions, to develop a versatile and\ngeneralist model.\n Evaluation: Underlining the need to monitor model performance\nacross a diverse range of radiology tasks to ensure comprehensive\nvalidation and broader applicability.\nWhile our model signiﬁcantly outperforms existing open-source\nmultimodal foundation models, we acknowledge that it does not yet\nmeet clinical criteria. Nevertheless, we believe this work provides a\nstarting point for future research and development toward more\ngeneralist medical AI models. With the remarkable and rapid\nadvancements in this ﬁeld, there will be continuous progress in\nseveral key areas. For example, more practical and invaluable data-\nsets, such as CT-RATE\n38, are expected to be released. Similarly, more\npowerful general multimodal foundation models, such as the latest\nDeepSeek-VL\n39 and Qwen2-VL40, will likely emerge, serving as stron-\nger base models along with increasingly sophisticated imaging\nencoders, like CT-ViT\n38 and LongViT41. By consistently integrating\nthese distinct impressive advancements in future work, we, together\nwith the entire research community, can drive the evolution of\nradiology foundation models toward broader adoption in practical\nclinical applications.. We will release all corresponding data, codes,\nand models. We believe this can greatly promote the development of\nmedical foundation models.\nMethods\nIn this section, we will detail our method. Notably, our study is based\non data obtained from open-source websites, as listed in Supple-\nmentary Table 5. Therefore, the relevant ethical regulations are\ngoverned by the original data-uploading processes outlined in each\ndataset’s collection pipeline (please refer to each dataset website in\nSupplementary Table 5 for more details). Speciﬁcally, for the data\nfrom Radiopaedia, which forms the main component of our newly\nproposed dataset, Radiopaedia is a peer-reviewed, open-edit radi-\nology resource collection website. Its mission is to“create the best\nradiology reference available and to make it available for free, for-\never, and for all.” We have obtained non-commercial use permission\nfrom various uploaders as well as the founder of Radiopaedia. The\nrelevant ethical regulations are governed underRadiopaedia privacy-\npolicy.\nDataset\nHere, we describe the procedure for constructing the datasets and\nbenchmark. In the section“Medical Multimodal Dataset (MedMD)”,w e\npresent several medical multimodal datasets and merge them with an\nextensive collection of preexisting datasets, resultingMedical Multi-\nmodalDataset (MedMD). MedMD is a large-scale, high-quality medical\nvision-language dataset, covering a wide range of anatomies with over\n5000 diseases, as shown in Fig.7a. We further construct aﬁltered\nradiology subset Radiology Multimodal Dataset (RadMD). In the\nsection“Radiology Evaluation Benchmark (RadBench)”,w ei n t r o d u c ea\nnew Radiology Benchmark for evaluation, termedRadBench,w i t h\nthree distinct tasks, e.g., visual question answering, report generation\nand rationale diagnosis, aiming to monitor the progress of developing\nfoundation models.\nMedical multimodal dataset (MedMD). To start, we construct a can-\ndidate data pool by pulling a variety of existing visual-language med-\nical datasets together, for example, MIMIC-CXR\n30 and PMC-OA31.\nDespite the scale of these high-quality datasets, they are fundamentally\nlimited in several aspects: (i) Data format. These datasets are only\ncomposed of 2D medical images, which do not fully capture the\ncomplexities in clinical use cases, for example, 3D medical imaging\nmodalities, like CT, MRI; (ii) Modality diversity. A noteworthy limita-\ntion arises from the fact only chest X-ray images are provided with\nmedical reports, training models on such data will clearly pose lim-\nitation on the generalizability to a broader range of imaging modalities\nand anatomical regions; (iii) Report quality. Another critical limitation\nlies in the use of data extracted fromﬁgures and captions from\nresearch papers. The gap between research-oriented data and real-\nworld clinical scenarios may not support accurate and reliable clinical\ndiagnoses. Therefore, to support the training of our proposed Radi-\nology Foundation Model (RadFM), we augment the dataset with four\nnew ones, including PMC-Inline, PMC-CaseReport, RP3D-Series, and\nMPx-Series, resulting in MedMD. MedMD has a total of 16M 2D image-\ntext pairs, including around 15.5M 2D images and 500k 3D scans with\ncorresponding captions or diagnosis labels, as shown in Supplemen-\ntary Table 3. More detailed introduction of different data sources can\nbe found in the Supplementary Section“Detailed Introduction for\nDifferent Data Sources”.\nGenerally speaking, we split the candidate data pool into two parts\n(i) interleaved image-language data that is collected from academic\npapers and (ii) image-language data constructed for visual-language\ninstruction tuning, as detailed below.\nInterleaved dataset. PMC-Inline. PMC-Inline contains 11M 2D radi-\nology images that are collected from PubMed Central papers. In con-\ntrast to existing work, for example, PMC-OA\n31,t h a to n l yc o n t a i n s\nﬁgures and corresponding captions, here, we focus on the inline\nreference from the main body of papers. For example, one paper may\ncontain many sentences like“A ss h o w ni nF i g .2,w ec a ns e e…” ,w e\nlocalise the keyword“Fig.2” and link its correspondingﬁgure back into\nsentences, ending up with interleaved images and texts, with rich\ncontext. This dataset shares the same format as MMC4\n27,w h i c hh a s\nshown to be effective in training foundation models in the computer\nvision community, for example, Flamingo\n22.\nVisual-language instruction tuning dataset . PMC-CaseReport.\nInspired by former works leveraging clinical case reports42,P M C -\nCaseReports is aﬁltered subset of PMC-Inline with around 103K case\nreports, where the doctors typically document the valuable clinical\ncases, based on their contact with the patients, such as family medical\nhistory, preliminary diagnosis, radiographic exam results, surgical\nrecords, etc., together with critical radiologic scans, that generally\nfollows the real timeline.\nSimilar to PMC-VQA\n8 that generates VQA pairs by querying\nChatGPT with image captions, we also generate 1.1M question-answer\npairs by querying ChatGPT with the sentences containing inline\nreferences in case reports. However, in contrast to PMC-VQA, we keep\nbackground information of the patients to simulate the clinical diag-\nnosis scenario, thus can be seen as a medical contextual VQA dataset.\nFor example, a question-answer pair may like“Question: A 58-year-old\nwoman presented to the emergency department… Postoperatively,\nher pain signiﬁcantly relieved. What did the MRI indicate? Answer: The\nMRI indicated tumor recurrence at L2 and S1-S2.”\nArticle https://doi.org/10.1038/s41467-025-62385-7\nNature Communications|         (2025) 16:7866 14\nRP3D. RP3D (RadioPaedia 3D) is a novel dataset with 3D radiology\nscans, sourced from the Radiopaedia website (https://radiopaedia.org/\n). All privacy issues have already been resolved by the clinician who\nuploaded the case. Speciﬁcally, each patient case comprises one or\nmore images from the same or different modalities, accompanied by\nhigh-quality captions that have been meticulously peer-reviewed by\nexperts in the Radiopaedia Editorial Board (https://radiopaedia.org/\neditors). We have included a response letter from Radiopaedia, with\nthe agreement for us to use the dataset for training under non-\ncommercial cases. In addition, for each disease, we can get corre-\nsponding radiological features across different modalities. We convert\nthe image-caption pairs into a variety of formats, namely, RP3D-Cap-\ntion, RP3D-Modality, RP3D-Rationale, and RP3D-VQA, depending on\ntheir corresponding text content. Speciﬁcally, RP3D-Caption denotes\nthe images paired with their corresponding captions; RP3D-Modality\nrefers to images with modality labels; RP3D-Rationale incorporates\nradiological features with disease labels for each case; RP3D-VQA\ninvolves visual question-answering pairs generated from captions by\nquerying ChatGPT, as illustrated in Supplementary Fig. 1.\nMPx. MPx is collected from the MedPix website (https://medpix.\nnlm.nih.gov/) and organized by cases. Each case contains multiple\nradiologic scans, along with general clinicalﬁndings, discussions, and\ndiagnostic results. In addition, MPx also provides annotations on the\nscan level, including information such as image modality, shooting\nplane, and captions for each scan. Thus, we separate it into MPx-Single\nand MPx-Multi, containing annotations on the case-level and scan-\nlevel, respectively.\nRadiology multimodal dataset (RadMD).F o rd o m a i n - s p e c iﬁc ﬁne-\ntuning, we ﬁlter out the non-radiology images from MedMD, and\nconstruct a clean subset, named Radiology Multimodal Dataset\n(RadMD), dedicating to supervised visual instruction tuning. It con-\ntains a total of3M images, spanning various data formats, modalities,\nand tasks, featuring over5000 diseases, as shown in Fig.7b.\nIn general, we have conducted the followingﬁltering process: (i)\nremove non-radiologic images; (ii) remove the entire PMC-OA and\nPMC-Inline datasets, as the images in PubMed are 2D-only, thus differ\nfrom real clinical cases, additionally, the writing styles between aca-\ndemic papers and real clinical reports are inconsistent; (iii) remove a\nlarge portion of 2D image cases from PMC-Series, to emphasize the 3D\nimages in training. (iv)ﬁlter out the information about patient age or\nstructure size, as the image spacing and patient background infor-\nmation are not provided. Speciﬁcally, we applied string matching\ntechniques using Python’s regular expressions to remove any sen-\ntences containing terms related to physical measurements, such as\n“mm”, “cm”, or decimal numbers (e.g.,“2.5 cm”), as these are indicative\nof missing or incomplete metadata related to patient age, structure\nsize, or image spacing. This step primarily addresses the problem in\nthe report generation tasks, where such metadata would otherwise\ncause incorrect or unpredictable descriptions.; (v) balance the number\nof normal and abnormal patients in the diagnosis datasets, as gen-\nerative models are sensitive to data imbalances. More comprehensive\ndetails regarding theﬁltering process and the resulting dataset sizes\ncan be found in Supplementary Table 3.\nRadiology evaluation benchmark (RadBench). In addition to the\ntraining set, we also introduce RadBench, a comprehensive evaluation\nbenchmark for monitoring progress in the development of radiology\nfoundation model for generative tasks. Considering that most existing\nmedical benchmarks may only include a plain label (like disease cate-\ngories), that are not suitable to assess the models’ long sentence\ngeneration ability, our RadBench is targeted at compensating for this.\nIn detail, RadBench isﬁrst randomly sampled from the RP3D\ndataset. Then, We further carry out meticulous manual veriﬁcation to\nensure data quality on all the samples. Speci\nﬁcally, we developed a\nhuman evaluation interface, visually presenting the data source,\nimage, question, and answer of each case. Eight human annotators\nwere asked to assess the quality of these cases by addressing the fol-\nlowing criteria:\n Image types: remove the images that do not fall in radiology.\n Question reasonability: keep the questions that can be answered\nfrom the given radiology image, for example, on visual question\nanswering, remove the question related to size; on report gen-\neration, remove cases containing sentences like“Compared with\nprevious cases”; on rationale diagnosis, remove cases lacking\ncorresponding radiological features areﬁltered out.\n Answer correctness: keep those with correct answers based on\nthe given text reports.\nAs a result, we have obtained 4229 for visual question answering,\n1468 for report generation, and 1000 for rationale diagnosis. Addi-\ntionally, we also consider nine existing tasks for our evaluation, which\ninclude plain diagnosis and medical VQA tasks. A detailed breakdown\nof each dataset, including task descriptions and modalities, is provided\nin Supplementary Table 4. Combining them with our RadBench, in\nevaluation, we will comprehensively assess models for four tasks, i.e.,\ndisease diagnosis, medical VQA, report generation, and rationale\ndiagnosis. The details of the four evaluation tasks and metrics are\nintroduced in the following.\nDisease diagnosis. This task involves analyzing the radiology images\nto determine the likelihood of speciﬁc diseases. Here, we modify this\ntask to an induction task, which uses introductory text explaining the\nclassiﬁcation task and providing the name of the queried disease at\nthe beginning of the prompt. Given a medical image, we randomly\nselect a disease and a prompt sentence like“Is {disease} shown in this\nimage” as input, querying the model to determine the existence of a\ncertain disease. Due to this being formulated as a generation task,\n“AUC” cannot be calculated, so we match the output with ground-\ntruth to calculate the ACC and F1 score. Similarly, we match the\noutput with a closed ground-truth list {“yes”, “no”} using dif-\nﬂib.SequenceMatcher, and choosing the most similar one as the\nprediction of the model. Considering ACC scores may suffer from\ndata unbalancing, we keep the same ratio to sample positive and\nnegative cases. In our dataset, we do not put prior on the disease, and\nover 5000 diseases are considered, with a balanced ratio of“yes” or\n“no” responses.\nM e d i c a lv i s u a lq u e s t i o na n s w e r i n g. This task is a combination of\npopular visual question-answering challenges. Given a medical image\nand a clinically relevant question in natural language as a prompt, the\nmedical VQA system is expected to predict a plausible and convincing\nanswer.\nRadiology report generation. This task focuses on the automatic\ngeneration of reports, i.e., summarizing the radiologicﬁndings based\non radiology images, such as X-rays, CT scans, and MRI scans. Given a\nmedical image, we randomly select a prompt sentence like“Please\ncaption this scan withﬁndings” as input.\nRationale diagnosis. This task involves analyzing radiology images to\npredict both the underlying disease and the typical radiologic features\nof different modalities, such as X-rays, CT scans, and MRI scans asso-\nciated with that disease. Speciﬁcally, we randomly select a prompt\nsentence like“Determine the disease that corresponds to the given\nradiographic images, starting with the established radiological fea-\ntures and concluding with the ultimate diagnosis.” Since we have\nevaluated disease diagnosis accuracy in the common“Disease Diag-\nnosis” setting, for rational diagnosis, we mainly focus on how well the\nfoundation model can give reasons.\nArticle https://doi.org/10.1038/s41467-025-62385-7\nNature Communications|         (2025) 16:7866 15\nBuilding generalist foundation model for radiology\nIn this section, we start by describing the paradigm for unifying dif-\nferent medical tasks into a generative framework, followed by detailing\nthe proposed RadFM model, and its training details. Our training\nadopts two types of datasets, namely, interleaved datasets and visual\ninstruction datasets. It is worth noting that their training objectives\ndiffer slightly, which will be detailed in the following.\nAu n iﬁed learning paradigm. In both of our proposed multimodal\ndatasets, i.e., MedMD and RadMD, each training sample is essentially\nconsisting of two elements, i.e.,X = fT , Vg,w h e r eT refers to the lan-\nguage part in the case, with special placeholder tokens for images, e.g.,\n“The patient is 47-year-old.〈image-1〉〈 image-2〉 We can see opacity on\nthe X-ray”. V r e f e rt ot h ev i s u a lp a r t sc o n t a i n i n gas e to f2 Do r3 Di m a g e\nscans, i.e., V = fv\n1, v2, ... , vN g, vi 2 RH × W × C or vi 2 RH × W × D × C ,\nH, W, D, C are height, width, depth, and channel, respectively, corre-\nsponding to the“〈image-i〉 ” token inT .I ng e n e r a l ,T and V can be\nconsidered as prompts input to model with interleaved language\nand image.\nThe goal is to model the likelihood of generated text tokens inT ,\nconditioned on interleaved scans as:\npðT jVÞ =\nY\npðT\nljV < l, T < lÞ, ð1Þ\nwhere T l represents the l-th token inT and V < l, T < l represent the\nimage and language text appearing before the l-th token. We use a\ngenerative model (ΦRadFM) to parameterize the probabilityp,a n do u r\nﬁnal training objective can be expressed as the negative log-likelihood\nof the correct next token in the text sequence:\nLreg = /C0\nX\nwl log ΦRadFMðT ljV < l, T < lÞ, ð2Þ\nwhere wl refers to a per-token weighting, aiming to either emphasize\nkey tokens or skip special tokens. Itsvalue differs for different datasets\nand we detail this in the following.\nInterleaved datasets. For samples in visual-language interleaved\ndataset, i.e., PMC-Inline, there are no strong question-and-answer\nrelationships between contexts, we extract medical-related words in\neach sentence by using uniﬁed medical language system (UMLS)43,a n d\ngive them a high loss weights. Additionally, we avoid calculate loss on\nthe image placeholder token. Overall,w\nl can be formulated as,\nwl =\n3, T l 2 USML\n1, T l =2 USML\n0, T l = h image-ii\n8\n><\n>:\n: ð3Þ\nNote that, PMC-Inline is the only datasetﬁti nt h i sc a s e .\nVisual instruction datasets. For samples from visual instruction\ndatasets like PMC-VQA8 or PMC-CaseReport, they are often in the\nformat of dialogue, for example,“What can you see from the image?\n〈image-1〉 I can see lesions.” or “Please describe the scans〈image-1〉.\nThe scan is…” ,w ef u r t h e rs e p a r a t et h el a n g u a g ep a r tT into instruc-\ntion and response, denoted asI and R respectively. For example, as in\nthe former two cases,I refers to“What can you see from the image?\n〈image-1〉 ” and “Please describe the scans〈image-1〉 ”. In a practical\nscenario, I is expected to be given by users, and the model is only\nrequired to output correct responses. Overall,wl can be formulated as,\nwl =\n3, T l 2 R & T l 2 USML\n1, T l 2 R & T l =2 USML\n0, T l 2 I\n8\n><\n>:\n: ð4Þ\nMost samples from MedMDﬁt the weighting formulation. All prompts\nused for instruction tuning are listed in the Supplementary Tables 8–11.\nWe describe the detailed prompting for different problem settings:\n Modality recognition. Here, we adopt two types of prompts, (i)\nwe use inductive prompts, and the 2D or 3D medical scan as input,\nfor example,“〈image-1〉 Is this image captured by {modality}?”,\nand the modality category is randomly sampled from the modality\nset, forming the text inputI and if the modality matches the\nground truth labels we set theR as “yes” otherwise “no”.( i i )w e\nuse open prompts, like“What’s the modality of the input scan\n〈image-1〉 ?” to form the I, and translate the corresponding\nmodality label intoR. Samples for training such functionality are\nfrom RP3D-Modality and MPx-Single, with modality annotations\navailable.\n Disease diagnosis. All the datasets listed as“image data” in Sup-\nplementary Table 3 are built for diagnosis, they only have binary\nlabels for diseases. Similarly to modality recognition, we use two\nprompts to transform them into our desired format, (i) we use\ninductive prompts, like“〈image-1〉 Does the patient have {dis-\nease}?” and the disease category is randomly sampled from a\ndisease set, forming the text inputI and if the disease matches the\nground truth labels we set theR as “yes” otherwise“no”,n o t et h a t ,\nduring sampling, we balance the positive and negative ratio, (ii)\nwe use open diagnosis prompts, like“Please make diagnosis based\non the images〈image-1〉〈 image-2〉\n.” to construct the instruction\n(I), and translate the positive disease labels into response (R), by\nsimply using their category names. A simple example is,I=\"Please\nmake diagnosis based on the image〈image-1〉.” with R = “Edema,\npneumothorax.”. With such instruction, the model is thus\nrequired to complete a difﬁcult task, i.e., directly outputting the\ndisease name.\n Visual question answering. Beyond the abovementioned task\nformulation, there are more complex questions that can be asked,\nsuch as those about the spatial relationships among objects\n(\"What is the location of the lesion?”) and common sense rea-\nsoning questions (\"Given the image context and patient history,\nwhat is likely to be the cause of the observed symptoms?”). A\nrobust medical VQA system must be capable of solving a wide\nrange of classic medical diagnosis tasks, as well as the ability to\nreason about images. Existing medical VQA datasets like VQA-\nRAD\n32,S L A K E33,P M C - V Q A8 and RP3D-VQA naturallyﬁti n t ot h i s\nparadigm. They contain a mixture of question types, thus the\nlanguage questions can naturally be treated as text instruction (I)\nand the corresponding answer as response (R). It is worth noting\nthat, our constructed PMC-CaseReport dataset also falls into this\ncategory, with more contextual information available for instruc-\ntion, for example, history diagnosis, is also available, thus\nproviding critical information for answering the question.\n Report generation. MIMIC-CXR\n30, RP3D-Caption, PMC-OA31,\nMPx-Multi, and MPx-Single are all captioning datasets, the task\nis to write a long caption or report given one or a set of images.\nThe language instruction for this task are like“What can youﬁnd\nfrom the scans〈image-1〉〈 image-2〉?”.\n Rationale diagnosis. We construct RP3D-Rationale based on the\nRP3D dataset. This task encompasses disease prediction and\nthe generation of typical radiological features associated with the\ndiagnosed disease. Speciﬁcally, we design some prompts like\n“What disease can be diagnosed from these radiological images\nand what speciﬁc features are typically observed on the images?\n〈image-1〉〈 image-2〉 ” as instruction (I), and response (R)r e f e r st o\nthe disease label along with radiological features collected from\nthe Radiopaedia website.\nArchitecture detail. In this section, we aim to describe the proposed\nmodel in detail. As shown in Fig.1c, our proposed RadFM model\nconsists of a visual encoderΦ\nvis, that can process both 2D and 3D\nmedical scans; a perceiver44 moduleΦper for aggregating a sequence of\nscans into aﬁxed number of tokens, for example, taken with different\nArticle https://doi.org/10.1038/s41467-025-62385-7\nNature Communications|         (2025) 16:7866 16\nmodalities (CT, MRI) or various time point; and a large language model\n(LLM)Φllm that enables to generate free-form text responses, based on\nthe input visual-language information.\nVisual encoding. Given one sample instance from our dataset,\ndenoted asX = fT , Vg,w h e r eV = fv1, v2, ... , vN g,w eﬁrst encode each\ninput image separately with an image-encoderΦvis.S p e c iﬁcally, we\nadopt 3D ViT here to be compatible with both 2D and 3D image input.\nFor 2D images, we expand a new dimension for depth by replicating\nthe slices. Therefore, each image scan can be denoted as\nv\ni 2 RH × W × Di × C ,w h e r eC denotes the image channels andH, W, Di are\nthe height, width, and depth of the image, respectively. The rationale\nbehind this design choice is as follows: (i) increasingly more radiology\ndiagnosis rely on 3D scans, for example, CT, MRI, the foundation\nmodel should certainly be able to process 3D data input; (ii) in 3D data,\nconsecutive slices are highly similar, thus padding 2D into 3D, on the\none hand, does not lead information loss, on the other hand, resem-\nbles a good approximation of 3D data; (iii) padding 2D images will only\naffects the tokenization layer, i.e., converting image patches into\ncontinuous embedding, while still keep the rest of model shared with\n3D scans, thus facilitating knowledge share.\nNote that, comparing to the typical visual encoding scenario that\nassumes different images have uniﬁed shape, wedo notnormalize the\ndepth dimension into an exact size, only round into a factor of 4,\ndepending on their original resolution. Note that, all the 2D images are\npadded into four slices on the depth channel. We convert the image\ninto 3D patches, embed them into a token sequence, and feed into the\nencoder (Φ\nvis). To retain the 3D position of these tokens, we adopt\nlearnable 3D position embeddings, the detailed procedure can be\nformulated as:\nv\ni = ΦvisðviÞ2 RPi × d, ð5Þ\nwherevi is the output embedding for imagevi, encoded with 3D ViT,Pi\nis the total number of tokens, andd is the feature dimension. Due to\nthe inconsistency in depth dimension,Pi varies across 2D and 3D\nimages, and the model can get to know the original image size by\npositional encoding.\nAggregation with perceiver. After visual encoding, we adopt a\nperceiver\n44 module Φper to aggregate visual representation. Speciﬁ-\ncally, Φper follows the classical perceiver architecture with a ﬁx\nnumber of learnable queries as the latent array input, and the visual\nembedding v\ni is treated as the byte array input, so that theﬁnal\noutput embeddings will be normalized into the same length with the\npre-deﬁned learnable query sequence. The aggregation procedure\ncan be formulated as:\nu\ni = ΦperðviÞ2 RP × d, ð6Þ\nwhere ui refers to the aggregated visual embedding,P denotes the\nnumber of learnable queries. Leveraging perceiver architecture, we\ncan map an arbitrary number of patch tokens into the same length,\nsuch that images of different sizes can be treated equally in the fol-\nlowing fusionﬂow.\nMultimodal fusion. To fuse the visual-language information, we\ninterleave the visual embedding with text embeddings from tokeni-\nzation, where the special image placeholder token is simply replaced\nwith the corresponding visual embedding. The resulting interleaved\nsequence is then passed into a decoder-only large language model\n(Φ\nllm), the self-attention transformer layers in LLM can thus naturally\nbe reused as multi-modal fusion modules:\np = Φllmð concatðt1, u1, t2, u2, t3, ... ÞÞ, ð7Þ\nwhereti, ui refer to the text and visual embeddings,p is the probability\ndistribution for the next token.\nTraining procedure. Our training procedure includes two stages,\nnamely, pretraining, and domain-speciﬁc ﬁnetuning, as shown in\nFig. 1b. Note that, all training settings remain identical at two stages,\nwith the only distinction lying in the training data, from generalist to\nradiologic-speciﬁc.\nGenerally, all the data used for model training is listed in Sup-\nplementary Table 2 with citations indicating their sources (those\nwithout citations denoting the data are contributed by this work). For\npretraining, all the listed data are employed. While for domain-speciﬁc\ninstruction tuning, we furtherﬁlter out some relatively low-quality\ndata, i.e., generated data without human veriﬁcation or non-radiology\ndata, focusing more on high-quality question-answering pairs. Next,\nwe will describe this in detail.\nPretraining. At this stage, we use all available data in MedMD as\nlisted in Supplementary Table 3, the main components of the data are\nPMC-Inline and PMC-OA\n31, which are all collected from 2.4M PMC\npapers. These two datasets contain diverse medical vocabularies and\nimages with cutting-edge medical knowledge, however, they are rela-\ntively noisy, so we only use them during pretraining in the hope that\nthe network can accumulate enough knowledge about medical-\nspeciﬁc terminologies and images. Additionally, we also include\nother VQA, captioning, and diagnosis datasets, as they are much\ncleaner.\nDomain-speciﬁc Instruction Tuning.A tt h i ss t a g e ,w ea d o p t\nRadMD for domain-speciﬁc instruction tuning, which contains over3M\nradiologic images, with high-quality language instructions and\nresponses. In this stage, we utilize RadMD for domain-speci ﬁc\ninstruction tuning, which includes over 3M radiological images\naccompanied by high-quality language instructions and responses.\nNotably, weﬁlter out PMC-Inline and PMC-OA, as these datasets are\nnot derived from real clinical scenarios. For the remaining data sour-\nces, we primarilyﬁlter out non-radiology-related content. Speciﬁcally,\nthe ﬁltering process targets the MPx-series, RP3D-series, and PMC-\nCaseReport datasets. For both MPx-series and RP3D-series, theﬁlter-\ning is straightforward since the original websites provide related\nimaging modalities for each case. For PMC-CaseReport, which is gen-\nerated from the case reports subset of PMC-Inline using ChatGPT, we\nrely on the image captions toﬁlter the cases. Only those with captions\nexplicitly mentioning radiology-related terms— such as “MRI”, “CT”,\n“X-ray”, “ultrasound”,o r“mammography”— are retained. We acknowl-\nedge that some noisy cases may still remain in the dataset. Therefore,\nin our evaluation dataset, RadBench, the selected test cases undergo\nadditional manual inspection to further ensure quality.\nTraining details. Image preprocessing. To dismiss the differences of\nmedical images in different modalities, certain preprocessing steps are\napplied. Speciﬁcally, (i) to align the intensity distributions, we employ\nmin-max normalization of all images; (ii) given that medical images can\nexist in either 3D or 2D formats (such as MRI being 3D and X-ray being\n2D), we convert all 2D images to 3D simply by expanding an extra\ndimension. Consequently, all images, irrespective of their original\nformat, can be processed uniformly as 3D images; (iii) to\nensure consistent sizes across all images, we resize them using the\ntorchvision.transforms.Resizefunction. For height and weight\ndimensions, we resize them to 512 × 512 for 2D images and 256 × 256\nfor 3D images because 3D data has more slices, thus taking more\ncomputational memorization. For the depth dimension, since our\nvisual encoder, a 3D vision transformer (ViT), requires the input image\nsizes to be divisible by the patch size of 32 × 32 × 4, we resize the depth\ndimension to the nearest multiple of 4 and will not surpass 64. Please\ncheck the Supplementary Table 6 to obtain more details.\nA detailed forward example. To better illustrate our model\narchitecture, we present a simple instruction tuning example: a radi-\nology image paired with the text prompt“Does the case〈image〉 have\npneumonia?”, with the ground truth response “Yes.” The model\nArticle https://doi.org/10.1038/s41467-025-62385-7\nNature Communications|         (2025) 16:7866 17\nforward procedure will include three main steps, i.e., visual encoding,\ntext fusion, and loss calculation.Visual encoding:A2 Di m a g ei sﬁrst\nexpanded into a pseudo-3D format by adding an extra dimension of\nsize 4. It is then processed by a 3D Vision Transformer (ViT) to produce\nvisual tokens. These are compressed to aﬁxed length of 32 using a\nperceiver module, ensuring consistent input regardless of image size.\nText fusion:The text prompt is tokenized using the LLM’se m b e d d i n g\nlayer, and the“〈image〉 ” placeholder is replaced with the visual tokens.\nThis fused sequence is input to the LLM’s causal self-attention layers\nfor multimodal understanding.Loss calculation:The model predicts\nt h en e x tt o k e n sa u t o - r e g r e s s i v e l y ,a n dt h el o s si sc o m p u t e da g a i n s tt h e\nground truth“Yes”. During pretraining, the same forward process is\nused, but the loss is calculated over all text tokens except the image\nplaceholder, following GPT-style training.\nImplementation. For the visual encoder, we adopt a 12-layer 3D\nViT with 768 feature dimensions and the perceiver is chosen as a six-\nlayer transformer decoder with a learnable latent array in 32 × 5120\ndimensions, so that all images will be embedded as a 32 × 5120 feature\nembedding after passing visual encoding and perceiver aggregation.\nWhen inserting them into the text embedding, we will add two extra\nspecial tokens 〈image〉, 〈/image〉 at the beginning and ending,\nrespectively, to distinguish them from common text tokens. For the\nlarge language model, we initialize it with the MedLLaMA-13B model\nintroduced by PMC-LLaMA\n25,w h i c hh a sf u r t h e rﬁnetuned the LLaMA-\n13B2 model on the medical corpus. Our ﬁnal model has 14B\nparameters.\nIn training, we vary the batch size, i.e., one batch size per device\nfor 3D images and four batch size per device for 2D images with four-\nstep gradient accumulation, and the max token length is set to be\n2048. We totally train the model for eight epochs, four epochs for\npretraining and four epochs for instruction tuning. In theﬁrst one\nepoch, we freeze the language model to align image embedding space\nwith that of texts, in the following epochs, all parameters are updated.\nTo improve the training speed, we adopt FSDP acceleration strategy\n45,\ntogether with automatic mixed precision (AMP) and gradient\ncheckpointing\n46. All models are implemented in PyTorch and trained\non 32 NVIDIA A100 GPUs with 80 GB memory.\nEvaluation\nIn this section, we introduce three evaluation settings, i.e., zero-shot,\nfew-shot and task-speciﬁc evaluation, together with the models in\ncomparison. Note that, theﬁrst two evaluations require no further\ntraining, while the last requires additionalﬁnetuning on speciﬁct a s k s .\nAfterward, we introduce the automatic metrics and human rating\nprogress.\nZero-shot and few-shot evaluation. Foundation models, as a gen-\neralist model, the most appealing characteristic is that they can be\napplied to various tasks just with proper prompting strategies, like\nzero-shot or few-shot prompting, without any speciﬁc training. In the\nzero-shot setting, models will be given task-related semantic instruc-\ntions to indicate which task it is expected to perform, and in the few-\nshot prompting scenario, some similar cases related to the task will be\ngiven instead. The insight of both is to use appropriate textual\ninstructions to prompt the model on what tasks to perform, while\nwhich one is more suitable for a certain model depends on its training\napproach.\nBaselines. For our RadFM, we mainly adopt zero-shot evaluation,\nas in the instruction tuning step, we focus on promoting the model to\nunderstand diverse zero-shot instructions. For other baselines, we\ncompare with the following publicly accessible foundation models\nunder these two settings, as follows:\n OpenFlamingo\n13. This is an open-source implementation of the\nprior state-of-the-art generalist visual-language model Flamingo22,\nthat was trained on large-scale data from general visual-language\ndomain. We utilized the released checkpoint forzero-shot and\nfew-shot evaluation in our study.\n MedVInT8. This is a visual instruction-tuned visual-language\nmodel based on LLaMA2, which was trained on PMC-VQA8.\nConsidering that the PMC-VQA data does not contain any few-\nshot cases, mainly targeting at zero-shot prompting cases, we\ndirectly use the released checkpoint of the MedVInT-TD model\nwith PMC-LLaMA and PMC-CLIP backbone for zero-shot\nevaluation.\n LLaVA-Med\n4. LLaVA-Med is a medical-speciﬁcal vision-language\nfoundation model trained based on LLaVA47 leveraging zero-shot\ninstruction tuning dataset generated from pubmed image-caption\npairs. Similar to MedVInT, it also mainly targets zero-shot\nprompting cases and we directly use the released checkpoint\nLLaVA-Med-v1.5 forzero-shotevaluation.\n Med-Flamingo\n6. This is a multimodal model developed based on\nOpenFlamingo-9B13, that can handles multi-image input interleav-\ning with texts. We use the released checkpoint forzero-shotand\nfew-shot evaluation.\n GPT-4V14. GPT-4V is widely considered as the most powerful\nmulti-modal foundation model, released by OpenAI. Since until\nour submission, GPT-4V can only input 4 images which can hardly\nallow few-shot cases with multiple images, thus we evaluate it in\nzero-shot cases Besides, GPT-4V can be only accessed through\nthe online chatting website, therefore, large-scale auto-evaluation\nis not feasible. In this paper, we only use it for evaluation under the\nhuman rating setting.\nFor OpenFlamingo and Med-Flamingo, we perform both zero-\nshot and few-shot evaluations in our study. Speciﬁcally, we follow the\nprompts derived from the ofﬁcial Med-Flamingo repository. The\nexample prompt for zero-shot evaluation:‘You are a helpful medical\nassistant. Please answer the question about the given image.〈image〉\nQuestion: the query question. Answer:”. In the few-shot setting, we\nexpand upon this format by supplying the models with additional\nexamples to guide their responses. This is structured as follows:“You\nare a helpful medical assistant. You are being provided with images, a\nquestion about the image, and an answer. Follow the examples and\nanswer the last question. 〈image〉 Question: [the ﬁrst question].\nAnswer: [theﬁrst answer].〈 — endofchunk— 〉〈 image〉 Question: [the\nsecond question]. Answer: [the second answer].〈 — endofchunk— 〉\n〈image〉 Question: the query question. Answer:”.\nTo our knowledge, there are currently no existing foundation\nmodels that can effectively handle both 2D and 3D radiology images.\nFor comparison, we have strong baseline models that are publicly\naccessible, for example, OpenFlamingo\n13,M e d V I n T8,L L a V A - M e d4,a n d\nMed-Flamingo6, which have demonstrated efﬁcacy in processing slices\nand making predictions. In addition, we also compare with GPT-\n4V(ision)14 use its online chatting website version.\nDatasets.W ee v a l u a t et h ea b o v ef o u n d a t i o nm o d e l so nR a d B e n c h\nand 9 exising datasets as introduced in section“Radiology evaluation\nbenchmark (RadBench)”. Additionally, we also evaluate them on\nPadChest48. It is a labeled large-scale, high-resolution chest x-ray\ndataset including 160,000 images obtained from 67,000 patients, with\n174 different radiographicﬁnding labels. We dismiss the classes with\ncases fewer than 10 together withthe seen classes appearing in our\ntraining set, resulting in 163 totally unseen classes. We therefore\nensure that not only images, but also categories in the texts never\nappear in the training, which requires more generalization ability of\nmodels.\nTask-speciﬁc evaluation. In addition to directly evaluating different\nfoundation models using zero-shot or few-shot prompting, without\nany training, our model can also serve as a pretrained model, that can\nbe adapted to different speciﬁc tasks by furtherﬁnetuning on its\nArticle https://doi.org/10.1038/s41467-025-62385-7\nNature Communications|         (2025) 16:7866 18\ncorresponding training set, giving up the ability to generalize between\ntasks, but getting better performance on a speciﬁct a s k .I ns u c hac a s e ,\nwe compare ourﬁnal results with different task-speciﬁc state-of-the-\narts (SOTAs) according to the related datasets. In detail, we use the\nfollowing datasets, and the corresponding SOTAs for comparison are\nlisted in Table3 with citations:\n VinDr-Mammo49 is a mammography diagnosis dataset compris-\ning 20,000 images (5000 four-view scans). Each scan was\nmanually annotated with aﬁve-level BI-RADS score. We view this\nas a multi-class classiﬁcation task with the ofﬁcial split following\nthe BenchMD\n50.\n CXR1451 is a widely-used chest X-ray diagnosis dataset containing\n112,120 frontal-view X-ray images of 30,805 (collected from the\nyear of 1992 to 2015) unique patients with 14ﬁnding labels. We\nfollow its ofﬁcial split and evaluate the SOTA\n52 on the split.\n LDCT53 Low dose computed tomography (LDCT) is a procedure\nthat uses an x-ray machine linked with a computer to create 3D\nimages of a patient’s tissues and organs. LIDC-IDRI\n53 dataset is\nused here, containing 1018 low-dose lung CTs, where each CT has\nsmall/large/no nodule labels. We follow BenchMD\n50 to set this\ndataset as a 3D diagnosis task and split it follow BenchMD.\n MosMedData54 is a set of 1110 3D CT cases labeled with COVID-19\nrelated ﬁndings, as well as without suchﬁndings. We view it as a\nclassiﬁcation task and split it randomly with 8:2 for training and\ntesting following54.\n COVID-CT55 is a set of 349 2D CT slices labeled with COVID-19\ncollected from 216 patients. We split it randomly with an 8:2 ratio\nfor training and testing.\n BraTs201932 is an MRI dataset with four MRI modalities T1WI,\nT2WI, T2FLAIR, and T1 contrast-enhanced(T1CE). There are 259\nvolumes of high-grade glioma (HGG) and 73 volumes of low-grade\nglioma (LGG). We follow the setting as DSM\n56 that uses T1CE to\ndiagnose the HGG or LGG. Due to the original paper did not\nrelease their splits we randomly split the dataset following 7:3 for\ntraining and testing and re-tested the SOTA on it.\n ADNI (Alzheimer’s disease neuroimaging initiative)\n57 is a large\ncollection alzheimer’s disease dataset with 3D brain MRI scans. We\nfollow the setting introduced in ref.58 and split it randomly 8:2 for\ntraining and testing.\n BTM-17 (Brain-tumor-17)59 is a challenge about classifying an MRI\ncase into 17 tumor types, with 4449 real images. We adopt its\nofﬁcial split.\n Lung-PET-CT-Dx\n60 consists of CT and PET-CT DICOM images of\n355 lung cancer subjects. We treat it as a diagnosis dataset to\nfurther distinguish whether one patient is diagnosed with\nAdenocarcinoma, small cell carcinoma, large cell carcinoma, or\nsquamous cell carcinoma. Considering its limited case number,\nwe split it with 7:3 (train:test) to ensure enough cases for\nevaluation.\n VQA-RAD\n32 is a radiology VQA dataset containing 3515 questions\nwith 517 possible answers. We follow the ofﬁcial dataset split for\nour evaluation.\n SLAKE33 is an English-Chinese medical VQA dataset composed of\n642 images and 14K questions. There are 224 possible answers in\ntotal. We only use the“English” part, and follow the ofﬁcial split.\n PMC-VQA\n8 is an English medical VQA dataset generated with\nauto-nlp methods containing 149K images with 227K questions.\nIts answers are diverse for different questions. Considering its test\nset is also auto-generated, we have manually cleaned it as\nmentioned in section “Radiology Evaluation Benchmark (Rad-\nBench)” and retest the SOTA MedVInt\n8 checkpoint on the cleaned\ntest set.\n MedDiffVQA61 is a large-scale dataset for difference medical\nVQA (involving historical comparison) in medical chest x-ray\nimages with 700,703 pairs of question-answer. We follow its\nofﬁcial split.\n IU-X-ray62 is a set of chest X-ray images paired with clinical\nreports. The dataset contains 7470 pairs of images and reports.\nWe follow the setting and split as CDGPT263 where we use a single-\nview image to generate the reports.\nEvaluation metrics. Machine rating. We evaluate on four distinct\ntasks, e.g., disease diagnosis, visual question answering, report gen-\neration and rationale diagnosis. The details of the four tasks and\nautomatic metrics are introduced in section“Radiology Evaluation\nBenchmark (RadBench)”.T oe v a l u a t et h em o d e l’s performance across\na range of tasks, distinct evaluation metrics are employed based on the\ntask type. For tasks with pre-deﬁned answer choices, such as disease\ndiagnosis, we adopted standard metrics developed in the community,\nfor example, F1 stands for“F1 score”,a n dA C Cs t a n d sf o r“Accuracy”.\nConversely, for tasks involving open-ended responses, like report\ngeneration and visual question answering (VQA) and rationale diag-\nnosis, alternative evaluation metrics, like BLEU, ROUGE and BERT-sim\nare employed. BLEU stands for“BiLingual Evaluation Understudy”\n64,\nROUGE stands for “Recall-Oriented Understudy for Gisting\nEvaluation”65.B E R T - s i ms t a n d sf o r“BERT similarity score”,t h eF 1B E R T\nscore between the generated answer and the correct answer66.F o r\nBLEU and ROUGE, if not speciﬁc pointing, we all use 1-gram by default.\nIn addition, inspired by the score RadCliQ12 designed speciﬁcally\nfor evaluating generated chest X-ray reports, we also propose two new\nmetrics, UMLS_Precision and UMLS_Recall, which aim to measure the\noverlapping ratio of medical-related words between ground truth and\npredicted response. Speciﬁcally, given a pair of ground-truth and\nprediction, we extract the medical-related words from them by using\nuniﬁed medical language system (UMLS)\n43, and count the overlap\nwords as true-positive. UMLS_Precision is deﬁned with the classical\nprecision concept, i.e., the number of true-positive divides the whole\ngenerated medical-related word number. On the other hand, UMLS_-\nRecall also follows the recall concept, i.e., the number of true-positive\nwords divides the total number of medical-related words in the\nground truth.\nDiscussion on automatic metrics. Despite these automatic\nm e t r i c sh a v eb e e nw i d e l ya d o p t e db yt h ec o m m u n i t y ,t h e yo f t e n\nstruggle to capture the semantic accuracy in generative tasks, for\nexample, question answering, report generation, and rationale gen-\neration. To address these limitations and ensure a more accurate\nevaluation of system performance, we incorporate human evaluation,\nleveraging the expertise of radiologists, to get a professional evalua-\ntion on the quality of generated answers.\nHuman rating. For the sake of clinical utility, we further involve\nmanual checking in the evaluation stage and compute the human\nrating score. Three radiologists were asked to rate the quality of the\ngenerated answers using a 0– 5 scale. Each radiologist hasﬁve years of\nclinical experience in radiology departments. One is afﬁliated with\nShanghai General Hospital, and the other two are from Shanghai Sixth\nPeople’s Hospital. All three completed their studies in“Medical ima-\nging and nuclear medicine” at Shanghai Jiao Tong University. Here are\nthe speciﬁcs of each rating:\n1. Garbled - The content is incomprehensible and lacks any\nreadability.\n2. Inaccurate- While readable, the content is entirely incorrect and\nlacks meaningful information.\n3. Partially informative- The content holds some reference value,\nyet its correctness is subpar.\n4. Moderately accurate- The content provides reference points,\nwith approximately half of the information being correct, but\ncontaining several errors.\nArticle https://doi.org/10.1038/s41467-025-62385-7\nNature Communications|         (2025) 16:7866 19\n5. Mostly accurate- The content is almost entirely correct, with\nonly a few omissions or errors present.\n6. Completely correct- The content is accurate in its entirety,\nwithout any mistakes.\nTo facilitate this assessment, we have developed a human\nevaluation interface, visually presenting the generative instances\nwith images, as depicted in Supplementary Fig. 2. Prior to the full\nevaluation, we conducted a preliminary exam involving 20 ran-\ndomly sampled test cases. This exam was designed to ensure that\nthe radiologists understood the evaluation criteria. All three radi-\nologists showed consistent results, with one exception: for one\ncase, one radiologist rated the answer as 2 while the others rated it\nas 3. This indicates that ourﬁve-point rating system was sufﬁciently\nclear for evaluating the model’s outputs. The exam results were also\nreviewed by a senior radiologist with over 10 years of experience\nfrom the radiology department of Shanghai Sixth People’s Hospital,\nfurther conﬁrming the validity of the evaluation process.In the\nevaluation, raters are provided with images, the question, the\ncorrect answer, and a set of generated responses from different\nmodels, arranged in a randomized order. The evaluation score\ngiven by the professional radiologists differs from the automatic\nevaluation metrics, offering greater accuracy andﬂexibility. In the\ncontext of the report generation example shown in theﬁgure, they\nfocus on the most crucial aspects, rather than solely on word\nmatching, recall or precision.\nNote that, human rating is only performed for the open-ended\ntasks, i.e., medical VQA, report generation and rationale diagnosis. As\nfor disease diagnosis, their answers areﬁxed without confusion; thus,\nthe automatic metrics can already well reﬂect the performance. Con-\nsidering the cost for human rating, for each open-ended task, we\nrandomly sample 400 test cases from RadBench, as they are generally\ncollected from clinical practice across the world, and can represent\nreal scenarios, resulting in1.2K cases for human rating in total.\nReporting summary\nFurther information on research design is available in the Nature\nPortfolio Reporting Summary linked to this article.\nData availability\nAll data used in this study can be downloaded via the links provided in\nSupplementary Table 5. Most datasets can be directly downloaded\nfrom the listed websites and used for non-commercial purposes. For\nthe RP3D and MPx datasets, due to licensing restrictions, we cannot\nrelease the original data directly. However, links to the ofﬁcial websites\nare provided. For MPx, please ensure you review the ofﬁcial MedPix\nlicenseand mail us to request the download link to the data. For RP3D,\nusers can contact the ofﬁcial Radiopaedia licensing team at licen-\nse@radiopaedia.org to obtain usage approval. Once approved, the\nconﬁrmation can be shared with us, and we will provide the detailed\ndata download link. Commonly, we will respond to inquiries regarding\nthe two datasets within 3–5 business days. Mostﬁgures in this paper\ninclude detailed numerical annotations; the only exception is Fig.4,f o r\nwhich the data is provided in the Source Dataﬁle. Source data are\nprovided with this paper.\nCode availability\nThe code is available on GitHub athttps://github.com/chaoyi-wu/\nRadFM67.\nReferences\n1. Bommasani, R. et al. On the opportunities and risks of foundation\nmodels. Preprint at arXiv:2108.07258 (2021).\n2. Touvron, H. et al. Llama: Open and efﬁcient foundation language\nmodels. Preprint at arXiv:2302.13971 (2023).\n3. Li, J., Li, D., Savarese, S. & Hoi, S. Blip-2: bootstrapping language-\nimage pre-training with frozen image encoders and large language\nmodels. InProc. 40th International Conference on Machine Learning\n19730– 19742 (PMLR, 2023).\n4. Li, C. et al. Llava-med: Traininga large language-and-vision assis-\ntant for biomedicine in one day. InProc. 37th International Con-\nference on Neural Information Processing Systems28541– 28564\n(NeurIPS, 2023).\n5. Moor, M. et al. Foundation models for generalist medical artiﬁcial\nintelligence.Nature 616,2 5 9– 265 (2023).\n6 . M o o r ,M .e ta l .M e d -ﬂamingo: a multimodal medical few-shot\nlearner. InProc. 3rd Machine Learning for Health Symposium\n353– 367 (PMLR, 2023).\n7 . T u ,T .e ta l .T o w a r d sg e n e r a l i s tb i o m e d i c a lA I .NEJM AI1,\nAIoa2300138 (2024).\n8. Zhang, X. et al. Development of a large-scale medical visual\nquestion-answering dataset.Commun. Med.4,2 7 7( 2 0 2 4 ) .\n9. Tiu, E. et al. Expert-level detection of pathologies from unannotated\nchest x-ray images via self-supervised learning.Nat. Biomed. Eng.6,\n1399– 1406 (2022).\n10. Zhang, X., Wu, C., Zhang, Y., Wang, Y. & Xie, W. Knowledge-\nenhanced pre-training for auto-diagnosis of chest radiology ima-\nges. Nat. Commun.14,4 5 4 2( 2 0 2 3 ) .\n11. Monshi, MaramMahmoudA., Poon, J. & Chung, V. Deep learning in\ngenerating radiology reports: a survey.Artif. Intell. Med.106,\n101878 (2020).\n12. Yu, F. et al. Evaluating progress in automatic chest x-ray radiology\nreport generation.Patterns4,1 0 0 8 0 2( 2 0 2 3 ) .\n13. Awadalla, A. et al. OpenFlamingo: an open-source framework for\ntraining large autoregressive vision-language models. Preprint at\narXiv 2308.01390 (2023).\n14. OpenAI (2023). Chatgpt can now see, hear, and speak.https://\nopenai.com/blog/chatgpt-can-now-see-hear-and-speak(2023).\n1 5 . P e n g ,B ,L i ,C . ,H e ,P . ,G a l l e y ,M .&G a o ,J .I n s t r u c t i o nt u n i n gw i t h\ngpt-4. Preprint at arXiv:2304.03277 (2023).\n16. OpenAI. Gpt-4 technical report. Preprint at arXiv abs/\n2303.08774 (2023).\n17. Wang, Y. et al. Super-naturalinstructions: generalization via\ndeclarative instructions on 1600+ NLP tasks. InProc. 2022\nConference on Empirical Methods in Natural Language Pro-\ncessing 5085– 5109 (Association for Computational Linguis-\ntics, 2022).\n18. Hamamci, I. E. et al. Generatect: text-conditional generation of 3D\nchest CT volumes. InProc. European Conference on Computer\nVision (ECCV)126– 143 (Springer, 2024).\n19. Wang, W. et al. When an image is worth 1,024 x 1,024 words: a case\nstudy in computational pathology. Preprint at\narXiv:2312.03558 (2023).\n20. Kaplan, J. et al. Scaling laws for neural language models. Preprint at\narXiv:2001.08361 (2020).\n21. Anil, R. et al. PaLM 2 technical report. Preprint at\narXiv:2305.10403 (2023).\n22. Alayrac, J.-B. et al. Flamingo: a visual language model for few-shot\nlearning. InProc. 36th International Conference on Neural Informa-\ntion Processing Systems23716– 23736 (NeurIPS, 2022).\n23. Singhal, K. et al. Towards expert-level medical question\nanswering with large language models.Nat. Med. 31,\n943– 950 (2025).\n24. Singhal, K. et al. Large language models encode clinical knowl-\nedge. Nature 620,1 7 2– 180 (2023).\n25. Wu, C., Zhang, X., Zhang, Y., Xie, W. & Wang, Y. PMC-LLaMA: further\nﬁnetuning LLaMA on medical papers.J. Am. Med. Inform.\nAssoc. (2023).\n26. Chen, Z. et al. Meditron-70b: scaling medical pretraining for large\nlanguage models. Preprint at arXiv:2311.16079 (2023).\nArticle https://doi.org/10.1038/s41467-025-62385-7\nNature Communications|         (2025) 16:7866 20\n27. Zhu, W. et al. Multimodal C4: an open, billion-scale corpus of\nimages interleaved with text. InProc. 37th International Con-\nference on Neural Information Processing Systems8958– 8974\n(NeurIPS, 2023).\n28. Krishna, R. et al. Visual genome: connecting language and vision\nusing crowdsourced dense image annotations.Int. J. Comput. Vis.\n123,3 2– 73 (2017).\n29. Schuhmann, C. et al. Laion-5b: an open large-scale dataset for\ntraining next generation image-text models.Adv. Neural Inf. Pro-\ncess. Syst.35,2 5 2 7 8– 25294 (2022).\n30. Johnson, AlistairE. W. et al. Mimic-cxr, a de-identiﬁed publicly\navailable database of chest radiographs with free-text reports.Sci.\nData 6, 317 (2019).\n31. Lin, W. et al. Pmc-clip: contrastive language-image pre-training\nusing biomedical documents. InMedical Image Computing and\nComputer Assisted Intervention (MICCAI) 2023.( S p r i n g e r ,\n2023).\n32. Lau, J. J., Gayen, S., Ben Abacha, A. & Demner-Fushman, D. A\ndataset of clinically generated visual questions and answers about\nradiology images.Sci. Data5,1 – 10 (2018).\n33. Liu, B. et al. Slake: a semantically-labeled knowledge-enhanced\ndataset for medical visual question answering. In2021 IEEE 18th\nInternational Symposium on Biomedical Imaging (ISBI)1650– 1654\n(IEEE, 2021).\n34. Zhang, K. et al. A generalist vision– language foundation model for\ndiverse biomedical tasks.Nat. Med.30,3 1 2 9– 3141 (2024).\n35. Chen, Z. et al. Chexagent: towards a foundation model for chest\nx-ray interpretation. Preprint at arXiv:2401.12208 (2024).\n36. Chen, J. et al. Huatuogpt-vision, towards injecting medical visual\nknowledge into multimodal llms at scale. Preprint at\narXiv:2406.19280 (2024).\n37. He, S. et al. Meddr: diagnosis-guided bootstrapping for large-scale\nmedical vision-language learning. Preprint at arXiv:2404.1527\n(2024).\n38. Hamamci, I. E. et al. A foundation model utilizing chest CT volumes\nand radiology reports for supervised-level zero-shot detection of\nabnormalities. Preprint at arXiv:2403.17834v1 (2024).\n39. Lu, H. et al. Deepseek-vl: towards real-world vision-language\nunderstanding. Preprint at arXiv:2403.05525 (2024).\n40. Wang, P. et al. Qwen2-vl: enhancing vision-language model’sp e r -\nception of the world at any resolution. Preprint at arXiv:2409.12191\n(2024).\n41. Ding, J. et al. Longnet: scaling transformers to 1,000,000,000\ntokens. InProc. 10th International Conference on Learning Repre-\nsentations(ICLR, 2023).\n4 2 . Z h a o ,Z . ,J i n ,Q . ,C h e n ,F . ,P e n g ,T .&Y u ,S .Al a r g e - s c a l ed a t a s e to f\npatient summaries for retrieval-based clinical decision support\nsystems.Sci. Data10, 909 (2023).\n43. Bodenreider, O. The uniﬁed medical language system (umls):\nintegrating biomedical terminology.Nucleic Acids Res.32,\nD267– D270 (2004).\n44. Jaegle, A. et al. Perceiver: general perception with iterative atten-\ntion. InInternational Conference on Machine Learning\n4651– 4664\n(PMLR, 2021).\n45. Zhao, Y. et al. Pytorch fsdp: experiences on scaling fully sharded\ndata parallel.Proc. VLDB Endow.16,3 8 4 8– 3860 (2023).\n46. Chen, T., Xu, B., Zhang, C. & Guestrin, C. Training deep nets\nwith sublinear memory cost. Preprint at\narXiv:1604.06174 (2016).\n47. Liu, H., Li, C., Wu, Q. & Lee, Y. J. Visual instruction tuning. InProc.\n37th International Conference on Neural Information Processing\nSystems 34892– 34916 (NeurIPS, 2023).\n48. Bustos, A., Pertusa, A., Salinas, JoseeMaría & de la Iglesia-Vayá,\nMaría Padchest: a large chest x-ray image dataset with multi-label\nannotated reports.Med. Image Anal.66,1 0 1 7 9 7( 2 0 1 9 ) .\n49. Nguyen, H. T. et al. Vindr-mammo: a large-scale benchmark dataset\nfor computer-aided diagnosis in full-ﬁeld digital mammography.\nSci. Data10,2 7 7( 2 0 2 3 ) .\n50. Wantlin, K. et al. Benchmd: a benchmark for modality-agnostic\nlearning on medical images and sensors. Preprint at\narXiv:2304.08486v2 (2023).\n51. Wang, X. et al. Chestx-ray8: hospital-scale chest x-ray database and\nbenchmarks on weakly-supervised classiﬁcation and localization of\ncommon thorax diseases. InProc. IEEE Conference on Computer\nVision and Pattern Recognition2097– 2106 (IEEE, 2017).\n52. Wu, C., Zhang, X., Zhang, Y., Wang, Y. & Xie, W. Medklip: medical\nknowledge enhanced language-image pre-training. InProc. IEEE\nInternational Conference on Computer Vision (ICCV)(IEEE, 2023).\n53. Armato III, S. G. et al. The lung image database consortium (lidc) and\nimage database resource initiative (idri): a completed reference\ndatabase of lung nodules on ct scans.Med. Phys.38,9 1 5– 931 (2011).\n54. Morozov, S. P. et al. Mosmeddata: chest CT scans with covid-19\nrelated ﬁndings dataset. Preprint at arXiv:2005.06465 (2020).\n55. Zhao, J., Zhang, Y., He, X. & Xie, P. Covid-ct-dataset: a ct scan\ndataset about covid-19. Preprint at arXiv:2003.13865 (2020).\n56. Dosovitskiy, A. et al. An image is worth 16 × 16 words: transformers\nfor image recognition at scale. InProc. 9th International Conference\non Learning Representations(ICLR, 2021).\n57. Petersen, RonaldCarl et al. Alzheimer’s disease neuroimaging\ninitiative (adni) clinical characterization.Neurology74,2 0 1– 209\n(2010).\n58. Korolev, S., Saﬁu l l i n ,A . ,B e l y a e v ,M .&D o d o n o v a ,Y .R e s i d u a la n d\nplain convolutional neural networks for 3d brain mri classiﬁcation.\nIn 2017 IEEE 14th International Symposium on Biomedical Imaging\n(ISBI 2017)835– 838 (IEEE, 2017).\n59. Feltrin, F. Brain tumor MRI images 17 classes. Kagglehttps://www.\nkaggle.com/datasets/fernando2rad/brain-tumor-mri-images-17-\nclasses (2024).\n60. Pam, A. & Tracy, N. A large-scale ct and pet/ct dataset for lung\ncancer diagnosis (lung-pet-ct-dx).Cancer Imaging Archive(2021).\n61. Hu, X. et al. Expert knowledge-aware image difference graph\nrepresentation learning for difference-aware medical visual ques-\ntion answering. InProc. 29th ACM SIGKDD Conference on Knowl-\nedge Discovery and Data Mining4156– 4165 (Association for\nComputing Machinery, 2023).\n62. Demner-Fushman, D. et al. Preparing a collection of radiology\nexaminations for distribution and retrieval.J. Am. Med. Inform.\nAssoc. 23,3 0 4– 310 (2016).\n6 3 . A l f a r g h a l y ,O . ,K h a l e d ,R . ,E l k o r a n y ,A . ,H e l a l ,M .&F a h m y ,A .\nAutomated radiology report generation using conditioned trans-\nformers.Inform. Med. Unlocked24, 100557 (2021).\n64. Papineni, K., Roukos, S., Ward, T. & Zhu, W.-J. Bleu: a method for\nautomatic evaluation of machine translation. InProc. 40th annual\nmeeting of the Association for Computational Linguistics311– 318\n(Association for Computational Linguistics, 2002).\n65. Lin, C.-Y. Rouge: a package for automatic evaluation of summaries.\nIn Text Summarization Branches Out74– 81 (Association for Com-\nputational Linguistics, 2004).\n66. Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q. & Artzi, Y. Bertscore:\nevaluating text generation with bert. InProc. 8th International\nConference on Learning Representations(ICLR, 2020).\n67. Wu, C. & Zhang, X. chaoyi-wu/radfm: Radfm_ofﬁcial_code (2025).\n68. Chatterjee, S., Nizamani, FarazAhmed, Nürnberger, A. & Speck, O.\nClassiﬁcation of brain tumours in mr images using deep spatios-\npatial models.Sci. Rep.12, 1505 (2022).\n69. Bazi, Y., Rahhal, MohamadMahmoudAl, Bashmal, L. & Zuair, M.\nVision– language model for visual question answering in medical\nimagery.Bioengineering10,3 8 0( 2 0 2 3 ) .\n70. van Sonsbeek, T., Derakhshani, M. M., Najdenkoska, I., Snoek, C. G.\nM. & Worring, M. Open-ended medical visual question answering\nArticle https://doi.org/10.1038/s41467-025-62385-7\nNature Communications|         (2025) 16:7866 21\nthrough preﬁx tuning of language models. InProc. Medical Image\nComputing and Computer Assisted Intervention (MICCAI)726– 736\n(Springer, 2023).\nAcknowledgements\nThis work is supported by the National Key R&D Program of China (No.\n2022ZD0160702), STCSM (No. 22511106101, No. 18DZ2270700, and No.\n21DZ1100100), 111 plan (No. BP0719010), and State Key Laboratory of\nUHD Video and Audio Production and Presentation. All the radiology\nexamples used in ourﬁgures are obtained from the Radiopaedia web-\nsite. We sincerely acknowledge their invaluable efforts.\nAuthor contributions\nAll listed authors clearly meet the ICMJE 4 criteria. C.W. and X.Z. con-\ntribute equally to this work, and Y.W. and W.X. are the corresponding\nauthors. Speciﬁcally, C.W., X.Z., Y.Z., H.H., Y.W., and W.X. all make\ncontributions to the conception or design of the work, and C.W. and X.Z.\nfurther perform acquisition, analysis, or interpretation of data for the\nwork. In writing, C.W. and X.Z. draft the work and Y.Z., H.H., Y.W., and\nW.X. review it critically for important intellectual content. All authors\napprove of the version to be published and agree to be accountable for\nall aspects of the work in ensuring that questions related to the accuracy\nor integrity of any part of the work are appropriately investigated and\nresolved.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41467-025-62385-7.\nCorrespondenceand requests for materials should be addressed to\nYanfeng Wang or Weidi Xie.\nPeer review informationNature Communicationst h a n k s ,S y n h oD o ,a n d\nthe other, anonymous, reviewer(s) for their contribution to the peer\nreview of this work. A peer reviewﬁle is available.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jur-\nisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the\nCreative Commons licence, and indicate if you modiﬁed the licensed\nmaterial. You do not have permission under this licence to share adapted\nmaterial derived from this article or parts of it. The images or other third\nparty material in this article are included in the article’s Creative\nCommons licence, unless indicatedotherwise in a credit line to the\nmaterial. If material is not included in the article’s Creative Commons\nlicence and your intended use is not permitted by statutory regulation or\nexceeds the permitted use, you will need to obtain permission directly\nfrom the copyright holder. To view a copy of this licence, visithttp://\ncreativecommons.org/licenses/by-nc-nd/4.0/.\n© The Author(s) 2025\nArticle https://doi.org/10.1038/s41467-025-62385-7\nNature Communications|         (2025) 16:7866 22",
  "topic": "Foundation (evidence)",
  "concepts": [
    {
      "name": "Foundation (evidence)",
      "score": 0.744026243686676
    },
    {
      "name": "Generalist and specialist species",
      "score": 0.6276752948760986
    },
    {
      "name": "Computer science",
      "score": 0.5798467993736267
    },
    {
      "name": "Scale (ratio)",
      "score": 0.5414704084396362
    },
    {
      "name": "Data science",
      "score": 0.5229607224464417
    },
    {
      "name": "Computational biology",
      "score": 0.3462023437023163
    },
    {
      "name": "World Wide Web",
      "score": 0.3377543091773987
    },
    {
      "name": "Medicine",
      "score": 0.32921943068504333
    },
    {
      "name": "Biology",
      "score": 0.2245957851409912
    },
    {
      "name": "Ecology",
      "score": 0.17593562602996826
    },
    {
      "name": "Geography",
      "score": 0.12982481718063354
    },
    {
      "name": "Archaeology",
      "score": 0.09096989035606384
    },
    {
      "name": "Cartography",
      "score": 0.08022278547286987
    },
    {
      "name": "Habitat",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4391012619",
      "name": "Shanghai Artificial Intelligence Laboratory",
      "country": null
    },
    {
      "id": "https://openalex.org/I4210100255",
      "name": "Beijing Academy of Artificial Intelligence",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    }
  ]
}