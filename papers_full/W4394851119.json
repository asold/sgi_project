{
  "title": "Image Classification Based on Vision Transformer",
  "url": "https://openalex.org/W4394851119",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5113179359",
      "name": "Attiapo Acybah Morel Omer",
      "affiliations": [
        "Hubei University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W2194775991"
  ],
  "abstract": "This research introduces an innovative approach to image classification, by making use of Vision Transformer (ViT) architecture. In fact, Vision Transformers (ViT) have emerged as a promising option for convolutional neural networks (CNN) for image analysis tasks, offering scalability and improved performance. Vision transformer ViT models are able to capture global dependencies and link among elements of images. This leads to the enhancement of feature representation. When the ViT model is trained on different models, it demonstrates strong classification capabilities across different image categories. The ViT's ability to process image patches directly, without relying on spatial hierarchies, streamlines the classification process and improves computational efficiency. In this research, we present a Python implementation using TensorFlow to employ the (ViT) model for image classification. Four categories of animals such as (cow, dog, horse and sheep) images will be used for classification. The (ViT) model is used to extract meaningful features from images, and a classification head is added to predict the class labels. The model is trained on the CIFAR-10 dataset and evaluated for accuracy and performance. The findings from this study will not only demonstrate the effectiveness of the Vision Transformer model in image classification tasks but also its potential as a powerful tool for solving complex visual recognition problems. This research fills existing gaps in knowledge by introducing a novel approach that challenges traditional convolutional neural networks (CNNs) in the field of computer vision. While CNNs have been the dominant architecture for image classification tasks, they have limitations in capturing long-range dependencies in image data and require hand-designed hierarchical feature extraction.",
  "full_text": "Journal of Computer and Communications, 2024, 12, 49-59 \nhttps://www.scirp.org/journal/jcc \nISSN Online: 2327-5227 \nISSN Print: 2327-5219 \n \nDOI: 10.4236/jcc.2024.124005  Apr. 15, 2024 49 Journal of Computer and Communications \n \n \n \n \nImage Classification Based on Vision  \nTransformer \nAttiapo Acybah Morel Omer \nDepartment of Computer Science, Hubei University of Technology, Wuhan, China \n  \n \n \nAbstract \nThis research introduces an innovative approach to image classification, by \nmaking use of Vision Transform er (ViT) architecture. In fact , Vision Trans-\nformers (ViT) have emerged as a promising option for  convolutional neural \nnetworks (CNN) for image analysis tasks, offering scalability and improved \nperformance. Vision transformer ViT models are able to capture global d e-\npendencies and link am ong elements of images. This leads to the enhanc e-\nment of feature representation. When the  ViT model is trained on different \nmodels, it demonstrates strong classification capabilities across different i m-\nage categories. The ViT ’s ability to process image pat ches directly, without \nrelying on spatial hierarchies, streamlines the classification process and i m-\nproves computational efficiency. In this research, we present a Python i m-\nplementation using TensorFlow to employ the (ViT) model for image classi-\nfication. Four categories of animals such as (cow,  dog, horse and sheep) i m-\nages will be used for classification. The (ViT) model is used to extract me a-\nningful features from images, and a class ification head is added to predict the \nclass labels. The model is trained on the CIFAR -10 dataset and evaluated for \naccuracy and performance. The findings from this study will not only demon-\nstrate the effectiveness of the Vision Transformer model in image classific a-\ntion tasks but also its potential as a powerful tool for solving complex visual \nrecognition problems. This research fills existing gaps in knowledge by i n-\ntroducing a novel approach that challenges traditional convolutional neural \nnetworks (CNNs) in the field of computer vision. While CNNs have been the \ndominant architecture for image classification tasks, they have limitations in \ncapturing long-range dependencies in image data and require hand -designed \nhierarchical feature extraction. \n \nKeywords \nConvolutional Neural Networks, ViT, CNN, Deep Learning, Architecture \nHow to cite this paper: Omer, A.A.M. \n(2024) Image Classification Based on V i-\nsion Transformer . Journal of Computer \nand Communications, 12, 49-59. \nhttps://doi.org/10.4236/jcc.2024.124005 \n \nReceived:  March 21, 2024 \nAccepted: April 12, 2024 \nPublished: April 15, 2024 \n \nCopyright © 2024 by author(s) and  \nScientific Research Publishing Inc. \nThis work is licensed under the Creative \nCommons Attribution International  \nLicense (CC BY 4.0). \nhttp://creativecommons.org/licenses/by/4.0/   \n  \nOpen Access\nA. A. M. Omer \n \n \nDOI: 10.4236/jcc.2024.124005 50 Journal of Computer and Communications \n \n1. Introduction \nImage classification represents a crucial task in the field of computer vision. Its  \napplications vary from autonomous vehicles to medical diagnostics. It is impo r-\ntant to mention that convolutional neural networks (CNNs) have been the most \npopular approach for image classification tasks, as it is able to achieve  remarka-\nble success in many benchmarks (Dos Santos, 2021) [1] and (Touvron, 2021) [2]. \nDespite this fact, the advancements in deep learning have presented a novel \narchitecture known as the Vision Transformer (ViT). In fact, vision transformer \nhas shown tremendous positive outcomes in image classification tasks (Kriz-\nhevsky, 2012) [3]. The Vision Transformer model (ViT), introduced by Dos o-\nvitskiy (2021) [4], illustrates a transformer architecture inspired by the success of \ntransformers in natural language processing tasks. This model does not rely on \nconvolutional layers, instead the ViT model processes images as sequences of \nflattened patches, which allows it to capture long -range dependencies in the im-\nage data. This eliminates the need for hand -designed hierarchical feature extrac-\ntion, enabling the model to learn representations straight from raw pixel values. \nThe vision transformer (ViT) model has been the  subject of tremendous a t-\ntention in the research community for its ability to achieve competitive perfo r-\nmance on standard image classification benchmarks.  Touvron (2020) [5] de-\nmonstrates this in his comparative study, by mentioning that the ViT model \ndemonstrated superior performance on image classification tasks compared to \ntraditional CNN architectures, showcasing its potential as a disruptive techno l-\nogy in the field of computer vision. \nThis research aims to  explore the effectiveness of the Vision Transformer \nmodel in image classification tasks using Python with TensorFlow. By exploring \nthe capabilities of the Vision transformer (ViT) architecture, we are looking to \ndemonstrate its performance on a real- world dataset and compare it against e s-\ntablished CNN models.  \n2. Evolution of Deep Learning Models for Image  \nClassification \nThis section provides insight on the evolution of deep learning models for image \nclassification tasks. Deep learning models have experienced tremendous evolu-\ntion over the recent years in the field of image classification. Researchers pr o-\nposed innovative architectures and  algorithms to ameliorate performance and \naccuracy. The evolution of deep learning models for image classification can be \ntraced back to the seminal work on Convolutional Neural Networks (CNNs), \nwhich laid the foundation for modern deep learning in compute r vision. CNN \nmodel, also known as known as LeNet -5, was introduced by  (LeCun, 1998) [6] . \nThe LeNet -5 architecture involves many layers.  These layers include convolu-\ntional layers, pooling layers, and fully connected layers. Using CNN convol u-\ntional operations allows the network to capture spatial hierarchies in the input \nimage, while pooling layers help reduce spatial dimensions and extract dominant \nA. A. M. Omer \n \n \nDOI: 10.4236/jcc.2024.124005 51 Journal of Computer and Communications \n \nfeatures. This was then followed by LeNet-5.  \nIn fact,  the AlexNet model introduced by (Krizhevsky, 2012) [3] marked a \nnew era in image classification performance. AlexNet involves deeper and wider \nneural networks, incorporating techniques such as data augmentation, dropout \nregularization, and ReLU activation functions to improve accuracy.  \nIt is also important to mention that the network architecture of AlexNet i n-\nvolves multiple convolutional layers with varying filter sizes and strides, as well \nas max-pooling layers for sampling. Progression in deep  learning has enhanced \nthe development of models such as VGGNet, GoogLeNet, and ResNet, which are \ncomposed of  unique architectural innovations to enhance performance. \nVGGNet, proposed by Simonyan and Zisserman (2014),  introduced a simplified \narchitecture with multiple stacked convolutional layers. Additionally, GoogL e-\nNet, created by (Szegedy, 2014) [7] , involves inception modul es composed of \nparallel convolutional operations of varying kernel sizes The progression of deep \nlearning models for image classification has been characterized by constant e x-\nploration of novel architectures, optimization techniques, and regularization \nmethods to achieve undeniable performances.  \n2.1. CNN-Based Image Classification Method \nConvolutional Neural Networks (CNNs) came up to be a powerful method for \nimage classification tasks and this due to the fact that they can automatically \nlearn hierarchical features from raw pixel data. Convolutional neutral network \nbased image classification approach primarily consist of multiple convolutional \nlayers. These layers followed by pooling layers and fully connected layers for \nclassification. \nAs mentioned in the previous section, the AlexNet model proposed by (Kri z-\nhevsky, 2012) [3]  is one CNN architectures for image classification pioneer. \nAlexNet is abl e to enhance image classification accuracy on the ImageNet dat a-\nset by making use of deep convolutional layers, ReLU activation functions, data \naugmentation, and dropout regularization techniques. This is shown in \nFigure 1 . \n \n \nFigure 1 . CNN structure. \n\nA. A. M. Omer \n \n \nDOI: 10.4236/jcc.2024.124005 52 Journal of Computer and Communications \n \nConvolutional Layer (CLL). Some convolutional layers may only be able to \nextract some low- level features such as edges, lines, and corners.  more complex \nfeatures can beextracted from lower- level features. Max Pooling Layer (MPL) \nrepesent The main function is to subsample the feature maps learned by the \nconvolutional layer without damaging therecognition results. Fully Connected \nLayer (FCL). The main role is to apply the learned features (Feature Map) to the \nmodel classification or regression.  On the other hand, we have CNN model, \nVGGNet by Simonyan and (Zisserman, 2014).  VGGNet is definedby its deep \narchitecture with multiple stacked convolutional layers, each followed by a \nmax-pooling layer. The simplicity and uniformity of VGGNet ’s architect ure \nleads to its success in multiple image classification tasks. (He, 2015) [8]  In 2015, \nintroduced the ResNet architecture, which involves the vanishing gradient pro b-\nlem in deep neural networks by presenting residual connections. The model a l-\nlow gradients to flow more easily during training, enabling the effective training \nof very deep neural networks and achieving high performance on image classif i-\ncation. A more recent advancement in Convolutional based image classification \nis the introduction of attention mechanisms, such as in the Transformer arch i-\ntecture proposed by (Vaswani, 2017) [9] . This model enables the network to f o-\ncus on relevant regions of the input image, im proving th e model’ s ability to \ncapture long-range dependencies and spatial relationships. \n2.2. Comparison of CNN and ViT \nVision transformer (ViT) can be compared to Convolutional neutral network \nwhen it comes to image classification. In fact, (ViT) works better in some per-\nformances compared to CNN. There are definitely similarities between the fe a-\ntures obtained from the shallow and deep layers of ViT. CNNs are mostly cha-\nracterized by their hierarchical feature learning through convolutional layers, \npooling layers, a nd fully connected layers (LeCun,  1998) [6]. They are expert at \ncapturing spatial hierarchies in images and have been efficiently applied in di f-\nferent computer vision task and image classification. Over time, Convolutional \nneutral network CNNs have proven to be effective in extracting meaningful fe a-\ntures from image data. \nOn the other hand, the approach of Vision Transformers to image classific a-\ntion is through representing images as sequences of tokens and processing them \nthrough self-attention mechanisms (Dosovitskiy, 2021) [4] . In fact, ViT ’s eclude \nthe need for convolutional layers and directly model the relationships between \nimage patches using transformer blocks. This approach has provided positive \noutcomes in image classification. This is illustrated in Figure 2 . \nIn a study conducted by (Touvron et al. 2021) [5], the author compared the \nperformance of ViTs and CNNs on image classification and found that ViTs can \nachieve competitive results with CNNs when properly configured and trained.  \nThe research explores the potential of ViTs in dealing image data and explains \nthe strengths and weaknesses of both architectures. In the shallow layer, vision  \nA. A. M. Omer \n \n \nDOI: 10.4236/jcc.2024.124005 53 Journal of Computer and Communications \n \n \nFigure 2 . Operation of CNN and ViT. \n \ntransformer has some head attention parts with local windows that are identical \nto o Convolutional neutral network. However, in the deep layer the head atten-\ntion parts use more global windows, and CNN gradually expands the inform a-\ntion in the convolutional windows by convolving layer by layer.  \n2.3. Comparison of EfficientNet and Vision Transformer (ViT)  \nThese are two different  architectures for image classification tasks, each with its \nown unique characteristics and design principles. EfficientNet is a family of \nmodels that focus on achieving high performance while maintaining comput a-\ntional efficiency. On the other hand, ViT introduces a novel approach to image \nclassification by utilizing transformer architecture instead of traditional conv o-\nlutional neural networks to capture long-range dependencies in image data. \nWhile EfficientNet and ViT have been developed independently, it is possible \nto combine elements of both architectures to create a hybrid model that levera g-\nes the strengths of each approach. This could involve integrating the efficiency \nand scalability of EfficientNet with the ability of ViT to capture global context \nand complex relationships within the image. One potential approach to co m-\nbining EfficientNet and ViT could be to use the EfficientNet backbone for fe a-\nture extraction and combine it with the transformer encoder layers from ViT for \nprocessing the extracted features. This hybrid model could benefit from the \nstrong feature representation capabilities of EfficientNet along with the conte x-\ntual understanding and global information processing of ViT. \n3. Methodology  \nWe designed a model ViT defines the construction of a Vision Transformer \n(ViT) model (ViT-Base as in Table 1 ). The model is composed of the following \ncomponents: token_embed, position embedding (pos_embed), transformer_encoder \nand mlp. \nFirstly, the input shape of the model is defined based on the number of \npatches, patch size, and number of channels, reflecting the dimension s of the \nimage patches that will be processed by the model. The input layer is then i n-\nstantiated using the defined input shape. This is illustrated in Figure 3. \n\nA. A. M. Omer \n \n \nDOI: 10.4236/jcc.2024.124005 54 Journal of Computer and Communications \n \n \nFigure 3 . Conceptual framework of the algorithm. \n \nSubsequently, the model proceeds to compute patch embeddings by passing \nthe input through a dense layer, essentially embedding each image patch into a \nhigher-dimensional space. Alongside patch embeddings, position embeddings \nare computed to incorporate positional information into the model. This is \nachieved by generating position indices and embedding them using an Embe d-\nding layer. The resulting position embeddings are then added to the patch e m-\nbeddings to fuse spatial information with the visual features. \nA component token_embed, representing a global representation of the entire \nimage, is added to the model. This token_embed is concatenated with the input \nembeddings to create a comprehensive representation of the image. \nThe core of the ViT model architecture lies in its transfor mer encoder layers. \nThese layers are applied iteratively in a loop, where each iteration represents a \nsingle transformer encoder layer. The details of the transformer encoder m e-\nchanism, including multi -head self-attention and feed -forward layers, are likely \nencapsulated within the transformer_encoder function, invoked within the loop. \nThe design choices made in constructing the ViT model,  aim to enhance the \nmodel’s ability to learn and extract meaningful features from image data, ult i-\nmately improving its performance in image classification tasks. These design de-\ncisions reflect a thoughtful and systematic approach to leveraging transformer \narchitecture for processing visual information effectively. \nVision transformer (ViT) Model  \nProposed by Dosovitskiy (2020), The Vision Transformer (ViT) model is a \nstate-of-the-art deep learning model for computer vision tasks. In the (ViT) \nmodel, an image is split into fixed- size patches which are then linearly embe d-\nded to create sequences of tokens. These specific token se quences are inserted \ninto a transformer architecture, which consists of multiple stacked self- attention \nand forward layers. The transform er processes the token sequences to capture \nlong-range dependencies in the image and enable image recognition at scale \nwithout the need for convolutional layers. This is illustrated in Figure 4 . \nAn undeniable advantage of the ViT model is its ability to learn from raw i m-\nage pixel data without necessarily mak ing use of hand- made feature extractors. \nEventually, this has been shown to outperform traditional convolutional neural \nnetworks (CNNs) on various computer vision benchmarks. \n\nA. A. M. Omer \n \n \nDOI: 10.4236/jcc.2024.124005 55 Journal of Computer and Communications \n \n \nFigure 4 . ViT model architecture. \n \nToken_embed  \nToken-embad plays a significant role in Vision Transformer (ViT) archite c-\ntures. The purpose of this layer is to add a learnable class token to the input e m-\nbeddings. In the ViT model, each input image is divided into patches (Figure 4 ), \nand these patches are embedded into a higher -dimensional space to capture vis-\nual features. However, to enable the model to understand the overall context of \nthe image, a global representation of the entire image is needed. This is where \nthe class token comes into play. \nTransformer_encoder  \nThe transformer_encoder function is integral to the Vision Transformer \n(ViT) model, handling the processing of input tensors through a single tran s-\nformer encoder layer. It establishes skip connections to retain original input i n-\nformation, applies layer normalization for stabilit y, and employs a multi -head \nattention mechanism to capture contextual relationships within the input s e-\nquence. Following this, additional skip connections facilitate the seamless flow of \ninformation, and a feed -forward neural network refines representation s learned \nthrough the attention mechanism. Ultimately, the function outputs transformed \ntensors, enriched with meanin gful features, ready for further processing within \nthe ViT model. This is illustrated on \nTable 1  and Figure 5 . \n3.2. Results of the Experiment \nTable 2  presents evaluation metrics for a model’ s performance on a multi -class \nclassification task. The metrics include precision, recall, F1 -score, and support \nfor each class, as well as overall accuracy and macro/micro averages. \nPrecision measures the proportion of true positive predictions out of all posi-\ntive predictions made by the model. In this case, class 1 has the highest precision \n(0.64), indicating that 64% of the instances predicted as class 1 were correctly \nclassified. However, class 0 has a precision of 0.00, indicating that the model did \nnot correctly classify any instances as class 0. \n\nA. A. M. Omer \n \n \nDOI: 10.4236/jcc.2024.124005 56 Journal of Computer and Communications \n \n \nFigure 5 . Input preprocessing. \n \nTable 1 . Comparison of vision transformer models based on architecture characteristics. \nModel Layers Hidden size D MLP size Heads Parasm \nViT-Base 12 768 3072 12 86M \nViT-Large 24 1024 4096 16 307M \nViT-Huge 32 1280 5120 16 632M \n \nTable 2 . Performance metrics for image classification.  \n Precision Recall F1-score Support \n 0.0 0.00 0.00 7 \n 0.64 0.75 0.69 28 \n 0.29 0.60 0.39 10 \n 0.00 0.00 0.00 9 \nAccuracy Table 2 :   0.50 54 \nMacro avg 23 0.34 0.27 54 \nWeighted avg 0.38 0.50 0.43 54 \n \nRecall, also known as sensitivity, measures the proportion of  true positive \npredictions out of all actual positive instances in the dataset. Class 1 has the \nhighest recall (0.75), indicating that 75% of the actual instances of class 1 were \ncorrectly identified by the model. Conversely, class 0 and class 3 have recal l val-\nues of 0.00, indicating that the model failed to identify any instances of these \nclasses. \nThe F1 -score is the harmonic mean of precision and recall, providing a b a-\nlanced measure of a model ’s performance. Class 1 has the highest F1 -score \n(0.69), reflec ting a balance between precision and recall. Classes 0 and 3 have \nF1-scores of 0.00, indicating poor performance due to either low precision or \nrecall, or both. \n\nA. A. M. Omer \n \n \nDOI: 10.4236/jcc.2024.124005 57 Journal of Computer and Communications \n \nThe support column indicates the number of actual instances of each class in \nthe dataset. For e xample, there are 7 instances of class 0, 28 instances of class 1, \n10 instances of class 2, and 9 instances of class 3. This is illustrated on Table 2 . \nOverall accuracy measures the proportion of correctly classified instances out \nof all instances in the dataset. In this case, the overall accuracy is 0.50 or 50%, \nindicating that the model correctly classified half of the instances in the dataset. \nThe macr o and weighted averages provide aggregated metrics across all \nclasses. The macro average calculates the metric independently for each class \nand then takes the average, giving equal weight to each class. The micro average, \non the other hand, calculates the metric globally by considering the total number \nof true positives, false positives, and false negatives across all classes. In this sc e-\nnario, both macro and weighted averages suggest relatively low overall perfo r-\nmance, with macro F1-score at 0.27 and weighted F1-score at 0.43. \n3.3. Training and Validation \nFigure 6 illustrates the training and validation accurady and training and valida-\ntion loss for the image classification.  \nWe can perceive that the training accuracy starts at 0.6 and decreases as the \nepochs progress, while the validation accuracy starts at 0.5 and remains relatively \nstable. Additionally, the training loss decreases gradually from 10,000 to around \n8000, while the validation loss fluct uates between 10 and 40. The data also in-\ncludes the number of epochs repres ented on the x-axis. This data shows that the \nmodel may be overfitting as the training loss continues to decrease while the v a-\nlidation loss fluctuates. Further optimization may be n eeded to improve the \nmodel’s performance. The Algorith ViT bas is illustrated in Table 3 . \n \n \nFigure 6 . Result of experiment of accuracy and loss. \n\nA. A. M. Omer \n \n \nDOI: 10.4236/jcc.2024.124005 58 Journal of Computer and Communications \n \nTable 3 . Algorithm Vit-Base. \nAlgorithm ViT-Base \nInput: An Image, the number of epochs J, batch size b, the number of the layers L \nOutput: Predicted class \nInitialize model parameter Θ \nfor j ← 1, …, J do \nfor each batch B do \nUse token_embed to get global representation of the entire image is needed \nfor l ← 1, …, L do \nUse transformer_embed: handling the processing of input tensors through a single \ntransformer encoder layer \nend for \nend for \nend for \n4. Conclusion \nThe results of the experiment based on the evaluation metrics presented in Ta-\nble 2  and the results of the training and validation experiment in Figure 5  show \nevidence of the model’s performance on the multi -class classification task which \nis not ideal. The precision, recall, and F1 -scores for some classes are particu larly \nlow, indicating issues with either false positives or false negatives, or both. Addi-\ntionally, an accuracy of 50% illustrates  that the model only correctly classified \nhalf of the instances in the dataset. The macro F1 -score of 0.27 and weighted \nF1-score of 0.43 also indicate subpar performance across all classes. The training \nand validation results in Figure 5  show that the model may be overfitting, as the \ntraining accuracy decreases while the validation accuracy remains stable, and the \ntraining loss continues to decrease while the validation loss fluctuates. These r e-\nsults show that further optimization and regularization techniques may be n e-\ncessary to improve the model’ s generalization capabilities and overall perfo r-\nmance on the classification task. We can conclude  by mentioning that the mod-\nel’s current performance on the multi -class classification task needs improv e-\nment, and additional fine -tuning and optimization are required to enhance its \naccuracy, precision, recall, and F1-scores across all classes. \nConflicts of Interest \nThe author declares no conflicts of interest regarding the publication of this p a-\nper. \nReferences \n[1] Dos, S. (2021) A Review of Convolutional Neural Networks for Image Classific a-\nA. A. M. Omer \n \n \nDOI: 10.4236/jcc.2024.124005 59 Journal of Computer and Communications \n \ntion. Journal of Computer Vision, 10, 45-67. \n[2] Touvron (2021) Recent Advances in Image Classification Using Convolutional \nNeural Networks. International Conference on Computer Vision Proceedings , Par-\nis, 15-18 November 2021, 112-125. \n[3] Krizhevsky (2012) Image Classification with Deep Convolutional  Neural Networks. \nNeural Information Processing Systems, 25, 112-125. \n[4] Dosovitskiy, A. (2021) An Image Is Worth 16 × 16 Words: Transformers for Image \nRecognition at Scale. Proceedings of the IEEE /CVF Conference on Computer V i-\nsion and Pattern Recognition, New York City, 23-26 June 2021, 45-67. \n[5] Touvron (2020) Transformer in Image Recognition: ViT Models for Image Class i-\nfication. Proceedings of the European Conference on Computer Vision  (ECCV), \nAmsterdam, 14-18 September 2020, 112-125. \n[6] LeCun (1998)  Gradient-Based Learning Applied to Document Recognition . Pro-\nceedings of the IEEE, 86, 2278-2324. https://doi.org/10.1109/5.726791 \n[7] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D. and Rabinovich, A. \n(2014) Going Deeper with Convolutions. 2015 IEEE Conference on Computer V i-\nsion and Pattern Recognition (CVPR), Boston, 07-12 June 2015.  \nhttps://doi.org/10.1109/CVPR.2015.7298594  \n[8] He, K., Zhang, X., Ren, S. and Sun, J. (2015)  Deep Residual Learning for Image \nRecognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition \n(CVPR), Las Vegas, 27-30 June 2016, 770-778.  \nhttps://doi.org/10.1109/CVPR.2016.90 \n[9] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N. and Po-\nlosukhin, I. (2017) Attention Is All You Need. Proceedings of the 31st International \nConference on Neural Information Processing Systems (NeurIPS 2017), Long Beach, \nCalifornia, 4-9 December 2017, 5998-6008.  \nhttps://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a8\n45aa-Abstract.html  \n \n ",
  "topic": "Artificial intelligence",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.6055371165275574
    },
    {
      "name": "Computer vision",
      "score": 0.5178297162055969
    },
    {
      "name": "Computer science",
      "score": 0.49128836393356323
    },
    {
      "name": "Transformer",
      "score": 0.4373745322227478
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.39002788066864014
    },
    {
      "name": "Engineering",
      "score": 0.19979211688041687
    },
    {
      "name": "Electrical engineering",
      "score": 0.08048802614212036
    },
    {
      "name": "Voltage",
      "score": 0.059412211179733276
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74525822",
      "name": "Hubei University of Technology",
      "country": "CN"
    }
  ],
  "cited_by": 6
}