{
  "title": "Improving Transformer-based Speech Recognition Using Unsupervised Pre-training",
  "url": "https://openalex.org/W2981991061",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2694389913",
      "name": "Jiang Dongwei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2604880560",
      "name": "Lei, Xiaoning",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288501713",
      "name": "LI Wubo",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Luo, Ne",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3197581907",
      "name": "Hu Yuxuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098066907",
      "name": "Zou Wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097262608",
      "name": "Li, Xiangang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2892009249",
    "https://openalex.org/W2972894903",
    "https://openalex.org/W2972943112",
    "https://openalex.org/W2515753980",
    "https://openalex.org/W1507177964",
    "https://openalex.org/W1526236009",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W343636949",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2962893195",
    "https://openalex.org/W2526425061",
    "https://openalex.org/W2327501763",
    "https://openalex.org/W2900898015",
    "https://openalex.org/W2972818416",
    "https://openalex.org/W2963850025",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2514741789",
    "https://openalex.org/W2168961642",
    "https://openalex.org/W2133856945",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2922709902",
    "https://openalex.org/W2913718171",
    "https://openalex.org/W2963939538",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2973049979",
    "https://openalex.org/W2973157397",
    "https://openalex.org/W2963236196",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2963242190",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "Speech recognition technologies are gaining enormous popularity in various industrial applications. However, building a good speech recognition system usually requires large amounts of transcribed data, which is expensive to collect. To tackle this problem, an unsupervised pre-training method called Masked Predictive Coding is proposed, which can be applied for unsupervised pre-training with Transformer based model. Experiments on HKUST show that using the same training data, we can achieve CER 23.3%, exceeding the best end-to-end model by over 0.2% absolute CER. With more pre-training data, we can further reduce the CER to 21.0%, or a 11.8% relative CER reduction over baseline.",
  "full_text": "IMPROVING TRANSFORMER-BASED SPEECH RECOGNITION USING UNSUPERVISED\nPRE-TRAINING\nDongwei Jiang, Xiaoning Lei, Wubo Li, Ne Luo, Yuxuan Hu, Wei Zou, Xiangang Li\nAI Labs, Didi Chuxing, Beijing, China\n{jiangdongwei, leixiaoning, liwubo, luone, huyuxuan i, zouwei, lixiangang}@didiglobal.com\nABSTRACT\nSpeech recognition technologies are gaining enormous popularity\nin various industrial applications. However, building a good speech\nrecognition system usually requires large amounts of transcribed\ndata, which is expensive to collect. To tackle this problem, an un-\nsupervised pre-training method called Masked Predictive Coding is\nproposed, which can be applied for unsupervised pre-training with\nTransformer based model. Experiments on HKUST show that using\nthe same training data, we can achieve CER 23.3%, exceeding the\nbest end-to-end model by over 0.2% absolute CER. With more pre-\ntraining data, we can further reduce the CER to 21.0%, or a 11.8%\nrelative CER reduction over baseline.\nIndex Terms‚Äî Unsupervised Learning, Unsupervised Pre-\ntraining, Predictive Coding, Speech Recognition, Transformer\n1. INTRODUCTION\nCurrent industrial end-to-end automatic speech recognition (ASR)\nsystems rely heavily on large amount of high quality transcribed au-\ndio data. However, transcribed data take substantial effort to ob-\ntain in industrial applications, while at the same time a lot of un-\ntranscribed data exist in online systems and cost little to collect. It\nis worthwhile to explore how to effectively use un-transcribed data\nto improve the performance of speech recognition systems when la-\nbeled data are limited.\nRecently, unsupervised pre-training has shown promising results\nin several areas, including Computer Vision (CV) [1], Natural Lan-\nguage Processing (NLP) [2, 3] and so on [4, 5, 6]. One work that\nstands out among these unsupervised pre-training methods is Bidi-\nrectional Encoder Representations from Transformers (BERT) [2],\nwhich used a Masked Language Model (MLM) pre-training objec-\ntive and obtained new state-of-the-art results on eleven NLP bench-\nmarks.\nIn speech area, researchers also proposed some unsupervised\npre-training algorithms. Contrastive Predictive Coding (CPC) [7]\nis one of those unsupervised approaches that extract representation\nfrom data by predicting future information. There are multiple con-\ncurrent approaches that generalize this approach and applied it in\nlearning speaker representation [8], extracting speech representation\n[5, 9] and performing speech emotion recognition [6]. Apart from\nCPC, [10] proposed a different pre-training model called Autore-\ngressive Predictive Coding (APC) and also got comparable results\non phoneme classiÔ¨Åcation and speaker veriÔ¨Åcation.\nBoth CPC and APC belong to the family of future predictive\ncoding. One constraint of these methods is they can only be ap-\nplied in uni-directional models but not Transformer based models.\nOriginally proposed as a better replacement of Recurrent Neural\nNetwork (RNN) for neural machine translation, Transformer based\nmodels got a lot of traction in the Ô¨Åeld of speech recognition recently\n[11, 12, 13, 14]. The beneÔ¨Åts of Transformer based models include\nfaster training speed, better utilization of related context informa-\ntion and better performance over RNN in many speech recognition\nbenchmarks [12]. In this work, we get intuition from BERT and\npropose a simple and effective pre-training method called Masked\nPredictive Coding (MPC). The design of MPC bears a lot of simi-\nlarity to the Masked Language Model (MLM) objective proposed in\nBERT. It enables unsupervised pre-training for models that incorpo-\nrate context from both directions.\nTo test the effectiveness MPC, experiments were conducted\non HKUST and AISHELL-1 with different pre-training data. Ex-\nperimental results on HKUST show that using the same training\ndata, we can achieve CER 23.3%, exceeding the best end-to-end\nmodel by over 0.2% absolute CER. Using about 1500 hours of open\nsource Mandarin data for unsupervised pre-training, we can reduce\nCER of a strong Transformer based baseline by up to 3.8% while\non AISHELL-1, pre-training with the same data reduces CER by\n14.7%. To better understand the effect of pre-training data size and\nspeaking style on Ô¨Åne-tuning task, experiments were also conducted\nwith our internal dataset. With matching pre-training dataset and\nmore data, our best performing HKUST model achieved a CER\nreduction of 11.8% while our best performing AISHELL-1 model\nachieved a CER reduction of 22.1% over baseline.\n2. RELATED WORK\nContrastive Predictive Coding (CPC) is proposed by van den Oord\net al. [7] to learn representations from high-dimensional signal in\nan unsupervised manner by utilizing next step prediction. The main\nbuilding blocks of CPC include a non-linear encodergenc and an au-\ntoregressive model gar. Given an input sequence (x1, x2, . . . , xT ),\ngenc encodes observations xt to a latent embedding space zt =\ngenc(xt) while zt is then fed to gar to produce a context represen-\ntation ct = gar(z‚â§t). Targeting at predicting future observations\nxt+k, a density ratio f(xt+k, ct) is modelled to maximally preserve\nthe mutual information between xt+k and ct. This design releases\nthe model from modelling complex relationship in the data which\nmight be unnecessary for extracting a good representation towards\ndownstream tasks.\nTo optimize the encoder genc and the autoregressive model gar,\nthe contrastive loss is minimized:\nLN = ‚àíE\nX\n[log f(xt+k, ct)‚àë\nxj ‚ààX fk(xj, ct)], (1)\nwhere N represents number of samples in X = {x1, x2, . . . , xN },\nwith one positive sample from distribution p(xt+k|ct) and the rest\nbeing negative samples from distribution p(xt+k).\narXiv:1910.09932v3  [cs.CL]  31 Oct 2019\nThough also belonging to the family of predictive models, APC\ngot intuition from recent progress in NLP based on language model\npre-training [4, 15, 16] and chose to directly optimize L1 loss be-\ntween input sequence and output sequence. SpeciÔ¨Åcally, given input\nsequence (x1, x2, . . . , xT ) and output sequence (y1, y2, . . . , yT ),\nthe loss is deÔ¨Åned as:\nL=\nT‚àín‚àë\ni=1\n|xi+n ‚àíyi|. (2)\n3. PROPOSED METHOD\nThe overall training procedure is shown in Fig. 1. Our training con-\nsists of two stages, pre-training on unsupervised data and Ô¨Åne-tuning\non supervised data. To introduce minimum changes to the model\nstructure, predictive coding was directly performed on FBANK in-\nput and encoder output. The encoder output is projected to the\nsame dimension as FBANK input in all of our experiments. After\nthe unsupervised pre-training procedure, we remove added layer for\npredictive coding and plug in Transformer decoder for Ô¨Åne-tuning\non downstream ASR tasks. Unlike previous work with predictive\ncoding [5], our setup does not introduce any additional parameters\ninto speech recognition model. All model parameters are end-to-end\ntrainable in the Ô¨Åne-tuning stage.\nDecoder\nCharacterPrediction\nTransformer Block1\nTransformer BlockN\nEncoder\nFBANK(b)\nFBANKPrediction\nTransformer Block1\nTransformer BlockN\nEncoder\nFBANK(a)\n...\n...\nMPC\nFig. 1. Our training procedure: (a) Pre-training: The encoder is used\nto predict FBANK utilizing Masked Predictive Coding. (b) Fine-\ntuning: Transformer decoder was added after the encoder, then the\nmodel is Ô¨Åne-tuned for character prediction.\nInspired by BERT [2], MPC uses Masked-LM (MLM) like\nstructure to perform predictive coding on Transformer based mod-\nels. The overall structure of MPC is depicted in Fig. 2. Similar\nsetup to [2] was used, where 15% of the tokens in each sequence\nare chosen to be masked during pre-training procedure, which in\nour case are frames in each audio. The chosen frames are replaced\nwith zero vectors for 80% of the time, with frames from random\npositions 10% of the time, and kept to be the same in the rest of the\ntime. L1 loss is computed between masked input FBANK features\nand encoder output at corresponding position. Dynamic masking\nproposed in [17] was also adopted where the masking pattern is\ngenerated every time a sequence is fed to the model.\nOne unique characteristic of sequence-to-sequence with atten-\ntion ASR models is it usually applies downsampling in the encoder\n[18]. Previous research shows temporal pooling encourages effec-\ntive encoding in different temporal resolution and makes alignments\nin the decoding easier [18]. Like CPC and APC, we Ô¨Årst tried not\nùë•\"ùë•#ùë•$ùë•%ùë•&ùë•' ùë•(ùë•\")ùë•\"\"ùë•\"#ùë•\"$ùë•*ùë•+ ùë•\"%ùë•\"&ùë•\"'\n‚Ä¶ùë•,ùë•,-*\n‚Ä¶\n‚Ä¶\n\t\tùëê\" \t\tùëß# \t\tùëê$ \t\tùëê,\n\t\tùëß\" \tùëöùëéùë†ùëò\t\tùëß$ \t\tùëß,\n‚Ä¶\nFig. 2. Masked Predictive Coding with eight-fold downsample.\nusing downsampling at all in pre-training stage but didn‚Äôt get much\nimprovements over the baseline. We suspect the local smoothness\nof speech signals combined with the powerful modelling ability of\nTransformer make convergence too easy in this setup. As a result,\nthe learned parameters contain very little information. So in this\nwork, downsampling was applied on input feature before feeding in\nencoder in pre-training stage. Downsampling in Ô¨Åne-tuning stage is\napplied inside the model.\n4. EXPERIMENTS\n4.1. Data\nWith reproducibility in mind, some open source Mandarin datasets\nwere collected from Linguistic Data Consortium (LDC) and OpenSLR\nfor pre-training. The corpora we collected include: HKUST Man-\ndarin Telephone Speech Corpus (HKUST/MTS) [19], AISHELL-1\n[22], aidatatang 200zh [23], MAGICDATA Mandarin Chinese Read\nSpeech Corpus [24], Free ST Chinese Mandarin Corpus (ST-CMDS)\n[25] and Primewords Chinese Corpus Set 1 [26]. Detailed informa-\ntion of these corpora is provided in Table 2. Note the development\nset and test set of HKUST and AISHELL-1 were not included in\npre-training. We name the combination of them Open Mandarin\nin the following experiments and it contains about 1500 hours of\nspeech in total. To understand the impact of pre-training data size\nand speaking style on downstream task, our internal dataset Didi\nDictation and Didi Callcenter were also included. Didi Dictation\ncontains approximately 10,000 hours of speech collected from our\ninternal mobile dictation application. Didi Callcenter also contains\napproximately 10,000 hours of speech collected from phone calls\nbetween our user and customer service staff. All of our internal data\nare anonymized and eligible to be used for research purposes.\nThe Ô¨Åne-tuning experiments are conducted on HKUST and\nAISHELL-1. For HKUST, speed perturbation of 0.9, 1.0 and 1.1\nwas used on training data and per-speaker normalization was ap-\nplied on FBANK features. For AISHELL, speed perturbation of\n0.9, 1.0 and 1.1 was also used on training data. All speech data are\ndownsampled to 8kHz because the sample rate for both HKUST and\nDidi Callcenter is 8kHz. Note downsampling would usually hurt\nthe accuracy of speech recognition system [27, 28] and results for\nAISHELL-1 is usually reported on the original 16kHz data.\nTable 1. Character Error Rates on HKUST and AISHELL-1 test set with previous work and unsupervised pre-training approach. Results with\n‚Äò8k‚Äô represents training data are downsampled to 8kHz sample rate. Results with ‚Äò16k‚Äô represents the sample rate of training data is 16kHz.\nPrevious work on AISHELL-1 are conducted with 16kHz sample rate speech data. Models with asterisk are our baseline results without\npre-training data. Relative Error Reduction Rates (RERR) are calculated as the percentage of error reduction compared to baseline.\nModel Method Hours Unsupervised HKUST AISHELL-1\nPre-training Data CER/% RERR/% CER/% RERR/%\nTDNN-hybrid [20] - - - 23.7 - 7.5 (16k) -\nLSTM enc + LSTM dec [21] - - - 29.4 - - -\nTransformer [12] - - - 23.5 - 6.7 (16k) -\nLSTM enc + LSTM dec* - - - 28.8 - 12.9 (8k) -\nLSTM enc + LSTM dec APC ‚àº1500 Open Mandarin 27.8 3.5 11.4 (8k) 11.6\nTransformer* - - 23.8 - 9.5 (8k) -\nTransformer MPC 168 HKUST 23.3 2.1 - -\nTransformer MPC ‚àº1500 Open Mandarin 22.9 3.8 8.1 (8k) 14.7\nTransformer MPC 5000 Didi Callcenter 21.7 8.8 7.8 (8k) 17.9\nTransformer MPC 10000 Didi Callcenter 21.0 11.8 7.7 (8k) 18.9\nTransformer MPC 10000 Didi Dictation 22.2 6.7 7.4 (8k) 22.1\nTable 2. Details of open mandarin datasets and our internal datasets.\nST-CMDS contains about 100 hours of speech data.\nDatasets Hours Speaking Style\nHKUST 168 Spontaneous\nAISHELL-1 178 Reading\naidatatang 200zh 200 Reading\nMAGICDATA 755 Reading\nST-CMDS ‚àº100 Reading\nPrimewords 100 Reading\nDidi Callcenter 10000 Spontaneous\nDidi Dictation 10000 Reading\n4.2. Experimental setups\nMost of our experiments are conducted on Transformer based mod-\nels using MPC as pre-training method. To provide comparison with\nprevious unsupervised pre-training methods, we also conducted\nexperiments on LSTM based, sequence-to-sequence with attention\nmodels using APC with num step ahead = 3. Mandarin charac-\nters are used as modeling units in all our experiments [14, 29].\nFor LSTM based models, we follow the model structure in [21],\nwhich consists of 4 unidirectional LSTM layers with 480 cells in the\nencoder network and 1 layer LSTM with 320 cells in the decoder\nnetwork. Layer normalization and residual connection were found\nto be very important for LSTM based models because the goal of\npre-training is to learn parameters that is invariant to the scaling and\nthe offset of the input. Without these structures, the network tends\nto wipe out the weights learned during pre-training and start from\nscratch for the downstream task. Layer normalization and residual\nconnection were added between each encoder LSTM layers.\nFor Transformer based models, we use similar model structure\nas paper [12] ( e = 12, d = 6, dmodel = 256, dff = 2048 and\ndhead = 4) for both HKUST and AISHELL-1. Downsampling is\nconducted between every three encoder Transformer blocks, result-\ning in an 8-fold downsample in total.\nFor pre-training, both model was trained on 4 GPU with a total\nbatch size of 256 for 500k steps. We used the Adam optimizer [30]\nand varied learning rate with warmup schedule [31] according to the\nformula:\nlrate = k ‚àód0.5\nmodel ‚àómin(n‚àí0.5, n‚àówarmup n‚àí1.5), (3)\nwhere n is the step number. k = 0.5 and warmup n = 8000\nwere chosen for all experiments. In the Ô¨Åne-tuning stage, we used\na total batch size of 128 with the same learning rate schedule and\ndivide learning rate by 10 only once when validation loss doesn‚Äôt\ndecrease for 5 epochs. Scheduled sampling with a sample rate of\n0.1 is applied to help reduce the effect of exposure bias. We also\nused 1e‚àí5 L2 normalization on network weights. As for decoding,\nthe model with lowest loss on validation set was chosen. CTC-joint\ndecoding is performed as proposed in [32] with CTC weight 0.3.\n4.3. Unsupervised pre-training\nThree representative benchmarks on HKUST and AISHELL-1 were\ndisplayed here. The Ô¨Årst is a traditional pipelined model optimized\nwith Lattice-Free Maximum Mutual Information (LF-MMI) objec-\ntive. The second is an end-to-end LSTM based model with CTC-like\nobjective. The third one is a recently released Transformer based\nmodel that achieved very good results on multiple ASR benchmarks.\nAs shown in Table 1, our baseline for HKUST matches the per-\nformance of previous work for both LSTM and Transformer based\nmodel. The baseline result for AISHELL-1 is worse than previous\nwork but still in reasonable range because we performed downsam-\nple on speech.\nWe Ô¨Årst conducted experiments with APC and MPC using Open\nMandarin as pre-training data. The work on APC [10] mainly pre-\nsented results with phone classiÔ¨Åcation and speaker veriÔ¨Åcation.\nHere we used it directly on the task of ASR and got 3.5% relative\nerror reduction over a strong baseline. As shown in Table 1, the\nrelative error reduction for MPC is similar to APC using the same\npre-training data.\nIn order to evaluate the performance improvements of the pro-\nposed method, we also conducted an experiment using only the\nHKUST training set to perform MPC un-supervised pre-training\nand Ô¨Åne-tuning. The result listed in Table 1 shows that 0.5% abso-\nlutely CER reduction can be obtained, exceeding the best end-to-end\nmodel using only HKUST training set from 23.5%[12] to 23.3%.\nTo analyze the effect of pre-training data size, experiments were\nconducted with models pre-trained using different amounts of inter-\nnal data with the same speaking style. The relative CER reduction\nincrease from 3.8% to 8.8% using 5000h of Didi Callcenter as pre-\ntraining data. Adding another 5000h of Didi Callcenter further re-\nduces CER by 11.8% over the baseline. The same trend can be also\nobserved for AISHELL-1.\nSpeaking style has a strong impact on performance of ASR sys-\ntems [33, 34]. To test whether speaking style of pre-training data\nwould affect the performance of downstream tasks, further experi-\nments with another pre-training model trained on 10000h Didi Dic-\ntation data is conducted. Comparing relative error reduction for\nHKUST and AISHELL-1 with our internal data, models pre-trained\nwith matching speaking style were found to brings more improve-\nments for downstream tasks. Interestingly, we noticed pre-training\nHKUST with 5000h of Didi Callcenter gets better performance than\n10000h of Didi Dictation, which adds another proof for the impor-\ntance of matching speaker style.\nTo get a better understanding of MPC, we also conducted exper-\niments with different pre-training steps and compared unsupervised\npre-training of MPC with supervised adaption. The following sec-\ntions explain our Ô¨Åndings in detail.\n4.4. Effect of pre-training steps\nIt is known in the NLP community that more pre-training steps give\na bigger performance boost for the downstream task [2, 17]. We\nalso performed Ô¨Åne-tuning on HKUST with pre-training model on\nDidi Callcenter at different steps with similar loss on validation set.\nFig. 3 shows models pre-trained with more steps does give a slight\nperformance boost. The use of unsupervised pre-training also makes\nspeech recognition model converge a lot faster and more pre-training\nsteps help in that direction.\nFig. 3. Convergence curve with different pre-training steps.\n4.5. Comparing with supervised adaption\nIn this section, we present comparison between unsupervised pre-\ntraining and supervised adaption. For supervised adaption, we Ô¨Årst\ntrain a base model using both audio and transcript from Open Man-\ndarin then Ô¨Åne-tune the model on HKUST and AISHELL-1. As\nshown in Table 3, the performance of supervised adaption is still bet-\nter than our best unsupervised pre-training results on both HKUST\nand AISHELL-1 despite using less data. However, our proposed\nmethod does not need any transcribed data and it reduces the cost to\nbuild a good speech recognition system signiÔ¨Åcantly.\nTable 3. Comparison of Character Error Rates between unsuper-\nvised pre-training and supervised adaption on the test set of HKUST\nand AISHELL-1.\nModel CER/%\nHKUST (Open Mandarin, Supervised Adaption) 20.3\nHKUST (Didi Callcenter, MPC Pre-training) 21.0\nAISHELL-1 (Open Mandarin, Supervised Adaption) 7.0\nAISHELL-1 (Didi Dictation, MPC Pre-training) 7.4\n5. DISCUSSIONS AND CONCLUSION\nIn this paper, we proposed Masked Predictive Coding (MPC), a\nmethod utilizing Masked-LM like structure for Transformer based\nspeech recognition models. Experimental results showed pre-\ntraining model with MPC is helpful to improve performance on\nsupervised speech recognition tasks. Experiments on HKUST show\nthat using the same training data, we can achieve CER 23.3%, ex-\nceeding the best end-to-end model by over 0.2% absolute CER. With\nmore pre-training data, we can further reduce the CER to 21.0%,\nor a 11.8% relative CER reduction over baseline. Results of our\nwork and APC seem to suggest language model-like pre-training\nobjectives can also help in speech tasks. We would like to explore\nthe usefulness of other language model based pre-training objectives\n[35, 36] in speech tasks.\nThe experimental setup in this paper is very similar to the lim-\nited resource scenario for industrial applications. Training set size\nof industrial applications is usually around hundreds to thousands\nof hours, but to achieve superior performance, tens of thousands of\nhours is usually needed. On the other hand, when an applications\ngoes online, it is very easy to collect lots of unsupervised data. Our\nexperiments reveled the performance of MPC increases steadily with\nthe increase of pre-training data, which makes it especially useful in\nlimited resource scenarios. In the future, we will conduct experi-\nments with ten times or even a hundred times more unsupervised\ndata to greatly reduce the gap between supervised adaption and en-\nable effective Ô¨Åne-tuning for industrial applications.\nIn this work, speaking style in pre-training speech data was\nfound to have a big impact on the performance of Ô¨Åne-tuning tasks\nand those having similar speaking style to target dataset were found\nto be more helpful. Apart from speaking style, other speech vari-\nabilities like speaking environment, speech scenarios and regional\ndialect [33, 37, 38] also have an impact on the performance of\nspeech recognition system. Lots of recent advance in NLP is driven\nby pre-training on texts with different style from different sources.\nInspired by these advances, our next step is to perform experiments\nwith unsupervised speech collected from different scenarios. We\nbelieve doing so would improve the performance of pre-training\nmodel and make it more robust.\n6. REFERENCES\n[1] C. Doersch, A. Gupta, and A. Efros, ‚ÄúUnsupervised visual\nrepresentation learning by context prediction,‚Äù in ICCV, 2015,\npp. 1422‚Äì1430.\n[2] J. Devlin, M. Chang, K. Lee, and K. Toutanova, ‚ÄúBERT: pre-\ntraining of deep bidirectional transformers for language under-\nstanding,‚Äù in NAACL-HLT (1), 2019, pp. 4171‚Äì4186.\n[3] S. Edunov, A. Baevski, and M. Auli, ‚ÄúPre-trained language\nmodel representations for language generation,‚Äù in NAACL-\nHLT (1), 2019, pp. 4052‚Äì4059.\n[4] R. Alec, N. Karthik, S. Tim, and S. Ilya, ‚ÄúImproving language\nunderstanding with unsupervised learning,‚Äù Tech. Rep., Tech-\nnical report, OpenAI, 2018.\n[5] S. Steffen, B. Alexei, C. Ronan, and A. Michael, ‚Äúwav2vec:\nUnsupervised pre-training for speech recognition,‚ÄùInterspeech\n2019, Sep 2019.\n[6] Z. Lian, Y . Li, J.Tao, and J. Huang, ‚ÄúImproving speech emotion\nrecognition via transformer-based predictive coding through\ntransfer learning,‚Äù arXiv preprint arXiv:1811.07691, 2018.\n[7] O. Aaron van den, Y . Li, and V . Oriol, ‚ÄúRepresentation\nlearning with contrastive predictive coding,‚Äù arXiv preprint\narXiv:1807.03748, 2018.\n[8] R. Mirco and B. Yoshua, ‚ÄúLearning speaker representations\nwith mutual information,‚Äù Interspeech 2019, Sep 2019.\n[9] P. Santiago, R. Mirco, S. Joan, B. Antonio, and et al, ‚ÄúLearning\nproblem-agnostic speech representations from multiple self-\nsupervised tasks,‚Äù Interspeech 2019, Sep 2019.\n[10] C. Yu-An, H. Wei-Ning, T. Hao, and G. James, ‚ÄúAn unsuper-\nvised autoregressive model for speech representation learning,‚Äù\nInterspeech 2019, Sep 2019.\n[11] L. Dong, F. Wang, and B. Xu, ‚ÄúSelf-attention aligner: A\nlatency-control end-to-end model for ASR using self-attention\nnetwork and chunk-hopping,‚Äù in ICASSP, 2019, pp. 5656‚Äì\n5660.\n[12] K. Shigeki, C. Nanxin, H. Tomoki, H. Takaaki, and et al, ‚ÄúA\ncomparative study on transformer vs rnn in speech applica-\ntions,‚Äù arXiv preprint arXiv:1909.06317, 2019.\n[13] L. Dong, S. Xu, and B. Xu, ‚ÄúSpeech-transformer: a no-\nrecurrence sequence-to-sequence model for speech recogni-\ntion,‚Äù in ICASSP. IEEE, 2018, pp. 5884‚Äì5888.\n[14] S. Zhou, L. Dong, S. Xu, and B. Xu, ‚ÄúA comparison of model-\ning units in sequence-to-sequence speech recognition with the\ntransformer on mandarin chinese,‚Äù in ICONIP (5), 2018, vol.\n11305, pp. 210‚Äì220.\n[15] R. Alec, J. Wu, C. Rewon, D. Luan, and et al, ‚ÄúLanguage\nmodels are unsupervised multitask learners,‚Äù OpenAI Blog,\nvol. 1, no. 8, 2019.\n[16] M. Peters, M. Neumann, M. Iyyer, M. Gardner, and et al,\n‚ÄúDeep contextualized word representations,‚Äù in NAACL-HLT,\n2018, pp. 2227‚Äì2237.\n[17] Y . Liu, O. Myle, G. Naman, J. Du, and et al, ‚ÄúRoberta: A\nrobustly optimized bert pretraining approach,‚Äù arXiv preprint\narXiv:1907.11692, 2019.\n[18] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, ‚ÄúListen, attend and\nspell: A neural network for large vocabulary conversational\nspeech recognition,‚Äù in ICASSP, 2016, pp. 4960‚Äì4964.\n[19] Y . Liu, P. Fung, Y . Yang, C. Cieri, and et al, ‚ÄúHKUST/MTS:\nA very large scale mandarin telephone speech corpus,‚Äù inISC-\nSLP, 2006, vol. 4274, pp. 724‚Äì735.\n[20] D. Povey, V . Peddinti, D. Galvez, P. Ghahremani, and et al,\n‚ÄúPurely sequence-trained neural networks for ASR based on\nlattice-free MMI,‚Äù in INTERSPEECH, 2016, pp. 2751‚Äì2755.\n[21] L. Dong, S. Zhou, W. Chen, and B. Xu, ‚ÄúExtending recurrent\nneural aligner for streaming end-to-end speech recognition in\nmandarin,‚Äù in INTERSPEECH, 2018, pp. 816‚Äì820.\n[22] H. Bu, J. Du, X. Na, B. Wu, and et al, ‚ÄúAISHELL-1: an open-\nsource mandarin speech corpus and a speech recognition base-\nline,‚Äù in O-COCOSDA, 2017, pp. 1‚Äì5.\n[23] Beijing DataTang Technology Co., Ltd, ‚Äúaidatatang 200zh, a\nfree Chinese Mandarin speech corpus,‚Äù .\n[24] Magic Data Technology Co., Ltd, ‚ÄúMAGICDATA\nMandarin Chinese Read Speech Corpus,‚Äù http:\n//www.imagicdatatech.com/index.php/home/\ndataopensource/data_info/id/101, 2019.\n[25] SurÔ¨Ångtech, ‚ÄúST-CMDS-20170001 1 Free ST Chinese Man-\ndarin Corpus,‚Äù .\n[26] Primewords Information Technology Co., Ltd., ‚ÄúPrime-\nwords Chinese Corpus Set 1,‚Äù 2018, https://www.\nprimewords.cn.\n[27] M. Pedro and S. Richard, ‚ÄúSources of degradation of speech\nrecognition in the telephone network,‚Äù in ICASSP, 1994, pp.\nI‚Äì109.\n[28] S. Petr, M. Pavel, and ÀáC. Jan, ‚ÄúTowards lower error rates in\nphoneme recognition,‚Äù in TSD, 2004, pp. 465‚Äì472.\n[29] W. Zou, D. Jiang, S. Zhao, G. Yang, and et al, ‚ÄúComparable\nstudy of modeling units for end-to-end mandarin speech recog-\nnition,‚Äù in ISCSLP. 2018, pp. 369‚Äì373, IEEE.\n[30] D. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic opti-\nmization,‚Äù in ICLR, 2015.\n[31] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, and et al,\n‚ÄúAttention is all you need,‚Äù inNIPS, 2017, pp. 5998‚Äì6008.\n[32] K. Suyoun, H. Takaaki, and W. Shinji, ‚ÄúJoint ctc-attention\nbased end-to-end speech recognition using multi-task learn-\ning,‚Äù ICASSP, Mar 2017.\n[33] B. Mohamed, D. Renato, D. Olivier, D. Stephane, and et al,\n‚ÄúAutomatic speech recognition and speech variability: A re-\nview,‚ÄùSpeech communication, vol. 49, no. 10-11, pp. 763‚Äì786,\n2007.\n[34] W. Mitch, T. Kelsey, H. Kate, and S. Amy, ‚ÄúEffect of speaking\nstyle on lvcsr performance,‚Äù inProc. ICSLP, 1996, vol. 96, pp.\n16‚Äì19.\n[35] Z. Dai, Z. Yang, Y . Yang, J. Carbonell, and et al, ‚ÄúTransformer-\nxl: Attentive language models beyond a Ô¨Åxed-length context,‚Äù\nin ACL (1), 2019, pp. 2978‚Äì2988.\n[36] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, and et al, ‚ÄúXlnet: Gen-\neralized autoregressive pretraining for language understand-\ning,‚Äù 2019.\n[37] Y . Qian, M. Bi, T. Tan, and K. Yu, ‚ÄúVery deep convolu-\ntional neural networks for noise robust speech recognition,‚Äù\nIEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, vol. 24, no. 12, pp. 2263‚Äì2276, 2016.\n[38] L. Bo, S. Tara N, S. Khe Chai, B. Michiel, and et al, ‚ÄúMulti-\ndialect speech recognition with a single sequence-to-sequence\nmodel,‚Äù in ICASSP, 2018, pp. 4749‚Äì4753.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6437749266624451
    },
    {
      "name": "Speech recognition",
      "score": 0.6391218900680542
    },
    {
      "name": "Computer science",
      "score": 0.6248398423194885
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4093169569969177
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38068878650665283
    },
    {
      "name": "Engineering",
      "score": 0.14727085828781128
    },
    {
      "name": "Voltage",
      "score": 0.06701299548149109
    },
    {
      "name": "Electrical engineering",
      "score": 0.06253707408905029
    }
  ]
}