{
    "title": "BertSRC: transformer-based semantic relation classification",
    "url": "https://openalex.org/W4294732911",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5076557109",
            "name": "Yeawon Lee",
            "affiliations": [
                "Yonsei University"
            ]
        },
        {
            "id": "https://openalex.org/A5012490825",
            "name": "Jinseok Son",
            "affiliations": [
                "Yonsei University"
            ]
        },
        {
            "id": "https://openalex.org/A5045749444",
            "name": "Min Song",
            "affiliations": [
                "Yonsei University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1972938566",
        "https://openalex.org/W3026298482",
        "https://openalex.org/W2252979829",
        "https://openalex.org/W2129767020",
        "https://openalex.org/W2052217781",
        "https://openalex.org/W3198824379",
        "https://openalex.org/W3212086557",
        "https://openalex.org/W2121844933",
        "https://openalex.org/W3164029002",
        "https://openalex.org/W2250521169",
        "https://openalex.org/W2962897570",
        "https://openalex.org/W2972795848",
        "https://openalex.org/W2598789561",
        "https://openalex.org/W1551842868",
        "https://openalex.org/W2744323956",
        "https://openalex.org/W2970998014",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W3198459947",
        "https://openalex.org/W3168290506",
        "https://openalex.org/W3195976550",
        "https://openalex.org/W4200599415",
        "https://openalex.org/W3129125493",
        "https://openalex.org/W3208587672",
        "https://openalex.org/W2388693050",
        "https://openalex.org/W1473718842",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W3046375318",
        "https://openalex.org/W2950813464",
        "https://openalex.org/W2251622960",
        "https://openalex.org/W2163107094",
        "https://openalex.org/W2166474856",
        "https://openalex.org/W1850865022",
        "https://openalex.org/W2097960255",
        "https://openalex.org/W2097352005",
        "https://openalex.org/W370140021",
        "https://openalex.org/W2169974160",
        "https://openalex.org/W4210278852",
        "https://openalex.org/W4236463097"
    ],
    "abstract": null,
    "full_text": "Lee et al. \nBMC Medical Informatics and Decision Making          (2022) 22:234  \nhttps://doi.org/10.1186/s12911-022-01977-5\nRESEARCH\nBertSRC: transformer-based semantic \nrelation classification\nYeawon Lee1, Jinseok Son2 and Min Song1* \nAbstract \nThe relationship between biomedical entities is complex, and many of them have not yet been identified. For many \nbiomedical research areas including drug discovery, it is of paramount importance to identify the relationships that \nhave already been established through a comprehensive literature survey. However, manually searching through \nliterature is difficult as the amount of biomedical publications continues to increase. Therefore, the relation classifica-\ntion task, which automatically mines meaningful relations from the literature, is spotlighted in the field of biomedical \ntext mining. By applying relation classification techniques to the accumulated biomedical literature, existing semantic \nrelations between biomedical entities that can help to infer previously unknown relationships are efficiently grasped. \nTo develop semantic relation classification models, which is a type of supervised machine learning, it is essential \nto construct a training dataset that is manually annotated by biomedical experts with semantic relations among \nbiomedical entities. Any advanced model must be trained on a dataset with reliable quality and meaningful scale \nto be deployed in the real world and can assist biologists in their research. In addition, as the number of such public \ndatasets increases, the performance of machine learning algorithms can be accurately revealed and compared by \nusing those datasets as a benchmark for model development and improvement. In this paper, we aim to build such \na dataset. Along with that, to validate the usability of the dataset as training data for relation classification models \nand to improve the performance of the relation extraction task, we built a relation classification model based on \nBidirectional Encoder Representations from Transformers (BERT) trained on our dataset, applying our newly proposed \nfine-tuning methodology. In experiments comparing performance among several models based on different deep \nlearning algorithms, our model with the proposed fine-tuning methodology showed the best performance. The \nexperimental results show that the constructed training dataset is an important information resource for the develop-\nment and evaluation of semantic relation extraction models. Furthermore, relation extraction performance can be \nimproved by integrating our proposed fine-tuning methodology. Therefore, this can lead to the promotion of future \ntext mining research in the biomedical field.\nKeywords: Relation extraction, Semantic relation classification, Corpus construction, Annotation method, Deep \nlearning, BERT, Fine-tuning\n© The Author(s) 2022. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco \nmmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nIntroduction\nBiomedical literature is rapidly accumulating, and a large \namount of this information is in the form of raw text, \nmaking it difficult to easily gather details on topics of \ninterest. Instead of the researcher manually putting effort \ninto collecting, reading, and understanding the literature, \nthey can make use of text mining techniques, includ -\ning text classification, clustering, topic modeling, and \nOpen Access\n*Correspondence:  min.song@yonsei.ac.kr\n1 Department of Library and Information Science, Yonsei University, Seoul, \nSouth Korea\nFull list of author information is available at the end of the article\nPage 2 of 18Lee et al. BMC Medical Informatics and Decision Making          (2022) 22:234 \ninformation extraction such as named entity recognition \n(NER) and relation extraction (RE). These can be effec -\ntively applied to the vast literature for automated infor -\nmation extraction, thus facilitating effective biomedical \nresearch processes [1].\nAmong the various biomedical text mining techniques \nevolving as a result of research achievements in the natu -\nral language processing (NLP) field, relation extraction \nis critical. Relation extraction is defined as extracting \nmeaningful associations between entities in literature. \nThere are several types of relation extraction, including \nsemantic relations, grammatical relations, negations, and \ncoreferences, depending on the focus and aim of the task \n[2].\nSpecifically, researchers in the biomedical field primar -\nily focus on semantic relations to identify various rela -\ntionships between bio-entities and to infer undiscovered \nknowledge. This perspective of information extraction \nmotivates limiting our scope to semantic relation extrac -\ntion. Semantic relation classification in the biomedi -\ncal field enables the automatic extraction of relations \nbetween biomedical entities such as diseases, medica -\ntions, chemicals, genes/proteins, or medical tests from \na particular work. Therefore, new relationships can be \ninferred, allowing scientific hypotheses or new knowl -\nedge to be discovered or confirmed by identifying mech -\nanisms of interaction between these entities or pathways \nto target materials; this in turn facilitates biological or \nnew drug development research, biological database \ncuration, drug repositioning, and clinical decision mak -\ning [3, 4].\nIn the machine learning field, relation extraction is \na classification task that predicts whether there is any \nsemantic interaction between two entities (binary-class \nclassification) or what type of relation the identified \ninteraction belongs to among multiple predefined rela -\ntion types (multi-class classification). Since classification \nis a type of supervised learning, in which a model is fitted \non a labeled training dataset, relation extraction involves \nannotating unstructured natural language text with \nnamed entities and relations between them. However, \nmanually constructing such a dataset takes a consider -\nable amount of time and resources. Especially annotat -\ning a dataset with semantic relation information is more \ntime-consuming and laborious than constructing a cor -\npus annotated with named entities. Entity annotation is a \ntask of simply recognizing bio-instances and categorizing \nthem into their proper type, whereas relation annotation \ntakes entity annotation as a prerequisite and determines \nthe semantic interaction between two entities, which fur-\nther relies on human judgment. Thus, building and shar -\ning quality datasets annotated by experts is a significant \ncontribution to the field of biomedical text mining.\nIntending to promote relation classification research in \nthe biomedical field, after reviewing the available bench -\nmark relational corpora (usually for protein–protein \ninteractions (PPIs)), this study presents our newly built \ncorpus annotated with unique relation types on a mean -\ningful scale. Then, to demonstrate the feasibility of our \ncorpus, we built a Bidirectional Encoder Representations \nfrom Transformers (BERT)-based relation classification \nmodel, called BertSRC, trained on our dataset. Moreover, \nwe proposed a new fine-tuning methodology regarding \nformatting input tokens for BERT, which is the second \ncontribution of our study. The corpus for semantic rela -\ntion classification and the BertSRC code are publicly \navailable at https:// github. com/ tsmmb io/ BertS RC.\nConstruction of a training dataset annotated with semantic \nrelations\nSemantic relation classification in the biomedical field \nhas been studied primarily as part of shared tasks aimed \nat evaluating and advancing NLP techniques. Currently, \nmost prestigious datasets tagged with semantic relations \nare from these tasks, such as BioNLP shared tasks on \nthe recognition of biological events, which introduced \nthe BioNLP-09, 11, 13 event corpus, and BioCreative \nshared tasks on PPI extraction, which generated the Bio -\nCreative-II relation corpus [5]. In most of these corpora, \nthe entities that mainly receive attention are genes/pro -\nteins, and much of the focus is centered around the rela -\ntions between them [6, 7]. Examples of such PPI corpora \ninclude LLL, BioInfer, IEPA, and HPRD50 [1].\nAlthough such benchmark corpora exist, it is not \nenough to objectively verify and compare many of the lat-\nest algorithms. Since it is not easy to build a new dataset \nmanually from the scratch, several approaches to com -\npensate for data shortages with relatively little effort have \nbeen suggested. Kanjirangat and Rinaldi [8] proposed \nthe shortest dependency path (SDP) feature to effectively \neliminate noise samples when augmenting data using \ndistant supervision. Sentences were parsed into a tree \nstructure and the dependency was calculated to obtain \nthe SDP between the two entity mentions. Only features \nwith SDP from entities were filtered and used as input to \nthe model in the form of a triplet. This strategy effectively \nproduced training data and the model performed well in \nbiomedical relation extraction tasks with a precision of \n0.65.\nLi et  al. [9] has constructed an event-centered PPI \nontology (PPIO, PPI Ontology) that includes the tem -\nporal and spatial vocabulary to represent the biologi -\ncal context of PPI events. Six key pieces of information \n(interactor, biological process, subcellular location, \netc.) were expressed by integrating other thesauruses or \nPage 3 of 18\nLee et al. BMC Medical Informatics and Decision Making          (2022) 22:234 \n \nontologies including Gene Ontology and Protein Ontol -\nogy. Designing such ontologies not only helps interpret \nthe context of biological PPIs in the literature but also \nfacilitates subsequent data construction by being a useful \ntool for PPI annotation tasks.\nThese works show that securing a reliable dataset in \nsufficient quantities remains challenging in RE. Specifi -\ncally, while algorithms are domain-independent, data -\nsets are not, causing a chronic lack of data. In this regard, \nmanually annotating a dataset for a particular domain \nis a high contribution to the study of text mining in that \ndomain, which is biology in this paper.\nAmong various semantic relation corpora, only those \nannotated with binary relations are included as the main \nscope of our study. A binary relation is a type of relation -\nship where a pair of two entities come as arguments. \nSince this type of relation is easy to understand and make \nuse of, it is the corpus annotated with this type of relation \nthat most current information extraction (IE) systems \nrequire. For this reason, we deliberately excluded com -\nplex relations or events where different levels of relations \ncould be nested and become an argument for the other \nrelation along with the named entities from the scope \nof our study. The table comparing several representative \nbenchmark corpora annotated with binary relations is \nprovided in  Section  A  of Additional file  1. We gathered \ninformation about these corpora by reviewing each cor -\npus or through the literature presenting or explaining the \ncorpus [10].\nAccording to Section A in Additional file  1, some cor -\npora annotate only genes/proteins as entities, while oth -\ners annotate other participating entity types exhaustively \nalong with genes/proteins. With regard to relation type, \nthere are some corpora that do not define separate rela -\ntion types at all; in contrast, they only determine the \npresence or absence of a connection within the scope \nthey have determined. Other corpora explicitly define \nrelation types and express dynamic hierarchical relation -\nships among them, forming complex ontologies. The \nformer case has a limitation in that there is only limited \nutility for bio-researchers because various aspects of the \nrelation between bio-entities and semantic relationships \nbetween those relations cannot be expressed. On the \nother hand, the latter has the advantage of being able to \nreflect detailed characteristics and meaning of relations. \nHowever, introducing many classes, such as in the case \nof BIOINFER, which has more than 60 relation types, \nrequires a larger dataset, a more thoughtfully designed \nstructure, and a complicated parameter tuning process in \norder to make a model generalize well. This could limit \nthe applicability of the corpus compared with the dataset \nworking with the model without much configuration.\nTherefore, we present a new corpus annotated with \nsemantic relation types differentiated from those of \nthe existing corpora to provide researchers with use -\nful and easily implemented resources for bio-text min -\ning. Regarding entity, our corpus exhaustively covers \nmany types including genes/proteins as long as they \nform meaningful relations. The relation is divided into \ntwo broader types according to the presence or absence \nof causality. If a causality doesn’t exist, it is considered an \nundirected link, which is a negative example, otherwise, \na directed link. On the other hand, a directed link, which \nis judged to be causal, can be classified into a more sub -\ndivided type, such as a positive cause and negative cause, \nif the direction of causality is clearly revealed in the sen -\ntence. This hierarchical structure of relations has no bio -\nlogical meaning and is intended to introduce certainty \nabout the interaction. It allows researchers to determine \nthe reliability of the relation revealed in the text on their \nown because the relation type itself defines the level of its \nspecificity and certainty.\nDeep learning‑based semantic relation classification \nmodel\nWe evaluated the feasibility of the dataset that we built \nby constructing a semantic relation classification model \nand training it on the dataset. Furthermore, we sought to \nimprove the performance of relation extraction by sug -\ngesting a new fine-tuning method. Since relation extrac -\ntion has significant applications in various NLP tasks, \nimproving the performance of relation extraction models \ncan also improve the quality of various application tasks \nsuch as information extraction and knowledge graph \nconstruction.\nMethods for implementing a relation classification \ntask are largely divided into rule-based, statistical learn -\ning-based, and deep neural network-based, as in other \ninformation extraction studies such as NER and entity \ndisambiguation [11]. Traditionally, rule-based and sta -\ntistical-based machine learning has been widely used, \nbut recently most NLP researches rely on a neural net -\nwork model based on distributed features that do not \nrequire syntactic parsing or complicated feature engi -\nneering. Zeng [12] achieved state-of-the-art performance \nby applying CNN, a deep learning architecture mainly \nused for image and video processing, to a relation clas -\nsification task, with only word and sentence level distri -\nbution vector as input, without discrete features, which \nwere effective in the traditional machine learning meth -\nods. After this groundbreaking study, many studies have \napplied deep learning algorithms such as CNN, RNN, \nand LSTM for various NLP tasks[12–18]. Kim [18] con -\nstructed a plant-disease relations corpus and proposed a \nclassification model trained on this corpus. They noted \nPage 4 of 18Lee et al. BMC Medical Informatics and Decision Making          (2022) 22:234 \nthat Zeng’s model outperformed the SVM, which had \nshown the best performance in classification tasks so far \nby applying CNN, and then deployed CNN as the basic \narchitecture of the model to verify the effectiveness of the \nconstructed corpus, obtaining an f-score of 0.764.\nCurrently, one of the deep learning architectures, \nGoogle’s Transformers [19], has replaced traditional \nrule-based, statistical-based NLP techniques as well as \nother deep learning architectures used in NLP’s various \ntasks such as CNN, RNN, and LSTM. The current lead -\ning NLP models such as BERT[20], GPT[21], and T5[22] \nannounced later are all based on this transformer block. \nIn particular, BERT is commonly used in biomedical text \nmining research because it is built on multiple trans -\nformers encoder blocks, which has the advantage of com-\npressing the sentence and mining semantic information \nfrom it [8, 23–25].\nBERT’s excellence in RE tasks stems from the fact that \nit is not only built on the transformer block which already \nhas a proven track record, but also the context of the \ninput sequence can be learned in both directions. In con -\ntrast to a language model for sequence generation such \nas GPT, a model that with a given sequence, predicts the \nnext word, BERT is a masked language model that ana -\nlyzes both directions of input in the pre-training stage to \nobtain embeddings for the language. By referring to the \nentire context of the sentence, it can produce vectors that \ncan well reflect the semantic meaning of each token in \nthe sentence. In other words, the quality of embeddings \nrepresenting text is ahead of other models. Thus, imple -\nmenting a model for NLP downstream tasks such as RE \nwith these embeddings performs well [20]. Therefore, \nBERT is an ideal deep learning architecture for our study, \nwhich aims at predicting the semantic relations between \nbio-entities in biomedical literature.\nHong et al. [25] created a dataset labeled with predicate \nrelations by performing automatic NER on SemMedDB \ndata and then clustering on predicates that appear with \nthe recognized entity pairs. Various deep learning mod -\nels were trained with the dataset to verify the usability of \ntheir dataset and propose the final state-of-the-art per -\nformance model optimized with the dataset. Experiments \ndemonstrated that the performance of BERT was better \nthan that of CNN or LSTM, which had been widely used \nin the existing NLP research. Among BERT and its vari -\nant architectures, BioBERT and SciBERT which are spe -\ncialized in biology and science literature showed excellent \nperformance with f1-scores of 0.86 and 0.84, respectively. \nThis is because BERT architectures, pre-trained on vast \nscientific literature, were able to learn sequential charac -\nteristics of text in the biomedical field.\nBioinformatics studies are also actively introduc -\ning BERT. Since the string sequence of the protein or \ngene has a structure similar to that of the natural lan -\nguage, NLP can be applied to analyze protein sequence. \nUsing BERT, proper embeddings for the protein can be \nacquired which extract its sequence information well. \nTo automatically identify sites of DNA 6 ma, Le et  al. \n[26] generated embeddings of DNA sequences through \nBERT blocks and used the CNN structure in the classifi -\ncation layer that predicts whether it is 6 ma sites or not. \nThe classification model performed better than other \nbaseline models with an accuracy of 79.3% and MCC of \n0.58. A similar study [27] also conducted a case study of \nidentifying DNA enhancers from DNA sequence repre -\nsentations, using hybrid models of BERT and CNN. The \nidentifying classifier used CNN, and the fixed vector for \neach nucleotide entering as input of this classifier was \nobtained through BERT. The BERT-based vectors have \nresulted in statistically significant improvements in sensi-\ntivity, acuity, and MCC than unidirectional word embed -\nding features such as Word2Vec and fastText. These \nstudies have shown that the combination of BERT and \nCNN has strength in modeling protein structures.\nBERT is also being utilized in the medical and clinical \nfields to automatically analyze various medical data such \nas electronic health records [24, 28]. The need for a sys -\ntematic review is emerging for evidence-based diagnosis \nand treatment in the medical field. However, it can be \ntime-consuming for the medical staff to manually per -\nform systematic reviews of numerous documents, result -\ning in outdated information. Therefore, automating SR \nwith NLP technology is attempted [28]. BERT showed \nstate-of-the-art performance in document classification \nand relation extraction tasks. In addition, experiments \nwith different settings for BERT have been conducted \nto propose the best model with optimized performance. \nAs a result, it was found that the size and class ratio of \nthe training data play an important role in the model’s \nperformance.\nAs such, more recent studies applying NLP technol -\nogy in a diverse domain, including the biofield, consist -\nently demonstrate that the BERT algorithm based on the \ntransformer structure performs very well, and in order \nto apply it to a specific domain, it is important to secure \ngood quality data, which is the goal of our study.\nIn summary, we built a training dataset for semantic \nrelation classification that annotates various bio-entity \ntypes and their semantic relational information. In addi -\ntion, we proposed a new fine-tuning method for BERT to \nimprove the performance of relation classification tasks. \nBy comparing the relation extraction performance of \nmodels with various methodologies trained on the con -\nstructed dataset including the proposed methodology, we \nevaluated the usability of our constructed dataset and the \nperformance of our proposed methodology. The creation \nPage 5 of 18\nLee et al. BMC Medical Informatics and Decision Making          (2022) 22:234 \n \nof these novel datasets and fine-tuning methodology for \nclassifying relations provides a meaningful contribution \nto this field and is expected to advance future semantic \nrelation classification research.\nThe following parts of this paper are organized as is: \nSect.  2. Material and Methods, sub-Sect.  2.1 reports on \nPubMed data collection and annotation procedures, \nincluding guidelines for building datasets. Section  2.2 \npresents setting details for constructing a classifica -\ntion model based on BERT architecture using the con -\nstructed dataset as training data. In addition, we present \na fine-tuning methodology to enhance classification \nperformance. In Sect.  3. Results, sub-Sect.  3.1 presents \nan overview of the constructed dataset, and 3.2 reports \nthe results of the model comparison experiment and the \nstructure of the best model. Finally, in Sect. 4, the conclu-\nsion and outlook of the entire paper are discussed.\nMaterial and methods\nConstruction of semantic relation corpus\nData preparation\nTo begin with, PubMed data that were published between \n2004 and 2019, a total of 15 years, were collected regard -\nless of the topic within the boundaries of the biomedical \nfield. A total of 1,500 candidate abstracts were obtained. \nAfter a brief inspection of the abstracts, we excluded \ndocuments that were too short or lacked sufficient enti -\nties for a relation to be assigned. The number of screened \npapers was 154, and a total of 1,346 filtered papers in our \ndataset were subject to annotations. The collected data \nwere then separated into sentences, which became the \nunit of annotation in this work. Sentences were distin -\nguished from one another based on the combination of \nthe PMID that they belonged to and the sentence identity \n(ID) that represented the sequential order of the sentence \nin the text. The title of each work was treated as the sen -\ntence with the sentence ID of 0.\nAnnotation\nWe extracted different types of entities from the titles \nand abstracts of papers provided by PubMed and then \nbuilt a semantic relation corpus that assigned semantic \nrelation types to those entities based on the contextual \ninformation surrounding them and an expert’s judgment. \nRecognizing different types of entities outside the scope \nof proteins/genes and granting those entities semantic \nrelations were highly dependent on manual expert cura -\ntion. Such studies have not been extensively performed in \nthe past.\nVarious bio-entities in each sentence were manually \nidentified and annotated with the corresponding entity \ntype. When annotating them or resolving any ambiguity \nafter the annotation process, annotators referred to three \ndatabases, namely, Pubtator, Uniprot, and GeneCards, \nthat contain information about bio-entities. Entity types \nthat were annotated, with their definitions given later \nin the paper, included biological processes, cells, com -\npounds, DNA, enzymes, genes, hormones, molecular \nfunctions, phenotypes, proteins, RNA, and viruses.\nAfter the process of annotating entities was complete, \nthe process of assigning the eight types of semantic rela -\ntions between those annotated entities followed. The \nrelation was decided based on the context of the sentence \nor the full abstract. The eight types of semantic relations \nincluded the following: undirected link, directed link, \npositive cause, positive increase, negative decrease, nega -\ntive cause, positive decrease, and negative increase.\nFor the sake of convenience, the annotation work was \nassisted by an online annotation tool developed by our \nresearch team. The software tool is based on an open-\nsource project TextAE (Text Annotation Editor),1 a visual \nannotation tool, that can support not only creating and \nsaving annotation results but also conveniently retriev -\ning and editing existing ones. This tool saved us signifi -\ncant time whenever we needed to revise the annotation \nguideline or amend previously accumulated annota -\ntions. The tool can be accessed at  http:// 165. 132. 151. 153/ \nannot ator/. Screenshots of the tool are given in the figure \nbelow (Fig. 1).\nAnnotation guidelines\nEntity annotations Recognition of named entities is a \nprerequisite in the relation extraction process. Discov -\nering the relationship between two entities is possible \nafter they are accurately recognized. To ensure accurate \nand consistent annotation, multiple annotators worked \nindependently at first, and in the event of a discrepancy, \na consensus was reached after multiple agreements to \nresolve it. Brief explanations of each entity type are shown \nbelow (Table 1). Please see Section B in Additional file  1 \nfor details.\nIf more than one entity was recognized in a sentence, \nthe sentence moved to the step of assigning the type of \nsemantic relation between the entities.\nSemantic relation annotations Relation annotation is \nperformed on two entities that appear within a sentence. \nIn principle, the assignment of relations is based on the \nsentence in which the entities appear, but when ambig -\nuous, the entire abstract of a piece can be read, and the \nannotator can decide based on the full context in the \nfinal stage. To define semantic relations between entities, \n1 Text annotation editor. TextAE, 2020. Accessed 2021 Aug 30. Available \nfrom: https:// textae. puban notat ion. org/\nPage 6 of 18Lee et al. BMC Medical Informatics and Decision Making          (2022) 22:234 \nwe extracted relational verbs and contextual informa -\ntion between two entities. When determining semantic \nrelations between entities, contextual information must \nbe considered along with the meaning of the verb. For \nexample, if “miR-194, ” “basal and insulin-stimulated glu-\ncose uptake, ” and “glycogen synthesis” were recognized as \nnamed entities in the sentence “Knockdown of miR-194 in \nL6 skeletal muscle cells induced an increase in basal and \ninsulin-stimulated glucose uptake and glycogen synthe -\nsis” [29], the verb associated with “miR-194, ” “basal and \ninsulin-stimulated glucose uptake, ” and “glycogen synthe-\nsis” would be “induce. ” Although the verb “induce” is usu-\nally classified as positive, the word “knockdown of” fol -\nlowed by “miR-194” means inhibition; therefore, it should \nbe classified as negative rather than positive. Thus, con -\ntextual information, which plays an important role in cor-\nrectly classifying relations, should be annotated together.\nFig. 1 A Sentence within the corpus that was subject to annotation. B Sentence annotation result visualized with TextAE. Annotation tool based \non TextAE: TextAE is a text annotation tool that can annotate named entity and relation information in the text. Each term (entity) can be dragged \nor double-clicked in Term Edit Mode, and the corresponding type can be selected to annotate them. If the corresponding entity type does not exist \nat the time, a new type can be defined. Likewise, in Relation Edit Mode, a type of relation can be selected or created and visualized. It is also possible \nto annotate multiple relations where one entity is associated with multiple other entities at once. The results of the annotation can be downloaded \nlocally in the form of a json file\nTable 1 Twelve entity types\nExplanation\nGenes, DNA, RNA, and Proteins A gene is the functional unit of heredity and the nucleotide sequence of \nDNA or RNA that holds instructions for synthesizing either RNA or protein\nEnzymes Proteins that act as biological catalysts\nHormones Signaling molecules that act distant from their site of production\nCompounds Additives such as drugs or chemicals\nMolecular functions Proteins with the role of binding such as hormones and antigen-antibodies\nPhenotypes Limited to human diseases\nBiological processes Processes/activities that occur within cells\nCells Smallest functional units of an organism with which biological experi-\nments are conducted to observe their mechanisms or to grow targeted \ncompounds\nViruses Used to indicate when experiments are conducted on a virus\nPage 7 of 18\nLee et al. BMC Medical Informatics and Decision Making          (2022) 22:234 \n \nThe semantic relations defined in this study were clas -\nsified into a total of eight types: undirected link, directed \nlink, positive cause, positive increase, negative decrease, \nnegative cause, positive decrease, and negative increase \n(Table  2). They were structured into three layers, and \neach level represents the extent of granularity. Initially, \na relation between two entities in a sentence was classi -\nfied into an undirected link and a directed link at the top \nlevel based on whether the causal relationship between \nthe two entities was clearly revealed. When a relation \nwas identified as a directed link where there was a causal \nrelationship based on the sentence, if it could be decided \nwhether the two entities were positively or negatively \ncorrelated, the relation proceeded down one level down \nand was matched with finer types (positive cause/nega -\ntive cause) or stopped at the first level (directed link). \nSimilarly, at the second level (positive cause/negative \ncause), if the sentence captured causality according to an \nincrease or decrease in the amount of each entity or its \nstrength with explicit expressions of quantity, the relation \nproceeded down to the lowest level (positive increase/\nnegative decrease, positive decrease/negative increase) \nor stopped at the second level (positive cause/nega -\ntive cause). With regard to the relation of an undirected \nlink, which is a correlation without causation revealed, \nsince this type of relation is rarely a subject of attention \nfor researchers, it does not need to be broken down to a \nmore granular level, and this type of relation was treated \nas a negative example. This stratified structure between \nthe types allows researchers to determine the reliability \nof the relation revealed. In addition, depending on the \npurpose and usefulness of the classification, it can be eas-\nily converted to binary class and integrated with other \nbenchmark datasets, which is commonly binary. In sum -\nmary, with respect to the relationships that researchers \nmight be interested in, each relation was classified into \nthe most detailed and specific type possible. Examples \nof sentences corresponding to each relation type and \ndetailed descriptions are provided in Section C of Addi -\ntional file 1.\nAnnotation procedure\nWe finished collecting data in February 2019 and per -\nformed trial annotations until April 2019. During the trial \nperiod, entities and relations were annotated for a small \namount of collected data referring to previous related \nworks for exploratory purposes. At this time, entities \nwere pre-annotated through an automated biomedi -\ncal NER/RE system called PKDE4J [30], and annotators \nhad to modify incorrect entity annotations or add missed \nannotations. Relations between these readily annotated \nentities were classified manually by four to six annotators. \nHowever, as the decision to include gene-related entities \nin our annotation scope in a more exhaustive way was \nmade, the method of entity annotation also shifted from \nutilizing PKDE4J to manual annotation to cover entity \ntypes that PKDE4J is not aimed at extracting, such as \nenzymes and viruses. Throughout the manual annotation \nprocess, the lead annotator, a biology expert with rich \nhands-on experience in bio-corpus construction projects, \nrefined the guidelines and detailed the workflows.\nThe full-scale annotation process based on the final \nguidelines and workflows began in May 2019. The anno -\ntators included the lead annotator and eight researchers \nworking on text mining in the biomedical field. More \nspecifically, two of the eight researchers were selected \nas senior annotators to coordinate the entire annota -\ntion process among the multiple annotators and played \nimportant roles in settling any ambiguities, such as medi-\nating disagreements that failed to be resolved in the \nprevious stage. The lead annotator controlled the final \nverification and resolved any remaining ambiguous cases.\nAs we decided to annotate both entities and relations \nmanually from the beginning, we developed a web-based \nannotation tool to streamline the annotation process \nand introduced this tool to our task in earnest starting in \nJune. The annotation process was completed at the end of \nJune 2020. The entire process, including data preparation, \ntook approximately one and a half years to complete.\nThe final annotation workflow consisted of three stages: \nannotation, error review, and final verification (Fig. 2).\n i. Annotation\n Annotators who had conducted research in the \nfield of text mining and had experience building \ncorpora in the biomedical field read the abstracts \nand manually annotated bio-entities by referring to \nthe Uniprot, GeneCards, and Pubtator databases. \nThey selected sentences in which two or more enti-\nties appeared and annotated the verb between the \ntwo entities and other contextual information that \ncould help resolve any ambiguities. Based on this \ninformation and the annotator’s judgment after \nreading the sentence, the relationship between two \nTable 2 Eight types of relations\nCausality Direction of causality Expressions of quantity\nDirected Link Positive Cause Positive Increase\nNegative Decrease\nNegative Cause Positive Decrease\nNegative Increase\nUndirected Link –\nPage 8 of 18Lee et al. BMC Medical Informatics and Decision Making          (2022) 22:234 \nentities was mapped into one of the eight semantic \nrelation types we defined in this work.\n ii. Error review\n Except for the lead and senior annotators, six \nannotators formed two teams of three to assess for \nsimple errors within their own teams. If there was \na disagreement within the teams, they attempted \nto reach an agreement in a reasonable direction \nthrough discussion and provision of evidence. The \ndiscussion usually resolved the disagreement. If \nthe issue persisted, however, the teams convened \nto discuss the issue and moved forward with an \nagreement or passed the disagreement on to the \nsenior annotators.\n The annotation result, which had undergone the \nfirst error review process, was delivered to the \nremaining two senior annotators who did not \nbelong to either of the teams, and they conducted \na second review process. The delivered annotation \nmodifications and unresolved discrepancies were \nreviewed once more by these senior annotators, \nand agreement was attempted. Any inconsistencies \nthat were not settled were finally resolved in the \nnext verification step.\n iii. Final verification\n To increase the quality of the dataset, the lead \nannotator verified the annotation result one last \ntime in consideration of the context of the original \ntext based on his or her biological knowledge. The \nlead annotator then corrected the result if neces -\nsary and decided whether to include it in the final \ndataset. The lead annotator not only adjudicated \ndisagreements that could not be settled in the pre -\nvious steps but also reviewed the entire dataset, \nincluding data that were already agreed upon by \nthe teams for final verification. In practice, only a \nfew simple errors were found after the multi-step \nreview process, and these were quickly corrected.\nAnnotations that remained ambiguous even in \nthe final verification stage were excluded from the \nfinal corpus.\nSemantic relation classification model training\nTo verify the credibility and usefulness of the semantic \nrelation corpus we established, we attempted to build a \nsemantic relation classification model that utilized the \ncorpus. In order to fully optimize performance for our \nmodel, we compared the performances of several deep \nlearning-based pre-training models and suggested new \nfine-tuning techniques for the model that produced opti -\nmal results. While performing the classification task, \nprecision, recall, and f1 score are commonly used as \nperformance metrics, so we adopt them to evaluate our \nmodel. The explanation and calculation formula for each \nmetric are as follows (Table 3).\nFig. 2 Annotation workflow\nPage 9 of 18\nLee et al. BMC Medical Informatics and Decision Making          (2022) 22:234 \n \nIn particular, all of the deep learning models used in \nour study are BERT-based architectures that are available \nthrough the well-known library Hugging face or are pre -\nsented in precedent studies. As mentioned earlier, BERT \nis good at compressing and understanding the meaning \nof the text, making it suitable for our purpose, which is \nto extract semantic relations. Along with that, it is easy \nto implement and capable of making predictions imme -\ndiately without much configuration, which is an impor -\ntant consideration to our work since the primary focus of \nthe study was to demonstrate the feasibility of the corpus \nthat we had constructed. For these reasons, we choose \nBERT as the model architecture to apply our data.\nPre‑training and Fine‑tuning stage of BERT\nBERT is a pre-trained model that leverages the structure \nof the transformer encoder [19, 20]. Pre-trained models \nlearn and utilize universal text embeddings rich in gram -\nmatical and semantic features from pre-training on a vast \namount of textual data, and only a simple additional layer \nis needed for the aiming task. The basic BERT model was \ntrained on the Book corpus (800 M words) and Wikipedia \n(2.5B), achieving SOTA in most common NLP tasks [20]. \nSince then, its variant models, which were pre-trained \non domain-specific data, such as BioBERT [31], SciBERT \n[32], and PubMedBERT [33] as well as advanced versions \nof BERT with tweaked pre-training methods or a struc -\nture of layers, such as ALBERT [34] and RoBERTa [35], \nhave been announced.\nIn the pre-training stage of BERT, a masked language \nmodel (MLM) and next sentence prediction (NSP) were \nutilized to learn various characteristics of natural lan -\nguages [20]. The MLM method is a method of randomly \nreplacing 15% of tokens with [MASK] tokens in the \ninput sentence, expecting the BERT model to predict \nthe original word of the [MASK] token. The pre-training \nof BERT involves two different sentences divided by the \n[SEP] token as input. At this time, 50% of the sentence \npairs are in order, with the next sentence being the actual \nsentence that follows the prior in the original text, and \nthe rest are not, with the first sentence being followed \nby a random sentence. NSP involves models learning to \ndetermine whether these two statements are in order. \nTo develop a BERT model trained with these methods \nin the pre-training stage to perform downstream NLP \ntasks, such as relation extraction, an additional layer for \ndetailed tasks is appended after the transformer encoder \nlayer which has learned weights, and further fine-tuning \nis performed using relevant data for the desired tasks. \nThis is how fine-tuning can provide a model for handling \ndetailed downstream tasks.\nDuring the fine-tuning stage, the simplest approach to \nprocessing input text for the relation classification task \nis using a single sentence containing the relationship \nbetween entities without any pre-processing treatment. \nHowever, to achieve optimal performance, introducing \na slightly more complex input data processing method is \nnecessary. Soares et  al. [36] compared several fine-tun -\ning methodologies for relation extraction, such as input \nstructure, architecture of the downstream layer, a train -\ning setup, and explored effective ways to produce good-\nquality output vectors that represent relation for a given \nsentence.\nMasked input\nYang et al. [37] demonstrated that there is a mismatch in \nthe original BERT model. The [MASK] tokens are used in \nthe pre-training process but not in the fine-tuning stage. \nTo compensate for this limitation, in this work, we use \na methodology that utilizes [MASK] tokens as the input \nsequence in both the pre-training and fine-tuning stages \nfor relation classification.\nIn the pre-training of BERT, input sentences contain -\ning [MASK] tokens are received as input data, and the \noriginal tokens for the [MASK] tokens within each sen -\ntence are predicted using the output of the final layer cor-\nresponding to the location of [MASK] tokens. Namely, \nthe output of the final layer at [MASK] token contains \ncontextual information needed to predict the original \ntoken replaced with [MASK]. Likewise, during the fine-\ntuning process, if the token corresponding to the enti -\nties of interest within the input sentence is replaced with \na [MASK] token, the final output layer at the location of \nthe replaced tokens can be considered to output a seman-\ntic and contextual vector for the token.\nThere are several objectives to using this approach. The \nfirst is to maintain consistency between the pre-training \nand fine-tuning training of the model. As mentioned \nTable 3 Evaluation metrics\nAccuracy = correct predictions\ntotal predictions Useful when target classes are well balanced\nRecall = True positives\nTrue positives+False negatives The ability of a model to find all relevant cases within a \ndataset\nPrecision= True positives\nTrue positives +False positives The ability of a model to identify only the relevant data points\nF 1 − score = 2(precision×recall)\nprecision+recall Combination between Precision and Recall\nUsed to punish extreme values\nPage 10 of 18Lee et al. BMC Medical Informatics and Decision Making          (2022) 22:234 \nabove, the pre-training process for BERT employs \n[MASK] tokens that are not introduced in the fine-tun -\ning stage, resulting in the disadvantage of inconsistent \nmodels. In this work, the [MASK] tokens are also uti -\nlized as input data in the fine-tuning process to increase \nthe consistency of the model. The second objective is to \neffectively convey to the model the information of enti -\nties that span over multiple tokens. In relation classifi -\ncation, each entity often consists of multiple words. To \ndeal with this problem, the aforementioned study by [36] \nintroduced the method entity marker–entity start, which \nemploys additional marker tokens, such as [E1], [/E1] and \n[E2], [/E2], before and after the entity to convey informa -\ntion about where the entity is located in the sentence. \nHowever, it still has the disadvantage of inconsistency \nbecause the tokens are not used for pre-training and the \nentity itself is not replaced (Fig. 3). Although this method \nenables the model to learn the span of entities in a given \nsentence, the model is limited in accurately recognizing \nadditional marker tokens that were unseen in pre-train -\ning. Furthermore, it fails to directly convey information \nabout the entities as a whole to the model. On the other \nhand, when replacing an entity itself with a [MASK] \ntoken, as suggested in this study, the entity exists as a \ntoken, and the output vector corresponding to the token \ncontains contextual information that helps to effectively \npredict the meaning of the original word for the token \n(i.e., an entity) (Fig. 4).\nFinally, it can alleviate the out-of-vocabulary (OOV) \nproblem that occurs when a token that the model did not \nlearn during training is introduced as input data. This \nproblem can be improved using word piece tokenizing \nwith BERT [20] but is not fully resolved. Tasks includ -\ning named entity recognition and relation classification \nare likely to cause OOV problems because entities often \ncontain proper nouns that have many variations of case \nor abbreviation (e.g., BERT, Bert, bert, bert algorithm). If \nan OOV problem occurs and the model cannot recognize \nentity tokens in a sentence correctly, it may struggle to \npredict the relation type between the entities. By replac -\ning the entity with the [MASK] token, the OOV problems \ncan be more effectively prevented.\nHowever, the masked input method has a fatal draw -\nback in that it loses the original token information of \nthe entity. Therefore, we propose the two masked sen -\ntence input method, the masked input method coupled \nwith the two-sentence input method, to overcome this \nweakness.\nTwo‑sentence input\nTwo-sentence input is a method of utilizing two identi -\ncal sentences that are linked with [SEP] tokens as input \ndata in fine-tuning. In this paper, specifically, we propose \nthe two masked sentence input method, which masks one \nof the two entities in each of the sentences (Fig.  5). The \nmasked entities are different from each other to keep one \nof the original entities unmasked, which is more benefi -\ncial than simply linking the duplicate of the sentence.\nThe first advantage of this method is that it maintains \nconsistency in the pre-training and fine-tuning stages. In \nthe original BERT paper, the pre-training stage exploited \ntwo sentences linked with the [SEP] token. During fine-\ntuning for the classification task, however, only one \nsentence was used, which can be disadvantageous to \nthe performance due to the fact that the model’s learn -\ning process is inconsistent. The second advantage is that \nthis method prevents information loss associated with \nthe masked input methodology. Within sentences that \nFig. 3 Entity marker–entity start: Input sentence with additional \nmarker tokens\nFig. 4 Masked input: Input sentence masked as [MASK] for entities\nFig. 5 Two masked sentence input\nFig. 6 Two-sentence entity token input\nPage 11 of 18\nLee et al. BMC Medical Informatics and Decision Making          (2022) 22:234 \n \nhave multiple entities, we can preserve token information \nby replacing only one entity at a time with the [MASK] \ntoken. This method completely prevents the loss of token \ninformation that can be caused by using masked input. \nFinally, this method conveys sequential information \nabout the entity to the model. In relation extraction tasks, \nthe relationship might be decided differently if the order \nof the first entity and the second entity are reversed. \nTherefore, it is critical that the model accurately recog -\nnizes the order of entities. Soares [36] used additional \nnumbered marker tokens to carry sequence information \nto the model, but there is a limit to the model’s recogni -\ntion of additional tokens that have been unseen during \npre-training. In our two masked sentence input method, \nthe first entity is replaced with the [MASK] token in the \nfirst sentence, and the second entity is replaced with the \n[MASK] token in the second sentence, effectively passing \nthe semantic and contextual information corresponding \nto the first and second entities to the model in order.\nAdditionally, for multilateral comparison, we included \nanother variant method—named the two-sentence entity \ntoken input method— in the comparison experiment \n(Fig. 6). This is a combination of the entity marker–entity \nstart and the two masked sentence input, which is replac -\ning two entities in a sentence differently with additional \nentity tokens, [E1] and [E2]. Since this is also a two-sen -\ntence input strategy, the two replaced tokens for each \nentity are differentiated from each other.\nDownstream layer structure\nWe also make a slight modification to the standard \nBERT downstream architecture for classification from \nthe original BERT paper [20] which uses a special token \n[CLS]. Instead of mapping the entire input sequence to \nonly a [CLS] special token, we produce two vectors cor -\nresponding to the location of the entities, to acquire \nsemantic representations for each entity. With these vec -\ntors as input, the added classification layer optimizes the \nweights for determining relations between entities. This \nmethod is suggested in [36] and turned out to have the \nadvantage of more effective learning for relation predic -\ntion by directly utilizing vectors corresponding to infor -\nmation of entities.\nFor comparison, we experimented with the conven -\ntional [CLS] token vector method, the aforementioned \nmethod that utilizes both vectors corresponding to enti -\nties, and the method that uses those two vectors along \nwith the [CLS] token vector altogether. We call these \nthree methods respectively CLS token layer, two-token \nlayer, and three-token layer.\nIn the CLS token layer, the input size of the classifica -\ntion output layer is set to 512, which is the output size \nof each layer of the pre-trained BERT model. In the two \ntoken layer, the input size is set to 2 × 512 (1024), which \nis the size of two outputs combined. In the three-token \nlayer, which receives three BERT layer outputs as input, \nthe input size is set to 3 × 512 (1536), which is the size \nof three outputs combined. The output size of the clas -\nsification layer for all the methods is set to 8, which is \nthe number of relation types or the number of classes \nto be predicted. The loss function of the model is cross-\nentropy loss.\nOur three main experiments–comparison of pre-\ntrained BERT models, comparison of masking input \nmethods, comparison of classification–use each model’s \nPyTorch implementation by HuggingFace. AdamW was \nused as an optimizer in all models, and the performance \nwas reported by applying the best-performing model to \nthe test data after training up to 10 epochs. The com -\nmonly applied hyperparameters are shown in Table 4.\nThe overview of the study is illustrated in Fig. 7.\nResults\nDataset overview\nWe constructed a semantic relation corpus consisting of \n1,346 abstracts annotated with 5,031 relations classified \ninto eight types. A total of 10,062 distinct bio-entities \nof 12 types were annotated with half of then in the left \nportion of the sentence and the rest in the right portion \nof the sentence. The general statistics for the corpus are \nshown in the table below (Table 5).\nThe verb in the sentence is one point of reference \nwhen classifying the relation type; however, it does not \nexclusively determine the relation type. A relation type \nis determined by comprehensively considering the verb \nand verb-related information in the sentence, contextual \ninformation around entities, and human interpretation. \nFor example, common context words such as “inhibition” \nand “decreased” can reverse the meaning of a verb. Thus, \nthe semantic relation type assigned to the sentence might \nbe the opposite of the original meaning of the verb.\nTable 4 Hyper parameters\nHyper Parameter\nnum_train_epochs 10\nlearning_rate 5e-5\nper_device_train_batch_size 16\nper_device_eval_batch_size 64\nwarmup_ratio 0.1\nweight_decay 0.01\nadam_beta1 0.9\nadam_beta2 0.999\nadam_epsilon 1e-8\nmax_grad_norm 1\nPage 12 of 18Lee et al. BMC Medical Informatics and Decision Making          (2022) 22:234 \nFig. 7 The overview of the study\nTable 5 A Entity types. B Relation types. Dataset overview\nTrain Validate Test\nLeft Right Left Right Left Right\nA\nBIOLOGICAL PROCESS 181 825 61 301 70 270 1,708\nCELL 60 25 15 12 16 12 140\nCOMPOUND 769 225 232 75 266 72 1,639\nDNA 4 1 4 9\nENZYME 67 34 28 8 20 17 174\nGENE 1,017 598 347 180 328 185 2,655\nHORMONE 25 10 6 3 7 4 55\nMOLECULAR FUNCTION 59 71 24 26 21 20 221\nPHENOTYPE 494 1053 168 355 160 368 2,598\nPROTEIN 241 142 84 33 84 46 630\nRNA 91 33 34 13 33 13 217\nVIRUS 10 1 3 2 16\n3,018 3,018 1,006 1,006 1,007 1,007 10,062\nTrain Validate Test\nB\nDirected Link 484 174 175 833\nNegative Cause 403 141 156 700\nNegative Decrease 208 82 74 364\nNegative Increase 145 52 48 245\nPositive Cause 662 219 206 1087\nPositive Decrease 89 18 26 133\nPositive Increase 172 54 47 273\nUndirected Link 855 266 275 1396\n3018 1006 1007 5031\nPage 13 of 18\nLee et al. BMC Medical Informatics and Decision Making          (2022) 22:234 \n \nSemantic relation classification model\nPerformance comparison between pre‑trained models\nWhile applying an effective masking methodology for \ninput sentences and downstream layers to the model is \ncrucial, it is also important to select a pre-trained lan -\nguage model that best fits our tasks and data as the base \nmodel. Therefore, we compared the performance of \nvarious existing pre-trained BERT models for our data -\nset with the same hyperparameters specified in Table  4, \nsetting two masked sentence input and two-token layer \nfor all models. The models were evaluated using fivefold \ncross-validation with all of the train, validation, and test \nsets combined (Table 6).\nComparisons have shown that PubMedBERT models \npre-trained on abstracts from PubMed and full-text arti -\ncles from PubMedCentral performed better than others. \nThis confirms that when building a downstream model \nusing a pre-trained language model, the data used for \npre-training should be homogeneous to those used for \nfine-tuning. Therefore, PubMedBERT, which performed \nthe best on our PubMed datasets, will be used as the base \nmodel in later experiments.\nPerformance comparison between methods of masking input\nFor this experiment, we used PubMedBERT, the language \nmodel with the best performance in the abovementioned \ncomparative experiment, as the base model and the CLS \ntoken layer as the downstream output layer. The methods \nwere evaluated using fivefold cross-validation with all of \nthe train, validation, and test sets combined (Table 7).\nWe specifically compared the performance of the fol -\nlowing methods: default method, which only adds \n[CLS] token in front of a sentence, entity marker–entity \nstart, which marks a span of entities, masked input, two \nmasked sentence input, and two sentence entity token \ninput, which masks entities with additional tokens other \nthan [MASK].\nThe comparison of input methods showed that the two \nsentence entity token input and the two masked sentence \ninput methods, which used two combined sentences as \ninput data, performed better than default method, the \nTable 6 Performance comparison of pre-trained language models\nThe best scores are in bold\n* Mean\n** Standard deviation\na Bert-base-uncased, Accessed July 20, 2022, Available from: https:// huggi ngface. co/ bert- base- uncas ed\nb Biobert-base-cased-v1.2, Accessed July 20, 2022, Available from: https:// huggi ngface. co/ dmis- lab/ biobe rt- base- cased- v1.2\nc BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext, Accessed July 20, 2022, Available from: https:// huggi ngface. co/ micro soft/ Biome dNLP- PubMe dBERT- base- \nuncas ed- abstr act- fullt ext\nd Roberta-base, Accessed July 20, 2022, Available from: https:// huggi ngface. co/ rober ta- base\ne Scibert_scivocab_uncased, Accessed July 20, 2022, Available from: https:// huggi ngface. co/ allen ai/ scibe rt_ scivo cab_ uncas ed\nModel Accuracy Precision Recall F1‑score\nBERT [20]a *0.849 **(0.003) 0.817 (0.010) 0.822 (0.019) 0.818 (0.011)\nBioBERT [31]b 0.861 (0.008) 0.835 (0.017) 0.846 (0.015) 0.839 \n(0.011)\nPubMedBERT[34]c 0.865 (0.014) 0.833 (0.020) 0.849 (0.009) 0.839 \n(0.015)\nRoBERTa [35]d 0.862 (0.009) 0.835 (0.018) 0.837 (0.009) 0.835 (0.010)\nSciBERT [32]e 0.862 (0.010) 0.836 (0.017) 0.843 (0.013) 0.838 (0.013)\nTable 7 Performance comparison of masking input methods\nThe best scores are in bold\n* Mean\n** Standard deviation\nMethod Accuracy Precision Recall F1‑score\nDefault *0.700 **(0.018) 0.646 (0.019) 0.647 (0.016) 0.642 (0.013)\nEntity Marker–Entity Start 0.857 (0.012) 0.825 (0.029) 0.836 (0.007) 0.828 (0.015)\nMasked Input 0.844 (0.013) 0.811 (0.023) 0.826 (0.011) 0.817 (0.012)\nTwo Masked Sentence Input 0.866 (0.014) 0.837 (0.018) 0.847 (0.013) 0.840 (0.014)\nTwo Sentence Entity Token Input 0.865 (0.012) 0.839 (0.019) 0.845 (0.014) 0.840 (0.015)\nPage 14 of 18Lee et al. BMC Medical Informatics and Decision Making          (2022) 22:234 \nentity marker–entity start method or the masked input \nmethod. Two masked sentence input, which replaces \nentities with [MASK] tokens, was superior to using addi -\ntional tokens, [E1] and [E2] tokens, as a way to replace \nentities.\nThese findings show that leveraging [MASK] tokens is \nbetter than introducing additional tokens such as [E1] \nand [E2] to replace entities. This confirms that maintain -\ning consistency between the pre-training and fine-tuning \nstages can lead to improved performance. Furthermore, \n[MASK] tokens, which have only been used in pre-train -\ning phases, can be appropriately utilized in downstream \ntasks. In addition, the two sentence input methodology \nperformed better than the methodologies where only one \nsentence is entered as input; this finding suggests that in \nsuch a relation classification task, using two identical sen-\ntences, each of which contains a masked entity, can lead \nto improved performance.\nPerformance comparison of downstream layers\nAdditional downstream layer construction is essential \nto fine-tuning a model for specific NLP tasks, using pre-\ntrained models. Relation extraction models based on \nBERT require the addition of the classification output \nlayer for relation prediction after the output of the trans -\nformer encoder. For performance comparisons between \ndifferent layer structures, we equally apply the two \nmasked sentence input methodology to the same Pub -\nMedBERT model with the same hyperparameters as the \nprevious experiments. In this experiment, we compare \nthe performance of the CLS token layer, using the output \nof the [CLS] token location, two-token layer, using the \noutput values of the two [MASK] token locations, and \nthree-token layer, using the output values of the [CLS] \nand two [MASK] token locations. The layer architectures \nwere evaluated using fivefold cross-validation with all of \nthe train, validation, and test sets combined (Table 8).\nThe two-mask token layer showed better performance \nthan the other two techniques. Even though it is not a \nlarge increase in performance, concatenating two vec -\ntors corresponding to entities and utilizing them for the \nprediction of relations is a better method than mapping \nthe entire input sequence to only a [CLS] special token \nor even just using a [CLS] token, which is a representa -\ntion of the entire sentence. Therefore, we can conclude \nthat the two-token layer is the architecture that produces \nrepresentations, which hold the most useful semantic \ninformation to predict a proper relation between the two \nentities. In other words, the number or location of the \nfinal output vectors in the downstream layer does matter \nin a RE task.\nFinal comparison of model performance\nThe experiments comparing different base models, input \nmethodologies, and output layer structures using our \ndatasets showed that utilizing PubMedBERT as a base \nmodel with the two masked sentence input methodol -\nogy and two-token layer applied performs best. Finally, \nwe compared this model with existing models presented \nTable 8 Performance comparison of downstream layers\nThe best scores are in bold\n*Mean\n**Standard deviation\nLayer architecture Accuracy Precision Recall F1‑score\nCLS token layer *0.858 **(0.016) 0.823 (0.026) 0.835 (0.015) 0.827 (0.017)\nTwo‑token layer 0.867 (0.014) 0.840 (0.018) 0.847 (0.011) 0.842 (0.013)\nThree-token layer 0.861 (0.013) 0.835 (0.027) 0.842 (0.009) 0.836 (0.016)\nTable 9 Performance comparison against models from related \nworks\nThe best scores are in bold\nModel F1\nWord2vec + CNN [38] 0.708\nEntity Attention Bi-LSTM [13] 0.787\nMatching the Blanks [36] 0.799\nOur Model 0.852\nTable 10 Per-class performance\nSupport: Number of instances in the test data represents 20% of the full data set, \nwhich is proportional to each class\nClass Precision Recall F1‑score Support\nDirected Link 0.949 0.851 0.898 175\nNegative Cause 0.902 0.888 0.895 156\nNegative Decrease 0.909 0.918 0.914 74\nNegative Increase 0.837 0.891 0.863 48\nPositive Cause 0.907 0.895 0.901 206\nPositive Decrease 0.714 0.741 0.727 26\nPositive Increase 0.759 0.732 0.746 47\nUndirected Link 0.840 0.906 0.872 275\n0.852 1007\nPage 15 of 18\nLee et al. BMC Medical Informatics and Decision Making          (2022) 22:234 \n \nin related works using our dataset. In addition to BERT-\nbased models that have shown SOTA performance in \nrelation extraction tasks, such as [36], we also included \nmodels based on other deep learning algorithms such as \nthe CNN [38] and entity attention Bi-LSTM, which is a \nsemantic relation classification model using bidirectional \nLSTM networks with entity-aware attention using latent \nentity typing [13]. In this final experiment, models were \ntrained on a train set and evaluated on a validation set. \nThe Table  9 reports the scores of the best models from \nthe validation step evaluated on the test set.\nThe experimental results confirm that the final pro -\nposed PubMedBERT-based model with the two masked \nsentence input methodology and two token layer per -\nformed best. The best model is illustrated in Fig. 8.\nTo determine how well our model predicts on each \nclass and examine situations where our model has limita-\ntions, we further analyzed the per-class performance for \neach of the eight types of relations (Table 10).\nIn general, per-class performance relied on the number \nof data instances under the class. Classes designated as \npositive decrease, and positive increase, which had the \nfewest data instances of 26 and 47, respectively, obtained \nthe lowest f1 scores among the different relation types. \nOther than the issue, the model showed generally even \nscores over the classes.\nWe closely examined the data points where the value \npredicted by the model differed from the annotated \ntarget value to objectively assess the limitations of our \nmodel or corpus and obtain insights for future reinforce -\nment. As a result of the observation, we were able to \nidentify two interesting patterns in false cases.\nFirst, our model revealed its weakness when the verb \nbetween entities did not directly convey the meaning \nof increase/decrease or a cause-and-effect relationship, \nsuch as “improve, ” “exacerbate, ” and “aggravate, ” making \nit difficult to accurately infer the relationship through \ncontext words surrounding entities. In this case, to cor -\nrectly determine the direction in the quantity of the \nright entity, knowing whether the entity instance itself \nheld a positive or negative meaning was necessary, such \nas in the sentence below:\nFig. 8 Best model: PubmedBERT-based model with the two masked sentence input method and two token layer applied\nPage 16 of 18Lee et al. BMC Medical Informatics and Decision Making          (2022) 22:234 \nMoreover, hepatic knockdown of HFREP1 improved \ninsulin resistance in both mice fed a high-fat diet \nand ob/ob mice\nThe target relation type associating “HFREP1” and \n“insulin resistance” belongs to the negative decrease \nclass, but the model incorrectly predicted it as the nega -\ntive increase class. To accurately classify their relation -\nship, in this case, the model needs to know whether \ninsulin resistance itself has a positive or negative mean -\ning. This type of error could be alleviated with a language \nmodel pre-trained on richer literature in the biomedi -\ncal field, resulting in more comprehensive coverage of \nsemantic meaning for bio-vocabulary.\nSecond, we found several cases of errors due to the \nconflict between the annotators’ contextual consid -\nerations of the entirety of the literature findings and the \nmodel predictions that exploit contextual words limited \nto each sentence in classifying the relationship between \nentities. An example of this is as follows:\nOur data suggest that titanium particles may cause \nless leukocyte activation and inflammatory tissue \nresponses than other particulate biomaterials used \nin total joint arthroplasty.\nFor this sentence, the annotator classified the relation -\nship between titanium particles and inflammatory tissue \nresponse as the negative cause class, and the model pre -\ndicted the positive cause class. The annotator compared \nthe relationship between these two entities to other enti -\nties in the sentence and focused on the intention of the \nsentence. However, if we simply considered the direc -\ntional association between the two entities of interest, \nwe could assign the positive cause class, which was the \nmodel prediction.\nTo avoid this controversial gray area, the data that \nrequired abstract and complex consideration of con -\ntext were excluded as much as possible from the corpus \nconstruction stage; as a result of this, few of these cases \nwere found. However, we specifically paid attention to \nthis example because it provided insights on how the \nmodel prediction works in these special circumstances \nand which direction to move forward in future research \nto overcome this limitation. In the example sentence, \nthe model prediction cannot be regarded as wrong, but \nthe main finding conveyed in the sentence must have \nbeen that titanium particles cause “less” inflammatory \nreactions, not the fact that they do. Therefore, this case \ndemonstrates that meaningful relation types, which bet -\nter reflect the intentions of the text and provide benefit \nto researchers, require elaborately reflecting not only the \ncausality between entities and its direction but also the \nrelative extent of the increased or decreased amount of \na particular entity. This is possible by pushing beyond \nthe limits of the current relation classification based on \nbinary entities and addressing subtle and complicated \ninteractions among multiple bio-entities appearing in a \nsentence.\nConclusion\nMachine learning-based relational classification tasks can \nbe successfully performed based on good quality training \ndata and well-designed algorithms. Especially, as Trans -\nformer-based algorithms become mainstream in NLP , the \nimportance of quality datasets rather than complex fea -\nture engineering is increasing. Thus, constructing train -\ning datasets annotated with bio-entities and the relations \nbetween them is an urgent task to promote text mining \nresearch in biomedical fields.\nIn this paper, we developed a corpus with a wide range \nof bio-entities, such as biological processes, cells, com -\npounds, DNA, enzymes, genes, hormones, molecular \nfunctions, phenotypes, proteins, RNA, and viruses, along \nwith their annotated semantic relations. The construc -\ntion of a corpus with multiple types of bio-entities and \ntheir rich relationships is essential to extracting complex \nand significant biological information from a wide range \nof bio-entity types that the biomedical literature con -\ntains. Considering this need, our newly constructed cor -\npus, built by manually tagging a wide range of bio-entities \nand their relations from a rich amount of biomedical lit -\nerature, represents a significant contribution. We com -\nprehensively annotated verbs situated between entities, \ncontextual information, such as positive/negative and \nactive/passive information that affects their meanings, \nand other meaningful information in the sentence as fea -\ntures to consider in assigning semantic relation types, \nopening the possibility of further research on semantic \nrelations. This corpus could be used as a reliable refer -\nence standard in the development of text mining systems.\nAnother contribution of this paper is that we dem -\nonstrated the utilization of the dataset that we built \nby training and evaluating BERT-based classification \nmodels leveraging the data and further presented a way \nto improve the performance of the relation classifica -\ntion task. Tweaking existing BERT-based models that \nare already known to show good performance for the \nclassification task, we devised a new technique that can \nachieve better performance by alleviating the limita -\ntions of existing models for RE. By introducing a [MASK] \ntoken respectively on two identical input sentences, we \neffectively improved problems such as OOV words and \ninconsistency between pre-training and fine-tuning that \nafflict existing relation extraction models. In the overall \ncomparison experiment between our model with all of \nthe suggested methods applied and the existing models \nPage 17 of 18\nLee et al. BMC Medical Informatics and Decision Making          (2022) 22:234 \n \nsuggested in the related works, our proposed model \nshowed the best performance, confirming that this \nmethodology is effective in fine-tuning BERT-based pre-\ntrained models for relation classification tasks.\nIn summary, the developed dataset for semantic rela -\ntion classification was successfully applied to train the \nclassification model. Therefore, this could be used as a \nvaluable resource for similar text mining research. We \nalso made significant improvements to the algorithms of \nthe relation classification model. We expect that the bio -\nlogical information extracted with high accuracy through \nour proposed dataset and relation extraction technique \nwill be used as a trusted source of information in the \ndevelopment of a biomedical text mining system. We also \nbelieve that the annotation processes we elaborated on \nhere will be of significant help to fellow researchers per -\nforming similar work.\nHowever, there remains room for improvement. To \nbegin with, in the per-class performance analysis, our \nsemantic relation based on the causality of binary enti -\nties and its direction showed limitations in sufficiently \ndescribing complex semantic associations among bio-\nentities in a sentence. For example, in cases where the \nrelative intensity of the association needs to be revealed \nfor meaningful knowledge discovery, the current rela -\ntion type might be insufficient. To tackle this problem, \nintroducing a complex semantic relation, of which the \ndegree is higher than two, can be considered. Also, we \nare exploring ways of implementing RE with generative \nmodels such as T5, allowing the models to output sen -\ntences, which will be a direct and flexible answer reflect -\ning the relation’s subtle nuance to the given prompt. \nSecondly, in the comparative experiment of the masking \ninput method, while our proposed methods–two-sen -\ntence input, masked input, and combined or modified \nmethods of these two–, were significantly superior to the \noriginal BERT method, the difference among them was \ntrivial. Especially, our best method, two masked sentence \ninput, outperformed entity marker-entity start, which \nwas proposed in the previous study [36] by only a small \nmargin. We additionally conducted an analysis of vari -\nance with the multiple measurements obtained through \nk-fold cross-validation, shown in Section  D in  Addi -\ntional file 1, and the performance difference among non-\noriginal BERT methods was not statistically significant, \nalthough it was possible to determine which model or \nmethodology is better than the rest. These minor dif -\nferences between suggested structures warrant more \nin-depth future research, leading to a novel effective \ninput treatment for RE providing significantly improved \nperformance compared with the existing methods. If \nfollow-up studies are conducted to address these listed \nlimitations based on the realizations obtained through \nthe experiments and analysis in this paper, we can expect \nfurther improvement in constructing a dataset and deep \nlearning model for effective semantic relation classifica -\ntion to be achieved in the near future.\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. \norg/ 10. 1186/ s12911- 022- 01977-5.\nAdditional file 1. Overview of Corpora.\nAcknowledgements\nNot applicable.\nAuthor contributions\nYL wrote the main manuscript text. JS conducted model experiments. MS \nconfirmed the contents of the manuscript. All authors reviewed the manu-\nscript. All authors read and approved the final manuscript.\nFunding\nThis work was supported by the National Research Foundation of Korea (NRF) \ngrant funded by the Korea government (MSIT) (No. 2022R1A2B5B02002359).\nAvailability of data and materials\nThe datasets generated during the current study are available at https:// \ngithub. com/ tsmmb io/ BertS RC.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare that they have no competing interests.\nAuthor details\n1 Department of Library and Information Science, Yonsei University, Seoul, \nSouth Korea. 2 Department of Digital Analytics, Yonsei University, Seoul, South \nKorea. \nReceived: 7 March 2022   Accepted: 11 August 2022\nReferences\n 1. Onye SC, Akkeles A, Dimililer N. Review of biomedical relation extraction, \nEuropean International. J Sci Technol. 2017;6:1–14.\n 2. Zhou D, Dayou Z, Yulan H. Biomedical relation extraction: from binary to \ncomplex. Comput Math Methods Med. 2014;2014:5589.\n 3. Chapman WW, Cohen KB. Guest editorial: current issues in biomedi-\ncal text mining and natural language processing. J Biomed Inform. \n2009;42(5):757–9. https:// doi. org/ 10. 1016/j. jbi. 2009. 09. 001.\n 4. Kilicoglu H, Rosemblat G, Fiszman M, Shin D. Broad-coverage biomedical \nrelation extraction with SemRep. BMC Bioinform. 2020;21(1):1–28. https:// \ndoi. org/ 10. 1186/ s12859- 020- 3517-7.\n 5. Luo Y, Uzuner Ö, Szolovits P . Bridging semantics and syntax with graph \nalgorithms—state-of-the-art of extracting biomedical relations. Brief \nBioinform. 2017;18(1):160–78.\n 6. Gurulingappa H, Rajput AM, Roberts A, Fluck J, Hofmann-Apitius M, Toldo \nL. Development of a benchmark corpus to support the automatic extrac-\ntion of drug-related adverse effects from medical case reports. J Biomed \nInform. 2012;45(5):885–92. https:// doi. org/ 10. 1016/j. jbi. 2012. 04. 008.\nPage 18 of 18Lee et al. BMC Medical Informatics and Decision Making          (2022) 22:234 \n 7. van Mulligen EM, Fourrier-Reglat A, Gurwitz D, Molokhia M, Nieto A, Trifiro \nG, Kors JA, Furlong LI. The EU-ADR corpus: annotated drugs, diseases, tar-\ngets, and their relationships. J Biomed Inform. 2012;45(5):879–84. https:// \ndoi. org/ 10. 1016/j. jbi. 2012. 04. 004.\n 8. Kanjirangat V, Rinaldi F. Enhancing biomedical relation extraction with \ntransformer models using shortest dependency path features and triplet \ninformation. J Biomed Inform. 2021;122:103893.\n 9. Li M, et al. The protein-protein interaction ontology: for better represent-\ning and capturing the biological context of protein interaction. BMC \nGenom. 2021;22(5):1–10.\n 10. Pyysalo S, Airola A, Heimonen J, et al. Comparative analysis of five protein-\nprotein interaction corpora. BMC Bioinform. 2008;9:S6.\n 11. Zong C, Xia R, Zhang J. Information extraction. In: Text Data Mining. \nSpringer; 2021. pp. 256–269.\n 12. Zeng D., et al. Relation classification via convolutional deep neural net-\nwork. In: Proceedings of COLING 2014, the 25th international conference \non computational linguistics: technical papers. 2014.\n 13. Lee J, Seo S, Choi YS. Semantic relation classification via bidirectional \nLSTM networks with entity-aware attention using latent entity typing. \nSymmetry. 2019;11(6):785.\n 14. Geng ZQ, Chen GF, Han YM, Lu G, Li F. Semantic relation extraction \nusing sequential and tree-structured LSTM with attention. Inf Sci. \n2020;509:183–92. https:// doi. org/ 10. 1016/j. ins. 2019. 09. 006.\n 15. Xiao M, Cong L. Semantic relation classification via hierarchical recurrent \nneural network with attention. In: Proceedings of COLING 2016, the 26th \nInternational Conference on Computational Linguistics: Technical Papers. \n2016.\n 16. Xu K., et al. Semantic relation classification via convolutional neural \nnetworks with simple negative sampling. arXiv preprint arXiv: 1506. 07650 \n(2015).\n 17. Shen XHY. Attention-based convolutional neural network for semantic \nrelation extraction. Anaesthesia Crit Care Pain Med. 2017;36(6):411–8. \nhttps:// doi. org/ 10. 1016/j. accpm. 2017. 08. 001.\n 18. Kim B, Choi W, Lee H. A corpus of plant–disease relations in the biomedi-\ncal domain. PLoS ONE. 2019;14(8):e0221582.\n 19. Vaswani A, et al. Attention is all you need. Adv Neural Inform Process Syst. \n2017;30:558.\n 20. Devlin J., et al. Bert: Pre-training of deep bidirectional transformers for \nlanguage understanding. arXiv preprint arXiv: 1810. 04805 (2018).\n 21. Radford A., et al. Improving language understanding by generative pre-\ntraining. (2018).\n 22. Raffel C, et al. Exploring the limits of transfer learning with a unified text-\nto-text transformer. J Mach Learn Res. 2020;21(140):1–67.\n 23. Liu J, et al. Relation classification via BERT with piecewise convolution and \nfocal loss. Plos One. 2021;16(9):e0257092.\n 24. Mitra A, et al. Relation classification for bleeding events from electronic \nhealth records using deep learning systems: an empirical study. JMIR \nMed Inform. 2021;9(7):e27527.\n 25. Hong G, et al. BioPREP: deep learning-based predicate classification with \nSemMedDB. J Biomed Inform. 2021;122:103888.\n 26. Le NQK, Quang-Thai H. Deep transformers and convolutional neural \nnetwork in identifying DNA N6-methyladenine sites in cross-species \ngenomes. Methods. 2022;204:199–206.\n 27. Le NQK, et al. A transformer architecture based on BERT and 2D convolu-\ntional neural network to identify DNA enhancers from sequence informa-\ntion. Briefings Bioinform. 2021;22(5):bbab005.\n 28. Aum S, Choe S. srBERT: automatic article classification model for system-\natic review using BERT. Syst Rev. 2021;10(1):1–8.\n 29. Latouche C, Natoli A, Reddy-Luthmoodoo M, Heywood SE, Armit-\nage JA, Kingwell BA. MicroRNA-194 modulates glucose metabolism \nand its skeletal muscle expression is reduced in diabetes. PLoS ONE. \n2016;11(5):e0155108–e0155108. https:// doi. org/ 10. 1371/ journ al. pone. \n01551 08.\n 30. Song M, Kim WC, Lee D, Heo GE, Kang KY. PKDE4J: entity and rela-\ntion extraction for public knowledge discovery. J Biomed Inform. \n2015;57:320–32. https:// doi. org/ 10. 1016/j. jbi. 2015. 08. 008.\n 31. Lee J, Yoon W, Kim S, Kim D, Kim S, So CH, Kang J. BioBERT: a pre-trained \nbiomedical language representation model for biomedical text mining. \nBioinformatics. 2020;36(4):1234–40. https:// doi. org/ 10. 1093/ bioin forma \ntics/ btz682.\n 32. Beltagy I, Kyle L, Arman C. SciBERT: a pretrained language model for \nscientific text. arXiv preprint arXiv: 1903. 10676 (2019).\n 33. Gu Y, et al. Domain-specific language model pretraining for biomedical \nnatural language processing. ACM Trans Comput Healthc. 2021;3(1):1–23.\n 34. Lan Z, et al. Albert: A lite bert for self-supervised learning of language \nrepresentations. arXiv preprint arXiv: 1909. 11942 (2019).\n 35. Liu Y, et al. Roberta: a robustly optimized bert pretraining approach. arXiv \npreprint arXiv: 1907. 11692 (2019).\n 36. Soares LB, et al. Matching the blanks: distributional similarity for relation \nlearning. arXiv preprint arXiv: 1906. 03158 (2019).\n 37. Yang Z, et al. Xlnet: generalized autoregressive pretraining for language \nunderstanding. Adv Neural Inform Process Syst. 2019;32:559.\n 38. Nguyen TH, Ralph G. Relation extraction: Perspective from convolutional \nneural networks. In: Proceedings of the 1st workshop on vector space \nmodeling for natural language processing. 2015.\n 39. Kim JD, Ohta T, Tateisi Y, Tsujii J. GENIA corpus-a semantically annotated \ncorpus for bio-textmining. Bioinformatics. 2003;19(SUPPL. 1):180–2. \nhttps:// doi. org/ 10. 1093/ bioin forma tics/ btg10 23.\n 40. MedlinePlus, What is a gene? https:// medli neplus. gov/ genet ics/ under \nstand ing/ basics/ gene/, 2021\n 41. Hirsch ED. The new dictionary of cultural literacy: what every american \nneeds to know. Boston: Houghton Mifflin; 2002.\n 42. National Institute of General Medical Sciences, What is genetics? https:// \nwww. nigms. nih. gov/ educa tion/ fact- sheets/ Pages/ genet ics. aspx, 2021\n 43. Alberts B, Johnson A, Lewis J, Raff M, Roberts K, Walter P . Molecular biol-\nogy of the cell. 4th ed. New York: Garland Science; 2002.\n 44. Shuster M. Biology for a changing world, with physiology. 2nd ed. New \nYork: Springer; 2014. p. 2014.\n 45. Neave N. Hormones and behaviour: a psychological approach. Cam-\nbridge: Cambridge Univ. Press; 2008.\n 46. MedlinePlus, Hormones. https:// medli neplus. gov/ hormo nes. html, 2021. \nAccessed 29 Aug 2021.\n 47. Encyclopedia Britannica, Hormones. https:// www. brita nnica. com/ summa \nry/ hormo ne, 2021.\n 48. GENIA corpus, http:// www. genia proje ct. org/ genia- corpus/ relat ion- \ncorpus, 2022. Accessed Feb 2022.\n 49. BioNLP Shared Task, https:// sites. google. com/ site/ bionl pst/ bionlp- \nshared- task- 2011/ entity- relat ions- suppo rting- task- rel, 2022. Accessed Feb \n2022.\n 50. Nédellec C. Learning language in logic-genic interaction extraction \nchallenge. In: 4. Learning language in logic workshop (LLL05). ACM-\nAssociation for Computing Machinery, 2005.\n 51. LLL corpus, http:// genome. jouy. inra. fr/ texte/ LLLch allen ge/, 2022. \nAccessed Feb 2022.\n 52. BioCreative-ii corpus, https:// biocr eative. bioin forma tics. udel. edu/ resou \nrces/ corpo ra/ biocr eative- ii- corpus/, 2022. Accessed Feb 2022.\n 53. Bunescu R, et al. Comparative experiments on learning informa-\ntion extractors for proteins and their interactions. Artif Intell Med. \n2005;33(2):139–55.\n 54. Pyysalo S, et al. BioInfer: a corpus for information extraction in the bio-\nmedical domain. BMC Bioinform. 2007;8(1):1–24.\n 55. Fundel K, Küffner R, Zimmer R. RelEx—relation extraction using depend-\nency parse trees. Bioinformatics. 2007;23(3):365–71.\n 56. Ding J, et al. Mining MEDLINE: abstracts, sentences, or phrases? Biocom-\nputing. 2002;2001:326–37.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations."
}