{
  "title": "LMVE at SemEval-2020 Task 4: Commonsense Validation and Explanation Using Pretraining Language Model",
  "url": "https://openalex.org/W3115157649",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2112969637",
      "name": "Shilei Liu",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2096844355",
      "name": "Yu Guo",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2162463123",
      "name": "Bo-Chao Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097001618",
      "name": "Feiliang Ren",
      "affiliations": [
        "Northeastern University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2746236423",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W3113425182",
    "https://openalex.org/W3006963874",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2947337775",
    "https://openalex.org/W2965373594"
  ],
  "abstract": "This paper introduces our system for commonsense validation and explanation. For Sen-Making task, we use a novel pretraining language model based architecture to pick out one of the two given statements that is againstcommon sense. For Explanation task, we use a hint sentence mechanism to improve the performance greatly. In addition, we propose a subtask level transfer learning to share information between subtasks.",
  "full_text": "Proceedings of the 14th International Workshop on Semantic Evaluation, pages 562–568\nBarcelona, Spain (Online), December 12, 2020.\n562\nLMVE at SemEval-2020 Task 4: Commonsense Validation and\nExplanation using Pretraining Language Model\nShilei Liu1, Yu Guo1, Bochao Li1 and Feiliang Ren12∗\n1School of Computer Science and Engineering, Northeastern University\n2Key Laboratory of Data Analytics and Optimization for Smart\nIndustry (Northeastern University), Ministry of Education\n{1901750,1871502,1901725}@stu.neu.edu.cn\nrenfeiliang@ise.neu.edu.cn\nAbstract\nThis paper describes our submission1 to subtask a and b of SemEval-2020 Task 4. For subtask a,\nwe use a ALBERT based model with improved input form to pick out the common sense statement\nfrom two statement candidates. For subtask b, we use a multiple choice model enhanced by hint\nsentence mechanism to select the reason from given options about why a statement is against\ncommon sense. Besides, we propose a novel transfer learning strategy between subtasks which\nhelp improve the performance. The accuracy scores of our system are 95.6 / 94.9 on ofﬁcial test\nset2 and rank 7th / 2nd on Post-Evaluation leaderboard.\n1 Introduction\nCommon sense veriﬁcation and explanation is an important and challenging task in artiﬁcial intelligence\nand natural language processing. This is a simple task for human beings, because human beings can make\nfull use of external knowledge accumulated in their daily lives. However, common sense veriﬁcation\nand reasoning is difﬁcult for machines. According to Wang (2019), even some state-of-the-art language\nmodels such as ELMO(Peters et al., 2018) and BERT(Devlin et al., 2019) have very poor performance. So\nit is crucial to integrate the ability of commonsense-aware to natural language understanding model(Davis,\n2017).\nSemEval-2020 task4(Wang et al., 2020) aims to improve the ability of common sense judgment for\nmodel, and we participated in two subtasks of this task. The dataset of SemEval-2020 task4 named\nComVE. Each instance in ComVE is composed of 5 sentences ⟨s1,s2,o1,o2,o3⟩. s1 and s2 will be used\nfor subtask a, and s1 or s2 with ⟨o1,o2,o3⟩will be used for subtask b.\nSubtask a(also known as Sen-Making task) aims to test a model’s ability of commonsense validation.\nSpeciﬁcally, given two statements ⟨s1,s2⟩whose lexical and syntactic are similar, the object of Sen-\nMaking model is to determine which statement is common sense(compared to another one). For example,\ns1 is put the elephant in the refrigeratorand s2 is put the turkey in the refrigerator, a good model needs to\njudge that the latter is more common sense.\nSubtask b(also known as Explanation task) is a multiple choice task that aims to ﬁnd the key reason\nwhy a given statement does not make sense. For example, given a sentence that violates common sense\nwith three options ⟨s,o1,o2,o3⟩, where sis he put an elephant into the fridge, o1 is an elephant is much\nbigger than a fridge, o2 is elephants are usually gray while fridges are usually white, and o3 is an elephant\ncannot eat a fridge, the model needs to judge that o1 is the correct option.\nThe ofﬁcial baseline of Sen-Making use a pretraining language model(PLM) to dynamic encoding\nthe two input sentence, and use a simple full connection neural network to calculate the perplexities\nrespectively, and choosing the one with lower scores as the correct one. We believe that the baseline\nmethod treats two sentences independently and ignores the inner relationship between the two sentences,\nso we propose a novel model structure that fully considers the interaction between statements.\n∗Corresponding author.\n1Our team name on leaderboard is NEUKG, and the ﬁrst three authors contributed equally to this work.\n2These scores were obtained during the Post-Evaluation phase, and our scores in the Evaluation phase are 90.04 / 93.8.\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://\ncreativecommons.org/licenses/by/4.0/.\n563\nThe ofﬁcial baseline of Explanation treats the task as BERT-like multiple choice task(Devlin et al.,\n2019). We think that the baseline model doesn’t make full use of the input data. So we design a structure\nto incorporate another statement that is common sense to existing model.\nIn addition, we believe that ﬁne-tuning on similar subtask can improve the performance of the current\nsubtask because there are many commonalities between the two subtasks, so we propose a novel transfer\nlearning mechanism between Sen-Making and Explanation.\nThe proposed system named LMVE, it is a neural network model(includes two sub-modules to solve\nboth subtask a and b) bases on large scale pretraining language model.\nOur contributions are as follows:\n• First, we propose subtask level transfer learningthat help share information between subtasks.\n• Second, we propose a novel structure to calculate the perplexity of sentence, which takes into account\nthe interaction between sentences in a pair.\n• Third, we propose the hint sentencemechanism that will help improve the performance of multiple\nchoice task.(subtask b).\n2 System Description\nWe consider our model for both Sen-Making and Explanation as two parts: encoder and decoder. Encoder\nis mainly used for getting the contextual representation of input sentence tokens. In recent years, some\npretraining language models including BERT(Devlin et al., 2019), RoBERTa(Liu et al., 2019) and\nALBERT(Lan et al., 2020) have been proven beneﬁcial for many natural language processing (NLP)\ntasks(Rajpurkar et al., 2016; Bowman et al., 2015). These pretrained models have learned general-purpose\nlanguage representations on a large amount of unlabeled data, therefore, adapting these models to the\ndownstream tasks can bring a good initialization for parameters and avoid training from scratch(Xu et\nal., 2020). So we tried some popular PLMs as encoders. Decoder consists of several simple linear layers\nwhose number of parameters are far less than encoder, and the role of decoder is to fuse the output of\nencoder and predict the answer.\n2.1 LMVE for Sen-Making Task\n>&/6@ 6HQW\u0003\u0014 >6(3@ 6HQW\u0003\u0015 >6(3@\n>&/6@ 6HQW\u0003\u0014>6(3@6HQW\u0003\u0015 >6(3@\n$/%(57\n$/%(57\n/LQHDU\n6RIWPD[\u0003\n(a)\n(b)\n/LQHDU\n3URE\u0003\n>&/6@ 6HQW\u0003\u0014 >6(3@\n>&/6@ >6(3@6HQW\u0003\u0015\n$/%(57\n$/%(57\n/LQHDU\n6RIWPD[\u0003\n/LQHDU\n3URE\u0003\nFigure 1: The model architecture for Sen-Making task, (a) is ofﬁcial baseline and (b) is ours. The yellow\npoint denotes the vector representation of the output sequence(same as below).\nFigure 1(a) is the ofﬁcial baseline model(Wang et al., 2019), which regards two sentences as inde-\npendent individuals. But in ComVE, there are certain similarities(lexical and grammatical) between the\ntwo statements, so we think that the interaction between the two sentences is helpful to improve the\nperformance of the model. Figure 1(b) gives an overview of our model for Sen-Making task which is\nmainly composed of three modules including token encoding, feature fusion and answer prediction.\nEncoding: Let {x1\n0,...,x 1\np}and {x2\n0,...,x 2\nq}represent one-hot vectors for the ﬁrst sentence and\nthe second sentence in an instance, we ﬁrst concatenate them and add some special tokens\nlike Figure 1(b), then we will get two sequences {x[CLS],x1\n0,...,x 1\np,x[SEP ],x2\n0,...,x 2\nq,x[SEP ]}and\n564\n{x[CLS],x2\n0,...,x 2\nq,x[SEP ],x1\n0,...,x 1\np,x[SEP ]}. The two sequences will be fed into ALBERT respec-\ntively. We use Ui ∈Rd×n and Vi ∈Rd×n denote outputs of i-th transformer block in ALBERT, where d\nis the hidden size of model, nis the sequence length, and i∈{0,...,L −1}.\nFusion: Some pretraining language model(BERT et al.) usually take the ﬁrst token (corresponds to\n[CLS]) of the output of last transformer block as the representation of a sequence, but we use the weighted\nsum of the representation of ﬁrst token in all transformer block outputs as the ﬁnal representation3. The\nfollowing equations describe the process of fusion:\nr0\ni = Ui\n0 r1\ni = Vi\n0 (1)\nαi = exp(ωi)∑\nj exp(ωj) (2)\nxk =\nL−1∑\ni=0\nαirk\ni (3)\nwhere ω∈Rd is a trainable parameter and i∈[0,L −1]. We can regard xk as the representation of k-th\nstatement(k∈{0,1}).\nAnswer Prediction: This module maps the output of the fusion layer to a probability distribution of\nanswer. Given w ∈Rd and b∈R as learnable parameters, it calculates the answer possibility as\np(k) = softmax(wxk + b) (4)\nWe deﬁne the training loss as cross entropy loss function:\nLa = −1\nN\nN∑\ni=1\np(yi) (5)\nwhere N is the number of samples in the dataset and yi ∈{0,1}.\n2.2 Hint Sentence mechanism\nBefore formally introducing our model for Explanation task, let’s ﬁrst introduce the hint sentence\nmechanism.\nIn ofﬁcial baseline(Wang et al., 2019), the against common sense statement will be concatenated with\nthree options respectively and fed into the model. We believe that this form of input does not make full\nuse of the data in the ComVE. Speciﬁcally, for an instance ⟨s1,s2,o1,o2,o3⟩in ComVE, it is assumed\nthat s1 does not conform to common sense, then ⟨s2,o1,o2,o3⟩will be used to train the baseline model or\nto predict answer. However, in this process,s1 was abandoned. We believe that another common sense\nstatement(s1) in statement pair contains some useful information, and should be incorporated into our\nmodel.\nSo we propose hint sentence mechanism: A hint sentence is common sense and its lexical and syntactic\nare similar to the given against common sense statement and they differ by only few words. In other\nwords, we call the another sentence in the sentence pair a hint sentence.\nThe process of how the hint sentenceis integrated into the existing model can be referred to next section\nand Figure 2. The results of ablation experiment(Sec 3.6) show that hint sentence mechanism can greatly\nimprove the performance of our model for Explanation task.\n2.3 LVME for Explanation Task\nFigure 2 gives an overview of our model for Explanation task. it also has three modules.\nLet {s0,...,s p}, {h0,...,h q}and {oi\n0,...,o i\nri }represent one-hot vectors for the input statement, hint\nsentence and i-th option in an instance, where i ∈{0,1,2}and p, q and r is the length of them, we\nﬁrst concatenate them and add some special tokens like Figure 2, then we will get three sequences\n3Subsequent experimental results show that the performance using the last 4 layers is the best.\n565\n[CLS] Statement [SEP] Hint \nSentence [SEP] Option A [SEP]\n[CLS] Statement [SEP] Hint \nSentence [SEP] Option B [SEP]\n[CLS] Statement [SEP] Hint \nSentence [SEP] Option C [SEP]\nALBERT\nALBERT\nALBERT\nLinear\nLinear\nLinear\n6RIWPD[\nFigure 2: The model architecture for Explanation task.\n{x[CLS],s0,...,s p,x[SEP ],h0,...,h q,x[SEP ],oi\n0,...,o i\nri ,x[SEP ]}. Then the three sequences will be fed\ninto ALBERT respectively.\nSimilar to last sub-section, each sequence will get a representation vector after fusion, and then the\nthree representation vector will pass a linear layer like Equation 4 to calculate the probability distributions\nof answer.\nWe deﬁne training loss as\nLb = −1\nN\nN∑\ni=1\np(yi) (6)\nwhere N is the number of samples in the dataset and yi ∈{0,1,2}is true label.\n2.4 Subtask Level Transfer Learning\nPretraining \nLanguage Model\nFine-tuning on\nSen-Making Task\nFine-tuning on\nExplanation Task\nTraining on\nExplanation Task\nTraining on\nSen-Making Task\nEncoder\nEncoder\nExplanation\nModel\nSen-Making\nModel\nFigure 3: The process of subtask level transfer learning.\nTransfer learning is a research problem in machine learning that focuses on storing knowledge gained\nwhile solving one problem and applying it to a different but related problem. PLM is a typical example of\ntransfer learning and we call it task level transfer learning.\nSen-Making task and Explanation task are both generalized multiple choice tasks, and there is an asso-\nciation between the input data for them, so we believe that in SemEval-2020 Task 4, ﬁne-tuning on similar\nsubtask can improve the performance of the current subtask.\nSubtask level transfer learningrefers to use the encoder after ﬁne-tuning on subtask a(Sen-Making) to\ntrain subtask b(Explanation) and vice versa. The process of Subtask level transfer learning are shown in\nFigure 3.\n3 Experiments and Analysis\n3.1 Dataset\nComVE include 10000 samples in train set and 1000 samples in dev/test set for both Sen-Making and\nExplanation task. The average length of two statements in the Sen-Making task are both 8.26, exactly the\nsame. The average length of true reasons is 7.63 in Explanation task.\nIt should be noted that in SemEval-2020 Task 4, the test set of Explanation task is issued only after\nSen-Making task is completed, so it is impossible to use the test set of Explanation task to reverse deduce\nthe answer of subtask a test set.\n3.2 Baseline\nTo verify the effectiveness of our model, we used ALBERT to replace the BERT in the ofﬁcial baseline,\nleaving the rest unchanged. We do not perform subtask level transfer learning (Sec 2.4) on them.\n566\n10 20 30 40 50 60 70 80 90 100\nTraining data(subtask a) size in %\n0.90\n0.91\n0.92\n0.93\n0.94\n0.95Accuracy\nw/o tranfer\nours\n10 20 30 40 50 60 70 80 90 100\nTraining data(subtask b) size in %\n0.90\n0.91\n0.92\n0.93\n0.94\n0.95\nw/o tranfer\nours\nFigure 4: Learning curve on the training dataset.\n3.3 Preprocessing\nData Augmentation:To enhance the robustness of our model, we use Google Sheets4 to perform back\ntranslation technology on original texts to get augmented texts. Speciﬁcally, given a training sample\n⟨s1,s2,o1,o2,o3⟩we ﬁrst translate the original statements s1,s2 to French and then translate them back\nto English (denoted as ˆs1, ˆs2). ⟨ˆs1, ˆs2,o1,o2,o3⟩will add to training dataset as a new sample. the size of\nthe dataset has doubled after augmentation.\nTokenization: We employ the tokenizer that comes with the HuggingFace(Wolf et al., 2019) PyTorch\nimplementation of ALBERT. The tokenizer lowercases the input and applies the SentencePiece encod-\ning(Kudo, 2018) to split input words into most frequent subwords present in the pre-training corpus.\nNon-English characters will be removed.\n3.4 Implementation Details\nWe use the Transformers5 toolkit to implemented our model and tune the hyper-parameters according to\nvalidation performance on the development set. The hidden size is equal to the corresponding PLM. To\ntrain our model, we employ the AdamW algorithm(Loshchilov and Hutter, 2019) with the initial learning\nrate as 2e-5 and the mini-batch size as 48.\nWe also prepared an ensemble model consisting of 7 models forSen-Making task and 19 forExplanation\ntask with different hyperparameter settings and random seeds. We used majority voting strategy to fuse\nthe candidate predictions of different models together.\nModel Params Sen-Making Explanation\nRandom - 49.52 32.77\nBERTbase(Devlin et al., 2019) 117M 88.56 85.32\nBERTlarge(Devlin et al., 2019) 340M 86.55 90.12\nXLNet(Yang et al., 2019) 340M 90.33 91.07\nSpanBERT(Joshi et al., 2019) 340M 89.46 90.47\nRoBERTa(Liu et al., 2019) 355M 93.56 92.37\nALBERTbase(Lan et al., 2020) 12M 86.63 84.37\nALBERTlarge(Lan et al., 2020) 18M 88.01 89.72\nALBERTxlarge(Lan et al., 2020) 60M 92.03 92.45\nOurs(ALBERTxxlarge) 235M 95.68 95.48\nOur-ensemble - 95.91 96.39\nTable 1: Performance with different encoder.\n4https://www.google.com/sheets/about\n5https://github.com/huggingface/transformers\n567\n3.5 Main Result\nThe result of our model for subtask a and subtask b are summarized in Table 1. We have tried different\npretraining language model as our encoder, and found that ALBERT based model achieves the state-of-\nthe-art performance.\nFigure 4 shows a learning curve computed over the provided training data with testing against the\ndevelopment set, and we can see that in the case of low-resource (only use 10%-20% training data of\ntarget task), the performance of introducing subtask level transfer learning is signiﬁcantly higher than\noriginal implementation.\nSen-Making Model Acc ∆a\nOur-single 95.68 -\nw/o method b† 94.88 0.80\nw/o data augmentation 95.43 0.25\nw/o weighted sum fusion 95.32 0.36\nw/o subtask level transfer 94.85 0.83\nBaseline 93.24 2.44\nExplanation Model Acc ∆b\nOur-single 95.48 -\nw/o hint sentence 93.47 2.01\nw/o data augmentation 95.39 0.09\nw/o weighted sum fusion 95.11 0.37\nw/o subtask level transfer 94.98 0.50\nBaseline 93.12 2.36\nTable 2: Ablation study on model components. †means we use the model structure as Figure 1(a) and\nBaseline means the model in Sec 3.2.\n3.6 Ablation Study\nTo get better insight into our model architecture, we conduct an ablation study on dev set of ComVE, and\nthe results are shown in Table 2.\nFrom the results we can see that subtask level transfer learninghas a relatively large contribution\nfor both subtask a and b, which conﬁrms our hypothesis that ﬁne-tuning on similar task can improve\nthe performance of the current task. Data augmentation and weighted sum fusion also have minor\ncontributions due to the more robust dataset and more robust model.\nFor subtask a, we can see that compared with baseline method(Figure 1(a)), concatenating another\nsentence as input can have a higher performance. We speculate that the reason is traditional method treat\ntwo statements as independent individuals, and our method takes into account the inherent connection\nbetween the two statements.\nFor subtask b, we can see from Table 2 that hint sentence makes a great contribution the overall\nimprovement. We think the reason is a common sense statement with a similar grammar and syntax does\nhelp the model to determine why the input sentence is against common sense.\n4 Conclusions\nThis paper introduces our system for commonsense validation and explanation. For Sen-Making task, we\nuse a novel pretraining language model based architecture to pick out one of the two given statements\nthat is against common sense. For Explanation task, we use a hint sentence mechanism to improve\nthe performance greatly. In addition, we propose a subtask level transfer learning to share information\nbetween subtasks.\nAs future work, we plan to integrate the external knowledge base(such as ConceptNet6) into common-\nsense inference.\nAcknowledgements\nWe thank the reviewers for their helpful comments. This work is supported by the National Natural\nScience Foundation of China (No. 61572120) and the Fundamental Research Funds for the Central\nUniversities (No.N181602013).\n6http://www.conceptnet.io/\n568\nReferences\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated cor-\npus for learning natural language inference. In Llu ´ıs M`arquez, Chris Callison-Burch, Jian Su, Daniele Pighin,\nand Yuval Marton, editors, Proceedings of the 2015 Conference on Empirical Methods in Natural Language\nProcessing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pages 632–642. The Association for Com-\nputational Linguistics.\nErnest Davis. 2017. Logical formalizations of commonsense reasoning: A survey. J. Artif. Intell. Res., 59:651–\n723.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirec-\ntional Transformers for Language Understanding. InProceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. 2019. Spanbert:\nImproving pre-training by representing and predicting spans. CoRR, abs/1907.10529.\nTaku Kudo. 2018. Subword regularization: Improving neural network translation models with multiple subword\ncandidates. In Iryna Gurevych and Yusuke Miyao, editors, Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long\nPapers, pages 66–75. Association for Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020.\n{ALBERT}: A Lite {BERT}for Self-supervised Learning of Language Representations. In International\nConference on Learning Representations.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR,\nabs/1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In 7th International Conference\non Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettle-\nmoyer. 2018. Deep Contextualized Word Representations. In Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 2227–2237, New Orleans, Louisiana. Association for Computational Linguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100, 000+ questions for\nmachine comprehension of text. In Jian Su, Xavier Carreras, and Kevin Duh, editors, Proceedings of the\n2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA,\nNovember 1-4, 2016, pages 2383–2392. The Association for Computational Linguistics.\nCunxiang Wang, Shuailong Liang, Yue Zhang, Xiaonan Li, and Tian Gao. 2019. Does it make sense? and why? A\npilot study for sense making and explanation. In Anna Korhonen, David R. Traum, and Llu´ıs M`arquez, editors,\nProceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy,\nJuly 28- August 2, 2019, Volume 1: Long Papers, pages 4020–4026. Association for Computational Linguistics.\nCunxiang Wang, Shuailong Liang, Yili Jin, Yilong Wang, Xiaodan Zhu, and Yue Zhang. 2020. SemEval-2020 task\n4: Commonsense validation and explanation. In Proceedings of The 14th International Workshop on Semantic\nEvaluation. Association for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, R’emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface’s transformers: State-of-the-\nart natural language processing. ArXiv, abs/1910.03771.\nYige Xu, Xipeng Qiu, Ligao Zhou, and Xuanjing Huang. 2020. Improving BERT Fine-Tuning via Self-Ensemble\nand Self-Distillation. arXiv:2002.10345 [cs], February. arXiv: 2002.10345.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019. XLNet:\nGeneralized Autoregressive Pretraining for Language Understanding. In Advances in Neural Information Pro-\ncessing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14\nDecember 2019, Vancouver, BC, Canada, pages 5754–5764.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8526227474212646
    },
    {
      "name": "SemEval",
      "score": 0.84511399269104
    },
    {
      "name": "Task (project management)",
      "score": 0.7880129218101501
    },
    {
      "name": "Sentence",
      "score": 0.6911141276359558
    },
    {
      "name": "Natural language processing",
      "score": 0.6784534454345703
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6462541818618774
    },
    {
      "name": "Commonsense reasoning",
      "score": 0.5729808807373047
    },
    {
      "name": "Language model",
      "score": 0.5556735396385193
    },
    {
      "name": "Commonsense knowledge",
      "score": 0.46533432602882385
    },
    {
      "name": "Transfer of learning",
      "score": 0.43511563539505005
    },
    {
      "name": "Mechanism (biology)",
      "score": 0.41950443387031555
    },
    {
      "name": "Knowledge-based systems",
      "score": 0.0862906277179718
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I12912129",
      "name": "Northeastern University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210152629",
      "name": "Eastern University",
      "country": "BD"
    }
  ],
  "cited_by": 4
}