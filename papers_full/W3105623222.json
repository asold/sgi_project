{
  "title": "A Time-Aware Transformer Based Model for Suicide Ideation Detection on Social Media",
  "url": "https://openalex.org/W3105623222",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2809845840",
      "name": "Ramit Sawhney",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2162085460",
      "name": "Harshit Joshi",
      "affiliations": [
        "University of Delhi"
      ]
    },
    {
      "id": "https://openalex.org/A2115674140",
      "name": "Saumya Gandhi",
      "affiliations": [
        "Visvesvaraya National Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2117028084",
      "name": "Rajiv Ratn Shah",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2739805805",
    "https://openalex.org/W2899402383",
    "https://openalex.org/W2104196611",
    "https://openalex.org/W943435019",
    "https://openalex.org/W2883944442",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W4238249433",
    "https://openalex.org/W2807452501",
    "https://openalex.org/W2790322417",
    "https://openalex.org/W2140556657",
    "https://openalex.org/W2402700",
    "https://openalex.org/W2557283755",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2965265935",
    "https://openalex.org/W1982746127",
    "https://openalex.org/W2890787190",
    "https://openalex.org/W4248237123",
    "https://openalex.org/W2911378332",
    "https://openalex.org/W2584959989",
    "https://openalex.org/W1940549951",
    "https://openalex.org/W3025747240",
    "https://openalex.org/W1572063013",
    "https://openalex.org/W2981984641",
    "https://openalex.org/W2963691377",
    "https://openalex.org/W2886741239",
    "https://openalex.org/W2150190681",
    "https://openalex.org/W2014402555",
    "https://openalex.org/W2225017599",
    "https://openalex.org/W2963777125",
    "https://openalex.org/W2045123881",
    "https://openalex.org/W2548906386",
    "https://openalex.org/W2805537619",
    "https://openalex.org/W201361503",
    "https://openalex.org/W2026096417",
    "https://openalex.org/W2889391310",
    "https://openalex.org/W2489406233",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W2053479646",
    "https://openalex.org/W2116498660",
    "https://openalex.org/W3035509916",
    "https://openalex.org/W2077497658",
    "https://openalex.org/W2998535576",
    "https://openalex.org/W2955204109",
    "https://openalex.org/W2511234952",
    "https://openalex.org/W3088524227",
    "https://openalex.org/W4301029487",
    "https://openalex.org/W2908543495",
    "https://openalex.org/W2405042511",
    "https://openalex.org/W3013437827",
    "https://openalex.org/W2906585657",
    "https://openalex.org/W2742491462",
    "https://openalex.org/W1604265591",
    "https://openalex.org/W2153923143",
    "https://openalex.org/W2034824117",
    "https://openalex.org/W2343924930",
    "https://openalex.org/W2793785670",
    "https://openalex.org/W1556840590",
    "https://openalex.org/W4229624464",
    "https://openalex.org/W3012770446",
    "https://openalex.org/W2083944372",
    "https://openalex.org/W4299598905",
    "https://openalex.org/W2900152803",
    "https://openalex.org/W1999908587",
    "https://openalex.org/W2150142541",
    "https://openalex.org/W2407086192",
    "https://openalex.org/W1965659825",
    "https://openalex.org/W2753446868",
    "https://openalex.org/W2250553926",
    "https://openalex.org/W1963495448",
    "https://openalex.org/W2912350748",
    "https://openalex.org/W2103859668",
    "https://openalex.org/W2965574147",
    "https://openalex.org/W2970205254",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2741447225",
    "https://openalex.org/W2987607344",
    "https://openalex.org/W2066064791",
    "https://openalex.org/W2978425953",
    "https://openalex.org/W3016260488",
    "https://openalex.org/W2524352516",
    "https://openalex.org/W4322588869",
    "https://openalex.org/W1888477366",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W312447532",
    "https://openalex.org/W2741004402",
    "https://openalex.org/W3089014067",
    "https://openalex.org/W2943063012",
    "https://openalex.org/W2154401220",
    "https://openalex.org/W2003834798"
  ],
  "abstract": "Social mediaâ€™s ubiquity fosters a space for users to exhibit suicidal thoughts outside of traditional clinical settings. Understanding the build-up of such ideation is critical for the identification of at-risk users and suicide prevention. Suicide ideation is often linked to a history of mental depression. The emotional spectrum of a userâ€™s historical activity on social media can be indicative of their mental state over time. In this work, we focus on identifying suicidal intent in English tweets by augmenting linguistic models with historical context. We propose STATENet, a time-aware transformer based model for preliminary screening of suicidal risk on social media. STATENet outperforms competitive methods, demonstrating the utility of emotional and temporal contextual cues for suicide risk assessment. We discuss the empirical, qualitative, practical, and ethical aspects of STATENet for suicide ideation detection.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 7685â€“7697,\nNovember 16â€“20, 2020.câƒ2020 Association for Computational Linguistics\n7685\nA Time-Aware Transformer Based Model for\nSuicide Ideation Detection on Social Media\nRamit Sawhney\nNetaji Subhas Institute of Technology\nramits.co@nsit.net.in\nHarshit Joshi\nUniversity of Delhi\nharshit113@ducic.ac.in\nSaumya Gandhi\nVisvesvaraya National Institute of Technology\ngandhisaumya8@gmail.com\nRajiv Ratn Shah\nIIIT Delhi\nrajivratn@iiitd.ac.in\nAbstract\nSocial mediaâ€™s ubiquity fosters a space for\nusers to exhibit suicidal thoughts outside of tra-\nditional clinical settings. Understanding the\nbuild-up of such ideation is critical for the\nidentiï¬cation of at-risk users and suicide pre-\nvention. Suicide ideation is often linked to a\nhistory of mental depression. The emotional\nspectrum of a userâ€™s historical activity on so-\ncial media can be indicative of their mental\nstate over time. In this work, we focus on\nidentifying suicidal intent in English tweets\nby augmenting linguistic models with histori-\ncal context. We propose STATENet, a time-\naware transformer based model for prelimi-\nnary screening of suicidal risk on social media.\nSTATENet outperforms competitive methods,\ndemonstrating the utility of emotional and tem-\nporal contextual cues for suicide risk assess-\nment. We discuss the empirical, qualitative,\npractical, and ethical aspects of STATENet for\nsuicide ideation detection.1\n1 Introduction\nGlobally, close to 800,000 people die by suicide\neach year, and 20 times more people attempt sui-\ncide. Suicide is the second leading cause of death\nin the 15 to 29 year age group (WHO, 2014)\nwith a rising suicide rate of 35% in the US since\n1999 (Hedegaard et al., 2020). Extending clinical\nand psychological care to people showing suicidal\nideation relies heavily on identifying those at risk.\nTragically, 80% of patients do not undergo psychi-\natric treatment, and about 60% of those who died of\nsuicide denied having suicidal thoughts to mental\nhealth practitioners (McHugh et al., 2019). Recent\nstudies (Coppersmith et al., 2018) also show that\npeople exhibiting suicidal ideation make frequent\nuse of social media, e.g., Twitter, to share their\n1https://github.com/midas-research/\nSTATENet_Time_Aware_Suicide_Assessment\n           josh is so cute\n           Hiiii belated merry christmas and advanced happy new year buddy\n           My friends tell me its difficult to trust me since I keep joking all the time\n           I do not want to be alive, I just want to die today. Please kill me already.\nIf someone does that I will be happy\n27/08/2011\n28/08/2011\n08/09/2011\n12/10/2014\n12/10/2014\n~ 3 years\nTweet to assess\nA userâ€™s tweeting historyTweet #1\nTweet #2\nTweet #3\nTweet #4\nNo suicidal or depressive tendencies\nConcerning, clear suicidal intent\nFigure 1: We study a user whose latest tweet is not in-\ndicative of suicidal intent. Without seeing the userâ€™s re-\ncent historic tweet, which shows self-harm tendencies,\nit is difï¬cult to accurately assess suicidal risk. How-\never, analyzing a userâ€™s tweeting history sequentially\nwithout factoring in time irregularities between tweets\nmay lead to an inaccurate representation of a userâ€™s\nmental state. Time-aware modeling of the temporal de-\npendency between historic tweets reduces the impact\nof tweets from 3 years ago, providing a more realistic\nrisk assessment.All examples in this paper have been\nparaphrased for user privacy (Chancellor et al., 2019).\nmental state, with eight out of ten disclosing their\nsuicidal thoughts and plans (Golden et al., 2009).\nWhile recent advances in computational social\nscience (Coppersmith et al., 2018; Ji et al., 2019)\nhave made progress in assessing suicidal risk on so-\ncial media, analyzing the linguistic traits of tweets\nis often not sufï¬cient for accurate suicidal intent\ndetection. Additional user-level contexts such as\ntweeting history can be instrumental in identify-\ning a build-up of negative emotions that are of-\nten linked to suicide ideation (Oliffe et al., 2012;\nRobins et al., 1959). Such a build-up can occur\nweeks, months, or even years before the onset of\nsuicidal ideation (Overholser, 2003) and suicidal\nactivity can also be inï¬‚uenced by past ideation\nor suicide attempts (Van Heeringen and MaruË‡sic,\n7686\n2003). Analyzing the user history and emotion\nspectrum, as shown in Figure1 can provide crucial\ncontext to estimate suicidal risk in a tweet authored\nby that user. Such anEmotional Historic Context\n(EHC)of a user over time can be characteristic of\ntheir mental health (Coppersmith et al., 2014).\nModeling temporal user context, either as a bag-\nof-tweets (Gaur et al., 2019), or sequentially (Cao\net al., 2019; Matero et al., 2019) helps in identi-\nfying suicidal intent. However, in Figure1, we\nshow that the impact of varying time intervals be-\ntween tweets is crucial for an accurate assessment.\nIt is critical to model the large gap between the\nuserâ€™s recent tweets that are collectively indicative\nof suicidal intent and those three years apart. Such\nunevenTemporal Tweeting Irregularities (TTI)\nranging from seconds to years (Wojcik and Hughes,\n2019) between successive tweets inï¬‚uence the as-\nsessment of a userâ€™s tweet differently. Sequential\nmodels such as Long Short Term Memory (LSTMs)\nnetworks assume that posting intervals are uniform,\nhindering the learning ability of a userâ€™s emotion\nspectrum over varying time intervals.\nContributions: Taking into account a userâ€™s\nemotional historic context and temporal tweeting\nirregularities, we propose STATENet:Suicidality\nassessmentTime-Aware TEmporalNetwork, a\nneural framework that evaluates the presence of\nsuicidal intent on social media(Sec.3.1). Building\non transfer learningâ€™s success in Natural Language\nProcessing, STATENet uses a dual transformer-\nbased architecture to learn the linguistic and emo-\ntional cues in tweets. STATENet jointly learns from\nthe language of the tweet(Sec.3.2) to be assessed,\nand the historic Plutchik-based (Plutchik, 1980)\nemotional spectrum of a user in a time-sensitive\nmanner(Sec.3.3). Through a series of experiments\n(Sec.4) on real-world data(Sec.4.1), we show that\nSTATENet signiï¬cantly outperforms competitive\nmethods(Sec. 5), with the F1 Score of 80%. We\ndemonstrate practical applicability through a qual-\nitative analysis(Sec.5.4), and discuss the ethical\nimplications of this study(Sec.6).\nAt a minimum, we establish validity for time-\naware emotional temporal context for identifying\nsuicide ideation on social media. We focus on the\nintersection of NLP and suicidal risk assessment\nby taking a step towards improving risk assess-\nment in anon-intrusive manner. Our work could\nbe considered as a preliminary screening tool that\noptimistically forms a component in a larger in-\nfrastructure involving psychologists, health care\nproviders, and social media enterprises.2 In prac-\ntice, STATENet would ï¬‚ag tweets asâ€œat-riskâ€for\nsuicidality as part of a human-in-the-loop system\nto support decisions about potential intervention.\n2 Related Work\nTraditional Methods:Researchers have devel-\noped various psychoclinical methods to measure\nsuicidal risk (Pestian et al., 2016), such as the Sui-\ncide Probability Scale (Bagge and Osman, 1998),\nDepression Anxiety Stress Scales-21 (Crawford\nand Henry, 2003), Adult Suicide Ideation Ques-\ntionnaire (wa Fu et al., 2007), Suicidal Affect-\nBehavior-Cognition Scale (Harris et al., 2015),\netc. While these methods are professional and ef-\nfective, they require participants to either answer\nquestionnaires (Venek et al., 2017) or engage in\ninterviews (Scherer et al., 2013), hence not reach-\ning suicidal people who are either unable to access\nthese resources or have a low motivation to seek\nprofessional help (Zachrisson et al., 2006; Essau,\n2005). Studies suggest that taking a suicide assess-\nment can negatively impact individuals showing\ndepressive symptoms (Harris and Goh, 2016).\nNLP Methods:In recent years, social media has\nshown promise in providing insights into the psy-\nchological state of individuals (Paul and Dredze,\n2011). Jashinsky et al.(2014) reported that Twitter\nis a viable tool for real-time monitoring (Braith-\nwaite et al., 2016) of suicide risk. Early efforts in\nutilizing social media include the use of user fea-\ntures (Masuda et al., 2013) and online suicide notes\n(Pestian et al., 2010; Huang et al., 2007). Since\nthen, the focus has been on using psycholinguis-\ntic lexicons such as LIWC (De Choudhury et al.,\n2016; Sawhney et al., 2018b) and textual features\nsuch as POS, tense, etc. for classiï¬cation (Ji et al.,\n2018; Huang et al., 2014). Shared tasks such as\nCLPsych (Zirikly et al., 2019) and CLEF eRISK\n(Losada et al., 2019) have seen a rise in the use\nof deep learning for suicidality prediction. CNN\nbased architectures (Du et al., 2018; Sawhney et al.,\n2018a; Shing et al., 2018; Naderi et al., 2019) and\nLSTM based architectures (Ji et al., 2018; Tadesse\net al., 2020) utilize pre-trained word embeddings\nto predict suicide risk. Although these text-based\nmethods capture the semantic nature of posts in\nisolation, no user associated context is provided\n2Similar to the type of algorithmic model deployed for\npost level screening on Facebook (Card, 2018).\n7687\nthat can give insight into the userâ€™s mental state\nto improve predictive power (Venek et al., 2017).\nA user-dependent, personalized context can truly\nprocess the â€œnaturalâ€ language of a user and under-\nstand the semantic context from the perspective of\nthat speciï¬c user (Flek, 2020). User context may\ninclude the userâ€™s emotion spectrum (Ren et al.,\n2016), social graph methods (Mishra et al., 2019)\nand temporal context (Mathur et al., 2020). Sui-\ncide risk assessment for preliminary screening has\nbeen done at both binary (suicidal intent present,\nsuicidal intent absent) (Cao et al., 2019; De Choud-\nhury et al., 2016; Mathur et al., 2020; Losada et al.,\n2019), and multiple (Zirikly et al., 2019; Vioules\net al., 2018; Gaur et al., 2019) levels of risk ranging\nfrom no risk to severe risk.\nContextual Methods: The best performing\nmodel, the dual context BERT (Matero et al., 2019),\nat the CLPsych 2019 shared task (Zirikly et al.,\n2019) for suicidal estimation on Reddit exempliï¬es\nthe utility of temporal context. The Dual Context\nBERT utilizes post level BERT embeddings passed\nsequentially through an attention-based RNN. Sim-\nilarly, Cao et al.(2019) employ a LSTM and\nfastText-based architecture for modeling temporal\ncontext. These RNN and LSTM based approaches\nassume that usersâ€™ historical posts are equally\nspaced in time, hindering the suicide ideation de-\ntection modelâ€™s ability to learn their relative im-\nportance in a time-aware manner. Time-aware se-\nquential models have shown improvements in other\nclinical tasks (Baytas et al., 2017), such as patient\nsubtyping, and in other domains like user activ-\nity modeling (Zhu et al.). More recently,Mathur\net al.(2020) and Sinha et al.(2019) have mod-\neled a userâ€™s historic emotion spectrum using latent\nrepresentations of GloVe embeddings of historic\ntweets. These latent features are then aggregated\nbased on speciï¬c functions such as exponential de-\ncay and sinusoids as opposed to learning them as\nsequences. These approaches assume that suicidal\nideation conforms to speciï¬c trajectories, which\nmay not generalize well across users (Giletta et al.,\n2015) and lose the context of individual historic\ntweets by aggregating them. Approaches besides\ndeep learning have also been explored, such as the\nwork done byVioules et al.(2018), which uses\nthe martingale framework (Ho, 2005) with senti-\nment scores and tweet level features such as likes\nto study two users on Twitter.\n \n \n \n \n \nğ’‰ğ‘´âˆ’ğŸğ’Š  \n ğ’‰ğ‘´ğ’Š \nğ’‰ğ‘´âˆ’ğŸğ’Š  \n \n  \n \nğš«ğ‘´âˆ’ğŸ \nğ»à·©ğ‘€ \n \nğ’˜ğŸ \n ğ’˜ğ‘ª \nâ‹¯ \nğ’•ğ’Š \n ğš«ğ‘´ \nğš«ğ‘´âˆ’ğŸ \nğ»à·©1 \nâ‹® \n \n \nğ‘¦à·¤ğ‘– \nğ‘¦à·œğ‘– \nğ‘¦ğ‘– \nğ‘»ğ’Šâ€² \nğ‘»ğ’Š \nğ‘¬ğŸğ’Šáˆ¬áˆ¬áˆ¬áˆ¬Ô¦ \n ğ‘¬ğŸğ’Šáˆ¬áˆ¬áˆ¬áˆ¬Ô¦ \n ğ‘¬ğ‘´ğ’Šáˆ¬áˆ¬áˆ¬áˆ¬áˆ¬áˆ¬Ô¦ \nâ‹® \nFigure 2: STATENet: Model Architecture\n3 Methodology\n3.1 Notations and Problem Formulation\nWe acknowledge that modeling suicidal intent\nas a binary classiï¬cation task is a strong sim-\npliï¬cation and in this work, we focus on identi-\nfying the presence of suicide ideation within a\ntweet using a user-level temporal context. We\ndenote a tweet to be assessed for suicidal risk\nas ti 2 T = {t1,t2,Â·Â·Â· ,tN} authored by a\nuser uj 2 U = {u1,u2,Â·Â·Â· ,uM} made at time\nâŒ§icurr. Each tweetti is associated with history\nHi,j =[ (hi1,âŒ§i1),(hi2,âŒ§i2),Â·Â·Â· ,(hi\nL,âŒ§i\nL)] wherehi\nk\nis a historic tweet by the useruj posted at time\nâŒ§i\nk with âŒ§i1 < âŒ§i2 < Â·Â·Â· < âŒ§i\nL < âŒ§icurr.W e\nformulate the problem as a classiï¬cation task to\npredict a labelyi for the tweetti, where,yi 2\n{suicidal intent present, suicidal intent absent}.\n3.2 Encoding the Tweet to be Assessed\nStudies have shown that the linguistic styles of\nsocial media users can aid in understanding their\nmental state (De Choudhury et al., 2013) and that\ntheir suicidal behaviour is correlated with suicidal\ntweets (Sueki, 2015). Static word embeddings\nsuch as GloVe (Pennington et al., 2014) have been\nused to encode tweets for detecting suicide ideation\n(Sinha et al., 2019) in the past. However, recent\n7688\nstudies have shown that pre-trained transformer\nmodels yield more comprehensive representations\nof linguistic features in a tweet (Salminen et al.,\n2020). We found that SentenceBERT (Reimers and\nGurevych, 2019) empirically outperforms embed-\ndings used in previous works such as FastText (Cao\net al., 2019), ELMo (Mohammadi et al., 2019),\netc. We use the 768-dimensional encoding obtained\nfrom SentenceBERT.3 Formally,\nT0\ni = SentenceBERT(ti) (1)\nwhereT0i 2 R768 is linearly transformed using a\ndense layer toTi 2 Rd with dimensiond.\n3.3 User Historical Emotion Spectrum\nIndividual Historic Tweet Encoding:Ampliï¬-\ncation of emotional factors such as emotional re-\nactivity (Tarrier et al., 2007), intensity (Links\net al., 2008) and instability (Palmier-Claus et al.,\n2012) can increase suicide risk. Building on this,\nwe extract the emotion spectrum of each historic\ntweethi\nk. Although proï¬cient in semantic model-\ning of text, general text encoders fail to capture the\nï¬ne-grained emotions expressed in social media\nposts. To capture ï¬ne-grained emotions, we uti-\nlize Plutchikâ€™s wheel of emotions (Plutchik, 1980).\nThis taxonomy suggests three hierarchical sets of\neight emotions arranged as four pairs of opposing\ndualities. The primary set of emotions described by\nthe wheel are: Joy - Sadness, Surprise - Anticipa-\ntion, Anger - Fear, and Trust - Disgust. We obtain\nan encoding that models the emotional spectrum\nof a historical tweet, and thus that of a user at a\nhistoric time. Based on empirical comparisons and\nthe success of transfer learning in NLP, we ï¬ne-\ntune pre-trained BERT embeddings on the Emonet\ndataset (Abdul-Mageed and Ungar, 2017). The\ndataset consists of a total of 1,608,233 tweets la-\nbeled across 24 emotions as per Plutchikâ€™s wheel\nof emotions. The presence of the primary emotions\nin the dataset is skewed towards joy, sadness, and\nfear, with their representation being 20.57%,8.85%,\nand 6.13%, respectively, with other emotions hav-\ning fewer samples. These are labeled using distant\nsupervision using a total of 665 emotion hashtags.\nWe call this transformer the PlutchikTransformer.\nThis transformer tokenizes each historical post and\nadds the[CLS]token at the beginning of each post.\nWe use the ï¬nal hidden state corresponding to this\n3SentenceBERT computes the mean of output vectors for\nall tokens to derive a ï¬xed size sentence embedding.\n \n  \n ğˆ ğˆ ğ­ğšğ§ğ¡ğˆ ğ’ˆ(â‹…) \nğ­ğšğ§ğ¡X + + \nX X \nğ­ğšğ§ğ¡\nÎ”ğ‘˜ ğ¡ğ’Œâˆ’ğŸğ¢  hğ‘˜i ğš«ğ’Œ+ğŸ ğ¡ğ’Œ+ğŸğ¢  \nğ‘¯Ìƒğ’Œğ’Š ğ‘¯Ìƒğ’Œâˆ’ğŸğ’Š  \nğ‘ªğ’Œâˆ’ğŸ ğ‘ªğ’Œ \nğ’ğ’Œ\nğ’•ğ’Œâˆ’ğŸ ğ’•ğ’Œ ğ’•ğ’Œ+ğŸ \nX \nFigure 3: Architecture of a Time-aware LSTM cell.\nFigure is adapted fromBaytas et al.(2017).\n[CLS]token (768-dimensional encoding) as the\naggregate representation of the emotional spectrum.\nWe deï¬ne the emotion vector (Ei\nk 2 R768) of each\nhistoric tweethi\nk as:\nEi\nk = PlutchikTransformer(hi\nk) (2)\nModeling Historical Tweets Sequentially:The\nemotional historic context of tweets can be used\nto model progressive emotional states of the au-\nthor of those tweets (Abdul-Mageed and Ungar,\n2017; De Choudhury et al., 2013). This makes re-\ncurrent neural networks (RNN), and particularly\nLSTMs (Hochreiter and Schmidhuber, 1997), the\nmost natural methods for encoding and learning\nfrom a sequence of a userâ€™s historical tweets.\nHowever, the time interval between the posting of\nhistoric tweets can vary widely, from a few sec-\nonds to a few years (Wojcik and Hughes, 2019).\nSuch variations can be an important factor in an-\nalyzing the emotional states of a user over time\n(Sueki, 2015). LSTM cells assume the input to\nbe equally spaced sequences and thus are unable\nto model irregularities in posting times of histori-\ncal tweets. Using this relative time difference be-\ntween the userâ€™s historical tweets can progressively\nmodel the userâ€™s emotions more accurately over\ntime. Hence, we propose the use of a Time-aware\nLSTM (T-LSTM) (Baytas et al., 2017) where time\nlapse between successive tweets is fed to the T-\nLSTM cell, as shown in Figure3. The T-LSTM\ncell thus incorporates the actual time differences\nbetween tweets, along with each historical tweetâ€™s\nemotional contextEi\nk.\nT-LSTM applies time decay to the memory ac-\ncording to the elapsed time between successive\nelements and weights the short-term memory cell\nCS\nk . Intuitively, the greater the time elapsed be-\n7689\ntween two tweets, the less impact they should have\non each other. To achieve this, T-LSTM uses a\nmonotonically decreasing function of elapsed time,\nwhich transforms time into appropriate weights.\nTime lapses are incorporated in the T-LSTM as:\nCS\nk\u00001 = tanh(WdCk\u00001 + bd) (Short-term memory)\nË†CS\nk\u00001 = CS\nk\u00001 â‡¤ g(\u0000k) (Discounted short-term memory)\nCLT\nk\u00001 = Ck\u00001 + CS\nk\u00001 (Long-term memory)\nCâ‡¤\nk\u00001 = CLT\nk\u00001 + Ë†CS\nk\u00001 (Adjusted previous memory)\nwhereCk\u00001 andCk are previous and current cell\nmemories, and{Wd,bd} are network parameters.\n\u0000k is the elapsed time between historic tweetshk\u00001\nand hk, andg(Â· ) is a heuristic decaying function\nthat reduces the effect of short-term memory as\u0000k\nincreases. We selectg(\u0000k)=1 /\u0000k empirically\nand as suggested inBaytas et al.(2017). For each\nhistoric tweethi\nk, the T-LSTM cell modiï¬es LSTM\ngate operations to compute the current hidden state\n( ËœHi\nk 2 Rd) by feedingCâ‡¤\nk\u00001 instead ofCk\u00001.\n3.4 Joint Network Optimization\nTo identify the presence of suicidal intent in a tweet,\nSTATENet jointly learns from the language of the\ntweet to be assessed and the emotional historic\nspectrum in a time-aware manner. For this we ap-\nply the concatenation operation\u0000 to Ti and ËœHi\nk\nrespectively, followed by a dense layer with Recti-\nï¬ed Linear Unit (ReLU)( Hahnloser et al., 2000)\nto form a prediction vector. Finally, a softmax func-\ntion (Goodfellow et al., 2016) is used to output the\nprobabilities of suicidal intent present.\nËœyi = ReLU(Wy(Ti \u0000 ËœHi\nk)+ by)\nË†yi = softmax(Ëœyi) (3)\nwhere Ë†yi is the ï¬nal suicide risk assessment and\n{Wy,by} are network parameters.\nTweet indicating suicidal intent form a very\nsmall proportion of the data (Ji et al., 2019). To ad-\ndress this problem of class imbalance(in practice,\nthe imbalance is much greater in the real world),\nwe train STATENet using Class-Balanced loss pro-\nposed byCui et al.(2019) along with Focal Loss\n(Lin et al., 2017). This loss function applies a\nclass-wise re-weighting scheme by introducing a\nweighting factor that is inversely proportional to\nthe number of samples. The loss functionL is:\nL = CBfocal(Ë†yi,yi; \u0000,\u0000) (4)\nwhereCBfocal is class-balanced focal loss,Ë†yi is\nthe predicted label andyi is the label of the current\ntweet.\u0000 and\u0000 are hyperparameters.\n4 Experiments\n4.1 Dataset\nWe use the Twitter timeline data of users from the\ndataset introduced bySinha et al.(2019). Sinha\net al.(2019) began with a collection of Twitter posts\nbased on a lexicon of 143 suicidal phrases. After\nmanual inspection of the dataset for trivially non-\nsuicidal tweets, their ï¬nal dataset contained 34,306\ntweets. Some of these tweets were authored by the\nsame user; thus, the total number of unique users\nfor which tweets were to be classiï¬ed was 32,558.\nWe summarize the annotation instructions (Sawh-\nney et al., 2018b) that were followed by two an-\nnotators, both students of Clinical Psychology, for\nannotating the collected 34,306 tweets:\nâ€¢ Suicidal Intent (SI) Present: Posts where\nsuicide ideation or previous attempts are dis-\ncussed in a somber and non-ï¬‚ippant tone.\nâ€¢ Suicidal Intent (SI) Absent: Tweets with no\nevidence for risk of suicide, including song\nlyrics, condolence message, awareness, news.\nIt is important to note that this process produced\nsuicide risk labels at the level of individual tweets\nand not for individual user histories. An acceptable\ninter-annotator agreement was achieved with a Co-\nhenâ€™s Kappa score (Cantor, 1996) of 0.72, under the\nsupervision of a professional clinical psychologist.\nThe resulting dataset contains 3984 suicidal tweets.\nThe Twitter timeline was collected for each user.\nThese timelines span over ten years from 2009 to\n2019. The mean number of tweets in user history is\n748(max 3,200)with a standard deviation of 789\ntweets. We trim the user history to the 100 most\nrecent tweets for users with a large number of his-\ntorical tweets.4 The mean time difference between\ntwo consecutive tweets for a user is two days with a\nstandard deviation of almost 24 days between two\ntweets, indicative of large variations across users.\n4070 users were found to have no historical tweets.\nData Preprocessing:We deidentiï¬ed the dataset\nby performing named entity recognition and re-\nmoving any identiï¬able information such as email\n4This was done due to memory and computation con-\nstraints faced during the training of STATENet.\n7690\naddresses, URLs, and names. Next, we follow stan-\ndard procedures of converting the text to lowercase,\nremoving punctuation and accents, striping whites-\npaces, and removing stopwords. We split the tweets\nin the dataset on the basis of users such that there\nis no overlap between users in the train, validation,\nand test set. We perform a stratiï¬ed 70:10:20 split\nacross the three sets, such that the train, valida-\ntion, and test sets consist of 24014, 3431, and 6861\ntweets, respectively. Although there may be mul-\ntiple tweets to be assessed by the same user, their\nassociated history differs according to the tweetsâ€™\nposting timestamps. We ensure that for each tweet\nto be classiï¬ed, only the historical tweets having\ntimestamps older than that of the tweet to be as-\nsessed are used for historic modeling.\n4.2 Experimental Settings\nBaseline Methods:We evaluate STATENet us-\ning the macro F1 and recall forsuicidal intent\npresent(recalls), against two types of baseline\nmethods; tweet level (TL) and user-level (UL). UL\nbaselines were adapted for tweet level assessment\nby concatenating embeddings of the tweet to be\nassessed with the user level features.\nRandom Forest + Tweet features(Sawhney\net al., 2018b): A non contextual TL approach\nthat applies Random Forests (RF) with tweet level\nfeatures including statistical, LIWC (Pennebaker\net al., 2001) features, n-grams and POS counts.\nC-LSTM (Sawhney et al., 2018a): We replicate\nthe TL deep Neural Network that uses CNN to cap-\nture local features and LSTMs for tweet encoding.\nSuicide Detection Model (SDM)(Cao et al.,\n2019): UL model that encodes tweets using ï¬ne-\ntuned FastText embeddings. Historic tweets were\npassed sequentially through LSTM + attention and\nconcatenated with the tweet to be assessed.\nContextual CNN (Gaur et al., 2019): Non-\nsequential UL model using GloVe embeddings for\nencoding tweets. Bag of tweets were concatenated\nand fed to a contextual CNN (Shin et al., 2018).\nExponential Decay(Sinha et al., 2019): TL\nmodel that weighs GloVe embeddings of historic\ntweets through an exponential decay function and\nensembles it with the GloVe embedding trained on\na BiLSTM + Attention for the tweet to be assessed.\nSurprise and Episodic Modeling(Mathur et al.,\n2020): Decision level ensemble TL model similar\nto Exponential Decay, but factors in sinusoidal and\nwhite Gaussian noise for historic tweet modeling.\nDualContextBert(Matero et al., 2019): Best per-\nforming UL model at CLPsych 2019. DualCon-\ntextBert uses BERT for encoding Reddit posts fed\nto an attention-based RNN layer. In our implemen-\ntation, we use all the userâ€™s historic tweets.\nExperimental Setup:We select hyperparame-\nters based on the highest Macro F1 obtained on the\nvalidation set for all models. We use grid search to\nexplore: number of features in hidden stateËœHD 2\n{8,64,128,256,512}, number of LSTM layers\nn 2 {1,2,5}, dropout\u0000 2 {0.0,0.1,Â·Â·Â· ,0.5},\n\u0000 2 {0.99,0.999,0.9999} and\u0000 2 {1.0,1.5,2.0}\nin class-balanced focal loss, initial learning rate\nIlr 2 {0.01,0.001,0.0005,0.0001}, warm-up\nstepsSws 2 {3,5,7}. The optimal hyperparam-\neters were found to be:ËœHD = 512, n =1 ,\n\u0000 =0 .5, \u0000 =0 .9999, \u0000 =2 .0, Ilr =0 .0001,\nSws =5 . We implement all methods with Py-\nTorch 1.5 (Paszke et al., 2019) and optimize using\nmini-batch AdamW with a batch size of 256 and\nIlr =0 .0001. We use the cosine scheduler with\na warmup step of 5 (Gotmare et al., 2018). We\ntrain the model for 20 epochs and apply early stop-\nping with a patience of 5 epochs. The model takes\n4,361s to train on an Nvidia Tesla K80 GPU.\n5 Results and Analysis\n5.1 Comparative Performance\nWe note from Table1 that STATENet signiï¬cantly\n(p< 0.005) outperforms competitive baselines.\nWe compare against both text only, and temporal\ncontextual models for suicidal risk assessment.\nSTATENet and other contextual models perform\nbetter than the non-contextual RF + tweet features\nand C-LSTM models. We believe this is because\ntemporal contextual models offer greater insight\ninto the authorâ€™s historical mental state, thereby\nincreasing predictive power. STATENet and se-\nquential models outperform the Contextual CNN,\nlikely due to their ability to better learn represen-\ntations from the temporal dependence in historical\ntweets, as opposed to Contextual CNNâ€™s bag of\ntweets approach. We also observe that STATENet\nsigniï¬cantly outperforms competitive sequential\nmodels. We postulate this to the ability of the time-\naware LSTM in STATENet to capture irregularities\nin tweeting intervals of users. Such time-aware\nmodeling likely learns more accurate latent rep-\nresentations of usersâ€™ emotional historic context.\nWhile exponential decay and episodic modeling\n7691\nType of Contextual Modeling Model Macro F1 \" Recalls \" Accuracy\"\nNone Random Forest + Tweet features 0.536 0.513 0.548\nC-LSTM 0.588 0.597 0.602\nNon Sequential Contextual CNN 0.729 0.587 0.803\nSequential Suicide Detection Model (SDM) 0.743 0.755 0.819\nDualContextBert 0.767 0.786 0.823\nSpeciï¬c Temporal Functions Exponential Decay 0.737 0.759 0.828\nSurprise and Episodic Modeling 0.741 0.762 0.831\nTimeaware Sequential STATENet 0.799 * 0.810* 0.851*\nTable 1: Mean of results obtained over 10 different runs. * indicates that the result is signiï¬cantly better than\nDualContextBert (p< 0.005) under Wilcoxonâ€™s Signed Rank test).Bolddenotes best performance.\nModel Component Macro F1 \" Recalls\"\nCurrent tweet only 0.731 0.551\nCurrent + Random History(Plutchik) 0.730 0.608*\nCurrent + Sequential History(BERT) 0.767* 0.786*\nCurrent + Sequential History(Plutchik) 0.778* 0.795*\nCurrent + TA History(Plutchik) 0.799* 0.810*\nTable 2: Ablation study over STATENet. We report\nthe mean of results obtained over ten different runs. *\nshows signiï¬cant compared to the current tweet (p<\n0.005) under Wilcoxonâ€™s Signed Rank test. Current:\nencoding of the tweet to be assessed. History: encoding\nof historical tweets. TA: Time-Aware.Bolddenotes the\nbest performance.\nperform well, we note that STATENet does bet-\nter, in terms of all metrics, particularly recall for\nthe suicidal intent present class. We believe this\nis because not every userâ€™s emotional historic con-\ntext may conform to ï¬xed trajectories that these\napproaches aggregate historic tweets on.\n5.2 Ablation Study\nTo assess EHC and TTI, we perform an ablation\nstudy (Table2) with different conï¬gurations. With-\nout considering historic tweets, the performance\nof the model drops drastically. We believe that\nadding historic tweets, even in a random order,\nadds additional contextual cues about the user, re-\nsulting in improved performance. We observe that\nthe PlutchikTransformervariant of Current + Se-\nquential History outperforms itsBERT counter-\npart. This can be attributed to the ability of the\nPlutchikTransformerto capture the EHC of a user.\nSTATENet jointly models the Current Tweet and\nEHC in a time-aware manner, overcoming the limi-\ntation of previous models that assume equal time\nintervals between posts. On inspecting the results\nfor the 647 users without any historic tweets, we\nï¬nd that STATENet performs well with a recall\nof 0.74 and macro F1 of 0.75. This reiterates the\nNon\nSequential\nSequential STATE\nNet\n75\n80 p=1.27e-6\np=5.51e-9\np=4.66e-7\nMacro F1\nNon\nSequential\nSequential STATE\nNet\n60\n70\n80\np=3.79e-5\np=5.55e-6\np=1.58e-7\nRecall for suicidal\nFigure 4: Conï¬dence intervals for evaluation metrics of\ntemporal variants over 10 different runs and data splits.\nability of linguistic only non-contextual models in\nsuicidal intent identiï¬cation. This is particularly\ninteresting, as, for users with no available history,\nassessment can still be performed to some degree.\n5.3 Temporal Analysis\nThe tweetâ€™s language should be studied with his-\ntorical context to better understand the userâ€™s emo-\ntional state over time, based on the EHC. To an-\nalyze the importance of the order and temporal\ndependency of historic tweets, we ï¬rst try a non-\nsequential, bag of tweets like variant. We feed the\nPlutchik transformer-based encodings to a Contex-\ntual CNN. We observe that the bag of tweets ap-\nproach is slightly better than the Contextual CNN\nbaseline, likely because of the transformer-based\nencoding as opposed to static GloVe embeddings\nused by the baseline. The non-sequential approach\ndrastically underperforms over ten runs in com-\nparison to temporal variants. Further investigat-\ning EHC and TTI, we ï¬rst feed historic tweets in\nsequential order (Sequential Model) to a regular\nLSTM, and then we factor in TTI through T-LSTM\nin STATENet. Figure4 shows that STATENet is\n7692\nUser 1 User 2 User 3\nti [14/07/2017]: i dont want\nto be here anymore again\n(SI Present)\n[08/03/2016]: been a year\nsince i lost the most\nimportant woman the loss\nhas never sunk (SI Absent)\n[05/01/2019]: Nobody be\nshocked when I snap and\ntake a life either my own\nor theirs (SI Present)\nhi\nk3 [13/07/2017]: yes i almost tried\nto kill myself again tonight yes it is\nonly been ten minutes and im now\nretweeting tweets\n[16/11/2015]: i wrote this a year\nago today and one year on i am box-\ning things up and moving into my\nown house it is crazy\n[29/12/2018] when you said your\nlast goodbye, i die a little bit inside,\ni lay in tears in bed all night, alone\nwithout you by my side\nhi\nk2 [27/05/2017]: i do not know if its\nseasonal depression or just me avoid-\ning christmas by staying in all day\n[19/11/2014]: I donâ€™t think i will\nsee the end of today, there is nothing\nleft for me to do\n[21/12/2018]: Do you collect any-\nthing. If so what is it? Memories\nhahhahahhaa\nhi\nk1 [02/06/2016]: i love my mother\nshe is great life is amazing\n[16/11/2014]: i deserve death,\ndear 16 old me it will never get better\n[16/12/2018]: I am alive and I am\nhappy about it dammit why even\nTable 3: Tweet to be assessed (ti) and historic tweets (hi\nk1, hi\nk2 andhi\nk3 are chronologically ordered) of three users\nalong with tweet timestamp information. We also show visualized self-attention (averaged over all 12 Sentence-\nBERT attention heads) per token. Darker intensity of the red color denotes higher attention weights.\nh1\n1 h1\n89 h1\n154\nAngerDisgustFearSadness\nJoyAnticipationSurpriseTrust\n0\n1\n(a) User 1\nh2\n10h2\n17 h2\n78\n(b) User 2\nh3\n1 h3\n11 h3\n27\n(c) User 3\nXSTATENet\nt1\nXSequential\nâ‡¥Current\nX\nt2\nâ‡¥\nâ‡¥\nâ‡¥\nt3\nâ‡¥\nâ‡¥\n(d) Model predictions for tweets\nFigure 5: (a), (b) and (c) are emotion intensity across 8 primary emotions based on the Plutchik Wheel for User\n1, 2, 3 over time respectively, from White to Blue.hi\nk representskth historic tweet associated with the current\ntweetti. In (d) Green and Red represent correct and incorrect assessment of suicidal risk respectively for different\ntweets. We display only the 8 primary emotions of the Plutchik wheel for brevity.\nsigniï¬cantly (p< 0.005)better than the sequen-\ntial but non-time-aware variant, and shows the least\nvariation in performance over 10 different runs. For\nthe difference in performance between the Sequen-\ntial Model and STATENet, we believe that is due\nto the temporal dependency of historic tweets on\nthe elapsed time between successive tweets.\n5.4 Qualitative Analysis\nFor a detailed insight and aiding interpretabil-\nity, we analyze some cases where STATENet per-\nforms well. We also highlight the limitations of\nSTATENet through error analysis. We qualitatively\nanalyze three interesting cases in Table3 and Fig-\nure5. We see that the tweet to be assessed for User\n1 does not show any explicit suicidal intent and\nalone may not be sufï¬cient to assess suicidal risk.\nHowever, temporal models correctly classify the\ntweet as they learn the build-up of sadness in the\nhistoric tweets, which we observe from the Pluchtik\nemotional intensity in Figure5a.\nWhen the current tweet of the user is non-\nindicative, temporal models can get additional con-\ntext by learning historic activity of the user. Often,\ntemporal patterns are variable, and posting frequen-\ncies vary drastically. These TTI present challenges\nin only relying on the sequence of historic tweets\nrather than the actual time lapses. For instance, ini-\ntial tweets of User 2 showed sadness and suicidal\nintent, whereas the recent historic tweet (h2\nk3) of\nthe user represents joy (Figure5b). LSTM-based\nmodels aggregate sadness and hence assume the\nhistory to be suicidal. Contrarily, STATENet is able\nto learn from the variable time-lapses and their rel-\native importance in the context of suicide ideation.\nHowever, we found some cases where all models\nfailed. For User 3, the current tweet does not con-\ntain strong semantic indicators of suicidal intent.\nMoreover, historic tweets do not show any recog-\nnizable emotional pattern (Figure5c). Such a case\npresents the complexities associated with suicide\nrisk assessment.Another interesting observation\nfrom Figure5 is that the learned Plutchik emotion\n7693\nintensity distribution for users is skewed towards\njoy (positive) and sadness (negative). Although the\nhighly granular emotional context captured by the\nPlutchikTransformer improves STATENetâ€™s perfor-\nmance (Sec.5.2), over the more generic language\nfeatures captured by BERT. We leave further ex-\nploring the impact of emotion granularity to our\nfuture research directions.\n6 Discussion\nEthical Considerations:The preponderance of\nthe work presented in our discussion presents\nheightened ethical challenges. As explored inCop-\npersmith et al.(2018), we address the trade-off\nbetween privacy and effectiveness. While data is es-\nsential in making models like STATENet effective,\nwe must work within the purview of acceptable\nprivacy practices to avoid coercion and intrusive\ntreatment. To that end, we utilize publicly available\nTwitter data in a purely observational (Norval and\nHenderson, 2017; Broer, 2020), and non-intrusive\nmanner. Although informed consent of each user\nwas not sought as it may be deemed coercive, au-\ntomated de-identiï¬cation of the dataset was per-\nformed to reduce the risk of including any identi-\nfying data in the raw data. All tweets shown as\nexamples in Figure1 and Section5.4 have been\nparaphrased as per themoderate disguisescheme\nsuggested inBruckman(2002) to protect the pri-\nvacy of individuals (Fiesler and Proferes, 2018).\nThe annotation of user data has been kept separately\nfrom raw user data on protected servers linked only\nthrough anonymous IDs (Benton et al., 2017). As-\nsessments made by STATENet are sensitive and\nshould be shared selectively to avoid misuse, such\nas Samaritanâ€™s Radar (Hsin et al., 2016). Our work\ndoes not make any diagnostic claims related to sui-\ncide. We study the social media posts in a purely\nobservational capacity (Norval and Henderson,\n2017) and do not intervene with the user experi-\nence in any way.\nLimitations: We acknowledge that studying sui-\ncidality is subjective in nature (Keilp et al., 2012)\nand that the interpretation of the analysis presented\nmay vary across individuals. Due to the situat-\nedness of language, the studied data may be sus-\nceptible to demographic, annotator, and medium-\nspeciï¬c biases (Hovy and Spruit, 2016). We recog-\nnize that suicide risk exists on a diverse spectrum,\nand the simpliï¬cation of binary labels could lead to\nartiï¬cial notions of risk (Bryan and Rudd, 2006).\nPractical Implications:Through STATENet,\nwe suggest a neural architecture for preliminary\nscreening of at-risk users on social media to aid\nthe prioritization of clinical resources. Our work\nobserves Twitter in a non-intrusive manner and\ndoes not intervene with the user experience in any\nway. STATENet should form part of a distributed\nhuman-in-the-loop (de Andrade et al., 2018) sys-\ntem for ï¬ner interpretation of risk. Focusing on\nSTATENetâ€™s practical applicability, we work with\ntweet level annotations rather than the more sub-\njective and difï¬cult to scale user-level annotations.\nWe emphasize on tweet-level prediction; however,\nSTATENet can also be applied for user-level sui-\ncide risk assessment given its dual text and historic\nmodeling components.\n7 Conclusion\nMotivated by the rising use of social media for ex-\nhibiting suicide ideation as opposed to standard\nclinical practice (McHugh et al., 2019), we present\nSTATENet. Building on psychological studies on\nanalyzing a userâ€™s temporal emotional spectrum,\nSTATENet models the time aware emotional con-\ntext of users through historical tweets for more\naccurate suicide risk estimation on social media.\nWe plan to explore the impact of varying amounts\nof historical context for a user in our future work.\nWe show STATENetâ€™s applicability as a prelimi-\nnary tool in assessing suicidality in tweets. We\npresent a qualitative analysis for a deeper under-\nstanding of STATENet. Through this work, we aim\nto form a component in a larger human-in-the-loop\ninfrastructure for analyzing potentially concerning\nsuicide-related social media posts. Priority-based\nsuicide risk assessment for ranking tweets for sui-\ncidal risk, rather than classifying them forms our\nfuture direction. Additionally, in the future, we\nwould also want to quantify the impact of vary-\ning degrees of granularity of learning emotional\nfeatures from tweets on STATENetâ€™s performance.\nAcknowledgements\nWe would like to think Alex Polozov, Kawin Etha-\nyarajh, Sebastian Gehrmann, Siva Reddy, and the\nanonymous reviewers for their extremely helpful\nfeedback and comments.\n7694\nReferences\nMuhammad Abdul-Mageed and Lyle Ungar. 2017.\nEmoNet: Fine-grained emotion detection with gated\nrecurrent neural networks. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n718â€“728, Vancouver, Canada. Association for Com-\nputational Linguistics.\nNorberto Nuno Gomes de Andrade, Dave Pawson, Dan\nMuriello, Lizzy Donahue, and Jennifer Guadagno.\n2018. Ethics and artiï¬cial intelligence: Suicide pre-\nvention on facebook. Philosophy & Technology,\n31(4):669â€“684.\nCourtney Bagge and Augustine Osman. 1998.The sui-\ncide probability scale: Norms and factor structure.\nPsychological Reports, 83(2):637â€“638.\nInci M Baytas, Cao Xiao, Xi Zhang, Fei Wang, Anil K\nJain, and Jiayu Zhou. 2017. Patient subtyping via\ntime-aware lstm networks. InProceedings of the\n23rd ACM SIGKDD international conference on\nknowledge discovery and data mining, pages 65â€“74.\nAdrian Benton, Glen Coppersmith, and Mark Dredze.\n2017. Ethical research protocols for social media\nhealth research. In Proceedings of the First ACL\nWorkshop on Ethics in Natural Language Process-\ning, pages 94â€“102, Valencia, Spain. Association for\nComputational Linguistics.\nScott R Braithwaite, Christophe Giraud-Carrier, Josh\nWest, Michael D Barnes, and Carl Lee Hanson.\n2016. Validating machine learning algorithms for\ntwitter data against established measures of suicidal-\nity. JMIR Mental Health, 3(2):e21.\nTineke Broer. 2020.Technology for our future? explor-\ning the duty to report and processes of subjectiï¬ca-\ntion relating to digitalized suicide prevention. Infor-\nmation, 11(3):170.\nAmy Bruckman. 2002. Studying the amateur artist: A\nperspective on disguising data collected in human\nsubjects research on the internet.Ethics and Infor-\nmation Technology, 4(3):217â€“231.\nCraig J Bryan and M David Rudd. 2006. Advances in\nthe assessment of suicide risk.Journal of clinical\npsychology, 62(2):185â€“200.\nAlan B Cantor. 1996. Sample-size calculations for co-\nhenâ€™s kappa.Psychological Methods, 1(2):150.\nLei Cao, Huijun Zhang, Ling Feng, Zihan Wei, Xin\nWang, Ningyun Li, and Xiaohao He. 2019. La-\ntent suicide risk detection on microblog via suicide-\noriented word embeddings and layered attention. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 1718â€“\n1728.\nCatherine Card. 2018.How facebook ai helps suicide\nprevention. Facebook Newsroom.\nStevie Chancellor, Michael L. Birnbaum, Eric D.\nCaine, Vincent M. B. Silenzio, and Munmun\nDe Choudhury. 2019.A taxonomy of ethical ten-\nsions in inferring mental health states from social\nmedia. In Proceedings of the Conference on Fair-\nness, Accountability, and Transparency, FAT* â€™19,\npage 79â€“88, New York, NY , USA. Association for\nComputing Machinery.\nGlen Coppersmith, Mark Dredze, and Craig Harman.\n2014. Quantifying mental health signals in twitter.\nIn Proceedings of the workshop on computational\nlinguistics and clinical psychology: From linguistic\nsignal to clinical reality, pages 51â€“60.\nGlen Coppersmith, Ryan Leary, Patrick Crutchley, and\nAlex Fine. 2018.Natural language processing of so-\ncial media as screening for suicide risk. Biomedical\nInformatics Insights, 10:117822261879286.\nJohn R. Crawford and Julie D. Henry. 2003.The\ndepression anxiety stress scales (DASS): Norma-\ntive data and latent structure in a large non-clinical\nsample. British Journal of Clinical Psychology,\n42(2):111â€“131.\nYin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and\nSerge Belongie. 2019. Class-balanced loss based on\neffective number of samples. InProceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition, pages 9268â€“9277.\nMunmun De Choudhury, Michael Gamon, Scott\nCounts, and Eric Horvitz. 2013. Predicting depres-\nsion via social media. InSeventh international AAAI\nconference on weblogs and social media.\nMunmun De Choudhury, Emre Kiciman, Mark Dredze,\nGlen Coppersmith, and Mrinal Kumar. 2016. Dis-\ncovering shifts to suicidal ideation from mental\nhealth content in social media. InProceedings of\nthe 2016 CHI conference on human factors in com-\nputing systems, pages 2098â€“2110.\nJingcheng Du, Yaoyun Zhang, Jianhong Luo, Yuxi\nJia, Qiang Wei, Cui Tao, and Hua Xu. 2018.Ex-\ntracting psychiatric stressors for suicide from so-\ncial media using deep learning. BMC medical in-\nformatics and decision making, 18(Suppl 2):43â€“43.\n30066665[pmid].\nCecilia A. Essau. 2005.Frequency and patterns of\nmental health services utilization among adolescents\nwith anxiety and depressive disorders. Depression\nand Anxiety, 22(3):130â€“137.\nCasey Fiesler and Nicholas Proferes. 2018. â€œpartici-\npantâ€ perceptions of twitter research ethics.Social\nMedia+ Society, 4(1):2056305118763366.\nLucie Flek. 2020.Returning the N to NLP: Towards\ncontextually personalized classiï¬cation models. In\n7695\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 7828â€“\n7838, Online. Association for Computational Lin-\nguistics.\nKing wa Fu, Ka Y . Liu, and Paul S. F. Yip. 2007.Pre-\ndictive validity of the chinese version of the adult\nsuicidal ideation questionnaire: Psychometric prop-\nerties and its short version.Psychological Assess-\nment, 19(4):422â€“429.\nManas Gaur, Amanuel Alambo, Joy Prakash Sain,\nUgur Kursuncu, Krishnaprasad Thirunarayan, Ra-\nmakanth Kavuluru, Amit Sheth, Randy Welton, and\nJyotishman Pathak. 2019. Knowledge-aware assess-\nment of severity of suicide risk for early intervention.\nIn The World Wide Web Conference, pages 514â€“525.\nMatteo Giletta, Mitchell J Prinstein, John RZ Abela,\nBrandon E Gibb, Andrea L Barrocas, and Ben-\njamin L Hankin. 2015. Trajectories of suicide\nideation and nonsuicidal self-injury among adoles-\ncents in mainland china: Peer predictors, joint de-\nvelopment, and risk for suicide attempts.Journal of\nconsulting and clinical psychology, 83(2):265.\nRobert N Golden, Carla Weiland, and Fred Peterson.\n2009. The truth about illness and disease. Infobase\nPublishing.\nIan Goodfellow, Yoshua Bengio, and Aaron Courville.\n2016.Deep learning. MIT press.\nAkhilesh Gotmare, Nitish Shirish Keskar, Caiming\nXiong, and Richard Socher. 2018. A closer\nlook at deep learning heuristics: Learning rate\nrestarts, warmup and distillation.arXiv preprint\narXiv:1810.13243.\nRichard HR Hahnloser, Rahul Sarpeshkar, Misha A\nMahowald, Rodney J Douglas, and H Sebastian Se-\nung. 2000. Digital selection and analogue ampliï¬ca-\ntion coexist in a cortex-inspired silicon circuit.Na-\nture, 405(6789):947â€“951.\nKeith M. Harris and Melissa Ting-Ting Goh. 2016.Is\nsuicide assessment harmful to participants? ï¬ndings\nfrom a randomized controlled trial. International\nJournal of Mental Health Nursing, 26(2):181â€“190.\nKeith M. Harris, Jia-Jia Syu, Owen D. Lello,\nY . L. Eileen Chew, Christopher H. Willcox, and\nRoger H. M. Ho. 2015.The ABCâ€™s of suicide risk\nassessment: Applying a tripartite approach to indi-\nvidual evaluations. PLOS ONE, 10(6):e0127442.\nHolly Hedegaard, Sally C Curtin, and Margaret Warner.\n2020. Increase in suicide mortality in the united\nstates, 1999â€“2018.\nShen-Shyang Ho. 2005. A martingale framework\nfor concept change detection in time-varying data\nstreams. InProceedings of the 22nd international\nconference on Machine learning, pages 321â€“327.\nSepp Hochreiter and JÂ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural Comput.,\n9(8):1735â€“1780.\nDirk Hovy and Shannon L Spruit. 2016. The social\nimpact of natural language processing. InProceed-\nings of the 54th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 591â€“598.\nHonor Hsin, John Torous, and Laura Roberts. 2016.\nAn adjuvant role for mobile health in psychiatry.\nJAMA Psychiatry, 73(2):103.\nXiaolei Huang, Lei Zhang, Tianli Liu, David Chiu,\nTingshao Zhu, and Xin Li. 2014.Detecting suicidal\nideation in chinese microblogs with psychological\nlexicons. CoRR, abs/1411.0778.\nY . Huang, T. Goh, and C. L. Liew. 2007. Hunting\nsuicide notes in web 2.0 - preliminary ï¬ndings. In\nNinth IEEE International Symposium on Multimedia\nWorkshops (ISMW 2007), pages 517â€“521.\nJared Jashinsky, Scott H. Burton, Carl L. Hanson,\nJosh West, Christophe Giraud-Carrier, Michael D.\nBarnes, and Trenton Argyle. 2014.Tracking sui-\ncide risk factors through twitter in the US. Crisis,\n35(1):51â€“59.\nShaoxiong Ji, Shirui Pan, Xue Li, Erik Cambria,\nGuodong Long, and Zi Huang. 2019. Suicidal\nideation detection: A review of machine learn-\ning methods and applications. arXiv preprint\narXiv:1910.12611.\nShaoxiong Ji, Celina Ping Yu, Sai fu Fung, Shirui Pan,\nand Guodong Long. 2018.Supervised learning for\nsuicidal ideation detection in online user content.\nComplexity, 2018:1â€“10.\nJohn G. Keilp, Michael F. Grunebaum, Marianne Gor-\nlyn, Simone LeBlanc, Ainsley K. Burke, Hanga Gal-\nfalvy, Maria A. Oquendo, and J. John Mann. 2012.\nSuicidal ideation and the subjective aspects of de-\npression. Journal of Affective Disorders, 140(1):75â€“\n81.\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming\nHe, and Piotr DollÂ´ar. 2017. Focal loss for dense ob-\nject detection. InProceedings of the IEEE interna-\ntional conference on computer vision, pages 2980â€“\n2988.\nPaul S Links, Rahel Eynan, Marnin J Heisel, and\nRosane Nisenbaum. 2008.Elements of affective\ninstability associated with suicidal behaviour in pa-\ntients with borderline personality disorder. The\nCanadian Journal of Psychiatry, 53(2):112â€“116.\nDavid E Losada, Fabio Crestani, and Javier Parapar.\n2019. Overview of erisk at clef 2019 early risk pre-\ndiction on the internet (extended overview).\nNaoki Masuda, Issei Kurahashi, and Hiroko Onari.\n2013. Suicide ideation of individuals in online so-\ncial networks. PloS one, 8:e62262.\n7696\nMatthew Matero, Akash Idnani, Youngseo Son, Sal-\nvatore Giorgi, Huy Vu, Mohammad Zamani, Parth\nLimbachiya, Sharath Chandra Guntuku, and H An-\ndrew Schwartz. 2019. Suicide risk assessment with\nmulti-level dual-context language and bert. InPro-\nceedings of the Sixth Workshop on Computational\nLinguistics and Clinical Psychology, pages 39â€“44.\nPuneet Mathur, Ramit Sawhney, Shivang Chopra,\nMaitree Leekha, and Rajiv Ratn Shah. 2020.Uti-\nlizing temporal psycholinguistic cues for suicidal\nintent estimation. Advances in Information Re-\ntrieval: 42nd European Conference on IR Re-\nsearch, ECIR 2020, Lisbon, Portugal, April 14â€“\n17, 2020, Proceedings, Part II, 12036:265â€“271.\nPMC7148016[pmcid].\nCatherine M McHugh, Amy Corderoy, Christo-\npher James Ryan, Ian B Hickie, and\nMatthew Michael Large. 2019. Association\nbetween suicidal ideation and suicide: meta-\nanalyses of odds ratios, sensitivity, speciï¬city and\npositive predictive value.BJPsych open, 5(2).\nRohan Mishra, Pradyumn Prakhar Sinha, Ramit Sawh-\nney, Debanjan Mahata, Puneet Mathur, and Rajiv\nRatn Shah. 2019.SNAP-BATNET: Cascading au-\nthor proï¬ling and social network graphs for suicide\nideation detection on social media. In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nStudent Research Workshop, pages 147â€“156, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nElham Mohammadi, Hessam Amini, and Leila Kos-\nseim. 2019.CLaC at CLPsych 2019: Fusion of neu-\nral features and predicted class probabilities for sui-\ncide risk assessment based on online posts. In Pro-\nceedings of the Sixth Workshop on Computational\nLinguistics and Clinical Psychology, pages 34â€“38,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nNona Naderi, Douglas Teodoro, Emilie Pasche, and\nPatrick Ruch. 2019. A baseline approach for early\ndetection of signs of anorexia and self-harm in red-\ndit posts.\nChris Norval and Tristan Henderson. 2017.Contextual\nconsent: Ethical mining of social media for health\nresearch. CoRR, abs/1701.07765.\nJohn L Oliffe, John S Ogrodniczuk, Joan L Bottorff,\nJoy L Johnson, and Kristy Hoyak. 2012. â€œyou feel\nlike you canâ€™t live anymoreâ€: Suicide from the per-\nspectives of canadian men who experience depres-\nsion. Social science & medicine, 74(4):506â€“514.\nJames Overholser. 2003. Predisposing factors in sui-\ncide attempts: life stressors. InEvaluating and treat-\ning adolescent suicide attempters, pages 41â€“52. El-\nsevier.\nJ. E. Palmier-Claus, P. J. Taylor, F. Varese, and D. Pratt.\n2012.Does unstable mood increase risk of suicide?:\ntheory, research and practice. Journal of Affective\nDisorders, 143(1-3):5â€“15.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019.Pytorch:\nAn imperative style, high-performance deep learn-\ning library. InAdvances in Neural Information Pro-\ncessing Systems 32, pages 8024â€“8035. Curran Asso-\nciates, Inc.\nMichael J Paul and Mark Dredze. 2011. You are\nwhat you tweet: Analyzing twitter for public health.\nIn Fifth International AAAI Conference on Weblogs\nand Social Media.\nJames W Pennebaker, Martha E Francis, and Roger J\nBooth. 2001. Linguistic inquiry and word count:\nLiwc 2001. Mahway: Lawrence Erlbaum Asso-\nciates, 71(2001):2001.\nJeffrey Pennington, Richard Socher, and Christopher D.\nManning. 2014.Glove: Global vectors for word rep-\nresentation. In Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 1532â€“1543.\nJohn Pestian, Henry Nasrallah, Pawel Matykiewicz,\nAurora Bennett, and Antoon Leenaars. 2010.Sui-\ncide note classiï¬cation using natural language pro-\ncessing: A content analysis. Biomedical informatics\ninsights, 2010(3):19â€“28. 21643548[pmid].\nJohn P. Pestian, Michael Sorter, Brian Connolly,\nKevin Bretonnel Cohen, Cheryl McCullumsmith,\nJeffry T. Gee, Louis-Philippe Morency, Stefan\nScherer, and Lesley Rohlfs and. 2016.A machine\nlearning approach to identifying the thought markers\nof suicidal subjects: A prospective multicenter trial.\nSuicide and Life-Threatening Behavior, 47(1):112â€“\n121.\nRobert Plutchik. 1980. A general psychoevolutionary\ntheory of emotion. InTheories of emotion, pages\n3â€“33. Elsevier.\nNils Reimers and Iryna Gurevych. 2019.Sentence-\nbert: Sentence embeddings using siamese bert-\nnetworks. InProceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing.\nAssociation for Computational Linguistics.\nFuji Ren, Xin Kang, and Changqin Quan. 2016.Ex-\namining accumulated emotional traits in suicide\nblogs with an emotion topic model. IEEE Journal\nof Biomedical and Health Informatics, 20(5):1384â€“\n1396.\nEli Robins, George E Murphy, Robert H Wilkinson Jr,\nSeymour Gassner, and Jack Kayes. 1959. Some clin-\nical considerations in the prevention of suicide based\n7697\non a study of 134 successful suicides.American\nJournal of Public Health and the Nations Health,\n49(7):888â€“899.\nJoni Salminen, Maximilian Hopf, Shammur A.\nChowdhury, Soon-gyo Jung, Hind Almerekhi, and\nBernard J. Jansen. 2020. Developing an on-\nline hate classiï¬er for multiple social media plat-\nforms. Human-centric Computing and Information\nSciences, 10(1):1.\nRamit Sawhney, Prachi Manchanda, Puneet Mathur,\nRajiv Shah, and Raj Singh. 2018a.Exploring and\nlearning suicidal ideation connotations on social me-\ndia with deep learning. In Proceedings of the 9th\nWorkshop on Computational Approaches to Subjec-\ntivity, Sentiment and Social Media Analysis, pages\n167â€“175, Brussels, Belgium. Association for Com-\nputational Linguistics.\nRamit Sawhney, Prachi Manchanda, Raj Singh, and\nSwati Aggarwal. 2018b.A computational approach\nto feature extraction for identiï¬cation of suicidal\nideation in tweets. InProceedings of ACL 2018, Stu-\ndent Research Workshop, pages 91â€“98, Melbourne,\nAustralia. Association for Computational Linguis-\ntics.\nS. Scherer, J. Pestian, and L. Morency. 2013. Inves-\ntigating the speech characteristics of suicidal ado-\nlescents. In 2013 IEEE International Conference\non Acoustics, Speech and Signal Processing, pages\n709â€“713.\nJoongbo Shin, Yanghoon Kim, Seunghyun Yoon, and\nKyomin Jung. 2018. Contextual-cnn: A novel ar-\nchitecture capturing uniï¬ed meaning for sentence\nclassiï¬cation. In2018 IEEE International Confer-\nence on Big Data and Smart Computing (BigComp),\npages 491â€“494. IEEE.\nHan-Chin Shing, Suraj Nair, Ayah Zirikly, Meir\nFriedenberg, Hal DaumÂ´e III, and Philip Resnik.\n2018. Expert, crowdsourced, and machine assess-\nment of suicide risk via online postings. InProceed-\nings of the Fifth Workshop on Computational Lin-\nguistics and Clinical Psychology: From Keyboard to\nClinic, pages 25â€“36, New Orleans, LA. Association\nfor Computational Linguistics.\nPradyumna Prakhar Sinha, Rohan Mishra, Ramit Sawh-\nney, Debanjan Mahata, Rajiv Ratn Shah, and Huan\nLiu. 2019.#suicidal - a multipronged approach to\nidentify and explore suicidal ideation in twitter. In\nProceedings of the 28th ACM International Confer-\nence on Information and Knowledge Management,\nCIKM â€™19, page 941â€“950, New York, NY , USA. As-\nsociation for Computing Machinery.\nHajime Sueki. 2015. The association of suicide-related\ntwitter use with suicidal behaviour: a cross-sectional\nstudy of young internet users in japan.Journal of\naffective disorders, 170:155â€“160.\nMichael Mesï¬n Tadesse, Hongfei Lin, Bo Xu, and\nLiang Yang. 2020. Detection of suicide ideation\nin social media forums using deep learning.Algo-\nrithms, 13(1):7.\nNicholas Tarrier, Patricia Gooding, Lynsey Gregg, Ju-\ndith Johnson, and Richard Drake. 2007.Suicide\nschema in schizophrenia: The effect of emotional\nreactivity, negative symptoms and schema elabora-\ntion. Behaviour Research and Therapy, 45(9):2090â€“\n2097.\nCornelis Van Heeringen and A MaruË‡sic. 2003. Under-\nstanding the suicidal brain.The British Journal of\nPsychiatry, 183(4):282â€“284.\nV . Venek, S. Scherer, L. Morency, A. â€œ. Rizzo, and\nJ. Pestian. 2017. Adolescent suicidal risk assess-\nment in clinician-patient interaction.IEEE Transac-\ntions on Affective Computing, 8(2):204â€“215.\nM. J. Vioules, B. Moulahi, J. Aze, and S. Bringay.\n2018. Detection of suicide-related posts in twitter\ndata streams. IBM Journal of Research and Devel-\nopment, 62(1):7:1â€“7:12.\nWHO. 2014.Preventing suicide: A global imperative.\nWorld Health Organization.\nStefan Wojcik and Adam Hughes. 2019. Sizing up twit-\nter users.\nHenrik D Zachrisson, Kjetil RÂ¨odje, and Arnstein Myk-\nletun. 2006.Utilization of health services in relation\nto mental health problems in adolescents: A popula-\ntion based survey. BMC Public Health, 6(1).\nYu Zhu, Hao Li, Yikang Liao, Beidou Wang, Ziyu\nGuan, Haifeng Liu, and Deng Cai. What to do next:\nModeling user behaviors by time-lstm.\nAyah Zirikly, Philip Resnik,Â¨Ozlem Uzuner, and Kristy\nHollingshead. 2019.CLPsych 2019 shared task:\nPredicting the degree of suicide risk in Reddit posts.\nIn Proceedings of the Sixth Workshop on Computa-\ntional Linguistics and Clinical Psychology, pages\n24â€“33, Minneapolis, Minnesota. Association for\nComputational Linguistics.",
  "topic": "Suicidal ideation",
  "concepts": [
    {
      "name": "Suicidal ideation",
      "score": 0.7108035683631897
    },
    {
      "name": "Social media",
      "score": 0.648612380027771
    },
    {
      "name": "Ideation",
      "score": 0.49346864223480225
    },
    {
      "name": "Psychology",
      "score": 0.4915502965450287
    },
    {
      "name": "Suicide ideation",
      "score": 0.48667508363723755
    },
    {
      "name": "Suicide prevention",
      "score": 0.40569037199020386
    },
    {
      "name": "Computer science",
      "score": 0.3628655672073364
    },
    {
      "name": "Poison control",
      "score": 0.340626984834671
    },
    {
      "name": "Medicine",
      "score": 0.13115054368972778
    },
    {
      "name": "Medical emergency",
      "score": 0.12121647596359253
    },
    {
      "name": "World Wide Web",
      "score": 0.11528471112251282
    },
    {
      "name": "Cognitive science",
      "score": 0.0912962257862091
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I110166357",
      "name": "University of Delhi",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I167153416",
      "name": "Visvesvaraya National Institute of Technology",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I68891433",
      "name": "Indian Institute of Technology Delhi",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I119939252",
      "name": "Indraprastha Institute of Information Technology Delhi",
      "country": "IN"
    }
  ],
  "cited_by": 93
}