{
  "title": "Evaluation of Fake News Detection with Knowledge-Enhanced Language Models",
  "url": "https://openalex.org/W4312433629",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4314051080",
      "name": "Chenxi Whitehouse",
      "affiliations": [
        "City, University of London"
      ]
    },
    {
      "id": "https://openalex.org/A116794489",
      "name": "Tillman Weyde",
      "affiliations": [
        "City, University of London"
      ]
    },
    {
      "id": "https://openalex.org/A2686323455",
      "name": "Pranava Madhyastha",
      "affiliations": [
        "City, University of London"
      ]
    },
    {
      "id": "https://openalex.org/A2292244220",
      "name": "Nikos Komninos",
      "affiliations": [
        "City, University of London"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2954646118",
    "https://openalex.org/W6733079832",
    "https://openalex.org/W3085357310",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W6782624342",
    "https://openalex.org/W2949695381",
    "https://openalex.org/W2604264634",
    "https://openalex.org/W3119467012",
    "https://openalex.org/W6767244118",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W2954365773",
    "https://openalex.org/W3017753658",
    "https://openalex.org/W6794483692",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W3006881356",
    "https://openalex.org/W3008170323",
    "https://openalex.org/W6758221243",
    "https://openalex.org/W3212490619",
    "https://openalex.org/W2808778018",
    "https://openalex.org/W3005441132",
    "https://openalex.org/W2983102021",
    "https://openalex.org/W6763240421",
    "https://openalex.org/W2946345909",
    "https://openalex.org/W3080563675",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2969762015",
    "https://openalex.org/W4299567010",
    "https://openalex.org/W2963416784",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3175604467",
    "https://openalex.org/W3157497262",
    "https://openalex.org/W3013655557",
    "https://openalex.org/W4386506836",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2582561810",
    "https://openalex.org/W3151929433"
  ],
  "abstract": "Recent advances in fake news detection have exploited the success of large-scale pre-trained language models (PLMs). The predominant state-of-the-art approaches are based on fine-tuning PLMs on labelled fake news datasets. However, large-scale PLMs are generally not trained on structured factual data and hence may not possess priors that are grounded in factually accurate knowledge. The use of existing knowledge bases (KBs) with rich human-curated factual information has thus the potential to make fake news detection more effective and robust. In this paper, we investigate the impact of knowledge integration into PLMs for fake news detection. We study several state-of-the-art approaches for knowledge integration, mostly using Wikidata as KB, on two popular fake news datasets - LIAR, a politics-based dataset, and COVID-19, a dataset of messages posted on social media relating to the COVID-19 pandemic. Our experiments show that knowledge-enhanced models can significantly improve fake news detection on LIAR where the KB is relevant and up-to-date. The mixed results on COVID-19 highlight the reliance on stylistic features and the importance of domain specific and current KBs. The code is available at https://github.com/chenxwh/fake-news-detection.",
  "full_text": "Evaluation of Fake News Detection with Knowledge-Enhanced Language Models\nChenxi Whitehouse, Tillman Weyde, Pranava Madhyastha, Nikos Komninos\nCity, University of London {chenxi.whitehouse, t.e.weyde, pranava.madhyastha, nikos.komninos.1}@city.ac.uk\nAbstract\nRecent advances in fake news detection have exploited the\nsuccess of large-scale pre-trained language models (PLMs).\nThe predominant state-of-the-art approaches are based on\nfine-tuning PLMs on labelled fake news datasets. However,\nlarge-scale PLMs are generally not trained on structured fac-\ntual data and hence may not possess priors that are grounded\nin factually accurate knowledge. The use of existing knowl-\nedge bases (KBs) with rich human-curated factual informa-\ntion has thus the potential to make fake news detection more\neffective and robust. In this paper, we investigate the impact\nof knowledge integration into PLMs for fake news detec-\ntion. We study several state-of-the-art approaches for knowl-\nedge integration, mostly using Wikidata as KB, on two pop-\nular fake news datasets - LIAR, a politics-based dataset,\nand COVID-19, a dataset of messages posted on social me-\ndia relating to the COVID-19 pandemic. Our experiments\nshow that knowledge-enhanced models can significantly im-\nprove fake news detection on LIAR where the KB is rele-\nvant and up-to-date. The mixed results on COVID-19 high-\nlight the reliance on stylistic features and the importance of\ndomain specific and current KBs. The code is available at\nhttps://github.com/chenxwh/fake-news-detection.\nIntroduction\nThe world is witnessing a growing epidemic of fake news,\nwhich includes misinformation, disinformation, rumours,\nhoaxes, and other forms of rapidly spread and factually in-\naccurate information (Sharma et al. 2019). Fake news has\nbeen observed to severely impact political processes because\nof the wide reach of social media (Allcott and Gentzkow\n2017). Misinformation related to medical issues, such as the\nCOVID-19 pandemic, can cost lives (O’Connor and Mur-\nphy 2020). Automated methods for fake news detection and\nmitigation are a critical yet technically challenging prob-\nlem (Thorne and Vlachos 2018).\nIn this paper, we focus on content-based fake news de-\ntection: methods that assess the truthfulness of news items\nbased only on the text without using metadata. State-of-the-\nart models for this task are driven by advances in large-scale\npre-trained language models (PLMs) (e.g. Liu et al. 2019a;\nKaliyar, Goswami, and Narang 2021), which are trained on\nCopyright © 2022, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nvast amounts of raw web-based text using self-supervised\nmethods (Rogers, Kovaleva, and Rumshisky 2020). A major\nlimitation of these models is the lack of explicit grounding\nto real world entities and relations, which makes it difficult\nto recover factual knowledge (Bender et al. 2021). On the\nother hand, knowledge bases (KBs) provide a rich source\nof structured and human-curated factual knowledge, often\ncomplementary to what is found in raw text. This has re-\ncently led to the development of KB-augmented language\nmodels. Fake news detection can particularly benefit from\nthe integration of KBs, making such models less dependent\nand reliant on surface level linguistic features.\nIn this study, we empirically analyse the impact of recent\nstate-of-the-art knowledge integration methods, which en-\nhance PLMs with KBs, for content-based fake news detec-\ntion tasks. We evaluate ERNIE (Zhang et al. 2019), Know-\nBert (Peters et al. 2019), KEPLER (Wang et al. 2021b) and\nK-ADAPTER (Wang et al. 2021a) on two distinct publicly\navailable datasets: LIAR (Wang 2017), a politically oriented\ndataset, and COVID-19 (Patwa et al. 2021), a dataset re-\nlated to the recent pandemic. We find that integrating knowl-\nedge can improve fake news detection accuracy, given that\nthe knowledge bases are relevant and up-to-date. Our exper-\niments are not designed to find new state-of-the-art models\nfor these datasets, but to investigate the effect of knowledge\nbase integration into PLMs.\nOur contributions are as follows: we evaluate multiple KB\nintegration methods for fake news detection, we investigate\nmodel and data aspects that can prevent KB integration from\nbeing effective or from being effectively measured, and we\ndiscuss the potential for real-world applications.\nIn the following sections, we present a brief overview\nof four state-of-the-art methods that integrate KBs with\nPLMs studied in this paper. We then introduce and com-\npare the datasets, the experiments with different knowledge-\nenhanced models, and the effectiveness of entity linking. We\ndiscuss our findings with respect to the necessary conditions\nfor KB integration to be effective and how to assess its effect\nin application scenarios. Finally, we discuss the challenges\nin fake news detection and promising future directions.\nMethod\nIn this section, we introduce the models with KB integration,\nand describe the datasets and our experimental setup.\nProceedings of the Sixteenth International AAAI Conference on Web and Social Media (ICWSM2022)\n1425\nKnowledge Integration for PLM\nStandard deep learning models obtain information from pre-\ndicting and classifying text as they are trained, but have no\nprior knowledge of, or interaction with, world knowledge.\nAlthough PLMs can effectively characterise linguistic pat-\nterns from text to generate high-quality context-aware rep-\nresentations, they are limited in their grasp of knowledge,\nconcepts, and relations, which are essential for some Nat-\nural Language Processing (NLP) tasks, including assessing\nthe truthfulness of a news item.\nOn the other hand, KBs like Wikidata (https://www.\nwikidata.org) and WordNet (Miller 1995) contain rich cu-\nrated information about the world. Thus, they could greatly\ncomplement PLMs if effective integration methods were\navailable. Several efforts have been made to integrate KBs\ninto PLMs. In this paper, we study the following models:\nERNIE injects knowledge into BERT (Devlin et al. 2019)\nby pre-training a language model on both large corpora and\nKBs. It uses TAGME (Ferragina and Scaiella 2010) to link\nentities to Wikidata. TAMGE identifies entity mentions in\ninput text and links them to associated entity embeddings,\nwhich are then fused into the corresponding positions of the\ntext. The knowledge-based learning objective is to predict\nthe correct token-entity alignment. ERNIE has enhanced\nperformance over BERT in entity typing and relation clas-\nsification (Zhang et al. 2019).\nKnowBert incorporates KBs into BERT using a knowl-\nedge attention and re-contextualisation mechanism. It iden-\ntifies entity spans in input text and incorporates an integrated\nentity linker in the model to retrieve entity embeddings from\na KB. The entity linker is responsible for entity disambigua-\ntion, which considers 30 entity candidates and uses their\nweighted average embedding. Knowledge-enhanced entity-\nspan representations are then re-contextualised with a word-\nto-entity attention technique. KnowBert has shown improve-\nment over BERT in relationship extraction, entity typing and\nword sense disambiguation (Peters et al. 2019).\nKEPLER integrates factual knowledge into PLMs by\nadding a knowledge embedding objective with the super-\nvision from a KB, and optimising it jointly with language\nmodelling objectives. KEPLER is trained to encode the en-\ntities from their contextual descriptions, which enhances the\nability of PLMs to extract knowledge from text. By keeping\nthe original structures of PLMs, KEPLER can be used in\ngeneral downstream NLP tasks without additional inference\noverhead. It is shown that KEPLER improves performance\nover RoBERTa (Liu et al. 2019b) in relationship extraction,\nentity typing and link prediction (Wang et al. 2021b).\nK-ADAPTER retains the PLMs unchanged, but adds\nlearnable adapter features that are trained in a multi-task\nsetting on relation prediction and dependency-tree predic-\ntion. Two kinds of knowledge adapters have been developed\nby Wang et al. (2021a): factual knowledge obtained from au-\ntomatically aligned text triples on Wikipedia and Wikidata,\nand linguistic knowledge obtained via dependency parsing.\nBoth have been found to improve relation classification, en-\ntity typing and question answering (Wang et al. 2021a).\nDatasets\nIn our experiments, we use LIAR and COVID-19 to study\nfake news detection. They both consist of short statements,\nbut with different content, time of collection, linguistic and\nstylistic features.\nLIAR was collected in 2017 from Politifact (https://www.\npolitifact.com). It includes 12.8k human-labelled short state-\nments about US politics from various contexts, i.e. news re-\nleases, TV interviews, campaign speeches, etc. Each state-\nment has been rated for truthfulness by a Politifact editor\nusing a six grade scale: “pants-fire”, “false”, “barely-true”,\n“half-true”, “mostly true”, and “true”. LIAR also provides\nmetadata (e.g. speaker, context), which we do not use in our\nexperiments. While Wang (2017) has been widely cited, we\nonly found three other results for our specific task (no meta-\ndata, six classes) (Alhindi, Petridis, and Muresan 2018; Liu\net al. 2019a; Chernyavskiy and Ilvovsky 2020), the latter has\nthe current best accuracy of 34.5%.\nCOVID-19 was collected in 2020 after the COVID-19\noutbreak. It consists of 10.5k posts related to the pandemic\nwhich are obtained from different social-media sites includ-\ning Twitter, Facebook, and Instagram. The fake posts were\ncollected from various fact-checking websites, i.e. Politi-\nfact and NewsChecker (https://newschecker.in), and the real\nposts were from Twitter using verified Twitter handles. Each\npost has a label, “real” or “fake”. It was used as a shared task\nin the CONSTRAINT 2021 workshop (Chakraborty 2021)\nwith the best reported accuracy of 98.69%.\nExperimental Setup\nWe use an empirical approach to study the effect of knowl-\nedge integration on fake news detection, to understand how\nknowledge is used by the model, and to evaluate the quality\nof the entity linker to the KB.\nERNIE and KnowBert are built on BERT-base, whereas\nKEPLER and K-ADAPTER are enhanced from RoBERTa-\nbase and RoBERTa-large, respectively. We follow the con-\ncept of an ablation study to investigate the influence of the\nexternal knowledge by comparing the performance of each\nknowledge-enhanced PLM with the corresponding baseline\nmodel. We note that ERNIE and KnowBert incorporate en-\ntity embeddings that are linked to the input. Therefore we\nvisualise the entities linked that contribute to the fake news\ndetection task in ERNIE, and design experiments to investi-\ngate the impact of entity disambiguation of KnowBert.\nWe evaluate the performance of the models on fake news\ndetection by fine-tuning the knowledge-enhanced PLMs on\nthe training set with the same hyper-parameter settings. The\ninput text is fed first to the PLM, and followed by a dropout\n(p = 0.1) and a linear layer. The output is then passed to a\nsoftmax layer for classification. We use AdamW optimiser\n(Loshchilov and Hutter 2019) (learning rate of5×10−6) and\ncross entropy as the loss function. Maximum input length is\nset to 128, and the batch size is 4. We train for 10 epochs and\nusually observe convergence after five. We perform five runs\nfor each experiment and report the average accuracy with the\nstandard deviation. Both LIAR and COVID-19 are already\n1426\nLIAR\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5Number of words per statement\nCOVID-19\n0\n5\n10\n15\n20\n25\n30\n35 Real\nFake\n(a) Word count\nper statement\nNOUN PROPN VERB ADJ ADV PUNCT NUMS\n0\n2\n4\n6\n8\n10Occurrence per statement\nReal\nFake (b) POS, punctuation, numbers\nin LIAR\nNOUN PROPN VERB ADJ ADV PUNCT NUMS HTTPS\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5Occurrence per statement\nReal\nFake (c) POS, punctuation, numbers,\nhttps in COVID-19\nFigure 1: Number of\nwords, POS tags, punctuation and numbers per statement in real and fake news inLIAR and COVID-19,\nand number of https-links per statement in COVID-19. The mean values are shown as white filled circles in the plot.\ndivided into train, validation, and test splits, which we use in\nour experiments as provided.\nLinguistic Feature Analysis We also perform linguistic\nfeature analysis following the work in Horne and Adali\n(2017) to investigate stylistic difference between real and\nfake news in the datasets. We use spaCy (https://spacy.io) to\nparse the statements and get the Part-of-Speech (POS) tags.\nFor LIAR, we group “pants-fire”, “false”, and “barely-\ntrue” as fake and “half-true”, “mostly true”, and “true” as\nreal. We compare the distribution of different words, POS\ntags (NOUN, PROPN, VERB, ADJ, ADV), punctuation,\nand number-like words in each statement in Figure 1.\nThe length of posts is quite different between the two\nclasses in COVID-19, with average of 32 and 22 for real\nand fake statements, respectively, as shown in Figure 1a.\nLIAR, on the other hand, has similar statement length, with\n18 words per statement for real and 17 for fake.\nIn general, COVID-19 has distinct linguistic features be-\ntween classes whereas LIAR shows more similar features.\nIn particular, COVID-19 contains links, mostly https links,\nwhich are listed as a separate category in Figure 1c, showing\na very skewed distribution.\nExperiments and Results\nFor our experiments, we use ERNIE, three pre-trained\nKnowBert models with different KBs (Wiki, WordNet,\nW+W), KEPLER, and K-ADAPTER with three adapters (F,\nL, F-L) in the published implementation, fine-tune the mod-\nels to our task, and compare the result with the baseline mod-\nels - BERT-base, RoBERTa-base, and RoBERTa-large.\nDetection Accuracy The detection accuracy of the\nknowledge-enhanced PLMs and the corresponding baselines\nis shown in Table 1. On LIAR, all knowledge-enhanced\nmethods improve over the baseline with KnowBert-W+W\nreaches the best overall result (improvement of +2.59 over\nBERT-base), whereas on COVID-19, only three of eight\nmodels show improvement, and only by a small margin.\nThe computational cost varies per approach. KEPLER\nretains the baseline PLM architecture, thus there is no\nMODEL BASE LIAR COVID-19\nBERT-Base (BB) - 26.36 ±0.58 97.51 ±0.19\nRoBERTa-Base (RB) - 26.71 ±0.93 97.61 ±0.26\nRoBERTa-Large (RL) - 27.36 ±0.79 97.92 ±0.17\nERNIE BB 27.53 ±0.13 97.30 ±0.18\nKnowBert-Wiki BB 27.64 ±0.09 97.37 ±0.09\nKEPLER RB 26.77 ±1.15 97.58 ±0.15\nK-ADAPTER-F RL 28.63 ±0.90\n∗ 97.92 ±0.10\nKnowBert-WordNet BB 26.95 ±0.45 97.00 ±0.06\nKnowBert-W+W BB 28.95 ±0.64\n∗ 97.56 ±0.15\nK-ADAPTER-L RL 28.46 ±0.87\n∗ 98.07 ±0.09\nK-ADAPTER-F-L RL 27.45 ±0.78 98.11 ±0.14\nTable 1: Detection accuracy results (average of five runs).\nThe first section corresponds to the baseline models. Mod-\nels in the second section use Wikidata KB. The third section\nshows models using other KBs and features. The best values\nwithin each section per dataset are marked in bold. The sub-\nscript numbers with ± show the standard deviation. Results\nwith ∗ indicate statistically significant improvements over\nthe baseline, both for mean (t-test, one-sided, p < .05) and\nmedian (Wilcoxon signed rank test, one-sided, p < .05).\noverhead compared to RoBERTa-base. K-ADAPTER also\nfreezes the RoBERTa-large layers, but there is an overhead\nof 9-23% from the adapters, while the overhead for Know-\nBert is 40-87% and 111-131% for ERNIE.\nKB Linking ERNIE and KnowBert create links between\nthe text and KB entities at runtime and the quality of this\nlinking influences the output. ERNIE uses TAGME and se-\nlects only one entity candidate per text span. In Figure 2 we\nshow the 50 most frequently selected KB entities for each\ndataset. We can see that in COVID-19, the most frequent\nentities are not content-related (“https”, “twitter”) while\n“COVID-19”, the most frequent relevant term in the dataset,\nis missing in the linked entities. For LIAR, on the other\nhand, the linked entities seem relevant. SinceLIAR was col-\nlected three years earlier, it is apparently a better match for\n1427\nFigure 2: Word clouds for the 50 most frequent entities\nlinked by ERNIE in LIAR (top) and COVID-19 (bottom).\nthe entity linker and the KB used. Another potential influ-\nence on the effectiveness of KB integration is the number of\nlinked entities. In contrast to ERNIE, KnowBert selects 30\nmost probable entities per text span. In a sensitivity study,\nwe restrict KnowBert-W+W to only one entity, which re-\nduces the accuracy onLIAR from 28.95% to 27.31%, below\nthe accuracy of ERNIE (27.53%).\nDiscussion\nThe reliable improvement of detection accuracy on LIAR\nby integrating PLMs with Wikidata shows the potential of\nknowledge integration exceeding the results obtained by in-\ntegrating multiple types of metadata by Wang (2017). On the\nother hand, the improvements are good but not dramatic for\nLIAR and not consistent for COVID-19. We can identify\ntwo aspects contributing to the result which are relevant to\nthe effective use of knowledge-enhanced models:\n1) Currentness and relevance of the KB: as COVID-19\nwas collected after most of the PLMs were trained, some\nterms such as “COVID-19” are not in the KB;\n2) Quality of the dataset: the COVID-19 dataset con-\ntains confounders that provide strong cues, overshadowing\nthe impact of the knowledge base. The most important one\nis the occurrence of https links, which appear in 95.3% of\nthe real posts but only 42.3% of the fake posts.\nThere is also potential to achieve more explainability\nand interpretability with direct KB integration at runtime.\nTake this statement from COVID-19: “DNA Vaccine: in-\njecting genetic material into the host so that host cells cre-\nate proteins that are similar to those in the virus against\nwhich the host then creates antibodies” as an example,\nKnowBert-W+W correctly classifies it as “real”, whereas\nBERT-base fails. We observe most mention spans in the\nstatement, i.e. “DNA”, “injecting”, “genetic”, “genetic ma-\nterial”, “host”, “cells”, etc. are correctly linked to entities\n“DNA”, “Injection\n(medicine)”, “Genetics”, “Genome”,\n“Host (biology)”, “Cell (biology)”, respectively, therefore\nit seems that the entity links may have contributed to\nKnowBert-W-W for this classification. However, the level\nof explainability is still limited.\nApplication Aspects Automatic fake news detection in\npractice adds two dynamic application aspects, which are\ndifficult to test with static datasets as our experiment on\nCOVID-19 has shown:\n(1) Dynamic adaptation: it is necessary to update the\nsystem to the changing characteristics of real and fake\nnews (Silva and Almeida 2021). Knowledge-enhanced mod-\nels that use KBs at runtime offer an opportunity to update the\nKB independent of the model. This has the advantage that\nfake news can be recognised as contradicting the KB before\nthere are any fake news examples.\n(2) Adversarial robustness: fake news authors are very\nlikely to take evasive action. Adapting the text style is rela-\ntively easy and could be automated, which makes the detec-\ntion with stylistic features difficult (see Zellers et al. 2019;\nSchuster et al. 2020).\nDeployment of fake news detection in social media will\nalso need human verification, e.g. when a user challenges\nactions taken against them. Here, KB integration can offer\nthe advantage of insight into knowledge that has been used\nin the detection for better explainability.\nRelated Work\nIn recent years large-scale PLMs i.e. BERT and RoBERTa\nhave dominated NLP tasks, including some content-based\nfake news detection (Kaliyar, Goswami, and Narang 2021).\nMost fake news detection approaches either combine text\nwith metadata (e.g. Ding, Hu, and Chang 2020), or fo-\ncus only on the source of the text (e.g. Nørregaard, Horne,\nand Adali 2019). For LIAR, Alhindi, Petridis, and Mure-\nsan (2018) extend the data with evidence sentences in\na new dataset called LIAR-PLUS to improve detection.\nChernyavskiy and Ilvovsky (2020) introduce a Deep Av-\neraging Network to model the discursive structure of the\ntext and use Siamese models on the extended text data. Liu\net al. (2019a) predict labels at two levels of granularity. For\nCOVID-19, there are a number of results from the CON-\nSTRAINTS 2021 workshop (Chakraborty 2021) which use\na wide variety of traditional and neural NLP models. None\nof these approaches uses external knowledge, so they could\nall benefit from KB integration.\nConclusion and Future Work\nIn this paper, we study the effectiveness of enhancing PLMs\nwith knowledge bases for fake news detection. We find that\nintegrating knowledge with PLMs can be beneficial on a\nstatic dataset but it depends on suitable KBs and the qual-\nity of the data. On the modelling level there are many routes\nfor improvement. For practical application, more insight on\nwhat knowledge is used would be useful as well as dynamic\nadaptation of the models and the KBs. Integrating KBs with\nPLMs offers potentially more robust and timely fake news\ndetection. However, a new evaluation approach, i.e. a test-\ning scenario that models dynamic knowledge as well as ad-\nversarial and automatic fake news generators, is needed to\nassess the true potential of knowledge integration.\n1428\nReferences\nAlhindi, T.; Petridis, S.; and Muresan, S. 2018. Where is\nYour Evidence: Improving Fact-checking by Justification\nModeling. In Proceedings of the First Workshop on Fact Ex-\ntraction and VERification (FEVER), 85–90. Brussels, Bel-\ngium: ACL.\nAllcott, H.; and Gentzkow, M. 2017. Social Media and Fake\nNews in the 2016 Election. Journal of Economic Perspec-\ntives, 31(2): 211–36.\nBender, E. M.; Gebru, T.; McMillan-Major, A.; and\nShmitchell, S. 2021. On the Dangers of Stochastic Par-\nrots: Can Language Models Be Too Big? In Proceedings\nof the 2021 ACM Conference on Fairness, Accountability,\nand Transparency, 610–623.\nChakraborty, T. 2021. Combating Online Hostile Posts in\nRegional Languages during Emergency Situation: First In-\nternational Workshop, CONSTRAINT 2021, Collocated with\nAAAI 2021, Virtual Event, February 8, 2021, Revised Se-\nlected Papers. Springer Nature.\nChernyavskiy, A.; and Ilvovsky, D. 2020. Recursive Neural\nText Classification Using Discourse Tree Structure for Argu-\nmentation Mining and Sentiment Analysis Tasks. In ISMIS,\n90–101. Springer.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In NAACL-HLT, 4171–4186.\nMinneapolis, Minnesota: ACL.\nDing, J.; Hu, Y .; and Chang, H. 2020. BERT-based Mental\nModel, a Better Fake News Detector. In Proceedings of the\n2020 6th international conference on computing and artifi-\ncial intelligence, 396–400.\nFerragina, P.; and Scaiella, U. 2010. TAGME: On-the-fly\nAnnotation of Short Text Fragments (by Wikipedia Entities).\nProceedings of the 19th ACM international conference on\nInformation and knowledge management.\nHorne, B.; and Adali, S. 2017. This Just In: Fake News\nPacks A Lot in Title, Uses Simpler, Repetitive Content in\nText Body, More Similar to Satire Than Real News. Pro-\nceedings of the International AAAI Conference on Web and\nSocial Media, 11(1): 759–766.\nKaliyar, R. K.; Goswami, A.; and Narang, P. 2021. Fake-\nBERT: Fake News Detection in Social Media with a BERT-\nbased Deep Learning Approach. Multimedia tools and ap-\nplications, 80(8): 11765–11788.\nLiu, C.; Wu, X.; Yu, M.; Li, G.; Jiang, J.; qing Huang, W.;\nand Lu, X. 2019a. A Two-Stage Model Based on BERT for\nShort Fake News Detection. In KSEM.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy,\nO.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V . 2019b.\nRoBERTa: A Robustly Optimized BERT Pretraining Ap-\nproach. CoRR, abs/1907.11692.\nLoshchilov, I.; and Hutter, F. 2019. Decoupled Weight De-\ncay Regularization. In 7th International Conference on\nLearning Representations, ICLR 2019.\nMiller, G. 1995. WordNet: A Lexical Database for English.\nCommun. ACM, 38: 39–41.\nNørregaard, J.; Horne, B. D.; and Adali, S. 2019. NELA-GT-\n2018: A Large Multi-Labelled News Dataset for the Study\nof Misinformation in News Articles. Proceedings of the\nInternational AAAI Conference on Web and Social Media ,\n13(01): 630–638.\nO’Connor, C.; and Murphy, M. 2020. Going Viral: Doctors\nMust Tackle Fake News in the Covid-19 Pandemic. Bmj,\n369(10.1136).\nPatwa, P.; Sharma, S.; Pykl, S.; Guptha, V .; Kumari, G.;\nAkhtar, M. S.; Ekbal, A.; Das, A.; and Chakraborty, T. 2021.\nFighting an Infodemic: COVID-19 Fake News Dataset. In\nCONSTRAINT@AAAI.\nPeters, M. E.; Neumann, M.; Logan, R.; Schwartz, R.; Joshi,\nV .; Singh, S.; and Smith, N. A. 2019. Knowledge En-\nhanced Contextual Word Representations. In EMNLP/IJC-\nNLP, 6086–6093. Hong Kong, China: ACL.\nRogers, A.; Kovaleva, O.; and Rumshisky, A. 2020. A\nPrimer in BERTology: What We Know about How BERT\nWorks. Transactions of the ACL, 8: 842–866.\nSchuster, T.; Schuster, R.; Shah, D. J.; and Barzilay, R. 2020.\nThe Limitations of Stylometry for Detecting Machine-\nGenerated Fake News. Computational Linguistics, 46(2):\n499–510.\nSharma, K.; Qian, F.; Jiang, H.; Ruchansky, N.; Zhang, M.;\nand Liu, Y . 2019. Combating Fake News.ACM Transactions\non Intelligent Systems and Technology (TIST), 10: 1 – 42.\nSilva, R. M.; and Almeida, T. A. 2021. How Concept Drift\ncan Impair the Classification of Fake News. In Anais do IX\nSymposium on Knowledge Discovery, Mining and Learning,\n121–128. SBC.\nThorne, J.; and Vlachos, A. 2018. Automated Fact Check-\ning: Task Formulations, Methods and Future Directions. In\nProceedings of the 27th International Conference on Com-\nputational Linguistics, 3346–3359. Santa Fe, New Mexico,\nUSA: ACL.\nWang, R.; Tang, D.; Duan, N.; Wei, Z.; Huang, X.; Ji, J.;\nCao, G.; Jiang, D.; and Zhou, M. 2021a. K-Adapter: Infus-\ning Knowledge into Pre-Trained Models with Adapters. In\nFindings of the Association for Computational Linguistics:\nACL-IJCNLP 2021, 1405–1418. Online: ACL.\nWang, W. Y . 2017. “Liar, Liar Pants on Fire”: A New Bench-\nmark Dataset for Fake News Detection. In Proceedings of\nthe 55th Annual Meeting of the ACL (Volume 2: Short Pa-\npers), 422–426. Vancouver, Canada: ACL.\nWang, X.; Gao, T.; Zhu, Z.; Zhang, Z.; Liu, Z.; Li, J.; and\nTang, J. 2021b. KEPLER: A Unified Model for Knowl-\nedge Embedding and Pre-trained Language Representation.\nTrans. Assoc. Comput. Linguistics, 9: 176–194.\nZellers, R.; Holtzman, A.; Rashkin, H.; Bisk, Y .; Farhadi, A.;\nRoesner, F.; and Choi, Y . 2019. Defending Against Neural\nFake News. In Advances in Neural Information Processing\nSystems, volume 32. Curran Associates, Inc.\nZhang, Z.; Han, X.; Liu, Z.; Jiang, X.; Sun, M.; and Liu, Q.\n2019. ERNIE: Enhanced Language Representation with In-\nformative Entities. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics, 1441–\n1451. Florence, Italy: ACL.\n1429",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7586781978607178
    },
    {
      "name": "Social media",
      "score": 0.6814241409301758
    },
    {
      "name": "Fake news",
      "score": 0.6747586727142334
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5389057397842407
    },
    {
      "name": "Domain knowledge",
      "score": 0.5206314921379089
    },
    {
      "name": "Data science",
      "score": 0.501373291015625
    },
    {
      "name": "Language model",
      "score": 0.49273231625556946
    },
    {
      "name": "Prior probability",
      "score": 0.4479859173297882
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33585458993911743
    },
    {
      "name": "Information retrieval",
      "score": 0.32609790563583374
    },
    {
      "name": "World Wide Web",
      "score": 0.1814051866531372
    },
    {
      "name": "Internet privacy",
      "score": 0.16861256957054138
    },
    {
      "name": "Bayesian probability",
      "score": 0.062090128660202026
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I180825142",
      "name": "City, University of London",
      "country": "GB"
    }
  ],
  "cited_by": 19
}