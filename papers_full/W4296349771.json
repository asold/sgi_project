{
    "title": "No secrets between the two of us: Privacy concerns over using AI agents.",
    "url": "https://openalex.org/W4296349771",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2278231318",
            "name": "Sohye Lim",
            "affiliations": [
                "Ewha Womans University"
            ]
        },
        {
            "id": "https://openalex.org/A2169117681",
            "name": "Hongjin Shim",
            "affiliations": [
                "Korea Information Society Development Institute"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2753883666",
        "https://openalex.org/W2133387828",
        "https://openalex.org/W2075220185",
        "https://openalex.org/W2888047309",
        "https://openalex.org/W3020776782",
        "https://openalex.org/W4242636445",
        "https://openalex.org/W2956843553",
        "https://openalex.org/W1791587663",
        "https://openalex.org/W2143545365",
        "https://openalex.org/W2755246076",
        "https://openalex.org/W2163080637",
        "https://openalex.org/W2996350641",
        "https://openalex.org/W3006245018",
        "https://openalex.org/W2914188784",
        "https://openalex.org/W3096078367",
        "https://openalex.org/W2551851611",
        "https://openalex.org/W2788781842",
        "https://openalex.org/W2789691990",
        "https://openalex.org/W2328332630",
        "https://openalex.org/W2139613108",
        "https://openalex.org/W2806569034",
        "https://openalex.org/W1866272482",
        "https://openalex.org/W1656810637",
        "https://openalex.org/W2222174245",
        "https://openalex.org/W3029119254",
        "https://openalex.org/W2901576515",
        "https://openalex.org/W2975449289",
        "https://openalex.org/W6664729310",
        "https://openalex.org/W2589387806",
        "https://openalex.org/W2125495423",
        "https://openalex.org/W2922503844",
        "https://openalex.org/W2083365005",
        "https://openalex.org/W2099386544",
        "https://openalex.org/W3151427199",
        "https://openalex.org/W3161415555",
        "https://openalex.org/W2051626970",
        "https://openalex.org/W2140778623",
        "https://openalex.org/W1991015565",
        "https://openalex.org/W2149595999",
        "https://openalex.org/W2113414442",
        "https://openalex.org/W2904051993",
        "https://openalex.org/W1976463060",
        "https://openalex.org/W2517471910",
        "https://openalex.org/W2929706395",
        "https://openalex.org/W2537632655",
        "https://openalex.org/W6658476690",
        "https://openalex.org/W2026805706",
        "https://openalex.org/W2096178490",
        "https://openalex.org/W1637481019",
        "https://openalex.org/W1968444823"
    ],
    "abstract": "The diverse spread of artificial intelligence (AI) agents provides evidence of the most notable changes in the current media landscape. AI agents mostly function based on voluntary and involuntary sharing of users’ personal information. Accordingly, users’ privacy concerns have become key to understanding the varied psychological responses towards AI agents. In this study, we adopt the “computers are social actors” paradigm to identify the association between a set of relational variables—intimacy, para-social interactions, and social presence—and privacy concerns and to determine whether a user’s motivations moderate this relationship. The results from an online survey (N = 562) revealed that this occurs primarily to gratify three AI agent user needs: entertainment motivation, instrumental motivation, and passing time. The results also confirmed that social presence and intimacy significantly influence users’ privacy concerns. These results support the moderating effect of both entertainment and instrumental motivation on the relationship between intimacy, para-social interaction, social presence, and privacy concerns about using AI agents. Further implications for privacy concerns in the context of AI-mediated communications are discussed.",
    "full_text": " \nLim, S., & Shim, H. (2022). No secrets between the two of us: Privacy concerns over using AI agents. \nCyberpsychology: Journal of Psychosocial Research on Cyberspace, 16(4), Article 3.  \nhttps://doi.org/10.5817/CP2022-4-3 \nNo Secrets Between the Two of Us: Privacy Concerns Over Using AI \nAgents \nSohye Lim1, & Hongjin Shim2 \n1 Ewha Womans University, Republic of Korea \n2 Korea Information Society Development Institute (KISDI), Republic of Korea \nAbstract \nThe diverse spread of artificial intelligence (AI) agents provides evidence of the most \nnotable changes in the current media landscape. AI agents mostly function based on \nvoluntary and involuntary sharing of users’ personal information. Accordingly, users’ \nprivacy concerns have become key to understanding the varied psychological \nresponses towards AI agents. In this study, we adopt the “computers are social actors” \nparadigm to identify the association between a set of relational variables—intimacy, \npara-social interactions, and social presence—and privacy concerns and to determine \nwhether a user’s motivations moderate this relationship. The results from an online \nsurvey (N = 562) revealed that this occurs primarily to gratify three AI agent user needs: \nentertainment motivation, instrumental motivation, and passing time. The results also \nconfirmed that social presence and intimacy significantly influence users’ privacy \nconcerns. These results support the moderating effect of both entertainment and \ninstrumental motivation on the relationship between intimacy, para-social interaction, \nsocial presence, and privacy concerns about using AI agents. Further implications for \nprivacy concerns in the context of AI-mediated communications are discussed. \n \nKeywords: AI; intelligent agent; privacy concerns; para-social interaction; \nsocial presence; intimacy; motivation \nEditorial Record \nFirst submission received: \nNovember 24, 2020 \n \nRevisions received: \nFebruary 2, 2022 \nApril 27, 2022 \nJuly 6, 2022 \n \nAccepted for publication: \nJuly 11, 2022 \n \nEditor in charge: \nMichel Walrave  \nIntroduction \nThe Artificial intelligence (AI) in various forms has rapidly become a feature of everyday life: it has been actively \napplied to enhance existing media services and has given rise to new forms of technology. These new \ndevelopments have mostly enabled unprecedented types of interactions with greater ease. Software that operates \nacross different platforms with “the ability to respond to users’ demands synchronically, engage in humanoid \ninteraction, even learn users’ behavior preferences and evolve over time” is referred to as AI agents (Cao et al., \n2019, p. 188). \nThe platform of AI agents in particular has advanced in almost all areas of information-based services to promote \nefficient and effective communication. Individuals use a wide range of AI agents such as Siri (Apple), Cortana \n(Microsoft), Alexa (Amazon), and Bixby (Samsung) that operate on various platforms like smartphones, smart \nspeakers, and car navigators.  \nIncreased use of such AI agents caters to various user needs; however, as well as the benefits and convenience \nthey provide, the usefulness and convenience of AI devices are inevitably linked to sharing and disclosure of users’ \n\n \npersonal information, given that data is at the core of all AI agent functionality (Elish & boyd, 2018). Consequently, \nthis raises critical questions regarding users’ privacy. Genpact (2017) reported that 71% of over 5,000 users polled \nin the United States, the United Kingdom, and Australia reported they did not want companies to use AI \ntechnologies that threaten infringement of their privacy, even if the technology could improves user experience. \nA recent study by Chung and S. Lee (2018) demonstrates how a wide range of personal information (from user \ninterests to sleeping/waking patterns) can be collected by virtual assistants. In accordance with increasing \nawareness of privacy-related issues, recent studies have identified user privacy as the most prominent concern \n(Aleipis & Patsakis, 2017; Liao et al., 2019), although theorization is still in its infancy. While a more rigorous \napproach is applied in some studies to elucidate the formation and effects of user privacy (Lutz & Newlands, 2021; \nLutz & Tamò-Larrieux, 2021), investigating the social nature of human-agent communication can offer a significant \ncontribution to the related field.  \nThe aim of this paper is to explore the conditions that affect users’ privacy concerns. Not all AI-agent users have \nthe same level of concern about their privacy during use. Drawing on the overarching “computers are social actors” \n(CASA) theoretical framework (Nass & Moon, 2000; Reeves & Nass, 1996), the psychology of close relationships \noffers a lens through which to understand user responses toward AI agents. Accordingly, three variables—\nintimacy, para-social interaction (PSI), and social presence—are applied that relate to the psychological \nrelationship between the user and AI agents and that reflect the extent to which the user perceives the AI-based \nagent as a human partner. It is assumed that once a user perceives a close relationship with the AI agent, they will \ndisclose more personal information, which in turn amplifies privacy concerns. In addition, users’ various \nmotivations are noted as a key variable that could bring about significant changes in the extent of users’ privacy \nconcerns. This study considers the moderating role of motivations in using AI agents to explore the relationship \nbetween factors associated with users’ privacy concerns in greater depth.  \nTheoretical Framework and Hypotheses \nA wide deployment of AI technology has engendered new forms of media agents in recent years. A number of \nsimilar terms have been used interchangeably to refer to speech-based AI technology, such as voice assistants, \nconversational agents, AI agents, and chatbots. Most AI agents are capable of providing “small talk,” offering factual \ninformation, and answering complex questions in real time. To illustrate, if a user tells their Apple Siri to marry \nthem, Siri will answer: “My end-user licensing agreement does not cover marriage. My apologies.” It is this hedonic \nuse of AI agents that distinguishes them from all preceding information systems, thus expanding new possibilities \nfor computer-human interaction by simulating the naturalness of human conversation. Previous findings suggest \nthat small talk deepens relationships between AI agents and human users (Park et al., 2019). Whether feelings that \nusers apply to their agents are similar to those experienced in interpersonal relationships presents an interesting \nway of exploring the issue of users’ privacy concerns about information transmitted and stored via the AI-agent. \nIf user experiences of AI interaction assimilate the psychological closeness that are unique to human-to-human \ncommunication, it could be questioned whether their concerns over personal information could be alleviated.  \nPresented with the new possibility of human-like conversations between AI agents and their users, CASA theory is \nconsidered the most suitable framework to underpin the current study. According to the CASA perspective, \nhumans treat computers and media like real people, mindlessly applying scripts to interact with humans and for \ninteractions with social technologies (Gambino et al., 2020). The CASA approach has gained validity across various \ntypes of media agent, such as embodied agents (Hoffmann et al., 2009), smart speakers (Foehr & Germelmann, \n2020), and chatbots (Ho et al., 2018). The CASA approach is particularly suited to elucidating the nature of human \ninteraction with social technologies that offer both “social cues” (Nass & Moon, 2000) and “sourcing” (Nass & \nSteuer, 1993). Social technologies need to present the user enough cues to induce social responses; moreover, \nusers should be able to perceive the technologies as autonomous sources. In this context, chatbots lie within the \nscope of CASA as social cues and sourcing are two important features of chatbots.  \nMore recently, it has been suggested that the new conceptual element “social affordances” (Gambino et al., 2020) \nshould be incorporated to assess how humans interpret the social potential of a media agent. For instance, Foehr \nand Germelmann (2020) argue that users not only consider anthropomorphic cues but regard technologies as \nsocial actors with which they form interpersonal relationships. The social affordances of AI agents are operating \nat an unprecedented level; however, this means the security of personal information is also increasingly being \nchallenged.  \n \n \nPrivacy Concerns Toward AI Agents \nThere is an exponential increase in the breadth and volume of personal information that can potentially be \ngathered through the use of AI agents. Communication with AI agents inevitably leads to the disclosure of personal \nand private information (with or without the user’s knowledge), ranging from product information sought by the \nuser to the food that the user is having delivered for dinner. Being mostly aware of this, users share information \nby their own volition to serve various other interests; however, they may not always be aware of the nature, type, \nor purpose of information collected. A study by Huang et al. (2020) reveals that users of smart speakers tend to \nlack understanding about what data is available and what is kept private. Accordingly, concerns about personal \ninformation infringement have become a major issue related to the expansion of AI-agent services. While privacy \nis a multifaceted concept, the “social informational privacy” definition used in Lutz and Tamò-Larrieux’s (2021, p. 7) \nstudy is applied here as we understand social robots to be a type of embodied AI agent. Thus, in this study, social \ninformational privacy focuses on users’ understanding of how information shared with AI agents is processed, \nespecially considering the anthropomorphic effect of the agent. \nIn particular, the “privacy calculus” theory sheds light on the mechanism by which users assess the cost and benefit \nof revealing private information. Personal privacy interests are understood to be an exchange of information and \nbenefits and provide the foundation for the trade-off between privacy risks and benefits when users are requested \nto provide their personal information (Smith et al., 2011). During communication with an AI agent, individuals often \ncalculate the potential benefits and risks that information sharing might bring about. In turn, this will affect \nindividuals’ usage behavior. Variables such as network externalities, trust, and information sensitivity are known \nto affect users’ benefit-and-risk analysis and the consequent provision of personal information (Kim et al., 2019). \nThe extent to which the user is concerned about the risk of disclosing their private information thus plays a key \nrole in determining not only adoption of the technology but also many qualitative aspects of the experience. \nMoorthy and Vu (2015) reported that AI agent users were more cautious in disclosing private than non-private \ninformation, emphasizing that privacy concerns are a major reason for not using AI agents.  \nPsychology of Close Relationships: Intimacy, Para-Social Interaction, and Social Presence \nBased on the CASA perspective, the psychology of close relationships might offer an important means of predicting \nprivacy concerns. Lutz and Tamò-Larrieux’s (2021) research on social robots found that social influence is a key \nfactor in privacy concerns: the degree the user anthropomorphizes social robots is directly related to the degree \nof privacy concerns they bear. Thus, it can be inferred that users’ privacy concerns may be affected by the extent \nto which they evaluate an AI agent to be worthy of a social relationship. Disclosure intimacy, para-social interaction \n(PSI), and social presence are the three predominant constructs that have been applied in human-agent \ncommunication to develop close relationships.  \nFirst, disclosure intimacy is a principal variable in explaining relationship-building. Related studies maintain that \npeople may experience a greater level of disclosure intimacy when interacting with AI agents compared to human \npartners due to reduced concerns about impression management and no risk of negative evaluation. For example, \nHo et al. (2018) examined the downstream effects that occurred after emotional and factual disclosures in \nconversations with either a supposed AI agent or a real person. Their experiment revealed that the effects of \nemotional disclosure were equivalent, regardless of who participants thought they were disclosing to. According \nto Ho et al. (2018), the disclosure processing frame could explain how the user’s experience of disclosure intimacy \ntoward an agent is not as weak as expected compared with those toward a human partner. The disclosure \nprocessing frame postulates that computerized agents reduce impression management and increase disclosure \nintimacy, especially in situations where a negative evaluation may be prominent, such as when asked potentially \nembarrassing questions (Lucas et al., 2014). Moreover, it suggests that more intimate disclosure from the user \nresults in emotional, relational, and psychological benefits in human-agent communication (Ho et al., 2018).  \nDespite this, intimacy is not without cost because disclosure of personal information may be associated with \ngreater privacy concerns. The famous novelist Barry (2014) illustrates an individual’s delicate balance between \nopenness and privacy with the quote: “so we exchange privacy for intimacy.” In a related study by S. Lee and Choi \n(2017), a high level of self-disclosure and reciprocity in communication with conversational agents was found to \nsignificantly increase trust, rapport, and user satisfaction. Assuming that exchanging and sharing personal \ninformation is likely to enhance relational intimacy, and that researchers have not empirically identified \na significant difference across human-to-human and human-to-agent communication, the extent of intimacy \n \nperceived by the user is likely to be a significant factor in their concern about privacy. Thus, the following \nhypothesis is proposed:  \nH1: Intimacy with AI agents will be negatively associated with users’ privacy concerns.  \nPara-social interaction is a unique and intriguing psychological state that may arise from users’ interactions with \nAI agents. The concept originally referred to a media user’s reaction to a media performer, whereby the media \nuser perceives the performer as an intimate conversational partner. Users perceive PSI as an intimate reciprocal \nsocial interaction, despite knowing it is only an illusion (Dibble et al., 2016). The media user may respond to the \ncharacter “similarly to how they feel, think and behave in [a] real-life encounter” (Klimmt et al., 2006). With \ntraditional media, the media performer might typically be a character (persona) within a media narrative, or a \nmedia figure such as a radio personality. Emerging interactive media have enabled more intense PSI experiences \nfor users through features that evoke a sense of interaction, while the parameters of such interactions have been \nextended by AI agents. Several distinctive features of AI agents, including names (e.g., Alexa for Echo; Eskine & \nLocander, 2014), vocal qualities (Schroeder & Epley, 2016), word frequency (Lortie & Guitton, 2011), and \nresponsiveness (Schuetzler et al., 2019), evoke particularly strong anthropomorphic responses among users, upon \nwhich they are able to build PSI.  \nThe possibility that users could form a particular relationship with AI agents due to new anthropomorphic features \nhas started to attract scholarly attention. For instance, Han and Yang (2018) argue that users could form para-\nsocial relationships with their chatbots, using names, human-like verbal reactions, and personality, all of which \nare important antecedents of satisfaction and continuance intentions. Moreover, they report the negative \ninfluence of users’ perceived privacy risks on PSI with AI agents (Han & Yang, 2018). That is, users’ PSI experiences \nare weaker when the privacy risk is considered to be greater. Accordingly, PSI appears to be a primary \npsychological response that is affected by AI agents’ features while simultaneously resulting in user satisfaction. \nAccordingly, we propose the following hypothesis:  \nH2: Para-social interaction with AI agents will be negatively associated with users’ privacy concerns.  \nSocial presence has been described as a major psychological effect brought about by human-agent \ncommunication. In short, social presence is a psychological state in which an individual perceives themself to exist \nwithin an interpersonal environment (Bailenson et al., 2002). A number of studies have attempted to identify agent \nfeatures as important for altering the extent of the social presence experience, primarily because social presence \nis demonstrated to be a key predictor of favorable attitudes and behaviors by affecting emotional closeness and \nsocial connectedness (Go & Sundar, 2019). Greater emotional closeness and social connectedness will naturally \nlead to a positive evaluation of the AI agents; therefore, many studies focus on the predictors and consequences \nof social presence (e.g., Go & Sundar, 2019; N. Lee & Kwon, 2013; Sundar et al., 2015). \nIn particular, a high level of contingency in the exchange of messages with an AI agent is postulated to increase \nthe user’s social presence. Sundar et al. (2015) found that higher message interactivity in a chat context heightens \na feeling of the other’s presence. Similarly, Go and Sundar (2019) discovered that the ability of a chatbot to deliver \ncontingent and interactive messages compensated for its impersonal nature. This suggests the experience of \nsocial presence is dictated not only by technological features but also by the dynamic nature of message exchange. \nIt can be inferred that both the quantitative and qualitative aspects of information exchange are deeply linked to \nthe experience of social presence. In this context, the experience of social presence can be assumed to be more \nimportant for the self-disclosure of sensitive and emotional information, such as feelings that are more intimate, \nthan for the disclosure of factual information such as names (Taddicken, 2014). In this study, it is suggested that \nusers’ experience of social presence should play a key role in the user’s sensitivity to disclosing their private \ninformation after using an AI agent with different motivations. This leads to the following hypothesis: \nH3: The social presence of AI agents will be negatively associated with users’ privacy concerns.  \nAI Agent Users’ Motivations \nAn AI agent is used to gratify various user needs, with the user’s motivation accounting for any ensuing \npsychological reaction. While little research has investigated AI-agent users’ motivations, Cho et al. (2019) broadly \ncategorized the nature of virtual assistant task types into utilitarian and hedonic, depending on the topics that \narise in the interaction. Hijjawi et al. (2016) suggested that conversations with a system typically consist of a \ncombination of question and non-question statements that rely on factors such as topic or context (for example, \n \ninformational vs. entertainment use). Brandtzæg and Følstad (2018) classified AI-agent users’ motivations into \n“productivity” (effectiveness/efficiency) in conducting tasks such as access to specific content, “entertainment and \nsocial experiences,” and “novelty.” Their results revealed that the majority of users seek either assistance or \ninformation to enhance the effectiveness or efficiency of their productivity, whereas others do so for fun \n(Brandtzæg & Følstad, 2018). The novelty aspect emphasizes an interest in new media technologies. Although AI \nagents generally serve a wider range of purposes for their users, researchers have not sufficiently investigated the \nrole that user motivation plays when using AI agents. The aim of this study is to provide a more comprehensive \nand generalizable catalog of user motivations to enhance understanding of the various psychological effects \narising from use of AI agents.  \nAt the same time, motivations that underlie the sharing of personal information should resonate with the privacy \ncalculus in human-computer interaction as well as interpersonal communication. Social media research provides \nevidence that supports the possibility of a meaningful association between users’ motivations and privacy \nconcerns (Hallam & Zanella, 2017; Heravi et al., 2018). For example, Slater (2007) found that the relationship \nbetween social gratification and information self-disclosure is reciprocal, following a reinforcing spiral process. \nWhen a user is hedonically motivated (rather than functionally motivated), the meaning and benefit gained from \nself-disclosure become widely different. Taddicken (2014) also illustrated the possible role played by user \nmotivation on the association between relational experiences and privacy concerns. Therefore, those who have \ndifferent motivations for using AI agents likely have varying levels of privacy concerns. This leads to the following \nresearch question and subsequent hypothesis: \nRQ1: What is the motivation for using AI agents? \nH4: Different types of motivation for using AI agents will significantly moderate the relationship between factors \nrelated to privacy concerns and users’ privacy concerns.  \nH4a: All motivations for using AI agents will significantly moderate the relationship between users’ intimacy and \nprivacy concerns.  \nH4b: All motivations for using AI agents will significantly moderate the relationship between users’ PSI and privacy \nconcerns.  \nH4c: All motivations for using AI agents will significantly moderate the relationship between users’ social presence \nand privacy concerns.  \nMethods \nProcedure and Sample \nFocus Group Interviews \nAs mentioned in the literature review, studies concerning AI agents have recently increased, although there is still \na lack of studies on motivation for their use. Accordingly, in this study, development of the questionnaire entailed \nconducting focus group interviews (FGIs) about AI agent motivations, which was then enriched by literature about \nAI agent motivation. Thus, the FGIs enabled us to develop items for measuring the motives of AI agents. We \nselected FGIs because of the potential for participants to interact and generate ideas beyond what each individual \ncan contribute (Carey & Asbury, 2016). Furthermore, it is possible to observe a significant amount of interaction \non a particular topic within a limited time. We conducted FGIs according to Weller's (1998) “structured interviewing \nand questionnaire construction.” Focus group interviews were conducted by each group from March 6 to 9, 2020. \nThe Interviewees \nThe recruitment of interview respondents1 was supported by a marketing agency. Interviewees comprised 36 \nusers (male and female) aged from 17 to 50 years with experience using AI agents.  \nThere were 16 participants who had used AI agents over a long period (at least six months). Because of the \nmultifunctionality of AI agents, it was considered highly likely that various motives for using AI agents would be \nrevealed. Accordingly, it was considered that participants with longer AI agent experience might mention various \nmotives for use. \n \nTable 1. Socio-Demographic Information of Interviewees—Extended Use (N = 16). \nAge Gender AI agent usage period \nMales Females Six months to less than a year More than a year \nUnder 19 1 4 3 2 \n20–30 4 2 5 1 \n31–40 2 2 1 3 \n41–50 0 1 1 0 \nTotal 7 9 10 6 \nThe Interview Process \nAs Table 1 shows, participants were divided into four groups according to age as follows: teens (n = 5), 20–30 \n(n = 6), 31–40 (n = 4), and 41–50 (n = 1). Interviews were conducted for approximately 60–70 mins (refreshments \nwere provided to create a comfortable atmosphere in the seminar room). During the FGI, we acted as a moderator, \nposing questions and adding prompts with follow-up questions.  \nThe FGI consisted of two stages. In the first stage, a “free listing” interview was conducted to enable interviewees \nto write freely why they used AI agents. During this stage, each of the 16 interviewees identified 10 motivations \n(i.e., items) for using AI agents based on their own experiences. Of the AI agent items (160 = 16 interviewees × 10 \nitems) collected through pre-listing, 42 were mentioned by at least two interviewees, which were then selected for \nthe next stage. This free listing helps to pose key questions in the second stage of the interview (Krueger & Casey, \n2014).  \nDuring the second stage2, the researchers explained the 42 items one-by-one to each group of interviewees in \nturn and counted the items corresponding to interviewees’ own motivation for AI agent use. Then, to exhaustively \nand exclusively elicit the items, the items were selected in the order of motivation for using AI agents with the least \nmention in each group. As a result, a total of 15 items for using AI agents were selected in Group A, and a total of \n13 items were selected in Group B. However, those considered problematic3 were excluded based on the \njudgment of the researchers (for example, I have an AI agent at home, so I am just using it). After qualitatively refining \ninterviewees’ items, we finally elicited 20 items from which the questionnaire could be developed to examine why \nusers employ AI agents.  \nMeasures  \nOnline survey recruited from a professional research company was conducted with 562 respondents in Korea in \nMay 4 to 29 of 2020. According to age and gender proportions of Korea, an invitation email was randomly sent to \nan online panel of the research company.  \nThe survey was conducted to investigate individuals’ motivations for using AI agents and the factors related to \nprivacy concerns while using AI agents. We focused on three measures: (1) PSI, social presence, and intimacy as \nindependent variables, (2) variables for motivation for using AI agents, and (3) AI agent users’ privacy concerns. \nThe survey took approximately 35−45 minutes. \nIndependent Variables4  \nHorton and Wohl (1956) defined PSI as a “simulacrum of conversational give-and-take” (p. 215) that users \nexperience in response to a media performer (the “persona”) in a media exposure situation. Here, PSIs—adopted \nand modified from Dibble and colleagues (2016)—were measured with eight items on a 7-point Likert scale (1 = \nstrongly disagree; 7 = strongly agree; Cronbach α = .88). The original PSI Scale (Dibble et al., 2016) was oriented to \ncapture reactions toward “Amy” (a romantic partner for the participants in Dibble et al.’s study). Therefore, the \ncurrent study adjusted the wording to reflect conversation with AI agents used for this study. The measurement \nincluded items such as, I talk to an AI agent like a friend, I like to talk to an AI agent, and I like hearing the voice of an \nAI agent. \nShort et al. (1976) defined social presence as “the degree of salience of the other person in the interaction and the \nconsequent salience of the interpersonal relationship” (p. 65). Social presence, adopted from Nowak and Biocca \n \n(2003), was measured with eight items and modified on a 7-point Likert scale (1 = strongly disagree; 7 = strongly \nagree; Cronbach α = .81). The measurement included items such as, An AI agent seems to accelerate interaction and \nI feel warm interacting with an AI agent. Nowak and Biocca (2003) verified the social presence implemented when \nan imagined partner and a respondent interact in a virtual environment through experiments. Thus, this study \nrevised the words and expressions of the items by assuming a conversation situation with AI agents (rather than \na virtual partner) in a survey situation rather than an experimental setting. \nHinde (1978) defines intimacy as “the number of different facets of the personality which are revealed to the \npartner and to what depth” (p. 378). Intimacy, adopted from Berschied et al. (1989), was measured using five items \non a 7-point Likert scale (1 = strongly disagree; 7 = strongly agree; Cronbach α = .94). Items included, I feel emotionally \nclose to an AI agent, An AI agent uses supportive statements to build favor with me, and I develop a sense of familiarity \nwith an AI agent. Because Berschied et al.’s (1989) definition of intimacy is formed from romantic, friend, and family \nrelationships, we modified the words and expressions in this study so that their intimacy scale could be used to \nmeasure the intimacy formed between AI agents and users. \nAI Agent Motivation Variables \nFor the moderated variable, motivations for using an AI agent were developed through FGIs and measured with \n20 items (see Table 3) on a 7-point Likert scale (1 = strongly disagree; 7 = strongly agree). Respondents were asked \nquestions about their motivation for using AI agents; for example, To get to know the latest issues in our society, To \nhave a conversation with an AI agent, To use an audio book service, and To manage the schedule.  \nPrivacy Concerns  \nFor the dependent variable, privacy concerns were measured and adapted to the current study from Mehta et al. \n(2015) with four items modified on a 7-point Likert scale (1 = strongly disagree; 7 = strongly agree; Cronbach α = .82). \nThese items included, While using AI agents, an AI agent seems to have access to my personal information, While using \nan AI agent, I am uncomfortable that my personal information will be leaked, Using an AI agent increases the possibility \nof infringement of personal information, and While using AI agents, I think that AI agent system manufacturers can know \nmy personal information.  \nResults \nHypotheses Tests \nTable 2 presents the results for the linear regression analysis performed to examine the potential relationships \nbetween the measured variables. The regression model predicted AI agent users’ privacy concerns that result from \nusing AI agents as a function of the socio-demographic variables and AI technology usage (Block 1), the motivations \nfor using AI agents (Block 2), the independent variables (Block 3), and the moderating effects between the \nmotivation variables for using AI agents and independent variables on predicting privacy concerns (Block 4).  \nWith regard to H1 (Block 3, Table 2), two of the independent variables (intimacy, B = 0.11, SE = 0.05, p = .046, PSI, \nB = −0.012, SE = 0.06, p = .063, and social presence, B = −0.18, SE = 0.06, p = .007 were significantly associated with \nprivacy concerns when the demographic variables—daily AI technology use (Block 1 of Table 2) and the \nmotivations for using AI agents (Block 2, Table 2)—were controlled, which was consistent with our first hypothesis. \nIn other words, when AI agent users feel close to AI agents, they are more concerned about exposure to privacy. \nConversely, when they perceived that AI agents had a high social presence, concerns about privacy exposure \ndecreased relatively. Therefore, H1 and H2 were supported.  \nRegarding H4 (Block 4, Table 2), simple slope analysis revealed significant interaction effects between some of the \nindependent variables and the motivation for using AI agents on AI agent users’ privacy concerns. As shown in \nTable 2, there was a significant interaction effect of intimacy × entertainment motivation (B = 0.10, SE = 0.04, \np = .004, Fig. 1). Thus, it can be inferred that the more AI agent users use AI agents to satisfy their entertainment \nneeds, the more they feel intimate with AI agents, and the more hesitant they are to risk their privacy.  \n \n \nTable 2. Results of Ordinary Least Squares Regressions Predicting Privacy Concern (N = 562). \nVariables B (SE) p \nBlock 1. Controls   \n (Intercept) 4.14 (0.22) <.001 \nSocio-demographic   \nGender (male = 1) −0.05 (0.08) .252 \nAge 0.01 (0.00) .519 \nEducation 0.13 (0.05) .009 \nDaily AI use   \nTime on AI usage  −0.07 (0.05) .159 \nR2 .01 .062 \nBlock 2. Motivations   \nEM 0.19 (0.04) <.001 \nIM 0.19 (0.04) <.001 \nPT 0.01 (0.03) .326 \nΔR2 .01 .034 \nBlock 3. Independent variables    \nIntimacy 0.11 (0.05) .046 \nPara-social interaction −0.01 (0.06) .283 \nSocial presence −0.18 (0.06) .007 \nΔR2 .02 .044 \nBlock 4. Independent variables × Moderator   \nIntimacy × EM 0.10 (0.04) .004 \nPara-social interaction ×EM 0.16 (0.07) <.001 \nSocial presence × EM −0.18 (0.05) <.001 \nIntimacy × IM −0.25 (0.05) <.001 \nPara-social interaction × IM −0.04 (0.05) .261 \nSocial presence × IM −0.07 (0.05) .445 \nIntimacy × PT 0.15 (0.04) .080 \nPara-social interaction × PT 0.13 (0.04) .249 \nSocial presence × PT −0.05 (0.04) .311 \nΔR2 .10 <.001 \nTotal R2 .135 <.001 \nNote. Coefficients indicate non-standardized OLS regression coefficients; standard errors are given in \nparentheses. EM = Entertainment motivation, IM = Instrumental motivation, PT = Passing time \n \nFigure 1. Moderation Effect of Entertainment Motivation (EM) on the Association  \nBetween Intimacy and AI Agent User’s Privacy Concerns. \n \n \n \n \n \n \n \n \n \n \n44.14.24.34.44.54.6Low Mod HighPrivacy ConcernIntimacyLow EMModerator EMHigh EM\n \nThe results also demonstrated a significant interaction effect of PSI × entertainment motivation (B = 0.16, SE = 0.07, \np < .001, Fig. 2). When AI agent users employ AI agents to satisfy their entertainment needs, the more they talk \nwith or think about AI agents in a human-like way, and the more sensitive they may become to privacy exposure. \nFigure 2. Moderation Effect of Entertainment Motivation (EM) on the Association  \nBetween Parasocial Interaction and AI Agent User’s Privacy Concerns. \n \nMeanwhile, the result suggested an interaction effect of social presence × entertainment motivation (B = −0.18, \nSE = 0.05, p < .001, Fig. 3). As AI agent users use AI agents to satisfy their amusement needs, the more often AI \nagent users perceive themselves as actually interacting with AI agents, the lower concerns about privacy exposure \ncan be. \nFigure 3. Moderation Effect of Entertainment Motivation (EM) on the Association  \nBetween Social Presence and AI Agent User’s Privacy Concerns. \n \n \nFor AI agent users with high instrumental motivation, the results show that there was a significant interaction \neffect of intimacy × instrumental motivation (B = −0.25, SE = 0.05, p < .001, Fig. 4). This suggests that the greater \nthe intimacy AI agent users feel toward AI agents when using them functionally, the fewer concerns they have \nabout their privacy. \nFinally, the findings indicated there was a marginally significant interaction effect of intimacy × passing time \n(B = 0.15, SE = 0.04, p = .080, Fig. 5). As AI agent users use AI agents to spend time, the more intimate they feel with \nthe AI agent, and the greater the concern about privacy exposure. \nTaken together, the results of our analyses partially support H4a, H4b, and H4c. \n \n3.83.944.14.24.34.44.54.6Low Mod HighPrivacy ConcernParasocial InteractionLow EMModerator EMHigh EM\n44.14.24.34.44.54.64.7Low Mod HighPrivacy ConcernPresenceLow EMModerator EMHigh EM\n \n \nFigure 4. Moderation Effect of Instrumental Motivation (IM) on the Association  \nBetween Intimacy and AI Agent User’s Privacy Concerns. \n  \nFigure 5. Moderation Effect of Passing Time (PT) on the Association  \nBetween Intimacy and AI Agent User’s Privacy Concerns. \n \nTesting the Research Question \nBecause there is little previous research that explores the motivations for using AI agents, we conducted FGIs and \nexplored the 20 items related to individuals’ motivation for using AI agents. \nTo address our research question (RQ1), we performed exploratory factor analysis (EFA) to investigate the distinct \nmotivations for AI agents. As shown in Table 3, a three-factor solution was identified, and five of the 20 items were \nremoved according to the .40–.30–.20 rule of Howard’s factor loading cutoff (Howard, 2016). These factors were \nlabeled as follows: entertainment motivation (EM: seven items), instrumental motivation (IM: six items), and \npassing time (PT: two items). The terms and definitions were then adapted from previous studies (O’Brien, 2010; \nRubin & Step, 2000; Schwartz & Wrzesniewski, 2016) to name and define these motivations in the context of \ncomputer-human interaction. These three factors accounted for 59.35% of the variance and exhibited strong \nreliability (i.e., the Cronbach's alpha and Pearson's r values were greater than 0.85 and 0.60, respectively), which \nindicates good face validity of the measurement items. As shown in Table 4, there was significant correlation \nbetween the factors, suggesting that such motivations are complementary rather than mutually exclusive. \n \n4.254.34.354.44.454.5High Mod LowPrivacy ConcernIntimacyLow IMModerator IMHigh IM\n44.14.24.34.44.54.6Low Mod HighPrivacy ConcernIntimacyLow PTModerator PTHigh PT\n \nTable 3. Factor Analysis Results for AI Agent Motivations (N = 562). \nQuestionnaire items Factors \nEM IM PT M SD \nTo get to know the latest issues in our society .58 .28 .24 3.92 1.56 \nTo order food .77 .22 .12 3.47 1.63 \nTo enjoy the game .75 .13 .16 3.32 1.63 \nTo read a book .76 .25 .13 3.46 1.62 \nTo have a conversation with a speaker .62 .11 .40 3.50 1.66 \nTo purchase goods .82 .20 .05 3.38 1.64 \nTo use audio book service .69 .11 .07 3.22 1.82 \nTo search the Internet .14 .57 .34 4.90 1.37 \nTo manage the schedule .23 .65 .22 4.71 1.57 \nTo listen to music .05 .77 .21 5.02 1.49 \nTo check the time. .21 .74 .18 4.63 1.62 \nTo connect to music services .23 .75 .16 4.70 1.65 \nTo check the weather forecast .19 .79 .10 4.84 1.65 \nTo rest alone .23 .27 .79 4.14 1.53 \nTo spend time .15 .29 .83 4.50 1.45 \nEigenvalue 5.51 4.40 1.96   \n% Variance explained 27.56 22.01 9.78   \nCronbach’s α .895 .862    \nPearson’s α   .673  \n(p < .001)   \nNote. Listed 16 items on the table indicate only clearly differentiated items. Four items were excluded. EM: Entertainment \nmotivation, IM: Instrumental motivation, PT: Passing time.  \n \n \n \nTable 4. Zero-Order Correlations Among Key Study Variables. \n  1 2 3 4 5 6 7 8 9 10 11 \n1. \nGender 1           \n2. Edu .070 1          \n3. Age .041 .095 * 1         \n4. AU .022 .029 .020 1        \n5. EM .134 ** .035 .156 ** .112 ** 1       \n6. IM −.006 .032 .080 .251 ** .499 ** 1      \n7. PT .043 −.041 .124 ** .144 ** .443 ** .525 ** 1     \n8. PSI .077 −.029 .174 ** .167 ** .578 ** .465 ** .457 ** 1    \n9. PN −.012 −.109 ** .173 ** .094 * .292 ** .035 .101 * .470 ** 1   \n10. IT .048 −.065 .201 ** .129 ** .496 ** .220 ** .273 ** .646 ** .713 ** 1  \n11. PC −.033 .083 * .017 −.050 .083 * .183 ** .046 .033 −.058 * .048 * 1 \nMean .49 2.84 39.9 1.85 3.46 4.80 4.32 4.18 4.03 3.63 4.39 \nSD .50 .76 10.56 .93 1.25 1.20 1.37 1.03 .96 1.33 .94 \nNote. Cell entries are two-tailed Pearson’s correlation coefficients. AU = AI usage, EM = Entertainment motivation A, \nIM = Instrumental motivation, PT = Passing time, PSI = Para-social interaction, PN = Presence, IT = Intimacy, PC = Privacy \nconcerns. *p < .05, **p < .01. N = 562. \n \n \n \nDiscussion \nThe findings of this study propose that psychological aspects of close relationships (including intimacy, PSI and \nsocial presence) offers valuable insights into understanding the relationship between AI agents and individuals’ \nprivacy concerns in the newly emerging communication environment. The first aim of the study was to determine \nthe motivations for AI agent use. The EFA results show that individuals used AI agents primarily to gratify three \nneeds: EM, IM, and PT. The findings indicate that individuals mainly use AI agents to satisfy their various \nrecreational needs (EM), to add efficiency to their own tasks related to their daily life (IM), and to rest (PT). In \nparticular, the EFA results suggest that IM is well suited to the concept of perceived usefulness derived from the \ntechnology acceptance model (TAM; Davis, 1989), given that individuals believe AI agent use could enhance their \nordinary task performance. \nThe second aim of the study was to examine whether intimacy, PSI, and social presence are significantly associated \nwith AI agent users’ privacy concerns. Social presence was found to negatively predict privacy concerns. This \nmeans that individuals who feel a greater social presence with AI agents are less likely to worry about their privacy \nwhile disclosing their personal information to AI agents. Early social presence research (Go & Sundar, 2019; \nTaddicken, 2014) postulated that the experience of social presence affects not only the self-disclosure of \nindividuals’ sensitive and emotional information but also the self-disclosure of factual information. This aligns with \nLutz and Tamò-Larrieux’s (2021) focus on social influence to illuminate the nature of privacy concerns among AI \nusers. Similarly, the significant association between social presence and privacy concerns confirmed in this study \npotentially suggests that an explanatory mechanism exists for the association between social presence made by \nAI agents and self-disclosure of private information. These results corroborate the CASA paradigm that suggests \ngreater social presence simulated by AI-human interaction effectively mitigates user concerns over the security of \ncommunication.  \nAnother intriguing result is that a higher level of intimacy was positively associated with users’ privacy concerns. \nAccording to disclosure intimacy processing (Ho et al., 2018; Lucas et al., 2014), computerized agents reduce \nimpression management; consequently, individuals should experience a greater level of disclosure intimacy. \nHowever, the study outcome shows that this might not conform to the process by which intimacy influences \nindividuals’ privacy concerns. One possible explanation is that AI agent users may feel alleviated from the \npossibility of negative evaluations by AI agents, although they may have concerns that their private information \nmay be transmitted through the computerized system. Even if this assumption is true, we should consider why \nusers are less concerned about their privacy when social presence is greater. The answer may be found in the \ndifference across the types of personal information associated with social presence and the types of personal \ninformation associated with intimacy: it is possible that the information associated with intimacy can be more \nsensitive than the information associated with social presence. Taken together, not all intimacy factors reduce \nprivacy concerns: intimacy might increase privacy concerns depending on the type of information. These results \nsupport Gambino et al.’s (2020) suggestion that CASA theory has to expand to embrace the complication stemming \nfrom the user’s development of more specified scripts for agent-interaction following growing exposure to and \nfamiliarity with media agents. Recent research by Ha et al. (2021) reports that user privacy concerns were \nsignificantly affected by the sensitivity of personal information along with the type of intelligent virtual assistant \n(IVA).  \nWe offer an attempt to investigate the moderation effects of the types of motivations between intimacy, PSI, and \nsocial presence on AI agent users’ privacy concerns. The findings confirm that users who experience greater \npersonification of their AI agents are more likely to bear greater privacy concerns when their primary motivation \nis entertainment (Fig. 2). This result is somewhat paradoxical, because PSI is induced through the self-disclosure \nof information, which includes personal information. One possible explanation is that users with higher PSI levels \nare likely to regard an AI agent as a personable communication partner rather than only a media platform. That \nis, inducing anthropomorphic responses may serve as a double-edged sword when the agent is perceived to be \nhuman enough to evaluate and judge a user’s impressions.  \nConsidering these results, the negative link between social presence and privacy concerns seems logical due to \nthe reduced gap between high and low instrumental motivation for AI agent use, as illustrated by Fig. 3. These \nresults indicate that AI agent users might be less vigilant with their personal information once they experience \ngreater social presence through AI agent use. This finding aligns with the previous research demonstrating that \nthe experience of social presence can be even more important for self-disclosure of more intimate, sensitive, and \n \nemotional information (Taddicken, 2014). This also implies that AI agent users may grow more insensitive to their \nprivacy concerns when experiencing a great level of social presence, even if they use AI agents as just an \ninstrumental device. This is comparable to Ha et al. (2021) results that show the role of IVAs (as either partner or \nservant) were significantly associated with the level of the user’s privacy concerns.  \nAnother remarkable finding of the current study is that intimacy results present differently based on which \nmotivations moderate the relationship between intimacy and privacy concerns. A higher level of intimacy is \npositively associated with privacy concerns when AI agents are used for entertainment motivations. However, \nheightened intimacy is negatively associated with privacy concerns for instrumental motivation. The user may feel \nmore liberal in revealing their private information while forming intimacy with an AI agent when the user operates \nan AI agent as a useful device serving various utilities in their daily life. These results imply that the motivations \nthat drive AI agent use are important because those motivations influence the user’s valuation of personal \ninformation.  \nConclusions \nGuided by the CASA theory and individual differences in psychology concerning close relationships, we have \nidentified key factors predicting privacy concerns in the context of AI agent use. We found that social presence \nwith AI agents is negatively associated with privacy concern. The findings of this study empirically support this \ndiscourse on privacy concern. Conversely, intimacy with AI agents is positively associated with users’ privacy \nconcerns. This inconsistency in the findings of the current study suggests that some factors in the psychology of \nclose relationships play a more central role in evoking privacy concern. That is, individuals’ “human intimacy” with \ntechnical objects such as AI agents can render them more sensitive to privacy than “technical presence.” \nWe also found that it is not only the factors related privacy concern but also the combination of motivations for AI \nagent use that lead individuals to differently perceive privacy concerns. According to the current study, a higher \nlevel of intimacy is positively associated with privacy concerns when AI agents are used for entertainment \nmotivations. AI agents are mainly installed on smart speakers and mobile phones. We use these devices for our \nown enjoyment in a variety of ways. Even if individuals enjoy AI technology, we hope that their secrets will not be \nrevealed through its use. \nFootnotes \n1 Demographic information of the interviewees is presented in Table A2 of Appendix. \n2 To facilitate the progress of FGIs, we divided the interviewees into two groups according to age: one group under \nthe age of 30 (Group A: n = 11) and the other aged 31 and over (Group B: n = 5). \n3 Group A finally selected 12 items after removing three problematic items. In Group B, five problematic items were \nremoved and eight items were selected as appropriate items. \n4 The complete items of independent variables are presented in Table A1 of Appendix. \nConflict of Interest \nThe authors have no conflicts of interest to declare. \nAuthors’ Contribution \nSohye Lim: formal analysis, investigation, project administration, resources, writing-original draft, writing-review \n& editing. Hongjin Shim: conceptualization, data curation, formal analysis, methodology, visualization, validation, \nwriting-original draft, writing-review & editing. \n \n \n \nReferences \nAlepis, E., & Patsakis, C. (2017). Monkey says, monkey does: Security and privacy on voice assistants. IEEE Access, \n5, 17841–17851. https://doi.org/10.1109/ACCESS.2017.2747626 \nBailenson, J. N., Beall, A. C., & Blascovich, J. (2002). Gaze and task performance in shared virtual environments. \nThe Journal of Visualization and Computer Animation, 13(5), 313–320. https://doi.org/10.1002/vis.297  \nBarry, M. (2014). Lexicon: A novel. Penguin Books. \nBerscheid, E., Snyder, M., & Omoto, A. M. (1989). The Relationship Closeness Inventory: Assessing the closeness \nof interpersonal relationships. Journal of Personality and Social Psychology, 57(5), 792–807. \nhttps://doi.org/10.1037/0022-3514.57.5.792 \nBrandtzæg, P. B., & Følstad, A. (2018). Chatbots: Changing user needs and motivations. Interactions, 25(5), 38–43. \nhttps://doi.org/10.1145/3236669 \nCao, C., Zhao, L., & Hu, Y. (2019). Anthropomorphism of Intelligent Personal Assistants (IPAs): Antecedents and \nconsequences. In PACIS 2019 proceedings, Article 187. AIS eLibrary. https://aisel.aisnet.org/pacis2019/187 \nCarey, M. A., & Asbury, J. (2016). Focus group research. Routledge. https://doi.org/10.4324/9781315428376 \nCho, E., Molina, M. D., & Wang, J. (2019). The effects of modality, device, and task differences on perceived \nhuman likeness of voice-activated virtual assistants. Cyberpsychology, Behavior, and Social Networking, 22(8), 515–\n520. https://doi.org/10.1089/cyber.2018.0571 \nChung, H., & Lee, S. (2018). Intelligent virtual assistant knows your life. arXiv. http://arxiv.org/abs/1803.00466 \nDavis, F. D. (1989). Perceived usefulness, perceived ease of use, and user acceptance of information technology. \nMIS Quarterly, 13(3), 319–340. https://doi.org/10.2307/249008 \nDibble, J., Hartmann, T., & Rosaen, S. (2016). Parasocial interaction and parasocial relationship: Conceptual \nclarification and a critical assessment of measures. Human Communication Research, 42(1), 21–44. \nhttps://doi.org/10.1111/hcre.12063  \nElish, M. C., & boyd, d. (2018). Situating methods in the magic of Big Data and AI. Communication Monographs, \n85(1), 57–80. https://doi.org/10.1080/03637751.2017.1375130 \nEskine, K. J., & Locander, W. H. (2014). A name you can trust? Personification effects are influenced by beliefs \nabout company values. Psychology & Marketing, 31(1), 48-53. https://doi.org/10.1002/mar.20674 \nFoehr, J., & Germelmann, C. C. (2020). Alexa, can I trust you? Exploring consumer paths to trust in smart voice-\ninteraction technologies. Journal of the Association for Consumer Research, 5(2), 181–205. \nhttps://doi.org/10.1086/707731 \nGambino, A., Fox, J., & Ratan, R. A. (2020). Building a stronger CASA: Extending the computers are social actors \nparadigm. Human-Machine Communication, 1, 71–86. https://doi.org/10.30658/hmc.1.5 \nGenpact (2017, December 16). Consumers want privacy, better data protection from artificial intelligence, finds new \nGenpact research. https://www.genpact.com/about-us/media/press-releases/2017-consumers-want-privacy-\nbetter-data-protection-from-artificial-intelligence-finds-new-genpact-research   \nGo, E., & Sundar, S. S. (2019) Humanizing chatbots: The effects of visual, identity and conversational cues on \nhumanness perceptions. Computers in Human Behavior, 97, 304–316. https://doi.org/10.1016/j.chb.2019.01.020 \nHa, Q. A., Chen, J. V., Uy, H. U., & Capistrano, E. P. (2021). Exploring the privacy concerns in using intelligent \nvirtual assistants under perspectives of information sensitivity and anthropomorphism. International Journal of \nHuman–Computer Interaction, 37(6), 512-527. https://doi.org/10.1080/10447318.2020.1834728 \nHallam, C., & Zanella, G. (2017). Online self-disclosure: The privacy paradox explained as a temporally discounted \nbalance between concerns and rewards. Computers in Human Behavior, 68, 217–227. \nhttps://doi.org/10.1016/j.chb.2016.11.033 \nHan, S., & Yang, H. (2018). Understanding adoption of intelligent personal assistants: A parasocial relationship \nperspective. Industrial Management and Data Systems, 118(3), 618–636. https://doi.org/10.1108/IMDS-05-2017-\n0214 \n \nHeravi, A., Mubarak, S., & Choo K. (2018). Information privacy in online social networks: Uses and gratification \nperspective. Computers in Human Behavior, 84, 441–459. https://doi.org/10.1016/j.chb.2018.03.016 \nHijjawi, M., Bandar, Z., & Crockett, K. (2016). A general evaluation framework for text based conversational agent. \nInternational Journal of Advanced Computer Science and Applications, 7(3), 23–33. \nhttps://doi.org/10.14569/IJACSA.2016.070304 \nHinde, R. A. (1978). Interpersonal relationships - in quest of a science. Psychological Medicine, 8(3), 373–386. \nhttps://doi.org/10.1017/S0033291700016056  \nHo, A., Hancock, J., & Miner, A. S. (2018). Psychological, relational, and emotional effects of self-disclosure after \nconversations with a chatbot. Journal of Communication, 68(4), 712–733. https://doi.org/10.1093/joc/jqy026  \nHoffmann, L., Krämer, N. C., Lam-Chi, A., & Kopp, S. (2009). Media equation revisited: Do users show polite \nreactions towards an embodied agent? In Z. Ruttkay, M. Kipp, A. Nijholt, & H. H. Vilhjálmsson (Eds.), Intelligent \nvirtual agents (pp. 159–165). Springer. https://doi.org/10.1007/978-3-642-04380-2_19 \nHorton, D., & Wohl, R. R. (1956). Mass communication and para-social interaction: Observations on intimacy at a \ndistance. Psychiatry, 19(3), 215–229. https://doi.org/10.1080/00332747.1956.11023049 \nHoward, M. C. (2016). A review of exploratory factor analysis decisions and overview of current practices: What \nwe are doing and how can we improve? International Journal of Human-Computer Interaction, 32(1), 51–62. \nhttps://doi.org/10.1080/10447318.2015.1087664 \nHuang, Y., Obada-Obieh, B., & Beznosov, K. (2020). Amazon vs. my brother: How users of shared smart speakers \nperceive and cope with privacy risks. In Proceedings of the 2020 CHI conference on human factors in computing \nsystems (pp. 1–13). Association for Computing Machinery. http://doi.org/10.1145/3313831.3376529  \nKim, D., Park, K., Park, Y., & Ahn, J.-H. (2019). Willingness to provide personal information: Perspective of privacy \ncalculus in IoT services. Computers in Human Behavior, 92, 273–281. https://doi.org/10.1016/j.chb.2018.11.022 \nKlimmt, C., Hartmann, T., Schramm, H., Bryant, J., & Vorderer, P. (2006). Parasocial interactions and relationships. \nIn J. Bryant & P. Vorderer (Eds.), Psychology of entertainment (pp. 291–313). Routledge. \nKrueger, R. A. & Casey, M. A. (2014). Focus groups: A practical guide for applied research. Sage Publications. \nLee, S., & Choi, J. (2017). Enhancing user experience with conversational agent for movie recommendation: \nEffects of self-disclosure and reciprocity. International Journal of Human-Computer Studies, 103, 95–105. \nhttps://doi.org/10.1016/j.ijhcs.2017.02.005 \nLee, N., & Kwon, O. (2013). Para-social relationships and continuous use of mobile devices. International Journal \nof Mobile Communication, 11(5), 465–484. https://doi.org/10.1504/IJMC.2013.056956  \nLiao, Y., Vitak, J., Kumar, P., Zimmer, M., & Kritikos, K. (2019). Understanding the role of privacy and trust in \nintelligent personal assistant adoption. In N. G. Taylor, C. Christian-Lamb, M. H. Martin, & B. Nardi (Eds.), \nInformation in contemporary society (pp. 102–113). Springer. https://doi.org/10.1007/978-3-030-15742-5_9 \nLortie, C. L., & Guitton, M. J. (2011). Judgment of the humanness of an interlocutor is in the eye of the beholder. \nPLoS One, 6(9), Article e25085. https://doi.org/10.1371/journal.pone.0025085  \nLucas G. M., Gratch J., King A., & Morency, L.-P. (2014). It’s only a computer: Virtual humans increase willingness \nto disclose. Computers in Human Behavior, 37, 94–100. https://doi.org/10.1016/j.chb.2014.04.043  \nLutz, C., & Newlands, G. (2021). Privacy and smart speakers: A multi-dimensional approach. The Information \nSociety, 37(3), 147–162. https://doi.org/10.1080/01972243.2021.1897914 \nLutz, C., & Tamò-Larrieux, A. (2021). Do privacy concerns about social robots affect use intentions? Evidence from \nan experimental vignette study. Frontiers in Robotics and AI, 8, Article 627958. \nhttps://doi.org/10.3389/frobt.2021.627958 \nMehta, R., Rice, S., Winter, S., Moore, J., & Oyman, K. (2015, April 3). Public perceptions of privacy toward the usage \nof unmanned aerial systems: A valid and reliable instrument [Poster presentation]. The 8th Annual Human Factors \nand Applied Psychology Student Conference, Daytona Beach, FL. https://commons.erau.edu/hfap/hfap-\n2015/posters/39/ \n \nMoorthy, A. E., & Vu, K.-P. (2015). Privacy concerns for use of voice activated personal assistant in the public \nspace. International Journal of Human-Computer Interaction, 31(4), 307–335. \nhttps://doi.org/10.1080/10447318.2014.986642 \nNass, C., & Steuer, J. (1993). Voices, boxes, and sources of messages: Computers and social actors. Human \nCommunication Research, 19(4), 504-527. https://doi.org/10.1111/j.1468-2958.1993.tb00311.x \nNass, C., & Moon, Y. (2000). Machines and mindlessness: Social responses to computers. Journal of Social Issues, \n56(1), 81–103. https://doi.org/10.1111/0022-4537.00153 \nNowak, K. L., & Biocca, F. (2003). The effect of the agency and anthropomorphism on users’ sense of \ntelepresence, co-presence, and social presence in virtual environments. Presence: Teleoperators and Virtual \nEnvironments 12(5), 481–494. https://doi.org/10.1162/105474603322761289 \nO’Brien, H. L. (2010). The influence of hedonic and utilitarian motivations on user engagement: The case of \nonline shopping experiences. Interacting with Computers, 22(5), 344–352. \nhttps://doi.org/10.1016/j.intcom.2010.04.001 \nPark, M., Aiken, M., & Salvador, L. (2019). How do humans interact with chatbots?: An analysis of transcripts. \nInternational Journal of Management & Information Technology, 14, 3338–3350. \nhttps://doi.org/10.24297/ijmit.v14i0.7921 \nReeves, B., & Nass, C. I. (1996). The media equation: How people treat computers, television, and new media like real \npeople. Cambridge University Press. \nRubin, A. M., & Step, M. M. (2000). Impact of motivation, attraction, and parasocial interaction on talk radio \nlistening. Journal of Broadcasting & Electronic Media, 44(4), 635–654. \nhttps://doi.org/10.1207/s15506878jobem4404_7 \nSchroeder, J., & Epley, N. (2016). Mistaking minds and machines: How speech affects dehumanization and \nanthropomorphism. Journal of Experimental Psychology: General, 145(11), 1427–1437. \nhttps://doi.org/10.1037/xge0000214 \nSchuetzler, R. M., Grimes, G. M., & Giborney, J. S. (2019). The effect of conversational agent skill on user behavior \nduring deception. Computers in Human Behavior, 97, 250–259. https://doi.org/10.1016/j.chb.2019.03.033 \nSchwartz, B., & Wrzesniewski, A. (2016). Internal motivation, instrumental motivation, and eudaimonia. In \nJ. Vittersø (Ed.), Handbook of eudaimonic well-being (pp. 123–134). Springer. https://doi.org/10.1007/978-3-319-\n42445-3_8 \nShort, J., Williams, E., & Christie, B. (1976). The social psychology of telecommunications. Wiley. \nSlater, M. D. (2007). Reinforcing spirals: The mutual influence of media selectivity and media effects and their \nimpact on individual behavior and social identity. Communication Theory, 17(3), 281–303. \nhttps://doi.org/10.1111/j.1468-2885.2007.00296.x \nSmith, H. J., Dinev, T., & Xu, H. (2011). Information privacy research: An interdisciplinary review. MIS Quarterly, \n35(4), 989–1015. https://doi.org/10.2307/41409970 \nSundar, S. S., Jia, H., Waddell, T. F., & Huang, Y. (2015). Toward a theory of interactive media effects (TIME): Four \nmodels for explaining how interface features affect user psychology. In S. S. Sundar (Ed.), The handbook of the \npsychology of communication technology (pp. 47–86). Wiley-Blackwell. \nhttp://dx.doi.org/10.1002/9781118426456.ch3 \nTaddicken, M. (2014). The ‘privacy paradox’ in the social web: The impact of privacy concerns, individual \ncharacteristics, and the perceived social relevance on different forms of self-disclosure. Journal of Computer-\nMediated Communication, 19(2), 248–273. https://doi.org/10.1111/jcc4.12052 \nWeller, S. C. (1998). Structured interviewing and questionnaire construction. In H. R. Bernard (Ed.), Handbook of \nmethods in cultural anthropology (pp. 365–409). AltaMira Press. \n  \n \nAppendix \nTable A1. Independent Variable Items—Parasocial Interaction, Social Presence and Intimacy. \nVariables Items Scales \nParasocial \nInteraction \n1. I talk to an AI agent like a friend. \n1 = strongly disagree  \n7 = strongly agree \n2. I like to talk to an AI agent. \n3. I enjoy interacting with an AI agent. \n4. I like hearing the voice of an AI agent. \n5. I see an AI agent as a natural.  \n6. I find an AI agent to be attractive. \n7. An AI agent makes me feel comfortable. \n8. An AI agent keeps me company while an AI agent is working. \nSocial \nPresence \n1. An AI agent seems to accelerate interaction. \n1 = strongly disagree  \n7 = strongly agree \n2. I feel warm interacting with an AI agent. \n3. I want a deeper relationship with an AI agent. \n4. I am willing to share personal information with an AI agent. \n5. I want to make the conversation more intimate. \n6. I am interested in talking to an AI agent. \n7. I am able to assess an AI agent’s reaction to what I say. \n8. I feel like I’m having a face-to-face meeting while I am talking to an AI agent.  \nIntimacy \n1. I feel emotionally close to an AI agent. \n1 = strongly disagree  \n7 = strongly agree \n2. I develop a sense of familiarity with an AI agent. \n3. An AI agent uses supportive statements to build favor with me.  \n4. An AI agent influences how I spend my free time. \n5. An AI agent influences my moods. \nTable A2. Socio-Demographic Information of Interviewees (N = 36). \nAge Gender AI agent usage period \nMales Females Not more than \na month \nTwo months \nto six months \nor less \nSix months \nto less than \na year \nMore than \na year \nUnder 19 4 6 2 3 3 2 \n20–30 5 5 1 3 5 1 \n31–40 4 6 1 5 3 1 \n41–50 2 4 1 4 1 0 \n \n© Author(s). The articles in Cyberpsychology: Journal of Psychosocial Research on Cyberspace are open access \narticles licensed under the terms of the Creative Commons BY-NC-ND 4.0 International License which permits \nunrestricted, non-commercial use, distribution and reproduction in any medium, provided the work is properly \ncited. \nCyberpsychology: Journal of Psychosocial Research on Cyberspace (https://cyberpsychology.eu/) \nISSN: 1802-7962 | Faculty of Social Studies, Masaryk University \n \nAbout Authors \nDr. Sohye Lim is a professor in the School of Communication and Media at Ewha Womans University, Korea. Her \nresearch interests include media users’ psychological responses to various emerging media technologies and A.I. \nmediated communication. \nDr. Hongjin Shim is a research fellow in Center for AI and Social Policy at the Korea Information Society \nDevelopment Institute. His research interest covers new communication technology, media psychology and media \neffect. \n✉ Correspondence to \nHongjin Shim, Korea Information Society Development Institute (KISDI), Republic of Korea, hjshim@kisdi.re.kr "
}