{
  "title": "Weakly Supervised Text Classification using Supervision Signals from a Language Model",
  "url": "https://openalex.org/W4287889335",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5101403806",
      "name": "Ziqian Zeng",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5102947707",
      "name": "Weimin Ni",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5046409172",
      "name": "Tianqing Fang",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5100331094",
      "name": "Xiang Li",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A5102892146",
      "name": "Xinran Zhao",
      "affiliations": [
        "Laboratoire d'Informatique de Paris-Nord",
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A5020880385",
      "name": "Yangqiu Song",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4311718268",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W1959608418",
    "https://openalex.org/W4236645963",
    "https://openalex.org/W2120779048",
    "https://openalex.org/W2890931111",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2755637027",
    "https://openalex.org/W2964071174",
    "https://openalex.org/W2556888587",
    "https://openalex.org/W3034588688",
    "https://openalex.org/W2740721704",
    "https://openalex.org/W2467240462",
    "https://openalex.org/W2108281845",
    "https://openalex.org/W2285986798",
    "https://openalex.org/W3106109117",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W3021533447",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2937160891",
    "https://openalex.org/W2951271661",
    "https://openalex.org/W2595304170",
    "https://openalex.org/W2903908313",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1909320841",
    "https://openalex.org/W1996430422",
    "https://openalex.org/W2889577585",
    "https://openalex.org/W2936539382",
    "https://openalex.org/W1493526108",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W4226144573",
    "https://openalex.org/W3120740533",
    "https://openalex.org/W3166913490",
    "https://openalex.org/W2964072618",
    "https://openalex.org/W2970254524",
    "https://openalex.org/W1959399437"
  ],
  "abstract": "Solving text classification in a weakly supervised manner is important for real-world applications where human annotations are scarce. In this paper, we propose to query a masked language model with cloze style prompts to obtain supervision signals. We design a prompt which combines the document itself and \"this article is talking about [MASK].\" A masked language model can generate words for the [MASK] token. The generated words which summarize the content of a document can be utilized as supervision signals. We propose a latent variable model to learn a word distribution learner which associates generated words to pre-defined categories and a document classifier simultaneously without using any annotated data. Evaluation on three datasets, AGNews, 20Newsgroups, and UCINews, shows that our method can outperform baselines by 2%, 4%, and 3%. © Findings of the Association for Computational Linguistics: NAACL 2022 - Findings.",
  "full_text": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 2295 - 2305\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nWeakly Supervised Text Classification using Supervision Signals\nfrom a Language Model\nZiqian Zeng1,2, Weimin Ni1, Tianqing Fang2, Xiang Li3, Xinran Zhao4, and Yangqiu Song2\n1Shien-Ming Wu School of Intelligent Engineering, South China University of Technology, China\n2Department of CSE, HKUST, Hong Kong, China\n3Department of Computer Science, University of Illinois, Urbana Champaign, US\n4Computer Science Department, Stanford University, US\nzqzeng@scut.edu.cn, mewmn@mail.scut.edu.cn, xiangl12@illinois.edu\nxzhaoar@stanford.edu, {tfangaa, yqsong}@cse.ust.hk\nAbstract\nSolving text classification in a weakly super-\nvised manner is important for real-world ap-\nplications where human annotations are scarce.\nIn this paper, we propose to query a masked\nlanguage model with cloze style prompts to ob-\ntain supervision signals. We design a prompt\nwhich combines the document itself and “this\narticle is talking about [MASK].” A masked\nlanguage model can generate words for the\n[MASK] token. The generated words which\nsummarize the content of a document can be\nutilized as supervision signals. We propose a\nlatent variable model to learn a word distribu-\ntion learner which associates generated words\nto pre-defined categories and a document clas-\nsifier simultaneously without using any anno-\ntated data. Evaluation on three datasets, AG-\nNews, 20Newsgroups, and UCINews, shows\nthat our method can outperform baselines by\n2%, 4%, and 3%.\n1 Introduction\nText classification is a fundamental task in Natu-\nral Language Processing (NLP) with diverse real-\nworld applications such as identifying relevant doc-\numents of a case in legal proceedings (Roitblat\net al., 2010), and classifying victim’s requests (e.g.,\nfood, shelter, and medical aids) on social media\nplatforms during earthquakes (Caragea et al., 2011).\nCurrent state-of-the-art text classification methods\n(Zhang et al., 2015; Zhou et al., 2016; Johnson and\nZhang, 2017) still need a large number of anno-\ntated data. However, in the real world, naturally\nannotated data are rare and human annotations are\nexpensive. Solving the text classification task with-\nout using annotated data but exploiting inexpensive\nsupervision signals is worth investigation.\nIn the weakly supervised setting, any annotated\ndocument is not accessible, but inexpensive super-\nvision signals such as label surface names or key-\nwords can be used. Existing weakly supervised text\nclassification methods (Meng et al., 2018, 2019;\nMekala and Shang, 2020; Meng et al., 2020) first\nused seed keywords to retrieve more keywords, and\nthen created pseudo labels for documents and then\ntrain a model in a “standard” supervised learning\nmanner. In previous work, supervision signals are\nrestricted to a small set of keywords from docu-\nments contents.\nRecent work shows that prompts can probe\nknowledge from PLMs (Devlin et al., 2019; Rad-\nford et al., 2019) and the knowledge can provide\nsupervision signals to solve different NLP tasks\nincluding relation extraction (Shin et al., 2020),\nquestion answering (Petroni et al., 2020), and\nsummarization (Radford et al., 2019). For exam-\nple, (Petroni et al., 2019) solved the knowledge\nbase completion task by querying an MLM with a\nprompt “Alan Turing was born in[MASK].” Using\nprompts to generate supervision signals for text\nclassification is worth exploring.\nWe propose to query an MLM with a prompt\nwhich combines the document itself and “this arti-\ncle is talking about [MASK].”, and use generated\nwords for the [MASK] token as supervision sig-\nnals. For example, in Figure 1, given a prompt\n“The radio telescope at arecibo observatory will\nbegin mapping the known galaxy on friday, scien-\ntists said. This article is talking about [MASK].”,\nan MLM predicts “astronomy”, “galaxies”, “ra-\ndio”, “science”, and “galaxy” for the [MASK] to-\nken. These words summarize the topic of the doc-\nument. Hence, they can be used as supervision\nsignals. Besides generating signal words, an intu-\nitive approach to obtain supervision signals is by\nextracting important words from documents. We\nwill compare two types of supervision signals.\nAfter obtaining signal words, we need to asso-\nciate these words to pre-defined categories. We\npropose a latent variable model (WDDC) to learn\na Word Distribution learner and a Document\nClassifier simultaneously without using any anno-\ntated data. A word distribution learner aims to learn\n2295\ns c i e n t i s t h e a r t p(w|c=’Business’)\nMasked LM\n[NAME]\np(w|c)\n. . . \nDocument x\nMasked LM k\n. . . \ns c i e n t i s t s t o c k \nq(C=’Science’|x ) Class 1: Class 2:\nS c i e n c e Top 5 words\nastronomy galaxies radio science galaxy\nThe radio telescope at arecibo observatory will begin mapping the known galaxy on friday, scientists said. This article is talking about [MASK].\na s t r o n o m y s t o c k p(w|c=’Business’)\na s t r o n o m y s t o c k p(w|c=’Science’)\nq(c|x)\np(w|C)p(C)\nB u s i n e s s . . . \nWord Distribution Learner Document Classifier\nC a t e g o r y : \nW D D C \nFigure 1: We combine a document and a cloze style sentence “This article is talking about [MASK]” to query a\nmasked LM. It generates a set of words for the [MASK] token. These words are likely to summarize the topic of a\ndocument. After obtaining words such as “astronomy” and “galaxies”, human beings can easily infer that this article\nis talking science rather than business because we know these words are frequently used in science topic. Word\ndistributions given pre-defined categories bridge supervision signals (generated words) and our goal (the category\nof a document). The proposed model (WDDC) can learn word distributions given pre-defined categories and a\ndocument classifier simultaneously.\na probability of a generated word wgiven a cate-\ngory c, i.e., p(w|c). A document classifier aims to\nlearn a probability of a categorycgiven a document\nx, i.e., p(c|x). These two goals could be optimized\nsimultaneously via maximizing the log-likelihood\nof generated words by introducing the category as\na latent variable. In our latent variable model, a\nword distribution learner and a document classifier\ncan be parameterized by any neural network.\nOur contributions are summarized as follows,\n•We propose to query an MLM using a prompt\nwhich combines the document and a cloze style\nsentence “this article is talking about [MASK]”.\nWe use generated words for the [MASK] token as\nsupervision signals in the weakly supervised text\nclassification task.\n•We propose a latent variable model (WDDC)\nto learn a word distribution learner which asso-\nciates generated words to pre-defined categories\nand a document classifier simultaneously without\nusing any annotated data.\n•The experimental results show that the pro-\nposed method WDDC can outperform other weakly\nsupervised baselines in three datasets.\nThe code is available at https://github.com/\nHKUST-KnowComp/WDDC.\n2 Related Work\nIn this section, we review the related work on query-\ning an MLM with prompts, weakly supervised text\nclassification, zero-shot text classification, and vari-\national methods.\nQuerying an MLM with Prompts. Querying an\nMLM with cloze style prompts provides a new di-\nrection to solve some NLP tasks in an unsupervised\nmanner. (Petroni et al., 2019) queried an MLM\nusing manually designed prompts to solve a knowl-\nedge base completion task. For example, in order\nto complete the missing entity X in (Alan Turing,\nborn in, X), they designed a prompt “Alan Turing\nwas born in [MASK]” to query an MLM. The gen-\nerated word for the [MASK] token can be directly\nused to complete the missing fact. By querying\nlanguage models, some NLP tasks such as relation\nextraction (Shin et al., 2020), question answering\n(Radford et al., 2019; Petroni et al., 2020), summa-\nrization (Radford et al., 2019) could be solved in an\nunsupervised manner. However, not all NLP tasks\ncan directly use generated words from an MLM in\ndownstream tasks. Some tasks such as sentiment\nanalysis and textual entailment (Shin et al., 2020)\nneed more steps for inference. For example, in the\nsentiment analysis task, (Shin et al., 2020) used an-\nnotated data to train a classifier that links generated\nwords to pre-defined categories. Our work does not\nrequire annotated data for inference.\nWeakly Supervised Text Classification. In the\nweakly supervised text classification task, any la-\nbeled documents are not allowed, but label sur-\n2296\nface names or limited word-level descriptions of\neach category can be used. Dataless (Chang et al.,\n2008; Song and Roth, 2014) used Explicit Se-\nmantic Analysis (ESA) vectors (Gabrilovich et al.,\n2007) to represent label name and documents. Pre-\ndictions are based on the label-document similar-\nity. Recently, (Meng et al., 2018, 2019; Mekala\nand Shang, 2020; Meng et al., 2020; Schick and\nSchütze, 2021; Schick and Schütze, 2021; Zhang\net al., 2022) trained neural text classifiers in an\nweakly supervised manner. They generated pseudo\nlabels for documents to pre-train a neural classi-\nfier and then performed self-training on unlabeled\ndata for model refinement. LOTClass (Meng et al.,\n2020) is relevant to our work because they also used\npre-trained language models. They used a LM to\nretrieve a set of semantically correlated words for\neach class, and then fine-tuned the LM to predict\nthese words. Finally, they performed self-training\non unlabeled data. Our work is different from LOT-\nClass because we obtain supervision signals by\nquerying an MLM with cloze style prompts and\nwe propose a latent variable model to learn docu-\nment classifier rather than using the self-training\nprocedure. PRBOOST (Zhang et al., 2022) is also\nrelevant to our work because they also use prompts\nto generate weak labels. PRBOOST first gener-\nated rules by using a small amount of labeled data,\nthen asked human annotators to select high-quality\nrules to generate week labels. Finally, they trained\na new model in a self-training manner. Our work\nis different from PRBOOST because our method\nassociates predicted words with labels in an unsu-\npervised manner while PRBOOST maps prompting\nbased rules to labels by involving human feedback.\nZero-Shot Text Classification. In zero-shot\nlearning settings, the classes covered by training\ninstances and the classes we aim to classify are dis-\njoint. Zero-shot learning text classification meth-\nods (Xia et al., 2018; Rios and Kavuluru, 2018;\nZhang et al., 2019; Liu et al., 2019) generalized\nseen classes to unseen classes by learning seman-\ntic relationships between classes and documents\nvia embeddings or semantic knowledge sources.\nHowever, zero-shot learning still requires annotated\ndata for the seen classes training. We cannot apply\nzero-shot learning methods to weakly supervised\nsettings where no annotated document is available.\nVariational Methods. Variational autoencoders\n(Kingma and Welling, 2014; Rezende et al., 2014)\nconsists of an encoder and a decoder. The encoder\nestimates posterior probabilities and the decoder es-\ntimates the reconstruction likelihood given a latent\nvariable. The objective function is to maximize the\nreconstruction likelihood of the observed variable.\nThe latent variable in V AEs is continuous variable.\nRecently, many research works (Titov and Khod-\ndam, 2015; Marcheggiani and Titov, 2016; Šuster\net al., 2016; Zhang et al., 2018; Chen et al., 2018;\nZeng et al., 2019; Liang et al., 2019) use V AEs to\nsolve different NLP tasks such as relation discov-\nery, question answering, sentiment classification,\netc. In above works, the latent variables are discrete\nvariables. For example, (Marcheggiani and Titov,\n2016) aimed to solve unsupervised open-domain\nrelation discovery. The objective function is to re-\nconstruct the likelihood of two entities. They intro-\nduced relation as the latent variable. The encoder\nis a relation classifier, which predicts a semantic\nrelation between two entities. The decoder recon-\nstructs entities given the predicted relation. Our\nmethod is also based on V AEs with a discrete la-\ntent variable but the estimated probabilities and the\nobjective function are different.\n3 Methodology\nIn this section, we first introduce how to obtain\nsupervision signals from an MLM and document\nitself, and then we introduce a latent variable model\nto learn a word distribution learner and a document\nclassifier simultaneously.\n3.1 Supervision Signals\n3.1.1 Signal Words\nGiven a document, our goal is to obtain topic rele-\nvant words which are used as supervision signals.\nTo achieve this, we append a cloze style sentence to\nthe document at the end as a prompt. A prompt is\ndesigned as “[CLS] + document + This article is\ntalking about [MASK]. + [SEP].” The [MASK]\ntoken serves as a placeholder for a topic relevant\nword which can summarize the document. It mim-\nics the reading comprehension task which is using\na word to summarize the content of a document.\nWe select top k generated words as supervision\nsignals.\nInstead of generating signal words, a natural way\nto obtain supervision signals is by extracting words\nfrom the document. To achieve this, we extract\nall nouns and proper nouns in the document using\npart-of-speech tagger (Kristina et al., 2003). Since\n2297\nTable 1: Signal words from an MLM and from the document(Doc).\nText Label Signal Words\nThe world ’s top two players Sports MLM: tennis, thailand, federer, seeds, wimbledon\nroger federer and andy roddick\nreached the semifinals friday Doc: world, players, federer, andy, roddick, semifinals,\nat the thailand open. friday, thailand, open\nThese circuits abound in most Science MLM: circuits, computers, electronics, computing, graphs\nelectronic project books.\nIt has LED indicators also. Doc: circuits, project, books, LED, indicators\nScientists discover a genetic Health MLM: suicide, genetics, cancer, hiv, health\nindicator that could help\nprevent suicides. Doc: scientists, indicator, suicides\nmost of the generated words from an MLM are\nnouns and proper nouns, so we only extract words\nwith two types of part-of-speech.\nTable 1 shows top 5 predictions from an MLM\n(BERT (Devlin et al., 2019)) given prompts and\nextracted nouns and proper nouns from documents.\nFor the first document, an MLM can infer that it\nis talking about a tennis match although “tennis”\ndoes not appear in the document. It also generates\nsome relevant words such as “ wimbledon.” In this\ncase, the MLM is better than extraction. For the\nsecond document, the first word from the MLM\nprecisely summarizes the document. However, the\nMLM also generates a few words which are related\nto computer. Unfortunately computer is also a cat-\negory in this dataset. Compared to the MLM, the\nextracting way is safer in this case. For the third\ndocument, an MLM generates “health” which is\nan exact match of the label surface name although\n“cancer” and “hiv” are not faithful to the original\ndocument. We will evaluate generation and extrac-\ntion methods in the experiment.\n3.1.2 Remove Non-discriminated Words\nWords generated from an MLM are not always cat-\negory discriminated. Non-discriminated words can\nharm the performance of inference. The intuition of\nremoving non-discriminated words is that if some\nwords appear in different categories with similar\nfrequency, then it is possible that these words are\nnot category-discriminated. Since we cannot ac-\ncess labels, the label in the following computation\nmeans the pseudo label. The pseudo label gener-\nation process is shown in section 3.1.3. Inspired\nby category-indicative measurement, (Mekala and\nShang, 2020), we define category-indicative index:\nCII(ci,w) =f(ci,w)\nf(ci) , (1)\nwhere f(ci,w) is the number of occurrences of the\nsignal word win the documents which are labeled\nas ci, and f(ci) is the total number of occurrences\nof all signal words in documents which are labeled\nas ci.\nWe define category-indicative ratio as,\nCIR(w) = CII(ci,w)\nCII(cj,w), (2)\nwhere CII(ci,w) is the maximum value among\nall categories, CII(cj,w) is the second maximum\nvalue all categories. Larger value of CIR(w) in-\ndicates wis more discriminated. If CII(cj,w) is\nequal to 0, we will assign a large value to CIR(w).\nIf CIR(w) <t, we considerwis not discriminated\nand we remove wfrom signal words set.\n3.1.3 Pseudo Label Generation\nWe assign pseudo labels to data based on label-\nword similarity. We represent a word using static\nrepresentation which is introduced by (Mekala and\nShang, 2020). Given a word w, static representa-\ntion SR(w) is computed by averaging the contex-\ntualized embeddings of all its occurrences in the\ncorpus. The label-word similarity is the cosine sim-\nilarity between the static representation of the label\nsurface name and the static representation of signal\nwords. If the label surface name or the supervision\nsignal contains more than one word, we take the\naverage of the static representations of all words.\nWe assign a sample with the pseudo label which\nyields the maximum similarity value among all\nclasses. And the similarity value should be greater\n2298\nthan a threshold γ. Setting a threshold can result in\nmore accurate pseudo label assignments although\nthe size of pseudo labeled data will shrink.\nTo summarize, there are three steps to obtain\nclean signal words: (1) Obtain signal words from\nan MLM or a document. (2) Generate pseudo la-\nbels. (3) Remove signal words which have low\ncategory-indicative ratio values.\n3.2 Model Training\nAfter getting clean signal words, we then propose\na latent variable model to learn a word distribution\nlearner and a document classifier simultaneously.\nSince there is no annotated data available, in\norder to best explain the observed data, i.e., sig-\nnal words, the objective of our model is to maxi-\nmize the log-likelihood of signal words. The ul-\ntimate goal is to identify the category of a docu-\nment, hence, we introduce a latent variable Crep-\nresenting the category, into the objective function.\nFurther, by applying Jensen’s inequality (Jensen\net al., 1906), we can derive an evidence lower\nbound (ELBO) of the log-likelihood. We define\nthe objective function as follows,\nLo =\n∑\nx∈X\n∑\nwr∈Rx\nlog p(wr)\n=\n∑\nx∈X\n∑\nwr∈Rx\nlog\n∑\nc\np(wr,c)\n=\n∑\nx∈X\n∑\nwr∈Rx\nlog\n∑\nc\nq(c|x)\n[p(wr,c)\nq(c|x)\n]\n≥\n∑\nx∈X\n∑\nwr∈Rx\n∑\nc\nq(c|x)\n[\nlog p(wr,c)\nq(c|x)\n]\n=\n∑\nx∈X\n∑\nwr∈Rx\nEq(C|x)\n[\nlog p(wr|c)p(c)\n]\n−\n∑\nx∈X\n∑\nwr∈Rx\nEq(C|x)\n[\nlog q(c|x)\n]\n, (3)\nwhere xis a document, X is a set of documents,\nRx is the set of signal words of document x, wr\nis a signal word, C is a discrete random variable\nrepresenting the category of a document, c is a\npossible value of variable C. For example, ccan\nbe science or business.\nThere are three probabilities in the Eq. (3).\nq(c|x) is the document classifier which is our ulti-\nmate goal. p(wr|c) is the word distribution learner\nwhich estimates the probability distribution of all\nsignal words given a possible value c. We use neu-\nral networks to parameterize p(wr|c) and q(c|x).\np(C) is a prior probability distribution. Since there\nare no annotated data available, we cannot estimate\np(C). Hence we assume it is a uniform distribution,\nand p(c) becomes a constant.\n3.2.1 Word Distribution Learner\nThe word distribution learner aims to estimate the\nprobability of a signal word wr given a possible\nvalue of category c. It is defined as follows,\np(wr|c) = exp\n(\nvT\nc wr\n)\n∑\nwr′ exp\n(\nvTc wr′\n), (4)\nwhere vc is a trainable vector associated with cand\nwr is the trainable word embedding of signal word\nwr. The intuition is that if a word (e.g., “scientist”)\nappears frequently under the science category, the\ncorresponding inner-product value is high, other-\nwise it is low.\nEq. (4) requires the summation over all sig-\nnal words. Since the size of the word vocabulary\ncan be large, we use the negative sampling tech-\nnique (Mikolov et al., 2013) to approximate Eq.\n(4). Specifically, we approximates log p(wr|c) as\nfollows,\nlog σ\n(\nvT\nc wr\n)\n+\n∑\nw′r∈N\nlog\n(\n1 −σ\n(\nvT\nc w′\nr\n))\n, (5)\nwhere w′\nr is a negative sample in the vocabulary,\nNis the set of negative samples and σ(·) is the\nsigmoid function.\nThe objective function with an approximated\nword distribution learner is defined as follows,\nL=\n∑\nx∈X\n∑\nwr∈Rx\nEq(C|x)\n[\nlog σ\n(\nvT\nc wr\n)\n+\n∑\nw′r∈N\nlog\n(\n1 −σ\n(\nvT\nc w′\nr\n))\n+ logp(c)\n]\n−Eq(C|x)\n[\nlog q(c|x)\n]\n. (6)\n3.2.2 Document Classifier\nMost existing deep neural models (DNN) can be\nused to parameterize q(C|x). As long as the input\nof DNNs is a document, and the output is a prob-\nability distribution of category C. Since models\nwhich involve latent variables are difficult to opti-\nmize, we give a good initialization of the document\nclassifier. We pre-train the document classifier us-\ning pseudo labeled data to initialize it.\n2299\nTable 2: Statistics and label surface names in AGNews, 20Newsgroup, and UCINews.\nDatasets # Train # Dev # Test # Class Label Surface Names\nAGNews 108,000 12,000 7,600 4 politics, sports, business, technology\ncomputer graphics,\nsports car,\n20Newsgroup 14,609 1,825 1,825 6 science electronics encryption health\naerospace,\npolitics gun homosexuality,\nreligion atheist christianity,\nsale\nUCINews 26,008 2,560 27,556 4 entertainment, technology, business, health\nTable 3: V ocabulary size of signal words that are gener-\nated from an MLM and that are extracted from the doc-\nument (Doc) after removing non-discriminated words.\nDataset MLM Doc\nAGNews 724 584\n20Newsgroup 1,037 413\nUCINews 584 442\n4 Experiments\nIn this section, we show the empirical performance\nof our method on the text classification task.\n4.1 Datasets\nWe evaluate all methods on three datasets.\n(1) AGNews consists of news articles. It is con-\nstructed by (Zhang et al., 2015), which has been\ngathered from more than 2000 news sources in\nmore than one year of activity.\n(2) 20Newsgroup comprises around 18,000\nposts. It is originally collected by (Lang, 1995).\nWe perform text classification on coarse-grained\ntopics. It is an unbalanced dataset.\n(3) UCINews consists of news pages collected\nfrom a web aggregator. It is maintained by (Dua\nand Graff, 2017).\nTable 2 provides statistics and label surface\nnames of three datasets. In 20Newgroups, we\nexpand label surface names by combining fine-\ngrained label surface names under the same coarse-\ngrained category.\nTable 3 shows the vocabulary size of signal\nwords that are generated from an MLM and that\nextracted from the document (Doc) after removing\nnon-discriminated words.\n4.2 Compared Methods\nDataless (Chang et al., 2008) is performed based\non vector similarity between documents and label\nsurface names using explicit semantic analysis rep-\nresentation. The prediction is the category that\nyields the maximum cosine similarity.\nLabel-Word Similarity is performed based on the\nvector similarity between words generated from\nan MLM and label surface names using the static\nrepresentation. The prediction is the category that\nyields the maximum cosine similarity.\nPseudo-CNN assigns pseudo labels to documents\nin the training set based on label-word similarity.\nWe train a CNN model using pseudo labeled sam-\nples in the training set. More details are provided\nin section 4.5.\nPseudo-BERT trains BERT (Devlin et al., 2019)\nBERT-base-uncased using the same pseudo\nlabeled data as Pseudo-CNN. More details are pro-\nvided in section 4.5.\nWeSTClass (Meng et al., 2018) first generates\npseudo labels for documents which contain user-\nprovided keywords. It pre-trains a neural network\nusing pseudo samples as the training set and then\nperforms a self-training process.\nLOTClass (Meng et al., 2020) constructs a cate-\ngory vocabulary for each class, using a pre-trained\nLM. The vocabulary contains words that are rel-\nevant to the label name. LOTClass fine-tunes an\nLM via word-level category prediction task, and\nthen performs self-training on unlabeled data to\ngeneralize the model.\nConWea (Mekala and Shang, 2020) leverages con-\ntextualized representations of word occurrences\nand seed word information to automatically dis-\ntinguish multiple senses of the same word. The\n2300\nTable 4: Micro F1 and macro F1 scores of all methods on AGNews, 20Newsgroup, and UCINews.\nMethods\nDatasets AGNews 20Newsgroup UCINews\nMicro Macro Micro Macro Micro Macro\nDataless (Chang et al., 2008) 0.6855 0.6844 0.5000 0.4700 0.6248 0.6253\nLabel-Word Similarity 0.7917 0.7884 0.7310 0.6390 0.6447 0.6390\nPseudo-CNN 0.8265 0.8237 0.7973 0.6825 0.7598 0.7632\nPseudo-BERT 0.8249 0.8219 0.8153 0.6896 0.7824 0.7820\nWeSTClass (Meng et al., 2018) 0.8279 0.8268 0.5300 0.4300 0.6983 0.6999\nLOTClass (Meng et al., 2020) 0.8659 0.8656 0.6121 0.5586 0.7320 0.7236\nConWea (Mekala and Shang, 2020) 0.7443 0.7401 0.6200 0.5700 0.3293 0.3269\nX-Class (Wang et al., 2021) 0.8574 0.8566 0.6515 0.6316 0.6885 0.6962\nWDDC-MLM 0.8826 0.8825 0.8121 0.6882 0.8150 0.8134\nWDDC-Doc 0.8668 0.8657 0.8570 0.8250 0.7814 0.7772\nCNN (Kim, 2014) 0.9025 0.9025 0.9397 0.9310 0.9002 0.8998\nBERT (Devlin et al., 2019) 0.9305 0.9306 0.9660 0.9569 0.9313 0.9315\ncontextualized corpus is used to train the classifier\nand expand seed words iteratively.\nX-Class (Wang et al., 2021) leverages BERT rep-\nresentations to generate class-oriented document\npresentations, then generates document-class pairs\nby clustering, and then fed pairs to a supervised\nmodel to train a text classifier.\nCNN(Kim, 2014) trains a text CNN using anno-\ntated training data in a supervised manner. It is an\nupper bound of weakly supervised methods.\nBERT fine-tunes BERT BERT-base-uncased\n(Devlin et al., 2019) using annotated training data.\nIt is an upper bound of weakly supervised methods.\nWDDC We use a text CNN(Kim, 2014) as the\ndocument classifier. Instead of randomly initializ-\ning CNN, we pre-train CNN using Pseudo-CNN.\nWDDC-MLM uses the supervision signals from\nan MLM while WDDC-Doc uses the supervision\nsignals from the document itself.\n4.3 Result Analysis\nTable 4 shows that our method outperforms weakly\nsupervised baselines by 2%, 4%, and 3% in AG-\nNews, 20Newsgroup, and UCINews, respectively.\nThe gaps between the upper bound CNN and our\nmethod are 2%, 8%, and 8% in AGNews, 20News-\ngroup, and UCINews, respectively. There are\nstill large performance gaps on 20Newsgroup and\nUCINews.\nLabel-Word Similarity and Dataless both use\nvector similarity for prediction. Label-Word Sim-\nilarity consistently outperforms Dataless, which\nshows that words generated from an MLM are use-\nful compared with documents. The performance\nof Pseudo-BERT is comparable with WeSTClass\nin AGNews and better than any other baselines in\n20Newsgroup and UCINews, which also shows the\neffectiveness of our pseudo label generation tech-\nnique. In 20Newsgroup, Macro F1 scores are lower\nthan Micro F1 scores in Pseudo-CNN, Pseudo-\nBERT, and WDDC-MLM methods. We found that\nthe number of pseudo labeled data of sale category\nis much lower than other categories. So CNN does\nnot have enough pseudo labeled data to learn the\nsale category. The F1 score of sale category is\nlower.\nIn AGNews and UCINews, WDDC-MLM out-\nperforms WDDC-Doc by 2% and 3%, respectively,\nwhich shows that signal words from an MLM are\nmore useful than extracted words from a docu-\nment. But in 20Newsgroup, WDDC-Doc outper-\nforms WDDC-MLM by 4%. The possible reason is\nthat some categories in 20Newsgroup are not com-\npletely disjoint. According to general knowledge,\nencryption is a field of computer, and computer is\na field of science. But in 20Newsgroup (refer to Ta-\nble 2), science and encryption belong to one class,\nand computer belongs to another class. MLMs can\ncapture general knowledge from training corpora\nsuch as Wikipedia. When given a document talk-\ning about encryption, an MLM probably generates\nwords about encryption as well as computer. In\nthis circumstance, generated words are misleading\nwhile extracted words are clean. We have detailed\n2301\nTable 5: Mean and standard deviation of micro and macro F1 scores on 5 independent runs.\nDataset\nMethod WDDC Baselines\nMicro F1 Macro F1 Micro F1 Macro F1\nMean Std Mean Std Mean Std Mean Std\nAGNews 0.8826 0.0013 0.8825 0.0013 0.8630 0.0038 0.8626 0.0037\n20Newsgroup 0.8570 0.0023 0.8250 0.0033 0.8153 0.0131 0.6896 0.0063\nUCINews 0.8150 0.0012 0.8134 0.0014 0.7824 0.0141 0.7820 0.0148\nTable 6: Some incorrect predictions in AGNews, 20Newsgroup, and UCINews.\nDataset Text Prediction Ground Truth Signal Words (MLM)\nAGNews Microsoft and Palmone today technology business windows, microsoft,\nannounced a partnership business, security,\nthat will likely have a negative technology, linux,\nimpact on good technology, privacy\na well capitalized startup.\n20News- For the system, or ‘family’, computer science software, virus,\ngroup key would appear to be linux, encryption ,\ncryptographically useless. ... ibm , nsa\nThe same key is used for\nboth encryption and decryption.\nUCINews Paraplegic teenager to kick off entertainment health football, sport, soccer\nWorld Cup thanks to robot suit. cricket, tennis\nanalysis in section 4.4.\nTable 5 shows mean and standard deviation of\nmicro and macro F1 scores of WDDC and best\nbaselines on 5 independent runs. We also con-\nducted t-tests, and p-values are all less than 0.001.\nWe concluded that our method outperforms base-\nlines significantly. Baselines refers to LOTClass,\nPseudo-BERT, and Pseudo-BERT on AGNews,\n20Newsgroup, and UCINews respectively.\n4.4 Case Study\n4.4.1 Analysis of Incorrect Predictions\nTable 6 shows some incorrect predictions. In the\nfirst example, some words in the original document\nsuch as “partnership” and “startup” indicate busi-\nness while other words such as “Microsoft” and\n“technology” indicate technology. Signal words\ngenerated from an MLM are all related to technol-\nogy. In AGNews dataset, there are a number of\nsamples talking about the stock price of technol-\nogy companies or cooperation between technology\ncompanies. An MLM inclines to focus on either\ntechnology or business and ignore the other one. Al-\nthough the extraction method can cover all words,\nthe model is likely to be confused when signal\nwords are related to two categories. In the second\nexample, an MLM generates words related to en-\ncryption as well as computer. Generated words\nmake sense because according to general knowl-\nedge, encryption is related to computer. Unfortu-\nnately, most of the signal words from an MLM are\nrelated to computer except one word “encryption.”\nWDDC-MLM is likely to predict it as computer.\nSignal words extracted from the document are “en-\ncryption” and “key”, which are more likely to guide\nthe model to predict the correct category. In the\nthird example, an MLM generates words that are all\nabout sports because the term “World Cup” appears\nin the original document. The modifier “paraplegic”\nplays an important role in identifying the true cate-\ngory. Both generation and extraction methods fail\nto capture that.\n4.4.2 Analysis of Word Distribution Learner\nThe word distribution learner aims to estimate the\nprobability of a signal word wr given a possible\nvalue of category c, i.e., p(wr|c). A good word dis-\ntribution learner should assign a high probability\n2302\nTable 7: Top 15 signal words that have large inner product values with different latent variable vectors respectively\non AGNews dataset. Signal words are generated by an MLM.\nLabel Signal Words\nPolitics iraq, syria, haiti, israel, murder, baghdad, suicide,\ntorture, war, islam, iran, terrorist, afghanistan, religion, terrorism\nSports injury, racing, baseball, soccer, boxing, player, relegation,\ncricket, quarterback, england, basketball, doping, football, golf, tennis\nBusiness profit, market, finance, agriculture, bankruptcy, energy, money,\ngrowth, price, insurance, recession, airline, oil, risk, inflation\nTechnology ipod, genetics, encryption, microsoft, internet, hacking, virus,\nbiotechnology, science, copyright, itunes, nasa, evolution, space, astronomy\nto category-indicated words, so that by maximiz-\ning Eq. (3), a large value of p(wr|c) leads to a\nlarge value of q(c|x), which means if a document\ncontains indicative words to category c, it possi-\nbly belongs to category c. Table 7 shows top 15\nsignal words that have large inner product values\nwith different latent variable vectors respectively\non AGNews dataset. As shown in Table 7, the se-\nlected words are category-indicated. For example,\nin the politics category, all words are about terror-\nism, war, and places where wars broke out, which\nare relevant to the politics topic. The word distribu-\ntion learner can be consider as a category-indicated\nkeywords expansion module.\n4.5 Implementation\nWe use the BERT ( bert-base-uncased)\nmodel to obtain supervision signals in AG-\nNews and 20Newsgroup. We use the BERT (\nbert-base-cased) to obtain supervision sig-\nnals in UCINews which contains many acronyms\nsuch as WHO and PTSD. We select top 20 pre-\ndictions as supervision signals three datasets. To\nremove non-discriminated words, we set the thresh-\nold tto 2 in three datasets.\nIn the pseudo label generation process, we set\nthe threshold γto 0.6, 0.75, and 0.55 in AGNews,\n20Newsgroup, and UCINews, respectively. Those\npseudo labeled training data are used in Pseudo-\nCNN and Pseudo-BERT. A higher γ may result\nin more accurate pseudo labels. But we need to\nbalance the size of pseudo labeled data because it\nwill shrink when γincreases.\nTo train WDDC, in each batch, we randomly\nselect 5 signal words among all signal words of\na document. The number of negative samples in\nthe approximated word distribution learner is set\nto 10. For Pseudo-CNN, CNN, and WDDC meth-\nods, the CNN architectures are the same. Four\ndifferent filter sizes {2,3,4,5}are applied. A max-\npooling layer is applied to each convolutional layer,\nand each convolutional layer has 100 filters. The\nmaximum length of input in the CNN is set to\n64, 128, and 64 in AGNews, 20Newsgroup, and\nUCINews, respectively. The input in the CNN\nis contextualized embeddings generated by BERT\n(bert-base-uncased).\nFor WeSTClass, we use a CNN as the docu-\nment classifier because it empirically outperforms\nLSTM in WeSTClass. The CNN architecture we\nused here is the same as the one described in their\npaper. We try our best to find good keywords and\ntune hyper-parameters for WeSTClass and LOT-\nClass. For all methods, we tune hyper-parameters\non development sets.\n5 Conclusion\nTo solve the weakly supervised classification task,\nwe propose to query a masked language model with\ncloze style prompts to obtain supervision signals.\nWe design a prompt which combines the document\nitself and “this article is talking about [MASK].”\nThe predictions for the “[MASK]” token are con-\nsidered as supervision signals because they sum-\nmarize the content of documents. We propose a\nlatent variable model (WDDC) to learn word distri-\nbutions given pre-defined categories and a neural\ndocument classifier simultaneously without using\nany annotated data. Evaluation on three datasets\nshows that our method can outperform weakly su-\npervised learning baselines.\n2303\nAcknowledgements\nThe authors of this paper were partially supported\nby the NSFC Fund (U20B2053) from the NSFC\nof China, the RIF (R6020-19 and R6021-20) and\nthe GRF (16211520) from RGC of Hong Kong,\nthe MHKJFS (MHP/001/19) from ITC of Hong\nKong and the National Key R&D Program of\nChina (2019YFE0198200) with special thanks\nto Hong Kong Mediation and Arbitration Centre\n(HKMAAC) and California University, School of\nBusiness Law & Technology (CUSBLT), and the\nJiangsu Province Science and Technology Collab-\noration Fund (BZ2021065). We also thank the\nanonymous reviewers for their valuable comments\nand suggestions that help improve the quality of\nthis manuscript.\nReferences\nCornelia Caragea, Nathan J. McNeese, Anuj R. Jaiswal,\nGreg Traylor, Hyun-Woo Kim, Prasenjit Mitra, Ding-\nhao Wu, Andrea H. Tapia, C. Lee Giles, Bernard J.\nJansen, and John Yen. 2011. Classifying text mes-\nsages for the haiti earthquake. In Proceedings of\nISCRAM.\nMingwei Chang, Lev Arie Ratinov, Dan Roth, and Vivek\nSrikumar. 2008. Importance of semantic rep- resenta-\ntion: dataless classification. In Proceedings of AAAI,\npages 830–835.\nWenhu Chen, Wenhan Xiong, Xifeng Yan, and William\nWang. 2018. Variational knowledge graph reasoning.\nIn Proceedings of NAACL-HLT, pages 1823–1832.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of NAACL-HLT, pages 4171–\n4186.\nDheeru Dua and Casey Graff. 2017. UCI machine learn-\ning repository.\nEvgeniy Gabrilovich, Shaul Markovitch, et al. 2007.\nComputing semantic relatedness using wikipedia-\nbased explicit semantic analysis. In Proceedings\nof IJCAI, pages 1606–1611.\nJohan Ludwig William Valdemar Jensen et al. 1906.\nSur les fonctions convexes et les inégalités entre les\nvaleurs moyennes. Acta mathematica, 30:175–193.\nRie Johnson and Tong Zhang. 2017. Deep pyramid\nconvolutional neural networks for text categorization.\nIn Proceedings of ACL, pages 562–570.\nYoon Kim. 2014. Convolutional neural networks for\nsentence classification. In Proceedings of EMNLP,\npages 1746–1751.\nDiederik P Kingma and Max Welling. 2014. Auto-\nencoding variational bayes. In Proceedings of ICLR.\nToutanova Kristina, Klein Dan, Manning Christopher,\nand Yoram Singer. 2003. Feature-rich part-of-speech\ntagging with a cyclic dependency network. In Pro-\nceedings of NAACL-HLT, pages 252–259.\nKen Lang. 1995. Newsweeder: Learning to filter net-\nnews. In Proceedings of ICML, pages 331–339.\nYan Liang, Xin Liu, Jianwen Zhang, and Yangqiu Song.\n2019. Relation discovery with out-of-relation knowl-\nedge base as supervision. In Proceedings of NAACL-\nHLT, pages 3280–3290.\nHan Liu, Xiaotong Zhang, Lu Fan, Xuandi Fu, Qimai Li,\nXiao-Ming Wu, and Albert Y . S. Lam. 2019. Recon-\nstructing capsule networks for zero-shot intent clas-\nsification. In Proceedings of EMNLP, pages 4798–\n4808.\nDiego Marcheggiani and Ivan Titov. 2016. Discrete-\nstate variational autoencoders for joint discovery and\nfactorization of relations. Transactions of the Associ-\nation for Computational Linguistics, 4:231–244.\nDheeraj Mekala and Jingbo Shang. 2020. Contextu-\nalized weak supervision for text classification. In\nProceedings of ACL, pages 323–333.\nYu Meng, Jiaming Shen, Chao Zhang, , and Jiawei Han.\n2018. Weakly-supervised neural text classification.\nIn Proceedings of CIKM, pages 983–992.\nYu Meng, Jiaming Shen, Chao Zhang, , and Jiawei Han.\n2019. Weakly-supervised hierarchical text classifica-\ntion. In Proceedings of AAAI, pages 6826–6833.\nYu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan Xiong,\nHeng Ji, Chao Zhang, and Jiawei Han. 2020. Text\nclassification using label names only: A language\nmodel self-training approach. In Proceedings of\nEMNLP, pages 9006–9017.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositionality.\nIn Proceedings of NeurIPS, pages 3111–3119.\nFabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim\nRocktäschel, Yuxiang Wu, Alexander H Miller, and\nSebastian Riedel. 2020. How context affects lan-\nguage models’ factual predictions. In Proceedings of\nAKBC.\nFabio Petroni, Tim Rocktäschel, Patrick Lewis, An-\nton Bakhtin, Yuxiang Wu, Alexander H Miller, and\nSebastian Riedel. 2019. Language models as knowl-\nedge bases? In Proceedings of EMNLP, pages 2463–\n2473.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog.\n2304\nDanilo Jimenez Rezende, Shakir Mohamed, and Daan\nWierstra. 2014. Stochastic backpropagation and ap-\nproximate inference in deep generative models. In\nProceedings of ICML, pages 1278–1286.\nAnthony Rios and Ramakanth Kavuluru. 2018. Few-\nshot and zero-shot multi-label learning for structured\nlabel spaces. In Proceedings of EMNLP, pages 3132–\n3142.\nHerbert L. Roitblat, Anne Kershaw, and Patrick Oot.\n2010. Document categorization in legal electronic\ndiscovery: computer classification vs. manual review.\nJournal of the Association for Information Science\nand Technology, 61(1):70–80.\nTimo Schick and Hinrich Schütze. 2021. Exploiting\ncloze questions for few shot text classification and\nnatural language inference. In Proceedings of EACL,\npages 255–269.\nTimo Schick and Hinrich Schütze. 2021. It’s not just\nsize that matters: Small language models are also\nfew-shot learners. In Proceedings of NAACL-HLT,\npages 2339–2352.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Autoprompt:\nEliciting knowledge from language models with au-\ntomatically generated prompts. In Proceedings of\nEMNLP, pages 4222–4235.\nYangqiu Song and Dan Roth. 2014. On dataless hier-\narchical text classification. In Proceedings of AAAI,\npages 1579–1585.\nSimon Šuster, Ivan Titov, and Gertjan van Noord. 2016.\nBilingual learning of multi-sense embeddings with\ndiscrete autoencoders. In Proceedings of NAACL-\nHLT, pages 1346–1356.\nIvan Titov and Ehsan Khoddam. 2015. Unsupervised\ninduction of semantic roles within a reconstruction-\nerror minimization framework. In Proceedings of\nNAACL-HLT, pages 1–10.\nZihan Wang, Dheeraj Mekala, and Jingbo Shang. 2021.\nX-class: Text classification with extremely weak su-\npervision. In Proceedings of NAACL-HLT, pages\n3043–3053.\nCongying Xia, Chenwei Zhang, Xiaohui Yan, Yi Chang,\nand Philip S. Yu. 2018. Zero-shot user intent detec-\ntion via capsule neural networks. In Proceedings of\nEMNLP, pages 3090–3099.\nZiqian Zeng, Wenxuan Zhou, Xin Liu, and Yangqiu\nSong. 2019. A variational approach to weakly su-\npervised document-level multi-aspect sentiment clas-\nsification. In Proceedings of NAACL-HLT, pages\n386–396.\nJingqing Zhang, Piyawat Lertvittayakumjorn, and Yike\nGuo. 2019. Integrating semantic knowledge to\ntackle zero-shot text classification. In Proceedings of\nNAACL-HLT, pages 1031–1040.\nRongzhi Zhang, Yue Yu, Pranav Shetty, Le Song,\nand Chao Zhang. 2022. Prboost: Prompt-based\nrule discovery and boosting for interactive weakly-\nsupervised learning. In Proceedings of ACL.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text classi-\nfication. In Proceedings of NeurIPS, pages 649–657.\nYuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexan-\nder J Smola, and Le Song. 2018. Variational reason-\ning for question answering with knowledge graph. In\nProceedings of AAAI, pages 6069–6076.\nPeng Zhou, Zhenyu Qi, Suncong Zheng, Jiaming Xu,\nHongyun Bao, and Bo Xu. 2016. Text classifica-\ntion improved by integrating bidirectional lstm with\ntwo-dimensional max pooling. In Proceedings of\nCOLING, pages 3485–3495.\n2305",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8447823524475098
    },
    {
      "name": "Classifier (UML)",
      "score": 0.6739913821220398
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6637313365936279
    },
    {
      "name": "Natural language processing",
      "score": 0.6186888217926025
    },
    {
      "name": "Language model",
      "score": 0.6121536493301392
    },
    {
      "name": "Security token",
      "score": 0.5915423631668091
    },
    {
      "name": "Labeled data",
      "score": 0.4764462113380432
    },
    {
      "name": "Word (group theory)",
      "score": 0.4619024693965912
    },
    {
      "name": "Linguistics",
      "score": 0.0820387601852417
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I90610280",
      "name": "South China University of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I157725225",
      "name": "University of Illinois Urbana-Champaign",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210156583",
      "name": "Laboratoire d'Informatique de Paris-Nord",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ]
}