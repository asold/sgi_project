{
  "title": "BioGPT: generative pre-trained transformer for biomedical text generation and mining",
  "url": "https://openalex.org/W4297253404",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4280914702",
      "name": "Luo, Renqian",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": null,
      "name": "Sun, Liai",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A4223307627",
      "name": "Xia, Yingce",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2105471080",
      "name": "Qin Tao",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A913949472",
      "name": "Zhang Sheng",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4221580603",
      "name": "Poon, Hoifung",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4209371238",
      "name": "Liu, Tie-Yan",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6766673545",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W6801409643",
    "https://openalex.org/W4221165572",
    "https://openalex.org/W6704842505",
    "https://openalex.org/W6800996715",
    "https://openalex.org/W2170189740",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W2174775663",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W6794495662",
    "https://openalex.org/W6776814788",
    "https://openalex.org/W3101327207",
    "https://openalex.org/W3214342214",
    "https://openalex.org/W4223491992",
    "https://openalex.org/W6751097180",
    "https://openalex.org/W3104415840",
    "https://openalex.org/W4221153690",
    "https://openalex.org/W1981208470",
    "https://openalex.org/W3035324702",
    "https://openalex.org/W2517194566",
    "https://openalex.org/W2951231735",
    "https://openalex.org/W3035219457",
    "https://openalex.org/W3034617555",
    "https://openalex.org/W2798734500",
    "https://openalex.org/W3102021238",
    "https://openalex.org/W6750384732",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6798057236",
    "https://openalex.org/W6796537594",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3202242582",
    "https://openalex.org/W3166508187",
    "https://openalex.org/W4295253143",
    "https://openalex.org/W4303105513",
    "https://openalex.org/W4238846128",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2250521169",
    "https://openalex.org/W4205807230",
    "https://openalex.org/W4250526791",
    "https://openalex.org/W4385573954",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W4205185581",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W3095696617",
    "https://openalex.org/W2949212908",
    "https://openalex.org/W4239019441",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3168090480",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W4286985375",
    "https://openalex.org/W3116427155",
    "https://openalex.org/W2962808855",
    "https://openalex.org/W4281644150",
    "https://openalex.org/W2912512851",
    "https://openalex.org/W4241901867",
    "https://openalex.org/W2895715183",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3154872984",
    "https://openalex.org/W4287817164",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4297068981",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3034902017",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2802226260",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3035763680",
    "https://openalex.org/W2346452181",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W3196690920"
  ],
  "abstract": "Abstract Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.",
  "full_text": "BioGPT 1\nBioGPT: Generative Pre-trained Transformer for\nBiomedical Text Generation and Mining\nRenqian Luo 1,∗, Liai Sun2, Yingce Xia 1,∗, Tao Qin 1,∗, Sheng Zhang 3,\nHoifung Poon 3 and Tie-Yan Liu1\n1Microsoft Research AI4Science, Beijing, China,2Peking University, Beijing, China and3Microsoft Research, Redmond, WA, USA\n∗Corresponding authors: Renqian Luo, Microsoft Research AI4Science, E-mail: renqianluo@microsoft.com; Yingce Xia, Microsoft Research\nAI4Science, E-mail: yinxia@microsoft.com; Tao Qin, Microsoft Research AI4Science, E-mail: taoqin@microsoft.com\nAbstract\nPre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success\nin the general natural language domain. Among the two main branches of pre-trained language models in the general\nlanguage domain, i.e., BERT (and its variants) and GPT (and its variants), the ﬁrst one has been extensively studied in\nthe biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety\nof discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In\nthis paper, we propose BioGPT, a domain-speciﬁc generative Transformer language model pre-trained on large scale\nbiomedical literature. We evaluate BioGPT on six biomedical NLP tasks and demonstrate that our model outperforms\nprevious models on most tasks. Especially, we get 44 .98%, 38 .42% and 40 .76% F1 score on BC5CDR, KD-DTI and\nDDI end-to-end relation extraction tasks respectively, and 78 .2% accuracy on PubMedQA, creating a new record. Our\ncase study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate ﬂuent\ndescriptions for biomedical terms. Code is available at https://github.com/microsoft/BioGPT.\nKey words:biomedical literature, generative pre-trained language model, text generation, text mining\nIntroduction\nText mining and knowledge discovery from biomedical\nliterature play important roles in drug discovery, clinical\ntherapy, pathology research, etc. Typical tasks include\nrecognizing named entities in the articles, mining the\ninteraction between drugs and proteins/diseases/other drugs,\nanswering questions given reference text, generating abstracts\nfor given phrases/words, etc. People have accumulated large\namounts of literature in the previous studies. For example,\nPubMed1, one of the most popular biomedical search engines,\ncovers more than 30 M articles and the number still rapidly\nincreases every day as new discoveries are continuously coming\nout. Therefore, automatically mining the knowledge from\nliterature becomes an urgent demand.\nPre-training models have demonstrated their powerful\ncapability in natural language processing (NLP). On the GLUE\nbenchmark, a widely used benchmark for natural language\nunderstanding, pre-training based methods outperform non-\npre-training methods by a large margin [1]2. There are two main\nkinds of pre-training models: (1) the BERT-like models [2, 3, 4],\nmainly for language understanding tasks; (2) the GPT-like\nmodels [5, 6, 7], mainly for language generation tasks.\nThese models are ﬁrst pre-trained on large scale corpora\ncollected from the Web via self-supervised learning task (e.g.,\nmasked language modeling for BERT, auto-regressive language\nmodeling for GPT), and then ﬁne-tuned on speciﬁc donwstream\ntasks. The BERT-like models are widely used in sequence\nclassiﬁcation and sequence labeling, where we need to encode\n1 https://pubmed.ncbi.nlm.nih.gov\n2 https://gluebenchmark.com/leaderboard\nthe complete document. In comparison, the GPT-like models\nare often used in generation tasks (e.g., abstract generation,\nknowledge triplet generation).\nBy witnessing the success of pre-training in general NLP,\npeople explore adapting these techniques into biomedical\ndomain. However, directly applying these models to the\nbiomedical domain leads to unsatisfactory performance due to\ndomain shift [8, 9]. A natural solution is to develop pre-training\nmodels on biomedical texts (e.g., PubMed). BioBERT [10]\nand PubMedBERT [9]) are two representative BERT-like\nmodels pre-trained on biomedical domain, and they obtain\nsuperior performances than general pre-trained models on\nbiomedical benchmarks. However, previous works mainly focus\non BERT models which are more appropriate for understanding\ntasks, not generation tasks. In comparison, GPT models\nhave demonstrated their abilities on generation tasks but\ndemonstrate inferior performance when directly applying to the\nbiomedical domain [11, 12].\nIn this work, we propose BioGPT, a domain-speciﬁc\ngenerative pre-trained Transformer language model for\nbiomedical text generation and mining. BioGPT follows the\nTransformer language model backbone, and is pre-trained on\n15M PubMed abstracts from scratch. We apply BioGPT to\nsix biomedical NLP tasks: end-to-end relation extraction on\nBC5CDR [13], KD-DTI [14] and DDI [15], question answering\non PubMedQA [16], document classiﬁcation on HoC [17], and\ntext generation. To adapt to the downstream tasks, we carefully\ndesign and analyze the target sequence format and the prompt\nfor better modeling the tasks. Experiments demonstrate that\nBioGPT achieves better performance compared to baseline\nmethods and other well-performing methods across all the\ntasks.\narXiv:2210.10341v3  [cs.CL]  3 Apr 2023\n2 Renqian Luo et al.\nRelated Work\nPre-trained Language Models\nIt has proven to be a very successful pattern in deep\nlearning to pre-train models on large scale unlabeled data\nvia careful designed self-supervision tasks and then transfer\nto downstream tasks by ﬁne-tuning on them. Downstream\ntasks can beneﬁt from the learned representations from the\npre-trained models. BERT [2] is a bidirectional transformer\nbased contextualized language model pre-trained on large scale\ntext corpus English Wikipedia and BooksCorpus. It is pre-\ntrained via carefully designed self-supervision tasks: masked\nlanguage modeling (MLM) task where random word tokens\nof the input text are replaced by a special token [MASK]\nwhich is to be predicted by the model from the context, and\nthe next sentence prediction (NSP) task where two sentences\nare to be predicted whether the second one is probable given\nthe ﬁrst one. The pre-trained BERT provides contextualized\nword representations that can be used by downstream tasks by\njust ﬁne-tuning on the tasks and has achieved great success\non various natural language understanding tasks. Subsequent\nworks mainly focus on pre-training on larger-scale data and\nmodels [3] and advanced pre-training task [4]. Though BERT\nand various biomedical BERT models have been successful\nin language understanding tasks and classiﬁcation tasks, few\neﬀorts have been devoted to generative models. As BERT learns\npowerful word representations through the Transformer encoder\nmodel architecture in a bi-directional way, it limits its ability\nof generation.\nGenerative Pre-trained Transformer (GPT) [5] is proposed\nfor language generation tasks via pre-training Transformer\ndecoder model on large scale text corpus in a classical casual\nlanguage modeling task where model learns to predict the\nnext word token only dependent on the previous word tokens.\nFurther, GPT-2 [6] and GPT-3 [7] with larger model size\npre-trained on larger scale text corpus are proposed with\nremarkable performance in various downstream tasks (e.g.,\ntranslation, summarization) including classiﬁcation tasks (e.g.,\nreading comprehension) even without ﬁne-tuning (zero-shot)\nvia appropriate prompts design.\nPre-trained Language Models in Biomedical Domain\nWhen applying to speciﬁc domain (e.g., biomedicine), BERT\nmodels pre-trained on general domain can be further improved\nif pre-trained on in-domain text data [8, 18, 10]. Speciﬁcally,\n[10] and [8] start from the original pre-trained BERT\nmodel [2] that are pre-trained on general domain (Wikipedia\nand BooksCorpus) and continue pre-training on biomedical\nliterature. Speciﬁcally, [10] continue pre-training using PubMed\nabstracts and PubMed Central full text articles and [8] continue\npre-training on both PubMed text and clinical notes from\nMIMIC-III [19]. As they are initialized from the original BERT\nthat are pre-trained on general domain, they use the same\nvocabulary as the original BERT, which is quite diﬀerent from\nthe target biomedical domain. Instead of continue pre-training\nfrom the pre-trained BERT model, [18] pre-train the BERT\nmodel from scratch on large corpus of scientiﬁc literature\n(mainly biomedical and computer science literature) where the\nvocabulary is more suitable for science domain but still contains\nout-domain information for biomedicine. [9] propose that it\nis a better strategy to pre-train on domain-speciﬁc data from\nscratch where the vocabulary is more suitable for the biomedical\ndomain. Consequently, they propose PubMedBERT which is\npre-trained on 14M PubMed abstracts from scratch. Similarly,\n[20] pre-train on 28 M data as in [8] also from scratch, using\nthe more advanced ELECTRA model. All these works have\nshown improved performance on plenty of biomedical literature\nlanguage processing tasks compared to the original BERT pre-\ntrained on general domain, while none of them is for biomedical\ngeneration tasks.\nNoticing the powerful generation ability of GPT models,\nit is quite curious how GPT models perform on biomedical\ndomain which is very diﬀerent from general domain. However,\nrecent works show that GPT models, even much more powerful\nGPT-3 model, perform poorly on biomedical tasks [11, 12]. A\nprevious work on pre-training GPT on biomedical literature\nis DARE [21]. However, they pre-train GPT on very limited\namount of data (only 0.5M PubMed abstracts) and use it only\nfor data-augmentation for relation extraction task. A recent\nwork on using GPT model is [22], where they design converters\nfor GPT-3 [7] for several unconventional downstream clinical\ntasks.\nDownstream Tasks\nIn this subsection, we introduce the downstream tasks we will\nwork on. A summary of those tasks is in Table 1. All these tasks\ncan be formulated as text generation / mining tasks.\nRelation Extraction\nRelation extraction is a key task for biomedicine and life\nscience research. Classical pipeline-based methods [33, 34, 23]\nresolve the task into several separate sub-tasks that require\nadditional intermediate annotations and information which\nmay suﬀer from the lack of intermediate annotated data and\nerror accumulation. Joint extraction aims to jointly extract the\nentities and the relations between them from the text. Sequence\nlabeling methods tackle the task by labeling the word tokens in\nthe text with diﬀerent tags to mark out all the entity mentions\nand then perform the relation classiﬁcation between them via\nclassiﬁers [35, 36, 37, 38]. Table ﬁlling methods formulate the\ntask as a table constituted by the Cartesian product of itself and\npredicts the relations between the token pairs [39, 40, 41]. These\nmethods may suﬀer from error accumulation caused by previous\ntagging process and laborious intermediate annotations (i.e.,\nnamed entity recognition). Text generation methods reframe\nthe task as a sequence-to-sequence learning task, by taking\nthe text as the input sequence and the triplet as the target\nsequence and employing an encoder-decoder network to learn to\ngenerate the triplet from the text [42, 43, 14, 24, 25]. However,\nmany joint extraction methods still require additional entity\ninformation [38, 44]. In this work, we focus on the end-to-\nend relation extraction, which formulates the task as an text\ngeneration task that takes only the text as the input and\ngenerates the relational triplets in an end-to-end way without\nadditional intermediate annotations [24, 14, 25].\nQuestion Answering\nQuestion answering (QA) is the task of answering questions\ngiven a context (reading comprehension). Typical methods\npredict a span in the source context as the answer text,\nor predicts a label (e.g., yes or no) for simpler tasks with\npredeﬁned categorical answers [26, 45, 27]. [9, 28, 29] mainly\nfocus on the biomedical domain question answering task via\npre-trained language models. Generative models [6, 7] directly\ngenerate the answer sequence or the label words.\nBioGPT 3\nTable 1.Summary of the downstream tasks\nTask Method Dataset\nRelation Extraction GLRE [23], REBEL [24], seq2rel [25] KD-DTI [14], BC5CDR [13], DDI [15]\nQuestion Answering QA-Net [26], LUKE [27], BERT [2], PubMedBERT [9],\nBioELECTRA [28], LinkBERT [29] PubMedQA [16], BioASQ [30, 31]\nDocument Classiﬁcation BERT [2], BlueBERT [8], SciBERT [18], SPECTER [32],\nPubMedBERT [9], BioELECTRA [28], LinkBERT [29] HoC [17], SciDocs [32]\nDocument Classiﬁcation\nDocument classiﬁcation is to classify a document into\npredeﬁned categories (single label or multi label). Recent\nworks on biomedical document classiﬁcation also leverage\nlarge pre-trained language models for understanding the text\nand predicting the label [8, 9, 28, 29]. Generative models\n[6, 7] generate the label words instead of predicting from the\npredeﬁned set.\nPre-training Method\nIn this section, we describe our BioGPT from the perspective\nof dataset, vocabulary, and model.\nDataset: Dataset is crucial for language model pre-training, in\nterms of amount, quality and domain. As Gu et al. [9] point,\ntraining only on in-domain data from scratch is important for\nspeciﬁc domain. Therefore, we only consider in-domain text\ndata and pre-train our model from scratch on the collected data.\nWe collected all the PubMed items 3 that were updated before\n2021 from the oﬃcial site4 using the wget tool. We then ﬁltered\nout all the empty items with only title but no abstract. We\nused the left 15 M items (each with both title and abstract) as\nour pre-training dataset.\nVocabulary: [9] also points that in-domain vocabulary is\nvital. Instead of using the vocabulary of GPT-2, we learn the\nvocabulary on our collected in-domain corpus. Speciﬁcally, we\nuse byte pair encoding (BPE) [46] to segment the words in the\ncorpus into word pieces and learn the vocabulary. We adopt the\nfastBPE5 implementation of BPE. The ﬁnal learned vocabulary\nsize is 42384.\nModel: We adopt the GPT-2 model architecture [6] as the\nbackbone of our BioGPT, which is a Transformer decoder [47].\nCurrently we cannot follow the GPT-3 setting due to its\nextremely large model with 15 billion parameters. The core\ncomponent of Transformer as well as our BioGPT is the multi-\nhead attention. Given the input, three linear transformations\nare applied to produce the query Q, the key K and the value\nV , and then the output is calculated as follows:\nMultihead(Q, K, V) = Concat(head1, head2, ··· , headh)W,\nheadi = softmax\n(\nQiKT\ni√\nd\n)\nVi,\n(1)\nwhere (1) h is the number of heads; (2) Q, K and V are\nequally split into Qi, Ki and Vi along the feature dimension,\ni ∈{1, 2, ··· , h}; (3) Concat denotes concatenating all inputs\nas a large tensor along the feature dimension; (4) W is\n3 https://pubmed.ncbi.nlm.nih.gov\n4 https://ftp.ncbi.nlm.nih.gov/pubmed/\n5 https://github.com/glample/fastBPE\nthe parameter for the aﬃne transformation. The output of\nmulti-head attention layer is then fed into a feed-forward\nlayer to construct a Transformer layer (or Transformer block).\nPractically, we adopt GPT-2 medium as the backbone network\nwhich has 24 layers, 1024 hidden size and 16 attention heads\nresulting in 355 M parameters in total, and our BioGPT has\n347M parameters (the diﬀerence only comes from the diﬀerent\nembedding size and output projection size caused by the\ndiﬀerent vocabulary size).\nTraining criteria: BioGPT is trained via the standard\nlanguage modeling task as the same as in [5, 6]. Let D= {xi}i\ndenote the collection of sequences, and sequence xi is made up\nof ni tokens, i.e., xi = (s1, s2, ··· , sni ). The training objective\nis to minimize the negative log-likelihood:\nmin − 1\n|D|\n|D|∑\ni=1\nni∑\nj=1\nlog P(sj|sj−1, sj−2, ··· , s1). (2)\nFine-tuning Method\nIn this section, we introduce how to adapt the pre-trained\nBioGPT to downstream tasks: end-to-end relation extraction,\nquestion answering (QA) and document classiﬁcation. The\ninputs of the tasks are all sequences, while they have diﬀerent\noutput formats.\nTo use BioGPT for these tasks, we need to convert the labels\ninto sequences. In this way, the downstream task is consistent\nwith the pre-training task in terms of the format.\nConsidering that BioGPT is pre-trained on massive natural\nlanguage corpus, we convert the labels to sequences in natural\nlanguage rather than the structured format using special tokens\nexplored in other works [14, 24, 25]. In this way, our reformed\nlabels are semantically smoother than using special tokens.\nWe will show the detailed implementation for each task and\nempirically verify the eﬀectiveness of our method later.\nEnd-to-end Relation Extraction\nTask description: Given a source sequence x, we need\nto ﬁnd all triplets ⟨head entityi, tail entityi, relation i⟩N\ni=1,\nthat can be inferred from x. N refers to the number of\nall possible triplets. Examples include extracting the drug-\ntarget-interaction, chemical-disease-relation and drug-drug-\ninteraction.\nMethod: We convert the triplets into a simple natural language\nsequence with the same grammatical structures. We explore\nthree forms in this paper:\n1. the “subject verb object” form ( svo), where the entities\ncorrespond to the head entity, the relation and the tail\nentity in the triplet.\n2. the “subject is the rel.noun of object” form ( is-of), where\nthe “rel.noun” refers to the noun form of the relation.\n4 Renqian Luo et al.\n3. the “ the relation between subject and object is rel.noun”\nform (rel-is).\nIf there are multiple relational triplets for an input\ndocument, we sort them according to their order of appearance\nin the document and use semicolons to concatenated them\ntogether.\nLet us use a ⟨drug, target, interaction ⟩triplet as example.\nSuppose we would like to extract triplet ⟨dextropropoxyphene\n(drug name), mu-type opioid receptor (target name),\ninhibitor (relation) ⟩from an input document. Then the svo\nrepresentation is:\ndextropropoxyphene inhibits mu-type opioid receptor.\nThe is-of form is:\ndextropropoxyphene is the inhibitor of mu-type opioid\nreceptor.\nThe rel-is form is:\nthe relation between dextropropoxyphene\nand mu-type opioid receptor is inhibitor.\nThe natural sentences can be converted back to triplets\nusing regular expression. Users can also design customized\nformats depending on tasks.\nQuestion Answering\nTask descriptionGiven a question, a reference context and an\nanswer, the goal is to answer the question given the reference\ncontext. The label is within the category of yes, no, or maybe.\nMethod: We pre-pend the description word “question:” and\n“context:” before the question and the context respectively and\nconcatenate them together as the source sequence. Then for the\ntarget sequence, we generate it using the format “the answer\nto the question given the context is label”. For example:\nsource: question: question text. context: context text.\ntarget: the answer to the question given the context is yes.\nDocument Classiﬁcation\nTask descriptionGiven a document text, the goal is to classify\nthe type of the document.\nMethod: We generate the target sequence using the format\n“the type of this document is label”. For example:\nthe type of this document is genomic instability and mutation.\nPrompt-based Fine-tuning\nWe have formatted the labels to target sequences. The last\nquestion is, how do we use the source and the target to ﬁne-tune\nand inference with BioGPT? A naive way is to concatenate the\nsource and the target sequences together but is diﬃcult for the\nmodel to generate during inference as it does not know what to\ngenerate for the speciﬁc task given the source text input.\nPrompt is recently extensively explored in NLP [48] to elicit\nthe knowledge from a pre-trained language model. Prompt\nis to append task-speciﬁc instructions to the input for the\nmodel to better generate output that meets the demand of the\ntask. GPT-3 [7] uses hard prompts (manually designed discrete\nlanguage phrases) to generate for diﬀerent tasks. Though hard\nprompts can achieve satisfactory performance, designing task\nspeciﬁc prompts is laborious and it is found that diﬀerent\nprompts lead to diﬀerent performance.\nIn this work, we mainly adopt soft prompts in preﬁx-\ntuning [49], which leverage continuous embeddings (virtual\ntokens) to steer the pre-trained language model by directly\nappending several additional virtual tokens before the text\nas the prompts. Such continuous embeddings are randomly\ninitialized and learned end-to-end on the downstream tasks\nto be task-speciﬁc. Diﬀerent from [49], we do not append\nthe virtual tokens to the very beginning of the source input,\nbut only before the target sequence (between the source and\nthe target). Equipped with the prompt, our ﬁnal sequence is\nconstructed as [ source; prompt; target], as depicted in Fig. 1.\nDuring the inference, we provide the source text and the prompt\nas the preﬁx for the language model to condition on and let the\nlanguage model to generate the target output as in Fig. 1.\nExperiments\nIn this section, we pre-train our BioGPT and evaluate\nit on the following four biomedical NLP tasks across six\ndatasets: end-to-end relation extraction on BC5CDR [13], KD-\nDTI [14] and DDI [15], question answering on PubMedQA [16],\ndocument classiﬁcation on HOC [17], and text generation\non self-created dataset. We use fairseq [50] as our code\nbase for implementation. We adopt the GPT-2 medium model\nconﬁguration as our backbone model conﬁguration. We perform\nBPE to learn to tokenize the corpus and construct the\nvocabulary instead of using the learned vocabulary from GPT-\n2 due to the domain gap between the biomedical domain and\nthe general domain.\nFor pre-training, we pre-train BioGPT on 8 NVIDIA V100\nGPUs for 200 k steps, with 1024 tokens per GPU and 64\naccumulated steps (i.e., the ﬁnal batch size is 1024 ×8 ×64 =\n524288 tokens). We use Adam [51] as the optimizer with a peak\nlearning rate of 2×10−4 and 20000 warm-up steps. The learning\nrate follows an inverse square root decay schedule after reaching\nthe peak as in [47].\nAll the ﬁne-tuning experiments are conducted on a single\nNVIDIA V100 GPU, with a batch size of 1024 tokens and 32\naccumulated steps.\nDuring the inference, we adopt beam search with beam\nsize=5 for the text generation task, and greedy search for all\nthe other tasks.\nWe make comparison to general domain GPT-2 for all the\nexperiments. Speciﬁcally, we use the GPT-2medium model from\nthe Hugging face library [52] 6 which is the backbone network\nof our BioGPT.\nEnd-to-end Relation Extraction\nRelation extraction is an important task in information\nextraction. Here we target at the end-to-end relation extraction\nsetting where the model takes the text as the input and\ndirectly generates the relational triplets. We mainly compare to\nREBEL [24], a recently proposed end-to-end triplet extraction\napproach based on sequence-to-sequence model, which employs\nBART pre-trained model [53] as the backbone model, and\nfurther enhances it by pre-training on additional large relational\ntriplet dataset created from Wikipedia as REBEL pt.\nBC5CDR\nBC5CDR is a dataset for chemical-disease-relation extraction\ntask introduced by [13] which consists of 500/500/500\n6 https://huggingface.co/gpt2-medium\nBioGPT 5\nFig. 1.Framework of BioGPT when adapting to downstream tasks\ndocuments as the training/validation/test set. We ﬁne-tune\nGPT-2medium and BioGPT for 100 epochs with a peak\nlearning rate 10−5 and 100 warm-up steps. We use continuous\nembeddings with length=9 as prompts and the rel-is target\nsequence format. Since BC5CDR is a binary relation dataset\nwhere the entities are labeled if the relationship exists instead\nof a speciﬁc relation type, we use the pattern “the relation\nbetween head entity and tail entity exists” as the target\nsequence format. We average the checkpoints of the last\n5 epochs for evaluation. We mainly measure and compare\nthe micro-F1 score. We compare BioGPT to REBEL and\nseq2rel [25] where both methods are end-to-end relation\nextraction methods based on sequence-to-sequence modeling.\nWe also compare with a pipeline-based extraction method,\nGLRE [23] which requires NER (named entity recognition)\ninformation as the intermediate annotations in the pipeline.\nOriginally, GLRE uses the ground truth NER information. To\nmake a fair comparison, we experiment with GLRE for two\nsettings: 1) using ground-truth NER information during the\ntraining and using open-source NER tool during the inference\n(i.e., GLRE (gt+pred)) and 2) using open-source NER tool for\nboth the training and the inference (i.e., GLRE (pred+pred)).\nWe use the open-source NER tool 7 for the NER tagging. We\ntry our best to run the baseline methods and evaluate them.\nFrom the results in Table 2, we can see that BioGPT\nachieves the best result (44 .98%) among all the methods, with\nlarge improvements. We have several ﬁndings: 1) pipeline-\nbased method GLRE signiﬁcantly drops when using NER\ntagged by open-source tools instead of ground truth NER.\nHowever, this is often the common case in practical situation\nwhere the annotations for NER are lacked or expensive to\ncollect. When applying open-source NER tools to some speciﬁc\ndomains, errors occur and lead to inferior performance of\nrelation extraction. 2) Compared to REBEL, BioGPT has a\nlarge gain with 8.28% improvement. Notice that seq2rel [25]\nis trained on both the training set and validation set, while\nour BioGPT is only trained on the training set and still\noutperforms it with 4.78% improvement. Moreover, when also\ntrained on both the training set and the validation set, BioGPT\n7 https://huggingface.co/samrawal/bert-base-uncased_\nclinical-ner\nTable 2.Results on BC5CDR chemical-disease-relation extraction\ntask. ’gt+pred’ means using ground truth NER information for\ntraining and using open-source NER tool to annotate NER for\ninference. ’pred+pred’ means using open-source NER tool for both\ntraining and inference. ’†’ means training on training and validation\nset.\nModel Precision Recall F1\nGLRE (gt+pred) 34.82 18.29 23.99\nGLRE (pred+pred) 23.00 4.88 8.05\nGPT-2 [6] 43.92 32.55 37.39\nREBEL [24] 34.28 39.49 36.70\nREBELpt [24] 40.94 21.20 27.94\nseq2rel [25]† 43.5 37.5 40.2\nBioGPT 49.44 41.28 44.98\nBioGPT† 49.52 43.25 46.17\nfurther improves to 46.17% with 5.97% improvement against\nseq2rel [25].\nKD-DTI\nKD-DTI is dataset for drug-target-interaction introduced\nby [14], consisting of 12 k/1k/1.3k documents as the\ntrain/validation/test set. We ﬁne-tune GPT-2 medium and\nBioGPT on the task for 30 epochs using Adam optimizer\nwith a peak learning rate of 10 −5 and 1000 warm-up steps.\nWe use continuous embeddings with length=9 as prompts and\nthe rel-is target sequence format for constructing the target\nsequence. We average the checkpoints of the last 5 epochs for\nevaluation. We mainly measure and compare the micro-F1 score\nand the results are listed in Table 3.\nWe compare BioGPT with GPT-2 medium, Transformer +\nPubMedBERT-attn evaluated in [14] and REBEL. It can be\nshown that BioGPT achieves 38.42% f1 score, with 14.23%,\n9.97% and 8.03% improvement compared to Transformer +\nPubMedBERT-attn, GPT-2 medium and REBEL. Particularly,\nit surpasses REBELpt by 5.1% which is further pre-trained on\nlarge relation extraction dataset while BioGPT does not.\n6 Renqian Luo et al.\nTable 3.Results on KD-DTI drug-target-interaction extraction task\nModel Precision Recall F1\nTransformer + PubMedBERT 25.35 24.14 24.19-attn [14]\nGPT-2medium 30.53 27.87 28.45\nREBEL 32.36 29.58 30.39\nREBELpt 35.73 32.61 33.32\nBioGPT 40.00 39.72 38.42\nDDI\nDDI extraction 2013 corpus is a dataset for drug-drug-\ninteraction task introduced by [15], consisting of 792\ntexts selected from the DrugBank database and other 233\nMedline abstracts. We use the original dataset and use a\ntrain/validation/test split of 664/50/191 ﬁles. We ﬁne-tune\nGPT-2medium and BioGPT for 100 epochs with a peak learning\nrate 10 −4 and 500 warm-up steps. We also use continuous\nembeddings with length=9 as prompts and the rel-is target\nsequence format. The last 5 epochs are averaged for evaluation.\nThe micro-F1 score is measured and compared.\nTable 4.Results on DDI drug-drug-interaction extraction task\nModel Precision Recall F1\nGPT-2medium 23.39 31.93 24.68\nREBEL 35.36 28.64 28.27\nREBELpt 46.59 39.60 40.56\nBioGPT 41.70 44.75 40.76\nThe results are shown in Table 4 from which we can see that\nBioGPT achieves 40.76% with 16.08% and 12.49% improvement\nagainst GPT-2medium and REBEL. It also surpasses REBEL pt\nwhich uses additional large relation extraction dataset for two-\nstage pre-training.\nQuestion Answering\nPubMedQA [16] is a biomedical question answering dataset.\nEach sample is constructed from a PubMed abstract, containing\na question, a reference context, a long answer, and a\nyes/no/maybe label which is the answer to the question. We\nuse the original train/validation/test split with 450, 50 and 500\nrespectively, noted as PQA-L in [16] for evaluation. We also use\nthe additional dataset noted as PQA-A and PQA-U in [16] for\nﬁne-tuning. We use the continuous embedding with length=9\nas the soft prompt. We format the data into source sequence\nand target sequence as described before. We apply techniques\nsuch as two-stage ﬁne-tuning [16] and noisy labels to improve\nthe performance. We measure and compare the classiﬁcation\naccuracy of the reasoning required setting described in [16].\nFrom the results in Table 5 we can see that BioGPT achieves\n78.2% accuracy with 6.0% improvement over previous best\nperformance obtained by BioLinkBERT [29], achieving a new\nstate-of-the-art on this task.\nDocument Classiﬁcation\nHoC (the Hallmarks of Cancers corpus) consists of 1580\nPubMed abstracts manually annotated at sentence level by\nTable 5.Results on PubMedQA question answering task\nModel Accuracy\nPubMedBERT [9] 55.8\nBioELECTRa [28] 64.2\nBioLinkBERTbase [29] 70.2\nBioLinkBERTlarge [29] 72.2\nBioGPT 78.2\nexperts with ten currently known hallmarks of cancer [17].\nWe follow the same training/test split as in [8]. We use the\ncontinuous embedding with length=1 as the prompt and format\nthe label into the target sequence as described before. We ﬁne-\ntune GPT-2 medium and BioGPT for 20000 steps with a peak\nlearning rate 10 −5 and 1000 warm-up steps. Micro-F1 score is\nmeasured and reported for comparison.\nTable 6.Results on HoC document classiﬁcation task\nModel F1\nBioBERT [10] 81.54\nPubMedBERT [9] 82.32\nPubMedBERTlarge 82.70\nBioLinkBERTbase 84.35\nGPT-2medium 81.84\nBioGPT 85.12\nWe can see from the results in Table 6 that BioGPT\nachieves 85.12% accuracy with 3.28% improvement over general\ndomain GPT-2, and surpasses BioBERT, PubMedBERT\nand BioLinkBERT with 3 .58%, 2 .8%, 0 .77% improvements\nrespectively.\nText Generation\nGPT, GPT-2 and GPT-3 demonstrate remarkable text\ngeneration ability. Given words, phrases or simple sentences as\npreﬁx, they can continue to generate text that are syntactically\ncorrect and semantically smooth conditioning on the given text.\nWe are also curious about the text generation ability of the\npre-trained BioGPT in the biomedical domain, and how does\ngeneral domain GPT-2 perform in the biomedical domain.\nWe evaluate the biomedical text generation ability of\nBioGPT and GPT-2medium. Specially, we extract all the entities\nwithin the triplets from the KD-DTI test set (i.e., drugs and\ntargets). Then for each drug/target name, we provide it to the\nlanguage model as the preﬁx and let the model generate text\nconditioned on it. We then investigate whether the generated\ntext is meaningful and ﬂuent.\nFor this task, no objective evaluation metric is reported\nhere. Instead, we provide a few examples here for\ndemonstration.\nFrom the results in Table 7, we can see that: (1) Given\nrelatively common names as input, for example in the ﬁrst\ntwo cases (i.e., Bicalutamide and JAK-3), GPT-2 can generate\nmeaningful and ﬂuent text that is related to the word\nand biomedicine, while BioGPT generates more speciﬁc and\nprofessional descriptions. (2) When given some uncommon\nnames (e.g., in the Apricitabine and Xylazine cases), GPT-\n2 cannot generate meaningful descriptions while BioGPT still\nBioGPT 7\nTable 7.Example of generated text\nInput Model Text\nBicalutamide GPT-2\nBicalutamide, a synthetic bicalutamide derivative, was found to increase the cellular proliferation of C.\nelegans cells. This eﬀect was not observed with the other synthetic bicalutamide derivatives. This is in\nline with our previous work which showed that bicalutamide increases the proliferation of C. elegans cells\nbut not in the presence of synthetic bicalutamide.\nBioGPT\nBicalutamide (Casodex) is an androgen receptor (AR) antagonist approved for the treatment of metastatic\ncastration-resistant prostate cancer (mCRPC) in patients who have progressed on or are ineligible for\ndocetaxel chemotherapy, as well as for the treatment of early-stage prostate cancer in men who have not\nprogressed on or are ineligible for docetaxel chemotherapy, as well as for the treatment of metastatic\ncastration-sensitive prostate cancer (mCSPC) in men who have not progressed on or are ineligible for\ndocetaxel chemotherapy.\nJanus kinase\n3 (JAK-3) GPT-2\nJanus kinase 3 (JAK-3) mediates the conversion of glucose to glycogen in skeletal muscle. The increase in\nglucose uptake by skeletal muscle is believed to be associated with a reduction in muscle protein breakdown\nand with the release of ketone bodies, which contribute to the metabolism of glucose.\nBioGPT\nJanus kinase 3 (JAK-3) is a member of the Janus kinase (JAK) family of non-receptor tyrosine kinases\nand plays an important role in the regulation of cell proliferation, diﬀerentiation, survival, migration, and\nangiogenesis.\nApricitabine GPT-2\nApricitabine, duloxetine, and citalopram) were investigated in patients with a history of treatment-\nresistant depression (n = 10). We found that treatment with the three drugs signiﬁcantly decreased\ndepressive symptoms, but not depressive symptoms over the next 12 weeks. We conclude that\nantidepressant drugs that reduce depressive symptoms and reduce depressive symptoms over time may be\nbeneﬁcial in patients with recurrent depression.\nBioGPT\nApricitabine is an oral prodrug of 5-aza-2 ’-deoxycytidine (5-aza-CdR), a DNA methyltransferase (DNMT)\ninhibitor, which has been approved by the US Food and Drug Administration (FDA) for the treatment\nof myelodysplastic syndrome (MDS) and acute myeloid leukemia (AML) in combination with low-dose\ncytarabine (Ara-C) and granulocyte colony-stimulating factor (G-CSF) for patients with intermediate-2\nor high-risk MDS or AML.\nXylazine GPT-2\nXylazine, the ”active ingredient” in ”bath salts” (see above) has been linked to numerous deaths, including\nthose of former University of California, Berkeley, professor Andrew Anglin, who posted a video of himself\nhaving a bath ﬁlled with it.\nBioGPT\nXylazine is an alpha 2-adrenoceptor agonist that has been used as a sedative and analgesic in veterinary\nmedicine for many years, but its eﬀects on the cardiovascular system have not been extensively studied in\nthe dog, and its eﬀects on the central nervous system (CNS) have not been well characterized in the dog,\ndespite the fact that xylazine has been widely used as a sedative and analgesic in veterinary medicine for\nmore than 30 years.\nPsoralen GPT-2\nPsoralen, the city of the spiritualist, and had been given to him by God, who wished to show him the\nway of salvation, and to draw him back to the true life of Christ, and to bring him into communion with\nhis fellow-man.\nBioGPT\nPsoralen photochemotherapy (PUVA) is a well-established treatment for psoriasis, but its use is limited\nby the risk of skin cancer, particularly squamous cell carcinoma (SCC) of the head and neck (H & N),\nwhich is the most common site of skin cancer in the United States (US)\nCP-673451 GPT-2 CP-673451) and 4-5-6-3-1-2-3-4-6-7-8-9-10-11-12-13-14-15-16-17-18-19-20-21-22-23-24-25-26-27-28-29-30-\n31-32-33-34-35-36-37-38-39-40-41-42-43-44-45-46-47-48-49-50...\nBioGPT\nCP-673451 is a potent, selective, and orally active inhibitor of human neutrophil elastase (HNE) and\nhuman cathepsin G (CatG) with in vitro and in vivo anti-inﬂammatory activity in a variety of animal\nmodels of inﬂammation and in a model of acute lung injury (ALI) in the rat induced by intratracheal\ninstillation of lipopolysaccharide (LPS) and tumor necrosis factor-alpha (TNF-alpha), a model of acute\nlung injury (ALI) in which neutrophils play an important role.\nBIIB-021 GPT-2\nBIIB-021A-2-1.pdf. Wright, C. (2002). ”The War on Terror: A Legal Theory of Terrorist Attacks,” in\nL.E.R. Wright, S.R. Warrick, and J.H. Rothman (Eds.), The War on Terror: Essays in Honor of Michael\nT. Klare (New York: Oxford University Press), 143-57.\nBioGPT\nBIIB-021 is a novel, orally active, non-peptide bradykinin B2 receptor antagonist with potent and long-\nlasting anti-inﬂammatory activity in animal models of acute and chronic inﬂammation and in a rat model\nof adjuvant-induced arthritis (AIA), an animal model of rheumatoid arthritis (RA), and in a rat model of\ncollagen-induced arthritis (CIA), an animal model of collagen-induced arthritis (CIA), in which arthritis\nis induced by immunization with bovine type II collagen (CII).\n8 Renqian Luo et al.\nTable 8.Generated text of keywords of interest\nInput Model Text\nCOVID-19 GPT-2\nCOVID-19-0067-7 Hip-hop-romance-detective-franchise-unveiled-in-coco-houston\nhttp://www.pinterest.com/pin/71178278499149422/ Black Widow: World War II’s Greatest Widow?\nhttp://www.pinterest.com/pin/71177842668650112/\nBioGPT\nCOVID-19 is a global pandemic caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-\n2), the causative agent of coronavirus disease 2019 (COVID-19), which has spread to more than 200\ncountries and territories, including the United States (US), Canada, Australia, New Zealand, the United\nKingdom (UK), and the United States of America (USA), as of March 11, 2020, with more than 800,000\nconﬁrmed cases and more than 800,000 deaths.\nSARS-CoV-2 GPT-2 SARS-CoV-2-CKM-T1-1/pdf/T1/SARS-CoV-2-CKM-T1-1.pdf Cancer http://www.cancer.gov/cancer\ntopics/programs/tox/tox-programs.html Other Research http://www.cdc.gov/cancer/cancer/index.html\nBioGPT SARS-CoV-2 is the causative agent of COVID-19, a severe acute respiratory syndrome (SARS) that has\ninfected more than 390,000 people worldwide and killed more than 250,000 people.\nThe\ntreatment\nof COVID-19\nGPT-2\nThe treatment of COVID-19 is similar to that of COVID-20 and COVID-22, in which COVID-19 is treated\nwith a variety of ionic, dopaminergic and serotonergic agents. In addition to its eﬀects on COVID-19, the\nuse of COVID-19 has been suggested to exert eﬀects on the function of other dopamine neurons, including\ndopamine D2 receptors, D3 receptors and serotonin transporters. These eﬀects include attenuation of the\nincrease in dopamine release induced by the central nervous system stimulant drug amphetamine and its\nmetabolites.\nBioGPT\nThe treatment of COVID-19 with remdesivir (remdesivir for COVID-19) has been approved by the US\nFood and Drug Administration (FDA) for the treatment of severe acute respiratory syndrome coronavirus 2\n(SARS-CoV-2) infection in patients with conﬁrmed SARS-CoV-2 infection and is currently being evaluated\nin clinical trials for the treatment of COVID-19 in patients with conﬁrmed SARS-CoV-2 infection, as well\nas in patients with suspected or conﬁrmed SARS-CoV-2 infection.\nThe drug\nthat can treat\nCOVID-19 is\nGPT-2\nThe drug that can treat COVID-19 is not eﬀective enough to cure patients with this rare genetic disease,\nbut it could help patients who have it. Researchers say this ﬁnding could mean that a drug could be\ndeveloped that would help patients with this rare genetic disorder. The study, published in the journal\nMolecular Psychiatry, also looked at how a drug might be used to treat patients with COPD, which causes\nchronic pain and lung damage. Researchers say the study could also lead to new drugs that treat COPD.\nBioGPT\nThe drug that can treat COVID-19 is hydroxychloroquine (HCQ), which has been shown to inhibit the\nreplication of the SARS-CoV-2 virus in cell culture and in animal models of COVID-19, and has been\napproved by the US Food and Drug Administration (FDA) for the treatment of patients with COVID-\n19 in March 2020, and by the European Medicines Agency (EMA) for the treatment of patients with\nCOVID-19 in April 2020, and by the European Medicines Agency (EMA) for the treatment of patients\nwith COVID-19 in May 2020.\nOmicron\nvariants of\nSARS-CoV-2\nGPT-2\nOmicron variants of SARS-CoV-2 have been detected in the blood of patients who contracted SARS-\nCoV-2 and survived it, but it is unclear whether these variants were caused by SARS-CoV-2 infection\nor if the SARS-CoV-2 variant in the blood was due to a misidentiﬁcation of SARS-CoV-2 from other\nSARS strains. We found that a polymorphism in the gene encoding the SARS-CoV-2-speciﬁc viral surface\nprotein was associated with SARS-CoV-2 infection in a cohort of patients with SARS-CoV-2 infection\nwho had an active SARS infection, suggesting that SARS-CoV-2 may be able to infect the host during an\nactive infection.\nBioGPT\nOmicron variants of SARS-CoV-2 have been isolated from patients with severe acute respiratory syndrome\n(SARS) and have been shown to be highly pathogenic in mice and ferrets, suggesting that they may play\na role in the pathogenesis of SARS-CoV-2 infection and the development of severe disease in patients with\nSARS-CoV-2 infection.\ngenerates speciﬁc descriptions. Especially in the Apricitabine\ncase, GPT-2 seems to generate a piece of text that comes\nfrom a speciﬁc scientiﬁc paper while BioGPT generates more\ngeneral description. (3) When given some very uncommon and\ndomain speciﬁc names that even lose semantic information from\ntheir surface names (e.g., Psoralen, CP-673451 and BIIB-021),\nGPT-2 trained on general completely failed to generate any\ninformative text. Given Psoralen, GPT-2 treats it as a city\nname and generates some text though ﬂuent but unrelated to\nthe given name. Given CP-673451, GPT-2 even begins to count\nnumbers. Given BIIB-021, GPT-2 treats it as a name of a pdf\ndocument. For these types, BioGPT is still able to generate\ntext that describes the names or is highly related to them.\nBesides these samples, we also manually input several\nkeywords or phrases that are of interest (e.g., COVID-19 related\nterms) and see what GPT-2 and our BioGPT generate. The\nresults are listed in Table 8, where we input many COVID-\n19 related key words/phrases as the preﬁx for the language\nmodel to condition on. We can see that GPT-2 treats the term\n“COVID-19” and “SARS-CoV-2” as some codes within a link\nor ﬁle name rather the entities we care about while BioGPT can\ngenerate clear descriptions. More interestingly, when prompting\n“The drug that can treat COVID-19 is”, BioGPT is able to\nBioGPT 9\nTable 9.Results on KD-DTI with diﬀerent target formats\nTarget format Precision Recall F1\n<head> head entity <tail> tail entity <relation> relation 38.21 40.21 37.32\nsvo (head entity relation tail entity) 37.95 37.77 36.57\nis-of (head entity is the relation of tail entity) 39.37 39.11 37.77\nrel-is (the relation between head entity and tail entity is relation) 38.93 40.70 38.38\nanswer it with the drug “hydroxychloroquine” which is indeed\nnoticed at MedlinePlus 8. Notice that GPT-2 is pre-trained on\nthe corpus before COVID-19 while BioGPT is pre-trained on\nthe corpus before 2021 that contains COVID-19 information,\ntherefore it is not surprising that BioGPT performs much\nbetter than GPT-2 on COIVD-19 related key words in Table 8.\nHowever, in the last example in Table 8, both models do not\nhave any knowledge of the Omicron variants of SARS-CoV-2\nwhich appear in the late 2021, while BioGPT still generates\nmore ﬂuent and relevant text compared to GPT-2.\nOverall, we can see that BioGPT pre-trained on in-domain\nbiomedical literature from scratch performs better than general\ndomain GPT-2 across various biomedical NLP tasks, and\nperforms better than most previous methods on respective\ntasks, achieving state-of-the-art on four out of six tasks.\nAblation Study\nIn this section, we conduct ablation study on the prompt design\nand the target sequence format of the label.\nTarget Sequence Format\nPrevious works [24 ? , 25, 14] directly format the labels into\nstructured formats using special tokens. Taking the triplet\ngeneration task as an example, in REBEL [24], the triplets are\nrepresented by:\n<triplet> head entity1 <subj> tail entity1 <obj> relation1\n<triplet> head entity2 <subj> tail entity2 <obj> relation2 ··· ,\nwhere <triplet>, <subj> and <obj> are special tokens to\nrepresent the start of the head entity, the tail entity and the\nrelation. [24, 14, 25] use similar method to process the targets.\nAlthough these methods achieved promising results in their\ntasks respectively, such formulation pattern is not the best\nchoice for BioGPT. Previous works use an encoder-decoder\nframework, where two separated modules are leveraged to\nprocess the input (by the encoder) and generate the answers\n(by the decoder). The two modules can be trained to ﬁt the\ntwo diﬀerent types of sequences (natural language sequence v.s.\nstructured sequence).\nIn contrast, in BioGPT, we use a uniﬁed module to\nencode context and generate answers. Intuitively, it is better\nto maintain the format consistency between the inputs and\nanswers. Consequently, instead of the structured target format\nwith special tokens as in previous works, we format the label\nwithin a natural language sentence for the language model to\nsmoothly learn and generate. However, there are also various\npatterns that can be used to construct the target sentence.\nWe explore several target sequence formats, including the\nstructured format, on the KD-DTI dataset for end-to-end\nrelation extraction task. We ﬁx the prompt to continuous\n8 https://medlineplus.gov/druginfo/meds/a601240.html\nembeddings with length=9. From the results in Table 9 we\ncan see that the formats in natural language perform better\nthan structured format, and that the rel-is format performs\nthe best among all the formats in terms of F1 which provides\na more semantically smooth and clear description. We also\nconduct experiments on BC5CDR and DDI to further compare\nthe structure format and the rel-is format. The F1 scores of\nthe structure format on BC5CDR and DDI are 42.85 and 38.60,\nwhile those two scores with rel-is format are 44.98 and 40.76,\nwhich further verify our conclusion.\nPrompt Design\nTable 10.Results on KD-DTI with diﬀerent prompts\nPrompts Precision Recall F1\nwe have that 38.55 38.37 36.95\nin conclusion, 39.03 39.45 37.76\nwe can conclude that 39.56 39.88 38.16\ncontinuous embeddings (length=1) 39.50 39.71 38.06\ncontinuous embeddings (length=5) 39.57 39.63 38.09\ncontinuous embeddings (length=9) 38.93 40.70 38.38\ncontinuous embeddings (length=13) 39.48 39.17 38.60\ncontinuous embeddings (length=17) 39.82 39.60 38.28\nWe conduct experiment with manually designed hard\nprompts and continuous embedding soft prompts on the KD-\nDTI extraction task. We ﬁx the target format to the rel-is\nformat (i.e., ”the relation between head entity and tail entity\nis relation”). From the results in Table 10 we can see that the\nbest performing prompt is continuous embeddings with length\nof 13 virtual tokens. Moreover, we have several observations: (1)\nDiﬀerent manually designed hard prompts result in diﬀerent\nperformance and more instructive and informative prompt\n(e.g., “we can conclude that”) achieve better performance. (2)\nGenerally, continuous embedding soft prompts perform better\nthan manually designed hard prompts. (3) The performance of\nthe continuous embedding soft prompts are roughly irrelevant\nto the length. In our previous experiments, we empirically\nchoose length=9 according to the performance on validation\nset.\nConclusion\nIn this work, we proposed BioGPT, a generative pre-trained\nTransformer language model for biomedical text generation\nand mining. We adopted GPT-2 as our backbone model\nand pre-trained on 15 M PubMed abstracts corpus. We\ncarefully designed and investigated the prompt and the\ntarget sequence format when applying pre-trained BioGPT\nto downstream tasks. We applied the pre-trained BioGPT to\nbiomedical NLP tasks: end-to-end relation extraction task,\n10 Renqian Luo et al.\nquestion answering task, document classiﬁcation task and text\ngeneration task. BioGPT achieves SOTA results on three end-\nto-end relation extraction tasks and one question answering\ntask. It also demonstrates better biomedical text generation\nability compared to GPT-2 on the text generation task. For\nfuture work, we plan to train larger scale BioGPT on larger\nscale biomedical data and apply to more downstream tasks.\nKey Points\nOur contributions are summarized as follows:\n• We propose BioGPT, a generative pre-trained\nTransformer language model on biomedical domain.\nBioGPT can be used for biomedical literature text\ngeneration and mining.\n• BioGPT achieves state-of-the-art results on four\nbenchmarks: BC5CDR, KD-DTI and DDI end-to-end\nrelation extraction task, and PubMedQA question\nanswering task. We also demonstrate the capability\nof biomedical text generation of BioGPT compared\nto standard GPT trained on general domain.\n• We study the prompt design and the target sequence\ndesign when applying BioGPT to downstream tasks\nand ﬁnd that target sequence with natural language\nsemantics are better than structured prompts\nexplored in previous works.\nScaling to Larger Size\nWe also scaled our model to larger size. We built BioGPT-\nLarge, based on the GPT-2 XL architecture (the largest version\nof GPT-2), with 1.5B model parameters. We ﬁne-tune and\nevaluate its performance on the downstream tasks, as shown\nin Table 11.\nTable 11.Performance of BioGPT-Large ﬁne-tuned on downstream\ntasks\nTask Metric Performance\nBC5CDR F1 50.12\nKD-DTI F1 38.39\nDDI F1 44.89\nPubMedQA Accuracy 81.0\nHoC F1 84.40\nReferences\n1. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,\nOmer Levy, and Samuel R. Bowman. GLUE: A multi-\ntask benchmark and analysis platform for natural language\nunderstanding. In International Conference on Learning\nRepresentations, 2019.\n2. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings\nof the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 4171–4186, 2019.\n3. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly\noptimized bert pretraining approach. arXiv preprint\narXiv:1907.11692, 2019.\n4. Kevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. Electra: Pre-training text\nencoders as discriminators rather than generators. In\nInternational Conference on Learning Representations ,\n2019.\n5. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. Improving language understanding by generative\npre-training. 2018.\n6. Alec Radford, Jeﬀrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. Language models are\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n7. Tom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\net al. Language models are few-shot learners. Advances\nin neural information processing systems , 33:1877–1901,\n2020.\n8. Yifan Peng, Shankai Yan, and Zhiyong Lu. Transfer\nlearning in biomedical natural language processing: An\nevaluation of BERT and ELMo on ten benchmarking\ndatasets. In Proceedings of the 18th BioNLP Workshop\nand Shared Task , pages 58–65, Florence, Italy, August\n2019. Association for Computational Linguistics.\n9. Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas,\nNaoto Usuyama, Xiaodong Liu, Tristan Naumann,\nJianfeng Gao, and Hoifung Poon. Domain-speciﬁc\nlanguage model pretraining for biomedical natural language\nprocessing. ACM Transactions on Computing for\nHealthcare (HEALTH), 3(1):1–23, 2021.\n10. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\nBioBERT: a pre-trained biomedical language representation\nmodel for biomedical text mining. Bioinformatics,\n36(4):1234–1240, 09 2019.\n11. Milad Moradi, Kathrin Blagec, Florian Haberl, and\nMatthias Samwald. Gpt-3 models are poor few-shot\nlearners in the biomedical domain. arXiv preprint\narXiv:2109.02555, 2021.\n12. Bernal Jim´ enez Guti´ errez, Nikolas McNeal, Clay\nWashington, You Chen, Lang Li, Huan Sun, and Yu Su.\nThinking about gpt-3 in-context learning for biomedical\nie? think again. arXiv preprint arXiv:2203.08410 , 2022.\n13. Jiao Li, Yueping Sun, Robin J. Johnson, Daniela Sciaky,\nChih-Hsuan Wei, Robert Leaman, Allan Peter Davis,\nCarolyn J. Mattingly, Thomas C. Wiegers, and Zhiyong Lu.\nBioCreative V CDR task corpus: a resource for chemical\ndisease relation extraction. Database, 2016, 05 2016.\n14. Yutai Hou, Yingce Xia, Lijun Wu, Shufang Xie, Yang\nFan, Jinhua Zhu, Wanxiang Che, Tao Qin, and Tie-Yan\nLiu. Discovering drug-target interaction knowledge from\nbiomedical literature. arXiv preprint arXiv:2109.13187 ,\n2021.\n15. Mar´ ıa Herrero-Zazo, Isabel Segura-Bedmar, Paloma\nMart´ ınez, and Thierry Declerck. The ddi corpus: An\nannotated corpus with pharmacological substances and\ndrug–drug interactions. Journal of biomedical informatics,\n46(5):914–920, 2013.\n16. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen,\nand Xinghua Lu. Pubmedqa: A dataset for biomedical\nresearch question answering. In Proceedings of the 2019\nConference on Empirical Methods in Natural Language\nBioGPT 11\nProcessing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP) , pages\n2567–2577, 2019.\n17. Simon Baker, Ilona Silins, Yufan Guo, Imran Ali, Johan\nH¨ ogberg, Ulla Stenius, and Anna Korhonen. Automatic\nsemantic classiﬁcation of scientiﬁc literature according to\nthe hallmarks of cancer. Bioinformatics, 32(3):432–440,\n2016.\n18. Iz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: A\npretrained language model for scientiﬁc text. InProceedings\nof the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-\nIJCNLP), pages 3615–3620, Hong Kong, China, November\n2019. Association for Computational Linguistics.\n19. Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H\nLehman, Mengling Feng, Mohammad Ghassemi, Benjamin\nMoody, Peter Szolovits, Leo Anthony Celi, and Roger G\nMark. Mimic-iii, a freely accessible critical care database.\nScientiﬁc data, 3(1):1–9, 2016.\n20. Giacomo Miolo, Giulio Mantoan, and Carlotta\nOrsenigo. Electramed: a new pre-trained language\nrepresentation model for biomedical nlp. arXiv preprint\narXiv:2104.09585, 2021.\n21. Yannis Papanikolaou and Andrea Pierleoni. Dare: Data\naugmented relation extraction with gpt-2. arXiv preprint\narXiv:2004.13845, 2020.\n22. Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon\nKim, and David Sontag. Large language models are\nzero-shot clinical information extractors. arXiv preprint\narXiv:2205.12689, 2022.\n23. Difeng Wang, Wei Hu, Ermei Cao, and Weijian Sun.\nGlobal-to-local neural networks for document-level relation\nextraction. arXiv preprint arXiv:2009.10359 , 2020.\n24. Pere-Llu´ ıs Huguet Cabot and Roberto Navigli. Rebel:\nRelation extraction by end-to-end language generation.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2021 , pages 2370–2381, 2021.\n25. John Giorgi, Gary D Bader, and Bo Wang. A sequence-to-\nsequence approach for document-level relation extraction.\narXiv preprint arXiv:2204.01098 , 2022.\n26. Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui\nZhao, Kai Chen, Mohammad Norouzi, and Quoc V\nLe. Qanet: Combining local convolution with global\nself-attention for reading comprehension. arXiv preprint\narXiv:1804.09541, 2018.\n27. Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki\nTakeda, and Yuji Matsumoto. Luke: deep contextualized\nentity representations with entity-aware self-attention.\narXiv preprint arXiv:2010.01057 , 2020.\n28. Kamal raj Kanakarajan, Bhuvana Kundumani, and\nMalaikannan Sankarasubbu. BioELECTRA:pretrained\nbiomedical text encoder using discriminators. In\nProceedings of the 20th Workshop on Biomedical\nLanguage Processing, pages 143–154, Online, June 2021.\nAssociation for Computational Linguistics.\n29. Michihiro Yasunaga, Jure Leskovec, and Percy Liang.\nLinkbert: Pretraining language models with document\nlinks. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Volume\n1: Long Papers), pages 8003–8016, 2022.\n30. George Tsatsaronis, Georgios Balikas, Prodromos\nMalakasiotis, Ioannis Partalas, Matthias Zschunke,\nMichael R Alvers, Dirk Weissenborn, Anastasia Krithara,\nSergios Petridis, Dimitris Polychronopoulos, et al. An\noverview of the bioasq large-scale biomedical semantic\nindexing and question answering competition. BMC\nbioinformatics, 16(1):1–28, 2015.\n31. Anastasios Nentidis, Konstantinos Bougiatiotis, Anastasia\nKrithara, and Georgios Paliouras. Results of the seventh\nedition of the bioasq challenge. In Joint European\nConference on Machine Learning and Knowledge\nDiscovery in Databases, pages 553–568. Springer, 2019.\n32. Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey,\nand Daniel S Weld. Specter: Document-level representation\nlearning using citation-informed transformers. arXiv\npreprint arXiv:2004.07180, 2020.\n33. Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and\nJun Zhao. Relation classiﬁcation via convolutional deep\nneural network. In Proceedings of COLING 2014, the 25th\ninternational conference on computational linguistics:\ntechnical papers, pages 2335–2344, 2014.\n34. Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen Li,\nHongwei Hao, and Bo Xu. Attention-based bidirectional\nlong short-term memory networks for relation classiﬁcation.\nIn Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 2:\nShort Papers) , pages 207–212, Berlin, Germany, August\n2016. Association for Computational Linguistics.\n35. Changzhi Sun, Yeyun Gong, Yuanbin Wu, Ming Gong,\nDaxin Jiang, Man Lan, Shiliang Sun, and Nan Duan.\nJoint type inference on entities and relations via graph\nconvolutional networks. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics,\npages 1361–1370, 2019.\n36. Yue Yuan, Xiaofei Zhou, Shirui Pan, Qiannan Zhu, Zeliang\nSong, and Li Guo. A relation-speciﬁc attention network for\njoint entity and relation extraction. InIJCAI, volume 2020,\npages 4054–4060, 2020.\n37. Jie Liu, Shaowei Chen, Bingquan Wang, Jiaxin Zhang,\nNa Li, and Tong Xu. Attention as relation: learning\nsupervised multi-head self-attention for relation extraction.\nIn Proceedings of the Twenty-Ninth International\nConference on International Joint Conferences on\nArtiﬁcial Intelligence, pages 3787–3793, 2021.\n38. Zhepei Wei, Jianlin Su, Yue Wang, Yuan Tian, and\nYi Chang. A novel cascade binary tagging framework for\nrelational triple extraction. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 1476–1488, 2020.\n39. Tsu-Jui Fu, Peng-Hsuan Li, and Wei-Yun Ma. Graphrel:\nModeling text as relational graphs for joint entity and\nrelation extraction. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics,\npages 1409–1418, 2019.\n40. Yucheng Wang, Bowen Yu, Yueyang Zhang, Tingwen Liu,\nHongsong Zhu, and Limin Sun. TPLinker: Single-stage\njoint extraction of entities and relations through token\npair linking. In Proceedings of the 28th International\nConference on Computational Linguistics , pages 1572–\n1582, Barcelona, Spain (Online), December 2020.\nInternational Committee on Computational Linguistics.\n41. Zhiheng Yan, Chong Zhang, Jinlan Fu, Qi Zhang, and\nZhongyu Wei. A partition ﬁlter network for joint entity and\nrelation extraction. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Processing ,\npages 185–197, Online and Punta Cana, Dominican\nRepublic, November 2021. Association for Computational\n12 Renqian Luo et al.\nLinguistics.\n42. Xiangrong Zeng, Daojian Zeng, Shizhu He, Kang Liu,\nand Jun Zhao. Extracting relational facts by an end-to-\nend neural model with copy mechanism. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) ,\npages 506–514, 2018.\n43. Ranran Haoran Zhang, Qianying Liu, Aysa Xuemo Fan,\nHeng Ji, Daojian Zeng, Fei Cheng, Daisuke Kawahara, and\nSadao Kurohashi. Minimize exposure bias of seq2seq models\nin joint entity and relation extraction. In Findings of\nthe Association for Computational Linguistics: EMNLP\n2020, pages 236–246, 2020.\n44. Dianbo Sui, Yubo Chen, Kang Liu, Jun Zhao, Xiangrong\nZeng, and Shengping Liu. Joint entity and relation\nextraction with set prediction networks. arXiv preprint\narXiv:2011.01675, 2020.\n45. Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nFuru Wei, and Ming Zhou. Reinforced mnemonic reader\nfor machine reading comprehension. arXiv preprint\narXiv:1705.02798, 2017.\n46. Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural\nmachine translation of rare words with subword units. In\nProceedings of the 54th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers) ,\npages 1715–1725, 2016.\n47. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez,  Lukasz Kaiser, and\nIllia Polosukhin. Attention is all you need. Advances in\nneural information processing systems, 30, 2017.\n48. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. Pre-train,\nprompt, and predict: A systematic survey of prompting\nmethods in natural language processing. arXiv preprint\narXiv:2107.13586, 2021.\n49. Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing\ncontinuous prompts for generation. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers) , pages 4582–4597, Online,\nAugust 2021. Association for Computational Linguistics.\n50. Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael Auli.\nfairseq: A fast, extensible toolkit for sequence modeling. In\nProceedings of NAACL-HLT 2019: Demonstrations, 2019.\n51. Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In ICLR (Poster), 2015.\n52. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pierric\nCistac, Tim Rault, R´ emi Louf, Morgan Funtowicz, Joe\nDavison, Sam Shleifer, Patrick von Platen, Clara Ma,\nYacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao,\nSylvain Gugger, Mariama Drame, Quentin Lhoest, and\nAlexander M. Rush. Transformers: State-of-the-art\nnatural language processing. In Proceedings of the 2020\nConference on Empirical Methods in Natural Language\nProcessing: System Demonstrations, pages 38–45, Online,\nOctober 2020. Association for Computational Linguistics.\n53. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. BART: Denoising\nsequence-to-sequence pre-training for natural language\ngeneration, translation, and comprehension. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7871–7880, Online, July\n2020. Association for Computational Linguistics.",
  "topic": "Generative grammar",
  "concepts": [
    {
      "name": "Generative grammar",
      "score": 0.749281644821167
    },
    {
      "name": "Transformer",
      "score": 0.6855605840682983
    },
    {
      "name": "Computer science",
      "score": 0.6161397099494934
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4785062372684479
    },
    {
      "name": "Natural language processing",
      "score": 0.40144556760787964
    },
    {
      "name": "Engineering",
      "score": 0.1577337086200714
    },
    {
      "name": "Electrical engineering",
      "score": 0.07079517841339111
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}