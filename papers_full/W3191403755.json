{
  "title": "Transfer Learning of Pre-trained Transformers for Covid-19 Hoax Detection in Indonesian Language",
  "url": "https://openalex.org/W3191403755",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5074961839",
      "name": "Lya Hulliyyatus Suadaa",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5053691313",
      "name": "Ibnu Santoso",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5025355336",
      "name": "Amanda Tabitha Bulan Panjaitan",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3127461984",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2939455697",
    "https://openalex.org/W3086966320",
    "https://openalex.org/W3110149585",
    "https://openalex.org/W3135415245",
    "https://openalex.org/W3084756066",
    "https://openalex.org/W3161590189",
    "https://openalex.org/W3028129654",
    "https://openalex.org/W3131684216",
    "https://openalex.org/W2769470793",
    "https://openalex.org/W4293228415",
    "https://openalex.org/W3164810016",
    "https://openalex.org/W3116295307",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3164073370"
  ],
  "abstract": "Nowadays, internet has become the most popular source of news. However, the validity of the online news articles is difficult to assess, whether it is a fact or a hoax. Hoaxes related to Covid-19 brought a problematic effect to human life. An accurate hoax detection system is important to filter abundant information on the internet. In this research, a Covid-19 hoax detection system was proposed by transfer learning of pre-trained transformer models. Fine-tuned original pre-trained BERT, multilingual pre-trained mBERT, and monolingual pre-trained IndoBERT were used to solve the classification task in the hoax detection system. Based on the experimental results, fine-tuned IndoBERT models trained on monolingual Indonesian corpus outperform fine-tuned original and multilingual BERT with uncased versions. However, the fine-tuned mBERT cased model trained on a larger corpus achieved the best performance.",
  "full_text": "IJCCS (Indonesian Journal of Computing and Cybernetics Systems) \nVol.15, No.3, July 2021, pp. 317~326 \nISSN (print): 1978-1520, ISSN (online): 2460-7258 \nDOI: https://doi.org/10.22146/ijccs.66205                          ◼    317 \n  \nReceived May 30th,2021; Revised July 31th, 2021; July 31th, 2021 \nTransfer Learning of Pre-trained Transformers  \nfor Covid-19 Hoax Detection in Indonesian Language \n \n \n \nLya Hulliyyatus Suadaa*1, Ibnu Santoso2, Amanda Tabitha Bulan Panjaitan3 \n1,2,3Program Studi Komputasi Statistik, Politeknik Statistika STIS, Jakarta, Indonesia \ne-mail: *1lya@stis.ac.id, 2ibnu@stis.ac.id, 3221709539@stis.ac.id  \n \n \nAbstrak \nPada saat ini, internet menjadi sumber berita paling populer. Akan tetapi, validitas dari \nartikel berita online sulit dinilai, apakah artikel tersebut berupa fakta atau hoaks. Hoaks terkait \nCovid-19 membawa efek problematik terhadap kehidupan manusia. Sistem pendeteksi hoaks \nyang akurat menjadi penting untuk menyarin g i nformasi yang tersebar di internet. Pada \npenelitian ini, sistem pendeteksi hoaks diusulkan dengan menerapkan transfer learning dari \npre-trained transformer model. Fine-tuned original pre-trained BERT, multilingual pre-trained \nmBERT, dan monolingual pre -trained IndoBERT  digunakan untuk menyelesaikan tugas \nklasifikasi pada sistem pendeteksi hoaks . Berdasarkan hasil eksperimen,  model fine-tuned \nIndoBERT yang dilatih di korpus  monolingual berbahasa Indonesia  lebih baik akurasinya \ndibandingkan fine-tuned orig inal dan multilingual BERT dengan versi  uncased. Akan tetapi , \nfine-tuned model mBERT dengan versi cased yang dilatih di corpus yang lebih besar mencapai \nkinerja yang paling baik dibandingkan dengan model lainnya. \n \nKata kunci—deteksi hoaks, transfer learning, pre-trained transformer, pemrosesan teks \nberbahasa Indonesia \n \n \n \nAbstract \n Nowadays, internet has become the most popular source of news. However, the validity \nof the online news articles is difficult to assess, whether it is a fact or a hoax. Hoaxes related to \nCovid-19 brought a problematic effect to human life. An accurate hoax detection system is \nimportant to filter abundant information on the internet.  In this research,  a Covid-19 hoax \ndetection system was proposed by transfer learning of pre -trained transformer models. Fine-\ntuned original pre -trained BERT, multilingual pre -trained mBERT, and monolingual pre -\ntrained IndoBERT were used to solve the classification task in the hoax detection system. Based \non the experimental results, fine-tuned IndoBERT models trained on monolingual Indonesian \ncorpus outperform fine-tuned original and multilingual BERT with uncased versions. However, \nthe fine-tuned mBERT cased model trained on a larger corpus achieved the best performance. \n \nKeywords—hoax detection, transfer learning, pre-trained transformer, Indonesian language \ntext processing \n \n \n \n     ◼              ISSN (print): 1978-1520, ISSN (online): 2460-7258 \nIJCCS  Vol. 15, No. 3, July 2021 :  317 – 326 \n318 \n1. INTRODUCTION \n \nNowadays, hoaxes can spread easily through the internet, as one of the dark sides of \ninformation technology development. Information technology facilitates people  to connect with \nothers and exchange information without constraint. Information that can be consumed is also \nincreasing in terms of quantity and variety. However, the information that is exchanged is often \nfalse, inaccurate, and maybe deliberately distributed with a specific purpose, called a hoax. \nThere are several definitions of a hoax that are not much different from various \ndictionaries [13] [14] [15]. Principally, a hoax is false information, regardless of the purpose of \ndisseminating the information.  In Indonesia, it is indicated that hoax information comes from \n800 thousand websites [16]. \nHoaxes related to covid -19 ha ve been circulating in the community through social \nmedia and various news sites. More than 2,300 reports of hoaxes and conception theo ries about \ncovid-19 have been recorded. Misinformation about covid-19 has affected the lives of at least \n800 people worldwide [17]. Considering the problematic effects of covid-19 hoaxes, we intend \nto solve hoax detection problems by classifying articles into a hoax and fact.  \nSeveral studies have elaborated hoax detection tasks and proposed using classic \nclassification models such as K Nearest Neighbor (KNN) [10], na ive Bayes classifier [11] [18], \nand random forest [1]. These models need manual feature engi neering by defining \nrepresentative features beforehand. However, a deeper analysis is needed to decide on better \nfeatures. Overcoming the feature engineering problem, a deep learning model was introduced to \nsolve classification tasks without feature engine ering. Features were extracted through their \nword embeddings representing the meaning of each token in the input texts. A recent study has \nbeen proved that deep learning models are superior to classic classifiers in a hoax detection \nsystem [12]. \nDue to the complexity of deep learning architecture,  deep learning models require a lot \nof training data. However, constructing a large dataset, especially for supervised tasks, is \nexpensive. Building deep learning models using such dataset also needs more considera ble \nresources, such as high -performance computer architecture.  Avoiding the need to train a new \nmodel from scratch, public pre-trained language models were proposed. \nRecent studies have shown that pre -trained models trained on a large corpus can be \nsuccessfully solved various downstream natural language processing tasks by transfer learning. \nOne of the pre -trained language representation models outperforming many task -specific \narchitectures is BERT. BERT, Bidirectional Encoder Representation from Transforme rs, is \ndesigned to pre-train deep representation of texts from both left and right directions [2]. BERT \ncan be easily fine -tuned for a classification task by simply adding one additional classification \nlayer, thus avoiding the need to train a new model fro m scratch. Since our dataset is limited, we \naim to take advantage of a larger pre -trained language representation by transfer learning the \npre-trained models to our article classification task and develop a  accurate hoax detection \nsystem. \nThe promising re sults of BERT architecture in obtaining deeper context representation \nof input texts encourage the development of BERT trained on the different corpus. Devlin et al. \nalso offered a multilingual BERT (mBERT) [2], a pre -trained BERT trained on Wikipedia \ndocuments for 104 languages, achieving impressive performance for zero -shot cross -lingual \ntransfer [3]. Several studies incorporated mBERT in Indonesian language processing tasks, such \nas aspect -based sentiment analysis in Indonesian review datasets [7]. Even the original pre -\ntrained BERT was elaborated in other Indonesian tasks, such as hate speech classification [8] \nand summarization [9]. \nRecently, two different versions of BERT trained on Indonesian corpus have been \nreleased in the same year, called IndoBERT . The first version of IndoBERT was proposed by \nthe IndoNLU team, which is trained on a large and clean Indonesian dataset (Indo4B) collected \nfrom publicly available sources such as social media texts, blogs, news, and websites [4].  \nIJCCS  ISSN (print): 1978-1520, ISSN (online): 2460-7258  ◼ \nTransfer Learning of Pre-trained Transformers for Covid-19 Hoax … (Lya Hulliyyatus Suadaa) \n319 \n \nAnother version of In doBERT, proposed by the IndoLEM team, is trained on \nIndonesian Wikipedia, news articles (Kompas, Tempo, and Liputan6), and Web Corpus [5]. \nThese monolingual BERT trained on the Indonesian language corpus encourage further research \nexploring transfer learni ng of pre-trained BERT models to various Indonesian language \nprocessing tasks.  \nIn this study, we elaborated the original pre -trained BERT, the multilingual BERT \n(mBERT) and the monolingual IndoBERT to develop a hoax detection system and presented the \nexperimental results. The performance of each model was compared and analyzed for selecting \nthe hoax detection system with the best accuracies. \n \n \n2. METHODS \n \nWe proposed transfer learning by finetuning pre -trained transformers models for hoax \ndetection tasks. A flow chart of our proposed system is illustrated in Figure 1.  \n \n \n \nFigure 1 Hoax Detection Phases with Transfer Learning \n \n2.1 Dataset  \nWe used a dataset of Covid -19 articles in Indonesian languages collected by [1] \nconsisted of hoax articles from Turnbac khoax.id and fact articles from Detik.com. Keywords \nthat were used for selecting the Covid -19 articles in this study are “covid”, “corona”, and \n“pandemik”. Page interfaces of hoax and fact article examples are shown in Figure 2 and Figure \n3, respectively. \n \n     ◼              ISSN (print): 1978-1520, ISSN (online): 2460-7258 \nIJCCS  Vol. 15, No. 3, July 2021 :  317 – 326 \n320 \n \n \nFigure 2  A Page Example of Hoax Article \n \n \n \n \nFigure 3  A Page Example of Fact Article \n \nA classification model was developed to detect whether an article is a hoax or fact. \nExamples of articles and their classes in the dataset are shown in Table 1. \n \nIJCCS  ISSN (print): 1978-1520, ISSN (online): 2460-7258  ◼ \nTransfer Learning of Pre-trained Transformers for Covid-19 Hoax … (Lya Hulliyyatus Suadaa) \n321 \n \nTable 1  Examples of articles and their classes in dataset \nNo Article Class \nTitle Body \n1 Jokowi Nyatakan \nIndonesia Di-Lockdown \nPresiden Jokowi dalam keterangan pers terkait \nbencana nasional non alam, wabah Covid-19 di \nIstana Bogor, Jawa Barat, Minggu (1 5/3/2020) \ntidak menyatakan bahwa Indonesia di -\nlockdown… \nHoax \n2 Ternyata virus corona \ndapat diobati dengan  cara \nberendam di AIR LAUT \nTidak bisa disembuhkan hanya dengan \nberendam di laut. Pasalnya, virus Corona tidak \nmenyerang permukaan tubuh seperti kulit , \nmelainkan menyerang sel-sel di dalam tubuh… \nHoax \n3 DKI-Jateng Terbanyak, \nBegini Sebaran 4.106 \nKasus Corona Baru 15 \nNovember \nJakarta -  Penambahan kasus baru virus Corona \n(COVID-19) hari ini berjumlah 4.106 kasus. \nDari 4.000-an kasus baru itu, terbanyak berada \ndi DKI Jakarta dan Jawa Tengah… \nFact \n4 Pemprov Kaltim Dapat \nJatah 2,2 Juta Vaksin \nCOVID-19 \nSamarinda - Provinsi Kalimantan Timur \n(Kaltim) mendapatkan bantuan sebanyak 2,2 \njuta vaksin COVID -19 dari pemerintah pusat. \nProvinsi Kaltim sebelumnya tidak masuk \ndaftar 10 daerah yang menjadi prioritas \npenerima bantuan vaksin virus Corona… \nFact \n \n2. 2 Pre-processing  \nWe used a pre -trained tokenizer to transform texts into sub -word tokens avoiding out -\nof-vocabulary problems. Following [2], [CLS] token is added at the beginning of articles tokens \nand [SEP] at the end of tokens. Then, we separated the title and body of articles into two \nsegments by inserting a [SEP] token.  Then, a ll tokens were transformed into token ids . An \nexample of our tokenization process in pre-processing phase is illustrated in Figure 4. \n \n \nFigure 4  An Example of Tokenization Process \n \nIn our study, we fed the original texts as our inputs to the tokenizer.  However, when \nuncased models were used, we did case-folding by reducing all letters to lowercase. \n \n2. 3 Transfer Learning of Pre-trained Models  \nWe fine-tuned a pre-trained transformers BERT to obtain context representation of our \ninput texts. We adapt fine-tuned BERT architecture in [2] to solve the classification task for our \nhoax detection system. Our proposed fine-tuned architecture is depicted in Figure 5. \n \n     ◼              ISSN (print): 1978-1520, ISSN (online): 2460-7258 \nIJCCS  Vol. 15, No. 3, July 2021 :  317 – 326 \n322 \n \n \n \nFigure 5  Fine-tuned BERT Architecture for Hoax Detection \n \nWe assigned token embeddings representing the meaning of each token, segment \nembeddings to discriminate the title and body of the article, and position embeddings covering \nthe token position in our input sequences. The summation of these embeddings was fed to the \nTransformer layer of BERT. We used the top context [CLS] token as a representation of \nsequence tokens. Then, w e added a c lassification layer to detect whether an article is a hoax or \nnot. We used several BERT models trained on different corpus: original BERT, multilingual \nBERT, and IndoBERT.  \n2. 3.1 BERT \nAn original BERT model is trained on the BooksCorpus (800M w ords) and Wikipedia \n(2,500M words) [2]. We used BERT-based-cased and BERT-based-uncased in our experiments. \n2. 3.2 Multilingual BERT \nA multilingual BERT (mBERT) is trained on Wikipedia documents for 104 languages, \nincluding Indonesian and has been efficiently fine-tuned for document classification in several \nlanguages [2][3]. mBERT-base-cased and mBERT-base-uncased were used in our study. \n2. 3.3 IndoBERT \nThere are two kinds of IndoBERT trained on a different corpus, proposed by IndoNLU \n[4] and IndoLEM [5] teams. IndoBERT of the IndoNLU is trained on around four billion words \nof Indonesian pre-processed text data (≈ 23 GB)  from publicly available sources such as social  \nmedia texts, blogs, news, and websites [4]. IndoBERT of the IndoLEM is trained on Indonesian \nWikipedia (74M words), Indonesian news articles (55M words), and an Indonesian Web Corpus \n(90M words) [5]. Both models provided only the uncased models. \n \n \n \nIJCCS  ISSN (print): 1978-1520, ISSN (online): 2460-7258  ◼ \nTransfer Learning of Pre-trained Transformers for Covid-19 Hoax … (Lya Hulliyyatus Suadaa) \n323 \n3. RESULTS AND DISCUSSION \n \nWe implemented our models in Pytorch and used a transformer library bui lt by the \nHuggingface team [6]. For optimization in the fine-tuned phase, we used Adam as the optimizer \nwith a batch size of 32 and a learning rate of 3×10 −6. As evaluation metrics, we reported \naccuracy, precision, recall, and F1 scores. \nWe fine -tuned the model s for seven epochs . Based on our experimental results, the \nmodels tended to overfit after the seventh epoch. The improvement of accuracy scores in the  \nfine-tuned phase of our proposed models is shown in Figure 6. We validate our models for each \nepoch in the train and test set. No validation set is available in this dataset. \n \n \n \nFigure 6  Accurracies of Our Proposed Models for Each Epoch in Train and Test Sets \n \nAs reported in Figure 6, the accuracies of all models were improved after each epoch. It \nproves that transfer learning from the pre-trained models increases the model performances. We \nsuccessfully took advantage of pre -trained models trained on a l arge corpus by fine -tuning the \nmodels in our task, even with a limited dataset. The accuracy scores of fine -tuned BERT in the \ntesting sets were sharply boosted in the first three epochs, then continue with a slight increase \nfor the next epoch. Unlike the o riginal fine -tuned BERT models, fine -tuned mBERT and \nIndoBERT models were slightly increased from the first epoch, but the accuracy score was \nstarted from more than 90%. \nWe compared our proposed models with transfer learning to Random Forest without \nfeature engineering and with feature engineering, the best classic classification model s for this \ntask reported in [1] . Our proposed models did not need feature engineering since text \nrepresentations were automatically obtained through their token embeddings. Our experimental \nresults are shown in Table 2. \n     ◼              ISSN (print): 1978-1520, ISSN (online): 2460-7258 \nIJCCS  Vol. 15, No. 3, July 2021 :  317 – 326 \n324 \n \nTable 2 Experimental results \nModels Accuracy Precision Recall F1 \nRandom Forest w/o Feature Engineering [1] 93.79 89.25 98.81 93.79 \nRandom Forest w Feature Engineering [1] 96.05 92.31 100 96 \nFine-tuned BERT-base-uncased 96.38 96.57 96.37 96.38 \nFine-tuned BERT-base-cased 96.38 96.38 96.47 96.38 \nFine-tuned mBERT-base-uncased 97.16 97.27 97.15 97.16 \nFine-tuned mBERT-base-cased 97.93 97.93 97.96 97.93 \nFine-tuned IndoBERT-base-uncased (IndoNLU) 97.67 97.74 97.67 97.67 \nFine-tuned IndoBERT-base-uncased (IndoLEM) 97.67 97.78 97.67 97.67 \n \n As seen in Table 2, our fine -tuned models achieved better accuracy, precision, and F1 \nscores than reported in the previous works. The original fine -tuned BERT with uncased an d \ncased models gave a similar performance with 96.38 of accuracy and F1 scores. Unlike the \noriginal BERT, the fine -tuned mBERT with cased model reported better performance than the \nuncased one. Since the mBERT model was trained on a larger corpus in variou s l anguages, \ndifferentiate capital and not capital letters of article texts improved our hoax detection \nperformances. The cased version models were efficiently handling words with capital letters by \nusing different embeddings.  \n           Both fine-tuned IndoBERT models, IndoNLU and IndoLEM, provided only an uncased \nversion. Both models achieved similar performance with 97.67 accuracies and outperformed \nfine-tuned BERT and mBERT uncased models. IndoBERT as a monolingual pre -trained model \ngave a better score  than mBERT, a multilingual one. However, the cased version of fine -tuned \nmBERT model outperformed all fine -tuned models since mBERT -cased trained on a larger \ncorpus than others. \n \n \n4. CONCLUSIONS \n \nWe proposed transfer learning of pre -trained models to solv e t he COVID -19 hoax \ndetection task. We fine -tuned original pre-trained BERT, multilingual pre-trained mBERT, and \nmonolingual pre-trained IndoBERT to our classification task and reported the results. Our fine -\ntuned IndoBERT models trained on monolingual Ind onesian corpus outperform ed fine-tuned \noriginal and multilingual BERT with uncased versions. However, the fine -tuned mBERT cased \nmodel trained a larger corpus achieved the best performance. \n \n \nREFERENCES \n \n[1] A. T . B. Panjaitan and I. Santoso , “ Deteksi Hoak s P ada Berita Berbahasa Indonesia \nSeputar COVID-19,” Jurnal FORMAT (Teknik Informatika)., vol. 10, no. 1, p. 76, 2021 \n[Online].                                                               Available: \nhttps://publikasi.mercubuana.ac.id/index.php/format/article/view/10978. [Accessed: 26-\nMay-2021] \n[2] J. Devlin, M. Chang, K. Lee, and K. Toutanova , “ BERT: Pre -training of Deep \nBidirectional Transformers for Language Understanding ,” Proceedings of the 2019 \nConference of the North American Chapter of the Associat ion for Computational \nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , p. \n4171–4186, 2019 [Online]. Available: http  https://www.aclweb.org/anthology/N19-\n1423/. [Accessed: 26-May-2021] \nIJCCS  ISSN (print): 1978-1520, ISSN (online): 2460-7258  ◼ \nTransfer Learning of Pre-trained Transformers for Covid-19 Hoax … (Lya Hulliyyatus Suadaa) \n325 \n[3] S. Wu and M. Dredze, “Beto, Bentz, Becas: The surprising cross-lingual effectiveness of \nBERT,” Proceedings of the 2019 Conference on Empirical Methods in Natural \nLanguage Processing , pp. 833-844, 2019 [Online]. Available: \nhttps://www.aclweb.org/anthology/D19-1077/. [Accessed: 26-May-2021] \n[4] B. Wilie, K. Vincentio, G. I. Winata, S. Cahyawijaya, X. Li, Z. Y. Lim, S. Soleman, R. \nMahendra, P. Fung, S. Bahar, and A. Purwarianti, “IndoNLU: Benchmark and Resources \nfor Evaluating Indonesian Natural Language Understanding ,” Proceedings of the 1st \nConference of the Asia-Pacific Chapter of the Association for Computational Linguistics \nand the 10th International Joint Conference on Natural Language Processing , pp. 843-\n857, 2020 [Online]. Available: https://www.aclweb.org/anthology/2020.aacl-main.85/. \n[Accessed: 26-May-2021] \n[5] F. Koto, A . Rahimi, J . H. Lau, and T. Baldwin, “ IndoLEM and IndoBERT: A \nBenchmark Dataset and Pre-trained Language Model for Indonesian NLP,” Proceedings \nof the 28th International Conference on Computational Linguistics , pp. 757-770, 2020 \n[Online]. Available: https://www.aclweb.org/anthology/2020.coling-main.66/. \n[Accessed: 26-May-2021] \n[6] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. \nLouf, M. Funtowicz, J. Davison, S. Shleifer, P. Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, \nT. L. Scao, S. Gugger, M. Drame, Q. Lhoest, and A. Rush. “Transformers: State-of-the-\nArt Natural Language Processing ,” Proceedings of the 2020 Conference on Empirical \nMethods in Natural Language Processing: System Demonstrations , pp. 38-45 [Online]. \nAvailable: https://www.aclweb.org/anthology/2020.emnlp-demos.6/ [Accessed: 26-May-\n2021] \n[7] A. N. Azhar and M. L. Khodra, “Fine-tuning Pretrained Multilingual BERT Model for \nIndonesian Aspect -based Sentiment Analysis ,” Proceedings of the 7th International \nConference on Advance Informatics: Concepts, Theory and Applications (ICAICTA  \n2020), 2020 [Online]. Available: https://ieeexplore.ieee.org/document/9428882. \n[Accessed: 26-May-2021] \n[8] Ilham Firdausi Putra; Ayu Purwarianti, “Improving Indonesian Text Classification Using \nMultilingual Language Model ,” Proceedings of the 7th International Conference on \nAdvance Informatics: Concepts, Theory and Applications (ICAICTA 2020) , 2020 \n[Online]. Available: https://ieeexplore.ieee.org/document/9429038. [Accessed: 26-May-\n2021] \n[9] R. Wijayanti; M . L. Khodra , and  D. H. Widyantoro , “ Indonesian Abstractive \nSummarization using Pre -trained Model ,” Proceedings of the 3rd East Indonesia \nConference on Computer and Information Technology (EIConCIT 2021), 2021 [Online]. \nAvailable: https://ieeexplore.ieee.org/document/9431880. [Accessed: 26-May-2021]  \n[10] E. Zuliarso, M. T. Anwar, K. Hadiono and I. Chasanah, “Detecting Hoaxes in Indonesian \nNews Using TF/TDM and K Nearest Neighbor ,” IOP Conference Series: Mater ials \nScience and Engineering, Vol. 835, 2019 [Online]. Available: , \nhttps://iopscience.iop.org/article/10.1088/1757-899X/835/1/012036. [Accessed: 26-May-\n2021] \n[11] I. Y. R. Pratiwi, R. A. Asmara, and F. Rahutomo, “Study of Hoax News Detection using \nNaïve Bayes Classifier in Indonesian Language,” Proceedings of the 11th International \nConference on Information & Communication Technology and System (ICTS) , 2017 \n[Online]. Available: https://ieeexplore.ieee.org/document/8265649. [Accessed: 26-May-\n2021] \n[12] B. P. Nayoga, R. Adipradana, R. Suryadia, and D. Suhartono, “ Hoax Analyzer for \nIndonesian News Using Deep Learning Models ,” Procedia Computer Science: Special \nIssues of the 5th International Conference on Computer Science and Computational \nIntelligence, 2020 [Online]. Available:  \nhttps://www.sciencedirect.com/science/article/pii/S1877050921000739. [Accessed: 26-\nMay-2021] \n     ◼              ISSN (print): 1978-1520, ISSN (online): 2460-7258 \nIJCCS  Vol. 15, No. 3, July 2021 :  317 – 326 \n326 \n[13] Cambridge Dictionary, “Definition of Hoax”, 2021 [Online]. Available: \nhttps://dictionary.cambridge.org/dictionary/english/hoax. [Accessed: 26-May-2021] \n[14] Collins Dictionary, “Definition of Hoax”, 2021 [Online]. \nhttps://www.collinsdictionary.com/dictionary/english/hoax. [Accessed: 26-May-2021] \n[15] Merriam Webster, “Definition of Hoax”, 2021 [Online]. https://www.merriam-\nwebster.com/dictionary/hoax. [Accessed: 26-May-2021] \n[16] Kominfo, “There are 800,000 Hoax Spreader Sites in Indonesia,” 12-Dec-2017 [Online], \nhttps://kominfo.go.id/content/detail/12008/ada-800000-situs-penyebar-hoax-di-\nindonesia/0/highlight_media. [Accessed: 26-May-2021] \n[17] Forbes, “Report: More Than 800 Deaths And 5,800 Hospitalizations Globally May Have \nResulted From COVID -19 Misinformation ,” 23-August-2020 [ Online]. \nhttps://www.forbes.com/sites/markhall/2020/08/23/coronavirus-misinformation/. \n[Accessed: 26-May-2021] \n[18] M. Granik and V. Mesyura, \"Fake news detection using naive Bayes classifier,\" \nProceedings of the 2017 IEEE First Ukraine Conference on Electrical and Computer \nEngineering (UKRCON),  pp. 900 -903, 2017 [Online]. Available:  \nhttps://ieeexplore.ieee.org/document/8100379. [Accessed: 26-May-2021] \n \n ",
  "topic": "Hoax",
  "concepts": [
    {
      "name": "Hoax",
      "score": 0.9685018658638
    },
    {
      "name": "Computer science",
      "score": 0.6436408162117004
    },
    {
      "name": "Transformer",
      "score": 0.6279608011245728
    },
    {
      "name": "The Internet",
      "score": 0.5314527750015259
    },
    {
      "name": "Transfer of learning",
      "score": 0.521436870098114
    },
    {
      "name": "Indonesian",
      "score": 0.47084495425224304
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46102389693260193
    },
    {
      "name": "Coronavirus disease 2019 (COVID-19)",
      "score": 0.42324429750442505
    },
    {
      "name": "Natural language processing",
      "score": 0.38274240493774414
    },
    {
      "name": "Speech recognition",
      "score": 0.375308632850647
    },
    {
      "name": "World Wide Web",
      "score": 0.2027369737625122
    },
    {
      "name": "Linguistics",
      "score": 0.1386413872241974
    },
    {
      "name": "Engineering",
      "score": 0.10780686140060425
    },
    {
      "name": "Electrical engineering",
      "score": 0.06495648622512817
    },
    {
      "name": "Medicine",
      "score": 0.06211400032043457
    },
    {
      "name": "Disease",
      "score": 0.0
    },
    {
      "name": "Alternative medicine",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Pathology",
      "score": 0.0
    },
    {
      "name": "Infectious disease (medical specialty)",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 13
}