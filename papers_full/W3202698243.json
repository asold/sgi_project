{
  "title": "Controllable Sentence Simplification with a Unified Text-to-Text Transfer Transformer",
  "url": "https://openalex.org/W3202698243",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2982509077",
      "name": "Kim Cheng Sheang",
      "affiliations": [
        "Pompeu Fabra University"
      ]
    },
    {
      "id": "https://openalex.org/A157790613",
      "name": "Horacio Saggion",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4248677198",
    "https://openalex.org/W2406289639",
    "https://openalex.org/W2109802560",
    "https://openalex.org/W4289753256",
    "https://openalex.org/W2060516133",
    "https://openalex.org/W2112282043",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W22627370",
    "https://openalex.org/W2467834614",
    "https://openalex.org/W2890771450",
    "https://openalex.org/W257074533",
    "https://openalex.org/W2534253848",
    "https://openalex.org/W2508661403",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2927213876",
    "https://openalex.org/W2970561469",
    "https://openalex.org/W2963658612",
    "https://openalex.org/W2741816383",
    "https://openalex.org/W158417997",
    "https://openalex.org/W2963023793",
    "https://openalex.org/W2108373063",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2156422881",
    "https://openalex.org/W2462123276",
    "https://openalex.org/W1970557026",
    "https://openalex.org/W2042262613",
    "https://openalex.org/W4211149393",
    "https://openalex.org/W2524789673",
    "https://openalex.org/W4287779338",
    "https://openalex.org/W2743653651",
    "https://openalex.org/W2963104691",
    "https://openalex.org/W2962897394",
    "https://openalex.org/W1507711477",
    "https://openalex.org/W2998235597",
    "https://openalex.org/W1647671624",
    "https://openalex.org/W3034639488",
    "https://openalex.org/W4287557728",
    "https://openalex.org/W2605243085",
    "https://openalex.org/W2962788902",
    "https://openalex.org/W2949806279",
    "https://openalex.org/W2774217864",
    "https://openalex.org/W3023237241",
    "https://openalex.org/W2991195882",
    "https://openalex.org/W2138238299",
    "https://openalex.org/W2962805889",
    "https://openalex.org/W2251812634",
    "https://openalex.org/W2159947192",
    "https://openalex.org/W1555854422",
    "https://openalex.org/W2962986840",
    "https://openalex.org/W3034999214"
  ],
  "abstract": "Recently, a large pre-trained language model called T5 (A Unified Text-to-Text Transfer Transformer) has achieved state-of-the-art performance in many NLP tasks. However, no study has been found using this pre-trained model on Text Simplification. Therefore in this paper, we explore the use of T5 fine-tuning on Text Simplification combining with a controllable mechanism to regulate the system outputs that can help generate adapted text for different target audiences. Our experiments show that our model achieves remarkable results with gains of between +0.69 and +1.41 over the current state-of-the-art (BART+ACCESS). We argue that using a pre-trained model such as T5, trained on several tasks with large amounts of data, can help improve Text Simplification.",
  "full_text": "Proceedings of the 14th International Conference on Natural Language Generation (INLG), pages 341–352,\nAberdeen, Scotland, UK, 20-24 September 2021. ©2021 Association for Computational Linguistics\n341\nControllable Sentence Simpliﬁcation with a Uniﬁed Text-to-Text Transfer\nTransformer\nKim Cheng Sheang, Horacio Saggion\nLaSTUS, TALN, Universitat Pompeu Fabra\nC/Roc Boronat, 138, 08018 Barcelona, Spain\n{kimcheng.sheang,horacio.saggion}@upf.edu\nAbstract\nRecently, a large pre-trained language model\ncalled T5 (A Uniﬁed Text-to-Text Transfer\nTransformer) has achieved state-of-the-art per-\nformance in many NLP tasks. However, no\nstudy has been found using this pre-trained\nmodel on Text Simpliﬁcation. Therefore in\nthis paper, we explore the use of T5 ﬁne-tuning\non Text Simpliﬁcation combining with a con-\ntrollable mechanism to regulate the system out-\nputs that can help generate adapted text for dif-\nferent target audiences. Our experiments show\nthat our model achieves remarkable results\nwith gains of between +0.69 and +1.41 over\nthe current state-of-the-art (BART+ACCESS).\nWe argue that using a pre-trained model such\nas T5, trained on several tasks with large\namounts of data, can help improve Text Sim-\npliﬁcation.1\n1 Introduction\nText Simpliﬁcation (TS) can be regarded as a natu-\nral language generation task where the generated\ntext has a reduced language complexity in both\nvocabulary and sentence structure while preserv-\ning its original information and meaning (Saggion,\n2017). Its applications can be used as reading as-\nsessment tools for people with low-literacy skills\nsuch as children (Watanabe et al., 2009), and non-\nnative speakers (Paetzold and Specia, 2016), or\npeople with cognitive disabilities such as autism\n(Barbu et al., 2015), aphasia (Carroll et al., 1999),\nand dyslexia (Rello et al., 2013a; Matausch and\nPeb¨ock, 2010). In addition, TS can also be used\nas a preprocessing step to improve the results of\nmany NLP tasks, e.g., Parsing (Chandrasekar et al.,\n1996), Information Extraction (Evans, 2011; Jon-\nnalagadda and Gonzalez, 2010), Question Genera-\ntion (Bernhard et al., 2012), Text Summarization\n1The code and data are available at https://github.\ncom/KimChengSHEANG/TS_T5\n(Siddharthan et al., 2004), and Machine Translation\n(ˇStajner and Popovi´c, 2016, 2019).\nIn recent years, research in TS has been mostly\nfocused on developing models based on deep neu-\nral networks (Vu et al., 2018; Zhao et al., 2018b;\nMartin et al., 2020b). However, and to the best of\nour knowledge, very few studies of transfer learn-\ning –where a model is ﬁrst pre-trained on a data-\nrich task and then ﬁne-tuned on downstream tasks–\nhave been explored in TS.\nIn this paper, we propose a transfer learning\nand controllable sentence simpliﬁcation model that\nharnesses the power of the Uniﬁed Text-to-Text\nTransfer Transformer (T5) pre-trained model (Raf-\nfel et al., 2020), combining it with control tokens\nto provide a way to generate output that adapts to\ndifferent target users. Such a model can be adjusted\nto ﬁt the need of different users without having to\nbuild everything from the ground up.\nWe make the following contributions:\n• We introduce a transfer learning approach\ncombined with a controllable mechanism for\nsentence simpliﬁcation task.\n• We make an improvement to the performance\nof the sentence simpliﬁcation system.\n• We introduce a new control token #words to\nhelp the model generate sentences by replac-\ning long complex words with shorter alterna-\ntives.\n• We conduct an evaluation and comparison be-\ntween different sizes of pre-trained models\nand a detailed analysis on the effect of each\ncontrol token.\n• We show that by choosing the right control\ntoken values and pre-trained model, the model\nachieves the state-of-the-art performance in\ntwo well-known benchmarking datasets.\n342\n2 Related Work\n2.1 Sentence Simpliﬁcation\nIt is often regarded as a monolingual translation\nproblem (Zhu et al., 2010; Coster and Kauchak,\n2011; Wubben et al., 2012), where the models are\ntrained on parallel complex-simple sentences ex-\ntracted from English Wikipedia and Simple English\nWikipedia (SEW) (Zhu et al., 2010).\nThere are many approaches based on statisti-\ncal Machine Translation (SMT), including phrase-\nbased MT (PBMT) ( ˇStajner et al., 2015), and\nsyntax-based MT (SBMT) (Xu et al., 2016). Nisioi\net al. (2017) introduced Neural Text Simpliﬁca-\ntion (NTS), a Neural-Machine-Translation-based\nsystem (NMT) which performs better than SMT.\nZhang and Lapata (2017) took a similar approach\nadding lexical constraints combining the NMT\nmodel with reinforcement learning. After the re-\nlease of Transformer (Vaswani et al., 2017), Zhao\net al. (2018a) introduced a Transformer-based ap-\nproach and integrated it with a paraphrase database\nfor simpliﬁcation called Simple PPDB (Pavlick and\nCallison-Burch, 2016a). The model outperforms\nall previous state-of-the-art models in sentence sim-\npliﬁcation.\nOur proposed model is also a sequence-to-\nsequence Transformer-based model, but instead\nof using the original Transformer by Vaswani et al.\n(2017), we use T5 (Raffel et al., 2020).\n2.2 Controllable Sentence Simpliﬁcation\nIn recent years, there has been increased interest\nin conditional training with sequence-to-sequence\nmodels. It has been applied to some NLP tasks\nsuch as controlling the length and content of sum-\nmaries (Kikuchi et al., 2016; Fan et al., 2017),\npoliteness in machine translation (Sennrich et al.,\n2016), and linguistic style in text generation (Ficler\nand Goldberg, 2017). Scarton and Specia (2018)\nintroduced the controllable TS model by embed-\nding grade level token <grade> into the sequence-\nto-sequence model. Martin et al. (2020b) took a\nsimilar approach adding 4 tokens into source sen-\ntences to control different aspects of the output\nsuch as length, paraphrasing, lexical complexity,\nand syntactic complexity. Kariuk and Karamshuk\n(2020) took the idea of using control tokens from\nMartin et al. (2020b) and used it in unsupervised\napproach by integrating those control tokens into\nthe back translation algorithm, which allows the\nmodel to self-supervise the process of learning\ninter-relations between a control sequence and the\ncomplexity of the outputs. The results of Scar-\nton and Specia (2018), Martin et al. (2020b), and\nKariuk and Karamshuk (2020) have shown that\nadding control tokens does help improve the per-\nformance of sentence simpliﬁcation models quite\nsigniﬁcantly.\nBuilding upon Martin et al. (2020b), we ﬁne-\ntune T5 with all control tokens as deﬁned in Martin\net al. (2020b) to control different aspects of the\noutput sentences. Moreover, we add one more\ncontrol token (number of words ratio) in order to\nbe able to generate new sentences with a similar\nlength as the source but shorter in word length as\nwe believe that the number characters ratio alone is\nnot enough for the model to generate shorter words.\n3 Model\nIn this work, we ﬁne-tune T5 pre-trained model\nwith the controllable mechanism on Text Simpliﬁ-\ncation. T5 (A Uniﬁed Text-to-Text Transfer Trans-\nformer) (Raffel et al., 2019) is pre-trained on a\nnumber of supervised and unsupervised tasks such\nas machine translation, document summarization,\nquestion answering, classiﬁcation tasks, and read-\ning comprehension, as well as BERT-style token\nand span masking (Devlin et al., 2019). There are\nﬁve different variants of T5 pre-trained models: T5-\nsmall (5 attention modules, 60 million parameters),\nand T5-base (12 attention modules, 220 million\nparameters). Due to the limited resources of Co-\nlab Pro, we are able to train only T5-small and\nT5-base.\n3.1 Control Tokens\nWe use control tokens to control different aspects of\nsimpliﬁcation such as compression ratio (#Chars),\nparaphrasing (Levenshtein similarity), lexical com-\nplexity (word rank), and syntactic complexity (the\ndepth of dependency tree) as deﬁned in (Martin\net al., 2020b). Then, we add another control to-\nken word ratio (#Words) to control word length.\nWe argue that word ratio is another important con-\ntrol token because normally word frequency cor-\nrelates well with familiarity, and word length can\nbe an additional factor as long words tend to be\nhard to read (Rello et al., 2013b). Moreover, cor-\npus studies of original and simpliﬁed texts show\nthat simple texts contain shorter and more frequent\nwords (Drndarevi´c and Saggion, 2012). Therefore,\nwe add word ratio to help the model generate sim-\n343\npliﬁed sentences with a similar amount of words\nand shorter in word length, whereas #Chars alone\ncould help the model regulate sentence length but\nnot word length.\n• #Chars (C): character length ratio between\nsource sentence and target sentence. The num-\nber of characters in target divided by that of\nthe source.\n• LevSim (L): normalized character-level Lev-\nenshtein similarity (Levenshtein, 1966) be-\ntween the source and target.\n• WordRank (WR): inverse frequency order\nof all words in the target divided by that of the\nsource.\n• DepTreeDepth (DTD): maximum depth of\nthe dependency tree of the target divided by\nthat of the source.\n• #Words (W): number of words ratio between\nsource sentence and target sentence. The num-\nber of words in target divided by that of the\nsource.\nTable 1 shows an example of a sentence embed-\nded with control tokens for training.\nSource\nsimplify: W 0.58 C 0.52 L 0.67 WR 0.92\nDTD 0.71 In architectural decoration Small\npieces of colored and iridescent shell have been\nused to create mosaics and inlays, which have\nbeen used to decorate walls, furniture and boxes.\nTarget\nSmall pieces of colored and shiny shell has been\nused to decorate walls, furniture and boxes.\nTable 1: This table shows how control tokens are em-\nbedded into the source sentence for training. The key-\nword simplify is added at the beginning of each source\nsentence to mark it as a simpliﬁcation task.\n4 Experiments\nOur model is developed using the Huggingface\nTransformers library (Wolf et al., 2019)2 with Py-\nTorch3 and Pytorch lightning4.\n2https://huggingface.co/transformers/\nmodel_doc/t5.html\n3https://pytorch.org\n4https://pytorchlightning.ai\n4.1 Datasets\nWe use the WikiLarge dataset (Zhang and Lapata,\n2017) for training. It is the largest and most com-\nmonly used text simpliﬁcation dataset containing\n296,402 sentence pairs from automatically aligned\ncomplex-simple sentence pairs English Wikipedia\nand Simple English Wikipedia which is compiled\nfrom (Zhu et al., 2010; Woodsend and Lapata,\n2011; Kauchak, 2013).\nFor validation and testing, we use TurkCorpus\n(Xu et al., 2016), which has 2000 samples for vali-\ndation and 359 samples for testing, and each com-\nplex sentence has 8 human simpliﬁcations. We also\nuse a newly created dataset called ASSET (Alva-\nManchego et al., 2020) for testing, which contains\n2000/359 samples (validation/test) with 10 simpli-\nﬁcations per source sentence.\n4.2 Evaluation Metrics\nFollowing previous research (Zhang and Lapata,\n2017; Martin et al., 2020a), we use automatic eval-\nuation metrics widely used in text simpliﬁcation\ntask.\nSARI (Xu et al., 2016) compares system outputs\nwith the references and the source sentence. It\nmeasures the performance of text simpliﬁcation on\na lexical level by explicitly measuring the goodness\nof words that are added, deleted and kept. So far, it\nis the most commonly adopted metric and we use\nit as an overall score.\nBLEU (Papineni et al., 2002) is originally de-\nsigned for Machine Translation and is commonly\nused previously. BLEU has lost its popularity on\nText Simpliﬁcation due to the fact that it correlates\npoorly with human judgments and often penalizes\nsimpler sentences (Sulem et al., 2018). We keep\nusing it so that we can compare our system with\nprevious systems.\nFKGL (Kincaid et al., 1975) In addition to SARI\nand BLEU, we use FKGL to measure readability;\nhowever, it does not take into account grammatical-\nity and meaning preservation.\nWe compute SARI, BLEU, and FKGL using\nEASSE (Alva-Manchego et al., 2019)5, a simpliﬁ-\ncation evaluation library.\n4.3 Training Details\nWe performed hyperparameters search using Op-\ntuna (Akiba et al., 2019) with T5-small and reduced\n5https://github.com/feralvam/easse\n344\nsize dataset to speed up the process. All models\nare trained with the same hyperparameters such\nas a batch size of 6 for T5-base and 12 for T5-\nsmall, maximum token of 256, learning rate of\n3e-4, weight decay of 0.1, Adam epsilon of 1e-8, 5\nwarm up steps, 5 epochs, and the rest of the param-\neters are left with default values from Transformers\nlibrary. Also, the seed is set to 12 for reproducibil-\nity. For the generation, we use beam size of 8.\nOur models are trained and evaluated using Google\nColab Pro, which has a random GPU T4 or P100.\nBoth have 16GB of memory, up to 25GB of RAM,\nand a time limit of 24h maximum for the execution\nof cells. Training of T5-base model for 5 epochs\nusually takes around 20 hours.\n4.4 Choosing Control Token Values at\nInference\nIn this experiment, we want to search for control\ntoken values that make the model generate the best\npossible simpliﬁcations. Thus, we select the values\nthat achieve the best SARI on the validation set\nusing the same tool that we use for hyperparameters\ntuning, Optuna (Akiba et al., 2019), and keep those\nvalues ﬁxed for sentences in the test set. We repeat\nthe same process for each evaluation dataset.\n4.5 Baselines\nWe benchmark our model against several well-\nknown state-of-the-art systems:\nYATS (Ferr´es et al., 2016)6 Rule-based system\nwith linguistically motivated rule-based syntactic\nanalysis and corpus-based lexical simpliﬁer which\ngenerates sentences based on part-of-speech tags\nand dependency information.\nPBMT-R (Wubben et al., 2012) Phrase-based\nMT system trained on a monolingual parallel cor-\npus with candidate re-ranking based on dissimilar-\nity using Levenshtein distance.\nUNTS (Surya et al., 2019) Unsupervised Neu-\nral Text Simpliﬁcation is based on the encode-\nattend-decode style architecture (Bahdanau et al.,\n2014) with a shared encoder and two decoders and\ntrained on unlabeled data extracted from English\nWikipedia dump.\nDress-LS (Zhang and Lapata, 2017) A Seq2Seq\nmodel trained with deep reinforcement learning\n6http://able2include.taln.upf.edu\ncombined with a lexical simpliﬁcation model to\nimprove complex word substitutions.\nDMASS+DCSS (Zhao et al., 2018b) A Seq2Seq\nmodel trained with the original Transformer ar-\nchitecture (Vaswani et al., 2017) combined with\nthe simple paraphrase database for simpliﬁcation\nPPDB. (Pavlick and Callison-Burch, 2016b).\nACCESS (Martin et al., 2020b) Seq2Seq system\ntrained with four control tokens attached to source\nsentence: character length ratio, Levenshtein simi-\nlarity ratio, word rank ratio, and dependency tree\ndepth ratio between source and target sentence.\nBART+ACCESS (Martin et al., 2020a) The sys-\ntem ﬁne-tunes BART (Lewis et al., 2020) and adds\nthe simpliﬁcation control tokens from ACCESS.\n4.6 Results\nWe evaluate our models automatically on two\ndifferent datasets TurkCorpus and ASSET. In\naddition, we also perform a human evaluation\non one of our models, which is described in\nSection 5. Table 2 reports the results of auto-\nmatic evaluation of our models compared with\nother state-of-the-art systems. Our model T5-\nbase+#chars+WordRank+LevSim+DepTreeDepth\nperforms best on TurkCorpus with the SARI score\nof 43.31, while the other model T5-base+All\nTokens performs best on ASSET with SARI score\nof 45.04 compared to the current state-of-the-art\nBART+ACCESS with the SARI score of 42.62 on\nTurkCorpus and 43.63 on ASSET. Following these\nresults, our models out-perform all the state-of-\nthe-art models in the literature in all approaches:\nrule-based, supervised and unsupervised approach\neven without using any additional resources.\n5 Human Evaluation\nIn addition to automatic evaluation, we performed\na human evaluation on the outputs of different sys-\ntems. Following recent works (Alva-Manchego\net al., 2017; Dong et al., 2019; Zhao et al., 2020),\nwe run our evaluation on Amazon Mechanical Turk\nby asking ﬁve workers to rate using 5-point lik-\nert scale on three aspects: (1) Fluency (or Gram-\nmaticality): is it grammatically correct and well-\nformed?, (2) Simplicity: is it simpler than the\noriginal sentence?, and (3) Adequacy (or Mean-\ning preservation): does it preserve meaning of the\noriginal sentence? More detailed instructions can\nbe found in Appendix A. For this evaluation, we\n345\nModel Data ASSET TurkCorpus\nSARI↑ BLEU↑ FKGL↓ SARI↑ BLEU↑ FKGL↓\nY ATS Rule-based 34.4 72.07 7.65 37.39 74.87 7.67\nPBMT-R PWKP (Wikipedia) 34.63 79.39 8.85 38.04 82.49 8.85\nUNTS Unsup. Data 35.19 76.14 7.60 36.29 76.44 7.60\nDress-LS WikiLarge 36.59 86.39 7.66 36.97 81.08 7.66\nDMASS+DCSS WikiLarge 38.67 71.44 7.73 39.92 73.29 7.73\nACCESS WikiLarge 40.13 75.99 7.29 41.38 76.36 7.29\nBART+ACCESS WikiLarge 43.63 76.28 6.25 42.62 78.28 6.98\nT5-base+#Chars+WordRank\n+LevSim+DepTreeDepth WikiLarge 44.91 71.96 6.32 43.31 66.23 6.17\nT5-base+All Tokens WikiLarge 45.04 71.21 5.88 43.00 64.42 5.63\nTable 2: We report SARI, BLEU and FKGL evaluation results of our model compared with others on TurkCorpus\nand ASSET test set (SARI and BLEU higher the better, FKGL lower the better). BLEU and FKGL scores are\nnot quite relevant for sentence simpliﬁcation, and we keep them just to compare with the previous models. All\nthe results of the literature are taken from Martin et al. (2020a), except Y ATS which is generated using its web\ninterface.\nrandomly select 100 sentences from different sim-\npliﬁcation systems trained on WikiLarge dataset,\nexcept Y ATS which is rule-based. Table 3 reports\nthe results in averaged values.\nModel Fluency Simplicity Adequacy\nY ATS 4.03* 3.62* 3.92*\nDMASS+DCSS 3.84* 3.70* 3.48*\nBART+ACCESS4.41 4.02 4.13\nOur Model 4.30 3.99 4.18\nTable 3: Results of human evaluation on 100 random\nsentences selected from TurkCorpus test set. Best re-\nsults are marked in bold, and results marked with an\n’*’ are signiﬁcantly lower than our model according to\npared t-test with p<0.01. Our model in use here is T5-\nbase+All Tokens.\nThe results have shown that our model per-\nforms lower in ﬂuency and about the same in\nsimplicity, and better in adequacy compared to\nBART+ACCESS. Based on our observation, there\nare two reasons that humans rated our model lower\non ﬂuency: (1) our model generates incorrect text\nformat (without spaces) in some sentences (exam-\nples in Table 4). The problem can be easily spotted\nby human, but it does not affect the automatic eval-\nuation as EASSE uses a tokenizer which can split\nthe whole sentence correctly. (2) Our model tends\nto produce longer sentences than BART+ACCESS\nand in some cases, the subject is repeated twice\nwhen the sentence is split into two (e.g., relative\nclause). The repetition is also considered as one\nof the key features of simpliﬁcation as it makes\ntext easier to understand, but for native or ﬂuent\nlanguage speakers, repetition and the longer sen-\ntence make the ﬂuency worse. Moreover, due to\nthese problems, the evaluators also tend to lower\nthe simplicity score as they consider it harder to\nread.\nSentence\nSo far the’celebrity’episodes have included Vic\nReeves, Nancy Sorrell, and Gaby Roslin.\nNew South Wales’biggest city and capital is\nSydney.\nTable 4: Examples of incorrect text format generated\nby our model.\n6 Ablation Study\nIn this section, we investigate the contribution of\neach token and different T5 pre-trained models to\nthe performance of the system. Table 5 reports\nthe scores of models trained on WikiLarge and\nevaluated with TurkCorpus and ASSET test set.\nTable 6 shows all control token values used for all\n346\nModel ASSET TurkCorpus\nSARI↑ BLEU↑ FKGL↓ SARI↑ BLEU↑ FKGL↓\nT5-small(No tokens) 29.85 90.39 8.94 34.50 94.16 9.44\nT5-small+ All Tokens 39.12 86.08 6.99 40.83 85.12 6.78\nT5-base(No tokens) 34.15 88.97 8.94 37.56 90.96 8.81\nT5-base:\n+#Words 38.51 84.02 7.45 38.86 89.10 8.61\n+#Chars 39.58 79.22 6.06 38.95 84.81 7.76\n+LevSim 41.58 82.52 6.53 40.90 85.45 7.55\n+WordRank 41.40 76.75 5.85 41.44 85.46 7.67\n+DepTreeDepth 40.08 81.94 6.56 39.18 87.60 7.81\nT5-base:\n+WordRank+LevSim 42.85 80.38 4.47 41.75 83.90 7.42\n+#Chars+WordRank+LevSim 44.89 56.76 5.93 42.91 67.09 6.53\n+#Words+#Chars+WordRank+LevSim 44.65 58.52 5.52 43.03 68.11 5.96\n+#Chars+WordRank+LevSim+DepTreeDepth 44.91 71.96 6.32 43.31 66.23 6.17\n+All Tokens 45.04 71.21 5.88 43.00 64.42 5.63\nTable 5: Ablation study on different T5 models and different control token values. Each model is trained and\nevaluated independently. We report SARI, BLEU and FKGL on TurkCorpus and ASSET test set. Control token\nvalues corresponded to each model are listed in the Table 6\nModel ASSET TurkCorpus\nT5-small(No tokens)\nT5-small+ All Tokens W 1.05C0.95WR0.75L0.75DTD0.75 W1.05C0.95WR0.85L0.85DTD0.85\nT5-base(No tokens)\nT5-base:\n+#Words W 0.75 W0.85\n+#Chars C 0.5 C0.75\n+LevSim L 0.75 L0.85\n+WordRank WR 0.25 WR0.85\n+DepTreeDepth DTD 0.5 DTD0.75\nT5-base:\n+WordRank+LevSim W 0.75 L0.75 W0.85 L0.85\n+#Chars+WordRank+LevSim C 0.95WR0.75LevSim0.75 C0.95WR0.85L0.85\n+#Words+#Chars+WordRank+LevSim W1.05C0.95WR0.75L0.75 W1.05C0.95WR0.75L0.75\n+#Chars+WordRank+LevSim+DepTreeDepth C0.95WR0.75L0.75DTD0.75 C0.95WR0.75L0.75DTD0.75\n+All Tokens W 1.05C0.95WR0.75L0.75DTD0.75 W1.05C0.95WR0.85L0.85DTD0.85\nTable 6: These are the control token values used for the ablation study in Table 5. Each model is trained and\nevaluated independently. The values are selected using the hyperparameters search tool mentioned in Section 4.4.\n347\nFigure 1: Inﬂuence of #Words and #Chars control tokens on the simpliﬁcation outputs. Red represents the outputs\nof the model trained with four tokens, without #Words control token. Blue represents the outputs of the model\ntrained with all ﬁve tokens. Green is the reference taken from TurkCorpus. The ﬁrst row shows the compression\nratio (number of chars ratio between system outputs and source sentences), and second row is the Levenshtein\nsimilarity (words similarity between system outputs and source sentences) of each model. We plot the results of\nthe 2000 validation sentences from TurkCorpus. Other control token values used here are set to 0.75, the example\nin Table 7.\nthe models in Table 5 which are selected using the\nsame process and tool as mentioned in Section 4.4.\nBased on the results, the larger model (T5-base)\nperforms better than the smaller one (T5-small) on\nboth datasets (+3.06 on TurkCorpus, +4.3 on AS-\nSET). It is due to the fact that larger model has\nmore information which could generate better and\nmore coherent text. Moreover, when added con-\ntrol tokens, the performance increases signiﬁcantly.\nWith only one token, WordRank performs best on\nTurkCorpus (+3.88 over T5-base) and LevSim on\nASSET (+7.43 over T5-base).\nUsing pre-trained model alone does not gain\nmuch improvement, only when combined with con-\ntrol tokens, the results improve by a big margin\n(+3.06 and +9.28 for T5-small with and without\ntokens), and (+5.75 and +10.89 for T5-base with\nand without tokens).\n6.1 Analysis on the effect of #Words\nOur goal of using #Words control token is to make\nthe model learn to generate shorter words whereas\n#Chars alone could help the model regulate the\nsentence length but not word length, so here we\ninvestigate how #Words and #Chars control tokens\naffect the outputs.\nFor the model with #Words token to work, it has\nto be incorporated with #Chars as #Words deter-\nmines the number of words and #Chars limits the\nnumber of characters in the sentence. In our exam-\nples Table 7, we set #Words to 1.0, which means\nthe number of words in the simpliﬁed sentence has\nto be similar to the original sentence, and #Chars\nis set to 0.5 and 0.75, which means keeping the\nsame amount of words but reduces 50% or 25% of\ncharacters.\nFigure 1 shows the differences in density distri-\nbution (ﬁrst row) and similarity (second row) be-\ntween model 1 in red without #Words token, model\n2 in blue with #Words tokens, and the one in green\nis the reference. The ﬁrst column #Chars is set\nto 0.25, second column #Chars=0.5, third column\n#Chars=0.75, fourth #Chars=1.0, and in all cases\n#words is set to 1.0. From the plots, we can see\nthat model 1 does more compression than model\n2, which means model 2 preserve more words than\nmodel 1.\nTable 7 shows some example sentences com-\nparing models with #Chars 0.75 and #Chars 0.5.\nWhen #Chars is set to 0.75, we do not see much dif-\nference between the two models, but when #Chars\nis set to 0.5, the two models have differences in\nterms of sentence length and word length. For\nexample, the word mathematics in the example\nnumber one is replaced with the word math in\nmodel 2 (with #Words) and removed by model 1\n348\nTokens Model 1: #Chars 0.5 WordRank 0.75 LevSim 0.75 DepTreeDepth 0.75\nModel 2: #Words 1.0 #Chars 0.5 WordRank 0.75 LevSim 0.75 DepTreeDepth 0.75\nSource: In order to accomplish their objective, surveyors use elements of geometry, engineering,\ntrigonometry, mathematics, physics, and law.\nModel 1: In order to accomplish their objective, surveyors use geometry, engineering, and law.\nModel 2: In order to do this, surveyors use geometry, engineering, trigonometry,math, physics,\nand law.\nSource: The municipality has about 5700 inhabitants.\nModel 1: The municipality has 5700.\nModel 2: The town has about 5700.\nSource: A hunting dog refers to any dog who assists humans in hunting.\nModel 1: A hunting dog is any dog who hunts.\nModel 2: A hunting dog is a dog who helps humans in hunting.\nTokens Model 1: #Chars 0.75 WordRank 0.75 LevSim 0.75 DepTreeDepth 0.75\nModel 2: #Words 1.0 #Chars 0.75 WordRank 0.75 LevSim 0.75 DepTreeDepth 0.75\nSource: The park has become a traditional location for mass demonstrations.\nModel 1: The park has become a popular place for demonstrations.\nModel 2: The park has become a place for people to show things.\nSource: Frances was later absorbed by an extratropical cyclone on November 21.\nModel 1: Frances was later taken in by an extratropical cyclone.\nModel 2: Frances was later taken over by a cyclone on November 21.\nSource: There are claims that thousands of people were impaled at a single time.\nModel 1: There are claims that thousands of people were killed.\nModel 2: There are also stories that thousands of people were killed at a time.\nTable 7: Examples showing the differences between the model with number of words ratio versus the one without.\nModel 1 trained with four tokens, without #Words control token, and model 2 trained with all ﬁve control tokens.\nAll control token values used to generate the outputs are listed in the rows Tokens. We use bold to highlight the\ndifferences.\n(without #Words). Second example, the word mu-\nnicipality is replaced by the word town by model\n2, and model 1 simply keeps the word and crops\nthe sentence (the same problem with the third ex-\nample). In addition, the fourth example, the word\nlocation is replaced by both models with the word\nplace, the phrase mass demonstration is reduced\nto demonstration by the model 1 whereas model\n2 changes to four shorter words people to show\nthings.\nThere are many cases where model 1 and model\n2 generate the same substitutions, but very often\nmodel 1 tends to crop the end of the sentence or\ndrops some words to fulﬁll the length constraint.\nWhereas model 2 tends to generate longer sen-\ntences than model 1, less crop, and very often re-\nplaces long complex words with shorter ones. Even\nthough, based on the results from Table 2, adding\nthe #Words control token does not signiﬁcantly im-\nprove the SARI score and sometimes even lowers\nthe score, it certainly holds its purpose.\n7 Conclusion\nIn this paper, we propose a method which lever-\nages a big pre-trained model (T5) ﬁne-tuning it\nfor the Controllable Sentence Simpliﬁcation task.\nThe experiments have shown good results of 43.31\nSARI on TurkCorpus evaluation set and of 45.04\non ASSET evaluation set, outperforming the cur-\nrent state-of-the-art model. Also, we have shown\nthat adding the control token #Words is useful for\ngenerating substitutions with a shorter lengths.\nAcknowledgments\nWe acknowledge support from the project\nContext-aware Multilingual Text Simpliﬁ-\n349\ncation (ConMuTeS) PID2019-109066GB-\nI00/AEI/10.13039/501100011033 awarded by\nMinisterio de Ciencia, Innovaci´on y Universidades\n(MCIU) and by Agencia Estatal de Investigaci´on\n(AEI) of Spain. Also, we would like to thank the\nthree anonymous reviewers for their insightful\nsuggestions.\nReferences\nTakuya Akiba, Shotaro Sano, Toshihiko Yanase,\nTakeru Ohta, and Masanori Koyama. 2019. Op-\ntuna: A next-generation hyperparameter optimiza-\ntion framework. In Proceedings of the 25th ACM\nSIGKDD international conference on knowledge dis-\ncovery & data mining, pages 2623–2631.\nFernando Alva-Manchego, Joachim Bingel, Gustavo\nPaetzold, Carolina Scarton, and Lucia Specia. 2017.\nLearning how to simplify from explicit labeling of\ncomplex-simpliﬁed text pairs. In Proceedings of\nthe Eighth International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers) ,\npages 295–305.\nFernando Alva-Manchego, Louis Martin, Antoine Bor-\ndes, Carolina Scarton, Benoˆıt Sagot, and Lucia Spe-\ncia. 2020. ASSET: A dataset for tuning and evalu-\nation of sentence simpliﬁcation models with multi-\nple rewriting transformations. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 4668–4679, Online. As-\nsociation for Computational Linguistics.\nFernando Alva-Manchego, Louis Martin, Carolina\nScarton, and Lucia Specia. 2019. EASSE: Easier au-\ntomatic sentence simpliﬁcation evaluation. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP): System Demonstra-\ntions, pages 49–54, Hong Kong, China. Association\nfor Computational Linguistics.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nEduard Barbu, M Teresa Mart ´ın-Valdivia, Eugenio\nMart´ınez-C´amara, and L Alfonso Ure ˜na-L´opez.\n2015. Language technologies applied to document\nsimpliﬁcation for helping autistic people. Expert\nSystems with Applications, 42(12):5076–5086.\nDelphine Bernhard, Louis De Viron, V ´eronique\nMoriceau, and Xavier Tannier. 2012. Question gen-\neration for french: collating parsers and paraphras-\ning questions. Dialogue & Discourse, 3(2):43–74.\nJohn A Carroll, Guido Minnen, Darren Pearce, Yvonne\nCanning, Siobhan Devlin, and John Tait. 1999. Sim-\nplifying text for language-impaired readers. In\nNinth Conference of the European Chapter of the As-\nsociation for Computational Linguistics.\nRaman Chandrasekar, Christine Doran, and Srinivas\nBangalore. 1996. Motivations and methods for text\nsimpliﬁcation. In COLING 1996 Volume 2: The\n16th International Conference on Computational\nLinguistics.\nWilliam Coster and David Kauchak. 2011. Simple en-\nglish wikipedia: a new text simpliﬁcation task. In\nProceedings of the 49th Annual Meeting of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 665–669.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nYue Dong, Zichao Li, Mehdi Rezagholizadeh, and\nJackie Chi Kit Cheung. 2019. Editnts: An neu-\nral programmer-interpreter model for sentence sim-\npliﬁcation through explicit editing. arXiv preprint\narXiv:1906.08104.\nBiljana Drndarevi ´c and Horacio Saggion. 2012. To-\nwards automatic lexical simpliﬁcation in spanish: an\nempirical study. In Proceedings of the First Work-\nshop on Predicting and Improving Text Readability\nfor target reader populations, pages 8–16.\nRichard J Evans. 2011. Comparing methods for\nthe syntactic simpliﬁcation of sentences in informa-\ntion extraction. Literary and linguistic computing ,\n26(4):371–388.\nAngela Fan, David Grangier, and Michael Auli. 2017.\nControllable abstractive summarization. arXiv\npreprint arXiv:1711.05217.\nDaniel Ferr´es, Montserrat Marimon, Horacio Saggion,\net al. 2016. Yats: yet another text simpliﬁer. In\nInternational Conference on Applications of Natural\nLanguage to Information Systems , pages 335–342.\nSpringer.\nJessica Ficler and Yoav Goldberg. 2017. Controlling\nlinguistic style aspects in neural language genera-\ntion. arXiv preprint arXiv:1707.02633.\nSiddhartha Jonnalagadda and Graciela Gonzalez. 2010.\nBiosimplify: an open source sentence simpliﬁcation\nengine to improve recall in automatic biomedical\ninformation extraction. In AMIA Annual Sympo-\nsium Proceedings, volume 2010, page 351. Ameri-\ncan Medical Informatics Association.\nOleg Kariuk and Dima Karamshuk. 2020. Cut: Con-\ntrollable unsupervised text simpliﬁcation. arXiv\npreprint arXiv:2012.01936.\n350\nDavid Kauchak. 2013. Improving text simpliﬁcation\nlanguage modeling using unsimpliﬁed text data. In\nProceedings of the 51st annual meeting of the associ-\nation for computational linguistics (volume 1: Long\npapers), pages 1537–1546.\nYuta Kikuchi, Graham Neubig, Ryohei Sasano, Hiroya\nTakamura, and Manabu Okumura. 2016. Control-\nling output length in neural encoder-decoders. arXiv\npreprint arXiv:1609.09552.\nJ Peter Kincaid, Robert P Fishburne Jr, Richard L\nRogers, and Brad S Chissom. 1975. Derivation of\nnew readability formulas (automated readability in-\ndex, fog count and ﬂesch reading ease formula) for\nnavy enlisted personnel. Technical report, Naval\nTechnical Training Command Millington TN Re-\nsearch Branch.\nReno Kriz, Joao Sedoc, Marianna Apidianaki, Car-\nolina Zheng, Gaurav Kumar, Eleni Miltsakaki, and\nChris Callison-Burch. 2019. Complexity-weighted\nloss and diverse reranking for sentence simpliﬁca-\ntion. arXiv preprint arXiv:1904.02767.\nVladimir I Levenshtein. 1966. Binary codes capable\nof correcting deletions, insertions, and reversals. In\nSoviet physics doklady , volume 10, pages 707–710.\nSoviet Union.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nLouis Martin, Angela Fan, ´Eric de la Clergerie, An-\ntoine Bordes, and Beno ˆıt Sagot. 2020a. Multilin-\ngual unsupervised sentence simpliﬁcation. arXiv\npreprint arXiv:2005.00352.\nLouis Martin, ´Eric Villemonte de La Clergerie, Benoˆıt\nSagot, and Antoine Bordes. 2020b. Control-\nlable Sentence Simpliﬁcation. In LREC 2020 -\n12th Language Resources and Evaluation Confer-\nence, Marseille, France. Due to COVID19 pan-\ndemic, the 12th edition is cancelled. The LREC\n2020 Proceedings are available at http://www.lrec-\nconf.org/proceedings/lrec2020/index.html.\nKerstin Matausch and Birgit Peb¨ock. 2010. Easyweb–a\nstudy how people with speciﬁc learning difﬁculties\ncan be supported on using the internet. In Interna-\ntional Conference on Computers for Handicapped\nPersons, pages 641–648. Springer.\nSergiu Nisioi, Sanja ˇStajner, Simone Paolo Ponzetto,\nand Liviu P Dinu. 2017. Exploring neural text sim-\npliﬁcation models. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 85–91.\nGustavo H Paetzold and Lucia Specia. 2016. Unsuper-\nvised lexical simpliﬁcation for non-native speakers.\nIn Proceedings of the Thirtieth AAAI Conference on\nArtiﬁcial Intelligence, pages 3761–3767.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nEllie Pavlick and Chris Callison-Burch. 2016a. Simple\nppdb: A paraphrase database for simpliﬁcation. In\nProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 143–148.\nEllie Pavlick and Chris Callison-Burch. 2016b. Simple\nPPDB: A paraphrase database for simpliﬁcation. In\nProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 143–148, Berlin, Germany. As-\nsociation for Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nLuz Rello, Ricardo Baeza-Yates, Stefan Bott, and Ho-\nracio Saggion. 2013a. Simplify or help? text\nsimpliﬁcation strategies for people with dyslexia.\nIn Proceedings of the 10th International Cross-\nDisciplinary Conference on Web Accessibility, pages\n1–10.\nLuz Rello, Susana Bautista, Ricardo Baeza-Yates,\nPablo Gerv´as, Raquel Herv´as, and Horacio Saggion.\n2013b. One half or 50%? an eye-tracking study of\nnumber representation readability. In IFIP Confer-\nence on Human-Computer Interaction , pages 229–\n245. Springer.\nHoracio Saggion. 2017. Automatic Text Simpliﬁcation.\nSynthesis Lectures on Human Language Technolo-\ngies, 10(1):1–137.\nCarolina Scarton and Lucia Specia. 2018. Learning\nsimpliﬁcations for speciﬁc target audiences. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short\nPapers), pages 712–718.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Controlling politeness in neural machine\ntranslation via side constraints. In Proceedings of\n351\nthe 2016 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 35–40.\nAdvaith Siddharthan, Ani Nenkova, and Kathleen\nMcKeown. 2004. Syntactic simpliﬁcation for im-\nproving content selection in multi-document summa-\nrization. In COLING 2004: Proceedings of the 20th\nInternational Conference on Computational Linguis-\ntics, pages 896–902, Geneva, Switzerland. COL-\nING.\nSanja ˇStajner, Iacer Calixto, and Horacio Saggion.\n2015. Automatic text simpliﬁcation for spanish:\nComparative evaluation of various simpliﬁcation\nstrategies. In Proceedings of the international con-\nference recent advances in natural language pro-\ncessing, pages 618–626.\nSanja ˇStajner and Maja Popovi´c. 2016. Can text simpli-\nﬁcation help machine translation? In Proceedings of\nthe 19th Annual Conference of the European Associ-\nation for Machine Translation, pages 230–242.\nSanja ˇStajner and Maja Popovi ´c. 2019. Automated\ntext simpliﬁcation as a preprocessing step for ma-\nchine translation into an under-resourced language.\nIn Proceedings of the International Conference on\nRecent Advances in Natural Language Processing\n(RANLP 2019), pages 1141–1150.\nElior Sulem, Omri Abend, and Ari Rappoport. 2018.\nBLEU is not suitable for the evaluation of text sim-\npliﬁcation. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Process-\ning, pages 738–744, Brussels, Belgium. Association\nfor Computational Linguistics.\nSai Surya, Abhijit Mishra, Anirban Laha, Parag Jain,\nand Karthik Sankaranarayanan. 2019. Unsupervised\nneural text simpliﬁcation. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 2058–2068, Florence,\nItaly. Association for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nTu Vu, Baotian Hu, Tsendsuren Munkhdalai, and Hong\nYu. 2018. Sentence simpliﬁcation with memory-\naugmented neural networks. In Proceedings of the\n2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 2 (Short Pa-\npers), pages 79–85, New Orleans, Louisiana. Asso-\nciation for Computational Linguistics.\nWillian Massami Watanabe, Arnaldo Candido Junior,\nVin´ıcius Rodriguez Uzˆeda, Renata Pontin de Mattos\nFortes, Thiago Alexandre Salgueiro Pardo, and San-\ndra Maria Alu´ısio. 2009. Facilita: reading assistance\nfor low-literacy readers. In Proceedings of the 27th\nACM international conference on Design of commu-\nnication, pages 29–36.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2019.\nHuggingface’s transformers: State-of-the-art natural\nlanguage processing. ArXiv, abs/1910.03771.\nKristian Woodsend and Mirella Lapata. 2011. Learn-\ning to simplify sentences with quasi-synchronous\ngrammar and integer programming. In Proceedings\nof the 2011 Conference on Empirical Methods in\nNatural Language Processing, pages 409–420.\nSander Wubben, Antal van den Bosch, and Emiel Krah-\nmer. 2012. Sentence simpliﬁcation by monolingual\nmachine translation. In Proceedings of the 50th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1015–\n1024, Jeju Island, Korea. Association for Computa-\ntional Linguistics.\nWei Xu, Courtney Napoles, Ellie Pavlick, Quanze\nChen, and Chris Callison-Burch. 2016. Optimizing\nstatistical machine translation for text simpliﬁcation.\nTransactions of the Association for Computational\nLinguistics, 4:401–415.\nXingxing Zhang and Mirella Lapata. 2017. Sentence\nsimpliﬁcation with deep reinforcement learning. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing , pages\n584–594, Copenhagen, Denmark. Association for\nComputational Linguistics.\nSanqiang Zhao, Rui Meng, Daqing He, Saptono Andi,\nand Parmanto Bambang. 2018a. Integrating trans-\nformer and paraphrase rules for sentence simpliﬁca-\ntion. arXiv preprint arXiv:1810.11193.\nSanqiang Zhao, Rui Meng, Daqing He, Andi Saptono,\nand Bambang Parmanto. 2018b. Integrating trans-\nformer and paraphrase rules for sentence simpliﬁ-\ncation. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3164–3173, Brussels, Belgium. Association\nfor Computational Linguistics.\nYanbin Zhao, Lu Chen, Zhi Chen, and Kai Yu.\n2020. Semi-supervised text simpliﬁcation with\nback-translation and asymmetric denoising autoen-\ncoders. In Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, volume 34, pages 9668–9675.\nZhemin Zhu, Delphine Bernhard, and Iryna Gurevych.\n2010. A monolingual tree-based translation model\nfor sentence simpliﬁcation. In Proceedings of the\n23rd International Conference on Computational\nLinguistics (Coling 2010), pages 1353–1361.\n352\nA Human Evaluation Interface\nFigure 2: Our interface is based on the one proposed by Kriz et al. (2019), and the consent form based on Alva-\nManchego et al. (2020).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8223774433135986
    },
    {
      "name": "Transformer",
      "score": 0.7984590530395508
    },
    {
      "name": "Sentence",
      "score": 0.6131625771522522
    },
    {
      "name": "Language model",
      "score": 0.6081016659736633
    },
    {
      "name": "Natural language processing",
      "score": 0.5670337677001953
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5645207762718201
    },
    {
      "name": "Text generation",
      "score": 0.5587893128395081
    },
    {
      "name": "Electrical engineering",
      "score": 0.0701463520526886
    },
    {
      "name": "Engineering",
      "score": 0.062420040369033813
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}