{
    "title": "Transformer training strategies for forecasting multiple load time series",
    "url": "https://openalex.org/W4387773507",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5070592781",
            "name": "Matthias Hertel",
            "affiliations": [
                "Karlsruhe Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5084746605",
            "name": "Maximilian Beichter",
            "affiliations": [
                "Karlsruhe Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5048040015",
            "name": "Benedikt Heidrich",
            "affiliations": [
                "Karlsruhe Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5029006114",
            "name": "Oliver Neumann",
            "affiliations": [
                "Karlsruhe Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5040295886",
            "name": "Benjamin Schäfer",
            "affiliations": [
                "Karlsruhe Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5001459055",
            "name": "Ralf Mikut",
            "affiliations": [
                "Karlsruhe Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5014228448",
            "name": "Veit Hagenmeyer",
            "affiliations": [
                "Karlsruhe Institute of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2294097774",
        "https://openalex.org/W4229448232",
        "https://openalex.org/W4313527398",
        "https://openalex.org/W4380885315",
        "https://openalex.org/W4362654051",
        "https://openalex.org/W3201755234",
        "https://openalex.org/W4396221671",
        "https://openalex.org/W4286250636",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W3024158773",
        "https://openalex.org/W4312686336",
        "https://openalex.org/W2754252319",
        "https://openalex.org/W2604847698",
        "https://openalex.org/W4375854091",
        "https://openalex.org/W6726027185",
        "https://openalex.org/W2773116114",
        "https://openalex.org/W4206696802",
        "https://openalex.org/W2084724629",
        "https://openalex.org/W4306876858",
        "https://openalex.org/W2787684890",
        "https://openalex.org/W3093565541",
        "https://openalex.org/W2597866042",
        "https://openalex.org/W3033926479",
        "https://openalex.org/W2907164325",
        "https://openalex.org/W4226020959",
        "https://openalex.org/W4283370720",
        "https://openalex.org/W6600373086",
        "https://openalex.org/W3155706277",
        "https://openalex.org/W2762503529",
        "https://openalex.org/W4293068799",
        "https://openalex.org/W3177318507",
        "https://openalex.org/W2075373489",
        "https://openalex.org/W4382362038"
    ],
    "abstract": "Abstract In the smart grid of the future, accurate load forecasts on the level of individual clients can help to balance supply and demand locally and to prevent grid outages. While the number of monitored clients will increase with the ongoing smart meter rollout, the amount of data per client will always be limited. We evaluate whether a Transformer load forecasting model benefits from a transfer learning strategy, where a global univariate model is trained on the load time series from multiple clients. In experiments with two datasets containing load time series from several hundred clients, we find that the global training strategy is superior to the multivariate and local training strategies used in related work. On average, the global training strategy results in 21.8% and 12.8% lower forecasting errors than the two other strategies, measured across forecasting horizons from one day to one month into the future. A comparison to linear models, multi-layer perceptrons and LSTMs shows that Transformers are effective for load forecasting when they are trained with the global training strategy.",
    "full_text": "Open Access\n© The Author(s) 2023. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// \ncreat iveco mmons. org/ licen ses/ by/4. 0/.\nRESEARCH\nHertel et al. Energy Informatics  2023, 6(Suppl 1):20\nhttps://doi.org/10.1186/s42162-023-00278-z\nEnergy Informatics\nTransformer training strategies \nfor forecasting multiple load time series\nMatthias Hertel1*, Maximilian Beichter1, Benedikt Heidrich1, Oliver Neumann1, Benjamin Schäfer1, \nRalf Mikut1 and Veit Hagenmeyer1 \nFrom The 12th DACH+ Conference on Energy Informatics 2023 \nVienna, Austria. 4-6 October 2023. https://www.energy-informatics2023.org/\nAbstract \nIn the smart grid of the future, accurate load forecasts on the level of individual clients \ncan help to balance supply and demand locally and to prevent grid outages. While \nthe number of monitored clients will increase with the ongoing smart meter rollout, \nthe amount of data per client will always be limited. We evaluate whether a Trans-\nformer load forecasting model benefits from a transfer learning strategy, where a global \nunivariate model is trained on the load time series from multiple clients. In experi-\nments with two datasets containing load time series from several hundred clients, we \nfind that the global training strategy is superior to the multivariate and local training \nstrategies used in related work. On average, the global training strategy results in 21.8% \nand 12.8% lower forecasting errors than the two other strategies, measured across fore-\ncasting horizons from one day to one month into the future. A comparison to linear \nmodels, multi-layer perceptrons and LSTMs shows that Transformers are effective \nfor load forecasting when they are trained with the global training strategy.\nKeywords: Load forecasting, Transformer, Global model, Time series, Smart grid\nIntroduction\nClimate change is one of the biggest challenges facing humanity, with the risk of dra -\nmatic consequences if certain limits of warming are exceeded (Pörtner et  al. 2022). \nTo mitigate climate change, the energy system must be decarbonized. A difficulty in \ndecarbonization is that renewable energy supply fluctuates depending on the weather. \nHowever, supply and demand must be balanced in the grid at every moment to prevent \noutages (Machowski et al. 1997). In addition, with the ongoing decentralization of the \nrenewable energy supply and the installation of large consumers, such as electric vehicle \nchargers and heat pumps, low-voltage grids are expected to reach their limits (Çakmak \nand Hagenmeyer 2022). Thus, to balance the grid and to avoid congestions, advanced \noperation and control mechanisms must be installed in the smart grid of the future \n(Ramchurn et al. 2012; Haben et al. 2021). This requires accurate forecasts on various \n*Correspondence:   \nmatthias.hertel@kit.edu\n1 Karlsruhe Institute of Technology, \nInstitute for Automation \nand Applied Informatics, \nHermann-von-Helmholtz-Platz 1, \n76344 Eggenstein-Leopoldshafen, \nGermany\nFull list of author information is \navailable at the end of the article\nPage 2 of 13Hertel et al. Energy Informatics  2023, 6(Suppl 1):20\naggregation levels, up to fine-grained low-voltage level load forecasts (Haben et al. 2021; \nOrdiano et al. 2018). Such fine-grained load forecasts can be used for demand-side man-\nagement, energy management systems, distribution grid state estimation, grid manage -\nment, storage optimization, peer-to-peer trading, peak shaving, smart electrical vehicle \ncharging, dispatchable feeders, provision of feedback to customers, anomaly detection \nand intervention evaluation (Haben et al. 2021; Yildiz et al. 2017; Voß et al. 2018; Grab -\nner et al. 2023; Werling et al. 2022). Moreover, the aggregation of fine-grained load fore-\ncasts can result in a more accurate forecast of the aggregated load (Hong et al. 2020).\nWith the smart meter rollout, fine-grained electrical load data will become available \nfor an increasing number of clients. In such a scenario where load time series from mul -\ntiple clients are available, different model training strategies are possible. The goal of our \nwork is to compare training strategies for the Transformer (Vaswani et al. 2017), which \nwas recently used for load forecasting (Zhang et al. 2022; Hertel et al. 2022a, b; Cao et al. \n2022; Giacomazzi et al. 2023; Huy et al. 2022).\nTask definition\nWe address the following multiple load time series forecasting problem: At a time step t, \ngiven the history of the electrical load of C clients xc\n0 ,... xc\nt with 1 ≤ c ≤ C  , the goal is to \npredict the next h electrical load values xc\nt+1 ,... ,xc\nt+h for all clients 1 ≤ c ≤ C  , where h \nis called the forecast horizon.\nContribution\nWe compare three training strategies for the Transformer in a scenario with multiple \nload time series. The training strategies are depicted in Fig. 1. \n1. A multivariate model training strategy, where a single model gets all load time series \nas input and forecasts all load time series simultaneously.\n2. A local model training strategy, where a separate univariate 1 model is trained for \neach load time series.\n3. A global model training strategy, where a generalized univariate model is used to \nforecast each load time series separately.\nWe compare our models with the models from related work (Zhou et al. 2021; Wu et al. \n2021; Zhou et al. 2022; Nie et al. 2022), as well as with multiple baselines. In particular, \nwe compare with the linear models used in Zeng et al. (2022), to figure out if Transform-\ners are effective for load forecasting and which training strategy is the most promising \none.\nPaper structure\nFirst, we describe the \"Related work\". Then, the Transformer architecture and the train -\ning strategies are described in the \" Approach\". This is followed by the \"Experimental \n1 By ’univariate’ we mean models which produce a forecast for a single time series. We still call models ’univariate’ when \nthey have multiple input variables, such as exogenous time and calendar features.\nPage 3 of 13\nHertel et al. Energy Informatics  2023, 6(Suppl 1):20\nsetup\", \"Results\" and a \"Discussion \". Finally, the paper concludes with the \"Conclusion \nand future work\".\nRelated work\nThis section first presents related work on long time series forecasting and load forecast-\ning with Transformers. Most of the load forecasting literature uses local models, but few \nworks use global models, which are presented next. The global training strategy can be \nunderstood as a transfer learning technique. We therefore discuss transfer learning in \nthe field of load forecasting at the end of this section.\nAs Transformer are often used for long time series forecasting with up to one month \nhorizon, various extensions to the Transformer architecture exist that aim to reduce the \ntime and space complexity. This is done by the Informer using ProbSparse self-atten -\ntion (Zhou et al. 2021), by the Autoformer using auto-correlation (Wu et al. 2021), by \nthe FEDformer using frequency enhanced decomposition (Zhou et  al. 2022) and by \nPatchTST using patching (Nie et  al. 2022). The proposed models are multivariate or \nlocal, except for the global PatchTST (Nie et al. 2022). The experiments in these works \nare conducted on six datasets from different domains, including one load forecasting \ndataset, which we also use in our experiments (see section Datasets). A global linear \nmodel called LTSF-Linear (Zeng et al. 2022) gives better results than the aforementioned \nmultivariate Transformers. Parallel to our work, global Transformers were shown to beat \nthe aforementioned multivariate Transformers (Murphy and Chen 2023). However, this \nwork does not optimize the model’s lookback size and therefore achieves sub-optimal \nFig. 1 The three training strategies, with models depicted as networks. An example with three load time \nseries, four days input and one day output is shown. a Multivariate: one model processes all load time series \nsimultaneously; b local: separate models (blue, orange, green) process each load time series; c global: one \nmodel (black) processes all load time series one at a time\nPage 4 of 13Hertel et al. Energy Informatics  2023, 6(Suppl 1):20\nresults. PatchTST (Nie et al. 2022) is a global Transformer with patched inputs and is \nsuperior to LTSF-Linear (Zeng et al. 2022) on the six datasets.\nTransformer architectures for short-term load forecasting are designed to use exter -\nnal calendar and weather features (Wang et al. 2022; Huy et al. 2022). An evaluation of \ndifferent architectures is undertaken in Hertel et al. (2022a). Further work modifies the \narchitecture for multi-energy load forecasting (Wang et al. 2022). Upstream decomposi -\ntions are used to improve the forecast quality (Ran et  al. 2023). These models are not \ncompared on a common benchmark dataset, but evaluated on different datasets on city \nor national level. There, usually only one load time series is available, which only allows \nfor local models. Furthermore, the models are not compared to the Transformer archi -\ntectures for long time series.\nGlobal load forecasting models are already used with convolutional neural networks \n(Voß et al. 2018) and N-BEATS (Grabner et al. 2023). A mixture between a multivariate \nand a global model is investigated in Shi et al. (2017), where a single recurrent neural \nnetwork (RNN) model is trained on randomly pooled subsets of the time series. Some \nworks cluster the time series and then train global or multivariate models for each clus -\nter (Han et al. 2020; Yang and Youn 2021). PatchTST (Nie et al. 2022) is a global Trans -\nformer with patched inputs. We compare to this approach in our experiments.\nThe authors of Pinto et al. (2022) and Himeur et al. (2022) present current literature \non transfer learning in the domain of energy systems. They define a taxonomy of transfer \nlearning methods and discuss different strategies of using transfer learning with build -\nings from different domains. Two works (Nawar et al. 2023; Gao et al. 2022) use transfer \nlearning by pre-training and fine-tuning Transformers. Transferability from one building \nto another is tested in Nawar et al. (2023), and from one district to another in Gao et al. \n(2022). In contrast to these works, our transfer learning approach is to train a general -\nized model on the data from many clients, without fine-tuning for a target time series.\nApproach\nWe use an encoder–decoder Transformer (Vaswani et  al. 2017) as a load forecasting \nmodel. This model architecture has self-attention and cross-attention as its main com -\nponents and was initially used for machine translation. It was used as a forecasting \nmodel in Wu et al. (2020) and later adopted for load forecasting (Zhang et al. 2022; Her -\ntel et al. 2022a, b). We use the model implementation from Hertel et al. (2022a).\nThe encoder gets L vectors as input, which represent the last L time steps, where L is \ncalled the lookback size. Each input vector consists of one (in the case of local and global \nmodels) or C (in the case of multivariate models) load values, and nine additional time \nand calendar features. The features are the hour of the day, the day of the week and the \nmonth (all cyclically encoded with a sine and a cosine function), whether it is a workday, \nwhether it is a holiday and whether the next day is a workday (all binary features). The \ninput to the decoder consists of h vectors, which represent the following h time steps for \nwhich a forecast will be made. In the decoder input, the load values are set to zero, so \nthat each value is forecasted independently from the previous forecasted values, allow -\ning for a direct multi-step forecast instead of generating all values iteratively. The input \nvectors to the encoder and the decoder are first fed through linear layers to increase the \nPage 5 of 13\nHertel et al. Energy Informatics  2023, 6(Suppl 1):20\ndimensionality to the hidden dimension of the model dmodel . Both the encoder and the \ndecoder consist of multiple layers with eight self-attention heads and the decoder lay -\ners have eight additional masked cross-attention heads. Finally, a linear layer transforms \nthe h decoder output vectors into a forecast with h × 1 (for local and global models) or \nh × C (for multivariate models) values. We varied the number of encoder and decoder \nlayers and the hidden dimension dmodel , and found three layers with dmodel = 128 to give \nthe best results. The full model architecture is shown in Fig. 2.\nTraining strategies\nWe compare multivariate, local and global Transformers. The training strategies are \ndepicted in Fig.  1 and are further explained in the following. Details on the inputs, \nFig. 2 Architecture of the Transformer forecasting model. The input and output dimensions differ for the \nmultivariate model and the local and global models. The shown dimensions refer to the Electricity dataset \nwith 321 clients\nTable 1 Training strategy details for the Electricity dataset with 321 load time series, 2.1 years \ntraining data and nine time and calendar features\nFor the local models, training data is the amount of training data per model\nTraining strategy Models Input size Output size Training data\nMultivariate 1 L × 330 h × 321 2.1 years\nLocal 321 L × 10 h × 1 2.1 years\nGlobal 1 L × 10 h × 1 321 ∗ 2.1 years\nPage 6 of 13Hertel et al. Energy Informatics  2023, 6(Suppl 1):20\noutputs, number of models and training data size for each training strategy are given in \nTable 1.\n• Multivariate training strategy: In the input to the model, each time step is repre -\nsented by a vector of size C + f , where C is the number of load time series and f is \nthe number of calendar features. The model forecasts C values for the next h time \nsteps, i.e. its output consists of h vectors of size C. A single model is used to forecast \nall time series simultaneously.\n• Local training strategy: Local models get only one time series as input and generate \na forecast for this time series. In the input, each time step is represented by a vector \nwith f + 1 entries for the f calendar features and the electrical load value. C separate \nmodels are trained for the C time series, each using the training data from one time \nseries.\n• Global training strategy: The global approach is a single model that generalizes for all \nload time series. The model gets one load time series as input and generates a fore -\ncast for that load time series. In contrast to the local models, only one global model \nis trained on samples from all load time series, and this model is used to forecast all \nload time series. This results in C times as many training data for the global model as \nfor a local model. To generate forecasts for all C time series, the global model is used \nC times with the history of one load time series as input.\nExperimental setup\nDatasets\nAs recommended in recent literature reviews on load forecasting (Haben et  al. 2021; \nHong et al. 2020; vom Scheidt et al. 2020), we conduct experiments on multiple datasets, \nnamely the Electricity and the Ausgrid solar home datasets. For both datasets we make a \ntemporal split and use the first 70% of each time series for training, the next 10% for vali-\ndation, and the last 20% for testing, as in related work (Wu et al. 2021; Zhou et al. 2022; \nNie et al. 2022; Zeng et al. 2022).\nThe Electricity dataset 2 is published in Lai et al. (2018) and used in related work on \nlong-term forecasting (Zhou et  al. 2021; Wu et  al. 2021; Zhou et  al. 2022; Nie et  al. \n2022; Zeng et al. 2022). It is a subset of the UCI Electricity Load Diagrams dataset 3 first \npresented in Rodrigues and Trindade (2018), only containing the time series without \nmissing values. The dataset contains hourly electrical load data from 321 clients of a \nPortuguese energy supplier. The clients are from different economic sectors, including \noffices, factories, supermarkets, hotels, restaurants, among others (Rodrigues and Trin -\ndade 2018). The time series range from 2012 to 2014.\nThe Ausgrid solar home dataset 4 contains solar generation and electrical load data \nfrom 300 clients 5 of an Australian energy supplier. The clients are private houses with \nrooftop solar systems. The time series range from July 2010 to June 2013. We only use \nthe electrical load data transformed into hourly resolution.\n2 https:// github. com/ laigu okun/ multi varia te- time- series- data.\n3 https:// archi ve. ics. uci. edu/ ml/ datas ets/ Elect ricit yLoad Diagr ams20 112014.\n4 https:// www. ausgr id. com. au/ Indus try/ Our- Resea rch/ Data- to- share/ Solar- home- elect ricity- data.\n5 We use 299 of the clients because one client had missing data.\nPage 7 of 13\nHertel et al. Energy Informatics  2023, 6(Suppl 1):20\nComparison methods\nWe compare our models with models from related work (Zhou et al. 2021; Wu et al. \n2021; Zhou et  al. 2022; Nie et  al. 2022; Zeng et  al. 2022), as well as with a persis -\ntence baseline, linear regression models, multi-layer perceptrons and long short-term \nmemory networks.\n• Models from related work: For Informer (Zhou et al. 2021), Autoformer (Wu et al. \n2021), FEDformer (Zhou et al. 2022), PatchTST (Nie et al. 2022) and LTSF-Linear \n(Zeng et al. 2022), we take the results reported in the publications where applica -\nble, and run the code published with the papers otherwise. All parameters except \nfor the forecast horizon are left unchanged.\n• Persistence baseline: The persistence baseline takes the value from one week before \nthe predicted hour as a forecast for the 24 h and 96 h horizons, and the value from \n1 month before the predicted hour as the 720 h forecast.\n• Linear regression: For each load time series, we train a linear regression model \nwith h outputs. The input consists of the last 336 load values and the nine time \nand calendar features for the current hour when the prediction is made (see \n“ Approach” for a description of the features). The main difference to LTSF-Linear \n(Zeng et al. 2022) is that the linear regression models are local models, but LTSF-\nLinear is a global model. Furthermore, the two approaches use different training \nalgorithms and LTSF-Linear does not use time and calendar features.\n• Multi-layer perceptron (MLP): As for the linear regression, we train a local MLP \nfor each load time series. The MLPs get the last 168 load values and the nine time \nand calendar features of the current hour as input. Using more than 168 load val -\nues as input did not improve the results. Each MLP has two hidden layers with \nReLU activation (ReLU 2023) and 1024 neurons per layer.\n• Long short-term memory (LSTM): We train multivariate, local and global LSTM \n(Hochreiter and Schmidhuber 1997) models. We use the same architecture as in \nKong et al. (2017), consisting of two LSTM layers with 20 units each and a linear \nprediction layer. Using larger models did not improve the results.\nTraining details\nAll models are trained with the AdamW optimizer (Loshchilov and Hutter 2019) \nusing the mean squared error loss. We use a batch size of 128 and a learning rate of \n0.0001 with 1000 warm-up steps and cosine decay with γ = 0.8 . When testing dif -\nferent lookback sizes L , we find one week to be optimal for the multivariate Trans -\nformer and the local Transformers. For the global Transformer, the results improve \nwith increasing lookback size until L = 336 (two weeks), and stay almost the same for \nL = 720 (one month). For Transformer models with two weeks input and one month \noutput, the batch size has to be reduced to 64 due to the quadratic memory consump -\ntion of the model. For the multivariate Transformer, the batch size is set to 32 as in \nrelated work (Zhou et al. 2021; Wu et al. 2021; Zhou et al. 2022). The validation error \nis evaluated every 10,000 training steps and at the end of every epoch. We use early \nPage 8 of 13Hertel et al. Energy Informatics  2023, 6(Suppl 1):20\nstopping to end the training when no more improvement on the validation set is seen \nfor ten evaluations. For the MLPs, the initial learning rate is set to 0.001 and decayed \nwith γ = 0.5 after every epoch.\nMetric\nAs in related work (Zhou et al. 2021; Wu et al. 2021; Zhou et al. 2022; Nie et al. 2022; \nZeng et  al. 2022), every load time series is standardized by subtracting its mean and \ndividing by its standard deviation and the metrics are computed on these standard -\nized time series. For every hour t ∈ T test in the test set, a forecasting model predicts \nthe next h hourly loads ˆyc\nt =ˆyc\nt,t+1 ,... ,ˆyc\nt,t+h for time series c. Then, the mean abso -\nlute error (MAE) between the predictions ˆyc = {ˆyc\ni ∀ i ∈ T test } and the ground truth \nyc = yc\n1 ,... ,yc\nT test is computed. As the final result, the MAE averaged across all C load \ntime series, the Ttest evaluation time points and the h forecasting steps is reported.\nThe mean squared error (MSE) is computed analogously, using the squared residuals \ninstead of the absolute residuals.\nResults\nForecast accuracy\nTable 2 shows the MAE results on the two datasets 6. On the Electricity dataset, the \nglobal Transformer is the best model for the 24  h horizon, and PatchTST is the best \nmodel for longer horizons. On the Ausgrid solar home dataset, PatchTST is the best \nmodel for all three horizons. The global Transformer beats the local Transformers and \nMAE (y,ˆy) = 1\nC ·|T test|·h\nC\nc= 1 t∈T test\nh\ni= 1\n|yc\nt+ i −ˆyc\nt,t+ i|.\nTable 2 MAE results on the two datasets, with 24, 96 and 720 h forecast horizon\nMV = multivariate, L = local, G = global. The best results are highlighted in bold and the best results per training strategy \nare highlighted in italic\nModel Strat- Input Electricity Ausgrid\negy (days) 24h 96h 720h 24h 96h 720h\nInformer (Zhou et al. 2021) MV 4 0.399 0.407 0.450 0.582 0.607 0.645\nAutoformer (Wu et al. 2021) MV 4 0.289 0.317 0.361 0.579 0.569 0.592\nFEDformer (Zhou et al. 2022) MV 4 0.284 0.297 0.343 0.560 0.566 0.609\nLSTM MV 7 0.400 0.402 0.407 0.611 0.618 0.613\nTransformer MV 7 0.366 0.384 0.382 0.584 0.586 0.576\nPersistence L – 0.279 0.279 0.447 0.647 0.647 0.717\nLinear regression L 14 0.203 0.233 0.296 0.496 0.524 0.565\nMLP L 7 0.199 0.236 0.308 0.499 0.532 0.567\nLSTM L 7 0.263 0.283 0.337 0.517 0.541 0.573\nTransformer L 7 0.256 0.289 0.354 0.535 0.563 0.583\nLTSF-Linear (Zeng et al. 2022) G 14 0.209 0.237 0.301 0.490 0.515 0.553\nPatchTST (Nie et al. 2022) G 14 0.190 0.222 0.290 0.468 0.494 0.522\nLSTM G 7 0.207 0.239 0.302 0.491 0.525 0.559\nTransformer G 14 0.184 0.225 0.312 0.482 0.514 0.533\n6 The MSE results show a similar pattern and can be found on GitHub.\nPage 9 of 13\nHertel et al. Energy Informatics  2023, 6(Suppl 1):20\nthe multivariate Transformer across all tested horizons. On average, it reduces the error \nby 21.8% compared to the multivariate Transformer and by 12.8% compared to the local \nTransformers. Compared to the best local model, the linear regression, it reduces the \nerror by 2.9%. Compared to the best multivariate model, FEDformer, it reduces the error \nby 15.4%. All multivariate models, including Informer, Autoformer, FEDformer and \nthe multivariate Transformer, perform poorly and do not beat the persistence baseline \nwith a lag of one week. The local linear regression models are slightly better than the \nglobal linear model, LTSF-Linear, on the Electricity dataset, but it is vice versa on the \nAusgrid solar home dataset. The MLP is in five out of six cases a bit worse than the lin -\near regression, with a 1.5% larger error on average. The local LSTMs are better than the \nlocal Transformers, but the Transformer is better as a multivariate model and as a global \nmodel (except for the one month horizon on the Electricity dataset). The forecast errors \nare lower on the Electricity dataset than on the Ausgrid dataset which is a more fine-\ngrained dataset containing single private houses.\nComputational cost\nThe training times are given in Table  3. The local Transformer models need by far the \nlongest time to train. Their training time increases sharply with longer forecast hori -\nzons. The multivariate Transformer trains fast and is even faster than the MLPs for \nshort horizons. Training a global Transformer is much faster than training the many \nlocal Transformers but takes longer than the linear regression, MLP and the multivariate \nTransformer. The LSTM always trains faster than the Transformer with the same train -\ning strategy.\nDiscussion\nBest Transformer training strategy: On the two datasets, the global Transformer is supe -\nrior to the multivariate and local Transformers. We hypothesize that this is a result of \nthe larger number of training samples for the global model (see Table  1). The Trans -\nformer benefits from more training data, even if the training data comes from different \nsources. The multivariate models on the other hand are prone to overfitting.\nBest Transformer architecture: PatchTST is the best model in five out of six cases. \nHowever, the difference to the global Transformer is small. This shows that the success \nTable 3 Training times in hours, measured on a machine with a Nvidia 3090 RTX GPU\nModel Electricity Ausgrid\n24h 96h 720h 24h 96h 720h\nLinear regression (local) 0.02 0.03 0.08 0.02 0.03 0.07\nMLP (local) 0.42 0.42 0.36 0.40 0.39 0.39\nLSTM (multivariate) 0.06 0.08 0.30 0.03 0.04 0.10\nLSTM (local) 8.25 7.61 7.20 4.27 3.55 3.49\nLSTM (global) 1.09 0.82 0.98 1.11 0.84 0.71\nTransformer (multivariate) 0.19 0.23 0.88 0.10 0.09 0.39\nTransformer (local) 14.20 16.82 102.09 8.33 9.74 62.53\nTransformer (global) 3.42 2.00 9.86 3.85 2.85 6.77\nPage 10 of 13Hertel et al. Energy Informatics  2023, 6(Suppl 1):20\nof PatchTST is mainly a result of its global training strategy. Its improvement upon the \nglobal Transformer can be due to the patching mechanism, a better hyperparameter \nconfiguration, or the encoder-only architecture. Among the multivariate models, Auto -\nformer (Wu et al. 2021) and FEDformer (Zhou et al. 2022) give better results than the \nmultivariate Transformer. It remains an open question whether these architectures are \nalso better global models than the standard Transformer and PatchTST (Nie et al. 2022). \nAnother promising architecture is the Temporal Fusion Transformer (Huy et al. 2022). \nIn previous work with just one aggregated time series, the Informer (Zhou et al. 2021) \nalso gave better results than the Transformer (Hertel et al. 2022a).\nComparison with the state of the art: The global Transformer achieves a better result \nfor short-term forecasting on the Electricity dataset than related work (Zhou et al. 2021; \nWu et al. 2021; Zhou et al. 2022; Nie et al. 2022; Zeng et al. 2022), and achieves close \nresults to the best results from PatchTST (Nie et al. 2022) for longer horizons and on \nthe Ausgrid solar home dataset. However, to establish a state of the art for short-term \nand medium-term load forecasting, a comparison to other forecasting models must be \nundertaken, including models that are not based on the Transformer architecture and \nthat are more sophisticated than our baselines. Using weather data could improve the \nforecasts, because some electrical load patterns, such as the usage of electrical heat -\ning, are weather-dependent. Weather features could affect which model gives the best \nresults, because some models might be better in capturing these dependencies than \nothers.\nLinear models: As in related work (Zeng et al. 2022), we observe that linear models are \nstrong baselines. The linear regression is in five out of six cases the best local model and \nonly outperformed by the local MLP for the one day horizon on the Electricity dataset. \nNo general answer can be given on whether the local linear regression models are better \nor the global LTSF-Linear is better, because each variant is better on one dataset.\nTask complexity: For longer horizons, the global Transformer’s performance compared \nto the linear models deteriorates. This can be due to the increasing complexity when the \nmodel forecasts many values simultaneously. We chose a direct multi-step forecasting \nmodel because good results were achieved with this procedure before (Nie et al. 2022; \nZeng et  al. 2022). However, other multi-step forecasting procedures, such as iterative \nsingle-step and iterative multi-step forecasting (An and Anh 2015; Sahoo et  al. 2020), \ncould be beneficial for long-term forecasting because they reduce the number of fore -\ncasted values per model run.\nTransfer learning: According to the definition of transfer learning in Pinto et al. (2022), \nthe global training strategy can be seen as a transfer learning method, because the model \nmust transfer knowledge between different types of buildings with different consump -\ntion patterns. Pre-training on other tasks than forecasting or on less similar data from \ndomains other than electricity, as well as fine-tuning for a time series of interest, could \nimprove the results. An advantage of the global model is that it can be applied to new \ntime series without retraining. In Hertel et al. (2022b) it was shown that the Transformer \ngeneralizes better to new time series than other approaches, but the forecasts are still \nbetter when training data from the target time series is available.\nPage 11 of 13\nHertel et al. Energy Informatics  2023, 6(Suppl 1):20\nOther forecasting tasks: The Transformer model and the different training strategies \nare not designed for load forecasting in particular, but can also be applied to other fore -\ncasting tasks. We hypothesize that the global training strategy can also be beneficial for \nother datasets containing multiple time series with similar patterns.\nConclusion and future work\nWe compare three Transformer training strategies for load forecasting on two datasets \nwith multiple years of data for multiple hundred clients. We show that the multivari -\nate training strategy used in related work on forecasting with Transformers (Zhou et al. \n2021; Wu et  al. 2021; Zhou et  al. 2022) is not optimal, and it is better to use a global \nmodel instead. This shows that the right training strategy is crucial to get good results \nfrom a Transformer. Our approach achieves better results than related work (Zhou \net al. 2021; Wu et al. 2021; Zhou et al. 2022), and comes close to the best results from \nPatchTST (Nie et al. 2022). In particular, our approach gives better results than the lin -\near models from Zeng et al. (2022) for one day to four days forecasting horizons, which \nshows that, with the right training strategy, Transformers are effective for load forecast -\ning. However, simple linear models give decent results for both short-term and medium-\nterm horizons and train much faster than the Transformers.\nIn the future, more sophisticated Transformer architectures could be tested with the \nglobal training strategy. A comparison to other forecasting methods could be under -\ntaken, and weather data could be incorporated into the models to see how it affects the \nresults. Experiments with other datasets and varying amounts of training data could \nshow under which circumstances the global Transformer model is better than other \napproaches. Additionally, transfer learning from other tasks and datasets could be \ntested. Future work could experiment with different datasets with varying amounts of \ndata to see how much training data is needed for the global model to surpass the local \nmodels. A compromise between local and global models could be established by first \nclustering similar time series and then training one global model per cluster. The cluster-\nspecific models would have less training data than the global model, but could benefit \nfrom the training data being more similar. Potentially, the global training strategy could \nalso be beneficial for other forecasting tasks than load forecasting.\nAcknowledgements\nWe thank the anonymous reviewers for their helpful comments.\nAbout this supplement\nThis article has been published as part of Energy Informatics Volume 6 Supplement 1, 2023: Proceedings of the 12th \nDACH+ Conference on Energy Informatics 2023. The full contents of the supplement are available online at https:// \nenerg yinfo rmati cs. sprin gerop en. com/ artic les/ suppl ements/ volume- 6- suppl ement-1.\nAuthor contributions\nMH: Conceptualisation, Investigation, Methodology, Software, Validation, Visualisation, Writing—original draft. MB: Writ-\ning—original draft, Writing—review and editing. BH, ON: Writing—review and editing. BS, RM, VH: Funding acquisition, \nSupervision, Writing—review and editing.\nFunding\nThis project is funded by the Helmholtz Association’s Initiative and Networking Fund through Helmholtz AI, the Helm-\nholtz Association under the Program “Energy System Design” , and the German Research Foundation (DFG) as part of the \nResearch Training Group 2153 “Energy Status Data: Informatics Methods for its Collection, Analysis and Exploitation” .\nAvailibility of data and materials\nSee section Datasets for the sources of the public datasets. Code is available on GitHub via https:// github. com/ KIT- IAI/ \ntrans former- train ing- strat egies.\nPage 12 of 13Hertel et al. Energy Informatics  2023, 6(Suppl 1):20\nDeclarations\nCompeting interests\nThe authors declare that they have no competing interests.\nPublished: 19 October 2023\nReferences\nA gentle introduction to the rectified linear unit (ReLU). https:// machi nelea rning maste ry. com/ recti fied- linear- activ ation- \nfunct ion- for- deep- learn ing- neural- netwo rks/. Accessed 28 Apr 2023\nAn NH, Anh DT (2015) Comparison of strategies for multi-step-ahead prediction of time series using neural network. In: \n2015 International Conference on Advanced Computing and Applications (ACOMP), pp. 142–149\nÇakmak HK, Hagenmeyer V (2022) Using open data for modeling and simulation of the all electrical society in eASiMOV. \nIn: 2022 Open Source Modelling and Simulation of Energy Systems (OSMSES)\nCao Y, Dang Z, Wu F, Xu X, Zhou F (2022) Probabilistic electricity demand forecasting with transformer-guided state space \nmodel. In: 2022 IEEE 5th International Conference on Automation, Electronics and Electrical Engineering (AUTEEE), \npp. 964–969. IEEE\nGao J, Hu W, Zhang D, Chen Y (2022) TgDLF2.0: Theory-guided deep-learning for electrical load forecasting via trans-\nformer and transfer learning. arXiv: 2210. 02448\nGiacomazzi E, Haag F, Hopf K (2023) Short-term electricity load forecasting using the temporal fusion transformer: Effect \nof grid hierarchies and data sources. arXiv preprint arXiv: 2305. 10559\nGrabner M, Wang Y, Wen Q, Blažič B, Štruc V (2023) A global modeling framework for load forecasting in distribution \nnetworks. IEEE Trans Smart Grid (Early Access)\nHaben S, Arora S, Giasemidis G, Voss M, Greetham DV (2021) Review of low voltage load forecasting: methods, applica-\ntions, and recommendations. Appl Energy 304:117798\nHan F, Pu T, Li M, Taylor G (2020) Short-term forecasting of individual residential load based on deep learning and \nk-means clustering. CSEE J Power Energy Syst 7(2):261–269\nHertel M, Ott S, Schäfer B, Mikut R, Hagenmeyer V, Neumann O (2022) Evaluation of transformer architectures for electri-\ncal load time-series forecasting. In: Proceedings 32. Workshop Computational Intelligence\nHertel M, Ott S, Schäfer B, Mikut R, Hagenmeyer V, Neumann O (2022) Transformer neural networks for building load \nforecasting. In: Tackling Climate Change with Machine Learning: Workshop at NeurIPS 2022\nHimeur Y, Elnour M, Fadli F, Meskin N, Petri I, Rezgui Y, Bensaali F, Amira A (2022) Next-generation energy systems for \nsustainable smart cities: roles of transfer learning. Sustain Cities Soc 85:104059\nHochreiter S, Schmidhuber J (1997) Long short-term memory. Neural Comput 9(8):1735–1780\nHong T, Pinson P , Wang Y, Weron R, Yang D, Zareipour H (2020) Energy forecasting: a review and outlook. IEEE Open \nAccess J Power Energy 7:376–388\nHuy PC, Minh NQ, Tien ND, Anh TTQ (2022) Short-term electricity load forecasting based on temporal fusion transformer \nmodel. IEEE Access 10:106296–106304\nKong W, Dong ZY, Jia Y, Hill DJ, Xu Y, Zhang Y (2017) Short-term residential load forecasting based on LSTM recurrent \nneural network. IEEE Trans Smart Grid 10(1):841–851\nLai G, Chang W-C, Yang Y, Liu H (2018) Modeling long-and short-term temporal patterns with deep neural networks. In: \nThe 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, pp. 95–104\nLoshchilov I, Hutter F (2019) Decoupled weight decay regularization. In: 7th International Conference on Learning Repre-\nsentations, ICLR 2019, New Orleans, LA, USA, May 6–9, 2019\nMachowski J, Bialek J, Bumby JR, Bumby J (1997) Power system dynamics and stability. Wiley, USA\nMurphy WMJ, Chen K (2023) Univariate vs multivariate time series forecasting with transformers. https:// openr eview. net/ \nforum? id= GpW32 7gxLTF\nNawar M, Shomer M, Faddel S, Gong H (2023) Transfer learning in deep learning models for building load forecasting: \nCase of limited data. arXiv: 2301. 10663\nNie Y, Nguyen NH, Sinthong P , Kalagnanam J (2022) A time series is worth 64 words: long-term forecasting with trans-\nformers. arXiv: 2211. 14730\nOrdiano JÁG, Waczowicz S, Hagenmeyer V, Mikut R (2018) Energy forecasting tools and services. WIREs Data Mining \nKnowl Discov 8(2)\nPinto G, Wang Z, Roy A, Hong T, Capozzoli A (2022) Transfer learning for smart buildings: a critical review of algorithms, \napplications, and future perspectives. Adv Appl Energy 100084\nPörtner H-O, Roberts DC, Adams H, Adler C, Aldunce P , Ali E, Begum RA, Betts R, Kerr RB, Biesbroek R et al (2022) Climate \nchange 2022: impacts, adaptation and vulnerability. IPCC Geneva, Switzerland\nRamchurn SD, Vytelingum P , Rogers A, Jennings NR (2012) Putting the “smarts” into the smart grid: a grand challenge for \nartificial intelligence. Commun ACM 55(4):86–97\nRan P , Dong K, Liu X, Wang J (2023) Short-term load forecasting based on CEEMDAN and transformer. Electric Power Syst \nRes 214:108885\nRodrigues F, Trindade A (2018) Load forecasting through functional clustering and ensemble learning. Knowl Informat \nSyst 57(1):229–244\nSahoo D, Sood N, Rani U, Abraham G, Dutt V, Dileep A (2020) Comparative analysis of multi-step time-series forecasting \nfor network load dataset. In: 2020 11th International Conference on Computing, Communication and Networking \nTechnologies (ICCCNT), pp. 1–7\nShi H, Xu M, Li R (2017) Deep learning for household load forecasting—a novel pooling deep RNN. IEEE Trans Smart Grid \n9(5):5271–5280\nPage 13 of 13\nHertel et al. Energy Informatics  2023, 6(Suppl 1):20\n \nVaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser L, Polosukhin I (2017) Attention is all you need. In: \nNIPS, pp. 5998–6008\nvom Scheidt F, Medinová H, Ludwig N, Richter B, Staudt P , Weinhardt C (2020) Data analytics in the electricity sector—a \nquantitative and qualitative literature review. Energy AI 1:100009\nVoß M, Bender-Saebelkampf C, Albayrak S (2018) Residential short-term load forecasting using convolutional neural \nnetworks. In: 2018 IEEE International Conference on Communications, Control, and Computing Technologies for \nSmart Grids (SmartGridComm), pp. 1–6\nWang C, Wang Y, Ding Z, Zheng T, Hu J, Zhang K (2022) A transformer-based method of multienergy load forecasting in \nintegrated energy system. IEEE Trans Smart Grid 13(4):2703–2714\nWerling D, Heidrich B, Çakmak HK, Hagenmeyer V (2022) Towards line-restricted dispatchable feeders using probabilistic \nforecasts for PV-dominated low-voltage distribution grids. In: Proceedings of the Thirteenth ACM International \nConference on Future Energy Systems, pp. 395–400\nWu N, Green B, Ben X, O’Banion S (2020) Deep transformer models for time series forecasting: The influenza prevalence \ncase. arXiv preprint arXiv: 2001. 08317\nWu H, Xu J, Wang J, Long M (2021) Autoformer: decomposition transformers with auto-correlation for long-term series \nforecasting. In: NeurIPS, pp. 22419–22430\nYang E, Youn C-H (2021) Individual load forecasting for multi-customers with distribution-aware temporal pooling. In: \nIEEE INFOCOM 2021-IEEE Conference on Computer Communications, pp. 1–10\nYildiz B, Bilbao JI, Dore J, Sproul AB (2017) Recent advances in the analysis of residential electricity consumption and \napplications of smart meter data. Appl Energy 208:402–427\nZeng A, Chen M, Zhang L, Xu Q (2022) Are transformers effective for time series forecasting? arXiv: 2205. 13504\nZhang G, Wei C, Jing C, Wang Y (2022) Short-term electrical load forecasting based on time augmented transformer. Int J \nComput Intell Syst 15(1):67\nZhou T, Ma Z, Wen Q, Wang X, Sun L, Jin R (2022) FEDformer: frequency enhanced decomposed transformer for long-\nterm series forecasting. In: International Conference on Machine Learning, pp. 27268–27286\nZhou H, Zhang S, Peng J, Zhang S, Li J, Xiong H, Zhang W (2021) Informer: Beyond efficient transformer for long \nsequence time-series forecasting. In: AAAI, pp. 11106–11115\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations."
}