{
  "title": "Active Retrieval Augmented Generation",
  "url": "https://openalex.org/W4389519118",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2947056188",
      "name": "Zhengbao Jiang",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2331775461",
      "name": "Frank Xu",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2734668338",
      "name": "Luyu Gao",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2139640379",
      "name": "Zhiqing Sun",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2100110296",
      "name": "Qian Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4308012672",
      "name": "Jane Dwivedi-Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098495947",
      "name": "Yi-Ming Yang",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2148123616",
      "name": "Jamie Callan",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A277131583",
      "name": "Graham Neubig",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4297899309",
    "https://openalex.org/W4312091845",
    "https://openalex.org/W2898875342",
    "https://openalex.org/W4221152111",
    "https://openalex.org/W4312091849",
    "https://openalex.org/W4312091824",
    "https://openalex.org/W4287393336",
    "https://openalex.org/W4301243929",
    "https://openalex.org/W3115947671",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W3169841173",
    "https://openalex.org/W4377142519",
    "https://openalex.org/W4376312383",
    "https://openalex.org/W4313304293",
    "https://openalex.org/W3142525300",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W4288087322",
    "https://openalex.org/W4389520103",
    "https://openalex.org/W3176182290",
    "https://openalex.org/W4322718421",
    "https://openalex.org/W1571498871",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W4388778348",
    "https://openalex.org/W4385573075",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W3034383590",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W4378508578",
    "https://openalex.org/W4298186941",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W2950681488",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4360834937",
    "https://openalex.org/W4302305884",
    "https://openalex.org/W4309955236",
    "https://openalex.org/W3173343821",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W4385572754",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4309088836",
    "https://openalex.org/W4385573898",
    "https://openalex.org/W4303441863",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W3199958362",
    "https://openalex.org/W4318719006",
    "https://openalex.org/W3176503701",
    "https://openalex.org/W4312091875",
    "https://openalex.org/W4310827238",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4364384032"
  ],
  "abstract": "Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7969–7992\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nActive Retrieval Augmented Generation\nZhengbao Jiang1∗ Frank F. Xu1∗ Luyu Gao1∗ Zhiqing Sun1∗ Qian Liu2\nJane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1\n1Language Technologies Institute, Carnegie Mellon University\n2Sea AI Lab 3FAIR, Meta\n{zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu\nAbstract\nDespite the remarkable ability of large lan-\nguage models (LMs) to comprehend and gen-\nerate language, they have a tendency to hal-\nlucinate and create factually inaccurate out-\nput. Augmenting LMs by retrieving informa-\ntion from external knowledge resources is one\npromising solution. Most existing retrieval aug-\nmented LMs employ a retrieve-and-generate\nsetup that only retrieves information once based\non the input. This is limiting, however, in\nmore general scenarios involving generation\nof long texts, where continually gathering in-\nformation throughout generation is essential. In\nthis work, we provide a generalized view of ac-\ntive retrieval augmented generation, methods\nthat actively decide when and what to retrieve\nacross the course of the generation. We propose\nForward-Looking Active REtrieval augmented\ngeneration (FLARE), a generic method which\niteratively uses a prediction of the upcoming\nsentence to anticipate future content, which is\nthen utilized as a query to retrieve relevant doc-\numents to regenerate the sentence if it contains\nlow-confidence tokens. We test FLARE along\nwith baselines comprehensively over 4 long-\nform knowledge-intensive generation tasks/-\ndatasets. FLARE achieves superior or compet-\nitive performance on all tasks, demonstrating\nthe effectiveness of our method.1\n1 Introduction\nGenerative language models (LMs) (Brown et al.,\n2020; Ouyang et al., 2022; OpenAI, 2023; Chowd-\nhery et al., 2022; Zhang et al., 2022; Touvron et al.,\n2023; Zhao et al., 2023) have become a founda-\ntional component in natural language processing\n(NLP) systems with their remarkable abilities. Al-\nthough LMs have memorized some world knowl-\nedge during training (Petroni et al., 2019; Roberts\net al., 2020; Jiang et al., 2020), they still tend to\n∗Lead contributors.\n1Code and datasets are available athttps://github.com/\njzbjyb/FLARE.\nhallucinate and create imaginary content (Maynez\net al., 2020; Zhou et al., 2021). Augmenting LMs\nwith retrieval components that look up relevant in-\nformation from external knowledge resources is a\npromising direction to address hallucination (Khan-\ndelwal et al., 2020; Izacard et al., 2022).\nRetrieval augmented LMs commonly use a\nretrieve-and-generate setup where they retrieve doc-\numents based on the user’s input, and then generate\na complete answer conditioning on the retrieved\ndocuments (Chen et al., 2017; Guu et al., 2020;\nLewis et al., 2020; Izacard and Grave, 2021; Sachan\net al., 2021; Lee et al., 2021; Jiang et al., 2022;\nIzacard et al., 2022; Nakano et al., 2021; Qian\net al., 2023; Lazaridou et al., 2022; Shi et al., 2023).\nThese single-time retrieval augmented LMs outper-\nform purely parametric LMs, particularly for short-\nform knowledge-intensive generation tasks such\nas factoid question answering (QA) (Kwiatkowski\net al., 2019; Joshi et al., 2017), where the informa-\ntion needs are clear in the user’s input, and it is\nsufficient to retrieve relevant knowledge once solely\nbased on the input.\nIncreasingly powerful large LMs have also\ndemonstrated abilities in more complex tasks that\ninvolve generating long-form output, such as long-\nform QA (Fan et al., 2019; Stelmakh et al., 2022),\nopen-domain summarization (Cohen et al., 2021;\nHayashi et al., 2021; Giorgi et al., 2022), and\n(chain-of-thought; CoT) reasoning (Wei et al.,\n2022; Ho et al., 2020; Geva et al., 2021; Hendrycks\net al., 2020). In contrast to short-form generation,\nlong-form generation presents complex informa-\ntion needs that are not always evident from the in-\nput alone. Similar to how humans gradually gather\ninformation as we create content such as papers,\nessays, or books, long-form generation with LMs\nwould require gathering multiple pieces of knowl-\nedge throughout the generation process . For ex-\nample, to generate a summary about a particular\ntopic, the initial retrieval based on the topic name\n7969\nGenerate a summary about Joe Biden.\nSearch results:   !![1]: …[2]: …\nJoe Biden (born November 20, 1942) is the 46th president of the United States.Joe Biden (born November 20, 1942) is the 46th president of the United States.\nHe graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science.\nJoe Biden attended the University of Pennsylvania, where he earned a law degree.\nRetriever\nInputStep 1\nSearch results:   !\"![1]: …[2]: …\n\"####\n\"#$\n#$\n$\nStep 2\nJoe Biden announced his candidacy for the 2020 presidential election on April 25, 2019.\nJoe Biden announced his candidacy for the 2020 presidential election on August 18, 2019.\"#%\n#%\nStep 3\nSearch results:   !\"\"[1]: …[2]: …Retrieveddocuments\nLM\nGeneration\n$\n%$\n%%\nFigure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user\ninput x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray\nitalic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the\nsystem retrieves relevant documents and regenerates the sentence.\n(e.g., Joe Biden) may not cover all aspects and de-\ntails. It is crucial to retrieve extra information as\nneeded during generation, such as when generat-\ning a certain aspect (e.g., Joe Biden’s education\nhistory) or a specific detail (e.g., the date of Joe\nBiden’s presidential campaign announcement).\nSeveral attempts have been made to retrieve mul-\ntiple times throughout generation. These attempts\ninclude methods that passively use the past context\nto retrieve additional information at a fixed interval\n(Khandelwal et al., 2020; Borgeaud et al., 2022;\nRam et al., 2023; Trivedi et al., 2022) which might\nnot accurately reflect what LMs intend to gener-\nate in the future or retrieve at inappropriate points.\nSome works in multihop QA decompose the full\nquestion into sub-questions, each of which is used\nto retrieve extra information (Press et al., 2022; Yao\net al., 2022; Khot et al., 2022; Khattab et al., 2022).\nWe ask the following question: can we create a\nsimple and generic retrieval augmented LM thatac-\ntively decides when and what to retrievethroughout\nthe generation process, and are applicable to a va-\nriety of long-form generation tasks? We provide a\ngeneralized view of active retrieval augmented gen-\neration. Our hypothesis regarding when to retrieve\nis that LMs should retrieve information only when\nthey lack the required knowledge to avoid unneces-\nsary or inappropriate retrieval that occurs in passive\nretrieval augmented LMs (Khandelwal et al., 2020;\nBorgeaud et al., 2022; Ram et al., 2023; Trivedi\net al., 2022). Given the observation that large LMs\ntend to be well-calibrated and low probability/con-\nfidence often indicates a lack of knowledge (Ka-\ndavath et al., 2022), we adopt an active retrieval\nstrategy that only retrieves when LMs generate low-\nprobability tokens. When deciding what to retrieve,\nit is important to consider what LMs intend to gen-\nerate in the future, as the goal of active retrieval is to\nbenefit future generations. Therefore, we propose\nanticipating the future by generating a temporary\nnext sentence, using it as a query to retrieve rel-\nevant documents, and then regenerating the next\nsentence conditioning on the retrieved documents.\nCombining the two aspects, we propose Forward-\nLooking Active REtrieval augmented generation\n(FLARE), as illustrated in Figure 1. FLARE iter-\natively generates a temporary next sentence, use\nit as the query to retrieve relevant documents if it\ncontains low-probability tokens and regenerate the\nnext sentence until reaches the end.\nFLARE is applicable to any existing LMs at\ninference time without additional training. Con-\n7970\nsidering the impressive performance achieved by\nGPT-3.5 (Ouyang et al., 2022) on a variety of\ntasks, we examine the effectiveness of our meth-\nods on text-davinci-003. We evaluate FLARE\non 4 diverse tasks/datasets involving generating\nlong outputs, including multihop QA (2WikiMul-\ntihopQA), commonsense reasoning (StrategyQA),\nlong-form QA (ASQA), and open-domain summa-\nrization (WikiAsp) (Ho et al., 2020; Geva et al.,\n2021; Stelmakh et al., 2022; Hayashi et al., 2021).\nOver all tasks, FLARE achieves superior or com-\npetitive performance compared to single-time and\nmulti-time retrieval baselines, demonstrating the\neffectiveness and generalizability of our method.\n2 Retrieval Augmented Generation\nWe formally define single-time retrieval augmented\ngeneration and propose the framework of active\nretrieval augmented generation.\n2.1 Notations and Definitions\nGiven a user input x and a document corpus D=\n{di}|D|\ni=1 (such as all Wikipedia articles), the goal of\nretrieval augmented LMs is to generate the answer\ny = [s1,s2,..., sm] = [w1,w2,...,w n] containing\nm sentences or n tokens leveraging information\nretrieved from the corpus.\nIn retrieval augmented LM, the LM typically\npairs with a retriever that can retrieve a list of\ndocuments Dq = ret(q) for a query q; the LM\nconditions on both the user input x and retrieved\ndocuments Dq to generate the answer. Since we\nfocus on examining various methods of determin-\ning when and what to retrieve, we follow exist-\ning methods (Ram et al., 2023; Trivedi et al.,\n2022) to prepend the retrieved documents before\nthe user input to aid future generation for both\nbaselines and our method for fair comparisons:\ny = LM([Dq,x]), where [·,·] is concatenation fol-\nlowing the specified order.\n2.2 Single-time Retrieval Augmented\nGeneration\nThe most common choice is to directly use the user\ninput as the query for retrieval and generate the\ncomplete answer at once y = LM([Dx,x]).\n2.3 Active Retrieval Augmented Generation\nTo aid long-form generation with retrieval, we pro-\npose active retrieval augmented generation. It is a\ngeneric framework that actively decides when and\nwhat to retrieve through the generation process,\nresulting in the interleaving of retrieval and genera-\ntion. Formally, at step t(t≥1), the retrieval query\nqt is formulated based on both the user input x and\npreviously generated output y<t = [y0,..., yt−1]:\nqt = qry(x,y<t),\nwhere qry(·) is the query formulation function. At\nthe beginning (t = 1), the previous generation is\nempty (y<1 = ∅), and the user input is used as the\ninitial query (q1 = x). Given retrieved documents\nDqt, LMs continually generate the answer until the\nnext retrieval is triggered or reaches the end:\nyt = LM([Dqt,x,y<t]),\nwhere yt represents the generated tokens at the cur-\nrent step t, and the input to LMs is the concatena-\ntion of the retrieved documents Dqt, the user input\nx, and the previous generation y<t. We discard\npreviously retrieved documents ∪t′<tDqt′ and only\nuse the retrieved documents from the current step\nto condition the next generation to prevent reaching\nthe input length limit of LMs.\n3 FLARE: Forward-Looking Active\nREtrieval Augmented Generation\nOur intuition is that (1) LMs should only retrieve\ninformation when they do not have the necessary\nknowledge to avoid unnecessary or inappropriate\nretrieval, and (2) the retrieval queries should reflect\nthe intents of future generations. We propose two\nforward-looking active retrieval augmented gener-\nation (FLARE) methods to implement the active\nretrieval augmented generation framework. The\nfirst method prompts the LM to generate retrieval\nqueries when necessary while generating the an-\nswer using retrieval-encouraging instructions, de-\nnoted as FLAREinstruct. The second method directly\nuses the LM’s generation as search queries, denoted\nas FLAREdirect, which iteratively generates the next\nsentence to gain insight into the future topic, and\nif uncertain tokens are present, retrieves relevant\ndocuments to regenerate the next sentence.\n3.1 FLARE with Retrieval Instructions\nInspired by Toolformer (Schick et al., 2023), a\nstraightforward way of expressing information\nneeds for retrieval is to generate “[Search(query)]”\nwhen additional information is needed (Schick\net al., 2023), e.g., “The colors on the flag of\nGhana have the following meanings. Red is for\n[Search(Ghana flag red meaning)] the blood of mar-\ntyrs, ...” When working with GPT-3.5 models that\n7971\nSearch results:   !![1]: …[2]: …\nJoe Biden attended\nSearch results:   !\"![1]: …[2]: …Search results:   !\"\"[1]: …[2]: …\n[Search(Joe Biden University)]\n[Search(Joe Biden degree)]the University of Pennsylvania, where he earned\na law degree.\nGenerate a summary about Joe Biden.Input$\n&$\n&#%$\n&%%%Generation\nRetriever\n$\n%$\n%%\nFigure 2: An illustration of forward-looking active re-\ntrieval augmented generation with retrieval instructions\n(FLAREinstruct). It iteratively generates search queries\n(shown in gray italic) to retrieve relevant information to\naid future generations.\noffer only API access, we elicit such behavior by\nfew-shot prompting (Brown et al., 2020).\nSpecifically, for a downstream task, we place\nthe search-related instruction and exemplars at the\nbeginning as skill 1, followed by the instruction and\nexemplars of the downstream task as skill 2. Given\na test case, we ask LMs to combine skills 1 and 2 to\ngenerate search queries while performing the task.\nThe structure of the prompt is shown in Prompt 3.1,\nand full details can be found in Prompt D.3.\nPrompt 3.1: retrieval instructions\nSkill 1. An instruction to guide LMs to generate search\nqueries.\nSeveral search-related exemplars.\nSkill 2. An instruction to guide LMs to perform a\nspecific downstream task (e.g., multihop QA).\nSeveral task-related exemplars.\nAn instruction to guide LMs to combine skills 1\nand 2 for the test case.\nThe input of the test case.\nAs shown in Figure 2, when the LM generates\n“[Search(query)]” (shown in gray italic), we stop\nthe generation and use the query terms to retrieve\nrelevant documents, which are prepended before\nthe user input to aid future generation until the\nnext search query is generated or reaches the end.\nAdditional implementation details are included in\nAppendix A.\n3.2 Direct FLARE\nSince we cannot fine-tune black-box LMs, we\nfound queries generated by FLAREinstruct through\nretrieval instructions might not be reliable. There-\nfore, we propose a more direct way of forward-\nlooking active retrieval that uses the next sentence\nto decide when and what to retrieve.\n3.2.1 Confidence-based Active Retrieval\nAs shown in Figure 1, at step t, we first generate a\ntemporary next sentence ˆst = LM([x,y<t]) with-\nout conditioning on retrieved documents. Then we\ndecide whether to trigger retrieval and formulate\nqueries based on ˆst. If the LM is confident aboutˆst,\nwe accept it without retrieving additional informa-\ntion; if not, we use ˆst to formulate search queries\nqt to retrieve relevant documents, and then regen-\nerate the next sentence st. The reason we utilize\nsentences as the basis of our iteration is due to their\nsignificance as semantic units that are neither too\nshort nor too lengthy like phrases and paragraphs.\nHowever, our approach can also utilize phrases or\nparagraphs as the basis.\nSince LMs tend to be well-calibrated that low\nprobability/confidence often indicates a lack of\nknowledge (Jiang et al., 2021; Kadavath et al.,\n2022; Varshney et al., 2022), we actively trigger\nretrieval if any token of ˆst has a probability lower\nthan a threshold θ ∈[0,1]. θ = 0means retrieval\nis never triggered, while θ = 1triggers retrieval\nevery sentence.\nyt =\n{\nˆst if all tokens of ˆst have probs ≥θ\nst = LM([Dqt,x,y<t]) otherwise\nwhere the query qt is formulated based on ˆst.\n3.2.2 Confidence-based Query Formulation\nOne way to perform retrieval is to directly use the\nnext sentence ˆst as the query qt. This shares a sim-\nilar spirit with methods that use generated hypo-\nthetical titles or paragraphs from LMs as retrieval\nqueries or evidences (Gao et al., 2022; Sun et al.,\n2022; Yu et al., 2022; Mao et al., 2021). We gen-\neralize such techniques to long-form generation\nwhere active information access is essential.\nWe found retrieving with the next sentence\nachieves significantly better results than with the\nprevious context, as shown later in subsection 6.2.\nHowever, it has a risk of perpetuating errors con-\ntained in it. For example, if the LM produces the\nsentence “Joe Biden attended the University of\nPennsylvania” instead of the correct fact that he\nattended the University of Delaware, using this er-\nroneous sentence as a query might retrieve mislead-\n7972\nJoe Biden attended the University of Pennsylvania, where he earned a law degree.\nAsk a question to which the answer is “the University of Pennsylvania”Ask a question to which the answer is “a law degree”\nWhat university did Joe Biden attend?What degree did Joe Biden earn?\nimplicit query by maskingexplicit query by question generationJoe Biden attended  , where he earned  .\nLM such as ChatGPT\nFigure 3: Implicit and explicit query formulation. To-\nkens with low probabilities are marked with underlines.\ning information. We propose two simple methods\nto overcome this issue as illustrated in Figure 3.\nMasked sentences as implicit queries. The first\nmethod masks out low-confidence tokens inˆst with\nprobabilities below a threshold β ∈[0,1], where a\nhigher βresults in more aggressive masking. This\nremoves potential distractions from the sentence to\nimprove retrieval accuracy.\nGenerated questions as explicit queries. An-\nother method is to generate explicit questions that\ntarget the low-confident span in ˆst. For example, if\nthe LM is uncertain about “the University of Penn-\nsylvania”, a question like “Which university did\nJoe Biden attend?” can help retrieve relevant in-\nformation. Self-ask (Press et al., 2022) achieved\nthis by manually inserting follow-up questions\ninto downstream task exemplars as shown later\nin Prompt D.2, which requires task-specific annota-\ntion efforts. Instead, we developed a universal ap-\nproach that generates questions for low-confidence\nspans without additional annotation. Specifically,\nWe first extract all spans from ˆst with probabilities\nbelow β. For each extracted span z, we prompt\ngpt-3.5-turbo to generate a question qt,z that\ncan be answered with the span:\nPrompt 3.2: zero-shot question generation\nUser input x.\nGenerated output so far y≤t.\nGiven the above passage, ask a question to which\nthe answer is the term/entity/phrase “z”.\nWe retrieve using each generated question and\ninterleave the returned documents into a single\nranking list to aid future generations. In summary,\nqueries qt are formulated based on ˆst as follows:\nqt =\n{\n∅ if all tokens of ˆst have probs ≥θ\nmask(ˆst) or qgen(ˆst) otherwise\n3.3 Implementation Details\nBase LM We validate our method on one of the\nmost advanced GPT-3.5 LMs text-davinci-003\nby iteratively querying their API.2\nDocument corpus and retrievers. Since we fo-\ncus on the integration of retrieval and generation,\nwe use off-the-shelf retrievers that take queries\nas inputs and return a list of relevant documents.\nFor datasets that mainly rely on knowledge from\nWikipedia, we use the Wikipedia dump from\nKarpukhin et al. (2020) and employ BM25 (Robert-\nson and Zaragoza, 2009) as the retriever. For\ndatasets that rely on knowledge from the open web,\nwe use the Bing search engine as our retriever.3\nRetrieved document formatting. Multiple re-\ntrieved documents are linearized according to their\nranking and then added to the beginning of the user\ninput using Prompt D.1.\nOther implementation details such as sentence to-\nkenization and efficiency are included Appendix A.\n4 Multi-time Retrieval Baselines\nExisting passive multi-time retrieval augmented\nLMs can also be formulated using our framework\n(subsection 2.3). In this section, we formally in-\ntroduce three baseline categories based on when\nand what to retrieve. These baselines are not exact\nreproductions of the corresponding paper because\nmany design choices differ which makes direct\ncomparisons impossible. We implemented them\nusing the same settings, with the only variation\nbeing when and what to retrieve.\nPrevious-window approaches trigger retrieval\nevery ltokens, where lrepresents the window size.\nGenerated tokens from the previous window are\nused as the query:\nqt = yt−1 (t≥2),\nyt = [w(t−1)l+1,...,w tl].\nSome existing methods in this category are RETRO\n(Borgeaud et al., 2022), IC-RALM (Ram et al.,\n2https://api.openai.com/v1/completions April 23.\n3https://www.microsoft.com/en-us/bing/apis/\nbing-web-search-api\n7973\n2023), which retrieve every few tokens, and KNN-\nLM (Khandelwal et al., 2020), which retrieves ev-\nery token.4 We follow Ram et al. (2023) to use a\nwindow size of l= 16.\nPrevious-sentence approaches trigger retrieval\nevery sentence and use the previous sentence as the\nquery, and IRCoT (Trivedi et al., 2022) belongs to\nthis category:\nqt = yt−1 (t≥2),\nyt = st.\nQuestion decomposition approaches manually\nannotated task-specific exemplars to guide LMs\nto generate decomposed sub-questions while pro-\nducing outputs. For example, self-ask (Press et al.,\n2022), a method in this category, manually inserts\nsub-questions in exemplars using Prompt D.2. For\nthe test case, retrieval is triggered dynamically\nwhenever the model generates a sub-question.\nThe aforementioned approaches can retrieve ad-\nditional information while generating. However,\nthey have notable drawbacks: (1) Using previously\ngenerated tokens as queries might not reflect what\nLMs intend to generate in the future. (2) Retriev-\ning information at a fixed interval can be inefficient\nbecause it might occur at inappropriate points. (3)\nQuestion decomposition approaches require task-\nspecific prompt engineering, which restricts their\ngeneralizability in new tasks.\n5 Experimental Setup\nWe evaluate the effectiveness of FLARE on 4 di-\nverse knowledge-intensive tasks using few-shot in-\ncontext learning (Radford et al., 2019; Brown et al.,\n2020; Liu et al., 2023). We follow previous works\n(Trivedi et al., 2022) to sub-sample at most 500\nexamples from each dataset due to the cost of run-\nning experiments. Datasets, metrics, and settings\nare summarized in Table 7 of Appendix B. The\nhyperparameters of FLARE are selected based on\nthe development set and listed in Table 9. FLARE\nrefers to FLAREdirect if not specifically stated.\nMultihop QA The goal of multihop QA is to\nanswer complex questions through information re-\ntrieval and reasoning. We use 2WikiMultihopQA\n(Ho et al., 2020) which contains 2-hop complex\n4Since KNN-LM uses the contextualized representation\ncorresponding to the current decoding position to retrieve rel-\nevant information which encodes all previous tokens. Strictly\nspeaking, qt should be y<t.\nquestions sourced from Wikipedia articles that re-\nquire composition, comparison, or inference, e.g.,\n“Why did the founder of Versus die?” We follow\nWang et al. (2022) to generate both the chain-of-\nthought and the final answer. Experimental setting\ndetails are included in Appendix B.\nWe use regular expressions to extract the final\nanswer from the output and compare it with the ref-\nerence answer using exact match (EM), and token-\nlevel F1, precision, and recall.\nCommonsense reasoning Commonsense reason-\ning requires world and commonsense knowledge\nto generate answers. We use StrategyQA (Geva\net al., 2021) which is a collection of crowdsourced\nyes/no questions, e.g., “Would a pear sink in wa-\nter?” We follow Wei et al. (2022) to generate both\nthe chain-of-thought and the final yes/no answer.\nDetails are included in Appendix B.\nWe extract the final answer and match it against\nthe gold answer using exact match.\nLong-form QA Long-form QA aims to generate\ncomprehensive answers to questions seeking com-\nplex information (Fan et al., 2019; Stelmakh et al.,\n2022). We use ASQA (Stelmakh et al., 2022) as our\ntestbed where inputs are ambiguous questions with\nmultiple interpretations, and outputs should cover\nall of them. For example, “Where do the Philadel-\nphia Eagles play their home games?” could be\nasking about the city, sports complex, or stadium.\nWe found in many cases it is challenging even for\nhumans to identify which aspect of the question\nis ambiguous. Therefore, we created another set-\nting (ASQA-hint) where we provide a brief hint\nto guide LMs to stay on track when generating an-\nswers. The hint for the above case is “This question\nis ambiguous in terms of which specific location or\nvenue is being referred to.” Experimental setting\ndetails are included in Appendix B.\nWe use metrics from Stelmakh et al. (2022), in-\ncluding EM, RoBERTa-based QA score (Disambig-\nF1), ROUGE (Lin, 2004), and an overall score com-\nbining Disambig-F1 and ROUGE (DR).\nOpen-domain summarization The goal of open-\ndomain summarization is to generate a comprehen-\nsive summary about a topic by gathering informa-\ntion from open web (Giorgi et al., 2022). We use\nWikiAsp (Hayashi et al., 2021) which aims to gen-\nerate aspect-based summaries about entities from\n20 domains in Wikipedia, e.g., “Generate a sum-\nmary about Echo School (Oregon) including the\n7974\n0.0\n20.0\n40.0\n60.0\n80.0\n2WikiMultihopQA StrategyQA ASQA ASQA-hint WikiAsp\nNo ret. Single-time ret. Previous-window ret. Forward-Looking Active REtrieval augmented generation (FLARE)\nFigure 4: Comparision between FLARE and baselines across all tasks/datasets. We report the primary metric for\neach dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp.\nfollowing aspects: academics, history.” Experimen-\ntal setting details are included in Appendix B.\nMetrics include ROUGE, named entity-based F1,\nand UniEval (Zhong et al., 2022) which measures\nfactual consistency.\n6 Experimental Results\nWe first report overall results across 4 tasks/datasets\nand compare the performance of FLARE with all\nthe baselines introduced in section 4. We then\nrun ablation experiments to study the efficacy of\nvarious design choices of our method.\n6.1 Comparison with Baselines\nOverall results. The overall performance of\nFLARE and baseline across all tasks/datasets are\nreported in Figure 4. FLARE outperforms all base-\nline on all tasks/datasets, indicating that FLARE\nis a generic method that can effectively retrieve\nadditional information throughout the generation.\nAmong various tasks, multihop QA shows the\nmost significant improvement. This is largely due\nto the task’s clear definition and specific objective\nof producing the final answer through a 2-hop rea-\nsoning process, which makes it easier for LMs to\ngenerate on-topic output. In contrast, ASQA and\nWikiAsp are more open-ended, which increases the\ndifficulty of both generation and evaluation. The\nimprovement on ASQA-hint is larger than that of\nASQA because identifying ambiguous aspects is\nchallenging even for humans in many cases, and\nproviding a generic hint helps LMs to stay on topic.\nThorough comparisons with baselines. The per-\nformance of all baselines on 2WikiMultihopQA\nare reported in Table 1. FLARE outperforms all\nbaselines by a large margin, which confirms that\nforward-looking active retrieval is highly effective.\nMost multi-time retrieval augmented approaches\noutperform single-time retrieval but with different\nMethods EM F 1 Prec. Rec.\nNo retrieval 28.2 36.8 36.5 38.6\nSingle-time retrieval 39.4 48.8 48.6 51.5\nMulti-time retrieval\nPrevious-window 43.2 52.3 51.7 54.5\nPrevious-sentence 39.0 49.2 48.9 51.8\nQuestion decomposition 47.8 56.4 56.1 58.6\nFLAREinstruct (ours) 42.4 49.8 49.1 52.5\nFLAREdirect (ours) 51.0 59.7 59.1 62.6\nTable 1: FLARE and baselines on 2WikiMultihopQA.\nPrevious-window (Borgeaud et al., 2022; Ram et al.,\n2023), previous-sentence (Trivedi et al., 2022), and ques-\ntion decomposition (Press et al., 2022; Yao et al., 2022)\nmethods are reimplemented for fair comparisons.\nmargins. The improvement of retrieving using the\nprevious sentence is relatively small which we hy-\npothesize is mainly because the previous sentence\noften describes entities or relations different from\nthose in the next sentence in 2WikiMultihopQA.\nWhile the previous-window approach might use\nthe first half of a sentence to retrieve information\npotentially helpful for generating the second half.\nAmong all baselines, the question decomposition\napproach (Press et al., 2022) achieves the best per-\nformance. which is not surprising since the in-\ncontext exemplars manually annotated with decom-\nposed sub-questions (Prompt D.2) guide LMs to\ngenerate sub-questions that align with the topic/in-\ntent of future generations. FLARE outperforms\nthis baseline, indicating that manual exemplar an-\nnotation is not necessary for effective future-aware\nretrieval. The gap between FLAREinstruct and ques-\ntion decomposition is large, indicating that teaching\nLMs to generate search queries using task-generic\nretrieval instructions and exemplars is challenging.\nWe report all metrics for the other datasets in\nTable 2. FLARE outperforms baselines with re-\nspect to all metrics. Retrieval using the previ-\n7975\nDatasets StrategyQA ASQA ASQA-hint WikiAsp\nMetrics EM EM D-F 1 R-L DR EM D-F 1 R-L DR UniEval E-F1 R-L\nNo retrieval 72.9 33.8 24.2 33.3 28.4 40.1 32.5 36.4 34.4 47.1 14.1 26.4\nSingle-time retrieval 68.6 40.0 27.1 34.0 30.4 43.2 34.8 37.4 36.0 52.4 17.4 26.9\nMulti-time retrieval\nPrevious-window 71.2 39.9 27.0 34.3 30.4 43.7 35.7 37.5 36.6 51.8 18.1 27.3\nPrevious-sentence 71.0 39.9 27.9 34.3 30.9 44.7 35.9 37.5 36.7 52.6 17.8 27.2\nFLARE (ours) 77.3 41.3 28.2 34.3 31.1 46.2 36.7 37.7 37.2 53.4 18.9 27.6\nTable 2: Comparison between FLARE and baselines on StrategyQA, ASQA, ASQA-hint, and WikiAsp. D-F1 is\nDisambig-F1, R-L is ROUGE-L, and E-F1 is named entity-based F1.\n2WikiMultihopQA ASQA-hint\nEM F 1 Prec. Rec. EM D-F 1 R-L DR\nPrevious 39.0 49.2 48.9 51.8 42.5 34.1 36.9 35.5\nNext 48.8 57.6 57.1 60.5 45.9 35.7 37.5 36.6\nTable 3: A head-to-head comparison between using the\nprevious sentence and the next sentence for retrieval.\n#Tokens EM F 1 Prec. Rec.\n16 43.2 52.3 51.7 54.5\n32 43.6 52.4 52.0 55.0\n48 40.0 49.3 49.0 52.0\nAll 39.0 48.5 48.2 51.1\nTable 4: Previous-window approaches using different\nnumbers of tokens as queries.\nous window underperforms single-time retrieval\non ASQA, which we hypothesize is because the\nprevious window does not accurately reflect future\nintent. Since we focus on evaluating factuality, met-\nrics with an emphasis on factual content (such as\nEM, Disambig-F1, UniEval) are more reliable than\nmetrics computed over all tokens (ROUGE-L).\n6.2 Ablation Study\nImportance of forward-looking retrieval. We\nfirst validate that forward-looking retrieval is more\neffective than past-context-based retrieval. We run\nablation experiments on 2WikiMultihopQA and\nASQA-hint comparing retrieval using the previ-\nous versus the next sentence. Specifically, both\nmethods retrieve every sentence and directly use\nthe complete previous/next sentence as queries. As\nshown in Table 3, using the next sentence to retrieve\nis clearly better than using the previous sentence,\nconfirming our hypothesis.\nWe also run previous-window approaches using\ndifferent numbers of past tokens as queries. As\nshown in Table 4, using too many tokens (>32) in\n%steps/sentences with retrieval\n0.0\n20.0\n40.0\n60.0\n80.0\n0.0 25.0 50.0 75.0 100.0\n2WikiMultihopQA StrategyQA\nFigure 5: Performance (EM) of FLARE with respect\nto the percentage of steps/sentences with retrieval on\n2WikiMultihopQA and StrategyQA.\nthe past hurts the performance, further confirming\nour hypothesis that previous context might not be\nrelevant to intent of future generations.\nImportance of active retrieval. Next, we inves-\ntigate how active retrieval threshold θaffects per-\nformance. To alter our method from not retrieving\nto retrieving every sentence, we adjust the confi-\ndence threshold θthat determines when to trigger\nretrieval from 0 to 1. We then calculate the pro-\nportion of steps/sentences where retrieval is acti-\nvated, and present the performance based on it. As\nshown in Figure 5, on 2WikiMultihopQA, the per-\nformance plateaus when the retrieval percentage\nexceeds 60%, indicating that retrieval when LMs\nare confident is not necessary. On StrategyQA, the\nperformance drops when the retrieval percentage\nexceeds 50%, indicating that unnecessary retrieval\ncan introduce noise and impede the original gen-\neration process. We found triggering retrieval for\n40%-80% of sentences usually leads to a good per-\nformance across tasks/datasets.\nEffectiveness of different query formulation\nmethods We study implicit query formation by\nmasking and explicit query formulation through\nquestion generation. In Table 5, we compare the\nperformance of FLARE with different masking\n7976\nβ EM F 1 Prec. Rec.\n0.0 0.488 0.576 0.571 0.605\n0.2 0.498 0.588 0.582 0.616\n0.4 0.510 0.597 0.591 0.627\n0.6 0.506 0.593 0.586 0.622\nTable 5: Performance of FLARE with respect to the\nmasking threshold βon 2WikiMultihopQA.\nASQA-hint WikiAsp\nEM D-F 1 R-L DR UniEval E-F 1 R-L\nImplicit 45.7 36.9 37.7 37.3 53.4 18.8 27.7\nExplicit 46.2 36.7 37.7 37.2 53.4 18.9 27.6\nTable 6: A comparison between implicit and explicit\nquery formulation methods in FLARE.\nthresholds β. Retrieving directly with the complete\nsentence (β = 0) is worse than masking tokens\nwith low probabilities, confirming our hypothesis\nthat low-confidence erroneous tokens can distract\nretrievers. We compare implicit and explicit query\nformulation methods in Table 6. Performances of\nboth methods are similar, indicating that both meth-\nods can effectively reflect information needs.\n7 Related Work\nWe refer to subsection 2.2 and section 4 for ex-\ntensively discussion on single-time and multi-time\nretrieval augmented LMs, which is the most rele-\nvant area to this paper.\nIterative and adaptive retrieval Iterative re-\ntrieval and refinement has been studied in both\ntext and code generation tasks (Peng et al., 2023;\nZhang et al., 2023; Zemlyanskiy et al., 2022; Yu\net al., 2023). FLARE differs from these methods in\nthe granularity of generation and retrieval strategies.\nAdaptive retrieval has been studied in single-time\nretrieval scenarios based on either question pop-\nularity or generation probabilities (Mallen et al.,\n2022; Li et al., 2023), while we focus on long-form\ngeneration requiring active information access.\nBrowser-enhanced LMs WebGPT (Nakano\net al., 2021) and WebCPM (Qin et al., 2023) train\nLMs to interact with browser to enhance factuality\nusing reinforcement learning or supervised train-\ning where multiple queries can be triggered before\ngeneration. FLARE is built on text-based retrievers\nbut can be combined with a browser to potentially\nimprove retrieval quality.\n8 Conclusion\nTo aid long-form generation with retrieval aug-\nmentation, we propose an active retrieval aug-\nmented generation framework that decides when\nand what to retrieve during generation. We imple-\nment this framework with forward-looking active\nretrieval that iteratively uses the upcoming sentence\nto retrieve relevant information if it contains low-\nconfidence tokens and regenerates the next sen-\ntence. Experimental results on 4 tasks/datasets\ndemonstrate the effectiveness of our methods. Fu-\nture directions include better strategies for active\nretrieval and developing efficient LM architectures\nfor active information integration.\n9 Limitations\nWe also conduct experiments on Wizard of\nWikipedia (Dinan et al., 2019) and ELI5 (Fan et al.,\n2019), and found that FLARE did not provide sig-\nnificant gains. Wizard of Wikipedia is a knowledge-\nintensive dialogue generation dataset where the out-\nput is relatively short (∼20 tokens on average) so\nretrieving multiple disparate pieces of information\nmight not be necessary. ELI5 (Fan et al., 2019)\nis a long-form QA dataset requiring in-depth an-\nswers to open-ended questions. Due to issues men-\ntioned in Krishna et al. (2021) such as difficulties\nof grounding generation in retrieval and evalua-\ntion, both single-time retrieval and FLARE did not\nprovide significant gains over not using retrieval.\nFrom an engineering perspective, interleaving gen-\neration and retrieval with a naive implementation\nincreases both overheads and the cost of generation.\nLMs need to be activated multiple times (once for\neach retrieval) and a caching-free implementation\nalso requires recomputing the previous activation\neach time after retrieval. This issue can be poten-\ntially alleviated with special architectural designs\nthat encode the retrieved documents Dqt and the\ninput/generation (x/y<t) independently.\nAcknowledgements\nThis work was supported in part by a grant from\nthe Singapore Defence Science and Technology\nAgency and the IBM PhD Fellowship. We thank\nChunting Zhou, Amanda Bertsch, Uri Alon, Hi-\nroaki Hayashi, Harsh Trivedi, Patrick Lewis, Timo\nSchick, Kaixin Ma, Shuyan Zhou, and Songwei Ge\nfor their insightful discussions and help with the\nexperiments.\n7977\nReferences\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\nCassirer, Andy Brock, Michela Paganini, Geoffrey\nIrving, Oriol Vinyals, Simon Osindero, Karen Si-\nmonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.\n2022. Improving language models by retrieving from\ntrillions of tokens. In International Conference on\nMachine Learning, ICML 2022, 17-23 July 2022, Bal-\ntimore, Maryland, USA, volume 162 of Proceedings\nof Machine Learning Research , pages 2206–2240.\nPMLR.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading wikipedia to answer open-\ndomain questions. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL 2017, Vancouver, Canada, July 30 -\nAugust 4, Volume 1: Long Papers, pages 1870–1879.\nAssociation for Computational Linguistics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways. CoRR, abs/2204.02311.\nNachshon Cohen, Oren Kalinsky, Yftah Ziser, and\nAlessandro Moschitti. 2021. Wikisum: Coherent\nsummarization dataset for efficient human-evaluation.\nIn Proceedings of the 59th Annual Meeting of the As-\nsociation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing, ACL/IJCNLP 2021, (Volume 2: Short\nPapers), Virtual Event, August 1-6, 2021, pages 212–\n219. Association for Computational Linguistics.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2019. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. In 7th International Conference on Learning\nRepresentations, ICLR 2019, New Orleans, LA, USA,\nMay 6-9, 2019. OpenReview.net.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\nier, Jason Weston, and Michael Auli. 2019. ELI5:\nlong form question answering. In Proceedings of\nthe 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages\n3558–3567. Association for Computational Linguis-\ntics.\nLuyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan.\n2022. Precise zero-shot dense retrieval without rele-\nvance labels. CoRR, abs/2212.10496.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. Transactions of the\nAssociation for Computational Linguistics , 9:346–\n361.\nJohn M. Giorgi, Luca Soldaini, Bo Wang, Gary D.\nBader, Kyle Lo, Lucy Lu Wang, and Arman Co-\nhan. 2022. Exploring the challenges of open\ndomain multi-document summarization. CoRR,\nabs/2212.10526.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. REALM: retrieval-\naugmented language model pre-training. CoRR,\nabs/2002.08909.\nHiroaki Hayashi, Prashant Budania, Peng Wang, Chris\nAckerson, Raj Neervannan, and Graham Neubig.\n2021. Wikiasp: A dataset for multi-domain aspect-\nbased summarization. Trans. Assoc. Comput. Lin-\nguistics, 9:211–225.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2020. Measuring massive multitask language under-\nstanding. CoRR, abs/2009.03300.\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\nand Akiko Aizawa. 2020. Constructing A multi-hop\nQA dataset for comprehensive evaluation of reason-\ning steps. In Proceedings of the 28th International\nConference on Computational Linguistics, COLING\n2020, Barcelona, Spain (Online), December 8-13,\n7978\n2020, pages 6609–6625. International Committee on\nComputational Linguistics.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\nEACL 2021, Online, April 19 - 23, 2021, pages 874–\n880. Association for Computational Linguistics.\nGautier Izacard, Patrick S. H. Lewis, Maria Lomeli,\nLucas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with\nretrieval augmented language models. CoRR,\nabs/2208.03299.\nZhengbao Jiang, Jun Araki, Haibo Ding, and Graham\nNeubig. 2021. How can we know When language\nmodels know? on the calibration of language mod-\nels for question answering. Trans. Assoc. Comput.\nLinguistics, 9:962–977.\nZhengbao Jiang, Luyu Gao, Jun Araki, Haibo Ding,\nZhiruo Wang, Jamie Callan, and Graham Neubig.\n2022. Retrieval as attention: End-to-end learning\nof retrieval and reading within a single transformer.\nCoRR, abs/2212.02027.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know. Trans. Assoc. Comput. Linguistics ,\n8:423–438.\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics, ACL\n2017, Vancouver, Canada, July 30 - August 4, Volume\n1: Long Papers, pages 1601–1611. Association for\nComputational Linguistics.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli\nTran-Johnson, Scott Johnston, Sheer El Showk, Andy\nJones, Nelson Elhage, Tristan Hume, Anna Chen,\nYuntao Bai, Sam Bowman, Stanislav Fort, Deep\nGanguli, Danny Hernandez, Josh Jacobson, Jack-\nson Kernion, Shauna Kravec, Liane Lovitt, Ka-\nmal Ndousse, Catherine Olsson, Sam Ringer, Dario\nAmodei, Tom Brown, Jack Clark, Nicholas Joseph,\nBen Mann, Sam McCandlish, Chris Olah, and Jared\nKaplan. 2022. Language models (mostly) know what\nthey know. CoRR, abs/2207.05221.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nS. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,\nand Wen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020, pages 6769–6781. Associa-\ntion for Computational Linguistics.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net.\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li,\nDavid Hall, Percy Liang, Christopher Potts, and\nMatei Zaharia. 2022. Demonstrate-search-predict:\nComposing retrieval and language models for\nknowledge-intensive NLP. CoRR, abs/2212.14024.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu,\nKyle Richardson, Peter Clark, and Ashish Sabharwal.\n2022. Decomposed prompting: A modular approach\nfor solving complex tasks. CoRR, abs/2210.02406.\nKalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021.\nHurdles to progress in long-form question answering.\nIn North American Association for Computational\nLinguistics.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur P. Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: a benchmark for question answering\nresearch. Trans. Assoc. Comput. Linguistics, 7:452–\n466.\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech\nStokowiec, and Nikolai Grigorev. 2022. Internet-\naugmented language models through few-shot\nprompting for open-domain question answering.\nCoRR, abs/2203.05115.\nHaejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paran-\njape, Christopher D. Manning, and Kyoung-Gu Woo.\n2021. You only need one model for open-domain\nquestion answering. CoRR, abs/2112.07381.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\nTim Rocktäschel, Sebastian Riedel, and Douwe\nKiela. 2020. Retrieval-augmented generation for\nknowledge-intensive NLP tasks. In Advances in Neu-\nral Information Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\nJunyi Li, Tianyi Tang, Wayne Xin Zhao, Jingyuan Wang,\nJian-Yun Nie, and Ji-Rong Wen. 2023. The web can\nbe your oyster for improving large language models.\nCoRR, abs/2305.10998.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\n7979\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Comput. Surv., 55(9):195:1–195:35.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\nHannaneh Hajishirzi, and Daniel Khashabi. 2022.\nWhen not to trust language models: Investigating\neffectiveness and limitations of parametric and non-\nparametric memories. CoRR, abs/2212.10511.\nYuning Mao, Pengcheng He, Xiaodong Liu, Yelong\nShen, Jianfeng Gao, Jiawei Han, and Weizhu Chen.\n2021. Generation-augmented retrieval for open-\ndomain question answering. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing, ACL/I-\nJCNLP 2021, (Volume 1: Long Papers), Virtual Event,\nAugust 1-6, 2021, pages 4089–4100. Association for\nComputational Linguistics.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factu-\nality in abstractive summarization. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 1906–1919, On-\nline. Association for Computational Linguistics.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\nXu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\nKrueger, Kevin Button, Matthew Knight, Benjamin\nChess, and John Schulman. 2021. Webgpt: Browser-\nassisted question-answering with human feedback.\nCoRR, abs/2112.09332.\nOpenAI. 2023. GPT-4 technical report. CoRR,\nabs/2303.08774.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F. Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. CoRR, abs/2203.02155.\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng,\nYujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou\nYu, Weizhu Chen, and Jianfeng Gao. 2023. Check\nyour facts and try again: Improving large language\nmodels with external knowledge and automated feed-\nback. CoRR, abs/2302.12813.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,\nand Alexander H. Miller. 2019. Language mod-\nels as knowledge bases? In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019 , pages 2463–2473. Association for\nComputational Linguistics.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels. arXiv preprint arXiv:2210.03350.\nHongjing Qian, Yutao Zhu, Zhicheng Dou, Haoqi Gu,\nXinyu Zhang, Zheng Liu, Ruofei Lai, Zhao Cao,\nJian-Yun Nie, and Ji-Rong Wen. 2023. Webbrain:\nLearning to generate factually correct articles for\nqueries by grounding on large web corpus. CoRR,\nabs/2304.04358.\nYujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao\nLiang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding,\nHuadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan\nLiu, Maosong Sun, and Jie Zhou. 2023. Webcpm: In-\nteractive web search for chinese long-form question\nanswering. CoRR, abs/2305.06849.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8).\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented lan-\nguage models. arXiv preprint arXiv:2302.00083.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2020, Online, Novem-\nber 16-20, 2020, pages 5418–5426. Association for\nComputational Linguistics.\nStephen E. Robertson and Hugo Zaragoza. 2009. The\nprobabilistic relevance framework: BM25 and be-\nyond. Found. Trends Inf. Retr., 3(4):333–389.\nDevendra Singh Sachan, Siva Reddy, William L. Hamil-\nton, Chris Dyer, and Dani Yogatama. 2021. End-to-\nend training of multi-document reader and retriever\nfor open-domain question answering. In Advances\nin Neural Information Processing Systems 34: An-\nnual Conference on Neural Information Processing\nSystems 2021, NeurIPS 2021, December 6-14, 2021,\nvirtual, pages 25968–25981.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon\nSeo, Rich James, Mike Lewis, Luke Zettlemoyer, and\nWen-tau Yih. 2023. REPLUG: retrieval-augmented\nblack-box language models. CoRR, abs/2301.12652.\n7980\nIvan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-\nWei Chang. 2022. ASQA: factoid questions meet\nlong-form answers. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2022, Abu Dhabi, United Arab\nEmirates, December 7-11, 2022, pages 8273–8288.\nAssociation for Computational Linguistics.\nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and\nDenny Zhou. 2022. Recitation-augmented language\nmodels. CoRR, abs/2210.01296.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurélien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. CoRR,\nabs/2302.13971.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar\nKhot, and Ashish Sabharwal. 2022. Interleav-\ning retrieval with chain-of-thought reasoning for\nknowledge-intensive multi-step questions. CoRR,\nabs/2212.10509.\nNeeraj Varshney, Man Luo, and Chitta Baral. 2022. Can\nopen-domain QA reader utilize external knowledge\nefficiently like humans? CoRR, abs/2211.12707.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V .\nLe, Ed H. Chi, and Denny Zhou. 2022. Self-\nconsistency improves chain of thought reasoning in\nlanguage models. CoRR, abs/2203.11171.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. CoRR, abs/2201.11903.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. CoRR, abs/2210.03629.\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. 2022. Generate\nrather than retrieve: Large language models are\nstrong context generators. CoRR, abs/2209.10063.\nWenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng\nJiang, and Ashish Sabharwal. 2023. Improving lan-\nguage models via plug-and-play retrieval feedback.\nCoRR, abs/2305.14002.\nYury Zemlyanskiy, Michiel de Jong, Joshua Ainslie,\nPanupong Pasupat, Peter Shaw, Linlu Qiu, Sumit\nSanghai, and Fei Sha. 2022. Generate-and-retrieve:\nUse your predictions to improve retrieval for seman-\ntic parsing. In Proceedings of the 29th International\nConference on Computational Linguistics, COLING\n2022, Gyeongju, Republic of Korea, October 12-17,\n2022, pages 4946–4951. International Committee on\nComputational Linguistics.\nFengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang\nZan, Yi Mao, Jian-Guang Lou, and Weizhu Chen.\n2023. Repocoder: Repository-level code completion\nthrough iterative retrieval and generation. CoRR,\nabs/2303.12570.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open\npre-trained transformer language models. ArXiv,\nabs/2205.01068.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Be-\nichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,\nChen Yang, Yushuo Chen, Zhipeng Chen, Jinhao\nJiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang\nLiu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.\n2023. A survey of large language models. CoRR,\nabs/2303.18223.\nMing Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu\nJiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and\nJiawei Han. 2022. Towards a unified multi-\ndimensional evaluator for text generation. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, EMNLP 2022,\nAbu Dhabi, United Arab Emirates, December 7-11,\n2022, pages 2023–2038. Association for Computa-\ntional Linguistics.\nChunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab,\nFrancisco Guzmán, Luke Zettlemoyer, and Marjan\nGhazvininejad. 2021. Detecting hallucinated content\nin conditional neural sequence generation. In Find-\nings of the Association for Computational Linguis-\ntics: ACL-IJCNLP 2021, pages 1393–1404, Online.\nAssociation for Computational Linguistics.\n7981\nA FLARE Implementation Details\nFLAREinstruct implementation details We\nfound that LMs can effectively combine retrieval\nand downstream task-related skills and generate\nmeaningful search queries while performing the\ntask. However, there are two issues: (1) LMs tend\nto generate fewer search queries than necessary.\n(2) Generating excessive search queries can\ndisrupt answer generation and adversely affect\nperformance. We address these issues using two\nmethods respectively. First, we increase the logit\nof the token “[” by 2.0 to improve the chances\nof LMs generating “[Search(query)]”. Second,\nwhenever LMs generate a search query, we use it\nto retrieve relevant information, promptly remove\nit from the generation, and generate the next few\ntokens while forbidding “[” by adding a large\nnegative value to the logit of “[”.\nThe initial query of FLARE. FLARE starts\nwith the user input x as the initial query to re-\ntrieve documents to generate the first sentence\nˆs1 = LM([Dx,x]) to bootstrap the iterative gener-\nation process. For the following steps, the tempo-\nrary forward-looking sentence is generated without\nretrieved documents.\nSentence tokenization. For each step t, we gen-\nerate 64 tokens which are longer than most sen-\ntences, and use NLTK sentence tokenizer 5 to ex-\ntract the first sentence and discard the rest.\nEfficiency As shown in subsection 6.2, on aver-\nage retrieval is triggered for 30% ∼60% of sen-\ntences depending on downstream tasks. In compar-\nision, KNN-LM (Khandelwal et al., 2020) retrieves\nevery token, RETRO or IC-RALM (Borgeaud et al.,\n2022; Ram et al., 2023) retrievers every 4∼32 to-\nkens, and IRCoT (Trivedi et al., 2022) retrieves\nevery sentence. Compared to single-time retrieval,\nhowever, interleaving retrieval and generation with\na naive implementation indeed increases overheads,\nwhich we discuss in the limitation section (sec-\ntion 9).\nB Datasets and Settings\nDatasets, metrics, and experimental settings are\nsummarized in Table 7.\n5https://www.nltk.org/api/nltk.tokenize.\nPunktSentenceTokenizer.html\nMultihop QA For “Why did the founder of Ver-\nsus die?”, the output we aim to generate is “The\nfounder of Versus was Gianni Versace. Gianni Ver-\nsace was shot and killed on the steps of his Miami\nBeach mansion on July 15, 1997. So the answer\nis shot.” We use 8 exemplars from Trivedi et al.\n(2022) listed in Prompt D.4 for in-context learn-\ning, BM25 as the retriever, and Wikipedia articles\nas the retrieval corpus. Similar to the observation\nin Trivedi et al. (2022), we found incorporating\nretrieval results for exemplars improves the per-\nformance, we use the input x of each exemplar to\nretrieve several documents and then add them using\nthe format in Prompt D.1. We found increasing the\nnumber of retrieval documents often increases per-\nformance. Therefore, we use the maximum number\nof documents that can fit within the input length\nlimit of text-davinci-003, which is 2 for 2Wiki-\nMultihopQA.\nCommonsense Reasoning For “Would a pear\nsink in water?”, the output we aim to generate is\n“The density of a pear is about 0.6g/cm3, which is\nless than water. Objects less dense than water float.\nThus, a pear would float. So the final answer is no.”\nWe use 6 exemplars from Wei et al. (2022) listed in\nPrompt D.5, BM25 on the Wikipedia corpus, and 3\nretrieved documents to run experiments.\nLong-form QA For “Where do the Philadelphia\nEagles play their home games?”, the output we\naim to generate is “We need to consider the dif-\nferent possible locations or venues that could be\nconsidered the home field of the Philadelphia Ea-\ngles. These include the city, the sports complex,\nor the stadium. Therefore, this question has 3 in-\nterpretations and the answers are: (1) The city is\nPhiladelphia. (2) The sports complex is the South\nPhiladelphia Sports Complex. (3) The stadium is\nthe Lincoln Financial Field stadium.” For both the\noriginal setting (ASQA) and the setting with hints\n(ASQA-hint), we manually annotate 8 exemplars\n(Prompt D.6 and D.8), use BM25 on the Wikipedia\ncorpus, and 3 retrieved documents to run experi-\nments.\nOpen-domain Summarization The original\nWikiAsp dataset is designed for multi-document\nsummarization and provides a list of references to\nsystems. We converted it into the open-domain\nsetting by removing the associated references and\ninstead gathering information from the open web.\nFor “Generate a summary about Echo School (Ore-\n7982\ngon) including the following aspects: academics,\nhistory.”, the output we aim to generate is “# Aca-\ndemics. In 2008, 91% of the school’s seniors re-\nceived their high school diploma... # History. The\nclass of 2008 was the 100th class in the school’s\nhistory.” where # is used to indicate aspects. We\nmanually annotate 4 exemplars (Prompt D.10), and\nuse the Bing search engine to retrieve 5 documents\nfrom the open web. To avoid leaking, we exclude\nseveral Wikipedia-related domains listed in Table 8\nfrom Bing’s search results.\nC Hyperparameters\nHyperparameters of FLARE on different datasets\nare listed in Table 9.\nD Prompts and Few-shot exemplars\nThe prompt used to linearize multiple documents\nis shown in Prompt D.1. The prompt used in self-\nask (Press et al., 2022) is shown in Prompt D.2.\nPrompts and exemplars of different tasks/datasets\nare shown in Prompt D.3, D.4, D.5, D.6, D.8, and\nD.10, respectively.\nPrompt D.1: document formatting\nSearch results:\n[1] Document 1\n[2] Document 2\n...\nThe user input x\nPrompt D.2: multihop QA with self-ask\nQuestion: Who lived longer, Theodor Haecker or Harry\nVaughan Watkins?\nAre follow up questions needed here: Yes.\nFollow up: How old was Theodor Haecker when he died?\nIntermediate answer: Theodor Haecker was 65 years old\nwhen he died.\nFollow up: How old was Harry Vaughan Watkins when he\ndied?\nIntermediate answer: Harry Vaughan Watkins was 69 years\nold when he died.\nSo the final answer is: Harry Vaughan Watkins.\n7983\nSettings 2WikiMultihopQA StrategyQA ASQA WikiAsp\n(Ho et al., 2020) (Geva et al., 2021) (Stelmakh et al., 2022) (Hayashi et al., 2021)\nDataset statistics\nTask multihop QA commonsense QA long-form QA open-domain summarization\n#Examples 500 229 500 500\nEvaluation settings\nMetrics EM, F 1, Prec., Rec. EM EM, Disambig-F 1, ROUGE, DR UniEval, entity-F1, ROUGE\nRetrieval settings\nCorpus Wikipedia Wikipedia Wikipedia open web\nRetriever BM25 BM25 BM25 Bing\nTop-k 2 3 3 5\nPrompt format\n#Exemplars 8 6 8 4\nRet. for exemplars ✓ ✗ ✗ ✗\nTable 7: Dataset statistics and experimental settings of different tasks.\nwikipedia.org, wikiwand.com, wiki2.org, wikimedia.org\nTable 8: Wikipedia-related domains excluded from Bing’s search results.\nDataset θ β Query formulation Combine single- & multi-time retrieval\n2WikiMultihopQA 0.8 0.4 implicit ✗\nStrategyQA 0.4 0.4 implicit ✗\nASQA & ASQA-hint 0.8 0.4 explicit ✓\nWikiAsp 0.8 0.4 explicit ✓\nTable 9: Hyperparameters of FLARE on different datasets.\n7984\nPrompt D.3: retrieval instructions for 2WikiMultihopQA\nSkill 1. Use the Search API to look up relevant information by writing “[Search(term)]” where “term” is the search term you\nwant to look up. For example:\nQuestion: But what are the risks during production of nanomaterials?\nAnswer (with Search): [Search(nanomaterial production risks)] Some nanomaterials may give rise to various kinds of lung\ndamage.\nQuestion: The colors on the flag of Ghana have the following meanings.\nAnswer (with Search): Red is for [Search(Ghana flag red meaning)] the blood of martyrs, green for forests, and gold for\nmineral wealth.\nQuestion: Metformin is the first-line drug for what?\nAnswer (with Search): [Search(Metformin first-line drug)] patients with type 2 diabetes and obesity.\nSkill 2. Answer questions by thinking step-by-step. First, write out the reasoning steps, then draw the conclu-\nsion. For example:\nQuestion: When did the director of film Hypocrite (Film) die?\nAnswer (with step-by-step): The film Hypocrite was directed by Miguel Morayta. Miguel Morayta died on 19 June 2013. So\nthe answer is 19 June 2013.\nQuestion: Are both Kurram Garhi and Trojkrsti located in the same country?\nAnswer (with step-by-step): Kurram Garhi is located in the country of Pakistan. Trojkrsti is located in the country of\nRepublic of Macedonia. Thus, they are not in the same country. So the answer is no.\nQuestion: Do director of film Coolie No. 1 (1995 Film) and director of film The Sensational Trial have the same\nnationality?\nAnswer (with step-by-step): Coolie No. 1 (1995 film) was directed by David Dhawan. The Sensational Trial was directed by\nKarl Freund. David Dhawan’s nationality is India. Karl Freund’s nationality is Germany. Thus, they do not have the same\nnationality. So the answer is no.\nQuestion: Who is Boraqchin (Wife Of Ögedei)’s father-in-law?\nAnswer (with step-by-step): Boraqchin is married to Ögedei Khan. Ögedei Khan’s father is Genghis Khan. Thus, Boraqchin’s\nfather-in-law is Genghis Khan. So the answer is Genghis Khan.\nQuestion: Who was born first out of Martin Hodge and Ivania Martinich?\nAnswer (with step-by-step): Martin Hodge was born on 4 February 1959. Ivania Martinich was born on 25 July 1995. Thus,\nMartin Hodge was born first. So the answer is Martin Hodge.\nQuestion: When did the director of film Laughter In Hell die?\nAnswer (with step-by-step): The film Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25,\n1963. So the answer is August 25, 1963.\nQuestion: Which film has the director died later, The Gal Who Took the West or Twenty Plus Two?\nAnswer (with step-by-step): The film Twenty Plus Two was directed by Joseph M. Newman. The Gal Who Took\nthe West was directed by Frederick de Cordova. Joseph M. Newman died on January 23, 2006. Fred de Cordova\ndied on September 15, 2001. Thus, the person to die later from the two is Twenty Plus Two. So the answer is Twenty Plus Two.\nQuestion: Who is the grandchild of Krishna Shah (Nepalese Royal)?\nAnswer (with step-by-step): Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah.\nThus, Krishna Shah has a grandchild named Prithvipati Shah. So the answer is Prithvipati Shah.\nNow, combine the aforementioned two skills. First, write out the reasoning steps, then draw the conclusion,\nwhere the reasoning steps should also utilize the Search API “[Search(term)]” whenever possible.\nQuestion: Where did Minbyauk Thihapate’s wife die?\nAnswer (with step-by-step & Search):\n7985\nPrompt D.4: exemplars of 2WikiMultihopQA\nQuestion: When did the director of film Hypocrite (Film) die?\nAnswer: The film Hypocrite was directed by Miguel Morayta. Miguel Morayta died on 19 June 2013. So the answer is 19\nJune 2013.\nQuestion: Are both Kurram Garhi and Trojkrsti located in the same country?\nAnswer: Kurram Garhi is located in the country of Pakistan. Trojkrsti is located in the country of Republic of Macedonia.\nThus, they are not in the same country. So the answer is no.\nQuestion: Do director of film Coolie No. 1 (1995 Film) and director of film The Sensational Trial have the same\nnationality?\nAnswer: Coolie No. 1 (1995 film) was directed by David Dhawan. The Sensational Trial was directed by Karl Freund. David\nDhawan’s nationality is India. Karl Freund’s nationality is Germany. Thus, they do not have the same nationality. So the\nanswer is no.\nQuestion: Who is Boraqchin (Wife Of Ögedei)’s father-in-law?\nAnswer: Boraqchin is married to Ögedei Khan. Ögedei Khan’s father is Genghis Khan. Thus, Boraqchin’s father-in-law is\nGenghis Khan. So the answer is Genghis Khan.\nQuestion: Who was born first out of Martin Hodge and Ivania Martinich?\nAnswer: Martin Hodge was born on 4 February 1959. Ivania Martinich was born on 25 July 1995. Thus, Martin Hodge was\nborn first. So the answer is Martin Hodge.\nQuestion: When did the director of film Laughter In Hell die?\nAnswer: The film Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25, 1963. So the\nanswer is August 25, 1963.\nQuestion: Which film has the director died later, The Gal Who Took the West or Twenty Plus Two?\nAnswer: The film Twenty Plus Two was directed by Joseph M. Newman. The Gal Who Took the West was directed by\nFrederick de Cordova. Joseph M. Newman died on January 23, 2006. Fred de Cordova died on September 15, 2001. Thus,\nthe person to die later from the two is Twenty Plus Two. So the answer is Twenty Plus Two.\nQuestion: Who is the grandchild of Krishna Shah (Nepalese Royal)?\nAnswer: Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah. Thus, Krishna Shah\nhas a grandchild named Prithvipati Shah. So the answer is Prithvipati Shah.\nQuestion: Which country the director of film Citizen Mavzik is from?\nAnswer:\n7986\nPrompt D.5: exemplars of StrategyQA\nGenerate a yes or no answer to the following question.\nQuestion: Do hamsters provide food for any animals?\nAnswer: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the final\nanswer is yes.\nGenerate a yes or no answer to the following question.\nQuestion: Could Brooke Shields succeed at University of Pennsylvania?\nAnswer: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the Uni-\nversity of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the final answer is yes.\nGenerate a yes or no answer to the following question.\nQuestion: Hydrogen’s atomic number squared exceeds number of Spice Girls?\nAnswer: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic number\nsquared is less than 5. So the final answer is no.\nGenerate a yes or no answer to the following question.\nQuestion: Is it common to see frost during some college commencements?\nAnswer: College commencement ceremonies can happen in December, May, and June. December is in the winter, so there\ncan be frost. Thus, there could be frost at some commencements. So the final answer is yes.\nGenerate a yes or no answer to the following question.\nQuestion: Could a llama birth twice during War in Vietnam (1945-46)?\nAnswer: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months.\nThus, a llama could not give birth twice during the War in Vietnam. So the final answer is no.\nGenerate a yes or no answer to the following question.\nQuestion: Would a pear sink in water?\nAnswer: The density of a pear is about 0.6g/cm3, which is less than water. Objects less dense than water float. Thus, a pear\nwould float. So the final answer is no.\nGenerate a yes or no answer to the following question.\nQuestion: Did Spiderman fight against Falcon in the MCU?\nAnswer:\n7987\nPrompt D.6: exemplars of ASQA\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\nQuestion: Who played bonnie in gone with the wind?\nAnswer: This question is ambiguous in terms of which version or adaptation of Gone with the Wind is being referred to. In\norder to figure out its interpretations, we need to consider different versions or adaptations of Gone with the Wind. Gone with\nthe Wind has two versions or adaptations: the 1939 film Gone with the Wind or the 2008 musical Gone with the Wind.\nTherefore, this question has 2 interpretations: (1) Who played Bonnie in the 1939 film Gone with the Wind? (2) Who played\nBonnie in the 2008 musical Gone with the Wind? The answers to all interpretations are: (1) The 1939 film Gone with the\nWind’s character Bonnie was played by Eleanore Cammack \"Cammie\" King. (2) The 2008 musical Gone with the Wind’s\ncharacter Bonnie was played by Leilah de Meza.\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\nQuestion: What is the second largest city in the usa?\nAnswer: This question is ambiguous in terms of the criteria being used to determine the second largest city in the USA.\nIn order to figure out its interpretations, we need to consider different criteria to determine a city’s size. City size can be\nmeasured by two criteria: population or area. Therefore, this question has 2 interpretations: (1) What is the second largest\ncity in the USA by population? (2) What is the second largest city in the USA by area? The answers to all interpretations are:\n(1) The second largest city in the USA by population is Los Angeles, California. (2) The second largest city in the USA by\narea is Juneau, Alaska.\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\nQuestion: When was bohemian rhapsody released as a single?\nAnswer: This question is ambiguous in terms of which country’s release of the single is being referred to. In order to figure\nout its interpretations, we need to consider different countries where Bohemian Rhapsody is released. Bohemian Rhapsody\nwas released in the United Kingdom and in the United States on different dates. Therefore, this question has 2 interpretations:\n(1) When was Bohemian Rhapsody released as a single in the United Kingdom? (2) When was Bohemian Rhapsody released\nas a single in the United States? The answers to all interpretations are: (1) Bohemian Rhapsody was released as a single in the\nUnited Kingdom on 31 October 1975. (2) Bohemian Rhapsody was released as a single in the United States on December 1975.\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\nQuestion: Where do the philadelphia eagles play their home games?\nAnswer: This question is ambiguous in terms of which specific location or venue is being referred to. In order to figure out its\ninterpretations, we need to consider the different possible locations or venues that could be considered the home field of the\nPhiladelphia Eagles. These include the city, the sports complex, or the stadium. Therefore, this question has 3 interpretations:\n(1) What city do the Philadelphia Eagles play their home games? (2) In what sports complex do the Philadelphia Eagles play\ntheir home games? (3) What stadium do the Philadelphia Eagles play their home games? The answers to all interpretations\nare: (1) Philadelphia Eagles play their home games in the city Philadelphia. (2) Philadelphia Eagles play their home games in\nthe South Philadelphia Sports Complex. (3) Philadelphia Eagles play their home games in the Lincoln Financial Field stadium.\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\nQuestion: When did xbox one come out in australia?\nAnswer: This question is ambiguous in terms of which specific version of the Xbox One is being referred to. In order to\nfigure out its interpretations, we need to consider the different versions of the Xbox One that have been released. Xbox One\nhas two versions: the Xbox One video game console or the Xbox One X high-end model. Therefore, this question has 2\ninterpretations: (1) When did the Xbox One release in Australia? (2) When did the Xbox One X release in Australia? The\nanswers to all interpretations are: (1) The Xbox One video game console was released in Australia on November 22, 2013.\n(2) The Xbox One X video game console was released in Australia on November 7, 2017.\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\nQuestion: When does the movie summer of 84 come out?\nAnswer: This question is ambiguous in terms of which release of the movie is being referred to. In order to figure out its\ninterpretations, we need to consider different releases of the movie Summer of ’84. The movie Summer of ’84 is first released\nat the Sundance Festival before it’s released throughout the US. Therefore, this question has 2 interpretations: (1) When\ndid the movie Summer of ’84 first release at the Sundance Festival? (2) When did the movie Summer of ’84 first release\nthroughout the US? The answers to all interpretations are: (1) Summer of ’84 was released at the Sundance Festival on\nJanuary 22, 2018. (2) Summer of ’84 was released throughout the US on August 10, 2018.\n7988\nPrompt D.7: exemplars of ASQA (cont.)\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\nQuestion: What was roy orbison’s first number one hit?\nAnswer: This question is ambiguous in terms of which specific chart or region is being referred to. In order to figure out its\ninterpretations, we need to consider the different charts and regions where Roy Orbison’s music was popular. Roy Orbison is\npopular in both the US Hot 100 and Canada, and the UK and Ireland. Therefore, this question has 2 interpretations: (1) What\nwas Roy Orbison’s first number one hit in the US Hot 100 and Canada? (2) What was Roy Orbison’s first number one hit in\nthe UK and Ireland? The answers to all interpretations are: (1) Running Scared was the first number one hit for Roy Orbison\nin the US Hot 100 and Canada. (2) Only the Lonely (Know the Way I Feel) was the first number one hit for Roy Orbison in\nthe UK and Ireland.\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\nQuestion: What is the criminal’s name in the breakfast club?\nAnswer: This question is ambiguous in terms of which specific name is being referred to - the character’s name or the actor’s\nname. In order to figure out its interpretations, we need to consider both possibilities: the character’s name or the actor’s\nname. Therefore, this question has 2 interpretations: (1) What is the criminal’s character name in The Breakfast Club? (2)\nWhat is the the name of the actor who played the criminal in The Breakfast Club? The answers to all interpretations are: (1)\nJohn Bender was the name of the criminal’s character in The Breakfast Club. (2) Judd Nelson was the actor of the criminal in\nThe Breakfast Club.\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\nQuestion: How many state parks are there in virginia?\nAnswer:\n7989\nPrompt D.8: exemplars of ASQA-hint\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpretations and\nanswer them one by one.\nQuestion: Who played bonnie in gone with the wind?\nHint: This question is ambiguous in terms of which version or adaptation of Gone with the Wind is being referred to.\nAnswer: In order to figure out its interpretations, we need to consider different versions or adaptations of Gone with the Wind.\nGone with the Wind has two versions or adaptations: the 1939 film Gone with the Wind or the 2008 musical Gone with the\nWind. Therefore, this question has 2 interpretations: (1) Who played Bonnie in the 1939 film Gone with the Wind? (2) Who\nplayed Bonnie in the 2008 musical Gone with the Wind? The answers to all interpretations are: (1) The 1939 film Gone with\nthe Wind’s character Bonnie was played by Eleanore Cammack \"Cammie\" King. (2) The 2008 musical Gone with the Wind’s\ncharacter Bonnie was played by Leilah de Meza.\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\ntions and answer them one by one.\nQuestion: What is the second largest city in the usa?\nHint: This question is ambiguous in terms of the criteria being used to determine the second largest city in the USA.\nAnswer: In order to figure out its interpretations, we need to consider different criteria to determine a city’s size. City size can\nbe measured by two criteria: population or area. Therefore, this question has 2 interpretations: (1) What is the second largest\ncity in the USA by population? (2) What is the second largest city in the USA by area? The answers to all interpretations are:\n(1) The second largest city in the USA by population is Los Angeles, California. (2) The second largest city in the USA by\narea is Juneau, Alaska.\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\ntions and answer them one by one.\nQuestion: When was bohemian rhapsody released as a single?\nHint: This question is ambiguous in terms of which country’s release of the single is being referred to.\nAnswer: In order to figure out its interpretations, we need to consider different countries where Bohemian Rhapsody is\nreleased. Bohemian Rhapsody was released in the United Kingdom and in the United States on different dates. Therefore,\nthis question has 2 interpretations: (1) When was Bohemian Rhapsody released as a single in the United Kingdom? (2) When\nwas Bohemian Rhapsody released as a single in the United States? The answers to all interpretations are: (1) Bohemian\nRhapsody was released as a single in the United Kingdom on 31 October 1975. (2) Bohemian Rhapsody was released as a\nsingle in the United States on December 1975.\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\ntions and answer them one by one.\nQuestion: Where do the philadelphia eagles play their home games?\nHint: This question is ambiguous in terms of which specific location or venue is being referred to.\nAnswer: In order to figure out its interpretations, we need to consider the different possible locations or venues that could be\nconsidered the home field of the Philadelphia Eagles. These include the city, the sports complex, or the stadium. Therefore,\nthis question has 3 interpretations: (1) What city do the Philadelphia Eagles play their home games? (2) In what sports\ncomplex do the Philadelphia Eagles play their home games? (3) What stadium do the Philadelphia Eagles play their home\ngames? The answers to all interpretations are: (1) Philadelphia Eagles play their home games in the city Philadelphia. (2)\nPhiladelphia Eagles play their home games in the South Philadelphia Sports Complex. (3) Philadelphia Eagles play their\nhome games in the Lincoln Financial Field stadium.\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\ntions and answer them one by one.\nQuestion: When did xbox one come out in australia?\nHint: This question is ambiguous in terms of which specific version of the Xbox One is being referred to.\nAnswer: In order to figure out its interpretations, we need to consider the different versions of the Xbox One that have been\nreleased. Xbox One has two versions: the Xbox One video game console or the Xbox One X high-end model. Therefore, this\nquestion has 2 interpretations: (1) When did the Xbox One release in Australia? (2) When did the Xbox One X release\nin Australia? The answers to all interpretations are: (1) The Xbox One video game console was released in Australia on\nNovember 22, 2013. (2) The Xbox One X video game console was released in Australia on November 7, 2017.\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\ntions and answer them one by one.\nQuestion: When does the movie summer of 84 come out?\nHint: This question is ambiguous in terms of which release of the movie is being referred to.\nAnswer: In order to figure out its interpretations, we need to consider different releases of the movie Summer of ’84. The\nmovie Summer of ’84 is first released at the Sundance Festival before it’s released throughout the US. Therefore, this question\nhas 2 interpretations: (1) When did the movie Summer of ’84 first release at the Sundance Festival? (2) When did the movie\nSummer of ’84 first release throughout the US? The answers to all interpretations are: (1) Summer of ’84 was released at the\nSundance Festival on January 22, 2018. (2) Summer of ’84 was released throughout the US on August 10, 2018.\n7990\nPrompt D.9: exemplars of ASQA-hint (cont.)\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpretations and\nanswer them one by one.\nQuestion: What was roy orbison’s first number one hit?\nHint: This question is ambiguous in terms of which specific chart or region is being referred to.\nAnswer: In order to figure out its interpretations, we need to consider the different charts and regions where Roy Orbison’s\nmusic was popular. Roy Orbison is popular in both the US Hot 100 and Canada, and the UK and Ireland. Therefore, this\nquestion has 2 interpretations: (1) What was Roy Orbison’s first number one hit in the US Hot 100 and Canada? (2) What\nwas Roy Orbison’s first number one hit in the UK and Ireland? The answers to all interpretations are: (1) Running Scared\nwas the first number one hit for Roy Orbison in the US Hot 100 and Canada. (2) Only the Lonely (Know the Way I Feel) was\nthe first number one hit for Roy Orbison in the UK and Ireland.\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\ntions and answer them one by one.\nQuestion: What is the criminal’s name in the breakfast club?\nHint: This question is ambiguous in terms of which specific name is being referred to - the character’s name or the actor’s\nname.\nAnswer: In order to figure out its interpretations, we need to consider both possibilities: the character’s name or the actor’s\nname. Therefore, this question has 2 interpretations: (1) What is the criminal’s character name in The Breakfast Club? (2)\nWhat is the the name of the actor who played the criminal in The Breakfast Club? The answers to all interpretations are: (1)\nJohn Bender was the name of the criminal’s character in The Breakfast Club. (2) Judd Nelson was the actor of the criminal in\nThe Breakfast Club.\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\ntions and answer them one by one.\nQuestion: How many state parks are there in virginia?\nHint: This question is ambiguous in terms of the time frame or period being referred to.\nAnswer:\n7991\nPrompt D.10: exemplars of WikiAsp\nGenerate a summary about Aslanhane Mosque including the following aspects: location, history with one aspect per line.\n# Location\nThe mosque is in the old quarter of ankara next to ankara castle. With an altitude of 947 metres (3,107 ft) it overlooks ankara\nat 39°56’12\"N 32°51’55\"E.\n# History\nThe mosque is one of the oldest mosques in Turkey still standing. It was built during the reign of Mesud II of the Anatolian\nSeljuks in 1290. Its architect was Ebubekir Mehmet. It was commissioned by two Ahi leaders named Hüsamettin and\nHasaneddin. However, in 1330, it was repaired by another Ahi leader named ¸ Serafettin after whom the mosque was named.\nAfter several minor repairs the mosque was restored by the directorate general of foundations in 2010-2013 term.\nGenerate a summary about Untold Legends: The Warrior’s Code including the following aspects: reception,\ngameplay, development with one aspect per line.\n# Reception\nThe game received \"mixed or average reviews\" according to video game review aggregator Metacritic.\n# Gameplay\nThe warrior’s code is a hack n’ slash action role-playing game, which concentrates on action-oriented combat.\n# Development\nAs a pre-order bonus, the game was shipped with a small action figure of the Guardian class.\nGenerate a summary about Raid on St. Augustine including the following aspects: aftermath, background with\none aspect per line.\n# Aftermath\nOnce the English had gone Menéndez and the rest of the Spanish settlers returned to find a smoldering ruins and very little\nleft. He soon and begged for help from the viceroy of Cuba and the settlement took a while to build itself back up. The\ndestroyed fort was replaced with the present day Castillo de San Marcos.\n# Background\nWar had already been unofficially declared by Philip II of Spain after the Treaty of Nonsuch in which Elizabeth I had\noffered her support to the rebellious Protestant Dutch rebels. The Queen through Francis Walsingham ordered Sir Francis\nDrake to lead an expedition to attack the Spanish New World in a kind of preemptive strike. Sailing from Plymouth,\nEngland, he struck first at Santiago in November 1585 then across the Atlantic at the Spanish new world city of Santo\nDomingo of which was captured and ransomed on 1 January 1586 and following that successfully attacked the important\ncity of Cartagena on 19 February. Drake wanted to strike at another Spanish city on the Main before finally visiting and\nreplenishing Sir Walter Raleigh’s new colony of Roanoke Colony on the American East Coast. Then after this he hoped\nto make the Transatlantic crossing back to England. The fleet headed north, and in late April Drake put into the Spanish\nCuban mainland and his men dug wells in search of fresh water and gathered supplies to help counter an outbreak of\ndysentery after which he moved on. The fleet traveled north within sight of land on the Florida peninsula sailing past\nthe West coast. On 27 May 1586 as they approached further north a small fort was spotted on the shore, with a small\ninlet close by. This was the location of St Augustine, the most northerly town in Spain’s New World Empire, and the\noldest permanent colonial settlement in North America. Drake knew of the place and was also aware of the fact that\nthe spanish under Pedro Menéndez de Avilés had ordered all of the French Huguenot colonists that had tried to settle\nin the area executed. Drake decided on one final opportunity to raid and plunder, and a chance to avenge his fellow Protestants.\nGenerate a summary about Lakewood (Livingston, Alabama) including the following aspects: architecture, his-\ntory with one aspect per line.\n# Architecture\nThe house has a plan that is relatively rare in early Alabama architecture. The plan features a brick ground floor that is topped\nby one-and-a-half-stories of wood-frame construction. The ground floor originally contained domestic spaces, with the\nformal rooms on the principle floor and bedrooms on the upper floor. A central hallway is present on all levels. The facade is\nfive bays wide, with central entrance doors on the ground and principle floors. The bays are divided by two-story Doric\npilasters, with the middle third of the facade occupied by a two-tiered tetrastyle Doric portico. Two curved wrought iron\nstaircases ascend from ground level to the front center of the upper portico, leading to the formal entrance.\n# History\nLakewood was built for Joseph lake, a native of North Carolina, by Hiram W. Bardwell, a master builder. Construction\nwas completed in 1840. Located adjacent to the University of West Alabama, Julia Strudwick Tutwiler, a Lake relative,\nperiodically resided in the house from 1881 to 1910 while she served as president of the university. It was then known as\nLivingston Normal College. The house was extensively photographed by Alex Bush for the Historic American Buildings\nSurvey in November and December 1936. Lakewood has continued to be owned by descendants of the Lake family to the\ncurrent day. The house and its surviving 10 acres (4.0 ha) of grounds were listed on the Places in Peril in 2012 due to the\nimmediate threat of its acquisition by developers.\nGenerate a summary about Carlos Moedas including the following aspects: biography, early life, political career\nwith one aspect per line.\n7992",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6052905917167664
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4212720990180969
    },
    {
      "name": "Natural language processing",
      "score": 0.40036648511886597
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    }
  ]
}