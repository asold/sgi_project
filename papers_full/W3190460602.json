{
  "title": "LETS: A Label-Efficient Training Scheme for Aspect-Based Sentiment Analysis by Using a Pre-Trained Language Model",
  "url": "https://openalex.org/W3190460602",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2766359442",
      "name": "Heereen Shim",
      "affiliations": [
        "KU Leuven"
      ]
    },
    {
      "id": "https://openalex.org/A2206743878",
      "name": "Dietwig Lowet",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116875276",
      "name": "Stijn Luca",
      "affiliations": [
        "Ghent University"
      ]
    },
    {
      "id": "https://openalex.org/A2014075120",
      "name": "Bart Vanrumste",
      "affiliations": [
        "KU Leuven"
      ]
    },
    {
      "id": "https://openalex.org/A2766359442",
      "name": "Heereen Shim",
      "affiliations": [
        "KU Leuven"
      ]
    },
    {
      "id": "https://openalex.org/A2206743878",
      "name": "Dietwig Lowet",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116875276",
      "name": "Stijn Luca",
      "affiliations": [
        "Ghent University"
      ]
    },
    {
      "id": "https://openalex.org/A2014075120",
      "name": "Bart Vanrumste",
      "affiliations": [
        "KU Leuven"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6730529904",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W6780996545",
    "https://openalex.org/W6639393196",
    "https://openalex.org/W6674330103",
    "https://openalex.org/W6617145748",
    "https://openalex.org/W2099550922",
    "https://openalex.org/W2971296908",
    "https://openalex.org/W3101345273",
    "https://openalex.org/W6764456104",
    "https://openalex.org/W2950899690",
    "https://openalex.org/W2895547478",
    "https://openalex.org/W2964288660",
    "https://openalex.org/W2562607067",
    "https://openalex.org/W2900401454",
    "https://openalex.org/W6760568010",
    "https://openalex.org/W2251648804",
    "https://openalex.org/W2962808042",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W3095830519",
    "https://openalex.org/W6756615331",
    "https://openalex.org/W2128518360",
    "https://openalex.org/W6763701032",
    "https://openalex.org/W2471138382",
    "https://openalex.org/W1995875735",
    "https://openalex.org/W2042932437",
    "https://openalex.org/W2785787385",
    "https://openalex.org/W6735374517",
    "https://openalex.org/W2964282813",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2951911250",
    "https://openalex.org/W3001060565",
    "https://openalex.org/W1513874326",
    "https://openalex.org/W6730042731",
    "https://openalex.org/W2039643849",
    "https://openalex.org/W3105522431",
    "https://openalex.org/W3014249243",
    "https://openalex.org/W2798820905",
    "https://openalex.org/W2597787948",
    "https://openalex.org/W2923978210",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W2903158431",
    "https://openalex.org/W2963238274",
    "https://openalex.org/W1872312298",
    "https://openalex.org/W2954278700",
    "https://openalex.org/W2952357537",
    "https://openalex.org/W2964059111",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W3043005121",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2951786554",
    "https://openalex.org/W2925618549",
    "https://openalex.org/W2135815793",
    "https://openalex.org/W2085989833",
    "https://openalex.org/W582134693",
    "https://openalex.org/W2964207259",
    "https://openalex.org/W2896457183"
  ],
  "abstract": "Recently proposed pre-trained language models can be easily fine-tuned to a wide range of downstream tasks. However, a large-scale labelled task-specific dataset is required for fine-tuning creating a bottleneck in the development process of machine learning applications. To foster a fast development by reducing manual labelling efforts, we propose a Label-Efficient Training Scheme (LETS). The proposed LETS consists of three elements: (i) task-specific pre-training to exploit unlabelled task-specific corpus data, (ii) label augmentation to maximise the utility of labelled data, and (iii) active learning to label data strategically. In this paper, we apply LETS to a novel aspect-based sentiment analysis (ABSA) use-case for analysing the reviews of the health-related program supporting people to improve their sleep quality. We validate the proposed LETS on a custom health-related program-reviews dataset and another ABSA benchmark dataset. Experimental results show that the LETS can reduce manual labelling efforts 2-3 times compared to labelling with random sampling on both datasets. The LETS also outperforms other state-of-the-art active learning methods. Furthermore, the experimental results show that LETS can contribute to better generalisability with both datasets compared to other methods thanks to the task-specific pre-training and the proposed label augmentation. We expect this work could contribute to the natural language processing (NLP) domain by addressing the issue of the high cost of manually labelling data. Also, our work could contribute to the healthcare domain by introducing a new potential application of NLP techniques.",
  "full_text": "Received June 20, 2021, accepted July 8, 2021, date of publication August 2, 2021, date of current version August 25, 2021.\nDigital Object Identifier 10.1 109/ACCESS.2021.3101867\nLETS: A Label-Efficient Training Scheme for\nAspect-Based Sentiment Analysis by Using\na Pre-Trained Language Model\nHEEREEN SHIM\n1,2, DIETWIG LOWET2, STIJN LUCA\n 3,\nAND BART VANRUMSTE1, (Senior Member, IEEE)\n1eMedia Research Laboratory and STADIUS, Department of Electrical Engineering (ESAT), KU Leuven, 3000 Leuven, Belgium\n2Philip Research, 5656 AE Eindhoven, The Netherlands\n3Department of Data Analysis and Mathematical Modelling, Ghent University, 9000 Ghent, Belgium\nCorresponding author: Heereen Shim (heereen.shim@kuleuven.be)\nThis work was supported by the European Union’s Horizon 2020 Research and Innovation Program under the Marie Skłodowska-Curie\nGrant 766139.\nABSTRACT Recently proposed pre-trained language models can be easily ﬁne-tuned to a wide range of\ndownstream tasks. However, a large-scale labelled task-speciﬁc dataset is required for ﬁne-tuning creating\na bottleneck in the development process of machine learning applications. To foster a fast development by\nreducing manual labelling efforts, we propose a Label-Efﬁcient Training Scheme (LETS). The proposed\nLETS consists of three elements: (i) task-speciﬁc pre-training to exploit unlabelled task-speciﬁc corpus\ndata, (ii) label augmentation to maximise the utility of labelled data, and (iii) active learning to label data\nstrategically. In this paper, we apply LETS to a novel aspect-based sentiment analysis (ABSA) use-case\nfor analysing the reviews of the health-related program supporting people to improve their sleep quality.\nWe validate the proposed LETS on a custom health-related program-reviews dataset and another ABSA\nbenchmark dataset. Experimental results show that the LETS can reduce manual labelling efforts 2-3 times\ncompared to labelling with random sampling on both datasets. The LETS also outperforms other state-of-\nthe-art active learning methods. Furthermore, the experimental results show that LETS can contribute to\nbetter generalisability with both datasets compared to other methods thanks to the task-speciﬁc pre-training\nand the proposed label augmentation. We expect this work could contribute to the natural language\nprocessing (NLP) domain by addressing the issue of the high cost of manually labelling data. Also, our work\ncould contribute to the healthcare domain by introducing a new potential application of NLP techniques.\nINDEX TERMSActive learning, machine learning, natural language processing, neural networks, sentiment\nanalysis.\nI. INTRODUCTION\nRecently proposed pre-trained language models [1]–[3] have\nshown their ability to learn contextualised language rep-\nresentations and can be easily ﬁne-tuned to a wide range\nof downstream tasks. Even though these language models\ncan be trained without manually labelled data thanks to the\nself-supervised pre-training paradigm, large-scale labelled\ndatasets are required for ﬁne-tuning to downstream tasks.\nData labelling can be labour-intensive and time-consuming\ncreating a bottleneck in the development process of machine\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Bin Liu\n.\nlearning applications. Moreover, in real-world scenarios,\nthe labelling scheme can be changed by adding or changing\nlabels after deployment. Therefore, it is critical to be able to\nﬁne-tune the model with a limited number of labelled data\nto reduce manual labelling efforts and foster fast machine\nlearning applications development.\nOne of the possible solutions is to apply active learn-\ning to reduce manual labelling efforts. Active learning is\nan algorithm designed to effectively minimise manual data\nlabelling by querying the most informative samples for train-\ning [4]. Active learning has been extensively studied [4], [5]\nand applied to various applications, from image recogni-\ntion [6], [7] to natural language processing (NLP)\nVOLUME 9, 2021\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ 115563\nH. Shimet al.: LETS: LETS for ABSA by Using a Pre-Trained Language Model\ntasks [8], [9]. Even though active learning guides how to\nstrategically annotate unlabelled data, it does not utilise the\nunlabelled data or labelled data for ﬁne-tuning. For example,\nunlabelled data points can be used for self-supervised learn-\ning or already labelled data points can be further utilised dur-\ning supervised learning, such as by using data augmentation\ntechniques.\nTo not only effectively reduce manual labelling efforts\nbut also maximise the utility of data, we propose a novel\nLabel-Efﬁcient Training Scheme, LETS in short. The pro-\nposed LETS integrates three elements as illustrated in Fig. 1:\n(i) a task-speciﬁc pre-training to exploit unlabelled\ntask-speciﬁc corpus data; (ii) label augmentation to max-\nimise the utility of labelled data; and (iii) active learning to\nstrategically prioritise unlabelled data points to be labelled.\nIn this paper, we apply LETS to a novel aspect-based sen-\ntiment analysis (ABSA) use-case for analysing the reviews\nof a mobile-based health-related program. The introduced\nhealth-related program is designed to support people to\nimprove their sleep quality by restricting sleep-related\nbehaviour. We aim to provide a tailored program by analysing\nreviews of individual experience. To the best of our knowl-\nedge, this is the ﬁrst attempt to implement an automated\nABSA system for health-related program reviews. To illus-\ntrate the success of the novel use-case, we have collected a\nnew dataset and experimentally show the effectiveness of the\nproposed LETS with the collected dataset and a benchmarks\ndataset.\nFIGURE 1. Overview of the proposedLabel-Efficient Training Scheme\n(LETS). Task-specific pre-training utilises unlabelled task-specific corpus\ndata setDc . Label augmentation exploits labelled data setDl . Active\nlearning algorithm selects data from the unlabelled data setDu for\nmanual labelling.\nThe main contributions of this paper include the\nfollowings:\n• A novel use-case of natural language processing and\nmachine learning techniques for the healthcare domain\nis introduced (Sec. III);\n• A novel label-efﬁcient training scheme that integrates\nmultiple components is proposed (Sec. IV);\n• A label augmentation technique is proposed to maximise\nthe utility of labelled data (Sec. IV-B2);\n• A new query function is proposed to search different\nboundaries with two uncertainty scores for active learn-\ning with the imbalanced dataset (Sec. IV-B3);\n• A new evaluation metric for an ABSA system is pro-\nposed to correctly evaluate the performance of a system\nin the end-to-end framework (Sec. V-C).\nII. RELATED WORK\nA. ASPECT-BASED SENTIMENT ANALYSIS\nABSA is a special type of sentiment analysis that aims\nto detect opinion toward ﬁne-grained aspects. Since ABSA\ncan capture insights about user experiences, ABSA has\nbeen widely studied in various industries, from consumer\nproduct sector [10], [11] to service sector [12]–[15].\nABSA entails two steps: aspect category detection and aspect\nsentiment classiﬁcation [16]. During the ﬁrst step, Aspect\nCategory Detection (ACD), a system aims to detect a set\nof the pre-deﬁned aspect categories that are described in\nthe given text. For example, in the domain of restaurant\nreview, the pre-deﬁned set of aspects can be {Food, Price,\nService, Ambience, Anecdotes/Miscellaneous} and the task\nis to detect {Price, Food} out of the text ‘‘This is not a\ncheap place but the food is worth to pay’’. During the sec-\nond step, Aspect Category Polarity (ACP), a system aims\nto classify a text into one of sentiment polarity labels\n(i.e., Positive, Negative, Neutral, etc) given a pair of text\nand aspect categories. For example, the task to produce a\nset of pairs, such as {(Price, Negative), (Food, Positive)}\ngiven the set of ground truth categories{Price, Food} and the\ntext.\nThere has been signiﬁcant improvement in ABSA sys-\ntems over the past few years thanks to the recent progress\nof deep neural network (DNN) based NLP models, [10],\n[12], [13], [15], [17]. For example, Sun et al. [15] pro-\npose a Bidirectional Embedding Representations from Trans-\nformers (BERT) [1] based ABSA system by casting an\nABSA task as a sentence-pair classiﬁcation task. Even though\nthis sentence-pair approach shows the state-of-the-art per-\nformance by exploiting the expanded labelled data set with\nsentence-aspect conversion 1 [15], it still requires a large\namount of labelled data.\nLater, Xu et al. [10] propose a post-training to utilise\nunlabelled corpus datasets to further train a pre-trained model\nfor ABSA. The proposed post-training exploits both the\ngeneral-purpose corpus dataset (i.e., texts from Wikipedia)\nand task-related corpus dataset (i.e., reading comprehension\ndataset) for the end task (i.e., review reading comprehen-\nsion). Xu et al. [10] showed utilising multiple unlabelled\ncorpus datasets can enhance the performance of the end\ntask. Extensive studies on utilising unlabelled corpus for\nfurther pre-training showed that the importance of using\ndomain-relevant data [18], [19]. However, domain-related\ncorpus datasets for further pre-training are possibly not\n1As it is described in the original paper [15], a sentence si in the original\ndata set can be expanded into multiple sentence-aspect pairs (s i, a1), (s i,\na2),..., (si, aN ) in the sentence pair classiﬁcation task, with aspect cate-\ngories an where n ∈{1, 2,..,N}.\n115564 VOLUME 9, 2021\nH. Shimet al.: LETS: LETS for ABSA by Using a Pre-Trained Language Model\navailable in some domain (e.g., healthcare) because of privacy\nissue.2\nB. ACTIVE LEARNING ALGORITHM\nActive learning that aims to select the most informative data\nto be labelled has been extensive studied [4], [5], [20], [21].\nThe core of active learning is a query function that com-\nputes score for each data point to be labelled. Existing\napproaches include uncertainty-based [22], [23], ensemble-\nbased [24], [25], and expected model change-based meth-\nods [4]. Thanks to their simplicity, uncertainty-based methods\nbelong to the most popular ones. Uncertainty-based methods\ncan use least conﬁdence scores [8], [20], [26], max margin\nscores [27], [28], or max entropy scores [29] for querying.\nOne of the earliest studies of active learning with DNN is\nby Wang et al. [6] for image classiﬁcation. They proposed\na Cost-Effective Active Learning (CEAL) framework that\nuses two different scores for querying. One is an uncertainty\nscore to select samples to be manually labelled. And the\nother is a certainty score to select samples to be labelled\nwith pseudo-labels which are their predictions. Both scores\nare computed based on the output of DNN. Wang et al.[6]\nshowed that the proposed CEAL works consistently well\ncompared to the random sampling, while there is no signiﬁ-\ncant difference in the choice of uncertainty measures, among\nthe least conﬁdence, max-margin, and max entropy.\nHowever, other researchers claim that using the output of\nDNN to model uncertainty could be misleading [7], [30].\nTo model uncertainty in DNN, Gal and Ghahramani [30]\nproposed Monte Carlo (MC) dropout as Bayesian approxi-\nmation that performs dropout [31] during inference phase.\nLater, Gal et al. [7] incorporated uncertainty obtained\nby MC dropout with Bayesian Active Learning by Dis-\nagreement (BALD) [32] to demonstrate a real-world appli-\ncation of active learning for image classiﬁcation. Also,\nShen et al.[8] applied BALD to an NLP task and experimen-\ntally showed that BALD slightly outperforms the traditional\nuncertainty method that uses the least conﬁdence scores. The\nresults from the large-scale empirical study Siddhant and\nLipton [9] also showed the effectiveness of BALD for various\nNLP tasks. Even though BALD outperforms the random\nsampling method, the differences between BALD and active\nlearning methods with the traditional uncertainty scores\n(i.e., least conﬁdence, max margin, and max entropy) are\nmarginal [8], [9]. Also, BALD is computationally more\nexpensive than the traditional methods because it requires\nmultiple forward passes. Therefore, the traditional uncer-\ntainty scores are reasonable options when deploying active\nlearning in a real-world setting.\nPractical concerns on how to implement active learn-\ning in real-world settings include the issue that a model\ncan perform poorly when the amount of labelled data is\n2For example, General Data Protection Regulation (GDPR) includes the\npurpose limitation principle mentioning that personal data be collected for\nspeciﬁed, explicit, and legitimate purposes, and not be processed further in\na manner incompatible with those purposes (Article 5(1)(b), GDPR).\nminimal [33]. This issue is referred to as the cold-start issue.\nIdeally, active learning could be most useful in low-resource\nsettings. In practice, however, it is more likely that the model\nmight work poorly with the limited number of labelled data at\nthe beginning of active learning [34]. Therefore, introducing\na component to ensures a certain level of performance with\nthe limited labelled data is important to address the cold-start\nissue.\nIII. ASPECT-BASED SENTIMENT ANALYSIS FOR\nHEALTH-RELATED PROGRAM REVIEWS\nThis section describes a mobile-based health-related pro-\ngram use-case that we call Caffeine Challenge. To conduct\naspect-based sentiment analysis on the reviews of Caffeine\nChallenge, an experimental dataset is collected and anno-\ntated. The next subsections explain the details of the use-case,\ndata collection protocol, and data labelling scheme with the\ninitial data analysis result.\nA. CAFFEINE CHALLENGE USE-CASE\nIn this study, we introduce a health-related program that\nis designed to help people improve their sleep quality by\nrestricting behaviour that might negatively affect their sleep\nquality. Having caffeinated beverage or desserts during the\nlate afternoon and evening is selected as a target behaviour\nfor this study. A challenge rule is restricting a caffeine\nintake after 13:00 for two weeks. During the program,\nparticipants use a mobile application to log their progress\nand receive notiﬁcations and recommendations of relevant\ninformation. At the end of the program, an in-app chatbot\n(conversational agent) asks about challenge experience and\nthe participants are allowed to provide answers in free-text\nsentences. Our goal is to understand users’ sentiments\ntowards different aspects of the program by analysing the\nreview data. To this end, we aim to develop an automated\nABSA system for health-related program reviews as illus-\ntrated in Table. 1 where a system detects opinions (senti-\nment polarity) expressed towards multiple aspects. Since the\nABSA system can capture detailed user opinions, it can be\nused to tailor the health-related program to individual users.\nTABLE 1. An example of aspect-based sentiment analysis based on the\nfree-text user review of a health-related program.\nB. EXPERIMENTAL DATA COLLECTION\nIn the real-world machine learning application implemen-\ntation process, multiple cycles on iterative development are\noften required: ﬁrstly, implementing a baseline model with\nVOLUME 9, 2021 115565\nH. Shimet al.: LETS: LETS for ABSA by Using a Pre-Trained Language Model\nFIGURE 2. Annotation result of the collected caffeine challenge dataset. Sentiment class distribution per aspect\ncategory (a) and the number of aspect-sentiment labels per text (b) are shown.\nexperimental data and then gradually updating the model with\nreal-world data. To develop the ﬁrst version of the ABSA sys-\ntem, we conducted a pilot study with a semi-realistic dataset\nthat is collected from an online survey via a crowd-sourcing\nplatform (Amazon Mturk). At the beginning of the sur-\nvey, an instruction containing details of the Caffeine Chal-\nlenge (i.e., its purpose, goal, procedure, and consent form),\nis given to the survey participants. Then each participant\nhas received a questionnaire regarding the experience of the\nCaffeine Challenge. Then the participants have requested\nto answer the questions by imagining that they have done\nthis challenge. In total, we recruited 1,000 participants and\ncollected 12, 000 answers and examples of collected data are\nshown in Appendix A.\nC. DATA LABELLING\nWe annotated a random subset of the collected data for\naspect-based sentiment analysis. Based on both health-related\nprogram and app development perspectives, seven different\naspects are deﬁned:\n1) Sleep Quality (SQ)\n2) Energy (E)\n3) Mood (M)\n4) Missing Caffeine (MC)\n5) Difﬁculty Level (DL)\n6) Physical Withdrawal Symptoms (PWS)\n7) App Experience (AE)\nEach aspect category is annotated with one of the senti-\nment values as follows: Positive, Neutral, Negative, and Not\nMentioned. Not Mentioned class is introduced as a place-\nholder for an empty sentiment value. For example, when\na sample does not describe any opinion towards a speciﬁc\naspect, then it is labelled as Not Mentioned for that aspect\ncategory. A labelling scheme of each aspect category is given\nin Appendix B.\nFig. 2 illustrates annotation results and Fig. 3 shows the\nexample of annotated data point. As it is shown in Fig. 2a,\nthe majority of sentiment label within all aspect categories is\nan empty sentiment label (Not Mentioned). Some categories\n(Sleep Quality, Energy, and Mood) appeared more frequently\ncompared to other categories (Missing Caffeine, Difﬁculty\nLevel, Physical Withdrawal Symptoms, and App Experi-\nence). The former group is denoted as majority aspect cat-\negories and the latter group is denoted as minority aspect\ncategories. Fig. 2b shows the distribution of the number of\naspect-sentiment labels per text, excluding Not Mentioned\nlabels. It is observed that most of the annotated texts have\neither one or two aspect-sentiment labels and only a few have\nmore than three aspect-sentiment labels.\nIV. LABEL-EFFICIENT TRAINING SCHEME FOR\nASPECT-BASED SENTIMENT ANALYSIS\nWe develop an automated ABSA system by utilising a\npre-trained language model. Also, a label-efﬁcient train-\ning scheme is proposed to reduce effectively manual\nlabelling efforts. The following subsections will explain the\nABSA system and the proposed label-efﬁcient training\nscheme in detail.\nA. ASPECT-BASED SENTIMENT ANALYSIS SYSTEM\nSimilar to the previous work by Sun et al. [15], we refor-\nmulate ABSA task as sentence-pair classiﬁcation by using\na pre-trained language model, BERT [1]. Fig. 4 illustrates a\nsentence-pair classiﬁcation approach for ABSA. As shown in\nthe ﬁgure, the proposed ABSA system produces the probabil-\nity distribution over sentiment classes C, including polarised\n115566 VOLUME 9, 2021\nH. Shimet al.: LETS: LETS for ABSA by Using a Pre-Trained Language Model\nFIGURE 3. An example of annotated data. Each annotated data point\nincludes free-text and labels which are pairs of aspect category and\nsentiment class.\nFIGURE 4. Illustration of aspect-based sentiment analysis (ABSA) as a\nsentence-pair classification by using bidirectional embedding\nrepresentations from transformer (BERT).\nsentiment classes S (e.g., Positive, Neutral, Negative, etc) and\nan empty placeholder (e.g., Not Mentioned), for the given\nfree-text sentence xi and aspect category ak . This formal-\nisation allows a single model to perform aspect category\ndetection and aspect sentiment classiﬁcation at the same\ntime. Also, adding an aspect category as the second part of\ninput can be seen as providing a hint to the model where to\nattend for creating a contextualised embedding. Moreover,\nthis formalisation allows expanding the training data set by\naugmenting labelled data, which will be explained in the\nfollowing section (Sec. IV-B2).\nFormally, an input is transformed into a format of xk\ni =\n[[CLS], xi, [SEP], ak , [SEP]], where xi =[w1\ni ,w2\ni ,...,\nwni\ni ] is the tokenised i-th free-text, ak =[a1\nk ,a2\nk ,..., amk\nk ] is\nthe tokenised k-th aspect category in K aspect categories, and\n[CLS] and [SEP] are special tokens indicating classiﬁca-\ntion and separation, respectively. Then the input is fed to the\nBERT model (f θ) that produces contextualised embeddings\nfor each token by using multi-head attention mechanism [1].\nThe contextualised embedding vector ek\ni ∈ Rd×1, corre-\nsponding to the classiﬁcation token [CLS], is used as the\nﬁnal representation of the given input xk\ni . Then a classiﬁca-\ntion layer projects ek\ni into the space of the target classes:\nek\ni =fθ(xk\ni ) (1)\nˆyk\ni =softmax(W ·ek\ni +b) (2)\nwhere ˆyk\ni ∈[0,1]|C|is the estimated probability distribution\nover the sentiment classes C for the given free-text sample xi\nand aspect category ak pair, and fθ, W ∈R|C|×d , and b ∈R|C|\nare trainable parameters.\nB. LABEL-EFFICIENT TRAINING SCHEME\nOne of the bottlenecks in developing an ABSA system\nwith a pre-trained language model is to create a large-scale\nlabelled task-speciﬁc dataset for ﬁne-tuning which requires\na labour-intensive manual labelling process. To mitigate this\nissue, we propose a Label-Efﬁcient Training Scheme, which\nwe refer as LETS. The proposed LETS consists of three\nelements to effectively reduce manual labelling efforts by\nutilising both unlabelled and labelled data. Fig. 1 illus-\ntrates the overview of the proposed LETS. The ﬁrst ele-\nment is task-speciﬁc pre-training to exploit the unlabelled\ntask-speciﬁc corpus data. The second element is label aug-\nmentation to maximise the utility of the labelled data. The\nthird element is active learning to efﬁciently prioritise the\nunlabelled data for manual labelling. The followings will\ndescribe the details of each element.\n1) TASK-SPECIFIC PRE-TRAINING\nTask-speciﬁc pre-training is used to exploit the unlabelled\ntask-speciﬁc corpus data. We adopt a novel pre-training strat-\negy of Masked Language Modelling (MLM) from BERT [1]\nto train an Attention-based model to capture bidirectional\nrepresentations within a sentence. More speciﬁcally, during\nthe MLM training procedure, the input is formulated with\na sequence of tokens that are randomly masked out with a\nspecial token [MASK] at a certain percentage p. Then the\ntraining objective is to predict those masked tokens. Since\nground truth labels are original tokens, MLM training can\nproceed without manual labelling.\n2) LABEL AUGMENTATION\nLabel augmentation is proposed to not only address the\ncold-start issue in active learning but also to maximise the\nutility of the labelled data. The proposed label augmentation\nalgorithm multiplies the labelled data set by replacing aspect\ncategories with similar words. This might look similar to\ncommon data augmentation techniques proposed by Wei and\nZou [35] that includes synonym replacement, random inser-\ntion, random swap, and random deletion. Our method, how-\never, modiﬁes only the second part of the input (i.e., aspect\ncategory) while keeping the original free-text part. The pro-\nposed label augmentation technique is applied to pre-deﬁned\naspect categories with polarised sentiment classes S\nVOLUME 9, 2021 115567\nH. Shimet al.: LETS: LETS for ABSA by Using a Pre-Trained Language Model\n(e.g., Positive, Neutral, Negative, etc). Algorithm 1 sum-\nmarises the proposed label augmentation technique.\nAlgorithm 1Label Augmentation\nData: Labelled training set Dl , a dictionary of similar\nwords per aspect category Dict, polarised\nsentiment classes S\nResult: Augmented training set ˆDl\nˆDl ←Dl\nfor dl ∈Dl do\ntxt ←getFreeText(dl )\nasps ←getAspects(dl )\nfor asp ∈asps do\nsenti ←getSentimentLabel(dl , asp)\nif senti ∈S then\nˆasps ←Dict(asp)\nfor ˆasp ∈ ˆasps do\nˆdl ←(txt, ˆasp, senti)\nˆDl ←addData( ˆdl )\nend for\nend if\nend for\nend for\n3) ACTIVE LEARNING\nActive learning is used to prioritise the unlabelled data points\nto be manually labelled and added to the training pool. The\ncore of active learning is a query function that scores the\ndata points to use a labelling budget effectively in terms of\nperformance improvement.\nEven though most of the existing active learning meth-\nods consider balanced datasets, one typical feature of a\nreal-world dataset is that it can be imbalanced [36]. As it\nis shown in Sec. III-C, the collected dataset is also highly\nimbalanced: there are majority aspect categories that more\noften appear in the training set and minority aspect cate-\ngories that less often appear in the training set. We observe\nthat a ﬁne-tuned ABSA model performs differently towards\nmajority and minority aspect classes. For example, Fig. 5\nillustrates the vector representations before the ﬁnal clas-\nsiﬁcation layer 3 plotted into 2-dimensional space by using\na dimensionality reduction algorithm [37]. From the ﬁgure,\nit is observed that the ﬁne-tuned model can create distinctive\nrepresentations between sentiment labels within the Sleep\nQuality aspect category, while the model fails to learn to\ndifferentiate data points among sentiment classes and empty\nsentiment class within the App Experience aspect category.\nThis shows that a ﬁne-tuned ABSA model performs relatively\nwell towards majority aspect categories and its prediction\nis reliable, whereas a model works poorly towards minority\naspect categories and it tends to fail to even detect the aspect\ncategories.\n3The ﬁne-tuned model at the initial step of active learning experiment\n(Sec. V-D1) is used.\nFIGURE 5. The final vector representations of inputs plotted\nin 2-dimensional space for Sleep Quality (a) and App Experience\n(b) aspect categories. green, yellow, red, and grey color indicate inputs\nwith Positive, Neutral, Negative, andNot Mentionedsentiment labels,\nrespectively. All data points were not used during the training phase.\nTherefore, we propose two uncertainty measures for\nmajority aspect categories and minority aspect categories,\nrespectively:\numajor =1 −Pr(ˆyk\ni =arg max\nc∈C\n(ˆyk\ni )|xk\ni ) (3)\numinor =1 −|Pr (ˆyk\ni =nm|xk\ni ) −\n∑\nS\n(Pr(ˆyk\ni =s|xk\ni ))| (4)\n=1 −|1 −2Pr(ˆyk\ni =nm|xk\ni )| (5)\nwhere Pr(ˆyk\ni =arg max\nc∈C\n(ˆyk\ni )|xk\ni ) is the highest probability in\nthe estimated probability distribution over sentiment classes\ngiven xk\ni , nm refers Not Mentioned, and S refers a polarised\nsentiment classes set (e.g., Positive, Neutral, Negative, etc).\numajor is the traditional least conﬁdence score and uminor is\nthe margin of conﬁdence score between an empty placeholder\n(i.e., Not Mentioned) and sum of other sentiment classes.\nAs it is shown in (5), uminor treats the model’s prediction as\nbinary classiﬁcation result (i.e., Not Mentionedor Mentioned)\nproducing high uncertainty scores when Pr(ˆyk\ni =nm|xk\ni ) is\nclose to 0.5. The intuition of introducing uminor is allowing\na model to focus on detecting whether the aspect category\nis mentioned or not. The proposed two uncertainty measures\nallow the model to search different boundaries during active\nlearning: the boundaries where the model is uncertain about\nits aspect category sentiment classiﬁcation result towards\nmajority classes is described by umajor . And the boundary\nwhere the model is uncertain about aspect category detection\nresult towards minority classes is described by uminor .\nAlgorithm 2 shows the proposed LETS that integrates three\nelements. Firstly, a pre-trained model is further pre-trained\nwith an unlabelled task-speciﬁc corpus data set. Then the\ntask-speciﬁc pre-trained model is used for initialisation dur-\ning active learning iterations. Active learning is repeated t\ntimes and each time a model is ﬁne-tuned with the labelled\ndata set that is augmented by the proposed label augmentation\ntechnique. At the end of each iteration step, n samples are\nqueried from the unlabelled set for manual labelling. For\nquerying, each Query function Qmajor and Qminor select n/2\nsamples where umajor and uminor are the highest, respectively.\n115568 VOLUME 9, 2021\nH. Shimet al.: LETS: LETS for ABSA by Using a Pre-Trained Language Model\nAlgorithm 2Label-Efﬁcient Training Scheme (LETS)\nData: Pre-trained model Mpt , unlabelled task-speciﬁc\ncorpus data set Dc, initial training set Dl ,\nunlabelled training set Du, total iteration t,\nlabelling budget n, query function for majority\ncategories Qmajor , query function for minority\ncategories Qminor\nResult: Fine-tuned model Mt , Labelled data set Dt\nMtspt ←task-speciﬁcPre-train(Mpt , Dc,)\ni =0\nDi ←Dl\nwhile i <t &|Du|>0 do\nD′\ni ←augmentLabel(Di)\nMi ←ﬁneTune(Mtspt , D′\ni)\ndmajor ←Qmajor (Du,Mi,n/2)\ndminor ←Qminor (Du,Mi,n/2)\nDi+1 ←Di\nDi+1 ←addData(addLabels(dmajor ∪dminor ))\nDu ←Du −{d major ∪dminor }\ni+= 1\nend whileV. EXPERIMENTS\nA. DATASETS\nWe evaluate the proposed method on two datasets. One is the\ncustom dataset that we collected for the Caffeine Challenge\nuse-case. The other is SemEval-2014 [16] task 4 dataset 4 that\nis the most widely used benchmark dataset for aspect-based\nsentiment analysis.\n1) CUSTOM DATASET: CAFFEINE CHALLENGE\nThe custom dataset, which is described in Sec. III, is named\nas a Caffeine Challenge dataset. We annotate a random\nsubset of the Caffeine Challenge dataset with 7 different\naspect categories (i.e., Sleep Quality, Energy, Mood, Missing\nCaffeine, Difﬁculty Level, Physical Withdrawal Symptoms,\nApp Experience) and 3 sentiment labels S ={Positive, Neu-\ntral, Negative} and an empty placeholder (i.e., Not Men-\ntioned). The aspect categories distribution of the Caffeine\nChallenge dataset is imbalanced as described in Sec. III.\nAspect categories are divided into subgroups of majority\nand minority aspect categories based on the frequency in\na training set: {Sleep Quality, Energy, Mood} as majority\naspect categories and {Missing Caffeine, Difﬁculty Level,\nPhysical Withdrawal Symptoms, and App Experience} as\nminority aspect categories.\nThe unlabelled corpus data set are used for task-speciﬁc\npre-training and the annotated data set is used for\nﬁne-tuning. Table 2 summarises the sizes of the Caffeine\nChallenge dataset used for the experiments. For task-speciﬁc\npre-training, sentences from the unlabelled corpus data set\nare used. For the ﬁne-tuning, 5-fold cross-validation splits\n4https://alt.qcri.org/semeval2014/task4/\nTABLE 2. Size of Caffeine Challenge dataset used for the experiments.\nSentences from the unlabeled corpus data set used as the task-specific\ncorpus data for task-specific pre-training. S-A pairs indicate\nsentence-aspect pairs and sentence-aspect pairs from the\ntraining set are used for fine-tuning.\nare created at the sentence level and sentence-aspect pairs are\nused for training.\n2) BENCHMARK DATASET: SemEval\nThe SemEval-2014 task 4 dataset contains restaurant reviews\nannotated with 5 aspect categories (Food, Price, Ser-\nvice, Ambience, Anecdotes/Miscellaneous) and 4 senti-\nment labels S ={Positive, Neutral, Negative, Conﬂict 5}.\nSince the SemEval dataset is also imbalanced, as illus-\ntrated in Appendix. C, we deﬁne majority and minority\ncategories: {Food, Anecdotes/Miscellaneous} and {Service,\nAmbience, Price} as majority and minority aspect categories,\nrespectively.\nWe used the original SemEval train set for the experiments\nto create 5-fold cross-validation splits. Table 3 summarises\nthe size of SemEval dataset used for the experiments. For\ntask-speciﬁc pre-training, sentences from the training set are\nused. For the ﬁne-tuning, sentence-aspect pairs are created\nwith an empty placeholder (Not Mentioned) for the sentences\nthat do not contain a sentiment label towards speciﬁc aspect\ncategories.\nTABLE 3. Size of SemEval dataset used for the experiments. Sentences\nfrom the training set are used as the task-specific corpus data for\ntask-specific pre-training. S-A pairs indicate sentence-aspect pairs and\nsentence-aspect pairs from the training set are used for fine-tuning.\nB. EXPERIMENTAL SETTINGS\n1) TASK-SPECIFIC PRE-TRAINING AND FINE-TUNING\nWe use the pre-trained uncased BERT-base model as\nthe pre-trained model (PT). The task-speciﬁc pre-trained\nmodel (TSPT) is created by further training the pre-\ntrained model on the task-speciﬁc corpus data with the\nmasked-language modelling (MLM) objective with masking\nprobability p =0.15. The TSPT is used to initialise the pro-\nposed method and the PT is used to initialise other methods\nduring the active learning process. For ﬁne-tuning, the ﬁnal\nclassiﬁcation layer is added and entire model parameters are\n5The conﬂict label applies when both positive and negative sentiment is\nexpressed about an aspect category [16]\nVOLUME 9, 2021 115569\nH. Shimet al.: LETS: LETS for ABSA by Using a Pre-Trained Language Model\nupdated. More detailed implementation and hyperparameter\nsettings are given in Appendix. D.\n2) LABEL AUGMENTATION\nLabel augmentation multiplies the amount of labelled data\nby generating synthesised pairs of sentence and aspect cat-\negories by replacing aspect categories with similar words.\nThe pre-deﬁned dictionary containing a list of similar words\nis used for label augmentation and label augmentation is\napplied to the only minority aspect categories to avoid inef-\nﬁcient augmentation. The pre-deﬁned dictionaries are given\nin Appendix E.\n3) ACTIVE LEARNING\nActive learning experiments are repeated 5 times with 5-fold\ncross-validation splits. At each fold, the initial labelled data\nset (i.e., seed data) is randomly selected from the training\nset at the sentence level and transformed into sentence-aspect\npairs. For the Caffeine Challenge dataset, 20% of the training\nset (n=455) is used as seed data (D l ) and the remaining data is\nused as unlabelled data (D u). For the SemEval dataset, 10%\nof the training set (n=1,220) is used as seed data (D l ) and\nthe remaining data is used as unlabelled data (D u). Active\nlearning is iterated with 10 steps with a ﬁxed labelling budget\n(n=|Du|/10). At the initial iteration step (t=0), a model is\ntrained on the seed data. During active learning steps, more\ndata are iteratively added to the training set by selecting\nunlabelled data.\nFor comparison, we implemented BALD by using MC\ndropout [30], Cost-Effective Active Learning (CEAL) [6],\nleast conﬁdence scores, and random sampling. For BALD,\nwe use the same approximation by Siddhant and Lipton [9]\nto compute uncertainty score as the fraction of models which\ndisagreed with the most popular choice. The number of\nstochastic forward passes for BALD is set to 10. For CEAL,\nthe least conﬁdence score is used for calculating uncertainty\nand the threshold for pseudo-labelling is set to 0.05 with a\ndecay rate of 0.0033. Since pseudo-labels are not included\nin the labelling budget, the active learning with CEAL can\nbe terminated early when there is no more data for manual\nlabelling. More details of these methods can be found in the\noriginal papers [6], [9].\nC. EVALUATION METRICS\nIn this paper, we used two different metrics to evalu-\nate the performance of an ABSA system. One metric is\naspect category detection (ACD) and the other metric is\naspect category sentiment classiﬁcation (ACSC). Aspect cat-\negory detection (ACD) is proposed by Pontiki et al. [16]\nand limited to evaluating aspect category detection ignor-\ning the performance of aspect category sentiment classiﬁ-\ncation. Aspect category polarity (ACP) metric is proposed\nto assess the sentiment classiﬁcation performance of a sys-\ntem [16]. However, as it is mentioned in the previous study by\nBrun and Nikoulina [14], the ACP metric presumes the\nground truth aspect categories and cannot be used to\ncorrectly evaluate an ABSA system end-to-end. To address\nthis issue, we introduce a new metric of aspect category sen-\ntiment classiﬁcation (ACSC) which is the modiﬁed version\nof ACP taking into account false aspect category detection\nresults.\n1) ASPECT CATEGORY DETECTION (ACD)\nACD is used to evaluate how a system accurately detects a set\nof aspect categories mentioned in the input text. F1 score is\nused which is deﬁned as:\nF1 =2 · P ·R\nP +R\nwhere precision (P) and recall (R) are:\nP =|E ∩G|\n|E| , R =|E ∩G|\n|G|\nwhere |∗| denotes the cardinality of a set *, E is the set of\naspect categories that a system estimates for each input, and\nG is the set of the target aspect categories. Micro-F 1 scores\nare calculated at sentence-level and averaged over all inputs\nand macro-F 1 scores are calculated and averaged at aspect\ncategory-level.\n2) ASPECT CATEGORY SENTIMENT CLASSIFICATION (ACSC)\nACSC is used to evaluate the performance of an ABSA sys-\ntem end-to-end. Since the proposed ABSA system produces\nmultiple sentence-pair predictions for a single text input,\nthe predictions are aggregated to compute (aspect, polarity)\npairs at sentence-level while eliminating the pairs that contain\nNot Mentioned as a target as well as a predicted sentiment\nclass. F1 scores are calculated on the (aspect, polarity) pairs\nat aspect-level following:\nP = TP\nTP +FP, R = TP\nTP +FN1 +FN2\nwhere TP, FP, FN1, and FN2 are deﬁned as in Table 4. Similar\nto ACD, both micro- and macro-averaged F1 are used.\nTABLE 4. Types of error used to compute aspect category sentiment\nclassification (ACSC) scores. TP, NA, FN1, FN2, FP refer to true positive,\nnot applicable, false negative type 1, false negative type 2, false positive,\nrespectively. TARG and PRED refer to a target sentiment class and a\npredicted sentiment class whereS is a set of polarised sentiment\nclasses (e.g., positive, neutral, negative, etc).\nD. RESULTS AND ANALYSIS\n1) EXP 1: CAFFEINE CHALLENGE\nFig. 6 illustrates the active learning results with the Caffeine\nChallenge dataset. Active learning results in ACD metrics\nare illustrated in Fig. 6a and Fig. 6b. All active learning\n115570 VOLUME 9, 2021\nH. Shimet al.: LETS: LETS for ABSA by Using a Pre-Trained Language Model\nFIGURE 6. Active learning results with the Caffeine Challenge dataset. Each line indicates averaged 5-fold results with standard deviation as shade.\nThe bottom X-axis indicates the active learning iteration step and the top x-axis indicates the number of manually labeled training data. Y-axis\nindicates the performance score.\nmethods show better performance improvement than random\nsampling. It is observed that all models achieve much lower\nperformances in macro-averaged scores than micro-averaged\nscores. These results show that the models perform worse\ntowards minority aspect categories in the Caffeine Challenge\ndataset. In micro-averaged ACD score, LETS outperforms\nother active learning methods in general. In macro-averaged\nACD score, CEAL achieves slightly better performance\nthan LETS. However, the ACD metrics are incomplete\nbecause they ignore sentiment classiﬁcation results.\nACSC metric is proposed to address the limitation of the\nACD metric and correctly evaluate the ABSA system end-\nto-end. Fig. 6c and Fig. 6d illustrate active learning results\nwith the respect to the ACSC metrics. From the ﬁgures,\nit is observed that the performances of all models decrease\ncompared to the observations from the ACD metrics. Similar\nto the results with the ACD metrics, LETS shows better\nperformance improvement compared to other active learning\nmethods. Speciﬁcally, from iteration step 0 to 1, the perfor-\nmance of LETS increases from 35.1% to 48.2%, while other\nmethod increase from 33.7% up to 47.1% in macro-averaged\nACSC metric. The most signiﬁcant difference is observed\nbetween LETS and random sampling. For example, random\nsampling achieves a similar performance of 48.2% at itera-\ntion step 2-4. Moreover, the difference between LETS and\nrandom sampling increases over iteration steps. The random\nsampling method at iteration step 6-7 and LETS at itera-\ntion 2 show similar performances in terms of macro-average\nACSC metric. These results suggest that LETS can reduce\nmanual labelling efforts 2-3 times better compared to the\nrandom sampling method. Also, LETS slightly outperforms\nother active learning methods at the beginning of the iteration\nstep with the respect to the ACSC metrics. This result shows\nthat the task-speciﬁc and the proposed label augmentation\nVOLUME 9, 2021 115571\nH. Shimet al.: LETS: LETS for ABSA by Using a Pre-Trained Language Model\nFIGURE 7. Active learning results with the SemEval dataset. Each line indicates averaged 5-fold results with standard deviation as\nshade. The bottom X-axis indicates the active learning iteration step and the top x-axis indicates the number of manually labeled\ntraining data. Y-axis indicates the performance score.\ncan contribute to better generalisability with the Caffeine\nChallenge data set.\nPerformance differences between LETS and random\nsampling method are statistically signiﬁcant (Wilcoxon\nsigned-rank test with p <.05) from iteration step 1 to 7 and\niteration step 2 to 5 in micro-and macro-averaged ACSC met-\nrics, respectively. However, performance differences between\nLETS and active learning methods are not statistically signiﬁ-\ncant (p >.05) throughout the entire iteration steps. In general,\nall methods show high variances of performances.\nOne interesting observation is CEAL achieves lower per-\nformances than LETS in terms of micro-averaged ACSC\nmetric, especially in the later iteration steps. This is different\nfrom the observation from the micro-averaged ACD metric.\nA possible explanation for this is as follows: CEAL uses\npseudo-labels. These pseudo-labels might not correct in terms\nof sentiment classes and errors might propagate throughout\nthe iteration steps. Since the ACD metrics ignore sentiment\nclassiﬁcation results, this error might not be detected. Results\nwith the macro-averaged ACSC metric show similar trends\nto the results with the macro-averaged ACD metric. These\nresults suggest LETS slightly outperforms CEAL in terms of\nend-to-end evaluation metric.\n2) EXP 2: SemEval\nFig. 7 illustrates the active learning results with SemEval\nbenchmark dataset. Compared to the results with the Caf-\nfeine Challenge dataset, it is observed that the results with\nthe SemEval dataset show less ﬂuctuated learning curves in\ngeneral. It is potentially because the SemEval dataset contains\nfewer aspect categories with more training data.\nAs illustrated in Fig. 7a and Fig. 7b, LETS shows slightly\nfaster learning curves compared to other methods in terms of\nthe ACD metrics. The random sampling method shows better\nlearning curves compared to other active learning methods\n(i.e., BALD, CEAL, least conﬁdence) in the ACD metrics.\nHowever, this does not imply that the random sampling\nmethod outperforms other active learning methods because\nthe ACD metrics ignore sentiment classiﬁcation results.\n115572 VOLUME 9, 2021\nH. Shimet al.: LETS: LETS for ABSA by Using a Pre-Trained Language Model\nFIGURE 8. Compared active learning results for ablation study with the caffeine challenge dataset. Each line indicates averaged 5-fold results\nwith standard deviation as shade. The bottom X-axis indicates the active learning iteration step and the top x-axis indicates the number of\nmanually labeled training data. Y-axis indicates the performance score. PT and TSTP refer to the model with pre-training and task-specific\npre-training, respectively. Masked language modeling is used for task-specific pre-training objective. +LA indicates that label augmentation is\napplied during the active learning process. All models use the proposed active learning method.\nFig. 7c and Fig. 7d show the active learning results in terms\nof the ACSC metrics. It is observed that the performances of\nall models decrease compared to the observations from the\nACD metrics because the ACSC metrics consider sentiment\nclassiﬁcation results. From the ﬁgures, we can also see that\nthe random sampling method achieves slower learning curves\ncompared to the active learning methods. These results are\nopposite from the results with the ACD metrics and imply\nthat the model trained with randomly sampled data tends to\nmore misclassify sentiment labels.\nIn the ACSC metrics, it is observed that LETS sub-\nstantially outperforms other active learning methods and\nrandom sampling method by showing fast performance\nimprovement. For example, from iteration step 0 to 1,\nthe performance of LETS substantially increases from 45.5%\nto 61.6%, while the performances of other methods only\nincrease from 38.3% to around 50.8% in macro-averaged\nACSC metric. Other methods achieve a similar performance\nof 61.6% at iteration step 2-3, which means that LETS\ncan reduce manual labelling effort 2-3 times better with\nthe SemEval dataset. Moreover, it is worth mentioning that\nLETS achieves signiﬁcantly (Wilcoxon signed-rank test with\np <.05) better performances than other methods at the begin-\nning and the end of iteration thanks to the task-speciﬁc\npre-training and label augmentation. Similar trends are also\nobserved in the micro-averaged ACSC metric. Similar to the\nresult with the Caffeine Challenge dataset, this result shows\nthat the task-speciﬁc and the proposed label augmentation\ncan also contribute to better generalisability with the SemEval\ndataset.\nPerformance differences between LETS and random sam-\npling method are statistically signiﬁcant (p <.05) through-\nout entire iteration steps in both micro-and macro-averaged\nACSC metrics. Also, performance differences between LETS\nand other active learning methods are statistically signiﬁcant\n(p <.05) from iteration 0 to 4 for BALD and from iteration\nVOLUME 9, 2021 115573\nH. Shimet al.: LETS: LETS for ABSA by Using a Pre-Trained Language Model\nFIGURE 9. Compared active learning results for ablation study with the SemEval dataset. Each line indicates averaged 5-fold\nresults with standard deviation as shade. The bottom X-axis indicates the active learning iteration step and the top x-axis\nindicates the number of manually labeled training data. Y-axis indicates the performance score. PT and TSTP refer to the model\nwith pre-training and task-specific pre-training, respectively. Masked language modeling is used for task-specific pre-training\nobjective. +LA indicates that label augmentation is applied during the active learning process. All models use the proposed active\nlearning method.\nstep 0 to 2 for CEAL and least conﬁdence methods, respec-\ntively, in both micro and macro-averaged ACSC metrics.\nE. DISCUSSION\nThe proposed LETS integrates multiple components, includ-\ning task-speciﬁc pre-training, label augmentation, and active\nlearning. To investigate the effect of task-speciﬁc pre-training\nwith label augmentation separately, we further analyse the\nperformances of a pre-trained model (PT) and task-speciﬁc\npre-trained model (TSPT) by ablating the label augmen-\ntation (LA) component. Fig. 8 and Fig. 9 summarise the\nablation study with the Caffeine Challenge dataset and the\nSemEval dataset, respectively. Note that all models use the\nproposed active learning method.\nFrom the Fig. 8 and Fig. 9, it is observed that each\ntask-speciﬁc pre-training and label augmentation provides\nperformance improvement in the ACSC metrics. Nonethe-\nless, more consistent improvement is observed when both\ncomponents are applied together. For example, the results\nfrom the Caffeine Challenge dataset, as illustrated in Fig. 8,\nshow that task-speciﬁc pre-training can contribute to per-\nformance improvement and label augmentation can further\nprovide performance boost, especially in early iteration steps.\nSimilar trends are also observed in the results from the\nSemEval dataset as illustrated Fig. 9. The major differences\nare the results from the SemEval dataset are more stable\nthroughout the iteration steps. The results from the Semeval\ndataset, as illustrated in Fig. 9, show signiﬁcant differences\n(p < .05) between the task-speciﬁc pre-trained model\nwith label augmentation (TSPT+LA) and the pre-trained\nmodel (PT) from iteration step 0 to step 4. This suggests that\nthe combination of task-speciﬁc pre-training and label aug-\nmentation can contribute statistically signiﬁcant performance\n115574 VOLUME 9, 2021\nH. Shimet al.: LETS: LETS for ABSA by Using a Pre-Trained Language Model\nimprovement for the SemEval dataset, in early iteration steps.\nInterestingly, each task-speciﬁc pre-training and label aug-\nmentation can also contribute to the similar performance\nimprovement of combining both of them. This suggests that\napplying either ask-speciﬁc pre-training or label augmenta-\ntion can be also beneﬁcial for the SemEval dataset.\nVI. LIMITATIONS AND FUTURE STUDIES\nEven though we show the effectiveness of the proposed\nmethod by validating with two different datasets, some\npoints can be further studied. Firstly, the Caffeine Challenge\ndataset is semi-realistic and not collected from actual users\nof a mobile application. This is mainly because the goal of\nthis paper was to conduct a pilot study of developing an\naspect-based sentiment analysis system for the healthcare\ndomain prior to having a mobile application available. There-\nfore, further study is needed to collect real-world data and\nconduct experiments to validate the developed system. Since\nthe real-world data are not labelled and the main contribution\nof this paper is proposing a label-efﬁcient training scheme,\nwe argue that the proposed method can be used to efﬁciently\nlabel the real-world data to further train the system.\nThe second limitation is the handcrafted rules of the pro-\nposed methods. The majority and minority classes were\ndeﬁned based on the frequency in the training sets. Further\nstudy could explore an algorithmic approach to distinguish\nbetween majority and minority classes. For example, in the\nactive learning setting, minority classes can be dynamically\ndeﬁned based on the labelled data set of the previous iteration\nstep. Also, the proposed label augmentation uses handcrafted\ndictionaries. A synonym search algorithm by using a lexical\ndatabase, such as WordNet [38], or a knowledge graph, such\nas ConceptNet [39], could be used for automatically creating\ndictionaries for the proposed label augmentation.\nThirdly, a remaining difﬁculty in applying this work is to\nknow when to start and when to stop active learning iterations.\nFor example, in our experiments (Sec. V), the size of seed data\nis set to 20% of the training set for the Caffeine Challenge\ndataset while it is set to 10% of the training set for the\nSemEval dataset. It is decided based on heuristics and future\nstudies could investigate the optimal size of the seed data.\nAlso, even though the proposed method achieves fast perfor-\nmance improvements at the beginning, it reaches a plateau\nin the middle of the active learning process. This is because\nwe consider a pool-based active learning scenario, which\nassumes a large amount of unlabelled data at the beginning\nof the process and the active learning iteration ends when\nthere is no more data to be labelled. To avoid unnecessary\niteration steps, a stopping strategy is needed. Potentially,\nstopping strategy can be deﬁned based on the stabilisation\nof predictions [40] or the certainty scores of predictions [41].\nVII. CONCLUSION\nIn this paper, we introduce a new potential application of\nABSA applied to health-related program reviews. To achieve\nthis, we collected a new dataset and developed an ABSA\nsystem. Also, we propose a novel label-efﬁcient training\nscheme to reduce manual labelling efforts. The proposed\nlabel-efﬁcient training scheme consists of the following\nelements: (i) task-speciﬁc pre-training to utilise unlabelled\ntask-speciﬁc corpus data, (ii) label augmentation to exploits\nthe labelled data, and (iii) active learning to strategically\nreduce manual labelling.\nThe effectiveness of the proposed method is examined via\nexperiments with two datasets. We experimentally demon-\nstrated the proposed method shows faster performance\nimprovement and achieves better performances over exist-\ning active learning methods, especially in terms of the end-\nto-end evaluation metrics. More speciﬁcally, experimental\nresults show that the proposed method can reduce manual\nlabelling effort 2-3 times compared to labelling with random\nsampling on both datasets. The proposed method also shows\nbetter performance improvements than the existing state-of-\nthe-art active learning methods. Furthermore, the proposed\nmethod shows better generalisability than other methods\nthanks to the task-speciﬁc pre-training and the proposed label\naugmentation.\nAs future work, we expect to collect actual user data from\na mobile application and implement the developed ABSA\nsystem with the proposed label-efﬁcient training scheme.\nMoreover, we will investigate a stopping strategy to terminate\nthe active learning process to avoid unnecessary iteration\nsteps.\nAPPENDIX A\nEXAMPLES OF THE COLLECTED DATA\nTable 5 shows examples of the collected data used for exper-\niments.\nAPPENDIX B\nEXPLANATION OF ASPECT CATEGORIES\nTable 6 summarises the explanation and examples of aspect\ncategories used in the paper.\nAPPENDIX C\nASPECT CATEGORY DISTRIBUTION OF THE SemEval\nDATASET\nFig. 10 illustrates the aspect category distribution of the train-\ning set from the SemEval dataset used for the experiments.\nAs it is shown in the ﬁgure, the SemEval dataset is imbal-\nanced and we deﬁne {Food, Anecdotes/Miscellaneous} and\n{Service, Ambience, Price} as majority and minority aspect\ncategories, respectively.\nAPPENDIX D\nIMPLEMENTATION AND TRAINING SETTINGS\nAll experiments were performed on the Windows 10 oper-\nating system and the detailed speciﬁcation of hardware\nand software is summarised in Table 7. For model imple-\nmentation, PyTorch version of BERT with the pre-trained\nweights (bert-base-uncased) [42] was used as the\npre-trained model (PT). During task-speciﬁc pre-training,\nVOLUME 9, 2021 115575\nH. Shimet al.: LETS: LETS for ABSA by Using a Pre-Trained Language Model\nTABLE 5. Example of question and answers. This example shows\n12 different responses from a single participant.\nFIGURE 10. Aspect category distribution of the training set from the\nSemEval dataset. Anecd/Misc refers Anecdotes/Miscellaneous aspect\ncategory.\nthe pre-trained model is further trained on the end task cor-\npus. For task-speciﬁc pre-training, we adopt masked lan-\nguage modelling [1] with masking probability p = 0.15.\nTABLE 6. Explanation and examples of aspect categories.\nTABLE 7. Detailed implementation specification.\nTABLE 8. Hyperparameters for task-specific pre-training (top) and\nfine-tuning (bottom).\nDuring task-speciﬁc pre-training, randomly sampled 10% of\ntraining data is used as a validation set for early-stopping.\nFor ﬁne-tuning, 5-fold cross validation splits are created\nby using K-Folds cross-validator function from scikit-learn\nlibrary.6 Also, a ﬁnal dense layer with softmax function is\nadded and cross entropy loss is used. Since the focus of\nthis paper is active learning experiments, we did not conduct\nhyperparameter tuning experiments but used hyperparameter\nvalues based on the recent study [18] as summaries in Table 8.\n6https://scikit-learn.org/stable/modules/generated/sklearn.model_\nselection.KFold.html\n115576 VOLUME 9, 2021\nH. Shimet al.: LETS: LETS for ABSA by Using a Pre-Trained Language Model\nAPPENDIX E\nPRE-DEFINED DICTIONARIES FOR LABEL\nAUGMENTATION\nPre-deﬁned dictionaries were used for label augmentation.\nFor the Caffeine Challenge dataset, the list of minority aspect\ncategories and the list of similar words for each aspect cate-\ngories are deﬁned as:\n• Missing caffeine: [Missing caffeine, Dislike decaffeine,\nNeed caffeine, Caffeine addiction]\n• Difﬁculty level: [Difﬁculty level, Hard to ﬁnish, cannot\ncomplete, Too difﬁcult]\n• Physical withdrawal symptoms: [Physical withdrawal\nsymptoms, Headache, Pain, Jitter]\n• App experience: [App experience, UI, UX, Design]\nFor SemEval dataset, the list of minority aspect categories\nand the list of similar words for each aspect categories are\ndeﬁned as:\n• Service: [Service, Staff] 7\n• Ambience: [Ambience, Atmosphere, Decor]\n• Price: [Price, Bill, Quality 8]\nACKNOWLEDGMENT\nStijn Luca and Bart Vanrumste are co-joint last authors.\nThis article reﬂects only the author’s view and the Research\nExecutive Agency (REA) is not responsible for any use that\nmay be made of the information it contains.\nREFERENCES\n[1] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ in Proc.\nConf. North Amer. Chapter Assoc. Comput. Linguistics, Hum. Lang. Tech-\nnol., vol. 1, Jun. 2019, pp. 4171–4186.\n[2] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, ‘‘Improving lan-\nguage understanding by generative pre-training,’’ OpenAI, San Francisco,\nCA, USA, Tech. Rep., Aug. 2021. [Online]. Available: https://openai.\ncom/blog/language-unsupervised/\n[3] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V . Le,\n‘‘XLNet: Generalized autoregressive pretraining for language understand-\ning,’’ in Proc. Adv. Neural Inf. Process. Syst., 2019, pp. 5753–5763.\n[4] B. Settles, ‘‘Active learning literature survey,’’ Dept. Comput. Sci., Univ.\nWisconsin-Madison, Madison, WI, USA, Tech. Rep. 1648, 2009.\n[5] S. Dasgupta and D. Hsu, ‘‘Hierarchical sampling for active learning,’’ in\nProc. 25th Int. Conf. Mach. Learn. (ICML), 2008, pp. 208–215.\n[6] K. Wang, D. Zhang, Y . Li, R. Zhang, and L. Lin, ‘‘Cost-effective active\nlearning for deep image classiﬁcation,’’ IEEE Trans. Circuits Syst. Video\nTechnol., vol. 27, no. 12, pp. 2591–2600, Dec. 2016.\n[7] Y . Gal, R. Islam, and Z. Ghahramani, ‘‘Deep Bayesian active learning with\nimage data,’’ in Proc. Int. Conf. Mach. Learn., 2017, pp. 1183–1192.\n[8] Y . Shen, H. Yun, Z. Lipton, Y . Kronrod, and A. Anandkumar, ‘‘Deep active\nlearning for named entity recognition,’’ in Proc. 2nd Workshop Represent.\nLearn. (NLP), 2017, pp. 252–256.\n[9] A. Siddhant and Z. C. Lipton, ‘‘Deep Bayesian active learning for natural\nlanguage processing: Results of a large-scale empirical study,’’ in Proc.\nConf. Empirical Methods Natural Lang. Process., 2018, pp. 2904–2909.\n[10] H. Xu, B. Liu, L. Shu, and S. Y . Philip, ‘‘Bert post-training for review read-\ning comprehension and aspect-based sentiment analysis,’’ in Proc. Conf.\nNorth Amer. Chapter Assoc. Comput. Linguistics, Hum. Lang. Technol.,\nvol. 1, 2019, pp. 2324–2335.\n[11] H. H. Dohaiha, P. W. C. Prasad, A. Maag, and A. Alsadoon, ‘‘Deep learning\nfor aspect-based sentiment analysis: A comparative review,’’ Expert Syst.\nAppl., vol. 118, pp. 272–299, Mar. 2019.\n7During experiments, we observed that adding more labels for Service\naspect category harms the performance.\n8Quality is not a similar word for price but it is used because the training\ndata set contains reviews mentioning price-quality relationship.\n[12] S. Ruder, P. Ghaffari, and J. G. Breslin, ‘‘A hierarchical model of reviews\nfor aspect-based sentiment analysis,’’ in Proc. Conf. Empirical Methods\nNatural Lang. Process., 2016, pp. 999–1005.\n[13] Y . Wang, M. Huang, X. Zhu, and L. Zhao, ‘‘Attention-based LSTM for\naspect-level sentiment classiﬁcation,’’ in Proc. Conf. Empirical Methods\nNatural Lang. Process.Austin, TX, USA: Association for Computational\nLinguistics, Nov. 2016, pp. 606–615.\n[14] C. Brun and V . Nikoulina, ‘‘Aspect based sentiment analysis into the wild,’’\nin Proc. 9th Workshop Comput. Approaches Subjectivity, Sentiment Social\nMedia Anal., 2018, pp. 116–122.\n[15] C. Sun, L. Huang, and X. Qiu, ‘‘Utilizing bert for aspect-based sentiment\nanalysis via constructing auxiliary sentence,’’ in Proc. Conf. North Amer.\nChapter Assoc. Comput. Linguistics, Hum. Lang. Technol., vol. 1, 2019,\npp. 380–385.\n[16] M. Pontiki, D. Galanis, J. Pavlopoulos, H. Papageorgiou,\nI. Androutsopoulos, and S. Manandhar, ‘‘SemEval-2014 task 4: Aspect\nbased sentiment analysis,’’ in Proc. 8th Int. Workshop Semantic Eval.\n(SemEval). Dublin, Ireland: Association for Computational Linguistics,\n2014, pp. 27–35.\n[17] W. Xue and T. Li, ‘‘Aspect based sentiment analysis with gated convolu-\ntional networks,’’ in Proc. 56th Annu. Meeting Assoc. Comput. Linguistics\n(ACL), Jul. 2018, pp. 2514–2523.\n[18] C. Sun, X. Qiu, Y . Xu, and X. Huang, ‘‘How to ﬁne-tune bert for text\nclassiﬁcation?’’ in Proc. China Nat. Conf. Chin. Comput. Linguistics.\nCham, Switzerland: Springer, 2019, pp. 194–206.\n[19] S. Gururangan, A. Marasović, S. Swayamdipta, K. Lo, I. Beltagy,\nD. Downey, and N. A. Smith, ‘‘Don’t stop pretraining: Adapt language\nmodels to domains and tasks,’’ in Proc. 58th Annu. Meeting Assoc. Comput.\nLinguistics, 2020, pp. 8342–8360.\n[20] D. D. Lewis and W. A. Gale, ‘‘A sequential algorithm for training text\nclassiﬁers,’’ in Proc. SIGIR. London, U.K.: Springer, 1994, pp. 3–12.\n[21] D. D. Lewis and J. Catlett, ‘‘Heterogeneous uncertainty sampling for super-\nvised learning,’’ in Machine Learning Proceedings 1994. Amsterdam,\nThe Netherlands: Elsevier, 1994, pp. 148–156.\n[22] A. Shelmanov, V . Liventsev, D. Kireev, N. Khromov, A. Panchenko,\nI. Fedulova, and D. V . Dylov, ‘‘Active learning with deep pre-\ntrained models for sequence tagging of clinical and biomedical\ntexts,’’ in Proc. IEEE Int. Conf. Bioinf. Biomed. (BIBM), Nov. 2019,\npp. 482–489.\n[23] L. Ein-Dor, A. Halfon, A. Gera, E. Shnarch, L. Dankin, L. Choshen,\nM. Danilevsky, R. Aharonov, Y . Katz, and N. Slonim, ‘‘Active learning for\nBERT: An empirical study,’’ in Proc. Conf. Empirical Methods Natural\nLang. Process. (EMNLP), 2020, pp. 7949–7962.\n[24] B. Lakshminarayanan, A. Pritzel, and C. Blundell, ‘‘Simple and scalable\npredictive uncertainty estimation using deep ensembles,’’ in Proc. Adv.\nNeural Inf. Process. Syst., 2017, pp. 6402–6413.\n[25] W. H. Beluch, T. Genewein, A. Nürnberger, and J. M. Köhler,\n‘‘The power of ensembles for active learning in image classiﬁcation,’’\nin Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018,\npp. 9368–9377.\n[26] J. Wu, V . S. Sheng, J. Zhang, H. Li, T. Dadakova, C. L. Swisher, Z. Cui, and\nP. Zhao, ‘‘Multi-label active learning algorithms for image classiﬁcation:\nOverview and future promise,’’ ACM Comput. Surveys, vol. 53, no. 2,\npp. 1–35, Jul. 2020.\n[27] M.-F. Balcan, A. Broder, and T. Zhang, ‘‘Margin based active learning,’’ in\nProc. Int. Conf. Comput. Learn. Theory. Berlin, Germany: Springer, 2007,\npp. 35–50.\n[28] J. Gonsior, M. Thiele, and W. Lehner, ‘‘WeakAL: Combining active\nlearning and weak supervision,’’ in Proc. Int. Conf. Discovery Sci.Cham,\nSwitzerland: Springer, 2020, pp. 34–49.\n[29] C. E. Shannon, ‘‘A mathematical theory of communication,’’ Bell Syst.\nTech. J., vol. 27, no. 3, pp. 379–423, Jul./Oct. 1948.\n[30] Y . Gal and Z. Ghahramani, ‘‘Dropout as a Bayesian approximation: Rep-\nresenting model uncertainty in deep learning,’’ in Proc. Int. Conf. Mach.\nLearn., 2016, pp. 1050–1059.\n[31] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov, ‘‘Dropout: A simple way to prevent neural networks\nfrom overﬁtting,’’ J. Mach. Learn. Res., vol. 15, no. 1, pp. 1929–1958,\n2014. [Online]. Available: http://jmlr.org/papers/v15/srivastava14a.html\n[32] N. Houlsby, F. Huszár, Z. Ghahramani, and M. Lengyel, ‘‘Bayesian\nactive learning for classiﬁcation and preference learning,’’ 2011,\narXiv:1112.5745. [Online]. Available: http://arxiv.org/abs/1112.5745\n[33] D. Reker, ‘‘Practical considerations for active machine learning in drug dis-\ncovery,’’Drug Discovery Today, Technol., vol. 32, pp. 73–79, Dec. 2020.\nVOLUME 9, 2021 115577\nH. Shimet al.: LETS: LETS for ABSA by Using a Pre-Trained Language Model\n[34] M. Yuan, H.-T. Lin, and J. Boyd-Graber, ‘‘Cold-start active learning\nthrough self-supervised language modeling,’’ in Proc. Conf. Empirical\nMethods Natural Lang. Process. (EMNLP), 2020, pp. 7935–7948.\n[35] J. Wei and K. Zou, ‘‘EDA: Easy data augmentation techniques for boosting\nperformance on text classiﬁcation tasks,’’ in Proc. Conf. Empirical Meth-\nods Natural Lang. Process., 9th Int. Joint Conf. Natural Lang. Process.\n(EMNLP-IJCNLP). Hong Kong, China: Association for Computational\nLinguistics, Nov. 2019, pp. 6382–6388.\n[36] S. Ertekin, J. Huang, L. Bottou, and L. Giles, ‘‘Learning on the border:\nActive learning in imbalanced data classiﬁcation,’’ in Proc. 16th ACM\nConf. Conf. Inf. Knowl. Manage. (CIKM), 2007, pp. 127–136.\n[37] L. Van der Maaten and G. Hinton, ‘‘Visualizing data using t-SNE,’’\nJ. Mach. Learn. Res., vol. 9, no. 11, pp. 1–27, 2008.\n[38] G. A. Miller, ‘‘WordNet: A lexical database for English,’’ Commun. ACM,\nvol. 38, no. 11, pp. 39–41, 1995.\n[39] R. Speer, J. Chin, and C. Havasi, ‘‘ConceptNet 5.5: An open multilingual\ngraph of general knowledge,’’ in Proc. AAAI Conf. Artif. Intell., 2017,\nvol. 31, no. 1, pp. 1–8.\n[40] M. Bloodgood and K. Vijay-Shanker, ‘‘A method for stopping active\nlearning based on stabilizing predictions and the need for user-adjustable\nstopping,’’ in Proc. 13th Conf. Comput. Natural Lang. Learn. (CoNLL),\n2009, pp. 39–47.\n[41] J. Zhu, H. Wang, E. Hovy, and M. Ma, ‘‘Conﬁdence-based stopping\ncriteria for active learning for data annotation,’’ ACM Trans. Speech Lang.\nProcess., vol. 6, no. 3, pp. 1–24, Apr. 2010.\n[42] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi,\nP. Cistac, T. Rault, R. Louf, M. Funtowicz, and J. Davison, ‘‘Hugging-\nFace’s transformers: State-of-the-art natural language processing,’’ 2019,\narXiv:1910.03771. [Online]. Available: http://arxiv.org/abs/1910.03771\nHEEREEN SHIM received the M.Sc. degree\nin electrical engineering from Chung-Ang\nUniversity, South Korea, in 2018. She is currently\npursuing the Ph.D. degree in engineering technol-\nogy with KU Leuven, Belgium, within a research\nframework HEalth-related Activity Recognition\nsystem based on IoT (HEART) funded by\nEuropean Commission. She has been also working\nat Philips Research, since 2018, where she has\nbeen working on natural language processing for\nhealth-related behaviour analytics based on free-text. Her research interests\ninclude scalable machine learning algorithms and natural language process-\ning for a conversational agent particularly for the healthcare domain.\nDIETWIG LOWET received the M.Sc. degree in\nphysics from the University of Antwerp, in 1996,\nand the P.D.Eng. degree in engineering from the\nComputer Science Faculty, Eindhoven University\nof Technology. He has been working at Philips\nResearch, since 1999, where he has worked on\nembedded software architectures for (connected)\nconsumer devices, data analytics, and insight\ngeneration from personal health data for digital\nlifestyle coaching services. His current work is on\nnatural language processing and understanding to analyze health-related free\ntext input.\nSTIJN LUCAreceived the M.Sc. degree in mathe-\nmatics from KU Leuven, Belgium, in 2003, and the\nPh.D. degree in mathematics from Hasselt Univer-\nsity, Belgium, in 2007. He is currently an Assistant\nProfessor at the Department of Data Analysis and\nMathematical Modelling, Faculty of Bioscience\nEngineering, Ghent University, Belgium. Previ-\nously, he was a Postdoctoral Fellow at KU Leuven\nand a Visiting Scholar at the University of Oxford,\nfrom 2014 to 2015. His research interests include\nthe theory and methods of statistical data analysis and their applications in\nlife sciences.\nBART VANRUMSTE (Senior Member, IEEE)\nreceived the M.Sc. degree in electrical engineer-\ning, the M.Sc. degree in biomedical engineering,\nand the Ph.D. degree in engineering from Ghent\nUniversity, in 1994, 1998, and 2001, respectively.\nHe worked as a Postdoctoral Fellow with the\nElectrical and Computer Engineering Department,\nUniversity of Canterbury, New Zealand, from 2001\nto 2003. From 2003 to 2005, he was a Postdoctoral\nFellow with the STADIUS Division, Department\nof Electrical Engineering (ESAT), KU Leuven. In 2005, he was appointed as\na Faculty Member initially with the University of Applied Sciences Thomas\nMore and since 2013 with the Faculty of Engineering Technology, KU\nLeuven. He currently teaches courses in statistics and machine learning.\nHe is a member of the eMedia Research Laboratory at Group T and the\nSTADIUS Division, Department of Electrical Engineering, KU Leuven. His\nresearch interests include decision support in healthcare in general and ICT\napplications for aging in place in particular. His current research activities\nfocus among others on multimodal sensor integration for monitoring older\npersons and patients with chronic diseases. He is a senior member of IEEE\nEngineering in Medicine and Biology Society and a member of the Interna-\ntional Society for Bioelectromagnetism.\n115578 VOLUME 9, 2021",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8619597554206848
    },
    {
      "name": "Bottleneck",
      "score": 0.7595157623291016
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6970030069351196
    },
    {
      "name": "Task (project management)",
      "score": 0.6657892465591431
    },
    {
      "name": "Machine learning",
      "score": 0.5799494981765747
    },
    {
      "name": "Sentiment analysis",
      "score": 0.5728095769882202
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5545592904090881
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.553892970085144
    },
    {
      "name": "Scheme (mathematics)",
      "score": 0.4561868906021118
    },
    {
      "name": "Labeled data",
      "score": 0.4463278651237488
    },
    {
      "name": "Language model",
      "score": 0.42337584495544434
    },
    {
      "name": "Process (computing)",
      "score": 0.41438430547714233
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Embedded system",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ]
}