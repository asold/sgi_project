{
    "title": "Multimodal Learning With Transformers: A Survey",
    "url": "https://openalex.org/W4376226279",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A1982578470",
            "name": "Peng Xu",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2116756113",
            "name": "Xiatian Zhu",
            "affiliations": [
                "University of Surrey"
            ]
        },
        {
            "id": "https://openalex.org/A2057169223",
            "name": "David A Clifton",
            "affiliations": [
                "University of Oxford"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6801693513",
        "https://openalex.org/W3034500398",
        "https://openalex.org/W6810263219",
        "https://openalex.org/W3209274285",
        "https://openalex.org/W6809938868",
        "https://openalex.org/W6790978476",
        "https://openalex.org/W6811387395",
        "https://openalex.org/W6792983607",
        "https://openalex.org/W6799166919",
        "https://openalex.org/W3184784418",
        "https://openalex.org/W6804036380",
        "https://openalex.org/W6801304075",
        "https://openalex.org/W6810593243",
        "https://openalex.org/W3202009528",
        "https://openalex.org/W3209859545",
        "https://openalex.org/W6620707391",
        "https://openalex.org/W6803953248",
        "https://openalex.org/W6779473860",
        "https://openalex.org/W6810814029",
        "https://openalex.org/W6788556936",
        "https://openalex.org/W3138794547",
        "https://openalex.org/W3035029089",
        "https://openalex.org/W6780924892",
        "https://openalex.org/W3125488228",
        "https://openalex.org/W4312384316",
        "https://openalex.org/W6796707148",
        "https://openalex.org/W4312707458",
        "https://openalex.org/W2965570621",
        "https://openalex.org/W3163035491",
        "https://openalex.org/W6787566904",
        "https://openalex.org/W4312709167",
        "https://openalex.org/W3175199633",
        "https://openalex.org/W3172863135",
        "https://openalex.org/W6783227185",
        "https://openalex.org/W3165938948",
        "https://openalex.org/W6776048684",
        "https://openalex.org/W4313142416",
        "https://openalex.org/W6783267081",
        "https://openalex.org/W6804171790",
        "https://openalex.org/W2981851019",
        "https://openalex.org/W6791353385",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W6790830454",
        "https://openalex.org/W6778485988",
        "https://openalex.org/W6784333009",
        "https://openalex.org/W6800217721",
        "https://openalex.org/W6804999546",
        "https://openalex.org/W6810613308",
        "https://openalex.org/W3182937942",
        "https://openalex.org/W2981920153",
        "https://openalex.org/W4312962707",
        "https://openalex.org/W2963307918",
        "https://openalex.org/W4312791030",
        "https://openalex.org/W4312044727",
        "https://openalex.org/W6810837128",
        "https://openalex.org/W3048306538",
        "https://openalex.org/W6810708803",
        "https://openalex.org/W3036435328",
        "https://openalex.org/W6839196789",
        "https://openalex.org/W3203949114",
        "https://openalex.org/W3197394194",
        "https://openalex.org/W4214635352",
        "https://openalex.org/W4230405732",
        "https://openalex.org/W6796956318",
        "https://openalex.org/W3184369217",
        "https://openalex.org/W3172845486",
        "https://openalex.org/W3177487519",
        "https://openalex.org/W6804781843",
        "https://openalex.org/W4286611322",
        "https://openalex.org/W3176481196",
        "https://openalex.org/W6811013733",
        "https://openalex.org/W3173220247",
        "https://openalex.org/W6803081274",
        "https://openalex.org/W3180355996",
        "https://openalex.org/W3137684688",
        "https://openalex.org/W6810356902",
        "https://openalex.org/W6800285227",
        "https://openalex.org/W4312563197",
        "https://openalex.org/W3178679506",
        "https://openalex.org/W3173290664",
        "https://openalex.org/W6802115918",
        "https://openalex.org/W3150074127",
        "https://openalex.org/W6783838515",
        "https://openalex.org/W3161801106",
        "https://openalex.org/W6804198819",
        "https://openalex.org/W3175126073",
        "https://openalex.org/W3159481202",
        "https://openalex.org/W2277195237",
        "https://openalex.org/W2002591263",
        "https://openalex.org/W3145450063",
        "https://openalex.org/W6676497082",
        "https://openalex.org/W2962960500",
        "https://openalex.org/W6795276571",
        "https://openalex.org/W6639102338",
        "https://openalex.org/W3176362845",
        "https://openalex.org/W6796761347",
        "https://openalex.org/W1933349210",
        "https://openalex.org/W3170767867",
        "https://openalex.org/W6788135285",
        "https://openalex.org/W3172514680",
        "https://openalex.org/W4212841753",
        "https://openalex.org/W3171125843",
        "https://openalex.org/W2890952074",
        "https://openalex.org/W3034298080",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W6789155089",
        "https://openalex.org/W6784761581",
        "https://openalex.org/W6787826751",
        "https://openalex.org/W6795711426",
        "https://openalex.org/W6784184991",
        "https://openalex.org/W4214601882",
        "https://openalex.org/W3171774521",
        "https://openalex.org/W3095545636",
        "https://openalex.org/W6779879114",
        "https://openalex.org/W6763701032",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W6790904380",
        "https://openalex.org/W6842585177",
        "https://openalex.org/W3202384916",
        "https://openalex.org/W4312523916",
        "https://openalex.org/W6842542540",
        "https://openalex.org/W3176641147",
        "https://openalex.org/W4221166385",
        "https://openalex.org/W2963916161",
        "https://openalex.org/W6803596987",
        "https://openalex.org/W6769311223",
        "https://openalex.org/W4249013746",
        "https://openalex.org/W6798195429",
        "https://openalex.org/W6797716411",
        "https://openalex.org/W4214604251",
        "https://openalex.org/W6797148833",
        "https://openalex.org/W6840693802",
        "https://openalex.org/W6785011006",
        "https://openalex.org/W6797265648",
        "https://openalex.org/W6810334672",
        "https://openalex.org/W6804812009",
        "https://openalex.org/W6805750369",
        "https://openalex.org/W4312884055",
        "https://openalex.org/W6802987763",
        "https://openalex.org/W6793759846",
        "https://openalex.org/W3197457832",
        "https://openalex.org/W3168640669",
        "https://openalex.org/W2984008963",
        "https://openalex.org/W3204670646",
        "https://openalex.org/W3128510757",
        "https://openalex.org/W3203711169",
        "https://openalex.org/W3017637887",
        "https://openalex.org/W2998356391",
        "https://openalex.org/W6758859460",
        "https://openalex.org/W4210867585",
        "https://openalex.org/W2970869018",
        "https://openalex.org/W3090449556",
        "https://openalex.org/W3176175717",
        "https://openalex.org/W6767853649",
        "https://openalex.org/W3193902142",
        "https://openalex.org/W3205276578",
        "https://openalex.org/W6767279747",
        "https://openalex.org/W3204966934",
        "https://openalex.org/W6803519607",
        "https://openalex.org/W6767211374",
        "https://openalex.org/W3201861986",
        "https://openalex.org/W3167917117",
        "https://openalex.org/W6766904570",
        "https://openalex.org/W3204270866",
        "https://openalex.org/W3176232375",
        "https://openalex.org/W2970231061",
        "https://openalex.org/W3034538190",
        "https://openalex.org/W4290927857",
        "https://openalex.org/W3034727271",
        "https://openalex.org/W4214663214",
        "https://openalex.org/W3203209821",
        "https://openalex.org/W3091588028",
        "https://openalex.org/W2986670728",
        "https://openalex.org/W3172675210",
        "https://openalex.org/W6804001748",
        "https://openalex.org/W3176974620",
        "https://openalex.org/W2997591391",
        "https://openalex.org/W3174257385",
        "https://openalex.org/W6797109355",
        "https://openalex.org/W6790019176",
        "https://openalex.org/W3169801598",
        "https://openalex.org/W6864544085",
        "https://openalex.org/W2903739847",
        "https://openalex.org/W3176724088",
        "https://openalex.org/W6810596876",
        "https://openalex.org/W6796242362",
        "https://openalex.org/W3008115128",
        "https://openalex.org/W6773248631",
        "https://openalex.org/W3174377922",
        "https://openalex.org/W3105232955",
        "https://openalex.org/W3174012740",
        "https://openalex.org/W6803123405",
        "https://openalex.org/W6775188310",
        "https://openalex.org/W3035284526",
        "https://openalex.org/W3169575318",
        "https://openalex.org/W3035265375",
        "https://openalex.org/W3171353004",
        "https://openalex.org/W2151194828",
        "https://openalex.org/W6811072154",
        "https://openalex.org/W3134294468",
        "https://openalex.org/W2886641317",
        "https://openalex.org/W3186479087",
        "https://openalex.org/W2151096985",
        "https://openalex.org/W6810124100",
        "https://openalex.org/W6779661269",
        "https://openalex.org/W2946165673",
        "https://openalex.org/W6798805250",
        "https://openalex.org/W3198299822",
        "https://openalex.org/W6802130436",
        "https://openalex.org/W2963351113",
        "https://openalex.org/W3113709932",
        "https://openalex.org/W6769642167",
        "https://openalex.org/W4313164293",
        "https://openalex.org/W6811554624",
        "https://openalex.org/W6803307422",
        "https://openalex.org/W6795807602",
        "https://openalex.org/W4312372834",
        "https://openalex.org/W6810168380",
        "https://openalex.org/W3106784008",
        "https://openalex.org/W6797613833",
        "https://openalex.org/W6765843572",
        "https://openalex.org/W6803675045",
        "https://openalex.org/W6810447287",
        "https://openalex.org/W6804185262",
        "https://openalex.org/W3170068246",
        "https://openalex.org/W6793736971",
        "https://openalex.org/W4214648142",
        "https://openalex.org/W6682889407",
        "https://openalex.org/W3034381157",
        "https://openalex.org/W6755977528",
        "https://openalex.org/W6787133006",
        "https://openalex.org/W3160310387",
        "https://openalex.org/W6789693907",
        "https://openalex.org/W3174517569",
        "https://openalex.org/W6779590353",
        "https://openalex.org/W2964051877",
        "https://openalex.org/W6747225971",
        "https://openalex.org/W2963155035",
        "https://openalex.org/W3160137267",
        "https://openalex.org/W3203898052",
        "https://openalex.org/W6795975431",
        "https://openalex.org/W6803840103",
        "https://openalex.org/W6797781246",
        "https://openalex.org/W6802517928",
        "https://openalex.org/W3203247393",
        "https://openalex.org/W3204221554",
        "https://openalex.org/W3185341429",
        "https://openalex.org/W3173683732",
        "https://openalex.org/W2963499153",
        "https://openalex.org/W2952132648",
        "https://openalex.org/W6805650942",
        "https://openalex.org/W2964040984",
        "https://openalex.org/W3177654849",
        "https://openalex.org/W3173528751",
        "https://openalex.org/W3034266838",
        "https://openalex.org/W6776929863",
        "https://openalex.org/W4225744324",
        "https://openalex.org/W3176431316",
        "https://openalex.org/W6796481600",
        "https://openalex.org/W4214562507",
        "https://openalex.org/W3167814535",
        "https://openalex.org/W3104152799",
        "https://openalex.org/W3195026654",
        "https://openalex.org/W3176824248",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W6794315554",
        "https://openalex.org/W3171668871",
        "https://openalex.org/W6787995345",
        "https://openalex.org/W3201703290",
        "https://openalex.org/W6787473416",
        "https://openalex.org/W6791108739",
        "https://openalex.org/W6798335185",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W4312658081",
        "https://openalex.org/W3111535274",
        "https://openalex.org/W6838322825",
        "https://openalex.org/W4214736485",
        "https://openalex.org/W6761628794",
        "https://openalex.org/W6772381481",
        "https://openalex.org/W6794783755",
        "https://openalex.org/W3150584131",
        "https://openalex.org/W6796414051",
        "https://openalex.org/W6638667902",
        "https://openalex.org/W3175836671",
        "https://openalex.org/W6780226713",
        "https://openalex.org/W6803728867",
        "https://openalex.org/W3184679245",
        "https://openalex.org/W6845935626",
        "https://openalex.org/W3213191779",
        "https://openalex.org/W2985076077",
        "https://openalex.org/W4312407537",
        "https://openalex.org/W3034556939",
        "https://openalex.org/W6780218876",
        "https://openalex.org/W3196936439",
        "https://openalex.org/W6785310428",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2619383789",
        "https://openalex.org/W3175095612",
        "https://openalex.org/W6789753369",
        "https://openalex.org/W3116651605",
        "https://openalex.org/W6800139874",
        "https://openalex.org/W3035635319",
        "https://openalex.org/W4312271977",
        "https://openalex.org/W6839745749",
        "https://openalex.org/W6788436139",
        "https://openalex.org/W4312877428",
        "https://openalex.org/W4312956471",
        "https://openalex.org/W3113067643",
        "https://openalex.org/W3213301750",
        "https://openalex.org/W3114896399",
        "https://openalex.org/W3133825286",
        "https://openalex.org/W3168527213",
        "https://openalex.org/W4281633937",
        "https://openalex.org/W3184735396",
        "https://openalex.org/W3124424060",
        "https://openalex.org/W3035486405",
        "https://openalex.org/W3166893724",
        "https://openalex.org/W3170072081",
        "https://openalex.org/W3202536355",
        "https://openalex.org/W4292945941",
        "https://openalex.org/W4210352519",
        "https://openalex.org/W4225832925",
        "https://openalex.org/W4313186260",
        "https://openalex.org/W3187418919",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W4297816851",
        "https://openalex.org/W3152798676",
        "https://openalex.org/W3113320078",
        "https://openalex.org/W3181262653",
        "https://openalex.org/W3122640483",
        "https://openalex.org/W4296406182",
        "https://openalex.org/W3014611590",
        "https://openalex.org/W3034772996",
        "https://openalex.org/W3178457761",
        "https://openalex.org/W2975357369",
        "https://openalex.org/W2969876226",
        "https://openalex.org/W3166163487",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4226182655",
        "https://openalex.org/W3113177135",
        "https://openalex.org/W3209532394",
        "https://openalex.org/W4287608901",
        "https://openalex.org/W639708223",
        "https://openalex.org/W4287117470",
        "https://openalex.org/W3209478434",
        "https://openalex.org/W4221166856",
        "https://openalex.org/W4200634001",
        "https://openalex.org/W4226177592",
        "https://openalex.org/W3207750165",
        "https://openalex.org/W4288281388",
        "https://openalex.org/W4312629998",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W4200630629",
        "https://openalex.org/W4287125738",
        "https://openalex.org/W3213018012",
        "https://openalex.org/W3129576130",
        "https://openalex.org/W3157286395",
        "https://openalex.org/W3126792443",
        "https://openalex.org/W4287022992",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W3213712995",
        "https://openalex.org/W1836465849",
        "https://openalex.org/W3169545043",
        "https://openalex.org/W4281758439",
        "https://openalex.org/W3212610063",
        "https://openalex.org/W4221162878",
        "https://openalex.org/W2968124245",
        "https://openalex.org/W4394659899",
        "https://openalex.org/W4323654151",
        "https://openalex.org/W3200411758",
        "https://openalex.org/W3212177353",
        "https://openalex.org/W4206706211",
        "https://openalex.org/W3116952214",
        "https://openalex.org/W2773514261",
        "https://openalex.org/W3001555892",
        "https://openalex.org/W3126337491",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W4221167444",
        "https://openalex.org/W3165647589",
        "https://openalex.org/W2966715458",
        "https://openalex.org/W4319777935",
        "https://openalex.org/W4287107550",
        "https://openalex.org/W4221143402",
        "https://openalex.org/W4225323055",
        "https://openalex.org/W3036601975",
        "https://openalex.org/W3211965499",
        "https://openalex.org/W4221153068",
        "https://openalex.org/W3193402170",
        "https://openalex.org/W4287163709",
        "https://openalex.org/W3093933225",
        "https://openalex.org/W3153465022",
        "https://openalex.org/W3154596443",
        "https://openalex.org/W3043840704",
        "https://openalex.org/W2109586012",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W4362603432",
        "https://openalex.org/W3035688398",
        "https://openalex.org/W3206072662",
        "https://openalex.org/W3212989890",
        "https://openalex.org/W4287113019",
        "https://openalex.org/W4229042118",
        "https://openalex.org/W3204138855",
        "https://openalex.org/W3132607382",
        "https://openalex.org/W4226538672",
        "https://openalex.org/W3217340782",
        "https://openalex.org/W3161120562",
        "https://openalex.org/W3176799298",
        "https://openalex.org/W2113870955",
        "https://openalex.org/W4286893447",
        "https://openalex.org/W3215899623",
        "https://openalex.org/W4226213156",
        "https://openalex.org/W4287077228",
        "https://openalex.org/W4226423782",
        "https://openalex.org/W3214328324",
        "https://openalex.org/W3144384481",
        "https://openalex.org/W3034445277",
        "https://openalex.org/W4287667694",
        "https://openalex.org/W3158375352",
        "https://openalex.org/W3208581766",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3094751268",
        "https://openalex.org/W3170863103",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W3034835156",
        "https://openalex.org/W4312922092",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4283821388",
        "https://openalex.org/W4312957757",
        "https://openalex.org/W3169064633",
        "https://openalex.org/W3201927751",
        "https://openalex.org/W4282919422",
        "https://openalex.org/W3119507053",
        "https://openalex.org/W3035652667",
        "https://openalex.org/W2156387975",
        "https://openalex.org/W4287214436",
        "https://openalex.org/W2914587137",
        "https://openalex.org/W3212456749",
        "https://openalex.org/W3176851559",
        "https://openalex.org/W3174906557",
        "https://openalex.org/W3213604094",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W2171033594"
    ],
    "abstract": "Transformer is a promising neural network learner, and has achieved great success in various machine learning tasks. Thanks to the recent prevalence of multimodal applications and Big Data, Transformer-based multimodal learning has become a hot topic in AI research. This paper presents a comprehensive survey of Transformer techniques oriented at multimodal data. The main contents of this survey include: (1) a background of multimodal learning, Transformer ecosystem, and the multimodal Big Data era, (2) a systematic review of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective, (3) a review of multimodal Transformer applications, via two important paradigms, i.e., for multimodal pretraining and for specific multimodal tasks, (4) a summary of the common challenges and designs shared by the multimodal Transformer models and applications, and (5) a discussion of open problems and potential research directions for the community.",
    "full_text": "1\nMultimodal Learning with Transformers:\nA Survey\nPeng Xu, Xiatian Zhu, and David A. Clifton\nAbstract—Transformer is a promising neural network learner, and has achieved great success in various machine learning tasks.\nThanks to the recent prevalence of multimodal applications and big data, Transformer-based multimodal learning has become a hot\ntopic in AI research. This paper presents a comprehensive survey of Transformer techniques oriented at multimodal data. The main\ncontents of this survey include: (1) a background of multimodal learning, Transformer ecosystem, and the multimodal big data era, (2) a\nsystematic review of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological\nperspective, (3) a review of multimodal Transformer applications, via two important paradigms,i.e., for multimodal pretraining and for\nspecific multimodal tasks, (4) a summary of the common challenges and designs shared by the multimodal Transformer models and\napplications, and (5) a discussion of open problems and potential research directions for the community.\nIndex Terms—Multimodal Learning, Transformer, Introductory, Taxonomy, Deep Learning, Machine Learning.\n✦\n1 I NTRODUCTION\nThe initial inspiration of Artificial Intelligence (AI) is to\nimitate human perception, e.g., seeing, hearing, touching,\nsmelling. In general, a modality is often associated with a\nspecific sensor that creates a unique communication chan-\nnel, such as vision and language [1]. In humans, a funda-\nmental mechanism in our sensory perception is the ability to\nleverage multiple modalities of perception data collectively\nin order to engage ourselves properly with the world under\ndynamic unconstrained circumstances, with each modality\nserving as a distinct information source characterized by\ndifferent statistical properties. For example, an image gives\nthe visual appearance of an “elephants playing in water”\nscene via thousands of pixels, whilst the corresponding text\ndescribes this moment with a sentence using discrete words.\nFundamentally, a multimodal AI system needs to ingest, in-\nterpret, and reason about multimodal information sources to\nrealize similar human level perception abilities. Multimodal\nlearning (MML) is a general approach to building AI models\nthat can extract and relate information from multimodal\ndata [1].\nThis survey focuses on multimodal learning with Trans-\nformers [2] (as demonstrated in Figure 1), inspired by their\nintrinsic advantages and scalability in modelling different\nmodalities (e.g., language, visual, auditory) and tasks ( e.g.,\nlanguage translation, image recognition, speech recognition)\nwith fewer modality-specific architectural assumptions (e.g.,\ntranslation invariance and local grid attention bias in vision)\n[3]. Concretely, the input to a Transformer could encompass\none or multiple sequences of tokens, and each sequence’s\nattribute (e.g., the modality label, the sequential order), nat-\nurally allowing for MML without architectural modification\n[4]. Further, learning per-modal specificity and inter-modal\ncorrelation can be simply realized by controlling the input\npattern of self-attention. Critically, there is a recent surge of\n• Peng Xu and David A. Clifton are with the University of Oxford, UK.\nXiatian Zhu is with the University of Surrey, UK.\nresearch attempts and activities across distinct disciplines\nexploring the Transformer architectures, resulting in a large\nnumber of novel MML methods being developed in re-\ncent years, along with significant and diverse advances in\nvarious areas [4], [5], [6], [7], [8]. This calls for a timely\nreview and summary of representative methods to enable\nresearchers to understand the global picture of the MML\nfield across related disciplines and more importantly to\ncapture a holistic structured picture of current achievements\nas well as major challenges.\nTaxonomy For better readability and reachability from and\nacross different disciplines, we adopt a two-tier structured\ntaxonomy based on the application and challenge dimen-\nsions respectively. This has several benefits: (1) Researchers\nwith expertise in specific applications can find those ap-\nplications appropriate to their own research domain before\nconnecting to other related domains. (2) Similar model de-\nsigns and architectures developed in different domains can\nbe summarized in an abstract, formula-driven perspective\nso that the mathematical ideas of various models formed\nin different applications can be correlated and contrasted\non common ground, crossing domain-specific restrictions.\nCrucially, our taxonomy offers an interesting stereo-view\nof individual works with the insights in both application\nspecificity and formulation generality. It is hoped that this\ncan help to break down domain boundaries and foster\nmore effective idea communication and exchange across\nmodalities. By using the prompt modelling strategy [9], [10]\nas a basis for investigation, we also include the classical\nclassification problem ( e.g., image classification) – usually\nregarded as a single modality learning application in con-\nventional MML surveys [1], [11], [12] – as a special MML\napplication. This has the potential to significantly enrich\nMML, as the classification problem is an AI topic amongst\nthe most extensive studies in the literature [13].\nScope This survey will discuss the multimodality spe-\ncific designs of Transformer architecture including, but\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2023.3275156\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2\nFig. 1. Overview of Transformer [2].\nnot limited to, the following modalities: RGB image\n[5], depth image [14], multispectral image [15], video\n[7], audio/speech/music [14], [16], [17], table [18], scene\ngraph/layout [19], [20], [21], [22], pose skeleton [23], SQL\n[24], [25], recipe [26], programming language [27], sign lan-\nguage [28], [29], [30], point cloud [31], symbolic knowledge\n(graph) [32], [33], multimodal knowledge graph [34], sketch\ndrawing [35], [36], [37], [38], 3D object/scene [39], [40], [41],\ndocument [42], [43], programming code [44] and Abstract\nSyntax Tree (AST) – a kind of graph [45], optical flow\n[46], medical knowledge (e.g., diagnosis code ontology [47]).\nNote that this survey will not discuss the multimodal papers\nwhere Transformer is used simply as the feature extractor\nwithout multimodal designs.\nRelated SurveysWe relate this paper to existing surveys of\nthe two specific dimensions MML and Transformers. There\nexist a few MML surveys [1], [11], [12]. In particular, [1] pro-\nposed a structured, acknowledged taxonomy by five chal-\nlenges, which we also adopt as part of our structure. Unlike\n[1], [11], and [12], which review general machine learning\nmodels, we instead focus on Transformer architectures and\ntheir self-attention mechanisms. Several surveys dedicated\nto Transformers have been recently introduced, with a range\nof emphases including general Transformers [48], efficient\ndesigns [49], visualization [50], computer vision tasks [51],\n[52], [53], [54], medical imaging [55], video tasks [56], and\nvision language pretraining [57]. While [51], [53], [54], [55]\nconsider MML, their reviews are somewhat limited in the\nscope, taxonomy, and coverage. To our knowledge, only\na few surveys on video-language pretraining (VLP) [57],\n[58], [59] are relevant to MML. However, VLP is only a\nsubdomain of MML. In this survey, we focus solely on the\nintersection of multimodal learning and Transformers.\nFeatures To our knowledge, this paper is the first com-\nprehensive review of the state of Transformer based mul-\ntimodal machine learning. The major features of this survey\ninclude\n(1) We highlight that Transformers have the advantage\nthat they can work in a modality-agnostic way. Thus, they\nare compatible with various modalities (and combinations\nof modalities). To support this view, we, for the first time,\noffer an understanding of the intrinsic traits of Transformers\nin a multimodal context from a geometrically topological\nperspective. We suggest that self-attention be treated as a\ngraph style modelling, which models the input sequence\n(both uni-modal and multimodal) as a fully-connected\ngraph. Specifically, self-attention models the embedding of\narbitrary tokens from an arbitrary modality as a graph node.\n(2) We discuss the key components of Transformers in a\nmultimodal context as mathematically as possible.\n(3) Based on Transformers, cross-modal interactions (e.g.,\nfusion, alignment) are essentially processed by self-attention\nand its variants. In this paper, we extract the mathematical\nessence and formulations of Transformer based MML prac-\ntices, from the perspective of self-attention designs.\nContributions Having presented our review of the land-\nscape of multimodal learning, Transformer ecosystem, and\nmultimodal big data era in Section 2, we summarize our\nmain contributions as the follows.\n(1) In Section 3, we present a systematic reviewing of\nVanilla Transformer, Vision Transformer, and multimodal\nTransformers, from a geometrically topological perspective.\n(2) We contribute a taxonomy for Transformer based\nMML from two complementary perspectives, i.e., applica-\ntion based and challenge based. In Section 4, we provide\na review of multimodal Transformer applications, via two\nimportant paradigms, i.e., for multimodal pretraining and\nfor specific multimodal tasks. In Section 5, we summarize\nthe common challenges and designs shared by the various\nmultimodal Transformer models and applications.\n(3) In Section 6, we discuss current bottlenecks, existing\nproblems, and potential research directions for Transformer\nbased MML.\n2 B ACKGROUND\n2.1 Multimodal Learning (MML)\nMML [1], [60], [61] has been an important research area in\nrecent decades; an early multimodal application – audio-\nvisual speech recognition was studied in 1980s [62]. MML\nis key to human societies. The world we humans live in\nis a multimodal environment, thus both our observations\nand behaviours are multimodal [63]. For instance, an AI\nnavigation robot needs multimodal sensors to perceive the\nreal-world environment [64], [65], [66], e.g., camera, LiDAR,\nradar, ultrasonic, GNSS, HD Map, odometer. Furthermore,\nhuman behaviours, emotions, events, actions, and humour\nare multimodal, thus various human-centred MML tasks\nare widely studied, including multimodal emotion recogni-\ntion [67], multimodal event representation [68], understand-\ning multimodal humor [69], face-body-voice based video\nperson-clustering [70], etc.\nThanks to the development of the internet and a wide\nvariety of intelligent devices in recent years, increasing\namounts of multimodal data are being transmitted over\nthe internet, thus an increasing number of multimodal ap-\nplication scenarios are emerging. In modern life, we can\nsee various multimodal applications, including commercial\nservices (e.g., e-commerce/commodity retrieval [71], vision-\nand-language navigation (VLN) [72], [73], [74], [75], [76]),\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2023.3275156\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n3\ncommunication (e.g., lip reading [77], sign language transla-\ntion [28], [29]), human-computer interaction [78], healthcare\nAI [79], [80], surveillance AI [81], etc.\nMoreover, in the era of Deep Learning, deep neural\nnetworks greatly promote the development of MML, and\nTransformers [2] are a highly competitive architecture fam-\nily, bringing new challenges and opportunities to MML. In\nparticular, the recent success of large language models and\ntheir multimodal derivatives [82], [83], [84], [85], [86] further\ndemonstrates the potential of Transformers in multimodal\nfoundation models.\n2.2 Transformers: a Brief History and Milestones\nTransformers are emerging as promising learners. Benefit\nfrom its self-attention, Vanilla Transformer [2] benefits from\na self-attention mechanism, and is a breakthrough model\nfor sequence-specific representation learning that was orig-\ninally proposed for NLP , achieving the state-of-the-art on\nvarious NLP tasks. Following the great success of Vanilla\nTransformer, a lot of derivative models have been pro-\nposed, e.g., BERT [4], BART [87], GPT [88], Longformer [43],\nTransformer-XL [89], XLNet [90].\nTransformers currently stand at the dominant position\nin NLP domains, and this motivates researchers try to apply\nTransformers to other modalities, such as visual domains. In\nearly attempts for visual domain, the general pipeline of the\nearly explorations is “CNN features + standard Transformer\nencoder”, and researchers achieved BERT-style pretraining,\nvia preprocessing raw images by resizing to a low resolution\nand reshaping into a 1D sequence [91].\nVision Transformer (ViT) [5] is a seminal work that\ncontributes an end-to-end solution by applying the encoder\nof Transformer to images. Both ViT and its variants have\nbeen widely applied to various computer vision tasks,\nincluding low-level tasks [92], recognition [93], detection\n[94], segmentation [95], etc, and also work well for both\nsupervised [93] and self-supervised [96], [97], [98] visual\nlearning. Moreover, some recently-released works provide\nfurther theoretical understanding for ViT, e.g., its internal\nrepresentation robustness [99], the continuous behaviour of\nits latent representation propagation [100], [101].\nMotivated by the great success of Transformer and ViT,\nVideoBERT [7] is a breakthrough work that is the first work\nto extend Transformer to the multimodal tasks. VideoBERT\ndemonstrates the great potential of Transformer in multi-\nmodal context. Following VideoBERT, a lot of Transformer\nbased multimodal pretraining models ( e.g., ViLBERT [102],\nLXMERT [103], VisualBERT [104], VL-BERT [105], UNITER\n[106], CBT [107], Unicoder-VL [108], B2T2 [109], VLP [110],\n12-in-1 [111], Oscar [112], Pixel-BERT [113], ActBERT [114],\nImageBERT [115], HERO [116], UniVL [117]) have become\nresearch topics of increasing interest in the field of machine\nlearning.\nIn 2021, CLIP [9] was proposed. It is a new milestone\nthat uses multimodal pretraining to convert classification as\na retrieval task that enables the pretrained models to tackle\nzero-shot recognition. Thus, CLIP is a successful practice\nthat makes full use of large-scale multimodal pretraining\nto enable zero-shot learning. Recently, the idea of CLIP is\nfurther studied, e.g., CLIP pretrained model based zero-shot\nsemantic segmentation [118], ALIGN [119], CLIP-TD [120],\nALBEF [121], and CoCa [122].\n2.3 Multimodal Big Data\nIn the past decade, with the rapid development of internet\napplications such as social media and online retail, massive\nmultimodal datasets have been proposed, e.g., Conceptual\nCaptions [123], COCO [124], VQA [125], Visual Genome\n[126], SBU Captions [127], Cooking312K [7], LAIT [115], e-\nSNLI-VE [128], ARCH [129], Adversarial VQA [130], OTT-\nQA [18], MULTIMODALQA (MMQA) [131], VALUE [132],\nFashion IQ [133], LRS2-BBC [134], ActivityNet [135], VisDial\n[136].\nSome emergent new trends among the recently released\nmultimodal datasets are:\n(1) Data scales are larger. Various recently released\ndatasets are million-scale, e.g., Product1M [137], Conceptual\n12M [138], RUC-CAS-WenLan [139] (30M), HowToVQA69M\n[140], HowTo100M [141], ALT200M [142], LAION-400M\n[143].\n(2) More modalities. In addition to the general modal-\nities of vision, text, and audio, further diverse modalities\nare emerging, e.g., Pano-AVQA [144] – the first large-scale\nspatial and audio-visual question answering dataset on360◦\nvideos, YouTube-360 (YT-360) [145] ( 360◦ videos), AIST++\n[146] (a new multimodal dataset of 3D dance motion and\nmusic), Artemis [147] (affective language for visual arts). In\nparticular, MultiBench [148] provides a dataset including 10\nmodalities.\n(3) More scenarios. In addition to common caption and\nQA datasets, more applications and scenarios have been\nstudied, e.g., CIRR [149] (real-life images), Product1M [137],\nBed and Breakfast (BnB) [150] (vision-and-language navi-\ngation), M3A [151] (financial dataset), X-World [152] (au-\ntonomous drive).\n(4) Tasks are more difficult. Beyond the straightforward\ntasks, more abstract multimodal tasks are proposed, e.g.,\nMultiMET [153] (a multimodal dataset for metaphor under-\nstanding), Hateful Memes [154] (hate speech in multimodal\nmemes).\n(5) Instructional videos have become increasingly popu-\nlar, e.g., cooking video YouCookII [155]. Aligning a sequence\nof instructions to a video of someone carrying out a task is\nan example of a powerful pretraining pretext task [7], [156].\nPretext tasks are pre-designed problems to force the models\nto learn representation by solving them.\nSimilar to other deep neural network architectures,\nTransformers are also data hungry. Therefore, their high-\ncapacity models and multimodal big data basis co-create the\nprosperity of the Transformer based multimodal machine\nlearning. For instance, big data bring zero-shot learning\ncapability to VLP Transformer models.\n3 T RANSFORMERS\nIn this section, we use mathematical formulations to re-\nview the key techniques of Vanilla Transformer [2], Vision\nTransformer [5], and multimodal Transformers 1, including\n1. In this survey, “multimodal Transformer” means “Transformer in\nmultimodal learning context”.\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2023.3275156\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n4\ntokenized inputs, self-attention, multi-head attention, basic\nTransformer layers/blocks, etc. We highlight that Vanilla\nTransformers can be understood from a geometrically topo-\nlogical perspective [157], because due to the self-attention\nmechanism, given each tokenized input from any modal-\nities, Vanilla self-attention (Transformer) can model it as a\nfully-connected graph in topological geometry space [158].\nCompared with other deep networks (for instance, CNN is\nrestricted in the aligned grid spaces/matrices), Transformers\nintrinsically have a more general and flexible modelling\nspace. This is a notable advantage of Transformers for\nmultimodal tasks. Sections 3.1, 3.2, and 3.3 will review the\nkey designs of Vanilla Transformer, Vision Transformer, and\nmultimodal Transformers, respectively.\n3.1 Vanilla Transformer\nVanilla Transformer has an encoder-decoder structure and is\nthe origin of the Transformer-based research field. It takes\ntokenized input (see Section 3.1.1). Both its encoder and\ndecoder are stacked by the Transformer layers/blocks, as\ndemonstrated in Figure 1. Each block has two sub-layers,i.e.,\na multi-head self-attention (MHSA) layer (see Section 3.1.2)\nand a position-wise fully-connected feed-forward network\n(FFN) (see Section 3.1.3). To help the back propagation of\nthe gradient, both MHSA and FFN use Residual Connection\n[159] (given an input x, the residual connection of any\nmapping f(·) is defined as x ← f(x) +x), followed by\nnormalization layer. Thus, assuming that the input tensor\nis Z, the output of MHSA and FFN sub-layers can be\nformulated as:\nZ ← N(sublayer(Z) +Z), (1)\nwhere sublayer(·) is the mapping implemented by the sub-\nlayer itself and N(·) denotes normalization, e.g., BN (·)\n[160], LN(·) [161].\nDiscussion There is an important unsolved problem that is\npost-normalization versus pre-normalization. The original\nVanilla Transformer uses post-normalization for each MHSA\nand FFN sub-layer. However, if we consider this from the\nmathematical perspective, pre-normalization makes more\nsense [162]. This is similar to the basic principle of the\ntheory of matrix, that normalization should be performed\nbefore projection, e.g., Gram–Schmidt process 2. This prob-\nlem should be studied further by both theoretical research\nand experimental validation.\n3.1.1 Input Tokenization\nTokenization Vanilla Transformer was originally proposed\nfor machine translation as a sequence-to-sequence model,\nthus it is straightforward to take the vocabulary se-\nquences as input. As mentioned previously, the original self-\nattention can model an arbitrary input as a fully-connected\ngraph, independently of modalities. Specifically, bothVanilla\nand variant Transformers take in the tokenized sequences,\nwhere each token can be regarded as a node of the graph.\nSpecial/Customized TokensIn Transformers, various spe-\ncial/customized tokens can be semantically defined as\n2. https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt\nprocess\nplace-holders in the token sequences, e.g., mask token\n[MASK] [4]. Some common special tokens are summarized\nin appendix. Special tokens can be used in both uni-modal\nand multimodal Transformers.\nPosition Embedding Position embeddings are added to\nthe token embeddings to retain positional information [4].\nVanilla Transformer uses sine and cosine functions to pro-\nduce position embedding. To date, various implementations\nof position embedding have been proposed. The concrete\nsolutions are outside the focus of this survey.\nDiscussion The main advantages of input tokenization in-\nclude the following:\n(1) Tokenization is a more general approach from a\ngeometrically topological perspective, achieved by mini-\nmizing constraints caused by different modalities. In gen-\neral, every modality has intrinsic constraints on modelling.\nFor instance, sentences have sequential structures that are\nwell-suited by RNN, and photos are restricted in aligned\ngrid matrices that CNN works well for. Tokenization helps\nTransformers inherently to process different modalities uni-\nversally via irregular sparse structures. Thus even Vanilla\nTransformer can encode multimodal inputs flexibly by just\nconcatenation, weighted summation, even without any mul-\ntimodal tailor-made modifications.\n(2) Tokenization is a more flexible approach to organize\nthe input information via concatenation/stack, weighted\nsummation, etc. Vanilla Transformer injects temporal infor-\nmation to the token embedding by summing position em-\nbedding. For instance, when use Transformer to model free-\nhand sketch drawing [163], each input token can integrate\nvarious drawing stroke patterns, e.g., stroke coordinates,\nstroke ordering, pen state (start/end).\n(3) Tokenization is compatible with the task-specific\ncustomized tokens, e.g., [MASK] token [4] for Masked Lan-\nguage Modelling, [CLASS] token [5] for classification.\nDiscussion How to understand position embedding to\nTransformers is an open problem. It can be understood\nas a kind of implicit coordinate basis of feature space, to\nprovide temporal or spatial information to the Transformer.\nFor cloud point [164] and sketch drawing stroke [163], their\ntoken element is already a coordinate, meaning that position\nembedding is optional, not necessary. Furthermore, position\nembedding can be regarded as a kind of general additional\ninformation. In other words, from a mathematical point of\nview, any additional information can be added, such as de-\ntail of the manner of position embedding, e.g., the pen state\nof sketch drawing stroke [163], cameras and viewpoints in\nsurveillance [165]. There is a comprehensive survey [166]\ndiscussing the position information in Transformers. For\nboth sentence structures (sequential) and general graph\nstructures (sparse, arbitrary, and irregular), position embed-\ndings help Transformers to learn or encode the underlying\nstructures. Considered from the mathematical perspective\nof self-attention, i.e., scaled dot-product attention, attentions\nare invariant to the positions of words (in text) or nodes\n(in graphs), if position embedding information is missing.\nThus, in most cases, position embedding is necessary for\nTransformers.\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2023.3275156\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n5\n3.1.2 Self-Attention and Multi-Head Self-Attention\nThe core component of Vanilla Transformer is the Self-\nAttention (SA) operation [2] that is also termed “Scaled Dot-\nProduct Attention”. Assume that X = [x1, x2, ··· ] ∈ RN×d\nis an input sequence of N elements/tokens, and an op-\ntional preprocessing is positional encoding by point-wise\nsummation Z ← X⊕P ositionEmbeddingor concatenation\nZ ← concat(X, P ositionEmbedding).\nSelf-Attention (SA) After preprocessing, embedding Z\nwill go through three projection matrices ( WQ ∈ Rd×dq ,\nWK ∈ Rd×dk , and WV ∈ Rd×dv , dq = dk) to generate three\nembeddings Q (Query), K (Key), and V (Value):\nQ = ZWQ, K = ZWK, V = ZWV . (2)\nThe output of self-attention is defined as\nZ = SA(Q, K, V) =Softmax\n \nQK⊤\np\ndq\n!\nV. (3)\nGiven an input sequence, self-attention allows each element\nto attend to all the other elements, so that self-attention\nencodes the input as a fully-connected graph. Therefore, the\nencoder of Vanilla Transformer can be regarded as a fully-\nconnected GNN encoder, and the Transformer family has\nthe non-local ability of global perception, similar to the Non-\nLocal Network [167].\nMasked Self-Attention (MSA)In practice, modification of\nself-attention is needed to help the decoder of Transformer\nto learn contextual dependence, to prevent positions from\nattending to subsequent positions, as\nZ = MSA (Q, K, V) =Softmax\n \nQK⊤\np\ndq\n⊙ M\n!\nV, (4)\nwhere M is a masking matrix. For instance, in GPT [88],\nan upper triangular mask to enable look-ahead attention\nwhere each token can only look at the past tokens. Masking\ncan be used in both encoder [163], [168] and decoder of\nTransformer, and has flexible implementations,e.g., 0-1 hard\nmask [163], soft mask [168].\nIn both uni-modal and multimodal practices, specific\nmasks are designed based on domain knowledge and prior\nknowledge. Essentially, MSA is used to inject additional\nknowledge to Transformer models, e.g., [24], [163], [169],\n[170].\nMulti-Head Self-Attention (MHSA) In practice, multiple\nself-attention sub-layers can be stacked in parallel and their\nconcatenated outputs are fused by a projection matrix W,\nto form a structure named Multi-Head Self-Attention:\nZ = MHSA (Q, K, V) =concat(Z1, ··· , ZH)W, (5)\nwhere each head Zh = SA(Qh, KhVh) and h ∈ [1, H],\nand W is a linear projection matrix. The idea of MHSA is a\nkind of ensemble. MHSA helps the model to jointly attend\nto information from multiple representation sub-spaces.\n3.1.3 Feed-Forward Network\nThe output of the multi-head attention sub-layer will go\nthrough the position-wise Feed-Forward Network (FFN)\nthat consists of successive linear layers with non-linear\nactivation. For instance, a two-layer FFN can be formulated\nas\nF F N(Z) =σ(ZW1 + b1)W2 + b2, (6)\nwhere W1, b1, W2, and b2 denote the weights and bi-\nases of the two linear transformations, while σ(·) is non-\nlinear activation, e.g., ReLU (·) [171], GELU (·) [172]. In\nsome Transformer literature, FFN is also termed Multi-Layer\nPerceptron (MLP).\n3.2 Vision Transformer\nVision Transformer (ViT) [5] has an image-specific input\npipeline in which the input image must be split into fixed-\nsize ( e.g., 16 × 16, 32 × 32) patches. After going through\nthe linearly embedded layer and adding the position em-\nbeddings, all the patch-wise sequences will be encoded\nby a standard Transformer encoder. Given an image X ∈\nRH×W×C (H height, W width, C channels), ViT needs\nto reshape X into a sequence of flattened 2D patches:\nxp ∈ RN×(P2·C), where (P × P) is the patch resolution\nand N = HW/P 2. To perform classification, a standard\napproach is to prepend an extra learnable embedding “clas-\nsification token” [CLASS] to the sequence of embedded\npatches:\nZ ← concat([CLASS], XW), (7)\nwhere W denotes the projection.\n3.3 Multimodal Transformers\nRecently, a large number of Transformers have been studied\nextensively for various multimodal tasks, and shown to be\ncompatible with various modalities in both discriminative\nand generative tasks.\nIn this section, we will review the key tech-\nniques/designs of the existing multimodal Transformer\nmodels, from the perspectives of multimodal input (Section\n3.3.1), self-attention variants (Section 3.3.2), and network\narchitectures (Section 3.3.3).\n3.3.1 Multimodal Input\nThe Transformer family is a general architecture that can\nbe formulated as a type of general graph neural network.\nSpecifically, self-attention can process each input as a fully-\nconnected graph, by attending to the global (non-local)\npatterns. Therefore, this intrinsic trait helps Transformers\ncan work in a modality agnostic pipeline that is compatible\nwith various modalities by treating the embedding of each\ntoken as a node of the graph.\nTokenization and Embedding ProcessingGiven an input\nfrom an arbitrary modality, users only need to perform\ntwo main steps, (1) tokenize the input, and (2) select an\nembedding space to represent the tokens, before inputting\nthe data into Transformers. In practice, both the tokenizing\ninput and selecting embedding for the token are vital for\nTransformers but highly flexible, with many alternatives.\nFor instance, given an image, the solution of tokenizing\nand embedding is not unique. Users can choose or design\ntokenization at multiple granularity levels – coarse-grained\nvs. fine-grained. e.g., use ROIs (obtained by an object de-\ntector) and CNN features as tokens and token embeddings\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2023.3275156\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n6\nTABLE 1\nTokenization and token embedding comparison for multi-modal inputs for Transformers. “ICD”: International Classification of Diseases.\nModalities Tokenization Token Embeddings References\nRGB RoI CNN embedding ViLBERT [102], LXMERT [103],\nRGB patch linear projection ViT [5]\nvideo clip of sampled frames 3D CNN embedding VideoBERT [7], CBT [107], ActBERT [114]\nvideo sampled frame 2D CNN embedding [173]\nvideo voxel of sampled frames linear projection VATT [174]\nvideo patch of sampled frame linear projection MBT [175]\n360◦ video clip of sampled frames 3D CNN embedding AVSA [145]\naudio frame (mel-spectrogram) CNN embedding [173], AVSA [145]\naudio waveform segment linear projection VATT [174]\naudio spectrogram patch linear projection MBT [175]\nspeech/audio waveform segment 1D-CNN (TCN) embedding [176], FaceFormer [39]\nspeech frame (mel-spectrogram) linear projection and gated CNN embeddingMeta-StyleSpeech [177]\nspeech frame (log-Mel filterbanks) linear projection AV-HuBERT [178]\nspeech/audio frame (log power spectrum) linear projection VSET [179]\nspeech frame (log-Mel filterbanks) 2D-CNN embedding FAT-MLM [180]\nmusic frame (35-dim music feature)linear projection FACT [146]\ntext word learned embedding VanillaTransformer [2]\ntext word GNN embedding MGNNS [181]\nSQL database schema table node, column node GNN embedding SpeechSQLNet [25]\ntextual question-graph word node GNN embedding SADGA [24]\nsketch key point of stroke linear projection and learnable embedding Multi-Graph Transformer [163]\nsketch patch of picture linear projection RVT [182]\ntable cell learned embedding [18]\n3D point cloud point non-linear projection Point Cloud Transformer [164]\nsource code code learned embedding GraphCodeBERT [44]\ndata flow of source code variable learned embedding GraphCodeBERT [44]\npose key point GCN embedding TriBERT [183]\nelectronic health records (EHRs)ICD code GNN embedding G-BERT [47]\nelectronic health records (EHRs)ICD code learned embedding Med-BERT [184]\nGigapixel Whole Slide Images patch CNN embedding MCAT [185]\n[102], use patches and linear projection as tokens and token\nembeddings [5], or use graph node (obtained by object\ndetector and graph generator) and GNN features as tokens\nand token embeddings [181]. Given a tokenization plan,\nthe subsequent embedding approaches can be diverse. For\nexample, for video input, a common tokenization is to treat\nthe non-overlapping windows (down-sampled) over the\nvideo as tokens, and their embeddings can then be extracted\nby various 3D CNNs, e.g., VideoBERT [7], CBT [107], and\nUniVL [117] use S3D [186], ActBERT uses ResNet-3D [187].\nTable 1 summarizes some common practices of multi-\nmodal inputs for Transformers, including RGB, video, au-\ndio/speech/music, text, graph, etc.\nDiscussion When considered from the perspective of ge-\nometric topology, each of the modalities listed in Table 1\ncan be regarded as a graph. An RGB image is essentially\na neat grid graph in the pixel space. Both video and au-\ndio are clip/segment based graphs over a complex space\ninvolving temporal and semantic patterns. Both 2D and 3D\ndrawing sketches [78], [163] are a kind of sparse graph if we\nconsider their key points along the drawing strokes. Similar\nto sketches, the human pose also is a kind of graph. 3D\npoint cloud is a graph in which each coordinate is a node.\nOther abstract modalities also can be interpreted as graphs,\ne.g., source code [44], data flow of source code [44], table\n[18], SQL database schema [25], text question graph [24],\nand electronic health records (EHRs) [184].\nToken Embedding Fusion In practice, Transformers al-\nlow each token position to contain multiple embeddings.\nThis is essentially a kind of early-fusion of embeddings,\nfor both uni-modal and multimodal Transformer models.\n(This will be discussed further in subsequent sections.) The\nmost common fusion is the token-wise summing of the\nmultiple embeddings, e.g., a specific token embedding ⊕\nposition embedding. Similar to the flexible tokenization,\ntoken embedding fusion is also flexible and widely applied\nto both uni-modal and multimodal Transformer applica-\ntions. In [81], token-wise weighted summing is used to\nperform early-fusion of RGB and grey-scale images for\nmultimodal surveillance AI. In particular, token embedding\nfusion has an important role in multimodal Transformer\napplications as various embeddings can be fused by token-\nwise operators, e.g., in VisualBERT [104] and Unicoder-VL\n[108], segment embeddings are token-wise added to indicate\nwhich modality (vision or language) each token is from, VL-\nBERT [105] injects global visual context to linguistic domain\nby “linguistic token embedding ⊕ full image visual feature\nembedding”, InterBERT [188] adds location information for\nROI by “ROI embedding ⊕ location embedding”, in Im-\nageBERT [115], five kinds of embeddings are fused “image\nembedding ⊕ position embedding ⊕ linguistic embedding\n⊕ segment embedding ⊕ sequence position embedding”.\n3.3.2 Self-Attention Variants in Multimodal Context\nIn multimodal Transformers, cross-modal interactions ( e.g.,\nfusion, alignment) are essentially processed by self-attention\nand its variants. Thus, in this section, we will review the\nmain multimodal modelling practices of Transformers, from\na perspective of self-attention designs, including (1) early\nsummation (token-wise, weighted), (2) early concatena-\ntion, (3) hierarchical attention (multi-stream to one-stream),\n(4) hierarchical attention (one-stream to multi-stream), (5)\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2023.3275156\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n7\nTABLE 2\nSelf-attention variants for multi-modal interaction/fusion. α and β denote weightings. “Att.”: Attention; “Concat.”/“Con.”: Concatenation; “Tfs”:\nTransformer layers.N(A) and N(B) denote the token sequence lengths of two modalities.\nSelf-Attention Definitions Streams Formulations Complexities References\nEarly Summation token-wise sum before Tfs 1 Z←Tf(αZ(A) ⊕βZ(B)) O(N2\n(A)) [46], [81]\nEarly Concat. token sequence concat. before Tfs 1 Z←Tf(C(Z(A),Z(B))) O((N(A) +N(B))2) [7], [44], [178], [180]\nHierarchical Att. 2-stream Tfs followed by concat. 2 →1 Z←Tf3(C(Tf1(Z(A)), Tf2(Z(B)))) O((N(A) +N(B))2) [146],\nHierarchical Att. early concat. followed by 2-stream Tfs1 →2\n\n\nC(Z(A),Z(B)) ←Tf1(C(Z(A),Z(B))),\nZ(A) ←Tf2(Z(A)),\nZ(B) ←Tf3(Z(B)).\nO((N(A) +N(B))2) [188]\nCross-Attention exchange query 2\n(Z(A) ←MHSA(QB,KA,VA)\nZ(B) ←MHSA(QA,KB,VB) O(N2\n(A)) [102], [144]\nCross-Att. to Con. 2-stream cross-att. followed by concat.2 →1\n\n\nZ(A) ←MHSA(QB,KA,VA)\nZ(B) ←MHSA(QA,KB,VB)\nZ←Tf(C(Z(A),Z(B)))\nO((N(A) +N(B))2) [69] [137], [189]\ncross-attention, and (6) cross-attention to concatenation. See\nTable 2 and Figure 2.\nFor brevity, we will state and compare the mathematical\nformulations in two-modality cases. Please note that all\ndiscussed self-attention and its variants are such flexible\nthat can be extended to multiple modality cases. Specifically,\nthe following formulations are modality-, tokenization-, and\nembedding- agnostic, as self-attention models the embed-\nding of arbitrary token from arbitrary modality as a node of\na graph.\nGiven inputs XA and XB from two arbitrary modalities,\nZ(A) and Z(B) denote their respective token embeddings. Let\nZ denoting the token embedding (sequence) produced by\nthe multimodal interactions. T f(·) stands for the processing\nof Transformer layers/blocks.\n(1) Early Summation In practice, early summation [46],\n[81] is a simple and effective multimodal interaction, where\nthe token embeddings from multiple modalities can be\nweighted summed at each token position and then pro-\ncessed by Transformer layers:\nZ ← T f(αZ(A) ⊕ βZ(B)) =MHSA (Q(AB), K(AB), V(AB)),\n(8)\nwhere ⊕ is element-wise sum, and α and β are weight-\nings. Concretely, Q(AB) = (αZ(A) ⊕ βZ(B))WQ\n(AB), K(AB) =\n(αZ(A) ⊕ βZ(B))WK\n(AB), and V(AB) = (αZ(A) ⊕ βZ(B))WV\n(AB).\nIts main advantage is that it does not increase computational\ncomplexity. However, its main disadvantage is due to the\nmanually set weightings. As discussed in Section 3.1.1 and\n3.3.1, summing position embedding is intrinsically a case of\nearly summation.\n(2) Early Concatenation Another straightforward solution\nis early concatenation [7], [44], [178], [180] that the token\nembedding sequences from multiple modalities are concate-\nnated and input into Transformer layers as\nZ ← T f(C(Z(A), Z(B))). (9)\nThus, all the multimodal token positions can be attended as\na whole sequence, such that the positions of each modality\ncan be encoded well by conditioning the context of other\nmodalities. VideoBERT [7] is the one of the first multi-\nmodal Transformer works, where video and text are fused\nvia early concatenation that can encode the global multi-\nmodal context well [188]. However, the longer sequence\nafter concatenation will increase computational complexity.\nEarly concatenation is also termed “all-attention” or “Co-\nTransformer” [137].\n(3) Hierarchical Attention (multi-stream to one-stream)\nTransformer layers can be combined hierarchically to at-\ntend to the cross-modal interactions. A common practice\nis that multimodal inputs are encoded by independent\nTransformer streams and their outputs are concatenated and\nfused by another Transformer [146]:\nZ ← T f3(C(T f1(Z(A)), T f2(Z(B)))). (10)\nThis kind of hierarchical attention is an implementation of\nlate interaction/fusion, and can be treated as a special case\nof early concatenation.\n(4) Hierarchical Attention(one-stream to multi-stream) In-\nterBERT [188] is another good practice of hierarchical atten-\ntion where concatenated multimodal inputs are encoded by\na shared single-stream Transformer that is followed by two\nseparate Transformer streams. This flow can be formulated\nas \n\n\nC(Z(A), Z(B)) ← T f1(C(Z(A), Z(B))),\nZ(A) ← T f2(Z(A)),\nZ(B) ← T f3(Z(B)).\n(11)\nThis method perceives the cross-modal interactions and\nmeanwhile preserves the independence of uni-modal rep-\nresentation.\n(5) Cross-Attention For two-stream Transformers, if the Q\n(Query) embeddings are exchanged/swapped in a cross-\nstream manner, the cross-modal interactions can also be\nperceived. This method is termed cross-attention or co-\nattention [190], which was first proposed in VilBERT [102]:\n(\nZ(A) ← MHSA (QB, KA, VA),\nZ(B) ← MHSA (QA, KB, VB). (12)\nCross-attention attends to each modality conditioned on the\nother and does not cause higher computational complexity,\nhowever if considered for each modality, this method fails\nto perform cross-modal attention globally and thus loses\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2023.3275156\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n8\nQ K V \nTL \n(a)\nQ K V \nTL (b)\nTL \nQ K V \nTL \nQ K V \nQ K V \nTL (c)\nQ K V \nTL \nTL \nQ K V \nTL \nQ K V (d)\nTL \nV K Q \nTL \nQ K V (e)\nTL \nV K Q \nTL \nQ K V \nQ K V \nTL (f)\nFig. 2. Transformer-based cross-modal interactions: (a) Early Summation, (b) Early Concatenation, (c) Hierarchical Attention (multi-stream to\none-stream), (d) Hierarchical Attention (one-stream to multi-stream), (e) Cross-Attention, and (f) Cross-Attention to Concatenation. “Q”: Query\nembedding; “K”: Key embedding; “V”: Value embedding. “TL ”: Transformer Layer. Best viewed in colour.\nthe whole context. As discussed in [188], two-stream cross-\nattention can learn cross-modal interaction, whereas there is\nno self-attention to the self-context inside each modality.\n(6) Cross-Attention to ConcatenationThe two streams of\ncross-attention [102] can be further concatenated and pro-\ncessed by another Transformer to model the global context.\nThis kind of hierarchically cross-modal interaction is also\nwidely studied [137], [189], and alleviates the drawback of\ncross-attention.\n\n\n\nZ(A) ← MHSA (QB, KA, VA),\nZ(B) ← MHSA (QA, KB, VB),\nZ ← T f(C(Z(A), Z(B))).\n(13)\nDiscussion All these aforementioned self-attention variants\nfor multimodal interactions are modality-generic, and can\nbe applied in flexible strategies and for multi-granular tasks.\nSpecifically, these interactions can be flexibly combined and\nnested. For instance, multiple cross-attention streams are\nused in hierarchical attention (one-stream to multi-stream)\nthat in a two-stream decoupled model [191] T f2 and T f3\nof Eq. 11 are implemented by cross-attention defined in\nEq. 12. Moreover, they can be extended to multiple ( ≥ 3)\nmodalities. TriBERT [183] is a tri-modal cross-attention (co-\nattention) for vision, pose, and audio, where given a Query\nembedding, its Key and Value embeddings are the con-\ncatenation from the other modalities. Cross-attention to\nconcatenation is applied to three modalities ( i.e., language,\nvideo, and audio) in [189].\n3.3.3 Network Architectures\nEssentially, various multimodal Transformers work due to\ntheir internal multimodal attentions that are the aforemen-\ntioned self-attention variants. Meanwhile, as illustrated in\nFigure 2, these attentions determine the external network\nstructures of the multimodal Transformers where they are\nembedded.\nIn general, if we consider from the angle of network\nstructures, (1) early summation and early concatenation\nwork in single-stream, (2) cross-attention work in multi-\nstreams, (3) hierarchical attention and cross-attention to\nconcatenation work in hybrid-streams. Thus, multimodal\nTransformers can be divided into single-stream ( e.g., Uniter\n[106], Visualbert [104], Vl-bert [105] , Unified VLP [110]),\nmulti-stream ( e.g., ViLBERT [102], Lxmert [103], ActBERT\n[114]), hybrid-stream (e.g., InterBERT [188]), etc.\nFrom the perspective of timing of interaction, these\nmultimodal attentions fall into three categories, i.e., early\ninteraction: early summation, early concatenation, and hi-\nerarchical attention (one-stream to multi-stream), late inter-\naction: hierarchical attention (multi-stream to one-stream),\nor throughout interaction: cross-attention, cross-attention to\nconcatenation.\nAs demonstrated in Figure 2 in [192], the multimodal\nTransformer models have another architecture taxonomy\nbased on the computational size of the components.\n4 A PPLICATION SCENARIOS\nIn this section we survey multimodal Transformers based\non the application scenarios. We consider two important\nparadigms: (1) Transformers for multimodal pretraining\n(Section 4.1, including both task-agnostic (Section 4.1.1) and\ntask-specific (Section 4.1.2) multimodal pretraining), and (2)\nTransformers for specific multimodal tasks (Section 4.2).\n4.1 Transformers for Multimodal Pretraining\nInspired by the great success of Transformer based pretrain-\ning in NLP community, Transformers are also widely stud-\nied for multimodal pretraining as the various large-scale\nmultimodal corpora is emerging. Recent work has demon-\nstrated that if pretrained on large scale multimodal corpora\nTransformer based models [7], [102], [103], [104], [105], [106],\n[110] clearly outperform other competitors in a wide range\nof multimodal down-stream tasks, and moreover achieve\nthe zero-shot generalization ability. These superiorities have\nled Transformer-based multimodal pretraining to become\na hot topic, which has two main directions, i.e., general\npretraining for agnostic down-stream tasks (Section 4.1.1),\ngoal-oriented pretraining for specific down-stream tasks\n(Section 4.1.2).\nWe focus on these key points: (1) What trends are\nemerging? (2) Where/how do the cross-modal interactions\ntake place during pretraining? (3) How to sort out and\nunderstand the pretraining pretext objectives? How can they\ndrive Transformers to learn the cross-modal interactions?\n4.1.1 Task-Agnostic Multimodal Pretraining\nRecently Transformer-oriented pretraining has been widely\nstudied involving diverse modality combinations, e.g.,\nvideo-text [7], [107], [117], image-text [102], [103], [104],\n[193], [194], [195], acoustic-text [180].\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2023.3275156\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n9\nAmong existing work, the following main trends are\nemerging:\n(1) Vision-language pretraining (VLP) is a major re-\nsearch problem in this field. VLP is including both “im-\nage + language” and “video + language”, also termed\nvisual-linguistic pretraining. A great deal of excellent work\nhas been proposed, e.g., VideoBERT [7], ViLBERT [102],\nLXMERT [103], VisualBERT [104], VL-BERT [105], UNITER\n[106], CBT [107], Unicoder-VL [108], B2T2 [109], VLP [110],\n12-in-1 [111], Oscar [112], Pixel-BERT [113], ActBERT [114],\nImageBERT [115], HERO [116], UniVL [117], SemVLP [196].\n(2) Speech can be used as text. Thanks to recent advances\nin automatic speech recognition (ASR) techniques, in a mul-\ntimodal context, speech can be converted to text by the off-\nthe-shelf speech recognition tools. For instance, VideoBERT\n[7] and CBT [107] make full use of speech rather than low-\nlevel sounds as a source of cross-modal supervision, by\nextracting high-level semantic text.\n(3) Overly dependent on the well-aligned multimodal\ndata. A majority of Transformer-based multimodal pre-\ntraining works in a self-supervised manner, however, it\nis overly dependent on the well-aligned multimodal sam-\nple pairs/tuples. For instance, large amount of image-\nlanguage pretraining Transformer models are pretrained\non large-scale image-text pairs, e.g., VisualBERT [104], VL-\nBERT [105], ViLBERT [102], LXMERT [103], UNITER [106].\nFor another example, the instructional videos ( e.g., cook-\ning) 3 are widely used as the pretraining corpora, e.g.,\nHowToVQA69M [140], HowTo100M [141], as in general,\ntheir visual clues/content and the spoken words have a\nhigher probability to align with each other, if compared\nwith other videos. However, using cross-modal alignment\nas cross-modal supervision is costly for large-scale appli-\ncations. Thus, how to use the weakly-aligned or even un-\npaired/unaligned multimodal data as the pretraining cor-\npora is still understudied. Some recent attempts [137], [199]\nstudy the use of weakly-aligned cross-modal supervision to\ntrain Transformers to learn the cross-modal interactions.\n(4) Most of the existing pretext tasks transfer well\nacross modalities. For instance, Masked Language Mod-\nelling (MLM) in the text domain has been applied to audio\nand image, e.g., Masked Acoustic Modelling [180], [200],\nMasked Image Region Prediction [190], while both Sentence\nOrdering Modelling (SOM) [201] in text domain and Frame\nOrdering Modelling (FOM) [116] in video domain share\nthe same idea. We will further discuss the pretext tasks for\nmultimodal Transformer pretraining in the follows.\n(5) Model structures are mainly in three categories. Es-\nsentially, in multimodal pretraining scenarios, Transformer\nmodels work based on those self-attention variants that are\ndiscussed in Section 3.3.2. Thus, if considered from the per-\nspective of model structures, the existing Transformers for\nmultimodal pretraining are also mainly in three categories,\ni.e., single-stream, multi-stream, hybrid-stream.\n(6) Cross-modal interactions can perform within var-\nious components/levels in the pretraining pipelines. For\nTransformer based multimodal pretraining, the key is to\ndrive the Transformer (encoder w/, w/o decoder) to learn\n3. Note that instructional videos also have weakly aligned cases [197],\n[198].\nthe cross-modal interactions. In the existing Transformer-\nbased multimodal pretraining practices, the cross-modal\ninteractions are flexible, which can perform within various\ncomponents/levels in the pretraining pipelines. In general,\nTransformer-based multimodal pretraining pipelines have\nthree key components, from bottom to top, i.e., tokeniza-\ntion, Transformer representation, objective supervision. For\nnot only the multimodal pretraining but also the specific\nmultimodal tasks, the cross-modal interactions can perform\nwithin arbitrary component(s) of the three. As discussed in\nSection 3.3.2, because self-attention models the embedding\nof an arbitrary token from an arbitrary modality as a node\nof a graph, the existing pretraining pipelines can, in gen-\neral, be transferred independently across modalities, unless\nconsidered with modality-specific objectives.\nDiscussion Vision Language Pretraining (VLP) follows\ntwo general pipelines: two-stage (need object detector, e.g.,\nFaster R-CNN [202]) (e.g., LXMERT [103], ViLBert [102], VL-\nBert [105], UNITER [106]) and end-to-end ( e.g., Pixel-Bert\n[113], SOHO [203], KD-VLP [204], Simvlm [199]). Two-stage\npipelines have a main advantage – object-aware perceiving,\nby using the supervised pre-trained visual detectors, how-\never these are based on a strong assumption that the visual\nrepresentations can be fixed.\nDiscussion How to look for more corpora that intrinsically\nhave well-aligned cross-modal supervision, such as instruc-\ntional videos, is still an open problem. However, weakly-\naligned cross-modal samples are popular in the real-life sce-\nnarios, for instance, enormous weakly aligned multimodal\ndata samples are emerging in e-commerce [137], due to\nfine-grained categories, complex combinations, and fuzzy\ncorrespondence. Well labelled/aligned cross-modal datasets\nare very costly in collecting and annotating; how to use\nweakly-aligned or even unaligned corpora crawled from\nthe web is a promising question. Some recently successful\npractice [9], [199], [205] used weakly aligned image-text\npairs to perform pretraining, and achieve both competitive\nperformance and zero-shot learning capability for image\nclassification, image-text retrieval, and open-ended visual\nquestion answering, etc. Because these practices in weak\nsupervision make full use of large-scale pretraining corpora,\nthey yield greater promise of zero-shot generalization.\nPretext Tasks In Transformer based multimodal pretrain-\ning, the pretraining tasks/objectives are also termed pre-\ntext tasks/objectives. To date, various pretext tasks have\nbeen studied, e.g., masked language modelling (MLM)\n[137], masked image region prediction/classification (also\ntermed masked object classification (MOC)) [137], [190],\nmasked region regression (MRR) [115], visual-linguistic\nmatching (VLM) ( e.g., image–text matching (ITM) [188],\nimage text matching (ITM), phrase-region alignment (PRA)\n[204], word-region alignment (WRA) [106], video-subtitle\nmatching (VSM) [116]), masked frame modelling (MFM)\n[116], frame order modelling (FOM) [116], next sentence pre-\ndiction (NSP) [4], [102], [190], masked sentence generation\n(MSG) [191], masked group modelling (MGM) [188], prefix\nlanguage modelling (PrefixLM) [199], video conditioned\nmasked language model [117], text conditioned masked\nframe model [117], visual translation language modelling\n(VTLM) [206], and image-conditioned masked language\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2023.3275156\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n10\nmodelling (also termed image-attended masked language\nmodelling) [207]. These down-stream task -agnostic pretext\npretraining is optional, and the down-stream task objectives\ncan be trained directly, which will be discussed in Sec-\ntion 4.1.2. Table 3 provides the common and representative\npretext tasks for Transformer based multimodal pretraining.\nIn practice, pretext tasks can be combined, and some repre-\nsentative cases are summarized in Table 3 of [57], Table 2 of\n[58].\nThe pretext tasks have multiple taxonomies:\n(1) Supervision. The common multimodal pretraining\nTransformers use well-aligned, weakly-aligned, and even\nunaligned multimodal sample pairs/tuples, to work in su-\npervised, weakly-supervised, and unsupervised manners,\nrespectively. Meanwhile, if we consider the definitions of\ntheir pretext tasks/objectives from supervision, the pre-\ntexts can be sorted into unsupervised/self-supervised ( e.g.,\nmasked language modelling (MLM) [7], [137]) and super-\nvised ( e.g., image-text matching (ITM) [188] [102], [103],\n[104], [106], [209]), etc. Nowadays, self-supervised attempts\nare the majority.\n(2) Modality. Considering the mathematical formula-\ntions, some pretexts are defined on single modality, e.g.,\nmasked language modelling [7], masked acoustic modelling\n[200], masked region regression (MRR) [115], while other\npretexts are defined on multiple modalities, e.g., image-\nconditioned masked language modelling (IMLM) [208],\nimage-text matching (ITM) [188], video-subtitle matching\n(VSM) [116]. Thus, from this mathematical view, the pretext\ntasks can be divided into two categories, i.e., uni-modal and\nmultimodal.\nHowever, this classification is not really accurate. It\nshould be highlighted that in multimodal pretraining Trans-\nformer models, even if the pretext objective formulations\nonly include uni-modal elements, pretexts can still involve\nother modalities, essentially conditioned on the clues from\nother modalities, by (a) prepositive token level interactions\nand/or Transformer level interactions, (b) co-training with\nother pretexts that involve other modalities. For instance,\nVL-BERT [105] uses two dual pretext tasks, i.e., masked\nlanguage modelling and masked RoI classification.\n(3) Motivation. If consider their motivations, the pretext\ntasks include masking, describing, matching, ordering, etc.\nSome recent surveys focus on VLP and compare the\nexisting VLP Transformer models from the angles of do-\nmain (image-text or video-text), vision feature extraction,\nlanguage feature extraction, architecture (single- or dual-\nstream), decoder (w/, w/o), pretext tasks/objectives, pre-\ntraining datasets, and down-stream tasks, e.g., Table 3 of\n[57], Table 2 of [58]. Different from these views, in this\nsurvey, we would propose our comparisons from some\nnew perspectives. Specifically: (1) The core of Transformer\necosystem is self-attention, thus we would compare the\nexisting multimodal pretraining Transformer models from\nthe angles of how and when the self-attention or its variants\nperform cross-modal interactions. (2) Considering from a\ngeometrically topological perspective, self-attention helps\nTransformers intrinsically work in a modality agnostic\npipeline that is compatible with various modalities by tak-\ning in the embedding of each token as a node of graph, thus\nwe would highlight that the existing VLP can be applied\nto other modalities, beyond visual and linguistic domains.\n(3) We suggest to treat the Transformer-based multimodal\npretraining pipelines having three key components, from\nbottom to top, i.e., tokenization, Transformer representation,\nobjective supervision.\nDiscussion In spite of the recent advances, multimodal\npretraining Transformer methods still have some obvious\nbottlenecks. For instance, as discussed by [208] in VLP\nfield, while the BERT-style cross-modal pretraining models\nproduce excellent results on various down-stream vision-\nlanguage tasks, they fail to be applied to generative tasks\ndirectly. As discussed in [208], both VideoBERT [7] and CBT\n[107] have to train a separate video-to-text decoder for video\ncaptioning. This is a significant gap between the pretraining\nmodels designed for discriminative and generative tasks, as\nthe main reason is discriminative task oriented pretraining\nmodels do not involve the decoders of Transformer. There-\nfore, how to design more unified pipelines that can work\nfor both discriminative and generative down-stream tasks\nis also an open problem to be solved. Again for instance,\ncommon multimodal pretraining models often underper-\nform for fine-grained/instance-level tasks as discussed by\n[137].\nDiscussion As discussed in [208], the masked language\nand region modelling as pre-training task have a main\nadvantage that the Transformer encoder learned from these\nsupervisions can encode both vision and language patterns\nbased on bidirectional context and it is naturally fit for\nthe semantic understanding tasks, e.g., VQA, image-text\nretrieval.\nDiscussion How to boost the performance for multimodal\npretraining Transformers is an open problem. Some prac-\ntices demonstrate that multi-task training (by adding aux-\niliary loss) [111], [137] and adversarial training [210] im-\nprove multimodal pretraining Transformers to further boost\nthe performance. Meanwhile, overly compound pretraining\nobjectives potentially upgrade the challenge of balancing\namong different loss terms, thus complicate the training\noptimization [199]. Moreover, the difficulty of the pretexts\nis also worth discussing. In general, if aim to learn more\nexplicit object concepts, more complex pretext losses will be\nused [204]. However, for pretexts, whether more complexity\nis better remains a question.\n4.1.2 Task-Specific Multimodal Pretraining\nIn practices of multimodal Transformers, the aforemen-\ntioned down-stream task -agnostic pretraining is optional,\nnot necessary, and down-stream task specific pretraining\nis also widely studied [150], [190], [208], [211]. The main\nreasons include: (1) Limited by the existing technique, it is\nextremely difficult to design a set of highly universal net-\nwork architectures, pretext tasks, and corpora that work for\nall the various down-stream applications. (2) There are non-\nnegligible gaps among various down-stream applications,\ne.g., task logic, data form, making it difficult to transfer from\npretraining to down-stream applications.\nTherefore, a large number of down-stream tasks still\nneed tailor-made pretraining to improve the performance.\nGuhur et al. [150] propose in-domain pretraining for vision-\nand-language navigation, as the general VLP focuses on\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2023.3275156\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n11\nTABLE 3\nPretext task comparison of multi-modal pretraining Transformer models (for agnostic down-stream tasks). “C-M Loss”: cross-modal loss; “Con.\nLoss”: loss conditioned on other modality/modalities.\nTypes (Motivations) Tasks C-M Loss Con. Loss References\nMasking\nMasked Language Modelling (MLM) ✓ [7], [137]\nImage-Conditioned Masked Language Modelling (IMLM) ✓ [206], [207] [208]\nText-Conditioned Masked Region Prediction ✓ [206]\nMasked Acoustic Modelling ✓ [180], [200]\nMasked Image Region Regression ✓ [115]\nMasked Image Region Prediction ✓ [190]\nMasked Frame Modelling (MFM) ✓ [116]\nMasked Sentence Generation (MSG) ✓ [191]\nVideo Conditioned Masked Language Model ✓ [117]\nText Conditioned Masked Frame Model ✓ [117]\nDescribing\nImage-conditioned Denoising Autoencoding (IDA) ✓ [208]\nText-conditioned Image Feature Generation (TIFG) ✓ [208]\nPrefix Language Modelling (PrefixLM) ✓ [199]\nMatching\nImage-Text Matching (ITM) ✓ [188] [102], [103], [104], [106], [209],\nPhrase-Region Alignment (PRA) ✓ [204]\nWord-Region Alignment (WRA) ✓ [106], [192]\nVideo-Subtitle Matching (VSM) ✓ [116]\nNext Sentence Prediction (NSP) ✓ [4], [102], [190]\nOrdering Sentence Ordering Modelling (SOM) ✓ [201]\nFrame Ordering Modelling (FOM) ✓ [116]\nlearning vision-language correlations, not designed for se-\nquential decision making as required in embodied VLN.\nMurahari et al . [190] present a visual dialogue oriented\napproach to leverage pretraining on general vision-language\ndatasets. XGPT [208] is tailor-made for image captioning, to\novercome the limitation that BERT-based cross-modal pre-\ntrained models fail to be applied to generative tasks directly.\nERNIE-ViLG [211] is designed for bidirectional image-text\ngeneration with Transformers.\nSpecial modalities have their own unique domain\nknowledge that can be used to design the specific pretrain\npretexts. GraphCodeBERT [44] uses two structure-aware\npretext tasks (i.e., predict where a variable is identified from,\ndata flow edge prediction between variables) for program-\nming source code. To learn from the spatial cues in 360◦\nvideo, Morgado et al. [145] propose to perform contrastive\naudio-visual spatial alignment of 360◦ video and spatial au-\ndio. Med-BERT [184] is a contextualized embedding model\npretrained on a structured electronic health record dataset\nof two million patients. Kaleido-BERT [212] is a VLP Trans-\nformer model tailor-made for the fashion domain.\n4.2 Transformers for Specific Multimodal Tasks\nRecent work has demonstrated that Transformer models\ncan encode various multimodal inputs in both classical and\nnovel discriminative applications, e.g., RGB & optical flow\n[46], RGB & depth [213], RGB & point cloud [214], RGB\n& LiDAR [215], [216], textual description & point cloud\n[31], acoustic & text [180], audio & visual observation for\nAudio-Visual Navigation [76], speech query & schema of\nSQL database [25], text question/query & the schema SQL\ndatabase [24], audio & tags [217], multimodal representation\nfor video [218], [219], text query & video [220], audio &\nvideo for audio visual speech enhancement (AVSE) [179],\naudio & video for Audio-Visual Video Parsing [173], audio\n& video for audio-visual speech recognition [134], video &\ntext for Referring Video Object Segmentation (RVOS) [221],\nsource code & comment & data flow [44], image & text for\nretrieval [222].\nMeanwhile, Transformers also contribute to various\nmultimodal generative tasks, including single-modality to\nsingle-modality (e.g., raw audio to 3D mesh sequence [39],\nRGB to 3D scene [40], single image to 3D human texture\nestimation [223], RGB to scene graph [19], [224], [225], [226],\ngraph to graph [33], knowledge graph to text [227], video\nto scene graph [228], video to caption [229], [230], [231],\n[232], image to caption [233], [234], [235], [236], [237], text\nto speech [238], text to image [205], [239], text to shape\n[240], RGB to 3D human pose and mesh [41], music to\ndance [241]), multimodality to single modality ( e.g., image\n& text to scene graph [242], Video Dialogue (text & audio &\nvisual to text) [243], Mono Audio & Depth to Binaural Audio\n[14], music piece & seed 3D motion to long-range future 3D\nmotions [146], X-raying image & question to answer [244],\nvideo & text & audio to text [245]), and multimodality to\nmultimodality (e.g., [246]).\n5 C HALLENGES AND DESIGNS\nComplementing the application scenario taxonomy dis-\ncussed in Section 4, we further survey prior work from the\nperspective of technical challenges. We discuss seven chal-\nlenges of Transformer based multimodal learning, includ-\ning fusion, alignment, transferability, efficiency, robustness,\nuniversalness, and interpretability. This further extends the\ntaxonomy introduced in [1] to tackle the higher diversity\nand wider scopes of existing Transformer based MML works\nin recent years.\n5.1 Fusion\nIn general, MML Transformers fuse information across mul-\ntiple modalities primarily at three levels: input ( i.e., early\nfusion), intermediate representation ( i.e., middle fusion),\nand prediction (i.e., late fusion). Common early fusion based\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2023.3275156\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n12\nMML Transformer models [7], [104], [108] are also known\nas one-stream architecture, allowing the adoption of the\nmerits of BERT due to minimal architectural modification.\nThe main difference between these one-stream models is the\nusage of problem-specific modalities with variant masking\ntechniques. With attention operation, a noticeable fusion\nscheme is introduced based on a notion of bottleneck to-\nkens [175]. It applies for both early and middle fusion by\nsimply choosing to-be-fused layers. We note that the simple\nprediction-based late fusion [247], [248] is less adopted\nin MML Transformers. This makes sense considering the\nmotivations of learning stronger multimodal contextual rep-\nresentations and great advance of computing power. For\nenhancing and interpreting the fusion of MML, probing the\ninteraction and measuring the fusion between modalities\n[249] would be an interesting direction to explore.\n5.2 Alignment\nCross-modal alignment is the key to a number of real-world\nmultimodal applications. Transformer based cross-modal\nalignment has been studied for various tasks, e.g., speaker\nlocalization in multi-speaker videos [250], speech translation\n[180], text-to-speech alignment [251], text-to-video retrieval\n[252], [253], [254], and visual grounding of natural language\n[255], [256], [257], [258], [259]. Recently, Transformer based\nalignment [9], [119], [260], [261], [262] has led to a surge\nof leveraging large quantities of web data ( e.g., image-text\npairs) for vision and language tasks.\nA representative practice is to map two modalities into\na common representation space with contrastive learning\nover paired samples. The models based on this idea are often\nenormous in size and expensive to optimize from millions\nor billions of training data. Consequently, successive works\nmostly exploit pretrained models for tackling various down-\nstream tasks [120], [263], [264], [265], [266]. These alignment\nmodels have the ability of zero-shot transfer particularly\nfor image classification via prompt engineering [267]. This\nnovel perspective is mind-blowing, given that image classi-\nfication is conventionally regarded as a unimodal learning\nproblem and zero-shot classification remains an unsolved\nchallenge despite extensive research [268]. This has been\nstudied for more challenging and fine-grained tasks ( e.g.,\nobject detection [269], visual question answering [103], [106],\n[112], [263], and instance retrieval [222], [263]) by imposing\nregion (semantic parts such as objects) level alignment. Fine-\ngrained alignment will however incur more computational\ncosts from explicit region detection and how to eliminate\nthis whilst keeping the region-level learning capability be-\ncomes a challenge. Several ideas introduced recently include\nrandom sampling [113], learning concept dictionary [203],\nuniform masking [270], patch projection [192], joint learning\nof a region detector [271], and representation aligning before\nmask prediction [263].\n5.3 Transferability\nTransferability is a major challenge for Transformer based\nmultimodal learning, involving the question of how to\ntransfer models across different datasets and applications.\nData augmentation and adversarial perturbation strate-\ngies help multimodal Transformers to improve the gener-\nalization ability. VILLA [210] is a two-stage strategy (task-\nagnostic adversarial pretraining, followed by task-specific\nadversarial finetuning) that improves VLP Transformers.\nIn practice, the distribution gap between training data\nand practical data is noticeable. For instance, supervised\ndata samples (well-labelled, well-aligned) are costly in prac-\ntical applications, thus how to transfer the supervised multi-\nmodal Transformers pretrained on well-aligned cross-modal\npairs/tuples to the weakly aligned test bed is challenging\n[137]. CLIP [9] is an inspiring solution that transfers knowl-\nedge across modalities by learning a shared multimodal em-\nbedding space, enabling zero-shot transfer of the model to\ndown-stream tasks. The main inspiration that CLIP presents\nthe community is that the pretrained multimodal (image\nand text) knowledge can be transferred to down-stream\nzero-shot image prediction by using a prompt template “ A\nphoto of a {label}.” to bridge the distribution gap\nbetween training and test datasets.\nOver-fitting is a major obstacle to transfer. Multimodal\nTransformers can be overly fitted to the dataset biases dur-\ning training, due to the large modelling capability. Some\nrecent practices exploit how to transfer the oracle model\ntrained on noiseless dataset to real dataset. For instance,\nKervadec et al. [272], [273] explore how transferable reason-\ning patterns are in VQA, and demonstrate that for LXMERT\n[103]/BERT-like reasoning patterns can be partially trans-\nferred from an ideal dataset to a real dataset.\nCross-task gap is another major obstacle to transfer\n[208], [274], due to the different reasoning and input-output\nworkflows, e.g., how to use multimodal datasets to finetune\nthe language pretrained model is difficult [274]. In real ap-\nplications, multimodal pretrained Transformers sometimes\nneed to handle the uni-modal data at inference stage due\nto the issue of missing modalities. One solution is using\nknowledge distillation, e.g., distilling from multimodal to\nuni-modal attention in Transformers [275], distilling from\nmultiple uni-modal Transformer teachers to a shared Trans-\nformer encoder [276]. There is a huge gap across discrimina-\ntive and generative multimodal tasks. As discussed in [208],\nthe BERT-like encoder-only multimodal Transformers ( e.g.,\nVideoBERT [7], CBT [107]) need separately to train decoders\nfor generation tasks. This could create a pretrain-finetune\ndiscrepancy detrimental to the generality. Recently, more\nand more attempts study this issue further, e.g., GilBERT\n[222] is a generative VLP models for a discriminative task,\ni.e., image-text retrieval.\nCross-lingual gap also should be considered for the\ntransferability of Transformer based multimodal learning,\ne.g., universal cross-lingual generalization from English to\nnon-English multimodal contexts [206], [277].\n5.4 Efficiency\nMultimodal Transformers suffer from two major efficiency\nissues: (1) Due to the large model parameter capacity, they\nare data hungry and thus dependent on huge scale training\ndatasets. (2) They are limited by the time and memory com-\nplexities that grow quadratically with the input sequence\nlength, which are caused by the self-attention. In multi-\nmodal contexts, calculation explosion will become worse\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2023.3275156\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n13\ndue to jointly high dimension representations. These two\nbottlenecks are interdependent and should be considered\ntogether.\nTo improve the training and/or inferring efficiency for\nmultimodal Transformers, recent efforts have attempted to\nfind various solutions, to use fewer training data and/or\nparameters. The main ideas can be summarized as the\nfollows.\n(1) Knowledge distillation. Distill the knowledge from\nthe trained larger Transformers to smaller Transformers [93].\nMiech et al. [278] conduct distillation from a slower model\n(early concatenation based Transformers,O((N(A) +N(B))2))\nto a faster one (independently dual branch Transformers,\nO(N2\n(A))).\n(2) Simplifying and compressing model. Remove the\ncomponents to simplify the pipelines. Taking the VLP Trans-\nformer models as an example, two-stage pipeline is costly as\nthey need object detector. One simplifying is processing the\nvisual input in convolution-free manner,e.g., E2E-VLP [271],\nViLT [192]. DropToken [174] reduces the training complexity\nvia random dropping a portion of the video and audio\ntokens from input sequence during training. DropToken can\nbe treated as an implementation of dropout or adversarial\ntraining. Weight-sharing is also a common practice for sim-\nplifying multimodal Transformer models. Wen et al . [279]\npresent a weight-sharing Transformer on top of the visual\nand textual encoders to align text and image. Lee et al. [280]\npropose a novel parameter sharing scheme based on low-\nrank approximation.\n(3) Asymmetrical network structures. Assign different\nmodel capacities and computational size properly for dif-\nferent modalities, to save parameters. See Figure 2 in [192].\n(4) Improving utilization of training samples. Liu et\nal. [281] train a simplified LXMERT by making full use\nof fewer samples at different granularities. Li et al . [282]\nuse fewer data to train CLIP by fully mining the potential\nself-supervised signals of (a) self-supervision within each\nmodality, (b) multi-view supervision across modalities, and\n(c) nearest-neighbour supervision from other similar pairs.\n(5) Compressing and pruning model. Search the optimal\nsub-structures/sub-networks of multimodal Transformers,\ne.g., playing Lottery Tickets with the VLP Transformer mod-\nels [283], adaptively freezing some layers during training\n[284].\n(6) Optimizing the complexity of self-attention. Trans-\nformers cost time and memory that grows quadratically\nwith the input sequence length [285]. One potential solution\nis optimizing the O(N2) complexity, e.g., Child et al . [286]\npresent sparse factorizations of the attention matrix to re-\nduce the quadratical complexity to O(n√n), Transformer-\nLS [287] is an efficient Transformer for both language and\nvision long sequence, with linear computational and mem-\nory complexity.\n(7) Optimizing the complexity of self-attention based\nmultimodal interaction/fusion. Nagrani et al. [175] propose\nFusion via Attention Bottlenecks (FSN, fusion bottleneck) to\nimprove the early concatenation based multimodal interac-\ntion. FSN passes on the messages through a small number\nof bottleneck latents, thus requiring the model to purify the\nmost necessary information from each modality for cross-\nmodal sharing. This strategy uses the fusion bottleneck as a\nbridge, and not only improves fusion performance, but also\nreduces computational cost.\n(8) Optimizing other strategies. Use optimal strategies\nto perform the common Transformer based multimodal in-\nteractions. Given the quadratic complexity of self-attention,\nusing early concatenation based multimodal interaction\nto synchronously fuse the inputs from multiple modali-\nties/views is costly. Yan et al . [288] present an efficient\nsolution that sequentially fuses information between all\npairs of two adjacent views in ascending order of sequence\nlength. This is intrinsically a greedy strategy.\n5.5 Robustness\nMultimodal Transformers pretrained on large-scale corpora\nachieve the state-of-the-art for various multimodal appli-\ncations, while their robustness is still unclear and under-\nstudied. This at least involves two key challenges, i.e., how\nto theoretically analyse the robustness, how to improve the\nrobustness.\nAlthough that recent attempts [99], [182], [289], [290]\nstudy and evaluate how the Transformer components/sub-\nlayers contribute to the robustness, the main bottleneck\nis that the community lacks theoretical tools to analyse\nthe Transformer family. Recently, the common practices to\nanalyse robustness are mainly based on experiment eval-\nuations [291], e.g., cross-dataset evaluations, perturbation-\nbased evaluations. Thus, some multimodal datasets [130],\n[292] are proposed for evaluating the robustness.\nRecent attempts mainly use two straightforward meth-\nods to improve the robustness for multimodal Transformer\nmodels: (1) augmentation and adversarial learning based\nstrategies [293], [294], (2) fine-grained loss functions [295].\nFor instance: VILLA [210] is a generic adversarial training\nframework that can be applied to various multimodal Trans-\nformers. Akula et al. [292] empirically demonstrate that ViL-\nBERT fails to exploit linguistic structure, and they propose\ntwo methods to improve the robustness of ViLBERT, one\nbased on contrastive learning and the other based on multi-\ntask learning.\n5.6 Universalness\nDue to the highly diversity of tasks and modalities of\nmultimodal learning, universalness is an important problem\nfor multimodal Transformer models. A large amount of\nrecent attempts [117], [296], [297], [298] study how to use\nas unified as possible pipelines to handle various modali-\nties and multimodal tasks. Ideally, the unified multimodal\nTransformers can be compatible with various data ( e.g.,\naligned and unaligned, uni-modal and multimodal) and\ntasks ( e.g., supervised and unsupervised, uni-modal and\nmultimodal, discriminative and generative), and meanwhile\nhave either few-shot or even zero-shot generalization abil-\nity. Thus, the current solutions for universalness goal for\nmultimodal Transformers are preliminary probes.\nThe currently unifying-oriented attempts mainly in-\nclude:\n(1) Unifying the pipelines for both uni-modal and multi-\nmodal inputs/tasks. As discussed Section 5.3, in practical\nscenarios, multimodal Transformers need to handle uni-\nmodal data due to the issue of missing modalities. Distilling\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2023.3275156\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n14\nmultimodal knowledge into small models that are adaptable\nto uni-modal data and tasks is a successful practice [275],\n[276].\n(2) Unifying the pipelines for both multimodal un-\nderstanding and generation. In general, for multimodal\nTransformer pipelines, understanding and discriminative\ntasks require Transformer encoders only, while genera-\ntion/generative tasks require both Transformer encoders\nand decoders. Existing attempts use multi-task learning\nto combine the understanding and generation workflows,\nwhere two kinds of workflows are jointly trained by multi-\ntask loss functions. From the perspective of model struc-\ntures, typical solutions include: (a) encoder + decoder,\ne.g., E2E-VLP [271]. (b) separate encoders + cross encoder\n+ decoder, e.g., UniVL [117], CBT [107]. (c) single uni-\nfied/combined encoder-decoder, e.g., VLP [110]. (d) two-\nstream decoupled design [191].\n(3) Unifying and converting the tasks themselves, e.g.,\nCLIP [9] converts zero-shot recognition to retrieval, thus\nreduces the costs of modifying the model.\nHowever, the aforementioned practices suffer some ob-\nvious challenges and bottlenecks, at least including:\n(1) Due to modality and task gaps, universal models\nshould consider the trade-off between universalness and\ncost. Unifying the pipelines of different modalities and\ntasks generally cause larger or more complicated model\nconfiguration, whereas for a specific modality or task, some\ncomponents are redundant.\n(2) Multi-task loss functions increase the complexity of\ntraining. How to co-train multiple objectives properly and\neffectively is challenging, due to that different objectives\ngenerally should be optimized in different strategies.\n5.7 Interpretability\nWhy and how Transformers perform so well in multimodal\nlearning has been investigated [106], [299], [300], [301],\n[302], [303], [304], [305], [306]. These attempts mainly use\nprobing task and ablation study. Cao et al . [299] design a\nset of probing tasks on UNITER [106] and LXMERT [103],\nto evaluate what patterns are learned in pretraining. Hen-\ndricks et al . [301] probe the image–language Transformers\nby fine-grained image–sentence pairs, and find that verb\nunderstanding is harder than subject or object understand-\ning. Chen et al . [106] examine the optimal combination of\npretraining tasks via ablation study, to compare how differ-\nent pretexts contribute to the Transformers. Despite these\nattempts, the interpretability of multimodal Transformers is\nstill under-studied to date.\n6 D ISCUSSION AND OUTLOOK\nDesigning the universal MML models to excel across all the\nunimodal and multimodal down-stream tasks with different\ncharacteristics simultaneously [115], [299] is a non-trivial\nchallenge. For instance, two-stream architectures [9], [263]\nare typically preferred over one-stream ones for cross-modal\nretrieval-like tasks in efficiency, since the representation of\neach modality can be pre-computed beforehand and reused\nrepeatedly. That being said, how to design task-agnostic\nMML architectures is still an open challenge, in addition\nto other design choices such as pretext and objective loss\nfunctions. Furthermore, a clear gap remains between the\nstate-of-the-art and this ultimate goal. In general, existing\nmultimodal Transformer models [9], [199], [263] are superior\nonly for specific MML tasks, as they are designed specifi-\ncally for only a subset of specific tasks [137], [142], [212],\n[249], [260], [261], [265], [266]. Encouragingly, several recent\nstudies towards universal modality learning in terms of\nmodality-agnostic network design [3] and more task-generic\narchitecture design [307], [308], [309] have been introduced,\nand it is hoped this will spark further investigation. To that\nend, instead of exhaustively exploring the vast model design\nspace, seeking in-depth understanding and interpretation of\na MML model’s behaviour might be insightful for superior\nalgorithm design, even though the interactions and synergy\nacross different modalities are intrinsically complex and\neven potentially inconsistent over tasks [249].\nFor more fine-grained MML, it is widely acknowledged\nthat discovering the latent semantic alignments across\nmodalities is critical. An intuitive strategy is to leverage\nsemantic parts (e.g., objects) pre-extracted by an off-the-shelf\ndetector for MML [103], [104], [105], [106], [112], [204], [310].\nThis, however, is not only complex and error-prone, but\ncomputationally costly [207]. Several remedies introduced\nrecently include random sampling [113], learning concept\ndictionary [203], jointly learning a region detector [271], and\nrepresentation aligning before mask prediction [263]. Given\nthe scale of MML training data, exploring this direction\nneeds exhaustive computational costs, and it is supposed\nthat industrial research teams with rich resources are more\nlikely to afford. Ideally, a favourable MML method would\nleave fine-grained semantic alignment across modalities to\nemerge on its own, which is worthy of careful investigation\nin the future.\nAs the learning scale expands exponentially, the training\ndata become inevitably noisy and heterogeneous [9], [199],\n[263]. It has been recently shown that properly tackling\nthe noise issue is useful [263], [309]. Another related facet\nis training strategy, e.g., how many stages of training is\nsuperior over the common one-stage policy [115]. Further,\nthe quadratic complexity with Transformers becomes more\nacute for multimodal data due to longer input. Despite\nextensive research on efficient variants [49], dedicated ef-\nficiency study for MML is still underestimated even empir-\nically and call for more investigation.\nIdentifying the strengths of Transformers for multimodal\nmachine learning is a big open problem. The following\nmain points can be summarized from the literature: (1)\nTransformers can encode implicit knowledge [32]. (2) The\nmulti-head brings multiple modelling sub-spaces that can\nfurther enhance the expressive ability of the model. Ide-\nally, multiple heads after training are good and different.\nThis is essentially a good practice of ensemble learning.\n(3) Transformers intrinsically have a nature of global ag-\ngregation that perceives the non-local patterns. (4) Thanks\nto the large model capacity, Transformer models handle\nthe challenging domain gaps and shifts ( e.g., linguistic and\nvisual) better via effective pretraining on large-scale corpora\n[294]. (5) Transformers can represent the inputs as graphs,\nwhich are intrinsically compatible with more modalities,\ne.g., table and SQL. (6) For modelling series and sequence\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2023.3275156\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n15\npatterns (e.g., time-series), Transformers have better training\nand inference efficiency against RNN-based models, thanks\nto their parallel computation in training and/or inference.\nTransformers are inherently permutation invariant for pro-\ncessing a sequence of points, e.g., well-suited for point cloud\nlearning [164]. (7) Tokenization makes Transformers flexible\nto organize multimodal inputs, as discussed in Section 3.1.1.\n7 C ONCLUSION\nThis survey focuses on multimodal machine learning with\nTransformers. We reviewed the landscape by introducing\nthe Transformer designs and training in the multimodal\ncontexts. We summarized the key challenges and solutions\nfor this emerging and exciting field. Moreover, we discussed\nopen problems and potential research directions. We hope\nthat this survey gives a helpful and detailed overview for\nnew researchers and practitioners, provides a convenient\nreference for relevant experts ( e.g., multimodal machine\nlearning researchers, Transformer network designers), and\nencourages future progress.\nREFERENCES\n[1] T. Baltru ˇsaitis, C. Ahuja, and L.-P . Morency, “Multimodal ma-\nchine learning: A survey and taxonomy,” TP AMI, 2018.\n[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin NeurIPS, 2017.\n[3] A. Jaegle, F. Gimeno, A. Brock, A. Zisserman, O. Vinyals, and\nJ. Carreira, “Perceiver: General perception with iterative atten-\ntion,” in ICML, 2021.\n[4] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,” arXiv, 2018.\n[5] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly\net al., “An image is worth 16x16 words: Transformers for image\nrecognition at scale,” arXiv, 2020.\n[6] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,”\nin ECCV, 2020.\n[7] C. Sun, A. Myers, C. Vondrick, K. Murphy, and C. Schmid,\n“Videobert: A joint model for video and language representation\nlearning,” in ICCV, 2019.\n[8] J. Chen, X. Tan, Y. Leng, J. Xu, G. Wen, T. Qin, and T.-Y. Liu,\n“Speech-t: Transducer for text to speech and beyond,” NeurIPS,\n2021.\n[9] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agar-\nwal, G. Sastry, A. Askell, P . Mishkin, J. Clark et al. , “Learning\ntransferable visual models from natural language supervision,”\narXiv, 2021.\n[10] M. Li, R. Xu, S. Wang, L. Zhou, X. Lin, C. Zhu, M. Zeng, H. Ji,\nand S.-F. Chang, “Clip-event: Connecting text and images with\nevent structures,” arXiv, 2022.\n[11] C. Zhang, Z. Yang, X. He, and L. Deng, “Multimodal intelligence:\nRepresentation learning, information fusion, and applications,”\nJSTSP, 2020.\n[12] A. Rahate, R. Walambe, S. Ramanna, and K. Kotecha, “Multi-\nmodal co-learning: Challenges, applications with datasets, recent\nadvances and future directions,” Information Fusion, 2022.\n[13] T. Hastie, R. Tibshirani, J. H. Friedman, and J. H. Friedman, The\nelements of statistical learning: data mining, inference, and prediction .\nSpringer, 2009, vol. 2.\n[14] K. K. Parida, S. Srivastava, and G. Sharma, “Beyond mono\nto binaural: Generating binaural audio from mono audio with\ndepth and cross modal attention,” in WACV, 2022.\n[15] F. Qingyun, H. Dapeng, and W. Zhaokui, “Cross-modality fusion\ntransformer for multispectral object detection,” arXiv, 2021.\n[16] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0:\nA framework for self-supervised learning of speech representa-\ntions,” NeurIPS, 2020.\n[17] A. Nagrani, C. Sun, D. Ross, R. Sukthankar, C. Schmid, and\nA. Zisserman, “Speech2action: Cross-modal supervision for ac-\ntion recognition,” in CVPR, 2020.\n[18] W. Chen, M.-W. Chang, E. Schlinger, W. Wang, and W. W. Cohen,\n“Open question answering over tables and text,” arXiv, 2020.\n[19] Y. Guo, L. Gao, X. Wang, Y. Hu, X. Xu, X. Lu, H. T. Shen,\nand J. Song, “From general to specific: Informative scene graph\ngeneration via balance adjustment,” in ICCV, 2021.\n[20] K. Gupta, J. Lazarow, A. Achille, L. Davis, V . Mahadevan, and\nA. Shrivastava, “Layouttransformer: Layout generation and com-\npletion with self-attention,” arXiv, 2020.\n[21] C.-F. Yang, W.-C. Fan, F.-E. Yang, and Y.-C. F. Wang, “Layout-\ntransformer: Scene layout generation with conceptual and spatial\ndiversity,” in CVPR, 2021.\n[22] R. Li, S. Zhang, and X. He, “Sgtr: End-to-end scene graph\ngeneration with transformer,” in CVPR, 2022.\n[23] P . Esser, R. Rombach, and B. Ommer, “Taming transformers for\nhigh-resolution image synthesis,” in CVPR, 2021.\n[24] R. Cai, J. Yuan, B. Xu, and Z. Hao, “Sadga: Structure-aware dual\ngraph aggregation network for text-to-sql,” NeurIPS, 2021.\n[25] Y. Song, R. C.-W. Wong, X. Zhao, and D. Jiang, “Speech-to-\nsql: Towards speech-driven sql query generation from natural\nlanguage question,” arXiv, 2022.\n[26] A. Salvador, E. Gundogdu, L. Bazzani, and M. Donoser, “Re-\nvamping cross-modal recipe retrieval with hierarchical trans-\nformers and self-supervised learning,” in CVPR, 2021.\n[27] Z. Zhao, K. Samel, B. Chen et al., “Proto: Program-guided trans-\nformer for program-guided tasks,” in NeurIPS, 2021.\n[28] H. Zhou, W. Zhou, W. Qi, J. Pu, and H. Li, “Improving\nsign language translation with monolingual data by sign back-\ntranslation,” in CVPR, 2021.\n[29] G. Varol, L. Momeni, S. Albanie, T. Afouras, and A. Zisser-\nman, “Read and attend: Temporal localisation in sign language\nvideos,” in CVPR, 2021.\n[30] H. Bull, T. Afouras, G. Varol, S. Albanie, L. Momeni, and A. Zis-\nserman, “Aligning subtitles in sign language videos,”arXiv, 2021.\n[31] L. Zhao, D. Cai, L. Sheng, and D. Xu, “3dvg-transformer: Relation\nmodeling for visual grounding on point clouds,” in ICCV, 2021.\n[32] K. Marino, X. Chen, D. Parikh, A. Gupta, and M. Rohrbach,\n“Krisp: Integrating implicit and symbolic knowledge for open-\ndomain knowledge-based vqa,” in CVPR, 2021.\n[33] P . Ammanabrolu and M. O. Riedl, “Learning knowledge graph-\nbased world models of textual environments,” arXiv, 2021.\n[34] X. Zhu, Z. Li, X. Wang, X. Jiang, P . Sun, X. Wang, Y. Xiao, and\nN. J. Yuan, “Multi-modal knowledge graph construction and\napplication: A survey,” arXiv, 2022.\n[35] P . Xu, Y. Huang, T. Yuan, K. Pang, Y.-Z. Song, T. Xiang, T. M.\nHospedales, Z. Ma, and J. Guo, “Sketchmate: Deep hashing for\nmillion-scale human sketch retrieval,” in CVPR, 2018.\n[36] P . Xu, Z. Song, Q. Yin, Y.-Z. Song, and L. Wang, “Deep self-\nsupervised representation learning for free-hand sketch,”TCSVT,\n2020.\n[37] P . Xu, K. Liu, T. Xiang, T. M. Hospedales, Z. Ma, J. Guo, and Y.-Z.\nSong, “Fine-grained instance-level sketch-based video retrieval,”\nTCSVT, 2020.\n[38] Y. Vinker, E. Pajouheshgar, J. Y. Bo, R. C. Bachmann, A. H.\nBermano, D. Cohen-Or, A. Zamir, and A. Shamir, “Clipasso:\nSemantically-aware object sketching,” arXiv, 2022.\n[39] Y. Fan, Z. Lin, J. Saito, W. Wang, and T. Komura, “Faceformer:\nSpeech-driven 3d facial animation with transformers,” arXiv,\n2021.\n[40] D. Shin, Z. Ren, E. B. Sudderth, and C. C. Fowlkes, “3d scene re-\nconstruction with multi-layer depth and epipolar transformers,”\nin ICCV, 2019.\n[41] K. Lin, L. Wang, and Z. Liu, “End-to-end human pose and mesh\nreconstruction with transformers,” in CVPR, 2021.\n[42] Y. Xu, Y. Xu, T. Lv, L. Cui, F. Wei, G. Wang, Y. Lu, D. Florencio,\nC. Zhang, W. Che et al., “Layoutlmv2: Multi-modal pre-training\nfor visually-rich document understanding,” arXiv, 2020.\n[43] I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long-\ndocument transformer,” arXiv, 2020.\n[44] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan,\nA. Svyatkovskiy, S. Fu et al., “Graphcodebert: Pre-training code\nrepresentations with data flow,” arXiv, 2020.\n[45] D. Z ¨ugner, T. Kirschstein, M. Catasta, J. Leskovec, and\nS. G ¨unnemann, “Language-agnostic representation learning of\nsource code from structure and context,” arXiv, 2021.\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2023.3275156\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n16\n[46] K. Gavrilyuk, R. Sanford, M. Javan, and C. G. Snoek, “Actor-\ntransformers for group activity recognition,” in CVPR, 2020.\n[47] J. Shang, T. Ma, C. Xiao, and J. Sun, “Pre-training of graph aug-\nmented transformers for medication recommendation,” arXiv,\n2019.\n[48] T. Lin, Y. Wang, X. Liu, and X. Qiu, “A survey of transformers,”\narXiv, 2021.\n[49] Y. Tay, M. Dehghani, D. Bahri, and D. Metzler, “Efficient trans-\nformers: A survey,” arXiv, 2020.\n[50] A. M. Bras ¸oveanu and R. Andonie, “Visualizing transformers\nfor nlp: a brief survey,” in International Conference Information\nVisualisation (IV), 2020.\n[51] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and\nM. Shah, “Transformers in vision: A survey,” arXiv, 2021.\n[52] Y. Liu, Y. Zhang, Y. Wang, F. Hou, J. Yuan, J. Tian, Y. Zhang, Z. Shi,\nJ. Fan, and Z. He, “A survey of visual transformers,” arXiv, 2021.\n[53] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang,\nA. Xiao, C. Xu, Y. Xu et al. , “A survey on vision transformer,”\narXiv, 2020.\n[54] Y. Xu, H. Wei, M. Lin, Y. Deng, K. Sheng, M. Zhang, F. Tang,\nW. Dong, F. Huang, and C. Xu, “Transformers in computational\nvisual media: A survey,” Computational Visual Media, 2022.\n[55] F. Shamshad, S. Khan, S. W. Zamir, M. H. Khan, M. Hayat, F. S.\nKhan, and H. Fu, “Transformers in medical imaging: A survey,”\narXiv, 2022.\n[56] J. Selva, A. S. Johansen, S. Escalera, K. Nasrollahi, T. B. Moeslund,\nand A. Clap ´es, “Video transformers: A survey,” arXiv, 2022.\n[57] L. Ruan and Q. Jin, “Survey: Transformer based video-language\npre-training,” arXiv, 2021.\n[58] F. Chen, D. Zhang, M. Han, X. Chen, J. Shi, S. Xu, and B. Xu,\n“Vlp: A survey on vision-language pre-training,” arXiv, 2022.\n[59] F. Li, H. Zhang, Y.-F. Zhang, S. Liu, J. Guo, L. M. Ni, P . Zhang, and\nL. Zhang, “Vision-language intelligence: Tasks, representation\nlearning, and large models,” arXiv, 2022.\n[60] L. Wu, S. L. Oviatt, and P . R. Cohen, “Multimodal integration-a\nstatistical view,” TMM, 1999.\n[61] W. Guo, J. Wang, and S. Wang, “Deep multimodal representation\nlearning: A survey,” IEEE Access, 2019.\n[62] B. P . Yuhas, M. H. Goldstein, and T. J. Sejnowski, “Integration of\nacoustic and visual speech signals using neural networks,” IEEE\nCommunications Magazine, 1989.\n[63] A. A. Lazarus et al., Multimodal behavior therapy. Springer, 1976.\n[64] D. Feng, C. Haase-Sch ¨utz, L. Rosenbaum, H. Hertlein, C. Glaeser,\nF. Timm, W. Wiesbeck, and K. Dietmayer, “Deep multi-modal\nobject detection and semantic segmentation for autonomous\ndriving: Datasets, methods, and challenges,” TITS, 2020.\n[65] Y. Liu, J. Zhang, L. Fang, Q. Jiang, and B. Zhou, “Multimodal\nmotion prediction with stacked transformers,” in CVPR, 2021.\n[66] A. Moudgil, A. Majumdar, H. Agrawal, S. Lee, and D. Batra,\n“Soat: A scene-and object-aware transformer for vision-and-\nlanguage navigation,” NeurIPS, 2021.\n[67] F. Lv, X. Chen, Y. Huang, L. Duan, and G. Lin, “Progressive\nmodality reinforcement for human multimodal emotion recog-\nnition from unaligned multimodal sequences,” in CVPR, 2021.\n[68] R. Zellers, X. Lu, J. Hessel, Y. Yu, J. S. Park, J. Cao, A. Farhadi, and\nY. Choi, “Merlot: Multimodal neural script knowledge models,”\narXiv, 2021.\n[69] M. K. Hasan, S. Lee, W. Rahman, A. Zadeh, R. Mihalcea, L.-\nP . Morency, and E. Hoque, “Humor knowledge enriched trans-\nformer for understanding multimodal humor,” in AAAI, 2021.\n[70] A. Brown, V . Kalogeiton, and A. Zisserman, “Face, body, voice:\nVideo person-clustering with multiple modalities,” arXiv, 2021.\n[71] L. Yu, J. Chen, A. Sinha, M. M. Wang, H. Chen, T. L. Berg, and\nN. Zhang, “Commercemm: Large-scale commerce multimodal\nrepresentation learning with omni retrieval,” arXiv, 2022.\n[72] K. Chen, J. K. Chen, J. Chuang, M. V ´azquez, and S. Savarese,\n“Topological planning with transformers for vision-and-\nlanguage navigation,” in CVPR, 2021.\n[73] Y. Hong, Q. Wu, Y. Qi, C. Rodriguez-Opazo, and S. Gould, “Vln\nbert: A recurrent vision-and-language bert for navigation,” in\nCVPR, 2021.\n[74] J. Zhang, J. Fan, J. Peng et al., “Curriculum learning for vision-\nand-language navigation,” in NeurIPS, 2021.\n[75] Y. Qi, Z. Pan, Y. Hong, M.-H. Yang, A. van den Hengel, and\nQ. Wu, “The road to know-where: An object-and-room informed\nsequential bert for indoor vision-language navigation,” in ICCV,\n2021.\n[76] C. Chen, Z. Al-Halah, and K. Grauman, “Semantic audio-visual\nnavigation,” in CVPR, 2021.\n[77] S. Ren, Y. Du, J. Lv, G. Han, and S. He, “Learning from the master:\nDistilling cross-modal advanced knowledge for lip reading,” in\nCVPR, 2021.\n[78] P . Xu, T. M. Hospedales, Q. Yin, Y.-Z. Song, T. Xiang, and L. Wang,\n“Deep learning for free-hand sketch: A survey,” TP AMI, 2022.\n[79] Y. Li, S. Rao, J. R. A. Solares, A. Hassaine, R. Ramakrishnan,\nD. Canoy, Y. Zhu, K. Rahimi, and G. Salimi-Khorshidi, “Behrt:\ntransformer for electronic health records,” Scientific reports, 2020.\n[80] Y. Li, H. Wang, and Y. Luo, “A comparison of pre-trained vision-\nand-language models for multimodal representation learning\nacross medical images and reports,” in BIBM, 2020.\n[81] P . Xu and X. Zhu, “Deepchange: A large long-term person re-\nidentification benchmark with clothes change,” arXiv, 2021.\n[82] M. Tsimpoukelli, J. Menick, S. Cabi, S. Eslami, O. Vinyals, and\nF. Hill, “Multimodal few-shot learning with frozen language\nmodels,” NeurIPS, 2021.\n[83] Y.-L. Sung, J. Cho, and M. Bansal, “Vl-adapter: Parameter-\nefficient transfer learning for vision-and-language tasks,” in\nCVPR, 2022.\n[84] J.-B. Alayrac, J. Donahue, P . Luc, A. Miech, I. Barr, Y. Hasson,\nK. Lenc, A. Mensch, K. Millican, M. Reynolds et al., “Flamingo: a\nvisual language model for few-shot learning,” NeurIPS, 2022.\n[85] W. Wang, H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Aggarwal,\nO. K. Mohammed, S. Singhal, S. Som et al., “Image as a foreign\nlanguage: Beit pretraining for all vision and vision-language\ntasks,” arXiv, 2022.\n[86] X. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P . Padlewski,\nD. Salz, S. Goodman, A. Grycner, B. Mustafa, L. Beyeret al., “Pali:\nA jointly-scaled multilingual language-image model,” arXiv,\n2022.\n[87] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed,\nO. Levy, V . Stoyanov, and L. Zettlemoyer, “Bart: Denoising\nsequence-to-sequence pre-training for natural language genera-\ntion, translation, and comprehension,” arXiv, 2019.\n[88] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Im-\nproving language understanding by generative pre-training,”\n2018.\n[89] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V . Le, and R. Salakhutdi-\nnov, “Transformer-xl: Attentive language models beyond a fixed-\nlength context,” arXiv, 2019.\n[90] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov,\nand Q. V . Le, “Xlnet: Generalized autoregressive pretraining for\nlanguage understanding,” NeurIPS, 2019.\n[91] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and\nI. Sutskever, “Generative pretraining from pixels,” inICML, 2020.\n[92] H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma, C. Xu,\nC. Xu, and W. Gao, “Pre-trained image processing transformer,”\nin CVPR, 2021.\n[93] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and\nH. J ´egou, “Training data-efficient image transformers & distilla-\ntion through attention,” in ICML, 2021.\n[94] J. Beal, E. Kim, E. Tzeng, D. H. Park, A. Zhai, and D. Kislyuk,\n“Toward transformer-based object detection,” arXiv, 2020.\n[95] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,\n“Swin transformer: Hierarchical vision transformer using shifted\nwindows,” arXiv, 2021.\n[96] X. Chen, S. Xie, and K. He, “An empirical study of training self-\nsupervised vision transformers,” arXiv, 2021.\n[97] M. Caron, H. Touvron, I. Misra, H. J ´egou, J. Mairal, P . Bojanowski,\nand A. Joulin, “Emerging properties in self-supervised vision\ntransformers,” arXiv, 2021.\n[98] H. Bao, L. Dong, and F. Wei, “Beit: Bert pre-training of image\ntransformers,” arXiv, 2021.\n[99] S. Paul and P .-Y. Chen, “Vision transformers are robust learners,”\narXiv, 2021.\n[100] M. Raghu, T. Unterthiner, S. Kornblith, C. Zhang, and A. Doso-\nvitskiy, “Do vision transformers see like convolutional neural\nnetworks?” NeurIPS, 2021.\n[101] S. Cao, P . Xu, and D. A. Clifton, “How to understand masked\nautoencoders,” arXiv, 2022.\n[102] J. Lu, D. Batra, D. Parikh, and S. Lee, “Vilbert: Pretraining task-\nagnostic visiolinguistic representations for vision-and-language\ntasks,” arXiv, 2019.\n[103] H. Tan and M. Bansal, “Lxmert: Learning cross-modality encoder\nrepresentations from transformers,” arXiv, 2019.\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2023.3275156\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n17\n[104] L. H. Li, M. Yatskar, D. Yin, C.-J. Hsieh, and K.-W. Chang,\n“Visualbert: A simple and performant baseline for vision and\nlanguage,” arXiv, 2019.\n[105] W. Su, X. Zhu, Y. Cao, B. Li, L. Lu, F. Wei, and J. Dai, “Vl-bert:\nPre-training of generic visual-linguistic representations,” arXiv,\n2019.\n[106] Y.-C. Chen, L. Li, L. Yu, A. El Kholy, F. Ahmed, Z. Gan, Y. Cheng,\nand J. Liu, “Uniter: Universal image-text representation learn-\ning,” in ECCV, 2020.\n[107] C. Sun, F. Baradel, K. Murphy, and C. Schmid, “Learning video\nrepresentations using contrastive bidirectional transformer,”\narXiv, 2019.\n[108] G. Li, N. Duan, Y. Fang, M. Gong, and D. Jiang, “Unicoder-vl: A\nuniversal encoder for vision and language by cross-modal pre-\ntraining,” in AAAI, 2020.\n[109] C. Alberti, J. Ling, M. Collins, and D. Reitter, “Fusion of detected\nobjects in text for visual question answering,” arXiv, 2019.\n[110] L. Zhou, H. Palangi, L. Zhang, H. Hu, J. Corso, and J. Gao,\n“Unified vision-language pre-training for image captioning and\nvqa,” in AAAI, 2020.\n[111] J. Lu, V . Goswami, M. Rohrbach, D. Parikh, and S. Lee, “12-in-\n1: Multi-task vision and language representation learning,” in\nCVPR, 2020.\n[112] X. Li, X. Yin, C. Li, P . Zhang, X. Hu, L. Zhang, L. Wang, H. Hu,\nL. Dong, F. Wei et al. , “Oscar: Object-semantics aligned pre-\ntraining for vision-language tasks,” in ECCV, 2020.\n[113] Z. Huang, Z. Zeng, B. Liu, D. Fu, and J. Fu, “Pixel-bert: Aligning\nimage pixels with text by deep multi-modal transformers,” arXiv,\n2020.\n[114] L. Zhu and Y. Yang, “Actbert: Learning global-local video-text\nrepresentations,” in CVPR, 2020.\n[115] D. Qi, L. Su, J. Song, E. Cui, T. Bharti, and A. Sacheti, “Image-\nbert: Cross-modal pre-training with large-scale weak-supervised\nimage-text data,” arXiv, 2020.\n[116] L. Li, Y.-C. Chen, Y. Cheng, Z. Gan, L. Yu, and J. Liu, “Hero:\nHierarchical encoder for video+ language omni-representation\npre-training,” arXiv, 2020.\n[117] H. Luo, L. Ji, B. Shi, H. Huang, N. Duan, T. Li, J. Li, T. Bharti,\nand M. Zhou, “Univl: A unified video and language pre-training\nmodel for multimodal understanding and generation,” arXiv,\n2020.\n[118] M. Xu, Z. Zhang, F. Wei, Y. Lin, Y. Cao, H. Hu, and X. Bai, “A\nsimple baseline for zero-shot semantic segmentation with pre-\ntrained vision-language model,” arXiv, 2021.\n[119] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. V .\nLe, Y. Sung, Z. Li, and T. Duerig, “Scaling up visual and vision-\nlanguage representation learning with noisy text supervision,”\narXiv, 2021.\n[120] Z. Wang, N. Codella, Y.-C. Chen, L. Zhou, J. Yang, X. Dai,\nB. Xiao, H. You, S.-F. Chang, and L. Yuan, “Clip-td: Clip targeted\ndistillation for vision-language tasks,” arXiv, 2022.\n[121] J. Li, R. Selvaraju, A. Gotmare, S. Joty, C. Xiong, and S. C. H. Hoi,\n“Align before fuse: Vision and language representation learning\nwith momentum distillation,” NeurIPS, 2021.\n[122] J. Yu, Z. Wang, V . Vasudevan, L. Yeung, M. Seyedhosseini, and\nY. Wu, “Coca: Contrastive captioners are image-text foundation\nmodels,” arXiv, 2022.\n[123] P . Sharma, N. Ding, S. Goodman, and R. Soricut, “Conceptual\ncaptions: A cleaned, hypernymed, image alt-text dataset for\nautomatic image captioning,” in ACL, 2018.\n[124] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P . Perona, D. Ramanan,\nP . Doll´ar, and C. L. Zitnick, “Microsoft coco: Common objects in\ncontext,” in ECCV, 2014.\n[125] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick,\nand D. Parikh, “Vqa: Visual question answering,” in ICCV, 2015.\n[126] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz,\nS. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma et al. , “Visual\ngenome: Connecting language and vision using crowdsourced\ndense image annotations,” IJCV, 2017.\n[127] V . Ordonez, G. Kulkarni, and T. Berg, “Im2text: Describing im-\nages using 1 million captioned photographs,” NeurIPS, 2011.\n[128] M. Kayser, O.-M. Camburu, L. Salewski, C. Emde, V . Do,\nZ. Akata, and T. Lukasiewicz, “e-vil: A dataset and benchmark\nfor natural language explanations in vision-language tasks,”\narXiv, 2021.\n[129] J. Gamper and N. Rajpoot, “Multiple instance captioning: Learn-\ning representations from histopathology textbooks and articles,”\nin CVPR, 2021.\n[130] L. Li, J. Lei, Z. Gan, and J. Liu, “Adversarial vqa: A new\nbenchmark for evaluating the robustness of vqa models,” arXiv,\n2021.\n[131] A. Talmor, O. Yoran, A. Catav, D. Lahav, Y. Wang, A. Asai,\nG. Ilharco, H. Hajishirzi, and J. Berant, “Multimodalqa: Complex\nquestion answering over text, tables and images,” arXiv, 2021.\n[132] L. Li, J. Lei, Z. Gan, L. Yu, Y.-C. Chen, R. Pillai, Y. Cheng, L. Zhou,\nX. E. Wang, W. Y. Wanget al., “Value: A multi-task benchmark for\nvideo-and-language understanding evaluation,” arXiv, 2021.\n[133] H. Wu, Y. Gao, X. Guo, Z. Al-Halah, S. Rennie, K. Grauman, and\nR. Feris, “Fashion iq: A new dataset towards retrieving images\nby natural language feedback,” in CVPR, 2021.\n[134] T. Afouras, J. S. Chung, A. Senior, O. Vinyals, and A. Zisserman,\n“Deep audio-visual speech recognition,” TP AMI, 2018.\n[135] R. Krishna, K. Hata, F. Ren, L. Fei-Fei, and J. Carlos Niebles,\n“Dense-captioning events in videos,” in ICCV, 2017.\n[136] A. Das, S. Kottur, K. Gupta, A. Singh, D. Yadav, J. M. Moura,\nD. Parikh, and D. Batra, “Visual dialog,” in CVPR, 2017.\n[137] X. Zhan, Y. Wu, X. Dong, Y. Wei, M. Lu, Y. Zhang, H. Xu, and\nX. Liang, “Product1m: Towards weakly supervised instance-level\nproduct retrieval via cross-modal pretraining,” in ICCV, 2021.\n[138] S. Changpinyo, P . Sharma, N. Ding, and R. Soricut, “Conceptual\n12m: Pushing web-scale image-text pre-training to recognize\nlong-tail visual concepts,” in CVPR, 2021.\n[139] Y. Huo, M. Zhang, G. Liu, H. Lu, Y. Gao, G. Yang, J. Wen,\nH. Zhang, B. Xu, W. Zheng et al., “Wenlan: Bridging vision and\nlanguage by large-scale multi-modal pre-training,” arXiv, 2021.\n[140] A. Yang, A. Miech, J. Sivic, I. Laptev, and C. Schmid, “Just ask:\nLearning to answer questions from millions of narrated videos,”\nin ICCV, 2021.\n[141] A. Miech, D. Zhukov, J.-B. Alayrac, M. Tapaswi, I. Laptev,\nand J. Sivic, “Howto100m: Learning a text-video embedding by\nwatching hundred million narrated video clips,” in ICCV, 2019.\n[142] X. Hu, Z. Gan, J. Wang, Z. Yang, Z. Liu, Y. Lu, and L. Wang,\n“Scaling up vision-language pre-training for image captioning,”\narXiv, 2021.\n[143] C. Schuhmann, R. Vencu, R. Beaumont, R. Kaczmarczyk,\nC. Mullis, A. Katta, T. Coombes, J. Jitsev, and A. Komatsuzaki,\n“Laion-400m: Open dataset of clip-filtered 400 million image-text\npairs,” arXiv, 2021.\n[144] H. Yun, Y. Yu, W. Yang, K. Lee, and G. Kim, “Pano-avqa:\nGrounded audio-visual question answering on 360deg videos,”\nin ICCV, 2021.\n[145] P . Morgado, Y. Li, and N. Vasconcelos, “Learning representations\nfrom audio-visual spatial alignment,” arXiv, 2020.\n[146] R. Li, S. Yang, D. A. Ross, and A. Kanazawa, “Ai choreographer:\nMusic conditioned 3d dance generation with aist++,” in ICCV,\n2021.\n[147] P . Achlioptas, M. Ovsjanikov, K. Haydarov, M. Elhoseiny, and L. J.\nGuibas, “Artemis: Affective language for visual art,” in CVPR,\n2021.\n[148] P . P . Liang, Y. Lyu, X. Fan, Z. Wu, Y. Cheng, J. Wu, L. Y. Chen,\nP . Wu, M. A. Lee, Y. Zhu et al. , “Multibench: Multiscale bench-\nmarks for multimodal representation learning,” arXiv, 2021.\n[149] Z. Liu, C. Rodriguez-Opazo, D. Teney, and S. Gould, “Image\nretrieval on real-life images with pre-trained vision-and-language\nmodels,” in ICCV, 2021.\n[150] P .-L. Guhur, M. Tapaswi, S. Chen, I. Laptev, and C. Schmid, “Air-\nbert: In-domain pretraining for vision-and-language navigation,”\nin ICCV, 2021.\n[151] R. Sawhney, M. Goyal, P . Goel, P . Mathur, and R. Shah, “Multi-\nmodal multi-speaker merger & acquisition financial modeling: A\nnew task, dataset, and neural baselines,” in ACL-IJCNLP, 2021.\n[152] J. Zhang, M. Zheng, M. Boyd, and E. Ohn-Bar, “X-world: Acces-\nsibility, vision, and autonomy meet,” in ICCV, 2021.\n[153] D. Zhang, M. Zhang, H. Zhang, L. Yang, and H. Lin, “Multimet:\nA multimodal dataset for metaphor understanding,” in ACL-\nIJCNLP, 2021.\n[154] D. Kiela, H. Firooz, A. Mohan, V . Goswami, A. Singh, P . Ringshia,\nand D. Testuggine, “The hateful memes challenge: Detecting hate\nspeech in multimodal memes,” arXiv, 2020.\n[155] L. Zhou, C. Xu, and J. J. Corso, “Towards automatic learning of\nprocedures from web instructional videos,” in AAAI, 2018.\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2023.3275156\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n18\n[156] J. Malmaud, J. Huang, V . Rathod, N. Johnston, A. Rabinovich,\nand K. Murphy, “What’s cookin’? interpreting cooking videos\nusing text, speech and vision,” arXiv, 2015.\n[157] M. M. Bronstein, J. Bruna, T. Cohen, and P . Veliˇckovi´c, “Geometric\ndeep learning: Grids, groups, graphs, geodesics, and gauges,”\narXiv, 2021.\n[158] V . P . Dwivedi and X. Bresson, “A generalization of transformer\nnetworks to graphs,” arXiv, 2020.\n[159] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” in CVPR, 2016.\n[160] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift,” in ICML,\n2015.\n[161] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,”\narXiv, 2016.\n[162] R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang,\nY. Lan, L. Wang, and T. Liu, “On layer normalization in the\ntransformer architecture,” in ICML, 2020.\n[163] P . Xu, C. K. Joshi, and X. Bresson, “Multigraph transformer for\nfree-hand sketch recognition,” TNNLS, 2021.\n[164] M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin, and S.-M.\nHu, “Pct: Point cloud transformer,” Computational Visual Media ,\n2021.\n[165] S. He, H. Luo, P . Wang, F. Wang, H. Li, and W. Jiang, “Transreid:\nTransformer-based object re-identification,” in ICCV, 2021.\n[166] P . Dufter, M. Schmitt, and H. Sch ¨utze, “Position information in\ntransformers: An overview,” arXiv, 2021.\n[167] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural\nnetworks,” in CVPR, 2018.\n[168] L. Zhou, Y. Zhou, J. J. Corso, R. Socher, and C. Xiong, “End-to-\nend dense video captioning with masked transformer,” in CVPR,\n2018.\n[169] B. Wang, R. Shin, X. Liu, O. Polozov, and M. Richardson, “Rat-\nsql: Relation-aware schema encoding and linking for text-to-sql\nparsers,” arXiv, 2019.\n[170] Z. Wang, H. You, L. H. Li, A. Zareian, S. Park, Y. Liang, K.-W.\nChang, and S.-F. Chang, “Sgeitl: Scene graph enhanced image-\ntext learning for visual commonsense reasoning,” arXiv, 2021.\n[171] X. Glorot, A. Bordes, and Y. Bengio, “Deep sparse rectifier neural\nnetworks,” in AISTATS, 2011.\n[172] D. Hendrycks and K. Gimpel, “Gaussian error linear units\n(gelus),” arXiv, 2016.\n[173] Y.-B. Lin, H.-Y. Tseng, H.-Y. Lee, Y.-Y. Lin, and M.-H. Yang,\n“Exploring cross-video and cross-modality signals for weakly-\nsupervised audio-visual video parsing,” NeurIPS, 2021.\n[174] H. Akbari, L. Yuan, R. Qian, W.-H. Chuang, S.-F. Chang, Y. Cui,\nand B. Gong, “Vatt: Transformers for multimodal self-supervised\nlearning from raw video, audio and text,” arXiv, 2021.\n[175] A. Nagrani, S. Yang, A. Arnab, A. Jansen, C. Schmid, and C. Sun,\n“Attention bottlenecks for multimodal fusion,” NeurIPS, 2021.\n[176] P .-A. Duquenne, H. Gong, and H. Schwenk, “Multimodal\nand multilingual embeddings for large-scale speech mining,”\nNeurIPS, 2021.\n[177] D. Min, D. B. Lee, E. Yang, and S. J. Hwang, “Meta-stylespeech:\nMulti-speaker adaptive text-to-speech generation,” arXiv, 2021.\n[178] B. Shi, W.-N. Hsu, K. Lakhotia, and A. Mohamed, “Learning\naudio-visual speech representation by masked multimodal clus-\nter prediction,” arXiv, 2022.\n[179] K. Ramesh, C. Xing, W. Wang, D. Wang, and X. Chen, “Vset:\nA multimodal transformer for visual speech enhancement,” in\nICASSP, 2021.\n[180] R. Zheng, J. Chen, M. Ma, and L. Huang, “Fused acoustic and\ntext encoding for multimodal bilingual pretraining and speech\ntranslation,” arXiv, 2021.\n[181] X. Yang, S. Feng, Y. Zhang, and D. Wang, “Multimodal sentiment\ndetection based on multi-channel graph neural networks,” in\nACL-IJCNLP, 2021.\n[182] X. Mao, G. Qi, Y. Chen, X. Li, R. Duan, S. Ye, Y. He, and H. Xue,\n“Towards robust vision transformer,” arXiv, 2021.\n[183] T. Rahman, M. Yang, and L. Sigal, “Tribert: Human-centric audio-\nvisual representation learning,” NeurIPS, 2021.\n[184] L. Rasmy, Y. Xiang, Z. Xie, C. Tao, and D. Zhi, “Med-bert:\npretrained contextualized embeddings on large-scale structured\nelectronic health records for disease prediction,” NPJ digital\nmedicine, 2021.\n[185] R. J. Chen, M. Y. Lu, W.-H. Weng, T. Y. Chen, D. F. Williamson,\nT. Manz, M. Shady, and F. Mahmood, “Multimodal co-attention\ntransformer for survival prediction in gigapixel whole slide im-\nages,” in ICCV, 2021.\n[186] S. Xie, C. Sun, J. Huang, Z. Tu, and K. Murphy, “Rethinking\nspatiotemporal feature learning for video understanding,” arXiv,\n2017.\n[187] D. Tran, H. Wang, L. Torresani, J. Ray, Y. LeCun, and M. Paluri,\n“A closer look at spatiotemporal convolutions for action recogni-\ntion,” in CVPR, 2018.\n[188] J. Lin, A. Yang, Y. Zhang, J. Liu, J. Zhou, and H. Yang, “Interbert:\nVision-and-language interaction for multi-modal pretraining,”\narXiv, 2020.\n[189] Y.-H. H. Tsai, S. Bai, P . P . Liang, J. Z. Kolter, L.-P . Morency,\nand R. Salakhutdinov, “Multimodal transformer for unaligned\nmultimodal language sequences,” in ACL, 2019.\n[190] V . Murahari, D. Batra, D. Parikh, and A. Das, “Large-scale pre-\ntraining for visual dialog: A simple state-of-the-art baseline,” in\nECCV, 2020.\n[191] Y. Li, Y. Pan, T. Yao, J. Chen, and T. Mei, “Scheduled sampling\nin vision-language pretraining with decoupled encoder-decoder\nnetwork,” in AAAI, 2021.\n[192] W. Kim, B. Son, and I. Kim, “Vilt: Vision-and-language trans-\nformer without convolution or region supervision,” in ICML,\n2021.\n[193] J. Yang, J. Duan, S. Tran, Y. Xu, S. Chanda, L. Chen, B. Zeng,\nT. Chilimbi, and J. Huang, “Vision-language pre-training with\ntriple contrastive learning,” arXiv, 2022.\n[194] L. H. Li, P . Zhang, H. Zhang, J. Yang, C. Li, Y. Zhong, L. Wang,\nL. Yuan, L. Zhang, J.-N. Hwanget al., “Grounded language-image\npre-training,” in CVPR, 2022.\n[195] H. Zhang, P . Zhang, X. Hu, Y.-C. Chen, L. Li, X. Dai, L. Wang,\nL. Yuan, J.-N. Hwang, and J. Gao, “Glipv2: Unifying localization\nand vision-language understanding,” NeurIPS, 2022.\n[196] C. Li, M. Yan, H. Xu, F. Luo, W. Wang, B. Bi, and S. Huang,\n“Semvlp: Vision-language pre-training by aligning semantics at\nmultiple levels,” arXiv, 2021.\n[197] A. Miech, J.-B. Alayrac, L. Smaira, I. Laptev, J. Sivic, and A. Zis-\nserman, “End-to-end learning of visual representations from\nuncurated instructional videos,” in CVPR, 2020.\n[198] T. Han, W. Xie, and A. Zisserman, “Temporal alignment networks\nfor long-term video,” in CVPR, 2022.\n[199] Z. Wang, J. Yu, A. W. Yu, Z. Dai, Y. Tsvetkov, and Y. Cao,\n“Simvlm: Simple visual language model pretraining with weak\nsupervision,” arXiv, 2021.\n[200] J. Chen, M. Ma, R. Zheng, and L. Huang, “Mam: Masked acoustic\nmodeling for end-to-end speech-to-text translation,” arXiv, 2020.\n[201] M. Golestani, S. Z. Razavi, Z. Borhanifard, F. Tahmasebian, and\nH. Faili, “Using bert encoding and sentence-level language model\nfor sentence ordering,” in TSD, 2021.\n[202] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-\ntime object detection with region proposal networks,” NeurIPS,\n2015.\n[203] Z. Huang, Z. Zeng, Y. Huang, B. Liu, D. Fu, and J. Fu, “Seeing\nout of the box: End-to-end pre-training for vision-language rep-\nresentation learning,” in CVPR, 2021.\n[204] Y. Liu, C. Wu, S.-y. Tseng, V . Lal, X. He, and N. Duan, “Kd-\nvlp: Improving end-to-end vision-and-language pretraining with\nobject knowledge distillation,” arXiv, 2021.\n[205] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford,\nM. Chen, and I. Sutskever, “Zero-shot text-to-image generation,”\narXiv, 2021.\n[206] M. Zhou, L. Zhou, S. Wang, Y. Cheng, L. Li, Z. Yu, and J. Liu,\n“Uc2: Universal cross-lingual cross-modal vision-and-language\npre-training,” in CVPR, 2021.\n[207] W. Hao, C. Li, X. Li, L. Carin, and J. Gao, “Towards learning\na generic agent for vision-and-language navigation via pre-\ntraining,” in CVPR, 2020.\n[208] Q. Xia, H. Huang, N. Duan, D. Zhang, L. Ji, Z. Sui, E. Cui,\nT. Bharti, and M. Zhou, “Xgpt: Cross-modal generative pre-\ntraining for image captioning,” in NLPCC, 2021.\n[209] L. Yao, R. Huang, L. Hou, G. Lu, M. Niu, H. Xu, X. Liang, Z. Li,\nX. Jiang, and C. Xu, “Filip: Fine-grained interactive language-\nimage pre-training,” arXiv, 2021.\n[210] Z. Gan, Y.-C. Chen, L. Li, C. Zhu, Y. Cheng, and J. Liu, “Large-\nscale adversarial training for vision-and-language representation\nlearning,” arXiv, 2020.\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2023.3275156\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n19\n[211] H. Zhang, W. Yin, Y. Fang, L. Li, B. Duan, Z. Wu, Y. Sun, H. Tian,\nH. Wu, and H. Wang, “Ernie-vilg: Unified generative pre-training\nfor bidirectional vision-language generation,” arXiv, 2021.\n[212] M. Zhuge, D. Gao, D.-P . Fan, L. Jin, B. Chen, H. Zhou, M. Qiu, and\nL. Shao, “Kaleido-bert: Vision-language pre-training on fashion\ndomain,” in CVPR, 2021.\n[213] Y. Wang, X. Chen, L. Cao, W. Huang, F. Sun, and Y. Wang,\n“Multimodal token fusion for vision transformers,” in CVPR,\n2022.\n[214] Y. Wang, T. Ye, L. Cao, W. Huang, F. Sun, F. He, and D. Tao,\n“Bridged transformer for vision and point cloud 3d object detec-\ntion,” in CVPR, 2022.\n[215] A. Prakash, K. Chitta, and A. Geiger, “Multi-modal fusion trans-\nformer for end-to-end autonomous driving,” in CVPR, 2021.\n[216] X. Bai, Z. Hu, X. Zhu, Q. Huang, Y. Chen, H. Fu, and C.-L. Tai,\n“Transfusion: Robust lidar-camera fusion for 3d object detection\nwith transformers,” in CVPR, 2022.\n[217] X. Favory, K. Drossos, T. Virtanen, and X. Serra, “Learning\ncontextual tag embeddings for cross-modal alignment of audio\nand tags,” in ICASSP, 2021.\n[218] V . Gabeur, C. Sun, K. Alahari, and C. Schmid, “Multi-modal\ntransformer for video retrieval,” in ECCV, 2020.\n[219] N. Shvetsova, B. Chen, A. Rouditchenko, S. Thomas, B. Kings-\nbury, R. S. Feris, D. Harwath, J. Glass, and H. Kuehne, “Ev-\nerything at once - multi-modal fusion transformer for video\nretrieval,” in CVPR, 2022.\n[220] Z. Wang, Y. Wu, K. Narasimhan, and O. Russakovsky, “Multi-\nquery video retrieval,” arXiv, 2022.\n[221] A. Botach, E. Zheltonozhskii, and C. Baskin, “End-to-end refer-\nring video object segmentation with multimodal transformers,”\narXiv, 2021.\n[222] W. Hong, K. Ji, J. Liu, J. Wang, J. Chen, and W. Chu, “Gilbert:\nGenerative vision-language pre-training for image-text retrieval,”\nin SIGIR, 2021.\n[223] X. Xu and C. C. Loy, “3d human texture estimation from a single\nimage with transformers,” in ICCV, 2021.\n[224] X. Lin, C. Ding, J. Zeng, and D. Tao, “Gps-net: Graph property\nsensing network for scene graph generation,” in CVPR, 2020.\n[225] W. Wang, R. Wang, and X. Chen, “Topic scene graph generation\nby attention distillation from caption,” in ICCV, 2021.\n[226] Y. Lu, H. Rai, J. Chang, B. Knyazev, G. Yu, S. Shekhar, G. W.\nTaylor, and M. Volkovs, “Context-aware scene graph generation\nwith seq2seq transformers,” in ICCV, 2021.\n[227] P . Ke, H. Ji, Y. Ran, X. Cui, L. Wang, L. Song, X. Zhu, and\nM. Huang, “Jointgt: Graph-text joint representation learning for\ntext generation from knowledge graphs,” arXiv, 2021.\n[228] Y. Teng, L. Wang, Z. Li, and G. Wu, “Target adaptive context\naggregation for video scene graph generation,” in ICCV, 2021.\n[229] M. Chen, Y. Li, Z. Zhang, and S. Huang, “Tvt: Two-view trans-\nformer network for video captioning,” in ACML, 2018.\n[230] K. Lin, L. Li, C.-C. Lin, F. Ahmed, Z. Gan, Z. Liu, Y. Lu,\nand L. Wang, “Swinbert: End-to-end transformers with sparse\nattention for video captioning,” arXiv, 2021.\n[231] C. Deng, S. Chen, D. Chen, Y. He, and Q. Wu, “Sketch, ground,\nand refine: Top-down dense video captioning,” in CVPR, 2021.\n[232] T. Wang, R. Zhang, Z. Lu, F. Zheng, R. Cheng, and P . Luo, “End-\nto-end dense video captioning with parallel decoding,” in ICCV,\n2021.\n[233] L. Huang, W. Wang, J. Chen, and X.-Y. Wei, “Attention on\nattention for image captioning,” in ICCV, 2019.\n[234] Y. Pan, T. Yao, Y. Li, and T. Mei, “X-linear attention networks for\nimage captioning,” in CVPR, 2020.\n[235] X. Yang, H. Zhang, G. Qi, and J. Cai, “Causal attention for vision-\nlanguage tasks,” in CVPR, 2021.\n[236] Y. Luo, J. Ji, X. Sun, L. Cao, Y. Wu, F. Huang, C.-W. Lin, and\nR. Ji, “Dual-level collaborative transformer for image caption-\ning,” arXiv, 2021.\n[237] G. Xu, S. Niu, M. Tan, Y. Luo, Q. Du, and Q. Wu, “Towards\naccurate text-based image captioning with content diversity ex-\nploration,” in CVPR, 2021.\n[238] N. Li, S. Liu, Y. Liu, S. Zhao, and M. Liu, “Neural speech synthesis\nwith transformer network,” in AAAI, 2019.\n[239] M. Ding, Z. Yang, W. Hong, W. Zheng, C. Zhou, D. Yin, J. Lin,\nX. Zou, Z. Shao, H. Yang et al. , “Cogview: Mastering text-to-\nimage generation via transformers,” arXiv, 2021.\n[240] A. Sanghi, H. Chu, J. G. Lambourne, Y. Wang, C.-Y. Cheng,\nand M. Fumero, “Clip-forge: Towards zero-shot text-to-shape\ngeneration,” arXiv, 2021.\n[241] R. Huang, H. Hu, W. Wu, K. Sawada, M. Zhang, and D. Jiang,\n“Dance revolution: Long-term dance generation with music via\ncurriculum learning,” arXiv, 2020.\n[242] Y. Zhong, J. Shi, J. Yang, C. Xu, and Y. Li, “Learning to generate\nscene graph from natural language supervision,” in ICCV, 2021.\n[243] S. Geng, P . Gao, M. Chatterjee, C. Hori, J. Le Roux, Y. Zhang,\nH. Li, and A. Cherian, “Dynamic graph representation learn-\ning for video dialog via multi-modal shuffled transformers,” in\nAAAI, 2021.\n[244] T. Jaunet, C. Kervadec, R. Vuillemot, G. Antipov, M. Baccouche,\nand C. Wolf, “Visqa: X-raying vision and language reasoning in\ntransformers,” TVCG, 2021.\n[245] X. Lin, G. Bertasius, J. Wang, S.-F. Chang, D. Parikh, and L. Tor-\nresani, “Vx2text: End-to-end learning of video-based text gener-\nation from multimodal inputs,” in CVPR, 2021.\n[246] J. Lin, R. Men, A. Yang, C. Zhou, Y. Zhang, P . Wang, J. Zhou,\nJ. Tang, and H. Yang, “M6: Multi-modality-to-multi-modality\nmultitask mega-transformer for unified pretraining,” in KDD,\n2021.\n[247] T. Chen and R. R. Rao, “Audio-visual integration in multimodal\ncommunication,” Proceedings of the IEEE, 1998.\n[248] A. Owens and A. A. Efros, “Audio-visual scene analysis with\nself-supervised multisensory features,” in ECCV, 2018.\n[249] H. Xue, Y. Huang, B. Liu, H. Peng, J. Fu, H. Li, and J. Luo,\n“Probing inter-modality: Visual parsing with self-attention for\nvision-and-language pre-training,” NeurIPS, 2021.\n[250] T.-D. Truong, C. N. Duong, H. A. Pham, B. Raj, N. Le, K. Luu\net al., “The right to talk: An audio-visual transformer approach,”\nin ICCV, 2021.\n[251] M. Chen, X. Tan, Y. Ren, J. Xu, H. Sun, S. Zhao, T. Qin, and\nT.-Y. Liu, “Multispeech: Multi-speaker text to speech with trans-\nformer,” arXiv, 2020.\n[252] S. Ging, M. Zolfaghari, H. Pirsiavash, and T. Brox, “Coot: Co-\noperative hierarchical transformer for video-text representation\nlearning,” arXiv, 2020.\n[253] M. Patrick, P .-Y. Huang, Y. Asano, F. Metze, A. Hauptmann,\nJ. Henriques, and A. Vedaldi, “Support-set bottlenecks for video-\ntext representation learning,” arXiv, 2020.\n[254] V . Gabeur, A. Nagrani, C. Sun, K. Alahari, and C. Schmid,\n“Masking modalities for cross-modal video retrieval,” in WACV,\n2022.\n[255] A. Sadhu, K. Chen, and R. Nevatia, “Video object grounding\nusing semantic roles in language description,” in CVPR, 2020.\n[256] Y. Zhang, M. Choi, K. Han, and Z. Liu, “Explainable semantic\nspace by grounding language to vision with cross-modal con-\ntrastive learning,” NeurIPS, 2021.\n[257] Y.-W. Chen, Y.-H. Tsai, and M.-H. Yang, “End-to-end multi-modal\nvideo temporal grounding,” NeurIPS, 2021.\n[258] S. Chen and B. Li, “Multi-modal dynamic graph transformer for\nvisual grounding,” in CVPR, 2022.\n[259] A. Yang, A. Miech, J. Sivic, I. Laptev, and C. Schmid, “Tubedetr:\nSpatio-temporal video grounding with transformers,” in CVPR,\n2022.\n[260] H. Xu, G. Ghosh, P .-Y. Huang, D. Okhonko, A. Aghajanyan,\nF. Metze, L. Zettlemoyer, and C. Feichtenhofer, “Videoclip: Con-\ntrastive pre-training for zero-shot video-text understanding,”\narXiv, 2021.\n[261] J. Lei, L. Li, L. Zhou, Z. Gan, T. L. Berg, M. Bansal, and J. Liu,\n“Less is more: Clipbert for video-and-language learning via\nsparse sampling,” in CVPR, 2021.\n[262] J. Yang, Y. Bisk, and J. Gao, “Taco: Token-aware cascade con-\ntrastive learning for video-text alignment,” in ICCV, 2021.\n[263] D. Li, J. Li, H. Li, J. C. Niebles, and S. C. Hoi, “Align and\nprompt: Video-and-language pre-training with entity prompts,”\narXiv, 2021.\n[264] H. Luo, L. Ji, M. Zhong, Y. Chen, W. Lei, N. Duan, and T. Li,\n“Clip4clip: An empirical study of clip for end to end video clip\nretrieval,” arXiv, 2021.\n[265] H. Fang, P . Xiong, L. Xu, and Y. Chen, “Clip2video: Mastering\nvideo-text retrieval via image clip,” arXiv, 2021.\n[266] M. Narasimhan, A. Rohrbach, and T. Darrell, “Clip-it! language-\nguided video summarization,” NeurIPS, 2021.\n[267] P . Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre-\ntrain, prompt, and predict: A systematic survey of prompting\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2023.3275156\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n20\nmethods in natural language processing,” ACM Computing Sur-\nveys, 2023.\n[268] Y. Xian, C. H. Lampert, B. Schiele, and Z. Akata, “Zero-shot\nlearning—a comprehensive evaluation of the good, the bad and\nthe ugly,” TP AMI, 2018.\n[269] X. Gu, T.-Y. Lin, W. Kuo, and Y. Cui, “Open-vocabulary object\ndetection via vision and language knowledge distillation,” arXiv,\n2021.\n[270] J. Cho, J. Lu, D. Schwenk, H. Hajishirzi, and A. Kembhavi, “X-\nlxmert: Paint, caption and answer questions with multi-modal\ntransformers,” in EMNLP, 2020.\n[271] H. Xu, M. Yan, C. Li, B. Bi, S. Huang, W. Xiao, and F. Huang,\n“E2e-vlp: End-to-end vision-language pre-training enhanced by\nvisual learning,” arXiv, 2021.\n[272] C. Kervadec, C. Wolf, G. Antipov, M. Baccouche, and M. Nadri,\n“Supervising the transfer of reasoning patterns in vqa,” arXiv,\n2021.\n[273] C. Kervadec, T. Jaunet, G. Antipov, M. Baccouche, R. Vuillemot,\nand C. Wolf, “How transferable are reasoning patterns in vqa?”\nin CVPR, 2021.\n[274] W. Rahman, M. K. Hasan, S. Lee, A. Zadeh, C. Mao, L.-P .\nMorency, and E. Hoque, “Integrating multimodal information in\nlarge pretrained transformers,” in ACL, 2020.\n[275] D. Agarwal, T. Agrawal, L. M. Ferrari, and F. Bremond, “From\nmultimodal to unimodal attention in transformers using knowl-\nedge distillation,” in AVSS, 2021.\n[276] Q. Li, B. Gong, Y. Cui, D. Kondratyuk, X. Du, M.-H. Yang, and\nM. Brown, “Towards a unified foundation model: Jointly pre-\ntraining transformers on unpaired images and text,” arXiv, 2021.\n[277] M. Ni, H. Huang, L. Su, E. Cui, T. Bharti, L. Wang, D. Zhang, and\nN. Duan, “M3p: Learning universal representations via multitask\nmultilingual multimodal pre-training,” in CVPR, 2021.\n[278] A. Miech, J.-B. Alayrac, I. Laptev, J. Sivic, and A. Zisserman,\n“Thinking fast and slow: Efficient text-to-visual retrieval with\ntransformers,” in CVPR, 2021.\n[279] K. Wen, J. Xia, Y. Huang, L. Li, J. Xu, and J. Shao, “Cookie: Con-\ntrastive cross-modal knowledge sharing pre-training for vision-\nlanguage representation,” in ICCV, 2021.\n[280] S. Lee, Y. Yu, G. Kim, T. Breuel, J. Kautz, and Y. Song, “Param-\neter efficient multimodal transformers for video representation\nlearning,” arXiv, 2020.\n[281] T. Liu, F. Feng, and X. Wang, “Multi-stage pre-training over\nsimplified multimodal pre-training models,” arXiv, 2021.\n[282] Y. Li, F. Liang, L. Zhao, Y. Cui, W. Ouyang, J. Shao, F. Yu,\nand J. Yan, “Supervision exists everywhere: A data efficient\ncontrastive language-image pre-training paradigm,” arXiv, 2021.\n[283] Z. Gan, Y.-C. Chen, L. Li, T. Chen, Y. Cheng, S. Wang, and J. Liu,\n“Playing lottery tickets with vision and language,” arXiv, 2021.\n[284] C. He, S. Li, M. Soltanolkotabi, and S. Avestimehr, “Pipetrans-\nformer: Automated elastic pipelining for distributed training of\nlarge-scale models,” in ICML, 2021.\n[285] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. R ´e, “Flashattention:\nFast and memory-efficient exact attention with io-awareness,”\nNeurIPS, 2022.\n[286] R. Child, S. Gray, A. Radford, and I. Sutskever, “Generating long\nsequences with sparse transformers,” arXiv, 2019.\n[287] C. Zhu, W. Ping, C. Xiao, M. Shoeybi, T. Goldstein, A. Anandku-\nmar, and B. Catanzaro, “Long-short transformer: Efficient trans-\nformers for language and vision,” in NeurIPS, 2021.\n[288] S. Yan, X. Xiong, A. Arnab, Z. Lu, M. Zhang, C. Sun, and\nC. Schmid, “Multiview transformers for video recognition,” in\nCVPR, 2022.\n[289] W. Wang and Z. Tu, “Rethinking the value of transformer com-\nponents,” in COLING, 2020.\n[290] M. Ma, J. Ren, L. Zhao, D. Testuggine, and X. Peng, “Are\nmultimodal transformers robust to missing modality?” in CVPR,\n2022.\n[291] A. Akula, V . Jampani, S. Changpinyo, and S.-C. Zhu, “Robust\nvisual reasoning via language guided neural module networks,”\nNeurIPS, 2021.\n[292] A. R. Akula, S. Gella, Y. Al-Onaizan, S.-C. Zhu, and S. Reddy,\n“Words aren’t enough, their order matters: On the robustness of\ngrounding visual referring expressions,” arXiv, 2020.\n[293] L. Li, Z. Gan, and J. Liu, “A closer look at the robustness of\nvision-and-language pre-trained models,” arXiv, 2020.\n[294] M. Zhang, T. Maidment, A. Diab, A. Kovashka, and R. Hwa,\n“Domain-robust vqa with diverse datasets and methods but no\ntarget labels,” in CVPR, 2021.\n[295] Y. Kant, A. Moudgil, D. Batra, D. Parikh, and H. Agrawal,\n“Contrast and classify: Training robust vqa models,” in ICCV,\n2021.\n[296] S. Pramanik, P . Agrawal, and A. Hussain, “Omninet: A unified\narchitecture for multi-modal multi-task learning,” arXiv, 2019.\n[297] P . Wang, A. Yang, R. Men, J. Lin, S. Bai, Z. Li, J. Ma, C. Zhou,\nJ. Zhou, and H. Yang, “Unifying architectures, tasks, and modal-\nities through a simple sequence-to-sequence learning frame-\nwork,” arXiv, 2022.\n[298] R. Girdhar, M. Singh, N. Ravi, L. van der Maaten, A. Joulin, and\nI. Misra, “Omnivore: A single model for many visual modalities,”\narXiv, 2022.\n[299] J. Cao, Z. Gan, Y. Cheng, L. Yu, Y.-C. Chen, and J. Liu, “Behind the\nscene: Revealing the secrets of pre-trained vision-and-language\nmodels,” in ECCV, 2020.\n[300] L. A. Hendricks, J. Mellor, R. Schneider, J.-B. Alayrac, and A. Ne-\nmatzadeh, “Decoupling the role of data, attention, and losses in\nmultimodal transformers,” TACL, 2021.\n[301] L. A. Hendricks and A. Nematzadeh, “Probing image-language\ntransformers for verb understanding,” arXiv, 2021.\n[302] S. Frank, E. Bugliarello, and D. Elliott, “Vision-and-language\nor vision-for-language? on cross-modal influence in multimodal\ntransformers,” arXiv, 2021.\n[303] H. Chefer, S. Gur, and L. Wolf, “Generic attention-model explain-\nability for interpreting bi-modal and encoder-decoder transform-\ners,” arXiv, 2021.\n[304] L. Parcalabescu, M. Cafagna, L. Muradjan, A. Frank, I. Calixto,\nand A. Gatt, “Valse: A task-independent benchmark for vision\nand language models centered on linguistic phenomena,” arXiv,\n2021.\n[305] T. Zhao, T. Zhang, M. Zhu, H. Shen, K. Lee, X. Lu, and J. Yin, “Vl-\nchecklist: Evaluating pre-trained vision-language models with\nobjects, attributes and relations,” arXiv, 2022.\n[306] E. Aflalo, M. Du, S.-Y. Tseng, Y. Liu, C. Wu, N. Duan, and V . Lal,\n“Vl-interpret: An interactive visualization tool for interpreting\nvision-language transformers,” in CVPR, 2022.\n[307] N. Mu, A. Kirillov, D. Wagner, and S. Xie, “Slip: Self-supervision\nmeets language-image pre-training,” arXiv, 2021.\n[308] H. Xu, G. Ghosh, P .-Y. Huang, P . Arora, M. Aminzadeh, C. Feicht-\nenhofer, F. Metze, and L. Zettlemoyer, “Vlm: Task-agnostic video-\nlanguage model pre-training for video understanding,” arXiv,\n2021.\n[309] J. Li, D. Li, C. Xiong, and S. Hoi, “Blip: Bootstrapping language-\nimage pre-training for unified vision-language understanding\nand generation,” arXiv, 2022.\n[310] P . Zhang, X. Li, X. Hu, J. Yang, L. Zhang, L. Wang, Y. Choi,\nand J. Gao, “Vinvl: Revisiting visual representations in vision-\nlanguage models,” in CVPR, 2021.\nPeng Xu is a postdoctoral research assistant in\nthe Department of Engineering Science at the\nUniversity of Oxford.\nXiatian Zhu is a Senior Lecturer at the Surrey In-\nstitute for People-Centred Artificial Intelligence,\nand Centre for Vision, Speech and Signal Pro-\ncessing (CVSSP), Faculty of Engineering and\nPhysical Sciences, University of Surrey.\nDavid A. Clifton is a Professor of Clinical Ma-\nchine Learning and leads the Computational\nHealth Informatics (CHI) Lab in the Department\nof Engineering Science at the University of Ox-\nford.\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2023.3275156\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/"
}