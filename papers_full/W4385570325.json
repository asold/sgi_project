{
  "title": "ANALOGICAL - A Novel Benchmark for Long Text Analogy Evaluation in Large Language Models",
  "url": "https://openalex.org/W4385570325",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3049707876",
      "name": "Thilini Wijesiriwardene",
      "affiliations": [
        "University of South Carolina"
      ]
    },
    {
      "id": "https://openalex.org/A3004291016",
      "name": "Ruwan Wickramarachchi",
      "affiliations": [
        "University of South Carolina"
      ]
    },
    {
      "id": "https://openalex.org/A5091921005",
      "name": "Bimal Gajera",
      "affiliations": [
        "Nirma University"
      ]
    },
    {
      "id": "https://openalex.org/A5091921006",
      "name": "Shreeyash Gowaikar",
      "affiliations": [
        "Birla Institute of Technology and Science, Pilani"
      ]
    },
    {
      "id": "https://openalex.org/A2487088267",
      "name": "Chandan Gupta",
      "affiliations": [
        "Indian Institute of Technology Delhi",
        "Indraprastha Institute of Information Technology Delhi"
      ]
    },
    {
      "id": "https://openalex.org/A2118470473",
      "name": "Aman Chadha",
      "affiliations": [
        "Amazon (United States)",
        "Stanford Medicine"
      ]
    },
    {
      "id": null,
      "name": "Aishwarya Naresh Reganti",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2235111190",
      "name": "Amit Sheth",
      "affiliations": [
        "University of South Carolina"
      ]
    },
    {
      "id": "https://openalex.org/A2147177936",
      "name": "Amitava Das",
      "affiliations": [
        "University of South Carolina"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3199123028",
    "https://openalex.org/W2170262891",
    "https://openalex.org/W3176514068",
    "https://openalex.org/W3192500523",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2130359236",
    "https://openalex.org/W1910131649",
    "https://openalex.org/W4221153690",
    "https://openalex.org/W2460442863",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W1540596182",
    "https://openalex.org/W2982567551",
    "https://openalex.org/W82342167",
    "https://openalex.org/W3080956857",
    "https://openalex.org/W4385573282",
    "https://openalex.org/W3012480764",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3174082608",
    "https://openalex.org/W4386506836",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2251066368",
    "https://openalex.org/W3043172396",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2620569799",
    "https://openalex.org/W1514897281",
    "https://openalex.org/W2963223306",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2164037733",
    "https://openalex.org/W2991223644",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3036875034",
    "https://openalex.org/W3099554308",
    "https://openalex.org/W1849368448",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2950824039",
    "https://openalex.org/W2996819463",
    "https://openalex.org/W2475891679",
    "https://openalex.org/W2556341958",
    "https://openalex.org/W4313163053",
    "https://openalex.org/W3193313290",
    "https://openalex.org/W2907447547",
    "https://openalex.org/W2979401726",
    "https://openalex.org/W4241881032",
    "https://openalex.org/W3105816068",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W3034995113",
    "https://openalex.org/W2742229469",
    "https://openalex.org/W2996132992"
  ],
  "abstract": "Thilini Wijesiriwardene, Ruwan Wickramarachchi, Bimal Gajera, Shreeyash Gowaikar, Chandan Gupta, Aman Chadha, Aishwarya Naresh Reganti, Amit Sheth, Amitava Das. Findings of the Association for Computational Linguistics: ACL 2023. 2023.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 3534–3549\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nANALOGICAL - A Novel Benchmark for Long Text Analogy Evaluation in\nLarge Language Models\nThilini Wijesiriwardene1,∗, Ruwan Wickramarachchi1, Bimal G. Gajera2,\nShreeyash Mukul Gowaikar3, Chandan Gupta4, Aman Chadha5,6,†\nAishwarya Naresh Reganti7,†, Amit Sheth1, Amitava Das1\n1AI Institute, University of South Carolina, USA, 2Nirma University, India,\n3BITS Pilani, Goa, India, 4IIIT Delhi, India, 5Amazon AI, USA,\n6Stanford, USA, 7Amazon, USA\nthilini@sc.edu\nAbstract\nOver the past decade, analogies, in the form\nof word-level analogies, have played a signifi-\ncant role as an intrinsic measure of evaluating\nthe quality of word embedding methods such\nas word2vec. Modern large language models\n(LLMs), however, are primarily evaluated on\nextrinsic measures based on benchmarks such\nas GLUE and SuperGLUE, and there are only a\nfew investigations on whether LLMs can draw\nanalogies between long texts. In this paper, we\npresent ANALOGICAL, a new benchmark to\nintrinsically evaluate LLMs across a taxonomy\nof analogies of long text with six levels of com-\nplexity – (i) word, (ii) word vs. sentence, (iii)\nsyntactic, (iv) negation, (v) entailment, and (vi)\nmetaphor. Using thirteen datasets and three\ndifferent distance measures, we evaluate the\nabilities of eight LLMs in identifying analog-\nical pairs in the semantic vector space. Our\nevaluation finds that it is increasingly challeng-\ning for LLMs to identify analogies when going\nup the analogy taxonomy.\n1 Introducing ANALOGIAL - a\nBenchmark for Analogy\nThe ability of humans to perceive a situation in one\ncontext as similar to that in a different context is\nknown as analogy-making. It is considered to be a\ncentral component of human cognition and learn-\ning. Analogy-making has received attention from a\nbroad audience, including cognitive scientists (Gen-\ntner and Markman, 1997; Holyoak et al., 2001),\nlinguists (Itkonen, 2005), and educators (Richland\nand Simms, 2015) during the last several decades.\nCurrent neural network-based word embeddings\nare primarily influenced by the distributional hy-\npothesis \"You shall know a word by the company it\nkeeps\" (Firth, 1957).\n∗Corresponding author\n† Work does not relate to position at Amazon.\n-200\n-150\n-100\n-50\n0\n50\n100\n-150-130-110-90-70-50-30-1010\nA member of my team will execute your orders with immense precision.One of our member will carry out your instructions minutely.\nRead for Slate's take on Jackson's findings.\nSlate had an opinion on Jackson's findings.\nI'm upset that my walkman broke and now I have to turn the stereo up really loudMy walkman broke so I'm upset now I just have to turn the stereo up real loud \nFigure 1: Expected vector space embeddings of three\nanalogical sentence pairs from a hypothetical LLM that\ncaptures sentence analogies accurately.\nDuring 2013-2017, less complex, word-level\nanalogies played a central role in intrinsically eval-\nuating the quality of word embedding methods,\nsuch as word2vec (Mikolov et al., 2013a), GloVe\n(Pennington et al., 2014), and fastText (Bojanowski\net al., 2017). Different types of textual analogies\ncan be identified, such as word analogies (Gladkova\net al., 2016a), proportional analogies (Mikolov\net al., 2013a), and long-text analogies (Ichien et al.,\n2020). The techniques to create word embeddings\nhave progressed from categorical (i.e., one-hot, bag-\nof-words) to continuous contextualized techniques\nexemplified by LLMs such as BERT (Devlin et al.,\n2018) and T5 (Raffel et al., 2022).\nHowever, only a few investigations have been\ndone on the capabilities of LLMs to draw analogies\nbetween long text (Czinczoll et al., 2022). For ex-\nample - embeddings of sentences ‘I can speak two\nlanguages.’ and ‘I am bilingual.’ should be close-\nby in vector space and ‘I like chocolate.’ and ‘I do\nnot like chocolate.’ should not be close-by. Per-\nformance evaluations of modern LLM are driven\nmainly by extrinsic measures based on benchmarks\nsuch as GLUE (Wang et al., 2018), and Super-\n3534\nAMEN              Famous last word\nSentence àa person on a horse jumps over a broken-down airplane•[RR]  àperson on a broken-down a  horse jumps over airplane•[RM] àa [MASK] on a [MASK] jumps over a broken-down [MASK]•[RD]  àa person on a horse broken-down airplane\nInterior ministry worker was killed by a mercenary group No interior ministry worker was killed by a mercenary group \nKeep your powder dry\nSarah liked to go running in her neighborhood. Whenever she went running, she always carried a canister of pepper spray and a whistle on her keychain. She trusted her neighbors, but she never knew who else might be out when she went jogging at 5 in the morning.::\nAll this is their information againThis information belongs to them\nBlender                  Appliance\nIncreasing Analogy Complexity\nNegation\nSyntactic Analogies\nWords vs. Sentences\nWords1\n2\n3\n4\nEntailment5\nMetaphor6 Sentence :: explanation\nSentence ::Entailment sentence::\n::Sentence :: Random reordering(RR)Sentence :: Random masking (RM)Sentence :: Random deletion (RD)\nSentence::Negation\nWordnet word :: Wordnet word meaningClue :: Crossword::Word pair :: Word pairWord :: Word::\nDefinitionsExamples\nCalculator : Compute            Clamp: Grip::\nabstraction#n#6           a general concept formed by extracting common features from specific examples::\nFigure 2: Analogy taxonomy with six levels. The definitions of the analogies at each level and examples for each\nanalogy type from the datasets are indicated.\nGLUE (Wang et al., 2019). We take this opportu-\nnity to introduce a new benchmark to intrinsically\nevaluate LLMs using analogies consisting of long\ntext (sentences, paragraphs). We hypothesize that\nan LLM should be able to organize the semantic\nvector space so that analogical lexical pairs are\ncloser to each other (see Figure 1).\nIn this paper, we introduce ANALOGICAL - a\nbenchmark based on an analogy taxonomy consist-\ning of six levels of analogy complexity - (i) word\nlevel, (ii) word vs. sentence level, (iii) syntactic\nlevel, (iv) negation level, (v) semantic (entailment)\nlevel and (vi) metaphor level. We proxy analogy\ncomplexity with the length of lexical items com-\npared. We derive five and identify eight datasets at\neach level of the analogy taxonomy.\nEuclidean distance and cosine similarity are the\nde facto standards for capturing analogy in the NLP\ncommunity. We show that, in contrast, such mea-\nsures suffer from several correlations and indirect\ndependencies among the vector dimensions. Fi-\nnally, we argue and empirically report that Maha-\nlanobis distance (Mahalanobis, 1936) better cap-\ntures the semantic equivalence in high dimensional\nvector spaces.\n2 Related Work\nIn this section, we elaborate on previous work on\nanalogy identification, the background of encoder-\nbased language models and distance measures used\nin analogy-based comparisons in NLP.\nThere have been previous work on analogy iden-\ntification by Turney (2008) applying a singular\nvalue decomposition (SVD) (Golub and Van Loan,\n2013) based approach, and by Mikolov et al.\n(2013a); Gladkova et al. (2016a) using static word\nembeddings with vector offset approaches. In more\ncontemporary literature, Ushio et al. (2021) evalu-\nates the ability of LMs such as BERT, GPT-2, and\nRoBERTa to identify word analogies in a zero-shot\nsetting with prompts. In this work, we perform\nmore comprehensive evaluations, including several\ntypes of analogies in addition to word analogies.\nWe also evaluate the analogy identification abilities\nof eight contemporary LLMs.\nCurrent neural network-based LMs play a pivotal\nrole in the present-day NLP landscape by perform-\ning exceptionally well in numerous NLP tasks such\nas machine translation (Zhang et al., 2015; Singh\net al., 2017), classification (Marwa et al., 2018),\nand sentiment analysis (Hoang et al., 2019). These\nLMs are trained on large, heterogeneous text cor-\npora resulting in pretrained LMs that are then used\non downstream tasks via supervised fine-tuning.\nThis work uses the pretrained LMs in a zero-shot\nsetting for embedding creation.\nPrevious research in NLP has used cosine dis-\ntance/ similarity, Euclidean distance and Maha-\nlanobis distance as popular distance measures to\nquantify the semantic similarity between text (Agar-\nwala et al., 2021; Han et al., 2021; Sunilkumar and\nShaji, 2019; Bollegala et al., 2009). Even though\n3535\nMahalanobis distance has been popularly used to\nmeasure the distance between a sample and a dis-\ntribution, it has been increasingly used to measure\nthe distance between two samples in a dataset (Bal-\nasubramanian et al., 2016; Rahman et al., 2018).\nThis work extends these distance measures to mea-\nsure the analogy between two lexical items.\n3 ANALOGICAL - Six Levels of Analogy\nANALOGICAL is a comprehensive benchmark fo-\ncusing on six distinct categories of analogies or-\nganized within a taxonomy. These categories are\ndetermined based on the level of complexity they\npose for current LLMs. Even though current lan-\nguage models perform exceptionally well on tasks\nthat involve pattern recognizing the underlying text\ndistribution and learning to infer correlations, they\nstruggle with complex and intricate tasks such as\nbasic symbol manipulation (Pi˛ ekos et al., 2021),\ncompositionality (Dankers et al., 2022), and ap-\npropriating commonsense knowledge (Zhou et al.,\n2020). In higher levels of this taxonomy, the LMs\nare required to identify analogies between long and\nmore abstract texts and, when doing so, have to\nface the complexities highlighted above. In the\nnext section, we formally introduce the analogy\ntaxonomy and the datasets representing each level\nin the taxonomy.\nAnalogies are often expressed as an explicit or\nimplicit relational similarity, involving two main\nlexical items. In this work, these two lexical items\nvary from single words to word phrases or sen-\ntences. More formally, we denote analogy as\nX ∶∶Y, where X and Y are the two lexical items\nand analogy is a symmetric relation. The taxonomy\nof analogy is divided into six levels (see figure 2)\nwhere complexity is increased from bottom to top.\nIn this section, we identify and introduce differ-\nent datasets corresponding to each level of com-\nplexity in the analogy taxonomy that can be used\nto evaluate the performances of several SOTA lan-\nguage models. Table 1 summarizes the dataset\nstatistics.\n3.1 Level One\n3.1.1 Word level\nIn this level of analogy, the two analogous lexical\nitems are either single words or word pairs. If all\nlexical items in a language are in set A, then the\nanalogy between two single words a∈W and b∈\nW are denoted by a∶∶b. An analogy between two-\nword pairs (also known as proportional analogies)\nwhere a,b,c,d, ∈ W is denoted by a ∶b ∶∶c ∶d.\nThis indicates that a is related to b as c is related\nto d.\n3.1.2 Datasets for Level One\nThis level represents word analogies. We identify\nfour datasets at this level. Two of them, namely\nthe Bigger Analogy Test Set (BATS)(Gladkova\net al., 2016b) and MSR Dataset (Gao et al., 2014),\ncontain analogies between two words. We use the\nMSR dataset as is and slightly modify the BATS\ndataset as below for our intended use.\nBATS Dataset consists of four main analogy\ntypes namely Morphology-inflections, Morphology-\nderivation, Semantics-encyclopedia and Semantics-\nlexicography. Semantics-lexicography data contain\nhypernyms, hyponyms and synonyms where one\nword is identified to be analogous to several other\nwords (e.g. afraid :: terrified/ horrified/ scared/\nstiff/ petrified/ fearful/ panicky). In this case, we\nidentify each element on the right as analogous to\nthe element on the left separately (e.g., for the ex-\nample above, afraid :: terrified, afraid :: horrified,\netc.).\nWe identify two other datasets for word pair\nanalogies in level one of the taxonomy. One is\nreferred to as the Google Dataset (Mikolov et al.,\n2013b), with syntactic and semantic analogies. The\nother comprises educational resources such as anal-\nogy problems from SAT exams (US college admis-\nsion tests) and other similar problems targeted at\nyounger students in US school system. We use\nthese data aggregated by Ushio et al. (2021) and\nidentify it as the SAT Dataset.\n3.2 Level Two\n3.2.1 Word vs. Sentence Level\nThis level consists of analogies between a word w\nand a sentence S, denoted by S ∶∶w. Sentence S\nis a sequence of words S =[a1,⋯,an] and word\nwis {w1,⋯,wn} ∈W.\n3.2.2 Datasets for Level Two\nThis level consists of two datasets with single\nwords and their analogous sentences. The first\ndataset, (Pwanson, 2016), is a crossword puzzle\ndataset where the crosswords are words and clues\nare sentences/phrases (e.g., amen :: famous last\nwords). We identify this dataset as the Cross-\nword Dataset. The second dataset is the WordNet\nDataset. WordNet is a large lexical database of En-\nglish words grouped into cognitive synonym sets\n3536\nknown as synsets (Miller, 1992). The two lexical\nterms of interest in this dataset are the WordNet\nwords and the different senses of these words ex-\nplained in a sentence/phrase.\n3.3 Level Three\n3.3.1 Syntactic Level\nThese analogies are between single sentences. We\npropose that a single sentence S with a word\nsequence [w1,⋯,wn] ∈ W is analogous to\na syntactically altered version of the same sen-\ntence. We generate altered versions of original\nsentences by random deletion, random reorder-\ning, and random masking of the words in the\nsentence. If an original sentence is denoted by\na word sequence [w1,w2,w3,w4,w5], an altered\nversion of the sentence SRD is created by ran-\ndomly deleting a consecutive range of tokens such\nas [w1,w4,w5]. Another altered version is cre-\nated by random reordering of the original sen-\ntence denoted by SRR where the altered sentence\nwould look like [w1,w2,w4,w3,w5]. The final\nalteration masks random words (SRM ) in the orig-\ninal sentence resulting in an altered version of\n[w1,[MASK],w3,[MASK],w5].\n3.3.2 Datasets for Level Three\nWe are looking at analogies between two syntac-\ntically equivalent sentences at this level. We are\nintroducing three datasets on three types of syn-\ntactic equivalence variants: random deletion, ran-\ndom masking, and random reordering. We use the\nsentence tagged as \"neutral\" in the SNLI dataset\n(Bowman et al., 2015) as the basis for creating all\nthree datasets introduced at this level. To create the\nRandom Deletion Dataset, we delete 20% of the\nwords in a sentence randomly; to create the Ran-\ndom Masking Dataset, we randomly replace 20%\nof tokens in a sentence with[MASK]. Finally, to cre-\nate the Random Reorder Dataset, we randomly\nreorder 20% of the words in a sentence. The orig-\ninal sentence and its altered version are identified\nas an analogous pair.\n3.4 Level Four\n3.4.1 Negation Level\nThe two lexical items considered in this level are\nsingle sentences, one negating the other denoted by\nSand SNG .\n3.4.2 Datasets for Level Four\nWe identify sentences and their negated forms as\na pair. Since a sentence and its negation are rec-\nognized as opposites to each other, we postulate\nthat this is a non-analogy. We use Stanford Contra-\ndiction Corpora (specifically the negation dataset)\n(De Marneffe et al., 2008). We extract the sentence\nwith negation markers and create sentence pairs\nfrom each of these extracted sentences by keeping\nthe negation marker and removing it. We identify\nthis dataset as Negation Dataset.\n3.5 Level Five\n3.6 Entailment Level\nThis level again contains analogies between sen-\ntences. The type of analogies contained in this\nlevel is entailing sentences. Textual Entailment at-\ntempts to infer one sentence from the other. We\npropose that entailment considers attributional and\nrelational similarities between sentences, making\nthem analogous. More formally given a sentence\nas S, its entailment sentence as SET , words in the\nsentence as w and words in the entailment sen-\ntence as w′, S = [w1⋯wn], SET = [w′\n1⋯w′\nn]\nand S ∶∶SET .\n3.6.1 Datasets for Level Five\nWe identify one dataset for this level and refer to\nit as the Entailment Dataset. We extract the sen-\ntence pairs tagged with the \"entailment\" relation-\nship from the SNLI dataset (Bowman et al., 2015)\nto create the data points.\n3.7 Level Six\n3.7.1 Metaphor Level\nThis is the highest level in the taxonomy with the\nmost complexity with regard to analogy identifica-\ntion, with the least attention from the NLP com-\nmunity. In this level, the two lexical items are\na sentence and a paragraph. If a sentence is de-\nnoted by S = [w1⋯wn], a paragraph is denoted\nby several sentences that do not include the original\nsentence. P =[s1⋯sn]. The analogy is indicated\nby S ∶∶P.\n3.7.2 Datasets for Level Six\nWe have metaphors at the top level of the analogy\ntaxonomy. We identify two datasets at this level.\nOne is \"ePiC\", a crowdsourced proverb dataset\nby Ghosh and Srivastava (2022) with narratives\nexplaining each proverb. Since the proverb and\nits explanation essentially have the same meaning,\n3537\nLevels in Analogy\nTaxonomy Dataset # Datapoints\nLevel One\nMSR 44584\nBATS* 2880\nGoogle 19544\nSAT 1106\nLevel Two Crossword 100000\nWordNet 104356\nLevel Three\nRandom Deletion* 100000\nRandom Masking* 100000\nRandom Reorder* 100000\nLevel Four Negation * 899\nLevel Five Entailment 100000\nLevel Six ePiC 42501\nQuotes 998\nTable 1: Statistics of datasets used at each level of the\nAnalogy taxonomy. Datasets derived by authors are\nindicated with *.\nwe assume that a proverb and its corresponding\nnarrative are analogous to each other. We refer to\nthis dataset as ePiC Dataset. Similarly, the second\ndataset (Rudrapal et al., 2017) includes quotes and\nthe elaborated meaning of each quote. We refer to\nthis dataset as the Quotes dataset.\n4 Large Language Models to Evaluate\nANALOGICAL\nModern LLMs are built upon the transformer archi-\ntecture (Vaswani et al., 2017). The LLMs we use in\nthis study fall into two classes based on their train-\ning objective. Masked language models (MLMs)\nare trained to predict randomly masked tokens (ran-\ndom words replaced by a [MASK] token) based on\nall the other words present in a sequence in a bidi-\nrectional manner. MLMs use the encoder portion\nof the transformer architecture. Encoder-decoder\nlanguage models(EDLMs) build upon the entire\nencoder-decoder architecture of transformers and\nare trained by predicting the original sequence of\ntext given a corrupted version of the text sequence.\nIn the current empirical study, we examine\nthe performance of eight popular pretrained lan-\nguage models on identifying analogies introduced\nin the analogy taxonomy without fine-tuning\n(zero-shot setting). We choose six MLM-based\nLLMs, namely (i) BERT (Devlin et al., 2018), (ii)\nRoBERTa (Liu et al., 2019), (iii) AlBERT (Lan\net al., 2019), (iv) LinkBERT (Yasunaga et al.,\n2022), (v) SpanBERT (Joshi et al., 2020), and (vi)\nXLNet (Yang et al., 2019), T5 (Raffel et al., 2020),\nan encoder-decoder-based model, and ELECTRA\n(Clark et al., 2020) an LLM with two transformers,\none as a generator and the other as a discrimina-\ntor. We include further details of these LLMs in\nAppendix C).\n5 Distance Measures and Their\nImportance\nPrevious work (Mikolov et al., 2013a; Gladkova\net al., 2016a) used static word embeddings with vec-\ntor offset approaches (such as 3CosMul, 3CosAdd)\nto identify word analogies. In this work, we\nuse the distance between the lexical items in\na high-dimensional vector space to identify the\nanalogy between two lexical items. We identify\nthree distance measures, namely, cosine distance\n(CD), Euclidean distance(ED), and Mahalanobis\ndistance(MD). Next, we briefly explain MD. CD\nand ED are explained in the appendix.\n5.1 Mahalanobis Distance (MD)\nED does not perform well if the vector dimensions\ndepend on each other. Mahalanobis distance (Ma-\nhalanobis, 1936), is a generalized extension of the\nEuclidean distance that takes into account the corre-\nlation between vector dimensions, thereby provid-\ning a balanced measure of dissimilarity. In the next\nsection, we show that word vectors’ dimensions are\nhighly correlated. Therefore, we use MD in this\nwork to get an accurate distance measure. Given\ntwo vectors A=[ai,⋯,an] and B =[bi,⋯,bn],\nMD between the two points are given by (C−1 in-\ndicates the covariance matrix of the dataset.):\nMD(−→A,−→B) =\n√\n(−→A−−→B)T C−1(−→A−−→B)\n5.2 Importance of Mahalanobis Distance as a\nDistance Measure\nVector representations of lexical items produced by\nLLMs are opaque due to the low interpretability\nof individual vector dimensions. Tsvetkov et al.\n(2015) introduce QVEC, which uses a subspace\nalignment technique to align linguistic properties\nwith distributional vector dimensions.\nWordnet divides verbs and nouns into 41\ncoarse semantic categories known as super-\nsenses. For example, NOUN.QUANTITY and\nNOUN.SHAPE is supersenses related to nouns and\nVERB.POSSESSION and VERB.CREATION are\nsupersenses related to verbs. SemCor is a corpus\ncontaining 13,174 noun lemmas and 5,686 verb\nlemmas from wordnet, and these are annotated\n3538\nFigure 3: Pearson correlation between 10 word-vector dimensions. VERB.CONSUMPTION is highly corre-\nlated with the dimension NOUN.QUANTITY and dimension of VERB.WEATHER is highly correlated with\nVERB.EMOTION.\nwith supersenses. Terms from SemCor are con-\nverted into linguistic word vectors based on term\nfrequency, resulting in a set of 4,199 linguistic word\nvectors, each with 41 interpretable dimensions.\nQVEC aligns distributional word vector dimen-\nsions with above described linguistically inter-\npretable word vector dimensions through Pear-\nson’s correlations-based matrix alignments. We\nuse the same methods to calculate Pearson’s cor-\nrelation between the 41 vector dimensions to\nidentify the correlations among them. Figure\n3 illustrates a subset of 10 vector dimensions\nand their correlations. We see that dimension\nVERB.CONSUMPTION is highly correlated with\nthe dimension NOUN.QUANTITY and dimension\nof VERB.WEATHER is highly correlated with\nVERB.EMOTION.\nDue to the correlated nature of vector dimen-\nsions, and the ability of MD to take into account\nthe correlations between vector dimensions when\ncalculating the distance measures, we identify MD\nas the best distance measure among CD, ED, and\nMD.\n6 Experiment Settings\nWe have set up comprehensive experiments across\neight LLMs, thirteen datasets, and three distance\nmeasures adding up to 312 (8×13×3) experiments.\nWe analyze the performance of LLMs across the\nanalogy taxonomy by comparing the normalized\ndistance measures. We present the complete results\ntable for all the experiments in Appendix A).\nThe embedding (representation) of each lexi-\ncal item in an analogical pair (word embedding,\nsentence embedding) is extracted from eight LMs\n(In this work, we use the simplest representation,\nwhich is the [CLS] token representation). The dis-\ntance measures between these two representations\nare then calculated using ED, CD, and MD. For\neach dataset containing analogical pairs, these dis-\ntance measures are calculated, and the mean of all\nthe data points of a dataset is considered the repre-\nsentative distance for that dataset (these distances\nare Min-Max normalized).\nGiven the analogy taxonomy (figure 2), except\nfor the negation dataset at level 4, all the other\ndatasets are positive analogies, meaning, that the\ntwo lexical items of a data point are considered\nanalogical to each other. Therefore the mean dis-\ntance values of these datasets should indicate such\nsimilarity (low cosine, Euclidean, and Mahalanobis\ndistances). For the negation dataset, the two lexical\nitems in a data point should not be analogical to\neach other. Therefore, the representative distance\nmeasures should be large. We discuss the imple-\nmentation details in appendix D.\n7 Benchmark Results\n7.1 Performance of LLMs on ANALOGICAL\nWe illustrate the performance of each LLM on dif-\nferent datasets at different levels of the analogy\ntaxonomy based on the three distance measures\nin Figure 4. We further analyze the performance\nof LLMs based on MD akin to the superiority of\nMD over CD and ED mentioned in section 5.2\n(see Table 2). When inspecting the performance\nof LLMs at the word level, for BATS and MSR\ndatasets, most LLMs perform considerably well\nwith mean distance values close to zero. When\nmoving into the word pair datasets (Google, SAT),\nall the LLMs struggle to perform with mean dis-\ntance values closer to one. In word pair datasets, it\nis crucial to understand the implicit relations among\nthe word pairs to model the analogies correctly in\nthe vector space. The suboptimal performance ex-\nhibited by LLMs on the aforementioned datasets\n3539\nCosine sim Cosine dis Cosine dis normalized\nDataset T5 BERT LinkBERT XLNet RoBERTa AlBERT SpanBERT Electra Dataset T5 BERT LinkBERT XLNet RoBERTa AlBERT SpanBERT Electra Best Value Dataset T5 BERT LinkBERT XLNet RoBERTa AlBERT SpanBERT Electra Best Value\nBATS_3.0 0.52 0.68 0.84 0.98 0.999 0.509 0.94 0.62 BATS_3.0 0.48 0.32 0.16 0.02 0.001 0.49 0.06 0.38 0.00 BATS_3.0 0.46 0.28 0.12 0.00 0.00 0.48 0.03 0.43 0.00\nMSR 0.51 0.91 0.84 0.99 0.999 0.483 0.93 0.59 MSR 0.49 0.09 0.16 0.01 0.001 0.52 0.07 0.41 0.00 MSR 0.47 0.01 0.12 0.00 0.00 0.51 0.05 0.48 0.00\nGoogle 0.78 0.24 0.14 0.23 0.000 0.310 0.94 0.92 Google 0.22 0.76 0.86 0.77 1.00 0.69 0.06 0.08 0.06 Google 0.14 0.77 0.87 0.78 1.00 0.70 0.03 0.00 0.00\nSAT 0.90 0.10 0.08 0.11 0.106 0.075 0.93 0.92 SAT 0.10 0.90 0.92 0.89 0.89 0.93 0.07 0.08 0.07 SAT 0.00 0.93 0.94 0.90 0.89 0.97 0.05 0.00 0.00\nCrossword 0.76 0.85 0.81 0.98 0.998 0.483 0.18 0.64 Crossword 0.24 0.15 0.19 0.02 0.002 0.52 0.82 0.36 0.00 Crossword 0.17 0.07 0.15 0.01 0.00 0.51 0.88 0.40 0.00\nWordnet 0.75 0.21 0.82 0.97 0.997 0.713 0.27 0.55 Wordnet 0.25 0.79 0.18 0.03 0.003 0.29 0.73 0.45 0.00 Wordnet 0.19 0.81 0.14 0.02 0.00 0.25 0.78 0.53 0.00\nRandom reordering 0.64 0.84 0.92 0.98 0.998 0.835 0.15 0.52 Random reordering 0.36 0.16 0.08 0.02 0.002 0.17 0.85 0.48 0.00 Random reordering 0.31 0.09 0.03 0.01 0.00 0.12 0.92 0.58 0.00\nRandom deletion 0.28 0.92 0.95 0.99 0.999 0.939 0.07 0.23 Random deletion 0.72 0.08 0.05 0.01 0.001 0.06 0.93 0.77 0.00 Random deletion 0.76 0.00 0.00 0.00 0.00 0.00 1.00 1.00 0.00\nRandom masking 0.57 0.84 0.93 0.98 0.998 0.867 0.18 0.35 Random masking 0.43 0.16 0.07 0.02 0.002 0.13 0.82 0.65 0.00 Random masking 0.41 0.09 0.02 0.01 0.00 0.08 0.88 0.82 0.00\nStanford Negation 0.08 0.04 0.02 0.01 0.000 0.044 0.97 0.90 Stanford Negation 0.92 0.96 0.98 0.99 1.00 0.96 0.03 0.10 0.03 Stanford Negation 1.00 1.00 1.00 1.00 1.00 1.00 0.00 0.03 0.00\nEntailment 0.46 0.84 0.89 0.99 0.998 0.894 0.19 0.26 Entailment 0.54 0.16 0.11 0.01 0.002 0.11 0.81 0.74 0.00 Entailment 0.54 0.09 0.07 0.00 0.00 0.05 0.88 0.95 0.00\nProverbs(Epic) 0.75 0.68 0.78 0.97 0.995 0.589 0.46 0.38 Proverbs(Epic) 0.25 0.32 0.22 0.03 0.005 0.41 0.54 0.62 0.00 Proverbs(Epic) 0.18 0.27 0.18 0.01 0.00 0.39 0.57 0.77 0.00\nQuotes 0.60 0.81 0.78 0.98 0.996 0.661 0.30 0.25 Quotes 0.40 0.19 0.22 0.02 0.004 0.34 0.70 0.75 0.00 Quotes 0.37 0.12 0.18 0.00 0.00 0.31 0.75 0.96 0.00\nEuclidean Euclidean normalized\nDataset T5 BERT LinkBERT XLNet RoBERTa AlBERT SpanBERT Electra Dataset T5 BERT LinkBERT XLNet RoBERTa AlBERT SpanBERT Electra Best value\nBATS_3.0 3.59 11.54 7.54 31.67 0.572 18.403 0.86 9.14 BATS_3.0 0.56 1.00 0.63 0.34 0.37 0.50 0.02 0.43 0.02\nMSR 3.68 5.78 7.52 30.77 0.522 19.659 1.08 9.83 MSR 0.59 0.26 0.62 0.30 0.33 0.56 0.09 0.51 0.09\nGoogle 4.14 6.21 10.25 31.90 0.121 11.130 1.59 9.75 Google 0.71 0.32 0.98 0.35 0.00 0.14 0.24 0.50 0.00\nSAT 5.29 7.89 10.42 42.77 0.258 18.617 1.19 14.17 SAT 1.00 0.53 1.00 0.74 0.11 0.51 0.12 0.97 0.11\nCrossword 4.70 7.56 8.52 41.89 0.750 26.038 2.23 14.41 Crossword 0.85 0.49 0.75 0.71 0.52 0.88 0.44 1.00 0.44\nWordnet 4.71 9.21 8.42 45.88 0.906 24.938 2.69 12.49 Wordnet 0.85 0.70 0.74 0.85 0.65 0.83 0.58 0.79 0.58\nRandom reordering 4.38 7.75 5.39 42.81 0.766 17.910 2.10 12.13 Random reordering 0.77 0.51 0.35 0.74 0.53 0.48 0.40 0.76 0.35\nRandom deletion 2.59 5.43 4.14 29.14 0.432 9.908 1.41 8.08 Random deletion 0.31 0.22 0.18 0.25 0.26 0.08 0.19 0.32 0.08\nRandom masking 4.18 7.71 4.96 50.01 0.796 15.769 2.26 10.04 Random masking 0.71 0.51 0.29 1.00 0.56 0.37 0.45 0.53 0.29\nStanford Negation 1.40 3.73 2.74 22.35 0.289 8.358 0.79 5.11 Stanford Negation 0.00 0.00 0.00 0.00 0.14 0.00 0.00 0.00 0.00\nEntailment 3.53 8.16 6.32 37.99 0.672 12.966 2.29 8.56 Entailment 0.55 0.57 0.47 0.57 0.46 0.23 0.46 0.37 0.23\nProverbs(Epic) 5.00 11.54 8.85 42.73 1.330 28.375 4.06 10.57 Proverbs(Epic) 0.93 1.00 0.80 0.74 1.00 1.00 1.00 0.59 0.59\nQuotes 4.39 9.01 9.05 36.51 0.967 26.671 3.34 8.21 Quotes 0.77 0.68 0.82 0.51 0.70 0.91 0.78 0.33 0.33\nMahalanobis Mahalanobis normalized\nDataset T5 BERT LinkBERT XLNet RoBERTa AlBERT SpanBERT Electra Dataset T5 BERT LinkBERT XLNet RoBERTa AlBERT SpanBERT Electra Best Value\nBATS_3.0 16.12 12.86 16.67 16.67 36.352 16.561 16.49 16.53 BATS_3.0 0.04 0.00 0.00 0.00 0.81 0.00 0.00 0.00 0.00\nMSR 24.05 28.40 28.69 28.87 36.579 30.994 25.61 26.33 MSR 0.32 0.38 0.55 0.55 0.82 0.64 0.25 0.26 0.25\nGoogle 41.38 54.11 33.03 33.03 38.169 38.251 53.72 54.50 Google 0.95 1.00 0.74 0.74 0.94 0.96 1.00 1.00 0.74\nSAT 42.86 52.95 38.70 38.67 38.403 35.205 44.43 47.85 SAT 1.00 0.97 1.00 1.00 0.95 0.83 0.75 0.82 0.75\nCrossword 31.68 37.29 38.70 38.41 36.855 38.266 36.47 37.51 Crossword 0.60 0.59 1.00 0.99 0.84 0.97 0.54 0.55 0.54\nWordnet 31.25 37.46 37.80 38.38 37.634 38.021 38.02 38.49 Wordnet 0.58 0.60 0.96 0.99 0.90 0.95 0.58 0.58 0.58\nRandom reordering 29.17 34.27 32.35 36.53 31.571 35.186 32.20 36.70 Random reordering 0.51 0.52 0.71 0.90 0.46 0.83 0.42 0.53 0.42\nRandom deletion 21.21 27.68 26.04 30.25 25.072 26.818 25.01 31.51 Random deletion 0.22 0.36 0.43 0.62 0.00 0.46 0.23 0.39 0.00\nRandom masking 26.43 31.28 28.28 37.27 30.349 33.188 31.08 34.33 Random masking 0.41 0.45 0.53 0.94 0.38 0.74 0.39 0.47 0.38\nStanford Negation 15.08 27.99 26.80 29.86 25.122 28.016 25.04 29.53 Stanford Negation 0.00 0.37 0.46 0.60 0.00 0.51 0.23 0.34 0.00\nEntailment 28.48 36.24 35.11 36.00 31.366 32.485 34.83 36.60 Entailment 0.48 0.57 0.84 0.88 0.45 0.71 0.49 0.53 0.45\nProverbs(Epic) 31.65 38.88 23.60 23.60 39.069 39.050 39.03 38.97 Proverbs(Epic) 0.60 0.63 0.31 0.32 1.00 1.00 0.61 0.59 0.31\nQuotes 31.21 38.27 38.37 38.60 37.887 38.542 38.32 38.59 Quotes 0.58 0.62 0.98 1.00 0.92 0.98 0.59 0.58 0.58\nMahalanobis Euclidean Cosine dis\nDataset Mahalanobis DistanceEuclidean Distance Cosine Distance \nBATS_3.0 0.000 0.020 0.001\nMSR 0.245 0.086 0.001\nGoogle 0.743 0.000 0.057\nSAT 0.751 0.113 0.072\nCrossword 0.537 0.439 0.002\nWordnet 0.578 0.579 0.003\nRandom reordering 0.422 0.345 0.002\nRandom deletion 0.000 0.077 0.001\nRandom masking 0.377 0.289 0.002\nStanford Negation 0.000 0.000 0.027\nEntailment 0.450 0.230 0.002\nProverbs(Epic) 0.315 0.588 0.005\nQuotes 0.580 0.333 0.004\nMahalanobis\nDataset T5 BERT LinkBERT XLNet RoBERTa AlBERT SpanBERT Electra\nGoogle 0.95 1.00 0.74 0.74 0.94 0.96 1.00 1.00\nSAT 1.00 0.97 1.00 1.00 0.95 0.83 0.75 0.82\nProverbs(Epic) 0.60 0.63 0.31 0.32 1.00 1.00 0.61 0.59\nQuotes 0.58 0.62 0.98 1.00 0.92 0.98 0.59 0.58\nDataset\nLanguage Model BATS_3.0 MSR Google SAT Crossword Wordnet Random reordering Random deletion Random masking Entailment Proverbs(Epic) Quotes\nT5 0.03743211630256450.3228506194376570.946935988125355 1 0.5975640890410770.5819537877090670.5072569880593420.2206648902545090.40842125916724 0.48251106358198 0.5963455063452930.580485854523968\nBERT 0 0.37671074773658 1 0.9718737832153360.5921114004948620.59628407637485 0.5190510359932520.3591934716922990.446521515130937 0.566744946685332 0.630786761834276 0.616\nLinkBERT 0 0.5456211022753430.7425654225978330.999959521675829 1 0.9590069741841570.7118184096646430.4253874130482460.526823514176348 0.837095200463546 0.3146563656838080.984684175609948\nmahalanobis XLNet 0 0.5546072883622470.743584083482888 1 0.9879159010844480.98657356030253 0.9025158298365940.6172333291772130.936456568103775 0.87840840865186 0.3151331656723560.996469607612509\nRoBERTa 0.8059160645971120.8221243116029840.9357536772735880.9524407231401590.8418750306848830.89749989298792 0.464369621665923 0 0.377072638684816 0.449685141221999 1 0.916\nAlBERT 0 0.6417998194039 0.9645000377287050.82904776221915 0.9651636113759250.9542690569526290.8281927271574290.4560692658745220.739328722909265 0.708067311151987 1 0.977423986519009\nSpanBERT 0 0.245149664857547 1 0.7505186429859410.5367008649140510.5782365247760010.4219408362357460.2288963088602890.391956521057925 0.492581058564905 0.6054788030709210.586439025991687\nElectra 0 0.258118356499199 1 0.8249417089457070.5524409495433640.5784296619462210.5312168390179850.3946337902368960.468779487311146 0.528692751176871 0.5910941303431180.580929757289283\nDataset BATS_3.0 MSR Google SAT Crossword Wordnet Random reordering Random deletion Random masking Entailment Proverbs(Epic) Quotes\nT5 0.4769589564257520.4871968839346070.2182850238219060.1001404589388920.2396835837478350.2539470421020590.3558444074440040.7224152947688960.434081788967883 0.542981942968197 0.2475810082195170.398753663734802\nBERT 0.3240613837811630.0889636512392410.7556737609846280.897964186992743 0.1464752615 0.7916922701706410.1588890628257890.080444233772817 0.157 0.156 0.3209811874290750.188239324490117\nLinkBERT 0.1632402016453950.1575580684075870.8579061162514340.92272226793054 0.1908247724427140.1814281890945670.0759618694445110.047477090273962 0.069 0.111 0.2178549667697430.218871302562575\nXLNet 0.0151183929405130.0144027000175210.7733116246238350.8906303572429370.0233912210584450.0281690445454460.0213561675932860.011322720221808 0.025 0.014 0.0258972849447260.015221762049215\nRoBERTa 0.001273686318520980.001087066348247960.9996194336306190.89355875937512 0.0020472258539580.003034145743898040.002197441782714970.000710607789985995 0.002 0.002 0.004877761147987990.00355196717915096\ncosine AlBERT 0.49146078146547 0.5167532518694440.6896909358340720.92536908778421 0.5171760612550970.2871840887608880.1650462060100730.0608179761009821 0.133 0.106 0.4114549059040390.339343132430987\nSpanBERT 0.0558288282021520.0684429238106170.0568547529410020.0724053278707630.8157978269201910.73037113021705 0.8497450725437680.9251540984917230.815291968436692 0.813357041798688 0.5376791441520620.696160268151469\nElectra 0.38045654178266 0.4143378517071770.0805781889187860.0838480017076620.3584100021896970.4501211074197850.48226164233911 0.773423280934676 0.647 0.738 0.616334926710810.747839858885589\nDataset BATS_3.0 MSR Google SAT Crossword Wordnet Random reordering Random deletion Random masking Entailment Proverbs(Epic) Quotes\nT5 0.5647624618783270.5860776478236060.705375536539133 1 0.8501533291453140.85116462282201 0.7669936878842250.3070836061675060.714247649569511 0.549596411462935 0.9259461311560270.769772703762236\nBERT 1 0.262683450125125 0.318 0.533 0.489 0.7018322111450810.5138726565321390.2168516782414530.509692952870302 0.566978223050339 0.9997717671682560.675694838824733\nEuclidean LinkBERT 0.6258734128525770.622582534578601 0.978 1.000 0.752 0.7403537205258380.3451405085136970.1821034711930880.289283506868608 0.466631511428084 0.7964231473837680.821785418144331\nXLNet 0.3368285117269240.304327408331904 0.345 0.738 0.706 0.8505285096522760.7397421496503840.245588565972008 1 0.565452604377761 0.7367873189345440.511980872820318\nRoBERTa 0.3730271283601540.332011368871161 0.000 0.113 0.520 0.6496699673950910.5335734491502170.2573133428051280.558083581609908 0.455617772229804 1 0.69963832087809\nAlBERT 0.5018294587105820.564598265050963 0.138 0.513 0.883 0.8282984520560160.4772285630203250.07747279180729610.370272294549648 0.23022998921592 1 0.91486736845856\nSpanBERT 0.01958769181089730.08629665818005060.2428025695467910.1202466097564090.4392315811168080.5789643823388890.3984952069990540.1871117986942670.448953379745676 0.459100223045672 1 0.779756501829483\nElectra 0.4330553092705420.507545064621251 0.500 0.975 1.000 0.7935178731258940.7551219637624710.3195423427210080.530639528633246 0.371010601257962 0.5875474050235590.333132118042271\nDataset SpanBERT\nBATS_3.0 0.000\nMSR 0.245\nGoogle 1.000\nSAT 0.751\nCrossword 0.537\nWordnet 0.578\nRandom reordering 0.422\nRandom deletion 0.229\nRandom masking 0.392\nStanford Negation 0.230\nEntailment 0.493\nProverbs(Epic) 0.605\nQuotes 0.586\nSpanBERT\nLinkBERT  \nXLNET SpanBERT\nT5  \nSpanBERT\nRoBERTa\nRoBERTa\nRoBERTa\nLinkBERT\nT5 \nSpanBERT\nRoBERTa\nRoBERTa\nSpanBERT\nLinkBERT\nALBERT\nLinkBERT\nRoBERTa\nELECTRA\nELECTRA\nRoBERTa RoBERTa\nSpanBER\nSpanBERT\nRoBERTa RoBERTa\nRoBERTa RoBERTa RoBERTa RoBERTaRoBERTa RoBERTa\nSpanBERT\nBERT \nLinkBER \nXLNET \nALBERTA \nSpanBERT \nELECTRA\nSpanBERT\nWord vs. \nSentence Level\nSyntactic \nAnalogies\nSpanBERT\nNegation Entailment Metaphor\n-0.100\n0.080\n0.260\n0.440\n0.620\n0.800\nBATS_3.0\nMSR\nGoogle\nSAT\nCrossword\nWordnet\nRandom reordering\nRandom deletion\nRandom masking\nStanford Negation\nEntailment\nProverbs(Epic)\nQuotes\nMahalanobis Distance\n Euclidean Distance \n Cosine Distance \nIncreasing Task Complexity \nWord Level\nChart Title\n0.00\n0.25\n0.50\n0.75\n1.00\nBATS_3.0 SAT Random reordering Stanford Negation Quotes\nT5\n BERT\n LinkBERT\n XLNet\n RoBERTa\n AlBERT\n SpanBERT\n Electra\nBest Value\nMahalanobis Distance\n-0.1\n0.1\n0.3\n0.5\n0.7\n0.9\n1.1\nT5 BERT LinkBERT XLNet RoBERTa AlBERT\n SpanBERT Electra\nCosine Distance\n-0.1\n0.1\n0.3\n0.5\n0.7\n0.9\n1.1\nEuclidean Distance\n-0.1\n0.1\n0.3\n0.5\n0.7\n0.9\n1.1\nBATS\nMSR\nGoogle\nSAT\nCrossword\nWordnet\nRandom R.\nRandom D.\nRandom M.\nEntailment\nProverbs\nQuotes\nSpanBERT\nBATS\nMSR\nGoogle\nSAT\nCrossword\nWordnet\nRandom R.\nRandom D.\nRandom M.\nEntailment\nProverbs\nQuotes\nSpanBERT\nBATS\nMSR\nGoogle\nSAT\nCrossword\nWordnet\nRandom R.\nRandom D.\nRandom M.\nEntailment\nProverbs\nQuotes\nRoBERTa\nFigure 4: Performance of LLMs across the thirteen datasets. All three distance measures are normalized to be\nin [0,1] range, 0 indicating the best performance (i.e., the least average distance between the analogous pairs).\nThe solid lines indicate the performance of the best-performing model across all the datasets (e.g., SpanBERT\noutperforms the other LLMs in most datasets based on Mean MD; therefore, the line represents the fluctuations of\nSpanBERT’s performance across the datasets).\nLanguage\nModel BATS_3.0 MSR Google SAT Crossword Wordnet Random\nReordering\nRandom\nDeletion\nRandom\nMasking Negation Entailment Proverbs\n(Epic) Quotes\nT5 0.04 0.32 0.95 1.00 0.60 0.58 0.51 0.22 0.41 0.00 0.48 0.60 0.58\nBERT 0.00 0.38 1.00 0.97 0.59 0.60 0.52 0.36 0.45 0.37 0.57 0.63 0.62\nLinkBERT 0.00 0.55 0.74 1.00 1.00 0.96 0.71 0.43 0.53 0.46 0.84 0.31 0.98\nXLNet 0.00 0.55 0.74 1.00 0.99 0.99 0.90 0.62 0.94 0.60 0.88 0.32 1.00\nRoBERTa 0.81 0.82 0.94 0.95 0.84 0.90 0.46 0.00 0.38 0.00 0.45 1.00 0.92\nAlBERT 0.00 0.64 0.96 0.83 0.97 0.95 0.83 0.46 0.74 0.51 0.71 1.00 0.98\nSpanBERT 0.00 0.25 1.00 0.75 0.54 0.58 0.42 0.23 0.39 0.23 0.49 0.61 0.59\nElectra 0.00 0.26 1.00 0.82 0.55 0.58 0.53 0.39 0.47 0.34 0.53 0.59 0.58\nTable 2: Mean MD values for all LLMs across all datasets. The range of Mean MD is [0,1] with zero being the best\nand one being the worst, except for Negation Dataset (for Negation Dataset one is the best and zero is the worst).\nindicates the necessity of equipping them with the\ncapability to identify implicit relationships. We\nbelieve that the integration of external knowledge\ninto LLMs is a potential solution to enhance their\nperformance on word pair analogies.\nAnalogies at level two (words vs. sentences) are\nalso illustrated to be challenging for the LLMs to\nidentify. These analogies are abstract since a single\nword represents the meaning of a sentence. Ab-\nstraction is an area of NLP that is yet to be studied\nsystematically (Lachmy et al., 2022). There are\nno widely established benchmarks to evaluate the\nperformance of LLMs on abstraction. Therefore\nwe postulate that it is hard for the LLMs to capture\nabstractions, performing poorly at this level.\nThe Random Reordering dataset is the hardest\ndataset for the LLMs at level three of analogy tax-\nonomy compared to Random Deletion and Random\nMasking datasets. The current analogous sentences\nare created using a simple mechanism of delet-\ning, reordering, or masking of words, as opposed\nto replacing nouns and/or verbs with their analo-\ngous counterparts. Therefore the resulting analo-\ngies should be easier for the LLMs to identify, as\nillustrated.\nAt the fifth level, pertaining to entailment, the\nmajority of LLMs demonstrate suboptimal perfor-\nmance, with the exception of T5, RoBERTa, and\nSpanBERT. Textual entailment consists of identify-\ning semantically related sentences, and interpreting\nsemantics is known to be a challenge to LLMs\n(Mayer, 2020), which explains the mean MD val-\nues closer to one.\nOut of eight, six language models struggle to per-\nform well at Metaphor Level. At this level, analo-\ngies are drawn between sentences and paragraphs,\nmainly introducing the issue of compositional-\nity. Compositionality suggests that the meanings\nof complex expressions are constructed from the\nmeanings of the less complex constituents (Fodor\nand Lepore, 2002). The inability of transformers\nto effectively capture the inherent compositionality\nin language, in the absence of suitable prompting\ntechniques, has been extensively observed (Key-\nsers et al., 2019; Furrer et al., 2020). We posit that\nthis limitation directly contributes to the subpar\nperformance of LLMs at this particular level.\n3540\nCosine sim Cosine dis Cosine dis normalizedDatasetT5BERTLinkBERTXLNetRoBERTaAlBERTSpanBERTElectra DatasetT5 BERTLinkBERTXLNetRoBERTaAlBERTSpanBERTElectraBest ValueDatasetT5BERTLinkBERTXLNetRoBERTaAlBERTSpanBERTElectraBest ValueBATS_3.00.520.680.840.980.9990.5090.940.62 BATS_3.00.480.320.160.020.0010.490.060.38 0.00BATS_3.00.460.280.120.000.000.480.030.430.00MSR0.510.910.840.990.9990.4830.930.59 MSR0.490.090.160.010.0010.520.070.41 0.00MSR 0.470.010.120.000.000.510.050.480.00Google0.780.240.140.230.0000.3100.940.92 Google0.220.760.860.771.000.690.060.08 0.06Google0.140.770.870.781.000.700.030.000.00SAT0.900.100.080.110.1060.0750.930.92 SAT0.100.900.920.890.890.930.070.08 0.07SAT 0.000.930.940.900.890.970.050.000.00Crossword0.760.850.810.980.9980.4830.180.64 Crossword0.240.150.190.020.0020.520.820.36 0.00Crossword0.170.070.150.010.000.510.880.400.00Wordnet0.750.210.820.970.9970.7130.270.55 Wordnet0.250.790.180.030.0030.290.730.45 0.00Wordnet0.190.810.140.020.000.250.780.530.00Random reordering0.640.840.920.980.9980.8350.150.52 Random reordering0.360.160.080.020.0020.170.850.48 0.00Random reordering0.310.090.030.010.000.120.920.580.00Random deletion0.280.920.950.990.9990.9390.070.23 Random deletion0.720.080.050.010.0010.060.930.77 0.00Random deletion0.760.000.000.000.000.001.001.000.00Random masking0.570.840.930.980.9980.8670.180.35 Random masking0.430.160.070.020.0020.130.820.65 0.00Random masking0.410.090.020.010.000.080.880.820.00Stanford Negation0.080.040.020.010.0000.0440.970.90 Stanford Negation0.920.960.980.991.000.960.030.10 0.03Stanford Negation1.001.001.001.001.001.000.000.030.00Entailment0.460.840.890.990.9980.8940.190.26 Entailment0.540.160.110.010.0020.110.810.74 0.00Entailment0.540.090.070.000.000.050.880.950.00Proverbs(Epic)0.750.680.780.970.9950.5890.460.38 Proverbs(Epic)0.250.320.220.030.0050.410.540.62 0.00Proverbs(Epic)0.180.270.180.010.000.390.570.770.00Quotes0.600.810.780.980.9960.6610.300.25 Quotes0.400.190.220.020.0040.340.700.75 0.00Quotes0.370.120.180.000.000.310.750.960.00\nEuclidean Euclidean normalizedDatasetT5BERTLinkBERTXLNetRoBERTaAlBERTSpanBERTElectra DatasetT5 BERTLinkBERTXLNetRoBERTaAlBERTSpanBERTElectraBest valueBATS_3.03.5911.547.5431.670.57218.4030.869.14 BATS_3.00.561.000.630.340.370.500.020.430.02 DistanceT5 BERTLinkBERTXLNetRoBERTaAlBERTSpanBERTElectraMSR3.685.787.5230.770.52219.6591.089.83 MSR 0.590.260.620.300.330.560.090.510.09 Cosine0.083662690.03741270.021034610.006443210.000323270.043819520.973336310.89691754Google4.146.2110.2531.900.12111.1301.599.75 Google0.710.320.980.350.000.140.240.500.00 Euclidean0 0 0 00.138841670 0 0SAT5.297.8910.4242.770.25818.6171.1914.17 SAT 1.000.531.000.740.110.510.120.970.11 Mahalanobis00.366751960.459775770.599376360.003579550.509342830.229719350.34233664Crossword4.707.568.5241.890.75026.0382.2314.41 Crossword0.850.490.750.710.520.880.441.000.44Wordnet4.719.218.4245.880.90624.9382.6912.49 Wordnet0.850.700.740.850.650.830.580.790.58Random reordering4.387.755.3942.810.76617.9102.1012.13 Random reordering0.770.510.350.740.530.480.400.760.35Random deletion2.595.434.1429.140.4329.9081.418.08 Random deletion0.310.220.180.250.260.080.190.320.08Random masking4.187.714.9650.010.79615.7692.2610.04 Random masking0.710.510.291.000.560.370.450.530.29Stanford Negation1.403.732.7422.350.2898.3580.795.11 Stanford Negation0.000.000.000.000.140.000.000.000.00Entailment3.538.166.3237.990.67212.9662.298.56 Entailment0.550.570.470.570.460.230.460.370.23Proverbs(Epic)5.0011.548.8542.731.33028.3754.0610.57 Proverbs(Epic)0.931.000.800.741.001.001.000.590.59Quotes4.399.019.0536.510.96726.6713.348.21 Quotes0.770.680.820.510.700.910.780.330.33\nMahalanobis Mahalanobis normalizedDatasetT5BERTLinkBERTXLNetRoBERTaAlBERTSpanBERTElectra DatasetT5 BERTLinkBERTXLNetRoBERTaAlBERTSpanBERTElectraBest ValueBATS_3.016.1212.8616.6716.6736.35216.56116.4916.53 BATS_3.00.040.000.000.000.810.000.000.000.00MSR24.0528.4028.6928.8736.57930.99425.6126.33 MSR 0.320.380.550.550.820.640.250.260.25Google41.3854.1133.0333.0338.16938.25153.7254.50 Google0.951.000.740.740.940.961.001.000.74SAT42.8652.9538.7038.6738.40335.20544.4347.85 SAT 1.000.971.001.000.950.830.750.820.75Crossword31.6837.2938.7038.4136.85538.26636.4737.51 Crossword0.600.591.000.990.840.970.540.550.54Wordnet31.2537.4637.8038.3837.63438.02138.0238.49 Wordnet0.580.600.960.990.900.950.580.580.58Random reordering29.1734.2732.3536.5331.57135.18632.2036.70 Random reordering0.510.520.710.900.460.830.420.530.42Random deletion21.2127.6826.0430.2525.07226.81825.0131.51 Random deletion0.220.360.430.620.000.460.230.390.00Random masking26.4331.2828.2837.2730.34933.18831.0834.33 Random masking0.410.450.530.940.380.740.390.470.38Stanford Negation15.0827.9926.8029.8625.12228.01625.0429.53 Stanford Negation0.000.370.460.600.000.510.230.340.00Entailment28.4836.2435.1136.0031.36632.48534.8336.60 Entailment0.480.570.840.880.450.710.490.530.45Proverbs(Epic)31.6538.8823.6023.6039.06939.05039.0338.97 Proverbs(Epic)0.600.630.310.321.001.000.610.590.31Quotes31.2138.2738.3738.6037.88738.54238.3238.59 Quotes0.580.620.981.000.920.980.590.580.58\nMahalanobisEuclideanCosine disDatasetMahalanobis DistanceEuclidean Distance Cosine Distance BATS_3.00.0000.0200.001MSR 0.2450.0860.001Google0.7430.0000.057SAT 0.7510.1130.072Crossword0.5370.4390.002Wordnet0.5780.5790.003Random reordering0.4220.3450.002Random deletion0.0000.0770.001Random masking0.3770.2890.002Entailment0.4500.2300.002Proverbs(Epic)0.3150.5880.005Quotes0.5800.3330.004\nMahalanobisDatasetT5 BERTLinkBERTGoogle 0.951.000.74XLNetRoBERTaAlBERTSpanBERTElectraSAT 1.000.971.000.740.940.961.001.00Proverbs(Epic)0.600.630.311.000.950.830.750.82Quotes 0.580.620.980.321.001.000.610.591.000.920.980.590.58DatasetLanguage ModelBATS_3.0MSRGoogleT5 0.0374321160.3228506190.946935988SAT CrosswordWordnetRandom reorderingRandom deletionRandom maskingStanford NegationEntailmentProverbs(Epic)QuotesBERT 00.3767107481 10.5975640890.5819537880.5072569880.220664890.40842125900.482511060.596345510.58048585LinkBERT00.5456211020.7425654230.9718737830.59211140.5962840760.5190510360.3591934720.4465215150.3667519570.566744950.630786760.616XLNet 00.5546072880.7435840830.99995952210.9590069740.711818410.4253874130.5268235140.4597757670.83709520.314656370.98468418mahalanobisRoBERTa0.8059160650.8221243120.93575367710.9879159010.986573560.902515830.6172333290.9364565680.5993763620.878408410.315133170.99646961AlBERT 00.6417998190.9645000380.9524407230.8418750310.8974998930.46436962200.3770726390.0035795490.449685141 0.916SpanBERT00.24514966510.8290477620.9651636110.9542690570.8281927270.4560692660.7393287230.5093428350.7080673110.97742399Electra 00.25811835610.7505186430.5367008650.5782365250.4219408360.2288963090.3919565210.2297193530.492581060.60547880.586439030.8249417090.552440950.5784296620.5312168390.394633790.4687794870.3423366430.528692750.591094130.58092976\nDatasetBATS_3.0MSRGoogleT5 0.4769589560.4871968840.218285024SATCrosswordWordnetRandom reorderingRandom deletionRandom maskingStanford NegationEntailmentProverbs(Epic)QuotesBERT0.3240613840.0889636510.7556737610.1001404590.2396835840.2539470420.3558444070.7224152950.4340817890.9163373140.542981940.247581010.39875366LinkBERT0.1632402020.1575580680.8579061160.8979641870.1464752620.791692270.1588890630.0804442340.1570.9630.1560.320981190.18823932XLNet0.0151183930.01440270.7733116250.9227222680.1908247720.1814281890.0759618690.047477090.0690.9790.1110.217854970.2188713RoBERTa0.0012736860.0010870660.9996194340.8906303570.0233912210.0281690450.0213561680.011322720.0250.9940.0140.025897280.01522176AlBERT0.4914607810.5167532520.6896909360.8935587590.0020472260.0030341460.0021974420.0007106080.0021.0000.0020.004877760.00355197cosineSpanBERT0.0558288280.0684429240.0568547530.9253690880.5171760610.2871840890.1650462060.0608179760.1330.9560.1060.411454910.33934313Electra0.3804565420.4143378520.0805781890.0724053280.8157978270.730371130.8497450730.9251540980.8152919680.0266636890.813357040.537679140.696160270.0838480020.3584100020.4501211070.4822616420.7734232810.6470.1030.7380.616334930.74783986DatasetBATS_3.0MSRGoogleT5 0.5647624620.5860776480.705375537SATCrosswordWordnetRandom reorderingRandom deletionRandom maskingStanford NegationEntailmentProverbs(Epic)QuotesBERT 1 0.262683450.31810.8501533290.8511646230.7669936880.3070836060.7142476500.549596410.925946130.7697727LinkBERT0.6258734130.6225825350.9780.5330.4890.7018322110.5138726570.2168516780.50969295300.566978220.999771770.67569484EuclideanXLNet0.3368285120.3043274080.3451.0000.7520.7403537210.3451405090.1821034710.28928350700.466631510.796423150.82178542RoBERTa0.3730271280.3320113690.0000.7380.7060.850528510.739742150.2455885661 00.56545260.736787320.51198087AlBERT0.5018294590.5645982650.1380.1130.5200.6496699670.5335734490.2573133430.5580835820.1388416660.4556177710.69963832SpanBERT0.0195876920.0862966580.242802570.5130.8830.8282984520.4772285630.0774727920.37027229500.2302299910.91486737Electra0.4330553090.5075450650.5000.120246610.4392315810.5789643820.3984952070.1871117990.4489533800.4591002210.77975650.9751.0000.7935178730.7551219640.3195423430.53063952900.37101060.587547410.33313212\nDatasetSpanBERTBATS_3.00.000MSR 0.245Google1.000SAT 0.751Crossword0.537Wordnet0.578Random reordering0.422Random deletion0.229Random masking0.392Stanford Negation0.230Entailment0.493Proverbs(Epic)0.605Quotes0.586\n-0.100.000.100.200.300.40\n0.500.600.700.80\nBATS_3.0MS RGoogleSATCrosswordWordnetRandomreorderingRandomdeletionRandommaskingEntailmentProverbs(Epic)Quotes\nMahalanobis DistanceEuclidean DistanceCosine Distance\nIncreasing AnalogyComplexity WordLevel DatasetsWords vs. Sentence Level DatasetsSyntacticAnalogies DatasetsEntailmentDatasetMetaphor Datasets\nSpanBERT\nLinkBERTXLNETSpanBERT\nT5SpanBERT \nSpanBERT\nRoBERTa\nRoBERTaRoBERTa\nLinkBERT\nT5\nSpanBERT\nRoBERTa\nRoBERTa\nSpanBERTLinkBERT\nALBERT\nLinkBERTRoBERTa\nELECTRA\nELECTRA\nRoBERTaRoBERTa\nSpanBERTSpanBERTRoBERTaRoBERTaRoBERTaRoBERTaRoBERTa RoBERTaRoBERTaRoBERTaSpanBERT\nBERTLinkBERTXLNETALBERTASpanBERTELECTRA\nSpanBERT\n0.000.200.400.600.801.001.20\nBATS_3.0MS RGoogleSATCr osswordWo rd netRandom reorderingRandom deletionRandom maskingStanford NegationEntailmentProverbs(Epic)Quotes\nChart Title\nT5 BERTLinkBERTXLNetRoBERTaAlBERTSpanBERTElectraBest Value\n00.20.40.60.811.2\n0 2 4 6 8 101214\nT5 BERTLinkBERTXLNetRoBERTaAlBERTSpanBERTElectra\n00.20.40.60.811.2\n0 2 4 6 8 101214\nT5 BERTLinkBERTXLNetRoBERTaAlBERTSpanBERTElectra\n00.20.40.60.811.2\n0 2 4 6 8 101214\nT5 BERTLinkBERTXLNetRoBERTaAlBERTSpanBERTElectra\n00.20.40.60.811.2\nT5\nBERT\nLinkBERT\nXLNet\nRoBERTa\nAlBERT\nSpanBERT\nElectra\nCosineEuclideanMahalanobis\nNormalizedDistance\nFigure 5: Best performing model(s) for each dataset in each level of the analogy taxonomy (Performance on the\nNegation Dataset is shown separately in Figure 6). The range of each normalized distance measure is [0,1], with\nzero being the best and one being the worst.\n7.2 Performance on Negation Dataset\nFigure 6 illustrates the performance of LLMs on\nthe Negation Dataset. XLNET performs the best\nwith a mean MD of 0.6. T5 and RoBERTa record\nthe poorest performance by placing the negations\npairs very closely in the vector space. This per-\nformance is justified based on previous research\non negation identification by pretrained language\nmodels (Kassner and Schütze, 2020).\n00.20.40.60.811.2\nT5\nBERT\nLinkBERT\nXLNet\nRoBERTa\nAlBERT\nSpanBERT\nElectra\nCosineEuclideanMahalanobis\nFigure 6: Performance of LLMs on the Negation dataset.\nThe range of each normalized distance measure is [0,1],\nwith zero being the worst and one being the best.\n7.3 Best performing LLMs\nIn Figure 5, we illustrate the best-performing mod-\nels and their performance at each level of the anal-\nogy taxonomy across the three distance measures,\nED, CD and MD. We see that RoBERTa performs\nthe best based on mean CD values close to zero\nat all most all levels. However, CD considers all\nvector dimensions of a lexical item to be equally\nvaluable and uncorrelated, which we reveal to be\nincorrect in section 5.2. Therefore we focus on\nthe best-performing LLMs based on their mean\nMD values. We see that except for the Random\nDeletion dataset, the best performance for other\ndatasets shows a general upward trend, indicating\nthat it is increasingly hard for LLMs to identify\nanalogous pairs when the complexity of the analo-\ngies increases.\n8 Conclusion & Future Avenues\nThis work introduces ANALOGICAL, a bench-\nmark for LLMs based on a taxonomy of six levels\nof analogies. Through comprehensive experiments,\nwe show that LLMs increasingly struggle to iden-\ntify analogies when the complexity of analogies\nincrease (going up the analogy taxonomy). The\ndatasets derived for level three are crude at this\ntime. In the future, we will incorporate more chal-\nlenging and comprehensive datasets to this level.\nWe also will move on from this empirical study\nto investigate why some LLMs perform well at\nspecific levels and not others.\n9 Limitations\nSyntactic analogies at level three consist of simple\nalterations of sentences based on deleting, reorder-\n3541\ning, and masking of random words. A more so-\nphisticated method of creating syntactic analogies\nwould be to replace nouns/ verbs in sentences with\nnouns and verbs of similar meaning, which is not\nexplored in this work.\nIn this study, we utilize the [CLS] token as the\nrepresentation of lexical items in analogies. While\nprevious research efforts have investigated the opti-\nmal representations of lexical items in Large Lan-\nguage Models (LLMs) (Reimers and Gurevych,\n2019; Li et al., 2020), we have chosen not to incor-\nporate these findings into our current investigation.\nThis work uses mean distance measures to cap-\nture the LLMs’ ability to identify analogies. How-\never, there could be data points more challenging\nfor the LLMs to capture than others within the\nsame dataset or across datasets at the same level\nof the analogy taxonomy. Relying solely on mean\ndistance values ignores this detail and considers all\nthe data points equal, which is suboptimal.\nAcknowledgements\nWe thank Dr. Krishnaprasad Thirunarayan for his\nvaluable feedback and the anonymous reviewers\nfor their constructive comments. This work was\nsupported in part by the NSF grant #2133842: EA-\nGER: Advancing Neuro-symbolic AI with Deep\nKnowledge-infused Learning. Any opinions, find-\nings, conclusions, or recommendations expressed\nin this material are those of the authors and do not\nnecessarily reflect the views of the funding organi-\nzation.\nReferences\nSaurabh Agarwala, Aniketh Anagawadi, and Ram Mo-\nhana Reddy Guddeti. 2021. Detecting semantic simi-\nlarity of documents using natural language process-\ning. Procedia Computer Science, 189:128–135.\nValarmathi Balasubramanian, Srinivasa Gupta Nagara-\njan, and Palanisamy Veerappagoundar. 2016. Maha-\nlanobis distance-the ultimate measure for sentiment\nanalysis. Int. Arab J. Inf. Technol., 13(2):252–257.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nDanushka T Bollegala, Yutaka Matsuo, and Mitsuru\nIshizuka. 2009. Measuring the similarity between\nimplicit semantic relations from the web. In Proceed-\nings of the 18th international conference on World\nwide web, pages 651–660.\nSamuel Bowman, Gabor Angeli, Christopher Potts, and\nChristopher D Manning. 2015. A large annotated\ncorpus for learning natural language inference. In\nProceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing, pages 632–\n642.\nSamuel Bowman, Luke Vilnis, Oriol Vinyals, Andrew\nDai, Rafal Jozefowicz, and Samy Bengio. 2016. Gen-\nerating sentences from a continuous space. In Pro-\nceedings of the 20th SIGNLL Conference on Compu-\ntational Natural Language Learning, pages 10–21.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than generators.\narXiv preprint arXiv:2003.10555.\nTamara Czinczoll, Helen Yannakoudakis, Pushkar\nMishra, and Ekaterina Shutova. 2022. Scientific\nand creative analogies in pretrained language mod-\nels. In Findings of the Association for Computational\nLinguistics: EMNLP 2022 , pages 2094–2100, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nVerna Dankers, Elia Bruni, and Dieuwke Hupkes. 2022.\nThe paradox of the compositionality of natural lan-\nguage: A neural machine translation case study. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 4154–4175, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nMarie-Catherine De Marneffe, Anna N Rafferty, and\nChristopher D Manning. 2008. Finding contradic-\ntions in text. In Proceedings of acl-08: Hlt , pages\n1039–1047.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nJohn Rupert Firth. 1957. \"A synopsis of linguistic theory\n1930-1955.\". Oxford University Press.\nJerry A Fodor and Ernest Lepore. 2002. The composi-\ntionality papers. Oxford University Press.\nDaniel Furrer, Marc van Zee, Nathan Scales, and\nNathanael Schärli. 2020. Compositional generaliza-\ntion in semantic parsing: Pre-training vs. specialized\narchitectures. arXiv preprint arXiv:2007.08970.\nBin Gao, Jiang Bian, and Tie-Yan Liu. 2014. Wordrep:\nA benchmark for research on learning word represen-\ntations. arXiv preprint arXiv:1407.1640.\nDedre Gentner and Arthur B Markman. 1997. Struc-\nture mapping in analogy and similarity. American\npsychologist, 52(1):45.\nSayan Ghosh and Shashank Srivastava. 2022. ePiC:\nEmploying proverbs in context as a benchmark for\nabstract language understanding. In Proceedings\n3542\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 3989–4004, Dublin, Ireland. Association for\nComputational Linguistics.\nAnna Gladkova, Aleksandr Drozd, and Satoshi Mat-\nsuoka. 2016a. Analogy-based detection of morpho-\nlogical and semantic relations with word embeddings:\nwhat works and what doesn’t. In Proceedings of the\nNAACL Student Research Workshop, pages 8–15, San\nDiego, California. Association for Computational\nLinguistics.\nAnna Gladkova, Aleksandr Drozd, and Satoshi Mat-\nsuoka. 2016b. Analogy-based detection of morpho-\nlogical and semantic relations with word embeddings:\nWhat works and what doesn’t. In Proceedings of the\nNAACL-HLT SRW, pages 47–54, San Diego, Califor-\nnia, June 12-17, 2016. ACL.\nGene H Golub and Charles F Van Loan. 2013. Matrix\ncomputations. JHU press.\nMengting Han, Xuan Zhang, Xin Yuan, Jiahao Jiang,\nWei Yun, and Chen Gao. 2021. A survey on the tech-\nniques, applications, and performance of short text\nsemantic similarity. Concurrency and Computation:\nPractice and Experience, 33(5):e5971.\nMickel Hoang, Oskar Alija Bihorac, and Jacobo Rouces.\n2019. Aspect-based sentiment analysis using bert.\nIn Proceedings of the 22nd nordic conference on\ncomputational linguistics, pages 187–196.\nK Holyoak, Dedre Gentner, and B Kokinov. 2001. The\nplace of analogy in cognition. The analogical mind:\nPerspectives from cognitive science, 119.\nNicholas Ichien, Hongjing Lu, and Keith J Holyoak.\n2020. Verbal analogy problem sets: An inventory\nof testing materials. Behavior research methods ,\n52(5):1803–1816.\nEsa Itkonen. 2005. Analogy as structure and process:\nApproaches in linguistics, cognitive psychology and\nphilosophy of science, volume 14. John Benjamins\nPublishing.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Spanbert:\nImproving pre-training by representing and predict-\ning spans. Transactions of the Association for Com-\nputational Linguistics, 8:64–77.\nNora Kassner and Hinrich Schütze. 2020. Negated and\nmisprimed probes for pretrained language models:\nBirds can talk, but cannot fly. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 7811–7818, Online. Asso-\nciation for Computational Linguistics.\nDaniel Keysers, Nathanael Schärli, Nathan Scales,\nHylke Buisman, Daniel Furrer, Sergii Kashubin,\nNikola Momchev, Danila Sinopalnikov, Lukasz\nStafiniak, Tibor Tihon, et al. 2019. Measuring com-\npositional generalization: A comprehensive method\non realistic data. arXiv preprint arXiv:1912.09713.\nRoyi Lachmy, Valentina Pyatkin, Avshalom Manevich,\nand Reut Tsarfaty. 2022. Draw me a flower: Process-\ning and grounding abstraction in natural language.\nTransactions of the Association for Computational\nLinguistics, 10:1341–1356.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nBohan Li, Hao Zhou, Junxian He, Mingxuan Wang,\nYiming Yang, and Lei Li. 2020. On the sentence\nembeddings from pre-trained language models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9119–9130, Online. Association for Computa-\ntional Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nPrasanta Chandra Mahalanobis. 1936. On the gener-\nalized distance in statistics. National Institute of\nScience of India.\nTolba Marwa, Ouadfel Salima, and Meshoul Souham.\n2018. Deep learning for online harassment detection\nin tweets. In 2018 3rd International Conference\non Pattern Analysis and Intelligent Systems (PAIS),\npages 1–5. IEEE.\nTobias Mayer. 2020. Enriching language models with\nsemantics. In ECAI 2020-24th European Conference\non Artificial Intelligence.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013a. Efficient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nTomas Mikolov, Kai Chen, Gregory S. Corrado, and\nJeffrey Dean. 2013b. Efficient estimation of word\nrepresentations in vector space. In International Con-\nference on Learning Representations.\nGeorge A. Miller. 1992. WordNet: A lexical database\nfor English. In Speech and Natural Language: Pro-\nceedings of a Workshop Held at Harriman, New York,\nFebruary 23-26, 1992.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pages 1532–1543, Doha, Qatar.\nAssociation for Computational Linguistics.\nPiotr Pi˛ ekos, Mateusz Malinowski, and Henryk\nMichalewski. 2021. Measuring and improving\n3543\nBERT’s mathematical abilities by predicting the or-\nder of reasoning. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 2:\nShort Papers), pages 383–394, Online. Association\nfor Computational Linguistics.\nSaul Pwanson. 2016. Download crossword data.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2022. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(1).\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\nShafin Rahman, Salman Khan, and Fatih Porikli. 2018.\nA unified approach for conventional zero-shot, gener-\nalized zero-shot, and few-shot learning. IEEE Trans-\nactions on Image Processing, 27(11):5652–5667.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nLindsey Engle Richland and Nina Simms. 2015. Anal-\nogy, higher order thinking, and education. Wiley In-\nterdisciplinary Reviews: Cognitive Science, 6(2):177–\n192.\nDwijen Rudrapal, Amitava Das, and Baby Bhattacharya.\n2017. Quotology-reading between the lines of quota-\ntions. In International Conference on Applications\nof Natural Language to Information Systems, pages\n292–296. Springer.\nShashi Pal Singh, Ajai Kumar, Hemant Darbari, Lenali\nSingh, Anshika Rastogi, and Shikha Jain. 2017. Ma-\nchine translation using deep learning: An overview.\nIn 2017 international conference on computer, com-\nmunications and electronics (comptelix), pages 162–\n167. IEEE.\nP Sunilkumar and Athira P Shaji. 2019. A survey on\nsemantic similarity. In 2019 International Confer-\nence on Advances in Computing, Communication\nand Control (ICAC3), pages 1–8. IEEE.\nYulia Tsvetkov, Manaal Faruqui, Wang Ling, Guillaume\nLample, and Chris Dyer. 2015. Evaluation of word\nvector representations by subspace alignment. In Pro-\nceedings of the 2015 Conference on Empirical Meth-\nods in Natural Language Processing , pages 2049–\n2054.\nPeter D Turney. 2008. The latent relation mapping\nengine: Algorithm and experiments. Journal of Arti-\nficial Intelligence Research, 33:615–655.\nAsahi Ushio, Luis Espinosa Anke, Steven Schockaert,\nand Jose Camacho-Collados. 2021. Bert is to nlp\nwhat alexnet is to cv: Can pre-trained language mod-\nels identify analogies? In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 3609–3624.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In Proceedings of\nthe 2018 EMNLP Workshop BlackboxNLP: Analyz-\ning and Interpreting Neural Networks for NLP, pages\n353–355.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. Advances in neural informa-\ntion processing systems, 32.\nMichihiro Yasunaga, Jure Leskovec, and Percy Liang.\n2022. Linkbert: Pretraining language models with\ndocument links. In Association for Computational\nLinguistics (ACL).\nJiajun Zhang, Chengqing Zong, et al. 2015. Deep neural\nnetworks in machine translation: An overview. IEEE\nIntell. Syst., 30(5):16–25.\nXuhui Zhou, Yue Zhang, Leyang Cui, and Dandan\nHuang. 2020. Evaluating commonsense in pre-\ntrained language models. In Proceedings of the AAAI\nConference on Artificial Intelligence , volume 34,\npages 9733–9740.\n3544\nA Detailed Results\nLanguage\nModel BATS_3.0 MSR Google SAT Crossword Wordnet Random\nReordering\nRandom\nDeletion\nRandom\nMasking\nStanford\nNegation Entailment Proverbs\n(Epic) Quotes\nT5 0.04 0.32 0.95 1.00 0.60 0.58 0.51 0.22 0.41 0.00 0.48 0.60 0.58\nBERT 0.00 0.38 1.00 0.97 0.59 0.60 0.52 0.36 0.45 0.37 0.57 0.63 0.62\nLinkBERT 0.00 0.55 0.74 1.00 1.00 0.96 0.71 0.43 0.53 0.46 0.84 0.31 0.98\nXLNet 0.00 0.55 0.74 1.00 0.99 0.99 0.90 0.62 0.94 0.60 0.88 0.32 1.00\nRoBERTa 0.81 0.82 0.94 0.95 0.84 0.90 0.46 0.00 0.38 0.00 0.45 1.00 0.92\nAlBERT 0.00 0.64 0.96 0.83 0.97 0.95 0.83 0.46 0.74 0.51 0.71 1.00 0.98\nSpanBERT 0.00 0.25 1.00 0.75 0.54 0.58 0.42 0.23 0.39 0.23 0.49 0.61 0.59\nElectra 0.00 0.26 1.00 0.82 0.55 0.58 0.53 0.39 0.47 0.34 0.53 0.59 0.58\nTable 3: Cosine Distance\nLanguage\nModel BATS_3.0 MSR Google SAT Crossword Wordnet Random\nReordering\nRandom\nDeletion\nRandom\nMasking\nStanford\nNegation Entailment Proverbs\n(Epic) Quotes\nT5 0.56 0.59 0.71 1.00 0.85 0.85 0.77 0.31 0.71 0.00 0.55 0.93 0.77\nBERT 1.00 0.26 0.32 0.53 0.49 0.70 0.51 0.22 0.51 0.00 0.57 1.00 0.68\nLinkBERT 0.63 0.62 0.98 1.00 0.75 0.74 0.35 0.18 0.29 0.00 0.47 0.80 0.82\nXLNet 0.34 0.30 0.35 0.74 0.71 0.85 0.74 0.25 1.00 0.00 0.57 0.74 0.51\nRoBERTa 0.37 0.33 0.00 0.11 0.52 0.65 0.53 0.26 0.56 0.14 0.46 1.00 0.70\nAlBERT 0.50 0.56 0.14 0.51 0.88 0.83 0.48 0.08 0.37 0.00 0.23 1.00 0.91\nSpanBERT 0.02 0.09 0.24 0.12 0.44 0.58 0.40 0.19 0.45 0.00 0.46 1.00 0.78\nElectra 0.43 0.51 0.50 0.97 1.00 0.79 0.76 0.32 0.53 0.00 0.37 0.59 0.33\nTable 4: Euclidean Distance (Normalized)\nB Details on Distance measures\nB.1 Euclidean Distance (ED)\nEuclidean distance is used to measure how far apart (in a straight line) two points are, in a vector space.\nIf point xand yare represented in a higher dimensional vector space by [x1,⋯,xn] and [y1,⋯,yn]\nrespectively, ED between xand yare given by:\nED(x,y) =\n√\n√√√√√√⎷\ni=n\n∑\ni=1\n(xi −yi)2\nValues of ED range from 0 to infinity. Zero indicates the two points are similar and larger numbers\nindicate the two points are far apart in the vector space and less similar.\nB.2 Cosine Distance (CD)\nCosine similarity is a standard measure of similarity which measure the angle between two points in a\nvector space by taking into account the orientations of the vectors regardless of the vector sizes. Given\npoints U =[ui,⋯,un] and V =[vi,⋯,vn] in high-dimensional space cosine similarity between uand\nvis given by:\nCS(U,V ) =cos(θ) = ∑i=n\ni=1 (uivi)\n√\n∑i=n\ni=1 u2\n√\n∑i=n\ni=1 v2\nWe convert cosine similarity to cosine distance for easy comparison with Euclidian and Mahanalobis\ndistances by subtracting cosine similarity from one.\n3545\nC Details on Large Language Models\nBidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018)is trained\non document-level corpora consisting of the BooksCorpus and English Wikipedia words through two\nunsupervised training tasks. In Masked Language Modeling (MLM) some tokens of input sequences are\nreplaced randomly by a [MASK] token requiring BERT to predict the masked tokens allowing the LM to\ncapture the directional nature of the language. To capture the relationships among sentences, BERT is\ntrained on a second training objective known as Next Sentence Prediction (NSP).\nXLNet (Yang et al., 2019) is a generalized autoregressive language model trained on corpora used by\nBERT as well as Giga5 (16GB text), ClueWeb 2012-B and Common Crawl corpora. XLNet improves\nupon BERT and introduces a permutation language modeling objective that retains benefits from both\nautoregressive and autoencoding pretraining objectives.\nRoBERTa (Liu et al., 2019)is as an optimized pretrained version of BERT, trained on a dataset ten\ntimes larger than BERT (16GB vs. 160GB) including the original dataset used to train BERT. In addition\nthree other corpora containing news articles, web content, and a filtered subset of the CommonCrawl\ncorpus were used. The training approach of RoBERTa differs from BERT as follows. RoBERTa modifies\nthe MLM task by moving from static masking to dynamic masking where the masked tokens change at\neach epoch, thereby effectively leading to an increase in the diversity of learning opportunities for the\nmodel. RoBERTa removes NSP loss from the training objective arguing that the NSP loss was no longer\nrequired for better performance.\nA Lite BERT for Self-supervised Learning of Language Representations (ALBERT) (Lan et al.,\n2019) targets to reduce the parameter size without affecting the performance of BERT. The LM is trained\nwith the same corpora as BERT, yet three main changes to the BERT’s design choices are made. The first\nchange is feature factorization where input and hidden layers are decoupled from each other. Input vectors\nare first projected, to a lower dimensional embedding space and then into the hidden space, reducing the\nparameter size significantly. Secondly, parameters are shared across all layers (feed-forward and attention\nlayers). Finally, ALBERT introduces a Sentence Order Prediction (SOP) loss in place of NSP, which is\nbased on inter-sentence coherence.\nEfficiently Learning an Encoder that Classifies Token Replacements Accurately (ELECTRA)\n(Clark et al., 2020) introduces a new, more efficient training task aiming to reduce the computing\npower and retain or exceed the performance of previous BERT-based models pretrained on MLM task.\nELECTRA’s architecture includes two transformers, a generator, and a discriminator. The generator\npredicts the masked token from an input sequence and the resulting sequence is sent to the discriminator,\nwhich then predicts which tokens are original and which are predicted by the generator.\nSpanBERT (Joshi et al., 2020) is specifically pretrained for improved predictions of spans of texts.\nSpanBERT introduces a new masking technique where spans of contiguous tokens are masked instead\nof individual tokens as in BERT. Also, the authors introduce a new training objective where the span\nboundary representations are used to predict the entire content of the masked span.\nText-to-Text Transfer Transformer (T5) (Raffel et al., 2020)aims to introduce a unified framework\nfor downstream NLP tasks. T5 is trained on the Colossal Clean Crawled Corpus (C4) introduced by\nthe authors by removing text that is not natural language from the Common Crawl corpus. T5 has the\nvanilla encoder-decoder transformer architecture with an unsupervised training objective introduced by\nthe authors inspired by MLM of BERT and word dropout regularization technique by (Bowman et al.,\n2016).\nA Knowledgeable Language Model Pretrained with Document Links (LinkBERT) (Yasunaga\net al., 2022) is an improvement over BERT, that incorporates document link knowledge into pretraining.\nThe LM is trained on two joint objectives, MLM and Document Relation Prediction (DRP) and uses the\nsame training dataset as BERT.\n3546\nD Implementation Details\nHugging Face1 implementation of the LLMs (base configuration) are used to extract the word/sentence\nrepresentations. In this study, we use the default configuration provided by Hugging Face (embedding\nsize 768) if not specified otherwise. Scikit-learn2 is used to implement the distance measures.\n1https://huggingface.co/models\n2https://scikit-learn.org/stable/index.html\n3547\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n9\n□\u0017 A2. Did you discuss any potential risks of your work?\nMy work does not have any potential risks\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nLeft blank.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nNot applicable. Left blank.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNot applicable. Left blank.\nC □\u0013 Did you run computational experiments?\nLeft blank.\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n6\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n3548\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n6\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n7\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n6, Appendix\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n3549",
  "topic": "Analogy",
  "concepts": [
    {
      "name": "Analogy",
      "score": 0.8249108791351318
    },
    {
      "name": "Computer science",
      "score": 0.5974454879760742
    },
    {
      "name": "Computational linguistics",
      "score": 0.5775265693664551
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5599020719528198
    },
    {
      "name": "Natural language processing",
      "score": 0.5018396377563477
    },
    {
      "name": "Artificial intelligence",
      "score": 0.424257755279541
    },
    {
      "name": "Linguistics",
      "score": 0.2996046543121338
    },
    {
      "name": "Philosophy",
      "score": 0.12211474776268005
    },
    {
      "name": "Geography",
      "score": 0.10081171989440918
    },
    {
      "name": "Cartography",
      "score": 0.07179465889930725
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I155781252",
      "name": "University of South Carolina",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I165831266",
      "name": "Nirma University",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I74796645",
      "name": "Birla Institute of Technology and Science, Pilani",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I68891433",
      "name": "Indian Institute of Technology Delhi",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I119939252",
      "name": "Indraprastha Institute of Information Technology Delhi",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I1311688040",
      "name": "Amazon (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210137306",
      "name": "Stanford Medicine",
      "country": "US"
    }
  ]
}