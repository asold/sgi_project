{
  "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context",
  "url": "https://openalex.org/W2964110616",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A5091869105",
      "name": "Zihang Dai",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5050303775",
      "name": "Zhilin Yang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5106542734",
      "name": "Yiming Yang",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5109861718",
      "name": "Jaime Carbonell",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5088551093",
      "name": "Quoc V. Le",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5071983998",
      "name": "Ruslan Salakhutdinov",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2891369367",
    "https://openalex.org/W2952729433",
    "https://openalex.org/W2964348070",
    "https://openalex.org/W2963970792",
    "https://openalex.org/W1800356822",
    "https://openalex.org/W2963351145",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W2963573053",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W2611669587",
    "https://openalex.org/W4302375066",
    "https://openalex.org/W4294555862",
    "https://openalex.org/W4297788867",
    "https://openalex.org/W2197913429",
    "https://openalex.org/W2962746461",
    "https://openalex.org/W2567070169",
    "https://openalex.org/W4293718192",
    "https://openalex.org/W2964059481",
    "https://openalex.org/W2952339051",
    "https://openalex.org/W2804845563",
    "https://openalex.org/W2952723479",
    "https://openalex.org/W2963983719",
    "https://openalex.org/W4300687381",
    "https://openalex.org/W2618854269",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2963735467",
    "https://openalex.org/W2963951265",
    "https://openalex.org/W1899504021",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2964347220",
    "https://openalex.org/W2950527759",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W4299838440",
    "https://openalex.org/W4303633609",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W2962754271",
    "https://openalex.org/W2900096133",
    "https://openalex.org/W2778817245",
    "https://openalex.org/W2510842514",
    "https://openalex.org/W2963266340",
    "https://openalex.org/W2964019776",
    "https://openalex.org/W2793273050",
    "https://openalex.org/W2605203995",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2553303224",
    "https://openalex.org/W4394642966",
    "https://openalex.org/W2962964385",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2963537482",
    "https://openalex.org/W2525246036",
    "https://openalex.org/W2951104886",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2963088785",
    "https://openalex.org/W2212703438",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W2519314406",
    "https://openalex.org/W2792764867",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2810075754",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2549416390",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2145543707",
    "https://openalex.org/W2207587218",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W2963304263",
    "https://openalex.org/W2963034893",
    "https://openalex.org/W2549476280",
    "https://openalex.org/W2964269252",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2795285343",
    "https://openalex.org/W2891815651",
    "https://openalex.org/W2792376130",
    "https://openalex.org/W2140679639"
  ],
  "abstract": "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",
  "full_text": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978–2988\nFlorence, Italy, July 28 - August 2, 2019.c⃝2019 Association for Computational Linguistics\n2978\nTransformer-XL: Attentive Language Models\nBeyond a Fixed-Length Context\nZihang Dai⇤12, Zhilin Yang⇤12, Yiming Yang1, Jaime Carbonell1,\nQuoc V . Le2, Ruslan Salakhutdinov1\n1Carnegie Mellon University,2Google Brain\n{dzihang,zhiliny,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com\nAbstract\nTransformers have a potential of learning\nlonger-term dependency, but are limited by a\nﬁxed-length context in the setting of language\nmodeling. We propose a novel neural ar-\nchitecture Transformer-XL that enables learn-\ning dependency beyond a ﬁxed length with-\nout disrupting temporal coherence. It con-\nsists of a segment-level recurrence mechanism\nand a novel positional encoding scheme. Our\nmethod not only enables capturing longer-term\ndependency, but also resolves the context frag-\nmentation problem. As a result, Transformer-\nXL learns dependency that is 80% longer than\nRNNs and 450% longer than vanilla Trans-\nformers, achieves better performance on both\nshort and long sequences, and is up to 1,800+\ntimes faster than vanilla Transformers during\nevaluation. Notably, we improve the state-of-\nthe-art results of bpc/perplexity to 0.99 on en-\nwiki8, 1.08 on text8, 18.3 on WikiText-103,\n21.8 on One Billion Word, and 54.5 on Penn\nTreebank (without ﬁnetuning). When trained\nonly on WikiText-103, Transformer-XL man-\nages to generate reasonably coherent, novel\ntext articles with thousands of tokens. Our\ncode, pretrained models, and hyperparameters\nare available in both Tensorﬂow and PyTorch1.\n1 Introduction\nLanguage modeling is among the important prob-\nlems that require modeling long-term dependency,\nwith successful applications such as unsupervised\npretraining (Dai and Le, 2015; Peters et al., 2018;\nRadford et al., 2018; Devlin et al., 2018). How-\never, it has been a challenge to equip neural\nnetworks with the capability to model long-term\ndependency in sequential data. Recurrent neu-\nral networks (RNNs), in particular Long Short-\n⇤Equal contribution. Order determined by swapping the\none inYang et al.(2017).\n1https://github.com/kimiyoung/\ntransformer-xl\nTerm Memory (LSTM) networks (Hochreiter and\nSchmidhuber, 1997), have been a standard solu-\ntion to language modeling and obtained strong\nresults on multiple benchmarks. Despite the\nwide adaption, RNNs are difﬁcult to optimize\ndue to gradient vanishing and explosion (Hochre-\niter et al., 2001), and the introduction of gat-\ning in LSTMs and the gradient clipping tech-\nnique (Graves, 2013) might not be sufﬁcient to\nfully address this issue. Empirically, previous\nwork has found that LSTM language models use\n200 context words on average (Khandelwal et al.,\n2018), indicating room for further improvement.\nOn the other hand, the direct connections be-\ntween long-distance word pairs baked in atten-\ntion mechanisms might ease optimization and en-\nable the learning of long-term dependency (Bah-\ndanau et al., 2014; Vaswani et al., 2017). Re-\ncently, Al-Rfou et al.(2018) designed a set of aux-\niliary losses to train deep Transformer networks\nfor character-level language modeling, which out-\nperform LSTMs by a large margin. Despite the\nsuccess, the LM training inAl-Rfou et al.(2018)\nis performed on separated ﬁxed-length segments\nof a few hundred characters, without any informa-\ntion ﬂow across segments. As a consequence of\nthe ﬁxed context length, the model cannot capture\nany longer-term dependency beyond the prede-\nﬁned context length. In addition, the ﬁxed-length\nsegments are created by selecting a consecutive\nchunk of symbols without respecting the sentence\nor any other semantic boundary. Hence, the model\nlacks necessary contextual information needed to\nwell predict the ﬁrst few symbols, leading to inef-\nﬁcient optimization and inferior performance. We\nrefer to this problem ascontext fragmentation.\nTo address the aforementioned limitations of\nﬁxed-length contexts, we propose a new architec-\nture called Transformer-XL (meaning extra long).\nWe introduce the notion of recurrence into our\n2979\ndeep self-attention network. In particular, instead\nof computing the hidden states from scratch for\neach new segment, we reuse the hidden states ob-\ntained in previous segments. The reused hidden\nstates serve as memory for the current segment,\nwhich builds up a recurrent connection between\nthe segments. As a result, modeling very long-\nterm dependency becomes possible because in-\nformation can be propagated through the recur-\nrent connections. Meanwhile, passing informa-\ntion from the previous segment can also resolve\nthe problem of context fragmentation. More im-\nportantly, we show the necessity of using relative\npositional encodings rather than absolute ones, in\norder to enable state reuse without causing tem-\nporal confusion. Hence, as an additional techni-\ncal contribution, we introduce a simple but more\neffective relative positional encoding formulation\nthat generalizes to attention lengths longer than the\none observed during training.\nTransformer-XL obtained strong results on ﬁve\ndatasets, varying from word-level to character-\nlevel language modeling. Transformer-XL is also\nable to generate relatively coherent long text arti-\ncles with thousands of tokens (see AppendixE),\ntrained on only 100M tokens.\nOur main technical contributions include intro-\nducing the notion of recurrence in a purely self-\nattentive model and deriving a novel positional en-\ncoding scheme. These two techniques form a com-\nplete set of solutions, as any one of them alone\ndoes not address the issue of ﬁxed-length con-\ntexts. Transformer-XL is the ﬁrst self-attention\nmodel that achieves substantially better results\nthan RNNs on both character-level and word-level\nlanguage modeling.\n2 Related Work\nIn the last few years, the ﬁeld of language mod-\neling has witnessed many signiﬁcant advances,\nincluding but not limited to devising novel ar-\nchitectures to better encode the context (Bengio\net al., 2003; Mikolov et al., 2010; Merity et al.,\n2016; Al-Rfou et al., 2018), improving regulariza-\ntion and optimization algorithms (Gal and Ghahra-\nmani, 2016) , speeding up the Softmax computa-\ntion (Grave et al., 2016a) , and enriching the output\ndistribution family (Yang et al., 2017).\nTo capture the long-range context in language\nmodeling, a line of work directly feeds a repre-\nsentation of the wider context into the network\nas an additional input. Existing works range\nfrom ones where context representations are man-\nually deﬁned (Mikolov and Zweig, 2012; Ji et al.,\n2015; Wang and Cho, 2015) to others that rely on\ndocument-level topics learned from data (Dieng\net al., 2016; Wang et al., 2017).\nMore broadly, in generic sequence modeling,\nhow to capture long-term dependency has been a\nlong-standing research problem. From this per-\nspective, since the ubiquitous adaption of LSTM,\nmany efforts have been spent on relieving the\nvanishing gradient problem, including better ini-\ntialization (Le et al., 2015), additional loss sig-\nnal (Trinh et al., 2018), augmented memory struc-\nture (Ke et al., 2018) and others that modify the in-\nternal architecture of RNNs to ease the optimiza-\ntion (Wu et al., 2016; Li et al., 2018). Different\nfrom them, our work is based on the Transformer\narchitecture and shows that language modeling as\na real-world task beneﬁts from the ability to learn\nlonger-term dependency.\n3 Model\nGiven a corpus of tokensx =( x1,...,x T ), the\ntask of language modeling is to estimate the joint\nprobability P (x), which is often auto-regressively\nfactorized asP (x)= Q\nt P (xt | x<t). With the\nfactorization, the problem reduces to estimating\neach conditional factor. In this work, we stick to\nthe standard neural approach to modeling the con-\nditional probability. Speciﬁcally, a trainable neu-\nral network is used to encode the contextx<t into\na ﬁxed size hidden state, which is multiplied with\nthe word embeddings to obtain the logits. The log-\nits are then fed into the Softmax function, yielding\na categorical probability distribution over the next\ntoken.\n3.1 Vanilla Transformer Language Models\nIn order to apply Transformer or self-attention to\nlanguage modeling, the central problem is how to\ntrain a Transformer to effectively encode an arbi-\ntrarily long context into a ﬁxed size representation.\nGiven inﬁnite memory and computation, a sim-\nple solution would be to process the entire con-\ntext sequence using an unconditional Transformer\ndecoder, similar to a feed-forward neural network.\nHowever, this is usually infeasible with the limited\nresource in practice.\nOne feasible but crude approximation is to split\nthe entire corpus into shorter segments of man-\n2980\nSegment 1\nx1 x2 x4x3\nSegment 2\nx8x5 x6 x7\n(a) Train phase.\nLimited Context\nx1 x2 x4x3 x5 x6\nLimited Context\nx2 x3 x5x4 x6x1\nLimited Context\nx3 x4 x6x5x2\nx1\n(b) Evaluation phase.\nFigure 1: Illustration of the vanilla model with a segment length 4.\nageable sizes, and only train the model within\neach segment, ignoring all contextual information\nfrom previous segments. This is the idea adopted\nby Al-Rfou et al.(2018). We call it thevanilla\nmodel and visualize it in Fig. 1a. Under this\ntraining paradigm, information never ﬂows across\nsegments in either the forward or backward pass.\nThere are two critical limitations of using a ﬁxed-\nlength context. First, the largest possible depen-\ndency length is upper bounded by the segment\nlength, which is a few hundred on character-level\nlanguage modeling (Al-Rfou et al., 2018). There-\nfore, although the self-attention mechanism is less\naffected by the vanishing gradient problem com-\npared to RNNs, the vanilla model is not able to\nfully exploit this optimization advantage. Second,\nthough it is possible to use padding to respect the\nsentence or other semantic boundaries, in practice\nit has been standard practice to simply chunk long\ntext into ﬁxed-length segments due to improved\nefﬁciency (Peters et al., 2018; Devlin et al., 2018;\nAl-Rfou et al., 2018). However, simply chunking\na sequence into ﬁxed-length segments will lead to\nthe context fragmentation problem as discussed in\nSection 1.\nDuring evaluation, at each step, the vanilla\nmodel also consumes a segment of the same length\nas in training, but only makes one prediction at the\nlast position. Then, at the next step, the segment\nis shifted to the right by only one position, and the\nnew segment has to be processed all from scratch.\nAs shown in Fig.1b, this procedure ensures that\neach prediction utilizes the longest possible con-\ntext exposed during training, and also relieves con-\ntext fragmentation issue encountered in training.\nHowever, this evaluation procedure is extremely\nexpensive. We will show that our proposed archi-\ntecture is able to substantially improve the evalua-\ntion speed.\n3.2 Segment-Level Recurrence with State\nReuse\nTo address the limitations of using a ﬁxed-length\ncontext, we propose to introduce a recurrence\nmechanism to the Transformer architecture. Dur-\ning training, the hidden state sequence computed\nfor the previous segment isﬁxed and cached to\nbe reused as an extended context when the model\nprocesses the next new segment, as shown in Fig.\n2a. Although the gradient still remains within a\nsegment, this additional input allows the network\nto exploit information in the history, leading to an\nability of modeling longer-term dependency and\navoiding context fragmentation. Formally, let the\ntwo consecutive segments of lengthL be s⌧ =\n[x⌧,1, ··· ,x ⌧,L] and s⌧+1 =[ x⌧+1,1, ··· ,x ⌧+1,L]\nrespectively. Denoting then-th layer hidden state\nsequence produced for the ⌧ -th segment s⌧ by\nhn\n⌧ 2 RL⇥d, where d is the hidden dimension.\nThen, then-th layer hidden state for segments⌧+1\nis produced (schematically) as follows,\nehn\u00001\n⌧+1 =\n⇥\nSG(hn\u00001\n⌧ ) \u0000 hn\u00001\n⌧+1\n⇤\n,\nqn\n⌧+1, kn\n⌧+1, vn\n⌧+1 = hn\u00001\n⌧+1 W>\nq , ehn\u00001\n⌧+1 W>\nk , ehn\u00001\n⌧+1 W>\nv ,\nhn\n⌧+1 = Transformer-Layer(qn\n⌧+1, kn\n⌧+1, vn\n⌧+1) .\nwhere the function SG(· ) stands for stop-gradient,\nthe notation[hu \u0000 hv] indicates the concatenation\nof two hidden sequences along the length dimen-\nsion, and W· denotes model parameters. Com-\npared to the standard Transformer, the critical dif-\nference lies in that the keykn\n⌧+1 and value vn\n⌧+1\nare conditioned on the extended contextehn\u00001\n⌧+1 and\nhence hn\u00001\n⌧ cached from the previous segment.\nWe emphasize this particular design by the green\npaths in Fig.2a.\nWith this recurrence mechanism applied to ev-\nery two consecutive segments of a corpus, it es-\nsentially creates a segment-level recurrence in the\nhidden states. As a result, the effective context be-\ning utilized can go way beyond just two segments.\nHowever, notice that the recurrent dependency be-\ntween hn\n⌧+1 and hn\u00001\n⌧ shifts one layer downwards\n2981\nx1 x2 x4x3 x8x5 x6 x7\nNew Segment\nx12x9 x10 x11\nFixed (No Grad)\nx1 x2 x4x3 x8x5 x6 x7\nFixed (No Grad)New Segment\n(a) Training phase.\nx1 x2 x4x3 x8x5 x6 x7 x12x9 x10 x11\nExtended Context\n(b) Evaluation phase.\nFigure 2: Illustration of the Transformer-XL model with a segment length 4.\nper-segment, which differs from the same-layer\nrecurrence in conventional RNN-LMs. Conse-\nquently, the largest possible dependency length\ngrows linearly w.r.t. the number of layers as well\nas the segment length, i.e., O(N ⇥ L), as vi-\nsualized by the shaded area in Fig. 2b. This\nis analogous to truncated BPTT (Mikolov et al.,\n2010), a technique developed for training RNN-\nLMs. However, different from truncated BPTT,\nour method caches a sequence of hidden states in-\nstead of the last one, and should be applied to-\ngether with the relative positional encoding tech-\nnique described in Section3.3.\nBesides achieving extra long context and re-\nsolving fragmentation, another beneﬁt that comes\nwith the recurrence scheme is signiﬁcantly faster\nevaluation. Speciﬁcally, during evaluation, the\nrepresentations from the previous segments can\nbe reused instead of being computed from scratch\nas in the case of the vanilla model. In our ex-\nperiments on enwiki8, Transformer-XL is up to\n1,800+ times faster than the vanilla model during\nevaluation (see Section4).\nFinally, notice that the recurrence scheme does\nnot need to be restricted to only the previous seg-\nment. In theory, we can cache as many previous\nsegments as the GPU memory allows, and reuse\nall of them as the extra context when processing\nthe current segment. Thus, we can cache a prede-\nﬁned length-M old hidden states spanning (pos-\nsibly) multiple segments, and refer to them as the\nmemory mn\n⌧ 2 RM⇥d, due to a clear connection to\nthe memory augmented neural networks (Graves\net al., 2014; Weston et al., 2014). In our experi-\nments, we setM equal to the segment length dur-\ning training, and increase it by multiple times dur-\ning evaluation.\n3.3 Relative Positional Encodings\nWhile we found the idea presented in the pre-\nvious subsection very appealing, there is a cru-\ncial technical challenge we haven’t solved in or-\nder to reuse the hidden states. That is, how can\nwe keep the positional information coherent when\nwe reuse the states? Recall that, in the standard\nTransformer, the information of sequence order is\nprovided by a set of positional encodings, denoted\nas U 2 RLmax⇥d, where the i-th row Ui corre-\nsponds to thei-th absolute position within a seg-\nment and Lmax prescribes the maximum possible\nlength to be modeled. Then, the actual input to the\nTransformer is the element-wise addition of the\nword embeddings and the positional encodings. If\nwe simply adapt this positional encoding to our\nrecurrence mechanism, the hidden state sequence\nwould be computed schematically by\nh⌧+1 = f (h⌧ , Es⌧+1 + U1:L)\nh⌧ = f (h⌧\u00001, Es⌧ + U1:L),\nwhere Es⌧ 2 RL⇥d is the word embedding se-\nquence of s⌧ , and f represents a transformation\nfunction. Notice that, bothEs⌧ and Es⌧+1 are as-\nsociated with the same positional encodingU1:L.\nAs a result, the model has no information to dis-\ntinguish the positional difference betweenx⌧,j and\nx⌧+1,j for anyj =1 ,...,L , resulting in a sheer\nperformance loss.\nIn order to avoid this failure mode, the funda-\nmental idea is to only encode therelative posi-\ntional information in the hidden states. Concep-\ntually, the positional encoding gives the model a\ntemporal clue or “bias” about how information\nshould be gathered, i.e., where to attend. For the\nsame purpose, instead of incorporating bias stati-\ncally into the initial embedding, one can inject the\nsame information into the attention score of each\nlayer. More importantly, it is more intuitive and\ngeneralizable to deﬁne the temporal bias in a rela-\ntive manner. For instance, when a query vectorq⌧,i\nattends on the key vectorsk⌧,i, it does not need\nto know the absolute position of each key vector\nto identify the temporal order of the segment. In-\nstead, it sufﬁces to know the relative distance be-\ntween each key vectork⌧,j and itselfq⌧,i, i.e.i\u0000j.\nPractically, one can create a set of relative posi-\n2982\ntional encodingsR 2 RLmax⇥d, where thei-th row\nRi indicates a relative distance ofi between two\npositions. By injecting the relative distance dy-\nnamically into the attention score, the query vector\ncan easily distinguish the representations ofx⌧,j\nand x⌧+1,j from their different distances, making\nthe state reuse mechanism feasible. Meanwhile,\nwe won’t lose any temporal information, as the ab-\nsolute position can be recovered recursively from\nrelative distances.\nPreviously, the idea of relative positional encod-\nings has been explored in the context of machine\ntranslation (Shaw et al., 2018) and music gener-\nation (Huang et al., 2018). Here, we offer a dif-\nferent derivation, arriving at a new form of rel-\native positional encodings, which not only has a\none-to-one correspondence to its absolute coun-\nterpart but also enjoys much better generalization\nempirically (see Section4). Firstly, in the standard\nTransformer (Vaswani et al., 2017), the attention\nscore between queryqi and key vectorkj within\nthe same segment can be decomposed as\nAabs\ni,j = E>\nxi W>\nq WkExj\n| {z }\n(a)\n+ E>\nxi W>\nq WkUj\n| {z }\n(b)\n+ U>\ni W>\nq WkExj\n| {z }\n(c)\n+ U>\ni W>\nq WkUj\n| {z }\n(d)\n.\nFollowing the idea of only relying on rela-\ntive positional information, we propose to re-\nparameterize the four terms as follows\nArel\ni,j = E>\nxi W>\nq Wk,EExj\n| {z }\n(a)\n+ E>\nxi W>\nq Wk,RRi\u0000j\n| {z }\n(b)\n+ u>Wk,EExj\n| {z }\n(c)\n+ v>Wk,RRi\u0000j\n| {z }\n(d)\n.\n• The ﬁrst change we make is to replace all ap-\npearances of the absolute positional embedding\nUj for computing key vectors in term(b) and\n(d) with its relative counterpartRi\u0000j. This es-\nsentially reﬂects the prior that only the relative\ndistance matters for where to attend. Note that\nR is a sinusoid encoding matrix (Vaswani et al.,\n2017) without learnable parameters.\n• Secondly, we introduce a trainable parameter\nu 2 Rd to replace the queryU>\ni W>\nq in term\n(c). In this case, since the query vector is the\nsame for all query positions, it suggests that the\nattentive bias towards different words should re-\nmain the same regardless of the query position.\nWith a similar reasoning, a trainable parameter\nv 2 Rd is added to substituteU>\ni W>\nq in term\n(d).\n• Finally, we deliberately separate the two weight\nmatrices Wk,E and Wk,R for producing the\ncontent-based key vectors and location-based\nkey vectors respectively.\nUnder the new parameterization, each term has\nan intuitive meaning: term(a) represents content-\nbased addressing, term (b) captures a content-\ndependent positional bias, term (c) governs a\nglobal content bias, and(d) encodes a global po-\nsitional bias.\nIn comparison, the formulation inShaw et al.\n(2018) only has terms(a) and (b), dropping the\ntwo bias terms(c) and (d). Moreover,Shaw et al.\n(2018) merge the multiplicationWkR into a sin-\ngle trainable matrixˆR, which abandons the induc-\ntive bias built into the original sinusoid positional\nencoding (Vaswani et al., 2017). In contrast, our\nrelative positional embeddingR adapts the sinu-\nsoid formulation. As a beneﬁt of the inductive\nbias, a model trained on a memory of some certain\nlength can automatically generalize to a memory\nseveral times longer during evaluation.\nEquipping the recurrence mechanism with our\nproposed relative positional embedding, we ﬁnally\narrive at the Transformer-XL architecture. For\ncompleteness, we summarize the computational\nprocedure for a N-layer Transformer-XL with a\nsingle attention head here. Forn =1 ,...,N :\nehn\u00001\n⌧ =\n⇥\nSG(mn\u00001\n⌧ ) \u0000 hn\u00001\n⌧\n⇤\nqn\n⌧ , kn\n⌧ , vn\n⌧ = hn\u00001\n⌧ Wn\nq\n>, ehn\u00001\n⌧ Wn\nk,E\n>, ehn\u00001\n⌧ Wn\nv\n>\nAn\n⌧,i,j = qn\n⌧,i\n>kn\n⌧,j + qn\n⌧,i\n>Wn\nk,RRi\u0000j\n+ u>k⌧,j + v>Wn\nk,RRi\u0000j\nan\n⌧ = Masked-Softmax(An\n⌧ )vn\n⌧\non\n⌧ = LayerNorm(Linear(an\n⌧ )+ hn\u00001\n⌧ )\nhn\n⌧ = Positionwise-Feed-Forward(on\n⌧ )\nwith h0\n⌧ := Es⌧ deﬁned as the word embed-\nding sequence. In addition, it is worth mention-\ning that a naive way to computeA requires com-\nputing Wn\nk,RRi\u0000j for all pairs(i, j), whose cost\nis quadratic w.r.t. the sequence length. How-\never, noticing that the value ofi \u0000 j only ranges\nfrom zero to the sequence length, we show a sim-\nple computation procedure in AppendixB, which\nreduces the cost to be linear w.r.t. the sequence\nlength.\n4 Experiments\n4.1 Main Results\nWe apply Transformer-XL to a variety of datasets\non both word-level and character-level language\n2983\nModel #Param PPL\nGrave et al.(2016b) - LSTM - 48.7\nBai et al.(2018) - TCN - 45.2\nDauphin et al.(2016) - GCNN-8 - 44.9\nGrave et al.(2016b) - Neural cache - 40.8\nDauphin et al.(2016) - GCNN-14 - 37.2\nMerity et al.(2018) - QRNN 151M 33.0\nRae et al.(2018) - Hebbian + Cache - 29.9\nOurs - Transformer-XL Standard 151M 24.0\nBaevski and Auli(2018) - Adaptive Input⇧ 247M 20.5\nOurs - Transformer-XL Large 257M 18.3\nTable 1: Comparison with state-of-the-art results on\nWikiText-103. ⇧ indicates contemporary work.\nModel #Param bpc\nHa et al.(2016) - LN HyperNetworks 27M 1.34\nChung et al.(2016) - LN HM-LSTM 35M 1.32\nZilly et al.(2016) - RHN 46M 1.27\nMujika et al.(2017) - FS-LSTM-4 47M 1.25\nKrause et al.(2016) - Large mLSTM 46M 1.24\nKnol (2017) - cmix v13 - 1.23\nAl-Rfou et al.(2018) - 12L Transformer 44M 1.11\nOurs - 12L Transformer-XL 41M 1.06\nAl-Rfou et al.(2018) - 64L Transformer 235M 1.06\nOurs - 18L Transformer-XL 88M 1.03\nOurs - 24L Transformer-XL 277M 0.99\nTable 2: Comparison with state-of-the-art results on en-\nwik8.\nmodeling to have a comparison with state-of-the-\nart systems, including WikiText-103 (Merity et al.,\n2016), enwik8 (LLC, 2009), text8 (LLC, 2009),\nOne Billion Word (Chelba et al., 2013), and Penn\nTreebank (Mikolov and Zweig, 2012).\nWikiText-103 is the largest available word-level\nlanguage modeling benchmark with long-term de-\npendency. It contains 103M training tokens from\n28K articles, with an average length of 3.6K to-\nkens per article, which allows testing the abil-\nity of long-term dependency modeling. We set\nthe attention length to 384 during training and\n1600 during evaluation. We adopted adaptive soft-\nmax and input representations (Baevski and Auli,\n2018; Grave et al., 2016a). As shown in Table1,\nTransformer-XL reduces the previous state-of-the-\nart (SoTA) perplexity from 20.5 to 18.3, which\ndemonstrates the superiority of the Transformer-\nXL architecture.\nThe dataset enwik8 contains 100M bytes of un-\nprocessed Wikipedia text. We compare our ar-\nchitecture with the previous results in Table 2.\nUnder the model size constraint, the 12-layer\nTransformer-XL achieves a new SoTA result, out-\nModel #Param bpc\nCooijmans et al.(2016) - BN-LSTM - 1.36\nChung et al.(2016) - LN HM-LSTM 35M 1.29\nZilly et al.(2016) - RHN 45M 1.27\nKrause et al.(2016) - Large mLSTM 45M 1.27\nAl-Rfou et al.(2018) - 12L Transformer 44M 1.18\nAl-Rfou et al.(2018) - 64L Transformer 235M 1.13\nOurs - 24L Transformer-XL 277M 1.08\nTable 3: Comparison with state-of-the-art results on\ntext8.\nModel #Param PPL\nShazeer et al.(2014) - Sparse Non-Negative 33B 52.9\nChelba et al.(2013) - RNN-1024 + 9 Gram 20B 51.3\nKuchaiev and Ginsburg(2017) - G-LSTM-2 - 36.0\nDauphin et al.(2016) - GCNN-14 bottleneck - 31.9\nJozefowicz et al.(2016) - LSTM 1.8B 30.6\nJozefowicz et al.(2016) - LSTM + CNN 1.04B 30.0\nShazeer et al.(2017) - Low-Budget MoE ⇠5B 34.1\nShazeer et al.(2017) - High-Budget MoE ⇠5B 28.0\nShazeer et al.(2018) - Mesh Tensorﬂow 4.9B 24.0\nBaevski and Auli(2018) - Adaptive Input⇧ 0.46B 24.1\nBaevski and Auli(2018) - Adaptive Input⇧ 1.0B 23.7\nOurs - Transformer-XL Base 0.46B 23.5\nOurs - Transformer-XL Large 0.8B 21.8\nTable 4: Comparison with state-of-the-art results on One\nBillion Word.⇧ indicates contemporary work.\nperforming the 12-layer vanilla Transformer from\nAl-Rfou et al.(2018) by 0.05, while both Trans-\nformer variants have a large margin over conven-\ntional RNN-based models. Notably, our 12-layer\narchitecture achieves the same result as the 64-\nlayer network fromAl-Rfou et al.(2018), using\nonly 17% of the parameter budget. In order to see\nwhether better performances can be obtained by\nincreasing the model size, we train 18-layer and\n24-layer Transformer-XLs with increased model\nsizes. With the attention length 784 during train-\ning and 3,800 during evaluation, we obtained a\nnew SoTA result and our method is the ﬁrst to\nbreak through 1.0 on widely-studied character-\nlevel benchmarks. Different fromAl-Rfou et al.\n(2018), Transformer-XL does not need any auxil-\niary losses, and thus all beneﬁts are credited to a\nbetter architecture.\nSimilar to but different from enwik8, text8 con-\ntains 100M processed Wikipedia characters cre-\nated by lowering case the text and removing any\ncharacter other than the 26 lettersa through z, and\nspace. Due to the similarity, we simply adapt the\nbest model and the same hyper-parameters on en-\nwik8 to text8 without further tuning. The compari-\n2984\nModel #Param PPL\nInan et al.(2016) - Tied Variational LSTM 24M 73.2\nZilly et al.(2016) - Variational RHN 23M 65.4\nZoph and Le(2016) - NAS Cell 25M 64.0\nMerity et al.(2017) - AWD-LSTM 24M 58.8\nPham et al.(2018) - Efﬁcient NAS 24M 58.6\nLiu et al.(2018) - Differentiable NAS 23M 56.1\nYang et al.(2017) - AWD-LSTM-MoS 22M 55.97\nMelis et al.(2018) - Dropout tuning 24M 55.3\nOurs - Transformer-XL 24M 54.52\nMerity et al.(2017) - AWD-LSTM+Finetune† 24M 57.3\nYang et al.(2017) - MoS+Finetune† 22M 54.44\nTable 5: Comparison with state-of-the-art results on\nPenn Treebank.† indicates using two-step ﬁnetuning.\nson with previous methods is summarized in Table\n3. Again, Transformer-XL achieves the new SoTA\nresult with a clear margin.\nOne Billion Word does not preserve any long-\nterm dependency because sentences have been\nshufﬂed. Consequently, this dataset mainly tests\nthe ability of modeling only short-term depen-\ndency. The comparison between Transformer-XL\nand the other methods is shown in Table4. Al-\nthough Transformer-XL is mainly designed to bet-\nter capture longer-term dependency, it dramati-\ncally improves the single-model SoTA from 23.7\nto 21.8. Speciﬁcally, Transformer-XL signiﬁ-\ncantly outperforms a contemporary method using\nvanilla Transformers (Baevski and Auli, 2018),\nsuggesting the advantage of Transformer-XL is\ngeneralizable to modeling short sequences.\nWe also report the results on word-level Penn\nTreebank in Table 5. Similar to AWD-LSTM\n(Merity et al., 2017), we apply variational dropout\nand weight average to Transformer-XL. With\nproper regularization, Transformer-XL achieves a\nnew SoTA result among models without two-step\nﬁnetuning. Penn Treebank has only 1M training\ntokens, which implies that Transformer-XL also\ngeneralizes well even on small datasets.\n4.2 Ablation Study\nWe conduct two sets of ablation studies to exam-\nine the effects of two proposed techniques used in\nTransformer-XL: the recurrence mechanism and\nthe new positional encoding scheme.\nThe ﬁrst study is performed on WikiText-103,\nwhich requires modeling long-term dependency.\nThe results are reported in Table6. Among the\ncompared encoding schemes,Shaw et al.(2018) is\nrelative, whileVaswani et al.(2017) andAl-Rfou\net al.(2018) are absolute. “Full” and “half” losses\nrefer to applying a cross entropy loss to all or the\nrecent half positions in the segment. We found\nthat absolute encodings only work well with half\nlosses because half losses exclude positions with\nvery short attention lengths during training for bet-\nter generalization. Table 6 shows that both the\nrecurrence mechanism and our encoding scheme\nare necessary to achieve the best performance, as\nwell as generalizing to longer attention sequences\nduring evaluation time. Although the backprop-\nagation length during training is only 128, with\nthe two techniques the attention length can be in-\ncreased to 640 at test time. In the standard setting\nwith 151M parameters, the perplexity decreases as\nthe attention length increases.\nSince the recurrence mechanism costs addi-\ntional memory, we also compare Transformer-XL\nwith baselines under the same GPU memory con-\nstraints. As shown in Table 10 in Appendix A,\ndespite using a shorter backpropagation length,\nTransformer-XL remains superior to the baselines.\nThe second study targets at isolating the ef-\nfects of resolving the context fragmentation prob-\nlem from the beneﬁt of capturing longer context\nlength. In order to achieve this goal, we deliber-\nately choose a dataset that does not require long-\nterm dependency, so that any improvement from\nestablishing the recurrence can be attributed to\nsolving the context fragmentation. Speciﬁcally,\nwe perform this controlled experiment on the One\nBillion Word dataset, which can only beneﬁt from\nremoving the context fragmentation. We train\na 20-layer Transformer-XL with⇠0.3B parame-\nters for 400K steps. As shown in Table7, using\nsegment-level recurrence substantially improves\nperformance even when long-term dependency is\nnot needed, which is consistent with our previous\ndiscussion that the recurrence mechanism resolves\nthe context fragmentation problem. Moreover, our\nrelative positional encodings is also superior to\nShaw et al.(2018) on short sequences.\n4.3 Relative Effective Context Length\nKhandelwal et al. (2018) proposed a method to\nevaluate theEffective Context Length(ECL) of a\nsequence model. ECL is the longest length to\nwhich increasing the context span would lead to\na gain more than a threshold. However, ECL ig-\nnores the fact that it is harder to get improve-\nment when a model already achieves a lower per-\n2985\nRemark Recurrence Encoding Loss PPL init PPL best Attn Len\nTransformer-XL (128M) 3 Ours Full 27.02 26.77 500\n- 3 Shaw et al.(2018) Full 27.94 27.94 256\n- 3 Ours Half 28.69 28.33 460\n- 7 Ours Full 29.59 29.02 260\n- 7 Ours Half 30.10 30.10 120\n- 7 Shaw et al.(2018) Full 29.75 29.75 120\n- 7 Shaw et al.(2018) Half 30.50 30.50 120\n- 7 Vaswani et al.(2017) Half 30.97 30.97 120\nTransformer (128M)† 7 Al-Rfou et al.(2018) Half 31.16 31.16 120\nTransformer-XL (151M) 3 Ours Full 23.43\n23.09 640\n23.16 450\n23.35 300\nTable 6: Ablation study on WikiText-103. For the ﬁrst two blocks, we use a slightly smaller model (128M parame-\nters). † indicates that the corresponding row is reduced to the same setting as the Transformer network in (Al-Rfou\net al., 2018), except that two auxiliary losses are not implemented in our experiments. “PPL init” refers to using\nthe same length as training. “PPL best” indicates the perplexity obtained by using the optimal length. “Attn Len”\nis the shortest possible attention length during evaluation to achieve the corresponding result (PPL best). Increas-\ning the attention length during evaluation improves performance only when our positional encoding is used. The\n“Transformer-XL (151M)” setting uses a standard parameter budget as previous work (Merity et al., 2018), where\nwe observe a similar effect when increasing the attention length during evaluation.\nMethod PPL\nOurs 25.2\nWith Shaw et al.(2018) encodings 25.7\nWithout recurrence 27.1\nTable 7: Ablation study on One Billion Word, a dataset\nwithout long-term dependency.\nModel r =0 .1 r =0 .5 r =1 .0\nTransformer-XL 151M 900 800 700\nQRNN 500 400 300\nLSTM 400 300 200\nTransformer-XL 128M 700 600 500\n- useShaw et al.(2018) encoding 400 400 300\n- remove recurrence 300 300 300\nTransformer 128 128 128\nTable 8: Relative effective context length (RECL) com-\nparison. See text for the deﬁnition of RECL andr. The\nﬁrst three models and the last four models are com-\npared as twomodel groups when we calculate RECL\n(RECL is computed on a model group rather than a sin-\ngle model). Each group has the same parameter budget.\nplexity using only a shorter context, and thus it\nis not suitable for fair comparison among mul-\ntiple models. We instead propose a new metric\ncalled Relative Effective Context Length(RECL).\nRECL is deﬁned on a model group instead of a\nsingle model, and the gain of a long context is\nmeasure by the relative improvement over thebest\nshort context model. As such, the model group\nshares the same baseline to enable fair compari-\nson. RECL also has a parameterr, which means\nconstraining the comparison on top-r hard exam-\nples. See AppedixC for more details about RECL.\nAs shown in Table8, Transformer-XL manages\nto model dependency of 900 words long on av-\nerage with r =0 .1. The RECL of Transformer-\nXL is 80% and 450% longer than recurrent net-\nworks and Transformer respectively. Both the re-\ncurrence mechanism and our positional encodings\ncontribute to a longer RECL. This further substan-\ntiates our argument that Transformer-XL is able to\nmodel longer-term dependency.\n4.4 Generated Text\nTrained only on WikiText-103 which is medium-\nsized, Transformer-XL is already able to generate\nrelatively coherent articles with thousands of to-\nkens without manual cherry picking, despite mi-\nnor ﬂaws. Please refer to AppendixE for samples.\n4.5 Evaluation Speed\nFinally, we compare the evaluation speed of our\nmodel with the vanilla Transformer model (Al-\nRfou et al., 2018). As shown in Table9, due to\nthe state reuse scheme, Transformer-XL achieves\nan up to 1,874 times speedup during evaluation.\n5 Conclusions\nTransformer-XL obtains strong perplexity results,\nmodels longer-term dependency than RNNs and\nTransformer, achieves substantial speedup during\n2986\nAttn Len How much Al-Rfou et al.(2018) is slower\n3,800 1,874x\n2,800 1,409x\n1,800 773x\n800 363x\nTable 9: Slowdown in terms of running time during\nevaluation. Evaluation is based on per-token time on\none GPU.\nevaluation, and is able to generate coherent text\narticles. We envision interesting applications of\nTransformer-XL in the ﬁelds of text generation,\nunsupervised feature learning, image and speech\nmodeling.\nAcknowledgments\nZD and YY were supported in part by National\nScience Foundation (NSF) under the grant IIS-\n1546329 and by the DOE-Ofﬁce of Science un-\nder the grant ASCR #KJ040201. ZY and RS\nwere supported in part by the Ofﬁce of Naval\nResearch grant N000141812861, the NSF grant\nIIS1763562, the Nvidia fellowship, and the Siebel\nscholarship.\nReferences\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\nAlexei Baevski and Michael Auli. 2018. Adaptive in-\nput representations for neural language modeling.\narXiv preprint arXiv:1809.10853.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nShaojie Bai, J Zico Kolter, and Vladlen Koltun.\n2018. An empirical evaluation of generic convolu-\ntional and recurrent networks for sequence model-\ning. arXiv preprint arXiv:1803.01271.\nYoshua Bengio, Réjean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model.Journal of machine learning research,\n3(Feb):1137–1155.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling.arXiv\npreprint arXiv:1312.3005.\nJunyoung Chung, Sungjin Ahn, and Yoshua Bengio.\n2016. Hierarchical multiscale recurrent neural net-\nworks. arXiv preprint arXiv:1609.01704.\nTim Cooijmans, Nicolas Ballas, César Laurent, Ça˘glar\nGülçehre, and Aaron Courville. 2016. Re-\ncurrent batch normalization. arXiv preprint\narXiv:1603.09025.\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079–3087.\nYann N Dauphin, Angela Fan, Michael Auli, and\nDavid Grangier. 2016. Language modeling with\ngated convolutional networks. arXiv preprint\narXiv:1612.08083.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nAdji B Dieng, Chong Wang, Jianfeng Gao, and John\nPaisley. 2016. Topicrnn: A recurrent neural net-\nwork with long-range semantic dependency.arXiv\npreprint arXiv:1611.01702.\nYarin Gal and Zoubin Ghahramani. 2016. A theoret-\nically grounded application of dropout in recurrent\nneural networks. InAdvances in neural information\nprocessing systems, pages 1019–1027.\nEdouard Grave, Armand Joulin, Moustapha Cissé,\nDavid Grangier, and Hervé Jégou. 2016a. Efﬁcient\nsoftmax approximation for gpus. arXiv preprint\narXiv:1609.04309.\nEdouard Grave, Armand Joulin, and Nicolas\nUsunier. 2016b. Improving neural language\nmodels with a continuous cache. arXiv preprint\narXiv:1612.04426.\nAlex Graves. 2013. Generating sequences with\nrecurrent neural networks. arXiv preprint\narXiv:1308.0850.\nAlex Graves, Greg Wayne, and Ivo Danihelka.\n2014. Neural turing machines. arXiv preprint\narXiv:1410.5401.\nDavid Ha, Andrew Dai, and Quoc V Le. 2016. Hyper-\nnetworks. arXiv preprint arXiv:1609.09106.\nSepp Hochreiter, Yoshua Bengio, Paolo Frasconi, Jür-\ngen Schmidhuber, et al. 2001. Gradient ﬂow in re-\ncurrent nets: the difﬁculty of learning long-term de-\npendencies.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory. Neural computation,\n9(8):1735–1780.\nCheng-Zhi Anna Huang, Ashish Vaswani, Jakob\nUszkoreit, Noam Shazeer, Curtis Hawthorne, An-\ndrew M Dai, Matthew D Hoffman, and Douglas Eck.\n2987\n2018. An improved relative self-attention mecha-\nnism for transformer with application to music gen-\neration. arXiv preprint arXiv:1809.04281.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2016. Tying word vectors and word classiﬁers:\nA loss framework for language modeling. arXiv\npreprint arXiv:1611.01462.\nYangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer,\nand Jacob Eisenstein. 2015. Document context lan-\nguage models.arXiv preprint arXiv:1511.03962.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring\nthe limits of language modeling. arXiv preprint\narXiv:1602.02410.\nNan Rosemary Ke, Anirudh Goyal ALIAS PARTH\nGOYAL, Olexa Bilaniuk, Jonathan Binas,\nMichael C Mozer, Chris Pal, and Yoshua Ben-\ngio. 2018. Sparse attentive backtracking: Temporal\ncredit assignment through reminding. InAdvances\nin Neural Information Processing Systems, pages\n7650–7661.\nUrvashi Khandelwal, He He, Peng Qi, and Dan Ju-\nrafsky. 2018. Sharp nearby, fuzzy far away: How\nneural language models use context.arXiv preprint\narXiv:1805.04623.\nBryon Knol. 2017. cmix v13. http://www.\nbyronknoll.com/cmix.html.\nBen Krause, Liang Lu, Iain Murray, and Steve Renals.\n2016. Multiplicative lstm for sequence modelling.\narXiv preprint arXiv:1609.07959.\nOleksii Kuchaiev and Boris Ginsburg. 2017. Factor-\nization tricks for lstm networks. arXiv preprint\narXiv:1703.10722.\nQuoc V Le, Navdeep Jaitly, and Geoffrey E Hin-\nton. 2015. A simple way to initialize recurrent\nnetworks of rectiﬁed linear units. arXiv preprint\narXiv:1504.00941.\nShuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo\nGao. 2018. Independently recurrent neural network\n(indrnn): Building a longer and deeper rnn. InPro-\nceedings of the IEEE Conference on Computer Vi-\nsion and Pattern Recognition, pages 5457–5466.\nHanxiao Liu, Karen Simonyan, and Yiming Yang.\n2018. Darts: Differentiable architecture search.\narXiv preprint arXiv:1806.09055.\nMultiMedia LLC. 2009. Large text compression\nbenchmark.\nGábor Melis, Charles Blundell, Tomáš Ko ˇcisk`y,\nKarl Moritz Hermann, Chris Dyer, and Phil Blun-\nsom. 2018. Pushing the bounds of dropout.arXiv\npreprint arXiv:1805.09208.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2017. Regularizing and optimizing lstm lan-\nguage models.arXiv preprint arXiv:1708.02182.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018. An analysis of neural language\nmodeling at multiple scales. arXiv preprint\narXiv:1803.08240.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture\nmodels. arXiv preprint arXiv:1609.07843.\nTomáš Mikolov, Martin Karaﬁát, Lukáš Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In\nEleventh Annual Conference of the International\nSpeech Communication Association.\nTomas Mikolov and Geoffrey Zweig. 2012. Context\ndependent recurrent neural network language model.\nSLT, 12(234-239):8.\nAsier Mujika, Florian Meier, and Angelika Steger.\n2017. Fast-slow recurrent neural networks. InAd-\nvances in Neural Information Processing Systems,\npages 5915–5924.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. arXiv preprint arXiv:1802.05365.\nHieu Pham, Melody Y Guan, Barret Zoph, Quoc V\nLe, and Jeff Dean. 2018. Efﬁcient neural architec-\nture search via parameter sharing. arXiv preprint\narXiv:1802.03268.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.URL https://s3-\nus-west-2. amazonaws. com/openai-assets/research-\ncovers/languageunsupervised/language under-\nstanding paper. pdf.\nJack W Rae, Chris Dyer, Peter Dayan, and Tim-\nothy P Lillicrap. 2018. Fast parametric learn-\ning with activation memorization. arXiv preprint\narXiv:1803.10049.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. arXiv preprint arXiv:1803.02155.\nNoam Shazeer, Youlong Cheng, Niki Parmar, Dustin\nTran, Ashish Vaswani, Penporn Koanantakool, Peter\nHawkins, HyoukJoong Lee, Mingsheng Hong, Cliff\nYoung, et al. 2018. Mesh-tensorﬂow: Deep learning\nfor supercomputers. In Advances in Neural Infor-\nmation Processing Systems, pages 10434–10443.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\nAndy Davis, Quoc Le, Geoffrey Hinton, and Jeff\nDean. 2017. Outrageously large neural networks:\nThe sparsely-gated mixture-of-experts layer.arXiv\npreprint arXiv:1701.06538.\n2988\nNoam Shazeer, Joris Pelemans, and Ciprian Chelba.\n2014. Skip-gram language modeling using sparse\nnon-negative matrix probability estimation. arXiv\npreprint arXiv:1412.1454.\nTrieu H Trinh, Andrew M Dai, Thang Luong, and\nQuoc V Le. 2018. Learning longer-term dependen-\ncies in rnns with auxiliary losses. arXiv preprint\narXiv:1803.00144.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nTian Wang and Kyunghyun Cho. 2015. Larger-\ncontext language modelling. arXiv preprint\narXiv:1511.03729.\nWenlin Wang, Zhe Gan, Wenqi Wang, Dinghan Shen,\nJiaji Huang, Wei Ping, Sanjeev Satheesh, and\nLawrence Carin. 2017. Topic compositional neural\nlanguage model.arXiv preprint arXiv:1712.09783.\nJason Weston, Sumit Chopra, and Antoine Bor-\ndes. 2014. Memory networks. arXiv preprint\narXiv:1410.3916.\nYuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua\nBengio, and Ruslan R Salakhutdinov. 2016. On\nmultiplicative integration with recurrent neural net-\nworks. In Advances in neural information process-\ning systems, pages 2856–2864.\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and\nWilliam W Cohen. 2017. Breaking the softmax bot-\ntleneck: A high-rank rnn language model. arXiv\npreprint arXiv:1711.03953.\nJulian Georg Zilly, Rupesh Kumar Srivastava,\nJan Koutník, and Jürgen Schmidhuber. 2016.\nRecurrent highway networks. arXiv preprint\narXiv:1607.03474.\nBarret Zoph and Quoc V Le. 2016. Neural architecture\nsearch with reinforcement learning.arXiv preprint\narXiv:1611.01578.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.7861325740814209
    },
    {
      "name": "Computer science",
      "score": 0.7611839771270752
    },
    {
      "name": "Language model",
      "score": 0.7015364170074463
    },
    {
      "name": "Transformer",
      "score": 0.6623331308364868
    },
    {
      "name": "Treebank",
      "score": 0.5795519351959229
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5598369836807251
    },
    {
      "name": "Hyperparameter",
      "score": 0.5235882997512817
    },
    {
      "name": "Natural language processing",
      "score": 0.4210675060749054
    },
    {
      "name": "Dependency (UML)",
      "score": 0.2742655873298645
    },
    {
      "name": "Engineering",
      "score": 0.09777900576591492
    },
    {
      "name": "Electrical engineering",
      "score": 0.08331140875816345
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}