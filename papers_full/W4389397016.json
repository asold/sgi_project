{
  "title": "A combination network of CNN and transformer for interference identification",
  "url": "https://openalex.org/W4389397016",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2105322448",
      "name": "Hu Zhang",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A312167979",
      "name": "Meng Zhao",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2005452123",
      "name": "Min Zhang",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2115818164",
      "name": "Sheng Lin",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2159337297",
      "name": "Youqiang Dong",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2096411585",
      "name": "Hai Wang",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2105322448",
      "name": "Hu Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A312167979",
      "name": "Meng Zhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2005452123",
      "name": "Min Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115818164",
      "name": "Sheng Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2159337297",
      "name": "Youqiang Dong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096411585",
      "name": "Hai Wang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3209420330",
    "https://openalex.org/W4224237294",
    "https://openalex.org/W3104445526",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W4312364072",
    "https://openalex.org/W6749678253",
    "https://openalex.org/W4311104691",
    "https://openalex.org/W2960022641",
    "https://openalex.org/W3124539583",
    "https://openalex.org/W6772954352",
    "https://openalex.org/W4312730569",
    "https://openalex.org/W4387623755",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2907382309",
    "https://openalex.org/W2969008381",
    "https://openalex.org/W6694508510",
    "https://openalex.org/W3119277176",
    "https://openalex.org/W2741230443",
    "https://openalex.org/W4229368265",
    "https://openalex.org/W6734136605",
    "https://openalex.org/W2887063112",
    "https://openalex.org/W4225418897",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4206437097",
    "https://openalex.org/W4226059106",
    "https://openalex.org/W6787503619",
    "https://openalex.org/W6735578933",
    "https://openalex.org/W6752673245",
    "https://openalex.org/W2790896200",
    "https://openalex.org/W6762670123",
    "https://openalex.org/W2069855711",
    "https://openalex.org/W4283021596",
    "https://openalex.org/W4210364703",
    "https://openalex.org/W4309744061",
    "https://openalex.org/W2795250928",
    "https://openalex.org/W3104028856",
    "https://openalex.org/W4205716622",
    "https://openalex.org/W2808030156",
    "https://openalex.org/W2997746189",
    "https://openalex.org/W2603396821",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2591951844",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3115572960",
    "https://openalex.org/W2272847350",
    "https://openalex.org/W2970843912"
  ],
  "abstract": "Communication interference identification is critical in electronic countermeasures. However, existed methods based on deep learning, such as convolutional neural networks (CNNs) and transformer, seldom take both local characteristics and global feature information of the signal into account. Motivated by the local convolution property of CNNs and the attention mechanism of transformer, we designed a novel network that combines both architectures, which make better use of both local and global characteristics of the signals. Additionally, recognizing the challenge of distinguishing contextual semantics within the one-dimensional signal data used in this study, we advocate the use of CNNs in place of word embedding, aligning more closely with the intrinsic features of the signal data. Furthermore, to capture the time-frequency characteristics of the signals, we integrate the proposed network with a cross-attention mechanism, facilitating the fusion of temporal and spectral domain feature information through multiple cross-attention computational layers. This innovation obviates the need for specialized time-frequency analysis. Experimental results demonstrate that our approach significantly improves recognition accuracy compared to existing methods, highlighting its efficacy in addressing the challenge of communication interference identification in electronic warfare.",
  "full_text": "Frontiers in Computational Neuroscience 01 frontiersin.org\nA combination network of CNN \nand transformer for interference \nidentification\nHu Zhang , Meng Zhao , Min Zhang *, Sheng Lin , Youqiang Dong  \nand Hai Wang \nSchool of Aerospace Science and Technology, Xidian University, Xi'an, China\nCommunication interference identification is critical in electronic countermeasures. \nHowever, existed methods based on deep learning, such as convolutional neural \nnetworks (CNNs) and transformer, seldom take both local characteristics and \nglobal feature information of the signal into account. Motivated by the local \nconvolution property of CNNs and the attention mechanism of transformer, \nwe  designed a novel network that combines both architectures, which make \nbetter use of both local and global characteristics of the signals. Additionally, \nrecognizing the challenge of distinguishing contextual semantics within the one-\ndimensional signal data used in this study, we advocate the use of CNNs in place \nof word embedding, aligning more closely with the intrinsic features of the signal \ndata. Furthermore, to capture the time-frequency characteristics of the signals, \nwe integrate the proposed network with a cross-attention mechanism, facilitating \nthe fusion of temporal and spectral domain feature information through multiple \ncross-attention computational layers. This innovation obviates the need for \nspecialized time-frequency analysis. Experimental results demonstrate that our \napproach significantly improves recognition accuracy compared to existing \nmethods, highlighting its efficacy in addressing the challenge of communication \ninterference identification in electronic warfare.\nKEYWORDS\ncommunication interference identification, electronic countermeasures, convolutional \nneural network, transformer, cross-attention mechanism\n1 Introduction\nInterference identification has received increasing attention in military and civilian \napplications (Zhang et al., 2013). Interference identification aims at recognizing the category of \ninterference without any prior information, which is of great importance for anti-\ninterference communications.\nInterference identification methods are commonly classified into two categories: feature-\nbased and learning-based methods. Feature-based techniques utilize parameters such as \namplitude, phase, and wavelet transform as extracted features for classifiers (Ibrahim et al., 2019; \nNishio et al., 2019). In their work, Zhang and Cao (2018) introduced a waveform classification \napproach based on Support Vector Machines (SVM) tailored for automotive radar interference.\nSubsequently, the integration of machine learning and swarm intelligence techniques has \nshown significant promise in yielding exemplary outcomes across diverse fields (Malakar et al., \n2020; Bacanin et al., 2021, 2022; Ramanan et al., 2022; Tuba et al., 2022).\nRecently, the widespread adoption of deep learning has garnered significant attention in \nvarious fields, including the analysis of clustered weather patterns (Chattopadhyay et al., 2020), \nOPEN ACCESS\nEDITED BY\nMiodrag Zivkovic,  \nSingidunum University, Serbia\nREVIEWED BY\nNebojsa Bacanin,  \nSingidunum University, Serbia  \nMilos Antonijevic,  \nSingidunum University, Serbia\n*CORRESPONDENCE\nMin Zhang  \n minzhang@xidian.edu.cn\nRECEIVED 08 October 2023\nACCEPTED 14 November 2023\nPUBLISHED 06 December 2023\nCITATION\nZhang H, Zhao M, Zhang M, Lin S, Dong Y and \nWang H (2023) A combination network of CNN \nand transformer for interference identification.\nFront. Comput. Neurosci. 17:1309694.\ndoi: 10.3389/fncom.2023.1309694\nCOPYRIGHT\n© 2023 Zhang, Zhao, Zhang, Lin, Dong and \nWang. This is an open-access article distributed \nunder the terms of the Creative Commons \nAttribution License (CC BY). The use, \ndistribution or reproduction in other forums is \npermitted, provided the original author(s) and \nthe copyright owner(s) are credited and that \nthe original publication in this journal is cited, \nin accordance with accepted academic \npractice. No use, distribution or reproduction is \npermitted which does not comply with these \nterms.\nTYPE Methods\nPUBLISHED 06 December 2023\nDOI 10.3389/fncom.2023.1309694\nZhang et al. 10.3389/fncom.2023.1309694\nFrontiers in Computational Neuroscience 02 frontiersin.org\nas well as image detection (Qian et al., 2021; Lin et al., 2022; Zhou \net al., 2022; Lin et al., 2023) and processing (Gawande et al., 2022; \nZivkovic et al., 2022; Zhang et al., 2023). Benefitting from the powerful \nfeature extraction capability of deep learning, learning-based methods \nalso have achieved good performance in identification of \ncommunication signals (Kattenborn et al., 2021 ; Sun et al., 2021 ). \nO’Shea et al. (2016) used convolutional neural networks (CNNs) to \nclassify wireless modulated signals, and the effectiveness of the \nmethod was experimentally demonstrated. After that, Schmidt et al. \n(2021) used CNNs to study the automatic recognition of interference \nsignals. Due to the simple structure of the network, the recognition \naccuracy could also be improved.\nIn Li et al. (2019), carried out radio signal recognition method \nbased on gated recurrent unit (GRU). Compared to CNNs, GRU has \nmore advantages in feature extraction of one-dimensional signals. \nHowever, it is difficult to make GRU into a multi-layer structure and \nthat limits its feature extraction capability for long sequences. Residual \nnetwork (ResNet) was employed for modulation mode identification \nin West and O’Shea (2017). The method alleviates the problem of \ngradient decay in deeper networks. However, excessive use of the \nresidual structure can also lead to a larger amount of model parameters \nand waste of computational resources. In Zhang et  al. (2018) , a \ncombination of CNNs and long short-term memory (LSTM) was \nproposed and experimental results showed that it has better \nrecognition performance than either CNNs or LSTM. It was shown \nthat the effective combination of composite networks can improve \nrecognition results. Zhang et  al. (2019) constructed four classical \nneural network models to identify three types of wireless interference \nsignals, which demonstrate the generality of the effectiveness of deep \nlearning at the considered task. Wang et  al. (2020)  achieved \nsatisfactory results in modulation mode classification by using two \nCNNs for weight sharing and designing a new loss function. \nInfluenced by the development of transformer (Vaswani et al., 2017; \nDosovitskiy et al., 2021; Liu et al., 2021), the utilizations of transformer \nin signal recognition field (Huang et al., 2022; Wang et al., 2022a) have \nachieved better performance than CNNs. In Wang et  al. (2022b), \nshort-time Fourier transform (STFT) was used for time-frequency \nanalysis, and this method exploits the multi-domain information of \nthe signal. However, signals in different domains need to be processed \nwith different branched networks, while the dedicated time-frequency \nanalysis step adds to the process of interference identification.\nInspired by the above study, we  explore the application of \ntransformer in interference identification. Moreover, considering that \nthe disadvantage of transformer in local feature capture capability, this \npaper designs a novel network architecture, which combines CNNs \nand transformer (CNNTF). This fusion is not only unique, but also \nenables more comprehensive signal analysis. In summary, this paper \nmakes the following contributions:\n • Firstly, we  introduce a CNNTF network. In contrast to the \nconventional practice of employing simple network \ncombinations, this paper introduces a novel approach by utilizing \nCNNs in lieu of word embedding. This decision stems from the \nrecognition of the inherent complexity associated with contextual \nsemantics in signal data, which poses challenges for \ncomprehension using word embedding techniques. This \nmodification significantly enhances the network’s applicability in \nextracting features from signal data, which equip it with both \nlocal and global extraction capabilities.\n • In addition, we  integrated CNNTF with a cross-attention \nmechanism (CNNTF-CA) to exploit the correlations between \ndifferent features. This integration allows the network to extract \nmultiple domain features simultaneously, without requiring any \nspecial time-frequency analysis. As a result, the network can \nassociate time-domain and frequency-domain features effectively. \nOur approach represents an innovative way to enhance the \ncapabilities of neural networks for feature extraction.\n • The experimental results validate the effectiveness of the \nproposed method.\n2 Signal model\nIn this section, five types of single interference signals, which \nconsists of single-tone (ST), multi-tone (MT), linear sweep (LS), \npartial band noise (PBN) and noise frequency modulation (NFM), are \nused. The signal model can be denoted as\n R tS te Jt eW tjf tt jf ttss JJ\n() = () + () + ()\n+ ()() + ()()22πϕ πϕ\n (1)\nwhere R t()  represents the received signal. St()  is communication \nsignal, fs  and ϕ s t()  are separately carrier frequency and initial phase \nof St() . J t()  is jamming signal, f tJ ()  and ϕ J t()  are carrier frequency \nand initial phase of J t() , respectively. Wt()  is additive white Gaussian \nnoise (AWGN).\nAdditionally, the interference signals can be expressed in both \ntime-domain and frequency-domain. Frequency domain data can \nbe obtained from time domain data by fast Fourier transform (FFT), \nwhich can be written as\n \nx kx ne\nn\nN\njk nN() = ()\n=\n−\n−\n∑\n0\n1\n2π /\n \n(2)\nwhere e jN− 2π /  denotes the rotation factor. n and k  denote the \ndiscrete points in the time and frequency domains, respectively.j  is \nthe imaginary part.\nAfter that, take the amplitude and phase of the FFT data to obtain \nthe amplitude spectrum and phase spectrum data.\n3 Methods\nIn this paper, we propose a CNNTF method which combines \nCNNs and transformer. Based on CNNTF , we  introduce a cross-\nattention mechanism to design the CNNTF-CA model, which can \neffectively fuse features from different domains to achieve the purpose \nof time-frequency analysis.\n3.1 CNNTF\nThe CNNTF is designed to combine CNN and the encoding of \ntransformer, discarding the word embedding layer of transformer. The \nutilization of this module has two main advantages. Firstly, for \ncommunication interference signals, the local correlation between \nZhang et al. 10.3389/fncom.2023.1309694\nFrontiers in Computational Neuroscience 03 frontiersin.org\nadjacent sampling points affects the training effect of the model and \nshould not be ignored. CNNs has the advantage of local connectivity \nin learning features specifically for features between adjacent samples \nof the signal sequence. Secondly, considering the complexity in \nextracting contextual semantics from 1D signal data, CNNs are \ndeemed more appropriate than word coding for effectively addressing \nthe practical challenges in this task.\nThe structure of the CNNs module is as follows. The dimensional \nconvolution kernel scans the interfering data sequence first. In order \nto avoid gradient dissipation, batch normalization (BN) and rectified \nlinear unit (ReLU) activation function processing are performed after \nthe convolutional operation.\nThe mathematical expressions below can model the operations of \nthe local 1-D convolution module:\n \nOF I\nOF WO\ncc\nBN\n= ()\n= ()()\n,\nReLUReLU c\nθ\n \n(3)\nwhere Fc ⋅()  means the convolution function, I  is the input signal \nand θ is the parameter in CNNs. FBN denotes the BN processing, and \nW stands for the weight of convolutional layer. In addition, Oc and \nOReLU are the output of CNNs layer and the ReLU activation layer, \nrespectively.\nThe transformer module consists of an attention layer (AL) and a \nfeedforward network (FFN). The attention function can \nbe described as\n \nAttentionQ KV QK\nd\nV\nT\n,, softma x() =\n\n\n\n\n\n  \n(4)\nwhere QK,  and V are the query, key and value matrices separately.\nThe FFN is used after AL, which is composed of two linear \ntranslation layers. After the first linear layer, a ReLU activation \nfunction is employed, and the whole process can be described as\n FFN xW Wx bb() =+ ()() +ReLU 21 12  (5)\nwhere WR CC1 ∈ ×  and WR CC2 ∈ ×  can be used to describe the \nweights of different layers, separately; b1 and b2 denote the offset \nquantity of different layers, respectively.\nThere is an interlayer between the attention and FFN layers, which \nconsists of residual connection (RC) and layer normalization (LN). \nThe reason for using the residual connection is to prevent gradient \ndissipation with the network depth increasing, which can \nbe formulated as follows:\n x Hx Fx Wll ll+ = () + ()1 ,  (6)\nwhere xl  and xl+1 are the input and output vectors of the lth \nlayer, respectively. H xl()  means the direct mapping; F xWll,()  \nrepresents the residual mapping. All the layers use residual \nconnections to each other. LN follows RC in the interlayer, which \nprovides better performance for the processing of batches with \nsmall size.\nTo ensure that the dimension of the output is consistent with that \nof the previous layer, a one-dimensional deconvolution layer is needed \nto reduce the dimension before the output. Then, after the linear layer \nand normalization, realized by SoftMax function, the output result \nis obtained.\n3.2 Cross-attention mechanism\nThe time domain and frequency domain are the basic properties \nof the communication signals. In the field of signal processing, there \nare usually special time-frequency analysis steps to combine the time-\nfrequency domain data, which will also make the interference \nidentification process more complex. Therefore, this paper introduces \nthe cross-attention mechanism to combine the characteristics of time \ndomain and frequency domain to play the role of time-frequency \nanalysis. In this paper, in order to reduce the time-frequency analysis \nprocess, a cross-attention mechanism is used to correlate the data \nfrom two different domains. The overall structure of CNNTF-CA is \nshown in Figure 1.\nThe detailed cross-attention calculation of layer1 process is shown \nin Figure 2.\nThe cross-attention operation of layer1 can be formulated by\n \nO QK\nd\nV\nO QK\nd\nV\nta T\nk\na\ntp T\nk\np\n1\n2\n=\n\n\n\n\n\n ⋅\n=\n\n\n\n\n\n ⋅\nsoftma x\nsoftma x\n \n(7)\nwhere Qq tq tq tt n=…{}\n12,, ,  is the query vector composed of \ntime-domain feature sequences. K ka ka kaa n=…{}\n12,, ,  and \nK kp kp kpp n=…{}\n12,, ,  represent the key vectors composed of feature \nsequence in the frequency domain after linear mapping. Besides, \nVv av av aa n=…{}\n12,, ,  and Vv pv pv pp n=…{}\n12,, ,  are value vectors. ⋅ \nmeans the dot product of the matrix. O1 and O2 represent the output \nof the first layer of two cross-attention modules.\nThe cross-attention operation of next layer can be  described \nas follows:\n \nO\nQK\nd\nVr\nO O\nT\nk\nO=\n\n\n\n\n\n\n\n ⋅softma x\n1 2\n2\n \n(8)\nwhere the query vector QO1  is constructed by linear transformation \nof O1. O2 is linearly transformed to obtain the key vector KO2  and the \nvalue vector VO2 . After that, QO1 , KO2  and VO2  are fed into the next \nlayer of the cross-attention module for deep feature fusion.\nThe result obtained after the cross-attention mechanism is the \ninput of the FFN, which can be described as\n O FFN OFFN r= ()  (9)\nwhere Or is the result of a two-level cross-attention module.\nThe output of the previous layer is subjected to an inverse \nconvolution operation, which can be formulated as\nZhang et al. 10.3389/fncom.2023.1309694\nFrontiers in Computational Neuroscience 04 frontiersin.org\n OC onvTransposeD Ofinal= ()softma x FFN(1  (10)\nwhere OFFN is the output of FFN, and O final is the identification \nresult of CNNTF- CA. ConvTransposeD1 ⋅()  represents the \ndeconvolution operation, which performs up sampling on data to \nensure that the output dimensions match the input dimensions.\n4 Experiments and results analysis\n4.1 Datasets\nWe select two signals, Binary phase Shift Keying (BPSK) and \nQuadrature Phase Shift Keying (QPSK), as the communication signal \nSt() . The carrier frequency is set to 2 MHz for signal St() . In addition, \nthe signal-noise-ratio (SNR) is set to [−20 dB, 18 dB] with an interval \nof 2 dB for the experiments in this paper.\nFor the interference data set, this paper firstly simulates five single \ninterference signals, generates 1,000 samples under each SNR, each \nsample is sampled 1,024 times in the time domain. The parameters \nsuch as the center frequency, period and bandwidth of each type of \ninterference signal are randomly distributed to simulate the real \nenvironment. Then the time domain data is changed by FFT to obtain \nthe amplitude spectrum and phase spectrum data.\nUnder each SNR, the time domain, amplitude spectrum and \nphase spectrum are used as the three characteristics of the signal to \nsplice and construct the data sets. The main simulation parameters for \neach type of interference signal are shown in Table 1. The interference \nsignals are generated in MATLAB and model training and testing \nusing python.\n4.2 Performance of the proposed CNNTF\nTo evaluate the performance of our proposed method, the \nCNNTF are compared with the state-of-the-art methods including \nCNN (O’Shea et al., 2016), LSTM (Rajendrans et al., 2018), ResNet \n(West and O’Shea, 2017), CLDNN ( Zhang et al., 2018 ), and GRU \n(Hong et al., 2017) in this paper.\nTable 2 shows the overall recognition accuracy of each model on \ndifferent sources. The overall accuracy represents the average \nrecognition accuracy of each model for various types of interference \nunder each SNR.\nIt can be  observed that the method we  proposed is higher in \nrecognition accuracy than current mainstream methods. The average \nrecognition accuracy of the six models for various types of recognition \naccuracy with SNR for six models is shown in Figure 3.\n4.3 Performance of the proposed \nCNNTF-CA\nOur proposed CNNTF demonstrates certain advantages over \nsimilar methods, owing to its capacity in extracting both global \nand local features, which brings in a high degree of information \nconcentration. CNN, LSTM and GRU could not extract both \nglobal and local features. Compared with ResNet and CLDNN, \nwhich consider both global and local feature information, the \nadvantages of the proposed CNNTF is slightly better. To further \nimprove the performance, we  introduced a cross-\nattention mechanism.\nFigure  4  shows comparison chart of overall recognition \naccuracy between CNNTF and CNNTF- CA. From the figure, it can \nbe observed that the recognition performance of CNNTF-CA has \nsignificantly improved under low SNR. The results are due to the \nuse of the cross-attention mechanism, the time-frequency features \nare deeply correlated and the features are more differentiated \nbetween each type of modulated signal. CNNTF only performs \nsimple feature splicing, so its performance is slightly worse than \nCNNTF-CA.\nTable 3 presents the recognition performances of CNNTF-CA for \neach type of interference.\nIt can be seen from Table 3 that ST has the highest probability of \nbeing accurately identified among the five types of interference signals. \nFIGURE 1\nThe structure diagram of CNNTF-CA. The CNNTF-CA contains structure of CNNTF, and PE is the positional encoding. A, T, and P represent amplitude \nspectrum, time domain and phase spectrum data.\nZhang et al. 10.3389/fncom.2023.1309694\nFrontiers in Computational Neuroscience 05 frontiersin.org\nIn addition, the recognition effect of interference on QPSK is better \nthan that on BPSK, which also shows that QPSK contains more \ninformation than BPSK. Simultaneously, PBN and NFM are the two \ntypes of interference that are most difficult to identify, whether under \nBPSK or QPSK. We display the recognition accuracy of CNNTF-CA \nfor each interference in Figure 5.\nThe recognition accuracy of the CNNTF-CA approach for various \ninterferences under BPSK and QPSK is depicted in Figure 5, as shown \nin this scientific figure.\nIt can be seen from the figure that the recognition accuracy curve \nof CNNTF-CA for different interferences has a similar trend, which \nalso reflects the versatility and mobility of CNNTF-CA. We find that \nTABLE 1 Interference signal simulation parameters.\nInterference type Parameter\nST Center frequency point random\nMT The number of tones is 3 ~ 8\nLS The initial frequency is randomly distributed between 20 MHz and 200 MHz, and the sweep slope is 20 ~ 100THz/s\nPBN The occupied bandwidth is random between 10 and 100 MHz\nNFM The mean value of modulated noise is 0, the variances is 1, and the frequency modulation coefficient K fm=0.4 ~ 2\nFIGURE 2\nCross-attention calculation detail diagram.  represents the dot product.\nTABLE 2 Overall accuracy (%) of different models.\nModulation mode Model Accuracy\nBPSK\nCNN 73.54%\nLSTM 71.37%\nResNet 75.41%\nCLDNN 74.88%\nGRU 74.42%\nCNNTF 77.31%\nQPSK\nCNN 76.54%\nLSTM 75.97%\nResNet 77.66%\nCLDNN 80.22%\nGRU 78.54%\nCNNTF 81.39%\nThe bold values are our own proposed methods.\nZhang et al. 10.3389/fncom.2023.1309694\nFrontiers in Computational Neuroscience 06 frontiersin.org\nFIGURE 4\nOverall recognition accuracy CNNTF and CNNTF-CA.\nthe recognition accuracy of different interference signals varies greatly, \nespecially when the SNR is low.\nIn order to present the results more intuitively, we  use \nhistograms in Figure 6  to depict the two signals with the best and \nworst effects in BPSK and QPSK at -20 dB, respectively. This \napproach aims to provide a more intuitive description of \nthe results.\nIt is apparent that the model favors the identification of ST signals; \nhowever, its performance in recognizing NFM interference signals \nremains inadequate.\nTABLE 3 Average accuracy (%) of CNNTF-CA.\nInterference type CNNTF-CA\nBPSK QPSK\nST 86.3% 90.1%\nMT 81.0% 85.5%\nLS 83.1% 85.3%\nPBN 76.0% 81.8%\nNFM 75.2% 82.5%\nFIGURE 3\nAccuracy diagram of interference recognition.\nZhang et al. 10.3389/fncom.2023.1309694\nFrontiers in Computational Neuroscience 07 frontiersin.org\n5 Conclusion\nThe performance of the proposed CNNTF-CA model is evaluated \nthrough the confusion matrices presented in Figures 7A,B for BPSK \nand QPSK, respectively, at a signal-to-noise ratio of -10 dB.\nAccording to the confusion matrix illustrated in Figure 7, which \nrepresents the accuracy of identifying various interference signals \nunder an SNR of -10 dB, it is apparent that the NFM and PBN signals \nexhibit relatively higher rates of misidentification when compared to \nthe other signals present in the single interference data set. Specifically, \nthe network demonstrates significant recognition errors in identifying \nNFM and PBN signals, highlighting a limitation that requires further \nattention in future research endeavors.\nIn addition, more precise assessment metrics can be derived based \non Table 4. It can be seen that ST and LS are more likely to be correctly \nidentified whether under BPSK or QPSK.\nFurthermore, it is evident that regardless of the type of interference \nsignal, the accurate recognition rate for QPSK is higher than that for \nBPSK, indicating the richer signal information contained within \nQPSK. These findings help the proposed model identify different \ninterference signals faster and more accurately, playing a more \nimportant role in actual confrontation scenarios.\nIn this paper, we propose a novel method that combines these \nCNN and transformer (CNNTF), to address the problem of \nidentifying five single interferences. Given the challenge of \nextracting contextual semantics from one-dimensional signals \nusing word encoding, this paper introduces a pioneering approach \nthat exploits CNN instead. This novel combination, tailored to the \nunique data characteristics of one-dimensional signals, represents \na significant contribution to the field. To further enhance the \nperformance of the CNNTF model, we also incorporate a cross-\nattention mechanism that facilitates the correlation of the time \nand frequency domains of the input signals. This mechanism \nreplaces the traditional approach of separate time-frequency \nanalysis, leading to improved accuracy and efficiency in the \nidentification and classification of different interference types. \nThe effectiveness of the proposed approach is evaluated through \nextensive experiments and comparisons with other state-of-\nthe-art methods. The experimental results demonstrate that the \nproposed CNNTF model with cross-attention mechanism \nachieves better performance in identifying and classifying \ndifferent types of interferences.\nDespite the promising results, it is important to acknowledge \ncertain limitations and directions for future research. Current research \nis mainly limited to the evaluation of the CNNTF-CA model in simple \nscenarios. Further research on its performance under complex \ninterference scenarios would be beneficial. To bridge the gap between \ntheory and practical implementation, future research efforts will focus \non optimizing the model’s robustness to changes in real-world signal \nconditions and extending its applicability to different signal \ninterference environments.\nFIGURE 5\nIdentification accuracy of CNNTF-CA for each interference under BPSK and QPSK. Among them, ST is single-tone interference, MT is multi-tone \ninterference, LS is linear scan interference, PBN and NFM represent partial band noise interference and noise frequency modulation interference \nrespectively.\nFIGURE 6\nIdentification accuracy of CNNTF-CA for each interference under \nBPSK and QPSK.\nZhang et al. 10.3389/fncom.2023.1309694\nFrontiers in Computational Neuroscience 08 frontiersin.org\nData availability statement\nThe original contributions presented in the study are included in \nthe article/supplementary material, further inquiries can be directed \nto the corresponding author.\nAuthor contributions\nHZ: Conceptualization, Methodology, Writing – original draft. \nMeZ: Conceptualization, Methodology, Writing – original draft. MiZ: \nFunding acquisition, Writing – review & editing. SL: Writing – \noriginal draft, Writing – review & editing. YD: Writing – review & \nediting. HW: Supervision, Writing – review & editing.\nFunding\nThe author(s) declare financial support was received for the \nresearch, authorship, and/or publication of this article. This research \nwas funded by the National Natural Science Foundation of China \nUnder Grant 12003018.\nAcknowledgments\nThe authors would like to thank the reviewers for their valuable \nand detailed comments that are crucial in improving the quality of \nthis manuscript.\nConflict of interest\nThe authors declare that the research was conducted in the \nabsence of any commercial or financial relationships that could \nbe construed as a potential conflict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the authors \nand do not necessarily represent those of their affiliated \norganizations, or those of the publisher, the editors and the \nreviewers. Any product that may be evaluated in this article, or \nclaim that may be made by its manufacturer, is not guaranteed or \nendorsed by the publisher.\nTABLE 4 Performance evaluation of CNNTF-CA under –10  dB.\nTypes BPSK QPSK\nPrecision Recall F1-score Precision Recall F1-score\nST 0.741 0.865 0.798 0.896 0.935 0.915\nMT 0.787 0.704 0.743 0.839 0.875 0.857\nLS 0.763 0.790 0.776 0.961 0.878 0.918\nPBN 0.475 0.495 0.485 0.794 0.732 0.762\nNFM 0.513 0.482 0.497 0.720 0.719 0.719\nBPSK QPSK\nAB\nFIGURE 7\nConfusion matrix of CNNTF-CA at -10  dB.\nZhang et al. 10.3389/fncom.2023.1309694\nFrontiers in Computational Neuroscience 09 frontiersin.org\nReferences\nBacanin, N., Stoean, R., Zivkovic, M., Petrovic, A., Rashid, T., and Bezdan, T. (2021). \nPerformance of a novel chaotic firefly algorithm with enhanced exploration for tackling \nglobal optimization problems. Applic. Dropout Regular.  9, 2–33. doi: 10.3390/\nmath9212705\nBacanin, N., Zivkovic, M., Ai-Turjman, F ., Pavel, V ., Strumberger, I., and Bezdan, T. \n(2022). Hybridized sine cosine algorithm with convolutional neural networks dropout \nregularization application. Sci. Rep. 12:6302. doi: 10.1038/s41598-022-09744-2\nChattopadhyay, A., Hassanzadeh, P ., and Pasha, S. (2020). Predicting clustered weather \npatterns: a test case for applications of convolutional neural networks to spatio-temporal \nclimate data. Sci. Rep. 10:1317. doi: 10.1038/s41598-020-57897-9\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., \net al. An image is worth 16x16 words: transformers for image recognition at scale. In \nProceedings of the international conference on learning representations (ICLR), Vienna, \nAustria (2021)\nGawande, U., Hajari, K., and Golhar, Y . (2022). Robust pedestrian detection using \nscale and illumination invariant mask r-cnn. Int. J. Comput. Sci. Eng. 25, 607–618. doi: \n10.1504/IJCSE.2022.127190\nHong, D., Zhang, Z., and Xu, X. (2017). Automatic modulation classification using \nrecurrent neural networks. In Proceedings of the IEEE 3 international conference on \ncomputer and communications (ICCC), Chengdu, China, 695–700.\nHuang, J., Li, X., Wu, B., Wu, X., and Li, P . (2022). Few-shot radar emitter signal \nrecognition based on attention-balanced prototypical network. Remote Sens. 14:6101. \ndoi: 10.3390/rs14236101\nIbrahim, M., Parrish, D. J., Brown, T. W . C., and McDonald, P . J. (2019). Decision tree \npattern recognition model for radio frequency interference suppression in NQR \nexperiments. Sensors 19, 3153–3156. doi: 10.3390/s19143153\nKattenborn, T., Leitloff, J., Schiefer, F ., and Hinz, S. (2021). Review on convolutional \nneural networks (CNN) in vegetation remote sensing. ISPRS J. Photogramm. Remote \nSens. 173, 24–49. doi: 10.1016/j.isprsjprs.2020.12.010\nLi, R., Hu, J., and Y ang, S. (2019). Deep gated recurrent unit convolution network for \nradio signal recognition. In Proceeding of the 2019 IEEE 19th International Conference \non Communication Technology (ICCT), Xi’an, China, 159–163.\nLin, S., Zhang, M., and Cheng, X. (2022). Dual collaborative constraints regularized \nlow rank and sparse representation via robust dirtionaries construction for hyperspectral \nanomaly detection. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 16, 2009–2024. doi: \n10.1109/JSTARS.2022.3214508\nLin, S., Zhang, M., and Cheng, X. (2023). Dynamic low-rank and sparse priors \nconstrained deep autoencoders for hyperspectral anomaly detection. IEEE Trans. \nInstrum. Meas. doi: 10.1109/TIM.2023.3323997\nLiu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., et al. (2021). Swin transformer: \nhierarchical vision transformer using shifted windows. In Proceedings of the IEEE \ninternational conference on computer vision (ICCV), Montreal, Canada\nMalakar, S., Ghosh, M., Bhowmik, S., Sarkar, R., and Nasipuri, M. (2020). A GA based \nhierarchical feature selection approach for handwritten word recognition. Neural \nComput. Applic. 32, 2533–2552. doi: 10.1007/s00521-018-3937-8\nNishio, T., Okamoto, H., Nakashima, K., Koda, Y ., Y amamoto, K., Morikura, M., et al. \n(2019). Proactive received power prediction using machine learning and depth images \nfor mmWave networks. IEEE J Select Areas Commun  37, 2413–2427. doi: 10.1109/\nJSAC.2019.2933763\nO’Shea, T., Timothy, J., Corgan, J., and Clancy, T.C. (2016). Convolutional radio \nmodulation recognition networks. In Proceedings of the engineering applications of \nneural networks (EANN), Aberdeen, UK, 213–226.\nQian, X., Cheng, X., and Cheng, G. (2021). Two-stream encoder GAN with \nprogressive training for co-saliency detection. IEEE Signal Process. Lett. 28, 180–184. \ndoi: 10.1109/LSP .2021.3049997\nRajendrans, S., Meert, W ., Giustiniano, D., Lenders, V ., and Pollin, S. (2018). Deep \nlearning models for wireless signal classification with distributed low-cost Spectrum \nsensors. IEEE Trans. on Cog. Commun. and Net.  4, 433–445. doi: 10.1109/\nTCCN.2018.2835460\nRamanan, M., Singh, L., Suresh Kumar, A., Suresh, A., Sampathkumar, A., Jain, V ., \net al. (2022). Secure blockchain enabled cyber-physical health systems using ensemble \nconvolution neural network classification. Computes Electr Engin.  101:108058. doi: \n10.1016/j.compeleceng.2022.108058\nSchmidt, M., Block, D., and Meier, U. (2021). Wireless interference identification with \nconvolutional neural networks. In Proceedings of the IEEE 15th international conference \non industrial informatics (INDIN), Palam de Mallorca, Spain, 180–185\nSun, Y ., Xue, B., Zhang, M., Y en, G. G., and Lv, J. (2021). Automatically designing CNN \narchitectures using the genetic algorithm for image classification. IEEE Trans. Cybern. \n50, 3840–3854. doi: 10.1109/TCYB.2020.2983860\nTuba, E., Strumberger, I., Tuba, I., Bacanin, N., and Tuba, M. (2022). Acute \nlymphoblastic leukemia detection by tuned convolutional neural network. In 2022 32nd \ninternational conference RADIOELEKTRONIKA (RADIOELEKTRONIKA).\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., et al. (2017). \nAttention is all you need. In Proceedings of the conference workshop neural information \nprocessing systems (NIPS)\nWang, P ., Cheng, Y ., Dong, B., and Peng, Q. (2022a). Bring Globality into convolutional \nneural networks for wireless interference classification. IEEE Wireless Commun. Lett. 11, \n538–542. doi: 10.1109/LWC.2021.3135901\nWang, P ., Cheng, Y ., Dong, B., Peng, Q., and Li, S. (2022b). Multi-domain networks \nfor wireless interference recognition. IEEE Trans. Veh. Technol. 71, 6534–6547. doi: \n10.1109/TVT.2022.3164908\nWang, P ., Cheng, Y ., Dong, B., and Xun, H. (2020). Convolutional neural network-\nbased interference recognition. In Proceedings of the IEEE 20th international conference \non communication technology (ICCT), Nanning, China 1–5.\nWest, N.E., and O’Shea, T.J. (2017). Deep architectures for modulation recognition. \nIn Proceedings of the IEEE 2017th international symposium on dynamic Spectrum \naccess networks (DySPAN), Baltimore, USA 1–6\nZhang, R., and Cao, S. (2018). Support vector machines for classification of automotive \nradar interference. In Proceedings of the 2018 IEEE radar conference, Oklahoma, USA, \n366–371.\nZhang, D., Ding, W ., Zhang, B., Xie, C., Li, H., Liu, C., et al. (2018). Automatic \nmodulation classification based on deep learning for unmanned aerial vehicles. Sensors \n18:924. doi: 10.3390/s18030924\nZhang, X., Seyfi, T., Ju, S., Ramjee, S., Gamal, A.E., and Eldar, Y .C. (2019). Deep \nlearning for interference identification: band, training SNR, and sample selection. In \nProceedings of the IEEE 20th international workshop on signal processing advances in \nwireless communications (SPAWC), Cannes, France 1–5\nZhang, L., Wang, H., and Li, T. (2013). Anti-jamming message-driven frequency \nhopping-part I: System design. IEEE Trans. Wirel. Commun. 12, 70–79. doi: 10.1109/\nTWC.2012.120312.111706\nZhang, Q., Xiao, J., and Tian, C. (2023). A robust deformed convolutional neural \nnetwork (CNN) for image denoising. CAAI Trans. Intell. Technol. 8, 331–342. doi: \n10.1049/cit2.12110\nZhou, K., Zhang, M., Wang, H., and Tan, J. (2022). Ship detection in SAR images based \non multi-scale feature extraction and adaptive feature fusion. Remote Sens. 14:755. doi: \n10.3390/rs14030755\nZivkovic, M., Bacanin, N., Antonijevic, M., Nikolic, B., Kvascev, G., Marjanovic, M., \net al. (2022). Hybrid CNN and XGBoost model tuned by modified arithmetic \noptimization algorithm for COVID-19 early diagnostics from X-ray images. Electronics \n3798, 1–30. doi: 10.3390/electronics11223798",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7964321374893188
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6234084367752075
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5549265146255493
    },
    {
      "name": "Transformer",
      "score": 0.4519902169704437
    },
    {
      "name": "Embedding",
      "score": 0.42706042528152466
    },
    {
      "name": "Deep learning",
      "score": 0.4270450472831726
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.40334582328796387
    },
    {
      "name": "Machine learning",
      "score": 0.3683582544326782
    },
    {
      "name": "Speech recognition",
      "score": 0.33745789527893066
    },
    {
      "name": "Engineering",
      "score": 0.07952225208282471
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I149594827",
      "name": "Xidian University",
      "country": "CN"
    }
  ],
  "cited_by": 6
}