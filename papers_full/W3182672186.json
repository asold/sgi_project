{
  "title": "Transformer with peak suppression and knowledge guidance for fine-grained image recognition",
  "url": "https://openalex.org/W3182672186",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2432503302",
      "name": "Liu Xin-da",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1617585440",
      "name": "Wang LiLi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2308016294",
      "name": "Han, Xiaoguang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2896632102",
    "https://openalex.org/W2895810868",
    "https://openalex.org/W2896328740",
    "https://openalex.org/W2460852148",
    "https://openalex.org/W3204093460",
    "https://openalex.org/W6685446202",
    "https://openalex.org/W6735797469",
    "https://openalex.org/W2789016894",
    "https://openalex.org/W6779076919",
    "https://openalex.org/W6769265706",
    "https://openalex.org/W2562417371",
    "https://openalex.org/W2995924296",
    "https://openalex.org/W6687751829",
    "https://openalex.org/W2972610293",
    "https://openalex.org/W6760333050",
    "https://openalex.org/W2966316879",
    "https://openalex.org/W3198436663",
    "https://openalex.org/W2891951760",
    "https://openalex.org/W3119339683",
    "https://openalex.org/W6772623564",
    "https://openalex.org/W2765268259",
    "https://openalex.org/W2951464224",
    "https://openalex.org/W6779673106",
    "https://openalex.org/W3115482207",
    "https://openalex.org/W2731821979",
    "https://openalex.org/W6765836972",
    "https://openalex.org/W6771134910",
    "https://openalex.org/W2998619563",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W6761275059",
    "https://openalex.org/W6778778669",
    "https://openalex.org/W3035422918",
    "https://openalex.org/W6735236233",
    "https://openalex.org/W1954152232",
    "https://openalex.org/W2533598788",
    "https://openalex.org/W3093234244",
    "https://openalex.org/W6755069125",
    "https://openalex.org/W6767578973",
    "https://openalex.org/W3093753688",
    "https://openalex.org/W6783767857",
    "https://openalex.org/W6769631430",
    "https://openalex.org/W3123825612",
    "https://openalex.org/W3028797339",
    "https://openalex.org/W6772237328",
    "https://openalex.org/W3108870912",
    "https://openalex.org/W6796771772",
    "https://openalex.org/W3175248300",
    "https://openalex.org/W6798113983",
    "https://openalex.org/W3166348865",
    "https://openalex.org/W3140405497",
    "https://openalex.org/W6750765396",
    "https://openalex.org/W2883888092",
    "https://openalex.org/W2555741539",
    "https://openalex.org/W6713132643",
    "https://openalex.org/W6600738943",
    "https://openalex.org/W6741414320",
    "https://openalex.org/W3213528704",
    "https://openalex.org/W2798381792",
    "https://openalex.org/W2997426000",
    "https://openalex.org/W1797268635",
    "https://openalex.org/W2601450892",
    "https://openalex.org/W3174336354",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2964137095",
    "https://openalex.org/W2951852399",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2998345525",
    "https://openalex.org/W2202499615",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4294375521",
    "https://openalex.org/W3093038900",
    "https://openalex.org/W2963393555",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2981954115",
    "https://openalex.org/W2963767865",
    "https://openalex.org/W3176278386",
    "https://openalex.org/W2986821660",
    "https://openalex.org/W3024041237",
    "https://openalex.org/W2963426391",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2964081807",
    "https://openalex.org/W3093094504",
    "https://openalex.org/W2892035828",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W2961018736",
    "https://openalex.org/W2981631843",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2997300818",
    "https://openalex.org/W2173180041",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W4299802238",
    "https://openalex.org/W3035220232",
    "https://openalex.org/W2990154684",
    "https://openalex.org/W4288622677",
    "https://openalex.org/W2155541015",
    "https://openalex.org/W3034382172",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2994759459",
    "https://openalex.org/W3105979354",
    "https://openalex.org/W3128999341",
    "https://openalex.org/W2913012226",
    "https://openalex.org/W3124951096",
    "https://openalex.org/W2980088508"
  ],
  "abstract": null,
  "full_text": "Transformer with Peak Suppression and Knowledge Guidance for Fine-grained\nImage Recognition\nXinda Liua, Lili Wanga,‚àó, Xiaoguang Hanb\naState Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China\nbShenzhen Research Institute of Big Data, Shenzhen, China\nAbstract\nFine-grained image recognition is challenging because discriminative clues are usually fragmented, whether from a single\nimage or multiple images. Despite their signiÔ¨Åcant improvements, the majority of existing methods still focus on the\nmost discriminative parts from a single image, ignoring informative details in other regions and lacking consideration of\nclues from other associated images. In this paper, we analyze the diÔ¨Éculties of Ô¨Åne-grained image recognition from a new\nperspective and propose a transformer architecture with the peak suppression module and knowledge guidance module,\nwhich respects the diversiÔ¨Åcation of discriminative features in a single image and the aggregation of discriminative clues\namong multiple images. SpeciÔ¨Åcally, the peak suppression module Ô¨Årst utilizes a linear projection to convert the input\nimage into sequential tokens. It then blocks the token based on the attention response generated by the transformer\nencoder. This module penalizes the attention to the most discriminative parts in the feature learning process, therefore,\nenhancing the information exploitation of the neglected regions. The knowledge guidance module compares the image-\nbased representation generated from the peak suppression module with the learnable knowledge embedding set to obtain\nthe knowledge response coeÔ¨Écients. Afterwards, it formalizes the knowledge learning as a classiÔ¨Åcation problem using\nresponse coeÔ¨Écients as the classiÔ¨Åcation scores. Knowledge embeddings and image-based representations are updated\nduring training simultaneously so that the knowledge embedding includes a large number of discriminative clues for\ndiÔ¨Äerent images of the same category. Finally, we incorporate the acquired knowledge embeddings into the image-based\nrepresentations as comprehensive representations, leading to signiÔ¨Åcantly higher recognition performance. Extensive\nevaluations on the six popular datasets demonstrate the advantage of the proposed method in performance. The source\ncode and models will be available online after the acceptance of the paper.\nKeywords: Fine-grained image recognition, food recognition, knowledge guidance, peak suppression, vision transformer\n1. Introduction\nAiming to distinguish the objects belonging to multi-\nple sub-categories of the same meta-category, Ô¨Åne-grained\nimage recognition has been one of the most fundamen-\ntal problems in the computer vision and multimedia com-\nmunities [1, 2, 3, 4, 5]. It is essential for a wide range\nof downstream applications such as rich image captioning\n[6], image generation [7], machine teaching [8], Ô¨Åne-grained\nimage retrieval [9], food recognition [10, 11], and food rec-\nommendation [12].\nFine-grained image recognition is challenging due to\nsubtle inter-class diÔ¨Äerences and signiÔ¨Åcant intra-class\nvariances. Most existing methods only consider the prob-\nlem of Ô¨Åne-grained recognition from the perspective of ob-\ntaining the discriminative characteristics of a single image\nbut ignore the clues provided by multiple images. In order\nto take the clues of multiple images into consideration, we\ntry to explain the diÔ¨Éculty of Ô¨Åne-grained image recogni-\n‚àóCorresponding author\n(a) (b) (c)\nFigure 1: Some Ô¨Åne-grained bird images sampling from the CUB-\n200-2011 dataset. (a) is Black Tern, both (b) and (c) are Long-\ntailed Jaeger. Discriminative parts are annotated by red boxes. The\ndetails in the red boxes of (a) and (b) are magniÔ¨Åed next to the\nimage. Discriminative clues are distributed in multiple regions of\nthe image, and the clues of a single image are usually incomplete.\ntion with a new perspective and attribute it to fragmented\ndiscriminative clues.\nThe fragmentation here has two implications: (1) From\nthe perspective of a single image, discriminative clues ap-\npear in diÔ¨Äerent local areas since the inter-class diÔ¨Äerences\ncould be subtle; As shown in Fig. 1 (a) and (b), the long-\nPreprint submitted to Elsevier December 13, 2021\narXiv:2107.06538v2  [cs.MM]  10 Dec 2021\ntailed jaeger is similar to the black tern overall, but the\nbeak of long-tailed jaeger is curved, and the beak of the\nblack tern is straight. This kind of discriminative part\nis usually tiny and distributed in diÔ¨Äerent image regions,\nas shown in Figure 1 (c). (2) From the perspective of\nmultiple images, each image contains only a part of the\ndiscriminative information about the category depending\non diÔ¨Äerent poses, scales, and rotations, due to signiÔ¨Åcant\nintra-class variances. As shown in Fig. 1 (b) and (c), these\ntwo pictures are long-tailed jaeger, the beak of the bird in\n(c) is diÔ¨Écult to distinguish, and there are no bird claws in\n(b). Therefore, the discriminative information contained\nin each image is incomplete.\nTo Ô¨Ånd and aggregate fragmented clues is the key to\nÔ¨Åne-grained image recognition. Despite their impressive\nresults, existing methods usually consider Ô¨Åne-grained im-\nage recognition only from few regions, ignoring many in-\nformative details in other regions and other associated im-\nages. For instance, if the beak of a speciÔ¨Åc bird is very\ndiÔ¨Äerent from other birds, the model may pay much at-\ntention to the beak of the bird while ignoring the claws\nand tail of the bird. When this happens, the model could\neasily make mistakes when the beak of the bird is not vis-\nible.\nGiven this challenge, we propose a Transformer with\nPeak Suppression and Knowledge Guidance (TPSKG) for\nÔ¨Åne-grained image recognition. The proposed Peak Sup-\npression (PS) module uses the transformer architecture to\nintegrate the local information and explores a training rou-\ntine to increase the diversity of discriminative features.\nThis PS module is designed to obtain as many discrimina-\ntive clues as possible from a single image. Simultaneously\nthe proposed Knowledge Guidance (KG) module incorpo-\nrates the learnable knowledge embedding into the image-\nbased representation for a comprehensive representation.\nThis KG module is used to aggregate discriminative infor-\nmation from multiple images.\nSpeciÔ¨Åcally, we are inspired by ViT [13] and use a trans-\nformer architecture to tackle the Ô¨Åne-grained image recog-\nnition problems. The input image is reshaped into a patch\nsequence without overlap and then linearly mapped to the\nsequential tokens. The transformer encoder uses the self-\nattention mechanism to integrate the information of the\ndiÔ¨Äerent tokens to obtain a global representation. Instead\nof integrating all the token information like the original\nViT, we deliberately remove the most discriminative to-\nken based on the value of the attention weight map in\ntraining to penalize strongly discriminative learning and\nenforce the network to pay attention to other neglected\ninformative areas for keeping the Ô¨Åne-grained representa-\ntion diversity.\nAfter that, we use a knowledge embedding set to explic-\nitly express the discriminative clues of the same category\nfrom diÔ¨Äerent images and formalize the learning of knowl-\nedge embedding as a classiÔ¨Åcation task. The knowledge\nguidance module measures the similarity of the knowledge\nembeddings and image-based representations generated\n(a) (b) (c)\nFigure 2: The eÔ¨Äects of the proposed approach for some samples from\nthe CUB-200-2011 and Stanford Dogs datasets. (a) is the original\nÔ¨Åne-grained image, (c) and (b) are the attention weights obtained\nfrom the vision transformer with and without the proposed method\n(TPSKG). The parts of the proposed method that are signiÔ¨Åcantly\ndiÔ¨Äerent from the original method are annotated by red boxes.\nfrom the peak suppression module to obtain the knowl-\nedge response coeÔ¨Écients. We use the knowledge response\ncoeÔ¨Écients as the classiÔ¨Åcation scores directly and use the\ncategory label as ground truth to supervise the knowl-\nedge learning. The image-based representations become\nmore discriminative through the joint training of Ô¨Åne-\ngrained classiÔ¨Åcation and knowledge learning tasks, and\nthe knowledge embeddings are also concurrently updated\nthrough iterations. The learning procedure of knowledge\nembeddings covers the entire training dataset, therefore\nthese embeddings become the comprehensive representa-\ntions containing various subtle and slight characteristics of\nall categories. Finally, we obtain the knowledge-based rep-\nresentations computed from the knowledge embedding set\nalong with the knowledge response coeÔ¨Écients, and inject\nthem into the image-based representations. The proposed\nknowledge embedding learning and exploitation lead to a\nsigniÔ¨Åcant boost for recognition performance.\nTo verify the eÔ¨Äectiveness of our method, we conduct ex-\ntensive experiments on the six popular benchmarks for the\nÔ¨Åne-grained image recognition task. Quantitative experi-\nmental results demonstrate that the proposed method can\nachieve competitive performance compared to the state-\nof-the-art approaches. As shown in Fig. 2, qualitative\nexperimental results demonstrate the advantages of our\nmethod in covering more informative areas and increasing\nthe diversity of expression at the same time. The quanti-\ntative analysis and visualization of knowledge embedding\nalso illustrate the eÔ¨Äectiveness of category-related knowl-\nedge embedding learning.\nIn summary, we make the following main contributions:\n(1) We provide a new perspective that the diÔ¨Éculty of\nÔ¨Åne-grained recognition lies in fragmented discriminative\n2\nclues. This perspective helps consider not only multiple\nregions from a single image but also multiple images.\n(2) We propose a vision transformer architecture with\npeak suppression and knowledge guidance for the Ô¨Åne-\ngrained image recognition task. Peak suppression eÔ¨Äec-\ntively increases the diversity of image representations via\naggregating the local features from multiple regions from\na single image. Knowledge guidance optimizes the Ô¨Ånal\nrepresentations with the knowledge embeddings learning\nfrom multiple images.\n(3) We formalize the knowledge learning as a classiÔ¨Å-\ncation problem and directly use the similarity between\nknowledge embeddings and image-based representations\nas the classiÔ¨Åcation score to update the knowledge em-\nbeddings related to the category.\n(4) We conduct extensive quantitative and qualitative\nexperiments to demonstrate the eÔ¨Äectiveness of the pro-\nposed method, which achieves competitive performance\ncompared to the state-of-the-art approaches on six pub-\nlic datasets.\nThe rest of this paper is organized as follows. Section\n2 reviews the related works. Section 3 elaborates on the\nproposed framework. Experimental results and analysis\nare reported in Section 4. Finally, we conclude the paper\nin Section 5.\n2. Related Works\nThis section introduces the most related researches into\nthe following categories: the Ô¨Åne-grained image recogni-\ntion task and the vision transformer architecture.\n2.1. Fine-grained Image Recognition\nThere are two prevailing paradigms in the current re-\nsearch in Ô¨Åne-grained image recognition. One is the local\nidentiÔ¨Åcation, and the other is the global discrimination.\nLocal-identiÔ¨Åcation approaches focus on locating the\ndiscriminative semantic parts of Ô¨Åne-grained objects to\nidentify the subtle diÔ¨Äerences among diÔ¨Äerent object cate-\ngories and construct mid-level representations correspond-\ning to these parts for the Ô¨Ånal classiÔ¨Åcation. Early works\n[14, 15] used strong supervised mechanisms with part\nbounding box annotations to learn localizing the discrim-\ninative parts. However, the part annotation is time-\nconsuming. Recent researches [16, 17, 18, 19] focused on\nweakly supervised recognition methods with only image-\nlevel labels to obtain accurate part localization to solve\nthis problem. Some patch-based methods [20, 21, 22] Ô¨Årst\ninitialize abundant region proposals and select the discrim-\ninative parts based on a speciÔ¨Åc strategy. There are also\nattention-based ways to localize the corresponding high\nareas related to the image label, such as [23, 24, 25, 10].\nGlobal-discrimination approaches generally learn the\nembeddings using a speciÔ¨Åc distance metric so that sam-\nples from the same category can be pulled close to each\nother while samples from diÔ¨Äerent categories are pushed\napart. For example, a bilinear model is used in [26] to learn\nthe interacted feature of two independent CNNs, which\nachieves remarkable Ô¨Åne-grained recognition performance.\nHowever, the exceptionally high dimensionality of bilinear\nfeatures still makes it impractical for realistic applications.\nChen et al. [27] enforced the classiÔ¨Åcation network to pay\nmore attention to discriminative regions for spotting the\ndiÔ¨Äerences by destructing and reconstructing the input im-\nage. Sun et al. [28] masked the most salient features for\nthe input images to force the network to use more subtle\nclues for its correct classiÔ¨Åcation. Zhuang et al.[29] learned\na mutual feature vector to capture semantic diÔ¨Äerences in\nthe input image pair.\nUnlike the methods described above, we consider the\ndiscriminative but not the most signiÔ¨Åcant part of a single\nimage, but also emphasize the discriminative information\naggregation in diÔ¨Äerent images. Hence, we propose a vision\ntransformer with peak suppression and knowledge guid-\nance, which can eÔ¨Äectively increase the richness of Ô¨Åne-\ngrained representations in a local area and eÔ¨Äectively ag-\ngregate patch features. Simultaneously, it emphasizes the\nlearning and utilization of the knowledge of distinguishing\ncharacteristics between diÔ¨Äerent image samples.\n2.2. Vision Transformer\nThe transformer architecture by [30] is proposed to deal\nwith the sequential data in the Ô¨Åeld of natural language\nprocessing [31, 32]. Inspired by the breakthroughs of trans-\nformer architectures in the Ô¨Åeld of natural language pro-\ncessing, researchers have recently applied transformer to\ncomputer vision tasks, such as image recognition [13, 33],\nobject detection [34, 35], segmentation [36], image super-\nresolution [37]. For example, Cordonnier et al. [38] proved\nthat a multi-head self-attention layer with a suÔ¨Écient num-\nber of heads is at least as expressive as any convolutional\nlayers. They extracted patches from the input image and\napplied full self-attention on top. iGPT [39] applies trans-\nformers to image pixels after reducing the image resolution\nand color space. It is worth noting that the Vision Trans-\nformer (ViT) [13] is a pure transformer that performs well\non the image classiÔ¨Åcation task when applied directly to\nthe sequences of image patches. Based on ViT, Touvron\net al.[33] transferred the model to the Ô¨Åne-grained visual\ncategorization and achieved competitive performance.\nTo sum up, the vision transformer maps group pixels\ninto a small number of visual tokens, representing a seman-\ntic concept in the image. These visual tokens are used di-\nrectly for image classiÔ¨Åcation, with the transformers being\nused to model the relationships among tokens. Our work\nis inspired by ViT and adopts the same method as ViT to\nbuild a transformer. Despite the eÔ¨Éciency of iGPT, ViT,\nand DeiT, these works only Ô¨Åne-tune the model on the\nÔ¨Åne-grained datasets directly to evaluate the eÔ¨Äectiveness\nof the model for transfer learning and ignore the character-\nistics of the Ô¨Åne-grained image recognition task. DiÔ¨Äerent\nfrom the above methods, we consider the characteristics of\n3\nthe Ô¨Åne-grained image and focus on the speciÔ¨Åc recognition\ntask.\n3. Framework\nThis section introduces the proposed framework, which\nis a transformer architecture for the Ô¨Åne-grained recogni-\ntion task. As shown in Fig. 3, this framework mainly con-\nsists of two components, namely Peak Suppression (PS)\nand Knowledge Guidance (KG). PS takes images as input\nand outputs the suppressed image-based representations to\nthe KG module. The KG module takes the image-based\nrepresentations and learns the knowledge embeddings, Ô¨Å-\nnally uses the fusion representations for the recognition\ntask. Section 3.1 introduces PS and Section 3.2 details\nKG.\n3.1. Peak Suppression\nInspired by the eÔ¨Äectiveness of the diversiÔ¨Åcation block\non convolutional neural networks, we proposed the peak\nsuppression module on the transformer architecture to pay\nmore attention to the other informative parts and obtain\nmore diverse expressions by suppressing the most discrim-\ninative regions. DiÔ¨Äerent from the CNNs-based method of\ndirectly operating feature maps using the category-speciÔ¨Åc\nactivation maps, the transformer-based method cannot\nachieve the goal by directly removing the most signiÔ¨Åcant\ncorresponding token in the last layer because all tokens\nhave interacted during the feedforward process in multi-\nhead attention layers. Therefore, we can only use the at-\ntention map to backtrack to the input image space and\nthen mask salient image regions in the image space.\nFormally, we follow the settings of [13] and use the ViT\nas the backbone. Let x‚ààRH√óW√óC denotes a given train-\ning image where ( H,W ) is the resolution of the image, C\nis the number of channels. The image xis reshaped into a\nsequence of Ô¨Çattened 2D patches xp ‚ààRN√óP2√óC, the res-\nolution of each image patch is (P,P ), and N = HW/P2 is\nthe resulting number of patches. These patches are con-\nverted to D dimensions embedding xpE ‚ààRN√óD as input\ntokens through a trainable linear projection. Attaching\nthe learnable embedding class token z0\n0, there are a total\nof N + 1 tokens. Position embeddings Epos are added to\nthe patch embeddings Epos ‚ààR(N+1)√óD to retain the po-\nsitional information. The transformer encoder takes the\nz0 as input and outputs zL and the attention weight ML,\nwhere L means the transformer encoder is composed of a\nstack of L identical layers. Each layer consists of multi-\nhead self-attention (MSA) and MLP blocks. Layernorm\n(LN) is applied before every block and residual connec-\ntions after every block.\nz0 = [xclass; x1\npE; x2\npE; ...; xN\np E] + Epos,\nz‚Ä≤\nl = MSA(LN(zl‚àí1)) + zl‚àí1,l = 1...L,\nzl = MLP(LN(z‚Ä≤\nl)) + z‚Ä≤\nl,l = 1...L.\n(1)\nWe use the Attention Rollout technique [40] to acquire\nthe attention map from the output token to the input\nspace. Given a transformer with Llayers, we need to Ô¨Çow\nthe attention from all positions in the Ô¨Ånal layer L to all\npositions in layer 1. At every transformer layer, we aver-\nage attention weights at each layer over all heads and get\nthe weight matrixMl that deÔ¨Ånes the attention value Ô¨Çows\nfrom all tokens in the previous layer to all tokens in the l\nlayer. Considering that there are residual connections in\nthe backbone, we deal with them by adding the identity\nmatrix I to the attention matrices and re-normalize the\nattention weights to keep the total attention in the range\nof 0 to 1. Finally, we recursively multiply the weight ma-\ntrices of all layers.\nÀúMl =\n{\n(Ml + I) ÀúMl‚àí1 ifl> 1,\nMl + I ifl = 1. (2)\nIn this equation, ÀúMl is attention rollout of the l layer,\nMl is raw attention of the l layer, and the multiplication\noperation is matrix multiplication. The ÀúML illustrates the\nmixing of attention among tokens across all layers.\nLet B ‚ààRN+1 denote the binary suppressing mask for\nthe input tokens. Each element in mask Bis in the domain\n{0,1}, where 0 indicates the corresponding location will\nbe suppressed while 1 means that no suppression will take\nplace. Note that the B0 is always 1 because it corresponds\nto the class token.\nAfter obtaining the attention map ÀúML ‚ààRN , we com-\npute the B by traversing the entire attention map and\nÔ¨Ånding the position of the largest response.\nBi =\n{\n0 if ÀúMi‚àí1\nL = max( ÀúML)\n1 otherwise. (3)\nwhere i‚àà{1,2,...,N }. In order to remove the inÔ¨Çuence of\npeaky token, we remove it from the forward process.\nÀÜz0 = [xclass; x1\npE; x2\npE; ...; xN\np E] ‚àóB+ Epos, (4)\nwhere ‚àódenotes element-wise multiplication. After the\nforward process, the transformer encoder outputs y as the\nimage-based representation.\nÀÜz‚Ä≤\nl = MSA(LN(ÀÜzl‚àí1)) + ÀÜzl‚àí1,l = 1...L,\nÀÜzl = MLP(LN(ÀÜz‚Ä≤\nl)) + ÀÜz‚Ä≤\nl,l = 1...L.\ny= LN(ÀÜz0\nL),\n(5)\nwhere ÀÜz0\nL denotes the class token vector of the output of\nL layer transformer encoder after peak suppression. We\nimplement the remove of Equation (4) by setting ‚àí‚àûto\nthe suppressed token vectors. After the softmax layer of\nEquation (5), the responses of these tokens will be close\nto 0.\nTo sum up, we suppress the peaky token based on the\nattention maps in the training phase. By suppressing the\ntokens, the network is forced to Ô¨Ånd the other informative\nregions instead of the most discriminative regions in the\nimage. The increase of the feature diversity can improve\nthe performance of the network in the test phase.\n4\nKnowledge  Guidance\nKnowledge \nResponse\ncoefficients\nFusion \nRepresentation\nPrediction\nImage-based\nRepresentation\nKnowledge Set\n‚Ä¶\n1st category\n2nd category\n3rd category\nnth category\nKnowledge-based\nRepresentation\nFusion\nRepresentation\nGround Truth\nYellow-breasted Chat\nPeak \nSuppression\nPosition\nEmbedding\n1 2 3 4 5 6 7 8 90\n*\nLinear Projection\n‚Ä¶\nAttention Map\nEpoch 1\n Epoch 2 Epoch t\nTransformer Encoder\nSuppress \nOperation \n‚Ä¶\nùëÜùëÜ\nùëÜùëÜ\nùëÜùëÜ\nùëÜùëÜ ùêøùêøùëòùëòùëòùëò\nùêøùêøùëüùëüùëüùëüùëüùëü\nKnowledge learning\nPrediction\nFigure 3: Overview of our framework. Here we visualize the case of peak suppression and knowledge guidance given a training batch with a\nimage and its corresponding label Yellow-breasted Chat. S(¬∑) means the similarity function. Only the presentation label is used to predict in\ntesting.\n3.2. Knowledge Guidance\nAfter obtaining the diversiÔ¨Åed discriminative clues of a\nsingle image, the knowledge guidance module fuses the in-\nformation of multiple images to get a more comprehensive\nfeature representation. The knowledge guidance module\nÔ¨Årst learns the knowledge embeddings related to the cate-\ngory. To this end, we propose a novel learning method that\nabstracts the learning of knowledge embeddings as a clas-\nsiÔ¨Åcation problem and directly uses the similarity between\nknowledge embeddings and image-based representations as\nthe classiÔ¨Åcation score. Subsequently, the knowledge guid-\nance module injects the obtained knowledge embeddings\ninto the image-based representations to get a more com-\nprehensive expression. Therefore, the knowledge guidance\nmodule contains two tasks, one is knowledge learning, and\nthe other is knowledge exploiting. We Ô¨Årst introduce the\nknowledge learning task.\n3.2.1. Knowledge Learning\nTo enforce the networks to learn the knowledge em-\nbedding for each category, we treat the knowledge learn-\ning task as a classiÔ¨Åcation task. Considering the multi-\nclass Ô¨Åne-grained image recognition task, let F= {fg}G\ng=1\ndenotes the Ô¨Åne-grained label set containing all G Ô¨Åne-\ngrained labels and X= {xj}J\nj=1 denotes the image train-\ning dataset containing the total J images. Through the\npeak suppression module of transformer encoder, we can\nacquire the representation set Y= {yj}J\nj=1,yj ‚ààRD from\nthe training dataset. Randomly initializing a knowledge\nembedding set of the D-dimension knowledge embedding\nK= {kg}G\ng=1,kg ‚ààRD, each kg means the knowledge em-\nbedding of the category fg.\nGiven a image xj with the corresponding ground-truth\nÔ¨Åne-grained label fg, the transformer encoder outputs its\nrepresentation yj ‚ààRD. The knowledge embedding set\ntries to distinguish which category this representation yj\nbelongs to by judging the similarity between this repre-\nsentation yj and every knowledge embedding kg in the K.\nWe obtain a knowledge response coeÔ¨Écients rj ‚ààRG.\nrj = Softmax(S(yj,K)), (6)\nwhere S(¬∑) is a similarity function. We have tried a vari-\nety of methods to calculate similarity, such as the neural\nnetworks and element-wise multiplication, and the results\nare not signiÔ¨Åcantly diÔ¨Äerent. In addition, the computa-\ntional complexity of element-wise multiplication is much\nsmaller. Therefore, without loss of generality, the element-\nwise multiplication is adopted in our experiments.\nWe then directly convert the knowledge response coeÔ¨É-\ncients rj into one-hot for a supervised learning. We deÔ¨Åne\nthe knowledge learning loss as\nLosskl = CrossEntropy(Onehot(rj),fg). (7)\nWe use Losskl to supervise knowledge learning pro-\ncessing to update the knowledge embedding set. The\nknowledge embeddings have gradually become the com-\nmon ground of diÔ¨Äerent instances of the same category\nduring the training procedure. As the representations be-\ncome more discriminative in training, the knowledge em-\nbeddings become better to express the category. Further-\nmore, in this training procedure, the knowledge learning\n5\ntask considers the representations of all training images,\nmaking the knowledge embeddings more comprehensive in\nthe expression of corresponding categories.\n3.2.2. Knowledge Exploiting\nIn order to eÔ¨Äectively use knowledge, we Ô¨Årst obtain a\nknowledge-based representation based on the knowledge\nresponse coeÔ¨Écients and the knowledge embeddings. The\nknowledge-based representation Œ¥j is computed as\nŒ¥j =\n‚àë\ng‚ààG\nrjkg. (8)\nThis knowledge-based representation includes a sum-\nmary of the Ô¨Åne-grained features of diÔ¨Äerent instances in\nthe same category and a summary of the diÔ¨Äerences in\ndiÔ¨Äerent categories.\nWe use the knowledge to guide the classiÔ¨Åcation by in-\njecting the knowledge-based representation into the Ô¨Ånal\nfusion Ô¨Åne-grained representation:\nuj = FC(LN(yj + Œ¥j)), (9)\nwhere FC is the fully connective layer.\nWe deÔ¨Åne a representation learning loss to supervise the\ntraining to obtain the fusion representation:\nLossrep = CrossEntropy(uj,fg). (10)\nWe optimize the knowledge learning task and the knowl-\nedge exploiting task simultaneously so that the image rep-\nresentation and the knowledge can be updated iteratively\nand promote each other during the training process.\nThus, the total loss function of the whole network can\nbe deÔ¨Åned as\nLoss= Losskl + ¬µ√óLossrep, (11)\nwhere ¬µ is a hyperparameter used to adjust the diÔ¨Äerent\nemphasis of the two tasks.\nDuring the training, our method explicitly obtains the\nknowledge embeddings of the diÔ¨Äerent Ô¨Åne-grained cate-\ngories. These knowledge embeddings are distinguishable\nand comprehensive, so we insert them into the image-based\nrepresentations to increase the comprehensiveness of fea-\ntures for the Ô¨Åne-grained recognition task.\nIn summary, the peak suppression module aims to con-\nsider more regions in a single image to obtain more di-\nverse expressions. Based on the peak suppression mod-\nule, the knowledge guidance module aims to extract and\nexploit the category-related embeddings based on the ex-\npression of multiple images. Therefore, these two modules\nare complementary in aggregating fragmented information\nat diÔ¨Äerent levels. In the next Section 4, we conduct suÔ¨É-\ncient experiments to prove the eÔ¨Äectiveness of the proposed\nmethod.\n4. Experiments\n4.1. Experimental Setup\n4.1.1. Datasets\nWe conduct our experiments on six Ô¨Åne-grained image\nrecognition datasets, including two publicly available bird\ndatasets CUB-200-2011 [41] and NABirds [42], one Ô¨Çower\ndataset Oxford 102 Flowers [43], one dog dataset Stanford\nDogs [44], and two food datasets ISIA Food-200 [10] and\nISIA Food-500 [45]. The detailed statistics about these six\ndatasets including class numbers and train/test distribu-\ntions are summarized in Table 1.\nTable 1: Fine-grained image dataset statistics.\nDataset # Class # Training # Testing\nCUB-200-2011 [41] 200 5,994 5,794\nOxford Flowers [43] 102 1,020 6,149\nStanford Dogs [44] 120 12,000 8,580\nNABirds [42] 555 23,929 24,633\nISIA Food-200 [10] 200 118,210 59,287\nISIA Food-500 [45] 500 239,379 120,143\n4.1.2. Implementation Details and Comparison\nMethods\nOur method is implemented on the Pytorch platform\nwith four Nvidia V100 GPUs. The input image size is 448\n√ó448 as most state-of-the-art Ô¨Åne-grained image recogni-\ntion approaches. By following the settings of NTS-NET\n[20], we use data augmentations, including random crop-\nping and horizontal Ô¨Çipping during the training proce-\ndure. Only the center cropping is involved in inference.\nThe model is trained with the stochastic gradient descent\n(SGD) with a batch size of 8 and momentum of 0.9 for all\ndatasets. The learning rate is set to 3e-2 initially, and the\nschedule applies a cosine decay function to an optimizer\nstep. In all our experiments, we use ViT-B-16 pre-trained\non ImageNet21k as the backbone. For all experiments,\nwe adopt the top-1 accuracy as the evaluation metric. To\ndemonstrate the advantages of our model, we list the some\nmethods for comparisons.\n‚Ä¢ DVAN [4]: DiversiÔ¨Åed visual attention network which\nrelieves the dependency on supervised information.\n‚Ä¢ MaxEnt [46]: Maximum entropy approach which pro-\nvides a training routine to maximize the entropy of\nthe output probability distributions.\n‚Ä¢ NTS-NET [20]: Navigator-teacher-scrutinizer net-\nwork which Ô¨Ånds consistent informative regions\nthrough multi-agent cooperation.\n‚Ä¢ Cross-X [47]: Multi-scale feature learning with ex-\nploiting the relationships between diÔ¨Äerent images\nand layers.\n‚Ä¢ GHNS [48]: A framework that generates features of\nhard negative samples.\n6\n‚Ä¢ CSC-Net [49]: Category-speciÔ¨Åc semantic coherency\nnetwork which semantically aligns the discriminative\nregions of the same subcategory.\n‚Ä¢ CDL [50]: Correlation-guided discriminative learning\nmodel which mines and exploits the discriminative po-\ntentials of correlations.\n‚Ä¢ MLA-CNN [51]: Multi-level attention model which\nuses the neural activations to generate multi-scale re-\ngions which are helpful for the Ô¨Åne-grained categoriza-\ntion.\n‚Ä¢ BiM-PMA [52]: Progressive mask attention model by\nleveraging both visual and language modalities.\n‚Ä¢ CIN [53]: Channel interaction network models\nchannel-wise interplay within an image and across im-\nages.\n‚Ä¢ DB [28]: End-to-end network with a diversiÔ¨Åcation\nblock to use more subtle clues.\n‚Ä¢ FDL [22]: Filtration and distillation learning model\nwith region proposing and feature learning.\n‚Ä¢ PMG [54]: Progressive multi-granularity method ex-\nploiting information based on the smaller granularity\ninformation found at the last step and the previous\nstage.\n‚Ä¢ API-NET [29]: Pairwise interaction network which\ncan progressively recognize a pair of images by inter-\naction.\n‚Ä¢ CPM [17]: Complementary part model in a weakly\nsupervised manner to retrieve information suppressed\nby dominant object parts detected by CNNs.\n‚Ä¢ GaRD [55]: Graph-based relation discovery approach\nto grasp the stronger contextual details.\n‚Ä¢ SnapMix [56]: Semantically proportional mixing\nwhich exploits CAM to lessen the label noise in aug-\nmenting Ô¨Åne-grained data.\n‚Ä¢ HGNet [57]: Hierarchical gate network to exploit the\ninterconnection among hierarchical categories.\n‚Ä¢ SCAPNet [58]: Scale-consistent attention part net-\nwork to guide part selection across multi-scales and\nkeep the selection scale consistent.\n‚Ä¢ CTF-CapsNet [59]: Coarse-to-Ô¨Åne capsule network to\nshape an increasingly specialized description.\n‚Ä¢ MSEC [60]: Multi-Scale Erasure and Confusion which\nrealizes confusion at diÔ¨Äerent scales in images and\nsub-regions.\n4.2. Ablation Analysis\nIn this section, we conduct a series of ablation studies\non the CUB-200-2011, Stanford Dog, Oxford 102 Flow-\ners, NABirds, ISIA Food-200, and ISIA Food-500 datasets\nto better understand the designation of the proposed TP-\nSKG. We use the performance of the original ViT-B-16 as\nthe ablation baseline.\n4.2.1. Impact of diÔ¨Äerent components\nTo investigate the contribution of each component in\nthe proposed method, we omit diÔ¨Äerent components of\nTPSKG and report the corresponding top-1 recognition\naccuracy. From the results reported in Table 2, we can\ndraw the following conclusions:\n(1) The recognition accuracy on the CUB-200-2011\ndataset drops from 91.3% to 91.0% and 90.9% when omit-\nting the PS module and the KG module respectively, which\ndemonstrates the eÔ¨Äectiveness of both of the components\nfor the Ô¨Åne-grained image recognition task. The experi-\nmental results on the other Ô¨Åve datasets also have similar\ntrends to the results of the CUB-200-2011 dataset, indi-\ncating that both the PS module and the KG module can\neÔ¨Äectively improve the recognition performance.\n(2) The network with only the PS module improves\nthe recognition accuracy of baseline by 0.5% (90.4% vs.\n90.9%) on CUB-200-2011 dataset, shows that details other\nthan the most signiÔ¨Åcant part are helpful for the Ô¨Åne-\ngrained image recognition task. At the same time, the\nnetwork with the KG module improves 0.6% (90.4% vs.\n91.0%) on the CUB-200-2011 dataset, showing that inte-\ngrating the discriminative information of multiple images\ncan eÔ¨Äectively improve the recognition performance of the\nnetwork. This result is consistent with our analysis of\ndiscriminative information fragmentation in Ô¨Åne-grained\nimage recognition tasks.\n(3) The PS module improves the recognition accuracy\nof baseline by 1.3% (59.9% vs. 61.2%) and the KG mod-\nule improves the recognition accuracy of baseline by 2.1%\n(59.9% vs. 62.0%) on the ISIA Food-500 dataset, the com-\nbination of these two modules can improve the recognition\naccuracy of baseline by 5.5% (59.9% vs. 65.4%). This ex-\nperimental phenomenon shows that the PS module and\nthe KG module are complementary and can promote each\nother. The more diverse the expression obtained by the\npeak suppression module, the stronger the expression abil-\nity of embedding learned by the knowledge guidance mod-\nule.\n4.2.2. Visualizations of knowledge embedding\nThe knowledge embedding is a category-related feature\nrepresentation learning with the similarity as the classiÔ¨Å-\ncation score directly. The embedding becomes the most\nsimilar to the feature expression in the corresponding cat-\negory and the most dissimilar to the feature expression in\nother categories. As shown in Fig. 4: (1) The knowledge-\nbased representation and the image-based representation\n7\nTable 2: Ablation results of the proposed TPSKG on the Ô¨Åne-grained image datasets.\nModel CUB-200-2011 Stanford Dog Oxford Flowers NABirds ISIA Food-200 ISIA Food-500\nViT-B-16 90.4 91.4 99.2 89.6 67.4 59.9\nw/o Peak Suppression 91.0 91.8 99.3 89.9 69.3 62.0\nw/o Knowledge Guidance 90.9 91.8 99.3 89.8 68.3 61.2\nAll TPSKG 91.3 92.5 99.5 90.1 69.5 65.4\nFigure 4: The t-SNE visualizations of image representations and\nknowledge embedding of 50 sample categories from the CUB-200-\n2011 dataset. Each dot stands for an image representation, and each\nstar represents the knowledge embedding of a category. The color\nindicates the categories.\nare in the same subspace, which is convenient for feature\nfusion. (2) These two representations of the same category\nare close in the subspace, indicating that knowledge-based\nrepresentation can express category-related information.\n(3) The knowledge-based representation is separable in the\nsubspace, so it is helpful for recognition tasks.\n4.2.3. Choice of ¬µ:\nSince Equation 11 requires selecting a hyperparameter\n¬µ, it is essential to study the inÔ¨Çuence of classiÔ¨Åcation\nperformance on the choice of ¬µ. We conduct this exper-\niment for four diÔ¨Äerent ¬µ on the CUB-200-2011 dataset.\nAs shown in Fig. 5, the experimental results show that\n(1) The performance is relatively robust to the choice of ¬µ\ngenerally. (2) The model performs better when the weight\nof Lossrep is slightly larger than Losskl. The probable\nreason is that the learning of knowledge relies on the com-\nbined eÔ¨Äect of representation and label. A slightly larger\nweight of Lossrep allows the network to learn the discrim-\ninative feature representation preferentially. Because the\nperformance of the model is not sensitive to the choice of\n¬µ, the weighting coeÔ¨Écient in Equation 11 is empirically\nchosen to be ¬µ=2 in all the following experiments.\n0.5 1 2 3\nThe choice of \n87\n88\n89\n90\n91\n92\n93\nAccuracy(%)\nFigure 5: Performance on the choice of ¬µ on the CUB-200-2011\ndataset.\n4.2.4. Computation complexity\nSince the computation and memory cost can be heavy\nfor global attention in vision transformer architecture, we\ncompare the proposed modules to the original ViT, both\nin terms of the number of parameters and MACs (Mul-\ntiply‚ÄìAccumulate Operations). As shown in Table 3, the\nproposed KG module achieves better performance com-\npared to the original ViT without signiÔ¨Åcantly increas-\ning computational complexity. Although the proposed\nPS module increases the computational complexity in the\ntraining phase, it does not increase the computational\ncomplexity at all in the inference phase, and at the same\ntime improves the recognition performance. The number\nof parameters has not increased signiÔ¨Åcantly.\nTable 3: Comparison results on Cub-200-2011 dataset.\nModels Input\nSize\nParams\n(M)\nTraining\nMACs(G)\nInference\nMACs(G)\nAccuracy\n(%)\nViT-B-16 448√ó448 86.4 67.14 67.14 90.4\nViT w/ PS448√ó448 86.4 134.4 67.14 90.9\nViT w/ KG448√ó448 86.6 67.14 67.14 91.0\nTPSKG 448√ó448 86.6 134.4 67.14 91.3\n4.3. Comparison with State-of-the-art\nFor further veriÔ¨Åcation for the TPSKG, we compare our\nmethod to the state-of-the-art methods on the six publicly\navailable Ô¨Åne-grained datasets in this section.\n4.3.1. CUB-200-2011\nWe compare the proposed method against many state-\nof-the-art Ô¨Åne-grained recognition models on CUB-200-\n2011, as shown in Table 4. The results show the following\nconclusions.\n(1) Overall, the proposed TPSKG performs better than\nthe state-of-the-art Ô¨Åne-grained methods, including the\n8\nglobal-discrimination approaches methods MaxEnt [46]\nand DB [28], and the part-based methods NTS-NET [20]\nand CPM [17].\n(2) Images with higher resolutions usually contain richer\ninformation and subtle details that are important for the\nÔ¨Åne-grained image recognition task. According to the lit-\nerature [61], higher resolution input images will produce\nbetter performance generally. Our method uses a smaller\nresolution than PMG [54], API-NET [29], and CPM [17]\nbut achieves a better performance. At the same time, our\nmethod is possible to perform better with higher resolu-\ntion.\n(3) CPM [17] has good performance using the stacked\nBiLSTMs to integrate the patch features. The perfor-\nmance of the original ViT method is equivalent to the\nCPM, which proves the eÔ¨Äectiveness of the transformer\nin feature aggregation and its potential in the Ô¨Åne-grained\nrecognition task.\n(4) Although the original ViT model has satisfactory\nperformance, we have improved it by 0.9%.\n(5) Both CIN [53] and API-NET [29] have achieved good\nresults based on the contrastive learning mechanism. It is\nworth noting that the improvement from our framework\nis orthogonal to those works, so the proposed TPSKG can\nalso beneÔ¨Åt from these methods.\nTable 4: Comparison results on CUB-200-2011 dataset.\nMethod Backbone Resolution Accuracy(%)\nMaxEnt [46] DenseNet-161 - 84.9\nMLA-CNN [51] VGG-19 448√ó448 85.7\nDVAN [4] VGG-16 224√ó224 87.1\nNTS-NET [20] ResNet-50 448√ó448 87.5\nCross-X [47] ResNet-50 448√ó448 87.7\nHGNet [57] ResNet-50 448√ó448 87.9\nCIN [53] ResNet-101 448√ó448 88.1\nMSEC [60] ResNet-50 448√ó448 88.3\nCDL [50] ResNet-50 448√ó448 88.4\nDB [28] ResNet-50 448√ó448 88.6\nHGNet [57] ResNet-50 448√ó448 88.7\nCTF-CapsNet [59] ResNet-50 448√ó448 88.9\nGHNS [48] ResNet-50 448√ó448 89.1\nFDL [22] DenseNet-161 448√ó448 89.1\nCSC-Net [49] ResNet-50 224√ó224 89.2\nSCAPNet [58] ResNet-50 224√ó224 89.5\nPMG [54] ResNet-50 550√ó550 89.6\nGaRD [55] ResNet-50 448√ó448 89.6\nSnapMix [56] ResNet-101 448√ó448 89.6\nCTF-CapsNet [59] ResNet-50 448√ó448 89.7\nAPI-NET [29] DenseNet-161 512√ó512 90.0\nCPM [17] GoogLeNet over 800 90.4\nViT-ResNet-50 ViT&ResNet-50448√ó448 89.2\nViT [13] ViT-B-16 448√ó448 90.4\nTPSKG ViT-B-16 448√ó448 91.3\n4.3.2. Stanford Dog\nAs can be seen from Table 5, our method shows more\nperformance improvement on the Stanford Dog dataset,\nwhich is 2.2% higher than the current state-of-the-art\nmethod API-NET [29] without using the contrastive learn-\ning mechanism and the high-resolution input. It is worth\nnoting that the performance of the original ViT also ex-\nceeds API-NET by 1.1%, which reÔ¨Çects that the trans-\nformer architecture can be well migrated to Ô¨Åne-grained\nrecognition task.\nTable 5: Comparison results on Stanford Dog dataset.\nMethod Backbone Resolution Accuracy(%)\nDVAN [4] VGG-16 224√ó224 81.5\nMaxEnt [46] DenseNet-161 - 83.6\nPC-CNN [62] DenseNet-161 224√ó224 83.8\nFDL [22] DenseNet-161 448√ó448 84.9\nMSEC [60] ResNet-50 448√ó448 85.6\nMLA-CNN [51] VGG-19 448√ó448 86.8\nDB [28] ResNet-50 448√ó448 87.7\nCross-X [47] ResNet-50 448√ó448 88.9\nAPI-NET [29] DenseNet-161 512√ó512 90.3\nViT-ResNet-50 ViT&ResNet-50 448√ó448 87.7\nViT [13] ViT-B-16 448√ó448 91.4\nTPSKG ViT-B-16 448√ó448 92.5\n4.3.3. Oxford 102 Flowers\nUnlike BiM-PMA [52], which uses all 2040 images in\nthe training set and validation set for training, we follow\nthe settings of PC-CNN [62] and PBC [63] and only use\n1020 images in training set for training to ensure a rel-\natively fair comparison. As can be seen from Table 6,\nalthough using fewer images, our method still achieves a\n2.1% performance improvement compared to BiM-PMA.\nAt the same time, our method still improves the recog-\nnition performance when the recognition performance of\nViT is excellent.\nTable 6: Comparison results on Oxford 102 Flowers dataset.\nMethod Backbone Resolution Accuracy(%)\nPC-CNN [62] DenseNet-161 224√ó224 93.6\nPBC [63] GoogleNet 224√ó224 96.1\nBiM-PMA [52] VGG-16 448√ó448 97.4\nViT-ResNet-50 ViT&ResNet-50 448√ó448 98.5\nViT [13] ViT-B-16 448√ó448 99.2\nTPSKG ViT-B-16 448√ó448 99.5\n4.3.4. NABirds\nThe NABirds dataset is a larger Ô¨Åne-grained dataset\nthan the CUB-200-2011 dataset containing 48,562 North\nAmerican bird images. Many methods with complex op-\nerations are not easy to experiment on a data set of this\norder of magnitude. If an image generates thousands of\nproposals, it means that tens of millions of proposals need\nto be processed. Table 7 reports the performance of sev-\neral methods on the NABirds dataset. DSTL [61] uses\nthe transfer learning strategy for the Ô¨Åne-grained image\nrecognition task consisting of more than one dataset. The\nresult of our method trained on a separate dataset exceeds\nDSTL by 2.2% in accuracy.\n9\nTable 7: Comparison results on NABirds dataset.\nMethod Backbone Resolution Accuracy(%)\nPC-CNN [62] DenseNet-161 224√ó224 82.8\nMaxEnt [46] DenseNet-161 - 83.0\nCross-X [47] ResNet-50 448√ó448 86.4\nHGNet [57] ResNet-50 448√ó448 86.4\nDSTL [61] Inception-v3 560√ó560 87.9\nGaRD [55] ResNet-50 448√ó448 88.0\nViT-ResNet-50 ViT&ResNet-50 448√ó448 86.7\nViT [13] ViT-B-16 448√ó448 89.6\nTPSKG ViT-B-16 448√ó448 90.1\n4.3.5. ISIA Food-200\nIn order to further verify the eÔ¨Äectiveness and explore\nthe scope of application of our method, we explored the\nfood recognition task on the ISIA Food-200 dataset. Un-\nlike other Ô¨Åne-grained objects, many types of food are non-\nrigid and lack a Ô¨Åxed spatial structure and semantic pat-\ntern. Therefore, it is challenging to capture speciÔ¨Åc seman-\ntic information from food images. Our method attempts\nto increase the diversity of representations to cover more\ndistinguished areas, which is more eÔ¨Äective for non-rigid\nobjects. Simultaneously, our method injects the extracted\nknowledge into the image-based representation so that a\nmore comprehensive understanding of food categories can\nbe used in the recognition task.\nTable 8 reports the performance of several methods on\nthe ISIA Food-200 dataset. The ViT is also mediocre on\nthis task. IG-CMAN [10] is a patch-based method sequen-\ntially localizing multiple informative image regions with\nmulti-scale from category level to ingredient-level guidance\nin a coarse-to-Ô¨Åne manner. Our method achieves the best\n69.5% without using the multi-scale strategy and outper-\nforms the state-of-the-art method IG-CMAN by 2.0% in\naccuracy. This result proves that our method has a more\nsigniÔ¨Åcant performance improvement in complex recogni-\ntion problems.\nTable 8: Comparison results on ISIA Food-200 dataset.\nMethod Backbone Resolution Accuracy(%)\nResNet-152 ResNet-152 224√ó224 61.1\nDenseNet-161 DenseNet-161 224√ó224 62.6\nIG-CMAN [10] DenseNet-161 224√ó224 67.5\nViT-ResNet-50 ViT&ResNet-50 448√ó448 62.5\nViT [13] ViT-B-16 448√ó448 67.4\nTPSKG ViT-B-16 448√ó448 69.5\n4.3.6. ISIA Food-500\nThe ISIA Food-500 is a more comprehensive food\ndataset than the ISIA Food-200 with a larger data volume\nand higher diversity. We evaluated the proposed TPSKG\nagainst diÔ¨Äerent Ô¨Åne-grained methods in Table 9. The per-\nformance of ViT-ResNet-50 and ViT show a pronounced\ndecline. The possible reason is that the volume and com-\nplexity of the ISIA Food-500 dataset are much higher than\nthat of the ISIA Food-200 dataset. This increase in com-\nplexity makes the impact of the loss of local region seman-\ntics more signiÔ¨Åcant. It can also be seen that the pro-\nposed method exceeds the original ViT signiÔ¨Åcantly, with\na gain of 5.5% in accuracy. When the performance of the\noriginal ViT is poor, our method still achieves competi-\ntive performance compared to the state-of-the-art method,\nwhich proves that our method does not rely heavily on the\nperformance of ViT. The proposed method obtains bet-\nter accuracy than the SGLANet without a complicated\nmulti-scale mechanism and spatial-channel attention. We\nwill try to leverage the multi-scale information to improve\nperformance in future work.\nTable 9: Comparison results on ISIA Food-500 dataset.\nMethod Backbone Resolution Accuracy(%)\nResNet-152 ResNet-152 224√ó224 57.0\nWRN-50 [64] WRN-50 224√ó224 60.1\nWS-DAN [65] Inception-v3 299√ó299 60.7\nNAS-NET [66] ResNet-152 331√ó331 60.7\nNTS-NET [20] ResNet-152 448√ó448 63.7\nSENet-154 SENet-154 224√ó224 63.8\nDCL [27] ResNet-152 448√ó448 64.1\nSGLANet [45] SENet-154 224√ó224 64.7\nViT-ResNet-50 ViT&ResNet-50 448√ó448 52.7\nViT [13] ViT-B-16 448√ó448 59.9\nTPSKG ViT-B-16 448√ó448 65.4\n4.3.7. Overall\nWe summarize the results of all datasets to obtain an\noverall understanding of the proposed method. We can\nÔ¨Ånd that (1) The recognition results of the diÔ¨Äerent meth-\nods for the six diÔ¨Äerent datasets show a very high degree\nof correspondence, indicating the strong reproducibility.\n(2) The performance of the hybrid model directly gen-\nerated by the simple combination of ViT and ResNet-50\ngenerally performs poorly on Ô¨Åne-grained image recogni-\ntion task and even has performance degradation compared\nto the ViT model. A possible explanation for these re-\nsults may be the lack of adequate semantic information\nin small regions. (3) The ViT model is generally suit-\nable for simple Ô¨Åne-grained recognition tasks and obtains\nclose to state-of-the-art results on multiple datasets, but it\ndoes not perform well for more complex food recognition\ntasks. (4) The proposed PS module and KG module have\neÔ¨Äectively improved the recognition performance, and the\nproposed method has achieved very superior performance\non all datasets. (5) The KG module improves the model\nperformance more signiÔ¨Åcantly than the PS module. The\npossible explanation for this might be that the beneÔ¨Åts of\nintegrating discriminative information in multiple images\nare more signiÔ¨Åcant than the coverage of more discrimina-\ntive information areas in one single image.\n4.4. Qualitative Visualization\nIn order to show the eÔ¨Äectiveness of the method more\nintuitively, we visualize the attention maps of the original\n10\nCUB-200-2011\n Stanford Dog ISIA Food-200\n ISIA Food-500\nNABirds\nOxford Flowers\nFigure 6: The attention map comparison between our method and the baseline in diÔ¨Äerent datasets. Top to below: original image, attention\nmap of the ViT, attention map of our method. The yellow means high weights and the blue means relatively low weights.\nCUB-200-2011 Stanford Dog ISIA Food-200 ISIA Food-500NABirdsOxford Flowers\n90.4%\n91.3%\n91.4%\n92.5%\n99.2%\n99.5%\n89.6%\n90.1%\n67.4%\n69.5%\n59.9%\n65.4%\nFigure 7: The t-SNE visualization of Ô¨Åne-grained image feature representations of (top row) before injecting the knowledge embedding,\n(bottom row) after injecting the knowledge embedding on the six Ô¨Åne-grained datasets. Each color represents a diÔ¨Äerent class. The upper\nright corner shows the accuracy of the corresponding method.\nViT and TPSKG models for sample images from six diÔ¨Äer-\nent datasets. As shown in Fig. 6, we Ô¨Ånd that although the\noriginal ViT can also perform localization and recognition,\nour method is better in both aspects. The attention map\nof the proposed TPSKG can not only locate the essential\nparts well but also cover more discriminative areas, which\nshows that the method is more robust than the original\nViT. For the CUB-200-2011, Stanford Dog, Oxford Flow-\ners and NABirds datasets with relatively simple scenes,\nfeatures without the diversity can complete the recogni-\ntion task. For relatively complex food recognition tasks\non the ISIA Food-200 and ISIA Food-500 datasets, the\ndiversity of features is more important, and our method\nhas improved more obviously, which is consistent with the\nresults of quantitative analysis.\nTo visually analyze the inÔ¨Çuence of the knowledge guid-\nance module, we visualize the feature representations be-\nfore/after injecting the knowledge embedding by employ-\ning t-SNE, on the six Ô¨Åne-grained image datasets as shown\nin Fig. 7. The visualized data includes all test set im-\nages of CUB-200-2011, Stanford Dogs, Oxford Flowers,\nNAbirds, ISIA Food-200 datasets and 50 sample categories\nfrom the ISIA Food-500 dataset due to excessive data vol-\nume. Since the classiÔ¨Åcation performance of the Ô¨Årst four\ndatasets is very high, the visualized images are not much\ndiÔ¨Äerent. But the latter two more complex food recog-\nnition datasets show signiÔ¨Åcant diÔ¨Äerences. The visual-\nization results of food recognition datasets in this Ô¨Ågure\nshow obvious intra-class clustering. This proves that the\nproposed KG module has a strong intra-class aggregation\nability. Although the t-SNE method cannot prove the\nmethod‚Äôs ability for the recognition task, it can be seen\nfrom the local structure that our method has a more vital\nability to aggregate features within the class. This inves-\ntigation conÔ¨Årms that feature representations will get into\nmore separable clusters after injecting the category-related\nknowledge embedding.\nIn addition, we further show the confusion matrix of our\n11\nFriednoodles\nMie_goreng\nPorkknuckle\nSchweinshaxe\nPork_ball Meatball\nChow_mein\nFriednoodles\nOlive SidedFlycatcher\nCaliforniaGull\nWesternGull\nArtic Tern\nCommonTern\nWesternWoodPewee\nÔºàaÔºâ ÔºàbÔºâ\nFigure 8: Confusion matrix of our method on the (a) CUB-200-2011 and (b) ISIA Food-200 datasets. Some instances of low recognition rate\ncategories are annotated by red boxes.\nmethod on the CUB-200-2011 and ISIA Food-200 datasets\nin Fig. 8, where the vertical axis shows the ground-truth\nclasses, and the horizontal axis shows the predicted classes.\nYellower colors indicate better performance. We can see\nthat our method still does not provide perfect performance\nfor some bird and food categories. We enlarge speciÔ¨Åc re-\ngions to highlight the misclassiÔ¨Åed results and show some\nsamples with low recognition rates. As shown in Fig. 8 (a),\nsome birds of the same meta-category are extremely dif-\nÔ¨Åcult to distinguish, such as California Gull and Western\nGull, Artic Tern and Common Tern. There are also images\nwith similar poses that cannot be recognized well, such as\nOlive Sided Flycatcher and Wester Wood Pewee, requir-\ning further study and exploration. From Fig. 8 (b) we can\nsee that these food categories are very similar in visual\nappearances, such as Chow mein, Mie Goreng, and Fried\nnoodles. Even humans cannot easily distinguish these food\ncategories based on images. Some food categories have the\nsame ingredients and diÔ¨Äerent cooking techniques, which\nare diÔ¨Écult to distinguish, such as pork knuckle and the\nSchweinshaxe. Many types of food are diÔ¨Écult to classify\nbased on images alone. A possible solution is to com-\nbine multiple media formats of information for the recog-\nnition task, such as ingredient lists and cooking processes.\nThe transformer architecture has promising applications in\nmultimedia, including the visual Ô¨Åeld and the NLP Ô¨Åeld,\nand provides the possibility for a uniÔ¨Åed framework.\n5. Conclusion\nFine-grained image recognition is an interesting and fun-\ndamental topic. In this paper, we investigate the prob-\nlem of Ô¨Åne-grained image recognition from the perspective\nof fragmented information integration. Furthermore, we\npresent a transformer with peak suppression and knowl-\nedge guidance (TPSKG) for the Ô¨Åne-grained image recog-\nnition task. Our method learns the diverse Ô¨Åne-grained\nrepresentations by the peak suppression module penalizing\nthe most discriminative parts. It then learns the knowl-\nedge embedding including a large number of discriminative\nclues for diÔ¨Äerent images of the same category, and injects\nthem into Ô¨Åne-grained representations leading to signiÔ¨Å-\ncantly higher recognition performance. The proposed net-\nwork can be trained end-to-end in one stage, requiring no\nbounding box/part annotations. Qualitative and quantita-\ntive evaluations on six public Ô¨Åne-grained datasets demon-\nstrate that the proposed TPSKG can achieve competitive\nperformance compared to the state-of-the-art approaches.\nReferences\nReferences\n[1] K. Han, J. Guo, C. Zhang, M. Zhu, Attribute-aware attention\nmodel for Ô¨Åne-grained representation learning, in: International\nConference on Multimedia, ACM, 2018, pp. 2040‚Äì2048.\n[2] X. He, Y. Peng, Only learn one sample: Fine-grained visual\ncategorization with one sample training, in: International Con-\nference on Multimedia, ACM, 2018, pp. 1372‚Äì1380.\n12\n[3] J. Li, L. Zhu, Z. Huang, K. Lu, J. Zhao, I read, I saw, I tell:\nTexts assisted Ô¨Åne-grained visual classiÔ¨Åcation, in: International\nConference on Multimedia, ACM, 2018, pp. 663‚Äì671.\n[4] B. Zhao, X. Wu, J. Feng, Q. Peng, S. Yan, DiversiÔ¨Åed visual\nattention networks for Ô¨Åne-grained object classiÔ¨Åcation, IEEE\nTransactions on Multimedia 19 (6) (2017) 1245‚Äì1256.\n[5] V. M. Ara¬¥ ujo, A. S. Britto Jr., L. S. Oliveira, A. L. Koerich,\nTwo-view Ô¨Åne-grained classiÔ¨Åcation of plant species, Neurocom-\nputing 467 (2022) 427‚Äì441.\n[6] L. A. Hendricks, S. Venugopalan, M. Rohrbach, R. J. Mooney,\nK. Saenko, T. Darrell, Deep compositional captioning: De-\nscribing novel object categories without paired training data,\nin: Conference on Computer Vision and Pattern Recognition,\nIEEE, 2016, pp. 1‚Äì10.\n[7] J. Bao, D. Chen, F. Wen, H. Li, G. Hua, CVAE-GAN: Ô¨Åne-\ngrained image generation through asymmetric training, in: In-\nternational Conference on Computer Vision, IEEE, 2017, pp.\n2764‚Äì2773.\n[8] O. M. Aodha, S. Su, Y. Chen, P. Perona, Y. Yue, Teaching\ncategories to human learners with visual explanations, in: Con-\nference on Computer Vision and Pattern Recognition, IEEE,\n2018, pp. 3820‚Äì3828.\n[9] K. Pang, Y. Yang, T. M. Hospedales, T. Xiang, Y. Song, Solv-\ning mixed-modal jigsaw puzzle for Ô¨Åne-grained sketch-based im-\nage retrieval, in: Conference on Computer Vision and Pattern\nRecognition, IEEE, 2020, pp. 10344‚Äì10352.\n[10] W. Min, L. Liu, Z. Luo, S. Jiang, Ingredient-guided cascaded\nmulti-attention network for food recognition, in: International\nConference on Multimedia, ACM, 2019, pp. 1331‚Äì1339.\n[11] W. Min, S. Jiang, J. Sang, H. Wang, X. Liu, L. Herranz, Being a\nsupercook: Joint food attributes and multimodal content mod-\neling for recipe retrieval and exploration, IEEE Transactions on\nMultimedia 19 (5) (2017) 1100‚Äì1113.\n[12] W. Min, S. Jiang, R. C. Jain, Food recommendation: Frame-\nwork, existing solutions, and challenges, IEEE Transactions on\nMultimedia 22 (10) (2020) 2659‚Äì2671.\n[13] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold,\nS. Gelly, J. Uszkoreit, N. Houlsby, An image is worth 16x16\nwords: Transformers for image recognition at scale, in: Inter-\nnational Conference on Learning Representations, 2021.\n[14] S. Huang, Z. Xu, D. Tao, Y. Zhang, Part-stacked CNN for Ô¨Åne-\ngrained visual categorization, in: Conference on Computer Vi-\nsion and Pattern Recognition, IEEE, 2016, pp. 1173‚Äì1182.\n[15] J. Donahue, Y. Jia, O. Vinyals, J. HoÔ¨Äman, N. Zhang, E. Tzeng,\nT. Darrell, Decaf: A deep convolutional activation feature for\ngeneric visual recognition, in: International Conference on Ma-\nchine Learning, Vol. 32, JMLR, 2014, pp. 647‚Äì655.\n[16] W. Min, S. Jiang, L. Liu, Y. Rui, R. C. Jain, A survey on food\ncomputing, ACM Computing Surveys 52 (5) (2019) 92:1‚Äì92:36.\n[17] W. Ge, X. Lin, Y. Yu, Weakly supervised complementary parts\nmodels for Ô¨Åne-grained image classiÔ¨Åcation from the bottom up,\nin: Conference on Computer Vision and Pattern Recognition,\nIEEE, 2019, pp. 3034‚Äì3043.\n[18] S. Jiang, W. Min, L. Liu, Z. Luo, Multi-scale multi-view deep\nfeature aggregation for food recognition, IEEE Transactions on\nImage Processing 29 (2020) 265‚Äì276.\n[19] J. Wang, N. Li, Z. Luo, Z. Zhong, S. Li, High-order-interaction\nfor weakly supervised Ô¨Åne-grained visual categorization, Neuro-\ncomputing 464 (2021) 27‚Äì36.\n[20] Z. Yang, T. Luo, D. Wang, Z. Hu, J. Gao, L. Wang, Learning to\nnavigate for Ô¨Åne-grained classiÔ¨Åcation, in: European Conference\non Computer Vision, Vol. 11218, Springer, 2018, pp. 438‚Äì454.\n[21] X. Liu, W. Min, S. Mei, L. Wang, S. Jiang, Plant disease recog-\nnition: A large-scale benchmark dataset and a visual region\nand loss reweighting approach, IEEE Transactions on Image\nProcessing 30 (2021) 2003‚Äì2015.\n[22] C. Liu, H. Xie, Z. Zha, L. Ma, L. Yu, Y. Zhang, Filtration and\ndistillation: Enhancing region attention for Ô¨Åne-grained visual\ncategorization, in: The AAAI Conference on ArtiÔ¨Åcial Intelli-\ngence, AAAI, 2020, pp. 11555‚Äì11562.\n[23] Y. Peng, X. He, J. Zhao, Object-part attention model for Ô¨Åne-\ngrained image classiÔ¨Åcation, IEEE Transactions on Image Pro-\ncessing 27 (3) (2018) 1487‚Äì1500.\n[24] H. Zheng, J. Fu, Z. Zha, J. Luo, T. Mei, Learning rich part\nhierarchies with progressive attention networks for Ô¨Åne-grained\nimage recognition, IEEE Transactions on Image Processing 29\n(2020) 476‚Äì488.\n[25] Z. Wang, S. Wang, S. Yang, H. Li, J. Li, Z. Li, Weakly su-\npervised Ô¨Åne-grained image classiÔ¨Åcation via guassian mixture\nmodel oriented discriminative learning, in: Conference on Com-\nputer Vision and Pattern Recognition, IEEE, 2020, pp. 9746‚Äì\n9755.\n[26] T. Lin, A. RoyChowdhury, S. Maji, Bilinear convolutional neu-\nral networks for Ô¨Åne-grained visual recognition, IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence 40 (6) (2018)\n1309‚Äì1322.\n[27] Y. Chen, Y. Bai, W. Zhang, T. Mei, Destruction and construc-\ntion learning for Ô¨Åne-grained image recognition, in: Conference\non Computer Vision and Pattern Recognition, IEEE, 2019, pp.\n5157‚Äì5166.\n[28] G. Sun, H. Cholakkal, S. Khan, F. S. Khan, L. Shao, Fine-\ngrained recognition: Accounting for subtle diÔ¨Äerences between\nsimilar classes, in: The AAAI Conference on ArtiÔ¨Åcial Intelli-\ngence, AAAI, 2020, pp. 12047‚Äì12054.\n[29] P. Zhuang, Y. Wang, Y. Qiao, Learning attentive pairwise inter-\naction for Ô¨Åne-grained classiÔ¨Åcation, in: The AAAI Conference\non ArtiÔ¨Åcial Intelligence, AAAI, 2020, pp. 13130‚Äì13137.\n[30] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, I. Polosukhin, Attention is all you need, in:\nConference on Neural Information Processing Systems, 2017,\npp. 5998‚Äì6008.\n[31] J. Devlin, M. Chang, K. Lee, K. Toutanova, BERT: pre-training\nof deep bidirectional transformers for language understanding,\nin: Conference of the Association for Computational Linguis-\ntics, Association for Computational Linguistics, 2019.\n[32] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan,\nP. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell,\nS. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan,\nR. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse,\nM. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark,\nC. Berner, S. McCandlish, A. Radford, I. Sutskever, D. Amodei,\nLanguage models are few-shot learners, in: Conference on Neu-\nral Information Processing Systems, 2020.\n[33] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles,\nH. J¬¥ egou, Training data-eÔ¨Écient image transformers & distilla-\ntion through attention, CoRR.\n[34] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov,\nS. Zagoruyko, End-to-end object detection with transformers,\nin: European Conference on Computer Vision, Vol. 12346,\nSpringer, 2020, pp. 213‚Äì229.\n[35] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, J. Dai, Deformable\nDETR: deformable transformers for end-to-end object detec-\ntion, CoRR.\n[36] L. Ye, M. Rochan, Z. Liu, Y. Wang, Cross-modal self-attention\nnetwork for referring image segmentation, in: Conference on\nComputer Vision and Pattern Recognition, IEEE, 2019, pp.\n10502‚Äì10511.\n[37] F. Yang, H. Yang, J. Fu, H. Lu, B. Guo, Learning texture\ntransformer network for image super-resolution, in: Conference\non Computer Vision and Pattern Recognition, 2020, pp. 5791‚Äì\n5800.\n[38] J. Cordonnier, A. Loukas, M. Jaggi, On the relationship be-\ntween self-attention and convolutional layers, in: International\nConference on Learning Representations, 2020.\n[39] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan,\nI. Sutskever, Generative pretraining from pixels, in: Interna-\ntional Conference on Machine Learning, Vol. 119, PMLR, 2020,\npp. 1691‚Äì1703.\n[40] S. Abnar, W. H. Zuidema, Quantifying attention Ô¨Çow in trans-\nformers, in: Association for Computational Linguistics, Associ-\nation for Computational Linguistics, 2020, pp. 4190‚Äì4197.\n13\n[41] C. Wah, S. Branson, P. Welinder, P. Perona, S. Belongie, The\nCaltech-UCSD Birds-200-2011 Dataset, Tech. Rep. CNS-TR-\n2011-001, California Institute of Technology (2011).\n[42] G. V. Horn, S. Branson, R. Farrell, S. Haber, J. Barry, P. Ipeiro-\ntis, P. Perona, S. J. Belongie, Building a bird recognition app\nand large scale dataset with citizen scientists: The Ô¨Åne print\nin Ô¨Åne-grained dataset collection, in: Conference on Computer\nVision and Pattern Recognition, IEEE, 2015, pp. 595‚Äì604.\n[43] M. Nilsback, A. Zisserman, Automated Ô¨Çower classiÔ¨Åcation over\na large number of classes, in: Indian Conference on Computer\nVision, Graphics & Image Processing,, IEEE, 2008, pp. 722‚Äì\n729.\n[44] A. Khosla, N. Jayadevaprakash, B. Yao, L. Fei-Fei, Novel\ndataset for Ô¨Åne-grained image categorization, in: Conference\non Computer Vision and Pattern Recognition, IEEE, 2011.\n[45] W. Min, L. Liu, Z. Wang, Z. Luo, X. Wei, X. Wei, S. Jiang, ISIA\nFood-500: A dataset for large-scale food recognition via stacked\nglobal-local attention network, in: International Conference on\nMultimedia, ACM, 2020.\n[46] A. Dubey, O. Gupta, R. Raskar, N. Naik, Maximum-entropy\nÔ¨Åne grained classiÔ¨Åcation, in: Conference on Neural Information\nProcessing Systems, 2018, pp. 635‚Äì645.\n[47] W. Luo, X. Yang, X. Mo, Y. Lu, L. Davis, J. Li, J. Yang,\nS. Lim, Cross-x learning for Ô¨Åne-grained visual categorization,\nin: International Conference on Computer Vision, IEEE, 2019,\npp. 8241‚Äì8250.\n[48] T. Kim, K. Hong, H. Byun, The feature generator of hard neg-\native samples for Ô¨Åne-grained image recognition, Neurocomput-\ning 439 (2021) 374‚Äì382.\n[49] S. Wang, Z. Wang, H. Li, W. Ouyang, Category-speciÔ¨Åc se-\nmantic coherency learning for Ô¨Åne-grained image recognition,\nin: International Conference on Multimedia, ACM, 2020, pp.\n174‚Äì183.\n[50] Z. Wang, S. Wang, P. Zhang, H. Li, W. Zhong, J. Li, Weakly su-\npervised Ô¨Åne-grained image classiÔ¨Åcation via correlation-guided\ndiscriminative learning, in: International Conference on Multi-\nmedia, ACM, 2019, pp. 1851‚Äì1860.\n[51] J. Ji, Y. Guo, Z. Yang, T. Zhang, X. Lu, Multi-level dictionary\nlearning for Ô¨Åne-grained images categorization with attention\nmodel, Neurocomputing 453 (2021) 403‚Äì412.\n[52] K. Song, X. Wei, X. Shu, R. Song, J. Lu, Bi-modal progressive\nmask attention for Ô¨Åne-grained recognition, IEEE Transactions\non Image Processing 29 (2020) 7006‚Äì7018.\n[53] Y. Gao, X. Han, X. Wang, W. Huang, M. R. Scott, Channel\ninteraction networks for Ô¨Åne-grained image categorization, in:\nThe AAAI Conference on ArtiÔ¨Åcial Intelligence, AAAI, 2020,\npp. 10818‚Äì10825.\n[54] R. Du, D. Chang, A. K. Bhunia, J. Xie, Z. Ma, Y. Song,\nJ. Guo, Fine-grained visual classiÔ¨Åcation via progressive multi-\ngranularity training of jigsaw patches, in: European Conference\non Computer Vision, Vol. 12365, Springer, 2020, pp. 153‚Äì168.\n[55] Y. Zhao, K. Yan, F. Huang, J. Li, Graph-based high-order re-\nlation discovery for Ô¨Åne-grained recognition, in: Conference on\nComputer Vision and Pattern Recognition, 2021, pp. 15079‚Äì\n15088.\n[56] S. Huang, X. Wang, D. Tao, Snapmix: Semantically propor-\ntional mixing for augmenting Ô¨Åne-grained data, in: The AAAI\nConference on ArtiÔ¨Åcial Intelligence, AAAI, 2021, pp. 1628‚Äì\n1636.\n[57] Y. Chen, J. Song, M. Song, Hierarchical gate network for Ô¨Åne-\ngrained visual recognition, Neurocomputing.\n[58] H. Liu, J. Li, D. Li, J. See, W. Lin, Learning scale-consistent\nattention part network for Ô¨Åne-grained image recognition, IEEE\nTransactions on Multimedia (2021) 1‚Äì1.\n[59] Z. Lin, J. Jia, F. Huang, W. Gao, A coarse-to-Ô¨Åne capsule net-\nwork for Ô¨Åne-grained image categorization, Neurocomputing 456\n(2021) 200‚Äì219.\n[60] Y. Zhang, Y. Sun, N. Wang, Z. Gao, F. Chen, C. Wang, J. Tang,\nMSEC: multi-scale erasure and confusion for Ô¨Åne-grained image\nclassiÔ¨Åcation, Neurocomputing 449 (2021) 1‚Äì14.\n[61] Y. Cui, Y. Song, C. Sun, A. Howard, S. J. Belongie, Large scale\nÔ¨Åne-grained categorization and domain-speciÔ¨Åc transfer learn-\ning, in: Conference on Computer Vision and Pattern Recogni-\ntion, IEEE, 2018, pp. 4109‚Äì4118.\n[62] A. Dubey, O. Gupta, P. Guo, R. Raskar, R. Farrell, N. Naik,\nPairwise confusion for Ô¨Åne-grained visual classiÔ¨Åcation, in: Eu-\nropean Conference on Computer Vision, Vol. 11216, Springer,\n2018, pp. 71‚Äì88.\n[63] C. Huang, H. Li, Y. Xie, Q. Wu, B. Luo, PBC: polygon-based\nclassiÔ¨Åer for Ô¨Åne-grained categorization, IEEE Transactions on\nMultimedia 19 (4) (2017) 673‚Äì684.\n[64] S. Zagoruyko, N. Komodakis, Wide residual networks, in:\nBritish Machine Vision Conference, BMVA, 2016.\n[65] T. Hu, H. Qi, See better before looking closer: Weakly super-\nvised data augmentation network for Ô¨Åne-grained visual classi-\nÔ¨Åcation, CoRR abs/1901.09891.\n[66] B. Zoph, V. Vasudevan, J. Shlens, Q. V. Le, Learning transfer-\nable architectures for scalable image recognition, in: Conference\non Computer Vision and Pattern Recognition, IEEE, 2018, pp.\n8697‚Äì8710.\n14",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6632422208786011
    },
    {
      "name": "Transformer",
      "score": 0.5753780603408813
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5459439754486084
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.42857038974761963
    },
    {
      "name": "Computer vision",
      "score": 0.4073472321033478
    },
    {
      "name": "Electrical engineering",
      "score": 0.11709779500961304
    },
    {
      "name": "Voltage",
      "score": 0.09060195088386536
    },
    {
      "name": "Engineering",
      "score": 0.0778435468673706
    }
  ]
}