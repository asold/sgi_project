{
  "title": "Perceptual Image Quality Assessment with Transformers",
  "url": "https://openalex.org/W3157455167",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2747449766",
      "name": "Cheon, Manri",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5027453967",
      "name": "Yoon Sung-Jun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4295151259",
      "name": "Kang, Byungyeon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2288768874",
      "name": "Lee Junwoo",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3109319753",
    "https://openalex.org/W3174760001",
    "https://openalex.org/W2141983208",
    "https://openalex.org/W2274287116",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W3017136408",
    "https://openalex.org/W2953590133",
    "https://openalex.org/W2102166818",
    "https://openalex.org/W2142884912",
    "https://openalex.org/W3132890542",
    "https://openalex.org/W2171349048",
    "https://openalex.org/W2037243393",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2067921844",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2015196405",
    "https://openalex.org/W2964060609",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3022710784",
    "https://openalex.org/W3124079390",
    "https://openalex.org/W2046119925",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2891158090",
    "https://openalex.org/W2962785568",
    "https://openalex.org/W2144468361",
    "https://openalex.org/W2798581339",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2982074157",
    "https://openalex.org/W2009272644",
    "https://openalex.org/W3035595647",
    "https://openalex.org/W3014734196",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2472413205",
    "https://openalex.org/W3194293177",
    "https://openalex.org/W2596816328",
    "https://openalex.org/W1974013408",
    "https://openalex.org/W1580389772",
    "https://openalex.org/W3123981332",
    "https://openalex.org/W3091249416",
    "https://openalex.org/W3119997354",
    "https://openalex.org/W2152059677",
    "https://openalex.org/W2571611310",
    "https://openalex.org/W2891761377",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W1964859077",
    "https://openalex.org/W3106603783",
    "https://openalex.org/W2159269332",
    "https://openalex.org/W2565312867",
    "https://openalex.org/W1972006393",
    "https://openalex.org/W2161907179",
    "https://openalex.org/W3109016160",
    "https://openalex.org/W1992869494",
    "https://openalex.org/W2132549992"
  ],
  "abstract": "In this paper, we propose an image quality transformer (IQT) that successfully applies a transformer architecture to a perceptual full-reference image quality assessment (IQA) task. Perceptual representation becomes more important in image quality assessment. In this context, we extract the perceptual feature representations from each of input images using a convolutional neural network (CNN) backbone. The extracted feature maps are fed into the transformer encoder and decoder in order to compare a reference and distorted images. Following an approach of the transformer-based vision models, we use extra learnable quality embedding and position embedding. The output of the transformer is passed to a prediction head in order to predict a final quality score. The experimental results show that our proposed model has an outstanding performance for the standard IQA datasets. For a large-scale IQA dataset containing output images of generative model, our model also shows the promising results. The proposed IQT was ranked first among 13 participants in the NTIRE 2021 perceptual image quality assessment challenge. Our work will be an opportunity to further expand the approach for the perceptual IQA task.",
  "full_text": "Perceptual Image Quality Assessment with Transformers\nManri Cheon, Sung-Jun Yoon, Byungyeon Kang, and Junwoo Lee\nLG Electronics\n{manri.cheon, sungjun.yoon, byungyeon.kang, junwoo.lee}@lge.com\nAbstract\nIn this paper, we propose an image quality transformer\n(IQT) that successfully applies a transformer architecture to\na perceptual full-reference image quality assessment (IQA)\ntask. Perceptual representation becomes more important in\nimage quality assessment. In this context, we extract the\nperceptual feature representations from each of input im-\nages using a convolutional neural network (CNN) back-\nbone. The extracted feature maps are fed into the trans-\nformer encoder and decoder in order to compare a refer-\nence and distorted images. Following an approach of the\ntransformer-based vision models [18, 55], we use extra\nlearnable quality embedding and position embedding. The\noutput of the transformer is passed to a prediction head in\norder to predict a ﬁnal quality score. The experimental re-\nsults show that our proposed model has an outstanding per-\nformance for the standard IQA datasets. For a large-scale\nIQA dataset containing output images of generative model,\nour model also shows the promising results. The proposed\nIQT was ranked ﬁrst among 13 participants in the NTIRE\n2021 perceptual image quality assessment challenge [23].\nOur work will be an opportunity to further expand the ap-\nproach for the perceptual IQA task.\n1. Introduction\nPerceptual image quality assessment (IQA) is an impor-\ntant topic in the multimedia systems and computer vision\ntasks [11, 42, 56]. One of the goals of the image process-\ning is to improve the quality of the content to an accept-\nable level for the human viewers. In this context, the ﬁrst\nstep toward generating acceptable contents is to accurately\nmeasure the perceptual quality of the content, which can\nbe performed via subjective and objective quality assess-\nment [49, 9, 25, 19]. The subjective quality assessment is\nthe most accurate method to measure the perceived qual-\nity, which is usually represented by mean opinion scores\n(MOS) from collected subjective ratings. However, it is\ntime-consuming and expensive. Thus, objective quality as-\nsessment performed by objective metrics is widely used to\nautomatically predict perceived quality [51, 52, 40, 59, 53].\nHowever, with the recent advances in deep learning-\nbased image restoration algorithms, accurate prediction of\nthe perceived quality has become more difﬁcult. In par-\nticular, image restoration models based on generative ad-\nversarial network (GAN) [20] have been developed in or-\nder to improve the perceptual aspect of the result images\n[48, 2, 12, 8]. However, it sometimes generates output im-\nages with unrealistic artifacts. The existing objective met-\nrics such as Peak Signal-to-Noise Ratio (PSNR), a structural\nsimilarity index (SSIM) [51], and conventional quality met-\nrics are insufﬁcient to predict the quality of this kind of out-\nputs. In this respect, recent works [61, 16, 21, 39, 3] based\non perceptual representation exhibit a better performance at\nthe perceptual IQA task. As various image restoration algo-\nrithms are developed, however, it is still required to develop\nthe IQA algorithm that accurately predicts the perceptual\nquality of images generated by emerging algorithms.\nIn recent years, based on the success in the natural lan-\nguage processing (NLP) ﬁeld, the transformer [46] archi-\ntecture has been applied in the computer vision ﬁeld [27].\nA wider research area in the computer vision has been im-\nproved based on the transformer, such as recognition task\n[4, 45, 18], generative modelling [37, 26, 7], low-level vi-\nsion [6, 54, 30], etc. However, few attempts were made\nin the ﬁeld of the image and video quality assessment. In\na recent study, You and Korhonen proposed the applica-\ntion of transformer in image quality assessment [55]. They\nachieved outstanding performance on two publicly available\nlarge-scale blind image quality databases. With our knowl-\nedge, however, this study is the only transformer-based ap-\nproach for image quality assessment. Therefore, it is ur-\ngently needed to investigate whether the transformer-based\napproach works well in the ﬁeld of perceptual image qual-\nity assessment. Especially, it should be investigated whether\nthis structure is applicable to a full-reference (FR) model\naiming to measure the perceptual similarity between two\nimages. In addition, it is also necessary to evaluate whether\nthis approach can accurately predict the perceptual quality\nfor the latest GAN-based artifacts.\nIn this study, we propose an Image Quality Transformer\narXiv:2104.14730v2  [cs.CV]  5 May 2021\nFigure 1. Model architecture of proposed image quality transformer (IQT). Note that FI denotes Fd and Fr in Eqs. 1 and 2, respectively.\n(IQT), which is the FR image quality assessment method as\nshown in Fig. 1. To tackle the perceptual aspects, a convo-\nlutional neural network (CNN) backbone is used to extract\nperceptual representations from an input image. Based on\nthe transformer encoder-decoder architecture, the proposed\nmodel is trained to predict the perceptual quality accurately.\nThe proposed model was ranked in the ﬁrst place among 13\nparticipants in the NTIRE 2021 challenge on perceptual im-\nage quality assessment [23] at the CVPR 2021.\nThe rest of this article is organized as follows. The fol-\nlowing section presents the related work. Section 3 de-\nscribes the proposed method and the experiments are given\nin Section 4. Finally, conclusions are given in Section 5.\n2. Related Work\nImage Quality Assessment. The most important goal of\nthe developing objective IQA is to accurately predict the\nperceived quality by human viewers. In general, the objec-\ntive IQA methods can be classiﬁed into three categories\naccording to the existence of reference information: FR\n[51, 52, 40, 32, 5], reduced-reference (RR) [43], and no-\nreference (NR) [36, 35] IQA methods. The NR method is\nuseful for the system because of its feasibility. However, the\nabsence of a reference makes it challenging to predict im-\nage quality accurately compared to the FR method. The FR\nmethod focuses more on visual similarity or dissimilarity\nbetween two images, and this method still plays an impor-\ntant role in the development of image processing system.\nThe representative of the commonly and widely used\nquality FR metric is the PSNR. It has the advantage of con-\nvenience for optimization; however, it tends to poorly pre-\ndict perceived visual quality. Wanget al. proposed the SSIM\n[51] that is based on the fact that the human visual system\n(HVS) is highly correlated to structural information. Since\nthat, various FR metrics have been developed to take into\naccount various aspects of human quality perception, e.g.,\ninformation-theoretic criterion [40, 41], structural similar-\nity [52, 59], etc. Recently, CNN-based IQA methods as well\nas other low-level computer vision tasks have been actively\nstudied [61, 3, 39, 17, 24]. Zhang et al. proposed a learned\nperceptual image patch similarity (LPIPS) metric [61] for\nFR-IQA. The LPIPS showed that trained deep features that\nare optimized by the Euclidean distance between distorted\nand reference images are effective for IQA compared to the\nconventional IQA methods. Ding et al. proposed the metric\nthat is robust to texture resampling and geometric transfor-\nmation based on spatial averages of the feature maps [16].\nVarious IQA methods including aforementioned metrics are\nincluded in our experiments for performance comparison.\nThe primary criterion of performance measurement is\nthe accuracy of the metrics. Pearson linear correlation coef-\nﬁcient (PLCC) followed by the third-order polynomial non-\nlinear regression [42] is usually used in order to evaluate\nthe accuracy of the methods. Spearman rank order correla-\ntion coefﬁcient (SRCC) and the Kendall rank order corre-\nlation coefﬁcient (KRCC) are used to estimate the mono-\ntonicity and consistency of the quality prediction. Addi-\ntional statistical method [29] and an ambiguity based ap-\nproach have also been proposed in [10]. In our study, we\nselect the SRCC, KRCC, and PLCC as performance evalu-\nation metrics.\nVision Transformer. The transformer [46] consists of\nmulti-head attentions (MHAs), multi-layer perceptrons\n(MLPs), layer normalizations (LNs) [1], and residual con-\nnections. Unlike the CNN, the transformer has a minimum\ninductive bias and can scale with the length of the input se-\nquence without limiting factors. Recently, it has emerged\nthat the transformer has combined with the CNN using the\nself-attention [4], and some of which have completely re-\nplaced CNN [47].\nThe transformer is mainly self-attention based approach.\nSince the self-attention layer aggregates global information\nfrom the entire input sequence, therefore, the model can\ncapture the entire image for measuring the perceptual qual-\nity of the whole image. Vision Transformer (ViT) [18] is a\nrepresentative success model among transformer-based vi-\nsion models. A hybrid architecture was proposed for image\nrecognition using a concord of CNN and the transformer\nencoder. It replaces the pixel patch embedding with the\npatches extracted from the CNN feature map. This archi-\ntecture could be applied well in the IQA task, because the\neffectiveness of the deep features on the perceptual IQA\ntask was demonstrated in recent studies [61, 16, 55]. In\nDETR [4], the encoder-decoder architecture is employed\nand the decoder takes learned positional embeddings as ob-\nject queries for object detection. This approach could be ap-\nplied to the FR-IQA model that compares two images and\nmeasures the similarity. To measure similarity, one of the\ntwo images can be adopted as the query information in the\nself-attention layer. From the successful approaches using\nthe transformer, we learn the direction to develop the per-\nceptual IQA method with the transformer.\nVision Transformer based IQA.Inspired by ViT, TRIQ\n[55] naturally attempts to solve the blind IQA task using the\ntransformer with the MLP head. In order to exploit ViT and\nhandle images with different resolution, the TRIQ model\ndeﬁnes the positional embedding with sufﬁcient length to\ncover the maximal image resolution. The transformer en-\ncoder employs adaptive positional embedding, which han-\ndles a image with arbitrary resolutions. The output of the\nencoder is fed into the MLP head and the MLP head pre-\ndicts the perceived image quality.\nBasically, similar to the TRIQ, our proposed model ap-\nplies the transformer architecture for the IQA task. How-\never, additional aspects are considered in order to design the\nperceptual FR-IQA with the transformer. First, the trans-\nMLP\nMulti-HeadSelf-Attention\nAdd & Norm\nAdd & Norm\nMulti-HeadSelf-Attention\nAdd & Norm\nMulti-HeadAttention\nAdd & Norm\nMLP\nAdd & NormDecoder\nEncoder×L\n×L\nqkv qkv\nqkv\nFigure 2. The transformer encoder and decoder.\nformer encoder-decoder architecture is an important point\nin our approach. The reference information and the differ-\nence information between the distorted and reference im-\nages are employed as an input into the transformer. Second,\nwe adopt the Siamese architecture to extract both the in-\nput feature representations from the reference and distorted\nimages. For each image, by concatenating multiple feature\nmaps extracted from intermediate layers, we obtained sufﬁ-\ncient information for the model.\n3. Proposed Method\nThe proposed method that is illustrated in Fig. 1 con-\nsists of three main components: a feature extraction back-\nbone, a transformer encoder-decoder, and a prediction head.\nFirst, we use a CNN backbone to extract feature repre-\nsentations from both reference and distorted input images.\nThe extracted feature maps are projected to ﬁxed size of\nvectors and ﬂattened. In order to predict perceived qual-\nity, the trainable extra [quality] embedding is added\nto the sequence of embedded feature. It is similar to ap-\nproach using [class] token in previous transformer mod-\nels [15, 18, 45]. The position embedding is also added in\norder to maintain the positional information. We pass this\ninput feature embedding into the transformer encoder and\ndecoder. The transformer encoder and decoder are based on\nthe standard architecture of the transformer [46], where the\nstructure is brieﬂy illustrated in Fig. 2. The ﬁrst vector of\nthe output embedding of the decoder is fed into the MLP\nhead in order to predict a single perceptual quality score.\nFeature Extraction Backbone. A conventional CNN\nnetwork, Inception-Resnet-V2 [44], is employed as the\nfeature extraction backbone network. Pretrained weights\non ImageNet [14] is imported and frozen. Feature maps\nTable 1. IQA datasets for performance evaluation and model training.\nDatabase # Ref. # Dist. Dist. Type # Dist. Type # Rating Rating Type Env.\nLIVE [42] 29 779 traditional 5 25k MOS lab\nCSIQ [32] 30 866 traditional 6 5k MOS lab\nTID2013 [38] 25 3,000 traditional 25 524k MOS lab\nKADID-10k [33] 81 10.1k traditional 25 30.4k MOS crowdsourcing\nPIPAL [22] 250 29k trad.+alg. outputs 40 1.13m MOS crowdsourcing\nfrom six intermediate layers of Inception-Resnet-V2, i.e.,\n{mixed 5b, block35 2, block35 4, block35 6, block35 8,\nblock35 10}, are extracted. The extracted feature maps\nhave the same shape flayer ∈RH×W×c, where c = 320,\nand they are concatenated into feature map. In other words,\nfor an input image I ∈RH0×W0×3, the feature map f ∈\nRH×W×C, where C = 6×c, is extracted.\nBoth reference and distorted images are used; therefore,\nthe two input feature maps, fref and fdist, are employed\nfor the transformer, respectively. In order to obtain differ-\nence information between reference and distorted images,\na difference feature map, fd, is also used. It can be simply\nobtained by subtraction between two feature maps of refer-\nence and distorted images, i.e., fdiff = fref −fdist.\nTransformer encoder. A difference feature embedding,\nFd ∈RN×D, is used as the input of the transformer en-\ncoder. We ﬁrst reduce the channel dimension of thefd to the\ntransformer dimension D using a 1 ×1 convolution. Then,\nwe ﬂatten the spatial dimensions, which means the number\nof patches in the feature map is set as N = H ×W. As\noften used in the vision transformer models [18, 55], we ap-\npend extra quality embedding at the beginning of the input\nfeature embedding as Fd0 . And the trainable position em-\nbedding Pd ∈R(1+N)×D are also added in order to retain\nthe positional information. The calculation of the encoder\ncan be formulated as\ny0 = [Fd0 + Pd0 , Fd1 + Pd1 , ..., FdN + PdN ],\nqi = ki = vi = yi−1,\ny′\ni = LN(MHA (qi, ki, vi) +yi−1),\nyi = LN(MLP (y′\ni) +y′\ni), i = 1, ..., L\n[FE0 , FE1 , ..., FEN ] =yL,\n(1)\nwhere L denotes the number of the encoder layers. The out-\nput of the encoder FE ∈R(1+N)×D has the same size to\nthat of the input feature embedding.\nTransformer decoder. The decoder takes the reference\nfeature embedding Fr ∈RN×D, obtained through the chan-\nnel reduction and ﬂattening. The extra quality embedding\nand position embedding are also added to it. The output of\nthe encoder, FE, is used as an input of the decoder, and it is\nused as a key-value in the second MHA layer. The calcula-\ntion of the decoder can be formulated as:\nyL = [FE0 , FE1 , ..., FEN ],\nz0 = [Fr0 + Pr0 , Fr1 + Pr1 , ..., FrN + PrN ],\nqi = ki = vi = zi−1,\nz′\ni = LN(MHA (qi, ki, vi) +zi−1),\nq′\ni = z′\ni, k ′\ni = v′\ni = yL,\nz′′\ni = LN(MHA (q′\ni, k′\ni, v′\ni) +z′\ni),\nzi = LN(MLP (z′′\ni ) +z′′\ni ), i = 1, ..., L\n[FD0 , FD1 , ..., FDN ] =zL,\n(2)\nwhere L denotes the number of decoder layers. The output\nembedding FD ∈R(1+N)×D of the decoder is ﬁnally ob-\ntained.\nHead. The ﬁnal quality prediction is computed in the pre-\ndiction MLP head. The ﬁrst vector of the decoder output,\nFD0 ∈R1×D in Eq. 2, is fed into the MLP head, which\ncontains the quality information. The MLP head consists of\ntwo fully connected (FC) layers, and the ﬁrst FC layer is\nused followed by the ReLU activation. The second FC layer\nhas one channel to predict a single score.\n4. Experiments\n4.1. Datasets\nWe employ ﬁve databases that are commonly used in\nthe research of perceptual image quality assessment. The\nLIVE Image Quality Assessment Database (LIVE) [42],\nthe Categorical Subjective Image Quality (CSIQ) database\n[32], and the TID2013 [38] are the databases that serve\nas baselines for full-reference IQA studies. These datasets\nonly include traditional distortion types and the subjective\nscores are measured in the controlled laboratory environ-\nment. KADID-10k [33] is a large-scale IQA dataset and is\nchosen as the training dataset in our experiment. It is three\ntimes larger compared to the TID2013 [38] and the ratings\nare collected from crowdsourcing. The PIPAL [22] dataset\nis used for both the training and evaluation of the model\nin this study. A large quantity of distorted images includ-\ning GAN based algorithms’ outputs and following human\nratings are included in the PIPAL dataset. It is challenging\nfor existing metrics to predict perceptual quality accurately\n[21]. Table 1 summarizes the characteristics of the datasets\nemployed in this study.\nTable 2. Performance comparison of the IQA methods on three standard IQA databases, i.e., LIVE [42], CSIQ [32], and TID2013 [38], in\nterms of SRCC and KRCC. The top three performing methods are highlighted in bold face. Some results are borrowed from [16, 21].\nMethod LIVE[42] CSIQ[32] TID2013[38]\nSRCC KRCC SRCC KRCC SRCC KRCC\nPSNR 0.873 0.680 0.810 0.601 0.687 0.496\nSSIM [51] 0.948 0.796 0.865 0.680 0.727 0.545\nMS-SSIM [52] 0.951 0.805 0.906 0.730 0.786 0.605\nVSI [58] 0.952 0.806 0.943 0.786 0.897 0.718\nMAD [32] 0.967 0.842 0.947 0.797 0.781 0.604\nVIF [40] 0.964 0.828 0.911 0.743 0.677 0.518\nFSIMc [59] 0.965 0.836 0.931 0.769 0.851 0.667\nNLPD [31] 0.937 0.778 0.932 0.769 0.800 0.625\nGMSD [53] 0.960 0.827 0.950 0.804 0.804 0.634\nWaDIQaM [3] 0.947 0.791 0.909 0.732 0.831 0.631\nPieAPP [39] 0.919 0.750 0.892 0.715 0.876 0.683\nLPIPS [61] 0.932 0.765 0.876 0.689 0.670 0.497\nDISTS [17] 0.954 0.811 0.929 0.767 0.830 0.639\nSWD [21] - - - - 0.819 0.634\nIQT (ours) 0.970 0.849 0.943 0.799 0.899 0.717\nIQT-C (ours) 0.917 0.737 0.851 0.649 0.804 0.607\n4.2. Implementation details\nWe denote our model trained on the KADID-10k as IQT.\nThe hyper-parameters for the model are set as follow: i) the\nnumber of encoder and decoder layer is set to 2 (i.e.,L = 2),\nii) the number of heads in the MHA is set to 4 (i.e.,H = 4),\niii) the transformer dimension is set to 256 (i.e., D = 256),\niv) dimension of the MLP in the encoder and decoder is set\nto 1024 (i.e., Dfeat = 1024), v) the dimension of the ﬁrst\nFC layer in MLP head is set to 512 (i.e., Dhead = 512).\nIn the training phase, a given image is cropped to obtain\nimage patches. The dimension of the patch fed into the pro-\nposed IQT is 256 ×256 ×3. The number of patches in the\nfeature map is set to N = 891. In the testing phase, image\npatches are also acquired from the given image pair. We ex-\ntract M overlapping patches and predict ﬁnal quality score\nby averaging M individual quality scores of the patches.\nThe stride size is set as large as possible to cover the entire\nimage with fewer patches.\nData augmentation including horizontal ﬂip and random\nrotation is applied during the training. The training is con-\nducted using an ADAM [28] optimizer with a batch size of\n16. Initial learning rate 2 ×10−4 and cosine learning rate\ndecay are set. The training loss is computed using a mean\nsquared error (MSE) loss function. Our network is imple-\nmented using Tensorﬂow framework. It roughly takes a half\nday with a single NVIDIA TITAN RTX to train our model.\n4.3. Results\nThe proposed IQT shows that the transformer based\nmodel is sufﬁciently competitive compared to existing ap-\nproaches for the dataset that has traditional distortions. Our\nmodel is trained on KADID-10k and, then, the performance\non the three standard IQA datasets is evaluated. The perfor-\nmance comparison result is reported in Table 2 and the scat-\nLIVE\nTID2013\nCSIQ\nPIPAL validation\nFigure 3. Scatter plots of ground-truth mean opinion scores\n(MOSs) against predicted scores of proposed IQT on LIVE, CSIQ,\nTID2013, and PIPAL datasets. The predicted scores are obtained\nfrom the model trained on KADID-10k dataset.\nter plots of the predicted scores of IQT and the ground-truth\nMOS are also presented in Fig. 3. For LIVE and TID2013\ndatabases, the proposed IQT shows the best performance\nin terms of SRCC. Also, it is ranked in the top three in\nall benchmarks in terms of SRCC and KRCC. In particu-\nlar, our method shows better performance than recent deep\nlearning-based methods [3, 39, 61, 16, 21] for most cases.\nExample images of the PIPAL validation dataset and fol-\nlowing PSNR, SSIM [51], MS-SSIM [52], LPIPS [61], and\nproposed IQT are illustrated in Fig. 4. From the left to the\nright, the perceptually better images to worse images are\nlisted based on MOS. Our proposed IQT predicts the qual-\nity scores similar to MOS in terms of the superiority. There\nexist the images that are clearly distinguished by all meth-\nods, however, it is difﬁcult to accurately predict perceptual\nFigure 4. Example images from validation dataset of the NTIRE 2021 challenge. For each distorted image, predicted scores of PSNR,\nSSIM [51], MS-SSIM [52], LPIPS [61], and proposed IQT are listed. MOS denotes the ground-truth human rating. The number in the\nparenthesis denotes the rank among considered distorted images in this ﬁgure.\nquality for some images.\nOur model is also evaluated on PIPAL [22] dataset. The\nIQT trained on KADID-10k dataset shows the best perfor-\nmance among all metrics. The benchmark results compar-\ning the existing IQA methods on PIPAL validation and test-\ning datasets are shown in Table 3. Corresponding scatter\nplots of the predicted scores of IQT and the ground-truth\nMOS for validation dataset are also presented in Fig. 3. It is\nshown that our method could also be a promising approach\nTable 3. Performance comparison of IQA methods on PIPAL [22]\ndataset. Main score is calculated with summation of PLCC and\nSRCC. The top performing method is highlighted in bold. Some\nresults are provided from the NTIRE 2021 IQA challenge report\n[23].\nMethod Validation Testing\nPLCC SRCC PLCC SRCC\nPSNR 0.292 0.255 0.277 0.249\nSSIM [51] 0.398 0.340 0.394 0.361\nMS-SSIM [52] 0.563 0.486 0.501 0.462\nVIF [40] 0.524 0.433 0.479 0.397\nVSNR [5] 0.375 0.321 0.411 0.368\nVSI [58] 0.516 0.450 0.517 0.458\nMAD [32] 0.626 0.608 0.580 0.543\nNQM [13] 0.416 0.346 0.395 0.364\nUQI [50] 0.548 0.486 0.450 0.420\nIFC [41] 0.677 0.594 0.555 0.485\nGSM [34] 0.469 0.418 0.465 0.409\nRFSIM [60] 0.304 0.266 0.328 0.304\nSRSIM [57] 0.654 0.566 0.636 0.573\nFSIM [59] 0.561 0.467 0.571 0.504\nFSIMc [59] 0.559 0.468 0.573 0.506\nNIQE [36] 0.102 0.064 0.132 0.034\nMA [35] 0.203 0.201 0.147 0.140\nPI [2] 0.166 0.169 0.145 0.104\nLPIPS-Alex [61] 0.646 0.628 0.571 0.566\nLPIPS-VGG [61] 0.647 0.591 0.633 0.595\nPieAPP [39] 0.697 0.706 0.597 0.607\nWaDIQaM [3] 0.654 0.678 0.548 0.553\nDISTS [17] 0.686 0.674 0.687 0.655\nSWD [22] 0.668 0.661 0.634 0.624\nIQT (ours) 0.741 0.718 - -\nIQT-C (ours) 0.876 0.865 0.790 0.799\nin the ﬁeld of quality assessment on various datasets includ-\ning generative models’ output images. Moreover, as shown\nin the results on the standard IQA datasets, our model shows\na robust performance on different dataset.\nFig. 5 shows the examples of attention maps from the\nIQT model. It refers to the area where the model focuses\nmore when predicting the perceptual quality. From our\nmodel architecture, the learned attention weights exist in the\nMHA layers of the encoder and decoder. We visualize the\nFigure 5. Visualization of attention maps from the proposed IQT.\nThe center-cropped images are randomly sampled from the PI-\nPAL [22] dataset. Attention maps are averaged over all attention\nweights in the encoder and decoder.\nTable 4. Comparison of performance on three standard IQA databases depending on the inputs to the transformer encoder and decoder.\nThe top performing method is highlighted in bold face.\nNo. Encoder Decoder LIVE CSIQ TID2013\nFdist Fref Fdiff Fdist Fref Fdiff SRCC/KRCC SRCC/KRCC SRCC/KRCC\n(1) ✓ ✓ 0.901 / 0.713 0.768 / 0.575 0.646 / 0.468\n(2) ✓ ✓ 0.934 / 0.767 0.855 / 0.670 0.739 / 0.548\n(3) ✓ ✓ 0.954 / 0.805 0.865 / 0.680 0.755 / 0.564\n(4) ✓ ✓ 0.967 / 0.838 0.944 / 0.803 0.884 / 0.698\n(5) ✓ ✓ 0.967 / 0.837 0.945 / 0.803 0.881 / 0.694\n(6) ✓ ✓ 0.969 / 0.843 0.945 / 0.803 0.897 / 0.714\n(7) ✓ ✓ 0.970 / 0.845 0.947 / 0.805 0.896 / 0.712\n(8) ✓ ✓ 0.968 / 0.840 0.942 / 0.795 0.889 / 0.704\nattention maps by averaging all of the attention weights and\nresizing to the image size. It is observed that the attention is\nspatially localized or spread uniformly across whole image\ndepending on the image and distortion type. It is important\nto see the entire image and, then, focus on a localized re-\ngion when one perceives the quality of the image. Our ap-\nproach to determine the important region based on the self-\nattention mechanism will be useful to predict the quality.\n4.4. Ablations\nThe use of the difference information between reference\nand distorted images is one of the important factors in the\nproposed architecture. As mentioned in the previous section\n3, the input into the encoder or decoder is a feature embed-\nding and there are three types available, i.e., Fref , Fdist,\nand Fdiff . To investigate the effect of input types and lo-\ncation, we conduct ablation experiment and the results of\nperformance comparison are shown in Table 4.\nFirst, it is found that the use of the difference feature em-\nbedding as the input is a better choice than using only refer-\nence and distorted feature embeddings directly on the input.\nIt is shown that the models (4)-(8) have better performance\nthan models (1)-(3) in Table 4. From this experiment, the\nmodel (7) is selected for our model design and this means\nthat the Fdiff and Fref are used into the encoder and de-\ncoder, respectively. When difference information enters the\nencoder or decoder, there is no signiﬁcant performance dif-\nference between putting distorted or reference feature em-\nbedding in the other side. We can ﬁnd the similar results be-\ntween the models (4) and (5), and between the models (6)\nand (7). From this experiment, it is concluded that the dif-\nference information is an important factor in the proposed\narchitecture for the IQA task.\nAn additional experiment is conducted to prove that the\ndifference information in the feature level is more effec-\ntive than that in the image level. The comparison results are\nshown in Table 5. Application of the difference information\nin the feature level that is important in our model design re-\nsults in a better performance for all datasets. In other words,\nthe difference information in perceptual space is more use-\nful to predict an image quality score compared to the RGB\nTable 5. Comparison of performance on the three standard IQA\ndatabases according to the method of using difference informa-\ntion. “Feature” refers to a difference operation conducted between\nfeature maps extracted from the backbone. “Image” refers to the\ndifference operation on RGB images.\nDiff. Info. LIVE CSIQ TID2013\nSRCC/KRCC SRCC/KRCC SRCC/KRCC\nFeature 0.970 / 0.845 0.947 / 0.805 0.896 / 0.712\nImage 0.954 / 0.809 0.946 / 0.798 0.862 / 0.671\ncolor space.\n4.5. NTIRE 2021 Perceptual IQA Challenge\nThis work is proposed to participate in the NTIRE 2021\nperceptual image quality assessment challenge [23]. The\nobjective of this challenge is to develop a model predicting\na value with high accuracy comparable to the ground-truth\nMOS. The PIPAL [22] dataset is used for the NTIRE 2021\nchallenge. For this challenge, we train our model on training\ndataset provided in the NTIRE 2021 challenge. The same\nmodel structure and both training and testing strategies are\napplied for the challenge. The model hyper-parameters are\nset as follow: L = 1, D = 128, H = 4, Dfeat = 1024,\nand Dhead = 128. The input image size of the model is set\nto 192 ×192 ×3; therefore, we set the number of patches\nin feature map N = 441. The model for the NTIRE 2021\nchallenge is denoted as IQT-C to distinguish from the pre-\nviously mentioned model IQT in Tables 2, 3 and 6.\nTable 6. Performance comparison of the participants on testing\ndataset of the NTIRE 2021 challenge. Main score is calculated\nas the sum of PLCC and SRCC. The number in the parenthesis\ndenotes the rank. Only a few of the teams are shown in this Table.\nThis result is provided from the NTIRE 2021 IQA challenge report\n[23].\nEntries PLCC SRCC Main Score ↑\nIQT-C (ours) 0.7896 (1) 0.7990 (2) 1.5885 (1)\nAnonymous 1 0.7803 (2) 0.8009 (1) 1.5811 (2)\nAnonymous 2 0.7707 (4) 0.7918 (3) 1.5625 (3)\nAnonymous 3 0.7709 (3) 0.7770 (4) 1.5480 (4)\nAnonymous 4 0.7615 (5) 0.7703 (6) 1.5317 (5)\nAnonymous 5 0.7468 (7) 0.7744 (5) 1.5212 (6)\nAnonymous 6 0.7480 (6) 0.7641 (7) 1.5121 (7)\nThe benchmark results of the IQT-C on validation and\ntesting datasets of the NTIRE 2021 challenge are shown in\nTable 3. The scatter plot is also illustrated in Fig. 6. The\nIQT-C shows the best performance among all metrics. In\naddition, a better performance than the IQT model trained\non the KADID-10k is also found. Final result of the chal-\nlenge in testing phase is reported in Table 6. The rankings of\nthe entries are determined in terms of main score, which is\ncalculated with summation of PLCC and SRCC. Our model\nwon the ﬁrst place in terms of the main score among all par-\nticipants. In terms of PLCC and SRCC, we obtain the ﬁrst\nand second highest scores, respectively.\nThe model trained on the PIPAL shows outstanding per-\nformance for the validation and testing dataset in Table 3.\nHowever, on the other hands, it tends to increase the risk\nof over-ﬁtting. When we evaluate the IQT-C model on the\nthree standard IQA datasets, it shows much lower perfor-\nmance than the IQT trained on KADID-10k (Table 2). It is\nnoteworthy noting that the IQT-C is the special case of our\napproach for the NTIRE 2021 challenge. However, there is a\nroom for improvement in terms of robustness for any other\ndistortion types when we train the IQT on PIPAL dataset.\nIn addition, future work is needed to improve the model to\nsolve this problem.\n5. Conclusion\nWe proposed an IQT and it is appropriately applied to a\nperceptual image quality assessment task by taking an ad-\nvantage of transformer encoder-decoder architecture. The\nIQT demonstrated the outstanding performance on the three\nstandard IQA databases compared to many existing meth-\nods. Our method also showed the best performance for the\nlatest IQA dataset that contains deep learning-based dis-\ntorted images. The IQT showed another promising exam-\nple that the transformer based approach can achieve a high\nperformance even in the perceptual quality assessment task.\nDespite the success of our model, there exists a room\nfor improvement. Further investigation of the transformer\nbased approach, especially considering more diverse reso-\nlutions and distortion types, is needed. In addition, develop-\ning a no-reference metric for perceptual quality assessment\nwill be desirable that can be used in real-world scenarios.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016. 3\n[2] Y . Blau, R. Mechrez, R. Timofte, T. Michaeli, and L. Zelnik-\nManor. The 2018 PIRM challenge on perceptual image\nsuper-resolution. In Eur. Conf. Comput. Vis. Worksh., pages\n1–22, 2018. 1, 6\n[3] S. Bosse, D. Maniry, K.-R. M ¨uller, T. Wiegand, and W.\nSamek. Deep neural networks for no-reference and full-\nFigure 6. Scatter plots of ground-truth mean opinion scores\n(MOSs) against predicted scores of IQT-C on the PIPAL valida-\ntion dataset.\nreference image quality assessment. IEEE Trans. Image Pro-\ncess., 27(1):206–219, 2017. 1, 2, 5, 6\n[4] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov,\nand S. Zagoruyko. End-to-end object detection with trans-\nformers. In Eur. Conf. Comput. Vis., pages 213–229, 2020.\n1, 3\n[5] D. M. Chandler and S. S. Hemami. VSNR: A wavelet-based\nvisual signal-to-noise ratio for natural images. IEEE Trans.\nImage Process., 16(9):2284–2298, 2007. 2, 6\n[6] H. Chen, Y . Wang, T. Guo, C. Xu, Y . Deng, Z. Liu, S. Ma,\nC. Xu, C. Xu, and W. Gao. Pre-trained image processing\ntransformer. arXiv preprint arXiv:2012.00364, 2020. 1\n[7] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and\nI. Sutskever. Generative pretraining from pixels. InInt. Conf.\nMach. Learn., pages 1691–1703, 2020. 1\n[8] M. Cheon, J.-H. Kim, J.-H. Choi, and J.-S. Lee. Generative\nadversarial network-based image super-resolution using per-\nceptual content losses. In Eur. Conf. Comput. Vis. Worksh.,\npages 1–12, 2018. 1\n[9] M. Cheon and J.-S. Lee. Subjective and objective quality as-\nsessment of compressed 4k uhd videos for immersive experi-\nence. IEEE Trans. Circuit Syst. Video Technol., 28(7):1467–\n1480, 2017. 1\n[10] M. Cheon, T. Vigier, L. Krasula, J. Lee, P. Le Callet, and\nJ.-S. Lee. Ambiguity of objective image quality metrics: A\nnew methodology for performance evaluation. Signal Pro-\ncessing: Image Communication, 93:116150, 2021. 3\n[11] S. Chikkerur, V . Sundaram, M. Reisslein, and L. J. Karam.\nObjective video quality assessment methods: A classiﬁca-\ntion, review, and performance comparison. IEEE Trans.\nBroadcasting, 57(2):165–182, 2011. 1\n[12] J.-H. Choi, J.-H. Kim, M. Cheon, and J.-S. Lee. Deep\nlearning-based image super-resolution considering quantita-\ntive and perceptual quality. Neurocomputing, 398:347–359,\n2020. 1\n[13] N. Damera-Venkata, T. D. Kite, W. S. Geisler, B. L. Evans,\nand A. C. Bovik. Image quality assessment based on a degra-\ndation model. IEEE Trans. Image Process. , 9(4):636–650,\n2000. 6\n[14] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nFei. Imagenet: A large-scale hierarchical image database. In\nIEEE/CVF Conf. Comput. Vis. Pattern Recog. , pages 248–\n255, 2009. 3\n[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018. 3\n[16] K. Ding, K. Ma, S. Wang, and E. P. Simoncelli. Image\nquality assessment: Unifying structure and texture similar-\nity. IEEE Trans. Pattern Anal. Mach. Intell., 2020. 1, 2, 3,\n5\n[17] K. Ding, K. Ma, S. Wang, and E. P. Simoncelli. Compari-\nson of full-reference image quality models for optimization\nof image processing systems. International Journal of Com-\nputer Vision, 129(4):1258–1281, 2021. 2, 5, 6\n[18] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G.\nHeigold, S. Gelly, et al. An image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 1, 3, 4\n[19] Y . Fang, H. Zhu, Y . Zeng, K. Ma, and Z. Wang. Per-\nceptual quality assessment of smartphone photography. In\nIEEE/CVF Conf. Comput. Vis. Pattern Recog., pages 3677–\n3686, 2020. 1\n[20] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D.\nWarde-Farley, S. Ozair, A. Courville, and Y . Bengio. Gener-\native adversarial nets. In Adv. Neural Inform. Process. Syst.,\npage 2672–2680, 2014. 1\n[21] J. Gu, H. Cai, H. Chen, X. Ye, J. Ren, and C. Dong. Im-\nage quality assessment for perceptual image restoration:\nA new dataset, benchmark and metric. arXiv preprint\narXiv:2011.15002, 2020. 1, 4, 5\n[22] J. Gu, H. Cai, H. Chen, X. Ye, J. Ren, and C. Dong. PIPAL: A\nlarge-scale image quality assessment dataset for perceptual\nimage restoration. In Eur. Conf. Comput. Vis., pages 633–\n651, 2020. 4, 6, 7\n[23] Jinjin Gu, Haoming Cai, Chao Dong, Jimmy S. Ren, Yu\nQiao, Shuhang Gu, Radu Timofte, et al. NTIRE 2021 chal-\nlenge on perceptual image quality assessment. In IEEE/CVF\nConf. Comput. Vis. Pattern Recog. Worksh., 2021. 1, 2, 6, 7\n[24] V . Hosu, H. Lin, T. Sziranyi, and D. Saupe. Koniq-10k: An\necologically valid database for deep learning of blind image\nquality assessment. IEEE Trans. Image Process., 29:4041–\n4056, 2020. 2\n[25] B. Hu, L. Li, J. Wu, and J. Qian. Subjective and objec-\ntive quality assessment for image restoration: A critical sur-\nvey. Signal Processing: Image Communication, 85:115839,\n2020. 1\n[26] Y . Jiang, S. Chang, and Z. Wang. Transgan: Two\ntransformers can make one strong gan. arXiv preprint\narXiv:2102.07074, 2021. 1\n[27] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and\nM. Shah. Transformers in vision: A survey. arXiv preprint\narXiv:2101.01169, 2021. 1\n[28] D. P. Kingma and J. Ba. Adam: A method for stochastic\noptimization. In Int. Conf. Learn. Represent., 2015. 5\n[29] Luk ´aˇs Krasula, Karel Fliegel, Patrick Le Callet, and Milo ˇs\nKl´ıma. On the accuracy of objective image and video qual-\nity models: New methodology for performance evaluation.\nIn Int. Conf. Quality of Multimedia Experience , pages 1–6,\n2016. 3\n[30] M. Kumar, D. Weissenborn, and N. Kalchbrenner. Coloriza-\ntion transformer. arXiv preprint arXiv:2102.04432, 2021. 1\n[31] V . Laparra, J. Ball´e, A. Berardino, and E. P. Simoncelli. Per-\nceptual image quality assessment using a normalized lapla-\ncian pyramid. Electronic Imaging, 2016(16):1–6, 2016. 5\n[32] E. C. Larson and D. M. Chandler. Most apparent distortion:\nfull-reference image quality assessment and the role of strat-\negy. Journal of electronic imaging, 19(1):011006, 2010. 2,\n4, 5, 6\n[33] H. Lin, V . Hosu, and D. Saupe. KADID-10k: A large-scale\nartiﬁcially distorted iqa database. In Int. Conf. Quality of\nMultimedia Experience, pages 1–3, 2019. 4\n[34] A. Liu, W. Lin, and M. Narwaria. Image quality assessment\nbased on gradient similarity. IEEE Trans. Image Process. ,\n21(4):1500–1512, 2012. 6\n[35] C. Ma, C.-Y . Yang, X. Yang, and M.-H. Yang. Learning a no-\nreference quality metric for single-image super-resolution.\nComputer Vision and Image Understanding, 158:1–16, 2017.\n2, 6\n[36] A. Mittal, R. Soundararajan, and A. C. Bovik. Making a\n“completely blind” image quality analyzer.IEEE Signal pro-\ncessing letters, 20(3):209–212, 2012. 2, 6\n[37] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer,\nA. Ku, and D. Tran. Image transformer. In Int. Conf. Mach.\nLearn., pages 4055–4064, 2018. 1\n[38] N. Ponomarenko, L. Jin, O. Ieremeiev, V . Lukin, K. Egiazar-\nian, J. Astola, B. V ozel, K. Chehdi, M. Carli, F. Battisti, et al.\nImage database TID2013: Peculiarities, results and perspec-\ntives. Signal processing: Image communication , 30:57–77,\n2015. 4, 5\n[39] E. Prashnani, H. Cai, Y . Mostoﬁ, and P. Sen. Pieapp: Percep-\ntual image-error assessment through pairwise preference. In\nIEEE/CVF Conf. Comput. Vis. Pattern Recog., pages 1808–\n1817, 2018. 1, 2, 5, 6\n[40] H. R. Sheikh and A. C. Bovik. Image information and visual\nquality. IEEE Trans. Image Process., 15(2):430–444, 2006.\n1, 2, 5, 6\n[41] H. R. Sheikh, A. C. Bovik, and G. De Veciana. An infor-\nmation ﬁdelity criterion for image quality assessment us-\ning natural scene statistics. IEEE Trans. Image Process. ,\n14(12):2117–2128, 2005. 2, 6\n[42] H. R. Sheikh, M. F. Sabir, and A. C. Bovik. A statistical\nevaluation of recent full reference image quality assessment\nalgorithms. IEEE Trans. Image Process., 15(11):3440–3451,\n2006. 1, 2, 4, 5\n[43] Rajiv Soundararajan and Alan C Bovik. RRED indices:\nReduced reference entropic differencing for image quality\nassessment. IEEE Trans. Image Process. , 21(2):517–526,\n2011. 2\n[44] C. Szegedy, S. Ioffe, V . Vanhoucke, and A. Alemi. Inception-\nv4, inception-resnet and the impact of residual connections\non learning. In AAAI, pages 1–7, 2017. 3\n[45] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablay-\nrolles, and H. J ´egou. Training data-efﬁcient image trans-\nformers & distillation through attention. arXiv preprint\narXiv:2012.12877, 2020. 1, 3\n[46] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all\nyou need. arXiv preprint arXiv:1706.03762, 2017. 1, 3\n[47] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam,\nAlan Yuille, and Liang-Chieh Chen. Axial-deeplab: Stand-\nalone axial-attention for panoptic segmentation. In Eur.\nConf. Comput. Vis., pages 108–126. Springer, 2020. 3\n[48] X. Wang, K. Yu, S. Wu, J. Gu, Y . Liu, C. Dong, Y . Qiao, and\nC. C. Loy. ESRGAN: Enhanced super-resolution generative\nadversarial networks. In Eur. Conf. Comput. Vis. Worksh. ,\npages 1–16, 2018. 1\n[49] Z. Wang. Applications of objective image quality assess-\nment methods [applications corner]. IEEE Signal Processing\nMagazine, 28(6):137–142, 2011. 1\n[50] Z. Wang and A. C. Bovik. A universal image quality index.\nIEEE Signal Processing Letters, 9(3):81–84, 2002. 6\n[51] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simon-\ncelli. Image quality assessment: from error visibility to struc-\ntural similarity. IEEE Trans. Image Process., 13(4):600–612,\n2004. 1, 2, 5, 6\n[52] Z. Wang, E. P. Simoncelli, and A. C. Bovik. Multiscale struc-\ntural similarity for image quality assessment. In Proc. IEEE\nAsilomar Conf. Signals, Systems and Computers , volume 2,\npages 1398–1402, 2003. 1, 2, 5, 6\n[53] W. Xue, L. Zhang, X. Mou, and A. C. Bovik. Gradient mag-\nnitude similarity deviation: A highly efﬁcient perceptual im-\nage quality index. IEEE Trans. Image Process., 23(2):684–\n695, 2014. 1, 5\n[54] F. Yang, H. Yang, J. Fu, H. Lu, and B. Guo. Learning\ntexture transformer network for image super-resolution. In\nIEEE/CVF Conf. Comput. Vis. Pattern Recog., pages 5791–\n5800, 2020. 1\n[55] J. You and J. Korhonen. Transformer for image quality as-\nsessment. arXiv preprint arXiv:2101.01097, 2020. 1, 3, 4\n[56] G. Zhai and X. Min. Perceptual image quality assessment: a\nsurvey. Science China Information Sciences, 63:1–52, 2020.\n1\n[57] L. Zhang and H. Li. SR-SIM: A fast and high performance\niqa index based on spectral residual. In IEEE Int. Conf. Im-\nage Process., pages 1473–1476, 2012. 6\n[58] L. Zhang, Y . Shen, and H. Li. VSI: A visual saliency-induced\nindex for perceptual image quality assessment. IEEE Trans.\nImage Process., 23(10):4270–4281, 2014. 5, 6\n[59] L. Zhang, D. Zhang, and X. Mou. FSIM: a feature similar-\nity index for image quality assessment. IEEE Trans. Image\nProcess., 20(8):2378–2386, 2011. 1, 2, 5, 6\n[60] L. Zhang, L. Zhang, and X. Mou. RFSIM: A feature based\nimage quality assessment metric using riesz transforms. In\nIEEE Int. Conf. Image Process., pages 321–324, 2010. 6\n[61] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang.\nThe unreasonable effectiveness of deep features as a percep-\ntual metric. In IEEE/CVF Conf. Comput. Vis. Pattern Recog.,\npages 586–595, 2018. 1, 2, 3, 5, 6",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7066890001296997
    },
    {
      "name": "Transformer",
      "score": 0.6986929774284363
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6402385830879211
    },
    {
      "name": "Image quality",
      "score": 0.5940941572189331
    },
    {
      "name": "Encoder",
      "score": 0.5916642546653748
    },
    {
      "name": "Perception",
      "score": 0.5086584687232971
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.455646812915802
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4525028467178345
    },
    {
      "name": "Embedding",
      "score": 0.4520929455757141
    },
    {
      "name": "Computer vision",
      "score": 0.39388012886047363
    },
    {
      "name": "Image (mathematics)",
      "score": 0.26824694871902466
    },
    {
      "name": "Engineering",
      "score": 0.14582109451293945
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    }
  ]
}