{
    "title": "Assessing and Understanding Creativity in Large Language Models",
    "url": "https://openalex.org/W4409892328",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2138526042",
            "name": "Yunpu Zhao",
            "affiliations": [
                "Chinese Academy of Sciences",
                "Institute of Computing Technology",
                "University of Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A311940369",
            "name": "Rui Zhang",
            "affiliations": [
                "Institute of Computing Technology",
                "Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2115458916",
            "name": "Wen-Yi Li",
            "affiliations": [
                "Institute of Software",
                "University of Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A1959451644",
            "name": "Ling Li",
            "affiliations": [
                "Institute of Software",
                "University of Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2138526042",
            "name": "Yunpu Zhao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A311940369",
            "name": "Rui Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2115458916",
            "name": "Wen-Yi Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1959451644",
            "name": "Ling Li",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6839316307",
        "https://openalex.org/W4385570371",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4283171081",
        "https://openalex.org/W4376653992",
        "https://openalex.org/W4381104068",
        "https://openalex.org/W4311887664",
        "https://openalex.org/W4323655724",
        "https://openalex.org/W4312671789",
        "https://openalex.org/W4226360574",
        "https://openalex.org/W4396833486",
        "https://openalex.org/W4391556516",
        "https://openalex.org/W2171960331",
        "https://openalex.org/W4379654027",
        "https://openalex.org/W2937584124",
        "https://openalex.org/W2088666829",
        "https://openalex.org/W2935826897",
        "https://openalex.org/W2938716636",
        "https://openalex.org/W2935889327",
        "https://openalex.org/W2939040872",
        "https://openalex.org/W2170161045",
        "https://openalex.org/W3005126552",
        "https://openalex.org/W2171882070",
        "https://openalex.org/W1859649255",
        "https://openalex.org/W2040408755",
        "https://openalex.org/W4391136507",
        "https://openalex.org/W4322720178",
        "https://openalex.org/W4404826238",
        "https://openalex.org/W4283321795",
        "https://openalex.org/W4381573110",
        "https://openalex.org/W4386052615",
        "https://openalex.org/W4396833426",
        "https://openalex.org/W2090022222",
        "https://openalex.org/W1977056678",
        "https://openalex.org/W2061367359",
        "https://openalex.org/W3081924543",
        "https://openalex.org/W4200130072",
        "https://openalex.org/W4379933518",
        "https://openalex.org/W6852874933",
        "https://openalex.org/W4392669753",
        "https://openalex.org/W4380136478",
        "https://openalex.org/W4385849309",
        "https://openalex.org/W4401042689",
        "https://openalex.org/W4378468563",
        "https://openalex.org/W4389519254",
        "https://openalex.org/W4387617694",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W3201174429",
        "https://openalex.org/W4383468911",
        "https://openalex.org/W4378713418",
        "https://openalex.org/W6838865847",
        "https://openalex.org/W1635768081",
        "https://openalex.org/W3188645211",
        "https://openalex.org/W2034284819",
        "https://openalex.org/W2150781787",
        "https://openalex.org/W1641003075",
        "https://openalex.org/W2946995079",
        "https://openalex.org/W4360884927",
        "https://openalex.org/W3014731244",
        "https://openalex.org/W4393065402",
        "https://openalex.org/W2016125673",
        "https://openalex.org/W2920897010",
        "https://openalex.org/W2602856279",
        "https://openalex.org/W2914304175",
        "https://openalex.org/W4399917667"
    ],
    "abstract": "Abstract In the field of natural language processing, the rapid development of large language model (LLM) has attracted increasing attention. LLMs have shown a high level of creativity in various tasks, but the methods for assessing such creativity are inadequate. Assessment of LLM creativity needs to consider differences from humans, requiring multiple dimensional measurement while balancing accuracy and efficiency. This paper aims to establish an efficient framework for assessing the level of creativity in LLMs. By adapting the modified Torrance tests of creative thinking, the research evaluates the creative performance of various LLMs across 7 tasks, emphasizing 4 criteria including fluency, flexibility, originality, and elaboration. In this context, we develop a comprehensive dataset of 700 questions for testing and an LLM-based evaluation method. In addition, this study presents a novel analysis of LLMs’ responses to diverse prompts and role-play situations. We found that the creativity of LLMs primarily falls short in originality, while excelling in elaboration. In addition, the use of prompts and role-play settings of the model significantly influence creativity. Additionally, the experimental results also indicate that collaboration among multiple LLMs can enhance originality. Notably, our findings reveal a consensus between human evaluations and LLMs regarding the personality traits that influence creativity. The findings underscore the significant impact of LLM design on creativity and bridge artificial intelligence and human creativity, offering insights into LLMs’ creativity and potential applications.",
    "full_text": " \nAssessing and Understanding Creativity in\nLarge Language Models\nYunpu Zhao 1,2          Rui Zhang 2          Wenyi Li 3          Ling Li 3\n1 Department of Computer Science, University of Science and Technology of China, Hefei 230026, China\n2 State Key Lab of Processors, Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China\n3 Institute of Software, University of Chinese Academy of Sciences, Beijing 100190, China\n \nAbstract:    In the field of natural language processing, the rapid development of large language model (LLM) has attracted increasing\nattention. LLMs have shown a high level of creativity in various tasks, but the methods for assessing such creativity are inadequate. As-\nsessment of LLM creativity needs to consider differences from humans, requiring multiple dimensional measurement while balancing ac-\ncuracy and efficiency. This paper aims to establish an efficient framework for assessing the level of creativity in LLMs. By adapting the\nmodified Torrance tests of creative thinking, the research evaluates the creative performance of various LLMs across 7 tasks, emphasiz-\ning 4 criteria including fluency, flexibility, originality, and elaboration. In this context, we develop a comprehensive dataset of 700 ques-\ntions for testing and an LLM-based evaluation method. In addition, this study presents a novel analysis of LLMs′ responses  to diverse\nprompts and role-play situations. We found that the creativity of LLMs primarily falls short in originality, while excelling in elaboration.\nIn addition, the use of prompts and role-play settings of the model significantly influence creativity. Additionally, the experimental res-\nults also indicate that collaboration among multiple LLMs can enhance originality. Notably, our findings reveal a consensus between hu-\nman evaluations and LLMs regarding the personality traits that influence creativity. The findings underscore the significant impact of\nLLM design on creativity and bridge artificial intelligence and human creativity, offering insights into LLMs′ creativity  and potential\napplications.\nKeywords:   Large language models (LLMs), creativity assessment, prompt engineering, cognitive psychology, divergent thinking.\nCitation:   Y.  Zhao,  R.  Zhang,  W.  Li,  L.  Li.  Assessing  and  understanding  creativity  in  large  language  models.  Machine Intelligence\nResearch, vol.22, no.3, pp.417–436, 2025. http://doi.org/10.1007/s11633-025-1546-4\n \n \n1   Introduction\nIn\n recent years, the realm of artificial intelligence (AI)\nhas  witnessed  a  meteoric  rise  in  the  development  and\nsophistication of large language models (LLMs)[1, 2].  LLMs\nhave  significantly  advanced  in  their  capabilities  in  ad-\ndressing a variety of conventional natural language pro-\ncessing tasks, such as reasoning and natural language un-\nderstanding[3–6].  Moreover, LLMs also have demonstrated\nsignificant value in widespread applications. From trans-\nforming rudimentary text into compelling narratives [7, 8],\nunlocking\n a new realm of storytelling, to solving complex\nalgorithmic problems[9],  these models have shown a semb-\nlance  of  what  could  be  interpreted  as  creativity.  The\npractical  manifestations  of  this  creativity  have  penet-\nrated  various  sectors,  including  science  research,  where\nthey assist in idea generation and suggestion[6];\n  education,\nby providing personalized learning experiences [10];  and in\nthe entertainment industry, creating music and art [11, 12].\nIn\n many of their applications, LLMs seem to exhibit the\nability  to  generate  original  text,  aiding  tasks  related  to\nimagination and creativity, suggesting that they may in-\ndeed possess elements of creativity.\nFrom  the  broad  capabilities  demonstrated  by  LLMs,\nthe creativity they exhibit is a key reason they are con-\nsidered powerful. However, behind the impressive abilit-\nies of LLMs lies a significant question that warrants care-\nful  examination:  Do  these  models  actually  possess  real\ncreativity, or is their apparent intelligence merely an illu-\nsion – a complex imitation of human thinking created by\ntheir  training  paradigm?  This  question  touches  on  the\nvery nature of LLM intelligence, which may not be easily\nexplained. Since LLMs have shown considerable creativ-\nity, understanding the extent and characteristics of this\ncreativity is essential. Gaining deeper insight into the cre-\nativity of LLMs can not only guide us in further improv-\ning  their  performance  but  also  in  enhancing  our  under-\nstanding of the nature of their creativity. This, in turn,\ninforms our daily use and application of these models, un-\nderscoring  the  need  for  an  effective  method  to  measure\nand assess their creativity. Specifically, creative abilities\n \nResearch Article\nManuscript  received  on  August  24,  2024;  accepted  on  January  15,\n2025; published online on April 28, 2025\nRecommended by Associate Editor Zhiyuan Liu\n \nColored  figures  are  available  in  the  online  version  at  https://link.\nspringer.com/journal/11633\n©  The Author(s) 2025\n \nMachine Intelligence Research\nwww.mi-research.net\n22(3), June 2025 , 417-436\nDOI: 10.1007/s11633-025-1546-4\n \n\nare critical for the following application scenarios. First,\nLLM\n can  inspire  humans  on  creative  tasks  and  provide\nnovel ideas, especially in research idea generation [13, 14].  It\nhas also been suggested that the use of LLM can also lead\nto homogenization of creativity[15].\n  Second, humor genera-\ntion  with  LLMs  offer  significant  value  in  both  creative\nand practical applications. By simulating human-like hu-\nmor,  LLMs  can  assist  in  content  creation  for  entertain-\nment,  marketing,  and  social  media.  Finally,  LLMs  can\nserve as powerful cocreators in creative writings by gener-\nating  narrative  ideas,  suggesting  plot  developments,  or\neven drafting sections of text that inspire further refine-\nment by human writers.\nCreativity, as a term, traditionally refers to the natur-\nal ability to think innovatively, to make unconventional\nconnections, and to devise solutions that are both novel\nand  effective[16].\n  Assessing  the  creativity  of  LLMs  is\nfraught with challenges. First, the question of creativity\ndoes not have clear answers to refer to. When we ask an\nLLM a question such as “what is the speed of light in va-\ncuum in meters per second?”, the answer can be formally\nvetted, given the objective nature of the topic. However,\nwhen posed with a prompt such as “what would be the\nimplications  if  animals  could  talk?”,  the  situation  be-\ncomes different in this case because there is no definitive\nanswer and the answer is open and divergent, making it\nchallenging to judge the correctness of the output [17].\n  Ad-\nditionally,  since  creativity  encompasses  various  aspects,\nincluding  originality  and  flexibility,  it  is  necessary  to\ndesign diverse tasks and criteria to measure these qualit-\nies effectively in LLMs. In addition, there are differences\nbetween LLMs and humans, which might lead to irrelev-\nant responses or serious logical issues, requiring us to ad-\nditionally assess these aspects. Finally, evaluating creativ-\nity necessitates a delicate balance between accuracy and\nefficiency,  rendering  traditional  human-based  evaluation\nmethods less practical. Therefore, it is imperative to ad-\ndress the challenges outlined above to make a robust and\nsound assessment of creativity in LLMs.\nRecognizing the need for a comprehensive assessment\nof LLM′ s creativity, we design an efficient framework to\nautomatically assess the creativity of LLMs by adapting\nand  modifying  the  Torrance  tests  of  creative  thinking\n(TTCT)[18],\n  a widely recognized tool in psychometrics′  re-\nsearch for human creativity assessment. To enhance the\ncredibility of the results and reduce the randomness, sev-\nen verbal tasks, which use verbal stimuli, were selected.\nWe  employed  GPT-4,  the  most  advanced  LLM,  to  ex-\npand the question set for each task, thereby constructing\nthe testing dataset. To ensure a thorough and objective\nevaluation  of  creativity  and  capture  creativity′s  various\nmanifestations, we combine diverse tasks and criteria. We\ndesign  a  comprehensive  test  protocol  incorporating  four\ncriteria for measuring creativity: Fluency, flexibility, ori-\nginality, and elaboration. We let the LLMs answer ques-\ntions from the constructed dataset, obtaining many ques-\ntion-answer pairs. We utilized GPT-4 as an evaluator to\nassess each answer, as the GPT-4 is capable of effectively\nassessing the openness of responses and identifying their\nshortcomings and errors. Under proper prompt engineer-\ning,  GPT-4  can  efficiently  and  effectively  complete  the\nevaluation  of  the  entire  dataset  results.  Thus,  we  can\nachieve a balance between efficiency and accuracy in our\nassessment method.\nWe selected six popular LLMs as test subjects, each\npossessing different architectures and parameter scales. In\naddition to the overall testing, we conducted some addi-\ntional  exploratory  experiments  that  investigate  the\nchanges of creativity levels exhibited by LLMs when giv-\nen  different  types  of  prompts  and  different  roles  that\nLLMs  play.  Then,  we  designed  a  collaboration  mechan-\nism for LLMs to explore the impact of multiple LLMs col-\nlaborating  on  creativity.  Last,  we  also  performed  some\npsychological experiments related to personality traits on\nthe  LLMs,  including  emotional  intelligence  (EI),  em-\npathy, the big five inventory (BFI) and self-efficacy. Be-\ncause we found in relevant psychological research show-\ning that human creativity is correlated with these person-\nality traits and we verified the consistency between LLMs\nand humans in this regard.\nOur experiments and analysis yielded several conclu-\nsions.  First,  there  are  significant  differences  in  creative\nperformance among different models, even among those of\nthe same scale with an equal number of parameters. This\nvariation primarily exists between different types of mod-\nels. Their differences are reflected mainly in the model ar-\nchitecture, parameter settings during training, alignment\nstrategies,  and  the  datasets  used  for  training.  Addition-\nally, we observed that models generally excel in the elab-\noration metric, but tend to be less adept in demonstrat-\ning originality. In addition, the type of prompt and the\nspecific role-play request given to the model also plays a\nsignificant  role  in  influencing  its  creative  output.  When\nthe  models  are  given  instructive  prompts  or  chain-of-\nthought  prompts,  there  is  a  significant  increase  in  the\nlevel of creativity. Additionally, having LLM play differ-\nent roles leads to notable differences; the role of a scient-\nist  demonstrates  the  highest  level  of  creativity.  Many\nroles even show a decrease compared to the default scen-\nario, but there is generally an improvement in originality.\nThen,  collaboration  among  multiple  LLMs  can  enhance\nthe  level  of  creativity,  with  the  most  notable  improve-\nment in originality. Finally, the results of the psycholo-\ngical  scale  revealed  consistency  between  LLMs  and  hu-\nmans  in  terms  of  associated  creativity  factors,  such  as\nemotional  intelligence  (EI),  empathy,  self-efficacy,  and\nothers. \n 418 Ma\n chine Intelligence Research 22(3), June 2025\n \n\n2   Related works\n \n2.1   Creativity assessment in psychological\nresearch\nThe\n question  of  creativity  assessment  has  been  a\nprominent  focus  on  the  creativity  research,  especially\nsince  the  1950s,  marking  the  inception  of  a  systematic\nstudy into individual differences in creativity [19].\n  For ex-\nample, Guilford pioneered the research on creativity and\nhis famous structure of intellect model was mainly about\ndefining  and  analyzing  the  factors  constituting  intelli-\ngence, where creativity plays a major driving force in his\ntheory[20].\n  In  recent  years,  many  new  developments  re-\ngarding the measurement of divergent thinking, consensu-\nal assessment technique and subjective ratings, and self-\nreport  methodology[21–23] have\n  emerged.  Although  ad-\nvances  in  methodology  and  technology  have  led  to  im-\nportant  developments  regarding  creativity  assessment,\nsome  assessment  methods  have  long  been  described  as\n“gold  standard” for  creativity  assessment[24, 25].\n  Among\nthem,  TTCT[18] has  been  the  most  widely  used  and  re-\nsearched test of creativity, having extensive data to sup-\nport  its  reliability  and  validity.  Research  on  TTCT  re-\nports good reliability scores for scoring and test-retest re-\nliability[26].\nTTCT\n is  designed  to  identify  and  assess  an  indi-\nvidual′s  creative  potential  by  exploring  various  dimen-\nsions. Contrasting conventional assessments that emphas-\nize convergent thinking, the test fosters divergent think-\ning,  encouraging  participants  to  generate  multiple  solu-\ntions  to  open-ended,  ambiguous  problems.  TTCT  has\nbeen widely applied in educational settings, organization-\nal assessments, demonstrating its versatility and compre-\nhensive  approach  to  measuring  creativity.  Its  ability  to\ntap  into  various  facets  of  creative  thinking  has  made\nTTCT a reliable and respected tool [27].\n  Owing to the au-\nthority  and  comprehensiveness  of  the  TTCT,  we  select\ntasks from the TTCT to construct our dataset. \n2.2   Creativity and personality: Findings in\npsychological research\nResearch\n has  revealed  that  creativity  is  not  solely  a\nfixed human personality trait. It evolves from a combina-\ntion  of  individual  processes  such  as  cognitive,  affective,\nbehavioral,  and  contextual  factors.  Some  psychologists\nhave conducted a detailed meta-analysis of papers explor-\ning  the  relationship  between  creativity  and  various  per-\nsonality traits[28, 29].\nThese\n studies′ results highlight a correlation between\ncreativity and a plethora of personal factors. Notably, ele-\nments such as emotional intelligence, divergent thinking,\nopenness  to  experience,  and  intrinsic  motivation  stand\nout  as  strong  influencers.  However,  factors  such  as  age,\nintelligence, and gender exhibit a relatively milder associ-\nation with creativity, signifying a varied spectrum of in-\nfluence  across  different  personal  traits.  Since  large  lan-\nguage models have exhibited some personality traits, we\nconducted experiments to test whether these findings also\nhold true in LLMs. \n2.3   Assessing the creativity of large lan-\nguage models\nThe\n emergence of abilities from LLMs continually sur-\npasses people′ s expectations, and the evaluation of vari-\nous  abilities  of  LLMs  has  received  widespread  atten-\ntion[30].  Currently,  most  evaluations  focus  on  the  ability\nof  LLMs  to  solve  tasks,  with  fewer  evaluations  combin-\ning aspects of psychology.\nAlthough  some  studies  have  focused  on  the  intersec-\ntion  of  LLM  with  psychology  and  cognitive  science[31],\nwork\n discussing the creativity of LLM is still in a relat-\nively  early  stage.  Current  studies  somewhat  focused  on\nexploring  the  creativity  of  LLMs,  primarily  from  the\nstandpoint  of  creativity  theory,  which  aims  to  elucidate\nthe definitions and challenges of applying creativity the-\nory  within  the  context  of  LLMs[32].\n  Some  initial  evalu-\nations  of  creativity  in  LLMs  have  also  been  underta-\nken[33–35].\n  However,  these  works  only  employed  simple\ntasks such as the alternative uses task (AUT) to assess\ncreativity,  and  the  lack  of  comparison  between  various\nLLMs limits the validity of their conclusions. It is worth\nmentioning  that  in  [36\n],  the  authors  used  the  standard\nTTCT  to  assess  GPT-4′s  creativity.  The  results  show\nthat GPT-4 achieved human top 1% levels in fluency and\noriginality,  along  with  a  high  score  in  flexibility.  This\nstudy leans more towards comparing advanced large lan-\nguage models (LLMs) with human benchmarks. The ori-\nginal  TTCT  test  protocol  does  not  seamlessly  adapt  to\nassessing  creativity  in  LLMs,  as  the  limited  sample  of\nquestions  could  induce  randomness  and  accidental  out-\ncomes, making hypothesis testing challenging when com-\nparing  different  models.  Furthermore,  expanding  the\nnumber of question sets leads to high time costs in hu-\nman-based evaluations.\nDue to the differences between humans and LLM, it is\nproblematic to directly use the TTCT′ s test protocol to\nbenchmark  LLMs′ creativity.  To  address  this  dilemma,\nwe  propose  a  new  framework  for  systematic  analysis\nLLM′s  creativity.  This  framework  comprises  carefully\ncrafted  metrics  used  in  TTCT  and  a  dataset  that  ac-\ncounts  for  seven  tasks.  We  will  dive  into  detail  of  the\nframework in Section 3. \n3   Overview of the framework\nIn\n this work, we design an overall framework to evalu-\nate  LLM′s  creativity,  as  shown  in Fig. 1.  First,  we  con-\nstructed  a  dataset  containing  700  questions  of  7  tasks\nY. Zhao et al. / Assessing and Understanding Creativity in Large Language Models 419 \n \n\nthat were derived and modified from the psychology scale\nof\n the TTCT and expanded the number of questions via\nGPT-4. We tested six models on four different criteria us-\ning  the  dataset  we  constructed.  Following  this,  we  con-\nducted a series of experiments on the creativity of LLMs\nwhen giving different types of prompts and assigning dif-\nferent roles to LLMs. Finally, we used the GPT-4 as the\nevaluator to obtain the performance results of the LLMs\nand  verify  the  consistency  of  the  LLM-based  evaluation\nwith humans. \n3.1   Dataset construction\nThis\n research utilized a modified version of the TTCT\nverbal test, which includes tasks based on verbal stimuli.\nThe  seven  selected  tasks:  1)  Unusual  uses,  2)  con-\nsequences,  3)  just  suppose,  4)  situations,  5)  common\nproblems,  6)  improvements,  and  7)  imaginative  stories,\nwere  chosen  to  capture  a  broad  spectrum  of  creative\nthinking  abilities.  These  tasks  are  adapted  from  the\nwidely used TTCT, which has also served as the basis for\nrecent work in the field of LLM evaluation [37].\n  The tasks\nwe choose align with widely accepted models of creativ-\nity such as Guilford′ s structure of the intellect model and\ninvolve both divergent and convergent thinking [20].  Mean-\nwhile, TTCT tasks, especially in their divergent thinking\nfocus, align with the Geneplore model [38] by emphasizing\nidea  generation  (fluency  and  originality)  and  flexibility\n(the  ability  to  shift  between  categories  or  approaches).\nThus,  the  tasks  capture  both  novelty  and  usefulness,\nwhich are central to most modern definitions of creativ-\nity.  This  makes  them  sufficient  for  assessing  a  holistic\nview of creative potential.\nSpecifically, each task includes one hundred questions\ngenerated by GPT-4 using few-shot prompts. The seven\ntasks were generally structured as follows:\n1) Task 1: Unusual uses. This task challenges indi-\nviduals in their ability to think of as many unusual and\ndiverse uses as possible for a common object within a lim-\nited time frame. The object in question is typically every-\nday and familiar, such as a brick, paper clip, or newspa-\nper.\n2) Task 2: Consequences. This task focuses on the\nability to foresee consequences or outcomes of an unusual\nor  hypothetical  situation.  For  example,  what  would  be\nthe implications if animals could talk?\n3) Task 3: Just suppose. This task encourages ima-\nginative and speculative thinking by asking participants\nto consider hypothetical, often fantastical, scenarios and\ntheir  implications.  For  example,  just  suppose  you  woke\nup  one  morning  and  found  you  could  fly.  What  would\nyou do? List as many things as you can think of.\n4) Task 4: Situation task. This task is designed to\nassess creative thinking by evaluating how individuals re-\nspond to and interpret a given situation. This task em-\nphasizes  understanding  social  dynamics,  empathy,  and\nthe ability to consider multiple perspectives or solutions.\nFor example, if all books were to disappear, how would\nyou gain knowledge?\n5) Task 5: Common problem. This task focuses on\neveryday problems that are familiar to most people, re-\nquiring participants to generate innovative and effective\n \nJudger: GPT-4 & human \n···\nUnusual uses task\nQ: Please list unusual uses of plastic\nbottle.\nConsequences task\nQ: What would happen if we could\ntime travel?\nSituation task\nQ: If the sun didn't rise tomorrow, how\nwould you ensure you had enough light\nduring the day?\nQuestion \ngenerating\nLLM\nRole play\nAct like a typical primary school student. Do \nfollowing task or answer following question…\nStudent\nAct like a typical natural scientist. Do following \ntask or answer following question…\nScientist\nAct like a typical music artist. Do following task \nor answer following question…\nArtist\nBasic prompt\nCommon problems task. The scenario is: \nPlanning a birthday party for a 5-year-old.\nInstructive prompt\nCommon problems task. There is no right or \nwrong answers, we're interested in how many \ndifferent problems you can identify and the \nvariety of issues you consider. Try to think \noutside the box and consider as many potential \nproblems as possible. The scenario is: Planning a\nbirthday party for a 5-year-old.\nChain of thought (CoT) prompt\nCommon problems task. Let's think step by step. \nThe scenario is: Planning a birthday party for a \n5-year-old.\nPrompt type\n7 tasks · 700 questions\n···\nGPT-3.5\nLlama-2-13b\nVicuna-7b\nVicuna-13b\nQwen\nLLMs\nLlama-2-70b\nTorrance® tests of creative thinking\nFlexibility\nOriginality\nThe uniqueness of the ideas generated.\nOriginal ideas are those that are rare or\nunconventional, differing from the norm. \nElaboration\nThe ability to expand upon, refine, and\nembellish an idea. It i\nnvolves adding details,\ndeveloping nuances, and building upon a\nbasic concept to make it more intricate or\ncomplex.\nFluency\nThe ability to produce a significant number\nof relevant ideas in response to a given\nquestion. In essence, fluency measures the\nquantity of ideas.\nCriteria\nCreative questions\nThe variety of categories from which one can \ngenerate ideas. It's the ability to think of alter-\nnatives, shift from one class or perspective to \nanother, and to approach a given problem or \ntask from different angles. \n \nFig. 1     Overview  of  the  creativity  assessment  framework.  A  TTCT-inspired  dataset  was  constructed  to  evaluate  LLMs  under  varied\nprompts and role-play settings. GPT-4 served as the evaluator to score model outputs.\n 420 Ma\nchine Intelligence Research 22(3), June 2025\n \n\nsolutions.  For  example,  organizing  a  cross-country  road\ntrip or building a tree house.\n6\n) Task 6: Improvement. This task focuses on as-\nsessing an individual′ s ability to enhance or modify exist-\ning objects or ideas. The given object is similar to the un-\nusual uses task.\n7)  Task  7:  Imaginative  stories. This  task  is  de-\nsigned  to  assess  creativity  through  narrative  and\nstorytelling  with  a  given  prompt.  This  task  emphasizes\nthe ability to construct original, coherent, and imaginat-\nive  stories,  showcasing  an  individual′s  creative  potential\nin terms of narrative ability. Examples of given prompts\nare “The Invisible Elephant” or “The Book that Wrote\nItself”.\nEach task includes 100 questions generated by GPT-4\nvia few-shot prompts. GPT-4 can generate a diverse and\ncomprehensive set of similar problems based on the given\nexamples,  and  all  problems  have  been  validated  by  hu-\nmans to ensure usability. In addition, we conducted ex-\nperimental validation of domain generality across differ-\nent  tasks.  Cronbach′s  Alpha  and  inter-task  correlations\nindicate that our task selection is effective and sufficient. \n3.2   Evaluation criteria\nTo\n provide a comprehensive evaluation of an individu-\nal′s  creative  abilities,  we  should  consider  not  only  the\nquantity of ideas they produce, but also the quality, di-\nversity, and depth of those ideas. We have four criteria\nfor creativity evaluation:\n1) Fluency.  This  refers  to  the  ability  to  produce  a\nsignificant number of relevant ideas in response to a giv-\nen question. In essence, fluency measures the quantity of\nideas.\n2) Flexibility. This assesses the variety of categories\nfrom  which  one  can  generate  ideas.  It  is  the  ability  to\nthink of alternatives, shift from one class or perspective\nto another, and to approach a given problem or task from\ndifferent angles.\n3) Originality. This measures the uniqueness of the\nideas generated. Original ideas are rare or unconvention-\nal, differing from the norm.\n4) Elaboration. This refers to the ability to expand\nupon, refine, and embellish an idea. It involves adding de-\ntails,  developing  nuances,  and  building  upon  a  basic\nconcept to make it more intricate or complex.\nThese criteria aim to provide a comprehensive assess-\nment  of  an  individual′s  creative  potential.  The  motiva-\ntion behind using these specific dimensions is grounded in\nthe theoretical and empirical research on creativity [39, 40],\nwhich\n suggests  that  creative  thinking  involves  not  just\nthe generation of new ideas but also the ability to manip-\nulate, refine, and apply these ideas effectively. The four\ncriteria  are  based  on  long-standing  psychological  frame-\nworks  for  creativity  assessment,  particularly  the  TTCT.\nThese  dimensions  collectively  capture  distinct  and  com-\nplementary facets of creative thinking and have been ex-\ntensively  validated  in  psychological  and  educational  re-\nsearch and are considered gold standards in creativity as-\nsessment. \n3.3   LLM-based evaluation\nStandard\n TTCT  evaluation  methods  require  trained\npsychologists to follow professional manuals to assess the\nresults, and an individual′ s single test only contains an-\nswers to a very limited number of questions. When evalu-\nating creativity in LLM, both the insufficient sample of\nresponses and the high human resource costs limit the ap-\nplication of creativity tests on LLMs. Recent psychologic-\nal research has focused on the automated assessment of\ncreativity[41, 42].\n  However, these methods often have limit-\nations, such as being tailored to specific tasks or requir-\ning prepared reference answers, which prevent their gen-\neralization to a variety of tasks and a larger number of\nquestions.\nWith the rapid development of LLM capabilities, the\nevaluation  methods  for  many  natural  language  pro-\ncessing  tasks  have  evolved  from  traditional  human  an-\nnotation to reference-based automated methods, and now,\nto methods on the basis of LLMs. LLMs are increasingly\nplaying  the  role  of  judges  in  tasks  such  as  question-an-\nswering,  translation,  and  text  quality  assessment[43–46],\ngiving\n rise to various evaluation framework [47–49].  Accord-\ning to experimental results from relevant literature, LLM\nexhibits higher correlation with human evaluations com-\npared  with  traditional  automated  technologies[50, 51].\n  In\nthis  study,  on  the  basis  of  the  evaluation  criteria  from\nSection  3.2,  we  utilize  GPT-4  to  score  the  answer.  For\neach  criterion,  the  LLM  needs  to  complete  the  Likert\nscale based on the responses. Additionally, we verified the\nconsistency  between  the  evaluations  made  by  LLM  and\nhuman evaluations. \n4   Evaluation and results\nWe\n conducted  a  statistical  analysis  of  the  creativity\nscores of 6 popular LLMs across seven tasks, totaling 700\nquestions.  We  unveiled  hidden  conclusions  within  the\ndata results from various dimensions. We compared the\ndifferences  in  creativity  levels  between  the  models,  and\nwe compared the performance variations under different\ncriteria within the same model. Subsequently, we experi-\nmented  with  many  types  of  prompts  to  see  whether\nchanges in prompts would affect the models′  levels of cre-\nativity. Since LLMs possess the ability to play user-spe-\ncified roles, we select six typical human identities to ex-\nplore  the  impact  on  creativity  under  different  role-play-\ning  conditions.  Finally,  we  utilize  some  psychological\nscales  to  test  the  LLMs,  investigating  the  correlation\nbetween the personality traits of the LLMs and creativ-\nity. \nY. Zhao et al. / Assessing and Understanding Creativity in Large Language Models 421 \n \n\n4.1   Experimental settings\n \n4.1.1   Tested models\nWe\n tested six of the most advanced LLMs, which are\nlisted below. All the models were implemented with the\nopen-source repository HuggingFace[52].\n1\n)  GPT-3.5. GPT-3.5  is  a  language  model  de-\nveloped by OpenAI, which is an advanced version of the\nGPT-3  model.  It  is  capable  of  generating  natural  lan-\nguage text and code. GPT-3.5 was trained on an Azure\nAI supercomputing infrastructure. The versions we used\nin the experiments are GPT-3.5-turbo-0613.\n2) LLaMA-2. LLaMA-2  is  a  family  of  state-of-the-\nart  open-access  large  language  models  released  by  Meta\nand  Microsoft[2].\n  It  is  built  upon  success  of  its  prede-\ncessor, LLaMA-1. LLaMA-2 is specifically designed to fa-\ncilitate  the  development  of  generative  AI-powered  tools\nand experiences. It is available for free research and com-\nmercial use. LLaMA-2 release introduces a family of pre-\ntrained and fine-tuned LLMs, ranging in scale from 7 B to\n70 B  parameters.  The  versions  we  used  in  the  experi-\nments are LLaMA-2-13b-chat-hf and LLaMA-2-70b-chat-\nhf.\n3) Vicuna. Vicuna is a lightweight, accurate, and ef-\nficient language model developed by a team of research-\ners  from  several  universities,  including  UC  Berkeley,\nCarnegie Mellon University, Stanford University, and UC\nSan Diego[44].\n  It was built from Meta′ s adaptable LLaMA\nmodel, which was fine-tuned on a dataset of around 70 000\nhuman-generated conversations from the ShareGPT web-\nsite. The versions we used in the experiments are Vicuna-\n7b-v1.5 and Vicuna-13b-v1.5.\n4)  Qwen. Qwen  (abbr.  Tongyi  Qianwen),  proposed\nby Alibaba Cloud [53].  It is a transformer-based large lan-\nguage  model,  which  is  pretrained  on  a  large  volume  of\ndata, including web texts, books, codes, etc. The versions\nwe used in the experiments are Qwen-7b-chat. \n4.1.2   Details of hyperparameters\nThe\n models used in our experiment primarily origin-\nate from the open-source HuggingFace platform. The spe-\ncific versions of these models have already been reported\nabove. In this section, we present the experimental para-\nmeters and other settings related to the experiment.\nFor  an  LLM  based  on  the  transformer  architecture,\nthere are certain parameters that directly affect the out-\nput of the model.\n1) Max tokens. This parameter controls the maxim-\num number of tokens to generate in the chat completion.\nIn our experiment, this value is uniformly set to 512, en-\nsuring that the output length is sufficient to maintain the\nquality of the answers.\n2) Temperature. The parameter is a crucial factor\nin determining the nature of the model′ s responses. This\nis  a  hyperparameter  that  influences  the  randomness  or\nunpredictability in the model′ s responses. Essentially, its\nmechanism  is  to  change  the  probability  distribution  of\nthe model′s output logits. However, according to our ex-\nperiments, changes in temperature do not significantly af-\nfect  creative  performance,  which  appears  quite  random.\nTherefore,  in  our  experiments,  the  temperature  is  uni-\nformly set to 1.\n3) Top_p. Top_p is also a parameter used to control\nthe diversity of the generated text, also known as “nucle-\nus sampling”. This parameter′ s full name is “top probab-\nility”, which is typically represented by a value between 0\nand 1, indicating the cumulative threshold of the highest\nprobabilities chosen in the probability distribution when\ngenerating the next token. In our experiments, top_p is\nuniformly set to 1.\n4)  Top_k. This  parameter  is  used  when  generating\nthe  next  token  to  limit  the  model  to  consider  only  the\ntop_k tokens with the highest probability. This strategy\ncan reduce the likelihood of the model generating mean-\ningless  or  repetitive  outputs,  while  also  improving  the\nspeed and efficiency of the model generation. In our ex-\nperiments, the top_k is uniformly set to 50.\nGPT-4 serves as the judge for our LLM-based evalu-\nation,  with  its  relevant  parameters  set  to  default.  The\nversion used is GPT-4-0613. In addition, all prompt tem-\nplates  used  in  the  experiment  are  provided  in  the  ap-\npendix. \n4.2   Results of different models and criteria\nWe\n assessed the responses of six language models to\n700 questions, with GPT-4 serving as the evaluator across\nall  creativity  dimensions.  We  first  evaluate  the  average\nscore of each model across all tasks, as shown in Fig. 2\n(a)\nand Table  1.  It  can  be  observed  that  GPT-3.5  has  the\nhighest\n level of creativity, followed by the LLaMA-2 ar-\nchitecture  models,  then  the  LLaMA-based  fine-tuned\nmodel  vicuna,  and  finally  Qwen.  The  experimental  res-\nults  from  the  perspective  of  the  model  suggest  that  the\ntype of model has a significant effect on creativity, where-\nas the scale of parameters does not have a decisive influ-\nence.  Different  types  of  models  vary  in  their  architec-\ntures, alignment strategies, and the datasets used during\ntraining. These factors are likely to be key determinants\nof the level of creativity. Similar findings can also be ob-\nserved  in  other  LLM  evaluation  papers[54–56].\n  For  ex-\nample, in Toolbench [56],  the 30 B version of LLaMA out-\nperforms the 65 B version of LLaMA in many tasks, and\ntext-daVinci-003 also performs better overall than GPT-\n3.5.\ny\nx\nTo further validate the ranks of the models, we con-\nducted\n pairwise  comparisons  between  the  models,  as\nshown in Fig. 2(b). Each cell in this heatmap represents\nthe win rate of the model on the -axis in terms of cre-\nativity score compared to the model on the -axis. The\nwin  rate  scores  are  consistent  with  the  strengths  and\nweaknesses of the models shown in Fig. 2\n(a), and we con-\nducted statistical tests for significance, which are marked\n 422 Machine Intelligence Research 22(3), June 2025\n \n\n \nTable 1    Comparative creativity scores across LLMs\nFluency Flexibility Originality Elaboration\nCommon problem task\nGPT\n-3.5 4.975 4.650 3.870 4.735\nLLaMA\n-2-13b 4.940 4.480 3.770 4.890\nLLaMA-2-70b 4.920 4.545 3.720 4.905\nQwen\n3.090 2.890 2.360 3.360\nVicuna-13b 4.910 4.320 3.510 4.415\nVicuna-7b 4.880 4.270 3.380 4.200\nConsequences task\nGPT-3.5 4.855 4.810 4.105 5.000\nLLaMA\n-2-13b 4.910 4.830 4.080 5.000\nLLaMA\n-2-70b 4.930 4.830 3.995 4.995\nQwen 4.410 4.430 3.610 4.875\nVicuna-13b 4.260 4.295 3.580 4.850\nVicuna-7b 4.535 4.435 3.660 4.920\nImprovement task\nGPT-3.5 5.000 4.970 4.620 4.980\nLLaMA\n-2-13b 4.980 4.850 4.150 4.890\nLLaMA-2-70b 4.965 4.800 4.085 4.900\nQwen 4.870 4.550 3.760 4.700\nVicuna-13b 4.970 4.410 3.600 4.380\nVicuna-7b 4.950 4.560 3.860 4.640\nImaginative stories task\nGPT-3.5 4.160 4.200 4.475 4.925\nLLaMA\n-2-13b 3.720 3.620 4.030 4.730\nLLaMA-2-70b 3.830 3.660 4.050 4.700\nQwen 3.240 3.510 3.740 4.430\nVicuna-13b 3.310 3.610 3.750 4.490\nVicuna-7b 3.280 3.470 3.760 4.580\nJust suppose task\nGPT-3.5 3.960 4.310 4.030 4.930\nLLaMA\n-2-13b 3.830 4.160 4.040 4.930\nLLaMA\n-2-70b 3.795 4.090 3.750 4.870\nQwen 3.580 3.840 3.250 4.640\nVicuna-13b 3.410 3.580 3.030 4.550\nVicuna-7b 3.480 3.860 3.240 4.600\nSituation task\nGPT-3.5 4.790 4.670 3.940 4.970\nLLaMA-2-13b 4.195 4.390 3.920 4.850\nLLaMA-2-70b 4.850 4.800 4.050 4.990\nQwen\n3.940 4.010 3.170 4.590\nVicuna-13b 3.970 3.970 3.140 4.600\nVicuna-7b 4.020 3.980 3.210 4.620\nY. Zhao et al. / Assessing and Understanding Creativity in Large Language Models 423 \n \n\nin Fig. 2.\nNext,\n we evaluate the average scores of each criterion\nacross all tasks, as shown in Fig. 2(d). The score for elab-\noration is consistently high across all tasks, while original-\nity is relatively lower, with fluency and flexibility scoring\nin the middle. The capabilities of LLMs inherently stem\nfrom training on human language corpora, so it is intuit-\nive that they score relatively lower in originality. The cre-\nativity of LLM is likely to be a manifestation of the com-\nbination  of  existing  human  knowledge,  and  how  to  im-\nprove the originality of LLM is an important future en-\ndeavor. The elaboration metric reflects the degree of re-\nfinement of a creative idea, and LLM′ s ability to articu-\nlate this has always been outstanding.\nHowever, if we focus on the performance of different\ntasks, we will find that there are significant variances in\nTable 1 (continued) Comparative creativity scores across LLMs\nFluency Flexibility Originality Elaboration\nUnusual uses task\nGPT\n-3.5 5.000 4.920 4.670 4.895\nLLaMA\n-2-13b 4.990 4.860 4.280 4.910\nLLaMA\n-2-70b 4.980 4.850 4.255 4.880\nQwen 4.905 4.210 3.690 4.130\nVicuna-13b 4.860 4.060 3.670 3.760\nVicuna-7b 4.910 4.640 3.940 4.300\n \nModels\n0\n1\n2\n3\n4\nCreativity score\n0.2\n0.4\n0.6\n0.8\n1.0\n4.8\n4.6\n4.4\n4.2\n4.0\n5.0\nCreativity score\nContextual relevance \nCoherence and consistency\nFluency Flexibility OriginalityElaboration\nCriteria\n0\n1\n2\n3\n4\nCreativity score\n*** *\n***\n***\n***\n***\n*** ***\nGPT-3.5\nLLaMA-2-70bLLaMA-2-13b\nVicuna-7bVicuna-13b\nQwen\nModels\nGPT-3.5\nLLaMA-2-70bLLaMA-2-13b\nVicuna-7bVicuna-13b\nQwen\nGPT-3.5\nLLaMA-2-70bLLaMA-2-13b\nVicuna-7bVicuna-13b\nQwen\nGPT-3.5\nLLaMA-2-70b\nLLaMA-2-13b\nVicuna-7b\nVicuna-13b\nQwen\n(a)\n(c) (d)\n(b)\n1.000 0.660 0.640 0.910 0.900 0.890\n0.340\n1.000 0.490 0.800 0.820 0.780\n0.360 0.510 1.000 0.830 0.830 0.800\n0.086 0.200 0.170 1.000 0.500 0.410\n0.100 0.180 0.170 0.500 1.000 0.420\n0.110 0.220 0.200 0.590 0.580 1.000\n \nFig. 2     Creativity  performance  of  different  LLMs  across  models  and  criteria.  (a)  Overall  creativity  scores  with  error  bars  showing\nstandard deviations. Significance is marked using the Wilcoxon signed-rank test. (b) Pairwise win rate heatmap. (c) Scores for relevance\nand  consistency.  (d)  Average  scores  across  four  creativity  dimensions.  (Colored  figures  are  available  in  the  online  version  at\nhttps://link.springer.com/journal/11633)\n 424 Machine Intelligence Research 22(3), June 2025\n \n\ncreativity performance under different tasks, as shown in\nFig. 3, which shows the radar charts of the performance of\nsix\n models  across  seven  tasks.  It  can  be  observed  that\nmost models exhibit a higher level of overall creativity in\nthe  common  problem,  consequences,  and  unusual  uses\ntasks, while the overall creativity level is lower in the just\nsuppose and imaginative stories tasks, reflecting the vary-\ning  degrees  of  creative  difficulty  presented  by  different\ntasks.\nAt  last,  there  are  some  differences  between  humans\nand  LLMs  when  answering  questions.  In  the  case  of\nLLMs responding to human prompts, issues such as irrel-\nevance  to  the  topic  or  logical  errors  may  arise.  On  the\nother  hand,  humans  generally  maintain  consistency  in\ntheir answers. So we have evaluated the responses of all\nmodels in this regard, and it is observable that there are\nsignificant differences in relevance and coherence among\nthe  various  models,  as  shown  in Fig. 2\n(c).  The  results\nshow  that  the  GPT-3.5  and  LLaMA  models  performed\nwell, while the Vicuna and Qwen models had poorer per-\nformance.  Sometimes,  Vicuna  and  Qwen  fail  to  under-\nstand  the  question  properly,  leading  to  irrelevant  an-\nswers. Sometimes, due to a misunderstanding of the ques-\ntion,  they  refuse  to  answer.  We  all  consider  these  as\nmanifestations of a lack of creativity. This issue may be\nrelated  to  the  alignment  strategies  employed  and  the\ntraining datasets of the models. \n4.3   Results of different prompt types\nThe\n prompt is a crucial component of the LLM model,\nas it provides the necessary context and information for\nLLMs to generate a relevant and coherent response. The\nquality and type of prompt can significantly impact the\nquality  of  the  generated  response.  Therefore,  we  believe\nthat the type of prompt can greatly influence the creativ-\nity of LLMs.\nIn  our  experiment,  we  designed  and  compared  four\ndifferent  types  of  prompts:  basic  prompt,  instructive\nprompt,  post-instructive  prompt  and  chain  of  thought\n \nGPT-3.5\nQwen\nV\nicuna-7b\nVicuna-13b\nLLaMA-2-13b\nLLaMA-2-70b\nElaboration\nFluency\nOriginality\n3.5\n4.0\n4.5\n5.0\n4.2\n4.4\n4.6\n4.8\n5.0\n4.2 4.4 4.6 4.8 5.03.5\n4.0\n4.5\n5.0\n3.5\n4.0\n4.5\n5.0\n4.0\n4.5\n5.0\n4.5\n5.0\nJust suppose Imaginative stories\nImprovement\nConsequences\nCommon problem\nSituation\nUnusual\nJust suppose Imaginative stories\nImprovement\nConsequences\nCommon problem\nSituation\nUnusual\nJust suppose Imaginative stories\nImprovement\nConsequences\nCommon problem\nSituation\nUnusual\nJust suppose Imaginative stories\nImprovement\nConsequences\nCommon problem\nSituation\nUnusual\n3.5\n4.0\n4.5\n5.0\n4.2\n4.4\n4.6\n4.8\n5.0\n4.6\n4.8\n5.03.5\n4.0\n4.5\n5.0\n4.0\n4.5\n5.0\n4.0\n4.5\n5.0\n4.8\n5.0\n3.5\n4.0\n4.5\n5.0\n4.2\n4.4\n4.6\n4.8\n5.0\n4.2 4.4 4.6 4.8 5.03.5\n4.0\n4.5\n5.0\n3.5\n4.0\n4.5\n5.0\n4.0\n4.5\n5.0\n4.0\n4.5\n5.0\n3.0\n3.5\n4.0\n4.5\n5.0\n4.0\n4.5\n5.0\n4.0 4.5 5.0\n4.0\n4.5\n5.0\n3.5\n4.0\n4.5\n5.0\n3.54.04.55.0\n4.0\n4.5\n5.0\nFlexibility\n \nFig. 3     Radar comparison of LLM creativity across tasks (Colored figures are available in the online version at https://link.springer.\ncom/journal/11633)\nY\n. Zhao et al. / Assessing and Understanding Creativity in Large Language Models 425 \n \n\n(CoT)  prompt.  Herein,  the  basic  prompt  contains  only\nthe\n essential  information  needed  to  describe  the  task,\nsimple and clear. The instructive prompt provides a de-\ntailed description of the expected answer, outlining what\nconstitutes  a  creative  response.  The  post  instruction\nprompt uses two rounds of prompts, starting with a ba-\nsic prompt for the LLM to give a basic answer, then giv-\ning  some  instruction  about  creativity  (the  same  as  in-\nstructive prompt). LLM revises the answer given in the\nfirst round on the basis of the given instruction and gives\nthe revised answer. Chain of thought is a technique that\nenables complex reasoning capabilities through intermedi-\nate  reasoning  steps  or  just  a  single  explicit  prompt  like\n“let′s think step by step”. We utilize the technique used\nin  [57]  to  design  our  CoT  prompt.  The  example  of\nprompts is shown in Fig. 1\n.\nAs shown in Figs. 4(a)–4(c), we obtained data on the\nperformance  of  LLMs  in  terms  of  creativity  across  all\ntasks and all criteria under different prompt types. From\nthe  perspective  of  the  task,  the  inclusion  of  instructive\nlanguage in prompts has improved creativity in all tasks\nexcept for “unusual uses”. The reason for the lack of im-\nprovement  in “unusual  uses” may  be  that  the  task  de-\nscription is already clear enough and the required diver-\ngent thinking ability is relatively simple. When the CoT\nprompt is used, there has been an increase in the level of\ncreativity  in  three  tasks,  indicating  that  some  tasks  re-\nquire  a  higher  level  of  convergent  thinking  ability  to\ndemonstrate  creativity.  In  the  case  of  post  instructions,\nthe greatest differences were shown between tasks. While\na few tasks, such as imaginative stories and just suppose,\nshowed some rise, most of the rest did not have a signific-\nant boosting effect, and even produced a drastic drop on\nthe unusual uses task. We speculate that the main reas-\non may be that under multiple rounds of dialog, the post-\ninstruction actually implicitly negates the initial response,\nresulting in the inability to be in a position to come up\nwith a more creative response on a relatively simple task,\nsuch as unusual uses. From the perspective of creativity\ncriteria, instructive prompts clearly significantly enhance\nboth flexibility and originality but do not increase elabor-\nation. On the other hand, CoT prompts slightly improve\nelaboration. Both types of prompts are beneficial to the\nfluency  of  the  responses.  In  summary,  the  creative  per-\nformance  of  LLMs,  like  their  other  abilities,  is  signific-\nantly influenced by the prompts. Effective prompt engin-\neering  is  greatly  beneficial  for  better  harnessing  the  po-\ntential  creativity  of  LLMs.  For  the  post  instruction\nprompt, only originality criteria have an obvious increase,\nand even a significant decrease in the fluency and flexibil-\nity criteria. For the same reason as previously stated, the\nsecond  round  of  the  responses  will  naturally  negate  the\ninitial responses, resulting in a lack of flexibility and flu-\nency in the final answer. \n4.4   Results of playing different roles\nLLMs\n possess the remarkable capability to adopt the\nroles specified by users, which can subsequently influence\ntheir outputs. This adaptability enables the models to de-\nliver  tailored  responses,  aligning  with  the  context  and\ncharacteristics  of  the  assumed  identities.  In  our  experi-\nment, we attempted to specify the exact identity and role\nof the LLM within the system prompt. The primary ob-\njective  of  this  approach  was  to  ascertain  whether  the\nLLM  could  enhance  its  creative  expression  by  adopting\nspecific roles and to determine if this influence is consist-\nent with the cognitive patterns observed in reality.\nAs shown in Fig. 4\n(d), we assigned six distinct roles to\nthe  LLM:  engineer,  farmer,  merchant,  scientist,  artist,\nand primary school student, requiring the model to per-\nform tasks in alignment with the characteristics of the re-\nspective  roles.  The  results  demonstrated  that  across  all\ncreativity  assessment  criteria,  the  creativity  level  of  the\nscientist surpassed that of the other six roles, reflecting a\ncorrelation between the accumulation of knowledge, edu-\ncational attainment, and the level of creativity. Further-\nmore, when the LLM was playing different roles than the\nscientist  was,  the  values  of  fluency  and  flexibility  have\ndecreased, yet originality has increased significantly. This\nsuggests that giving LLM specific roles induces more ori-\nginal responses. This experiment reveals the weakness of\nLLM′s lack of originality in its default situation. \n4.5   Results of creativity under collabora-\ntion\nIn\n reality,  creative  activities  can  be  accomplished\nthrough collaboration and discussion among multiple in-\ndividuals. The literature indicates that the process of cre-\native collaboration can increase the innovativeness of the\noutcomes[58, 59].\n  Inspired by this, we believe that the res-\nults produced through the collaboration with LLMs have\nstronger creativity than those generated by a single LLM.\nBased on the above analysis, this section explores the\nuse  of  multiple  agents  engaging  in  multi-round  discus-\nsions  on  questions  from  the  dataset,  ultimately  produ-\ncing  a  joint  final  answer.  After  the  previous  LLM\nprovides an answer, the subsequent LLM will use that an-\nswer as inspiration to give its own response. Once a pre-\ndetermined number of rounds is reached, the final result\nis  presented.  In  our  experiment,  using  GPT-3.5  as  the\nbase model, we explored the changes in scores under dif-\nferent creativity criteria when the number of LLMs is 2\nand 3 (we call it agent) and the number of rounds is 2\nand  3.  We  compared  these  scores  with  the  creativity\nscores obtained under default conditions.\nAs shown in Fig. 5\n, we presented scatter plots of the\ncreativity  scores  under  different  criteria,  varying  by  the\nnumber  of  rounds  and  agents.  The  area  of  each  scatter\n 426 Machine Intelligence Research 22(3), June 2025\n \n\npoint represents the level of creativity. From the results,\nwe\n can  see  some  interesting  findings:  First,  when  the\nnumber  of  rounds  is  one,  an  increase  in  the  number  of\nagents leads to a decrease in the level of creativity across\n \nTask name Instructive\nprompt\nCoT\nprompt\nUnusual uses task −\n−\nConsequences task  *  ***\nJust suppose task  ***  ***\nSituation task  ***  ***\nCommon problem task  * −\nImprovement task  * −\nImaginative stories task  ***\nCriteria Instructive\nprompt\nCoT\nprompt\nFluency  ***  *\nFlexibility  *** −\nOriginality  *** −\nElaboration −  *\nFluency Flexibility Originality Elaboration\nStandard\n3.00\n3.25\n3.50\n3.75\n4.00\n4.25\n4.50\n4.75\n5.00 Engineering \nFarmer \nMerchant\nMusic artist  \nNatural scientist\nPrimary school student\nFluency Flexibility Originality Elaboration\nCriteria\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nCreativity scoreCreativity scoreCreativity score\nBasic \nCoT\nInstruction\nPost instruction\nCommon\nproblem\nConsequences Improvement Just\nsuppose\nSituation Unusual\nuses\nImaginative\nstories \nModels\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0 Basic \nCoT\nInstruction\nPost instruction\n(a)\n(b)\n(c)\n(d)\n–\n  √\n∗ ∗ ∗ p ∗ p\nFig. 4     Effects of prompt types and role-play settings on LLM creativity. (a) Prompt type impact across tasks and criteria;  “ ”  denotes\nsignificant improvement, “–”  denotes no effect. Significance is marked (  for   <  0.000 1,    for   <  0.05; Wilcoxon signed-rank test).\n(b) Creativity scores across criteria by prompt type. (c) Creativity across tasks by prompt type. (d) Role-based performance across all\ntasks;  the  horizontal  line  indicates  baseline  without  role-play.  (Colored  figures  are  available  in  the  online  version  at  https://link.\nspringer.com/journal/11633)\nY. Zhao et al. / Assessing and Understanding Creativity in Large Language Models 427 \n \n\nthe four criteria. This might be due to the lack of mul-\ntiple\n reviews of the answers by the same agent in a single\nround  of  interaction,  leading  to  the  answers  of  later-\nranked agents constantly negating previous answers. Ad-\nditionally, in the cases of originality, flexibility, and elab-\noration, an increase in both rounds and agents enhances\nthe level of creativity, with the most significant improve-\nment  observed  in  originality.  This  supports  the  conclu-\nsion that collaboration can enhance creativity and is con-\nsistent with human behavior. Lastly, there are some ex-\nceptions  to  the  above  conclusion,  such  as  a  decrease  in\nfluency when there are two agents and three rounds. This\ncould be due to excessive discussion that makes the an-\nswers overly concise. \n4.6   Investigation of the relationship\nbetween LLM′s creativity and its per-\nsonality traits\n \n4.6.1   Psychology scales\nIn\n this  experiment,  we  explored  the  relationships\nbetween  personality  traits  and  creative  performance  of\nsome large models, using some public psychological scales\nand related literature.\nWe  use  the  situational  test  of  emotion  management\n(STEM)  for  the  assessment  of  emotional  intelligence[60].\nSTEM\n evaluates  an  individual′s  ability  to  manage  emo-\ntions in various situations. It is based on the concept of\nemotional intelligence, which involves recognizing, under-\nstanding, and managing one′ s own emotions and those of\nothers. The test typically presents a series of hypothetic-\nal  scenarios  to  the  participants.  Each  scenario  is  de-\nsigned to assess different aspects of emotional intelligence,\nsuch  as  emotional  awareness  and  regulation.  The  parti-\ncipants are asked how they would respond to each situ-\nation.  Their  responses  are  then  analyzed  to  determine\ntheir EI levels. STEM is used in various settings, includ-\ning  organizational  training,  psychological  research,  and\npersonal development. It helps in identifying areas where\nemotional  intelligence  can  be  improved,  which  is  valu-\nable in both personal and professional contexts.\nWe use the Toronto empathy questionnaire (TEQ) for\nassessing  LLM′s  empathy  level[61].\n  The  TEQ  was  de-\nveloped by researchers at the University of Toronto. It is\ngrounded in the idea that empathy is a multi-dimension-\nal  construct,  involving  both  cognitive  and  affective  ele-\nments. The questionnaire consists of 16 items, each rated\non  a  5-point  Likert  scale.  These  items  are  designed  to\n \n3\n2\n2\n3\nFluency\nAgent\nRound\n3\n2\n2 3\nFlexibility\nAgent\nRound\n3\n2\n2 3\nOriginality\nAgent\nRound\n3\n2\n2 3\nElaboration\nAgent\nRound\n \nFig. 5     Effect of agent collaboration on creativity scores\n 428 Ma\nchine Intelligence Research 22(3), June 2025\n \n\nmeasure  the  respondent′s  emotional  and  cognitive  re-\nsponses  to  the  experiences  and  feelings  of  others.  The\nTEQ is used in various fields, including psychological re-\nsearch,  clinical  settings,  and  social  science  studies.  It\nhelps in understanding how individuals emotionally con-\nnect  with  others,  which  can  be  important  in  contexts\nsuch as therapy, counselling, and social work.\nWe  use  generalized  self-efficacy  scale[62] \n to  assess\nLLM′s  self-efficacy.  The  scale  was  developed  by  Ralf\nSchwarzer and Matthias Jerusalem in 1995. This is part\nof a larger body of research on self-efficacy and psycholo-\ngical  well-being.  The  generalized  self-efficacy  scale  is  a\nshort survey consisting of 10 items. The respondents rate\neach item on a scale, typically from 1 to 4, where higher\nscores  indicate  greater  self-efficacy.  Unlike  scales  that\nmeasure  task-specific  or  situation-specific  self-efficacy,\nthis scale assesses a general sense of personal competence\nto deal effectively with a variety of stressful situations. It\nis widely used in psychological research, clinical psycho-\nlogy, and health psychology. It′ s also utilized in organiza-\ntional  and  educational  settings  to  understand  and  en-\nhance  individuals′ beliefs  in  their  own  capabilities.  The\ngeneralized  self-efficacy  scale  has  been  validated  in  nu-\nmerous studies across different cultures and is known for\nits reliability and construct validity.\nFinally, we applied the classic big five inventory (BFI)\ntest  to  LLMs[63].\n  The  big  five  personality  traits  include\nopenness,  conscientiousness,  extraversion,  agreeableness,\nand  neuroticism  (often  abbreviated  as  OCEAN).  These\ntraits represent a broad range of human personality char-\nacteristics and are believed to be universal. The BFI typ-\nically  comprises  short  statements  that  respondents  rate\nbased on how accurately they reflect their own behavior\nor  personality  traits.  The  BFI  is  valued  for  its  balance\nbetween brevity and comprehensive coverage of the five-\nfactor  model.  This  demonstrates  good  reliability  and\nvalidity,  making  it  a  trusted  tool  in  personality  assess-\nment. \n4.6.2   Investigation results\nWe\n have  mentioned  that,  creativity  evolves  from  a\ncombination of individual processes such as cognitive, af-\nfective, behavioral, and contextual factors. In this section,\nwe subject LLMs to a series of psychometric tests tradi-\ntionally used to assess human personality traits. Our aim\nis to explore whether, akin to humans, there is a correla-\ntion between various personality factors and the creative\ncapabilities of these advanced computational systems.\nIn our experiment, we selected eight personality traits:\nemotional  intelligence,  empathy,  self-efficacy,  openness,\nconscientiousness,  extraversion,  agreeableness,  and  neur-\noticism. The latter five are the classic big five personal-\nity traits. The meta-analytic literature[28, 29] \n in the field of\npsychology suggests that each of these eight traits correl-\nates with levels of creativity.\nτ ρ\nAs  shown  in Table  2,  we  conducted  experiments  on\nLLMs\n and  reported  the  correlations  between  the  men-\ntioned  personality  traits  and  creativity.  We  chose  the\nKendall  and Spearman  as the correlation coefficients\nand performed hypothesis testing. The experimental res-\nults indicate that the levels of emotional intelligence, em-\npathy, conscientiousness, extraversion, and neuroticism in\nlarge language models have a significant positive correla-\ntion with creativity levels, whereas agreeableness shows a\nsignificant negative correlation. Apart from agreeableness\nand openness, the influence of the remaining personality\ntraits on creativity in large models is consistent with hu-\nman performance. \n4.7   Task domain generality evaluation\nFrom\n the perspective of domain generality [64],  creativ-\nity is viewed as a transferable skill that can be applied\nacross  different  fields  or  domains  (e.g.,  arts,  sciences,\nbusiness).  The  selected  tasks – unusual  uses,  con-\nsequences, just suppose, situation, common problem, im-\nprovement, and imaginative stories – are domain-general\nin  nature,  meaning  they  assess  creativity  without  being\ntied  to  any  specific  subject  matter  or  expertise.  These\ntasks are content-neutral and focus on core cognitive pro-\ncesses such as idea generation, problem-solving, and ima-\n \nτ ρ pTable 2    Reports of Kendall′s  , Spearman′s  , and  -values for correlations between selected personality\ntraits and LLM creativity. Significant results are highlighted in bold.\nEmotional intelligence Empathy Self-efficacy Openness\nτKendall \nCorrelation coefficient 0.382 5 0.382 5 0.440 1 0.032 9\np-value < 0. 000 1 < 0.000 1 < 0.000 1 0.605 4\nρSpearman \nCorrelation coefficient 0.502 6 0.502 6 0.561 3 0.052 6\np-value < 0. 000 1 < 0.000 1 < 0.000 1 0.536 9\nConscientiousness Extraversion Agreeableness Neuroticism\nτKendall \nCorrelation coefficient 0.369 1 0.263 6 –0.370 0 0.253 3\np-value < 0. 000 1 < 0.000 1 < 0.000 1 < 0.000 1\nρSpearman \nCorrelation coefficient 0.489 7 0.339 4 –0.496 7 0.345 5\np-value < 0. 000 1 < 0.000 1 < 0.000 1 < 0.000 1\nY. Zhao et al. / Assessing and Understanding Creativity in Large Language Models 429 \n \n\ngination,  which  are  fundamental  to  creativity  across  all\ndomains.\n Because they are not specialized in a particular\nfield (e.g., only artistic creativity or scientific innovation),\nthey can assess the general creative potential of an indi-\nvidual  that  applies  across  different  contexts.  This  do-\nmain-general approach ensures that the tasks are broadly\napplicable and can capture creative abilities that are rel-\nevant in multiple disciplines, making the assessment more\nholistic and inclusive. Thus, their domain generality con-\ntributes to the completeness of the task set, as it ensures\nthat  the  creativity  being  measured  is  not  limited  to  a\nsingle context but represents a more universal capability.\nα > 0.8\nTo  validate  the  domain  generality  under  our  experi-\nmental\n settings, we computed Cronbach′ s Alpha for each\ncreativity criterion across the tasks, with results showing\nhigh internal consistency (  for all criteria), indic-\nating that the tasks are sufficient for measuring domain-\ngeneral creativity, as shown in Fig. 6. The strong correla-\ntions\n between  tasks  across  different  models  further  sup-\nport the idea that the task set captures a shared underly-\ning  construct,  ensuring  that  the  evaluation  provides  a\nholistic view of the models′ creative capabilities.\n \n \nCronbach′s α for each task\nCronbach′s α\nAcceptable threshold (0.7)0.982 0.937\n0.863\n0.948 0.944\n0.957 0.895\n1.0\n0.8\n0.6\n0.4\n0.2\n0\nCP Cons IM IS JS Sit Unu\n \nαFig. 6     Bar  chart  showing  Cronbach′ s    values  for  each  task,\nwith  the  acceptable  threshold  (0.8)  indicated  by  a  red  dashed\nline.  All  tasks  exceed  this  threshold.  (Colored  figures  are\navailable  in  the  online  version  at  https://link.springer.com/\njournal/11633)\n \nWhat′s\n more,  we  do  inter-task  correlation  analysis\nbetween the scores of the 7 tasks across the 6 models. As\nshown in Fig. 7, results suggest that all tasks measure a\nshared,\n domain-general aspect of creativity. In summary,\nthe TTCT tasks can be justified as sufficiently broad and\nrepresentative  of  core  creative  processes  such  as  diver-\ngent  thinking,  originality,  and  flexibility.  However,  they\nare not exhaustive, and their sufficiency largely depends\non the specific definition of creativity being employed. \n4.8   Evaluating reliability of GPT-4 as a\njudge\nAs\n in  the  experiments  above,  GPT-3.5  generally\nachieves  high  creativity  performance  across  multiple\ntasks, though specific tasks, such as the consequences and\nsituation  tasks  under  certain  criteria,  do  not  rank  it  as\nthe top performer. Despite GPT-4 and GPT-3.5 being de-\nveloped  by  OpenAI  and  likely  sharing  similar  model\nstructures, we posit that GPT-4 does not exhibit signific-\nant preference when serving as a judge. To validate that\nGPT-4 does not favor GPT-3.5 responses, we conducted\nadditional comparison tests. We select LLaMA-3-8b from\nthe LLaMA model family, which is close to GPT-3.5 in\ncreativity  performance.  We  used  it  as  an  examiner  to\nscore the LLM responses in our experiment with the same\nsystem  prompt.  This  allowed  us  to  obtain  comparative\nscores for the same data from two different judges.\nTo assess the consistency between judges, we applied\nthe  intra-class  correlation  coefficient  (ICC),  a  statistical\nmeasure evaluating agreement across different raters, spe-\ncifically  using  the  ICC(3,1)  model  to  gauge  single-rater,\nabsolute agreement. This method helps determine if the\ntwo judges provide similar ratings under identical condi-\ntions.  Additionally,  we  calculated  conventional  correla-\ntion coefficients to further validate consistency. As shown\nin Fig. 8\n, the results show a high level of agreement, with\nan ICC of 0.99 between GPT-4 and LLaMA-3-8b, indicat-\ning excellent consistency. Pearson, Spearman, and Kend-\nall correlations of 0.64, 0.61, and 0.46, respectively, indic-\nate  moderate  correlation  in  overall  rankings,  although\nthere are variations in how the models rank specific tasks.\nThese findings suggest that while GPT-4 and LLaMA-3-\n8b are consistent in their ratings, they may differ in inter-\npreting and prioritizing certain task aspects. \n4.9   Model-human agreement evaluation\nTo\n confirm  that  the  assessment  methods  based  on\nLLMs are overall reasonable and consistent with human\njudgement,  we  sample  the  responses  generated  by  these\nmodels and hire humans to evaluate them. We presented\nthe  answer  pairs  generated  by  the  LLMs  to  20  native\nEnglish-speaking  participants  globally  (10  male)  recrui-\n \nCP Cons IM IS JS Sit Unu\nCP Cons IM IS JS Sit Unu\n1.00\n0.75\n0.50\n0.25\n0\n−0.25\n−0.50\n−0.75\n−1.00\n1.00 0.61 0.52 0.61 0.50 0.63 0.62\n0.61\n1.00 0.93 0.81 0.95 0.93 0.98\n0.52 0.93 1.00 0.93 0.94 0.87 0.96\n0.61\n0.81 0.93 1.00 0.83 0.84 0.84\n0.50 0.95 0.94 0.83 1.00 0.90 0.95\n0.63\n0.93 0.87 0.84 0.90 1.00 0.90\n0.62 0.98 0.96 0.84 0.95 0.90 1.00\n \nFig. 7     The  heatmap  displays  Pearson  correlation  coefficients\nbetween  creativity  scores  across  task  pairs,  as  evaluated  by  the\nsame  judge.  (Colored  figures  are  available  in  the  online  version\nat https://link.springer.com/journal/11633)\n 430 Machine Intelligence Research 22(3), June 2025\n \n\n±\nted  from  Prolific  (https://www.prolific.co/),  and  paid\neach  participant  £15.  The  average  reward  per  hour  for\nthe participants was £14.59. The average participant age\nwas  32.9  20.1.  In  the  experiment,  we  sampled  seven\ntasks,  resulting  in  84  pairs  of  questions  and  answers,\nwhich means there are 84 trials. These pairs consist of an-\nswers from different models to the same question within\nthe same task, and are presented to the participants.\nOn each trial of the task, participants were asked to\nmake a binary decision about which of the two answers is\nmore creative according to the given criteria. The parti-\ncipants  also  have  the  option  to  choose  that  there  is  no\nsignificant  difference  in  creativity  between  the  two  re-\nsponses. A progress bar at the top of the screen indicated\nto participants how many trials they had completed and\nhad remained to complete. After the final human evalu-\nation  data  are  obtained,  we  calculate  the  consistency\nbetween the human assessment results and those of the\nLLMs for the overall score and each criterion.\nWe  use  Kendall′s  coefficient  and  Spearman′s  coeffi-\ncient for this calculation. Since the participants′  data are\nbased on relative win-loss relationships, we need to pre-\nprocess the human evaluation results. For the tie results,\nwe convert the human assessment results to the average\nscore of two answers evaluated by LLM; for non-tie res-\nults, we assign the higher score evaluated by the LLM to\nthe  winning  response  in  the  human  results.  The  results\nare shown in Table 3\n.\nThese moderate correlations indicate statistically sig-\nnificant alignment between GPT-4′ s evaluations and hu-\nman judgments, despite not achieving perfect consensus.\nFor  subjective  and  multifaceted  constructs  such  as  cre-\nativity,  moderate  correlations  are  notable,  as  complete\nagreement among human raters themselves is often diffi-\ncult  to  achieve.  This  alignment  supports  the  view  that\nGPT-4  captures  key  aspects  of  human  evaluation  while\nacknowledging  that  further  refinement  is  needed  to  en-\nhance the alignment. \n5   Conclusions and discussions\nIn\n this article, we have presented a framework to as-\nsess and understand the creativity of LLMs. The core of\nthis framework consists of 7 tasks and LLM-based evalu-\nation protocol that can be used to assess LLM′ s creativ-\nity  along  four  criteria.  The  proposed  framework  can  be\nused  to  assess  the  creative  performance  of  LLMs  from\nmultiple dimensions, while also exploring the factors that\ninfluence the creativity of these models and the relation-\nships  with  other  model  characteristics.  To  illustrate  the\nuse  and  usefulness  of  our  framework,  we  constructed  a\ndataset containing 700 questions that encompass various\ntypes of tasks measuring divergent thinking.\nThrough  our  further  analysis  and  experiments,  we\ndemonstrated that the creativity of LLMs is significantly\ninfluenced by the type of model architecture, the type of\nprompts it receives, and the model′ s system prompts. At\nthe same time, we also revealed a correlation between the\nlevels of creativity of LLMs and their personality traits.\nThis  work  is  beneficial  for  our  deeper  understanding  of\nthe  representations  of  LLMs  and  trying  to  establish  a\nbridge  between  artificial  intelligence  models  and  human\ncognitive models.\nAlthough we propose an effective framework for meas-\nuring the creativity of LLMs, it still has some limitations\nthat  need  to  be  addressed  by  future  work.  First,  LLMs\nuse text as both input and output, which allows them to\nborrow  from  psychological  methods  of  creativity  assess-\nment  such  as  TTCT,  which  uses  a  verbal  task  with\nverbal  stimuli.  However,  with  the  rapid  development  of\nAI models, those accepting multimodal inputs are emer-\nging[65],\n  which  we  call  large  multi-modal  model  (LMM).\nDesigning a variety of tasks beyond verbal question-and-\n \nComparison of ICC, Pearson, Spearman, and\nKendall correlation\nICC\nPearson Spearman Kendall\n0.99\n0.64 0.61\n0.46\nCorrelation/consistency value\n1.0\n0.8\n0.6\n0.4\n0.2\n0\n \nFig. 8     Comparison  of  ICC,  Pearson,  Spearman,  and  Kendall\ncorrelation values, demonstrating the consistency and agreement\nbetween GPT-4 and LLaMA-3-8b as judges across tasks. (Colo-\nred  figures  are  available  in  the  online  version  at  https://link.\nspringer.com/journal/11633)\n \np-valuesTable 3    Correlations and corresponding   between GPT-4-based and human-based evaluations across\ncreativity criteria (fluency, flexibility, originality, elaboration, and overall)\nFluency Flexibility Originality Elaboration Overall\nτKendall \nCorrelation coefficient\n0.588 9 0.578 2 0.556 7 0.451 2 0.499 6\np-value < 0. 000 1 < 0.000 1 < 0.000 1 < 0.000 1 < 0.000 1\nρSpearman \nCorrelation coefficient\n0.621 4 0.614 9 0.592 3 0.468 5 0.556 4\np-value < 0. 000 1 < 0.000 1 < 0.000 1 < 0.000 1 < 0.000 1\nY. Zhao et al. / Assessing and Understanding Creativity in Large Language Models 431 \n \n\nanswer formats for assessing the creativity of these LMM\nis an important direction for future research.\nSecond,\n LLMs are not the only generative models be-\ning capable of expressing creativity; there are also image\ngeneration models that are based on diffusion models and\nmodels  for  generating  music[66, 67],  in  other  words,  can\ngenerate multi-modal outputs. How to assess the content\nproduced by these other types of models to measure their\nlevel of creativity is also a question worth considering. In\naddition, the power of LLM allows developers to use it to\ndevelop a wide variety of plug-ins, integrate it with ex-\nternal programs or software, and even construct an agent\nsystem[68],\n  and the creativity in these cases is bound to be\ndifferent and needs to be investigated.\nMoreover,  while  this  study  demonstrates  the  overall\nalignment between GPT-4 and human evaluations of cre-\nativity,  it  does  not  delve  into  criterion-specific  correla-\ntions  (e.g.,  fluency,  flexibility,  originality,  elaboration).\nBy  expanding  the  dataset,  leveraging  separate  evalu-\nations  for  each  creativity  criterion,  and  employing  ad-\nvanced statistical techniques such as criterion-specific in-\ntraclass  correlation  coefficients,  finer-grained  alignment\nstudies  can  illuminate  the  strengths  and  limitations  of\nLLM-based evaluation frameworks.\nLast, we believe that the creativity exhibited by LLMs\nis  only  an  outcome-oriented  interpretation.  Whether  AI\nmodels  possess  true  creativity  from  a  human  cognitive\nperspective remains an open question in the field of artifi-\ncial intelligence. LLM′s expression of creativity is likely to\nbe  an  imitation  of  human  creativity  through  a  large\namount  of  learning.  Understanding  the  creativity  of\nLLMs is also beneficial for uncovering the inner secrets of\nthe model “black box”, and for a deeper understanding of\nthe nature of intelligence and cognition. Although analys-\ning the nature of creativity is difficult, our analysis and\nevaluation of LLM creativity performance is fundamental\nto study of the kernel of creativity. \nAppendix\n \nA.1   Example prompts\nL\nikert scale scoring.\nYou are an expert of psychology. Your objective is to\nassess  the  subject′s  creativity  through  their  answers  to\nsome question/answering task related to divergent think-\ning. You will be given a question-answer pair. Your task\nis to score the answer.\nYou  should  rate  the  answer  on  five  metrics.  For  all\nfive metrics, assign a score between 1 and 5, with 5 being\nthe highest. Five metrics are:\n1) Fluency. Fluency refers to the ability to generate a\nlarge  quantity  of  ideas  or  solutions  to  a  given  problem.\nThis measure isn't concerned with the quality or unique-\nness of the ideas, but rather the sheer volume. The more\nideas one can produce, the higher the fluency is.\n2) Flexibility. Flexibility is the capacity to shift one′ s\nthinking and to produce a wide range of ideas from differ-\nent  categories  or  perspectives.  It  involves  being  able  to\nthink outside of the box and to switch from one type of\nidea to another.\n3) Originality. Originality refers to the ability to come\nup with unique or novel ideas that differ from the norm.\nIt′s  not  just  about  producing  many  ideas  (fluency),  but\nalso about producing ideas that are different from what\nothers might typically think of.\n4)  Elaboration.  Elaboration  is  the  ability  to  expand\nupon or add detail to ideas. It involves taking a simple\nidea and building upon it, adding complexity and depth.\nElaboration  isn't  just  about  creating  more,  but  about\ndeepening what is there.\n5) Finally, you will provide an overall score between 1\nand 5, with 5 being the highest.\nYou should only give the score, format like: Fluency: 3\nQuestion: {Question} Answer: {Answer}\nInstructive prompts (unusual uses task as the\nexample):\nUnusual uses task.\nThe purpose of this task is to measure your ability to\ncome up with creative and unique uses for everyday ob-\njects. We're looking for out-of-the-box thinking here.\nYou  will  be  presented  with  a  common  object,  and\nyour task is to suggest as many unusual, innovative, or\nnon-traditional  uses  for  the  object  as  you  can  think  of.\nPlease  remember,  the  goal  is  not  to  think  of  the  most\ncommon or typical uses, but to try and imagine unique or\nunusual ways the object could be used.\nHere are the objects: {Objects}\nCollaboration prompts.\nThese are answers to the question from other agents:\nOne agent solution: {Answers}\n···\nOne agent solution: {Answers}\nUsing the answers from other agents as reference and\ninspiration, can you give an updated answer? Make sure\nto give your answer at the end of the response.\nQuestion: {Question} \nAcknowledgements\nThis\n work  was  partially  supported  by  the  National\nNatural  Science  Foundation  of  China  (Nos.  U22A2028,\n61925208,  62102399,  62302478,  62302483,  62222214,\n62372436,  62302482  and  62302480),  CAS  Project  for\nYoung  Scientists  in  Basic  Research,  China  (No.  YSBR-\n029), Youth Innovation Promotion Association CAS and\nXplore Prize, China. \nDeclarations of conflict of interest\nThe\n authors declared that they have no conflicts of in-\n 432 Machine Intelligence Research 22(3), June 2025\n \n\nterest to this work. \nOpen Access\nThis\n article is licensed under a Creative Commons At-\ntribution  4.0  International  License,  which  permits  use,\nsharing, adaptation, distribution and reproduction in any\nmedium or format, as long as you give appropriate credit\nto the original author(s) and the source, provide a link to\nthe  Creative  Commons  licence,  and  indicate  if  changes\nwere made.\nThe images or other third party material in this art-\nicle  are  included  in  the  article′s  Creative  Commons  li-\ncence,  unless  indicated  otherwise  in  a  credit  line  to  the\nmaterial. If material is not included in the article′ s Creat-\nive  Commons  licence  and  your  intended  use  is  not  per-\nmitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the\ncopyright holder.\nTo  view  a  copy  of  this  licence,  visit http://creative-\ncommons.org/licenses/by/4.0/.\nReferences\n  S.  Bubeck,  V.  Chandrasekaran,  R.  Eldan,  J.  Gehrke,  E.\nHorvitz,  E.  Kamar,  P.  Lee,  Y.  T.  Lee,  Y.  Z.  Li,  S.  Lund-\nberg, H. Nori, H. Palangi, M. T. Ribeiro, Y. Zhang. Sparks\nof  artificial  general  intelligence:  Early  experiments  with\nGPT-4,  [Online],  Available:  https://arxiv.org/abs/2303.\n12712, 2023.\n[1]\n  H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi,\nY. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhos-\nale, D. Bikel, L. Blecher, C. C. Ferrer, M. Y. Chen, G. Cu-\ncurull, D. Esiobu, J. Fernandes, J. Fu, W. Y. Fu, B. Fuller,\nC. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini,\nR.  Hou,  H.  Inan,  M.  Kardas,  V.  Kerkez,  M.  Khabsa,  I.\nKloumann,  A.  Korenev,  P.  S.  Koura,  M.  A.  Lachaux,  T.\nLavril, J. Lee, D. Liskovich, Y. H. Lu, Y. N. Mao, X. Mar-\ntinet,  T.  Mihaylov,  P.  Mishra,  I.  Molybog,  Y.  X.  Nie,  A.\nPoulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten,\nR.  Silva,  E.  M.  Smith,  R.  Subramanian,  X.  E.  Tan,  B.\nTang,  R.  Taylor,  A.  Williams,  J.  X.  Kuan,  P.  X.  Xu,  Z.\nYan,  I.  Zarov,  Y.  C.  Zhang,  A.  Fan,  M.  Kambadur,  S.\nNarang, A. Rodriguez, R. Stojnic, S. Edunov, T. Scialom.\nLlama  2:  Open  foundation  and  fine-tuned  chat  models,\n[Online],  Available:  https://arxiv.org/abs/2307.09288,\n2023.\n[2]\n  Y. H. Wu, A. Q. Jiang, W. D. Li, M. N. Rabe, C. Staats,\nM. Jamnik, C. Szegedy. Autoformalization with large lan-\nguage  models.  In  Proceedings of the 36th International\nConference on Neural Information Processing Systems,\nNew Orleans, USA, Article number 2344, 2022.\n[3]\n  T.  R.  Laskar,  M.  S.  Bari,  M.  Rahman,  A.  H.  Bhuiyan,  S.\nJoty,  J.  Huang.  A  systematic  study  and  comprehensive\nevaluation  of  ChatGPT  on  benchmark  datasets.  In  Pro-\nceedings of Findings of the Association for Computational\nLinguistics, Toronto, Canada, pp. 431–469, 2023. DOI: 10.\n18653/v1/2023.findings-acl.29.\n[4]\n  J.  Devlin,  M.  W.  Chang,  L.  Kenton,  K.  Toutanova.\nBERT: Pre-training of deep bidirectional transformers for\nlanguage  understanding.  In  Proceedings of Conference of\nthe North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies,\n[5]\nMinneapolis,  USA,  pp. 4171–4186,  2019.  DOI:  10.18653/\nv1/N19-1423.\n G.  Di  Fede,  D.  Rocchesso,  S.  P.  Dow,  S.  Andolina.  The\nidea  machine:  LLM-based  expansion,  rewriting,  combina-\ntion,  and  suggestion  of  ideas.  In  Proceedings of the 14th\nConference on Creativity and Cognition,  Venice,  Italy,\npp. 623–627, 2022. DOI: 10.1145/3527927.3535197.\n[6]\n M.  Elzohbi,  R.  Zhao.  Creative  data  generation:  A  review\nfocusing on text and poetry. In Proceedings of the 14th In-\nternational Conference on Computational Creativity,\nOntario, Canada, pp. 29–38, 2023.\n[7]\n Z.  Zhao,  S.  Song,  B.  Duah,  J.  Macbeth,  S.  Carter,  M.  P.\nVan, N. S. Bravo, M. Klenk, K. Sick, A. L. S. Filipowicz.\nMore human than human: LLM-generated narratives out-\nperform  human-LLM  interleaved  narratives.  In  Proceed-\nings of the 15th Conference on Creativity and Cognition,\nNewYork, USA, pp. 368–370, 2023. DOI: 10.1145/3591196.\n3596612.\n[8]\n Y. J. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser,\nR. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago,\nT. Hubert, P. Choy, C. De Masson d′Autume, I. Babusch-\nkin,  X.  Y.  Chen,  P.  S.  Huang,  J.  Welbl,  S.  Gowal,  A.\nCherepanov,  J.  Molloy,  D.  J.  Mankowitz,  E.  Sutherland\nRobson,  P.  Kohli,  N.  De  Freitas,  K.  Kavukcuoglu,  O.\nVinyals. Competition-level code generation with AlphaCo-\nde.  Science,  vol. 378, no. 6624, pp. 1092–1097, 2022.  DOI:\n10.1126/science.abq1158.\n[9]\n E. Kasneci, K. Sessler, S. Küchemann, M. Bannert, D. De-\nmentieva, F. Fischer, U. Gasser, G. Groh, S. Günnemann,\nE.  Hüllermeier,  S.  Krusche,  G.  Kutyniok,  T.  Michaeli,  C.\nNerdel,  J.  Pfeffer,  O.  Poquet,  M.  Sailer,  A.  Schmidt,  T.\nSeidel, M. Stadler, J. Weller, J. Kuhn, G. Kasneci. ChatG-\nPT for good? On opportunities and challenges of large lan-\nguage  models  for  education.  Learning and Individual Dif-\nferences,  vol. 103, Article  number  102274,  2023.  DOI:  10.\n1016/j.lindif.2023.102274.\n[10]\n S. Y. Li, W. J. Yu, T. P. Gu, C. Z. Lin, Q. Wang, C. Qian,\nC.  C.  Loy,  Z.  W.  Liu.  Bailando:  3D  dance  generation  by\nactor-critic GPT with choreographic memory. In Proceed-\nings of IEEE/CVF Conference on Computer Vision and\nPattern Recognition, New Orleans, USA, pp. 11050–11059,\n2022. DOI: 10.1109/CVPR52688.2022.01077.\n[11]\n B.  Banar,  S.  Colton.  A  systematic  evaluation  of  GPT-2-\nbased music generation. In Proceedings of the 11th Inter-\nnational Conference on Artificial Intelligence in Music,\nSound, Art and Design,  Madrid,  Spain,  pp. 19–35, 2022.\nDOI: 10.1007/978-3-031-03789-4_2.\n[12]\n Y. R. Liu, S. Chen, H. C. Cheng, M. X. Yu, X. Ran, A. Mo,\nY.  L.  Tang,  Y.  Huang.  How  AI  processing  delays  foster\ncreativity:  Exploring  research  question  Co-creation  with\nan LLM-based agent. In Proceedings of CHI Conference on\nHuman Factors in Computing Systems,  Honolulu,  USA,\nArticle number 17, 2024. DOI: 10.1145/3613904.3642698.\n[13]\n H. Shin, S. Choi, J. Y. Cho, S. Admoni, H. Lim, T. Kim, H.\nHong, M. Lee, Kim, J. Towards an evaluation of LLM-gen-\nerated  inspiration  by  developing  and  validating  inspira-\ntion  scale.  In  Proceedings of the 1st HEAL Workshop at\nCHI Conference on Human Factors in Computing Sys-\ntems, Honolulu, USA, 2024.\n[14]\n B.  R.  Anderson,  J.  H.  Shah,  M.  Kreminski.  Homogeniza-\ntion  effects  of  large  language  models  on  human  creative\nideation.  In  Proceedings of the 16th Conference on Cre-\nativity & Cognition,  Chicago,  USA,  pp. 413–425,  2024.\nDOI: 10.1145/3635636.3656204.\n[15]\n M. A. Runco, G. J. Jaeger. The standard definition of cre-[16]\nY\n. Zhao et al. / Assessing and Understanding Creativity in Large Language Models 433 \n \n\nativity.  Creativity Research Journal,  vol. 24,  no. 1,\npp. 92–96, 2012. DOI: 10.1080/10400419.2012.650092.\n  T.  Chakraborty,  S.  Masud.  Judging  the  creative  prowess\nof  AI.  Nature Machine Intelligence,  vol. 5,  no. 6,  Article\nnumber 558, 2023. DOI: 10.1038/s42256-023-00664-y.\n[17]\n  E. P. Torrance. Torrance Test of Creative Thinking: Dir-\nections Manual and Scoring Guide, Lexington, USA: Per-\nsonnel Press, 1966.\n[18]\n  B.  Barbot,  R.  Reiter-Palmon.  Creativity  assessment:  Pit-\nfalls,  solutions,  and  standards.  Psychology of Aesthetics,\nCreativity, and the Arts,  vol. 13, no. 2, pp. 131–132, 2019.\nDOI: 10.1037/aca0000251.\n[19]\n  R.  J.  Sternberg,  E.  L.  Grigorenko.  Guilford′s  structure  of\nintellect model and model of creativity: Contributions and\nlimitations.  Creativity Research Journal,  vol. 13, no. 3–4,\npp. 309–316, 2001. DOI: 10.1207/S15326934CRJ1334_08.\n[20]\n  S. Acar, M. A. Runco. Divergent thinking: New methods,\nrecent  research,  and  extended  theory.  Psychology of Aes-\nthetics, Creativity, and the Arts, vol. 13, no. 2, pp. 153–158,\n2019. DOI: 10.1037/aca0000231.\n[21]\n  G.  M.  Cseh,  K.  K.  Jeffries.  A  scattered  CAT:  A  critical\nevaluation of the consensual assessment technique for cre-\nativity research. Psychology of Aesthetics, Creativity, and\nthe Arts,  vol. 13, no. 2, pp. 159–166, 2019.  DOI:  10.1037/\naca0000220.\n[22]\n  J.  C.  Kaufman.  Self-assessments  of  creativity:  Not  ideal,\nbut better than you think. Psychology of Aesthetics, Cre-\nativity, and the Arts, vol. 13, no. 2, pp. 187–192, 2019. DOI:\n10.1037/aca0000217.\n[23]\n  B.  Barbot,  R.  W.  Hass,  R.  Reiter-Palmon.  Creativity  as-\nsessment in psychological research: (Re) setting the stand-\nards.  Psychology of Aesthetics, Creativity, and the Arts,\nvol. 13, no. 2, pp. 233–240, 2019. DOI: 10.1037/aca0000233.\n[24]\n  K. H. Kim. The APA 2009 division 10 debate: Are the Tor-\nrance  tests  of  creative  thinking  still  relevant  in  the  21st\ncentury?  Psychology of Aesthetics, Creativity, and the\nArts,  vol. 5,  no. 4,  pp. 302–308,  2011.  DOI:  10.1037/\na0021917.\n[25]\n  J.  A.  Plucker.  Is  the  proof  in  the  pudding?  Reanalyses  of\ntorrance′s (1958 to present) longitudinal data. Longitudin-\nal Studies of Creativity,  M.  A.  Runco,  Ed.,  New  York,\nUSA:  Routledge,  pp. 103–114,  1999.  DOI:  10.4324/\n9780203063330.\n[26]\n  K. H. Kim. Can we trust creativity tests? A review of the\nTorrance  tests  of  creative  thinking  (TTCT).  Creativity\nResearch Journal,  vol. 18, no. 1, pp. 3–14, 2006.  DOI:  10.\n1207/s15326934crj1801_2.\n[27]\n  S.  da  Costa,  D.  Páez,  F.  Sánchez,  M.  Garaigordobil,  S.\nGondim.  Personal  factors  of  creativity:  A  second  order\nmeta-analysis.  Journal of Work and Organizational Psy-\nchology,  vol. 31, no. 3, pp. 165–173, 2015.  DOI:  10.1016/j.\nrpto.2015.06.002.\n[28]\n  H. H. Ma. The effect size of variables associated with cre-\nativity:  A  meta-analysis.  Creativity Research Journal,\nvol. 21,  no. 1,  pp. 30–42,  2009.  DOI:  10.1080/1040041080\n2633400.\n[29]\n  Y. P. Chang, X. Wang, J. D. Wang, Y. Wu, L. Y. Yang, K.\nJ. Zhu, H. Chen, X. Y. Yi, C. X. Wang, Y. D. Wang, W.\nYe, Y. Zhang, Y. Chang, P. S. Yu, Q. Yang, X. Xie. A sur-\nvey  on  evaluation  of  large  language  models.  ACM Trans-\nactions on Intelligent Systems and Technology,  vol. 15,\nno. 3, Article number 39, 2024. DOI: 10.1145/3641289.\n[30]\n  R.  Shiffrin,  M.  Mitchell.  Probing  the  psychology  of  AI[31]\nmodels. Proceedings of the National Academy of Sciences\nof the United States of America,  vol. 120, no. 10, Article\nnumber  e2300963120,  2023.  DOI:  10.1073/pnas.23009\n63120.\n G.  Franceschelli,  M.  Musolesi.  On  the  creativity  of  large\nlanguage  models,  [Online],  Available:  https://arxiv.org/\nabs/2304.00008, 2023.\n[32]\n D. Summers-Stay, S. Lukin, C. Voss. Brainstorm, then se-\nlect:  A  generative  language  model  improves  its  creativity\nscore.  In  Proceedings of AAAI Workshop on Creative AI\nAcross Modalities, 2023.\n[33]\n C. Stevenson, I. Smal, M. Baas, R. P. P. P. Grasman, H. L.\nJ. van der Maas. Putting GPT-3′s creativity to the (altern-\native  uses)  test.  In  Proceedings of the 13th International\nConference on Computational Creativity, Bozen-Bolzano,\nItaly, pp. 164–168, 2022.\n[34]\n S.  A.  Naeini,  R.  Saqur,  M.  Saeidi,  J.  Giorgi,  B.  Taati.\nLarge language models are fixated by red herrings: Explor-\ning  creative  problem  solving  and  einstellung  effect  using\nthe  only  connect  wall  dataset.  In  Proceedings of the 37th\nInternational Conference on Neural Information Pro-\ncessing Systems,  New  Orleans,  USA,  Article  number  246,\n2023.\n[35]\n E.  E.  Guzik,  C.  Byrge,  C.  Gilde.  The  originality  of  ma-\nchines:  AI  takes  the  Torrance  test.  Journal of Creativity,\nvol. 33, no. 3, Article number 100065, 2023. DOI: 10.1016/\nj.yjoc.2023.100065.\n[36]\n T. Chakrabarty, P. Laban, D. Agarwal, S. Muresan, C. S.\nWu.  Art  or  artifice?  Large  language  models  and  the  false\npromise of creativity. In Proceedings of CHI Conference on\nHuman Factors in Computing Systems,  Honolulu,  USA,\nArticle number 30, 2024. DOI: 10.1145/3613904.3642731.\n[37]\n T. I. Lubart. Models of the creative process: Past, present\nand  future.  Creativity Research Journal,  vol. 13, no. 3–4,\npp. 295–308, 2001. DOI: 10.1207/S15326934CRJ1334_07.\n[38]\n M.  Batey.  The  measurement  of  creativity:  From  defini-\ntional  consensus  to  the  introduction  of  a  new  heuristic\nframework.  Creativity Research Journal,  vol. 24,  no. 1,\npp. 55–65, 2012. DOI: 10.1080/10400419.2012.649181.\n[39]\n D. Piffer. Can creativity be measured? An attempt to cla-\nrify  the  notion  of  creativity  and  general  directions  for  fu-\nture research. Thinking Skills and Creativity, vol. 7, no. 3,\npp. 258–264, 2012. DOI: 10.1016/j.tsc.2012.04.009.\n[40]\n R. E. Beaty, D. R. Johnson. Automating creativity assess-\nment  with  SemDis:  An  open  platform  for  computing  se-\nmantic  distance.  Behavior Research Methods,  vol. 53,\nno. 2, pp. 757–780, 2021. DOI: 10.3758/s13428-020-01453-w.\n[41]\n S.  Acar,  K.  Berthiaume,  K.  Grajzel,  D.  Dumas,  C.  Flem-\nister, P. Organisciak. Applying automated originality scor-\ning to the verbal form of Torrance tests of creative think-\ning.  Gifted Child Quarterly,  vol. 67, no. 1, pp. 3–17, 2023.\nDOI: 10.1177/00169862211061874.\n[42]\n Y.  S.  Bai,  J.  H.  Ying,  Y.  X.  Cao,  X.  Lv,  Y.  Z.  He,  X.  Z.\nWang,  J.  F.  Yu,  K.  S.  Zeng,  Y.  J.  Xiao,  H.  Z.  Lyu,  J.  Y.\nZhang, J. Z. Li, L. Hou. Benchmarking foundation models\nwith  language-model-as-an-examiner.  In  Proceedings of\nthe 37th International Conference on Neural Information\nProcessing Systems, New Orleans, USA, 2023.\n[43]\n L. M. Zheng, W. L. Chiang, Y. Sheng, S. Y. Zhuang, Z. H.\nWu, Y. H. Zhuang, Z. Lin, Z. H. Li, D. C. Li, E. P. Xing,\nH.  Zhang,  J.  E.  Gonzalez,  I.  Stoica.  Judging  LLM-as-a-\njudge  with  MT-bench  and  chatbot  arena.  In  Proceedings\nof the 37th International Conference on Neural Informa-\ntion Processing Systems, New Orleans, USA, Article num-\n[44]\n 434 Machine Intelligence Research 22(3), June 2025\n \n\nber 2020, 2023.\n  Y. J. Bang, S. Cahyawijaya, N. Lee, W. L. Dai, D. Su, B.\nWilie, H. Lovenia, Z. W. Ji, T. Z. Yu, W. Chung, Q. V. Do,\nY.  Xu,  P.  Fung.  A  multitask,  multilingual,  multimodal\nevaluation  of  ChatGPT  on  reasoning,  hallucination,  and\ninteractivity.  In  Proceedings of the 13th International\nJoint Conference on Natural Language Processing and the\n3rd Conference of the Asia-Pacific Chapter of the Associ-\nation for Computational Linguistics, Nusa Dua, Indonesia,\n2023. DOI: 10.18653/v1/2023.ijcnlp-main.45.\n[45]\n  Y. Wang, Z. Yu, Z. Zeng, L. Yang, C. Wang, H. Chen, C.\nJiang, R. Xie, J. Wang, X. Xie, et al. PandaLM: An auto-\nmatic  evaluation  benchmark  for  LLM  instruction  tuning\noptimization.  In  Proceedings of the 12th International\nConference on Learning Representations, Vienna, Austria,\n2024.\n[46]\n  C. M. Chan, W. Z. Chen, Y. S. Su, J. X. Yu, W. Xue, S. H.\nZhang, J. Fu, Z. Y. Liu. ChatEval: Towards better LLM-\nbased evaluators through multi-agent debate. In Proceed-\nings of the 12th International Conference on Learning\nRepresentations, Vienna, Austria, 2024.\n[47]\n  W. J. Zhong, R. X. Cui, Y. D. Guo, Y. B. Liang, S. Lu, Y.\nL. Wang, A. Saied, W. Z. Chen, N. Duan. AGIEval: A hu-\nman-centric benchmark for evaluating foundation models.\nIn Proceedings of Findings of the Association for Compu-\ntational Linguistics,  Mexico  City,  Mexico,  pp. 2299–\n2314, 2024. DOI: 10.18653/v1/2024.findings-naacl.149.\n[48]\n  Y. Dubois, X. C. Li, R. Taori, T. Y. Zhang, I. Gulrajani, J.\nBa, C. Guestrin, P. Liang, T. B. Hashimoto. AlpacaFarm:\nA  simulation  framework  for  methods  that  learn  from  hu-\nman  feedback.  In  Proceedings of the 37th International\nConference on Neural Information Processing Systems,\nNew Orleans, USA, Article number 1308, 2024.\n[49]\n  Y. Liu, D. Iter, Y. C. Xu, S. H. Wang, R. C. Xu, C. G. Zhu.\nG-Eval:  NLG  evaluation  using  Gpt-4  with  better  human\nalignment.  In  Proceedings of Conference on Empirical\nMethods in Natural Language Processing,  Singapore,\npp. 2511–2522, 2023. DOI: 10.18653/v1/2023.emnlp-main.\n153.\n[50]\n  D.  Demszky,  D.  Y.  Yang,  D.  S.  Yeager,  C.  J.  Bryan,  M.\nClapper, S. Chandhok, J. C. Eichstaedt, C. Hecht, J. Jam-\nieson, M. Johnson, M. Jones, D. Krettek-Cobb, L. Lai, N.\nJonesmitchell, D. C. Ong, C. S. Dweck, J. J. Gross, J. W.\nPennebaker.  Using  large  language  models  in  psychology.\nNature Reviews Psychology,  vol. 2,  no. 11,  pp. 688–701,\n2023. DOI: 10.1038/S44159-023-00241-5.\n[51]\n  T.  Wolf,  L.  Debut,  V.  Sanh,  J.  Chaumond,  C.  Delangue,\nA.  Moi,  P.  Cistac,  T.  Rault,  R.  Louf,  M.  Funtowicz,  J.\nDavison,  S.  Shleifer,  P.  von  Platen,  C.  Ma,  Y.  Jernite,  J.\nPlu,  C.  W.  Xu,  T.  Le  Scao,  S.  Gugger,  M.  Drame,  Q.\nLhoest,  A.  Rush.  Transformers:  State-of-the-art  natural\nlanguage processing. In Proceedings of Conference on Em-\npirical Methods in Natural Language Processing: System\nDemonstrations, pp. 38–45, 2020. DOI: 10.18653/v1/2020.\nemnlp-demos.6.\n[52]\n  J.  Z.  Bai,  S.  Bai,  Y.  F.  Chu,  Z.  Y.  Cui,  K.  Dang,  X.  D.\nDeng, Y. Fan, W. B. Ge, Y. Han, F. Huang, B. Y. Hui, L.\nJi, M. Li, J. Y. Lin, R. J. Lin, D. H. Liu, G. Liu, C. Q. Lu,\nK. M. Lu, J. X. Ma, R. Men, X. Z. Ren, X. C. Ren, C. Q.\nTan, S. N. Tan, J. H. Tu, P. Wang, S. J. Wang, W. Wang,\nS. G. Wu, B. F. Xu, J. Xu, A. Yang, H. Yang, J. Yang, S.\nS.  Yang,  Y.  Yao,  B.  W.  Yu,  H.  Y.  Yuan,  Z.  Yuan,  J.  W.\nZhang, X. X. Zhang, Y. C. Zhang, Z. R. Zhang, C. Zhou, J.\nR.  Zhou,  X.  H.  Zhou,  T.  H.  Zhu.  Qwen  technical  report,\n[Online],  Available:  https://arxiv.org/abs/2309.16609,\n2023.\n[53]\n S.  Lin,  J.  Hilton,  O.  Evans.  TruthfulQA:  Measuring  how\nmodels  mimic  human  falsehoods.  In  Proceedings of the\n60th Annual Meeting of the Association for Computation-\nal Linguistics,  Dublin,  Ireland,  pp. 3214–3252, 2022.  DOI:\n10.18653/v1/2022.acl-long.229.\n[54]\n T.  Xiang,  L.  Z.  Li,  W.  Y.  Li,  M.  B.  Bai,  L.  Wei,  B.  W.\nWang, N. Garcia. CARE-MI: Chinese benchmark for mis-\ninformation  evaluation  in  maternity  and  infant  care.  In\nProceedings of the 37th Conference on Neural Information\nProcessing Systems, 2023.\n[55]\n Q.  T.  Xu,  F.  L.  Hong,  B.  Li,  C.  R.  Hu,  Z.  Y.  Chen,  J.\nZhang. On the tool manipulation capability of open-source\nlarge language models. In Proceedings of the 37th Confer-\nence on Neural Information Processing Systems, 2023.\n[56]\n T.  Kojima,  S.  S.  Gu,  M.  Reid,  Y.  Matsuo,  Y.  Iwasawa.\nLarge  language  models  are  zero-shot  reasoners.  In  Pro-\nceedings of the 36th International Conference on Neural\nInformation Processing Systems,  New  Orleans,  USA,\npp. 22199–22213, 2022.\n[57]\n P.  B.  Paulus,  M.  Dzindolet,  N.  W.  Kohn.  Collaborative\ncreativity-group  creativity  and  team  innovation.  Hand-\nbook of Organizational Creativity,  M.  D.  Mumford,  Ed.,\nAmsterdam, The Netherlands: Elsevier, pp. 327–357, 2012.\nDOI: 10.1016/B978-0-12-374714-3.00014-8.\n[58]\n M.  S.  Barrett,  A.  Creech,  K.  Zhukov.  Creative  collabora-\ntion  and  collaborative  creativity:  A  systematic  literature\nreview.  Frontiers in Psychology,  vol. 12,  Article  number\n713445, 2021. DOI: 10.3389/fpsyg.2021.713445.\n[59]\n C. MacCann, R. D. Roberts. New paradigms for assessing\nemotional  intelligence:  Theory  and  data.  Emotion,  vol. 8,\nno. 4, pp. 540–551, 2008. DOI: 10.1037/a0012746.\n[60]\n R. N. Spreng, M. C. McKinnon, R. A. Mar, B. Levine. The\nToronto  empathy  questionnaire:  Scale  development  and\ninitial  validation  of  a  factor-analytic  solution  to  multiple\nempathy  measures.  Journal of Personality Assessment,\nvol. 91,  no. 1,  pp. 62–71,  2009.  DOI:  10.1080/0022389080\n2484381.\n[61]\n R.  Schwarzer,  M.  Jerusalem.  Generalized  self-efficacy\nscale.  Measures in Health Psychology: A User′s Portfolio,\nJ. Weinman, S. Wright, M. Johnston, Eds., Windsor, UK:\nNFER-NELSON, pp. 35–37, 1995.\n[62]\n O.  P.  John,  S.  Srivastava.  The  big  five  trait  taxonomy:\nHistory, measurement, and theoretical perspectives. Hand-\nbook of Personality: Theory and Research,  2nd  ed.,  L.  A.\nPervin, O. P. John, Eds., New York, USA: Guilford Press,\n1999.\n[63]\n M.  H.  Qian,  J.  A.  Plucker,  X.  D.  Yang.  Is  creativity  do-\nmain specific or domain general? Evidence from multilevel\nexplanatory  item  response  theory  models.  Thinking Skills\nand Creativity, vol. 33, Article number 100571, 2019. DOI:\n10.1016/j.tsc.2019.100571.\n[64]\n D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery,\nB. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. H. Yu, W.\nL.  Huang,  Y.  Chebotar,  P.  Sermanet,  D.  Duckworth,  S.\nLevine,  V.  Vanhoucke,  K.  Hausman,  M.  Toussaint,  K.\nGreff, A. Zeng, I. Mordatch, P. Florence. PaLM-E: An em-\nbodied  multimodal  language  model,  [Online],  Available:\nhttps://arxiv.org/abs/2303.03378, 2023.\n[65]\n F. A. Croitoru, V. Hondru, R. T. Ionescu, M. Shah. Diffu-\nsion  models  in  vision:  A  survey.  IEEE Transactions on\nPattern Analysis and Machine Intelligence,  vol. 45, no. 9,\npp. 10850–10869,  2023.  DOI:  10.1109/TPAMI.2023.3261\n988.\n[66]\n F.  Carnovalini,  A.  Rodà.  Computational  creativity  and\nmusic generation systems: An introduction to the state of\n[67]\nY. Zhao et al. / Assessing and Understanding Creativity in Large Language Models 435 \n \n\nthe  art.  Frontiers in Artificial Intelligence,  vol. 3, Article\nnumber 14, 2020. DOI: 10.3389/frai.2020.00014.\n  L. Wang, C. Ma, X. Y. Feng, Z. Y. Zhang, H. Yang, J. S.\nZhang, Z. Y. Chen, J. K. Tang, X. Chen, Y. K. Lin, W. X.\nZhao,  Z.  W.  Wei,  J.  R.  Wen.  A  survey  on  large  language\nmodel  based  autonomous  agents.  Frontiers of Computer\nScience,  vol. 18, no. 6, Article  number  186345,  2024.  DOI:\n10.1007/s11704-024-40231-1.\n[68]\nYunpu Zhao received the B. Sc. degree in\ncomputer science from Wuhan University,\nChina  in  2022.  He  is  a  Ph. D. degree  can-\ndidate  at  the  University  of  Science  and\nTechnology of China, China.\n     His  research  interests  include  deep\nlearning  and  multimodal  large  language\nmodels.\n     E-mail: zyp351791@mail.ustc.edu.cn\n     ORCID iD: 0000-0001-7747-7040\nRui Zhang  received  the  Ph. D. degree  in\ncomputer  application  technology  from  the\nInstitute  of  Computing  Technology,\nChinese  Academy  of  Sciences,  China  in\n2019. She is an associate professor in State\nKey Lab (SKL) of Processors at the Insti-\ntute  of  Computing  Technology,  Chinese\nAcademy of Sciences, China.\n     Her  research  interests  include  deep\nlearning and multimodal large language model.\n     E-mail: zhangrui@ict.ac.cn (Corresponding author)\n     ORCID iD: 0000-0001-8691-8549\nWenyi  Li received  the  B. Eng. degree  in\ncomputer science and technology from the\nUniversity  of  Chinese  Academy  of  Sci-\nences,  China  in  2023.  He  is  currently  a\nPh. D. degree candidate at the Institute of\nSoftware,  Chinese  Academy  of  Sciences\n(ISCAS), China.\n     His  research  interests  include  computer\nvision,  code  generation,  and  large  lan-\nguage model reasoning.\n     E-mail: liwenyi2023@iscas.ac.cn\nLing Li  received the B. Sc. degree in com-\nputer science and technology from Wuhan\nUniversity,  China  in  2004,  and  the  Ph. D.\ndegree  in  computer  architecture  from  the\nInstitute  of  Computing  Technology,\nChinese  Academy  of  Sciences,  China  in\n2009. She is currently a professor at the In-\nstitute  of  Software,  Chinese  Academy  of\nSciences,  China.  She  has  authored  four\nbooks (including AI Computing Systems) and over 70 papers on\njournals  and  conferences  (including  TIP,  TC,  TPDS,  ICML,\nNeurIPS, AAAI, ISCA, MICRO).\n     Her research interest is intelligent computing.\n     E-mail: liling@iscas.ac.cn\n 436 Machine Intelligence Research 22(3), June 2025\n \n"
}