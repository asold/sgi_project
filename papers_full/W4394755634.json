{
  "title": "Multi-Branch Mutual-Distillation Transformer for EEG-Based Seizure Subtype Classification",
  "url": "https://openalex.org/W4394755634",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4288965319",
      "name": "Peng, Ruimin",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": null,
      "name": "Du, Zhenbang",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1884958333",
      "name": "Zhao Changming",
      "affiliations": [
        "Dongfeng Motor Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2746385945",
      "name": "Luo jingwei",
      "affiliations": [
        "Guangzhou Electronic Technology (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2148719452",
      "name": "Liu Wenzhong",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2364722614",
      "name": "Chen Xinxing",
      "affiliations": [
        "Southern University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2743965220",
      "name": "Wu, Dongrui",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2784258871",
    "https://openalex.org/W4319295260",
    "https://openalex.org/W4391381848",
    "https://openalex.org/W6841059395",
    "https://openalex.org/W2592509339",
    "https://openalex.org/W2594644573",
    "https://openalex.org/W2994921215",
    "https://openalex.org/W2156192068",
    "https://openalex.org/W1981211771",
    "https://openalex.org/W3165430704",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W6774302960",
    "https://openalex.org/W2754084392",
    "https://openalex.org/W6748163181",
    "https://openalex.org/W2963125010",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2620998106",
    "https://openalex.org/W4313182973",
    "https://openalex.org/W3032515910",
    "https://openalex.org/W2973010960",
    "https://openalex.org/W4376121133",
    "https://openalex.org/W3006560451",
    "https://openalex.org/W2752782242",
    "https://openalex.org/W4294690816",
    "https://openalex.org/W2559463885",
    "https://openalex.org/W4372263898",
    "https://openalex.org/W2904170036",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3034695001",
    "https://openalex.org/W2987861506",
    "https://openalex.org/W6752186649",
    "https://openalex.org/W2981441441",
    "https://openalex.org/W6794695087",
    "https://openalex.org/W6745136726",
    "https://openalex.org/W6787473312",
    "https://openalex.org/W2963290013",
    "https://openalex.org/W2101807845",
    "https://openalex.org/W4285309821",
    "https://openalex.org/W2081895431",
    "https://openalex.org/W2163492840",
    "https://openalex.org/W6732520560",
    "https://openalex.org/W6764051988",
    "https://openalex.org/W4392903754",
    "https://openalex.org/W3102455230",
    "https://openalex.org/W3113303810"
  ],
  "abstract": "Cross-subject electroencephalogram (EEG) based seizure subtype classification is very important in precise epilepsy diagnostics. Deep learning is a promising solution, due to its ability to automatically extract latent patterns. However, it usually requires a large amount of training data, which may not always be available in clinical practice. This paper proposes Multi-Branch Mutual-Distillation (MBMD) Transformer for cross-subject EEG-based seizure subtype classification, which can be effectively trained from small labeled data. MBMD Transformer replaces all even-numbered encoder blocks of the vanilla Vision Transformer by our designed multi-branch encoder blocks. A mutual-distillation strategy is proposed to transfer knowledge between the raw EEG data and its wavelets of different frequency bands. Experiments on two public EEG datasets demonstrated that our proposed MBMD Transformer outperformed several traditional machine learning and state-of-the-art deep learning approaches. To our knowledge, this is the first work on knowledge distillation for EEG-based seizure subtype classification.",
  "full_text": "IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 32, 2024 831\nMulti-Branch Mutual-Distillation Transformer for\nEEG-Based Seizure Subtype Classification\nRuimin Peng\n , Graduate Student Member, IEEE, Zhenbang Du\n , Changming Zhao,\nJingwei Luo, Wenzhong Liu\n , Member, IEEE, Xinxing Chen\n , Member, IEEE,\nand Dongrui Wu\n , Fellow, IEEE\nAbstract— Cross-subject electroencephalogram (EEG)\nbased seizure subtype classification is very important in\nprecise epilepsy diagnostics. Deep learning is a promis-\ning solution, due to its ability to automatically extract\nlatent patterns. However, it usually requires a large amount\nof training data, which may not always be available\nin clinical practice. This paper proposes Multi-Branch\nMutual-Distillation (MBMD) Transformer for cross-subject\nEEG-based seizure subtype classification, which can be\neffectively trained from small labeled data. MBMD Trans-\nformer replaces all even-numbered encoder blocks of the\nvanilla Vision Transformer by our designed multi-branch\nencoder blocks. A mutual-distillation strategy is pro-\nposed to transfer knowledge between the raw EEG data\nand its wavelets of different frequency bands. Experi-\nments on two public EEG datasets demonstrated that\nour proposed MBMD Transformer outperformed several\ntraditional machine learning and state-of-the-art deep learn-\ning approaches. To our knowledge, this is the first work\non knowledge distillation for EEG-based seizure subtype\nclassification.\nIndex Terms— Transformer, knowledge distillation, EEG,\nseizure subtype classification.\nI. I NTRODUCTION\nE\nPILEPSY is a widespread neurological disorder charac-\nterized by the rapid and early abnormal electrical activity\nof neurons in the brain, affecting more than 50 million people\nglobally [1], [2]. Among them, over 30% have intractable\nepilepsy, which significantly impacts the patients’ emotional,\nManuscript received 17 December 2023; revised 19 January 2024;\naccepted 10 February 2024. Date of publication 13 February 2024;\ndate of current version 20 February 2024. This work was supported\nby the National Key Research and Development Program of China\nunder Grant 2022YFE0204700. (Corresponding authors: Xinxing Chen;\nDongrui Wu.)\nRuimin Peng, Zhenbang Du, Wenzhong Liu, and Dongrui Wu are\nwith the Belt and Road Joint Laboratory on Measurement and Control\nTechnology, Huazhong University of Science and Technology, Wuhan\n430074, China (e-mail: drwu@hust.edu.cn).\nChangming Zhao is with the AI Platform, Software Engineering\nResearch Center, Dongfeng Corporation Research and Development\nInstitute, Wuhan 430072, China.\nJingwei Luo is with China Electronic System Technology Company\nLtd., Beijing 100089, China.\nXinxing Chen is with the Guangdong Provincial Key Laboratory\nof Human-Augmentation and Rehabilitation Robotics in Universities,\nSouthern University of Science and Technology, Shenzhen 518055,\nChina (e-mail: chenxx@sustech.edu.cn).\nDigital Object Identifier 10.1109/TNSRE.2024.3365713\nbehavioral, and cognitive functions, severely limiting their\nability to engage in daily activities [2], [3]. Furthermore,\ndisruptions in cognition and consciousness in severe cases\nimpose significant risks to the patient’s safety and well-being.\nConsequently, precise diagnosis and effective treatment for\nepilepsy are very important.\nClinical diagnosis of epilepsy heavily relies on the expertise\nof medical professionals to analyze patients’ electroencephalo-\ngram (EEG), which is demanding and time-consuming.\nTherefore, an automatic seizure diagnosis system, which ana-\nlyzes EEG recordings automatically and rapidly, is highly\ndesirable. Seizure detection, i.e., recognizing and marking the\nictal fragments in EEG recordings, has been extensively stud-\nied in the literature; however, seizure subtype classification,\nwhich is critical in determining the appropriate therapies with\nmedicine or surgery [4], has not received enough attention.\nThe 2017 International League Against Epilepsy guide-\nline [5], [6] categorizes epileptic seizures into generalized\nseizures [e.g., absence seizures (ABSZ), tonic seizures\n(TNSZ), and tonic-clonic seizures (TCSZ)], focal seizures\n(FSZ), and a combination of generalized and focal seizures.\nDifferent seizure types may have different prorogation pat-\nterns, e.g., ABSZ diffuses to the entire brain, whereas\nFSZ only affects a local area. Additionally, many seizures\nare caused by specific and personalized diseased tissue.\nThese characteristics make it very challenging for automatic\ncross-patient seizure subtype classification.\nConventional seizure subtype classification approaches usu-\nally involve three steps: data pre-processing, feature extraction,\nand classification [7]. Previous studies have extracted a large\nnumber of features to be used in machine learning models,\ne.g., support vector machine (SVM) [8], ridge classifier (RC),\nlogistic regression (LR) [9], and gradient boosting decision\ntree (GBDT). However, these manually extracted features may\nnot be optimal.\nDeep neural networks, e.g., convolutional neural networks\n(CNNs), recurrent neural networks, and autoencoders, have\nalso been extensively used for automatic EEG feature extrac-\ntion [10]. Recently, Transformer [11] based models have\nachieved great success in numerous tasks.\nA deep learning model may be very large. To reduce\nthe model size and enhance the training efficiency, various\ntechniques for model compression have been proposed, e.g.,\n© 2024 The Authors. This work is licensed under a Creative Commons Attribution 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by/4.0/\n832 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 32, 2024\npruning [12], low-rank approximation [13], quantization [14],\ncompact network design [15], and knowledge distillation [16].\nKnowledge distillation typically employs a large teacher model\nto provide soft labels, guiding the training of a more compact\nstudent network. By leveraging the teacher’s knowledge, the\nsmaller student model mimics the teacher’s output, achieving\ncomparable or even better performance. Different from the\nconventional use of a fixed teacher model, Zhang et al.\n[17] proposed mutual learning, where every student model\ncan learn from others. Fig. 1 illustrates the general ideas of\nknowledge distillation and mutual learning.\nMost model compression techniques require a large teacher\nmodel trained on big data. However, for data scarcity scenarios\nlike seizure subtype classification, a large teacher model may\nnot be available. In this situation, self-distillation could be used\nto distill a network’s knowledge and guide its training. Self-\ndistillation can be implemented by using data augmentation or\nauxiliary structure [18], as illustrated in Fig. 2. Data augmen-\ntation based self-distillation enforces consistent predictions\namong augmented copies of the same instance or two instances\nfrom the same class. Auxiliary structure based self-distillation\ndesigns additional branches for the backbone network, and\nenforces them to be similar.\nThis paper proposes a novel multi-branch mutual-distillation\n(MBMD) Transformer for EEG-based seizure subtype classi-\nfication, which can be effectively trained from small labeled\ndata. Our main contributions are :\n1) We design a novel multi-branch Transformer. It uses\nmulti-branch encoder blocks, which employ a\nmulti-branch feedforward network (FFN) to process the\nwavelets decomposed from the raw EEG signals with\ndifferent frequency bands. All wavelets use the same\nclass label as the raw data, and the block output is an\nensemble of all branches.\n2) We propose a novel mutual-distillation strategy to facil-\nitate knowledge transfer between the raw EEG data and\nthe wavelets. It enables the model to uncover more\nhidden information and achieve improved performance.\n3) To our knowledge, this study is the first attempt to intro-\nduce self-distillation in cross-patient seizure subtype\nclassification.\nThe remainder of this paper is organized as fol-\nlows. Section II briefly reviews related works. Section III\ndescribes the details of our proposed MBMD Transformer.\nSection IV presents the performance of MBMD Trans-\nformer on two seizure subtype classification datasets. Finally,\nSection V draws conclusions and outlines some future research\ndirections.\nII. R ELATED WORKS\nThis section reviews prior works on EEG-based seizure\nsubtype classification and self-distillation.\nA. EEG-Based Seizure Subtype Classification\nBoth traditional machine learning and deep learning have\nbeen used in EEG-based seizure subtype classification.\nAn important consideration in traditional machine learn-\ning is to extract meaningful features from EEG signals.\nFig. 1. (a) Knowledge distillation; and, (b) mutual learning.\nFig. 2. Self-distillation strategies. (a) Data augmentation; and, (b) aux-\niliary structure.\nVanabelle et al. [19] employed 22 features from the time and\nfrequency domains to train an XGBoost model. Tian et al. [20]\nused time, frequency, and time-frequency domain features in\nmultiple classifiers for seizure detection. Zhao et al. [21]\nextracted 41 time/spectral/time-frequency domain and nonlin-\near features to train semi-supervised and unsupervised transfer\nboosting algorithms for seizure subtype classification.\nMany deep learning approaches have been proposed to\neliminate manual feature extraction. Li et al. [22] pro-\nposed CE-stSENet, a multi-scale Squeeze-and-Excitation\nnetwork [23] to extract temporal and spectral representations.\nPeng et al. [24] developed a time information enhancement\nmodule to improve the classical EEGNet [25]. They further\nproposed Wavelet2Vec [26], which combined wavelet decom-\nposition with Vision Transformer (ViT) [27]. Tang et al. [4]\ndeveloped a self-supervised algorithm to train a recurrent\ngraph neural network.\nB. Self-Distillation\nSelf-distillation distills knowledge from the internal network\nrather than external ones. An important consideration is how\nto acquire additional knowledge.\nFrom the perspective of data augmentation, Xu and Liu [28]\nproposed Data-Distortion Guided Self-Distillation (DDGSD)\nPENG et al.: MBMD TRANSFORMER FOR EEG-BASED SEIZURE SUBTYPE CLASSIFICATION 833\nFig. 3. A vanilla ViT for EEG signal classification.\nfor training CNNs, e.g., ResNet [29]. It employed random mir-\nroring and cropping techniques to augment data and enforced\nconsistency in predictions across different augmentations of\nthe same image during network training. Similarly, Yun et al.\nproposed class-wise self-knowledge distillation [30], which\nrandomly samples an auxiliary batch sharing the same label\nas the primary training batch and aligns their predictions.\nAnother self-distillation methodology involves the design\nof auxiliary structures. Zhang et al. devised Be Your Own\nTeacher (BYOT) [31] that embedded classifiers for the shallow\nResBlocks of ResNet. These shallow branches functioned as\nstudents to learn knowledge from the prediction provided by\nthe deepest block, thereby enhancing the learning capacity\nof the shallow blocks. Lan et al. designed an On-the-Fly\nNative Ensemble (ONE) [32], which added multiple branches\nof high-level ResBlocks on shared low-level ones. During\ntraining, the knowledge of ensemble prediction would be dis-\ntilled into individual branches for enhancing model learning.\nHou et al. [33] presented a Self Attention Distillation (SAD)\napproach, which conducted layer-wise and top-down attention\ndistillation to augment the representation learning process for\nlane detection. Ge et al. [34] proposed the BAtch Knowl-\nedge Ensembling (BAKE) technique, refining soft targets for\nanchor images by propagating and ensembling knowledge\nfrom other samples within the same mini-batch. Furthermore,\nthe recently proposed Self-Knowledge-Distillation from image\nMixture (MixSKD) [18] leveraged Mixup [35], a popular\ndata augmentation approach, with auxiliary feature alignment\nmodules to transform feature maps from shallow layers to\nmatch with the final feature map.\nIII. M ETHODOLOGY\nThis section introduces our proposed MBMD Trans-\nformer and its training strategy. The code is available at\nhttps://github.com/rmpeng/EBE-Transformer.\nA. Vanilla ViT for EEG Signal Classification\nFig. 3 illustrates the training process of a vanilla ViT for\nEEG signal classification. Our proposed MBMD Transformer\nis modified from it, as introduced later in this section.\nEEG signals with dimensionality [ C × 1 × L], where C\nis the number of channels and L the number of time domain\nsample points, are first segmented into multiple fragments and\nencoded with positional encoding. The generated embeddings\nare then input into a Transformer encoder block, comprised of\na multi-head attention layer and an FFN layer. This encoder\nblock is repeated N times to learn the latent patch represen-\ntations. The average of all patch representations after the Nth\nencoder block is taken as the feature for final classification.\nThe classical cross-entropy loss Lce is used in training the\nvanilla ViT:\nLce = −1\nK\nK∑\nk=1\nlog\n(\np (y = k|x, θ)\n)\n(1)\nwhere K is the number of class, x the raw input EEG trial,\nand θ the model parameters.\nB. Our Proposed MBMD Transformer\nOur proposed MBMD Transformer replaces every\neven-numbered encoder block in Fig. 3 by a multi-branch\nencoder block, as shown in Fig. 4.\nFig. 4(a) explains the training and test process of MBMD\nTransformer. In the training phase, MBMD Transformer first\nuses Wavelet Packet Decomposition (WPD) [26] to generate\nauxiliary wavelets corresponding to 6 different EEG fre-\nquency bands, namely, δ (0-4 Hz), θ (4-8 Hz), α (8-16 Hz),\nβ (16-32 Hz), γ (32-64 Hz), and the remaining (other ), which\nhave the same label as the raw EEG trial. Fig. 5 illustrates the\nprocess of decomposing a 128 Hz EEG trial.\nNext, the raw EEG and its auxiliary wavelets enter the\nsame linear projection layer to generate embeddings, which\nare fed into a traditional Transformer encoder block and our\nproposed multi-branch encoder block, as shown in Fig. 4(b).\nDifferent embeddings share the same multi-head attention\nlayer and the same FFN layer in the traditional encoder block,\nbut the same multi-head attention layer and separate FNN\nlayers in the multi-branch encoder block (e.g., FFN δ for δ\nwave, as shown in Fig. 4(c)). Note that in the multi-branch\nencoder block, the raw EEG data are processed by all 6 Expert\nFFNs and their average is computed. A wavelet attention\nmechanism is developed to weight the 6 Expert FNNs, as intro-\nduced in Subsection III-C. The traditional encoder block and\nmulti-branch encoder block pair is repeated N\n2 times, resulting\nin a total of N encoder blocks. The final representations of the\nraw EEG data and the 6 wavelets are concatenated for clas-\nsification. Predictions from the 6 wavelets also serve as soft\nlabels for mutual-distillation, as explained in Subsection III-D.\nIn the test phase, only the raw EEG trials (but not the\nwavelets) are fed into the MBMD Transformer for classifi-\ncation. Every multi-branch encoder block sends the ensemble\noutputs of all 6 Expert FFNs to the next traditional encoder\nblock.\nC. Mutual-Distillation\nAllen-Zhu and Li [36] introduced a ‘multi-view’ concept to\nexplain why ensemble/knowledge distillation succeeds in deep\nlearning, i.e., self-distillation could be regarded as implicitly\ncombining ensemble and knowledge distillation to improve the\ntest accuracy. Inspired by the ‘multi-view’ theory, we propose\n834 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 32, 2024\nFig. 4. MBMD Transformer with mutual-distillation. (a) Training and test; (b) the overall structure; and, (c) auxiliary data processing (take δ wave\nas an example).\nFig. 5. WPD of 128 Hz EEG signal.\na mutual-distillation strategy to enhance model learning from\nvarious auxiliary wavelets. It takes the prediction from the raw\nEEG data, and these more accurate predictions from the auxil-\niary branches, as peer teachers. Multiple peer teachers enable\nthe model to learn more comprehensive patterns, enhancing\nthe overall training effectiveness.\nMore specifically, let Fbranch =\n{\nFδ, Fθ , Fα, Fβ, Fγ ,\nFother\n}\nbe the feature sets learned by models Mbranch ={\nMδ, Mθ , Mα, Mβ, Mγ , Mother\n}\nwith their corresponding\nExpert FFNs, and Fdata be the feature set of raw data learned\nby the ensemble model Mdata . Mutual-distillation aligns\nMdata and Mbranch , so they together can learn a larger feature\nset Fdata ∪ Fbranch .\nAs illustrated in the training phase of Fig. 4(a), knowledge\nfrom the raw EEG data is transferred to supervise the training\nof each branch, whereas the predictions from the branches are\nalso used to improve the performance on the raw data.\nGenerally, knowledge distillation adopts Kullback-Leibler\ndivergence Lkl to measure the consistency between the pre-\ndiction probability distributions of the student model ps and\nthe teacher model pt [16]:\nLkl ( pt ||ps) = −1\nK\nK∑\nk=1\npt (y = k|x, θ)log pt (y = k|x, θ)\nps (y = k|x, θ),\n(2)\nwhere p (y = k|x, θ) = sof tmax (zy=k) is the prediction\nprobability for class k. To avoid ignoring low probabili-\nties, we introduce a distillation temperature T to enhance\nknowledge transfer:\npdistill (y = k|x, θ) = exp(zy=k/T )\n∑ K\nm=1 exp(zy=m/T )\n. (3)\nTo summarize, the loss for mutual-distillation Ldistill is:\nLdistill = 1\n2\nB∑\nb=1\n(\nLkl ( pdistill\ndata ||pdistill\nb )+Lkl ( pdistill\nb ||pdistill\ndata )\n)\n.\n(4)\nD. Multi-Branch Encoder Block\nThe multi-branch encoder block splits the FFN layer in the\ntraditional Transformer encoder block into multiple branches,\neach for a distinct wavelet frequency band. Fig. 6 illustrates\nthe process.\nAs depicted in Fig. 6(a), each Expert FFN handles a differ-\nent wavelet, ensuring the learned embeddings from different\nwavelets are independent, which are essential to mutual-\ndistillation. As described in Fig. 6(b), all Expert FFNs process\nthe raw EEG data, and the final embedding is their average.\nWe use a branch-wise wavelet attention mechanism to learn\nan adaptive weight vector w for the expert FNNs. Initially,\nall branches are assigned equal weights, which are iteratively\nadjusted during training. Before the ensemble operation, a\nsoftmax function is applied to normalize w. The classification\nfor raw data zdata is:\nzdata =\nB∑\nb=1\nsoftmax(wb) · zb, (5)\nwhere B is the number of branches, and wb ∈ w =\n[w1, w2, . . . , wB ] and zb =\n[\nzy=1, zy=2, . . . ,zy=K ]\nare the\nb-th branch’s weight and classification, respectively.\nPENG et al.: MBMD TRANSFORMER FOR EEG-BASED SEIZURE SUBTYPE CLASSIFICATION 835\nFig. 6. The multi-branch encoder block for processing (a) the auxiliary\nwavelets; and, (b) the raw EEG data.\nL1 regularization Lnorm is used to promote the weight\nsparsity:\nLnorm =\nB∑\nb=1\n|wb| (6)\nE. Overall Loss Function for MBMD Transformer\nThe overall loss function for MBMD Transformer training\nis:\nL = Lce + Ldistill + λLnorm , (7)\nwhere λ is a hyperparameter to trade-off the strength of\nnormalization, which was set to 0 .01 in our experiments.\nIV. E XPERIMENTS\nThis section presents the experimental results on two seizure\nsubtype classification datasets, to validate the performance of\nour proposed MBMD Transformer.\nA. Datasets, Preprocessing, and Experimental Settings\nTwo public seizure datasets, CHSZ [24] and TUSZ (V1.5.2)\n[37], were used. The former includes EEG recordings from\n27 pediatric patients, and the latter from 68 patients spanning\nall age groups. This study focused on four typical seizure\nsubtypes: ABSZ, FSZ, TNSZ, and TCSZ. Table I summarizes\nthe characteristics of the two datasets.\nAll recordings were first down-sampled to 128 Hz. We then\napplied a 50 Hz notch filter, a 64 Hz low-pass filter, and\ndetrending, to remove EEG artifacts. Next, we performed\nre-referencing to generate standardized 20-channel record-\nings [22], which were then segmented using a 4-second sliding\nwindow, with a 50% overlap between two successive windows.\nAll deep models used batch size 32, AdamW optimizer with\nweight decay 5 e−5, learning rate 0 .001, and early stopping\nTABLE I\nSUMMARY OF CHSZ AND TUSZ D ATASETS\nwith patience 10 in training. All Transformer-based models\nused 4 encoder blocks, patch size 64, and embedding dimen-\nsionality 128. All self-distillation approaches used identical\ndistillation temperature T = 6.\nTo consider class-imbalance, the raw accuracy (ACC),\nbalanced classification accuracy [38] (BCA; the average\nof per-class accuracies), and weighted F1 score [39] (the\nweighted average of per-class F1 scores) were used together\nfor model performance evaluation.\nWe followed [24] to conduct three-fold cross-patient vali-\ndations. All reported results were the average of ten repeats.\nB. Overall Performance\nMBMD Transformer was first compared with nine existing\nEEG-based seizure subtype classification approaches, includ-\ning four traditional approaches (SVM, RC, LR, and GBDT)\nand five state-of-the-art deep models (EEGNet, TIE-EEGNet,\nCE-stSENet, ViT, and WaveletTransformer). We followed [21]\nto extract 41 features for the four traditional approaches.\nTable II demonstrates MBMD Transformer’s superior per-\nformance: it obtained the highest ACC, BCA and weighted\nF1 scores on both datasets. Particularly, compared with\nthe 4-layer WaveletTransformer, an ensemble model of six\nfour-layer vanilla ViTs (one for each wavelet, a total of\n4*6 encoders), MBMD Transformer improved the BCA by\nat least 1 .4%, using only two traditional encoder blocks\nand two multi-branch encoder blocks (4 encoders). When\nWaveletTransformer used only one layer (a total of 1*6\nencoders, similar to MBMD Transformer), its performance\nwas much worse than MBMD Transformer, suggesting that\nMBMD Transformer can use the encoders more effectively.\nC. Effectiveness of Mutual-Distillation\nTo demonstrate the effectiveness of mutual-distillation in\nMBMD Transformer, we compared it with five existing\nself-distillation approaches (BYOT, DDGSD, ONE, BAKE,\nand SAD). They were originally proposed for computer vision\ntasks, using CNN-based backbones. For fair comparison,\nwe replaced their CNN-based backbones with the ViT back-\nbone.\nTable III shows the performance. On the CHSZ dataset,\nMBMD Transformer achieved the highest ACC and F1 score,\nand the second-highest BCA, only lower than DDGSD by\n0.005. On the TUSZ dataset, MBMD Transformer ranked first\non F1 score and second on ACC and BCA (the latter two\nwere only lower than the best by 0 .001). Overall, MBMD\nTransformer had the best performance.\nAblation studies were performed to further investigate the\neffectiveness of mutual-distillation. We replaced the loss item\n836 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 32, 2024\nTABLE II\nPERFORMANCE (MEAN ±STD) OF DIFFERENT SEIZURE SUBTYPE CLASSIFICATION APPROACHES ON CHSZ\nAND TUSZ D ATASETS . THE BEST PERFORMANCE IN EACH COLUMN IS MARKED IN BOLD\nTABLE III\nPERFORMANCE (MEAN ±STD) OF DIFFERENT DISTILLATION APPROACHES ON CHSZ AND TUSZ D ATASETS .\nTHE BEST PERFORMANCE IN EACH COLUMN IS MARKED IN BOLD, AND THE SECOND BEST WITH AN UNDERLINE\nLdistill with Le\nkl , which retains only the knowledge flow from\neach wavelet branch to the raw data, but not the opposite :\nLe\nkl =\nB∑\nb=1\nLkl ( pdistill\nb ||pdistill\ndata ). (8)\nTable IV shows the results. Compared with the performance\nusing Lce only, adding Le\nkl always improved the performance,\nand replacing Le\nkl by our proposed Ldistill further improved\nthe performance on both datasets and for all three measures.\nThis demonstrated the advantage of bi-directional knowledge\ntransfer over single-directional transfer.\nD. Effectiveness of Wavelet Attention\nAblation studies were also performed to investigate the\neffectiveness of our proposed branch-wise wavelet attention\nmechanism. It was compared with two baselines: an average\nstrategy and a sample-wise gate network. The former averaged\nthe six branches’ outputs as the prediction, whereas the latter\nadded a one-hidden-layer perceptron to predict the branch\nweights for each sample, inspired by Mixture-of-Experts mod-\nels [40], [41]. Additionally, a normalization term Limp [42]\nwas incorporated to encourage all branches to have similar\nimportance.\nTable V shows that our proposed wavelet attention mech-\nanism with Lnorm achieved the best ACC and F1 score on\nboth datasets, and also the best BCA on CHSZ. The average\nstrategy had the best BCA on TUSZ, only 0 .005 higher than\nthe wavelet attention mechanism. The gate network had the\nlowest overall performance.\nE. Effectiveness of the Multi-Branch Encoder Block\nWe also investigated the effectiveness of our proposed\nmulti-branch encoder block, by comparing the following four\nmodels:\n1) Vanilla ViT model, which consisted of four sequentially\nconnected traditional encoder blocks.\n2) Our proposed MBMD Transformer, which replaced the\nsecond and fourth traditional encoder blocks of the\nvanilla ViT model with the multi-branch encoder block.\n3) MBMD Transformer-1, which replaced the last tradi-\ntional encoder block of the vanilla ViT model by a\nmulti-branch encoder block.\n4) MBMD Transformer-2, which replaced the last two\nblocks of the vanilla ViT model by two multi-branch\nencoder blocks.\nFig. 7 shows the results. Our proposed MBMD Transformer\nachieved the best performance, but its two variants also out-\nperformed the vanilla ViT model, suggesting the effectiveness\nof the multi-branch encoder block.\nF . Parameter Sensitivity Analysis\nThis subsection studies the sensitivity of MBMD Trans-\nformer performance to its two important parameters,\ndistillation temperature T and the number of wavelets.\nFig. 8 shows the results for T ∈ [3, 9]. Generally, on both\ndatasets, MBMD Transformer achieved higher ACC, BCA and\nF1 scores than ViT for all T ; however, different distillation\ntemperatures resulted in different performance improvements.\nSo, it is desirable to use a validation set to pick the optimal T .\nPENG et al.: MBMD TRANSFORMER FOR EEG-BASED SEIZURE SUBTYPE CLASSIFICATION 837\nTABLE IV\nPERFORMANCE (MEAN ±STD) OF DIFFERENT DISTILLATION STRATEGIES ON CHSZ AND TUSZ D ATASETS .\nTHE BEST PERFORMANCE IN EACH ROW IS MARKED IN BOLD\nTABLE V\nPERFORMANCE (MEAN ±STD) OF DIFFERENT ENSEMBLE STRATEGIES ON CHSZ AND TUSZ D ATASETS .\nTHE BEST PERFORMANCE IN EACH ROW IS MARKED IN BOLD\nFig. 7. Performance of the vanilla ViT and three MBMD Transformer\nvariants on (a) CHSZ; and, (b) TUSZ.\nTo study the sensitivity of MBMD Transformer performance\nto the number of wavelet branches, we designed two MBMD\nTransformer variants, with two and three wavelet branches,\nrespectively. Specifically, in the 2-branch MBMD Transformer,\nδ, θ and α wavelets were merged into a single low-frequency\nband (0 − 16 Hz), and the remaining wavelets were merged\ninto a single high-frequency band (16 − 128 Hz). The three\nfrequency bands used in the 3-branch MBMD Transformer\nwere δ ∪ θ, α ∪ β and γ ∪ others , respectively.\nFig. 9 shows the results. All three MBMD Transformer vari-\nants performed similarly on both datasets, all outperforming\nFig. 8. Sensitivity of MBMD Transformer to the distillation temperature\nT on (a) CHSZ; and, (b) TUSZ.\nthe vanilla ViT model. In conclusion, our proposed MBMD\nTransformer is not sensitive to the number of wavelet branches.\nG. Discussion\nTo validate that our proposed mutual-distillation strategy\ncould enhance learning by ensembling and distilling the\n838 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 32, 2024\nFig. 9. Sensitivity of MBMD Transformer to the number of wavelet\nbranches on (a) CHSZ; and, (b) TUSZ.\n‘multi-view’ [36] feature sets from the auxiliary wavelet\nbranches, we conducted the following three experiments :\n1) Experiment 1: Training a vanilla ViT model on the raw\nEEG data, and testing on the test set’s raw and wavelets\ndata. This experiment aimed to demonstrate how the\nsingle ViT trained on the raw data performs on different\nwavelets.\n2) Experiment 2: Training a unique vanilla ViT model for\neach wavelet, and testing on the corresponding wavelet\nof the test data. This experiment aimed to investigate the\nViT learning ability on each wavelet.\n3) Experiment 3: Training our proposed MBMD Trans-\nformer, and testing on the raw data and each wavelet of\nthe test data. This experiment aimed to validate that our\nproposed MBMD Transformer can achieve better per-\nformance on the raw data by utilizing latent information\n(features) from the wavelets.\nFig. 10 shows the results. The single ViT trained in Exper-\niment 1 had low BCAs on all six wavelets, due to the\nmismatching between training and test. The separate ViTs\ntrained in Experiment 2 achieved generally the highest BCAs\non the corresponding wavelets, due to perfect matching\nbetween training and test. Finally, compared with the ViTs\nin Experiment 2, our proposed MBMD Transformer in Exper-\niment 3 achieved comparable or only slightly lower BCAs\non the individual wavelets, but higher BCAs on the raw data,\nvalidating the benefits of utilizing latent information (features)\nfrom the wavelets.\nFig. 10. BCAs on six different wavelets and the raw data on (a) CHSZ;\nand, (b) TUSZ.\nV. C ONCLUSION AND FUTURE RESEARCH\nThis paper has proposed MBMD Transformer for\nEEG-based seizure subtype classification. It replaces all\neven-numbered encoder blocks of the vanilla ViT model by\nmulti-branch encoder blocks, which use mutual-distillation\nbetween the raw EEG data and its wavelet branches to transfer\nknowledge between them. Experiments on the public CHSZ\nand TUSZ datasets demonstrated that our proposed MBMD\nTransformer outperformed several traditional machine learning\nand state-of-the-art deep learning approaches in cross-subject\nseizure subtype classification.\nThe following directions will be considered in our future\nresearch:\n1) Data augmentation in MBMD Transformer considered\nonly the time-spectral characteristics of EEG signals.\nOther augmentation strategies, e.g., time domain and\nfrequency domain, could also be explored.\n2) This paper only considered supervised training. Similar\nto π−model [43], we could also consider semi-\nsupervised training, which minimizes the difference\nbetween the outputs of the branches and the ensemble.\n3) MBMD Transformer may also be applied to other\nfrequency sensitive BCI paradigms, e.g., sleep stage\nclassification [44].\nREFERENCES\n[1] Z. Lasefr, S. S. V . N. R. Ayyalasomayajula, and K. Elleithy, “Epilepsy\nseizure detection using EEG signals,” in Proc. IEEE 8th Annu. Ubiqui-\ntous Comput., Electron. Mobile Commun. Conf. (UEMCON), New York\nCity, NY , USA, Oct. 2017, pp. 162–167.\n[2] S. Wong et al., “EEG datasets for seizure detection and prediction—\nA review,” Epilepsia Open, vol. 8, pp. 252–267, Jun. 2023.\nPENG et al.: MBMD TRANSFORMER FOR EEG-BASED SEIZURE SUBTYPE CLASSIFICATION 839\n[3] B. Singhal and F. Pooja, “Unveiling intractable epileptogenic brain\nnetworks with deep learning algorithms: A novel and comprehensive\nframework for scalable seizure prediction with unimodal neuroimaging\ndata in pediatric patients,” 2023, arXiv:2309.02580.\n[4] S. Tang et al., “Self-supervised graph neural networks for improved\nelectroencephalographic seizure analysis,” in Proc. Int. Conf. Learn.\nRepresent., Apr. 2022, pp. 1–23.\n[5] I. E. Scheffer et al., “ILAE classification of the epilepsies: Position paper\nof the ILAE commission for classification and terminology,” Epilepsia,\nvol. 58, no. 4, pp. 512–521, Apr. 2017.\n[6] R. S. Fisher et al., “Operational classification of seizure types by\nthe international league against epilepsy: Position paper of the ILAE\ncommission for classification and terminology,” Epilepsia, vol. 58, no. 4,\npp. 522–530, Apr. 2017.\n[7] P. Boonyakitanont, A. Lek-uthai, K. Chomtho, and J. Songsiri, “A review\nof feature extraction and performance evaluation in epileptic seizure\ndetection using EEG,” Biomed. Signal Process. Control , vol. 57,\nMar. 2020, Art. no. 101702.\n[8] S. Li, W. Zhou, Q. Yuan, S. Geng, and D. Cai, “Feature extraction and\nrecognition of ictal EEG using EMD and SVM,” Comput. Biol. Med.,\nvol. 43, no. 7, pp. 807–816, 2013.\n[9] K. Samiee, P. Kovács, and M. Gabbouj, “Epileptic seizure classification\nof EEG time-series using rational discrete short-time Fourier transform,”\nIEEE Trans. Biomed. Eng., vol. 62, no. 2, pp. 541–552, Feb. 2015.\n[10] A. Shoeibi et al., “Epileptic seizures detection using deep learning\ntechniques: A review,” Int. J. Environ. Res. Public Health, vol. 18,\nno. 11, p. 5780, 2021.\n[11] A. Vaswani et al., “Attention is all you need,” in Proc. Adv. Neural Inf.\nProcess. Syst., Long Beach, CA, USA, Dec. 2017, pp. 1–11.\n[12] D. Blalock, J. J. Gonzalez Ortiz, J. Frankle, and J. Guttag, “What is the\nstate of neural network pruning?” in Proc. Mach. Learn. Syst., Austin,\nTX, USA, Mar. 2020, pp. 129–146.\n[13] X. Yu, T. Liu, X. Wang, and D. Tao, “On compressing deep models by\nlow rank and sparse decomposition,” in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit. (CVPR), Honolulu, HI, USA, Jul. 2017, pp. 67–76.\n[14] A. Polino, R. Pascanu, and D. Alistarh, “Model compression via\ndistillation and quantization,” in Proc. Int. Conf. Learn. Represent.,\nVancouver, BC, Canada, May 2018, pp. 1–21.\n[15] X. Zhang, X. Zhou, M. Lin, and J. Sun, “ShuffleNet: An extremely\nefficient convolutional neural network for mobile devices,” in\nProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , Jun. 2018,\npp. 6848–6856.\n[16] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural\nnetwork,” 2015, arXiv:1503.02531.\n[17] Y . Zhang, T. Xiang, T. M. Hospedales, and H. Lu, “Deep mutual\nlearning,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.,\nSalt Lake City, UT, USA, Jun. 2018, pp. 4320–4328.\n[18] C. Yang et al., “MixSKD: Self-knowledge distillation from mixup for\nimage recognition,” in Proc. Eur. Conf. Comput. Vis., Tel Aviv, Israel,\nOct. 2022, pp. 534–551.\n[19] P. Vanabelle, P. De Handschutter, R. El Tahry, M. Benjelloun, and\nM. Boukhebouze, “Epileptic seizure detection using EEG signals and\nextreme gradient boosting,” J. Biomed. Res., vol. 34, no. 3, p. 228, 2020.\n[20] X. Tian et al., “Deep multi-view feature learning for EEG-based epileptic\nseizure detection,” IEEE Trans. Neural Syst. Rehabil. Eng., vol. 27,\nno. 10, pp. 1962–1972, Oct. 2019.\n[21] C. Zhao, R. Peng, and D. Wu, “Source-free domain adaptation (SFDA)\nfor privacy-preserving seizure subtype classification,” IEEE Trans. Neu-\nral Syst. Rehabil. Eng., vol. 31, pp. 2315–2325, 2023.\n[22] Y . Li, Y . Liu, W.-G. Cui, Y .-Z. Guo, H. Huang, and Z.-Y . Hu, “Epileptic\nseizure detection in EEG signals using a unified temporal-spectral\nsqueeze-and-excitation network,” IEEE Trans. Neural Syst. Rehabil.\nEng., vol. 28, no. 4, pp. 782–794, Apr. 2020.\n[23] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit., Salt Lake City, UT,\nUSA, Jun. 2018, pp. 7132–7141.\n[24] R. Peng et al., “TIE-EEGNet: Temporal information enhanced EEGNet\nfor seizure subtype classification,” IEEE Trans. Neural Syst. Rehabil.\nEng., vol. 30, pp. 2567–2576, 2022.\n[25] V . Lawhern, A. Solon, N. Waytowich, S. M. Gordon, C. Hung,\nand B. J. Lance, “EEGNet: A compact convolutional neural network\nfor EEG-based brain–computer interfaces,” J. Neural Eng., vol. 15,\nno. 5, 2018, Art. no. 056013.\n[26] R. Peng et al., “W A VELET2VEC: A filter bank masked autoencoder\nfor EEG-based seizure subtype classification,” in Proc. IEEE Int. Conf.\nAcoust., Speech Signal Process. (ICASSP) , Rhodes Island, Greece,\nJun. 2023, pp. 1–5.\n[27] A. Dosovitskiy et al., “An image is worth 16 × 16 words: Transformers\nfor image recognition at scale,” in Proc. Int. Conf. Learn. Represent.,\nMay 2021, pp. 1–22.\n[28] T.-B. Xu and C.-L. Liu, “Data-distortion guided self-distillation for deep\nneural networks,” in Proc. AAAI Conf. Artif. Intell., Honolulu, HI, USA,\nJan. 2019, pp. 5565–5572.\n[29] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Las Vegas, NV , USA, Jun. 2016, pp. 770–778.\n[30] S. Yun, J. Park, K. Lee, and J. Shin, “Regularizing class-wise predictions\nvia self-knowledge distillation,” in Proc. IEEE/CVF Conf. Comput. Vis.\nPattern Recognit. (CVPR), Jun. 2020, pp. 13873–13882.\n[31] L. Zhang, J. Song, A. Gao, J. Chen, C. Bao, and K. Ma, “Be your own\nteacher: Improve the performance of convolutional neural networks via\nself distillation,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV),\nSeoul, South Korea, Oct. 2019, pp. 3712–3721.\n[32] X. Lan, X. Zhu, and S. Gong, “Knowledge distillation by on-the-fly\nnative ensemble,” in Proc. Adv. Neural Inf. Process. Syst., Montreal,\nQC, Canada, Dec. 2018, pp. 1–11.\n[33] Y . Hou, Z. Ma, C. Liu, and C. C. Loy, “Learning lightweight lane\ndetection CNNs by self attention distillation,” in Proc. IEEE/CVF\nInt. Conf. Comput. Vis. (ICCV) , Seoul, South Korea, Oct. 2019,\npp. 1013–1021.\n[34] Y . Ge et al., “Self-distillation with batch knowledge ensembling\nimproves ImageNet classification,” in Proc. IEEE/CVF Conf. Comput.\nVis. Pattern Recognit., Jun. 2021, pp. 1–13.\n[35] H. Zhang, M. Cisse, Y . N. Dauphin, and D. Lopez-Paz, “ mixup: Beyond\nempirical risk minimization,” in Proc. Int. Conf. Learn. Represent.,\nVancouver, BC, Canada, May 2018, pp. 1–13.\n[36] Z. Allen-Zhu and Y . Li, “Towards understanding ensemble, knowl-\nedge distillation and self-distillation in deep learning,” 2020,\narXiv:2012.09816.\n[37] V . Shah et al., “The temple university hospital seizure detection corpus,”\nFrontiers Neuroinform., vol. 12, p. 83, Nov. 2018.\n[38] K. H. Brodersen, C. S. Ong, K. E. Stephan, and J. M. Buhmann, “The\nbalanced accuracy and its posterior distribution,” in Proc. 20th Int. Conf.\nPattern Recognit., Istanbul, Turkey, Aug. 2010, pp. 3121–3124.\n[39] D. Harbecke, Y . Chen, L. Hennig, and C. Alt, “Why only micro-\nF1? Class weighting of measures for relation classification,” 2022,\narXiv:2205.09460.\n[40] A. Subasi, “EEG signal classification using wavelet feature extraction\nand a mixture of expert model,” Exp. Syst. Appl. , vol. 32, no. 4,\npp. 1084–1093, May 2007.\n[41] E. D. Übeyli, “Wavelet/mixture of experts network structure for EEG\nsignals classification,” Expert Syst. Appl., vol. 34, no. 3, pp. 1954–1962,\nApr. 2008.\n[42] N. Shazeer et al., “Outrageously large neural networks: The sparsely-\ngated mixture-of-experts layer,” in Proc. Int. Conf. Learn. Represent.,\nToulon, France, Apr. 2017, pp. 1–19.\n[43] S. Laine and T. Aila, “Temporal ensembling for semi-supervised learn-\ning,” in Proc. Int. Conf. Learn. Represent., San Juan, Puerto Rico,\nMay 2016, pp. 1–13.\n[44] S. Zheng and D. Wu, “Semi-supervised domain adaptation for EEG-\nbased sleep stage classification,” in Proc. IEEE Int. Conf. Acoust.,\nSpeech, Signal Process., Seoul, South Korea, Apr. 2024.",
  "topic": "Electroencephalography",
  "concepts": [
    {
      "name": "Electroencephalography",
      "score": 0.7482380270957947
    },
    {
      "name": "Computer science",
      "score": 0.7063526511192322
    },
    {
      "name": "Transformer",
      "score": 0.6737256050109863
    },
    {
      "name": "Artificial intelligence",
      "score": 0.643733561038971
    },
    {
      "name": "Encoder",
      "score": 0.5159605145454407
    },
    {
      "name": "Distillation",
      "score": 0.5001826286315918
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4914630651473999
    },
    {
      "name": "Machine learning",
      "score": 0.47761648893356323
    },
    {
      "name": "Deep learning",
      "score": 0.4238578677177429
    },
    {
      "name": "Transfer of learning",
      "score": 0.4163801372051239
    },
    {
      "name": "Mutual information",
      "score": 0.4125255346298218
    },
    {
      "name": "Speech recognition",
      "score": 0.3394409120082855
    },
    {
      "name": "Engineering",
      "score": 0.13579022884368896
    },
    {
      "name": "Neuroscience",
      "score": 0.11017593741416931
    },
    {
      "name": "Psychology",
      "score": 0.10260152816772461
    },
    {
      "name": "Chemistry",
      "score": 0.059277623891830444
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I47720641",
      "name": "Huazhong University of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210153068",
      "name": "Dongfeng Motor Group (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210150595",
      "name": "Guangzhou Electronic Technology (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I3045169105",
      "name": "Southern University of Science and Technology",
      "country": "CN"
    }
  ]
}