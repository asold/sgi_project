{
  "title": "N-gram language models for massively parallel devices",
  "url": "https://openalex.org/W2510461687",
  "year": 2016,
  "authors": [
    {
      "id": "https://openalex.org/A5018769709",
      "name": "Nikolay Bogoychev",
      "affiliations": [
        "University of Edinburgh"
      ]
    },
    {
      "id": "https://openalex.org/A5109031255",
      "name": "Adam Lopez",
      "affiliations": [
        "University of Edinburgh"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2134800885",
    "https://openalex.org/W2250894086",
    "https://openalex.org/W2252042678",
    "https://openalex.org/W2063319420",
    "https://openalex.org/W2112544287",
    "https://openalex.org/W2171913306",
    "https://openalex.org/W2069864295",
    "https://openalex.org/W1507039213",
    "https://openalex.org/W2121740319",
    "https://openalex.org/W2250262609",
    "https://openalex.org/W2189576523",
    "https://openalex.org/W2150299430",
    "https://openalex.org/W2333081718",
    "https://openalex.org/W1980034063",
    "https://openalex.org/W34089377",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W162552777",
    "https://openalex.org/W4246219036",
    "https://openalex.org/W4246472747",
    "https://openalex.org/W1675130169",
    "https://openalex.org/W3145128584",
    "https://openalex.org/W2251246760",
    "https://openalex.org/W16967297",
    "https://openalex.org/W2109664771"
  ],
  "abstract": "For many applications, the query speed of N -gram language models is a computational bottleneck.Although massively parallel hardware like GPUs offer a potential solution to this bottleneck, exploiting this hardware requires a careful rethinking of basic algorithms and data structures.We present the first language model designed for such hardware, using B-trees to maximize data parallelism and minimize memory footprint and latency.Compared with a single-threaded instance of KenLM (Heafield, 2011), a highly optimized CPUbased language model, our GPU implementation produces identical results with a smaller memory footprint and a sixfold increase in throughput on a batch query task.When we saturate both devices, the GPU delivers nearly twice the throughput per hardware dollar even when the CPU implementation uses faster data structures.",
  "full_text": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1944–1953,\nBerlin, Germany, August 7-12, 2016.c⃝2016 Association for Computational Linguistics\nN-gram language models for massively parallel devices\nNikolay Bogoychev and Adam Lopez\nUniversity of Edinburgh\nEdinburgh, United Kingdom\nAbstract\nFor many applications, the query speed of\nN-gram language models is a computa-\ntional bottleneck. Although massively par-\nallel hardware like GPUs offer a poten-\ntial solution to this bottleneck, exploiting\nthis hardware requires a careful rethink-\ning of basic algorithms and data structures.\nWe present the ﬁrst language model de-\nsigned for such hardware, using B-trees to\nmaximize data parallelism and minimize\nmemory footprint and latency. Compared\nwith a single-threaded instance of KenLM\n(Heaﬁeld, 2011), a highly optimized CPU-\nbased language model, our GPU imple-\nmentation produces identical results with\na smaller memory footprint and a sixfold\nincrease in throughput on a batch query\ntask. When we saturate both devices, the\nGPU delivers nearly twice the throughput\nper hardware dollar even when the CPU\nimplementation uses faster data structures.\nOur implementation is freely available at\nhttps://github.com/XapaJIaMnu/gLM\n1 Introduction\nN-gram language models are ubiquitous in speech\nand language processing applications such as ma-\nchine translation, speech recognition, optical char-\nacter recognition, and predictive text. Because\nthey operate over large vocabularies, they are often\na computational bottleneck. For example, in ma-\nchine translation, Heaﬁeld (2013) estimates that\ndecoding a single sentence requires a million lan-\nguage model queries, and Green et al. (2014) esti-\nmate that this accounts for more than 50% of de-\ncoding CPU time.\nTo address this problem, we turn to mas-\nsively parallel hardware architectures, exempli-\nFigure 1: Theoretical ﬂoating point performance\nof CPU and GPU hardware over time (Nvidia Cor-\nporation, 2015).\nﬁed by general purpose graphics processing units\n(GPUs), whose memory bandwidth and compu-\ntational throughput has rapidly outpaced that of\nCPUs over the last decade (Figure 1). Exploiting\nthis increased power is a tantalizing prospect for\nany computation-bound problem, so GPUs have\nbegun to attract attention in natural language pro-\ncessing, in problems such as parsing (Canny et\nal., 2013; Hall et al., 2014), speech recognition\n(Chong et al., 2009; Chong et al., 2008), and\nphrase extraction for machine translation (He et\nal., 2015). As these efforts have shown, it is\nnot trivial to exploit this computational power, be-\ncause the GPU computational model rewards data\nparallelism, minimal branching, and minimal ac-\ncess to global memory, patterns ignored by many\nclassic NLP algorithms (Section 2).\nWe present the ﬁrst language model data struc-\nture designed for this computational model. Our\ndata structure is a trie in which individual nodes\nare represented by B-trees, which are searched\nin parallel (Section 3) and arranged compactly in\n1944\nmemory (Section 4). Our experiments across a\nrange of parameters in a batch query setting show\nthat this design achieves a throughput six times\nhigher than KenLM (Heaﬁeld, 2011), a highly efﬁ-\ncient CPU implementation (Section 5). They also\nshow the effects of device saturation and of data\nstructure design decisions.\n2 GPU computational model\nGPUs and other parallel hardware devices have a\ndifferent computational proﬁle from widely-used\nx86 CPUs, so data structures designed for serial\nmodels of computation are not appropriate. To\nproduce efﬁcient software for a GPU we must be\nfamiliar with its design (Figure 2).\n2.1 GPU design\nA GPU consists of many simple computational\ncores, which have neither complex caches nor\nbranch predictors to hide latencies. Because they\nhave far fewer circuits than CPU cores, GPU cores\nare much smaller, and many more of them can ﬁt\non a device. So the higher throughput of a GPU is\ndue to the sheer number of cores, each executing a\nsingle thread of computation (Figure 2). Each core\nbelongs to a Streaming Multiprocessor (SM), and\nall cores belonging to a SM must execute the same\ninstruction at each time step, with exceptions for\nbranching described below. This execution model\nis very similar to single instruction, multiple data\n(SIMD) parallelism.1\nComputation on a GPU is performed by an in-\nherently parallel function or kernel, which deﬁnes\na grid of data elements to which it will be applied,\neach processed by a block of parallel threads.\nOnce scheduled, the kernel executes in parallel on\nall cores allocated to blocks in the grid. At min-\nimum, it is allocated to a single warp—32 cores\non our experimental GPU. If fewer cores are re-\nquested, a full warp is still allocated, and the un-\nused cores idle.\nA GPU offers several memory types, which dif-\nfer in size and latency (Table 1). Unlike a CPU\nprogram, which can treat memory abstractly, a\nGPU program must explicitly specify in which\nphysical memory each data element resides. This\nchoice has important implications for efﬁciency\nthat entail design tradeoffs, since memory closer\n1Due to differences in register usage and exceptions for\nbranching, this model is not pure SIMD. Nvidia calls it\nSIMT (single instruction, multiple threads).\nFigure 2: GPU memory hierarchy and computa-\ntional model (Nvidia Corporation, 2015).\nMemory type Latency Size\nRegister 0 4B\nShared 4–8 16KB–96KB\nGlobal GPU 200–800 2GB–12GB\nCPU 10K+ 16GB–1TB\nTable 1: Latency (in clock cycles) and size of dif-\nferent GPU memory types. Estimates are adapted\nfrom Nvidia Corporation (2015) and depend on\nseveral aspects of hardware conﬁguration.\nto a core is small and fast, while memory further\naway is large and slow (Table 1).\n2.2 Designing efﬁcient GPU algorithms\nTo design an efﬁcient GPU application we must\nobserve the constraints imposed by the hardware,\nwhich dictate several important design principles.\nAvoid branching instructions. If a branching\ninstruction occurs, threads that meet the branch\ncondition run while the remainder idle (a warp di-\nvergence). When the branch completes, threads\nthat don’t meet the condition run while the ﬁrst\ngroup idles. So, to maximize performance, code\nmust be designed with little or no branching.\nUse small data structures. Total memory on a\nstate-of-the-art GPU is 12GB, expected to rise to\n24GB in the next generation. Language models\nthat run on CPU frequently exceed these sizes, so\nour data structures must have the smallest possible\nmemory footprint.\nMinimize global memory accesses. Data in\nthe CPU memory must ﬁrst be transferred to the\ndevice. This is very slow, so data structures must\nreside in GPU memory. But even when they\n1945\nData structure Size Query Ease of Construction Lossless\nspeed backoff time\nTrie (Heaﬁeld, 2011) Small Fast Yes Fast Yes\nProbing hash table (Heaﬁeld, 2011) Larger Faster Yes Fast Yes\nDouble array (Yasuhara et al., 2013) Larger Fastest Yes Very slow Yes\nBloom ﬁlter (Talbot and Osborne, 2007) Small Slow No Fast No\nTable 2: A survey of language model data structures and their computational properties.\nreside in global GPU memory, latency is high, so\nwherever possible, data should be accessed from\nshared or register memory.\nAccess memory with coalesced reads. When\na thread requests a byte from global memory,\nit is copied to shared memory along with many\nsurrounding bytes (between 32 and 128 depending\non the architecture). So, if consecutive threads\nrequest consecutive data elements, the data is\ncopied in a single operation (a coalesced read ),\nand the delay due to latency is incurred only once\nfor all threads, increasing throughput.\n3 A massively parallel language model\nLet w be a sentence, wi its ith word, and N the\norder of our model. An N-gram language model\ndeﬁnes the probability of was:\nP(w) =\n|w|∏\ni=1\nP(wi|wi−1...wi−N+1) (1)\nA backoff language model (Chen and Goodman,\n1999) is deﬁned in terms of n-gram probabil-\nities P(wi|wi−1...wi−n+1) for all n from 1 to\nN, which are in turn deﬁned by n-gram pa-\nrameters ˆP(wi...wi−n+1) and backoff parameters\nβ(wi−1...wi−n+1). Usually ˆP(wi...wi−n+1) and\nβ(wi−1...wi−n+1) are probabilities conditioned\non wi−1...wi−n+1, but to simplify the following\nexposition, we will simply treat them as numeric\nparameters, each indexed by a reversedn-gram. If\nparameter ˆP(wi...wi−n+1) is nonzero, then:\nP(wi|wi−1...wi−n+1) = ˆP(wi...wi−n+1)\nOtherwise:\nP(wi|wi−1...wi−n+1) =\nP(wi|wi−1...wi−n+2) ×β(wi−1...wi−n+1)\nThis recursive deﬁnition means that the probabil-\nity P(wi|wi−1...wi−N+1) required for Equation 1\nmay depend on multiple parameters. If r(<N ) is\nthe largest value for which ˆP(wi|wi−1...wi−r+1)\nis nonzero, then we have:\nP(wi|wi−1...wi−N+1) = (2)\nˆP(wi...wi−r+1)\nN∏\nn=r+1\nβ(wi−1...wi−n+1)\nOur data structure must be able to efﬁciently ac-\ncess these parameters.\n3.1 Trie language models\nWith this computation in mind, we surveyed sev-\neral popular data structures that have been used\nto implement N-gram language models on CPU,\nconsidering their suitability for adaptation to GPU\n(Table 2). Since a small memory footprint is cru-\ncial, we implemented a variant of the trie data\nstructure of Heaﬁeld (2011). We hypothesized that\nits slower query speed compared to a probing hash\ntable would be compensated for by the throughput\nof the GPU, a question we return to in Section 5.\nA trie language model exploits two impor-\ntant guarantees of backoff estimators: ﬁrst, if\nˆP(wi...wi−n+1) is nonzero, then ˆP(wi...wi−m+1)\nis also nonzero, for all m < n ; second, if\nβ(wi−1...wi−n+1) is one, then β(wi−1...wi−p+1)\nis one, for all p > n . Zero-valued n-\ngram parameters and one-valued backoff pa-\nrameters are not explicitly stored. To com-\npute P(wi|wi−1...wi−N+1), we iteratively retrieve\nˆP(wi...wi−m+1) for increasing values of m un-\ntil we fail to ﬁnd a match, with the ﬁnal nonzero\nvalue becoming ˆP(wi...wi−r+1) in Equation 2.\nWe then iteratively retrieve β(wi−1...wi−n+1) for\nincreasing values of n starting from r + 1 and\ncontinuing until n = N or we fail to ﬁnd a\nmatch, multiplying all retrieved terms to compute\nP(wi|wi−1...wi−N+1) (Equation 2). The trie is\ndesigned to execute these iterative parameter re-\ntrievals efﬁciently.\nLet Σ be a our vocabulary, Σn the set of\nall n-grams over the vocabulary, and Σ[N] the\n1946\nAustralia\nis\nof\none\nmany\none\nhuman\nPoland\nare\nis\nare\nis\nexist\nis\nFigure 3: Fragment of a trie showing the path of\nN-gram is one of in bold. A query for theN-gram\nevery one of traverses the same path, but sinceev-\nery is not among the keys in the ﬁnal node, it re-\nturns the n-gram parameter ˆP(of|one) and returns\nto the root to seek the backoff parameter β(every\none). Based on image from Federico et al. (2008).\nset Σ1 ∪ ... ∪ ΣN . Given an n-gram key\nwi...wi−n+1 ∈Σ[N], our goal is to retrieve value\n⟨ˆP(wi...wi−n+1),β(wi...wi−n+1)⟩. We assume a\nbijection from Σ to integers in the range 1,..., |Σ|,\nso in practice all keys are sequences of integers.\nWhen n= 1, the set of all possible keys is just\nΣ. For this case, we can store keys with nontriv-\nial values in a sorted array Aand their associated\nvalues in an array V of equal length so that V[j] is\nthe value associated with key A[j]. To retrieve the\nvalue associated with key k, we seek j for which\nA[j] = k and return V[j]. Since A is sorted, j\ncan be found efﬁciently with binary or interpolated\nsearch (Figure 4).\nWhen n > 1, queries are recursive. For\nn < N , for every wi...wi−n+1 for which\nˆP(wi...wi−n+1) > 0 or β(wi...wi−n+1) <\n1, our data structure contains associated arrays\nKwi...wi−n+1 and Vwi...wi−n+1 . When key k\nis located in Awi...wi−n+1 [j], the value stored\nat Vwi...wi−n+1 [j] includes the address of arrays\nAwi...wi−n+1k and Vwi...wi−n+1k. To ﬁnd the values\nassociated with an n-gram wi...wi−n+1, we ﬁrst\nsearch the root arrayAfor j1 such that A[j1] = wi.\nWe retrieve the address ofAwi from V[j1], and we\nthen search for j2 such that Awi [j2] = wi−1. We\ncontinue to iterate this process until we ﬁnd the\nvalue associated with the longest sufﬁx of our n-\ngram stored in the trie. We therefore iteratively\nretrieve the parameters needed to compute Equa-\ntion 2, returning to the root exactly once if backoff\nparameters are required.\n3.1.1 K-ary search and B-trees\nOn a GPU, the trie search algorithm described\nabove is not efﬁcient because it makes extensive\nuse of binary search, an inherently serial algo-\nrithm. However, there is a natural extension of\nbinary search that is well-suited to GPU: K-ary\nsearch (Hwu, 2011). Rather than divide an array\nin two as in binary search, K-ary search divides it\ninto K equal parts and performs K −1 compar-\nisons simultaneously (Figure 5).\nTo accommodate large language models, the\ncomplete trie must reside in global memory, and\nin this setting, K-ary search on an array is inef-\nﬁcient, since the parallel threads will access non-\nconsecutive memory locations. To avoid this, we\nrequire a data structure that places the Kelements\ncompared by K-ary search in consecutive memory\nlocations so that they can be copied from global to\nshared memory with a coalesced read. This data\nstructure is a B-tree (Bayer and McCreight, 1970),\nwhich is widely used in databases, ﬁlesystems and\ninformation retrieval.\nInformally, a B-tree generalizes binary trees in\nexactly the same way that K-ary search general-\nizes binary search (Figure 6). More formally, a\nB-tree is a recursive data structure that replaces\narrays A and V at each node of the trie. A B-\ntree node of size K consists of three arrays: a 1-\nindexed array Bof K−1 keys; a 1-indexed array\nV of K −1 associated values so that V[j] is the\nvalue associated with key B[j]; and, if the node is\nnot a leaf, a 0-indexed array C of K addresses to\nchild B-trees. The keys in B are sorted, and the\nsubtree at address pointed to by child C[j] repre-\nsents only key-value pairs for keys between B[j]\nand B[j + 1] when 1 ≤j < K, keys less than\nB[1] when j = 0, or keys greater thanB[K] when\nj = K.\nTo ﬁnd a key k in a B-tree, we start at the\nroot node, and we seek j such that B[j] ≤k <\nB[j+ 1]. If B[j] = kwe return V[j], otherwise if\nthe node is not a leaf node we return the result of\nrecursively querying the B-tree node at the address\nC[j] (C[0] if k < B[1] or C[K] if k > B[K]). If\nthe key is not found in array Bof a leaf, the query\nfails.\nOur complete data structure is a trie in which\neach node except the root is a B-tree (Figure 7).\nSince the root contains all possible keys, its keys\nare simply represented by an array A, which can\nbe indexed in constant time without any search.\n1947\nFigure 4: Execution of a binary search for key 15. Each row represents a time step and highlights the\nelement compared to the key. Finding key 15 requires four time steps and four comparisons.\nFigure 5: Execution of K-ary search with the same input as Figure 4, for K = 8. The ﬁrst time step\nexecutes seven comparisons in parallel, and the query is recovered in two time steps.\nFigure 6: In a B-tree, the elements compared in K-ary search are consecutive in memory. We also show\nthe layout of an individual entry.\n4 Memory layout and implementation\nEach trie node represents a unique n-gram\nwi...wi−n+1, and if a B-tree node within the\ntrie node contains key wi−n, then it must\nalso contain the associated values ˆP(wi...wi−n),\nβ(wi...wi−n), and the address of the trie node rep-\nresenting wi...wi−n (Figure 6, Figure 3). The en-\ntire language model is laid out in memory as a\nsingle byte array in which trie nodes are visited\nin breadth-ﬁrst order and the B-tree representation\nof each node is also visited in breadth-ﬁrst order\n(Figure 7).\nSince our device has a 64-bit architecture, point-\ners can address 18.1 exabytes of memory, far more\nthan available. To save space, our data struc-\nture does not store global addresses; it instead\nstores the difference in addresses between the par-\nent node and each child. Since the array is aligned\nto four bytes, these relative addresses are divided\nby four in the representation, and multiplied by\nfour at runtime to obtain the true offset. This en-\nables us to encode relative addresses of 16GB, still\nlarger than the actual device memory. We esti-\nmate that relative addresses of this size allow us\nto store a model containing around one billion n-\ngrams.2 Unlike CPU language model implementa-\ntions such as those of Heaﬁeld (2011) and Watan-\nabe et al. (2009), we do not employ further com-\npression techniques such as variable-byte encod-\ning or LOUDS, because their runtime decompres-\nsion algorithms require branching code, which our\nimplementation must avoid.\nWe optimize the node representation for coa-\nlesced reads by storing the keys of each B-tree\nconsecutively in memory, followed by the corre-\nsponding values, also stored consecutively (Figure\n6). When the data structure is traversed, only key\narrays are iteratively copied to shared memory un-\ntil a value array is needed. This design minimizes\nthe number of reads from global memory.\n4.1 Construction\nThe canonical B-tree construction algorithm (Cor-\nmen et al., 2009) produces nodes that are not\nfully saturated, which is desirable for B-trees that\n2We estimate this by observing that a model containing 423M\nn-grams takes 3.8Gb of memory, and assuming an approxi-\nmately linear scaling, though there is some variance depend-\ning on the distribution of the n-grams.\n1948\nFigure 7: Illustration of the complete data structure, showing a root trie node as an array representing\nunigrams, and nine B-trees, each representing a single trie node. The trie nodes are numbered according\nto the order in which they are laid out in memory.\nFigure 8: Layout of a single B-tree node for K = 4. Relative addresses of the four child B-tree nodes\n(array C) are followed by three keys (array B), and three values (array V), each consisting of an n-gram\nprobability, backoff, and address of the child trie node.\nsupport insertion. However, our B-trees are im-\nmutable, and unsaturated nodes of unpredictable\nsize lead to underutilization of threads, warp di-\nvergence, and deeper trees that require more iter-\nations to query. So, we use a construction algo-\nrithm inspired by Cesarini and Soda (1983) and\nRosenberg and Snyder (1981). It is implemented\non CPU, and the resulting array is copied to GPU\nmemory to perform queries.\nSince the entire set of keys and values is known\nin advance for each n-gram, our construction al-\ngorithm receives them in sorted order as the array\nA described in Section 3.1. The procedure then\nsplits this array into K consecutive subarrays of\nequal size, leaving K−1 individual keys between\neach subarray.3 These K−1 keys become the keys\nof the root B-tree. The procedure is then applied\nrecursively to each subarray. When applied to an\narray whose size is less than K, the algorithm re-\nturns a leaf node. When applied to an array whose\n3Since the size of the array may not be exactly divisible by\nK, some subarrays may differ in length by one.\nsize is greater than or equal toKbut less than 2K,\nit splits the array into a node containing the ﬁrst\nK−1 keys, and a single leaf node containing the\nremaining keys, which becomes a child of the ﬁrst.\n4.2 Batch queries\nTo fully saturate our GPU we execute many\nqueries simultaneously. A grid receives the com-\nplete set of N-gram queries and each block pro-\ncesses a single query by performing a sequence of\nK-ary searches on B-tree nodes.\n5 Experiments\nWe compared our open-source GPU language\nmodel gLM with the CPU language model\nKenLM (Heaﬁeld, 2011). 45 KenLM can use two\nquite different language model data structures:\na fast probing hash table, and a more compact\nbut slower trie, which inspired our own language\nmodel design. Except where noted, our B-tree\n4https://github.com/XapaJIaMnu/gLM\n5https://github.com/kpu/kenlm/commit/9495443\n1949\nnode size K = 31 , and we measure throughput\nin terms of query speed, which does not include\nthe cost of initializing or copying data structures,\nor the cost of moving data to or from the GPU.\nWe performed our GPU experiments on an\nNvidia Geforce GTX, a state-of-the-art GPU, re-\nleased in the ﬁrst quarter of 2015 and costing 1000\nUSD. Our CPU experiments were performed on\ntwo different devices: one for single-threaded tests\nand one for multi-threaded tests. For the single-\nthreaded CPU tests, we used an Intel Quad Core i7\n4720HQ CPU released in the ﬁrst quarter of 2015,\ncosting 280 USD, and achieving 85% of the speed\nof a state-of-the-art consumer-grade CPU when\nsingle-threaded. For the multi-threaded CPU tests\nwe used two Intel Xeon E5-2680 CPUs, offering\na combined 16 cores and 32 threads, costing at\nthe time of their release 3,500 USD together. To-\ngether, their performance speciﬁcations are sim-\nilar to the recently released Intel Xeon E5-2698\nv3 (16 cores, 32 threads, costing 3,500USD). The\ndifferent CPU conﬁgurations are favorable to the\nCPU implementation in their tested condition: the\nconsumer-grade CPU has higher clock speeds in\nsingle-threaded mode than the professional-grade\nCPU; while the professional-grade CPUs provide\nmany more cores (though at lower clock speeds)\nwhen fully saturated. Except where noted, CPU\nthroughput is reported for the single-threaded con-\ndition.\nExcept where noted, our language model is\nthe Moses 3.0 release English 5-gram language\nmodel, containing 88 million n-grams.6 Our\nbenchmark task computes perplexity on data ex-\ntracted from the Common Crawl dataset used\nfor the 2013 Workshop on Machine Translation,\nwhich contains 74 million words across 3.2 mil-\nlion sentences.7 Both gLM and KenLM produce\nidentical perplexities, so we are certain that our\nimplementation is correct. Except where noted,\nthe faster KenLM Probing backend is used. The\nperplexity task has been used as a basic test of\nother language model implementations (Osborne\net al., 2014; Heaﬁeld et al., 2015).\n5.1 Query speed\nWhen compared to single-threaded KenLM, our\nresults (Table 3) show that gLM is just over six\n6http://www.statmt.org/moses/RELEASE-3.0/models/fr-\nen/lm/europarl.lm.1\n7http://www.statmt.org/wmt13/training-parallel-\ncommoncrawl.tgz\nLM (threads) Throughput Size (GB)\nKenLM probing (1) 10.3M 1.8\nKenLM probing (16) 49.8M 1.8\nKenLM probing (32) 120.4M 1.8\nKenLM trie (1) 4.5M 0.8\ngLM 65.5M 1.2\nTable 3: Comparison of gLM and KenLM on\nthroughput (N-gram queries per second) and data\nstructure size.\ntimes faster than the fast probing hash table, and\nnearly ﬁfteen times faster than the trie data struc-\nture, which is quite similar to our own, though\nslightly smaller due to the use of compression.\nThe raw speed of the GPU is apparent, since we\nwere able to obtain our results with a relatively\nshort engineering effort when compared to that of\nKenLM, which has been optimized over several\nyears.\nWhen we fully saturate our professional-grade\nCPU, using all sixteen cores and sixteen hyper-\nthreads, KenLM is about twice as fast as gLM.\nHowever, our CPU costs nearly four times as much\nas our GPU, so economically, this comparison fa-\nvors the GPU.\nOn ﬁrst glance, the scaling from one to six-\nteen threads is surprisingly sublinear. This is not\ndue to vastly different computational power of the\nindividual cores, which are actually very simi-\nlar. It is instead due to scheduling, cache con-\ntention, and—most importantly—the fact that our\nCPUs implement dynamic overclocking: the base\nclock rate of 2.7 GHz at full saturation increases\nto 3.5 GHz when the professional CPU is under-\nutilized, as when single-threaded; the rates for the\nconsumer-grade CPU similarly increase from 2.6\nto 3.6 GHz.8\n5.2 Effect of B-tree node size\nWhat is the optimal K for our B-tree node size?\nWe hypothesized that the optimal size would be\none that approaches the size of a coalesced mem-\nory read, which should allow us to maximize\nparallelism while minimizing global memory ac-\ncesses and B-tree depth. Since the size of a coa-\nlesced read is 128 bytes and keys are four bytes,\nwe hypothesized that the optimal node size would\nbe around K = 32 , which is also the size of\na warp. We tested this by running experiments\n8Intel calls this Intel Turbo Boost.\n1950\nFigure 9: Effect of BTree node size on throughput\n(ngram queries per second)\nthat varied K from 5 to 59, and the results (Fig-\nure 9) conﬁrmed our hypothesis. As the node size\nincreases, throughput increases until we reach a\nnode size of 33, where it steeply drops. This re-\nsult highlights the importance of designing data\nstructures that minimize global memory access\nand maximize parallelism.\nWe were curious about what effect this node\nsize had on the depth of the B-trees representing\neach trie node. Measuring this, we discovered that\nfor bigrams, 88% of the trie nodes have a depth of\none—we call these B-stumps, and they can be ex-\nhaustively searched in a single parallel operation.\nFor trigrams, 97% of trie nodes are B-stumps, and\nfor higher order n-grams the percentage exceeds\n99%.\n5.3 Saturating the GPU\nA limitation of our approach is that it is only ef-\nfective in high-throughput situations that continu-\nally saturate the GPU. In situations where a lan-\nguage model is queried only intermittently or only\nin short bursts, a GPU implementation may not\nbe useful. We wanted to understand the point\nat which this saturation occurs, so we ran ex-\nperiments varying the batch size sent to our lan-\nguage model, comparing its behavior with that\nof KenLM. To understand situations in which the\nGPU hosts the language model for query by an\nexternal GPU, we measure query speed with and\nwithout the cost of copying queries to the device.\nOur results (Figure 10) suggest that the device\nis nearly saturated once the batch size reaches a\nthousand queries, and fully saturated by ten thou-\nsand queries. Throughput remains steady as batch\nsize increases beyond this point. Even with the\ncost of copying batch queries to GPU memory,\nFigure 10: Throughput ( N-gram queries per sec-\nond) vs. batch size for gLM, KenLM probing, and\nKenLM trie.\nRegular LM Big LM\nKenLM 10.2M 8.2M\nKenLM Trie 4.5M 3.0M\ngLM 65.5M 55M\nTable 4: Throughput comparison (ngram queries\nper second) between gLM and KenLM with a 5\ntimes larger model and a regular language model.\nthroughput is more than three times higher than\nthat of single threaded KenLM. We have not in-\ncluded results of multi-threaded KenLM scaling\non Figure 10 but they are similar to the single-\nthreaded case: throughput (as shown on Table\n3) plateaus at around one hundred sentences per\nthread.\n5.4 Effect of model size\nTo understand the effect of model size on query\nspeed, we built a language model with 423 million\nn-grams, ﬁve times larger than our basic model.\nThe results (Table 4) show an 18% slowdown for\ngLM and 20% slowdown for KenLM, showing\nthat model size affects both implementations sim-\nilarly.\n5.5 Effect of N-gram order on performance\nAll experiments so far use an N-gram order of\nﬁve. We hypothesized that lowering the ngram or-\nder of the model would lead to faster query time\n(Table 5). We observe that N-gram order af-\nfects throughput of the GPU language model much\nmore than the CPU one. This is likely due to ef-\nfects of backoff queries, which are more optimized\nin KenLM. At higher orders, more backoff queries\noccur, which reduces throughput for gLM.\n1951\n5-gram 4-gram 3-gram\nKenLM 10.2M 9.8M 11.5M\nKenLM Trie 4.5M 4.5M 5.2M\ngLM 65.5M 71.9M 93.7M\nTable 5: Throughput comparison (ngram queries\nper second) achieved using lower order ngram\nmodels.\n5.6 Effect of templated code\nOur implementation initially relied on hard-coded\nvalues for parameters such as B-tree node size\nand N-gram order, which we later replaced with\nparameters. Surprisingly, we observed that this\nled to a reduction in throughput from 65.6 mil-\nlion queries per second to 59.0 million, which we\ntraced back to the use of dynamically allocated\nshared memory, as well as compiler optimizations\nthat only apply to compile-time constants. To re-\nmove this effect, we heavily templated our code,\nusing as many compile-time constants as possi-\nble, which improves throughput but enables us to\nchange parameters through recompilation.\n5.7 Bottlenecks: computation or memory?\nOn CPU, language models are typically memory-\nbound: most cycles are spent in random mem-\nory accesses, with little computation between ac-\ncesses. To see if this is true in gLM we exper-\nimented with two variants of the benchmark in\nFigure 3: one in which the GPU core was under-\nclocked, and one in which the memory was un-\nderclocked. This effectively simulates two varia-\ntions in our hardware: A GPU with slower cores\nbut identical memory, and one with slower mem-\nory, but identical processing speed. We found that\nthroughput decreases by about 10% when under-\nclocking the cores by 10%. On the other hand,\nunderclocking memory by 25% reduced through-\nput by 1%. We therefore conclude that gLM is\ncomputation-bound. We expect that gLM will\ncontinue to improve on parallel devices offering\nhigher theoretical ﬂoating point performance.\n6 Conclusion\nOur language model is implemented on a GPU,\nbut its general design (and much of the actual\ncode) is likely to be useful to other hardware\nthat supports SIMD parallelism, such as the Xeon\nPhi. Because it uses batch processing, our on-chip\nlanguage model could be integrated into a ma-\nchine translation decoder using strategies similar\nto those used to integrate an on-network language\nmodel nearly a decade ago (Brants et al., 2007).\nAn alternative method of integration would be to\nmove the decoder itself to GPU. For phrase-based\ntranslation, this would require a translation model\nand dynamic programming search algorithm on\nGPU. Translation models have been implemented\non GPU by He et al. (2015), while related search\nalgorithms for (Chong et al., 2009; Chong et al.,\n2008) and parsing (Canny et al., 2013; Hall et al.,\n2014) have been developed for GPU. We intend to\nexplore these possibilities in future work.\nAcknowledgements\nThis work was conducted within the\nscope of the Horizon 2020 Innovation Ac-\ntion Modern MT, which has received funding from\nthe European Unions Horizon 2020 research and\ninnovation programme under grant agreement No\n645487.\nWe thank Kenneth Heaﬁeld, Ulrich Germann,\nRico Sennrich, Hieu Hoang, Federico Fancellu,\nNathan Schneider, Naomi Saphra, Sorcha Gilroy,\nClara Vania and the anonymous reviewers for pro-\nductive discussion of this work and helpful com-\nments on previous drafts of the paper. Any errors\nare our own.\nReferences\nR. Bayer and E. McCreight. 1970. Organization and\nmaintenance of large ordered indices. In Proceed-\nings of the 1970 ACM SIGFIDET (Now SIGMOD)\nWorkshop on Data Description, Access and Control,\nSIGFIDET ’70, pages 107–141.\nT. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean.\n2007. Large language models in machine transla-\ntion. In In Proceedings of EMNLP-CoNLL.\nJ. Canny, D. Hall, and D. Klein. 2013. A multi-teraﬂop\nconstituency parser using GPUs. In Proceedings of\nEMNLP.\nF. Cesarini and G. Soda. 1983. An algorithm to con-\nstruct a compact B-tree in case of ordered keys. In-\nformation Processing Letters, 17(1):13–16.\nS. F. Chen and J. Goodman. 1999. An empirical\nstudy of smoothing techniques for language model-\ning. Computer Speech & Language, 13(4):359–393.\nJ. Chong, Y . Yi, A. Faria, N. R. Satish, and K. Keutzer.\n2008. Data-parallel large vocabulary continuous\n1952\nspeech recognition on graphics processors. Techni-\ncal Report UCB/EECS-2008-69, EECS Department,\nUniversity of California, Berkeley, May.\nJ. Chong, E. Gonina, Y . Yi, and K. Keutzer. 2009.\nA fully data parallel WFST-based large vocabulary\ncontinuous speech recognition on a graphics pro-\ncessing unit. In Proceedings of Interspeech.\nT. H. Cormen, C. E. Leiserson, R. L. Rivest, and\nC. Stein. 2009. Introduction to Algorithms, Third\nEdition. The MIT Press, 3rd edition.\nM. Federico, N. Bertoldi, and M. Cettolo. 2008.\nIRSTLM: an open source toolkit for handling large\nscale language models. In Proceedings of Inter-\nspeech, pages 1618–1621. ISCA.\nS. Green, D. Cer, and C. Manning. 2014. Phrasal:\nA toolkit for new directions in statistical machine\ntranslation. In Proceedings of WMT.\nD. Hall, T. Berg-Kirkpatrick, and D. Klein. 2014.\nSparser, better, faster GPU parsing. In Proceedings\nof ACL.\nH. He, J. Lin, and A. Lopez. 2015. Gappy pattern\nmatching on GPUs for on-demand extraction of hi-\nerarchical translation grammars. TACL, 3:87–100.\nK. Heaﬁeld, R. Kshirsagar, and S. Barona. 2015.\nLanguage identiﬁcation and modeling in specialized\nhardware. In Proceedings of ACL-IJCNLP, July.\nK. Heaﬁeld. 2011. KenLM: faster and smaller lan-\nguage model queries. In Proceedings of WMT ,\npages 187–197, July.\nK. Heaﬁeld. 2013. Efﬁcient Language Modeling Al-\ngorithms with Applications to Statistical Machine\nTranslation. Ph.D. thesis, Carnegie Mellon Univer-\nsity, September.\nW.-m. W. Hwu. 2011. GPU Computing Gems Emer-\nald Edition. Morgan Kaufmann Publishers Inc., San\nFrancisco, CA, USA, 1st edition.\nNvidia Corporation. 2015. Nvidia CUDA\nCompute Uniﬁed Device Architecture Pro-\ngramming Guide . Nvidia Corporation.\nhttps://docs.nvidia.com/cuda/cuda-c-programming-\nguide/.\nM. Osborne, A. Lall, and B. V . Durme. 2014. Ex-\nponential reservoir sampling for streaming language\nmodels. In Proceedings of ACL, pages 687–692.\nA. L. Rosenberg and L. Snyder. 1981. Time- and\nspace-optimality in B-trees. ACM Trans. Database\nSyst., 6(1):174–193, Mar.\nD. Talbot and M. Osborne. 2007. Smoothed Bloom ﬁl-\nter language models: Tera-scale LMs on the cheap.\nIn Proceedings of EMNLP-CoNLL, pages 468–476.\nT. Watanabe, H. Tsukada, and H. Isozaki. 2009. A\nsuccinct N-gram language model. In Proc. of ACL-\nIJCNLP.\nM. Yasuhara, T. Tanaka, J. ya Norimatsu, and M. Ya-\nmamoto. 2013. An efﬁcient language model using\ndouble-array structures. In EMNLP, pages 222–232.\n1953",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8884320855140686
    },
    {
      "name": "Bottleneck",
      "score": 0.8528498411178589
    },
    {
      "name": "Massively parallel",
      "score": 0.7518011927604675
    },
    {
      "name": "Parallel computing",
      "score": 0.7488148212432861
    },
    {
      "name": "Memory footprint",
      "score": 0.673473596572876
    },
    {
      "name": "Throughput",
      "score": 0.5624246001243591
    },
    {
      "name": "Data parallelism",
      "score": 0.4405614137649536
    },
    {
      "name": "Latency (audio)",
      "score": 0.43560469150543213
    },
    {
      "name": "Footprint",
      "score": 0.4272284507751465
    },
    {
      "name": "Computer architecture",
      "score": 0.33252060413360596
    },
    {
      "name": "Parallelism (grammar)",
      "score": 0.23359432816505432
    },
    {
      "name": "Embedded system",
      "score": 0.22613441944122314
    },
    {
      "name": "Programming language",
      "score": 0.1690208613872528
    },
    {
      "name": "Operating system",
      "score": 0.15154895186424255
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Wireless",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98677209",
      "name": "University of Edinburgh",
      "country": "GB"
    }
  ],
  "cited_by": 4
}