{
  "title": "∞-former: Infinite Memory Transformer-former: Infinite Memory Transformer",
  "url": "https://openalex.org/W4285272557",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2284266408",
      "name": "Pedro Henrique Martins",
      "affiliations": [
        "Instituto de Telecomunicações"
      ]
    },
    {
      "id": "https://openalex.org/A2494224694",
      "name": "Zita Marinho",
      "affiliations": [
        "Instituto de Telecomunicações"
      ]
    },
    {
      "id": "https://openalex.org/A2122534586",
      "name": "André Martins",
      "affiliations": [
        "Instituto de Telecomunicações"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3034955736",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W4288024261",
    "https://openalex.org/W2982756474",
    "https://openalex.org/W3035168240",
    "https://openalex.org/W1602017060",
    "https://openalex.org/W4287725215",
    "https://openalex.org/W3006983028",
    "https://openalex.org/W4288624561",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W3134891661",
    "https://openalex.org/W4288087322",
    "https://openalex.org/W1732222442",
    "https://openalex.org/W4287019748",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W3037798801",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3157700644",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3169064633",
    "https://openalex.org/W3045227173",
    "https://openalex.org/W3209206732",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W3125498921",
    "https://openalex.org/W2952556884",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W3123615524",
    "https://openalex.org/W2039179672",
    "https://openalex.org/W2905016804",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2891826200",
    "https://openalex.org/W3034513031",
    "https://openalex.org/W4299567010",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4303633609"
  ],
  "abstract": "Transformers are unable to model long-term memories effectively, since the amount of computation they need to perform grows with the context length. While variations of efficient transformers have been proposed, they all have a finite memory capacity and are forced to drop old information. In this paper, we propose the ∞-former, which extends the vanilla transformer with an unbounded long-term memory. By making use of a continuous-space attention mechanism to attend over the long-term memory, the ∞-former's attention complexity becomes independent of the context length, trading off memory length with precision.In order to control where precision is more important, ∞-former maintains \"sticky memories,\" being able to model arbitrarily long contexts while keeping the computation budget fixed.Experiments on a synthetic sorting task, language modeling, and document grounded dialogue generation demonstrate the ∞-former's ability to retain information from long sequences.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 5468 - 5485\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\n∞-former: Inﬁnite Memory Transformer\nPedro Henrique MartinsÈ Zita MarinhoÁç André F. T. MartinsÈÉÆ\nÈInstituto de Telecomunicações ÁDeepMind çInstitute of Systems and Robotics\nÉLUMLIS (Lisbon ELLIS Unit), Instituto Superior Técnico ÆUnbabel\nLisbon, Portugal\npedrohenriqueamartins@tecnico.ulisboa.pt,\nzmarinho@google.com, andre.t.martins@tecnico.ulisboa.pt.\nAbstract\nTransformers are unable to model long-term\nmemories effectively, since the amount of com-\nputation they need to perform grows with\nthe context length. While variations of efﬁ-\ncient transformers have been proposed, they\nall have a ﬁnite memory capacity and are\nforced to drop old information. In this paper,\nwe propose the ∞-former, which extends the\nvanilla transformer with an unbounded long-\nterm memory. By making use of a continuous-\nspace attention mechanism to attend over the\nlong-term memory, the ∞-former’s attention\ncomplexity becomes independent of the con-\ntext length, trading off memory length with\nprecision. In order to control where pre-\ncision is more important, ∞-former main-\ntains “sticky memories,” being able to model\narbitrarily long contexts while keeping the\ncomputation budget ﬁxed. Experiments on\na synthetic sorting task, language modeling,\nand document grounded dialogue generation\ndemonstrate the ∞-former’s ability to retain\ninformation from long sequences.1\n1 Introduction\nWhen reading or writing a document, it is impor-\ntant to keep in memory the information previously\nread or written. Humans have a remarkable ability\nto remember long-term context, keeping in mem-\nory the relevant details (Carroll, 2007; Kuhbandner,\n2020). Recently, transformer-based language mod-\nels have achieved impressive results by increasing\nthe context size (Radford et al., 2018, 2019; Dai\net al., 2019; Rae et al., 2019; Brown et al., 2020).\nHowever, while humans process information se-\nquentially, updating their memories continuously,\nand recurrent neural networks (RNNs) update a\nsingle memory vector during time, transformers do\nnot – they exhaustively query every representation\nassociated to the past events. Thus, the amount\n1The code is available at https://github.com/\ndeep-spin/infinite-former.\nof computation they need to perform grows with\nthe length of the context, and, consequently, trans-\nformers have computational limitations about how\nmuch information can ﬁt into memory. For exam-\nple, a vanilla transformer requires quadratic time to\nprocess an input sequence and linear time to attend\nto the context when generating every new word.\nSeveral variations have been proposed to address\nthis problem (Tay et al., 2020b). Some propose\nusing sparse attention mechanisms, either with\ndata-dependent patterns (Kitaev et al., 2020; Vyas\net al., 2020; Tay et al., 2020a; Roy et al., 2021;\nWang et al., 2021) or data-independent patterns\n(Child et al., 2019; Beltagy et al., 2020; Zaheer\net al., 2020), reducing the self-attention complexity\n(Katharopoulos et al., 2020; Choromanski et al.,\n2021; Peng et al., 2021; Jaegle et al., 2021), and\ncaching past representations in a memory (Dai\net al., 2019; Rae et al., 2019). These models are\nable to reduce the attention complexity, and, conse-\nquently, to scale up to longer contexts. However, as\ntheir complexity still depends on the context length,\nthey cannot deal with unbounded context.\nIn this paper, we propose the∞-former (inﬁnite\nformer; Fig. 1): a transformer model extended with\nan unbounded long-term memory (LTM), which\nallows the model to attend to arbitrarily long con-\ntexts. The key for making the LTM unbounded\nis a continuous-space attention framework (Mar-\ntins et al., 2020) which trades off the number\nof information units that ﬁt into memory (basis\nfunctions) with the granularity of their represen-\ntations. In this framework, the input sequence is\nrepresented as a continuous signal, expressed as\na linear combination of N radial basis functions\n(RBFs). By doing this, the ∞-former’s attention\ncomplexity is O(L2 + L×N) while the vanilla\ntransformer’s isO(L×(L+LLTM)), where Land\nLLTM correspond to the transformer input size and\nthe long-term memory length, respectively (details\nin §3.1.1). Thus, this representation comes with\n5468\ntwo signiﬁcant advantages: (i) the context can be\nrepresented using a number of basis functions N\nsmaller than the number of tokens, reducing the\nattention computational cost; and (ii) N can be\nﬁxed, making it possible to represent unbounded\ncontext in memory, as described in §3.2 and Fig. 2,\nwithout increasing its attention complexity. The\nprice, of course, is a loss in resolution: using a\nsmaller number of basis functions leads to lower\nprecision when representing the input sequence as\na continuous signal, as shown in Fig. 3.\nTo mitigate the problem of losing resolution, we\nintroduce the concept of “sticky memories” (§3.3),\nin which we attribute a larger space in the LTM’s\nsignal to regions of the memory more frequently\naccessed. This creates a notion of “permanence” in\nthe LTM, allowing the model to better capture long\ncontexts without losing the relevant information,\nwhich takes inspiration from long-term potentiation\nand plasticity in the brain (Mills et al., 2014; Bamji,\n2005).\nTo sum up, our contributions are the following:\n• We propose the ∞-former, in which we ex-\ntend the transformer model with a continuous\nlong-term memory (§3.1). Since the attention\ncomputational complexity is independent of\nthe context length, the ∞-former is able to\nmodel long contexts.\n• We propose a procedure that allows the model\nto keep unbounded context in memory (§3.2).\n• We introduce sticky memories, a procedure\nthat enforces the persistence of important in-\nformation in the LTM (§3.3).\n• We perform empirical comparisons in a syn-\nthetic task (§4.1), which considers increas-\ningly long sequences, in language modeling\n(§4.2), and in document grounded dialogue\ngeneration (§4.3). These experiments show\nthe beneﬁts of using an unbounded memory.\n2 Background\n2.1 Transformer\nA transformer (Vaswani et al., 2017) is composed\nof several layers, which encompass a multi-head\nself-attention layer followed by a feed-forward\nlayer, along with residual connections (He et al.,\n2016) and layer normalization (Ba et al., 2016).\nLet us denote the input sequence as\nX = [x1,...,x L] ∈RL×e, where L is the\ninput size and e is the embedding size of the\nattention layer. The queries Q, keys K, and values\nV, to be used in the multi-head self-attention\ncomputation are obtained by linearly projecting\nthe input, or the output of the previous layer, X,\nfor each attention head h:\nQh = XhWQh, Kh = XhWKh,Vh = XhWVh,\n(1)\nwhere WQh,WKh,WVh ∈Rd×d are learnable\nprojection matrices, d = e/H, and H is the num-\nber of heads. Then, the context representation\nZh ∈RL×d, that corresponds to each attention\nhead h, is obtained as:\nZh = softmax\n(QhK⊤\nh√\nd\n)\nVh, (2)\nwhere the softmax is performed row-wise. The\nhead context representations are concatenated to\nobtain the ﬁnal context representation Z ∈RL×e:\nZ = [Z1,...,Z H]WR, (3)\nwhere WR ∈Re×e is another projection matrix\nthat aggregates all head’s representations.\n2.2 Continuous Attention\nContinuous attention mechanisms (Martins et al.,\n2020) have been proposed to handle arbitrary con-\ntinuous signals, where the attention probability\nmass function over words is replaced by a probabil-\nity density over a signal. This allows time intervals\nor compact segments to be selected.\nTo perform continuous attention, the ﬁrst step\nis to transform the discrete text sequence rep-\nresented by X ∈RL×e into a continuous signal.\nThis is done by expressing it as a linear combina-\ntion of basis functions. To do so, each xi, with\ni∈{1,...,L }, is ﬁrst associated with a position\nin an interval, ti ∈[0,1], e.g., by setting ti = i/L.\nThen, we obtain a continuous-space representation\n¯X(t) ∈Re, for any t∈[0,1] as:\n¯X(t) = B⊤ψ(t), (4)\nwhere ψ(t) ∈RN is a vector of N RBFs, e.g.,\nψj(t) = N(t; µj,σj), with µj ∈ [0,1], and\nB ∈RN×e is a coefﬁcient matrix. B is obtained\nwith multivariate ridge regression (Brown et al.,\n1980) so that ¯X(ti) ≈xi for each i ∈[L], which\nleads to the closed form (see App. A for details):\nB⊤= X⊤F⊤(FF⊤+ λI)−1 = X⊤G, (5)\n5469\nwhere F = [ψ(t1),...,ψ (tL)] ∈RN×L packs the\nbasis vectors for the Llocations. As G∈RL×N\nonly depends of F, it can be computed ofﬂine.\nHaving converted the input sequence into a con-\ntinuous signal ¯X(t), the second step is to attend\nover this signal. To do so, instead of having a\ndiscrete probability distribution over the input se-\nquence as in standard attention mechanisms (like\nin Eq. 2), we have a probability density p, which\ncan be a Gaussian N(t; µ,σ2), where µ and σ2\nare computed by a neural component. A unimodal\nGaussian distribution encourages each attention\nhead to attend to a single region, as opposed to\nscattering its attention through many places, and\nenables tractable computation. Finally, having p,\nwe can compute the context vector cas:\nc= Ep\n[¯X(t)\n]\n. (6)\nMartins et al. (2020) introduced the continuous\nattention framework for RNNs. In the following\nsection (§3.1), we will explain how it can be used\nfor transformer multi-head attention.\n3 Inﬁnite Memory Transformer\nTo allow the model to access long-range context,\nwe propose extending the vanilla transformer with\na continuous LTM, which stores the input embed-\ndings and hidden states of the previous steps. We\nalso consider the possibility of having two mem-\nories: the LTM and a short-term memory (STM),\nwhich consists in an extension of the transformer’s\nhidden states and is attended to by the transformer’s\nself-attention, as in the transformer-XL (Dai et al.,\n2019). A diagram of the model is shown in Fig. 1.\n3.1 Long-term Memory\nFor simplicity, let us ﬁrst assume that the long-\nterm memory contains an explicit input discrete se-\nquence X that consists of the past text sequence’s\ninput embeddings or hidden states,2 depending on\nthe layer3 (we will later extend this idea to an un-\nbounded memory in §3.2).\nFirst, we need to transform X into a continuous\napproximation ¯X(t). We compute ¯X(t) as:\n¯X(t) = B⊤ψ(t), (7)\nwhere ψ(t) ∈RN are basis functions and coef-\nﬁcients B ∈RN×e are computed as in Eq. 5,\n2We stop the gradient with respect to the word embeddings\nor hidden states before storing them in the LTM.\n3Each layer of the transformer has a different LTM.\nMasked Self-attention\nSTM \n+ \nFigure 1: ∞-former’s attention diagram with sequence\nof text, Xt, of size L = 2 and STM of size LSTM =\n2. Circles represent input embeddings or hidden states\n(depending on the layer) for head hand query i. Both\nthe self-attention and the attention over the LTM are\nperformed in parallel for each head and query.\nB⊤ = X⊤G. Then, we can compute the LTM\nkeys, Kh ∈RN×d, and values, Vh ∈RN×d, for\neach attention head h, as:\nKh = BhWKh, V h = BhWVh, (8)\nwhere WKh,WVh ∈Rd×d are learnable projection\nmatrices.4 For each query qh,i for i∈{1,...,L },\nwe use a parameterized network, which takes as\ninput the attention scores, to compute µh,i ∈]0,1[\nand σ2\nh,i ∈R>0:\nµh,i =sigmoid\n(\naﬃne\n(Kh qh,i√\nd\n))\n(9)\nσ2\nh,i =softplus\n(\naﬃne\n(Kh qh,i√\nd\n))\n. (10)\nThen, using the continuous softmax transforma-\ntion (Martins et al., 2020), we obtain the probability\ndensity ph,i as N(t; µh,i,σ2\nh,i).\nFinally, having the value function ¯Vh(t) given\nas ¯Vh(t) = V⊤\nh ψ(t),we compute the head-speciﬁc\nrepresentation vectors as in Eq. 6:\nzh,i = Eph,i[ ¯Vh] = V⊤\nh Eph,i[ψ(t)] (11)\nwhich form the rows of matrix ZLTM,h ∈RL×d\nthat goes through an afﬁne transformation,\nZLTM = [ZLTM,1,...,Z LTM,H]WO.\nThe long-term representation, ZLTM, is then\nsummed to the transformer context vector, ZT , to\nobtain the ﬁnal context representation Z ∈RL×e:\nZ = ZT + ZLTM, (12)\nwhich will be the input to the feed-forward layer.\n4Parameter weights are not shared between layers.\n5470\ncontraction\n+ \nfunction\nevaluation\nconcatenation regression \nFigure 2: Diagram of the unbounded memory update procedure. This is performed in parallel for each embedding\ndimension, and repeated throughout the input sequence. We propose two alternatives to select the positions used\nfor the function evaluation: linearly spaced or sticky memories.\n3.1.1 Attention Complexity\nAs the ∞-former makes use of a continuous-\nspace attention framework (Martins et al., 2020)\nto attend over the LTM signal, its key matrix\nsize Kh ∈RN×d depends only on the number\nof basis functions N, but not on the length\nof the context being attended to. Thus, the\n∞-former’sattention complexity is also indepen-\ndent of the context’s length. It corresponds to\nO(L×(L+ LSTM) + L×N) when also using\na short-term memory and O(L2 + L×N) when\nonly using the LTM, both≪O(L×(L+ LLTM)),\nwhich would be the complexity of a vanilla trans-\nformer attending to the same context. For this rea-\nson, the ∞-former can attend to arbitrarily long\ncontexts without increasing the amount of compu-\ntation needed.\n3.2 Unbounded Memory\nWhen representing the memory as a discrete se-\nquence, to extend it, we need to store the new hid-\nden states in memory. In a vanilla transformer, this\nis not feasible for long contexts due to the high\nmemory requirements. However, the ∞-former\ncan attend to unbounded context without increasing\nmemory requirements by using continuous atten-\ntion, as next described and shown in Fig. 2.\nTo be able to build an unbounded representation,\nwe ﬁrst sample M locations in [0,1] and evaluate\n¯X(t) at those locations. These locations can simply\nbe linearly spaced, or sampled according to the\nregion importance, as described in §3.3.\nThen, we concatenate the corresponding vectors\nwith the new vectors coming from the short-term\nmemory. For that, we ﬁrst need to contract this\nfunction by a factor of τ ∈]0,1[ to make room for\nthe new vectors. We do this by deﬁning:\nXcontracted(t) = X(t/τ) = B⊤ψ(t/τ). (13)\nThen, we can evaluate ¯X(t) at the M locations\n0 ≤t1,t2,...,t M ≤τ as:\nxm = B⊤ψ(tm/τ), for m∈[M], (14)\nand deﬁne a matrix Xpast = [x1,x2,...,x M ]⊤∈\nRM×e with these vectors as rows. After that, we\nconcatenate this matrix with the new vectors Xnew,\nobtaining:\nX =\n[\nXpast⊤,Xnew⊤\n]⊤\n∈R(M+L)×e. (15)\nFinally, we simply need to perform multivari-\nate ridge regression to compute the new coefﬁ-\ncient matrix B ∈RN×e, via B⊤= X⊤G, as in\nEq. 5. To do this, we need to associate the vec-\ntors in Xpast with positions in [0,τ] and in Xnew\nwith positions in ]τ,1] so that we obtain the matrix\nG∈R(M+L)×N . We consider the vectors posi-\ntions to be linearly spaced.\n3.3 Sticky Memories\nWhen extending the LTM, we evaluate its current\nsignal at M locations in [0,1], as shown in Eq. 14.\nThese locations can be linearly spaced. However,\nsome regions of the signal can be more relevant\nthan others, and should consequently be given a\nlarger “memory space” in the next step LTM’s sig-\nnal. To take this into account, we propose sampling\nthe M locations according to the signal’s relevance\nat each region (see Fig. 6 in App. B). To do so,\nwe construct a histogram based on the attention\ngiven to each interval of the signal on the previ-\nous step. For that, we ﬁrst divide the signal into\n5471\nD linearly spaced bins {d1,...,d D}. Then, we\ncompute the probability given to each bin, p(dj)\nfor j ∈{1,...,D }, as:\np(dj) ∝\nH∑\nh=1\nL∑\ni=1\n∫\ndj\nN(t; µh,i,σ2\nh,i) dt, (16)\nwhere H is the number of attention heads and L\nis the sequence length. Note that Eq. 16’s integral\ncan be evaluated efﬁciently using the erf function:\n∫ b\na\nN(t; µ,σ2) = 1\n2\n(\nerf\n( b√\n2\n)\n−erf\n( a√\n2\n))\n.\n(17)\nThen, we sample the M locations at which the\nLTM’s signal is evaluated at, according to p. By\ndoing so, we evaluate the LTM’s signal at the re-\ngions which were considered more relevant by the\nprevious step’s attention, and will, consequently\nattribute a larger space of the new LTM’s signal to\nthe memories stored in those regions.\n3.4 Implementation and Learning Details\nDiscrete sequences can be highly irregular and,\nconsequently, difﬁcult to convert into a continuous\nsignal through regression. Because of this, before\napplying multivariate ridge regression to convert\nthe discrete sequence X into a continuous signal,\nwe use a simple convolutional layer (with stride =\n1 and width = 3) as a gate, to smooth the sequence:\n˜X = sigmoid (CNN(X)) ⊙X. (18)\nTo train the model we use the cross entropy loss.\nHaving a sequence of text X of length Las input,\na language model outputs a probability distribution\nof the next word p(xt+1 |xt,...,x t−L). Given a\ncorpus of T tokens, we train the model to minimize\nits negative log likelihood:\nLNLL = −\nT−1∑\nt=0\nlog p(xt+1 |xt,...,x t−L). (19)\nAdditionally, in order to avoid having uniform\ndistributions over the LTM, we regularize the con-\ntinuous attention given to the LTM, by minimizing\nthe Kullback-Leibler (KL) divergence, DKL, be-\ntween the attention probability density, N(µh,σh),\nand a Gaussian prior, N(µ0,σ0). As different\nheads can attend to different regions, we set µ0 =\nµh, regularizing only the attention variance, and\nget:\nLKL =\nT−1∑\nt=0\nH∑\nh=1\nDKL (N(µh,σh) ||N(µh,σ0))\n(20)\n=\nT−1∑\nt=0\nH∑\nh=1\n1\n2\n(σ2\nh\nσ2\n0\n−log\n(σh\nσ0\n)\n−1\n)\n. (21)\nThus, the ﬁnal loss that is minimized corre-\nsponds to:\nL= LNLL + λKLLKL, (22)\nwhere λKL is a hyperparameter that controls the\namount of KL regularization.\n4 Experiments\nTo understand if the ∞-former is able to model\nlong contexts, we ﬁrst performed experiments on a\nsynthetic task, which consists of sorting tokens by\ntheir frequencies in a long sequence (§4.1). Then,\nwe performed experiments on language modeling\n(§4.2) and document grounded dialogue genera-\ntion (§4.3) by ﬁne-tuning a pre-trained language\nmodel.5\n4.1 Sorting\nIn this task, the input consists of a sequence of\ntokens sampled according to a token probability\ndistribution (which is not known to the system).\nThe goal is to generate the tokens in the decreasing\norder of their frequencies in the sequence. One\nexample can be:\n1 2 1 3 1 0 3 1 3 2  \n1 occurs 4 times; 3 occurs 3 times, etc.\n<SEP > 1 3 2 0\nTo understand if the long-term memory is being\neffectively used and the transformer is not only\nperforming sorting by modeling the most recent\ntokens, we design the token probability distribution\nto change over time: namely, we set it as a mixture\nof two distributions, p= αp0 + (1 −α)p1, where\nthe mixture coefﬁcient α∈[0,1] is progressively\nincreased from 0 to 1 as the sequence is generated.\nThe vocabulary has 20 tokens and we experiment\nwith sequences of length 4,000, 8,000, and 16,000.\n5See App.D for further experiments on language modeling.\n5472\n4000 8000 16000\nSequence length\n30\n40\n50\n60\n70\n80\n90\n100Acc(%)\nTransformer-XL\nCompressive\n-former\n0 500 1000 1500 2000\nNumber of basis functions\n70.0\n72.5\n75.0\n77.5\n80.0\n82.5\n85.0\n87.5\n90.0Acc (%)\nAccuracy\nRegression error\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\n0.225\n0.250\nRegression error\nFigure 3: Left: Sorting task accuracy for sequences of length 4,000, 8,000, and 16,000. Right: Sorting task\naccuracy vs regression mean error, when varying the number of basis functions, for sequences of length 8,000.\nBaselines. We consider the transformer-XL 6\n(Dai et al., 2019) and the compressive transformer7\n(Rae et al., 2019) as baselines. The transformer-XL\nconsists of a vanilla transformer (Vaswani et al.,\n2017) extended with a short-term memory which is\ncomposed of the hidden states of the previous steps.\nThe compressive transformer is an extension of the\ntransformer-XL: besides the short-term memory, it\nhas a compressive long-term memory which is com-\nposed of the old vectors of the short-term memory,\ncompressed using a CNN. Both the transformer-XL\nand the compressive transformer require relative\npositional encodings. In contrast, there is no need\nfor positional encodings in the memory in our ap-\nproach since the memory vectors represent basis\ncoefﬁcients in a predeﬁned continuous space.\nFor all models we used a transformer with 3 lay-\ners and 6 attention heads, input size L = 1 ,024\nand memory size 2,048. For the compressive trans-\nformer, both memories have size 1,024. For the\n∞-former, we also consider a STM of size 1,024\nand a LTM with N = 1,024 basis functions, hav-\ning the models the same computational cost. Fur-\nther details are described in App. C.1.\nResults. As can be seen in the left plot of Fig. 3,\nthe transformer-XL achieves a slightly higher\naccuracy than the compressive transformer and\n∞-former for a short sequence length (4,000). This\nis because the transformer-XL is able to keep al-\nmost the entire sequence in memory. However,\nits accuracy degrades rapidly when the sequence\nlength is increased. Both the compressive trans-\n6We use the authors’ implementation available athttps:\n//github.com/kimiyoung/transformer-xl.\n7We use our implementation of the model.\nformer and ∞-former also lead to smaller accura-\ncies when increasing the sequence length, as ex-\npected. However, this decrease is not so signiﬁcant\nfor the ∞-former, which indicates that it is better\nat modeling long sequences.\nRegression error analysis. To better understand\nthe trade-off between the ∞-former’s memory pre-\ncision and its computational efﬁciency, we ana-\nlyze how its regression error and sorting accuracy\nvary when varying the number of basis functions\nused, on the sorting task with input sequences of\nlength 8,000. As can be seen in the right plot of\nFig. 3, the sorting accuracy is negatively correlated\nwith the regression error, which is positively cor-\nrelated with the number of basis functions. It can\nalso be observed, that when increasing substantially\nthe number of basis functions the regression error\nreaches a plateau and the accuracy starts to drop.\nWe posit that the latter is caused by the model hav-\ning a harder task at selecting the locations it should\nattend to. This shows that, as expected, when in-\ncreasing ∞-former’s efﬁciency or increasing the\nsize of context being modeled, the memory loses\nprecision.\n4.2 Language Modeling\nTo understand if long-term memories can be used to\nextend a pre-trained language model, we ﬁne-tune\nGPT-2 small (Radford et al., 2019) on Wikitext-\n103 (Merity et al., 2017) and a subset of PG-19\n(Rae et al., 2019) containing the ﬁrst 2,000 books\n(≈200 million tokens) of the training set. To do\nso, we extend GPT-2 with a continuous long-term\nmemory (∞-former) and a compressed memory\n(compressive transformer) with a positional bias,\n5473\nWikitext-103 PG19\nGPT-2 16.85 33.44\nCompressive 16.87 33.09\n∞-former 16.64 32.61\n∞-former (SM) 16.61 32.48\nTable 1: Perplexity on Wikitext-103 and PG19.\nbased on Press et al. (2021).8\nFor these experiments, we consider transform-\ners with input size L = 512, for the compressive\ntransformer we use a compressed memory of size\n512, and for the∞-former we consider a LTM with\nN = 512 Gaussian RBFs and a memory threshold\nof 2,048 tokens, having the same computational\nbudget for the two models. Further details and\nhyperparameters are described in App. C.2.\nResults. The results reported in Table 1 show that\nthe ∞-former leads to perplexity improvements on\nboth Wikitext-103 and PG19, while the compres-\nsive transformer only has a slight improvement\non the latter. The improvements obtained by the\n∞-former are larger on the PG19 dataset, which\ncan be justiﬁed by the nature of the datasets: books\nhave more long range dependencies than Wikipedia\narticles (Rae et al., 2019).\n4.3 Document Grounded Dialogue\nIn document grounded dialogue generation, besides\nthe dialogue history, models have access to a doc-\nument concerning the conversation’s topic. In the\nCMU Document Grounded Conversation dataset\n(CMU-DoG) (Zhou et al., 2018), the dialogues are\nabout movies and a summary of the movie is given\nas the auxiliary document; the auxiliary document\nis divided into parts that should be considered for\nthe different utterances of the dialogue. In this\npaper, to evaluate the usefulness of the long-term\nmemories, we make this task slightly more chal-\nlenging by only giving the models access to the\ndocument before the start of the dialogue.\nWe ﬁne-tune GPT-2 small (Radford et al., 2019)\nusing an approach based on Wolf et al. (2019).\nTo allow the model to keep the whole document\non memory, we extend GPT-2 with a continuous\nLTM (∞-former) with N = 512 basis functions.\nAs baselines, we use GPT-2, with and without ac-\n8The compressive transformer requires relative positional\nencodings. When using only GPT-2’s absolute positional en-\ncodings the model gives too much attention to the compressed\nmemory, and, consequently, diverges. Thus, we adapted it by\nusing positional biases on the attention mechanism.\nPPL F1 Rouge-1 Rouge-L Meteor\nGPT-2 w/o doc 19.43 7.82 12.18 10.17 6.10\nGPT-2 18.53 8.64 14.61 12.03 7.15\nCompressive 18.02 8.78 14.74 12.14 7.29\n∞-former 18.02 8.92 15.28 12.51 7.52\n∞-former (SM) 18.04 9.01 15.37 12.56 7.55\nTable 2: Results on CMU-DoG dataset.\ncess (GPT-2 w/o doc) to the auxiliary document,\nwith input size L = 512, and GPT-2 with a com-\npressed memory with attention positional biases\n(compressive), of size 512. Further details and\nhyper-parameters are stated in App. C.3.\nTo evaluate the models we use the metrics: per-\nplexity, F1 score, Rouge-1 and Rouge-L (Lin,\n2004), and Meteor (Banerjee and Lavie, 2005).\nResults. As shown in Table 2, by keeping\nthe whole auxiliary document in memory, the\n∞-former and the compressive transformer are\nable to generate better utterances, according to\nall metrics. While the compressive and ∞-former\nachieve essentially the same perplexity in this task,\nthe ∞-former achieves consistently better scores\non all other metrics. Also, using sticky memo-\nries leads to slightly better results on those metrics,\nwhich suggests that attributing a larger space in the\nLTM to the most relevant tokens can be beneﬁcial.\nAnalysis. In Fig. 4, we show examples of ut-\nterances generated by ∞-former along with the\nexcerpts from the LTM that receive higher atten-\ntion throughout the utterances’ generation. In these\nexamples, we can clearly see that these excerpts\nare highly pertinent to the answers being generated.\nAlso, in Fig. 5, we can see that the phrases which\nare attributed larger spaces in the LTM, when using\nsticky memories, are relevant to the conversations.\n5 Related Work\nContinuous attention. Martins et al. (2020) in-\ntroduced 1D and 2D continuous attention, using\nGaussians and truncated parabolas as densities.\nThey applied it to RNN-based document classi-\nﬁcation, machine translation, and visual question\nanswering. Several other works have also proposed\nthe use of (discretized) Gaussian attention for natu-\nral language processing tasks: Guo et al. (2019)\nproposed a Gaussian prior to the self-attention\nmechanism to bias the model to give higher atten-\ntion to nearby words, and applied it to natural lan-\nguage inference; You et al. (2020) proposed the use\n5474\nCast: Macaulay Culkin as Kevin. Joe Pesci as\nHarry. Daniel Stern as Marv. John Heard as Peter.\nRoberts Blossom as Marley. \n   ...\nThe film stars Macaulay Culkin as Kevin\nMcCallister, a boy who is mistakenly left behind\nwhen his family flies to Paris for their Christmas\nvacation. Kevin initially relishes being home alone,\nbut soon has to contend with two would-be\nburglars played by Joe Pesci and Daniel Stern.\nThe film also features Catherine O'Hara and John\nHeard as Kevin's parents. \nPrevious utterance: Or maybe rent, anything is\nreason to celebrate..I would like to talk about a\nmovie called \"Home Alone\" \nAnswer: Macaulay Culkin is the main actor and it\nis a comedy.\nPrevious utterance: That sounds like a great\nmovie. Any more details?\nAnswer: The screenplay came out in 1990 and\nhas been on the air for quite a while. \nMovie Name: Home Alone. Rating: Rotten\nTomatoes: 62% and average: 5.5/10, Metacritic\nScore: 63/100, CinemaScore: A. Year: 1990. The\nMcCallister family is preparing to spend Christmas\nin Paris, gathering at Peter and Kate's home\noutside of Chicago on the night before their\ndeparture. Peter and Kate's youngest son, eight-\nyear-old Kevin, is being ridiculed by his siblings\nand cousins. A fight with his older brother, Buzz,\nresults in Kevin getting sent to the third floor of the\nhouse for punishment, where he wishes that his\n     ...\nFigure 4: Examples of answers generated by ∞-former on a dialogue about the movie “Home Alone”. The\nexcerpts from the LTM that are more attended to throughout the utterances generation are highlighted on each\ncolor, correspondingly.\nToy Story: Tom Hanks as Woody  |  animated buddy comedy  |  Toy Story was the first feature length computer animated film  | \nproduced by Pixar  | toys pretend to be lifeless whenever humans are present  |  focuses on the relationship between Woody and Gold  | \nfashioned pull string cowboy doll\nLa La Land: Ryan Gosling  |  Emma Stone as Mia  |  Hollywood  |  the city of Los Angeles  |  Meta critics: 93/100  |  2016  |  During a gig\nat a restaurant Sebastian slips into a passionate jazz  |  despite warning from the owner  |  Mia overhears the music as she passes by  | \nfor his disobedience  \nFigure 5: Phrases that hold larger spaces of the LTM, when using sticky memories, for two dialogue examples (in\nApp. E).\nof hard-coded Gaussian attention as input-agnostic\nself-attention layer for machine translation; Dubois\net al. (2020) proposed using Gaussian attention as a\nlocation attention mechanism to improve the model\ngeneralization to longer sequences. However, these\napproaches still consider discrete sequences and\ncompute the attention by evaluating the Gaussian\ndensity at the token positions. Farinhas et al. (2021)\nextend continuous attention to multimodal densi-\nties, i.e., mixtures of Gaussians, and applied it to\nVQA. In this paper, we opt for the simpler case,\nan unimodal Gaussian, and leave sparse and multi-\nmodal continuous attention for future work.\nEfﬁcient transformers. Several methods have\nbeen proposed that reduce the transformer’s at-\ntention complexity, and can, consequently, model\nlonger contexts. Some of these do so by perform-\ning sparse attention, either by selecting pre-deﬁned\nattention patterns (Child et al., 2019; Beltagy et al.,\n2020; Zaheer et al., 2020), or by learning these\npatterns from data (Kitaev et al., 2020; Vyas et al.,\n2020; Tay et al., 2020a; Roy et al., 2021; Wang\net al., 2021). Other works focus on directly re-\nducing the attention complexity by applying the\n(reversed) kernel trick (Katharopoulos et al., 2020;\nChoromanski et al., 2021; Peng et al., 2021; Jae-\ngle et al., 2021). Closer to our approach are the\ntransformer-XL and compressive transformer mod-\nels (Dai et al., 2019; Rae et al., 2019), which extend\nthe vanilla transformer with a bounded memory.\nMemory-augmented language models. RNNs,\nLSTMs, and GRUs (Hochreiter et al., 1997; Cho\net al., 2014) have the ability of keeping a memory\nstate of the past. However, these require backprop-\nagation through time which is impractical for long\nsequences. Because of this, Graves et al. (2014),\nWeston et al. (2014), Joulin and Mikolov (2015)\nand Grefenstette et al. (2015) proposed extending\nRNNs with an external memory, while Chandar\net al. (2016) and Rae et al. (2016) proposed efﬁ-\ncient procedures to read and write from these mem-\nories, using hierarchies and sparsity. Grave et al.\n(2016) and Merity et al. (2017) proposed the use\nof cache-based memories which store pairs of hid-\nden states and output tokens from previous steps.\nThe distribution over the words in the memory is\nthen combined with the distribution given by the\nlanguage model. More recently, Khandelwal et al.\n(2019) and Yogatama et al. (2021) proposed using\nnearest neighbors to retrieve words from a key-\nbased memory constructed based on the training\ndata. Similarly, Fan et al. (2021) proposed retriev-\ning sentences from a memory based on the training\ndata and auxiliary information. Khandelwal et al.\n(2019) proposed interpolating the retrieved words\nprobability distributions with the probability over\nthe vocabulary words when generating a new word,\nwhile Yogatama et al. (2021) and Fan et al. (2021)\nproposed combining the information at the architec-\nture level. These models have the disadvantage of\nneeding to perform a retrieval step when generating\n5475\neach token / utterance, which can be computation-\nally expensive. These approaches are orthogonal\nto the ∞-former’s LTM and in future work the two\ncan be combined.\n6 Conclusions\nIn this paper, we proposed the ∞-former: a trans-\nformer extended with an unbounded long-term\nmemory. By using a continuous-space attention\nframework, its attention complexity is independent\nof the context’s length, which allows the model\nto attend to arbitrarily long contexts while keep-\ning a ﬁxed computation budget. By updating the\nmemory taking into account past usage, the model\nkeeps “sticky memories”, enforcing the persistence\nof relevant information in memory over time. Ex-\nperiments on a sorting synthetic task show that ∞-\nformer scales up to long sequences, maintaining\na high accuracy. Experiments on language model-\ning and document grounded dialogue generation\nby ﬁne-tuning a pre-trained language model have\nshown improvements across several metrics.\nEthics Statement\nTransformer models that attend to long contexts,\nto improve their generation quality, need large\namounts of computation and memory to perform\nself-attention. In this paper, we propose an exten-\nsion to a transformer model that makes the attention\ncomplexity independent of the length of the con-\ntext being attended to. This can lead to a reduced\nnumber of parameters needed to model the same\ncontext, which can, consequently, lead to gains in\nefﬁciency and reduce energy consumption.\nOn the other hand, the ∞-former, as well as the\nother transformer language models, can be used on\nquestionable scenarios, such as the generation of\nfake news (Zellers et al., 2019), defamatory text\n(Wallace et al., 2019), or other undesired content.\nAcknowledgments\nThis work was supported by the European Research\nCouncil (ERC StG DeepSPIN 758969), by the\nP2020 project MAIA (contract 045909), by the\nFundação para a Ciência e Tecnologia through\nproject PTDC/CCI-INF/4703/2021 (PRELUNA,\ncontract UIDB/50008/2020), by the EU H2020\nSELMA project (grant agreement No 957017), and\nby contract PD/BD/150633/2020 in the scope of\nthe Doctoral Program FCT - PD/00140/2013 NET-\nSyS. We thank Jack Rae, Tom Schaul, the SAR-\nDINE team members, and the reviewers for helpful\ndiscussion and feedback.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization.\nShernaz X Bamji. 2005. Cadherins: actin with the cy-\ntoskeleton to form synapses. Neuron.\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nAn automatic metric for MT evaluation with im-\nproved correlation with human judgments. In Proc.\nWorkshop on intrinsic and extrinsic evaluation mea-\nsures for machine translation and/or summarization.\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer.\nPhilip J Brown, James V Zidek, et al. 1980. Adaptive\nmultivariate ridge regression. The Annals of Statis-\ntics.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language Models are Few-Shot\nLearners. In Proc. NeurIPS.\nD.W. Carroll. 2007. Psychology of Language . Cen-\ngage Learning.\nSarath Chandar, Sungjin Ahn, Hugo Larochelle, Pascal\nVincent, Gerald Tesauro, and Yoshua Bengio. 2016.\nHierarchical memory networks.\nRewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever. 2019. Generating long sequences with\nsparse transformers.\nKyunghyun Cho, Bart van Merriënboer, Dzmitry Bah-\ndanau, and Yoshua Bengio. 2014. On the Properties\nof Neural Machine Translation: Encoder–Decoder\nApproaches. In Proc. Workshop on Syntax, Seman-\ntics and Structure in Statistical Translation.\nKrzysztof Choromanski, Valerii Likhosherstov, David\nDohan, Xingyou Song, Andreea Gane, Tamas Sar-\nlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\nLukasz Kaiser, et al. 2021. Rethinking attention\nwith performers. In Proc. ICLR (To appear).\nZihang Dai, Zhilin Yang, Yiming Yang, William W\nCohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. 2019. Transformer-xl: Attentive lan-\nguage models beyond a ﬁxed-length context. In\nProc. ACL.\nYann Dubois, Gautier Dagan, Dieuwke Hupkes, and\nElia Bruni. 2020. Location Attention for Extrapola-\ntion to Longer Sequences. In Proc. ACL.\n5476\nAngela Fan, Claire Gardent, Chloé Braud, and An-\ntoine Bordes. 2021. Augmenting Transformers with\nKNN-Based Composite Memory for Dialog. Trans-\nactions of the Association for Computational Lin-\nguistics.\nAntónio Farinhas, André F. T. Martins, and P. Aguiar.\n2021. Multimodal Continuous Visual Attention\nMechanisms.\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2016. Improving Neural Language Models with a\nContinuous Cache. In Proc. ICLR.\nAlex Graves, Greg Wayne, and Ivo Danihelka. 2014.\nNeural turing machines.\nEdward Grefenstette, Karl Moritz Hermann, Mustafa\nSuleyman, and Phil Blunsom. 2015. Learning to\ntransduce with unbounded memory. Proc. NeurIPS.\nMaosheng Guo, Yu Zhang, and Ting Liu. 2019. Gaus-\nsian Transformer: A Lightweight Approach for Nat-\nural Language Inference. In Proc. AAAI.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proc. CVPR.\nSepp Hochreiter, J urgen Schmidhuber, and Corso\nElvezia. 1997. Long Short-Term Memory. Neural\nComputation.\nAndrew Jaegle, Felix Gimeno, Andrew Brock, Andrew\nZisserman, Oriol Vinyals, and Joao Carreira. 2021.\nPerceiver: General Perception with Iterative Atten-\ntion.\nArmand Joulin and Tomas Mikolov. 2015. Inferring\nalgorithmic patterns with stack-augmented recurrent\nnets. Proc. NeurIPS.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and François Fleuret. 2020. Transformers are\nrnns: Fast autoregressive transformers with linear at-\ntention. In Proc. ICML.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2019. Generalization\nthrough Memorization: Nearest Neighbor Language\nModels. In Proc. ICLR.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nMethod for Stochastic Optimization. In Proc. ICLR.\nNikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. In Proc.\nICLR.\nChristof Kuhbandner. 2020. Long-Lasting Verbatim\nMemory for the Words of Books After a Single\nReading Without Any Learning Intention. Frontiers\nin Psychology.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Proc. Workshop on Au-\ntomatic Summarization.\nAndré FT Martins, Marcos Treviso, António Farinhas,\nVlad Niculae, Mário AT Figueiredo, and Pedro MQ\nAguiar. 2020. Sparse and Continuous Attention\nMechanisms. In Proc. NeurIPS.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer Sentinel Mixture\nModels. In Proc. ICLR.\nFergil Mills, Thomas E Bartlett, Lasse Dissing-Olesen,\nMarta B Wisniewska, Jacek Kuznicki, Brian A\nMacvicar, Yu Tian Wang, and Shernaz X Bamji.\n2014. Cognitive ﬂexibility and long-term depres-\nsion (LTD) are impaired following β-catenin stabi-\nlization in vivo. In Proc. of the National Academy of\nSciences.\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy\nSchwartz, Noah Smith, and Lingpeng Kong. 2021.\nRandom Feature Attention. In Proc. ICLR (To ap-\npear).\nOﬁr Press, Noah A Smith, and Mike Lewis. 2021.\nTrain short, test long: Attention with linear biases\nenables input length extrapolation.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nJack W Rae, Jonathan J Hunt, Tim Harley, Ivo Dani-\nhelka, Andrew Senior, Greg Wayne, Alex Graves,\nand Timothy P Lillicrap. 2016. Scaling memory-\naugmented neural networks with sparse reads and\nwrites. In Proc. NeurIPS.\nJack W Rae, Anna Potapenko, Siddhant M Jayakumar,\nChloe Hillier, and Timothy P Lillicrap. 2019. Com-\npressive Transformers for Long-Range Sequence\nModelling. In Proc. ICLR.\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and\nDavid Grangier. 2021. Efﬁcient content-based\nsparse attention with routing transformers. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:53–68.\nYi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-\nCheng Juan. 2020a. Sparse sinkhorn attention. In\nProc. ICML.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald\nMetzler. 2020b. Efﬁcient transformers: A survey.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proc. NeurIPS.\nApoorv Vyas, Angelos Katharopoulos, and François\nFleuret. 2020. Fast transformers with clustered at-\ntention. In Proc. NeurIPS.\n5477\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gard-\nner, and Sameer Singh. 2019. Universal Adversarial\nTriggers for Attacking and Analyzing NLP. In Proc.\nEMNLP-IJCNLP.\nShuohang Wang, Luowei Zhou, Zhe Gan, Yen-Chun\nChen, Yuwei Fang, Siqi Sun, Yu Cheng, and Jingjing\nLiu. 2021. Cluster-Former: Clustering-based Sparse\nTransformer for Question Answering. In Proc. ACL\nFindings.\nJason Weston, Sumit Chopra, and Antoine Bordes.\n2014. Memory networks.\nThomas Wolf, Victor Sanh, Julien Chaumond, and\nClement Delangue. 2019. Transfertransfo: A trans-\nfer learning approach for neural network based con-\nversational agents.\nDani Yogatama, Cyprien de Masson d’Autume, and\nLingpeng Kong. 2021. Adaptive Semiparametric\nLanguage Models. Transactions of the Association\nfor Computational Linguistics, 9:362–373.\nWeiqiu You, Simeng Sun, and Mohit Iyyer. 2020.\nHard-Coded Gaussian Attention for Neural Machine\nTranslation. In Proc. ACL.\nManzil Zaheer, Guru Guruganesh, Avinava Dubey,\nJoshua Ainslie, Chris Alberti, Santiago Ontanon,\nPhilip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\net al. 2020. Big bird: Transformers for longer se-\nquences.\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. Proc. NeurIPS.\nKangyan Zhou, Shrimai Prabhumoye, and Alan W\nBlack. 2018. A Dataset for Document Grounded\nConversations. In Proc. EMNLP.\n5478\nA Multivariate ridge regression\nThe coefﬁcient matrix B ∈ RN×e is obtained\nwith multivariate ridge regression criterion so that\n¯X(ti) ≈xi for each i ∈[L], which leads to the\nclosed form:\nB⊤= arg min\nB⊤\n||B⊤F −X⊤||2\nF+ λ||B||2\nF\n(23)\n= X⊤F⊤(FF⊤+ λI)−1 = X⊤G,\nwhere F = [ψ(t1),...,ψ (tL)] packs the basis vec-\ntors for L locations and ||·|| F is the Frobenius\nnorm. As G∈RL×N only depends of F, it can be\ncomputed ofﬂine.\nB Sticky memories\nWe present in Fig. 6 a scheme of the sticky memo-\nries procedure. First we sample M locations from\nthe previous step LTM attention histogram (Eq.\n16). Then, we evaluate the LTM’s signal at the\nsampled locations (Eq. 14). Finally, we consider\nthat the sampled vectors, Xpast, are linearly spaced\nin [0,τ]. This way, the model is able to attribute\nlarger spaces of its memory to the relevant words.\nC Experimental details\nC.1 Sorting\nFor the compressive transformer, we consider com-\npression rates of size 2 for sequences of length\n4,000, from 2 to 6 for sequences of length 8,000,\nand from 2 to 12 for sequences of length 16,000.\nWe also experiment training the compressive trans-\nformer with and without the attention reconstruc-\ntion auxiliary loss. For the ∞-former we con-\nsider 1,024 Gaussian RBFs N(t; ˜µ,˜σ2) with ˜µlin-\nearly spaced in [0,1] and ˜σ ∈{.01,.05}. We set\nτ = 0.75 and for the KL regularization we used\nλKL = 1 ×10−5 and σ0 = 0.05.\nFor this task, for each sequence length, we cre-\nated a training set with 8,000 sequences and valida-\ntion and test sets with 800 sequences. We trained\nall models with batches of size 8 for 20 epochs on 1\nNvidia GeForce RTX 2080 Ti or 1 Nvidia GeForce\nGTX 1080 Ti GPU with≈11 Gb of memory, using\nthe Adam optimizer (Kingma and Ba, 2015). For\nthe sequences of length 4,000 and 8,000 we used a\nlearning rate of 2.5 ×10−4 while for sequences of\nlength 16,000 we used a learning rate of 2 ×10−4.\nThe learning rate was decayed to 0 until the end of\ntraining with a cosine schedule.\nC.2 Pre-trained Language Models\nIn these experiments, we ﬁne-tune the GPT-2 small,\nwhich is composed of 12 layers with 12 attention\nheads, on the English dataset Wikitext-103 and on\na subset of the English dataset PG199 containing\nthe ﬁrst 2,000 books. We consider an input size\nL= 512 and a long-term memory with N = 512\nGaussian RBFs N(t; ˜µ,˜σ2) with ˜µlinearly spaced\nin [0,1] and ˜σ ∈{.005,.01}and for the KL regu-\nlarization we use λKL = 1 ×10−6 and σ0 = 0.05.\nWe set τ = 0.5. For the compressive transformer\nwe also consider a compressed memory of size 512\nwith a compression rate of 4, and train the model\nwith the auxiliary reconstruction loss.\nWe ﬁne-tuned GPT-2 small with a batch size of\n1 on 1 Nvidia GeForce RTX 2080 Ti or 1 Nvidia\nGeForce GTX 1080 Ti GPU with≈11 Gb of mem-\nory, using the Adam optimizer (Kingma and Ba,\n2015) for 1 epoch with a learning rate of 5 ×10−5\nfor the GPT-2 parameters and a learning rate of\n2.5 ×10−4 for the LTM parameters.\nC.3 Document Grounded Generation\nIn these experiments, we ﬁne-tune the GPT-2\nsmall, which is composed of 12 layers with 12\nattention heads, on the English dataset CMU -\nDocument Grounded Conversations10 (CMU-DoG.\nCMU-DoG has 4112 conversations, being the pro-\nportion of train/validation/test split 0.8/0.05/0.15.\nWe consider an input size L= 512 and a long-\nterm memory with N = 512 Gaussian RBFs\nN(t; ˜µ,˜σ2) with ˜µ linearly spaced in [0,1] and\n˜σ ∈{.005,.01}and for the KL regularization we\nuse λKL = 1 ×10−6 and σ0 = 0 .05. We set\nτ = 0.5. For the compressive transformer we con-\nsider a compressed memory of size 512 with a\ncompression rate of 3, and train the model with the\nauxiliary reconstruction loss. We ﬁne-tuned GPT-2\nsmall with a batch size of 1 on 1 Nvidia GeForce\nRTX 2080 Ti or 1 Nvidia GeForce GTX 1080 Ti\nGPU with ≈11 Gb of memory, using the Adam\noptimizer (Kingma and Ba, 2015) with a linearly\ndecayed learning rate of 5 ×10−5, for 5 epochs.\nD Additional experiments\nWe also perform language modeling experiments\non the Wikitext-103 dataset11 (Merity et al., 2017)\n9Dataset available at https://github.com/deepmind/pg19.\n10Dataset available at https://github.com/festvox/datasets-\nCMU_DoG.\n11Dataset available at https://blog.einstein.ai/the-wikitext-\nlong-term-dependency-language-modeling-dataset/.\n5479\nsampling\nattention histogram\nfunction\nevaluation\nFigure 6: Sticky memories procedure diagram. The dashed vertical lines correspond to the position of the words\nin the LTM signal.\nwhich has a training set with 103 million tokens and\nvalidation and test sets with 217,646 and 245,569\ntokens, respectively. For that, we follow the stan-\ndard architecture of the transformer-XL (Dai et al.,\n2019), which consists of a transformer with 16 lay-\ners and 10 attention heads. For the transformer-XL,\nwe experiment with a memory of size 150. For\nthe compressive transformer, we consider that both\nmemories have a size of 150 and a compression\nrate of 4. For the ∞-former we consider a short-\nterm memory of size 150, a continuous long-term\nmemory with 150 Gaussian RBFs, and a memory\nthreshold of 900 tokens.\nFor this experiment, we use a transformer with\n16 layers, 10 heads, embeddings of size 410, and\na feed-forward hidden size of 2100. For the com-\npressive transformer, we follow Rae et al. (2019)\nand use a compression rate of 4 and the attention\nreconstruction auxiliary loss. For the ∞-former we\nconsider 150 Gaussian RBFs N(t; ˜µ,˜σ2) with ˜µ\nlinearly spaced in [0,1] and ˜σ ∈{.01,.05}. We\nset τ = 0.5 and for the KL regularization we used\nλKL = 1 ×10−5 and σ0 = 0.1.\nWe trained all models, from scratch, with\nbatches of size 40 for 250,000 steps on 1 Nvidia\nTitan RTX or 1 Nvidia Quadro RTX 6000 with\n≈24 GPU Gb of memory using the Adam opti-\nmizer (Kingma and Ba, 2015) with a learning rate\nof 2.5 ×10−4. The learning rate was decayed to 0\nuntil the end of training with a cosine schedule.\nSTM LTM Perplexity\nTransformer-XL 150 —- 24.52\nCompressive 150 150 24.41\n∞-former 150 150 24.29\n∞-former (Sticky memories) 150 150 24.22\nTable 3: Perplexity on Wikitext-103.\nResults. As can be seen in Table 3, extending the\nmodel with a long-term memory leads to a better\nperplexity, for both the compressive transformer\nand ∞-former. Moreover, the ∞-former slightly\noutperforms the compressive transformer. We can\nalso see that using sticky memories leads to a some-\nwhat lower perplexity, which shows that it helps\nthe model to focus on the relevant memories.\nAnalysis. To better understand whether ∞-\nformer is paying more attention to the older mem-\nories in the LTM or to the most recent ones, we\nplotted a histogram of the attention given to each\nregion of the long-term memory when predicting\nthe tokens on the validation set. As can be seen in\nFig. 7, in the ﬁrst and middle layers, the ∞-former\ntends to focus more on the older memories, while\nin the last layer, the attention pattern is more uni-\nform. In Figs. 8 and 9, we present examples of\nwords for which the∞-former has lower perplexity\nthan the transformer-XL along with the attention\ngiven by the ∞-former to the last layer’s LTM. We\ncan see that the word being predicted is present sev-\n5480\neral times in the long-term memory and ∞-former\ngives higher attention to those regions.\nTo know whether the sticky memories approach\nattributes a larger space of the LTM’s signal to\nrelevant phrases or words, we plotted the memory\nspace given to each word 12 present in the long-\nterm memory of the last layer when using and not\nusing sticky memories. We present examples in\nFigs. 10 and 11 along with the phrases / words\nwhich receive the largest spaces when using sticky\nmemories. We can see in these examples that this\nprocedure does in fact attribute large spaces to old\nmemories, creating memories that stick over time.\nWe can also see that these memories appear to be\nrelevant as shown by the words / phrases in the\nexamples.\nE Additional examples\nIn Fig. 12, we show additional examples of utter-\nances generated by ∞-former along with the ex-\ncerpts from the LTM that receive higher attention\nthroughout the utterances’ generation.\nAdditionally, ground truth conversations con-\ncerning the movies “Toy Story” and “La La Land”,\nfor which the sticky memories are stated in Fig. 5,\nare shown in Tables 4 and 5, respectively.\n12The (V oronoi) memory space attributed to each word is\nhalf the distance from the previous word plus half the distance\nto the next word in the LTM’s signal, being the word’s location\ncomputed based on the sampled positions from which we\nevaluate the signal when receiving new memory vectors.\n5481\n0.0 0.2 0.4 0.6 0.8 1.0\nLTM signal (first layer)\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10p\n0.0 0.2 0.4 0.6 0.8 1.0\nLTM signal (mid layer)\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10p\n0.0 0.2 0.4 0.6 0.8 1.0\nLTM signal (last layer)\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10p\nFigure 7: Histograms of attention given to the LTM by∞-former, for the ﬁrst (on the left), middle (on the middle),\nand last (on the right) layers. The dashed vertical lines represent the limits of the memory segments ( τ) for the\nvarious memory updates.\nthe Pet Shop Boys' synthpop cover of the song (titled\n\"Where the Streets Have No Name (I Can't Take My Eyes\noff You) \"). Bono parodied this by occasionally adopting the\ndeadpan vocal style used in the Pet Shop Boys' cover.\nCritics welcomed the song in the group's setlist: The\nIndependent said the song \"induces instant euphoria, as U2\ndo what they're best at, slipping into epic rock mode,\nplaying music made for the arena\". In two other local\nnewspaper reviews, critics praised the song's inclusion in a\nsequence of greatest hits.\nFor the PopMart Tour of 1997–1998, U2 returned to the\nelectronic dance arrangement they occasionally played on\nthe Zoo TV Tour. The set's massive video screen displayed\na video that\nHot Press described as an \"astonishing 2001-style trip into\nthe heart of a swirling, psychedelic tunnel that sucks the\naudience in towards a horizontal monolith\". Near the end of\nthe song, peace doves were shown on the screen and\nbright beams of light flanking the set's golden arch were\nprojected upwards. Hot Press said the effect transformed\nthe stadium into a \"UFO landing site\". Shortly before the\nthird leg of the Elevation Tour, the September 11 attacks\noccurred in New York City and Washington D.C.. During the\nband's first show in New York City following the attacks, the\nband performed \"Where the Streets Have No Name\", and\nwhen the stage lights illuminated the audience, the band\nsaw tears streaming down the faces of many fans. The\nexperience was\none inspiration for the song \"City of Blinding Lights\". The\nband paid tribute to the 9/11 victims during their\nperformance of the song at the Super Bowl XXXVI halftime\nshow on 3 February 2002. The performance featured the\nnames of the September 11 victims projected onto a large\nwhite banner behind the band. U2's appearance was later\nranked number 1 on Sports Illustrated's list of \"Top 10\nSuper Bowl Halftime Shows\". For the Vertigo Tour, the\ngroup originally considered dropping the song from their\nsetlists, but Mullen and Clayton successfully argued against\nthis. All 131 of the Vertigo Tour concerts featured a\nperformance of the song, which were accompanied by the\nstage's LED video curtains displaying African flags. On the\ntour's opening night, this reminded Bono that he had\nGT : as the respective audio releases of the latter two concerts, Zoo TV Live and Hasta la Vista Baby! U2\nFigure 8: Example of attention given by ∞-former to the last layer’s long-term memory, when predicting the\nground truth word “U2”. The words in the LTM which receive higher attention (>0.05) are shaded.\nand fixed defences, Australia may be made practically\ninvulnerable\". According to Air Force historian Alan\nStephens this paper \"in effect, defined the anti-lodgment\nconcept which has been a persistent feature of RAAF\nstrategic thinking\". \nHeadlam, completed a flying instructors course in July 1936\nand joined the staff of No. 1 FTS. He was promoted to flight\nlieutenant on 1 March 1937. Commencing in July 1938, he\nwas one of six students to take part in the RAAF's first Long\nSpecialist Navigation Course, run by Flight Lieutenants Bill\nGaring and Alister Murdoch at Point Cook. The course\ninvolved several epic training flights that attracted\nconsiderable media attention, including a twelve-day,\n10,800-kilometre (6,700 mi) round-Australia trip by\nthree Avro Ansons, one of which was piloted by Headlam,\nin November. The following month, Headlam led the three\nAnsons on a six-day journey back and forth over Central\nAustralia. He subsequently passed the navigation course\nwith a special distinction. On 27 January 1939 he was\nposted to RAAF Station Laverton, Victoria, as a flight\ncommander. He served initially with No. 2 Squadron, before\ntransferring to No. 1 Squadron on 29 August. Both units\noperated Ansons.\nWorld War II\nFollowing the outbreak of World War II, No. 1 Squadron\nwas engaged in convoy escort and maritime\nreconnaissance duties off south-eastern Australia.\nHeadlam continued to serve with the squadron as a flight\ncommander until 15 January 1940, when he was assigned\nto Headquarters Laverton\nas the station navigation officer. On 27 March he was\nposted to the staff of RAAF Headquarters, Melbourne. He\nwas promoted to squadron leader on 1 June 1940. Two\nweeks later he married Katherine Bridge at St Paul's\nAnglican Church in Frankston; the couple would have a son\nand a daughter. Headlam was given command of No. 2\nSquadron at Laverton on 15 April 1941, and raised to wing\ncommander on 1 July. Equipped with Lockheed Hudsons,\nthe squadron mainly conducted maritime patrols in\nsouthern waters until 5 December, when four of its aircraft\nwere ordered to Darwin,  Northern Territory, in response to\nfears of Japanese aggression in the Pacific. On 7\nDecember, this detachment established itself at Penfui,\nnear Koepang in Dutch Timor, while No. 2 Squadron's eight\nremaining Hudsons\nGT: for the first time on 26 January 1942, and attacked regularly thereafter, damaging some aircraft. The intact Hudsons were withdrawn to Darwin but Headlam\nFigure 9: Example of attention given by ∞-former to the last layer’s long-term memory, when predicting the\nground truth word “Headlam”. The words in the long-term memory which receive higher attention (bigger than\n0.05) are shaded.\n5482\nPhrases / words:\n\"transport gasoline\"  |  \"American Civil Rigths\"  |  \"along with Michael\"  |  \"community center\"  |  \"residents began to move\" |  \"Landmarks\nComission\"  |  \"Meridian Main\"  |  \"projects\"  |  \"the historic train station\"  |  \"Weidmann's Restaurant\"  |  \"Arts\"  |  \"Meridian Main Street\" \n|  \"in late 2007\"  |  \"effort\"  |  \"Alliance serves\"  |  \"Plans were underway\"  |  \"Building\"  |  \"Mayor Cheri\"  |  \"the Alliance\"  |  \"promote\nfurther development\"  |  \"assist businesses\"  |      \"Street program\"\nFigure 10: Example of the memory space attributed to each word in the last layer’s long-term memory (after 5\nupdates) without / with the sticky memories procedure, along with the words / phrases which have the largest mem-\nory spaces when using sticky memories (top peaks with space>.005). Excerpt of the sequence being generated in\nthis example: “Given Meridian’s site as a railroad junction, its travelers have attracted the development of many\nhotels. ”The dashed vertical lines represent the limits of the memory segments for the various memory updates.\nPhrases / words:\n\"July 1936\"  |  \"Headlam continued to serve\"  | \"27 March\"  |  \"in Frankston\"  |  \"daugther\"  |  \"four of its aircraft\"  |  \"in response to fears of\nJapanese\"  |  \"stationed at Darwin\"  |  \"attacked the Japanese\"  |  \"forced it aground\"  |  \"dispersed at Penfui\"  |  \"three Japanese\nfloatplanes\"  |  \"attacked regularly\"  |  \"withdrawn to Darwin\"  |  \"his staff remained at Penfui\"  |  \"ordered to evacuate\"  |  \"assistance from\nSparrow Force\"  |  \"Four of No. 2 Squadron's Hudsons were destroyed\"  |  \"relocated to Daly Waters\" \nFigure 11: Example of the memory space attributed to each word in the last layer’s long-term memory (after\n5 updates) without / with the sticky memories procedure, along with the words / phrases which have the largest\nmemory spaces when using sticky memories (top peaks with space>.005) Excerpt of the sequence being generated\nin this example: “Headlam became Ofﬁcer Commanding North-Western Area in January 1946. Posted to Britain\nat the end of the year, he attended the Royal Air Force Staff College, Andover, and served with RAAF Overseas\nHeadquarters, London. ” The dashed vertical lines represent the limits of the memory segments for the various\nmemory updates.\n5483\nCast: Jesse Eisenberg as Mark Zuckerberg. Andrew\nGarfield as Eduardo Saverin. Justin Timberlake as\nSean Parker. Armie Hammer as Cameron and Tyler\nWinklevoss. Max Minghella as Divya Narendra. \nCritical Response: David Fincher's film has the rare\nquality of being not only as smart as its brilliant hero,\nbut in the same way. It is cocksure, impatient, cold,\nexciting and instinctively perceptive. The Social\nNetwork is the movie of the year\n   ...\nPrevious utterance: So, what movie are we going to\nchat about today? Right, the one about Zuckerberg? \nAnswer: Yep, Jesse Eisenberg plays Zuckerberg.\nPrevious utterance: So, have you seen it?\nAnswer: Yeah. Its about the founder of Facebook,\nMark Zuckerberg who was basically dumped by his\ngirlfriend, Erica, so he created \"TheFacebook.\"\nIn October 2003, 19-year-old Harvard University\nstudent Mark Zuckerberg is dumped by his girlfriend\nErica Albright. Returning to his dorm, Zuckerberg\nwrites an insulting entry about Albright on his\nLiveJournal blog and then creates a campus website\ncalled Facemash by hacking into college databases to\nsteal photos of female students, then allowing site\nvisitors to rate their attractiveness. After traffic to the\nsite crashes parts of Harvard's computer network, \n   ...\nFigure 12: Examples of answers generated by ∞-former on a dialogue about the movie “The Social Network”.\nThe excerpts from the LTM that are more attended to throughout their generation are highlighted on each color\ncorrespondingly.\n- Hi\n- Yo you really need to watch Toy Story. It has 100% on Rotten Tomatoes!\n- Really! 100% that’s pretty good What’s it about\n- It’s an animated buddy-comedy where toys come to life\n- who stars in it\n- The main characters are voiced by Tom Hanks and Tim Allen\n- does it have any other critic ratings\n- Yep, metacritic gave it 95/100 and Cinemascore gave it an A\n- how old is it?\n- It’s a 1995 ﬁlm so 23 years\n- The old ones are always good :) I heard there were some sad parts in it is that true\n- Yeah actually, the movie starts off pretty sad as the toys fear that they might be replaced and that they have to move\n- is this a disney or dreamworks movie\n- Disney, pixar to be exact\n- Why do the toys think they will be replaced :(\n- they thought so because Andy was having a birthday party and might get new toys\n- What part does Tom Hanks play\n- Woody, the main character\n- How about Tim Allen\n- Buzz, the main antagonist at ﬁrst then he becomes a friend\n- What kind of toy is Woody?\n- A cowboy doll\n- What is Buzz\n- A space ranger\n- so do all the toys talk\n- yep! but andy doesn’t know that\n- Is andy a little kid or a teen\n- He’s 6!\n- Sounds good. Thanks for the info. Have a great day\nTable 4: Ground truth conversation about movie “Toy Story”.\n5484\n- hey\n- hey\n- i just watched la la land. It is a movie from 2016 starring ryan gosling and emma stone. they are too artists (one actress and one\npanist) and they fall in love and try to achieve their dreams. its a great movie\n- It’s a wonderful movie and got a score of 92% on rotten tomatoes\n- yes, i think it also won an oscar\n- Yes but I thought it was a little dull\n- metacritics rating is 93/100 as well its pretty critically acclaimed\n- the two leads singing and dancing weren’t exceptional\n- i suppose it is not for everyone\n- It also sags badly in the middle I like how Sebastian slipped into a passionate jazz despite warnings from the owner.\n- what do you think of the cover of \"i ran so far away?\" in the movie, sebastian found the song an insult for a serious musician\n- I don’t know, it is considered an insult for serious musicians not sure why\n- yeah\n- The idea of a one woman play was daring\n- it was interesting how sebastian joined a jazz fusion band he couldnt ﬁnd real happiness in any of the bands he was in its hard\n- It is considering she didn’t know of any of that until she attended one of his concerts\n- yeah, that is daring the movie kind of speaks to a lot of people. she accussed him of abandoning his dreams but sometimes thats\nwhat you have to do.\n- Not nice that she leaves because he told her she liked him better when he was unsuccessful The play was a disaster so he didn’t\nmiss anything when he missed it.\n- yeah, but i dont blame her for dumping him for that\n- She should didn’t want to support him and she had to move back\n- id be pretty upset as well to boulder city nevada\n- yes she didn’t want to forgive him, I didn’t understand that\n- well because that was a big deal to her and he missed it\n- if she was with him when he was unsuccessful, she could have supported him to follow his dreams or other dreams\n- i suppose that is true\n- she wasn’t successful either\n- yeah she wasnt nobody showed up to her play\n- so why the big hulabaloo about him\n- not sure\n- she was selﬁsh I guess He missed her play because he had to go for a photo shoot with the band that he had previously missed\n- yeah but he should have kept better track and scheduled it better\n- this shows that he was trying to commit some and follow his dreams although not necessarily like them so she would be please\nif he didn’t attend the photo shoot a second time, and came to her show\n- i deﬁnitely felt bad for both of them though in that scene\n- it’s more of a do or don’t he is still condemned I feel bad for him because he tried he tried to get her back by apologizing as\nwell she didn’t want any of it\n- yeah because she felt like he didnt care enough because he missed it he’s the one that suggested the one woman play\n- They could have started all over again just like the beginning\n- maybe so\n- did she fail because of the one-woman play? she could have tried something else if she felt that\n- she wanted to give it a shot\n- she did and it failed, he did and it failed they just had to compromise so they could be together again, which was how the\nhappiness was He signed up for the band after hearing her talking to her mom about how he is working\n- on his career I think he did a lot for her\nTable 5: Ground truth conversation about movie “La La Land”.\n5485",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7122305631637573
    },
    {
      "name": "Computer science",
      "score": 0.6834198832511902
    },
    {
      "name": "Computation",
      "score": 0.584918737411499
    },
    {
      "name": "Memory model",
      "score": 0.4393930733203888
    },
    {
      "name": "Theoretical computer science",
      "score": 0.38799262046813965
    },
    {
      "name": "Algorithm",
      "score": 0.37279772758483887
    },
    {
      "name": "Arithmetic",
      "score": 0.3437049984931946
    },
    {
      "name": "Parallel computing",
      "score": 0.21847635507583618
    },
    {
      "name": "Mathematics",
      "score": 0.17792034149169922
    },
    {
      "name": "Electrical engineering",
      "score": 0.11064887046813965
    },
    {
      "name": "Engineering",
      "score": 0.09290540218353271
    },
    {
      "name": "Voltage",
      "score": 0.0907253623008728
    },
    {
      "name": "Shared memory",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4387152517",
      "name": "Instituto Superior Técnico",
      "country": null
    },
    {
      "id": "https://openalex.org/I4210120471",
      "name": "Instituto de Telecomunicações",
      "country": "PT"
    }
  ],
  "cited_by": 14
}