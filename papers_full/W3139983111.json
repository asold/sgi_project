{
  "title": "COTR: Correspondence Transformer for Matching Across Images",
  "url": "https://openalex.org/W3139983111",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2013552087",
      "name": "JIANG WEI",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4284682960",
      "name": "Trulls, Eduard",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4284682959",
      "name": "Hosang, Jan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2877411301",
      "name": "Tagliasacchi, Andrea",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223282411",
      "name": "Yi, Kwang Moo",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2118877769",
    "https://openalex.org/W2951870616",
    "https://openalex.org/W3034395814",
    "https://openalex.org/W2964123261",
    "https://openalex.org/W3041380886",
    "https://openalex.org/W3043075211",
    "https://openalex.org/W3104213423",
    "https://openalex.org/W3092233714",
    "https://openalex.org/W1491719799",
    "https://openalex.org/W2962705366",
    "https://openalex.org/W2987672160",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2740578684",
    "https://openalex.org/W2751023760",
    "https://openalex.org/W3035477606",
    "https://openalex.org/W2286655030",
    "https://openalex.org/W2085261163",
    "https://openalex.org/W2963876278",
    "https://openalex.org/W2963157250",
    "https://openalex.org/W2964156315",
    "https://openalex.org/W3043813766",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963627347",
    "https://openalex.org/W2131747574",
    "https://openalex.org/W2307770531",
    "https://openalex.org/W2967756832",
    "https://openalex.org/W3110520755",
    "https://openalex.org/W2741885505",
    "https://openalex.org/W3035563186",
    "https://openalex.org/W3034275286",
    "https://openalex.org/W3035559192",
    "https://openalex.org/W2320444803",
    "https://openalex.org/W2020399841",
    "https://openalex.org/W2981799316",
    "https://openalex.org/W3109908659",
    "https://openalex.org/W2090518410",
    "https://openalex.org/W2897093986",
    "https://openalex.org/W2949825757",
    "https://openalex.org/W2099927035",
    "https://openalex.org/W3034843944",
    "https://openalex.org/W2963782415",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3104100999",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2963760790",
    "https://openalex.org/W603908379",
    "https://openalex.org/W2963537932",
    "https://openalex.org/W2963748588",
    "https://openalex.org/W2981352086",
    "https://openalex.org/W2561074213",
    "https://openalex.org/W2104853049",
    "https://openalex.org/W2979458572",
    "https://openalex.org/W2033959528",
    "https://openalex.org/W2973665503",
    "https://openalex.org/W2471962767",
    "https://openalex.org/W2770902190",
    "https://openalex.org/W2897097528",
    "https://openalex.org/W2963674285",
    "https://openalex.org/W2155302366",
    "https://openalex.org/W2579352318",
    "https://openalex.org/W3103187163",
    "https://openalex.org/W1532362218",
    "https://openalex.org/W2990655570",
    "https://openalex.org/W3140551255",
    "https://openalex.org/W764651262",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2151103935",
    "https://openalex.org/W2033819227",
    "https://openalex.org/W153084048",
    "https://openalex.org/W2115579991",
    "https://openalex.org/W2964073646"
  ],
  "abstract": "We propose a novel framework for finding correspondences in images based on a deep neural network that, given two images and a query point in one of them, finds its correspondence in the other. By doing so, one has the option to query only the points of interest and retrieve sparse correspondences, or to query all points in an image and obtain dense mappings. Importantly, in order to capture both local and global priors, and to let our model relate between image regions using the most relevant among said priors, we realize our network using a transformer. At inference time, we apply our correspondence network by recursively zooming in around the estimates, yielding a multiscale pipeline able to provide highly-accurate correspondences. Our method significantly outperforms the state of the art on both sparse and dense correspondence problems on multiple datasets and tasks, ranging from wide-baseline stereo to optical flow, without any retraining for a specific dataset. We commit to releasing data, code, and all the tools necessary to train from scratch and ensure reproducibility.",
  "full_text": "COTR: Correspondence Transformer for Matching Across Images\nWei Jiang1, Eduard Trulls 2, Jan Hosang 2, Andrea Tagliasacchi 2,3, Kwang Moo Yi 1\n1University of British Columbia, 2Google Research, 3University of Toronto\nAbstract\nWe propose a novel framework for ﬁnding corresponden-\nces in images based on a deep neural network that, given\ntwo images and a query point in one of them, ﬁnds its cor-\nrespondence in the other. By doing so, one has the option\nto query only the points of interest and retrieve sparse cor-\nrespondences, or to query all points in an image and obtain\ndense mappings. Importantly, in order to capture both local\nand global priors, and to let our model relate between image\nregions using the most relevant among said priors, we real-\nize our network using a transformer. At inference time, we\napply our correspondence network by recursively zooming\nin around the estimates, yielding a multiscale pipeline able\nto provide highly-accurate correspondences. Our method\nsigniﬁcantly outperforms the state of the art on both sparse\nand dense correspondence problems on multiple datasets\nand tasks, ranging from wide-baseline stereo to optical ﬂow,\nwithout any retraining for a speciﬁc dataset. We commit\nto releasing data, code, and all the tools necessary to train\nfrom scratch and ensure reproducibility.\n1. Introduction\nFinding correspondences across pairs of images is a fun-\ndamental task in computer vision, with applications ranging\nfrom camera calibration [ 22, 28] to optical ﬂow [ 32, 15],\nStructure from Motion (SfM) [ 56, 28], visual localiza-\ntion [55, 53, 36], point tracking [35, 68], and human pose\nestimation [43, 20]. Traditionally, two fundamental research\ndirections exist for this problem. One is to extract sets of\nsparse keypoints from both images and match them in order\nto minimize an alignment metric [33, 55, 28]. The other is\nto interpret correspondence as a dense process, where ev-\nery pixel in the ﬁrst image maps to a pixel in the second\nimage [32, 60, 77, 72].\nThe divide between sparse and dense emerged naturally\nfrom the applications they were devised for. Sparse methods\nhave largely been used to recover a single global camera\nmotion, such as in wide-baseline stereo, using geometrical\nconstraints. They rely on local features [ 34, 74, 44, 13]\nFigure 1. The Correspondence Transformer – (a) COTR\nformulates the correspondence problem as a functional mapping\nfrom point x to point x′, conditional on two input images I and\nI′. (b) COTR is capable of sparse matching under different mo-\ntion types, including camera motion, multi-object motion, and\nobject-pose changes. (c) COTR generates a smooth correspon-\ndence map for stereo pairs: given (c.1,2) as input, (c.3) shows the\npredicted dense correspondence map (color-coded ‘x’ channel),\nand (c.4) warps (c.2) onto (c.1) with the predicted correspondences.\nand further prune the putative correspondences formed with\nthem in a separate stage with sampling-based robust match-\ners [18, 3, 12], or their learned counterparts [ 75, 7, 76, 64,\n54]. Dense methods, by contrast, usually model small tem-\nporal changes, such as optical ﬂow in video sequences, and\nrely on local smoothness [35, 24]. Exploiting context in\nthis manner allows them to ﬁnd correspondences at arbitrary\nlocations, including seemingly texture-less areas.\nIn this work, we present a solution that bridges this divide,\na novel network architecture that can express both forms of\nprior knowledge – global and local – and learn them implic-\nitly from data. To achieve this, we leverage the inductive\nbias that densely connected networks possess in representing\nsmooth functions [1, 4, 48] and use a transformer [73, 10, 14]\n1\narXiv:2103.14167v2  [cs.CV]  17 Aug 2021\nto automatically control the nature of priors and learn how to\nutilize them through its attention mechanism. For example,\nground-truth optical ﬂow typically does not change smoothly\nacross object boundaries, and simple (attention-agnostic)\ndensely connected networks would have challenges in mod-\nelling such a discontinuous correspondence map, whereas a\ntransformer would not. Moreover, transformers allow encod-\ning the relationship between different locations of the input\ndata, making them a natural ﬁt for correspondence problems.\nSpeciﬁcally, we express the problem of ﬁnding corres-\npondences between images I and I′in functional form, as\nx′= FΦ(x |I,I′), where FΦ is our neural network archi-\ntecture, parameterized by Φ, x indexes a query location in I,\nand x′indexes its corresponding location in I′; see Figure 1.\nDifferently from sparse methods, COTR can matcharbitrary\nquery points via this functional mapping, predicting only as\nmany matches as desired. Differently from dense methods,\nCOTR learns smoothness implicitly and can deal with large\ncamera motion effectively.\nOur work is the ﬁrst to apply transformers to obtain accu-\nrate correspondences. Our main technical contributions are:\n• we propose a functional correspondence architecture that\ncombines the strengths of dense and sparse methods;\n• we show how to apply our method recursively at multi-\nple scales during inference in order to compute highly-\naccurate correspondences;\n• we demonstrate that COTR achieves state-of-the-art per-\nformance in both dense and sparse correspondence prob-\nlems on multiple datasets and tasks, without retraining;\n• we substantiate our design choices and show that the trans-\nformer is key to our approach by replacing it with a simpler\nmodel, based on a Multi-Layer Perceptron (MLP).\n2. Related works\nWe review the literature on both sparse and dense match-\ning, as well as works that utilize transformers for vision.\nSparse methods. Sparse methods generally consist of three\nstages: keypoint detection, feature description, and fea-\nture matching. Seminal detectors include DoG [ 34] and\nFAST [51]. Popular patch descriptors range from hand-\ncrafted [34, 9] to learned [ 42, 66, 17] ones. Learned fea-\nture extractors became popular with the introduction of\nLIFT [74], with many follow-ups [ 13, 44, 16, 49, 5, 71].\nLocal features are designed with sparsity in mind, but have\nalso been applied densely in some cases [67, 32]. Learned\nlocal features are trained with intermediate metrics, such as\ndescriptor distance or number of matches.\nFeature matching is treated as a separate stage, where\ndescriptors are matched, followed by heuristics such as the\nratio test, and robust matchers, which are key to deal with\nhigh outlier ratios. The latter are the focus of much research,\nwhether hand-crafted, following RANSAC [ 18, 12, 3],\nconsensus- or motion-based heuristics [ 11, 31, 6, 37], or\nlearned [75, 7, 76, 64]. The current state of the art builds on\nattentional graph neural networks [54]. Note that while some\nof these theoretically allow feature extraction and matching\nto be trained end to end, this avenue remains largely unex-\nplored. We show that our method, which does not divide the\npipeline into multiple stages and is learned end-to-end, can\noutperform these sparse methods.\nDense methods. Dense methods aim to solve optical ﬂow.\nThis typically implies small displacements, such as the mo-\ntion between consecutive video frames. The classical Lucas-\nKanade method [35] solves for correspondences over local\nneighbourhoods, while Horn-Schunck [24] imposes global\nsmoothness. More modern algorithms still rely on these\nprinciples, with different algorithmic choices [59], or focus\non larger displacements [8]. Estimating dense corresponden-\nces under large baselines and drastic appearance changes\nwas not explored until methods such as DeMoN [ 72] and\nSfMLearner [77] appeared, which recovered both depth and\ncamera motion – however, their performance fell somewhat\nshort of sparse methods [ 75]. Neighbourhood Consensus\nNetworks [50] explored 4D correlations – while powerful,\nthis limits the image size they can tackle. More recently,\nDGC-Net [38] applied CNNs in a coarse-to-ﬁne approach,\ntrained on synthetic transformations, GLU-Net [ 69] com-\nbined global and local correlation layers in a feature pyramid,\nand GOCor [70] improved the feature correlation layers to\ndisambiguate repeated patterns. We show that we outper-\nform DGC-Net, GLU-Net and GOCor over multiple datasets,\nwhile retaining our ability to query individual points.\nAttention mechanisms. The attention mechanism enables\na neural network to focus on part of the input. Hard at-\ntention was pioneered by Spatial Transformers [26], which\nintroduced a powerful differentiable sampler, and was later\nimproved in [27]. Soft attention was pioneered by transform-\ners [73], which has since become the de-facto standard in\nnatural language processing – its application to vision tasks\nis still in its early stages. Recently, DETR [10] used Trans-\nformers for object detection, whereas ViT [14] applied them\nto image recognition. Our method is the ﬁrst application of\ntransformers to image correspondence problems. 1\nFunctional methods using deep learning. While the idea\nexisted already,e.g. to generate images [58], using neural net-\nworks in functional form has recently gained much traction.\nDeepSDF [45] uses deep networks as a function that returns\nthe signed distance ﬁeld value of a query point. These ideas\nwere recently extended by [21] to establish correspondences\nbetween incomplete shapes. While not directly related to im-\nage correspondence, this research has shown that functional\nmethods can achieve state-of-the-art performance.\n1A concurrent relevant work for feature-less image matching was pro-\nposed shortly after our work became public [63].\n3. Method\nWe ﬁrst formalize our problem (Section 3.1), then detail\nour architecture (Section 3.2), its recursive use at inference\ntime (Section 3.3), and our implementation (Section 3.4).\n3.1. Problem formulation\nLet x ∈[0,1]2 be the normalized coordinates of thequery\npoint in image I, for which we wish to ﬁnd the correspond-\ning point, x′∈[0,1]2, in image I′. We frame the problem of\nlearning to ﬁnd correspondences as that of ﬁnding the best\nset of parameters Φ for a parametric function FΦ\n(\nx|I,I′)\nminimizing\narg min\nΦ\nE\n(x,x′,I,I′)∼D\nLcorr + Lcycle, (1)\nLcorr =\nx′−FΦ\n(\nx |I,I′)2\n2 , (2)\nLcycle =\nx −FΦ\n(\nFΦ\n(\nx |I,I′)\n|I,I′)2\n2 , (3)\nwhere Dis the training dataset of ground correspondences,\nLcorr measures the correspondence estimation errors, and\nLcycle enforces correspondences to be cycle-consistent.\n3.2. Network architecture\nWe implement FΦ with a transformer. Our architecture,\ninspired by [10, 14], is illustrated in Figure 2. We ﬁrst crop\nand resize the input into a 256 ×256 image, and convert it\ninto a downsampled feature map size 16 ×16 ×256 with a\nshared CNN backbone, E. We then concatenate the represen-\ntations for two corresponding images side by side, forming\na feature map size 16 ×32 ×256, to which we add posi-\ntional encoding P(with N=256 channels) of the coordinate\nfunction Ω (i.e. MeshGrid(0:1,0:2) of size 16×32×2) to\nproduce a context feature map c (of size 16 ×32 ×256):\nc =\n[\nE(I),E(I′)\n]\n+ P(Ω), (4)\nwhere [·] denotes concatenation along the spatial dimension –\na subtly important detail novel to our architecture that we\ndiscuss in greater depth later on. We then feed the context\nfeature map c to a transformer encoder TE, and interpret its\nresults with a transformer decoder TD, along with the query\npoint x, encoded by P– the positional encoder used to\ngenerate Ω. We ﬁnally process the output of the transformer\ndecoder with a fully connected layerDto obtain our estimate\nfor the corresponding point, x′.\nx′= FΦ\n(\nx|I,I′)\n= D(TD(P(x) ,TE(c))) . (5)\nFor architectural details of each component please refer to\nsupplementary material.\nImportance of context concatenation . Concatenation of\nthe feature maps along the spatial dimension is critical, as\nFigure 2. The COTR architecture – We ﬁrst process each image\nwith a (shared) backbone CNN E to produce feature maps size\n16x16, which we then concatenate together, and add positional\nencodings to form our context feature map. The results are fed\ninto a transformer T , along with the query point(s) x. The output\nof the transformer is decoded by a multi-layer perceptron D into\ncorrespondence(s) x′.\nit allows the transformer encoder TEto relate between loca-\ntions within the image (self-attention), and across images\n(cross-attention). Note that, to allow the encoder to distin-\nguish between pixels in the two images, we employ a single\npositional encoding for the entire concatenated feature map;\nsee Fig. 2. We concatenate along the spatial dimension rather\nthan the channel dimension, as the latter would create arti-\nﬁcial relationships between features coming from the same\npixel locations in each image. Concatenation allows the\nfeatures in each map to be treated in a way that is similar\nto words in a sentence [ 73]. The encoder then associates\nand relates them to discover which ones to attend to given\ntheir context – which is arguably a more natural way to ﬁnd\ncorrespondences.\nLinear positional encoding. We found it critical to use a\nlinear increase in frequency for the positional encoding, as\nopposed to the commonly used log-linear strategy [73, 10],\nwhich made our optimization unstable; see supplementary\nmaterial. Hence, for a given location x = [x,y] we write\nP(x) =\n[\np1(x),p2(x),...,p N\n4\n(x)\n]\n, (6)\npk(x) =\n[\nsin(kπx⊤),cos(kπx⊤)\n]\n, (7)\nwhere N = 256 is the number of channels of the feature\nmap. Note that pk generates four values, so that the output\nof the encoder Pis size N.\nQuerying multiple points. We have introduced our frame-\nwork as a function operating on a single query point,x. How-\never, as shown in Fig. 2, extending it to multiple query points\nis straightforward. We can simply input multiple queries at\nonce, which the transformer decoder TDand the decoder D\nwill translate into multiple coordinates. Importantly, while\ndoing so, we disallow self attention among the query points\nin order to ensure that they are solved independently.\nFigure 3. Recursive COTR at inference time – We obtain ac-\ncurate correspondences by applying our functional approach re-\ncursively, zooming into the results of the previous iteration, and\nrunning the same network on the pair of zoomed-in crops. We grad-\nually focus on the correct correspondence, with greater accuracy.\n3.3. Inference\nWe next discuss how to apply our functional approach at\ninference time in order to obtain accurate correspondences.\nInference with recursive with zoom-in. Applying the pow-\nerful transformer attention mechanism to vision problems\ncomes at a cost – it requires heavily downsampled feature\nmaps, which in our case naturally translates to poorly local-\nized correspondences; see Section 4.6. We address this by\nexploiting the functional nature of our approach, applying\nout network FΦ recursively. As shown in Fig. 3, we itera-\ntively zoom into a previously estimated correspondence, on\nboth images, in order to obtain a reﬁned estimate. There\nis a trade-off between compute and the number of zoom-in\nsteps. We ablated this carefully on the validation data and\nsettled on a zoom-in factor of two at each step, with four\nzoom-in steps. It is worth noting that multiscale reﬁnement\nis common in many computer vision algorithms [ 32, 15],\nbut thanks to our functional correspondence model, realizing\nsuch a multiscale inference process is not only possible, but\nalso straightforward to implement.\nCompensating for scale differences. While matching im-\nages recursively, one must account for a potential mismatch\nin scale between images. We achieve this by making the\nscale of the patch to crop proportional to the commonly vis-\nible regions in each image, which we compute on the ﬁrst\nstep, using the whole images. To extract this region, we\ncompute the cycle consistency error at the coarsest level,\nfor every pixel, and threshold it at τvisible=5 pixels on the\n256 ×256 image; see Fig. 4. In subsequent stages – the\nzoom-ins – we simply adjust the crop sizes over I and I′\nso that their relationship is proportional to the sum of valid\npixels (the unmasked pixels in Fig. 4).\nDealing with images of arbitrary size . Our network ex-\npects images of ﬁxed 256×256 shape. To process images of\narbitrary size, in the initial step we simply resize (i.e. stretch)\nthem to 256 ×256, and estimate the initial correspondences.\nIn subsequent zoom-ins, we crop square patches from the\noriginal image around the estimated points, of a size com-\nmensurate with the current zoom level, and resize them to\nFigure 4. Estimating scale by ﬁnding co-visible regions – We\nshow two images we wish to put in correspondence, and the es-\ntimated regions in common – image locations with a high cycle-\nconsistency error are masked out.\n256 ×256. While this may seem a limitation on images with\nnon-standard aspect ratios, our approach performs well on\nKITTI, which are extremely wide (3.3:1). Moreover, we\npresent a strategy to tile detections in Section 4.4.\nDiscarding erroneous correspondences. What should we\ndo when we query a point is occluded or outside the viewport\nin the other image? Similarly to our strategy to compensate\nfor scale, we resolve this problem by simply rejecting corres-\npondences that induce a cycle consistency error (3) greater\nthan τcycle=5 pixels. Another heuristic we apply is to termi-\nnate correspondences that do not converge while zooming\nin. We compute the standard deviation of the zoom-in es-\ntimates, and reject correspondences that oscillate by more\nthan τstd=0.02 of the long-edge of the image.\nInterpolating for dense correspondence. While we could\nquery every single point in order to obtain dense esti-\nmates, it is also possible to densify matches by computing\nsparse matches ﬁrst, and then interpolating using barycentric\nweights on a Delaunay triangulation of the queries. This\ninterpolation can be done efﬁciently using a GPU rasterizer.\n3.4. Implementation details\nDatasets. We train our method on the MegaDepth\ndataset [30], which provides both images and corresponding\ndense depth maps, generated by SfM [ 56]. These images\ncome from photo-tourism and show large variations in ap-\npearance and viewpoint, which is required to learn invariant\nmodels. The accuracy of the depth maps is sufﬁcient to learn\naccurate local features, as demonstrated by [16, 54, 71]. To\nﬁnd co-visible pairs of images we can train with, we ﬁrst\nﬁlter out those with no common 3D points in the SfM model.\nWe then compute the common area between the remaining\npairs of images, by projecting pixels from one image to the\nother. Finally, we compute the intersection over union of the\nprojected pixels, which accounts for different image sizes.\nWe keep, for each image, the 20 image pairs with the largest\noverlap. This simple procedure results in a good combina-\ntion of images with a mixture of high/low overlap. We use\n115 scenes for training and 1 scene for validation.\nImplementation. We implement our method in Py-\nTorch [46]. For the backbone Ewe use a ResNet50 [ 23],\ninitialized with weights pre-trained on ImageNet [52]. We\nuse the feature map after its fourth downsampling step (after\nthe third residual block), which is of size 16 ×16 ×1024,\nwhich we convert into16×16×256 with 1×1 convolutions.\nFor the transformer, we use 6 layers for both encoder and de-\ncoder. Each encoder layer contains a self-attention layer with\n8 heads, and each decoder layer contains an encoder-decoder\nattention layer with 8 heads, but with no self-attention lay-\ners, in order to prevent query points from communicating\nbetween each other. Finally, for the network that converts\nthe Transformer output into coordinates, D, we use a 3-layer\nMLP, with 256 units each, followed by ReLU activations.\nOn-the-ﬂy training data generation . We select training\npairs randomly, pick a random query point in the ﬁrst image,\nand ﬁnd its corresponding point on the second image using\nthe ground truth depth maps. We then select a random zoom\nlevel among one of ten levels, uniformly spaced, in log scale,\nbetween 1×and 10×. We then crop a square patch at the\ndesired zoom level, centered at the query point, from the ﬁrst\nimage, and a square patch that contains the corresponding\npoint in the second image. Given this pair of crops, we\nsample 100 random valid correspondences across the two\ncrops – if we cannot gather at least 100 valid points, we\ndiscard the pair and move to the next.\nStaged training. Our model is trained in three stages. First,\nwe freeze the pre-trained backbone E, and train the rest\nof the network, for 300k iterations, with the ADAM opti-\nmizer [29], a learning rate of 10−4, and a batch size of 24.\nWe then unfreeze the backbone and ﬁne-tune everything end-\nto-end with a learning rate of 10−5 and a batch size of 16,\nto accommodate the increased memory requirements, for\n2M iterations, at which point the validation loss plateaus.\nNote that in the ﬁrst two stages we use the whole images,\nresized to 256 ×256, as input, which allows us to load the\nentire dataset into memory. In the third stage we introduce\nzoom-ins, generated as explained above, and train everything\nend-to-end for a further 300k iterations.\n4. Results\nWe evaluate our method with four different datasets, each\naimed for a different type of correspondence task. We do not\nperform any kind of re-training or ﬁne-tuning. They are:\n• HPatches [2]: A dataset with planar surfaces viewed under\ndifferent angles/illumination settings, and ground-truth\nhomographies. We use this dataset to compare against\ndense methods that operate on the entire image.\n• KITTI [19]: A dataset for autonomous driving, where\nthe ground-truth 3D information is collected via LIDAR.\nWith this dataset we compare against dense methods on\ncomplex scenes with camera and multi-object motion.\n• ETH3D [57]: A dataset containing indoor and outdoor\nscenes captured using a hand-held camera, registered with\nSfM. As it contains video sequences, we use it to evaluate\nhow methods perform as the baseline widens by increasing\nthe interval between samples, following [69].\nMethod AEPE ↓ PCK-1px↑ PCK-3px↑ PCK-5px↑\nLiteFlowNet [25]CVPR’18 118.85 13.91 – 31.64\nPWC-Net [61, 62]CVPR’18, TPAMI’1996.14 13.14 – 37.14\nDGC-Net [38]W ACV’19 33.26 12.00 – 58.06\nGLU-Net [69]CVPR’20 25.05 39.55 71.52 78.54\nGLU-Net+GOCor [70]NeurIPS’20 20.16 41.55 – 81.43\nCOTR 7.75 40.91 82.37 91.10\nCOTR+Interp. 7.98 33.08 77.09 86.33\nTable 1. Quantitative results on HPatches – We report Average\nEnd Point Error (AEPE) and Percent of Correct Keypoints (PCK)\nwith different thresholds. For PCK-1px and PCK-5px, we use\nthe numbers reported in literature. We bold the best method and\nunderline the second best.\n• Image Matching Challenge (IMC2020) [ 28]: A dataset\nand challenge containing wide-baseline stereo pairs from\nphoto-tourism images, similar to those we use for train-\ning (on MegaDepth). It takes matches as input and mea-\nsures the quality the poses estimated using said matches.\nWe evaluate our method on the test set and compare against\nthe state of the art in sparse methods.\n4.1. HPatches\nWe follow the evaluation protocol of [ 69, 70], which\ncomputes the Average End Point Error (AEPE) for all valid\npixels, and the Percentage of Correct Keypoints (PCK) at a\ngiven reprojection error threshold – we use 1, 3, and 5 pixels.\nImage pairs are generated taking the ﬁrst (out of six) images\nfor each scene as reference, which is matched against the\nother ﬁve. We provide two results for our method: ‘COTR’,\nwhich uses 1,000 random query points for each image pair,\nand ‘COTR + Interp.’, which interpolates correspondences\nfor the remaining pixels using the strategy presented in Sec-\ntion 3.3. We report our results in Table 1.\nOur method provides the best results, with and without\ninterpolation, with the exception of PCK-1px, where it re-\nmains close to the best baseline. We note that the results for\nthis threshold should be taken with a grain of salt, as several\nscenes do not satisfy the planar assumption for all pixels.\nTo provide some evidence for this, we reproduce the results\nfor GLU-Net [69] using the code provided by the authors to\nmeasure PCK at 3 pixels, which was not computed in the\npaper. 2 COTR outperforms it by a signiﬁcant margin.\n4.2. KITTI\nTo evaluate our method in an environment more complex\nthan simple planar scenes, we use the KITTI dataset [39, 40].\nFollowing [70, 65], we use the training split for this evalua-\ntion, as ground-truth for the test split remains private – all\nmethods, including ours, were trained on a separate dataset.\nWe report results both in terms of AEPE, and ‘Fl.’ – the\n2While GLU-Net+GOCor slightly edges out GLU-Net, code was not\navailable at the time of submission.\nInput (shown: one image) GLU-Net [69]CVPR’20 COTR(ours) GLU-Net [69] CVPR’20 COTR(ours)Optical ﬂow Optical ﬂow Error map Error map\nFigure 5. Qualitative examples on KITTI – We show the optical ﬂow and its corresponding error map (“jet” color scheme) for three\nexamples from KITTI-2015, with GLU-Net [69] as a baseline. COTR successfully recovers both the global motion in the scene, and the\nmovement of individual objects, even when nearby cars move in opposite directions (top) or partially occlude each other (bottom).\nMethod KITTI-2012 KITTI-2015\nAEPE↓ Fl.[%]↓ AEPE↓ Fl.[%]↓\nLiteFlowNet [25]CVPR’18 4.00 17.47 10.39 28.50\nPWC-Net [61, 62]CVPR’18, TPAMI’19 4.14 20.28 10.35 33.67\nDGC-Net [38]W ACV’19 8.50 32.28 14.97 50.98\nGLU-Net [69]CVPR’20 3.34 18.93 9.79 37.52\nRAFT [65]ECCV’20 2.15 9.30 5.04 17.8\nGLU-Net+GOCor [70]NeurIPS’20 2.68 15.43 6.68 27.57\nCOTR3 1.28 7.36 2.62 9.92\nCOTR+Interp.3 2.26 10.50 6.12 16.90\nTable 2. Quantitative results on KITTI – We report the Average\nEnd Point Error (AEPE) and the ﬂow outlier ratio (‘Fl’) on the 2012\nand 2015 versions of the KITTI dataset. Our method outperforms\nmost baselines, with the interpolated version being on par with\nRAFT, and slightly edging out GLU-Net+GOCor.\npercentage of optical ﬂow outliers. As KITTI images are\nlarge, we randomly sample 40,000 points per image pair,\nfrom the regions covered by valid ground truth.\nWe report the results on both KITTI-2012 and KITTI-\n2015 in Table 2. Our method outperforms all the baselines\nby a large margin. Note that the interpolated version also per-\nforms similarly to the state of the art, slightly better in terms\nof ﬂow accuracy, and slightly worse in terms of AEPE, com-\npared to RAFT [65]. It is important to understand here that,\nwhile COTR provides a drastic improvement over compared\nmethods, we are evaluating only on points where COTR re-\nturns conﬁdent results, which is about 81.8% of the queried\nlocations – among the 18.2% of rejected queries, 67.8% fall\nout of the borders of the other image, which indicates that\nour ﬁltering is reasonable. This shows that COTR provides\nhighly accurate results in the points we query and retrieve\nestimates for, and is currently limited by the interpolation\nstrategy. This suggests that improved interpolation strate-\ngies based on CNNs, such as those used in [41], would be a\npromising direction for future research.\nIn Fig. 5 we further highlight cases where our method\nshows clear advantages over the competitors – we see that\nthe objects in motion, i.e., cars, result in high errors with\n3We ﬁlter out points that do not satisfy the cycle-consistency constraint,\nthus the results are not directly comparable.\nMethod AEPE↓\nrate=3 rate=5 rate=7 rate=9 rate=11 rate=13 rate=15\nLiteFlowNet [25]CVPR’18 1.66 2.58 6.05 12.95 29.67 52.41 74.96PWC-Net [61, 62]CVPR’18, TPAMI’191.75 2.10 3.21 5.59 14.35 27.49 43.41DGC-Net [38]W ACV’19 2.49 3.28 4.18 5.35 6.78 9.02 12.23GLU-Net [69]CVPR’20 1.98 2.54 3.49 4.24 5.61 7.55 10.78RAFT [65]ECCV’20 1.92 2.12 2.33 2.58 3.90 8.63 13.74\nCOTR 1.66 1.82 1.97 2.13 2.27 2.41 2.61COTR+Interp. 1.71 1.92 2.16 2.47 2.85 3.23 3.76\nTable 3. Quantitative results for ETH3D – We report the Aver-\nage End Point Error (AEPE) at different sampling “rates” (frame\nintervals). Our method performs signiﬁcantly better as the rate\nincreases and the problem becomes more difﬁcult.\nGLU-Net, which is biased towards a single, global motion.\nOur method, on the other hand, successfully recovers the\nﬂow ﬁelds for these cases as well, with minor errors at the\nboundaries, due to interpolation. These examples clearly\ndemonstrate the role that attention plays when estimating\ncorrespondences on scenes with moving objects.\nFinally, we stress that while our method is trained on\nMegaDepth, an urban dataset exhibiting only global, rigid\nmotion, for which ground truth is only available on station-\nary objects (mostly building facades), our method proves\ncapable of recovering the motion of objects moving in differ-\nent directions; see Fig. 5, bottom. In other words, it learns\nto ﬁnd precise, local correspondences within images, rather\nthan global motion.\n4.3. ETH3D\nWe also report results on the ETH3D dataset, follow-\ning [69, 70]. This task is closer to the ‘sparse’ scenario, as\nperformance is only evaluated on pixels corresponding to\nSfM locations with valid ground truth, which are far fewer\nthan for HPatches or KITTI. We summarize the results in\nterms of AEPE in Table 3, sampling pairs of images with an\nincreasing number of frames between them (the sampling\n“rate”), which correlates with baseline and, thus, difﬁculty.\nOur method produces the most accurate correspondences\nfor every setting, tied with LiteFlowNet [ 25] at a 3-frame\ndifference, and drastically outperforms every method as the\nGLU-Net [69] COTR GLU-Net [69] COTR\nIndoors Outdoors\nFigure 6. Qualitative examples on ETH3D –We show results for\nGLU-Net [69] and COTR for two examples, one indoors and one\noutdoors. Correspondences are drawn in green if their reprojection\nerror is below 10 pixels, and red otherwise.\nbaseline increases4; see qualitative results in Fig. 6.\n4.4. Image Matching Challenge\nAccurate, 6-DOF pose estimation in unconstrained ur-\nban scenarios remains too challenging a problem for dense\nmethods. We evaluate our method on a popular challenge\nfor pose estimation with local features, which measures per-\nformance in terms of the quality of the estimated poses, in\nterms of mean average accuracy (mAA) at a 5◦and 10◦error\nthreshold; see [28] for details.\nWe focus on the stereo task. 5 As this dataset contains\nimages with unconstrained aspect ratios, instead of stretching\nthe image before the ﬁrst zoom level, we simply resize the\nshort-edge to 256 and tile our coarse, image-level estimates –\ne.g. an image with 2:1 aspect ratio would invoke two tiling\ninstances. If this process generates overlapping tiles ( e.g.\nwith a 4:3 aspect ratio), we choose the estimate that gives\nbest cycle consistency among them. We pair our method with\nDEGENSAC [12] to retrieve the ﬁnal pose, as recommended\nby [28] and done by most participants.\nWe summarize the results in Table 4. We consider the top\nperformers in the 2020 challenge (a total of 228 entries can\nbe found in the leaderboards [link]). As the challenge places\na limit on the number of keypoints, instead of matches, we\nconsider both categories (up to 2k and up to 8k keypoints\nper image), for fairness – note that our method has no notion\nof keypoints, instead, we query at random locations.6\nWith 2k matches and excluding the methods that feature\nsemantic masking – a heuristic employed in the challenge by\n4We could not report exact numbers for GLU-Net+GOCor as they were\nnot reported, and their implementation was not yet publicly available at the\ntime of submission, but our method should comfortably outperform it in\nevery setting; see [70], Fig 4.\n5The challenge features two tracks: stereo, and multi-view (SfM). Our\napproach works on arbitrary locations and has no notion of ‘keypoints’ (we\nuse random points). For this reason, we do not consider the multiview\ntask, as SfM requires “stable” points to generate 3D landmarks. We plan to\nre-train the model and explore its use on keypoint locations in the future.\n6While we limit the number of matches for each image pair, because we\nuse random points for each pair, the number of points we use per image\nmay grow very large. Hence, our method does not ﬁt into the ‘traditional’\nimage matching pipeline, requiring additional considerations to use this\nbenchmark; we thank the organizers for accommodating our request.\nFigure 7. Qualitative examples for IMC2020 – We visualize the\nmatches produced by COTR (with N = 512) for some stereo pairs\nin the Image Matching Challenge dataset. Matches are coloured\nred to green, according to their reprojection error (high to low).\nMethod Num. Inl. ↑mAA(5◦)↑mAA(10◦)↑\n8k-keypoints\n/globeDoG [34]+HardNet [42]+ModiﬁedGuidedMatching 762.00.476 0.611/globeDoG [34]+HardNet [42]+OANet [76]+GuidedMatching 765.3 0.4710.603/globeDoG [34]+HardNet [42]+AdaLAM [11]+DEGENSAC [12] 627.7 0.460 0.583/globeDoG [34]+HardNet8 [47]+PCA+BatchSampling+DEGENSAC [12] 583.1 0.464 0.590\n2k-keypoints\n/globeSP [13]+SG [54]+DEGENSAC [12]+SemSeg+HAdapt (441.5) (0.452) (0.590)/globeSP [13]+SG [54]+DEGENSAC [12]+SemSeg (404.7) (0.429) (0.568)/globeSP [13]+SG [54]+DEGENSAC [12] 320.5 0.416 0.552/globeDISK [71]+DEGENSAC [12] 404.2 0.388 0.513/globeDoG [34]+HardNet [42]+CustomMatch+DGNSC [12] 245.4 0.369 0.492/globeDoG [34]+HardNet [42]+MAGSAC [3] 181.8 0.318 0.438/globeDoG [34]+LogPolarDesc [17]+DEGENSAC [12] 162.2 0.333 0.457\nOurs\nCOTR+DEGENSAC [12] (N= 2048) 1676.6 0.444 0.580COTR+DEGENSAC [12] (N= 1024) 840.3 0.435 0.571COTR+DEGENSAC [12] (N= 512) 421.3 0.418 0.555COTR+DEGENSAC [12] (N= 256) 211.7 0.392 0.529COTR+DEGENSAC [12] (N= 128) 106.8 0.356 0.492\nTable 4. Stereo performance on IMC2020 – We report mean\nAverage Accuracy (mAA) at 5 ◦and 10◦, and the number of in-\nlier matches, for the top IMC2020 entries, on all test scenes. We\nhighlight the best method in bold and underline the second-best.\nWe exclude entries with components speciﬁcally tailored to the\nchallenge, which are enclosed in parentheses, but report them for\ncompleteness. Finally, we report results with different number\nof matches (N) under pure COTR and one entry with 2048 key-\npoints under COTR guided matching. Pure COTR outperforms\nall methods in the 2k-keypoints category (other than those speciﬁ-\ncally excluded) with as few as 512 matches per image. With /globewe\nindicate clickable URLs to the leaderboard webpage.\nsome participants to ﬁlter out keypoints on transient struc-\ntures such as the sky or pedestrians – COTR ranks second\noverall. These results showcase the robustness and generality\nof our method, considering that it was not trained speciﬁcally\nto solve wide-baseline stereo problems. In contrast, the other\ntop entries are engineered towards this speciﬁc application.\nWe also provide results lowering the cap on the number of\nmatches (see N in Table 4), showing that our method outper-\nforms vanilla SuperGlue [54] (the winner of the 2k-keypoint\ncategory) with as few as 512 input matches, and DISK [71]\n(the runner-up) with as few as 256 input matches. Qualitative\nexamples on IMC are illustrated in Fig. 7.\n4.5. Object-centric scenes\nWhile our evaluation focuses on outdoor scenes, our mod-\nels can be applied to very different images, such as those\nSource Target GLU-Net [69] CVPR’20 COTR\nFigure 8. Object-centric scenes – We compute dense corres-\npondences with GLU-Net [ 69] and COTR, and warp the source\nimage to the target image with the resulting ﬂows. GLU-Net fails\nto capture the bottles being swapped, contrary to our method.\npicturing objects. We show one such example in Fig. 8,\nwhere COTR successfully estimates dense correspondences\nfor two of objects moving in different directions – despite\nthe fact that this data looks nothing alike the images it was\ntrained with. This shows the generality of our approach.\n4.6. Ablation studies\nFiltering. We validate the effectiveness of ﬁltering out bad\ncorrespondences (Section 3.3) on the ETH3D dataset, where\nit improves AEPE by roughly 5% relative. More importantly,\nit effectively removes correspondences with a potentially\nhigh error. This allows the dense interpolation step to pro-\nduce better results. We ﬁnd that on average 1.2% of the\ncorrespondences are ﬁltered out on this dataset – below 1%\nup to ‘rate=9’, gradually increasing until 3.65% at ‘rate=15’.\nOn the role of the transformer. Transformers are powerful\nattention mechanisms, but also costly. It is fair to wonder\nwhether a simpler approach would sufﬁce. We explore the\nuse of MLPs in place of transformers, forming a pipeline\nsimilar to [21], and train such a variant – see supplementary\nmaterial for details. In Fig. 9, we see that the MLP yields\nglobally-smooth estimates, as expected, which fail to model\nthe discontinuities that occur due to 3D geometry. On the\nother hand, COTR with the transformer successfully aligns\nsource and target even when such discontinuities exist.\nZooming. To evaluate how our zooming strategy affects the\nlocalization accuracy of the correspondences, we measure\nthe errors in the estimation at each zoom level, in pixels.\nWe use the HPatches dataset, with more granularity than\nwe use for inference, and display the histogram of pixels\nerrors at each zoom level in Fig. 10. As we zoom-in, the\ndistribution shifts to the left and gets squeezed, yielding\nmore accurate estimates. While zooming in more is nearly\nalways beneﬁcial, we found empirically that four zoom-ins\nwith a factor of two at each zoom provides a good balance\nbetween compute and accuracy.\n5. Conclusions and future work\nWe introduced a functional network for image correspon-\ndence that is capable to addressboth sparse and dense match-\ning problems. Through a novel architecture and recursive in-\nSource Target With MLP With transformer\nFigure 9. Transformer vs MLPs –We show examples of warping\nthe source image onto the target image using estimated dense ﬂows,\nfor two stereo pairs from (top row) the Image Matching Challenge\ntest set and (bottom row) scene ‘0360’ of the MegaDepth dataset,\nwhich was not used for training nor validation. We use both COTR\nand a variant replacing the transformer with MLPs. We compute\ndense correspondences at the coarsest level (for ease of illustration),\nand use them to warp the source image onto the target image. Note\nhow the MLP cannot capture the various discontinuities that occur\ndue to the non-planar 3D structure, and instead tries to solve the\nproblem with planar warps which produce clear artefacts (top row),\nand is also unable to match the dome of the palace (bottom row).\nOur method with the transformer (bottom row) succeeds in both.\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0\nEnd Pixel Error\n0\n5000\n10000\n15000\n20000\n25000\n30000Number of Pixels\n1.0X\n1.06X\n1.14X\n1.22X\n1.32X\n1.43X\n1.56X\n1.72X\n1.92X\n2.17X\n2.5X\n2.94X\n3.57X\n4.55X\n6.25X\n10.0X\nFigure 10. Zooming – We plot the distribution of the end pixel\nerror (EPE) at different zoom-in levels, on the HPatches dataset.\nThe error clearly decreases as more zoom is applied.\nference scheme, it achieves performance on par or above the\nstate of the art on HPatches, KITTI, ETH3D, and one scene\nfrom IMC2020. As future work, in addition to the improve-\nments we have suggested throughout the paper, we intend\nto explore the application of COTR to semantic and multi-\nmodal matching, and incorporate reﬁnement techniques to\nfurther improve the quality of its dense estimates.\nAcknowledgements\nThis work was supported by the Natural Sciences and\nEngineering Research Council of Canada (NSERC) Discov-\nery Grant, Google’s Visual Positioning System, Compute\nCanada, and Advanced Research Computing at the Univer-\nsity of British Columbia.\nReferences\n[1] Matan Atzmon and Yaron Lipman. SAL: Sign Agnostic Learn-\ning of Shapes from Raw Data. In Conference on Computer\nVision and Pattern Recognition, 2020. 1\n[2] Vassileios Balntas, Karel Lenc, Andrea Vedaldi, and Krystian\nMikolajczyk. HPatches: A Benchmark and Evaluation of\nHandcrafted and Learned Local Descriptors. In Conference on\nComputer Vision and Pattern Recognition, 2017. 5\n[3] Daniel Barath, Jiri Matas, and Jana Noskova. MAGSAC:\nMarginalizing Sample Consensus. In Conference on Computer\nVision and Pattern Recognition, 2019. 1, 2, 7\n[4] Ronen Basri, Meirav Galun, Amnon Geifman, David Jacobs,\nYoni Kasten, and Shira Kritchman. Frequency Bias in Neural\nNetworks for Input of Non-uniform Density. In International\nConference on Machine Learning, 2020. 1\n[5] Aritra Bhowmik, Stefan Gumhold, Carsten Rother, and Eric\nBrachmann. Reinforced Feature Points: Optimizing Feature\nDetection and Description for a High-level Task. InConference\non Computer Vision and Pattern Recognition, 2020. 2\n[6] JiaWang Bian, Wen-Yan Lin, Yasuyuki Matsushita, Sai-Kit\nYeung, Tan-Dat Nguyen, and Ming-Ming Cheng. GMS: Grid-\nbased Motion Sstatistics for Fast, Ultra-robust Feature Corre-\nspondence. In Conference on Computer Vision and Pattern\nRecognition, 2017. 2\n[7] Eric Brachmann and Carsten Rother. Neural-Guided RANSAC:\nLearning Where to Sample Model Hypotheses. InInternational\nConference on Computer Vision, 2019. 1, 2\n[8] Thomas Brox and Jitendra Malik. Large Displacement Optical\nFlow: Descriptor Matching in Variational Motion Estimation.\nIEEE Transactions on Pattern Analysis and Machine Intelli-\ngence, 33(3):500–513, 2010. 2\n[9] Michael Calonder, Vincent Lepetit, Christoph Strecha, and\nPascal Fua. BRIEF: Binary Robust Independent Elementary\nFeatures. In European Conference on Computer Vision, 2010.\n2\n[10] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European Confer-\nence on Computer Vision, pages 213–229. Springer, 2020. 1,\n2, 3, 12\n[11] Luca Cavalli, Viktor Larsson, Martin Ralf Oswald, Torsten\nSattler, and Marc Pollefeys. Handcrafted outlier detection\nrevisited. In European Conference on Computer Vision, 2020.\n2, 7\n[12] Ondrej Chum, Tomas Werner, and Jiri Matas. Two-view\nGeometry Estimation Unaffected by a Dominant Plane. In\nConference on Computer Vision and Pattern Recognition, 2005.\n1, 2, 7\n[13] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-\nnovich. SuperPoint: Self-Supervised Interest Point Detection\nand Description. In Conference on Computer Vision and Pat-\ntern Recognition Workshops, 2018. 1, 2, 7\n[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain\nGelly, et al. An Image is Worth 16x16 Words: Transformers\nfor Image Recognition at Scale. In International Conference\non Learning Representations, 2021. 1, 2, 3\n[15] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser,\nCaner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt,\nDaniel Cremers, and Thomas Brox. FlowNet: Learning Op-\ntical Flow with Convolutional Networks. In International\nConference on Computer Vision, 2015. 1, 4\n[16] Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Polle-\nfeys, Josef Sivic, Akihiko Torii, and Torsten Sattler. D2-Net:\nA Trainable CNN for Joint Description and Detection of Lo-\ncal Features. In Conference on Computer Vision and Pattern\nRecognition, 2019. 2, 4\n[17] Patrick Ebel, Anastasiia Mishchuk, Kwang Moo Yi, Pascal\nFua, and Eduard Trulls. Beyond Cartesian Representations for\nLocal Descriptors. In International Conference on Computer\nVision, 2019. 2, 7\n[18] Martin A Fischler and Robert C Bolles. Random Sample\nConsensus: A Paradigm for Model Fitting with Applications to\nImage Analysis and Automated Cartography. Communications\nof the ACM, 24(6):381–395, 1981. 1, 2\n[19] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel\nUrtasun. Vision Meets Robotics: The KITTI Dataset. Interna-\ntional Journal of Robotics Research, 2013. 5\n[20] Rıza Alp G¨uler, Natalia Neverova, and Iasonas Kokkinos.\nDensePose: Dense Human Pose Estimation In The Wild. In\nConference on Computer Vision and Pattern Recognition, 2018.\n1\n[21] Oshri Halimi, Ido Imanuel, Or Litany, Giovanni Trappolini,\nEmanuele Rodol`a, Leonidas Guibas, and Ron Kimmel. To-\nwards Precise Completion of Deformable Shape. European\nConference on Computer Vision, 2020. 2, 8, 12\n[22] Richard Hartley and Andrew Zisserman. Multiple View Ge-\nometry in Computer Vision. Cambridge University Press, 2\nedition, 2003. 1\n[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep Residual Learning for Image Recognition. In Conference\non Computer Vision and Pattern Recognition, 2016. 4, 12\n[24] Berthold KP Horn and Brian G Schunck. Determining Optical\nFlow. Artiﬁcial intelligence, 17(1-3):185–203, 1981. 1, 2\n[25] Tak-Wai Hui, Xiaoou Tang, and Chen Change Loy. Lite-\nFlowNet: A Lightweight Convolutional Neural Network for\nOptical Flow Estimation. In Conference on Computer Vision\nand Pattern Recognition, 2018. 5, 6\n[26] Max Jaderberg, Karen Simonyan, Andrew Zisserman, and Ko-\nray Kavukcuoglu. Spatial transformer networks. In Advances\nin Neural Information Processing Systems, 2015. 2\n[27] Wei Jiang, Weiwei Sun, Andrea Tagliasacchi, Eduard Trulls,\nand Kwang Moo Yi. Linearized Multi-sampling for Differen-\ntiable Image Transformation. In International Conference on\nComputer Vision, 2019. 2\n[28] Yuhe Jin, Dmytro Mishkin, Anastasiia Mishchuk, Jiri Matas,\nPascal Fua, Kwang Moo Yi, and Eduard Trulls. Image Match-\ning Across Wide Baselines: From Paper to Practice. Interna-\ntional Journal of Computer Vision , pages 1–31, 2020. 1, 5,\n7\n[29] Diederik P Kingma and Jimmy Ba. Adam: A Method for\nStochastic Optimization. In International Conference on Learn-\ning Representations, 2014. 5\n[30] Zhengqi Li and Noah Snavely. MegaDepth: Learning Single-\nView Depth Prediction From Internet Photos. In Conference\non Computer Vision and Pattern Recognition, 2018. 4\n[31] Wen-Yan Lin, Fan Wang, Ming-Ming Cheng, Sai-Kit Ye-\nung, Philip HS Torr, Minh N Do, and Jiangbo Lu. CODE:\nCoherence Based Decision Boundaries for Feature Correspon-\ndence. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 2017. 2\n[32] Ce Liu, Jenny Yuen, and Antonio Torralba. SIFT Flow: Dense\nCorrespondence Across Scenes and its Applications. IEEE\nTransactions on Pattern Analysis and Machine Intelligence ,\n33(5):978–994, 2010. 1, 2, 4\n[33] David G Lowe. Distinctive Image Features from Scale-\nInvariant Keypoints. International Journal of Computer Vision,\n60:91–110, 2004. 1\n[34] David G Lowe. Distinctive image Features from Scale-\nInvariant Keypoints. International Journal of Computer Vision,\n60(2):91–110, 2004. 1, 2, 7\n[35] Bruce D Lucas, Takeo Kanade, et al. An Iterative Image\nRegistration Technique with an Application to Stereo Vision.\nIn International Joint Conference on Artiﬁcial Intelligence ,\n1981. 1, 2\n[36] Simon Lynen, Bernhard Zeisl, Dror Aiger, Michael Bosse,\nJoel Hesch, Marc Pollefeys, Roland Siegwart, and Torsten Sat-\ntler. Large-scale, Real-time Visual-inertial Localization Revis-\nited. International Journal of Robotics Research, 39(9):1061–\n1084, 2020. 1\n[37] Jiayi Ma, Ji Zhao, Junjun Jiang, Huabing Zhou, and Xiaojie\nGuo. Locality Preserving Matching. International Journal of\nComputer Vision, 2019. 2\n[38] Iaroslav Melekhov, Aleksei Tiulpin, Torsten Sattler, Marc\nPollefeys, Esa Rahtu, and Juho Kannala. DGC-Net: Dense Ge-\nometric Correspondence Network. In IEEE Winter Conference\non Applications of Computer Vision, 2019. 2, 5, 6\n[39] Moritz Menze, Christian Heipke, and Andreas Geiger. Joint\n3D Estimation of Vehicles and Scene Flow. InISPRS Workshop\non Image Sequence Analysis (ISA), 2015. 5\n[40] Moritz Menze, Christian Heipke, and Andreas Geiger. Object\nScene Flow. International Journal of Photogrammetry and\nRemote Sensing, 2018. 5\n[41] Moustafa Meshry, Dan B Goldman, Sameh Khamis, Hugues\nHoppe, Rohit Pandey, Noah Snavely, and Ricardo Martin-\nBrualla. Neural Rerendering in the Wild. In Conference on\nComputer Vision and Pattern Recognition, 2019. 6\n[42] Anastasiya Mishchuk, Dmytro Mishkin, Filip Radenovic, and\nJiri Matas. Working Hard to Know Your Neighbor’s Mar-\ngins: Local Descriptor Learning Loss. In Advances in Neural\nInformation Processing Systems, 2017. 2, 7\n[43] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked Hour-\nglass Networks for Human Pose estimation. In European\nConference on Computer Vision, 2016. 1\n[44] Yuki Ono, Eduard Trulls, Pascal Fua, and Kwang Moo Yi.\nLF-Net: Learning Local Features from Images. In Advances\nin Neural Information Processing Systems, 2018. 1, 2\n[45] Jeong Joon Park, Peter Florence, Julian Straub, Richard New-\ncombe, and Steven Lovegrove. DeepSDF: Learning Continu-\nous Signed Distance Functions for Shape Representation. In\nConference on Computer Vision and Pattern Recognition, 2019.\n2\n[46] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin,\nNatalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\nKopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan\nTejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie\nBai, and Soumith Chintala. PyTorch: An Imperative Style,\nHigh-Performance Deep Learning Library. In Advances in\nNeural Information Processing Systems, 2019. 4\n[47] Milan Pultar. Improving the HardNet Descriptor. arXiv\nPreprint, 2020. 7\n[48] Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix\nDraxler, Min Lin, Fred Hamprecht, Yoshua Bengio, and Aaron\nCourville. On the Spectral Bias of Neural Networks. In Inter-\nnational Conference on Machine Learning, 2019. 1\n[49] Jerome Revaud, Philippe Weinzaepfel, C´esar De Souza, Noe\nPion, Gabriela Csurka, Yohann Cabon, and Martin Humen-\nberger. R2D2: Repeatable and Reliable Detector and Descrip-\ntor. Advances in Neural Information Processing Systems, 2019.\n2\n[50] Ignacio Rocco, Mircea Cimpoi, Relja Arandjelovi´c, Akihiko\nTorii, Tomas Pajdla, and Josef Sivic. Neighbourhood Consen-\nsus Networks. Advances in Neural Information Processing\nSystems, 2018. 2\n[51] Edward Rosten and Tom Drummond. Machine Learning for\nHigh-Speed Corner Detection. In European Conference on\nComputer Vision, 2006. 2\n[52] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and Li\nFei-Fei. ImageNet Large Scale Visual Recognition Challenge.\nInternational Journal of Computer Vision , 115(3):211–252,\n2015. 4\n[53] Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and\nMarcin Dymczyk. From Coarse to Fine: Robust Hierarchical\nLocalization at Large Scale. In Conference on Computer Vision\nand Pattern Recognition, 2019. 1\n[54] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,\nand Andrew Rabinovich. SuperGlue: Learning Feature Match-\ning with Graph Neural Networks. In Conference on Computer\nVision and Pattern Recognition, 2020. 1, 2, 4, 7\n[55] Torsten Sattler, Bastian Leibe, and Leif Kobbelt. Improving\nImage-based Localization by Active Correspondence Search.\nIn European Conference on Computer Vision, pages 752–765,\n2012. 1\n[56] Johannes Lutz Sch ¨onberger and Jan-Michael Frahm.\nStructure-from-Motion Revisited. In Conference on Computer\nVision and Pattern Recognition, 2016. 1, 4\n[57] Thomas Sch¨ops, Johannes L. Sch¨onberger, Silvano Galliani,\nTorsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas\nGeiger. A Multi-View Stereo Benchmark with High-Resolution\nImages and Multi-Camera Videos. In Conference on Computer\nVision and Pattern Recognition, 2017. 5\n[58] Kenneth O Stanley. Compositional Pattern Producing Net-\nworks: A Novel Abstraction of Development. Genetic pro-\ngramming and evolvable machines, 8(2):131–162, 2007. 2\n[59] Deqing Sun, Stefan Roth, and Michael J Black. Secrets of\nOptical Flow Estimation and Their Principles. In Conference\non Computer Vision and Pattern Recognition, 2010. 2\n[60] Deqing Sun, Stefan Roth, and Michael J Black. A Quantita-\ntive Analysis of Current Practices in Optical Flow Estimation\nand the Principles Behind Them. International Journal of\nComputer Vision, 106(2):115–137, 2014. 1\n[61] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz.\nPWC-Net: CNNs for Optical Flow Using Pyramid, Warping,\nand Cost V olume. In Conference on Computer Vision and\nPattern Recognition, 2018. 5, 6\n[62] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz.\nModels Matter, so Does Training: An Empirical Study of\nCNNs for Optical Flow Estimation. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 42(6):1408–1423,\n2019. 5, 6\n[63] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and\nXiaowei Zhou. LoFTR: Detector-Free Local Feature Matching\nwith Transformers. CVPR, 2021. 2\n[64] Weiwei Sun, Wei Jiang, Eduard Trulls, Andrea Tagliasacchi,\nand Kwang Moo Yi. ACNe: Attentive Context Normalization\nfor Robust Permutation-Equivariant Learning. In Conference\non Computer Vision and Pattern Recognition, 2020. 1, 2\n[65] Zachary Teed and Jia Deng. RAFT: Recurrent All-Pairs Field\nTransforms for Optical Flow. In European Conference on\nComputer Vision, 2020. 5, 6, 12, 13\n[66] Yurun Tian, Xin Yu, Bin Fan, Fuchao Wu, Huub Heijnen,\nand Vassileios Balntas. SOSNet: Second Order Similarity\nRegularization for Local Descriptor Learning. In Conference\non Computer Vision and Pattern Recognition, 2019. 2\n[67] Engin Tola, Vincent Lepetit, and Pascal Fua. Daisy: An Efﬁ-\ncient Dense Descriptor Applied to Wide-baseline Stereo. IEEE\nTransactions on Pattern Analysis and Machine Intelligence ,\n32(5):815–830, 2009. 2\n[68] Carlo Tomasi and T Kanade Detection. Tracking of Point\nFeatures. International Journal of Computer Vision, 1991. 1\n[69] Prune Truong, Martin Danelljan, and Radu Timofte. GLU-\nNet: Global-Local Universal Network for Dense Flow and\nCorrespondences. In Conference on Computer Vision and\nPattern Recognition, 2020. 2, 5, 6, 7, 8\n[70] Prune Truong, Martin Danelljan, Luc Van Gool, and Radu\nTimofte. GOCor: Bringing Globally Optimized Correspon-\ndence V olumes into Your Neural Network. In Advances in\nNeural Information Processing Systems, 2020. 2, 5, 6, 7\n[71] Michał J Tyszkiewicz, Pascal Fua, and Eduard Trulls. DISK:\nLearning Local Features with Policy Gradient. In Advances in\nNeural Information Processing Systems, 2020. 2, 4, 7\n[72] Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Niko-\nlaus Mayer, Eddy Ilg, Alexey Dosovitskiy, and Thomas Brox.\nDemon: Depth and Motion Network for Learning Monocu-\nlar Stereo. In Conference on Computer Vision and Pattern\nRecognition, 2017. 1, 2\n[73] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in Neural\nInformation Processing Systems, 2017. 1, 2, 3, 12\n[74] Kwang Moo Yi, Eduard Trulls, Vincent Lepetit, and Pascal\nFua. LIFT: Learned Invariant Feature Transform. In European\nConference on Computer Vision, pages 467–483, 2016. 1, 2\n[75] Kwang Moo Yi, Eduard Trulls, Yuki Ono, Vincent Lepetit,\nMathieu Salzmann, and Pascal Fua. Learning to Find Good\nCorrespondences. In Conference on Computer Vision and\nPattern Recognition, 2018. 1, 2\n[76] Jiahui Zhang, Dawei Sun, Zixin Luo, Anbang Yao, Lei Zhou,\nTianwei Shen, Yurong Chen, Long Quan, and Hongen Liao.\nLearning Two-View Correspondences and Geometry Using\nOrder-Aware Network. International Conference on Computer\nVision, 2019. 1, 2, 7\n[77] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G.\nLowe. Unsupervised Learning of Depth and Ego-Motion from\nVideo. In Conference on Computer Vision and Pattern Recog-\nnition, 2017. 1, 2\nCOTR: Correspondence Transformer for Matching Across Images\nSupplementary Material\nTraining loss Validation loss\nFigure B. Unstable training and validation loss for COTR with\nlog-linear positional encoding. We terminate the training earlier as\nthe loss diverges.\nTraining loss Validation loss\nFigure A. Training and validation loss for COTR with linear\npositional encoding. Both losses slowly converge to a stable status.\nA. Compute\nThe functional (and recursive) nature of our approach,\ncoupled with the use of a transformer, means that our method\nhas signiﬁcant compute requirements. Our currently non-\noptimized prototype implementation queries one point at\na time, and achieves 35 correspondences per second on a\nNVIDIA RTX 3090 GPU. This limitation could be addressed\nby careful engineering in terms of tiling and batching. Our\npreliminary experiments show no signiﬁcant drop in per-\nformance when we query different points inside a given\ncrop – we could thus potentially process any queries at the\ncoarsest level in a single operation, and drastically reduce\nthe number of operations in the zoom-ins (depending on\nhow many queries overlap in a given crop). We expect this\nwill speed up inference drastically. In addition to batching\nthe queries at inference time, we plan to explore its use on\nnon-random points (such as keypoints) and advanced inter-\npolation techniques.\nB. Log-linear vs Linear\nHere, we empirically demonstrate that linear positional\nencoding is important. We train two COTR models with\ndifferent positional encoding strategies; see Section 3.2.\nOne model uses log-linear increase in the frequency of the\nsine/cosine function, and the other uses linear increase in-\nstead. Fig. A shows thatCOTR successfully converges using\nthe linear increase strategy. However, as shown in Fig. B,\nCOTR fails to converge with the commonly used log-linear\nstrategy [73, 10]. We suspect that this is because the task\nof ﬁnding correspondences does not involve very high fre-\nquency components, but further investigation is necessary\nand is left as future work.\nC. Architectural details for COTR\nBackbone. We use the lower layers of ResNet50 [ 23] as\nour CNN backbone. We extract the feature map with 1024\nchannels after layer3, i.e., after the fourth downsampling\nstep. We then project the feature maps with 1024 channels\nwith 1 ×1 convolution to 256 channels to reduce the amount\nof computation that happens within the transformers.\nTransformers. We use 6 layers in both the transformer\nencoder and the decoder. Each encoder layer contains an 8-\nhead self-attention module, and each decoder layer contains\nan 8-head encoder-decoder attention module. Note that we\ndisallow the self-attention in the decoder, in order to maintain\nthe independence between queries – queries should not affect\neach other.\nMLP. Once the transformer decoder process the results,\nwe obtain a 256 dimensional vector that represents where\nthe correspondence should be. We use a 3-layer MLP to\nregress the corresponding point coordinates from the 256-\ndimensional latent vector. Each layer contains 256 neurons,\nfollowed by ReLU activations.\nD. Architectural details for the MLP variant\nBackbone. We use the same backbone inCOTR. The differ-\nence here is that, once the feature map with 256 channels is\nobtained, we apply max pooling to extract the global latent\nvector for the image, as suggested in [ 21]. We also tried\na variant where we do not apply global pooling and use a\nfully-connected layer to bring it down to a manageable size\nof 1024 neurons but it quickly provided degenerate results,\nwhere all correspondence estimates were at the centre.\nMLP. With the latent vectors from each image, we use\na 3 layer MLP to regress the correspondence coordinates.\nSpeciﬁcally, the input to the coordinate regressor is a 768-\ndimensional vector, which is the concatenation of two global\nlatent vectors for the input images and the positional en-\ncoded query point. Similarly to the MLP used in COTR,\neach linear layer contains 256 neurons, and followed by\nReLU activations.\nE. Comparing with RAFT [65]\nRAFT [65] performs better in KITTI-type of scenarios,\nnot necessarily so for other cases. To show this, we provide\nMethod ETH3D\nAEPE↓rate=3 rate=5 rate=7 rate=9 rate=11 rate=13 rate=15RAFT [65]ECCV’20 1.92 2.12 2.33 2.58 3.90 8.63 13.74COTR 1.66 1.82 1.97 2.13 2.27 2.41 2.61COTR +Interp. 1.71 1.92 2.16 2.47 2.85 3.23 3.76\nMethod KITTI 2012 KITTI 2015 HPatches\nAEPE↓ Fl↓ AEPE↓ Fl↓ AEPE↓PCK-1px↑PCK-3px↑PCK-5px↑RAFT [65]ECCV’202.15 9.30 5.00 17.4 44.3 31.22 62.48 70.85COTR 1.28 7.36 2.62 9.92 7.75 40.91 82.37 91.10COTR +Interp. 2.26 10.50 6.12 16.907.98 33.08 77.09 86.33\nMethod Image Matching Challenge\nNum. Inl.↑ mAA(5◦)↑ mAA(10◦)↑\nRAFT [65]ECCV’20+DEGENSAC (N= 2048) 1066.1 0.163 0.259COTR +DEGENSAC (N= 2048)1686.2 0.515 0.678\nTable A. RAFT on ETH3D, KITTI, HPatches, and IMC2020.\nresults for RAFT [65] on all other datasets in Table A. On\nKITTI, sparse COTR still performs best, and with the inter-\npolation strategy it is roughly on par with RAFT [ 65]. On\nother datasets, COTR outperforms RAFT [ 65] by a large\nmargin1.\n1Note that RAFT [65] requires two input images of the same size. We\nresize them to 1024×1024 for HPatches and the Image Matching Challenge.",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7435846328735352
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6276853084564209
    },
    {
      "name": "Inference",
      "score": 0.570833683013916
    },
    {
      "name": "Prior probability",
      "score": 0.5653271079063416
    },
    {
      "name": "Transformer",
      "score": 0.49300500750541687
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4540686011314392
    },
    {
      "name": "Zoom",
      "score": 0.42398035526275635
    },
    {
      "name": "Data mining",
      "score": 0.3220841884613037
    },
    {
      "name": "Bayesian probability",
      "score": 0.13096889853477478
    },
    {
      "name": "Lens (geology)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Petroleum engineering",
      "score": 0.0
    }
  ],
  "topic": "Computer science"
}