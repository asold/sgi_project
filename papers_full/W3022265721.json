{
    "title": "Quantifying Attention Flow in Transformers",
    "url": "https://openalex.org/W3022265721",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A197925216",
            "name": "Samira Abnar",
            "affiliations": [
                "University of Amsterdam"
            ]
        },
        {
            "id": "https://openalex.org/A1997952565",
            "name": "Willem Zuidema",
            "affiliations": [
                "University of Amsterdam"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2973291083",
        "https://openalex.org/W1507039213",
        "https://openalex.org/W2760327630",
        "https://openalex.org/W1514535095",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2939556020",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W2970726176",
        "https://openalex.org/W2118463056",
        "https://openalex.org/W2962958286",
        "https://openalex.org/W2549835527",
        "https://openalex.org/W2562607067",
        "https://openalex.org/W2972761935",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2977162702",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2948771346",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2934842096",
        "https://openalex.org/W2972854536"
    ],
    "abstract": "In the Transformer model, \"self-attention\" combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.",
    "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4190–4197\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n4190\nQuantifying Attention Flow in Transformers\nSamira Abnar\nILLC, University of Amsterdam\ns.abnar@uva.nl\nWillem Zuidema\nILLC, University of Amsterdam\nw.h.zuidema@uva.nl\nAbstract\nIn the Transformer model, “self-attention”\ncombines information from attended embed-\ndings into the representation of the focal em-\nbedding in the next layer. Thus, across lay-\ners of the Transformer, information originating\nfrom different tokens gets increasingly mixed.\nThis makes attention weights unreliable as ex-\nplanations probes. In this paper, we consider\nthe problem of quantifying this ﬂow of infor-\nmation through self-attention. We propose two\nmethods for approximating the attention to in-\nput tokens given attention weights, attention\nrollout and attention ﬂow, as post hoc methods\nwhen we use attention weights as the relative\nrelevance of the input tokens. We show that\nthese methods give complementary views on\nthe ﬂow of information, and compared to raw\nattention, both yield higher correlations with\nimportance scores of input tokens obtained us-\ning an ablation method and input gradients.\n1 Introduction\nAttention (Bahdanau et al., 2015; Vaswani et al.,\n2017) has become the key building block of neu-\nral sequence processing models, and visualizing\nattention weights is the easiest and most popular\napproach to interpret a model’s decisions and to\ngain insights about its internals (Vaswani et al.,\n2017; Xu et al., 2015; Wang et al., 2016; Lee et al.,\n2017; Dehghani et al., 2019; Rockt ¨aschel et al.,\n2016; Chen and Ji, 2019; Coenen et al., 2019; Clark\net al., 2019). Although it is wrong to equate atten-\ntion with explanation (Pruthi et al., 2019; Jain and\nWallace, 2019), it can offer plausible and mean-\ningful interpretations (Wiegreffe and Pinter, 2019;\nVashishth et al., 2019; Vig, 2019). In this paper,\nwe focus on problems arising when we move to the\nhigher layers of a model, due to lack of token iden-\ntiﬁability of the embeddings in higher layers (Brun-\nner et al., 2020).\nWe propose two simple but effective methods to\ncompute attention scores to input tokens (i.e.,token\nattention) at each layer, by taking raw attentions\n(i.e., embedding attention) of that layer as well as\nthose from the precedent layers. These methods\nare based on modelling the information ﬂow in the\nnetwork with a DAG (Directed Acyclic Graph), in\nwhich the nodes are input tokens and hidden em-\nbeddings, edges are the attentions from the nodes\nin each layer to those in the previous layer, and\nthe weights of the edges are the attention weights.\nThe ﬁrst method, attention rollout, assumes that\nthe identities of input tokens are linearly combined\nthrough the layers based on the attention weights.\nTo adjust attention weights, it rolls out the weights\nto capture the propagation of information from in-\nput tokens to intermediate hidden embeddings. The\nsecond method, attention ﬂow, considers the atten-\ntion graph as a ﬂow network. Using a maximum\nﬂow algorithm, it computes maximum ﬂow values,\nfrom hidden embeddings (sources) to input tokens\n(sinks). In both methods, we take the residual con-\nnection in the network into account to better model\nthe connections between input tokens and hidden\nembedding. We show that compared to raw atten-\ntion, the token attentions from attention rollout and\nattention ﬂow have higher correlations with the im-\nportance scores obtained from input gradients as\nwell as blank-out, an input ablation based attribu-\ntion method. Furthermore, we visualize the token\nattention weights and demonstrate that they are bet-\nter approximations of how input tokens contribute\nto a predicted output, compared to raw attention.\nIt is noteworthy that the techniques we propose\nin this paper, are not toward making hidden embed-\ndings more identiﬁable, or providing better atten-\ntion weights for better performance, but a new set\nof attention weights that take token identity prob-\nlem into consideration and can serve as a better\ndiagnostic tool for visualization and debugging.\n4191\n(a) Embedding attentions\n (b) Attention rollout\n (c) Attention ﬂow\nFigure 1: Visualisation of attention weights.\nFigure 2: Raw Attention maps for the CLS token at\ndifferent layers.\n2 Setups and Problem Statement\nIn our analysis, we focus on the verb number pre-\ndiction task, i.e., predicting singularity or plurality\nof a verb of a sentence, when the input is the sen-\ntence up to the verb position. We use the subject-\nverb agreement dataset (Linzen et al., 2016). This\ntask and dataset are convenient choices, as they of-\nfer a clear hypothesis about what part of the input\nis essential to get the right solution. For instance,\ngiven “the key to the cabinets ” as the input, we\nknow that attending to “key” helps the model pre-\ndict singular as output while attending to “cabinets”\n(an agreement attractor, with the opposite number)\nis unhelpful.\nWe train a Transformer encoder, with GPT-\n2 Transformer blocks as described in (Radford\net al., 2019; Wolf et al., 2019) (without masking).\nThe model has 6 layers, and 8 heads, with hid-\nden/embedding size of 128. Similar to Bert (De-\nvlin et al., 2019) we add a CLS token and use\nits embedding in the ﬁnal layer as the input to\nthe classiﬁer. The accuracy of the model on the\nsubject-verb agreement task is 0.96. To facilitate\nreplication of our experiments we will make the\nimplementations of the models we use and algo-\nrithms we introduce publicly available at https:\n//github.com/samiraabnar/attention_flow.\nWe start by visualizing raw attention in Figure 1a\n(like Vig 2019). The example given here is cor-\nrectly classiﬁed. Crucially, only in the ﬁrst couple\nof layers, there are some distinctions in the atten-\ntion patterns for different positions, while in higher\nlayers the attention weights are rather uniform. Fig-\nure 2 (left) gives raw attention scores of the CLS\ntoken over input tokens (x-axis) at different lay-\ners (y-axis), which similarly lack an interpretable\npattern.These observations reﬂect the fact that as\nwe go deeper into the model, the embeddings are\nmore contextualized and may all carry similar in-\nformation. This underscores the need to track down\nattention weights all the way back to the input layer\nand is in line with ﬁndings of Serrano and Smith\n(2019), who show that attention weights do not\nnecessarily correspond to the relative importance\nof input tokens.\nTo quantify the usefulness of raw attention\nweights, and the two alternatives that we consider\nin the next section, besides input gradients, we\nemploy an input ablation method, blank-out, to es-\ntimate an importance score for each input token.\nBlank-out replaces each token in the input, one\nby one, with UNK and measures how much it af-\nfects the predicted probability of the correct class.\nWe compute the Spearman’s rank correlationco-\nefﬁcient between the attention weights of the CLS\nembedding in the ﬁnal layer and the importance\nscores from blank-out. As shown in the ﬁrst row\nof Table 1, the correlation between raw attention\nweights of the CLS token and blank-out scores is\nrather low, except for the ﬁrst layer. As we can see\nin Table 2 this is also the case when we compute\nthe correlations with input gradients.\nL1 L2 L3 L4 L5 L6\nRaw 0.69±0.27 0.10±0.43 -0.11±0.49 -0.09±0.52 0.20±0.45 0.29±0.39Rollout 0.32±0.26 0.38±0.27 0.51±0.26 0.62±0.26 0.70±0.25 0.71±0.24Flow 0.32±0.26 0.44±0.29 0.70±0.25 0.70±0.22 0.71±0.22 0.70±0.22\nTable 1: SpearmanR correlation of attention based im-\nportance with blank-out scores for 2000 samples from\nthe test set for the verb number prediction model.\n3 Attention Rollout and Attention Flow\nAttention rollout and attention ﬂow recursively\ncompute the token attentions in each layer of a\n4192\nL1 L2 L3 L4 L5 L6\nRaw 0.53±0.33 0.16±0.38 -0.06±0.42 0.00±0.47 0.24±0.40 0.46±0.35Rollout 0.22±0.31 0.27±0.32 0.39±0.32 0.47±0.32 0.53±0.32 0.54±0.31Flow 0.22±0.31 0.31±0.34 0.54±0.32 0.61±0.28 0.60±0.28 0.61±0.28\nTable 2: SpearmanR correlation of attention based im-\nportance with input gradients for 2000 samples from\nthe test set for the verb number prediction model.\ngiven model given the embedding attentions as in-\nput. They differ in the assumptions they make\nabout how attention weights in lower layers affect\nthe ﬂow of information to the higher layers and\nwhether to compute the token attentions relative to\neach other or independently.\nTo compute how information propagates from\nthe input layer to the embeddings in higher lay-\ners, it is crucial to take the residual connections\nin the model into account as well as the attention\nweights. In a Transformer block, both self-attention\nand feed-forward networks are wrapped by resid-\nual connections, i.e., the input to these modules is\nadded to their output. When we only use attention\nweights to approximate the ﬂow of information in\nTransformers, we ignore the residual connections.\nBut these connections play a signiﬁcant role in\ntying corresponding positions in different layers.\nHence, to compute attention rollout and attention\nﬂow, we augment the attention graph with extra\nweights to represent residual connections. Given\nthe attention module with residual connection, we\ncompute values in layerl+1 as Vl+1 = Vl+WattVl,\nwhere Watt is the attention matrix. Thus, we have\nVl+1 = (Watt + I)Vl. So, to account for residual\nconnections, we add an identity matrix to the at-\ntention matrix and re-normalize the weights. This\nresults in A = 0.5Watt + 0.5I, where A is the raw\nattention updated by residual connections.\nFurthermore, analyzing individual heads re-\nquires accounting for mixing of information be-\ntween heads through a position-wise feed-forward\nnetwork in Transformer block. Using attention roll-\nout and attention ﬂow, it is also possible to analyze\neach head separately. We explain in more details\nin Appendix A.1. However, in our analysis in this\npaper, for simplicity, we average the attention at\neach layer over all heads.\nAttention rollout Attention rollout is an intuitive\nway of tracking down the information propagated\nfrom the input layer to the embeddings in the higher\nlayers. Given a Transformer withL layers, we want\nto compute the attention from all positions in layer\nli to all positions in layer lj, where j < i. In the\nattention graph, a path from node v at position k\nin li, to node u at position m in lj, is a series of\nedges that connect these two nodes. If we look\nat the weight of each edge as the proportion of\ninformation transferred between two nodes, we\ncan compute how much of the information at v\nis propagated to u through a particular path by\nmultiplying the weights of all edges in that path.\nSince there may be more than one path between\ntwo nodes in the attention graph, to compute the\ntotal amount of information propagated fromv to u,\nwe sum over all possible paths between these two\nnodes. At the implementation level, to compute the\nattentions from li to lj, we recursively multiply the\nattention weights matrices in all the layers below.\n˜A(li) =\n{\nA(li) ˜A(li−1) if i > j\nA(li) if i = j (1)\nIn this equation, ˜A is attention rollout, A is raw at-\ntention and the multiplication operation is a matrix\nmultiplication. With this formulation, to compute\ninput attention we set j = 0.\nAttention ﬂow In graph theory, a ﬂow network\nis a directed graph with a “capacity” associated\nwith each edge. Formally, given G = (V, E) is a\ngraph, where V is the set of nodes, and E is the set\nof edges in G; C = {cuv ∈R |∀u, vwhere eu,v ∈\nE ∧u ̸= v}denotes the capacities of the edges and\ns, t∈V are the source and target (sink) nodes re-\nspectively; ﬂow is a mapping of edges to real num-\nbers, f : E →R, that satisﬁes two conditions: (a)\ncapacity constraint: for each edge the ﬂow value\nshould not exceed its capacity, |fuv ≤cuv|; (b)\nﬂow conservation: for all nodes except s and t the\ninput ﬂow should be equal to output ﬂow –sum\nof the ﬂow of outgoing edges should be equal to\nsum of the ﬂow of incoming edges. Given a ﬂow\nnetwork, a maximum ﬂow algorithm ﬁnds a ﬂow\nwhich has the maximum possible value between s\nand t (Cormen et al., 2009).\nTreating the attention graph as a ﬂow network,\nwhere the capacities of the edges are attention\nweights, using any maximum ﬂow algorithm, we\ncan compute the maximum attention ﬂow from any\nnode in any of the layers to any of the input nodes.\nWe can use this maximum-ﬂow-value as an approx-\nimation of the attention to input nodes. In attention\nﬂow, the weight of a single path is the minimum\nvalue of the weights of the edges in the path, in-\nstead of the product of the weights. Besides, we\n4193\nattention rollout\nattention ﬂow\nFigure 3: Attention maps for the CLS token\n.\ncan not compute the attention for node s to node\nt by adding up the weights of all paths between\nthese two nodes, since there might be an overlap\nbetween the paths and this might result in overﬂow\nin the overlapping edges.\nIt is noteworthy that both of the proposed meth-\nods can be computed in polynomial time.O(d∗n2)\nfor attention rollout and O(d2 ∗n4) for attention\nﬂow, where d is the depth of the model, and n is\nthe number of tokens.\n4 Analysis and Discussion\nNow, we take a closer look at these three views of\nattention. Figure 1 depicts raw attention, attention\nrollout and attention ﬂow for a correctly classiﬁed\nexample across different layers. It is noteworthy\nthat the ﬁrst layer of attention rollout and attention\nﬂow are the same, and their only difference with\nraw attention is the addition of residual connec-\ntions. As we move to the higher layers, we see that\nthe residual connections fade away. Moreover, in\ncontrast to raw attention, the patterns of attention\nrollout and attention ﬂow become more distinctive\nin the higher layers.\nFigures 2 and 3 show the weights from raw at-\ntention, attention rollout and attention ﬂow for the\nCLS embedding over input tokens (x-axis) in all\n6 layers (y-axis) for three examples. The ﬁrst ex-\nample is the same as the one in Figure 1. The sec-\nond example is “the article on NNP large systems\n<?>”. The model correctly classiﬁes this exam-\nple and changing the subject of the missing verb\nfrom “article” to “articles” ﬂips the decision of the\nmodel. The third example is “here the NNS differ\nin that the female <?>”, which is a miss-classiﬁed\nexample and again changing “NNS” (plural noun)\nto “NNP” (singular proper noun) ﬂips the decision\nof the model.\nFor all cases, the raw attention weights are al-\nmost uniform above layer three (discussed before).\nraw attention\nattention rollout\nattention ﬂow\n(a) “The author talked to Sara about mask book”\nraw attention\nattention rollout\nattention ﬂow\n(b) “Mary convinced John of mask love”\nFigure 4: Bert attention maps. We look at the attention\nweights from the mask embedding to the two potential\nreferences for it, e.g. “author” and “Sara” in (a) and\n“Mary” and “John” in (b). The bars, at the left, show\nthe relative predicted probability for the two possible\npronouns, “his” and “her”.\nIn the case of the correctly classiﬁed example, we\nobserve that both attention rollout and attention\nﬂow assign relatively high weights to both the sub-\nject of the verb, “article’ and the attractor, “sys-\ntems”. For the miss-classiﬁed example, both at-\ntention rollout and attention ﬂow assign relatively\nhigh scores to the “NNS” token which is not the\nsubject of the verb. This can explain the wrong\nprediction of the model.\nThe main difference between attention rollout\nand attention ﬂow is that attention ﬂow weights are\namortized among the set of most attended tokens,\nas expected. Attention ﬂow can indicate a set of\ninput tokens that are important for the ﬁnal decision.\nThus we do not get sharp distinctions among them.\nOn the other hand, attention rollout weights are\nmore focused compared to attention ﬂow weights,\nwhich is sensible for the third example but not as\nmuch for the second one.\nL1 L3 L5 L6\nRaw 0.12 ±0.21 0.09±0.21 0.08±0.20 0.09±0.21\nRollout 0.11±0.19 0.12±0.21 0.13±0.21 0.13±0.20\nFlow 0.11 ±0.19 0.11±0.21 0.12±0.22 0.14±0.21\nTable 3: SpearmanR correlation of attention based im-\nportance with input gradients for 100 samples from the\ntest set for the DistillBERT model ﬁne tuned on SST-2.\nFurthermore, as shown in Table 1 and 2 both\nattention rollout and attention ﬂow, are better\ncorrelated with blank-out scores and input gradi-\nents compared to raw attention, but attention ﬂow\n4194\nweights are more reliable than attention rollout.\nThe difference between these two methods is rooted\nin their different views of attention weights. At-\ntention ﬂow views them as capacities, and at every\nstep of the algorithm, it uses as much of the capac-\nity as possible. Hence, attention ﬂow computes the\nmaximum possibility of token identities to propa-\ngate to the higher layers. Whereas attention rollout\nviews them as proportion factors and at every step,\nit allows token identities to be propagated to higher\nlayers exactly based on this proportion factors. This\nmakes attention rollout stricter than attention ﬂow,\nand so we see that attention rollout provides us with\nmore focused attention patterns. However, since\nwe are making many simplifying assumptions, the\nstrictness of attention rollout does not lead to more\naccurate results, and the relaxation of attention ﬂow\nseems to be a useful property.\nAt last, to illustrate the application of atten-\ntion ﬂow and attention rollout on different tasks\nand different models, we examine them on two\npretrained BERT models. We use the models\navailable at https://github.com/huggingface/\ntransformers.\nTable 3 shows the correlation of the importance\nscore obtained from raw attention, attention rollout\nand attention ﬂow from a DistillBERT (Sanh et al.,\n2019) model ﬁne-tuned to solve “SST-2” (Socher\net al., 2013), the sentiment analysis task from the\nglue benchmark (Wang et al., 2018). Even though\nfor this model, all three methods have very low\ncorrelation with the input gradients, we can still see\nthat attention rollout and attention ﬂow are slightly\nbetter than raw attention.\nFurthermore, in Figure 4, we show an example\nof applying these methods to a pre-trained Bert\nto see how it resolves the pronouns in a sentence.\nWhat we do here is to feed the model with a sen-\ntence, masking a pronoun. Next, we look at the\nprediction of the model for the masked word and\ncompare the probabilities assigned to “her” and\n“his”. Then we look at raw attention, attention roll-\nout and attention ﬂow weights of the embeddings\nfor the masked pronoun at all the layers. In the\nﬁrst example, in Figure 4a, attention rollout and\nattention ﬂow are consistent with each other and\nthe prediction of the model. Whereas, the ﬁnal\nlayer of raw attention does not seem to be consis-\ntent with the prediction of the models, and it varies\na lot across different layers. In the second exam-\nple, in Figure 4b, only attention ﬂow weights are\nconsistent with the prediction of the model.\n5 Conclusion\nTranslating embedding attentions to token atten-\ntions can provide us with better explanations about\nmodels’ internals. Yet, we should be cautious about\nour interpretation of these weights, because, we\nare making many simplifying assumptions when\nwe approximate information ﬂow in a model with\nthe attention weights. Our ideas are simple and\ntask/architecture agnostic. In this paper, we in-\nsisted on sticking with simple ideas that only re-\nquire attention weights and can be easily employed\nin any task or architecture that uses self-attention.\nWe should note that all our analysis in this paper is\nfor a Transformer encoder, with no casual masking.\nSince in Transformer decoder, future tokens are\nmasked, naturally there is more attention toward\ninitial tokens in the input sequence, and both atten-\ntion rollout and attention ﬂow will be biased toward\nthese tokens. Hence, to apply these methods on a\nTransformer decoder, we should ﬁrst normalize\nbased on the receptive ﬁeld of attention.\nFollowing this work, we can build the attention\ngraph with effective attention weights (Brunner\net al., 2020) instead of raw attentions. Furthermore,\nwe can come up with a new method that adjusts the\nattention weights using gradient-based attribution\nmethods (Ancona et al., 2019).\nAcknowledgements\nWe thank Mostafa Dehghani, Wilker Aziz, and the\nanonymous reviewers for their valuable feedback\nand comments on this work. The work presented\nhere was funded by the Netherlands Organization\nfor Scientiﬁc Research (NWO), through a Gravita-\ntion Grant 024.001.006 to the Language in Interac-\ntion Consortium.\nReferences\nMarco Ancona, Enea Ceolini, Cengiz ¨Oztireli, and\nMarkus Gross. 2019. Gradient-Based Attribution\nMethods, pages 169–191. Springer International\nPublishing.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In proceedings of\nthe 2015 International Conference on Learning Rep-\nresentations.\nGino Brunner, Yang Liu, Damian Pascual, Oliver\nRichter, Massimiliano Ciaramita, and Roger Watten-\n4195\nhofer. 2020. On identiﬁability in transformers. In\nInternational Conference on Learning Representa-\ntions.\nHanjie Chen and Yangfeng Ji. 2019. Improving the in-\nterpretability of neural sentiment classiﬁers via data\naugmentation. arXiv preprint arXiv:1909.04225.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 276–286, Florence, Italy. Association\nfor Computational Linguistics.\nAndy Coenen, Emily Reif, Ann Yuan, Been Kim,\nAdam Pearce, Fernanda Vi´egas, and Martin Watten-\nberg. 2019. Visualizing and measuring the geometry\nof bert. arXiv preprint arXiv:1906.02715.\nThomas H. Cormen, Charles E. Leiserson, Ronald L.\nRivest, and Clifford Stein. 2009. Introduction to\nAlgorithms, Third Edition , 3rd edition. The MIT\nPress.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Łukasz Kaiser. 2019. Univer-\nsal transformers. In proceedings of the 2019 Inter-\nnational Conference on Learning Representations.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies. Association for Computational Lin-\nguistics.\nSarthak Jain and Byron C. Wallace. 2019. Attention\nis not Explanation. In proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 3543–3556. Association\nfor Computational Linguistics.\nJaesong Lee, Joong-Hwi Shin, and Jun-Seok Kim.\n2017. Interactive visualization and manipulation\nof attention-based neural machine translation. In\nproceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 121–126.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn\nsyntax-sensitive dependencies. Transactions of the\nAssociation for Computational Linguistics , 4:521–\n535.\nDanish Pruthi, Mansi Gupta, Bhuwan Dhingra, Gra-\nham Neubig, and Zachary C Lipton. 2019. Learning\nto deceive with attention-based explanations. arXiv\npreprint arXiv:1909.07913.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8).\nTim Rockt ¨aschel, Edward Grefenstette, Karl Moritz\nHermann, Tomas Kocisky, and Phil Blunsom. 2016.\nReasoning about entailment with neural attention.\nIn International Conference on Learning Represen-\ntations (ICLR).\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version of\nbert: smaller, faster, cheaper and lighter.\nSoﬁa Serrano and Noah A. Smith. 2019. Is attention\ninterpretable? In proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics. Association for Computational Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1631–1642, Seattle, Washington, USA. Asso-\nciation for Computational Linguistics.\nShikhar Vashishth, Shyam Upadhyay, Gaurav Singh\nTomar, and Manaal Faruqui. 2019. Attention in-\nterpretability across nlp tasks. arXiv preprint\narXiv:1909.11218.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nJesse Vig. 2019. Visualizing attention in transformer-\nbased language models. arXiv preprint\narXiv:1904.02679.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP , pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nYequan Wang, Minlie Huang, Li Zhao, et al. 2016.\nAttention-based lstm for aspect-level sentiment clas-\nsiﬁcation. In proceedings of the 2016 conference on\nempirical methods in natural language processing ,\npages 606–615.\nSarah Wiegreffe and Yuval Pinter. 2019. Attention is\nnot not explanation. In proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP). Association for Computational Linguis-\ntics.\n4196\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,\nAaron Courville, Ruslan Salakhudinov, Rich Zemel,\nand Yoshua Bengio. 2015. Show, attend and tell:\nNeural image caption generation with visual atten-\ntion. In proceedings of International Conference on\nMachine Learning, pages 2048–2057.\n4197\nA Appendices\nA.1 Single Head Analysis\nFor analysing the attention weights, with multi-\nhead setup, we could either analyze attention heads\nseparately, or we could average all heads and have a\nsingle attention graph. However, we should be care-\nful that treating attention heads separately could\npotentially mean that we are assuming there is no\nmixing of information between heads, which is not\ntrue as we combine information of heads in the\nposition-wise feed-forward network on top of self-\nattention in a transformer block. It is possible to\nanalyse the role of each head in isolation of all other\nheads using attention rollout and attention ﬂow. To\nnot make the assumption that there is no mixing\nof information between heads, for computing the\n“input attention”, we will treat all the layers below\nthe layer of interest as single head layers, i.e., we\nsum the attentions of all heads in the layers below.\nFor example, we can compute attention rollout for\nhead k at layer i as ˜A(i, k) = A(i, k) ¯A(i), where,\n¯A(i) is attention rollout computed for layer i with\nthe single head assumption."
}