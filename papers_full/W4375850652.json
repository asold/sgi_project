{
    "title": "Building Blocks for a Complex-Valued Transformer Architecture",
    "url": "https://openalex.org/W4375850652",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2162940858",
            "name": "Florian Eilers",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2114362709",
            "name": "Xiaoyi Jiang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W93167102",
        "https://openalex.org/W6696670905",
        "https://openalex.org/W1519761144",
        "https://openalex.org/W6790144085",
        "https://openalex.org/W2998874549",
        "https://openalex.org/W6792695861",
        "https://openalex.org/W6772203101",
        "https://openalex.org/W2167581055",
        "https://openalex.org/W6730401039",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6780226713",
        "https://openalex.org/W4212802476",
        "https://openalex.org/W3137984567",
        "https://openalex.org/W2962996460",
        "https://openalex.org/W3205975767",
        "https://openalex.org/W3096408984",
        "https://openalex.org/W2143572124",
        "https://openalex.org/W6788556936",
        "https://openalex.org/W6784333009",
        "https://openalex.org/W4306955484",
        "https://openalex.org/W6760601182",
        "https://openalex.org/W3121141908",
        "https://openalex.org/W6756383013",
        "https://openalex.org/W3019166713",
        "https://openalex.org/W6796210077",
        "https://openalex.org/W3085139254",
        "https://openalex.org/W3125044414",
        "https://openalex.org/W2998161426",
        "https://openalex.org/W3183864931",
        "https://openalex.org/W6738884980",
        "https://openalex.org/W6757632829",
        "https://openalex.org/W3037264106",
        "https://openalex.org/W3035294798",
        "https://openalex.org/W3015651375",
        "https://openalex.org/W4206706211",
        "https://openalex.org/W2949756029",
        "https://openalex.org/W3139049060",
        "https://openalex.org/W3127737671",
        "https://openalex.org/W2995971510",
        "https://openalex.org/W2919624000",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2969262604",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2559688696",
        "https://openalex.org/W2290300449",
        "https://openalex.org/W3167860837"
    ],
    "abstract": "Most deep learning pipelines are built on real-valued operations to deal with\\nreal-valued inputs such as images, speech or music signals. However, a lot of\\napplications naturally make use of complex-valued signals or images, such as\\nMRI or remote sensing. Additionally the Fourier transform of signals is\\ncomplex-valued and has numerous applications. We aim to make deep learning\\ndirectly applicable to these complex-valued signals without using projections\\ninto $\\\\mathbb{R}^2$. Thus we add to the recent developments of complex-valued\\nneural networks by presenting building blocks to transfer the transformer\\narchitecture to the complex domain. We present multiple versions of a\\ncomplex-valued Scaled Dot-Product Attention mechanism as well as a\\ncomplex-valued layer normalization. We test on a classification and a sequence\\ngeneration task on the MusicNet dataset and show improved robustness to\\noverfitting while maintaining on-par performance when compared to the\\nreal-valued transformer architecture.\\n",
    "full_text": "BUILDING BLOCKS FOR A COMPLEX-V ALUED TRANSFORMER ARCHITECTURE\nFlorian Eilers∗, Xiaoyi Jiang†\nFaculty of Mathematics and Computer Science, University of M¨unster, M¨unster, Germany\nABSTRACT\nMost deep learning pipelines are built on real-valued operations to\ndeal with real-valued inputs such as images, speech or music signals.\nHowever, a lot of applications naturally make use of complex-valued\nsignals or images, such as MRI or remote sensing. Additionally\nthe Fourier transform of signals is complex-valued and has numer-\nous applications. We aim to make deep learning directly applicable\nto these complex-valued signals without using projections into R2.\nThus we add to the recent developments of complex-valued neural\nnetworks by presenting building blocks to transfer the transformer\narchitecture to the complex domain. We present multiple versions of\na complex-valued Scaled Dot-Product Attention mechanism as well\nas a complex-valued layer normalization. We test on a classification\nand a sequence generation task on the MusicNet dataset and show\nimproved robustness to overfitting while maintaining on-par perfor-\nmance when compared to the real-valued transformer architecture.\nIndex Terms— Deep learning techniques, Complex-valued\nneural networks, Transformer architecture\n1. INTRODUCTION\nIn recent years, many applications have benefited from the fast de-\nvelopment and high quality results of deep learning methods. Most\nof these methods focus on real-valued pipelines for applications with\nreal-valued signals, such as natural images or encodings of natural\nlanguage processing. There is however a great amount of applica-\ntions that naturally deal with complex-valued signals, such as MRI\nimages [1, 2] or remote sensing [3] and the Fourier transform of\nreal-valued signals [4, 5] or images [6, 7] and it has been shown\nthat fully complex-valued architectures often (but not always [8]) de-\nliver superior performance when dealing with complex-valued sig-\nnals. The complex numbers come with an intrinsic algebraic struc-\nture that can not be captured by the simple isomorphism ofC ∼ R2,\nespecially because there is no natural way to define multiplication\nin R2, which, however, is an important part of many deep learning\nbuilding blocks. [9] has provided a lot of those building blocks, such\nas complex-valued convolution, batch normalization and initializa-\ntion. These building blocks are of great help for a large amount of\ncurrent architectures, especially in image and signal processing. In\nmany fields, architectures building on the idea of attention mecha-\nnisms have successfully been applied. Especially the immense suc-\ncess of the transformer architecture [10] has shown that attention\nbased architectures can be superior and have since become standard\nin many applications. We seek to provide a solid generalization of\nthe building blocks of the transformer architecture in the complex\ndomain and show experimental evidence that it improves robustness\n∗Member of the CiM IMPRS Graduate School M¨unster\n†This work was partly supported by the Deutsche Forschungsgemein-\nschaft (DFG) – CRC 1450 – 431460824 and European Union’s Horizon 2020\nunder the Marie Sklodowska-Curie grant agreement No 778602 Ultracept.\nto overfitting while maintaining on-par performance when compared\nto the real-valued transformer architecture.\nOur key contributions are: 1) Newly developed building blocks\nconsisting of: a. derivation of a complex-valued attention mecha-\nnism, generalizing the Scaled Dot-Product attention [10]; b. intro-\nduction of complex-valued layer normalization. 2) Adaptation of\nbuilding blocks from existing complex-valued neural networks for\nthe transformer architecture. 3) Demonstration of improved robust-\nness to overfitting while maintaining on-par results compared to the\nreal-valued model.\nThe combination of the first two contributions provide the foun-\ndation for a mathematically rigorous complex-valued transformer ar-\nchitecture. The source code for the full architecture and all experi-\nments is available as a Pytorch module 1.\n2. RELATED WORK\nComplex-valued neural networks have been researched for a long\ntime [11, 12]. An early standard book and foundation for much re-\nsearch to come is the work by Hirose [13]. Recently, an increasing\nnumber of works in complex-valued neural networks have been pub-\nlished [14], driven by the interest in applications, which naturally\ndeal with complex-valued signals: remote sensing [15, 16], MRI\nprocessing [17, 1] and frequency analysis through Fourier transform.\n[9] provides building blocks for complex-valued neural net-\nworks. They present complex versions of linear layers, convolu-\ntional layers, batch normalization, initialization and different acti-\nvation functions. They also comment on complex differentiability,\nreferring to earlier works [18]. Complex-valued building blocks\nhave been used to develop a multitude of architectures, such as\ncomplex-valued generative adversarial networks [2, 3], complex-\nvalued convolutional recurrent networks [19] and a complex-valued\nU-net [4]. There has also been recent interest in optimizing com-\nputability on GPUs for complex-valued neural networks [20].\nThe transformer architecture [10] was a great success in natu-\nral language processing and has since become dominant in the field\n[21, 22]. It has spread into vision [23, 24], music [25] and more ap-\nplications [26]. Additionally, there has been many works to improve\nand rework the architecture [27].\nTo the best of our knowledge, there are only two works concern-\ning the design of a complex-valued transformer architecture. [5] pro-\nposes a complex-valued transformer, motivated by the multiplicative\nstructure of the Dot-Product attention. They separate the product\nQ(KT ) into eight real-valued products and then apply real-valued\nattention to all summands separately. While being a well motivated\nchoice, they use real-valued encoding matrices, making the network\nnot fully complex-valued. Using complex-valued encoding matrices\nwould lead to a total of 64 summands and an unreasonable compu-\ntational blowup. While testing their framework against competitive\n1https://zivgitlab.uni-muenster.de/ag-pria/\ncv-transformer\narXiv:2306.09827v1  [cs.LG]  16 Jun 2023\nLegend:\nEE = Encoder\nEmbedding\nDE = Decoder\nEmbedding\nPE = Positional\nEncoding\n(M)MHA = (Masked)\nMulti-Head Attention\nLN = Layer\nnormalization\nFF = Feed Forward\nN x = repeat N times\nN x\nN x\nClass Seq.\nGen.\nOutput Prob.\nLinear\nSigmoid\nInput Output\nFF\nLN\nPE\nEE DE\nLN\nLN LN\nSigmoid\nOutput Prob.\nLinear\nLN\nMHA MMHA\nMHA\nFF\nQ K V\n⟨ , ⟩\nℂ\nMask\nd k\n--\n(opt.)\nℜ( )\nsoftmax\n⟨ , ⟩\nℂ\n1\n2\nFig. 1: Left: The general transformer architecture as introduced in\n[10]. Building blocks, whose complex-valued versions are derived in\nthis paper are highlighted in red. Right: The complex-valued Scaled\nDot-Product Attention as suggested in subsection 4.1.\nmodels, such as real-valued transformers and LSTMs, they do not\ntest against other definitions of complex-valued attention modules.\nAdditionally, in their experimental part, they do not use an indepen-\ndent test set but rather just evaluate on the validation set. We answer\nthe questions this work left open by incorporating their idea into\nour experiments and using it as one of many valid definitions for a\ncomplex-valued transformer. [28] proposes a complex-valued meta-\nlearning framework for signal recognition. As a byproduct, they de-\nfine a complex-valued attention. However, they do not evaluate dif-\nferent options and they do not utilize the Dot-Product in the complex\ndomain. Additionally, they propose to use the complex variance for\nnormalization instead of the more flexible covariance matrix. Their\ndefinition is, as one of many, incorporated in our framework.\nThere are some more works on different kinds of complex-\nvalued attention modules [15, 16, 29]. These use different kinds of\nconvolutional architectures but are not complex-valued versions of\nthe Scaled Dot-Product attention [10].\n3. TRANSFORMER\nThis section serves as a brief description to the transformer architec-\nture as introduced in [10].\n3.1. Architecture\nThe architecture consists of an encoder and decoder module. The\nencoder module alone can be used for classification tasks while the\nfull architecture can be used for sequence generation where the core\nidea is to input the original input into the encoder and the earlier\noutputs of the sequence generation into the decoder. Both modules\nstart with an embedding and a positional encoding of their respec-\ntive inputs. Afterwards the modules consist of Multi-Head Attention\nmechanism with residual connections followed by a layer normal-\nization and a feed forward module - a small MLP with two linear\nlayers and an activation. Details can be seen in Figure 1.\n3.2. Real-valued Attention\nThe Scaled Dot-Product Attention is the core of the Transformer ar-\nchitecture. The input consists of three matrices, called query Q, key\nK and value V . First the Dot-Product of the key and query is calcu-\nlated, then scaled by the square-root of their equal dimensions √dk.\nThe output is normalized by the softmax function and afterwards\nmultiplied with the values. Defining the softmax for a vector X of\nlength n as\nsoftmax(X) = σ(X) = exp(X)Pn\ni=1 exp(Xi) (1)\nwe can formulate the Scaled Dot-Product Attention as\nAtt(Q, K, V) = σ\n\u0012Q(K)T\n√dk\n\u0013\nV (2)\nThis core concept of Scaled Dot-Product Attention is extended to the\nmore general concept of Multi-Head Attention (MHA) by applying\nlearnable linear projectionsWQ, WK, WV to the inputs and project\nit back with another learnable linear projection WO:\nMHA (Q, K, V) = Concat(h1, . . . , hk)WO\ni ,\nwhere hi = Att(QWQ\ni , KWK\ni , V WV\ni ) (3)\nFor the training process, it is necessary to mask future events in the\ndecoder input. This is obtained by addition of −∞ to the respective\ntokens before the softmax, which results in an attention score of 0.\n4. COMPLEX-V ALUED BUILDING BLOCKS\nThe general purpose of this section is the introduction of existing\nbuilding blocks and the development of new ones where needed\n(subsections 4.1 and 4.2) for a mathematically rigorous extension\nof the transformer architecture to the complex domain. [5] has al-\nready introduced complex-valued fully connected feed forward lay-\ners. Since the position within a sequence is real-valued, we adopt\nthe sine and cosine positional encoding as originally used in [10] but\nother positional encodings would be possible [30, 31].\n4.1. Complex-valued Attention\nWhen generalizing the Scaled Dot-Product Attention to the complex\ndomain a problem arises: the max operation does not work in C\nand the softmax does not either. However, the core idea behind the\noperation σ(XY T ) for X, Y∈ Rn is to define a similarity between\nX and Y which is then scaled to (0, 1) by the softmax non-linearity\nσ. The operation without softmax-rescaling can be described as the\nDot-Product in Rn. Using this concept in Cn leads to:\n⟨X, Y⟩ =\nnX\ni=1\nXi ¯Yi =\nnX\ni=1\n|X||Y |exp(i(ϕXi − ϕYi )) (4)\nWhen neglecting the magnitudes of X and Y , we get\nexp(i(ϕXi − ϕYi )) = cos(ϕXi − ϕYi ) + i sin(ϕXi − ϕYi ) (5)\nThe real part of this term, cos ϕXi − ϕYi , maximizes at 1 for ϕXi −\nϕYi = 0, which is equivalent toXi = Yi. It strictly decreases, when\n|ϕXi − ϕYi | growth, up to a minimum of −1 at |ϕXi − ϕYi | = π,\nwhich is equivalent to Xi = −Yi. Thus we have that R(⟨X, Y⟩)\nmeasures the similarity of X and Y for every component and adds\nthese up. Additionally, we get two desired properties for a similarity\nmeasure: Symmetry and rotational invariance. Symmetry holds be-\ncause the conjugate symmetry of the Dot-Product does not change\nits real part:\nR⟨Q, K⟩ = R⟨K, Q⟩ = R⟨K, Q⟩ (6)\nZ=⟨Q, K⟩\nZ=Q(KT )\nFig. 2: Illustration of Dot-Product vs Q(KT ) in C. 1st and 2nd\ncolumn show behavior for the extreme cases of Q = K and Q =\n−K. 3rd and 4th column show rotational invariance of the Dot-\nProduct vs behavior on rotations of Q(KT ).\nBy rotational invariance we mean: If Q and K are (elementwise)\nboth rotated by a fixed angle α, ⟨Q, K⟩ does not change (Figure 2).\nexp(i((ϕXi + α) − (ϕYi + α))) = exp(i(ϕXi − ϕYi )) (7)\nNote that the equality ⟨X, Y⟩ = Q(KT ) holds in the real domain,\nbut not in the complex domain. While symmetry still holds when\nusing Q(K)T , the rotational invariance does not (Figure 2).\nWhen taking the magnitude into account, this similarity is scaled\nby the factors |X| and |Y |, meaning that high values are obtained for\nvectors of high magnitude pointing in the same direction. We can\nnow formulate the complex-valued Dot-Product Attention as:\nCAtt(A, B) = σ\n\u0012R⟨Q, K⟩√dk\n\u0013\nV (8)\nThis pipeline is presented on the right in Figure 1.\nThe motivation of Scaled Dot-Product attention leads to Equa-\ntion 8. However, other possibilities to generalize the real-valued\nScaled Dot-Product attention are using the absolute value with and\nwithout keeping the phase information and using both the real and\nthe imaginary part. We define the following possibilities and test\nall these in section 5. |z|C denotes the absolute value of a complex\nnumber z and sgn(z) its sign (e.g. z\n|z| if z ̸= 0 and 1 o/w):\nAAtt(A, B) =σ\n\u0012|⟨Q, K⟩|C√dk\n\u0013\nV (9)\nAP Att(A, B) =σ\n\u0012|⟨Q, K⟩|C√dk\n\u0013\nsgn(⟨Q, K⟩)V (10)\nRIAtt(A, B) =\n\u0012\nσ\n\u0012R⟨Q, K⟩√dk\n\u0013\n+ i σ\n\u0012I⟨Q, K⟩√dk\n\u0013\u0013\nV (11)\nAdditionally, it is possible to replace the dot product⟨Q, K⟩ in every\nversion with Q (K)T . Using Q (K)T , AAtt and CAtt have been\nused before [28], we test these variants in section 5. Note K 7→ K\nis not linear in C and thus cannot be learned directly by WK.\nUsing any of these formulations of the complex-valued Scaled\nDot-Product Attention the adoption of Multi-Head Attention as de-\nscribed in subsection 3.2 is straightforward. We can replace the\nlearnable linear projections WQ, WK, WV and WO by complex-\nvalued linear projections [9] and can then use the formulations as\ndescribed in [10] and Equation 3. The necessary masking of future\nresults in the training process as described in subsection 3.2 works in\nthis framework by applying the mask after the respective mappings\nfrom the complex to the real domain (such as R, I, | · |).\nWe also compare to the approach of [5] which relies on splitting\nthe product Q (K)T into (real-valued) summands and applying real-\nvalued attention per summand.\n4.2. Complex-valued Layer Normalization\nNormalization layers play a big role in the success of most neural\nnetwork architectures. A complex-valued version has been proposed\nfor batch normalization [9], however layer normalization is prefer-\nable for methods like LSTM or RNNs [32] as well as the transformer\narchitecture [10]. It is insufficient to normalize the real and imagi-\nnary part of the complex-valued layer independently, since this may\nlead to very elliptic shapes of the output distribution [9]. This brings\nthe need of a complex-valued version of layer normalization. The\nnecessary building blocks are the complex-valued expected value\nand the covariance matrix. For some complex vector z ∈ Cn these\nare defined as\nE(z) = 1\nn\nnX\ni=1\nzi (12)\nCovC(z) =\n\u0012\nVar(R(z)) Cov(R(z), I(z))\nCov(R(z), I(z)) Var(I(z))\n\u0013\n(13)\nwhere Var and Cov denote the (real-valued) Variance and Covari-\nance, respectively.\nLet X be the output of a layer, the normalized output is then:\n\u0012\nR(CLN(X))\nI(CLN(X))\n\u0013\n= Cov\n−1\n2\nC (X)\n\u0012\nR(X − E(X))\nI(X − E(X))\n\u0013\n(14)\nThis compact form can easily be calculated with fast closed form\nsolutions for the inverse and the square root of 2 × 2 matrices. It is\npossible to manipulate the output distribution with learnable param-\neters. It can be shifted with a learnable parameter β ∈ C and scaled\nwith a learnable covariance matrix, a positive definite 2 × 2 matrix.\nTo ensure the positive definiteness of the resulting matrix, we utilize:\n\u0012\na b\nb c\n\u0013\npositive definite ⇔ a >0, c >0, b2 < ac (15)\nThus, we can scale and shift the output ˆX of the layer normalization\nwith 5 degrees of freedom by learning a covariance matrix ζ and a\nshifting parameter β ∈ C and get:\n\u0012\nR( ˆX)\nI( ˆX)\n\u0013\n= ζ\n1\n2 Cov\n−1\n2\nC (X)\n\u0012\nR(X − E(X))\nI(X − E(X))\n\u0013\n+ β (16)\nThe output distribution then has covariance ζ and expected value β.\n5. EXPERIMENTAL RESULTS\nOverall we perform two experiments: Automatic music transcrip-\ntion performed by the transformer encoder and a sequence genera-\ntion task performed by the full transformer architecture. We com-\npare the introduced methods for a complex-valued attention module\nas described in Equations 8-11 using the proposed Dot-Product as\nwell as the version using Q(KT ). Additionally, we compare to the\napproach of [5] and to the real-valued transformer as a baseline. For\nthe latter, the real and imaginary part of the real-valued input was\nstacked alternating resulting in an input dimension of twice the origi-\nnal dimension. Both tasks are trained and evaluated on the MusicNet\ndataset [33]. The dataset consists of 330 pieces of music divided into\n39438 samples consisting of 64 time steps, which are interpreted as\none input token. These samples are split into 35111 training, 2030\nvalidation and 3897 test samples, where the pieces of music between\nthe splits do not overlap. We perform Fourier transform on the data\nArchitecture Classification Seq. generation\nC-Transformer (ours) 14m 27m\nYang et al [5] 12m 20m\nR-Transformer [10] 18m 33m\nTable 1: Number of real-valued trainable parameters. For complex-\nvalued parameters the real and imaginary parts count separately.\nClassification Sequence Generation\nAttention Dot-Prod Q(KT ) Dot-Prod Q(KT )\nCAtt 0.7164 0.7142 0.3272 0.3283\nAPAtt 0.6965 0.6926 0.2240 0.3231\nAAtt 0.7117 0.7099 0.3172 0.3271\nRIAtt 0.7070 0.7059 0.3201 0.3236\nYang et al [5] x 0.7088 x 0.3072\nReal [10] 0.7109 x 0.0737 x\nTable 2: Average precision results on test set for both tasks. Dot-\nProd refers to the use of Dot-Product as described in subsection 4.1.\nas preprocessing, as well as resampling as done in [5] with a method\nintroduced by [34].\nFor both experiments the important hyperparameters are: Batch-\nsize 35, 100 epochs, dropout 0.1, learning rate 10−4, embedding di-\nmension 320, 6 layers with 8 attention heads and a hidden dimension\nin the feed forward module of 2048. As the encoder embedding we\nuse a four layer complex-valued CNN followed by a fully connected\nlayer. For the decoder embedding we used a fully connected embed-\nding, since the input here are labels rather then a continuous signal.\nTo test the impact of the convolutions on the encoder, we perform\na small ablation study on just the proposed method (Equation 8) by\nremoving the CNN in the encoder embedding.\n5.1. Automatic music transcription task\nFor the multiclass classification problem, we classify into 128\nclasses, as offered in the dataset. The results in Table 2 show com-\npetitive behavior of most methods after 100 epochs. The best result\nby a slight margin is obtained by the proposed method as introduced\nin Equation 8. For most attention variants, it shows that the inner\nproduct version performs better, than using the product Q(KT ).\nAdditionally, all complex-valued architectures show improved ro-\nbustness to overfitting (Figure 3), with no or minor decreases after\nlonger training time, while the real transformer shows massive\noverfitting starting after 10 epochs.\n5.2. Sequence generation task\nFor the sequence generation task, we split each sample into 43 in-\nput time steps and 21 time steps to be generated. The output to be\ngenerated are the notes of the missing 21 time steps of the sam-\nples in an iterative way, where in each iteration the input of the\ndecoder is the output of the earlier iterations. The results show\nthat the real transformer is not able to learn the sequence gener-\nation properly. The low training loss implies that the inability to\nlearn is due to heavy overfitting. The robustness to overfitting al-\nready shown in subsection 5.1 seems to solve this problem, where\nall complex-valued methods learn reasonably well. The best perfor-\nmance is again obtained by the proposed method Equation 8. While\nsome of the other methods introduced in this paper perform similarly\nwell, the method of [5] performs noticeably worse.\nClassification Seq. generation\nC-Attention w/o conv. 0.5240 0.1652\nC-Attention with conv. 0.7164 0.3272\nTable 3: Average precision results on test set for both task as a small\nablation study on the impact of the convolutional encoder.\n0 20 40 60 80 100\nEpochs\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75Validation AVS\n0 20 40 60 80 100\nEpochs\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.10Validation Loss\n0 20 40 60 80 100\nEpochs\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08Train Loss\n0 20 40 60 80 100\nEpochs\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35Validation AVS\n0 20 40 60 80 100\nEpochs\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30Validation Loss\n0 20 40 60 80 100\nEpochs\n0.00\n0.02\n0.04\n0.06\n0.08Train Loss\nC-DP\nAP-DP\nA-DP\nRI-DP\nReal\nC-QK\nAP-QK\nA-QK\nRI-QK\n[5]\nFig. 3: Training and validation Results. Average Precision Score\n(A VS) and Loss are displayed. Left: Classification, right: Sequence\ngeneration. ”DP” denotes Dot-Product, ”QK” denotes Q(KT ).\n5.3. Discussion\nTable 3 shows that the complex-valued transformer architecture\nwithout convolutions is able to learn a meaningful solution, but the\nCNN in the encoder is necessary for state-of-the-art results.\nOverall, the real transformer struggles with overfitting on both\ntasks. The complex-valued transformers improve in this regard while\nmaintaining on-par (subsection 5.1) or superior (subsection 5.2) per-\nformance. This result is in line with earlier results, showing superior\nrobustness to overfitting for, e.g., CNNs [35] or RCNNs [36]. We\nare the first ones to show this for the transformer architecture.\n6. CONCLUSION\nWe presented building blocks for a complex-valued transformer ar-\nchitecture. That includes newly developed formulations of complex-\nvalued attention mechanisms as well as a complex-valued layer nor-\nmalization. We have shown that it improves robustness to overfitting\non a classification and a sequence generation task, while maintain-\ning competitive performance compared to the real-valued algorithm\nwhen applied to complex-valued signals. This opens up the oppor-\ntunity to incorporate the transformer architecture into a more broad\nclass of applications that naturally make use of complex-valued im-\nages. Additionally, the complex-valued Fourier transform of signals\ncan now directly be used in the transformer architecture without us-\ning the isomorphism C → R2 that results in a loss in robustness\nagainst overfitting. This work also serves as a base for the further\ndevelopment of complex-valued versions of extensions to the trans-\nformer [27] and other architectures that use the attention mechanism.\n7. REFERENCES\n[1] E. Cole, J. Cheng, J. Pauly, and S. Vasanawala, “Analysis of\ndeep complex-valued convolutional neural networks for MRI\nreconstruction and phase-focused applications,”Magnetic Res-\nonance in Medicine, vol. 86, no. 2, pp. 1093–1109, 2021.\n[2] B. Vasudeva, P. Deora, S. Bhattacharya, and P. M. Pradhan,\n“Compressed sensing mri reconstruction with Co-VeGAN:\nComplex-valued generative adversarial network,” in WACV,\n2022, pp. 1779–1788.\n[3] X. Li, Q. Sun, L. Li, X. Liu, H. Liu, L. Jiao, and F. Liu, “SSCV-\nGANs: Semi-supervised complex-valued GANs for PolSAR\nimage classification,” IEEE Access, vol. 8, pp. 146560–\n146576, 2020.\n[4] H.-S. Choi, J.-H. Kim, J. Huh, A. Kim, J.-W. Ha, and K. Lee,\n“Phase-aware speech enhancement with deep complex U-Net,”\nin ICLR, 2018.\n[5] M. Yang, M. Q. Ma, D. Li, Y .-H. H. Tsai, and R. Salakhut-\ndinov, “Complex transformer: A framework for modeling\ncomplex-valued sequence,” in ICASSP, 2020, pp. 4232–4236.\n[6] Y . Yang and S. Soatto, “FDA: Fourier domain adaptation for\nsemantic segmentation,” in CVPR, 2020, pp. 4084–4094.\n[7] Q. Xu, R. Zhang, Y . Zhang, Y . Wang, and Q. Tian, “A Fourier-\nbased framework for domain generalization,” in CVPR, 2021,\npp. 14383–14392.\n[8] D. Yin, C. Luo, Z. Xiong, and W. Zeng, “PHASEN: A phase-\nand-harmonics-aware speech enhancement network,” in AAAI,\n2020.\n[9] C. Trabelsi, O. Bilaniuk, Y . Zhang, D. Serdyuk, S. Subrama-\nnian, J. Felipe Santos, S. Mehri, N. Rostamzadeh, Y . Bengio,\nand C. J. Pal, “Deep complex networks,” in ICLR, 2018.\n[10] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all\nyou need,” in NIPS, 2017, pp. 5998–6008.\n[11] M. Kataoka, M. Kinouchi, and M. Hagiwara, “Music infor-\nmation retrieval system using complex-valued recurrent neural\nnetworks,” in IEEE SMC, 1998, pp. 4290–4295.\n[12] Y . Kuroe, N. Hashimoto, and T Mori, “On energy function\nfor complex-valued neural networks and its applications,” in\nICONIP, 2002.\n[13] A. Hirose, Complex-Valued Neural Networks: Theories and\nApplications, World Scientific, 2003.\n[14] J. Bassey, L. Qian, and X. Li, “A survey of complex-valued\nneural networks,” arXiv:2101.12249, 2021.\n[15] Y .-P. Zhang, Q. Zhang, Le Kang, Y . Luo, and L. Zhang,\n“End-to-end recognition of similar space cone–cylinder tar-\ngets based on complex-valued coordinate attention networks,”\nIEEE Transactions on Geoscience and Remote Sensing, vol.\n60, pp. 1–14, 2021.\n[16] S. Ren and F. Zhou, “Polsar image classification with complex-\nvalued residual attention enhanced U-Net,” in IGARSS, 2021,\npp. 3045–3048.\n[17] P. Virtue, X. Yu Stella, and M. Lustig, “Better than real:\nComplex-valued neural nets for MRI fingerprinting,” in ICIP,\n2017, pp. 3953–3957.\n[18] A. Hirose and S. Yoshida, “Generalization characteristics of\ncomplex-valued feedforward neural networks in relation to sig-\nnal coherence,” IEEE Transactions on Neural Networks and\nLearning Systems, vol. 23, no. 4, pp. 541–551, 2012.\n[19] Y . Hu, Y . Liu, S. Lv, M. Xing, S. Zhang, Y . Fu, J. Wu,\nB. Zhang, and L. Xie, “DCCRN: Deep complex convolution\nrecurrent network for phase-aware speech enhancement,” in\nINTERSPEECH, 2020.\n[20] H. Zhang et al., “An optical neural chip for implementing\ncomplex-valued neural network,” Nature Communications,\nvol. 12, pp. 457, 2021.\n[21] D. W. Otter, J. R. Medina, and J. K. Kalita, “A survey of the\nusages of deep learning for natural language processing,”IEEE\nTransactions on Neural Networks and Learning Systems, vol.\n32, no. 2, pp. 604–624, 2020.\n[22] D. Hu, “An introductory survey on attention mechanisms in\nNLP problems,” in IntelliSys, 2019.\n[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. Zhai, T. Unterthiner, M. Dehghani, M. Minderer,\nG. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, “An image\nis worth 16x16 words: Transformers for image recognition at\nscale,” in ICLR, 2021.\n[24] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and\nM. Shah, “Transformers in vision: A survey,”ACM Computing\nSurveys, vol. 54, pp. 200:1–200:41, 2021.\n[25] C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, I. Simon,\nC. Hawthorne, N. Shazeer, A. M. Dai, M. D. Hoffman, M. Din-\nculescu, and D. Eck, “Music transformer: Generating music\nwith long-term structure,” in ICLR, 2019.\n[26] T. Lin, Y . Wang, X. Liu, and X. Qiu, “A survey of transform-\ners,” AI Open, vol. 3, pp. 111–132, 2022.\n[27] Y . Tay, M. Dehghani, D. Bahri, and D. Metzler, “Efficient\ntransformers: A survey,” ACM Computing Surveys, vol. 55,\nno. 6, pp. 109:1–109:28, 2023.\n[28] Y . Dong, Y . Peng, M. Yang, S. Lu, and Q. Shi, “Signal trans-\nformer: Complex-valued attention and meta-learning for signal\nrecognition,” arXiv:2106.04392, 2021.\n[29] H.-W. Cho, S. Choi, Y .-R. Cho, and J. Kim, “Complex-valued\nchannel attention and application in ego-velocity estimation\nwith automotive radar,”IEEE Access, vol. 9, pp. 17717–17727,\n2021.\n[30] B. Wang, D. Zhao, C. Lioma, Q. Li, P. Zhang, and J. G. Simon-\nsen, “Encoding word order in complex embeddings,” in ICLR,\n2020.\n[31] X. Chu, Z. Tian, B. Zhang, X. Wang, X. Wei, H. Xia, and\nC. Shen, “Conditional positional encodings for vision trans-\nformers,” arXiv:2102.10882, 2021.\n[32] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,”\narXiv:1607.06450, 2016.\n[33] J. Thickstun, Z. Harchaoui, and S. M. Kakade, “Learning fea-\ntures of music from scratch,” in ICLR, 2017.\n[34] J. O. Smith, “Digital audio resampling home page,” https:\n//ccrma.stanford.edu/˜jos/resample/”, 2020.\n[35] N. Guberman, “On complex valued convolutional neural net-\nworks,” arXiv:1602.09046, 2016.\n[36] F. Zhao, G. Ma, W. Xie, and H. Liu, “Semi-supervised re-\ncurrent complex-valued convolution neural network for polsar\nimage classification,” in IGARSS, 2019."
}