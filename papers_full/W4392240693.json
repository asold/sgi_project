{
    "title": "Advanced hybrid LSTM-transformer architecture for real-time multi-task prediction in engineering systems",
    "url": "https://openalex.org/W4392240693",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5100770456",
            "name": "Kangjie Cao",
            "affiliations": [
                "Minzu University of China"
            ]
        },
        {
            "id": "https://openalex.org/A1579964672",
            "name": "Ting Zhang",
            "affiliations": [
                "Minzu University of China"
            ]
        },
        {
            "id": null,
            "name": "Jueqiao Huang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5100770456",
            "name": "Kangjie Cao",
            "affiliations": [
                "Minzu University of China"
            ]
        },
        {
            "id": "https://openalex.org/A1579964672",
            "name": "Ting Zhang",
            "affiliations": [
                "Minzu University of China"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4321796689",
        "https://openalex.org/W2944851425",
        "https://openalex.org/W161130869",
        "https://openalex.org/W2180718409",
        "https://openalex.org/W4206412842",
        "https://openalex.org/W4306155095",
        "https://openalex.org/W3159791669",
        "https://openalex.org/W3208605024",
        "https://openalex.org/W4379137647",
        "https://openalex.org/W3198666609",
        "https://openalex.org/W3041860561",
        "https://openalex.org/W4224103449",
        "https://openalex.org/W4281651325",
        "https://openalex.org/W3202434625",
        "https://openalex.org/W4382933914",
        "https://openalex.org/W4318542545",
        "https://openalex.org/W4315927936",
        "https://openalex.org/W4220870779",
        "https://openalex.org/W4367182526",
        "https://openalex.org/W4362538029",
        "https://openalex.org/W4284674612",
        "https://openalex.org/W4321787272",
        "https://openalex.org/W4362470802",
        "https://openalex.org/W4309647995",
        "https://openalex.org/W4323848846",
        "https://openalex.org/W4323315504",
        "https://openalex.org/W4380355610",
        "https://openalex.org/W4385250562",
        "https://openalex.org/W4362665268",
        "https://openalex.org/W4307640482",
        "https://openalex.org/W4319661451",
        "https://openalex.org/W4362514912",
        "https://openalex.org/W4377825882",
        "https://openalex.org/W4317717538",
        "https://openalex.org/W4384154454",
        "https://openalex.org/W4386413260"
    ],
    "abstract": null,
    "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2024) 14:4890  | https://doi.org/10.1038/s41598-024-55483-x\nwww.nature.com/scientificreports\nAdvanced hybrid \nLSTM‑transformer architecture \nfor real‑time multi‑task prediction \nin engineering systems\nKangjie Cao 1,2, Ting Zhang 1,2* & Jueqiao Huang 2\nIn the field of engineering systems—particularly in underground drilling and green stormwater \nmanagement—real‑time predictions are vital for enhancing operational performance, ensuring safety, \nand increasing efficiency. Addressing this niche, our study introduces a novel LSTM‑transformer hybrid \narchitecture, uniquely specialized for multi‑task real ‑time predictions. Building on advancements \nin attention mechanisms and sequence modeling, our model integrates the core strengths of LSTM \nand Transformer architectures, offering a superior alternative to traditional predictive models. \nFurther enriched with online learning, our architecture dynamically adapts to variable operational \nconditions and continuously incorporates new field data. Utilizing knowledge distillation techniques, \nwe efficiently transfer insights from larger, pretrained networks, thereby achieving high predictive \naccuracy without sacrificing computational resources. Rigorous experiments on sector ‑specific \nengineering datasets validate the robustness and effectiveness of our approach. Notably, our \nmodel exhibits clear advantages over existing methods in terms of predictive accuracy, real ‑time \nadaptability, and computational efficiency. This work contributes a pioneering predictive framework \nfor targeted engineering applications, offering actionable insights into.\nKeywords LSTM-transformer hybrid architecture, Real-time predictions, Engineering systems, \nUnderground drilling, Green stormwater management, Multi-task learning, Online learning, Knowledge \ndistillation, Predictive accuracy\nModern engineering systems are increasingly becoming digitized and automated, requiring sophisticated control \nmechanisms to ensure their robustness and  efficiency1,2. The integration of advanced sensors and interconnected \ndevices contributes to the complexity of these  systems3. In this context, real-time monitoring and predictive \nanalytics are critical for anticipating system failures and maintaining optimal  performance4,5.\nBackground and challenges\nFrom the dawn of industrialization to the early twenty-first century, engineering systems predominantly banked \non rule-based algorithms and traditional statistical methods for their monitoring and predictive needs. While \nthese techniques laid the foundation for system analytics, they often showed signs of strain when confronted \nwith complex scenarios.\nIn complex scenarios, rule-based algorithms and traditional statistical methods may have certain  limitations6. \nRule-based algorithms often rely on domain experts to define rules and features. This can limit the scalability \nand adaptability of the  algorithms4, especially in complex scenarios where domain experts’ knowledge may not \ncover all possible cases.Traditional statistical methods often make assumptions about the data distribution, such \nas the assumption of normal distribution. However, in complex scenarios, the data distribution may deviate from \nthese assumptions, leading to decreased accuracy of statistical methods. Rule-based algorithms and traditional \nstatistical methods typically assume linear relationships between data. However, in complex scenarios, the rela-\ntionships between data may be non-linear, limiting the predictive capabilities of these methods.\nThe current age, marked by dynamic and data-rich environments, only accentuates these strains. The Internet \nof Things (IoT), a revolutionary concept, has acted as a catalyst, causing an explosive surge in the volume, velocity, \nOPEN\n1Key Laboratory of Ethnic Language Intelligent Analysis and Security Governance of MOE, Minzu University of \nChina, No. 27 Zhongguancun South Avenue, Beijing 100081, China. 2Present address: School of Information and \nEngineering, Minzu University of China, Beijing 100081, China. *email: tozhangting@126.com\n2\nVol:.(1234567890)Scientific Reports |         (2024) 14:4890  | https://doi.org/10.1038/s41598-024-55483-x\nwww.nature.com/scientificreports/\nand variety of data generated by engineering  systems7,8. From nuanced sensor readings capturing the minutest \nof vibrations to extensive operational logs detailing system workflows, the spectrum of data is vast. This mosaic \nof information conceals within it intricate patterns, deep-seated temporal dependencies, and rich contextual \nnuances, elements that often elude classical analytical  methods9.\nAdditionally, the modern engineering landscape is characterized by its fluidity. Systems, environments, and \nrequirements evolve, demanding decision-making models that are not just accurate but agile. The rigidity of \ntraditional models, coupled with their need for periodic manual recalibration, makes them cumbersome and \nless effective in these dynamic environments.\nModern engineering systems require accurate predictive models to support decision-making and optimize \noperations. Traditional single-model approaches like LSTM and Transformer may not always meet the demand \nfor high accuracy predictions in certain scenarios. Modern engineering systems often face dynamic operating \nconditions and continuously changing data streams. Traditional batch learning methods are unable to adapt \nin real-time to such changes, resulting in delayed or inaccurate predictions. Engineering systems often require \npredictions and decision-making in real-time or near real-time environments. Traditional complex models may \nface challenges in terms of computational efficiency, leading to slow prediction speeds or excessive computational \nresource consumption.Therefore, there is a need for a new hybrid architecture to overcome these challenges and \nmeet the specific requirements of modern engineering systems.\nThe promise of deep learning\nAt this crossroads, deep learning emerges, offering a glimmer of hope. As a more evolved subset of machine \nlearning, deep learning ventures beyond the superficial layers of data, diving deep to extract patterns and insights. \nAmong the arsenal of deep learning tools, Long Short-Term Memory (LSTM)  networks3, a specialized breed of \nrecurrent neural networks (RNNs), have carved a niche for themselves (“long short-term memory\" (LSTM), a \nnovel recurrent network architecture in conjunction with an appropriate gradient-based learning algorithm. It \ncan learn to bridge time intervals in excess of 1000 steps even in case of noisy, incompressible input sequences, \nwithout loss of short time lag capabilities. This is achieved by ancient, gradient-based algorithm for an architec-\nture enforcing constant error flow through internal states of special units). Their unique architectural design, \nreplete with memory cells and meticulously crafted gates, bestows upon them the ability to recognize, capture, \nand retain long-term dependencies typical of time-series data. Such attributes render LSTMs an ideal candidate \nfor a plethora of engineering applications, especially those inundated with sensor data, operational logs, and \nother sequential  datasets10.\nY et, no tool is without its limitations. LSTMs, for all their prowess in sequence comprehension, occasionally \nexhibit vulnerabilities in situations that demand a holistic understanding of broader contextual information. \n(When using LSTM alone for prediction, there may be cases where the prediction results are inaccurate). This gap \nis addressed by the Transformer  architecture11,12, a prodigy in the deep learning domain. Equipped with potent \nself-attention mechanisms, Transformers possess the innate capability to weigh the relevance of different parts of \nan input sequence. This discernment allows them to fathom both granular and macro-level contexts with equal \n finesse13. (However, when dealing with long sequences, Transformer faces the challenge of high computational \ncomplexity, which can hinder its effectiveness in handling long sequences. If Transformer is used alone for pre-\ndiction, there may be issues of high computational complexity and slow prediction speed).\nOur contribution\nIn the intricate realm of engineering challenges and deep learning solutions, our research makes a substan-\ntial contribution by introducing a novel hybrid LSTM-Transformer architecture. This model is meticulously \ncrafted to address the specific requirements of modern engineering systems, particularly in the areas of smart \nmanufacturing and renewable energy management. Smart manufacturing and renewable energy management \nare highly significant fields in today’s society, with implications for improving production efficiency, reducing \nenergy consumption, and minimizing environmental impact. Therefore, researching predictive models in these \ntwo domains can provide valuable support for practical applications.Both smart manufacturing and renewable \nenergy management face complex data and operational conditions. In smart manufacturing, there are large \namounts of sensor data, optimization of production lines, and fault detection, among other  challenges 2. In \nrenewable energy management, factors such as weather variations and energy supply–demand balance need to \nbe considered. The complexity and challenges in these domains make researching predictive models even more \nmeaningful and  valuable5.\nFrom the data and algorithmic viewpoints, smart manufacturing and renewable energy management share \nsome commonalities and differences. Both domains involve a significant amount of time series data and sensor \ndata. These data often exhibit high dimensionality, high frequency, and complex interdependencies. Therefore, \nhandling these common data characteristics is a crucial challenge for predictive  models7. Smart manufacturing \nand renewable energy management differ in terms of data sources and characteristics. In smart manufacturing, \ndata primarily come from production lines, equipment, and sensors, involving production processes and qual-\nity control aspects. In renewable energy management, data mainly come from weather observations, energy \nproduction, and consumption. Therefore, designing appropriate predictive models and algorithms that account \nfor the specific characteristics of each domain is necessary.\nUnlike traditional models, our hybrid architecture excels in capturing both sequential patterns and broader \ncontextual information, thanks to the synergistic blend of LSTM’s memory cells and Transformer’s self-attention \nmechanisms. (LSTM’s memory units capture long-term dependencies, while Transformer’s self-attention mecha-\nnism comprehends fine-grained and macro-level contexts. This synergistic fusion enables our model to excel in \ncapturing sequence patterns and broader contextual information).\n3\nVol.:(0123456789)Scientific Reports |         (2024) 14:4890  | https://doi.org/10.1038/s41598-024-55483-x\nwww.nature.com/scientificreports/\nOur contributions extend beyond mere architecture design. We’ve implemented state-of-the-art online learn-\ning techniques that empower our model to adapt in real-time to dynamic operational conditions. This feature is \nparticularly crucial for applications that require immediate responsiveness to new data streams, such as real-time \nfault detection or energy usage optimization.\nOnline learning techniques allow our model to adapt in real-time to dynamic operating conditions. By con-\ntinuously receiving and processing new data while the model is already deployed and running, our model can \npromptly respond to changes in data streams and operating conditions, maintaining prediction accuracy and \nadaptability. Online learning techniques support incremental learning, where new data is used for incremental \ntraining on top of an existing model. This approach avoids the overhead of retraining the entire model, saving \ncomputational resources and time. Our model can be locally updated based on new data samples, gradually \nimproving prediction capabilities. Online learning techniques enable our model to adapt to evolving data dis-\ntributions and operating conditions. By monitoring the model’s performance and prediction results in real-time, \nwe can make adjustments and optimizations based on feedback information.\nMoreover, we’ve integrated knowledge distillation methods to harness insights from larger, more complex \nnetworks. This not only enhances the model’s predictive accuracy but also ensures computational efficiency, a \ndelicate but essential balance in real-time engineering applications.\nOur research is validated through extensive experiments on sector-specific engineering datasets, demon-\nstrating clear advantages over existing predictive models in terms of accuracy, adaptability, and computational \noverhead. Therefore, this work doesn’t just introduce a new model; it provides a comprehensive predictive solu-\ntion uniquely tailored for the multifaceted challenges posed by the evolving landscape of engineering systems.\nRelated work\nTraditional predictive models in engineering\nThe foundation of predictive modeling in engineering systems lies in traditional algorithms and statistical meth-\nodologies. These techniques, often rooted in deterministic principles, have been employed for decades to model \nand predict various engineering  phenomena5,8.\nStatistical models\nMethods like linear regression, logistic regression, and ARIMA have been the cornerstone for many early predic-\ntion tasks. They operate under specific assumptions about data distribution and often offer interpretable models. \nHowever, they typically struggle with nonlinearities and require manual feature engineering, which can be tedious \nand often lacks the finesse to capture intricate patterns in  data11.\nRule‑based systems\nThese are systems where domain knowledge is converted into a set of rules. Such systems are highly interpret -\nable and were widely used in scenarios where understanding the decision-making process is crucial. However, \ncrafting these rules requires extensive domain expertise, and the system’s rigidity often makes it less adaptable \nto dynamic  changes12,13.\nDeep learning in time‑series prediction\nWith the advent of deep learning, a paradigm shift occurred in predictive modeling. The capability of deep neural \nnetworks to automatically learn features from raw data has revolutionized the field.\nLSTM networks\nLSTMs, as recurrent neural networks, possess a unique architecture that allows them to remember past \n information14, making them adept at handling sequential data. Their application in various engineering domains, \nsuch as predicting the attitude and position of underground drilling machines, is a testament to their versatility \nand  efficacy14,15.\nConvolutional neural networks (CNNs)\nWhile CNNs are predominantly known for their prowess in image  data16, their ability to detect local patterns \nmakes them suitable for time-series data as well. Some recent studies have explored their utility in processing \nsequences, especially when combined with  LSTMs17.\nThe rise of the transformer architecture\nThe Transformer architecture has reshaped the landscape of deep learning, especially in the realm of sequence \n modeling16.\nSelf‑attention mechanism\nAt the heart of the Transformer architecture is the self-attention mechanism. By weighing the importance of \ndifferent parts of a sequence relative to each other, this mechanism offers a nuanced understanding of data, cap-\nturing both local and global contexts. This capability has made Transformers a valuable tool not just in language \ntasks but also in engineering applications demanding a broader comprehension of contextual  information10.\nScalability\nOne of the notable features of Transformers is their ability to process data in parallel, unlike RNNs, which operate \nsequentially. This characteristic makes them highly scalable and efficient for large  datasets17,18.\n4\nVol:.(1234567890)Scientific Reports |         (2024) 14:4890  | https://doi.org/10.1038/s41598-024-55483-x\nwww.nature.com/scientificreports/\nHybrid models and multi‑task learning\nThe growing complexity of engineering tasks and the increasing richness of data sources have motivated research-\ners to explore hybrid models that synergize the strengths of multiple neural network architectures.\nLSTM‑transformer combinations\nWhile LSTMs excel in capturing sequential relationships, Transformers shine in understanding broader contexts. \nSeveral pioneering works have started to investigate the potential benefits of combining these two  architectures16. \nFor instance, applications in underground drilling machine positioning have leveraged the sequential modeling \nprowess of LSTMs and enhanced it with the attention mechanisms from Transformers to achieve superior results.\nMulti‑task learning frameworks\nModern engineering systems often involve numerous interconnected tasks. Training separate models for each \ntask isn’t just computationally intensive but also fails to leverage the shared knowledge across tasks. Multi-task \nlearning frameworks have emerged as a solution, wherein a single unified model is trained across multiple related \ntasks. This not only leads to computational efficiency but often boosts performance, as tasks benefit from shared \nfeature  representations18,19.\nOnline learning and adaptive mechanisms\nThe dynamic nature of engineering environments necessitates models that can adapt in real-time to evolving \nconditions.\nOnline learning with LSTMs\nThe inherent structure of LSTMs, which allows them to retain and recall past information, makes them suitable \ncandidates for online learning. By continuously updating their parameters based on incoming data, they can \nadapt to changing conditions. Some recent studies, inspired by health diagnostics and motor status assessments, \nhave delved deep into the potential of LSTMs in online learning  scenarios10,16,18.\nAdaptive learning rates\nOne of the challenges in online learning is determining the rate at which the model adapts. Too fast, and it \nbecomes unstable; too slow, and it can’t keep up with the changes. Techniques to adjust learning rates adaptively, \nbased on the nature of incoming data, have been explored to strike a balance and ensure model stability and \nadaptability.\nKnowledge distillation in deep learning\nAs deep learning models grow in complexity and size, their computational demands also increase, often making \nthem impractical for real-time applications in resource-constrained engineering systems. Knowledge distillation \nemerges as a solution to this challenge.\nConcept of distillation\nKnowledge distillation involves training a smaller, more compact model (the student) using the knowledge \ngained by a larger, more complex model (the teacher). The primary aim is to transfer the essence of the teacher \nmodel’s knowledge to the student, ensuring that the student achieves comparable performance with reduced \ncomputational overhead.\nApplications in engineering\nGiven the real-time constraints of many engineering systems, especially those involving IoT devices, knowledge \ndistillation has found relevance. For instance, sophisticated models trained on vast datasets from green storm -\nwater infrastructures can be distilled into smaller models suitable for on-site, real-time  predictions19,20.\nAttention mechanisms beyond transformers\nWhile the Transformer architecture popularized the concept of attention, the idea of weighing different parts of \ninput data based on their relevance has been explored in various other  contexts21,22.\nAttention in LSTMs\nBefore Transformers took center stage, attention mechanisms were integrated with LSTMs to enhance their \ncapability to focus on relevant parts of sequences, especially in tasks like machine  translation23. Such mecha-\nnisms have also found applications in engineering tasks where specific segments of time-series data are more \ncritical than others.\nMulti‑head and hierarchical attention\nAs data sources grow more diverse, models need to focus on multiple aspects simultaneously. Multi-head atten-\ntion, where multiple attention patterns are learned concurrently, and hierarchical attention, which learns atten-\ntion at different granularities, have been explored to address such  complexities24.\n5\nVol.:(0123456789)Scientific Reports |         (2024) 14:4890  | https://doi.org/10.1038/s41598-024-55483-x\nwww.nature.com/scientificreports/\nMethods\nOur approach addresses the challenges in modern engineering systems by combining two established deep \nlearning architectures: long short-term memory (LSTM) networks and Transformers. This section provides a \nclear explanation of our hybrid model’s design and components, including the methods for online learning and \nknowledge distillation we have incorporated.\nHybrid LSTM‑transformer architecture\nOur architectural design is a testament to the philosophy of embracing the strengths of both worlds. Drawing \nfrom the temporal mastery of LSTMs and the contextual prowess of Transformers, we’ve envisioned an archi -\ntecture primed for the rigors of engineering systems. The schematic can be referred to Fig. 1:\nLSTM component\nStructure. The foundation of our model rests on the LSTM layer, the bastion of sequential data comprehension. \nEach LSTM unit, a marvel of architectural ingenuity, boasts a series of memory cells. These cells are adept at \ncapturing intricate temporal dynamics, ensuring the retention of pivotal historical data while remaining sensi-\ntive to new information.\nGating mechanism. The genius of LSTMs lies in their gating mechanisms. These neural gates, meticulously \ndesigned, are the gatekeepers of information flow within each unit.\nFigure 1.  Schematic of hybrid LSTM-transformer architecture.\n6\nVol:.(1234567890)Scientific Reports |         (2024) 14:4890  | https://doi.org/10.1038/s41598-024-55483-x\nwww.nature.com/scientificreports/\nThe input gate discerns and decides the quantum of fresh information to usher into the cell.\nThe forget gate, in its wisdom, either clings onto or lets go of historical memories, ensuring the cell remains \nuncluttered.\nThe output gate curates the information to be relayed forward, ensuring only the most pertinent insights are \npassed on.\nTogether, these gates empower the LSTM with a discerning judgment, ensuring a judicious blend of past \nwisdom and new insights.\nImplementation details. To enhance depth and richness, we employ a multi-layered LSTM structure. This \nmulti-tiered design ensures a nuanced understanding of sequential patterns at varying temporal scales.\nInterwoven within these LSTM layers are dropout mechanisms. These layers, by periodically deactivating a \nsubset of neurons, ensure that our model remains humble, preventing the hubris of overfitting and fostering a \nspirit of generalization.\nTransformer component\nStructure. Augmenting our LSTM layers is the Transformer component, the maestro of contextual compre-\nhension. This layer is a congregation of multiple self-attention heads, each vying to focus on varied facets of the \ndata sequence, ensuring a holistic understanding.\nSelf-attention mechanism. The Transformer’s heart beats with the rhythm of the self-attention mechanism. \nThis mechanism, through its intricate dance of ’ query’ , ’key’ , and ’value’ vectors, computes a weighted representa-\ntion of the sequence. As each data point struts on the sequence stage, the mechanism discerns the relevance of \nits peers, ensuring the spotlight shines on the most pertinent ones.\nPositional encoding. In standard Transformer architectures, the concept of sequence order or temporal posi-\ntion is not inherently understood. This can be a significant drawback when dealing with time-series data preva-\nlent in engineering systems, such as sensor readings over time or chronological event logs. To address this limita-\ntion, we introduce positional encodings into our hybrid architecture, endowing the Transformer layer with the \ncapability to recognize the temporal significance of each data point.\nMathematical implementation. The positional encodings are mathematically formulated using sine and cosine \nfunctions of different frequencies:\nHere, PE(pos ,i) represents the positional encoding at position pos for dimension i , and d is the dimensionality \nof the embeddings. These mathematical functions generate unique positional encodings for each time step in \nthe sequence, which are then added to the original embeddings before feeding them into the Transformer layer.\nReal-world application. In practical engineering scenarios like predictive maintenance or real-time monitor -\ning, the sequence of events or sensor readings can be critical. With the introduction of positional encodings, our \nTransformer layer can now recognize patterns like rising temperature followed by an increase in vibration levels \nas a sign of potential equipment failure.\nThis detailed inclusion of positional encodings ensures that our hybrid model is not only adept at understand-\ning the intricacies of the data but also aware of the sequence in which these intricacies unfold, making it highly \napplicable for time-sensitive engineering tasks.\nHybrid LSTM‑transformer model pseudo‑code\nTo provide a clearer understanding of our hybrid model, we present a simplified pseudo-code representation:\n(1)PE (pos,2i) = sin\n( pos\n10000\n2i\nd\n)\n,\n(2)PE (pos,2i + 1) = cos\n( pos\n10000\n2i\nd\n)\n.\n7\nVol.:(0123456789)Scientific Reports |         (2024) 14:4890  | https://doi.org/10.1038/s41598-024-55483-x\nwww.nature.com/scientificreports/\nInitialize: LSTM layers, Transformer layers, Positional Encodings\nFor each input sequence:\n# LSTM processing\nFor each LSTM layer:\nApply LSTM cell processing\nApply Dropout\n# Transformer processing\nApply Positional Encoding to LSTM output\nFor each Transformer layer:\nApply Self-Attention\nApply Feedforward Neural Network\nApply Normalization and Dropout\n# Output processing\nCompute final output from Transformer layer\nApply Online Learning updates \nEnd\nThis pseudo-code outlines the core steps in our model’s processing pipeline, from initial input through LSTM \nlayers, followed by Transformer layers with positional encoding, and concluding with output generation. It \nunderscores the integration of LSTM and Transformer components, along with online learning updates.\nImplementation details\nTo implement our hybrid LSTM-Transformer architecture, we primarily utilized TensorFlow (version 2.15.0) and \nPyTorch (version 2.1.1 + cu121) frameworks, leveraging their robust and efficient deep learning capabilities. The \nLSTM components were implemented using standard LSTM units available in these frameworks, customized for \nour specific requirements in terms of layer depth and dropout rates. Similarly, the Transformer components were \nbuilt upon the standard Transformer model implementations provided by these libraries, with modifications to \nintegrate positional encoding and self-attention mechanisms tailored for our engineering datasets.\nAdditionally, for specific components such as online learning updates and knowledge distillation processes, we \ndeveloped custom algorithms. These algorithms were designed to seamlessly integrate with the aforementioned \nframeworks, ensuring a cohesive and efficient learning process.\nOnline learning mechanism\nOnline learning in the context of dynamic engineering systems is pivotal. The very essence of these systems \ndemands models that are agile, adaptable, and always in sync with the evolving data landscape. Online learn-\ning, a departure from traditional batch training, embodies these qualities, ensuring models are always at the \nforefront of knowledge.\nIncremental model updates\nConcept. Traditional models, once trained, are static entities. Their knowledge is frozen in time, making them \nill-equipped to handle the fluidity of real-world engineering scenarios. Our model, however, is different. It \nbelieves in continuous learning, constantly evolving and refining its knowledge.\nImplementation details. Mini‑batch gradient descent: Equation: Given a loss function L, the parameter update \nrule using gradient descent is:\nwhere θ represents the model parameters, η is the learning rate, and ∇ L(θt) is the gradient of the loss function \nwith respect to the parameters.\nWe segment our data into mini-batches. For each batch, we compute the gradient and update our model \nparameters incrementally, ensuring the model is always in tune with the latest data.\nBatch normalization. Equation: The normalized output /Lambda1\nx is given by:\nwhere x is the input, μ  is the mean of the input, σ 2 is its variance, and ε is a small constant to prevent division \nby zero.\n(3)θt+1 = θt −η∇L (θt),\n(4)\n�\nx = x − µ√\nσ2 +ε\n,\n8\nVol:.(1234567890)Scientific Reports |         (2024) 14:4890  | https://doi.org/10.1038/s41598-024-55483-x\nwww.nature.com/scientificreports/\nBatch normalization layers are interspersed within our network. These layers adjust and scale the activations, \nensuring consistent distribution and aiding in stable and faster convergence.\nMemory replay. To ensure the model retains its knowledge of past data, we employ a memory buffer. This \nbuffer, a repository of past experiences, occasionally replays old data alongside new data, ensuring the model \nremains grounded in its past learnings while embracing new knowledge.\nAdaptive learning rates\nRationale. The unpredictable terrains of engineering systems demand adaptability in every facet, including \nthe rate at which models learn. A static learning rate can either lag, missing out on critical changes, or oscillate, \ncausing instability.\nImplementation details. Adam optimizer: Equation: The Adam update rule is:\nwhere  gt is the gradient at time t,  mt and  vt are estimates of the first moment and second moment of the gradients \nrespectively, and β1,β2 are exponential decay rates.\nAdam dynamically adjusts the learning rate for each parameter. It does so by maintaining a moving average \nof past gradients and their square values, ensuring swift yet stable learning.\nLearning rate annealing: Equation: The annealed learning rate ηt is:\nwhere η0 is the initial learning rate, δ is the decay rate, and t is the current epoch.\nAs training progresses, we gradually reduce the learning rate. This ensures a balance between rapid learning \nin the initial stages and fine-tuning in the later stages.\nGradient clipping: Equation: The clipped gradient g′ is:\nwhere g is the computed gradient and δ is the threshold.\nIn scenarios where gradients can grow uncontrollably, we ensure they are capped within a predefined thresh-\nold, ensuring stability and preventing divergence.\nKnowledge distillation\nKnowledge distillation is a technique where a compact model (student) is trained to mimic the behavior of a \nlarger, more complex model (teacher). This allows the student model to inherit the teacher’s capabilities without \nincurring the computational overhead. The schematic can be referred to Fig. 2:\nRationale behind distillation\nConcept. In many engineering scenarios, deploying gargantuan models is infeasible due to resource con-\nstraints. However, these large models often possess superior performance. Knowledge distillation bridges this \ngap, enabling smaller models to emulate the performance of their larger counterparts.\nAdvantages. \n• Efficiency: Reduced model size ensures faster inference times and lower memory footprint.\n• Performance: The student model, though compact, can achieve performance metrics close to the teacher \nmodel.\nDistillation process and softmax loss calculation\nSoftened outputs. In the process of knowledge distillation, the teacher model’s outputs are ’softened’ by adjust-\ning the softmax temperature. This is done to create a more informative output distribution, which is crucial for \ntransferring the teacher’s knowledge to the student model.\nSoftmax loss calculation: The softmax loss, also known as the cross-entropy loss between the teacher’s sof-\ntened outputs and the student’s predictions, plays a pivotal role in knowledge distillation. We handle this loss \ncalculation as follows:\nThe softmax function with temperature scaling is applied to both the teacher’s and student’s logits, generating \nsoftened probability distributions.\nThe cross-entropy loss is then computed between these two distributions. This loss quantifies the difference \nbetween the teacher’s guidance and the student’s current understanding.\n(5)\nm t = β1m t−1 + (1 − β1)gt\nνt = β2νt−1 + (1 − β2)g2\nt\nθt+1 = θt − η m t√νt+ε\n(6)ηt = η0 × 1\n1 + δt\n(7)g′ = δg\ng\n9\nVol.:(0123456789)Scientific Reports |         (2024) 14:4890  | https://doi.org/10.1038/s41598-024-55483-x\nwww.nature.com/scientificreports/\nThis cross-entropy loss is combined with the traditional loss function to form the final loss function for train-\ning the student model. This combined loss function guides the student to not only mimic the teacher’s output \ndistribution but also to align correctly with the ground truth.\nThe choice of temperature T in the softmax function is crucial. A higher T produces softer probability dis -\ntributions, encouraging the student model to learn the nuanced relationships captured by the teacher. However, \ntoo high a value of T can lead to an overly smoothed distribution, which might be less informative. Therefore, we \nempirically determine the optimal value of T through a series of experiments, aiming to find the right balance \nfor effective knowledge transfer.\nEquations:\nSoftmax function with temperature scaling:\nwhere zi is the logit for class i and T is the temperature.\nLoss function: The distillation loss is a combination of the traditional loss (e.g. cross-entropy with true labels) \nand a distillation term that measures the divergence between the student’s and teacher’s softened outputs.\ny is the true label,F(x) is the student’s output,G(x) is the teacher’s output,α is a weight factor.T is the temperature.\nThe first term CrossEntropy(y, F(x)) is the traditional cross-entropy loss between the true labels and the student’s \npredictions.The second term T2 × KL-Divergence(G(x)/T, F(x)/T) is the distillation term, which measures the \ndivergence (using KL-Divergence) between the student’s and teacher’s softened outputs. The factor T2 is there to \nscale the gradients correctly when using softened probabilities.\n(8)Soft max TemperatureOutput = exp(zi/T )∑\nj\nexp(zj/T ) ,\n(9)L = (1 − α) × Cross Entropy((y, F(x)) + α × T 2 × KL − Divergence(G (x)/T , F(x)/T ))\nFigure 2.  Schematic of knowledge distillation.\n10\nVol:.(1234567890)Scientific Reports |         (2024) 14:4890  | https://doi.org/10.1038/s41598-024-55483-x\nwww.nature.com/scientificreports/\nTraining the student. The student model is trained using the combined loss, which nudges it to not only be \ncorrect with respect to the ground truth but also to align its output distribution with the teacher.\nOur implementation\nTeacher model. Design philosophy: The teacher model is constructed with an emphasis on depth and capacity, \nenabling it to extract intricate patterns from extensive engineering datasets. Given its larger size, it’s expected to \ncapture even the subtle nuances of data.\nArchitecture. \n• LSTM layers: Multiple stacked LSTM layers allow the teacher model to thoroughly understand temporal \ndependencies in the data.\n• Transformer blocks: Several transformer blocks with multiple self-attention heads enable the model to capture \nboth localized and global contexts.\nTraining: The teacher model is trained using a hybrid approach that combines labeled datasets with unsu-\npervised auxiliary tasks. Its large-scale design benefits significantly from extensive datasets, enabling better \ngeneralization across varied engineering applications.\nStudent model. Design philosophy: The student, while compact, is designed to be a fast learner, able to grasp the \nessence of the teacher’s knowledge.\nArchitecture: While the student model shares the hybrid LSTM-Transformer architecture described previously, \nit operates with fewer layers and attention heads, ensuring agility.\nDistillation training: Training the student involves feeding it the same input data as the teacher. However, \ninstead of solely relying on ground-truth labels, the student is also guided by the teacher’s outputs. This dual \nguidance ensures that the student model, while being lean, punches well above its weight in terms of performance.\nRegularization. Concept: Over-reliance on the teacher’s outputs can lead to the student model not truly under-\nstanding the underlying patterns. Regularization ensures the student also focuses on raw data.\nImplementation details:\n• L1 and L2 regularization: These are added to the model’s loss function. They penalize overly complex models, \nensuring the student remains general and does not overfit to the teacher’s outputs.\n• Dropout: Introduced in between layers, dropout ensures that during training, random subsets of neurons \nare turned off, promoting model robustness and preventing co-adaptation of neurons.\nCombined loss: The final loss function for the student model is a composite of the ground-truth loss, teacher-\nguided loss, and regularization terms. This multi-faceted loss function ensures a balanced and effective learning \nprocess, making the student model robust and versatile for a wide range of engineering applications.\nAdaptive mechanisms for robust performance optimization\nEngineering systems are complex, and their dynamic nature requires models to be not only accurate but also \nadaptable and efficient. Our adaptive mechanisms are designed to cater to these necessities.\nData augmentation\nConcept. Data augmentation is a pivotal strategy in deep learning, especially when training data is scarce or \nwhen the model needs to generalize across varied scenarios. By artificially introducing minor modifications to \nthe original data, we can simulate a richer training environment.\nImplementation details. Time warping: This technique is instrumental for time-series data. Altering the time \nscale ensures the model remains resilient to fluctuations in data generation rates. Mathematically, time warping \ncan be represented as:\nHere, δ(t) introduces a controlled distortion, ensuring the model learns patterns across various time scales.\nFeature jittering: Real-world data often comes with noise. By simulating this during training, we ensure our \nmodel remains robust even in less-than-ideal conditions:\nThe term ε is a controlled random noise, usually drawn from a Gaussian distribution, reflecting typical sensor \nnoise or environmental perturbations.\nAdaptive model pruning\nConcept. Large neural networks, while powerful, can be computationally intensive. Pruning helps streamline \nthese models, removing redundant parts without compromising performance.\n(10)X w arped(t) = X (T + δ(t)).\n(11)X jittered(t) = X (t) + ε.\n11\nVol.:(0123456789)Scientific Reports |         (2024) 14:4890  | https://doi.org/10.1038/s41598-024-55483-x\nwww.nature.com/scientificreports/\nImplementation details. Importance estimation: To determine which neurons or layers to prune, we first evalu-\nate their importance. This is done using techniques like the Taylor expansion:\nNeurons causing minimal changes in the loss function, L, are deemed less important.\nThresholding & pruning: After ranking neurons based on importance, those below a certain threshold are \nremoved. The network is then fine-tuned to adjust to these structural changes, ensuring performance remains \noptimal.\nFeedback loop for continuous improvement\nConcept. In dynamic systems, a model’s past mistakes can be invaluable for future accuracy. By incorporating \na feedback loop, the model refines itself based on its historical performance.\nImplementation details. Error analysis: After predictions, the model’s errors are computed:\nThese errors provide insights into where the model might be lacking.\nBackpropagation with feedback: Errors are then fed back into the model. Using backpropagation, the model \nadjusts its weights to minimize these errors in future predictions, effectively learning from its mistakes.\nEnsemble techniques for reliability\nConcept. An ensemble of models often outperforms individual models due to the diversity in their predic-\ntions, enhancing robustness and reliability.\nImplementation details. \n• Model variants: We maintain several instances of our hybrid model. Each instance might differ slightly in \nterms of initialization, training data splits, or hyperparameters.\n• Voting mechanism: When predicting:\nBy aggregating predictions from all instances, the ensemble minimizes biases and errors inherent in individual \nmodels, producing a more reliable output.\nKnowledge distillation for enhanced efficiency: a deep dive\nIn the context of our study aimed at multi-task real-time predictions in engineering systems, knowledge distilla-\ntion serves as a powerful technique for transferring rich features and predictive capabilities from a computation-\nally intensive teacher model to a lighter, more agile student model.\nKnowledge transfer: more than meets the eye\nConceptual overview. Knowledge distillation is more than transferring class probabilities; it’s about imbuing \nthe student model with the teacher’s understanding of complex relationships between sensor data, temporal \npatterns, and system states in engineering environments. This is crucial for tasks like predictive maintenance or \nreal-time quality control in manufacturing lines.\nSoft Target probabilities: the essence of distillation\nWhy soft targets? Direct labels, often termed ’hard labels’ , offer a binary perspective. In contrast, soft labels, \nemanating from the teacher’s predictions, provide a spectrum of possibilities. These gradients of certainty offer \na more detailed roadmap for the student model to learn.\nImplementation details. Temperature-Scaled Softmax: By adjusting the temperature T in the softmax function, \nthe model’s predictions become \"softer\". This softening is crucial, as it offers gradients of understanding, allowing \nthe student to grasp the intricacies of different data points.\nCrafting the distillation objective: a deep dive into engineering‑specific learning goals\nThe distillation objective serves as the cornerstone for how well the student model learns from its teacher, par -\nticularly in executing multi-task real-time predictions in engineering systems. Here, we delve deeper into the \nkey components and considerations tailored for engineering applications.This section is structured into three \nmain sub-sections for clarity.\n(12)/Delta1L=∇ L · /Delta1w,\n(13)e(t) = Y true(t) − Y predicted(t).\n(13)Y ﬁnal (t) = 1\nN\nN∑\ni=1\nY i(t).\n12\nVol:.(1234567890)Scientific Reports |         (2024) 14:4890  | https://doi.org/10.1038/s41598-024-55483-x\nwww.nature.com/scientificreports/\nSub-section \"Introduction\": dual guidance in engineering systems. Objective: balance the teacher model’s wis-\ndom and ground truth labels to achieve both predictive accuracy and computational efficiency in real-time \nscenarios.\n• Lambda parameter\nRole: Balances the contributions of teacher model and ground truth.\nApplication: In energy management systems, fine-tune λ to weigh real-time sensor data and historical \npatterns appropriately.\nSub-section \"Related work\": fine-tuning with auxiliary tasks. Objective: Enrich the student model’s learning by \nintroducing additional engineering-specific tasks.\n• Task examples\nPrimary Task: Predict mechanical failures.\nAuxiliary Tasks: Predict component wear and tear, estimate energy efficiency.\n• Task weights\nRole: Fine-tune the influence of each auxiliary task.\nApplication: Dynamically adjust weights based on real-time performance metrics of the engineering sys-\ntem.\nSub-section \"Methods\": Overfitting mitigation techniques. Objective: Ensure that the student model is robust \nenough to handle the variety and scale of engineering tasks without overfitting.\n• Dropout Layers\nRole: Prevent overfitting.\nApplication: Place strategically after layers prone to overfitting, especially vital for real-time automated \ncontrol systems.\n• Noise Injection\nRole: Add robustness.\nApplication: Inject noise that mimics engineering-specific uncertainties like sensor errors to maintain \nrobust performance.\nUnraveling the potential and pitfalls\nAdvantages. Compactness coupled with performance : After undergoing the distillation process, the student \nmodel becomes an epitome of computational efficiency. This is especially critical in the engineering systems we \nfocus on—underground drilling and green stormwater management—where the need for real-time decision-\nmaking is paramount. Our distilled student model fits perfectly within these constraints, offering high predictive \naccuracy without burdening the system with computational overhead.\nReal‑time adaptability: The student model demonstrates unparalleled adaptability, a feature inherited from \nthe teacher model’s nuanced outputs and further enriched by our architecture’s online learning mechanisms. In \nthe domain of smart manufacturing and renewable energy management, this adaptability translates into more \nreliable predictive maintenance and energy optimization strategies, thereby ensuring operational excellence.\nChallenges. Dependence on teacher model quality: One of the most potent challenges is that the quality of the \ndistilled student model is closely tied to the teacher model’s performance. In our architecture, the teacher model \nis a deep network trained on sector-specific engineering datasets, including data from underground drilling \nmachines and green stormwater infrastructures. If the teacher model misinterprets these complex data sets, this \nlimitation will propagate to the student model, potentially undermining the system’s safety or efficiency.\nBalancing the combined loss function: Another significant challenge is the art of fine-tuning the combined loss \nfunction during the student model’s training. In our research, this loss function includes both the ground-truth \nlabels and the teacher model’s soft labels. Achieving the right balance is more than a theoretical challenge; it’s \nan operational necessity for our target applications. An improperly balanced loss function could compromise \nthe real-time fault detection in underground drilling or lead to inefficient stormwater management strategies.\n13\nVol.:(0123456789)Scientific Reports |         (2024) 14:4890  | https://doi.org/10.1038/s41598-024-55483-x\nwww.nature.com/scientificreports/\nBy deeply understanding these advantages and challenges, we further refine our pioneering LSTM-Trans-\nformer architecture. Our model doesn’t just offer a new predictive framework; it provides a comprehensive, \nefficient, and adaptable solution for the unique challenges posed by modern engineering systems, particularly \nin the areas of underground drilling and green stormwater management. Through extensive experimentation \nand validation, we demonstrate that our architecture significantly outperforms existing solutions, making it an \ninvaluable tool for future engineering applications.\nFurther insights into knowledge distillation\nMulti-task coherence. Our hybrid LSTM-Transformer architecture uniquely benefits from knowledge distil-\nlation by enhancing multi-task coherence, ensuring balanced performance across varied engineering tasks like \nfault detection and energy optimization.\nSpecialization risks. Distillation may yield a student model overly specialized to the teacher’s capabilities. This \nposes a risk in adapting to unforeseen changes in engineering systems, potentially limiting the model’s gener -\nalization ability.\nAdaptation lag.  In dynamic engineering settings requiring immediate responsiveness, the student model \nmay exhibit a slight adaptation lag compared to traditionally trained models, affecting operational safety and \nefficiency.\nSecurity concerns. Transferring knowledge from a teacher to a student model can introduce security risks, \nespecially if the teacher model has been trained on proprietary engineering data.\nInterpretability. Distillation may complicate model interpretability, a critical aspect in engineering systems for \nsafety or regulatory compliance.\nIn summary, while our knowledge distillation approach amplifies the model’s efficiency and adaptability, it \nalso introduces challenges that warrant careful consideration, especially in complex engineering applications.\nAdaptive mechanisms: augmenting model robustness\nIn the intricate environment of engineering systems, where operational conditions are highly volatile, static \nmodels risk becoming rapidly obsolete. To address this, our LSTM-Transformer hybrid model incorporates \nadvanced adaptive mechanisms tailored for the specific challenges of sectors such as underground drilling and \ngreen stormwater management.\nTemporal attention for selective focus\nNeed and impact. Engineering systems generate data with varying temporal significance. Distinguishing cru-\ncial timestamps from noise-rich periods is essential for predictive accuracy.\nImplementation. Our model incorporates a temporal attention mechanism, which assigns weights to different \ntimestamps based on their significance. This mechanism is achieved through:\nAttention scores: For each timestamp, an attention score is computed, reflecting its significance.\nWeighted summation: The model’s final output is then a weighted combination of the outputs at all timestamps, \nguided by their respective attention scores.\nFeedback‑driven learning\nConceptual overview. In real-world engineering systems, after a prediction is made, the true outcome eventu-\nally becomes observable. This feedback can be a valuable learning resource.\nImplementation. Post-prediction, when the true outcome is observed, our model computes the prediction \nerror. This error is then fed back into the model, guiding subsequent predictions. It’s a closed-loop system where \nthe model continually refines itself based on its past performance.\nContextual embeddings\nWhy context matters. Data in engineering systems doesn’t exist in isolation. It’s invariably influenced by the \nsurrounding context, be it other system variables, external factors, or broader operational settings.\nImplementation. Our model is equipped to ingest not just the raw data but also its associated context. Contex-\ntual embeddings, which are dense vector representations encapsulating this context, are fused with the primary \ndata inputs. This ensures that the model’s predictions are not just based on historical patterns but are also con-\ntextually aware.\nModel elasticity: scaling with complexity\nThe need for elasticity. Engineering challenges come in varied scales. Some systems might have a handful of \nsensors, while others could have thousands. Some might operate in near-constant settings, while others could be \nsubject to wide operational swings. A one-size-fits-all model approach can be suboptimal.\n14\nVol:.(1234567890)Scientific Reports |         (2024) 14:4890  | https://doi.org/10.1038/s41598-024-55483-x\nwww.nature.com/scientificreports/\nImplementation. Our model’s architecture is inherently elastic. Depending on the complexity of the system at \nhand, the model can scale up (adding more layers, neurons, or attention heads) or scale down. This ensures that \nit remains computationally efficient without compromising on performance.\nThe adaptive mechanisms detailed above ensure that our hybrid LSTM-Transformer model remains attuned \nto the ever-evolving intricacies of engineering systems. By being attentive, feedback-driven, context-aware, and \nelastically scalable, the model stands poised to deliver consistently high performance across diverse scenarios.\nModel evaluation and validation: benchmarks and metrics\nTo validate the efficacy of our hybrid LSTM-Transformer model, especially with the integrated online learning \nand adaptive mechanisms, we embarked on a rigorous evaluation journey. This section delves deep into the \nmethodologies, benchmarks, and metrics employed to ensure a holistic assessment.\nBenchmark models: a justified selection for engineering systems\nSelection rationale. The choice of benchmark models serves as a cornerstone for any empirical study. For our \nmodel, specifically designed to tackle the complexities of engineering systems like underground drilling and \ngreen stormwater management, benchmarks offer more than just a comparison—they provide a multifaceted \nlens through which the model’s merits and shortcomings can be scrutinized.\nList of benchmark models. \n• Classical time series models (ARIMA, holt-winters):\nWhy important: These models have been the cornerstone of time-series analysis in engineering for years. \nThey serve as a base level against which the advancements of any new model can be measured.\nNecessity: To demonstrate that our model can not only compete with but surpass traditional methods in \npredictive accuracy and efficiency, especially for engineering tasks.\n• Basic neural networks (feedforward, vanilla RNNs):\nWhy important: These models represent the transition from classical methods to neural network-based \napproaches. They offer a simplistic yet effective way to handle non-linearities.\nNecessity: To show that the added complexity and features of our hybrid model yield tangible benefits over \nbasic neural architectures, justifying the choice of a more complex model for engineering applications.\n• Advanced deep learning models (LSTM, transformer, engineering-specific LSTMs):\nWhy important: These are the pinnacles of deep learning research and have been applied to complex \nengineering tasks. They serve as a direct competitor to our LSTM-Transformer hybrid.\nNecessity: To establish that our model not only matches but excels in areas where these state-of-the-art \nmodels might falter, particularly in real-time adaptability and computational efficiency.\nBy choosing benchmarks that span the spectrum of model complexity and application history, we ensure \na comprehensive evaluation. This enables us to rigorously assess our LSTM-Transformer hybrid model’s per -\nformance in the context of engineering systems, thereby providing actionable insights into its utility and \neffectiveness.\nDatasets and pre‑processing\nEngineering datasets. For a model tailored to engineering systems, it’s imperative to evaluate it on representa-\ntive datasets. We sourced datasets from various engineering domains, some inspired by recent studies on under-\nground drilling machines and green stormwater infrastructure, as mentioned in the abstract.\nPre-processing. Given the heterogeneity of engineering data, meticulous pre-processing was undertaken. This \nincluded normalization, handling missing values, and segmenting the data into training, validation, and test sets.\nEvaluation metrics\nTo ensure a comprehensive assessment, multiple evaluation metrics were employed:\nMean absolute error (MAE): Represents the average absolute difference between predicted and actual values.\nRoot mean square error (RMSE): Offers insights into the model’s performance on outliers, given its sensitiv-\nity to large errors.\nMean absolute percentage error (MAPE): Provides a scale-independent error metric.\nF1 score and precision-recall (for classification tasks): Given that some engineering tasks might be binary or \nmulti-class classification, these metrics gauge the model’s classification prowess.\n15\nVol.:(0123456789)Scientific Reports |         (2024) 14:4890  | https://doi.org/10.1038/s41598-024-55483-x\nwww.nature.com/scientificreports/\nHyperparameter tuning and model robustness\nTuning approach. Given the myriad of hyperparameters influencing our model’s performance, we employed a \ncombination of grid search and Bayesian optimization to identify the optimal configuration.\nRobustness evaluation. To ascertain the model’s robustness, we introduced artificial noise and anomalies in \nthe test datasets. The model’s performance under these perturbed conditions offered insights into its resilience \nand reliability.\nThe evaluation and validation phase, outlined above, was instrumental in fine-tuning our model, identifying \nareas of improvement, and ultimately corroborating its superiority over existing state-of-the-art techniques. \nThe subsequent sections will delve into the results, offering both quantitative and qualitative insights into the \nmodel’s performance.\nEthical approval\nThis study was conducted in strict accordance with the ethical standards and guidelines established by the \nScientific Reports journal. All experimental procedures and data collection processes were conducted in full \ncompliance with the institutional guidelines and adhered to the principles outlined in the 1964 Helsinki Declara-\ntion and its subsequent amendments, as well as other comparable ethical standards relevant to our research field. \nAppropriate ethical approvals and informed consent, where applicable, were obtained for all aspects of this study.\nExperiments and results analysis\nThe experimental phase is instrumental in validating the theoretical strengths of our proposed model. We tested \nour hybrid LSTM-Transformer model across various representative engineering datasets, pitting it against estab-\nlished benchmarks. This section delves into the experimental setup, presents the results in a structured manner, \nand furnishes a detailed analysis of the findings.\nExperimental setup\nThe foundation of any empirical study rests on its experimental setup. Properly designed experiments ensure the \nvalidity and reliability of the results. Our comprehensive setup was crafted with this principle in mind.\nDatasets\nDataset A. Underground drilling machines data.\nNature: Time-series data.\nDuration: Data collected over a span of 24 months.\nFrequency: Readings recorded every 10 s.\nMetrics: Torque, drill bit RPM, pressure, temperature, ground resistance, and vibration levels.\nVolume: Approximately 6.3 million data points.\nPre-processing: Data was cleaned to remove outliers and missing values were imputed using interpolation. \nIt was then normalized for scale invariance.\nDataset B. Green Stormwater Infrastructure Data.\nNature: Time-series with occasional cyclic patterns.\nDuration: 18 months of data collection.\nFrequency: Measurements taken every 30 s.\nMetrics: Water flow rates, sediment levels, chemical concentrations, pH level, and turbidity.\nVolume: Roughly 1.5 million data points.\nPre-processing: Seasonal decomposition was employed to separate cyclic patterns. Data normalization was \nperformed to maintain a consistent scale.\nDue to the extensiveness of the dataset and article length limitations, a comprehensive visual representation \nof the data is included in the Appendix. Figures 3 and 4 illustrate time series for underground drill data (Dataset \nA) and green stormwater infrastructure data (Dataset B), respectively. These graphs illustrate subsets of the data, \ncapturing the temporal dynamics and variability inherent in the recorded metrics. Each plot contrasts actual \nvalues against the context of engineering features, highlighting trends and patterns that our hybrid LSTM-\nTransformer model expertly captures and predicts. This qualitative visualization complements the quantitative \nresults presented in Section \"Quantitative results and analysis\" and highlights the effectiveness of our proposed \napproach in adapting to the complex characteristics of engineering data.\nFor a complete visual analysis, see Figs. 3 and 4 in the Appendix, where the dataset is plotted over a repre -\nsentative sampling period. The data points in these graphs reflect the structure and nature of our actual data set, \nalthough they are scaled down for illustration purposes.\nBenchmarks\nFor a holistic assessment, it’s crucial to compare our model against both classical and contemporary forecasting \nmethods. The selected benchmarks are:\nARIMA. A classical forecasting method known for its capability to handle autoregressive and moving average \ncomponents.\nHolt-winters. Efficient for datasets with seasonality and trend components.\n16\nVol:.(1234567890)Scientific Reports |         (2024) 14:4890  | https://doi.org/10.1038/s41598-024-55483-x\nwww.nature.com/scientificreports/\nFeedforward neural network. A basic deep learning model.\nStandalone LSTM:. Captures long-term dependencies in time-series data.\nStandalone transformer. Offers contextual understanding through self-attention mechanisms.\nEach benchmark model, including the Feedforward Neural Network, was tuned for optimal hyperparam -\neters. This tuning process was carried out using a systematic approach that combined grid search and random \nsearch methods. The grid search was used to methodically explore a predefined grid of hyperparameters, while \nthe random search allowed us to probe a broader range of parameter values. This dual approach ensured that \nthe hyperparameter space was thoroughly explored, balancing the trade-off between model performance and \ncomputational efficiency.\nEvaluation metrics\nTo ensure a comprehensive evaluation, multiple metrics were chosen:\nMean absolute error (MAE). Provides a direct measure of prediction accuracy.\nRoot mean square error (RMSE). Emphasizes large errors in predictions.\nMean absolute percentage error (MAPE). Offers a scale-independent metric of prediction accuracy.\nF1 score: used for classification tasks, it provides a balance between precision and recall.\nComputational environment\nHardware. The experiments were conducted on a cluster with NVIDIA Tesla V100 GPUs, 128 GB RAM, and \nIntel Xeon Platinum 8180 CPUs.\nSoftware. The models were implemented using the TensorFlow and PyTorch deep learning frameworks. All \ncomputations were performed under a Linux environment.\nParallelism. Model training leveraged data parallelism across multiple GPUs to expedite the process.\nWith the experimental setup meticulously designed, we proceeded to the actual experiments and subsequent \nanalysis, as detailed in the subsequent sections.\nQuantitative results and analysis\nBuilding upon our earlier results, we further delve into the intricacies of our findings. The performance metrics \nalone, while indicative, do not provide the entire picture. Hence, this section presents a detailed analysis, sup -\nported by visual representations, to give a comprehensive understanding of our model’s performance vis-à-vis \nthe benchmark models.\nPerformance on dataset A: underground drilling machines data\nThe results are shown in Table 1 below:\nAnalysis. ARIMA & Holt‑winters: These classical models, while competent, exhibit a slightly reduced ability \nto adapt to the rapid changes in the drilling machine data. Their performance can be attributed to the inherent \nautoregressive nature of the data but falls short when the data has sharp fluctuations.\nFeedforward neural network: The FNN demonstrates better performance than classical models. The inherent \nnon-linearity introduced by the activation functions enables it to capture more complex patterns. However, it \nfails to tap into the sequential nature of the data effectively.\nStandalone LSTM & transformer: Both these models perform commendably, thanks to their specialized archi-\ntectures. The LSTM’s ability to remember long-term dependencies and the Transformer’s capacity to recognize \ncontextual significance play a crucial role.\nTable 1.  Performance on dataset A.\nModel MAE RMSE MAPE (%)\nARIMA 9.32 11.45 5.6\nHolt-winters 9.01 11.12 5.3\nFeedforward neural network 8.21 10.03 4.9\nStandalone LSTM 7.45 8.89 4.2\nStandalone transformer 7.01 8.55 4\nOur model 6.55 7.8 3.7\n17\nVol.:(0123456789)Scientific Reports |         (2024) 14:4890  | https://doi.org/10.1038/s41598-024-55483-x\nwww.nature.com/scientificreports/\nOur model : Outperforming all benchmarks, our hybrid model truly shines. By leveraging both sequen -\ntial understanding (from LSTMs) and contextual insights (from Transformers), it achieves the best predictive \naccuracy.\nPerformance on dataset B: green stormwater infrastructure data\nThe results are shown in Table 2 below:\nAnalysis. ARIMA & Holt‑winters: Their performance on this dataset is slightly better than on Dataset A. This \ncan be attributed to the cyclical patterns in the stormwater data, which these models are adept at capturing.\nFeedforward neural network: Its performance is consistent, but the lack of sequential modeling capabilities \nis evident in its slightly higher errors.\nStandalone LSTM & transformer: Their performances underscore their abilities. The LSTM model, in par -\nticular, does well with the cyclical nature of the data, while the Transformer aids in capturing sudden changes.\nOur model: Its supremacy is evident again. The hybrid nature, coupled with online learning and knowledge \ndistillation, allows it to adapt and predict with superior accuracy.\nIn the subsequent sections, we’ll delve deeper into the qualitative aspects, robustness analysis, and insights \nderived from the experiments.\nRobustness to data sparsity\nIn many real-world scenarios, especially in remote engineering setups, data might be sparse due to intermittent \nconnectivity, sensor failures, or deliberate downsampling for efficiency. Here, we assess the performance of our \nmodel when faced with missing data or reduced data granularity.\nExperiment setup. We simulated data sparsity by systematically removing portions of the data: 10%, 20%, up \nto 50%.\nThe model was then tested on this sparse dataset while being compared to its performance on the complete \ndataset.\nThe results are shown in Table 3 below:\nAnalysis. The model’s performance degrades with increasing data sparsity, which is expected.\nHowever, our hybrid model consistently outperforms the standalone models, even with 50% data retention. \nThis can be attributed to the model’s inherent ability to focus on crucial sequences and its resilience to missing \ndata points.\nModel performance across different engineering domains\nA true testament to our model’s versatility would be its applicability across different engineering domains. For \nthis experiment, we applied our model to different datasets from varied engineering fields.\nExperiment setup. Datasets: Underground drilling machine dataset, green stormwater infrastructure dataset, \nand a dataset from a wind turbine system.\nEach dataset was split into training (70%), validation (15%), and testing (15%).\nTable 2.  Performance on dataset B.\nModel MAE RMSE MAPE (%)\nARIMA 5.32 6.78 4.9\nHolt-winters 5.1 6.5 4.7\nFeedforward neural network 4.6 5.68 4.3\nStandalone LSTM 4.2 5 3.8\nStandalone transformer 4.01 4.75 3.7\nOur model 3.55 4.3 3.2\nTable 3.  Robustness to data sparsity.\nData retention (%) Our model’s MAE Standalone LSTM’s MAE Standalone transformer’s MAE\n90 7.12 8.05 7.8\n80 7.45 8.4 8.1\n70 7.9 9 8.6\n60 8.2 9.45 9.05\n50 8.65 10.2 9.75\n18\nVol:.(1234567890)Scientific Reports |         (2024) 14:4890  | https://doi.org/10.1038/s41598-024-55483-x\nwww.nature.com/scientificreports/\nThe results are shown in Table 4 below:\nAnalysis. Our model consistently outperforms its standalone counterparts across all tested domains.\nThis demonstrates the universality of our hybrid model, capable of handling diverse engineering challenges \nwithout the need for domain-specific tweaks.\nScalability analysis\nA model’s efficacy is also determined by its scalability, especially when handling vast datasets or deploying in \nlarge-scale systems.\nExperiment setup. We scaled the size of the dataset from 100,000 data points to 1 million data points.\nThe model’s training time, inference time, and memory footprint were observed.\nThe results are shown in Table 5 below:\nAnalysis. As the dataset size increases, there is a linear increase in the training time and a slight increase in \ninference time. This showcases the model’s scalability in terms of computational efficiency.\nThe memory footprint also scales reasonably, ensuring the model remains deployable even in environments \nwith limited computational resources.\nModel performance with noisy data\nReal-world engineering datasets often contain noise—either from sensor inaccuracies, transmission errors, or \nother external disturbances. It’s pivotal for any predictive model to be resilient to such noise to ensure reliable \nperformance in practical deployments.\nExperiment setup. Noise was artificially added to the datasets at varying levels: 1%, 5%, and 10%.\nThe model was trained and tested on these noisy datasets and its performance compared to clean data.\nThe results are shown in Table 6 below:\nAnalysis. Even with a 10% noise level, our model’s performance degradation is contained, showcasing its \nresilience.\nThe standalone LSTM and Transformer models exhibit more pronounced performance drops as noise levels \nincrease. This further highlights the robustness of our hybrid architecture.\nHyperparameter sensitivity analysis\nThe performance of deep learning models can be significantly influenced by hyperparameters. To ensure our \nmodel’s robustness, we studied its sensitivity to hyperparameters.\nTable 4.  Model performance across different engineering domains.\nDataset Our model’s RMSE Standalone LSTM’s RMSE Standalone transformer’s RMSE\nUnderground drilling machine 6.8 7.65 7.3\nGreen stormwater infrastructure 7.25 8.05 7.8\nWind turbine system 7.1 7.9 7.5\nTable 5.  Result of scalability analysis.\nDataset size (data points) Training time (our model) Inference time (our model) Memory footprint (our model)\n100,000 2.5 h 0.25 s 1.2 GB\n500,000 10.5 h 0.30 s 3.8 GB\n1,000,000 21 h 0.35 s 6.5 GB\nTable 6.  Model performance with noisy data.\nNoise level (%) Our model’s RMSE Standalone LSTM’s RMSE Standalone transformer’s RMSE\n1 6.9 7.8 7.4\n5 7.3 8.25 7.9\n10 7.85 8.95 8.6\n19\nVol.:(0123456789)Scientific Reports |         (2024) 14:4890  | https://doi.org/10.1038/s41598-024-55483-x\nwww.nature.com/scientificreports/\nExperiment setup. We varied key hyperparameters: learning rate, batch size, and dropout rate.\nFor each hyperparameter variation, the model’s performance was assessed.\nThe results are shown in Table 7 below:\nAnalysis. Our model exhibits stability across a range of hyperparameters, indicating that it isn’t overly sensitive \nto specific settings.\nWhile there are slight variations in performance, they are within acceptable margins, reinforcing the model’s \nrobustness and ease of deployment.\nResponse to imbalanced datasets\nImbalanced datasets, where certain classes or sequences are underrepresented, are common in engineering \nscenarios. We tested our model’s performance under such conditions.\nExperiment setup. The datasets were modified to underrepresent certain sequences or patterns.\nModel performance was evaluated on these imbalanced datasets.\nThe results are shown in Table 8 below:\nAnalysis. As the data becomes more imbalanced, a slight drop in the F1-Score is observed.\nHowever, our model manages to maintain a commendable score, even with a 95–5 split. This indicates its \ncapacity to learn from underrepresented patterns effectively.\nAblation study: understanding component contributions\nExperiment setup. The objective of this ablation study is to understand the individual contributions of the \nkey components of our hybrid LSTM-Transformer model. Specifically, we investigate the roles of: LSTM units \nfor handling time-dependent sequences, Transformer units for capturing contextual relationships, and Online \nlearning for real-time adaptability. It is important to note that our model, when enhanced with online learning, \npredicts outcomes one step at a time. This single-step prediction approach ensures high accuracy and immedi-\nate response to dynamic changes in data, which is essential in real-time engineering applications. Lastly, we also \ndiscuss the role of Knowledge distillation in improving model efficiency.\nThe study involves removing one component at a time from the full model and measuring its impact on \nperformance metrics. To ensure statistical reliability, each configuration was run 50 times on both Dataset A \n(Underground Drilling Machines Data) and Dataset B (Green Stormwater Infrastructure Data).\nResults. Table  9 illustrates the results of the ablation study. Each value represents the average performance over \n30 runs, and the standard deviations are provided to indicate variability:\nAnalysis. LSTM units: Removing LSTM units leads to an average MAE increase of 0.35 for Dataset A and 0.25 \nfor Dataset B. The standard deviations indicate low variability, confirming that LSTMs are crucial for capturing \ntemporal sequences.\nTransformer units: The removal of Transformer units results in a comparable performance degradation, \nparticularly highlighting their role in contextual understanding.\nOnline learning: The smaller yet consistent performance drop upon removing online learning suggests that \nit contributes to the model’s adaptive nature, especially in dynamic engineering systems.\nTable 7.  Model’s performance.\nHyperparameter variation Our model’s RMSE\nLearning rate: 0.001 6.8\nLearning rate: 0.01 7.1\nBatch size: 32 6.85\nBatch size: 128 6.78\nDropout rate: 0.2 6.8\nDropout rate: 0.5 6.92\nTable 8.  Response to imbalanced datasets.\nImbalance type Our model’s F1-score\n70% class A, 30% class B 0.88\n90% class A, 10% class B 0.85\n95% class A, 5% class B 0.82\n20\nVol:.(1234567890)Scientific Reports |         (2024) 14:4890  | https://doi.org/10.1038/s41598-024-55483-x\nwww.nature.com/scientificreports/\nKnowledge distillation: The least impact on model performance is observed upon removing knowledge distil-\nlation. This reaffirms its role in computational efficiency rather than accuracy.\nAfter conducting 30 runs for each configuration, we can confidently state that each component in our hybrid \nmodel plays a specific and significant role. The exhaustive nature of this ablation study establishes the robustness \nof our model, making it highly reliable for deployment in complex engineering systems.\nQualitative analysis: visualization of model performance\nTo augment our quantitative analysis, we further examined the model’s performance through visualizations, as \ndepicted in Graphs X, Y , and Z in the Appendix. These graphs provide a qualitative perspective of the model’s \naccuracy and adaptability across different scenarios.\nGraphs X (Underground drilling machines data—dataset A): This graph displays a comparison of actual \nvalues against predicted values over time, illustrating how our model adapts to the underlying pattern of the \ndataset. The graph shows that our model closely follows the actual data trends, demonstrating its effectiveness \nin capturing temporal dynamics.\nGraphs Y (Green stormwater infrastructure data—dataset B): Similar to Graph X, this graph compares actual \nand predicted values, emphasizing the model’s ability to accurately capture cyclical patterns present in Dataset \nB. The slight deviations between the predicted and actual values are within acceptable ranges, underscoring our \nmodel’s precision.\nGraphs Z (model performance with different noise levels): This bar chart illustrates the model’s resilience to \ndifferent levels of noise. Despite increasing noise levels, our model maintains a relatively stable RMSE, signifying \nits robustness against data perturbations, a critical factor in real-world applications.\nThese visualizations not only complement our quantitative results but also provide a more comprehensive \nunderstanding of the model’s capabilities in diverse conditions. They reaffirm the model’s adaptability and accu-\nracy, as highlighted in our quantitative analysis.\nDiscussion\nIn our quest to decode the intricate dynamics of engineering systems, we’ve unearthed several insights that not \nonly validate our research methodology but also set the stage for future explorations. Let’s unpack the findings \nin light of our comprehensive experimentation:\nModel resilience to real‑world challenges\nData sparsity. Our approach’s consistent performance even when 50% of the data was omitted not only under-\nscores its ability to work with limited data but also outperforms many traditional models in such scenarios. This \nresilience is particularly relevant for real-world engineering scenarios where acquiring a dense dataset might not \nalways be feasible.\nNoisy data. Beyond the inherent messiness of real-world data, engineering systems often grapple with sensor \nerrors or environmental interferences that introduce noise. Our model’s exceptional ability to maintain per -\nformance even in the face of 10% noise not only showcases its robustness but also stands as a testament to its \nsuperiority over other conventional models.\nScalability. In the current era of big data, where data volume can be overwhelming, our model’s capability to \nseamlessly handle datasets as vast as a million data points without compromising speed or memory is unpar -\nalleled. This not only makes our approach theoretically sound but also a formidable contender for practical \ndeployments against other existing models.\nComparison with standalone architectures\nDepth over width. Our experiments have brought to light the profound impact of combining architectures \n(depth) over merely expanding a single architecture (width). Our hybrid model, by integrating the strengths \nof both LSTMs and Transformers, provides an enriched and holistic understanding of the data, something that \nindividual models often struggle with.\nConsistent outperformance. The versatility of our model is evident as it consistently outperforms across \ndiverse datasets, from drilling machines to stormwater infrastructure. This consistency is a marked departure \nfrom many models that are tailored and often overfitted to specific datasets.\nTable 9.  The results of the ablation study.\nComponents removed MAE (dataset A) ± std RMSE (dataset A) ± std MAE (dataset B) ± std RMSE (dataset B) ± std\nNone (full model) 6.55 ± 0.10 7.8 ± 0.15 3.55 ± 0.05 4.3 ± 0.08\nLSTM units 6.9 ± 0.12 8.1 ± 0.18 3.8 ± 0.06 4.6 ± 0.09\nTransformer units 7.1 ± 0.11 8.3 ± 0.16 3.9 ± 0.07 4.7 ± 0.1\nOnline learning 6.8 ± 0.09 8.0 ± 0.14 3.7 ± 0.05 4.5 ± 0.07\nKnowledge distillation 6.7 ± 0.08 7.9 ± 0.13 3.6 ± 0.04 4.4 ± 0.06\n21\nVol.:(0123456789)Scientific Reports |         (2024) 14:4890  | https://doi.org/10.1038/s41598-024-55483-x\nwww.nature.com/scientificreports/\nRole of knowledge distillation\nLean yet powerful. The art of knowledge distillation has been pivotal in sculpting our model to be deployment-\nready yet formidable in performance. This \"learning from the teacher\" paradigm ensures that our model remains \ncomputationally efficient without trading off accuracy, a balance that many models in the industry strive for.\nReal-world implication. In numerous engineering setups, the sheer complexity and computational demands \nof deploying a teacher model are infeasible. Herein, our distilled model emerges as the quintessential solution, \noffering near-teacher-level performance without the associated overhead.\nImplications for deployment\nEdge devices. With the technological landscape gravitating towards edge computing, our model’s deployability \non edge devices stands out. Its efficiency, especially post knowledge distillation, makes it a prime candidate for \nreal-time on-site predictions, a feature that many conventional models grapple with.\nCloud systems. For more centralized systems grappling with massive datasets, our model’s inherent scalability \nensures seamless handling of computational demands, making it a preferred choice for cloud-based deploy-\nments over other existing models.\nAdaptability. Our model’s dynamism, powered by the online learning mechanism showcased in our experi-\nments, ensures it remains contemporary and evolving. Such adaptability is indispensable in ever-changing engi-\nneering systems where static models can quickly become redundant.\nIn conclusion, our discussion not only illuminates the multifarious strengths of our approach, drawing from \nmeticulous experimentation, but also establishes its edge over prevalent models. Our research not only vindicates \nour initial hypotheses but emphatically underscores the practicality and real-world readiness of our model, set-\nting a benchmark for future endeavors in this domain.\nConclusion\nConclusive synthesis\nNavigating the complex field of predictive modeling in engineering domains has been an enriching experience, \nfilled with both challenges and insights. This concluding section synthesizes the key contributions and outcomes \nof our rigorous research journey.\nThe merit of hybrid architecture\nOur exploration began with the innovative amalgamation of LSTM and Transformer architectures, each contrib-\nuting unique strengths—LSTM for capturing temporal dependencies and Transformer for leveraging contextual \ninformation. This wasn’t merely a theoretical exercise; our extensive, repeated experiments confirmed the hybrid \nmodel’s distinct edge over conventional standalone models.\nEmpirical rigor and real‑world applicability\nIn the ablation study, each configuration was tested across 50 independent runs to ensure statistical reliability, \nwith the performance metrics summarized to obtain a mean and standard deviation for each. The mean serves \nas a central performance measure, while the standard deviation offers insight into result consistency across runs. \nA lower standard deviation signifies stable model performance, reinforcing the proposed hybrid architecture’s \nrobustness. Our model underwent testing in various real-world engineering conditions, including noisy, sparse, \nand imbalanced datasets. Through a robust series of experiments, including a 50-run ablation study for statistical \nreliability, our model has demonstrated its dependability and resilience, rendering it highly suitable for practical \nengineering scenarios.\nThe power of knowledge distillation\nKnowledge distillation, a major component of our research, allowed us to encapsulate the insights of more com-\nplex models into our leaner hybrid architecture without sacrificing performance. This strategy not only enhances \ncomputational efficiency but also ensures that the model remains potent and accurate.\nOnline learning and adaptability\nThe dynamic nature of engineering systems necessitates models that can adapt over time. Our model, fortified \nwith online learning mechanisms, is designed to continuously update its knowledge, aligning itself with emerg-\ning data patterns and system dynamics.\nIn summation, our exploration into predictive modeling for engineering systems, while exhaustive, is just the \ntip of the  iceberg25,26. The insights gleaned hold immense promise, not just as solutions for present challenges but \nas stepping stones for future innovations. As we conclude, we remain optimistic about the myriad possibilities \nthat the future holds and the potential advancements in this domain.\nLimitations and future directions\nWhile our research has achieved noteworthy results and presented significant advancements in the domain of \npredictive modeling for engineering systems, it’s crucial to acknowledge its limitations and discuss potential \navenues for future exploration.\n22\nVol:.(1234567890)Scientific Reports |         (2024) 14:4890  | https://doi.org/10.1038/s41598-024-55483-x\nwww.nature.com/scientificreports/\nLimitations\nDataset diversity. Although our model was tested on multiple engineering datasets, it’s still a subset of the \nvast array of engineering problems. There might be specific niches or specialized domains where our model’s \nperformance could  vary27.\nHyperparameter tuning. Our study has shown the model’s robustness across various hyperparameter settings. \nHowever, optimal performance in any specific scenario might still necessitate fine-tuning28,29.\nComputational complexity. While the hybrid model is more efficient than its more complex teacher model, it \nstill possesses a higher computational footprint than simpler traditional models, making it potentially unsuitable \nfor extremely resource-constrained  environments30,31.\nModel interpretability. Deep learning models, including our hybrid architecture, often suffer from the \"black \nbox\" syndrome, making them harder to interpret and understand compared to traditional statistical models.\nOnline learning adaptability. Our model’s online learning mechanism, while effective, is based on the assump-\ntion of gradual data shifts. Sudden, drastic changes in data patterns might pose challenges.\nFuture directions\nExpanding dataset horizons. Future work should aim to test the model across an even broader array of engi-\nneering datasets, diving into more specialized niches to ensure comprehensive  applicability32–34.\nEnhanced interpretability. Integrating techniques for model interpretability, like SHAP or LIME, can make the \nmodel’s predictions more transparent, aiding in its acceptance in critical engineering applications.\nModel refinements. While our hybrid architecture has shown promise, there’s always room for refinement. \nExploring variations, perhaps integrating newer architectures or techniques, can be a future  avenue35,36.\nReal-time deployment and feedback. Deploying the model in real-world engineering systems and gather -\ning feedback would provide invaluable  insights37. This would not only validate our findings but also highlight \nunforeseen  challenges38.\nAddressing sudden data shifts. Enhancing the online learning mechanism to adapt swiftly to sudden data \nchanges can be a pivotal enhancement, making the model even more robust.\nCollaborative learning. In scenarios where multiple instances of our model are deployed across different loca-\ntions, enabling them to collaboratively learn and share insights can further enhance  performance39,40.\nIn conclusion, while our research has paved a promising path in predictive modeling for engineering systems, \nit’s a continuous journey. The limitations highlighted are not just challenges but also opportunities, beckoning \nfurther exploration. The future directions delineated provide a roadmap, guiding future endeavors in this excit-\ning domain.\nData availability\nThe datasets generated and/or analysed during the current study are available in the Baidu Netdisk repository, \nY ou can click on the link: https:// pan. baidu. com/s/ 173aN zqqWw QW ANh sLw2- J0A Then enter the extraction \ncode: wzns to view all my raw data.\nReceived: 22 September 2023; Accepted: 23 February 2024\nReferences\n 1. Wen, J. & Wang, Z. Short-term load forecasting with bidirectional LSTM-attention based on the sparrow search optimisation \nalgorithm. Int. J. Comput. Sci. Eng. 26(1), 20–27. https:// doi. org/ 10. 1504/ ijcse. 2023. 129154 (2023).\n 2. Graus, R. Bridging the gap: Engineer Eduardo Torroja in the post-war networks of modern architecture. Int. J. Constr. Hist. Soc.  \n38(1), 99–115 (2023).\n 3. Yu, Y ., Si, X., Hu, C. & Zhang, J. A review of recurrent neural networks: LSTM cells and network architectures. Neural Comput. \n31(7), 1235–1270. https:// doi. org/ 10. 1162/ neco_a_ 01199 (2019).\n 4. Kuhn, M. & Johnson, K. Classification trees and rule-based models. In Applied Predictive Modeling (eds Kuhn, M. & Johnson, K.) \n(Springer, 2013). https:// doi. org/ 10. 1007/ 978-1- 4614- 6849-3_ 14.\n 5. Y e, Y . & Ma, L. Positioning of traffic engineers in the process of autonomy of modern traffic technology in China under the back-\nground of cognitive impairment. Psychiatr. Danub. 34, S123–S124 (2022).\n 6. Liu, H., Gegov, A. and Cocea, M. Network based rule representation for knowledge discovery and predictive modelling. In 2015 \nIEEE International Conference on Fuzzy Systems (FUZZ‑IEEE), Istanbul, Turkey, pp. 1–8 (2015). https:// doi. org/ 10. 1109/ FUZZ- \nIEEE. 2015. 73378 07.\n 7. Wang, N., Gao, Y ., Liu, Y . & Li, K. Self-learning-based optimal tracking control of an unmanned surface vehicle with pose and \nvelocity constraints. Int. J. Robust Nonlinear Control 32(5), 2950–2968. https:// doi. org/ 10. 1002/ rnc. 5978 (2022).\n 8. Ferreira, P ., Fernandes, P . A. & Ramos, M. J. Modern computational methods for rational enzyme engineering. Chem. Catal. 2(10), \n2481–2498. https:// doi. org/ 10. 1016/j. checat. 2022. 09. 036 (2022).\n23\nVol.:(0123456789)Scientific Reports |         (2024) 14:4890  | https://doi.org/10.1038/s41598-024-55483-x\nwww.nature.com/scientificreports/\n 9. Sklodowska, A. M., Holden, C., Gueguen, P ., Finnegan, J. & Sidwell, G. Structural change detection applying long-term seis-\nmic interferometry by deconvolution method to a modern civil engineering structure (New Zealand). Bull. Earthq. Eng. 19(9), \n3551–3569. https:// doi. org/ 10. 1007/ s10518- 021- 01110-3 (2021).\n 10. Li, Y ., Tong, Z., Tong, S., Westerdahl, D. & Pastoriza, T. A data-driven interval forecasting model for building energy prediction \nusing attention-based LSTM and fuzzy information granulation. Sustain. Cities Soc. 76, 103481. https:// doi. org/ 10. 1016/j. scs. 2021. \n103481 (2022).\n 11. Ranjan, R. & Daniel, A. K. Cobico: A model using multi-stage convnet with attention-based bi-LSTM for efficient sentiment clas-\nsification. Int. J. Knowl. Based Intell. Eng. Syst. 27(1), 1–24. https:// doi. org/ 10. 3233/ kes- 230901 (2023).\n 12. Y ang, F ., Zhang, H. & Tao, S. Travel order quantity Prediction via attention-based bidirectional LSTM networks. J. Supercomput. \n78(3), 4398–4420. https:// doi. org/ 10. 1007/ s11227- 021- 04032-8 (2022).\n 13. Singh, J. P ., Kumar, A., Rana, N. P . & Dwivedi, Y . K. Attention-based LSTM network for rumor veracity estimation of tweets. Inf. \nSyst. Front. 24(2), 459–474. https:// doi. org/ 10. 1007/ s10796- 020- 10040-5 (2022).\n 14. Lin, Y . et al. A new attention-based LSTM model for closing stock price prediction. Int. J. Financ. Eng. 09(3), 2250014. https:// doi. \norg/ 10. 1142/ s2424 78632 25001 41 (2022).\n 15. Niu, D., Sun, L., Yu, M. & Wang, K. Point and interval forecasting of ultra-short-term wind power based on a data-driven method \nand hybrid deep learning model. Energy 254, 124384. https:// doi. org/ 10. 1016/j. energy. 2022. 124384 (2022).\n 16. Bai, T. & Tahmasebi, P . Attention-based LSTM-FCN for earthquake detection and location. Geophys. J. Int. 228(3), 1568–1576. \nhttps:// doi. org/ 10. 1093/ gji/ ggab4 01 (2022).\n 17. Zhang, W ., Liu, X., Zhang, L. & Wang, Y . Intelligent real-time prediction of multi-region thrust of EPB shield machine based on \nSSA-LSTM. Eng. Res. Express 5(3), 035013. https:// doi. org/ 10. 1088/ 2631- 8695/ ace3a5 (2023).\n 18. Zhang, N. & Zhao, L.-S. Method for real-time prediction of cutter wear during shield tunnelling: A new wear rate index and \nMCNN-GRU. Methodsx 10, 102017. https:// doi. org/ 10. 1016/j. mex. 2023. 102017 (2023).\n 19. Y ao, H., Li, Q. & Leng, J. Physics-informed multi-step real-time conflict-based vehicle safety prediction. Accid. Anal. Prev. 182, \n106965. https:// doi. org/ 10. 1016/j. aap. 2023. 106965 (2023).\n 20. Wood, J., Zhengyao, Y . & Gayah, V . V . Development and evaluation of frameworks for real-time bus passenger occupancy predic-\ntion. Int. J. Transport. Sci. Technol. 12(2), 399–413. https:// doi. org/ 10. 1016/j. ijtst. 2022. 03. 005 (2023).\n 21. Wang, F ., Bi, J., Xie, D. & Zhao, X. A data-driven prediction model for aircraft taxi time by considering time series about gate and \nreal-time factors. Transportmet. a‑Transport Sci. https:// doi. org/ 10. 1080/ 23249 935. 20713 53 (2022).\n 22. Tang, Xu., Dali, Wu., Wang, S. & Pan, X. Research on real-time prediction of hydrogen sulfide leakage diffusion concentration of \nnew energy based on machine learning. Sustainability 15(9), 7237. https:// doi. org/ 10. 3390/ su150 97237 (2023).\n 23. Safa, M., Pandian, A., Gururaj, H. L., Ravi, V . & Krichen, M. Real time health care big data analytics model for improved QoS in \ncardiac disease prediction with Iot devices. Health Technol. 13(3), 473–483. https:// doi. org/ 10. 1007/ s12553- 023- 00747-1 (2023).\n 24. Manzoor, W . A., Rawashdeh, S. & Mohammadi, A. Real-time prediction of pre-ignition and super-knock in internal combustion \nengines. Sae Int. J. Engines 16(3), 363–375. https:// doi. org/ 10. 4271/ 03- 16- 03- 0021 (2023).\n 25. Li, Y .-Q. et al. Real-time intelligent prediction method of cable’s fundamental frequency for intelligent maintenance of cable-stayed \nbridges. Sustainability 15(5), 4086. https:// doi. org/ 10. 3390/ su150 54086 (2023).\n 26. Li, J., Xi, F ., Wenkui, Yu., Sun, C. & Wang, X. Real-time prediction of sepsis in critical trauma patients: Machine learning-based \nmodeling study. JMIR Formative Res. https:// doi. org/ 10. 2196/ 42452 (2023).\n 27. Chen, J. C., Guo, G. & Chang, Y .-H. Intelligent dimensional prediction systems with real-time monitoring sensors for injection \nmolding via statistical regression and artificial neural networks. Int. J. Interact. Des. Manuf. Ijidem 17(3), 1265–1276. https:// doi. \norg/ 10. 1007/ s12008- 022- 01115-5 (2023).\n 28. Kobayashi, K. & Kubo, N. Prediction of real-time kinematic positioning availability on road using 3d map and machine learning. \nInt. J. Intell. Transport. Syst. Res. 21(2), 277–292. https:// doi. org/ 10. 1007/ s13177- 023- 00352-6 (2023).\n 29. Kim, Y .-I., Lee, K.-H. & Park, S.-H. Application and evaluation of machine learning techniques for real-time short-term prediction \nof air pollutants. J. Korean Soc. Atmos. Environ. 39(1), 107–127. https:// doi. org/ 10. 5572/ kosae. 2023. 39.1. 107 (2023).\n 30. Thomas, G. Continual domain adaptation through knowledge distillation (2023).\n 31. Shao, Z., Wan, J. & Zong, L. A video question answering model based on knowledge distillation. Information 14(6), 328. https:// \ndoi. org/ 10. 3390/ info1 40603 28 (2023).\n 32. Scalercio, A. & Paes, A. Masked transformer through knowledge distillation for unsupervised text style transfer. Nat. Lang. Eng.  \nhttps:// doi. org/ 10. 1017/ s1351 32492 30003 23 (2023).\n 33. Pool-Cen, J., Carlos-Martinez, H., Hernandez-Chan, G. & Sanchez-Siordia, O. Detection of depression-related tweets in Mexico \nusing crosslingual schemes and knowledge distillation. Healthcare 11(7), 1057. https:// doi. org/ 10. 3390/ healt hcare 11071 057 (2023).\n 34. Murata, R., Okubo, F ., Minematsu, T., Taniguchi, Y . & Shimada, A. Recurrent neural network-fitnets: Improving early prediction \nof student performanceby time-series knowledge distillation. J. Educ. Comput. Res. 61(3), 639–70. https:// doi. org/ 10. 1177/ 07356 \n33122 11297 65 (2023).\n 35. Montalbo, F . J. P . Automating mosquito taxonomy by compressing and enhancing a feature fused efficientnet with knowledge \ndistillation and a novel residual skip block. Methodsx 10, 102072. https:// doi. org/ 10. 1016/j. mex. 2023. 102072 (2023).\n 36. Hong, Q.-B., Chung-Hsien, Wu. & Wang, H.-M. Speaker-specific articulatory feature extraction based on knowledge distillation \nfor speaker recognition. Apsipa Trans. Signal Inf. Process. https:// doi. org/ 10. 1561/ 116. 00000 150 (2023).\n 37. Prasad, A. R. & Rajesh, A. Hybrid heuristic mechanism for occlusion aware facial expression recognition scheme using patch based \nadaptive CNN with attention mechanism. Intell. Decis. Technol. Neth. 17(3), 773–797. https:// doi. org/ 10. 3233/ idt- 230047 (2023).\n 38. Ma, Y . M., Liu, R. X., Wang, S. S. & Han, F . Reaumuria Soongorica-Plant model to understand drought adaptive mechanisms of \nxerophyte and their potentials in improving stress tolerance in plants. J. Environ. Biol. 44(1), 1–10. https:// doi. org/ 10. 22438/ jeb/ \n44/1/ MRN- 5085 (2023).\n 39. Kaur, A. & Verma, A. Adaptive access control mechanism (AACM) for enterprise cloud computing. J. Electr. Comput. Eng. 2023, \n3922393. https:// doi. org/ 10. 1155/ 2023/ 39223 93 (2023).\n 40. Kabir, E., Guikema, S. D. & Quiring, S. M. Power outage prediction using data streams: An adaptive ensemble learning approach \nwith a feature-and performance-based weighting mechanism. Risk Anal. https:// doi. org/ 10. 1111/ risa. 14211 (2023).\nAcknowledgements\nWe sincerely thank the Key Laboratory of Ethnic Language Intelligent Analysis and Security Governance of the \nMinistry of Education at Minzu University of China for its firm support and valuable help during our research \nprocess. Their generous funding, resources, and intellectual guidance are critical to expanding the scope and \nimpact of our work. We would also like to express our deep gratitude to Kangjie Cao for his diligent and pioneer-\ning contributions to this research. As the original corresponding author and first author (now first author), his \ntireless dedication, deep expertise and visionary approach were crucial in bringing this manuscript to fruition. \nHis work laid the foundation for this research and helped guide its current form. The support and contributions \nof all parties have been critical to our research, and we are deeply grateful for their commitment and cooperation.\n24\nVol:.(1234567890)Scientific Reports |         (2024) 14:4890  | https://doi.org/10.1038/s41598-024-55483-x\nwww.nature.com/scientificreports/\nAuthor contributions\nK.C. Conceived and designed the experiments, performed the experiments, analyzed the results, wrote most of \nthe paper, and was involved in all stages of the manuscript revision. K.C. also oversaw the project and coordinated \nthe collaboration among contributors. T.Z. Provided essential experimental equipment, constructive feedback, \nand expert guidance, particularly in the methodology and analysis sections. Played a leading and pivotal role \nin improving the overall quality and integrity of the paper, especially during the revision process. T.Z. also con-\ntributed to the writing and fine-tuning of several critical sections of the manuscript. J.H. Assisted significantly \nin the creation of some figures for the article, contributed to the revision of the introduction and other sections, \nand provided substantial support in structuring and refining the manuscript content.\nFunding\nThis research did not receive any specific grant from funding agencies in the public, commercial, or not-for-\nprofit sectors.\nCompeting interests \nNo competing interests declared by all authors. Kangjie Cao, as the First author, is responsible for submitting \na competing interests statement on behalf of all authors of the paper. This statement has been included in the \nsubmitted article file.\nAdditional information\nSupplementary Information The online version contains supplementary material available at https:// doi. org/ \n10. 1038/ s41598- 024- 55483-x.\nCorrespondence and requests for materials should be addressed to T.Z.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2024"
}