{
  "title": "Hi-BEHRT: Hierarchical Transformer-Based Model for Accurate Prediction of Clinical Events Using Multimodal Longitudinal Electronic Health Records",
  "url": "https://openalex.org/W4310645210",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2898879642",
      "name": "Yikuan Li",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2215634661",
      "name": "Mohammad Mamouei",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2399110041",
      "name": "Gholamreza Salimi Khorshidi",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2751608690",
      "name": "Shishir Rao",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2186042737",
      "name": "Abdelaali Hassaine",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2237919805",
      "name": "Dexter Canoy",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A297530023",
      "name": "Thomas Lukasiewicz",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2589677415",
      "name": "Kazem Rahimi",
      "affiliations": [
        "University of Oxford"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2122653375",
    "https://openalex.org/W2162189888",
    "https://openalex.org/W2395172628",
    "https://openalex.org/W2481271618",
    "https://openalex.org/W6726186668",
    "https://openalex.org/W3017637887",
    "https://openalex.org/W3160137267",
    "https://openalex.org/W6804281351",
    "https://openalex.org/W2803290558",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6776048684",
    "https://openalex.org/W6779163297",
    "https://openalex.org/W3008736151",
    "https://openalex.org/W2119852447",
    "https://openalex.org/W2770196824",
    "https://openalex.org/W2944890781",
    "https://openalex.org/W2791794153",
    "https://openalex.org/W6678271563",
    "https://openalex.org/W2033945449",
    "https://openalex.org/W6766375017",
    "https://openalex.org/W3204990179",
    "https://openalex.org/W6780218876",
    "https://openalex.org/W1565746575",
    "https://openalex.org/W6746932923",
    "https://openalex.org/W6754723338",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3213708588",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2775461895",
    "https://openalex.org/W4300007355",
    "https://openalex.org/W3096500023"
  ],
  "abstract": "Electronic health records (EHR) represent a holistic overview of patients' trajectories. Their increasing availability has fueled new hopes to leverage them and develop accurate risk prediction models for a wide range of diseases. Given the complex interrelationships of medical records and patient outcomes, deep learning models have shown clear merits in achieving this goal. However, a key limitation of current study remains their capacity in processing long sequences, and long sequence modelling and its application in the context of healthcare and EHR remains unexplored. Capturing the whole history of medical encounters is expected to lead to more accurate predictions, but the inclusion of records collected for decades and from multiple resources can inevitably exceed the receptive field of the most existing deep learning architectures. This can result in missing crucial, long-term dependencies. To address this gap, we present Hi-BEHRT, a hierarchical Transformer-based model that can significantly expand the receptive field of Transformers and extract associations from much longer sequences. Using a multimodal large-scale linked longitudinal EHR, the Hi-BEHRT exceeds the state-of-the-art deep learning models 1% to 5% for area under the receiver operating characteristic (AUROC) curve and 1% to 8% for area under the precision recall (AUPRC) curve on average, and 2% to 8% (AUROC) and 2% to 11% (AUPRC) for patients with long medical history for 5-year heart failure, diabetes, chronic kidney disease, and stroke risk prediction. Additionally, because pretraining for hierarchical Transformer is not well-established, we provide an effective end-to-end contrastive pre-training strategy for Hi-BEHRT using EHR, improving its transferability on predicting clinical events with relatively small training dataset.",
  "full_text": "1106 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 27, NO. 2, FEBRUARY 2023\nHi-BEHRT: Hierarchical Transformer-Based\nModel for Accurate Prediction of Clinical\nEvents Using Multimodal Longitudinal\nElectronic Health Records\nYikuan Li , Mohammad Mamouei , Gholamreza Salimi-Khorshidi, Shishir Rao , Abdelaali Hassaine ,\nDexter Canoy, Thomas Lukasiewicz, and Kazem Rahimi\nAbstract— Electronic health records (EHR) represent a\nholistic overview of patients’ trajectories. Their increasing\navailability has fueled new hopes to leverage them and\ndevelop accurate risk prediction models for a wide range\nof diseases. Given the complex interrelationships of med-\nical records and patient outcomes, deep learning models\nhave shown clear merits in achieving this goal. However,\na key limitation of current study remains their capacity in\nprocessing long sequences, and long sequence modelling\nand its application in the context of healthcare and EHR re-\nmains unexplored. Capturing the whole history of medical\nencounters is expected to lead to more accurate predic-\ntions, but the inclusion of records collected for decades and\nfrom multiple resources can inevitably exceed the recep-\ntive ﬁeld of the most existing deep learning architectures.\nThis can result in missing crucial, long-term dependencies.\nTo address this gap, we present Hi-BEHRT, a hierarchical\nTransformer-based model that can signiﬁcantly expand the\nreceptive ﬁeld of Transformers and extract associations\nfrom much longer sequences. Using a multimodal large-\nscale linked longitudinal EHR, the Hi-BEHRT exceeds the\nstate-of-the-art deep learning models 1% to 5% for area\nunder the receiver operating characteristic (AUROC) curve\nand 1% to 8% for area under the precision recall (AUPRC)\ncurve on average, and 2% to 8% (AUROC) and 2% to\n11% (AUPRC) for patients with long medical history for\n5-year heart failure, diabetes, chronic kidney disease, and\nstroke risk prediction. Additionally, because pretraining for\nManuscript received 18 March 2022; revised 21 October 2022; ac-\ncepted 20 November 2022. Date of publication 25 November 2022;\ndate of current version 6 February 2023. The work of Yikuan Li and\nKazem Rahimi are supported by British Heart Foundation under Grant\nFS/PhD/21/29110. The work of Kazem Rahimi and Dexter Canoy under\nGrant PG/18/65/33872. The work of Kazem Rahimi was also supported\nin part by the UKRI’s Global Challenges Research Fund under Grant\nES/P0110551/1, and in part by the Oxford NIHR Biomedical Research\nCentre and the Oxford Martin School, University of Oxford. (Correspond-\ning author: Kazem Rahimi.)\nYikuan Li, Mohammad Mamouei, Gholamreza Salimi-Khorshidi,\nShishir Rao, Abdelaali Hassaine, Dexter Canoy, and Kazem Rahimi\nare with the Deep Medicine, Oxford Martin School, University of\nOxford, OX1 2JD Oxford, U.K. (e-mail: yikuan.li@wrh.ox.ac.uk;\nmohammad.mamouei@wrh.ox.ac.uk; reza.khorshidi@wrh.ox.ac.uk;\nshishir.rao@wrh.ox.ac.uk; abdelaali.hassaine@wrh.ox.ac.uk; dex-\nter.canoy@wrh.ox.ac.uk; kazem.rahimi@wrh.ox.ac.uk).\nThomas Lukasiewicz is with the Department of Computer\nScience, University of Oxford, OX1 2JD Oxford, U.K. (e-mail:\nthomas.lukasiewicz@cs.ox.ac.uk).\nDigital Object Identiﬁer 10.1109/JBHI.2022.3224727\nhierarchical Transformer is not well-established, we provide\nan effective end-to-end contrastive pre-training strategy for\nHi-BEHRT using EHR, improving its transferability on pre-\ndicting clinical events with relatively small training dataset.\nIndex Terms— Deep learning, electronic health records,\nrisk prediction.\nI. I NTRODUCTION\nR\nISK models play an important role in disease prognosis, di-\nagnosis, and intervention. Currently, most risk models are\nconventional statistical models based on expert-selected predic-\ntors. For instance, QRISK, Framingham risk score, and ASSIGN\nrisk score are commonly used models for cardiovascular diseases\n[1]. However, with the growing access to EHR, especially linked\nlongitudinal EHR, from millions of patients, we now have an\nunprecedented opportunity to achieve a better understanding of\npatients’ health trajectories, and to develop novel risk prediction\nmodels, which can capture important predictors and their long-\nterm interdependencies towards more accurate risk prediction.\nEHR provide up-to-date and comprehensive information\nabout patients. It allows a clinician to assess the entire patient\njourney and represents what is actually available in clinical\npractice [2]. Due to its complexity and heterogeneity, modelling\nusing large-scale EHR is a challenge. Some of the previous\nworks have shown that deep learning is an effective method and\ndeep learning models outperform standard statistical models in\nvarious complex risk prediction tasks using EHR. For instance,\nNguyen et al. [3] introduced a convolutional neural network\n(CNN) model named Deepr for the prediction of readmission\nand it substantially outperformed the logistic regression (LR).\nChoi et al. [4] applied a shallow recurrent neural network (RNN)\nfor the prediction of heart failure and greatly improved the\nprediction performance compared to the LR and multi-layer per-\nceptron. Recently, Transformer-based models have gained wide\npopularity for risk prediction using EHR due to their superior\nperformance for handling sequential data. In one of the earliest\nworks in applying Transformer to EHR, Li et al. [5] proposed\nBEHRT and presented the idea of utilizing multiple embeddings\n(i.e., event, visit, age, and position) to represent a patient’s\nmedical history. It substantially outperformed the CNN and\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nLI et al.: H i-BEHRT: HIERARCHICAL TRANSFORMER-BASED MODEL FOR ACCURATE PREDICTION OF CLINICAL EVENTS 1107\nRNN-based models for the subsequent visit risk prediction. In\na similar attempt, another Transformer adaptation, Med-BERT\n[6], had a similar structure as the BEHR. It only included event\nand position embedding for patient representation learning and\ntrained on EHR data from 20 million patients. Building on these\nworks, a recent study proposed CEHR-BERT [7], which used\ndifferent strategy for the incorporation of event, visit, age, and\nposition, together with the additional artiﬁcial time tokens, for\npatient representation. It achieved outstanding performance for\nfour different risk prediction tasks.\nHowever, existing works as applied to EHR have largely been\nbased on US EHR [8] and relied on a fraction of information\navailable in the datasets (typically disease and medications from\nhospital records). This would typically involve maximally a few\nhundred records from a patient [4], [6]. A realistic demand\nof using more comprehensive information from the records\ncan substantially increase the number of available records and\nexpand the EHR sequence length for modelling. Therefore,\nadapting risk prediction model to handle patients with thousands\nof records or even longer EHR sequence and avoid missing\nimportant historical information is highly desired, but still re-\nmains as a critical bottleneck. For instance, the complexity of\nTransformer-based models grows quadratically as the sequence\nlength grows [9], and a Transformer model with over 100 million\nparameters can only handle a sequence with maximally 512\nsequence length [10].\nRecent research has proposed two potential solutions for the\nTransformer-based models: 1) the use of sparse attention or\nlower-rank approximation to replace the classic self-attention\nmechanism in Transformer (e.g., Longformer [11] and Lin-\nformer [12]); 2) and the use of hierarchical model architectures\nto extract local temporal features and reduce sequence length\nbefore feeding into the higher-level Transformer architecture\n[13]. Both approaches substantially reduce the complexity of\nTransformer models and adapt them to better handle data with\nlonger sequence. Considering that medical events naturally have\nstronger local correlation (i.e., the closer two events are in time,\nthe more likely that they are also semantically related), the ﬁrst\nobjective of this paper is to apply the hierarchical model architec-\nture [13] to further enhance a state-of-the-art Transformer-based\nrisk prediction model, BEHRT [5]. The hierarchical BEHRT\n(Hi-BEHRT) model will be able to handle risk prediction for\npatients with more comprehensive and longer EHR than BEHRT.\nWe will investigate and compare model performance on incident\nrisk prediction for four diseases: heart failure (HF), diabetes,\nchronic kidney disease (CKD), and stroke. In addition, by con-\ntrast with BEHRT which used diagnosis only as model input,\nwe will also include information about medications, procedures,\ntests, blood pressure (BP) measurement, drinking status, smok-\ning status, and body mass index (BMI), which are important\npredictors of outcomes and periodically measured in routine\nclinical practice.\nIn addition to the model architecture, the pre-training strategy\nis also a critical component in modelling. Most of the current\npre-training methods for Transformer are either based on the\nmasked language model (MLM) or the sequence pair prediction\n(e.g., the next sentence and the next segment prediction) or both\n[5], [10], [14]. Because the deﬁnition of the “next sentence”\nin EHR is not as clearly deﬁned as the concept in natural\nlanguage processing, MLM is deemed a more suitable approach\nin EHR applications [5], [14]. In terms of the training task, MLM\npredicts the masked records in a sequence using their contexts.\nHowever, in the hierarchical structure, the lower-level feature\nextractor has transformed the records into high dimensional\nrepresentations, thus, deﬁning a clear label for MLM training\nis difﬁcult. Some of the previous works using hierarchical\nTransformer only initialized the weight of embeddings or certain\ncomponents of a model using weights pre-trained on other tasks\n[13]. Therefore, pre-training the entire hierarchical Transformer\narchitecture and ﬁne-tuning on down-stream tasks using EHR\nis still not well-established. Recent proposed self-supervised\npre-training framework on contractive learning, more specif-\nically, bootstrap your own latent [15] (BYOL) provides an\nalternative approach for pre-training. It directly compares the\nlatent representations from the model, expecting the different\naugmentations of the same input to have similar representations.\nTherefore, it is ﬂexible in terms of the model architecture and\ncan be adopted to the Hi-BEHRT model. However, the reported\nadvantage of BYOL in pre-training and overcoming data scarcity\nhas only been tested on an image dataset and whether this\ncan have some advantages when applied to sequential EHR is\nunclear. To this end, the second objective of this paper is to\nevaluate the usability of BYOL for sequential Hi-BEHRT model\nusing EHR.\nII. M ETHODS\nA. Data Source and Cohort Selection\nWe undertook a cohort study in a large population of primary\ncare patients using Clinical Practice Research Datalink (CPRD)\n[16]. It is one of the largest deidentiﬁed longitudinal EHR dataset\nthat contains patient data from approximate 7% of the U.K.\npopulation [17]. Primary care records from CPRD can also\nlink to secondary care records from Hospital Episodes Statistics\nAdmitted Patient Care data and death registration data from the\nOfﬁce for National Statistics. It is broadly representative in terms\nof age, sex, and ethnicity. We identiﬁed an open cohort of patients\naged 16 years and older and contributed to data between Jan\n1, 1985 and Sep 30, 2015. Patients were eligible for inclusion\nif their records were labelled as “accept” by the CPRD quality\ncontrol [16] and they were linked to Hospital Episodes Statistics.\nThis led to a cohort with 4063811 patients.\nAmong the identiﬁed patients, we split them into 60%, 10%,\nand 30% for training, tuning, and validation for the risk predic-\ntion task, respectively. Additionally, the patients in the training\nand tuning cohorts were also used for pre-training. Within each\nrisk prediction cohort, we identiﬁed two important dates for\neach patient, an incident date of the event of interest and a\nbaseline date. All the records before the baseline date were used\nas learning period to predict the 5-year risk of an event of interest\nafter it. Given the known inaccuracies in recording of timing of\nevents, we also ignored the events that occurred within 1-year\nafter the baseline date. We achieved this by ﬁrstly identifying\nthe incident date for all positive cases, and randomly selected\n1108 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 27, NO. 2, FEBRUARY 2023\na baseline date within a 1-year to 5-year window before the\nincident date. For patients who did not have recorded event of\ninterest, we considered them as negative patients, and randomly\nselected a baseline date for each of them with a guarantee of\nhaving at least 5 years of records after the baseline date. For\nall risk prediction tasks, we also excluded patients who had less\nthan 3 years of records (learning period) before the baseline date.\nB. Case Identiﬁcation\nIn this study, we focused on 5-year risk estimation of the\nincidence of HF, diabetes, CKD, and stroke. HF was deﬁned as\na composite condition of rheumatic heart failure, hypertensive\nheart and disease with (congestive) heart failure and renal failure,\nIschemic cardiomyopathy, chronic cor pulmonale, congestive\nheart failure, cardiomyopathy, left ventricular failure, and car-\ndiac, heart, or myocardial failure; Diabetes was deﬁned as a\ncomposite condition of type 1 and type 2 diabetes mellitus,\nmalnutrition-related diabetes mellitus, other speciﬁed diabetes\nmellitus, and pre-existing malnutrition-related diabetes melli-\ntus [18]; CKD included chronic kidney disease from stage 1\nto stage 5, kidney transplant failure and rejection, obstructive\nand reﬂux uropathy, acute renal failure, nephrotic syndrome,\nhypertensive renal failure, type 1 and 2 diabetes mellitus with\nkidney complications, chronic tubulo-interstitial nephritis [19];\nand the identiﬁcation of stroke used a composite condition of\ncerebrovascular diseases, subarachnoid hemorrhage, intracere-\nbral hemorrhage, sequelae of cerebrovascular disease, cerebral\ninfarction, and occlusion and stenosis of cerebral arteries [19].\nWe further deﬁned the incident HF, diabetes, CKD, and stroke\nas the ﬁrst record of corresponding disease in primary care or\nhospital admission records from any diagnostic position. The\nlist of International classiﬁcation of diseases, tenth revision [20]\n(ICD-10) codes been used to identify these four diseases can be\nfound in Supplementary.\nC. Data Processing\nWe included records from diagnoses, medications, hospital\nprocedures, general practice (GP) tests, BP measurements (both\nsystolic and diastolic pressure), drinking status, smoking status,\nand BMI. For diagnosis, we used ICD-10 as the standard format,\nthus, we mapped all diagnostic records from the primary care\n(Read [21]) to the ICD-10 level 4 as the work proposed by Has-\nsaine et al. [22]. In brief, all codes were mapped to ICD-10 using\na vocabulary provided by NHS digital [23] and SNOMED-CT\n[24]. The NHS digital vocabulary had higher priority than the\nSNEMOED-CT wherever a conﬂict occurred. For medications,\nprocedure, and test, we used British National Formulary (BNF)\n[25] coding scheme in the section level, The Ofﬁce of Population\nCensuses and Surveys (OPCS) Classiﬁcation of Interventions\nand Procedures codes, and Read code, respectively. Both drink-\ning and smoking status were recorded as categorical values,\nincluding current drinker/smoker, ex, and non. For continuous\nvalues, we included systolic pressure, diastolic pressure, and\nBMI within range 80 to 200 mmHg, 50 to 140 mmHg, and 16\nto 50 kg/m 2, respectively. Afterwards, we categorized systolic\nand diastolic pressure into bins with a 5-mmHg step size (e.g.,\n80-85 mmHg). BMI was processed the same way with a step size\n1 kg/m 2. In the end, all records of a patient were formatted as\na sequence and ordered by the event date. For the convenience\nof modelling, additional feature of each record, age, was also\ncalculated by the event date and the patient’s date of birth. All\nthe records in the pre-training dataset were used for pre-training.\nHowever, in the risk prediction dataset, only records before the\nbaseline date were used as learning period for risk prediction\ntasks.\nD. T raining and Validation of the Models\nWe used the same hyper-parameters for pre-training and all\nfour risk prediction tasks. The hyper-parameters were tuned on\nthe tuning set of HF risk prediction task. Afterwards, we reported\narea under the receiver operating characteristic (AUROC) curve\nand area under the precision recall (AUPRC) curve validated on\nthe validation set of each risk prediction task with model trained\non data from both training and tuning set.\nE. Model Derivation and Development\nFig. 1 uses a hypothetical patient to show the model archi-\ntecture of the BEHRT model [5] and the proposed Hi-BEHRT\nmodel. For the BEHRT model, four types of embeddings were\ntaken as inputs. The token embeddings were projected from all\navailable codes or categorical variables of records from diag-\nnosis, medication, procedure, test, BP measurement, drinking\nstatus, smoking status, and BMI. The age embeddings were\nrepresentations of the age in year. The segmentation embed-\ndings alternatively changed between different visits with value\n0 and 1 and the position embeddings monotonically increase\nacross different visits. The embedding of each encounter (i.e., a\nrecord and its corresponding age, segmentation, and position) is\nrepresented by the summation of the record, age, segmentation,\nand position embeddings. The inputs were followed with a\nTransformer model to extract the feature interactions, and the\nlatent representation of the ﬁrst timestep in the last layer was\nprojected by a pooling layer for risk prediction.\nInstead of extracting the interaction of records in the entire\nmedical history, we used a similar idea as the work proposed by\nPappagari el al. [13] for the Hi-BEHRT. The Hi-BEHRT model\nused a sliding window to segment the full medical history into\nsmaller segments and applied a Transformer as a local feature\nextractor to extract temporal interaction within each segment.\nBecause medical records naturally have stronger correlation\nwhen they are closer in time, we would expect the local feature\nextractor learns the most representative latent representation for\neach segment. Afterwards, we applied another Transformer as\na feature aggregator to globally summarize the local features\nextracted in all segments. Similarly, a risk prediction was made\nbased on the latent representation from a pooling layer.\nF . Pre-training\nWe applied BYOL [15] for the self-supervised pretraining.\nIt was originally designed for image representation learning. In\nLI et al.: H i-BEHRT: HIERARCHICAL TRANSFORMER-BASED MODEL FOR ACCURATE PREDICTION OF CLINICAL EVENTS 1109\nFig. 1. Model architecture for BEHRT and hierarchical BEHRT. Despite the segment representation appearing non-overlapping (right), they are\nbuilt on overlapping tokens in the sliding window (left). Age, segment, and position embedding are used for providing rich temporal contextual\ninformation for modelling.\nthis work, we implemented this idea with MLM [10] and adapted\nit to pre-train our Hi-BEHRT model.\n1) BYOL: The main idea of BYOL is that the different aug-\nmentations of the same data should have similar representations.\nTherefore, this framework has two networks for training: an on-\nline network and a target network. The online network includes\nan encoder, a projector, and a predictor; the target network has\na similar architecture as the online network but with a different\nset of weights. As shown in Fig. 2, the objective of the task is to\nminimize the mean squared loss between the output of the online\npredictor and the output of the target projector. The weights\nof the online network are updated through backpropagation;\nhowever, the weights of the target network are the exponential\nmoving average of the weights of the online network. This can\nbe shown as following,\nζ ← τζ +( 1−τ) θ,\nwhere ζ and θ are the weights of the target network and the\nonline network, respective, and τ is a decay factor between 0\nand 1.\n2) BYOL for Hierarchical BEHRT:In our study, the encoder\nwas the Hi-BEHRT model, and both the projector and the\npredictor were a one hidden layer multi-layer perceptron net-\nwork. Additionally, to adapt BYOL for EHR, we applied two\naugmentation strategies at present study for the raw EHR and the\nsegment representations, respectively. We referred them as EHR\naugmentation and segment augmentation as shown in Fig. 2 .\nThe purpose of EHR augmentation is to enrich the EHR data\nand increase the data diversity. Segment augmentation is the\nkey component of applying BYOL for Hi-BEHRT. Firstly, we\nonly applied segment augmentation to a certain proportion of\ntime steps in the latent segment representation space, similar\nto the idea of MLM in BERT [10]. Additionally, unlike the\nBYOL, which applies augmentation to both online and target\nnetwork, we only augmented the segment representations in\nthe online network and did not augment anything in the target\nFig. 2. Illustration of BYOL and BYOL for hierarchical BEHRT. BYOL\n(top) includes four components, data augmentation, encoder, projector,\nand predictor. Both online and target network have a similar model archi-\ntecture. BYOL for hierarchical BEHRT (bottom) has similar framework,\nthe segment augmentation corresponds to the augmentation in BYOL\n(top), and the segment augmentation is conducted on temporal features\nextracted by local feature extractor.\nnetwork (as shown in Fig. 2 ). The intuition is that we expect\nthe context can provide sufﬁcient information for the network\nto reproduce the representation for the augmented time steps.\nTherefore, our objective was to use the online and the target\nnetwork to reduce the dissimilarity of the representation of the\naugmented time steps and their original representations, and this\n1110 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 27, NO. 2, FEBRUARY 2023\ncan be achieved by optimizing the summation of the similarity\nloss of all augmented time steps.\n3) EHR Augmentation: EHR augmentation was applied be-\nfore sliding window. We applied random crop for the EHR\nwith a probability Pc and randomly masked each token with\na probability Pm. Random crop means randomly select a subset\nperiod of EHR. This allows to create more diverse medical\nrecords for a patient by assuming that patient enters and leaves\nthe study at a different time.\n4) Segment Augmentation: Segment augmentation was ap-\nplied to segment representations before feeding them into feature\naggregator. We either masked a segment representation by cast-\ning it to 0 or added a standard Gaussian noise to deprecate the\nlatent representation.\nG. Implementation Details\n1) Data Augmentation: For pre-training, we ﬁrstly used 50%\nprobability ( Pc) to random crop a EHR sequence, followed\nby a random mask with 20% probability ( Pm) to mask each\nrecord. We additionally had 50% probability to augment the\nsegment representation, including 85% probability of masking\na representation to 0 and 15% probability of adding standard\nGaussian noise to deprecate it.\n2) Architecture: For comparison, we implemented Hi-\nBEHRT and compared with BEHRT and two other state-of-the-\nart benchmark models, Med-BERT [6] and CEHR-BERT [7].\nThe BEHRT model used hyper-parameters searched in another\nwork [26], and we adopted the same parameters of Transformer\nfor Med-BERT and CEHR-BERT. For Hi-BEHRT, we applied\ngrid search for model tuning (more details can be found in\nSupplementary). More speciﬁcally, for BEHRT model, we used\nhidden size 150, number of attention heads 6, intermediate size\n108, number of layers 8, maximum sequence length 256, dropout\nrate 0.2, and attention dropout rate 0.3. For Hi-BEHRT model,\nwe extended the maximum sequence length to 1220, which\ncovered the full EHR for approximate 97% patients across all\nfour risk prediction tasks. We used 50 and 30 as the size and the\nstride for the sliding window, respectively. Similarly, we used\nhidden size 150, number of attention heads 6, intermediate size\n108, dropout rate 0.2, and attention dropout rate 0.3. The number\nof layers for the feature extractor and feature aggregator is 4 and\n4, respectively. Additionally, we used moving average decay\nfactor 0.996 as recommended by BYOL to update the target\nnetwork, and hidden size 150 for both projector and predictor in\nthe pre-training stage for the Hi-BEHRT model. Gaussian error\nlinear units (GELU) were used as activation function for both\nmodels.\n3) Model Complexity: For a Transformer model, the space\nand time complexity of the self-attention are О(L2+Ld) and\nО(L2d), respectively, where L represents the sequence length\nand d represents the hidden dimension size. Comparing to\nTransformer (e.g., BEHRT), the sequence lengths of the local\nfeature extractor and the feature aggregator in the Hierarchical\nTransformer (e.g., Hi-BEHRT) are the sliding window size (50)\nand the number of segments (39 for maximum sequence length\n1220 with window size 50 and stride size 30), respectively. Both\nof them are substantially shorter than the sequence length of the\nTransformer model (256). Even though the space complexity of\nthe local feature extractor increases linearly as the increase of\nthe number of segments, the Hi-BEHRT is still much smaller\nthan the BEHRT model, and the longer the EHR sequence, the\nbigger the difference.\n4) Optimization: We used Adam optimizer with a three-stage\nlearning rate scheduler [27] and early stopping strategy, over 100\nepochs, to train all models for risk prediction. The three-stage\nlearning scheduler included 10%, 40%, and 50% epochs for\nwarm-up, hold, and cosine decay, respectively. We used batch\nsize 128 and swept the hold learning rate among 5e-5, 1e-4, and\n5e-4, and reported the one with the best performance for each of\nthe risk prediction task. In terms of the early stopping strategy,\nwe stopped training once the validation loss doesn’t decrease\nfor 6 epochs. We followed the same strategy to ﬁne-tune the\npre-trained Hi-BEHRT model for the downstream risk predic-\ntion tasks. For the pre-training task, we used a similar set up as\nthe BYOL paper [15]. More speciﬁcally, we used a stochastic\ngradient decent optimizer with momentum 0.9, with a cosine\ndecay learning rate schedule, over 1000 epochs, with a warm-up\nperiod of 10 epochs. However, due to resource limitation, we\nwere only able to ﬁnish 35 epochs for a 10-day training using\nbatch size of 256 split over 2 P100 GPUs.\nIII. R ESULTS\nA. Descriptive Analysis of the Cohorts\nAfter cohort selection, we included 2844733 patients in the\npre-training cohort, and 2438352, 406381, and 1219078 pa-\ntients in the training, tuning, and validation cohort, respectively.\nMoreover, 1995 diagnosis codes, 378 medication codes, 275\ntest codes, 960 procedure codes, 24 systolic BP categories, 17\ndiastolic BP categories, and 34 BMI categories were considered\nfor modelling (we created bins from continuous measures for\ncategorization and details can be found in Methods C). The\ndescriptive analysis of the selected cohort for HF, diabetes, CKD,\nand stroke risk prediction are shown in Table I. We deﬁne the\nlearning period as the time period of EHR up to the baseline.\nDue to the random selection for the baseline for the negative\npatients and the exclusion of patients who has less than 3 years\nof learning period, there are certain variability for the number\nof patients included across different risk prediction tasks.\nB. Model Performance Evaluation\nWe assessed the performance of Hi-BEHRT on four risk pre-\ndiction tasks and compared it with the performance of BEHRT,\nMed-BERT, and CEHRT-BERT; and the Hi-BEHRT ﬁnetuned\non self-supervised pre-training task on the validation set. Ad-\nditionally, we applied subgroup analysis to evaluate model per-\nformance on patients with different learning period in respect\nof the EHR length. For patients who have EHR length longer\nthan the pre-deﬁned maximum length in BEHRT model (256),\nwe used the latest 256 EHR records in terms of the event date\nfor modelling and the Hi-BEHRT model is processed the same\nLI et al.: H i-BEHRT: HIERARCHICAL TRANSFORMER-BASED MODEL FOR ACCURATE PREDICTION OF CLINICAL EVENTS 1111\nTABLE I\nDESCRIPTIVE ANAL YSIS OF PATIENTS\nway but with maximum length 1220. We ran each experiment\nover three seeds and reported the average performance.\n1) Evaluation on Risk Prediction: Table II shows the per-\nformance comparison for the general population. With smaller\nmodel size and less model complexity, the Hi-BEHRT model\nshows superior performance and outperforms the benchmark\nTABLE II\nMODEL PERFORMANCE EVALUATION FOR RISK PREDICTION\nmodels on all risk prediction tasks with 1% to 5% and 1% to 10%\nabsolute improvement for AUROC and AUPRC, respectively.\nThe improvement is more substantial for HF, diabetes, and CKD\nrisk prediction and compared with BEHRT and Med-BERT, and\nit signiﬁcantly (p-value < 0.05 using Wilcoxon Signed-Rank\ntest [28]) outperformed the best performing benchmark model,\nCEHRT-BERT, for the risk prediction tasks. While the results\nprove the superior performance of Hi-BEHRT in risk prediction\nin the general population, its strength is to enhance the prediction\nfor patients with longer EHR.\n2) Subgroup Analysis for Patients With Different Learning\nPeriod in Respect of EHR Length: To better understand how\nBEHRT, Med-BERT, CEHR-BERT, and Hi-BEHRT handle pa-\ntients with longer learning period, we evaluated model perfor-\nmance on subgroups of patients that have EHR length 0–256 and\nlonger than 256 in learning period, respectively. Additionally,\nbecause patients with longer EHR (i.e., > 256) have higher\npercentage of positive cases, we included one more experiment,\nwhich preserved all negative samples and bootstrapped a subset\nof positive samples to create an evaluation dataset with similar\nproportion of positive cases as the subgroup of patients with\nshorter EHR (i.e., 0–256), for a fairer comparison. We repeated\nbootstrap for 5 times and reported the averaged results. Here,\n256 is the maximum EHR length a Transformer (i.e., BEHRT,\nMed-BERT, and CEHR-BERT) model can handle in our exper-\niment.\nTable III shows that Hi-BEHRT outperforms the benchmark\nmodels, especially BEHRT and Med-BERT, in almost all sub-\ngroups for all four risk prediction tasks. More speciﬁcally, for pa-\ntients who have EHR length longer than 256, Hi-BEHRT model\nshows approximate 2%–6% and 2%–11% improvement in terms\nof AUROC and AUPRC, respectively. Additionally, when com-\nparing subgroups of patients with EHR length 0–256 and >\n256 but with similar percentage of positive cases, Hi-BEHRT\nshows a more substantial improvements with the inclusion of\nmore records, proving its advantage on processing long EHR.\nFor instance, for stroke risk prediction, where Hi-BEHRT shows\nminor improvement in the general population ( Table II), the Hi-\nBEHRT has similar performance as the benchmark models for\npatients with shorter sequence length, while the improvement for\npatients with longer EHR is more substantial. Such observation\nreassures the viability of Hi-BEHRT for handling patients with\nlonger EHR. Moreover, we also notice for HF risk prediction,\n1112 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 27, NO. 2, FEBRUARY 2023\nTABLE III\nSUBGROUP ANAL YSIS FOR PATIENTS WITH DIFFERENT EHR L ENGTH IN\nLEARNING PERIOD\nthe performance of BEHRT, Med-BERT, and CEHR-BERT is\nconsiderably worse than the Hi-BEHRT on short EHR subgroup.\nEven within the models themselves, their performance on short\nEHR subgroup is far worse than their performance on long\nEHR group. This is quite unusual considering the differences\nof its performance between long and short EHR subgroups are\nrelatively small for other diseases. One potential reason is HF is\nan extremely imbalanced task with only 4.7% positive cases and\nmost of the positive cases are within the subgroup of patients\nwho have long EHR. Because the benchmark models used global\nattention and took the latest 256 records for risk prediction on\npatients with long EHR, and these patients could have different\ncontextual information compared to patients with short EHR.\nTherefore, the training of the benchmark models can be driven to\nhave better discrimination performance for identifying positive\ncases in long EHR group than in the short EHR group. However,\nwith the inclusion of almost the entire EHR sequence and using\nsliding window to constrain the receptive ﬁeld for the local\nfeature extractor, Hi-BEHRT has a better focus on identifying\nlocal temporal features and distinguishing patterns between long\nand short EHR sequence, leading to a more comprehensive risk\nestimation under both circumstances.\n3) Semi-supervised T raining for Risk Prediction:Next, we\nevaluated the performance of the Hi-BEHRT obtained when\nTABLE IV\nPERFORMANCE COMPARISON ON TRANSFERRING PRE-TRAINED\nREPRESENTATIONS TO RISK PREDICTION TASK\nﬁne-tuning pre-trained representation on the risk prediction task\nwith a small subset of the training dataset (i.e., both training and\ntuning sets). We bootstrapped 1% and 5% of the training dataset\nfor three runs and reported the averaged AUROC and AUPRC\nevaluated on the validation set. Table IVshows that comparing to\nHi-BEHRT model trained from scratch, the pre-trained represen-\ntation can provide substantial improvement when ﬁne-tuning on\nsmall dataset. In general, for models ﬁne-tuned on the pre-trained\nrepresentation, they can achieve similar performance as the\nmodel trained on 5% of training dataset without pre-training\nwhen only been trained on 1% of the training dataset. Ad-\nditionally, the improvement is clearer when there is a higher\npercentage of positive cases. For example, CKD with 9.3% of\npositive cases on 5% subset of training dataset improves 7% and\n3% for AUPRC and AUROC, respectively; and stroke with 13%\nof positive cases on 5% subset of training dataset improves 12%\nand 7% for AUPRC and AUROC, respectively, comparing to the\nHi-BEHRT model without pre-training.\nC. Ablation Analysis\nIn this section, we use HF risk prediction task as an example\nto present ablations on Hi-BEHRT to give an illustration of its\nbehavior and performance. We reported the average performance\nof models trained over three random seeds.\n1) T raining Size and Performance:In Table IV , we ﬁne-\ntuned the pre-trained representations of Hi-BEHRT over 1%\nand 5% of the training dataset. In this section, we used HF risk\nprediction as an example to further explore the difference of\nmodel performance between with and without pre-training over\n1%, 5%, 10%, 20%, 50%, and 100% of the training dataset. As\nshown in Fig. 3 , model with pre-training has better performance\non small dataset (i.e., 1%, 5%, and 10%), and the model with\npre-training substantially outperforms the model without pre-\npretraining for 31% and 4% in terms of AUPRC and AUROC,\nrespectively, on a subset of training set with 10% of samples and\ntheir performance shows strong diminish returns of pretraining\nLI et al.: H i-BEHRT: HIERARCHICAL TRANSFORMER-BASED MODEL FOR ACCURATE PREDICTION OF CLINICAL EVENTS 1113\nFig. 3. Performance of Hi-BEHRT trained on a fraction of training\ndataset.\nFig. 4. Ablation on modalities. Evaluate Hi-BEHRT on patients with\ninclusion of diagnosis (D), medication (M), procedure (P), test (T), BP\nmeasurement (B), BMI (I), smoking status (S), and drinking status (A).\nat about 20% of training data size. In general, all metrics from\nboth models trained with and without pre-training follow the\npower law learning curve [29], which includes the small data\nregion (model struggle to learn from a small number of training\nsamples), power-law region (a region that substantially improves\nmodel performance with inclusion of more training samples),\nand irreducible error region (a region that represents the lower-\nbound error and model will be unable to improve if within this\nregion). The ﬁgure shows the model with pre-training can reach\nthe power-law region with smaller sample size. Additionally,\nﬁgure also shows that our model reaches the irreducible error\nregion with around 50% samples.\n2) Ablation of Modality: EHR from different modalities can\npotentially provide different information for modelling. Di-\nagnosis and medication are commonly used modality in risk\nprediction task. In this experiment, we investigated additional\nEHR from procedure, test, BP measurement, BMI measurement,\ndrinking and smoking status, and compared how the richness\nof the EHR affected the model performance. To this end, we\nused the Hi-BEHRT model reported in Table II and evaluated\nthe model performance on all patients with the inclusion of one\nadditional modality a time besides diagnosis and medication.\nAs shown in Fig. 4, we see a trend that with the inclusion of\nmore modalities, the model performance in terms of AUPRC\nand AUROC improves. However, the contribution of a modality\nis highly related to the frequency of that modality in the dataset\nand its importance. Firstly, model greatly improves with the\ninclusion of test, BP measurement, and smoking status. One\nof the most obvious reasons is that all of them have relatively\nhigh frequency of recording in the dataset ( Table I ). On the\ncontrary, BMI, drinking status, and procedure have very poor\ncontribution due to their scarcity, and with the inclusion of other\nmodalities, their occurrence and contribution become more neg-\nligible. More speciﬁcally, for example, there were 960 procedure\ncodes included in the dataset, however, there were only 4.4\nprocedure related records on average for each patient. Therefore,\nthe scarcity of the procedure codes in the dataset can limit its\ncontribution to the prediction. Secondly, with similar frequency\nof recording, the inclusion of BP measurement shows greater\nimprovement than the inclusion of smoking status, indicating\nBP measurement is a more important modality than smoking\nstatus for HF risk prediction.\nIV. D ISCUSSION\nIn this article, we proposed an enhanced BEHRT model,\nHi-BEHRT, for risk prediction. It can incorporate long EHR\nsequences from various modalities, address the shortcomings\nof vanilla Transformers in processing long sequential data, and\navoid missing important historical information in risk predic-\ntion. With the capability of using the full medical records, Hi-\nBEHRT outperformed BEHRT, Med-BERT, and CEHR-BERT\nin terms of AUROC (1%-5%) and AUPRC (1%-8%) for all four\ninvestigated (HF, diabetes, CKD, and stroke) risk prediction\ntasks. Despite the improvement of Hi-BEHRT for the general\npopulation, its strength was to further enhance the risk prediction\nfor patients with long EHR sequence. The experiments demon-\nstrated a 2%-6% and 2%-11% improvement in terms of AUROC\nand AUPRC, respectively, among patients with long EHR across\nfour risk prediction tasks.\nOur work has several novelties. Unlike most existing deep\nlearning risk models that focused on using diagnoses only [5],\n[6], in this work, we investigated EHR from other modalities,\n1114 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 27, NO. 2, FEBRUARY 2023\nincluding medication, procedure, test, BP measurement, BMI,\ndrinking status, and smoking status. Since they provided addi-\ntional information for modelling, as expected, the Hi-BEHRT\nmodel achieved better performance with the inclusion of more\nmodalities. Therefore, our work highlighted the great beneﬁt\nof including rich medical history for accurate risk prediction.\nHowever, we further observed that the contribution of a modality\nto the model performance is in general highly related to its\nfrequency of recording in the dataset. It is partially due to\nthe fact that higher frequencies of recording can provide more\ninformation for prediction. Another possible explanation is that\nthe information provided by modalities with low frequency are\noverpowered by the modalities with much higher frequency. This\nphenomenon has been described in the natural language process-\ning literature for the embedding of words as well which tend to\nbe biased towards higher word frequencies [30]. Therefore, one\npotential future work will be to investigate how to incorporate\nfeatures or modalities with low frequency in a more meaningful\nway for risk prediction.\nFurthermore, to better understand the advantage of including\npatients’ complete medical history for modelling, we compared\nHi-BEHRT to the benchmark models in terms of handling pa-\ntients with different learning periods. We conducted a subgroup\nanalysis to evaluate model performance on patients who have\nEHR length within the capacity of Transformer models (i.e., less\nthan or equal to 256 in our study) and longer than the Transformer\nmodels’ capacity (i.e., more than 256) in the learning period.\nWe found that the Hi-BEHRT model showed similar or better\nperformance than the benchmark models on risk prediction tasks\nfor patients within the relatively short EHR length group, but\nit greatly improved model performance with the inclusion of\nmore records (i.e., > 256). However, due to the limitation of\nsequence length in the benchmark Transformer models, the\ndifference of model performance between patients with long\nEHR and short EHR is relatively small. Additionally, we notice\nfor very imbalanced outcomes, for example HF, the majority of\nthe positive cases occur in patients with longer EHR and these\npatients can have different contextual patterns compared to the\npatients with short EHR. By making risk prediction with only\na fraction of the latest records in the benchmark Transformer\nmodels when patients have long EHR sequence, they treated\npatients with long EHR the same way as making prediction\nfor patient with short EHR sequence. Therefore, the models,\nwhich rely on the global attention, can be driven to have better\ndiscrimination performance for the positive cases with long EHR\nrecords in the training and have relatively poor capability of\nidentifying positive cases with short EHR sequence. On the\ncontrary, with the inclusion of the entire EHR, together with\nlocal feature extractor and global feature aggregator to identify\ntemporal and global patterns, the Hi-BEHRT model is more\ncapable of distinguishing different patterns of positive cases in\nboth long and short EHR sequences. Considering the majority\n(70%) of the population have relatively short EHR length (less\nthan 256) in our risk prediction tasks and probably in most of\nthe cases in reality, this can be an important additional feature\nof our proposed model.\nIn addition to model architecture, we also evaluated the usabil-\nity of a contrastive learning pre-training strategy, BYOL, in this\nwork. We combined the framework, which was originally de-\nsigned for image pre-training, with the MLM task, and adapted\nit to pre-train our sequential model. With the pre-training, the\nHi-BEHRT model can achieve similar performance using only\n1% of training data as the model trained without pre-training\nusing 5% of training data. With additional ablation analysis,\nwe concluded that the pre-training can potentially expanded the\npower-law region [29] and allowed the model to reach power-law\nregion with smaller data size. However, our results also indicated\nthat the model performance almost saturated when using 50%\nof training dataset. It means the model achieves the irreducible\nerror region. Future work should investigate more robust model\narchitectures to shift the power-law curve and improve the model\naccuracy.\nOne of the major contributions of this work is the provi-\nsion of a framework for risk prediction with the inclusion of\nlong and comprehensive EHR. With the growing accessibil-\nity and usability of EHR systems, risk prediction using long\nEHR can be inevitable and have important implications for\nmedical practice. To our best knowledge, long sequence mod-\nelling and its application in the context of healthcare and EHR\nremains unexplored. Our work proposed a potential solution\nto tackle this problem and investigated its beneﬁt comparing\nto model that makes prediction using only a fraction of the\nEHR. Moreover, we provided a self-supervised pre-training\nframework for the proposed model, and pre-training can adapt\nrisk prediction model to handle tasks with less training data\navailable, which is highly desired in most of the scenarios.\nWe also encourage future work to further explore other long\nsequence modelling strategies (e.g., Longformer [11]) for EHR\nmodelling.\nOur study also has limitations. First, we focused on the risk\nof HF, diabetes, CKD, and stroke. As such, the conclusion may\nnot generalize to other diseases. Additionally, our work relied on\ninternal validation and the model performance under data shifts\nor in the external cohorts requires further investigation.\nAPPENDIX\nA. Additional Information on Dataset\nIn this section, we provide more information on modalities\nthat are not commonly included in the modelling. More specif-\nically, we will introduce procedure and test.\n1) Procedure: Procedure is CPRD linked data collected\nfrom Hospital Episode Statistics (HES) Admitted Patient Care\n(EHS APC) data. It is recorded at the point of admission to, or\nattendances at NHS healthcare providers. All procedure infor-\nmation is coded using the U.K. Ofﬁce of Population, Census\nand Surveys classiﬁcation (OPCS) 4.6, and procedures that are\nnot covered by OPCS code is not included in the system. Each\nrecord in the system is speciﬁed with a start and an end date,\nas well as event date. We used OPCS code and event date to\nstructure the timeline of a patient’s EHR history for modelling.\nLI et al.: H i-BEHRT: HIERARCHICAL TRANSFORMER-BASED MODEL FOR ACCURATE PREDICTION OF CLINICAL EVENTS 1115\n2) T est:Test is recorded in the CPRD test table and coded as\nRead code. It includes information on history/symptoms, exami-\nnation/signs, diagnostic procedures, and laboratory procedures.\nIn the experiment, we only used the information in the Read\ncode level, which represents what examinations or procedures\nare carried out. More detailed quantitative information was\nexcluded.\nB. Clinical Codes for HF , Diabetes, CKD, and Stroke\nTABLE V\nICD-10 C ODES USED TO IDENTIFY PATIENTS WITH HEART FAILURE IN\nHOSPITAL DISCHARGE RECORDS AND GENERAL PRACTICE RECORDS\nTABLE VI\nICD-10 C ODES USED TO IDENTIFY PATIENTS WITH DIABETES IN HOSPITAL\nDISCHARGE RECORDS AND GENERAL PRACTICE RECORDS\nC. Model Evaluation Stratiﬁed By Baseline Age\nWe evaluated model performance stratiﬁed by the baseline\nage. The comparison was conducted on three subgroups of\npatients: 1) patients with baseline age between 35 and 50 years\nold (young adult); 2) patients with baseline age between 50 and\n70 years old (middle-aged adult), and 3) patients with baseline\nage 70–90 years old (older adult). Table IX shows that the\nhierarchical BEHRT model has better performance across all\nsubgroups, and it substantially outperforms for BEHRT model\non HF and diabetes risk prediction tasks, especially for patients\nwith younger age.\nTABLE VII\nICD-10 C ODES USED TO IDENTIFY PATIENTS WITH CKD IN HOSPITAL\nDISCHARGE RECORDS AND GENERAL PRACTICE RECORDS\nTABLE VIII\nICD-10 C ODES USED TO IDENTIFY PATIENTS WITH STROKE IN HOSPITAL\nDISCHARGE RECORDS AND GENERAL PRACTICE RECORDS\nD. Size and Overlap of Sliding Window\nFor Hi-BEHRT model, we used sliding window to segment\nthe raw EHR into segments. As shown in Table X when window\nsize is relatively small (i.e., 50), the size of the stride does not\nhave signiﬁcant impact in terms of predictive performance, and\nthe bigger stride size can potentially decrease the number of\nsegments and reduce model complexity. However, for the larger\nwindow size (i.e., 100), the stride size becomes more important,\nand some level of overlap between segments is necessary. With-\nout any overlap for window size 100, the AUPRC decreases\n4% comparing to the model with stride size 50. Additionally,\nthe analysis shows that not larger window size always the\nbetter choice. For instance, AUPRC of window size 100 without\noverlap decreases 2% comparing to AURPC of window size\n50 without overlap. Without overlap, larger window can lead\nto shorter length in the segment level, and a balance between\nwindow size and length of segment might be more preferred in\nthe hierarchical structure.\n1116 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 27, NO. 2, FEBRUARY 2023\nTABLE IX\nBASELINE AGE STRATIFIED SUBGROUP ANAL YSIS\nTABLE X\nPERFORMANCE OF HF R ISK PREDICTION WITH DIFFERENT WINDOW AND\nSTRIDE SIZE\nTABLE XI\nHI-BEHRT HYPER-PARAMETER TUNING\nE. Hyper-Parameter T uning\nWe set up hierarchical BEHRT with similar hyper-parameters\nas the BEHRT model and used it as a reference model to\ntune the hidden size and intermediate size of the Transformer.\nMore speciﬁcally, we applied grid search for hidden size among\n[90, 150, 240] and intermediate size among [108, 256]. All\nexperiments were conducted on the 5-year HF risk prediction\ntask. Table XI shows that hidden size 150 and intermediate size\n108 can achieve similar performance as the model with larger\nsize.\nF . Evaluation for Multiple Levels of Hierarchy\nIn this section, we investigated how the number of levels of\nhierarchy in Hi-BEHRT can inﬂuence the model performance\nin risk prediction. Speciﬁcally, we compared the performance\nof Hi-BEHRT with two and three levels of hierarchy. This\nis because each additional level can substantially reduce the\nsequence length. For instance, a sequence with maximum length\n1225 would reduce to sequence length 118 with window size\n50 and stride size 10 after the ﬁrst level of hierarchy and would\nfurther reduce to 7 after the second level of hierarchy. Therefore,\nour dataset limited the number of levels we can investigate, and it\nwould not make sense to investigate Hi-BEHRT with more than\nthree levels of hierarchy. We encourage future work to replicate\nour work to more comprehensively investigate Hi-BEHRT with\nmore levels of hierarchy. In our experiment, we only modiﬁed\nthe feature extractor and kept the total number of layers in feature\nextractor the same for both comparators. More speciﬁcally, the\ntwo-level Hi-BEHRT had one level of hierarchy with four layers\nof Transformer for the extractor while the three-level Hi-BEHRT\nincluded two levels of hierarchy with a two-layer Transformer\nfor each hierarchy. Both comparators used window size 50 and\nstride size 10 and the rest parameters were the same as reported\nin the manuscript. The results show that both models achieved\nAUROC 0.96 and AUPRC 0.76 for HF risk prediction, and\nthere is no material difference between two-level and three-level\nHi-BEHRT in our dataset.\nACKNOWLEDGMENT\nThe funders had no role in study design, data collection and\nanalysis, decision to publish, or preparation of the manuscript.\nThe views expressed are those of the authors and not necessar-\nily those of the OMS, the BHF, the GCRF, the NIHR or the\nDepartment of Health and Social Care.\nREFERENCES\n[1] T.-P. van Staa, M. Gulliford, E. S.-W. Ng, B. Goldacre, and L. Smeeth, “Pre-\ndiction of cardiovascular risk using Framingham, ASSIGN and QRISK2:\nHow well do they predict individual rather than population risk?,” PLoS\nOne, vol. 9, no. 10, 2014, Art. no. e106455.\n[2] B. A. Goldstein, A. M. Navar, M. J. Pencina, and J. Ioannidis, “Opportu-\nnities and challenges in developing risk prediction models with electronic\nhealth records data: A systematic review,” J. Amer. Med. Inform. Assoc.,\nvol. 24, no. 1, pp. 198–208, 2017.\n[3] P. Nguyen, T. Tran, N. Wickramasinghe, and S. Venkatesh, “Deepr: A\nconvolutional net for medical records,” IEEE J. Biomed. Heal. Inform.,\nvol. 21, no. 1, pp. 22–30, Jan. 2017, doi: 10.1109/JBHI.2016.2633963.\n[4] E. Choi, M. T. Bahadori, J. A. Kulas, A. Schuetz, W. F. Stewart, and J. Sun,\n“RETAIN: An interpretable predictive model for healthcare using reverse\ntime attention mechanism,” in Proc. Adv. Neural Inf. Process. Syst., 2016,\npp. 3512–3520.\n[5] Y . Li et al., “BEHRT: Transformer for electronic health records,” Sci. Rep.,\nvol. 10, no. 1, 2020, Art. no. 7155, doi: 10.1038/s41598-020-62922-y.\n[6] L. Rasmy, Y . Xiang, Z. Xie, C. Tao, and D. Zhi, “Med-BERT: Pretrained\ncontextualized embeddings on large-scale structured electronic health\nrecords for disease prediction,” npj Digit. Med., vol. 4, no. 1, 2021,\nArt. no. 86, doi: 10.1038/s41746-021-00455-y.\n[7] C. Pang et al., “CEHR-BERT: Incorporating temporal information\nfrom structured EHR data to improve prediction tasks,” Proc. Mach.\nLearn. Health, vol. 158, pp. 239–260, 2021. [Online]. Available: https:\n//proceedings.mlr.press/v158/pang21a.html\n[8] J. M. Banda, M. Seneviratne, T. Hernandez-Boussard, and N. H. Shah, “Ad-\nvances in electronic phenotyping: From rule-based deﬁnitions to machine\nlearning models,” Annu. Rev. Biomed. Data Sci., vol. 1, no. 1, pp. 53–68,\n2018, doi: 10.1146/annurev-biodatasci-080917-013315.\n[9] A. Vaswani et al., “Attention is all you need,” in Proc. Adv. Neural Inf.\nProcess. Syst., 2017, pp. 5999–6009.\n[10] J. Devlin, M.-W. Chang, K. Lee, K. T. Google, and A. I. Lan-\nguage, “BERT: Pre-training of deep bidirectional transformers for lan-\nguage understanding,” in Proc. Conf. North , 2019, pp. 4171–4186,\ndoi: 10.18653/V1/N19-1423.\n[11] I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long-document\ntransformer,” 2020, arXiv:2004.05150.\n[12] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma, “Linformer: Self-\nattention with linear complexity,” 2020, arXiv:2006.04768.\n[13] R. Pappagari, P. Zelasko, J. Villalba, Y . Carmiel, and N. Dehak, “Hier-\narchical transformers for long document classiﬁcation,” in Proc. IEEE\nAutom. Speech Recognit. Understanding Workshop, 2019, pp. 838–844,\ndoi: 10.1109/ASRU46091.2019.9003958.\nLI et al.: H i-BEHRT: HIERARCHICAL TRANSFORMER-BASED MODEL FOR ACCURATE PREDICTION OF CLINICAL EVENTS 1117\n[14] L. Rasmy, Y . Xiang, Z. Xie, C. Tao, and D. Zhi, “Med-BERT: Pretrained\ncontextualized embeddings on large-scale structured electronic health\nrecords for disease prediction,” NPJ Digit. Med., vol. 4, no. 1, p. 86, 2021,\ndoi: 10.1038/s41746-021-00455-y.\n[15] J.-B. Grill et al., “Bootstrap your own latent—A new approach to\nself-supervised learning,” Adv. Neural Inf. Process. Syst. , vol. 33,\npp. 21271–21284, 2020. [Online]. Available: https://proceedings.neurips.\ncc/paper/2020/ﬁle/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf\n[16] E. Herrett et al., “Data resource proﬁle: Clinical practice research\ndatalink (CPRD),” Int. J. Epidemiol., vol. 44, no. 3, pp. 827–836, 2015,\ndoi: 10.1093/ije/dyv098.\n[17] N. Conrad et al., “Temporal trends and patterns in heart failure incidence:\nA population-based study of 4 million individuals,” Lancet, vol. 391,\nno. 10120, pp. 572–580, 2018.\n[18] V . Kuan et al., “A chronological map of 308 physical and mental\nhealth conditions from 4 million individuals in the English National\nHealth Service,” Lancet Digit. Heal., vol. 1, no. 2, pp. e63–e77, 2019,\ndoi: 10.1016/S2589-7500(19)30012-3.\n[19] J. Tran et al., “Patterns and temporal trends of comorbidity among adult\npatients with incident cardiovascular disease in the U.K. between 2000\nand 2014: A population-based cohort study,” PLoS Med., vol. 15, no. 3,\n2018, Art. no. e1002513.\n[20] G. R. Bramer, “International statistical classiﬁcation of diseases and\nrelated health problems - Tenth revision,” World Health Statist. Quarterly,\nvol. 41, no. 1, pp. 32–36, 1988.\n[21] J. Chisholm, “The read clinical classiﬁcation,” BMJ Br. Med. J., vol. 300,\nno. 6732, 1990, Art. no. 1092.\n[22] A. Hassaine et al., “Learning multimorbidity patterns from electronic\nhealth records using non-negative matrix factorisation.” Jul. 2019, Ac-\ncessed: Jun. 22, 2020. [Online]. Available: http://arxiv.org/abs/1907.\n08577\n[23] NHS-Digital, “Read-ICD10 cross map,” 2013. [Online]. Available: https:\n//nhs-digital.citizenspace.com/uktc/crossmaps/\n[24] NHS-Digital, “SNOMED codes,” 2022. [Online]. Available: https://\ndigital.nhs.uk/services/terminology-and-classiﬁcations/snomed-ct\n[25] J. F. Committee, “British National Formulary,” Accessed: Feb. 27, 2020.\n[Online]. Available: https://bnf.nice.org.uk/drugs/paracetamol/\n[26] Y . Li et al., “Deep Bayesian Gaussian processes for uncertainty estimation\nin electronic health records,” Sci. Rep., vol. 11, no. 1, p. 20685, 2021,\ndoi: 10.1038/s41598-021-00144-6.\n[27] A. Baevski, H. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A\nframework for self-supervised learning of speech representations,” inProc.\nAdv. Neural Inf. Process. Syst., 2020, pp. 12449–12460.\n[28] J. Demšar, “Statistical comparisons of classiﬁers over multiple data sets,”\nJ. Mach. Learn. Res., vol. 7, pp. 1–30, 2006.\n[29] J. Hestness et al., “Deep learning scaling is predictable, empirically,” 2017,\narXiv:1712.00409.\n[30] C. Gong, D. He, X. Tan, T. Qin, L. Wang, and T.-Y . Liu, “FRAGE:\nFrequency-agnostic word representation,” in Proc. 32nd Int. Conf. Neural\nInf. Process. Syst., 2018, pp. 1341–1352.",
  "topic": "Health records",
  "concepts": [
    {
      "name": "Health records",
      "score": 0.6898906826972961
    },
    {
      "name": "Computer science",
      "score": 0.5984150767326355
    },
    {
      "name": "Electronic health record",
      "score": 0.5090574622154236
    },
    {
      "name": "Data modeling",
      "score": 0.4557305574417114
    },
    {
      "name": "Transformer",
      "score": 0.4382556974887848
    },
    {
      "name": "Data mining",
      "score": 0.40757471323013306
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38425737619400024
    },
    {
      "name": "Engineering",
      "score": 0.14182201027870178
    },
    {
      "name": "Health care",
      "score": 0.1334874927997589
    },
    {
      "name": "Economic growth",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I40120149",
      "name": "University of Oxford",
      "country": "GB"
    }
  ],
  "cited_by": 100
}