{
    "title": "Industrial applications of large language models",
    "url": "https://openalex.org/W4409626943",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2172351028",
            "name": "Mubashar Raza",
            "affiliations": [
                "COMSATS University Islamabad"
            ]
        },
        {
            "id": "https://openalex.org/A2883458953",
            "name": "Zarmina Jahangir",
            "affiliations": [
                "Riphah International University"
            ]
        },
        {
            "id": "https://openalex.org/A2890135266",
            "name": "Muhammad Bilal Riaz",
            "affiliations": [
                "Applied Science Private University",
                "VSB - Technical University of Ostrava"
            ]
        },
        {
            "id": "https://openalex.org/A2433911023",
            "name": "Muhammad Jasim Saeed",
            "affiliations": [
                "Riphah International University"
            ]
        },
        {
            "id": "https://openalex.org/A2955161672",
            "name": "Muhammad Awais Sattar",
            "affiliations": [
                "Luleå University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2172351028",
            "name": "Mubashar Raza",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2883458953",
            "name": "Zarmina Jahangir",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2890135266",
            "name": "Muhammad Bilal Riaz",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2433911023",
            "name": "Muhammad Jasim Saeed",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2955161672",
            "name": "Muhammad Awais Sattar",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2747680751",
        "https://openalex.org/W4318618520",
        "https://openalex.org/W3144293453",
        "https://openalex.org/W4386764545",
        "https://openalex.org/W3152268000",
        "https://openalex.org/W2963704628",
        "https://openalex.org/W3044438666",
        "https://openalex.org/W4318069287",
        "https://openalex.org/W2982784916",
        "https://openalex.org/W3163832451",
        "https://openalex.org/W4367365458",
        "https://openalex.org/W6996569244",
        "https://openalex.org/W3175170601",
        "https://openalex.org/W2885195348",
        "https://openalex.org/W3163652268",
        "https://openalex.org/W2977526300",
        "https://openalex.org/W4394828356",
        "https://openalex.org/W3169483174",
        "https://openalex.org/W6600129504",
        "https://openalex.org/W3164670515",
        "https://openalex.org/W4394862623",
        "https://openalex.org/W4399995547",
        "https://openalex.org/W4406272066",
        "https://openalex.org/W4392782765",
        "https://openalex.org/W6765899115",
        "https://openalex.org/W2039133703",
        "https://openalex.org/W4387304047",
        "https://openalex.org/W2944851425",
        "https://openalex.org/W2573587735",
        "https://openalex.org/W3215058526",
        "https://openalex.org/W4239025696",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W3144041472",
        "https://openalex.org/W2202042777",
        "https://openalex.org/W4285992209",
        "https://openalex.org/W3092047717",
        "https://openalex.org/W4283654020",
        "https://openalex.org/W4360910032",
        "https://openalex.org/W4366478678",
        "https://openalex.org/W4323271987",
        "https://openalex.org/W6757260900",
        "https://openalex.org/W4366779715",
        "https://openalex.org/W4387511974",
        "https://openalex.org/W4378983216",
        "https://openalex.org/W4386869376",
        "https://openalex.org/W4389977189",
        "https://openalex.org/W4388994228",
        "https://openalex.org/W4400962082",
        "https://openalex.org/W6601867051",
        "https://openalex.org/W4400118952",
        "https://openalex.org/W6600376255",
        "https://openalex.org/W4392196220",
        "https://openalex.org/W4385632485",
        "https://openalex.org/W4399207330",
        "https://openalex.org/W6600225990",
        "https://openalex.org/W4387809804",
        "https://openalex.org/W4396918564",
        "https://openalex.org/W6600686112",
        "https://openalex.org/W4404781337",
        "https://openalex.org/W4391232619",
        "https://openalex.org/W4393186514",
        "https://openalex.org/W4404002404",
        "https://openalex.org/W4392384650",
        "https://openalex.org/W4400531852",
        "https://openalex.org/W4400525231",
        "https://openalex.org/W6605361631",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W4391129153",
        "https://openalex.org/W6607643177",
        "https://openalex.org/W4294631187",
        "https://openalex.org/W4406260324",
        "https://openalex.org/W4366292751",
        "https://openalex.org/W4402648519",
        "https://openalex.org/W4403334020",
        "https://openalex.org/W4404202190",
        "https://openalex.org/W4403821960",
        "https://openalex.org/W6817647423",
        "https://openalex.org/W4400969772",
        "https://openalex.org/W6830788403",
        "https://openalex.org/W4409248734",
        "https://openalex.org/W4401863475",
        "https://openalex.org/W4404782882",
        "https://openalex.org/W4391136507",
        "https://openalex.org/W4396832160",
        "https://openalex.org/W4382397550",
        "https://openalex.org/W3104163040",
        "https://openalex.org/W4407410276",
        "https://openalex.org/W4226031686",
        "https://openalex.org/W4404132493",
        "https://openalex.org/W4403223094",
        "https://openalex.org/W4392943273",
        "https://openalex.org/W4366368023",
        "https://openalex.org/W4386187806",
        "https://openalex.org/W4312257891",
        "https://openalex.org/W6702248584",
        "https://openalex.org/W4391306611",
        "https://openalex.org/W4281673407",
        "https://openalex.org/W4385338706",
        "https://openalex.org/W4390187104",
        "https://openalex.org/W3087432510",
        "https://openalex.org/W3101284630",
        "https://openalex.org/W3161997752",
        "https://openalex.org/W3194782062",
        "https://openalex.org/W2950813464",
        "https://openalex.org/W6815217517",
        "https://openalex.org/W3128376221",
        "https://openalex.org/W3133729339",
        "https://openalex.org/W2588932259",
        "https://openalex.org/W4401043365",
        "https://openalex.org/W4401478472",
        "https://openalex.org/W6600339963",
        "https://openalex.org/W6830301964",
        "https://openalex.org/W4396876379",
        "https://openalex.org/W6600553734",
        "https://openalex.org/W4385227045",
        "https://openalex.org/W4390971165",
        "https://openalex.org/W4390992195",
        "https://openalex.org/W4391377921",
        "https://openalex.org/W4385059308",
        "https://openalex.org/W4391770734",
        "https://openalex.org/W4400958765",
        "https://openalex.org/W4386293515",
        "https://openalex.org/W4281622825",
        "https://openalex.org/W2988111968",
        "https://openalex.org/W4401821398",
        "https://openalex.org/W4401386758",
        "https://openalex.org/W4402915134",
        "https://openalex.org/W4392851477",
        "https://openalex.org/W4391849646",
        "https://openalex.org/W4404783726",
        "https://openalex.org/W4390240279",
        "https://openalex.org/W4400113714",
        "https://openalex.org/W4372055459",
        "https://openalex.org/W4402969864",
        "https://openalex.org/W4400442843",
        "https://openalex.org/W4399209296",
        "https://openalex.org/W4391467422",
        "https://openalex.org/W4394580929",
        "https://openalex.org/W4401106983",
        "https://openalex.org/W4401541163",
        "https://openalex.org/W4396843958",
        "https://openalex.org/W4400529373",
        "https://openalex.org/W4402100595",
        "https://openalex.org/W4394630908",
        "https://openalex.org/W4392297482",
        "https://openalex.org/W4392383784",
        "https://openalex.org/W4391071102",
        "https://openalex.org/W4401834466",
        "https://openalex.org/W4401950166",
        "https://openalex.org/W4387163553",
        "https://openalex.org/W4396833607",
        "https://openalex.org/W4400207472",
        "https://openalex.org/W4406343535",
        "https://openalex.org/W4399211904",
        "https://openalex.org/W4401615828",
        "https://openalex.org/W4394947904",
        "https://openalex.org/W4405257814",
        "https://openalex.org/W4400904647",
        "https://openalex.org/W4390231192",
        "https://openalex.org/W4388994251",
        "https://openalex.org/W4402836789",
        "https://openalex.org/W4390581705",
        "https://openalex.org/W4404880017",
        "https://openalex.org/W4399363358",
        "https://openalex.org/W4406302454",
        "https://openalex.org/W4388999266",
        "https://openalex.org/W4386256552",
        "https://openalex.org/W4385562710",
        "https://openalex.org/W4405204459",
        "https://openalex.org/W4402439937",
        "https://openalex.org/W4386729429",
        "https://openalex.org/W4405755749",
        "https://openalex.org/W4401695977",
        "https://openalex.org/W4392873136",
        "https://openalex.org/W6636398873",
        "https://openalex.org/W6609612681",
        "https://openalex.org/W4392402185",
        "https://openalex.org/W4389519117",
        "https://openalex.org/W4394999670",
        "https://openalex.org/W4400113208",
        "https://openalex.org/W4394855484",
        "https://openalex.org/W4391733222",
        "https://openalex.org/W4381848566",
        "https://openalex.org/W3202833319",
        "https://openalex.org/W4401857670"
    ],
    "abstract": null,
    "full_text": "Industrial applications of large \nlanguage models\nMubashar Raza1, Zarmina Jahangir2, Muhammad Bilal Riaz4,5, Muhammad Jasim Saeed2 & \nMuhammad Awais Sattar3\nLarge language models (LLMs) are artificial intelligence (AI) based computational models designed \nto understand and generate human like text. With billions of training parameters, LLMs excel \nin identifying intricate language patterns, enabling remarkable performance across a variety of \nnatural language processing (NLP) tasks. After the introduction of transformer architectures, they \nare impacting the industry with their text generation capabilities. LLMs play an innovative role \nacross various industries by automating NLP tasks. In healthcare, they assist in diagnosing diseases, \npersonalizing treatment plans, and managing patient data. LLMs provide predictive maintenance \nin automotive industry. LLMs provide recommendation systems, and consumer behavior analyzers. \nLLMs facilitates researchers and offer personalized learning experiences in education. In finance and \nbanking, LLMs are used for fraud detection, customer service automation, and risk management. LLMs \nare driving significant advancements across the industries by automating tasks, improving accuracy, \nand providing deeper insights. Despite these advancements, LLMs face challenges such as ethical \nconcerns, biases in training data, and significant computational resource requirements, which must \nbe addressed to ensure impartial and sustainable deployment. This study provides a comprehensive \nanalysis of LLMs, their evolution, and their diverse applications across industries, offering researchers \nvaluable insights into their transformative potential and the accompanying limitations.\nKeywords Large Language models, LLMs, NLP , Transformers\nCommunication has vital importance in each and every aspect of human life. There are a number of languages \nthat are used by humans to communicate with each other. In this era of advanced and innovative technologies. \nWe required efficient ways to communicate with machines1,2. Natural language processing (NLP) is a fragment \nof AI that provides mechanism for humans to interact with computers and machines3–5. NLP involves enabling \ncomputers to understand, interpret, and generate human language in a way that is both meaningful and useful. \nNLP combines computational linguistics with AI techniques to process and analyze large amounts of natural \nlanguage data. NLP encompasses a variety of tasks, including text analysis, extraction of meaningful information, \nmachine translation for converting text between languages, speech recognition to convert spoken language into \ntext, and text generation for producing human-like text6,7. Machines do not posse the ability to understand and \ngenerate content in human languages. AI enables them to understand and generate content in human language8. \nTo give the machines with human like ability to read, write and generate the content is a scientific challenge9.\nAdvancements in AI, NLP , and availability of immense amount of training data, contributed to the \nevolution of LLMs. LLMs are fragment of language models (LMs) that uses neural networks, immense number \nof training parameters, and unlabeled text 10. They have the self-supervised learning approach which allows \nthem to train on huge amount of unlabeled data 11. This approach provides them advantages over supervised \nlearning models, as the data does not require to be labelled manually. Most of the LLMs are built on transformer \narchitectures12. Transformer is advanced architecture based on neural networks. Due to the presence of self-\nattention mechanism, the transformer has the ability to better understand the connection between different \ninput variables and parameters 13. LLMs work on two stage training pipeline which enhances their learning \nefficiency. The first training stag is pre training and the second training stage of pipeline is fine tuning 14. In the \nfirst stage they are trained on immense amount of unlabeled data using self-supervised training approach. In \nthe second stage they are trained on specific and labelled data. Combination of these two stages enable them to \nprovide high accuracy15.\n1Department of Computer Science, COMSATS University, Sahiwal Campus, Islamabad, Pakistan. 2Department of \nComputer Science, Riphah International University, Lahore Campus, Lahore, Pakistan. 3Department of Computer \nScience, Electrical and Space Engineering, Luleå University of Technology, Luleå, Sweden. 4IT4Innovations, VSB \n– Technical University of Ostrava, Ostrava, Czech Republic. 5Applied Science Research Center, Applied Science \nPrivate University, Amman, Jordan. email: muhammad.bilal.riaz@vsb.cz; bilalsehole@gmail.com\nOPEN\nScientific Reports |        (2025) 15:13755 1| https://doi.org/10.1038/s41598-025-98483-1\nwww.nature.com/scientificreports\n\nLLMs have their foundations in the early language models and neural networks. The early language models \nwere developed using statistical models and n-gram models 16,17. The early language models were failed to \nexpress the long terms and context in languages18. After the availability of large data sets, the researchers started \nto explore neural networks in more detail in such a way that they could help to improve language models. \nFinally the researchers achieved their millstone in the form of Recurrent Neural Networks (RNNs) and LSTMs19. \nThe RNNs were able to model the sequential data in languages but they also have limitations in term of long-\nterm dependencies. The LLMs were started to emerge after the introduction of transformer architecture 20. \nThe transformer architecture is a efficient in handling long-term dependencies 21. Today’s advanced language \nmodels including Bidirectional Encoder Representations from Transformers (BERT), Generative Pre-trained \nTransformer (GPT), mT5, RoBERTa, XLNet and LLaMA are based on LLM’s and transformer architectures22–24.\nThe LLMs use pipeline in their architecture25. In the first stage of pipeline the training data is preprocessed. \nThe second stage of pipeline consists of model training where data passes through different steps including \nrandom parameter initialization, numerical data input, loss function calculation, parameter optimization, and \niterative training. After the training stage the model can be tested and deployed. Research shows that LLMs \nhave the to perform potential outstanding in specialized NLP tasks and applications in specific domains. LLMs \nalso performed well in different domains of industry including automotive, e-commerce, education, finance \nand banking, health care and medicine26–31. In the automotive industry, LLMs enhance in-car virtual assistants, \nenabling voice commands and real-time translation services, improving driver safety and convenience. \nE-commerce platforms leverage LLMs for personalized shopping experiences, optimizing search results, \nproduct recommendations, and customer service interactions using chatbots. In education, LLMs facilitate \npersonalized learning, automated grading, and intelligent tutoring systems, making education more effective. \nThe finance and banking industry benefits from LLMs through advanced fraud detection and risk assessment. \nHealthcare and medicine utilize LLMs for predictive diagnostics, patient data analysis, and leading to improved \npatient outcomes and operational efficiencies. Across these domains of industries, LLMs drive innovation by \nautomating tasks, enhancing user experiences, and providing deep insights.\nBackground of LLMs\nThe concept of machine translation and language models emerged after the Alan Turing’s 1950 paper 32. Alan \nTuring’s paper is a foundation of AI and language processing. From 1950 to 1980 statistical methods and \nrule-based systems were explored for language models and language processing 33. Those early models and \nlanguage processing methods had limitations in case of complex language processing tasks. In 1986 RNNs \nwere introduced34. RNNs are based on neural networks 35. RNNs are capable of processing language data but \nthey have limitations when it comes to long range dependencies in languages 36. In 1997 Long Short-Term \nMemory (LSTM) were introduced. They addressed the limitations of RNNs19,37. LSTMs have limitations related \nto word embeddings, and representations of words as numerical vectors 38. In 1997, Bidirectional Long Short-\nTerm Memory (BiLSTM) networks were introduced, extending the functionality of LSTMs by processing input \nsequences in both forward and backward directions. While traditional LSTMs handle long-term dependencies \nonly in a unidirectional manner, BiLSTMs overcome this limitation by capturing contextual information from \nboth past and future contexts simultaneously39. After 2010 deep learning revolutionized NLP , with models like \nWord2Vec and GloVe creating powerful word embeddings 40,41. Neural machine translation (NMT) emerged, \nsurpassing traditional statistical methods.\nWord2Vec is a word embedding technique in NLP that generates dense vector representations for words, \ncapturing their meanings and relationships based on contextual usage 42. It was developed in 2013, by a team \nled by Tomas Mikolov at Google 43. Word2Vec employs two main architectures: Continuous Bag of Words \n(CBOW) and Skip-gram 44. CBOW predicts a target word from surrounding context words, while Skip-\ngram predicts context words from a target word. By using techniques like negative sampling and hierarchical \nsoftmax, Word2Vec efficiently learns these word embeddings from large text corpora 45. These embeddings \nplace semantically similar words close to each other in the vector space, enabling applications such as word \nsimilarity measurement, clustering, analogies, and as features in various machine learning models46. Word2Vec \nhas significantly advanced the field of NLP by providing a robust and scalable way to understand and manipulate \ntextual data.\nGlobal Vectors for Word Representation (GloVe), is a powerful word embedding technique used in NLP to \ncapture the semantic relationships between words47. It was introduced in 2014. IT was developed by researchers \nat Stanford University, GloVe differs from Word2Vec by leveraging both local context information and global \nstatistical information from a corpus36. It constructs a co-occurrence matrix, where each entry represents how \nfrequently a pair of words appears together within a certain context window48. By factorizing this matrix, GloVe \ngenerates word vectors that encode meaningful semantic relationships, ensuring that similar words are placed \ncloser together in the vector space49. This approach combines the strengths of traditional count-based methods \nand predictive models, resulting in embeddings that perform well on a variety of NLP tasks, such as machine \ntranslation.\nIn 2015, Google introduced the initial LLM that uses deep learning algorithms 50. It was referred as Google \nNeural Machine Translation (GNMT) model. This model uses huge amount of training data during training. \nAs compared to previous language models it was able to handle complex NLP tasks. In 2017, transformer \narchitecture was introduced51. Transformer architecture played an vital role in the development of LLMs such as \nBERT52. The main objective behind the development of the transformer models was to overcome the limitations \nof earlier models such as RNNs and LSTM53. Transformer models are able to capture the long-term dependencies \nin text. In 2018, Google introduced BERT54. Introduction of BERT was a vital advancement in NLP . The BERT \nis a pre-trained model and it can be fine-tunned on specific domain of NLP . OpenAI introduced GPT model in \n2018 55. It was based on transformer architecture. The first version of GPT is called GPT-1. In 2019, GPT-2 was \nScientific Reports |        (2025) 15:13755 2| https://doi.org/10.1038/s41598-025-98483-1\nwww.nature.com/scientificreports/\nintroduced56. GPT-2 consists of 1.5 billion parameters57. In 2020, GPT-3 was introduced and in 2023 GPT-4 was \nintroduced58.\nMotivation\nAlthough researchers have discussed and covered the applications of LLM’s in various industries but the previous \nstudies have limitations. These studies did not cover the many important aspects of LLM’s in industries including \ndomain specific applications, modern architectures, security, privacy and ethics. Most of the studies covered one \nor two domains of industry. They did not cover applications of LLMs in top industries and their comparison. \nMost of the studies are not peer reviewed research works 8,28,59–62. Absence of these key points motivated the \nauthors to write this review articles. This review article extensively explores the current review articles to identify \ntheir limitations and cover their gaps. The objective of this study is to cover the modern architectures of LLMs, \napplications of LLMs in top industries and to address the issues of security, privacy, and ethics related to the \nuse of LLMs in industries. In this review article we focused on top industries including finance and banking, \nhealthcare and medicine, education, ecommerce, and automotive. Table  1 summarizes the comparison of this \nstudy with previous studies.\nMain contribution\n• Providing an extensive overview of LLMs including their background, evaluation, modern architectures, and \ntheir applications in top industries.\n• Describing the applications of LLMs in top industries including finance and banking, healthcare and medi -\ncine, education, ecommerce, and automotive.\n• Describing the open issues and challenges of LLMs related to data security, privacy and ethics.\n• Describing the important aspects of LLMs.\n• Describing contemporary LLMs and their architectures.\n• Investigating the evaluation metrics for LLMs in detail.\nStudies\nSecurity \nand Privacy Ethics Scope Approach\nCan et al., 27 No No Investigates the Multimodal LLMs in autonomous driving and related \nsystems.\nReview of existing literature and tools related to LLMs \nin autonomous driving systems.\nZijian et al., 61 No No Investigates the LLMs in mobility forecasting within transportation \nsystems.\nReview of existing literature related to LLMs in \ntransportation systems.\nDingkai et al., 63 No No Explores the applications of LLMs in intelligent transportation systems. Review of existing literature related to LLMs in \nintelligent transportation systems.\nQingyang et al., 28 No Ye s Examines the applications of LLMs in e-commerce. Review of existing literature related to LLMs in \ne-commerce.\nXiaonan et al., 64 No No Investigates the LLMs based recommendations in e-commerce. Review of existing literature related to LLMs in \ne-commerce.\nJin et al., 65 No No Examines the LLM based personalization systems to improve customer \nexperience.\nReview of existing literature related to LLM based \npersonalization systems.\nShen et al., 62 No No Investigates the LLM based technologies in educational settings. Survey of existing literature related to applications of \nLLMs in education.\nHanyi et al., 66 No No Investigates the use of LLMs for smart education. Survey of existing literature related to applications of \nLLMs in smart education.\nStefan et al., 67 No No Explores the potential of LLMs in education, and how their challenges \nmight be addressed through game-based learning.\nReview of existing literature related to LLMs and their \nchallenges in education.\nLixiang et al., 68 No Ye s Explore the uses of LLMs for automating educational tasks, while \naddressing the associated challenges.\nReview of existing literature related to LLMs in \neducation.\nNadia et al., 69 No No Examines the impact of ChatGPT on education by focusing on its \ninfluence on the teaching.\nReview of existing literature related to impact of \nChatGPT on education.\nJean et al., 30 No No Investigates the applications of financial LLMs in finance domain. Review of existing literature related to financial LLMs.\nYinheng et al., 60 No No Investigates the applications of LLMs in finance domain. Review of existing literature related to LLMs in \nfinance.\nHuaqin et al., 70 No No Investigates the applications of LLMs in finance domain. Review of existing literature related to LLMs in \nfinance.\nGodwin et al., 71 No No Investigates the applications of LLMs in banking industry. Review of existing literature related to LLMs in \nbanking.\nYining et al., 73 No Ye s Examines the application of LLMs in the various healthcare domains. Review of existing literature related to LLMs in \nmedical industry.\nY anxin et al., 74 No No Investigates the applications of LLMs in the medical industry. Review of existing literature related to LLMs in \nmedical industry.\nPing et al., 75 No No Explore the integration of generative AI and LLMs into healthcare and \nmedical practices.\nReview of existing literature related to LLMs in \nmedical industry.\nThis Study Ye s Ye s Investigates the applications of LLMs in industries including healthcare, \nmedicine, automotive, e-commerce, education, finance and banking. Detailed review on Industrial Applications of LLMs.\nTable 1. Comparison with previous studies.\n \nScientific Reports |        (2025) 15:13755 3| https://doi.org/10.1038/s41598-025-98483-1\nwww.nature.com/scientificreports/\nThe remaining sections of this study have been organized as: The section II covers and discusses the literature, \nsection III covers the methodology, section IV discusses the LLMs in detail, section V discusses the domain \nspecific applications, section VI Case studies and empirical evidence, section VII discusses the issues and \nchallenges, section VIII Ethical considerations and responsible deployment, section IX covers the future \ndirections of LLMs, section X discusses the limitations of this study and section XI covers the conclusion.\nLiterature review\nIn last decade LLMs have become evolution in AI. The number of advancements and improvements in LLMs are \ngrowing day by day. The applications of LLMs in industries are also increasing in an unpredictable manner. Many \nresearch works have been conducted in the past to explore the LLMs and their applications in the industries.\nCan et al., 27 investigate multimodal LLMs for autonomous driving. The authors cover the background of \nmultimodal LLMs and the development of multimodal using LLMs. The study also covers the background of \nautonomous driving. The study investigates the role of multimodal LLMs in transportation and driving. Zijian \net al., 61 explore the applications and role of LLMs in transportation system. The study examines how LLMs \nimproves the transportation system by forecasting the traffic information. The study examines how LLMs are \nhelpful in handling the demands and limitations of transportation system. The study also explores the utilization \nof LLMs in predicting the human travel. The study highlights if we use LLMs in transportation system then \nwe can predict the human travel and manage the demands of transport. In this way the LLMs are also helpful \nto manage the urban planning. Dingkai et al., 63 investigate the applications of LLMs in autonomous vehicles, \ntraffic management, and transportation safety. The study also discusses the limitations and advantages of LLMs \nin traffic management and autonomous driving. The study explores some datasets used to train LLMs for traffic \nmanagement and autonomous driving. The research delves into the development of LLMs for the said domains \nand fields.\nQingyang et al.,28 present the fairness, applications, and challenges faced by LLMs in e-commerce industry. \nThe study reveals that LLMs offer innovative solutions in e-commerce industry and they enhance the customer \nexperience. The research work discusses the pretraining, finetuning, and prompting of LLMs. The study covers \nthe role of LLMs in product reviews, customer support and product recommendations. The research explores the \nbroad applications of LLMs in e-commerce. Xiaonan et al.,64 introduce applications of LLMs in e-commerce in \nthe form of recommendation systems. They review the latest advances in LLM techniques for recommendation \nsystems. Additionally, the authors provide a comprehensive discussion on the future directions of LLM-driven \nrecommendation systems. This study addresses the urgent need for a deeper understanding of LLM-driven \nrecommendation systems due to the rapid development in this research area. Jin et al.,65 discuss the applications \nof LLMs in human computer interaction, personalization systems and recommendation system. The study \ninvestigates the impact of LLMs for improving customer experience.\nShen et al., 62 investigate the different technologies and tools of LLMs in education. The paper investigates \nthe tools that are related to students and teachers. The study also discusses the technological advancements in \nLLMs related to education. The authors discuss the risks associated with the use of LLMs in education. Their \nresearch provides comprehensive study of LLMs in education. Hanyi et al., 66 investigate the role of LLMs in \neducation. The study summarizes the role of LLMs in improving teaching methodology and education models. \nThe study then discusses the integration of LLMs in education. Stefan et al., 67 investigate the applications of \nLLMs in education. The research addresses the challenges faced by LLMs in education. The author discusses that \nplayful and game-based learning can solve those challenges of LLMs. The study also discusses the generative AI \nin education. Lixiang et al.,68 investigate the practical challenges of LLMs in education. They discuss the ethical \nconcerns of using LLMs for grading, feedback, and question generation. The study addresses that these ethical \nconcerns are obstacles for LLMs in education. Nadia et al.,69 discusses the impact of ChatGPT in education and \nteaching process. The study summarizes the research conducted after the introduction of ChatGPT. The study \ndiscusses that the impact of ChatGPT is positive but it is critical for teachers and students.\nJean et al.,30 investigate the applications of LLMs in finance. The study discusses the performance, history, \nand techniques of LLMs in finance. They discuss the training data, finetuning methods and datasets of LLMs \nin finance. Yinheng et al., 60 discusses the current techniques used with LLMs in finance. The study covers \nthe pre training, zero shot learning, custom training, and pre training of LLMs in finance. The research \nemphasizes the decision framework for professionals of finance to select appropriate LLMs. Huaqin et al., 70 \ninvestigated the applications of LLMs in financial domain. The study discusses that the use of LLMs is increasing \ngradually in finance. They claim that professionals are using LLMs for financial report generation, analyzing \ninvestor sentiments, and forecasting the trends of market. Godwin et al., 71 discuss the applications of LLMs \nin banking industry. The study discusses the role of LLMs in banking domain. The study focuses on the text-\nbased communication, and personalized interactions. The study also investigates the role of LLMs in customer \nsupport, automation of tasks, and decision-making process. Christian et al.,72 investigate the LLMs for financial \nadvice. They claim that larger models are better as compare to models trained on average size of datasets. The \nresearch delves into usefulness of LLMs for financial advice.\nYining et al., 73 summarize the applications of LLMs in the medical industry. The cited study covers \nmedical text data processing, public health awareness, and clinical settings of LLMs. The study also covers \ninformation extraction, summarization and question answering techniques related to LLMs. Y anxin et al., 74 \ninvestigate development of LLMs for medicine industry. The study explores the techniques used in LLMs for \nmedicine domain. The study also discusses the directions for the integration of LLMs in medicine industry. \nPing et al.,75 cover the role of generative AI and LLMs in healthcare industry. They investigate the integration \nof generative AI and LLMs in healthcare. The study focuses on the benefits of LLMs in healthcare including \ndecision making process, information retrieval and medical data management. The study also compares the \nLLMs in healthcare with the typical rule-based AI systems and other machine learning models. Marco et al., 31 \nScientific Reports |        (2025) 15:13755 4| https://doi.org/10.1038/s41598-025-98483-1\nwww.nature.com/scientificreports/\nsummarize the applications of LLMs in healthcare including biomedical NLP , literature summarization, and \nclinical documentation management.\nLimitations and drawbacks of existing studies\nThis section examines the limitations and gaps in existing research on LLMs. It highlights the areas where \nprevious studies fall short. Some existing research works do not address an evaluation of LLMS, their modern \narchitectures, and their applications in top industries, for example27,61,63,64. Whereas this research work provides \nan extensive overview of LLMs including their background, evaluation, modern architectures, and their \napplications in top industries including finance and banking, healthcare and medicine, education, ecommerce, \nand automotive. The research works do not investigate the critical open issues and challenges of LLMs related to \ndata security, privacy and ethics62,66,67,69. Whereas this research work thoroughly investigates and highlights the \nopen issues and challenges of LLMs related to data security, privacy and ethics. A large number of studies do not \ndelve into contemporary LLMs and their architectures30,70,71,73. Whereas this research analyzes and investigates \nthe contemporary LLMs and their architectures. The studies cited in this review paper are industry-specific and \ncover only one or two industries69,72,76–78, whereas this review focuses on the applications, issues, and challenges \nof LLMs in the top and most important industries.\nMethodology\nThe research material cited in this study have been acquired from well-known and recognized scientific journals \nand conferences from 2020 to 2024. The research articles have been searched from well-known research \nplatforms including IEEE Xplore, ACM Digital Library, Google Scholar, ScienceDirect, Springer. Initially more \nthan 300 papers were selected related to keywords. After the initial selection, a comprehensive study of papers \nhas been conducted and we finalized more than 100 research articles. The final research articles have selected \nbased on keywords, topic and industrial domains. To conduct a comprehensive search, the main keywords used \nare “LLMs” , “Natural Language processing” , “Deep Learning” and “machine learning” . The combinations of these \nkeywords and some other keywords specific to different domains of industries are used to compile material \nfor this study. These keywords helped to find the relevant articles for this study. The extensive search has been \nconducted to find the relevant and quality articles. Table 2 shows the details of keywords and their combinations \nwhich are used to conduct the literature search for this study.\nLarge Language models\nLLMs models are combination of AI algorithms and NLP techniques. They are powerful tools which are used in \nvarious industries to enhance language related applications. They generate human like text based on the context \nSr #. Keywords and their Combinations Domain\n1 Large Language Models in Healthcare Healthcare\n2 LLMs for medical diagnosis Healthcare\n3 Natural Language Processing in Healthcare Healthcare\n4 Machine Learning and LLMs in healthcare Healthcare\n5 Deep Learning and LLMs in healthcare Healthcare\n6 Large Language Models in Automotive Industry Automotive\n7 LLMs for autonomous vehicles Automotive\n8 Natural Language Processing in automotive Automotive\n9 Machine Learning and LLMs in vehicle Automotive\n10 Deep Learning and LLMs in vehicle Automotive\n11 Large Language Models in E-commerce E-commerce\n12 Natural Language Processing in E-commerce E-commerce\n13 Machine Learning and LLMs in customer service E-commerce\n14 Deep Learning and LLMs in customer service E-commerce\n15 Large Language Models in Education Education\n16 Machine Learning and LLMs in Education Education\n17 Natural Language Processing in education Education\n18 Deep Learning and LLMs in Education Education\n19 Large Language Models in Finance Finance\n20 LLMs for financial forecasting Finance\n21 Machine Learning and LLMs in financial analysis Finance\n22 Deep Learning and LLMs in financial analysis Finance\n23 Natural Language Processing in banking Banking\n24 Machine Learning and LLMs in banking Banking\n25 Deep Learning and LLMs in banking Banking\nTable 2. Keywords and their combinations used to search literature.\n \nScientific Reports |        (2025) 15:13755 5| https://doi.org/10.1038/s41598-025-98483-1\nwww.nature.com/scientificreports/\nuser inputs. They have improved the chatbots, content generation platforms, virtual assistants and customer \nservice automation.\nImportant aspects of Llms\nLLMs have very complex architecture and they have various aspects depending upon the category of model. \nFigure 1 organizes the important aspects of LLMs. In the subsection, the important aspects of LLMs have been \ncovered.\nAttention mechanism\nThe attention mechanism is a vital part of LLMs 79. The attention mechanism is used to find the representation \nof input sequences connecting different tokens 79–81. The main idea is to calculate a set of attention weights \nthat determine the importance of each part of the input data in relation to each part of the output data. There \nare various attention mechanisms used in LLMs including self-attention, multi-head attention, and positional \nencoding82.\nTraining methods\nDifferent training methods are used for the model training of machine learning models but LLMs are trained \nusing distributed methodologies. The reason is that LLMs require huge amount of data and computational \npower83. Distributed methodologies of LLMs are model parallelism, data parallelism, optimizer parallelism, \ntensor parallelism, pipeline parallelism, and federated learning84,85.\nFig. 1. Important Aspects of LLMs.\n \nScientific Reports |        (2025) 15:13755 6| https://doi.org/10.1038/s41598-025-98483-1\nwww.nature.com/scientificreports/\nParameter tuning\nThe LLMs are first pre-trained on huge datasets and then they can be fine-tuned for specific applications86. This \napproach of training provides us customized models. Customized models are highly efficient and accurate 87. \nThere are various parameter tuning techniques used to fine tune the LLMs including prompt tuning, prefix \ntuning, adapter tuning, parameter efficient fine tuning, sparse fine tuning, and parameter sharing 88–90. These \nkinds of techniques optimize the performance without requiring extensive computational resources and \nlarge amount of data 91. Parameter tuning techniques allow the LLMs to be deployed in resource constrained \nenvironment92.\nTransformer architecture\nTransformer architecture is a foundation of various NLP models. The Transformer architecture is a deep learning \nmodel introduced by Vaswani et al., 93 in 2017. The main purpose of transformer was to handle sequential \ndata in neural networks 94. The key components of transformer architecture are input embedding, positional \nencoding, encoder, decoder, and attention mechanism. Transformer architecture uses attention mechanism to \nfind dependencies95,96. The architecture can handle inputs of varying lengths. Due to its efficiency and versatility, \nthis architecture has been replaced typical neural networks 97. Transformer architecture has been specifically \nused in LLMs.\nEvaluation metrics\nTo assess the performance of LLMs, various metrics are used depending on the specific task.\n i. Intrinsic evaluation metrics: These metrics measure the model’s linguistic and semantic capabilities without \nrelying on downstream tasks. It Evaluates the fluency of the language model98.\n• Perplexity: Measures how well a language model predicts a test set. A lower perplexity shows better perfor -\nmance. It evaluates the fluency of the language models99,100.\n• Bleu (bilingual evaluation understudy): Compares the overlap between n-grams of the generated and refer-\nence texts. It is common in machine translation101.\n• Meteor (metric for evaluation of translation with explicit ordering): It was introduced as an Improvement \nupon BLEU by considering synonyms, stemming, and word order102.\n• Bertscore: Uses contextual embeddings to compute semantic similarity between generated and reference \ntext103.\n ii. Human evaluation metrics: Human evaluation metrics are used for assessing and comparing how LLMs \nperform on evaluation sets. It is a way to assess the performance of LLMs by asking people to judge the \nmodel’s output. Human evaluations are often combined with automated metrics to provide a more compre-\nhensive view of the model’s performance104.\n• Fluency: Evaluates how naturally and smoothly the model’s output adheres to linguistic norms. It measures \nthe model’s ability to produce grammatically correct, coherent, and contextually appropriate sentences, mak-\ning the output easy to read and understand105.\n• Coherence: Refers to a quantitative or qualitative measure of how logically consistent, contextually appropri-\nate, and smoothly connected the output of the model is within a given text. Coherence evaluates whether the \ngenerated content aligns with the input prompt and flows logically without contradictions, abrupt topic shifts, \nor incoherent phrasing106.\n• Relevance: A measure of how closely the model’s outputs align with the user’s query. It evaluates the appro-\npriateness, accuracy, and contextual suitability of the generated response107.\n• Bias and fairness: They are used to evaluate and ensure the equitable behavior of LLMs across different de -\nmographic groups, contexts, or use cases108.\n iii. Efficiency and resource utilization metrics : LLMs requires a comparable large number of computational \nresources as compared to ordinary machine learning models. So, specialized evaluation metrics are used to \nevaluate them for resource utilization. These are crucial for deploying LLMs in production environments109.\n• Inference time: The amount of time it takes for a trained ML model, such as LLMs, to process input data and \nproduce an output or response110.\n• Memory and compute requirements: Refers to the computational resources necessary to train, fine-tune, or \ndeploy the LLMs111.\n• Energy efficiency: A metric that measures how effectively an LLM utilizes computational resources (electrici-\nty, memory, and processing power) to perform tasks such as generating text, answering queries, or processing \ndata. It evaluates the trade-off between energy consumption and performance, aiming to optimize the balance \nbetween environmental sustainability and computational output112.\n iv. Novel metrics: The ordinary evaluation metrics are not enough to judge and evaluate the developing LLMs \nso with the growing capabilities of LLMs, newer metrics are being developed.\n• Holistic evaluation of language model (helm): HELM was introduced in 2021 by a team of researchers from \nthe University of California, Berkeley, who wanted to provide a more holistic and actionable evaluation of \nLLMs in real-world applications 113. The goal was to help developers and researchers better understand the \npotential and limitations of these models across a wide range of use cases. It was introduced to evaluate LLMs \nScientific Reports |        (2025) 15:13755 7| https://doi.org/10.1038/s41598-025-98483-1\nwww.nature.com/scientificreports/\nin a comprehensive and multifaceted manner, going beyond traditional benchmarks like accuracy or perplex-\nity. Instead of focusing only on isolated metrics, HELM aims to assess models in the context of multiple tasks \nand domains to understand their overall performance, fairness, robustness, and potential societal impacts114. \nThis evaluation metric considers various aspects of LLM behavior, such as bias, ethical considerations, inter-\npretability, and performance across diverse use cases115.\n• Winograd schema challenge : It was introduced by Terry Winograd in 2011. It refers to a metric used to \nevaluate the performance of LLMs in terms of their ability to understand and resolve ambiguities in natural \nlanguage116. It specifically tests a model’s ability to resolve pronouns in sentences, where the correct answer \ndepends on world knowledge and context, rather than simple syntactic rules. A Winograd Schema consists \nof a pair of sentences that differ by only one or two words, typically involving a pronoun. The challenge is \nthat, in each pair, the pronoun’s reference is ambiguous, and the correct reference can only be determined by \nreasoning about the context117.\n• Knowledge F1 : It was introduced in 2020 as part of the “Fact-based Evaluation of Language Models” ap -\nproach, mainly to evaluate models like T5 and BERT on tasks that require factual knowledge retrieval 118. \nIt is a performance measure used in evaluating LLMs based on their ability to recall factual knowledge. It \ncombines precision and recall into a single metric, where precision measures how many of the facts retrieved \nby the model are correct, and recall assesses how many of the correct facts are retrieved by the model. This \nmetric is particularly useful for tasks that require factual accuracy, such as question answering and knowledge \nretrieval119.\nNatural Language Understanding (nlu) and generation Language generation (nlg)\nNLU and NLG are two critical components of LLMs that enable machines to process and produce human \nlanguage. NLU refers to a model’s ability to comprehend and interpret input text, extracting meaning, context, \nand intent from natural language. It involves tasks such as sentiment analysis, entity recognition, and syntactic \nparsing120. On the other hand, NLG focuses on generating human-like, coherent, and contextually appropriate \ntext based on input data. Together, NLU and NLG allow LLMs to understand complex queries, engage in \nmeaningful conversations, and produce relevant responses, making them fundamental in applications like \nchatbots, translation, summarization, and content creation121.\nScalability\nScalability refers to the ability of the model to handle increasing amounts of data, users, or tasks without a \nsignificant decrease in performance122. It involves the model’s capacity to expand in both computational power \nand complexity, enabling it to process larger datasets, generate more sophisticated outputs, and support a \ngrowing number of simultaneous requests. Scalability in LLMs also relates to their ability to be deployed across \ndifferent environments, from local machines to cloud infrastructures, ensuring that as demand increases, the \nsystem can adapt and maintain its effectiveness123.\nTransfer learning\nTransfer learning is a technique where a model pre-trained on a large dataset is fine-tuned on a specific, often \nsmaller, domain-specific dataset. This approach leverages the knowledge acquired during pre-training to improve \nperformance on specialized tasks without requiring the model to be trained from scratch124. By transferring the \ngeneral understanding developed during pre-training, LLMs can quickly adapt to new tasks, making them more \nefficient and effective in scenarios with limited task-specific data. This is particularly useful in industries or fields \nwhere labeled data is rare but general language knowledge is applicable125.\nContemporary Llms\nContemporary LLMs represent a significant advancement in the field of artificial intelligence and NLP . These \nmodels are built upon deep learning architectures with billions of parameters, enabling them to understand \nand generate human-like text with outstanding fluency. Figure  2 organizes the contemporary LLMs. In the \nsubsections of this section the contemporary LLMs have been covered in detail.\nGpt series\nGPT series have 4 LLMs including GPT-1, GPT-2, GPT-3 and GPT-4. GPT-4 is latest and advanced model58,126,127. \nAll of these models are built on transformer architecture128. There models are trained on huge datasets and later \non fine-tuned for specific applications. These models are used in generating coherent, contextually relevant text, \nmaking them useful for applications in content creation, language translation, and conversational agents129.\nBert\nBERT was developed by google in 2018 to address the limitations of earlier language models 130,131. It was \nspecifically developed to handle the intricacies of languages132. BERT is a remarkable advancement to handle the \ncontext of words in the sentences. Earlier language models process the text sequentially whereas BERT processes \nthe text in both directions, right to left and left to right133,134. This approach allows the BERT to capture the full \ncontext of words in sentences. This bidirectional approach improves the performance of language models to \ndeeply understand the context of words in text.\nAlbert\nA lite BERT or ALBERT was developed by google in the late 2019 135. It is a lighter version of BERT. It uses \noptimization techniques such as factorized embedding parameterization and cross-layer parameter sharing \nto reduce the number of parameters 136. These techniques make it memory efficient and suitable for resource \nScientific Reports |        (2025) 15:13755 8| https://doi.org/10.1038/s41598-025-98483-1\nwww.nature.com/scientificreports/\nconstrained environment137. The main purpose behind the development of this model was to design a lightweight \nLLM without compromising the performance.\nT5\nThe Text-to-text transfer transformer or T5 was developed by google in 2019 138. This model uses the approach of \ntext-to-text task. Text to text task means the input and output of T5 are always in the form for strings139. T5 uses \ntransformer architecture. It was pre trained on a huge and multipurpose dataset which is called C4 or Colossal \nClean Crawled Corpus140. The main purpose behind the development of this model was to simplify the approach \nto NLP tasks. This single model can be fine-tuned for various applications and NLP tasks.\nXLNet\nXLNet was designed by researchers at Carnegie Mellon University, Pittsburgh and Google in 2019 141. It is a \ncombination of autoregressive LLMs and bidirectional LLMs 142. During training it use permutations of the \ninput sequences. The use of permutation approach assists the model to achieve remarkable performance on a \nvariety of NLP tasks 143. The main purpose behind the development of XLNet was to overcome the limitations \nof bidirectional models.\nLaMDA\nLaMDA was developed by researchers at Google in 2021 144. LaMDA is a conversational model. The main \npurpose behind the development of LaMDA was conversations 145. LaMDA was trained on dialogue datasets \nso that it can be engaged in human like conversations146. LaMDA is suitable for virtual assistants and customer \nsupport applications. LaMDA was developed to improve interactions between AI and humans, improving the \nuser experience.\nLLAMA\nThe Large Language Model Meta AI (LLAMA) was developed by AI researchers at Facebook in 2023 147. \nThis model was developed to enhance the capabilities of LLMs in NLP tasks. The main purpose behind the \ndevelopment of this model was to improve the performance of LLMs in research and practical applications 148. \nThis model uses transformer architecture and advanced training techniques to get the outstanding performance \non various NLP tasks.\nFig. 2. Contemporary LLMs.\n \nScientific Reports |        (2025) 15:13755 9| https://doi.org/10.1038/s41598-025-98483-1\nwww.nature.com/scientificreports/\nDomain specific applications\nLLMs are transforming various industries by providing tailored solutions that leverage specialized knowledge. \nBy focusing on specific domains, LLMs can deliver highly accurate, context-aware insights that drive innovation \nand efficiency across various fields. In this study we discuss the applications of LLMs in top industries including \nautomotive, e-commerce, education, finance and banking, health care and medicine. Figure  3 summarizes the \napplications of LLMs.\nHealth care and medicine\nLLMs have diverse applications in medical field. LLMs have been warmly welcomed in many medical domains. \nLLMs helps the medical professionals in problem solving and learning 149. According to the studies specialized \nAI chatbots will be further improved in near future150. LLMs are helpful in clinical decision making151.\nLLMs are used in question answering related to medical fields 152. LLMs are helpful in for the investigation \nof patient data, and medical surveys 153. LLMs are used in biotechnology field to address the challenges bio 154. \nLLMs provide information to patient related to healthcare and treatment155. LLMs are used for X-ray analysis156. \nMedical text analysis is a challenge for medical professionals 153. Medical text consists of the aberrations and \ntechnical terms related to specific fields 157. LLMs help the medical professionals to structure the raw medical \nFig. 3. Domain Specific Applications of LLMs.\n \nScientific Reports |        (2025) 15:13755 10| https://doi.org/10.1038/s41598-025-98483-1\nwww.nature.com/scientificreports/\ndata and retrieve specific information from the medical text. LLMs help to extract information from medical \nnotes158.\nZhou et al., 159 investigated the traditional Chinese medicine for epidemic prevention and treatment. They \nproposed a LLM for question answering for epidemic prevention and treatment using traditional Chinese \nmedicine. They said traditional Chinese medicine has rich and practical literature related to epidemic prevention \nand treatment. The literature is complex and huge which is a challenge for medical professionals to extract \nthe required information. The literature consists of numerous books. They said their proposed LLM can solve \nthis problem and it is better than traditional models. They said the proposed model can efficiently handle the \nhuge and complex literature related to traditional Chinese medicine. Another study conducted by Zhe et al.,160 \nrelated traditional Chines medicine. They fine-tuned different LLMs for traditional Chinese medicine formula \nclassification. They claimed that their fine-tuned models achieved better accuracy and performance as compared \nto previous models.\nYutao et al., 161 studied the medication guidance and drug reaction prediction and proposed a model for \nmedication guidance and drug reaction forecasting. Their model uses two stage training, in first part the model \nis trained on drug datasets.\nand in second part the model is trained on real datasets of patients. Their model is helpful for medical \nprofessionals to improve the healthcare services. Abdulkader et al., 162 fine tunned the T5 model for medical \nreport summarization. Medical reports are not easy to understand for the public so they fine tunned the model \nso that could help the public. Senay et al.,78 proposed a model to automate the administrative tasks in healthcare. \nTheir model is useful for healthcare professionals. Their proposed model can perform appointment scheduling, \ndocumentation, and retrieval of medical records. Dimitrios et al., proposed a model to provide diagnostic \nsuggestions to the patients. They said the diagnosis process is a complex one and their proposed model can be \nhelpful for the patients. Luis et al., 163 proposed an Internet of Medical Things (IoMT) system integrated with \nLLM. Their proposed model can monitor Parkinson’s disease (PD), which is a neurodegenerative disorder.\nImpacts of Llms on health care and medicine\nLLMs are reducing time for clinical documentation and reporting by automating the tasks for example medical \ntranscription and summarization. Nuance Dragon Medical One software uses LLMs to transcribe doctor-patient \nconversations with up to 98% accuracy, decreasing documentation time by 30–40% 164,165. LLMs accelerated \nprocessing of massive amounts of medical literature to identify relevant studies, trends, and insights. IBM \nWatson for Health analyzes big datasets to find patterns, and insights saving researchers 20–30% of their time \nin literature reviews 166,167. LLMs provide personalized treatment plans based on genetic, environmental, and \nlifestyle factors168. LLMs have improved insurance claim processing time by 30%, reducing errors169.\nLimitations of Llms in health care and medicine\nLLMs often face difficulty with domain-specific accuracy in medical contexts. For example, general-purpose \nGPT-4 may generate responses that lack the depth or precision required for complex medical cases 170. LLMs \nrequire enormous, high-quality datasets to perform efficiently. In health care and medicine, data availability \nis often restricted by privacy laws (for example, HIPAA) and ethical considerations 171. Deploying LLMs in \nhealth care systems requires substantial infrastructure investment and integration with existing workflows 172. \nRegulatory laws for LLMs in health care are still evolving, making it difficult to attain compliance173.\nProven vs. emerging applications of Llms in health care and medicine\nThere are many applications of LLMs that can be found in health care and medicine. Some of them are \nproven while some of them are still under development. Proven applications include clinical documentation, \ndiagnostic decision support, and chatbots for patient engagement. Whereas emerging applications include AI-\nDriven surgery assistance, and autonomous health monitoring systems. Researchers are still working on LLMs \nintegration with robots and it still require much improvement and validation174. Autonomous health monitoring \nsystems are still mainly in prototype stages and have yet to be adopted on a broad scale due to concerns over \naccuracy170.\nAutomotive\nLLMs have countless applications in the automotive industry. LLMs have applications in automotive industry \nincluding chain management, predicting shortage, and improving production schedules. In case of autonomous \nvehicles, applications of LLMs are NLP assistant, voice-activated controls, and real-time navigation and \npredictive maintenance77. Bhavin et al.,175 proposed a multi model vehicle system based on LLM and cloud. The \nproposed system is a real time automotive system which can assist the driver in driving and provide navigation \nsupport and object detection. The proposed model also provides the data privacy and security in system. Zhi-Qi \net al.,176 proposed a model based on LLMs for electric vehicle battery supply chain. Their proposed model can \npredict the disruption in the supply chain of batteries for electric vehicles. Zeba et al.,177 proposed a model using \nfusion approach based on LLM and computer vision. The proposed model detects and differentiates the objects \non road. The model also detects the lines on road. After the detection the proposed model translate the detected \nobjects into text form for driver. Mobina et al.,178 proposed a model in the form of digital voice assistant. Their \nproposed model is based on LLMs and SVM classifier. The model translates the voice commands for vehicle.\nImpacts of Llms on automotive\nLLMs assist in design and engineering to streamline the process. BMW uses LLMs to create design process for \nvehicle parts and reduce the time required179. Google uses LLMs for its self-driving cars 180. Tesla uses LLMs in \nits autopilot system. Audi uses LLMs in its chatbot to provide information related to vehicle features, and sales181. \nScientific Reports |        (2025) 15:13755 11| https://doi.org/10.1038/s41598-025-98483-1\nwww.nature.com/scientificreports/\nToyota and Honda use LLMs to optimize inventory levels and delivery schedules, leading to a 15–20% reduction \nin supply chain costs182. Automotive companies use LLMs to perform analysis on legal documents. Mercedes-\nBenz uses LLMs to analyze regulatory changes related to vehicle safety and emissions, streamlining compliance \nreporting and reducing compliance-related costs by 10–15% 183,184.\nLimitations of Llms in automotive\nEnvironmental factors and road conditions vary across different areas that may lead to consistency issues with \nLLMs integrated in autonomous driving systems 185. Autonomous driving system driven vehicle diagnostics \nrequire diverse, and high-quality datasets to perform efficiently186. However, these datasets may not fully capture \nthe range of real-world conditions and environmental factors. Training, implementing and deploying LLMs in \nautomotive industry require computational and financial resources77.\nProven vs. emerging applications of Llms in automotive\nProven applications of automotive industry include autonomous driving and driver assistance, predictive \nmaintenance and diagnostics, manufacturing process optimization, voice assistants and in-car interaction187,188. \nWhereas full autonomous driving (Level 5), AI-driven design and customization, real-time traffic and route \noptimization, and personalized vehicle experience are still emerging and researchers are working on them for \nimprovements189–191.\nE-commerce\nLLMs are widely adopted in the e-commerce industry. These days e-commerce has become the vital part of \nglobal economy. After the advancements in the internet, the traditional shopping approaches have been replaced \nby e-commerce. E-commerce provides convenience to the consumers. Although e-commerce has many benefits \nbut it has challenges as well, for example language barrier for consumers. To overcome the challenges of \ne-commerce, LLMs play an important role. Dehong et al., 192 proposed an LLM based e-commerce machine \ntranslation model. They claimed that their proposed model can handle domain specific terms and keywords, \nproviding better performance and accuracy in translation. They fine tunned their model on Chinese and English \nbilingual terms. Kaidi Chen et al.,193 proposed an LLM based machine translation model for e-commerce. They \nclaimed that their proposed model can handle the domain related words and special formulas of e-commerce \ndomain. They claimed that their proposed model provides more robustness as compared to previous machine \ntranslation models. Chenhao et al., 194 proposed an LLM- ensemble based model for product attribute value \nextraction. They claimed that their proposed model improves the recommendation system for consumers by \nimproving the process of product value extraction. Ben et al.,195 proposed an LLM based relevance modeling for \ne-commerce search engines. They claimed that their proposed model ensures that products selected based on \nconsumer query is aligned with the intent of consumer.\nImpacts of Llms on e-commerce\nAmazon uses LLM powered chatbots to handle and process customer inquiries 196. These chatbots can resolve \n70–80% of customer service queries without human intervention197. This reduces the need for human customer \nservice agents lessens the cost. Amazon uses LLMs for product recommendations and personalization 198. \nWalmart LLMs to analyze customer profile data and behavior across different areas, permitting for personalized \nemail marketing 199. Zara uses LLMs to predict demand for different clothing styles across various regions, \nadjusting production schedules and inventory levels accordingly182. Other different brands use LLMs for content \ncreation and social media200.\nLimitations of Llms in e-commerce\nIn case of recommendation systems, LLMs may face challenges in understanding vague queries or generating \naccurate recommendations when customer input is imprecise 201. The performance of LLMs totally relies on \nthe quality and range of training data, which may not always represent the full range of customer behaviors 202. \nIt can be computationally intensive for LLMs to Handle large-scale, real-time operations in e-commerce, such \nas dynamic pricing updates 195. LLMs operate as black-box systems, making it problematic to explain why a \nparticular recommendation or search result was made203.\nProven vs. emerging applications of Llms in e-commerce\nAlthough LLMs have significant impact on e-commerce but some of its applications are proven and some are \nstill emerging. Proven applications include customer service and support, product recommendations, marketing \nand advertising, fraud detection and prevention, search optimization, pricing optimization, content creation \nand copywriting204,205. Whereas emerging applications include inventory management and demand forecasting, \nregional and cultural adaptation, advanced behavioral analytics, and ethical recommendations206,207.\nEducation\nLarge Language Models (LLMs) are transforming the education industry by enhancing personalized learning \nand administrative efficiency. They can assist in crafting customized learning experiences by analyzing student \nperformance and tailoring content to individual needs. They also support teachers by automating administrative \ntasks, generating lesson plans, and grading assignments. Additionally, LLMs facilitate language translation \nfor diverse learners, fostering a more inclusive educational environment. Ehsan et al., 208 proposed a method \nfor distilling fine-tuned LLMs into smaller, efficient neural networks for deployment on resource-constrained \ndevices. The authors trained a student model using the prediction probabilities of a teacher model achieving \ncomparable accuracy with state-of-the-art models while being significantly smaller and faster. Zheyuan et \nScientific Reports |        (2025) 15:13755 12| https://doi.org/10.1038/s41598-025-98483-1\nwww.nature.com/scientificreports/\nal.,209 proposed an LLM based framework. Their proposed model is a multi-agent classroom simulation \nmodel. Their proposed model is a classroom mechanism for automatic classroom teaching. Liuqing et al., 210 \nproposed education approach to learn bio-inspired design. They claimed that bio-inspired designs are difficult to \nunderstand and learn. They also said that learning bio-inspire designs depends upon on teacher. The authors said \nthat their proposed model is helpful to learn bio-inspired designs. Victor et al., 211 proposed a web application \nbased on LLMs for education. The authors said their proposed application is subscription free. The teachers can \nupload their own datasets to fine tune the model.\nImpacts of Llms on education\nLLMs are providing personalized learning to students. Duolingo which is an education based mobile app, uses \nLLMs to enhance learning and engagements by 20% 212. LLMs are also used to create educational content including \nquizzes, assignments, and lecture plans. Khan Academy uses LLMs to create practice problems for students 213. \nLLMs identify gaps in student understanding through data analysis and offer targeted interventions 214. LLMs \nprovide assistance to researchers by summarizing the research articles and identifying the trends. LMMs are \nused to provide personalized feedback to students215.\nLimitations of Llms in education\nLLMs may misinterpret user queries when applied to multiple topics. LLMs may produce incorrect information \nfor complex and diverse topics216. LLMs often lack the ability to provide effectively to various learning preferences, \nsuch as visual, and auditory. Training data may not represent all cultural, linguistic, or regional contexts resulting \nin biased outputs217. Finetuning LLMs with existing syllabus and regulatory standards can be a time consuming \nand complex task218.\nProven vs. emerging applications of Llms in education\nProven applications of LLMs include personalized learning, automated content generation, intelligent tutoring \nsystems, language translation, enhanced assessment and feedback 219. Emerging applications include advanced \nbehavioral analytics, adaptive simulations for teacher training, regional and cultural adaptation, and gamified \nlearning experiences220.\nFinance and banking\nLLMs are gradually being used in finance and banking to enhance customer service, and improve decision-\nmaking. They can analyze massive amounts of data to provide insights, automate tasks such as customer queries \nand fraud detection, and assist in risk management. Shijie et al.,221 proposed a model specialized for finance. They \nsaid that they trained their model on massive financial dataset. Zarza et al., 222 proposed a model for financial \nplanning and budgeting based on LLM. They claimed that their proposed model is effective for both houses and \ncorporates. Their proposed model suggests solutions to manage budget plans. Boyu et al.,223 proposed a model for \nfinancial sentiment analysis and investment decision making. They claimed that their proposed model improves \nthe sentiment analysis and investment decision making process. George et al., 76 proposed a model which is a \ncombination of LLM and cloud environment. The authors said the proposed model improves the operations and \ncompliance in banking system. The authors said their proposed models overcomes the traditional challenges of \nbanking. They said their model also enhances the customer experience. Daniel et al., 224 proposed a model for \nautomatic topic modeling and their categorization for tagging retail banking transactions. They used LLM and \nzero shot prompting.\nImpacts of Llms on finance and banking\nLLMs can enhance fraud detection systems by analyzing the transactional data. JPMorgan Chase uses LLMs \nto keep an eye on transactions, which reduces the fraud chances by 40% 225. Bank of America obtained an \nLLMs based chatbot named Erica which handles customer inquiries and bill payments 226. Citi bank uses AI \nmodels to reduce loan approval time by 50% 227. Compliance team of Standard Chartered Bank report a 40% \nreduction in manual reviews and an 85% increase in detection precision228. BloombergGPT provides insights on \nmarket conditions by analyzing and processing textual data from numerous sources. BloombergGPT is trained \non 50-billion parameters and it was built for finance229.\nLimitations of Llms in finance and banking\nDue to biased training data LLMs can predict inaccurate market trends resulting in financial losses 70. Banking \nand finance produce huge amount of live data which can overwhelm the LLMs in real time applications 230. \nAttackers may exploit vulnerabilities of LLMs to manipulate the outputs 231. Ensuring customer data privacy \nduring training and implementation of LLMs is a challenge232.\nProven vs. emerging applications of Llms in finance and banking\nIn finance and banking, proven applications of LLMs are fraud detection, customer support automation, \ndocument processing, trading and market predictions, and personalized banking 70. Emerging applications are \nadvanced behavioral analytics, emotional feedback analysis, and blockchain integration233.\nCase studies and empirical evidence\nTo provide the concrete evidence of integration of LLMs in industry, this section investigates some practical \napplications. IBM Watson uses LLMs to analyze extensive medical literature and patient data, providing evidence-\nbased diagnoses and treatment recommendations. This application assists the healthcare professionals in making \ninformed decisions, thereby improving patient outcomes and streamlining the diagnostic process 234–236. Alexa \nScientific Reports |        (2025) 15:13755 13| https://doi.org/10.1038/s41598-025-98483-1\nwww.nature.com/scientificreports/\nis cloud-based voice assistant that was developed by Amazon in 2014. Now Amazon Alexa is powered by LLMs. \nAmazone has implemented a custom LLM to make Alexa more efficient in its conversations. Alexa depends \nupon LLM to understand, process and response user queries 237. Coca-Cola is a famous international brand. \nIt always uses innovative marketing strategies since its birth. It is one of the most iconic brands. Coca-Cola \nhas integrated an advanced language model GPT-4 in its marketing operations. Coca-Cola is using GPT-4 for \ncontent generation, information retrieval, social media, and generating comprehensive reports234,238,239.\nThe famous OTT platform Netflix is using LLMs for personalized recommendation system. The main \npurpose of the Netflix is to innovate its personalized recommendation system. Netflix uses LLMs to analyzes \nextensive data of users, find valuable insights and provide the users with their desired content 234,240. Spotify is \na famous audio streaming service. Like Netflix Spotify also uses LLMs to improve its music recommendation \nsystem. Spotify uses LLMs to analyze user listening habits, playlists, and interactions with the platform. LLMs \nprovide Spotify a way to understand user preferences and recommend them with their desired music 234,241. \nThe New Y ork Times is famous leading global media. It is using LLMs to improve advertising strategies. LLMs \nenables advertisers to maximize their influence by suggesting the best places for ad campaigns based on the \nadvertisement’s messaging. By refining strategies LLMs enhance campaign performance234,242.\nOpen issues and challenges\nLLMs provide a massive number benefits but they also have issues and challenges. Over the time many issues \nand challenges have been overcome by researchers but many of them are still open for research and debate243. In \nthis section we highlight the open issues and challenges of LLMs.\nOpen issues\nThis section covers the open issues of LLMs in industry. Figure 4 summarizes the open issues of LLMs.\n• Ethical Issues: The LLMs are trained on massive datasets. The questions that arise here are: Who can use the \ndataset? How can the dataset be used? And when can the dataset be used? The ethical issue related to the use \nof dataset are still open to discuss. The datasets can consist of biased data leading to the biased outputs from \nLLMs244. The LLMs can also provide hate speech and misinformation.\n• Data Privacy Issues: The training datasets of LLMs can consist of personal data which is an open issue for \nLLMs. Data privacy preserving techniques are required to train the models without compromising user pri -\nvacy. As the use of data is increasing in LLM models, the privacy concerns are also increasing59.\n• Adversarial and Cyber-attacks: LLMs are vulnerable to cyber-attacks. Security of LLMs is an open issue. Im-\nproving the security of LLMs against cyber-attacks is a big concern245. LLMs can be vulnerable to adversarial \ninputs that manipulate their outputs in harmful ways. Understanding how to strengthen models against such \nattacks is a critical area of research.\nFig. 4. Open Issues of LLMs.\n \nScientific Reports |        (2025) 15:13755 14| https://doi.org/10.1038/s41598-025-98483-1\nwww.nature.com/scientificreports/\n• Environmental Impact: Training and deploying LLMs require considerable computational resources, lead-\ning to significant energy consumption and carbon emission. The environmental footprint of these models is \nan open issue that calls for the development of more energy-efficient algorithms246.\n• Explainability and Transparency: LLMs operate as black-box models, making it difficult to understand how \nthey generate specific outputs. This lack of explainability raises concerns in critical domains like healthcare \nand finance, where understanding the rationale behind decisions is essential247.\n• Hate Speech and Misinformation : LLMs can unintentionally generate harmful content, including hate \nspeech or misinformation, which can have real-world consequences. The responsibility of developers to mit-\nigate these risks is a critical area for further exploration248.\n• Data Poisoning: Attackers may introduce malicious data into training sets, leading to compromised model \nintegrity249.\n• Cost Efficiency : The financial burden associated with developing and maintaining LLMs remains a signifi -\ncant barrier for many organizations. High costs related to data acquisition, processing power, and ongoing \nmodel training can deter smaller enterprises from leveraging these technologies250.\nChallenges\nThe use of LLMs is increasing gradually in the industry leading to challenges. This section covers the open \nchallenges of LLMs in industry. Figure 5 Summarizes the challenges of LLMs.\n• Massive Datasets: LLMs are trained on massive and complex datasets. The source of datasets is internet. Due \nto their size and complexity, it is a challenge to maintain the security and privacy and quality of datasets 251. \nHandling and processing a massive data are a challenge itself.\n• Computational Resources: Due to the huge amount of training datasets LLMs require huge set of computa-\ntional resources252. Some models require special hardware for their training. Energy consumption is high for \nLLMs training. These are the open challenges for LLMs.\n• Biased Outputs: Biased outputs from LLMs presents a significant challenge, as LLMs can unintentionally \nreflect and amplify biases present in their training data253. This can lead to unfair results, particularly in sen-\nsitive areas such as hiring, law enforcement, or healthcare, where impartiality is critical.\n• Regulatory Compliance: With the emergence of regulations such as GDPR, ensuring compliance while uti-\nlizing large datasets poses a challenge. Organizations must navigate these legal frameworks while balancing \ninnovation with privacy rights254.\n• Non-English Language Support: A significant gap exists in the performance of LLMs across different lan -\nguages, particularly non-English languages. This limitation restricts access to advanced AI capabilities for \nnon-English speaking populations. Efforts must be directed towards developing robust models that can un -\nderstand and generate content in a variety of languages without compromising quality255.\n• Multimodal Integration: The integration of multiple data modalities (text, images, audio) into LLMs pre -\nsents an open challenge that could expand their capabilities significantly. Current models primarily focus on \ntext-based inputs, which limits their applicability in diverse fields such as healthcare diagnostics or customer \nservice where multimodal understanding is crucial256.\nFig. 5. Challenges of LLMs.\n \nScientific Reports |        (2025) 15:13755 15| https://doi.org/10.1038/s41598-025-98483-1\nwww.nature.com/scientificreports/\n• Accuracy and Hallucinations: Ensuring the accuracy of outputs generated by LLMs is paramount. The phe-\nnomenon of “hallucinations, ” where models produce reasonable but incorrect information, poses risks in ap-\nplications that rely on factual accuracy. Addressing this challenge requires improved training methodologies \nand validation processes to enhance the reliability of generated content257.\nEthical considerations and responsible deployment\nThe deployment of LLMs in industry raises significant ethical concerns that must be carefully addressed to \nensure responsible use. Key ethical considerations include the potential for bias in model predictions, privacy \nissues regarding sensitive data, and the opacity of decision-making processes within these systems 258,259. \nTo mitigate these risks, it is essential to implement frameworks that prioritize transparency, fairness, and \naccountability. For instance, organizations should establish rigorous bias detection mechanisms, maintain clear \ndata governance policies, and ensure that LLMs are understandable and auditable. Additionally, it is important \nto involve interdisciplinary teams comprising ethicists, domain experts, and technologists, during the design and \ndeployment phases to continuously evaluate the social impact LLMs. Practical recommendations for responsible \ndeployment include adopting established ethical guidelines, such as the “Ethics Guidelines for Trustworthy AI” \nfrom the EU, and implementing regular audits to assess the ethical performance of LLMs260,261. By encouraging \na culture of accountability and ongoing scrutiny, we can ensure that LLMs are deployed in ways that align with \nsocial values and mitigate the risks of harm.\nFuture directions of Llms\nLLMs are prominent emerging technologies and continue to be a dynamic area of AI research. Efforts are focused \non advancing techniques to tackle data privacy concerns, enhance security, and address ethical considerations \nsuch as bias and fairness. Additionally, the integration of LLMs with domain-specific knowledge through \nspecialized fine-tuning will enable more accurate and context-aware applications. As these advancements are \nmade, LLMs will be deployed in ways that maximize their benefits while ensuring ethical use, scalability, and \nbroader societal advantages, steering in a new era of AI innovation.\nLimitations\nOne of the important limitations of this study is availability of limited review literature related to LLMs and their \napplications in the industry. Best efforts are made to find the related literature. The available material is covered \nthoroughly to address and cover all the related aspects. Due to the restriction of resources this study covered \napplications of LLMs in prominent industries, although LLMs have applications in some other industries as well. \nWe covered and discussed the details of modern LLM models and architectures but an in-depth analysis can be \ndone on each of them.\nConclusion\nBased on neural networks and transformer architecture, LLMs have evolved in a remarkable way. LLMs have \nchanged the field of NLP . LLMs resulted in extraordinary expansion in NLP . LLMs have revolutionized the text \ngeneration and processing. The use of LLMs is increasing and expending in the industry. Applications of LLMs \ncan be found in almost each and every domain of industry. Although LLMs are considered evolutionary and \npowerful across various fields, they also have limitations and challenges. This study has provided an insightful \nand meaningful review of LLMs and their applications in the industry. This research work covered the important \naspects of LLMs, and contemporary LLMs. The study has also examined industrial domain specific applications \nof LLMs, including healthcare and medicine, automotive, e-commerce, education finance and banking. The \nresearch work covered the open issues of LLMs including ethical issues, data privacy issues, security issues, \nenvironmental impacts, explainability and transparency. The study also covered the open challenges of LLMs \nincluding massive datasets, computational resources, biased outputs and regulatory compliance. As the field \nof LLMs research and development is expanding swiftly, this review would be a valuable literature for the \nresearchers looking for literature related to the applications of LLMs in the industry. The study focused on the \nsignificance of LLMs in the industry. LLMs represent advancements in NLP and AI, revolutionizing the domain \nof problem-solving in the industry; however, they are still under development and require many improvements.\nData availability\nThe datasets used and/or analyzed during the current study available from the corresponding author on reason-\nable request.\nReceived: 17 October 2024; Accepted: 11 April 2025\nReferences\n 1. Khurana, D., Koli, A., Khatter, K. & Singh, S. Natural Language processing: state of the art, current trends and challenges. \nMultimed Tools Appl. 82, 3713–3744 (2023).\n 2. Kosch, T. et al. A survey on measuring cognitive workload in human-computer interaction. ACM Comput. Surv. 55, 1–39 (2023).\n 3. Chowdhary, K. & Chowdhary, K. R. Natural Language processing. Fundam Artif. Intell. 2020, 603–649 (2020).\n 4. Fanni, S. C., Febi, M., Aghakhanyan, G. & Neri, E. Natural language processing. in Introduction to Artificial Intelligence 87–99 \n(Springer, 2023).\n 5. Eisenstein, J. Introduction To Natural Language Processing (MIT Press, 2019).\n 6. Bayer, M. et al. Data augmentation in natural Language processing: a novel text generation approach for long and short text \nclassifiers. Int. J. Mach. Learn. Cybern. 14, 135–150 (2023).\nScientific Reports |        (2025) 15:13755 16| https://doi.org/10.1038/s41598-025-98483-1\nwww.nature.com/scientificreports/\n 7. Li, J., Tang, T., Zhao, W . X., Nie, J. Y . & Wen, J. R. Pre-trained Language models for text generation: A survey. ACM Comput. Surv. \n56, 1–39 (2024).\n 8. Zhao, W . X. et al. A survey of large language models. arXiv Prepr. arXiv2303.18223 (2023).\n 9. Riedl, M. O. Human-centered artificial intelligence and machine learning. Hum. Behav. Emerg. Technol. 1, 33–36 (2019).\n 10. Jiang, Z., Xu, F . F ., Araki, J. & Neubig, G. How can we know what Language models know? Trans. Assoc. Comput. Linguist. 8, \n423–438 (2020).\n 11. Shen, Y . et al. ChatGPT and other large language models are double-edged swords. Radiology vol. 307 e230163 at (2023).\n 12. Myagmar, B., Li, J. & Kimura, S. Cross-Domain sentiment classification with bidirectional contextualized transformer Language \nmodels. IEEE Access. 7, 163219–163230 (2019).\n 13. Singh, S. & Mahmood, A. The NLP cookbook: modern recipes for transformer based deep learning architectures. IEEE Access. 9, \n68675–68702 (2021).\n 14. Y ang, J. et al. Harnessing the power of Llms in practice: A survey on Chatgpt and beyond. ACM Trans. Knowl. Discov Data. 18, \n1–32 (2024).\n 15. Huang, Y . et al. Advancing transformer architecture in long-context large language models: A comprehensive survey. arXiv Prepr. \narXiv2311.12351 (2023).\n 16. Melis, G., Dyer, C. & Blunsom, P . On the state of the art of evaluation in neural language models. arXiv Prepr. arXiv1707.05589 \n(2017).\n 17. Mikolov, T. & others. Statistical language models based on neural networks. (2012).\n 18. Naseem, U., Razzak, I., Khan, S. K. & Prasad, M. A comprehensive survey on word representation models: from classical to state-\nof-the-art word representation Language models. Trans. Asian Low-Resource Lang. Inf. Process. 20, 1–35 (2021).\n 19. Sherstinsky, A. Fundamentals of recurrent neural network (RNN) and long short-term memory (LSTM) network. Phys. D \nNonlinear Phenom. 404, 132306 (2020).\n 20. Subakan, C., Ravanelli, M., Cornell, S., Bronzi, M. & Zhong, J. Attention is all you need in speech separation. in ICASSP –2021 \nIEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 21–25 (2021). (2021).\n 21. Xu, M. et al. A survey of resource-efficient llm and multimodal foundation models. arXiv Prepr. arXiv2401.08092 (2024).\n 22. Jwa, H., Oh, D., Park, K., Kang, J. M. & Lim, H. Exbake: automatic fake news detection model based on bidirectional encoder \nrepresentations from Transformers (bert). Appl. Sci. 9, 4062 (2019).\n 23. Y enduri, G. et al. Gpt (generative pre-trained transformer)--a comprehensive review on enabling technologies, potential \napplications, emerging challenges, and future directions. IEEE Access. 12, 54608–54649 (2024).\n 24. Xue, L. et al. mT5: A massively multilingual pre-trained text-to-text transformer. arXiv Prepr. arXiv11934 (2020). (2010).\n 25. Zhou, X., Zhao, X. & Li, G. LLM-Enhanced Data Management. arXiv Prepr. arXiv2402.02643 (2024).\n 26. Khare, Y . et al. Mmbert: Multimodal bert pretraining for improved medical vqa. in. IEEE 18th International Symposium on \nBiomedical Imaging (ISBI) 1033–1036 (2021). (2021).\n 27. Cui, C. et al. A survey on multimodal large language models for autonomous driving. in Proceedings of the IEEE/CVF Winter \nConference on Applications of Computer Vision 958–979 (2024).\n 28. Ren, Q. et al. A survey on fairness of large language models in e-commerce: progress, application, and challenge. arXiv Prepr. \narXiv2405.13025 (2024).\n 29. Parker, M. J., Anderson, C., Stone, C. & Oh, Y . A large Language model approach to educational survey feedback analysis. Int. J. \nArtif. Intell. Educ. 1–38 (2024).\n 30. Lee, J., Stevens, N., Han, S. C. & Song, M. A survey of large language models in finance (finllms). arXiv Prepr. arXiv2402.02315 \n(2024).\n 31. Cascella, M. et al. The breakthrough of large Language models release for medical applications: 1-year timeline and perspectives. \nJ. Med. Syst. 48, 22 (2024).\n 32. Turing, A. M. Computing machinery and intelligence. Creat Comput. 6, 44–53 (1980).\n 33. Masri, N. et al. Survey of rule-based systems. Int. J. Acad. Inf. Syst. Res. 3, 1–23 (2019).\n 34. Grossberg, S. Recurrent neural networks. Scholarpedia 8, 1888 (2013).\n 35. Salehinejad, H., Sankar, S., Barfett, J., Colak, E. & Valaee, S. Recent advances in recurrent neural networks. arXiv Prepr. \narXiv1801.01078 (2017).\n 36. Johnson, S. J., Murty, M. R. & Navakanth, I. A detailed review on word embedding techniques with emphasis on word2vec. \nMultimed Tools Appl. 83, 37979–38007 (2024).\n 37. Yu, Y ., Si, X., Hu, C. & Zhang, J. A review of recurrent neural networks: LSTM cells and network architectures. Neural Comput. \n31, 1235–1270 (2019).\n 38. Zhao, Z., Chen, W ., Wu, X., Chen, P . C. Y . & Liu, J. LSTM network: a deep learning approach for short-term traffic forecast. IET \nIntell. Transp. Syst. 11, 68–75 (2017).\n 39. Kowsher, M. et al. LSTM-ANN \\& BiLSTM-ANN: hybrid deep learning models for enhanced classification accuracy. Procedia \nComput. Sci. 193, 131–140 (2021).\n 40. Church, K. W . Word2Vec. Nat. Lang. Eng. 23, 155–162 (2017).\n 41. Pennington, J., Socher, R. & Manning, C. D. Glove: Global vectors for word representation. in Proceedings of the conference on \nempirical methods in natural language processing (EMNLP) 1532–1543 (2014). (2014).\n 42. Di Gennaro, G., Buonanno, A. & Palmieri, F . A. N. Considerations about learning Word2Vec. J. Supercomput. 77, 1–16 (2021).\n 43. Ma, L. & Zhang, Y . Using Word2Vec to process big text data. in IEEE International Conference on Big Data (Big Data) 2895–2897 \n(2015). (2015).\n 44. Abubakar, H. D., Umar, M. & Bakale, M. A. Sentiment classification: review of text vectorization methods: bag of words, Tf-Idf, \nWord2vec and Doc2vec. SLU J. Sci. Technol. 4, 27–33 (2022).\n 45. Sivakumar, S. et al. Review on word2vec word embedding neural net. in. international conference on smart electronics and \ncommunication (ICOSEC) 282–290 (2020). (2020).\n 46. Curto, G., Jojoa Acosta, M. F ., Comim, F . & Garcia-Zapirain, B. Are AI systems biased against the poor? A machine learning \nanalysis using Word2Vec and glove embeddings. AI \\& Soc. 39, 617–632 (2024).\n 47. Singgalen, Y . A. Implementation of global vectors for word representation (GloVe) model and social network analysis through \nwonderland Indonesia content reviews. J. Sist Komput Dan. Inf. 5, 559–569 (2024).\n 48. Sitender, S., Sushma, N. S. & Sharma, S. K. Effect of GloVe, Word2Vec and fastText embedding on english and hindi neural \nmachine translation systems. in Proceedings of Data Analytics and Management: ICDAM 2022 433–447Springer, (2023).\n 49. Kang, S., Kong, L., Luo, B., Zheng, C. & Wu, J. Principle research of word vector representation in natural language processing. in \nInternational Conference on Electronic Information Engineering and Computer Science (EIECS vol. 12602 54–60 (2023). (2022).\n 50. Adawiyah, A. R., Baharuddin, B., Wardana, L. A. & Farmasari, S. Comparing post-editing translations by Google NMT and \nY andex NMT. TEKNOSASTIK 21, 23–34 (2023).\n 51. Mo, Y ., Qin, H., Dong, Y ., Zhu, Z. & Li, Z. Large language model (llm) ai text generation detection based on transformer deep \nlearning algorithm. arXiv Prepr. arXiv2405.06652 (2024).\n 52. Oliaee, A. H., Das, S., Liu, J. & Rahman, M. A. Using bidirectional encoder representations from Transformers (BERT) to classify \ntraffic crash severity types. Nat. Lang. Process. J. 3, 100007 (2023).\nScientific Reports |        (2025) 15:13755 17| https://doi.org/10.1038/s41598-025-98483-1\nwww.nature.com/scientificreports/\n 53. Wibawa, A. P ., Cahyani, D. E., Prasetya, D. D., Gumilar, L. & Nafalski, A. Detecting emotions using a combination of bidirectional \nencoder representations from Transformers embedding and bidirectional long short-term memory. Int. J. Electr. \\& Comput. Eng. \n13, 2088–8708 (2023).\n 54. Areshey, A. & Mathkour, H. Transfer learning for sentiment classification using bidirectional encoder representations from \nTransformers (BERT) model. Sensors 23, 5232 (2023).\n 55. Hendy, A. et al. How good are gpt models at machine translation? a comprehensive evaluation. arXiv Prepr. arXiv2302.09210 \n(2023).\n 56. Hanna, M., Liu, O. & Variengien, A. How does GPT-2 compute greater-than? Interpreting mathematical abilities in a pre-trained \nLanguage model. Adv. Neural Inf. Process. Syst. 36, 76033–76060 (2024).\n 57. Bharathi Mohan, G. et al. Text summarization for big data analytics: a comprehensive review of GPT 2 and BERT approaches. \nData Anal. Internet Things Infrastruct. 247–264 (2023).\n 58. Kalyan, K. S. A survey of GPT-3 family large Language models including ChatGPT and GPT-4. Nat. Lang. Process. J. 6, 100048 \n(2023).\n 59. Y an, B. et al. On protecting the data privacy of large language models (llms): A survey. arXiv Prepr. arXiv2403.05156 (2024).\n 60. Li, Y ., Wang, S., Ding, H. & Chen, H. Large language models in finance: A survey. in Proceedings of the fourth ACM international \nconference on AI in finance 374–382 (2023).\n 61. Zhang, Z. et al. Large language models for mobility in transportation systems: A survey on forecasting tasks. arXiv Prepr. \narXiv2405.02357 (2024).\n 62. Wang, S. et al. Large language models for education: A survey and outlook. arXiv Prepr. arXiv2403.18105 (2024).\n 63. Zhang, D., Zheng, H., Yue, W . & Wang, X. Advancing ITS Applications with LLMs: A Survey on Traffic Management, \nTransportation Safety, and Autonomous Driving. in International Joint Conference on Rough Sets 295–309 (2024).\n 64. Xu, X., Xu, Z., Ling, Z., Jin, Z. & Du, S. Emerging Synergies Between Large Language Models and Machine Learning in Ecommerce \nRecommendations. arXiv Prepr. arXiv2403.02760 (2024).\n 65. Chen, J. et al. When large Language models Meet personalization: perspectives of challenges and opportunities. World Wide Web. \n27, 42 (2024).\n 66. Xu, H., Gan, W ., Qi, Z., Wu, J. & Yu, P . S. Large Language Models for Education: A Survey. arXiv Prepr. arXiv2405.13001 (2024).\n 67. Huber, S. E. et al. Leveraging the potential of large Language models in education through playful and game-based learning. Educ. \nPsychol. Rev. 36, 25 (2024).\n 68. Y an, L. et al. Practical and ethical challenges of large Language models in education: A systematic scoping review. Br. J. Educ. \nTechnol. 55, 90–112 (2024).\n 69. Y ahyazadeh, N. The Influence of ChatGPT in Education: A Comprehensive Review. (2023).\n 70. Zhao, H. et al. Revolutionizing finance with llms: An overview of applications and insights. arXiv Prepr. arXiv2401.11641 (2024).\n 71. Godwin Olaoye, H. J. The Evolving Role of Large Language Models (LLMs) in Banking. (2024).\n 72. Fieberg, C., Hornuf, L. & Streich, D. Using large Language models for financial advice. Available SSRN 4850039, 92 (2024).\n 73. Huang, Y ., Tang, K. & Chen, M. A. Comprehensive Survey on Evaluating Large Language Model Applications in the Medical \nIndustry. arXiv Prepr. arXiv2404.15777 (2024).\n 74. Zheng, Y . et al. Large Language Models for Medicine: A Survey. arXiv Prepr. arXiv2405.13055 (2024).\n 75. Yu, P ., Xu, H., Hu, X. & Deng, C. Leveraging generative AI and large Language models: a Comprehensive Roadmap for Healthcare \nIntegration. in Healthcare vol. 11 2776 (2023).\n 76. George, J. G. Transforming Banking in the Digital Age: The Strategic Integration of Large Language Models and Multi-Cloud \nEnvironments.\n 77. Dhillon, A. S. & Torresin, A. Advancing Vehicle Diagnostic: Exploring the Application of Large Language Models in the \nAutomotive Industry. (2024).\n 78. Gebreab, S. A., Salah, K., Jayaraman, R., ur Rehman, M. & Ellaham, S. LLM-Based Framework for Administrative Task Automation \nin Healthcare. in 12th International Symposium on Digital Forensics and Security (ISDFS) 1–7 (2024). (2024).  h t t p s : / / d o i . o r g / 1 0 . 1 1 \n0 9 / I S D F S 6 0 7 9 7 . 2 0 2 4 . 1 0 5 2 7 2 7 5       \n 79. Jin, H. et al. Llm maybe longlm: Self-extend llm context window without tuning. arXiv Prepr. arXiv2401.01325 (2024).\n 80. Zhang, T., Yi, J. W ., Y ao, B., Xu, Z. & Shrivastava, A. Nomad-attention: Efficient llm inference on cpus through multiply-add-free \nattention. arXiv Prepr. arXiv2403.01273 (2024).\n 81. Lin, X. et al. Efficient LLM Training and Serving with Heterogeneous Context Sharding among Attention Heads. arXiv Prepr. \narXiv2407.17678 (2024).\n 82. Lu, Y . et al. LongHeads: Multi-Head Attention is Secretly a Long Context Processor. arXiv Prepr. arXiv2402.10685 (2024).\n 83. Liu, Y . et al. Understanding llms: A comprehensive overview from training to inference. arXiv Prepr. arXiv2401.02038 (2024).\n 84. Raje, A. & Communication-Efficient, L. L. M. Training for Federated LearningPh. D. thesis, Carnegie Mellon University \nPittsburgh, PA,. (2024).\n 85. Zeng, F ., Gan, W ., Wang, Y . & Philip, S. Y . Distributed training of large language models. in IEEE 29th International Conference on \nParallel and Distributed Systems (ICPADS) 840–847 (2023). (2023).\n 86. McKinzie, B. et al. Mm1: Methods, analysis \\& insights from multimodal llm pre-training. arXiv Prepr. arXiv2403.09611 (2024).\n 87. Abbasiantaeb, Z., Yuan, Y ., Kanoulas, E. & Aliannejadi, M. Let the llms talk: Simulating human-to-human conversational qa via \nzero-shot llm-to-llm interactions. in Proceedings of the 17th ACM International Conference on Web Search and Data Mining 8–17 \n(2024).\n 88. Lin, X. et al. Data-efficient Fine-tuning for LLM-based Recommendation. in Proceedings of the 47th International ACM SIGIR \nConference on Research and Development in Information Retrieval 365–374 (2024).\n 89. Liu, Q. et al. When MOE Meets LLMs: Parameter Efficient Fine-tuning for Multi-task Medical Applications. in Proceedings of the \n47th International ACM SIGIR Conference on Research and Development in Information Retrieval 1104–1114 (2024).\n 90. Christophe, C. et al. Med42–Evaluating Fine-Tuning Strategies for Medical LLMs: Full-Parameter vs. Parameter-Efficient \nApproaches. arXiv Prepr. arXiv2404.14779 (2024).\n 91. Xue, T., Wang, Z. & Ji, H. Parameter-efficient tuning helps language model alignment. arXiv Prepr. arXiv2310.00819 (2023).\n 92. Han, Z., Gao, C., Liu, J. & Zhang, S. Q. & others. Parameter-efficient fine-tuning for large models: A comprehensive survey. arXiv \nPrepr. arXiv2403.14608 (2024).\n 93. Vaswani, A. et al. Attention is all you need. Adv. Neural Inf. Process. Syst. 30, 5998–6008 (2017).\n 94. Bhattamishra, S., Patel, A., Blunsom, P . & Kanade, V . Understanding in-context learning in transformers and llms by learning to \nlearn discrete functions. arXiv Prepr. arXiv2310.03016 (2023).\n 95. Y ousri, R. & Safwat, S. How Big Can It Get? A comparative analysis of LLMs in architecture and scaling. in International \nConference on Computer and Applications (ICCA) 1–5 (2023). (2023).\n 96. Peng, B., Narayanan, S. & Papadimitriou, C. On limitations of the transformer architecture. arXiv Prepr. arXiv2402.08164 (2024).\n 97. Du, W . et al. Stacking Y our Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training. arXiv Prepr. \narXiv2405.15319 (2024).\n 98. Cao, Y . T. et al. On the intrinsic and extrinsic fairness evaluation metrics for contextualized language representations. arXiv Prepr. \narXiv2203.13928 (2022).\n 99. Meister, C. & Cotterell, R. Language model evaluation beyond perplexity. arXiv Prepr. arXiv2106.00085 (2021).\nScientific Reports |        (2025) 15:13755 18| https://doi.org/10.1038/s41598-025-98483-1\nwww.nature.com/scientificreports/\n 100. Colla, D., Delsanto, M., Agosto, M., Vitiello, B. & Radicioni, D. P . Semantic coherence markers: the contribution of perplexity \nmetrics. Artif. Intell. Med. 134, 102393 (2022).\n 101. Soni, A. Enhancing Multilingual Table-to-Text Generation with QA Blueprints: Overcoming Challenges in Low-Resource \nLanguages. in International Conference on Signal Processing and Advance Research in Computing (SPARC)  vol. 1 1–7 (2024). \n(2024).\n 102. Chauhan, S. et al. Semantic-syntactic similarity based automatic machine translation evaluation metric. IETE J. Res. 70, 3823–\n3834 (2024).\n 103. Mander, S., Phillips, J. & LiSAScore Exploring Linear Sum Assignment on BertScore. in International Conference on Applications \nof Natural Language to Information Systems 249–257 (2024).\n 104. Shankar, S., Zamfirescu-Pereira, J. D., Hartmann, B., Parameswaran, A. & Arawjo, I. Who validates the validators? aligning llm-\nassisted evaluation of llm outputs with human preferences. in Proceedings of the 37th Annual ACM Symposium on User Interface \nSoftware and Technology 1–14 (2024).\n 105. Wang, Y . et al. The fluency-based semantic network of LLMs differs from humans. Comput. Hum. Behav. Artif. Hum. 3, 100103 \n(2025).\n 106. Anderson, C., Vandenberg, B., Hauser, C., Johansson, A. & Galloway, N. Semantic coherence dynamics in large language models \nthrough layered syntax-aware memory retention mechanism. (2024).\n 107. Meng, C., Arabzadeh, N., Askari, A., Aliannejadi, M. & de Rijke, M. Query performance prediction using relevance judgments \ngenerated by large language models. arXiv Prepr. arXiv2404.01012 (2024).\n 108. Chu, Z., Wang, Z. & Zhang, W . Fairness in large Language models: A taxonomic survey. ACM SIGKDD Explor. Newsl. 26, 34–48 \n(2024).\n 109. Bai, G. et al. Beyond efficiency: A systematic survey of resource-efficient large language models. arXiv Prepr. arXiv2401.00625 \n(2024).\n 110. Lukasik, M., Narasimhan, H., Menon, A. K., Yu, F . & Kumar, S. Metric-aware LLM inference. arXiv Prepr. arXiv2403.04182 \n(2024).\n 111. Wolters, C., Y ang, X., Schlichtmann, U. & Suzumura, T. Memory Is All Y ou Need: An Overview of Compute-in-Memory \nArchitectures for Accelerating Large Language Model Inference. arXiv Prepr. arXiv2406.08413 (2024).\n 112. Stojkovic, J., Zhang, C., Goiri, Í., Torrellas, J. & Choukse, E. Dynamollm: Designing llm inference clusters for performance and \nenergy efficiency. arXiv Prepr. arXiv2408.00741 (2024).\n 113. Kenthapadi, K., Sameki, M. & Taly, A. Grounding and evaluation for large language models: Practical challenges and lessons \nlearned (survey). in Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining  6523–6533 \n(2024).\n 114. Laskar, M. T. R. et al. A systematic survey and critical review on evaluating large language models: Challenges, limitations, and \nrecommendations. in Proceedings of the Conference on Empirical Methods in Natural Language Processing 13785–13816 (2024). \n(2024).\n 115. Chang, Y . et al. A survey on evaluation of large Language models. ACM Trans. Intell. Syst. Technol. 15, 1–45 (2024).\n 116. Reese, M. L. & Smirnova, A. Comparing ChatGPT and Humans on World Knowledge and Common-sense Reasoning Tasks: \nA case study of the Japanese Winograd Schema Challenge. in Extended Abstracts of the CHI Conference on Human Factors in \nComputing Systems 1–9 (2024).\n 117. Zahraei, P . S., Emami, A. & WSC+ Enhancing The Winograd Schema Challenge Using Tree-of-Experts. arXiv Prepr. \narXiv2401.17703 (2024).\n 118. Christen, P ., Hand, D. J. & Kirielle, N. A. Review of the F-Measure: its history, properties, criticism, and alternatives. ACM \nComput. Surv. 56, 1–24 (2023).\n 119. Jiang, Z., Anastasopoulos, A., Araki, J., Ding, H. & Neubig, G. X-FACTR: Multilingual factual knowledge retrieval from pretrained \nlanguage models. in Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)  5943–5959 \n(2020). (2020).\n 120. Shaik, R. & Kishore, K. S. Enhancing Text Generation in Joint NLG/NLU Learning Through Curriculum Learning, Semi-\nSupervised Training, and Advanced Optimization Techniques. arXiv Prepr. arXiv2410.13498 (2024).\n 121. Dong, C. et al. A survey of natural Language generation. ACM Comput. Surv. 55, 1–38 (2022).\n 122. Kenton, Z. et al. On scalable oversight with weak llms judging strong llms. arXiv Prepr. arXiv2407.04622 (2024).\n 123. Huang, Y . et al. New solutions on LLM acceleration, optimization, and application. in Proceedings of the 61st ACM/IEEE Design \nAutomation Conference 1–4 (2024).\n 124. Cassano, F . et al. Knowledge transfer from high-resource to low-resource programming languages for code llms. Proc. ACM \nProgram. Lang. 8, 677–708 (2024).\n 125. Kazi, N. & Kahanda, I. Enhancing Transfer Learning of LLMs through Fine-Tuning on Task-Related Corpora for Automated \nShort-Answer Grading. in International Conference on Machine Learning and Applications (ICMLA) 1687–1691 (2023). (2023).\n 126. Waisberg, E. et al. GPT-4: a new era of artificial intelligence in medicine. Ir. J. Med. Sci. 192, 3197–3200 (2023).\n 127. Liu, X. et al. GPT understands, too. AI Open (2023).\n 128. Chitty-Venkata, K. T., Emani, M., Vishwanath, V . & Somani, A. K. Neural architecture search for Transformers: A survey. IEEE \nAccess. 10, 108374–108412 (2022).\n 129. Wang, B. et al. DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. in NeurIPS (2023).\n 130. Devlin, J., Chang, M. W ., Lee, K., Toutanova, K. & Bert Pre-training of deep bidirectional transformers for language understanding. \narXiv Prepr. arXiv1810.04805 (2018).\n 131. Tan, Y ., Jiang, L., Chen, P ., Tong, C. & DQMix-BERT Distillation-aware Quantization with Mixed Precision for BERT \nCompression. in IEEE International Conference on Systems, Man, and Cybernetics (SMC) 311–316 (2023). (2023).  h t t p s : / / d o i . o r g \n/ 1 0 . 1 1 0 9 / S M C 5 3 9 9 2 . 2 0 2 3 . 1 0 3 9 4 6 4 2       \n 132. Riaz, M. T., Shah Jahan, M., Khawaja, S. G., Shaukat, A. & Zeb, J. TM-BERT: A Twitter Modified BERT for Sentiment Analysis on \nCovid-19 Vaccination Tweets. in 2nd International Conference on Digital Futures and Transformative Technologies (ICoDT2) 1–6 \n(2022). (2022). https://doi.org/10.1109/ICoDT255437.2022.9787395\n 133. Anggrainingsih, R., Hassan, G. M. & Datta, A. C. E. B. E. R. T. Concise and efficient BERT-Based model for detecting rumors on \nTwitter. IEEE Access. 11, 80207–80217 (2023).\n 134. Sohrab, M. G., Asada, M., Rikters, M., Miwa, M. & BERT-NAR-BERT A Non-Autoregressive Pre-Trained Sequence-to-Sequence \nmodel leveraging BERT checkpoints. IEEE Access. 12, 23–33 (2024).\n 135. Lan, Z. et al. Albert: A lite bert for self-supervised learning of language representations. arXiv Prepr. arXiv11942 (2019). (1909).\n 136. Tripathy, J. K., Chakkaravarthy, S. S., Satapathy, S. C. & Sahoo, M. Vaidehi, V . ALBERT-based fine-tuning model for cyberbullying \nanalysis. Multimed Syst. 28, 1941–1949 (2022).\n 137. Chiang, C. H., Huang, S. F . & Lee, H. Pretrained language model embryology: The birth of ALBERT. arXiv Prepr. arXiv02480 \n(2020). (2010).\n 138. Mastropaolo, A. et al. Studying the usage of text-to-text transfer transformer to support code-related tasks. in. IEEE/ACM 43rd \nInternational Conference on Software Engineering (ICSE) 336–347 (2021). (2021).\n 139. Ni, J. et al. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. arXiv Prepr. arXiv2108.08877 (2021).\n 140. Phan, L. N. et al. Scifive: a text-to-text transformer model for biomedical literature. arXiv Prepr. arXiv2106.03598 (2021).\nScientific Reports |        (2025) 15:13755 19| https://doi.org/10.1038/s41598-025-98483-1\nwww.nature.com/scientificreports/\n 141. Y ang, Z. et al. Xlnet: generalized autoregressive pretraining for Language Understanding. Adv. Neural Inf. Process. Syst. 32, 5753–\n5763 (2019).\n 142. Topal, M. O., Bas, A. & van Heerden, I. Exploring transformers in natural language generation: Gpt, bert, and xlnet. arXiv Prepr. \narXiv2102.08036 (2021).\n 143. Adoma, A. F ., Henry, N. M. & Chen, W . Comparative analyses of bert, roberta, distilbert, and xlnet for text-based emotion \nrecognition. in 17th International Computer Conference on Wavelet Active Media Technology and Information Processing \n(ICCWAMTIP) 117–121 (2020). (2020).\n 144. Thoppilan, R. et al. Lamda: Language models for dialog applications. arXiv Prepr. arXiv2201.08239 (2022).\n 145. Morales, L., Herrera, M., Camacho, O., Leica, P . & Aguilar, J. LAMDA control approaches applied to trajectory tracking for mobile \nrobots. IEEE Access. 9, 37179–37195 (2021).\n 146. Ruiz, F . A., Isaza, C. V ., Agudelo, A. F . & Agudelo, J. R. A new criterion to validate and improve the classification process of \nLAMDA algorithm applied to diesel engines. Eng. Appl. Artif. Intell. 60, 117–127 (2017).\n 147. Touvron, H. et al. Llama: Open and efficient foundation language models. arXiv Prepr. arXiv2302.13971 (2023).\n 148. Zhang, R. et al. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv Prepr. arXiv2303.16199 \n(2023).\n 149. Sayin, B., Minervini, P ., Staiano, J. & Passerini, A. Can LLMs Correct Physicians, Y et? Investigating Effective Interaction Methods \nin the Medical Domain. arXiv Prepr. arXiv2403.20288 (2024).\n 150. Wang, S., Liu, T., Kinoshita, S. & Y okoyama, H. M. LLMs May improve medical communication: social science perspective. \nPostgrad. Med. J. 101, qgae101 (2024).\n 151. Kim, Y . et al. Adaptive Collaboration Strategy for LLMs in Medical Decision Making. arXiv Prepr. arXiv2404.15155 (2024).\n 152. Wang, Y ., Ma, X. & Chen, W . Augmenting black-box llms with medical textbooks for clinical question answering. arXiv Prepr. \narXiv2309.02233 (2023).\n 153. Goel, A. et al. Llms accelerate annotation for medical information extraction. in Machine Learning for Health (ML4H) 82–100 \n(2023).\n 154. Aparicio, V ., Gordon, D., Huayamares, S. G., Luo, Y . & BioFinBERT Finetuning Large Language Models (LLMs) to Analyze \nSentiment of Press Releases and Financial Text Around Inflection Points of Biotech Stocks. arXiv Prepr. arXiv2401.11011 (2024).\n 155. Kumar, R., Gattani, D. R. K. & Singh, K. Enhancing Medical History Collection using LLMs. in Proceedings of the Australasian \nComputer Science Week 140–143 (2024). (2024).\n 156. Wang, Z., Luo, X., Jiang, X., Li, D. & Qiu, L. LLM-RadJudge: Achieving Radiologist-Level Evaluation for X-Ray Report Generation. \narXiv Prepr. arXiv2404.00998 (2024).\n 157. Garc\\’\\ia-Ferrero, I. et al. MedMT5: An Open-Source Multilingual Text-to-Text LLM for the Medical Domain. in Proceedings \nof the Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)  \n11165–11177 (2024). (2024).\n 158. Y ang, R. et al. Large Language models in health care: development, applications, and challenges. Heal Care Sci. 2, 255–263 (2023).\n 159. Zhou, Z., Y ang, T. & Hu, K. Traditional Chinese Medicine Epidemic Prevention and Treatment Question-Answering Model \nBased on LLMs. in IEEE International Conference on Bioinformatics and Biomedicine (BIBM) 4755–4760 (2023). (2023).  h t t p s : / / d \no i . o r g / 1 0 . 1 1 0 9 / B I B M 5 8 8 6 1 . 2 0 2 3 . 1 0 3 8 5 7 4 8       \n 160. Wang, Z., Li, K., Ren, Q., Y ao, K. & Zhu, Y . Traditional Chinese Medicine Formula Classification Using Large Language Models. \nin IEEE International Conference on Bioinformatics and Biomedicine (BIBM) 4647–4654 (2023). (2023).  h t t p s : / / d o i . o r g / 1 0 . 1 1 0 9 / B \nI B M 5 8 8 6 1 . 2 0 2 3 . 1 0 3 8 5 7 7 6       \n 161. Dou, Y . et al. ShennongGPT: A Tuning Chinese LLM for Medication Guidance. in. IEEE International Conference on Medical \nArtificial Intelligence (MedAI) 67–72 (2023). (2023). https://doi.org/10.1109/MedAI59581.2023.00017\n 162. Helwan, A., Azar, D. & Ozsahin, D. U. Medical Reports Summarization Using Text-To-Text Transformer. in Advances in Science and \nEngineering Technology International Conferences (ASET) 1–4 (2023). (2023). https://doi.org/10.1109/ASET56582.2023.10180671\n 163. Cardenas, L., Parajes, K., Zhu, M., Zhai, S. & AutoHealth Advanced LLM-Empowered Wearable Personalized Medical Butler \nfor Parkinson’s Disease Management. in IEEE 14th Annual Computing and Communication Workshop and Conference (CCWC) \n375–379 (2024). (2024). https://doi.org/10.1109/CCWC60891.2024.10427622\n 164. Karttunen, P ., Vavekanand, R., Xu, Y ., Milani, S. & Li, H. Large Language models in healthcare decision support: A review. \nAvailable SSRN 4892593 (2023).\n 165. Krishnan, G. et al. Artificial intelligence in clinical medicine: catalyzing a sustainable global healthcare paradigm. Front. Artif. \nIntell. 6, 1227091 (2023).\n 166. Kumar, A. et al. A survey on IBM watson and its services. in Journal of Physics: Conference Series vol. 2273 12022 (2022).\n 167. Piotrkowicz, A., Johnson, O. & Hall, G. Finding relevant free-text radiology reports at scale with IBM Watson content analytics: \na feasibility study in the UK NHS. J. Biomed. Semant. 10, 21 (2019).\n 168. Lyu, Y . et al. Gp-gpt: Large language model for gene-phenotype mapping. arXiv Prepr. arXiv2409.09825 (2024).\n 169. Kang, I., Van Woensel, W . & Seneviratne, O. Using large Language models for generating smart contracts for health insurance \nfrom textual policies. In (Eds. Shaban-Nejad, M. Michalowski, & S. Bianco) AI for Health Equity and Fairness: Leveraging AI To \nAddress Social Determinants of Health 129–146 (Springer, 2024).\n 170. Nazi, Z., Al & Peng, W . Large language models in healthcare and medical domain: A review. in Informatics vol. 11 57 (2024).\n 171. Nankya, M., Mugisa, A., Usman, Y ., Upadhyay, A. & Chataut, R. Security and privacy in E-Health systems: A review of AI and \nmachine learning techniques. IEEE Access. 12 (2024).\n 172. Reddy, S. Generative AI in healthcare: an implementation science informed translational path on application, integration and \ngovernance. Implement. Sci. 19, 27 (2024).\n 173. Mennella, C., Maniscalco, U., De Pietro, G. & Esposito, M. Ethical and regulatory challenges of AI technologies in healthcare: A \nnarrative review. Heliyon (2024).\n 174. Bedi, S. et al. Testing and evaluation of health care applications of large Language models: a systematic review. JAMA 10 (2024).\n 175. Desai, B. & Patil, K. Secure and scalable Multi-Modal vehicle systems: A Cloud-Based framework for Real-Time LLM-Driven \ninteractions. Innov. Comput. Sci. J. 9, 1–11 (2023).\n 176. Cheng, Z. Q. et al. SHIELD: LLM-Driven Schema Induction for Predictive Analytics in EV Battery Supply Chain Disruptions. \narXiv Prepr. arXiv2408.05357 (2024).\n 177. Wase, Z. M., Madisetti, V . K. & Bahga, A. Object detection Meets LLMs: model fusion for safety and security. J. Softw. Eng. Appl. \n16, 672–684 (2023).\n 178. Moeini, M., Ahmadian, R. & Ghatee, M. Calibrated SVM for Probabilistic Classification of In-Vehicle Voices into Vehicle \nCommands via Voice-to-Text LLM Transformation. in 8th International Conference on Smart Cities, Internet of Things and \nApplications (SCIoT) 180–188 (2024). (2024).\n 179. Bilgram, V . & Laarmann, F . Accelerating innovation with generative AI: AI-augmented digital prototyping and innovation \nmethods. IEEE Eng. Manag Rev. 51, 18–25 (2023).\n 180. Osten, W ., Bett, C. & Situ, G. The challenge of making self-driving cars: may AI help to overcome the risks, or should we focus on \nreliable sensor technologies? in Interferometry and Structured Light 2024 vol. 13135 8–21 (2024).\n 181. Li, L. et al. Data-centric evolution in autonomous driving: A comprehensive survey of big data system, data mining, and closed-\nloop technologies. arXiv Prepr. arXiv2401.12888 (2024).\n 182. Sanders, N. R. Supply Chain Management: A Global Perspective (John Wiley \\& Sons, 2025).\nScientific Reports |        (2025) 15:13755 20| https://doi.org/10.1038/s41598-025-98483-1\nwww.nature.com/scientificreports/\n 183. Mueller-Saegebrecht, S. & Lippert, I. In Tandem with ChatGPT-4: How LLM Enhance Entrepreneurship Education and Business \nModel Innovation. in Academy of Management Proceedings vol. 2024 15473 (2024).\n 184. Borah, A. & Rutz, O. Enhanced sales forecasting model using textual search data: fusing dynamics with big data. Int. J. Res. Mark. \n41 (2024).\n 185. Y ang, Z., Jia, X., Li, H. & Y an, J. Llm4drive: A survey of large language models for autonomous driving. in NeurIPS 2024 Workshop \non Open-World Agents (2023).\n 186. Baccari, S., Hadded, M., Ghazzai, H., Touati, H. & Elhadef, M. Anomaly detection in connected and autonomous vehicles: A \nsurvey, analysis, and research challenges. IEEE Access. 12 (2024).\n 187. Mart\\’\\inez, I. The Future of the Automotive Industry (Springer, 2021).\n 188. Abdelati, M. H., Mokbel, E. F . F ., Abdelwali, H. A., Matar, A. H. & Rabie, M. Revolutionizing automotive engineering with \nartificial neural networks: applications, challenges, and future directions. J. Sci. Insights. 1, 155–169 (2024).\n 189. Garikapati, D. & Shetiya, S. S. Autonomous vehicles: evolution of artificial intelligence and the current industry landscape. Big \nData Cogn. Comput. 8, 42 (2024).\n 190. Muzahid, A. J. M., Zhao, X. & Wang, Z. Survey on Human-Vehicle Interactions and AI Collaboration for Optimal Decision-\nMaking in Automated Driving. arXiv Prepr. arXiv2412.08005 (2024).\n 191. Tyagi, A. K., Mishra, A. K. & Kukreja, S. Role of Artificial Intelligence Enabled Internet of Things (IoT) in the Automobile \nIndustry: Opportunities and Challenges for Society. in International Conference on Cognitive Computing and Cyber Physical \nSystems 379–397 (2023).\n 192. Gao, D. et al. LLMs-based machine translation for E-commerce. Expert Syst. Appl. 258, 125087 (2024).\n 193. Chen, K. et al. General2Specialized LLMs Translation for E-commerce. in Companion Proceedings of the ACM on Web Conference \n2024 670–673 (2024).\n 194. Fang, C. et al. Llm-ensemble: Optimal large language model ensemble method for e-commerce product attribute value extraction. \nin Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval 2910–2914 \n(2024).\n 195. Chen, B., Dai, H., Ma, X., Jiang, W . & Ning, W . Robust Interaction-based Relevance Modeling for Online E-Commerce and LLM-\nbased Retrieval. arXiv Prepr. arXiv2406.02135 (2024).\n 196. Dam, S. K., Hong, C. S., Qiao, Y . & Zhang, C. A complete survey on llm-based ai chatbots. arXiv Prepr. arXiv2406.16937 (2024).\n 197. Casheekar, A., Lahiri, A., Rath, K., Prabhakar, K. S. & Srinivasan, K. A contemporary review on chatbots, AI-powered virtual \nconversational agents, ChatGPT: applications, open challenges and future research directions. Comput. Sci. Rev.  52, 100632 \n(2024).\n 198. Roumeliotis, K. I., Tselikas, N. D. & Nasiopoulos, D. K. Precision-Driven product recommendation software: unsupervised \nmodels, evaluated by GPT-4 LLM for enhanced recommender systems. Software 3, 62–80 (2024).\n 199. Guyt, J. Y ., Datta, H. & Boegershausen, J. Unlocking the potential of web data for retailing research. J. Retail. 100, 130–147 (2024).\n 200. Soni, V . Large Language models for enhancing customer lifecycle management. J. Empir. Soc. Sci. Stud. 7, 67–89 (2023).\n 201. Lin, J. et al. How can recommender systems benefit from large language models: A survey. arXiv Prepr. arXiv2306.05817 (2023).\n 202. Roumeliotis, K. I., Tselikas, N. D. & Nasiopoulos, D. K. LLMs in e-commerce: a comparative analysis of GPT and LLaMA models \nin product review evaluation. Nat. Lang. Process. J. 6, 100056 (2024).\n 203. Wu, L. et al. A survey on large Language models for recommendation. World Wide Web. 27, 60 (2024).\n 204. Provasi, V . The AI revolution: evaluating impact and consequences in copywriting. (2023).\n 205. Johnsen, M. AI in Digital Marketing (Walter de Gruyter GmbH \\& Co KG, 2024).\n 206. Richey, R. G. Jr, Chowdhury, S., Davis-Sramek, B., Giannakis, M. & Dwivedi, Y . K. Artificial intelligence in logistics and supply \nchain management: A primer and roadmap for research. Journal of Business Logistics vol. 44 532–549 at (2023).\n 207. Li, Y . et al. Large language models for manufacturing. arXiv Prepr. arXiv2410.21418 (2024).\n 208. Latif, E., Fang, L., Ma, P . & Zhai, X. Knowledge distillation of llm for education. arXiv Prepr. arXiv2312.15842 (2023).\n 209. Zhang, Z. et al. Simulating Classroom Education with LLM-Empowered Agents. arXiv Prepr. arXiv2406.19226 (2024).\n 210. Chen, L. et al. BIDTrainer: An LLMs-driven Education Tool for Enhancing the Understanding and Reasoning in Bio-inspired \nDesign. in Proceedings of the CHI Conference on Human Factors in Computing Systems 1–20 (2024).\n 211. Diez-Rozas, V ., Estevez-Ayres, I., Alario-Hoyos, C. & Callejo, P . & Delgado Kloos, C. A Web Application for a Cost-Effective Fine-\nTuning of Open-Source LLMs in Education. in International Conference on Artificial Intelligence in Education 267–274 (2024).\n 212. Ouyang, Z., Jiang, Y . & Liu, H. The effects of Duolingo, an AI-Integrated technology, on EFL learners’ willingness to communicate \nand engagement in online classes. Int. Rev. Res. Open. Distrib. Learn. 25, 97–115 (2024).\n 213. Shahzad, T. et al. A comprehensive review of large Language models: issues and solutions in learning environments. Discov \nSustain. 6, 27 (2025).\n 214. Upadhyay, A., Farahmand, E., Muñoz, I., Akber Khan, M. & Witte, N. Influence of LLMs on learning and teaching in higher \neducation. Available SSRN 4716855 (2024).\n 215. Chen, Z. et al. Evolution and prospects of foundation models: from large Language models to large multimodal models. Comput. \nMater. \\& Contin 80, 1753 (2024).\n 216. Xu, R. et al. Knowledge conflicts for llms: A survey. arXiv Prepr. arXiv2403.08319 (2024).\n 217. Dai, S. et al. Bias and unfairness in information retrieval systems: New challenges in the llm era. in Proceedings of the 30th ACM \nSIGKDD Conference on Knowledge Discovery and Data Mining 6437–6447 (2024).\n 218. Razafinirina, M. A., Dimbisoa, W . G. & Mahatody, T. Pedagogical alignment of large Language models (LLM) for personalized \nlearning: A survey, trends and challenges. J. Intell. Learn. Syst. Appl. 16, 448–480 (2024).\n 219. Liu, S., Guo, X., Hu, X. & Zhao, X. Advancing generative intelligent tutoring systems with GPT-4: design, evaluation, and a \nmodular framework for future learning platforms. Electronics 13, 4876 (2024).\n 220. Y ousefi, M., Mullick, J. & Wang, Q. Preparing teachers and students for the challenges of society 5.0: integration of cognitive \ncomputing, learning analytics, and gamification of learning. Preconceptions Policies Strateg Challenges Educ. 5.0, 75–99 (2024).\n 221. Wu, S. et al. Bloomberggpt: A large language model for finance. arXiv Prepr. arXiv2303.17564 (2023).\n 222. de Zarzà, I., de Curtò, J., Roig, G. & Calafate, C. T. Optimized financial planning: integrating individual and cooperative budgeting \nmodels with LLM recommendations. AI 5, 91–114 (2023).\n 223. Zhang, B., Y ang, H., Zhou, T., Babar, A. & Liu, X. Y . M. Enhancing financial sentiment analysis via retrieval augmented large \nlanguage models. in Proceedings of the fourth ACM international conference on AI in finance 349–356 (2023).\n 224. de Moraes, D. et al. S. Using Zero-shot Prompting in the Automatic Creation and Expansion of Topic Taxonomies for Tagging \nRetail Banking Transactions. arXiv Prepr. arXiv2401.06790 (2024).\n 225. Policepatil, S. et al. IGI Global,. Financial Sector Hyper-Automation: Transforming Banking and Investing Procedures. in \nExamining Global Regulations During the Rise of Fintech 299–318 (2025).\n 226. Reda, M. A. Intelligent Assistant Agents: Comparative Analysis of Chatbots through Diverse Methodologies. GSJ 12, (2024).\n 227. Alagic, A. et al. Machine learning for an enhanced credit risk analysis: A comparative study of loan approval prediction models \nintegrating mental health data. Mach. Learn. Knowl. Extr. 6, 53–77 (2024).\n 228. Saxena, A., Verma, S. & Mahajan, J. Transforming banking: the next frontier. In (eds. Saxena, A., Verma, S. & Mahajan, J.) \nGenerative AI in Banking Financial Services and Insurance: A Guide To Use Cases, Approaches, and Insights  85–121 (Springer, \n2024).\n 229. Quinonez, C. & Meij, E. A new era of AI-assisted journalism at Bloomberg. AI Mag 45 (2024).\nScientific Reports |        (2025) 15:13755 21| https://doi.org/10.1038/s41598-025-98483-1\nwww.nature.com/scientificreports/\n 230. Johnsen, M. Developing AI Applications With Large Language ModelsMaria Johnsen,. (2025).\n 231. Abdali, S., He, J., Barberan, C. J. & Anarfi, R. Can llms be fooled? investigating vulnerabilities in llms. arXiv Prepr. arXiv2407.20529 \n(2024).\n 232. Das, B. C., Amini, M. H. & Wu, Y . Security and privacy challenges of large Language models: A survey. ACM Comput. Surv. 57 \n(2024).\n 233. Nie, Y . et al. A Survey of Large Language Models for Financial Applications: Progress, Prospects and Challenges. arXiv Prepr. \narXiv2406.11903 (2024).\n 234. MindySupport. 9 Cool Case Studies of Global Brands Using LLMs and & Generative, A. I. at (2024).  h t t p s :   /  / h a c k e r n o o  n . c o  m   / 9 - c \no   o l - c  a s  e - s t u d   i e  s -  o f - g l   o b a l - b   r a n d s -   u s i n   g - l l  m s - a n d - g e n  e r a t i v e - a i\n 235. Carneros-Prado, D. et al. Comparative study of large language models as emotion and sentiment analysis systems: A case-specific \nanalysis of GPT vs. IBM Watson. in International Conference on Ubiquitous Computing and Ambient Intelligence 229–239 (2023).\n 236. Chow, J. C. L., Wong, V ., Sanders, L. & Li, K. Developing an AI-assisted educational chatbot for radiotherapy using the IBM \nWatson assistant platform. in Healthcare vol. 11 2417 (2023).\n 237. Dong, X. L., Moon, S., Xu, Y . E., Malik, K. & Yu, Z. Towards next-generation intelligent assistants leveraging llm techniques. in \nProceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining 5792–5793 (2023).\n 238. Martin, A. Artificial Intelligence Transformations in Digital Advertising: Historical Progression, Emerging Trends, and Strategic \nOutlook. (2024).\n 239. Zhao, H. et al. A Comprehensive Survey of Large Language Models in Management: Applications, Challenges, and Opportunities. \nChallenges, Oppor. (August 14, (2024). (2024).\n 240. Agrawal, S., Trenkle, J. & Kawale, J. Beyond Labels: Leveraging Deep Learning and LLMs for Content Metadata. in Proceedings of \nthe 17th ACM Conference on Recommender Systems 1 (2023).\n 241. Jeffri, J. A. C. & Tamizhselvi, A. Enhancing Music Discovery: A Real-Time Recommendation System using Sentiment Analysis \nand Emotional Matching with Spotify Integration. in 8th International Conference on Electronics, Communication and Aerospace \nTechnology (ICECA) 1365–1373 (2024). (2024).\n 242. Pearson, S. Computational advertising for meaningful brands, the public purpose, and a sustainable ecology: A call for research \ninto a systems approach and modeling applications of LLMs in marketing and advertising. J. Curr. Issues \\& Res. Advert . 45, \n357–367 (2024).\n 243. Pesl, R. D., Stötzner, M., Georgievski, I. & Aiello, M. Uncovering LLMs for Service-Composition: Challenges and Opportunities. \nin International Conference on Service-Oriented Computing 39–48 (2023).\n 244. Jiao, J., Afroogh, S., Xu, Y ., Phillips, C. & Navigating, L. L. M. Ethics: Advancements, Challenges, and Future Directions. arXiv \nPrepr. arXiv2406.18841 (2024).\n 245. Wu, F ., Zhang, N., Jha, S., McDaniel, P . & Xiao, C. A new era in llm security: Exploring security concerns in real-world llm-based \nsystems. arXiv Prepr. arXiv2402.18649 (2024).\n 246. Rojas, S. Evaluating the environmental impact of large Language models: sustainable approaches and practices. Innov. Comput. \nSci. J. 10, 1–6 (2024).\n 247. Ivanov, Y . Understanding the inner workings of large Language models: interpretability and explainability. MZ J. Artif. Intell. 1, \n1–5 (2024).\n 248. Chen, C. & Shu, K. Combating misinformation in the age of Llms: opportunities and challenges. AI Mag. 45, 354–368 (2024).\n 249. Bowen, D. et al. Data Poisoning in LLMs: Jailbreak-Tuning and Scaling Laws. arXiv Prepr. arXiv2408.02946 (2024).\n 250. Musser, M. A cost analysis of generative language models and influence operations. arXiv Prepr. arXiv2308.03740 (2023).\n 251. Liu, Y ., Cao, J., Liu, C., Ding, K. & Jin, L. Datasets for large language models: A comprehensive survey. arXiv Prepr. arXiv2402.18041 \n(2024).\n 252. Kausik, B. N. Scaling Efficient LLMs. arXiv Prepr. arXiv2402.14746 (2024).\n 253. Long, D. X. et al. LLMs Are Biased Towards Output Formats! Systematically Evaluating and Mitigating Output Format Bias of \nLLMs. arXiv Prepr. arXiv2408.08656 (2024).\n 254. Hassani, S. Enhancing Legal Compliance and Regulation Analysis with Large Language Models. arXiv Prepr. arXiv2404.17522 \n(2024).\n 255. Zhang, X., Li, S., Hauer, B., Shi, N. & Kondrak, G. Don’t Trust ChatGPT when Y our Question is not in English: A Study of \nMultilingual Abilities and Types of LLMs. arXiv Prepr. arXiv2305.16339 (2023).\n 256. Hamzah, F . & Sulaiman, N. Multimodal integration in large language models: A case study with mistral llm. (2024).\n 257. Perković, G., Drobnjak, A. & Botički, I. Hallucinations in llms: Understanding and addressing challenges. in 2024 47th MIPRO \nICT and Electronics Convention (MIPRO) 2084–2088 (2024).\n 258. Pressman, S. M. et al. AI and ethics: a systematic review of the ethical considerations of large language model use in surgery \nresearch. in Healthcare vol. 12 825 (2024).\n 259. Stamboliev, E. & Christiaens, T. How empty is trustworthy AI? A discourse analysis of the ethics guidelines of trustworthy AI. \nCrit. Policy Stud. 19, 1–18 (2024).\n 260. D\\’\\iaz-Rodr\\’\\iguez, N. et al. Connecting the Dots in trustworthy artificial intelligence: from AI principles, ethics, and key \nrequirements to responsible AI systems and regulation. Inf. Fusion. 99, 101896 (2023).\n 261. Hickman, E., Petrin, M. & Trustworthy AI and corporate governance: the EU’s ethics guidelines for trustworthy artificial \nintelligence from a company law perspective. Eur. Bus. Organ. Law Rev. 22, 593–625 (2021).\nAcknowledgements\nThe Authors are thankful to their respective universities.\nAuthor contributions\nM.R. and Z.J.; Methodology, Resources, Visualization, Validation, writing original draft, writing review and \nediting, Formal analysis, M.A.S. and M.B.R.; Investigation, Supervision, Methodology, Resources, Visualiza -\ntion, Validation, writing original draft, Writing review and editing, Formal analysis, Conceptualization, Funding \nacquisition and administration. M.J.S.; Methodology, Resources, Visualization, Writing – review and editing, \nvalidation.\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nScientific Reports |        (2025) 15:13755 22| https://doi.org/10.1038/s41598-025-98483-1\nwww.nature.com/scientificreports/\nEthical approval\nThis article does not contain any studies with human participants or animals performed by any of the authors.\nAdditional information\nCorrespondence and requests for materials should be addressed to M.B.R.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2025 \nScientific Reports |        (2025) 15:13755 23| https://doi.org/10.1038/s41598-025-98483-1\nwww.nature.com/scientificreports/"
}