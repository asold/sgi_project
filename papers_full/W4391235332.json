{
  "title": "Long-Term Prediction of Sea Surface Temperature by Temporal Embedding Transformer With Attention Distilling and Partial Stacked Connection",
  "url": "https://openalex.org/W4391235332",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5101723979",
      "name": "Hao Dai",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A5100542011",
      "name": "Zhigang He",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A5103988826",
      "name": "Guomei Wei",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A5083640886",
      "name": "Famei Lei",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A5101508702",
      "name": "Xining Zhang",
      "affiliations": [
        "Huaqiao University"
      ]
    },
    {
      "id": "https://openalex.org/A5100366503",
      "name": "Weijie Zhang",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A5110702648",
      "name": "Shaoping Shang",
      "affiliations": [
        "Xiamen University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2599858746",
    "https://openalex.org/W2063603732",
    "https://openalex.org/W2133817912",
    "https://openalex.org/W2462302850",
    "https://openalex.org/W1994870786",
    "https://openalex.org/W2009687803",
    "https://openalex.org/W3015493526",
    "https://openalex.org/W2168650778",
    "https://openalex.org/W2026618042",
    "https://openalex.org/W2795350985",
    "https://openalex.org/W3214062507",
    "https://openalex.org/W4285585675",
    "https://openalex.org/W3190009638",
    "https://openalex.org/W3138791243",
    "https://openalex.org/W2584200484",
    "https://openalex.org/W2800885793",
    "https://openalex.org/W2964858211",
    "https://openalex.org/W3020952709",
    "https://openalex.org/W2967059064",
    "https://openalex.org/W2967877666",
    "https://openalex.org/W2997473273",
    "https://openalex.org/W3030113778",
    "https://openalex.org/W2778580105",
    "https://openalex.org/W2619995677",
    "https://openalex.org/W3184603868",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6760601182",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W6685562342",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W6749825310",
    "https://openalex.org/W3177318507",
    "https://openalex.org/W6763309814",
    "https://openalex.org/W2792764867"
  ],
  "abstract": "Sea surface temperature (SST) is one of the most important parameters in the global ocean&#x2013;atmosphere system, and its long-term changes will have a significant impact on global climate and ecosystems. Accurate prediction of SST, therefore, especially the improvement of long-term predictive skills is of great significance for fishery farming, marine ecological protection, and planning of maritime activities. Since the effective and precise description of the long-range dependence between input and output requires higher model prediction ability, it is an extremely challenging task to achieve accurate long-term prediction of SST. Inspired by the successful application of the transformer and its variants in natural language processing similar to time-series prediction, we introduce it to the SST prediction in the China Sea. The model <bold>Trans</bold>former with temporal embedding, attention <bold>D</bold>is<bold>t</bold>illing, and <bold>St</bold>acked connection in <bold>Part</bold> (TransDtSt-Part) is developed by embedding the temporal information in the classic transformer, combining attention distillation and partial stacked connection, and performing generative decoding. High-resolution satellite-derived data from the National Oceanic and Atmospheric Administration is utilized, and long-term SST predictions with day granularity are achieved under univariate and multivariate patterns. With root mean square error and mean absolute error as metrics, the TransDtSt-Part outperforms all competitive baselines in five oceans (i.e., subareas of Bohai, Yellow Sea, East China Sea, Taiwan Strait, and South China Sea) and six prediction horizons (i.e., 30, 60, 90, 180, 270, and 360 days). Experimental results demonstrate that the performance of the innovative model is encouraging and promising for the long-term prediction of SST.",
  "full_text": "1 \nJ-STARS \n \n \n \n This work is supported by the Fujian Province Marine Economic \nDevelopment Subsidy Fund Project (ZHHY-2019-2). (Corresponding \nauthors: Hao Dai and Shaoping Shang.) \nZhigang He, Guomei Wei, Famei Lei, Weijie Zhang, and Shaoping S hang \nare with the Institute of Ocean Exploration Technology, College of Ocean and \nEarth Sciences, Xiamen University, 361005 Xiamen, China (e-mail s: \nzghe@xmu.edu.cn; weiguomei@163.com; lfm101659@163.com; \noot@xmu.edu.cn; spshang@xmu.edu.cn).  \nXining Zhang is with Fujian Key Laboratory of Light Propagation  a n d  \nTransformation, College of Information Science and Engineering,  H u a q i a o  \nUniversity, 361021 Xiamen, China (e-mail: zhangxn_hqu@163.com).  \nColor versions of one or more of the figures in this article ar e available \nonline at http://ieeexplore.ieee.org. \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3357191\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n1 \nJ-STARS \n \n \nHao Dai*, Zhigang He, Guomei Wei, Famei Lei, Xining Zhang, Weijie Zhang, and Shaoping Shang* \n \nAbstract—Sea surface temperature (SST) is one of the most \nimportant parameters in the global ocean-atmosphere system, \nand its long-term changes will have a significant impact on global \nclimate and ecosystems. Accurate prediction of SST, therefore, \nespecially the improvement of long-term predictive skills is of  \ngreat significance for fishery farming, marine ecological \nprotection, and planning of maritime activities. Since the effective \nand precise description of the long-range dependence between \ninput and output requires higher model prediction ability, it is an \nextremely challenging task to achieve accurate long-term \nprediction of SST. Inspired by the successful application of th e \nTransformer and its variants in natural language processing \nsimilar to time series prediction, we introduce it to the SST \nprediction in the China Sea. The model TransDtSt-Part is \ndeveloped by embedding the temporal information in the classic \nTransformer, combining attention distillation and partial stacked \nconnection, and performing generative decoding. High-resolution  \nsatellite-derived data from the National Oceanic and \nAtmospheric Administration is utilized, and long-term SST \npredictions with day granularity are achieved under univariate \nand multivariate patterns. With Root Mean Square Error and \nMean Absolute Error as metrics, the TransDtSt-Part \noutperforms all competitive baselines in five oceans (i.e., subareas \nof Bohai, Yellow Sea, East China Sea, Taiwan Strait, and South \nChina Sea) and six prediction horizons (i.e., 30, 60, 90, 180, 270, \nand 360 days). Experimental results demonstrate that the \nperformance of the innovative model is encouraging and \npromising for the long-term prediction of SST. \n \nIndex Terms —Attention distilling, China Sea, Long-term \nprediction, Partial stacked connection, Sea surface temperature , \nTemporal Transformer \n1 \nI. INTRODUCTION  \nSea Surface Temperature (SST) provides basic information \nabout the global climate system and is an important parameter \nfor weather prediction and atmospheric model simulations [1], \n[2]. SST measurements benefit a wide range of operational \napplications, including climate monitoring, fishery farming, \nmaritime commercial activities, etc. From north to south, the \nChina Sea is mainly composed of the Bohai Sea, the Yellow \nSea, the East China Sea, the Taiwan Strait, and the South \nChina Sea. The China Sea spans four climatic zones: \n \n \ntemperate zone, warm temperate zone, subtropical zone, and \ntropical zone. The confluence of ocean currents and the \ndevelopment of fronts have created many important fishing \ngrounds, and the marine aquaculture industry is developed. \nMeanwhile, red tide disasters are more serious in spring and \nsummer every year. Moreover, the China Sea is part of the \nMaritime Silk Road and also has one of the busiest shipping \nlanes in the world. Hence, accurate prediction of SST in the \nChina Sea is crucial for an in-depth understanding of marine \nfishery farming, ecological change dynamics, and maritime \nactivity planning, which are very important to the production \nand lives of Chinese people. \nSST is mainly affected by many factors such as solar \nradiation, air-sea heat flux, and diurnal winds, which form a \ncomplex and changeable vertical structure that changes over \ntime. Changes in solar radiation affect the energy balance at \nthe sea surface. Variations in cloud cover, atmospheric \nconditions, and the time of day influence the radiation balance, \nimpacting SST. Positive heat flux, where the ocean gains heat \nfrom the atmosphere, leads to an increase in SST, and negative \nheat flux results in a decrease. Diurnal winds can affect the \nvertical mixing of the ocean layers. During the day, solar \nheating can lead to the development of sea breezes, causing \nmixing and influencing SST. At night, cooling processes may \ndominate. Due to the irregularity of thermal radiation, flux, \nand the uncertainty of wind blowing over the sea surface, it is  \ndifficult to construct effective and reliable mathematical \nequations to describe the causal relationship between SST and \nthese factors, resulting in many difficulties in accurately \npredicting SST [3], [4]. \nCurrently, SST prediction methods are mainly divided into \nthree categories: numerical models based on physics, data-\ndriven techniques, and hybrids of the two. The numerical \nmodels rely on the dynamical and thermal equations based on \nthe required initial and boundary conditions. They describe the \nphysical states using partial differential equations and make \npredictions of future SST after conducting a large number of \ncalculations to derive numerical solu tions [5]\n-[7]. Since no \nclear physical explanation can be given for the mechanism of \nSST generation and evolution, the construction of such models \nis generally inaccurate, relatively complex, and \ncomputationally expensive. They are more suitable for large-\nLong-term Prediction of Sea Surface \nTemperature by Temporal Embedding \nTransformer with Attention Distilling and \nPartial Stacked Connection \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3357191\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n2 \nJ-STARS \n \nscale SST prediction with rough resolution.  Starting from the \ncharacteristics of the data and internal laws, the data-driven \nmethods learn rules from the data and make predictions by \ntraining a large number of known samples. The methods \nmainly include a statistical approach [8], genetic algorithm [9]\n, \nand deep learning [10] -[25]. Under the premise that a large \namount of reliable observation data is available, this type of \nmethod can obtain satisfactory prediction results quickly. Now \nthe hybrid approach combines the numerical model and \nartificial neural network for SST prediction [4]. Due to the use \nof two models, such a method is the most intricate, consuming \nthe most computing resources and taking the longest time. \nThrough literature research, we find that most studies on \nSST prediction using daily average data only focus on short- \nand medium-term prediction (lead time ≤ 10 days ) probably \nbecause it is difficult to find reliable dependencies from the \nlong-term complex time patterns in the future [10]-[12], [14]-\n[16], [18], [19], [21]-[23]. The so-called long-term prediction  \nis weekly mean or monthly mean as the granularity interval \nwhich is too rough [13], [17], [20], [24], [25]. No long-term \nprediction studies with a prediction horizon ≥ 30 days based \non daily average SST have been reported. \nExtending the lead time of SST is a key requirement for \npractical applications such as marine ecosystems and long-\nterm planning of marine activities. Benefiting from the self-\nattention mechanism, Transformer has gained a huge \nadvantage in modeling sequential data dependencies such as \nnatural language processing (NLP, [26]) and audio processing \n[27]. This brings light to its introduction for SST prediction.  \nHowever, the prediction task is extremely challenging in the \nlong-term prediction horizon setting. First, it is unreliable t o \ndiscover temporal dependencies directly from long-term time \nseries because dependencies may be masked by entangled \ntemporal patterns. Second, due to the quadratic complexity of \nthe sequence length, the canonical Transformer with the self-\nattention mechanism is computationally prohibitive and \ndifficult to be directly used for long-term prediction.  \nBased on the traditional Transformer, therefore, this paper \nuses generative decoding, embeds timing information, and \nadds attention distillation/partial stacked connection to \nconstruct the model named TransDtSt-Part. TransDtSt-Part is \nthe abbreviation for the model of Transformer with temporal \nembedding, attention Distilling, and Stacked connection in \nPart. Five typical oceans of the China Sea are selected. With \nthe help of daily average SST, using univariate and \nmultivariate prediction patterns, the long-term prediction skil l \nof TransDtSt-Part is comprehensively evaluated by comparing \nit with multiple baseline models. \nThe remainder of the paper is organized as follows. The \ndetails of the model proposed in this paper are provided in \nSection II. Section III clarifies the data used in this study a nd \nthe research area. Section IV evaluates the proposed model via \nexperiments, followed by a conclusion of this work in Section \nV. \nII.\n METHODS \nA. Classic Transformer \nSimilar to Recurrent Neural Networks (RNN), the classic \nTransformer [28] is designed to process sequential input data \nand is applied to tasks such as translation and text \nsummarization. It features self-attention, which differentially  \nweights the importance of each part of the input (including the  \nrecursive output). Unlike RNNs, Transformers process all \ninputs at once during training. The self-attention mechanism \nprovides context for any position in the input sequence. Hence,  \nto a large extent, Transformer solves the problems of RNN \ntraining inefficiency and long-range dependency insufficiency. \nIt should be noted that during inference, the Transformer \ndecoder still uses the autoregressive method for dynamic \ndecoding. \nThe classic Transformer for NLP first parses the input text \ninto tokens through a byte-pair-encoded tokenizer, and each \ntoken is converted into a vector through word embedding. \nThen, the tagged position information is added to the word \nembedding.  \nFig. 1. Classic Transformer architecture. \nAs shown in Fig. 1, the Transformer model uses an encoder \nand decoder architecture. The encoder consists of encoding \nlayers that iteratively process the input layer by layer, while  \nthe decoder consists of decoding layers that perform the same \noperation on the output of the encoder. \nThe function of each encoder layer is to generate an \nencoding containing information about which parts of the \ninput are related to each other. It passes its encoding as inpu t \nto the next encoder layer. Each decoder layer does the \nopposite, taking all the encodings and using their combined \ncontextual information to generate an output sequence. To \nachieve this, each encoder and decoder layer uses a multi-head \nscaled dot-product attention mechanism. For each part of the \ninput, attention weighs the relevance of all other parts and \nextracts information from them to produce output. Each \ndecoder layer has an additional cross-attention for \nincorporating the output of the encoder. Both the encoder and \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3357191\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n3 \nJ-STARS \n \ndecoder layers have a feed-forward neural network for \nadditional processing of the output and contain residual \nconnection and layer normalization steps. \nB. TransDtSt-Part Model \nLSTM and its variants, which have been used in NLP, have \nalso been employed to predict SST [23], [24]. Inspired by the \ngreat success of Transformer and the similarity to a certain \nextent between SST prediction and sequence tasks such as \nmachine translation, according to the characteristics of SST \ntime series prediction, we make the improvements to construct \nthe model TransDtSt-Part: \n1) employing generative decoding to improve the \nrecursive output form of the traditional Transformer which is \ninefficient and has accumulated errors, to heighten the \nprediction skill. \n2) considering the time-dimensional information that \nhas not been employed in the previous SST prediction works,  \nand adding timestamp embedding. \n3) performing attention distilling and partial stacked \nconnection of the encoder to improve the prediction accuracy \nand enhance the robustness. \nThe paper designs a deep learning network for SST long-\nterm prediction and the structure of the TransDtSt-Part with \nattention distilling and partial stacked connection is exhibite d \nin Fig. 2. \n \nFig. 2. Structure of TransDtSt-Part. \nFrom bottom to top, these three improvements correspond \nto the black dotted boxes marked (1), (2), and (3) in Fig. 2 \nrespectively. These improvements will be detailed in a) - c). \na) Generative decoding \nA s  s h o w n  i n  F i g .  2 ,  t h e  i n p u t  X\nen of the stacked encoder \nbefore embedding is the daily average data of SST for n days \nfrom the historical (t-n)th day to the current tth day (seqlen). The \nstart token strategy is successfully applied to NLP [26], and \nwe extend it into a generative way in the paper. Instead of \nchoosing specific flags as the token, we sample a long \nsequence in the input,\n that is: sampling a sequence of length \n(labellen) in the input sequence of the stacked encoder before \nembedding (namely, the daily average data of SST from the \nday (t-x)\nth to the day tth in Fig. 2, x≤n, i. e., labellen≤seqlen). The \ninitial values of the expected prediction horizons (day (t+1) th-\nday (t+m) th, a total of pred len days) are m zeros and are \nconnected as the input of the decoder before embedding. \nIn Fig. 2, The SST sequence Xtoken is used as the start token \nto lead the initial values of the target sequence into the clas sic \nTransformer decoder. Through a forward process, the multi-\nstep prediction output (namely, the \"outputs\" in Fig. 2) can be  \nobtained. In this way, the sudden drop in the inference speed \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3357191\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n4 \nJ-STARS \n \nof the original \"dynamic decoding\" in the long prediction is \nalleviated, and the accumulation of errors is avoided.  \nAs shown in Fig. 3, for a new sample, the lookback window \nSST with a length of seqlen moves forward one step as a whole, \nand the pred len prediction also moves one step forward \naccordingly. Repeat this to form k samples and one batch. \n \nFig. 3. Sample formation instructions. \nb) Temporal embedding \nIn previous studies on deep learning to predict SST, \ntemporal dimension information has not been exploited. The \nability to achieve remote independence requires global \ninformation, for example, hierarchical timestamps (week, \nmonth, and year). These are rarely exploited in canonical self-\nattention, so a query-key mismatch between the encoder and \ndecoder may lead to a potential drop in prediction \nperformance. \nIn this paper, before the SST enters the stacked encoder and \ndecoder, we perform 3 forms of embedding on the input and \nsuperimpose them. The three forms of embedding are: \n(1) Position embedding: similar to NLP, it is required to \ndeal with longer inputs in long-term SST prediction, so a \nparallel input strategy is adopted. However, considering the \ncontextual relationship between time series SST data, position \nembedding needs to be added. Hence, we follow the \nembedding operation in [28]. Specifically, it is formalized by \n() ()\n() ()\nmodel\nmodel\n2i\ndpos,2i\n2i\ndpos,2i+1\nPE = sin pos / 10000\nPE = cos pos / 10000\n \n \n \n \n ( 1 )  \nwhere pos represents position, i  represents dimension, and \nmodeld  represents embedding vector dimension. \n(2) Timestamp embedding: temporal embedding is \nperformed on the timestamp information corresponding to the \nencoder and decoder inputs respectively to access the global \ncontext information. Embedded coding is performed according \nto the timestamp interval type. This article uses daily average  \nSST data, that is, the timestamp interval is \"day\". Assuming \nthat each day is indexed from 0, it is encoded into week, \nmonth, and year. The encoding formulas are as follows: \n \nencoded\n2 indexdayofweekindexdayofweek = -1 6\n×  ( 2 )   \n encoded\n2 indexdayofmonthindexdayofmonth = -1 30\n×  ( 3 )  \n encoded\n2 indexdayofyearindexdayofyear = -1 365\n×  ( 4 )  \nwhere [ ]indexdayofweek 0,...,6∈ , \n[ ]indexdayofmonth 0,...,30∈ , and \n[ ]indexdayofyear 0,...,365∈ . \nW i t h  t h e  h e l p  o f  ( 2 ) ,  ( 3 ) ,  a n d  ( 4 ) ,  t h e  t i m e s t a m p  \" d a y \"  i s  \nencoded into three vectors and then passes through a linear \nlayer with the input dimension 3 and output dimension d model \nfor timestamp embedding. \n(3) Scalar projection: to align the dimension, one-\ndimensional convolution is performed with kernelsize 3, stride \n1, padding 1, and the circular padding mode, and the encoder \nand decoder inputs are separately projected. \nc)\n Attention distilling and partial stacked connection \nIn the long-term prediction task of SST, more computing \nresources are consumed because of long time series. To \nimprove the prediction ability of the model, given the \nredundant combination of values V in the feature map of the \nentire encoder, we first use the distilling operation to sample , \ngreatly reducing the input size, and prioritize the attention \nscores with dominant features. Then the input is halved \nencoder-by-encoder to enhance the distillation robustness, and \nthe distilling layer is reduced accordingly to align the output  \nof each encoder. Finally, the outputs of all or partial encoder s \nare concatenated to form the final feature map. \nThe whole process of the encoder stacking is shown in Fig. \n4 (take stacking 3 encoders as an example). \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3357191\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n5 \nJ-STARS \n \n \nFig. 4. Encoder stacking process in TransDtSt-Part network. \n \nAccording to the encoder settings of the traditional \nTransformer, Fig. 5(a) exhibits that the Encoder Layer mainly \nincludes scaling dot product multi-head self-attention, residual \nconnection, layer normalization, feedforward, activation, and \ndropout. Where, feedforward mainly includes the one-\ndimensional convolution Conv1d with parameters kernelsize 1, \npadding 0, and stride 1 to realize the effect of a fully \nconnected structure [28], activation, and dropout. \nAs shown in Fig. 5(b), the Distilling Layer mainly consists \nof downsampling (one-dimensional convolution, parameters: \nconvolution kernelsize 3, padding 2, stride 1, padding mode \ncircular), batch normalization, activation (function selected a s \nELU [29], and maximum pooling. In particular, stride=2 in the \nmaximum pooling is employed to achieve halved self-\nattention \"distilling\".\n \n \nFig. 5. Encoder Layer and Distilling Layer in stacked encoder. \n(a) Encoder Layer.  (b) Distilling Layer. \nIt should be pointed out that we investigate the effect of the \nencoder stacking connection length on the SST prediction \nperformance, and discover that probably due to receiving more \nlong-term information, a longer stack is more sensitive to the \ninput, resulting in the prediction effect of connecting all \nencoders is inferior to that of partial connecting encoders (Fo r \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3357191\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n6 \nJ-STARS \n \ndetails, please refer to the prediction indicators of models \nTransDtSt-All and TransDtSt-Part in Supplementary Material \nA. Ablation study.). Therefore, in the model TransDtSt-Part, \nwe take the most robust strategy of joining Encoder I and \nEncoder III of Fig. 4. \nIII.\n DATA AND RESEARCH AREA \nA. Data \nSimilar to many works of literature that employ deep \nlearning techniques to predict SST [19]-[22], the source data \ncome from multi-year daily average data in the Optimum \nInterpolation high-resolution SST dataset Version 2 (OISST) \nprovided by the Physical Sciences Laboratory of National \nOceanic and Atmospheric Administration. The time range of \nthe dataset is from September 1981 to the present and the \nspatial coverage is 89.875°S to 89.875°N, 0.125°E to \n359.875°E with a spatial resolution of 0.25° latitude by 0.25° \nlongitude. Models are trained, validated, and tested on OISST \nas ground truth. \nB. Research area \nTo accurately compare the effects of our method in different \nChina Sea areas, the interest subareas of the five typical \noceans should be equal in size. Since the Taiwan Strait is a \nstrip-shaped region and the research area that can be selected \nis relatively limited, 5x5 pixels is basically the largest squa re \nregion in the OISST dataset. Meanwhile, we also consider the \napplication effect of our model in nearshore and offshore areas. \nThe sub-regions of the Taiwan Strait and Bohai Sea include \nthe coastal areas, while those in the Yellow Sea, East China \nSea, and South China Sea belong to the open sea. As shown in \nthe red boxes of Fig. 6, the longitude and latitude coordinate \nranges of the five sea areas from north to south are Bohai Sea \n(119.625°E-120.625°E, 38.625°N-39.625°N), Yellow Sea \n(122.125°E-123.125°E, 35.125°N-36.125°N), East China Sea \n(124.125°E-125.125°E, 29.125°N-30.125°N), Taiwan Strait \n(118.875°E-119.875°E, 23.625°N-24.625°N), and South \nChina Sea (115.125°E-116.125°E, 19.125°N-20.125°N). \n \nFig. 6. Research area. \nIV. EXPERIMENTS AND RESULTS \nA. Dataset partitioning and preprocessing method \nWe use a total of 40 years of data from 1982-2021 and \ndivide by the volume of 7:1:2, i.e., 1982-2009 as the training \ndataset, 2010-2013 as the validation set for hyper-parameters \ntuning, and 2014 -2021 as a test set to evaluate the \ngeneralization ability of the model in the face of new data. In  \nthis paper, mean-variance normalization is employed to \npreprocess the SST training set and applied to the validation \nand test datasets in the same way. \nB. Baseline models \nFour baseline models are selected for predictive \nperformance comparison, i.e., RNN-based model LSTM [30], \nCNN-based model TCN [31], Transformer-based model \nInformer [32], and interpretable time series prediction model \nN-BEATS [33]. \nC. Hardware platform and software environment \nThe experiments of the proposed model and baseline \nmodels are carried out on the following hardware \nconfigurations: CPU-Intel i9-9900k, RAM-64G, NVIDIA \nGeforce RTX 2080Ti 11G. The software environment adopts \nthe Win10 Operating System, Integrated Development \nEnvironment Pycharm, and deep learning framework PyTorch \n(1.10.1). \n D. Hyperparameters \nThe hyperparameters to be determined in the prediction of \nthe model TransDtSt-Part are: \n1) Number of encoder layers: elayers=[3, 4, 5, 6]; \n2) Number of decoder layers: dlayers=[1, 2]; \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3357191\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n7 \nJ-STARS \n \n3) Generate dimensions of all sub-layers and embedding \nlayers in the model: dmodel=[128, 256, 512]; \n4) Dimension of feedforward network layer: d ff=[512, \n1024] and dff > dmodel; \n5) Training epoch: epoch=[10, 15, 20]; \n6) Training with the early stop strategy, involving \nwaiting for patience: patience=[3, 5]. \nSince the TransDtSt-Part model has a large number of \nhyperparameter combinations, the screening workload is \nrelatively large. Considering the cost of time and computing \nresources, for some lead time in a certain ocean and \nunivariate/multivariate prediction pattern, we first fix the \nparameters epoch=10, patience=3, and use Grid Search to \ndetermine the best parameters of e layers, d layers, d model, and d ff. \nThat is: construct networks through various hyperparameter \ncombinations within the screening range, model fitting on the \ntraining dataset, and choose the hyperparameters \ncorresponding to the best prediction results on the validation \nset as the optimal values. Then fix the selected best \nhyperparameters (e layers, d layers, d model, and d ff), set different \nepoch/patience combinations, compare the prediction \nperformances on the validation dataset, and determine the best \nepoch and patience. (Taking the Bohai Sea as an example, \nSection B in the Supplementary Material shows the optimal \nhyperparameter determination process and results.) \nOther fixed hyperparameters: the activation function is gelu, \nthe initial learning rate lr=0.0001, the number of heads \nnheads=8, batchsize=32, and dropout=0.05. \nE. Loss function and optimizer \nWe choose MSE as the loss function when predicting SST, \nand the loss is passed back to the whole model from the \ndecoder output. The optimizer selects Adam. \nF. Predictive pattern \nSpecifically, the univariate prediction pattern refers to \ntaking the historical data of the geographical center grid poin t \nof the interesting region as input, feeding each model, and \nobtaining the multi-step prediction of the point at one time. \nThe multivariate p r e d i c t i o n  p a t t e r n  r e f e r s  t o  u s i n g  a l l  t h e  \nhistorical data of the 25 grid points of the interesting area a s \ninput, feeding each model, and getting multi-step predictions \nfor all points at once. \nG. Metrics \nAs shown in Fig. 3, the metrics of whole length pred len \npredictions in all samples are calculated to evaluate the \npredictive skill. \nAccording to (5) and (6), the RMSE and MAE between the \nground truth and the prediction are calculated as evaluation \nindicators. A lower RMSE or MAE indicates a better \nprediction. \n \n()\nn 2\nii\ni=1\ny- x\nRMSE = n\n\n ( 5 )  \n  \n \nn\nii\ni=1\ny- x\nMAE = n\n\n ( 6 )  \nWhere ()iy i = 1,..., n is the prediction, ()ix i = 1,..., nis the \nground truth, and n  is the number of samples. \nH. Main results \nTable. I shows univariate SST predictive skill for five sea \nsub-regions and lead times of  \n{ }lenpred 30,60,90,180, 270,and360∈  days. Also, Table. II is \nfor the multivariate case. Since the change cycle of SST is in \nyears, seq len= 3 6 0  d a y s  i s  c h o s e n .  A n d  p r e dlen≤labellen≤seqlen, \nseqlen=labellen+predlen could generally achieve better prediction \nresults (Section C in Supplementary Material will give the \nprediction performance with fixed seqlen and different labellen.). \nHence, when { }lenpred 30,60,90,180∈  days, seqlen is equal to \n360 days, correspondingly { }lenlabel 330,300, 270,180∈  days. \nAnd when { }lenpred 270,360∈  days, seqlen=labellen=360 days. \nAdditionally, we examine the relationship between encoder \ninput length and model performance. So Table. I and Table. II \nalso include the prediction errors of each model when \n{ }lenseq 360,540,720∈  days, and labellen= predlen=360 days. \nIn the prediction effect comparison of the baseline models, \nwe call the models LSTM, N-BEATS, and TCN of the Library \nDarts and use the Library Optuna for hyperparameter \nscreening. Note that N-BEATS achieves multivariate \nprediction by flattening the model input into a 1-D series and \nr e s h a p i n g  t h e  o u t p u t  i n t o  a  t e n s o r  o f  a p p r o p r i a t e  s i z e ,  w h i l e  \nTCN requires seq\nlen to be greater than predlen during prediction, \nso the symbol \"×\" is employed to fill in the prediction blanks \nof seq\nlen= predlen=360 days in Table. I-II. \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3357191\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n8 \nJ-STARS \n \nTABLE I \nSST UNIVARIATE PREDICTIVE SKILL OF THE MODELS WITH DIFFERENT PREDICTION HORIZONS IN THE FIVE SEA AREAS \n a means seqlen=540 days, labellen=360 days, and predlen=360 days while b means seqlen =720 days, labellen =360 days, and predlen =360 days. \nBold highlights the optimal prediction results for each lead time in every sea area. \nThe \"Count\" row represents the count of the optimal value of each model under all lead times in all sea areas.  \nModels \nMetrics \nTransDtSt-Part LSTM N-BEATS TCN Informer \nRMSE \n(℃) \nMAE \n(℃) \nRMSE \n(℃) \nMAE \n(℃) \nRMSE \n(℃) \nMAE\n(℃) \nRMSE \n(℃) \nMAE \n(℃) \nRMSE \n(℃) \nMAE\n(℃) \nBohai, China \n30 0.957 0.747 3.144 2.513 1.006 0.780 4.385 3.892 1.037 0.810\n60 1.052 0.820 5.386 4.218 1.087 0.857 7.799 6.897 1.155 0.917\n90 1.170 0.917 7.632 5.903 1.179 0.937 11.463 10.294 1.181 0.940\n180 1.160 0.912 12.088 9.939 1.202 0.961 16.166 14.551 1.233 0.989\n270 1.123 0.884 13.373 11.315 1.379 1.104 11.484 10.329 1.195 0.950\n360 1.142 0.900 11.748 9.606 1.288 1.023 × × 1.195 0.942\n360a 1.100 0.854 8.492 7.522 1.243 0.985 10.153 7.586 1.194 0.941\n360b 1.082 0.849 11.785 9.658 1.372 1.092 1.476 1.178 1.184 0.934\nYellow Sea, China \n30 0.876 0.658 2.522 2.028 0.880 0.663 3.384 2.977 0.950 0.714\n60 0.981 0.742 4.179 3.279 1.000 0.771 5.919 5.375 1.059 0.805\n90 1.000 0.751 5.776 4.533 1.022 0.785 8.805 7.862 1.105 0.846\n180 1.055 0.802 8.996 7.381 1.154 0.896 11.846 10.617 1.152 0.882\n270 1.070 0.828 9.521 7.958 1.161 0.905 8.658 7.711 1.173 0.919\n360 1.068 0.816 9.297 7.584 1.204 0.936 × × 1.152 0.888\n360a 1.036 0.787 6.619 5.787 1.144 0.891 9.126 8.362 1.102 0.852\n360b 1.004 0.760 9.221 7.430 1.157 0.886 1.292 0.989 1.114 0.855\nEast China Sea \n30 0.793 0.602 2.094 1.644 0.798 0.613 3.076 2.704 0.821 0.628\n60 0.868 0.665 3.496 2.708 0.880 0.687 5.229 4.692 0.885 0.684\n90 0.893 0.692 4.690 3.596 0.902 0.703 7.061 6.303 0.916 0.713\n180 0.937 0.738 7.519 6.139 1.016 0.801 9.249 8.340 0.976 0.759\n270 0.930 0.722 8.146 6.889 0.946 0.744 7.097 6.387 0.957 0.749\n360 0.935 0.732 7.457 6.078 0.958 0.750 × × 0.975 0.765\n360a 0.943 0.739 5.119 4.499 0.963 0.751 4.072 3.587 0.959 0.748\n360b 0.920 0.718 7.458 6.102 0.960 0.753 1.096 0.853 0.926 0.719\nTaiwan Strait \n30 1.088 0.833 1.773 1.364 1.094 0.842 2.199 1.790 1.109 0.857\n60 1.109 0.864 2.600 2.056 1.132 0.879 3.611 3.099 1.133 0.881\n90 1.138 0.882 3.285 2.558 1.146 0.884 4.597 4.061 1.152 0.889\n180 1.122 0.864 4.950 3.968 1.217 0.951 6.293 5.454 1.156 0.901\n270 1.160 0.904 5.046 4.196 1.175 0.909 4.799 4.228 1.190 0.921\n360 1.135 0.881 4.716 3.772 1.203 0.934 × × 1.159 0.897\n360a 1.154 0.896 3.422 2.914 1.237 0.955 2.257 1.666 1.181 0.903\n360b 1.161 0.906 4.888 3.931 1.212 0.939 1.302 0.997 1.200 0.922\nSouth China Sea \n30 0.754 0.599 1.394 1.101 0.773 0.610 1.543 1.235 0.764 0.611\n60 0.805 0.647 1.886 1.465 0.835 0.666 2.467 2.019 0.831 0.671\n90 0.817 0.655 2.501 1.915 0.836 0.663 3.430 2.919 0.873 0.706\n180 0.862 0.696 3.376 2.751 0.907 0.719 4.408 3.985 0.911 0.741\n270 0.850 0.686 3.883 3.220 0.916 0.732 2.860 2.419 0.893 0.715\n360 0.856 0.692 3.486 2.802 0.944 0.761 × × 0.898 0.723\n360a 0.884 0.713 2.459 2.161 0.940 0.754 1.112 0.881 0.923 0.741\n360b 0.895 0.708 3.293 2.659 0.951 0.761 1.001 0.786 0.920 0.728\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3357191\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n9 \nJ-STARS \n \nTABLE II \nSST MULTIVARIATE PREDICTIVE SKILL OF THE MODELS WITH DIFFERENT PREDICTION HORIZONS IN THE FIVE SEA AREAS \n   \nModels \nMetrics \nTransDtSt-Part LSTM N-BEATS TCN Informer \nRMSE \n(℃) \nMAE \n(℃) \nRMSE \n(℃) \nMAE\n(℃) \nRMSE \n(℃) \nMAE\n(℃) \nRMSE \n(℃) \nMAE \n(℃) \nRMSE \n(℃) \nMAE\n(℃) \nBohai, China \n30 1.029 0.800 3.019 2.377 1.059 0.824 4.371 3.875 1.131 0.879\n60 1.136 0.899 5.079 3.929 1.187 0.928 8.054 7.120 1.212 0.955\n90 1.224 0.968 7.056 5.532 1.263 0.997 10.459 9.136 1.273 1.005\n180 1.244 0.999 10.512 8.572 1.279 1.004 13.720 12.019 1.321 1.053\n270 1.247 0.979 11.587 9.642 1.262 0.990 9.494 8.299 1.248 0.983\n360 1.162 0.910 10.951 8.945 1.345 1.069 × × 1.258 0.997\n360a 1.184 0.932 8.398 7.440 1.419 1.138 10.320 8.885 1.240 0.974\n360b 1.165 0.921 10.265 8.574 1.357 1.069 1.463 1.163 1.198 0.941\nYellow Sea, China \n30 0.889 0.672 2.443 1.914 0.929 0.712 3.699 3.290 1.039 0.799\n60 1.018 0.785 4.240 3.221 1.027 0.794 6.644 5.966 1.079 0.832\n90 1.031 0.772 5.921 4.321 1.060 0.816 8.633 7.640 1.130 0.866\n180 1.187 0.947 8.728 6.951 1.269 0.969 11.622 10.182 1.252 0.994\n270 1.112 0.866 8.480 6.745 1.201 0.925 9.028 8.055 1.137 0.885\n360 1.026 0.883 8.619 6.967 1.263 0.989 × × 1.148 0.890\n360a 1.063 0.826 6.661 5.861 1.279 0.995 9.721 8.681 1.159 0.907\n360b 1.049 0.807 8.844 7.245 1.273 0.972 1.296 0.997 1.171 0.919\nEast China Sea \n30 0.847 0.644 2.253 1.765 0.848 0.655 2.811 2.425 0.889 0.688\n60 0.903 0.691 3.553 2.653 0.953 0.738 5.017 4.479 0.918 0.716\n90 0.923 0.713 5.156 3.953 0.952 0.739 6.839 6.095 0.961 0.751\n180 0.946 0.741 6.702 5.571 0.971 0.758 9.741 8.692 0.992 0.772\n270 0.944 0.738 7.535 6.324 0.959 0.754 6.866 6.178 1.000 0.780\n360 0.972 0.758 7.061 5.750 1.000 0.783 × × 0.980 0.771\n360a 0.975 0.753 5.075 4.468 0.997 0.784 8.267 7.303 0.994 0.779\n360b 0.963 0.746 6.598 5.423 1.087 0.862 1.139 0.891 0.975 0.760\nTaiwan Strait \n30 1.112 0.850 2.048 1.579 1.201 0.919 2.452 2.031 1.122 0.863\n60 1.168 0.901 2.587 2.126 1.193 0.919 3.767 3.210 1.176 0.915\n90 1.191 0.912 2.971 2.065 1.202 0.930 5.290 4.572 1.192 0.919\n180 1.166 0.895 4.869 3.786 1.169 0.901 6.975 6.073 1.283 0.987\n270 1.175 0.906 4.625 3.691 1.183 0.911 4.867 4.217 1.237 0.946\n360 1.162 0.900 5.272 4.205 1.203 0.925 × × 1.186 0.910\n360a 1.248 0.940 3.674 3.121 1.253 0.974 6.695 5.869 1.240 0.948\n360b 1.190 0.915 4.671 3.764 1.296 1.004 1.386 1.043 1.283 0.988\nSouth China Sea \n30 0.759 0.597 1.367 1.110 0.767 0.599 1.599 1.305 0.832 0.669\n60 0.809 0.651 1.913 1.446 0.826 0.658 2.465 2.049 0.840 0.676\n90 0.829 0.669 2.367 1.803 0.850 0.676 3.044 2.571 0.875 0.702\n180 0.861 0.691 3.323 2.717 0.877 0.695 3.510 3.060 0.915 0.735\n270 0.867 0.698 3.643 3.037 0.897 0.717 3.168 2.706 0.887 0.714\n360 0.877 0.702 3.012 2.447 0.927 0.738 × × 0.894 0.720\n360a 0.847 0.680 2.435 2.152 0.955 0.764 2.781 2.473 0.918 0.738\n360b 0.891 0.705 3.443 2.777 0.941 0.750 0.968 0.763 0.930 0.741\nCount 40 40 0 0 0 0 0 0 0 0 \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3357191\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n10 \nJ-STARS \n \nAnalysis of SST univariate prediction results \nIt can be found in Table. I that: \n1) From the perspective of the lead times of 30-360 days \nin each ocean, the prediction error of the model TransDtSt-\nPart, which uses generative decoding to predict multiple \nvalues in one step, has generally resisted the extended lead \ntime, showing the characteristics of a steady and slow rise \nwith the prediction horizon increasing. \n2)\n For all oceans and all lead times, the prediction error \nof TransDtSt-Part is smaller than other baseline models from \nthe statistical values of the optimal prediction results (i.e.,  the \nC o u n t  r o w ) .  I t  i s  v e r i f i e d  t h a t  t h e  l o n g - t e r m  S S T  p r e d i c t i v e  \nskill of TransDtSt-Part is satisfactory with the day as the fin e-\ngrained interval. When compared with N-BEATS which has \ncloser predictive performance, according to the prediction \nimprovement rate (RMSE and MAE ≤5%, 5%-10%, and \n≥10%) of TransDtSt-Part relative to N-BEATS, we count the \ncounts of all lead times for all research subareas (a total of 40 \ncounting points), as shown in Fig. 7(a). \n \nFig. 7.  Univariate, all oceans, number of prediction \nimprovement rate intervals of TransDtSt-Part relative to N-\nBEATS. (a) All lead times. (b) Lead times ≥ 180 days. \n \nA s  c a n  b e  s e e n  i n  F i g .  7 ( a ) ,  a b o u t  h a l f  o f  t h e  p r e d i c t o r s  \nimprove below 5%. However, if we consider the statistical \nresults of the prediction horizons ≥ 180 days (that is, Fig. 7(b), \na total of 25 count points), we find that the counts whose \nmetrics improve below 5% are greatly decreased, while the \nnumber of improvements more than 5% does not change. This \nphenomenon reveals that compared with N-BEATS, a \nrelatively large improvement occurs at a longer lead time. In \nother words, TransDtSt-Part improves more for a longer \nprediction horizon. \n3)\n The prediction errors in the open seas of the Yellow \nSea, the East China Sea, and the South China Sea are \nrelatively small (RMSE~0.75°C -1°C, MAE~0.6°C -0.8°C), \nwhile those in the Bohai Sea and the Taiwan Strait are \nrelatively large (RMSE~0.95°C -1.2°C, MAE~0.75°C-0.9°C). \nThe prediction effect of the open sea is better than that of \nother oceans, which may be attributed to the larger SST \nfluctuations in the coastal area, while the SST in the open \nocean area is relatively stable [17], [21], [22], [24]. \nDivided by sea area, the number of prediction improvement \nrates of TransDtSt-Part relative to N-BEATS is calculated for \nall lead times, as shown in Fig. 8. It can be found that the \nnumbers of the three improvement rate intervals in the Yellow \nSea and the Bohai Sea are roughly the same, while the \nimprovement rates in the East China Sea, South China Sea, \nand Taiwan Strait are concentrated below 10%. \n \nFig. 8. Univariate , every ocean, all lead times, number of \nprediction improvement rate intervals of TransDtSt-Part \nrelative to N-BEATS. (a) Yellow Sea. (b) East China Sea. (c) \nSouth China Sea.  (d) Bohai Sea. (e) Taiwan Strait. \n \n4) The prediction accuracy of the model TransDtSt-Part \nis significantly higher than that of the models LSTM for each \nocean and each lead time. This may be due to the fact that \nLSTM adopts autoregressive decoding, so the model \nprediction error accumulates as the lead time increases. Since \nthe period of the SST signal is about 360 days, the TCN \nprediction error is smaller than other lead times when seq\nlen is \nan integer multiple of 360 days (i.e., seq len=720 days). The \npredictive skill of TransDtSt-Part that considers full attentio n \nis better than that of Informer only with part of the attention  \ncoefficients. \nAnalysis of SST multivariate prediction results \nSimilar to the analysis of univariate prediction results, the \nfollowing conclusions on SST multivariate prediction results \n(Table. II) are drawn： \n1) The model TransDtSt-Part also has a trend of a \nsteady and slow rise in performance as the lead time becomes \nlonger. \n2) In all research areas, the prediction error of \nTransDtSt-Part is smaller than that of other baseline models at  \nall prediction horizons. \n \nFig. 9.  Multivariate, all oceans, number of prediction \nimprovement rate intervals of TransDtSt-Part relative to N-\nBEATS. (a) All lead times. (b) Lead times ≥ 180 days. \nFrom Fig. 9, compared with N-BEATS, TransDtSt-Part \nimproves more for longer lead times. \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3357191\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n11 \nJ-STARS \n \n3) The prediction errors in the open seas of the Yellow \nSea, the East China Sea, and the South China Sea are \nrelatively small (RMSE~0.75°C-1.20°C, MAE~0.60°C-\n0.95°C), while those in the Bohai Sea and the Taiwan Strait \nare relatively large (RMSE~1°C-1.25°C, MAE~0.80°C-1.0°C). \n \nFig. 10. Multivariate, every ocean, all lead times, number of \nprediction improvement rate intervals of TransDtSt-Part \nrelative to N-BEATS. (a) Yellow Sea. (b) East China Sea. (c) \nSouth China Sea. (d) Bohai Sea. (e) Taiwan Strait. \n \nFig. 10 exhibits the statistical value of the number of \nprediction improvement rate intervals of TransDtSt-Part \nrelative to N-BEATS for all prediction horizons and research \nareas. The numbers of the three improvement rate intervals in \nthe Yellow Sea are roughly equal, the improvement rates in \nthe Bohai Sea are distributed at both ends of ≤5% and ≥10%, \nand the improvement rates in the East China Sea, South China \nSea, and Taiwan Strait are concentrated below 10%. \nRelationship between encoder input length and model \nperformance \nFrom Table. I-II, the prediction results of TransDtSt-Part \npresent area-specific for { }lenseq 360 540 720∈ ，，  days, \nlabellen=360 days, and pred len=360 days. Specifically, for the \nunivariate case, the prediction errors of the Bohai Sea and the  \nYellow Sea decrease with the increase of seq len, while the \nprediction errors of the East China Sea, the Taiwan Strait, and  \nthe South China Sea increase. For the multivariate case, the \nprediction error of the East China Sea decreases with the seqlen \nincreasing. The prediction errors of the Bohai Sea, the Yellow \nSea, and the Taiwan Strait increase first and then decrease. \nBut the South China Sea is the opposite, first decreasing and \nthen increasing. \nUnivariate prediction showcase \nFig. 11 shows the prediction slices of the model TransDtSt-\nPart and the baseline models in the Bohai Sea with seqlen =720 \ndays, labellen=360 days, and pred len=360 days. Fig. 12 shows \nthe correlation between each model’s prediction and Ground \ntruth. \n \nFig. 11. Under the univariate pattern, the predictions (predlen=360 days with seqlen=720 days) of (a) TransDtSt-Part,  (b) LSTM, \n(c) N-BEATS, (d) TCN, and (e) Informer on the Bohai Sea. The or ange/blue curves stand for slices of the prediction/ground \ntruth. \n \nFig. 12. Under the univariate pattern, the correlations between the predictions (pred len=360 days with seq len=720 days) of (a) \nTransDtSt-Part,  (b) LSTM, (c) N-BEATS, (d) TCN, (e) Informer a nd the ground truth on the Bohai Sea. The dark red dash line \nstands for the 1:1 line. \n  \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3357191\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n12 \nJ-STARS \n \nIt can be seen from Fig. 11(b) that due to the autoregressive \ndecoding, the LSTM prediction error continues to accumulate, \nand the prediction curve seriously deviates from the ground \ntruth. From Fig. 11(c), (d), and Fig. 12(c), (d), N-BEATS and \nTCN have larger prediction errors at some troughs (2°C-5°C) \nand peaks (22°C-25°C). Although In Fig. 11(a), (e), and Fig. \n12(a), (e), Informer and TransDtSt-Part have relatively poor \nperformance in the low-value part of SST, they will be more \nin line with the ground truth in the high-value part of SST. \nCompared with Informer, the overall predictive skill of \nTransDtSt-Part considering all attention coefficients is better. \nMultivariate prediction showcase \nTaking the South China Sea as an example, when the predlen \nis 270 days, Fig. 13 exhibits the slices of the last dimension of \nthe prediction results and the corresponding Ground truth. Fig. \n14 shows the correlation between the prediction and Ground \ntruth at this time. \n \nFig. 13. Based on the multivariate pattern, the predictions (pred len=270 days with seq len=360 days) of (a) TransDtSt-Part, (b) \nLSTM, (c) N-BEATS, (d) TCN, and (e) Informer on the South China  Sea. The orange/blue curves stand for slices of the \nprediction/ground truth.  \n \nFig. 14. Based on the multivariate pattern, the correlations between the predictions (pred len=270 days with seq len=360 days) of \n(a) TransDtSt-Part,  (b) LSTM, (c) N-BEATS, (d) TCN, (e) Inform er and the ground truth on the South China Sea. The dark red \ndash line stands for the 1:1 line. \n \nThe prediction performance of LSTM in Fig. 13(b) and Fig. \n14(b) is even worse, the model cannot make normal \npredictions and finally presents a straight line. TCN has also \nbeen unable to capture the individual long-range dependencies \nbetween outputs and inputs for SST long-sequence (Fig. 13(d) \nand Fig. 14(d)). From Fig. 13(c) and Fig. 14(c), N-BEATS \nexhibits overestimation at the lower SST (22°C-24°C) and \nunderestimation at the higher SST (28°C-30.5°C) under this \ncase. In Fig. 13(a) and Fig. 13(e), Informer and TransDtSt-Part \ncan still accurately grasp the long-term change trend of SST. \nOn the whole, Informer's prediction is more fluctuating, and \nthe prediction curve of TransDtSt-Part is smoother. Both \nInformer and TransDtSt-Part have large prediction errors in \nthe range of 24℃-28℃ (Fig. 14(a) and Fig. 14(e)). \nSeasonal prediction error analysis \nFor all SST prediction slice data in the Univariate prediction \nshowcase (i.e., Bohai Sea with seq len=720 days, label len=360 \ndays, and pred len=360 days) and Multivariate prediction \nshowcase (i.e., South China Sea with seq len=360 days, \nlabellen=360 days, and predlen=270 days), according to the four \nseasons of Winter (Jan.-Mar.), Spring (Apr.-Jun.), Summer \n(Jul.-Sep.), and Autumn (Oct.-Dec.), the average predictive \nskills of TransDtSt-Part and the baseline models are calculated \nrespectively, as shown in Table. III and Table. IV. \n \n \n \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3357191\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n13 \nJ-STARS \n \nTABLE III \nSEASONAL PREDICTION ERROR - UNIVARIATE PREDICTION SHOWCASE OF BOHAI SEA \n \nTABLE IV \nSEASONAL PREDICTION ERROR - MULTIVARIATE PREDICTION SHOWCASE OF SOUTH CHINA SEA \nIt is easy to find from Table. III and Table. IV that except \nfor the Spring in the univariate prediction of Bohai Sea, the \nprediction performance of the TransDtSt-Part model is slightly \ninferior to that of the Informer and its predictive skills are the \nbest in other seasons. It proves the excellent seasonal SST \nprediction ability of the TransDtSt-Part model. \nV. CONCLUSION \nWe focus on the long-term prediction of SST in the China \nSea at a fine-grained daily level in the paper. Transformer has  \npowerful time series modeling capabilities, but it also with \nsome disadvantages such as high computational complexity, \nlow autoregressive decoding efficiency, and easy \naccumulation of errors. We make targeted improvements to \nbuild the model TransDtSt-Part: using generative decoding, \nembedding time-dimensional information, and introducing \nattention distilling and partial stacked connection. Among the \nextensive experiments of two prediction patterns and multiple \nlead times in the five China Sea regions, the prediction \nperformance of the model TransDtSt-Part outperforms all \ncompetitive baseline models to varying degrees, proving its \nexcellent long-term predictive skill of SST. It may be helpful \nfor many urgent long-term requirements in marine and climate \napplications. \nA\nCKNOWLEDGMENT \nSpecial thanks for the support from High-Resolution SST \ndata provided by the NOAA/OAR/ESRL PSL, Boulder, \nColorado, USA. All Daily mean SST data used in this study \nwere obtained from NOAA/OAR/ESRL PSL at \nhttps://psl.noaa.gov/data/gridded/data.noaa.oisst.v2.highres.ht\nml. \nWe would also like to thank the third-party Python library \nDarts and Optuna. \nR\nEFERENCES \n[1] M. Bouali,  O. T. Sato, and P. S. Polito, “Temporal trends in sea surface \ntemperature gradients in t he South Atlantic Ocean,” Remote Sens. \nEnviron., vol. 194, pp. 100-114, Jun. 2017, doi: \n10.1016/j.rse.2017.03.008. \n[2] T. D. Herbert, L. C. Peterson, K. T. Lawrence, and Z. H. Liu,  \n“Tropical ocean temperatures o ver the past 3.5 million years,” Science, \nvol. 328 no. 5985, pp. 1530-1534, Jun. 2010, doi: \n10.1126/science.1185435. \n[3] K. Patil, M. C. Deo, S. Ghosh, an d M. Ravichandran, “Predicting  s ea \nsurface temperatures in the North Indian Ocean with nonlinear \nautoregressive neural networks,” Int. J. Oceanog. , vol. 2013, Apr.  \n2013, Art. no. 302479, doi: 10.1155/2013/302479. \n[4] K. Patil, M. C. Deo, and M. Ravic handran, “Prediction of sea su rface \ntemperature by combining numerical and neural techniques,” J. Atmos. \nOcean. Technol. , vol. 33, no. 8, pp. 1715-1726, Aug. 2016, doi: \n10.1175/JTECH-D-15-0213.1. \n[5] T. N. Krishnamurti, A. Chakraborty ,  R .  K r i s h n a m u r t i ,  W .  K .  D e w ar, \nand C. A. Clayson, “Seasonal Prediction of Sea Surface Temperat ure \nAnomalies Using a Suite of 13 Coupled Atmosphere-Ocean Models,” J. \nClimate, vol: 19, pp. 6069-6088, Dec. 2006, doi:  10.1175/JCLI3938.1. \n[6] T. N. Stockdale, M. A. Balmaseda,  and A. Vidard, “Tropical Atla ntic \nSST prediction with coupled ocean–atmosphere GCMs,” J. Climate , \nvol. 19, no. 23, pp. 6047-6061, Dec. 2006,  doi: 10.1175/JCLI3947.1. \n[7] Y .  F .  W ang,  Z .  H. Z hang,  a nd P .  Hua ng,  “ A n im pr ove d m odel- ba s ed \nanalogue forecasting for the prediction of the tropical Indo-Pa cific sea \nsurface temperature in a coupled climate model,”  Int. J. Climatol. , vol. \n40, no. 15, pp. 6346-6360, Dec. 2020, doi: 10.1002/joc.6584. \n[8] J. S. Kug, I. S. Kang, J. Y. Lee, and J. G. Jhun, “A statistica l approach \nto Indian Ocean sea surface temp erature prediction using a dyna mical \nENSO prediction,” Geophys. Res. Lett. , vol. 31, no. 9, May. 2004, Art. \nno. L09212, doi: 10.1029/2003GL019209. \n[9] Neetu, R. Sharma, S. Basu, A. Sa rkar, and P. K. Pal, “Data-adap tive \nprediction of sea-surface temperature in the Arabian Sea,” IEEE Geosci. \nRemote Sens. Lett. , vol. 8, no. 1, pp. 9-13, Jan. 2011, doi: \n10.1109/LGRS.2010.2050674. \n[10] S. G. Aparna, G. D 'Souza, and N. B. Arjun, “Prediction of daily sea \nsurface temperature using artificial neural networks,” Int. J. Remote \nSens., vol. 39, no. 12, pp.  4214-4231, Apr. 2018, doi: \n10.1080/01431161.2018.1454623. \n[11] S. Y. Hou, W. G. Li, T. Y. Liu, S. G. Zhou, J. H. Guan, R. F. Q in, and \nZ. F. Wang,  “D2CL: A dense dilated convolutional LSTM model fo r \nsea surface temperature prediction,” IEEE J. Sel. Top. Appl. Earth \nModel \nRMSE (℃) MAE (℃) \nWinter Spring Summer Autumn Winter Spring Summer Autumn \nTransDtSt-Part 0.857  0.823 0.816 0.865 0.711 0.673 0.658 0.705 \nLSTM 4.319  3.257 3.261 3.206 3.857 2.762 2.678 2.706 \nN-BEATS 0.902  0.872 0.826 0.897 0.735 0.721 0.666 0.722 \nTCN 3.195  2.845 3.454 3.048 2.735 2.431 3.109 2.556 \nInformer 0.901  0.804 0.843 0.925 0.743 0.654 0.676 0.747 \nModel \nRMSE (℃) MAE (℃) \nWinter Spring Summer Winter Spring Summer \nTransDtSt-Part 1.069  1.045 1.072 1.084 0.842 0.833 \nLSTM 11.938  10.977 12.439 11.009 9.791 9.413 \nN-BEATS 1.357  1.344 1.342 1.380 1.091 1.073 \nTCN 1.482  1.471 1.452 1.446 1.206 1.198 \nInformer 1.220  1.139 1.155 1.155 0.974 0.914 \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3357191\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n14 \nJ-STARS \n \nObserv. Remote Sens. , vol. 14, pp. 12514-12523, 2021, doi: \n10.1109/JSTARS.2021.3128577. \n[12] S. Y. Hou, W. G. Li, T. Y. Liu, S. G. Zhou, J. H. Guan, R. F. Q in, and \nZ. F. Wang, “MUST: A multi-source spatio-temporal data fusion m odel \nfor short-term sea surface temperature prediction,” Ocean Eng. , vol. \n259, Sep. 2022, Art. no. 111932. doi: 10.1016/j.oceaneng.2022.111932. \n[13] M. Jahanbakht, W. Xiang, and M. R. Azghadi, “Sea surface \ntemperature forecasting with ensemble of stacked deep neural \nnetworks,” IEEE Geosci. Remote Sens. Lett. , vol. 19, 2022, Art. no. \n1502605,  doi: 10.1109/LGRS.2021.3098425. \n[14] J. J. Liu, B. G. Jin, J. K. Yang, and L. Y. Xu, “Sea surface te mperature \nprediction using a cubic B-spline interpolation and spatiotempo ral \nattention mechanism,” Remote Sens. Lett. , vol. 12, no. 5, pp.478-487, \nMay. 2021, doi: 10.1080/2150704X.2021.1897182. \n[15] K. Patil, and M. C. Deo, “Prediction of daily sea surface tempe rature \nusing efficient neural networks,” Ocean Dyn., vol. 67, pp. 357-368, Apr. \n2017, doi: 10.1007/s10236-017-1032-9. \n[16] K. Patil and M. C. Deo, “Basin- Scale prediction of sea surface \ntemperature with Artificial Neural Networks,” J. Atmos. Ocean. \nTechnol., vol. 35, no. 7, pp. 1441-1455, Jul. 2018, doi: \n10.1175/JTECH-D-17-0217.1. \n[17] L. Wei, L. Guan, and L. Q. Qu, “Prediction of sea surface tempe rature \nin the South China Sea by Artificial Neural Networks,” IEEE Geosci. \nRemote Sens. Lett. , vol. 17, no. 4, pp. 558-562, Apr. 2020, doi: \n10.1109/LGRS.2019.2926992. \n[18] S. Wolff, F. O'Donnch, and B. Chen, “Statistical and machine le arning \nensemble modelling to forecast sea surface temperature,” J. Mar. Syst., \nvol. 208, Aug. 2020, Art. no. 103347, doi: \n10.1016/j.jmarsys.2020.103347. \n[19] C. J. Xiao et al., “A spatiotemporal deep learning model \nfor sea surface temperature field prediction using time-series satellite \ndata,” Environ. Modell. Softw. , vol. 120, Oct. 2019, Art. no. 104502, \ndoi: 10.1016/j.envsoft.2019.104502. \n[20] J. Xie, J. Y. Zhang, J. Yu, and L. Y. Xu, “An adaptive scale se a \nsurface temperature predicting method based on Deep Learning with att\nention mechanism,” IEEE Geosci. Remote Sens. Lett., vol. 17, no. 5, pp. \n740-744, May 2020, doi: 10.1109/LGRS.2019.2931728. \n[21] L. Y. Xu, Q. Li, J. Yu, L. Wang, J. Xie, and S. X. Shi, “Spatio -temporal \npredictions of SST time series in China’s offshore waters using  a  \nregional convolution Long Short-Term Memory (RC-LSTM) network,”  \nInt. J. Remote Sens. , vol. 41, no. 9, pp. 3368-3389, May. 2020, doi: \n10.1080/01431161.2019.1701724. \n[22] L. Y. Xu, Y. F. Li, J. Yu, Q. Li, and S. X. Shi, “Prediction of  sea \nsurface temperature using a multiscale deep combination neural \nnetwork,” Remote Sens. Lett. , vol. 11, no. 7, pp. 611-619, July, 2020, \ndoi: 10.1080/2150704X.2020.1746853. \n[23] Y. T. Yang, J. Y. Dong, X. Sun, E. Lima, Q. Mu, and X. H. Wang,  “A \nCFCC-LSTM model for sea surface temperature prediction,” IEEE \nGeosci. Remote Sens. Lett. , vol. 15, no. 2, pp. 207-211, Feb. 2018, doi: \n10.1109/LGRS.2017.2780843. \n[24] Q. Zhang, H. Wang, J. Y. Dong, G. Q. Zhong, and X. Sun, “Predic tion \nof sea surface temperature using Long Short-term Memory,” IEEE \nGeosci. Remote Sens. Lett. , vol. 14, no. 10, pp. 1745-1749, Oct. 2017, \ndoi: 10.1109/LGRS.2017.2733548. \n[25] X .  Y .  Z h a n g ,  Y .  Q .  L i ,  A .  C .  F r e r y ,   a n d  P .  R e n ,  “ S e a  s u r f a c e  \ntemperature prediction with Memory Graph Convolutional Networks ,” \nIEEE Geosci. Remote Sens. Lett. , vol. 19, Jul. 2022, Art. no. 8017105, \ndoi: 10.1109/LGRS.2021.3097329. \n[26] J. Devlin, M. W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-tr aining \nof deep bidirectional transformers for language understanding,”  in Proc \nNAACL, Minneapolis, Minnesota, USA, 2019, pp. 4171-4186, doi: \n10.48550/arXiv.1810.04805. \n[27] C. A. Huang, et al., “Music trans former: Generating music with long-\nterm structure,” in Proc ICLR, New Orleans, LA, USA, 2019, doi: \n10.48550/arXiv.1809.04281. \n[28] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N . \nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need ,” in \nProc NIPS , Long Beach, CA, USA, 2017, pp. 5998-6008, doi: \n10.48550/arXiv.1706.03762. \n[29] D. Clevert, T. Unterthiner, and S. Hochreiter, “Fast and Accura te Deep \nNetwork Learning by Exponential Linear Units (ELUs),” in Proc ICLR, \nSan Juan, Puerto Rico, 2016, doi:  \n10.48550/arXiv. 1511.07289. \n[30] S. Hochreiter and J. Schmidhuber, “Long short-term memory,”  Neural \nComput., vol. 9, no. 8, pp. 1735-1780, 1997, doi: \n10.1162/neco.1997.9.8.1735. \n[31] S. J. Bai, J. Z. Kolter, and V. K oltun,  “An empirical evaluati on of \ngeneric convolutional and recurrent networks for sequence model ing”, \narXiv preprint arXiv:1803.01271, 2018, doi: \n10.48550/arXiv.1803.01271. \n[32] H. Y. Zhou, S. H. Zhang, J. Q. Peng, S. Zhang, J. X. Li, H. Xio ng, and \nW. C. Zhang, “Informer: Beyond efficient transformer for long \nsequence time-series forecasting,” in Proc AAAI, Virtual Event, 2021, \npp. 11106-11115, doi: 10.48550/arXiv.2012.07436. \n[33]  B. N. Oreshkin, D. Carpov,  N. Chapados, and Y. Bengio, “N-BEA TS: \nNeural basis expansion analysis for interpretable time series \nforecasting,” in Proc ICLR\n, Addis Ababa, Ethiopia, 2020, doi: \n10.48550/arXiv.1905.10437. \n \nHao Dai  was born in 1982. He received his Ph.D. degree from Zhejiang \nUniversity in 2012. He is currently a Senior Engineer with the Institute of \nOcean Exploration Technology, College of Ocean and Earth Scienc es, \nXiamen University, China. His research interest is Artificial I ntelligence \nOceanography and ocean metrology. \n \nZhigang He  was born in 1973. He received the Ph.D. degree in ocean \nchemistry from Xiamen University, Xiamen, China in 2005. He is currently an \nAssociate Professor with Xiamen University. His research intere sts include \nocean observation technology, circulation in the South China Se a and its \nadjacent region. \n \nGuomei Wei  was born in 1985. She received the M.S. degree in \nenvironmental sciences from Xiamen University, Xiamen, China in  2010. Her \nresearch interest is quality control technology for radar data. \n \nFamei Lei  was born in 1984. He received the M.S. degree in \nenvironmental sciences from Xiamen University, Xiamen, China in  2012. His \nresearch interest includes data processing of ocean observation instruments. \n \nXining Zhang  was born in 1982. She received her Ph.D. degree from \nZhejiang University in 2012. She is now an Associate Professor at the College \nof Information Science and Engineering, Huaqiao University. Her  r e s e a r c h  \ninterests include surface plasmo n properties in metal micro/nan o structures, \nand optical devices based on micro/nanofibers. \n \nWeijie Zhang was born in 1986. She received the M.S. degree in \nenvironmental sciences from Xiamen University, Xiamen, China in  2015. Her \nresearch interest includes visualization in ocean science. \n \nShaoping Shang w a s  b o r n  i n  1 9 6 2 .  H e  i s  c u r r e n t l y  a  P r o f e s s o r  a t  t h e  \nCollege of Ocean and Earth Sciences, Xiamen University, Xiamen, China. His \nresearch interests are ocean observation, marine information te chnology, \nnumerical modeling and remote sensing of marine environment. \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3357191\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Sea surface temperature",
  "concepts": [
    {
      "name": "Sea surface temperature",
      "score": 0.7882493734359741
    },
    {
      "name": "Univariate",
      "score": 0.5834828615188599
    },
    {
      "name": "Computer science",
      "score": 0.5758249163627625
    },
    {
      "name": "Climatology",
      "score": 0.5200902223587036
    },
    {
      "name": "Transformer",
      "score": 0.47007304430007935
    },
    {
      "name": "Environmental science",
      "score": 0.44639524817466736
    },
    {
      "name": "Atmospheric model",
      "score": 0.4294032156467438
    },
    {
      "name": "Embedding",
      "score": 0.4199336767196655
    },
    {
      "name": "Meteorology",
      "score": 0.35498490929603577
    },
    {
      "name": "Multivariate statistics",
      "score": 0.3426175117492676
    },
    {
      "name": "Machine learning",
      "score": 0.2707963287830353
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2594199776649475
    },
    {
      "name": "Geology",
      "score": 0.18628481030464172
    },
    {
      "name": "Geography",
      "score": 0.10001668334007263
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I191208505",
      "name": "Xiamen University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I119045251",
      "name": "Huaqiao University",
      "country": "CN"
    }
  ],
  "cited_by": 21
}