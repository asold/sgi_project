{
  "title": "Transformers: \"The End of History\" for NLP?",
  "url": "https://openalex.org/W3159588055",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4286987281",
      "name": "Chernyavskiy, Anton",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2641682746",
      "name": "Ilvovsky Dmitry",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223231446",
      "name": "Nakov, Preslav",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3125056032",
    "https://openalex.org/W3011279327",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2978871345",
    "https://openalex.org/W2970487286",
    "https://openalex.org/W2996851481",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W3113763975",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W2953369973",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W3104613728",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2973071945",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W3083091152",
    "https://openalex.org/W2098162425",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2796108585",
    "https://openalex.org/W2962903510",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2004763266",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2946817437",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2986266667",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2976444281",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W2948947170"
  ],
  "abstract": "Recent advances in neural architectures, such as the Transformer, coupled with the emergence of large-scale pre-trained models such as BERT, have revolutionized the field of Natural Language Processing (NLP), pushing the state of the art for a number of NLP tasks. A rich family of variations of these models has been proposed, such as RoBERTa, ALBERT, and XLNet, but fundamentally, they all remain limited in their ability to model certain kinds of information, and they cannot cope with certain information sources, which was easy for pre-existing models. Thus, here we aim to shed light on some important theoretical limitations of pre-trained BERT-style models that are inherent in the general Transformer architecture. First, we demonstrate in practice on two general types of tasks -- segmentation and segment labeling -- and on four datasets that these limitations are indeed harmful and that addressing them, even in some very simple and naive ways, can yield sizable improvements over vanilla RoBERTa and XLNet models. Then, we offer a more general discussion on desiderata for future additions to the Transformer architecture that would increase its expressiveness, which we hope could help in the design of the next generation of deep NLP architectures.",
  "full_text": "Transformers: “The End of History”\nfor Natural Language Processing?\nAnton Chernyavskiy1\u0000 , Dmitry Ilvovsky1, and Preslav Nakov2\n1 HSE University, Russian Federation\naschernyavskiy 1@edu.hse.ru, dilvovsky@hse.ru\n2 Qatar Computing Research Institute, HBKU, Doha, Qatar\npnakov@hbku.edu.qa\nAbstract. Recent advances in neural architectures, such as the Trans-\nformer, coupled with the emergence of large-scale pre-trained models\nsuch as BERT, have revolutionized the ﬁeld of Natural Language Pro-\ncessing (NLP), pushing the state of the art for a number of NLP tasks.\nA rich family of variations of these models has been proposed, such as\nRoBERTa, ALBERT, and XLNet, but fundamentally, they all remain\nlimited in their ability to model certain kinds of information, and they\ncannot cope with certain information sources, which was easy for pre-\nexisting models. Thus, here we aim to shed light on some important theo-\nretical limitations of pre-trained BERT-style models that are inherent in\nthe general Transformer architecture. First, we demonstrate in practice\non two general types of tasks—segmentation and segment labeling—and\non four datasets that these limitations are indeed harmful and that ad-\ndressing them, even in some very simple and na¨ ıve ways, can yield sizable\nimprovements over vanilla RoBERTa and XLNet models. Then, we of-\nfer a more general discussion on desiderata for future additions to the\nTransformer architecture that would increase its expressiveness, which\nwe hope could help in the design of the next generation of deep NLP\narchitectures.\nKeywords: Transformers · Limitations · Segmentation · Sequence Clas-\nsiﬁcation.\n1 Introduction\nThe history of Natural Language Processing (NLP) has seen several stages: ﬁrst,\nrule-based, e.g., think of the expert systems of the 80s, then came the statistical\nrevolution, and now along came the neural revolution. The latter was enabled\nby a combination of deep neural architectures, specialized hardware, and the\nexistence of large volumes of data. Yet, the revolution was going slower in NLP\ncompared to other ﬁelds such as Computer Vision, which were quickly and deeply\ntransformed by the emergence of large-scale pre-trained models, which were in\nturn enabled by the emergence of large datasets such as ImageNet.\narXiv:2105.00813v2  [cs.CL]  23 Sep 2021\n2 A. Chernyavskiy et al.\nThings changed in 2018, when NLP ﬁnally got its “ImageNet moment” with\nthe invention of BERT [9]. 1 This was enabled by recent advances in neural\narchitectures, such as the Transformer [31], followed by the emergence of large-\nscale pre-trained models such as BERT, which eventually revolutionized NLP\nand pushed the state of the art for a number of NLP tasks. A rich family of\nvariations of these models have been proposed, such as RoBERTa [20], ALBERT\n[18], and XLNet [34]. For some researchers, it felt like this might very well be\nthe “End of History” for NLP (` a la Fukuyama2).\nIt was not too long before researchers started realizing that BERT and Trans-\nformer architectures in general, despite their phenomenal success, remained fun-\ndamentally limited in their ability to model certain kinds of information, which\nwas natural and simple for the old-fashioned feature-based models. Although\nBERT does encode some syntax, semantic, and linguistic features, it may not\nuse them in downstream tasks [16]. It ignores negation [11], and it might need to\nbe combined with Conditional Random Fields (CRF) to improve its performance\nfor some tasks and languages, most notably for sequence classiﬁcation tasks [28].\nThere is a range of sequence tagging tasks where entities have diﬀerent lengths\n(not 1-3 words as in the classical named entity recognition formulation), and\nsometimes their continuity is required, e.g., for tagging in court papers. More-\nover, in some problem formulations, it is important to accurately process the\nboundaries of the spans (in particular, the punctuation symbols), which turns\nout to be something that Transformers are not particularly good at (as we will\ndiscuss below).\nIn many sequence classiﬁcation tasks, some classes are described by speciﬁc\nfeatures. Besides, a very large contextual window may be required for the cor-\nrect classiﬁcation, which is a problem for Transformers because of the quadratic\ncomplexity of calculating their attention weights. 3\nIs it possible to guarantee that BERT-style models will carefully analyze all\nthese cases? This is what we aim to explore below. Our contributions can be\nsummarized as follows:\n– We explore some theoretical limitations of pre-trained BERT-style models\nwhen applied to sequence segmentation and labeling tasks. We argue that\nthese limitations are not limitations of a speciﬁc model, but stem from the\ngeneral Transformer architecture.\n– We demonstrate in practice on two diﬀerent tasks (one on segmentation, and\none on segment labeling) and on four datasets that it is possible to improve\nover state-of-the-art models such as BERT, RoBERTa, XLNet, and this can\nbe achieved with simple and na¨ ıve approaches, such as feature engineering\nand post-processing.\n1 A notable previous promising attempt was ELMo [21], but it became largely outdated\nin less than a year.\n2 http://en.wikipedia.org/wiki/The_End_of_History_and_the_Last_Man\n3 Some solutions were proposed such as Longformer [3], Performer [4], Linformer [33],\nLinear Transformer [15], and Big Bird [35].\nTransformers: “The End of History” for Natural Language Processing? 3\n– Finally, we propose desiderata for attributes to add to the Transformer archi-\ntecture in order to increase its expressiveness, which could guide the design\nof the next generation of deep NLP architectures.\nThe rest of our paper is structured as follows. Section 2 summarizes related\nprior research. Section 3 describes the tasks we address. Section 4 presents the\nmodels and the modiﬁcations thereof. Section 5 outlines the experimental setup.\nSection 6 describes the experiments and the evaluation results. Section 7 provides\nkey points that lead to further general potential improvements of Transformers.\nSection 8 concludes and points to possible directions for future work.\n2 Related Work\nStudies of what BERT learns and what it can represent There is a large number\nof papers that study what kind of information can be learned with BERT-style\nmodels and how attention layers capture this information; a survey is presented\nin [26]. It was shown that BERT learns syntactic features [12,19], semantic roles\nand entities types [30], linguistic information and subject-verb agreement [13].\nNote that the papers that explore what BERT-style models can encode do not\nindicate that they directly use such knowledge [16]. Instead, we focus on what\nis not modeled, and we explore some general limitations.\nLimitations of BERT/Transformer Indeed, Kovaleva et al. (2019) [16] revealed\nthat vertical self-attention patterns generally come from pre-training tasks rather\nthan from task-speciﬁc linguistic reasoning and the model is over-parametrized.\nEttinger (2020) [11] demonstrated that BERT encodes some semantics, but is\nfully insensitive to negation. Sun et al. (2020) [29] showed that BERT-style\nmodels are erroneous in simple cases, e.g., they do not correctly process word se-\nquences with misspellings. They also have bad representations of ﬂoating point\nnumbers for the same tokenization reason [32]. Moreover, it is easy to attack\nthem with adversarial examples [14]. Durrani et al. (2019) [10] showed that\nBERT subtoken-based representations are better for modeling syntax, while\nELMo character-based representations are preferable for modeling morphology.\nIt also should be noticed that hyper-parameter tuning is a very non-trivial task,\nnot only for NLP engineers but also for advanced NLP researchers [23]. Most\nof these limitations are low-level and technical, or are related to a speciﬁc ar-\nchitecture (such as BERT). In contrast, we single out the general limitations of\nthe Transformer at a higher level, but which can be technically conﬁrmed, and\nprovide desiderata for their elimination.\nFixes of BERT/Transformer Many improvements of the original BERT model\nhave been proposed: RoBERTa (changed the language model masking, the learn-\ning rate, the dataset size), DistilBERT [27] (distillation to signiﬁcantly reduce the\nnumber of parameters), ALBERT (cross-layer parameter sharing, factorized em-\nbedding parametrization), Transformer-XL [8] (recurrence mechanism and rel-\native positional encoding to improve sequence modeling), XLNet (permutation\n4 A. Chernyavskiy et al.\nlanguage modeling to better model bidirectional relations), BERT-CRF [1, 28]\n(dependencies between the posteriors for structure prediction helped in some\ntasks and languages), KnowBERT [22] (incorporates external knowledge). Most\nof these models pay attention only to 1–2 concrete ﬁxes, whereas our paper aims\nat more general Transformer limitations.\n3 Tasks\nIn this section, we describe two tasks and four datasets that we used for experi-\nments.\n3.1 Propaganda Detection\nWe choose the task of Detecting Propaganda Techniques in News Articles (SemEval-\n2020 Task 11)4 as the main for experiments. Generally, it is formulated as ﬁnding\nand classifying all propagandistic fragments in the text [6]. To do this, two sub-\ntasks are proposed: ( i) span identiﬁcation (SI), i.e., selection of all propaganda\nspans within the article, ( ii) technique classiﬁcation (TC), i.e., multi-label clas-\nsiﬁcation of each span into 14 classes. The corpus with a detailed description of\npropaganda techniques is presented in [7].\nThe motivation for choosing this task is triggered by several factors. First,\ntwo technically diﬀerent problems are considered, which can be formulated at a\ngeneral level (multi-label sequence classiﬁcation and binary token labeling). Sec-\nond, this task has speciﬁcity necessary for our research, unlike standard named\nentity recognition. Thus, traditional NLP methods can be applied over the set\nof hand-crafted features: sentiment, readability scores, length, etc. Here, length\nis a strong feature due to the data statistics [7]. Moreover, spans can be nested,\nwhile span borders vary widely and may include punctuation symbols. Moreover,\nsometimes Transformer-based models face the problem of limited input sequence\nlength. In this task, such a problem appears with the classiﬁcation of “Repeti-\ntion” spans. By deﬁnition, this class includes spans that have an intentional\nrepetition of the same information. This information can be repeated both in\nthe same sentence and in very distant parts of the text.\n3.2 Keyphrase Extraction\nIn order to demonstrate the transferability of the studied limitations between\ndatasets, we further experimented with the task of Extracting Keyphrases and\nRelations from Scientiﬁc Publications, using the dataset from SemEval-2017\nTask 10 [2]. We focus on the following two subtasks: (i) keyphrase identiﬁcation\n(KI), i.e., search of all keyphrases within the text, ( ii) keyphrase classiﬁcation\n(KC), i.e., multi-class classiﬁcation of given keyphrases into three classes. Ac-\ncording to the data statistics, the length of the phrases is a strong feature. Also,\nphrases can be nested inside one other, and many of them are repeated across\ndiﬀerent articles. So, these subtasks allow us to demonstrate a number of issues.\n4 The oﬃcial task webpage: http://propaganda.qcri.org/semeval2020-task11/\nTransformers: “The End of History” for Natural Language Processing? 5\n4 Method\nInitially, we selected the most successful approach as the baseline from Transformer-\nbased models (BERT, RoBERTa, ALBERT, and XLNet). In both propaganda\ndetection subtasks, it turned out to be RoBERTa, which is an optimized ver-\nsion of the standard BERT with a modiﬁed pre-training procedure. Whereas\nin both keyphrase extraction subtasks, it turned out to be XLNet, which is a\nlanguage model that aims to better study bidirectional links or relationships in\na sequence of words. From a theoretical point of view, investigated results and\nresearched problems should be typical for other Transformer-based models, such\nas BERT and DistilBERT. Nonetheless, we additionally conduct experiments\nwith both XLNet and RoBERTa for both tasks for a better demonstration of\nthe universality of our ﬁndings.\n4.1 Token Classiﬁcation\nWe reformulate the SI and the KI tasks as “named entity recognition” tasks.\nSpeciﬁcally, in the SI task, for each span, all of its internal tokens are assigned\nto the “ PROP” class and the rest to “ O” (Outside). Thus, this is a binary token\nclassiﬁcation task. At the same time, various types of encoding formats are\nstudied. Except for the above described Inside-Outside classiﬁcation, we further\nconsider BIO (Begin) and BIEOS (Begin, End, Single are added) tags encodings.\nSuch markups theoretically can provide better processing for border tokens [25].\nIn order to ensure the sustainability of the trained models, we create an\nensemble of three models trained with the same hyper-parameters, but using\ndiﬀerent random seeds. We merge the intersecting spans during the ensemble\nprocedure (intervals union).\nEnd-to-End Training with CRFs Conditional Random Fields (CRF) [17] can\nqualitatively track the dependencies between the tags in the markup. Therefore,\nthis approach has gained great popularity in solving the problem of extracting\nnamed entities with LSTMs or RNNs. Advanced Transformer-based models gen-\nerally can model relationships between words at a good level due to the attention\nmechanism, but adding a CRF layer theoretically is not unnecessary. The idea\nis that we need to model the relationships not only between the input tokens\nbut also between the output labels.\nOur preliminary study showed that both RoBERTa and XLNet are make\nclassiﬁcation errors even when choosing tags in named entity recognition (NER)\nencodings with clear rules. For example, in the case of BIO, the “ I-PROP” tag\ncan only go after the “ B-PROP” tag. However, RoBERTa produced results with\na sequence of tags such as “ O-PROP I-PROP O-PROP” for some inputs. Here, it is\nhard to determine where the error was, but the CRF handles such cases from a\nprobabilistic point of view. We use the CRF layer instead of the standard model\nclassiﬁcation head to apply the end-to-end training. Here, we model connections\nonly between neighboring subtokens since our main goal is the proper sequence\nanalysis. Thus, the subtokens that are not placed at the beginning of words are\nignored (i.e., of the format ##smth).\n6 A. Chernyavskiy et al.\nRoBERTa, XLNet, and Punctuation Symbols In the SI task, there is one more\nproblem that even the CRF layer cannot always handle. It is the processing of\npunctuation and quotation marks at the span borders. Clark et al. (2019) [5] and\nKovaleva et al. (2019) [16] showed that BERT generally has high token–token\nattention to the [SEP] token, to the periods, and to the commas, as they are\nthe most frequent tokens. However, we found out that large attention weights to\npunctuation may still not be enough for some tasks.\nIn fact, a simple rule can be formulated to address this problem: a span cannot\nbegin or end with a punctuation symbol, unless it is enclosed in quotation marks.\nWith this observation in mind, we apply post-processing of the spans borders\nby adding missing quotation marks and also by ﬁltering punctuation symbols in\ncase they were absent.\n4.2 Sequence Classiﬁcation\nWe model the TC task in the same way as the KC task, that is, as a multi-class\nsequence classiﬁcation problem. We create a fairly strong baseline to achieve\nbetter results. First, the context is used, since spans in both tasks can have\nvarious meanings in diﬀerent contexts. Thus, we select the entire sentence that\ncontains the span for this purpose. In this case, we consider two possible options:\n(i) highlight the span with the special limiting tokens and submit to the model\nonly one input; ( ii) make two inputs: one for the span and one for the context.\nMoreover, to provide a better initialization of the model and to share some\nadditional knowledge from other data in the TC task, we apply the transfer\nlearning strategy from the SI task.\nJust like in the token classiﬁcation problem, we compose an ensemble of mod-\nels for the same architecture, but with three diﬀerent random seed initializations.\nWe do this in order to stabilize the model, and this is not a typical ensemble of\ndiﬀerent models.\nInput Length BERT does not have a mechanism to perform explicit charac-\nter/word/subword counting. Exactly this problem and the lack of good consis-\ntency between the predicted tags may cause a problem with punctuation (quo-\ntation marks) in the sequence tagging task, since BERT theoretically cannot\naccurately account for the number of opening/closing quotation marks (as it\ncannot count).\nIn order to explicitly take into account the input sequence size in the model,\nwe add a length feature to the [CLS] token embedding, as it should contain all\nthe necessary information to solve the task (see Figure 1). It may be also useful\nto pre-process the length feature through binning. In this case, it is possible to\nadditionally create trainable embeddings associated with each bin or directly\nto add an external knowledge from a gazetteer containing relevant information\nabout the dataset according to the given bin (we will consider gazetteers below).\nIn addition to the input length in characters (or in tokens), it may be useful\nto add other quantitative features such as the number of question or exclamation\nsymbols.\nTransformers: “The End of History” for Natural Language Processing? 7\nspan context CLS SEP \nemb. averaged emb.\n+ \nspan length\nClassiﬁer \nPrediction\nRoBER T a \nFig. 1. The RoBERTa model takes an input span and the context (sentence with the\nspan). It combines the embedding of the [CLS] token, the averaged embedding of all\nspan tokens, and the span length as a feature.\nSpan Embeddings In the TC task, we concatenate the[CLS] token representation\nwith the span embedding obtained by averaging all token embeddings from the\nlast layer to submit to the classiﬁer (end-to-end training). Note that the added\nembedding contains information about the degree of propaganda in the classiﬁed\nspan as an initialization, since we transfer a model from another task. Moreover,\nthis model can reconﬁgure it to serve other features during the training process.\nAlso, it may be useful to join embeddings obtained by the max-pool operation\nor taken from other layers.\nTraining a Hand-Crafted Gazetteer Gazetteers can provide external relevant\ninformation about entities in NER tasks. As some propaganda techniques are\noften described by the same words, it might be a good idea to construct and\nto use a gazetteer of words for each technique. While in NER, gazetteers are\nexternally constructed to provide additional knowledge, here we use the training\ndata to construct our gazetteer. We create a hash map, where the keys are spans\npre-processed by the Porter stemmer [24], and the values are distributions of the\nclasses in which spans are present in the training dataset.\nThere are several ways to use this gazetteer. First, we can use these frequency\nrepresentations as additional features and concatenate them with the [CLS]\ntoken in the same way as described for the length and the span embedding.\nHowever, in this case, over-ﬁtting may occur since such a feature will contain\na correct label. The second method is based on post-processing. The idea is to\nincrease the probability of each class of spans by some value (e.g., +0.5) if the\nspan of this class is present in the gazetteer.\n8 A. Chernyavskiy et al.\nClass Insertions Earlier, we described the problem of non-perfect spatially con-\nsistent class predictions for the token labeling task. For the sequence classiﬁca-\ntion task, it may be expressed as the incorrect nesting of classes. That is, the\nmodel can produce a markup in which the span of class A is nested in the span\nof another class B, but there are no such cases in the training data. If we believe\nthat the training set gives us an almost complete description of the researched\nproblem, such a classiﬁcation obviously cannot be correct.\nThe simplest solution is again post-processing. One possibility is to choose\na pair of spans that have maximal predicted probability and the correct nest-\ning. Another option is to choose a pair of classes with a maximal probability\np(x)p(y)p(A). Here, p(x) is the predicted probability that the span has the label\nx, and p(A) is the estimated probability of the nesting case A, where a span\nof class x is inside the span of class y. To estimate p(A), we calculate the co-\noccurrence matrix of nesting classes in the training set, and we apply softmax\nwith temperature t over this matrix to obtain probabilities. The temperature\nparameter is adjusted for each model on validation. We use the ﬁrst approach in\nthe TC task. As there are only three classes and all class insertions are possible,\nwe apply the second approach with t = 0.26 in the KC task.\nSpeciﬁc Classes: “Repetition” In some cases, the entire text of the input docu-\nment might be needed as a context (rather than just the current sentence) in\norder for the model to be able to predict the correct propaganda technique for\na given span. This is the case of the repetition technique.\nAs a solution, we apply a special post-processing step. Let k be the number\nof occurrences of the considered span in the set of spans allocated for prediction\nwithin the article and p be the probability of the repetition class predicted by\nthe source model. We apply the following formula:\nˆp =\n\n\n\n1, if k ≥3 or ( k = 2 and p ≥t1)\n0, if k = 1 and p ≤ t2\np, otherwise\n(1)\nWe use the following values for the probability thresholds: t1 = 0.001 and\nt2 = 0.99. Note that since the repetition may be contained in the span itself, it\nis incorrect to nullify the probabilities of the unique spans.\nMulti-label Classiﬁcation If the same span can have multiple labels, it is nec-\nessary to apply supplementary post-processing of the predictions. Thus, if the\nsame span is asked several times during the testing process (the span is deter-\nmined by its coordinates in the text, and in the TC task, multiple labels are\nsignalled by repeating the same span multiple times in the test set), then we\nassign diﬀerent labels to the diﬀerent instances of that span, namely the top\namong the most likely predictions.\nTransformers: “The End of History” for Natural Language Processing? 9\n5 Experimental Setup\nBelow, we describe the data we used and the parameter settings for our experi-\nments for all the tasks.\n5.1 Data\nPropaganda Detection The dataset provided for the SemEval-2020 task 11 con-\ntains 371 English articles for training, 75 for development, and 90 for testing.\nTogether, the training and the testing sets contain 6,129 annotated spans. While\nthere was an original partitioning of the data into training, development, and\ntesting, the latter was only available via the task leaderboard, and was not re-\nleased. Thus, we additionally randomly split the training data using a 80:20 ratio\nto obtain new training and validation sets. The evaluation measure for the SI\ntask is the variant of the F 1 measure described in [7]: it penalizes for predicting\ntoo long or too short spans (compared to the gold span) and generally correlates\nwith the standard F 1 score for tokens. Micro-averaged F 1 score is used for the\nTC task, which is equivalent to accuracy.\nKeyphrase Extraction The dataset provided for SemEval-2017 task 10 contains\n350 English documents for training, 50 for development, and 100 for testing. In\ntotal, the training and the testing sets contain 9,945 annotated keyphrases. The\nevaluation measure for both sub-tasks is micro-averaged F 1 score.\n5.2 Parameter Setting\nWe started with pre-trained model checkpoints and baselines as from the Hug-\ngingFace Transformers library,5 and we implemented our modiﬁcations on top\nof them. We used RoBERTa-large and XLNet-large, as they performed better\nthan their base versions in our preliminary experiments.\nWe selected hyper-parameters according to the recommendations in the orig-\ninal papers using our validation set and we made about 10–20 runs to ﬁnd the\nbest conﬁguration. We used grid-search over {5e-6, 1e-5, 2e-5, 3e-5, 5e-5}for the\noptimal learning rate. Thus, we ﬁx the following in the propaganda detection\nproblem: learning rate of 2e-5 (3e-5 for XLNet in the TC task), batch size of 24,\nmaximum sequence length of 128 (128 is ﬁxed as it is long enough to encode the\nspan; besides, there are very few long sentences in our datasets), Adam optimizer\nwith a linear warm-up of 500 steps. The sequence length and the batch size are\nselected as the maximum possible for our GPU machine (3 GeForce GTX 1080\nGPUs). We performed training for 30 epochs with savings every two epochs and\nwe selected the best checkpoints on the validation set (typically, it was 10–20\nepochs). We found that uncased models should be used for the SI task, whereas\nthe cased model were better for the TC task.\n5 http://github.com/huggingface/transformers\n10 A. Chernyavskiy et al.\nTask Approach F1\nSI\nRoBERTa (BIO encoding) 46 .91\n+ CRF 48 .54↑1.63\n+ punctuation post-processing 47 .54↑0.63\nOverall 48.87↑1.96\nXLNet (BIO encoding) 46 .47\n+ CRF 46 .68↑0.21\n+ punctuation post-processing 46 .76↑0.29\nOverall 47.05↑0.58\nKI\nRoBERTa (BIO encoding) 57 .85\n+ CRF 58 .59↑0.74\nXLNet (BIO encoding) 58 .80\n+ CRF 60 .11↑1.31\nTable 1. Analysis of RoBERTa and XLNet modiﬁcations for sequential classiﬁcation\ntasks: span identiﬁcation and keyphrase identiﬁcation. Overall is the simultaneous ap-\nplication of two improvements.\nFor keyphrase extraction, for the KI task, we used a learning rate of 2e-5\n(and 3e-5 for RoBERTa-CRF), a batch size of 12, a maximum sequence length\nof 64, Adam optimizer with a linear warm-up of 60 steps. For the KC task, we\nused a learning rate of 2e-5 (1e-5 for XLNet-Length) and a head learning rate of\n1e-4 (in cases with the Length feature), batch size of 20 (10 for XLNet-Length),\nmaximum sequence length of 128, and the Adam optimizer with a linear warm-\nup of 200 steps. We performed training for 10 epochs, saving each epoch and\nselecting the best one on the validation set.\nThe training stage in a distributed setting takes approximately 2.38 minutes\nper epoch (+0.05 for the avg. embedding modiﬁcation) for the TC task. For the\nSI task, it takes 6.55 minutes per epoch for RoBERTa (+1.27 for CRF), and\n6.75 minutes per epoch for XLNet (+1.08 for CRF).\n6 Experiments and Results\n6.1 Token Classiﬁcation\nWe experimented with BIOES, BIO, and IO encodings, and we found that BIO\nperformed best, both when using CRF and without it. Thus, we used the BIO\nencoding in our experiments. We further observed a much better recall with\nminor loss in precision for our ensemble with span merging.\nA comparison of the described approaches for the SI and the KI tasks is pre-\nsented in Table 1. Although the sequential predictions of the models are generally\nconsistent, adding a CRF layer on top improves the results. Manual analysis of\nthe output for the SI task has revealed that about 3.5% of the predicted tags\nwere illegal sequences, e.g., an “ I-PROP” tag following an “ O” tag.\nTransformers: “The End of History” for Natural Language Processing? 11\nTechnique Classiﬁcation\nApproach RoBERTa XLNet\nBaseline 62 .75 58 .23\n+ length 63 .50↑0.75 59.64↑1.41\n+ averaged span embededding 62.94↑0.19 59.64↑1.41\n+ multi-label 63 .78↑1.03 59.27↑1.04\n+ gazetteer post-processing 62 .84↑0.10 58.33↑0.10\n+ repetition post-processing 66 .79↑4.04 62.46↑3.67\n+ class insertions 62 .65↓0.10 57.85↓0.38\nTable 2. Analysis of the improvements using RoBERTa and XLNet for the TC task\non the development set. Shown is micro-F 1 score.\nKeyphrase Classiﬁcation\nApproach RoBERTa XLNet\nBaseline 77 .18 78 .50\n+ length 77 .38↑0.20 78.65↑0.15\n+ gazetteer post-processing 77 .43↑0.25 78.69↑0.19\n+ class insertions 77 .82↑0.64 78.69↑0.19\nTable 3. Analysis of improvements for the KC task using RoBERTa and XLNet on\nthe development set in the multi-label mode. Shown is micro-F 1 score.\nAlso, we ﬁgured out that neither XLNet nor RoBERTa could learn the de-\nscribed rule for quotes and punctuation symbols. Moreover, adding CRF also\ndoes not help solve the problem according to the better “overall” score in the\ntable. We analyzed the source of these errors. Indeed, there were some anno-\ntation errors. However, the vast majority of the errors related to punctuation\nat the boundaries were actually model errors. E.g., in an example like <\"It is\nwhat it is.\">, where the entire text (including the quotation marks) had to be\ndetected, the model would propose sequences like <\"It is what it is> or <It\nis what it is.>. Thus, there is a common problem for all Transformer-based\nmodels—lack of consistency for sequential tag predictions.\n6.2 Sequence Classiﬁcation\nWe took models that use separate inputs (span and context) for all experiments,\nas they yielded better results on the validation set. The results for the customized\nmodels are shown in Tables 2 and 3 for the Technique Classiﬁcation (TC) and the\nKeyphrase Classiﬁcation (KC) tasks, respectively. We also studied the impact\nof the natural multi-label formulation of the TC task (see Table 4). We can see\nthat all directions of quality changes were the same.\n12 A. Chernyavskiy et al.\nTechnique Classiﬁcation\nApproach RoBERTa XLNet\nBaseline+multi-label 63 .78 59 .27\n+ length 64 .72↑0.94 60.68↑1.41\n+ averaged span embededding 64.25↑0.47 60.77↑1.50\n+ gazetteer post-processing 63 .87↑0.09 59.36↑0.09\n+ repetition post-processing 67 .54↑3.76 63.50↑4.23\n+ class insertions 63 .69↓0.09 58.89↓0.38\nTable 4. Analysis of the improvements for the TC task using RoBERTa and XLNet\non the development set in the multi-label mode. Shown is micro-F 1 score.\nTechnique Classiﬁcation\nApproach F1-score\nRoBERTa 62 .08\n+ length and averaged span embedding 62 .27↑0.19\n+ multi-label correction 63 .50↑1.23\n+ class insertions 63 .69↑0.19\n+ repetition post-processing 66 .89↑3.20\n+ gazetteer post-processing 67 .07↑0.18\nTable 5. An incremental analysis of the proposed approach for the TC task on the\ndevelopment set.\nAlthough positional embeddings are used in BERT-like models, our experi-\nments showed that they are not enough to model the length of the span. Indeed,\nthe results for systems that explicitly use length improved both for RoBERTa\nand for XLNet, for both tasks.\nAccording to the source implementation of RoBERTa, XLNet, and other sim-\nilar models, only the [CLS] token embedding is used for sequence classiﬁcation.\nHowever, in the TC task, it turned out that the remaining tokens can also be\nuseful, as in the averaging approach.\nMoreover, the use of knowledge from the training set through post-processing\nwith a gazetteer consistently improved the results for both models. Yet, it can\nalso introduce errors since it ignores context. That is why we did not set 100%\nprobabilities for the corrected classes.\nAs for the sequential consistency of labels, the systems produced output with\nunacceptable nesting of spans of incompatible classes. Thus, correcting such cases\ncan also have a positive impact (see Table 3). However, a correct nesting does not\nguarantee correct ﬁnal markup, since we only post-process predictions. Better\nresults can be achieved if the model tries to learn this as part of training.\nTransformers: “The End of History” for Natural Language Processing? 13\nThe tables show that the highest quality increase for the TC task was achieved\nby correcting the repetition class. This is because this class is very frequent, but\nit often requires considering a larger context.\nWe also examined the impact of each modiﬁcation on RoBERTa for the TC\ntask, applying an incremental analysis on the development set (Table 5). We can\nsee that our proposed modiﬁcations are compatible and can be used together.\nFinally, note that while better pre-training could make some of the discussed\nproblems less severe, it is still true that certain limitations are more “theoretical”\nand that they would not be resolved by simple pre-training. For example, there\nis nothing in the Transformer architecture that would allow it to model the\nsegment length, etc.\n7 Discussion\nBelow we describe a desiderata to add to the Transformer in order to increase\nits expressiveness, which could guide the design of the next generation of general\nTransformer architectures.\nLength We have seen that length is important for the sequence labeling task.\nHowever, it would be important for a number of other NLP tasks, e.g., in seq2seq\nmodels. For example, in Neural Machine Translation, if we have an input sentence\nof length 20, it might be bad to generate a translation of length 2 or of length\n200. Similarly, in abstractive neural text summarization, we might want to be\nable to inform the model about the expected target length of the summary:\nshould it be 10 words long? 100-word long?\nExternal Knowledge Gazetteers are an important source of external knowledge,\nand it is important to have a mechanism to incorporate such knowledge. A\npromising idea in this direction is KnowBERT [22], which injects Wikipedia\nknowledge when pre-training BERT.\nGlobal Consistency For structure prediction tasks, such as sequence segmenta-\ntion and labeling, e.g., named entity recognition, shallow parsing, and relation\nextraction, it is important to model the dependency between the output labels.\nThis can be done by adding a CRF layer on top of BERT, but it would be nice\nto have this as part of the general model. More generally, for many text gen-\neration tasks, it is essential to encourage the global consistency of the output\ntext, e.g., to avoid repetitions. This is important for machine translation, text\nsummarization, chat bots, dialog systems, etc.\nSymbolic vs. Distributed Representation Transformers are inherently based on\ndistributed representations for words and tokens. This can have limitations,\ne.g., we have seen that BERT cannot pay attention to speciﬁc symbols in the in-\nput such as speciﬁc punctuation symbols like quotation marks. Having a hybrid\nsymbolic-distributed representation might help address these kinds of limita-\ntions. It might also make it easier to model external knowledge, e.g., in the form\nof gazetteers.\n14 A. Chernyavskiy et al.\n8 Conclusion and Future Work\nWe have shed light on some important theoretical limitations of pre-trained\nBERT-style models that are inherent in the general Transformer architecture.\nIn particular, we demonstrated on two diﬀerent tasks—one on segmentation,\nand one on segment labeling—and four datasets that these limitations are indeed\nharmful and that addressing them, even in some very simple and na¨ ıve ways, can\nyield sizable improvements over vanilla BERT, RoBERTa, and XLNet models.\nThen, we oﬀered a more general discussion on desiderata for future additions to\nthe Transformer architecture in order to increase its expressiveness, which we\nhope could help in the design of the next generation of deep NLP architectures.\nIn future work, we plan to analyze more BERT-style architectures, especially\nsuch requiring text generation, as here we did not touch the generation compo-\nnent of the Transformer. We further want to experiment with a pre-formulation\nof the task as span enumeration instead of sequence labeling with BIO tags.\nMoreover, we plan to explore a wider range of NLP problems, again with a focus\non such involving text generation, e.g., machine translation, text summarization,\nand dialog systems.\nAcknowledgments\nAnton Chernyavskiy and Dmitry Ilvovsky performed this research within the\nframework of the HSE University Basic Research Program.\nPreslav Nakov contributed as part of the Tanbih mega-project ( http://\ntanbih.qcri.org/), which is developed at the Qatar Computing Research In-\nstitute, HBKU, and aims to limit the impact of “fake news,” propaganda, and\nmedia bias by making users aware of what they are reading.\nReferences\n1. Arkhipov, M., Troﬁmova, M., Kuratov, Y., Sorokin, A.: Tuning multilingual trans-\nformers for language-speciﬁc named entity recognition. In: Proceedings of the 7th\nWorkshop on Balto-Slavic Natural Language Processing (BSNLP’19). pp. 89-93.\nFlorence, Italy (2019)\n2. Augenstein, I., Das, M., Riedel, S., Vikraman, L., McCallum, A.: SemEval 2017\ntask 10: ScienceIE - extracting keyphrases and relations from scientiﬁc publica-\ntions. In: Proceedings of the 11th International Workshop on Semantic Evaluation\n(SemEval’17). pp. 546-555. Vancouver, Canada (2017)\n3. Beltagy, I., Peters, M.E., Cohan, A.: Longformer: The long-document transformer.\nIn: ArXiv (2020)\n4. Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarl´ os, T.,\nHawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., Belanger, D., Colwell, L., Weller,\nA.: Rethinking attention with performers. In: Proceedings of the 9th International\nConference on Learning Representations (ICLR’21). (2021)\n5. Clark, K., Khandelwal, U., Levy, O., Manning, C.D.: What does BERT look at?\nAn analysis of BERT’s attention. ArXiv (2019)\nTransformers: “The End of History” for Natural Language Processing? 15\n6. Da San Martino, G., Barr´ on-Cede˜ no, A., Wachsmuth, H., Petrov, R., Nakov,\nP.: SemEval-2020 task 11: Detection of propaganda techniques in news articles.\nIn: Proceedings of the 14th International Workshop on Semantic Evaluation (Se-\nmEval’20), Barcelona, Spain (2020)\n7. Da San Martino, G., Yu, S., Barr´ on-Cede˜ no, A., Petrov, R., Nakov, P.: Fine-grained\nanalysis of propaganda in news article. In: Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP’19). pp. 5636-5646.\nHong Kong, China (2019)\n8. Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q., Salakhutdinov, R.: Trans-\nformerXL: Attentive language models beyond a ﬁxed-length context. Proceed-\nings of the 57th Annual Meeting of the Association for Computational Linguistics\n(ACL’19). pp. 2978-2988. Florence, Italy (2019)\n9. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep\nbidirectional transformers for language understanding. In: Proceedings of the 2019\nConference of the North American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies (NAACL-HLT’19). pp. 4171-\n4186. Minneapolis, MN, USA (2019)\n10. Durrani, N., Dalvi, F., Sajjad, H., Belinkov, Y., Nakov, P.: One size does not\nﬁt all: Comparing NMT representations of diﬀerent granularities. In: Proceedings\nof the 2019 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies (NAACL-HLT’19). pp.\n1504-1516. Minneapolis, MN, USA (2019)\n11. Ettinger, A.: What BERT is not: Lessons from a new suite of psycholinguistic\ndiagnostics for language models. Transactions of the Association for Computational\nLinguistics 8, 34-48 (2020)\n12. Goldberg, Y.: Assessing bert’s syntactic abilities (2019)\n13. Jawahar, G., Sagot, B., Seddah, D.: What does BERT learn about the structure\nof language? In: Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics (ACL’19). pp. 3651-3657. Florence, Italy (2019)\n14. Jin, D., Jin, Z., Zhou, J.T., Szolovits, P.: Is BERT really robust? A strong baseline\nfor natural language attack on text classiﬁcation and entailment. In: Proceedings\nof the 34th Conference on Artiﬁcial Intelligence (AAAI’20). pp. 8018-8025 (2019)\n15. Katharopoulos, A., Vyas, A., Pappas, N., Fleuret, F.: Transformers are RNNs:\nFast autoregressive transformers with linear attention. In: Proceedings of the 37th\nInternational Conference on Machine Learning (ICML’20). pp. 5156-5165 (2020)\n16. Kovaleva, O., Romanov, A., Rogers, A., Rumshisky, A.: Revealing the dark secrets\nof BERT. In: Proceedings of the Conference on Empirical Methods in Natural\nLanguage Processing and the International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP’19). pp. 4365-4374. Hong Kong, China (2019)\n17. Laﬀerty, J.D., McCallum, A., Pereira, F.C.N.: Conditional random ﬁelds: Proba-\nbilistic models for segmenting and labeling sequence data. In: Proceedings of the\nEighteenth International Conference on Machine Learning (ICML’01). pp. 282-289.\nWilliamstown, MA, USA (2001)\n18. Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., Soricut, R.: ALBERT:\nA lite BERT for self-supervised learning of language representations. In: ArXiv\n(2019)\n19. Liu, N.F., Gardner, M., Belinkov, Y., Peters, M.E., Smith, N.A.: Linguistic knowl-\nedge and transferability of contextual representations. In: Proceedings of the 2019\n16 A. Chernyavskiy et al.\nConference of the North American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies (NAACL-HLT’19). pp. 1073-\n1094. Minneapolis, MN, USA (2019)\n20. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,\nZettlemoyer, L., Stoyanov, V.: RoBERTa: A robustly optimized BERT pretraining\napproach. In: ArXiv (2019)\n21. Peters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., Zettle-\nmoyer, L.: Deep contextualized word representations. In: Proceedings of the 2018\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies (NAACL-HLT’18). pp. 2227-2237. New\nOrleans, LA, USA (2018)\n22. Peters, M.E., Neumann, M., Logan, R., Schwartz, R., Joshi, V., Singh, S., Smith,\nN.A.: Knowledge enhanced contextual word representations. In: Proceedings of\nthe 2019 Conference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP’19). pp. 43-54. Hong Kong, China. (2019)\n23. Popel, M., Bojar, O.: Training tips for the transformer model. The Prague Bulletin\nof Mathematical Linguistics 110(1), 43-70 (2018)\n24. Porter, M.F.: An algorithm for suﬃx stripping. Program 14(3), 130-137 (1980)\n25. Ratinov, L.A., Roth, D.: Design challenges and misconceptions in named entity\nrecognition. In: Proceedings of the Thirteenth Conference on Computational Nat-\nural Language Learning (CoNLL’09). pp. 147-155. Boulder, CO, USA. (2009)\n26. Rogers, A., Kovaleva, O., Rumshisky, A.: A Primer in BERTology: What We Know\nAbout How BERT Works. Trans. Assoc. Comput. Linguistics 8: 842-866 (2020)\n27. Sanh, V., Debut, L., Chaumond, J., Wolf, T.: DistilBERT, a distilled version of\nBERT: smaller, faster, cheaper and lighter. In: ArXiv (2019)\n28. Souza, F., Nogueira, R., Lotufo, R.: Portuguese named entity recognition using\nBERT-CRF. In: Arxiv (2019)\n29. Sun, L., Hashimoto, K., Yin, W., Asai, A., Li, J., Yu, P., Xiong, C.: Adv-BERT:\nBERT is not robust on misspellings! generating nature adversarial samples on\nBERT. In: Arxiv (2020)\n30. Tenney, I., Xia, P., Chen, B., Wang, A., Poliak, A., McCoy, R.T., Kim, N., Durme,\nB.V., Bowman, S.R., Das, D., Pavlick, E.: What do you learn from context? Prob-\ning for sentence structure in contextualized word representations. In: Arxiv (2019)\n31. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\nL., Polosukhin, I.: Attention is all you need. In: Arxiv (2017)\n32. Wallace, E., Wang, Y., Li, S., Singh, S., Gardner, M.: Do NLP models know num-\nbers? Probing numeracy in embeddings. In: Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP’19). pp. 5307-5315.\nHong Kong, China (2019)\n33. Wang, S., Li, B.Z., Khabsa, M., Fang, H., Ma, H.: Linformer: Self-attention with\nlinear complexity. In: Arxiv (2020)\n34. Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.R., Le, Q.V.: XLNet:\nGeneralized autoregressive pretraining for language understanding. In: Proceedings\nof the Annual Conference on Neural Information Processing Systems (NeurIPS’19),\npp. 5753-5763. (2019)\n35. Zaheer, M., Guruganesh, G., Dubey, A., Ainslie, J., Alberti, C., Onta˜ n´ on, S., Pham,\nP., Ravula, A., Wang, Q., Yang, L., Ahmed, A.: Big bird: Transformers for longer\nsequences. In: Proceedings of the Annual Conference on Neural Information Pro-\ncessing Systems (NeurIPS’20). (2020)",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7885336875915527
    },
    {
      "name": "Computer science",
      "score": 0.7525783777236938
    },
    {
      "name": "Architecture",
      "score": 0.7108153104782104
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6530091166496277
    },
    {
      "name": "Natural language processing",
      "score": 0.5475101470947266
    },
    {
      "name": "Segmentation",
      "score": 0.48274749517440796
    },
    {
      "name": "Machine learning",
      "score": 0.35254740715026855
    },
    {
      "name": "Engineering",
      "score": 0.09371215105056763
    },
    {
      "name": "History",
      "score": 0.08265161514282227
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I118501908",
      "name": "National Research University Higher School of Economics",
      "country": "RU"
    }
  ],
  "cited_by": 18
}