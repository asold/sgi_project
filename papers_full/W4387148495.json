{
  "title": "Tile Classification Based Viewport Prediction with Multi-modal Fusion Transformer",
  "url": "https://openalex.org/W4387148495",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5103324108",
      "name": "Zhiahao Zhang",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2104494385",
      "name": "Yiwei Chen",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2153169851",
      "name": "Weizhan Zhang",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2098436128",
      "name": "Caixia Yan",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2149828499",
      "name": "Qinghua Zheng",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A1981730961",
      "name": "Qi Wang",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A4321234584",
      "name": "Wangdu Chen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2784243934",
    "https://openalex.org/W2896449077",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W4251137679",
    "https://openalex.org/W3094338163",
    "https://openalex.org/W3179280609",
    "https://openalex.org/W3136920339",
    "https://openalex.org/W2914790472",
    "https://openalex.org/W2954684697",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W3091202141",
    "https://openalex.org/W3083847600",
    "https://openalex.org/W3021395787",
    "https://openalex.org/W3165727273",
    "https://openalex.org/W2963428739",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4312560592",
    "https://openalex.org/W3034727271",
    "https://openalex.org/W3030482512",
    "https://openalex.org/W2895957446",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W4283375183",
    "https://openalex.org/W2766324256",
    "https://openalex.org/W2992753629",
    "https://openalex.org/W2799064164",
    "https://openalex.org/W2943420393",
    "https://openalex.org/W3038754372",
    "https://openalex.org/W4293794975",
    "https://openalex.org/W3081459285",
    "https://openalex.org/W4360764276",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W4318148331",
    "https://openalex.org/W4221086736",
    "https://openalex.org/W3154476408",
    "https://openalex.org/W2990039632",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2884414611",
    "https://openalex.org/W2947567055"
  ],
  "abstract": "Viewport prediction is a crucial aspect of tile-based 360 video streaming system. However, existing trajectory based methods lack of robustness, also oversimplify the process of information construction and fusion between different modality inputs, leading to the error accumulation problem. In this paper, we propose a tile classification based viewport prediction method with Multi-modal Fusion Transformer, namely MFTR. Specifically, MFTR utilizes transformer-based networks to extract the long-range dependencies within each modality, then mine intra- and inter-modality relations to capture the combined impact of user historical inputs and video contents on future viewport selection. In addition, MFTR categorizes future tiles into two categories: user interested or not, and selects future viewport as the region that contains most user interested tiles. Comparing with predicting head trajectories, choosing future viewport based on tile's binary classification results exhibits better robustness and interpretability. To evaluate our proposed MFTR, we conduct extensive experiments on two widely used PVS-HM and Xu-Gaze dataset. MFTR shows superior performance over state-of-the-art methods in terms of average prediction accuracy and overlap ratio, also presents competitive computation efficiency.",
  "full_text": "arXiv:2309.14704v5  [cs.CV]  17 Jun 2025\nTile Classification Based Viewport Prediction with\nMulti-modal Fusion Transformer\nZhihao Zhangâˆ—\nXiâ€™an Jiaotong University\nXiâ€™an, China\nzh1142@stu.xjtu.edu.cn\nYiwei Chenâˆ—\nXiâ€™an Jiaotong University\nXiâ€™an, China\nchenyiwei2000@stu.xjtu.edu.com\nWeizhan Zhangâ€ \nXiâ€™an Jiaotong University\nXiâ€™an, China\nzhangwzh@mail.xjtu.edu.cn\nCaixia Yan\nXiâ€™an Jiaotong University\nXiâ€™an, China\nyancaixia@xjtu.edu.cn\nQinghua Zheng\nXiâ€™an Jiaotong University\nXiâ€™an, China\nqhzheng@mail.xjtu.edu.cn\nQi Wang\nMIGU Video\nShanghai, China\nwangqi@migu.cn\nWangdu Chen\nMIGU Video\nShanghai, China\nchenwangdu@migu.cn\nABSTRACT\nViewport prediction is a crucial aspect of tile-based 360â—¦video\nstreaming system. However, existing trajectory based methods lack\nof robustness, also oversimplify the process of information construc-\ntion and fusion between different modality inputs, leading to the\nerror accumulation problem. In this paper, we propose a tile classifi-\ncation based viewport prediction method with Multi-modal Fusion\nTransformer, namely MFTR. Specifically, MFTR utilizes transformer-\nbased networks to extract the long-range dependencies within each\nmodality, then mine intra- and inter-modality relations to capture\nthe combined impact of user historical inputs and video contents\non future viewport selection. In addition, MFTR categorizes fu-\nture tiles into two categories: user interested or not, and selects\nfuture viewport as the region that contains most user interested\ntiles. Comparing with predicting head trajectories, choosing future\nviewport based on tileâ€™s binary classification results exhibits better\nrobustness and interpretability. To evaluate our proposed MFTR,\nwe conduct extensive experiments on two widely used PVS-HM\nand Xu-Gaze dataset. MFTR shows superior performance over state-\nof-the-art methods in terms of average prediction accuracy and\noverlap ratio, also presents competitive computation efficiency.\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Planning under uncertainty ; â€¢\nInformation systems â†’Multimedia information systems .\nâˆ—Both authors contributed equally to this research.\nâ€ Corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nMM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada\nÂ© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0108-5/23/10. . . $15.00\nhttps://doi.org/10.1145/3581783.3613809\nKEYWORDS\nviewport prediction; multi-modal fusion; transformer network; tile\nclassification\nACM Reference Format:\nZhihao Zhang, Yiwei Chen, Weizhan Zhang, Caixia Yan, Qinghua Zheng,\nQi Wang, and Wangdu Chen. 2023. Tile Classification Based Viewport\nPrediction with Multi-modal Fusion Transformer. In Proceedings of the\n31st ACM International Conference on Multimedia (MM â€™23), October 29-\nNovember 3, 2023, Ottawa, ON, Canada. ACM, New York, NY, USA, 9 pages.\nhttps://doi.org/10.1145/3581783.3613809\n1 INTRODUCTION\nWith the rapid growth of Virtual Reality(VR),360â—¦video has gained\nmore attention due to its immersive and interactive experience to\nviewers. There are tons of 360â—¦videos on major video platforms\nsuch as YouTube and Meta. However, streaming 360â—¦video de-\nmands massive bandwidth compared to transfer regular planar\nvideo [38]. For example, transmitting a 4K 360â—¦video requires a\nnetwork throughput of 25 Mbps and above while a 4K regular video\nonly needs 5 âˆ’10 Mbps. Thus, 360â—¦video delivery under limited\nbandwidth network is challenging and needs to be optimized.\nRecently, tile-based streaming approaches [5, 27, 32, 34] have\nbeen proposed, they divide each 360â—¦video frame into equal sized\ntiles, and stream usersâ€™ viewport which consists of several con-\ntiguous tiles in higher resolution. Specifically, prevailing methods\nforecast usersâ€™ head position coordinates first, and then select a\nviewport whose geometric center is closest to the coordinates. For\nexample, [8, 29, 31, 38] concentrate on temporal dimension, encode\nhistorical temporal features and forecast head positions, transfering\nviewport prediction task to typical time series prediction problem.\nHowever, these approaches ignore the importance of visual contents\non usersâ€™ focus changes, and predict head positions in a step-by-step\nway which leads to an error accumulation problem. To alleviate\nthese limitations, methods such as [1, 11, 21, 37, 41] take both vi-\nsual and temporal features into consideration, utilize CNN based\nnetworks along with recurrent neural network (RNN) to predict\nMM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada Zhihao Zhang et al.\n(a) Trajectory based\n (b) Tile classification based\nFigure 1: (a) With a slight difference of predicted head coor-\ndinates (colored stars), selected future viewports (colored\nbounding boxes) exhibit noticeable positional deviations.\n(b) The proposed MFTR method based on tile classification,\nwhich divides tiles into two two categories: user-interested\ntiles (colored) and non-user interested tiles (colorless). The\npredicted result (red bounding box) is a region of viewport-\nsize that contains the majority of user-interested tiles.\nthe coordinates of head positions jointly. Although noticeable im-\nprovements have been made by fusing two modality features to\nforecast viewport jointly, these approaches simplify the process of\ninformation construction and fusion, while the prediction process\nis still mechanized.\nTo be more specific, there are two main defects in previous\napproaches. First, prevailing methods are both trajectory based,\nforecast viewport through predicting head positions, which lack\nof robustness. For example, as shown in Fig.1(a), with a small shift\nin the predicted head coordinates, the corresponding viewports\nare obviously different. Therefore, a more stable viewport predic-\ntion mechanism should be designed. Second, these conventional\nmethods simplify feature construction process and fuse different\nmodality features using mechanized processes like concatenation,\ncomplete the prediction task in a step-to-step way. These network\nstructures lead to an error accumulation problem, resulting in a\ndrastic drop in prediction accuracy with the prolongation of the pre-\ndiction time. Thus, we argue that a more comprehensive network\nstructure should be considered.\nIn order to solve the problem of low robustness, we propose a\ntile-classified scheme, predict the final viewport based on tile binary\nclassification results rather than head trajectary coordinates. As\nillustrated in Fig.1(b), we classify all the tiles into two categories,\none is the tiles that users are interested in, and the others are not,\nand then choose the viewport with highest overlapping ratio with\nthe interested tiles. By doing so, proposed tile classification based\napproach selects the viewport with the highest probability of user\ninterest instead of single head position, which is more robust and\ninterpretable.\nBenefiting from the success of transformer architecture [33] in\nmodality relationship modeling, we propose a transformer-based\nprediction framework, which explicitly considers both visual and\ntemporal features to jointly forecast the viewport of interest. Our\nmethod employs multi-head attention module along with learn-\nable tokens to effectively mine interactions between modalities and\naccurately classify the interested tiles. In this sense, we call it Multi-\nmodal Fusion Transformer(MFTR). The proposed MFTR includes\nfive key designs. First, Temporal Branch contains two LSTM net-\nworks to encode head and eye movements respectively, followed by\na Temporal Transformer to mine the motion trend within the same\nmodal and extract the long-range temporal dependencies. Second,\nVisual Branch adopts MobileNetV2 backbone along with a Visual\nTransformer to obtain more robust visual representations. Then,\nwe add extra modality token on temporal and visual embeddings to\nrepresent different modal properties, after which Temporal-Visual\nFusion Module utilizes Temporal-Visual Transformer to explore\nintra- and inter-modality relations and aggregate them. Moreover,\nPosition Prediction Head supports the training of Temporal Trans-\nformer by providing more supervision information about fused\ntemporal features. At last, in Tile Classification Head, we gener-\nate score map for each tile in every time stamp to represent the\nprobability that each tile may be concerned, and the final viewport\nis the region which contains most tiles with the score above the\nthreshold.\nTo summary, the major contributions are as follows:\nâ€¢We propose a novel tile classification based viewport pre-\ndiction scheme, transfer viewport prediction to a binary\ntile classification task providing better robustness and inter-\npretability.\nâ€¢We propose a Multi-modal Fusion Transformer network\nnamely MFTR, extracting the long-range dependencies within\nsingle modality and modeling the combined influence of both\ntemporal and visual features on the tiles that users interested\nin.\nâ€¢We demonstrate our method outperforms state-of-the-art\napproaches on two widely used datasets, shows higher pre-\ndiction accuracy with competitive computation efficiency\nespecially for long term prediction.\n2 RELATED WORK\n2.1 Viewport Prediction\nViewport prediction task [6, 39] aims to predict the future viewport\nbased on userâ€™s past viewing history. There are basically two kinds\nof schemes to predict future viewport:Content-Agnostic Approaches:\n[7, 8, 13, 18, 26, 29, 31, 38] leverage typical networks to capture\ntemporal dependencies in order to predict future head motions. [13]\ntook userâ€™s viewpoint position history as sequences and predict the\nfuture viewpoint positions through an LSTM network. [4] leveraged\nthe past viewport scan path to predict userâ€™s future directions of\nmovements. Content-Aware Approaches: Unlike Content-Agnostic\nApproaches, [1, 2, 11, 14, 15, 19, 36, 37, 40, 42, 43] predict future\nviewport using temporal features and visual contents jointly. [41]\nutilizes a graph convolutional network (GCN) to output the saliency\nmaps of the 360 video frames. Moreover, [21] propose a spherical\nconvolution-empowered viewport prediction method to eliminate\nthe projection distortion of 360-degree video.\nMFTR method belongs to Content-Aware Approaches, but dif-\nferent from the above trajectory based methods aim at predicting\nfuture head motions, we transfer the viewport prediction to a tile\nclassification task, providing a more robust and stable method which\nstrives to find out tiles of user interest.\n2.2 Transformer in Multi-modal Tasks\nTransformer is first proposed in [33] to handle neural machine trans-\nlation task. Attention module, as the core part in transform, focus\nTile Classification Based Viewport Prediction with\nMulti-modal Fusion Transformer MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada\nAdd & Norm\nMulti-Head\nAttention\nFeed\nForward\nAdd & Norm\nğ‘Ã—\nLinear Projection\nTransformer Encoder\nInput Embeddings\nPositional\n Encoding\n(a) Encoder Layer\nEye Movements\n... ...\nHead Movements\n... ...\nConcatenate\n... ...\nTemporal Transformer Visual Transformer\n...\nTemporal-Visual Transformer\n... ...... ...\nTemporal Branch\nTemporal-Visual Fusion Module Position Prediction Head Tile Classification Head\nVisual Branch\n...â„ğ‘#$ 0 0 0 1 1ğ‘£#\nğ‘’#) ğ‘’*) ğ‘’+) ğ‘’,) ğ‘’-) â„#) â„*) â„+) â„,) â„-)\nLiner Projection Liner Projection\nLSTM LSTM\nMobileNet-V2 ...\n...\nâ„’/01\nğ‘£ğ‘-23\nRegression MLP\n1ğ‘£-23ğ‘£-2#â„ğ‘-$\nâ„’451\n \nâ„ğ‘$-632#\n... ...... ... ğ‘£#) ğ‘£*) ğ‘£+) ğ‘£-23)ğ‘£-)\nRegression MLP\nLiner Projection\nReLU Activation\nğ‘‡\tğ‘šğ‘ğ‘ğ‘ \nğ‘£ğ‘-2*ğ‘£ğ‘-2#\nğ‘Ã—\nğ‘>âˆ—ğ‘@\n0 0 0 0 0 0\n0 1 0 1 1 0\n0 1 1 1 1 0\n0 0 1 0 0 0\n0 0 1 0 1 0\n0 1 1 1 1 0\n0 0 1 0 1 0\n0 0 0 1 0 0\nğ‘¡ğ‘“#$ğ‘¡ğ‘“*$ğ‘¡ğ‘“+$ ğ‘¡ğ‘“,$ ğ‘¡ğ‘“-$\nğ‘¡ğ‘“# ğ‘¡ğ‘“* ğ‘¡ğ‘“- (b) MFTR Model Overview\nFigure 2: (a) Transformer encoder architecture revisit. (b) An overview of our proposed framework. MFTR contains five\ncomponents: (1) Temporal Branch extracts fused temporal feature from head, eye movement histories; (2) Visual Branch\nencodes the visual features from given 360 video frames, and enhances them by Visual Transformer; (3) Temporal-Visual\nFusion Module fuses different modal features to indicate userâ€™s interested areas influenced by motion trend and visual area; (4)\nPosition Prediction Head provides more supervision information and assist in the training of Temporal Transformer; (5) Tile\nClassification Head determines the user interested tiles and choose the future viewport.\non aggregating the information of the whole sequence with adap-\ntive weights, could fully capture long-term dependency in parallel.\nRecently, transformer networks have achieved great success in NLP\nfield [9, 16, 28] and vision tasks [3, 10, 12, 22, 23]. Inspired by trans-\nformerâ€™s intrinsic advantages in modeling different modalities, a\nlarge number of transformers have been studied extensively for var-\nious multi-modal tasks. Visual-linguistic pre-training [17, 20, 24, 25]\nconstruct representations of images and texts jointly. In general,\nmulti-modal transformer works need to perform two steps: tok-\nenize the input and devise several transformer encoder layers for\njoint learning. In our work, we perform transformer capturing\nlong-range dependencies within single modaity and modeling the\ncombined influence of multi-modal features through multi-modal\ntransformers.\n3 METHOD\n3.1 Problem Formulation\nViewport prediction is to predict userâ€™s future viewport for the in-\ncoming seconds conditioned on user historical traces and360â—¦video\ncontents. Formally, given temporal inputs, include past head orienta-\ntions Ëœğ» =\nn\nËœâ„1,..., Ëœâ„ğ‘¡\no\nand eye movements Ëœğ¸ = {Ëœğ‘’1,..., Ëœğ‘’ğ‘¡}from time\nstamp 1 to ğ‘¡. Where Ëœâ„ğ‘– = {(ğœ™ğ‘–,ğœ‘ğ‘–)|âˆ’ ğœ‹ â‰¤ğœ™ğ‘– â‰¤ğœ‹,âˆ’ğœ‹/2 â‰¤ğœ‘ğ‘– â‰¤ğœ‹/2}\nand Ëœğ‘’ğ‘– = {(ğ‘¥ğ‘–,ğ‘¦ğ‘–)|0 â‰¤ğ‘¥ğ‘– â‰¤1,0 â‰¤ğ‘¦ğ‘– â‰¤1}denotes the polar coordi-\nnates and the cartesian coordinates at time stamp ğ‘– respectively.\nAs visual inputs, Ëœğ‘‰ = {Ëœğ‘£1,..., Ëœğ‘£ğ‘¡,..., Ëœğ‘£ğ‘¡+ğ‘‡}express sequences of 360â—¦\nvideo frames where Ëœğ‘£ğ‘– corresponds to visual contents at timeğ‘–. More-\nover, {ğ‘¡+1,...,ğ‘¡ +ğ‘‡}are defined as predicted time window, ğ‘‡ is\nthe prediction length. The overall goal is to predict userâ€™s viewport\nin the predicted time window, formulated as ğ‘ƒ = {ğ‘ğ‘¡+1,...,ğ‘ ğ‘¡+ğ‘‡},\nwhere ğ‘ğ‘¡+ğ‘– means the position coordinates of viewport. We need to\ntrain a objective function ğ‘“(Â·)with learnable parameters ğ‘Šâˆ—given\ntemporal and visual inputs,\nğ‘ƒ = ğ‘“\n\u0010\nËœğ», Ëœğ¸, Ëœğ‘‰;ğ‘Šâˆ—\n\u0011\n. (1)\n3.2 Multi-head Attention Revisit\nBefore detailing the architecture of MFTR, we first revisit the Multi-\nHead Attention mechanism, which is the core module of our frame-\nwork. Given queryğ‘„, keyğ¾, and valueğ‘‰ withğ‘‘channel dimensions,\nMulti-Head Attention [33] calculates the attended output as:\nğ´ğ‘¡ğ‘¡(ğ‘„,ğ¾,ğ‘‰ )= ğ‘†ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ (ğ‘„ğ¾ğ‘‡/\nâˆšï¸\nğ‘‘ğ‘˜)ğ‘‰,\nâ„ğ‘– = ğ´ğ‘¡ğ‘¡(ğ‘„ğ‘Šğ‘„\nğ‘– ,ğ¾ğ‘Š ğ¾\nğ‘– ,ğ‘‰ğ‘Šğ‘‰\nğ‘– ),\nğ‘€ğ‘¢ğ‘™ğ‘¡ğ‘–ğ»ğ‘’ğ‘ğ‘‘ (ğ‘„,ğ¾,ğ‘‰ )= ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡(â„1,Â·Â·Â· ,â„ğ‘›)ğ‘Šğ‘‚,\n(2)\nwhere the parameter matrices ğ‘Šğ‘„\nğ‘– ,ğ‘Šğ¾\nğ‘– âˆˆRğ‘‘Ã—ğ‘‘ğ‘˜, ğ‘Šğ‘‰\nğ‘– âˆˆRğ‘‘Ã—ğ‘‘ğ‘£,\nand ğ‘Šğ‘‚ âˆˆRğ‘›ğ‘‘ğ‘£Ã—ğ‘‘, and ğ‘›represents the number of parallel atten-\ntion layers. Specifically, when setting ğ‘„, ğ¾, ğ‘‰ to the same value,\nthe module called Multi-head Self-Attention (MSA).\nThe typical transformer has encoder-decoder structure, while\nwe only use transformer encoder as the basic block, like Figure\n2(a) illustrated. Each transformer encoder has two sub-networks:\nMSA and feed forward network (FFN), and designed in residual\nstructures. Concretely, let us denote the input asğ‘¥ğ‘–, the transformer\nMM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada Zhihao Zhang et al.\nencoderâ€™s output is:\nğ‘¥â€²\nğ‘– = ğ‘“ğ¿ğ‘(ğ‘¥ğ‘– +ğ‘“ğ‘€ğ‘†ğ´(ğ‘¥ğ‘–)),\nğ‘¥ğ‘–+1 = ğ‘“ğ¿ğ‘(ğ‘¥â€²\nğ‘– +ğ‘“ğ¹ğ¹ğ‘ (ğ‘¥â€²\nğ‘–)), (3)\nwhere ğ‘“ğ¿ğ‘ means layer normalization, ğ‘“ğ‘€ğ‘†ğ´ indicates multi-head\nself-attention outputs, and ğ‘“ğ¹ğ¹ğ‘ implies feed-forward network\nwhich is composed of fully connected layers and ReLU activation\nlayers.\n3.3 Multi-modal Fusion Transformer\nWe propose a tile classification based multi-modal transformer\n(MFTR) for viewport prediction task which is depicted in Fig. 2(b).\nThe model consists of five main building blocks: (1)Temporal Branch\nutilizes Temporal Transformer to construct comprehensive tempo-\nral features from past head and eye movements; (2) Visual Branch\ngenerates robust visual features from given 360â—¦video frames; (3)\nTemporal-Visual Fusion Module apply transformer to capture intra-\nand inter-modality contexts; (4)Position Prediction Head forecasts\nhead position as supervision information during training phase; (5)\nTile Selection Head chooses user interested tiles and forecast the\nfuture viewport. In the following, we explain each module of MFTR\nin details.\nTemporal Branch. Different from time series problems, viewport\nswitching is more objective which means there are many user-\nside factors that affect their choice for the future viewport. We\nposit that user historical head movements and eye motions have\na decisive role in future viewport choosing. Based on this, to bet-\nter indicate usersâ€™ moving habits, we employ two long shot term\nmemory (LSTM) networks to encode temporal inputs respectively,\nand use Temporal Transformer to fuse them. Both two LSTMs\nhave hidden size 256, which means the representations of head\nand gaze positions share same channel dimension. The Temporal\nTransformer composed by 6 transformer encoders [ 33], and the\nchannel dimension set as 512.\nGiven the past head movements Ëœğ» âˆˆRğ‘¡Ã—2 and eye histories\nËœğ¸ âˆˆRğ‘¡Ã—2, after leveraging fully connected layer (FC) to increase\nchannel dimensions, we obtain ğ» âˆˆRğ‘¡Ã—ğ¶â„, ğ¸ âˆˆRğ‘¡Ã—ğ¶ğ‘’. Then we\nemploy two LSTM networks to generate temporal embeddings ğ»â€²\nand ğ¸â€², which share the same shape asğ», ğ¸. Through concatenation,\nthe temporal features fğ‘‡ğ¹ âˆˆRğ‘¡Ã—ğ¶ğ‘¡ğ‘“ can be acquired, where ğ¶ğ‘¡ğ‘“ =\nğ¶â„ +ğ¶ğ‘’ = 512. After that, we use a Temporal Transformer to\nmine the motion trend within single modality, and gain the fused\ntemporal features ğ‘‡ğ¹ = {ğ‘¡ğ‘“1,ğ‘¡ğ‘“2,Â·Â·Â· ,ğ‘¡ğ‘“ğ‘¡}âˆˆRğ‘¡Ã—ğ¶ğ‘¡ğ‘“ .\nVisual Branch. 360â—¦video content is another a key factor since\nusers can also be attracted by the contents in the videos. In Visual\nBranch, we start with a backbone network and followed by Visual\nTransformer to acquire more robust visual representions. Unlike\ntypical convolutional backbones, we employ MobileNet-V2[30] be-\ncause of its lightweight and portability. Concretely, MobileNet-V2\ngenerates visual features of 1000 channel dimensions for each video\nframe. And Visual Transformer shares the same structure with\nTemporal Transformer.\nSpecifically, given the input frames Ëœğ‘‰ âˆˆR(ğ‘¡+ğ‘‡)Ã—3Ã—ğ»Ã—ğ‘Š, we\nexploit MobileNet-V2 network to generate visual features ğ‘‰â€² âˆˆ\nR(ğ‘¡+ğ‘‡)Ã—1000. Then, we leverage a fully connected layer to reduce\nthe dimension and obtain newlyğ‘‰â€²â€²âˆˆR(ğ‘¡+ğ‘‡)Ã—ğ¶ğ‘£, where we setğ¶ğ‘£\nas 512. Moreover, we further add position embeddings onğ‘‰â€²â€²[3] to\nlet the transformer encoder sensitive to positions of visual sequence.\nVisual Transformer explores the sequence of image features and\noutput more robust representations ğ‘‰ = {ğ‘£1,Â·Â·Â· ,ğ‘£ğ‘¡+1,Â·Â·Â· ,ğ‘£ğ‘¡+ğ‘‡}âˆˆ\nR(ğ‘¡+ğ‘‡)Ã—ğ¶ğ‘£ which share the same size with ğ‘‰â€²â€².\nTemporal-Visual Fusion Module. Thanks to the attention mecha-\nnism for its good performance in modality relationship modeling.\nTemporal-Visual Fusion Module is also a stack of 6 transformer\nencoders, to explore intra- and inter-modality relations and ag-\ngregate temporal-visual features together. Specifically, since the\nfused temporal embeddings ğ‘‡ and visual representations ğ‘‰ share\nthe same channel dimension, the joint input for this fusion module\ncan be formulated as:\nğ¼ğ‘› =\nğ‘¡ğ‘’ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ğ‘ \nz                    }|                    {\n[ğ‘¡ğ‘“1 +ğ‘¡ğ‘“2 +Â·Â·Â·+ ğ‘¡ğ‘“ğ‘¡,ğ‘£1,Â·Â·Â· ,ğ‘£ğ‘¡,Â·Â·Â· ,ğ‘£ğ‘¡+ğ‘‡]\n|                    {z                    }\nğ‘£ğ‘–ğ‘ ğ‘¢ğ‘ğ‘™ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ğ‘ \n, (4)\nwhere each ğ‘¡ğ‘“ğ‘– and ğ‘£ğ‘– has same dimension ğ¶ = ğ¶ğ‘¡ğ‘“ = ğ¶ğ‘£ = 512.\nFollowing [17] we further add a modal-type embedding on the\njoint input to let the model distinguish the differences between two\nmodalities. Because fused results represent the combined impact\nof temporal and visual features while {ğ‘£ğ‘¡+1,Â·Â·Â· ,ğ‘£ğ‘¡+ğ‘‡}imply the\nvisual contents in predicted time window, we apply the transformer\noutputs of these features ğ‘‰ğ‘ƒ = {ğ‘£ğ‘ğ‘¡+1,Â·Â·Â· ,ğ‘£ğ‘ğ‘¡+ğ‘‡}âˆˆRğ‘‡Ã—ğ¶ for the\nnext stage of prediction.\nPosition Prediction Head. During training phase, we forecast\nthe possible head positions in the predicted time window to pro-\nvide more supervision information and assist in the training of\nTemporal Transformer. Since Temporal Transformer could mine\nlong-distance dependencies in temporal dimension, we employ\na basic MLP with hidden size 128 and 64 to forecast the possi-\nble head motions. More Concretely, MLP regresses the channel\ndimension ğ¶ğ‘¡ğ‘“ of temporal features ğ‘‡ğ¹ into 2 and output gğ»ğ‘ƒ =n\nfâ„ğ‘1,Â·Â·Â· ,fâ„ğ‘ğ‘¡âˆ’ğ‘‡+1,Â·Â·Â· ,fâ„ğ‘ğ‘¡\no\nâˆˆ Rğ‘¡Ã—2. We select last ğ‘‡ elements\nğ»ğ‘ƒ =\nn\nfâ„ğ‘ğ‘¡âˆ’ğ‘‡+1,Â·Â·Â· ,fâ„ğ‘ğ‘¡\no\n= {â„ğ‘1,Â·Â·Â· ,â„ğ‘ğ‘‡}âˆˆRğ‘‡Ã—2 as the pre-\ndicted head movement results. We adopt Mean-Squared Loss to\nmeasure the quality of temporal feature fusion, thus the regression\nloss Lğ‘ğ‘œğ‘  for Position Prediction Head is defined as:\nLğ‘ğ‘œğ‘  = 1\nğ‘‡\nğ‘‡âˆ‘ï¸\nğ‘–=1\n(â„ğ‘ğ‘– âˆ’ Ë†â„ğ‘ğ‘–)2, (5)\nwhere â„ğ‘ğ‘– is predicted head movements while Ë†â„ğ‘ğ‘– represents the\nground truth head coordinates at time stamp ğ‘–.\nTile Classification Head. Since trajectory based approaches lack\nof robustness, we predict future viewport based on tile classification\nresults. Every frame could be divided into ğ‘ = ğ‘â„ Ã—ğ‘ğ‘¤ = 10 Ã—20\ntiles in our work, so that we apply a simple MLP on transformer\nfused featureğ‘£ğ‘ğ‘– to generate score mapğ‘†ğ‘– âˆˆRğ‘â„Ã—ğ‘ğ‘¤ at time stamp\nğ‘–. Each element ğ‘†ğ‘–ğ‘š,ğ‘› âˆˆ[0,1]indicates how interested the user is\nin each tile, then we set a score threshold ğ›¾ = 0.55 to classify the\ntiles into two categories: the tiles whose scores exceed ğ›¾ represent\nuser-interested while the others are not. Score map can be turned to\nTile Classification Based Viewport Prediction with\nMulti-modal Fusion Transformer MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada\na 0-1 matrix ğ‘Œğ‘– âˆˆRğ‘â„Ã—ğ‘ğ‘¤. Then we select viewport-sized regions\nğ‘ƒ = {ğ‘ğ‘¡+1,Â·Â·Â· ,ğ‘ğ‘–,Â·Â·Â· ,ğ‘ğ‘¡+ğ‘‡}which contain most user interested\ntiles as the predicted viewports for each time stamp ğ‘–.\nSince we have done binary classification task of all tiles, we adopt\nbasic Cross-Entropy Loss as the loss function in this module, like:\nLğ‘ğ‘™ğ‘  = âˆ’ 1\nğ‘ğ‘¢ğ‘š\nğ‘¡+ğ‘‡âˆ‘ï¸\nğ‘–=ğ‘¡+1\nğ‘ğ‘¤âˆ‘ï¸\nğ‘š=1\nğ‘â„âˆ‘ï¸\nğ‘›=1\n((Ë†ğ‘Œğ‘–\nğ‘š,ğ‘› Â·ğ‘™ğ‘œğ‘”(ğ‘†ğ‘–\nğ‘š,ğ‘›)\n+(1 âˆ’Ë†ğ‘Œğ‘–\nğ‘š,ğ‘›)Â·ğ‘™ğ‘œğ‘”(1 âˆ’ğ‘†ğ‘–\nğ‘š,ğ‘›)),\n(6)\nwhere ğ‘ğ‘¢ğ‘š = ğ‘‡ Ã—ğ‘ğ‘¤ Ã—ğ‘â„, and Ë†ğ‘Œğ‘– âˆˆRğ‘â„Ã—ğ‘ğ‘¤ is a 0-1 matrix\nwhich could represent whether tile is included in the ground truth\nviewport positions Ë†ğ‘ğ‘–, each element in the matrix Ë†ğ‘Œğ‘–ğ‘š,ğ‘› âˆˆ{0,1}.\n3.4 Training Objective\nThe above MFTR formulation yeilds a fully supervised end-to-end\ntraining with a total objective:\nL= ğ›¼Lğ‘ğ‘œğ‘  +ğ›½Lğ‘ğ‘™ğ‘ , (7)\nwhere ğ›¼ and ğ›½ are hyper parameters to balance these two losses,\nLğ‘ğ‘œğ‘  and Lğ‘ğ‘™ğ‘  are the loss function of Position Prediction Head\nand Tile Classification Head respectively.\n4 EXPERIMENTS\nIn this section, we evaluate proposed method MFTR and analyze\nthe results.\n4.1 Experimental Setting\nDatasets. Our experiments are conducted on two widely-used\ndatasets: PVS-HM [35] dataset and Xu-Gaze [37] dataset. PVS-HM\ndataset comprises 76 360â—¦videos with resolutions ranging from 3ğ¾\nto 8ğ¾. Total58 participants viewed all the videos, used an HTC Vive\nto capture their head movements and eye fixation traces at the frame\nlevel. In addition, Xu-Gaze dataset consists of 208 high definition\ndynamic 360â—¦videos with a frame rate of 25. Total 45 participants\nwatched these videos, more than 900,000 head movement traces\nand eye traces were recorded, resulting in a total watching time of\nover 100 hours.\nTo align with the problem setting, we configured the time inter-\nval to one second. In other words, to represent the userâ€™s viewing\ncontent during each second, we extracted one frame from the 360â—¦\nvideo frames per second. After collecting the video set, we then\ncalculated the viewport position for each second based on the cor-\nresponding user head and eye traces. Specifically, we mapped the\nhead data onto the video frame coordinates, followed by the selec-\ntion of the viewport whose geometric center is closest to the point\nas the ground truth. In order to tackle the problem formulation, we\nselected the historical traces of continuous ğ‘¡ seconds and the video\nframes of ğ‘¡ +ğ‘‡ seconds as the input sequence. The ground truth\nfor the upcoming ğ‘‡ seconds of viewport positions was utilized.\nWe implemented a sliding-window mechanism with one-second to\ngenerate additional input sequences. This implies that the start time\nof two adjacent input sequences varied by one second. Finally, we\nrandomly split the generated sequences into a test set, validation\nset, and training set, with a ratio of 1 : 1 : 8.\nMetrics. To evaluate the performance of our proposed method\nMFTR, we use average prediction accuracy (AP) which gives clearly\nquantitative results to measure the accuracy of predicted results. It\nis calculated by:\nğ´ğ‘ƒ = 1\nğ‘‡\nğ‘‡âˆ‘ï¸\nğ‘–=1\n(Ë†ğ‘ğ‘– == ğ‘ğ‘–), (8)\nwhere Ë†ğ‘ğ‘– denotes the ground truth viewport and ğ‘ğ‘– denotes pre-\ndicted viewport at timestamp ğ‘–.\nFollowing the previous work [ 4, 21, 35] we also use the the\nviewport overlap ratio metric (AO) which is the average percentage\nof the part of predicted viewports that inside the actual viewport.\nIt can be calculated by:\nğ´ğ‘‚ = 1\nğ‘‡\nğ‘‡âˆ‘ï¸\nğ‘–=1\nğ‘†(Ë†ğ‘ğ‘–,ğ‘Š,ğ» )âˆ©ğ‘†(ğ‘ğ‘–,ğ‘Š,ğ» )\nğ‘†(Ë†ğ‘ğ‘–,ğ‘Š,ğ» ) , (9)\nwhere ğ‘†()is function to calculate the viewport region given view-\nport center coordinate ğ‘ğ‘¡, height ğ» and width ğ‘Š.\nSimilar to [21], we incorporate delay time, which refers to the\ntime required for prediction in one batch using the trained model\nweights. Specifically, we set the prediction lengthğ‘‡ to 5 seconds and\nbatch size to16. Delay time could testify simplicity and computation\nefficiency of our method.\nImplementation Details. Our model consists of LSTMs with three\nhidden layers, each with a hidden size of 256, and transformers with\na dimension of 512. To reduce computational complexity, we down-\nsampled the resolution of each selected 360-degree video frame\nfrom 3840 Ã—2048 to 720/ğ‘¡ğ‘–ğ‘šğ‘’ğ‘ 360. Furthermore, we set the length\nof the historical sequence to five seconds (t=5) and the maximum\nprediction length to five seconds (T=5). We divided each 360-degree\nvideo frame into 10x20 tiles, with each viewport containing 4 Ã—9\ntiles. During training, we set ğ›¼ and ğ›½ to 0.35 and 0.65, respectively.\nWe used the AdamW optimizer with a learning rate of 10âˆ’3 and\nimplemented early stopping on the validation set if the accuracy did\nnot improve over ten epochs. The batch size was 16, and the model\nwas trained for 200 epochs. Both of the experiments were conducted\non GTX-3090 platform, with each dataset requiring approximately\ntwo hours of training time.\n4.2 Performance Evaluation\nIn order to evaluate the effectiveness of our proposed method MFTR,\nwe compared it against three baseline studies: Offline-DHP [ 35],\nVPT360 [4], and SPVP360 [21]. We opted to compare our approach\nwith Offline-DHP, which is the most widely recognized option, as\nwell as two of the most recent open-source state-of-the-art methods\nunder the same data and settings. It should be noted that SPVP360, a\nrecently published method, has attained the state-of-the-art results\nin viewport prediction.\nResults on PVS-HM dataset. For the prediction of future view-\nports, we employed the past five seconds of user trajectories and\nvideo contents, with prediction lengths ranging from one second\nto five seconds. Table 1 compares the AP and AO metrics of our\nproposed MFTR method with selected state-of-the-art methods.\nAnd the result is represented in line format in Fig. 3(a) and Fig. 3(b).\nOur results indicate that: i) MFTR outperforms the state-of-the-art\nMM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada Zhihao Zhang et al.\nPrediction length 1s 2s 3s 4s 5s\nMetrics AP AO AP AO AP AO AP AO AP AO\nOffline-DHP 0.787 0.833 0.667 0.746 0.542 0.598 0.465 0.531 0.409 0.447\nVPT360 0.850 0.904 0.745 0.821 0.658 0.713 0.604 0.641 0.559 0.605\nSPVP360 0.901 0.937 0.820 0.887 0.756 0.811 0.684 0.728 0.602 0.650\nMFTR 0.954 0.981 0.872 0.905 0.825 0.865 0.773 0.814 0.730 0.787\nTable 1: Average prediction accuracy (AP) and average overlap ratio (AO) comparison with baseline methods on PVS-HM dataset.\nPrediction length 1s 2s 3s 4s 5s\nMetrics AP AO AP AO AP AO AP AO AP AO\nOffline-DHP 0.694 0.754 0.636 0.645 0.559 0.589 0.511 0.537 0.434 0.491\nVPT360 0.747 0.813 0.662 0.726 0.610 0.652 0.557 0.594 0.486 0.533\nSPVP360 0.824 0.857 0.731 0.783 0.639 0.695 0.573 0.630 0.518 0.566\nMFTR 0.856 0.905 0.767 0.819 0.702 0.754 0.651 0.698 0.597 0.655\nTable 2: Average prediction accuracy (AP) and average overlap ratio (AO) comparison with baseline methods on Xu-Gaze dataset\nMethod Offline-DHP VPT360 SPVP360 MFTR\nDelay(ms) 102 21 124 74\nAP(%) 40.9 55.9 60.2 73.0\nTable 3: Comparison of delay time and average precision (AP)\nwith baseline methods.\nmethods by a significant margin. Specifically, the overall average\nprediction accuracy (AP) and average overlap ratio (AO) improved\nby 7.82% and 6.78%, respectively, even when compared to the best\nbaseline method SPVP360. This improvement can be attributed to\nthe proposed tile-guided prediction mechanism, which reduces fluc-\ntuations and provides a more robust viewport selection mechanism.\nii) As a metric for model stability, we propose utilizing the percent-\nage reduction in prediction time from one second to five seconds.\nCompared to the previous state-of-the-art method SPVP360, MFTR\nshows stability improvements of 7.50% and 9.30% in terms of AP\nand AO respectively. This result indicates our transformer-based\nframework can effectively explore long-range dependencies in sin-\ngle modality and successfully model interactions between different\nmodalities.\nResults on Xu-Gaze dataset. We conducted experiments on the\nXu-Gaze dataset using the same training setting as on the PVS-HM\ndataset to further validate our results. As depicted in Table 2, Fig.\n3(c) and Fig. 3(d), our proposed method MFTR still achieves signifi-\ncant improvements in terms of average prediction accuracy (AP)\nand overlap ratio, with gains of 5.76% and 6.00%, respectively, com-\npared to the baseline method SPVP360. Furthermore, our method\nMFTR exhibits improved stability over long-term predictions with\ngains of 4.60% and 1.99% in terms of AP and AO, respectively. The\ntransferability of our model to a different dataset such as Xu-Gaze\nconfirms the effectiveness of our approach.\nComputation efficiency. In order to demonstrate the computa-\ntional effectiveness of our approach MFTR, we conducted delay\ntime experiments on GTX-3090 platform given trained model. We\nmeasured the prediction time in one batch to represent delay, where\n0 1 2 3 4 5\nPrediction Length (s)\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Average Overlap\nMFTR\nOffline-DHP\nVPT360\nSPVP360\n(a) AP comparison on PVS-HM\n0 1 2 3 4 5\nPrediction Length (s)\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Average Overlap\nMFTR\nOffline-DHP\nVPT360\nSPVP360 (b) AO comparison on PVS-HM\n0 1 2 3 4 5\nPrediction Length (s)\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Prediction Accuracy\nMFTR\nOffline-DHP\nVPT360\nSPVP360\n(c) AP comparison on Xu-Gaze\n0 1 2 3 4 5\nPrediction Length (s)\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Average Overlap\nMFTR\nOffline-DHP\nVPT360\nSPVP360 (d) AO comparison on Xu-Gaze\nFigure 3: Average precision (AP) and Average overlap (AO)\ncomparison with baseline methods on PVS-HM and XuGaze\ndataset.\nwe set the prediction length to 5 seconds and batch size to 16. We\ncompared our delay time and average prediction accuracy (AP) with\nbaseline methods on the PVS-HM dataset. Table 3 shows that our\nMFTR method achieves the best AP performance, with a delay time\nthat is only slightly higher than that of VPT360 (which considers\nonly userâ€™s historical temporal traces) and much lower than that of\nOffline-DHP and SPVP360. Our experiment on delay time serves\nas evidence of the computational efficiency and simplicity of our\nmodel, reducing the likelihood of model fitting and improving the\nefficiency of video transmission.\nTile Classification Based Viewport Prediction with\nMulti-modal Fusion Transformer MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada\nScore MapScore MapViewportViewport\nFigure 4: Visualization of score map and corresponding predicted viewport. 1st and 3rd rows depict the generated scores of each\ntile to represent how interested user is in. 2nd and 4th rows are corresponding determined viewport. We can observe that the\nselected viewports (red bounding boxes) and ground-truth viewports (yellow bounding boxes) are perfectly coincided.\nPrediction length 1s 2s 3s 4s 5s\nw/o Temporal Transformer0.741 0.689 0.573 0.504 0.436\nw/o Position Prediction Head0.890 0.822 0.771 0.673 0.639\nw/o Visual Transformer 0.869 0.803 0.751 0.671 0.627\nw/o Temporal-Visual Fusion0.904 0.847 0.779 0.684 0.633\nw/o Tile Classification Head0.881 0.795 0.755 0.702 0.642\nMFTR 0.954 0.872 0.825 0.773 0.730\nTable 4: Ablation studies of different modules.\nPrediction length 1s 2s 3s 4s 5s\n2 layers 0.811 0.766 0.681 0.601 0.514\n4 layers 0.933 0.851 0.796 0.720 0.674\n6 layers 0.954 0.872 0.825 0.773 0.730\n8 layers 0.894 0.825 0.731 0.671 0.580\nTable 5: Prediction performance on PVS-HM dataset of our\nmethod with different number of encoder layers.\nPrediction length 1s 2s 3s 4s 5s\nğ›¼\n0.25\nğ›½\n0.75 0.892 0.849 0.782 0.703 0.645\n0.30 0.70 0.912 0.854 0.811 0.748 0.673\n0.35 0.65 0.954 0.872 0.825 0.773 0.730\n0.40 0.60 0.921 0.850 0.809 0.756 0.704\n0.45 0.55 0.874 0.805 0.752 0.698 0.633\nTable 6: Prediction performance of our method on PVS-HM\ndataset with different hyper-parameters ğ›¼, ğ›½.\n4.3 Ablation Study\nTo verify the effectiveness of designs in our proposed framework,\nwe conduct a series of ablation experiments on PVS-HM dataset\nand report the results of average prediction accuracy (AP) as the\nmain indicator.\nEffects of different components. As shown in Table 4, we present\nexperimental results to measure the contributions of each compo-\nnent in MFTR. Our results indicate that the absence of the Temporal\nTransformer leads to a significant decrease in model performance,\nunderscoring the efficacy and robustness of our Temporal Trans-\nformer in capturing user motion patterns. Additionally, the removal\nof the Position Prediction Head results in a performance drop of\n7.18%, which underscores the significance of this component in\ntraining temporal feature construction, as well as the efficacy of\nutilizing Lğ‘ğ‘™ğ‘  to supervise model training.\nThe absence of the Visual Transformer results in an 8.66% de-\ncrease in the overall prediction accuracy, indicating the transformer-\ngenerated sequential visual features from 360â—¦video frames indeed\nexplore the dependencies within the visual modality and improve\nthe performance. Moreover, the Temporal-Visual Fusion Module\nimproves prediction stability by 6.14%, emphasizing its importance\nin modeling the impact of both temporal and visual modalities and\nmining inter-modality relations. Lastly, we evaluate the effective-\nness of our Tile Selection Head by comparing it with the method\nof constructing viewports through predicted head movements. The\nTile Selection Head significantly enhances viewport accuracy by\n7.58%, demonstrating the efficiency of our tile classification-based\nprediction mechanism compared to viewport prediction tasks.\nEffects of encoder layer numbers. We conducted experiments to\nevaluate the impact of the multi-head attention mechanism by vary-\ning the number of encoder layers in the transformer structures. The\nresults are presented in Table 5, where we observe that increasing\nthe number of encoder layers from 2 to 6 improves the average\nprediction accuracy (AP) by 15.62%, while further increasing the\nnumber to 8 causes a drop of 9.06% compared to 6 layers. This\nsuggests that the performance of the model is not always positively\ncorrelated with the number of encoder layers, and that a trans-\nformer with 6 encoder layers can effectively enhance single-modal\nfeatures and capture inter- and intra-modality relations.\nMM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada Zhihao Zhang et al.\nEffects of ğ›¼ and ğ›½. The hyper-parameters ğ›¼, ğ›½ in Equation 7 are\nused to balance fusion process within the temporal modality, as well\nas between different modalities. If the proportion of head prediction\nin the gradient return is reduced by settingğ›¼to a small value, it leads\nto incomplete temporal representations and ultimately reduces\nthe overall performance of our method. Likewise, if ğ›½ is set to a\nsmall value, the tile classification process may be incomplete and\nthe multi-modal fusion may not be fully achieved in depth. To\ndetermine the optimal hyper-parameter, we train our model with\nvarious combinations as shown in Table 6, ğ›¼ = 0.35 and ğ›½ = 0.65\nprovides a great balance and achieves best performance for MFTR.\nIt appears that the weight of ğ›½ is higher, because Lğ‘ğ‘™ğ‘  promotes\nglobal convergence, serves as a general measure for evaluating\nthe impact of temporal-visual features. While Lğ‘ğ‘œğ‘  is more locally,\nspecifically utilized to facilitate fusion process of temporal features.\n4.4 Visualization\nFig. 4 provides qualitative examples of tile classification results and\nthe corresponding determined viewports, allowing for a deeper in-\nsight into our method. Heatmaps of classification scoresğ‘†âˆˆRğ‘â„Ã—ğ‘ğ‘¤\nfor all tiles in each selected360â—¦video frame are drawn, and chosen\nviewport regions based on tile classification results are presented.\nOur results demonstrate that our MFTR model accurately distin-\nguishes the tiles of user interest and predicts viewports (red bound-\ning boxes) that perfectly coincide with ground-truth viewports\n(yellow bounding boxes), explicitly demonstrating the effectiveness\nof our method. Additionally, our viewport selection method based\non the score map from tile classification is more interpretable when\ncompared with trajectory-based approaches.\n5 CONCLUSION\nIn this work, we have developed a tile classification based viewport\nprediction method with Multi-modal Fusion Transformer (MFTR),\nwhich novelly transfers viewport prediction to a tile classification\ntask. MFTR first extracts long-range dependencies within the same\nmodality through transformers. Then it fuses different modalities\ninto cross-modal representation to model the combined influence\nof user historical inputs and video contents. Moreover, MFTR clas-\nsifies the future tiles into two categories: user interested tile and\nnot. The future viewport could be determined in a more robust\nmanner of selecting regions containing most tiles of user interest.\nThe experimental results indicates that our proposed method MFTR\nsignificantly surpasses the state-of-the-art methods with higher\nprediction accuracy and competitive computation efficiency.\nREFERENCES\n[1] A Deniz Aladagli, Erhan Ekmekcioglu, Dmitri Jarnikov, and Ahmet Kondoz. 2017.\nPredicting head trajectories in 360 virtual reality videos. In 2017 International\nConference on 3D Immersion (IC3D) . IEEE, 1â€“6.\n[2] Yixuan Ban, Lan Xie, Zhimin Xu, Xinggong Zhang, Zongming Guo, and Yue\nWang. 2018. Cub360: Exploiting cross-users behaviors for viewport prediction in\n360 video adaptive streaming. In2018 IEEE International Conference on Multimedia\nand Expo (ICME) . IEEE, 1â€“6.\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexan-\nder Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection with\ntransformers. In European conference on computer vision . Springer, 213â€“229.\n[4] Fang-Yi Chao, Cagri Ozcinar, and Aljosa Smolic. 2021. Transformer-based Long-\nTerm Viewport Prediction in 360Â° Video: Scanpath is All You Need.. In MMSP.\n1â€“6.\n[5] Jinyu Chen, Xianzhuo Luo, Miao Hu, Di Wu, and Yipeng Zhou. 2020. Sparkle:\nUser-aware viewport prediction in 360-degree video streaming.IEEE Transactions\non Multimedia 23 (2020), 3853â€“3866.\n[6] Federico Chiariotti. 2021. A survey on 360-degree video: Coding, quality of\nexperience and streaming. Computer Communications 177 (2021), 133â€“155.\n[7] Lovish Chopra, Sarthak Chakraborty, Abhijit Mondal, and Sandip Chakraborty.\n2021. Parima: Viewport adaptive 360-degree video streaming. In Proceedings of\nthe Web Conference 2021 . 2379â€“2391.\n[8] Yago Sanchez de la Fuente, Gurdeep Singh Bhullar, Robert Skupin, Cornelius\nHellge, and Thomas Schierl. 2019. Delay impact on MPEG OMAFâ€™s tile-based\nviewport-dependent 360 video streaming. IEEE Journal on Emerging and Selected\nTopics in Circuits and Systems 9, 1 (2019), 18â€“28.\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding.arXiv\npreprint arXiv:1810.04805 (2018).\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).\n[11] Xianglong Feng, Viswanathan Swaminathan, and Sheng Wei. 2019. Viewport\nprediction for live 360-degree mobile video streaming using user-content hybrid\nmotion tracking. Proceedings of the ACM on Interactive, Mobile, Wearable and\nUbiquitous Technologies 3, 2 (2019), 1â€“22.\n[12] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua\nLiu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, et al. 2022. A survey on vision\ntransformer. IEEE transactions on pattern analysis and machine intelligence 45, 1\n(2022), 87â€“110.\n[13] Mohammadreza Jamali, StÃ©phane Coulombe, Ahmad Vakili, and Carlos Vazquez.\n2020. LSTM-based viewpoint prediction for multi-quality tiled video coding in\nvirtual reality streaming. In 2020 IEEE International Symposium on Circuits and\nSystems (ISCAS) . IEEE, 1â€“5.\n[14] Xiaolan Jiang, Si Ahmed Naas, Yi-Han Chiang, Stephan Sigg, and Yusheng Ji.\n2020. SVP: Sinusoidal viewport prediction for 360-degree video streaming. IEEE\nAccess 8 (2020), 164471â€“164481.\n[15] Yuang Jiang, Konstantinos Poularakis, Diego Kiedanski, Sastry Kompella, and\nLeandros Tassiulas. 2022. Robust and Resource-efficient Machine Learning Aided\nViewport Prediction in Virtual Reality. arXiv preprint arXiv:2212.09945 (2022).\n[16] Yue Kang, Zhao Cai, Chee-Wee Tan, Qian Huang, and Hefu Liu. 2020. Natural\nlanguage processing (NLP) in management research: A literature review. Journal\nof Management Analytics 7, 2 (2020), 139â€“172.\n[17] Wonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt: Vision-and-language trans-\nformer without convolution or region supervision. In International Conference on\nMachine Learning . PMLR, 5583â€“5594.\n[18] Dongwon Lee, Minji Choi, and Joohyun Lee. 2021. Prediction of head movement\nin 360-degree videos using attention model. Sensors 21, 11 (2021), 3678.\n[19] Chenge Li, Weixi Zhang, Yong Liu, and Yao Wang. 2019. Very long term field\nof view prediction for 360-degree video streaming. In 2019 IEEE Conference on\nMultimedia Information Processing and Retrieval (MIPR) . IEEE, 297â€“302.\n[20] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang. 2020. Unicoder-\nvl: A universal encoder for vision and language by cross-modal pre-training. In\nProceedings of the AAAI Conference on Artificial Intelligence , Vol. 34. 11336â€“11344.\n[21] Jie Li, Ling Han, Chong Zhang, Qiyue Li, and Zhi Liu. 2022. Spherical Convolu-\ntion empowered Viewport Prediction in 360 Video Multicast with Limited FoV\nFeedback. ACM Transactions on Multimedia Computing, Communications, and\nApplications (TOMM) (2022).\n[22] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,\nand Baining Guo. 2021. Swin transformer: Hierarchical vision transformer us-\ning shifted windows. In Proceedings of the IEEE/CVF international conference on\ncomputer vision . 10012â€“10022.\n[23] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han\nHu. 2022. Video swin transformer. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition . 3202â€“3211.\n[24] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. Vilbert: Pretrain-\ning task-agnostic visiolinguistic representations for vision-and-language tasks.\nAdvances in neural information processing systems 32 (2019).\n[25] Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee. 2020.\n12-in-1: Multi-task vision and language representation learning. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 10437â€“\n10446.\n[26] Afshin Taghavi Nasrabadi, Aliehsan Samiei, and Ravi Prakash. 2020. Viewport\nprediction for 360 videos: a clustering approach. In Proceedings of the 30th ACM\nWorkshop on Network and Operating Systems Support for Digital Audio and Video .\n34â€“39.\n[27] Anh Nguyen, Zhisheng Yan, and Klara Nahrstedt. 2018. Your attention is unique:\nDetecting 360-degree video saliency in head-mounted display for head movement\nprediction. In Proceedings of the 26th ACM international conference on Multimedia .\n1190â€“1198.\n[28] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al . 2019. Exploring the\nTile Classification Based Viewport Prediction with\nMulti-modal Fusion Transformer MM â€™23, October 29-November 3, 2023, Ottawa, ON, Canada\nlimits of transfer learning with a unified text-to-text transformer. arXiv preprint\narXiv:1910.10683 (2019).\n[29] Miguel Fabian Romero Rondon, Lucile Sassatelli, Ramon Aparicio Pardo, and\nFrederic Precioso. 2019. Revisiting Deep Architectures for Head Motion Prediction\nin 360 {\\deg}Videos. arXiv preprint arXiv:1911.11702 (2019).\n[30] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-\nChieh Chen. 2018. Mobilenetv2: Inverted residuals and linear bottlenecks. In\nProceedings of the IEEE conference on computer vision and pattern recognition .\n4510â€“4520.\n[31] Sam Van Damme, Maria Torres Vega, and Filip De Turck. 2022. Machine Learning\nbased Content-Agnostic Viewport Prediction for 360-Degree Video. ACM Trans-\nactions on Multimedia Computing, Communications, and Applications (TOMM) 18,\n2 (2022), 1â€“24.\n[32] Jeroen van der Hooft, Maria Torres Vega, Stefano Petrangeli, Tim Wauters, and\nFilip De Turck. 2019. Optimizing adaptive tile-based virtual reality video stream-\ning. In 2019 IFIP/IEEE Symposium on Integrated Network and Service Management\n(IM). IEEE, 381â€“387.\n[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Å ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Processing Systems , Vol. 30.\n[34] Lan Xie, Zhimin Xu, Yixuan Ban, Xinggong Zhang, and Zongming Guo. 2017.\n360probdash: Improving qoe of 360 video streaming using tile-based http adaptive\nstreaming. In Proceedings of the 25th ACM international conference on Multimedia .\n315â€“323.\n[35] Mai Xu, Yuhang Song, Jianyi Wang, MingLang Qiao, Liangyu Huo, and Zulin\nWang. 2018. Predicting head movement in panoramic video: A deep reinforcement\nlearning approach. IEEE transactions on pattern analysis and machine intelligence\n41, 11 (2018), 2693â€“2708.\n[36] Tan Xu, Bo Han, and Feng Qian. 2019. Analyzing viewport prediction under\ndifferent VR interactions. In Proceedings of the 15th International Conference on\nEmerging Networking Experiments And Technologies . 165â€“171.\n[37] Yanyu Xu, Yanbing Dong, Junru Wu, Zhengzhong Sun, Zhiru Shi, Jingyi Yu,\nand Shenghua Gao. 2018. Gaze prediction in dynamic 360 immersive videos. In\nproceedings of the IEEE Conference on Computer Vision and Pattern Recognition .\n5333â€“5342.\n[38] Qin Yang, Junni Zou, Kexin Tang, Chenglin Li, and Hongkai Xiong. 2019. Single\nand sequential viewports prediction for 360-degree video streaming. In2019 IEEE\nInternational Symposium on Circuits and Systems (ISCAS) . IEEE, 1â€“5.\n[39] Abid Yaqoob, Ting Bi, and Gabriel-Miro Muntean. 2020. A survey on adaptive 360\nvideo streaming: Solutions, challenges and opportunities. IEEE Communications\nSurveys & Tutorials 22, 4 (2020), 2801â€“2838.\n[40] Abid Yaqoob and Gabriel-Miro Muntean. 2021. A combined field-of-view\nprediction-assisted viewport adaptive delivery scheme for 360 Â° videos. IEEE\nTransactions on Broadcasting 67, 3 (2021), 746â€“760.\n[41] Lei Zhang, Weizhen Xu, Donghuan Lu, Laizhong Cui, and Jiangchuan Liu. 2022.\nMFVP: Mobile-Friendly Viewport Prediction for Live 360-Degree Video Stream-\ning. In 2022 IEEE International Conference on Multimedia and Expo (ICME) . IEEE,\n1â€“6.\n[42] Zhihao Zhang, Haipeng Du, Shouqin Huang, Weizhan Zhang, and Qinghua\nZheng. 2022. VRFormer: 360-Degree Video Streaming with FoV Com-\nbined Prediction and Super resolution. In 2022 IEEE Intl Conf on Parallel\n& Distributed Processing with Applications, Big Data & Cloud Computing,\nSustainable Computing & Communications, Social Computing & Networking\n(ISPA/BDCloud/SocialCom/SustainCom). IEEE, 531â€“538.\n[43] Yucheng Zhu, Guangtao Zhai, Xiongkuo Min, and Jiantao Zhou. 2019. The\nprediction of saliency map for head and eye movements in 360 degree images.\nIEEE Transactions on Multimedia 22, 9 (2019), 2331â€“2344.",
  "topic": "Viewport",
  "concepts": [
    {
      "name": "Viewport",
      "score": 0.9572105407714844
    },
    {
      "name": "Computer science",
      "score": 0.782799482345581
    },
    {
      "name": "Tile",
      "score": 0.6175022721290588
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5456829071044922
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.5369408130645752
    },
    {
      "name": "Modal",
      "score": 0.5259487628936768
    },
    {
      "name": "Machine learning",
      "score": 0.3632051944732666
    },
    {
      "name": "Data mining",
      "score": 0.33809399604797363
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "cited_by": 8
}