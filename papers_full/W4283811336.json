{
  "title": "Width &amp; Depth Pruning for Vision Transformers",
  "url": "https://openalex.org/W4283811336",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1518878301",
      "name": "Fang Yu",
      "affiliations": [
        "Institute of Computing Technology",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2099535595",
      "name": "Kun Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102035757",
      "name": "Meng Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102674343",
      "name": "Yuan Cheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097149163",
      "name": "Wei Chu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1975028629",
      "name": "Li Cui",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1518878301",
      "name": "Fang Yu",
      "affiliations": [
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences",
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2099535595",
      "name": "Kun Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102035757",
      "name": "Meng Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102674343",
      "name": "Yuan Cheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097149163",
      "name": "Wei Chu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1975028629",
      "name": "Li Cui",
      "affiliations": [
        "Institute of Computing Technology",
        "Chinese Academy of Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6778485988",
    "https://openalex.org/W2993466051",
    "https://openalex.org/W3034742519",
    "https://openalex.org/W2748428003",
    "https://openalex.org/W2610140147",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W6786708909",
    "https://openalex.org/W3017722917",
    "https://openalex.org/W2944223741",
    "https://openalex.org/W6600281463",
    "https://openalex.org/W6687888618",
    "https://openalex.org/W3033737024",
    "https://openalex.org/W6800633732",
    "https://openalex.org/W2204750386",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W4295308583",
    "https://openalex.org/W4287901267",
    "https://openalex.org/W2949227999",
    "https://openalex.org/W4312560592",
    "https://openalex.org/W3101163004",
    "https://openalex.org/W3035038672",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W3035422918",
    "https://openalex.org/W3094522247",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3034234149",
    "https://openalex.org/W3159778524",
    "https://openalex.org/W3190277406",
    "https://openalex.org/W2975381464",
    "https://openalex.org/W3210279979",
    "https://openalex.org/W3168124404",
    "https://openalex.org/W3194959296",
    "https://openalex.org/W3138796575",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W4312340826",
    "https://openalex.org/W3172288629",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2962677625",
    "https://openalex.org/W3035682985",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2962851801",
    "https://openalex.org/W3153842237",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W3171516518",
    "https://openalex.org/W4288347855",
    "https://openalex.org/W4287330514",
    "https://openalex.org/W3094502228"
  ],
  "abstract": "Transformer models have demonstrated their promising potential and achieved excellent performance on a series of computer vision tasks. However, the huge computational cost of vision transformers hinders their deployment and application to edge devices. Recent works have proposed to Ô¨Ånd and remove the unimportant units of vision transformers. Despite achieving remarkable results, these methods take one dimension of network width into consideration and ignore network depth, which is another important dimension for pruning vision transformers. Therefore, we propose a Width &amp; Depth Pruning (WDPruning) framework that reduces both width and depth dimensions simultaneously. SpeciÔ¨Åcally, for width pruning, a set of learnable pruning-related parameters is used to adaptively adjust the width of transformer. For depth pruning, we introduce several shallow classiÔ¨Åers by using the intermediate information of the transformer blocks, which allows images to be classiÔ¨Åed by shallow classiÔ¨Åers instead of the deeper classiÔ¨Åers. In the inference period, all of the blocks after shallow classiÔ¨Åers can be dropped so they don‚Äôt bring additional parameters and computation. Experimental results on benchmark datasets demonstrate that the proposed method can signiÔ¨Åcantly reduce the computational costs of mainstream vision transformers such as DeiT and Swin Transformer with a minor accuracy drop. In particular, on ILSVRC-12, we achieve over 22% pruning ratio of FLOPs by compressing DeiT-Base, even with an increase of 0.14% Top-1 accuracy.",
  "full_text": "Width & Depth Pruning for Vision Transformers\nFang Yu1,2, Kun Huang3, Meng Wang3, Yuan Cheng3*, Wei Chu3, Li Cui1‚àó\n1Institute of Computing Technology, Chinese Academy of Sciences\n2University of Chinese Academy of Science\n3Ant Financial Services Group\n{yufang,lcui}@ict.ac.cn, {hunterkun.hk,darren.wm,chengyuan.c,weichu.cw}@antgroup.com\nAbstract\nTransformer models have demonstrated their promising po-\ntential and achieved excellent performance on a series of\ncomputer vision tasks. However, the huge computational cost\nof vision transformers hinders their deployment and appli-\ncation to edge devices. Recent works have proposed to find\nand remove the unimportant units of vision transformers. De-\nspite achieving remarkable results, these methods take one di-\nmension of network width into consideration and ignore net-\nwork depth, which is another important dimension for prun-\ning vision transformers. Therefore, we propose a Width &\nDepth Pruning (WDPruning) framework that reduces both\nwidth and depth dimensions simultaneously. Specifically, for\nwidth pruning, a set of learnable pruning-related parameters\nis used to adaptively adjust the width of transformer. For\ndepth pruning, we introduce several shallow classifiers by us-\ning the intermediate information of the transformer blocks,\nwhich allows images to be classified by shallow classifiers\ninstead of the deeper classifiers. In the inference period, all\nof the blocks after shallow classifiers can be dropped so they\ndon‚Äôt bring additional parameters and computation. Exper-\nimental results on benchmark datasets demonstrate that the\nproposed method can significantly reduce the computational\ncosts of mainstream vision transformers such as DeiT and\nSwin Transformer with a minor accuracy drop. In particular,\non ILSVRC-12, we achieve over 22% pruning ratio of FLOPs\nby compressing DeiT-Base, even with an increase of 0.14%\nTop-1 accuracy.\nIntroduction\nThe transformer architecture has been widely adopted in nat-\nural language processing (NLP) tasks and obtained supe-\nrior results. Recently, Vision Transformer (ViT) (Dosovit-\nskiy et al. 2020) and its follow-ups have demonstrated the\nstate-of-the-art results in image classification (Jiang et al.\n2021; Meng 2021; Hassani et al. 2021), object detection\n(Carion et al. 2020; Zhu et al. 2020) and instance segmen-\ntation (Wang et al. 2021). However, the transformer variants\nrequire intensive computation resources, run-time memory\nand storage requirements, hindering their practical applica-\ntions on the edge devices with limited storage and computa-\ntion resources.\n*Corresponding author\nCopyright ¬© 2022, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nThere are emerging efforts to improve the efficiency of\ntransformers. A common approach to compress transform-\ners is known as weight pruning (Han et al. 2015). Though\ntheoretically plausible, the weight pruning results in unstruc-\ntured sparse matrices that are difficult to support on com-\nmon hardware, making it challenging to obtain inference\nspeedups despite a significant reduction in model size. In\ncontrast, another common practice for efficient transformers\nis to prune the structures that are of less importance, such\nas rows of weight matrix, token embeddings or whole self-\nattentions matrices. The model pruned by structured pruning\nis hardware-friendly and can be well supported by various\noff-the-shelf computing platforms.\nIn this context, some structured pruning methods have\nbeen proposed for transformer-like models. Designed for\nNLP tasks, PoWER (Goyal et al. 2020) progressively elim-\ninates the word tokens during the forward pass and acceler-\nates the BERT models (Devlin et al. 2018). Yao et al. pro-\nposed MLPruning (Yao et al. 2021), a multilevel pruning\nframework, to prune the structure of BERT-related trans-\nformer. Michel et al. proposed to prune self-attention head\n(Michel, Levy, and Neubig 2019) and observe that pruning\na large percentages of attention heads in BERT model has\nlittle impact on the performance. However, with the emer-\ngence of vision transformers variants such as Swin Trans-\nformers (Liu et al. 2021a) and VOLO (Yuan et al. 2021),\nsome novel attention mechanisms (e.g., shift-window atten-\ntion, outlook attention) are proposed. The previous pruning\nmethods are not to be performed in such variant structures.\nDesigned for vision transformers, VTP (Zhu et al. 2021) ex-\ntends network slimming (Liu et al. 2017) to reduce the out-\nput dimension of the linear matrix by training, pruning and\nfine-tuning. Specifically, the vision transformers need to be\nlearned by imposing the structured sparsity regularization on\nsoft mask scores. After training, VTP discards the small soft\nmask scores with values below a manually chosen thresh-\nold, and fine-tunes the injured models. In addition, the pre-\nvious practices for pruning transformers take one dimension\nof network width (e.g., linear matrix) into consideration and\nignore network depth (e.g., transformer block), which is an-\nother important dimension for pruning model. In practice,\nwe found that depth pruning could achieve larger parallelism\nefficiency than width pruning under the same pruning rate\nbecause of the less synchronization and fragmentation.\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n3143\nLinear *ùë¥ùë¥\nScaled Dot-Product Attention\nLinear*ùëÄùëÄ1\nX X X\nH\nQ K V\n o2\n o3\n o6\n o7\n o8\n o9\nsaliency score saliency score\nWproj , Wfc1 , Wfc2 Wqkv\nqkv1\n qkv2\n qkv3\n(b) Width Pruning for Linear Matrix\n o1\n o4\n o5\n(c) Width Pruning for attention head(a) Masked Operation\nlearnable threshold learnable threshold\nLinear*ùëÄùëÄ1 Linear*ùëÄùëÄ1\nFigure 1: An illustration of width pruning. (a) The mask operation in multi-head self-attention. (b) Width pruning for linear\nmatrices (Wproj, Wfc1 , Wfc2 ). (c) Width pruning for attention heads (Wqkv). Blue colored cells represent the pruned weights.\nIn this work, we propose a width & depth pruning frame-\nwork (WDPruning) that directly learns a compact vision\ntransformer from scratch without fine-tuning. For reducing\nwidth, we use the learnable saliency score and score thresh-\nold to prune the multi-head self-attention (MSA) and lin-\near matrix. To meet the predefined pruning ratio, the score\nthreshold dynamically updates the pruning ratio of differ-\nent layers by Augmented Lagrangian method during train-\ning. For reducing depth, we introduce several shallow clas-\nsifiers by using the intermediate information of the trans-\nformer block, which allows images to be classified by shal-\nlow classifiers instead of the deeper classifiers. In the infer-\nence period, all of the blocks after shallow classifiers can be\ndropped. Our contributions are summarized:\n‚Ä¢ We propose a parameter-learnable method for width\npruning via learning the saliency score and per-layer\nthreshold, thereby obtaining a better non-uniform spar-\nsity budget across layers.\n‚Ä¢ We propose to use additional plug-in classifiers to prune\nthe transformer blocks in the tail of the transformer. It can\nconstruct a sequential version of pruned model for drop-\nping blocks in one training pass, which have an easy con-\ntrol of the trade-off between network performance and\npruning rate.\n‚Ä¢ Extensive experiments demonstrate the effectiveness and\nefficiency of our method. In particular, we improve the\nthroughput by over 15% while the drop of accuracy is\nwithin 1% for DeiT-Base.\nRelated Works\nVision transformers. The powerful multi-head self-\nattention mechanism (Vaswani et al. 2017) has motivated\nthe research of studying transformers on a variety of vision\ntasks. Carion et al. proposed DETR (Carion et al. 2020) to\nsolve object detection prediction by transformer model in\nan end-to-end manner, which is also the first work to ap-\nply transformer to vision tasks. Vision Transformer (ViT)\n(Dosovitskiy et al. 2020) applied the transformer encoder\nto realize image classification with non-overlapping image\npatches. DeiT (Touvron et al. 2021) is proposed to improve\nits performance by hard distillation. In (Jiang et al. 2021),\nLV-ViT introduced the extra labels of patch tokens and fur-\nther improved the performance of ViT. Liu et al. proposed\na high-efficient Swin Transformer (Liu et al. 2021a) which\nis compatible with a broad range of vision tasks. Yuan et\nal. proposed Vision Outlooker (Yuan et al. 2021) which en-\ncode the fine-level features into the token representations\nand achieved an accuracy of 87% on ILSVRC-12. Although\nboth ViT and its follow-ups achieved in state-of-the-art per-\nformance in classification, their huge computation cost and\nmodel size hinder their practical applications.\nStructured pruning for transformers. In NLP tasks,\nsome works have proposed some structured pruning meth-\nods for transformer-like models. Michel et al. focus on re-\nducing the number of heads in Multi-head self-attention\nmodule (Michel, Levy, and Neubig 2019) and accelerate the\nBERT models (Devlin et al. 2018). Fan et al. proposed Lay-\nerDrop method (Fan, Grave, and Joulin 2019) to randomly\ndrop out some layers in the training process, and predict\nwith some missing layers. However, all these works focus\non pruning transformers in NLP tasks, which is unable to di-\nrectly support the visual transformers and variant structures,\nsuch as shift-window attention (Liu et al. 2021a), outlook\nattention (Yuan et al. 2021). Designed for vision transform-\ners, Patch Slimming (Tang et al. 2021) is proposed to discard\nuseless patch tokens by minimizing reconstruction error of\npatch token. Similarly, DynamicViT (Rao et al. 2021) also\nprunes the less informative tokens by training a prediction\nmodule. However, although it can reduce the computation\namount of vision transformers, the extra prediction mod-\nule is time-consuming for transformer inference. VTP (Zhu\net al. 2021) introduces learnable coefficients to evaluate the\nimportance of output embedding dimensions of linear matrix\nin training process, and the neurons with small coefficients\nare removed according to a threshold. However, VTP needs\nto manually try thresholds for all layers, and needs to fine-\ntune the models after pruning. Different from the previous\nworks, we propose to prune vision transformers from wide\nand depth dimensions, which is orthogonal with other prun-\ning approaches. For example, it can be combined with patch\npruning to realize higher compression and acceleration rate.\n3144\nMethodology\nFormulation\nOur method WDPruning aims to prune the entire regions of\npruning units in vision transformers (e.g., matrices in lin-\near layers, attention heads in MSA and transformer blocks).\nThis process can be generalized in the form of layer-wise\npruning ratios like (r1, r2, ..., rL) for a vision transformer A\nwith L blocks:\n(r1, r2, ..., rL)‚àó = arg minL(A(r1, r2, ..., rL)),\ns.t. P(A(r1, r2, ..., rL)‚àó) < C (1)\nwhere Lis loss function and rl is a pruning ratio applied\nto the l-th block. rl ‚àà [0, 1) means width pruning and\nrl = 1 means depth pruning for transformer blocks. P(¬∑)\nis an evaluation metric, where the parameters and compu-\ntation cost of pruned transformer should follow predefined\nconstraints Csuch as inference latency, floating-point oper-\nations (FLOPs).\nWidth Pruning\nTo shrink the width of vision transformer, we add a binary\nmask variable and its saliency score for linear or attention\nlayer, which is of the same size and shape as the layer‚Äôs\nweight tensor. The mask determines which of the weights\nparticipate in the forward pass. In the pruning stage, we ze-\nroize the weights with score values below learnable thresh-\nolds until a predefined compression ratio is reached. Figure 1\nillustrates this process. We give quantitative analysis later in\nthis section to demonstrate the learning process of saliency\nscores and thresholds.\nLearnable saliency score We first consider how to prune\nweight matrices in linear layer. Let us denote the weight ma-\ntrix by W ‚àà Rm√ón and its mask by M ‚àà Rm√ón with\nm and n being the number of output and input dimension.\nHere each element in M is a binary matrix {0, 1}indicat-\ning corresponding output in linear matrix is pruned or not.\nNote that the input in linear matrix needs to be removed if\nits corresponding mask value is 0 in the previous layer (see\nAppendix A for details). The binary mask M can be deter-\nmined by Eq. (2):\nMj =\n\u001a0 Sj ‚â§Œ¶(S|m √ór)\n1 otherwise (2)\nwhere S is the saliency score of M, and Œ¶(z|k) is a func-\ntion, which returns the value of the k smallest entries in z.\nThe binary mask M means zeroing out entries in W with its\nsaliency score smaller than Œ¶(S|m √ór) in absolute magni-\ntude. In next subsection, we provide a detailed explanation\nof how we get the layer-wise pruning ratio r according to a\nlearnable threshold parameter.\nWe perform element-wise (Hadamard) product between\nlinear matrix and mask in the forward pass. The j-th output\nof linear layer is as follows:\naj =\nmX\nk=1\nMj,kWj,kxk. (3)\nFor the backward pass, since the gradient of binary step\nfunction of mask is 0 everywhere it is defined, the mask\ncan be seemed as straight-through estimatorwhere the tech-\nnique proposed by (Ramanujan et al. 2020). Thus the gradi-\nent of mask ‚àÇaj\n‚àÇMj\ncan be ignored in the backward pass. By\nthe chain rule, the gradient of loss Lw.r.t. saliency score Sj\nis calculated as follows, which indicates the learning process\nof saliency score in linear matrix:\n‚àÇL\n‚àÇSj\n= ‚àÇL\n‚àÇaj\n¬∑ ‚àÇaj\n‚àÇMj\n¬∑‚àÇMj\n‚àÇSj\n= ‚àÇL\n‚àÇaj\nmX\nk=1\nWj,kxj. (4)\nWe apply a similar idea to the self attention layer. The\nmasked MSA is formulated as:\nMSA(x) = Wproj\nHX\nh=1\nMhAttnh(x), (5)\nwhere H is the head number of self attention, Attn is self\nattention mechanism, and Wproj is a linear transformation\nmatrix. The self attention consists of query Wq ‚àà Rn√ód,\nkey Wk ‚ààRn√ód and value Wv ‚ààRn√ód, where these three\nmatrices form a qkv matrix Wqkv ‚ààRn√ó3d. In particular,\nthe computation of self attention is:\nAttnh(x) = Œ±hWh,vx,\nwhere Œ±h =softmax(xTWT\nh,qWh,kx‚àö\nd\n).\n(6)\nIn Eq. (6), Œ±h is the h-th attention map. A mask Mh ‚àà\nR1√ó3d needs to cover the whole attention head (qkvmatrix).\nBy the chain rule, we obtain the gradient of self-attention\nwith respect to the saliency score Sh ‚ààR1:\n‚àÇL\n‚àÇSh\n= ‚àÇL\n‚àÇMSA ¬∑‚àÇMSA\n‚àÇMh\n¬∑‚àÇMh\n‚àÇSh\n= ‚àÇL\n‚àÇMSAAttnh(x).\n(7)\nBased on Eq. (4) and Eq. (7), we update the saliency\nscores over dataset by AdamW optimizer with a learning\nrate Œ±s. The magnitude of saliency score illustrates the prod-\nuct accumulation of activation and gradient of activation\nw.r.t. a pruning unit. The units that obtain larger saliency\nscores learn the more important inductive bias customized to\nthe learning task, thus being retained in the pruning process.\nIn Appendix B, we prove the stable convergence of pruning\nby saliency score in the optimization.\nLearnable threshold Learning a mask over the pruning\nunit also presents problems, namely the difficulty of con-\nverting this mask into binary decisions which would require\ncarefully handcrafted thresholds. We thus propose a differ-\nentiable thresholding operation, making it adjustable in a su-\npervised learning framework. Inspired by MLPruning (Yao\net al. 2021), we use sigmoid functionœÉ to generate a pruning\nratio rl for each layer l by bounding the threshold parameter\nŒ≤l within (0, 1):\nrl = œÉ(Œ≤l). (8)\nTo learn the layer-wise pruning ratio and control the size\nof the pruned model, we utilize an Augmented Lagrangian\n3145\nClassifier 2\nTransformer Block 1\nclass tokenpatch tokens\nPrune\n...\nTransformer Block 2\nTransformer Block L\nClassifier 1\nClassifier L\nClassifier 2\nTransformer Block 1\nclass tokenpatch tokens\nTransformer Block 2\nFigure 2: An illustration of depth pruning. The additional\nclassifiers (denoted by blue cells) are connected to the\nclass tokens of intermediate embeddings, respectively. The\nplugged-in classifiers are trained together with the whole\ntransformer. After training, we choose a classifier as final\noutput, and remove the all subsequent transformer blocks,\nwhich reduce the depth of transformer.\nmethod (Bastings, Aziz, and Titov 2019) to construct a reg-\nularization on the threshold parameter. Let R be the current\npruning ratio of parameters and Rt be the target pruning ra-\ntio. The Augmented Lagrangian method imposes an equality\nconstraint R = Rt by introducing a violation penalty:\nLp =\n\u001aŒª1(Rt ‚àíR) + Œª2(Rt ‚àíR)2, R t > R,\n0, otherwise, (9)\nwhere Œª1, Œª2 ‚ààR1 are two Lagrangian multipliers. The R\nis accumulated by all pruned layer:\nR =\nLX\nl=1\nrl ‚àónl\nN (10)\nwhere nl is the number of parameters in the l-th layer, N\nis the total number of parameters in vision transformer. The\ngradient of Lp w.r.t. Œ≤l in the l-th layer is:\n‚àÇLp\n‚àÇŒ≤l\n= nlœÉ(Œ≤l)(1 ‚àíœÉ(Œ≤l))\nN (‚àíŒª1 + 2Œª2(Rt ‚àíR)). (11)\nIt can be learned from Eq. (11) that the magnitude of gra-\ndient is larger when some layer have the larger nl, which\nit is more inclined to be pruned. Moreover, the gradient of\nthreshold parameter is very small in the early stage because\nof the sigmoid function. Hence, the scalar coefficient Œª1, Œª2\nneeds to be carefully tuned for both initial values and sched-\nules. To resolve this issue, we put Œª1, Œª2 in the training pro-\ncess, which are jointly optimized with the network weights\nand facilitate the learning process of score threshold.\nDepth Pruning\nIn addition to pruning the width, WDPruning further reduces\nthe blocks of visual transformers. In Vision Transformer\n(Dosovitskiy et al. 2020), the patch tokens and class token\nare interacted with each other through transformer blocks.\nThe class tokens in the early stages of the transformer have\nAlgorithm 1: Width & Depth Pruning Framework\nInput: Transformer W; Dataset D; Pruning ratio Rt;\nPlug-in classifiers C; Epoch E;\n1 Initialize e = 0, Œª1 = 0, Œª2 = 0, score S = 0 and\nthreshold parameter Œ≤i = 10 for each layer i;\n2 while e < E do\n3 for each iteration‚ààepoch edo\n/* Forward pass */\n4 Obtain the threshold of each layer by œÉ(Œ≤);\n5 Obtain the masks via Eq. (2);\n6 Obtain the masked weight matrices via\nEq. (3) and Eq. (5);\n7 Count the current pruning ratio R;\n8 Compute the loss function via Eq. (12);\n/* Backward pass */\n9 Update W and plug-in classifiers C;\n10 Update saliency scores by Eq. (4) and Eq. (7);\n11 Update threshold parameters by Eq. (11);\n12 Update Œª1 and Œª2;\n13 end\n14 end\n/* Width pruning */\n15 Prune the attention heads and linear matrices whose\nmasks are 0;\n/* Depth pruning */\n16 Choose an optimal classifier as transformer output,\nand remove all subsequent transformer blocks;\nthe ability of accurate classification. Motivated by this idea,\nwe further prune the redundant transformer blocks in the tail,\nas shown in Fig. 2. Concretely, we connect additional clas-\nsifiers to class tokens of several selected embeddings. The\nplugged-in classifiers are trained together with the process\nof width pruning. Thus, the whole loss function of training\nobjective is:\nL= LCE + Lp +\nKX\ni=1\nLci , (12)\nwhere LCE is cross-entropy loss of the last classifier, Lc\nis the cross-entropy loss of plugged-in classifier head and\nK is the total number of plug-in classifiers. After training,\nwe obtain a transformer model with multiple depth versions\nsince each classifier has its own prediction ability to a\ncertain degree. We select a shallow classifier to achieve\nan optimal trade-off between efficiency and accuracy by\nevaluating on the validation set. In the inference period, all\nof the blocks after the selected classifiers are dropped. This\nshallower pruned transformer merits in both speedup and\naccuracy.\nA similar scheme is BranchyNet (Teerapittayanon,\nMcDanel, and Kung 2016) which proposed an early-exit\nmechanism (Zhou et al. 2020; Xin et al. 2020) to exit the\ninference process in the early stage of model based on a\nhigher entropy of output softmax than a threshold. However,\nthe additional control logic of BranchyNet is hard to make\n3146\nIdx Model width-depth Top-1 (%) Top-5 (%) FLOPs (G) #Param. (M) Throughput (im/s) CPU latency (ms)\n1\nDeiT-Tiny\n0.0-12 (Base) 72.20 91.10 1.3 5.4 3403 332\n2 0.3-12 71.10 (-1.10) 90.09 (-1.01) 0.9 (-30.8%) 3.8 (-29.6%) 3780 (+11.1%) 285 (-14.1%)\n3 0.3-11 70.34 (-1.86) 89.82 (-1.28) 0.7 (-46.2%) 3.5 (-35.2%) 4082 (+20.0%) 265 (-20.2%)\n4\nDeiT-Small\n0.0-12 (Base) 79.80 95.00 4.6 21.3 1547 410\n5 0.3-12 78.55 (-1.25) 94.37 (-0.63) 3.1 (-32.6%) 15.0 (-29.6%) 1712 (+10.7%) 364 (-11.2%)\n6 0.3-11 78.38 (-1.42) 94.05 (-0.95) 2.6 (-43.5%) 13.3 (-37.6%) 1830 (+18.3%) 338 (-17.5%)\n7\nDeiT-Base\n0.0-12 (Base) 81.80 95.59 17.5 85.1 598 783\n8 0.2-12 81.94 (+0.14) 95.62 (+0.03) 13.6 (-22.3%) 68.2 (-19.9%) 655 (+9.5%) 692 (-11.6%)\n9 0.2-10 80.85 (-0.95) 95.39 (-0.20) 10.8 (-38.3%) 59.4 (-30.2%) 696 (+16.4%) 639 (-18.4%)\n10 0.3-12 81.09 (-0.71) 95.54 (-0.05) 11.0 (-37.1%) 60.6 (-28.8%) 669 (+11.9%) 648 (-17.2%)\n11 0.3-11 80.76 (-1.04) 95.36 (-0.23) 9.90 (-43.4%) 55.3 (-35.0%) 707 (+18.2%) 590 (-24.6%)\n12 0.0-24 (Base) 83.00 96.50 8.7 47.3 609 847\n13 Swin Trans- 0.1-24 82.41 (-0.59) 96.21 (-0.29) 7.6 (-12.6%) 42.7 (-9.7%) 640 (+5.1%) 809 (-4.5%)\n14 Small 0.2-24 82.20 (-0.80) 96.12 (-0.38) 6.8 (-21.8%) 37.4 (-20.9%) 667 (+9.5%) 776 (-8.4%)\n15 0.2-22 81.80 (-1.20) 96.01 (-0.49) 6.3 (-27.6%) 32.8 (-30.6%) 702 (+15.3%) 763 (-9.9%)\nTable 1: Main results on ILSVRC-12. We count the FLOPs, parameters and inference throughput (images per second), latency\nfor 4 representative vision transformers. ‚Äúwidth-depth‚Äù denotes the predefined pruning ratio for width and depth pruning, and\nwe choose the classifier at this depth and prune the subsequent blocks. ‚ÄúBase‚Äù denotes the unpruned baseline model.\nthe just-in-time deployment, e.g. TensorRT 1. In contrast,\nour depth pruning can obtain an end-to-end pruned model,\nwhich is easy to be converted to ONNX 2 format for edge\ndeployment.\nComputational Complexity Analysis\nIn our method, additional parameters and computation in\ntraining process mainly comes from element-wise product\nbetween weights and masks, as well as the pruning-related\nparameters (i.e., saliency scores, thresholds and plug-in clas-\nsifiers), which are updated by AdamW optimizer. The addi-\ntional time consumption of WDPruning is acceptable. The\ndetailed procedure is presented in Algorithm 1.\nExperiment\nIn this section, we empirically investigate the effective-\nness of the proposed WDPruning on benchmark datasets:\nCIFAR-10 and ILSVRC-12 (Russakovsky et al. 2015). To\nunderstand our method, extensive ablation studies are con-\nducted on these datasets.\nDatasets\nCIFAR-10 contains 50k training images and 10k validat-\ning images, which are categorized into 10 classes for image\nclassification. Compared with CIFAR-10, ILSVRC-12 is a\nlarger scale image classification dataset, which comprises\n1.28 million images from 1k categories for training and 50k\nimages for validation.\nImplementation Details\nWe conduct experiments on the standard ViT models (Doso-\nvitskiy et al. 2020) (DeiT (Touvron et al. 2021)), and an\nimproved variant Swin Transformer (Liu et al. 2021a). We\nmainly prune the DeiT-Tiny, DeiT-Small and DeiT-Base\n1https://developer.nvidia.com/tensorrt\n2https://onnx.ai\nwhich have 12 blocks, as well as Swin Transformer-Small\nwith 24 blocks. For pruning shift-window attention in Swin\nTransformer, due to the deletion of attention leading to the\nproblem of dimension mismatch, we maintain the dimen-\nsion alignment by padding zero. For all experiments, we use\nthe pre-trained vision transformer models to initialize the\nbackbone models. In the pruning procedure, the DeiT mod-\nels are trained for 100 epochs and Swin Transformers are\ntrained for 60 epochs. The initial learning rate is 0.0005.\nWe use AdamW optimizer with a momentum of 0.9 for\noptimization. We set the weight decay to 0.05. In the ex-\nperiments, we keep the same data augmentation strategy as\nDeiT and Swin Transformer, including Random Augment\n(Cubuk et al. 2020), Mixup (Zhang et al. 2017), and CutMix\n(Yun et al. 2019). For the pruning setting, the initial value\nof saliency score and threshold parameter are 0 and 10, re-\nspectively. The learning rates of saliency scores and thresh-\nold parameters are set by 0.025 initially, and they are fine-\ntuned with AdamW with cosine learning rate decay strategy.\nThe initialization of Œª1 and Œª2 is 0, which are also added in\nAdamW optimizer to train. As for the block pruning setting,\nwe insert two additional classifiers in the 10-th and 11-th\ntransformer blocks for DeiT, and insert two additional clas-\nsifiers in 22-th and 23-th blocks for Swin Transformers.\nMain Results\nIn Table 1, we summarize the main results on ILSVRC-\n12 where we adjust the pruning ratio Rt for width pruning\nand block selection for depth pruning. We report the top-\n1/5 accuracy, the number of FLOPs and parameters, GPU\nthroughout and the latency on CPU. The GPU throughout\nis obtained by measuring the forward time on a NVIDIA\nRTX 3090 GPU with a batchsize of 1024, and the latency on\nCPU is measured on AMD EPYC 7502 32-Core CPU with a\nbatchsize of 1. We observe that WDPruning exhibits favor-\nable complexity/accuracy trade-offs at different complexity\nlevels, where our method can reduce the computational costs\nby 10%-43% and accelerate the inference at runtime by 5%-\n3147\nModel Top-1 Accu.(%) Top-1 ‚Üì(%) Top-5 Accu. (%) Top-5 ‚Üì(%) GFLOPs GFLOPs ‚Üì(%)\nDeiT-Tiny 72.20 0 91.10 0 1.3 0\nSCOP (Tang et al. 2020) 68.90 3.30 89.00 2.10 0.8 38.5\nPoWER‚àó (Goyal et al. 2020) 69.40 2.80 89.20 1.90 0.8 38.5\nHVT (Pan et al. 2021) 69.70 2.50 89.40 1.70 0.7 46.2\nWDPruning-0.3-11 70.34 1.86 89.82 1.28 0.7 46.2\nDeiT-Small 79.80 0 95.00 0 4.6 0\nSCOP (Tang et al. 2020) 77.50 2.30 93.50 1.50 2.6 43.5\nPoWER‚àó (Goyal et al. 2020) 78.30 1.50 94.00 1.00 2.7 41.3\nHVT (Pan et al. 2021) 78.00 1.80 93.83 1.17 2.4 47.8\nWDPruning-0.3-11 78.38 1.42 94.05 0.95 2.6 43.5\nDeiT-Base 81.80 0 95.59 0 17.5 0\nSCOP (Tang et al. 2020) 79.70 2.10 94.50 1.09 10.2 41.7\nPoWER‚àó(Goyal et al. 2020) 80.10 1.70 94.60 0.99 10.4 40.6\nVTP (Zhu et al. 2021) 80.70 1.10 95.00 0.59 10.0 42.9\nWDPruning-0.3-11 80.76 1.04 95.36 0.23 9.9 43.4\nTable 2: Comparison with the state-of-the-art methods on ILSVRC-12. We compare our method with these methods with Top-\n1/5 accuracy and FLOPs. ‚Äú‚Üì‚Äù represents the reduction rate. ‚Äú‚àó‚Äù denotes that the results come from (Pan et al. 2021).\n10% 20% 30% 40% 50% 60% 70%94%\n95%\n96%\n97%\n98%\n99%Accuracy\nEpected Pruning ratio\n Baseline\n Width-Pruning\n0 10 20 30 40 50 600.0\n0.1\n0.2\n0.3\n0.4\n0.5Pruning ratio\nEpoch\n32\nFigure 3: Left: Accuracy for DeiT-Small on CIFAR-10 re-\ngarding pruning ratio. Solid line and shadow denote the\nmean value and standard deviation, respectively. Right: The\npruning ratio varying epoch when Rt = 0.5.\n25% with the neglectable influence on accuracy. Compared\nto the base model DeiT-Base, the Top-1 accuracy of our\nmethod (idx=8) even increases by 0.14% when leading to\na reduction of 19.9% in the number of parameters, and the\ninference speed on CPU is accelerated by 11.6%. When 2\nextra blocks are removed (idx=9) based on DeiT-Base with\nwidth pruning (idx=8), the Top-1 accuracy only drops by\n0.95%. It is worth noting that after pruning with different ra-\ntio of width and depth, we obtain two models with the sim-\nilar FLOPs (idx=9, idx=10). Although the DeiT with depth\npruning (idx=9) has a greater accuracy drop, it gains more\nGPU throughput. It indicates that depth pruning is more con-\nducive to parallel acceleration and demonstrates the limita-\ntion of single-dimension width pruning. Compared to the\nbase model Swin Transformer-Small (idx=12), the pruned\ncounterpart (idx=15) achieves a 30.6% decrease in the num-\nber of parameters and a 15.3% increase in GPU throughput\nwith only an accuracy drop of 1.2%.\nComparison with Other Methods\nWe compare our method with state-of-the-art pruning meth-\nods on ILSVRC-12. The following state-of-the-art methods\nare compared: SCOP (Tang et al. 2020), PoWER (Goyal\n1 2 3 4 5 6 7 8 9 10 11 120.0\n0.1\n0.2\n0.3\n0.4\n0.5Pruning ratio\nTransformer Block Index\n Attn\n Proj\n Fc1\n Fc2\nFigure 4: The pruning ratio of units on each block when\npruning ratio is 0.3.\net al. 2020), HVT (Pan et al. 2021) and VTP (Zhu et al.\n2021). SCOP is a channel pruning method and (Tang et al.\n2021) re-implemented it to reduce the patches in vision\ntransformers. (Pan et al. 2021) re-implemented PoWER to\nprogressively eliminate patch tokens during the forward\npass. HVT progressively pooled the visual tokens to shrink\nthe sequence length and reduced the computational cost of\nDeiT. VTP trained the DeiT for 100 epochs by imposing ‚Ñì1\nregularization on soft masks, pruned the soft masks whose\nvalues are under a threshold, and finetuned for 100 epochs.\nThe experimental results are shown in Table 2. The pro-\nposed WDPruning is competitive with the state-of-the-art\nmethods. Concretely, for pruning DeiT-Tiny, compared with\nHVT, WDPruning achieves higher Top-1 accuracy (70.34%\nvs. 69.70%) and Top-5 accuracy (89.82% vs. 89.40%) with\nthe same reduction in FLOPs of 46.2%. The results on\nILSVRC-12 demonstrate WDPruning can produce more\ncompact vision transformer with better performance com-\npared to other single-dimension pruning methods.\nAblation Study and Discussion\nEffect of the predefined pruning ratio. The predefined\npruning ratio controls the targeted amount of pruned param-\neters in the width pruning process. To investigate its effect,\nwe train DeiT-Small on the CIFAR-10 with a range of prun-\n3148\nMethod Top-1(%) Top-5(%) GFLOPs\nDeiT-Small 79.80 95.00 4.6\nuniform pruning-0.3 75.26 92.33 3.2\nOurs-0.3 78.55 94.37 3.1\nTable 3: Learnable pruning ratio vs. uniform pruning.\nModel Layers Accu.(%) GFLOPs Param.(M)\nDeiT-Small\n12 98.50 4.24 21.95\n10 98.35 3.54 18.40\n8 98.11 2.84 14.86\n6 96.55 2.15 11.30\n4 92.81 1.45 7.77\n2 74.08 0.75 4.22\nTable 4: Results of depth pruning on CIFAR-10 regarding\ninserting 5 classifiers in the intermediate class tokens.\ning ratio varying from 0% to 70%. As shown on the left of\nFig. 3, we report the accuracy under different pruning ratio.\nWhen the pruning ratio is less than 20%, we observe that\nthe compressed transformers perform better than the uncom-\npressed one, which can be considered as the regularization\neffect introduced. As we expect, increasing pruning ratio re-\nsults in higher degree of transformer compression with more\nimpact on overall accuracy as well. On the right of Fig. 3, we\nreport the change in pruning ratio varying epochs when the\npredefined pruning ratio is 0.5. The pruning ratio of model\nwas dynamically allocated and increased with epoch. The\noverall pruning procedure is finished since epoch 32.\nLearnable pruning ratio vs. uniform pruning. We fur-\nther compare the performance of learnable pruning ratio\nwith the uniform pruning. We prune the DeiT-Small on\nILSVRC-12 by learnable pruning ratio and uniform prun-\ning, respectively. We set the pruning ratio to 0.3 (Rt = 0.7),\nand the other experimental settings are the same. From the\nTable. 3, we learn that the accuracy of uniform pruning is\nonly 75.26%, which incurs a large accuracy drop (4.54%).\nIn Fig. 4, we visualize the pruning ratio for each block of\nDeiT-Small after the pruning procedure. We learn that our\nmethod allocates the pruning ratio to each layer accordingly\nand adaptively learns the compact structure of each block\nin the transformer, which is beneficial to learn a structured\npruning network with good performance.\nEffect of depth pruning. To study the effect of trans-\nformer block pruning, we train DeiT-Small with 5 plug-in\nlearnable classifiers that are inserted in transformer blocks\nof {2,4,6,8,10}. In this experiment, WDPruning does not\nprune the width of vision transformer and we focus on the\neffect of depth pruning. The results are shown in Table 4, and\nwe learn from that the DeiT-Small can be losslessly pruned\naway 4 transformer blocks where the accuracy is retained\nabove 98%. This experiment demonstrates we can obtain\nmultiple models with different depth versions in one training\nby depth pruning.\nVisualization. We use Attention Rollout (Abnar and\nZuidema 2020) to project the attention maps from the out-\nput token to the input image, and visualize the 6-th block\nof multi-head self-attention of DeiT-Small. As shown in\nFigure 5: Input image (left) and visualization of atten-\ntion maps (right) of DeiT-block6. Attention maps with red\nbounding boxes are the attention heads to be retained.\nDataset Method mAP #Param. Throughput\nMarket-1501 TransReID 88.5% 85.1M 459im/s\nOur 87.7% 59.6M 572im/s\nTable 5: Results of pruning TransReID on Market-1501.\nFig. 5 the attention maps with red bounding boxes are re-\nmained after the pruning procedure, and we record their ul-\ntimate saliency scores. From the figure, these pruned atten-\ntion maps mainly focus more on the patch of background,\nand contribute less to the classification object. In contrast,\nthe attention maps that are retained obtain higher saliency\nscores and contribute more to the foreground object.\nIts application to image retrieval tasks. As described\nabove, our method is effective in image classification. To\nfurther analyze the generalization of our method, we use\nDeiT-Base as a backbone network to deploy TransReID (He\net al. 2021) for image retrieval task, and then compress DeiT-\nBase by reducing 30% parameters of the backbone network.\nIn the experimental implementation, we evaluate the perfor-\nmance with mean Average Precision (mAP) on Market-1501\n(Zheng et al. 2015) dataset. As shown in Table 5, our pruned\ntransformer shows a good result. The mAP of our method\nis slightly lower than the baseline, but the throughput is 572\nimages per second, which is higher than the baseline. This\ndemonstrates that our method has good generalization on\nother vision tasks.\nConclusion\nIn this work, we present a structured pruning framework to\nreduce the width and depth of vision transformers. In our ex-\nperiments, we demonstrate very promising pruning results\non image classification and image retrieval tasks. We can\nadaptively adjust the depth and width of vision transform-\ners based on budgets at hand and difficulties of each task.\nWe believe these pruning results can further inspire the de-\nsign of more compact vision transformers. In the future, we\nplan to extend the proposed methods from image domain to\nvideo transformers, e.g. Video Swin Transformer (Liu et al.\n2021b), Video Transformer Network (Neimark et al. 2021).\n3149\nAcknowledgements\nWe sincerely thank the anonymous reviewers for valuable\ncomments and suggestions. The paper is supported by the\nNational Natural Science Foundation of China (NSFC) un-\nder Grant No. 61672498.\nReferences\nAbnar, S.; and Zuidema, W. 2020. Quantifying attention\nflow in transformers. arXiv preprint arXiv:2005.00928.\nBastings, J.; Aziz, W.; and Titov, I. 2019. Interpretable neu-\nral predictions with differentiable binary variables. arXiv\npreprint arXiv:1905.08160.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In European Conference on Computer\nVision, 213‚Äì229. Springer.\nCubuk, E. D.; Zoph, B.; Shlens, J.; and Le, Q. V . 2020. Ran-\ndaugment: Practical automated data augmentation with a re-\nduced search space. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition Work-\nshops, 702‚Äì703.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nFan, A.; Grave, E.; and Joulin, A. 2019. Reducing trans-\nformer depth on demand with structured dropout. arXiv\npreprint arXiv:1909.11556.\nGoyal, S.; Choudhury, A. R.; Raje, S.; Chakaravarthy, V .;\nSabharwal, Y .; and Verma, A. 2020. Power-bert: Accelerat-\ning bert inference via progressive word-vector elimination.\nIn International Conference on Machine Learning, 3690‚Äì\n3699. PMLR.\nHan, S.; Pool, J.; Tran, J.; and Dally, W. 2015. Learning\nboth weights and connections for efficient neural network. In\nAdvances in Neural Information Processing Systems, 1135‚Äì\n1143.\nHassani, A.; Walton, S.; Shah, N.; Abuduweili, A.; Li, J.;\nand Shi, H. 2021. Escaping the big data paradigm with com-\npact transformers. arXiv preprint arXiv:2104.05704.\nHe, S.; Luo, H.; Wang, P.; Wang, F.; Li, H.; and Jiang, W.\n2021. Transreid: Transformer-based object re-identification.\narXiv preprint arXiv:2102.04378.\nJiang, Z.; Hou, Q.; Yuan, L.; Zhou, D.; Jin, X.; Wang, A.;\nand Feng, J. 2021. Token labeling: Training a 85.5% top-1\naccuracy vision transformer with 56m parameters on ima-\ngenet. arXiv preprint arXiv:2104.10858.\nLiu, Z.; Li, J.; Shen, Z.; Huang, G.; Yan, S.; and Zhang, C.\n2017. Learning Efficient Convolutional Networks Through\nNetwork Slimming. In Proceedings of International Con-\nference on Computer Vision, 2736‚Äì2744.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021a. Swin transformer: Hierarchical\nvision transformer using shifted windows. arXiv preprint\narXiv:2103.14030.\nLiu, Z.; Ning, J.; Cao, Y .; Wei, Y .; Zhang, Z.; Lin, S.; and\nHu, H. 2021b. Video swin transformer. arXiv preprint\narXiv:2106.13230.\nMeng, L. 2021. Armour: Generalizable Compact Self-\nAttention for Vision Transformers. arXiv preprint\narXiv:2108.01778.\nMichel, P.; Levy, O.; and Neubig, G. 2019. Are sixteen heads\nreally better than one? arXiv preprint arXiv:1905.10650.\nNeimark, D.; Bar, O.; Zohar, M.; and Asselmann, D.\n2021. Video transformer network. arXiv preprint\narXiv:2102.00719.\nPan, Z.; Zhuang, B.; Liu, J.; He, H.; and Cai, J. 2021. Scal-\nable visual transformers with hierarchical pooling. arXiv\npreprint arXiv:2103.10619.\nRamanujan, V .; Wortsman, M.; Kembhavi, A.; Farhadi,\nA.; and Rastegari, M. 2020. What‚Äôs Hidden in a Ran-\ndomly Weighted Neural Network? In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 11893‚Äì11902.\nRao, Y .; Zhao, W.; Liu, B.; Lu, J.; Zhou, J.; and Hsieh,\nC.-J. 2021. DynamicViT: Efficient Vision Transform-\ners with Dynamic Token Sparsification. arXiv preprint\narXiv:2106.02034.\nRussakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.;\nMa, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.;\net al. 2015. Imagenet large scale visual recognition chal-\nlenge. International Journal of Computer Vision, 115(3):\n211‚Äì252.\nTang, Y .; Han, K.; Wang, Y .; Xu, C.; Guo, J.; Xu, C.; and\nTao, D. 2021. Patch Slimming for Efficient Vision Trans-\nformers. arXiv preprint arXiv:2106.02852.\nTang, Y .; Wang, Y .; Xu, Y .; Tao, D.; Xu, C.; Xu, C.; and Xu,\nC. 2020. Scop: Scientific control for reliable neural network\npruning. arXiv preprint arXiv:2010.10732.\nTeerapittayanon, S.; McDanel, B.; and Kung, H.-T. 2016.\nBranchynet: Fast inference via early exiting from deep neu-\nral networks. In 2016 23rd International Conference on Pat-\ntern Recognition (ICPR), 2464‚Äì2469. IEEE.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J¬¥egou, H. 2021. Training data-efficient image trans-\nformers & distillation through attention. In International\nConference on Machine Learning, 10347‚Äì10357. PMLR.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, ≈Å.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998‚Äì6008.\nWang, Y .; Xu, Z.; Wang, X.; Shen, C.; Cheng, B.; Shen, H.;\nand Xia, H. 2021. End-to-end video instance segmentation\nwith transformers. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, 8741‚Äì\n8750.\n3150\nXin, J.; Tang, R.; Lee, J.; Yu, Y .; and Lin, J. 2020. DeeBERT:\nDynamic Early Exiting for Accelerating BERT Inference. In\nProceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, 2246‚Äì2251.\nYao, Z.; Ma, L.; Shen, S.; Keutzer, K.; and Mahoney,\nM. W. 2021. MLPruning: A Multilevel Structured Pruning\nFramework for Transformer-based Models. arXiv preprint\narXiv:2105.14636.\nYuan, L.; Hou, Q.; Jiang, Z.; Feng, J.; and Yan, S. 2021.\nV olo: Vision outlooker for visual recognition.arXiv preprint\narXiv:2106.13112.\nYun, S.; Han, D.; Oh, S. J.; Chun, S.; Choe, J.; and Yoo,\nY . 2019. Cutmix: Regularization strategy to train strong\nclassifiers with localizable features. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\n6023‚Äì6032.\nZhang, H.; Cisse, M.; Dauphin, Y . N.; and Lopez-Paz, D.\n2017. mixup: Beyond empirical risk minimization. arXiv\npreprint arXiv:1710.09412.\nZheng, L.; Shen, L.; Tian, L.; Wang, S.; Wang, J.; and Tian,\nQ. 2015. Scalable person re-identification: A benchmark. In\nProceedings of the IEEE international conference on com-\nputer vision, 1116‚Äì1124.\nZhou, W.; Xu, C.; Ge, T.; McAuley, J.; Xu, K.; and Wei,\nF. 2020. BERT Loses Patience: Fast and Robust Inference\nwith Early Exit. Advances in Neural Information Processing\nSystems, 33.\nZhu, M.; Han, K.; Tang, Y .; and Wang, Y . 2021. Vision\nTransformer Pruning. KDD 2021 workshop.\nZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J. 2020.\nDeformable detr: Deformable transformers for end-to-end\nobject detection. arXiv preprint arXiv:2010.04159.\n3151",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7841986417770386
    },
    {
      "name": "Computer science",
      "score": 0.6667182445526123
    },
    {
      "name": "Computation",
      "score": 0.5384007096290588
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47595497965812683
    },
    {
      "name": "FLOPS",
      "score": 0.4480932354927063
    },
    {
      "name": "Inference",
      "score": 0.4348929524421692
    },
    {
      "name": "Computer vision",
      "score": 0.3649992346763611
    },
    {
      "name": "Algorithm",
      "score": 0.2408229112625122
    },
    {
      "name": "Engineering",
      "score": 0.14878839254379272
    },
    {
      "name": "Parallel computing",
      "score": 0.11161181330680847
    },
    {
      "name": "Voltage",
      "score": 0.10233676433563232
    },
    {
      "name": "Electrical engineering",
      "score": 0.08362036943435669
    }
  ]
}