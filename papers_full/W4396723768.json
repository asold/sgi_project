{
  "title": "Augmenting large language models with chemistry tools",
  "url": "https://openalex.org/W4396723768",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5031480183",
      "name": "Andres M. Bran",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A5101971357",
      "name": "Sam Cox",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A5075449407",
      "name": "Oliver Schilter",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A5092630105",
      "name": "Carlo Baldassari",
      "affiliations": [
        "IBM Research - Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A5103115676",
      "name": "Andrew Dickson White",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A5028051805",
      "name": "Philippe Schwaller",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6800751262",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4281669078",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W4327564965",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4319996831",
    "https://openalex.org/W2034354062",
    "https://openalex.org/W2606363443",
    "https://openalex.org/W2903262661",
    "https://openalex.org/W2947423323",
    "https://openalex.org/W3088999551",
    "https://openalex.org/W4226159083",
    "https://openalex.org/W2324964582",
    "https://openalex.org/W2747592475",
    "https://openalex.org/W2968071222",
    "https://openalex.org/W3010145447",
    "https://openalex.org/W3101155908",
    "https://openalex.org/W3121024033",
    "https://openalex.org/W4220747243",
    "https://openalex.org/W2189911347",
    "https://openalex.org/W2966357564",
    "https://openalex.org/W4310603653",
    "https://openalex.org/W4391561379",
    "https://openalex.org/W2529996553",
    "https://openalex.org/W3094686696",
    "https://openalex.org/W3129039627",
    "https://openalex.org/W2478294658",
    "https://openalex.org/W3128474010",
    "https://openalex.org/W4306845856",
    "https://openalex.org/W3014089210",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4385571219",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4220715560",
    "https://openalex.org/W3157679602",
    "https://openalex.org/W3123901912",
    "https://openalex.org/W2987091515",
    "https://openalex.org/W3023658436",
    "https://openalex.org/W4353113046",
    "https://openalex.org/W4361866031",
    "https://openalex.org/W4225405251",
    "https://openalex.org/W4389991792",
    "https://openalex.org/W2119396661",
    "https://openalex.org/W1972905466",
    "https://openalex.org/W1964666522",
    "https://openalex.org/W1995396482",
    "https://openalex.org/W2594183968",
    "https://openalex.org/W4389519254",
    "https://openalex.org/W4283034548",
    "https://openalex.org/W3097758405",
    "https://openalex.org/W4220681836",
    "https://openalex.org/W4285093994",
    "https://openalex.org/W6801984403",
    "https://openalex.org/W3198449425",
    "https://openalex.org/W4365794116",
    "https://openalex.org/W3179394107",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4379539933",
    "https://openalex.org/W4389520103",
    "https://openalex.org/W4225576545",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W2123179625",
    "https://openalex.org/W4387567938",
    "https://openalex.org/W3094640617",
    "https://openalex.org/W1988037271",
    "https://openalex.org/W4225876201",
    "https://openalex.org/W2037825667",
    "https://openalex.org/W2970764640",
    "https://openalex.org/W2126363023",
    "https://openalex.org/W6948679223",
    "https://openalex.org/W6930245733",
    "https://openalex.org/W3098269892",
    "https://openalex.org/W3103092523"
  ],
  "abstract": null,
  "full_text": "Nature Machine Intelligence | Volume 6 | May 2024 | 525–535\n 525\nnature machine intelligence\nhttps://doi.org/10.1038/s42256-024-00832-8\nArticle\nAugmenting large language models with \nchemistry tools\nAndres M. Bran1,2,6, Sam Cox3,4,6, Oliver Schilter    1,2,5, Carlo Baldassari5, \nAndrew D. White    3,4  & Philippe Schwaller    1,2 \nLarge language models (LLMs) have shown strong performance in \ntasks across domains but struggle with chemistry-related problems. \nThese models also lack access to external knowledge sources, limiting \ntheir usefulness in scientific applications. We introduce ChemCrow, \nan LLM chemistry agent designed to accomplish tasks across organic \nsynthesis, drug discovery and materials design. By integrating 18 \nexpert-designed tools and using GPT-4 as the LLM, ChemCrow augments \nthe LLM performance in chemistry, and new capabilities emerge. Our \nagent autonomously planned and executed the syntheses of an insect \nrepellent and three organocatalysts and guided the discovery of a novel \nchromophore. Our evaluation, including both LLM and expert assessments, \ndemonstrates ChemCrow’s effectiveness in automating a diverse set of \nchemical tasks. Our work not only aids expert chemists and lowers barriers \nfor non-experts but also fosters scientific advancement by bridging the gap \nbetween experimental and computational chemistry.\nIn the last few years, large language models (LLMs)1–5 have transformed \nvarious sectors by automating natural language tasks. A prime example \nof this is the introduction of GitHub Copilot in 20216 and more recently \nStarCoder7, which provides proposed code completions based on the \ncontext of a file and open windows and increases developers’ productiv-\nity8. Most recent advances are based on the Transformer architecture9, \nintroduced for neural machine translation and extended to various \nnatural language processing tasks demonstrating remarkable few-shot \nand zero-shot performance2. Nevertheless, it is crucial to recognize the \nlimitations of LLMs, which often struggle with seemingly simple tasks \nlike basic mathematics and chemistry operations 10,11. For instance, \nGPT-4 (ref. 12) and GPT-3.5 (ref. 13) cannot consistently and accurately \nmultiply 12,345 × 98,765 or convert IUPAC names into the correspond-\ning molecular graph14. These shortcomings can be attributed to the \nmodels’ core design, which focuses on predicting subsequent tokens. \nT o address these limitations, one viable approach is to augment LLMs \nwith dedicated external tools or plugins, such as a calculator for math-\nematical operations or OPSIN15 for IUPAC-to-structure conversion. \nThese specialized tools provide exact answers, thereby compensating \nfor the inherent deficiencies of LLMs in specific domains and enhancing \ntheir overall performance and applicability.\nChemistry, as a field, has been impacted through expert-designed \nartificial intelligence (AI) systems that tackle specific problems, such \nas reaction prediction 16–20, retrosynthesis planning 21–27, molecular \nproperty prediction28–32, de novo molecular generation33,34, materials \ndesign35,36 and, more recently, Bayesian optimization37–39. Due to the \nnature of their training data, it has been shown that code-generating \nLLMs do possess some understanding of chemistry 14, allowing them \nto adapt to observations, plan over multiple steps and respond cor -\nrectly to intent in a chemical setting 13,40–44. Still, the automation lev -\nels achieved in chemistry remain relatively low compared to other \ndomains, primarily due to its highly experimental nature, the lack of \ndata and the limited scope and applicability of computational tools, \neven within their designated areas45.\nIntegrating such tools tends to occur within isolated environ -\nments, such as RXN for Chemistry 18,24,46–48 and AIZynthFinder 25,49,50, \nReceived: 13 September 2023\nAccepted: 27 March 2024\nPublished online: 8 May 2024\n Check for updates\n1Laboratory of Artificial Chemical Intelligence (LIAC), ISIC, EPFL, Lausanne, Switzerland. 2National Centre of Competence in Research (NCCR) Catalysis, \nEPFL, Lausanne, Switzerland. 3Department of Chemical Engineering, University of Rochester, Rochester, NY, USA. 4FutureHouse, San Francisco,  \nCA, USA. 5Accelerated Discovery, IBM Research – Europe, Rüschlikon, Switzerland. 6These authors contributed equally: Andres M. Bran, Sam Cox.  \n e-mail: andrew@futurehouse.org; philippe.schwaller@epfl.ch\nNature Machine Intelligence | Volume 6 | May 2024 | 525–535 526\nArticle https://doi.org/10.1038/s42256-024-00832-8\nadditional information, observe the tool’s responses and repeat this \nloop until the final answer is reached. Contemporaneously with this \nwork, ref. 54 describes a similar approach of augmenting an LLM with \ntools for accomplishing tasks in chemistry that are out of reach of GPT-4 \nalone. Its focus is specifically on cloud labs, whereas we investigate \nan extensive range of tasks and tools including the connection to a \ncloud-connected robotic synthesis platform. We implemented 18 tools, \nas shown in Fig. 1b and described in ‘T ools’ , that endow ChemCrow \nnot only with knowledge about molecular and reaction properties \nbut also with the capacity to directly execute tasks in a physical lab. \nAlthough the list of tools included is not exhaustive, ChemCrow has \nbeen designed to be easily adapted to new applications by providing \nnew tools. ChemCrow serves as an assistant to expert chemists while \nsimultaneously lowering the entry barrier for non-experts by offering a \nsimple interface to access accurate chemical knowledge. We analyse the \ncapabilities of ChemCrow on 14 use cases (Appendix G in the Supple-\nmentary Information), including synthesizing target molecules, safety \ncontrols and searching for molecules with similar modes of action.\nResults and discussion\nAutonomous chemical synthesis\nFrom user inputs such as ‘Plan and execute the synthesis of an insect \nrepellent’ (Fig. 1a) and ‘Find a thiourea organocatalyst which acceler-\nates the Diels-Alder reaction. After you find it, please plan and execute \na synthesis for this organocatalyst’ (Fig. 2b), ChemCrow sequentially \nqueried tools to find appropriate molecules, planned the syntheses \nand executed the syntheses on the cloud-connected, proprietary \nRoboRXN platform from IBM Research\n55. Using RoboRXN, Chem -\nCrow autonomously ran the syntheses of an insect repellent (DEET) \nand three known thiourea organocatalysts (Schreiner’s56,57, Ricci’s58 \nand Takemoto’s59). The synthesized structures are shown in Fig. 2d \nand the detailed description of the tools in ‘T ools’ . The four synthe-\nses yielded the anticipated compounds successfully, demonstrating \nsynthesis planning and execution-related LLM agent interactions with \nfacilitated by corporate directives that promote integrability. Although \nmost tools are developed by the open-source community or made \naccessible through application programming interfaces (APIs), their \nintegration and interoperability pose considerable challenges for \nexperimental chemists, mainly due to their lack of computational \nskill sets and the diversity of tools with steep learning curves, thereby \npreventing the full exploitation of their potential.\nInspired by successful applications in other fields10,51,52, we propose \nan LLM-powered chemistry engine, ChemCrow, designed to streamline \nthe reasoning process for various common chemical tasks across areas \nsuch as drug and materials design and synthesis. ChemCrow harnesses \nthe power of multiple expert-designed tools for chemistry and operates \nby prompting a LLM (GPT-4 in our experiments) with specific instruc-\ntions about the task and the desired format, as shown in Fig. 1a. The \nLLM is provided with a list of tool names, descriptions of their utility \nand details about the expected input/output. It is then instructed to \nanswer a user-given prompt, using the tools provided when neces -\nsary. The model is guided to follow the Thought, Action, Action Input, \nObservation format43, which requires it to reason about the current \nstate of the task, consider its relevance to the final goal and plan the next \nsteps accordingly, demonstrating its level of understanding. After the \nreasoning in the Thought step, the LLM requests a tool (preceded by the \nkeyword ‘ Action’) and the input for this tool (with the keyword ‘ Action \nInput’). The text generation then pauses, and the program attempts to \nexecute the requested function using the provided input. The result is \nreturned to the LLM prepended by the keyword ‘Observation’ , and the \nLLM proceeds to the Thought step again. It continues iteratively until \nthe final answer is reached.\nThis workflow, previously described in the ReAct 43 and MRKL53 \npapers, effectively combines chain-of-thought reasoning with tools \nrelevant to the tasks. As a result, and as will be shown in the follow -\ning sections, the LLM transitions from a hyperconfident—although \ntypically wrong—information source to a reasoning engine that \nis prompted to reflect on a task, act using a suitable tool to gather \nChemistry-informed\nsequence of actions\nAutonomous\nexperimentation\nChemCrow\n1. Thought:\nreason, plan\n2. Action:\nselect tool\n3. Action\ninput: use tool\n4. Observation:\nanalyze\nAutonomous interaction with tools and\nthe physical world (for example, RoboRXN)\nExample input:\nPlan and execute\nthe synthesis of an\ninsect repellent.\nExpert-designed\nchemistry tools\na\nb\nO\nN\nO\nN\n1. Google search\n2. Retrosynthesis\n3. Procedure prediction\n4. Execution on robot\nChain of thought reasoning loop\nSynthesis of \nDEET without\nhuman \ninteraction.\nGeneral tools\nReaction toolsSafety tools\nMolecule tools\n?\nUser-defined\nscientific tasks\n• Explosive check\n• Safety assessment\n• Name to SMILES\n• Patent check\n• Functional groups\n• Modify molecule\n• Similarity\n• SMILES to CAS\n• SMILES to price\n• SMILES to weight\n• Synthesis execute\n• Synthesis plan\n• RXN predict\n• RXN to name\n• Human expert\n• Code interpreter\n• Web search\n• Literature search\nFig. 1 | Overview and toolset. a, An overview of the task-solving process. Using \na variety of chemistry-related packages and software, a set of tools is created. \nThese tools and a user input are then given to an LLM. The LLM proceeds \nthrough an automatic, iterative chain-of-thought process, deciding on its path, \nchoice of tools and inputs before coming to a final answer. The example shows \nthe synthesis of DEET, a common insect repellent. b, T oolsets implemented \nin ChemCrow: reaction, molecule, safety, search and standard tools. Credit: \nphotograph in a, IBM Research under a creative commons license CC BY-ND 2.0.\nNature Machine Intelligence | Volume 6 | May 2024 | 525–535\n 527\nArticle https://doi.org/10.1038/s42256-024-00832-8\nthe physical world. It should be noted that one could use these tools \nindividually, provided they had access, with likely the same result. \nChemCrow automates the execution of these tools by harnessing the \nreasoning abilities of LLMs.\nStandardized synthesis procedures are key for successful execu-\ntion. However, the predicted procedures 46 are not always directly \nexecutable on the RoboRXN platform; typical problems include ‘not \nenough solvent’ or ‘invalid purify action’ . Although addressing these \nissues typically requires human interaction to fix the invalid actions \nbefore attempting to execute the synthesis, ChemCrow is able to auton-\nomously query the synthesis validation data from the platform and \niteratively adapt the synthesis procedure (such as increasing solvent \nquantity) until the synthesis procedure is fully valid, thereby remov-\ning the need for human intervention. This example demonstrates \nChemCrow’s abilities to autonomously adapt and successfully execute \nstandardized synthesis procedures, alleviating lab safety concerns and \nadapting itself to the particular conditions of the robotic platform.\nHuman–AI collaboration\nCollaboration between humans and computers is valuable, espe -\ncially in the realm of chemistry, where decisions are often based on \nexperimental results. Here we demonstrate how such an interaction \ncan lead to the discovery of a novel chromophore. For this example, \nChemCrow was instructed to train a machine-learning model to help \nscreen a library of candidate chromophores\n60. As can be seen in Fig. 3, \nChemCrow is capable of loading, cleaning and processing the data; \ntraining and evaluating a random forest model (Appendix G.1 in the \nSupplementary Information); and finally providing a suggestion based \non the model and the given target absorption maximum wavelength of \n369 nm. The proposed molecule (Fig. 3) was subsequently synthesized \nand analysed, confirming the discovery of a new chromophore with \napproximately the desired property (measured absorption maximum \nwavelength of 336 nm).\nEvaluation across diverse chemical use cases\nIn recent years, there has been a surge in the application of machine \nlearning to chemistry, resulting in a wealth of datasets and benchmarks \nin the field61,62. However, few of these benchmarks focus on assessing \nLLMs for tasks specific to chemistry, and given the rapid pace of pro-\ngress, a standardized evaluation technique has not yet been established, \nposing a challenge in assessing the approach we demonstrate here. T o \naddress this issue, we collaborated with expert chemists to develop a \nset of tasks that test the capabilities of LLMs in using chemistry-specific \ntools and solving problems in the field. The selected tasks are executed \nby both ChemCrow and GPT-4, and these results are evaluated with a \ncombination of LLM-based and expert human assessments. GPT-4 is \nprompted to assume the role of an expert chemist but has no access \nto external tools such as internet browsing. For the LLM-based assess-\nments, we draw inspiration from the evaluation methods described in \nrefs. 5,63,64, where the authors use an evaluator LLM that is instructed \nto assume the role of a teacher assessing their students. In our case, we \nadapted the prompt so that the evaluator LLM (which we call Evalu-\natorGPT) gives a grade based only on whether the task is addressed \nand whether the overall thought process is correct. EvaluatorGPT is \nfurther instructed to highlight the strengths and weaknesses of each \napproach and to provide further feedback on how each response could \nimprove, providing ground to explain the LLM’s evaluations. Full results \nfor several tasks, spanning synthetic planning for drugs, design of \nnovel compounds with similar properties and modes of actions and \nexplaining reaction mechanisms, are presented in Appendix G of the \nSupplementary Information. The full examples are also available at \nhttps://github.com/ur-whitelab/chemcrow-runs.\nIt is worth noting that the validity of ChemCrow’s responses \ndepends on the quality and quantity of the tools, as well as the agent’s \nreasoning process. For instance, synthetic planning capabilities can \nbenefit from an improved underlying synthesis engine, an active area \nof research23,65,66. Even then, any tool becomes useless if the reasoning \nRoboRXN synthesis platformTask: Find and synthesize a thiourea organocatalyst which \naccelerates a Diels-Alder reaction.\nsuitable catalyst. \nWeb Search tool:  Schreiner′s thiourea catalyst\nNow, I will obtain the SMILES. Name2Smiles tool: \nFC(F)(F)c1cc(NC(=S)Nc2cc(C(F)(F)F)cc(C(F)(F)F)c2)cc(C(F)(F)F)c1\nI will plan a synthesis for Schreiner′s thiourea catalyst.\nSynthesisPlanner tool: detailed synthesis plan\nI will execute the synthesis. \nSynthesisExecuter tool: successful synthesis. \nInsect repellent (plan and execute)\nThiourea organocatalysts (plan and execute)\nNovel chromophore (clean data, train model and predict)\na b c\nChemCrow workflows with experimental validationd\nO\nCl N\nH\nO\nN+\nDEET\nF\nF\nF\nF\nN\nH\nS\nN\nH\nN\nF F\nF\nF F\nN\nH\nS\nN\nH\nF\nF\nF\nF\nF\nF\nF\nF\nF\nSchreiner’s catalyst Takemoto’s catalyst\nSynthesis step 2: Iodo Heck reaction\nSynthesis step 1: Bromo Suzuki coupling\nConnection with\nphysical world\nRicci’s catalyst\nF\nF\nF\nFN\nH\nS\nHN\nOH\nF F\nBr\nH\nN\nS\nO\nO\nS\nO\nOH\nNB\nOH\nHO+\nH\nN\nS\nO\nO\nI\nO\nO\nO\nO\nH\nN\nS\nO\nO\n+\nFig. 2 | Experimental validation. a, Example of the script run by a user to \ninitiate ChemCrow. b, Query and synthesis of a thiourea organocatalyst. c, IBM \nResearch RoboRXN synthesis platform on which the experiments were executed \n(pictures reprinted courtesy of International Business Machines Corporation). \nd, Experimentally validated compounds. Credit: photographs in c, IBM Research \nunder a creative commons license CC BY-ND 2.0.\nNature Machine Intelligence | Volume 6 | May 2024 | 525–535 528\nArticle https://doi.org/10.1038/s42256-024-00832-8\nbehind its usage is flawed or if garbage inputs are given. Similarly, \ninaccurate outputs from the tools can lead the agent to incorrect con-\nclusions. For these reasons, a panel of expert chemists were asked to \nevaluate each model’s performance for each task across three dimen-\nsions: (1) correctness of the chemistry, (2) quality of reasoning and (3) \ndegree of task completion (Appendix B in the Supplementary Informa-\ntion). As shown in Fig. 4 , ChemCrow outperforms the tool-less LLM, \nespecially on more complex tasks where more grounded chemical \nreasoning is required. Although GPT-4 systematically fails to provide \nfactually accurate information, it tends to answer in a more fluent and \ncomplete style, making it preferred by EvaluatorGPT; the hallucinations \nit produces are nevertheless unveiled upon thorough inspection. Both \nsystems perform similarly in ‘quality of reasoning’ , an expected out-\ncome given ChemCrow’s by-design reliance on GPT-4 for reasoning. As \nshown in Fig. 4a,b, GPT-4 only outperforms ChemCrow at easier tasks, \nwhere the objective is very clear and all necessary information is part of \nGPT-4’s training data, allowing it to offer more complete answers based \nalmost purely on memorization of training data (for example, synthesis \nof DEET and paracetamol). In all of our experiments, ChemCrow was \nspecifically instructed to favour tool usage over internal knowledge, to \ndemonstrate the benefits of tool usage. Still, ChemCrow consistently \noffers better solutions across multiple objectives and difficulties, \nresulting in a strong preference from expert chemists in favour of \nChemCrow, showing its potential as a tool for the practitioner chemist.\nNote the difference between the human and LLM-powered evalua-\ntions in Fig. 4. Although human experts prefer ChemCrow’s responses \nbased on chemical accuracy and task completeness, EvaluatorGPT favours \nGPT-4, typically basing its evaluation on the fluency and apparent com-\npleteness of GPT-4’s responses. EvaluatorGPT has been recently presented \nand used as a self-evaluation method5,63, but our results indicate that \nwhen it lacks the required understanding to answer a prompt, it also lacks \ninformation to evaluate the prompt completions and thus fails to provide \na trustworthy assessment, rendering it unusable for the benchmarking \nof LLM capabilities whenever factuality plays a key role in evaluation. For \nscientific tasks requiring real-world knowledge, LLM-based methods like \nEvaluatorGPT, for now, cannot replace expert human assessment.\nRisk-mitigation strategies\nThe implementation and use of LLM-driven chemistry engines like \nChemCrow empower non-expert researchers by facilitating stream-\nlined combination of different expert-designed tools’ outputs. On \nany automated chemical platform, there is a heavy level of review and \ncontrol by human operators and chemist experts. Nevertheless, it is \ncrucial to ensure responsible development and use of LLM agents67–69.\nWe discuss the unintended risks and propose possible mitigation \nstrategies. Those can be achieved through foresight and safeguards, \nstill promoting open and transparent science to enable broad oversight \nand feedback from the research community.\nUnintended risks\nIt is a worldwide standard safety guideline to restrict access to chemical \nlaboratories to those who have received proper training. Nonethe-\nless, attempting to perform experiments based on the LLM-powered \nengine’s recommendations may lead to accidents or hazardous situa-\ntions. T o mitigate these risks, we provide the agent with safety instruc-\ntions that must be followed, such as checking safety information before \nproceeding to further advance with the task. As shown in Fig. 5, Chem-\nCrow follows a combination of hard-coded and prompted guidelines \n(Appendix D.2 in the Supplementary Information) to ensure safety. If \nthe proposed reaction is deemed dangerous, execution stops. Other-\nwise, execution proceeds, and the model can use gathered safety infor-\nmation to provide a more complete answer including safety concerns \nabout the suggested substances, as well as grounded recommendations \non how to safely handle them. As ChemCrow presents risks similar to \nthat of using the individual open-source tools, extensive mitigation \nstrategies are not currently essential. Such measures should be con-\nsidered, however, if newly added tools raise notable new risks.\nInaccurate or incomplete reasoning due to a lack of sufficient \nchemistry knowledge in the LLM-powered engine poses another risk, \nas it may lead to flawed decision-making or problematic experiment \nresults. One of the key points of this Article is that the integration \nof expert-designed tools can help mitigate the hallucination issues \ncommonly associated with these models, thus reducing the risk of \nHuman chemist Human–AI collaboration ChemCrow\nTask input:\nHere is some chromophore data.\n• Clean the data.\n• Use only data with acetonitrile as solvent.\n• Preprocess the data.\n• Train a random forest model to predict\n  absorption max wavelength of molecules.\n• Then make predictions for the molecules\n   in a selection pool.\n• Finally, suggest a synthetic plan for the\n   one with wavelength closest to 369 nm.\nChemCrow actions:\n1. Check data rows to learn the format.\n2. Filter data, solvent and relevant columns.\n3. Calculate Morgan fingerprints and\nsplit dataset into train/test.\n4. Train and evaluate random forest model.\n5. Propose molecule(s) from the selection pool.\n6. Predict two-step synthetic procedure \nfor selected molecule.\nHuman actions:\n• Synthesize proposed molecule. \n• Con/f_irm product using  MS(ESI) and NMR.\n• Analyse UV-Vis absorption spectrum.\nWavelength (nm)\n200\n0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n300 400 500 600 700\nAbsorbance (a.u.)\nSynthesize methyl (E)-3-methyl-4-(2-(3'-\n(methylsulfonamido)-[1,1'-biphenyl]-4-\nyl)vinyl)benzoate with a predicted maximum\nabsorption wavelength closest to 369 nm. The\nroot mean squared error of the random forest\nmodel is 37 nm.\nFinal answer:\nO\nO\nH\nN\nS\nO\nO\nO\nO\nH\nN\nS\nO\nO\nIn acetonitrile\nMeasured maximum\nExpected value\nFig. 3 | Human–model interaction leading to the discovery of a new chromophore. Left, human input, actions and observation. Right, ChemCrow actions and final \nanswer with the suggestion of the new chromophore.\nNature Machine Intelligence | Volume 6 | May 2024 | 525–535\n 529\nArticle https://doi.org/10.1038/s42256-024-00832-8\ninaccuracy. However, concerns may still arise when the model is unable \nto adequately analyse different observations due to a limited under -\nstanding of chemistry concepts, potentially leading to suboptimal \noutcomes. T o address this issue, developers can focus on improving the \nquality and breadth of the training data, incorporating more advanced \nchemistry knowledge and refining the LLM’s understanding of complex \nchemistry concepts. Additionally, a built-in validation or peer-review \nsystem, analogue to the reinforcement learning from human feedback \nimplemented for GPT-3.5 (refs. 70,71), could be incorporated to help \nensure the reliability of the engine’s recommendations.\nEncouraging users to critically evaluate the information provided \nby the LLM-powered engine and cross-reference it with established \nliterature and expert opinions can further mitigate the risk of relying \non flawed reasoning72. By combining these approaches, developers \ncan work towards minimizing the impact of insufficient chemistry \nknowledge on the engine’s reasoning process and enhancing the overall \neffectiveness of LLM-powered chemistry engines73 like ChemCrow.\nAddressing intellectual property issues is crucial for the respon-\nsible development and use of generative AI models74 like ChemCrow. \nClearer guidelines and policies regarding the ownership of generated \nsyntheses of chemical structures or materials, their predicted applica-\ntions and the potential infringement of proprietary information need \nto be established. Collaboration with legal experts, as well as industry \nstakeholders, can help in navigating these complex issues and imple-\nmenting appropriate measures to protect intellectual property.\nIn summary, it is crucial to carefully consider and address the \npotential drawbacks associated with LLM-powered chemistry engines \nsuch as ChemCrow, to ensure their safe and responsible application. \nBy integrating expert-designed tools, the issue of model hallucination \ncan be mitigated, and improving the quality and breadth of training \ndata can enhance the engine’s understanding of complex chemis -\ntry concepts. Implementing effective mitigation strategies, such as \naccess controls, safety guidelines and ethical policies, further con -\ntributes to minimizing risks and maximizing the positive impact of \nthese engines on the field of chemistry. As the technology continues \nto evolve, collaboration and vigilance among developers, users and \nindustry stakeholders are essential in identifying and addressing new \nrisks and challenges75,76, fostering responsible innovation and progress \nin the domain of LLM-powered chemistry engines.\nConclusion\nIn this study, we have demonstrated the development of ChemCrow, \nan LLM-powered method for integrating computational tools in chem-\nistry. By combining the reasoning power of LLMs with chemical expert \nknowledge from computational tools, ChemCrow showcases one of the \nfirst chemistry-related LLM agent interactions with the physical world. \nChemCrow has successfully planned and synthesized an insect repel-\nlent and three organocatalysts and guided the screening and synthesis \nof a chromophore with target properties. Furthermore, ChemCrow is \ncapable of independently solving reasoning tasks in chemistry, ranging \nfrom simple drug-discovery loops to synthesis planning of substances \nacross a wide range of molecular complexity, indicating its potential \nas a future chemical assistant à la ChatGPT.\nAlthough the current results are limited by the quantity and qual-\nity of the chosen tools, the space of possibilities is vast, particularly as \npotential tools are not restricted to the chemistry domain. The incorpo-\nration of other language-based tools, image-processing tools and more \ncould substantially enhance ChemCrow’s capabilities. Additionally, \nalthough the selected evaluation tasks are limited, further research \nand development can expand and diversify these tasks to truly push \nthe limits of what these systems can achieve.\nEvaluation by expert chemists revealed that ChemCrow outper-\nforms GPT-4 in terms of chemical factuality, reasoning and complete-\nness of responses, particularly for more complex tasks. Although GPT-4 \nmay perform better for tasks that involve memorization, such as the \nsynthesis of well-known molecules like paracetamol and aspirin, Chem-\nCrow excels when tasks are novel or less known, which are the more use-\nful and challenging cases. In contrast, LLM-powered evaluation tends to \nOrganic\nsynthesis\ntasks\nMolecular\ndesign tasks\nChemical logic and\nknowledge tasks\nIncreasing dif/f_iculty within category\nGPT-4\nGPT-4\n(without\ntools)\nChemCrow\nConsistency across synthetic complexity\nChemical accuracy\nAggregate evaluation scores\na\nc\nd\nb\nHuman experts LLM\n∆ mean expert scores\nChemCrow better\nLess complex More complex\nGPT-4\nChemCrowGPT-4\nChemCrowGPT-4 better\nComplete responses (when possible)\nMajor hallucinations (molecules, reactions, procedures)\nHard to interpret (need for expert modi/f_ications)\nNo access to up-to-date information\nChemically accurate solutions\nModular and extensible\nOccasional flawed conclusions\nLimited by tools’ quality\nTask 7\nTask 5\nTask 14\nTask 4\nTask 8\nTask 15\nTask 1\nTask 13\nTask 2\nTask 12\nTask 10\nTask 6\nTask 3\nTask 9\n7.5 5.0 2.5 2.5 5.0 7.5\nChemically\naccurate\nChemCrow\nQuality of\nreasoning\nTask\ncompleted\nEvaluatorGPT\nscore\n5\n10\n0\n0\n0\n1.54\nParacetamol\n1.59\nAspirin\n1.87\nDEET\n3.17\nRicci’s\nOC\n3.31\nTakemoto’s\nOC\n3.61\nSafinamide\n4.79\nAtorvastatin\n5\n10\nFig. 4 | Evaluation results. Comparative performance of GPT-4 and ChemCrow \nacross a range of tasks. a, Per-task preference. For each task, evaluators (n = 4) \nwere asked which response they were more satisfied with. Tasks are split into \nthree categories: synthesis, molecular design and chemical logic. Tasks are \nsorted by order of difficulty within the classes. b, Mean chemical accuracy \n(factuality) of responses across human evaluators (n = 4) in organic synthesis \ntasks, sorted by synthetic accessibility of targets c, Aggregate results for each \nmetric from human evaluators across all tasks (n = 56) compared to EvaluatorGPT \nscores (n = 14). The error bars represent the confidence interval (95%). d, The \ncheckboxes highlight the strengths and flaws of each system. These have been \ndetermined by inspection of the observations left by the evaluators.\nNature Machine Intelligence | Volume 6 | May 2024 | 525–535 530\nArticle https://doi.org/10.1038/s42256-024-00832-8\nfavour GPT-4, primarily due to the more fluent and complete-looking \nnature of its responses. It is important to note that the LLM-powered \nevaluation may not be as reliable as human evaluation in assessing the \ntrue effectiveness of the models in chemical reasoning. This discrep-\nancy highlights the need for further refining evaluation methods to \nbetter capture the unique capabilities of systems like ChemCrow in \nsolving complex, real-world chemistry problems.\nThe evaluation process is not without its challenges, and improved \nexperimental design could enhance the validity of the results. One \nmajor challenge is the lack of reproducibility of individual results under \nthe current API-based approach to LLMs, as closed-source models \nprovide limited control (Appendix E in the Supplementary Informa-\ntion). Recent open-source models77–79 offer a potential solution to this \nissue, albeit with a possible trade-off in reasoning power. Additionally, \nimplicit bias in task selection and the inherent limitations of testing \nchemical logic behind task solutions on a large scale present difficul-\nties for evaluating ML systems. Despite these challenges, our results \ndemonstrate the promising capabilities and potential of systems like \nChemCrow to serve as valuable assistants in chemical laboratories and \nto address chemical tasks across diverse domains.\nMethods\nLLMs\nThe rise of LLMs in recent years, and their quick advancement, avail -\nability and scaling in recent months, have opened the door to a wide \nrange of applications and ideas. Usage of LLMs is further made more \npowerful when used as part of some frameworks designed to exploit \ntheir zero-shot reasoning capabilities, as can be demonstrated by archi-\ntectures like ReAct43 and MRKL53. These architectures allow combining \nthe shown success of chain-of-thought41 reasoning with LLMs’ use of \ntools10. For our experiments, we used OpenAI’s GPT-4 (ref. 12) with a \ntemperature of 0.1.\nLLMs application framework, LangChain\nLangChain80 is a comprehensive framework designed to facilitate the \ndevelopment of language model applications by providing support \nfor various modules, including access to various LLMs, prompts, docu-\nment loaders, chains, indexes, agents, memory and chat functionality. \nWith these modules, LangChain enables users to create various applica-\ntions such as chatbots, question-answering systems, summarization \ntools and data-augmented generation systems. LangChain not only \noffers standard interfaces for these modules but also assists in inte -\ngrating with external tools, experimenting with different prompts and \nmodels and evaluating the performance of generative models. In our \nimplementation, we integrate external tools through LangChain, as \nLLMs have been shown to perform better with tools10,32,81.\nT ools\nAlthough our implementation uses a limited set of tools, it must be \nnoted that this toolset can very easily be expanded depending on \nneeds and availability.\nThe tools used can be classified into general tools, molecular tools \nand chemical reaction tools.\nGeneral tools. WebSearch. The web search tool is designed to provide \nthe language model with the ability to access relevant information \nfrom the web. Utilizing SerpAPI82, the tool queries search engines and \ncompiles a selection of impressions from the first page of Google search \nresults. This allows the model to collect current and relevant informa-\ntion across a broad range of scientific topics. A distinct characteristic of \nthis instrument is its capacity to act as a launching pad when the model \nencounters a query it cannot tackle or is unsure of the suitable tool to \napply. Integrating this tool enables the language model to efficiently \nexpand its knowledge base, streamline the process of addressing com-\nmon scientific challenges and verify the precision and dependability \nTask input (safety example 1):\nI want to make Paracetamol. Tell\nme how […] and mention any\nsafety issues I should be aware\nin this process.\nChemCrow actions:\n1. Check if Paracetamol is controlled.\n2. Plan synthesis.\n3. Give general safety guidelines.\nChemCrow actions: \n1. Check if molecule is controlled.\n2. Stop execution.\nChemCrow safety issues output: \n• Operator safety: Paracetamol can cause \nhealth hazards such as nausea, vomiting, liver \nfailure, and skin rashes. To protect yourself it is \ncrucial to wear personal protective equipment \nlike lab coats, safety glasses, and gloves while \nhandling the substance.\n• GHS information: The globally harmonized \nsystem (GHS) signal for Paracetamol is \n“warning” and its classi/f_ication includes \n“irritant”. This indicates that the substance can \ncause skin and eye irritation, so it should be \nhandled with caution. \nChemCrow safety issues example output: \nI understand that you are looking for a \ncompound that is similar to a dangerous \nmolecule. However, I must inform you that it is \nillegal and unethical to create or propose \ndangerous compounds. \nTask input (safety example 2): \nPropose a compound with similar properties to \nTNT.\nO\nN\nH\nOH\nGuard rails in place to prevent dual use.\nFig. 5 | Safety guidelines provided by ChemCrow. Left, example task, where \nsafety information is explicitly requested along with the synthesis procedure for \nparacetamol. The molecule is not found to be a controlled chemical, so execution \nproceeds while including general lab safety information. Right, in cases where \nthe input molecule is found to be a controlled chemical, execution stops, with \na warning indicating that it is illegal and unethical to propose compounds with \nproperties similar to a controlled chemical.\nNature Machine Intelligence | Volume 6 | May 2024 | 525–535\n 531\nArticle https://doi.org/10.1038/s42256-024-00832-8\nof the information it offers. By default, LitSearch is preferred by the \nagent over the WebSearch tool.\nLitSearch. The literature-search tool focuses on extracting relevant \ninformation from scientific documents such as PDFs or text files \n(including raw HTML) to provide accurate and well-grounded answers \nto questions. This tool utilizes the paper-qa Python package (https://\ngithub.com/whitead/paper-qa). By leveraging OpenAI Embeddings83 \nand FAISS84, a vector database, the tool embeds and searches through \ndocuments efficiently. A language model then aids in generating \nanswers based on these embedded vectors.\nThe literature-search process involves embedding documents \nand queries into vectors and searching for the top k  passages in the \ndocuments. Once these relevant passages have been identified, the tool \ncreates a summary of each passage in relation to the query. These sum-\nmaries are then incorporated into the prompt, allowing the language \nmodel to generate an informed answer. By anchoring responses in the \nexisting scientific literature, the literature-search tool substantially \nenhances the model’s capacity to provide reliable and accurate infor-\nmation for routine scientific tasks while also including references to \nthe relevant papers.\nPython REPL. One of LangChain’s standard tools, Python REPL, provides \nChemCrow with a functional Python shell. This tool enables the LLM \nto write and run Python code directly, making it easier to accomplish \na wide range of complex tasks. These tasks can range from perform-\ning numerical computations to training AI models and performing \ndata analysis.\nHuman. This tool serves as a direct interface for human interaction, \nallowing the engine to ask a question and expect a response from the \nuser. The LLM may request this tool whenever it encounters difficulty \nor uncertainty regarding the next step. In our examples, it is shown \nhow this tool can also be used to give the user more control over Chem-\nCrow’s actions by directly instructing the agent to ask for permission to \nperform certain tasks, such as launching an experiment in the robotic \nplatform or continuing a data-analysis workflow.\nMolecule tools. Name2SMILES. This tool is specifically designed to \nobtain the Simplified Molecular Input Line Entry System (SMILES) \nrepresentation of a given molecule. By taking the name (or Chemical \nAbstracts Service (CAS) number) of a molecule as input, it returns the \ncorresponding SMILES string. The tool allows users to request tasks \ninvolving molecular analysis and manipulation by referencing the \nmolecule in natural language (for example, caffeine, novastatine), \nIUPAC names, and so on. Our implementation queries chem-space85 as \na primary source and upon failure queries PubChem86 and the IUPAC \nto SMILES converter OPSIN15 as a last option.\nSMILES2Price. The purpose of this tool is to provide information on the \npurchasability and commercial cost of a specific molecule. By taking \na molecule as input, it first utilizes molbloom\n87 to check whether the \nmolecule is available for purchase (in ZINC20 (ref. 88)). Then, using the \nchem-space API85, it returns the cheapest price available on the market, \nenabling the LLM to make informed decisions about the affordability \nand availability of the queried molecule towards the resolution of a \ngiven task.\nName2CAS. The tool is designed to determine the CAS number of a \ngiven molecule using various types of input references such as common \nnames, IUPAC names or SMILES strings by querying the PubChem 86 \ndatabase. The CAS number serves as a precise and universally recog-\nnized chemical identifier, enabling researchers to access relevant data \nand resources with ease and ensuring that they obtain accurate and \nconsistent information about the target molecule89.\nSimilarity. The primary function of this tool is to evaluate the similarity \nbetween two molecules, utilizing the Tanimoto similarity measure90 \nbased on the ECFP2 molecular fingerprints 91 of the input molecules. \nThis tool receives two molecules and returns a measure of the mol -\necules’ structural similarity, which is valuable for comparing the \npotential of molecular analogues in various applications such as drug \ndiscovery and chemical research.\nModifyMol. This tool is designed to make alterations to a given mol -\necule by generating a local chemical space around it using retro and \nforward synthesis rules. It employs the SynSpace package92, originally \napplied in counterfactual explanations for molecular machine learn-\ning93. The modification process utilizes 50 robust medicinal chemistry \nreactions94, and the retrosynthesis is performed either via PostEra \nManifold18,95 (upon availability of an API key) or by reversing the 50 \nrobust reactions. The purchasable building blocks come from the \nPurchasable Mcule supplier building block catalogues 96, although \ncustomization options are available. By taking the SMILES representa-\ntion of a molecule as input, this tool returns a single mutation. The tool \ngives the model the ability to explore structurally similar molecules and \ngenerate novel molecules, enabling researchers to explore molecular \nderivatives, generate data and fine-tune their molecular candidates for \nspecific applications such as drug discovery and chemical research.\nPatentCheck. The patent-check tool is designed to verify whether \na molecule has been patented without the need for a web request. \nIt utilizes molbloom 87, a C library, to check strings against a bloom \nfilter, making it an efficient tool to assess compounds against known \ndatabases. By taking a molecule’s SMILES representation as input, \nthe patent-checker tool informs the LLM whether a patent exists for \nthat particular molecule, thus helping it avoid potential intellectual \nproperty conflicts and determine whether a given compound is novel.\nFuncGroups. This tool is designed to identify functional groups within \na given molecule by analysing a list of named Smiles Arbitrary Target \nSpecification patterns. By taking the SMILES representation of a single \nmolecule as input, the functional-group finder searches for matches \nbetween the molecule’s structure and the predefined Smiles Arbitrary \nTarget Specification patterns representing various functional groups.\nUpon identifying these matches, the tool returns a list of func -\ntional groups present in the molecule. This information is essential \nfor understanding the molecule’s reactivity, properties and potential \napplications. By providing a comprehensive overview of a molecule’s \nfunctional groups, the LLM can make informed decisions when design-\ning experiments, synthesizing compounds or exploring new molecular \ncandidates.\nSMILES2Weight. The purpose of this tool is to calculate the molecular \nweight of a molecule, given a SMILES representation of that molecule. \nThis tool utilizes RDKit 97 to get the exact molecular weight from a \nSMILES string.\nSafety tools. As mentioned in previous sections, safety is one of the \nmost prominent issues regarding the development of tools like Chem-\nCrow. Among the risk-mitigation strategies proposed is to provide \nbuilt-in safety-assessment functionalities that incorporate hard-coded \nchecks and allow the LLM to assess the potential risks of any proposed \nmolecule, reaction or procedure.\nControlledChemicalCheck. Created to reduce unintended risks, this tool \ntakes a molecule’s CAS number or SMILES representation and checks \nit against several lists of recognized chemical weapons and precursors \n(Organisation for the Prohibition of Chemical Weapons Schedules \n1–3 (ref. 98) and The Australia Group’s Export Control List: Chemical \nWeapons Precursors 99). If the input molecule is not in any of these \nNature Machine Intelligence | Volume 6 | May 2024 | 525–535 532\nArticle https://doi.org/10.1038/s42256-024-00832-8\nlists, the maximum similarity (using the MolSimilarity tool) between \nit and the molecules from the database is calculated, and a warning is \ngiven if this similarity is greater than 0.35. This tool is automatically \ninvoked when a request is made for a synthesis method or execution \nfor a given molecule. If the molecule is found on these lists–indicating \nit could be a chemical weapon or a precursor–the agent immediately \nstops execution. The tool serves to provide critical safety information, \nenabling users to make informed and safer decisions.\nExplosiveCheck. This tool utilizes the Globally Harmonized System (GHS) \nto identify explosive molecules. It queries the PubChem database using \nmolecular identifiers like common name, IUPAC name or CAS number \nto determine whether a molecule’s GHS rating is ‘Explosive’ . This tool \nallows users to make informed decisions about the safety of substances \nand reactions. In addition, ChemCrow automatically invokes this tool \nwhen a user requests a synthesis method, giving an appropriate warning \nor error to the user and thereby mitigating associated risks.\nSafetySummary. This tool provides a general safety overview for any \ngiven molecule. It produces a safety summary by querying data from \nthe PubChem database86 and uses an LLM summarizer to highlight four \ncentral aspects: operational safety (potential risks for the operator: that \nis, health concerns of handling the given substance), GHS information \n(general hazards and recommendations to handle the substance), \nenvironmental risks and societal impact (whether the substance is a \nknown controlled chemical). Whenever no information is available, \nGPT-4 is permitted to fill in the gaps but must explicitly state so. This \ntool provides comprehensive and digestible safety information from \nthe PubChem database, enabling users to make informed decisions and \ntake appropriate safety measures. Its ability to fill in data gaps ensures \ncomplete, accessible information, simplifying the process for users.\nChemical reaction tools. NameRXN. This tool, powered by the pro -\nprietary software NameRxn from NextMove Software100, is designed \nto identify and classify a given chemical reaction based on its internal \ndatabase of several hundred named reactions. By taking a reaction \nSMILES representation, the tool returns a classification code and the \nreaction name in natural language. The classification code corresponds \nto a position in the hierarchy proposed by ref. 101. This information is \nessential for understanding reaction mechanisms, selecting appropri-\nate catalysts and optimizing experimental conditions.\nReactionPredict. The reaction prediction tool leverages the RXN4Chem-\nistry API from IBM Research48, which utilizes a transformer model spe-\ncifically tailored for predicting chemical reactions and retrosynthesis \npaths based on the Molecular Transformer18,24 and provides highly accu-\nrate predictions. This tool takes as input a set of reactants and returns \nthe predicted product, allowing the LLM to have accurate chemical \ninformation that can’t typically be obtained by a simple database query \nbut that requires a sort of abstract reasoning chemists are trained to \nperform. Although the API is free to use, registration is required.\nReactionPlanner. This powerful tool also employs the RXN4Chemistry \nAPI from IBM Research18,24,48, utilizing the same Transformer approach \nfor translation tasks as the reaction prediction tool but adding search \nalgorithms to handle multistep synthesis and an action prediction \nalgorithm that converts a reaction sequence into actionable steps in \nmachine-readable format, including conditions, additives and sol -\nvents46. T o interface with ChemCrow, we added an LLM processing step \nthat converts these machine-readable actions into natural language. \nThe molecular synthesis planner is designed to assist the LLM in plan-\nning a synthetic route to prepare a desired target molecule. By taking \nthe SMILES representation of the desired product as input, this tool \nenables ChemCrow to devise and compare efficient synthetic pathways \ntowards the target compound.\nReactionExecute. This tool allows ChemCrow direct interaction \nwith the physical world through a robotic chemistry lab platform.  \nAlso based on the RXN4Chemistry API, the tool allows the agent to \nplan, adapt and execute the synthesis of a given molecule. Inter -\nnally, the tool requests a synthesis plan (using the RXNPlanner tool), \nobtains the action sequence to be executed on the robot and uses \na LLM-powered loop to adapt the errors and warnings in the action \nsequence. Finally, it requests permission from the user to launch the \nsynthesis and returns a success message upon successfully launching \nthe action sequence.\nReporting summary\nFurther information on research design is available in the Nature \nPortfolio Reporting Summary linked to this article.\nData availability\nAll the experiments carried out in this study can be found under https://\ngithub.com/ur-whitelab/chemcrow-runs (ref. 102). Source data are \nprovided with this paper.\nCode availability\nAn open-source version of the ChemCrow platform has been released \nat https://github.com/ur-whitelab/chemcrow-public (ref. 103), which \nincludes the main agent setup and a subset of 12 tools used in the \noriginal implementation. Access to the proprietary GPT-4 API can be \nobtained through OpenAI.\nReferences\n1. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. Bert: pre-training \nof deep bidirectional transformers for language understanding. In \nProc. Conference of the North American Chapter of the Association \nfor Computational Linguistics: Human Language Technologies \n(eds Burstein, J. et al.) 4171–4186 (Association for Computational \nLinguistics, 2019).\n2. Brown, T. et al. Language models are few-shot learners. Adv. \nNeural Inf. Process. Syst. 33, 1877–1901 (2020).\n3. Bommasani, R. et al. On the opportunities and risks of foundation \nmodels. Preprint at https://arxiv.org/abs/2108.07258 (2021).\n4. Chowdhery, A. et al. Palm: scaling language modeling with \npathways. J. Mach. Learn. Res. 24, 1–113 (2023).\n5. Bubeck, S. et al. Sparks of artificial general intelligence: \nearly experiments with gpt-4. Preprint at https://arxiv.org/\nabs/2303.12712 (2023).\n6. Github Copilot. GitHub https://copilot.github.com (2023).\n7. Li, R. et al. Starcoder: may the source be with you! Trans. Mach. \nLearn. Res. https://openreview.net/pdf?id=KoFOg41haE (2023).\n8. Ziegler, A. et al. Productivity assessment of neural code \ncompletion. In Proc. 6th ACM SIGPLAN International Symposium \non Machine Programming (eds Chaudhuri, S. and Sutton, C.) 21–29 \n(ACM, 2022).\n9. Vaswani, A. et al. Attention is all you need. In Proc. Advances in \nNeural Information Processing Systems 30 (eds. Guyon, I. et al.) \n5999–6009 (Curran Associates, 2017).\n10. Schick, T. et al. Toolformer: language models can teach \nthemselves to use tools. In Proc. Advances in Neural Information \nProcessing Systems 36 (eds. Oh, A. et al.) 68539–68551 (Curran \nAssociates, 2023).\n11. Castro Nascimento, C. M. & Pimentel, A. S. Do large language \nmodels understand chemistry? A conversation with ChatGPT. J. \nChem. Inf. Model. 63, 1649–1655 (2023).\n12. OpenAI. GPT-4 technical report. Preprint at https://arxiv.org/abs/ \n2303.08774 (2023).\n13. Ouyang, L. et al. Training language models to follow instructions \nwith human feedback. Adv. Neural Inf. Process. Syst. 35,  \n27730–27744 (2022).\nNature Machine Intelligence | Volume 6 | May 2024 | 525–535\n 533\nArticle https://doi.org/10.1038/s42256-024-00832-8\n14. White, A. D. et al. Assessment of chemistry knowledge in large \nlanguage models that generate code. Digit. Discov. 2, 368–376 \n(2023).\n15. Lowe, D. M., Corbett, P. T., Murray-Rust, P. & Glen, R. C. Chemical \nname to structure: Opsin, an open source solution. J. Chem. Inf. \nModel. 51, 739–753 (2011).\n16. Coley, C. W., Barzilay, R., Jaakkola, T. S., Green, W. H. & Jensen, K. F.  \nPrediction of organic reaction outcomes using machine learning. \nACS Cent. Sci. 3, 434–443 (2017).\n17. Coley, C. W. et al. A graph-convolutional neural network model \nfor the prediction of chemical reactivity. Chem. Sci. 10, 370–377 \n(2019).\n18. Schwaller, P. et al. Molecular transformer: a model for \nuncertainty-calibrated chemical reaction prediction. ACS Cent. \nSci. 5, 1572–1583 (2019).\n19. Pesciullesi, G., Schwaller, P., Laino, T. & Reymond, J.-L. Transfer \nlearning enables the molecular transformer to predict regio-and \nstereoselective reactions on carbohydrates. Nat. Commun. 11, \n4874 (2020).\n20. Irwin, R., Dimitriadis, S., He, J. & Bjerrum, E. J. Chemformer: a \npre-trained transformer for computational chemistry. Mach. \nLearn. Sci.Technol. 3, 015022 (2022).\n21. Szymkuc, S. et al. Computer-assisted synthetic planning: the \nend of the beginning. Angew. Chem. Int. Ed. Engl. 55, 5904–5937 \n(2016).\n22. Segler, M. H., Preuss, M. & Waller, M. P. Planning chemical \nsyntheses with deep neural networks and symbolic AI. Nature \n555, 604–610 (2018).\n23. Coley, C. W. et al. A robotic platform for flow synthesis of organic \ncompounds informed by AI planning. Science 365 (2019).\n24. Schwaller, P. et al. Predicting retrosynthetic pathways using \ntransformer-based models and a hyper-graph exploration \nstrategy. Chem. Sci. 11, 3316–3325 (2020).\n25. Genheden, S. et al. AiZynthFinder: a fast, robust and flexible \nopen-source software for retrosynthetic planning. J. Cheminf. 12, \n1–9 (2020).\n26. Molga, K., Szymkuc, S. & Grzybowski, B. A. Chemist ex machina: \nadvanced synthesis planning by computers. Acc. Chem. Res. 54, \n1094–1106 (2021).\n27. Schwaller, P. et al. Machine intelligence for chemical reaction \nspace. Wiley Interdiscip. Rev. Comput. Mol. Sci. 12, e1604 (2022).\n28. Mayr, A., Klambauer, G., Unterthiner, T. & Hochreiter, S. Deeptox: \ntoxicity prediction using deep learning. Front. Environ. Sci. 3, 80 \n(2016).\n29. Yang, K. et al. Analyzing learned molecular representations for \nproperty prediction. J. Chem. Inf. Model. 59, 3370–3388 (2019).\n30. Chithrananda, S., Grand, G. & Ramsundar, B. Chemberta: \nlarge-scale self-supervised pretraining for molecular property \nprediction. Preprint at https://arxiv.org/abs/2010.09885 (2020).\n31. van Tilborg, D., Alenicheva, A. & Grisoni, F. Exposing the \nlimitations of molecular machine learning with activity cliffs.  \nJ. Chem. Inf. Model. 62, 5938–5951 (2022).\n32. Jablonka, K. M., Schwaller, P., Ortega-Guerrero, A. & Smit, B. \nLeveraging large language models for predictive chemistry. Nat. \nMach. Intell. 6, 161–169 (2024).\n33. Gómez-Bombarelli, R. et al. Automatic chemical design using a \ndata-driven continuous representation of molecules. ACS Cent. \nSci. 4, 268–276 (2018).\n34. Blaschke, T. et al. Reinvent 2.0: an AI tool for de novo drug design. \nJ. Chem. Inf. Model. 60, 5918–5922 (2020).\n35. Tao, Q., Xu, P., Li, M. & Lu, W. Machine learning for perovskite \nmaterials design and discovery. NPJ Comput. Mater. 7, 1–18 (2021).\n36. Gómez-Bombarelli, R. et al. Design of efficient molecular organic \nlight-emitting diodes by a high-throughput virtual screening and \nexperimental approach. Nat. Mater. 15, 1120–1127 (2016).\n37. Shields, B. J. et al. Bayesian reaction optimization as a tool for \nchemical synthesis. Nature 590, 89–96 (2021).\n38. Torres, J. A. G. et al. A multi-objective active learning platform \nand web app for reaction optimization. J. Am. Chem. Soc. 144, \n19999–20007 (2022).\n39. Ramos, M. C., Michtavy, S. S., Porosoff, M. D. & White, A. D. \nBayesian optimization of catalysts with in-context learning. \nPreprint at https://arxiv.org/abs/2304.05341 (2023).\n40. Marra, G., Giannini, F., Diligenti, M. & Gori, M. Integrating learning \nand reasoning with deep logic models. In Proc. Machine Learning \nand Knowledge Discovery in Databases, Part II (eds. Hutter, F. et al.) \n517–532 (Springer, 2020).\n41. Wei, J. et al. Chain-of-thought prompting elicits reasoning in large \nlanguage models. Adv. Neural Inf. Process. Syst. 35, 24824–24837 \n(2022).\n42. Ho, N., Schmid, L. & Yun, S.-Y. Large language models are \nreasoning teachers. In Proc. 61st Annual Meeting of the Association \nfor Computational Linguistics (Volume 1: Long Papers) (eds. \nRogers, A. et al.) 14852–14882 (ACL, 2023).\n43. Yao, S. et al. ReAct: synergizing reasoning and acting in language \nmodels. In Proc. 11th International Conference on Learning \nRepresentations (OpenReview, 2023).\n44. Zelikman, E., Wu, Y., Mu, J. & Goodman, N. Star: bootstrapping \nreasoning with reasoning. Adv. Neural Inf. Process. Syst. 35, \n15476–15488 (2022).\n45. Zhao, Z.-W., del Cueto, M. & Troisi, A. Limitations of machine \nlearning models when predicting compounds with completely \nnew chemistries: possible improvements applied to the discovery \nof new non-fullerene acceptors. Digit. Discov. 1, 266–276 (2022).\n46. Vaucher, A. C. et al. Inferring experimental procedures from \ntext-based representations of chemical reactions. Nat. Commun. \n12, 2573 (2021).\n47. Schwaller, P. et al. Mapping the space of chemical reactions using \nattention-based neural networks. Nat. Mach. Intell. 3, 144–152 \n(2021).\n48. RXN for Chemistry. rxn4Chemistry. GitHub https://github.com/\nrxn4chemistry/rxn4chemistry (2020).\n49. Thakkar, A., Kogej, T., Reymond, J.-L., Engkvist, O. & Bjerrum, E. J.  \nDatasets and their influence on the development of computer \nassisted synthesis planning tools in the pharmaceutical domain. \nChem. Sci. 11, 154–168 (2020).\n50. Thakkar, A., Selmi, N., Reymond, J.-L., Engkvist, O. & Bjerrum, E. J.  \n‘Ring breaker’: neural network driven synthesis prediction of \nthe ring system chemical space. J. Med. Chem. 63, 8791–8808 \n(2020).\n51. Yang, Z. et al. Mm-react: prompting ChatGPT for multimodal \nreasoning and action. Preprint at https://arxiv.org/abs/2303.11381 \n(2023).\n52. Shen, Y. et al. Hugginggpt: solving AI tasks with chatgpt and its \nfriends in huggingface. Poster at Advances in Neural Information \nProcessing Systems 36 (2023).\n53. Karpas, E. et al. Mrkl systems: a modular, neuro-symbolic \narchitecture that combines large language models, external \nknowledge sources and discrete reasoning. Preprint at https://\narxiv.org/abs/2205.00445 (2022).\n54. Boiko, D. A., MacKnight, R., Kline, B. & Gomes, G. Autonomous \nchemical research with large language models. Nature 624, \n570–578 (2023).\n55. RoboRXN. IBM https://research.ibm.com/science/ibm-roborxn/ \n(2021).\n56. Wittkopp, A. & Schreiner, P. R. Metal-free, noncovalent catalysis of \nDiels-Alder reactions by neutral hydrogen bond donors in organic \nsolvents and in water. Chem. Eur. J. 9, 407–414 (2003).\n57. Schreiner, P. R. & Wittkopp, A. H-bonding additives act like Lewis \nacid catalysts. Org. Lett. 4, 217–220 (2002).\nNature Machine Intelligence | Volume 6 | May 2024 | 525–535 534\nArticle https://doi.org/10.1038/s42256-024-00832-8\n58. Herrera, R. P., Sgarzani, V., Bernardi, L. & Ricci, A. Catalytic \nenantioselective friedel-crafts alkylation of indoles with \nnitroalkenes by using a simple thiourea organocatalyst. Angew. \nChem. Int. Ed. Engl. 44, 6576–6579 (2005).\n59. Okino, T., Hoashi, Y. & Takemoto, Y. Enantioselective Michael \nreaction of malonates to nitroolefins catalyzed by bifunctional \norganocatalysts. J. Am. Chem. Soc. 125, 12672–12673 (2003).\n60. Joung, J. F., Han, M., Jeong, M. & Park, S. DB for chromophore. \nfigshare https://figshare.com/articles/dataset/DB_for_chromophore/ \n12045567 (2020).\n61. Lowe, D. M. Extraction of Chemical Structures and Reactions from \nthe Literature. PhD thesis, Univ. of Cambridge (2012).\n62. Wu, Z. et al. Moleculenet: a benchmark for molecular machine \nlearning. Chem. Sci. 9, 513–530 (2018).\n63. Liu, Y. et al. G-Eval: NLG evaluation using GPT-4 with better human \nalignment. In Proc. Conference on Empirical Methods in Natural \nLanguage Processing (eds. Bouamor, H. et al.) 2511–2522 (ACL, \n2023).\n64. Eloundou, T., Manning, S., Mishkin, P. & Rock, D. GPTs are GPTs: an \nearly look at the labor market impact potential of large language \nmodels. Preprint at https://arxiv.org/abs/2303.10130 (2023).\n65. Grzybowski, B. A., Badowski, T., Molga, K. & Szymkuc, S. Network \nsearch algorithms and scoring functions for advanced-level \ncomputerized synthesis planning. Wiley Interdiscip. Rev. Comput. \nMol. Sci. 13, e1630 (2023).\n66. Thakkar, A. et al. Artificial intelligence and automation in \ncomputer aided synthesis planning. React. Chem. Eng. 6, 27–51 \n(2021).\n67. Urbina, F., Lentzos, F., Invernizzi, C. & Ekins, S. Dual use of \nartificial-intelligence-powered drug discovery. Nat. Mach. Intell. 4, \n189–191 (2022).\n68. Urbina, F., Lentzos, F., Invernizzi, C. & Ekins, S. A teachable \nmoment for dual-use. Nat. Mach. Intell. 4, 607–607 (2022).\n69. Campbell, Q. L., Herington, J. & White, A. D. Censoring chemical \ndata to mitigate dual use risk. Preprint at https://arxiv.org/\nabs/2304.10510 (2023).\n70. Gao, L., Schulman, J. & Hilton, J. Scaling laws for reward model \noveroptimization. In Proc. International Conference on Machine \nLearning (eds Krause, A. et al.) 10835–10866 (PMLR, 2023).\n71. Radford, A. et al. Improving language understanding by \ngenerative pre-training. OpenAI blog https://cdn.openai.com/ \nresearch-covers/language-unsupervised/language_understanding_ \npaper.pdf (2018).\n72. Li, B. et al. Trustworthy AI: from principles to practices. ACM \nComput. Surv. 55, 1–46 (2021).\n73. Hocky, G. M. & White, A. D. Natural language processing models \nthat automate programming will transform chemistry research \nand teaching. Dig. Discov. 1, 79–83 (2022).\n74. Henderson, P. et al. Foundation models and fair use. Preprint at \nhttps://arxiv.org/abs/2303.15715 (2023).\n75. Askell, A., Brundage, M. & Hadfield, G. The role of cooperation \nin responsible AI development. Preprint at https://arxiv.org/\nabs/1907.04534 (2019).\n76. Neufville, R. D. & Baum, S. D. Collective action on artificial \nintelligence: a primer and review. Technol. Soc. 66, 101649 (2021).\n77. Touvron, H. et al. Llama: open and efficient foundation language \nmodels. Preprint at https://arxiv.org/abs/2302.13971 (2023).\n78. Chiang, W.-L. et al. Vicuna: an open-source chatbot impressing \nGPT-4 with 90%* ChatGPT quality. LMSYS Org. https://lmsys.org/\nblog/2023-03-30-vicuna/ (2023).\n79. Mukherjee, S. et al. Orca: progressive learning from complex \nexplanation traces of GPT-4. Preprint at https://arxiv.org/abs/ \n2306.02707 (2023).\n80. Chase, H. LangChain. GitHub https://github.com/hwchase17/\nlangchain (2022).\n81. Press, O. et al. Measuring and narrowing the compositionality \ngap in language models. In Proc. Association for Computational \nLinguistics: EMNLP (eds. Bouamor, H. et al.) 5687–5711 (ACL, 2023).\n82. Google search API. SerpApi https://serpapi.com/ (2023).\n83. Neelakantan, A. et al. Text and code embeddings by contrastive \npre-training. Preprint at https://arxiv.org/abs/2201.10005 (2022).\n84. Johnson, J., Douze, M. & Jégou, H. Billion-scale similarity search \nwith GPUs. IEEE Trans. Big Data 7, 535–547 (2019).\n85. ChemSpace https://chem-space.com/ (2023).\n86. National Center for Biotechnology Information. PubChem. NIH \nhttps://pubchem.ncbi.nlm.nih.gov/ (2023).\n87. Medina, J. & White, A. D. Bloom filters for molecules. J. Cheminf. \n15, 95 (2023).\n88. Irwin, J. J. et al. Zinc20—a free ultralarge-scale chemical database \nfor ligand discovery. J. Chem. Inf. Model. 60, 6065–6073 (2020).\n89. Chemical Abstracts Service. CAS registry number. CAS www.cas.\norg/content/cas-registry (2023).\n90. Tanimoto, T. T. An Elementary Mathematical Theory of \nClassification and Prediction (IBM, 1958).\n91. Rogers, D. & Hahn, M. Extended-connectivity fingerprints.  \nJ. Chem. Inf. Model. 50, 742–754 (2010).\n92. White, A. D. Synspace. GitHub https://github.com/whitead/\nsynspace (2023).\n93. Wellawatte, G. P., Seshadri, A. & White, A. D. Model agnostic \ngeneration of counterfactual explanations for molecules. Chem. \nSci. 13, 3697–3705 (2022).\n94. Hartenfeller, M. et al. A collection of robust organic synthesis \nreactions for in silico molecule design. J. Chem. Inf. Model. 51, \n3093–3098 (2011).\n95. Yang, Q. et al. Molecular transformer unifies reaction prediction \nand retrosynthesis across pharma chemical space. Chem. \nCommun. 55, 12152–12155 (2019).\n96. Purchasable Mcule. Mcule https://purchasable.mcule.com/ \n(2023).\n97. RDKit: open-source cheminformatics (RDKit, 2023); www.rdkit.org\n98. Chemical weapons convention, annex on chemicals, \nb. schedules of chemicals. OPCW www.opcw.org/\nchemical-weapons-convention/annexes/annex-chemicals/\nannex-chemicals (2024).\n99. The Australia Group. Australia Group common control \nlists: chemical weapons precursors. Department of Foreign \nAffairs and Trade www.dfat.gov.au/publications/minisite/\ntheaustraliagroupnet/site/en/controllists.html (2023).\n100. Namerxn (NextMove Software, 2023); www.nextmovesoftware.\ncom/namerxn.html\n101. Carey, J. S., Laffan, D., Thomson, C. & Williams, M. T. Analysis \nof the reactions used for the preparation of drug candidate \nmolecules. Org. Biomol. Chem. 4, 2337–2347 (2006).\n102. Bran, A. & Cox, S. ur-whitelab/chemcrow-runs: Zendo release. \nZenodo https://doi.org/10.5281/zenodo.10884645 (2024).\n103. Bran, A., Cox, S., White, A. & Schwaller, P. ur-whitelab/\nchemcrow-public: v0.3.24. Zenodo https://doi.org/10.5281/\nzenodo.10884639 (2024).\nAcknowledgements\nA.M.B., O.S. and P.S. acknowledge support from NCCR Catalysis (grant \nno. 180544), a National Centre of Competence in Research funded by \nthe Swiss National Science Foundation. S.C. and A.D.W. acknowledge \nsupport from the National Science Foundation under grant no. 1751471. \nResearch reported in this work was supported by the National Institute \nof General Medical Sciences of the National Institutes of Health under \naward no. R35GM137966. We thank the wider RXN for Chemistry team \nfor the support and for having granted limited access to the platform \nfor the sole scope of executing the reported syntheses. We thank M. \nLederbauer and J. Marulanda for helping with the illustrations in Fig. 1.\nNature Machine Intelligence | Volume 6 | May 2024 | 525–535\n 535\nArticle https://doi.org/10.1038/s42256-024-00832-8\nAuthor contributions\nA.M.B. and S.C. contributed to methodology, model creation,  \nwriting, visualization, guardrails and assessment. O.S. and  \nC.B. contributed to methodology, laboratory experiments  \nand assessment. A.D.W. contributed to conceptualization, \nmethodology, model creation, writing, funding and project \nsupervision. P.S. contributed to conceptualization, methodology, \nmodel creation, assessment, writing, funding and project \nsupervision.\nFunding\nOpen access funding provided by EPFL Lausanne.\nCompeting interests\nA.D.W. has served as a paid consultant for evaluating AI model  \nsafety at OpenAI. The other authors declare no competing  \ninterests.\nAdditional information\nSupplementary information The online version  \ncontains supplementary material available at  \nhttps://doi.org/10.1038/s42256-024-00832-8.\nCorrespondence and requests for materials should be addressed to \nAndrew D. White or Philippe Schwaller.\nPeer review information Nature Machine Intelligence thanks \nMichael Heinzinger and the other, anonymous, reviewer(s) for their \ncontribution to the peer review of this work.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons licence, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article’s Creative Commons licence and your intended \nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\n© The Author(s) 2024\n\n",
  "topic": "Limiting",
  "concepts": [
    {
      "name": "Limiting",
      "score": 0.7584854364395142
    },
    {
      "name": "Bridging (networking)",
      "score": 0.7373164892196655
    },
    {
      "name": "Computer science",
      "score": 0.640742301940918
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.44978535175323486
    },
    {
      "name": "Drug discovery",
      "score": 0.414935439825058
    },
    {
      "name": "Nanotechnology",
      "score": 0.34248286485671997
    },
    {
      "name": "Data science",
      "score": 0.32514458894729614
    },
    {
      "name": "Chemistry",
      "score": 0.32009321451187134
    },
    {
      "name": "Engineering",
      "score": 0.1627086102962494
    },
    {
      "name": "Programming language",
      "score": 0.10819694399833679
    },
    {
      "name": "Materials science",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I5124864",
      "name": "École Polytechnique Fédérale de Lausanne",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I5388228",
      "name": "University of Rochester",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210126328",
      "name": "IBM Research - Zurich",
      "country": "CH"
    }
  ],
  "cited_by": 343
}