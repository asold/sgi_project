{
    "title": "Zero- and Few-Shot NLP with Pretrained Language Models",
    "url": "https://openalex.org/W4285254250",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2800204358",
            "name": "Iz Beltagy",
            "affiliations": [
                "Allen Institute"
            ]
        },
        {
            "id": "https://openalex.org/A1983754593",
            "name": "Arman Cohan",
            "affiliations": [
                "Allen Institute"
            ]
        },
        {
            "id": "https://openalex.org/A4320549284",
            "name": "Robert Logan IV",
            "affiliations": [
                "University of California, Irvine"
            ]
        },
        {
            "id": "https://openalex.org/A2139505590",
            "name": "Se-Won Min",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2143533944",
            "name": "Sameer Singh",
            "affiliations": [
                "University of California, Irvine"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3035542229",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3153427360",
        "https://openalex.org/W3172642864",
        "https://openalex.org/W4205991051",
        "https://openalex.org/W3098267758",
        "https://openalex.org/W4298343034",
        "https://openalex.org/W4288376504",
        "https://openalex.org/W3166986030",
        "https://openalex.org/W3173777717",
        "https://openalex.org/W4205857304",
        "https://openalex.org/W3166846774",
        "https://openalex.org/W4286961470",
        "https://openalex.org/W3182696977",
        "https://openalex.org/W3118781290",
        "https://openalex.org/W4287867774",
        "https://openalex.org/W3173617765",
        "https://openalex.org/W4286987939",
        "https://openalex.org/W3205068155",
        "https://openalex.org/W4287075708",
        "https://openalex.org/W3196731672",
        "https://openalex.org/W3164972323",
        "https://openalex.org/W3172943453",
        "https://openalex.org/W4300996741",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3167602185",
        "https://openalex.org/W2963809228",
        "https://openalex.org/W4206214875",
        "https://openalex.org/W3174770825",
        "https://openalex.org/W4287026929",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W2962369866",
        "https://openalex.org/W3152515526"
    ],
    "abstract": "The ability to efficiently learn from little-to-no data is critical to applying NLP to tasks where data collection is costly or otherwise difficult. This is a challenging setting both academically and practically—particularly because training neutral models typically require large amount of labeled data. More recently, advances in pretraining on unlabelled data have brought up the potential of better zero-shot or few-shot learning (Devlin et al., 2019; Brown et al., 2020). In particular, over the past year, a great deal of research has been conducted to better learn from limited data using large-scale language models. In this tutorial, we aim at bringing interested NLP researchers up to speed about the recent and ongoing techniques for zero- and few-shot learning with pretrained language models. Additionally, our goal is to reveal new research opportunities to the audience, which will hopefully bring us closer to address existing challenges in this domain.",
    "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 32 - 37\nMay 22-27, 2022 ©2022 Association for Computational Linguistics\nZero- and Few-Shot NLP with Pretrained Language Models\nIz Beltagy† Arman Cohan†∗ Robert L. Logan IV‡ Sewon Min∗ Sameer Singh‡\n†Allen Institute for AI, Seattle, W A ‡University of California, Irvine\n∗Paul G. Allen School, University of Washington, Seattle, W A\n{beltagy, armanc} @allenai.org\n{rlogan, sameer} @uci.edu\nsewon@cs.washington.edu\n1 Introduction\nThe ability to efficiently learn from little-to-no data\nis critical to applying NLP to tasks where data\ncollection is costly or otherwise difficult. This\nis a challenging setting both academically and\npractically—particularly because training neutral\nmodels typically require large amount of labeled\ndata. More recently, advances in pretraining on un-\nlabelled data have brought up the potential of better\nzero-shot or few-shot learning (Devlin et al., 2019;\nBrown et al., 2020). In particular, over the past\nyear, a great deal of research has been conducted\nto better learn from limited data using large-scale\nlanguage models.\nIn this tutorial, we aim at bringing interested\nNLP researchers up to speed about the recent and\nongoing techniques for zero- and few-shot learning\nwith pretrained language models. Additionally, our\ngoal is to reveal new research opportunities to the\naudience, which will hopefully bring us closer to\naddress existing challenges in this domain.\nThe detailed content of the tutorial is described\nin Section 2. The tutorial will start by motivat-\ning the challenge of learning from limited data,\nand providing an overview of historical few-shot\nNLP techniques. The tutorial will then start mainly\nfocusing on recent few-shot learning methods us-\ning language models. It will cover methods from\nmanual engineering, better inference algorithms\nto better tuning methods. We will then discuss\nthe impact of different pretraining objectives, and\nmeta-training strategies. Lastly, we will survey the\ncurrent landscape of evaluation benchmarks, and\ntheir limitations. We will conclude the tutorial by\nsuggesting open questions, and providing coding\nexamples and web-based demonstrations instruct-\ning attendees how to easily use these methods using\npublic resources.\n2 Tutorial Content and Outline\nThis tutorial covers methods for zero- and\nfew-shot learning with pretrained language mod-\nels (LMs). The tutorial will be 3 hours\nlong. Tutorial materials will be made avail-\nable at: https://github.com/allenai/\nacl2022-zerofewshot-tutorial.\nIntroduction - (10 minutes) We will start by\nmotivating why zero- and few-shot learning are\nimportant. In many situations, labelled data may\nbe costly or otherwise difficult to procure. Lan-\nguage model finetuning, the predominant training\nparadigm in use today, exhibits poor performance\nin low-data regimes (Dodge et al., 2020). Further-\nmore, as LMs continue to grow in size, so do the\nassociated costs of training and storing separate\nweights for each downstream task. Recent work\non zero- and few-shot learning with pretrained lan-\nguage models can provide a potential solution.\nEarlier work - (15 minutes) In the second sec-\ntion, we will review well-established methods for\nzero- and few-shot learning that do not necessar-\nily use LMs, including data augmentation, semi-\nsupervised learning, consistency training and co-\ntraining (Miyato et al., 2017; Clark et al., 2018; Xie\net al., 2020; Chen et al., 2020).\nLanguage models as few-shot learners - (20 min-\nutes) In the third section, we will focus on few-\nshot approaches using LMs without any tuning.\nThe fundamental observation in this section is that,\nby reformulating tasks as complete-the-sentence\nproblems and potentially including training exam-\nples in-context, large pretrained language models\ncan be used to solve NLP tasks without having\nto resort to finetuning. We will survey a few key\npapers, notably GPT-3 (Brown et al., 2020), and\nfollow up work demonstrating the limitations of in-\ncontext learning (Perez et al., 2021). We will also\ndiscuss alternative approaches for calibrating and\n32\nscoring LM outputs (Zhao et al., 2021; Holtzman\net al., 2021; Min et al., 2021).\nPrompt-based finetuning - (25 minutes) In the\nnext section, we will discuss prompt-based fine-\ntuning, which relaxes the restriction that the LM\nweights cannot be updated. We will introduce the\ntechnique of pattern exploiting training (Schick\nand Schütze, 2021a,b; Le Scao and Rush, 2021,\nPET) which utilizes manually written cloze style\nprompts in conjunction with language model fine-\ntuning to attain higher accuracy and improved sta-\nbility over the finetuning approach proposed by\nDevlin et al. (2019). We will then discuss a variety\nof related works that seek to streamline PET (Tam\net al., 2021; Logan IV et al., 2021). In particular\nwe will cover methods that try to automate the task\nof prompt-construction, either in the vocabulary\nspace (Shin et al., 2020; Gao et al., 2021b), or the\nembedding space (Li and Liang, 2021; Lester et al.,\n2021; Zhong et al., 2021; Qin and Eisner, 2021).\nWe will contrast these methods with non-tuning\nmethods covered in the previous section, in terms\nof their performance, memory and computation re-\nquirement, amount of required engineering, and\nmore.\nPretraining - (20 minutes) The following sec-\ntion will focus on the factor underlying the suc-\ncess of these methods—language model pretrain-\ning. First, we will provide a review of popular\nlanguage model pretraining objectives and architec-\ntures. Topics will include: causal (Radford et al.,\n2019) vs. masked (Devlin et al., 2019) pretraining,\nencoder-only (Devlin et al., 2019; Liu et al., 2019)\nvs. decoder-only (Radford et al., 2019) vs. encoder-\ndecoder architectures (Lewis et al., 2020; Raffel\net al., 2020), and the impact of training data (Agha-\njanyan et al., 2021; Saxton et al., 2019; Gao et al.,\n2021a).\nMeta-training - (25 minutes) Next we will dis-\ncuss meta-training approaches that train the LM\nto adapt to zero- and few-shot use cases. A vari-\nety of work has demonstrated that transfer learning\nis extremely effective when trained on a diverse\nset of tasks and prompts (Wei et al., 2021; Sanh\net al., 2021). Furthermore, recent papers propose\nto learn from instructions where the model is given\ninstructions that humans would often read when\nperforming a new task, e.g., in a crowdsourcing\ntask (Efrat and Levy, 2020; Mishra et al., 2021).\nEvaluation benchmarks - (25 minutes) We will\nthen discuss few-shot evaluation benchmarks such\nas FLEX (Bragg et al., 2021), FewNLU (Zheng\net al., 2021), The BIG-Bench (BIG-bench collab-\noration, 2021) and CrossFit (Ye et al., 2021). We\nwill discuss the problems in existing evaluations\nand how new few-shot evaluation benchmarks were\ncarefully designed to measure a variety of scopes\nin generalization. We will also cover benchmarks\nspecifically for instruction learning (Efrat and Levy,\n2020; Mishra et al., 2021).\nOpen questions and future work - (20 minutes)\nThe future work section will discuss open ques-\ntions and future research directions like the need\nfor multilingual evaluation data, challenges in eval-\nuation, reducing engineering efforts and variance\nand more.\nCoding example - (20 minutes) Finally, we\nwill demonstrate code examples for representative\nfew-shot methods using the most widely-used li-\nbraries/APIs at the time of the event, such as the\nTransformers library. This will help audience to\neasily use publicly available resources for real-\nworld few-shot applications.\n3 Type of the Tutorial\nThis tutorial will cover cutting-edge research in\nzero- and few-shot learning with pretrained lan-\nguage models. This topic has not been previously\ncovered in *CL tutorials.\n4 Breadth\nThe tutorial covers a diverse set of topics related to\nzero- and few-shot learning including pretraining,\nprompting, finetuning, evaluation, open research\nquestions, etc. The tutorial also briefly discusses\npre-language models work but not in depth. Note\nthat most of the work we will cover is not authored\nby the presenters.\n5 Diversity Considerations\nThe methods and techniques we are going to\npresent are language-agnostic and can be easily\napplied to non-English data and tasks. Zero- and\nfew-shot learning can be relevant for low-resource\nlanguages and tasks (assuming there exist unla-\nbeled resources to build a pretrained model). The\ntutorial covers work from diverse groups, both geo-\ngraphically (America, Europe, Asia) and gender.\n33\nFor instructors, three are senior and two are ju-\nnior NLP researchers, one is female, and they rep-\nresent two universities and one industry research\nlab.\n6 Prerequisites\nWe assume attendees are familiar with:\n• Machine Learning: Basic knowledge of com-\nmon recent neural network architectures, par-\nticularly Transformers.\n• Computational linguistics: Familiarity with\nthe concept of pretrained language models, as\nwell as standard NLP tasks such as text clas-\nsification, natural language generation, and\nquestion answering.\n7 Reading List\nReading the following papers is nice to have but\nnot required for attendance.\n• Language Models are Few-Shot Learn-\ners (Brown et al., 2020)\n• It’s Not Just Size That Matters: Small Lan-\nguage Models Are Also Few-Shot Learn-\ners (Schick and Schütze, 2021b)\n• Finetuned Language Models are Zero-Shot\nLearners (Wei et al., 2021)\n• FLEX: Unifying Evaluation for Few-Shot\nNLP (Bragg et al., 2021)\n8 Instructors\nIn alphabetical order,\nIz Beltagy Iz Beltagy is a Research Scientist at\nAI2 focusing on language modeling, transfer learn-\ning, summarization, explainability and efficiency.\nHis research has been recognized with a best paper\nhonorary mention at ACL 2020 and an outstand-\ning paper award at AKBC 2021. He was a co-\ninstructor of the tutorial on “Beyond Paragraphs:\nNLP for Long Sequences” (NAACL-HLT 2021).\nHe worked as a Teaching Assistant at the Univer-\nsity of Texas at Austin teaching computer science.\nEmail: beltagy@allenai.org\nHomepage: beltagy.net\nArman Cohan Arman Cohan is a Research Sci-\nentist at AI2 and an Affiliate Assistant Professor at\nUniversity of Washington, focusing on representa-\ntion learning and transfer learning methods, as well\nas NLP applications in specialized domains and sci-\nentific text. His research has been recognized with\na best paper award at EMNLP 2017, an honorable\nmention at COLING 2018, and Harold N. Glass-\nman Distinguished Doctoral Dissertation award in\n2019. He was a co-instructor of the tutorial on\n“Beyond Paragraphs: NLP for Long Sequences”\n(NAACL-HLT 2021).\nEmail: armanc@allenai.org\nHomepage: armancohan.com\nRobert L. Logan IV Robert L. Logan IV is a\nPh.D. student at the University of California, Irvine,\nadvised by Sameer Singh and Padhraic Smyth. His\nresearch focuses on problems at the intersection\nof information extraction and language modeling,\nand encompasses recently published work on lan-\nguage model prompting that is relevant to this\nproposal. He has presented invited talks at the\nSoCal NLP Symposium (2019), the CHASE-CI\nWorkshop (2019), and the UCI Center for Machine\nLearning Seminar (2021).\nEmail: rlogan@uci.edu\nHomepage: rloganiv.github.io\nSewon Min Sewon Min is a Ph.D. student in the\nPaul G. Allen School of Computer Science & En-\ngineering at the University of Washington, advised\nby Hannaneh Hajishirzi and Luke Zettlemoyer. Her\nresearch focuses on natural language understand-\ning, question answering, and knowledge represen-\ntation. She was a co-instructor of the tutorial on\n“Beyond Paragraphs: NLP for Long Sequences”\n(NAACL-HLT 2021), and was a co-organizer of the\n3rd Workshop on Machine Reading for Question\nAnswering (EMNLP 2021), Competition on Effi-\ncient Open-domain Question Answering (NeurIPS\n2020), and Workshop on Structured and Unstruc-\ntured KBs (AKBC 2020, 2021).\nEmail: sewon@cs.washington.edu\nHomepage: shmsw25.github.io\nSameer Singh Sameer Singh is an Associate Pro-\nfessor of Computer Science at the University of\nCalifornia, Irvine and an Allen AI Fellow at the\nAllen Institute for AI. He is working on large-scale\nand interpretable machine learning models for NLP.\nHis work has received paper awards at ACL 2020,\nAKBC 2020, EMNLP 2019, ACL 2018, and KDD\n34\n2016. Sameer has presented a number of tutori-\nals, many relevant to this proposal, such as Deep\nAdversarial Learning Tutorial at NAACL 2019,\nMining Knowledge Graphs from Text Tutorial at\nWSDM 2018 and AAAI 2017, tutorial on Inter-\npretability and Explanations in NeurIPS 2020 and\nEMNLP 2020, and tutorial on Robustness in NLP\nat EMNLP 2021. Sameer has also received teach-\ning awards at UCI.\nEmail: sameer@uci.edu\nHomepage: http://sameersingh.org/\n9 Ethical Statement\nThis tutorial covers work that extensively uses large\n(up to hundreds of billions of parameters) language\nmodels, which are associated with substantial finan-\ncial and environmental costs (Strubell et al., 2019),\nas well as other harms (Bender et al., 2021).\nReferences\nArmen Aghajanyan, Dmytro Okhonko, Mike Lewis,\nMandar Joshi, Hu Xu, Gargi Ghosh, and Luke Zettle-\nmoyer. 2021. HTLM: Hyper-Text Pre-Training and\nPrompting of Language Models. ArXiv preprint,\nabs/2107.06955.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language mod-\nels be too big? In Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency, FAccT ’21, page 610–623, New York, NY ,\nUSA. Association for Computing Machinery.\nBIG-bench collaboration. 2021. Beyond the imitation\ngame: Measuring and extrapolating the capabilities\nof language models. In preparation.\nJonathan Bragg, Arman Cohan, Kyle Lo, and Iz Belt-\nagy. 2021. FLEX: Unifying Evaluation for Few-Shot\nNLP. ArXiv preprint, abs/2107.07170.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nJiaao Chen, Zichao Yang, and Diyi Yang. 2020. Mix-\nText: Linguistically-informed interpolation of hid-\nden space for semi-supervised text classification. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 2147–\n2157, Online. Association for Computational Lin-\nguistics.\nKevin Clark, Minh-Thang Luong, Christopher D. Man-\nning, and Quoc Le. 2018. Semi-supervised sequence\nmodeling with cross-view training. In Proceedings\nof the 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 1914–1925, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah Smith. 2020.\nFine-tuning pretrained language models: Weight ini-\ntializations, data orders, and early stopping. ArXiv\npreprint, abs/2002.06305.\nAvia Efrat and Omer Levy. 2020. The Turking Test: Can\nLanguage Models Understand Instructions? ArXiv\npreprint, abs/2010.11982.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2021a.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. ArXiv preprint, abs/2101.00027.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021b.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nAri Holtzman, Peter West, Vered Shwartz, Yejin Choi,\nand Luke Zettlemoyer. 2021. Surface form com-\npetition: Why the highest probability answer isn’t\nalways right. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 7038–7051, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nTeven Le Scao and Alexander Rush. 2021. How many\ndata points is a prompt worth? In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2627–2636,\nOnline. Association for Computational Linguistics.\n35\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582–\n4597, Online. Association for Computational Lin-\nguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. ArXiv preprint, abs/1907.11692.\nRobert L. Logan IV , Ivana Balaževi ´c, Eric Wallace,\nFabio Petroni, Sameer Singh, and Sebastian Riedel.\n2021. Cutting Down on Prompts and Parameters:\nSimple Few-Shot Learning with Language Models.\nArXiv preprint, abs/2106.13353.\nSewon Min, Mike Lewis, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2021. Noisy Channel Language\nModel Prompting for Few-Shot Text Classification.\nArXiv preprint, abs/2108.04106.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2021. Cross-Task Generaliza-\ntion via Natural Language Crowdsourcing Instruc-\ntions. ArXiv preprint, abs/2104.08773.\nTakeru Miyato, Andrew M. Dai, and Ian J. Goodfel-\nlow. 2017. Adversarial training methods for semi-\nsupervised text classification. In 5th International\nConference on Learning Representations, ICLR 2017,\nToulon, France, April 24-26, 2017, Conference Track\nProceedings. OpenReview.net.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.\nTrue Few-Shot Learning with Language Models.\nArXiv preprint, abs/2105.11447.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying LMs with mixtures of soft prompts.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5203–5212, Online. Association for Computa-\ntional Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the Lim-\nits of Transfer Learning with a Unified Text-to-Text\nTransformer. Journal of Machine Learning Research,\n21(140):1–67.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun\nRaja, et al. 2021. Multitask prompted training en-\nables zero-shot task generalization. ArXiv preprint,\nabs/2110.08207.\nDavid Saxton, Edward Grefenstette, Felix Hill, and\nPushmeet Kohli. 2019. Analysing mathematical rea-\nsoning abilities of neural models. In7th International\nConference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019. OpenRe-\nview.net.\nTimo Schick and Hinrich Schütze. 2021a. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 255–269, Online. Association for Computa-\ntional Linguistics.\nTimo Schick and Hinrich Schütze. 2021b. It’s not just\nsize that matters: Small language models are also few-\nshot learners. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2339–2352, Online. Association\nfor Computational Linguistics.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric\nWallace, and Sameer Singh. 2020. AutoPrompt: Elic-\niting Knowledge from Language Models with Auto-\nmatically Generated Prompts. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4222–4235,\nOnline. Association for Computational Linguistics.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3645–3650, Florence, Italy. Asso-\nciation for Computational Linguistics.\nDerek Tam, Rakesh R. Menon, Mohit Bansal, Shashank\nSrivastava, and Colin Raffel. 2021. Improving and\nsimplifying pattern exploiting training. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 4980–4991,\n36\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2021. Finetuned\nLanguage Models Are Zero-Shot Learners. ArXiv\npreprint, abs/2109.01652.\nQizhe Xie, Zihang Dai, Eduard H. Hovy, Thang Luong,\nand Quoc Le. 2020. Unsupervised data augmenta-\ntion for consistency training. In Advances in Neural\nInformation Processing Systems 33: Annual Confer-\nence on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual.\nQinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021.\nCrossFit: A few-shot learning challenge for cross-\ntask generalization in NLP. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 7163–7189, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In\nProceedings of the 38th International Conference on\nMachine Learning, ICML 2021, 18-24 July 2021, Vir-\ntual Event, volume 139 of Proceedings of Machine\nLearning Research, pages 12697–12706. PMLR.\nYanan Zheng, Jing Zhou, Yujie Qian, Ming Ding,\nJian Li, Ruslan Salakhutdinov, Jie Tang, Sebastian\nRuder, and Zhilin Yang. 2021. FewNLU: Bench-\nmarking State-of-the-Art Methods for Few-Shot\nNatural Language Understanding. ArXiv preprint,\nabs/2109.12742.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [MASK]: Learning vs. learning\nto recall. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 5017–5033, Online. Association\nfor Computational Linguistics.\n37"
}