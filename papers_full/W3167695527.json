{
  "title": "Vision Transformers with Patch Diversification",
  "url": "https://openalex.org/W3167695527",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222271214",
      "name": "Gong, Chengyue",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226167183",
      "name": "Wang, Dilin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1979955179",
      "name": "Li Meng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2753958842",
      "name": "Chandra, Vikas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1960924044",
      "name": "Liu Qiang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2981525344",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W3014641072",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W3132890542",
    "https://openalex.org/W2737258237",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2803676284",
    "https://openalex.org/W2037386840",
    "https://openalex.org/W3126721948",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W2998508940",
    "https://openalex.org/W2998496395",
    "https://openalex.org/W3166513219",
    "https://openalex.org/W3007125775",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2340897893",
    "https://openalex.org/W3016719260",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W3109301572",
    "https://openalex.org/W3145444543",
    "https://openalex.org/W3137963805",
    "https://openalex.org/W3136635488",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2884822772",
    "https://openalex.org/W3010476030",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W3153842237",
    "https://openalex.org/W2966610483",
    "https://openalex.org/W3155420132",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3119686997",
    "https://openalex.org/W3147387781",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W3129603602",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W3083504878",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3035452548",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3132503749",
    "https://openalex.org/W2963727650",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3146097248",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W3102631365",
    "https://openalex.org/W3034634365",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W3128633047"
  ],
  "abstract": "Vision transformer has demonstrated promising performance on challenging computer vision tasks. However, directly training the vision transformers may yield unstable and sub-optimal results. Recent works propose to improve the performance of the vision transformers by modifying the transformer structures, e.g., incorporating convolution layers. In contrast, we investigate an orthogonal approach to stabilize the vision transformer training without modifying the networks. We observe the instability of the training can be attributed to the significant similarity across the extracted patch representations. More specifically, for deep vision transformers, the self-attention blocks tend to map different patches into similar latent representations, yielding information loss and performance degradation. To alleviate this problem, in this work, we introduce novel loss functions in vision transformer training to explicitly encourage diversity across patch representations for more discriminative feature extraction. We empirically show that our proposed techniques stabilize the training and allow us to train wider and deeper vision transformers. We further show the diversified features significantly benefit the downstream tasks in transfer learning. For semantic segmentation, we enhance the state-of-the-art (SOTA) results on Cityscapes and ADE20k. Our code is available at https://github.com/ChengyueGongR/PatchVisionTransformer.",
  "full_text": "Vision Transformers with Patch Diversiﬁcation\nChengyue Gong1, Dilin Wang 2, Meng Li 2, Vikas Chandra 2, Qiang Liu 1\n1 University of Texas at Austin 2 Facebook\n{cygong, lqiang}@cs.utexas.edu, {wdilin, meng.li, vchandra}@fb.com\nAbstract\nVision transformer has demonstrated promising performance on challenging computer vision tasks.\nHowever, directly training the vision transformers may yield unstable and sub-optimal results. Recent\nworks propose to improve the performance of the vision transformers by modifying the transformer\nstructures, e.g., incorporating convolution layers. In contrast, we investigate an orthogonal approach\nto stabilize the vision transformer training without modifying the networks. We observe the instability\nof the training can be attributed to the signiﬁcant similarity across the extracted patch representations.\nMore speciﬁcally, for deep vision transformers, the self-attention blocks tend to map different patches\ninto similar latent representations, yielding information loss and performance degradation. To alleviate\nthis problem, in this work, we introduce novel loss functions in vision transformer training to explicitly\nencourage diversity across patch representations for more discriminative feature extraction. We empirically\nshow that our proposed techniques stabilize the training and allow us to train wider and deeper vision\ntransformers. We further show the diversiﬁed features signiﬁcantly beneﬁt the downstream tasks in transfer\nlearning. For semantic segmentation, we enhance the state-of-the-art (SOTA) results on Cityscapes and\nADE20k to 83.6 and 54.5 mIoU, respectively. Our code is available at https://github.com/\nChengyueGongR/PatchVisionTransformer.\n1 Introduction\nRecently, vision transformers have demonstrated promising performance on various challenging computer\nvision tasks, including image classiﬁcation (Dosovitskiy et al., 2020), object detection (Zhu et al., 2020;\nCarion et al., 2020), multi-object tracking (Meinhardt et al., 2021), image generation (Jiang et al., 2021a) and\nvideo understanding (Bertasius et al., 2021). Compared with highly optimized convolutional neural networks\n(CNNs), e.g., ResNet (He et al., 2016) and EfﬁcientNet (Tan and Le, 2019), transformer encourages non-local\ncomputation and achieves comparable, and even better performance when pre-trained on large scale datasets.\nFor a vision transformer, an image is usually split into patches and the sequence of linear embeddings of\nthese patches are provided as the input to the stacked transformer blocks (Dosovitskiy et al., 2020). A vision\ntransformer can learn the patch representations effectively using the self-attention block, which aggregates\nthe information across the patches (Vaswani et al., 2017). The learned patch representations are then used for\nvarious vision tasks, such as image classiﬁcation, image segmentation, and object detection. Hence, learning\nhigh-quality and informative patch representations becomes key to the success of a vision transformer.\nThough promising results on vision tasks have been demonstrated for vision transformers, it is found\nthat the training of vision transformers is not very stable, especially when the model becomes wider and\ndeeper (Touvron et al., 2021). To understand the origin of the training instability, we use two popular vision\ntransformer variants, i.e., DeiT (Touvron et al., 2020) and SWIN-Transformer (Liu et al., 2021), and study the\n1\narXiv:2104.12753v3  [cs.CV]  11 Jun 2021\nextracted patch representations of each self-attention layer. We ﬁnd the average patch-wise absolute cosine\nsimilarity between patch representations increases signiﬁcantly for late layers in both two models. For a\n24-layer DeiT-Base model, the cosine similarity can reach more than 0.7 after the last layer. This indicates a\nhigh correlation and duplication among the learned patch representations. Such behavior is undesired as it\ndegrades overall representation power of patch representations and reduces the learning capacity of powerful\nvision transformers. Our ﬁndings shed light on the empirical results found by (Touvron et al., 2021), and may\npartially explain the reason why simply increasing the depth of standard vision transformers cannot boost the\nmodel performance.\nTo alleviate the problem in vision transformers, we propose three different techniques. First, We propose\nto directly promote the diversity among different patch representations by penalizing the patch-wise cosine\nsimilarity. Meanwhile, we observe the input patch representations to the ﬁrst self-attention layer are often\nmore diversiﬁed as they rely solely on the input pixels. Based on this observation, we propose a patch-wise\ncontrastive loss to encourage the learned representations of the same patch to be similar between the ﬁrst and\nlayer layers while force the representations of different patches in one given image to be different.\nThirdly, we propose a patch-wise mixing loss. Similar to Cutmix (Yun et al., 2019), we mix the input\npatches from two different images and use the learned patch representations from each image to predict its\ncorresponding class labels. With the patch-wise mixing loss, we are forcing the self-attention layers to only\nattend to patches that are most relevant to its own category, and hence, learning more discriminating features.\nEmpirically, leveraging the proposed diversity-encouraging training techniques, we signiﬁcantly improve the\nimage classiﬁcation for standard vision transformers on ImageNet without any architecture modiﬁcations\nor any extra data. Speciﬁcally, we achieve 83.3% top-1 accuracy on ImageNet with an input resolution of\n224×224 for DeiT. We also ﬁnetune the checkpoint trained on ImageNet-22K from SWIN-Transformer (Liu\net al., 2021) and achieve 87.4% top-1 accuracy on ImageNet. By transferring the backbone model to semantic\nsegmentation, we enhance the SOTA results on Cityscapes and ADE20k valid set to 83.6 and 54.5 mIoU,\nrespectively.\n2 Preliminaries: Vision Transformers\nIn this section, we give a brief introduction to vision transformers (Dosovitskiy et al., 2020). Given a training\nexample (x,y), where x and ydenote the input image and the label, respectively. A vision transformer ﬁrst\nsplits x into a set of non-overlapping patches, i.e., x = (x1,··· ,xn), where xi denotes a patch and nis\nthe total number of patches. Each patch xi is then transformed into a latent representation via a projection\nlayer, and augmented with a position embedding. A learnable class patch is introduced to capture the\nlabel information. During training, a vision transformer model gradually transforms patch representations\nh[ℓ] = (h[ℓ]\nclass,h[ℓ]\n1 ,··· ,h[ℓ]\nn ) with a stack of self-attention layers (Vaswani et al., 2017). Here, ℓdenotes\nthe layer index and h[ℓ]\nclass and h[ℓ]\ni denote the learned class patch and the image patch of xi at the ℓ-th\nlayer, respectively. Let Ldenote the total number of layers and gdenote a classiﬁcation head. The vision\ntransformer is trained by minimizing a classiﬁcation loss L(g(h[L]\nclass),y). See Figure 1 (a) for an overview of\nvision transformers. It has been reported that the training of vision transformer can suffer from instability\nissue, especially for deep models (Touvron et al., 2020).\n2\nPatch \nprojection \nlayer\n.\n.\n.\n[class] patch\n+ Position \nEmbeddings\nSelf-attention \nLayers\n.\n.\n.\nClassiﬁcation head\nPatch \nrepresentations\nCosine similarity\n0 1 2 3 40.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nDEIT-Base24\nSWIN-Base\nResNet-50\nBlock index\n(a) An illustration of vision transformers (b) Pairwise absolute cosine similarity\nFigure 1: (a) An overview of vision transformers by following (Dosovitskiy et al., 2020). Each image patch\nis ﬁrst transformed to a latent representation using a linear patch projection layer. The dog image is from\nImageNet (Deng et al., 2009). (b) Comparisons of patch-wise absolute cosine similarities. All similarities are\ncomputed with 10,000 sub-sampled images from the ImageNet training set without data augmentation.\n3 Examining Patch Diversity in Vision Transformers\nTo understand the origin of the training instability, we study the patch representations learned after each\nself-attention layer. Ideally, we would hope the patch representations to be diverse and capture different\ninformation from the inputs. We study the diversity of the patch representations by computing the patch-wise\nabsolute cosine similarity. Consider a sequence of patch representations h = [hclass,h1,··· ,hn]. We deﬁne\nthe patch-wise absolute cosine similarity as\nP(h) = 1\nn(n−1)\n∑\ni̸=j\n|h⊤\ni hj|\n∥hi ∥2∥hj ∥2\n.\nHere we ignore the class patch. Larger values of P(h) indicate a higher correlation among patches and vice\nversa.\nWe test two variants of vision transformers pre-trained on the ImageNet dataset (Deng et al., 2009), including\na 24-layer DeiT-Base model (Touvron et al., 2020) (denoted as DeiT-Base24) and a SWIN-Base (Liu et al.,\n2021) model. We also evaluate a ResNet-50 (He et al., 2016) model for comparison. For the DeiT-Base24\nmodel, we uniformly select 5 layers to compute the patch-wise cosine similarity. For the SWIN-Base model\nand ResNet-50 model, we select the input representation to each down-sampling layer. Speciﬁcally for the\nResNet-50 model, we regard the representations at each spatial location as an individual patch.\nAs shown in Figure 1 (b), the patch-wise cosine similarity P(·) of the patch representations learned by\nDeiT-Base24 and SWIN-Base gradually increases with the the depth of layers. For DeiT-Base24, the average\ncosine similarity becomes larger than 0.7 for the representations after the last layer. In contrast, ResNet-50\nlearns relative diversiﬁed features across the network without an obvious increase of the patch-wise cosine\nsimilarity. Such high similarity across the learned patch representations is undesired as it degrades the\nlearning capacity of the vision transformer models and limits the actual information extracted from the input\nimages. There is also risk for the patch representations to degenerate with the increase of model depth, which\nmay partially explain the high training instability of deep models.\n3\n4 DiversePatch: Promoting Patch Diversiﬁcation for Vision Transformers\nTo alleviate the observed problem, we propose three regularization techniques to encourage diversity across\nthe learned patch representations.\nPatch-wise cosine loss As a straightforward solution, we propose to directly minimize the patch-wise\nabsolute value of cosine similarity between different patch representations. Given the ﬁnal-layer patch\nrepresentations h[L] of an input x, we add a patch-wise cosine loss to the training objective:\nLcos(x) =P(h[L]).\nThis regularization loss explicitly minimizes the pairwise cosine similarity between different patches. Similar\napproaches have also been adopted for training improved representations in graph neural networks (Chen\net al., 2020a) and diversiﬁed word embeddings in language models (Gao et al., 2019). Meanwhile, this\nregularization loss can be viewed as minimizing the upper bound of the largest eigen-value of h (Merikoski,\n1984), hence improving the expressiveness of the representations. See Figure 2 (a) for an illustration.\nPatch-wise contrastive loss Secondly, as shown in Figure 1 (b), representations learned in early layers\nare more diverse compared to that in deeper layers. Hence, we propose a contrastive loss which uses the\nrepresentations in early layers, and regularizes the patches in deeper layers to reduce the similarity of patch\nrepresentations. Speciﬁcally, given an image x, let h[1] = {h[1]\ni }i and h[L] = {h[L]\ni }i denote its patches at\nthe ﬁrst and the last layer, respectively. We constrain each h[L]\ni to be similar to h[1]\ni and to be different to any\nother patches h[1]\nj̸=i as follows (see Figure 2 (b)),\nLcontrastive(x) =−1\nn\nn∑\ni=1\nlog exp(h[1]\ni\n⊤\nh[L]\ni )\nexp(h[1]\ni\n⊤\nh[L]\ni ) + exp(h[1]\ni\n⊤\n( 1\nn−1\n∑\nj̸=i h[L]\nj ))\n,\nIn practice, we stop the gradient on h[1].\nPatch-wise mixing loss Thirdly, instead of just using the class patch for the ﬁnal prediction, we propose to\ntrain each patch to predict the class label as well. This can be combined with Cutmix (Yun et al., 2019) data\naugmentation to provide additional training signals for the vision transformer. As shown in Figure 2 (c), we\nmix the input patches from two different images and attach an additional shared linear classiﬁcation head to\neach output patch representations for classiﬁcation. The mixing loss forces each patch to only attend to its a\nsubset of patches from the same input images and ignore unrelated patches. Hence, it effectively prevents\nsimple averaging across different patches to yield more informative and useful patch representations. This\npatch mixing loss can be formulated as below,\nLmixing(x) = 1\nn\nn∑\ni=1\nLce(g(h[L]\ni ),yi),\nwhere h[L]\ni represents patch representations in the last layer, gdenotes the additional linear classiﬁcation\nhead, yi stands for the patch-wise class label and Lce denotes the cross entropy loss.\n4\nAlgorithm We improve the training of vision transformers by simply jointly minimizing the weighted\ncombination of α1Lcos + α2Lcontrastive + α3Lmixing. Our method does not require any network modiﬁcations,\nand in the meantime, is not restricted to any speciﬁc architectures. In our experiments, we simply set\nα1 = α2 = α3 = 1without any particular hyper-parameters tuning.\n.\n.\n.\nPairwise \npatch cosine \nsimilarity\nVision\nTransformer\nLast Layer\nInput\n.\n.\n.\n.\n.\n.\nFirst Layer Last Layer\nPositive \nPair\nNegative \nPair\nInput Self-attention \nLayers\n.\n.\n.\nCutmix \nLoss\nVision\nTransformer\nBird\nDog\nDog\nDog\nLast Layer\n(a) Patch-wise Cosine Loss (b) Patch-wise Contrastive Loss (c) Patch-wise Mixing Loss\nFigure 2: An illustration of our patch diversiﬁcation promoting losses. (a) Patch-wise cosine loss. (b)\nPatch-wise contrastive loss. (c) Patch-wise mixing loss.\n5 Experiments\nIn this section, we apply our method to improve the training of a variety of vision transformers, including\nDeiT (Touvron et al., 2020) and SWIN-Transformer (Liu et al., 2021), and evaluate on a number of image\nclassiﬁcation and semantic segmentation benchmarks. We show that our training strategy promotes patch\ndiversiﬁcation and learns transformers with signiﬁcantly better transfer learning performance on downstream\nsemantic segmentation tasks.\n5.1 Main Results on ImageNet\nWe use DeiT (Touvron et al., 2020) and SWIN transformers (Liu et al., 2021) as our baseline models, and\nimproving these models by incorporating our patch diversiﬁcation-promoting losses to the training procedure.\nSettings For DeiT based models, we closely follow the training settings provided in Touvron et al. (2020)1.\nWe train all DeiT baselines and our models for 400 epochs. We use stochastic depth dropout and linearly\nincrease the depth dropout ratio from 0 to .5 following Huang et al. (2016). Additionally, we use stochastic\ndepth dropout ratio of 0.5 for DeiT-Base24 and DeiT-Small24, which allows us to train deeper DeiT models\nwithout diverging. For our method, we remove MixUp (Zhang et al., 2017) and repeated data augmentation\n(Hoffer et al., 2020) as they are not compatible with our path-wise mixing loss. Detailed ablation study is in\nSection 5.3.\nFor SWIN transformers, we use the ofﬁcial code for training, evaluation and ﬁnetuning 2. Speciﬁcally, we\ndownload the ofﬁcial SWIN-Base models pretrained with resolution of224×224 and 384×384, respectively.\nWe further ﬁnetune them for another 30 epochs with or without our patch diversiﬁcation losses. In particular,\nwe use a batch size of 1024 (128 X 8 GPUS), a constant learning rate of 10−5 and a weight decay of 10−8 for\nﬁnetuning.\n1https://github.com/facebookresearch/deit\n2https://github.com/microsoft/Swin-Transformer\n5\nMethod # Params (M) Input Size + Conv Top-1 (%)\nCNNs\nResNet-152 (He et al., 2016) 230 224 ✓ 78.1\nDenseNet-201 (Huang et al., 2017) 77 224 ✓ 77.6\nEffNet-B8 (Gong et al., 2020) 87 672 ✓ 85.8\nEffNetV2-L (Tan and Le, 2021) 121 384 ✓ 85.7\nNFNet (Brock et al., 2021) 438 576 ✓ 86.5\nLambdaNet-420 (Bello, 2021) 87 320 ✓ 84.9\nCNNs + CVT-21 (Wu et al., 2021) 32 224 ✓ 82.5\nTransformers CVT-21 32 384 ✓ 83.3\nLV-ViT-M(Jiang et al., 2021b) 56 224 ✓ 84.0\nLV-ViT-L 150 448 ✓ 85.3\nDeiT (scratch)\nDeiT-Small12 (Touvron et al., 2020)\n22 224 × 80.4\n+ DiversePatch (ours) 81.2\nDeiT-Small24 44 224 × 80.3\n+ DiversePatch (ours) 82.2\nDeiT-Base12 86 224 × 82.1\n+ DiversePatch (ours) 82.9\nDeiT-Base24 172 224 × 82.1\n+ DiversePatch (ours) 83.3\nDeiT-Base12 86 384 × 83.6\n+ DiversePatch (ours) 84.2\nSWIN (ﬁnetune)\nSWIN-Base (Liu et al., 2021)\n88 224 × 83.4\n+DiversePatch (ours) 83.7\nSWIN-Base 86 384 × 84.5\n+ DiversePatch (ours) 84.7\nTable 1: Top-1 accuracy results on ImageNet. We train all DeiT based models from scratch for 400 epochs.\nFor SWIN-Transformer based models, we ﬁnetune from existing checkpoints for 30 epochs. Results without\nany extra data are reported.\nResults on ImageNet As demonstrated in Table 1, for all the model architectures we evaluated, our method\nleads to consistent improvements upon its corresponding baseline model. For example, for DeiT based\nmodels, we improve the top-1 accuracy of DeiT-Small12 from 80.4% to 81.2%, and improve the top-1\naccuracy of DeiT-Base12 from 82.1% to 82.9%. A similar trend can also been for SWIN based models.\nAdditionally, our method allows us to train more accurate vision transformers by simply stacking more layers.\nNaive training of DeiT models cannot beneﬁt from larger and deeper models due to rapid patch representation\ndegradation throughout the networks. As we can see from Table 1, with standard training, the results from\nDeiT-Small24 and DeiT-Base24 are not better than the results from DeiT-Small12 and DeiT-Base12; on\nthe other hand, our method alleviates over-smoothing and learns more diversiﬁed patch features, enabling\nfurther improvements to the large model regime. Speciﬁcally, our method achieves 82.2% and 83.3% top-1\naccuracy for DeiT-Small24 and DeiT-Base24, respectively, which are1.9% and 1.2% higher compared to\ntheir corresponding DeiT baselines.\nTo further verify reproducibility of our results, we run DeiT-Base12 for three trials, achieving top-1 accuracy\n6\nof 82.86%, 82.82%, 82.89%; the round to round performance variant is negligible and the s.t.d is smaller\nthan 0.1.\nAvg. Cosine similarity\n0 1 2 3 40.3\n0.4\n0.5\n0.6\n0.7\n0.8\nDEIT-Base24\nDEIT-Base24 + Ours\nSWIN-Base\nSWIN-Base + Ours\nResNet-50\nBlock Index\nFigure 3: Comparison on average patch-wise absolute\ncosine similarity.\nFollowing the studies in Section 3, we plot the patch-\nwise absolute cosine similarity for the patch features\nlearned by our method in Figure 3. As shown in\nFigure 3, the patch-wise absolute cosine similarity\nis reduced signiﬁcantly for both DeiT-Base24 and\nSWIN-Base; the cosine similarity among the learned\npatches in the last layer is similar to the result from\nthe ResNet-50 baseline.\nWe also compare with recently proposed CNN and\ntransformer hybrids in Table 1, including CVT (Wu\net al., 2021) and LV-ViT (Jiang et al., 2021b). These\nhybrid models introduce additional convolution lay-\ners to both the patch projection layers and self-attention layers, and achieve better classiﬁcation accuracy on\nImageNet compared to pure transformers like DeiT and SWIN-transformers. These hybrid approaches are\nnon-conﬂict to our method, and we will further study the diverse patch representations in these architectures\nand apply our method to them.\nResults with ImageNet-22K Pre-training We also ﬁne-tune ViT-Large models (Dosovitskiy et al., 2020)\nand SWIN-Large models that are pretrained on ImageNet-22K (Russakovsky et al., 2015) to further push\nthe limits of accuracy on ImageNet. ImageNet-22k contains 22k classes and 14M images. Speciﬁcally,\nwe directly download the ImageNet-22K pre-trained models provided in ViT and SWIN-Transformer and\nﬁnetune these checkpoints on ImageNet training set with 30 epochs, a batch size of 1024, a constant learning\nrate of 10−5 and a weight decay of 10−8.\nTable 2 shows the ﬁnetuning accuracy on ImageNet. Our method again leads consistent improvements across\nall evaluated settings. Speciﬁcally, we improve the VIT-Large top-1 accuracy from 85.1% to 85.3% and\narchive 87.4% top-1 accuracy with SWIN-Large. As a future work, we will further pretrain the models on\nImageNet-22k with our method to see whether the improvement is larger.\nModel Input Size Top-1 Acc (%)\nVIT-Large (Dosovitskiy et al., 2020) 224 83.6\n+ DiversePatch (ours) 224 83.9\nVIT-Large 384 85.1\n+ DiversePatch (ours) 384 85.3\nSWIN-Large + ImageNet22k 384 87.3\n+ DiversePatch (ours) 384 87.4\nTable 2: Results on ImageNet by ﬁnetuing from ImageNet-22K pretrained vision transformers.\n5.2 Transfer Learning on Semantic Segmentation\nSemantic segmentation requires a backbone network to extract representative and diversiﬁed features from\nthe inputs; the downstream semantic segmentation performance critically relies on the quality of the extracted\n7\nfeatures. In this section, we use SWIN-Base pretrained on ImageNet (see Table 1) and SWIN-large pretrained\non both ImageNet22k and ImageNet (see Table 2) as our backbone models and ﬁnetune them on two widely-\nused semantic segmentation datasets, ADE20K (Zhou et al., 2017) and Cityscapes (Cordts et al., 2016), to\nverify the transferability our pretrained models.\nIn particular, we show the backbone models trained with our diversity-promoting losses are especially helpful\nfor downstream segmentation tasks. By using our pretrained SWIN-Large ( 87.4% top-1 accuracy), our\nresults outperform all existing methods and establish a new state-of-the-art on ADE20K and the Cityscapes\nvalidation dataset, achieving mIOU of 54.5% and 83.6% mIoU, respectively.\nDatasets ADE20K (Zhou et al., 2017) is a large scene parsing dataset, covering 150 object and stuff\ncategories. ADE-20K contains 20K images for training, 2K images for validation, and 3K images for test\nimages. Cityscapes (Cordts et al., 2016) dataset labels 19 different categories (with an additional unknown\nclass) and consists of 2975 training images, 500 validation images and 1525 testing images. We do evaluation\non the ADE20K and Cityscapes validation set in this paper.\nBaselines We introduce both state-of-the-art CNNs (e.g. Xiao et al., 2018; Zhang et al., 2020; Wang et al.,\n2020; Bai et al., 2020) and recent proposed transformer models (e.g. Liu et al., 2021; Zheng et al., 2020;\nRanftl et al., 2021) as our baselines.\nSettings We closely follow the ﬁnetuning settings proposed in SWIN transformers (Liu et al., 2021) 3.\nSpeciﬁcally, we use UperNet (Xiao et al., 2018) in mmsegmentation (MMSegmentation, 2020) as our testbed.\nDuring training, we use AdamW (Loshchilov and Hutter, 2017) optimizer with a learning rate of 6 ×10−5\nand a weight decay of 0.01. We use a cosine learning rate decay and a linear learning rate warmup of 1,500\niterations. We ﬁnetune our models for 160K iterations and 80K iterations on ADE20K and Cityscapes training\nset, respectively. We adopt the default data augmentation scheme in mmsegmentation (MMSegmentation,\n2020) 4 and train with 512×512 and 769×769 crop size for ADE20K and Cityscapes, respectively, following\nthe default setting in mmsegmentation. Additionally, following SWIN transformers (Liu et al., 2021), we use\na stochastic depth dropout of 0.3 for the ﬁrst 80% of training iterations, and increase the dropout ratio to 0.5\nfor the last 20% of training iterations.\nFollowing SWIN transformers (Liu et al., 2021), we report both the mIoUs from the single-scale evaluation\nand the mIoUs from the multi-scale ﬂipping evaluation (Zhang et al., 2018). Speciﬁcally, for multi-scale\nﬂipping testing, we enumerate testing scales of {0.5, 0.75, 1.0, 1.25, 1.5, 1.75} and random horizontal ﬂip by\nfollowing common practices in the literature (e.g. Zhang et al., 2020; Liu et al., 2021; Zheng et al., 2020).\nThe images are evaluated at 2048×1024 and 2049×1025 for ADE20K and cityscapes, respectively.\nResults We summarize our results in Table 3 and Table 4. Our method improves the training of SWIN-\nTransformers and learns backbones capable of extracting more diversiﬁed features from the inputs, leading to\nnew state-of-the-art segmentation performance on both ADE-20K and Cityscapes. Our achieve 54.5% mIoU\non ADE-20K and 83.6% mIoU on the Cityscapes validation set, outperforming all existing approaches.\n3https://github.com/SwinTransformer/Swin-Transformer-Semantic-Segmentation\n4https://github.com/open-mmlab/mmsegmentation\n8\nModel #Params (M) mIoU (%) mIoU (ms+ﬂip) (%)\nOCNet (Yuan et al., 2019) 56 45.5 N/A\nUperNet (Xiao et al., 2018) 86 46.9 N/A\nResNeSt-200 + DeepLab V3 88 N/A 48.4\nSETR-Base(Zheng et al., 2020) 98 N/A 48.3\nSETR-Large (Zheng et al., 2020) 308 N/A 50.3\nDPT-ViT-Hybrid(Ranftl et al., 2021) 90 N/A 49.0\nDPT-ViT-Large(Ranftl et al., 2021) 307 N/A 47.6\nSwin-Base 121 48.1 49.7\n+ DiversePatch (ours) 121 48.4±0.2 50.1±0.2\nSwin-Large 234 52.0 53.5\n+ DiversePatch (ours) 234 53.1±0.1 54.5±0.1\nTable 3: State-of-the-art on ADE20K. ‘ms+ﬂip’ refers to mutli-scale testing with ﬂipping (Zhang et al., 2018),\nand ‘#Params’ denotes the number of parameters.\nModel #Params (M) mIoU (%) mIoU (ms+ﬂip) (%)\nOCNet (Yuan et al., 2019) 56 80.1 N/A\nHRNetV2 + OCR (Wang et al., 2020) 70 81.6 N/A\nPanoptic-DeepLab (Cheng et al., 2019) 44 80.5 81.5\nMultiscale DEQ (Bai et al., 2020) 71 80.3 N/A\nResNeSt-200 + DeepLab V3 121 N/A 82.7\nSETR-Base(Zheng et al., 2020) 98 N/A 78.1\nSETR-Large (Zheng et al., 2020) 308 N/A 82.1\nSwin-Base 121 80.4 81.5\n+ DiversePatch (ours) 121 80.8±0.1 81.8±0.1\nSwin-Large 234 82.3 83.1\n+ DiversePatch (ours) 234 82.7±0.2 83.6±0.1\nTable 4: State-of-the-art on the Cityscapes validation set.\n5.3 Ablation Studies\nOn the efﬁcacy of our regularization strategies Our method introduces three regularization terms to\npromote patch diversiﬁcation. In this part, we use DeiT-Base24 and SWIN-Base as our baseline models and\nablate the effectiveness of our patch-wise cosine loss, patch-wise contrastive loss and patch-wise mixing\nloss by enumerating all different combination of training strategies. We proceed by exactly following\nthe training settings in section 5.1. We summarize our results in Table 5. As we can see from Table 5,\nall diversiﬁcation-promoting losses are helpful and all combinations lead to improved top-1 accuracy on\nImageNet. Speciﬁcally, we improved DeiT-Base24 from 82.1% to 83.3% on top-1 accuracy by combing all\nthree losses; for SWIN-Base, we did not ablate thepatch-wise contrastive loss, because the number of patches\nwas reduced throughout the network due to down-sampling. In this case, we boost the top-1 accuracy from\n83.5% and 83.7% by incorporating the patch-wise cosine loss and patch-wise mixing loss into the training\nprocedure. And the patch representations learned by our method are particularly useful in down-stream\nsemantic segmentation tasks, as demonstrated in section 5.2.\n9\nPatch-wise\ncosine loss\nPatch-wise\ncontrastive\nloss\nPatch-wise\nmixing loss\nDeiT-Base24\n(%)\nSwin-Base\n(%)\n× × × 82.1 83.4\n✓ × × 82.5 83.5\n× ✓ × 82.6 N/A\n× × ✓ 82.8 83.4\n✓ × ✓ 83.1 83.7\n✓ ✓ × 83.1 N/A\n× ✓ ✓ 83.3 N/A\n✓ ✓ ✓ 83.3 N/A\nTable 5: Ablating the impact of different combinations of our proposed patch diversiﬁcation losses.\nOn the stabilization of training Vision transformers are prone to overﬁtting, and training successful\nvision transformers often requires careful hyper-parameters tuning. For example, DeiT uses a bag of\ntricks for stabilized training, including RandAugment (Cubuk et al., 2020), MixUp (Zhang et al., 2017),\nCutMix (Yun et al., 2019), random erasing (Zhong et al., 2020), stochastic depth (Huang et al., 2016),\nrepeated augmentation (Hoffer et al., 2020), etc. As we shown in Table 6, removing some of these training\ntricks leads to signiﬁcant performance degradation for DeiT. While our patch-wise diversiﬁcation losses\noffer a natural regularization to prevent overﬁtting, and may therefore leading to a more stabilized training\nprocess. The models trained via our method yield consistently competitive results across different training\nsettings. Additionally, we show our method could further beneﬁt from more advanced design of talking-heads\nattentions (Shazeer et al., 2020), longer training time and deeper architecture design, achieving consistent\nimprovements upon our DeiT baselines.\nModel DeiT-Base12 DeiT-Base12 + DiversePatch (ours)\nStandard (300 epochs) 81.8 82.6\n+ Talking Head 81.8 82.7\n- Repeat Augmentation 78.4 82.7\n- Random Erasing 77.5 82.7\n- Mixup 80.3 82.7\n- Drop Path 78.8 80.2\n+ 400 Epochs 82.1 82.9\n+ Depth (24 Layer) 82.1 83.3\nTable 6: More stabilized training of DeiT models with our patch diversiﬁcation promoted losses.\n6 Related Works and Discussions\nTransformers for image classiﬁcation Transformers (Vaswani et al., 2017) have achieved great success\nin natural language understanding (Devlin et al., 2018; Radford et al., 2019), which motivates recent works\nin adapting transformers to computer vision. For example, iGPT (Chen et al., 2020b) views an image as a\nlong sequences of pixels and successfully trains transformers to generate realistic images; Dosovitskiy et al.\n(2020) splits each image into a sequence of patches and achieves competitive performance on challenging\n10\nImageNet when pretraining on a large amount of data; Touvron et al. (2020) leverages knowledge distillation\nto improve the training efﬁcacy of vision transformers and achieves better speed vs. accuracy on ImageNet\ncompared to EfﬁcientNets.\nRecently, a variety of works focus on improving vision transformers by introducing additional convolutional\nlayers to take advantage of the beneﬁts of inductive bias of CNNs, e.g., (Han et al., 2021; Liu et al., 2021; Wu\net al., 2021; Zhou et al., 2021; Jiang et al., 2021b; Touvron et al., 2021; Valanarasu et al., 2021; Arnab et al.,\n2021; Xie et al., 2021; Yuan et al., 2021). Among them, LV-ViT (Jiang et al., 2021b) is most related to our\nwork, LV-ViT Jiang et al. (2021b) also introduces patch-wise auxiliary loss for training, which is equivalent\nto our patch-wise mixing loss; LV-ViT is a concurrent work of our submission. Compared to LV-ViT, our\nmethod focuses on improving the patch diversity of vision transformers, which is motivated from a very\ndifferent perspective.\nDiverse representations in CNNs In CNNs, the diverse representation often refers to feature diversity\nacross different channels (e.g. Lee et al., 2018; Liu et al., 2018). In vision transformers, patches could be\nviewed as feature vectors from different spatial locations in CNNs. CNNs’ feature maps at different spatial\nlocations are naturally diversiﬁed as only local operators are used. Transformers, on the other hand, use\nglobal attention to fuse features across the different locations, and tends to learn similar presentations without\nregularization.\nLimitations and negative societal impacts For the limitation part, in this work, we mainly focus on\nvision transformer. Therefore, our method can only be applied to transformer models with images as input.\nCurrently, we only focus on supervised learning problems, and do not study unsupervised or semi-supervised\nlearning. Although our work has positive impact for the research community, we also have potential negative\nsocial impacts. Once we open-source our model checkpoints and code, we will have no control of anyone\nwho can get access to the code and model checkpoints.\n7 Conclusion\nIn this paper, we encourage diversiﬁed patch representations when training image transformers. We address\nthe problem by proposing three losses. Empirically, without changing the transformer model structure, by\nmaking patch representations diverse, we are able to train larger, deeper models and obtain better performance\non image classiﬁcation tasks. Applying our pretrained model to semantic segmentation tasks, we obtain\nSOTA results on two popular datasets, ADE20K and Cityscapes.\nFor future works, we plan to study how to encourage diversiﬁed patch representations for different tasks. We\nwill also incorporate the proposed loss into self-supervised learning settings, and study if the transformer\nmodel can serve as a better self-supervised learner for computer vision tasks when the patch representations\nare more diverse.\n11\nReferences\nAnurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luˇci´c, and Cordelia Schmid. Vivit: A\nvideo vision transformer. arXiv preprint arXiv:2103.15691, 2021.\nShaojie Bai, Vladlen Koltun, and J Zico Kolter. Multiscale deep equilibrium models. arXiv preprint\narXiv:2006.08656, 2020.\nIrwan Bello. Lambdanetworks: Modeling long-range interactions without attention. arXiv preprint\narXiv:2102.08602, 2021.\nGedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video\nunderstanding? arXiv preprint arXiv:2102.05095, 2021.\nAndrew Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-scale image\nrecognition without normalization. arXiv preprint arXiv:2102.06171, 2021.\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision,\npages 213–229. Springer, 2020.\nDeli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the over-smoothing\nproblem for graph neural networks from the topological view. In Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, volume 34, pages 3438–3445, 2020a.\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative\npretraining from pixels. In International Conference on Machine Learning, pages 1691–1703. PMLR,\n2020b.\nBowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu, Thomas S Huang, Hartwig Adam, and Liang-Chieh\nChen. Panoptic-deeplab. arXiv preprint arXiv:1910.04751, 2019.\nMarius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,\nUwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3213–3223,\n2016.\nEkin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data\naugmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition Workshops, pages 702–703, 2020.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255.\nIeee, 2009.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-\nterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n12\nJun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. Representation degeneration problem in\ntraining natural language generation models. arXiv preprint arXiv:1907.12009, 2019.\nChengyue Gong, Tongzheng Ren, Mao Ye, and Qiang Liu. Maxup: A simple way to improve generalization\nof neural network training. arXiv preprint arXiv:2002.09024, 2020.\nKai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer.\narXiv preprint arXiv:2103.00112, 2021.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.\nElad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoeﬂer, and Daniel Soudry. Augment your batch:\nImproving generalization through instance repetition. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 8129–8138, 2020.\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic\ndepth. In European conference on computer vision, pages 646–661. Springer, 2016.\nGao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolu-\ntional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages\n4700–4708, 2017.\nYifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan: Two transformers can make one strong gan.\narXiv preprint arXiv:2102.07074, 2021a.\nZihang Jiang, Qibin Hou, Daquan Zhou Li Yuan, Xiaojie Jin, Anran Wang, and Jiashi Feng. Token labeling:\nTraining a 85.5% top-1 accuracy vision transformer with 56m parameters on imagenet. arXiv preprint\narXiv:2104.10858, 2021b.\nHsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Singh, and Ming-Hsuan Yang. Diverse image-\nto-image translation via disentangled representations. In Proceedings of the European conference on\ncomputer vision (ECCV), pages 35–51, 2018.\nWeiyang Liu, Rongmei Lin, Zhen Liu, Lixin Liu, Zhiding Yu, Bo Dai, and Le Song. Learning towards\nminimum hyperspherical energy. arXiv preprint arXiv:1805.09298, 2018.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030,\n2021.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101,\n2017.\nTim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and Christoph Feichtenhofer. Trackformer: Multi-\nobject tracking with transformers. arXiv preprint arXiv:2101.02702, 2021.\nJorma Kaarlo Merikoski. On the trace and the sum of elements of a matrix.Linear algebra and its applications,\n60:177–185, 1984.\nMMSegmentation. MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark. https:\n//github.com/open-mmlab/mmsegmentation, 2020.\n13\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nRené Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. arXiv\npreprint arXiv:2103.13413, 2021.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej\nKarpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.\nInternational journal of computer vision, 115(3):211–252, 2015.\nNoam Shazeer, Zhenzhong Lan, Youlong Cheng, Nan Ding, and Le Hou. Talking-heads attention. arXiv\npreprint arXiv:2003.02436, 2020.\nMingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks. In\nInternational Conference on Machine Learning, pages 6105–6114. PMLR, 2019.\nMingxing Tan and Quoc V Le. Efﬁcientnetv2: Smaller models and faster training. arXiv preprint\narXiv:2104.00298, 2021.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé\nJégou. Training data-efﬁcient image transformers & distillation through attention. arXiv preprint\narXiv:2012.12877, 2020.\nHugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou. Going deeper\nwith image transformers. arXiv preprint arXiv:2103.17239, 2021.\nJeya Maria Jose Valanarasu, Poojan Oza, Ilker Hacihaliloglu, and Vishal M Patel. Medical transformer:\nGated axial-attention for medical image segmentation. arXiv preprint arXiv:2102.10662, 2021.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.\nJingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu,\nMingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition.\nIEEE transactions on pattern analysis and machine intelligence, 2020.\nHaiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing\nconvolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021.\nTete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Uniﬁed perceptual parsing for scene\nunderstanding. In Proceedings of the European Conference on Computer Vision (ECCV), pages 418–434,\n2018.\nJiangtao Xie, Ruiren Zeng, Qilong Wang, Ziqi Zhou, and Peihua Li. So-vit: Mind visual tokens for vision\ntransformer. arXiv preprint arXiv:2104.10935, 2021.\nLi Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng,\nand Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv\npreprint arXiv:2101.11986, 2021.\nYuhui Yuan, Xilin Chen, and Jingdong Wang. Object-contextual representations for semantic segmentation.\narXiv preprint arXiv:1909.11065, 2019.\n14\nSangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix:\nRegularization strategy to train strong classiﬁers with localizable features. InProceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 6023–6032, 2019.\nHang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi, and Amit\nAgrawal. Context encoding for semantic segmentation. In Proceedings of the IEEE conference on\nComputer Vision and Pattern Recognition, pages 7151–7160, 2018.\nHang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Haibin Lin, Zhi Zhang, Yue Sun, Tong He, Jonas\nMueller, R Manmatha, et al. Resnest: Split-attention networks. arXiv preprint arXiv:2004.08955, 2020.\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk\nminimization. arXiv preprint arXiv:1710.09412, 2017.\nSixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng\nFeng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence\nperspective with transformers. arXiv preprint arXiv:2012.15840, 2020.\nZhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In\nProceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pages 13001–13008, 2020.\nBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing\nthrough ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 633–641, 2017.\nDaquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Qibin Hou, and Jiashi Feng. Deepvit:\nTowards deeper vision transformer. arXiv preprint arXiv:2103.11886, 2021.\nXizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable\ntransformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.\n15",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7223901152610779
    },
    {
      "name": "Computer science",
      "score": 0.6364067196846008
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5293382406234741
    },
    {
      "name": "Discriminative model",
      "score": 0.5218721628189087
    },
    {
      "name": "Segmentation",
      "score": 0.48693978786468506
    },
    {
      "name": "Computer vision",
      "score": 0.3840123414993286
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.32108527421951294
    },
    {
      "name": "Engineering",
      "score": 0.22196653485298157
    },
    {
      "name": "Voltage",
      "score": 0.11120375990867615
    },
    {
      "name": "Electrical engineering",
      "score": 0.10764002799987793
    }
  ],
  "institutions": [],
  "cited_by": 42
}