{
    "title": "Automatic Component Prediction for Issue Reports Using Fine-Tuned Pretrained Language Models",
    "url": "https://openalex.org/W4312658847",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2654490619",
            "name": "Dae-Sung Wang",
            "affiliations": [
                "Chung-Ang University"
            ]
        },
        {
            "id": "https://openalex.org/A4211860667",
            "name": "Chan-Gun Lee",
            "affiliations": [
                "Chung-Ang University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2079317829",
        "https://openalex.org/W2113351233",
        "https://openalex.org/W2107878631",
        "https://openalex.org/W2911964244",
        "https://openalex.org/W2152565070",
        "https://openalex.org/W6752422711",
        "https://openalex.org/W3129148558",
        "https://openalex.org/W4239025696",
        "https://openalex.org/W4239510810",
        "https://openalex.org/W1565746575",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W2152311353",
        "https://openalex.org/W1974758710",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W2147697413",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2121044470",
        "https://openalex.org/W6731031554",
        "https://openalex.org/W2088624470",
        "https://openalex.org/W4288388993",
        "https://openalex.org/W2740861372",
        "https://openalex.org/W3035030897",
        "https://openalex.org/W6766673545",
        "https://openalex.org/W6757817989",
        "https://openalex.org/W2087743880",
        "https://openalex.org/W2963035373",
        "https://openalex.org/W1576514601",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W3081168214",
        "https://openalex.org/W2024932032",
        "https://openalex.org/W2156163116",
        "https://openalex.org/W6600367688",
        "https://openalex.org/W2980708516",
        "https://openalex.org/W1793174126",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2984582583",
        "https://openalex.org/W2287648897",
        "https://openalex.org/W4226334468",
        "https://openalex.org/W6779469252",
        "https://openalex.org/W3094847939",
        "https://openalex.org/W1980867644",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W8870360",
        "https://openalex.org/W3126074026",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2563351168"
    ],
    "abstract": "Various issues or bugs are reported during the software development. It takes considerable effort, time, and cost for the software developers to triage these issues manually. Many previous studies have proposed various method to automate the triage process by predicting component using word-based language models. However, these methods still suffer from unsatisfactory performance due to their structural limitations and ignorance of the word context. In this paper, we propose a novel technique based on pretrained language models and it aims to predict a component of an issue report. Our approach fine-tunes the pretrained language models to conduct multilabel classifications. The proposed approach outperforms the previous state-of-the-art method by more than 30&#x0025; with respect to the recall at <inline-formula> <tex-math notation=\"LaTeX\">${k}$ </tex-math></inline-formula> on all the datasets considered in our experiment. This improvement suggests that fine-tuned pretrained language models can help us to predict issue components effectively.",
    "full_text": "Received 30 November 2022, accepted 12 December 2022, date of publication 15 December 2022,\ndate of current version 21 December 2022.\nDigital Object Identifier 10.1 109/ACCESS.2022.3229426\nAutomatic Component Prediction for Issue\nReports Using Fine-Tuned Pretrained\nLanguage Models\nDAE-SUNG WANG\nAND CHAN-GUN LEE\nDepartment of Computer Science and Engineering, Chung-Ang University, Seoul 06974, South Korea\nCorresponding author: Chan-Gun Lee (cglee@cau.ac.kr)\nThis work was supported by the National Research Foundation of Korea (NRF) Grant through the Korea Government [Ministry of Science,\nICT & Future Planning (MSIP)] under Grant 2022R1F1A1074031.\nABSTRACT Various issues or bugs are reported during the software development. It takes considerable\neffort, time, and cost for the software developers to triage these issues manually. Many previous studies\nhave proposed various method to automate the triage process by predicting component using word-based\nlanguage models. However, these methods still suffer from unsatisfactory performance due to their structural\nlimitations and ignorance of the word context. In this paper, we propose a novel technique based on pretrained\nlanguage models and it aims to predict a component of an issue report. Our approach ﬁne-tunes the pretrained\nlanguage models to conduct multilabel classiﬁcations. The proposed approach outperforms the previous\nstate-of-the-art method by more than 30% with respect to the recall at k on all the datasets considered in our\nexperiment. This improvement suggests that ﬁne-tuned pretrained language models can help us to predict\nissue components effectively.\nINDEX TERMS Component recommendation, machine learning, natural language processing, pretrained\nlanguage model, software engineering.\nI. INTRODUCTION\nNumerous software issues are reported daily during the test-\ning and maintenance phases. As the size and complexity\nof software have recently increased, the number of issues\ntends to increase, and the need to manage them promptly\nhas become urgent. Typical modern software development\nprocesses rely on issue tracking systems, such as Jira 1 or\nBugzilla,2 to manage the issues systematically.\nNevertheless, it is still challenging for the human triagers\nto handle the issues everyday due to their complexity and\nvolume. An issue report contains various information, such\nas title/summary, description, reporter, product, component,\npriority, severity, and other details. Fig. 1 presents an example\nof an issue report highlighting the title (e.g., ‘‘PDF search\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Francisco J. Garcia-Penalvo\n.\n1https://www.atlassian.com/software/jira\n2https://www.bugzilla.org/\nshould default to unlimited white space’’), description (e.g.,\n‘‘PDF often include additional spaces...’’), and component\n(e.g., ‘‘PDF Viewer’’).\nThe component is essential information for the software\nengineer or developer to determine the locality of the issue or\nbug. It is not uncommon for a typical software project to have\nmany components, which can grow due to project evolution\nover time [7]. The manual triager (manager) follows a triage\nprocess to assign the issue reports, as illustrated in Fig. 2.\nIn case the triager fails to assign a component properly, the\ntriager should repeat the steps of the process. Therefore, the\ntriager is expected to have deep and extensive knowledge\nof the projects, software modules, and code bases [7], [19].\nHowever, such an expectation becomes challenging when the\nscope and size of the project ever grows, which is common\nfor most software.\nMoreover, this process is laborious and requires consid-\nerable time, cost, and effort, reducing the software qual-\nity. In the Eclipse project, an example of a real industrial\n131456\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ VOLUME 10, 2022\nD.-S. Wang, C.-G. Lee: Automatic Component Prediction for Issue Reports Using Fine-Tuned Pretrained Language Models\nFIGURE 1. The example of an issue report from Mozilla Firefox project in\nBugzilla repository.\nenvironment, approximately 25% of the issue reports were\nreassigned due to incorrect triage, including component mis-\nmatch [2]. Incorrect triage is time-consuming and costly\nat nearly two person-hours per day in triaging issue\nreports [1]. Thus, from a software engineering perspective,\nthe time-consuming and costly triage process should be\nimproved and requires automation with proper tool support.\nRecently, Choetkiertikul et al. [7] proposed a long\nshort-term memory (LSTM)-based architecture [16], a deep\nlearning model for software component recommendation\ncalled DeepSoft-C, which learns semantic features from the\ntextual description of issue reports for component recommen-\ndation [7]. Cosine similarity [33] was applied for information\nretrieval to extract sets of textual similarity features from the\ndescription in the issue reports. By conducting semantic and\nsimilarity features, a single set of features is generated and\ninput into a multilabel neural network to identify the compo-\nnents relevant to the issue report. They also demonstrated the\nimportance of data quality by conducting experiments using\na regenerated dataset based on the number of issues for each\ncomponent.\nThese approaches suggest the possibility of predicting\ncomponents using the deep learning method with the title\nand description data from the issue report. Unfortunately,\nthey still have some problems due to their structural limi-\ntations. The word-based LSTM models do not use the full\nsentence information of the issue report, so these methods\nfail to exploit information useful for predicting components.\nIn addition, the problem of long-term dependencies in the\nLSTM method can make the learning inefﬁcient [3]. Issue\nreports may contain long sequences of words and various\ninformation. Thus, these two problems should be addressed\nto enable better prediction performance.\nAs suggested in several pieces of literature [7], [41], com-\nponent information is considered very useful in the bug triage\nprocess. However, few studies have assessed component\nFIGURE 2. Triage process for issue reports.\nprediction for issue reports compared to studies on developer\nprediction. In addition, we argue that recent advances in deep\nlearning approaches have not been actively applied to the\ncomponent prediction problem. Previous work [7] using the\nword-based LSTM method demonstrates the possibility that\ndeep learning–based approaches can effectively predict com-\nponents of an issue report, but the reported performance indi-\ncates that further improvement is still needed. This study was\nmotivated by the scarcity of component prediction research\nbased on recent language models and the demand for more\naccurate prediction performance.\nThis paper proposes a ﬁne-tuned pretrained model-based\ncomponent prediction technique that predicts components\nfrom the issue report data. In addition, through the ﬁne-tuned\npretrained language model, we overcome the structural lim-\nitations of the LSTM-based method. Thus, we evaluate the\npretrained language models by comparing the performance\nwith the baseline paper [7] to gain conﬁdence in the pro-\nposed ﬁne-tuned pretrained language model. We extract the\ndatasets from the Eclipse Foundation, Eclipse Community,\nand Bugzilla Firefox, a reliable open-source issue repository.\nEach dataset consists of almost 20,000 issues and 137 compo-\nnents. In addition, we implement a task-based dataset down-\nsampling method to address the biased dataset problem.\nThus, the proposed ﬁne-tuned pretrained language mod-\nels outperform state-of-the-art methods [7]. Furthermore, the\ntask-based dataset downsampling method demonstrates the\nimportance of the dataset quality in the pretrained language\nmodel-based method that Gururangan et al. [14] proposed.\nThe contributions of this paper are as follows:\n• We propose a novel approach of component prediction\nfor issue reports using ﬁne-tuned pretrained language\nmodel to overcome the structural limitation of the word-\nbased model.\n• We compare the proposed approach with the state-of-\nthe-art method for predicting component using their\ndata. The experimental results reveal that approximately\n30% of improvement is achieved by the proposed\napproach compared to the previous method.\n• We propose a task-based downsampling method to\naddress the biased dataset problem. The experimental\nresults shows a signiﬁcant increase in the recall at k\nVOLUME 10, 2022 131457\nD.-S. Wang, C.-G. Lee: Automatic Component Prediction for Issue Reports Using Fine-Tuned Pretrained Language Models\nFIGURE 3. Structure of the model used in the previous study [7].\nTABLE 1. Dataset statistics for previous work [7].\nand a steady decline in validation loss after applying the\nproposed method.\n• We build a new dataset gathered from various open-issue\nrepositories to validate the performance of the proposed\ntechnique’s task-based module. Unlike other conven-\ntional datasets targeting for bug ﬁxer prediction only, the\ndataset we build consists of the components, developers,\ntitles, and descriptions to predict the components and\nbug ﬁxers. The dataset is publicly available on GitHub. 3\nThe rest of the paper is organized as follows. Section II\ndescribes the related work, and Section III presents the\ndesign and implementation of the proposed method. Next,\nSection IV provides the experimental results and Section V\nexplains the threats to validity of our results. Finally,\nSection VI explains the conclusions and future work.\nII. RELATED WORK\nSome researchers have proposed machine and deep\nlearning–based methods to predict components. This section\npresents previous studies on component prediction and auto-\nmated bug triage, which exploit the same information in\n3https://github.com/daesungwang/Components-Prediction\nthe issue reports: the title and description. The difference\nbetween component prediction and bug triage is predicting\nthe component or developer. In addition, we summarize the\nconcepts and recent trends in pretrained language models.\nA. AUTOMATIC COMPONENT PREDICTION\nSureka et al. [35] attempted to predict components\nusing a machine learning method consisting of term\nfrequency-inverse document frequency (TF-IDF) [31] and\na component reassignment graph. The TF-IDF method can\nobtain content-based textual features. While experimenting,\nthey found that the reassignment of the component causes a\ndecrease in accuracy. Thus, they also adopted a component\nreassignment graph based on the changes in the issue report.\nTherefore, they constructed a predictive model that combines\nTF-IDF and a component reassignment graph to obtain the\ntop-k results for components. In addition, they gathered the\ndataset from the Eclipse and Mozilla open-issue repositories\nconsisting of approximately 20,000 reports. As in this paper,\nthey also used the title and description information in training\nthe model.\nYan et al. [38] proposed a discriminative probability latent\nsemantic analysis (DPLSA) model [12] to predict compo-\nnents that concentrate on the topic of the issue report. They\nfocused on which component highly corresponds to terms\nconcerning its function in the issue report. Therefore, they\nconstructed a semantic analysis model with issue reports as a\ndocument and the component as a category, as Lu et al. [25]\nproposed. The experiment achieved the top-k results from the\ntitle and description dataset, consisting of 6,000 issue reports\nfor ten components. They achieved improved recall results\nat k, indicating the top-k results of components. However,\ntheir dataset consists of only ten components, fewer than the\ndataset we used. Therefore, the model by Yan et al. [38]\ndemonstrates the possibility of predicting components using\nsemantic analysis with fewer components than other datasets.\n131458 VOLUME 10, 2022\nD.-S. Wang, C.-G. Lee: Automatic Component Prediction for Issue Reports Using Fine-Tuned Pretrained Language Models\nTABLE 2. Summary of the selected previous work.\nChoetkiertikul et al. [7] proposed the word-based\nLSTM [16] deep learning method to predict components.\nAs illustrated in Fig. 3, they implemented the LSTM for\nsemantic feature extraction to predict components using\nword2vec [8] in the word embedding layer from the title and\ndescription information in the issue report. The experiment\nusing the LSTM-based method found that an issue report may\nbe assigned to more than one component, requiring multi-\nple labels. Thus, they adopted a multilabeling classiﬁcation\nmethod to classify the component in a multilabeling situation,\nas Nam et al. [27] proposed.\nFurthermore, Choetkiertikul et al. [7] found that an imbal-\nance of the issue report dataset causes a decrease in accuracy.\nThus, they deleted the issue reports with a small number of\nissues to address the data imbalance problem. As a result, they\nimproved the accuracy using deep learning and deletion. The\nimprovement indicates that adjusting the imbalanced dataset\nis an integral part of the automatic component prediction\nﬁeld. As presented in Table 1, they collected datasets from\n11 open-issue repositories, consisting of 142,083 issues with\nan average of 12,916 issues each. They used this dataset to\nevaluate the performance of their word-based LSTM model\nby the recall at k, which is the top-k results of the components.\nChoetkiertikul et al. [7] revealed the importance of dataset\nquality and component prediction possibilities using a deep\nlearning method with the dataset deletion and word-based\nLSTM methods. Thus, to gain conﬁdence in the perfor-\nmance of the proposed ﬁne-tuned pretrained language model,\nwe also adopted the dataset from Choetkiertikul et al. [7] to\nevaluate the proposed model.\nKangwanwisit et al. [20] conducted various word-based\nexperiments to ﬁnd proper feature extraction and classi-\nﬁcation techniques for the component prediction. For the\nfeature extraction task, various techniques such as bag of\nword (BoW) [42], N-gram [5] IDF, and TF-IDF were con-\nsidered. The experimental results showing the performance\nof the classiﬁcation task when using Support Vector Machine\n(SVM) [9], Gradient Boosting (GB) [15], and Random Forest\n(RF) [4] were also reported. Their analysis indicated that\nTF-IDF and RF were the best in extracting the textual fea-\ntures and classifying the issues, respectively. They collected\napproximately 68,000 reports with title and description infor-\nmation for the experiments.\nAs described above and in Table 2, some approaches\nhave predicted components using machine learning or deep\nlearning methods. Each study has proposed a different word-\nbased method, but the studies use the same information from\nthe issue report dataset: the title and description. In addi-\ntion, various approaches toward automatic bug triage [21],\n[26], [39] using deep learning methods have been actively\nconducted recently. Thus, we also reviewed automatic bug\ntriage research and found that it uses the title and description\ninformation from the issue report repository to predict a\ndeveloper (assignee). Thus, the bug triage research method\ncan be adopted in the ﬁeld of component prediction due\nto the similarity of information and method. As a result,\nwe describe the bug triage research below to investigate this\nfor component prediction.\nB. AUTOMATED BUG TRIAGE\nLee et al. [21] proposed the convolutional neural net-\nwork (CNN)-based [32] deep learning model to predict the\nassignee (developer) using the title and description informa-\ntion extracted from industrial issue reports. They collected\n14,583 issues consisting of 225 assignees.\nIn a bug triage ﬁeld, the number of the assignees becomes\na class, which is the component in component prediction.\nThus, it is difﬁcult to compare the prediction performance\nbetween the proposed and Lee et al. [21] method. However,\nthe similarity of the information that the data feature extracts\nfrom the title and description can prove that deep learning\nmethods, such as the CNN, and pretrained language models\ncan adopt component prediction.\nMani et al. [26] collected a dataset from Google\nChromium, Mozilla Core, and Mozilla Firefox, which are\nreliable open-issue repositories. They aimed to predict the\nassignee (developer) using the recurrent neural network\n(RNN) [16]. However, they found that some developers\nwho contribute less than others decrease the performance\nof the RNN-based model. Thus, they deleted the issue\nreports according to the number of contributions of the\ndeveloper. As a result, they obtained a signiﬁcant result\nin top-k accuracy, which refers to the top- k developers,\nfrom the contribution-based and RNN-based deep learning\nmethods. In addition, they suggested the importance of the\ndataset quality by conducting contribution dataset regenera-\ntion. Thus, these results are evidence of the proposed second\ntask method, which downsamples the dataset by deleting the\ncomponents with few issues.\nVOLUME 10, 2022 131459\nD.-S. Wang, C.-G. Lee: Automatic Component Prediction for Issue Reports Using Fine-Tuned Pretrained Language Models\nTABLE 3. Dataset summary for the tasks 1 to 3.\nZaidi et al. [39] adopted the bidirectional encoder repre-\nsentations from transformers (BERT) [11] model, which is\na pretrained language model to predict the developer using\nthe title and description information extracted from the work\nby Mani et al. [26] and data collected from Mozilla 4 and\nNetBeans5. They also adopted the contribution-based down-\nsampling method that Mani et al. [7] used to address dataset\nproblems. Through the experiments, they achieved better top-\nk accuracy results than Lee et al. [21] and Mani et al. [7] Thus,\nthis ﬁnding indicates that the proposed ﬁne-tuned pretrained\nlanguage model can predict components correctly because\nthe component prediction dataset has fewer classes, which\nwere developers in Zaidi et al. [39] study.\nThrough the details above, we found important informa-\ntion for automatic component prediction. First, the dataset\nof every related work should have or share the method to\naddress the data imbalance in an issue report dataset. Due to\nits generation method, data imbalance is a characteristic of\nthe issue report dataset. Many issues come from users, which\ncan bias the model toward common functions, such as the\nuser interface (UI), general, and debug, which occur easily\nand more often than other components.\nSecond, component prediction and bug triage have simi-\nlarities due to their methods and information. Previous stud-\nies do not consider the source code or stack trace because\nthey cannot view all of the information in the issue report.\nIf the stack trace or source code becomes model input, the\nlanguage-based model trained by common sentence data can-\nnot understand the meaning of the input, reducing the model\nperformance.\nThe third critical point is the lack of issue report data. Most\nrelated studies shared or constructed the dataset to predict the\ncomponents or developers, making it challenging to compare\nthe performance of each study’s proposed method. Thus,\n4https://bugzilla.mozilla.org/\n5https://netbeans.apache.org/\nwe suggest a task-based dataset downsampling method and\nour dataset for issue report-based research to overcome these\nproblems.\nC. PRETRAINED LANGUAGE MODEL\nBefore sentence-based pretrained language models were pro-\nposed, word2vec [8], FastText [18], and Glove [28], which\nare based on word-embedding methods, were primarily used.\nThese word-embedding-based methods initialize the embed-\nding layer to train the models and use the embedding algo-\nrithm in word2vec to implement the already-trained embed-\nding layer. These methods have some problems because each\nword is embedded into the same vector; thus, they cannot\nunderstand the information of the entire sentence.\nFurthermore, transformer-based [36] language models,\nsuch as ELMo [29] and BERT [11], have been proposed\nto overcome the limitations of the word-based model. Var-\nious embedding vectors exist in an issue report dataset, the\nmain source of automatic component prediction. Therefore,\nwe adopted the transformer-based pretrained language model\nBERT to increase the accuracy of component prediction.\nD. BERT AND IMPROVED MODELS\nIn 2018, Google proposed BERT [11], a pretrained language\nmodel based on text data using Wikipedia for 2.5 billion\nwords and a book corpus of 800 million words using trans-\nformers. Moreover, BERT uses large-scale data to require rel-\natively considerable computing resources and learning time\ncompared to existing models. However, it has better results\nthan existing models by ﬁne-tuning pretrained models in\ncomplex problems. Subsequently, several follow-up studies\nhave been conducted to improve the performance of BERT.\nIn this paper, we ﬁne-tuned the BERT-based [11] model to\novercome the limits of word-based models. We compared the\naccuracy of automatic component prediction with previous\nstudies by ﬁne-tuning the robustly optimized BERT approach\n(RoBERTa) [23] and the BERT based on it.\n131460 VOLUME 10, 2022\nD.-S. Wang, C.-G. Lee: Automatic Component Prediction for Issue Reports Using Fine-Tuned Pretrained Language Models\nFIGURE 4. Example of dataset pre-processing. Boxes show the original\ndata (above) and the Pre-processed data (below), respectively.\nFacebook Artiﬁcial Intelligence proposed RoBERTa to\nimprove the performance of the existing BERT model. They\ntrained the BERT model by adding training data, adjusting\nhyperparameters, and reselecting the training data to conduct\nreplication learning. Through deletion, next sentence predic-\ntion improves the maximum sequence length, diversiﬁes the\nmasking pattern, and improves performance compared with\nBERT.\nIII. DESIGN AND IMPLEMENTATION\nThis section discusses the method and experiments of the\nﬁne-tuned pretrained language model. Moreover, it presents\nthe task-based downsampling method to improve the perfor-\nmance of the automatic component prediction model.\nA. DATASET COLLECTION\nSection I and II describe the necessity of adopting an appro-\npriate issue report dataset. Thus, we attempted to deter-\nmine an appropriate dataset from previous work. However,\nwe could not ﬁnd a suitable dataset without Choetkier-\ntikul et al. [7] data because most datasets based on the\nbug triage method to determine the developer do not\ninclude component information. Thus, we adopted Choetkier-\ntikul et al. [7] dataset to evaluate the performance of the\nproposed ﬁne-tuned pretrained language model. By adopting\ntheir dataset, we can directly compare performance to demon-\nstrate the robustness and conﬁdence of the proposed method.\nWe also generated a dataset from reliable open-issue repos-\nitories, as presented in Table 3. These data came from\nBugzilla Firefox,6 Eclipse Foundation,7 and Eclipse Commu-\n6https://bugzilla.mozilla.org\nFIGURE 5. Validation loss graphs for the task-based experiments on\nBugzilla Firefox dataset.\nnity.7 To create the proposed dataset, we selected resolved,\nsolved, and closed issues, meaning that the components were\nalready assigned. Thus, the dataset comprised around 60,000\nissues consisting of 137 components.\nB. DATASET PREPROCESSING\nFig. 1 indicates that issue reports include various informa-\ntion (e.g., title, description, component, product, type, etc.).\nHowever, in this paper, we only used the title and descrip-\ntion information to predict components, similar to previous\nwork [7], [35], [38] Thus, we arranged the dataset to be\nsuitable for the proposed model, as illustrated in Fig. 4. The\nissue report dataset consists of natural language that is not\nstandardized, and many people do not follow standards; thus,\ndirectly using the dataset as input to a model is challenging\nwithout preprocessing. In addition, the dataset consists of\nvarious issue report repositories and Choetkiertikul et al. [7]\nwork. Therefore, a suitable preprocessing method must be\nbuilt for the proposed model.\nBased on the issue report datasets, the preprocessing meth-\nods are as follows:\n7https://bugs.eclipse.org/bugs\nVOLUME 10, 2022 131461\nD.-S. Wang, C.-G. Lee: Automatic Component Prediction for Issue Reports Using Fine-Tuned Pretrained Language Models\nTABLE 4. The number of issues in the projects for each task based method.\n1) Delete the new line tags (e.g., \\n and \\p) from the JSON\ndata format,\n2) Delete the URL in a sentence that does not need it for\ncomponent prediction,\n3) Delete the stack trace,\n4) Delete the hex code,\n5) Separate the dataset into training and testing data,\n6) Return a tokenized dataset suitable for the model.\nThe preprocessing data method in this paper is similar to\nthe method for the existing transformer-based model. Espe-\ncially in an issue report dataset, there can be stack traces.\nHowever, in component prediction, the stack trace is not\nessential data because it consists of method calls or source\ncode. The BERT-based language model is pretrained using\nWikipedia and a data corpus consisting of common sentences\nthat do not have source code information. Thus, the source\ncode information threatens the BERT-based model.\nFurthermore, we implemented the human triager method\nusing the title and description information to assign compo-\nnents suitable for component prediction. Thus, we decided\nto delete the stack trace, which is nonessential in component\nprediction. Last, we shufﬂed the dataset to avoid focusing on\none class while separating data into 80% for training and 20%\nfor testing.\nC. DEALING WITH DATA IMBALANCE\nAs listed in Table 4, most datasets are biased in speciﬁc com-\nponents. Through the experiment, Fig. 8 and Table 6 reveal\nthat bias issues can be obstacles to training the model. Thus,\nwe propose a task-based method to address data imbalance\nproblems.\nIn the second task, we deleted the components with fewer\nthan 50 issues, similar to Choetkiertikul et al. [7] method.\nChoetkiertikul et al. [7] invalidated the number of compo-\nnents that they deleted. Thus, we empirically deleted com-\nponents with fewer than 50 issues, demonstrating improve-\nments in the dataset. We deleted about 1% of the dataset.\nTo determine the conﬁdence of the second task deletion\nmethod, we compared the performance between deleting\n50 and 100 issues (Fig. 5). We found that deleting 50 issues\n131462 VOLUME 10, 2022\nD.-S. Wang, C.-G. Lee: Automatic Component Prediction for Issue Reports Using Fine-Tuned Pretrained Language Models\nFIGURE 6. The structure of our learning model for automatic component prediction.\nexhibited improvement compared with the original data, but\nno difference was found between deleting 50 and 100 issues.\nTherefore, we deleted 50 issues for the second task.\nThe validation loss had not yet stabilized in the ﬁrst and\nsecond tasks (Fig. 5). To stabilize it, we downsampled the\ndataset with more issues than average. We obtained a better\nresult through the third task, as presented in Fig. 8, without\ncompromising the issue report dataset characteristics.\nBy comparing each task, we determined the proper dataset\nnormalization method suitable for automatic component pre-\ndiction when most datasets are imbalanced.\nThe data handling method presented in Table 4 addresses\nthe following tasks:\nTask 1: Keeping the original dataset without removing any-\nthing.\nTask 2: Deleting the issue reports with the components\nappearing in the dataset fewer than 50 times.\nTask 3: Downsampling the issue reports by limiting the\nnumber of issues per component to the average of\nthe total issues.\nD. FINE-TUNED PRETRAINED LANGUAGE MODELS\nThe pretrained language models implemented in this paper\nare BERT and RoBERTa, which are based on the trans-\nformer [36]. Thus, we must tokenize the data using a BERT-\nbased tokenizer, which is suitable for the proposed model that\nis unlike the word-based methods. As illustrated in Fig. 6,\nwe can obtain tokenized testing and training data through\ndata preprocessing. The model trains using the training data\nwhile conducting backpropagation to update the model. After\ntraining the model, the model is reveriﬁed to classify the\nrecall at k, which indicates the top-k components.\nThe existing BERT-based models exhibit a decrease in\naccuracy while conducting multilabel classiﬁcation [22],\nwhich is difﬁcult to classify for many classes, such as\nVOLUME 10, 2022 131463\nD.-S. Wang, C.-G. Lee: Automatic Component Prediction for Issue Reports Using Fine-Tuned Pretrained Language Models\nFIGURE 7. The overview of fine-tuned hidden layers.\ncomponent prediction or bug triage. Thus, it is necessary\nto ﬁne-tune the models to conduct multilabel classiﬁcation.\nDue to its limitations, it is impossible to predict components\nthat consist of many classes, as presented in Table 3. Thus,\nas depicted in Fig. 7, we ﬁne-tuned the model to classify many\nclasses to predict components.\nWe ﬁne-tuned the pretrained language models, BERT and\nRoBERTa, for the classiﬁcation task. Note that the number of\ncomponents (classes) that need to be classiﬁed varies depend-\ning on the project (e.g., Eclipse Platform: 19, Eclipse Founda-\ntion: 41, Bugzilla Firefox: 51). Thus, the classiﬁer layer needs\nto be ﬁne-tuned to handle different number of classes. Fur-\nthermore, we collected the best parameters, such as weight\ndecay to 5e-6 and learning rate to 5e-6 through the exper-\niment, which are important metrics in BERT-based models\n[34]. Those best parameters are described in Section III-F.\nE. EXPERIMENT ENVIRONMENT\nDuring the evaluation, we ran the previous study’s exper-\niments to compare the performance of the proposed ﬁne-\ntuned model. The task-based experiment was conducted in\nthe same environment: Intel(R) Xeon(R) Silver 4214R CPU\n@ 2.40 GHz and two Nvidia GeForce RTX 3090 with 24 GB.\nF. EVALUATION OPTIMIZATION\nIn this paper, we used the dataset from a previous study to\nevaluate the performance of the proposed ﬁne-tuned model.\nWe also employed this dataset to evaluate the performance of\nthe downsampling task-based method. We set the parameter\nvalues for the weight decay to 5e-6, the learning rate to 5e-6,\nand the batch size to 300. To avoid overﬁtting problems,\nwe conducted a set on various weight decay and learning\nrate values (e.g., from 1e-6 to 5e-6). We found that the\ntraining with the weight decay and learning rate at values\nother than 5e-6 makes the model vulnerable to underﬁtting\nor overﬁtting. We set the dropout value to 0.1.\nWe trained the proposed model for 1,500 steps during the\nexperiment and applied early stopping when the validation\nloss stabilized. We obtained a steady decrease in validation\nloss and an increase in validation accuracy throughout the\nexperiment. Thus, this result suggests that applying the down-\nsampling method proposed in Task 3 can effectively handle\nimbalanced issue report data. We adopted the AdamW [24]\nalgorithm for the optimization method and set the β1 to\n0.9 and β2 to 0.999, respectively. In addition, we adopted a\ndeepspeed [30] scheduler to schedule the learning rate during\nthe training. Thus, we set the minimum learning rate to 0 and\nthe maximum learning rate to 5e-6 during the 150 steps.\nWe adopt the recall@k metric to evaluate the performance of\nthe proposed method.\nIV. RESULTS\nThe baseline models [7], [20], [35], [38] consist of a\nword-based method with its own vocabulary, which might\nnot consist of suitable information for predicting components\nin new issues. Thus, the proposed ﬁne-tuned pretrained\nlanguage models predict components using the sequence\ninformation from the sentence, which can respond to the new\nissues correctly. It is much more effective, time-consuming,\nand cost-effective in the modern software ﬁeld to manage new\nissues.\nFurthermore, we constructed the model with the title,\ndescription, and component data that the triager uses to triage\nthe component in the real world. Thus, the proposed method\nis more effective in any triage system that manually triages\nthe components.\nWe evaluated the proposed component prediction method\nand addressed the following research questions:\n• RQ 1)Are the proposed ﬁne-tuned pretrained language\nmodels more effective than the method from the previ-\nous work in automatic component prediction?\n• RQ 2)Is the proposed method signiﬁcantly better than\nthe baseline method to predict the top-k components?\n• RQ 3)Are the proposed task-based methods appropriate\nin an imbalanced issue report dataset?\nA. RESEARCH QUESTION 1\nTable 5 presents the validation recall score of the pre-\ntrained BERT-based component prediction model, pretrained\nRoBERTa-based prediction model, and DeepSoft-C method.\nThe RoBERTa and BERT-based models achieved better recall\nscores than DeepSoft-C for all datasets. The RoBERTa-based\nmodel had better recall for the top ﬁve components than\nBERT for every dataset except the Hbase and Fedora\nCloudSync datasets.\n131464 VOLUME 10, 2022\nD.-S. Wang, C.-G. Lee: Automatic Component Prediction for Issue Reports Using Fine-Tuned Pretrained Language Models\nTABLE 5. Evaluation result of R@k(recall atk) compared with the baseline paper [7].\nTABLE 6. Evaluation result of R@kfor the task based methods.\nThe Infrastructure data comprise 10,166 issue reports with\n51 components. It turns out that the dataset is highly imbal-\nanced. Note that the BERT-based model shows only 1%\nhigher accuracy than DeepSoft-C. In contrast, RoBERTa\nachieves 11% higher accuracy than DeepSoft-C. Similarly,\nthe Fedora CloudSync dataset is imbalanced, too, and it has\nonly 1,617 issues and 22 components. We suspect that the\ndata imbalance contributed to the overﬁtting and rendered the\naccuracy difference of RoBERTa and DeepSoft-C to only 3%.\nIssue reports may contain a homonym which cannot be\ndistinguished by a word-based model and this might lead to\ndecreased accuracy. In addition, component prediction typi-\ncally requires large number of classes, and the learning ability\ntends to decrease when the number of classes increases. Our\nexperimental results show that using the ﬁne-tuned pretrained\nlanguage model proposed in this paper can yield good results\nin such environments.\nA similar trend was found for the top-10 component\nprediction. The BERT and RoBERTa models outperformed\nDeepSoft-C. However, the RoBERTa-based model outper-\nformed the BERT-based model for the Hbase dataset, which\ndiffers from the observation for the top ﬁve components.\nRecall scores at 15 components also presented a trend similar\nto the top ﬁve components, where the RoBERTa-based model\nhad better results than the BERT-based model for all datasets\nexcept the Fedora CloudSync and Hbase datasets.\nIn an automatic component prediction evaluation, the recall\nat ﬁve is a strict evaluation for the models. Thus, the higher\nrecall scores at ﬁve suggest that the proposed ﬁne-tuning\nmethod is suitable for component prediction using pre-\ntrained language models. In summary, the proposed pre-\ntrained RoBERTa-based ﬁne-tuned model is a better than\nthe proposed pretrained BERT-based ﬁne-tuned model and\nbaseline DeepSoft-C method for component prediction.\nIt is common to ﬁne-tune the BERT-based language mod-\nels to a speciﬁc task as shown in various previous approaches\nsuch as [40] and [37]. Our experimental results reveal that\nour ﬁne-tuned pretrained language models also achieve better\nperformance through the ﬁne-tuning methods described in\nSection III and Fig.7. To the best of our knowledge, this is\nthe ﬁrst work that uses a sentence-based model to predict\ncomponents for issue reports.\nB. RESEARCH QUESTION 2\nThroughout the experiments, the proposed methods showed\nbetter recall at k compared to the baseline method as pre-\nsented in Table 5. Speciﬁcally, to evaluate the performance\nof the proposed method, we adopted 11 datasets, which came\nfrom the baseline paper [7]. In addition to the comparison\nof the experimental results, we performed statistical tests to\ncheck the signiﬁcance of the results by comparing the recalls\nat the top 5, 10, and 15 components.\nThe Friedman test [13] was performed to check the overall\nsigniﬁcance of the results with a 95% conﬁdence interval.\nThe test had a p-value of less than 0.05, conﬁrming the\nsigniﬁcance of the results. Then, a Wilcoxon–Holm posthoc\ntest [17] was performed to identify the signiﬁcantly better\nmodel for top-k component prediction among the proposed\nand baseline models. The pairwise testing had a p-value of\nless than 0.05 for all pairs, conﬁrming the signiﬁcance of the\nmethods.\nFurthermore, the Demšar diagram [10] in Fig. 9 was built\nto rank the comparative methods for predicting the top 5,\n10, and 15 components. The RoBERTa-based method was\nVOLUME 10, 2022 131465\nD.-S. Wang, C.-G. Lee: Automatic Component Prediction for Issue Reports Using Fine-Tuned Pretrained Language Models\nFIGURE 8. Validation loss for the tasks 1-3 on Bugzilla Firefox dataset3. Task 2 stopped at the step 1,000 due to its early stabilization.\nthe best method out of all three methods. The average ranks\nfor RoBERTa, BERT, and DeepSoft-C are 1.1818, 2, and 3,\nrespectively. The Demšar diagram conﬁrms the signiﬁcant\ndifference in the results of all three methods because there is\nno connection between the methods in the Demšar diagram\nfor the top 5, 10, and 15. In the Demšar diagram, a connection\nbetween the two methods indicates an insigniﬁcant difference\nin the results. In summary, the statistical tests suggest that the\nRoBERTa-based method and the BERT-based methods are\nsigniﬁcantly better than the BERT-based component predic-\ntion method.\nC. RESEARCH QUESTION 3\nWe evaluated the task-based method by comparing the dif-\nference in the validation loss graph described in Fig. 8.\nWe experimented with the Bugzilla Firefox dataset using the\nﬁne-tuned BERT model.\nTask 1 was evaluated with the original dataset, which did\nnot apply any methods. We found that the validation loss\ndoes not decrease evenly. Thus, we adjusted the learning\nrate, the standard method to address the problem. Despite\nour efforts, the validation loss and accuracy do not evenly\ndecrease or increase. Therefore, the collected dataset also has\na data imbalance problem, like most previous studies.\nFIGURE 9. Demšar diagram for the experimental results.\nChoetkiertikul et al. [7] deleted components with insufﬁ-\ncient issues on their standard to address the Task 1 problem.\nThus, we deleted the components that consisted of fewer\nthan 50 issues. As we described in Fig 5 and Section III,\nChoetkiertikul et al. [7] did not explain the states of the com-\nponent deletion. Therefore, we deleted 50 issues, empirically\ncomparing the validation loss when deleting components with\n131466 VOLUME 10, 2022\nD.-S. Wang, C.-G. Lee: Automatic Component Prediction for Issue Reports Using Fine-Tuned Pretrained Language Models\n0 or fewer than 50 or 100 issues. In Task 2, we obtained good\nresults, demonstrating a more even decrease in the validation\nloss than in Task 1. However, Fig. 8 reveals that the decrease\nof validation loss still ﬂuctuates beginning at 400 steps.\nTherefore, Task 2 suggests that only deleting the components\nis not always the proper method to address imbalanced data.\nWe proposed Task 3 to resolve the limitations of Task 2,\nwhich is downsampling the dataset. Table 4 indicates that\nmost issue report data are easily biased due to their charac-\nteristics. During the triage cycle, common issues are triaged\ninto common components (e.g., Bugzilla Firefox: General,\nEclipse Foundation: CI-Jenkins, Eclipse Platform: UI), which\nfrequently occurs during the software life cycle. Thus, only\ndeleting components with a few issues is insufﬁcient to\naddress the biased data. These bias issues are considered\ncharacteristic of the issue report repository.\nTherefore, we downsampled the components to obtain\na higher-than-average number of issues for each reposi-\ntory. Through Task 3, based on the downsampling method,\nwe obtained an even decrease in validation loss (Fig. 8). Fur-\nthermore, Table 6 presents a 54% increase in the recall at 15 in\nthe Eclipse Foundation dataset, the most imbalanced data.\nThus, the proposed task-based method that downsampled the\ndataset without compromising the characteristics of the issue\nreport repository is useful in component prediction.\nV. THREATS TO VALIDITY\nThe lack of issue report datasets with component information\nmay threaten the robustness of this research. Our study uses\ndatasets imported from several open issue repositories, and\neach dataset contains almost 20,000 issues per repository.\nCompared to the datasets of the previous work [6] targeting\nfor the component prediction which contain an average of\n12,916 issues per repository, the size of our datasets seems\ncomparable. In contrast, a recent study [26] targeting for the\ndeveloper prediction uses a dataset containing nearly 200,000\nissue reports. Note that the component ﬁeld can be omitted\nwhen writing a issue report. Therefore, there are many issue\nreports without the component information, which makes it\ndifﬁcult for researchers to obtain enough datasets for the\ncomponent prediction.\nAnother potential threat is that we utilize only the title and\ndescription information in an issue report when predicting\nits component information, which is similarly done in the\nprevious work [7]. As mentioned in Section I and Fig. 1 each\nissue report may contain various information other than the\ntile and description. For example, the information such as\nissue reporter and product name could be also utilized for the\nprediction. Furthermore, the component information can be\nused to predict the developer and vice versa. We are planning\nto explore this in our next study.\nWe use a task-based method to address the dataset\nimbalance problem in this study. During the experiments,\nwe observed improvements in the recall at k and a steady\ndecrease in validation loss. However, deleting the biased\ncomponent with more than the average number of issues is not\na fundamental solution for an imbalanced dataset. Especially,\ndeleting issues might lose important information for training\nthe model even if we shufﬂe the dataset.\nVI. CONCLUSION\nIn this paper, we proposed an approach to improve the per-\nformance of automatic component prediction by using the\npretrained language models which are sentence-based deep\nlearning. We argued that the previous approaches relying\non word-based deep learning models did not provide suf-\nﬁcient performance. Speciﬁcally, we ﬁne-tuned the BERT\nand RoBERTa to ﬁt the issue report dataset consisting of\nvarious classes. We believe that because the sentence-based\npretrained models use information from the whole sequence\nof information, it also contributed to the performance of\nthe proposed approach, providing better experimental results\nthan those from the study by Choetkiertikul et al. [7]\nWe collected the issue reports from various open-source\nissue repositories to evaluate the performance of the pro-\nposed model. Using the task-based methods, we effectively\naddressed the imbalance problem in the dataset. We obtained\na 45% improvement compared with the method from the pre-\nvious work. The method for Task 3 exhibits a 54% improve-\nment in the recall at 15 in the Eclipse Foundation dataset in\nTable 6. Furthermore, the task-based method also improves\nthe performance of the training model and obtains high accu-\nracy on all datasets. This method improves the performance\nof component prediction and demonstrates the importance of\ndataset quality.\nFinally, the experimental results for the proposed method\nsuggest that ﬁne-tuned pretrained language models are effec-\ntive for automatic component prediction. Our proposed\nmethod showed decent accuracy even in a small dataset envi-\nronment, indicating that if sufﬁcient datasets with compo-\nnents are prepared in the future, we can expect better per-\nformance in component prediction. Thus, we plan to employ\ndata augmentation methods in issue reports in future work.\nRetraining a pretrained language model suitable for an issue\nreport dataset can improve automatic component predic-\ntion. Especially in industrial environments, triaging issues\nmanually can be very inefﬁcient due to frequent software\nupdates and expanded scope of the project and stakeholder.\nThus, we expect to apply our proposed approach to industrial\nprojects and evaluate its effectiveness.\nREFERENCES\n[1] J. Anvik, L. Hiew, and G. C. Murphy, ‘‘Who should ﬁx this bug?’’ in Proc.\n28th Int. Conf. Softw. Eng., May 2006, pp. 361–370.\n[2] J. Anvik and G. C. Murphy, ‘‘Reducing the effort of bug report triage:\nRecommenders for development-oriented decisions,’’ ACM Trans. Softw.\nEng. Methodol., vol. 20, no. 3, pp. 1–35, Aug. 2011.\n[3] Y . Bengio, P. Simard, and P. Frasconi, ‘‘Learning long-term dependencies\nwith gradient descent is difﬁcult,’’ IEEE Trans. Neural Netw., vol. 5, no. 2,\npp. 157–166, Mar. 1994.\n[4] L. Breiman, ‘‘Random forests,’’ Mach. Learn., vol. 45, no. 1, pp. 5–32,\n2001.\n[5] A. Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig, ‘‘Syntactic\nclustering of the web,’’ Comput. Netw. ISDN Syst., vol. 29, nos. 8–13,\npp. 1157–1166, Sep. 1997.\nVOLUME 10, 2022 131467\nD.-S. Wang, C.-G. Lee: Automatic Component Prediction for Issue Reports Using Fine-Tuned Pretrained Language Models\n[6] M. Choetkiertikul, H. K. Dam, T. Tran, T. Pham, and A. Ghose, ‘‘Pre-\ndicting components for issue reports using deep learning with information\nretrieval,’’ in Proc. 40th Int. Conf. Softw. Eng., Companion Proceeedings,\nMay 2018, pp. 244–245.\n[7] M. Choetkiertikul, H. K. Dam, T. Tran, T. Pham, C. Ragkhitwetsagul, and\nA. Ghose, ‘‘Automatically recommending components for issue reports\nusing deep learning,’’ Empirical Softw. Eng., vol. 26, no. 2, pp. 1–39,\nMar. 2021.\n[8] K. W. Church, ‘‘Word2Vec,’’ Nat. Lang. Eng., vol. 23, no. 1, pp. 155–162,\n2017.\n[9] C. Cortes and V . Vapnik, ‘‘Support-vector networks,’’ Mach. Learn.,\nvol. 20, no. 3, pp. 273–297, 1995.\n[10] J. Demšar, ‘‘Statistical comparisons of classiﬁers over multiple data sets,’’\nJ. Mach. Learn. Res., vol. 7, pp. 1–30, Dec. 2006.\n[11] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ 2018,\narXiv:1810.04805.\n[12] S. T. Dumais, ‘‘Latent semantic analysis,’’ Annu. Rev. Inf. Sci. Technol.,\nvol. 38, no. 1, pp. 188–230, 2004.\n[13] M. Friedman, ‘‘The use of ranks to avoid the assumption of normality\nimplicit in the analysis of variance,’’ J. Amer. Statist. Assoc., vol. 32,\nno. 200, pp. 675–701, Dec. 1937.\n[14] S. Gururangan, A. Marasović, S. Swayamdipta, K. Lo, I. Beltagy,\nD. Downey, and N. A. Smith, ‘‘Don’t stop pretraining: Adapt language\nmodels to domains and tasks,’’ 2020, arXiv:2004.10964.\n[15] T. Hastie, R. Tibshirani, and J. Friedman, ‘‘Boosting and additive trees,’’\nin The Elements of Statistical Learning. New York, NY , USA: Springer,\n2009, pp. 337–387.\n[16] S. Hochreiter and J. Schmidhuber, ‘‘Long short-term memory,’’ Neural\nComput., vol. 9, no. 8, pp. 1735–1780, 1997.\n[17] S. Holm, ‘‘A simple sequentially rejective multiple test procedure,’’ Scan-\ndin. J. Statist., vol. 6, no. 2, pp. 65–70, 1979.\n[18] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. Jégou, and\nT. Mikolov, ‘‘FastText.Zip: Compressing text classiﬁcation models,’’ 2016,\narXiv:1612.03651.\n[19] G. Kakarontzas, I. Stamelos, S. Skalistis, and A. Naskos, ‘‘Extracting\ncomponents from open source: The component adaptation environment\n(COPE) approach,’’ in Proc. 38th Euromicro Conf. Softw. Eng. Adv. Appl.,\nSep. 2012, pp. 192–199.\n[20] P. Kangwanwisit, M. Choetkiertikul, C. Ragkhitwetsagul, T. Sunetnanta,\nR. Maipradit, H. Hata, and K. Matsumoto, ‘‘A component recommendation\nmodel for issues in software projects,’’ in Proc. 19th Int. Joint Conf.\nComput. Sci. Softw. Eng. (JCSSE), Jun. 2022, pp. 1–6.\n[21] S.-R. Lee, M.-J. Heo, C.-G. Lee, M. Kim, and G. Jeong, ‘‘Applying deep\nlearning based automatic bug triager to industrial projects,’’ in Proc. 11th\nJoint Meeting Found. Softw. Eng., Aug. 2017, pp. 926–931.\n[22] W. Liu, P. Zhou, Z. Zhao, Z. Wang, H. Deng, and Q. Ju, ‘‘FastBERT: A self-\ndistilling BERT with adaptive inference time,’’ 2020, arXiv:2004.02178.\n[23] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V . Stoyanov, ‘‘RoBERTa: A robustly optimized BERT\npretraining approach,’’ 2019, arXiv:1907.11692.\n[24] I. Loshchilov and F. Hutter, ‘‘Decoupled weight decay regularization,’’\n2017, arXiv:1711.05101.\n[25] Y . Lu, Q. Mei, and C. Zhai, ‘‘Investigating task performance of probabilis-\ntic topic models: An empirical study of PLSA and LDA,’’ Inf. Retr., vol. 14,\nno. 2, pp. 178–203, 2011.\n[26] S. Mani, A. Sankaran, and R. Aralikatte, ‘‘DeepTriage: Explor-\ning the effectiveness of deep learning for bug triaging,’’ in Proc.\nACM India Joint Int. Conf. Data Sci. Manage. Data , Jan. 2019,\npp. 171–179.\n[27] J. Nam, J. Kim, E. L. Mencía, I. Gurevych, and J. Fürnkranz, ‘‘Large-scale\nmulti-label text classiﬁcation—Revisiting neural networks,’’ in Proc. Joint\nEur. Conf. Mach. Learn. Knowl. Discovery Databases. Nancy, France:\nSpringer, 2014, pp. 437–452.\n[28] J. Pennington, R. Socher, and C. Manning, ‘‘Glove: Global vectors for\nword representation,’’ in Proc. Conf. Empirical Methods Natural Lang.\nProcess. (EMNLP), 2014, pp. 1532–1543.\n[29] E. Matthew Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer, ‘‘Deep contextualized word representations,’’ 2018,\narXiv:1802.05365.\n[30] J. Rasley, S. Rajbhandari, O. Ruwase, and Y . He, ‘‘DeepSpeed: System\noptimizations enable training deep learning models with over 100 billion\nparameters,’’ in Proc. 26th ACM SIGKDD Int. Conf. Knowl. Discovery\nData Mining, Aug. 2020, pp. 3505–3506.\n[31] S. Robertson, ‘‘Understanding inverse document frequency: On theoret-\nical arguments for IDF,’’ J. Documentation, vol. 60, no. 5, pp. 503–520,\nOct. 2004.\n[32] P. Y . Simard, D. Steinkraus, and J. C. Platt, ‘‘Best practices for convo-\nlutional neural networks applied to visual document analysis,’’ in Proc.\n7th Int. Conf. Document Anal. Recognit., Edinburgh, U.K., vol. 3, 2003,\npp. 958–963.\n[33] A. Singhal, ‘‘Modern information retrieval: A brief overview,’’ IEEE Com-\nput. Soc. Tech. Committee Data Eng., vol. 24, no. 4, pp. 35–43, Dec. 2001.\n[34] C. Sun, X. Qiu, Y . Xu, and X. Huang, ‘‘How to ﬁne-tune bert for text\nclassiﬁcation?’’ in Proc. China Nat. Conf. Chin. Comput. Linguistics.\nKunming, China: Springer, pp. 194–206, 2019.\n[35] A. Sureka, ‘‘Learning to classify bug reports into components,’’ in Proc.\nInt. Conf. Model. Techn. Tools Comput. Perform. Eval.Prague, Czech\nRepublic: Springer, 2012, pp. 288–303.\n[36] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., vol. 30, 2017, pp. 1–11.\n[37] D. Wadden, U. Wennberg, Y . Luan, and H. Hajishirzi, ‘‘Entity, relation,\nand event extraction with contextualized span representations,’’ 2019,\narXiv:1909.03546.\n[38] M. Yan, X. Zhang, D. Yang, L. Xu, and J. D. Kymer, ‘‘A component rec-\nommender for bug reports using discriminative probability latent semantic\nanalysis,’’Inf. Softw. Technol., vol. 73, pp. 37–51, May 2016.\n[39] S. F. A. Zaidi, H. Woo, and C.-G. Lee, ‘‘Toward an effective bug triage\nsystem using transformers to add new developers,’’ J. Sensors, vol. 2022,\npp. 1–19, Apr. 2022.\n[40] T. Zhang, F. Wu, A. Katiyar, K. Q. Weinberger, and Y . Artzi, ‘‘Revisiting\nfew-sample BERT ﬁne-tuning,’’ 2020, arXiv:2006.05987.\n[41] W. Zhang, ‘‘Efﬁcient bug triage for industrial environments,’’ in\nProc. IEEE Int. Conf. Softw. Maintenance Evol. (ICSME), Sep. 2020,\npp. 727–735.\n[42] Y . Zhang, R. Jin, and Z. Zhou, ‘‘Understanding bag-of-words model:\nA statistical framework,’’ Int. J. Mach. Learn. Cybern., vol. 1, nos. 1–4,\npp. 43–52, Dec. 2010.\nDAE-SUNG WANG was born in Jeonju,\nSouth Korea. He received the B.S. degree in soft-\nware engineering from Jeonbuk National Univer-\nsity, Jeonju. He is currently pursuing the M.S.\ndegree in computer science and engineering with\nChung-Ang University, Seoul, South Korea. His\nresearch interests include software engineering,\nrequirement engineering, and natural language\nprocessing.\nCHAN-GUN LEEwas born in Seoul, South Korea,\nin 1972. He received the B.S. degree in computer\nengineering from Chung-Ang University, Seoul,\nin 1996, the M.S. degree in computer science from\nthe Korea Advanced Institute of Science and Tech-\nnology (KAIST), Daejeon, in 1998, and the Ph.D.\ndegree in computer science from The University\nof Texas at Austin, Austin, TX, USA, in 2005.\nFrom 2005 to 2007, he was a Senior Software\nEngineer at Intel, Hillsboro, Oregon. Since 2007,\nhe has been a Professor with the Department of Computer Science and Engi-\nneering, Chung-Ang University. He has authored more than 30 articles and\nconference papers. His research interests include software engineering and\nreal-time systems. He was a recipient of the Korea Foundation of Advanced\nStudies (KFAS) Fellowship, from 1999 to 2005.\n131468 VOLUME 10, 2022"
}