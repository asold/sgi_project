{
  "title": "Effectiveness of generative AI-large language models’ recognition of veteran suicide risk: a comparison with human mental health providers using a risk stratification model",
  "url": "https://openalex.org/W4409113625",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2109133188",
      "name": "Sean A. Lauderdale",
      "affiliations": [
        "University of Houston - Clear Lake"
      ]
    },
    {
      "id": "https://openalex.org/A5101714918",
      "name": "Randee Schmitt",
      "affiliations": [
        "University of Houston - Clear Lake"
      ]
    },
    {
      "id": "https://openalex.org/A5116886270",
      "name": "Breanna Wuckovich",
      "affiliations": [
        "University of Houston - Clear Lake"
      ]
    },
    {
      "id": "https://openalex.org/A5107100732",
      "name": "Natashaa Dalal",
      "affiliations": [
        "University of Houston - Clear Lake"
      ]
    },
    {
      "id": "https://openalex.org/A5074308790",
      "name": "Hela Desai",
      "affiliations": [
        "University of Houston - Clear Lake"
      ]
    },
    {
      "id": null,
      "name": "Shealyn Tomlinson",
      "affiliations": [
        "University of Houston - Clear Lake"
      ]
    },
    {
      "id": "https://openalex.org/A2109133188",
      "name": "Sean A. Lauderdale",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5101714918",
      "name": "Randee Schmitt",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5116886270",
      "name": "Breanna Wuckovich",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5107100732",
      "name": "Natashaa Dalal",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5074308790",
      "name": "Hela Desai",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Shealyn Tomlinson",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4210469184",
    "https://openalex.org/W3216245651",
    "https://openalex.org/W4365518269",
    "https://openalex.org/W2979089815",
    "https://openalex.org/W2627054532",
    "https://openalex.org/W3093607675",
    "https://openalex.org/W4293546484",
    "https://openalex.org/W3207057434",
    "https://openalex.org/W3049024012",
    "https://openalex.org/W4400350337",
    "https://openalex.org/W2966152549",
    "https://openalex.org/W4390700529",
    "https://openalex.org/W6879765156",
    "https://openalex.org/W4309651064",
    "https://openalex.org/W4386887838",
    "https://openalex.org/W2021844788",
    "https://openalex.org/W4403021952",
    "https://openalex.org/W4399130765",
    "https://openalex.org/W4233120920",
    "https://openalex.org/W3161573205",
    "https://openalex.org/W1895534063",
    "https://openalex.org/W1987670694",
    "https://openalex.org/W4253932945",
    "https://openalex.org/W4210557097",
    "https://openalex.org/W2010284717",
    "https://openalex.org/W2914444775",
    "https://openalex.org/W2970766316",
    "https://openalex.org/W3094588291",
    "https://openalex.org/W4220980812",
    "https://openalex.org/W4393953415",
    "https://openalex.org/W4400141990",
    "https://openalex.org/W4394579747",
    "https://openalex.org/W4200394808",
    "https://openalex.org/W4221052430",
    "https://openalex.org/W4401000814",
    "https://openalex.org/W4378470708",
    "https://openalex.org/W4280549083",
    "https://openalex.org/W4366317365",
    "https://openalex.org/W3112631310",
    "https://openalex.org/W6910611909",
    "https://openalex.org/W4376872703",
    "https://openalex.org/W4384807943",
    "https://openalex.org/W4226436127"
  ],
  "abstract": "Background With over 6,300 United States military veterans dying by suicide annually, the Veterans Health Administration (VHA) is exploring innovative strategies, including artificial intelligence (AI), for suicide risk assessment. Machine learning has been predominantly utilized, but the application of generative AI-large language models (GAI-LLMs) remains unexplored. Objective This study evaluates the effectiveness of GAI-LLMs, specifically ChatGPT-3.5, ChatGPT-4o, and Google Gemini, in using the VHA’s Risk Stratification Table for identifying suicide risks and making treatment recommendations in response to standardized veteran vignettes. Methods We compared the GAI-LLMs’ assessments and recommendations for both acute and chronic suicide risks to evaluations by mental health care providers (MHCPs). Four vignettes, representing varying levels of suicide risk, were used. Results GAI-LLMs’ assessments showed discrepancies with MHCPs, particularly rating the most acute case as less acute and the least acute case as more acute. For chronic risk, GAI-LLMs’ evaluations were generally in line with MHCPs, except for one vignette rated with higher chronic risk by the GAI-LLM. Variation across GAI-LLMs was also observed. Notably, ChatGPT-3.5 showed lower acute risk ratings compared to ChatGPT-4o and Google Gemini, while ChatGPT-4o identified higher chronic risk ratings and recommended hospitalization for all veterans. Treatment planning by GAI-LLMs was predicted by chronic but not acute risk ratings. Conclusion While GAI-LLMs offers potential suicide risk assessment comparable to MHCPs, significant variation exists across different GAI-LLMs in both risk evaluation and treatment recommendations. Continued MHCP oversight is essential to ensure accuracy and appropriate care. Implications These findings highlight the need for further research into optimizing GAI-LLMs for consistent and reliable use in clinical settings, ensuring they complement rather than replace human expertise.",
  "full_text": "Effectiveness of generative\nAI-large language models’\nrecognition of veteran suicide\nrisk: a comparison with human\nmental health providers using a\nrisk stratiﬁcation model\nSean A.Lauderdale*, RandeeSchmitt, Breanna Wuckovich,\nNatashaa Dalal, Hela Desai and ShealynTomlinson\nDepartment of Psychological and Behavioral Sciences, University of Houston– Clear Lake, Houston,\nTX, United States\nBackground: With over 6,300 United States military veterans dying by suicide\nannually, the Veterans Health Administration (VHA) is exploring innovative\nstrategies, including arti ﬁcial intelligence (AI), for suicide risk assessment.\nMachine learning has been predominantly utilized, but the application of\ngenerative AI-large language models (GAI-LLMs) remains unexplored.\nObjective: This study evaluates the effectiveness of GAI-LLMs, speci ﬁcally\nChatGPT-3.5, ChatGPT-4o, and Google Gemini, in using the VHA ’sR i s k\nStrati ﬁcation Table for identifying suicide risks and making treatment\nrecommendations in response to standardized veteran vignettes.\nMethods: We compared the GAI-LLMs’assessments and recommendations for\nboth acute and chronic suicide risks to evaluations by mental health care\nproviders (MHCPs). Four vignettes, representing varying levels of suicide risk,\nwere used.\nResults: GAI-LLMs’assessments showed discrepancies with MHCPs, particularly\nrating the most acute case as less acute and the least acute case as more acute.\nFor chronic risk, GAI-LLMs’evaluations were generally in line with MHCPs, except\nfor one vignette rated with higher chronic risk by the GAI-LLM. Variation across\nGAI-LLMs was also observed. Notably, ChatGPT-3.5 showed lower acute risk\nratings compared to ChatGPT-4o and Google Gemini, while ChatGPT-4o\nidentiﬁed higher chronic risk ratings and recommended hospitalization for all\nveterans. Treatment planning by GAI-LLMs was predicted by chronic but not\nacute risk ratings.\nConclusion: While GAI-LLMs offers potential suicide risk assessment comparable\nto MHCPs, signiﬁcant variation exists across different GAI-LLMs in both risk\nevaluation and treatment recommendations. Continued MHCP oversight is\nessential to ensure accuracy and appropriate care.\nFrontiers inPsychiatry frontiersin.org01\nOPEN ACCESS\nEDITED BY\nTao Wang,\nNorthwestern Polytechnical University, China\nREVIEWED BY\nSuhas Srinivasan,\nStanford University, United States\nFeifan Liu,\nUniversity of Massachusetts Medical School,\nUnited States\nInbar Levkovich,\nTel-Hai College, Israel\n*CORRESPONDENCE\nSean A. Lauderdale\nlauderdale@uhcl.edu\nRECEIVED 13 December 2024\nACCEPTED 13 March 2025\nPUBLISHED 03 April 2025\nCITATION\nLauderdale SA,Schmitt R,Wuckovich B,\nDalal N,Desai H andTomlinson S (2025)\nEffectiveness of generative AI-large language\nmodels’recognition of veteran suicide risk: a\ncomparison with human mental health\nproviders using a risk stratiﬁcation model.\nFront. Psychiatry 16:1544951.\ndoi: 10.3389/fpsyt.2025.1544951\nCOPYRIGHT\n© 2025 Lauderdale, Schmitt, Wuckovich, Dalal,\nDesai and Tomlinson. This is an open-access\narticle distributed under the terms of the\nCreative Commons Attribution License (CC BY).\nThe use, distribution or reproduction in other\nforums is permitted, provided the original\nauthor(s) and the copyright owner(s) are\ncredited and that the original publication in\nthis journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted\nwhich does not comply with these terms.\nTYPE Original Research\nPUBLISHED 03 April 2025\nDOI 10.3389/fpsyt.2025.1544951\nImplications: These ﬁndings highlight the need for further research into\noptimizing GAI-LLMs for co nsistent and reliable use in clinical settings,\nensuring they complement rather than replace human expertise.\nKEYWORDS\nveterans, suicide, risks, artiﬁcial intelligence, risk stratiﬁcation model\nIntroduction\nUnited States veterans’ suicide rates remain substantial and\nrepresent a major cause of preventable morbidity (1). Historically,\nveterans have faced a disproportionately higher risk of suicide\ncompared to the general population (2). According to the 2023\nNational Veteran Suicide Prevention Report (1), there were 6,392\nveteran deaths by suicide in 2021, and suicide mortality was the\nsecond-leading cause of death among veterans under the age of 45\nyears. Men and women veterans have higher rates of suicide by\nﬁrearms compared to non-veterans, with the risk of women veteran\nsuicide rate by ﬁrearm greatly exceeding (281.1%) those of non-\nveteran women. Early detection is crucial in suicide prevention (3,\n4) and evidence from investigations incorporating screening,\neducation, and active risk monitoring have produced reductions\nin suicide attempts across active duty military members and\nveterans (5). Despite these efforts and the substantial resource\ninvestment by the Veterans Health Administration (VHA; 6),\nveterans ’ suicide rates continue an upward trajectory ( 1),\nindicating a need to develop innovative strategies for veteran\nsuicide assessment and prevention (5).\nIn response, the VHA has implemented multiple strategies to\ndetect, screen, and prevent veteran suicide. One strategy is the use of\nartiﬁcial intelligence (AI). At present, the VHA has relied on a single\nAI strategy, known as machine learning (ML;7), to detect risk of\nveteran suicide risk. Machine learning employs statistical\nalgorithms to analyze data andﬂag critical risks found in records,\nsuch as electronic health records ( 8). Introduced in 2017, the\nRecovery and Engagement and Coordination for Health-Veteran\nEnhanced Treatment (REACH VET; 9, 10) algorithm scans\nveterans ’ electronic health records for 61 pre-identi ﬁed risk\nfactors that place veterans in the top.1% risk for suicide. Once\nidentiﬁed, veterans are contacted by their most recent provider for\nintervention. An evaluation of REACH VET that included veterans\nbefore and after implementation indicated that REACH VET use\nproduced substantial reductions in suicide attempts and mental\nh e a l t ha d m i s s i o n sa sw e l la si n c r e a s e dc o m p l e t e do u t p a t i e n t\nappointments and suicide safety plans. Unfortunately, suicide\nmortality rates did not change according to the evaluation, but\nthis may have been due to low statistical power (10). Numerous\nother investigations have also found that ML can be an effective\nstrategy to detect suicide risks in community samples ( 11).\nAlthough the REACH VET outcomes are promising, substantial\nconcerns still exist, including latency of risk identiﬁcation (up to\ntwo weeks;9) and use of risks largely predictive of suicide in White,\nmale veterans, excluding suicide risks of other veteran populations\n(e.g., women, minorities;12).\nAlthough the use of AI in mental health to facilitate clinical\ndecision-making and treatment (13, 14\n) has shown promise, the\nVHA has not reported the use of generative artiﬁcial intelligence-\nlarge language models (GAI-LLMs) in their AI portfolio ( 7).\nGenerative AI-LLMs build on ML strategies (e.g., Deep Learning\nand Neural Networks) to construct narrative, human-like responses\nbased on information from a broad range of training sources,\nincluding internet web pages, art, and books, and response ratings\nprovided by humans or smaller large language models (LLMs;8).\nGAI-LLM applications have been evaluated against mental health\nprofessionals (MHCPs) to assess their accuracy in detecting various\nclinical conditions and making treatment recommendations, with\nsome success noted. In a study that assessed the ability of GAI-LLMs\n(e.g., ChatGPT-3.5, 4, Bard [now called Gemini]) to recognize major\ndepression as compared to human physicians, the GAI-LLMs were\naccurate in identifying major depression, less likely to make biased\ntreatment recommendations, and more likely to make treatment\nrecommendations consistent with evidence-based guidelines (15).\nOther investigations have also found that GAI-LLMs are more\naccurate than humans in identifying mental disorders, such as\nBorderline Personality Disor der, and making evidence-based\ntreatment recommendations (16). These ﬁndings are intriguing\nbecause GAI-LLMs have the potential to identify those at risk for\nsuicide and provide immediate feedback to MHCPs, which is quicker\nthan the risk identi ﬁcation provided by ML approaches. This\nrepresents a signiﬁcant advancement in AI capabilities and utility\nand is critical as investigations indicate that healthcare professionals\nare willing to follow recommendations made by GAI-LLMs (17).\nIn addition to GAI-LLMs’potential to assess for the presence of\nmental disorders, a growing body of literature suggests GAI-LLMs\nhave the ability to identify suicide risks and probability of suicide\nattempts. A study conducted by Levkovich and Elyoseph ( 18)\ninvestigated ChatGPT-3.5 and 4’s ability to assess suicide ideation\nand risk as compared to MHCPs. Both GAI-LLMs were provided\nvignettes of a character displaying either high or low perceived\nburdensomeness and thwarted belongingness – prominent\ntheoretically-based suicide risks ( 19). Compared to MHCPs,\nChatGPT-4 predicted risk for suicide attempts at a similar rate;\nhowever, ChatGPT-3.5 underestimated suicide attempt risk. When\nLauderdale et al. 10.3389/fpsyt.2025.1544951\nFrontiers inPsychiatry frontiersin.org02\nassessing risk for suicidal ideation, ChatGPT-3.5’s ratings were\nsimilar to MHCPs, while ChatGPT-4’s risk ratings were higher.\nChatGPT-4 reported that vignette characters experienced more\nintense emotional pain than MHCPs, while both GAI-LLMs\nreported lower resilience compared to MHCPs. Theﬁndings were\nattributed to the varying de ﬁnitions and conceptualizations of\nemotional pain, resilience, and the nuances in clinical cases that\ncurrent GAI-LLMs may not capture as effectively as MHCPs.\nSpeciﬁcally, it can be speculated that these “soft” risks (e.g.,\nthwarted belongingness) require inferences from contextual\nelements experienced by individuals, such as stressors and their\nassociated emotional, cognitive, and behavioral responses, which\nGAI-LLMs may struggle to identify because they are not based on\nspeciﬁc, identiﬁable behaviors. Nonetheless, the ﬁndings indicate\nthat GAI-LLMs have potential utility in recognizing suicide risks.\nMore recently, Shinin-Altman and colleagues (20, 21) built off\nthis work and added several other suicide risks to vignettes to\ndetermine if this information would affect GAI-LLMs ’ risk\nassessment for suicide ideation, attempts, or death by suicide.\nWhen a history of depression or access to weapons was added to\nvignettes, Shinin-Altman and associates (21) found that ChatGPT-4\nrated characters at an increased risk for suicide ideation, attempts,\nand death. These interactions were not statistically signiﬁcant for\nChatGPT-3.5; however, ChatGPT-3.5 rated having a history of\ndepression as increasing risk for suicide attempts and death by\nsuicide. Interestingly, ChatGPT-3.5 only rated vignette characters at\nan increased risk for suicide attempts, but not suicidal ideation or\ndeath by suicide, when the characters were described as having\naccess to a weapon. Across all vignettes, ChatGPT-4 rated risks\nhigher than ChatGPT-3.5, but no gender differences were found to\nbe signiﬁcant. When considering previous suicide attempts, age, or\ngender as risks, Shinan-Altman and colleagues’ (20) ﬁndings were\nalso nuanced. Although age did not seem to affect any risk ratings,\nseveral interactions for gender and previous suicide attempts were\nfound to be signiﬁcant. For ChatGPT-4, the gender by previous\nsuicide attempt interaction was associated with higher estimated\nrisk for suicide attempts for women. ChatGPT-3.5 identiﬁed an\nincreased risk for serious suicide attempts and suicidal ideation for\nmen with a history of previous a ttempts. ChatGPT-3.5 also\nindicated a higher risk of seri ous suicide attempt, while\nChatGPT-4 rated a higher risk for suicidal ideation for vignettes\nwith a history of previous suicide attempts. In sum, theseﬁndings\ngenerally demonstrate that the addition of risk factors in vignettes\ninﬂuences GAI-LLMs ’ suicide risk assessment; however, there\nremains variability across GAI-LLMs and room for improvement\nin GAI-LLMs ’ recognition of risks that are inferred from\nclinical context.\nThe ﬁndings by Shinan-Altman and colleagues ( 20, 21)\nhighlight the potential utility of GAI-LLMs in assessing suicide\nrisk. However, it remains undetermined whether GAI-LLMs can\neffectively utilize human-developed models to identify suicide risks\nand make treatment recommendations. With human MHCPs,\nevidence demonstrates that structured professional judgment\nmodels, which provide a framework for systematically assessing\nand using suicide risks to make intervention recommendations, are\neffective strategies for detecting suicide risks and attempts as well as\nretrospectively predicting death by suicide (22– 24).\nIf GAI-LLMs can leverage an effective suicide risk assessment\nmodel, it may enhance the recognition of suicide risks and improve\nclinical decision-making for MHCPs if integrated into clinical\npractice. Over the years, various models for assessing suicide risk\nhave been developed, each vary i n gi ni t sf o c u so np a t i e n t s’\ndemographic and clinic al characteristics ( 11, 23, 25, 26).\nContemporary approaches to suicide risk assessment encourages\na multimethod approach combining both actuarial (rating scales)\nand clinical assessment strategies (clinical interviews), with\nprofessional guidelines advocating for the assessment of evidence-\nbased risk and warning signs (11). However, assessing suicide risk\nposes challenges for MHCPs due to the complexity of balancing\nrisks, protective factors, mental and medical histories, and patients’\npreferences for care (5, 27, 28).\nTo address this complexity and improve suicide risk assessment\nof veterans, the Veterans Health Administration (VHA)\nimplemented evidence-based, standardized suicide screenings in\n2018. This initiative, known as the Suicide Risk Identi ﬁcation\nStrategy (Risk ID), integrated standardized suicide risk screening\nwith a comprehensive evaluation for veterans who receive a positive\nscreening score from an actuarial measure. Risk ID evolved over\nseveral years, and now utilizes the Columbia Suicide Severity Rating\nScale screener (C-SSRS;29), followed by the Comprehensive Suicide\nRisk Evaluation (CSRE;30) for veterans scoring in the critical range\non the C-SSRS. The CSRE is a semi-structured interview facilitating\nidentiﬁcation of suicidal risks, a history of suicidal behaviors, and\npreparatory actions, along with guidelines for treatment that is\ninformed by a risk stratiﬁcation table that explicitly links identiﬁed\nsuicide risks to treatment recommendations. In November 2020,\nRisk ID was fully implemented across all veterans receiving VHA\nservices, achieving universal assessment with over ﬁve million\nveterans screened for suicide between 2018 and 2019 (31). The\nimplementation of Risk ID was shown to increase veteran contact\nand engagement with VHA mental health care services (32).\nWithin Risk ID, the inclusion of a risk stratiﬁcation table was\nconsidered to be an important element for guiding MHCP ’s\ndecisions about veterans ’ treatment following screening and\nassessment. The risk strati ﬁcation table is a key feature of a\nstructured professional judgment model given the explicit linkage\nbetween risk assessment and treatment planning. Based on the work\nof Wortzel and colleagues ( 28), the risk strati ﬁcation table\nemphasizes the use of suicide risk severity and temporality to\ninform treatment recommendations, balancing care needs with\ntreatment recommendations in order to suggest care in the least\nrestrictive environment. This multidimensional model represents\nan improvement over previous models, which rate risk solely based\non symptom severity, and failed to link risks with treatment need.\nAccording to the risk stratiﬁcation table, acute and chronic risks are\nassociated with varying levels (low, intermediate, and high) that are\nbest addressed with speci ﬁc treatment options (e.g., inpatient,\nintensive outpatient, or treatment as usual). Acute suicide risk\nrefers to suicidal ideation that lasts for a short period, typically\nover minutes to days, in combination with suicide risks and\nLauderdale et al. 10.3389/fpsyt.2025.1544951\nFrontiers inPsychiatry frontiersin.org03\nwarning signs at various severity levels. High acute risk is associated\nwith suicidal ideation and the inability to remain safe without\nexternal support, whereas intermediate acute risk may require\npsychiatric hospitalization or intensive outpa tient treatment\ndepending on the presence of the suicidal intent, identi ﬁed\nreasons for living, and/or severity of psychiatric symptoms. In\ncontrast, chronic risk lasts longer, extending beyond days, and\ninclude various levels of risks and warning factors associated with\nsuicide. Intermediate chronic risk may involve numerous suicidal\nrisks such as substance misuse, housing instability, and medical\nconditions occurring in the presence of protective factors (e.g.,\nwanting to live for children and/or religious beliefs). The risk\nstrati ﬁcation table was formalized by the Veterans\nAdministration ’s Rocky Mountain Mental Illness Research,\nEducation, and Clinical Center (MIRECC; 33), which offers\nextensive training on its implementation across the VHA.\nTo date, one investigation has assessed the efﬁcacy of the risk\nstratiﬁcation table in guiding MHCPs ’ identiﬁcation of acute/\nchronic risks and treatment planning. In order to do so, Litschi\nand colleagues (34) ﬁrst convened a panel of experts to develop six\nstandardized training vignettes representing veterans of diverse\nages, racial/ethnic identiﬁcation, branch of military service, and\nacute/chronic risks consistent with the risk stratiﬁcation table and\nresearch literature about veteran suicide risks. The vignettes were\ndistributed to clinicians provi ding services to veterans for\nevaluation followed by distribution to veteran suicide researchers.\nBoth groups rated the vignettes on acute and chronic risks using the\nrisk stratiﬁcation table and provided feedback for better alignment.\nBased on these ratings, consensus was reached for four vignettes,\nand two were modiﬁed further to achieve alignment with the risk\nstrati ﬁcation table. For the investigation of use of the risk\nstratiﬁcation table, the ﬁnalized vignettes were distributed to\nMHCPs employed by the Cohen Veterans Network, which\nprovides outpatient mental health care treatment to military\nservice members, veterans, and their family members.\nApproximately 42 MHCPs (social workers, counselors, and\npsychologists) responded to an online survey in which they were\nrandomly assigned to read four of the vignettes and rate the\nveterans’ acute and chronic risks using the risk stratiﬁcation table.\nAdditionally, they identiﬁed treatment plans based on risk ratings.\nLitschi and associates (34) found that the acute and chronic risk\nratings made by MHCPs mostly aligned with the pre-investigation\nclinicians ’ and researchers ’ ratings. Participants ’ treatment\ndecisions varied in relationship to vignette characteristics, but not\nthe MHCPs ’ background (e.g., mental health profession or\nfamiliarity with the risk strati ﬁcation table), indicating that\nMHCPs from multiple disciplin es and training backgrounds\ncould apply the risk strati ﬁcation table for making treatment\ndecisions. Moreover, Litschi et al. ( 34) found that acute risk\nperception was associated with treatment disposition; vignettes\nwith the highest acute risk ratings were more likely to be\nrecommended for hospitalization, partial hospitalization, or\nintensive outpatient treatment. MHCPs reported in qualitative\nresponses that 1) the perception of chronic risk inﬂuenced acute\nrisk identiﬁcation, 2) a variety of factors were used to determine\ntreatment disposition other than acute risk perception, and 3)\nMHCPs varied in their understanding and application of critical\nconcepts such as suicide intent and suicide preparatory behaviors.\nLitschi and colleagues ’ investigation demonstrated that\nstandardized vignettes may be useful in assessing clinical\ndecisions, such as treatment planning, and the risk stratiﬁcation\ntable facilitated assessment of veteran suicide risk.\nThe ﬁndings from this investigation are intriguing because they\ndemonstrate that MHCPs can beneﬁt from the use of a stratiﬁed\nrisk assessment model to assign risk ratings and plan treatment\ncommensurate with those risks. What has yet to be assessed is\nwhether GAI-LLMs can apply a risk assessment model to accurately\ndetect suicide risk and make appropriate treatment\nrecommendations similar to MHCPs. With GAI-LLMs\ndemonstrating promise that they can facilitate clinical decisions\nin detection of mental disorders [Major Depressive Disorder (15)\nand Borderline Personality Disorder (16)] and suicide risks (18, 20,\n21), exploring the ability of GAI-LLMs to apply a risk stratiﬁcation\nmodel is critical as GAI-LLMs are being increasingly incorporated\ninto mental health practice (13). It is also important to assess GAI-\nLLMs’ ability to follow human assessment models as research has\nfound that healthcare providers rely on risk assessments made by\nGAI-LLMs, even if these assessments are biased (17). Given this,\nour investigation used an innovative approach combining GAI-\nLLM suicide risk assessment with the use of standardized vignettes\nof veterans and the MIRECC’s risk stratiﬁcation table (33)t o\ndetermine if GAI-LLMs are able to identify suicidal risk and\nmake treatment recommendations comparable to MHCPs. Based\non the previousﬁndings, we expected that 1) GAI-LLMs’acute and\nchronic risk ratings would be similar to MHCPs, 2) GAI-LLMs\nwould make more restrictive tr eatment recommendations for\nstandardized vignette characters with higher acute and chronic\nrisks, and 3) GAI-LLMs’ acute and chronic risk ratings would\npredict treatment disposition decisions. We also assessed differences\nin acute and chronic risk assessment across GAI-LLMs, but made\nno speciﬁc predictions as the previous research has shown that\nGAI-LLMs’ risk assessments vary widely across investigations,\nwhich are likely due to variations in GAI-LLM training.\nMethods\nGAI-LLMs and human participants\nThe GAI-LLMs selected for thi s investigation included\nChatGPT-3.5, ChatGPT-4o, and Google Gemini. These GAI-\nLLMs were selected based on their name recognition, frequency\nof use, accessibility, and high ratings in reasoning, language, and\ninstruction following assessments (35, 36).\nAll responses provided by the GAI-LLMs were compared to\nMHCPs’ ratings (N = 42;34) of the same vignettes. The MHCPs’\nresponses were taken from Litschi and colleagues ’ (34)\ninvestigation. In this investigation, Litschi and colleagues\nrecruited participants from a network of clinics across the United\nStates providing mental health care services to active duty service\nLauderdale et al. 10.3389/fpsyt.2025.1544951\nFrontiers inPsychiatry frontiersin.org04\nmembers, veterans, and their family members. A total of 161\nMHCPs were invited to participate in the online investigation,\nand a total of 42 completed the investigation. Participants did not\nreceive compensation for participation. A majority of the MHCPs\nidentiﬁed as women (88.1%) and were social workers (43.9%),\nmarriage and family counselors (19%), professional counselors\n(38%), and psychologists (4.7%). Approximately 90% reported\nhaving two or more years of mental health care experience and\nmost had some familiarity with the risk stratiﬁcation table (73.8%).\nApproximately 30.9% reported at least one previous client surviving\na suicide attempt during services or after termination, and 11.9%\nexperienced a client dying by suicide during services or after\ntermination. Because the GAI-LLMs’ responses were compared to\npublished data, there was no n eed for informed consent for\nthis investigation.\nMaterials\nFour standardized vignettes aboutﬁctional veterans (Bill, Darrell,\nLupe, and Linda; 34, 37) were used to assess the GAI-LLMs ’\nidentiﬁcation of suicide risks. The vignettes reﬂected veterans with\na range of acute and chronic risks, racial/ethnic identiﬁcation (White,\nBlack, and Latine), age (28-55 years of age), gender (two men and two\nwomen), and military service branch (United States Marine Corps,\nArmy, Coast Guard, and Air Force). As an example of vignette\ncontent, Bill, the most acute and chronic risk vignette, was described\nas a 55-year old white man, who had experienced a relationship\nbreak-up and was homeless. He was also described as having several\nprevious psychiatric hospitalizations, suicide attempts, and current\nsuicidal ideation. Bill had immediate access to guns, rehearsed\nshooting himself, and refused to discuss securing his weapons. All\nvignettes used in the investigation are provided online (37). From\nprevious research (34), MHCPs rated the veterans’risks as follows: 1)\nBill: high acute and chronic ris k; 2) Darrell: high acute risk,\nintermediate chronic risk; 3) Lupe: intermediate acute risk, high\nchronic risk; 4) Linda: high acute risk, low chronic risk.\nProcedures\nA zero-prompt approach was used, meaning the GAI-LLMs\nwere given prompts to respond to without example responses or\niterative training. This approach is an effective strategy in revealing\nGAI-LLM capabilities because the output is reﬂective of its neutral\nperformance (38). For each trial, the GAI-LLM was opened in a\nprivacy browser, provided with a vignette, the risk stratiﬁcation\ntable, and asked to rate the veteran’s acute and chronic risks. The\nGAI-LLM was also asked to specify the level of care needed by the\nveteran. Speciﬁcally, all GAI-LLM were 1) directed to read the\nvignette, 2) review instructions and examples from the risk\nstratiﬁcation table for acute risk, and 3) assign the level of acute\nrisk using the rating scale provided (see below) in one prompt. In a\nsecond prompt, the GAI-LLM were directed to 1) review\ninstructions and examples from the risk stratiﬁcation table for\nchronic risk, 2) assign level of chronic risk, and 3) select\ntreatment disposition for the veteran using the treatment\ndisposition rating scale (see below). All prompts and prompting\nmaterials are available from the corresponding author upon\nreasonable request.\nAfter the GAI-LLM responded, the data was copied and the tab\nwas closed. These steps were repeated ten times with each of the\nthree GAI-LLM with the veterans assessed in a sequential\norder, ensuring that data collection for each veteran was\ncompleted before proceeding to the next. A total of 30 trials (N =\n120) were generated for each vignette. All data was collected from\nJuly 23-July 24, 2024.\nMeasures\nAcute and Chronic Suicide Risks (34, 37). The GAI-LLMs were\nasked to rate each veterans’ level of acute and chronic risk (“Using\nthe criteria above, please evaluate this veteran’s ACUTE/CHRONIC\nrisk for suicide”) using a 1 (Low)t o9( High) response scale. Using\nthese items, Litschi and colleagues (34) found that higher acute/\nchronic risk was associated with MHCPs recommending more\nintensive care needs.\nTreatment Disposition (34, 37). The GAI-LLMs were asked to\nindicate the level of care needed by each veteran (“What is your\ndisposition determination for this veteran?”) using a 1 (Plan for\nhospitalization (voluntary/involuntary) )t o4( No further action\nrequired, follow-up as usual) response scale. Litschi and colleagues\n(34) found that veterans rated with higher acute risk were also rated\nas needing more intensive treatment.\nData analysis plan\nAcute and chronic risk ratings for each vignette were compared to\nratings made by the MHCPs using independent groupst-tests. A one-\nway ANOVA with four levels (severity of veterans’risks, with Bill the\nhighest and Linda, the lowest) collapsing across GAI-LLMs was used\nto assess GAI-LLMs’treatment disposition decisions for the veterans.\nFor comparison across GAI-LLMs, three one-way ANOVAs\nwith three levels (for GAI-LLM) were used to assess differences in\nacute risk ratings, chronic risk ratings, and treatment disposition.\nAll statistically signiﬁcant main and interaction effects from the\nANOVAs were assessed using Tukey’s HSD to control for Type 1\nerror. Hierarchical regression analysis controlling for veterans’ risk\n(coded 1 - 4; Bill = 1, Darrell = 2, Lupe = 3, and Linda = 4) and GAI-\nLLM (coded 1– 3; ChatGPT-3.5 = 1; ChatGPT - 4.o = 2; Google\nGemini = 3) was used to assess prediction of treatment disposition\nby GAI-LLMs’ acute and chronic risk ratings. On theﬁrst step, we\nincluded veterans and GAI-LLMs as our analyses found differences\nacross these variables. On the next step, acute and chronic risks\nwere added to the model to assess if these variables were predictive\nof treatment disposition decisions by the GAI-LLMs. All data were\nscreened for outliers and none were detected. All data analyses were\ncompleted using JASP version 0.19.2 (39).\nLauderdale et al. 10.3389/fpsyt.2025.1544951\nFrontiers inPsychiatry frontiersin.org05\nResults\nComparisons of acute risk ratings between GAI-LLMs and\nMHCPs are shown in Figure 1. As seen in Figure 1, the most\nacute vignette rated by MHCPs (Bill;M = 8.70,SD = 0.60) was rated\nas less acute by GAI-LLMs (M = 8.20,SD = 0.96;t(60) = 2.48,p <.05;\nHedge’s g = 0.63, 95% CI: 0.12., 1.13). The least acute risk vignette\nrated by MHCPs (Lupe;M = 5.80, SD = 1.70) was rated as more\nacute by GAI-LLMs (M = 7.60, SD = 0.86; t(59) = 5.19,p <.001;\nHedge’s g = 1.31, 95% CI: 0.76., 1.86). There were no other\ndifferences between MHCPs and GAI-LLMs in acute risk ratings\nfor the other veterans (allps >.05). Comparisons of chronic risk\nratings between GAI-LLMs and MHCPs are shown in\nFigure 2.There were no differences between GAI-LLMs and the\nMHCPs for most chronic risk ratings (allp’s >.05); however, the\nvignette rated with less chronic risk by MHCPs (Linda;M = 3.30,\nSD = 1.50) was rated with more chronic risk by GAI-LLMs (M =\n5.20, SD = 1.83; t(49) = 4.35,p <.001; Hedge’s g = 1.12, 95% CI:\n0.58., 1.66).\nA one-way ANOVA with four levels of veterans assessing mean\ndifferences in treatment disposition collapsing across GAI-LLMs\nwas statistically signiﬁcant (F(3,116) = 12.72,p <.001, h2 = .25). As\nseen inTable 1, the GAI-LLMs indicated that the veterans with the\nhigher chronic suicide risk (Bill, Darrell, and Lupe) required more\nintensive treatment than the veteran with the lowest chronic risk\n(Linda; all ps <.01). No other differences between vignettes\nwere found.\nTo assess variation in ratings across GAI-LLMs, three one-way\nANOVAs with three levels (GAI-LLM) were calculated for acute\nrisk, chronic risk, and treatment disposition. Means differences\nacross GAI-LLMs are shown inTable 2. For acute risk, the overall\nmodel was statistically signiﬁcant (F(2,117) = 64.44,p <.001, h2 =\n.52). ChatGPT-3.5 rated veterans with less acute risk than\nChatGPT-4o or Google Gemini (p’s <.001). ChatGPT-4o rated\nveterans as having more acute risk than Google Gemini (p <.001).\nFor chronic risk, the overall model was statistically signiﬁcant (F\n(2,117) = 4.34, p <.05, h\n2 = .07). ChatGPT-4o identiﬁed more\nchronic risk for veterans than Google Gemini (p <.05).\nA one-way ANOVA assessing treatment disposition was also\nstatistically signi ﬁcant (F(2,117) = 15.87, p <.001, h2 =. 2 1 ) .\nChatGPT-4o indicated every ve teran should be hospitalized\ncompared to Google Gemini (p <.001). ChatGPT-3.5 was also\nlikely to recommend more restrictive treatment than Google\nGemini (p <.01). The ChatGPT GAI-LLMs did not differ from\neach other (p >.05).\nTo assess risks associated with treatment disposition decisions\nby GAI-LLMs, we used hierarchical regression analysis. Prior to the\nregression analysis, bivariate correlations were calculated to assess\nintercorrelations between model variables. Acute risk ratings were\npositively correlated with chronic risk ratings (r(128) = .24,p <.01).\nAcute risk ratings were not correlated with treatment disposition\nratings (r(128) = -.14, p >.05), while chronic risk ratings were (r\n(128) = -.64,p <.001). On theﬁrst step of the hierarchical regression\nmodel, we included both veterans and GAI-LLMs. On the second\nstep, we included both acute and chronic risks ratings identiﬁed by\nGAI-LLM. The results are shown in Table 3 and suggest that\nchronic risk rating, but not acute, were predictive of GAI-LLMs’\ntreatment disposition decisions for veterans after controlling for\nveterans’level of risks. GAI-LLMs also contributed to the prediction\nof treatment disposition in theﬁnal model and the results suggest\nthat Google Gemini was more likely to recommend less restrictive\ntreatment for the veterans.\nFIGURE 1\nAcute risk ratings by GAI-LLM and MHPs. GAI-LLM, Generative Artiﬁcial Intelligence-Large Language Model; MHCPs, Mental health care providers.\nTotal number of GAI-LLM veteran ratings wasn = 30 per veteran. Total number of MHCPs ranged fromn = 21 - 32 per veteran. *p <.05. ***p <.001.\nLauderdale et al. 10.3389/fpsyt.2025.1544951\nFrontiers inPsychiatry frontiersin.org06\nDiscussion\nThis investigation aimed to evaluate the capabilities of various\nGAI-LLMs, speciﬁcally ChatGPT-3.5, ChatGPT-4o, and Google\nGemini, in using the VHA’s risk stratiﬁcation table to identify\nacute and chronic suicide risks and propose treatment plans aligned\nwith the identiﬁed risks. We also assessed if GAI-LLMs’suicide risk\nassessments matched those of human MHCPs who provide mental\nhealth services to veterans. Our investigation represents an\nimportant innovation by incorporating standardized vignettes to\ncompare GAI-LLMs’suicide risk assessment performance to that of\nMHCPs. This methodological approach enhances the evaluation of\nGAI-LLM capabilities through accurate assessment against\nestablished standards. Prior research has not utilized this\napproach, resulting in a signiﬁcant gap in understanding GAI-\nLLMs’ ability to identify suicide risks. This is crucial as recent data\nindicates that suicide rates for veterans in the United States are\ndisproportionately higher compared to the general population, and\nearly detection and prevention of suicidal risks and behaviors can be\ncritical to reducing suicide deaths (3, 40). Although GAI-LLM use\nin the mental healthﬁeld is relatively novel, interest from healthcare\nproviders ( 41) and the public ( 42, 43) is growing rapidly,\nnecessitating a better underst anding of the capabilities and\nlimitations of GAI-LLMs in mental health care. Given the\nincreased integration of AI into mental health care and suicide\nrisk detection, this study provides a necessary examination of GAI-\nLLMs’ efﬁcacy in acute and chronic suicide risk assessments and\ntreatment recommendations.\nBased on previous literature (18), it was expected that GAI-\nLLMs would assess acute and chronic suicide risk similarly to\nMHCPs when using the VHA’s risk stratiﬁcation table, which is a\ncritical element of a structured professional judgment model for\nsuicide prevention. This hypothesis was mostly supported as our\ninvestigation found that GAI-LLMs’ ratings only slightly differed\nfrom those of MHCPs. For acute risk ratings, the GAI-LLMs\ndiffered on two of the vignettes compared to MHCPs. For the\nstandardized vignette rated as most acute by MHCPs, GAI-LLMs’\nratings were slightly less acute; however, GAI-LLMs still rated this\nvignette as having the highest acute risk compared to the other\nvignettes. GAI-LLMs also differed signiﬁcantly in the acute risk\nrating for the vignette rated as least acute by MHCPs. Here, GAI-\nLLMs rated this veteran’s acuity as greater than the MHCPs, but the\nGAI-LLMs’ ratings were lower compared to the other vignettes,\nwhich was consistent with the pattern of ratings made by MHCPs\n(34). When comparing GAI-LLMs’ chronic risk ratings to those of\nMHCPs, the GAI-LLMs only differed on one vignette, which was\nrated as having higher chronic risk than ratings made by the\nMHCPs. These ﬁndings are largely co nsistent with recent\nTABLE 1 Differences in treatment disposition ratings collapsed across GAI-LLM.\nStandardized Vignettes\nBill Mean (SD) Darrell Mean (SD) Lupe Mean (SD) Linda Mean (SD)\nTreatment Disposition 1.00(0.00) a 1.37(0.56)b 1.10(0.31)c 1.87(1.01)a,b,c\nN, 120. GAI-LLM, Generative artiﬁcial intelligence. Cells with the same superscripts are statistically different.\nFIGURE 2\nChronic risk ratings by GAI-LLM and MHPs. GAI-LLM, Generative Artiﬁcial Intelligence-Large Language Model; MHCPs, Mental health care providers.\nTotal number of GAI-LLM veteran ratings wasn = 30 per veteran. Total number of MHCPs ranged fromn = 21 - 32 per veteran. ***p <.001.\nLauderdale et al. 10.3389/fpsyt.2025.1544951\nFrontiers inPsychiatry frontiersin.org07\ninvestigations demonstrating that GAI-LLMs perform similarly to\nMHCPs in recognizing mental health disorders (15, 16) but shows\ngreater variation in identifying suicide risks (18). These results\nsuggest that GAI-LLMs are likely more accurate at tasks involving\nrecognition of discrete symptoms mapping onto speciﬁc criteria,\nbut may suffer performance decrements when attempting to\nidentify variables, such as risks, that require inferences from\nclinical context. That is,“soft” risk factors may pose a bit more of\na challenge for GAI-LLMs to accurately identify. This is not likely\ndue to GAI-LLMs having dif ﬁculty with understanding and\npredicting human emotion as past investigations indicate that\nGAI-LLMs are adept at such tasks (44), but rather it may be that\nintegrating contextual elements ( e.g., stressors, thoughts, and\nemotions) to reliably recognize suicide risk may be at the upper\nlimit of GAI-LLMs’ inferencing abilities. Notably, MHCPs also\nreport difﬁculty in making decisions when“soft” risk factors are\ninvolved and vary in the weight they assign to risk and protective\nfactors, even when using structured or standardized assessments\nand materials (34).\nTwo critical implications from theseﬁndings are GAI-LLMs’\nneed for continued human MHCPs ’ oversight to ensure the\nrelevance and accuracy of assessments. Also clear from our results\nis that GAI-LLMs require on-going training to improve\nperformance in detection of risk factors for psychopathological\nconditions. It is comforting to see that GAI-LLMs tended to make\nmostly more conservative risk ratings than MHCPs, although such\nratings could lead to inappropriately restrictive mental health care.\nNumerous GAI-LLMs applications have demonstrated that effective\ntraining for processes in mental health care are possible (e.g.,\nproviding reﬂections; 13), suggesting that GAI-LLMs’ suicide risk\nidentiﬁcation can be improved. With continued advancement in\nGAI-LLMs’ capabilities, especially with processing human spoken\nlanguage, it is easy to conceive that GAI-LLMs may be integrated\ninto professional service appointments, such as assessments and\npsychotherapy, as a real-time“co-pilot” that is able to synthesize\nsession content and ﬂag noteworthy thoughts, behaviors, and\nemotions relevant for clinical c onsideration, including those\ncritical for suicide risk. Proprietary applications using GAI-LLMs\nhave emerged that are able to summarize and perform sentiment\nanalysis of narrative content provided by clients between\npsychotherapy sessions for use in psychotherapy sessions. It is\neasily conceivable that MHCPs could use this technology in real-\ntime to identify any risks that may need further evaluation or\nintervention, providing an improvement over ML strategies which\ntake much longer to provide critical information.\nAlthough GAI-LLMs are in need of continued training and have\ncertain limitations requiring human MHCP oversight, it is also\nimportant to address the practical implications of implementing\nGAI-LLMs as a support tool in practice. In particular, MHCPs may\nbeneﬁt from receiving training in how to use and prompt GAI-\nLLMs effectively. They also will beneﬁ\nt from developing familiarity\nwith GAI-LLMs’ capabilities and limitations, and using caution\nwhen interpreting GAI-LLMs’outputs as bias and stigma have been\ndetected in their narratives. These capabilities fall under the broad\numbrella of GAI-LLM literacy, which has been investigated and\ndiscussed in primary and secon dary education for years, but\nextension of these concepts to adults, and MHCPs, has been limited.\nSpeciﬁcally, MHCPs need to develop the GAI-LLM literacy\ncompetencies of awareness, usage, evaluation, and ethical\nunderstanding through education and training (45). To facilitate\nawareness, MHCP ’s must learn how GAI-LLMs have been\nintegrated into numerous applications, particularly those used to\nprovide mental health care and support to people in need of mental\nhealth services ( 13). Usage education would entail providing\ndescriptions of GAI-LLM speciﬁc applications for mental health,\nincluding the development of GAI-LLM applications to assess for\nanxiety and depression as well as GAI-LLM that provide\npsychotherapy (13). Critical to this discussion is the strength and\nlimitations of the evidence-base for GAI-LLMs providing mental\nhealth assessment and treatment. Evaluation entails encouraging\nMHCPs to recognize that GAI-LLM generated information may\nTABLE 2 Differences in GAI-LLM for acute risk, chronic risk, and treatment disposition.\nChatGPT-3.5 Mean (SD) ChatGPT-4o Mean (SD) Google Gemini Mean (SD)\nAcute Risk 7.05(0.22) a 8.73(0.64)a,b 7.85(0.92)a,b\nChronic Risk 6.80(1.16) 7.58(1.66) b 6.55(1.95)b\nTreatment Disposition 1.25(0.71) a 1.00(0.00)b 1.75(0.78)a,b\nGAI-LLM, Generative artiﬁcial intelligence large language models. Cells with the same superscript are statistically different from each other.\nTABLE 3 Prediction of GAI-LLM treatment disposition by acute and\nchronic risks after controlling for veterans and GAI-LLM.\nVariable b R2adj DR2\nStep 1 .24*** .24***\nVeteran .38***\nGAI-LLM .30***\nStep 2 .47*** .25***\nVeteran .05\nGAI-LLM .31***\nAcute Risk -.11\nChronic Risk -.56***\nGAI-LLM, Generative artiﬁcial intelligent programs. Veteran coded 1 - 4; Bill, 1, Darrell, 2,\nLupe, 3, and Linda, 4. GAI-LLM coded 1– 3; ChatGPT-3.5, 1; ChatGPT-4o, 2; Google\nGemini, 3.\n***p <.001.\nLauderdale et al. 10.3389/fpsyt.2025.1544951\nFrontiers inPsychiatry frontiersin.org08\nseem compelling, but has substantial limitations given that it can be\ninaccurate, show bias, and express public stigma. MHCPs using\nGAI-LLMs should be encouraged to verify any information received\nfrom GAI-LLMs with credible sources. Finally, MHCPs using GAI-\nLLMs should be informed about the ethical concerns of GAI-LLM\nuse. Of these, knowledge of how user’s data, including questions\nposed to GAI-LLMs and users’responses to GAI-LLMs, are used to\ndevelop and train GAI-LLMs is critical as conﬁdentiality of user’s\ndata is not guaranteed (13). Potential MHCP GAI-LLM users need\nto know this information so they can make informed decisions\nabout whether they should incorporate GAI-LLMs into professional\nservice activities in light of ethical and legal standards. At a\nminimum, MHCPs who want to use GAI-LLMs need to search\nfor those applications that meet legal requirements for\nelectronic conﬁdentiality.\nOur investigation also assessed GAI-LLMs’ ability to select\nappropriate treatment recommendations given the risks identiﬁed\nfrom the vignettes, which has not been attempted in previous GAI-\nLLM investigations (20). Our results were mostly consistent with\nour hypotheses and demonstrated that GAI-LLMs tended to select\nmore restrictive treatment plans for veterans when chronic risk was\nelevated. Interestingly, these ﬁndings dovetail with Litschi and\ncolleagues ( 34)s u g g e s t i n gt h a tM H C P s’ treatment disposition\nratings were associated with acute risk ratings, which were\nstrongly inﬂuenced by chronic risk ratings. The implication is\nthat GAI-LLMs, like humans, may emphasize chronic risk\nassessment in determining appropriateness of treatment. The\nreasons for these ﬁndings are unclear. Again, on-going (16) and\npast research (15) suggest that GAI-LLMs are adept at selecting\nevidence-based interventions for a variety of mental health\nconditions. It is possible that the GAI-LLMs in this investigation\ndemonstrated a preference for addressing long-term risks as a\nmeans of treatment planning for suicide, which suggests that\nGAI-LLMs may need further training to incorporate acute risks\nin developing treatment recommendations to provide support in\nthe least restrictive environment. It is also possible that the order in\nwhich GAI-LLMs were asked to make decisions about acute and\nchronic risks may have inﬂuenced treatment dispositions decisions.\nIn our investigation, the chronic risk assessment was the decision\nGAI-LLMs made just prior to providing treatment disposition, as it\nwas in the original Litschi and colleagues’ (34) investigation with\nMHCPs. Thus, GAI-LLMs may have used its chronic risk ratings as\na guide for treatment planning. The VHA risk stratiﬁcation table\nexplicitly informed GAI-LLMs that if a vignette showed high acuity,\nthen inpatient hospitalization would likely be necessary, bringing\ninto question what led GAI-LLMs to prioritize chronic risk over\nacute risks ratings in determining treatment disposition. Regardless,\nthe data emphasizes concerns regarding GAI-LLMs ’ ability to\naccurately assess risks and use them to make informed\ntreatment decisions.\nThe present investigation also revealed variations in GAI-LLMs’\nperformance, which is consistent with past research (18, 20, 21).\nChatGPT-3.5 had lower ratings for acute risk compared to\nChatGPT-4o and Google Gemini. However, ChatGPT-4o rated\nveterans as having more acute and chronic risks compared to\nGoogle Gemini and recommended hospitalization for every\nveteran. This treatment disposition was seen as more restrictive\ncompared to the other two GAI-LLMs, which, on average,\nrecommended outpatient treatment. These ﬁndings reﬂect that\nthere are important variations between GAI-LLMs, which are\nlikely due to differences in training for each model. GAI-LLM\ntraining is considered proprietary and guarded as trade secrets in\nGAI-LLM development, which limits appreciation of how GAI-\nLLM training may affect its responses. If GAI-LLMs use are\ncontinued in mental health applications, which in all likelihood it\nwill be, GAI-LLM developers are strongly encouraged to\nincorporate MHCPs and individuals with lived experiences (e.g.,\npeople who have experienced mental health disorders, people who\nhave experienced suicide risks, and veterans) into their GAI-LLM\ntraining processes in order to address how GAI-LLM prioritizes\ninformation when making decisions about mental health risks\nand treatment.\nThe present investigation has several limitations to be\nconsidered when considering the ﬁndings. One such limitation\nwas the inability to counterbalance important factors, such as\ngender, race/ethnic identi ﬁcation, and age, presented in the\nvignettes. This leads to potential confounds when considering the\nresults given that these factors were inseparable from the acute/\nchronic risks ratings. Future investigations should assess how these\nfactors may shape GAI-LLMs ’ responses given that other\ninvestigations have found that GAI-LLMs will show biased\nresponses when provided these characteristics (16, 20, 21, 46, 47).\nThese ﬁndings are not surprising as GAI-LLMs training sources are\nhuman derived, meaning that biases and stigma are incorporated\ninto GAI-LLM. It is hoped that GAI-LLM developers will work to\nincorporate mental health care professionals and people with lived\nexperiences into their development of GAI-LLM applications used\nfor mental health as a means of identifying and reducing the\npresence of bias and stereotypes incorporated into GAI-LLMs ’\nalgorithms. Related to this, GAI-LLM developers are encouraged\nto provide greater transparency about their training sources and to\nincorporate strategic prompt ing strategies to allow for the\nsystematic identiﬁcation of bias and stereotypes so these can be\naddressed in training. Methodical use of red-teaming (48) and\nincorporation of multiple GAI-LLM guardrails (in which GAI-\nLLM’s responses are evaluated multiple times prior to release)\nwould also be useful in reducing biased GAI-LLM responding\n(49). Consideration of these approaches are critical given that\nmany in the general population value GAI-LLMs’ responses (50)\nand people with stigmatized conditions prefer GAI-LLM\ninterventions to avoid stigmatization by human MHCPs (42, 43).\nThis suggests that bias and stereotypes expressed by GAI-LLMs has\nthe potential to result in substantial harm to unsuspecting users\nwho are turning to GAI-LLM for help.\nAn additional noteworthy limitation is that when working with\nGAI-LLMs, researchﬁndings are time limited given that GAI-LLM\nare continuously evolving and developers frequently release\nupdated applications. Newer versions of GAI-LLMs do not always\nLauderdale et al. 10.3389/fpsyt.2025.1544951\nFrontiers inPsychiatry frontiersin.org09\nmean improvement, although that tends to be the trend (51). To\naddress this limitation, future investigators are encouraged to\nreplicate and extend our results with newer versions of the GAI-\nLLM used in this investigation. Another avenue of future research\nwould be to iteratively assess GAI-LLMs’ training to determine if\nrisk assessment and treatment recommendations can be improved.\nEffective use of GAI-LLMs is highly dependent on the prompts\nused, and a variety of iterative prompt strategies could be used to\nﬁne-tune GAI-LLMs’ performance in identifying suicide risks and\ntreatment planning ( 38). Future efforts can also look into\nd e v e l o p i n gs t r a t e g i e sf o rG A I - L L M st oe n h a n c ei t sa b i l i t yt o\neffectively weigh risk factor s when determining treatment\ndisposition. As part of this process, careful attention should be\npaid to the information that GAI-LLMs are using to assign acute\nand chronic risks to determine if these are consistent with the way\nrisks have been conceptualized in the suicide prevention literature.\nWith the increased integration of GAI-LLMs into the mental health\nﬁeld and the potential for use of GAI-LLMs to provide real-time risk\nidentiﬁcation and psychotherapy, additional training is needed to\ncapitalize on the promise GAI-LLMs offer.\nA ﬁnal limitation of our research was the reliance on\nstandardized vignettes. These v ignettes contained information\ndesigned to assist in making acute and chronic risk assessments,\naligning with our aim to evaluate whether GAI-LLMs could\nperform comparably to MHCPs. However, we did not evaluate\nGAI-LLMs’performance using real-world clinic information, where\ncrucial information about acute and chronic risks might be missing.\nThis limitation affects the generalizability of ourﬁndings and we\nrecommend future studies to explore GAI-LLMs’effectiveness using\nclinical data from real-world environments to better assess its\npotential generalizability to clinical practice.\nOverall, the current investigation demonstrated that when given\na suicide risk assessment model to follow, GAI-LLMs are able to\nidentify acute and chronic suicide risks in veterans similarly to\nMHCPs, with some variation. This reﬂects the potential of GAI-\nLLMs to be further incorporated into mental health practice while\nhighlighting the need for on-going training toﬁne-tune GAI-LLM\nperformance. It is important to note that GAI-LLMs show a gap in\nassessing what factors should determine treatment disposition,\nemphasizing its current limitations in evaluating the next steps\nwhen presented with veterans with suicidal ideation. All in all, these\nﬁndings contribute to the ever-growing body of GAI-LLM literature\nand, most importantly, the results suggest that GAI-LLMs have the\npromise of providing real-time identiﬁcation of suicide risks and\ntreatment recommendations. However, further reﬁnement and on-\ngoing human MHCP oversight is critical.\nData availability statement\nThe raw data supporting the conclusions of this article will be\nmade available by the authors, without undue reservation.\nEthics statement\nEthical approval was not required for the study involving\nhumans in accordance with the local legislation and institutional\nrequirements. Written informed consent to participate in this study\nwas not required from the participants or the participants’ legal\nguardians/next of kin in accordance with the national legislation\nand the institutional requirements.\nAuthor contributions\nSL: Conceptualization, Formal Analysis, Investigation,\nMethodology, Project administration, Writing – original draft,\nWriting – review & editing. RS: Investigation, Methodology,\nProject administration, Supervision, Writing – original draft,\nWriting – review & editing. BW: Data curation, Project\nadministration, Writing – original draft, Writing – review &\nediting. ND: Conceptu alization, Writing – original draft,\nWriting – review & editing. HD: Conceptualization, Writing –\noriginal draft, Writing – review & editing. ST: Data curation,\nInvestigation, Writing– review & editing.\nFunding\nThe author(s) declare that noﬁnancial support was received for\nthe research and/or publication of this article.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial orﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nGenerative AI statement\nThe author(s) declare that no Generative AI was used in the\ncreation of this manuscript.\nPublisher’s note\nAll claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their af ﬁliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nLauderdale et al. 10.3389/fpsyt.2025.1544951\nFrontiers inPsychiatry frontiersin.org10\nReferences\n1. US Department of Veterans Affairs. 2023 national veteran suicide prevention\nannual report(2023). Available online at: https://www.mentalhealth.va.gov/docs/data-\nsheets/2023/2023-National-Veteran-Suicide-Prevention-Annual-Report-FINAL-508.\npdf (Accessed December 19, 2024).\n2. Schafer KM, Duffy M, Kennedy G, Stentz L, Leon J, Herrerias G, et al. Suicidal\nideation, suicide attempts, and suicide death among Veterans and service members: A\ncomprehensive meta-analysis of risk factors. Military Psychol . (2022) 34:129– 46.\ndoi: 10.1080/08995605.2021.1976544\n3. Horowitz LM, Ryan PC, Wei AX, Boudreaux ED, Ackerman JP, Bridge JA.\nScreening and assessing suicide risk in medical settings: feasible strategies for early\ndetection. Focus (Am Psychiatr Publ). (2023) 21:145. doi: 10.1176/appi.focus.20220086\n4. Stanley B, Mann JJ. The need for innovation in health care systems to improve\nsuicide prevention. JAMA Psychiatry . (2020) 77:96 – 8. doi: 10.1001/\njamapsychiatry.2019.2769\n5. Nelson HD, Denneson LM, Low AR, Bauer BW, O’Neil M, Kansagara D, et al.\nSuicide risk assessment and prevention: A systematic review focusing on veterans.\nPsychiatr Serv. (2017) 68:1003– 15. doi: 10.1176/appi.ps.201600384\n6. Perlis RH, Fihn SD. Hard truths about suicide prevention.JAMA Netw Open.\n(2020) 3:e2022713. doi: 10.1001/jamanetworkopen.2020.22713\n7. VA. AI inventory(2024). Available online at: https://www.research.va.gov/naii/ai-\ninventory.cfm (Accessed July 7, 2024).\n8. IBM. What is artiﬁcial learning (AI)? Available online at: https://www.ibm.com/\ntopics/artiﬁcial-intelligence (Accessed January 21, 2025).\n9. Matarazzo BB, Eagan A, Landes SJ, Mina LK, Clark K, Gerard GR, et al. The\nVeterans Health Administration REACH VET program: suicide predictive modeling in\npractice. Psychiatr Serv. (2023) 74:206– 9. doi: 10.1176/appi.ps.202100629\n10. McCarthy JF, Cooper SA, Dent KR, Eagan AE, Matarazzo BB, Hannemann CM,\net al. Evaluation of the Recovery Engagement and Coordination for Health-Veterans\nEnhanced Treatment suicide risk modeling clinical program in the Veterans Health\nAdministration. JAMA Netw Open . (2021) 4:e2129900. doi: 10.1001/\njamanetworkopen.2021.29900\n11. Bernert RA, Hilberg AM, Melia R, Kim JP, Shah NH, Abnousi F. Artiﬁcial\nintelligence and suicide prevention: a systematic review of machine learning\ninvestigations. Int J Environ Res Public Health . (2020) 17:5929. doi: 10.3390/\nijerph17165929\n12. Glantz A. Why is the female veteran rate spiking? Could it be AI? Available\nonline at: https://fuller project.org/story/arti ﬁcial-intelligence-veteran-suicide-\nprevention-algorithm-favors-men-2/ (Accessed June 1, 2024).\n13. Casu M, Triscari S, Battiato S, Guarnera L, Caponnetto P. AI chatbots for mental\nhealth: A scoping review of effectiveness, feasibility, and applications.Appl Sci. (2024)\n14:5889. doi: 10.3390/app14135889\n14. Fonseka TM, Bhat V, Kennedy SH. The utility of artiﬁcial intelligence in suicide\nrisk prediction and the management of suicidal behaviors.Aust New Z J Psychiatry.\n(2019) 53:954– 64. doi: 10.1177/0004867419864428\n15. Elyoseph Z, Levkovich I, Shinan-Altman S. Assessing prognosis in depression:\ncomparing perspectives of AI models, mental health professionals and the general\npublic. Family Med Community Health. (2024) 12. doi: 10.1136/fmch-2023-002583\n16. Lauderdale S, Grifﬁn S, Lahman K, Mbaba E, Tomlinson S. Unveiling public\nstigma for borderline personality disorder: A comparative study of artiﬁcial intelligence\nand mental health care providers.J Ment Health Pers. (2024) 12.\n17. Adam H, Balagopalan A, Alsentzer E, Christia F, Ghassemi M. Mitigating the\nimpact of biased artiﬁcial intelligence in emergency decision-making.Commun Med\n(Lond). (2022) 2:82. doi: 10.1038/s43856-022-00214-4\n18. Levkovich I, Elyoseph Z. Suicide risk assessments through the eyes of ChatGPT-\n3.5 versus ChatGPT-4: Vignette study.JMIR Ment Health. (2023) 10. doi: 10.2196/\n51232\n19. Joiner TE Jr., Van Orden KA, Witte TK, Selby EA, Ribeiro JD, Lewis R, et al.\nMain predictions of the interpersonal – psychological theory of suicidal behavior:\nEmpirical tests in two samples of young adults.J Abnorm Psychol. (2009) 118:634.\ndoi: 10.1037/a0016500\n20. Shinan-Altman S, Elyoseph Z, Levkovich I. Integrating previous suicide\nattempts, gender, and age into suicide risk assessment using advanced arti ﬁcial\nintelligence models.J Clin Psychiatry. (2024) 85:57125. doi: 10.4088/JCP.24m15365\n21. Shinan-Altman S, Elyoseph Z, Levkovich I. The impact of history of depression\nand access to weapons on suicide risk assessment: a comparison of ChatGPT-3.5 and\nChatGPT-4. PeerJ. (2024) 12. doi: 10.7717/peerj.17468\n22. Fagan J, Ijaz A, Papaconstantinou A, Lynch A, O’Neill H, Kennedy HG. The\nsuicide risk assessment and management manual (S-RAMM) validation study II.Irish J\npsychol Med. (2009) 26:107– 13. doi: 10.1017/S0790966700000380\n23. Gray NS, John A, McKinnon A, Raybould S, Knowles J, Snowden RJ. Structured\nprofessional judgment to assist the evaluation and safety planning of suicide risk: The\nRisk of Suicide Protocol (RoSP). Front Psychiatry. (2021) 12:607120. doi: 10.3389/\nfpsyt.2021.607120\n24. Ijaz A, Papaconstantinou A, O ’Neill H, Kennedy HG. The suicide risk\nassessment and management manual (S-RAMM) validation study 1. Ir J Psychol\nMed. (2009) 26:54– 8. doi: 10.1017/S0790966700000215\n25. Kral MJ, Sakinofsky I. Clinical model for suicide risk assessment.Death Stud.\n(1994) 18:311– 26. doi: 10.1080/07481189408252680\n26. Chu C, Klein KM, Buchman-Schmitt JM, Hom MA, Hagan CR, Joiner TE.\nRoutinized assessment of suicide risk in clinical practice: an empirically informed\nupdate. J Clin Psychol. (2015) 71:1186– 200. doi: 10.1002/jclp.2015.71.issue-12\n27. Sequeira L, Strudwick G, De Luca V, Strauss J, Wiljer D. Exploring uniformity of\nclinical judgment: a vignette approach to understanding healthcare professionals ’\nsuicide risk assessment practices. J Patient Saf. (2022) 18:e962– 70. doi: 10.1097/\nPTS.0000000000000973\n28. Wortzel HS, Homaifar B, Matarazzo B, Brenner LA. Therapeutic risk\nmanagement of the suicidal patient: stratifying risk in terms of severity and\ntemporality. JP s y c h i a t rP r a c t. (2014) 20:63 – 7. doi: 10.1097/\n01.pra.0000450321.06612.7a\n29. Posner K. Columbia-suicide severity rating scale.Irish Journal of Psychological\nMedicine (2016) 26:54– 8. doi: 10.1037/t52667-000\n30. Sall J, Brenner L, Millikan Bell AM, Colston MJ. Assessment and management of\npatients at risk for suicide: synopsis of the 2019 U.S. Department of veterans affairs and\nU.S. Department of defense clinical practice guidelines. Ann Intern Med. (2019)\n171:343– 53. doi: 10.7326/M19-0687\n31. Bahraini N, Brenner LA, Barry C, Hostetter T, Keusch J, Post EP, et al.\nAssessment of rates of suicide risk screening and prevalence of positive screening\nresults among US veterans after implementation of the veterans affairs suicide risk\nidenti ﬁcation strategy. JAMA Netw Open .( 2 0 2 0 )3 .d o i :1 0 . 1 0 0 1 /\njamanetworkopen.2020.22531\n32. Bahraini N, Reis DJ, Matarazzo BB, Hostetter T, Wade C, Brenner LA. Mental\nhealth follow-up and treatment engagement following suicide risk screening in the\nVeterans Health Administration.PloS One. (2022) 17. doi: 10.1371/journal.pone.0265474\n33. Mental Illness, Research, Education, and Clinical Center. Risk strati\nﬁcation table\n(2024). Available online at: https://www.mirecc.va.gov/visn19/trm/table.asp (Accessed\nApril 26, 2024).\n34. Litschi MA, Lancaster SL, Linkh DJ, Renno S, Neal-Walden TA, Lawless CE,\net al. Evaluating suicide risk stratiﬁcation in outpatient settings: A vignette-based\napproach. Clin Psychol Psychother. (2024) 31. doi: 10.1002/cpp.2965\n35. Livebench.ai. Available online at: https://livebench.ai//?Reasoning=\na&Language=a&IF=a (Accessed May 30, 2024).\n36. White C, Dooley S, Roberts M, Pal A, Feuer B, Jain S, et al. LiveBench: A\nchallenging, contaminat ion-free LLM benchmark. arXiv . (2024), 2406.19314.\ndoi: 10.48550/arXiv.2406.19314\n37. Cohen veterans network(2024). Available online at: https://www.\ncohenveteransnetwork.org/srs/ (Accessed April 26, 2024).\n38. Sivarajkumar S, Kelley M, Samolyk-Mazzanti A, Visweswaran S, Wang Y. An\nempirical evaluation of prompting strategies for large language models in zero-shot\nclinical natural language processing: algorithm development and validation study.\nJMIR Med Inform. (2024) 12:e55318. doi: 10.2196/55318\n39. JASP Team. JASP (Version 0.19.2)[Computer software. Available online at:\nhttps://jasp-stats.org/.\n40. Schafer KM, Duffy M, Kennedy G, Stentz L, Leon J, Herrerias G, et al. Suicidal\nideation, suicide attempts, and suicide death among veterans and service members: a\ncomprehensive meta-analysis of risk factors. Mil Psychol . (2022) 34:129 – 46.\ndoi: 10.1080/08995605.2021.1976544\n41. Scott IA, Carter SM, Coiera E. Exploring stakeholder attitudes towards AI in\nclinical practice.BMJ Health Care Inform. (2021) 28. doi: 10.1136/bmjhci-2021-100450\n42. Aktan ME, Turhan Z, Dolu I. Attitudes and perspectives towards the preferences\nfor artiﬁcial intelligence in psychotherapy.Comput Hum Behav. (2022) 133:107273.\ndoi: 10.1016/j.chb.2022.107273\n43. Hoffman BD, Oppert ML, Owen M. Understanding young adults’ attitudes\ntowards using AI chatbots for psychotherapy: The role of self-stigma.Comput Hum\nBehav: Artif Humans. (2024) 2:100086. doi: 10.1016/j.chbah.2024.100086\n44. Elyoseph Z, Hadar-Shoval D, Asraf K, Lvovsky M. ChatGPT outperforms\nhumans in emotional awareness evaluations. Front Psychol . (2023) 14:1199058.\ndoi: 10.3389/fpsyg.2023.1199058\n45. Wang B, Rau PLP, Yuan T. Measuring user competence in using artiﬁcial\nintelligence: validity and reliability of artiﬁcial intelligence literacy scale. Behav Inf\nTechnol. (2023) 42:1324– 37. doi: 10.1080/0144929X.2022.2072768\n46. Heinz MV, Bhattacharya S, Trudeau B, Quist R, Song SH, Lee CM, et al. Testing\ndomain knowledge and risk of bias of a large-scale general artiﬁcial intelligence model\nin mental health. Digit Health . (2023) 9:20552076231170499. doi: 10.1177/\n20552076231170499\n47. Straw I, Callison-Burch C. Artiﬁcial Intelligence in mental health and the biases\nof language based models.PloS One. (2020) 15. doi: 10.1371/journal.pone.0240376\nLauderdale et al. 10.3389/fpsyt.2025.1544951\nFrontiers inPsychiatry frontiersin.org11\n48. Ganguli D, Lovitt L, Kernion J, Askell A, Bai Y, Kadavath S, et al. Red teaming\nlanguage models to reduce harms: methods, scaling behaviors, and lessons learned.\narXiv. (2022) 2407:7786. doi: 10.48550/arXiv.2407.07786\n49. Schuler D. How and why to build an AI gateway(2024). Available online at: https://\nwww.phdata.io/blog/how-and-why-to-build-an-ai-gateway/ (Accessed October 11, 2024).\n50. Shahsavar Y, Choudhury A. User intentions to use ChatGPT for self-diagnosis\nand health-related purposes: cross-sectional survey study.JMIR Hum Factors. (2023)\n10:e47564. doi: 10.2196/47564\n51. Chen L, Zaharia M, Zou J. How is ChatGPT’s behavior changing over time?\narXiv preprint arXiv:2307.09009. (2023). doi: 10.48550/arXiv.2307.09009\nLauderdale et al. 10.3389/fpsyt.2025.1544951\nFrontiers inPsychiatry frontiersin.org12",
  "topic": "Mental health",
  "concepts": [
    {
      "name": "Mental health",
      "score": 0.6763143539428711
    },
    {
      "name": "Risk stratification",
      "score": 0.5936168432235718
    },
    {
      "name": "Risk assessment",
      "score": 0.4552000164985657
    },
    {
      "name": "Medicine",
      "score": 0.41704726219177246
    },
    {
      "name": "Generative grammar",
      "score": 0.41268640756607056
    },
    {
      "name": "Psychology",
      "score": 0.3978102505207062
    },
    {
      "name": "Psychiatry",
      "score": 0.30638131499290466
    },
    {
      "name": "Artificial intelligence",
      "score": 0.23236629366874695
    },
    {
      "name": "Computer science",
      "score": 0.22920045256614685
    },
    {
      "name": "Computer security",
      "score": 0.12533244490623474
    },
    {
      "name": "Internal medicine",
      "score": 0.1134258508682251
    }
  ]
}