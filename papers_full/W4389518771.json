{
    "title": "ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models",
    "url": "https://openalex.org/W4389518771",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2099607494",
            "name": "Zhipeng Chen",
            "affiliations": [
                "Renmin University of China",
                "Beijing Institute of Big Data Research"
            ]
        },
        {
            "id": "https://openalex.org/A2102854352",
            "name": "Kun Zhou",
            "affiliations": [
                "Renmin University of China",
                "Beijing Institute of Big Data Research"
            ]
        },
        {
            "id": "https://openalex.org/A2478820358",
            "name": "Beichen Zhang",
            "affiliations": [
                "Renmin University of China",
                "Beijing Institute of Big Data Research"
            ]
        },
        {
            "id": "https://openalex.org/A2102511683",
            "name": "Zheng Gong",
            "affiliations": [
                "Renmin University of China",
                "Beijing Institute of Big Data Research"
            ]
        },
        {
            "id": "https://openalex.org/A2098250721",
            "name": "Zhao Xin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3212238123",
            "name": "Ji-Rong Wen",
            "affiliations": [
                "Beijing Institute of Big Data Research",
                "Renmin University of China"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4379539660",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4320165837",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W4377865087",
        "https://openalex.org/W4281557623",
        "https://openalex.org/W4385571505",
        "https://openalex.org/W4221161695",
        "https://openalex.org/W4320005767",
        "https://openalex.org/W4309591663",
        "https://openalex.org/W4385570391",
        "https://openalex.org/W4304195432",
        "https://openalex.org/W2889787757",
        "https://openalex.org/W4385573855",
        "https://openalex.org/W4319049323",
        "https://openalex.org/W4379473918",
        "https://openalex.org/W4327810433",
        "https://openalex.org/W4366566341",
        "https://openalex.org/W4385571421",
        "https://openalex.org/W4281483047",
        "https://openalex.org/W4362508231",
        "https://openalex.org/W4361866031",
        "https://openalex.org/W4377371656",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W4313304293",
        "https://openalex.org/W4378474282",
        "https://openalex.org/W4283768109",
        "https://openalex.org/W3156636935",
        "https://openalex.org/W4388626886",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4386044080",
        "https://openalex.org/W4282916217",
        "https://openalex.org/W4362515116",
        "https://openalex.org/W4389520779",
        "https://openalex.org/W3134642945",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4366565380",
        "https://openalex.org/W4385567594",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4238867056",
        "https://openalex.org/W4377130677"
    ],
    "abstract": "Although large language models (LLMs) have achieved excellent performance in a variety of evaluation benchmarks, they still struggle in complex reasoning tasks which require specific knowledge and multi-hop reasoning. To improve the reasoning abilities, we propose ChatCoT, a tool-augmented chain-of-thought reasoning framework for chat-based LLMs (e.g., ChatGPT). In ChatCoT, we model the chain-of-thought (CoT) reasoning as multi-turn conversations, to utilize tools in a more natural way through chatting. At each turn, LLMs can either interact with tools or perform the reasoning. Our approach can effectively leverage the multi-turn conversation ability of chat-based LLMs, and integrate the thought chain following and tools manipulation in a unified way. Specially, we initialize the early turns of the conversation by the knowledge about tools, tasks, and reasoning format, and propose an iterative tool-augmented reasoning step to perform step-by-step tool-augmented reasoning. The experiment results on two complex reasoning datasets (MATH and HotpotQA) have shown the effectiveness of ChatCoT on complex reasoning tasks, achieving a 7.9% relative improvement over the state-of-the-art baseline.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 14777‚Äì14790\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nChatCoT: Tool-Augmented Chain-of-Thought Reasoning on\nChat-based Large Language Models\nZhipeng Chen1,3‚àó, Kun Zhou2,3‚àó, Beichen Zhang1,3, Zheng Gong1,3,\nWayne Xin Zhao1,3‚Ä† and Ji-Rong Wen1,2,3\n1Gaoling School of Artificial Intelligence, Renmin University of China.\n2School of Information, Renmin University of China.\n3Beijing Key Laboratory of Big Data Management and Analysis Methods.\n{zhipeng_chen,gongzheng0109,jrwen}@ruc.edu.cn,francis_kun_zhou@163.com\n{zhangbeichen724,batmanfly}@gmail.com\nAbstract\nAlthough large language models (LLMs) have\nachieved excellent performance in a variety of\nevaluation benchmarks, they still struggle in\ncomplex reasoning tasks which require specific\nknowledge and multi-hop reasoning. To im-\nprove the reasoning abilities, we propose Chat-\nCoT, a tool-augmented chain-of-thought rea-\nsoning framework for chat-based LLMs (e.g.,\nChatGPT). In ChatCoT, we model the chain-\nof-thought (CoT) reasoning as multi-turn con-\nversations, to utilize tools in a more natural\nway through chatting. At each turn, LLMs\ncan either interact with tools or perform the\nreasoning. Our approach can effectively lever-\nage the multi-turn conversation ability of chat-\nbased LLMs, and integrate the thought chain\nfollowing and tools manipulation in a unified\nway. Specially, we initialize the early turns\nof the conversation by the knowledge about\ntools, tasks, and reasoning format, and pro-\npose an iterativetool-augmented reasoningstep\nto perform step-by-step tool-augmented rea-\nsoning. The experiment results on two com-\nplex reasoning datasets (MATH and HotpotQA)\nhave shown the effectiveness of ChatCoT on\ncomplex reasoning tasks, achieving a 7.9%\nrelative improvement over the state-of-the-art\nbaseline. Our code and data are available at:\nhttps://github.com/RUCAIBOX/ChatCoT.\n1 Introduction\nRecently, large language models (LLMs) (Zhao\net al., 2023a) have shown great potential as general-\npurpose task solvers in a variety of real-world ap-\nplications. With excellent few-shot and zero-shot\nability, LLMs, such as GPT-4 (OpenAI, 2023) and\nLLaMA (Touvron et al., 2023), can even outper-\nform full-data supervised-tuned models on many\ntasks with suitable prompting strategies.\nAmong these prompting strategies, chain-of-\nthought (CoT) prompting (Wei et al., 2022; Ko-\n‚àóEqual contributions.\n‚Ä†Corresponding author.\njima et al., 2022) has been a prominent approach\nto eliciting the reasoning abilities of LLMs. It\nincorporates the intermediate reasoning steps of\nexemplars into the input prompt, to instruct LLMs\nto solve a question step by step. Despite the re-\nmarkable improvement by CoT prompting, LLMs\nstill have difficulties in solving complex reasoning\ntasks that involve specific functionalities, such as\narithmetic calculation and information retrieval (Lu\net al., 2022; Qian et al., 2022). To address this is-\nsue, external tools (e.g., calculator, search engine)\nhave been employed to fulfill the basic functional-\nities (Schick et al., 2023; Paranjape et al., 2023),\neasing the burden of LLMs. With proper interfaces,\nLLMs can be guided by prompts to manipulate\ntools when necessary.\nHowever, as tools are not intrinsically inte-\ngrated with LLMs, incorporating external tools\nwould have to interrupt the CoT reasoning pro-\ncess of LLMs. Such an issue would become more\nintractable on complex reasoning tasks that fre-\nquently invoke the use of tools. To address it, exist-\ning work either relies on LLMs to prearrange the\ntool use plan for subsequent execution (Zhou et al.,\n2022; Jiang et al., 2023b), or needs to design for-\nmal actions pertaining to specific tasks (Dua et al.,\n2022; Khattab et al., 2022; Jiang et al., 2023a). De-\nspite the effectiveness, the two types of methods\nstill suffer from potential issues: the former one\ncannot interact with tools after generating the plan,\neven seeing obvious mistakes; while the latter one\nhas to frequently switch between reasoning with\nLLMs and taking actions, hurting the continuity of\nthe CoT reasoning process.\nTo overcome these disadvantages, we seek a\nmore unified way to integrate CoT reasoning and\ntool manipulation. As the key idea, we consider\ntools manipulation by LLMs as the interaction be-\ntween LLMs and tools, in which LLMs send the\nuse requests and tools respond to support specific\nfunctions. Further, inspired by the recent progress\n14777\nof ChatGPT-like LLMs (called chat-based LLMs),\nwe model the interaction process between LLMs\nand tools as a multi-turn conversation, and leverage\nthe excellent chatting capacities for manipulating\ntools by LLMs. At each turn, the LLM can freely\ninteract with tools when in need, otherwise perform\nthe reasoning by itself. The conversation contin-\nues until the final answer is derived by LLMs. In\nthis process, as chat-based LLMs can well under-\nstand the multi-turn context, they can follow the\nthought chain in the whole conversation and natu-\nrally invoke the tools accordingly, thus keeping the\ncontinuity of the reasoning process.\nTo this end, in this paper, we propose ChatCoT,\na tool-augmented chain-of-thought reasoning strat-\negy for chat-based LLMs. As the major merit, Chat-\nCoT can perform the CoT reasoning across multi-\nturn conversation, and freely interact with tools at\nimmediate steps. Concretely, we first store the use-\nful knowledge at early turns of the conversation,\nincluding tools, tasks, and multi-turn reasoning for-\nmat, to help LLMs utilize task-specific knowledge\nto perform reasoning or manipulate tools. Then, we\niterate a specially designed tool-augmented reason-\ning step in which LLMs interact with tools, to per-\nform step-by-step tool-augmented reasoning, until\nobtaining the final answer.\nTo evaluate the effectiveness, we implement\nChatCoT on ChatGPT, and conduct experi-\nments on two complex reasoning benchmarks,\ni.e., MATH (Hendrycks et al., 2021) and Hot-\npotQA (Yang et al., 2018). Experimental results\nshow that ChatCoT achieves very promising per-\nformance on MATH with 7.9% relative improve-\nment in average over the SOTA baselines ( i.e.,\nPHP (Zheng et al., 2023)). Besides, our approach\ncan also be integrated with other strategies, e.g.,\nself-consistency, and ChatCoT can achieve better\nperformance by incorporating these strategies.\n2 Related Work\nTool-Augmented Large Language Models. With\nthe large-scale parameters and pre-training cor-\npus, large language models (LLMs) ( e.g., Flan\nT5 (Chung et al., 2022), ChatGPT (OpenAI, 2022)\nand LLaMA (Touvron et al., 2023)) have demon-\nstrated strong zero-shot and few-shot ability in\nNLP tasks (e.g., language generation, reasoning).\nHowever, LLMs have still struggled with com-\nplex reasoning tasks requiring task-specific knowl-\nedge and multi-step reasoning ( e.g., mathemati-\ncal problem solving). Previous work (Zhao et al.,\n2022, 2023b; Luo et al., 2023) has constructed task-\nspecific corpus and utilized continue pre-training\nand instruction tuning to inject relative knowledge\ninto LLMs and enhance the complex reasoning abil-\nity of LLMs. In order to further reduce the mistakes\nmade by LLMs, existing methods have explored to\naugment LLMs with external tools. They can be\nroughly categorized into the following two types.\nThe first type of methods (Gao et al., 2023; Parisi\net al., 2022; Qiao et al., 2023) train the model pa-\nrameters to support the utilization of the external\ntools, where they collect or synthesize the tool-\naugmented examples to tune the model parame-\nters (Schick et al., 2023; Patil et al., 2023; Hao\net al., 2023). Another type of methods (Gao et al.,\n2022; Yao et al., 2022; Zhang et al., 2023) uti-\nlize carefully designed prompts to guide LLMs to\nuse external tools. They focus on devising proper\nprompts or tools manipulation ways to select and\nuse tools when necessary (Liang et al., 2023; Shen\net al., 2023; Yao et al., 2022). In this work, we\nfollow the second type of methods and propose a\ntool-augmented chain-of-thought reasoning strat-\negy that can better solve complex reasoning tasks.\nChain-of-Thought Reasoning. To further enhance\nthe reasoning capacity of LLMs, Chain-of-Thought\n(CoT) prompting strategy (Wei et al., 2022; Kojima\net al., 2022) has been proposed to guide LLMs to\ngenerate intermediate reasoning steps which can\nboost the performance of LLMs. Through spe-\ncial instructions (e.g., ‚ÄúLet us think step by step‚Äù)\nand in-context exemplars with detailed intermedi-\nate reasoning steps, LLMs can perform step-by-\nstep reasoning to reach the final answer. Based\non CoT, recent work has also proposed several\nmethods to further improve the performance, in-\ncluding problem decomposition (Zhou et al., 2022;\nDua et al., 2022), appropriate exemplars selec-\ntion (Ye et al., 2022; Shi et al., 2023), results post-\nprocessing (Wang et al., 2022; Madaan et al., 2023;\nZheng et al., 2023), and changing the reasoning for-\nmat (Yao et al., 2023; Wu et al., 2023). However, as\nthe generation process of CoT is one-pass, the uti-\nlization of tools in intermediate steps would have to\ninterpret it, hurting the continuity of the generation\nprocess. In this work, we propose a unified way\nto integrate CoT reasoning and tool manipulation,\nwhich utilizes the excellent multi-turn chatting ca-\npacity of LLMs to perform CoT reasoning across\nmulti-turn conversations.\n14778\n3 Preliminary\nIn this section, we present the task setting, then\nintroduce the Chain-of-Though prompting strategy\nand the tool manipulation in reasoning tasks.\nTask Setting. In this work, we focus on improv-\ning the reasoning ability of large language mod-\nels (LLMs) on complex tasks, e.g., solving math-\nematics competition problems. Unlike tasks that\ncan be solved by humans via straightforward skills\nor tools, complex tasks require advanced knowl-\nedge (e.g., mathematical theorem) and multi-step\nreasoning to reach the answer. Typically, a com-\nplex problem includes three types of texts, namely\nproblem statement, solution text, and answer key,\ndenoted as ùëÑ, ùëÜ and ùê¥, respectively. The prob-\nlem statement ùëÑ introduces the background and\ndescription of a complex problem, and the solution\ntext illustrates the detailed solving process to ob-\ntain the answer key. All of them are composed of\na sequence of tokens, where each token is either\na text word or a mathematical symbol. Formally,\ngiven the problem statement ùëÑ, we aim to utilize\nLLMs to perform multi-step reasoning, to finally\ngenerate its accurate answer ùê¥.\nChain-of-Thought Prompting. To elicit the pow-\nerful reasoning ability of LLMs for complex tasks,\nChain-of-Thought (CoT) prompt strategy (Wei\net al., 2022) has been widely used to guide LLMs\nfor performing step-by-step reasoning. Generally,\nthe CoT prompt consists of few exemplars whose a\nseries of intermediate reasoning steps {ùêº1,¬∑¬∑¬∑ ,ùêºùëõ}\nare also involved. Each exemplar can be denoted\nas ùê∏ = ‚ü®ùëÑ,{ùêº1,¬∑¬∑¬∑ ,ùêºùëõ},ùê¥‚ü©. Formally, given the\nquestion and few exemplars, a CoT prompt is com-\nposed by integrating them as a long input of the\nLLM, which can prompt the LLM to generate a\nsimilar thought chain that leads to the final answer.\nTool Manipulation. Previous work has revealed\nthat LLMs are struggling with basic functionality\n(e.g., arithmetical calculation (Schick et al., 2023)),\nwhich can be solved by using specific external tools\n(e.g., calculator), denoted as {ùëá1,...,ùëá ùëõ}. To ma-\nnipulate tools, existing work mostly relies on writ-\ning a detailed prompt for describing how to use\navailable tools for the LLM, then incorporates it to\nguide the selection of useful tools and generate the\ntool arguments, and finally calls the tool API to ob-\ntain the result. Following this way, in this work, we\nfocus on three useful tools that have been widely\nused by humans to solve complex problems:\n‚Ä¢Calculator: Given a mathematical expression,\nthe calculator can compute the value of it or sim-\nplify it according to arithmetic rules (e.g., combin-\ning like terms and reduction of fractions).\n‚Ä¢Equation Solver: Given the equations system\nand unknown variables, the equation solver can\nautomatically calculate the value of the contained\nunknown variables through relative algorithms.\n‚Ä¢Retriever: Given a query, the retriever aims to\nextract the most relevant information (e.g., docu-\nments) from a number of candidates. According to\nthe types of the retrieved corpus, it can be imple-\nmented by specialized models, e.g., dense retrieval\nmodel.\nWe implement the first two tools by using dif-\nferent functions of SymPy (Meurer et al., 2017), a\nPython library for mathematical symbolic calcu-\nlation. For the retriever, we adopt SimCSE (Gao\net al., 2021), a sentence embedding model to mea-\nsure the text semantic similarity. Note that when\nthe input expression or equation is ill-formed or\nunsolved, the above tools would return an error.\n4 Approach\nIn this section, we present our proposed ChatCoT, a\nnew chain-of-thought (CoT) prompting framework\nbased on multi-turn conversations, for improving\nchat-based LLMs on complex reasoning tasks with\ntools. The overall illustration of our proposed Chat-\nCoT is shown in Figure 1.\n4.1 Overview\nFor complex tasks ( e.g., advanced mathematical\nproblems), LLMs require to frequently manipulate\nthe tools when in need, to fulfill the intractable\nintermediate issues. However, as tools are not in-\ntrinsically integrated with LLMs, previous work\nmostly relies on the LLM to generate the plan of\nmanipulating tools and then execution (Gao et al.,\n2022; Lu et al., 2023), or immediately call tools by\nstopping the continuous generation of LLMs (Yao\net al., 2022). The above both ways are not suitable\nfor the frequent interactions between LLMs and\ntools, due to the error accumulation in planning\nand frequent interruptions in LLM generation.\nIn our approach, we decompose the chain-of-\nthought reasoning process of LLMs into a multi-\nround conversation. In each turn, LLMs just require\nto concentrate on manipulating tools or accomplish-\ning reasoning in the current step, and the whole\n14779\n[Reasoning] Iterative Tool-augmented Reasoning\nProblem: A function ùëì has the property that ùëì(3ùë•‚àí1) = ùë•^2+ùë•+1for all real numbers ùë•. What is ùëì(5)?\nSolution: Let‚Äôs think step by step. Let ùë¢ = 3ùë• ‚àí1. Then ùë• =\nùë¢+1\n3 , and ùëì ùë¢ =\nùë¢+1\n3\n2\n+\nùë¢+1\n3 +1 =\nùë¢2+2ùë¢+1\n9 +\nùë¢+1\n3 +1 =\nùë¢2+5ùë¢+13\n9 . In particular, ùëì 5 =\n52+5√ó5+13\n9 = 7.\nProblem: Solve for ùëí, given that 2ùëë is 8less than 17ùëí, and 2ùëí is 9less than ùëë.\nSolution: Let‚Äôs think step by step. We begin with a system of two equations 2d = 17e‚àí8, 2e = d‚àí9.       Since the second equation can also be rewritten as d = 2e+9, \nwe can plug this expression for d back into the first equation and solve for e.      2d = 17e‚àí8 ‚Üí 2 2e+9 = 17e‚àí8 ‚Üí 4e+18 = 17e‚àí8 ‚Üí ‚àí13e = ‚àí26 ‚Üí e = 2.\nTool Knowledge\nReasoning Format\nProblem: A function ùëì ‚Ä¶‚Ä¶\nFirst, we need to find out ‚Ä¶‚Ä¶\n‚Ä¶‚Ä¶\nTask Knowledge\nProblem: ‚Ä¶‚Ä¶\nSolution:  ‚Ä¶‚Ä¶\nTest Problem: Solve \nSolve for ùëí ‚Ä¶‚Ä¶\nRetrieval \nSet\nCalculator can help you ‚Ä¶‚Ä¶\nEquation Solver can help ‚Ä¶‚Ä¶\nDo you understand?\nYes, I understand ‚Ä¶‚Ä¶\nI give you some example.\nProblem: The product ‚Ä¶‚Ä¶\nYou can use ‚Ä¶‚Ä¶\nYes, I understand. I will solve the \nproblem step by step and use tool to \nhelp me.\nProblem: A function ùëì has the property \nthat ùëì(3ùë•‚àí1) = ùë•^2+ùë• +1 ‚Ä¶‚Ä¶\nFirst, we need to find out ‚Ä¶‚Ä¶\n‚Ä¶ ‚Ä¶\nProblem: Solve for ùëí, given that 2ùëë is 8less than \n17ùëí, and 2ùëí is 9less than ùëë.\nFirst, we need to express ùëë in terms of ùëí ‚Ä¶‚Ä¶\nTo solve this sub-problem, which tool can we use? \nYou can choose calculator, equation solver or ‚Ä¶‚Ä¶\nEquation Solver\nLLM for Reasoning\nLLM for Tools Selection\nWhat is the argument ‚Ä¶‚Ä¶\n2ùëë = 17ùëí‚àí8, 2ùëí = ùëë‚àí9‚Ä¶‚Ä¶\nResults: ùëë = 13,ùëí = 2‚Ä¶‚Ä¶\nTools Execution\nCalculator Retriever\n‚Ä¶ ‚Ä¶\n[Prompting] Conversational Memory\nNext Iteration Step\nChatCoT\nùë∏:\nùë∞ùüè:\nùë∞ùüê:\nùë∞ùüë:\nVanilla CoT\n[Prompting] Context\nùë∏:\nùë∞ùüè: ùë∞ùüê:\nùë∞ùüë: ùë®:\n[Reasoning] One-time Inference\n‚Ä¶ ‚Ä¶\nFigure 1: The comparison of vanilla CoT and ChatCoT, illustrated for a mathematical problem. For vanilla CoT, the\ncontent underlined are generated by LLMs. For ChatCoT, the conversational knowledge memory is initialized to\nprovide tools, task and reasoning format knowledge. Then, the tool-augmented reasoning step is iterated multiple\ntimes to perform step-by-step reasoning, until obtaining the answer.\nreasoning process would keep on pushing without\npremature planning and sudden interruption. In\nthis way, the whole reasoning process would be\nconverted to a conversation between LLMs and an\nagent, which follows pre-defined rules to guide\nLLMs and manipulate the tool. By designing\nproper chatting strategies, the agent would auto-\nmatically elicit LLMs to perform reasoning and\nselect a tool, or invoke the tool for execution.\nIn our approach, we first initialize the multi-turn\nconversation by feeding chat-based LLMs with\nthe background knowledge, i.e., the description\nof tools, relevant task exemplars, and the demon-\nstration of decomposed chain-of-thought in chat,\nwhich are the conversational knowledge memory\nfor supporting the following reasoning. Then, we\npropose the tool-augmented reasoning procedure\nthat leverages LLMs to perform reasoning with\ntools in the current step and iterate it to fulfill\nall sub-tasks in the whole reasoning process, until\nreaching the answer. We introduce the details of\nthe two components in the following.\n4.2 Initializing Conversational Knowledge\nMemory\nTo guide chat-based LLMs to follow our proposed\nChatCoT using external tools, it is essential to de-\nsign proper prompts in context. In our approach,\nas we reformulate the chain-of-thought reasoning\ninto a decomposed multi-turn conversation, we can\nalso feed the essential prompts into LLMs at early\nturns as the context, to initialize the conversation\nbackground knowledge. It can be seen as the in-\ncontext knowledge memory in the format of dia-\nlogue that stores useful knowledge for helping chat-\nbased LLMs manipulate tools or perform reasoning.\nHere, we consider three types of knowledge about\ntools, task, and multi-turn reasoning format, respec-\ntively. The details of prompts are in Appendix A.\nTools Knowledge. As LLMs have never seen tools\nduring pre-training, for each tool in Section 3, we\nhand-craft its description in the following pattern:\n‚Äú[ùëá] can help you [ùëå]‚Äù, where [ùëá] is the tool name\nand [ùëå] shows its detailed functionality. Then, we\nmerge all the descriptions and design the input\n14780\nprompt to tell LLMs about the knowledge of all\ntools. We also hand-craft the expected response\nof the LLM. It will be also fed into the LLM, to\nindicate the LLM that it has accepted our prompt\nand should follow it.\nRetrieval-Augmented Task Knowledge. Since\nLLMs can learn the task knowledge from in-\ncontext exemplars, we leverage a retriever to se-\nlect the most relevant instance from the training\ndataset, to provide more useful knowledge for the\ngiven question. Concretely, we train SimCSE (Gao\net al., 2021), a sentence embedding method that\ncan measure the semantic similarity of texts, via\nthe unsupervised training strategy on the training\nset. Then, we leverage it to retrieve the top-ùëòmost\nsemantically similar exemplars, and concatenate\ntheir problem statement ùëÑand solution ùëÜto com-\npose the input prompt. Similarly, we also feed it\nwith our expected response into the LLM.\nMulti-turn Reasoning Format. To elicit LLMs\nfollowing multi-turn reasoning format, we man-\nually annotate the whole multi-round dialogue\nùêº1,¬∑¬∑¬∑ ,ùêºùëõ of randomly sampled five questions\nfrom the training set, to create the exemplars. Then,\nwe feed the dialogues of all the exemplars into the\nchat-based LLM round by round, as the context to\nguide LLMs to follow it for performing reasoning.\nSummary. The above three types of multi-turn\nutterances are pre-defined with corresponding con-\ntents and formats, which compose the conversa-\ntional knowledge memory of our approach. It\nwould be leveraged to initialize the conversational\ncontext, and support the following step-by-step rea-\nsoning for answering the question.\n4.3 Iterative Tool-augmented Reasoning\nBased on the above conversational knowledge\nmemory, we iterate the tool-augmented reasoning\nstep to perform step-by-step tool-augmented rea-\nsoning, until finally obtain the answer.\n4.3.1 Tool-augmented Reasoning Step\nThe tool-augmented reasoning step can be iterated\nin multiple times. In each iteration, based on the\ncurrent results, we first leverage LLMs to perform\nreasoning, then select the proper tool by LLMs,\nand finally execute the selected tool to obtain the\nintermediate result in the current step.\nLLM for Reasoning. Guided by the exemplars in\nthe conversation history, LLMs are able to decom-\npose the whole reasoning process into multi-turn\nchat. Specially, LLMs would be elicited by the\ncontextual exemplars to directly perform reason-\ning in natural language based on the current result,\nwithout specialized prompts or instructions. Conse-\nquently, LLMs can rely on the retrieval-augmented\ntask knowledge in context, to generate the natu-\nral language solution till the point that needs the\nfunctionality of tools.\nLLM for Tools Selection. After reasoning, we\nutilize the LLM to select a useful tool ( e.g., cal-\nculator), which will be employed to provide the\nrequired functionality for the LLM. Here, the input\nprompt of the LLM is ‚ÄúTo solve this sub-problem,\nwhich tool can we use?‚Äù After feeding it into the\nLLM, if the LLM requires to utilize tools, it will\nselect a suitable one, and then we further ask the\nLLM to formulate the input arguments of the tool,\ne.g., mathematical expression. Otherwise, it will\nanswer ‚ÄúDo not use tool‚Äù, and the LLM will con-\ntinue to perform reasoning.\nTools Execution. Given the selected tool and for-\nmulated arguments by LLMs, we can execute the\ntool with the arguments to obtain the result in the\ncurrent iteration. Here, we also consider that the re-\nsults from the tool may be not satisfied by the LLM,\ne.g., irrelevant retrieved documents. In this case,\nwe can also add several feedback rounds where the\nLLM judges if the result is useful or expected, and\nthen reuse the tool to acquire a new result.\n4.3.2 Iteration for Step-by-Step Reasoning\nWe iterate the above step based on the in-context\nconversation knowledge memory, to perform step-\nby-step reasoning on the given question ùëÑ. We\nstart the whole iteration process using the following\nprompt: ‚ÄúYou should solve the problem step by step\nand you should follow the react in the history [ùëÑ]‚Äù.\nThen, after reaching the answer key, the iteration\nprocess will be stopped by LLMs. In practice, we\nfind that chat-based LLMs are prone to continue\nchatting although the answer key has appeared in\nthe reasoning process. Thus, we set the maximum\nchat turns, and devise the following prompt to force\nLLMs to stop reasoning and conclude the answer:\n‚ÄúBase on the context, what is the answer?‚Äù.\nAs our proposed approach only decomposes the\none-pass chain-of-thought reasoning into multi-\nturn chat and adds the utilization of tools, it is\nagnostic to the task types and tools implementa-\ntion. Therefore, it is a general framework that can\n14781\nDataset Category Train Dev /Test\nMATH\nAlgebra 1744 1187\nCP 771 474\nPrecalculus 746 546\nPrealgebra 1205 871\nGeometry 870 479\nIA 1295 903\nNT 869 540\nHotpotQA Distractor 90477 7405\nTable 1: Statistics of the two complex reasoning datasets.\nCP, IA, and NT denote Counting and Probability, Inter-\nmediate Algebra, and Number Theory, respectively.\nbe applied to a variety of complex reasoning tasks\nthat require suitable tools. Besides, our approach\nalso supports the recently proposed improvement\nstrategies based on the chain-of-thought method,\ne.g., self-consistency (Wang et al., 2022). We con-\nduct corresponding experiments in Section 5.3 to\nvalidate it.\n5 Experiment\nIn this section, we conduct experiments to evaluate\nthe effectiveness of ChatCoT. The implementation\ndetails can be found in Appendix B.\n5.1 Experimental settings\nDatasets. We consider two complex reasoning\ndatasets for evaluation, i.e., MATH (Hendrycks\net al., 2021) and HotpotQA (Yang et al., 2018). The\ndetails of these two datasets are shown in Table 1.\nWe adopt accuracy as the evaluation metric.\n‚Ä¢MATH is composed of challenging compe-\ntition mathematical problems which require ad-\nvanced mathematical knowledge. It is divided into\nseven categories, i.e., Algebra, Counting and Prob-\nability, Precalculus, Prealgebra, Geometry, Inter-\nmediate Algebra, and Number Theory. We adopt\nthe calculator and an equation solver as external\ntools to help LLMs.\n‚Ä¢HotpotQA is a multi-hop question answering\ndataset, where each question is associated with a\ncollection of paragraph candidates containing sev-\neral golden contents which are useful for reasoning.\nWe use the development set under the distractor set-\nting of HotpotQA for evaluation, where the anno-\ntation of golden paragraphs is not aware to LLMs.\nWe employ the retriever as the external tool.\nBaselines. We mainly compare our approach with\nthe following prompting strategies based on Chat-\nGPT (OpenAI, 2022):\n‚Ä¢Chain-of-Thought (CoT) (Wei et al., 2022)\nis a prominent method to boost the performance\nof LLMs in reasoning tasks. In CoT, LLMs are\nprompted to generate the intermediate reasoning\npath and reasoning step by step to reach the final an-\nswer. Previous work has shown that the utilization\nof external tools and similar exemplars improves\nthe performance of CoT. Therefore, we implement\nexternal tools to help LLMs reason and retrieve to\nhelp LLMs select exemplars, which are namedCoT\nw/ Tool, and CoT w/ Retri, respectively.\n‚Ä¢Learning-to-Program (LP) (Guo et al., 2023)\nguides LLMs to program in natural language by\nlearning solutions in the training set, and elicits\nLLMs to solve tasks following the program.\n‚Ä¢Progressive-Hint Prompting (PHP) (Zheng\net al., 2023) proposes to iteratively refine the solu-\ntion based on the answer hints from previous trials.\nThe iterative method achieves SOTA on MATH.\nTo provide a more complete evaluation, we\nalso report the performance of various LLM back-\nbones with the vanilla CoT prompting, including\nPaLM (Chowdhery et al., 2022), PaLM 2 (Google,\n2023), Minerva (Lewkowycz et al., 2022), Galac-\ntica (Taylor et al., 2022), LLaMA (Touvron et al.,\n2023) and GPT-3 (Brown et al., 2020).\n5.2 Main Results\nWe present the evaluation results of our approach\non MATH and HotpotQA datasets in Table 2 and\nTable 3 respectively.\nFirst, for the comparison of backbones for CoT\nprompting, ChatGPT achieves the best perfor-\nmance, demonstrating its outstanding mathematical\nreasoning ability. Our method elicits the reasoning\nprocess by leveraging the strong multi-turn dia-\nlogue ability of ChatGPT, thus leading to a better\nrelease of the reasoning ability from ChatGPT.\nSecond, retrieval-augmented methods ( e.g.,\nChatCoT, CoT w/ Retri) outperform other baselines.\nThe reason is that retrieved exemplars may contain\nmore relevant knowledge and reasoning steps that\nare beneficial to solve the given problem. On Ge-\nometry tasks of MATH, CoT w/ Retri achieves the\nlargest improvement over vanilla CoT than other\nsub-tasks. Another possible reason is that ChatGPT\nis more unfamiliar to the knowledge and symbol of\ngeometry than others. Without similar exemplars,\nit is difficult for LLMs to well understand them.\n14782\nModels Prompt\nStrategy\nMATH\nAlgebra CP PC PA Geometry IA NT Avg.\nGPT-3 CoT 6.0 4.7 4.0 7.7 3.1 4.4 4.4 5.2\nPaLM CoT 9.7 8.4 4.4 19.2 7.3 3.5 6.0 8.8\nLLaMA CoT - - - - - - - 10.6\nGalactica CoT 29.0 13.9 12.8 27.2 12.3 9.6 11.7 20.4\nMinerva CoT 51.3 28.0 18.0 55.0 26.8 13.7 21.2 33.6\nPaLM 2 CoT - - - - - - - 34.3\nChatGPT\nCoT 48.1 31.4 21.1 56.6 22.3 18.3 29.1 35.1\nCoT w/ Tool 35.9 22.6 9.3 40.5 13.6 9.4 19.4 23.8\nCoT w/ Retri 52.7 32.7 18.9 58.4 29.2 19.9 31.7 37.7\nLP 49.6 30.2 16.3 52.3 22.5 16.9 29.8 34.0\nPHP 51.1 33.7 16.1 57.7 25.4 17.1 35.1 36.5\nChatCoT 56.1 34.2 23.8 59.2 29.9 19.5 32.6 39.4\nTable 2: Experimental results on MATH dataset. PC and PA denotePrecalculus and Prealgebra, respectively. Avg.\nis the average value of all categories. The best are denoted in bold and the second-best are underlined.\nMethods HotpotQA\nCoT 38.0\nCoT w/ Tool 31.4\nChatCoT w/o Feedback 53.8\nChatCoT 59.2\nTable 3: The results on HotpotQA. We report the results\nof the development set under the distractor setting.\nThird, given the results of CoT and CoT w/ Tool\non MATH and HotpotQA, we can find that directly\nutilizing external tools during reasoning is not a\nsuitable way, which may hurt the performance of\nLLMs. The reason may be that injecting tool us-\nage into the CoT reasoning process will hurt the\ncontinuity of reasoning.\nFinally, ChatCoT achieves state-of-the-art per-\nformance on MATH dataset based on ChatGPT and\noutperforms other baselines on HotpotQA. Com-\npared with the previous SOTA method PHP, Chat-\nCoT outperforms six of seven sub-tasks on MATH\ndataset and achieves 7.9% relative improvement on\naverage accuracy over the PHP method. The ex-\nperiment results have verified the effectiveness of\nChatCoT on complex reasoning tasks. By leverag-\ning conversational knowledge memory and multi-\nround dialogue to reasoning, ChatCoT has the ad-\nvantage to utilize plug-and-play tools. Moreover,\non the Number Theory tasks of MATH, we can find\nthat PHP achieves the best performance. The rea-\nson may be that there are fewer equations that need\nto be computed or simplified. Thus, the advantage\nMethods MATH\nTK RATK MRF PC Geo NT\n‚úî ‚úî ‚úî 23.8 29.9 32.6\n‚úó ‚úî ‚úî 23.3 29.2 30.6\n‚úî ‚úó ‚úî 20.0 27.4 31.0\n‚úî ‚úî ‚úó 21.6 24.2 32.2\n‚úó ‚úó ‚úî 16.7 21.1 29.3\nTable 4: The results of ablation study. TK, RATK,\nand MRF denote if using tool knowledge, retrieval-\naugmented task knowledge, and multi-turn reasoning\nformat at early turns of the conversation, respectively.\nGeo is the abbreviation of Geometry.\nof the utilization of tools becomes less obvious.\n5.3 Detailed Analysis\nIn order to further verify the effectiveness of each\ncomponent in ChatCoT, we conduct experiments\nabout ablation, adaption, tools utilization and ex-\npense. We present the case study in Appendix C.1.\nAblation Study. In the ablation study, we evaluate\nthe effectiveness of conversational memory, includ-\ning tool knowledge memory, retrieval-augmented\nknowledge memory, and multi-turn reasoning for-\nmat memory. As shown in Table 4, removing\nany type of conversational memory will reduce\nthe performance of ChatCoT, which indicates the\neffectiveness of these memories in complex reason-\ning. In particular, removing retrieval-augmented\nknowledge memory or multi-turn reasoning format\nmemory will lead to a large drop, which shows\n14783\nMethods CP NT\nCoT + SC 35.2+3.8% 34.4+5.3%\nChatCoT + SC 40.1+5.9% 38.3+5.7%\nTable 5: The evaluated accuracy of combining our\napproach with self-consistency. SC denotes self-\nconsistency. We also report the absolute improvement\ncompared with vanilla methods on subscripts.\nMethods Frequency Success\nCoT w/ Tool 3.0% 85.7%\nChatCoT w/o TK 56.0% 93.0%\nChatCoT w/o MRF 10.0% 64.2%\nChatCoT 70.0% 92.0%\nTable 6: Frequency and success rate of tool manipu-\nlation on Number Theory task of MATH. TK, MRF\ndenote tool knowledge, multi-turn reasoning format at\nearly turns of the conversation respectively.\nthat mathematical knowledge and reasoning for-\nmat knowledge is important for LLMs in reasoning\ntasks, while LLMs can learn the usage of external\ntools from exemplars without descriptions.\nCombination with Improvement Strategies.\nChatCoT is a general method to enhance the abil-\nity of tool manipulation of LLMs. It can be\nintegrated with improvement strategies and fur-\nther boost the performance of LLMs on reasoning\ntasks. To evaluate the applicability of ChatCoT\nto improvement strategies designed for CoT, we\ncompare ChatCoT with CoT on two subtasks of\nMATH, where both methods are augmented with\nself-consistency (Wang et al., 2022), a represen-\ntative improvement strategy for CoT prompting.\nConcretely, we sample 5 outputs for majority vot-\ning in self-consistency. As shown in Table 5, self-\nconsistency brings improvement in both CoT and\nChatCoT. In particular, the absolute improvement\nof ChatCoT is slightly higher than CoT, showing\nthat ChatCoT can adapt to self-consistency well.\nThe reason is that, with the decomposing of reason-\ning procedures, the intermediate steps of ChatCoT\nare more confident, and small mistakes will be\ncorrected easily. Moreover, we construct the case\nstudy about the combination with ChatCoT and\nSelf-Refine (Madaan et al., 2023) in Appendix C.2.\nTools Utilization Analysis. As we mentioned\nabove, in complex reasoning tasks, infrequently\nor incorrectly utilizing external tools might lead to\nwrong answers. Thus, we conduct the experiment\nMethods Generated Tokens\nCoT 224.6\nCoT w/ Tool 296.2\nSelf-Consistency 1017.4\nChatCoT 355.2\nTable 7: The comparison of the number of generated\ntokens from LLMs among different prompt strategies.\nabout whether LLMs can frequently or correctly\nleverage based on different methods. Table 6 ex-\npresses the performance of tools utilization in the\nNumber Theory task of MATH of baseline and our\napproach. ‚ÄúFrequency‚Äù denotes the ratio of prob-\nlems where LLMs correctly leverage tools. ‚ÄúSuc-\ncess‚Äù denotes the rate of LLMs utilizing tools suc-\ncessfully among all the times of invoking tools. We\ncan observe that ChatCoT achieves a balance of fre-\nquency and ratio of success. Tool knowledge pro-\nvides the function of tools for LLMs and improves\nthe frequency that LLMs utilize the tools. LLMs\ncan learn how to leverage external tools through\nthe multi-turn reasoning format and boost the ratio\nof successful utilization of tools. Without any of\nthem, the frequency and ratio of success will drop\nwhich might not be conducive to reasoning.\nNumber of Generated Tokens Analysis. Despite\nguiding LLMs to reason through multi-turn dia-\nlogue, the computation expense of ChatCoT is not\nsignificantly larger than the CoT method. In Ta-\nble 7, we present the average number of generated\ntokens from LLMs in several methods on MATH\ndataset, which reflects the computation expense to a\ncertain degree. We can observe that ChatCoT is on\nthe same order of magnitude with other baselines\n(e.g., CoT and CoT w/ Tool). Therefore, ChatCoT\ndoes not bring significant expenses compared to\nexisting prompting methods.\n6 Conclusion\nIn this paper, we have proposed ChatCoT, a new\nframework to manipulate the tools for the CoT rea-\nsoning. It naturally integrates the reasoning process\nand manipulating tools through a form of multi-turn\nconversations. At each turn, LLMs can either in-\nteract with tools or perform the reasoning by itself.\nOur approach can effectively leverage the multi-\nturn conversation ability of chat-based LLMs. Ex-\nperimental results on two complex reasoning tasks\nincluding MATH and HotpotQA have verified the\n14784\neffectiveness of ChatCoT.\nCurrently, our experiments are mainly conducted\non mathematical reasoning tasks, and we will test\nthe effectiveness of the proposed approach to more\ntypes of reasoning tasks. Besides, we will also\nconsider extending the number of available tools\nfor solving different tasks.\nLimitations\nIn this section, we discuss the limitations of our\nwork. First, we do not utilize GPT-4 in our experi-\nment or evaluate the performance of GPT-4 in the\nChatCoT framework. That is because our appli-\ncation for GPT-4 has not been accepted. Second,\nChatCoT is designed for chat-based LLMs and it\nis hardly compatible with other LLMs. However,\nmost LLMs support multi-turn conversation cur-\nrently and they perform well on reasoning tasks.\nBesides, although LLMs have achieved strong abil-\nity in reasoning tasks, the requirement of compu-\ntation expense and GPU resource is higher than\nother pre-trained language models which have mil-\nlions of parameters. The utilization of LLMs will\nproduce more carbon dioxide and pollute the envi-\nronment.\nAcknowledgement\nThis work was partially supported by National Nat-\nural Science Foundation of China under Grant No.\n62222215, Beijing Natural Science Foundation un-\nder Grant No. L233008 and 4222027, and Beijing\nOutstanding Young Scientist Program under Grant\nNo. BJJWZYJH012019100020098. And this work\nis also partially supported by the Outstanding Inno-\nvative Talents Cultivation Funded Programs 2021\nof Renmin University of China. Xin Zhao is the\ncorresponding author.\nReferences\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways. CoRR, abs/2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Y . Zhao,\nYanping Huang, Andrew M. Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V . Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\nCoRR, abs/2210.11416.\nDheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt\nGardner. 2022. Successive prompting for decompos-\ning complex questions. In Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2022, Abu Dhabi, United\nArab Emirates, December 7-11, 2022, pages 1251‚Äì\n1265.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2022. PAL: program-aided language\nmodels. CoRR, abs/2211.10435.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2023. Pal: Program-aided language\nmodels.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimcse: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 7-11 November, 2021 , pages 6894‚Äì\n6910. Association for Computational Linguistics.\n14785\nGoogle. 2023. Palm 2 technical report. Google.\nYiduo Guo, Yaobo Liang, Chenfei Wu, Wenshan\nWu, Dongyan Zhao, and Nan Duan. 2023. Learn-\ning to program with natural language. CoRR,\nabs/2304.10464.\nShibo Hao, Tianyang Liu, Zhen Wang, and Zhiting\nHu. 2023. Toolkengpt: Augmenting frozen lan-\nguage models with massive tools via tool embeddings.\nCoRR, abs/2305.11554.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and Ja-\ncob Steinhardt. 2021. Measuring mathematical prob-\nlem solving with the MATH dataset. In Proceedings\nof the Neural Information Processing Systems Track\non Datasets and Benchmarks 1, NeurIPS Datasets\nand Benchmarks 2021, December 2021, virtual.\nJinhao Jiang, Kun Zhou, Zican Dong, Keming Ye,\nWayne Xin Zhao, and Ji-Rong Wen. 2023a. Struct-\ngpt: A general framework for large language model\nto reason over structured data. arXiv preprint\narXiv:2305.09645.\nXue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang,\nand Ge Li. 2023b. Self-planning code generation\nwith large language model. CoRR, abs/2303.06689.\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li,\nDavid Hall, Percy Liang, Christopher Potts, and\nMatei Zaharia. 2022. Demonstrate-search-predict:\nComposing retrieval and language models for\nknowledge-intensive NLP. CoRR, abs/2212.14024.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In NeurIPS.\nAitor Lewkowycz, Anders Andreassen, David Dohan,\nEthan Dyer, Henryk Michalewski, Vinay V . Ra-\nmasesh, Ambrose Slone, Cem Anil, Imanol Schlag,\nTheo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur,\nGuy Gur-Ari, and Vedant Misra. 2022. Solving quan-\ntitative reasoning problems with language models. In\nNeurIPS.\nYaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu,\nYan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji,\nShaoguang Mao, Yun Wang, Linjun Shou, Ming\nGong, and Nan Duan. 2023. Taskmatrix.ai: Com-\npleting tasks by connecting foundation models with\nmillions of apis. CoRR, abs/2303.16434.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-\nWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jian-\nfeng Gao. 2023. Chameleon: Plug-and-play compo-\nsitional reasoning with large language models. arXiv\npreprint arXiv:2304.09842.\nPan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and\nKai-Wei Chang. 2022. A survey of deep learning for\nmathematical reasoning. CoRR, abs/2212.10535.\nHaipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-\nguang Lou, Chongyang Tao, Xiubo Geng, Qingwei\nLin, Shifeng Chen, and Dongmei Zhang. 2023. Wiz-\nardmath: Empowering mathematical reasoning for\nlarge language models via reinforced evol-instruct.\nCoRR, abs/2308.09583.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nSean Welleck, Bodhisattwa Prasad Majumder,\nShashank Gupta, Amir Yazdanbakhsh, and Peter\nClark. 2023. Self-refine: Iterative refinement with\nself-feedback. CoRR, abs/2303.17651.\nAaron Meurer, Christopher P. Smith, Mateusz Pa-\nprocki, Ondrej Cert√≠k, Sergey B. Kirpichev, Matthew\nRocklin, Amit Kumar, Sergiu Ivanov, Jason Keith\nMoore, Sartaj Singh, Thilina Rathnayake, Sean Vig,\nBrian E. Granger, Richard P. Muller, Francesco\nBonazzi, Harsh Gupta, Shivam Vats, Fredrik Johans-\nson, Fabian Pedregosa, Matthew J. Curry, Andy R.\nTerrel, Step√°n Roucka, Ashutosh Saboo, Isuru Fer-\nnando, Sumith Kulal, Robert Cimrman, and An-\nthony M. Scopatz. 2017. Sympy: symbolic com-\nputing in python. PeerJ Comput. Sci., 3:e103.\nOpenAI. 2022. Introducing chatgpt. OpenAI Blog.\nOpenAI. 2023. Gpt-4 technical report. OpenAI.\nBhargavi Paranjape, Scott Lundberg, Sameer Singh,\nHannaneh Hajishirzi, Luke Zettlemoyer, and\nMarco Tulio Ribeiro. 2023. Art: Automatic multi-\nstep reasoning and tool-use for large language mod-\nels. arXiv preprint arXiv:2303.09014.\nAaron Parisi, Yao Zhao, and Noah Fiedel. 2022.\nTALM: tool augmented language models. CoRR,\nabs/2205.12255.\nShishir G. Patil, Tianjun Zhang, Xin Wang, and\nJoseph E. Gonzalez. 2023. Gorilla: Large lan-\nguage model connected with massive apis. CoRR,\nabs/2305.15334.\nJing Qian, Hong Wang, Zekun Li, Shiyang Li, and\nXifeng Yan. 2022. Limitations of language mod-\nels in arithmetic and symbolic induction. CoRR,\nabs/2208.05051.\nShuofei Qiao, Honghao Gui, Huajun Chen, and Ningyu\nZhang. 2023. Making language models better\ntool learners with execution feedback. CoRR,\nabs/2305.13068.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess√¨, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\nCoRR, abs/2302.04761.\n14786\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhugging face.\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan\nScales, David Dohan, Ed H. Chi, Nathanael Sch√§rli,\nand Denny Zhou. 2023. Large language models can\nbe easily distracted by irrelevant context. CoRR,\nabs/2302.00093.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\nScialom, Anthony Hartshorn, Elvis Saravia, An-\ndrew Poulton, Viktor Kerkez, and Robert Stojnic.\n2022. Galactica: A large language model for science.\nCoRR, abs/2211.09085.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth√©e Lacroix,\nBaptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur√©lien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. CoRR.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V .\nLe, Ed H. Chi, and Denny Zhou. 2022. Self-\nconsistency improves chain of thought reasoning in\nlanguage models. CoRR, abs/2203.11171.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,\nand Denny Zhou. 2022. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nNeurIPS.\nYiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li,\nErkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng,\nQingyun Wu, and Chi Wang. 2023. An empirical\nstudy on challenging math problem solving with GPT-\n4. CoRR, abs/2306.01337.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning. In Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing,\nBrussels, Belgium, October 31 - November 4, 2018,\npages 2369‚Äì2380.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L. Griffiths, Yuan Cao, and Karthik\nNarasimhan. 2023. Tree of thoughts: Deliberate\nproblem solving with large language models. CoRR,\nabs/2305.10601.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. CoRR, abs/2210.03629.\nXi Ye, Srinivasan Iyer, Asli Celikyilmaz, Ves Stoy-\nanov, Greg Durrett, and Ramakanth Pasunuru. 2022.\nComplementary explanations for effective in-context\nlearning. CoRR, abs/2211.13892.\nBeichen Zhang, Kun Zhou, Xilin Wei, Wayne Xin Zhao,\nJing Sha, Shijin Wang, and Ji-Rong Wen. 2023. Eval-\nuating and improving tool-augmented computation-\nintensive math reasoning. CoRR, abs/2306.02408.\nWayne Xin Zhao, Kun Zhou, Zheng Gong, Beichen\nZhang, Yuanhang Zhou, Jing Sha, Zhigang Chen,\nShijin Wang, Cong Liu, and Ji-Rong Wen. 2022. Ji-\nuzhang: A chinese pre-trained language model for\nmathematical problem understanding. In KDD ‚Äô22:\nThe 28th ACM SIGKDD Conference on Knowledge\nDiscovery and Data Mining, Washington, DC, USA,\nAugust 14 - 18, 2022, pages 4571‚Äì4581.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Be-\nichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,\nChen Yang, Yushuo Chen, Zhipeng Chen, Jinhao\nJiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang\nLiu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.\n2023a. A survey of large language models. CoRR,\nabs/2303.18223.\nXin Zhao, Kun Zhou, Beichen Zhang, Zheng Gong,\nZhipeng Chen, Yuanhang Zhou, Ji-Rong Wen, Jing\nSha, Shijin Wang, Cong Liu, and Guoping Hu. 2023b.\nJiuzhang 2.0: A unified chinese pre-trained language\nmodel for multi-task mathematical problem solving.\nIn Proceedings of the 29th ACM SIGKDD Confer-\nence on Knowledge Discovery and Data Mining,\nKDD 2023, Long Beach, CA, USA, August 6-10,\n2023.\nChuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo\nLi, and Yu Li. 2023. Progressive-hint prompting\nimproves reasoning in large language models. CoRR,\nabs/2304.09797.\nDenny Zhou, Nathanael Sch√§rli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed H. Chi. 2022.\nLeast-to-most prompting enables complex reasoning\nin large language models. CoRR, abs/2205.10625.\n14787\nA Details of Conversation Memory\nIn this part, we present the details of the prompt in\nconversation Memory.\nTool Knowledge. The two turns of utterances are:\nUser: ‚ÄúYou can use tool to help you solve the\nproblem and I give you the instruction of tools\nusage. [ùëá1] can help you [ùëå1] ¬∑¬∑¬∑ Do you under-\nstand?‚Äù\nLLM: ‚ÄúYes, I understand. I will use tool to help\nme solve the problem.‚Äù.\nRetrieval-Augmented Task Knowledge.The two-\nturn utterances are:\nUser: ‚ÄúI give you some example. Problem: [ùëÑ1]\nSolution: [ùëÜ1] ¬∑¬∑¬∑ You can use the knowledge and\nthoery in these problem. Do you understand?‚Äù\nLLM: ‚ÄúYes, I understand. I will solve the prob-\nlem step by step and use tool to help me.‚Äù.\nMulti-turn Reasoning Format. The multi-turn\nutterances are based on the following pattern:\nUser: ‚ÄúProblem: [ùëÑ\n‚Ä≤\n1] Let‚Äôs think step by step\nand use knowledge in similar problem to solve this\nproblem.‚Äù\nLLM: ‚Äú[ùêº1]‚Äù.\n¬∑¬∑¬∑\nLLM: ‚Äú[ùêºùëõ]‚Äù.\nB Implementation Details\nDuring the evaluation, we utilize ChatGPT (gpt-\n3.5-turbo) (OpenAI, 2022) as our backbone model,\nand fine-tune RoBERTa (Liu et al., 2019) following\nSimCSE (Gao et al., 2021) on the training sets of\nMATH and HotpotQA separately as the retriever in\ncorresponding tasks.\nFor MATH, we leverage 5-shot setting. The ex-\nemplars of CoT and CoT w/ Tool are randomly\nsampled, while exemplars of CoT w/ Retri are re-\ntrieved top-5 similar problems by the retriever. For\nChatCoT, 2 retrieval exemplars and 3 annotated\nexemplars will be adopted. For HotpotQA, we\nleverage 4-shot setting which is similar to MATH,\ndue to the length limitation of input. For the CoT\nmethod, we retrieve the top-3 relevant paragraphs\nfrom the paragraph collection as evidence of the\ngiven question. In ChatCoT, as the retrieved para-\ngraphs might be not useful for LLMs, LLMs can\nsend feedback to the retriever to show other results\nat most 5 times.\nC Case Study\nC.1 Framework of ChatCoT\nIn order to better present the reasoning process of\nChatCoT, we conduct the case study of two prob-\nlems in MATH dataset, which is shown in Figure 2.\nThe beginning prompt contains knowledge of\ntools, tasks, and reasoning format. In the tool\nknowledge, we introduce the usage and function\nof external tools. For task knowledge, we retrieve\nsimilar problems and corresponding solutions from\nthe training set as retrieval exemplars through se-\nmantics similarity, which might contain relevant\nknowledge. The reasoning format is used to guide\nLLMs to reason through multi-round dialogue. The\nbeginning prompt and corresponding response are\npre-set to enhance the sensitivity of LLMs to in-\nstructions.\nInstructed by the beginning prompt, LLMs it-\nerate the tool-augmented reasoning step for step-\nby-step reasoning in ChatCoT. We illustrate the\nflow of ChatCoT with the example on the left in\nFigure 2. Given the math problem, LLMs first rea-\nson out that ùëë should be expressed in terms of ùëí.\nThen, LLMs communicate with the agent to se-\nlect the appropriate external tool Equation Solver\nand specify the corresponding arguments to invoke\nthe tool, including the unknown symbols and the\nsystem of equations. Based on the selection, The\nassistant executes the tool invocation and returns\nthe solution of the system of equations to LLMs.\nAfter execution, LLMs can perform the next rea-\nsoning step based on the given result. The iterative\ntool-augmented reasoning process can be iterated\nseveral times until LLMs find out the answer. In\nthis case, LLMs find that the original problem has\nbeen solved, thus obtaining the final answer and\nending the reasoning process.\nC.2 Combination with Self-Refine\nTo better show how LLMs correct mistakes through\nChatCoT, we present a case study of the combina-\ntion of ChatCoT and Self-Refine in the following:\nUser: ‚Äú Problem: What is the remainder when\n1313 +5 is divided by 6?\nLet‚Äôs think step by step‚Äù\nLLM: First, we need to find out the value of\n1313 +5 using the properties of exponents.\n......\nLLM: ‚ÄúSecond, we need to find out the remain-\nder when 302875106592258 is divided by 6.‚Äù\n14788\nUser: ‚ÄúTo solve this sub-problem, which tool\ncan we use?‚Äù\nLLM: ‚ÄúCalculator‚Äù\nUser: ‚ÄúGive me the equation to calculate‚Äù\nLLM: ‚Äú302875106592258 mod 6‚Äù\nUser: ‚Äú Results: 302875106592258 mod 6 =\n1.81725063955355ùëí+15 ‚àóùëöùëúùëë.\nContinue reasoning‚Äù\nLLM: ‚Äú Finally, we know that the remainder\nwhen 1313 +5 is divided by 6 is the same as the\nremainder when 302875106592258 is divided by\n6, which is 2 ‚Äù\nUser: ‚ÄúThe solution above might some mistake,\nyou should check the solution and get the final\nanswer.‚Äù\nLLM: ‚ÄúApologies for the mistake. Let‚Äôs correct\nthe solution.\nTo find the remainder when 1313 +5 is divided\nby 6, we can simplify the expression 1313 first.\nUsing the property of remainders, we can ob-\nserve that 13 ‚â° 1 (mod 6). Therefore, 1313 ‚â°\n113 ‚â°1 (mod 6).\nNow, we can rewrite the expression as1313 +5 ‚â°\n1 +5 ‚â°6 ‚â° 0 (mod 6).\nHence, the remainder when 1313 +5 is divided\nby 6 is 0.‚Äù\nIn this example, we can observe that LLMs try to\ncalculate the exact value of1313+5 at first, and then\nget the remainder. However, the calculator does\nnot understand the expression ‚Äú302875106592258\nmod 6‚Äù and returns the wrong result. Through\nrefining, LLM changes the solution and gets the\ncorrect answer.\n14789\nYou can use tool to help you solve the problem and I give you the instruction ‚Ä¶‚Ä¶ \nCalculator can help you ‚Ä¶‚Ä¶ Equation solver can help you ‚Ä¶‚Ä¶ Do you understand?\nYes, I understand. I will use tool to help me solve the problem.\nRetrieval Set\nProblem 1 & Solution 1 ‚Ä¶‚Ä¶ Problem n & Solution n\nI give you some example.\nProblem: The product ‚Ä¶‚Ä¶\nYou can use ‚Ä¶‚Ä¶\nYes, I understand. I will solve the problem step by step and use tool to help me.\nProblem: A function ùëì has the property that ùëì(3ùë• ‚àí1) = ùë•2 +ùë• +1 ‚Ä¶‚Ä¶\nLet's think step by step and use knowledge in similar problem to solve this problem\nFirst, we need to find out the value of ùë• that corresponds to ùëì(5)‚Ä¶‚Ä¶\n‚Ä¶ ‚Ä¶\nYou should solve the problem step by step and you should follow the react in the history ‚Ä¶‚Ä¶\nYes, I understand. I will follow my response in the conversation history ‚Ä¶‚Ä¶\nProblem: Solve for ùëí, given that 2ùëë is \n8 less than 17ùëí, and 2ùëí is 9 less than ùëë. \nLet‚Äôs think step by step ‚Ä¶‚Ä¶\nProblem: What is the value of\n3\n45 +45 +45 +45\nLet's think step by step ‚Ä¶‚Ä¶\nFirst, we need to simplify \nunder the radical. ‚Ä¶‚Ä¶\nFirst, we need to express ùëë\nin terms of ùëí ‚Ä¶‚Ä¶\nGive me the unknown variable \nGive me the equation system\nùëë, ùëí\nEquation Solver\nTo solve this sub-problem, which tool \ncan we use? ‚Ä¶‚Ä¶\nResults: ùíÖ = ùüèùüë, ùíÜ = ùüê\nContinue reasoning\n2ùëë = 17ùëí ‚àí8, 2ùëí = ùëë ‚àí9\nFinally, we can conclude \nthat ùëë = 13 and ùëí = 2. \nTherefore, the answer is 2.\nTo solve this sub-problem, which \ntool can we use? ‚Ä¶‚Ä¶\n3\n46\nGive me the equation ‚Ä¶‚Ä¶\nResults:\nùüë\nùüíùüî = ùüèùüî\nContinue reasoning\nTherefore,‚Ä¶‚Ä¶\n3\n46 = 16\nTherefore, the answer is \n16.0.\nTool Knowledge\nTask Knowledge\nReasoning Format\nLLM for Reasoning\nLLM for Tools Selection\nTools Execution\n‚Ä¶ ‚Ä¶\nCalculator\nConversational \nKnowledge\nMemory\nI give you some example.\nProblem: What is the ‚Ä¶‚Ä¶\nYou can use ‚Ä¶‚Ä¶\nExternal Tools\nCalculator, Equation Solver, ‚Ä¶‚Ä¶\nInput Question\nTool-Augmented\nReasoning\nFigure 2: An illustration example for ChatCoT from MATH.\n14790"
}