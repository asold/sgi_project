{
  "title": "Training Language Models with Memory Augmentation",
  "url": "https://openalex.org/W4385573102",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2886587608",
      "name": "Zexuan Zhong",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A2097808136",
      "name": "Tao Lei",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A2095803999",
      "name": "Danqi Chen",
      "affiliations": [
        "Princeton University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W3207024370",
    "https://openalex.org/W4286905627",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3175863856",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2963088785",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3123615524",
    "https://openalex.org/W4287649493",
    "https://openalex.org/W2963045354",
    "https://openalex.org/W4288087322",
    "https://openalex.org/W4386566764",
    "https://openalex.org/W4288024261",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W3204302053",
    "https://openalex.org/W3169283738",
    "https://openalex.org/W2138621090",
    "https://openalex.org/W2131494463",
    "https://openalex.org/W3037798801",
    "https://openalex.org/W4205694376",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W4287323699",
    "https://openalex.org/W3134891661",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W4300553569",
    "https://openalex.org/W4288336773",
    "https://openalex.org/W3171494313",
    "https://openalex.org/W4224251244",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W3166509360",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W4287667694",
    "https://openalex.org/W4287122359",
    "https://openalex.org/W4225909425",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W3157700644",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3033529678"
  ],
  "abstract": "Recent work has improved language models (LMs) remarkably by equipping them with a non-parametric memory component. However, most existing approaches only introduce mem-ories at testing time or represent them using a separately trained encoder, resulting in suboptimal training of the language model. In this work, we present TRIME, a novel yet simple training approach designed for training LMs with memory augmentation. Our approach uses a training objective that directly takes in-batch examples as accessible memory. We also present new methods for memory construction and data batching, which are used for adapting to different sets of memories—local, long-term, and external memory—at testing time. We evaluate TRIME on multiple language modeling and machine translation benchmarks and show that it is able to achieve significant improvements across all the settings. Concretely, TRIME reduces the perplexity from 18.70 to 15.37 on WIKITEXT-103, by effectively leveraging a large memory set from the training corpus. Compared to standard LM training, TRIME adds negligible computational overhead and is compatible with different neural architectures, making it a versatile solution for training memory-augmented LMs.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5657–5673\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nTraining Language Models with Memory Augmentation\nZexuan Zhong† Tao Lei∗ Danqi Chen†\n†Princeton University\n{zzhong, danqic}@cs.princeton.edu, taole@google.com\nAbstract\nRecent work has improved language models\n(LMs) remarkably by equipping them with a\nnon-parametric memory component. However,\nmost existing approaches only introduce mem-\nories at testing time or represent them using a\nseparately trained encoder, resulting in subop-\ntimal training of the language model. In this\nwork, we present T RIME , a novel yet simple\ntraining approach designed for training LMs\nwith memory augmentation. Our approach\nuses a training objective that directly takes in-\nbatch examples as accessible memory. We also\npresent new methods for memory construction\nand data batching, which are used for adapt-\ning to different sets of memories—local, long-\nterm, and external memory—at testing time.\nWe evaluate TRIME on multiple language mod-\neling and machine translation benchmarks and\nshow that it is able to achieve signiﬁcant im-\nprovements across all the settings. Concretely,\nTRIME reduces the perplexity from 18.70 to\n15.37 on WIKI TEXT-103, by effectively lever-\naging a large memory set from the training\ncorpus. Compared to standard LM training,\nTRIME adds negligible computational over-\nhead and is compatible with different neural\narchitectures, making it a versatile solution for\ntraining memory-augmented LMs.1\n1 Introduction\nMemory augmentation has become a remarkable\napproach to enhance language modeling perfor-\nmance without signiﬁcantly increasing the amount\nof parameters and computation. By accessing\nmemory units such as a neural cache of recent\ninputs (Merity et al., 2017; Grave et al., 2017b)\nand an external look-up table (Khandelwal et al.,\n2020), a memory-augmented language model (LM)\nenjoys increased memorization capacity and sets\n∗TL currently works at Google Research. The collabora-\ntion was initialized before TL joined Google.\n1Our code and pre-trained models are publicly available at\nhttps://github.com/princeton-nlp/TRIME.\nForward pass\nencoder\nJobs became CEO of _\nand\nApple\nfirst …\nsimilarity\n… works at Microsoft\n… Jobs became CEO \n… returned to Apple\n… moves to Apple\nTarget token’s embedding\nOther token embeddings\nPositive in-batch memory\nNegative in-batch memory\nprediction (target: “Apple”)\n…\nMicrosoft\ncolor |V| token \nembeddings\nIn-batch \nmemories\nBack-propagation\nFigure 1: An illustration of our training objective. Our\nobjective aligns the hidden representation with both to-\nken embeddings and a set of in-batch contextualized\nrepresentations that are constructed during training.\nnew state-of-the-art records in various language\nmodeling benchmarks.\nA major limitation of existing approaches, how-\never, is that the memory units are either intro-\nduced at testing time (Grave et al., 2017b,a; Khan-\ndelwal et al., 2020) or taken from a separately\ntrained model (Yogatama et al., 2021). As a con-\nsequence, they are not directly optimized during\nthe training process, resulting in a missed oppor-\ntunity to achieve even stronger results. In this pa-\nper, we pioneer and present a novel yet simple\ntraining approach TRIME (Training with In-batch\nMemories)2, that is well-suited for memory aug-\nmentation in language modeling. Our approach\nmakes two major departures compared to standard\nlanguage model training:\nTraining objective Inspired by contrastive rep-\nresentation learning, we propose a training objec-\ntive that directly leverages in-batch examples as\naccessible memory (Figure 1). Our training ob-\n2We can also interpret TRIME as three types of memories,\nas we will elaborate in the paper.\n5657\njective is closely connected to neural cache mod-\nels (Grave et al., 2017b; Merity et al., 2017) and\nnearest-neighbor language models (Khandelwal\net al., 2020), where the next-token probabilities are\ncalculated by comparing encoder outputs against\nstatic token embeddings and memory representa-\ntions. However, previous work only considers in-\ncorporating memories at testing time, while we do\nfor both training and testing.\nIn-batch memory construction With this train-\ning objective in mind, the key challenge is how\nto construct memories effectively during training\nwhile keeping it efﬁcient. We identify three types\nof memories that can be leveraged at testing time\nand have been explored in the literature: (a) local\nmemory denotes the words that appear in the re-\ncent past and are modeled using attention (Vaswani\net al., 2017); (b) long-term memory3 denotes long-\nrange context from the same document but cannot\nbe directly accessed due to the limit of input length;\n(c) external memory is used to store the entire train-\ning set or any additional corpus (Khandelwal et al.,\n2020; Borgeaud et al., 2021).\nTo better leverage these memories at testing time,\nwe devise new data batching strategies to improve\nthe construction of training memories (§4). By\npacking consecutive segments from the same docu-\nment in one training batch, our model can access\nlong-term memories beyond the attention context.\nWe pack segments from other documents that have\nhigh lexical overlap as a proxy to all external mem-\nory units. Importantly, these working memories are\ngenerated on the ﬂy during training, allowing us to\nback-propagate to all memory representations.\nWe instantiate TRIME in three models by consid-\nering different sets of training and testing memories\n(Table 1) and evaluate them on multiple language\nmodeling and machine translation benchmarks. We\nhighlight our results as follows:\n• We ﬁrst show that we can simply optimize a\nlanguage model using our training objective with-\nout long-term and external memory. Without any\nother modiﬁcations, we demonstrate that a 247M\nTransformer-based model can achieve an improved\nperplexity from 18.70 to 17.76 on WIKI TEXT-\n103 (Merity et al., 2017) with negligible overhead.\nThis model can be viewed as a simple replacement\n3Long-term memory may have different interpretations in\nother contexts and we use long-term memory to refer to long-\nrange context in modeling long sequences, following previous\nwork (Martins et al., 2022; Wu et al., 2022).\nTraining Testing\nMemory Memory\nvanilla LM None None\ncont. cache None MlocalorMlong\nkNN-LM None Mext\nTRIMELM Mlocal Mlocal\nTRIMELMlong §4.2 Mlocal, Mlong\nTRIMELMext §4.3 Mlocal, Mlong, Mext\nTable 1: A comparison between our T RIME language\nmodels and previous approaches: vanilla LM, continu-\nous cache (Grave et al., 2017b,a), kNN-LM (Khandel-\nwal et al., 2020). Mlocal, Mlong, Mext denote local,\nlong-term and external memories respectively (§2.2).\nfor vanilla language models.\n• By training with consecutive segments in the\nsame batch, our approach is capable of leveraging\nvery long context at testing time—up to 15k-25k to-\nkens on WIKI TEXT-103 and ENWIK 8 (Mahoney,\n2009). Our approach achieves at least competitive\nperformance as previous works (Dai et al., 2019;\nMartins et al., 2022; Ji et al., 2022) that modify\nthe Transformer architecture to incorporate mem-\nories from previous segments, yet our solution is\nconceptually simpler and computationally cheaper.\n• Finally, we train language models by incor-\nporating all other segments in the same batch as\nmemories. Our model works better with a large\ndatastore at testing time and improves over the\nkNN-LM model (Khandelwal et al., 2020) by re-\nducing the test perplexity from 16.23 to 15.41 on\nWIKI TEXT-103 . We also demonstrate signiﬁcant\nimprovements over the kNN-MT baseline (Khan-\ndelwal et al., 2021) on an IWSLT’14 De-En ma-\nchine translation task.\nIn summary, we propose a simple approach TRIME\nfor optimizing language models with memory aug-\nmentation and demonstrate consistent and signiﬁ-\ncant gains in multiple experimental settings. Our\napproach only uses memories at the ﬁnal predic-\ntion step, and hence adds little computational over-\nhead and can be combined with different model\narchitectures such as recurrent networks and other\nattention variants (Lei, 2021; Dai et al., 2019; Rae\net al., 2020). We hope that our work can encour-\nage the research community to think about better\ntraining objectives for language models, given their\nsigniﬁcant societal impacts (Brown et al., 2020;\nChowdhery et al., 2022; Zhang et al., 2022).\n5658\n2 Preliminaries\n2.1 Language Modeling\nIn this paper, we mainly focus on improving lan-\nguage models, although our solutions may ex-\ntend to most text generation tasks (see one exam-\nple of machine translation in §5.4). Neural lan-\nguage models take a sequence of tokens as context\nct =x1,...,x t−1 and map it to a vector representa-\ntion fθ(ct) ∈Rd, where fθ(⋅) is parameterized by\na neural network. The next-token probability is:\nP(w/divides.alt0ct) ∝ exp(E/uni22BA\nwfθ(ct)), (1)\nwhere Ew ∈Rd denotes the output embedding of\ntoken w∈V. The parameters are optimized to min-\nimize the negative log-likelihood of ground truth\nxt during training.\n2.2 Memory Augmentation\nWe consider memory as a set of context-target pairs\n{(ci,xi)}following Grave et al. (2017b); Khandel-\nwal et al. (2020). These context-target pairs can\nbe aggregated to obtain the next-token probability\nweighted by the similarity between hidden repre-\nsentations.4 We formalize three types of context-\ntarget memories as follows:\nLocal memory The local memory is simply the\npreceding tokens in the same input. Speciﬁcally,\nfor ct =x1,...,x t−1, it is deﬁned as:\nMlocal(ct) ={(cj,xj)}1≤j≤t−1. (2)\nGrave et al. (2017b) use the local memory at test-\ning time, denoted by the “continuous cache” model.\nHowever, it has been argued less effective for\nTransformer-based models because they can al-\nready learn to leverage recent tokens in the self-\nattention layers (Khandelwal et al., 2020). Inter-\nestingly, we show that using local memory is still\nbeneﬁcial if we consider it during training.\nLong-term memory Long-term memory de-\nnotes long-range context from the same document,\nbut they cannot be directly accessed by attention.\nFor example, if a document contains 10K tokens,\nonly a short segment of text (e.g., 100-3K to-\nkens) can be fed into a Transformer model because\nthe complexity scales quadratically with the input\n4Other memory-augmented models differ in when the\nmemory was introduced, such as using them in attention, and\nretrieve texts of different granularity as memory (Guu et al.,\n2020; Borgeaud et al., 2021).\nlength. Formally, we divide a document into con-\nsecutive segments s(1),...,s (T), where a segment\ns(i)contains Lcontexts s(i) ={c(i)\n1 ,...,c (i)\nL }. The\nlong-term memory for c(i)\nt is:\nMlong(c(i)\nt ) ={(c(k)\nj ,x(k)\nj )}1≤k<i,1≤j≤L. (3)\nPrevious works (Dai et al., 2019; Rae et al., 2020;\nMartins et al., 2022; Ji et al., 2022; Wu et al., 2022;\nLei, 2021) leverage hidden representations from\nprevious segments with modiﬁed Transformer ar-\nchitectures to learn long-range dependency. Our\napproach does not modify the model architecture\nand is compatible with these neural architectures.5\nExternal memory Finally, external memory as-\nsumes a large corpus D and the external memory\nset can be deﬁned as:\nMext ={(cj,xj) ∈D}. (4)\nD can be simply the training corpus, or a domain-\nspeciﬁc corpus when the testing domain shifts\n(§5.3). Note that /divides.alt0Mext/divides.alt0is usually several or-\nders of magnitude larger than previous two types\n(e.g., 108); accessing all the memories is computa-\ntionally expensive and requires approximate near-\nest neighbor search (Johnson et al., 2019).\n3 Training with In-batch Memories\nIn this section, we propose a new training ap-\nproach TRIME for language model training. Com-\npared to standard language model training, our\ntraining objective assumes a set of training memo-\nries Mtrain ={(cj,xj)}. We differentiate training\nmemories from testing memories, as they are con-\nstructed on the ﬂy during training and may deviate\nfrom the testing memories used during inference.\nImportantly, the training memories are constructed\nfrom the same training batch, which enables back-\npropagating the training signal to the current hidden\nrepresentation as well as all the memory represen-\ntations. We will discuss how to construct training\nmemories in the next section (§4) and only discuss\nthe training objective in a general form.\nOur training objective is illustrated in Figure 1.\nGiven a memory set M and a context c, TRIME\n5Note that continuous cache can be naturally extended to\nlong-term memory, as we will experiment later. The earlier\ncontinuous cache work was applied to LSTMs on long se-\nquences, as LSTMs can linearly scale with long sequences\nand there is no need to segment documents.\n5659\n(a) Default batching\nDoc A\nDoc B\nDoc C\nBM25 \nselected\n(b) Batching consecutive \nsegments\n(c) Batching lexically  \nsimilar segments\nCurrent token In memory Not in memory\nRandomly \nsampledBatch size B\nSegment len L\nFigure 2: We present several data batching methods and memory construction strategies, in order to adapt to\ndifferent sets of testing memories. (a) default batching: all the segments are randomly drawn from the training\ncorpus (§4.1); (b) we batch consecutive segments from the same document (m> 1) in one training batch to better\nleverage long-range contexts (§4.2); (c) we batch lexically-similar segments in one training batch selected by\nBM25 to better incorporate a large datastore at testing time (§4.3).\ndeﬁnes the next-token probability distribution as:\nP(w/divides.alt0c) ∝ exp(E/uni22BA\nwfθ(c))+\n/summation.disp\n(cj,xj)∈Mtrain∶xj=w\nexp(sim(gθ(c),gθ(cj))). (5)\nHere, fθ(c) is the output representation of a Trans-\nformer model and Ew is the token embedding.\ngθ(⋅) denotes the representations that can be used\nto compute similarity between cand all the con-\ntexts cj in the memory Mtrain. It is possible to\nsimply take gθ = fθ; however, we ﬁnd that tak-\ning gθ to be the input of the ﬁnal feed-forward\nlayer in Transformer works better, which is con-\nsistent with the observation in Khandelwal et al.\n(2020). In addition, sim(⋅,⋅) is a similarity func-\ntion and we found using the scaled dot-product\nsim(q,k) = q⋅k√\nd (Vaswani et al., 2017) leads to\nstable training and better performance in our pre-\nliminary experiments.\nThis training objective can be viewed as a con-\ntrastive loss (Hadsell et al., 2006): for a context-\ntarget pair (c,w∗), the goal is to align the query rep-\nresentation fθ(c) (and gθ(c)) with the static token\nrepresentation Ew∗, and contextualized represen-\ntations that share the same next token i.e., gθ(cj)\nfor xj = w∗. Our objective handles rare words\nnicely—if w∗does not appear in the training mem-\nory, the objective will fall back to aligning fθ(c)\nwith only the word embedding Ew∗. Similar to\nthe vanilla training loss (Eq. 1), our TRIME loss is\noptimized to minimize the negative log-likelihood\nof next token w∗and all the parameters θand Ew\nare updated during training.\nOur training objective is also inspired by\nthe success of contrastive learning in dense re-\ntrieval (Karpukhin et al., 2020). As we will show\nin §6, it can help improve retrieving contexts that\nshare the same next token effectively when the\nset of testing memories is large. Our objective\nis also closely connected to the objective used in\nGrave et al. (2017b); Khandelwal et al. (2020),\nwhich linearly interpolates the distribution of stan-\ndard language modeling, and a distribution de-\nﬁned by cache/external datastore, e.g., P(w/divides.alt0c) =\n(1−λ)Plm(w/divides.alt0c)+λPkNN(w/divides.alt0c). Our work differs\nfrom previous works that we use this objective dur-\ning training (and testing), while they only used itat\ntesting time—the key is how to construct training\nmemories that we will elaborate next.6\n4 Adaption to Different Memories\nInference We are interested in incorporating the\nthree types of memories deﬁned in §2.2 and their\ncombinations at testing time. The testing objec-\ntive is basically the same as the training objective\n(Eq. 5) except that we take testing memories as a\ncombination of Mlocal, Mlong and Mext. As Mext\ncan be very large, we approximate it by retrieving\nthe top-K closest terms to gθ(c). We tune a tem-\nperature term τ to adjust the weight of the memory\ncomponent (see Appendix A for details).\nNotation Throughout this section, we use Lto\ndenote segment length, Bto denote the total num-\nber of segments used in the one training batch, and\nmto denote the number of consecutive segments\nfrom each document in the batch. Correspond-\ningly, each batch will contain b≈ B\nm different doc-\numents. L, B and m are hyper-parameters that\n6Grave et al. (2017b) described a “global normalization”\nvariant in the paper, which is similar to our objective. However,\nthey only used it at testing time and only considered short-term\ncontexts in calculating the distribution. Other works (Merity\net al., 2017; See et al., 2017) trained a pointer network with a\nlearned gating component for the interpolation—we attempted\ntraining with a similar objective earlier and found it to perform\nworse than our current objective.\n5660\nModel #Params Dev (↓) Test(↓) Speed(↑)\nTransformer (Baevski and Auli, 2019) 247M 17.96 18.65 -\n+ continuous cache (Grave et al., 2017b) 247M 17.67 18.27 -\nTransformer-XL (Dai et al., 2019) 257M - 18.30 -\nTransformer (our run) 247M 18.04 18.70 3.6k t/s\n+ continuous cache 247M 17.65 18.26 ↓0.44 3.6k t/s\n#TRIMELM 247M 17.10 17.76 ↓0.94 3.6k t/s\n#TRIMELMlong 247M 17.01 17.64 ↓1.06 3.6k t/s\nkNN-LM (our run) 247M 16.40 16.37 300 t/s\n+ continuous cache 247M 16.23 16.23 ↓0.14 300 t/s\n#TRIMELMext (w/oMlong) 247M 15.62 15.55 ↓0.82 300 t/s\n#TRIMELMext 247M 15.51 15.41 ↓0.94 300 t/s\nkNN-LM (Khandelwal et al., 2020)† 247M 16.06 16.12 50 t/s\n+ continuous cache (Grave et al., 2017b)† 247M 15.81 15.79 ↓0.33 50 t/s\n#TRIMELM†\next 247M 15.40 15.37 ↓0.75 50 t/s\nTable 2: Performance of our T RIME models on W IKI TEXT-103 (247M models, L = 3,072). †: the results are\nbased on computing actual distances instead of using approximated distances returned by FAISS indexes, which\nrequires a large SSD storage. To measure the speed of models (tokens/second), we run the model with a single\nNVIDIA RTX 3090 GPU and run the FAISS indexer with 32 CPUs.\nwe will choose for training, and will vary as we\nconsider different memories during inference.\nA key challenge is that the testing memories can\nbe very large (e.g., /divides.alt0Mlong/divides.alt0∼104 and /divides.alt0Mext/divides.alt0∼108\nin our experiments) and it is computationally in-\nfeasible to keep training memories the same as\ntesting memories. In the following, we will dis-\ncuss three ways of constructing training memories\nand data batching, aiming to reduce the discrep-\nancy between training and testing. Along the way,\nwe will also present three major model instanti-\nations: TRIME LM, TRIME LMlong, TRIME LMext\n(Table 1), which combine the training strategies\nand different sets of testing memories.\n4.1 Local Memory\nMlocal only considers all the previous tokens in the\nsame segment. It is straightforward that we can\nsimply use Mtrain =Mlocal. As shown in Fig. 2(a),\nwe basically do not need to makeany modiﬁcations\ncompared to standard language model training. All\nwe need is to replace the training objective of Eq. 1\nby our objective in Eq. 5, by incorporating(cj,xj),\n∀j < t in the memory during both training and\ntesting. The computational overhead is also negli-\ngible compared to running neural encoders on the\nsegment x1,...,x L itself. We denote this model as\nTRIME LM, which can be viewed as a lightweight\nreplacement for vanilla language models. As we\nwill show in the experiments, simply incorporating\nlocal memory provides a notable gain on multi-\nple LM benchmarks, showing the effectiveness of\ntraining with memories explicitly.\n4.2 Long-term Memory\nIn order to enable long-term memory augmenta-\ntion, we pack multiple consecutive segments from\nthe same document in a training batch (i.e., m> 1).\nFor a context-target pair(c,w) in the training batch,\nits accessible memory Mtrain includes tokens from\nprevious segments as well as the preceding tokens\nin the same segment. Figure 2(b) illustrates the\ntraining batch construction and the training mem-\nory for a given token. At testing time, we can use\na much longer context: we simply enumerate the\nnumber of segments used in Meval and choose the\noptimum based on the development set.\nWe denote this model as TRIME LMlong. It\nshares a similar motivation with many previous\nworks which aim to leverage memory from pre-\nvious segments through attention recurrence (Dai\net al., 2019; Ji et al., 2022), or memory compres-\nsion (Rae et al., 2020; Martins et al., 2022; Wu\net al., 2022). However, our solution deviates signif-\nicantly from previous approaches. First, previous\nworks need to store the hidden representations (of\nevery layer) from previous segments and modify\nthe self-attention layers to incorporate them. Our\napproach does not modify the architecture and only\nuses the outputs from the last layer. Additionally,\nprevious works use stale memory representations\nand do not back-propagate gradients to the rep-\n5661\nresentations of previous segments, whereas our\nbatching method enables gradient propagation to\nthe memory and previous segments.7 As we will\nshow in the experiments, our approach is competi-\ntive with previous works while being conceptually\nsimpler and computationally cheaper.\n4.3 External Memory\nFinally, we consider external memory Mext. Since\nMext contains the context-target pairs in a large\ncorpus such as the entire training set, we need\nto retrieve top- K pairs from Mext measured by\nsim(gθ(c),gθ(cj)) through (approximate) similar-\nity search (more details are given in §5.2).\nSince the retrieved contexts at testing time are\nexpected to be similar to the query context, we\npropose a simple heuristic for constructing train-\ning memories Mtrain by packing segments that\nhave large lexical overlap into the same batch us-\ning BM25 scores (Robertson and Zaragoza, 2009).\nSpeciﬁcally, we start with a single segment and\nrepeatedly add segments with highest BM25 scores\ninto the same batch (Appendix B). A high BM25\nscore indicates that two segments have high lexical\noverlap and can serve as a good proxy to nearest\nneighbors in the external memory, which improves\nour model predictions at testing time. Mtrain con-\ntains all tokens from other segments as well as the\nprevious tokens in the same segment (Figure 2(c)).\nWe set m = 1 during training as many segments\nfrom the same document tend to have high lexical\noverlap and denote this model by TRIME LMext.\nIn practice, when considering tokens from both\nthe current segment and other segments in the\nbatch, we observe that the model tends to lever-\nage local memory more and ignore other segments.\nTo encourage the use of information from other seg-\nments, we exclude the local memory from Mtrain\nwith a probability of pduring training (we ﬁnd that\np=90% works the best, see Appendix H). This sig-\nniﬁcantly improves performance when the model\nis evaluated with a large set of external memory.\n5 Experiments\n5.1 Datasets and Tasks\nWe evaluate our approach on two popular language\nmodeling benchmarks: WIKI TEXT-103 (Merity\n7We also attempted using segments in previous training\nbatches as stale representations and did not ﬁnd any improve-\nment in preliminary experiments.\net al., 2017), ENWIK 8 (Mahoney, 2009), and a ma-\nchine translation benchmark: IWSLT’14 D e-En.\nWe also evaluate domain-adaptation performance\non the BOOKS CORPUS dataset (Zhu et al., 2015).\nWIKI TEXT-103 is a word-level language mod-\neling dataset consisting of 103M training tokens.\nWe evaluate on two model conﬁgurations: one uses\na 247M Transformer model and a segment length\nL = 3,072 and another one uses a 150M Trans-\nformer model with a segment length L=150.\nENWIK 8 is a character-level language modeling\ndataset that contains a total of 100M characters. We\nuse a 12-layer Transformer model with a hidden\ndimension 512 and segment length L=512.\nBOOKS CORPUS is a word-level language mod-\neling dataset. We build our own train/dev/test splits\nwhich consist of 100M/250K/250K tokens. On\nthis dataset, we evaluate the models trained on\nWIKI TEXT-103 to study how our approach can\nadapt to new domain without re-training.\nIWSLT’14 De-En is a machine translation task,\nwhich consists of 170K translation pairs. We use\na Transformer encoder-decoder model. See Ap-\npendix C for how we adapt our approach to the\nmachine translation task.\nSee Appendix C for data statistics and task se-\ntups and Appendix D for model conﬁgurations.\n5.2 Training and Inference Details\nWe implement our approach using the Fairseq li-\nbrary (Ott et al., 2019). For TRIME LMlong and\nTRIME LMext, we tune the number of segments\nused in Mlong on the development set during evalu-\nation. Our TRIME LMext model requires building a\nlarge datastore at testing time and we use the FAISS\nlibrary (Johnson et al., 2019) for approximate near-\nest neighbor search (details in Appendix D).\nWe ﬁrst train our model with the standard LM\nobjective (Eq. 1) for the ﬁrst 5% updates. Without\nthis warmup stage, we observe the training process\nto be unstable probably due to a large variance in\nthe estimated distributions. We use different mem-\nories when evaluating different instantiations of\nTRIME , as shown in Table 1. We ﬁnd that when a\nlarge set of external memory Mext is considered\nduring inference, the performance can be improved\nby linearly interpolating the output distribution and\na distribution over the memory, similarly to kNN-\nLM (Khandelwal et al., 2020). Thus, we apply an\nadditional linear interpolation to our output proba-\nbility distribution when considering external mem-\n5662\nModel Dev (↓) Test(↓)\nTransformer 28.11 29.14\nTransformer-XL 23.42 24.56\nCompressive Transformer - 24.41\n∞-former - 24.22\nLaMemo 22.98 23.77\nTransformer (our run) 25.31 25.87\n+ continuous cache∗ 22.95 23.59 ↓2.28\n#TRIMELM 24.45 25.60 ↓0.27\n#TRIMELMlong 21.76 22.66 ↓3.21\nTable 3: Performance on the W IKI TEXT-103 dataset\n(150M models, L = 150). T RIME LMlong uses a\nlong-term memory with 15,000 tokens. Transformer-\nXL: (Dai et al., 2019), Compressive Transformer:\n(Rae et al., 2020), ∞-former: (Martins et al., 2022),\nLaMemo: (Ji et al., 2022). ∗: cache adapted to long-\nterm memory.\nory Mext (see Appendix A for details).\n5.3 Results: Language Modeling\nTRIME LM vs. vanilla LM We ﬁrst compare\nour TRIME LM model which only uses local mem-\nory during training and testing. Table 2 shows that\nadding a continuous cache during inference can\nimprove the performance of vanilla Transformer\nfrom 18.70 to 18.26, and our TRIME LM further\nimproves the perplexity to 17.76. These results sug-\ngest that even though the attention mechanism can\n“see” local context, using local memory during both\ntraining and testing can still improve model perfor-\nmance. TRIME LM has no computational overhead\ncompared to vanilla LM (indicated by the “speed”\ncolumn), making it a simple and better replacement\nfor vanilla language models. Similar trends can be\nobserved in Table 3 and Table 4 (25.87 vs. 25.60\nand 1.16 vs. 1.12). The improvement is much\nsmaller though, due to a much smaller segment\nlength L. More analysis is given in Appendix G.\nTRIME LMlong leverages long contexts We\nthen examine our TRIME LMlong model which\nis trained with the data batching method de-\nscribed in §4.2. As shown in Table 3 and Ta-\nble 4, TRIME LMlong improves vanilla Transformer\nmodels substantially (i.e., 25.87 → 22.66 on\nWIKI TEXT-103 and 1.16 → 1.05 on ENWIK 8) by\nleveraging long-range contexts at inference time.\nWe ﬁnd the model achieves its best results when\nleveraging 15,000 tokens on WIKI TEXT-103 and\n24,576 tokens on ENWIK 8, even though the seg-\nments used during training are much shorter (L=\n150 and 512 respectively). We also add continuous\nModel #Params Dev (↓) Test(↓)\nT12 44M - 1.11\nTransformer-XL 41M - 1.06\nAdapt-Span 39M 1.04 1.02\nLongformer 41M 1.02 1.00\nExpire-Span 38M - 0.99\nTransformer (L=512) 38M 1.18 1.16\n+ continuous cache∗ 38M 1.16 1.17 ↑0.01\n#TRIMELM 38M 1.14 1.12 ↓0.04\n#TRIMELMlong 38M 1.08 1.05 ↓0.11\nSRU++ (L= 512) 42M 1.05 1.03\n#TRIMELMlong 42M 1.03 1.01 ↓0.02\nSRU++ (L= 2,048) 42M 1.02 0.99\n#TRIMELMlong 42M 1.00 0.98 ↓0.01\nTable 4: Performance on the E NWIK 8 dataset.\nTRIME LMlong achieves the best results by using a long-\nterm memory of a size 24,576. T12: (Al-Rfou et al.,\n2019), Transformer-XL: (Dai et al., 2019), Adapt-Span:\n(Sukhbaatar et al., 2019), Longformer: (Beltagy et al.,\n2020), Expire-Span: (Sukhbaatar et al., 2021), SRU++:\n(Lei, 2021). ∗: cache adapted to long-term memory.\ncache to the vanilla Transformer model and ﬁnd\nit to underperform our model, demonstrating the\nimportance of joint training using our approach.\nCompared to previous methods which explic-\nitly leverage hidden representations from previous\nsegments (Dai et al., 2019; Rae et al., 2020; Mar-\ntins et al., 2022; Ji et al., 2022; Lei, 2021), our\napproach achieves better or at least competitive per-\nformance. Different from these approaches which\nneed to store all the hidden representations of ev-\nery layer and modify the model architecture, we\nonly incorporate the outputs from the last layer—\nrequiring less computations and GPU memory. Our\napproach is orthogonal and can be applied on top\nof these models. To verify this, we adapt our ap-\nproach to SRU++ (Lei, 2021) (see details in Ap-\npendix E). As shown in the bottom block of Ta-\nble 4, TRIME LMlong gains consistently improve-\nment over vanilla SRU++, outperforming previ-\nously reported results given the same model size.\nTRIME LMext vs. kNN-LM Finally, our\nmodel TRIME LMext outperforms the kNN-LM\nmodel (Khandelwal et al., 2020), which uses exter-\nnal memory only at testing time—improving the\nperplexity from 16.23 to 15.41 on WIKI TEXT-103\n(Table 2). We also evaluate a model which does not\nuse long-term memory (denoted by TRIME LMext\nw/o Mlong) for a fair comparison with kNN-LM\nwith continuous cache and the difference is very\nsmall (15.55 vs 15.41). Our results suggest that by\nusing contrastive loss and BM25 batching (§4.3),\n5663\nModel Mext Dev(↓) Test(↓)\nTransformer - 62.72 53.98\n#TRIMELM - 59.39 49.25\n#TRIMELMlong - 49.21 39.50\nkNN-LM + cont. cache WIKI 53.27 43.24\n#TRIMELMext WIKI 47.00 37.70\nkNN-LM + cont. cache BOOKS 42.12 32.87\n#TRIMELMext BOOKS 36.97 27.84\nTable 5: Domain-adaption performance on the\nBOOKS CORPUS dataset. All models are trained\non W IKI TEXT-103 and evaluated on B OOKS COR-\nPUS without re-training or ﬁne-tuning and we con-\nsider using W IKI TEXT-103 and B OOKS CORPUS to\nbuild the external datastore respectively. We use a\nlong-term memory of a size 49,152 for T RIME LMlong,\nTRIME LMext, and continuous cache in this experiment.\nthe model learns to better retrieve and leverage\ninformation from a large external memory.\nDomain adaptation We evaluate the domain-\nadaptation performance of TRIME on BOOKS COR-\nPUS (Zhu et al., 2015). We take models that are\ntrained on WIKI TEXT-103 and evaluate them on\nBOOKS CORPUS without any re-training or ﬁne-\ntuning. As shown in Table 5, a vanilla Trans-\nformer model trained on WIKI TEXT-103 per-\nforms poorly on BOOKS CORPUS . TRIME LM and\nTRIME LMlong can signiﬁcantly improve the per-\nformance as they leverage local or long-term mem-\nory to adapt to the new domain. By building\nthe external memory using BOOKS CORPUS , both\nkNN-LM and TRIME LMext perform much better\non BOOKS CORPUS compared to the vanilla Trans-\nformer model. TRIME LMext outperforms kNN-\nLM on domain adaptation. This indicates that al-\nthough the memory representations are optimized\non one domain, our approach does not overﬁt, and\nbuilding an external memory using the target do-\nmain dataset enables the model to perform well\nwith domain shifts.\n5.4 Results: Machine Translation\nTo showcase the generality of our training approach\nTRIME to other generation tasks, we evaluate our\napproach on the IWSLT’14 de-en translation task.\nSince it is a sentence-level task, we do not use\nany local or long-term memory ( Mlocal, Mlong),\nas there are few repetitive tokens. We denote our\nmodel as TRIME MText.\nAs shown in Table 6, our approach improves the\nvanilla Transformer by 1.15 BLEU score and out-\nperforms kNN-MT (Khandelwal et al., 2021). This\nModel BLEU (↑)\nTransformer enc-dec 32.58\nkNN-MT 33.15 ↑0.57\n#TRIMEMText 33.73↑1.15\nTable 6: Results on the IWSLT’14 De-En test set. We\nadapt TRIME to the machine translation task. We use a\nbeam size of 4 during evaluation.\nMethod Test memory\nMlocal, Mlocal,\nMlocal Mlong Mlong, Mext\nTRIMELM 17.10 17.17 17.40\nTRIMELMlong 17.12 17.01 16.48\nTRIMELMext 17.99 17.80 15.51\nTable 7: Evaluating our three models (w/ different train-\ning methods) on different sets of testing memories. The\nresults are based on the development set of WIKI TEXT-\n103 (247M models, L=3,072).\ndemonstrates that our approach is able to improve\nthe performance on other language generation tasks\nwith different memory access.\n6 Analysis\nWe conduct ablation studies and analysis to further\nunderstand individual components of our approach.\nDue to the limited computation budget, some ex-\nperiments on WIKI TEXT-103 are conducted with\na small 7M Transformer model (8 layers, hidden di-\nmension 128) in this section and the trends are gen-\nerally similar for smaller models (see Appendix D\nand Appendix F for details).\nMemory construction We ﬁrst study how differ-\nent data batching and memory construction strate-\ngies affect the performance when different testing\nmemories are used. We compare our three models\n(TRIME LM, TRIME LMlong, TRIME LMext) in Ta-\nble 7. This ablation study clearly shows that pack-\ning consecutive segments and segments with high\nBM25 scores in the same training batch and con-\nstructing memories properly can improve the per-\nformance when the long-range and external memo-\nries are used. This demonstrates the importance of\nclosing the gap between training and inference.\nLeveraging long-range contexts We study if\nour model is able to handle large long-term mem-\nory. As Figure 3 shows, our model is able to ef-\nfectively handle long-range context (more than 10k\ntokens), which goes beyond typical attention con-\ntext. Compared to continuous cache (Grave et al.,\n5664\n0 5000 10000\n \n62\n64\n66\n68Perplexity\nWikiText103, L=3072\nContinuous cache\nTrimeLMlong\n0 5000 10000\n \n55\n60\n65\n70Perplexity\nWikiText103, L=512\nContinuous cache\nTrimeLMlong\n0 5000 10000\nLong-term memory size\n60\n70\n80Perplexity\nWikiText103, L=150\nContinuous cache\nTrimeLMlong\n0 5000 10000\nLong-term memory size\n1.10\n1.15\n1.20Bit per character\nenwik8, L=512\nContinuous cache\nTrimeLMlong\nFigure 3: Performance of TRIME LMlong (with different\nsegment lengths L) and continuous cache (adapted to\nlong-term memory) on the W IKI TEXT-103 (7M mod-\nels) and ENWIK 8 development sets.\n2017b,a), the improvement of our approach be-\ncomes larger when more long-term memory is in-\ncorporated. This suggests that our model is able to\nleverage long-range context much more effectively.\nAdditional analysis We conduct more ablation\nstudies and analysis in Appendix G. We summa-\nrize them as follows. (1) Our ablation studies\nshow using BM25 batching method and enabling\nback-propagation to update memory representa-\ntions are important for our approach (Table 11).\n(2) TRIME LM is able to leverage local memory\neffectively to improve performance with different\nsegment lengths L (Table 12). (3) TRIME LMext\noutperforms kNN-LM in terms of top-K retrieval\naccuracy given the external memory set (Table 13).\n(4) We study the perplexity of tokens in differ-\nent frequency groups and ﬁnd that TRIME LM\nand TRIME LMlong achieve larger improvements\non rare words while TRIME LMext improves results\nacross the board (Table 14).\n7 Related Work\nMemory-augmented language models We\nhave discussed continuous cache, kNN-LM\nand models that leverage representations from\nlong-range context in the previous sections.\nYogatama et al. (2021) also aim to combine several\ntypes of memories by learning an adaptive gating\nfunction; however, their external memory uses a\npre-trained vanilla language model. Borgeaud et al.\n(2021) demonstrate a remarkable performance\nby augmenting LMs with an external datastore\nof trillion of tokens and their datastore is built\nbased on chunks of text using off-the-shelf\nBERT embeddings (Devlin et al., 2019). Our\napproach differs from prior works in the following\naspects, which help our model achieve superior\nperformance with little overhead: (1) we update the\nmemory representations through back-propagation\nfrom the end loss; (2) our model does not modify\nthe base architecture; (3) we consider different\ntypes of memories in a uniﬁed framework.\nGNN-LM (Meng et al., 2022) augments LMs with\na graph neural network to aggregate information\nof retrieved items from external memory, which\nmakes an orthogonal contribution to our paper.\nTransformers for long inputs A large body of\nresearch has investigated how to scale self-attention\nmechanism to long contexts, either through sparse\nattention (Liu et al., 2018; Child et al., 2019;\nBeltagy et al., 2020; Zaheer et al., 2020) or sub-\nquadratic-time attention (Wang et al., 2020; Choro-\nmanski et al., 2020; Peng et al., 2021; Katharopou-\nlos et al., 2020). See Tay et al. (2020) for a com-\nprehensive survey of efﬁcient Transformers. Our\napproach is orthogonal, as we only change the train-\ning objective and data batching to enable models\nto use large contexts during inference.\nMemory-augmented models for downstream\ntasks While our paper focuses on improving lan-\nguage models with memory augmentation, other\nworks improve models for downstream tasks with\na retrieval component, such as question answer-\ning (Kumar et al., 2016; de Masson D’Autume\net al., 2019; Karpukhin et al., 2020; Guu et al.,\n2020; Zemlyanskiy et al., 2021; de Jong et al.,\n2022; Chen et al., 2022; Izacard and Grave, 2021;\nSingh et al., 2021), dialogue (Fan et al., 2021), and\nother knowledge-intensive NLP tasks (Lewis et al.,\n2020; Petroni et al., 2021).\n8 Conclusion\nIn this work, we propose TRIME , a training ap-\nproach for language modeling. We present three\nmodel instantiations TRIME LM, TRIME LMlong,\nTRIME LMext: Through carefully-designed data\nbatching and memory construction during training,\nwe show that our models can leverage long-range\ncontexts and external memory effectively at test-\ning time. Our approach adds little computational\noverhead and does not modify model architectures,\nmaking it compatible with other neural models and\ntechniques. For future work, we are interested in\ntraining TRIME with large language models and\nother text generation tasks.\n5665\nLimitations\nWe discuss limitations of our research as follows.\n• Despite the strong performance achieved by\nour approach when incorporating a large set\nof external memory, it results in a reduced\ninference efﬁciency at the same time due to\nthe nearest neighbor search. For example, the\nmodel is 10×slower when incorporating ex-\nternal memory. This issue can be more crucial\nwhen the external memory is even larger. Po-\ntential solutions to this issue include (1) con-\nstructing the memory using a coarser granular-\nity (e.g., text blocks) (Borgeaud et al., 2021);\n(2) compressing the external memory set and\nreducing the dimension of memory represen-\ntations (He et al., 2021).\n• We mainly experiment with Transformer-\nbased models and additionally adapt our ap-\nproach to SRU++ (Lei, 2021). We believe\nour approach is compatible with other archi-\ntectures or techniques such as Transformer-\nXL (Dai et al., 2019) and Compressive Trans-\nformer (Rae et al., 2020). We plan to explore\nthem as future work.\n• We evaluate our approach on machine trans-\nlation to test the generality of TRIME to other\ngeneration tasks. However, due to compute\nlimitation, we only evaluate it on a small\ndataset (i.e., IWSLT’14), which consists of\n4M tokens in the external memory. We leave\nthe evaluation on larger machine translation\ndatasets as future work.\n• Our paper mainly studies language model-\ning tasks and machine translation tasks. Al-\nthough we believe our approach is compati-\nble with all language generation tasks, how\nto adapt TRIME to natural language under-\nstanding tasks such as text classiﬁcation still\nremains an open question.\n• The biggest model we experimented with con-\nsists of 247M parameters due to our com-\npute limit. The state-of-the-art auto-regressive\nLMs contain hundreds of billions of param-\neters (Brown et al., 2020). We hope to see\nfuture efforts in scaling up our approach and\nevaluating the effectiveness on large LMs.\nEthical Considerations\nOur proposed approach leverages external mem-\nory to achieve strong results on multiple language\nmodeling benchmarks. In our experiments, we con-\nstruct the external memory using the corpus on\nwhich the model is trained, while it can be con-\nstructed using any corpus. In general, we suggest\npractitioners constructing external memory using\na public corpus, as retrieving from the external\ndatastore can cause information leakage from the\ncorpus. We acknowledge this ethical considera-\ntion and caution those who apply our approach to\nprivacy-sensitive domains.\nAcknowledgments\nWe thank Jane Pan, Howard Chen, Alexander Wet-\ntig, Tianyu Gao, Kaiyu Yang, Mengzhou Xia, Jin-\nhyuk Lee, and the members of Princeton NLP\ngroup for helping with proofreading and providing\nvaluable feedback. This research is partially sup-\nported by the James Mi *91 Research Innovation\nFund for Data Science and a gift from Apple. ZZ\nis also supported by a JP Morgan PhD fellowship.\nReferences\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2019. Character-level lan-\nguage modeling with deeper self-attention. In Con-\nference on Artiﬁcial Intelligence (AAAI), volume 33,\npages 3159–3166.\nAlexei Baevski and Michael Auli. 2019. Adaptive in-\nput representations for neural language modeling. In\nInternational Conference on Learning Representa-\ntions (ICLR).\nIz Beltagy, Matthew E Peters, and Arman Cohan.\n2020. Longformer: The long-document Trans-\nformer. arXiv preprint arXiv:2004.05150.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge van den Driessche, Jean-Baptiste Lespiau,\nBogdan Damoc, Aidan Clark, et al. 2021. Improv-\ning language models by retrieving from trillions of\ntokens. arXiv preprint arXiv:2112.04426.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in Neural Information Process-\ning Systems (NeurIPS), 33:1877–1901.\nWenhu Chen, Pat Verga, Michiel de Jong, John Wi-\neting, and William Cohen. 2022. Augmenting\n5666\npre-trained language models with qa-memory for\nopen-domain question answering. arXiv preprint\narXiv:2204.04581.\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse Transformers. arXiv preprint\narXiv:1904.10509.\nKrzysztof Choromanski, Valerii Likhosherstov, David\nDohan, Xingyou Song, Andreea Gane, Tamas Sar-\nlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\nLukasz Kaiser, et al. 2020. Rethinking attention\nwith Performers. arXiv preprint arXiv:2009.14794.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. PaLM: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc V Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Association for Computa-\ntional Linguistics (ACL).\nMichiel de Jong, Yury Zemlyanskiy, Nicholas FitzGer-\nald, Fei Sha, and William Cohen. 2022. Mention\nmemory: incorporating textual knowledge into trans-\nformers through entity mention attention. In Inter-\nnational Conference on Learning Representations\n(ICLR).\nCyprien de Masson D’Autume, Sebastian Ruder, Ling-\npeng Kong, and Dani Yogatama. 2019. Episodic\nmemory in lifelong language learning. Advances in\nNeural Information Processing Systems (NeurIPS) ,\n32.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional Transformers for language under-\nstanding. In North American Chapter of the Associ-\nation for Computational Linguistics (NAACL).\nAngela Fan, Claire Gardent, Chloé Braud, and An-\ntoine Bordes. 2021. Augmenting Transformers with\nknn-based composite memory for dialog. Transac-\ntions of the Association of Computational Linguis-\ntics (TACL), 9:82–99.\nEdouard Grave, Moustapha M Cisse, and Armand\nJoulin. 2017a. Unbounded cache model for online\nlanguage modeling with open vocabulary. Advances\nin Neural Information Processing Systems (NIPS) ,\n30.\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2017b. Improving neural language models with a\ncontinuous cache. In International Conference on\nLearning Representations (ICLR).\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\nsupat, and Ming-Wei Chang. 2020. REALM:\nRetrieval-augmented language model pre-training.\nIn International Conference on Machine Learning\n(ICML).\nRaia Hadsell, Sumit Chopra, and Yann LeCun. 2006.\nDimensionality reduction by learning an invariant\nmapping. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) , volume 2,\npages 1735–1742.\nJunxian He, Graham Neubig, and Taylor Berg-\nKirkpatrick. 2021. Efﬁcient nearest neighbor lan-\nguage models. In Empirical Methods in Natural\nLanguage Processing (EMNLP).\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open\ndomain question answering. In European Chap-\nter of the Association for Computational Linguistics\n(EACL).\nHaozhe Ji, Rongsheng Zhang, Zhenyu Yang, Zhipeng\nHu, and Minlie Huang. 2022. LaMemo: Language\nmodeling with look-ahead memory. In North Amer-\nican Chapter of the Association for Computational\nLinguistics (NAACL).\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen,\nand Wen-tau Yih. 2020. Dense passage retrieval\nfor open-domain question answering. In Empirical\nMethods in Natural Language Processing (EMNLP).\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and François Fleuret. 2020. Transformers are\nRNNs: Fast autoregressive Transformers with linear\nattention. In International Conference on Machine\nLearning (ICML), pages 5156–5165.\nUrvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2021. Nearest neigh-\nbor machine translation. In International Confer-\nence on Learning Representations (ICLR).\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In International Conference on Learning\nRepresentations (ICLR).\nAnkit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer,\nJames Bradbury, Ishaan Gulrajani, Victor Zhong,\nRomain Paulus, and Richard Socher. 2016. Ask me\nanything: Dynamic memory networks for natural\nlanguage processing. In International Conference\non Machine Learning (ICML), pages 1378–1387.\nTao Lei. 2021. When attention meets fast recurrence:\nTraining language models with reduced compute. In\nEmpirical Methods in Natural Language Processing\n(EMNLP).\n5667\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive NLP tasks. Advances in\nNeural Information Processing Systems (NeurIPS) ,\n33:9459–9474.\nPeter J Liu, Mohammad Saleh, Etienne Pot, Ben\nGoodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\nShazeer. 2018. Generating Wikipedia by summariz-\ning long sequences. In International Conference on\nLearning Representations (ICLR).\nMatt Mahoney. 2009. Large text compression bench-\nmark.\nPedro Henrique Martins, Zita Marinho, and André FT\nMartins. 2022. ∞-former: Inﬁnite memory Trans-\nformer. In Association for Computational Linguis-\ntics (ACL).\nYuxian Meng, Shi Zong, Xiaoya Li, Xiaofei Sun, Tian-\nwei Zhang, Fei Wu, and Jiwei Li. 2022. Gnn-lm:\nLanguage modeling based on global contexts via\ngnn. In International Conference on Learning Rep-\nresentations (ICLR).\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In International Conference on Learning Repre-\nsentations (ICLR).\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nNAACL-HLT 2019: Demonstrations.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. BLEU: a method for automatic eval-\nuation of machine translation. In Association for\nComputational Linguistics (ACL), pages 311–318.\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy\nSchwartz, Noah A Smith, and Lingpeng Kong. 2021.\nRandom feature attention. In International Confer-\nence on Learning Representations (ICLR).\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James\nThorne, Yacine Jernite, Vladimir Karpukhin, Jean\nMaillard, et al. 2021. KILT: a benchmark for knowl-\nedge intensive language tasks. In North American\nChapter of the Association for Computational Lin-\nguistics (NAACL), pages 2523–2544.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Belgium, Brussels. Association for Computa-\ntional Linguistics.\nJack W Rae, Anna Potapenko, Siddhant M Jayaku-\nmar, and Timothy P Lillicrap. 2020. Compressive\nTransformers for long-range sequence modelling. In\nInternational Conference on Learning Representa-\ntions (ICLR).\nStephen Robertson and Hugo Zaragoza. 2009. The\nprobabilistic relevance framework: BM25 and be-\nyond. Now Publishers Inc.\nAbigail See, Peter J Liu, and Christopher D Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Association for Computa-\ntional Linguistics (ACL), pages 1073–1083.\nDevendra Singh, Siva Reddy, Will Hamilton, Chris\nDyer, and Dani Yogatama. 2021. End-to-end train-\ning of multi-document reader and retriever for open-\ndomain question answering. Advances in Neural In-\nformation Processing Systems (NIPS).\nSainbayar Sukhbaatar, Édouard Grave, Piotr Bo-\njanowski, and Armand Joulin. 2019. Adaptive atten-\ntion span in Transformers. In Association for Com-\nputational Linguistics (ACL), pages 331–335.\nSainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen\nRoller, Arthur Szlam, Jason Weston, and Angela\nFan. 2021. Not all memories are created equal:\nLearning to forget by expiring. InInternational Con-\nference on Machine Learning (ICML) , pages 9902–\n9912. PMLR.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald\nMetzler. 2020. Efﬁcient Transformers: A survey.\narXiv preprint arXiv:2009.06732.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in Neural Information Process-\ning Systems (NIPS), 30.\nSinong Wang, Belinda Z Li, Madian Khabsa, Han\nFang, and Hao Ma. 2020. Linformer: Self-\nattention with linear complexity. arXiv preprint\narXiv:2006.04768.\nYuhuai Wu, Markus N Rabe, DeLesley Hutchins, and\nChristian Szegedy. 2022. Memorizing Transformers.\nIn International Conference on Learning Represen-\ntations (ICLR).\nDani Yogatama, Cyprien de Masson d’Autume, and\nLingpeng Kong. 2021. Adaptive semiparametric\nlanguage models. Transactions of the Association\nof Computational Linguistics (TACL), 9:362–373.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big Bird: Transformers for\nlonger sequences. In Advances in Neural Infor-\nmation Processing Systems (NeurIPS) , volume 33,\npages 17283–17297.\nYury Zemlyanskiy, Joshua Ainslie, Michiel de Jong,\nPhilip Pham, Ilya Eckstein, and Fei Sha. 2021.\nReadtwice: Reading very large documents with\n5668\nmemories. In North American Chapter of the As-\nsociation for Computational Linguistics (NAACL).\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al.\n2022. OPT: Open pre-trained Transformer language\nmodels. arXiv preprint arXiv:2205.01068.\nXin Zheng, Zhirui Zhang, Junliang Guo, Shujian\nHuang, Boxing Chen, Weihua Luo, and Jiajun Chen.\n2021. Adaptive nearest neighbor machine transla-\ntion. In Association for Computational Linguistics\n(ACL), pages 368–374.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In International Conference on\nComputer Vision (ICCV), pages 19–27.\n5669\nA Inference Method\nTesting objective Formally speaking, our testing\nobjective is basically the same as the training ob-\njective (Eq. 5):\nP(w/divides.alt0c) ∝ exp(E/uni22BA\nwfθ(c))+\n/summation.disp\n(cj,xj)∈Meval∶xj=w\nexp(sim(gθ(c),gθ(cj))\nτ ), (6)\nexcept that we take Meval as a combination of\nMlocal, Mlong and Mext. As Mext can be very\nlarge, we approximate it by retrieving the top-K\nclosest terms to gθ(c). Formally, Meval of three\ninstantiations of TRIME is constructed as follows,\nMeval=\n/uni23A7/uni23AA/uni23AA/uni23AA/uni23AA/uni23A8/uni23AA/uni23AA/uni23AA/uni23AA/uni23A9\nMlocal (TRIMELM)\nMlocal∪Mlong (TRIMELMlong)\nMlocal∪Mlong∪kNN(Mext,gθ(c)) (TRIMELMext)\n(7)\nwhere kNN(Mext,gθ(c)) returns the top-K closest\nterms to gθ(c) in the memory set Mext. Addi-\ntionally, because Meval may be different from the\ntraining memories, we tune a temperature termτ to\nadjust the weight of the memory component when\ncalibrating the distribution, based on the develop-\nment set.\nLinear interpolation when usingMext We ﬁnd\nthat when a large set of external memory Mext is\nconsidered during inference, the performance can\nbe improved by calibrating a separated distribution\nover the memory and interpolating the output dis-\ntribution and the memory distribution, similarly\nto kNN-LM (Khandelwal et al., 2020). We think\nthis is because the distribution of the similarity\nvalues has been signiﬁcantly shifted during infer-\nence, while the relative ranking preserves. As a\nresult, having values from two different distribu-\ntions in one softmax normalization is sub-optimal\ncompared to computing two separated probabilities\nand interpolating them.\nThus, we apply an additional linear interpolation\nto our output probability distribution. Speciﬁcally,\nwe ﬁrst use Eq. 6 to compute the distributionP(w/divides.alt0\nc). Then, we compute a probability distribution\nover the tokens in memory P′(w/divides.alt0c) as follow,\nP′(w/divides.alt0c) ∝ ∑(cj,xj)∈Meval∶xj=wexp(sim(gθ(c),gθ(cj))\nτ′ ).\n(8)\nWe linearly interpolate these two probability distri-\nbutions with a coefﬁcient λand get the ﬁnal output\nPﬁnal(w/divides.alt0c):\nPﬁnal(w/divides.alt0c) =(1 −λ)P(w/divides.alt0c) +λP′(w/divides.alt0c).\n(9)\nWe tune the temperature terms and λon the devel-\nopment set.\nAlgorithm 1: Packing segments using\nBM25 scores. SimSeg(I,c,k ) returns the\ntop-k most similar segments to c in the\nBM25 indexer I. ( k = 20 when packing\nsegments in our experiments.)\nData: training segments S={s1,...,s /divides.alt0S/divides.alt0}\nBM25 Indexer: I\nHyper-parameters: k, batch size B\nOutput: training batches T\nl← list();\nc← None;\nwhile /divides.alt0S/divides.alt0≠0 do\nif cis None then\nc← random_sample(S);\nend\nl.append(c);\nS.remove(c);\nn← None;\nfor c′in SimSeg(I,c,k ) do\nif c′in Sthen\nn← c′;\nbreak;\nend\nend\nc← n;\nend\nT ← {[l1,...,l B],[lB+1,...,l 2B],... };\nreturn T;\nB Packing Segments Using BM25 Scores\nIn §4.3, we construct training memories Mtrain by\npacking segments that have large lexical overlap\ninto the same batch using BM25 (Robertson and\nZaragoza, 2009). Algorithm 1 shows the process to\npack segments into training batches. We start with\na single segment and repeatedly add segments with\nhighest BM25 scores into the same batch.\nC Dataset Statistics and Tasks\nWe evaluate our approach on three benchmarks:\nWIKI TEXT-103 , ENWIK 8, and IWSLT’14. We\nalso evaluate our approach on BOOKS CORPUS for\n5670\nTrain Dev Test /divides.alt0V/divides.alt0len\nWIKITEXT-103 110M 0.2M 0.3M 270K 3.6K\nENWIK8 94M 5.2M 5.2M 256 -\nBOOKSCORPUS 100M 250K 250K 270K 90K\nIWSLT’14 160K 7K 6K 16K 25\nTable 8: Statistics of the four datasets used in our paper.\nWIKI TEXT-103 and B OOKS CORPUS are a word-level\nLM task and E NWIK 8 is a character-level language\nmodeling task, and IWSLT’14 is a German-English\nmachine translation task. len: denotes the average num-\nber of tokens over training examples in each dataset.\nWhen evaluating models on B OOKS CORPUS without\nre-training, we use the W IKI TEXT-103’s vocabulary.\nIWSLT’14 is a sentence-level task, so incorporating\nlong-range context will not help.\ndomain adaptation (Appendix 5.3). Table 8 shows\nthe statistics.\nWIKI TEXT-103 (Merity et al., 2017) is a word-\nlevel language modeling dataset consisting of\n103M training tokens. Following standard prac-\ntice, we use adaptive softmax and adaptive to-\nken embeddings (Baevski and Auli, 2019) in our\nmodel and report perplexity. In order to better\ncompare with previous work, we evaluate on two\nmodel conﬁgurations—one uses a 247M Trans-\nformer model and a segment length L = 3,072\nfollowing Baevski and Auli (2019); Khandelwal\net al. (2020) and another one uses a 150M Trans-\nformer model with segment length L=150 follow-\ning Dai et al. (2019). More details are provided in\nAppendix D.\nENWIK 8 (Mahoney, 2009) is a character-level\nlanguage modeling dataset that contains a total of\n100M characters. Following previous work, we\nreport bit-per-character (bpc) on this dataset. We\nuse a 12-layer Transformer model with a hidden\ndimension 512 and segment length L=512.\nWe also evaluate the IWSLT’14 DE→ EN ma-\nchine translation task, which consists of 170K trans-\nlation pairs. Following Khandelwal et al. (2021),\nwe build an external memory by taking all the\ntranslation contexts and the corresponding target\ntoken ((x,y<t),yt) on the training set. We use\nthe output representation as f((x,y<t)) and the in-\nput representation of last FFN layer as g((x,y<t))\nto compute the loss. Similarly, we use BM25 to\nbatch training data – we encourage two target sen-\ntences with a high BM25 score to be in the same\ntraining batch (see Algorithm 1). We use the de-\nfault model conﬁguration in the Fairseq library (Ott\net al., 2019), and sacrebleu (Post, 2018) to compute\nBLEU scores (Papineni et al., 2002).\nWe evaluate our approach for domain adaptation\non the BOOKS CORPUS dataset (Zhu et al., 2015),\nwhich is a word-level language modeling dataset.\nThe complete BOOKS CORPUS dataset consists of\n0.7B tokens. We build our own train/dev/test splits\nwhich consist of 100M/250K/250K tokens respec-\ntively. The train set is only used to build external\nmemory. On this dataset, we evaluate the models\ntrained on WIKI TEXT-103 to study how our ap-\nproach can adapt to new domain without re-training\nor ﬁne-tuning. The model we used on this dataset\nis the 247M Transformer model with a segment\nlength L=3,072.\nD Model Conﬁgurations and\nHyperparameters\nTable 9 shows the model conﬁgurations and hyper-\nparameters that we used in our experiments. Fol-\nlowing Baevski and Auli (2019), during training,\nwe train the model with ﬁxed-length segments; dur-\ning evaluation, we evaluate on the tokens at the\nend of the segment (i.e., an evaluation segment can\noverlap with others).\nWhen evaluating with large external memory, we\nalways retrieve top-K (K =1,024) context-target\npairs for language modeling. For machine transla-\ntion, we tune K={1,2,4,8,16,32,64}following\nZheng et al. (2021).\nE Applying T RIME LMlong to SRU++\nWe apply our approach to SRU++ (Lei, 2021) and\nwe believe our approach is also compatible with\nother architectures such as Transformer-XL (Dai\net al., 2019). SRU++ is a language model which\ncombines recurrent units and the attention mecha-\nnism. SRU++ use hidden representations from the\nprevious segment at attention layers to incorporate\nlong-range contexts, similarly to Dai et al. (2019).\nTo apply our approach to SRU++, we follow\ntheir data-batching method as it is required due to\nthe recurrence of the model architecture. We con-\nstruct the training memory using all the contexts\nin the current segment (i.e., local memory) and all\ncontexts in the previous segment (i.e., long mem-\nory). Note that the memory representations from\nthe previous segment will be stale, thus we do not\nback-propagate to that part. During training, we up-\ndate the model with 400K steps and a batch size of\n5671\nDataset WIKITEXT-103 ENWIK8 IWSLT’14\nModel\n#Params 247M 150M 7M 38M 39M\n#Layers 16 16 8 12 6+6\nHidden dimension 1024 410 128 512 512\nFFN intermediate dimension4096 2100 512 2048 1024\nAdaptive softmax? yes yes yes no no\nTraining\nSegment length 3072 150 3072 512 -\n#Tokens per update 73728 36000 24576 49152 16384\nGradient accumulation 3 4 1 4 1\nBatch size per update 24 240 8 96 -\n#Consecutive segments 4 60 8 24 -\n#In-batch memories 24576 9000 24576 12288 -\nEvaluation\nSegment length 512 64 512 80 -\n#Optimal long-term memories12288 15000 12288 24576 -\nOptimizer and scheduler\nOptimizer type nag adam adam adam adam\nLearning rate 1.0 2.5e-4 5e-4 2.5e-4 5e-4\nGrad crop norm 0.1 0.0 0.0 0.0 0.0\nUpdate steps 286000 400000 200000 400000 170000\nScheduler type cosine cosine inverse_sqrt cosine cosine\nLinear warmup steps 16000 0 8000 0 4000\nTable 9: Model conﬁgurations and hyperparameters in our experiments.\nModel Dev (↓) Test(↓)\nTransformer 82.68 83.66\n+ continuous cache 67.45 68.08\nkNN-LM 51.88 52.24\n+ continuous cache 45.15 45.82\n# TRIME LM 54.78 54.69 ↓28.97\n# TRIME LMlong 60.71 60.10 ↓23.56\n# TRIME LMext 41.50 42.36 ↓41.30\nTable 10: Performance of the 7M Transformer models\non the WIKI TEXT-103 dataset.\n16. For other hyper-parameters and the optimizer,\nwe follow the default ones in their implementation.\nDuring inference, we can use more contexts to\nconstruct memory. We train with different segment\nlengths, i.e., L= 512 or L= 2048. For the model\ntrained with L= 512, it can leverage a long-term\nmemory of a size 6,144 during inference; for the\nmodel trained with L = 2048, it can leverage a\nlong-term memory of a size 12,228.\nF Performance of the 7M model on\nWIKI TEXT-103\nWe conduct our ablation studies and analyses in\n§6 with an 8-layer Transformer model due to the\nModel Dev (↓)\nTRIME LMext 41.50\nw/o BM25 batching 45.71\nw/o back-prop to memory 45.15\nTable 11: Ablation studies of using BM25 batching and\nenabling back-propagation to memory representations\nduring training. The numbers are on the W IKI TEXT-\n103 development set (7M models).\nModel L=3072 L=512 L=150\nTransformer 82.70 81.15 81.40\n+ cont. cache 67.45 71.29 75.10\n# TRIMELM 54.89 63.22 71.82\nTable 12: Performance on the W IKI TEXT-103 devel-\nopment set (7M models). We vary the segment Lhere\nto study the effectiveness of using local memory.\nModel Top-1 Top-8 Top-64 Top-1024\nkNN-LM 25.82 50.03 69.85 86.97\n#TRIMELMext 27.64 51.16 70.43 87.18\nTable 13: Retrieval performance on external memory\nof our model (7M) and kNN-LM (Khandelwal et al.,\n2020) on the W IKI TEXT-103 development set. We re-\nport top-Kretrieval accuracy (K=1,8,64,1024).\n5672\nFrequency > 10k 1k-10k 100-1k 10-100 ≤10 avg\nTransformer 3.35 4.11 13.63 30.46 240.39 18.04\nkNN-LM + cont. cache 3.14 3.85 12.92 26.90 196.03 16.23\n#TRIMELM 3.47 4.15 13.57 28.05 198.33 17.10\n#TRIMELMlong 3.43 4.13 13.62 27.89 194.89 17.01\n#TRIMELMext 3.15 3.84 12.50 25.41 171.61 15.51\nTable 14: Averaged perplexity in each frequency bucket on the WIKI TEXT-103 development set (247M models).\np 0.0 0.1 0.5 0.9 1.0\nPerplexity 54.33 49.85 45.08 41.50 41.86\nTable 15: The performance of our model TRIME LMext\non the development set (WIKI TEXT-103, 7M models).\nWe disable the local memory with a probability of p\nduring training.\nlimited computation budget. The model consists of\n7M parameters, 8 layers and 4 heads in each layer.\nThe embedding dimension is 128 and the interme-\ndiate dimension of FFN is 512. The model takes a\nsegment of 3072 tokens as input. We compare our\napproach with baselines on this model architecture.\nAs shown in Table 10, our approach improves over\nthe baselines by a large margin. This shows that\nmodeling memory explicitly is essential when the\nmodel capacity is limited.\nG Additional Analysis\nAblation study on TRIME LMext We study the\nimportance of packing segments with high BM25\nscores in the same training batch, as well as the ef-\nfectiveness of enabling back-propagation to mem-\nory representations during training. As shown in\nTable 11, when we random batch training segments\n(instead of using BM25 scores), the perplexity in-\ncreases to 45.71 ( +4.21). Also, enabling back-\npropagation to memory is crucial for our approach\n— the performance is much worse if we disable it.\nEffectiveness of using local memory We study\nthe effectiveness of our modelTRIME LM that uses\nonly local memory with different segment lengths\nL. As shown in Table 12, our model signiﬁcantly\noutperforms the baselines in all the settings. This\nsuggests that our model can leverage local memory\nvery effectively to improve performance.\nRetrieval performance on external memory\nWhen external memory is used in our experiments,\nwe perform nearest-neighbor search over the entire\nmemory set Mext to retrieve the top K keys (we\nuse K = 1024). Table 13 compares the retrieval\naccuracy of our approach and kNN-LM (Khandel-\nwal et al., 2020) for different K. Our approach\noutperforms kNN-LM in terms of retrieval results;\nthis explains how our ﬁnal perplexity surpasses\nkNN-LM when incorporating external memory.\nPerplexity breakdown for different frequencies\nWe aim to understand which type of memories im-\nproves perplexity of tokens in different frequency\ngroups. We group tokens into 5 buckets according\nto their frequency on the development set. Table 14\nshows the results for different models. TRIME LM\nand TRIME LMlong improve the perplexity of rare\nwords (i.e., frequency≤1k) while achieving similar\nor slightly worse results for frequent words com-\npared to the Transformer baseline. TRIME LMext\nimproves perplexity in all the buckets. Interestingly,\nkNN-LM with continuous cache does not perform\nsigniﬁcantly better compared to TRIME LM and\nTRIME LMlong although these two models do not\nuse external memory. This suggests that jointly\ntraining memory representations and the language\nmodel particularly help improve the performance\nof rare words.\nH Tuning p for training with external\nmemory\nWhen training the model with local and external\nmemory, to avoid the model to only relies on high-\nquality local memory, we disable the local memory\nwith a probability of p. Here we study how pwill\naffect the ﬁnal performance of our model. The re-\nsults of using differentpare shown in Table 15. We\nﬁnd that when p = 0, the model performs poorly\nwith external memory as the model learns to only\nleverage local memory and ignores external mem-\nory during training. By increasing p, this issue is\nmitigated. We set p=0.9 in our main experiments.\n5673",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8711659908294678
    },
    {
      "name": "Perplexity",
      "score": 0.7062833905220032
    },
    {
      "name": "Language model",
      "score": 0.6096083521842957
    },
    {
      "name": "Machine translation",
      "score": 0.5329269766807556
    },
    {
      "name": "Memory model",
      "score": 0.5318671464920044
    },
    {
      "name": "Overhead (engineering)",
      "score": 0.5026910305023193
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.48094338178634644
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46156755089759827
    },
    {
      "name": "Machine learning",
      "score": 0.43628424406051636
    },
    {
      "name": "Training (meteorology)",
      "score": 0.4110790193080902
    },
    {
      "name": "Programming language",
      "score": 0.20192494988441467
    },
    {
      "name": "Parallel computing",
      "score": 0.16572123765945435
    },
    {
      "name": "Shared memory",
      "score": 0.09507277607917786
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Meteorology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20089843",
      "name": "Princeton University",
      "country": "US"
    }
  ]
}