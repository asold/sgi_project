{
    "title": "Latent Relation Language Models",
    "url": "https://openalex.org/W2997012196",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2018627362",
            "name": "Hiroaki Hayashi",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2798624852",
            "name": "Zecong Hu",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2226924701",
            "name": "Chenyan Xiong",
            "affiliations": [
                "Microsoft Research (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A277131583",
            "name": "Graham Neubig",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2018627362",
            "name": "Hiroaki Hayashi",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2798624852",
            "name": "Zecong Hu",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2226924701",
            "name": "Chenyan Xiong",
            "affiliations": [
                "Microsoft Research (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A277131583",
            "name": "Graham Neubig",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6720997899",
        "https://openalex.org/W2410217169",
        "https://openalex.org/W2894175714",
        "https://openalex.org/W2086699924",
        "https://openalex.org/W6680532216",
        "https://openalex.org/W2493916176",
        "https://openalex.org/W6748787061",
        "https://openalex.org/W2009169514",
        "https://openalex.org/W2911109671",
        "https://openalex.org/W2798935874",
        "https://openalex.org/W2612773933",
        "https://openalex.org/W2141608913",
        "https://openalex.org/W2304113845",
        "https://openalex.org/W6754324986",
        "https://openalex.org/W6666761814",
        "https://openalex.org/W2740663516",
        "https://openalex.org/W2561658355",
        "https://openalex.org/W2523790121",
        "https://openalex.org/W6697747940",
        "https://openalex.org/W2951048068",
        "https://openalex.org/W2339995566",
        "https://openalex.org/W6727099177",
        "https://openalex.org/W2743945814",
        "https://openalex.org/W2526471240",
        "https://openalex.org/W6607333740",
        "https://openalex.org/W2107598941",
        "https://openalex.org/W2411934291",
        "https://openalex.org/W2798904449",
        "https://openalex.org/W6756040250",
        "https://openalex.org/W2119874156",
        "https://openalex.org/W6640862754",
        "https://openalex.org/W6713098461",
        "https://openalex.org/W2084531783",
        "https://openalex.org/W6755030400",
        "https://openalex.org/W6753728727",
        "https://openalex.org/W7010254504",
        "https://openalex.org/W2963925965",
        "https://openalex.org/W2963631907",
        "https://openalex.org/W2402268235",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2140679639",
        "https://openalex.org/W2889583850",
        "https://openalex.org/W1500117362",
        "https://openalex.org/W2893600504",
        "https://openalex.org/W2963324947",
        "https://openalex.org/W2963824800",
        "https://openalex.org/W2964325845",
        "https://openalex.org/W2963831883",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2964165364",
        "https://openalex.org/W4299838440",
        "https://openalex.org/W2889009749",
        "https://openalex.org/W2519314406",
        "https://openalex.org/W2476140796",
        "https://openalex.org/W2963091658",
        "https://openalex.org/W2963676655",
        "https://openalex.org/W4298422451",
        "https://openalex.org/W2962708992",
        "https://openalex.org/W2251862950",
        "https://openalex.org/W179875071"
    ],
    "abstract": "In this paper, we propose Latent Relation Language Models (LRLMs), a class of language models that parameterizes the joint distribution over the words in a document and the entities that occur therein via knowledge graph relations. This model has a number of attractive properties: it not only improves language modeling performance, but is also able to annotate the posterior probability of entity spans for a given text through relations. Experiments demonstrate empirical improvements over both word-based language models and a previous approach that incorporates knowledge graph information. Qualitative analysis further demonstrates the proposed model's ability to learn to predict appropriate relations in context. 1",
    "full_text": "The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence (AAAI-20)\nLatent Relation Language Models\nHiroaki Hayashi,1∗ Zecong Hu,1∗ Chenyan Xiong,2 Graham Neubig1\n1Carnegie Mellon University, 2Microsoft Research AI\n{hiroakih, zeconghu, gneubig}@cs.cmu.edu, Chenyan.Xiong@microsoft.com\nAbstract\nIn this paper, we propose Latent Relation Language Models\n(LRLMs), a class of language models that parameterizes the\njoint distribution over the words in a document and the en-\ntities that occur therein via knowledge graph relations. This\nmodel has a number of attractive properties: it not only im-\nproves language modeling performance, but is also able to\nannotate the posterior probability of entity spans for a given\ntext through relations. Experiments demonstrate empirical\nimprovements over both word-based language models and\na previous approach that incorporates knowledge graph in-\nformation. Qualitative analysis further demonstrates the pro-\nposed model’s ability to learn to predict appropriate relations\nin context.\n†\n1 Introduction\nLanguage models (LMs) calculate the probability P(X) of\ntextual data X, and are a core model class of interest to NLP.\nLMs are used as testbeds for evaluation of generative models\nof text, and have applications such as rescoring of upstream\nlanguage generation inputs (Sundermeyer, Schl¨uter, and Ney\n2012), grammatical error correction (Felice et al. 2014), or\npre-training of sentence representations (Peters et al. 2018).\nNeural networks are used to model this probability in state-\nof-the-art LMs (Bengio et al. 2003; Mikolov et al. 2010;\nMerity et al. 2017).\nTextual data X comprise a wide variety of words to\nbe modeled, from closed-class function words, to common\nnouns or verbs, to named entities and numbers (Zipf 1949).\nNotably, words on the rarer end of this spectrum are often\nmore semantically or topically important, as evidenced by\nthe success of heuristics such as TF-IDF (Salton and McGill\n1986), which up-weight words with low frequency. Previ-\nous work has noted that while neural LMs greatly outper-\nform alternatives such as n-gram models on frequent words,\nthey often under-perform on these rare words due to their\nlimited parameter budget, which puts them at a disadvan-\ntage compared to non-parametric models like count-based\nCopyright c⃝ 2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.∗Equal Contribution.\n†Code & Data: https://github.com/neulab/lrlm.\n<nationality><position\nheld>\nlawyer\n(“attorney”, ...)\n American\npresident of the United States\nTopic: Barack Obama\n Knowledge Graph\n Article\n<occupation>\nBarack Hussein Obama II (...; born August 4, 1961) is an\nAmerican[nationality] attorney[occupation] and polit-\nician[occupation] who served as the 44th president of the\nUnited States[position held] from 2009 to 2017. ...\npolitician<occupation>\n...\nFigure 1: Overview of our task of language modeling condi-\ntioned on a knowledge graph. For a given topic, we want to\nlearn a language model that leverages the knowledge graph\nthrough relations when modeling the text.\nn-grams (Neubig and Dyer 2016).\nMethods to mitigate this bottleneck have been proposed\nin the context of conditional LMs, which instead model the\nconditional probability P(X|C), where C is some con-\ntext given to the model. For instance, in sequence transduc-\ntion tasks, there are mechanisms to copy from the source\nsequence (Gu et al. 2016) or use word or phrase dictio-\nnaries (Arthur, Neubig, and Nakamura 2016) to improve\nmodeling of low-frequency words. Perhaps more interest-\ning from an LM perspective are methods conditioned on\ninformation from structured knowledge sources such as\nknowledge graphs (Ahn et al. 2016; Parvez et al. 2018;\nLogan et al. 2019), tables (Lebret, Grangier, and Auli 2016),\nor grammars (Konstas and Lapata 2013). These methods are\nanalogous to human language production, where the under-\nlying knowledge is converted into linguistic realizations.\nIn this work, we propose Latent Relation Language Mod-\nels (LRLMs), a class of conditional LMs that takerelational\ninformation between entities in a knowledge graph as con-\ntext. Speciﬁcally, our model is able to generate either words\nfrom a ﬁxed word vocabulary, or a span of words deﬁned\naccording to their relations with a topic entity of interest, as\nshown in Figure 1. The choices of which method of gener-\nation to use is deﬁned as a latent variable sequence Z.W e\n7911\nuse Latent Predictor Networks (LPNs; Ling et al. (2016))\nto jointly learn P(X,Z |C), thus tractably marginalizing\nover all the possible spans. Compared to other word-by-\nword generation methods that condition LMs on knowledge\ngraphs (KGs; Ahn et al. (2016); Wang et al. (2018)), the\nspan-based generation from the KGs alleviates problems of\nmalformed or incomplete mentions. Moreover, the posterior\nprobabilities of Z can be considered as entity links, which\nare of interest in their own right in the information extraction\nﬁeld (Ceccarelli et al. 2013; Ganea and Hofmann 2017).\nWe apply the model on articles from Wikipedia ( X),\nwith the help of relational information ( C) such as Wiki-\ndata (Vrandeˇci´c and Kr¨otzsch 2014) or Freebase (Bollacker\net al. 2008) regarding each article topic. Empirical results\non open vocabulary language modeling show that the pro-\nposed model outperforms previous approaches on the same\ntask, demonstrating that LRLMs provide an effective way to\ncondition on this context. We also demonstrate the merit of\nexplicitly modeling latent relations by examining the poste-\nrior probabilities over the chosen relations Z, which are in\nconcert with human intuitions about how relations are being\nexpressed in the text.\n2 Language Modeling Conditioned on\nStructured Knowledge\nIn this section, we deﬁne the task of open-vocabulary lan-\nguage modeling conditioned on structured data.\nTask Deﬁnition\nKnowledge graphs (KGs) can be represented as a directed\nlabeled graph G =( V,E ) consisting of a set of nodes\nV = {v\n1,...,v |V|} and a set of relation edges E =\n{ei :⟨si,ωi,oi⟩| si,oi ∈ V, ωi ∈ R}. Relation ei contains\nsi, ωi, and oi as the subject, relation type, and object. R\nis the set of all relation types. Each node vi ∈ V rep-\nresents either an entity or an attribute 1, and is associated\nwith a set of surface forms (also called aliases) A(vi)=\n{ai,1,...,a i,|A(vi)|} that can be used to refer to vi.F o r\ninstance in Figure 1, the subject “ Barack Obama” is con-\nnected to both “ politician” and “ lawyer” with the rela-\ntion <occupation>, and the object entity “ politician”\nhas “political figure” and “polit.” as additional\naliases. Notably surface forms of many objects in the KG\ncan be multiple words, and thus it is necessary to have ma-\nchinery to deal with this fact.\nGiven this KG, we further deﬁne a topic entity s about\nwhich we would like to generate a piece of text. Our con-\nditional language modeling problem is then deﬁned as the\nproblem of modeling the conditional probability of text X:\nP(X|G,s). In particular, we consider a subgraph G\n′ =\n(V′,E′) of the original KGGby extracting nodes and edges\ndirectly related to the topic entity s:\nV′ : {s}∪{ oi |⟨s,∗,oi⟩∈ E},\nE′ : {ei :⟨s,ωi,oi⟩|⟨ s,ωi,oi⟩∈ E ∧oi ∈ V′}.\n1A value speciﬁed with a relation from an entity (e.g., dates).\nWe consider an open-vocabulary setting where all word\ntypes within X are incorporated. Perplexity under this set-\nting provides a more realistic measure than under closed-\nvocabulary setting by taking into account words that rarely\nor never appear in the training set, which, as previously\nnoted, are particularly important for conveying the main\ncontent of the text.\nWhy Condition on Knowledge Graphs?\nKGs provide two important beneﬁts for neural LMs. First,\nthe high coverage of rarer words due to entities being often\ninfrequent addresses lack of textual supervision for predict-\ning these words. More importantly, KGs have the potential\nto help LMs generate factually consistent text by providing\nconsistent associations between entities. Normal LMs would\nhave to rely on supervision purely from textual data, which\nmay not provide a learning signal strong enough to accu-\nrately generate these facts. For instance, results from Rad-\nford et al. (2019) show that even with a very large model\ntrained on massive amounts of data, samples can be factu-\nally incorrect, although being ﬂuent and coherent.\n3 Latent Relation Language Models\nIn this setion, we describe our proposed framework of Latent\nRelation Language Models (LRLMs).\nDeﬁnition\nKnowledge from the KG subgraph G′ can be incorporated\ninto generation by copying aliases from related entities into\nthe generated text. For instance in Figure 2, to generate\nObama’s birth date, the model can of course pick words from\nits vocabulary. But it is more straightforward to copy from\nthe <birth date> relation of the topic entity “ Barack\nObama”, which gives the correct birth date.\nHowever, it is insufﬁcient to model probabilities for such\nchoices conditioning only on G\n′ and s, because it is un-\nknown to us which text spans are matched to which rela-\ntions. Na¨ıve solutions like simple text matching algorithms\nwould yield many false positives. For example, “ New Y ork\nCity” has an alias “New York”, which matches “New Y ork”\n(state) and parts of “New Y ork City Council”.\nTo circumvent this lack of relation annotation, we treat\nrelations corresponding to such text spans as latent variables.\nFormally, letX = {x\ni}N\ni=1 be the sequence ofN tokens, and\nZ = {(σt,πt,ρt)}T\nt=1 a sequence of latent variable triplets\ndescribing text span matches:\n• The span variable σt :=( ℓt,rt) speciﬁes a token subse-\nquence xσt = {xi}rt\ni=ℓt\n.\n• The source variable πt ∈{ REL ,WORD } denotes the gen-\neration source of the span xσt .\n• The relation variable ρt :=( et,at) describes the match-\ning relation and surface form of the span xσt , and is only\nused when πt = REL .\nFor Z to be a valid sequence of latent variables, the fol-\nlowing conditions must be satisﬁed:\n• Span variables {σt}T\nt=1 form a segmentation of X, i.e.,\nℓt = rt−1 +1 for t =2 ,...,T . This also implies T ≤ N.\n7912\nRelation\nWord\n……\nBarack Hussein Obama II born August 4 , 1961\n<birth name> <birth date>\n<given name> <family name>\nModel\nGenerated\nText\nborn<s> …Barack Hussein Obama II August 4 , 1961 …\n1x 2x 3x 4x 8x 9x 10x 11x 12x\n1σ 3σ 4σ\n… …\nChosen Span\nPossible Span\nFigure 2: While generating, our model switches between the two sources: “Relation” and “Word”. Circles represent hidden\nstates up to each token, and edges represent possible span matches. Here we show one valid derivation with solid lines, and\nother options as dashed lines. We also show an “annotation” of the generated tokens by the spans and sources we choose.\n• If π\nt = WORD , then ℓt = rt.\n• If πt = REL , then ρt =( et,at) where et = ⟨s,ωt,ot⟩\nshould satisfy et ∈ E′, at ∈A (ot), and xσt = at, i.e., ρt\nmust correspond to a valid surface form of an object that\nis related to the topic entity s and matches the text span.\nLet Z be the set of all valid latent variable sequences. We\ncan now model the probability by marginalizing over Z:\nP(X|G′,s)=\n∑\nZ∈Z\nP(X,Z |G′,s). (1)\nFor sake of brevity, unless noted otherwise, we drop G′ and\ns from the conditions in the following sections.\nTraining\nGiven the latent variable sequence Z, we follow Ling et\nal. (2016) in factoring the joint probability:\nP(X,Z )=\nT∏\nt=1\nP(σt,πt,ρt,xσt |x<ℓt )\n=\nT∏\nt=1\nP(πt |x<ℓt )P(σt,xσt ,ρt |πt,x<ℓt ),\nhere x<i is the sequence of ﬁrst i−1 tokens in X. Figure 2\nshows an example of generation according to this factoriza-\ntion, and Algorithm 1 precisely deﬁnes the process of gen-\nerating at time step t.\nWe marginalize overZ according to Eq 1 and optimize for\nthe marginal likelihood. Since the probability at time steptis\nindependent of previous latent variables, the marginalization\nis tractable using the forward-backward algorithm (Baum\net al. 1970). The forward probability α\ni is deﬁned as the\nmarginal probability of the sequence up to the i-th token\n(speciﬁcally, α0 =1 ), computed as follows:\nαi =\n∑\n(σ:(ℓ,r),π,ρ)∈τi\nαℓ−1P(σ,π,ρ,x σ |x<ℓ),\nwhere τi is deﬁned as the set of valid latent variable tuples\n(σ :( ℓ,r),π,ρ ) such that r = i, i.e., all valid spans ending\nat the i-th token. The marginal probability we optimize for\nis then αN. The backward probability βi which is required\nfor gradient computation can be similarly calculated.\nParameterization\nWe use neural networks to parameterize all probability dis-\ntributions mentioned above. Decisions for time step t are\nbased on aD-dimensional hidden stateh\nℓt . This hidden state\ncan be generated by any neural sequence model, and we ex-\nperiment with multiple models to demonstrate the generality\nof our approach.\nSource Selection Source selection is done using a simple\nlinear model followed by a softmax function applied to the\nlatest word-level hidden state h\nℓt :\nP(πt |x<ℓt )=s o f t m a x (Wπhℓt + bπ),\nwhere Wπ ∈ R2×D,bπ ∈ R2 are trainable parameters.\nWord Generation Like conventional word-level neural\nlanguage models, we have the option to generate the next\ntoken from a ﬁxed vocabulary. This option is used to gener-\nate any word that isn’t part of an object entity participating\nin a relation. The probability is:\nP(x\nℓt |x<ℓt ) = softmax(Linearw(hℓt )),\nwhere Linear(h) is a linear transform with a bottleneck of\ndimension K into a vector over vocabulary size L:\nLinear(h)= W1(W2h + b2)+ b1,\nwhere W1 ∈ RL×K, b1 ∈ RL, W2 ∈ RK×D, b2 ∈ RD\nare trainable parameters. Empirically we found this low-rank\nversion to outperform a full linear transform.\nUnknown Word Generation Since our task is language\nmodeling under an open-vocabulary setting, we must be able\nto generate words even if they are out of vocabulary. Fol-\nlowing Luong and Manning (2016), we do so by having a\ncharacter-level LM “spell-out” any unknown words. If the\nunknown word is x = c\n1 ...c |c| with |c| characters:\nP(x|x<ℓt )= P(<UNK> |x<ℓt )P(c1 ...c |c|; θchar),\nwhere θchar are the parameters of the character LM. We pre-\ntrain this model on the set of all unique words in the training\nset and ﬁx its parameters while training LRLM.\n7913\nAlgorithm 1Generative Process of LRLM\nInput previous span σt−1 =( ℓt−1,rt−1), previously generated tokens x<rt−1 .\nOutput span σt =( ℓt,rt), source πt, relation ρt =( et,at), and token subsequence xσt .\n1: ℓt ← rt−1 +1 ⊿ Update the beginning of span. :1\n2: ˆπt ∼ P(πt |x<ℓt ) ⊿ Choose whether to generate a word or relation. :2\n3: if ˆπt = WORD then ⊿ Generating a word. :3\n4: P(σt,xσt ,ρt |πt = WORD ,x<ℓt ) := P(xℓt |x<ℓt ) ⊿ Simplify the probability. :4\n5: ˆxℓt ∼ P(xℓt |x<ℓt ) ⊿ Choose a word from model vocabulary. :5\n6: if ˆxℓt = <UNK> then\n7: ˆxℓt ∼ P(c1 ...c |c|; θchar) ⊿ Generate a word using a character model. :7\n8: else if ˆxℓt = <EOS> then\n9: End generation.\n10: end if\n11: else if ˆπt = REL then ⊿ Generating a relation. :11\n12: P(σt,xσt ,ρt |πt = REL ,x<ℓt ) := P(et |x<ℓt )P(at |et,x<ℓt ) ⊿ Factor the probability. :12\n13: ˆet ∼ P(et |x<ℓt ) ⊿ Choose a relation. :13\n14: ˆat ∼ P(at |ˆet,x<ℓt ) ⊿ Choose a surface form from the selected relation. :14\n15: ˆxσt ← ˆat ⊿ Generate a phrase. :15\n16: end if\nTable 1: Training set statistics: number of training docu-\nments, vocabulary size, relations per head entity, tokens per\ndocument, and entity mentions per document.\nDataset Doc V ocab Rel/Ent Tok/Doc Ment/Doc\nWikiFacts 7856 40.0k 82.71 157.25 16.04\nWikiText-S 27685 71.1k 11.38 295.75 11.20\nWikiText-F 27685 264k 11.38 3559.91 73.01\nRelation Generation The goal of relation generation is to\nﬁnd the most suitable span that can be copied into the text.\nAs Line 12 of Algorithm 1 depicts, this is factorized into two\nsteps: relation selection and surface form selection.\n• Relation selection. We utilize pre-trained KG embed-\ndings from OpenKE (Han et al. 2018) for entities and re-\nlation types. For a relation e\ni : ⟨s,ωi,oi⟩, we concatenate\nKG embeddings for ωi and oi to obtain the relation em-\nbedding ei.2 We then compute the probability of selecting\neach relation as:\nP(ei |x<ℓt )=s o f t m a x (e⊤\ni Linearo(hℓt )).\n• Surface form selection. We featurize surface forms via\nfastText embeddings (Bojanowski et al. 2017) pre-trained\non the training corpus, and calculate probability of surface\nform a\nk as:\nP(ak |ei,x<ℓt )=s o f t m a x (f⊤\nak (Wahℓt + ba)),\nwhere fak is the fastText embedding for ak and Wa, ba\nare trainable parameters.\n4 Datasets\nWe use two datasets with different characteristics for exper-\niments; statistics are shown in Table 1.\n2We train embeddings for each relation type not covered by pre-\ntrained embeddings, and an UNK embedding for attributes and en-\ntities not covered by pre-trained embeddings.\nWikiFacts\nWikiFacts (Ahn et al. 2016) is a collection of Wikipedia ar-\nticles restricted to /film/actor domain entities in Free-\nbase (Bollacker et al. 2008). 3 Each example consists of the\nﬁrst section of the original article. Since ofﬁcial splits for\nevaluation are not provided, we follow previous work and\nperformed a random split of 80/10/10%.\nThis dataset assumes a single alias for each entity ( i.e.,\n∀o ∈ V\n′; |A(o)| =1 ). Hence, the surface form selection\nmodule acts as oracle, where it always assigns a probability\nof 1 to the correct surface form.\nWikiText\nWhile WikiFacts has been used in previous work on LMs\nusing structured data (Ahn et al. 2016), the domain is lim-\nited. To investigate the capability of knowledge-infused LMs\nin an open-domain setting with a wide variety of relations,\nwe build a large-scale open-domain dataset from the exist-\ning WikiText-103 dataset (Merity et al. 2017) by associating\narticles with entities in Wikidata (Vrande ˇci´c and Kr ¨otzsch\n2014). We employ the same data splits from the original\ndataset. Bridging KGs and the articles from WikiText-103\ninvolves two steps (more details in Appendix A).\n• Constructing subgraphs for articles. As discussed in\nSection 2, we take the original KG and extract a relevant\nsubgraph G\n′ for each article. While there are many op-\ntions on how to extract this subgraph, we choose the sub-\ngraph G\n′consisting of direct neighborsof the topic entity\nfor each article. This forms a star-shaped subgraph, with\nthe topic entity as the central node, connected by the re-\nlated entities and attributes. We found on average 11.38\nneighbors and 3.1 surface forms for each neighbor.\n3The original WikiFacts also includes topic entities from other\narticles linked to the page to be generated. However, these (gold)\nentities are inaccessible when actually attempting to generate new\narticles. We experiment without them, but also report results with\nthem in Appendix C.\n7914\nTable 2: Perplexity values of different models on open vocabulary language modeling, lower is better. Best results are in bold.\nAsterisk symbols represent statistical signiﬁcance according to Wilcoxon signed-rank test (Dror et al. 2018) against the best\nbaseline model, with p< 0.05 (\n∗) and p< 0.01 (∗∗), respectively.\nBase model Dataset Dev Test\nVanilla LM Alias LM NKLM LRLM Vanilla LM Alias LM NKLM LRLM\nLSTM\nWikiFacts 231.03 213.34 96.77 93.55 225.40 207.57 93.18 88.37∗\nWikiText-S 68.37 70.07 46.16 45.84 86.12 87.75 55.98 55.38\nWikiText-F 45.13 46.18 44.46 42.18∗ 49.47 50.88 48.54 45.70∗\nTransformer-XL\nWikiFacts 172.27 158.54 99.46 84.76∗∗ 167.91 154.27 94.36 79.35∗∗\nWikiText-S 42.63 39.65 43.05 37.75∗∗ 52.96 50.60 52.51 44.98∗∗\nWikiText-F 30.14 31.20 32.19 29.56∗∗ 33.01 34.37 35.27 32.20∗∗\n• Linking mentions with the KG.For each object entity in\nG′, we search for occurrences of all surface forms in the\narticle while allowing token overlaps among them. Note\nthat, similarly to distant supervision for relation extrac-\ntion (Mintz et al. 2009), this string-matching process can\nproduce false positive mentions. We rely on our model’s\nability to handle such noisy mentions by learning to as-\nsign high probabilities only on the correct mentions.\nWe name the dataset obtained through this process as\nWikiText-F (Full). We also create WikiText-S (Short) by\nonly using the ﬁrst sections of WikiText-F documents.\n5 Experimental Settings\nIn this section, we explain the evaluation metric, conﬁgura-\ntions, and baseline models compared against LRLM.\nEvaluation Measure\nWe report token-level perplexity under theopen-vocabulary\nsetting. We use pre-trained character-level LMs from Sec-\ntion 3 for each dataset to discount the probability of out-of-\nvocabulary words based on its spelling.\n4 This is done for all\ntested models, both proposed and baselines.\nModel Conﬁguration\nFor WikiFacts, we use a ﬁxed word vocabulary size of\n40,000 following Ahn et al. (2016). For WikiText-derived\ndatasets, we include all words with frequencies no less than\n3 in our dataset following Merity et al. (2017). We use adap-\ntive embeddings (Baevski and Auli 2019) and adaptive soft-\nmax (Grave et al. 2017) to handle large vocabulary.\nTo calculate the hidden stateh\nx<i , we test two varieties of\nneural sequence models: standard LSTMs (Hochreiter and\nSchmidhuber 1997), and the state-of-the-art Transformer-\nXL (Dai et al. 2019). We implement all models in Py-\nTorch (Paszke et al. 2017). Training details and hyperparam-\neters are summarized in Appendix B.\n4This contrasts to UPP (Ueberla 1994), which adjusts likeli-\nhood of OOV words based on a uniform probability equivalent to\nthe size of the vocabulary, which does not actually measure the\nability to generate words outside of training data. Results using\nclosed vocabulary setting or UPP can be found in Appendix C and\nE, respectively.\nBaselines\nWe compare LRLM against three baselines that utilizes in-\nformation from KGs to various degrees.\nVanilla language model (Vanilla LM) This is a stan-\ndard language model baseline that does not condition on\nKGs, such as LSTM (Merity, Keskar, and Socher 2017) or\nTransformer-XL (Dai et al. 2019).\nAlias-prepended language model (Alias LM) The same\nmodel as above, but prepending to the text the concatenated\naliases of all entities in G\n′which appear in the article.5 This\ngives a simple baseline LM conditioned on the KG.\nNeural Knowledge Language Model (NKLM)Similarly\nto LRLM, the Neural Knowledge Language Model (NKLM;\nAhn et al. (2016)) also has the ability to copy from a given\nset of KG triples, but differs from LRLM in several ways:\n1. LRLM allows generation of multi-word entities at once,\nwhile NKLM predicts one word at a time and the model\nneeds to repeatedly predict the right relation until copying\nof an object is done.\n2. LRLM marginalizes over all derivations of a sequence,\nwhich allows processing of overlapped tokens among\nspans, while NKLM makes all decisions in a hard fash-\nion and cannot handle such overlapped tokens.\n6\nThe original NKLM does not differentiate between sur-\nface forms, so we incorporate the same surface form selec-\ntion module as LRLM for fair comparison.\n6 Results and Analysis\nIn this section, we summarize the main results and perform\nanalyses of the learned model.\nMain Results\nPerplexities over the datasets are shown in Table 2. We ob-\nserve that for both sequence models, LRLM outperforms the\nbaselines on all datasets and improvements are more signif-\nicant on the stronger sequence model. Particularly on the\n5This simulates the table-to-text generation setting, where all\ninput entities should be mentioned in the generated sentence.\n6Due to this limitation, we perform additional data preprocess-\ning on WikiText for NKLM, detailed in Appendix D.\n7915\ns = Sonic the Hedgehog (1991 video game)\nSonic the Hedgehog[TITLE] Sonic the Hedgehog[TITLE] Sonic the Hedgehog[TITLE] ( Jordon VS . 䬨䬷\n䭙䴈䬷䭄䬐䬓䬮䴈 , Picardi Sutorīto Faitā Jobs Sutorīto Faitā ) is A[CERO rating] platform video\ngame[instance of] developed by Sonic Team[developer] and published by Sega[publisher] . In 2008 , the\ngame[instance of] was released for Sega[platform] 3[platform] and 1320 platforms in March 2009 ,\nreplacing the original game[instance of] Sony[platform] Drive[platform] counterpart Sonic the\nHedgehog[characters] for the Android[platform] GameCube[platform] . It was re - released on March 12 ,\n2010 , in ...\nNKLM:\nWarm-up: <s> = Sonic the Hedgehog ( 1991 video game ) = \nSonic the Hedgehog[TITLE] ( also known as Sonic the Hedgehog 3 and Sonic[series] the Hedgehog 2 ) is a\n1986 role - playing video game developed by Sonic Team[developer] and published by Sony Computer\nEntertainment ( SEGA[publisher] ) for the PlayStation 3[platform] ( Xbox 360[platform] ) . It was\ndeveloped and published by Sega[publisher] in 1997 for the Wii , and was ported as a third installment in\nthe Sonic the Hedgehog[series] series and released in Japan in 1996 . On the ...\nLRLM:\n(ω, o) = {\n  (<TITLE>, Sonic the Hedgehog (1991 video game)),\n  (<instance of>,  video game),\n  (<CERO rating>,  A),\n  (<developer>,      Sonic Team),\n  (<publisher>,      Sega),\n  (<platform>,        Sega Mega Drive),\n  (<platform>,        Wii),\n  (<platform>,        Nintendo GameCube),\n  (<platform>,        Xbox 360),\n  (<platform>,        Playstation 3),\n  (<platform>,        Android),\n  (<characters>,    Sonic the Hedgehog),\n  (<series>, \n                  Sonic the Hedgehog (video game series)),\n  ...\n}\nSonic the Hedgehog is an action action - adventure video game published by Sonic of programmers for the\n1999 Nintendo GameCube 's SNES video game Sonic the Hedgehog 2 . It was released for the Nintendo DS\non September 16 , 1994 in North America and Europe in the latter part of the original Halo 2 . It was played\nin a post - apocalyptic fantasy fantasy universe , by Nintendo Computer Entertainment on March 6 , 1999\nunder the ...\nVanilla\nLM:\nis the twelfth video game developed and published by EA Sports . It is the sequel to the 1992 Sonic - 6\ngameSonic the Hedgehog for the Nintendo Genesis , created by Sonic system creator Pinball Ka . Doctor the\nHedgehog has since gone on a hiatus in choosing an estimated global community when it quickly becomes a\nlaunch member . The game puts Princess unlock from a mansion , which once everything is devastated by a\nbro and I ...\nAlias\nLM:\nFigure 3: Samples from the models for the topic entity “ Sonic the Hedgehog (1991 video game)” with the corresponding\nsubgraph on the right. Square brackets denote the relation type of copied objects. Highlighted spans in light green are full\nmentions, and those in dark red are partial mentions. Underlined tokens are unknown words sampled from the character model.\ntwo WikiText-derived datasets, our model outperformed the\nsimpler Vanilla LM and Alias LM baselines, while NKLM\nhad difﬁculty utilizing the KGs and in some cases results\nin worse perplexities than these baselines. Alias LM under-\nperformed Vanilla LM in some cases, demonstrating that this\nsimpler and more indirect method of conditioning on the lin-\nearized KG is not sufﬁcient to achieve stable improvements.\nGenerated Samples\nTo illustrate behaviors of the learned models, we take the\nmodels using Transformer-XL trained on WikiText-S, draw\n10 samples while conditioning on G\n′ and s = “Sonic the\nHedgehog”, and show the sample with lowest perplexity in\nFigure 3. We highlight tokens generated by the relation pre-\ndictor and use different colors to represent full and partial\nmentions. A full mention is an identical copy of an entity\nsurface form, while a partial mention is an incomplete sub-\nphrase of an entity surface form. A perfect model should not\ngenerate partial mentions as it leads to possibly corrupted\nphrases, and should generate the same set of full mentions\nas the gold article.\nAlthough NKLM generates more mentions, it suffers\nfrom generating partial mentions because it 1) is unaware of\nthe length of surface forms, and 2) requires making copy de-\ncisions as many times as the surface form lengths. As shown\nin Figure 3, we often observe NKLM repeating the same en-\ntity, or switching entities halfway through ( e.g.,“ Sega 3”).\nIn contrast, LRLM, by design, only generates full mentions.\nWe quantitatively show this in Table 3 by counting the\naverage number of partial and full mentions in samples. We\ntook 10 samples from 10 random topic entities in the devel-\nopment set, and manually annotated “valid” full mentions,\nTable 3: Average number of partially generated, fully gen-\nerated, and valid and invalid full mentions over 100 samples\nfrom the development set or gold human-generated article.\nPartial Full Valid Invalid\nNKLM 16.9 7.81 6.37 1.44\nLRLM − 6.32 5.63 0.69\nGold − 9.00 9.00 0.00\nwhich we deemed as semantically correct based on the sen-\ntential context. NKLM generates more invalid mentions than\nLRLM, most of which are false positives and repetitions\nof the same mention. LRLM has almost no repetitions, but\nsometimes incorrectly predicts the “theme” of the topic en-\ntity, e.g., generating an article about a TV episode for a topic\nentity of a song.\nPosterior Probability of Spans\nOne of the advantages of our model is its capability to calcu-\nlate the posterior probability of a span being generated as a\nrelation in existing text. We calculate the joint probability of\na span (σ =( ℓ,r)) and the surrounding text\n7 by marginaliz-\ning over the latent variable Z for both sides of context, and\nnormalize over all possible spans:\nP(X,Z )= αℓ−1 ·P(Z|x<ℓ) ·βr+1,\nP(Z|X)= P(X,Z ) /\n∑\nZ∈Z\nP(X,Z ),\n7We consider the text segment in the batch where the span ap-\npears as the surrounding text.\n7916\nTable 4: Posterior probability of spans (underlined) in con-\ntexts. word represents word-based generation. The second\nrelation in the last example means generation of “ the” us-\ning word, followed by relation-based generation of “United\nStates” using the <origin> relation.\nTitle: Sorry (Madonna Song)\n... song by American singer Madonna from her tenth ...\nRelations:\n<performer> 0.9697\n<lyrics by> 0.0289\nword 0.0014\n... written and produced by Madonna and Stuart Price , ...\nRelations:\n<performer> 0.1545\n<lyrics by> 0.7693\nword 0.0762\n... continuation from the “ Hung Up ” music video . ...\nRelations: <follows> 1.0000\nword 0.0000\n... . However , in the United States , the song did ...\nRelations:\n<origin> 0.0000\nword →<origin> 0.0003\nword 0.9997\nwhere αi and βi are the forward and backward probabili-\nties computed following Section 3. Table 4 shows spans with\nposterior probabilities of various relation types from an arti-\ncle about “Sorry (Madonna song)”. The model demonstrates\nthe ability to relate the entity “ Madonna” to the topic with\nappropriate relation types based on context. We also observe\nthat the model tends to generate multi-word spans through\nrelations rather than word-by-word from vocabulary. How-\never, our model often favors word-based generation for com-\nmon phrases even if related entities exist.\nEffect of Subgraph Size\nFinally, we measure the performance of models with re-\nspect to the richness of resources available for condition-\ning. We group WikiFacts articles into 10 bins by the num-\nber of relations available, and plot binned word-average log-\nprobabilities in Figure 4. While all models have slightly\nhigher log-probabilities as the number of relations increase,\nLRLM achieves the largest gain.\n7 Related Work\nA variety of entity-aware LMs exist, conditioning on in-\nformation sources such as coreference annotations (Ji et\nal. 2017), entity annotations (Logan et al. 2019), or key-\nwords (Kiddon, Zettlemoyer, and Choi 2016; Parvez et al.\n2018). Among them, NKLM (Ahn et al. 2016) uses rela-\ntional information and is the most relevant. Our proposed\nLRLM formulation is more successful at lowering perplex-\nity and allows calculating posterior probabilities of relations.\nIncorporating KGs for natural language generation (NLG)\nhas a long history (Goldberg, Driedger, and Kittredge 1994;\nReiter et al. 2005; Chen and Mooney 2008). With the\nFigure 4: Word-average log-probabilities on development\nset of WikiFacts grouped by the average number of relations.\nLRLM shows a larger gain over the baselines as the number\nof relations increases.\nrecent advancement of neural sequence modeling, preva-\nlent approaches for language generation from KGs employ\nsequence-to-sequence models with special attention mecha-\nnisms tailored for input structures such as graphs (Wang et\nal. 2018) or tables (Liu et al. 2018). Unlike our focus, how-\never, this class of research focuses on learning discriminative\nmodels that do not explicitly generate the referent entity as\nlatent variables, like we do in Section 6.\nWhile not directly related to our core task, there have been\na number of other methods for incorporating latent variables\ninto NLG problems. Latent structure has included predict-\ning latent sequences of topics (Wiseman, Shieber, and Rush\n2018), chunking of word sequences inton-grams (Buckman\nand Neubig 2018), deciding between input sources (Gu et al.\n2016), or generating compressed summary tokens (Miao and\nBlunsom 2016). Our model borrows its underlying structure\nfrom Ling et al. (2016), who focused on an entirely differ-\nent task of source code generation. We use a similar method\nfor selecting latent sources for Wikipedia article language\nmodeling with a repository of KG triples.\n8 Conclusion\nIn this work, we propose Latent Relation Language Models,\na class of conditional LMs on knowledge graphs which mod-\nels text as a latent sequence of spans matching related enti-\nties in the KG. The generative framework allows the model\nto not only outperform previous work, but also score spans\nwith their posterior relation probability, which can be used\nfor downstream tasks.\nAcknowledgements\nThis research was supported in part by Funai Foundation\nfor Information Technology and Amazon. The authors also\nthank the reviewers and NeuLab members for their helpful\nfeedback, and Qian Wang for the ﬁgure design.\nReferences\nAhn, S.; Choi, H.; P ¨arnamaa, T.; and Bengio, Y . 2016. A neural\nknowledge language model. CoRR arXiv:1608.00318.\n7917\nArthur, P.; Neubig, G.; and Nakamura, S. 2016. Incorporating\ndiscrete translation lexicons into neural machine translation. In\nEMNLP, 1557–1567.\nBaevski, A., and Auli, M. 2019. Adaptive input representations for\nneural language modeling. In ICLR.\nBaum, L. E.; Petrie, T.; Soules, G.; and Weiss, N. 1970. A max-\nimization technique occurring in the statistical analysis of proba-\nbilistic functions of markov chains. The Annals of Mathematical\nStatistics 41(1):164–171.\nBengio, Y .; Ducharme, R.; Vincent, P.; and Jauvin, C. 2003. A\nneural probabilistic language model. JMLR 3(Feb):1137–1155.\nBojanowski, P.; Grave, E.; Joulin, A.; and Mikolov, T. 2017. En-\nriching word vectors with subword information. TACL 5:135–146.\nBollacker, K.; Evans, C.; Paritosh, P.; Sturge, T.; and Taylor, J.\n2008. Freebase: A collaboratively created graph database for struc-\nturing human knowledge. In SIGMOD, 1247–1250.\nBuckman, J., and Neubig, G. 2018. Neural lattice language models.\nTACL 6:529–541.\nCeccarelli, D.; Lucchese, C.; Orlando, S.; Perego, R.; and Trani, S.\n2013. Learning relatedness measures for entity linking. In CIKM,\n139–148.\nChen, D. L., and Mooney, R. J. 2008. Learning to sportscast: A\ntest of grounded language acquisition. In ICML, 128–135.\nDai, Z.; Yang, Z.; Yang, Y .; Carbonell, J.; Le, Q.; and Salakhutdi-\nnov, R. 2019. Transformer-XL: Attentive language models beyond\na ﬁxed-length context. In ACL, 2978–2988.\nDror, R.; Baumer, G.; Shlomov, S.; and Reichart, R. 2018. The\nhitchhiker’s guide to testing statistical signiﬁcance in natural lan-\nguage processing. In ACL, 1383–1392.\nFelice, M.; Yuan, Z.; Andersen, Ø. E.; Yannakoudakis, H.; and\nKochmar, E. 2014. Grammatical error correction using hybrid\nsystems and type ﬁltering. In CoNLL, 15–24.\nGanea, O.-E., and Hofmann, T. 2017. Deep joint entity disam-\nbiguation with local neural attention. In EMNLP, 2619–2629.\nGoldberg, E.; Driedger, N.; and Kittredge, R. I. 1994. Using\nnatural-language processing to produce weather forecasts. IEEE\nExpert 9(2):45–53.\nGrave, ´E.; Joulin, A.; Ciss´e, M.; Grangier, D.; and J´egou, H. 2017.\nEfﬁcient softmax approximation for GPUs. In ICML, volume 70\nof Proceedings of Machine Learning Research, 1302–1310.\nGu, J.; Lu, Z.; Li, H.; and Li, V . O. 2016. Incorporating copying\nmechanism in sequence-to-sequence learning. InACL, 1631–1640.\nHan, X.; Cao, S.; Lv, X.; Lin, Y .; Liu, Z.; Sun, M.; and Li, J. 2018.\nOpenKE: An open toolkit for knowledge embedding. In EMNLP,\n139–144.\nHochreiter, S., and Schmidhuber, J. 1997. Long short-term mem-\nory. Neural Computation 9(8):1735–1780.\nJi, Y .; Tan, C.; Martschat, S.; Choi, Y .; and Smith, N. A. 2017.\nDynamic entity representations in neural language models. In\nEMNLP, 1830–1839.\nKiddon, C.; Zettlemoyer, L.; and Choi, Y . 2016. Globally coherent\ntext generation with neural checklist models. InEMNLP, 329–339.\nKonstas, I., and Lapata, M. 2013. A global model for concept-to-\ntext generation. Journal of Artiﬁcial Intelligence Research48:305–\n346.\nLebret, R.; Grangier, D.; and Auli, M. 2016. Neural text generation\nfrom structured data with application to the biography domain. In\nEMNLP, 1203–1213.\nLing, W.; Blunsom, P.; Grefenstette, E.; Hermann, K. M.; Koˇcisk´y,\nT.; Wang, F.; and Senior, A. 2016. Latent predictor networks for\ncode generation. In ACL, 599–609.\nLiu, T.; Wang, K.; Sha, L.; Chang, B.; and Sui, Z. 2018. Table-to-\ntext generation by structure-aware seq2seq learning. AAAI.\nLogan, R.; Liu, N. F.; Peters, M. E.; Gardner, M.; and Singh, S.\n2019. Barack’s wife Hillary: Using knowledge graphs for fact-\naware language modeling. In ACL, 5962–5971.\nLuong, M.-T., and Manning, C. D. 2016. Achieving open vocabu-\nlary neural machine translation with hybrid word-character models.\nIn ACL, 1054–1063.\nMerity, S.; Xiong, C.; Bradbury, J.; and Socher, R. 2017. Pointer\nsentinel mixture models. In ICLR.\nMerity, S.; Keskar, N. S.; and Socher, R. 2017. Regularizing and\noptimizing LSTM language models. CoRR arXiv:1708.02182.\nMiao, Y ., and Blunsom, P. 2016. Language as a latent variable:\nDiscrete generative models for sentence compression. In EMNLP,\n319–328.\nMikolov, T.; Karaﬁ´at, M.; Burget, L.; ˇCernock`y, J.; and Khudanpur,\nS. 2010. Recurrent neural network based language model. In\nINTERSPEECH.\nMintz, M.; Bills, S.; Snow, R.; and Jurafsky, D. 2009. Distant\nsupervision for relation extraction without labeled data. In ACL,\n1003–1011.\nNeubig, G., and Dyer, C. 2016. Generalizing and hybridizing\ncount-based and neural language models. In EMNLP, 1163–1172.\nParvez, M. R.; Chakraborty, S.; Ray, B.; and Chang, K.-W. 2018.\nBuilding language models for text with named entities. In ACL,\n2373–2383.\nPaszke, A.; Gross, S.; Chintala, S.; Chanan, G.; Yang, E.; DeVito,\nZ.; Lin, Z.; Desmaison, A.; Antiga, L.; and Lerer, A. 2017. Auto-\nmatic differentiation in PyTorch.\nPeters, M.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee,\nK.; and Zettlemoyer, L. 2018. Deep contextualized word represen-\ntations. In NAACL, 2227–2237.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language models are unsupervised multitask\nlearners. Preprint.\nReiter, E.; Sripada, S.; Hunter, J.; Yu, J.; and Davy, I. 2005. Choos-\ning words in computer-generated weather forecasts. Artiﬁcial In-\ntelligence 167(1-2):137–169.\nSalton, G., and McGill, M. J. 1986. Introduction to Modern Infor-\nmation Retrieval. New York, NY , USA: McGraw-Hill, Inc.\nSundermeyer, M.; Schl ¨uter, R.; and Ney, H. 2012. LSTM neural\nnetworks for language modeling. In INTERSPEECH.\nUeberla, J. 1994. Analysing a simple language model · some gen-\neral conclusions for language models for speech recognition.Com-\nputer Speech & Language8(2):153–176.\nVrandeˇci´c, D., and Kr ¨otzsch, M. 2014. Wikidata: A free collabo-\nrative knowledgebase. Communications of the ACM57(10):78–85.\nWang, Q.; Pan, X.; Huang, L.; Zhang, B.; Jiang, Z.; Ji, H.; and\nKnight, K. 2018. Describing a knowledge base. In INLG, 10–21.\nWiseman, S.; Shieber, S.; and Rush, A. 2018. Learning neural\ntemplates for text generation. In EMNLP, 3174–3187.\nZipf, G. K. 1949. Human behavior and the principle of least effort:\nAn introduction to human eoclogy. Addison-Wesley Press.\n7918"
}