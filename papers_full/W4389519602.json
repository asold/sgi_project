{
  "title": "GPT4All: An Ecosystem of Open Source Compressed Language Models",
  "url": "https://openalex.org/W4389519602",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5093241510",
      "name": "Yuvanesh Anand",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3159726619",
      "name": "Zach Nussbaum",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5093241512",
      "name": "Adam Treat",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2118127078",
      "name": "Aaron Miller",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097950935",
      "name": "Richard Guo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2109286178",
      "name": "Benjamin Schmidt",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2792523659",
      "name": "Brandon Duderstadt",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2982368201",
      "name": "Andriy Mulyar",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4367000491",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4362655426",
    "https://openalex.org/W4385572634",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4380353763"
  ],
  "abstract": "Yuvanesh Anand, Zach Nussbaum, Adam Treat, Aaron Miller, Richard Guo, Benjamin Schmidt, Brandon Duderstadt, Andriy Mulyar. Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023). 2023.",
  "full_text": "Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023), pages 59–64\nDecember 6, 2023 ©2023 Association for Computational Linguistics\nGPT4All: An Ecosystem of Open Source Compressed Language Models\nYuvanesh Anand\nNomic AI\nyuvanesh@nomic.ai\nZach Nussbaum\nNomic AI\nzach@nomic.ai\nAdam Treat\nNomic AI\nadam@nomic.ai\nAaron Miller\nNomic AI\naaron@nomic.ai\nRichard Guo\nNomic AI\nrichard@nomic.ai\nBen Schmidt\nNomic AI\nben@nomic.ai\nGPT4All Community\nPlanet Earth\nBrandon Duderstadt∗\nNomic AI\nbrandon@nomic.ai\nAndriy Mulyar∗\nNomic AI\nandriy@nomic.ai\nAbstract\nLarge language models (LLMs) have recently\nachieved human-level performance on a range\nof professional and academic benchmarks. The\naccessibility of these models has lagged behind\ntheir performance. State-of-the-art LLMs re-\nquire costly infrastructure, are only accessible\nvia rate-limited, geo-locked, and censored web\ninterfaces, and lack publicly available code and\ntechnical reports.\nIn this paper, we tell the story of GPT4All, a\npopular open source repository that aims to\ndemocratize access to LLMs. We outline the\ntechnical details of the original GPT4All model\nfamily, as well as the evolution of the GPT4All\nproject from a single model into a fully fledged\nopen source ecosystem. It is our hope that\nthis paper acts as both a technical overview of\nthe original GPT4All models as well as a case\nstudy on the subsequent growth of the GPT4All\nopen source ecosystem.\n1 Introduction\nOn March 14 2023, OpenAI released GPT-4, a large\nlanguage model capable of achieving human level per-\nformance on a variety of professional and academic\nbenchmarks. Despite the popularity of the release,\nthe GPT-4 technical report (OpenAI, 2023) contained\nvirtually no details regarding the architecture, hard-\nware, training compute, dataset construction, or training\nmethod used to create the model. Moreover, users could\nonly access the model through the internet interface at\nchat.openai.com, which was severely rate limited and\nunavailable in several locales (e.g. Italy) (BBC News,\n2023). Additionally, GPT-4 refused to answer a wide\n∗ Shared Senior Authorship\nvariety of queries, responding only with the now infa-\nmous \"As an AI Language Model, I cannot...\" prefix\n(Vincent, 2023). These transparency and accessibility\nconcerns spurred several developers to begin creating\nopen source large language model (LLM) alternatives.\nSeveral grassroots efforts focused on fine tuning Meta’s\nopen code LLaMA model (Touvron et al., 2023; McMil-\nlan, 2023), whose weights were leaked on BitTorrent\nless than a week prior to the release of GPT-4 (Verge,\n2023). GPT4All started as one of these variants.\nIn this paper, we tell the story of GPT4All. We com-\nment on the technical details of the original GPT4All\nmodel (Anand et al., 2023), as well as the evolution of\nGPT4All from a single model to an ecosystem of several\nmodels. We remark on the impact that the project has\nhad on the open source community, and discuss future\ndirections. It is our hope that this paper acts as both a\ntechnical overview of the original GPT4All models as\nwell as a case study on the subsequent growth of the\nGPT4All open source ecosystem.\n2 The Original GPT4All Model\n2.1 Data Collection and Curation\nTo train the original GPT4All model, we collected\nroughly one million prompt-response pairs using the\nGPT-3.5-Turbo OpenAI API between March 20, 2023\nand March 26th, 2023. In particular, we gathered GPT-\n3.5-Turbo responses to prompts of three publicly avail-\nable datasets: the unified chip2 subset of LAION OIG,\na random sub-sample of Stackoverflow Questions, and\na sub-sample of Bigscience/P3 (Sanh et al., 2021). Fol-\nlowing the approach in Stanford Alpaca (Taori et al.,\n2023), an open source LLaMA variant that came just be-\nfore GPT4All, we focused substantial effort on dataset\ncuration.\nThe collected dataset was loaded into Atlas (Nomic,\n2023)—a visual interface for exploring and tagging mas-\nsive unstructured datasets —for data curation. Using At-\n59\nlas, we identified and removed subsets of the data where\nGPT-3.5-Turbo refused to respond, had malformed out-\nput, or produced a very short response. This resulted in\nthe removal of the entire Bigscience/P3 subset of our\ndata, as many P3 prompts induced responses that were\nsimply one word. After curation, we were left with a set\nof 437,605 prompt-response pairs, which we visualize\nin Figure 1a.\n2.2 Model Training\nThe original GPT4All model was a fine tuned variant\nof LLaMA 7B. In order to train it more efficiently, we\nfroze the base weights of LLaMA, and only trained a\nsmall set of LoRA (Hu et al., 2021) weights during the\nfine tuning process. Detailed model hyper-parameters\nand training code can be found in our associated code\nrepository1.\n2.3 Model Access\nWe publicly released all data, training code, and model\nweights for the community to build upon. Further, we\nprovided a 4-bit quantized version of the model, which\nenabled users to run it on their own commodity hard-\nware without transferring data to a 3rd party service.\nOur research and development costs were dominated\nby ∼$800 in GPU spend (rented from Lambda Labs and\nPaperspace) and ∼$500 in OpenAI API spend. Our final\nGPT4All model could be trained in about eight hours\non a Lambda Labs DGX A100 8x 80GB for a total cost\nof ∼$100.\n2.4 Model Evaluation\nWe performed a preliminary evaluation of our model\nusing the human evaluation data from the Self Instruct\npaper (Wang et al., 2023). We reported the ground truth\nperplexity of our model against what was, to our knowl-\nedge, the best openly available alpaca-lora model at the\ntime, provided by user chainyo on HuggingFace. Both\nmodels had very large perplexities on a small number of\ntasks, so we reported perplexities clipped to a maximum\nof 100. We found that GPT4All produces stochastically\nlower ground truth perplexities than alpaca-lora (Anand\net al., 2023).\n3 From a Model to an Ecosystem\n3.1 GPT4All-J: Repository Growth and the\nimplications of the LLaMA License\nThe GPT4All repository grew rapidly after its release,\ngaining over 20000 GitHub stars in just one week, as\nshown in Figure 2. This growth was supported by an\nin-person hackathon hosted in New York City three days\nafter the model release, which attracted several hundred\nparticipants. As the Nomic discord, the home of online\ndiscussion about GPT4All, ballooned to over 10000\npeople, one thing became very clear - there was massive\ndemand for a model that could be used commercially.\n1https://github.com/nomic-ai/gpt4all\nThe LLaMA model that GPT4All was based on was\nlicensed for research only, which severely limited the\nset of domains that GPT4All could be applied in. As\na response to this, the Nomic team repeated the model\ntraining procedure of the original GPT4All model, but\nbased on the already open source and commercially li-\ncensed GPT-J model (Wang and Komatsuzaki, 2021).\nGPT4All-J also had an augmented training set, which\ncontained multi-turn QA examples and creative writing\nsuch as poetry, rap, and short stories. The creative writ-\ning prompts were generated by filling in schemas such\nas \"Write a [CREATIVE STORY TYPE] about [NOUN]\nin the style of [PERSON].\" We again employed Atlas\nto curate the prompt-response pairs in this data set.\nOur evaluation methodology also evolved as the\nproject grew. In particular, we began evaluating\nGPT4All models using a suite of seven reasoning\ntasks that were used for evaluation of the Databricks\nDolly (Conover et al., 2023b) model, which was re-\nleased on April 12, 2023. Unfortunately, GPT4All-J did\nnot outperform other prominent open source models on\nthis evaluation. As a result, we endeavoured to create a\nmodel that did.\n3.2 GPT4All-Snoozy: the Emergence of the\nGPT4All Ecosystem\nGPT4All-Snoozy was developed using roughly the same\nprocedure as the previous GPT4All models, but with a\nfew key modifications. First, GPT4All-Snoozy used the\nLLaMA-13B base model due to its superior base metrics\nwhen compared to GPT-J. Next, GPT4All-Snoozy incor-\nporated the Dolly’s training data into its train mix. After\ndata curation and deduplication with Atlas, this yielded\na training set of 739,259 total prompt-response pairs.\nWe dubbed the model that resulted from training on this\nimproved dataset GPT4All-Snoozy. As shown in Figure\n1, GPT4All-Snoozy had the best average score on our\nevaluation benchmark of any model in the ecosystem at\nthe time of its release.\nConcurrently with the development of GPT4All, sev-\neral organizations such as LMSys, Stability AI, BAIR,\nand Databricks built and deployed open source language\nmodels. We heard increasingly from the community that\nthey wanted quantized versions of these models for local\nuse. As we realized that organizations with ever more\nresources were developing source language models, we\ndecided to pivot our effort away from training increas-\ningly capable models and towards providing easy access\nto the plethora of models being produced by the open\nsource community. Practically, this meant spending our\ntime compressing open source models for use on com-\nmodity hardware, providing stable and simple high level\nmodel APIs, and supporting a GUI for no code model\nexperimentation.\n3.3 The Current State of GPT4All\nToday, GPT4All is focused on improving the accessi-\nbility of open source language models. The repository\n60\n(a)\n (b)\n (c)\n (d)\nFigure 1: TSNE visualizations showing the progression of the GPT4All train set. Panel (a) shows the original\nuncurated data. The red arrow denotes a region of highly homogeneous prompt-response pairs. The coloring denotes\nwhich open dataset contributed the prompt. Panel (b) shows the original GPT4All data after curation. This panel,\nas well as panels (c) and (d) are 10 colored by topic, which Atlas automatically extracts. Notice that the large\nhomogeneous prompt-response blobs no longer appearl. Panel (c) shows the GPT4All-J dataset. The \"starburst\"\nclusters introduced on the right side of the panel correspond to the newly added creative data. Panel (d) shows\nthe final GPT4All-snoozy dataset. All datasets have been released to the public, and can be interactively explored\nonline. In the web version of this article, you can click on a panel to be taken to its interactive visualization.\nModel BoolQ PIQA HellaSwag WinoG. ARC-e ARC-c OBQA Avg.\nGPT4All-J 6B v1.0* 73.4 74.8 63.4 64.7 54.9 36 40.2 58.2\nGPT4All-J v1.1-breezy* 74 75.1 63.2 63.6 55.4 34.9 38.4 57.8\nGPT4All-J v1.2-jazzy* 74.8 74.9 63.6 63.8 56.6 35.3 41 58.6\nGPT4All-J v1.3-groovy* 73.6 74.3 63.8 63.5 57.7 35 38.8 58.1\nGPT4All-J Lora 6B* 68.6 75.8 66.2 63.5 56.4 35.7 40.2 58.1\nGPT4All LLaMa Lora 7B* 73.1 77.6 72.1 67.8 51.1 40.4 40.2 60.3\nGPT4All 13B snoozy* 83.3 79.2 75 71.3 60.9 44.2 43.4 65.3\nGPT4All Falcon 77.6 79.8 74.9 70.1 67.9 43.4 42.6 65.2\nNous-Hermes (Nous-Research, 2023b) 79.5 78.9 80 71.9 74.2 50.9 46.4 68.8\nNous-Hermes2 (Nous-Research, 2023c) 83.9 80.7 80.1 71.3 75.7 52.1 46.2 70.0\nNous-Puffin (Nous-Research, 2023d) 81.5 80.7 80.4 72.5 77.6 50.7 45.6 69.9\nDolly 6B* (Conover et al., 2023a) 68.8 77.3 67.6 63.9 62.9 38.7 41.2 60.1\nDolly 12B* (Conover et al., 2023b) 56.7 75.4 71 62.2 64.6 38.5 40.4 58.4\nAlpaca 7B* (Taori et al., 2023) 73.9 77.2 73.9 66.1 59.8 43.3 43.4 62.5\nAlpaca Lora 7B* (Wang, 2023) 74.3 79.3 74 68.8 56.6 43.9 42.6 62.8\nGPT-J* 6.7B (Wang and Komatsuzaki, 2021) 65.4 76.2 66.2 64.1 62.2 36.6 38.2 58.4\nLLama 7B* (Touvron et al., 2023) 73.1 77.4 73 66.9 52.5 41.4 42.4 61.0\nLLama 13B* (Touvron et al., 2023) 68.5 79.1 76.2 70.1 60 44.6 42.2 63.0\nPythia 6.7B* (Biderman et al., 2023) 63.5 76.3 64 61.1 61.3 35.2 37.2 56.9\nPythia 12B* (Biderman et al., 2023) 67.7 76.6 67.3 63.8 63.9 34.8 38 58.9\nFastchat T5* (Zheng et al., 2023) 81.5 64.6 46.3 61.8 49.3 33.3 39.4 53.7\nFastchat Vicuña* 7B (Zheng et al., 2023) 76.6 77.2 70.7 67.3 53.5 41.2 40.8 61.0\nFastchat Vicuña 13B* (Zheng et al., 2023) 81.5 76.8 73.3 66.7 57.4 42.7 43.6 63.1\nStableVicuña RLHF* (Stability-AI, 2023) 82.3 78.6 74.1 70.9 61 43.5 44.4 65.0\nStableLM Tuned* (Stability-AI, 2023) 62.5 71.2 53.6 54.8 52.4 31.1 33.4 51.3\nStableLM Base* (Stability-AI, 2023) 60.1 67.4 41.2 50.1 44.9 27 32 46.1\nKoala 13B* (Geng et al., 2023) 76.5 77.9 72.6 68.8 54.3 41 42.8 62.0\nOpen Assistant Pythia 12B* 67.9 78 68.1 65 64.2 40.4 43.2 61.0\nMosaic MPT7B (MosaicML-Team, 2023) 74.8 79.3 76.3 68.6 70 42.2 42.6 64.8\nMosaic mpt-instruct (MosaicML-Team, 2023) 74.3 80.4 77.2 67.8 72.2 44.6 43 65.6\nMosaic mpt-chat (MosaicML-Team, 2023) 77.1 78.2 74.5 67.5 69.4 43.3 44.2 64.9\nWizard 7B (Xu et al., 2023) 78.4 77.2 69.9 66.5 56.8 40.5 42.6 61.7\nWizard 7B Uncensored (Xu et al., 2023) 77.7 74.2 68 65.2 53.5 38.7 41.6 59.8\nWizard 13B Uncensored (Xu et al., 2023) 78.4 75.5 72.1 69.5 57.5 40.4 44 62.5\nGPT4-x-Vicuna-13b (Nous-Research, 2023a) 81.3 75 75.2 65 58.7 43.9 43.6 63.2\nFalcon 7b (Almazrouei et al., 2023) 73.6 80.7 76.3 67.3 71 43.3 44.4 65.2\nFalcon 7b instruct (Almazrouei et al., 2023) 70.9 78.6 69.8 66.7 67.9 42.7 41.2 62.5\ntext-davinci-003 88.1 83.8 83.4 75.8 83.9 63.9 51.0 75.7\nTable 1: Evaluations of all language models in the GPT4All ecosystem as of August 1, 2023. Code models are not\nincluded. OpenAI’s text-davinci-003 is included as a point of comparison. The best overall performing model in the\nGPT4All ecosystem, Nous-Hermes2, achieves over 92% of the average performance of text-davinci-003. Models\nmarked with an asterisk were available in the ecosystem as of the release of GPT4All-Snoozy. Note that at release,\nGPT4All-Snoozy had the best average performance of any model in the ecosystem. Bolded numbers indicate the\nbest performing model as of August 1, 2023.\n61\nFigure 2: Comparison of the github start growth of GPT4All, Meta’s LLaMA, and Stanford’s Alpaca. We conjecture\nthat GPT4All achieved and maintains faster ecosystem growth due to the focus on access, which allows more users\nto meaningfully participate.\nprovides compressed versions of open source models\nfor use on commodity hardware, stable and simple high\nlevel model APIs, and a GUI for no code model ex-\nperimentation. The project continues to increase in\npopularity, and as of August 1 2023, has garnered over\n50000 GitHub stars and over 5000 forks.\nGPT4All currently provides native support and\nbenchmark data for over 35 models (see Figure 1), and\nincludes several models co-developed with industry part-\nners such as Replit and Hugging Face. GPT4All also\nprovides high level model APIs in languages includ-\ning Python, Typescript, Go, C#, and Java, among oth-\ners. Furthermore, the GPT4All no code GUI currently\nsupports the workflows of over 50000 monthly active\nusers, with over 25% of users coming back to the tool\nevery day of the week. (Note that all GPT4All user\ndata is collected on an opt inbasis.) GPT4All has be-\ncome the top language model integration in the popular\nopen source AI orchestration library LangChain (Chase,\n2022), and powers many popular open source projects\nsuch as PrivateGPT (imartinez, 2023), Quiver (StanGi-\nrard, 2023), and MindsDB (MindsDB, 2023), among\nothers. GPT4All is the 3rd fastest growing GitHub\nrepository of all time (Leo, 2023), and is the 185th most\npopular repository on the platform, by star count.\n4 The Future of GPT4All\nIn the future, we will continue to grow GPT4All, sup-\nporting it as the de facto solution for LLM accessibil-\nity. Concretely, this means continuing to compress and\ndistribute important open-source language models de-\nveloped by the community, as well as compressing and\ndistributing increasingly multimodal AI models. Fur-\nthermore, we will expand the set of hardware devices\nthat GPT4All models run on, so that GPT4All models\n“just work\" on any machine, whether it comes equipped\nwith Apple Metal silicon, NVIDIA, AMD, or other edge-\naccelerated hardware. Overall, we envision a world\nwhere anyone, anywhere, with any machine, can access\nand contribute to the cutting edge of AI.\nLimitations\nBy enabling access to large language models, the\nGPT4All project also inherits many of the ethical con-\ncerns associated with generative models. Principal\namong these is the concern that unfiltered language\nmodels like GPT4All enable malicious users to generate\ncontent that could be harmful and dangerous (e.g., in-\nstructions on building bioweapons). While we recognize\nthis risk, we also acknowledge the risk of concentrating\nthis technology in the hands of a limited number of in-\ncreasingly secretive research groups. We believe that\nthe risk of concentrating the benefits of language model\ntechnology in the hands of a small number of people\nsignificantly outweighs the risk of misuse, and hence\nwe prefer to make the technology as widely available as\npossible.\nFinally, we realize the challenge in assigning credit\nfor large-scale open source initiatives. We make a first\nattempt at fair credit assignment by explicitly includ-\ning the GPT4All open source developers as authors on\nthis work, but recognize that this is insufficient fully\ncharacterize everyone involved in the GPT4All effort.\nFurthermore, we acknowledge the difficulty in citing\nopen source works that do not necessarily have standard-\nized citations, and do our best in this paper to provide\nURLs to projects whenever possible. We encourage\nfurther research in the area of open source credit as-\nsignment, and hope to be able to support some of this\nresearch ourselves in the future.\n62\nReferences\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-\nshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\nMerouane Debbah, Etienne Goffinet, Daniel Hes-\nlow, Julien Launay, Quentin Malartic, Badreddine\nNoune, Baptiste Pannier, and Guilherme Penedo.\n2023. Falcon-40B: an open large language model\nwith state-of-the-art performance.\nYuvanesh Anand, Zach Nussbaum, Brandon Duder-\nstadt, Benjamin Schmidt, and Andriy Mulyar. 2023.\nGpt4all: Training an assistant-style chatbot with\nlarge scale data distillation from gpt-3.5-turbo.\nhttps://github.com/nomic-ai/gpt4all.\nBBC News. 2023. Chatgpt banned in italy over privacy\nconcerns. BBC News.\nStella Biderman, Hailey Schoelkopf, Quentin An-\nthony, Herbie Bradley, Kyle O’Brien, Eric Hal-\nlahan, Mohammad Aflah Khan, Shivanshu Puro-\nhit, USVSN Sai Prashanth, Edward Raff, Aviya\nSkowron, Lintang Sutawika, and Oskar van der Wal.\n2023. Pythia: A suite for analyzing large language\nmodels across training and scaling.\nHarrison Chase. 2022. langchain. https://github.\ncom/langchain-ai/langchain.\nMike Conover, Matt Hayes, Ankit Mathur, Xiangrui\nMeng, Jianwei Xie, Jun Wan, Ali Ghodsi, Patrick\nWendell, and Matei Zaharia. 2023a. Hello dolly:\nDemocratizing the magic of chatgpt with open mod-\nels.\nMike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,\nJun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell,\nMatei Zaharia, and Reynold Xin. 2023b. Free dolly:\nIntroducing the world’s first truly open instruction-\ntuned llm.\nXinyang Geng, Arnav Gudibande, Hao Liu, Eric Wal-\nlace, Pieter Abbeel, Sergey Levine, and Dawn Song.\n2023. Koala: A dialogue model for academic re-\nsearch. Blog post.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021. Lora: Low-rank adaptation of\nlarge language models.\nimartinez. 2023. privategpt. https://github.com/\nimartinez/privateGPT.\nOscar Leo. 2023. GitHub: The Fastest Growing Repos-\nitories of All Time.\nRobert McMillan. 2023. A meta platforms leak put\npowerful ai in the hands of everyone. The Wall\nStreet Journal.\nMindsDB. 2023. Mindsdb. https://github.com/\nmindsdb/mindsdb. GitHub repository.\nMosaicML-Team. 2023. Introducing mpt-7b: A new\nstandard for open-source, commercially usable llms.\nAccessed: 2023-08-07.\nNomic. 2023. Atlas. https://atlas.nomic.ai/.\nNous-Research. 2023a. gpt4-x-vicuna-13b.\nhttps://huggingface.co/NousResearch/\ngpt4-x-vicuna-13b . Model on Hugging Face.\nNous-Research. 2023b. Nous-hermes-13b.\nhttps://huggingface.co/NousResearch/\nNous-Hermes-13b. Model on Hugging Face.\nNous-Research. 2023c. Nous-hermes-llama-2-7b.\nhttps://huggingface.co/NousResearch/\nNous-Hermes-llama-2-7b . Model on Hugging\nFace.\nNous-Research. 2023d. Redmond-puffin-13b.\nhttps://huggingface.co/NousResearch/\nRedmond-Puffin-13B. Model on Hugging Face.\nOpenAI. 2023. Gpt-4 technical report.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\nManan Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla,\nTaewoon Kim, Gunjan Chhablani, Nihal Nayak,\nDebajyoti Datta, Jonathan Chang, Mike Tian-Jian\nJiang, Han Wang, Matteo Manica, Sheng Shen,\nZheng Xin Yong, Harshit Pandey, Rachel Bawden,\nThomas Wang, Trishala Neeraj, Jos Rozen, Ab-\nheesht Sharma, Andrea Santilli, Thibault Fevry, Ja-\nson Alan Fries, Ryan Teehan, Stella Biderman, Leo\nGao, Tali Bers, Thomas Wolf, and Alexander M.\nRush. 2021. Multitask prompted training enables\nzero-shot task generalization.\nStability-AI. 2023. Stablelm. https://github.com/\nStability-AI/StableLM. GitHub repository.\nStanGirard. 2023. quivr. https://github.com/\nStanGirard/quivr. GitHub repository.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin,\nEdouard Grave, and Guillaume Lample. 2023.\nLlama: Open and efficient foundation language\nmodels.\nThe Verge. 2023. Meta’s powerful ai language model\nhas leaked online — what happens now? The Verge.\nJames Vincent. 2023. As an ai generated language\nmodel: The phrase that shows how ai is polluting\nthe web. The Verge.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-6B:\nA 6 Billion Parameter Autoregressive Language\nModel. https://github.com/kingoflolz/\nmesh-transformer-jax.\n63\nEric J. Wang. 2023. alpaca-lora. https://github.\ncom/tloen/alpaca-lora. GitHub repository.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2023. Self-instruct: Aligning lan-\nguage models with self-generated instructions.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,\nPu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. 2023. Wizardlm: Empowering large language\nmodels to follow complex instructions.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\nllm-as-a-judge with mt-bench and chatbot arena.\n64",
  "topic": "Open source",
  "concepts": [
    {
      "name": "Open source",
      "score": 0.7096737027168274
    },
    {
      "name": "Open source software",
      "score": 0.6833000183105469
    },
    {
      "name": "Miller",
      "score": 0.5860909819602966
    },
    {
      "name": "Computer science",
      "score": 0.5369199514389038
    },
    {
      "name": "Software",
      "score": 0.43049055337905884
    },
    {
      "name": "Natural language processing",
      "score": 0.4074964225292206
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36858612298965454
    },
    {
      "name": "Cognitive science",
      "score": 0.35722169280052185
    },
    {
      "name": "Linguistics",
      "score": 0.3274557590484619
    },
    {
      "name": "Programming language",
      "score": 0.31929588317871094
    },
    {
      "name": "Philosophy",
      "score": 0.3159332275390625
    },
    {
      "name": "Ecology",
      "score": 0.18315234780311584
    },
    {
      "name": "Psychology",
      "score": 0.16076722741127014
    },
    {
      "name": "Biology",
      "score": 0.07868671417236328
    }
  ]
}