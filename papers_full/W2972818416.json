{
  "title": "A Comparative Study on Transformer vs RNN in Speech Applications",
  "url": "https://openalex.org/W2972818416",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2636132906",
      "name": "Shigeki Karita",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117872926",
      "name": "Nanxin Chen",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2162255116",
      "name": "Tomoki Hayashi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1988701769",
      "name": "Takaaki Hori",
      "affiliations": [
        "Mitsubishi Electric (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2553817423",
      "name": "Hirofumi Inaguma",
      "affiliations": [
        "Kyoto University"
      ]
    },
    {
      "id": "https://openalex.org/A2138368223",
      "name": "Ziyan Jiang",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2972639726",
      "name": "Masao Someki",
      "affiliations": [
        "Nagoya University"
      ]
    },
    {
      "id": "https://openalex.org/A3020487139",
      "name": "Nelson Enrique Yalta Soplin",
      "affiliations": [
        "Waseda University"
      ]
    },
    {
      "id": "https://openalex.org/A2123816768",
      "name": "Ryuichi Yamamoto",
      "affiliations": [
        "Line Corporation (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A2108704591",
      "name": "Xiaofei Wang",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2117472677",
      "name": "Shinji Watanabe",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2403859495",
      "name": "Takenori Yoshimura",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2973076897",
      "name": "Wangyou Zhang",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2636132906",
      "name": "Shigeki Karita",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117872926",
      "name": "Nanxin Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2162255116",
      "name": "Tomoki Hayashi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1988701769",
      "name": "Takaaki Hori",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2553817423",
      "name": "Hirofumi Inaguma",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2138368223",
      "name": "Ziyan Jiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2972639726",
      "name": "Masao Someki",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3020487139",
      "name": "Nelson Enrique Yalta Soplin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2123816768",
      "name": "Ryuichi Yamamoto",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108704591",
      "name": "Xiaofei Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117472677",
      "name": "Shinji Watanabe",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2403859495",
      "name": "Takenori Yoshimura",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2973076897",
      "name": "Wangyou Zhang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2799473636",
    "https://openalex.org/W1526236009",
    "https://openalex.org/W2884797218",
    "https://openalex.org/W2559809918",
    "https://openalex.org/W6601563604",
    "https://openalex.org/W6691509046",
    "https://openalex.org/W2166637769",
    "https://openalex.org/W2767017151",
    "https://openalex.org/W6629717138",
    "https://openalex.org/W2963242190",
    "https://openalex.org/W6713762819",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2085628288",
    "https://openalex.org/W179875071",
    "https://openalex.org/W6739366949",
    "https://openalex.org/W2767052532",
    "https://openalex.org/W2605131327",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W6745289305",
    "https://openalex.org/W6763832098",
    "https://openalex.org/W6736996214",
    "https://openalex.org/W6752527106",
    "https://openalex.org/W2962778134",
    "https://openalex.org/W2024490156",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W6679434410",
    "https://openalex.org/W2327501763",
    "https://openalex.org/W2964243274",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W2886180730",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W6757079273",
    "https://openalex.org/W2901607128",
    "https://openalex.org/W6749669830",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W2962780374",
    "https://openalex.org/W6631362777",
    "https://openalex.org/W2796108585",
    "https://openalex.org/W2786835190",
    "https://openalex.org/W6775053297",
    "https://openalex.org/W2799800213",
    "https://openalex.org/W2972389417",
    "https://openalex.org/W6600284362",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W2963418779",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3101648800",
    "https://openalex.org/W2963609956",
    "https://openalex.org/W2250357346",
    "https://openalex.org/W2101045344",
    "https://openalex.org/W3012492057",
    "https://openalex.org/W2951418500",
    "https://openalex.org/W3103005696",
    "https://openalex.org/W3104636952",
    "https://openalex.org/W4297747548",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2963499433",
    "https://openalex.org/W2970730223",
    "https://openalex.org/W6908809",
    "https://openalex.org/W2404126548",
    "https://openalex.org/W37526647",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W2765486990",
    "https://openalex.org/W2407080277",
    "https://openalex.org/W2627092829",
    "https://openalex.org/W2809456172",
    "https://openalex.org/W2903739847",
    "https://openalex.org/W2944255943",
    "https://openalex.org/W2943845043",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2946200149"
  ],
  "abstract": "Sequence-to-sequence models have been widely used in end-to-end speech\\nprocessing, for example, automatic speech recognition (ASR), speech translation\\n(ST), and text-to-speech (TTS). This paper focuses on an emergent\\nsequence-to-sequence model called Transformer, which achieves state-of-the-art\\nperformance in neural machine translation and other natural language processing\\napplications. We undertook intensive studies in which we experimentally\\ncompared and analyzed Transformer and conventional recurrent neural networks\\n(RNN) in a total of 15 ASR, one multilingual ASR, one ST, and two TTS\\nbenchmarks. Our experiments revealed various training tips and significant\\nperformance benefits obtained with Transformer for each task including the\\nsurprising superiority of Transformer in 13/15 ASR benchmarks in comparison\\nwith RNN. We are preparing to release Kaldi-style reproducible recipes using\\nopen source and publicly available datasets for all the ASR, ST, and TTS tasks\\nfor the community to succeed our exciting outcomes.\\n",
  "full_text": "A COMPARATIVE STUDY ON TRANSFORMER VS RNN IN SPEECH APPLICATIONS\nShigeki Karita1,\n(Alphabetical Order) Nanxin Chen3, Tomoki Hayashi5,6, Takaaki Hori7, Hirofumi Inaguma8, Ziyan Jiang3,\nMasao Someki5, Nelson Enrique Yalta Soplin2, Ryuichi Yamamoto4, Xiaofei Wang3, Shinji Watanabe3,\nTakenori Yoshimura5,6, Wangyou Zhang9\n1NTT Communication Science Laboratories, 2Waseda University,3Johns Hopkins University,\n4LINE Corporation, 5Nagoya University, 6Human Dataware Lab. Co., Ltd.,\n7Mitsubishi Electric Research Laboratories, 8Kyoto University,9Shanghai Jiao Tong University\nABSTRACT\nSequence-to-sequence models have been widely used in end-to-\nend speech processing, for example, automatic speech recognition\n(ASR), speech translation (ST), and text-to-speech (TTS). This\npaper focuses on an emergent sequence-to-sequence model called\nTransformer, which achieves state-of-the-art performance in neural\nmachine translation and other natural language processing applica-\ntions. We undertook intensive studies in which we experimentally\ncompared and analyzed Transformer and conventional recurrent\nneural networks (RNN) in a total of 15 ASR, one multilingual ASR,\none ST, and two TTS benchmarks. Our experiments revealed vari-\nous training tips and signiﬁcant performance beneﬁts obtained with\nTransformer for each task including the surprising superiority of\nTransformer in 13/15 ASR benchmarks in comparison with RNN.\nWe are preparing to release Kaldi-style reproducible recipes using\nopen source and publicly available datasets for all the ASR, ST, and\nTTS tasks for the community to succeed our exciting outcomes.\nIndex Terms— Transformer, Recurrent Neural Networks,\nSpeech Recognition, Text-to-Speech, Speech Translation\n1. INTRODUCTION\nTransformer is a sequence-to-sequence (S2S) architecture originally\nproposed for neural machine translation (NMT) [1] that rapidly\nreplaces recurrent neural networks (RNN) in natural language pro-\ncessing tasks. This paper provides intensive comparisons of its\nperformance with that of RNN for speech applications; automatic\nspeech recognition (ASR), speech translation (ST), and text-to-\nspeech (TTS).\nOne of the major difﬁculties when applying Transformer to\nspeech applications is that it requires more complex conﬁgurations\n(e.g., optimizer, network structure, data augmentation) than the con-\nventional RNN based models. Our goal is to share our knowledge\non the use of Transformer in speech tasks so that the community can\nfully succeed our exciting outcomes with reproducible open source\ntools and recipes.\nCurrently, existing Transformer-based speech applications [2]–\n[4] still lack an open source toolkit and reproducible experiments\nwhile previous studies in NMT [5], [6] provide them. Therefore, we\nwork on an open community-driven project for end-to-end speech\napplications using both Transformer and RNN by following the\nsuccess of Kaldi for hidden Markov model (HMM)-based ASR [7].\nSpeciﬁcally, our experiments provide practical guides for tuning\nTransformer in speech tasks to achieve state-of-the-art results.\nIn our speech application experiments, we investigate several\naspects of Transformer and RNN-based systems. For example, we\nmeasure the word/character/regression error from the ground truth,\ntraining curve, and scalability for multiple GPUs.\nThe contributions of this work are:\n•We conduct a larges-scale comparative study on Transformer\nand RNN with signiﬁcant performance gains especially for the\nASR related tasks.\n•We explain our training tips for Transformer in speech applica-\ntions: ASR, TTS and ST.\n•We provide reproducible end-to-end recipes and models pre-\ntrained on a large number of publicly available datasets in our\nopen source toolkit ESPnet [8]1.\nRelated studies\nAs Transformer was originally proposed as an NMT system [1], it\nhas been widely studied on NMT tasks including hyperparameter\nsearch [9], parallelism implementation [5] and in comparison with\nRNN [10]. On the other hand, speech processing tasks have just pro-\nvided their preliminary results in ASR [2], [11], ST [3] and TTS [4].\nTherefore, this paper aims to gather the previous basic research and\nto explore wider topics (e.g., accuracy, speed, training tips) in our\nexperiments.\n2. SEQUENCE-TO-SEQUENCE RNN\n2.1. Uniﬁed formulation for S2S\nS2S is a variant of neural networks that learns to transform a source\nsequence X to a target sequence Y [12]. In Fig. 1, we illustrate a\ncommon S2S structure for ASR, TTS and ST tasks. S2S consists of\ntwo neural networks: an encoder\nX0 = EncPre(X), (1)\nXe = EncBody(X0), (2)\nand a decoder\nY0[1 :t−1] =DecPre(Y[1 :t−1]), (3)\nYd[t] =DecBody(Xe,Y0[1 :t−1]), (4)\nYpost[1 :t] =DecPost(Yd[1 :t]), (5)\nwhere Xis the source sequence (e.g., a sequence of speech features\n(for ASR and ST) or characters (for TTS)), eis the number of layers\n1https://github.com/espnet/espnet\narXiv:1909.06317v2  [cs.CL]  28 Sep 2019\nBi-directional\nRNN / Self Attention\nSource Attention\n+\nUni-directional\nRNN / Self Attention\nASR/ST: Linear (CE)\nTTS: Post-net\nASR: CE, CTC\nST: CE\nTTS: L1, L2, BCE\nASR/ST: Subsample\nTTS: Pre-net\nASR/ST: Embed\nTTS: Pre-net\nSource\nSequence\nTarget\nSequence\nASR: Linear (CTC)\n\u0000× ×\u0000\nLoss\n\u0000\n\u00000\n\u0000\u0000\n[1: \u0000−1]\u00000\n[1: \u0000−1]\u0000\u0000\n[\u0000]\u0000post\nEncPre DecPre\nEncBody DecBody\nDecPost\nEncoder Decoder\n\u0000[1: \u0000−1]\n\u0000[\u0000]\nFig. 1. Sequence-to-sequence architecture in speech applications.\nin EncBody, dis the number of layers in DecBody,tis a target frame\nindex, and all the functions in the above equations are implemented\nby neural networks. For the decoder input Y[1 : t−1], we use a\nground-truth preﬁx in the training stage, while we use a generated\npreﬁx in the decoding stage. During training, the S2S model learns\nto minimize the scalar loss value\nL= Loss(Ypost,Y ) (6)\nbetween the generated sequence Ypost and the target sequence Y.\nThe remainder of this section describes RNN-based univer-\nsal modules: “EncBody” and “DecBody”. We regard “EncPre”,\n“DecPre”, “DecPost” and “Loss” as task-speciﬁc modules and we\ndescribe them in the later sections.\n2.2. RNN encoder\nEncBody(·) in Eq. (2) transforms a source sequence X0 into an in-\ntermediate sequence Xe. Existing RNN-based EncBody (·) imple-\nmentations [13]–[15] typically adopt a bi-directional long short-term\nmemory (BLSTM) that can perform such an operation thanks to its\nrecurrent connection. For ASR, an encoded sequenceXe can also be\nused for source-level frame-wise prediction using connectionist tem-\nporal classiﬁcation (CTC) [16] for joint training and decoding [17].\n2.3. RNN decoder\nDecBody(·) in Eq. (4) generates a next target frame with the en-\ncoded sequence Xe and the preﬁx of target preﬁx Y0[1 : t−1].\nFor sequence generation, the decoder is mostly unidirectional. For\nexample, uni-directional LSTM with an attention mechanism [13]\nis often used in RNN-based DecBody (·) implementations. That at-\ntention mechanism emits source frame-wise weights to sum the en-\ncoded source frames Xe as a target frame-wise vector to be trans-\nformed with the preﬁx Y0[0 :t−1]. We refer to this type of attention\nas “encoder-decoder attention”.\n3. TRANSFORMER\nTransformer learns sequential information via a self-attention mech-\nanism instead of the recurrent connection employed in RNN. This\nsection describes the self-attention based modules in Transformer in\ndetail.\n3.1. Multi-head attention\nTransformer consists of multiple dot-attention layers [18]:\natt(Xq,Xk,Xv) = softmax\n(XqXk⊤\n√\ndatt\n)\nXv, (7)\nwhere Xk,Xv ∈Rnk×datt\nand Xq ∈Rnq×datt\nare inputs for this\nattention layer, datt is the number of feature dimensions, nq is the\nlength of Xq, and nk is the length ofXk and Xv. We refer toXqXk⊤\nas the “attention matrix”. Vaswani et al. [1] considered these inputs\nXq,Xk and Xv to be a query and a set of key-value pairs, respec-\ntively.\nIn addition, to allow the model to deal with multiple attentions\nin parallel, Vaswani et al. [1] extended this attention layer in Eq. (7)\nto multi-head attention (MHA):\nMHA(Q,K,V ) = [H1,H2,...,H dhead ]Whead, (8)\nHh = att(QWq\nh,KW k\nh,VW v\nh), (9)\nwhere K,V ∈ Rnk×datt\nand Q ∈ Rnq×datt\nare inputs for\nthis MHA layer, Hh ∈ Rnq×datt\nis the h-th attention layer\noutput ( h = 1 ,...,d head), Wq\nh,Wk\nh,Wv\nh ∈ Rdatt×datt\nand\nWhead ∈Rdattdhead×datt\nare learnable weight matrices and dhead\nis the number of attentions in this layer.\n3.2. Self-attention encoder\nWe deﬁne Transformer-based EncBody(·) used for Eq. (2) unlike the\nRNN encoder in Section 2.2 as follows:\nX′\ni = Xi + MHAi(Xi,Xi,Xi),\nXi+1 = X′\ni + FFi(X′\ni), (10)\nwhere i = 0,...,e −1 is the index of encoder layers, and FFi is\nthe i-th two-layer feedforward network:\nFF(X[t]) = ReLU(X[t]Wff\n1 + bff\n1)Wff\n2 + bff\n2, (11)\nwhere X[t] ∈ Rdatt\nis the t-th frame of the input sequence X,\nWff\n1 ∈Rdatt×dff\n,Wff\n2 ∈Rdff ×datt\nare learnable weight matrices,\nand bff\n1 ∈Rdff\n,bff\n2 ∈Rdatt\nare learnable bias vectors. We refer to\nMHAi(Xi,Xi,Xi) in Eq. (10) as “self attention”.\n3.3. Self-attention decoder\nTransformer-based DecBody(·) used for Eq. (4) consists of two at-\ntention modules:\nYj[t]′= Yj[t] + MHAself\nj (Yj[t],Yj[1 :t],Yj[1 :t]),\nY′′\nj = Yj + MHAsrc\nj (Y′\nj ,Xe,Xe),\nYj+1 = Y′′\nj + FFj(Y′′\nj ), (12)\nwhere j = 0,...,d −1 is the index of the decoder layers. We refer\nto the attention matrix between the decoder input and the encoder\noutput in MHAsrc\nj (Y′\nj ,Xe,Xe) as “encoder-decoder attention’ as\nsame as the one in RNN in Sec 2.3. Because the unidirectional de-\ncoder is useful for sequence generation, its attention matrices at the\nt-th target frame are masked so that they do not connect with future\nframes later than t. This masking of the sequence can be done in\nparallel using an elementwise product with a triangular binary ma-\ntrix. Because it requires no sequential operation, it provides a faster\nimplementation than RNN.\n3.4. Positional encoding\nTo represent the time location in the non-recurrent model, Trans-\nformer adopts sinusoidal positional encoding:\nPE[t] =\n{\nsin t\n10000t/datt if tis even,\ncos t\n10000t/datt if tis odd. (13)\nThe input sequencesX0,Y0 are concatenated with(PE[1],PE[2],... )\nbefore EncBody(·) and DecBody(·) modules.\n4. ASR EXTENSIONS\nIn our ASR framework, the S2S predicts a target sequenceY of char-\nacters or SentencePiece [19] from an input sequence Xfbank of log-\nmel ﬁlterbank speech features.\n4.1. ASR encoder architecture\nThe source Xin ASR is represented as a sequence of 83-dim log-mel\nﬁlterbank frames with pitch features [20]. First, EncPre (·) trans-\nforms the source sequence X into a subsampled sequence X0 ∈\nRnsub×datt\nby using two-layer CNN with 256 channels, stride size 2\nand kernel size 3 in [2], or VGG-like max pooling in [21], where\nnsub is the length of the output sequence of the CNN. This CNN\ncorresponds to EncPre(·) in Eq. (1). Then, EncBody (·) transforms\nX0 into a sequence of encoded features Xe ∈Rnsub×datt\nfor the\nCTC and decoder networks.\n4.2. ASR decoder architecture\nThe decoder network receives the encoded sequenceXe and the pre-\nﬁx of a target sequence Y[1 : t−1] of token IDs: characters or\nSentencePiece [19]. First, DecPre (·) in Eq. (3) embeds the tokens\ninto learnable vectors. Next, DecBody (·) and single-linear layer\nDecPost(·) predicts the posterior distribution of the next token pre-\ndiction Ypost[t] given Xe and Y[1 :t−1].\n4.3. ASR training and decoding\nDuring ASR training, both the decoder and the CTC module pre-\ndict the frame-wise posterior distribution of Y given corresponding\nsource X: ps2s(Y|X) and pctc(Y|X), respectively. We simply use\nthe weighted sum of those negative log likelihood values:\nLASR = −αlog ps2s(Y|X) −(1 −α) logpctc(Y|X), (14)\nwhere αis a hyperparameter.\nIn the decoding stage, the decoder predicts the next token given\nthe speech feature X and the previous predicted tokens using beam\nsearch, which combines the scores of S2S, CTC and the RNN lan-\nguage model (LM) [22] as follows:\nˆY = argmax\nY ∈Y∗\n{λlog ps2s(Y|Xe) + (1−λ) logpctc(Y|Xe)\n+ γlog plm(Y)}, (15)\nwhere Y∗is a set of hypotheses of the target sequence, and γ,λ are\nhyperparameters.\n5. ST EXTENSIONS\nIn ST, S2S receives the same source speech feature and target token\nsequences in ASR but the source and target languages are different.\nIts modules are also deﬁned in the same ways as in ASR. However,\nST cannot cooperate with the CTC module introduced in Section 4.3\nbecause the translation task does not guarantee the monotonic align-\nment of the source and target sequences unlike ASR [23].\n6. TTS EXTENSIONS\nIn the TTS framework, the S2S generates a sequence of log-mel ﬁl-\nterbank features and predicts the probabilities of the end of sequence\n(EOS) given an input character sequence [15].\n6.1. TTS encoder architecture\nThe input of the encoder in TTS is a sequence of IDs corresponding\nto the input characters and the EOS symbol. First, the character\nID sequence is converted into a sequence of character vectors with\nan embedding layer, and then the positional encoding scaled by a\nlearnable scalar parameter is added to the vectors [4]. This process\nis a TTS implementation of EncPre(·) in Eq. (1). Finally, the encoder\nEncBody(·) in Eq. (2) transforms this input sequence into a sequence\nof encoded features for the decoder network.\n6.2. TTS decoder architecture\nThe inputs of the decoder in TTS are a sequence of encoder fea-\ntures and a sequence of log-mel ﬁlterbank features. In training,\nground-truth log-mel ﬁlterbank features are used with an teacher-\nforcing manner while in inference, predicted ones are used with an\nautoregressive manner.\nFirst, the target sequence of 80-dim log-mel ﬁlterbank features\nis converted into a sequence of hidden features by Prenet [15] as a\nTTS implementation of DecPre(·) in Eq. (3). This network consists\nof two linear layers with 256 units, a ReLU activation function, and\ndropout followed by a projection linear layer with datt units. Since\nit is expected that the hidden representations converted by Prenet\nare located in the similar feature space to that of encoder features,\nPrenet helps to learn a diagonal encoder-decoder attention [4]. Then\nthe decoder DecBody (·) in Eq. (4), whose architecture is the same\nas the encoder, transforms the sequence of encoder features and that\nof hidden features into a sequence of decoder features. Two linear\nlayers are applied for each frame of Yd to calculate the target feature\nand the probability of the EOS, respectively. Finally, Postnet [15]\nis applied to the sequence of predicted target features to predict its\ncomponents in detail. Postnet is a ﬁve-layer CNN, each layer of\nwhich is a 1d convolution with 256 channels and a kernel size of\n5 followed by batch normalization, a tanh activation function, and\ndropout. These modules are a TTS implementation of DecPost(·) in\nEq. (5).\n6.3. TTS training and decoding\nIn TTS training, the whole network is optimized to minimize two\nloss functions in TTS; 1) L1 loss for the target features and 2) binary\ncross entropy (BCE) loss for the probability of the EOS. To address\nthe issue of class imbalance in the calculation of the BCE, a constant\nweight (e.g. 5) is used for a positive sample [4].\nAdditionally, we apply a guided attention loss [24] to accelerate\nthe learning of diagonal attention to only the two heads of two layers\nfrom the target side. This is because it is known that the encoder-\ndecoder attention matrices are diagonal in only certain heads of a\nfew layers from the target side [4]. We do not introduce any hyper-\nparameters to balance the three loss values. We simply add them all\ntogether.\nIn inference, the network predicts the target feature of the next\nframe in an autoregressive manner. And if the probability of the EOS\nbecomes higher than a certain threshold (e.g. 0.5), the network will\nstop the prediction.\n7. ASR EXPERIMENTS\n7.1. Dataset\nIn Table 1, we summarize the 15 datasets we used in our ASR\nexperiment. Our experiment covered various topics in ASR in-\ncluding recording (clean, noisy, far-ﬁeld, etc), language (English,\nJapanese, Mandarin Chinese, Spanish, Italian) and size (10 - 960\nhours). Except for JSUT [25] and Fisher-CALLHOME Spanish,\nour data preparation scripts are based on Kaldi’s “s5x” recipe [7].\nTechnically, we tuned all the conﬁgurations (e.g., feature extraction,\nSentencePiece [19], language modeling, decoding, data augmen-\ntation [26], [27]) except for the training stage to their optimum in\nthe existing RNN-based system. We used data augmentation for\nseveral corpora. For example, we applied speed perturbation [27] at\nratio 0.9, 1.0 and 1.1 to CSJ, CHiME4, Fisher-CALLHOME Span-\nish, HKUST, and TED-LIUM2/3, and we also applied SpecAug-\nment [26] to Aurora4, LibriSpeech, TED-LIUM2/3 and WSJ.2\n7.2. Settings\nWe adopted the same architecture for Transformer in [41] ( e =\n12,d = 6,dff = 2048,dhead = 4,datt = 256) for every corpus ex-\ncept for the largest, LibriSpeech ( dhead = 8,datt = 512). For RNN,\nwe followed our existing best architecture conﬁgured on each corpus\nas in previous studies [17], [42].\nTransformer requires a different optimizer conﬁguration from\nRNN because Transformer’s training iteration is eight times faster\nand its update is more ﬁne-grained than RNN. For RNN, we fol-\nlowed existing best systems for each corpus using Adadelta [43] with\nearly stopping. To train Transformer, we basically followed the pre-\nvious literature [2] (e.g., dropout, learning rate, warmup steps). We\ndid not use development sets for early stopping in Transformer. We\nsimply ran 20 – 200 epochs (mostly 100 epochs) and averaged the\nmodel parameters stored at the last 10 epochs as the ﬁnal model.\nWe conducted our training on a single GPU for larger corpora\nsuch as LibriSpeech, CSJ and TED-LIUM3. We also conﬁrmed\nthat the emulation of multiple GPUs using accumulating gradients\nover multiple forward/backward steps [5] could result in similar per-\nformance with those corpora. In the decoding stage, Transformer\nand RNN share the same conﬁguration for each corpus, for example,\nbeam size (e.g., 20 – 40), CTC weight λ(e.g., 0.3), and LM weight\nγ(e.g., 0.3 – 1.0) introduced in Section 4.3.\n2We chose datasets to apply these data augmentation methods by prelim-\ninary experiments with our RNN-based system.\n0 100000 200000 300000 400000 500000\nTime [sec]\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Validation accuracy\nTransformer 1GPU (best acc 0.930060)\nTransformer 2GPU (best acc 0.931801)\nTransformer 4GPU (best acc 0.948303)\nRNN 1GPU (best acc 0.905068)\nRNN 2GPU (best acc 0.885973)\nRNN 4GPU (best acc 0.895651)\nFig. 2. ASR training curve with LibriSpeech dataset. Minibatches\nhad the maximum number of utterances for each models on GPUs.\n7.3. Results\nTable 2 summarizes the ASR results in terms of character/word er-\nror rate (CER/WER) on each corpora. It shows that Transformer\noutperforms RNN on 13/15 corpora in our experiment. Although\nour system has no pronunciation dictionary, part-of-speech tag nor\nalignment-based data cleaning unlike Kaldi, our Transformer pro-\nvides comparable CER/WERs to the HMM-based system, Kaldi on\n7/12 corpora. We conclude that Transformer has ability to outper-\nform the RNN-based end-to-end system and the DNN/HMM-based\nsystem even in low resource (JSUT), large resource (LibriSpeech,\nCSJ), noisy (AURORA4) and far-ﬁeld (REVERB) tasks. Table 3\nalso summarizes the LibriSpeech ASR benchmark with ours and\nother reports because it is one of the most competitive task. Our\ntransformer results are comparable to the best performance in [26],\n[44], [45].\nFig. 2 shows an ASR training curve obtained with multiple\nGPUs on LibriSpeech. We observed that Transformer trained with\na larger minibatch became more accurate while RNN did not. On\nthe other hand, when we use a smaller minibatch for Transformer,\nit typically became under-ﬁtted after the warmup steps. In this task,\nTransformer achieved the best accuracy provided by RNN about\neight times faster than RNN with a single GPU.\n7.4. Discussion\nWe summarize the training tips we observed in our experiment:\n•When Transformer suffers from under-ﬁtting, we recommend\nincreasing the minibatch size because it also results in a faster\ntraining time and better accuracy simultaneously unlike any\nother hyperparameters.\n•The accumulating gradient strategy [5] can be adopted to emu-\nlate the large minibatch if multiple GPUs are unavailable.\n•While dropout did not improve the RNN results, it is essential\nfor Transformer to avoid over-ﬁtting.\n•We tried several data augmentation methods [26], [27]. They\ngreatly improved both Transformer and RNN.\n•The best decoding hyperparameters γ,λ for RNN are generally\nthe best for Transformer.\nTransformer’s weakness is decoding. It is much slower than\nKaldi’s system because the self-attention requires O(n2) in a naive\nimplementation, where nis the speech length. To directly compare\nthe performance with DNN-HMM based ASR systems, we need to\ndevelop a faster decoding algorithm for Transformer.\n8. MULTILINGUAL ASR EXPERIMENTS\nThis section compares the ASR performance of RNN and Trans-\nformer in a multilingual setup given the success of Transformer for\nTable 1. ASR dataset description. Names listed in “test sets” correspond to ASR results in Table 2. We enlarged corpora marked with (*) by\nthe external WSJ train si284 dataset (81 hours).\ndataset language hours speech test sets\nAISHELL [28] zh 170 read dev / test\nAURORA4 [29] (*) en 15 noisy read (dev 0330) A / B / C / D\nCSJ [30] ja 581 spontaneous eval1 / eval2 / eval3\nCHiME4 [31] (*) en 108 noisy far-ﬁeld multi-ch read dt05 simu / dt05real / et05simu / et05real\nCHiME5 [32] en 40 noisy far-ﬁeld multi-ch conversational dev worn / kinect\nFisher-CALLHOME Spanish es 170 telephone conversational dev / dev2 / test / devtest / evltest\nHKUST [33] zh 200 telephone conversational dev\nJSUT [25] ja 10 read (our split)\nLibriSpeech [34] en 960 clean/noisy read dev clean / devother / testclean / testother\nREVERB [35] (*) en 124 far-ﬁeld multi-ch read et near / etfar\nSWITCHBOARD [36] en 260 telephone conversational (eval2000) callhm / swbd\nTED-LIUM2 [37] en 118 spontaneous dev / test\nTED-LIUM3 [38] en 452 spontaneous dev / test\nV oxForge [39] it 16 read (our split)\nWSJ [40] en 81 read dev93 / eval92\nTable 2. ASR results of char/word error rates. Results marked with (*) were evaluated in our environment because the ofﬁcial results were\nnot provided. Kaldi ofﬁcial results were retrieved from the version “c7876a33”.\ndataset token error Kaldi (s5) ESPnet RNN (ours) ESPnet Transformer (ours)\nAISHELL char CER N/A / 7.4 6.8 / 8.0 6.0/ 6.7\nAURORA4 char WER (*) 3.6 / 7.7 / 10.0 / 22.3 3.5 / 6.4 / 5.1 / 12.3 3.3/ 6.0/ 4.5/ 10.6\nCSJ char CER (*) 7.5 / 6.3 / 6.9 6.6 / 4.8 / 5.0 5.7/ 4.1/ 4.5\nCHiME4 char WER 6.8/ 5.6/ 12.1/ 11.4 9.5 / 8.9 / 18.3 / 16.6 9.6 / 8.2 / 15.7 / 14.5\nCHiME5 char WER 47.9/ 81.3 59.3 / 88.1 60.2 / 87.1\nFisher-CALLHOME Spanish char WER N/A 27.9 / 27.8 / 25.4 / 47.2 / 47.927.0/ 26.3/ 24.4/ 45.3/ 46.2\nHKUST char CER 23.7 27.4 23.5\nJSUT char CER N/A 20.6 18.7\nLibriSpeech BPE WER 3.9 / 10.4 / 4.3 / 10.8 3.1 / 9.9 / 3.3 / 10.8 2.2/ 5.6/ 2.6/ 5.7\nREVERB char WER 18.2 / 19.9 24.1 / 27.2 15.5/ 19.0\nSWITCHBOARD BPE WER 18.1/ 8.8 28.5 / 15.6 18.1/ 9.0\nTED-LIUM2 BPE WER 9.0/ 9.0 11.2 / 11.0 9.3 / 8.1\nTED-LIUM3 BPE WER 6.2/ 6.8 14.3 / 15.0 9.7 / 8.0\nV oxForge char CER N/A 12.9 / 12.6 9.4/ 9.1\nWSJ char WER 4.3/ 2.3 7.0 / 4.7 6.8 / 4.4\nTable 3. Comparison of the Librispeech ASR benchmark\ndevclean dev other test clean test other\nRWTH (E2E) [44] 2.9 8.8 3.1 9.8\nRWTH (HMM) [45] 2.3 5.2 2.7 5.7\nGoogle SpecAug. [26] N/A N/A 2.5 5.8\nESPnet Transformer (ours) 2.2 5.6 2.6 5.7\nthe monolingual ASR tasks in the previous section. In accordance\nwith [46], we prepared 10 different languages, namely WSJ (En-\nglish), CSJ (Japanese) [30], HKUST [33] (Mandarin Chinese), and\nV oxForge (German, Spanish, French, Italian, Dutch, Portuguese,\nRussian). The model is based on a single multilingual model, where\nthe parameters are shared across all the languages and whose out-\nput units include the graphemes of all 10 languages (totally 5,297\ngraphemes and special symbols). We used a default setup for both\nRNN and Transformer introduced in Section 7.2 without RNNLM\nshallow fusion [21].\nFigure 3 clearly shows that our Transformer signiﬁcantly outper-\nformed our RNN in 9 languages. It realized a more than 10% relative\nimprovement in 8 languages and with the largest value of 28.0% for\nrelative improvement in V oxForge Italian. When compared with the\nRNN result reported in [46], which used a deeper BLSTM (7 layer)\nand RNNLM, our Transformer still provided superior performance\nin 9 languages. From this result, we can conclude that Transformer\nalso outperforms RNN in multilingual end-to-end ASR.\nCharacter Error Rate (%)0.0\n10.0\n20.0\n30.0\n40.0\n50.0\nGermanEnglishSpanishFrenchItalian\nJapanese\nDutch\nPortuguese\nRussianMandarinAverage\nWatanabe et al. RNN ESPnet RNN (ours) ESPnet Transformer (ours)\nFig. 3. Comparison of multilingual end-to-end ASR with the RNN\nin Watanabe et al. [46], ESPnet RNN, and ESPnet Transformer.\n9. SPEECH TRANSLATION EXPERIMENTS\nOur baseline end-to-end ST RNN is based on [23], which is similar\nto the RNN structure used in our ASR system, but we did not use a\nconvolutional LSTM layer in the original paper. The conﬁguration\nof our ST Transformer was the same as that of our ASR system.\nWe conducted our ST experiment on the Fisher-CALLHOME\nEnglish–Spanish corpus [47]. Our Transformer improved the BLEU\nscore to 17.2 from our RNN baseline BLEU 16.5 on the CALL-\nHOME “evltest” set. While training Transformer, we observed more\nserious under-ﬁtting than with RNN. The solution for this is to\nuse the pretrained encoder from our ASR experiment since the ST\ndataset contains Fisher-CALLHOME Spanish corpus used in our\n0 20000 40000 60000 80000 100000\nTime [sec]\n0.4\n0.6\n0.8\n1.0L1 loss\nTacotron2 validation loss\n(1 GPU, Best loss=0.329)\nTransformer validation loss\n(1 GPU, Best loss=0.320)\nTransformer validation loss\n(3 GPUs, Best loss=0.316)\nFig. 4. TTS training curve on M-AILABS.\n0 20000 40000 60000 80000 100000\nTime [sec]\n0.4\n0.6\n0.8\n1.0L1 loss\nTacotron2 validation loss\n(1 GPU, Best loss=0.390)\nTransformer validation loss\n(3 GPUs, Best loss=0.398)\nFig. 5. TTS training curve on LJSpeech.\nASR experiment.\n10. TTS EXPERIMENTS\n10.1. Settings\nOur baseline RNN-based TTS model is Tacotron 2 [15]. We fol-\nlowed its model and optimizer setting. We reuse existing TTS\nrecipes including those for data preparation and waveform gener-\nation that we conﬁgured to be the best for RNN. We conﬁgured\nour Transformer-based conﬁgurations introduced in Section 3 as\nfollows: e= 6,d = 6,datt = 384,dff = 1536,dhead = 4. The input\nfor both systems was the sequence of characters.\n10.2. Results\nWe compared Transformer and RNN based TTS using two corpora:\nM-AILABS [48] (Italian, 16 kHz, 31 hours) and LJSpeech [49] (En-\nglish, 22 kHz, 24 hours). A single Italian male speaker (Riccardo)\nwas used in the case of M-AILABS. Figures 4 and 5 show train-\ning curves in the two corpora. In these ﬁgures, Transformer and\nRNN provide similar L1 loss convergence. As seen in ASR, we\nobserved that a larger minibatch results in better validation L1 loss\nfor Transformer and faster training, while it has a detrimental effect\non the L1 loss for RNN. We also provide generated speech mel-\nspectrograms in Fig. 6 and 73. We conclude that Transformer-based\nTTS can achieve almost the same performance as RNN-based.\n10.3. Discussion\nOur lessons for training Transformer in TTS are as follows:\n•It is possible to accelerate TTS training by using a large mini-\nbatch as well as ASR if a lot of GPUs are available.\n•The validation loss value, especially BCE loss, could be over-\nﬁtted more easily with Transformer. We recommend monitoring\nattention maps rather than the loss when checking its conver-\ngence.\n•Some heads of attention maps in Transformer are not always\ndiagonal as found with Tacotron 2. We needed to select where\nto apply the guided attention loss [24].\n3Our audio samples generated by Tacotron 2, Transformer, and Fast-\nSpeech are available at https://bit.ly/329gif5\n0\n20\n40\n60\n0\n20\n40\n60\n0 50 100 150 200\nTime [frame]\n0\n20\n40\n60\nFig. 6. Samples of mel-spectrograms on M-AILABs. (top) ground-\ntruth, (middle) Tacotron 2 sample, (bottom) Transformer sample.\nThe input text is “E PERCH `E SUBITO VIENE IN MENTE CHE\nIDDIO NON PU `O AVER FATTO UNA COSA INGIUSTA”.\n0\n20\n40\n60\n0\n20\n40\n60\n0 100 200 300 400 500\nTime [frame]\n0\n20\n40\n60\nFig. 7. Samples of mel-spectrograms on LJSpeech. (top) ground-\ntruth, (middle) Tacotron 2 sample, (bottom) Transformer sample.\nThe input text is “IS NOT CONSISTENT WITH THE STANDARDS\nWHICH THE RESPONSIBILITIES OF THE SECRET SERVICE RE-\nQUIRE IT TO MEET. ”.\n•Decoding ﬁlterbank features with Transformer is also slower\nthan with RNN (6.5 ms vs 78.5 ms per frame, on CPU w/ sin-\ngle thread). We also tried FastSpeech [50], which realizes non-\nautoregressive Transformer-based TTS. It greatly improves the\ndecoding speed (0.6 ms per frame, on CPU w/ single thread) and\ngenerates comparable quality of speech with the autoregressive\nTransformer.\n•A reduction factor introduced in [51] was also effective for\nTransformer. It can greatly reduce training and inference time\nbut slightly degrades the quality.\nAs future work, we need further investigation of the trade off be-\ntween training speed and quality, and the introduction of ASR tech-\nniques (e.g., data augmentation, speech enhancement) for TTS.\n11. SUMMARY\nWe presented a comparative study of Transformer and RNN in\nspeech applications with various corpora, namely ASR (15 mono-\nlingual + one multilingual), ST (one corpus), and TTS (two corpora).\nIn our experiments on these tasks, we obtained the promising results\nincluding huge improvements in many ASR tasks and explained how\nwe improved our models. We believe that the reproducible recipes,\npretrained models and training tips described in this paper will\naccelerate Transformer research directions on speech applications.\n12. REFERENCES\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you\nneed,” in Advances in Neural Information Processing Sys-\ntems 30, 2017, pp. 5998–6008.\n[2] L. Dong, S. Xu, and B. Xu, “Speech-transformer: A no-\nrecurrence sequence-to-sequence model for speech recogni-\ntion,” in ICASSP, 2018, pp. 5884–5888.\n[3] L. Cross Vila, C. Escolano, J. A. R. Fonollosa, and M. R.\nCosta-Juss`a, “End-to-end speech translation with the trans-\nformer,” inProc. IberSPEECH 2018, 2018, pp. 60–63.\n[4] N. Li, S. Liu, Y . Liu, S. Zhao, M. Liu, and M. T. Zhou, “Neu-\nral speech synthesis with transformer network,” inThe AAAI\nConference on Artiﬁcial Intelligence (AAAI), 2019.\n[5] M. Ott, S. Edunov, D. Grangier, and M. Auli, “Scaling neural\nmachine translation,” inProceedings of the Third Conference\non Machine Translation: Research Papers, 2018, pp. 1–9.\n[6] A. Vaswani, S. Bengio, E. Brevdo, F. Chollet, A. Gomez,\nS. Gouws, L. Jones, Ł. Kaiser, N. Kalchbrenner, N. Parmar,\nR. Sepassi, N. Shazeer, and J. Uszkoreit, “Tensor2Tensor for\nneural machine translation,” inProceedings of the 13th Con-\nference of the Association for Machine Translation in the\nAmericas, 2018, pp. 193–199.\n[7] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek,\nN. Goel, M. Hannemann, P. Motlicek, Y . Qian, P. Schwarz,\nJ. Silovsky, G. Stemmer, and K. Vesely, “The Kaldi speech\nrecognition toolkit,” in IEEE 2011 Workshop on Automatic\nSpeech Recognition and Understanding, 2011.\n[8] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y .\nUnno, N. Enrique Yalta Soplin, J. Heymann, M. Wiesner,\nN. Chen, A. Renduchintala, and T. Ochiai, “ESPnet: End-to-\nend speech processing toolkit,” in Proc. Interspeech, 2018,\npp. 2207–2211.\n[9] M. Popel and O. Bojar, “Training tips for the transformer\nmodel,” Prague Bull. Math. Linguistics, vol. 110, pp. 43–70,\n2018.\n[10] S. M. Lakew, M. Cettolo, and M. Federico, “A comparison\nof transformer and recurrent neural networks on multilingual\nneural machine translation,” in Proceedings of the 27th In-\nternational Conference on Computational Linguistics, 2018,\npp. 641–652.\n[11] S. Zhou, L. Dong, S. Xu, and B. Xu, “Syllable-based\nsequence-to-sequence speech recognition with the trans-\nformer in Mandarin Chinese,” in Proc. Interspeech, 2018,\npp. 791–795.\n[12] I. Sutskever, O. Vinyals, and Q. V . Le, “Sequence to sequence\nlearning with neural networks,” inAdvances in Neural Infor-\nmation Processing Systems 27, Z. Ghahramani, M. Welling,\nC. Cortes, N. D. Lawrence, and K. Q. Weinberger, Eds.,\n2014, pp. 3104–3112.\n[13] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine trans-\nlation by jointly learning to align and translate,”International\nConference on Learning Representations, 2015.\n[14] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend and\nspell: A neural network for large vocabulary conversational\nspeech recognition,” ICASSP, vol. 2016-May, pp. 4960–\n4964, 2016.\n[15] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang,\nZ. Chen, Y . Zhang, Y . Wang, R. Ryan, R. A. Saurous, Y .\nAgiomyrgiannakis, and Y . Wu, “Natural TTS synthesis by\nconditioning wavenet on MEL spectrogram predictions,” in\nICASSP, 2018, pp. 4779–4783.\n[16] A. Graves, S. Fern ´andez, F. J. Gomez, and J. Schmidhuber,\n“Connectionist temporal classiﬁcation: Labelling unseg-\nmented sequence data with recurrent neural networks,” in\nICML, ser. ACM International Conference Proceeding Se-\nries, vol. 148, 2006, pp. 369–376.\n[17] T. Hori, J. Cho, and S. Watanabe, “End-to-end speech recog-\nnition with word-based rnn language models,” in 2018 IEEE\nSpoken Language Technology Workshop, SLT 2018, Athens,\nGreece, December 18-21, 2018, 2018, pp. 389–396.\n[18] T. Luong, H. Pham, and C. D. Manning, “Effective ap-\nproaches to attention-based neural machine translation,” in\nProceedings of the 2015 Conference on Empirical Methods\nin Natural Language Processing, 2015, pp. 1412–1421.\n[19] T. Kudo and J. Richardson, “SentencePiece: A simple and\nlanguage independent subword tokenizer and detokenizer for\nneural text processing,” in Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language Processing:\nSystem Demonstrations, 2018, pp. 66–71.\n[20] P. Ghahremani, B. BabaAli, D. Povey, K. Riedhammer, J.\nTrmal, and S. Khudanpur, “A pitch extraction algorithm\ntuned for automatic speech recognition,” in ICASSP, 2014,\npp. 2494–2498.\n[21] T. Hori, S. Watanabe, Y . Zhang, and W. Chan, “Advances\nin joint CTC-attention based end-to-end speech recognition\nwith a deep CNN encoder and RNN-LM,” in Proc. Inter-\nspeech, 2017, pp. 949–953.\n[22] T. Mikolov, M. Karaﬁat, L. Burget, J. Cernocky, and S. Khu-\ndanpur, “Recurrent neural network based language model,”\nProc. Interspeech, pp. 1045–1048, 2010.\n[23] R. J. Weiss, J. Chorowski, N. Jaitly, Y . Wu, and Z. Chen,\n“Sequence-to-sequence models can directly translate foreign\nspeech,” in Proc. Interspeech, 2017, pp. 2625–2629.\n[24] H. Tachibana, K. Uenoyama, and S. Aihara, “Efﬁciently\ntrainable text-to-speech system based on deep convolutional\nnetworks with guided attention,” in ICASSP, IEEE, 2018,\npp. 4784–4788.\n[25] R. Sonobe, S. Takamichi, and H. Saruwatari, “JSUT cor-\npus: Free large-scale japanese speech corpus for end-to-end\nspeech synthesis,” CoRR, vol. abs/1711.00354, 2017.\n[26] D. S. Park, W. Chan, Y . Zhang, C. Chiu, B. Zoph, E. D.\nCubuk, and Q. V . Le, “SpecAugment: A simple data augmen-\ntation method for automatic speech recognition,” in ArXiv,\n2019.\n[27] T. Ko, V . Peddinti, D. Povey, and S. Khudanpur, “Audio aug-\nmentation for speech recognition,” in Sixteenth Annual Con-\nference of the International Speech Communication Associa-\ntion, 2015.\n[28] H. Bu, J. Du, X. Na, B. Wu, and H. Zheng, “AISHELL-\n1: An open-source Mandarin speech corpus and a speech\nrecognition baseline,” in2017 20th Conference of the Orien-\ntal Chapter of the International Coordinating Committee on\nSpeech Databases and Speech I/O Systems and Assessment\n(O-COCOSDA), 2017, pp. 1–5.\n[29] D. Pearce and J. Picone, “Aurora working group: Dsr front\nend lvcsr evaluation au/384/02,” Inst. for Signal & Inform.\nProcess., Mississippi State Univ., Tech. Rep, 2002.\n[30] K. Maekawa, H. Koiso, S. Furui, and H. Isahara, “Sponta-\nneous speech corpus of Japanese,” inProceedings of the Sec-\nond International Conference on Language Resources and\nEvaluation (LREC’00), 2000.\n[31] J. Barker, R. Marxer, E. Vincent, and S. Watanabe, “The third\nCHiME speech separation and recognition challenge: Anal-\nysis and outcomes,” Computer Speech & Language, vol. 46,\npp. 605–626, 2017.\n[32] J. Barker, S. Watanabe, E. Vincent, and J. Trmal, “The\nﬁfth ’CHiME’ speech separation and recognition challenge:\nDataset, task and baselines,” in Proc. Interspeech , 2018,\npp. 1561–1565.\n[33] Y . Liu, P. Fung, Y . Yang, C. Cieri, S. Huang, and D. Graff,\n“HKUST/MTS: A very large scale mandarin telephone\nspeech corpus,” in Chinese Spoken Language Processing ,\n2006, pp. 724–735.\n[34] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-\nriSpeech: An ASR corpus based on public domain audio\nbooks,” in ICASSP, 2015, pp. 5206–5210.\n[35] K. Kinoshita, M. Delcroix, S. Gannot, E. A. P. Habets, R.\nHaeb-Umbach, W. Kellermann, V . Leutnant, R. Maas, T.\nNakatani, B. Raj, A. Sehr, and T. Yoshioka, “The REVERB\nchallenge: A benchmark task for reverberation-robust asr\ntechniques,” in New Era for Robust Speech Recognition:\nExploiting Deep Learning, 2017, pp. 345–354.\n[36] J. Godfrey, E. Holliman, and J. McDaniel, “SWITCH-\nBOARD: Telephone speech corpus for research and de-\nvelopment,” inICASSP, vol. 1, 1992, pp. 517–520.\n[37] A. Rousseau, P. Deleglise, and Y . Esteve, “TED-LIUM: An\nautomatic speech recognition dedicated corpus,” inProceed-\nings of the Eighth International Conference on Language Re-\nsources and Evaluation (LREC’12), 2012.\n[38] F. Hernandez, V . Nguyen, S. Ghannay, N. Tomashenko,\nY . Esteve, O. Jokisch, and R. Potapova, “TED-LIUM 3:\nTwice as much data and corpus repartition for experiments\non speaker adaptation,” in Speech and Computer , 2018,\npp. 198–208.\n[39] VoxForge, http://www.voxforge.org.\n[40] D. B. Paul and J. M. Baker, “The design for the Wall Street\nJournal-based CSR corpus,” in Proceedings of the Work-\nshop on Speech and Natural Language , ser. HLT ’91, 1992,\npp. 357–362.\n[41] S. Karita, N. E. Y . Soplin, S. Watanabe, M. Delcroix, A.\nOgawa, and T. Nakatani, “Improving Transformer-Based\nEnd-to-End Speech Recognition with Connectionist Tempo-\nral Classiﬁcation and Language Model Integration,” in Proc.\nInterspeech, 2019, pp. 1408–1412.\n[42] A. Zeyer, K. Irie, R. Schluter, and H. Ney, “Improved train-\ning of end-to-end attention models for speech recognition,”\nin Proc. Interspeech, 2018, pp. 7–11.\n[43] M. D. Zeiler, “Adadelta: An adaptive learning rate method,”\nCoRR, vol. abs/1212.5701, 2012.\n[44] K. Irie, A. Zeyer, R. Schl ¨uter, and H. Ney, “Language model-\ning with deep transformers,”ArXiv preprint arXiv:1905.04226,\n2019.\n[45] C. L ¨uscher, E. Beck, K. Irie, M. Kitza, W. Michel, A. Zeyer,\nR. Schl ¨uter, and H. Ney, “RWTH ASR systems for Lib-\nriSpeech: Hybrid vs attention-w/o data augmentation,”ArXiv\npreprint arXiv:1905.03072, 2019.\n[46] S. Watanabe, T. Hori, and J. R. Hershey, “Language indepen-\ndent end-to-end architecture for joint language identiﬁcation\nand speech recognition,” in IEEE Automatic Speech Recog-\nnition and Understanding Workshop (ASRU) , IEEE, 2017,\npp. 265–271.\n[47] M. Post, G. Kumar, A. Lopez, D. Karakos, C. Callison-\nBurch, and S. Khudanpur, “Improved speech-to-text transla-\ntion with the Fisher and Callhome Spanish–English speech\ntranslation corpus,” in Proceedings of the International\nWorkshop on Spoken Language Translation (IWSLT), 2013.\n[48] I. Solak, The M-AILABS speech dataset , https://www.\ncaito.de/2019/01/the- m- ailabs- speech-\ndataset/, 2019.\n[49] K. Ito, The LJ Speech dataset , https : / / keithito .\ncom/LJ-Speech-Dataset/, 2017.\n[50] Y . Ren, Y . Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and\nT.-Y . Liu, “FastSpeech: Fast, robust and controllable text to\nspeech,”ArXiv e-prints, arXiv:1905.09263, arXiv:1905.09263,\n2019.\n[51] Y . Wang, R. Skerry-Ryan, D. Stanton, Y . Wu, R. J. Weiss,\nN. Jaitly, Z. Yang, Y . Xiao, Z. Chen, S. Bengio, Q. Le, Y .\nAgiomyrgiannakis, R. Clark, and R. A. Saurous, “Tacotron:\nTowards End-to-End Speech Synthesis,” in Proc. Inter-\nspeech, 2017, pp. 4006–4010.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.828636884689331
    },
    {
      "name": "Computer science",
      "score": 0.7661212682723999
    },
    {
      "name": "Recurrent neural network",
      "score": 0.7158658504486084
    },
    {
      "name": "Machine translation",
      "score": 0.6649037599563599
    },
    {
      "name": "Speech recognition",
      "score": 0.6456910371780396
    },
    {
      "name": "Natural language processing",
      "score": 0.47855645418167114
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4379500448703766
    },
    {
      "name": "Artificial neural network",
      "score": 0.4346567392349243
    },
    {
      "name": "Speech translation",
      "score": 0.41729235649108887
    },
    {
      "name": "Engineering",
      "score": 0.13018199801445007
    },
    {
      "name": "Voltage",
      "score": 0.10307016968727112
    },
    {
      "name": "Electrical engineering",
      "score": 0.06359374523162842
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210159266",
      "name": "Mitsubishi Electric (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I22299242",
      "name": "Kyoto University",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I60134161",
      "name": "Nagoya University",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I150744194",
      "name": "Waseda University",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I4210096607",
      "name": "Line Corporation (Japan)",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    }
  ]
}