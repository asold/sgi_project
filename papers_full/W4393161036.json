{
    "title": "TextGT: A Double-View Graph Transformer on Text for Aspect-Based Sentiment Analysis",
    "url": "https://openalex.org/W4393161036",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2111608295",
            "name": "Shuo Yin",
            "affiliations": [
                "Ocean University of China"
            ]
        },
        {
            "id": "https://openalex.org/A1991417471",
            "name": "Guoqiang Zhong",
            "affiliations": [
                "Ocean University of China"
            ]
        },
        {
            "id": "https://openalex.org/A2111608295",
            "name": "Shuo Yin",
            "affiliations": [
                "Ocean University of China"
            ]
        },
        {
            "id": "https://openalex.org/A1991417471",
            "name": "Guoqiang Zhong",
            "affiliations": [
                "Ocean University of China"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1552217991",
        "https://openalex.org/W4285168678",
        "https://openalex.org/W3100060077",
        "https://openalex.org/W2971821075",
        "https://openalex.org/W4221163422",
        "https://openalex.org/W2757541972",
        "https://openalex.org/W6799772243",
        "https://openalex.org/W6775947557",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2251124635",
        "https://openalex.org/W6787995345",
        "https://openalex.org/W2891778157",
        "https://openalex.org/W4302774380",
        "https://openalex.org/W2079735306",
        "https://openalex.org/W3180659574",
        "https://openalex.org/W2624431344",
        "https://openalex.org/W2465099471",
        "https://openalex.org/W6772452955",
        "https://openalex.org/W2519887557",
        "https://openalex.org/W2900763475",
        "https://openalex.org/W3169575312",
        "https://openalex.org/W4285483923",
        "https://openalex.org/W2784814091",
        "https://openalex.org/W3176719207",
        "https://openalex.org/W2799007071",
        "https://openalex.org/W3183430956",
        "https://openalex.org/W3210828003",
        "https://openalex.org/W3117670243",
        "https://openalex.org/W2740899359",
        "https://openalex.org/W2978508283",
        "https://openalex.org/W3160694286",
        "https://openalex.org/W6691216643",
        "https://openalex.org/W4281706128",
        "https://openalex.org/W6810359552",
        "https://openalex.org/W2923978210",
        "https://openalex.org/W2970748008",
        "https://openalex.org/W2412751481",
        "https://openalex.org/W3035740499",
        "https://openalex.org/W3167287584",
        "https://openalex.org/W6745537798",
        "https://openalex.org/W2296071000",
        "https://openalex.org/W3017464126",
        "https://openalex.org/W2562607067",
        "https://openalex.org/W4226208698",
        "https://openalex.org/W3202465916",
        "https://openalex.org/W2971220558",
        "https://openalex.org/W3100451998",
        "https://openalex.org/W4287854714"
    ],
    "abstract": "Aspect-based sentiment analysis (ABSA) is aimed at predicting the sentiment polarities of the aspects included in a sentence instead of the whole sentence itself, and is a fine-grained learning task compared to the conventional text classification. In recent years, on account of the ability to model the connectivity relationships between the words in one sentence, graph neural networks have been more and more popular to handle the natural language processing tasks, and meanwhile many works emerge for the ABSA task. However, most of the works utilizing graph convolution easily incur the over-smoothing problem, while graph Transformer for ABSA has not been explored yet. In addition, although some previous works are dedicated to using both GNN and Transformer to handle text, the methods of tightly combining graph view and sequence view of text is open to research. To address the above issues, we propose a double-view graph Transformer on text (TextGT) for ABSA. In TextGT, the procedure in graph view of text is handled by GNN layers, while Transformer layers deal with the sequence view, and these two processes are tightly coupled, alleviating the over-smoothing problem. Moreover, we propose an algorithm for implementing a kind of densely message passing graph convolution called TextGINConv, to employ edge features in graphs. Extensive experiments demonstrate the effectiveness of our TextGT over the state-of-the-art approaches, and validate the TextGINConv module. The source code is available at https://github.com/shuoyinn/TextGT.",
    "full_text": "TextGT: A Double-View Graph Transformer on Text for Aspect-Based Sentiment\nAnalysis\nShuo Yin, Guoqiang Zhong*\nCollege of Computer Science and Technology, Ocean University of China\nyinshuo@stu.ouc.edu.cn, gqzhong@ouc.edu.cn\nAbstract\nAspect-based sentiment analysis (ABSA) is aimed at pre-\ndicting the sentiment polarities of the aspects included in a\nsentence instead of the whole sentence itself, and is a ï¬ne-\ngrained learning task compared to the conventional text clas-\nsiï¬cation. In recent years, on account of the ability to model\nthe connectivity relationships between the words in one sen-\ntence, graph neural networks have been more and more pop-\nular to handle the natural language processing tasks, and\nmeanwhile many works emerge for the ABSA task. How-\never, most of the works utilizing graph convolution easily\nincur the over-smoothing problem, while graph Transformer\nfor ABSA has not been explored yet. In addition, although\nsome previous works are dedicated to using both GNN and\nTransformer to handle text, the methods of tightly combining\ngraph view and sequence view of text is open to research. To\naddress the above issues, we propose a double-view graph\nTransformer on text (TextGT) for ABSA. In TextGT, the\nprocedure in graph view of text is handled by GNN lay-\ners, while Transformer layers deal with the sequence view,\nand these two processes are tightly coupled, alleviating the\nover-smoothing problem. Moreover, we propose an algorithm\nfor implementing a kind of densely message passing graph\nconvolution called TextGINConv, to employ edge features in\ngraphs. Extensive experiments demonstrate the effectiveness\nof our TextGT over the state-of-the-art approaches, and vali-\ndate the TextGINConv module. The source code is available\nat https://github.com/shuoyinn/TextGT.\nIntroduction\nAspect-based sentiment analysis (ABSA), as a ï¬ne-grained\nlearning task, has been more and more popular in natural\nlanguage processing (NLP) these years (Wang et al. 2016;\nTang, Qin, and Liu 2016; Sun, Huang, and Qiu 2019). The\ngoal of ABSA is to recognize the sentiment polarities of\nthe aspects in a given sentence like a review. The task can\nbe formalized as follows: Given an n-word sentence S =\n(w1; w2; :::; wn) where wi denotes the i-th word, and a k-\nword aspect Sa = (wa1 ; wa2 ; :::; wak ) \u001aS with aj standing\nfor a word index in S, we need to predict the sentiment po-\nlarity of Sa. Note that a sentence with more than one aspects\ncan be seen as multiple instances each with one aspect. We\n*Corresponding author\nCopyright Â© 2024, Association for the Advancement of Artiï¬cial\nIntelligence (www.aaai.org). All rights reserved.\nBottles    of     wine    are     cheap    and     good\nNNS VBP JJ CC\ncasenmod cop cc\nNN JJ\nnsubj\nconj\nIN\nFigure 1: An example sentence with aspect words high-\nlighted in gold. This text has already been preprocessed by\na dependency parser.\nexemplify the aforementioned task using a sentence with a\n3-word aspect, as shown in Figure 1.\nDue to the great success of deep learning, there are a\nfew approaches to dealing with the ABSA task based on\nrecurrent neural networks (RNNs) or convolutional neural\nnetworks (CNNs) (Dong et al. 2014; V o and Zhang 2015),\nbut they fail to leverage the syntactical information among\nwords. Recently, with the development of graph representa-\ntion learning, research on graph neural networks (GNNs) for\nABSA has been widely used. As shown in Figure 1, the sen-\ntence with an aspect is ï¬rstly parsed by an external depen-\ndency extractor to get the corresponding syntax tree which is\nregarded as a graph with edge features. Then kinds of GNNs\ndesigned in the previous works take the dependency graph\nas the input and make the subsequent word representations\nhave the syntactic structure information.\nHowever, most GNNs are message passing â€œsmoothersâ€\nand have the problem of over-smoothing (Li, Han, and Wu\n2018; Chen et al. 2020; Oono and Suzuki 2020). Hope-\nfully, some previous works (Wu et al. 2021; Chen, Oâ€™Bray,\nand Borgwardt 2022; Rampasek et al. 2022) point out that\ngraph Transformers (GTs) have the potential to alleviate\nover-smoothing. In recent years, several vision Transformers\n(ViTs) utilizing convolution have become the cutting-edge\nmethods (Li et al. 2021b; Guo et al. 2021; Chen et al. 2021;\nPeng et al. 2021; Li et al. 2022), whilst GTs also have been\na popular topic due to their good performance on graph rep-\nresentation learning tasks like node classiï¬cation and link\nprediction (Dwivedi and Bresson 2020; Ying et al. 2021;\nKreuzer et al. 2021).\nAlthough there are many works dedicated to design high-\nperformance graph Transformers, few of them are proposed\nfor NLP tasks but graph learning ones. Our motivation is to\nintroduce a well-performed graph Transformer into ABSA,\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n19404\nas a text representation learning task, and thus alleviate\nthe problem of over-smoothing. Moreover, inspired by the\nway of coupling CNN and Transformer tightly to get well-\nperformed ViTs (Guo et al. 2021; Li et al. 2022), we de-\nvise a graph Transformer constructed with graph convolu-\ntional layers and Transformer layers that appear serially and\none by one alternately. In this way, each block of the result-\ning model is able to learn cohesively in the graph view and\nmeanwhile the sequence view of text.\nOur contributions are listed as follows:\nâ€¢ To tightly combine the two text learning processes in the\ngraph view and the sequence view, we propose a novel\ndouble-view graph Transformer on text called TextGT.\nâ€¢ We propose a new algorithm to implement graph con-\nvolutional modules which densely pass messages con-\nstructed with edge features, and one of such modules\ncalled TextGINConv is speciï¬cally employed as the\ngraph-view operator in our TextGT.\nâ€¢ We have conducted extensive experiments on the ABSA\ntask to demonstrate the effectiveness of TextGT and\nTextGINConv.\nRelated Work\nIn this section, we ï¬rstly review the state-of-the-art GNN\nmethods for ABSA, and then several representative graph\nTransformer works proposed in recent years.\nAspect-Based Sentiment Analysis with GNNs\nWang et al. (2020) propose R-GAT, a tailored graph atten-\ntion network (GAT) (Velickovic et al. 2018) to leverage the\nedge features, and using both the dependency relation at-\ntention and the normal attention parallelly; besides, they re-\nshape and prune the syntax tree before fed to the network.\nDualGCN (Li et al. 2021a) and DGEDT (Tang et al. 2020)\nalso employ the parallel architecture where graph convolu-\ntion and self-attention process the input graph simultane-\nously and then their outputs will be fused by a biafï¬ne mod-\nule. Tian et al. (2021) propose a type-aware GCN (T-GCN)\nwhere they use the relation type matrix and the adjacency\nmatrix to get a weight matrix, by which node representations\nare aggregated. SSEGCN (Zhang, Zhou, and Wang 2022)\nmakes use of aspect-aware attention and designs mask ma-\ntrices based on different distances between nodes in the syn-\ntax tree, and thus both the syntactic information and the se-\nmantic information are taken into account. AG-VSR (Feng\net al. 2022) applies a V AE-like encoder-decoder structure\nas the pooling method to get the variational sentence rep-\nresentation, which is then used to help predict the sen-\ntiment polarity of the aspect. Sentic GCN (Liang et al.\n2021) utilizes SenticNet (Cambria et al. 2010), an addi-\ntional opinion mining tool, to assist GCN in text model-\ning, and therefore it actually introduces external knowledge.\nChen et al. (2022) induce aspect-speciï¬c discrete opinion\ntrees by taking attention scores as syntactic distances, and\nthey use reinforcement learning to train the tree inducer.\nNiu et al. (2022) propose CHGMAN to encode heteroge-\nneous graphs they constructed, and thus the inter-aspect re-\nlationships and aspect-context relationships can be consid-\nered simultaneously. However, the aforementioned works\nare actually GNN methods which have the problem of over-\nsmoothing. Although DGEDT uses Transformer to learn\nlong range dependencies, the biafï¬ne fusion method to com-\nbine parallel graph convolution and self-attention may limit\nthe model performance.\nGraph Transformer\nDwivedi and Bresson (2020) introduce Transformer into\ngraph representation learning, and their graph Transformer\nperforms attention computation only in the neighborhood of\na node instead of across the whole graph. Wu et al. (2021)\nsimply stack a multi-layer Transformer encoder on a multi-\nlayer GNN and thus the resulting model called GraphTrans\ncan have the advantages of both. Graphormer (Ying et al.\n2021) and SAN (Kreuzer et al. 2021) both utilize speciï¬c\ngraph-related positional/structural encodings (PE/SE) and\ntailored attention mechanisms adapted for the graph data.\nChen et al. (2022) interpret a graph Transformer from the\nperspective of kernel function and make their model (called\nSAT) aware of the structural information of graphs. Ram-\npasek et al. (2022) summarize many kinds of PE/SE of graph\nTransformer and propose GraphGPS, a general framework\nof GTs widely applied to graph prediction tasks. Never-\ntheless, the models mentioned above are designed mainly\nfor graph learning tasks, like node classiï¬cation and graph\nclassiï¬cation for molecules, while graph Transformers for\nABSA in NLP has not been specially explored yet.\nAnalysis on the Over-Smoothing Problem\nThereinafter, we use graph convolution to denote any lo-\ncally message passing operation on graphs, like one layer\nof GCN (Kipf and Welling 2017), GraphSAGE (Hamil-\nton, Ying, and Leskovec 2017), GIN (Xu et al. 2019),\nGAT (Velickovic et al. 2018), APPNP (Klicpera, Bo-\njchevski, and GÂ¨unnemann 2019) or PNA (Corso et al. 2020);\nand for the sake of simplicity, we use â€œTransformerâ€ to\nspecially refer to Transformer encoder. Here we analyze\nGNN, Transformer and our double-view graph Transformer\nin terms of model architecture.\nGNN handles text in the syntax tree (graph) view,\nhaving the risk of over-smoothing. As shown in Fig-\nure 2 (a), several graph convolutional modules constitute a\nmulti-layer GNN. Since graph convolution can be regarded\nas a smoothing procedure, the constructed GNN is actu-\nally a node feature smoother. With the GNN getting deep,\nthe node representations will be over-smoothed, i.e., they\nwill tend to be similar values and hard to distinguish (Li,\nHan, and Wu 2018; Chen et al. 2020; Oono and Suzuki\n2020). Therefore, for GNNs used in NLP, over-smoothing\nlimits the model depth and thus limits the performance.\nNote that for the ABSA task at hand, the previous GNNs\nsuch as R-GAT (Wang et al. 2020), DualGCN (Li et al.\n2021a), SSEGCN (Zhang, Zhou, and Wang 2022) and AG-\nVSR (Feng et al. 2022) all employ no more than 2 layers in\nmost cases.\nTransformer deals with text in the sequence view, also\nhaving the problem of over-smoothing.See Figure 2 (b),\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n19405\nover-smoothed(a)\nâ€¦\nâ€¦\n(c)\n(b)\nGraph Conv\nGraph Conv\nGraph Conv\nâ€¦\nGraph Conv\nGraph Conv\nTransformer Layer\nTransformer Layer\nTransformer Layer\nTransformer Layer\nTransformer Layer\nGraph ConvTransformer Layerover-smoothed\nbeneficial\nFigure 2: (a) GNN; (b) Transformer; (c) our double-view\ngraph Transformer.\nTransformerâ€™s full-attention mechanism takes the sequence\nas a complete graph and thus it can be regarded as a special\nGNN. Still, over-smoothing also occurs in this scenario if\nthe model gets deep enough, as theoretically proved by Shi\net al. (2022).\nOur double-view graph Transformer can alleviate\nover-smoothing. Different from an entire GNN or Trans-\nformer, the proposed double-view graph Transformer on text\n(TextGT) tightly combines the graph operator and the se-\nquence operator (i.e., GNN and Transformer), as Figure 2 (c)\nshows. Since Transformer can help GNN alleviate over-\nsmoothing (Wu et al. 2021; Chen, Oâ€™Bray, and Borgwardt\n2022; Rampasek et al. 2022), we also take advantage of it.\nIn TextGT, we alternate the graph convolution layers and the\nTransformer layers one by one in series, corresponding to\nsyntactic information in the graph view and semantic infor-\nmation in the sequence view, respectively. Intuitively, since\nthese 2 aforementioned smoothing procedures are for dif-\nferent views, they can just somehow â€œhinderâ€ each other\nproceeding easily, and thus help each other prevent over-\nsmoothing in time. Hence, the resulting model has the poten-\ntial to alleviate the problem of over-smoothing, which makes\nthe intermediate representations beneï¬cial to the learning\ntask.\nMethodology\nIn the following, we ï¬rst elaborate on the whole pipeline\nof our model for ABSA, including the architecture of the\nproposed TextGT. Then TextGINConv as the graph convo-\nlutional module of TextGT is introduced in detail.\nPipeline\nOur model includes 4 parts: dependency parser, text encoder,\nTextGT and pooling method. Figure 3 shows the whole\npipeline.\nDependency parser and preprocessing. We employ\nStanford CoreNLP (https://stanfordnlp.github.io/CoreNLP)\nto preprocess each sentence into a syntax tree, where depen-\ndency relationships of some word pairs are parsed out. In\nsuch a way we can get the corresponding graph with an ad-\njacency matrix and the element values represent the types of\nedges. Except for the syntax tree, the dependency parser also\noutputs the parts of speech (POS) of the words in a same sen-\ntence, which can serve as the PE of the graph Transformer.\nText encoder. There are two options of text encoders\nfor TextGT: one is BiLSTM (Graves and Schmidhuber\n2005) trained from scratch, and the other is BERT (De-\nvlin et al. 2019) already pre-trained. For BiLSTM we use\nGloVe (https://github.com/stanfordnlp/GloVe) word vectors\nas the frozen embeddings, while word embeddings in BERT\nis trainable. For either of them, we ï¬rstly have\nHemb = embeddingw(S); (1)\nwhere Hemb 2 Rn\u0002dw represents dw-dimensional word\nembedding matrix of the sentence S. Then if BiLSTM is\nused, we have\nH0 = BiLSTM (Hemb); (2)\nwhere H0 2Rn\u0002d is the d-dimensional word hidden fea-\ntures; while if BERT then\nH0; CLS = BERT (Hemb); (3)\nwhere CLS 2 Rd is the representation of a special to-\nken used for global pooling, i.e., the output from the BERT\npooler.\nPE. We utilize POS embeddings and relative distance em-\nbeddings as the node PE for our graph Transformer. As\nstated earlier, the POS of each word in a sentence is parsed in\nthe preprocessing step. As for the relative distances from the\naspect words, we set a maximum and a minimum, and a spe-\nciï¬c learnable embedding vector is assigned to each value.\nThe embeddings of POS and relative distance are concate-\nnated to the corresponding word embeddings before fed into\nthe text encoder.\nTextGT architecture.As shown in Figure 3, one block\nof TextGT basically consists of 3 components: TextGIN-\nConv, middle layer and Transformer layer. We regard texts\n(sentences) as double-view data, being graphs in one view\nand temporal sequences in the other. On the one hand, our\nTextGINConv processes texts in the graph view, as a dense\nimplementation of GINE (Hu et al. 2020). Roughly speak-\ning, TextGINConv is composed of message constructor, ag-\ngregator and updater (i.e., an MLP), which will be detailed\nin the following subsection. On the other hand, Transformer\nlayer processes texts as sequential data and its attention\nmechanism is performed on all the words globally in a same\ninstance. In addition, there is a middle layer between the\ngraph operator and the sequence operator, making the two\nmodules compatible with and adapted to each other and thus\nthey can be combined well. The formulas of the aforemen-\ntioned procedures are as follows:\nOl = TextGINConv (l)(Hl\u00001; A); (4)\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n19406\nTransformer Layer\nMessage \nConstructing\nMessage \nAggregating\nMLP\nMiddle Layer\n(Norm & ReLU)\nL Ã—\nTextGINConv Classifier\n(Linear+Softmax)\nğ’‘ğ’ğ’” ğ’ğ’†ğ’ˆ ğ’ğ’†ğ’–\nText Encoder (BiLSTM/BERT)\nAvg Pooling\nc\nConcat\n[CLS]\nBottles   of     wine   are   cheap  and  good\nğ‘¤1 ğ‘¤2 ğ‘¤3 ğ‘¤4 ğ‘¤5 ğ‘¤6 ğ‘¤7\nğ’‰ğŸ ğ’‰ğŸ ğ’‰ğŸ‘ ğ’‰ğŸ’ ğ’‰ğŸ“ ğ’‰ğŸ” ğ’‰ğŸ•\nDependency Parser\n&  Preprocessing\nPE/SE\nnmod nsubj\ncase\nnmod case\ncop\nnsubj cop cc conj\ncc\nconj\nBottles\nof\nwine\nare\ncheap\nand\ngood\nğ‘¯ğ¿\nTextGT\nFigure 3: The pipeline of our model for ABSA, including the overview of the proposed TextGT which alternates graph convo-\nlutional layers and Transformer layers.\n^Hl = ReLU(LayerNorm (l)(Ol)); (5)\n^Ol = TransformerLayer (l)( ^Hl); (6)\nHl = ^Ol + Hl\u00001; (7)\nwhere Hl\u00001; Hl 2Rn\u0002d are the word intermediate repre-\nsentations after the (l\u00001)-th and l-th TextGT blocks respec-\ntively (totally L blocks), Ol; ^Ol 2Rn\u0002d are the outputs of\nthe graph convolution layer and the Transformer layer in the\nl-th block, and ^Hl 2Rn\u0002d is the output of the middle layer,\ni.e., LayerNorm followed by ReLU. There is a skip connec-\ntion from the beginning to the end in each TextGT block,\nwhich can also be considered as a jumping knowledge con-\nnection (Xu et al. 2018).\nPooling method.We take the output of the L-th TextGT\nblock as the ï¬nal word representations, and then an average\npooling is performed on all the word representations from\nthe same aspect to extract the aspect representation, formu-\nlated as:\n\u0016h =\nkP\nj=1\nHL\naj\nk ; (8)\nwhere \u0016h 2 Rd denotes the aspect representation, and the\nmeanings of aj as well as k is described in the introduction.\nAfterwards, if BERT serves as the text encoder, then the in-\nput of the classiï¬er is p = [\u0016hkCLS], where p 2R2d repre-\nsents the concatenating result of \u0016h and CLS; otherwise, if\nBiLSTM is used then p = \u0016h 2Rd.\nTextGINConv\nTo accommodate graph learning for text, with node and\nedge features both in dense batch instead of sparse batch\nimplementation in Pytorch Geometric (https://pytorch-\ngeometric.readthedocs.io/en/latest/index.html), we design a\ndense version of GINE Conv speciï¬cally designed for text\n(thus called TextGINConv). Figure 4 shows how TextGIN-\nConv deals with an example graph, whose adjacency matrix\nwith edge types is denoted asA 2Rn\u0002n, and d-dimensional\nnode features as X 2Rn\u0002d. Before message passing, the\nmask matrix is generated based on A, meanwhile, X is ex-\npanded along the row and column to Xrow 2Rn\u0002n\u0002d and\nXcol 2Rn\u0002n\u0002d, respectively.\nIn terms of message passing mechanism, the message\nconstructing procedure is:\nE = embeddinge(A); (9)\nM = mask(Xrow + E); (10)\nwhere E 2Rn\u0002n\u0002d denotes the embeddings of the edges,\nand M 2Rn\u0002n\u0002d is the constructed messages correspond-\ning to edges. After masking via the 0-1 mask matrix, we can\nguarantee any non-edge feature vector is ï¬lled with zero in\nM. Then the aggregation is as follows:\n^X = fx(\nnX\nj=1\nM\u0001j); (11)\n^E = fe(E + Xrow + Xcol); (12)\nwhere ^X 2Rn\u0002d and ^E 2Rn\u0002n\u0002d represent the aggregat-\ning results of nodes and edges respectively, while fx and fe\nare the updating functions like MLP.\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n19407\n21\n45\n3\n0 1 1 0 0\n1 0 1 0 0\n1 1 0 1 0\n0 0 1 0 1\n0 0 0 1 0\n1\n2\n3\n4\n5\n1 2 3 4 5\na b\na c\nb c d\nd e\ne\n1\n2\n3\n4\n5\n1 2 3 4 5\nmask\nğ’†54\nğ’†43 ğ’†45\nğ’†31 ğ’†32 ğ’†34\nğ’†21 ğ’†23\nğ’†12 ğ’†13\n1 2 3 4 5\n1\n2\n3\n4\n5\nğ’™5\nğ’™4\nğ’™3\nğ’™2\nğ’™1\nğ’™5\nğ’™4\nğ’™3\nğ’™2\nğ’™1\nğ’™5\nğ’™4\nğ’™3\nğ’™2\nğ’™1\nğ’™5\nğ’™4\nğ’™3\nğ’™2\nğ’™1\nğ’™5\nğ’™4\nğ’™3\nğ’™2\nğ’™1\nğ’™5\nğ’™4\nğ’™3\nğ’™2\nğ’™1\nexpand\nğŸ ğŸ ğŸ ğ’54 ğŸ\nğŸ ğŸ ğ’43 ğŸ ğ’45\nğ’31ğ’32 ğŸ ğ’34 ğŸ\nğ’21 ğŸ ğ’23 ğŸ ğŸ\nğŸ ğ’12ğ’13 ğŸ ğŸ\n1 2 3 4 5\n1\n2\n3\n4\n5\naggregate\nà·ğ’™ğŸ“\nà·ğ’™ğŸ’\nà·ğ’™ğŸ‘\nà·ğ’™ğŸ\nà·ğ’™ğŸ\nà·œğ’†54\nà·œğ’†43 à·œğ’†45\nà·œğ’†31 à·œğ’†32 à·œğ’†34\nà·œğ’†21 à·œğ’†23\nà·œğ’†12 à·œğ’†13\n1 2 3 4 5\n1\n2\n3\n4\n5\nğ’™1 ğ’™2 ğ’™3 ğ’™4 ğ’™5\nğ’™1 ğ’™2 ğ’™3 ğ’™4 ğ’™5\nğ’™1 ğ’™2 ğ’™3 ğ’™4 ğ’™5\nğ’™1 ğ’™2 ğ’™3 ğ’™4 ğ’™5\nğ’™1 ğ’™2 ğ’™3 ğ’™4 ğ’™5\nembed\na\nb c\nd\ne\nadd\nadd\nadd\nadd\nadd\nğ‘¨ ğ‘¬\nğ‘´\nğ‘¿ ğ‘¿row ğ‘¿ğ’„ğ’ğ’\nFigure 4: The illustration of our proposed TextGINConv.\nOne signiï¬cant step of TextGINConv is to use the mask\nto ï¬lter out the redundant edge features, i.e., set mij = 0\nif there is no edge between node i and j, which guarantees\nnoise messages will not inï¬‚uence node/edge representations\nupdating in the next layer. Note that TextGINConv can be\ngeneralized to a GNN framework to handle graphs with edge\nfeatures and dense adjacency matrices, e.g., we can just nor-\nmalize messages with node degrees / attention scores before\naggregation, to realize TextGCNCon/TextGATConv.\nIn our method, for simplicity the edge updating proce-\ndures are not used (purple arrows in Figure 4) and the edge\nfeatures remain constant while forward propagating.\nExperiments\nThis section presents the experiments we conducted for\ncomparison, ablation study, depth study, case study and vi-\nsualization.\nDatasets\nWe conduct our experiments on 4 public benchmark datasets\nfor ABSA. Restaurant and Laptop are from SemEval\n2014 (Pontiki et al. 2014), consisting of reviews related with\nrestaurants and laptops, respectively; Rest16 is also a collec-\ntion of reviews about restaurants, provided from SemEval\n2016 (Hercig et al. 2016); and Twitter, built by Dong et\nal. (2014), is composed of twitter posts.\nComparison to the State-of-the-Art Methods\nWe compare TextGT to the state-of-the-art baselines, which\nare divided into 4 categories based on whether they use a\npre-trained model (i.e., BERT) and whether they use a GNN.\n(Methods without released source code are out of the com-\nparison.)\nBaselines:\na. Methods without GNN or BERT including IAN (Ma\net al. 2017), RAM (Chen et al. 2017), MGAN (Fan, Feng,\nand Zhao 2018) and TNet (Li et al. 2018).\nb. Methods with GNN but without BERT including AS-\nGCN (Zhang, Li, and Song 2019), CDT (Sun et al. 2019),\nBiGCN (Zhang and Qian 2020), kumaGCN (Chen, Teng,\nand Zhang 2020), InterGCN (Liang et al. 2020), R-\nGAT (Wang et al. 2020), DualGCN (Li et al. 2021a),\nDGEDT (Tang et al. 2020), SSEGCN (Zhang, Zhou, and\nWang 2022) and AG-VSR (Feng et al. 2022).\nc. BERT (Devlin et al. 2019).\nd. Methods with both GNN and BERT including\nBERT4GCN (Xiao et al. 2021), T-GCN (Tian, Chen, and\nSong 2021), Sentic GCN (Liang et al. 2021), R-GAT,\nDualGCN, DGEDT, SSEGCN and AG-VSR.\nSetup. According to different situations of the baselines,\nwe conduct 2 comparison experiments: as shown in Table 1,\nthe ï¬rst is on Restaurant, Laptop and Twitter to demonstrate\nthe effectiveness of our TextGT with BiLSTM (top) and that\nwith BERT (bottom); for the lack of statistics from origi-\nnal papers of GNN baselines on Rest16 and in case of unfair\ncomparison (some use external knowledge or additional sup-\nplement datasets), Table 2 only validates TextGT+BERT by\ncomparing to other BERT-based models.\nResults. From Table 1 and 2 we can see that whether the\nmetric is accuracy or F1 score, our TextGT (BiLSTM) is\nconsistently the best among all the methods without BERT,\nand TextGT+BERT achieves the best results in most cases.\nAlthough on Laptop and Twitter our TextGT+BERT does\nnot take the ï¬rst place, the results are comparable and close\nto the best ones. Note that in Table 2 Sentic GCN is a method\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n19408\nModels\nRestaurant Laptop T\nwitter\nAccuracy \" Micro-F1 \" Accuracy \" Micr\no-F1 \" Accuracy \" Micro-F1 \"\nIAN (IJCAI,\n2017) 78.60 â€“ 72.10 â€“ â€“ â€“\nRAM (EMNLP, 2017) 80.23 70.80 74.49 71.35 69.36 67.30\nMGAN (EMNLP, 2018) 81.25 71.94 75.39 72.47 72.54 70.81\nTNet (ACL, 2018) 80.69 71.27 76.54 71.75 74.90 73.60\nASGCN (EMNLP, 2019) 80.77 72.02 75.55 71.05 72.15 70.40\nCDT (EMNLP, 2019) 82.30 74.02 77.19 72.99 74.66 73.66\nBiGCN (EMNLP, 2020) 81.97 73.48 74.59 71.84 74.16 73.35\nkumaGCN (EMNLP, 2020) 81.43 73.64 76.12 72.42 72.45 70.77\nInterGCN (COLING, 2020) 82.23 74.01 77.86 74.32 â€“ â€“\nR-GAT (ACL, 2020) 83.30 76.08 77.42 73.76 75.57 73.82\nDGEDT (ACL, 2020) 83.90 75.10 76.80 72.30 74.80 73.40\nDualGCN (ACL, 2021) 84.27 78.08 78.48 74.74 75.92 74.29\nSSEGCNy (NAA\nCL, 2022) 83.29 76.31 77.22 73.53 74.74 73.32\nAG-VSR (KBS, 2022) 83.45 76.05 78.16 74.77 75.78 74.33\nOur T\nextGT 85.17 79.70 78.64 74.91 76.51 75.26\nBERT\n(NAACL, 2019) 85.97 80.09 79.91 76.00 75.92 75.18\nR-GAT+BERT (ACL, 2020) 86.60 81.35 78.21 74.07 76.15 74.88\nDGEDT+BERT (ACL, 2020) 86.30 80.00 79.80 75.60 77.90 75.40\nBERT4GCN (EMNLP, 2021) 84.75 77.11 77.49 73.01 74.73 73.76\nT-GCN+BERT (NAACL, 2021) 86.16 79.95 80.88 77.03 76.45 75.25\nDualGCN+BERT (ACL, 2021) 87.13 81.16 81.80 78.10 77.40 76.02\nSSEGCN+BER\nTy (NAACL, 2022) 86.15 79.96 79.75 76.38 77.70 76.36\nAG-VSR+BER\nT (KBS, 2022) 86.34 80.88 79.92 75.85 76.45 75.04\nOur T\nextGT+BERT 87.31 82.27 81.33 78.71 77.70 76.45\nTable 1: Comparison on the benchmark datasets.yindicates the baselines are rerun based on the open source code, and the other\nresults are from the related original papers or (Li et al. 2021a). The best results are highlighted in boldface while the second\nbest ones are underlined, and lacking results are marked as â€œâ€“â€.\nModels\nRest16\nAccuracy \" Micro-F1 \"\nBERT\n(NAACL, 2019) 90.10 74.16\nR-GAT+BERT (ACL, 2020) 89.71 76.62\nDGEDT+BERT (ACL, 2020) 91.90 79.00\nDualGCN+BERTy (ACL, 2021) 90.99 78.37\nSSEGCN+BERTy (NAACL, 2022) 91.60 79.66\nSentic GCN+BER\nT (KBS, 2022) 91.97 79.56\nOur T\nextGT+BERT 92.21 81.48\nTable 2: Comparison on Rest16.\nusing external knowledge, which is unfair to us, but our\nTextGT still outperforms it by an obvious margin.\nAblation Study\nWe conduct ablation studies to demonstrate the effectiveness\nof each components making up TextGT. We replace Trans-\nformer layer with TextGINConv and the resulting model is\ncalled TextGIN. Moreover, we replace TextGINConv with\nTextGCNConv/TextGATConv, then get TextGT (GCN) /\nTextGT (GAT). As shown in Table 3, neither TextGIN nor\nTransformer alone works well, and GIN is most suitable for\nTextGT among 3 commonly used graph convolutional mod-\nules. Additionally, LayerNorm + ReLU can narrow the gap\nbetween TextGINConv and the Transformer layer thus mak-\ning the overall model perform better.\nDepth Study\nTo further show the superiority of TextGT, we vary model\nlayers and observe test accuracy with respect to model depth.\nFrom Figure 5, we can see the performance of R-GAT, Dual-\nGCN, TextGIN and Transformer all drop dramatically with\nthe depth increasing, while that of our TextGT keeps at a\nhigh level steadily. Consequently, TextGT is able to allevi-\nate over-smoothing and is robust to the increase in model\ndepth.\nCase Study\nWe display some test cases in Table 4, to show the ability\nof TextGT to discriminate aspects. These reviews are ran-\ndomly selected across different datasets. The results indicate\nTextGT can decide the polarity of an aspect better than the\nbaselines, beneï¬ting from the tightly combined 2 operations\nof the graph view and the sequence view, which shows the\nadvantage of alternating graph convolution and global atten-\ntion to model the context syntactically and semantically.\nVisualization\nWe further validate TextGT from the perspective of model\ninterpretation. Figure 6 is the visualization result of a word\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n19409\nModels\nRestaurant Laptop T\nwitter\nAccuracy \" Micro-F1 \" Accuracy \" Micr\no-F1 \" Accuracy \" Micro-F1 \"\nTe\nxtGIN 80.07 72.96 74.68 70.45 74.89 74.15\nTransformer 81.77 74.33 75.16 70.97 75.48 74.44\nw/o middle layer 83.82 77.54 75.63 71.97\n76.22 74.79\nTe\nxtGT (GCN) 83.56 76.89 77.37 73.77 75.33 74.09\nT\nextGT (GAT) 82.13 75.35 75.63 71.51 75.78 74.81\nTe\nxtGT (GIN) 85.17 79.70 78.64 74.91 76.51 75.26\nTable 3: Ablation results on the benchmark datasets.\n# Re\nviews DGEDT DualGCN SSEGCN TextGT\n1 [Pizzas] were e\nxcellent in addition to [appetizers]\nand [main courses]. (P4, P4, O8) (P 4, O8, O8) (P 4, O8, P4) (P 4, P4, P4)\n2\nItâ€™s\ngood to go there for [drinks] if you donâ€™t want\nto get drunk because you â€™ll be lucky if you can get\none [drink] an hour, the [service] is so bad.\n(O4, P8, N4) (O 4, N8, N4) (N 8, P8, N4) (O 4, O4, N4)\n3 I use\nit mostly for [content creation]([audio],\n[video], [photo editing]) and its reliable. (O8, O8, O8, O8) (O 8, O8, O8, O8) (P 4, O8, O8, O8) (P 4, P4, O8, P4)\n4 Unfortunately,\nit runs [XP] and Microsoft is\ndropping [support] next April. (N8, N4) (N 8, N4) (N 8, N4) (O 4, N4)\nTable 4: Case study. We compare our TextGT with the other 3 state-of-the-art methods.\n1 2 3 4 5 6 7 8 9\nmodel depth\n50\n55\n60\n65\n70\n75\n80\n85\n90\n95accuracy\nT extGT\nR-GAT\nDualGCN\nT extGIN\nTransformer\nFigure 5: Depth study on Laptop.\nattention matrix extracted from an intermediate layer of\nTextGT. The corresponding sentence is â€œThis little place\nis wonderfully warm welcomingâ€ with a one-word aspect\nâ€œplaceâ€ and obviously its polarity is positive. Note that the\naspect word â€œplaceâ€ attaches most attention on the sentimen-\ntal and informative words â€œwonderfully warm welcomingâ€,\nwhile almost ignores the interfering sentiment word â€œlittleâ€.\nConclusion\nIn this paper, we analyze the over-smoothing problem in\nGNN and Transformer from the perspective of model struc-\nture. Then we propose a double-view graph Transformer\nThis little placeis wonderfullywarmwelcoming.\nThis\nlittle\nplace\nis\nwonderfully\nwarm\nwelcoming\n.\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nFigure 6: A visualization result.\non text called TextGT for ABSA. TextGT alternates the\n2 learning processes in the graph view and the sequence\nview. Adapting GNN for text representation learning syntac-\ntically, we propose an algorithm for implementing a kind of\ndensely message passing graph convolution called TextGIN-\nConv. Extensive experiments on public benchmark datasets\ndemonstrate the effectiveness of our method. In the future\nwork, we plan to apply our TextGT to other NLP tasks, and\nwe will further explore other schemes to combine graph con-\nvolution and self-attention to get high-performance graph\nTransformers for NLP.\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n19410\nAcknowledgments\nThis work was partially supported by the National Key Re-\nsearch and Development Program of China under Grant\nNo. 2018AAA0100400, HY Project under Grant No.\nLZY2022033004, the Natural Science Foundation of Shan-\ndong Province under Grants No. ZR2020MF131 and No.\nZR2021ZD19, the Science and Technology Program of\nQingdao under Grant No. 21-1-4-ny-19-nsh, and Project of\nAssociative Training of Ocean University of China under\nGrant No. 202265007.\nReferences\nCambria, E.; Speer, R.; Havasi, C.; and Hussain, A. 2010.\nSenticNet: A Publicly Available Semantic Resource for\nOpinion Mining. 14â€“18.\nChen, C.; Teng, Z.; Wang, Z.; and Zhang, Y . 2022. Discrete\nOpinion Tree Induction for Aspect-based Sentiment Analy-\nsis. 2051â€“2064.\nChen, C.; Teng, Z.; and Zhang, Y . 2020. Inducing Target-\nspeciï¬c Latent Structures for Aspect Sentiment Classiï¬ca-\ntion. 5596â€“5607.\nChen, D.; Lin, Y .; Li, W.; Li, P.; Zhou, J.; and Sun, X. 2020.\nMeasuring and relieving the over-smoothing problem for\ngraph neural networks from the topological view. In AAAI.\nChen, D.; Oâ€™Bray, L.; and Borgwardt, K. 2022. Structure-\naware transformer for graph representation learning. In\nICML, 3469â€“3489. PMLR.\nChen, P.; Sun, Z.; Bing, L.; and Yang, W. 2017. Recurrent\nAttention Network on Memory for Aspect Sentiment Anal-\nysis. 452â€“461.\nChen, Y .; Dai, X.; Chen, D.; Liu, M.; Dong, X.; Yuan, L.;\nand Liu, Z. 2021. Mobile-Former: Bridging MobileNet and\nTransformer. 2022 IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), 5260â€“5269.\nCorso, G.; Cavalleri, L.; Beaini, D.; Li`o, P.; and VeliË‡ckoviÂ´c,\nP. 2020. Principal neighbourhood aggregation for graph\nnets. NeurIPS, 13260â€“13271.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), 4171â€“4186. Asso-\nciation for Computational Linguistics.\nDong, L.; Wei, F.; Tan, C.; Tang, D.; Zhou, M.; and Xu,\nK. 2014. Adaptive Recursive Neural Network for Target-\ndependent Twitter Sentiment Classiï¬cation. 49â€“54.\nDwivedi, V . P.; and Bresson, X. 2020. A Generalization of\nTransformer Networks to Graphs. CoRR, abs/2012.09699.\nFan, F.; Feng, Y .; and Zhao, D. 2018. Multi-grained Atten-\ntion Network for Aspect-Level Sentiment Classiï¬cation. In\nProceedings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, 3433â€“3442. Association\nfor Computational Linguistics.\nFeng, S.; Wang, B.; Yang, Z.; and Ouyang, J. 2022. Aspect-\nbased sentiment analysis with attention-assisted graph and\nvariational sentence representation. Knowl. Based Syst.,\n258: 109975.\nGraves, A.; and Schmidhuber, J. 2005. Framewise phoneme\nclassiï¬cation with bidirectional LSTM and other neural net-\nwork architectures. Neural networks, 18 5-6: 602â€“10.\nGuo, J.; Han, K.; Wu, H.; Xu, C.; Tang, Y .; Xu, C.; and\nWang, Y . 2021. CMT: Convolutional Neural Networks\nMeet Vision Transformers. 2022 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 12165â€“\n12175.\nHamilton, W. L.; Ying, Z.; and Leskovec, J. 2017. Induc-\ntive Representation Learning on Large Graphs. In NeurIPS,\n1024â€“1034.\nHercig, T.; Brychcin, T.; Svoboda, L.; and Konkol, M. 2016.\nUWB at SemEval-2016 Task 5: Aspect Based Sentiment\nAnalysis. 342â€“349.\nHu, W.; Liu, B.; Gomes, J.; Zitnik, M.; Liang, P.; Pande,\nV . S.; and Leskovec, J. 2020. Strategies for Pre-training\nGraph Neural Networks. In ICLR.\nKipf, T. N.; and Welling, M. 2017. Semi-Supervised Classi-\nï¬cation with Graph Convolutional Networks. In ICLR.\nKlicpera, J.; Bojchevski, A.; and GÂ¨unnemann, S. 2019. Pre-\ndict then Propagate: Graph Neural Networks meet Personal-\nized PageRank. In ICLR.\nKreuzer, D.; Beaini, D.; Hamilton, W. L.; L Â´etourneau, V .;\nand Tossou, P. 2021. Rethinking Graph Transformers with\nSpectral Attention. In NeurIPS, 21618â€“21629.\nLi, J.; Xia, X.; Li, W.; Li, H.; Wang, X.; Xiao, X.; Wang,\nR.; Zheng, M.; and Pan, X. 2022. Next-ViT: Next Genera-\ntion Vision Transformer for Efï¬cient Deployment in Realis-\ntic Industrial Scenarios. ArXiv, abs/2207.05501.\nLi, Q.; Han, Z.; and Wu, X.-M. 2018. Deeper insights into\ngraph convolutional networks for semi-supervised learning.\nIn AAAI.\nLi, R.; Chen, H.; Feng, F.; Ma, Z.; Wang, X.; and Hovy,\nE. 2021a. Dual Graph Convolutional Networks for Aspect-\nbased Sentiment Analysis. 6319â€“6329.\nLi, X.; Bing, L.; Lam, W.; and Shi, B. 2018. Transforma-\ntion Networks for Target-Oriented Sentiment Classiï¬cation.\nArXiv, abs/1805.01086.\nLi, Y .; Yao, T.; Pan, Y .; and Mei, T. 2021b. Contextual\nTransformer Networks for Visual Recognition. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence, PP:\n1â€“1.\nLiang, B.; Su, H.; Gui, L.; Cambria, E.; and Xua, R. 2021.\nAspect-based sentiment analysis via affective knowledge en-\nhanced graph convolutional networks. Knowl. Based Syst.,\n235: 107643.\nLiang, B.; Yin, R.; Gui, L.; Du, J.; and Xu, R. 2020. Jointly\nLearning Aspect-Focused and Inter-Aspect Relations with\nGraph Convolutional Networks for Aspect Sentiment Anal-\nysis. In Proceedings of the 28th International Conference\non Computational Linguistics, 150â€“161. International Com-\nmittee on Computational Linguistics.\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n19411\nMa, D.; Li, S.; Zhang, X.; and Wang, H. 2017. Interactive\nAttention Networks for Aspect-Level Sentiment Classiï¬ca-\ntion. ArXiv, abs/1709.00893.\nNiu, H.; Xiong, Y .; Gao, J.; Miao, Z.; Wang, X.; Ren, H.;\nZhang, Y .; and Zhu, Y . 2022. Composition-based Hetero-\ngeneous Graph Multi-channel Attention Network for Multi-\naspect Multi-sentiment Classiï¬cation. In Proceedings of the\n29th International Conference on Computational Linguis-\ntics, 6827â€“6836. International Committee on Computational\nLinguistics.\nOono, K.; and Suzuki, T. 2020. Graph Neural Networks Ex-\nponentially Lose Expressive Power for Node Classiï¬cation.\nIn ICLR.\nPeng, Z.; Huang, W.; Gu, S.; Xie, L.; Wang, Y .; Jiao, J.; and\nYe, Q. 2021. Conformer: Local Features Coupling Global\nRepresentations for Visual Recognition. 2021 IEEE/CVF\nInternational Conference on Computer Vision (ICCV), 357â€“\n366.\nPontiki, M.; Galanis, D.; Pavlopoulos, J.; Papageorgiou, H.;\nAndroutsopoulos, I.; and Manandhar, S. 2014. SemEval-\n2014 Task 4: Aspect Based Sentiment Analysis. In Interna-\ntional Workshop on Semantic Evaluation, 27â€“35.\nRampasek, L.; Galkin, M.; Dwivedi, V . P.; Luu, A. T.; Wolf,\nG.; and Beaini, D. 2022. Recipe for a General, Powerful,\nScalable Graph Transformer. In NeurIPS.\nShi, H.; Gao, J.; Xu, H.; Liang, X.; Li, Z.; Kong, L.;\nLee, S. M. S.; and Kwok, J. 2022. Revisiting Over-\nsmoothing in BERT from the Perspective of Graph. ArXiv,\nabs/2202.08625.\nSun, C.; Huang, L.; and Qiu, X. 2019. Utilizing BERT for\nAspect-Based Sentiment Analysis via Constructing Auxil-\niary Sentence. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, Volume\n1 (Long and Short Papers), 380â€“385. Association for Com-\nputational Linguistics.\nSun, K.; Zhang, R.; Mensah, S.; Mao, Y .; and Liu, X. 2019.\nAspect-Level Sentiment Analysis Via Convolution over De-\npendency Tree. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), 5679â€“5688. Association for\nComputational Linguistics.\nTang, D.; Qin, B.; and Liu, T. 2016. Aspect Level Sentiment\nClassiï¬cation with Deep Memory Network. In Proceedings\nof the 2016 Conference on Empirical Methods in Natural\nLanguage Processing, 214â€“224. Association for Computa-\ntional Linguistics.\nTang, H.; Ji, D.; Li, C.; and Zhou, Q. 2020. Dependency\nGraph Enhanced Dual-transformer Structure for Aspect-\nbased Sentiment Classiï¬cation. 6578â€“6588.\nTian, Y .; Chen, G.; and Song, Y . 2021. Aspect-based Sen-\ntiment Analysis with Type-aware Graph Convolutional Net-\nworks and Layer Ensemble. 2910â€“2922.\nVelickovic, P.; Cucurull, G.; Casanova, A.; Romero, A.; Li`o,\nP.; and Bengio, Y . 2018. Graph Attention Networks. In\nICLR.\nV o, D.; and Zhang, Y . 2015. Target-Dependent Twitter Senti-\nment Classiï¬cation with Rich Automatic Features. In Yang,\nQ.; and Wooldridge, M. J., eds., Proceedings of the Twenty-\nFourth International Joint Conference on Artiï¬cial Intelli-\ngence, IJCAI 2015, Buenos Aires, Argentina, July 25-31,\n2015, 1347â€“1353. AAAI Press.\nWang, K.; Shen, W.; Yang, Y .; Quan, X.; and Wang, R. 2020.\nRelational Graph Attention Network for Aspect-based Sen-\ntiment Analysis. 3229â€“3238.\nWang, Y .; Huang, M.; Zhu, X.; and Zhao, L. 2016.\nAttention-based LSTM for aspect-level sentiment classiï¬-\ncation. In Proceedings of the 2016 conference on empirical\nmethods in natural language processing, 606â€“615.\nWu, Z.; Jain, P.; Wright, M.; Mirhoseini, A.; Gonzalez,\nJ. E.; and Stoica, I. 2021. Representing long-range context\nfor graph neural networks with global attention. NeurIPS,\n13266â€“13279.\nXiao, Z.; Wu, J.; Chen, Q.; and Deng, C. 2021. BERT4GCN:\nUsing BERT Intermediate Layers to Augment GCN for\nAspect-based Sentiment Classiï¬cation. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natural Lan-\nguage Processing, 9193â€“9200. Association for Computa-\ntional Linguistics.\nXu, K.; Hu, W.; Leskovec, J.; and Jegelka, S. 2019. How\nPowerful are Graph Neural Networks? In ICLR.\nXu, K.; Li, C.; Tian, Y .; Sonobe, T.; Kawarabayashi, K.-i.;\nand Jegelka, S. 2018. Representation learning on graphs\nwith jumping knowledge networks. In International con-\nference on machine learning, 5453â€“5462. PMLR.\nYing, C.; Cai, T.; Luo, S.; Zheng, S.; Ke, G.; He, D.; Shen,\nY .; and Liu, T. 2021. Do Transformers Really Perform Badly\nfor Graph Representation? In NeurIPS, 28877â€“28888.\nZhang, C.; Li, Q.; and Song, D. 2019. Aspect-based Sen-\ntiment Classiï¬cation with Aspect-speciï¬c Graph Convolu-\ntional Networks. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), 4568â€“4578. Association for\nComputational Linguistics.\nZhang, M.; and Qian, T. 2020. Convolution over Hier-\narchical Syntactic and Lexical Graphs for Aspect Level\nSentiment Analysis. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language Process-\ning (EMNLP), 3540â€“3549. Association for Computational\nLinguistics.\nZhang, Z.; Zhou, Z.; and Wang, Y . 2022. SSEGCN: Syntac-\ntic and Semantic Enhanced Graph Convolutional Network\nfor Aspect-based Sentiment Analysis. 4916â€“4925.\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n19412"
}