{
    "title": "OpenFOAMGPT: A retrieval-augmented large language model (LLM) agent for OpenFOAM-based computational fluid dynamics",
    "url": "https://openalex.org/W4408150519",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2101039172",
            "name": "Sandeep Pandey",
            "affiliations": [
                "Technische Universität Ilmenau"
            ]
        },
        {
            "id": "https://openalex.org/A2066410461",
            "name": "Ran Xu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2515631929",
            "name": "Wenkang Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2106891439",
            "name": "Xu Chu",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2795982117",
        "https://openalex.org/W4402327095",
        "https://openalex.org/W3018098279",
        "https://openalex.org/W4404346592",
        "https://openalex.org/W4405193613",
        "https://openalex.org/W4286909924",
        "https://openalex.org/W2904260561",
        "https://openalex.org/W2534240011",
        "https://openalex.org/W2807826281",
        "https://openalex.org/W4391735147",
        "https://openalex.org/W2920959147",
        "https://openalex.org/W4389624813",
        "https://openalex.org/W2780698255",
        "https://openalex.org/W2790277586",
        "https://openalex.org/W4385722611",
        "https://openalex.org/W3095421728",
        "https://openalex.org/W4297009732",
        "https://openalex.org/W4385664535",
        "https://openalex.org/W4382246105",
        "https://openalex.org/W4397028247",
        "https://openalex.org/W4385681450",
        "https://openalex.org/W4386874599",
        "https://openalex.org/W4395703652",
        "https://openalex.org/W4404960420",
        "https://openalex.org/W4405185373",
        "https://openalex.org/W4387773384",
        "https://openalex.org/W4391862446",
        "https://openalex.org/W4400703072",
        "https://openalex.org/W4403580088",
        "https://openalex.org/W4402269933",
        "https://openalex.org/W4402413193",
        "https://openalex.org/W4405301082",
        "https://openalex.org/W4391101734",
        "https://openalex.org/W4399511738",
        "https://openalex.org/W4392935214",
        "https://openalex.org/W4401440766",
        "https://openalex.org/W2110187357",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4403798678",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W3101378293",
        "https://openalex.org/W3166992610"
    ],
    "abstract": "This work presents a large language model (LLM)-based agent OpenFOAMGPT tailored for OpenFOAM-centric computational fluid dynamics (CFD) simulations, leveraging two foundation models from OpenAI: the GPT-4o (GPT means Generative Pre-trained Transformer) and a chain-of-thought–enabled o1 preview model. Both agents demonstrate success across multiple tasks. While the price of token with o1 model is six times as that of GPT-4o, it consistently exhibits superior performance in handling complex tasks, from zero-shot/few-shot case setup to boundary condition modifications, zero-shot turbulence model adjustments, and zero-shot code translation. Through an iterative correction loop, the agent efficiently addressed single-phase and multiphase flow, heat transfer, Reynolds-averaged Navier–Stokes modeling, large eddy simulation, and other engineering scenarios, often converging in a limited number of iterations at low token costs. To embed domain-specific knowledge, we employed a retrieval-augmented generation pipeline, demonstrating how preexisting simulation setups can further specialize the agent for subdomains such as energy and aerospace. Despite the great performance of the agent, human oversight remains crucial for ensuring accuracy and adapting to shifting contexts. Fluctuations in model performance over time suggest the need for monitoring in mission-critical applications. Although our demonstrations focus on OpenFOAM, the adaptable nature of this framework opens the door to developing LLM-driven agents into a wide range of solvers and codes. By streamlining CFD simulations, this approach has the potential to accelerate both fundamental research and industrial engineering advancements.",
    "full_text": "OpenFOAMGPT: a RAG-Augmented LLM Agent for\nOpenFOAM-Based Computational Fluid Dynamics\nSandeep Pandey ∗\nInstitute of Thermodynamics and Fluid Mechanics,\nTechnische Universit¨ at Ilmenau, Ilmenau D-98684, Germany\nRan Xu ∗\nCluster of Excellence SimTech, University of Stuttgart, Stuttgart, Germany\nWenkang Wang\nInternational Research Institute for Multidisciplinary Science,\nBeihang University, 100191 Beijing, China\nXu Chu †\nFaculty of Environment, Science and Economy,\nUniversity of Exeter, Exeter EX4 4QF, United Kingdom and\nCluster of Excellence SimTech, University of Stuttgart, Stuttgart, Germany\n1\narXiv:2501.06327v1  [physics.flu-dyn]  10 Jan 2025\nAbstract\nThis work presents a large language model (LLM)-based agent OpenFOAMGPT tai-\nlored for OpenFOAM-centric computational fluid dynamics (CFD) simulations, leveraging\ntwo foundation models from OpenAI: the GPT-4o and a chain-of-thought (CoT)–enabled\no1 preview model. Both agents demonstrate success across multiple tasks. While the price\nof token with o1 model is six times as that of GPT-4o, it consistently exhibits superior\nperformance in handling complex tasks, from zero-shot case setup to boundary condition\nmodifications, turbulence model adjustments, and code translation. Through an iterative\ncorrection loop, the agent efficiently addressed single- and multi-phase flow, heat trans-\nfer, RANS, LES, and other engineering scenarios, often converging in a limited number\nof iterations at low token costs. To embed domain-specific knowledge, we employed a\nretrieval-augmented generation (RAG) pipeline, demonstrating how preexisting simulation\nsetups can further specialize the agent for sub-domains such as energy and aerospace. De-\nspite the great performance of the agent, human oversight remains crucial for ensuring\naccuracy and adapting to shifting contexts. Fluctuations in model performance over time\nsuggest the need for monitoring in mission-critical applications. Although our demonstra-\ntions focus on OpenFOAM, the adaptable nature of this framework opens the door to\ndeveloping LLM-driven agents into a wide range of solvers and codes. By streamlining\nCFD simulations, this approach has the potential to accelerate both fundamental research\nand industrial engineering advancements.\n∗ These authors contributed equally to this work.\n† x.chu@exeter.ac.uk\n2\nI. INTRODUCTION\nRecently, the field of fluid mechanics has increasingly adopted data-driven method-\nologies, propelled by abundant high-fidelity simulation data and rapid advancements\nin machine learning [1–6]. These approaches have been developed to model tur-\nbulence both with and without governing equations [7–12], as well as to address\ncomplex heat transfer problems [13, 14]. In addition, machine learning techniques\nhave supported experimental measurements [15] and facilitated scientific discovery,\nfor instance by enabling causal inference in fluid flows [16–18].\nMore recently, Large Language Models (LLMs) [19] are rapidly emerging as power-\nful tools in scientific and engineering domains, offering unprecedented capabilities in\nnatural language understanding [20], automated reasoning [21], and decision-making.\nFor research and engineering, these models hold the potential to transform conven-\ntional workflows by enabling more efficient problem-solving [22, 23], advanced op-\ntimization [21, 24], and accelerated scientific discovery [25, 26]. As recent studies\ndemonstrate, LLMs can serve as intelligent assistants that both enhance traditional\nmethodologies and pave the way for novel approaches to analysis, design, and sim-\nulation. Buehler [27], Ni and Buehler [28] presented MechGPT, a fine-tuned large\nlanguage model designed to unify disparate knowledge domains—such as mechanics\nand biology—within the context of multiscale materials failure. By leveraging Onto-\nlogical Knowledge Graphs for interpretability, retrieval-augmented generation, and\nexpanded context lengths, MechGPT supports interdisciplinary insight, hypothesis\ngeneration, and the integration of new data sources. Ghafarollahi and Buehler [29] in-\ntroduced a physics-aware generative AI platform, AtomAgents, which autonomously\ntackles multi-objective materials design by combining LLM with multiple AI agents\noperating in a dynamic environment. Moreover, Ghafarollahi and Buehler [30] pro-\nposed a multi-agent AI model for automated metallic alloy discovery, integrating\n3\nLLM-driven reasoning and planning, specialized AI agents for domain expertise, and\na graph neural network for rapid retrieval of key physical properties, thereby acceler-\nating complex materials design tasks. In addition, Buehler [31] introduced Cephalo,\na family of multimodal vision LLM for materials science, combining visual and lin-\nguistic data to interpret complex scenes, generate precise textual descriptions, and\nenable advanced design workflows.\nA prominent application of LLMs in fluid mechanics lies in their ability to fa-\ncilitate equation discovery. For instance, Du et al. [32] have demonstrated how\nLLMs can autonomously extract governing equations from data, capturing complex\nnonlinear relationships without extensive human intervention. Beyond equation dis-\ncovery, LLMs have also been employed to streamline shape optimization problems.\nZhang et al. [33] for example, introduced a framework that leverages these models\nfor optimizing geometric profiles, such as airfoils or axisymmetric configurations, to\nachieve reduced drag. Emerging research extends these advancements into areas\nsuch as microfluidics [34], where LLMs assist in decision-making for robotic motion\nplanning under fluid flow constraints or guide microscale swimmers through com-\nplex fluidic environments. Zhu et al. [35] introduces a framework that combines\nLLM with spatiotemporal-aware encoders to predict unsteady fluid dynamics, sig-\nnificantly improving accuracy and efficiency compared to traditional CFD methods.\nBy leveraging pre-trained LLMs and graph neural networks, the model effectively\nintegrates spatial and temporal information, achieving superior performance in long-\nhorizon predictions for fluid datasets like airfoil and cylinder flows. Kim et al. [36]\nevaluated the potential of ChatGPT-generated MATLAB code guided by narrative-\nbased prompts for geotechnical engineering applications, including seepage flow anal-\nysis, slope stability assessment, and X-ray tomography image processing. Although\nChatGPT cannot fully replace conventional programming, it effectively refines code,\nminimizes syntax errors, and provides a logical starting framework when carefully di-\n4\nrected by domain-specific guidance. Equally significant are the applications of LLMs\nin automating and orchestrating CFD simulations. Chen et al. [37] proposed an\nLLM-based multi-agent system capable of automating CFD workflows through nat-\nural language interactions. By employing retrieval-augmented generation (RAG),\nthis platform can identify and correct potential errors, substantially lowering the\ntechnical barriers typically associated with CFD simulations and making these tools\nmore accessible to non-specialists.\nThese advances illustrate how LLMs are reshaping the landscape of scientific re-\nsearch including fluid mechanics, from equation discovery and shape optimization\nto the automation of complex CFD workflows. In this work, we introduce an LLM-\nbased, RAG-augmented agent for OpenFOAM, aiming to bridge the gap by providing\na robust, conversational interface that can both retrieve domain-specific information\nand generate contextually informed instructions. With this LLM-based agent for\nCFD, our goal is to automate the workflow of performing simulations, significantly\nlower the expertise threshold, and thus boost overall productivity. Beyond Open-\nFOAM, we seek to illustrate that this flexible framework can be adapted to a wide\nrange of other solvers and codes.\nII. METHODOLOGY\nOpenFOAM (Open Source Field Operation and Manipulation) is a widely utilized,\nopen-source CFD solver package [38]. It offers a diverse collection of solvers for a\nbroad range of applications, including incompressible [39–41] and compressible flows\n[42], heat transfer [43–47], and multiphase systems [48–50]. Unlike commercial CFD\npackages, OpenFOAM’s fully open-source architecture grants users the flexibility to\nmodify existing solvers, develop new physical models. By relying on a robust C++\nobject-oriented design, it ensures maintainability, extensibility, and high parallel\n5\nefficiency, making it a standard platform for both academic research and industrial\nengineering workflows. Its proven accuracy, adaptability, and scalability have made\nOpenFOAM an ideal environment for coupling with LLM-based agents that can\ninteractively guide users through simulation setup, execution, and analysis. The\nversion we use here is OpenFOAM-v2406 release.\nGPT (Generative Pre-trained Transformer) is a family of AI foundamtion model\ndeveloped by OpenAI that generates human-like text by predicting the next word\nbased on the context of previous words. It is already widely used for tasks like\nchatbots, content creation, and code generation. We develop the OpenFOAM spe-\ncific agent based on two foundation models of OpenAI: ChatGPT-4o (4o) [19] and\nChatGPT-o1 preview (o1), each accessed through the OpenAI API. While GPT-4\nis widely recognized for its robust language understanding, o1 [51, 52] distinguishes\nitself through the chain-of-thought (CoT) mechanism, which enables better reason-\ning and more detailed problem-solving steps. The API token pricing highlights a\ncost difference: the 4o model is priced at $2.50 per million input tokens and $10 per\nmillion output tokens, while the o1 model costs $15.00 per million input tokens and\n$60 per million output tokens.\nFigure 1 depicts the current, multi-layered structure of our agentOpenFOAMGPT.\nAt the top, a system prompt is combined with a user query. The Builder module\nthen interprets these instructions—consulting the RAG database as needed for\ndomain-specific information—and transforms them into a structured plan. Next, the\nExecutor orchestrates the workflow: it can direct the query to the LLM model for\nfurther reasoning or hand off tasks to the OpenFOAM agent for simulation-specific\nactions. Inside the OpenFOAM agent, an Interpreter, Builder, and Runner col-\nlaborate to set up and execute the necessary OpenFOAM operations. Throughout\nthe simulation, error logs are monitored; if a failure is detected, the error data is\nappended to the user query and the process iterates. Otherwise, the workflow con-\n6\nFIG. 1: The design of the agent structure\ncludes successfully. To ensure domain-specific completeness, the agent uses retrieval-\naugmented generation (RAG) when constructing or refining its plans. A dedicated\nRAG database—derived from OpenFOAM tutorial case descriptions—provides rel-\nevant details such as solver names, case names, and flow types. These descriptions\nare embedded into 1536-dimensional vectors via OpenAI’s text-embedding-3-small\nmodel, allowing the agent to look up or incorporate analogous cases from public or\n7\nprivate repositories seamlessly.\nThe addition of domain knowledge through RAG (Figure 2) leads to a better\nprompting and accurate results [53]. It is implemented through the LangChain frame-\nwork [54]. Although a LLM can reason about general knowledge, its performance\nin specialized tasks improves significantly when supplemented by domain-specific re-\nsources. By querying an indexed library of OpenFOAM tutorials, the RAG layer\nprovides the LLM with validated best practices and updated methodologies. This\nadditional context helps bridge typical knowledge gaps inherent to pre-trained mod-\nels, ensuring that generated responses remain current with community standards\nin the field of CFD. Moreover, by further extending the knowledge base to include\nspecialized topics—such as aerodynamics, process engineering, and heat transfer de-\nsign, our general-purpose CFD agent can be seamlessly adapted to serve specific\nsub-disciplines. This modular structure not only maintains flexibility but also en-\nables deep, domain-focused expertise, leading to more accurate and comprehensive\nsolutions across a variety of fluid flow applications.\nUser\n(Asks Question)\nEngine\n(Builds Prompt)\nLLM\n(Generates Response)\nUser\n(Receives Response)\nRetrieval Library\nQuestion Full Prompt Response\nRetrieval QueryRetrieved Texts\nFIG. 2: The structure of RAG\n8\nIII. EVALUATIONS\nA. Performance of zero-shot prompting\nIt remains uncertain whether the developed agent OpenFOAMGPT can design\nand run simulations without further training. To evaluate the its capabilities, we\nselected six cases from the OpenFOAM tutorials encompassing single- and multiphase\nflow, laminar and turbulent regimes, and ranging from straightforward to application-\noriented examples.\n• Cavity flow: This case simulates a laminar, isothermal, incompressible flow\nin a two-dimensional square domain using the icoFoam solver. The geometry\nconsists of a square with all boundaries as walls. The top wall moves in the\nx-direction at a speed of 1 m/s, while the other three walls are stationary.\n• PitzDaily: It simulates incompressible, turbulent flow through the PitzDaily\ngeometry using the k- ϵ model and the simpleFoam solver. The geometry is a\nsudden expansion channel in a two-dimensional plane, with a narrow entrance\nand a wider section after the sudden expansion. The entire channel forms a\ntwo-dimensional rectangular structure.\n• Hotroom: It is a turbulent natural convection inside a tall cavity which is\nsimulated using the k- ϵ model and the buoyantBoussinesqSimpleFoam solver.\nThe cavity has a two-dimensional rectangular shape with walls as boundaries.\nThe left and right walls are adiabatic, the top wall is cold, and the bottom wall\nis hot.\n• Dambreak: It represents a simplified dam break with laminar flow, simulated\nusing the VOF-based multi-phase solver (interFoam). The tank is a two-\ndimensional square, with a rectangular obstacle located at the center of the\n9\nlower boundary. The top boundary is open, while the other three boundaries\nare stationary walls. The setup consists of a column of water at rest behind\na membrane on the left side of the tank. At time t=0 s, the membrane is\nremoved, causing the water to collapse. As the water impacts the obstacle, a\ncomplex flow structure is generated, including several pockets of trapped air.\n• Particle column: This case simulates the distribution and dynamic evolution\nof particle flow in a two-dimensional column using the MPPICFoam solver.\nThe column is a rectangular domain, with an inlet at the top and an outlet\nat the bottom. The fluid is modeled as an incompressible, single-phase flow\ngoverned by the Navier-Stokes equations, while the particle dynamics are mod-\neled using the Lagrangian tracking method. The multiphase particle dynam-\nics model (MPPIC) is employed to account for particle collisions and contact\nstress. Forces acting on the particles include fluid drag, collision forces, and\ngravity.\n• Mixed vessel: This case simulates fluid flow in a rotary mixer or agitator using\nthe pimpleFoam solver. The geometry is a two-dimensional cylinder with four\nrectangular barriers evenly distributed on both the outer and inner walls. The\nouter wall is stationary, while the inner wall rotates, with its motion simulated\nusing a moving grid.\nZero-shot prompting attempts to generate desired output based upto limited in-\nstructions without any example. As LLMs are trained on vast datasets, therefore,\nit often results in desired outputs. However, if underlying problem is difficult then\nmodel might not provide desired output. In this subsection, we analyze the perfor-\nmance of LLM models with zero-shot prompting.\n10\nCase gpt-4o o1-mini o1-preview\nCavity flow ✓ ✓ ✓\nPitzDaily × (blockMeshDict incorrect) × (blockMeshDict incorrect) × (blockMeshDict incorrect)\nHotroom × (blockMeshDict incorrect) × (transportProperties incorrect) ✓\nDambreak × (blockMeshDictincorrect) × (blockMeshDict incorrect) × (blockMeshDict incorrect)\nParticle column × (0 incorrect) × (0 incorrect) × (kinematicCloudProperties incorrect)\nMixed vessel × ( blockMeshDict incorrect) × (blockMeshDict incorrect) × (blockMeshDict incorrect)\nTABLE I: Evaluation zero-shot prompting of the agent with OpenAI’s gpt-4o, o1-\nmini, and o1-preview.\nTable I compares the performance of OpenFOAMGPT using foundation mod-\nels 4o and o1, without RAG. When prompted solely with natural language, the\nagent employing model 4o can only produce simple two-dimensional square geome-\ntries (e.g., Cavity flow). Changing the model to o1-mini shows no improvement. In\ncontrast, the agent using model o1-preview generates more complex geometries (e.g.,\nHotroom), although relying entirely on natural language is not ideal for specifying\nintricate shapes. Providing a blockMeshDict file as part of the input is therefore\nmore effective. After supplying a detailed blockMeshDict file, the agent with o1\ndemonstrates improved performance: even without RAG, it successfully handles the\nCavity flow, PitzDaily, Hotroom, Dambreak, and Mixed vessel cases. However, for\nmore complex problems such as Particle column—requiring unidirectional flow com-\nbined with particle flow—the agent encounters an unsolvable error when relying on\nthe LLM alone.\n11\ncase\n4o o1-preview\nIterations Token count Cost ($) Iterations Token count Cost ($)\nCavity flow 2 6632 0.0327 1 9236 0.3956\nPitzDaily 4 19941 0.0808 7 40214 0.9069\nHotroom 9 37268 0.1269 5 36754 0.93795\nDambreak 5 22572 0.0853 8 20755 0.6294\nParticle column 10 96666 0.3645 5 77157 1.6475\nMixed vessel 6 72769 0.2744 9 42750 1.0148\nTABLE II: Comparison of the agent (based on 4o and o1-preview) with RAG and\nwithout RAG.\nB. Few-Shot Prompting with RAG\nIn the previous subsection, OpenFOAMGPT based on GPT-4 (4o) without\nRAG capabilities succeeded only in the simplest cavity flow case, failing to set up\nmore complex simulations. Here, we evaluate the improvement gained by enabling\nretrieval-augmented generation (RAG), with a summary of results presented in Ta-\nble II. Figure 3 illustrates the simulation outcomes at the last time step. But in\nthe case of particle column, the output of kinematicCloudProperties is different\nfrom the file provided in RAG because the output file has a length limit. Once\nequipped with RAG, the agent successfully sets up all tested cases, underscoring the\nsignificance of retrieving relevant domain knowledge for complex CFD workflows.\nFor the simplest cavity flow, only two interaction rounds were required, consum-\ning approximately 6.6 thousand tokens. More complicated tasks, such as the par-\nticle column case, demanded ten interaction rounds and used around 96 thousand\ntokens. Application-oriented setups generally generated higher token usage, partic-\n12\nFIG. 3: Case simulation results. (a) Cavity flow (b) PitzDaily (c) Hotroom (d)\nDambreak (e) Particle column (f) Mixed vessel.\nularly when multiple output files or specialized model inputs were involved. Despite\nthese increases, the associated monetary cost with 4o model varied from $0.03 to\n$0.36 per test scenario, a number that remains vastly lower than typical labor ex-\npenses. Execution times were under ten minutes in each instance, largely dependent\non the number of interaction rounds and the amount of reference data consulted,\nwhile creating input files and running the solver remain swift. Switching from the\n4o model to the o1 model does not yield the anticipated reduction in iterations or\ntoken usage. Because the o1 model has a higher token price, overall costs rise signif-\nicantly—up to $1.60 in the particle column case. Consequently, given the differences\nin token pricing, the 4o model appears to be a more cost-effective choice.\n13\nC. Alternate simulation conditions: initial- and boundary conditions, mesh\nresolution, thermophysical properties\nCase Alternate condition gpt-4o\no1-\npreview\nCavity flow\nVelocity of top wall movement 1m/s → 2m/s ✓\nVelocity of top wall movement 1m/s → 5 sin\n\u0000\n2π t\n0.1\n\u0001\n× ✓\nMesh resolution 20×20×1 → ×15×1 ✓\nendTime 3 → 5 ✓\nPitzDaily Speed of the inlet 10m/s → 20m/s ✓\nHotroom Temperature of HOT WALL 310K → 320K ✓\nDambreak Liquid inside the membrane water → oil ✓\nParticle column\nVelocity of the fluid and particles 1m/s → 2m/s ✓\nType of fluid Air → CO × ✓\nMixed vessel Angular speed of rotation 20rad/s → 15rad/s ✓\nTABLE III: Tasks of alternate initial- and boundary conditions.\nIn engineering research and design, it is often necessary to explore a broad param-\neter space by adjusting simulation conditions—such as initial and boundary condi-\ntions or thermophysical properties—ranging from simple, homogeneous, steady-state\nparameters to unsteady, heterogeneous scenarios. Table III summarizes these tasks,\nshowing that the agent with o1-preview model can successfully handle all changes.\nEven the agent using the 4o model can accomplish most of these objectives.\nA noteworthy example involves assigning an unsteady boundary condition, 5 sin\n\u0000\n2π t\n0.1\n\u0001\n,\nin the cavity flow setup. The prompt includes the requirement but no information\nof how it should be done. In fact, OpenFOAMGPT chose the right boundary con-\n14\ndition codedFixedValue, not others such as fixedValue, oscillatingFixedValue.\nThis operation is more complex than simply replacing a constant value, yet the\nOpenFOAMGPT with the foundation model o1 managed it by generating advanced\nOpenFOAM scripting (excerpted in Listing 1). This accomplishment illustrates o1’s\ncapacity for high-level reasoning and planning, further emphasizing its potential for\nsophisticated CFD applications.\n1 movingWall\n2 {\n3 type codedFixedValue;\n4 value uniform (0 0 0);\n5 redirectType sineVelocity;\n6\n7 code\n8 #{\n9 scalar amplitude = 5.0; // Amplitude in m/s\n10 scalar period = 0.1; // Period in seconds\n11 scalar omega = 2.0 * M_PI / period;\n12 vector velocity(amplitude * sin(omega * this->db().time().value()), 0, 0);\n13 operator==(velocity);\n14 #};\n15 }\nListing 1: Generated code in the U file.\nD. Zero-Shot addition or replacement of turbulence models (RANS and\nLES)\nMany engineering simulations involve turbulent flows that require closure models\nfor the Reynolds-averaged Navier–Stokes equations or subgrid-scale formulations in\nlarge eddy simulations. Numerous turbulence models are available—each with its\nown level of complexity, computational cost, and approach to flow anisotropy—making\nit routine for R&D teams to test and compare different models. However, this pro-\n15\ncess extends well beyond merely switching a model’s name, as it often necessitates\nadjusting/adding/deleting initial and boundary conditions for different transport\nequations, as well as selecting suitable numerical schemes. In this subsection, we\nexamine the zero-shot capability of OpenFOAMGPT in adding or swapping tur-\nbulence models across various cases. Our goal was to transition from the standard\nk-ϵ model to other RANS models, including RNG k-ϵ, k-ω SST, and the three-\nequation eddy-viscosity model k −kl −ω as well as the Launder–Reece–Rodi (LRR)\nReynolds-stress turbulence model. It is worth noting that the k − kl − ω model\nappears less commonly used and is more complex. But the foundation model o1\nperforms effectively in this context (excerpted in Listing 2). For LES, we replaced\nthe dynamicKEqn model with a standard Smagorinsky turbulence model. Table IV\nsummarizes these tasks and provides key observations. Although the agent success-\nfully handled most scenarios, it encountered difficulties when applying a k-ϵ model\nto the particle column case. The particle column scenario itself involves higher com-\nplexity, posing challenges. Nevertheless, our results underscore the agent’s potential\nto significantly streamline or even automate advanced CFD tasks with minimal\nhuman input.\n1 simulationType RAS;\n2 RAS\n3 {\n4 RASModel kkLOmega;\n5 turbulence on;\n6 printCoeffs on;\n7 kkLOmegaCoeffs\n8 {\n9 sigmaK 2.0;\n10 sigmaL 0.66666;\n11 alphaOmega 0.52;\n12 betaOmega 0.072;\n13 }\n14 }\n16\nListing 2: Generated code in the turbulenceProperties file.\nCase Alternate turbulence models gpt-4o o1-preview\nCavity flow\nkEpsilon→ RNGkEpsilon/kOmegaSST/LRR ✓\nkEpsilon → kkLOmega × ✓\nPitzDaily\nkEpsilon→ kOmegaSST ✓\ndynamicKEqn (LES) → Smagorinsky (LES) ✓\nDambreak Laminar flow → kEpsilon ✓\nParticle\ncolumn Laminar flow→ kEpsilon ×\n×\n(kinematicCloudProperties\nincorrect)\nMixed vessel Laminar flow → kEpsilon × ✓\nTABLE IV: Tasks of add/alternate turbulence models\nE. Zero-Shot code translation with o1-preview\nCode translation is a frequent and essential operation in software development. It\nis often necessary to migrate or port code between different versions or distributions\nof the same framework. For instance, two distinct OpenFOAM distributions have\nbeen developed and maintained independently over the years: www.openfoam.com\nand www.openfoam.org. These versions are generally incompatible with each other.\nAs a result, users and developers regularly face the need to import a case from one\ndistribution to the other, which requires substantial modifications to source code\nand library configurations. In this subsection, we illustrate how OpenFOAMGPT\nwith the o1 model can facilitate this cross-platform translation process.\n17\nTask Web-based LLM (o1-preview) Agent with o1-preview\nT-Junction (RANS ) × functions for post-processing in controlDict ✓\nChannel395 (LES) ✓ ✓\nPitzDaily (RANS) × Incompatible dimensions for operation ✓\nMotorbike (RANS) × × snappyHexMeshDict incorrect\nTABLE V: Code translation from OpenFOAM 12 platform ( www.openfoam.org) to\nV2406 platform (www.openfoam.com)\nIn Table V, we present the results of translating four tutorial cases (T-Junction,\nChannel395, PitzDaily, and Motorbike) from the OpenFOAM 12 platform ( www.\nopenfoam.org) to the OpenFOAM V2406 platform ( www.openfoam.com). During\nthe translation process, we provided code only for the OpenFOAM 12 platform and\ndid not include any information related to the OpenFOAM V2406 platform. When\nusing the web-based LLM (o1-preview), Channel395 is the only case that trans-\nlates successfully without inconsistencies. The remaining three cases encountered\nvarious errors, ranging from inaccurate function entries in controlDict to dimen-\nsion mismatches in the field operations. Such issues highlight the complexities of\ncross-platform code translation, emphasizing the need for meticulous validation and\ntargeted debugging. However, when using the agent, only the Motorbike case ex-\nperienced errors that prevented it from running, while the PitzDaily case failed to\nconverge after a few steps. This observation underscores the effectiveness of using\nagent to resolve issues in similar situations, further emphasizing the necessity of its\nuse.\n18\nIV. CONCLUSION, LIMITATIONS AND OUTLOOK\nWe have presented a LLM-based agent OpenFOAMGPT for OpenFOAM-\nfocused computational fluid dynamics, integrating two foundation models from Ope-\nnAI—namely, the widely used GPT-4 (4o) and a chain-of-thought (CoT)–enabled\no1 model. Although o1 incurs about six times the token cost, it demonstrated a\nclear edge in handling complex tasks, underscoring its potential value in specialized\nCFD workflows. Through iterative correction loop, OpenFOAMGPT successfully\ntackled practical simulation tasks such as zero-shot case setup, modifications to ini-\ntial and boundary conditions, zero-shot turbulence model alternation, zero-shot code\ntranslation, and more. Representative test scenarios spanned single- and multi-phase\nflow, heat transfer, RANS, LES, and various engineering cases, all of which were\nresolved within a limited number of iteration loops at low token expenses.\nDespite the robust performance of the pre-trained LLMs—particularly the CoT-\nenabled o1 model—our findings highlight the importance of retrieval-augmented gen-\neration (RAG) for embedding domain-specific knowledge. By incorporating existing\nsimulation setups into the RAG pipeline, the agent can be further specialized for\nsub-domains such as energy sector, aerospace, or any area requiring tailored CFD\nsolutions. Nonetheless, a measure of human oversight remains critical to ensure\ncorrectness and adapt to evolving contexts. Furthermore, fluctuations in model per-\nformance over time serve as a reminder that users must carefully monitor this change\n(with no official reminder) updates for mission-critical or high-precision applications.\nLooking ahead, continued enhancements in hardware and frequent release of new\npowerful models will likely lower token costs and further enhance reasoning capa-\nbilities of the foundation model. While our case study centered on OpenFOAM,\nthe adaptability of this approach paves the way for integrating LLM-based agents\ninto a broad spectrum of solvers and codes. By reducing barriers to high-fidelity\n19\nsimulations, our framework holds promise for accelerating innovation across both\nfundamental research and industrial engineering processes.\n[1] K. Duraisamy, G. Iaccarino, and H. Xiao, Turbulence modeling in the age of data,\nAnnual Review of Fluid Mechanics 51, 357 (2019).\n[2] G. Yang, R. Xu, Y. Tian, S. Guo, J. Wu, and X. Chu, Data-driven methods for flow\nand transport in porous media: a review, International Journal of Heat and Mass\nTransfer 235, 126149 (2024).\n[3] S. Pandey, J. Schumacher, and K. R. Sreenivasan, A perspective on machine learning\nin turbulent flows, Journal of Turbulence 21, 567 (2020).\n[4] W. Wang and X. Chu, Optimized flow control based on automatic differentiation in\ncompressible turbulent channel flows, arXiv preprint arXiv:2410.23415 (2024).\n[5] A. Cremades, S. Hoyas, and R. Vinuesa, Additive-feature-attribution methods: a\nreview on explainable artificial intelligence for fluid dynamics and heat transfer, In-\nternational Journal of Heat and Fluid Flow 112, 109662 (2025).\n[6] R. Vinuesa and S. L. Brunton, Enhancing computational fluid dynamics with machine\nlearning, Nature Computational Science 2, 358 (2022).\n[7] J. Wu, H. Xiao, and E. Paterson, Physics-informed machine learning approach for\naugmenting turbulence models: A comprehensive framework, Physical Review Fluids\n3, 074602 (2018).\n[8] J. Ling, A. Kurzawski, and J. Templeton, Reynolds averaged turbulence modelling\nusing deep neural networks with embedded invariance, Journal of Fluid Mechanics\n807, 155 (2016).\n[9] A. Beck, D. Flad, and C. Munz, Deep neural networks for data-driven LES closure\nmodels, Journal of Computational Physics 398, 108910 (2019).\n20\n[10] X. Chu and S. Pandey, Non-intrusive, transferable model for coupled turbulent\nchannel-porous media flow based upon neural networks, Physics of Fluids 36, 025112\n(2024).\n[11] X. Yang, S. Zafar, J.-X. Wang, and H. Xiao, Predictive large-eddy-simulation wall\nmodeling via physics-informed neural networks, Physical Review Fluids 4, 034602\n(2019).\n[12] A. Beck and M. Kurz, Toward discretization-consistent closure schemes for large eddy\nsimulation using reinforcement learning, Physics of Fluids 35 (2023).\n[13] W. Chang, X. Chu, A. Fareed, S. Pandey, J. Luo, B. Weigand, and E. Laurien,\nHeat transfer prediction of supercritical water with artificial neural networks, Applied\nThermal Engineering 131, 815 (2018).\n[14] X. Chu, W. Chang, S. Pandey, J. Luo, B. Weigand, and E. Laurien, A computationally\nlight data-driven approach for heat transfer and hydraulic characteristics modeling of\nsupercritical fluids: From dns to dnn, International Journal of Heat and Mass Transfer\n123, 629 (2018).\n[15] R. Vinuesa, S. L. Brunton, and B. J. McKeon, The transformative potential of machine\nlearning for experiments in fluid mechanics, Nature Reviews Physics 5, 536 (2023).\n[16] W. Wang, X. Chu, A. Lozano-Dur´ an, R. Helmig, and B. Weigand, Information transfer\nbetween turbulent boundary layer and porous media, Journal of Fluid Mechanics920,\nA21 (2021).\n[17] W. Wang, A. Lozano-Dur´ an, R. Helmig, and X. Chu, Spatial and spectral characteris-\ntics of information flux between turbulent boundary layers and porous media, Journal\nof Fluid Mechanics 949, A16 (2022).\n[18] Y. Liu, W. Wang, G. Yang, H. Nemati, and X. Chu, The interfacial modes and modal\ncausality in a dispersed bubbly turbulent flow, Physics of Fluids 35 (2023).\n[19] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,\n21\nJ. Altenschmidt, S. Altman, S. Anadkat, et al., Gpt-4 technical report, arXiv preprint\narXiv:2303.08774 (2023).\n[20] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz, E. Agirre,\nI. Heintz, and D. Roth, Recent advances in natural language processing via large\npre-trained language models: A survey, ACM Computing Surveys 56, 1 (2023).\n[21] P. Ma, T.-H. Wang, M. Guo, Z. Sun, J. B. Tenenbaum, D. Rus, C. Gan, and W. Ma-\ntusik, Llm and simulation as bilevel optimizers: A new paradigm to advance physical\nscientific discovery, arXiv preprint arXiv:2405.09783 (2024).\n[22] L. Song, C. Zhang, L. Zhao, and J. Bian, Pre-trained large language models for in-\ndustrial control, arXiv preprint arXiv:2308.03028 (2023).\n[23] Y.-J. Wang, B. Zhang, J. Chen, and K. Sreenath, Prompt a robot to walk with large\nlanguage models, arXiv preprint arXiv:2309.09969 (2023).\n[24] K. Huang, Y. Qu, H. Cousins, W. A. Johnson, D. Yin, M. Shah, D. Zhou, R. Altman,\nM. Wang, and L. Cong, Crispr-gpt: An llm agent for automated design of gene-editing\nexperiments, arXiv preprint arXiv:2404.18021 (2024).\n[25] K. Chibwe, D. Mantilla-Calderon, and F. Ling, Evaluating gpt models for auto-\nmated literature screening in wastewater-based epidemiology, ACS Environmental Au\n(2024).\n[26] M. C. Ramos, C. Collison, and A. D. White, A review of large language models and\nautonomous agents in chemistry, Chemical Science (2024).\n[27] M. J. Buehler, Mechgpt, a language-based strategy for mechanics and materials mod-\neling that connects knowledge across scales, disciplines, and modalities, Applied Me-\nchanics Reviews 76, 021001 (2024).\n[28] B. Ni and M. J. Buehler, Mechagents: Large language model multi-agent collabo-\nrations can solve mechanics problems, generate new data, and integrate knowledge,\nExtreme Mechanics Letters 67, 102131 (2024).\n22\n[29] A. Ghafarollahi and M. J. Buehler, Atomagents: Alloy design and discovery\nthrough physics-aware multi-modal multi-agent artificial intelligence, arXiv preprint\narXiv:2407.10022 (2024).\n[30] A. Ghafarollahi and M. J. Buehler, Rapid and automated alloy design with graph neu-\nral network-powered llm-driven multi-agent systems, arXiv preprint arXiv:2410.13768\n(2024).\n[31] M. J. Buehler, Cephalo: Multi-modal vision-language models for bio-inspired materials\nanalysis and design, Advanced Functional Materials , 2409531 (2024).\n[32] M. Du, Y. Chen, Z. Wang, L. Nie, and D. Zhang, Large language models for automatic\nequation discovery of nonlinear dynamics, Physics of Fluids 36 (2024).\n[33] X. Zhang, Z. Xu, G. Zhu, C. M. J. Tay, Y. Cui, B. C. Khoo, and L. Zhu, Using large\nlanguage models for parametric shape optimization (2024), arXiv:2412.08072 [cs.CE].\n[34] Z. Xu and L. Zhu, Training microrobots to swim by a large language model (2024),\narXiv:2402.00044 [cs.RO].\n[35] M. Zhu, A. Bazaga, and P. Li` o, Fluid-llm: Learning computational fluid dynamics\nwith spatiotemporal-aware large language models, arXiv preprint arXiv:2406.04501\n(2024).\n[36] D. Kim, T. Kim, Y. Kim, Y.-H. Byun, and T. S. Yun, A chatgpt-matlab frame-\nwork for numerical modeling in geotechnical engineering applications, Computers and\nGeotechnics 169, 106237 (2024).\n[37] Y. Chen, X. Zhu, H. Zhou, and Z. Ren, Metaopenfoam: an llm-based multi-agent\nframework for cfd, arXiv preprint arXiv:2407.21320 (2024).\n[38] H. G. Weller, G. Tabor, H. Jasak, and C. Fureby, A tensorial approach to computa-\ntional continuum mechanics using object-oriented techniques, Computers in physics\n12, 620 (1998).\n[39] S. Pandey, X. Chu, and E. Laurien, Investigation of in-tube cooling of carbon dioxide at\n23\nsupercritical pressure by means of direct numerical simulation, International Journal\nof Heat and Mass Transfer 114, 944 (2017).\n[40] S. Pandey, X. Chu, E. Laurien, and B. Weigand, Buoyancy induced turbulence modu-\nlation in pipe flow at supercritical pressure under cooling conditions, Physics of Fluids\n30, 065105 (2018).\n[41] X. Chu, B. Weigand, and V. Vaikuntanathan, Flow turbulence topology in regular\nporous media: From macroscopic to microscopic scale with direct numerical simula-\ntion, Physics of Fluids 30, 065102 (2018).\n[42] Y. Liu, X. Chu, G. Yang, and B. Weigand, Simulation and analytical modeling of\nhigh-speed droplet impact onto a surface, Physics of Fluids 36 (2024).\n[43] C. Evrim, X. Chu, F. Silber, A. Isaev, S. Weihe, and E. Laurien, Flow features and\nthermal stress evaluation in turbulent mixing flows, International Journal of Heat and\nMass Transfer 178, 121605 (2021).\n[44] C. Evrim, X. Chu, and E. Laurien, Analysis of thermal mixing characteristics in\ndifferent t-junction configurations, International Journal of Heat and Mass Transfer\n158, 120019 (2020).\n[45] X. Chu, E. Laurien, and D. McEligot, Direct numerical simulation of strongly heated\nair flow in a vertical pipe, International Journal of Heat and Mass Transfer 101, 1163\n(2016).\n[46] X. Chu, G. Yang, S. Pandey, and B. Weigand, Direct numerical simulation of convec-\ntive heat transfer in porous media, International Journal of Heat and Mass Transfer\n133, 11 (2019).\n[47] Z. Li, D. T. Banuti, J. Ren, J. Lyu, H. Wang, and X. Chu, Microscale physics\nand macroscale convective heat transfer in supercritical fluids, arXiv preprint\narXiv:2406.04119 (2024).\n[48] Y. Liu, A. Geppert, X. Chu, B. Heine, and B. Weigand, Simulation of an annular\n24\nliquid jet with a coaxial supersonic gas jet in a medical inhaler, Atomization and\nSprays 31 (2021).\n[49] Y. Liu, X. Chu, W. Wang, and B. Weigand, Large-eddy simulation, convective in-\nstability, and modal causality of coaxial supersonic air–water jets considering a swirl\neffect, Physics of Fluids 35 (2023).\n[50] G. Yang, X. Chu, V. Vaikuntanathan, S. Wang, J. Wu, B. Weigand, and A. Terzis,\nDroplet mobilization at the walls of a microfluidic channel, Physics of Fluids32 (2020).\n[51] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al.,\nChain-of-thought prompting elicits reasoning in large language models, Advances in\nneural information processing systems 35, 24824 (2022).\n[52] T. Zhong, Z. Liu, Y. Pan, Y. Zhang, Y. Zhou, S. Liang, Z. Wu, Y. Lyu, P. Shu, X. Yu,\net al., Evaluation of openai o1: Opportunities and challenges of agi, arXiv preprint\narXiv:2409.18486 (2024).\n[53] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. K¨ uttler,\nM. Lewis, W.-t. Yih, T. Rockt¨ aschel, et al., Retrieval-augmented generation for\nknowledge-intensive nlp tasks, Advances in Neural Information Processing Systems\n33, 9459 (2020).\n[54] L. Contributors, Langchain (2024), gitHub repository.\n25"
}