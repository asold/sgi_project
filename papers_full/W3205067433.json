{
  "title": "MSP: Multi-Stage Prompting for Making Pre-trained Language Models Better Translators",
  "url": "https://openalex.org/W3205067433",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2556503561",
      "name": "Zhixing Tan",
      "affiliations": [
        "Center for Information Technology",
        "Beijing Academy of Artificial Intelligence",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2102743053",
      "name": "Xiangwen Zhang",
      "affiliations": [
        "Kuaishou (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2100431015",
      "name": "Shuo Wang",
      "affiliations": [
        "Tsinghua University",
        "Center for Information Technology",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A1983143503",
      "name": "Yang Liu",
      "affiliations": [
        "Center for Information Technology",
        "Beijing Academy of Artificial Intelligence",
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W3200578235",
    "https://openalex.org/W2970925270",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2996854111",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W2963506925",
    "https://openalex.org/W2994928925",
    "https://openalex.org/W3100311862",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W3152956381",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2997763445",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W3182414949",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W3173783648",
    "https://openalex.org/W3139080614",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3099925113",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3093345276",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3153805297",
    "https://openalex.org/W3176617251",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W3198002980",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2807535859"
  ],
  "abstract": "Prompting has recently been shown as a promising approach for applying pre-trained language models to perform downstream tasks. We present Multi-Stage Prompting, a simple and automatic approach for leveraging pre-trained language models to translation tasks. To better mitigate the discrepancy between pre-training and translation, MSP divides the translation process via pre-trained language models into three separate stages: the encoding stage, the re-encoding stage, and the decoding stage. During each stage, we independently apply different continuous prompts for allowing pre-trained language models better shift to translation tasks. We conduct extensive experiments on three translation tasks. Experiments show that our method can significantly improve the translation performance of pre-trained language models.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 6131 - 6142\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nMSP: Multi-Stage Prompting for Making\nPre-trained Language Models Better Translators\nZhixing Tan1,3,4, Xiangwen Zhang6, Shuo Wang1,3,4, and Yang Liu1,2,3,4,5\n1Department of Computer Science and Technology, Tsinghua University, Beijing, China\n2Institute for AI Industry Research, Tsinghua University, Beijing, China\n3Institute for Artificial Intelligence, Tsinghua University, Beijing, China\n4Beijing National Research Center for Information Science and Technology\n5International Innovation Center of Tsinghua University, Shanghai, China\n6Kuaishou Tech, Co.\nAbstract\nPrompting has recently been shown as a promis-\ning approach for applying pre-trained language\nmodels to perform downstream tasks. We\npresent Multi-Stage Prompting, a simple and\nautomatic approach for leveraging pre-trained\nlanguage models to translation tasks. To better\nmitigate the discrepancy between pre-training\nand translation, MSP divides the translation\nprocess via pre-trained language models into\nmultiple separate stages: the encoding stage,\nthe re-encoding stage, and the decoding stage.\nDuring each stage, we independently apply\ndifferent continuous prompts for allowing pre-\ntrained language models better shift to trans-\nlation tasks. We conduct extensive experi-\nments on three translation tasks. Experiments\nshow that our method can significantly improve\nthe translation performance of pre-trained lan-\nguage models. 1\n1 Introduction\nPrompting (Brown et al., 2020; Lester et al., 2021),\nwhich refers to the approach of generating task-\nspecific outputs from language models (LMs)\nby conditioning on extra information (known as\nprompts), has emerged as a new way of using\nLMs to perform natural language processing (NLP)\ntasks (Gao et al., 2020; Liu et al., 2021). While\nbeing efficient in parameters (Lester et al., 2021),\nprompting can enable mixed-task inference, which\nis not possible for other related approaches like\nfinetuning or adapter-based tuning (Li and Liang,\n2021; Lester et al., 2021). Prompting also opens\nthe possibility of using a single pre-trained LM to\nperform all NLP tasks (Liu et al., 2021).\nMachine translation (MT), which involves trans-\nformations between two languages, is considered\none of the most challenging tasks in NLP (Koehn\nCorresponding to: Z. Tan ( zxtan@tsinghua.edu.cn)\nand Y . Liu (liuyang2011@tsinghua.edu.cn)\n1Source code is available at https://github.com/\nTHUNLP-MT/PLM4MT.\nand Knowles, 2017). While neural machine trans-\nlation (NMT) (Sutskever et al., 2014; Bahdanau\net al., 2015; Vaswani et al., 2017) is the current\nde facto approach for machine translation, using\npre-trained LMs as translators via prompting is ap-\npealing in several aspects. For example, for the\nmethod described in this paper, supporting a new\ntranslation direction with a pre-trained LM occu-\npies disk spaces below 20M, which is much smaller\nthan training a separate neural machine translation\nmodel, where the model size is typically larger than\n60M per language pair for the Transformer archi-\ntecture. 2 Furthermore, the pre-trained LM also re-\ntains the ability to perform other downstream tasks,\nwhich is an important characteristic that has not\nbeen validated available on neural machine transla-\ntion models.\nHowever, it is challenging to leverage pre-trained\nLMs to translation tasks via prompting. First, find-\ning an appropriate prompt for a translation task\nis not trivial and requires specific designs (Brown\net al., 2020; Gao et al., 2020; Li and Liang, 2021;\nLester et al., 2021). Second, the prompting method\nwith a single prompt may be sub-optimal for steer-\ning pre-trained LMs to translation tasks, as there\nis a clear discrepancy between the objectives of\ntranslation and pre-training. Translation imposes\nstrict semantic equivalence and language space\nconstraint, in which a source sentence must trans-\nlate to a semantically equivalent sentence in the\ntarget language space. As the objective of pre-\ntraining is usually to reconstruct parts of the in-\nput sentence (Radford et al., 2018; Devlin et al.,\n2019), the generation of a pre-trained LM condi-\ntioned on a source sentence will likely be in the\nsource language space with non-equivalent seman-\ntics. Therefore, using a single prompt to guide the\nLM for mitigating both the semantic and language\ngap is likely to be sub-optimal. Third, prevalent\n2Assume using the transformer-base setting with a vocab-\nulary size of 32K.\n6131\nmGPT\nx1 x2 x3 <S> y1 y2 y3\ny1 y2 y3 </S>\nPrompt\nInputs\nPositions1 2 3 4 5 6 7\n(a) Basic (single-stage) prompting for MT.\nmGPTmGPT mGPT\nx1 x2 x3 x1 x2 x3 <S> y1 y2 y3\ny1 y2 y3 </S>\nInputs\nPositions1 2 3 1 2 3 1 2 3 4\nPromptPromptPrompt (b) Multi-stage prompting.\nFigure 1: Overview of using prompts for steering a multilingual GPT (mGPT) model to machine translation tasks.\nNote that we reset the position ids during each stage in multi-stage prompting for ease of implementation. All stages\nuse the same mGPT model.\ngenerative LMs such as GPTs use a decoder-only\narchitecture (Radford et al., 2018), which is uni-\ndirectional and may be sub-optimal for encoding\nsource sentences (Devlin et al., 2019). While re-\ncent works in prompting like prefix-tuning (Li and\nLiang, 2021) or prompt tuning (Lester et al., 2021)\nalleviate the first challenge by introducing differen-\ntiable continuous prompts, the last two challenges\nremain to be addressed.\nIn this paper, we present Multi-Stage Prompting\n(MSP) for addressing the challenges of steering\npre-trained language models to translation tasks.\nMSP encapsulates the idea of breaking transla-\ntion tasks into simpler consecutive stages, allow-\ning the pre-trained LM to learn “smoother transi-\ntions” to translation tasks by providing different\nprompts at different stages. For GPT-style pre-\ntrained LMs, we design a three-stage prompting\nscheme for modeling the translation process, which\nconsists of an encoding stage, a re-encoding stage,\nand a decoding stage. Specifically, the pre-trained\nLM focuses on learning source representations\nat the encoding stage and learns refined bidirec-\ntional representations by re-encoding source sen-\ntences at the re-encoding stage. Therefore, the LM\ncan produce better translations with refined source\nrepresentations at the decoding stage. Following\nprefix-tuning (Li and Liang, 2021) and prompt tun-\ning (Lester et al., 2021), we use independent train-\nable continuous prompts at different stages, which\nare learned through back-propagation. The differ-\nence between basic (single-stage) prompting and\nmulti-stage prompting is illustrated in Figure 1.\nWe demonstrate the effectiveness of our method\nwith a multilingual GPT (mGPT) model on\nRomanian-English, English-German, and English-\nChinese translation tasks. Experiments verify that\ncompared with prompt tuning or prefix-tuning,\nMSP can significantly improve the translation per-\nformance of pre-trained LMs. Our method im-\nproves the translation performance of pre-trained\nlanguage models via prompt tuning and prefix-\ntuning by 18.6 and 4.1 BLEU points on average\nover the three translation tasks, respectively, sug-\ngesting that MSP is a more effective prompting\nmethod for translation tasks.\n2 Background\n2.1 Prompting\nPrompting is an approach of using an LM to per-\nform downstream tasks by adding extra informa-\ntion for the LM to condition during its genera-\ntion (Lester et al., 2021). This extra information,\nalso known as a prompt, plays an important role in\nprompting methods and is often prepended to LM’s\ninput for better control of its generation. Depend-\ning on the form of prompts, prompting methods\ncan be divided into two categories: using textual\nprompts or using continuous prompts.\nTextual prompts are typically composed of natu-\nral language tokens. As a representative approach\nof textual prompts, Brown et al. (2020) use manu-\nally designed prompts to steer GPT-3’s generation.\nA typical prompt used in GPT-3 consists of a task\ndescription and a few task-specific examples. Gao\net al. (2020) and Shin et al. (2020) propose differ-\nent automatic methods to generate textual prompts.\nTextual prompts are typically understandable by\nhumans. However, Shin et al. (2020) indicate that\nautomatically generated textual prompts may lack\ninterpretability.\nContinuous prompts, which consist of a se-\nquence of continuous vectors, have gained increas-\ning popularity recently. For example, in (Li and\nLiang, 2021), the continuous prompts consist of a\nsequence of key-value pairs (also called prefixes).\nLester et al. (2021) propose a simplified version\nof continuous prompts, which consists of virtual\n6132\ntokens that are only added to the embedding layer.\nCompared with textual prompts, using continuous\nprompts is generally more powerful but less inter-\npretable (Lester et al., 2021).\n2.2 mGPT\nIn this paper, we use GPT (Radford et al., 2018,\n2019; Brown et al., 2020) as the backbone LM for\nmachine translation tasks. GPTs are a series of\ncausal language models based on the Transformer\narchitecture (Vaswani et al., 2017). To be more suit-\nable for translation tasks that involve multiple lan-\nguages, we introduce a multilingual GPT (mGPT)\nmodel instead of using a standard GPT-2 model. 3\nThe main difference between mGPT and GPT-2\nis the training data. mGPT is trained on the mC4\ndataset (Xue et al., 2021), which is a multilingual\ndataset covering over 101 languages. For further\ndetails about mGPT, please refer to Appendix A.1.\nLet z = [z1, . . . , zn] be a sequence of tokens,\nmGPT uses an autoregressive Transformer network\nto model the conditional probability P(zt|z<t),\nwhere t ∈ [1, n] and z<t = [z1, . . . , zt−1]. We use\nfLM(z, H; θ) to denote the Transformer network,\nwhere z is a word embedding, H is a sequence of\npast activations, and θ denotes the parameters of\nthe Transformer network.\nInitially, the inputs to the Transformer network\nare z1 and H0, where H0 is an empty sequence.\nThe Transformer network produces two outputs:\nthe final output g1 ∈ Rd and the activation h1 ∈\nR2N×d, 4 where d denotes the hidden size of the\nTransformer network andN is the number of layers\nof the Transformer network.\nFor subsequent inputs zt and Ht−1, where\nHt−1 = [h1, . . . ,ht−1], the computation is for-\nmally described as\ngt, ht = fLM(ezt , Ht−1), (1)\nwhere ezt denotes the word embedding of zt. To\nmake the notation simpler, we use the following\nequation to denote the repeated application of fLM\nover a sequence zi:j = [zi, . . . , zj] given past acti-\nvations A:\nGi:j, Hi:j = fLM(Zi:j, A), (2)\nwhere Zi:j = [ezi , . . . ,ezj ], Gi:j = [gi, . . . ,gj],\nand Hi:j = [hi, . . . ,hj].\n3We release our checkpoint at https://huggingface.\nco/THUMT/mGPT.\n4h is a concatenation of a set of key-value pairs\n{⟨k(i), v(i)⟩|i = 1. . . N} in the Transformer network.\np(e)1 p(e)2 x(i)1 x(i)2 x(i)3\nx(i+1)1 x(i+1)2 x(i+1)3\np(i)1 p(i)2\np(i+1)1 p(i+1)2\nx(i)1 x(i)2 x(i)3\nx(i+1)1 x(i+1)2 x(i+1)3\nPrompt Inputs\nFigure 2: A deep continuous prompt is prepended to the\ninputs in all attention layers, which affects the computa-\ntion of all attention layers. We do not distinguish keys\nand values here for simplicity.\nFinally, the conditional probability P(zt|z<t) is\nmodeled as follows:\nP(zt|z<t) = exp (eT\nzt · gt)\nP|V |\ni=1 exp (eTzi · gt)\n, (3)\nwhere |V | is the vocabulary size, and “·” denotes\nmatrix production.\n3 Multi-Stage Prompting\nWe propose multi-stage prompting (MSP), a sim-\nple and lightweight method for steering pre-trained\nLMs to translation tasks. We first describe the con-\ncept of deep continuous prompts in Section 3.1.\nThen we detail the stages and training objective\nin Section 3.2 and Section 3.3, respectively. Fi-\nnally, we describe the reparameterization of deep\ncontinuous prompts in Section 3.4.\n3.1 Deep Continuous Prompts\nWe adopt “continuous prompts” (Li and Liang,\n2021; Lester et al., 2021) instead of using textual\nprompts in our method. Using continuous prompts\nallows learning through differentiable methods like\nback-propagation (Lester et al., 2021). To be spe-\ncific, we use deep continuous prompts which are in\nthe same form as in (Li and Liang, 2021). Formally,\na prompt P is a sequence of L continuous vectors\n[p1, . . . ,pL]. Each vector pi (1 ≤ i ≤ L) is a con-\ncatenation of key-value pairs in all N Transformer\nlayers, which directly affect the computation of ev-\nery attention layer. Therefore, the dimension of pi\nis 2N × d. We give an illustration of conditioning\non a deep continuous prompt in Figure 2.\n3.2 Stages\nTo effectively mitigate the semantic and language\ngap between the pre-training and translation, we\n6133\np(e)1 p(e)2 mGPT\nx1 x2 x3 x4 x5\nh(e)1 h(e)2 h(e)3 h(e)4 h(e)5\nmGPT\nx1 x2 x3 x4 x5\np(r)1 p(r)2\nh(r)1 h(r)2 h(r)3 h(r)4 h(r)5\np(d)1 p(d)2\nmGPT\ny0 y1 y2 y3 y4\ny1 y2 y3 y4 </S>\nThe Encoding StageThe Re-Encoding StageThe Decoding Stage\nFigure 3: Detailed computations involved in the multi-stage prompting for machine translation tasks. We use\nrectangles to denote prompt vectors and rounded rectangles to denote activations.\npropose multi-stage prompting which divides the\nprocedure of using pre-trained LMs as translators\ninto three separate stages: the encoding, the re-\nencoding, and the decoding stages. Given different\nprompts at different stages, the pre-trained LM is\nexpected to behave differently during each stage\nand is more capable of generating translations.\nGiven a source sentence x = [x1, . . . , xS] and a\ntarget sentence y = [y1, . . . , yT ], the details of the\nthree stages are described as follows:\nThe Encoding Stage. At the encoding stage, the\npre-trained LM encodes the source sentence x into\na sequence of activations H1:S\ne by using an encod-\ning stage prompt Pe. This procedure is the same as\nbasic prompting. Formally, it can be described as\nfollows:\nG1:S\ne , H1:S\ne = fLM(X1:S, Pe). (4)\nThe Re-encoding Stage. At the re-encoding\nstage, the pre-trained LM produces fine-grained\nrepresentations of the source sentence by re-\nencoding x given past activations H1:S\ne and a re-\nencoding stage prompt Pr, which allows each rep-\nresentation to condition on all words in x. This\nprocedure can be described as\nG1:S\nr , H1:S\nr = fLM(X1:S, JPr; H1:S\ne K), (5)\nwhere JPr; H1:S\ne K denotes the concatenation of\ntwo sequences Pr and H1:S\ne . It is also possible\nto employ more than one re-encoding stage, allow-\ning the pre-trained LM to obtain further refined\nrepresentations of the source sentence.\nThe Decoding Stage. Finally, we obtain the hid-\nden vectors G1:T\nd for predicting the probability of\nthe target sentence y at the decoding stage, given\nthe refined source representations H1:S\nr and a de-\ncoding stage prompt Pd:\nG1:T\nd , H1:T\nd = fLM(Y 1:T , JPd; H1:S\nr K). (6)\nFigure 3 gives a detailed illustration of MSP. By\ndividing the translation process into multiple stages\nand applying different prompts, we expect the pre-\ntrained LM model can generate better translations.\n3.3 Training Objective\nWe use the cross-entropy loss for learning prompts.\nGiven G1:T\nd = [g(d)\n1 , . . . ,g(d)\nT ] in Eq. (6), the train-\ning objective is formally described as follows:\nL = − 1\nT\nTX\nt=1\nlog P(yt|y<t, x)\n= − 1\nT\nTX\nt=1\nlog exp (eT\nzt · g(d)\nt )\nP|V |\ni=1 exp (eTzi · g(d)\nt )\n.\n(7)\nNote that the parameters θ of the pre-trained LM\nare fixed during training.\n3.4 Reparameterization\nLi and Liang (2021) suggest that using a neural net-\nwork to reparameterize continuous prompts is more\nrobust to different choices of hyperparameters. In\ncontrast to their approach which uses an MLP net-\nwork to reparameterize continuous prompts, we in-\ntroduce a much simpler scaled reparameterization\nmethod, in which a continuous prompt is reparam-\neterized as a product of a learnable scalar and an\nembedding. More precisely, the reparameterization\nof the three prompts are as follows:\nPe = max(αe, 1.0) × ϕe, (8)\nPr = max(αr, 1.0) × ϕr, (9)\nPd = max(αd, 1.0) × ϕd, (10)\n6134\nwhere ϕe ∈ R2N×d, ϕr ∈ R2N×d, and ϕd ∈\nR2N×d. αe, αr, and αd are initialized to 1.0 at\nthe beginning of training. Therefore, the set of\ntrainable parameters ϕ in our method is ϕ =\n{αe, αr, αd, ϕe, ϕr, ϕd}, which contains much\nless tunable parameters than an MLP network.\nScaled reparameterization enables directly ad-\njusting the value of prompts by a tunable scaling\nfactor, leading to a much faster convergence with-\nout loss of performance. Further analysis is pre-\nsented in Section 4.7.\n4 Experiments\n4.1 Setup\nDatasets We conduct experiments on Romanian-\nEnglish (Ro-En), English-German (En-De), and\nEnglish-Chinese (En-Zh) translation tasks to ver-\nify our proposed method. For the Ro-En transla-\ntion task, we used the WMT16 Romanian-English\ndataset, which consists of 0.6M bilingual sentence\npairs and 2M back-translated sentence pairs.5 We\nused newsdev2016 as the development set and new-\nstest2016 as the test set. For the En-De translation\ntask, we used the WMT14 English-German dataset,\nwhich consists of 4.5M sentence pairs. The de-\nvelopment set is newstest2013 and the test set is\nnewstest2014. For the En-Zh translation task, we\nused the WMT20 English-Chinese dataset as the\ntraining corpus, which consists of 28M sentence\npairs. The development set is newstest2019 and the\ntest set is newstest2020. The details of preprocess-\ning and postprocessing are given in Appendix A.2.\nMetric. We used case-sensitive BLEU (Pap-\nineni et al., 2002) as the evaluation metric. The\nBLEU score is calculated using the SACRE BLEU\ntoolkit (Post, 2018).6\nBaselines. We used the mGPT model as the back-\nbone LM in all our experiments, which contains\n560M parameters. We compare our method with\nthe following prompting methods: 7\n• Prompt tuning (Lester et al., 2021). A prompt-\ning method that only prepends virtual tokens\nto the embedding layer of pre-trained LMs.\n5http://data.statmt.org/rsennrich/wmt16_\nbacktranslations/ro-en\n6Signature: nrefs:1|case:mixed|eff:no|tok:{13a,zh}|\nsmooth:exp|version:2.0.0\n7In our preliminary experiments, we also experimented\nwith the few-shot approach as described in (Brown et al.,\n2020). However, we found mGPT often failed to generate\nmeaningful translations.\n• Prefix-tuning (Li and Liang, 2021). A prompt-\ning method that uses deep continuous prompts,\nwhich prepend virtual tokens to all key-value\npairs in attention layers of pre-trained LMs.\nWe use an MLP network to reparameterize\na continuous prompt during training as sug-\ngested in (Li and Liang, 2021).\nImplementations. All our models are trained on\na machine with 8 RTX 3090Ti GPUs. For all\nprompting methods, we set the prompt length to\n128. For the training, we use the Glorot uniform\ninitilalizer (Glorot and Bengio, 2010) to initialize\ntunable parameters unless otherwise noted. We use\nAdam (Kingma and Ba, 2015) (β1 = 0.9, β2 = 0.98\nand ϵ = 1× 10−9) as the optimizer with a batch size\nof roughly 32K tokens. We use the same learning\nrate schedule as described in (Vaswani et al., 2017).\nThe number of warmup steps is set to 4K. We set\nthe maximum learning rate to 0.02 for prompt tun-\ning and MSP, and 7e-4 for prefix-tuning.8 We train\nprompts for a total of 80K steps for prompt tun-\ning and prefix-tuning, and 40K steps for MSP. For\nthe inference, we use the beam search algorithm\nto obtain translation from the mGPT model, and\nthe beam size is set to 4. The length penalty is\ndetermined by the results evaluated on the devel-\nopment set. We set the length penalty to 1.0 for\nthe En-Zh translation task and 0.0 for other transla-\ntion tasks. We implement our models on top of the\nTHUMT (Tan et al., 2020) toolkit and the Trans-\nformers library (Wolf et al., 2020).\n4.2 Main Results\nTable 1 shows the results for the Ro-En, En-De,\nand En-Zh translation tasks.\nAs the most parameter-efficient among the three\nprompting methods, prompt tuning introduces only\n131K parameters during training for each transla-\ntion task. However, it only achieves 9.4 BLEU\npoints on average over the three translation tasks.\nLester et al. (2021) indicate that language model\ncapacity is a key ingredient for prompt tuning to\nsucceed. As mGPT is a pre-trained LM with only\n560M parameters, the results coincide with the con-\nclusion of Lester et al. (2021).\nPrefix-tuning, which uses deep continuous\nprompts, achieves an average of 23.9 BLEU points\nover the three translation tasks. The results indicate\nthat using deep continuous prompts is beneficial\n8We found using a large learning rate for prefix-tuning\nwould result in unstable training.\n6135\nMethod #Params. Ro-En En-De En-Zh Average\nPrompt Tuning 131K 17.7 5.9 4.5 9.4\nPrefix-Tuning 26M 32.5 17.5 21.9 23.9\nMSP (Ours) 19M 34.7 21.2 28.1 28.0\nTable 1: BLEU score on three different translation tasks for different prompting methods. All prompting methods\nuse the same pre-trained language model “mGPT”. “#Params.” denotes the number of tunable parameters during\ntraining.\nLM Architecture #M-Params. Method BLEU\nmT5-XXL (Zhang et al., 2021) Encoder-Decoder 13B Finetuning 24.0\nCPM-2 (Zhang et al., 2021) Encoder-Decoder 11B Prompt Tuning 24.1\nCPM-2 (Zhang et al., 2021) Encoder-Decoder 11B Finetuning 26.2\nErnie 3.0 (Sun et al., 2021a) Encoder-Decoder 10B Finetuning 26.8\nmGPT (Ours) Decoder 560M MSP 28.1\nTable 2: Comparisons with previous studies on the WMT20 En-Zh translation task. “#M-Params.” indicates the\nnumber of parameters of pre-trained LMs.\nfor steering mGPT to translation tasks. However,\nintroducing deep continuous prompts inevitably re-\nquires more free parameters. The MLP network\nused in prefix-tuning introduces about 26M param-\neters for each translation task during training in our\nexperiments.\nFinally, MSP achieves 28.0 BLEU points on\naverage over the three translation directions and\noutperforms prompt tuning and prefix-tuning by\n18.6 and 4.1 BLEU points, respectively. MSP in-\ntroduces 19M parameters for each translation task\nduring training, which is more than prompt tuning\nbut less than prefix-tuning. MSP explicitly divides\nthe translation process using mGPT into separate\nstages, which are not present in prompt tuning and\nprefix-tuning. The results suggest that MSP is more\neffective in instructing pre-trained LMs to perform\ntranslation than prompt tuning and prefix-tuning.\n4.3 Comparison with Other LMs\nTable 2 gives the results of mT5-XXL (Zhang et al.,\n2021), CPM-2 (Zhang et al., 2021), Ernie 3.0 (Sun\net al., 2021a), and mGPT on the WMT20 En-Zh\ntranslation task. Except for mGPT, other LMs are\nbased on the encoder-decoder architecture. Despite\nusing a much smaller pre-trained LM with about\n5% parameters of mT5-XXL, CPM-2, and Ernie\n3.0, MSP achieves the best performance on the En-\nZh translation task. Therefore, we show that MSP\nis an efficient and effective approach to steering\npre-trained LMs to translation tasks.\n4.4 Comparison with Transformer\nWe compare our method with the state-of-the-\nart Transformer NMT model (Vaswani et al.,\n2017) 9 on the TedTalks dataset (Blackwood et al.,\n2018) and the WMT14 English-German dataset.\nTedTalks dataset is an English-centric multilingual\ncorpus including 59 languages with around 3K to\n200K sentence pairs per language pair. For the sake\nof simplicity, we only report results for 5 selected\nlanguages that contain more than 150K sentence\npairs. However, the Transformer model is trained\non all available parallel sentences covering 59 lan-\nguages, serving as a strong NMT baseline. For\nmGPT with MSP, we individually train the model\non each language pair following the same proce-\ndure described in this paper.\nThe results of “X→En” and “En→X” directions\nare shown in Table 3. Although mGPT with MSP\nis independently trained on each language pair,\nthe model still outperforms the strong multilingual\nNMT baseline by 3.4 and 3.9 BLEU points on\n“X-En” and “En-X” directions, respectively. The\nresults demonstrate that using pre-trained LMs as\ntranslators with an appropriate prompting method\nhas the potential to excel a strong Transformer\nNMT model.\nTable 4 shows the comparison between Trans-\nformer and our mGPT model with MSP on the En-\n9We used the transformer-big setting. Tokenizations and\nvocabularies are the same with mGPT for fair comparisons.\n6136\nModel #Params. Bg Es It Ru Tr Avg.\nX→En\nTransformer 437M 35.2 38.0 34.2 22.6 21.0 30.2\nmGPT (MSP) 19M 38.9 42.1 37.8 24.4 24.9 33.6\nEn→X\nTransformer 437M 29.2 34.0 29.2 16.7 11.6 24.1\nmGPT (MSP) 19M 34.1 38.4 32.8 19.2 15.6 28.0\nTable 3: Results on the TedTalks “X→En” and “En→X” translation directions. For MSP, each translation direction\nintroduces 19M parameters.\nModel #Params. BLEU\nTransformer (big) 450M 27.9\nmGPT (MSP) 19M 21.2\nTable 4: Results on the WMT14 En-De dataset.\n“#Params.” denotes the number of tunable parameters\nduring training.\nDe translation task. While there is still a noticeable\nperformance gap between Transformer and mGPT\nwith MSP, using mGPT as a translator with MSP\nis much more parameter-efficient than training a\nseparate NMT model. Supporting En-De transla-\ntion with mGPT only introduces 19M parameters\nwith MSP method. In comparison, the model size\nof the Transformer model for En-De translation\nis 450M. While mGPT model can perform other\ndownstream tasks by providing different prompts,\nsuch abilities have not been validated on the Trans-\nformer NMT model. Besides being efficient in disk\nspaces, learning prompts for the En-De translation\ntask are also faster than training a separate NMT\nmodel. It takes 21 hours to train prompts for MSP,\nwhereas 72 hours for training a Transformer model.\n4.5 Effect of Prompt Length\nFigure 4 shows the effect of prompt length for\nprefix-tuning and MSP. We omit the comparison to\nprompt tuning because of its inferior performance.\nWe found that using longer prompts generally leads\nto better performance for both prefix-tuning and\nMSP, but with diminishing returns. This finding\nis consistent with previous studies (Li and Liang,\n2021; Lester et al., 2021). Furthermore, MSP con-\nsistently outperforms prefix-tuning when using the\nsame prompt length. Even MSP with a prompt\nlength of 64 performs better than prefix-tuning with\na prompt length of 256 (19.0 vs. 18.2). The results\nfurther confirm that MSP is a better prompting\n64 128 192 256\n16\n18\n20\n22\n19.0\n21.2\n22.2 22.4\n14.8\n17.5\n18.2 18.2\nPrompt Length\nBLEU\nMulti-Stage Prompting\nPrefix-Tuning\nFigure 4: Comparison between MSP and prefix-tuning\non the WMT14 En-De translation task with different\nprompt lengths.\nmethod than prefix-tuning for steering pre-trained\nLMs to translation tasks. For the inference time,\nwe found longer prompts do not significantly affect\nthe decoding speed on GPUs as the computation of\nattention layers are highly parallel, which is also\nconsistent with the findings of Li and Liang (2021).\n4.6 Effect of Stages\nTable 5 shows the comparison of using differ-\nent stage settings on the WMT14 En-De and the\nWMT20 En-Zh translation tasks. For single-stage\nprompting, we also adopt scaled reparameterization\ninstead of MLP reparameterization for a fair com-\nparison. On the WMT14 En-De translation task,\nusing single-stage prompting achieves 17.9 BLEU\npoints. By comparison, explicitly separating en-\ncoding and decoding stages improve the translation\nperformance over single-stage prompting by 2.3\nBLEU points, which indicates the importance of\ndifferentiating stages. Adding a re-encoding stage\nfurther improves the translation performance by\n1.0 BLEU point, suggesting that the re-encoding\nstage is effective. Adding a second re-encoding\nstage further improves the translation performance\n6137\nMethod #Params. Training Inference En-De En-Zh\nSingle-stage 6.3M 14h 0.10 s/sent. 17.9 22.8\nTwo-stage (encoding/decoding) 12.6M 14h 0.10 s/sent. 20.2 25.2\n+ Re-encoding (default) 19.0M 21h 0.11 s/sent. 21.2 28.1\n+ 2nd Re-encoding 25.1M 29h 0.11 s/sent. 21.8 28.4\n+ Prompt sharing 6.3M 21h 0.11 s/sent. 19.8 24.5\nTable 5: Comparison of using different stage settings on the WMT14 En-De translation task and WMT20 Zh-En\ntranslation task. “#Params.” denotes the number of trainable parameters. “Training” denotes the total training\ntime. “Inference” denotes the inference speed measured on the test set using 8 GPUs. “s/sent.” denotes seconds per\nsentence. All experiments use scaled reparameterization for fair comparison.\n0 8 16 24 32 40 48 56 64 72 80\n10\n12\n14\n16\n18\n20\n22 21.1 21.1\nK Steps\nBLEU\nScaled Reparameterization\nw/o.Reparameterization\nFigure 5: Comparison between using scaled reparame-\nterization and without using reparameterization on the\nWMT14 translation task. The BLEU score is evaluated\non newstest2013.\nby 0.6 BLEU points. Although adding stages intro-\nduces more trainable parameters, it should be noted\nthat sharing a single prompt for the encoding/re-\nencoding/decoding stages also improves over the\nsingle-stage prompting by 1.9 BLEU points. The\nresults suggest that most improvements are at-\ntributed to the explicit separation of stages rather\nthan increased parameters. Adding more stages\ngenerally slows the training speed. However, we\ndo not observe notable inference speed drop as re-\nencoding stages are computed one time in parallel\nduring inference. On the En-Zh translation task, the\nresults are consistent with the results on the En-De\ntranslation task. Therefore, we conclude that using\nmore stages helps improve the translation quality.\n4.7 Effect of Reparameterization\nFigure 5 shows the comparison between MSP using\nscaled reparameterization and without using repa-\nrameterization. Using scaled reparameterization\nconverges faster than without using reparameteriza-\ntion. These two methods achieve nearly the same\nPrompt Distribution\nw/oprompt en (16%), ru (10%)\nPrefix-tuning zh (80%), ja (12%)\nMSP (encoding stage) en (51%), la (14%)\nMSP (re-encoding stage) en (24%), la (17%)\nMSP (decoding stage) zh (91%), ja (9%)\nTable 6: Language distribution of the free genera-\ntions using mGPT by conditioning on different prompts\nlearned by different prompting methods on the WMT20\nEn-Zh dataset.\ntranslation performance when the training is con-\nverged. As a result, using scaled reparameterization\ncan make the convergence much faster and reduce\nthe total training time.\n4.8 Analysis\nKnowledge. As continuous prompts are learned\nusing bilingual sentence pairs, an interesting ques-\ntion arises: Is the translation knowledge stored in\nthe continuous prompts or the pre-trained LM? To\nanswer this question, we discard the prompts and\nfeed the mGPT model a concatenation of a parallel\nsentence pair as an input, and calculate the cosine\nsimilarities between the source and target hidden\nactivations on each mGPT layer. We found that al-\nthough the prompts are not given, the nearest pairs\nof tokens between the source and target language\nfrequently turn out to coincide with bilingual align-\nments. This finding reveals to some extent that\nthe translation knowledge mainly resides in the\npre-trained LM instead of the learned continuous\nprompts, while the prompts play a role in guiding\nthe model to perform translation during generation.\nExamples are given in Appendix A.3.\nBottleneck. We study the bottleneck of the cur-\nrent prompting method. We train a separate Trans-\nformer encoder and an adapter network that directly\n6138\nmaps a source sentence into a deep continuous\nprompt, leaving the mGPT model only serving as\na decoder. This model introduces 378M tunable\nparameters and achieves 25.9 BLEU points on the\nWMT14 En-De translation task. Compared with\n21.2 BLEU points by MSP, the result shows that\nthere is still room to advance the translation perfor-\nmance of pre-trained LM by improving the prompt-\ning method, such as using dynamic prompts (Liu\net al., 2021) for each input sentence. However,\nas translation knowledge may come from the pre-\ntrained LM, the translation performance may be\nbottlenecked by the capability of the backbone LM.\nInterpretability. We did not find our learned\nprompts to be interpretable, which agrees with\nthe findings of Shin et al. (2020) and Lester et al.\n(2021). However, we do observe prompts of dif-\nferent stages changing the behavior of mGPT sig-\nnificantly. Specifically, we sample 100 examples\ngenerated from mGPT by providing prompts of dif-\nferent stages learned on the English-Chinese trans-\nlation task and identify the language ids of gen-\nerated texts using the langid toolkit. The top-2\nidentified language distributions of each generation\nare shown in Table 6. Without providing prompts,\nmGPT generates a random sentence from a random\nlanguage. By given continuous prompts learned by\nprefix-tuning, the mGPT mostly generates texts re-\nlated to Chinese. For MSP, it is noticeable that there\nis a transition from English to Chinese. mGPT\ngenerates English-related text given the encoding\nstage prompt. The distribution of languages be-\ncomes smoother when providing the prompt at the\nre-encoding stage. Finally, mGPT generates Chi-\nnese texts dominantly given the decoding stage\nprompt. The results coincide with our intuition that\nMSP helps the pre-trained LM to learn “smoother\ntransitions” to the translation task.\n5 Related Work\nPrompting. Brown et al. (2020) propose to use\na task description and a few examples to adapt\nthe GPT-3 model to downstream tasks, which is\nreferred to as in-context learning. Their prompts\nare manually designed. Gao et al. (2020) present\nLM-BFF for automatic prompts generation. They\nuse T5 model (Raffel et al., 2020) to generate tem-\nplates for prompting pre-trained LMs. Li and Liang\n(2021) propose prefix-tuning, which uses continu-\nous vectors as prompts. These prompts are trained\nusing task-specific data and optimized through\nback-propagation. Lester et al. (2021) propose\nprompt tuning, which is similar to prefix-tuning\nbut with fewer trainable parameters. Our method\nis also based on prompting. We use continuous\nprompts for steering PLMs to translation tasks. Un-\nlike Li and Liang (2021) and Lester et al. (2021)\nwho present general frameworks, our method is\nfocused on improving the translation performance\nof pre-trained LMs.\nUsing Pre-trained Models as Translators.\nStickland et al. (2021) investigate using BART and\nmBART models for machine translation tasks, their\napproach relies on adapter networks and finetuning\nparts of pre-trained LMs. Guo et al. (2020) build a\nnon-autoregressive NMT model by using a source\nBERT model as the encoder and a target BERT as\nthe decoder with adapter layers. Sun et al. (2021b)\npropose grafting a source BERT model and a target\nGPT model for translation tasks. Bapna and Firat\n(2019) propose using small adapter layers to adapt\na base NMT model to new translation tasks. All\nthese methods are adapter-based, which injects ad-\nditional tunable modules into the pre-trained mod-\nels. As a result, the pre-trained models lose the\nability to perform mixed-task inference. Our ap-\nproach is based on prompting, which only uses\nprompts for steering the pre-trained LMs to trans-\nlation tasks. Zhang et al. (2021) investigate using\nprompt tuning for steering CPM-2 model to the\nWMT20 English-Chinese translation task. Further-\nmore, their approach applied to encoder-decoder\narchitecture pre-trained LMs while ours applied to\ndecoder-only pre-trained LMs.\n6 Conclusion\nWe have presented multi-stage prompting, a\nmethod for making pre-trained language models\nbetter translators. Experiments show that with\nmulti-stage prompting, pre-trained LMs can gen-\nerate better translations, showing the potential of\nusing pre-trained LMs for translation tasks.\nAcknowledgements\nThis work was supported by the National Key R&D\nProgram of China (No. 2018YFB1005103), the\nNational Natural Science Foundation of China (No.\n62006138, No. 61925601), Institute Guo Qiang\nat Tsinghua University, and Huawei Noah’s Ark\nLab. We thank Kehai Chen for the discussion of\nthis work and all anonymous reviewers for their\nvaluable comments and suggestions on this work.\n6139\nReferences\nDzmitry Bahdanau, KyungHyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In Proceedings of\nICLR.\nAnkur Bapna and Orhan Firat. 2019. Simple, scalable\nadaptation for neural machine translation. In Pro-\nceedings of EMNLP, pages 1538–1548.\nGraeme Blackwood, Miguel Ballesteros, and Todd\nWard. 2018. Multilingual neural machine transla-\ntion with task-specific attention. In Proceedings of\nCOLING, pages 3112–3122.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In Advances in Neural Information Process-\ning Systems, volume 33, pages 1877–1901.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL, pages 4171–\n4186.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2020.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of ACL, pages 3816–3830.\nXavier Glorot and Yoshua Bengio. 2010. Understanding\nthe difficulty of training deep feedforward neural net-\nworks. In Proceedings of the thirteenth international\nconference on artificial intelligence and statistics ,\nvolume 9, pages 249–256.\nJunliang Guo, Zhirui Zhang, Linli Xu, Hao-Ran Wei,\nBoxing Chen, and Enhong Chen. 2020. Incorpo-\nrating BERT into Parallel Sequence Decoding with\nAdapters. In Advances in Neural Information Pro-\ncessing Systems, volume 33, pages 10843–10854.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proceedings\nof ICLR.\nPhilipp Koehn and Rebecca Knowles. 2017. Six chal-\nlenges for neural machine translation. InProceedings\nof the First Workshop on Neural Machine Translation,\npages 28–39.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of EMNLP , pages 3045–\n3059.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of ACL, pages 4582–4597.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\narXiv preprint arXiv:2107.13586.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: A method for automatic evalu-\nation of machine translation. In Proceedings of ACL,\npages 311–318.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation, pages 186–191.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. OpenAI blog.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV ,\nEric Wallace, and Sameer Singh. 2020. AutoPrompt:\nEliciting Knowledge from Language Models with\nAutomatically Generated Prompts. In Proceedings\nof EMNLP, pages 4222–4235.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-LM: Training multi-billion\nparameter language models using model parallelism.\narXiv preprint arXiv:1909.08053.\nAsa Cooper Stickland, Xian Li, and Marjan Ghazvinine-\njad. 2021. Recipes for adapting pre-trained monolin-\ngual and multilingual models to machine translation.\nIn Proceedings of EACL, pages 3440–3453.\nYu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding,\nChao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen,\nYanbin Zhao, Yuxiang Lu, et al. 2021a. Ernie 3.0:\nLarge-scale knowledge enhanced pre-training for lan-\nguage understanding and generation. arXiv preprint\narXiv:2107.02137.\nZewei Sun, Mingxuan Wang, and Lei Li. 2021b. Multi-\nlingual translation via grafting pre-trained language\nmodels. In Findings of EMNLP, pages 2735–2747.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nAdvances in Neural Information Processing Systems,\n27:3104–3112.\nZhixing Tan, Jiacheng Zhang, Xuancheng Huang, Gang\nChen, Shuo Wang, Maosong Sun, Huanbo Luan, and\nYang Liu. 2020. THUMT: An Open-Source Toolkit\nfor Neural Machine Translation. In Proceedings of\nAMTA, pages 116–122.\n6140\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30, pages 5998–6008.\nThomas Wolf, Julien Chaumond, Lysandre Debut, Vic-\ntor Sanh, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Morgan Funtowicz, Joe Davison, Sam\nShleifer, et al. 2020. Transformers: State-of-the-\nart natural language processing. In Proceedings of\nEMNLP: System Demonstrations, pages 38–45.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof NAACL, pages 483–498.\nZhengyan Zhang, Yuxian Gu, Xu Han, Shengqi Chen,\nChaojun Xiao, Zhenbo Sun, Yuan Yao, Fanchao Qi,\nJian Guan, Pei Ke, et al. 2021. CPM-2: Large-\nscale Cost-effective Pre-trained Language Models.\nAIOpen, 2:216–224.\nA Appendix\nA.1 Details of Multilingual GPT\nWe used a multilingual GPT (mGPT) (Radford\net al., 2019) model as the pre-trained language\nmodel in all our experiments. The mGPT model\nis trained using the Megatron-LM toolkit (Shoeybi\net al., 2019) 10 with the default GPT-2 configura-\ntion on the mC4 dataset (Xue et al., 2021),11 which\ncontains massive web crawled data covering 101\nlanguages. The model consists of 24 transformer\nlayers, and the hidden size d of the model is set to\n1,024. We used the same tokenization and vocab-\nulary as the mT5 model (Xue et al., 2021). The\nvocabulary size is 250,100. The total number of\nparameters of the mGPT model is about 560M. We\ntrain the mGPT model with a batch size of about\n512K tokens for 200K steps.\nA.2 Preprocessing and Postprocessing\nWe do not apply any additional preprocessing dur-\ning pre-training. Preprocessing like tokenization\nis done automatically with the sentencepiece pro-\ngram. For learning prompts, we do not apply ad-\nditional preprocessing on translation tasks except\nRomanian-English translation task, where we use a\nscript 12 to remove diacritics in the Romanian side.\n10https://github.com/NVIDIA/Megatron-LM\n11https://huggingface.co/datasets/mc4\n12https://github.com/rsennrich/wmt16-scripts/\nblob/master/preprocess/normalise-romanian.py\nBecause the mT5 tokenizer automatically uses Uni-\ncode NFKC normalization, which results in non-\nstandard punctuation for Chinese (e.g. “，”→ “,”).\nTherefore, for postprocessing, we use a rule-based\nmethod to replace non-standard punctuation with\nstandard counterparts for Chinese.\nA.3 Alignment Examples\nTable 7 provides examples of induced alignments\nfrom the mGPT model without using prompts. We\ncompute cosine similarities between target hidden\nkeys and source hidden keys of the 15th Trans-\nformer layer of the mGPT model and align the\ntarget word and the source word with the highest\ncosine similarity.\n6141\nEnglish \"They say there were boys around, that was not the case at all,\" he said.\nChinese 他表示：“他们说周围有好几个男孩子，但事实并非如此。”\nTokenized English_\" They _say _there _were _ boys _around , _that _was _not _the _case _at _all ,\" _he _said .\nTokenized Chinese_ 他表示:“他们说周围有好几个男孩 子, 但事实并非如此。”\nAlignments 他/_he 表示/_said :“/_\"他们/They说/_say周围/_around有/_were 好/boys\n几个/_were男孩/boys子/boys ,/, 但/_that事实/_case并非/_not如此/_all 。”/.\nEnglish Saudi Arabia To Offer Tourist Visas For First Time, Abolish Abaya Rule\nChinese 沙特阿拉伯首次提供旅游签证，废除阿巴亚长袍规定\nTokenized English_Saudi _Arabia _To _Offer _Tourist _Visa s _For _First _Time , _Ab olish _A baya _Rule\nTokenized Chinese_ 沙特阿拉伯首次提供旅游签证, 废除 阿巴亚长袍规定\nAlignments 沙/_Saudi特/_Arabia阿拉/_Arabia伯/_Arabia首次/_Offer提供/_Offer 旅游/_Tourist\n签证/_Visa ,/,废/olish除/olish阿/_Saudi巴/baya亚/baya长/_Rule 袍/_Visa规定/_Rule\nTable 7: Alignments induced from the mGPT model. We use “/” to separate Chinese and English tokens.\n6142",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8576709032058716
    },
    {
      "name": "Translation (biology)",
      "score": 0.6700341701507568
    },
    {
      "name": "Encoding (memory)",
      "score": 0.6661449670791626
    },
    {
      "name": "Natural language processing",
      "score": 0.6328564882278442
    },
    {
      "name": "Language model",
      "score": 0.6291119456291199
    },
    {
      "name": "Machine translation",
      "score": 0.6198477745056152
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5832480788230896
    },
    {
      "name": "Decoding methods",
      "score": 0.5145637392997742
    },
    {
      "name": "Process (computing)",
      "score": 0.5092557668685913
    },
    {
      "name": "Stage (stratigraphy)",
      "score": 0.42909735441207886
    },
    {
      "name": "Speech recognition",
      "score": 0.3959299325942993
    },
    {
      "name": "Programming language",
      "score": 0.1371574103832245
    },
    {
      "name": "Algorithm",
      "score": 0.06004151701927185
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210100255",
      "name": "Beijing Academy of Artificial Intelligence",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I29955533",
      "name": "Center for Information Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4401726859",
      "name": "Kuaishou (China)",
      "country": null
    }
  ]
}