{
  "title": "Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning",
  "url": "https://openalex.org/W4322759401",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4222365800",
      "name": "Yang, Antoine",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222162670",
      "name": "Nagrani, Arsha",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3168530371",
      "name": "Seo, Paul Hongsuck",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222365801",
      "name": "Miech, Antoine",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281795361",
      "name": "Pont-Tuset, Jordi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2742358871",
      "name": "Laptev, Ivan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3191045335",
      "name": "Sivic, Josef",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2565758713",
      "name": "Schmid, Cordelia",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4306177974",
    "https://openalex.org/W4229042118",
    "https://openalex.org/W2963541336",
    "https://openalex.org/W3217059257",
    "https://openalex.org/W3203711169",
    "https://openalex.org/W4312784228",
    "https://openalex.org/W3034815696",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W4287125738",
    "https://openalex.org/W4287113019",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W4287757777",
    "https://openalex.org/W2883910824",
    "https://openalex.org/W3172523222",
    "https://openalex.org/W3214448253",
    "https://openalex.org/W4312922092",
    "https://openalex.org/W4312683960",
    "https://openalex.org/W4224275552",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W4313190371",
    "https://openalex.org/W2963177403",
    "https://openalex.org/W2962799512",
    "https://openalex.org/W2968101724",
    "https://openalex.org/W2985144848",
    "https://openalex.org/W4312508181",
    "https://openalex.org/W4214663214",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2962677524",
    "https://openalex.org/W4283009931",
    "https://openalex.org/W4312247461",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3217102353",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W4312463400",
    "https://openalex.org/W2963753226",
    "https://openalex.org/W2962681491",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W2983918066",
    "https://openalex.org/W4310486995",
    "https://openalex.org/W4281390460",
    "https://openalex.org/W3168640669",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2607119937",
    "https://openalex.org/W3215626407",
    "https://openalex.org/W3174257385",
    "https://openalex.org/W3115868806",
    "https://openalex.org/W3167118264",
    "https://openalex.org/W4226289673",
    "https://openalex.org/W4229449886",
    "https://openalex.org/W2984008963",
    "https://openalex.org/W3035365026",
    "https://openalex.org/W2963351113",
    "https://openalex.org/W3204670646",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2519328139",
    "https://openalex.org/W4225005503",
    "https://openalex.org/W4312680644",
    "https://openalex.org/W4312359819",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W4287777632",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2997591391",
    "https://openalex.org/W4283066680",
    "https://openalex.org/W4221166856",
    "https://openalex.org/W2164290393",
    "https://openalex.org/W3035237998",
    "https://openalex.org/W3105232955",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W4312735840",
    "https://openalex.org/W3166893724",
    "https://openalex.org/W4298053470",
    "https://openalex.org/W2964214371",
    "https://openalex.org/W2896739098",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W4312910992",
    "https://openalex.org/W3035688398",
    "https://openalex.org/W3006320872",
    "https://openalex.org/W4283028531",
    "https://openalex.org/W2963811641",
    "https://openalex.org/W4226182655",
    "https://openalex.org/W4309181071",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W3184735396",
    "https://openalex.org/W2952132648",
    "https://openalex.org/W4221166385",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W3154596443",
    "https://openalex.org/W2951098185",
    "https://openalex.org/W4386076661",
    "https://openalex.org/W4282919422",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4225700096",
    "https://openalex.org/W4305007698",
    "https://openalex.org/W4221142658",
    "https://openalex.org/W4312271977",
    "https://openalex.org/W4313186260",
    "https://openalex.org/W2997706915",
    "https://openalex.org/W3204588463",
    "https://openalex.org/W3038476992",
    "https://openalex.org/W3159619744",
    "https://openalex.org/W2524365899",
    "https://openalex.org/W2597958930",
    "https://openalex.org/W2471143248",
    "https://openalex.org/W3215139360",
    "https://openalex.org/W3199245537",
    "https://openalex.org/W3178322352",
    "https://openalex.org/W2968104955",
    "https://openalex.org/W4320712030",
    "https://openalex.org/W4287116734",
    "https://openalex.org/W4313011746",
    "https://openalex.org/W4296406182",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W2556388456",
    "https://openalex.org/W2425121537",
    "https://openalex.org/W3097607319",
    "https://openalex.org/W3034727271",
    "https://openalex.org/W3105858579",
    "https://openalex.org/W2739107216",
    "https://openalex.org/W3193402170",
    "https://openalex.org/W2963916161",
    "https://openalex.org/W4225323055",
    "https://openalex.org/W2964216549",
    "https://openalex.org/W3184784418",
    "https://openalex.org/W3217799312",
    "https://openalex.org/W3173459793",
    "https://openalex.org/W3047922786",
    "https://openalex.org/W4320086121",
    "https://openalex.org/W3197457832",
    "https://openalex.org/W4312864639"
  ],
  "abstract": "In this work, we introduce Vid2Seq, a multi-modal single-stage dense event captioning model pretrained on narrated videos which are readily-available at scale. The Vid2Seq architecture augments a language model with special time tokens, allowing it to seamlessly predict event boundaries and textual descriptions in the same output sequence. Such a unified model requires large-scale training data, which is not available in current annotated datasets. We show that it is possible to leverage unlabeled narrated videos for dense video captioning, by reformulating sentence boundaries of transcribed speech as pseudo event boundaries, and using the transcribed speech sentences as pseudo event captions. The resulting Vid2Seq model pretrained on the YT-Temporal-1B dataset improves the state of the art on a variety of dense video captioning benchmarks including YouCook2, ViTT and ActivityNet Captions. Vid2Seq also generalizes well to the tasks of video paragraph captioning and video clip captioning, and to few-shot settings. Our code is publicly available at https://antoyang.github.io/vid2seq.html.",
  "full_text": "Vid2Seq: Large-Scale Pretraining of a Visual Language Model\nfor Dense Video Captioning\nAntoine Yang†* Arsha Nagrani§ Paul Hongsuck Seo§ Antoine Miech♯\nJordi Pont-Tuset§ Ivan Laptev† Josef Sivic¶ Cordelia Schmid§\n§Google Research †Inria Paris and D´epartement d’informatique de l’ENS, CNRS, PSL Research University\n♯ DeepMind ¶Czech Institute of Informatics, Robotics and Cybernetics at the Czech Technical University in Prague\nhttps://antoyang.github.io/vid2seq.html\nLarge-scale pretrainingfrom narrated videosDense video captioning\nHey guys today I am going to teach you how to skiThe kids make it look easyFirst slope, congratz!\nVid2Seq<1s><8s>The man is fastening the dog. <20s><50s>The dogs are pullingthe sled. <45s><49s>The man is saying hello.\nFigure 1. Vid2Seq is a visual language model that predicts dense event captions together with their temporal grounding in the video by\ngenerating a single sequence of tokens (right). This ability is enabled by large-scale pretraining on unlabeled narrated videos (left).\nAbstract\nIn this work, we introduce Vid2Seq, a multi-modal\nsingle-stage dense event captioning model pretrained on\nnarrated videos which are readily-available at scale. The\nVid2Seq architecture augments a language model with spe-\ncial time tokens, allowing it to seamlessly predict event\nboundaries and textual descriptions in the same output se-\nquence. Such a unified model requires large-scale training\ndata, which is not available in current annotated datasets.\nWe show that it is possible to leverage unlabeled narrated\nvideos for dense video captioning, by reformulating sen-\ntence boundaries of transcribed speech as pseudo event\nboundaries, and using the transcribed speech sentences as\npseudo event captions. The resulting Vid2Seq model pre-\ntrained on the YT-Temporal-1B dataset improves the state\nof the art on a variety of dense video captioning bench-\nmarks including YouCook2, ViTT and ActivityNet Captions.\nVid2Seq also generalizes well to the tasks of video para-\ngraph captioning and video clip captioning, and to few-shot\nsettings. Our code is publicly available at [1].\n1. Introduction\nDense video captioning requires the temporal localiza-\ntion and captioning of all events in an untrimmed video [46,\n102, 131]. This differs from standard video captioning [63,\n70, 80], where the goal is to produce a single caption for\na given short video clip. Dense captioning is significantly\nmore difficult, as it raises the additional complexity of lo-\ncalizing the events in minutes-long videos. However, it also\nbenefits from long-range video information. This task is\n*This work was done when the first author was an intern at Google.\npotentially highly useful in applications such as large-scale\nvideo search and indexing, where the video content is not\nsegmented into clips.\nExisting methods mostly resort to two-stage ap-\nproaches [37, 46, 100], where events are first localized and\nthen captioned. To further enhance the inter-task inter-\naction between event localization and captioning, some\napproaches have introduced models that jointly solve the\ntwo tasks [20, 102, 131]. However, often these approaches\nstill require task-specific components such as event coun-\nters [102]. Furthermore, they exclusively train on manu-\nally annotated datasets of limited size [35, 46, 130], which\nmakes it difficult to effectively solve the task. To address\nthese issues, we take inspiration from recent sequence-\nto-sequence models pretrained on Web data which have\nbeen successful on a wide range of vision and language\ntasks [4, 11, 13, 105, 117].\nFirst, we propose a video language model, called\nVid2Seq. We start from a language model trained on Web\ntext [78] and augment it with special time tokens that repre-\nsent timestamps in the video. Given video frames and tran-\nscribed speech inputs, the resulting model jointly predicts\nall event captions and their corresponding temporal bound-\naries by generating a single sequence of discrete tokens, as\nillustrated in Figure 1 (right). Such a model therefore has\nthe potential to learn multi-modal dependencies between\nthe different events in the video via attention [94]. However\nthis requires large-scale training data, which is not avail-\nable in current dense video captioning datasets [35,46,130].\nMoreover, collecting manual annotations of dense captions\nfor videos is expensive and prohibitive at scale.\n1\narXiv:2302.14115v2  [cs.CV]  21 Mar 2023\nHence we propose to pretrain Vid2Seq by leveraging un-\nlabeled narrated videos which are readily-available at scale.\nTo do this, we reformulate sentence boundaries of tran-\nscribed speech as pseudo event boundaries, and use the tran-\nscribed speech sentences as pseudo event captions. We then\npretrain Vid2Seq with a generative objective, that requires\npredicting the transcribed speech given visual inputs, and\na denoising objective, which masks spans of transcribed\nspeech. Note that transcribed speech may not describe the\nvideo content faithfully, and is often temporally misaligned\nwith the visual stream [32, 43, 71]. For instance, from the\nexample in Figure 1 (left), one can understand that the grey\nskier has descended a slope from the last speech sentence\nwhich is said after he actually descended the slope. Intu-\nitively, Vid2Seq is particularly suited for learning from such\nnoisy supervision as it jointly models all narrations and the\ncorresponding timestamps in the video.\nWe demonstrate the effectiveness of our pretrained\nmodel through extensive experiments. We show the im-\nportance of pretraining on untrimmed narrated videos, the\nability of Vid2Seq to use both the visual and speech modali-\nties, the importance of the pretraining objectives, the benefit\nof joint caption generation and localization, as well as the\nimportance of the language model size and the scale of the\npretraining dataset. The pretrained Vid2Seq model achieves\nstate-of-the-art performance on various dense video cap-\ntioning benchmarks [35, 46, 130]. Our model also excels\nat generating paragraphs of text describing the video: with-\nout using ground-truth event proposals at inference time,\nour model outperforms all prior approaches including those\nthat rely on such proposals [50,76,128]. Moreover, Vid2Seq\ngeneralizes well to the standard task of video clip cap-\ntioning [9, 109]. Finally, we introduce a new few-shot\ndense video captioning setting in which we finetune our pre-\ntrained model on a small fraction of the downstream train-\ning dataset and show benefits of Vid2Seq in this setting.\nIn summary, we make the following contributions:\n(i) We introduce Vid2Seq for dense video captioning.\nGiven multi-modal inputs (transcribed speech and video),\nVid2Seq predicts a single sequence of discrete tokens that\nincludes caption tokens interleaved with special time to-\nkens that represent event timestamps. (ii) We show that\ntranscribed speech and corresponding timestamps in unla-\nbeled narrated videos can be effectively used as a source\nof weak supervision for dense video captioning. (iii) Fi-\nnally, our pretrained Vid2Seq model improves the state of\nthe art on three dense video captioning datasets (YouCook2,\nViTT, ActivityNet Captions), two video paragraph caption-\ning benchmarks (YouCook2, ActivityNet Captions) and two\nvideo clip captioning datasets (MSR-VTT, MSVD), and\nalso generalizes well to few-shot settings.\nOur code implemented in Jax and based on the Scenic\nlibrary [19] is publicly released at [1].\n2. Related Work\nDense video captioning.Dense video captioning lies at the\nintersection of event localization [25, 29, 33, 62, 65, 66, 84,\n127] and event captioning [30,63,75,97,104]. The majority\nof existing methods for dense video captioning [37, 38, 46,\n100, 103] consist of a temporal localization stage followed\nby an event captioning stage. To enrich inter-task interac-\ntions, recent works [8,10,20,61,73,79,82,83,100,102,131]\njointly train the captioning and localization modules. In\nparticular, Wang et al. [102] propose to view dense video\ncaptioning as a set prediction task, and jointly perform\nevent localization and captioning for each event in paral-\nlel. In contrast, our model generates event boundaries and\ncaptions conditioned on the previously generated events.\nDeng et al. [20] propose to first generate a paragraph and\nthen ground each sentence in the video. We also generate\nall captions as a single output sequence, however our out-\nput already includes event timestamps. Zhang et al. [125]\npropose to generate event boundaries sequentially, but sep-\narately perform event localization and single event caption-\ning, and only use visual input. Most related to our work,\nZhu et al . [133] also perform dense video captioning by\ngenerating a single output sequence. Their method, how-\never, infers event locations directly from the timestamps of\ntranscribed speech and, hence, can only detect events that\nclosely follow the speech. In contrast, our model generates\nevent timestamps as special tokens and can produce dense\ncaptions for videos with limited speech, as we demonstrate\non the ActivityNet Captions dataset.\nVideo and language pretraining.Following the success of\nimage-text pretraining [14, 21, 23, 24, 28, 34, 36, 39–41, 53–\n55,58,60,68,69,85,88,92,93,99,118–120,124,129], recent\nworks have explored video-text pretraining [3–5, 26, 31, 32,\n43,49,52,56,71,72,74,80,81,89,96,98,108,111,111–115,\n121, 122]. These methods show strong improvements on\nvarious tasks such as text-video retrieval [5,71], video ques-\ntion answering [112, 122] and video clip captioning [4, 80].\nWhile these works mostly learn global video representa-\ntions to tackle video-level prediction tasks, we here fo-\ncus on learning detailed representations to address a dense\nprediction task requiring reasoning over multiple events in\nuntrimmed videos. Several works have explored long-form\nvideo-text pretraining [90] and video-text pretraining for\ntemporal localization tasks [7, 48, 64, 106, 110, 116]. How-\never these works focus on video understanding tasks while\nour pretraining approach is tailored for a generative task that\nnot only requires the model to reason over multiple events\nin the video, but also to describe them by natural language.\nA few works explore pretraining for dense video cap-\ntioning. Zhang et al . [125] pretrain on ActivityNet Cap-\ntions to improve the downstream performance on the same\ndataset. In contrast, we propose a pretraining method that\ndoes not rely on any manual annotation, and show its ben-\n2\n<6>  <10>    Pleasestaycalm!    <86>    <92>   Hey     myfriend! [BOS]    <1>      <17>     The    man    is <89>     <98>      The    man    issayinghello. \nTransformer TextDecoderht\nDecoderTokenEmbedderhs\nOutput eventsequencez\nTranscribedspeech sequence\nInput videoframes  x\nEncoder TokenEmbeddergsSpatial Encoder f s\n... \n<1>    <17>    The      man     isfastening… <98>     The      man     issayinghello. [EOS]Visual and speech embeddings[xt, yt]Visual tokenTime tokenTexttoken\n… …staycalmman          ishey  <0> <1>       <99>\n... \n... \n... \nInput transcribedspeech3.02s   à4.99s: Pleasestaycalm!42.87s à45.97s: Hey myfriend! \nOutput dense eventcaptions0.50s à8.53s: The man isfasteningthe dog. 20.08s à49.70s: The dogsare pullingthe sled. 44.68s à49.20s: The man issayinghello.\nText+ Time TokenizationTime TokenizationVideo timeline, durationT=49.70squantizedin N=100bins s1=3.02s tstart1=s1×\"#=$.&'×(&&)*.+&=6\nVisual Encoder  f\nTextEncoder  g\nTextDecoderh\nxt1\nTemporal Encoder  f t Transformer TextEncoder  gt\n LanguageModeling Head hl\nText+ Time  Tokens\nxt2 xtF-1 xtF yt1yt2 yt3 yt4 yt5 yt6 yt7 yt8yt9 yts\nxs1 xs2 xsF-1 xsF ys1ys2 ys3 ys4 ys5 ys6 ys7 ys8ys9 yss zs1 zs2 zs3 zs4 zs5zs6 zsL-6 zsL-5 zsL-4zsL-3zsL-2 zsL-1 zsL\nFigure 2. Vid2Seq model overview. We formulate dense event captioning as a sequence-to-sequence problem, using special time tokens\nto allow the model to seamlessly understand and generate sequences of tokens containing both textual semantic information and temporal\nlocalization information grounding each text sentence in the video. In detail, all input video frames x and the transcribed speech sequence\ny are first processed with a Visual Encoder f (a frozen Spatial Encoder fs followed by a Temporal Encoder ft) and a Text Encoder g (a\nToken Embedder gs followed by a Transformer Encoder gt), respectively. Then the Text Decoder h (composed of a Token Embedder hs,\na Transformer Encoder ht and a Language Modeling Head hl) autoregressively generates the output event sequence z by cross-attending\nto the visual and speech embeddings xt and yt.\nefits on multiple downstream datasets. Huang et al . [35]\nexplore pretraining on narrated instructional videos, but\nonly consider event captioning using ground truth propos-\nals as their model does not handle localization. Finally,\n[35, 133] explore pretraining on a domain specific text-only\ndataset [45]. In contrast, we propose to pretrain on a generic\nvideo corpus [121] and show benefits on various domains.\nUnifying tasks as language modeling.Recent works [11–\n13, 15, 17, 44, 59, 101, 117, 132] have shown that it is possi-\nble to cast various computer vision problems as a language\nmodeling task, addressing object detection [11], grounded\nimage captioning [117] or visual grounding [132]. In this\nwork we also cast visual localization as a language mod-\neling task. However, unlike prior work focused on image-\nlevel spatial localization, we address the different problem\nof event localization in time, in untrimmed videos.\n3. Method\nThe goal of dense video captioning is to temporally lo-\ncalize and describe with natural language all events in an\nuntrimmed input video. Therefore a key challenge is to\neffectively model the relationships between the different\nevents in the video, as for example, it is easier to predict\nthat the dogs are pulling the sled if we know that the man\nhas just fastened a dog (see Figure 1 (right)). Furthermore,\ndue to the dense nature of the task, there can be many events\nin a long video and the requirement is to output a natural\nlanguage caption for each event. Hence, another key chal-\nlenge is that the manual collection of annotations for this\ntask is particularly expensive. To tackle these challenges,\nwe first develop a unified multi-modal model that jointly\npredicts event boundaries and captions as a single sequence\nof tokens, as explained in Section 3.1 and Figure 2. Second,\nwe design a pretraining strategy that effectively leverages\ncross-modal supervision in the form of transcribed speech\nfrom unlabeled narrated videos by reformulating sentence\nboundaries as pseudo event boundaries, as presented in Sec-\ntion 3.2 and Figure 3.\n3.1. Model\nWe wish to design a model for dense video captioning\nthat can capture relationships between events using visual\nand (transcribed) speech cues in order to effectively local-\nize and describe these events in untrimmed minutes-long\nvideos. To tackle this challenge, we cast dense video cap-\ntioning as a sequence-to-sequence problem where the input\nand output sequences contain both the semantic information\nabout the event in the form of natural language descriptions\nand the temporal localization of the events in the form of\ntemporal timestamps. In addition, to best leverage both the\nvisual and the language signal, we develop an appropriate\nmulti-modal encoder-decoder architecture. As illustrated\n3\nin Figure 2, our architecture takes as input video frames\nx = {xi}F\ni=1 together with the transcribed speech sequence\ny = {yj}S\nj=1. The output of our model is an event sequence\nz = {zk}L\nk=1, where each event contains both its textual\ndescription and timestamps corresponding to the temporal\nevent locations in the video. Below we explain the structure\nof the transcribed speech and event sequences constructed\nfor our model as well as details of our model architecture.\nSequence construction. To model inter-event relation-\nships in dense event captioning annotations (or the readily-\navailable transcribed narration, see Section 3.2), we cast\ndense video captioning as predicting a single output se-\nquence of tokens z. This output event sequence is con-\nstructed by leveraging a text tokenizer augmented with spe-\ncial time tokens. Furthermore, we enable our architecture\nto jointly reason about the semantic and temporal informa-\ntion provided in the transcript of the input narration by con-\nstructing the input transcript sequencey in a similar manner\nas the event sequence z. Details are given next.\nTime tokenization. We start from a text tokenizer with a\nvocabulary size V , and augment it with N additional time\ntokens, resulting in a tokenizer with V + N tokens. The\ntime tokens represent relative timestamps in a video, as we\nquantize a video of durationT into N equally-spaced times-\ntamps. In detail, we use the SentencePiece tokenizer [47]\nwith vocabulary size V = 32, 128 and N = 100.\nEvent sequence. Our introduced tokenizer enables us to\nconstruct sequences that contain both video timestamps and\ntext video descriptions. We next explain how we con-\nstruct the output event sequence z. Note that videos have\na variable number of events in standard dense video cap-\ntioning datasets [35, 46, 130]. Each event k is character-\nized by a text segment, a start time and an end time. We\nfirst construct for each event k a sequence by concatenat-\ning its start time token tstartk , its end time token tendk and\nits text tokens [zk1 , ..., zklk\n]. Then we order all these se-\nquences in increasing order of their start times and con-\ncatenate them. In practice, each text segment ends with\na dot symbol indicating the separation between different\nevents. Finally, the event sequence is obtained by prepend-\ning and appending a BOS and an EOS tokens to indi-\ncate the start and the end of sequence, respectively, i.e.\nz = [BOS, tstart1 , tend1 , z11 , ..., z1l1\n, tstart2 , ..., EOS].\nTranscribed speech sequence.To enable the model to use\nboth the transcribed speech and its corresponding times-\ntamps, we convert the speech transcript into a speech se-\nquence y similarly as the input training dense event captions\nz. This is done by segmenting the raw speech transcript into\nsentences with the Google Cloud API1, and using each tran-\nscribed speech sentence with its corresponding timestamps\nanalogously as an event in the previously explained process.\n1https://cloud.google.com/speech-to-text/docs/automatic-punctuation.\nArchitecture. We wish to design an architecture that can\neffectively model relationships between different events in\nuntrimmed minutes-long videos. To tackle this challenge,\nwe propose a multi-modal encoder-decoder architecture, il-\nlustrated in Figure 2, that gradually refines and outputs\nthe event sequence described above. In detail, given an\nuntrimmed minutes-long video, the visual encoder f em-\nbeds its frames while the text encoder g embeds transcribed\nspeech and the corresponding timestamps. Then a text de-\ncoder h predicts event boundaries and text captions using\nthe visual and transcribed speech embeddings. The individ-\nual modules are described next.\nVisual encoder.The visual encoder operates on a sequence\nof F frames x ∈ RF×H×W×C where H, W and C are the\nheight, width and the number of channels of each frame. A\nvisual backbone fs first encodes each frame separately and\noutputs frame embeddings xs = fs(x) ∈ RF×d, where\nd is the embedding dimension. Then a transformer en-\ncoder [94] ft models temporal interactions between the dif-\nferent frames, and outputs F contextualized visual embed-\ndings xt = ft(xs + xp) ∈ RF×d, where xp ∈ RF×d are\nlearnt temporal positional embeddings, which communicate\ntime information from visual inputs to the model. In de-\ntail, the visual backbone is CLIP ViT-L/14 [22, 77] at res-\nolution 224 × 224 pixels, pretrained to map images to text\ndescriptions with a contrastive loss on Web-scraped image-\ntext pairs. We keep the backbone frozen for efficiency.\nText encoder. The text encoder operates on a transcribed\nspeech sequence of S tokens y ∈ {1, ..., V+N}S, where V\nis the text vocabulary size,N is the size of the vocabulary of\ntime tokens and S is the number of tokens in the transcribed\nspeech sequence. Note that the transcribed speech sequence\nincludes time tokens to input the temporal information from\nthe transcribed speech into the model. An embedding\nlayer gs ∈ R(V +N)×d embeds each token independently\nand outputs semantic embeddings ys = gs(y) ∈ RS×d.\nThen a transformer encoder gt computes interactions in the\ntranscribed speech sequence and outputs S contextualized\nspeech embeddings yt = gt(ys) ∈ RS×d.\nText decoder. The text decoder generates the event se-\nquence z by using the encoder embeddings, which are ob-\ntained by concatenating the visual and speech embeddings\nxt and yt. The text decoder is based on a causal transformer\ndecoder ht that cross-attends to the encoder outputs, and\nat each autoregressive step k, self-attends to the previously\ngenerated tokens ˆzt\n<k to output a contextualized representa-\ntion zt\nk = ht(hs(ˆzt\n<k), xt, yt) ∈ Rd where hs ∈ R(V +N)×d\nis the decoder token embedding layer. Then a language\nmodeling head hl ∈ Rd×(V +N) predicts a probability dis-\ntribution over the joint vocabulary of text and time tokens\nin order to predict the next token in the event sequence, i.e.\nzl\nk = hl(zt\nk) ∈ RV +N .\nText initialization. We initialize the text encoder and the\n4\nCorruptedtranscribedspeech sequence<3><5> Hey guys today I am going to [X] … <93> [Y] congratz!\nInput videoframes\n[X] teachyouhow to ski [Y] <96> First slope[Z]\nRecoveredtranscribedspeech sequence\nTranscribedspeech sequence<3><5> Hey guys today I am going to teach you how to… <93><96> First slope, congratz!\nGenerativeTask\nDenoisingTask\nVid2Seq\nVid2Seq\nFigure 3. Pretraining tasks. To train Vid2Seq on unlabeled nar-\nrated videos, we design two pretraining objectives. Top: gener-\native objective, given visual inputs x only, the task is to generate\nthe transcribed speech sequence y. Bottom: denoising objective,\ngiven visual inputsx and the corrupted speech sequence˜y, the task\nis to generate the sequence of recovered speech segments ¯y.\ntext decoder with T5-Base [78] which has been pretrained\non Web text corpora with a denoising loss. Therefore\ntheir implementation and parameters also closely follow\nT5-Base, e.g. they use relative positional embeddings and\nshare their token embedding layer gs = hs ∈ R(V +N)×d.\n3.2. Training\nIn this Section, we describe how we leverage a large\namount of unlabeled narrated videos to train the previously\ndescribed dense event captioning model. We first present\nthe pretraining method used to effectively train Vid2Seq\nusing cross-modal supervision in readily-available narrated\nvideos in Section 3.2.1 and Figure 3. Then we explain how\nwe finetune our architecture for various downstream tasks\nincluding dense event captioning in Section 3.2.2.\n3.2.1 Pretraining on untrimmed narrated videos\nWe wish to leverage narrated videos for pretraining as they\nare easily available at scale [72,121]. However these videos\ndo not contain dense event captioning annotations. There-\nfore we use as supervisory signal the transcribed speech\nsentences and their corresponding timestamps. As speech\ntranscripts are not always visually grounded and often tem-\nporally misaligned [32, 43, 71], we note that they only pro-\nvide weak supervision. Furthermore, speech transcripts\ndrastically differ from dense event captioning annotations.\nFor instance, in the YT-Temporal-1B dataset [121], a video\ncontains 120 speech sentences on average which is an order\nof magnitude more than the number of events in standard\ndense video captioning datasets [35, 46, 130]. Our Vid2Seq\nmodel is particularly suitable for using such weak super-\nvision as it constructs the speech sequence similarly as a\nmanually annotated event sequence, and jointly contextual-\nizes the speech boundaries and semantic information on the\nlevel of potentially minutes-long videos (see Section 3.1)\nrather than at a shorter clip-level, enabling our model to\nlearn long-term relationships between the different speech\nsegments: in experiments we show that pretraining on en-\ntire minutes-long videos is highly beneficial.\nWe next describe the two proposed training objectives,\nwhich are both based on a maximum likelihood objective.\nFormally, given visual inputsx, encoder text sequencey and\na decoder target text sequence z, both objectives are based\non minimizing the following loss:\nLθ(x, y, z) = − 1PL−1\nk=1 wk\nL−1X\nk=1\nwk log pθ(zk+1|x, y, z1:k),\n(1)\nwhere L is the length of the decoder target sequence, wk is\nthe weight for k-th token in the sequence, which we set to\nwk = 1 ∀k in practice, θ denotes the trainable parameters in\nthe model and pθ is the output probability distribution over\nthe vocabulary of text and time tokens.\nGenerative objective. This objective uses the transcribed\nspeech as a (pseudo-)supervisory signal to teach the decoder\nto predict a sequence of events given visual inputs. Given\nvideo frames x, which are fed to the encoder, the decoder\nhas to predict the transcribed speech sequence y (see Fig-\nure 3), which serves as a proxy dense event captioning an-\nnotation. Note that no text input is given to the encoder for\nthis task as using transcribed speech both as input and target\nwould lead the model to learn text-only shortcuts.\nDenoising objective. As no text input is given to the en-\ncoder for the generative proxy task, the generative objective\nonly trains the visual encoder and the text decoder, but not\nthe text encoder. However when our model is used for dense\nvideo captioning, the text encoder has a significant impor-\ntance as it encodes speech transcripts. Hence we introduce\na denoising objective that aims at jointly aligning the visual\nencoder, the text encoder and the text decoder. Inspired by\nT5 [78] in the text domain, we randomly mask spans of (text\nand time) tokens in the transcribed speech sequence with a\nprobability P and an average span length M. The encoder\ninput is composed of the video frames x together with the\ncorrupted speech sequence ˜y, which contains sentinel to-\nkens that uniquely identify the masked spans. The decoder\nthen has to predict a sequence ¯y constructed with the cor-\nresponding masked spans for each sentinel token, based on\nvisual inputs x and speech context ˜y (see Figure 3).\n3.2.2 Downstream task adaptation\nOur architecture and task formulation enables us to tackle\ndense video captioning with a generic language modeling\ntraining objective and inference procedure. Note that as a\nby-product of our generic architecture, our model can also\nbe used to generate paragraphs about entire videos by sim-\nply removing the time tokens from the output sequence, and\ncan also be easily adapted to video clip captioning with the\nsame finetuning and inference recipe.\nFinetuning. To finetune our model for dense video cap-\ntioning, we use a maximum likelihood objective based on\nthe event sequence (see Equation 1). Given video frames\nx and speech transcripts y, the decoder has to predict the\nevent sequence z.\n5\nInference. The text decoder autoregressively generates the\nevent sequence by sampling from the model likelihood. In\npractice, we use beam search as we find that it improves\nthe captioning quality compared with argmax sampling or\nnucleus sampling. Finally, the event sequence is converted\ninto a set of event predictions by simply reversing the se-\nquence construction process.\n4. Experiments\nThis section demonstrates the effectiveness of our pre-\ntrained Vid2Seq model and compares our method to the\nstate of the art. We first outline our experimental setup in\nSection 4.1. We then present ablation studies in Section 4.2.\nThe comparison to the state of the art in dense video cap-\ntioning, video paragraph captioning and video clip caption-\ning is presented in Section 4.3. Next, we present results in a\nnew few-shot dense video captioning setting in Section 4.4.\nFinally, we show qualitative results in Section 4.5.\n4.1. Experimental setup\nDatasets. For pretraining, following prior work show-\ning the benefits of pretraining on a diverse and large\ndataset [122], we use the YT-Temporal-1B dataset [121],\nwhich includes 18 million narrated videos collected from\nYouTube. We evaluate Vid2Seq on three downstream dense\nvideo captioning datasets: YouCook2 [130], ViTT [35] and\nActivityNet Captions [46]. YouCook2 has 2K untrimmed\nvideos of cooking procedures. On average, each video\nlasts 320s and is annotated with 7.7 temporally-localized\nsentences. ViTT consists of 8K untrimmed instructional\nvideos. On average, each video lasts 250s and is annotated\nwith 7.1 temporally-localized short tags. ActivityNet Cap-\ntions contains 20k untrimmed videos of various human ac-\ntivities. On average, each video lasts 120s and is annotated\nwith 3.7 temporally-localized sentences. For video clip cap-\ntioning, we use two standard benchmarks,MSR-VTT [109]\nand MSVD [9]. For all datasets, we follow the standard\nsplits for training, validation and testing. Note that we only\nuse videos available on YouTube at the time of the work, re-\nsulting in 10 to 20% less videos than in the original datasets.\nImplementation details.We extract video frames at 1FPS,\nand subsample or pad the sequence of frames to F frames\nwhere we set F = 100. The text encoder and decoder se-\nquence are truncated or padded to L = S = 1000 tokens.\nOur model has 314M trainable parameters. We use the\nAdam optimizer [42]. We pretrain our model for 200,000\niterations with a batch size of 512 videos split on 64 TPU\nv4 chips, which lasts a day. We sum both pretraining objec-\ntives with equal weighting to get our final pretraining loss.\nMore details are included in Appendix Section B.\nEvaluation metrics. For captioning, we use CIDEr [95]\n(C) and METEOR [6] (M). For dense video captioning,\nwe follow the commonly used evaluation tool [46] which\ncalculates matched pairs between generated events and the\nPretraining input YouCook2 ActivityNet\nUntrimmed Time tokensS C F1 S C F1\n1. No pretraining 4.0 18.0 18.1 5.4 18.8 49.2\n2. ✗ ✗ 5.5 27.8 20.5 5.5 26.5 52.1\n3. ✓ ✗ 6.7 35.0 23.3 5.6 27.4 52.2\n4. ✓ ✓ 7.9 47.1 27.3 5.8 30.1 52.4\nTable 1. Ablation showing the impact of using untrimmed\nvideos and adding time tokens during pretraining.When we\nuse untrimmed video-speech inputs, time information from tran-\nscribed speech sentence boundaries is integrated via time tokens.\nground truth across IoU thresholds of {0.3, 0.5, 0.7, 0.9 },\nand compute captioning metrics over the matched pairs.\nHowever, these metrics do not take into account the story\nof the video. Therefore we also use SODA c [27] (S) for an\noverall dense video captioning evaluation. To further isolate\nthe evaluation of event localization, we report the average\nprecision and average recall across IoU thresholds of {0.3,\n0.5, 0.7, 0.9} and their harmonic mean, the F1 Score.\n4.2. Ablation studies\nThe default Vid2Seq model predicts both text and time\ntokens, uses both visual frames and transcribed speech as\ninput, builds on the T5-Base language model, and is pre-\ntrained on untrimmed videos from YT-Temporal-1B with\nboth the generative and denoising losses. Below we ablate\nthe importance of each of these factors on the downstream\ndense video captioning performance by reporting results on\nYouCook2 and ActivityNet Captions validation sets.\nPretraining on untrimmed narrated videos by exploit-\ning transcribed speech sentence boundaries.In Table 1,\nwe evaluate the effectiveness of our pretraining task for-\nmulation that uses untrimmed videos and integrates sen-\ntence boundaries of transcribed speech via time tokens.\nIn contrast, most video clip captioning pretraining meth-\nods [35, 70, 80] use short, trimmed, video-speech segments\nfor pretraining. We adapt this strategy in our model and find\nthat it indeed yields significant performance improvements\nover the baseline that uses no video-text pretraining (row\n2 vs row 1). However, larger improvements are obtained\nby using untrimmed video-speech inputs (row 3 vs row 2).\nMoreover, using time tokens to integrate time information\nfrom transcribed speech drastically improves performance\n(row 4 vs row 3). This shows the benefits of exploiting sen-\ntence boundaries of transcribed speech via time tokens and\nof using untrimmed videos during pretraining. In Appendix\nSection C.2, we show additional ablations to quantify how\nthe performance improves by pretraining on longer narrated\nvideos that contain more speech sentences.\nInput modalities and pretraining objectives.In Table 2,\nwe analyze the importance of input modalities and pretrain-\ning tasks on the downstream dense video captioning perfor-\nmance. The model with visual inputs only (no transcribed\nspeech as input) benefits significantly from pretraining with\nthe generative objective (row 3 vs row 1). This shows the\n6\nFinetuning InputPretraining loss YouCook2 ActivityNet\nVisual SpeechGenerative DenoisingS C F1 S C F1\n1. ✓ ✗ No pretraining3.0 15.6 15.45.4 14.2 46.5\n2. ✓ ✓ No pretraining4.0 18.0 18.15.4 18.8 49.2\n3. ✓ ✗ ✓ ✗ 5.7 25.3 23.55.9 30.251.8\n4. ✓ ✓ ✓ ✗ 2.5 10.3 15.94.8 17.0 48.8\n5. ✓ ✓ ✓ ✓ 7.9 47.1 27.35.8 30.152.4\nTable 2. Effect of input modalities and pretraining losses.\nCaptioning Pretraining YouCook2 ActivityNet\nRecall Precision F1 Recall Precision F1\n1. ✗ ✗ 17.8 19.4 17.7 47.3 57.9 52.0\n2. ✓ ✗ 17.2 20.6 18.1 42.5 64.1 49.2\n3. ✗ ✓ 25.7 21.4 22.8 52.5 53.0 51.1\n4. ✓ ✓ 27.9 27.8 27.3 52.7 53.9 52.4\nTable 3. Effect of joint captioning and localization on the lo-\ncalization performance.The variant that does not caption corre-\nsponds to a localization-only variant that only predicts time tokens.\nLanguage\nModel\nPretraining YouCook2 ActivityNet\n# Videos Dataset S C F1 S C F1\n1. T5-Small 15M YTT 6.1 31.1 24.3 5.5 26.5 52.2\n2. T5-Base ∅ ∅ 4.0 18.0 18.1 5.4 18.8 49.2\n3. T5-Base 15K YTT 6.3 35.0 24.4 5.1 24.4 49.9\n4. T5-Base 150K YTT 7.3 40.1 26.7 5.4 27.2 51.3\n5. T5-Base 1M5 YTT 7.8 45.5 26.8 5.6 28.7 52.2\n6. T5-Base 1M HTM 8.3 48.3 26.6 5.8 28.8 53.1\n7. T5-Base 15M YTT 7.9 47.1 27.3 5.8 30.1 52.4\nTable 4. Effect of language model size and pretraining data.\nHTM: HowTo100M [72], YTT: YT-Temporal-1B [121].\neffectiveness of using the transcribed speech as a proxy an-\nnotation for dense video captioning pretraining. However,\nthis model is pretrained with visual inputs only and its per-\nformance largely drops when it is finetuned with both visual\nand transcribed speech inputs (row 4 vs row 3). With both\nmodalities, adding the denoising loss strongly benefits our\nmodel (row 5 vs rows 4 and 2). We conclude that the de-\nnoising objective benefits multi-modal reasoning.\nEffect of captioning on localization.In Table 3, we com-\npare the event localization performance of our model with\na localization-only variant that only predicts event bound-\naries. We find that the model that jointly predicts event\nboundaries and captions localizes better and benefits more\nfrom pretraining than the localization-only baseline (row 4\nvs row 3), which demonstrates the importance of contextu-\nalizing the noisy timestamps of the transcribed speech with\nthe speech semantic content during pretraining.\nModel size and pretraining data.In Table 4, we show\nthat the language model size has a great importance on the\nperformance, as the model with T5-Base outperforms its\nvariant with T5-Small (row 7 vs row 1). We also eval-\nuate the importance of the size of the pretraining dataset\nof narrated videos by constructing subsets such that larger\nsubsets include the smaller ones. We find that scaling up\nthe size of the pretraining dataset is beneficial, and that\nour pretraining method yields important benefits when only\nusing 150K narrated videos for pretraining (row 4). We\nMethod BackboneYouCook2 (val)ViTT (test)ActivityNet (val)\nS C M S C M S C M\nMT [131] TSN — 6.1 3.2 — — — — 9.3 5.0\nECHR [103] C3D — — 3.8 — — — 3.2 14.7 7.2\nPDVC [102] TSN 4.4 22.7 4.7 — — — 5.4 29.0 8.0\nPDVC [102]† CLIP 4.9 28.9 5.7 — — — 6.0 29.3 7.6\nUEDVC [125]TSN — — — — — — 5.5 — —\nE2ESG [133] C3D — 25.0* 3.5 — 25.0 8.1 — — —-\nVid2Seq (Ours)CLIP 7.9 47.1 9.313.5 43.5 8.55.8 30.1 8.5\nTable 5. Comparison to the state of the art for dense video cap-\ntioning. * Results provided by the authors. † Results of our exper-\niments using the official codebase.\nMethod BackboneYouCook2 (val)ViTT (test)ActivityNet (val)\nRecall PrecisionRecall PrecisionRecall Precision\nPDVC [102] TSN — — — — 55.4 58.1PDVC [102]† CLIP — — — — 53.2 54.7UEDVC [125]TSN — — — — 59.0 60.3E2ESG [133]C3D 20.7* 20.6* 32.2* 32.1* — —Vid2Seq (Ours)CLIP 27.9 27.8 42.6 46.2 52.7 53.9\nTable 6. Comparison to the state of the art for event localization.\n* Results provided by the authors. † Results of our experiments\nusing the official codebase.\nfurther show that our pretraining method generalizes well\nto the HowTo100M dataset [72]. The model pretrained\non HowTo100M (row 6) actually achieves best results on\nYouCook2, as these datasets are from a similar domain. Fi-\nnally, we ablate the importance of the size and pretraining\nof the visual backbone in Appendix Section C.2.\n4.3. Comparison to the state of the art\nDense video captioning. In Table 5, we compare our\napproach to state-of-the-art dense video captioning meth-\nods using cross-entropy training 1 on the YouCook2, ViTT\nand ActivityNet Captions datasets. Vid2Seq sets new\nstate of the art on all three datasets. In particular, our\nmethod improves the CIDEr metric by 18.2 and 0.8 points\non YouCook2 and ActivityNet Captions over PDVC. Our\nmethod also outperforms E2ESG [133] which uses in-\ndomain text-only pretraining on Wikihow. These results\ndemonstrate the strong dense event captioning ability of our\npretrained Vid2Seq model.\nEvent localization.In Table 6, we evaluate the event local-\nization performance of our dense video captioning model in\ncomparison with prior work. On both YouCook2 and ViTT,\nVid2Seq outperforms prior work [133] tackling dense video\ncaptioning as a single sequence generation task. However,\nour model underperforms compared to PDVC [102] and\nUEDVC [102] on ActivityNet Captions. We emphasize that\nour approach integrates less prior knowledge about tempo-\nral localization than both these approaches, which include\ntask specific components such as event counters [102] or\nseparately train a model for the localization subtask [125].\nVideo paragraph captioning. In Table 7, we com-\npare our approach to state-of-the-art video paragraph cap-\ntioning methods on the YouCook2 and ActivityNet Cap-\n1We do not include methods directly optimizing the test metric [20,73].\n7\nVis2Seq\nGT\nInput Speech\nAn athleteisseenstanding readybeforea large track. The womanthrowsa javelinoff intothe distance and isshownagainafterwards. Shethrowsherhands up to cheerand wraps herselfin a flag.A womanruns witha javelin. Shethrowsitonto the field. Shethrowsa second javelin. Shewavesto the crowdand holdsup a flag.\nNext Oh isChristina Oh Beck full mostconsistent off the top womenjavelinthrowersaroundat the moment. Well, that'sanotherveryfine. She'sgotover the yearsknow whatmajor gold medalsuntilnow.Christina Oh begfor whata wonderfulrecord.…InputFrames\nASRØ Ø Ø Ø Ø Ø Ø\nVis2SeqGTA weightlifterisstanding on a stage. He lifts the barbellbeforedroppingit. He jumps up and down in excitement.\nA verystrongman isshownin a competitionHe lifts a veryheavyweightover hishead. He thendrops the weightto the groundbeforeshakinghishands.\nFrames\nØ\nFigure 4. Example of dense event captioning predictions of Vid2Seq on ActivityNet Captions validation set, compared with ground-truth.\nMethod Backbone YouCook2 (val)ActivityNet (val-ae)\nC M C M\nWith GT Proposals\nVTransformer [131]V (ResNet-200) + F32.3 15.7 22.2 15.6\nTransformer-XL [18]V (ResNet-200) + F26.4 14.8 21.7 15.1\nMART [50] V (ResNet-200) + F35.7 15.9 23.4 15.7\nGVDSup [128] V (ResNet-101) + F + O— — 22.9 16.4\nAdvInf [76] V (ResNet-101) + F + O— — 21.0 16.6\nPDVC [102] V + F (TSN) — — 27.3 15.9\nWith Learnt Proposals\nMFT [107] V + F (TSN) — — 19.1 14.7\nPDVC [102] V + F (TSN) — — 20.5 15.8\nPDVC [102]† V (CLIP) — — 23.6 15.9\nVid2Seq (Ours) V (CLIP) 50.1 24.0 28.0 17.0\nTable 7. Comparison to the SoTA for video paragraph captioning.\n† Results of our experiments using the official codebase. V/F/O\nrefers to visual/flow/object features.\nMethod Trained\nParameters\nPretraining\nData\nMSR-VTT (test)MSVD (test)\nC M C M\nORG-TRL [126]— ∅ 50.9 28.8 95.2 36.4\nSwinBERT [63]229M ∅ 53.8 29.9 120.6 41.3\nVid2Seq (Ours)314M ∅ 57.2 30.0 120.3 41.4\nMV-GPT [80] 354M HowTo100M60.0 29.9∗ — —\nVid2Seq (Ours)314M HowTo100M61.5 30.4 140.6 44.5\nVid2Seq (Ours)314M YT-Temporal-1B64.6 30.8 146.2 45.3\nTable 8. Comparison to the SoTA for video clip captioning. *\nindicates results re-evaluated by the same evaluation toolkit.\nData YouCook2 ViTT ActivityNet\nS C M S C M S C M\n1. 1% 2.4 10.1 3.3 2.0 7.4 1.9 2.2 6.2 3.2\n2. 10% 3.8 18.4 5.2 10.7 28.6 6.0 4.3 20.0 6.1\n3. 50% 6.2 32.1 7.6 12.5 38.8 7.8 5.4 27.5 7.8\n4. 100% 7.9 47.1 9.3 13.5 43.5 8.5 5.8 30.1 8.5\nTable 9. Few-shot dense event captioning, by finetuning Vid2Seq\nusing a small fraction of the downstream training dataset.\ntions datasets. Vid2Seq outperforms all prior methods on\nboth datasets, including the ones using ground-truth event\nboundary proposals at inference time [18, 50, 76, 102, 128,\n131], showing strong video paragraph captioning ability.\nVideo clip captioning. In Table 8, we compare our ap-\nproach to state-of-the-art video clip captioning methods on\nthe MSR-VTT and MSVD datasets. Vid2Seq improves over\nprior methods in their respective pretraining data setting\nwhile using a comparable number of trained parameters.\nWe conclude that our pretrained Vid2Seq model generalizes\nwell to the standard video clip captioning setting.\n4.4. Few-shot dense video captioning\nTo further evaluate the generalization capabilities of our\npretrained Vid2Seq model, we propose a new few-shot\ndense video captioning setting where we finetune Vid2Seq\nusing only a fraction of the downstream training dataset.\nFrom Table 9, we observe important improvements when\nusing 10% compared to 1% of training data (row 2 vs 1).\nIn Appendix Section C.1 we further show that pretraining is\nessential in this few-shot setting.\n4.5. Qualitative examples\nIn Figure 4, we show an example of dense event caption-\ning predictions from Vid2Seq. This example shows that our\nmodel can predict meaningful event boundaries and cap-\ntions, and that the predicted captions and boundaries dif-\nfer considerably from the transcribed speech input (show-\ning the importance of the visual tokens in the input). More\nexamples are provided in Appendix Section A.\n5. Conclusion\nWe introduced Vid2Seq, a visual language model that\nperforms dense video captioning by generating a single se-\nquence of tokens including both text and time tokens given\nmulti-modal inputs. We showed that Vid2Seq benefits from\nlarge-scale pretraining on unlabeled untrimmed narrated\nvideos by leveraging transcribed speech sentences and cor-\nresponding temporal boundaries. Vid2Seq achieves state-\nof-the-art results on various dense event captioning datasets,\nas well as multiple video paragraph captioning and standard\nvideo clip captioning benchmarks. Finally, we believe the\nsequence-to-sequence design of Vid2Seq has the potential\nto be extended to a wide range of other video tasks such as\ntemporally-grounded video question answering [51, 56, 57]\nor temporal action localization [16, 67, 123].\nAcknowledgements. The work was partially funded by a Google\ngift, the French government under management of Agence Nationale de\nla Recherche as part of the ”Investissements d’avenir” program, refer-\nence ANR-19-P3IA-0001 (PRAIRIE 3IA Institute), the Louis Vuitton ENS\nChair on Artificial Intelligence, the European Regional Development Fund\nunder project IMPACT (reg. no. CZ.02.1.01/0.0/0.0/15 003/0000468). We\nthank Anurag Arnab, Minsu Cho, Anja Hauth, Ashish Thapliyal, Bo Pang,\nBryan Seybold and the entire Ganesha team for helpful discussions.\n8\nReferences\n[1] Vid2Seq project webpage. https://antoyang.\ngithub.io/vid2seq.html. 1, 2\n[2] Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul\nNatsev, George Toderici, Balakrishnan Varadarajan, and\nSudheendra Vijayanarasimhan. Youtube-8m: A large-\nscale video classification benchmark. arXiv preprint\narXiv:1609.08675, 2016. 14\n[3] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong\nChuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. Vatt:\nTransformers for multimodal self-supervised learning from\nraw video, audio and text. NeurIPS, 2021. 2\n[4] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Men-\nsch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. In NeurIPS,\n2022. 1, 2\n[5] Max Bain, Arsha Nagrani, G ¨ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In ICCV, 2021. 2\n[6] Satanjeev Banerjee and Alon Lavie. Meteor: An auto-\nmatic metric for mt evaluation with improved correlation\nwith human judgments. In Proceedings of the acl workshop\non intrinsic and extrinsic evaluation measures for machine\ntranslation and/or summarization, 2005. 6\n[7] Meng Cao, Tianyu Yang, Junwu Weng, Can Zhang, Jue\nWang, and Yuexian Zou. Locvtp: Video-text pre-training\nfor temporal localization. In ECCV, 2022. 2\n[8] Aman Chadha, Gurneet Arora, and Navpreet Kaloty. iPer-\nceive: Applying common-sense reasoning to multi-modal\ndense video captioning and video question answering. In\nWACV, 2021. 2\n[9] David L Chen and William B Dolan. Collecting highly par-\nallel data for paraphrase evaluation. In ACL, 2011. 2, 6,\n14\n[10] Shaoxiang Chen and Yu-Gang Jiang. Towards bridging\nevent captioner and sentence localizer for weakly super-\nvised dense event captioning. In CVPR, 2021. 2\n[11] Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Ge-\noffrey Hinton. Pix2seq: A language modeling framework\nfor object detection. In ICLR, 2022. 1, 3\n[12] Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J\nFleet, and Geoffrey Hinton. A unified sequence interface\nfor vision tasks. In NeurIPS, 2022. 3\n[13] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergio-\nvanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman,\nAdam Grycner, Basil Mustafa, Lucas Beyer, et al. PaLI:\nA jointly-scaled multilingual language-image model. arXiv\npreprint arXiv:2209.06794, 2022. 1, 3\n[14] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,\nFaisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu.\nUNITER: Universal image-text representation learning. In\nECCV, 2020. 2\n[15] Zhiyang Chen, Yousong Zhu, Zhaowen Li, Fan Yang, Wei\nLi, Haixin Wang, Chaoyang Zhao, Liwei Wu, Rui Zhao,\nJinqiao Wang, et al. Obj2seq: Formatting objects as se-\nquences with class prompt for visual tasks. In NeurIPS,\n2022. 3\n[16] Feng Cheng and Gedas Bertasius. TALLformer: Tempo-\nral action localization with long-memory transformer. In\nECCV, 2022. 8\n[17] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying\nvision-and-language tasks via text generation. In ICML,\n2021. 3\n[18] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell,\nQuoc V Le, and Ruslan Salakhutdinov. Transformer-XL:\nAttentive language models beyond a fixed-length context.\nIn ACL, 2019. 8\n[19] Mostafa Dehghani, Alexey Gritsenko, Anurag Arnab,\nMatthias Minderer, and Yi Tay. Scenic: A jax library for\ncomputer vision research and beyond. In CVPR, 2022. 2\n[20] Chaorui Deng, Shizhe Chen, Da Chen, Yuan He, and Qi\nWu. Sketch, ground, and refine: Top-down dense video\ncaptioning. In CVPR, 2021. 1, 2, 7\n[21] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin John-\nson. RedCaps: Web-curated image-text data created by the\npeople, for the people. In NeurIPS Datasets and Bench-\nmarks, 2021. 2\n[22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An im-\nage is worth 16x16 words: Transformers for image recog-\nnition at scale. In ICLR, 2021. 4\n[23] Zi-Yi Dou, Aishwarya Kamath, Zhe Gan, Pengchuan\nZhang, Jianfeng Wang, Linjie Li, Zicheng Liu, Ce Liu,\nYann LeCun, Nanyun Peng, et al. Coarse-to-fine vision-\nlanguage pre-training with fusion in the backbone. In\nNeurIPS, 2022. 2\n[24] Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuo-\nhang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan\nZhang, Lu Yuan, Nanyun Peng, et al. An empirical study of\ntraining end-to-end vision-and-language transformers. In\nCVPR, 2022. 2\n[25] Victor Escorcia, Fabian Caba Heilbron, Juan Carlos\nNiebles, and Bernard Ghanem. Daps: Deep action propos-\nals for action understanding. In ECCV, 2016. 2\n[26] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang\nWang, Lijuan Wang, and Zicheng Liu. VIOLET: End-to-\nend video-language transformers with masked visual-token\nmodeling. arXiv preprint arXiv:2111.12681, 2021. 2\n[27] Soichiro Fujita, Tsutomu Hirao, Hidetaka Kamigaito, Man-\nabu Okumura, and Masaaki Nagata. SODA: Story oriented\ndense video captioning evaluation framework. In ECCV,\n2020. 6\n[28] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng,\nand Jingjing Liu. Large-scale adversarial training for\nvision-and-language representation learning. In NeurIPS,\n2020. 2\n[29] Jiyang Gao, Zhenheng Yang, Kan Chen, Chen Sun, and\nRam Nevatia. Turn tap: Temporal unit regression network\nfor temporal action proposals. In ICCV, 2017. 2\n9\n[30] Lianli Gao, Zhao Guo, Hanwang Zhang, Xing Xu, and\nHeng Tao Shen. Video captioning with attention-based lstm\nand semantic consistency. IEEE Transactions on Multime-\ndia, 2017. 2\n[31] Yuying Ge, Yixiao Ge, Xihui Liu, Dian Li, Ying Shan, Xi-\naohu Qie, and Ping Luo. Bridging video-text retrieval with\nmultiple choice questions. In CVPR, 2022. 2\n[32] Tengda Han, Weidi Xie, and Andrew Zisserman. Temporal\nalignment networks for long-term video. In CVPR, 2022.\n2, 5\n[33] Fabian Caba Heilbron, Juan Carlos Niebles, and Bernard\nGhanem. Fast temporal activity proposals for efficient de-\ntection of human actions in untrimmed videos. In CVPR,\n2016. 2\n[34] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang,\nZicheng Liu, Yumao Lu, and Lijuan Wang. Scaling up\nvision-language pre-training for image captioning. In\nCVPR, 2022. 2\n[35] Gabriel Huang, Bo Pang, Zhenhai Zhu, Clara Rivera, and\nRadu Soricut. Multimodal pretraining for dense video cap-\ntioning. In AACL-IJCNLP, 2020. 1, 2, 3, 4, 5, 6, 14, 17\n[36] Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu,\nDongmei Fu, and Jianlong Fu. Seeing out of the box:\nEnd-to-end pre-training for vision-language representation\nlearning. In CVPR, 2021. 2\n[37] Vladimir Iashin and Esa Rahtu. A better use of audio-visual\ncues: Dense video captioning with bi-modal transformer. In\nBMVC, 2020. 1, 2\n[38] Vladimir Iashin and Esa Rahtu. Multi-modal dense video\ncaptioning. In CVPR Workshops, 2020. 2\n[39] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li,\nand Tom Duerig. Scaling up visual and vision-language\nrepresentation learning with noisy text supervision. In\nICML, 2021. 2\n[40] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel\nSynnaeve, Ishan Misra, and Nicolas Carion. MDETR\n- modulated detection for end-to-end multi-modal under-\nstanding. In ICCV, 2021. 2\n[41] Wonjae Kim, Bokyung Son, and Ildoo Kim. ViLT: Vision-\nand-language transformer without convolution or region su-\npervision. In ICML, 2021. 2\n[42] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In ICLR, 2015. 6, 14\n[43] Dohwan Ko, Joonmyung Choi, Juyeon Ko, Shinyeong Noh,\nKyoung-Woon On, Eun-Sol Kim, and Hyunwoo J Kim.\nVideo-text representation learning via differentiable weak\ntemporal alignment. In CVPR, 2022. 2, 5\n[44] Alexander Kolesnikov, Andr ´e Susano Pinto, Lucas Beyer,\nXiaohua Zhai, Jeremiah Harmsen, and Neil Houlsby.\nUvim: A unified modeling approach for vision with learned\nguiding codes. In NeurIPS, 2022. 3\n[45] Mahnaz Koupaee and William Yang Wang. Wikihow:\nA large scale text summarization dataset. arXiv preprint\narXiv:1810.09305, 2018. 3\n[46] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and\nJuan Carlos Niebles. Dense-captioning events in videos. In\nICCV, 2017. 1, 2, 4, 5, 6, 14\n[47] Taku Kudo and John Richardson. Sentencepiece: A sim-\nple and language independent subword tokenizer and deto-\nkenizer for neural text processing. In ACL, 2018. 4\n[48] Jie Lei, Tamara L Berg, and Mohit Bansal. Detecting mo-\nments and highlights in videos via natural language queries.\nIn NeurIPS, 2021. 2\n[49] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg,\nMohit Bansal, and Jingjing Liu. Less is more: ClipBERT\nfor video-and-language learning via sparse sampling. In\nCVPR, 2021. 2\n[50] Jie Lei, Liwei Wang, Yelong Shen, Dong Yu, Tamara L\nBerg, and Mohit Bansal. MART: Memory-augmented re-\ncurrent transformer for coherent video paragraph caption-\ning. In ACL, 2020. 2, 8, 14\n[51] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg.\nTVQA: Localized, compositional video question answer-\ning. In EMNLP, 2018. 8\n[52] Dongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles,\nand Steven CH Hoi. Align and prompt: Video-and-\nlanguage pre-training with entity prompts. In CVPR, 2022.\n2\n[53] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang,\nand Ming Zhou. Unicoder-VL: A universal encoder for vi-\nsion and language by cross-modal pre-training. In AAAI,\n2020. 2\n[54] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBLIP: Bootstrapping language-image pre-training for uni-\nfied vision-language understanding and generation. In\nICML, 2022. 2\n[55] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,\nShafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi.\nAlign before fuse: Vision and language representation\nlearning with momentum distillation. In NeurIPS, 2021.\n2\n[56] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng\nYu, and Jingjing Liu. HERO: Hierarchical encoder\nfor video+language omni-representation pre-training. In\nEMNLP, 2020. 2, 8\n[57] Linjie Li, Jie Lei, Zhe Gan, Licheng Yu, Yen-Chun Chen,\nRohit Pillai, Yu Cheng, Luowei Zhou, Xin Eric Wang,\nWilliam Yang Wang, et al. V ALUE: A multi-task bench-\nmark for video-and-language understanding evaluation. In\nNeurIPS Track on Datasets and Benchmarks, 2021. 8\n[58] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-\nwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu\nYuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded\nlanguage-image pre-training. In CVPR, 2022. 2\n[59] Wanhua Li, Zhexuan Cao, Jianjiang Feng, Jie Zhou, and\nJiwen Lu. Label2Label: A language modeling framework\nfor multi-attribute learning. In ECCV, 2022. 3\n[60] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xi-\naowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong,\nFuru Wei, et al. Oscar: Object-semantics aligned pre-\ntraining for vision-language tasks. In ECCV, 2020. 2\n[61] Yehao Li, Ting Yao, Yingwei Pan, Hongyang Chao, and\nTao Mei. Jointly localizing and describing events for dense\nvideo captioning. In CVPR, 2018. 2\n10\n[62] Chuming Lin, Jian Li, Yabiao Wang, Ying Tai, Donghao\nLuo, Zhipeng Cui, Chengjie Wang, Jilin Li, Feiyue Huang,\nand Rongrong Ji. Fast learning of temporal action proposal\nvia dense boundary generator. In AAAI, 2020. 2\n[63] Kevin Lin, Linjie Li, Chung-Ching Lin, Faisal Ahmed, Zhe\nGan, Zicheng Liu, Yumao Lu, and Lijuan Wang. Swin-\nBERT: End-to-end transformers with sparse attention for\nvideo captioning. In CVPR, 2022. 1, 2, 8\n[64] Kevin Qinghong Lin, Alex Jinpeng Wang, Mattia Soldan,\nMichael Wray, Rui Yan, Eric Zhongcong Xu, Difei Gao,\nRongcheng Tu, Wenzhe Zhao, Weijie Kong, et al. Egocen-\ntric video-language pretraining. In NeurIPS, 2022. 2\n[65] Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, and Shilei\nWen. BMN: Boundary-matching network for temporal ac-\ntion proposal generation. In ICCV, 2019. 2\n[66] Tianwei Lin, Xu Zhao, Haisheng Su, Chongjing Wang, and\nMing Yang. BSN: Boundary sensitive network for temporal\naction proposal generation. In ECCV, 2018. 2\n[67] Xiaolong Liu, Qimeng Wang, Yao Hu, Xu Tang, Shiwei\nZhang, Song Bai, and Xiang Bai. End-to-end temporal ac-\ntion detection with transformer. In IEEE Transactions on\nImage Processing, 2022. 8\n[68] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViL-\nBERT: Pretraining task-agnostic visiolinguistic representa-\ntions for vision-and-language tasks. In NeurIPS, 2019. 2\n[69] Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi\nParikh, and Stefan Lee. 12-in-1: Multi-task vision and lan-\nguage representation learning. In CVPR, 2020. 2\n[70] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan\nDuan, Tianrui Li, Xilin Chen, and Ming Zhou. Uni-\nViLM: A unified video and language pre-training model for\nmultimodal understanding and generation. arXiv preprint\narXiv:2002.06353, 2020. 1, 6, 17\n[71] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan\nLaptev, Josef Sivic, and Andrew Zisserman. End-to-end\nlearning of visual representations from uncurated instruc-\ntional videos. In CVPR, 2020. 2, 5\n[72] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,\nMakarand Tapaswi, Ivan Laptev, and Josef Sivic.\nHowTo100M: Learning a text-video embedding by watch-\ning hundred million narrated video clips. In ICCV, 2019. 2,\n5, 7, 14\n[73] Jonghwan Mun, Linjie Yang, Zhou Ren, Ning Xu, and Bo-\nhyung Han. Streamlined dense video captioning. In CVPR,\n2019. 2, 7\n[74] Arsha Nagrani, Paul Hongsuck Seo, Bryan Seybold, Anja\nHauth, Santiago Manen, Chen Sun, and Cordelia Schmid.\nLearning audio-video modalities from image captions. In\nECCV, 2022. 2\n[75] Yingwei Pan, Ting Yao, Houqiang Li, and Tao Mei. Video\ncaptioning with transferred semantic attributes. In CVPR,\n2017. 2\n[76] Jae Sung Park, Marcus Rohrbach, Trevor Darrell, and Anna\nRohrbach. Adversarial inference for multi-sentence video\ndescription. In CVPR, 2019. 2, 8\n[77] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, 2021. 4, 17\n[78] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,\nand Peter J Liu. Exploring the limits of transfer learning\nwith a unified text-to-text transformer. JMLR, 2020. 1, 5\n[79] Tanzila Rahman, Bicheng Xu, and Leonid Sigal. Watch,\nlisten and tell: Multi-modal weakly supervised dense event\ncaptioning. In ICCV, 2019. 2\n[80] Paul Hongsuck Seo, Arsha Nagrani, Anurag Arnab, and\nCordelia Schmid. End-to-end generative pretraining for\nmultimodal video captioning. In CVPR, 2022. 1, 2, 6, 8, 17\n[81] Paul Hongsuck Seo, Arsha Nagrani, and Cordelia Schmid.\nLook before you speak: Visually contextualized utterances.\nIn CVPR, 2021. 2\n[82] Zhiqiang Shen, Jianguo Li, Zhou Su, Minjun Li, Yurong\nChen, Yu-Gang Jiang, and Xiangyang Xue. Weakly super-\nvised dense video captioning. In CVPR, 2017. 2\n[83] Botian Shi, Lei Ji, Yaobo Liang, Nan Duan, Peng Chen,\nZhendong Niu, and Ming Zhou. Dense procedure caption-\ning in narrated instructional videos. In ACL, 2019. 2\n[84] Zheng Shou, Dongang Wang, and Shih-Fu Chang. Tempo-\nral action localization in untrimmed videos via multi-stage\ncnns. In CVPR, 2016. 2\n[85] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guil-\nlaume Couairon, Wojciech Galuba, Marcus Rohrbach, and\nDouwe Kiela. FLA V A: A foundational language and vision\nalignment model. In CVPR, 2022. 2\n[86] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya\nSutskever, and Ruslan Salakhutdinov. Dropout: A simple\nway to prevent neural networks from overfitting. JMLR,\n2014. 15\n[87] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai,\nRoss Wightman, Jakob Uszkoreit, and Lucas Beyer. How\nto train your vit? data, augmentation, and regularization in\nvision transformers. In TMLR, 2022. 17\n[88] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu\nWei, and Jifeng Dai. VL-BERT: Pre-training of generic\nvisual-linguistic representations. In ICLR, 2019. 2\n[89] Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy,\nand Cordelia Schmid. VideoBERT: A joint model for video\nand language representation learning. In ICCV, 2019. 2\n[90] Yuchong Sun, Hongwei Xue, Ruihua Song, Bei Liu, Huan\nYang, and Jianlong Fu. Long-form video-language pre-\ntraining with multimodal temporal contrastive learning. In\nNeurIPS, 2022. 2\n[91] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the inception ar-\nchitecture for computer vision. In CVPR, 2016. 15\n[92] Hao Tan and Mohit Bansal. LXMERT: Learning cross-\nmodality encoder representations from transformers. In\nEMNLP, 2019. 2\n[93] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, SM Es-\nlami, Oriol Vinyals, and Felix Hill. Multimodal few-shot\nlearning with frozen language models. In NeurIPS, 2021. 2\n11\n[94] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In NeurIPS,\n2017. 1, 4\n[95] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. CIDEr: Consensus-based image description eval-\nuation. In CVPR, 2015. 6\n[96] Alex Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge,\nXudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, Xi-\naohu Qie, and Mike Zheng Shou. All in one: Explor-\ning unified video-language pre-training. arXiv preprint\narXiv:2203.07303, 2022. 2\n[97] Bairui Wang, Lin Ma, Wei Zhang, and Wei Liu. Recon-\nstruction network for video captioning. In CVPR, 2018. 2\n[98] Jinpeng Wang, Yixiao Ge, Guanyu Cai, Rui Yan, Xudong\nLin, Ying Shan, Xiaohu Qie, and Mike Zheng Shou.\nObject-aware video-language pre-training for retrieval. In\nCVPR, 2022. 2\n[99] Jianfeng Wang, Xiaowei Hu, Zhe Gan, Zhengyuan Yang,\nXiyang Dai, Zicheng Liu, Yumao Lu, and Lijuan Wang.\nUFO: A unified transformer for vision-language represen-\ntation learning. arXiv preprint arXiv:2111.10023, 2021. 2\n[100] Jingwen Wang, Wenhao Jiang, Lin Ma, Wei Liu, and Yong\nXu. Bidirectional attentive fusion with context gating for\ndense video captioning. In CVPR, 2018. 1, 2\n[101] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,\nZhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and\nHongxia Yang. Unifying architectures, tasks, and modali-\nties through a simple sequence-to-sequence learning frame-\nwork. In ICML, 2022. 3\n[102] Teng Wang, Ruimao Zhang, Zhichao Lu, Feng Zheng, Ran\nCheng, and Ping Luo. End-to-end dense video captioning\nwith parallel decoding. In ICCV, 2021. 1, 2, 7, 8, 14\n[103] Teng Wang, Huicheng Zheng, Mingjing Yu, Qian Tian, and\nHaifeng Hu. Event-centric hierarchical representation for\ndense video captioning. IEEE Transactions on Circuits and\nSystems for Video Technology, 2020. 2, 7\n[104] Xin Wang, Wenhu Chen, Jiawei Wu, Yuan-Fang Wang, and\nWilliam Yang Wang. Video captioning via hierarchical re-\ninforcement learning. In CVPR, 2018. 2\n[105] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia\nTsvetkov, and Yuan Cao. SimVLM: Simple visual language\nmodel pretraining with weak supervision. In ICLR, 2022. 1\n[106] Zixu Wang, Yujie Zhong, Yishu Miao, Lin Ma, and Lu-\ncia Specia. Contrastive video-language learning with fine-\ngrained frame sampling. In AACL-IJCNLP, 2022. 2\n[107] Yilei Xiong, Bo Dai, and Dahua Lin. Move forward and\ntell: A progressive generator of video descriptions. In\nECCV, 2018. 8\n[108] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko,\nArmen Aghajanyan, Florian Metze, Luke Zettlemoyer, and\nChristoph Feichtenhofer. VideoCLIP: Contrastive pre-\ntraining for zero-shot video-text understanding. InEMNLP,\n2021. 2\n[109] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. MSR-VTT: A\nlarge video description dataset for bridging video and lan-\nguage. In CVPR, 2016. 2, 6, 14\n[110] Mengmeng Xu, Erhan Gundogdu, Maksim Lapin, Bernard\nGhanem, Michael Donoser, and Loris Bazzani. Con-\ntrastive language-action pre-training for temporal localiza-\ntion. arXiv preprint arXiv:2204.12293, 2022. 2\n[111] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun,\nBei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Ad-\nvancing high-resolution video-language representation with\nlarge-scale video transcriptions. In CVPR, 2022. 2\n[112] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and\nCordelia Schmid. Just ask: Learning to answer questions\nfrom millions of narrated videos. In ICCV, 2021. 2\n[113] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and\nCordelia Schmid. Learning to answer visual questions from\nweb videos. IEEE TPAMI, 2022. 2\n[114] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev,\nand Cordelia Schmid. TubeDETR: Spatio-temporal video\ngrounding with transformers. In CVPR, 2022. 2\n[115] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and\nCordelia Schmid. Zero-shot video question answering via\nfrozen bidirectional language models. In NeurIPS, 2022. 2\n[116] Jianwei Yang, Yonatan Bisk, and Jianfeng Gao. Taco:\nToken-aware cascade contrastive learning for video-text\nalignment. In ICCV, 2021. 2\n[117] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu,\nFaisal Ahmed, Zicheng Liu, Yumao Lu, and Lijuan Wang.\nCrossing the format boundary of text and boxes: Towards\nunified vision-language modeling. In ECCV, 2021. 1, 3\n[118] Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua\nWu, and Haifeng Wang. ERNIE-ViL: Knowledge enhanced\nvision-language representations through scene graph. In\nAAAI, 2020. 2\n[119] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-\njtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive\ncaptioners are image-text foundation models. Transactions\non Machine Learning Research, 2022. 2\n[120] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,\nXiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,\nBoxin Li, Chunyuan Li, et al. Florence: A new\nfoundation model for computer vision. arXiv preprint\narXiv:2111.11432, 2021. 2\n[121] Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yan-\npeng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack\nHessel, Ali Farhadi, and Yejin Choi. MERLOT Reserve:\nNeural script knowledge through vision and language and\nsound. In CVPR, 2022. 2, 3, 5, 6, 7, 14\n[122] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu,\nJae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi.\nMERLOT: Multimodal neural script knowledge models. In\nNeurIPS, 2021. 2, 6\n[123] Chenlin Zhang, Jianxin Wu, and Yin Li. Actionformer: Lo-\ncalizing moments of actions with transformers. In ECCV,\n2022. 8\n[124] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun\nChen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu\nYuan, Jenq-Neng Hwang, and Jianfeng Gao. GLIPv2: Uni-\nfying localization and vision-language understanding. In\nNeurIPS, 2022. 2\n12\n[125] Qi Zhang, Yuqing Song, and Qin Jin. Unifying event detec-\ntion and captioning as sequence generation via pre-training.\nIn ECCV, 2022. 2, 7\n[126] Ziqi Zhang, Yaya Shi, Chunfeng Yuan, Bing Li, Peijin\nWang, Weiming Hu, and Zheng-Jun Zha. Object relational\ngraph with teacher-recommended learning for video cap-\ntioning. In CVPR, 2020. 8\n[127] Yue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xi-\naoou Tang, and Dahua Lin. Temporal action detection with\nstructured segment networks. In ICCV, 2017. 2\n[128] Luowei Zhou, Yannis Kalantidis, Xinlei Chen, Jason J\nCorso, and Marcus Rohrbach. Grounded video description.\nIn CVPR, 2019. 2, 8, 14\n[129] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Ja-\nson J Corso, and Jianfeng Gao. Unified vision-language\npre-training for image captioning and VQA. InAAAI, 2020.\n2\n[130] Luowei Zhou, Chenliang Xu, and Jason J Corso. Towards\nautomatic learning of procedures from web instructional\nvideos. In AAAI, 2018. 1, 2, 4, 5, 6, 14\n[131] Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard\nSocher, and Caiming Xiong. End-to-end dense video cap-\ntioning with masked transformer. In CVPR, 2018. 1, 2, 7,\n8\n[132] Chaoyang Zhu, Yiyi Zhou, Yunhang Shen, Gen Luo,\nXingjia Pan, Mingbao Lin, Chao Chen, Liujuan Cao, Xi-\naoshuai Sun, and Rongrong Ji. SeqTR: A simple yet uni-\nversal network for visual grounding. In ECCV, 2022. 3\n[133] Wanrong Zhu, Bo Pang, Ashish Thapliyal, William Yang\nWang, and Radu Soricut. End-to-end dense video caption-\ning as sequence generation. In COLING, 2022. 2, 3, 7\n13\nAppendix\nIn this Appendix, we present the following additional\nmaterial:\n(i) Additional qualitative examples of dense video cap-\ntioning predictions (Section A).\n(ii) Additional information about our experimental setup\n(Section B);\n(iii) Additional experimental results (Section C), including\nan ablation on the importance of pretraining for few-\nshot dense video captioning (Section C.1) and addi-\ntional ablation studies in the standard fully-supervised\ndense video captioning setting (Section C.2).\nA. Qualitative examples of dense video cap-\ntioning predictions\nIn Figure 4, we show qualitative results of dense event\ncaptioning by our Vid2Seq model. Here in Figures 5 and 6 ,\nwe show additional results on examples from the YouCook2\nand ActivityNet Captions datasets. These results show that\nVid2Seq can predict meaningful dense captions and event\nboundaries in diverse scenarios, with or without transcribed\nspeech input, e.g. series of instructions in cooking recipes\n(Figure 5) or actions in human sports or leisure activities\n(first three examples in Figure 6). The last example in Fig-\nure 6 illustrates a failure case where the model hallucinates\nevents that are not visually grounded such as ‘one man hats\noff to the camera‘.\nB. Experimental setup\nIn this section, we complement the information provided\nin Section 4.1 about the datasets we use (Section B.1). We\nalso give additional implementation details (Section B.2).\nB.1. Datasets\nYT-Temporal-1B[121] consists of 18.821M unlabeled nar-\nrated videos covering about 150 years of video content for\npretraining. Compared with HowTo100M [72], this dataset\nwas created to cover a wider range of domains and not only\ninstructional videos.\nHowTo100M [72] consists of 1.221M unlabeled narrated\ninstructional videos covering about 15 years of video con-\ntent for pretraining.\nYouCook2 [130] has 1,790 untrimmed videos of cooking\nprocedures. On average, each video lasts 320s and is an-\nnotated with 7.7 temporally-localized imperative sentences.\nThe dataset is split into 1,333 videos for training and 457\nvideos for validation.\nViTT [35] consists of 7,672 untrimmed instructional videos\nfrom the YouTube-8M dataset [2]. Compared to YouCook2,\nViTT was created to better reflect the distribution of in-\nstructional videos in the wild. On average, each video lasts\n250s and is annotated with 7.1 temporally-localized short\ntags. The dataset is split into 5,476, 1,102 and 1,094 videos\nfor training, validation and testing, respectively. Videos in\nthe validation and test sets are provided with multiple sets\nof dense event captioning annotations. Following [35], we\ntreat each set of annotations as a single example during eval-\nuation and discard videos with more than 3 sets of annota-\ntions.\nActivityNet-Captions [46] contains 14,934 untrimmed\nvideos of various human activities. Different from\nYouCook2 and ViTT where most videos contain transcribed\nspeech content, we find that 68% of videos in Activi-\ntyNet Captions do not have transcribed narration. On av-\nerage, each video lasts 120s and is annotated with 3.7\ntemporally-localized sentences. The dataset is split into\n10,009 and 4,925 videos for training and validation, respec-\ntively. Videos in the validation set are provided with two\nsets of dense video captioning annotations. Following prior\nwork [102], we use both sets of annotations for evaluation,\nby computing the average of the scores over each set for\nSODA c and by using the standard evaluation tool [46] for\nall other dense event captioning metrics. For video para-\ngraph captioning, we follow [102] and report results on the\n’val-ae’ split that includes 2,460 videos [50, 128].\nMSR-VTT [109] consists of 10,000 open domain video\nclips. The duration of each video clip is between 10 and\n30 seconds. 20 natural language descriptions are manually\nannotated for each clip. The dataset is split into 6,513, 497\nand 2,990 videos for training, validation and testing, respec-\ntively.\nMSVD [9] consists of 1,970 open domain video clips. The\nduration of each video clip is between 10 and 30 seconds.\nEach video clip has roughly 40 manually annotated cap-\ntions. The dataset is split into 1,200, 100 and 670 videos\nfor training, validation and testing, respectively.\nB.2. Implementation details\nArchitecture. The visual temporal transformer encoder\nft, the text encoder gt and the text decoder ht all have\n12 layers, 12 heads, embedding dimension 768, and MLP\nhidden dimension of 2048. The text encoder and decoder\nsequences are truncated or padded to L = S = 1000 to-\nkens during pretraining, and S = 1000 and L = 256 tokens\nduring finetuning. At inference, we use beam search decod-\ning where we track the top 4 sequences and apply a length\nnormalization of 0.6.\nTraining. We use the Adam optimizer [42] with β =\n(0.9, 0.999) and no weight decay. During pretraining, we\nuse a learning rate of 1e−4, warming it up linearly (from\n14\nInput Speech\nInputFrames\nVis2Seq\nGTCut the chicken. Pound the chicken. Whisk the eggs.\nTrim off the excessfat of chickenbreastand cutitintohalves.\nCover the chickenin plastic wrap and pound itout. Crack twolarge eggsintoa bowl and whisk themtogether.\nMix breadcrumbsand parmesan cheesetogether.\nAddbreadcrumbsgratedparmesan cheeseand italianbreadcrumbsto a bowl.\nCoat the chickenin the flourmixture the eggmixture and thenthe breadcrumbs.Coat the chickenin the flourmixture and thenthe breadcrumbs.\nAddoilto a pan.Fry the chickenin the pan.\nAddmarinara sauce and cheeseon top of the chicken.\nFry the chickenin a pan withoil.\nPour tomatosauce and mozzarella cheeseon top of the chicken.\nBakethe chickenin an oven.\nBakethe chickenin an oven.\nI'mgoingto start off withtwobonelessskinlesschickenbreastshere.\nI'mjustgoingto trim off the grislyparts and the excessfat maybesomeof the skin that'sleftover on there.\nI'vegota pieceof wax paperhereand I put thatonto mycuttingboard[…] and I'mgoingto pound out mybreasthalvesuntiltheyare about 1/2 an inchthicker.\n……The first thingI'mgoingto needisan eggwash.\nSo I'mgoingto taketwolarge eggsand crack thoseintoa bowl and if yougetanyshellsin there, besure to getthose[…]\n…Now, I'musingmyhomemadeItalianbreadcrumbshere. …I'mjustgoingto mix thistogetherand nowwecan start breadingourchicken.\nNow, the breadingprocess isreallysimple on thisyoujustwantto takeone of your[…] \n…I'vegotmysmallcast-ironskilleton medium-high heathereand I'mgoingto put in about a quarter of an inchor soof extra virginolive oilintothe bottomof thatand I'mgoingto let thatcome up to temperatureand thenI'mgoingto start fryingup mychickenpieces.\n…We'regoingto bebakingtheseand thatwillfinish cooking them. …And if you'dlike to follow me on Google Plus Facebook and/or Pinterest all mylinks willbein the description box.\nInput Speech\nInputFrames\nVis2Seq\nGT Finelychop a cabbageto smallpieces. Add20g saltcarawayseedsjuniperberriesand dill.\nMassage the cabbagewiththe seasoning.\nTakea large cabbageintoa bowl and chop itintosmallpieces. Addsomesaltto the cabbageand mix. Addcarawayseedsjuniperberriesand dilltipsto the cabbageand mix.\nPut the mixture in a jar and pressfirmto the bottom.\nPlace the mixture intoa glass jar.\nSeal the jar and put in darkplace for 4 weeks.\nSeal the jar.\nAnd todaywhatI wantto do isshareone of myfavorite recipesand in myopinion[…]\nSo thenwe'regoingto chop the cabbageyoucan use a mandolinto do this.\n… …So the ruleof thumbwithsaltisthatyouwantabout 2.5 to 3 percent of the weightof the cabbage.\n… So nowwe'regoingto getin reallymix thatsaltintothe cabbagewhichisgoingto takeabout tenminutes soreallyworkitit'sa good workout.\n…And afterthatitcan bepoppedintothe fridgeor poppedintoanotherjar and enjoyed, youknow, as a condiment once a day[…]\nThere wego. So in thiscase, we'vegotabout 20 grams. …So it'sgonnabereallywellcoveredand obviouslyit'simportant to washyourhands beforeyoudo thisthatseemslike a prettyobviousthingto do but […]\nSo no oxygenthat'sthat'sthe idea.…If youdo have freshdill, I wouldrecommendusingthatnowfor the fun part, whatyouwantto do iswereallyneedto release all of the juicesfromthiscabbage\n…\nMix floursaltand peppertogether.\nPlace the chickenin a bakingdish.\nFigure 5. Examples of dense event captioning predictions of Vid2Seq on the validation set of YouCook2, compared with ground-truth.\n0) for the first 1000 iterations, and keeping it constant for\nthe remaining iterations. During finetuning, we use a learn-\ning rate of 3e−4, warming it up linearly (from 0) for the\nfirst 10% of iterations, followed by a cosine decay (down\nto 0) for the remaining 90%. During finetuning, we use a\nbatch size of 32 videos split on 16 TPU v4 chips. We fine-\ntune for 40 epochs on YouCook2, 20 epochs on ActivityNet\nCaptions and ViTT, 5 epochs on MSR-VTT and 10 epochs\non MSVD. We clip the maximum norm of the gradient to\n0.1 during pretraining, and 1 during finetuning. For data\naugmentation, we use random temporal cropping. For reg-\nularization, we use label smoothing [91] with value 0.1 and\ndropout [86] with probability 0.1.\nC. Experiments\nIn this section, we provide additional experiments that\ncomplement the results presented in Section 4. We first\nshow the importance of pretraining in our proposed few-\nshot setting in Section C.1. Then we provide additional\n15\nØ Ø Ø Ø Ø Ø Ø\nA weightlifterisstanding on a stage. He lifts the barbellbeforedroppingit. He jumps up and down in excitement.\nA verystrongman isshownin a competitionHe lifts a veryheavyweightover hishead. He thendrops the weightto the groundbeforeshakinghishands.\nØ\nVis2Seq\nGT\nInput SpeechInputFrames\nØ Ø Ø Ø Ø Ø Ø ØInput SpeechInputFrames\nVis2Seq\nGTA group of childrenare seenswimmingin a pool.The kids hit a ballback and forthin the water.\nA pictureof a sky isshownand leads intoa group of boys playinga gameof water polo.The camera pans arounda smallgroup of kids playingand thena man chasesa ballaround. The boys continue playingand one man hatsoff to the camera.\nØ Ø Ø Ø Ø Ø Ø ØInput Speech\nVis2Seq\nGTA man walksup to parallelbars whilespectators, competitors, and officialsare in the background.\nThe man performsa routine on the parallelbars. The man finisheshisroutine and dismounts.\nA man walksup to a set of unevenbars.He mountsthe bars, thenspins himselfaround. He doesa handstand, thendismounts.\nInputFrames\nTheyfightover the ball, tryingto getitintothe goal.\nØ Ø Ø Ø Ø Ø Ø ØInput Speech\nVis2Seq\nGTA man isseenlookingat the camera and leads intohimplayinga poker gamewithothers. One man deals cardsand chips whilespeakingto one another. Theycontinue playingand speakingto one another.\nAman issittingbehinda table playingpoker. He deals cardsto the people, thenheputsthemon the table. The man putsthe cardson the table, and putsthe chips in the middle.\nInputFrames\nFigure 6. Examples of dense event captioning predictions by Vid2Seq on the validation set of ActivityNet Captions, compared with ground-\ntruth. The first three examples show successful predictions, while the last example illustrates a failure case where the model hallucinates\nevents that are not visually grounded (‘one man hats off to the camera‘). Note that in all of these videos, there is no transcribed speech.\nablation studies in the standard fully-supervised setting in\nSection C.2, where we ablate various factors including pre-\ntraining on long narrated videos, the pretraining dataset and\nthe size of the visual backbone, the time tokenization pro-\ncess and the number of time tokens, the sequence construc-\ntion process, the temporal positional embeddings and the\ninitialization of the language model.\nC.1. Importance of pretraining in few-shot settings\nIn Section 4.2, we show the benefits of our pretrain-\ning method in the fully-supervised setting, i.e. when using\n100% of the downstream training dataset. In Table 10, we\n16\nData Pretrain YouCook2 ViTT ActivityNet\nS C M S C M S C M\n1. 1% ✗ 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.1\n2. 1% ✓ 2.4 10.1 3.3 2.0 7.4 1.9 2.2 6.2 3.2\n3. 10% ✗ 0.1 0.0 0.2 3.3 0.4 3.3 3.4 11.9 4.6\n4. 10% ✓ 3.8 18.4 5.2 10.7 28.6 6.0 4.3 20.0 6.1\n5. 50% ✗ 1.8 8.5 2.4 6.5 18.7 3.9 4.6 13.1 6.3\n6. 50% ✓ 6.2 32.1 7.6 12.5 38.8 7.8 5.4 27.5 7.8\n7. 100% ✗ 4.0 18.0 4.6 7.9 21.2 6.2 5.4 18.8 7.1\n8. 100% ✓ 7.9 47.1 9.313.5 43.5 8.55.8 30.1 8.5\nTable 10. Impact of our pretraining on few-shot dense event\ncaptioning, by finetuning Vid2Seq using a small fraction of the\ndownstream training dataset.\nMax number\nof narrations\nYouCook2 ActivityNet\nS C F1 S C F1\n1. No pretraining4.0 18.0 18.1 5.4 18.8 49.2\n2. 1 6.0 32.1 22.1 5.1 22.9 48.1\n3. 10 6.5 34.6 23.6 5.4 27.1 50.3\n4. ∞ 7.9 47.1 27.3 5.8 30.1 52.4\nTable 11. Ablation showing the importance of pretraining on\nlong narrated videos, by varying the maximum number of narra-\ntion sentences that a randomly cropped video can cover.∞means\nthe cropping is unrestricted and can sample arbitrarily long videos.\nPretraining Data Model YouCook2 ActivityNet\nS C F1 S C F1\n1. ImageNet ViT-B/16 6.6 40.2 24.3 4.5 17.2 49.3\n2. CLIP ViT-B/16 7.7 46.3 26.5 5.6 28.4 51.7\n3. CLIP ViT-L/14 7.9 47.1 27.3 5.8 30.1 52.4\nTable 12. Ablation on the pretraining data and model size of\nthe visual backbonefs.\nfurther show that our pretraining method has a considerable\nimportance in the few-shot setting defined in Section 4.4,\ni.e. when using a smaller fraction of the downstream train-\ning dataset. In particular, our pretraining method enables\nour Vid2Seq model to have a non zero performance when\nusing only 1% of the downstream training dataset (rows 1\nand 2).\nC.2. Additional ablation studies\nWe here complement ablation studies reported in Sec-\ntion 4.2, using the same default settings, evaluation metrics\nand downstream datasets.\nPretraining on long narrated videos. In Table 1, we\nshow the benefits of pretraining on untrimmed videos in\ncomparison with the standard practice of pretraining on\nshort, trimmed, video-speech segments [35, 70, 80]. In Ta-\nble 11, we further evaluate the importance of sampling long\nnarrated videos during pretraining. By default, at each train-\ning iteration, we randomly temporally crop each narrated\nvideo without constraints, resulting in a video that can span\nTokenization N YouCook2 ActivityNet\nS C F1 S C F1\n1. Absolute 20 0.3 0.2 0.9 3.2 23.0 23.1\n2. Absolute 100 3.5 25.7 12.0 4.8 25.5 41.5\n3. Absolute 500 7.9 39.8 24.3 5.4 28.1 48.6\n4. Relative 20 7.2 39.6 23.7 5.6 29.0 49.4\n5. Relative 100 7.9 47.1 27.3 5.8 30.1 52.4\n6. Relative 500 7.2 40.0 25.0 5.7 28.6 52.5\nTable 13. Ablation on time tokenization (relative or absolute)\nand the number of time tokensN.\nDot symbol\nbetween segments\nTime tokens\nPosition\nYouCook2 ActivityNet\nS C F1 S C F1\n1. ✗ After text 7.9 48.3 26.7 5.6 29.8 51.1\n2. ✓ After text 8.3 50.9 26.2 5.7 30.4 51.8\n3. ✗ Before text8.0 50.0 27.3 5.6 28.2 50.7\n4. ✓ Before text7.9 47.1 27.3 5.8 30.1 52.4\nTable 14. Ablation on the sequence construction process.\nover hundreds of transcribed speech sentences. We here\nevaluate a baseline that constrains this cropping process\nsuch that the cropped video only spans over a given max-\nimum number of narration sentences. Even with a maxi-\nmum of 10 narration sentences, this baseline significantly\nunderperforms our model trained in default settings where\nwe sample longer untrimmed narrated videos (rows 1, 2 and\n3). This demonstrates that our model benefits from pretrain-\ning on long narrated videos.\nVisual features. In Table 4, we show the benefits of scal-\ning up the size of the pretraining dataset of narrated videos\nand the size of the language model. In Table 12, we further\nanalyze the importance of the pretraining dataset and size of\nthe visual backbone fs. We find that CLIP pretraining [77]\nconsiderably improves over ImageNet pretraining [87] with\nthe same ViT-B/16 visual backbone model (row 2 vs 1).\nFurthermore, scaling up the visual backbone size from ViT-\nB/16 to ViT-L/14 brings additional improvements (row 3 vs\n2).\nTime tokenization and number of time tokens.In Ta-\nble 13, we further ablate the time tokenization process pre-\nsented in Section 3.1. Our default time tokens represent rel-\native timestamps in a video, as we quantize a video of dura-\ntion T into N equally-spaced timestamps. Another possibil-\nity is to use time tokens that represent absolute timestamps\nin the video, i.e. the k-th token represents the k-th second in\nthe video. For both these variants, we vary the number of\ntime tokens N. For the relative time tokens, increasing N\nmakes the quantization more fine-grained but also spreads\nthe data into more time tokens. On the other hand, for the\nabsolute time tokens, increasing N increases the video du-\n17\nTemporal\nembeddings\nYouCook2 ActivityNet\nS C F1 S C F1\n1. ✗ 6.8 42.0 24.9 5.3 27.0 50.6\n2. ✓ 7.9 47.1 27.3 5.8 30.1 52.4\nTable 15. Ablation on the temporal positional embeddings.\nLanguage Model\nInitialization\nVideo-text\nPretraining\nYouCook2 ActivityNet\nS C F1 S C F1\n1. ✗ ✗ 0.9 4.2 7.6 4.3 23.7 41.2\n2. ✓ ✗ 4.0 18.0 18.1 5.4 18.8 49.2\n3. ✗ ✓ 8.8 51.3 28.4 5.7 28.7 51.2\n4. ✓ ✓ 7.9 47.1 27.3 5.8 30.1 52.4\nTable 16. Ablation on language model initialization and pre-\ntraining.\nration that the time tokens can cover. We find that the best\ndense video captioning results are obtained with the relative\ntime tokens and N = 100 time tokens (row 5).\nSequence construction. In Table 14, we further ablate the\nsequence construction process presented in Section 3.1. Our\ndefault sequence inserts the start and end time tokens of\neach segment before its corresponding text sentence. An-\nother possibility is to insert time tokens after each corre-\nsponding text sentence. We find that both variants achieve\nsimilar results (rows 2 and 4), with the default sequence\n(row 4) resulting in slightly higher event localization per-\nformance (F1 Score) but slightly lower dense captioning re-\nsults overall. Furthermore, we observe that the dot symbols\nindicating the separation between different events have low\nimportance (rows 1 and 2, rows 3 and 4).\nTemporal positional embeddings. In Table 1, we show\nthat time tokens in the speech sequence provide temporal\ninformation about the speech transcript to our model. In\nTable 15, we also evaluate the importance of the temporal\npositional embeddings which communicate temporal infor-\nmation from the visual stream to our model. We find that\nthese temporal embeddings are beneficial (row 2 vs 1).\nLanguage model initialization and pretraining.In Ta-\nble 4, we show the benefits of using T5-Base instead of\nT5-Small. In Table 16, we further investigate the impor-\ntance of initializing the language model from weights pre-\ntrained on Web text. Without pretraining on narrated videos,\nwe find that text-only initialization is helpful (rows 1 and\n2). Interestingly, after pretraining on narrated videos, we\nfind that text-only initialization has little importance (rows\n3 and 4), as it slightly improves the performance on Activ-\nityNet Captions while resulting in a slight drop of perfor-\nmance on YouCook2. We believe that this may be because\nof the domain gap between Web text and the imperative-\nstyle dense captions in YouCook2, which are more similar\nto transcribed speech in YT-Temporal-1B.\n18",
  "topic": "Closed captioning",
  "concepts": [
    {
      "name": "Closed captioning",
      "score": 0.9934303760528564
    },
    {
      "name": "Computer science",
      "score": 0.8453665971755981
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.7089358568191528
    },
    {
      "name": "Event (particle physics)",
      "score": 0.6341913938522339
    },
    {
      "name": "Paragraph",
      "score": 0.6313130855560303
    },
    {
      "name": "Sentence",
      "score": 0.5660646557807922
    },
    {
      "name": "Natural language processing",
      "score": 0.5658317804336548
    },
    {
      "name": "Language model",
      "score": 0.555342435836792
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4413677752017975
    },
    {
      "name": "Scale (ratio)",
      "score": 0.41809794306755066
    },
    {
      "name": "Speech recognition",
      "score": 0.4012581408023834
    },
    {
      "name": "Image (mathematics)",
      "score": 0.18559333682060242
    },
    {
      "name": "World Wide Web",
      "score": 0.10057505965232849
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210090411",
      "name": "DeepMind (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I44504214",
      "name": "Czech Technical University in Prague",
      "country": "CZ"
    }
  ]
}