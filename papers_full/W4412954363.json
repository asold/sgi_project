{
  "title": "Exploring the role of large language models in the scientific method: from hypothesis to discovery",
  "url": "https://openalex.org/W4412954363",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2096150816",
      "name": "Yanbo Zhang",
      "affiliations": [
        "Tufts University"
      ]
    },
    {
      "id": "https://openalex.org/A3123047215",
      "name": "Sumeer A. Khan",
      "affiliations": [
        "King Abdullah University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2168644998",
      "name": "Adnan Mahmud",
      "affiliations": [
        "Zuse Institute Berlin",
        "Network Rail"
      ]
    },
    {
      "id": "https://openalex.org/A3171037164",
      "name": "Huck Yang",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2142973922",
      "name": "Alexander Lavin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2023647893",
      "name": "Michael Levin",
      "affiliations": [
        "Tufts University"
      ]
    },
    {
      "id": "https://openalex.org/A2071642219",
      "name": "Jérémy Frey",
      "affiliations": [
        "University of Southampton"
      ]
    },
    {
      "id": "https://openalex.org/A1983711249",
      "name": "Jared Dunnmon",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2101100271",
      "name": "James Evans",
      "affiliations": [
        "Santa Fe Institute",
        "University of Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A2000084407",
      "name": "Alan Bundy",
      "affiliations": [
        "University of Edinburgh"
      ]
    },
    {
      "id": "https://openalex.org/A1089869590",
      "name": "Saso Dzeroski",
      "affiliations": [
        "Jožef Stefan Institute"
      ]
    },
    {
      "id": "https://openalex.org/A1533552536",
      "name": "Jesper Tegner",
      "affiliations": [
        "Karolinska University Hospital",
        "Science for Life Laboratory",
        "Karolinska Institutet",
        "King Abdullah University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A32872769",
      "name": "Hector Zenil",
      "affiliations": [
        "Centre for Sustainable Healthcare",
        "The Francis Crick Institute",
        "King's College London",
        "The Alan Turing Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2096150816",
      "name": "Yanbo Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3123047215",
      "name": "Sumeer A. Khan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2168644998",
      "name": "Adnan Mahmud",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3171037164",
      "name": "Huck Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2142973922",
      "name": "Alexander Lavin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2023647893",
      "name": "Michael Levin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2071642219",
      "name": "Jérémy Frey",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1983711249",
      "name": "Jared Dunnmon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101100271",
      "name": "James Evans",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2000084407",
      "name": "Alan Bundy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1089869590",
      "name": "Saso Dzeroski",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1533552536",
      "name": "Jesper Tegner",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A32872769",
      "name": "Hector Zenil",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4385490607",
    "https://openalex.org/W4362563893",
    "https://openalex.org/W3170949898",
    "https://openalex.org/W4365145591",
    "https://openalex.org/W3082465854",
    "https://openalex.org/W4323697696",
    "https://openalex.org/W3215998283",
    "https://openalex.org/W2915854813",
    "https://openalex.org/W4385722611",
    "https://openalex.org/W4311767166",
    "https://openalex.org/W4313462484",
    "https://openalex.org/W3111353854",
    "https://openalex.org/W4292122375",
    "https://openalex.org/W3216014139",
    "https://openalex.org/W3132956480",
    "https://openalex.org/W4367188881",
    "https://openalex.org/W4287307484",
    "https://openalex.org/W4365143687",
    "https://openalex.org/W4205164650",
    "https://openalex.org/W4295951577",
    "https://openalex.org/W3084171410",
    "https://openalex.org/W4291023040",
    "https://openalex.org/W3016097036",
    "https://openalex.org/W4383346782",
    "https://openalex.org/W3138378275",
    "https://openalex.org/W4389173934",
    "https://openalex.org/W4388525110",
    "https://openalex.org/W4391293808",
    "https://openalex.org/W4396681775",
    "https://openalex.org/W4391301614",
    "https://openalex.org/W4387559225",
    "https://openalex.org/W4405185373",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4321649710",
    "https://openalex.org/W4399597451",
    "https://openalex.org/W4390730779",
    "https://openalex.org/W4379539325",
    "https://openalex.org/W4387430850",
    "https://openalex.org/W4399655920",
    "https://openalex.org/W4391631327",
    "https://openalex.org/W6910787293",
    "https://openalex.org/W4392619039",
    "https://openalex.org/W4384662964",
    "https://openalex.org/W4389519254",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W4226159083",
    "https://openalex.org/W4297253404",
    "https://openalex.org/W6891912544",
    "https://openalex.org/W4389991792",
    "https://openalex.org/W4396723768",
    "https://openalex.org/W4395703652",
    "https://openalex.org/W4399151368",
    "https://openalex.org/W6838461927",
    "https://openalex.org/W4404349982",
    "https://openalex.org/W4392168151",
    "https://openalex.org/W4392736559",
    "https://openalex.org/W4405669265",
    "https://openalex.org/W4399922932",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W6790978476",
    "https://openalex.org/W3203931103",
    "https://openalex.org/W4401857375",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W4393241776",
    "https://openalex.org/W2967956122",
    "https://openalex.org/W1561403150",
    "https://openalex.org/W4389518213",
    "https://openalex.org/W4399175313",
    "https://openalex.org/W4402672034",
    "https://openalex.org/W4401058225",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W6873698140",
    "https://openalex.org/W4403363385",
    "https://openalex.org/W4399357038",
    "https://openalex.org/W4378505261",
    "https://openalex.org/W4404266811",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4225621975",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W2982316861",
    "https://openalex.org/W4388299828",
    "https://openalex.org/W3042021489",
    "https://openalex.org/W6855844941",
    "https://openalex.org/W4402667050",
    "https://openalex.org/W3034789704",
    "https://openalex.org/W4313418308",
    "https://openalex.org/W2100390860",
    "https://openalex.org/W3122503641",
    "https://openalex.org/W4319083882",
    "https://openalex.org/W4386655789",
    "https://openalex.org/W3036479503",
    "https://openalex.org/W4393299873",
    "https://openalex.org/W4389518758",
    "https://openalex.org/W4394653315",
    "https://openalex.org/W4411037930",
    "https://openalex.org/W4404283941",
    "https://openalex.org/W4385447813",
    "https://openalex.org/W4404782659",
    "https://openalex.org/W4401042999",
    "https://openalex.org/W4380715521",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W2964612906",
    "https://openalex.org/W4404792833",
    "https://openalex.org/W4383987670",
    "https://openalex.org/W4402670429",
    "https://openalex.org/W4306801963",
    "https://openalex.org/W4389520477",
    "https://openalex.org/W4389518784",
    "https://openalex.org/W4389519598",
    "https://openalex.org/W4385774833",
    "https://openalex.org/W4404780828",
    "https://openalex.org/W4389518992",
    "https://openalex.org/W4410357125",
    "https://openalex.org/W4399401573",
    "https://openalex.org/W4378942694",
    "https://openalex.org/W4284701759",
    "https://openalex.org/W4403122979",
    "https://openalex.org/W4383473944",
    "https://openalex.org/W4391631244",
    "https://openalex.org/W4390690092",
    "https://openalex.org/W4387355948",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4402671787",
    "https://openalex.org/W4384648622",
    "https://openalex.org/W2970726176",
    "https://openalex.org/W2934842096",
    "https://openalex.org/W4390678101",
    "https://openalex.org/W4405957810",
    "https://openalex.org/W2947443352",
    "https://openalex.org/W3046392261",
    "https://openalex.org/W4386541616",
    "https://openalex.org/W6863969017",
    "https://openalex.org/W4400734098",
    "https://openalex.org/W4384200891",
    "https://openalex.org/W2949066452",
    "https://openalex.org/W2257979135",
    "https://openalex.org/W2902907165",
    "https://openalex.org/W6929261859",
    "https://openalex.org/W2127222213",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4376874793",
    "https://openalex.org/W6798182279",
    "https://openalex.org/W4389519928",
    "https://openalex.org/W4377130677",
    "https://openalex.org/W2276021375",
    "https://openalex.org/W1988955171",
    "https://openalex.org/W2919353913",
    "https://openalex.org/W4247067363",
    "https://openalex.org/W4393396970",
    "https://openalex.org/W3178108585"
  ],
  "abstract": "Abstract We review how Large Language Models (LLMs) are redefining the scientific method and explore their potential applications across different stages of the scientific cycle, from hypothesis testing to discovery. We conclude that, for LLMs to serve as relevant and effective creative engines and productivity enhancers, their deep integration into all steps of the scientific process should be pursued in collaboration and alignment with human scientific goals, with clear evaluation metrics.",
  "full_text": "npj |artiﬁcial intelligence Perspective\nhttps://doi.org/10.1038/s44387-025-00019-5\nExploring the role of large language\nmodels in the scientiﬁc method: from\nhypothesis to discovery\nCheck for updates\nYanbo Zhang1,S u m e e rA .K h a n2,3,4, Adnan Mahmud5,6, Huck Yang7, Alexander Lavin8, Michael Levin1,9,\nJeremy Frey10,J a r e dD u n n m o n11, James Evans12,13,A l a nB u n d y14, Saso Dzeroski15,\nJesper Tegner2,16,17,18,19 &H e c t o rZ e n i l20,21,22,23,24\nWe review how Large Language Models (LLMs) are redeﬁning the scientiﬁc method and explore their\npotential applications across different stages of the scientiﬁc cycle, from hypothesis testing to\ndiscovery. We conclude that, for LLMs to serve as relevant and effective creative engines and\nproductivity enhancers, their deep integration into all steps of the scientiﬁc process should be pursued\nin collaboration and alignment with human scientiﬁc goals, with clear evaluation metrics.\nWith recent Nobel Prizes recognising AI contributions to science, Large\nLanguage Models (LLMs) are transforming scientiﬁc research by enhancing\nproductivity and reshaping the scientiﬁc method. LLMs are now involved in\nexperimental design, data analysis, and workﬂows, particularly in chemistry\nand biology.\nRecent advances in artiﬁcial intelligence (AI) have transformed mul-\ntiple areas of society, the worldeconomy, and academic and scientiﬁc\npractice. Generative AI and Large Language Models (LLMs) present\nunprecedented opportunities to transform scientiﬁc practice, advance Sci-\nence, and accelerate technological innovation. Nobel Prizes in Physics and\nChemistry were awarded to several AI leaders for their contributions to AI\nand frontier models, such as Large Language Models (LLMs). This promises\nto transform or contribute to scientiﬁc research by enhancing productivity\nand supporting various stages of the scientiﬁcm e t h o d .T h eu s eo fA Ii n\nscience is booming across numerous scientiﬁc areas and is impacting dif-\nferent parts of the scientiﬁcm e t h o d .\nDespite the potential of LLMs for hypothesis generation and data\nsynthesis, AI and LLMs face challenges in fundamental science and scientiﬁc\ndiscovery. Hence, our premise in our perspective is that AI, in general, has so\nfar been limited in its impact on fundamental science, which is deﬁned here\nas the discovery of new principles or new scientiﬁc laws. Here, we review\nhow LLMs are currently used— as a technological tool— to augment the\nscientiﬁcp r o c e s si np r a c t i c ea n dh o wt h e ym a yb eu s e di nt h ef u t u r ea st h e y\nbecome more powerful tools and develop into powerful scientiﬁc assistants.\nCombining data-driven techniques with symbolic systems, such a system\ncould fuse into hybrid engines that may lead to novel research directions.\nWe aim to describe the gap between LLMs as technical tools and“creative\nengines” that could enable new high-quality scientiﬁc discoveries and pose\nnovel questions and hypotheses to human scientists. Weﬁrst review the\ncurrent use of LLMs in Science, aiming toidentify limitations that need to be\naddressed when moving toward creative engines.\nThere is solid recognition and excitement for the transformative\npotential of AI in Science. For example, leading machine learning con-\nferences (NeurIPS, ICML) have recently (2021–2023) arranged targeted\nworkshops on AI4Science. Some recent reviews and papers include\n1–38.T h i s\ndemonstrates the energy and potential of using automated (i.e., AI tools) for\nScience. This“dream” can be traced back to the times of Turing and the\nemergence of Artiﬁcial Intelligence in the 1950s\n39. With recent advance-\nments in computational techniques, vastly increased production of scientiﬁc\ndata, and the rapid evolution of machinelearning, this long-held vision can\nbe transformed into reality. Yet, most current reviews and original papers\nfocus on speciﬁcally designed machine learning architectures targeting\nparticular application domains or problems.\nFor example, recent reviews have explored how to use variants of Deep\nLearning, Geometric Deep Learning, or Generative AI in its generality\n(including different architectures such as CNNs, GNNs, GANs, diffusion\nmodels, VAEs, and Transformers) as a tool for assisting Science\n3,11,13,15,19,22.\nFor example, Wang et al.1, reviews breakthroughs in how speciﬁc techni-\nques such as geometric deep learning, self-supervised learning, neural\noperators, and language modelling have augmented Science in protein\nfolding, nuclear fusion, and drug discovery. An essential thread in their\nreview is the vital notion of representation, pointing out that different AI\narchitectures can support valuable representations of scientiﬁcd a t aa n d\nthereby augment Science. Recent papers demonstrate the appeal and the\npotential of using AI-driven and augmented tools for automating\nscience\n1,4,13,40. Traditional scientiﬁc advancements have been primarily dri-\nven by hypothesis-led experimentationand theoretical development, often\nlimited by human cognitive capacities and manual data processing. For\nexample, the formulation of Newtonian mechanics required meticulous\nobservation and mathematical formalization over extended periods. Here,\nthe rise of AI4Science represents a paradigmatic revolution that could reach\nbeyond human cognitive limitations.AI-driven advancements promise to\nA full list of afﬁliations appears at the end of the paper.e-mail: hector.zenil@kcl.ac.uk\nnpj Artiﬁcial Intelligence|            (2025) 1:14 1\n1234567890():,;\n1234567890():,;\nenable rapid processing and analysis of massive data sets, revealing complex\npatterns that surpass human analytical capabilities. For example, Deep-\nMind’s AlphaFold dramatically transformed protein structure prediction, a\nlongstanding scientiﬁc challenge, using deep learning to predict protein\nfolding accurately. Furthermore, AI4Science could reverse the slowdown in\nscientiﬁc productivity in recent years, where literature search and peer-\nreview evaluation\n41–43 are bottlenecks.\nIn contrast to previous reviews, here weﬁrst address the use of LLMs,\nregardless of the speciﬁc underlying architecture, and their use as a tool for\nthe scientiﬁc process. We assess how different areas of science use LLMs in\ntheir respective scientiﬁc process. This analysis sets the stage for asking how\nLLMs can synthesize information, generate new ideas and hypotheses, guide\nthe scientiﬁc method, and augment fundamental scientiﬁc discoveries.\nHere, we ask to what extent AI can be described as a“general method of\ninvention,” which could open up new paradigms and directions of scientiﬁc\ninvestigations. Hence, complementary to a purely representational and\narchitectural viewpoint of AI4Science, weﬁnd it constructive to ask and\nassess to what extent the nature of the scientiﬁc process, both its inductive\na n dd e d u c t i v ec o m p o n e n t s ,c a na n ds h o u l db et r a n s f o r m e db yA I\ntechniques.\nCurrent use of LLMs— from specialised scientiﬁc\ncopilots to LLM-assisted scientiﬁc discoveries\nT h ea b i l i t yo fL a r g eL a n g u a g eM o dels (LLMs) to process and generate\nhuman-like text, handle vast amounts of data, and analyse complex patterns\nwith potentially some reasoning capabilities has increasingly set the stage for\nthem to be used in scientiﬁc research across various disciplines. Their\napplications range from simple tasks, such as acting as copilots to assist\nscientists, to complex tasks, such as autonomously performing experiments\nand proposing novel hypotheses. We willﬁrst introduce the fundamental\nconcepts of LLMs and then review their various applications in scientiﬁc\ndiscovery.\nPrompting LLMs: from chatbot to prompt engineering\nCurrent mainstream LLMs are primarily conditional generative models,\nwhere the input, such as the beginning of a sentence or instructions, serves as\na condition, and the output is the generated text, such as a reply. This text is\ntypically sampled auto-regressively: the next token (considered the building\nblock of words) is sampled from a predicted distribution. See Fig.1A.\nGiven LLMs’capabilities in computation and emerging potential for\nreasoning, which we deﬁne as the ability to solve tasks that require rea-\nsoning, they can be considered programming languages that use human\nlanguage as the code that instructs them to perform desired tasks. This\ncode takes the form of“prompts.” For instruct-tuned LLMs, the prompt\noften consists of three parts: the system prompt and the user prompt, with\nan LLM’s reply considered the assistant prompt. Hence, a chat is fre-\nquently composed of <system > <user > <assistant > <user > <\nassistant>, see Fig. 1B. The system prompt typically includes general\ninstructions for the LLMs, such as behaviour, meta-information, format,\netc. The user prompt usually contains detailed instructions and questions.\nUsing these prompts, the LLMs generate replies under the role of\n“assistant.\nSince LLMs do not have background knowledge about the user, and\nprompts are their major input, designing a good prompt is often critical to\nachieving the desired output and superior performance. Researchers have\nshown that speciﬁc prompts, including accuracy, creativity, and reasoning,\ncan signiﬁcantly improve output performance. Speciﬁcally, the chain-of-\nthought (CoT) method\n44 can instruct LLMs to think step-by-step, leading to\nbetter results. Beyond these, the Retrieval-augmented Generation (RAG)\nmethod45 can incorporate a large amount of context by indexing the con-\ntents and retrieving relevant materials, then combining the retrieved\ninformation with prompts to generatethe output. Due to the importance of\nprompts and LLM agents, designing prompts is now often called“prompt\nengineering,” and many techniques and tricks have been developed in this\narea\n46,47, such as asking JSON format outputs, formulating clear instructions,\nsetting temperatures, etc47,48.\nWhile carefully designed prompts can accomplish many tasks, they are\nn o tr o b u s ta n dr e l i a b l ee n o u g hf o rc o m p lex tasks requiring multiple steps or\nnon-language computations, nor canthey explore autonomously. LLM\nagents are developed for these requirements, especially for complex tasks.\nLLM agents are autonomous systems powered by LLMs, which can actively\nseek to observe environments, make decisions, and perform actions using\nexternal tools\n49. In many cases, we need to ensure reliability, achieve high-\nperformance levels, enable automation, or process large amounts of context.\nThese tasks cannot be accomplished solely with LLMs and require inte-\ngrating LLMs into agent systems. Early examples include AutoGPT\n50 and\nBabyAGI51, where LLMs are treated as essential tools within the agent\nsystem (Fig.1C). In scientiﬁc discovery, LLM agents become even more\ncritical due to the complexity of science and its high-performance\nrequirements. Many tools have also been developed to provide easy access\nto these prompting and agent methods, such as LangChain 52 and\nLlamaIndex53. Automated prompt design methods, such as DSPy54 and\nTextGrad55, are also being developed to design prompts and LLM agents in a\ndata-driven way.\nFig. 1 | Auto-regressive generation, dialogue prompting, and agent frameworks\nin large language models. ALLMs generate sentences in an auto-regressive manner,\nsampling tokens from a predicted distribution at each step.B A typical prompt for\nLLMs consists of a system prompt and a user prompt. The LLM will then respond as an\nassistant. A multi-round dialogue will repeat the user and assistant contents.C LLM\nagents are systems that use a large language model as its core reasoning and decision-\nmaking engine, enabling it to interpret instructions, plan actions, and autonomously\ninteract with external tools, environments, or other LLM agents to fulﬁl a given goal.\nhttps://doi.org/10.1038/s44387-025-00019-5 Perspective\nnpj Artiﬁcial Intelligence|            (2025) 1:14 2\nLLMs as practical scientiﬁc copilots\nThe ability of LLMs to work with a large body of text is being exploited in the\npractice of science. For example, LLMs assist in proposing novel ideas,\nwriting scientiﬁc papers and generating computer code, thereby improving\nproductivity; they also adapt texts for diverse audiences ranging from\nexperts to broader audiences, thus supporting communication in science.\nFurthermore, LLMs can sift through vast bodies of scientiﬁcl i t e r a t u r e\nto identify relevant papers,ﬁndings, and trends. Such reviewing of the\nrelevant literature helps investigators quickly digest and identify gaps in\nenormous bodies of scientiﬁc knowledge. These capabilities can also miti-\ngate discursive barriers across different scientiﬁc ﬁelds, supporting inter-\ndisciplinary scientiﬁc collaborations and knowledge sharing. Recently,\nchatbots have emerged in several disciplines as virtual assistants answering\nscientiﬁc queries posed by scientists. Such tools exploit the power of LLMs to\nextract and detect patterns, data, and knowledge. These techniques may also\nserve as important tools in science education and communication.\nThese examples demonstrate the rise of LLMs in extracting and sharing\ninformation and the exciting open research frontier of the potential of\nreasoning that they represent in different scientiﬁcd o m a i n s\n56–58.F o r\ninstance, Cauﬁeld et al. proposed the SPIRES method59, which uses LLMs to\nextract structured data from the literature. Beyond data extraction, LLMs\nhave also shown evidence of outperforming annotation tasks\n60,61,e n a b l i n g\nscientists to scale data annotation. Some domain-speciﬁcm o d e l sa l s os h o w\nsuperior performance in classiﬁcation, annotation, and prediction tasks62–64.\nWith the help of RAG methods45, LLMs can directly apply their information\nextraction and distillationcapabilities to large amounts of text data. With the\ncombination of diverse capabilities of LLMs interconnected through LLM-\nagents, the recent“AI co-scientist65” demonstrates impressive ability in\ngenerating novel research ideas by leveraging existing literature, engaging in\ninternal LLM-agent debates, and reﬁning its outputs. This process leads to\nconstructive progress when applied to real scientiﬁct a s k s .\nMoreover, LLMs are currently used to automate the experimental\ndesign and the execution of experiments. For example, Boiko et al.66 pro-\npose an autonomous LLM capable of performing chemical experiments.\nThis work employs an LLM planner to manage the experimental process,\nsuch as drawing patterns on plates orconducting more complex chemical\nsyntheses. Compared to hard-codedplanners, the LLM-based planner is\nmore ﬂexible and can handle unexpectedsituations. Similar kinds of loop\nand tool usage are also shown in ref.67, which includes literature tools,\nconsulting with humans, experimental tools, and safety tools.\nIn the biological domain, for instance, the CRISPR-GPT\n68 represents a\nsigniﬁcant advancement in biological research. It utilizes LLMs to automate\nthe design of gene-editing experiments, enhancing both the efﬁciency and\nprecision of genetic modiﬁcations, which is pivotal in speeding up genomic\nresearch and applications. Another advance in the application of LLMs in\nthe biological domain is BioDiscoveryAgent\n69. These tools augment scien-\ntists’capabilities and accelerate scientiﬁcd i s c o v e r y .\nThe capabilities described thus far capture the current use of LLMs as\nknowledge engines. Summarising, extracting, interfacing, and reasoning\nabout (scientiﬁc) text, alongside automating experimental design and\nexecution. While immensely useful, it remains an open frontier on how to\ndo this safely and efﬁciently. It largely depends on how prompting is per-\nformed and how LLM agent systems are designed.\nFoundation models for science\nA key observation when using LLMs as clever text engines or exploiting the\nunderlying machine learning (neural) architecture for solving speciﬁcs c i -\nentiﬁc problems was the importance of scale. Larger models trained on\nlarger amounts of data, or spending larger amounts of computation during\ninference time yielded an increase in performance\n56,70,71.T h ed i s c o v e r yo f\nsuch scaling laws72 demonstrated that LLMs’performance improves as the\nnumber of parameters increases. Thus, we can expect the above trends to\ngrow in importance as these systems are trained on ever larger amounts of\ndata. Emergent behaviours, such as reasoning were suggested when models\nincreased in scale\n73. Concurrent with the appreciation of scaling laws came\nthe realisation that instead of using LLMs for specialised problems or as text\nengines, one could potentially train them on large amounts of data, not\nnecessarily text, but different modalities of scientiﬁc data. This is the idea of a\nfoundation model. These are large-scale pre-trained models that, when\ntrained with enough data of different types, such models“learn” or\n“encapsulate” k n o w l e d g eo fal a r g es c i e n t iﬁcd o m a i n .T h en o t i o no ff o u n -\ndation models refers to their generality in that they can be adapted to many\ndifferent applications, unlike task-speciﬁc engineered models solving a\nspecialised task such as protein foldin g .N o t a b l y ,t h ef a m o u st r a n s f o r m e r\narchitecture that fuels LLMs has become the architecture of choice when\nconstructing the foundational models in different domains of science. These\nself-supervised models are usually pre-trained on extensive and diverse\ndatasets. This enables them to learn from massive unlabelled data since\nmasking parts of the data and then requiring the model to predict the\no c c l u d e dp a r t sp r o v i d e sf o u n d a t i o nm o d e l sw i t ht h e i rl e a r n i n go b j e c t i v e .\nThis technique is used when training LLMs on large amounts of text. The\nidea is thus exploited in scientiﬁcd o m a i n sw h e r em u l t i - m o d a ld a t ai su s e d\nto train self-supervised foundationmodels. Once trained, the model can be\nﬁne-tuned for various downstream tasks without requiring additional\ntraining. Consequently, the same model can be applied to a wide range of\ndownstream tasks. The foundation model lossly compresses or encapsulates\na large body of scientiﬁc “knowledge” inherent in the training data.\nLeveraging these ideas, there has been a rise in the number of foun-\ndation models of science. For example, the Evo and Evo 2 models enable\nprediction and generation tasks from the molecular to the genome scale\n74.\nWhile Evo is trained on millions of prokaryotic and phage genomes, Evo 275\nincludes massive eukaryotic genomes, and both demonstrate zero-shot\nfunction prediction across DNA, RNA, and protein modalities. It excels at\nmultimodal generation tasks, as shown by generating synthetic CRISPR-Cas\nmolecular complexes and transposable systems. The functional activity of\nEvo-generated CRISPR-Cas molecular complexes and IS200 and IS605\ntransposable systems was experimentally validated, representing theﬁrst\nexamples of protein-RNA and protein-DNA co-design using a language\nmodel. Similarly, scGPT is for learning single cell transcriptional data\n76,\nChemBERT encodes molecular structures as strings, which then can be used\nfor different downstream tasks such as drug discovery and material\nscience\n77. Similarly, OmniJet-α is theﬁrst cross-task foundation model in\nparticle physics, enhancing performance with reduced training needs78.\nAdditionally, multiple physics pretraining (MPP) introduces a task-agnostic\napproach to modelling multiple physical systems, improving predictions\nacross various physics applications without extensiveﬁne-tuning\n79.T h e\nLLM-SR80 implements similar symbolic regression methods iteratively,\ngenerating and evaluating hypotheses, using the evaluation signal to reﬁne\nand search for more hypotheses.\nIncorporating diverse scientiﬁc data modalities, which represent dif-\nferent“languages” to interact with observations beyond natural language, is\ncrucial. There are two major approaches emerging: (1) End-to-end training\non domain-speciﬁc modalities: Models like ChemBERT77 (using chemical\nSMILES strings) and scGPT76 (using single-cell data), as mentioned above,\nare directly trained on these specialized data types. (2) Separate training with\ncompositional capabilities: This involves training separate encoders for new\nmodalities or enabling LLM agents to utilize tools that interact with these\nmodalities. For instance, models like BiomedCLIP\n81 connect biological\nimages with natural language, while PaperCLIP82 and AstroCLIP83 link\nastronomical images and spectral data to textual descriptions. Furthermore,\nframeworks like ChemCrow\n84 leverage the tool-using abilities of LLMs to\nconnect with non-natural-language modalities, such as chemical\nanalysis tools.\nYet, as with text-based LLMs, several challenges remain. These include\npotential biases in datasets, which can bias the performance and output of\nthese models. Since science is mainly about understanding systems, the scale\nand opaqueness of these models make interpretation a particularly chal-\nlenging problem. Also, several observations, such as their capability for\ngeneralisation, multi-modality, and apparent emergent capabilities, have led\nto intense discussions at the research frontier on the extent to which these\nhttps://doi.org/10.1038/s44387-025-00019-5 Perspective\nnpj Artiﬁcial Intelligence|            (2025) 1:14 3\nfoundation models can reason within and beyond their training regimes.\nThe text-based LLMs (or models incorporated with text modality) discussed\nabove are constructed using these techniques. Examples include GPT-4\n(OpenAI)\n85, BERT (Bidirectional Encoder Representation from\nTransformers)86, CLIP (Contrastive Language-Image Pre-training,\nOpenAI)87,a n dD A L L - Ef r o mO p e n A I88.\nThese foundation models have the potential to achieve profes-\nsional human-level performance or even surpass human capabilities\nwhen trained using reinforcement learning, particularly with feedback\nfrom reliable formal systems. For example, AlphaProof 89 has become\nstate-of-the-art in au tomated theorem-proving systems, achieving\nmathematical capabilities comparable to human competitors at IMO\n2024. Approximately one million inf ormal mathematical problems\nwere translated into the formal language LEAN, a mathematical proof\nveriﬁcation language, enabling the LLM to be trained through rein-\nforcement learning. Solutions generated by the LLM in LEAN are\neither proved or disproved by the LEAN compiler, with the resulting\ncorrect or incorrect solutions serving as feedback to reﬁne the LLM.\nWhile this approach has been explicitly applied within the mathema-\ntical domain, it demonstrates signiﬁcant potential for training LLMs to\nsurpass human performance in highly complex and deductive rea-\nsoning. Although developing formal systems for general tasks remains\nchallenging, reinforcement learning methods are employed to build\nfoundation models with enhanced deductive capabilities, leading to the\nrise of reasoning models such as OpenAI o1/o3\n70, Deepseek R156,a n d\nothers. In scientiﬁc domains such as physics, external and reliable\nfeedback mechanisms are already used to improve answer quality90,\nhighlighting the potential for creating domain-speci ﬁcf o u n d a t i o n\nmodels.\nIn conclusion, the rise of foundation models will continue to affect\nand disrupt science due to their powerful nature, scaling properties, and\nability to handle very different data modalities. However, for our pur-\nposes, the question remains of what extent foundation models could be a\nproper gateway to making fundamental scientiﬁc discoveries. To what\nextent can foundation models be creative and reason outside their\ntraining domains?\nToward large language models as creative sparring\npartners\nWhat is required for an AI to be able to discover new principles of scientiﬁc\nlaws from observations, available conjectures, and data analysis? Broadly,\ncan generative AI develop to become a“creative engine” that can make\nfundamental scientiﬁc discoveries and pose new questions and hypotheses?\nEinstein famously stated,“If I had an hour to solve a problem, I’ds p e n d\n55 min thinking about the problem and 5 min thinking about solutions”.T h i s\nunderscores the importance of carefully considering the question or pro-\nblem itself, as posing hypotheses effectively can be the most intellectually\ndemanding part of Science. As aﬁrst approximation, the ability to pose\nnovel hypotheses is— at least for us humans— what appears to be essential\nfor making novel discoveries. Thus, what is required for an AI to advance\nbeyond a valuable tool for text generation and engineered systems for sol-\nving a particular problem? Or could foundation models provide a possible\npath forward?\nIn our view, if LLMs are to contribute to fundamental Science, it is\nnecessary to assess what putative roles LLMs can play in the core of the\nscientiﬁc process. To this end, we discuss below how LLMs can augment the\nscientiﬁc method. This includes how LLMs could support observations,\nautomate experimentation, and generate novel hypotheses. We will also\nexplore how human scientists can collaborate with LLMs.\nAugmenting the scientiﬁcm e t h o d\nAs aﬁrst approximation, scientiﬁc discovery can be described as a reward-\nsearching process, where scientists propose hypothetical ideas and verify or\nf a l s i f yt h e mt h r o u g he x p e r i m e n t s91. Under this Popperian formulation,\nLLMs can assist scientiﬁc discovery in two ways (Fig.2): On the one hand,\nLLMs could assist in the hypothesis-proposing stage, helping scientistsﬁnd\nnovel, valuable, or potentially high-reward directions or even propose\nhypotheses that human scientists might have difﬁculty generating. On the\nother hand, LLMs have the potential to make experiments more efﬁcient,\naccelerate the search process, and reduce experimental costs.\nAt the stage of proposing hypotheses, scientists choose unknown areas\nto explore, which requires a deep command of domain knowledge, incor-\nporating observational data, and manipulating existing knowledge in novel\nFig. 2 |Illustration of the scientiﬁc discovery process: Scientiﬁc research can be\nformulated as a search for rewards in an abstract knowledge space. By synthesizing\nexisting knowledge— represented by blue disks (human-discovered) and stars\n(human-machine discovered))— in novel ways, new knowledge (indicated by red\nstars) can be explored. For speciﬁc research, scientists or LLMs need to traverse the\nhypotheses-experiment-observation loop, where hypotheses are proposed based on\nexisting knowledge (including LLM knowledge, and additional literature provided\nvia RAG methods), observation, and the creativity of LLMs. Then, with aid of\nexternal tools such as programming languages, formal validations, and other\nmethodologies, experiments are conducted to test the hypotheses or gather data for\nfurther analysis. The experimental results can be observed and described through the\nobservation process, facilitated by domain-speciﬁc models and the multi-modality\ncapabilities of language models. All these parts–observation, proposing hypotheses,\nconducting experiments, and automation–can be assisted by LLMs and LLM-agents,\nconsidering the non-trivial implementations of scientiﬁc environments in silico.\nhttps://doi.org/10.1038/s44387-025-00019-5 Perspective\nnpj Artiﬁcial Intelligence|            (2025) 1:14 4\nways45,92. Their expertise and creativity could carry the potential for pro-\nposing novel research hypotheses.\nThen, at the veriﬁcation stage, experiments are conducted to obtain\nrelevant information and test hypotheses. This requires the ability to plan\nand design experiments effectively. Given LLMs’planning capabilities and\npotential understanding of causality93–95, they can help scientists design\nexperiments. By incorporating tool-using abilities96,L L M sc a nd i r e c t l y\nimplement experiments. LLM agents can perform complex workﬂows and\nundertake repetitive explorations that are time-consuming for human sci-\nentists. This allows us to search for novel results efﬁciently, which is key to\nscientiﬁcd i s c o v e r y97,98.\nThis process often involves a trial-and-error loop for a research topic or\nquestion. Thus, scientiﬁc discovery requires the following steps: observation,\nhypothesis proposal, experimentation, and automating the loop (see Figure\n2 and Table1 for detailed illustrations).\nExpanding or narrowing the observation process\nScientists rely upon observational results for guidance in proposing\nhypotheses, designing and reﬁning experiments, evaluating experimental\nresults, and validating their hypotheses. In general, observations act as\ndimension reduction methods\n99, which include annotating, classiﬁcation,\nand information extraction.\nGeneral purpose LLMs, such as GPT-4, Llama, can be good observers\nfor language and image data for general purposes. Their in-context and\nzero-shot learning capabilities can be used as universal classiﬁers to extract\nspeciﬁc information from these data, suchas annotation and evaluation. In\ndomains like NLP and Social Science, annotating and evaluating language\ndata at scale is a fundamental task fordownstream experiments. Trained\nhumans or crowd-workers have oftendone such jobs. However, LLMs, such\nas ChatGPT, can perform higher or comparable performance levels relative\nto crowd-workers on annotation tasks, especially on more challenging\ntasks\n60,61.\nBesides language processing, scientists must also describe complex\nbehaviours at scale qualitatively. LLMs show potential in describing such\ncomplex black-box systems, where we observe only their inputs and outputs\nwithout knowledge of their underlying mechanisms. Although deciphering\nsuch systems can often become a stand-alone research question, having a\nqualitative description can still be helpful when faced with large-scale data.\nWith LLMs, black-box systems, such aslanguage input-output, mathema-\ntical input-output pairs, fMRI data\n100, or observational data, can be descri-\nbed using natural language100.\nBeyond text and text-represented systems, different data modalities\nrepresent different“languages” to interact with observations, and domain-\nspeciﬁc modalities are extremely important for scientiﬁc discovery. Scien-\ntiﬁc research often involves other data types, including image, video, audio,\ntable101,102,o re v e ng e n e r a lﬁles103,a sw e l la sd o m a i n - s p e c iﬁc modalities like\ngenomic sequences, chemical graphs, or spectra76,77,82,83. Multi-modality\nLLMs can play the observer role vis-a-v i st h e s ed a t a .H o w e v e r ,m o s tm u l t i -\nmodality LLMs are still struggling to handle some domain-speciﬁcd a t a\nformats, such as genomic data or chemical compounds, which may require\nconverting and where information may be lost during the conversion\nprocess.\nFor highly specialised domains, domain-speciﬁcL L M st r a i n e do n\nspecialised data can achieve superior performance within their respective\nﬁelds (representing the end-to-end approach discussed earlier). For exam-\nple, with the same number of parameters, BioGPT\n64 outperforms GPT-2\nmedium104 when trained on domain-speciﬁc data. Even with fewer para-\nmeters, models like PubMedBERT62 can perform at a level comparable to\nGPT-2 medium. In the chemical domain, LLMs have been pre-trained on\nchemical SMILES data\n105, enabling them to infer molecular properties and\nbiological activities51. LLM-inspired models are also useful for case-speciﬁc\ntasks. In106, transformers are trained on cellular automata to study the\nrelationship between complexity and intelligence. This highlights the\nimportance of exploring domain-speciﬁc and case-speciﬁc LLM and the\nopportunities for further exploration in this area.\nExperimentation and automation\nThe experiment is a critical part of all research steps, including making\nobservations and validating the hypothesis. Both humans and LLMs need\nexternal tools to implement experiments. Speciﬁcally, this involves calling\nexternal functions or directly generating and running code. LLMs that have\nbeen ﬁne-tuned for tool usage\n85,96 can generate structured entities (often in\nJSON) that contain the function name and inputs to be implemented by\nexternal functions. These functions are versatile and can include simple\ncalculations, laboratory control functions, external memory, requests for\nassistance from human scientists, etc. LLMs can also direct programming by\ngenerating and running code for complex experiments requiringﬁne-\ngrained control or enhancing the calculation abilities of LLMs\n107,108.B e y o n d\nthis, generated programmes can also call other functions or be saved into a\nfunction library, enabling the combinatory development of complex\nactions\n109.\nFor complex experiments, planningbecomes important, which involves\nsetting up an objective and decomposing it into practical steps. This is critical\nto solving complex tasks while sustaining coherent behaviour. While the\nplanning capabilities of LLMs are questioned in many studies, certain tools\nand methods still demonstrate valuable assistance. The chain-of-thought\n(CoT)\n44 method signiﬁcantly improves various tasks by decomposing a\nquestion into steps. In complex taskswith more steps, where LLMs seek long-\nterm objectives and interact with environments, they can generate plans in\nnatural language based on given objectives\n110. It is also important to adapt to\nobservations and unexpected results .F o rt h i sr e a s o n ,m e t h o d sl i k e\nReﬂexion111, ReAct112 combine the CoT and planning, dynamically update its\nplans, manage exceptions, and utilizes external information. And it also\novercomes hallucination and error propagation in the chain-of-thought.\nTable 1 | How LLMs can assist scientiﬁc methods at different stages\nScientiﬁc Method Solutions\nObservation - Replacing human evaluation and annotations 60,61\n- Simplifying observed data by providing qualitative descriptions100,115\n- Domain-speciﬁc LLMs can perform better on classiﬁcation and prediction tasks62–64\nHypotheses - Literature review: using an LLM ’s own trained knowledge142,143,201, or using the RAG method to access up-to-date information45,92,121.\n- Novelty: hallucinations of LLMs can sometimes beneﬁt novelty202; using the role-play method, LLMs can increase their novelty136; LLMs can also\npropose novel ideas iteratively121.\n- Observation-based Hypotheses: LLMs can propose hypotheses based on100,115,116,139,141.\nExperiment - Implement experiment: LLMs can use external tools 85,96, API calls66, or directly write code203 to implement experiments.\n- Experiment planning: chain-of-thought44, ReAct112.\n- Safety: Hardcoded pipeline for safety67; Human conﬁrmation50\nAutomation - LLM agent: LLM-based planner 66, multi-LLM agent109\n- Scaling: complex tasks116–118; knowledge accumulation109\n- Enhance: Iteratively optimising proposed hypotheses121, and experiments112.\n- Human-in-the-loop\nhttps://doi.org/10.1038/s44387-025-00019-5 Perspective\nnpj Artiﬁcial Intelligence|            (2025) 1:14 5\nAutomation is a signiﬁcant aspect of LLM-assisted research, serving as\na key contributor to accelerating scientiﬁc discovery. Automation involves\nrepetition and feedback loops113. LLMs can be seen as a function— prompt\nin, reply out— with human users as the driving force behind making LLMs\nproduce output. To automate such a system, the key is to replace the human\nuser. For instance, an LLM-powered chemical reaction system can perform\nSuzuki and Sonogashira reactions by incorporating an LLM-based planner,\nwhich replaces the human user. The planner reasons through the given task\nand determines the next steps, including searching the internet for infor-\nmation on both reactions, writing Python code to calculate experimental\nparameters, andﬁnally calling external tools to conduct the experiments. At\neach step, the results, i.e., the search outcomes and calculation results, are fed\nback to the LLM-based planner to automate the system\n66. Another approach\nis to replace the human user with multiple LLMs and allow them to com-\nmunicate with each other\n114. Since such automation is not fully hard-coded\nand the core of this automation is also an LLM, they can exhibit some\nemergent behaviour66,114, adapting unexpected situations, which is vital for\nexploring new knowledge. Speciﬁcally, automated LLMs can help in three\ndimensions of scientiﬁc discovery: scaling, enhancing, and validation.\nScaling. Automated LLM agents can scale previously challenging\nexperiments for large-scale studies. Examples include inferring under-\nlying functions from input-output pairs\n115. The LLMs perform multiple\nrounds of trial and error toﬁnd the correct function. This approach can\nextend to neuron interpretation of GPT-2 using GPT-4, which has bil-\nlions of parameters\n116. This method involves two layers of loops: the trial-\nand-error process and the application to all the billions of neurons117,118.\nBoth layers are time-consuming for human scientists, and LLMs make\nsuch studies feasible. Another example is when LLMs are used to infer the\nfunctionality of human brain voxels from fMRI activation data, their\nproposed functions areﬁrst validated by calculating the probability of\nobserving the activation data given a speciﬁc functional hypothesis.\nSubsequently, the hypotheses with the high probability are selected to aid\nin generating new hypotheses and improving overall performance\n100. Lab\nexperiments can also be parallelized with the help of LLMs, which further\naccelerate the experiment speed and increase the potential for scaling the\nscope of experiments\n110,119.\nEnhancing. The scientiﬁc methods, such as hypothesis generation,\nexperiments, and observations, can all be enhanced by automation.\nOne direct application is using LLMs as optimisers: by iteratively\nproviding historical solutions and scores, LLMs can propose new\nsolutions and ultimately achieve superior performance\n120.I nb o t ht h e\nhypothesis-validation loop and in experimental trials, failed cases\nconstitute valuable feedba ck. When evaluators and re ﬂection are\nincorporated into the workﬂow, LLMs can improve their decisions,\nshowing signiﬁcant performance improvements compared to simply\nusing LLMs\n111. Iteration can also enhance the hypothesis generation\nstage. By comparing hypotheses wit h existing literature on related\ntopics, LLMs can iteratively improvenovelty by using this literature as\na source of negative examples121. Another enhancement comes from\naccumulating knowledge, which is critical to research success. Many\nexploration tasks require accumulating knowledge and developing new\nstrategies based on this knowledge\n122. For example, Voyager109 uses\nGPT-4 to examine the space of the Minecraft game. This study consists\nof three main parts: an automatic curriculum to propose exploration\nobjectives, an iterative prompting mechanism to write code to control\nthe game, and a skill library to accumulate the knowledge and skills\ngained during the exploration, which is then reused in future\nexplorations. Equipped with all these components, this LLM-assisted\nexplorer can explore the game more efﬁciently. While game environ-\nments in silico are a non-trivial departure from real worlds in situ, they\nare not too dissimilar from the biochemical simulation engines\n123 that\nscientists rely on today. However, the current“physics” engines in-\ngame systems are still inconsistent with the physical sciences, and new\nsimulation software technologiesare needed to allow for any AI-based\nexploration of multi-p hysics environments 113.F r o mam a c r o s c o p i c\nviewpoint, scienti ﬁc discovery can also be considered a quality-\ndiversity search process124,125, and this Voyager study has shown how\nLLMs can assist diversity search in a new way by proposing objectives,\niteratively solving problems, and contributing to and utilising litera-\nture (skill library).\nValidation. Automated LLM agents are critical for validating hypothesis.\nBeyond scaling and enhancing performance, research often involves\nmultiple rounds of the hypothesis experiment loop to meet scientiﬁc\ndiscovery’s rigor and safety requirements. This loop is essential given the\nprobabilistic nature of LLMs\n126 and the hallucination problem of\nLLMs127,128. Experiments show that repeatedly verifying the results from\nLLMs’observations and proposed hypotheses increases the likelihood of\nobtaining reliable results129,130. A promising direction is leveraging formal\nsystems to validate results and hypotheses by translating generated\nhypotheses and answers into formal languages, such as LEAN or\nProver9\n131,132. For instance, in ref. 131, LLMs ﬁrst generate multiple\nanswers. These answers are then translated into the LEAN language and\nveriﬁed using the LEAN compiler to choose the correct responses. With\nthese ﬁltered answers, LLMs can aggregate toward a ﬁnal answer.\nAnother example involves using Python code to aid validation. While\ngeneral programming languages are often not considered formal systems,\nthey can still disprove certain hypotheses. In ref. 133, LLMs were\nprompted to solve the Abstraction and Reasoning Corpus (ARC) tasks\n134,\nwhich involve identifying underlying laws and making predictions based\non new initial states. LLMs initially propose hypotheses, which are then\ntranslated into Python code. This Python code is used to disprove\nincorrect hypotheses. Although these non-formal systems cannot fully\nvalidate hypotheses, they partially perform validation and improve pre-\ndictive accuracy. While humans could also conduct such translation and\nvalidation processes, the high speed of hypothesis generation by LLMs\nmakes automated approaches more suitable. A limitation, however, is the\nreliance on LLMs to translate hypotheses into formal languages, which\nmay introduce errors in the process. This suggests the need for caution\nwhen interpreting results, even if they have been tested using formal\nsystems.\nExpanding the literature review and the hypothesis horizon\nIn brief, advancing beyond current knowledge includes using LLMs to\nexplore unknown territories in knowledge space, encompassing human\ndiscoverable, human-machine discoverable, non-human-machine dis-\ncoverable, and the entirety of the knowledge space, as illustrated in Fig.2.\nNamely, to perform hypothesis generation and develop predictive models of\nmore complex systems. Hence, can LLMs do open-ended Exploration of the\nHypothesis Space? Can LLMs also explore complex environments in an\nopen-ended way? These are open-e nded challenges addressing the\n(unknown) limits of the capabilities of LLMs and Generative AI.\nProposing hypotheses is a crucial step in scientiﬁc discovery, perhaps\nthe most important since it often involves signiﬁcant creativity and inno-\nvation. Scientists propose hypothesesto explore unknowntopics or address\nresearch questions. This step often involves novel ideas, recombining\nexisting literature, and key insights. Experiment design and subsequent\nveriﬁcation are based on these hypotheses. Thus, hypothesis proposing is a\ncentral step that connects observation and experiments.\nEvidence indicates that LLMs can propose novel ideas, such as drug\ncombinations\n135, with designed prompting, thus underscoring the impor-\ntance of prompting, as discussed previously. An example is the use of LLMs\nfor drug discovery: In ref.135 LLMs are prompted to propose novel com-\nbinations of drugs for treating MCF7 breast cancer cells while incorporating\nadditional constraints such as avoiding harm to healthy cells and prioritizing\nFDA-approved and readily accessible drugs. The experiment results\ndemonstrate that LLMs can effectively propose hypothetical drug combi-\nnations. More advanced techniques can also improve novelty, such as asking\nhttps://doi.org/10.1038/s44387-025-00019-5 Perspective\nnpj Artiﬁcial Intelligence|            (2025) 1:14 6\nLLMs to role-play as scientists136 or iteratively provide feedback on existing\nsimilar ideas66. This is further exempliﬁed by the Virtual Lab project137,\nwhere AI agents, powered by LLMs, were used to design novel nanobody\nbinders against SARS-CoV-2 variants. LLMs effectively functioned as\nhypothesis generators, facilitating rapid and innovative scientiﬁcd i s c o v e r y\nthat translates to validated experimental results in real-world applications.\nAlthough some human evaluations show that LLM-generated ideas have\nlower novelty\n138, the fast speed at which LLMs propose ideas can still be\nvaluable. With proper instruction and background knowledge, LLMs can\nact as zero-shot hypothesis generators139. LLMs can also generate hypoth-\neses semantically or numerically based on observations about the under-\nlying mechanisms for language processing and mathematical tasks\n140,141.\nWith neuron activation heatmaps, GPT-4 can propose potential explana-\ntions for neuron behaviour116.\nBesides directly proposing hypotheses, a signiﬁcant part of creativity\ncombines existing knowledge, making literature research critical. With their\nvast stored compressed knowledge142,143, LLMs can be viewed as databases\nqueried using natural language123. This not only accelerates the search but\nalso breaks down barriers of domain terminology, making it easier to access\ninterdisciplinary knowledge. For accessing more up-to-date and domain-\nspeciﬁc information, LLMs can help scientists by using the RAG method\nand accessing internet information, see Fig.2. Generally, text embedding is\nused for semantically searching vector databases 45,92. For example,\nSTORM144 proposes an LLM-powered literature review agent that, for a\ngiven topic, actively searches literature on the internet from different per-\nspectives and automatically generates follow-up questions to improve depth\nand thoroughness. Another important example is Deep Research\n145,146,\nwhich integrates internet browsing and reasoning to deliver more in-depth\nand relevant literature review results. LLMs can propose novel hypotheses\nby retrieving related literature as inspiration,ﬁnding semantically similar\ncontent, connecting concepts, and utilising citations based on research\ntopics and motivation\n121. Alternatively, it may require additional ingredients\nor experiments to extrapolate and search outside current knowledge\ndomains.\nThis case also highlights the importance of the hypothesis-experiment-\nobservation loop, where each step is critical: hypotheses rely on observa-\ntions, experiments require hypotheses and planning, and observations\ndepend on experiments. Such a self-dependent loop is typical in scientiﬁc\ndiscovery and can be initiated eitherby starting with a tangible step in the\nhypothesis-experiment-observa t i o np r o c e s so rb ya l l o w i n gh u m a n\nintervention.\nHuman scientists in the loop\nW h i l ew es h o w c a s et h ec a p a b i l ities of LLMs in assisting scientiﬁcd i s c o v e r y ,\nhuman scientists remain indispensable.During the literature review stage,\nwith the help of LLM agents, humans can contribute by providing deeper\nperspectives or guiding the focus toward the needs of human scientists144.I n\nthe reasoning processes, by identifying uncertain reasoning and thoughts,\nhumans can correct LLMs. This signiﬁcantly improves the accuracy of the\nchain-of-thought method, making the LLMs more reliable147.H u m a ns c i -\nentists can be involved in further improving safety and reliability. For\nexample, ORGANA110, an LLM-powered chemical task robot, uses LLMs to\ninteract with humans via natural language and actively engages with users\nfor disambiguation and troubleshooting. Beyond this, humans can assist\nLLMs to enhance performance with a reduced workload. For example, by\ninvolving humans in the hypothesis-proposing stage to select generated\nhypotheses, LLMs can perform similarly to humans\n133. At the experiment\nstage, many lab experiments still require human implementation and cor-\nrection of invalid experimental plans\n67, and LLMs can request human help\non these experiments66.\nWhile the methods described above focus on LLMs as drivers of sci-\nentiﬁc enquiry, we must clarify that human-in-the-loop is more aptly cast as\nLLM-in-the-loop, emphasising“assistance” or augmentation as the practical\nvalue-added dimension of LLMs. The opportunities described in this paper\nshow potential to shift this mode of scientiﬁcp r a c t i c et ob em o r er e l i a n to n\nAI-driven approaches, but not without signiﬁcant advances in AI for Science\napproaches with respect to physics-infused ML and causal reasoning and in\nrigorous testing systems for LLMs interacting with the natural world.\nChallenges and opportunities\nWhile LLMs have shown signs of delivering promising results and of having\npositive impacts on scientiﬁc discovery, investigators have recognised their\nlimitations, such as hallucinations, limited reasoning capabilities, and lack of\ntransparency. Compared to everyday usage, when applied to scientiﬁc\ndomains, these limitations require careful consideration, as scientiﬁcp r o -\ncesses and discoveries require high standards of truthfulness, complex\nreasoning, and interpretability. The scientiﬁc community’s increasing\nrecognition and communication of these limitations of LLMs is essential to\nenabling solutions while also limiting expectations. Such rigour is a cor-\nnerstone of science and engineering, and a requirement if LLMs are to play a\npractical role.\nBeyond all this, LLMs also affect scientiﬁc research at the scientiﬁc\ncommunity level. While many papers and reviews involve LLMs’assistance,\nLLMs still face challenges in producing qualiﬁed reviews.\nThe scientiﬁc community must also decide how much it leaves to LLMs\nto drive science, even when associations with‘reasoning’, mostly currently\nundeserved, are made in exchange for the potential to explore hypothesis\nand solution regions that might otherwise remain unexplored by human\nexploration alone. See Table2 for a concise overview of these challenges and\ncorresponding mitigation strategies.\nHallucinations as putative sources of unintended novel\nhypotheses\nHallucinations produced by LLMs, also called confabulation or delu-\nsion, refer to artiﬁcially intelligent systems generating responses that\ncontain false or misleading infor mation presented as fact. This is\nanalogous to hallucination in human psychology, though, for LLMs, it\nm a n i f e s t sa su n j u s t iﬁed responses or beliefs rather than perceptual\nhallucinations\n148. Hallucinating LLMs can make unsupported claims,\nthus failing to meet a prior set of standards. While some“incorrect”\nLLM responses may reﬂect nuances in the training data not apparent\nto human reviewers 149, this argument has been challenged as not\nrobust to real-world use150.\nIn scientiﬁc discovery, hallucination becomes a critical hurdle when\napplying LLMs to literature review, data processing, or reasoning. Various\nmethods have been developed to mitigate hallucinations151.U s i n gR A G\nmethods, LLMs can reference accurate source contexts and up-to-date\ninformation, which can reduce hallucinations\n152.\nKnowledge graphs can also provide some relief to reduce\nhallucinations64. A self-RAG method can also reduce hallucinations, where\nthe LLMs generate and verify the reference contexts, and outputs are also\nveriﬁed by the LLMs themselves\n148,153 proposes an even simpler solution:\ncreate answers for the same query multiple times and vote for theﬁnal\nanswer. This method can signiﬁcantly improve the accuracy of outputs.\nRepetition from prompt variation and reiteration can also detect\nhallucinations–by ﬁnding contradictions\n154. By repeatedly generating the\nsame context, LLMs may sometimes generate contradictory content, which\ncan beﬁxed by the LLMs themselves iteratively.\nA n o t h e rm e t h o dt om i t i g a t eh a l l u c i n a t i o n si st h r o u g hs e l f - v e r iﬁcation.\nThis often involves decomposing the generated content into multiple fact\ncheckpoints. For example, the Chain-of-Veriﬁcation method uses separate\nLLM agents to verify them individually and update the original answer155.\nSuch a veriﬁcation process can also adopt RAG methods for greater\nreliability156.\nAn important origin of hallucinations is the auto-regressive generation\nprocess of mainstream LLMs, where errors may accumulate during\ngeneration146. Hence, as discussed above, a general way to mitigate hallu-\ncinations is to decompose the end-to-end generation process using chain-\nof-thoughts, the RAG method, multiple agents, feedback, and\niteration loops.\nhttps://doi.org/10.1038/s44387-025-00019-5 Perspective\nnpj Artiﬁcial Intelligence|            (2025) 1:14 7\nWhile signiﬁcant research efforts target the challenge of how to control\nor limit hallucinations, we may ask to what extent hallucinations are a bug or\na feature. For example, could hallucination provide a gateway to creativity in\nthat it could represent a steady stream of novel conjectures? An LLM could\nthen be used toﬁlter such a string of hallucinated hypotheses and rank them\nto recommend which ones to test. This remains unexplored territory, as far\nas we can tell.\nAnother approach to treating hallucinations is to move beyond a\nbinary perspective of trust versus distrust. Instead, similar to statistical\nconﬁdence, we may quantify the extent to which research conducted by\nLLM agents can be trusted. Current studies primarily focus on conﬁdence\nmeasurements at the foundation model level\n157–159 and the output level160.\nSome research has also proposed multidimensional assessments of LLM\ntrustworthiness161. Additionally, efforts have been made to enable LLMs to\nexpress their conﬁdence levels157,162. However, conﬁdence measurements at\nthe LLM agent level are primarily limited to success rates rather than\ntrustworthiness, particularly when dealing with open-ended tasks. More-\nover, existing measurements predominantly rely on post-hoc quantiﬁca-\ntions, which restrict their applicability in scientiﬁc research\n163. Therefore,\npredictive trustworthiness quantiﬁcation frameworks for LLM agents that\ncollectively consider foundation models, tasks, tools usage, workﬂow, and\nexternal feedback are needed.\nThe value of reasoning and interpretation in AI-led science\nWhile LLMs have been suggested to perform reasoning on some tasks, they\nexhibit severe defects in logical reasoning and serious limitations with\nrespect to common sense reasoning. Notably, while LLMs can correctly\nanswer“Xi st h ec a p i t a lo fY”, they struggle to accurately determine that“Y’s\ncapital is X.” This is known as the“reversal curse”\n164. Another example is\nshufﬂing the order of conditions in a query, which may reduce the perfor-\nmance of LLMs. When the conditions are provided logically, the LLMs can\noften perform correct inferences but may fail when the conditions do not\nfollow a speciﬁco r d e r\n165. LLMs can also fail at simple puzzles, such as\ndetermining the odd-numbered dates on a list of famous people ’s\nbirthdays166,o ri ns i m p l eq u e s t i o n sl i k e“Alice has N brothers, and she also\nhas M sisters. How many sisters does Alice’sb r o t h e rh a v e ?” Many LLMs,\nwhile achieving high performance on other benchmarks, have shown a\nlower success rate on this task167. When faced with unseen tasks, which are\ncommon for human scientists in research, LLMs exhibit a signiﬁcant drop in\naccuracy even on simple questions,such as performing 9-base number\naddition or writing Python code with indexing starting at 1 instead of 0. This\nsuggests that LLMs may rely more on pattern matching than on reasoning,\ncontrary to what many assume\n168–170. Consequently, caution is advised when\napplying LLMs to novel reasoning tasks, and incorporating human over-\nsight into the process is recommended170,171. Another crucial aspect of rea-\nsoning is planning capability. As discussed earlier, planning is essential for\nimplementing experiments. While techniques such as ReAct and Reﬂection\ndemonstrate some planning capabilities in LLMs, their effectiveness\nremains questionable. Current state-of-the-art LLMs often fail at simple\nplanning tasks172, such as Blocksword, and are unable to verify the cor-\nrectness of their plans172. In contrast, humans generally excel at creating\neffective plans for such tasks. However, studies also indicate that integrating\nLLMs with traditional methods or solvers can enhance planning success\nrates, reduce research time\n172,a n dp r o v i d em o r eﬂexible ways to interact\nwhen developing plans173.\nSome reasoning improvement methods, such as self-correction, can\nalso fail. When LLMs receive feedback without new information or correct\nlabels, self-correction can often lead to compromised performance\n174.S u c h\nself-correction prompts may even bias the LLMs, causing them to turn\ncorrect answers into incorrect ones. To mitigate this problem, directly\nincluding all requirements and criteria in the query prompt is suggested\ninstead of providing them as feedback. This result also indicates that to make\ncorrections, effective feedback needs to include external information, such\nas experimental results and trustworthy sources\n174.\nWhile some progress has been made, more advanced methods are\nneeded to address these reasoning-related challenges. One crucial aspect to\nconsider is consistency– when different LLM agents generate different\nresponses to the same query, the result is considered inconsistent. Notably,\nthe self-consistent method\n175 uses LLMs to answer the same question\nmultiple times and chooses the most frequent answer. The answers also help\npeople estimate uncertainty175, given that LLMs often behave too\nconﬁdently176. Similar methods have also been proposed in ref.153.O t h e r\nmethods use different LLM agents tosuggest different ideas and then\nconduct a multi-round debate to arrive at aﬁnal answer177. As illustrated in\nFig. 2, these LLM-agent methods can beneﬁta l ls t e p si nt h eh y p o t h e s i s -\nexperiment-observation loop.\nA straightforward but challenging route to scientiﬁc discovery is to\nﬁne-tune or directly train a model. In ref.178, the authors propose an\ninnovative solution to the Reversal Curse through“reverse training,” which\ninvolves training models with both the original and reversed versions of\nfactual statements. Considering the requirements for rigor and prudence in\nscientiﬁc research, attention must be given to the limitations of reasoning\ntasks. This is particularly important given that LLMs often exhibit reduced\nperformance in reasoning correctness when encountering novel tasks— a\nfrequent occurrence in scientiﬁc research, where the focus is on exploring\nunknown knowledge.\nTable 2 | Challenges and opportunities in applying LLM in scientiﬁc discovery\nChallenges Solution & Opportunities\nHallucinations LLMs may generate false or misleading information 148 RAG: external source204, knowledge graph205, self-RAG206\nRepeat: sampling and majority vote153,154\nVeriﬁcation: verify the generated content and reﬁne the results155,156\nReasoning - Reversal curse 164\n- Order of conditions165\n- Alice in Wonderland167\n- Limited self-correction174\n- Internal reasoning166\nConsistency methods:\nself-consistency175, multi-agent vote153, multi-agent debate177, Tree-\nof-Thoughts207\n- Learning: Buffer-of-thoughts178\n- In-context learning\nTransparency &\nInterpretability\n- Traditional methods like gradient-based methods are a\nchallenge to apply to LLM due to the scale\n- Self-explanation is also not trustworthy181,208\nProbing methods: Logit Lens method179, Patchscope209\nInterpret data & other systems: explain black-box functions100,115,\nunderstand complex neural networks116.\nProviding symbolic understanding with the help of in-context\nexamples and knowledge\n80.\nScientiﬁc Community - LLMs are not yet good reviewers 189,191,192\n- LLMs are widely used in paper and review writing189–192\nLLMs can mitigate the disadvantage of non-native English\nspeakers195,210–212\nLLMs can help inter-disciplinary research195\nThere is much room for AI4Science as aﬁeld toﬁll in gaps under the listed themes, especially as LLM research matures over the next several years; consistent with all innovations in scientiﬁc practices and\nengineering standards, the demonstration-through-validation of said innovations requires time and resources several fold greater than are available in“safer” domains (where LLMs are currently\nembedded).\nhttps://doi.org/10.1038/s44387-025-00019-5 Perspective\nnpj Artiﬁcial Intelligence|            (2025) 1:14 8\nThe challenge to understand LLMs, and the opportunity to\nunderstand by using GenAI and LLMs\nA comprehensive scientiﬁc interpretation stimulates discussion and fur-\nther discoveries among scientists. This is especially important for LLM-\nassisted scientiﬁc discovery— given that current LLMs are mostly black\nboxes, it becomes difﬁcult to trust LLM outputs. To understand LLMs’\nbehaviour, LLMs’language capabilities can be leveraged. There are two\ntypes of methods. First, LLMs’ hidden states can be used in what are\nknown as probing methods. The Logit Lens method179 applies the\nunembedding layer to hidden states or transformed hidden states,\nenabling semantic understanding of LLMs’hidden states. Representation\nengineering methods\n180 can further detect and control emotion, dis-\nhonesty, harmfulness, etc., at the token level, allowing people to read their\nhidden activities. Besides these, dictionary learning methods can also be\nused to understand LLMs’hidden states and activations, leading to aﬁne-\ngrained understanding of LLMs.\nThe second method is to ask LLMs to explain their reasoning. For\ninstance, the CoT method or reasoning models\n56 can explain the thought\nprocess before generating results or ask LLMs to explain their reasoning\nafter generating results. However, the self-explanation of LLMs is also\nquestionable. Their explanations are often inconsistent with their beha-\nviours, and we cannot use their explanat i o n st op r e d i c tt h e i rb e h a v i o u r si n\ncounterfactual scenarios\n181. This suggests that LLMs’self-explanation may\nnot accurate and not generalizable. Beyond this, LLMs may also hallucinate\nin their self-explanations, including content that is not factually grounded181,\nmaking their self-explanations even less trustworthy.\nDespite the many difﬁculties in understanding LLMs, they present a\nsigniﬁcant opportunity for understanding other systems— they can be used\nto understand data, interpret other systems, and then prompt humans. By\ndirectly showing input and output pairs to LLMs, including language input-\noutput pairs\n115, mathematical function input-output pairs, or experimental\ndata, LLMs can be made to explain these black-box systems, including, fMRI\ndata, complex systems like GPT-2, or, potentially, papers written by human\nscientists that are becoming increasingly difﬁcult to reproduce because of\nvarious forces at play (e.g., an increasing number of publications and\ndubious incentives). This indicates the potential of applying LLMs to explain\ndata and other systems, even though understanding them may still be\nchallenging\n116.\nThis capacity to interpret systemsis not limited to human language.\nFoundation models in speciﬁcs c i e n t iﬁc domains offer domain-grounded\ninterpretability, distinct from the pitfalls of LLM self-explanation. Pre-\ntrained on vast, specialized data, these models learn the“language” inherent\nin that data. For instance, scGPT in single-cell biology demonstrates this: its\nlearned representations align with established biological knowledge, and it\nutilizes attention-map visualizations to enhance transparency, elucidating\ngene interactions that are subsequently validated by domain-speciﬁc\nevidence\n76. Although the faithfulness of attention weight interpretability has\nbeen questioned in various cases182,183, it remains widely used in many AI-\nfor-science applications184. Models, including LLMs that use transformer\narchitectures, often inherently beneﬁt from such emergent interpretability\nfor both feature attribution and interaction highlighting.\nThese approaches support viewing complex domain representations,\nsuch as Gene Regulatory Networks (GRNs), as a form of decipherable\n“domain language.” Understanding these intrinsic languages through\nmodels whose interpretations are rooted in veriﬁable domain semantics,\nrather than potentially unreliable self-explanations, provides a robust\nmethod for advancing scientiﬁc discovery in specializedﬁelds.\nThe impact on scientiﬁc practice and the community\nAn open question is how much human science and scientists will be willing\nto let AI, through technology like LLMs, drive scientiﬁcd i s c o v e r y .W o u l d\nscientists be satisﬁed letting LLMs set the agenda and conduct experiments\nw i t hl i t t l et on os u p e r v i s i o n ,o rd ow eexpect to supervise AI always, driven\nby the fear of multiple levels of misalignment? This is a misalignment\nbetween human scientiﬁc interests and the actual practice of science,\npossibly forcing AI and LLMs to produce data as humans do, with its\nadvantages or disadvantages, including constraining the search space and,\ntherefore, the solution space.\nBeyond the individual deployment of the scientiﬁc method, scien-\ntiﬁc discovery also happens at the community level, where scientists\npublish their work, share ideas, and collaborate. We can consider the\nscientists and even entire scientiﬁc community as an agent that learns\nfrom experiments and research publications in a manner similar to\nreinforcement learning processes\n185. However, learning from failed\nresearch (or negative results) is just as important as learning from suc-\ncessful studies\n186,187, yet it is currently undervalued188. This may be\nbecause failed research is far more common than successful research.\nHowever, with the massive text-processing capabilities of LLMs, we now\nhave the opportunity to systematically share and learn from failures.\nTherefore, we advocate for journals and conference to encourage the\npublication of failed studies and negative results.\nThis learning process also depend on human values emphasising\ncommunication, mutual understanding, and peer review. Evidence shows a\nsigniﬁcant adoption of LLM-assisted paper writing and peer review in recent\nyears. Estimates indicate that 1% to 10% of papers are written with LLM\nassistance\n189.In computer science, up to 17.5% of research papers are esti-\nmated to be assisted by LLMs, aﬁgure that mainly reﬂects the output of\nresearchers with strict time constraints190. Beyond papers, estimates also\nshow that around 7–15% of reviews are written with LLM assistance191,192.\nWhile LLMs can provide feedback that shows a high degree of overlap\nwith human reviewers, they are not proﬁcient at assessing the quality and\nnovelty of research193. This limitation is especially signiﬁcant for high-\nquality research194. Beyond this, LLM-assisted reviews tend to assign higher\nscores to papers than human reviewers evaluating the same papers191.U p o n\ncloser examination, LLMs also exhibit a homogenisation problem– they\ntend to provide similar critiques for different papers193,195.\nDespite LLMs displaying limitations at tasks such as peer reviewing\nand raising ethical concerns in directly generating academic content, they\nmay still beneﬁt scientiﬁc communication. For example, most researchers\ntoday are non-native English speakers, so they can beneﬁt from LLMs’\nlanguage capabilities thatﬁt their diverse demands for proofreading\n195,\nhelping alleviate the current bias towards Western Science. On another\napplication, LLMs’ code explanation capabilities may help scientists\nunderstand poorly documented code, making existing knowledge and work\nmore accessible to a broader range of scientists\n195.W i t hs i g n iﬁcant growth in\nusing LLMs for writing papers, such impacts will become increasingly\nimportant\n190.\nConclusions\nIn this perspective paper, we reviewed the rapid development and inte-\ngration of large language models (LLMs) in scientiﬁc research, highlighting\nthe profound implications of these models for the scientiﬁc process. LLMs\nhave evolved from tools of convenience— performing tasks like summar-\nising literature, generating code, and analysing datasets— to emerging as\npivotal aids in hypothesis generation,experimental design, and even process\nautomation. As AI advances, foundation models have emerged, repre-\nsenting adaptable, scalable models with the potential to apply across diverse\nscientiﬁc domains, reinforcing the collaborative synergy between humans\nand machines.\nLLMs have reshaped how researchers approach the vast amounts of\nscientiﬁc information available today. By efﬁciently summarising literature\nand detecting knowledge gaps, scientists can speed up literature review and\nidea generation. Furthermore, LLMs facilitate interdisciplinary research,\nbridging the knowledge divide bysummarising complex ideas acrossﬁelds,\nthereby fostering collaborationspreviously limited by domain-speciﬁc\nlanguage and methods. Beyond these beneﬁts, the massive text-processing\ncapabilities of LLMs create new opportunities for utilizing failed research\nfailed research, which has received limited attention. Therefore, we\nencourage the scientiﬁc community to promote the publication of negative\nresults and failed research.\nhttps://doi.org/10.1038/s44387-025-00019-5 Perspective\nnpj Artiﬁcial Intelligence|            (2025) 1:14 9\nThe utility of LLMs in designing experiments is another notable\nadvancement. Models like CRISPR-GPT in biology exemplify this by\nautomating gene-editing experiment designs, signiﬁcantly accelerating\ngenomics research. Moreover, LLM-powered autonomous systems like\nBioDiscoveryAgent indicate a shift towards AI-driven experimental pro-\ncesses that can augment researchers’ efﬁciency and, more importantly,\nenable scientiﬁc exploration previously constrained by resource limitations.\nSo, Large Language Models (LLMs)present two contrasting roles in\nscientiﬁc discovery: accuracy in experimental phases and creativity in\nhypothesis generation. On the one hand, scientiﬁc research requires LLMs\nto be reliable, accurate, and capable oflogical reasoning, particularly for\nexperimental validation. On the other,there is value in promoting creative\n“hallucinations” or speculative ideas at the hypothesis stage, which mirrors\nhuman intuition and expands research boundaries\n196.\nBesides the general foundation models like GPT-4, Claude and\nDeepseek, domain speciﬁc foundation models have shown special potential\nfor applying LLMs in scientiﬁc research. Notable examples such as Evo and\nChemBERT showcase the success of domain-speciﬁc adaptations in geno-\nmics and chemistry, where they excel inpredicting gene interactions and\nmolecular properties. These foundational models also highlight a promising\napproach by treating genomic, chemical, and other scientiﬁcd a t aa sn e w\nmodalities for LLMs, similar to howimages, videos, and audio are con-\nsidered as modalities. Integrating these modalities often follows two main\nstrategies: end-to-end training,where models like ChemBERT develop\ndeep, intrinsic capabilities on specialized data, potentially exceeding human\nperformance on speciﬁc tasks; and compositional approaches, which offer\ngreaterﬂexibility by leveraging intermediate modalities common to human\nscientists (like vision and text) or specialized tools. While end-to-end\nmethods provide depth, compositionalﬂexibility is crucial for adapting to\ndiverse and rapidly changing scientiﬁc demands. Consequently, combining\nand scaling these scientiﬁc modalities, particularly when models can be\nseamlessly inserted into various scientiﬁcw o r kﬂows, has the potential to\nprofoundly transform scientiﬁcr e s e a r c h .\nDespite the promise, current limitations pose signiﬁcant hurdles to\nfully realising LLMs as independent scientiﬁc agents. Among these are\nreasoning limitations, interpretability issues, and challenges like“halluci-\nnations”— where LLMs generate plausible-sounding but inaccurate infor-\nmation. While helpful in generating hypotheses, these models require\ncareful oversight to prevent misleading or unveriﬁed information from\ninﬂuencing scientiﬁc processes.\nThe challenges of reasoning and hallucinations pose serious concerns\nregarding the use of LLMs in scientiﬁc discovery. Instead of treating LLMs as\nsimply trustworthy or untrustworthy in a binary manner, we suggest an\nanalogy to statistical conﬁdence, using a continuous value— it may term as\nalgorithmic conﬁdence— to quantify the trustworthiness of an LLM agent\nsystem in scientiﬁc research. We further suggest that all LLM-assisted\nresearch should either be veriﬁed by humans or undergo algorithmic con-\nﬁdence testing.\nThe interpretability of LLMs also remains a complex issue. Their black-\nbox nature can obstruct transparency, limiting trust in outputs that affect\nhigh-stakes scientiﬁc decision-making. Consequently, researchers continue\nto explore methods such as probing, logitlens techniques, and visualisation\nof neuron activations to demystify the decision-making processes within\nthese models. Increased interpretability will be critical as we strive for\nethically responsible and scientiﬁcally sound applications. On the other\nhand, it is essential to recognize thatLLMs are showcasing their potential to\nexplain other black-box systems through their language and reasoning\ncapabilities.\nIntegrating LLMs into scientiﬁcw o r kﬂows brings ethical considera-\ntions, particularly regarding transparency and fairness. For instance, LLMs\nhold the potential for democratising access to scientiﬁc information, aiding\nresearchers from non-English speaking backgrounds in publication and\ncollaborative research. However, theyalso risk perpetuating biases present\nin training data, thereby inﬂuencing scientiﬁc outputs and potentially\nreinforcing existing disparities in research. Another concern involves the\nover-reliance on AI in scientiﬁc processes. As we incorporate LLMs deeper\ninto workﬂows, human oversight becomes essential to maintaining scien-\ntiﬁc rigor and addressing potential misalignments between AI-generated\noutputs and human-deﬁned research goals. The question of how much\nautonomy AI should have in guiding scientiﬁc inquiries raises ongoing\ndebate about accountability and the evolving role of human oversight.\nTo harness LLMs as creativity engines, moving beyond task-oriented\napplications to generate new scientiﬁc hypotheses and theories is para-\nmount. For LLMs to contribute meaningfully to fundamental scientiﬁc\ndiscoveries, they must be equipped torecognize patterns and autonomously\ngenerate novel, insightful questions— a hallmark of scientiﬁc creativity. This\nwould require advancements in prompt engineering, automated experi-\nmentation, iterative reasoning, and building an AI that evolves its approach\nbased on experimental feedback. However, a signiﬁcant gap in general\nreasoning capabilities separates current models from domain-speciﬁc\nsuperhuman systems like AlphaGo/AlphaZero\n197,198. AlphaGo leveraged a\ncritical symmetry where an“answer” (a move) inherently generates a new\n“question” (the next board state challenge)— a dynamic largely absent in\ntoday’s reasoning models, yet key for mastering novel tasks. For scientiﬁc\ndiscovery, developing this symmetry is crucial, as the ability to ask questions\nis as important as answering them; though some preliminary work has\nexplored this\n199, it remains an unsolved and highly challenging problem.\nThe evolution of LLMs and foundation models signals a transformative\nera for science. While current applications largely support scientists in\nmanaging data and expediting workﬂo w s ,t h ef u t u r em a ys e et h e s em o d e l sa s\nintegral components of the scientiﬁc process. By addressing challenges in\naccuracy, interpretability, and ethical concerns, we can enhance their relia-\nbility and pave the way for responsible AI in scientiﬁcc o n t e x t s .\nLooking ahead, the collaboration between AI and human scientists\nwill likely deﬁne the next generation of discovery. As we reﬁne foundation\nmodels to become more adaptable and creative, they may transition from\nmerely assisting to potentially leading explorations into uncharted sci-\nentiﬁc domains. The challenge lies in responsibly developing these models\nto ensure they complement and elevate human expertise without com-\npromising scientiﬁc integrity. Ultimately, LLMs and foundation models\nmay come to represent a synthesis of human and artiﬁcial intelligence,\neach amplifying the strengths of the other. With continued research and\nethical vigilance, LLMs have the potential to accelerate and deepen sci-\nentiﬁc discovery, heralding a new era where AI not only supports but\ninspires new frontiers in science\n200. This includes embracing and learning\nfrom scientiﬁc failures and leveraging them to drive comprehensive\nexploration. As LLMs evolve, they may reshape scientiﬁc methodologies,\nimpacting how science values discovery and reproducibility and may\nultimately redeﬁne the purpose of scientiﬁc inquiry. However, the sci-\nentiﬁc community must also decide how much it leaves to AI to drive\nscience, even when associations with ‘reasoning’, mostly currently\nundeserved, are made in exchange for the potential to explore hypothesis\nand solution regions that might otherwise remain unexplored by human\nexploration alone.\nData availability\nNo datasets were generated or analysed during the current study.\nReceived: 10 April 2025; Accepted: 23 June 2025;\nReferences\n1. Wang, H. et al. Scientiﬁc discovery in the age of artiﬁcial intelligence.\nNature 620,4 7–60 (2023).\n2. Alkhateeb, A. & Aeon. Can Scientiﬁc Discovery Be Automated?The\nAtlanctic (2017).\n3. Jain, M. et al. GFlowNets for AI-driven scientiﬁc discovery.Digit.\nDiscov. 2, 557–577 (2023).\n4. Kitano, H. Nobel turing challenge: creating the engine for scientiﬁc\ndiscovery. NPJ Syst. Biol. Appl.7, 29 (2021).\nhttps://doi.org/10.1038/s44387-025-00019-5 Perspective\nnpj Artiﬁcial Intelligence|            (2025) 1:14 10\n5. Cornelio, C. et al. Combining data and theory for derivable scientiﬁc\ndiscovery with AI-Descartes.Nat. Commun.14,1–10 (2023).2023 14:1.\n6. Kim, S. et al. Integration of neural network-based symbolic\nregression in deep learning for scientiﬁc discovery.IEEE Trans.\nNeural Netw. Learn Syst.32, 4166–4177 (2021).\n7. Gil, Y., Greaves, M., Hendler, J. & Hirsh, H. Amplify scientiﬁc\ndiscovery with artiﬁcial intelligence: Many human activities are a\nbottleneck in progress.Science (1979)346, 171–172 (2014).\n8. Kitano, H. Arti ﬁcial Intelligence to Win the Nobel Prize and Beyond:\nCreating the Engine for Scientiﬁc Discovery.AI Mag.37,3 9–49\n(2016).\n9 . B e r e n s ,P . ,C r a n m e r ,K . ,L a w r e n c e ,N .D . ,v o nL u x b u r g ,U .&\nMontgomery, J. AI for science: an emerging agenda.arXiv preprint\narXiv:2303.04217;https://doi.org/10.48550/arXiv.2303.04217(2023).\n10. Li, Z., Ji, J. & Zhang, Y. From Kepler to Newton: explainable AI for\nscience. arXiv preprintarXiv:2111.12210; https://doi.org/10.48550/\narXiv.2111.12210 (2021).\n11. Baker, N. et al. Workshop report on basic research needs for\nscientiﬁc machine learning: core technologies for artiﬁcial\nintelligence. https://doi.org/10.2172/1478744 (2019).\n12. Manta, C. D., Hu, E. & Bengio, Y. GFlowNets for causal discovery: an\noverview. OpenReview (ICML, 2023).\n13. Vinuesa, R., Brunton, S. L. & McKeon, B. J. The transformative\npotential of machine learning for experiments inﬂuid mechanics.\nNat. Rev. Phys.5, 536–545 (2023).\n14. del Rosario, Z. & del Rosario, M. Synthesizing domain science with\nmachine learning.Nat. Comput. Sci.2, 779–780 (2022).\n15. Krenn, M., Landgraf, J., Foesel, T. & Marquardt, F. Artiﬁcial\nintelligence and machine learning for quantum technologies.Phys.\nRev. A107, 010101 (2022).\n16. van der Schaar, M. et al. How artiﬁcial intelligence and machine\nlearning can help healthcare systems respond to COVID-19.Mach.\nLearn 110,1 –14 (2021).\n17. Zhang, T. et al. AI for global climate cooperation: modeling global\nclimate negotiations, agreements, and long-term cooperation in\nRICE-N. arXiv preprintarXiv:2208.07004; https://doi.org/10.48550/\narXiv.2208.07004 (2022).\n18. Pion-Tonachini, L. et al. Learning from learning machines: a\nnew generation of AI technology to meet the needs of science.\n(2021).\n19. Keith, J. A. et al. Combining machine learning and computational\nchemistry for predictive insights into chemical systems.Chem. Rev.\n121, 9816–9872, https://doi.org/10.1021/acs.chemrev.1c00107\n(2021).\n20. Birhane, A., Kasirzadeh, A., Leslie, D. & Wachter, S. Science in the\nage of large language models.Nat. Rev. Phys.5, 277–280 (2023).\n21. Georgescu, I. How machines could teach physicists new scientiﬁc\nconcepts. Nat. Rev. Phys.4, 736–738 (2022).\n22. Noé, F., Tkatchenko, A., Müller, K.-R. & Clementi, C. Machine\nlearning for molecular simulation.Annu. Rev. Phys. Chem.https://\ndoi.org/10.1146/annurev-physchem-042018 (2020).\n23. Moor, M. et al. Foundation models for generalist medical artiﬁcial\nintelligence. Nature 616, 259–265 (2023).\n24. Rajpurkar, P., Chen, E., Banerjee, O. & Topol, E. J. AI in health and\nmedicine. Nat. Med.28,3 1–38 (2022).\n25. Acosta, J. N., Falcone, G. J., Rajpurkar, P. & Topol, E. J. Multimodal\nbiomedical AI.Nat. Med.https://doi.org/10.1038/s41591-022-\n01981-2 (2022).\n26. Topol, E. J. Welcoming new guidelines for AI clinical research.Nat.\nMed. 26, 1318–1320 (2020).\n27. Topol, E. Deep Medicine: How Artiﬁcial Intelligence Can Make\nHealthcare Human Again.(Hachette UK, 2019).\n28. Krishnan, R., Rajpurkar, P. & Topol, E. J. Self-supervised learning in\nmedicine and healthcare.Nat. Biomed. Eng.https://doi.org/10.\n1038/s41551-022-00914-1 (2022).\n29. Stoyanovich, J., Bavel, J. J., Van & West, T. V. The imperative of\ninterpretable machines.Nat. Mach. Intell.2, 197–199 (2020).\n30. Meskó, B. & Topol, E. J. The imperative for regulatory oversight of\nlarge language models (or generative AI) in healthcare.NPJ Digit.\nMed. 6, 120 (2023).\n31. Willcox, K. E., Ghattas, O. & Heimbach, P. The imperative of physics-\nbased modeling and inverse theory in computational science.Nat.\nComput. Sci.1, 166–168 (2021).\n32. Webster, P. Six ways large language models are changing\nhealthcare. Nat. Med.29, 2969–2971 (2023).\n33. Eriksen, A. V., Möller, S. & Ryg, J. Use of GPT-4 to diagnose complex\nclinical cases.NEJM AI1 (2023).\n34. Ishmam, M. F., Shovon, M. S. H. & Dey, N. From image to language: a\ncritical analysis of visual question answering (VQA) approaches,\nchallenges, and opportunities.Inf. Fusion106, 102270 (2024).\n35. Li, C. et al. Multimodal foundation models: from specialists to\ngeneral-purpose assistants.Found. Trends Comput. Graph. Vis.16,\n1–214 (2024).\n36. Omiye, J. A., Gui, H., Rezaei, S. J., Zou, J. & Daneshjou, R. Large\nlanguage models in medicine: the potentials and pitfalls: a narrative\nreview. Ann. Intern. Med.177, 210–220 (2024).\n37. Raghu, M. & Schmidt, E. A survey of deep learning for scientiﬁc\ndiscovery. arXiv preprint arXiv:2003.11755(2020).\n38. Song, S. et al. DeepSpeed4Science initiative: enabling large-scale\nscientiﬁc discovery through sophisticated AI system technologies.\nIn NeurIPS 2023 AI for Science Workshop(2023).\n39. McCarthy, J., M. M., R. N., & S. C. E. Dartmouth summer research\nproject on artiﬁcial intelligence. InDartmouth Summer Research\nProject on Artiﬁcial Intelligence(1956).\n40. Ramos, M. C., Collison, C. J. & White, A. D. A review of large\nlanguage models and autonomous agents in chemistry.Chem. Sci.\n16, 2514–2572 (2025).\n41. Zenil, H. & King, R. Artiﬁcial intelligence in scientiﬁc discovery:\nChallenges and opportunities. InScience: Challenges,\nOpportunities and the Future of Research(OECD Publishing, 2023).\n42. Zenil, H. & King, R. A framework for evaluating the AI-driven\nautomation of science. InScience: Challenges, Opportunities and\nthe Future of Research(OECD Publishing, 2023).\n43. Zenil, H. & King, R. The Far Future of AI in Scientiﬁc Discovery. InAI\nFor Science(eds Choudhary F. & Hey, T.) (World Scientiﬁc, 2023).\n44. Wei, J. et al. Chain-of-thought prompting elicits reasoning in large\nlanguage models.Adv. Neural Inf. Process Syst.35 (2022).\n45. Lewis, P. et al. Retrieval-augmented generation for knowledge-\nintensive nlp tasks.Adv. Neural Inf. Process Syst.33, 9459–\n9474\n(2020).\n46. White, J. et al. A prompt pattern catalog to enhance prompt\nengineering with ChatGPT.arXiv preprintarXiv:2302.11382; https://\ndoi.org/10.48550/arXiv.2302.11382 (2023).\n47. Schulhoff, S. et al. The prompt report: a systematic survey of prompt\nengineering techniques.arXiv preprintarXiv:2406.06608; https://\ndoi.org/10.48550/arXiv.2406.06608 (2025).\n48. Fulford, I. & Ng, A. ChatGPT prompt engineering for developers.\nDeepLearning.AI https://www.deeplearning.ai/short-courses/\nchatgpt-prompt-engineering-for-developers/ (2023).\n49. Cheng, Y. et al. Exploring large language model based intelligent\nagents: deﬁnitions, methods, and prospects.arXiv preprint\narXiv:2401.03428;https://doi.org/10.48550/arXiv.2401.03428(2024).\n50. Yang, H., Yue, S. & He, Y. Auto-GPT for online decision making:\nbenchmarks and additional opinions.arXiv preprint\narXiv:2306.02224; https://doi.org/10.48550/arXiv.2306.02224\n(2023).\n51. Nakajima, Y. yoheinakajima/babyagi. Preprint athttps://github.com/\nyoheinakajima/babyagi (2024).\n52. Chase, H. LangChain. https://github.com/langchain-ai/langchain\n(2022).\nhttps://doi.org/10.1038/s44387-025-00019-5 Perspective\nnpj Artiﬁcial Intelligence|            (2025) 1:14 11\n53. Liu, J. LlamaIndex. URL: ‘https://github.com/jerryjliu/llama_index’\n(2022).\n54. Khattab, O. et al. DSPy: Compiling Declarative Language Model\nCalls into Self-Improving Pipelines.12th International Conference on\nLearning Representations, ICLR 2024(2023).\n55. Yuksekgonul, M. et al. TextGrad: automatic‘differentiation’via text.\n(2024).\n56. DeepSeek-AI et al. DeepSeek-R1: incentivizing reasoning capability\nin LLMs via reinforcement learning. (2025).\n57. Shao, Z. et al. DeepSeekMath: pushing the limits of mathematical\nreasoning in open language models. (2024).\n58. Li, Z.-Z. et al. From system 1 to system 2: a survey of reasoning large\nlanguage models. (2025).\n59. Cau ﬁeld, J. H. et al. Structured prompt interrogation and recursive\nextraction of semantics (SPIRES): a method for populating\nknowledge bases using zero-shot learning.Bioinformatics 40,\nbtae104 (2024).\n60. Gilardi, F., Alizadeh, M. & Kubli, M. ChatGPT outperforms crowd\nworkers for text-annotation tasks.Proc. Natl. Acad. Sci. USA120,\ne2305016120 (2023).\n61. Liu, Y. et al. G-Eval: NLG Evaluation using Gpt-4 with Better Human\nAlignment. EMNLP 2023— 2023 Conference on Empirical Methods\nin Natural Language Processing, Proceedings2511–2522 https://\ndoi.org/10.18653/V1/2023.EMNLP-MAIN.153 (2023)\n62. Gu, Y. et al. Domain-speciﬁc language model pretraining for\nbiomedical natural language processing.ACM Trans. Comput.\nHealthc. (HEALTH)3,1 –23 (2021).\n63. Irwin, R., Dimitriadis, S., He, J. & Bjerrum, E. J. Chemformer: a pre-\ntrained transformer for computational chemistry.Mach. Learn Sci.\nTechnol. 3, 015022 (2022).\n64. Luo, R. et al. BioGPT: generative pre-trained transformer for\nbiomedical text generation and mining.Brief. Bioinform.23,\nbbac409 (2022).\n65. Gottweis, J. et al. Towards an AI co-scientist.https://doi.org/10.\n48550/arXiv.2502.18864 (2025).\n66. Boiko, D. A., MacKnight, R., Kline, B. & Gomes, G. Autonomous\nchemical research with large language models.Nature 624, 570–578\n(2023).\n67. M. Bran, A. et al. Augmenting large language models with chemistry\ntools. Nat. Mach. Intell.6,1 –11 (2024).\n68. Qu, Y. et al. CRISPR-GPT: An LLM Agent for Automated Design of\nGene-Editing Experiments. bioRxiv https://www.biorxiv.org/\ncontent/10.1101/2024.04.25.591003v2, https://doi.org/10.1101/\n2024.04.25.591003 (2024).\n69. Roohani, Y. H. et al. BioDiscoveryAgent: an AI agent for designing\ngenetic perturbation experiments. InProc. 13th Int. Conf. Learn.\nRepresent. (ICLR); https://openreview.net/forum?id=HAwZGLcye3\n(2025).\n70. OpenAI et al. OpenAI o1 System Card.arXiv:2305.14947v2 (2024).\n71. OpenAI. Learning to Reason with LLMs.https://openai.com/index/\nlearning-to-reason-with-llms/ (2024).\n72. Kaplan, J. et al. Scaling laws for neural language models.arXiv\npreprint arXiv:2001. 08361(2020).\n73. Wei, J. et al. Emergent abilities of large language models.Trans.\nMach. Learn. Res.https://openreview.net/forum?id=yzkSU5zdwD\n(2022).\n74. Nguyen, E. et al. Sequence modeling and design from molecular to\ngenome scale with Evo.Science (1979)386, eado9336 (2024).\n75. Brixi, G. et al. Genome modeling and design across all domains of life\nwith Evo 2. bioRxiv (2025).\n76. Cui, H. et al. scGPT: toward building a foundation model for single-\ncell multi-omics using generative AI.Nat. Methods21, 1470–1480\n(2024).\n77. Chithrananda, S., Grand, G. & Ramsundar, B. ChemBERTa: large-scale\nself-supervised pretraining for molecular property prediction. (2020).\n78. Birk, J., Hallin, A. & Kasieczka, G. OmniJet-α: theﬁrst cross-task\nfoundation model for particle physics.Mach. Learn Sci. Technol.5,\n035031 (2024).\n79. McCabe, M. et al. Multiple physics pretraining for spatiotemporal\nsurrogate models. InProc. 38th Annu. Conf. Neural Inf. Process. Syst.\n(NeurIPS); https://openreview.net/forum?id=DKSI3bULiZ (2024).\n80. Shojaee, P., Meidani, K., Gupta, S., Farimani, A. B. & Reddy, C. K.\nLLM-SR: scientiﬁc equation discovery via programming with large\nlanguage models.ArXiv abs/2404.18400, (2024).\n81. Zhang, S. et al. A multimodal biomedical foundation model trained\nfrom ﬁfteen million image–text pairs.NEJM AI2, AIoa2400640 (2025).\n82. Mishra-Sharma, S., Song, Y. & Thaler, J. Paperclip: associating\nastronomical observations and natural language with multi-modal\nmodels. InProc. 1st Conf. Lang. Model.https://openreview.net/\nforum?id=8TdcXwfNRB (2024).\n83. Parker, L. et al. AstroCLIP: a cross-modal foundation model for\ngalaxies. Mon. Not. R. Astron Soc.531, 4990–5011 (2024).\n84. M. Bran, A. et al. Augmenting large language models with chemistry\ntools. Nat. Mach. Intell.6, 525–\n535 (2024).\n85. OpenAI et al. GPT-4 Technical Report. (2023).\n86. Devlin, J., Chang, M. W., Lee, K. & Toutanova, K. BERT: pre-training\nof deep bidirectional transformers for language understanding. In\nProc. 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language\nTechnologies 1, 4171–4186 (2018)..\n87. Radford, A. et al. Learning transferable visual models from natural\nlanguage supervision.Proc. Mach. Learn Res.139,8 7 4 8–8763 (2021).\n88. Ramesh, A. et al. Zero-shot text-to-image generation.Proc. Mach.\nLearn Res.139, 8821–8831 (2021).\n89. DeepMind. AlphaProof: AI achieves silver-medal standard solving\nInternational Mathematical Olympiad problems.https://deepmind.\ngoogle/discover/blog/ai-solves-imo-problems-at-silver-medal-\nlevel/ (2024).\n90. Zelikman, E., Wu, Y., Mu, J. & Goodman, N. D. STaR: bootstrapping\nreasoning with reasoning. (2022).\n91. Popper, K. Karl Popper: The Logic of Scientiﬁc Discovery. (1959).\n92. Fan, W. et al. A Survey on RAG Meeting LLMs: towards retrieval-\naugmented large language models.Proceedings of the ACM\nSIGKDD International Conference on Knowledge Discovery and\nData Mining6491–6501, https://doi.org/10.1145/3637528.3671470\n(2024).\n93. Li, Y., Xu, M., Miao, X., Zhou, S. & Qian, T. Prompting large language\nmodels for counterfactual generation: an empirical study.2024 Joint\nInternational Conference on Computational Linguistics, Language\nResources and Evaluation, LREC-COLING 2024 Main Conference\nProceedings 13201–13221 (2023).\n94. Kiciman, E., Ness, R., Sharma, A. & Tan, C. Causal reasoning and\nlarge language models: opening a new frontier for causality.Trans.\nMach. Learn. Res.https://openreview.net/forum?id=mqoxLkX210\n(2024).\n95. Jiralerspong, T., Chen, X., More, Y., Shah, V. & Bengio, Y. Efﬁcient\ncausal graph discovery using large language models.arXiv preprint\narXiv:2402. 01207(2024).\n96. Schick, T. et al. Toolformer: language models can teach themselves\nto use tools.Adv. Neural Inf. Process Syst.36, (2023).\n97. Chen, Q., Ho, Y.-J. I., Sun, P. & Wang, D. The Philosopher’s Stone for\nScience--the catalyst change of ai for scientiﬁc creativity.Pin and\nWang, Dashun, The Philosopher’s Stone for Science--The Catalyst\nChange of AI for Scientiﬁc Creativity (March 5, 2024)(2024).\n98. Zenil, H. et al. An algorithmic information calculus for causal discovery\nand reprogramming systems.iScience19, 1160–1172 (2019).\n99. Pattee, H. H., Rączaszek-Leonardi, J. & Pattee, H. H. Evolving self-\nreference: matter, symbols, and semantic closure.Laws, Language\nand Life: Howard Pattee’s classic papers on the physics of symbols\nwith contemporary commentary211–\n226 (2012).\nhttps://doi.org/10.1038/s44387-025-00019-5 Perspective\nnpj Artiﬁcial Intelligence|            (2025) 1:14 12\n100. Singh, C., Morris, J. X., Aneja, J., Rush, A. M. & Gao, J. Explaining\npatterns in data with language models via interpretable\nautoprompting. arXiv preprint arXiv:2210. 01848(2022).\n101. Li, P. et al. Table-GPT: tableﬁne-tuned GPT for diverse table tasks.\nProc. ACM Manag. Data2, 176 (2024).\n102. Zhan, J. et al. AnyGPT: uniﬁed multimodal LLM with discrete\nsequence modeling. inProc. 62nd Annu. Meet. Assoc. Comput.\nLinguist. (ACL) 9637–9662, https://doi.org/10.18653/v1/2024.acl-\nlong.521 (2024).\n103. Wu, S. et al. Beyond language models: byte models are digital world\nsimulators. arXiv preprintarXiv:2402.19155; https://doi.org/10.\n48550/arXiv.2402.19155 (2024).\n104. Radford, A. et al. Language models are unsupervised multitask\nlearners. OpenAI blog1, 9 (2019).\n105. Weininger, D. SMILES, a chemical language and information\nsystem. 1. Introduction to methodology and encoding rules.J.\nChem. Inf. Comput Sci.28,3 1–36 (1988).\n106. Zhang, S. et al. Intelligence at the edge of chaos. InProc. 13th Int.\nConf. Learn. Represent.(ICLR); https://openreview.net/forum?id=\nIeRcpsdY7P (2025).\n107. Lyu, C. et al. Large language models as code executors: an\nexploratory study.arXiv preprintarXiv:2410.06667; https://doi.org/\n10.48550/arXiv.2410.06667 (2024).\n108. Jiang, J. et al. A survey on large language models for code\ngeneration. arXiv preprintarXiv:2406.00515; https://doi.org/10.\n48550/arXiv.2406.00515(2024).\n109. Wang, G. et al. Voyager: an open-ended embodied agent with large\nlanguage models.Trans. Mach. Learn. Res.https://openreview.net/\nforum?id=ehfRiF0R3a (2024).\n110. Darvish, K. et al. ORGANA: a robotic assistant for automated\nchemistry experimentation and characterization. (2024).\n111. Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K. & Yao, S.\nReﬂexion: language agents with verbal reinforcement learning.Adv.\nNeural Inf. Process Syst.36 (2023).\n112. Yao, S. et al. ReAct: synergizing reasoning and acting in language\nmodels. InProc. 11th Int. Conf. Learn. Represent.(ICLR); https://\nopenreview.net/forum?id=WE_vluYUL-X (2023).\n113. Lavin, A. et al. Simulation intelligence: towards a new generation of\nscientiﬁc methods.ArXiv abs/2112.03235, (2021).\n114. Park, J. S. et al. Generative agents: interactive simulacra of human\nbehavior. UIST '23: Proc. 36th Annual ACM Symposium on User\nInterface Software and Technology\nhttps://doi.org/10.1145/\n3586183.3606763 (2023).\n115. Singh, C. et al. Explaining black box text modules in natural language\nwith language models.arXiv preprint arXiv:2305. 09863(2023).\n116. Bills, S. et al. Language models can explain neurons in language\nmodels. URL https://openaipublic. blob. core. windows. net/neuron-\nexplainer/paper/index. html. (Date accessed: 14. 05. 2023)(2023).\n117. Yin, Y., Wang, Y., Evans, J. A. & Wang, D. Quantifying the dynamics\nof failure across science, startups and security.Nature 575, 190–194\n(2019).\n118. Kauffman, S. A.Investigations. (Oxford University Press, 2000).\n119. Burger, B. et al. A mobile robotic chemist.Nature 583, 237–241\n(2020). 2020 583:7815.\n120. Yang, C. et al. Large language models as optimizers. InProc. 12th\nInt. Conf. Learn. Represent.(ICLR); https://openreview.net/forum?\nid=Bb4VGOWELI (2024).\n121. Wang, Q., Downey, D., Ji, H. & Hope, T. SciMON: scientiﬁc\ninspiration machines optimized for novelty. InProc. 62nd Annu.\nMeet. Assoc. Comput. Linguist.(ACL) 279–299 (2024).\n122. Ellis, K. et al. DreamCoder: growing generalizable, interpretable\nknowledge with wake–sleep Bayesian program learning.\nPhilosophical Transactions of the Royal Society A381 (2020).\n123. Hazan, H. & Levin, M. Exploring the behavior of bioelectric circuits\nusing evolution heuristic search.Bioelectricity 4, 207–227 (2022).\n124. Fleming, L. Recombinant uncertainty in technological search.\nManag. Sci.47, 117–132 (2001).\n125. Weitzman, M. Optimal Search for the Best Alternative. vol. 78\n(Department of Energy, 1978).\n126. Van Dis, E. A. M., Bollen, J., Zuidema, W., van Rooij, R. & Bockting, C.\nL. ChatGPT:ﬁve priorities for research.Nature 614, 224–226 (2023).\n127. Edwards, B. Why ChatGPT and Bing Chat are so good at making\nthings up.Ars Technica(2023).\n128. DeepMind. Shaking the foundations: delusions in sequence models\nfor interaction and control.www.deepmind.com (2023).\n129. Wang, R. et al. Hypothesis search: inductive reasoning with\nlanguage models. InProc. 12th Int. Conf. Learn. Represent.(ICLR)\nhttps://openreview.net/forum?id=G7UtIGQmjm (2024).\n130. Lavin, A. et al. Technology readiness levels for machine learning\nsystems. Nat. Commun.13, (2020).\n131. Zhou, J. P. et al. Don’t Trust: Verify— Grounding LLM Quantitative\nReasoning with Autoformalization.12th International Conference on\nLearning Representations, ICLR 2024(2024).\n132. Olausson, T. X. et al. LINC: A Neurosymbolic Approach for Logical\nReasoning by Combining Language Models with First-Order Logic\nProvers. EMNLP 2023 - 2023 Conference on Empirical Methods in\nNatural Language Processing, Proceedings5153–5176 https://doi.\norg/10.18653/V1/2023.EMNLP-MAIN.313 (2023).\n133. Wang, R. et al. Hypothesis Search: Inductive Reasoning with\nLanguage Models.12th International Conference on Learning\nRepresentations, ICLR 2024(2023).\n134. Chollet, F. On the measure of intelligence.arXiv preprint\narXiv:1911.01547; https://doi.org/10.48550/arXiv.1911.01547\n(2019).\n135. Abdel-Rehim, A. et al. Scientiﬁc hypothesis generation by a large\nlanguage model: laboratory validation in breast cancer treatment.R.\nSoc. Interface22, 20240674 (2025).\n136. Zhao, Y. et al. Assessing and understanding creativity in large\nlanguage models. arXiv:2401.12491 [cs.CL] (2024).\n137. Swanson, K., Wu, W., Bulaong, N. L., Pak, J. E. & Zou, J. The Virtual\nLab: AI Agents Design New SARS-CoV-2 Nanobodies with\nExperimental Validation. bioRxiv (2024). 2024.11.11.623004. .\n138. Girotra, K., Meincke, L., Terwiesch, C. & Ulrich, K. T. Ideas are dimes\na dozen: Large language models for idea generation in innovation.\nAvailable at SSRN 4526071(2023).\n139. Qi, B. et al. Large Language Models are Zero Shot Hypothesis\nProposers. arXiv preprint arXiv:2311. 05965(2023).\n140. Singh, C. et al. Explaining black box text modules in natural language\nwith language models. (2023).\n141. Liu, T. J. B., Boulle, N., Sarfati, R. & Earls, C. LLMs learn governing\nprinciples of dynamical systems, revealing an in-context neural\nscaling law. InProc. 2024 Conf. Empir. Methods Nat. Lang. Process.\n(EMNLP) 15097–15117; https://doi.org/10.18653/v1/2024.emnlp-\nmain.842 (2024).\n142. Delétang, G. et al. Language Modeling Is Compression.12th\nInternational Conference on Learning Representations, ICLR 2024\n(2023).\n143. Huang, Y., Zhang, J., Shan, Z. & He, J. Compression represents\nintelligence linearly. (2024).\n144. Shao, Y. et al. Assisting in writing wikipedia-like articles from scratch\nwith large language models. InProc. 2024 Conference of of the North\nAmerican Chapter of the Association for Computational Linguistics:\nHuman Language Technologies1, 6252–6278 (2024)..\n145. OpenAI. Introducing deep research.https://openai.com/index/\nintroducing-deep-research/ (2025).\n146. Perplexity AI. Introducing Perplexity Deep Research.https://www.\nperplexity.ai/hub/blog/introducing-perplexity-deep-research(2025).\n147. Cai, Z., Chang, B. & Han, W. Human-in-the-loop through chain-of-\nthought. arXiv preprintarXiv:2306.07932; https://doi.org/10.48550/\narXiv.2306.07932 (2023).\nhttps://doi.org/10.1038/s44387-025-00019-5 Perspective\nnpj Artiﬁcial Intelligence|            (2025) 1:14 13\n148. Ji, Z. et al. Survey of hallucination in natural language generation.\nACM Comput Surv.55,1 –38 (2022).\n149. Matsakis, L. Artiﬁcial Intelligence May Not‘Hallucinate’After All.\nWired (2019).\n150. Gilmer, J. & Hendrycks, D. A Discussion of‘adversarial examples are\nnot bugs, they are features’: adversarial example researchers need\nto expand what is meant by‘robustness’. Distill 4, 00019.1 (2019).\n151. Sahoo, P. et al. A comprehensive survey of hallucination in large\nlanguage, image, video and audio foundation models. inFindings\nAssoc. Comput. Linguist.: EMNLP 2024 11709–11724; 10.18653/\nv1/2024.ﬁndings-emnlp.685 (2024).\n152. Varshney, N. et al. A stitch in time saves nine: detecting and mitigating\nhallucinations of LLMs by validating low-conﬁdence generation.\narXiv preprintarXiv:2307.03987; https://doi.org/10.48550/arXiv.\n2307.03987 (2023).\n153. Li, J., Zhang, Q., Yu, Y., Fu, Q. & Ye, D. More agents is all you need.\nTrans. Mach. Learn. Res.https://openreview.net/forum?id=\nbgzUSZ8aeg (2024).\n154. Mündler, N., He, J., Jenko, S. & Vechev, M. Self-contradictory\nhallucinations of large language models: evaluation, detection and\nmitigation. InProc. 12th Int. Conf. Learn. Represent.(ICLR); https://\nopenreview.net/forum?id=EmQSOi1X2f (2024).\n155. Dhuliawala, S. et al. Chain-of-Veriﬁcation Reduces Hallucination in\nLarge Language Models.Findings of the Association for\nComputational Linguistics ACL 20243563–3578 https://doi.org/10.\n18653/V1/2024.FINDINGS-ACL.212 (2024).\n156. Gao, L. et al. RARR: Researching and Revising What Language\nModels Say, Using Language Models.Proc. Annu. Meet. Assoc.\nComput.l Linguist.1, 16477–16508 (2023).\n157. Zhu, C., Xu, B., Wang, Q., Zhang, Y. & Mao, Z. On the calibration of\nlarge language models and alignment. (2022).\n158. Li, J., Cheng, X., Zhao, W. X., Nie, J. Y. & Wen, J. R. HaluEval: A\nLarge-Scale Hallucination Evaluation Benchmark for Large\nLanguage Models.EMNLP 2023 - 2023 Conference on Empirical\nMethods in Natural Language Processing, Proceedings6449–6464\nhttps://doi.org/10.18653/V1/2023.EMNLP-MAIN.397 (2023).\n159. Lin, S., Hilton, J. & Evans, O. TruthfulQA: measuring how models\nmimic human falsehoods.Proc. Annu. Meet. Assoc. Comput.\nLinguist. 1\n, 3214–3252 (2021).\n160. Min, S. et al. FActScore: Fine-grained Atomic Evaluation of Factual\nPrecision in Long Form Text Generation.EMNLP 2023 - 2023\nConference on Empirical Methods in Natural Language Processing,\nProceedings 12076–12100 https://doi.org/10.18653/v1/2023.\nemnlp-main.741 (2023).\n161. Liu, Y. et al. Trustworthy LLMs: a Survey and Guideline for Evaluating\nLarge Language Models’Alignment. (2023).\n162. Li, M. et al. Think Twice Before Trusting: Self-Detection for Large\nLanguage Models through Comprehensive Answer Reﬂection. (2024).\n163. Ye, Q., Fu, H. Y., Ren, X. & Jia, R. How Predictable Are Large Language\nModel Capabilities? A Case Study on BIG-bench.Findings of the\nAssociation for Computational Linguistics: EMNLP 20237493–7517\nhttps://doi.org/10.18653/v1/2023.ﬁndings-emnlp.503(2023).\n164. Berglund, L. et al. The reversal curse: LLMs trained on“Ai sB” fail to\nlearn “Bi sA”.I nProc. 12th Int. Conf. Learn. Represent.(ICLR)\nhttps://openreview.net/forum?id=GPKTIktA0k (2024).\n165. Chen, X., Chi, R. A., Wang, X. & Zhou, D. Premise order matters in\nreasoning with large language models.Proc. Mach. Learn Res.235,\n6596–6620 (2024).\n166. Allen-Zhu, Z. & Li, Y. Physics of language models: part 3.2,\nknowledge manipulation. InProc. 13th Int. Conf. Learn. Represent.\n(ICLR); https://openreview.net/forum?id=oDbiL9CLoS (2025).\n167. Nezhurina, M., Cipolina-Kun, L., Cherti, M. & Jitsev, J. Alice in\nWonderland: simple tasks showing complete reasoning breakdown in\nstate-of-the-art large language models.arXiv preprint\narXiv:2406.02061;https://doi.org/10.48550/arXiv.2406.02061(2025).\n168. Dziri, N. et al. Faith and Fate: Limits of Transformers on\nCompositionality. Adv Neural Inf. Process Syst.36, (2023).\n169. Delétang, G. et al. Neural Networks and the Chomsky Hierarchy.\n11th International Conference on Learning Representations, ICLR\n2023 (2022).\n170. McCoy, R. T., Yao, S., Friedman, D., Hardy, M. D. & Grifﬁths, T. L.\nEmbers of autoregression show how large language models are\nshaped by the problem they are trained to solve.Proc. Natl. Acad.\nSci. USA121, e2322420121 (2024).\n171. Wu, Z. et al. Reasoning or reciting? Exploring the capabilities and\nlimitations of language models through counterfactual tasks.Proc.\n2024 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang.\nTechnol., NAACL 20241, 1819–1862 (2024).\n172. Kambhampati, S. et al. LLMs Can’t Plan, But Can Help Planning in\nLLM-Modulo Frameworks. (2024).\n173. Pallagani, V. et al. On the Prospects of Incorporating Large\nLanguage Models (LLMs) in Automated Planning and Scheduling\n(APS). Proc. Int. Conf. Automated Plan. Sched.34, 432–444 (2024).\n174. Huang, J. et al. Large Language Models Cannot Self-Correct Reasoning\nYet. 12th International Conference on Learning Representations, ICLR\n2024 (2023).\n175. Wang, X. et al. Self-Consistency Improves Chain of Thought\nReasoning in Language Models.11th International Conference on\nLearning Representations, ICLR 2023(2022).\n176. Zhou, K., Hwang, J., Ren, X. & Sap, M. Relying on the unreliable: the\nimpact of language models’reluctance to express uncertainty. In\nProc. 62nd Annu. Meet. Assoc. Comput. Linguist.(ACL) 3623–3643;\nhttps://doi.org/10.18653/v1/2024.acl-long.198 (2024).\n177. Du, Y., Li, S., Torralba, A., Tenenbaum, J. B. & Mordatch, I. Improving\nFactuality and Reasoning in Language Models through Multiagent\nDebate. Proc. Mach. Learn Res235, 11733–11763 (2023).\n178. Golovneva, O., Allen-Zhu, Z., Weston, J. E. & Sukhbaatar, S. Reverse\ntraining to nurse the reversal curse. InProc. 1st Conf. Lang. Model.\nhttps://openreview.net/forum?id=HDkNbfLQgu (2024).\n179. interpreting GPT: the logit lens— LessWrong.https://www.lesswrong.\ncom/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens.\n180. Zou, A. et al. Representation engineering: A top-down approach to ai\ntransparency. arXiv preprint arXiv:2310. 01405(2023).\n181. Chen, Y. et al. Do models explain themselves? counterfactual\nsimulatability of natural language explanations. InProc. 41st Int.\nConf. Mach. Learn.(ICML) 7880–7904; https://proceedings.mlr.\npress/v235/chen24bl.html (2024).\n182. Wiegreffe, S. & Pinter, Y. Attention is not not Explanation.EMNLP-\nIJCNLP 2019 - 2019 Conference on Empirical Methods in Natural\nLanguage Processing and 9th International Joint Conference on\nNatural Language Processing, Proceedings of the Conference\n11–20 https://doi.org/10.18653/V1/D19-1002 (2019).\n183. Jain, S. & Wallace, B. C. Attention is not explanation.In Proc. 2019\nConference of the North3543–3556 https://doi.org/10.18653/V1/\nN19-1357 (2019).\n184. Zhang, Y. et al. Attention is all you need: utilizing attention in AI-\nenabled drug discovery.Brief. Bioinform.25,1 –22 (2023).\n185. Narayanan, S. et al. Aviary: training language agents on challenging\nscientiﬁc tasks. (2024).\n186. Taragin, M. I. Learning from negativeﬁndings. Isr. J. Health Policy\nRes 8,1 –4 (2019).\n187. Bik, E. M. Publishing negative results is good for science.Access\nMicrobiol 6, 000792 (2024).\n188. Echevarriá, L., Malerba, A. & Arechavala-Gomeza, V. Researcher’s\nPerceptions on Publishing“Negative” Results and Open Access.\nNucleic Acid Ther.31, 185 (2021).\n189. Gray, A. ChatGPT‘contamination’: estimating the prevalence of LLMs in\nthe scholarly literature.https://doi.org/10.1002/leap.1578(2024).\n190. Liang, W. et al. Mapping the Increasing Use of LLMs in Scientiﬁc\nPapers. (2024).\nhttps://doi.org/10.1038/s44387-025-00019-5 Perspective\nnpj Artiﬁcial Intelligence|            (2025) 1:14 14\n191. Latona, G. R., Ribeiro, M. H., Davidson, T. R., Veselovsky, V. & West,\nR. The AI Review Lottery: Widespread AI-Assisted Peer Reviews\nBoost Paper Scores and Acceptance Rates. (2024).\n192. Liang, W. et al. Monitoring AI-modiﬁed content at scale: a case study\non the impact of ChatGPT on AI conference peer reviews. inProc.\n41st Int. Conf. Mach. Learn.(ICML) 29575–29620; https://\nproceedings.mlr.press/v235/liang24b.html (2024).\n193. Liang, W. et al. Can large language models provide useful feedback on\nresearch papers? A large-scale empirical analysis.NEJM AI1, (2024).\n194. Thelwall, M. Can ChatGPT evaluate research quality?J. Data Inf. Sci.\n9,1 –21 (2024).\n195. Meyer, J. G. et al. ChatGPT and large language models in academia:\nopportunities and challenges.BioData Min.16, 20 (2023).\n196. Yanai, I. & Lercher, M. Night science.Genome Biol.20,1 –3 (2019).\n197. Silver, D. et al. Mastering the game of Go with deep neural networks\nand tree search.Nature 529, 484–489 (2016).2016 529:7587.\n198. Silver, D. et al. A general reinforcement learning algorithm that\nmasters chess, shogi, and Go through self-play.Science (1979)362,\n1140–1144 (2018).\n199. Zhao, A. et al. Absolute Zero: reinforced self-play reasoning with\nzero data.arXiv preprintarXiv:2505.03335; https://doi.org/10.\n48550/arXiv.2505.03335 (2025).\n200. Tegnér, J. N. et al. Computational disease modeling— Fact or\nﬁction? BMC Syst. Biol.3,1 –3 (2009).\n201. Petroni, F. et al. Language Models as Knowledge Bases?EMNLP-\nIJCNLP 2019— 2019 Conference on Empirical Methods in Natural\nLanguage Processing and 9th International Joint Conference on\nNatural Language Processing, Proceedings of the Conference\n2463–2473, https://doi.org/10.18653/v1/d19-1250 (2019).\n202. Lee, M. A mathematical investigation of hallucination and creativity\nin gpt models.Mathematics 11, 2320 (2023).\n203. Chen, M. et al. Evaluating Large Language Models Trained on Code.\n(2021).\n204. Peng, B. et al. Check your facts and try again: Improving large\nlanguage models with external knowledge and automated feedback.\narXiv preprint arXiv:2302. 12813(2023).\n205. Luo, L., Li, Y.-F., Haffari, G. & Pan, S. Reasoning on graphs: faithful\nand interpretable large language model reasoning. (2023).\n206. Ji, Z. et al. Towards Mitigating LLM Hallucination via Self Reﬂection.\n1827–\n1843, https://doi.org/10.18653/V1/2023.FINDINGS-EMNLP.\n123 (2023).\n207. Yao, S. et al. Tree of Thoughts: Deliberate Problem Solving with\nLarge Language Models.Adv. Neural Inf. Process Syst.36 (2023).\n208. Ye, X. & Durrett, G. The Unreliability of Explanations in Few-shot\nPrompting for Textual Reasoning.Adv. Neural Inf. Process Syst.35\n(2022).\n209. Ghandeharioun, A., Caciularu, A., Pearce, A., Dixon, L. & Geva, M.\nPatchscopes: a unifying framework for inspecting hidden\nrepresentations of language models.Proc. Mach. Learn Res.235,\n15466–15490 (2024).\n210. Hyland, K. Academic publishing and the myth of linguistic injustice.\nJ. Second Lang. Writ.31,5 8–69 (2016).\n211. Clavero, M. “Awkward wording. Rephrase”: linguistic injustice in\necological journals. (2010).\n212. Strauss, P. Shakespeare and the English poets: the inﬂuence of\nnative speaking English reviewers on the acceptance of journal\narticles. Publications 7, 20 (2019).\nAuthor contributions\nY.Z., J.T., S.K., H.Y., A.M. and H.Z. wrote most of the paper. Y.Z. prepared\nﬁgures with input from H.Z. and J.T. All authors reviewed the manuscript at\ndifferent stages.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nCorrespondenceand requests for materials should be addressed to\nHector Zenil.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional\nclaims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long\nas you give appropriate credit to the original author(s) and the source,\nprovide a link to the Creative Commons licence, and indicate if changes\nwere made. The images or other third party material in this article are\nincluded in the article’s Creative Commons licence, unless indicated\notherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons licence and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to\nobtain permission directly from the copyright holder. To view a copy of this\nlicence, visithttp://creativecommons.org/licenses/by/4.0/.\n© Crown 2025\n1Allen Discovery Center at Tufts University, Medford, MA, USA.2Living Systems Lab, KAUST, Thuwal, Saudi Arabia.3Biological and Environmental Science and\nEngineering Division, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia.4SDAIA-KAUST Center of Excellence in Data Science and Artiﬁcial\nIntelligence, Thuwal, Saudi Arabia.5Intelligent Infrastructure Team, Network Rail, UK.6Department for AI in Society, Science, and Technology, Zuse Institute Berlin,\nBerlin, Germany.7NVIDIA Research, Santa Clara, USA.8Pasteur Labs, Brooklyn, NY, USA.9Wyss Institute for Biologically Inspired Engineering, Harvard University,\nBoston, MA, USA.10Department of Chemistry, University of Southampton, Southampton, Hampshire, UK.11Department of Biomedical Data Science, Stanford\nUniversity, Stanford, CA, USA.12Knowledge Lab, Department of Sociology, University of Chicago, Chicago, IL, USA.13Santa Fe Institute, Santa Fe, NM, USA.14School\nof Informatics, The University of Edinburgh, Edinburgh, UK.15Department of Knowledge Technologies, Jožef Stefan Institute, Ljubljana, Slovenia.16Biological and\nEnvironmental Science and Engineering Division, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia.17Unit of Computational\nMedicine, Department of Medicine, Center for Molecular Medicine, Karolinska Institutet, Karolinska University Hospital, Stockholm, Sweden.18Computer, Electrical and\nMathematical Sciences and Engineering Division, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia.19Science for Life Laboratory,\nSolna, Sweden.20Algorithmic Dynamics Lab, Research Departments of Biomedical Computing and Digital Twins, School of Biomedical Engineering and Imaging\nSciences, King’s College London, London, UK.21King’sI n s t i t u t ef o rA r t iﬁcial Intelligence, London, UK.22The Alan Turing Institute, London, UK.23Oxford Immune\nAlgorithmics, Oxford University Innovation and London Institute for Healthcare Engineering, London, UK.24Cancer Interest Group, Francis Crick Institute, London, UK.\ne-mail: hector.zenil@kcl.ac.uk\nhttps://doi.org/10.1038/s44387-025-00019-5 Perspective\nnpj Artiﬁcial Intelligence|            (2025) 1:14 15",
  "topic": "Scientific discovery",
  "concepts": [
    {
      "name": "Scientific discovery",
      "score": 0.4506703019142151
    },
    {
      "name": "Computer science",
      "score": 0.42249417304992676
    },
    {
      "name": "Psychology",
      "score": 0.3975648880004883
    },
    {
      "name": "Data science",
      "score": 0.38830628991127014
    },
    {
      "name": "Cognitive science",
      "score": 0.3644651770591736
    },
    {
      "name": "Epistemology",
      "score": 0.3444262146949768
    },
    {
      "name": "Linguistics",
      "score": 0.3285853862762451
    },
    {
      "name": "Philosophy",
      "score": 0.17344823479652405
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I121934306",
      "name": "Tufts University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I71920554",
      "name": "King Abdullah University of Science and Technology",
      "country": "SA"
    },
    {
      "id": "https://openalex.org/I195893171",
      "name": "Zuse Institute Berlin",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I81935515",
      "name": "Network Rail",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210127875",
      "name": "Nvidia (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I43439940",
      "name": "University of Southampton",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1308548392",
      "name": "Santa Fe Institute",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I40347166",
      "name": "University of Chicago",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I98677209",
      "name": "University of Edinburgh",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I3006985408",
      "name": "Jožef Stefan Institute",
      "country": "SI"
    },
    {
      "id": "https://openalex.org/I28166907",
      "name": "Karolinska Institutet",
      "country": "SE"
    },
    {
      "id": "https://openalex.org/I1335113835",
      "name": "Karolinska University Hospital",
      "country": "SE"
    },
    {
      "id": "https://openalex.org/I2800139495",
      "name": "Science for Life Laboratory",
      "country": "SE"
    },
    {
      "id": "https://openalex.org/I4210153400",
      "name": "King's College Hospital",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I183935753",
      "name": "King's College London",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210128584",
      "name": "The Alan Turing Institute",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I2801081054",
      "name": "The Francis Crick Institute",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I40120149",
      "name": "University of Oxford",
      "country": "GB"
    }
  ],
  "cited_by": 1
}