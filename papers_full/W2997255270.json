{
  "title": "On the comparability of Pre-trained Language Models",
  "url": "https://openalex.org/W2997255270",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4283488800",
      "name": "Aßenmacher, Matthias",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222999907",
      "name": "Heumann, Christian",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1816313093",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2989499211",
    "https://openalex.org/W2953958347",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2806120502",
    "https://openalex.org/W2086161653",
    "https://openalex.org/W2998579922",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W2784121710",
    "https://openalex.org/W2950501607",
    "https://openalex.org/W2740721704",
    "https://openalex.org/W4919037",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2971008823",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2793353489",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2948223045",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W2427527485"
  ],
  "abstract": "Recent developments in unsupervised representation learning have successfully established the concept of transfer learning in NLP. Mainly three forces are driving the improvements in this area of research: More elaborated architectures are making better use of contextual information. Instead of simply plugging in static pre-trained representations, these are learned based on surrounding context in end-to-end trainable models with more intelligently designed language modelling objectives. Along with this, larger corpora are used as resources for pre-training large language models in a self-supervised fashion which are afterwards fine-tuned on supervised tasks. Advances in parallel computing as well as in cloud computing, made it possible to train these models with growing capacities in the same or even in shorter time than previously established models. These three developments agglomerate in new state-of-the-art (SOTA) results being revealed in a higher and higher frequency. It is not always obvious where these improvements originate from, as it is not possible to completely disentangle the contributions of the three driving forces. We set ourselves to providing a clear and concise overview on several large pre-trained language models, which achieved SOTA results in the last two years, with respect to their use of new architectures and resources. We want to clarify for the reader where the differences between the models are and we furthermore attempt to gain some insight into the single contributions of lexical/computational improvements as well as of architectural changes. We explicitly do not intend to quantify these contributions, but rather see our work as an overview in order to identify potential starting points for benchmark comparisons. Furthermore, we tentatively want to point at potential possibilities for improvement in the field of open-sourcing and reproducible research.",
  "full_text": "arXiv:2001.00781v1  [cs.CL]  3 Jan 2020\nO N T H E C O M PA R A B I L I T Y O F P R E - T R A I N E D L A N G UAG E M O D E L S\nA P R E P R I N T\nMatthias Aßenmacher\nDepartment of Statistics\nLudwig-Maximilians-Universität\nMunich, Germany\nassenmacher@stat.uni-muenchen.de\nChristian Heumann\nDepartment of Statistics\nLudwig-Maximilians-Universität\nMunich, Germany\nchris@stat.uni-muenchen.de\nJanuary 6, 2020\nAB S T R AC T\nRecent developments in unsupervised representation learn ing have successfully established the\nconcept of transfer learning in NLP . Mainly three forces are driving the improvements in this area\nof research:\nMore elaborated architectures are making better use of cont extual information. Instead of simply\nplugging in static pre-trained representations, these are learned based on surrounding context\nin end-to-end trainable models with more intelligently des igned language modelling objectives.\nAlong with this, larger corpora are used as resources for pre -training large language models in a\nself-supervised fashion which are afterwards ﬁne-tuned on supervised tasks. Advances in paral-\nlel computing as well as in cloud computing, made it possible to train these models with growing\ncapacities in the same or even in shorter time than previousl y established models. These three\ndevelopments agglomerate in new state-of-the-art (SOT A) r esults being revealed in a higher and\nhigher frequency . It is not always obvious where these impro vements originate from, as it is not\npossible to completely disentangle the contributions of th e three driving forces.\nW e set ourselves to providing a clear and concise overview on several large pre-trained language\nmodels, which achieved SOT A results in the last two years, wi th respect to their use of new ar-\nchitectures and resources. W e want to clarify for the reader where the differences between the\nmodels are and we furthermore attempt to gain some insight in to the single contributions of lexi-\ncal/computational improvements as well as of architectura l changes. W e explicitly do not intend\nto quantify these contributions, but rather see our work as a n overview in order to identify po-\ntential starting points for benchmark comparisons. Furthe rmore, we tentatively want to point at\npotential possibilities for improvement in the ﬁeld of open -sourcing and reproducible research.\nK eywords Natural Language Processing · Pre-trained language models · T ransfer learning · Model comparison\n1 Introduction\nFor the approaches towards most NLP tasks, researchers turn to using pre-trained word embeddings\n(Mikolov et al., 2013; Pennington et al., 2014; Bojanowski e t al., 2017) as a key component of their models. The\nrepresentations map each word of a sequence ( w1, . . . , wT ) to a real valued vector of dimension d . A drawback of\nthese kinds of externally learned features is that they are ( i) ﬁxed, i.e. can not be adapted to a speciﬁc domain they\nare used in, and (ii) context independent, i.e. there ’ s only one embedding for a word by which it is represented in\nany context.\nMore recently , transfer learning approaches, as for exampl e using convolutional neural networks (CNNs) pre-\ntrained on ImageNet (Krizhevsky et al., 2012) in computer vi sion, have entered the discussion. T ransfer learning\nin NLP context means pre-training a network with a self-supe rvised1 objective on large amounts of plain text and\n1 With self-supervised learning we refer to a technique, where the labels are automatically generated from the data itself\nwithout relying on external labels.\nA P R E P R I N T - J A N UA RY 6, 2020\nﬁne-tune its weights afterwards on a task speciﬁc, labelled data set. For a comprehensive overview on the current\nstate of transfer learning in NLP , we recommend the excellen t tutorial and blog post by Ruder et al. (2019) 2 .\nWith ULMFiT ( Universal Language Model Fine Tuning), Howard and Ruder (2018) proposed a LSTM-based\n(Hochreiter and Schmidhuber, 1997) approach for transfer l earning in NLP using AWD-LSTMs (Merity et al., 2017).\nAfter pre-training on a large unlabelled corpus, a task-spe ciﬁc layer is added to the network and the whole network\nis ﬁne-tuned using labelled data. This model can be characte rised as unidirectional contextual, while a bidirection-\nally contextual LSTM-based model was presented in ELMo ( Embeddings from Language Models) by Peters et al.\n(2018).\nT able 1: Summarizaton of the basic facts of the evaluated mod el architectures. Despite not being a central part\nof this evaluation, W ord2V ec and F astT ext are added as basel ine comparisons. With T ransfer learning integration ,\nwe try to specify to which degree the model is capable for tran sfer learning. W e distinguish between embedding\nmodels and end-to-end trainable transfer learning models.\nArchitectural Details\nModel Release Architecture Contextuality T ransfer learni ng integration\nW ord2V ec 01/2013 FCNN None Embedding model\nF astT ext 07/2016 FCNN None Embedding model\nULMFiT 01/2018 Forward LSTM Unidirectional Fully end-to-e nd trainable\nELMo 02/2018 biLSTM Bidirectional Embedding model\nGPT 06/2018 T ransformer Unidirectional Fully end-to-end t rainable\nBERT 10/2018 T ransformer Bidirectional Fully end-to-end t rainable\nGPT2 02/2019 T ransformer Unidirectional Fully end-to-end trainable\nXLNet 06/2019 Autoregressive Bidirectional Fully end-to- end trainable\nT ransformer\nRoBERT a 07/2019 T ransformer Bidirectional Fully end-to-e nd trainable\nALBERT 09/2019 T ransformer Bidirectional Fully end-to-en d trainable\nThe bidirectionality in ELMo is achieved by using biLSTMs in stead of AWD-LSTMs. On the other hand, ULMFiT\nuses a more \"pure\" transfer learning approach compared to EL Mo, as the ELMo-embeddings are extracted from\nthe pre-training model and are not ﬁne-tuned in conjunction with the weights of the task-speci ﬁc architecture.\nThe OpenAI GPT ( Generative Pre-Training, Radford et al. (2018)) is a model which resembles th e characteristics of\nULMFiT in two crucial points. It is a unidirectional languag e model and it allows stacking tasks speciﬁc layers on\ntop after pre-training, i.e. it is fully end-to-end trainab le. The major differences between these two models is the\narchitecture inside the LM, where OpenAI GPT uses the T ransf ormer architecture (V aswani et al., 2017).\nInstead of processing one of the input tokens at a time, like r ecurrent architectures (LSTMs, GRUs) do, the T rans-\nformer takes in the whole sequence all at once. This is possib le because it utilizes a variant of the Attention mech-\nanism (Bahdanau et al., 2014), which allows to model depende ncies without having to feed the data to the model\nsequentially . At the same time, the OpenAI GPT can be charact erised as unidirectional model as it just takes into ac-\ncount the left side of the context. Its successor OpenAI GPT2 (Radford et al., 2019) possesses (despite some smaller\narchitectural changes) mainly the same model architecture and can thus also be termed as unidirectional contex-\ntual.\nOriginal BERT ( Bidirectional Encoder Representations from Transformers, Devlin et al. (2018)), and consequently\nthe other two BERT -based approaches discussed here (Liu et a l., 2019; Lan et al., 2019) as well, differ from the GPT\nmodels by the fact that they are bidirectional T ransformer m odels. Devlin et al. (2018) developed Masked Language\nModelling (MLM) as a special training objective which allows the use of a bidirectional T ransformer without com-\npromising the language modelling objective. XLNet (Y ang et al., 2019) on the contrary relies on an objective which\nthe authors call Permutation Language Modelling (PLM) and thus also achieves to model a bidirectional contex t\ndespite being an auto-regressive model. A brief overview on the characteristics of the explained models can be\nfound in table 1.\n2 https://ruder.io/state-of-transfer-learning-in-nlp/\n2\nA P R E P R I N T - J A N UA RY 6, 2020\n2 Related work\nIn their stimulating paper , Raffel et al. (2019) take severa l steps in a similar direction by trying to ensure compa-\nrability among different transformer-based models. They p erform various experiments with respect to the trans-\nfer learning ability of a transformer encoder-decoder arch itecture by varying the pre-training objective (Different\nvariants of denoising vs. language modelling), the pre-tra ining resources (their newly introduced C4 corpus vs.\nvariants thereof ) and the parameter size (from 200M up to 11B ). Especially , their approach of introducing a new\ncorpus and creating subsets resembling previously used cor pora like RealNews (Zellers et al., 2019) or OpenW eb-\nT ext (Gokaslan and Cohen, 2019) is a promising approach in or der to ensure comparability .\nHowever , their experiments do not cover an important point w e trying to address in our paper:\nFocussing on only one speciﬁc architecture does not yield an answer to the question which components explain\nthe performance differences between two models where the ov erall architecture differs as well (e.g. Attention-\nbased vs. LSTM-based). Y ang et al. (2019) also address model comparability to some extent by performing an\nablation study to compare their XLNet explicitly to BERT (De vlin et al., 2018). In this ablation study , they train six\ndifferent XLNet-based models where they modify different p arts of the models in order to quantify how these de-\nsign choices inﬂuence performance. At the same time they res trict themselves to an architecture of the same size\nas BERT -base and use the same lexical resources for pre-trai ning. Liu et al. (2019) vary their RoBERT a model with\nrespect to model size and use of pre-training resources in or der to perform an ablation study aiming at compara-\nbility to BERT . Lan et al. (2019) go even one step further with their ALBERT model by also comparing their model\nto BERT with regard to run time and width/depth of the model.\nDespite all these experiments are highly valuable steps int o the direction of better comparability , there are still no\nclear guidelines on which comparisons to perform in order to ensure a maximum degree of model comparability\nwith respect to potentially inﬂuential factors.\n3 Materials and Methods\nFirst, we will present the different available corpora whic h were utilised for pre-training the models and compare\nthem with respect to their size, the domain they’ re from and t heir accessibility . Subsequently , we will brieﬂy intro-\nduce common benchmark data sets which the models are ﬁne-tun ed and evaluated on.\nWhile the conceptual differences between the evaluated mod els have already been addressed in the introduction,\nthe models will now be described in more detail. This is drive n by the intention to emphasise differences beyond\nthe obvious, conceptual ones.\n3.1 T raining corpora\nW e will start this chapter by brieﬂy introducing the pre-tra ining resources, which are commonly used. While there\nare some corpora that are commonly used by most of the models, some other corpora are often just used by one\nmodel in conjunction with one of the more popular ones. An ove rview is to be found in table 2.\nEnglish Wikipedia Devlin et al. (2018) state that they used data from the Englis h Wikipedia and provide a manual\nfor crawling it, but no actual data set. Their data encompass ed around 2.5B words. Wikipedia data sets are available\nin the T ensorﬂow Datasets-module3.\nCommonCrawl Among other resources, Y ang et al. (2019) used data from Comm onCrawl. Besides stating that\nthey ﬁltered out short or low-quality content no further inf ormation is given. Since CommonCrawl is a dynamic\ndatabase, which is updated on a monthly base, and the extract ed amount of data always depends on the user , we\ncan not provide a word count for this source in table 2.\nClueW eb (Callan et al., 2009), Giga5 (Parker et al., 2011) The information about the use of ClueW eb and Giga5\nis similarly sparse as for CommonCrawl (all three were used f or pre-training XLNet). ClueW eb was obtained by\ncrawling ∼ 2.8M web pages in 2012, Giga5 was crawled between 01/2009 and 12/2010.\n1B W ord Benchmark 4 (Chelba et al., 2013) This corpus, actually introduced as a benchmark data set by\nChelba et al. (2013) back in 2013, combines multiple data set s from the EMNLP 2011 workshop on Statistical\n3 https://www.tensorflow.org/datasets/catalog/wikipedia\n4 https://ai.google/research/pubs/pub41880\n3\nA P R E P R I N T - J A N UA RY 6, 2020\nT able 2: Pre-training resources used by the language models (sorted by release date). Concerning the Accessability,\nthe category Crawling Manual can be ranked between the two other categories. In this case, the authors did not\nprovide the data, but at least a (more or less detailed) manua l for crawling the data (or similar data) oneself. The\ndollar signs in brackets signify the necessity of a payment i n order to get access to the corpus. There ’ s no informa-\ntion on RealNews (Zellers et al., 2019) and C4 (Raffel et al., 2019) as these corpora were not used by the evaluated\nmodels.\nCorpus Source W ord-count † Accessability Used by\nEnglish Wikipedia Abadi et al. (2015) ∼ 2.500M Fully available BERT ; XLNet;\nRoBERT a; ALBERT\nCommonCrawl https://commoncrawl.org/ Unclear Fully available XLNet\nClueW eb 2012-B Callan et al. (2009) Unclear Fully available ($$) XLNet\nGiga5 Parker et al. (2011) Unclear Fully available ($$) XLNe t\n1B W ord Benchmark Chelba et al. (2013) ∼ 830M Fully available ELMo\nBooksCorpus Zhu et al. (2015) ∼ 985M Not available OpenAI GPT ; BERT ;\nXLNet; RoBERT a;\nALBERT\nWikitext-103 Merity et al. (2016) ∼ 103M Fully available ULMFit\nCC-News Liu et al. (2019) Unclear Crawling Manual RoBERT a\nStories T rinh and Le (2018) ∼ 7.000M‡ Fully available RoBERT a\nW ebT ext Radford et al. (2019) Unclear Not available OpenAI G PT2\nOpenW ebT ext Gokaslan and Cohen (2019) Unclear Fully availa ble RoBERT a\n† W e report the word-count as given in the respective articles proposing the corpora. Note that the number of\ntokens reported in depends on the tokenization scheme used by a spec iﬁc model.\n‡ Stated by one of the authors on twitter: https:/twitter .com /thtrieu_/status/1096672446864748545\nMachine T ranslation 5 ( WMT11). The authors normalised and tokenized the corpus an d performed further pre-\nprocessing steps in dropping duplicate sentences as well as discarding words with a count below three. Addition-\nally they randomised the ordering of the sentences in the cor pus. This constitutes a corpus with a vocabulary of\n793.471 words and a total word count of 829.250.940 words.\nBooksCorpus6 (Zhu et al., 2015) With their work from 2015, Zhu et al. introduced two corpora: the MovieBook\nDataset and the BooksCorpus, with the latter one being heavi ly used for pre-training language models (cf. table 2).\nIn their work, they used the BooksCorpus in order to train a mo del for retrieving sentence similarity .\nOverall, the corpus comprises 984.846.357 words 7 in 74.004.228 sentences obtained from analysing 11.038 boo ks.\nThe vocabulary consists of 1.316.420 unique words, making t he corpus lexically more diverse than the 1B W ord\nBenchmark as it possesses a by 66% larger vocabulary whereas having a word count which is only 19% higher .\nUnfortunately it is not available for public download anymo re, the authors just provide a link to the ebook-store\nwhere they scraped the corpus.\nWikitext-1038 (Merity et al., 2016) Merity et al. (2016) emphasised the necessity for a new large scale language\nmodelling data set by stressing the shortcomings of other co rpora. They explicitly highlight the occurrence of\ncomplete articles, which allow the models to learn long rang e dependencies, as one of the main beneﬁts of their\ncorpus. This property is, according to Merity et al. (2016), not given in the 1B W ord Benchmark as the sentence\nordering is randomised there. With a count of 103.227.021 to kens and a vocabulary size of 267.735 it is about one\neighth of the 1B W ord Benchmark’ s size concerning token coun t and about one third concerning the vocabulary\nsize. Note, that there is also a smaller corpus available 9 , which is a subset of about 2% of the size of Wikitext-103.\n5 http://statmt.org/wmt11/\n6 https://yknzhu.wixsite.com/mbweb\n7 Devlin et al. (2018) report that the BooksCorpus consists of on ly 800M words; we assume that the differences are attributed\nto potentially different pre-processing\n8 http://academictorrents.com/details/a4fee5547056c845e31ab952598f43b42333183c\n9 Wikitext-2: http://academictorrents.com/details/ac7ffa98b66427246a316a81b2ea31c9b58ea5b6\n4\nA P R E P R I N T - J A N UA RY 6, 2020\nCC-News (Nagel, 2016) The CC-News corpus was presented and used in Liu et al. (2019) . They used a web crawler\nproposed by Hamborg et al. (2017) to extract data from the Com monCrawl News data set (Nagel, 2016) and ob-\ntained a data set similar to the RealNews data set (Zellers et al., 2019).\nStories10 (T rinh and Le, 2018) This data set is also a speciﬁc subset of the CommonCrawl data . The authors\nbuilt the data based on questions in common sense reasoning t asks. They extracted nearly 1M documents, most\nof which are taken from longer , coherent stories (hence the n ame of the corpus). One of the authors stated on\ntwitter11 that the corpus contains approximately 7B words.\nW ebT ext (Radford et al., 2019) The data set GPT2 was pre-trained on, is not publicly availab le and was obtained\nby creating \"a new web scrape which emphasised document qual ity\" (Radford et al., 2019).\nOpenW ebT ext12 (Gokaslan and Cohen, 2019) As a reaction to Radford et al. (2019) not releasing their pre-\ntraining corpus, Gokaslan and Cohen (2019) started an initi ative to emulate an open-source version of the W ebT ext\ncorpus.\nIt becomes obvious that there is a lot of heterogeneity with r espect to the observed combinations of availability\nand the clear speciﬁcation of the corpus size as word count. S ome corpora specify their size in gigabytes, but do\nnot provide a token count or a vocabulary size. Thus, we can st ate that there is some lack of transparency when\nit comes to the lexical resources used for per-training. Esp ecially , the missing availability of the BooksCorpus is\nproblematic as this corpus is heavily used for pre-training .\n3.2 Benchmark data sets for ﬁne-tuning\nBesides describing pre-training resources, it is also impo rtant to have a look at the data sets which are commonly\nused for benchmarking ﬁne-tuned language models and thus de termine new SOT A results.\nGLUE13 (W ang et al., 2018) The General Language Understanding Evaluation (GLUE) benchmark is a freely avail-\nable collection of nine data sets which models can be evaluat ed on. It also provides a ﬁxed train-dev-test split with\nheld out labels for the test set, as well as a leader board whic h displays the top submissions and the current SOT A.\nThe relevant metric for the SOT A is an aggregate measure of th e nine single task metrics.\nT able 3 provides the basic information on the data sets inclu ded in GLUE. The benchmark includes two binary clas-\nsiﬁcation tasks with single-sentence inputs (CoLa [W arsta dt et al., 2018] and SST -2 [Socher et al., 2013]) and ﬁve\nbinary classiﬁcation tasks with inputs that consist of sent ence-pairs (MRPC [Dolan and Brockett, 2005], QQP 14 ,\nQNLI [W ang et al., 2018], RTE [W ang et al., 2018] and WNLI [W an g et al., 2018]). The remaining two tasks also\ntake sentence-pairs as input but have a multi-class classiﬁ cation objective with either three (MNLI [Williams et al.,\n2017]) or ﬁve classes (STS-B [Cer et al., 2017]).\nT able 3: A brief summarizaton of the different data sets whic h all together form the GLUE benchmark. This table is\nbasically a rearrangement of table 1 from W ang et al. (2018) w ith slightly reduced information as it is just thought\nto be an overview on the different tasks and data set sizes.\nSingle-Sentence Similarity/Paraphrase Inference\ntask CoLa SST -2 MRPC STS-B QQP MNLI QNLI RTE WNLI\n|train| 8.5k 67k 3.7k 7k 364k 393k 105k 2.5k 634\n|test| 1k 1.8k 1.7k 1.4k 391k 20k 5.4k 3k 146\ndomain misc. movies news misc. social QA misc. wiki news, wiki ﬁctio n\nSuperGLUE15 (W ang et al., 2019) As a reaction to human baselines being surpassed by the top ra nked models,\nW ang et al. (2019) proposed a set of benchmark data sets simil ar to, but, according to the authors, more difﬁcult\n10 https://console.cloud.google.com/storage/browser/commonsense-reasoning/reproduce/stories_corpus\n11 https://twitter.com/thtrieu_/status/1096672446864748545\n12 http://academictorrents.com/details/36c39b25657ce1639ccec0a91cf242b42e1f01db\n13 https://gluebenchmark.com/\n14 https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs\n15 https://super.gluebenchmark.com/\n5\nA P R E P R I N T - J A N UA RY 6, 2020\nthan GLUE. On average, the size of the provided training data is smaller than in GLUE and, differently to GLUE,\nthe data is also split in ’ train ’ , ’ dev’ and ’ test’ as in GLUE. As of the writing of this paper , there is a large difference\nbetween the use of GLUE and SuperGLUE concerning the number o f models evaluated on the respective bench-\nmark.\nT able 4: A brief summarizaton of the different data sets whic h all together form the SuperGLUE benchmark. This\ntable is basically a rearrangement of table 1 from W ang et al. (2019) with slightly reduced information as it is just\nthought to be an overview on the different tasks and data set s izes.\nCoreference Disambig. Inference Question Answering\ntask WSC WiC RTE CB BoolQ COPA MultiRC ReCoRD\n|train| 554 6k 2.5k 250 9.4k 400 5.1k 101k\n|dev| 104 638 278 57 3.3k 100 953 10k\n|test| 146 1.4k 300 250 3.2k 500 1.8k 10k\ndomain ﬁction misc. news, wiki misc. google, wiki blogs, art misc. n ews\nIt is considered to be more difﬁcult than GLUE as it contains m ore complex tasks than just single-sentence or\nsentence-pair classiﬁcation. SuperGLUE also features cor eference resolution and question answering tasks. Un-\nfortunately , it did not make sense to include it as a part of ou r model comparison, as (at the time of writing) only\ntwo of the discussed models were evaluated on SuperGLUE.\nSQuAD16 (Rajpurkar et al., 2016, 2018) In its ﬁrst version, the Stanford Question Answering Dataset (SQuAD) 1.1\n(Rajpurkar et al., 2016) consists of 100.000+ questions exp licitly designed to be answerable by reading segments of\nWikipedia articles. The task is to correctly locate the segm ent in the text which contains the answer . A shortcoming\nof this task is the omission of situations where the the quest ion is not answerable by reading the provided article.\nRajpurkar et al. (2018) address this problem in SQuAD 2.0 by a dding 50.000 handcrafted unanswerable questions\nto the SQuAD 1.1 data set. On their homepage, the authors prov ide a train and development set as well as an ofﬁcial\nleader board. The test set is completely held out. Instead, t he participants are required to upload their models to\nCodaLab17 . The SQuAD 1.1 data is, in an augmented form (termed QNLI), al so part of the GLUE benchmark.\nRACE18 (Lai et al., 2017) The Large-scale ReAding Comprehension Dataset From Examinations (RACE) contains\n(english) exam questions for Chinese students (middle and h igh school). In most of the articles, where the model is\nevaluated on RACE, it is described to be especially challeng ing due to (i) the length of the passages, (ii) the inclusion\nof reasoning questions and (iii) the intentionally tricky d esign of the questions in order to test a human ’ s ability in\nreading comprehension. The data set can be subdivided in RAC E-M (middle school examination) and RACE-H\n(high school examination) and comprises a total of 97.687 qu estions on 27.933 passages of text.\n3.3 Evaluated Models\nULMFit (Howard and Ruder, 2018) The ﬁrst \"pure\" transfer learning applied in NLP was ULMFiT i n the begin-\nning of 2018. The core of the model builds on the work from Meri ty et al. (2017) as it uses AWD-LSTMs, which\nis a LSTM-variant that makes use of DropConnect (W an et al., 2 013) for better regularisation and applies aver-\naged stochastic gradient descent (ASGD) for optimization ( Polyak and Juditsky, 1992). This model consists of a\n400 dimensional embedding layer followed by three LSTM laye rs, each of which encompasses 1150 hidden units.\nHoward and Ruder (2018) stack a softmax classiﬁer with a hidd en layer size of 50 on top of this architecture for\npre-training the model. This ﬁnal layer is complemented by a task speciﬁc ﬁnal layer during ﬁne tuning. The vo-\ncabulary size is limited to 30k words as in Johnson and Zhang ( 2017).\nIn contrast to the other models discussed in this paper , ULMF iT was not evaluated on the GLUE benchmark but on\nseveral other data sets (IMDb [Maas et al., 2011], TREC-6 [V o orhees and Tice, 1999], Y elp-bi, Y elp-full, AG’ s news,\nDBpedia [all Zhang et al., 2015]).\nELMo (Peters et al., 2018) As already stated in section 1, ELMo differs from ULMFit with respect to its usability\nfor transfer learning. The pre-trained ELMo-embeddings ar e plugged in at the lowest layer of an arbitrary NLP\n16 https://rajpurkar.github.io/SQuAD-explorer/\n17 https://codalab.org/\n18 http://www.qizhexie.com/data/RACE_leaderboard.html\n6\nA P R E P R I N T - J A N UA RY 6, 2020\nT able 5: An overview on the data sets which ULMFit was ﬁne-tun ed and evaluated on. It is an extension of ta-\nble 1 (Howard and Ruder, 2018), adding information on the siz e of the test set and the domain. All six tasks are\nclassiﬁcation tasks, where the target variables have betwe en 2 and 14 classes.\nQuestion Sentiment T opic\ntask TREC-6 IMDb Y elp-bi Y elp-full AG’ s news DBpedia\n|train| 5 k 25k 560k 650k 120k 560k\n|test| 0.5k 25k 38k 50k 7.6k 70k\ndomain open-domain movies social QA social QA news wiki\nmodel in order to use them for a downstream task 19 . In case of ELMo this means the following: As ELMo consists\nof multiple biLSTM layers, one can extract multiple interme diate-layer representations from the model. These\nrepresentations are used for computing a (task-speciﬁc) we ighted combination, which is concatenated with static\ncontext-independent word embeddings. So the model weights of ELMo are not updated during the training of\nthe downstream model, but only the weights, which are learne d for combining the intermediate-layer representa-\ntions from ELMo, are. Peters et al. (2018) evaluate an ELMo-b ased model on SQuAD and other tasks, but when it\ncomes to GLUE there are multiple ELMo-based architectures a vailable on the leaderboard 20. Thus, here we will\nconcentrate on the best-performing ELMo-based model, a BiL STM-model with Attention (W ang et al., 2018).\nOpenAI GPT (Radford et al., 2018) The OpenAI GPT is a pure attention-based architecture the do es not make\nuse of any recurrent layers. Pre-training is performed by co mbining Byte-Pair encoded (Sennrich et al., 2015) to-\nken embeddings with learned position embeddings, feeding t hem into a multi-layer transformer decoder archi-\ntecture with a standard language modelling objective. By us ing a decoder architecture the model does at each\nstep only have access to the preceding tokens in the sequence . Thus, the GPT model is a unidirectional attention-\nbased architecture. Fine-tuning was, amongst others, perf ormed on the nine tasks that together form the GLUE\nbenchmark.\nBERT (Devlin et al., 2018) This model can be seen as a reference point for everything tha t came thereafter . Simi-\nlar to GPT it uses Byte-Pair Encoding (BPE) with a vocabulary size of 30k. By introducing the MLM training objec-\ntive, the authors were able to combine deep bidirectionalit y with the self-attention mechanism for the ﬁrst time. In\naddition to the MLM objective it also utilizes as next-sente nce prediction (NSP) objective, the usefulness of which\nhas been debated in other research papers (Liu et al., 2019). The BERT -BASE model consists of 12 bidirectional\ntransformer-encoder blocks (24 for BERT -LARGE) as describ ed in V aswani et al. (2017) with 12 (16 respectively)\nattention heads per block and an embedding size of 768 (1024 r espectively). The need to better understand the\nbehaviour of these huge networks even constituted a new ﬁeld of research called BERT ology, aiming at explaining\nthe inner workings of BERT -based models.\nOpenAI GPT2 (Radford et al., 2019) With GPT2, the OpenAI team published a scaled-up version of G PT in 2019.\nCompared to its predecessor , it contains some smaller chang es concerning the placement of layer normalisation\nand residual connections. Overall, there are four differen t versions of GPT2 with the smallest one being equal to\nGPT , the medium one being of similar size as BERT -LARGE and th e xlarge one being released as the actual GPT2\nmodel with 1.5B parameters.\nXLNet (Y ang et al., 2019) In order to overcome (what they call) the pretraining-ﬁnetune discrepancy , which is a\nconsequence of BERT’ s masking approach, and to simultaneou sly include bidirectional contexts, Y ang et al. (2019)\npropose the PLM objective for their XLNet. They use two-stream self-attent ion for preserving the position infor-\nmation of the token to be predicted, which would otherwise be lost due to the permutation of the sequence. While\nthe ﬁrst of the two streams ( content stream attention ) resembles the standard self-attention from a transformer -\ndecoder , the other stream ( query stream attention ) doesn ’ t allow the token to see itself but just the preceding tokens\nof the permuted sequence.\n19 The authors also mention that additionally adding the ELMO-embe ddings at one of the ﬁnal layers might improve perfor-\nmance for some architectures and tasks\n20 https://gluebenchmark.com/leaderboard\n7\nA P R E P R I N T - J A N UA RY 6, 2020\nRoBERT a (Liu et al., 2019) With RoBERT a (short for Robustly optimized BERT a pproach), Liu et al. (2019) intro-\nduce an exact (architectural) replicate of BERT with tuned h yperparameters and a larger corpus used for pre-\ntraining. The masking strategy for pre-training is changed from static (masking once during pre-processing) to\ndynamic (masking every sequence just before feeding it to th e model), the additional NSP objective is removed,\nthe BPE-level vocabulary is adjusted and increased to 50k an d RoBERT a is trained on larger batches than BERT . All\nof these adjustments improve performance of the model and ma ke it competitive to the previously SOT A results of\nXLNet.\nALBERT (Lan et al., 2019) By addressing the steady increase of the model size as a poten tial problem, ALBERT\n(short for A L ite BERT) goes into another direction compared to most of post-BERT a rchitectures. Lan et al. (2019)\napply parameter-reduction techniques in order to train fas ter models with lower memory demands that, at the\nsame time, yield a comparable performance to SOT A models. In our work we will always refer to ALBERT -XXLARGE,\nwhich is the best performing ALBERT model. Note, that also th e much smaller ALBERT models yielded results\ncomparable to or even better than BERT .\n4 Model comparison\nThe two tables below will try to give a comprehensive overvie w on the differences of the previously discussed\nmodel architectures. While table 6 will only attempt to give an overview on the amount of computation that was\nneeded to train a given architecture on a given corpus, we wil l directly try to relate model architecture and size as\nwell as usage of lexical resources to model performance in ta ble 7.\nT able 6: Summarizaton of the basic facts of the evaluated tra nsfer learning model architectures. W ord2V ec, F astT ext\nand ELMo are not included as these are no end-to-end trainabl e models, meaning that the model size also depends\nof the used model after obtaining the embeddings. The parame ter size of ULMFiT is assumed to be the larger\nvalue from Merity et al. (2017), since Howard and Ruder (2018 ) use plain AWD-LSTMs with a vocabulary size of\n30k tokens like Johnson and Zhang (2016, 2017). V alues for GP T2-XLARGE are taken from Strubell et al. (2019).\nCompute Resources\nModel Computational Resources T raining time pfs-days † size lexical\nULMFiT NA NA NA 33M 0.18GB\nGPT 8 GPUs (P600) ∼ 30 days 0.96 117M < 13GB\nBERT -BASE 4 Cloud TPUs (16 chips) ∼ 4 days 0.96 [2.24] ‡ 110M 13GB\nBERT -LARGE 16 Cloud TPUs (64 chips) ∼ 4 days 3.84 [8.96] ‡ 340M 13GB\nGPT2-MEDIUM NA NA NA 345M 40GB\nGPT2-XLARGE 8 v3 Cloud TPUs (32 chips) ∼ 7 days 7.84 1.500M 40GB\nXLNet-LARGE 128 v3 Cloud TPUs (512 chips) ∼ 2.5 days 44.8 340M 126GB\nRoBERT a DGX-1 GPUs (8x32GB V100) NA NA 360M 160GB\nALBERT 64 – 1024 v3 Cloud TPUs NA NA 233M 16GB\n† Estimation according to the formula proposed on https://openai.com/blog/ai-and-compute/:\npfs-days = number of units × PFLOPS/unit × days trained × utilization, with an assumed\nutilization of one third. Information on PFLOPS/unit for TPUs from https://cloud.google.com/tpu/.\n‡ W e provide two numbers here, as Devlin et al. (2018) do not spe cify whether they use v2 or v3 TPUs. The ﬁrst\nnumber assumes the use of v2 TPUs, the one in square brackets assumes use of v3 TPUs.\nOne thing that we can learn from table 6 is the unfortunate lac k of details when it comes to reporting the computa-\ntional resources used for training the models. While Howard and Ruder (2018) do not provide any information at\nall on the computational resources utilised for pre-traini ng ULMFiT , the other articles are also not over-informative\nwhen it comes to reporting them. Unfortunately , there are no clear guidelines on how to appraise resource con-\nsumption when it comes to evaluating and comparing models. T his may be partly attributed to the rapidly growing\nhardware possibilities due to modern cloud computing archi tectures, but in our opinion it should nevertheless be\naccounted for , since it may pose environmental issues (Stru bell et al., 2019) and also limits portability to smaller\ndevices.\n8\nA P R E P R I N T - J A N UA RY 6, 2020\nThe second thing is that it is also important to consider the d ifferences displayed in the tables 6 and 7 when com-\nparing the model performances. When comparing two models of approximately the same size (e.g. BERT -BASE\nversus GPT), it seems to be obvious that the superior perform ance of BERT -BASE originates purely from its more\nelaborated model architecture (cf. table 1) because of the s imilar size. But one should also be aware of the larger\npre-training resources (BERT -BASE uses at least twice as mu ch data for pre-training) as well as the unknown dif-\nferences in usage of computing power . W e estimated the amoun t of compute used by a model as the pfs-days,\nresulting in an estimation for BERT -BASE being not less than the one for GPT .\nAnother aspect which should not be ignored when evaluating p erformance is the use of ensemble models. As can\nbe seen in the ﬁrst column of table 7, the three ensemble model s seem to outperform both of the BERT models by\na large margin. Only parts of these differences may be attrib uted to the model architecture, as the ensembling as\nwell as the larger pre-training resources might also give an advantage to these models. As there are unfortunately\nno single model performance values available for XLNet, RoBERT a and A LBERT on the ofﬁcial GLUE leaderboard,\nwe also compare the single model performances from Lan et al. (2019) obtained on the dev sets ( WNLI excluded).\nFrom this comparison we can get a good impression of how high t he contribution of model ensembling might be:\nThe difference between BERT -LARGE and the XLNet ensemble in the ofﬁcial scores (7.9 percentage points) is more\nthan twice as high as the difference on the dev score (3.4 perc entage points).\nIn order to address the differences in size of the pre-traini ng resources, Y ang et al. (2019) make the extremely in-\nsightful effort to compare a BASE variant of XLNet to BERT -BA SE (same size and same pre-training resources).\nWhile the F1 score on the SQuAD v2.0 dev set is still remarkabl y higher than for BERT -BASE (almost comparable\nto BERT -LARGE) it does not show a large improvement on the RAC E test set anymore (which might have been\nexpected due to the large improvement of XLNet-LARGE over bo th BERT models).\nT able 7: Performance of different models on GLUE, SQuAD and R ACE as well as model size and resource usage\ncompared to BERT -BASE (except for GLUE dev set performance, where BERT -LARGE is the reference). Performance\ndifferences on the benchmark data sets are given in percenta ge points, while the differences in size/resources are\ngiven as factors, e.g. BERT -LARGE has 3.1 times the size of BE RT -BASE and performs 2.2 percentage points better\non GLUE. W e omit SuperGLUE in this table as of the time of writi ng only BERT and RoBERT a were evaluated on\nit. ULMFiT and OpenAI GPT2 are also omitted as there are no per formance values on these data sets publicly\navailable. Highest improvements over the reference model i n bold. For ELMo we do not provide a model size,\nsince the performance values are from two different models ( cf. section 3.3).\nDisplayed performance measures are Matthews Correlation ( GLUE), F1 score (SQuAD) and Accuracy (RACE).\nGLUE SQuAD RACE Resources\nModel leaderboard dev ♥ v1.1 (dev) v2.0 (dev) test size lexical\nBERT -BASE 78.3 – 88.5 76.3 ♣ 65.0 ♦ 110M 13GB\nELMo-based - 8.3 – - 2.9 – – – –\nGPT - 5.5 – – – - 6.0 1.1x < 0.5x\nBERT -LARGE + 2.2 84.05 + 2.4 + 5.6 + 7.0 ♦ 3.1x 1.0x\nXLNet-BASE – – – + 5.03 + 1.05 ∼ 1.0x 1.0x\nXLNet-LARGE + 10.1 ♠ + 3.39 + 6.0 + 12.5 + 16.75 3.1x 9.7x\nRoBERT a + 10.2 ♠ + 5.19 + 6.1 + 13.1 + 18.2 3.3x 12.3x\nRoBERT a-BASE – + 2.30 – – – 1.0x 12.3x\nRoBERT a‡ – + 3.79 + 5.1 + 11.0 – 3.3x 1.2x †\nALBERT + 11.1 ♠ + 5.91 + 5.6 + 13.9 + 21.5 2.1x 1.2x †\n♠ Ensemble performance; No single model performance availab le\n♥ Own calculations based on Lan et al. (2019) table 13; WNLI is o mitted\n♣ Result for BERT -BASE on SQuAD v2.0 is taken from Y ang et al. (2 019) table 6\n♦ Result for BERT -BASE on RACE is taken from Zhang et al. (2019) table 2\n† Liu et al. (2019) and Lan et al. (2019) specify the BooksCorpu s + English Wikipedia as 16GB\n‡ This variant of RoBERT a uses only BooksCorpus + English Wiki pedia for pre-training\nThe comparability of RoBERT a from the GLUE leaderboard (mod el ensemble and larger pre-training resources)\nto BERT -LARGE is again limited, but the authors performed se veral experiments in order to show the usefulness\nof their model optimisations. When pre-training BERT -LARG E and a single RoBERT a model on comparable lexical\n9\nA P R E P R I N T - J A N UA RY 6, 2020\nresources (BooksCorpus + English Wikipedia; 13GB for BERT v s. 16GB for RoBERT a), the RoBERT a model still shows\na signiﬁcant improvement over BERT -LARGE, even if it decrea ses somewhat in size (compared to the difference\nbetween BERT -LARGE and the ensemble model). In another abla tion study , Liu et al. (2019) train a BASE variant\nof RoBERT a on their larger pre-training resources. Even tho ugh comprising only about one third of the size of\nBERT -LARGE, the larger pre-training corpus in conjunction with the optimised training leads to a slightly better\nperformance on the GLUE dev set (without WNLI). Unfortunate ly we cannot compare RoBERT a-BASE to BERT -\nBASE, as we neither have the \"ofﬁcial\" leaderboard score for RoBERT a-BASE nor the \"in-ofﬁcial\" dev set score for\nBERT -BASE.\nT able 8: Performance of BERT -LARGE and XLNet-LARGE on the be nchmark data sets used by Howard and Ruder\n(2018) as well as model size and resource usage compared to ULMFiT . Speciﬁcation of the differences are displayed\nas in table 7, highest improvements over the reference model in bold. Note that we report accuracies here, as\nopposed to Howard and Ruder (2018) and Y ang et al. (2019), in o rder to provide a more similar interpretation of\nthese values compared to the values in table 7. Displayed per formance measures are Accuracy for all tasks.\nSentiment T opic Resources\nModel IMDb Y elp-bi Y elp-full AG’ s news DBpedia size lexical\nULMFiT 95.40 97.84 70.02 94.99 99.20 33M 0.18GB\nBERT -LARGE + 0.09 + 0.27 + 0.66 – + 0.16 10.3x 72.2x\nXLNet-LARGE + 0.81 + 0.61 + 2.28 + 0.52 + 0.18 10.3x 222.2x\nIn order to also set the results of ULMFiT into context, we pre sent the results published by Y ang et al. (2019) along-\nside with the information on model size and use of lexical res ources in table 8. Despite being much larger and\nutilising some orders of magnitude larger corpora for pre-t raining, both BERT -LARGE and XLNet-LARGE do not\nexhibit that large improvements over the performance of ULM FiT . This might partly originate from the simplicity\n(compared to GLUE & co.) of the tasks, but partly also from the already achieved high performances where no\nextremely large improvements are possible anymore.\n5 Discussion\nThis chapter reﬂects the main takeaways from the above compa risons and tries to raise some issues for future\nresearch practices. W e do not claim to have a solution to thes e potentially problematic aspects but think that these\npoints are highly debatable.\nWhy no benchmark corpus for pre-training? It is good and well-established practice to use benchmark da ta sets\nlike GLUE, SuperGLUE (not yet used that often), SQuAD and RAC E for comparing the performance of pre-trained\nlanguage models on different types of NLP/NLU tasks. Many re cently published articles (Liu et al., 2019; Y ang et al.,\n2019; Lan et al., 2019) perform (partly extensive) ablation studies controlling for pre-training resources in order to\nmake (versions of ) their models comparable to BERT as \"bench mark model\", which is really important as it helps\nthe reader to get an intuition for the impact of pre-training resources. Nevertheless, it is unfortunately not perfect\ndue to two critical issues: (i) BERT (and all the other models consequently as well) make use of the BooksCorpus\n(Zhu et al., 2015) which is not publicly available and (ii) th is only leads to model comparisons in a low pre-training\nresource environment (compared to more recent models) and y ields no insight on the behaviour of the reference\nmodel (e.g. BERT) in a high(er) pre-training resource conte xt. So we view statements of the type \"Model architec-\nture A is superior to model architecture B on performing task X.\" somewhat critical and would propose to phrase\nit in a way comparable to the following statement: \"Model architecture A is superior to model architecture B on\nperforming task X, when pre-trained on a small/large corpus of low/high quality data from domain Y for time Z.\"\nWhy no standardised description of (computational) resour ces? When writing this article, it sometimes turned\nout difﬁcult to really get one (measure) for how much compute was used to pre-train the model described in an\narticle. In our opinion, this is not a carelessness of the aut hors but rather the lack of a clear reporting standard. W e\nfound ourselves confronted with the following situations:\na) No information at all (Radford et al., 2019)\nb) Information on the used hardware (Liu et al., 2019; Lan et a l., 2019)\n10\nA P R E P R I N T - J A N UA RY 6, 2020\nc) Information on the used hardware and training time (Devli n et al., 2018; Y ang et al., 2019)\nd) Calculation of a standardised measure (Radford et al., 20 18)21\nWhile situation a) is clearly unsatisfactory and should be avoided, scenarios b) and c) basically provide (almost)\nall of the necessary information but miss out on going the las t ﬁnal step to scenario d) where the reporting would\nreach universal comparability across different articles. A quite nice and intuitive way was also proposed on the\nOpenAI-blog22 for estimating the GPU time needed for model training. This i s of course not as exact as a computa-\ntion based on the counts of operations in a model, but require s on the other hand no deep insight into the model\narchitecture and is thus applicable to a a wide range of archi tectures without much effort.\nShouldn ’ t performance be evaluated in relation to size and r esource consumption? As larger models have a\nhigher capacity for learning good representations and usin g larger pre-training resources should also improve\ntheir quality , varying these two components simultaneousl y with the model architecture might lead to interference\nbetween the individual inﬂuences on model performance. So t he intent of this aspect has a slight overlap with the\nquestion posed above, but while the above is more or less abou t introducing some kind of reference, this is about\ncarefully varying and evaluating the effects of different p arts of the model.\n6 Conclusion\nAs can be seen from the above analysis, there is a clear lack of a concise guideline for fair comparisons of large pre-\ntrained language models. It is not sufﬁcient to just rank mod els by their performance on the common benchmark\ndata sets as this does not take into account all the other fact ors mentioned in this analysis.\nT able 9: Proposal of starting points when thinking about rep orting standards for pre-trained LMs. W e categorise\nthe reporting of the experimental time and the benchmark performance of the un-tuned model as not easily feasible,\nas one has to be aware of these standards in order to track the t ime of all experiments. Also, deﬁning what is an\n\"un-tuned\" version is not always that simple. With \"un-tune d\" we mean not further tuned during pre-training .\nEvaluation\nReporting Standard Feasibility Current Relevance for OK?\nrealisation reproducability\nModel architecture Easy Every article Crucial ✓\nNumber of parameters Easy Most articles Crucial ✓\nHyperparameters\n– T uning method Easy Some articles High /beware\n– T uning time Easy No article Medium /beware\nExperimental time Difﬁcult No article Low /beware\nComputational resources Easy Most articles High /beware\nT raining time Easy Most articles Medium /beware\nLexical resources\n– Information Easy Every article Crucial ✓\n– A vailability Difﬁcult Some articles Crucial /beware\nBenchmark performance\n– Un-T uned single model Difﬁcult No article Low /beware\n– T uned single model Easy Every article Crucial /beware\nA further aspect (which is not explicitly addressed here) is the reporting of resources (time and compute) spent on\nmodel development, including all experimental runs and tri als, and hyperparameter tuning during pre-training.\nIn our opinion, this is important with respect to two facets: On the one hand side it is important to take into ac-\ncount energy and environmental considerations when traini ng deep learning models (Strubell et al., 2019), on the\nother hand it is also a signal to the reader/user for how difﬁc ult it is to train (and to ﬁne-tune) the model. This\nmight have implications for the usage of a model as transfer l earning model for diverse downstream tasks. Models\n21 The calculation was not published as part of the article but is to b e found in a corresponding blog post:\nhttps://openai.com/blog/language-unsupervised/\n22 https://openai.com/blog/ai-and-compute/\n11\nA P R E P R I N T - J A N UA RY 6, 2020\nthat have already been tuned to a high degree during pre-trai ning to reach a certain level of performance, have, in\nthe long run, maybe less potential for further improvements than models which do so without much hyperparam-\neter tuning.\nT aking all these considerations into account, we want to ten tatively propose starting points (cf. table 9) for deﬁn-\ning reporting standards which are globally accepted and app lied when it comes to comparing pre-trained language\nmodels. W e carefully try to categorise the different facets according to feasibility ( How much effort does it take to re-\nport this? ), current realisation ( How many research papers are reporting this? ) and their relevance for reproducible\nresearch ( How crucial is this for performing reproducible research? ). All these categorisations are of more or less\nsubjective nature due to the fact that they cannot be quantiﬁ ed and are based on just a handful of the most inﬂu-\nential research papers.\nW e are aware of the fact, that it might take a large collective effort in order to establish some set of standards but\nwe think that it is an absolutely crucial step to describe all the aspects we mentioned in a way that is as transparent\nas possible in order to foster replicability and reproducab ility .\nReferences\nAbadi, M., Agarwal, A., Barham, P ., Brevdo, E., Chen, Z., Cit ro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M., Ghe-\nmawat, S., Goodfellow , I., Harp, A., Irving, G., Isard, M., J ia, Y ., Jozefowicz, R., Kaiser , L., Kudlur , M., Levenberg,\nJ., Mané, D ., Monga, R., Moore, S., Murray , D ., Olah, C., Schu ster , M., Shlens, J., Steiner , B., Sutskever , I., T alwar ,\nK., T ucker , P ., V anhoucke, V ., V asudevan, V ., Viégas, F ., Vi nyals, O ., W arden, P ., W attenberg, M., Wicke, M., Yu, Y .,\nand Zheng, X. (2015). T ensorFlow: Large-scale machine lear ning on heterogeneous systems. Software available\nfrom tensorﬂow .org.\nBahdanau, D ., Cho, K., and Bengio, Y . (2014). Neural machine translation by jointly learning to align and translate.\narXiv preprint arXiv:1409.0473 .\nBojanowski, P ., Grave, E., Joulin, A., and Mikolov , T . (2017 ). Enriching word vectors with subword information.\nT ransactions of the Association for Computational Linguis tics, 5:135–146.\nCallan, J., Hoy , M., Y oo, C., and Zhao, L. (2009). Clueweb09 d ata set.\nCer , D ., Diab, M., Agirre, E., Lopez-Gazpio, I., and Specia, L. (2017). Semeval-2017 task 1: Semantic textual\nsimilarity-multilingual and cross-lingual focused evalu ation. arXiv preprint arXiv:1708.00055 .\nChelba, C., Mikolov , T ., Schuster , M., Ge, Q., Brants, T ., Ko ehn, P ., and Robinson, T . (2013). One billion word bench-\nmark for measuring progress in statistical language modeli ng. arXiv preprint arXiv:1312.3005 .\nDevlin, J., Chang, M.-W ., Lee, K., and T outanova, K. (2018). Bert: Pre-training of deep bidirectional transformers\nfor language understanding. arXiv preprint arXiv:1810.04805 .\nDolan, W . B. and Brockett, C. (2005). Automatically constru cting a corpus of sentential paraphrases. In Proceedings\nof the Third International W orkshop on Paraphrasing (IWP20 05).\nGokaslan, A. and Cohen, V . (2019). Openwebtext corpus.\nHamborg, F ., Meuschke, N., Breitinger , C., and Gipp, B. (201 7). News-please: a generic news crawler and extractor .\nIn 15th International Symposium of Information Science (ISI 2 017), pages 218–223.\nHochreiter , S. and Schmidhuber , J. (1997). Long short-term memory . Neural computation , 9(8):1735–1780.\nHoward, J. and Ruder , S. (2018). Universal language model ﬁn e-tuning for text classiﬁcation. arXiv preprint\narXiv:1801.06146.\nJohnson, R. and Zhang, T . (2016). Convolutional neural netw orks for text categorization: Shallow word-level vs.\ndeep character-level. arXiv preprint arXiv:1609.00718 .\nJohnson, R. and Zhang, T . (2017). Deep pyramid convolutiona l neural networks for text categorization. In Proceed-\nings of the 55th Annual Meeting of the Association for Comput ational Linguistics ( V olume 1: Long Papers) , pages\n562–570.\nKrizhevsky , A., Sutskever , I., and Hinton, G. E. (2012). Ima genet classiﬁcation with deep convolutional neural\nnetworks. In Advances in neural information processing systems , pages 1097–1105.\nLai, G., Xie, Q., Liu, H., Y ang, Y ., and Hovy , E. (2017). Race: Large-scale reading comprehension dataset from\nexaminations. arXiv preprint arXiv:1704.04683 .\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P ., and So ricut, R. (2019). Albert: A lite bert for self-supervised\nlearning of language representations. arXiv preprint arXiv:1909.11942 .\n12\nA P R E P R I N T - J A N UA RY 6, 2020\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D ., Levy , O ., Lewis, M., Zettlemoyer , L., and Stoyanov , V . (2019).\nRoberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 .\nMaas, A. L., Daly , R. E., Pham, P . T ., Huang, D ., Ng, A. Y ., and P otts, C. (2011). Learning word vectors for sentiment\nanalysis. In Proceedings of the 49th annual meeting of the association fo r computational linguistics: Human\nlanguage technologies-volume 1 , pages 142–150. Association for Computational Linguistic s.\nMerity , S., Keskar , N. S., and Socher , R. (2017). Regularizi ng and optimizing lstm language models. arXiv preprint\narXiv:1708.02182.\nMerity , S., Xiong, C., Bradbury , J., and Socher , R. (2016). P ointer sentinel mixture models. arXiv preprint\narXiv:1609.07843.\nMikolov , T ., Chen, K., Corrado, G., and Dean, J. (2013). Efﬁc ient estimation of word representations in vector space.\narXiv preprint arXiv:1301.3781 .\nNagel, S. (2016). Cc-news. https://commoncrawl.org/2016/10/news-dataset-availa ble/.\nParker , R., Graff, D ., Kong, J., Chen, K., and Maeda, K. (2011 ). English gigaword ﬁfth edition, june. Linguistic Data\nConsortium, LDC2011T07 , 12.\nPennington, J., Socher , R., and Manning, C. (2014). Glove: G lobal vectors for word representation. In Proceedings\nof the 2014 conference on empirical methods in natural langu age processing (EMNLP) , pages 1532–1543.\nPeters, M. E., Neumann, M., Iyyer , M., Gardner , M., Clark, C. , Lee, K., and Zettlemoyer , L. (2018). Deep contextual-\nized word representations. arXiv preprint arXiv:1802.05365 .\nPolyak, B. T . and Juditsky , A. B. (1992). Acceleration of sto chastic approximation by averaging. SIAM Journal on\nControl and Optimization , 30(4):838–855.\nRadford, A., Narasimhan, K., Salimans, T ., and Sutskever , I . (2018). Improving language un-\nderstanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language understanding paper . pdf .\nRadford, A., Wu, J., Child, R., Luan, D ., Amodei, D ., and Suts kever , I. (2019). Language models are unsupervised\nmultitask learners. OpenAI Blog , 1(8).\nRaffel, C., Shazeer , N., Roberts, A., Lee, K., Narang, S., Ma tena, M., Zhou, Y ., Li, W ., and Liu, P . J. (2019). Exploring\nthe limits of transfer learning with a uniﬁed text-to-text t ransformer . arXiv preprint arXiv:1910.10683 .\nRajpurkar , P ., Jia, R., and Liang, P . (2018). Know what you do n ’ t know: Unanswerable questions for squad. arXiv\npreprint arXiv:1806.03822 .\nRajpurkar , P ., Zhang, J., Lopyrev , K., and Liang, P . (2016). Squad: 100,000+ questions for machine comprehension\nof text. arXiv preprint arXiv:1606.05250 .\nRuder , S., Peters, M. E., Swayamdipta, S., and W olf, T . (2019 ). T ransfer learning in natural language processing. In\nProceedings of the 2019 Conference of the North American Cha pter of the Association for Computational Linguis-\ntics: T utorials , pages 15–18, Minneapolis, Minnesota. Association for Com putational Linguistics.\nSennrich, R., Haddow , B., and Birch, A. (2015). Neural machi ne translation of rare words with subword units. arXiv\npreprint arXiv:1508.07909 .\nSocher , R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D . , Ng, A., and Potts, C. (2013). Recursive deep models\nfor semantic compositionality over a sentiment treebank. I n Proceedings of the 2013 conference on empirical\nmethods in natural language processing , pages 1631–1642.\nStrubell, E., Ganesh, A., and McCallum, A. (2019). Energy an d policy considerations for deep learning in nlp. arXiv\npreprint arXiv:1906.02243 .\nT rinh, T . H. and Le, Q. V . (2018). A simple method for commonse nse reasoning. arXiv preprint arXiv:1806.02847 .\nV aswani, A., Shazeer , N., Parmar , N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser , Ł., and Polosukhin, I. (2017).\nAttention is all you need. In Advances in neural information processing systems , pages 5998–6008.\nV oorhees, E. M. and Tice, D . M. (1999). The trec-8 question an swering track evaluation. In TREC, volume 1999,\npage 82. Citeseer .\nW an, L., Zeiler , M., Zhang, S., Le Cun, Y ., and Fergus, R. (201 3). Regularization of neural networks using dropcon-\nnect. In International conference on machine learning , pages 1058–1066.\nW ang, A., Pruksachatkun, Y ., Nangia, N., Singh, A., Michael , J., Hill, F ., Levy , O ., and Bowman, S. R. (2019). Superglue:\nA stickier benchmark for general-purpose language underst anding systems. arXiv preprint arXiv:1905.00537 .\n13\nA P R E P R I N T - J A N UA RY 6, 2020\nW ang, A., Singh, A., Michael, J., Hill, F ., Levy , O ., and Bowm an, S. R. (2018). Glue: A multi-task benchmark and\nanalysis platform for natural language understanding. arXiv preprint arXiv:1804.07461 .\nW arstadt, A., Singh, A., and Bowman, S. R. (2018). Neural net work acceptability judgments. arXiv preprint\narXiv:1805.12471.\nWilliams, A., Nangia, N., and Bowman, S. R. (2017). A broad-c overage challenge corpus for sentence understanding\nthrough inference. arXiv preprint arXiv:1704.05426 .\nY ang, Z., Dai, Z., Y ang, Y ., Carbonell, J., Salakhutdinov , R ., and Le, Q. V . (2019). Xlnet: Generalized autoregressive\npretraining for language understanding. arXiv preprint arXiv:1906.08237 .\nZellers, R., Holtzman, A., Rashkin, H., Bisk, Y ., F arhadi, A ., Roesner , F ., and Choi, Y . (2019). Defending against neura l\nfake news. arXiv preprint arXiv:1905.12616 .\nZhang, S., Zhao, H., Wu, Y ., Zhang, Z., Zhou, X., and Zhou, X. ( 2019). Dual co-matching network for multi-choice\nreading comprehension. arXiv preprint arXiv:1901.09381 .\nZhang, X., Zhao, J., and LeCun, Y . (2015). Character-level c onvolutional networks for text classiﬁcation. In Advances\nin neural information processing systems , pages 649–657.\nZhu, Y ., Kiros, R., Zemel, R., Salakhutdinov , R., Urtasun, R ., T orralba, A., and Fidler , S. (2015). Aligning books and\nmovies: T owards story-like visual explanations by watchin g movies and reading books. In Proceedings of the\nIEEE international conference on computer vision , pages 19–27.\nList of abbreviations\nA WD A veraged stochastic gradient decent weight-dropped\nbiLSTM bi-directional Long short-term memory\nBPE Byte-Pair Encoding\nCNN Convolutional neural network\nFCNN Fully connected neural network\nGRU Gated recurrent unit\nLSTM Long short-term memory\nMLM Masked Language Modelling\nNLP Natural Language Processing\nNLU Natural Language Understanding\nPLM Permutation Language Modelling\nSOTA State-of-the-art\n14",
  "topic": "Comparability",
  "concepts": [
    {
      "name": "Comparability",
      "score": 0.8247385025024414
    },
    {
      "name": "Computer science",
      "score": 0.7975695133209229
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.6315574645996094
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6122593879699707
    },
    {
      "name": "Representation (politics)",
      "score": 0.593338668346405
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5932875871658325
    },
    {
      "name": "Language model",
      "score": 0.5063718557357788
    },
    {
      "name": "Natural language processing",
      "score": 0.465038001537323
    },
    {
      "name": "Transfer of learning",
      "score": 0.4281138479709625
    },
    {
      "name": "Machine learning",
      "score": 0.41872987151145935
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}