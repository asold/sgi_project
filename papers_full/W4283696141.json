{
    "title": "Protoformer: Embedding Prototypes forÂ Transformers",
    "url": "https://openalex.org/W4283696141",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A3015010935",
            "name": "Ashkan Farhangi",
            "affiliations": [
                "University of Central Florida"
            ]
        },
        {
            "id": "https://openalex.org/A1897517138",
            "name": "Ning Sui",
            "affiliations": [
                "University of Central Florida"
            ]
        },
        {
            "id": "https://openalex.org/A1852764121",
            "name": "Nan Hua",
            "affiliations": [
                "University of Central Florida"
            ]
        },
        {
            "id": "https://openalex.org/A2167737943",
            "name": "Haiyan Bai",
            "affiliations": [
                "University of Central Florida"
            ]
        },
        {
            "id": "https://openalex.org/A2104697960",
            "name": "Arthur Huang",
            "affiliations": [
                "University of Central Florida"
            ]
        },
        {
            "id": "https://openalex.org/A2165841276",
            "name": "Zhishan Guo",
            "affiliations": [
                "University of Central Florida"
            ]
        },
        {
            "id": "https://openalex.org/A3015010935",
            "name": "Ashkan Farhangi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1897517138",
            "name": "Ning Sui",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1852764121",
            "name": "Nan Hua",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2167737943",
            "name": "Haiyan Bai",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2104697960",
            "name": "Arthur Huang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2165841276",
            "name": "Zhishan Guo",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6763692313",
        "https://openalex.org/W6702248584",
        "https://openalex.org/W3158455298",
        "https://openalex.org/W2997090102",
        "https://openalex.org/W2990019157",
        "https://openalex.org/W2964232608",
        "https://openalex.org/W2962762068",
        "https://openalex.org/W2954731415",
        "https://openalex.org/W2076723282",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3034185248",
        "https://openalex.org/W2470673105",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4301183982",
        "https://openalex.org/W2939507640",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2113459411"
    ],
    "abstract": null,
    "full_text": "Protoformer: Embedding Prototypes for\nTransformers\nAshkan Farhangiâ‹†, Ning Sui, Nan Hua, Haiyan Bai, Arthur Huang, and\nZhishan Guo\nUniversity of Central Florida, Orlando FL, USA\nashkan.farhangi@ucf.edu, zhishan.guo@ucf.edu\nAbstract. Transformers have been widely applied in text classiï¬ca-\ntion. Unfortunately, real-world data contain anomalies and noisy labels\nthat cause challenges for state-of-art Transformers. This paper proposes\nProtoformer, a novel self-learning framework for Transformers that can\nleverage problematic samples for text classiï¬cation. Protoformer fea-\ntures a selection mechanism for embedding samples that allows us to\neï¬ƒciently extract and utilize anomalies prototypes and diï¬ƒcult class\nprototypes. We demonstrated such capabilities on datasets with diverse\ntextual structures (e.g., Twitter, IMDB, ArXiv). We also applied the\nframework to several models. The results indicate that Protoformer can\nimprove current Transformers in various empirical settings.\nKeywords: Text Classiï¬cation Â· Twitter Analysis Â· Class Prototype\n1 Introduction\nFor real-world textual datasets, anomalies are known as samples that depart\nfrom the standard samples. Such anomalies tend to have scattered textual dis-\ntributions, which can cause performance drops for state-of-art Transformer mod-\nels [13]. Moreover, models that rely on supervised learning can suï¬€er from in-\ncorrect convergence when provided with noisy labeled data gathered from In-\nternet [14]. Hence, there is a need to automatically detect the anomalies and\nadjust noisy labels to make the model more robust to complex noisy datasets.\nAs human annotations can be highly time-and-cost ineï¬ƒcient, it is more\ncommon that noisy labels are gathered from the Internet. For instance, Twitter\nhas been increasingly adopted to understand human behavior [3]. However, such\ndata tend to complex and often contain noisy labels. This can make the standard\nsupervised learning objective lead to incorrect convergence [4].\nOne of the applications of this study is to classify college studentsâ€™ academic\nmajor choices based on their historical Tweets. When students follow a certain\ncollegeâ€™s oï¬ƒcial account, it might indicate that the student belongs to that\nmajor. However, there are uncertainties about the correctness of the labels.\nTherefore, the supervised modelâ€™s results can become untrustworthy.\nâ‹† Corresponding author.\narXiv:2206.12710v1  [cs.CL]  25 Jun 2022\n2 A. Farhangi et al.\nDifficult Class Prototypes\nAnomaly Prototypes\nConventional \nDecision Boundary\nAnomaly Prototypes\nClass A Sample\nClass B Sample\nConventional \nClass Prototypes Conventional \nClass PrototypesDifficult Class \nPrototypes\nFig. 1.Distribution of embeddings for real world data samples is often scatterd. Al-\nthough conventional class prototypes are easier to select, diï¬ƒcult class prototypes and\nanomaly prototypes require a more careful approach in selection and play a critical\nrole in improving the decision boundary.\nThere are some prior works on prototype embeddings. CleanNet [7] proposes\nproviding extra supervision for the training. Subsequently, SMP [5] proposes us-\ning multiple prototypes to capture embeddings with high density without extra\nhuman supervision. However, both approaches do not provide a solution for\ntroublesome embeddings that are scattered and are often minorities, as shown\nin Figure 1. To alleviate this issue, we select prototypes through their contextual\nembeddings in a way to not only cover the diï¬ƒcult-to-classify samples but also\nrepresent minority samples of the dataset (i.e., anomalies).\nWe propose Protoformer framework that selects and leverages multiple em-\nbedding prototypes to enable Transformerâ€™s specialization ability to classify\nnoisy labeled data populated with anomalies. Speciï¬cally, we improve the gen-\neralization ability of Transformers for problematic samples of a class through\ndiï¬ƒcult class prototypes and their specialization ability for minority samples\nof a class through anomaly prototypes . We show that the representations of\nboth prototypes are necessary to improve the modelâ€™s performance. Protoformer\nleverages these prototypes in a self-learning procedure to further improve the\nrobustness of textual classiï¬cation. To our best knowledge, this is the ï¬rst study\nthat extracts and leverages anomaly prototypes for Transformers.\nIn summary, the contributions are threefold:\nâ€¢We propose a novel framework that learns to leverage harder to classify and\nanomaly samples. This acts as a solution for classifying datasets with complex\nsamples crawled from the Internet.\nâ€¢The framework contains a label adjustment procedure and thus is robust to\nnoise. This makes the framework suitable for noisy Internet data and can be\nused to promote a more robust Transformer model. Leveraging the similarity in\nthe embedding space and a ranking metric, we can identify questionable labels\nand provide a certain level of adjustment. This mitigates the potential negative\nimpact on the training.\nProtoformer: Embedding Prototypes for Transformers 3\nâ€¢We evaluate the framework based on multiple datasets with both clean and\nnoisy labels. Results show that our model improves the testing accuracy from\n95.7% to 96.8% on the IMDB movie review dataset. For a self-gathered Twitter\ndataset with noisier labels, the classiï¬cation accuracy improved with a greater\nmargin (from 56.7% to 81.3%).\n2 Problem Formulation\nGiven a sample text as xi, X = {x1,x2,Â·Â·Â· ,xN }represents all the N samples of\nthe dataset, while Ë†Y = {Ë†y1,Ë†y2,Â·Â·Â· ,Ë†yN }indicates the corresponding noisy labels\nfrom the Internet. The noisy label Ë† yi âˆˆ{0,1}Â¯c is a binary vector format with\nonly one non-zero element, indicating the class label of xi, where Â¯c is the total\nnumber of classes. A Transformer model FW can be used as a classiï¬cation\nmodel to produce an estimated label FW (xi) âˆˆ [0,1]Â¯c, where W represents\nthe parameters. The optimization strategy is based on the cross-entropy loss\nfunction:\nL(FW (xi),Ëœyi) = âˆ’\nÂ¯câˆ‘\nj=1\nËœyi,j log (FW (xi)j) , (1)\nIn addition, labels from the internet are often noisy. Hence, as detailed in\nSection 3.4, the labels can be adjusted according to the similarities of the class\nprototypes, resulting in adjusted labels Ëœyi âˆˆ[0,1]Â¯câ€”it is a probability distribu-\ntion, and thus âˆ‘Â¯c\nj=1 Ëœyi,j = 1. Even when we have suï¬ƒcient conï¬dence in the\noriginal labels, we can use it as a complementary supervision.\nSpeciï¬cally, for each batch with m samples, we would pursue the following\noptimization problem:\nWâˆ—= argminW\n1\nm\nmâˆ‘\ni=1\nL(FW (xi),Ëœyi) (2)\n3 Design of Protoformer\nThis section provides the details of Protoformer. Speciï¬cally, we describe a pro-\ncedure for extracting the diï¬ƒcult class prototypes (Section 3.1). Subsequently,\nwe describe a procedure for extracting anomaly prototypes (Section 3.2). Both\ntypes of prototypes are then used in a multi-objective self-learning training pro-\ncess that optimizes the network parameters for robust text classiï¬cation (Section\n3.3). In order to handle noisy labeled data, we adjust the noisy labels through\na label adjustment procedure that uses the prototype similarities (Section 3.4).\n3.1 Diï¬ƒcult Class Prototypes\nDiï¬ƒcult class prototypes act as the representatives for the problematic samples\nof the dataset. For example, Figure 2 showcases the ï¬ne-tuned embeddings of\n4 A. Farhangi et al.\nClass A Correct\nClass A Wrong\nClass B Correct\nClass B Wrong\nDistribution of validation set embeddings for IMDB dataset\n0.0 0.5 1.0\nHighest Logit\n0.0\n0.2\n0.4\n0.6Density\nCorrect\nWrong\nFig. 2. Left:distribution of the embedding for IMDB dataset. Presence of anomalies\naand problematic samples cause misclassiï¬cation. Right: Distribution of the highest\noutput logits (scaled 0-1) for the same model. The higher values of the largest logits\ncan represent the conï¬dence of the networkâ€™s classiï¬cation.\na benchmark dataset gathered from the Internet (i.e., IMDB). Although the\nmajority of samples of each class are located closely together, there are anomaly\nsamples that are scattered and often far from the majority. Unfortunately, these\nharder-to-classify samples are not the target focus of the state-of-art models\nin text classiï¬cation. Moreover, traditional clustering methods (e.g., K-means)\nare not designed to capture or cluster such samples that are scattered and\ndistributed throughout the embedding space.\nIntuitively, these problematic samples can cause the greatest error. For in-\nstance, Figure 2 also shows the classiï¬cation error of the ï¬ne-tuned BERT [2]\nmodel where the majority of the classiï¬cation error stems from harder-to-classify\nsamples (over 51%). Such error arises when the highest classiï¬cation logit val-\nues are still low and in between classes, which indicates the indecisiveness of the\nTransformer. Following [5], we deï¬ne the similarity of the extracted embeddings\nthrough pairwise similarity score (i.e., cosine distance) of any two inputs xi\nand xj as:\nsij = e (xi)T Â·e (xj)\nâˆ¥e (xi)âˆ¥2 âˆ¥e (xj)âˆ¥2\n, (3)\nwhere e(x) is the embedding vector of sample x, extracted from the ï¬rst layer\nof the Transformer1.\nTo determine the closeness of embeddings, we also deï¬ne the proximity\nmetric p for each embedding as:\npi =\nmâˆ‘\nj=1\nsign(sij âˆ’sc) , (4)\n1 For large-scale datasets, one can randomly choose a limited number (e.g., q) of samples per\nclass to develop a triangular similarity matrix SqÃ—q which can enhance the computational\neï¬ƒciency.\nProtoformer: Embedding Prototypes for Transformers 5\nSimilarity\nProximity\nConfidence\nSamples\nAnomaly Prototype\nDifficult Class Prototype\n2000\n 1000\n 0 1000 2000\nProximity\n0.2\n0.3\n0.4\n0.5\n0.6Similarity\nSamples\nAnomaly Prototype\nDifficult Class Prototype\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nConfidence\nFig. 3.Selected embedding prototypes of a single class of Twitter dataset. Diï¬ƒcult\nclass prototypes have higher proximity, while anomaly prototypes suï¬€er from low prox-\nimity due to their complex nature.\nwhere sign(x) is a sign function2 and sc is an arbitrary value from the similarity\nmatrix (default as 20-percentile). Intuitively, a higher proximity indicates that\nthe textual embeddings have more similar embeddings around them and are\nâ€˜closerâ€™ to every other sample in the embedding space.\nFollwing [12], problematic samples cause low conï¬dence in output logits of\nthe model. Hence, we deï¬ne the conï¬dence metric c as:\nci = |\nlargest logit\nî´— î´šî´™ î´˜\nmaxË†c1 FW (xi)Ë†c1 âˆ’\nsecond largest logit\nî´— î´šî´™ î´˜\nmaxË†c2 FW (xi)Ë†c2 | (5)\nwhere logits are scaled (0-1 range) and are taken from the output before the\nsoftmax layer after a preliminary training stage. Intuitively, when the conï¬dence\nis low (near zero), the model indecisivess is the highest.\nWe can now represent the embeddings in a three-dimensional space as shown\nin Figure 3 (similarity-proximity-conï¬dence). The diï¬ƒcult class prototype se-\nlection follows three general rules: (i) it should prioritize low conï¬dence samples\n(i) it should be â€˜farâ€™ enough from existing prototypes (if any), (iii) it should\nhave high â€˜proximityâ€™ when possible. To this end, the ï¬rst prototype with the\nlowest conï¬dence, highest proximity, and highest similarity is chosen. Then, the\nsubsequent diï¬ƒcult class prototypes are chosen in a logsparse [8] manner for\nevery round with an exponential selection step of sample size (log 2(N)). Note\nthat the samples are selected based on the low conï¬dence, then high proxim-\nity but should have the lowest average similarity with the previously selected\nprototypes to be distinctive from each other. This strategy ensures us that the\ndiï¬ƒcult class prototype are well represent problematic samples of the dataset.\n2 sign(x) = 1 for x >0, sign(x) = 0 for x = 0, and sign(x) = âˆ’1 otherwise.\n6 A. Farhangi et al.\nNext, at a certain round (t), a prototype set Xc =\n{\nx(1)\nc ,..., x(t)\nc\n}\nis already\nformed for the c-th class, c = 1,..., Â¯c. Given any text xi, we can calculate the\naverage cosine similarity between sample xi and the selected prototype embed-\ndings as:\nsc\ni,(c) = 1\nt\ntâˆ‘\nj=1\nsi,c(j) , (6)\nwhere sc\ni,(c) is the average similarity of diï¬ƒcult class embeddings in the jth\niteration for the c-th class. This average similarity can then be used as a com-\nplementary supervsion:\nzc\ni = argmaxc{sc\ni,(c)|c= 1,..., Â¯c}. (7)\nAs shown in Figure 3, diï¬ƒcult prototypes are chosen with low conï¬dence\nlevels, where they have the least similarity among the previously selected pro-\ntotypes. During this process, we ensure that the subsequent prototypes stay far\nenough from existing prototypes so that there are limited redundant represen-\ntations of the similar samples.\n3.2 Anomaly Prototypes\nAnomaly prototypes are the selected sample prototypes that represent the scat-\ntered and shattered minority samples of a dataset. Such samples are often harder\nto detect and tend to deviate from normal samples.\nGiven that the remaining classiï¬cation error can be caused by such anoma-\nlies, itâ€™s important to not only capture such anomalies robustly but also leverage\nthem for the optimization objectives of Transformers.\nSo far, diï¬ƒcult class prototypes can cover the problematic samples as they\nare detected by having high proximity and similarity. However, a certain portion\nof prototypes may be located â€˜farâ€™ from the diï¬ƒcult class prototypes and often\nrepresent the minority members of a class, as indicated by the red dots in the\nin Figure 3. Such prototypes represent the minority of samples as they have a\nlower density.\nThe prototype with the least proximity pmin is selected in the ï¬rst round.\nThis ensures us that the elected prototype is representative of the minority\nsamples. We then select the subsequent ones in the same logsparse manner as\nbefore, ensuring that the prototypes have the least similarity. The similarity\nscore is calculated in a similar manner to Equation (6) while including the\nanomaly prototypes in the summation.\nFigure 2 also illustrates the process, where gray dots represent all sample\nembeddings, and red dots indicate the embeddings of selected anomaly proto-\ntypes.\n3.3 Multi-Objective Self-Learning\nTransformers used in text classiï¬cation often rely on a single source of super-\nvision which is the given labels. However, such design choice limits the Trans-\nProtoformer: Embedding Prototypes for Transformers 7\nğ¿(ğ¹ğ‘Š(xğ‘–),ğ‘§ğ‘–\nğ‘)\nClass Prototypes\nSimilarity\n+Anomaly Prototypes\nSimilarity\nFC +\nSoftmax\nğ¿ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™\n1âˆ’(Î±+ Î²)\nÎ²\nxğ‘– Î±Embedding Layer\nğ¹ğ‘Š\nTransformer\nModel\nDropout ğ¿ ğ¹ğ‘Š xğ‘– , à·œğ‘¦ğ‘–\nğ¿(ğ¹ğ‘Š(xğ‘–),ğ‘§ğ‘–\nğ‘)\nFig. 4.Protoformer leverages the embedding space to derive the diï¬ƒcult class and\nanomaly prototypes. The network is trained jointly on Transformer and similarity of\nembedding prototypes. The total loss is dependent on the Î± and Î² values which are\nestimated in the training phase.\nformerâ€™s ability to perform well when the datasets are noisy labeled. Moreover,\nanomaly samples appear less in training compared to samples with high sim-\nilarity. Note that majority of self-learning objectives for Transformers are to\nprovide the greatest level of classiï¬cation accuracy for all samples regardless of\nwhether they are in the majority or minority. Intuitively, such a self-learning\nobjective does not guarantee that the model suits well for minority classes due\nto their lower occurrence. In order to incorporate our prototypes during the\ntraining and test stage, we introduce a multi-objective self-learning mechanism\nto Protoformer.\nAs shown in Figure 4, the similarities of embedding prototypes are used as\nself-supervision to train the Protoformer FW after its ï¬ne-tuning state. The\nself-supervision is provided by the class prototype as below:\nLproto = 1\nm\nmâˆ‘\ni=1\n(Î±Â·L(FW (xi),zc\ni ) + Î²Â·L(FW (xi),za\ni )), (8)\nwhere the weight factors Î±,Î² âˆˆ[0,1) and Î±+Î² <1 indicate the concentration of\nTransformer on the similarities of self-supervision of diï¬ƒcult class prototypes zc\ni\nand anomaly prototypes za\ni . Hence, the overall loss is calculated by minimizing\nthe classiï¬cation loss based on three components:\nLtotal = (1 âˆ’(Î±+ Î²)) Â· 1\nm\nmâˆ‘\ni=1\n(L(FW (xi),Ë†yi) + Lproto, (9)\nTo this end, when the networkâ€™s predictions are in between classes, the net-\nwork can improve its training by the self-supervision provided by the similarity\nof diï¬ƒcult class prototype zc\ni and anomaly prototype za\ni . Hence, we continue the\ntraining procedure iteratively until convergence: W(t+1) â†W(t) âˆ’Î¾âˆ‡(Ltotal) ,\nwhere the gradient descent vector âˆ‡(Ltotal) holds the partial derivatives of\nweights and biases of the total loss function, and Î¾ is the learning rate. We use\na fully connected layer over the ï¬nal hidden state corresponding to the output\ntoken of the Transformer (i.e., [CLS] token). The softmax activation function\nis then applied to the hidden layer to provide classiï¬cation. It is important to\nnote that this procedure can also be implemented solely during the test stage,\n8 A. Farhangi et al.\nwhich can make the calculation timing complexity of Protoformer similar to the\nï¬ne-tuning process.\n3.4 Noisy Labels Enhancement\nTo mitigate the eï¬€ect of noisy labels throughout the datasets, we are enhancing\nthe labels through the similarities of embedding prototypes. This allows Proto-\nformer to be robust toward datasets when the labels are not fully trustworthy.\nConsequently, when the labels are wrong, the training procedure of Transformers\nprovides suboptimal weights, which makes the classiï¬cation results untrustwor-\nthy.\nSpeciï¬cally, we can obtain the adjusted label of the a noisy labeled sample\nthrough maximum similarity to the diï¬ƒcult class prototype:\nËœyi = argmaxc{si,(c)|c= 1,..., Â¯c}, (10)\nwhere si,(c) is the cosine similarity deï¬ned in Equation (6) and the enhanced\nlabels Ëœy can be used as a replacement for the noisy labels. Thus, the overall loss\nis calculated in a similar manner as Equation (9), while we are replacing the\noriginal noisy labels with the adjusted label.\n4 Experiments\nIn this section, we provide descriptions for the datasets. We also describe the ex-\nperimental settings and evaluation results. Lastly, we provide an analysis section\nthat further discusses the eï¬€ectiveness of Protoformer components.\n4.1 Benchmark Datasets & Baselines\nWe have experimented with three challenging real-world datasets 3. The brief\ndiscussion for each dataset is as follows:\nTable 1.Summary statistics of the evaluation dataset.\nDataset Twitter-Uni IMDb Arxiv-10\n# Examples 25,000 25,000 100,000\n# Train 20,000 20,000 80,000\n# Validation 2,500 2,500 10,000\n# Test 2,500 2,500 5,000\n# Classes 8 2 10\nTwitter-Uni3. We crawled over 12 million historical Tweets of 25,000 Twit-\nter proï¬les from 8 U.S. college followers. As an example, the college of engineer-\ning holds near 3000 followers, which are labeled as engineering. Note that most\nexisting benchmark Twitter datasets fail to hold high-quality labels that are\n3 Self-gathered datasets are accessible at https://github.com/ashfarhangi/Protoformer\nProtoformer: Embedding Prototypes for Transformers 9\nprovided by the original Twitter users. To alleviate this issue, we extracted a\nset of students that stated their major in their Twitter bio. This set can serve as\nground truth of the clean labels. We made this challenging new dataset available\nonline, which can be used for future text classiï¬cation or noisy label correction\nstudies.\nArXiv-103. We also crawled the abstracts and titles of 100 thousand ArXiv\nscientiï¬c papers on ten research categories that include subcategories of com-\nputer science, physics, and math. The dataset is downsampled to contain exactly\n10 thousand samples per category.\nIMDB. The third dataset is the benchmark IMDb movie reviews [10]. The\ndataset is widely used as the sentiment classiï¬cation task. It contains 25 thou-\nsand samples per sentiment (positive or negative). Both IMDb and ArXiv-10\ndatasets are originally labeled by the authors. It is however good to note that\nthe labels are still susceptible to noisy labels.\nThe baseline methods for comparison include:\nâ€“ SVM [11], supervised learning with a linear separator to maximize the mar-\ngin between classes, with the ï¬ne-tuned embeddings derived from the Trans-\nformers.\nâ€“ HAN [6], a hierarchical attention network for textual classiï¬cation with word\nand sentence-level attention mechanisms.\nâ€“ DocBERT [1], a document Transformer model with an LSTM architecture\nrather than a fully connected layer.\nâ€“ RoBERTa [9], a Transformer with an improved pretraining procedure. Specif-\nically, showing improvement by removing the next sentence prediction pre-\ntraining objective.\nTable 2.Hyperparameters of the Protoformer used for each dataset.\nParameter Twitter-Uni IMDb Arxiv-10\nBatch size 32 64 32\nLearning rate 5 Ã— 10âˆ’5 3 Ã— 10âˆ’5 5 Ã— 10âˆ’5\nWeight decay 5 Ã— 10âˆ’5 1 Ã— 10âˆ’5 1 Ã— 10âˆ’4\nPreliminary training epochs 5 3 2\nFine-tuning epochs 20 10 10\nTraining time 1:49h 1:32h 1:45h\nTransformer DistilBERT BERT RoBERTa\n4.2 Experimental Settings\nTo showcase the generalization ability of our framework, we selected a unique\nTransformer for each dataset (Table 2). The hyperparameters are based on the\nhighest Macro-F1 score obtained on the validation set for all models (following\nthe standard 80-10-10 split). We used a grid search approach to explore the\n10 A. Farhangi et al.\nTable 3.Evaluation of the Protoformer and baseline methods.\nTwitter IMDb ArXiv\nModel Ma-F1 Recall Acc Ma-F1 Recall Acc Ma-F1 Recall Acc\nSVM [11] 0.384 0.361 0.391 0.744 0.733 0.748 0.691 0.654 0.708\nHAN [15] 0.412 0.392 0.425 0.894 0.882 0.896 0.732 0.696 0.746\nDocBERT [1] 0.521 0.506 0.534 0.932 0.921 0.936 0.752 0.727 0.764\nRoBERTa [9] 0.555 0.531 0.567 0.952 0.941 0.957 0.769 0.732 0.779\nProtoformer 0.802 0.784 0.813 0.964 0.952 0.9680.784 0.744 0.794\nhyperparameters: size of fully connected layer HD âˆˆ{256,512,768,1024}and\ndropout Î´ âˆˆ{0.0,0.1,Â·Â·Â· ,0.9}. The experiments are conducted using PyTorch\non a cloud workstation using Nvidia Tesla A100 GPU.\n4.3 Experimental Results\nFor a less noisy labeled datasets such as IMDB and Arvix, the evaluated meth-\nods performed comparatively. Note that the majority of the classiï¬cation error\nappears when the network does not show conï¬dence in its classiï¬cation, as was\npreviously shown for the IMDB dataset in Figure 5. The Protoformer is also\nable to provide a competitive accuracy for cleaner datasets such as IMDb and\nArXiv-10. Among the baselines, the performance of RoBERTa [9] is favorable\ncompared to others. This is partly due to the diï¬€erent pretraining objectives\nfrom DocBERT. As shown in Table 3, Protoformer resulted in the highest mar-\ngin of accuracy for a noisy dataset, improving the Macro-F1 score from 55.5%\nto 80.2% for the Twitter-Uni dataset. We observed that this dataset provides\nthe greatest diï¬ƒculties for baseline methods where the models often misclassify\nproblematic samples. To this end, we report a detailed accuracy breakdown for\nthe Twitter dataset in Figure 5. The ï¬ne-tuning process for Transformers such\nas DocBERT, RoBERTa results in suboptimal classiï¬cation. Leveraging the se-\nlected prototypes, Protoformer was able to improve its classiï¬cation accuracy on\nthe harder and more complex samples (e.g., management students that are sim-\nilar to other classes). To this end, the ï¬ne-tuning process alone does not result\nin adequate accuracy due to the noise of the dataset. The combination of both\nembedding prototypes allows the Transformer to have a solution for anomalies\nand problematic samples of the dataset and further improves its generalization\nability through diï¬ƒcult class prototypes.\n4.4 Analysis\nIn this section, we provide an extensive analysis of the performance of Proto-\nformer, as well as the role of each type of prototype on the overall performance.\nHence, we limited the number of prototypes per class for the Twitter dataset\nand reported the changes. The results in Figure 5 show that a single prototype is\nnot suï¬ƒcient to provide competitive accuracy even with the help of a ï¬ne-tuned\nProtoformer: Embedding Prototypes for Transformers 11\nBusiness Education Engineering Sciences Medicine HospitalityManagment Arts\n0.0\n0.2\n0.4\n0.6\n0.8\nAccuracy increase for Twitter dataset\nAnomaly Prototype\nClass Prototype\nTransformer\nFig. 5. Left:Accuracy increase from the initial (light blue), class protoype (blue)\nand class anomalies (red), for Twitter dataset. Right: Inï¬‚uence of anomaly labeling of\nhurricanes for Collier county gross hotel sales revenue. Middle: Number of anomaly\nprototypes (AP) and diï¬ƒcult class prototypes (CP) per class for Twitter dataset.\nHigher number of prototypes resulted in marginal improvement while the combination\nof both category of prototypes gives us the optimal accuracy. Right: Testing accuracy\nwith respect to the weight factors ( Î± and Î²) ranging from 0 to 1.\nTransformer. However, as the number of prototypes increased, we observed im-\nprovements in the accuracy of the Protoformer. The prototype selection proce-\ndure previously discussed ensures that there are multiple prototypes for every\nproximity metric, and the calculation of them is computationally expensive even\nfor the large-scale dataset. Moreover, the weight factors are reported separately\nto showcase the eï¬€ect of their self-supervision for the Twitter dataset. The re-\nsults show that relying on the noisy labels ( Î±and Î² = 0) during training would\nbe suboptimal and perform poorly on conï¬rmed test data. Moreover, the accu-\nracy would be optimal when weight factors sum to 0 .5 (i.e., Î±=0.2, Î²= 0.3).\n5 Conclusion\nIn this work, we developed a novel Transformer framework, Protoformer, that\nleverages the embedding prototypes of the dataset to enhance its generaliza-\ntion and specialization abilities. It also includes a procedure for handling noisy\nlabels. Various experiments are conducted to demonstrate the eï¬€ectiveness of\nProtoformer over state-of-art topic and sentiment classiï¬cation methods. For fu-\nture work, we are interested in applying Protoformer for the image recognition\ntasks. We also like to explore the use of Protoformer on spherical and hyperbolic\nembedding space.\nAcknowledgement\nOur work has been supported by the US National Science Foundation under\ngrants No. 2028481, 1937833, and 1850851.\nReferences\n1. Adhikari, A., Ram, A., Tang, R., Lin, J.: Docbert: Bert for document classiï¬cation.\narXiv preprint arXiv:1904.08398 (2019)\n12 A. Farhangi et al.\n2. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint\narXiv:1810.04805 (2018)\n3. Fiok, K., Karwowski, W., Gutierrez, E., Saeidi, M., Aljuaid, A.M., Davahli, M.R.,\nTaiar, R., Marek, T., Sawyer, B.D.: A study of the eï¬€ects of the covid-19 pandemic\non the experience of back pain reported on twitterÂ® in the united states: A natural\nlanguage processing approach. International Journal of Environmental Research\nand Public Health 18(9), 4543 (2021)\n4. Garg, S., Vu, T., Moschitti, A.: Tanda: Transfer and adapt pre-trained transformer\nmodels for answer sentence selection. In: Proceedings of the AAAI Conference on\nArtiï¬cial Intelligence. vol. 34, pp. 7780â€“7788 (2020)\n5. Han, J., Luo, P., Wang, X.: Deep self-learning from noisy labels. In: Proceedings\nof the IEEE/CVF International Conference on Computer Vision. pp. 5138â€“5147\n(2019)\n6. Krishnan, R., Shalit, U., Sontag, D.: Structured inference networks for nonlinear\nstate space models. In: Proceedings of the AAAI Conference on Artiï¬cial Intelli-\ngence. vol. 31 (2017)\n7. Lee, K.H., He, X., Zhang, L., Yang, L.: Cleannet: Transfer learning for scalable\nimage classiï¬er training with label noise. In: Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition. pp. 5447â€“5456 (2018)\n8. Li, S., Jin, X., Xuan, Y., Zhou, X., Chen, W., Wang, Y.X., Yan, X.: Enhancing\nthe locality and breaking the memory bottleneck of transformer on time series\nforecasting. Advances in Neural Information Processing Systems 32 (2019)\n9. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,\nZettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692 (2019)\n10. Maas, A., Daly, R.E., Pham, P.T., Huang, D., Ng, A.Y., Potts, C.: Learning word\nvectors for sentiment analysis. In: Proceedings of the 49th annual meeting of the\nassociation for computational linguistics: Human language technologies. pp. 142â€“\n150 (2011)\n11. Meyer, D., Leisch, F., Hornik, K.: The support vector machine under test. Neuro-\ncomputing 55(1-2), 169â€“186 (2003)\n12. Pleiss, G., Zhang, T., Elenberg, E., Weinberger, K.Q.: Identifying mislabeled data\nusing the area under the margin ranking. Advances in Neural Information Pro-\ncessing Systems 33, 17044â€“17056 (2020)\n13. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\nKaiser,  L., Polosukhin, I.: Attention is all you need. Advances in neural infor-\nmation processing systems 30 (2017)\n14. Wei, H., Feng, L., Chen, X., An, B.: Combating noisy labels by agreement: A\njoint training method with co-regularization. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. pp. 13726â€“13735 (2020)\n15. Yang, Z., Yang, D., Dyer, C., He, X., Smola, A., Hovy, E.: Hierarchical attention\nnetworks for document classiï¬cation. In: Proceedings of the 2016 conference of the\nNorth American chapter of the association for computational linguistics: human\nlanguage technologies. pp. 1480â€“1489 (2016)"
}