{
    "title": "HeadPosr: End-to-end Trainable Head Pose Estimation using Transformer Encoders",
    "url": "https://openalex.org/W4210726167",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2682597091",
            "name": "Naina Dhingra",
            "affiliations": [
                "ETH Zurich"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6788477181",
        "https://openalex.org/W2807744618",
        "https://openalex.org/W6790555728",
        "https://openalex.org/W2737047298",
        "https://openalex.org/W6788135285",
        "https://openalex.org/W2957744218",
        "https://openalex.org/W1946919140",
        "https://openalex.org/W6790531269",
        "https://openalex.org/W1643185111",
        "https://openalex.org/W2998505840",
        "https://openalex.org/W2737644856",
        "https://openalex.org/W6782980240",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2923153064",
        "https://openalex.org/W3108516375",
        "https://openalex.org/W2087681821",
        "https://openalex.org/W2012885984",
        "https://openalex.org/W2589255576",
        "https://openalex.org/W6791517108",
        "https://openalex.org/W3119574651",
        "https://openalex.org/W6762381305",
        "https://openalex.org/W1990937109",
        "https://openalex.org/W1976948919",
        "https://openalex.org/W2982705218",
        "https://openalex.org/W3035385851",
        "https://openalex.org/W2949662773",
        "https://openalex.org/W2047875689",
        "https://openalex.org/W1963599662",
        "https://openalex.org/W6662335928",
        "https://openalex.org/W2088028039",
        "https://openalex.org/W6640455264",
        "https://openalex.org/W2215711223",
        "https://openalex.org/W2082308025",
        "https://openalex.org/W2129210471",
        "https://openalex.org/W2963377935",
        "https://openalex.org/W2341528187",
        "https://openalex.org/W1896788142",
        "https://openalex.org/W2964014798",
        "https://openalex.org/W2058961190",
        "https://openalex.org/W2963644257",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W3104300620",
        "https://openalex.org/W3028400540",
        "https://openalex.org/W2136000821",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2945311944",
        "https://openalex.org/W3117707723",
        "https://openalex.org/W3099461062",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W1935685005",
        "https://openalex.org/W2047508432",
        "https://openalex.org/W3140576409",
        "https://openalex.org/W3130878351",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4287324101",
        "https://openalex.org/W3104792420",
        "https://openalex.org/W4287259027",
        "https://openalex.org/W3101998545"
    ],
    "abstract": "In this paper, HeadPosr is proposed to predict the head poses using a single\\nRGB image. \\\\textit{HeadPosr} uses a novel architecture which includes a\\ntransformer encoder. In concrete, it consists of: (1) backbone; (2) connector;\\n(3) transformer encoder; (4) prediction head. The significance of using a\\ntransformer encoder for HPE is studied. An extensive ablation study is\\nperformed on varying the (1) number of encoders; (2) number of heads; (3)\\ndifferent position embeddings; (4) different activations; (5) input channel\\nsize, in a transformer used in HeadPosr. Further studies on using: (1)\\ndifferent backbones, (2) using different learning rates are also shown. The\\nelaborated experiments and ablations studies are conducted using three\\ndifferent open-source widely used datasets for HPE, i.e., 300W-LP, AFLW2000,\\nand BIWI datasets. Experiments illustrate that \\\\textit{HeadPosr} outperforms\\nall the state-of-art methods including both the landmark-free and the others\\nbased on using landmark or depth estimation on the AFLW2000 dataset and BIWI\\ndatasets when trained with 300W-LP. It also outperforms when averaging the\\nresults from the compared datasets, hence setting a benchmark for the problem\\nof HPE, also demonstrating the effectiveness of using transformers over the\\nstate-of-the-art.\\n",
    "full_text": "HeadPosr: End-to-end Trainable Head Pose Estimation using\nTransformer Encoders\nNaina Dhingra\nETH Zurich\nndhingra@ethz.ch\nAbstract— In this paper, HeadPosr is proposed to predict the\nhead poses using a single RGB image. HeadPosr uses a novel\narchitecture which includes a transformer encoder. In concrete,\nit consists of: (1) backbone; (2) connector; (3) transformer\nencoder; (4) prediction head. The signiﬁcance of using a\ntransformer encoder for HPE is studied. An extensive ablation\nstudy is performed on varying the (1) number of encoders; (2)\nnumber of heads; (3) different position embeddings; (4) differ-\nent activations; (5) input channel size, in a transformer used in\nHeadPosr. Further studies on using: (1) different backbones, (2)\nusing different learning rates are also shown. The elaborated\nexperiments and ablations studies are conducted using three\ndifferent open-source widely used datasets for HPE, i.e., 300W-\nLP, AFLW2000, and BIWI datasets. Experiments illustrate that\nHeadPosr outperforms all the state-of-art methods including\nboth the landmark-free and the others based on using landmark\nor depth estimation on the AFLW2000 dataset and BIWI\ndatasets when trained with 300W-LP. It also outperforms when\naveraging the results from the compared datasets, hence setting\na benchmark for the problem of HPE, also demonstrating the\neffectiveness of using transformers over the state-of-the-art.\nI. I NTRODUCTION\nFacial expression detection, tracking, and modeling is an\nactively researched topic over the past years [2], [18]. There\nare several sub-problems of facial expression detection such\nas face recognition, age estimation, identiﬁcation, HPE, etc.\nwhich are being analysed extensively by computer vision\ncommunity. In this paper, we focus on HPE, which is an\nelemental step in computer vision used for identifying faces,\nlocalizing eye gaze, or approximating social interaction. It\nalso aids in providing information in expression recognition,\nattention analysis, identity detection [31], etc. It has several\ndirect applications in various ﬁelds such as human-robot\ninteraction, helping blind and visually impaired people in\nnon-verbal communication [5], assistance in driver behavior\nanalysis, autonomous driving, etc.\nHPE can be solved using facial keypoint detection. Some\napproaches [2], [44], [18] that use keypoints based detection\nillustrate robustness and ﬂexibility in problems dealing with\nextreme pose changes and with varied visual scene occlusion\n(by matching correspondences in the images to their 3D\nface models). However, they face major challenges in the\nsituations where it is difﬁcult to detect landmarks. Moreover,\nthese keypoints in literature are used for facial expression\nanalysis. The head poses are generally a side result of the\nmain task [18], [24] and are not very accurate on their\nown. Another solution is a holistic approach using image\nFig. 1. Sample image sequence taken from BIWI dataset. The corre-\nsponding head pose illustration is shown in the second row using ground\ntruth head pose predictions. Different colored lines depict: Blue, shows the\nfacing direction of the subject; Green shows downward direction; Red points\ntowards the side.\nintensities to estimate the 3D head pose. This approach is\ndemonstrated to perform better than the keypoints based one\nin [25]. The keypoints based approach has several drawbacks:\n(1) it requires good location of the keypoints, otherwise, the\npose can not be estimated accurately; (2) the performance\nrelated to the estimation of head pose largely depends on\nthe aspects of the 3D head models. Furthermore, the process\nof deforming the models is expensive and cumbersome as it\nhas to be adapted to each participant. In some approaches,\nRGB-D images are used which provide good accuracy but\nhave several drawbacks [25]. Data rates for RGB-D cameras\nare more than for RGB cameras which in turn require higher\nstorage abilities and data transfer rates. Power requirements\nare also higher for RGB-D cameras. RGB-D cameras use\nactive sensing which makes them difﬁcult to be used in\nuncontrolled environments, especially outdoors. Due to these\nreasons, RGB-D based cameras cannot be used in applica-\ntions such as mobile based devices, autonomous navigation,\nsecurity purposes, computer graphics, etc. Consequently, in\nthis work, a holistic based approach is used for estimating the\nhead pose in RGB images. The image sequences with their\nground truth head poses from BIWI dataset are illustrated in\nFigure 1.\nInspired from the performance of transformer network [32]978-1-6654-3176-7/21/$31.00 ©2021 IEEE\narXiv:2202.03548v1  [cs.CV]  7 Feb 2022\nin natural language processing ﬁeld, many researchers in\ncomputer vision ﬁeld [8] are exploring its application in\nimage classiﬁcation, image segmentation, image captioning,\nimage registration, etc. Considering its positives for these\napplications, transformers are explored in this work for\nHPE. To our best knowledge, this is the ﬁrst work to use\ntransformer encoder to capture the spatial embedding from\nthe head pose features for HPE.\nThe main contributions of this paper are as follows:\n• Proposing a method to predict head pose from the\nRGB images. For this, a novel architecture is developed\nbased on a transformer encoder. It consists of four sub-\nparts: (1) backbone: to extract spatial image features;\n(2) connector: to downsample the output features from\nthe backbone and then to further reshape them suitable\ninput for a transformer encoder in the form of sequential\ndata; (4) prediction head: to manipulate the output from\nthe transformer encoder to predict the three head pose\nangles, i.e., yaw, pitch, roll.\n• Comparing proposed HeadPosr extensively with the\nstate-of-the-art methods.\n• Performed extensive ablation study for understanding\nthe factors in estimating the head poses using HeadPosr.\n• Qualitatively illustrating and comparing the results of\nthe proposed approach to the ground truth, and a state-\nof-the-art.\n• Outperforming published methods which use a single\nimage for estimation of head poses when trained us-\ning 300W-LP and tested using combined AFLW2000\nand BIWI datasets. It also performs better than all\napproaches when taken average of all the results. We\nalso expect that our work will give further advances in\nhead pose estimation using transformers.\nThis paper is structured as follows. Related work is\ndiscussed in section II. The proposed transformer based\nnetwork for HPE is described in section III. The experiments\nare elaborately illustrated in section IV, results are stated in\nsection V, ablation study is discussed in section VI. Finally,\nsection VII summarizes the presented work and prospects of\nfuture work.\nII. R ELATED WORK\nA. Head Pose Estimation\nHPE can be grouped broadly in two categorizes: (1) the\nﬁrst category deals with detecting facial landmarks (key-\npoints or features) and adopting a reference 3D head model\nfor corresponding feature matching; (2) the second category\nuses a complete face to recognize the head pose. They either\nuse a appearance face model or they estimate the head pose\ndirectly from an image. We will discuss literature study on\nthese two categories as follows:\n1) Landmark Based Techniques: The head pose is esti-\nmated by detecting the correspondences between 2D image\nand 3D model landmarks. In [6], a cascaded pose regressor\nis proposed which reﬁnes the regression output at every\nreﬁnement. Other regression based techniques are used in\n[3], [35] which sketch rough faces to align them to real faces\nusing regression. Some techniques detect the keypoints on\nfaces using trained appearance models [21]. In [29], a three\nlevel CNN is used to detect the position of landmarks on the\nface. In [44], a cascaded neural network termed as 3D dense\nface alignment is proposed. This network ﬁts a 3D morphable\nface model to an image. In [12], a novel technique to\noptimize the regression task of estimating 3DMM parameters\nis used. It predicts the rotation matrix using a network. In\n[18], a CNN based network is proposed to iteratively reﬁne\nthe position of facial keypoints. It also provides head pose\ninformation as a side result in terms of Euler angles. In [24],\na multi-task CNN model which can predict pose estimation,\nfeature localization, gender recognition, and face detection\nfrom the same network is proposed.\nThe performance of all these techniques is conditioned\non the landmark detection accuracy, hence due to these\nreasons we do not follow a landmark based approach. Also,\nit increases computation when pose can also be estimated\nwithout locating the keypoints.\n2) Non Landmark Based Techniques: These techniques\nimproved the state-of-the-art results on HPE. In [10], depth\nimages and discriminative random regression forests are used\nfor HPE. [22] also uses depth images and registered 3D\nmorphable models to these images. It reﬁnes the registration\nincrementally. In [25], a CNN based network is proposed\nhaving multi-losses. This network uses an image to predict\nEuler angles and demonstrates that the HPE without land-\nmarks performs better than with the landmarks. [14] built\na technique using quaternion that removes the gimbal lock\nissue which comes with Euler angles. [37], too uses a CNN\nbased model to perform regression stage-wise. They use\nfeature aggregation technique to group the spatial features\nalong with attention mechanism. [4] uses rotation matrix\nwith three vectors for representing the HPE and develops\na CNN based architecture using the new representation.\n[40] proposes a feature decoupling network which learns\nthe discriminative feature corresponding to each pose an-\ngle. It adopts a new cross-category loss for the network\noptimization. [43] proposes WHENet which uses multi-loss\ntechnique for training and this network is applicable for full\nrange of head yaws. The better estimation of head pose using\nnon-landmark techniques has induced this work to use this\ncategory of technique.\nB. Transformers\nAfter achieving signiﬁcant success in natural language\nprocessing [32], transformers have gained a lot of attention\namong computer vision researchers. ViT [8] for image\nclassiﬁcation uses a transformer on sequences of image\npatches which outperforms CNNs. Various different versions\nof vision transformers have also been proposed recently:\nusing a pyramid structure like CNNs [33], performing\ndata efﬁcient training using distillation [30], learning an\nabstract representation for self attention to improve the\nefﬁciency [34], manipulating it to have layer wise token\nto token transformation for encoding localized structure\nHead Pose Predictions\nIllustration\nBackbone\n+\nAdd\n&\nNorm\nPosition-Wise\nFeed Forward\nM\nM\nS\nInput\nImage\nAdd\n&\nNorm\nPrediction\nHead\nYaw\nPitch\nRoll\nPose\nPredictions\nDownsampler\nReshaper\nTransformer\nEncoder\nConnector\nSequential\nFeatures\nForward Explain\nM\nS\nMatMul\nScale &\nSoftmax\nPosition\nEmbedding\nFig. 2. The framework of HeadPosr is described in this ﬁgure. It consists of fours parts: (1) backbone: input RGB image is fed into backbone network.\nThe output features are passed on to (2) connector which down-samples the input features and reshapes them to have a sequential form. These sequential\nfeatures are used as input to a (3) transformer encoder. The output of the transformer encoder is fed into prediction head which outputs three vectors\ncorresponding to the head pose, i.e., yaw, pith and roll. The whole framework is trained in end to end fashion.\nfor individual token [39]. Vision transformers have gained\npopularity in tasks such as object segmentation, object\ndetection, pose estimation, etc. The use of transformer\nfor 3D human pose estimation [19] and 3D hand pose\nestimation [15] has motivated this work to use transformer\nfor HPE. This is the ﬁrst work as per our knowledge on\nHPE which employs transformers in the architecture.\nIII. M ETHODOLOGY\nA. Problem Formulation\nRGB image based HPE problem can be formulated as:\nGiven N training images I = in|n = 1, ....,N that have pose\nvectors vn corresponding to each image in. This vn pose\nvector is a 3D vector, i.e., vn = [y, p,r], where y, p, r\ncorresponds to yaw, pitch, and roll angles, respectively. Thus,\nthe aim of this problem is to ﬁnd a relation R such that\n¯v = R(i). This relation can predict the pose vector ¯v as closely\nas possible to ground truth pose vector v. The relation R is\ncalculated using the optimization of the mean absolute error\n(MAE) between the ground truth and predicted poses.\n(I) =1\nN\nN\n∑\nn=1\n|¯vn −v| (1)\nwhere ¯vn = R(in) is the estimated pose vector from the\nproposed HeadPosr network for the given image in. So, this\nformulates into a regression problem. Hence, this network\nneeds to regress the pose vectors to solve the task of HPE.\nFor this regression task, a transformer based architecture is\nbuilt which employs CNN-based backbone to learn the head\nposes.\nB. HeadPosr Architecture\nHeadPosr network consists of a CNN based backbone,\nconnector, transformer encoder, and head pose prediction\nhead. Figure 2, shows the architecture of the network. The\nsubstructures of the network are described as follows:\n1) Backbone: The backbone in the architecture consists of\nconvolutional layers, very similar to those used in [28], [36].\nGiven a set of images, I ∈RB×C×H×W , B stands for batch-\nsize, C stands for channels in the image, H stands for height,\nW stands for width of the image. This CNN with stride S\ngenerates lower-resolution feature maps F ∈RB×C′×H/S×W/S.\nIn this work, different versions of ResNets [13] with various\nstrides S are used. To keep the network simple, we keep only\nthe initial layers of both the networks to extract features.\n2) Connector: Connector transform the output features\nfrom the backbone into a sequential data. It consists of two\nparts: (1) Downsampler, which is a 1 ×1 convolutional layer.\nIt preserves the shape except the number of channels of the\nfeatures. It is used as downsampler to reduce the dimension\nas well as to keep the condensed information. (2) Reshaper is\nused to change the shape of the features in a way so that the\ndata is in sequential form. These output sequential features\nare used as an input to transformer encoder. The reshaper\nmultiplies the height and width of the output features from\nthe downsampler and then permutates them to transform into\na sequential form. Considering the output set of features from\nthe backbone as F ∈RB×C′×H/S×W/S, this output is channel\ndownsampled as F′∈RB×d×H/S×W/S′\nwhere d < C. This F′\nis ﬂattened by reshaper in a way such that F′∈RB×A×d,\nwhere A = H/S ×W/S. This resulting formation of F′ is a\nsequential form and is ﬁt to be an input to a transformer.\n3) Transformer: In this architecture, we only use the\nencoder part of the transformer because regression is an\nencoding task and does not require a decoder to upscale the\nencoder output. The used transformer encoder architecture is\na standard one [32]. This encoder compresses the sequential\ninput to an encoded output. It passes through number of\nself-attention layers and feed-forward networks. Transformer\nencoder is able to understand the global information due to\nthe self attention mechanism on the sequential information.\nTransformer Description: Transformer works on the princi-\nple of multi-head self attention. This multi-head mechanism\ndemands a sequential input F′ ∈RA×d to it. This is the\nreason that connector is used to convert the output from the\nbackbone to a sequential input. The inputs are projected to\nV ∈RA×d values, K ∈RA×d keys, and Q ∈RA×d queries.\nThey are calculated as:\nQi = ˆF′∗WQ\ni (2)\nKi = ˆF′∗WK\ni (3)\nVi = ˆF′∗WV\ni (4)\nwhere i stands for each attention head used. In the ablation\nstudy (Section VI), the effect of varying the number of\nattention heads in HeadPosr is shown. WQ\ni ∈Rd×dk , WK\ni ∈\nRd×dk and WV\ni ∈Rd×dv are parameter matrices. For detailed\nexplanation of the transformers, refer to [32]. In this paper,\nsimilar to [28], [36], position embedding is added instead of\ncalculating the V . The transformer encoder without position\nembedding is a permutation-invariant which would disregard\nthe spatial structure in the image. The effect of adding\ndifferent types of positional embedding is also shown in\nablation study (Section VI). So, the resulting scaled dot-\nproduct attention head output Zi is formed as:\nZi = so f tmax\n(Qi ∗KT\ni√dk\n)\n(5)\nThe output Zi from each attention head is concatenated to\nget:\nZ = concatenate(Z1,Z2, ...,Znheads ) (6)\nIn this paper, effect of changing the number of attention\nheads is also studied in ablation study (Section VI). The\nconcatenated output acts as an input to a fully-connected\nlayer. The output of the fully connected layer has same\ndimensions as the sequential input to the transformer encoder\nˆF′∈RA×d. The add and norm layer adds the input to the\noutput of the multi-head attention as a residual connection.\nThe normalization helps to deal with the ‘covariate shift’\nproblem by recalculating the mean and variance of the\ncumulative inputs in every layer.\n4) Head Pose Prediction Head: A very straightforward\nhead pose prediction head is used in the network. Given the\noutput from the transformer as F′\no ∈RB×A×C′\n, where A =\nH/S ×W/S, this head needs to predict RB×3. Firstly, F′\no ∈\nRB×A×C′\nis reshaped to F′\no ∈RB×C′×H/S×W/S. If H/S and\nW/S are equal and smaller than 8, then the convolution layer\nwith kernel size H/S ×W/S where H = W is used. It also\nreduces the channel dimension from A to 3 (corresponding to\nthree head poses). In the ablation study (Section VI), effect\nof using fully connected layer is also studied in comparison\nto convolutional layer.\nIV. E XPERIMENTS\nA. Datasets\nFig. 3. Some of the sample images from the datasets used for the\nexperiments. Namely, ﬁrst row: 300W-LP [44]; second row: AFLW2000\n[45]; third row: BIWI [9] dataset\nThe experiments were performed using three open-source\ndatasets, i.e., 300W-LP [44], AFLW2000 [45], and BIWI [9]\ndatasets. Some sample images from all the three datasets are\nshown in Figure 3.\n1) 300W-LP Dataset: This dataset is an extension of\n300W dataset [26] which is a combination of several datasets,\nfor instance, HELEN [42], AFW [46], IBUG [26], LFPW\n[1], etc. They used the technique of face proﬁling using 3D\nmeshing for expanding this dataset to generate 61,225 image\nsamples for large poses and used ﬂipping to further expand\nto 122,450 synthesized image samples.\n2) AFLW2000: This dataset has 2000 images from AFLW\n[17] dataset. It has 3D faces with 68 landmarks corre-\nsponding to those faces. These faces have a wide range of\nilluminations, expressions and pose variations in its images.\n3) BIWI: This dataset has total of 15,678 frames from 24\nvideos with 20 different subjects captured in the controlled\nindoor environment. This dataset does not include bounding\nboxes for the human heads. In this paper, similar to [4], [37],\nMTCNN [41] is used to detect the human faces and crop the\nface to get the face bounding box output.\nThis work uses the same protocols for training and testing\nas used by [4], [37], [7], to have fair comparison with these\napproaches. We also ﬁltered out the samples which have\nEuler angle outside the range of [-99°, 99°] as done by [4],\n[37], [7]. The two protocols used for the experiments are as\nfollows.\n• In this protocol, the model is trained with 300W-LP\nsynthetic dataset and then the tests are conducted on the\ntrained model by using AFLW2000 and BIWI datasets\n(both are real-world datasets), separately.\n• In this set of experiments, training is performed by\ndividing the dataset into 70% training and 30% testing\ndatasets as also done by [4], [37]. These statistics results\ninto 16 videos for training and 8 videos for testing from\nthe 24 total videos. This protocol is used by several\nstate-of-the-art work even when they used different\ntypes of modalities, i.e., RGB, RGB-D, or videos. In\nthese paper, only RGB based data is used to perform\nthe evaluation of our approach.\nFor both the protocols, MAE results are reported for Euler\nangles.\nB. Implementation Details\nHeadPosr is implemented using Pytorch library. It em-\nploys the same data augmentation techniques as used by\n[37], [4] to have the fair comparison. The random cropping\nand scaling (by a factor of 0.8 ∼1.2) was carried out in\ntraining. The network is trained for 90 epochs using an\nAdam optimizer with initial learning rate as 0.01. Further,\nthis learning rate decayed by a factor of 0.1 after the interval\nof 30 epochs. Batch size used for training with 300W-LP\ndataset is 16 and with BIWI dataset is 8. The implementation\nwas performed on RTX 2080 Ti GPU.\nV. R ESULTS\nHeadPosr is compared to state-of-art-methods as shown\nin Table I and II. These compared networks are as follows.\n3DDFA [44] utilizes CNNs to correspond the 3D model\ninto an image. It is shown to work quite efﬁciently with\nocclusion scenes. KEPLER [18] uses modiﬁed GoogleNet\nnetwork to predict landmarks and pose simultaneously.\nDlib [16] is a well known library which is used for\nlandmark prediction, HPE, face detection, etc. FAN [2]\nuses multi-scale features by merging features from multiple\nlayers at multiple stages. Hopenet [25] uses ResNet, similar\nto HeadPosr, but is trained on mean squared error and\ncross entropy loss. Shao [27] uses CNN based network by\nadjusting the margins of detected face bounding box. SSR-\nNet-MD [38], FSA-Caps-Fusion [37], and TriNet [4] use\nthe same basic technique of network having multiple stages\nusing soft stage-wise regression. VGG16 [11] is a network\nhaving combination of CNN and RNN using Bayesian ﬁlters\nanalysis. DeepHeadPose [23] uses low resolution depth\nimages applying regression and classiﬁcation to predict the\npose. Martin [20] uses depth images for predicting head\npose and uses registration between depth images and 3D\nmodel. WHENet [43] uses efﬁcientNet as backbone with\nmulti-loss approaches and with changes to loss functions.\nFor ﬁrst set of experiments when HeadPosr is trained with\n300W-LP dataset, Table I shows that it outperforms all the\ncompared networks for the experiments trained using 300W-\nLP and hence setting it as a benchmark. It also performs\nquite closely on both the BIWI and AFLW2000 dataset.\nFor the second set of experiment when HeadPosr is\ntrained with BIWI dataset, it outperforms all the approaches\nin predicting the yaw angle. It even surpasses the results\nfrom using different modalities. The MAE is more than that\nof TriNet. But it overall outperforms TriNet when taking\naverage from both the sets of experiments.\nA. Effectiveness of using a transformer\nFrom the experimental study, it is evident that the using\ntransformer encoder along with ResNet architecture performs\nquite well. Hopenet [25] which is also a ResNet based\narchitecture has MAE equivalent to 6.41 on AFLW2000\nand 4.90 on BIWI dataset when trained using 300W-LP\ndataset. When HeadPosr is compared to the performance\nof Hopenet, it can be seen in Table I that it outperforms\nAFLW2000 and BIWI datasets by 36%, and 18% in\nMAE respectively, when trained on 300W-LP dataset. These\nquantitative improvements also point out that transformers\nindeed are useful in employing for the task of HPE. There\ncould be various ways of using them for this particular\nproblem. This paper emphasizes that the transformers should\nbe explored more for research in HPE.\nB. Qualitative Examples\nFigure 4 illustrates the HPE images from two datasets,\nnamely, AFLW2000 and BIWI dataset. Both these datasets\nare used for testing when the network is trained with 300W-\nLP dataset. In this ﬁgure, the comparison between ground\ntruth head pose illustration is made with the output from\nFSA-Net, and from HeadPosr. It can be seen that HeadPosr\nperforms better than FSA-Net and is more closely related to\nthe ground truth predictions illustration.\nVI. A BLATION STUDY\nThe ablation study is conducted to understand the in-\nﬂuence of various parameters in the HeadPosr. For each\ncomparison of the study to have fair comparison, we use\nsame parameters for that study except the parameter being\nstudied.\nA. Transformer\nSeveral parameters of the transformer encoder were varied\nto check their effect on the HPE.\n1) Number of Encoders: The number of encoder layers\nused for transformer encoder are varied as shown in Table\nIII. It is seen that three encoders performs better than others\nwhen MAE is averaged on both the testing datasets.\n2) Number of Heads: The number of attention heads\nin the transformer encoder were varied from 1 to 16 by\nmultiples of 2 to check their effects. The quantitative values\nare described in Table III. It can be seen that the number of\nheads equal to 4 has better quantitative results than other\ncompared number of heads (taking average of both the\ndatasets).\n3) Type of activation: In this work, two types of activation\nfunction in the transformer encoder were tested, i.e., Relu\nand Gelu. It is seen that Relu performed better than Gelu in\nHeadPosr as shown in Table III.\nTABLE I\nCOMPARISON OF THE HeadPosr WITH THE STATE -OF-THE -ART METHODS ON BIWI[9]( LEFT ) AND AFLW2000[44]( RIGHT ) DATASET. THE NETWORK\nIS TRAINED ON 300W-LP DATASET. (EH64 MEANS 6 ENCODERS , 4 HEADS IN A TRANSFORMER )\nBIWI AFLW2000 Total\nMethod Yaw Pitch Roll MAE Yaw Pitch Roll MAE Total MAE\n3DDFA [44] 36.20 12.30 8.78 19.10 5.40 8.53 8.25 7.39 13.25\nKEPLER [18] 8.80 17.3 16.2 13.9 - - - - 13.9\nDlib (68 points) [16] 16.8 13.8 6.19 12.2 23.1 13.6 10.5 15.8 14\nFAN (12 points) [2] 6.36 12.3 8.71 9.12 8.53 7.48 7.63 7.88 8.5\nHopenet(a = 1) [25] 4.81 6.61 3.27 4.90 6.92 6.64 5.67 6.41 5.65\nHopenet(a = 2) [25] 5.12 6.98 3.39 5.18 6.47 6.56 5.44 6.16 5.67\nShao [27] 4.59 7.25 6.15 6.00 5.07 6.37 4.99 5.48 5.74\nSSR-Net-MD [38] 4.49 6.31 3.61 4.65 5.14 7.09 5.89 6.01 5.33\nFSA-Caps-Fusion [37] 4.27 4.96 2.76 4.00 4.50 6.08 4.64 5.07 4.85\nTriNet [4] 4.11 4.76 3.05 3.97 4.04 5.78 4.20 4.67 4.32\nWHENet [43] 3.99 4.39 3.06 3.81 5.11 6.24 4.92 5.42 4.61\nWHENet-V [43] 3.60 4.10 2.73 3.48 4.44 5.75 4.31 4.83 4.16\nHeadPosr EH38 4.08 5.10 3.02 4.06 4.60 4.86 2.87 4.11 4.08\nHeadPosr EH64 3.37 5.44 2.69 3.83 4.64 5.84 4.30 4.92 4.37\nFig. 4. Qualitative Results of HeadPosr in comparison to FSA-Net [37]. The red line after sixth column divides the ﬁgure in two parts: Left is from\nAFLW2000 dataset; Right is from BIWI dataset. The ﬁrst row are test images from the datasets; second row is illustration of ground truth on the test\nimage; third row is the output illustration from FSA-Net; forth row is the output illustration from the HeadPosr\nTABLE II\nCOMPARISON RESULTS OF HeadPosr WITH DIFFERENT METHODS ON\nBIWI DATASET. HeadPosr IS TRAINED ON BIWI DATASET WITH 70%\nTRAINING AND 30% TESTING DATA , USING ONLY RGB IMAGES .\nYaw Pitch Roll MAE\nRGB-based\nDeepHeadPose [23] 5.67 5.18 - -\nSSR-Net-MD [38] 4.24 4.35 4.19 4.26\nVGG16 [11] 3.91 4.03 3.03 3.66\nFSA-Caps-Fusion [37] 2.89 4.29 3.60 3.60\nTriNet 2.99 3.04 2.44 2.80\nHeadPosr EH64 2.59 4.03 3.53 3.38\nRGB+Depth\nDeepHeadPose [23] 5.32 4.76 - -\nMartin [20] 3.6 2.5 2.6 2.9\nRGB+Time\nVGG16+RNN [11] 3.14 3.48 2.6 3.07\n4) Position Embedding: Three types of position embed-\nding are used to perform the experiments: (1) no embedding;\n(2) sine embedding; and (3) learnable embedding by adding\na parameter. Learnable embedding performs better than the\nother two compared embeddings (see Table III).\nB. Type of Backbone\nDifferent versions of ResNet are evaluated such as: (1)\nResNet18; (2) ResNet34; and (3) ResNet50. This evaluation\nis performed using BIWI dataset (as in protocol 2) for\ntraining and testing. It is seen in the evaluation as shown\nin Table IV that ResNet50 works better than the other two\ncompared networks as a backbone in the HeadPosr. In our\nexperiments, we use ResNet50.\nTABLE III\nABLATION STUDY RESULTS PERFORMED USING HeadPosr. THE FIRST SET OF ABLATIONS ARE ON VARYING THE PARAMETERS OF THE TRANSFORMER\nENCODER SUCH AS ENCODERS , HEADS , ACTIVATIONS , AND POSITION EMBEDDINGS . THE SECOND SET IS ON VARYING THE LEARNING RATE , USING\nDIFFERENT VERSIONS OF PREDICTION HEAD , AND DIMENSION D OF THE INPUT FEATURE CHANNEL OF TRANSFORMER\nBIWI AFLW Total\nYaw Pitch Roll MAE Yaw Pitch Roll MAE Total MAE\nNo. of Encoder Layers\n1 4.75 4.92 2.84 4.17 4.47 5.68 4.09 4.74 4.45\n2 4.70 4.87 2.72 4.10 4.27 5.84 4.20 4.77 4.43\n3 4.16 5.24 2.93 4.11 4.34 5.06 4.25 4.55 4.33\n4 4.40 4.90 2.78 4.02 4.38 5.77 4.15 4.76 4.39\n5 4.45 4.93 2.94 4.10 4.33 5.74 4.23 4.76 4.43\n6 4.54 4.72 2.82 4.02 4.35 5.87 4.10 4.44 4.23\nNo. of Heads\n1 4.81 4.82 2.86 4.16 4.34 5.83 4.13 4.76 4.46\n2 4.47 4.85 2.75 4.02 4.49 5.85 4.27 4.87 4.45\n4 3.37 5.44 2.69 3.83 4.64 5.84 4.30 4.92 4.38\n8 4.90 4.41 2.87 4.06 4.47 5.81 4.26 4.84 4.45\n16 3.78 5.34 2.90 4.01 4.41 6.07 4.23 4.90 4.46\nActivations\nRelu 4.08 5.10 3.02 4.06 4.60 4.86 2.87 4.11 4.08\nGelu 4.16 5.24 2.93 4.11 4.44 6.06 4.35 4.95 4.53\nPosition Embeddings\nNo Embedding 4.41 4.87 2.77 4.01 4.59 5.94 4.36 4.96 4.48\nLearnable 4.90 4.41 2.87 4.06 4.47 5.81 4.26 4.84 4.45\nSine 4.16 5.24 2.93 4.11 4.44 6.06 4.35 4.95 4.53\nLearning Rate\n0.01 12.09 13.94 10.67 12.23 12.39 12.89 13.90 13.06 12.26\n0.001 4.08 5.10 3.02 4.06 4.60 4.86 2.87 4.11 4.08\n0.0001 4.72 4.57 2.69 3.99 4.34 5.89 4.36 4.86 4.42\nTypes of Prediction Head\nConv 1 ×1 4.58 4.60 2.86 4.01 4.37 4.70 3.83 4.30 4.15\nConv 8 ×8 4.08 5.10 3.02 4.06 4.60 4.86 2.87 4.11 4.08\nFC layer 4.32 4.73 3.01 4.01 4.25 4.82 4.20 4.42 4.21\nValue of d\n16 4.08 5.10 3.02 4.06 4.60 4.86 2.87 4.11 4.08\n32 4.23 4.89 4.99 4.70 4.74 4.62 4.01 4.45 4.57\nTABLE IV\nABLATION STUDY USING DIFFERENT BACKBONES FOR COMPARING\nRESULTS ON RGB IMAGES FROM BIWI DATASET (TRAINING\nPERFORMED USING BIWI DATASET ). T HREE DIFFERENT BACKBONES\nARE EVALUATED IN THE HeadPosr, I.E., R ESNET18, R ESNET32, AND\nRESNET50. T HE EVALUATION IS PERFORMED USING MAE.\nYaw Pitch Roll MAE\nNetworks\nResNet18 3.20 4.98 3.85 4.01\nResNet32 3.09 4.94 3.54 3.85\nResNet50 2.59 4.03 3.53 3.38\nC. Type of Prediction Head\nPrediction head is used after transformer encoder and at\nthe end of the network. It converts the output features into\nthree output angles. Three types of prediction heads are\nstudied: (1) fully connected layer; (2) convolutional layer\nwith kernel size equal to length and width of the features\n(considering length = width ≤8); (3) convolutional layer\nwith kernel size 1 and summation of features length and\nwidth axis. The experimental results are shown in Table III.\nConsidering the results, for further experiments, convolu-\ntional layer with kernel size 8 is used which has features\nhaving 8 ×8 length and width.\nD. Channels of Input Features to Transformer\nThe input to transformer has d channels (output from the\nconnector). In the experiments, two dimensions of channels\nwere studied, i.e. 16 and 32. From the table, it is evident that\n16 performed better than 32 channels for the HeadPosr.\nVII. C ONCLUSION AND FUTURE WORK\nIn this paper, HeadPosr, a novel technique for HPE is pro-\nposed that can predict head poses using a transformer. This is\nachieved by using a network having a CNN based backbone,\na connector for manipulating the features to be used as input\nfor the transformer, a transformer encoder, and prediction\nhead. HeadPosr outperforms the already existing techniques\nwhen trained using 300W-LP. Overall taking average from\nBIWI and AFLW2000 datasets, it exceeds the performance\nof the state-of-the-art methods. To our knowledge, it is\nthe ﬁrst approach of using transformer for HPE. Hence,\nit paves a way to consider them for wider research in the\nproblem of HPE. In the future, it is intended to extend\nthis work by reducing the network size, exploring different\ntransformer structures, and different loss function techniques.\nFurther expansion would be to consider the modiﬁcation of\nrepresentation of head pose using a transformer.\nREFERENCES\n[1] P. N. Belhumeur, D. W. Jacobs, D. J. Kriegman, and N. Kumar.\nLocalizing parts of faces using a consensus of exemplars. IEEE\ntransactions on pattern analysis and machine intelligence , 35:2930–\n2940, 2013.\n[2] A. Bulat and G. Tzimiropoulos. How far are we from solving the\n2d & 3d face alignment problem?(and a dataset of 230,000 3d facial\nlandmarks). In Proceedings of the IEEE International Conference on\nComputer Vision, pages 1021–1030, 2017.\n[3] X. Cao, Y . Wei, F. Wen, and J. Sun. Face alignment by explicit shape\nregression. International Journal of Computer Vision , 107:177–190,\n2014.\n[4] Z. Cao, Z. Chu, D. Liu, and Y . Chen. A vector-based representation\nto enhance head pose estimation. In Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision , pages 1188–\n1197, 2021.\n[5] N. Dhingra and A. Kunz. Res3atn-deep 3d residual attention network\nfor hand gesture recognition in videos. In 2019 International Confer-\nence on 3D Vision (3DV) , pages 491–501, 2019.\n[6] P. Doll ´ar, P. Welinder, and P. Perona. Cascaded pose regression. In\n2010 IEEE Computer Society Conference on Computer Vision and\nPattern Recognition, pages 1078–1085, 2010.\n[7] B. Doosti, S. Naha, M. Mirbagheri, and D. J. Crandall. Hope-net:\nA graph-based model for hand-object pose estimation. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6608–6617, 2020.\n[8] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al.\nAn image is worth 16x16 words: Transformers for image recognition\nat scale. arXiv preprint arXiv:2010.11929 , 2020.\n[9] G. Fanelli, M. Dantone, J. Gall, A. Fossati, and L. Van Gool. Random\nforests for real time 3d face analysis. International journal of computer\nvision, 101:437–458, 2013.\n[10] G. Fanelli, T. Weise, J. Gall, and L. Van Gool. Real time head pose\nestimation from consumer depth cameras. In Joint pattern recognition\nsymposium, pages 101–110, 2011.\n[11] J. Gu, X. Yang, S. De Mello, and J. Kautz. Dynamic facial analysis:\nFrom bayesian ﬁltering to recurrent neural network. In Proceedings\nof the IEEE conference on computer vision and pattern recognition ,\npages 1548–1557, 2017.\n[12] J. Guo, X. Zhu, Y . Yang, F. Yang, Z. Lei, and S. Z. Li. Towards\nfast, accurate and stable 3d dense face alignment. arXiv preprint\narXiv:2009.09960, 2020.\n[13] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE conference on computer vision\nand pattern recognition , pages 770–778, 2016.\n[14] H.-W. Hsu, T.-Y . Wu, S. Wan, W. H. Wong, and C.-Y . Lee. Quatnet:\nQuaternion-based head pose estimation with multiregression loss.\nIEEE Transactions on Multimedia , 21:1035–1046, 2018.\n[15] L. Huang, J. Tan, J. Liu, and J. Yuan. Hand-transformer: Non-\nautoregressive structured modeling for 3d hand pose estimation. In\nEuropean Conference on Computer Vision , pages 17–33, 2020.\n[16] V . Kazemi and J. Sullivan. One millisecond face alignment with an\nensemble of regression trees. In Proceedings of the IEEE conference\non computer vision and pattern recognition , pages 1867–1874, 2014.\n[17] M. Koestinger, P. Wohlhart, P. M. Roth, and H. Bischof. Annotated\nfacial landmarks in the wild: A large-scale, real-world database for\nfacial landmark localization. In 2011 IEEE international conference\non computer vision workshops , pages 2144–2151, 2011.\n[18] A. Kumar, A. Alavi, and R. Chellappa. Kepler: Keypoint and\npose estimation of unconstrained faces by learning efﬁcient h-cnn\nregressors. In 2017 12th ieee international conference on automatic\nface & gesture recognition (fg 2017) , pages 258–265, 2017.\n[19] W. Li, H. Liu, R. Ding, M. Liu, and P. Wang. Lifting transformer for\n3d human pose estimation in video. arXiv:2103.14304, 2021.\n[20] M. Martin, F. Van De Camp, and R. Stiefelhagen. Real time head\nmodel creation and head pose estimation on consumer depth cameras.\nIn 2014 2nd International Conference on 3D Vision , volume 1, 2014.\n[21] I. Matthews and S. Baker. Active appearance models revisited.\nInternational journal of computer vision , 60:135–164, 2004.\n[22] G. P. Meyer, S. Gupta, I. Frosio, D. Reddy, and J. Kautz. Robust\nmodel-based 3d head pose estimation. In Proceedings of the IEEE\ninternational conference on computer vision , pages 3649–3657, 2015.\n[23] S. S. Mukherjee and N. M. Robertson. Deep head pose: Gaze-direction\nestimation in multimodal video. IEEE Transactions on Multimedia ,\n17:2094–2107, 2015.\n[24] R. Ranjan, V . M. Patel, and R. Chellappa. Hyperface: A deep multi-\ntask learning framework for face detection, landmark localization, pose\nestimation, and gender recognition. IEEE transactions on pattern\nanalysis and machine intelligence , 41:121–135, 2017.\n[25] N. Ruiz, E. Chong, and J. M. Rehg. Fine-grained head pose estimation\nwithout keypoints. In Proceedings of the IEEE conference on computer\nvision and pattern recognition workshops , pages 2074–2083, 2018.\n[26] C. Sagonas, G. Tzimiropoulos, S. Zafeiriou, and M. Pantic. 300 faces\nin-the-wild challenge: The ﬁrst facial landmark localization challenge.\nIn Proceedings of the IEEE International Conference on Computer\nVision Workshops, pages 397–403, 2013.\n[27] M. Shao, Z. Sun, M. Ozay, and T. Okatani. Improving head pose\nestimation with a combined loss and bounding box margin adjustment.\nIn 2019 14th IEEE International Conference on Automatic Face &\nGesture Recognition (FG 2019) , pages 1–5, 2019.\n[28] L. Stofﬂ, M. Vidal, and A. Mathis. End-to-end trainable multi-instance\npose estimation with transformers. arXiv preprint arXiv:2103.12115 ,\n2021.\n[29] Y . Sun, X. Wang, and X. Tang. Deep convolutional network cascade\nfor facial point detection. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , pages 3476–3483, 2013.\n[30] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and\nH. J ´egou. Training data-efﬁcient image transformers & distillation\nthrough attention. arXiv preprint arXiv:2012.12877 , 2020.\n[31] L. Tran, X. Yin, and X. Liu. Disentangled representation learning\ngan for pose-invariant face recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition , pages 1415–\n1424, 2017.\n[32] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin. Attention is all you need.\narXiv:1706.03762, 2017.\n[33] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu,\nP. Luo, and L. Shao. Pyramid vision transformer: A versatile\nbackbone for dense prediction without convolutions. arXiv preprint\narXiv:2102.12122, 2021.\n[34] L. Wu, X. Liu, and Q. Liu. Centroid transformers: Learning to abstract\nwith attention. arXiv preprint arXiv:2102.08606 , 2021.\n[35] X. Xiong and F. De la Torre. Global supervised descent method. In\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 2664–2673, 2015.\n[36] S. Yang, Z. Quan, M. Nie, and W. Yang. Transpose: Towards\nexplainable human pose estimation by transformer. arXiv:2012.14214,\n2020.\n[37] T.-Y . Yang, Y .-T. Chen, Y .-Y . Lin, and Y .-Y . Chuang. Fsa-net:\nLearning ﬁne-grained structure aggregation for head pose estimation\nfrom a single image. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 1087–1096, 2019.\n[38] T.-Y . Yang, Y .-H. Huang, Y .-Y . Lin, P.-C. Hsiu, and Y .-Y . Chuang. Ssr-\nnet: A compact soft stagewise regression network for age estimation.\nIn IJCAI, volume 5, page 7, 2018.\n[39] L. Yuan, Y . Chen, T. Wang, W. Yu, Y . Shi, F. E. Tay, J. Feng, and\nS. Yan. Tokens-to-token vit: Training vision transformers from scratch\non imagenet. arXiv preprint arXiv:2101.11986 , 2021.\n[40] H. Zhang, M. Wang, Y . Liu, and Y . Yuan. Fdn: Feature decoupling\nnetwork for head pose estimation. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence , pages 12789–12796, 2020.\n[41] K. Zhang, Z. Zhang, Z. Li, and Y . Qiao. Joint face detection and\nalignment using multitask cascaded convolutional networks. IEEE\nSignal Processing Letters , 23:1499–1503, 2016.\n[42] E. Zhou, H. Fan, Z. Cao, Y . Jiang, and Q. Yin. Extensive facial land-\nmark localization with coarse-to-ﬁne convolutional network cascade.\nIn Proceedings of the IEEE international conference on computer\nvision workshops, pages 386–391, 2013.\n[43] Y . Zhou and J. Gregson. Whenet: Real-time ﬁne-grained estimation\nfor wide range head pose. arXiv:2005.10353, 2020.\n[44] X. Zhu, Z. Lei, X. Liu, H. Shi, and S. Z. Li. Face alignment across\nlarge poses: A 3d solution. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , pages 146–155, 2016.\n[45] X. Zhu, Z. Lei, J. Yan, D. Yi, and S. Z. Li. High-ﬁdelity pose\nand expression normalization for face recognition in the wild. In\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 787–796, 2015.\n[46] X. Zhu and D. Ramanan. Face detection, pose estimation, and\nlandmark localization in the wild. In 2012 IEEE conference on\ncomputer vision and pattern recognition , pages 2879–2886, 2012."
}