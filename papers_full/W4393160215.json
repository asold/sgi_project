{
    "title": "Cached Transformers: Improving Transformers with Differentiable Memory Cachde",
    "url": "https://openalex.org/W4393160215",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2135669807",
            "name": "Zhao-yang Zhang",
            "affiliations": [
                "Chinese University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2920883564",
            "name": "Wenqi Shao",
            "affiliations": [
                "Chinese University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2245429481",
            "name": "Yixiao Ge",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2103410182",
            "name": "Xiaogang Wang",
            "affiliations": [
                "Chinese University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2168876151",
            "name": "Jinwei Gu",
            "affiliations": [
                "Chinese University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A1912778456",
            "name": "Ping Luo",
            "affiliations": [
                "University of Hong Kong"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6840426214",
        "https://openalex.org/W6778485988",
        "https://openalex.org/W3036224891",
        "https://openalex.org/W6757517122",
        "https://openalex.org/W6802318714",
        "https://openalex.org/W3113120245",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W6735463952",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W2127836646",
        "https://openalex.org/W2081345994",
        "https://openalex.org/W6639102338",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W4221160836",
        "https://openalex.org/W6898505805",
        "https://openalex.org/W6785783668",
        "https://openalex.org/W3134454711",
        "https://openalex.org/W2769311391",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W3175515048",
        "https://openalex.org/W6771249120",
        "https://openalex.org/W2931428928",
        "https://openalex.org/W3181262653",
        "https://openalex.org/W2963393721",
        "https://openalex.org/W2933138175",
        "https://openalex.org/W4288024261",
        "https://openalex.org/W4287324101",
        "https://openalex.org/W3033529678",
        "https://openalex.org/W3103682594",
        "https://openalex.org/W3204130541",
        "https://openalex.org/W2963842551",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2962859295",
        "https://openalex.org/W2963542740",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W4287704453",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W4225909425",
        "https://openalex.org/W3166738839",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W4312920106",
        "https://openalex.org/W2953830716",
        "https://openalex.org/W3035014997",
        "https://openalex.org/W2964199361",
        "https://openalex.org/W4285595435",
        "https://openalex.org/W4295838474",
        "https://openalex.org/W4394667418"
    ],
    "abstract": "This work introduces a new Transformer model called Cached Transformer, which uses Gated Recurrent Cached (GRC) attention to extend the self-attention mechanism with a differentiable memory cache of tokens. GRC attention enables attending to both past and current tokens, increasing the receptive field of attention and allowing for exploring long-range dependencies. By utilizing a recurrent gating unit to continuously update the cache, our model achieves significant advancements in \\textbf{six} language and vision tasks, including language modeling, machine translation, ListOPs, image classification, object detection, and instance segmentation. Furthermore, our approach surpasses previous memory-based techniques in tasks such as language modeling and displays the ability to be applied to a broader range of situations.",
    "full_text": "Cached Transformers: Improving Transformers with Differentiable Memory\nCache\nZhaoyang Zhang1, Wenqi Shao1, Yixiao Ge2, Xiaogang Wang1, Jinwei Gu1, Ping Luo3\n1The Chinese University of Hong Kong\n2Tencent Inc.\n3The University of Hong Kong\n{zhaoyangzhang@link., wqshao@link., xgwang@ee., jwgu@}cuhk.edu.hk, geyixiao831@gmail.com, pluo@cs.hku.hk\nAbstract\nThis work introduces a new Transformer model called\nCached Transformer, which uses Gated Recurrent Cached\n(GRC) attention to extend the self-attention mechanism with\na differentiable memory cache of tokens. GRC attention en-\nables attending to both past and current tokens, increasing the\nreceptive field of attention and allowing for exploring long-\nrange dependencies. By utilizing a recurrent gating unit to\ncontinuously update the cache, our model achieves significant\nadvancements in six language and vision tasks, including lan-\nguage modeling, machine translation, ListOPs, image classi-\nfication, object detection, and instance segmentation. Further-\nmore, our approach surpasses previous memory-based tech-\nniques in tasks such as language modeling and displays the\nability to be applied to a broader range of situations.\nIntroduction\nThe design of Transformer (Vaswani et al. 2017), a deep\nmodel stacking self-attention and feed-forward layers, has\nachieved remarkable progress in various tasks. Compared\nto the traditional deep models, a key characteristic of\nTransformer is the self-attention mechanism, which enables\nglobal receptive field and allows each token to access all the\nother tokens in a data batch, providing a flexible scheme\nto capture contextual representation (Vaswani et al. 2017;\nDosovitskiy et al. 2021; Carion et al. 2020) . Such paradigm\nis however in a complexity square to sequence length, thus\nnot suitable to model long-term dependencies. In this work,\nwe aim to extend the conventional transformer models using\nattention with a long-term token representation in a mem-\nory cache, which enables larger and longer receptive field at\nminimal additional computations.\nCapturing long-range relationships between tokens and\nsamples is crucial for various tasks due to several reasons.\n(i) In sequential data such as language sentences, there can\nexist dependencies between tokens that are far away from\neach other. For example, an event or character can be re-\nferred to from time to time across multiple paragraphs in an\narticle. Failing to capture such dependencies can result in\npoor performance in natural language processing tasks. (ii)\nModeling cross-sample relationships can also be useful for\nnon-sequential data like images. For example, incorporating\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n5 20 25  10              15 \n76\n78\n80\n82\n84\nViT\nPVT\nCached PVT (ours) \nPVTv2\nCached PVTv2 (ours)\nSwin\nCvT\n(%)\n(G)\nFigure 1: Performance-Complexity Curve: Top-1 accuracy\n(%) comparisons on ImageNet with respect to model capac-\nity (FLOPs) using vision transformers (Parameter-efficiency\ncurves).Curves of our cached models are consistently on top\nof their corresponding baselines (PVT and PVTv2), indi-\ncating the effectiveness of GRC-cached models considering\nboth complexity and accuracy.\na memory module that stores prototypical feature represen-\ntations can enable instance-invariant feature learning, lead-\ning to improved performance in vision tasks (Long et al.\n2022; Deng et al. 2022). Furthermore, other studies (Wang\net al. 2020b; Zhong et al. 2019) have demonstrated that us-\ning cross-batch memory to store previous embeddings can\nbe beneficial for visual representation learning. (iii) Longer-\nrange attention has also been shown to enhance the represen-\ntation learning ability of models, as demonstrated in works\nlike (Dai et al. 2019; Wu et al. 2022; Tay et al. 2021b).\nHowever, longer dependency modeling makes computa-\ntions more expensive. For example, the vanilla Transformer\nhas O(T2) computational complexity in each attention mod-\nule when handling a token sequence of length T. Although\nsome works apply efficient alternatives, such as low-rank de-\ncomposition (Wang et al. 2020a; Zhu et al. 2021), block-\nbased sparsification (Zaheer et al. 2020), and local sensi-\ntive hashing (Kitaev, Kaiser, and Levskaya 2020), they still\nhave complexity linear to the token length (O (T)) and thus\nunable to efficiently capture sparse long-range dependency.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n16935\nAnother line of research (Wu et al. 2022) reduces the com-\nplexity of attention module by selecting top-k token pairs\nfrom a memory cache for the current tokens, but the cost\nof maintaining a huge cache of tokens for all layers is still\nsignificant. Hence, developing efficient and effective mech-\nanisms for capturing long-range dependencies remains an\nactive area of research.\nTo address these issues, we propose a novel family of\nTransformer models called Cached Transformer, which has\na Gated Recurrent Cache (GRC) that enables Transform-\ners to access historical knowledge, as ilustrated in Fig. 2.\nThe GRC is implemented as a meta-learner that compresses\nthe historical representation into embedding vectors and up-\ndates them adaptively with a gating mechanism, avoiding\nthe need for a large memory cache. The GRC updates the\npast representation with a reset gate that suppresses histor-\nical caches and an update gate that further updates the sup-\npressed caches using the current token sequences. This de-\nsign allows the GRC to access previously seen knowledge\nin a computationally efficient way. Based on the GRC, we\nimplement a semi-cached attention mechanism that attends\nto both the latent and current tokens.\nWe propose Cached Transformer with Gated Recurrent\nCache (GRC) and make the followingcontributions, which\nmake it more appealing than prior arts in several aspects.\n• GRC is built on a general differentiable formulation and\nis compatible with various attention schemes, Trans-\nformer networks, and tasks. We demonstrate that GRC\ncan be easily plugged into diverse Transformer-variants\nsuch as Transformer-XL (Dai et al. 2019), ViT (Doso-\nvitskiy et al. 2021), PVT (Wang et al. 2021, 2022),\nSwin (Liu et al. 2021) Bigbird (Zaheer et al. 2020), and\nReformer (Kitaev, Kaiser, and Levskaya 2020).\n• GRC can cache all representations of arbitrary length re-\ncurrently, independent of sequence length, while exist-\ning cache-based methods can only capture recent tokens\n(Rae et al. 2019; Dai et al. 2019) or require KNN search-\ning at each step (Wu et al. 2022).\n• Besides efficiency, GRC surpasses previous memory-\nbased methods (Dai et al. 2019; Burtsev et al. 2020; Bu-\nlatov, Kuratov, and Burtsev 2022) by a large margin on\nboth vision (Table 2) and language tasks (Table 5).\n• GRC yields consistent improvements not only in sequen-\ntial data such as texts but also in spatial context such as\nimage classification (Table 1) and object detection (Ta-\nble 3). To our knowledge, existing works of Vision Trans-\nformers mainly focused on learning intra-sample tokens,\nwhile GRC is the first attempt to model cross-sample re-\nlationships by attending over inter-sample tokens, such\nas tokens from different independent images.\n• We observe that models with GRC may attend more\nover the cache than the regular self-attention. We inves-\ntigate this behavior in image classification and find that\nGRC can separate features into two parts, attending over\ncaches yielding instance-invariant features, as well as\nattending over self, yielding instance-specific features\n(See in Fig. 4). This behavior is similar to that of a vec-\ntor prototype (Caron et al. 2020), which enables cross-\nsample regularization to avoid overfitting.\nExtensive experiments show that the Cached Transformer\nwith GRC achieves promising results on various vision\nand language Transformer backbones. (i) Language: In\nthe IWSLT14 De-En benchmark for machine translation,\nPreNormed Transformer+GRC yields 36.0 BLEU, outper-\nforming the baselines by 0.5. In the challenging long-range-\narena benchmark (Tay et al. 2021a), GRC improves state-of-\nthe-art methods with different attention types including Re-\nformer (Kitaev, Kaiser, and Levskaya 2020), Bigbird (Za-\nheer et al. 2020), and regular Transformer (Vaswani et al.\n2017) consistently by up to 1.2% accuracy. (ii) Vision: For\nimage classification on ImageNet (Krizhevsky, Sutskever,\nand Hinton 2012), we plug GRC into the recent vision trans-\nformers of different scales, such as ViT (Dosovitskiy et al.\n2021), PVT (Wang et al. 2021), PVTv2 (Wang et al. 2022),\nSwin (Liu et al. 2021), and obtain up to3.3% accuracy gain.\nAs shown in Fig. 1, our cached model with PVTv2 backbone\nachieves superior performance considering both the model\ncomplexity and accuracy. We further evaluate GRC on the\nCOCO (Lin et al. 2014) dataset for object detection and in-\nstance segmentation, where PVT+GRC can yield more than\n4.0 box AP improvement.\nRelated Works\nCached Language Models. Cache models are effective in\nlong-range modeling , and are firstly introduced by (Kupiec\n1989; Kuhn and De Mori 1990) for speech recognition. In\ngeneral, a cache model stores representations of the past,\nwhich are usually unigrams or key-value pairs for future\ncomputation. Transformer-XL (Dai et al. 2019) further ap-\nplies this technique to transformers, where the cache stores\nprevious key-value pairs in attentions from previous training\nsteps. Many memory-based methods are explored following\nTransformer-XL: For instance, MT (Burtsev et al. 2020) and\nRMT (Bulatov, Kuratov, and Burtsev 2022) use extra mem-\nory tokens to store local and global information for different\nsegments of inputs. (Rae et al. 2019) compress the tokens\nbefore they’re saved in the cache to reduce memories and\ncomputations. However, these methods often use cache in a\nfixed-length and first-in-first-out (FIFO) manner, which lim-\nits the amount of tokens that can be memorized in sequence.\nIn contrast, our proposed GRC-based Cached Transformers\nlearn to build the cache adaptively with a complexity that is\nindependent of the attention range.\nVision Transformers. Vision transformers and their vari-\nants have recently achieved remarkable success in various\nvision tasks. The original Vision Transformer (ViT) model\n(Dosovitskiy et al. 2021) was the first to split images into\npatch sequences and feed them into transformer encoders.\nHowever, existing methods focus mainly on intra-sample\ntokens, whereas our proposed GRC enhances vision trans-\nformers by learning instance-invariant features via attend-\ning over inter-sample tokens. This allows GRC-based trans-\nformers to capture richer contextual information and achieve\neven better performance on vision tasks. For a more compre-\nhensive understanding of the related literature, please refer\nto the Appendix.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n16936\nAttention\nGRC Updates\nCached Attention\nGRC Updates\n......\n......\nAttention\nSelf Attention\n......\nFigure 2: Comparisons of vanilla self-attention and cached\nattentions at training stage. The self-attention only attends\nto the token itself (X t). While in cached attention, outputs\nat training step t (denoted by Yt) are derived by attending\nover a Gated Recurrent Cache (GRC, i.e., Ct derived from\nhistorical tokens X0 to Xt), and the current token (Xt).\nMethodology\nIn this section, we first revisit the vanilla language and vi-\nsion transformer models, then introduce implementation of\nCached Transformers with Gated Recurrent Cache(GRC).\nVanilla Transformer\nWe begin with a brief review of the standard transformer\narchitecture. The transformer model (Vaswani et al. 2017)\nis constructed by stacking multi-head self-attention blocks\nand feed-forward layers which is usually a two-layer lin-\near transformation with activation. Each transformer block\nis fed with T × D input tokens, where T is the number of\ntokens and D represents the size of token embedding.\nSelf-attention mechanism. As shown in Fig.2, the self-\nattention module first projects each input X into Q (query),\nK (key), and V (value) using linear transformations. Typi-\ncally, the self-attention is performed in a multi-head manner\nwhere the input will be divided into multiple heads for par-\nallel computation. The output of the attention head h can be\nwritten as :\noh\nself = softmax(QhKT\nh /\np\nD/H)Vh, (1)\nwhere oh\nself is the output of head h of the self-attention and\nH is the number of heads. The output from heads will be\nconcatenated and then fed into another linear transforma-\ntions with normalization and residual connections.\nLimitations. As shown in Eqn.(1), the vanilla self-\nattention mechanism used in Transformers is highly sen-\nsitive to sequence length, with a computational complex-\nity of O(T2) with respect to the sequence length T. This\nmeans that the computational cost grows rapidly as the se-\nquence length increases, which limits the model’s ability\nto capture long-term relationships in the data. As a result,\nvanilla Transformers can only model relatively short se-\nquences of tokens in language tasks, and it also makes it\nchallenging to develop cross-task memory modules (Wang\net al. 2020b; Zhong et al. 2019) in a attention-based way\nfor vision tasks. Towards this issue, we introduce the pro-\nposed Cached Transformers, which provides a more flexible\nparadigm for capturing long-term dependencies, leading to\nconsistent improvements for both vision and language tasks.\nCached Transformer\nTo extend receptive fields of both language and vision trans-\nformers, in this section we will introduce our implementa-\ntions of Cached Transformers, which maintains a continu-\nous cache termed Gated Recurrent Cache (GRC) to support\nefficient long-term representation learning. The core idea\nis to hold token embedding as caches which can dynam-\nically record historical samples according to their signifi-\ncance. The Cached Transformer will then gain additional ca-\npabilities to encode both the current and accumulated infor-\nmation by attending to the gathering of caches C and inputs\nX. Such an attention scheme is described as GRC-Attention,\nand the following parts present more details.\nGeneral implementations. The proposed Cached Trans-\nformers enable attending over caches on arbitrary multi-\nlayers architectures accepting sequential inputs. Typically,\nthe Cached Transformer models can be derived by replac-\ning their self-attention blocks with the proposed GRC-\nAttention. Fig. 3 (b) gives overall illustrations of how the\nGRC-Attention is conducted.\nConsidering input sequence Xt ∈ RB×T×D, where B is\nthe batch size and t denotes training steps, GRC-attention\nattends to both the memory cache and the current tokens.\nWe formulate GRC-attention by\nOh = σ(λh) ∗ oh\nmem + (1 − σ(λh)) ∗ oh\nself , (2)\nwhere Oh and oh\nmem are the outputs of the GRC-attention\nand Cached attention (i.e., attention over memory cache)\nin the head h, respectively. oh\nself is the output of the self-\nattention in Eqn.(1). Moreover, in Eqn.(2), σ(·) is the sig-\nmoid function and λh is a head-wise learnable ratio trading\noff self-attention and Cached attention 1.\nTo construct the triplet key, query and value for Cached\nattention, we choose a portion of Xt as input ¯Xt ∈\nRB×T×Dm, which is derived by slicing Xt on channel di-\nmension. Note that Dm = rD2 indicates channels used\nfor memorizing the past tokens embedding, where r is the\ncaching ratio. With ¯Xt, the accumulated cache Ct−1 will\nthen be updated to Ct according to the GRC update rules\nas shown in Fig. 3. We describe the construction of GRC in\nSec in detail. The Cached attention can be then conducted\nby using ¯Xt as queries and Ct as keys and values, written\nas:\noh\nmem = softmax( ¯Qh ¯KT\nh /\np\nDm/H) ¯Vh, (3)\nwhere ¯Qh, ¯Kh and ¯Vh are obtained by linear projections of\nh-th head of ¯Xt, Ct and Ct respectively.\nGeneralizations. Note that while we typically formulate\nCached Transformer as a self-attention based model, it can\nalso be an arbitrary transformer variant. In other words,\nthe attention mechanism used to acquire oh\nself and oh\nmem in\n1All of the λh is initialized to be 0.\n2At most cases we adopt Dm = D\n2 to reduce the complexity\nof Cached attention , which means we choose half of the inputs to\nupdate caches\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n16937\nTokens \nCaches \nFC\nUpdated caches\nSelf-Attention\nGRC\nUpdates\nCached Attention + Outputs\n(a) GRC Updates (b) GRC-Attention\nReset caches\n: reset gates \n: update gates \nFigure 3: The illustration of proposed GRC-Attention in Cached Transformers. (a) Details of the updating process of Gated\nRecurrent Cache. The updated cache Ct is derived based on current tokens ¯Xt and cache of last step Ct−1. The reset gates gr\nreset the previous cache Ct−1 to reset cache ˜Ct, and the update gates gu controls the update intensity. (b) Overall pipeline of\nGRC-Attention. Inputs will attend over cache and themselves respectively, and the outputs are formulated as interpolation of\nthe two attention results.\nEqn.(2) can be substituted by any other attention-like func-\ntions, such as sparse attentions (Zaheer et al. 2020) or local\nhashing (Kitaev, Kaiser, and Levskaya 2020). Further ex-\nperiments will provide validations of Cached Transformers\non several transformer variants.\nGated Recurrent Cache Update\nThis section describes the formulation and updating of pro-\nposed Gated Recurrent Cache (GRC).\nCache Initialization. The GRC is characterized to be\nfixed-length vectors Ct ∈ RTm×Dm. Unlike previous works\nthat formulate cache to be tokens or words directly (Tu et al.\n2018; Dai et al. 2019), GRC embeds historical tokens im-\nplicitly. By learning to embed arbitrary length samples into\nCt, GRC allows traversing caches in constant time that is\nindependent of the number of memorized tokens. The cache\nC0 will be initialized to be Tm-length zero vectors before\ntraining, and then updated as depicted in Fig. 3(a).\nGating Mechanism. Inspired by gated RNNs (Cho et al.\n2014), we adopt the gating mechanism to enable GRC to\ndynamically capture dependencies at different time scales.\nSpecifically, the updating process of Ct is filtered by update\ngates gu and reset gates gr. Considering updating GRC at\ntime step t, we first calculate the gates gu and gr:\ngu = σ(Wu[ ¯Xt, Ct−1]) and gr = σ(Wr[ ¯Xt, Ct−1]), (4)\nwhere σ denotes sigmoid function and [·, ·] concatenates to-\nkens in channel dimension. For valid concatenation, ¯Xt is\ninterpolated into a Tm-by-Dm token. The updated cache Ct\nis formulated by a linear interpolation as given by:\nCt = (1 − gu)Ct−1 + gu ˜Ct and ˜Ct = Wc[ ¯Xt, gr ⊙ Ct−1]\n(5)\nwhere ⊙ is element-wise multiplication. In above process,\nthe update gates gu decides how much current sample ¯Xt\nupdates the cache and the reset gates gr suppress the accu-\nmulated cache to forget unimportant components. Note that\nshape of the derived Ct is B × Tm × Dm as Xt is involved,\nand we therefore average across the batch dimension to fit\nthe cache size.\nArchitecture Top-1 (%) Top-5 (%) ∆ Top-1 (%)\nViT-S 79.9 95.0 -\nViT-S (Cached) 81.3 95.5 + 1.4\nPVT-T 75.1 92.3 -\nPVT-T (Cached) 78.4 94.2 + 3.3\nPVT-S 79.9 95.0 -\nPVT-S (Cached) 81.8 95.9 + 1.9\nPVT-M 81.2 95.7 -\nPVT-M (Cached) 83.0 96.4 + 1.8\nSwin-T 81.2 95.5 -\nSwin-T (Cached) 82.1 95.9 + 0.9\nPVTv2-B2 82.0 95.9 -\nPVTv2-B2 (Cached) 82.6 96.2 + 0.6\nPVTv2-B 83.2 96.3 -\nPVTv2-B3 (Cached) 83.7 96.4 + 0.5\nPVTv2-B4 83.6 96.3 -\nPVTv2-B4 (Cached ) 84.1 96.6 + 0.5\nTable 1: Performance of various Cached Transformers eval-\nuated on ImageNet. ”(Cached)” indicates models imple-\nmented with the proposed GRC-Attention. Top-1 / Top-5 /∆\nTop-1 denotes top-1 accuracy / top-5 accuracy / top-1 accu-\nracy difference respectively. The cached models outperform\ntheir corresponding baselines consistently.\nExperiments\nThis section extensively evaluates the effectiveness of the\nproposed Cached Transformer and Gated Recurrent Cache\n(GRC) in both vision and language tasks, including language\nmodeling on WikiText-103, Long Listops of Long Range\nArena (Tay et al. 2021a), machine translation on IWSLT14\n(Cettolo et al. 2014) / IWSLT15 (Cettolo et al. 2015), im-\nage classification on ImageNet (Krizhevsky, Sutskever, and\nHinton 2012), and object detection and instance segmenta-\ntion on COCO2017 (Lin et al. 2014). In addition, as the\ncached models are newly introduced to vision transformers,\nwe also perform thorough discussions on the role of the pro-\nposed caches and their significance. All of the experiments\nare conducted on Tesla V100 GPUs.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n16938\nInputs\nSelf-Attention \nCached Attention \nFigure 4: Visualizations of averaged features output from self-attention and cached attention, which is obtained by feeding\nimages of ImageNet validation sets to trained cached ViT-S. The results are obtained by averaging features over channel(and\nhead) dimension. Both ¯oself and ¯omem are unflattened to 14 × 14 for better comparisons. Dark pixels mean small values.\nImage Classification\nExperiments Setup. We first evaluate our methods on\nImagenet-1k for image classification. We implement our\nGRC-Attention as a general pytorch module which main-\ntains fixed-length buffers as cache. In image classification\ntask, we set the cache ratior to be 0.5 and keep cache length\nTm equal to the length of image patches T. For fair com-\nparisons, we directly replace the self-attention layers in cor-\nresponding transformers with our GRC-Attention module\nwithout varying the architecture and hyperparameters. To\nmaintain spatial token structures, we add positional encod-\nings to our proposed GRC-Attention like other vision trans-\nformers. Both the baselines and their cached counterparts are\ntrained with 224 × 224 size inputs using 16 GPUs. To fully\nvalidate the proposed cache mechanism, we evaluate GRC-\nAttention on four recent vision transformers including: ViTs\n(Dosovitskiy et al. 2021), PVT (Wang et al. 2021), Swin-\nTransformer (Liu et al. 2021) and PVT-v2 (Wang et al.\n2022). Without bells and whistles, all of the training settings\nfor cached models are kept consistent with the original base-\nlines including data augmentation, optimizer type, learning\nrates and training epochs.\nClassification Results. Table 1 reports overall perfor-\nmance of cached transformers on corresponding baselines.\nAs shown, transformers implemented with GRC-Attention\nconsistently outperform their no-cache counterparts by\nyielding significantly higher accuracy, demonstrating the ef-\nfectiveness of our proposed caching mechanism. For in-\nstance, by enabling cache, PVT-Tiny can achieve78.4% top-\n1 accuracy and 94.2% top-5 accuracy, surpassing the orig-\ninal PVT-Tiny by 3.3% and 1.9% respectively. Moreover,\neven for the recent stronger backbone PVTv2, our proposed\ncached mechanism can still keep> 0.5 top-1 improvements.\nComplexity Analysis. In current settings where cache ra-\ntio r = 0.5, replacing all the attention layers with GRC-\nAttention will cost approximately an extra 10% − 15%\nFLOPs and Params. Considering the performance improve-\nments, the extra computations are acceptable (See in Fig. 1)\nand more efficient than increasing the depth and width of\nmodels.\nSignificance of Cached Attention. To verify that the\nabove performance gains mainly come from attending over\ncaches, we analyze the contribution of omem by visualizing\nthe learnable attention ratio σ(λh). Please be reminded that\nin Eq 2, outputs of GRC-Attention is derived by interpolat-\ning outputs of cached attentionoh\nmem and self-attention oh\nself\naccording to σ(λh). Hence, σ(λh) can be used to represent\nthe relative significance of oh\nmem and oh\nself . Fig. 5 depicts\nthe learned σ(λh) for each head respect to layers in ViT-S,\nPVT-Tiny and PVT-Small. As we can see, for more than half\nof the layers, σ(λh) is larger than 0.5, denoting that outputs\nof those layers are highly dependent on the cached attention.\nBesides, we also notice an interesting fact that the models al-\nways prefer more cached attention except for the last several\nlayers. This makes us curious about the roles of cached at-\ntention: what is the feature that models actually learn by at-\ntending over caches? The following paragraph answers this\nquestion.\nRoles of Cached Attention. We investigate the function\nof GRC-Attention by visualizing their interior feature maps.\nWe choose the middle layers of cached ViT-S, averaging\nthe outputs from self-attention oself and cached attention\n(omem) across the head and channel dimension, and then\nnormalizing them into [0, 1]. The corresponding results are\ndenoting as ¯oself and ¯omem, respectively. Fig. 4 provides\nvisualizations of ¯oself and ¯omem obtained by feedings im-\nages of ImageNet validation sets to trained cached ViT-S.\nAs ¯oself and ¯omem are sequences of patches, they are unflat-\ntened to 14×14 shape for better comparison. From Fig. 4 we\ncan see, features derived by the above two attentions are vi-\nsually complementary. In GRC-Attention, omem is derived\nby attending over the proposed cache (GRC) containing\ncompressive representations of historical samples, and thus\nbeing adept in recognizing public and frequently showing-\nup patches of this class. While for oself from self-attention\nbranch, it can focus on finding out more private andcharac-\nteristic features of current instance.\nWith above postulates, we can attempt to explain the reg-\nularity of σ(λh) in Fig. 5: employing more omem (larger\nσ(λh) ) in former layers can help the network to distinguish\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n16939\nFigure 5: Visualizations of learnedσ(λh) for each head with respect to layer number (from shallow to deep) in different models:\nViT-S, PVT-Tiny and PVT-Small. Note that the ViT-S has 6 heads for all the layers, while PVT-Tiny and PVT-Small adopt a\nprogressive head strategy where head numbers increase from 1 to 8 gradually. Circles with different colors denote those different\nheads. σ(λh) controls the interpolation ratio of cached attention outputsomem which reflects head-wise contributions of cached\nattention to the final outputs. Note that σ(λh) > 0.5 means cached attention contributes more than self-attention. As shown, in\nall of the three models, σ(λh) > 0.5 holds for more than half of the GRC-Attention layers, deducing that the model outputs are\nsignificantly dependent on the cache.\nModel No cache Attention-based cache\nGRC\nViT-S 79.9 80.0 81.3\nPVT-T 75.1 74.8 78.4\nPVT-S 79.9 79.6 81.8\nTable 2: Performance(Top-1 Accuracy) comparisons of\ncached models using GRC and attention-based\nArchitecture APb APb\n50 APb\n75\nAPm APm\n50 APm\n75\nPVT-Tiny 36.7 59.2 39.3 35.1 56.7 37 .3\n+\nCached 41.0 63.4 44.8 38.3 60.4 41.1\nPVT-Small 40.4 62.9 43.8 36.3 60.1 40 .3\n+\nCached 44.5 67.1 48.6 41.0 64.0 44.1\nPVT-Medium 42.0 64.4 45.6 39.0 61.6 42 .1\n+\nCached 46.6 68.2 51.0 42.3 65.3 45.5\nTable 3: Object detection and instance segmentation perfor-\nmance on COCO val2017 following Mask R-CNN 1× set-\ntings.\nthis instance coarsely, and employing more oself (smaller\nσ(λh)) enable the model to make fine-grained decision.\nCross-sample regularization. The above paragraph also\nshows that our proposed cache performs similarly to vector\nprototypes (Caron et al. 2020), storing public features of\nthe same class implicitly and allowing models to classify in-\nputs with both the public and characteristic representations.\nIn such a way, the predictions are not only dependent on the\ncurrent inputs but also on related cached samples, thus pro-\nviding a cross-sample regularization to avoid overfitting.\nGRC v.s. other memory-based methods. We perform\nfurther ablations to compare GRC and attention-based\nmemory for image classification in ImageNet-1k. We de-\nploy Transformer-XL-style caches to Vision Transform-\ners(including ViT-S, PVT-Tiny and PVT-Small) and com-\npare them to corresponding GRC-cached models. As shown\nin Table 2, GRC-cached models consistently outperform\ntheir attention-based cache and no-cache counterparts. Be-\nsides, it can be noted that the attention-based cache can\nhardly improve the model performance.\nObject Detection and Instance Segmentation.\nExperiments Setup. We further assess the generalization\nof our GRC-Attention on object detection / instance segmen-\ntation track using COCO2017 dataset (Lin et al. 2014). The\nmodels are trained on the COCO train2017 (118k images)\nand evaluated on val2017 (5k images). We use the cached\nPVT as backbone and adopt the Mask R-CNN detector (He\net al. 2017) to verify the effectiveness of GRC-Attention.\nThe standard COCO metrics of Average Precision (AP) for\nbounding box detection (APbb) and instance segmentation\n(APm) are used to evaluate our methods. All of the train-\ning settings and hyperparameters are kept the same as PVT\noriginal implementation (Wang et al. 2021), and all of the\ninvolved models are trained for 12 epochs using 8 GPUs.\nFor both the cached PVT and baselines, backbones are firstly\npretrained on ImageNet and then fine-tuned for detection.\nResuts. As shown in Table 3, when using Mask R-CNN\nfor object detection, the cached PVTs significantly outper-\nform their baselines. For example, the AP of cached PVT-\nMedium is 4.6 (46.6 vs. 42.0) points better than its no-cache\ncounterparts. Similar results can also be found in instance\nsegmentation results, where cached PVT-Medium achieves\n3.3 higher AP m (39.0 vs. 42.3). These results demonstrate\nthe generalizability of the proposed caching mechanism.\nLanguage Modeling\nExperimental Setup In this work, we conduct exper-\niments to compare the performance of Gated Recurrent\nCache (GRC) with Transformer-XL (Dai et al. 2019) on a\nlanguage modeling task using the WikiText-103 benchmark.\nTo implement GRC-cached language models, we use the\npublicly available fairseq framework and follow the default\nmemory-based Transformer-XL configurations as our base-\nlines, including model architecture and training settings. To\nensure a fair comparison, we compare GRC-cached mod-\nels with two other memory-based methods, Memory Trans-\nfomer (MT) (Burtsev et al. 2020) and Recurrent Mem-\nory Transformer (RMT) (Bulatov, Kuratov, and Burtsev\n2022). We implement GRC-cached models by replacing the\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n16940\nArchitecture IWSLT14 IWSLT15\nDe-En Es-En En-Fr De-En En-Vi Cs-En\nTransformer 35.5 41.4 41.5 36.1 29.8 28.8\nTransformer (GRC-cached) 36.0(+ 0.5) 41.8(+ 0.4) 41.7(+ 0.2) 36.3(+ 0.2) 30.2(+ 0.4) 29.4(+ 0.6)\nTable 4: Neural machine translation results using Pre-Norm Transformers in terms of BLEU scores.\nArchitecture No cache MT RM\nT GRC\nTransformer-XLbase 24.0 23.99 23.95 22.9\nTransformer\n-XLlarge 18.3 - - 17.9\nTable 5: Comparison of performance(Test PPL) for GRC\nand other Memory-based methods (Burtsev et al. 2020; Bu-\nlatov, Kuratov, and Burtsev 2022) on WikiText-103. The\nsmaller is better. GRC outperform Transformer-XL and pre-\nvious memory-based methods for language modeling by a\nlarge margin of 1.1 PPL.\ncaching scheme with the GRC approach while keeping all\ndata augmentation and hyper-parameters unchanged for a\nmore fair comparison.\nComparison to Other Memory-Based Methods We\npresent the performance of GRC-cached models com-\npared to Transformer-XL baselines and other memory-based\nmethods in Table 5. The results show that GRC-cached mod-\nels outperform Transformer-XL and other memory-based\nmethods in terms of perplexity on both base and large-scale\nmodels. For instance, GRC-cached Transformer-XL base\nachieves up to 1.1 lower PPL compared to the baseline\nTransformer-XL and 1.05 lower PPL to the RMT, demon-\nstrating the superiority of GRC over previous memory-based\nTransformer methods.\nLong Range Arena\nExperiments Setup. We extensively conduct experiments\non recently proposed Long Range Arena (LRA) benchmarks\n(Tay et al. 2021a) to validate our proposed methods under\nthe long-context scenario. To demonstrate the long-range se-\nquence modeling capability of GRC-Attention and the cor-\nresponding cache mechanism, we choose the challenging\nLong ListOps task in LRA, which is a longer variation of\nListOps task (Nangia and Bowman 2018) with up to 2k\nlength sequences and considerablely difficult. In this task,\nwe also extend GRC-Attention to efficient attention variants\nby replacing the self-attention function (See section ). Con-\ncretely, we compare GRC-Attention to their no-cache coun-\nterparts on baselines including Transformer (Vaswani et al.\n2017), BigBird (Zaheer et al. 2020) and Reformer (Kitaev,\nKaiser, and Levskaya 2020). For those efficient attentions\nlike BigBird and Reformer, we only import gated recurrent\ncache and maintain their inner attention function unchanged.\nAll of the experiments are under default settings in (Tay\net al. 2021a).\nResults. Table 6 reports Long ListOps results. As shown,\ncached models consistently outperform their baselines (in-\ncluding the SOTA methods Reformer) significantly. For in-\nstance, by employing GRC, BigBird model can achieve 1.39\nhigher accuracy. These results show the long-range sequence\nmodeling ability of GRC as well as its generalizability to\nother attention variants.\nArchitecture baseline GRC-cached ∆\nTransformer 36.23 37.40 + 1.17\nBigBird 36.06 37.45 + 1.39\nReformer 37.27 37.85 + 0.58\nTable 6: Results on Long ListOPs task in LRA in terms\nof accuracy. The ”cached” column indicates cached mod-\nels whose attention layers are implemented as generalized\nGRC-Attention. ∆ denotes the difference between proposed\ncached models and baselines.\nNeural Machine Translation\nExperiments Setups. We experiment our methods on\nwidely used public datasets IWSLT14 and IWSLT15. Mul-\ntiple language sources3are included to fully verify effective-\nness of the proposed GRC, and models are trained for each\ntrack individually. We adopt the Pre-Norm Transformer set-\ntings in (Wang et al. 2019) and implement the models using\nfairseq-py (Ott et al. 2019) framework. Following (Wang\net al. 2019; Ott et al. 2019), we generally increase the learn-\ning rates by 2 and average the last 10 checkpoints for in-\nference. We employ the proposed GRC-cached models by\nreplacing all attention modules of transformer encoder lay-\ners with GRC-Attention. The cache length Tm is set to be\n64 for all cached models. All the transformers in this task\nare using six encoder layers and six decoder layers. For a\nfair comparison, both the baselines and cached models are\ntrained under identical settings.\nResults. We use BLEU (Papineni et al. 2002) as evalua-\ntion metrics and compare GRC cached transformers to their\nbaselines in Table 4. It can be seen that consistent improve-\nments can be reached by applying GRC-Attention to base-\nlines. For tracks like IWSLT14 De-En and IWSLT15 Cs-En,\nthe increments can achieve 0.5/0.6 points, which is actually\nsignificant for these tasks.\nDiscussion\nWe introduce Cached Transformer with Gated Recurrent\nCache (GRC), a simple extension to Transformer-based\nmodels that significantly increases the length of attention\ncontext by allowing access to historical states through a gat-\ning mechanism. GRC embeds previous tokens, whether they\nare close or distant, as fixed-length vectors, without com-\nplexity dependence on the number of cached tokens. Con-\nsequently, GRC model token dependencies over a broader\nrange of input, resulting in improved accuracy and perfor-\nmance across diverse Transformers-variants with different\narchitectures and attention functions, on a variety of vision\nand language tasks.\n3IWSLT14: German-English(De-En), Spanish-English(Es-En)\nand English-French(En-Fr), IWSLT15: German-English(De-En),\nEnglish-Vietnamese(En-Vi) and Czech-English(Cs-En)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n16941\nReferences\nBulatov, A.; Kuratov, Y .; and Burtsev, M. 2022. Recurrent\nmemory transformer. Advances in Neural Information Pro-\ncessing Systems, 35: 11079–11091.\nBurtsev, M. S.; Kuratov, Y .; Peganov, A.; and Sapunov,\nG. V . 2020. Memory transformer. arXiv preprint\narXiv:2006.11527.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In European conference on computer vi-\nsion, 213–229. Springer.\nCaron, M.; Misra, I.; Mairal, J.; Goyal, P.; Bojanowski, P.;\nand Joulin, A. 2020. Unsupervised learning of visual fea-\ntures by contrasting cluster assignments. Advances in Neu-\nral Information Processing Systems, 33: 9912–9924.\nCettolo, M.; Niehues, J.; St ¨uker, S.; Bentivogli, L.; Cattoni,\nR.; and Federico, M. 2015. The IWSLT 2015 Evaluation\nCampaign. In Proceedings of the 12th International Work-\nshop on Spoken Language Translation: Evaluation Cam-\npaign, 2–14. Da Nang, Vietnam.\nCettolo, M.; Niehues, J.; St¨uker, S.; Bentivogli, L.; and Fed-\nerico, M. 2014. Report on the 11th IWSLT evaluation cam-\npaign. In Proceedings of the 11th International Workshop on\nSpoken Language Translation: Evaluation Campaign, 2–17.\nLake Tahoe, California.\nCho, K.; Van Merri ¨enboer, B.; Bahdanau, D.; and Ben-\ngio, Y . 2014. On the properties of neural machine\ntranslation: Encoder-decoder approaches. arXiv preprint\narXiv:1409.1259.\nDai, Z.; Yang, Z.; Yang, Y .; Carbonell, J.; Le, Q. V .; and\nSalakhutdinov, R. 2019. Transformer-xl: Attentive lan-\nguage models beyond a fixed-length context. arXiv preprint\narXiv:1901.02860.\nDeng, W.; Marsh, J.; Gould, S.; and Zheng, L. 2022. Fine-\nGrained Classification via Categorical Memory Networks.\nIEEE Transactions on Image Processing, 31: 4186–4196.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In International Conference on Learn-\ning Representations.\nHe, K.; Gkioxari, G.; Doll´ar, P.; and Girshick, R. 2017. Mask\nr-cnn. In Proceedings of the IEEE international conference\non computer vision, 2961–2969.\nKitaev, N.; Kaiser, Ł.; and Levskaya, A. 2020. Reformer:\nThe efficient transformer. arXiv preprint arXiv:2001.04451.\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Im-\nagenet classification with deep convolutional neural net-\nworks. Advances in neural information processing systems,\n25.\nKuhn, R.; and De Mori, R. 1990. A cache-based natural\nlanguage model for speech recognition. IEEE transactions\non pattern analysis and machine intelligence, 12(6): 570–\n583.\nKupiec, J. 1989. Probabilistic models of short and long\ndistance word dependencies in running text. In Speech\nand Natural Language: Proceedings of a Workshop Held at\nPhiladelphia, Pennsylvania, February 21-23, 1989.\nLin, T.-Y .; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-\nmanan, D.; Doll ´ar, P.; and Zitnick, C. L. 2014. Microsoft\ncoco: Common objects in context. In European conference\non computer vision, 740–755. Springer.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vi-\nsion transformer using shifted windows. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion, 10012–10022.\nLong, A.; Yin, W.; Ajanthan, T.; Nguyen, V .; Purkait, P.;\nGarg, R.; Blair, A.; Shen, C.; and van den Hengel, A. 2022.\nRetrieval augmented classification for long-tail visual recog-\nnition. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 6959–6969.\nNangia, N.; and Bowman, S. R. 2018. Listops: A di-\nagnostic dataset for latent tree learning. arXiv preprint\narXiv:1804.06028.\nOtt, M.; Edunov, S.; Baevski, A.; Fan, A.; Gross, S.; Ng, N.;\nGrangier, D.; and Auli, M. 2019. fairseq: A Fast, Extensible\nToolkit for Sequence Modeling. In NAACL-HLT (Demon-\nstrations).\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBleu: a Method for Automatic Evaluation of Machine Trans-\nlation. In ACL.\nRae, J. W.; Potapenko, A.; Jayakumar, S. M.; and Lillicrap,\nT. P. 2019. Compressive transformers for long-range se-\nquence modelling. arXiv preprint arXiv:1911.05507.\nTay, Y .; Dehghani, M.; Abnar, S.; Shen, Y .; Bahri, D.; Pham,\nP.; Rao, J.; Yang, L.; Ruder, S.; and Metzler, D. 2021a. Long\nRange Arena : A Benchmark for Efficient Transformers. In\nInternational Conference on Learning Representations.\nTay, Y .; Dehghani, M.; Aribandi, V .; Gupta, J.; Pham, P. M.;\nQin, Z.; Bahri, D.; Juan, D.-C.; and Metzler, D. 2021b.\nOmninet: Omnidirectional representations from transform-\ners. In International Conference on Machine Learning,\n10193–10202. PMLR.\nTu, Z.; Liu, Y .; Shi, S.; and Zhang, T. 2018. Learning to re-\nmember translation history with a continuous cache. Trans-\nactions of the Association for Computational Linguistics, 6:\n407–420.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. Advances in neural information pro-\ncessing systems, 30.\nWang, Q.; Li, B.; Xiao, T.; Zhu, J.; Li, C.; Wong, D. F.; and\nChao, L. S. 2019. Learning deep transformer models for\nmachine translation. arXiv preprint arXiv:1906.01787.\nWang, S.; Li, B. Z.; Khabsa, M.; Fang, H.; and Ma, H.\n2020a. Linformer: Self-attention with linear complexity.\narXiv preprint arXiv:2006.04768.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n16942\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2021. Pyramid vision trans-\nformer: A versatile backbone for dense prediction without\nconvolutions. InProceedings of the IEEE/CVF International\nConference on Computer Vision, 568–578.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2022. PVT v2: Improved base-\nlines with Pyramid Vision Transformer. Computational Vi-\nsual Media, 1–10.\nWang, X.; Zhang, H.; Huang, W.; and Scott, M. R. 2020b.\nCross-batch memory for embedding learning. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 6388–6397.\nWu, Y .; Rabe, M. N.; Hutchins, D.; and Szegedy,\nC. 2022. Memorizing transformers. arXiv preprint\narXiv:2203.08913.\nZaheer, M.; Guruganesh, G.; Dubey, K. A.; Ainslie, J.; Al-\nberti, C.; Ontanon, S.; Pham, P.; Ravula, A.; Wang, Q.; Yang,\nL.; et al. 2020. Big bird: Transformers for longer sequences.\nAdvances in Neural Information Processing Systems, 33:\n17283–17297.\nZhong, Z.; Zheng, L.; Luo, Z.; Li, S.; and Yang, Y . 2019.\nInvariance matters: Exemplar memory for domain adaptive\nperson re-identification. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\n598–607.\nZhu, C.; Ping, W.; Xiao, C.; Shoeybi, M.; Goldstein, T.;\nAnandkumar, A.; and Catanzaro, B. 2021. Long-short trans-\nformer: Efficient transformers for language and vision. Ad-\nvances in Neural Information Processing Systems, 34.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n16943"
}