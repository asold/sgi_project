{
    "title": "Segmentation Transformer: Object-Contextual Representations for Semantic Segmentation",
    "url": "https://openalex.org/W3159637683",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2374254801",
            "name": "Yuan, Yuhui",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2364482120",
            "name": "Chen, Xiaokang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2186661828",
            "name": "Chen Xi-lin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2350795573",
            "name": "Wang Jingdong",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3022478135",
        "https://openalex.org/W2993235622",
        "https://openalex.org/W2963236837",
        "https://openalex.org/W2125215748",
        "https://openalex.org/W2963266682",
        "https://openalex.org/W2964252655",
        "https://openalex.org/W2964091144",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W1817277359",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W2781228439",
        "https://openalex.org/W2990775046",
        "https://openalex.org/W2963727650",
        "https://openalex.org/W2630837129",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2963073398",
        "https://openalex.org/W2963318290",
        "https://openalex.org/W2981899103",
        "https://openalex.org/W2584471766",
        "https://openalex.org/W2903703378",
        "https://openalex.org/W2115150266",
        "https://openalex.org/W2997473137",
        "https://openalex.org/W2103897297",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3023001672",
        "https://openalex.org/W2509680863",
        "https://openalex.org/W2991471181",
        "https://openalex.org/W2965182628",
        "https://openalex.org/W2962802951",
        "https://openalex.org/W2888340395",
        "https://openalex.org/W3096653763",
        "https://openalex.org/W3009224666",
        "https://openalex.org/W2737258237",
        "https://openalex.org/W2963978393",
        "https://openalex.org/W2598915960",
        "https://openalex.org/W2891778567",
        "https://openalex.org/W2799213142",
        "https://openalex.org/W2886667086",
        "https://openalex.org/W2940262938",
        "https://openalex.org/W2948080074",
        "https://openalex.org/W2990032492",
        "https://openalex.org/W2965391153",
        "https://openalex.org/W2022508996",
        "https://openalex.org/W2989684653",
        "https://openalex.org/W2964309882",
        "https://openalex.org/W2964254867",
        "https://openalex.org/W2561196672",
        "https://openalex.org/W2910628332",
        "https://openalex.org/W2963971305",
        "https://openalex.org/W2129259959",
        "https://openalex.org/W2890779863",
        "https://openalex.org/W2560023338",
        "https://openalex.org/W2955058313",
        "https://openalex.org/W2798721181",
        "https://openalex.org/W2892220259",
        "https://openalex.org/W2536208356",
        "https://openalex.org/W2088049833",
        "https://openalex.org/W2921781974",
        "https://openalex.org/W2780664485",
        "https://openalex.org/W2340897893",
        "https://openalex.org/W2412782625",
        "https://openalex.org/W2985276900",
        "https://openalex.org/W2963840672",
        "https://openalex.org/W1903029394",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W2928684767",
        "https://openalex.org/W2600144439",
        "https://openalex.org/W2890782586",
        "https://openalex.org/W2965853874",
        "https://openalex.org/W2963319519",
        "https://openalex.org/W2981689412",
        "https://openalex.org/W2955813853",
        "https://openalex.org/W2999219213",
        "https://openalex.org/W2991062542",
        "https://openalex.org/W2883779225",
        "https://openalex.org/W2593403058",
        "https://openalex.org/W2960251737",
        "https://openalex.org/W3028392891"
    ],
    "abstract": "In this paper, we address the semantic segmentation problem with a focus on the context aggregation strategy. Our motivation is that the label of a pixel is the category of the object that the pixel belongs to. We present a simple yet effective approach, object-contextual representations, characterizing a pixel by exploiting the representation of the corresponding object class. First, we learn object regions under the supervision of ground-truth segmentation. Second, we compute the object region representation by aggregating the representations of the pixels lying in the object region. Last, % the representation similarity we compute the relation between each pixel and each object region and augment the representation of each pixel with the object-contextual representation which is a weighted aggregation of all the object region representations according to their relations with the pixel. We empirically demonstrate that the proposed approach achieves competitive performance on various challenging semantic segmentation benchmarks: Cityscapes, ADE20K, LIP, PASCAL-Context, and COCO-Stuff. Cityscapes, ADE20K, LIP, PASCAL-Context, and COCO-Stuff. Our submission \"HRNet + OCR + SegFix\" achieves 1-st place on the Cityscapes leaderboard by the time of submission. Code is available at: https://git.io/openseg and https://git.io/HRNet.OCR. We rephrase the object-contextual representation scheme using the Transformer encoder-decoder framework. The details are presented in~Section3.3.",
    "full_text": "Segmentation Transformer: Object-Contextual\nRepresentations for Semantic Segmentation⋆\nYuhui Yuan1,2,3, Xiaokang Chen3, Xilin Chen1,2, and Jingdong Wang3\n1 Key Lab of Intelligent Information Processing of Chinese Academy of Sciences\n(CAS), Institute of Computing Technology, CAS\n2 University of Chinese Academy of Sciences\n3 Microsoft Research Asia\n{yuhui.yuan, v-xiaokc, jingdw}@microsoft.com, xlchen@ict.ac.cn\nAbstract. In this paper, we study the context aggregation problem in\nsemantic segmentation. Motivated by that the label of a pixel is the\ncategory of the object that the pixel belongs to, we present a simple\nyet eﬀective approach, object-contextual representations, characteriz-\ning a pixel by exploiting the representation of the corresponding ob-\nject class. First, we learn object regions under the supervision of the\nground-truth segmentation. Second, we compute the object region rep-\nresentation by aggregating the representations of the pixels lying in\nthe object region. Last, we compute the relation between each pixel\nand each object region, and augment the representation of each pixel\nwith the object-contextual representation which is a weighted aggre-\ngation of all the object region representations. We empirically demon-\nstrate our method achieves competitive performance on various bench-\nmarks: Cityscapes, ADE20K, LIP, PASCAL-Context and COCO-Stuﬀ.\nOur submission “HRNet + OCR + SegFix” achieves the 1st place on the\nCityscapes leaderboard by the ECCV 2020 submission deadline. Code is\navailable at: https://git.io/openseg and https://git.io/HRNet.OCR.\nWe rephrase the object-contextual representation scheme using the\nTransformer encoder-decoder framework. The ﬁrst two steps, object re-\ngion learning and object region representation computation, are inte-\ngrated as the cross-attention module in the decoder: the linear projec-\ntions used to classify the pixels, i.e., generate the object regions, are\ncategory queries, and the object region representations are the cross-\nattention outputs. The last step is the cross-attention module we add to\nthe encoder, where the keys and values are the decoder output and the\nqueries are the representations at each position. The details are presented\nin Section 3.3.\nKeywords: Segmentation Transformer; Semantic Segmentation; Con-\ntext Aggregation\n⋆ The OCR (Object-Contextual Representation) approach is equivalent to the Trans-\nformer encoder-decode scheme. We rephrase the OCR approach using the Trans-\nformer language.\narXiv:1909.11065v6  [cs.CV]  30 Apr 2021\n2 Yuhui Yuan, Xiaokang Chen, Xilin Chen, Jingdong Wang\n1 Introduction\nSemantic segmentation is a problem of assigning a class label to each pixel for\nan image. It is a fundamental topic in computer vision and is critical for var-\nious practical tasks such as autonomous driving. Deep convolutional networks\nsince FCN [47] have been the dominant solutions. Various studies have been\nconducted, including high-resolution representation learning [7,54], contextual\naggregation [80,6] that is the interest of this paper, and so on.\nThe context of one position typically refers to a set of positions, e.g., the sur-\nrounding pixels. The early study is mainly about the spatial scale of contexts, i.e.,\nthe spatial scope. Representative works, such as ASPP [6] and PPM [80], exploit\nmulti-scale contexts. Recently, several works, such as DANet [18], CFNet [77]\nand OCNet [72,71], consider the relations between a position and its contextual\npositions, and aggregate the representations of the contextual positions with\nhigher weights for similar representations.\nCityscapes ADE20K PASCAL-Context COCO-Stuﬀ\n0\n20\n40\n60\n80\n100\n75.8\n39.7\n45.8\n32.6\n79.6\n44.3\n53.3\n38.4\n88.8\n54.3\n63.7\n46.1\nmIoU (%)\nBaseline\nOCR\nGT-OCR\nFig. 1: Illustrating the eﬀectiveness of our OCR scheme. GT-OCR estimates\nthe ideal object-contextual representations through exploiting the ground-truth, which\nis the upper-bound of our method. OCR reports the performance of our proposed\nobject-contextual representations. The three methods, Baseline, OCR and GT-OCR,\nuse the dilated ResNet-101 with output stride 8 as the backbone. We evaluate their\n(single-scale) segmentation results on Cityscapes val, ADE20K val, PASCAL-Context\ntest and COCO-Stuﬀ test separately.\nWe propose to investigate the contextual representation scheme along the\nline of exploring the relation between a position and its context. The motiva-\ntion is that the class label assigned to one pixel is the category of the object 4\nthat the pixel belongs to . We aim to augment the representation of one pixel\nby exploiting the representation of the object region of the corresponding class.\nThe empirical study, shown in Fig. 1, veriﬁes that such a representation aug-\nmentation scheme, when the ground-truth object region is given, dramatically\nimproves the segmentation quality 5.\n4 We use “object” to represent both “things” and “stuﬀ” following [16,53].\n5 See Section 3.4 for more details.\nSegmentation Transformer: OCR for Semantic Segmentation 3\nOur approach consists of three main steps. First, we divide the contextual\npixels into a set of soft object regions with each corresponding to a class, i.e.,\na coarse soft segmentation computed from a deep network (e.g., ResNet [25] or\nHRNet [54]). Such division is learned under the supervision of the ground-truth\nsegmentation. Second, we estimate the representation for each object region by\naggregating the representations of the pixels in the corresponding object region.\nLast, we augment the representation of each pixel with the object-contextual\nrepresentation (OCR). The OCR is the weighted aggregation of all the object\nregion representations with the weights calculated according to the relations\nbetween pixels and object regions.\nThe proposed OCR approach diﬀers from the conventional multi-scale con-\ntext schemes. Our OCR diﬀerentiates the same-object-class contextual pixels\nfrom the diﬀerent-object-class contextual pixels, while the multi-scale context\nschemes, such as ASPP [6] and PPM [80], do not, and only diﬀerentiate the\npixels with diﬀerent spatial positions. Fig. 2 provides an example to illustrate\nthe diﬀerences between our OCR context and the multi-scale context. On the\nother hand, our OCR approach is also diﬀerent from the previous relational con-\ntext schemes [64,18,72,75,77]. Our approach structures the contextual pixels into\nobject regions and exploits the relations between pixels and object regions. In\ncontrast, the previous relational context schemes consider the contextual pixels\nseparately and only exploit the relations between pixels and contextual pix-\nels [18,72,77] or predict the relations only from pixels without considering the\nregions [75].\nWe evaluate our approach on various challenging semantic segmentation\nbenchmarks. Our approach outperforms the multi-scale context schemes, e.g.,\nPSPNet, DeepLabv3, and the recent relational context schemes, e.g., DANet,\nand the eﬃciency is also improved. Our approach achieves competitive perfor-\nmance on ﬁve benchmarks: 84.5% on Cityscapes test, 45.66% on ADE20K val,\n56.65% on LIP val, 56 .2% on PASCAL-Context test and 40 .5% on COCO-\nStuﬀ test. Besides, we extend our approach to Panoptic-FPN [30] and verify\nthe eﬀectiveness of our OCR on the COCO panoptic segmentation task, e.g.,\nPanoptic-FPN + OCR achieves 44.2% on COCO val.\n2 Related Work\nMulti-scale context. PSPNet [80] performs regular convolutions on pyramid\npooling representations to capture the multi-scale context. The DeepLab se-\nries [5,6] adopt parallel dilated convolutions with diﬀerent dilation rates (each\nrate captures the context of a diﬀerent scale). The recent works [24,68,84,72] pro-\npose various extensions, e.g., DenseASPP [68] densiﬁes the dilated rates to cover\nlarger scale ranges. Some other studies [7,42,19] construct the encoder-decoder\nstructures to exploit the multi-resolution features as the multi-scale context.\nRelational context. DANet [18], CFNet [77] and OCNet [72,71] augment the\nrepresentation for each pixel by aggregating the representations of the contextual\npixels, where the context consists of all the pixels. Diﬀerent from the global\n4 Yuhui Yuan, Xiaokang Chen, Xilin Chen, Jingdong Wang\n(a) ASPP\n (b) OCR\nFig. 2: Illustrating the multi-scale context with the ASPP as an example\nand the OCR context for the pixel marked with I. (a) ASPP: The context is\na set of sparsely sampled pixels marked with I, I. The pixels with diﬀerent colors\ncorrespond to diﬀerent dilation rates. Those pixels are distributed in both the object\nregion and the background region. (b) Our OCR: The context is expected to be a set of\npixels lying in the object (marked with color blue). The image is chosen from ADE20K.\ncontext [46], these works consider the relation (or similarity) between the pixels,\nwhich is based on the self-attention scheme [64,61], and perform a weighted\naggregation with the similarities as the weights.\nDouble Attention and its related work [8,75,9,40,38,74,35,26] and ACFNet [75]\ngroup the pixels into a set of regions, and then augment the pixel representations\nby aggregating the region representations with the consideration of their context\nrelations predicted by using the pixel representation.\nOur approach is a relational context approach and is related to Double Atten-\ntion and ACFNet. The diﬀerences lie in the region formation and the pixel-region\nrelation computation. Our approach learns the regions with the supervision of\nthe ground-truth segmentation. In contrast, the regions in previous approaches\nexcept ACFNet are formed unsupervisedly. On the other hand, the relation be-\ntween a pixel and a region is computed by considering both the pixel and region\nrepresentations, while the relation in previous works is only computed from the\npixel representation.\nCoarse-to-ﬁne segmentation. Various coarse-to-ﬁne segmentation schemes\nhave been developed [17,20,34,59,28,33,85] to gradually reﬁne the segmentation\nmaps from coarse to ﬁne. For example, [34] regards the coarse segmentation map\nas an additional representation and combines it with the original image or other\nrepresentations for computing a ﬁne segmentation map.\nOur approach in some sense can also be regarded as a coarse-to-ﬁne scheme.\nThe diﬀerence lies in that we use the coarse segmentation map for generating\na contextual representation instead of directly used as an extra representation.\nWe compare our approach with the conventional coarse-to-ﬁne schemes in the\nsupplementary material.\nRegion-wise segmentation.There exist many region-wise segmentation meth-\nods [1,2,23,22,65,50,2,60] that organize the pixels into a set of regions (usually\nsuper-pixels), and then classify each region to get the image segmentation result.\nOur approach does not classify each region for segmentation and instead uses\nSegmentation Transformer: OCR for Semantic Segmentation 5\nBackbone\nPixel Representations Pixel-Region Relation\nObject Contextual Representations\nAugmented Representations\n⨂\n⨂\n⨂\nSoft Object Regions Object Region Representations\nLoss\nFig. 3: Illustrating the pipeline of OCR. (i) form the soft object regions in the\npink dashed box. (ii) estimate the object region representations in thepurple dashed box;\n(iii) compute the object contextual representations and the augmented representations\nin the orange dashed box. See Section 3.2 and 3.3 for more details.\nthe region to learn a better representation for the pixel, which leads to better\npixel labeling.\n3 Approach\nSemantic segmentation is a problem of assigning one label li to each pixel pi of\nan image I, where li is one of K diﬀerent classes.\n3.1 Background\nMulti-scale context. The ASPP [5] module captures the multi-scale context\ninformation by performing several parallel dilated convolutions with diﬀerent\ndilation rates [5,6,70]:\nyd\ni =\n∑\nps=pi+d∆t\nKd\ntxs. (1)\nHere, ps = pi+d∆t is the sth sampled position for the dilation convolution with\nthe dilation rate d (e.g., d= 12,24,36 in DeepLabv3 [6]) at the position pi. t is\nthe position index for a convolution, e.g., {∆t = (∆w,∆h)|∆w = −1,0,1,∆h =\n−1,0,1}for a 3 ×3 convolution. xs is the representation at ps. yd\ni is the output\nrepresentation at pi for the dth dilated convolution.Kd\nt is the kernel parameter at\nposition tfor for the dth dilated convolution. The output multi-scale contextual\nrepresentation is the concatenation of the representations output by the parallel\ndilated convolutions.\nThe multi-scale context scheme based on dilated convolutions captures the\ncontexts of multiple scales without losing the resolution. The pyramid pooling\nmodule in PSPNet [80] performs regular convolutions on representations of dif-\nferent scales, and also captures the contexts of multiple scales but loses the\nresolution for large scale contexts.\nRelational context. The relational context scheme [18,72,77] computes the\ncontext for each pixel by considering the relations:\nyi = ρ(\n∑\ns∈I\nwisδ(xs)), (2)\n6 Yuhui Yuan, Xiaokang Chen, Xilin Chen, Jingdong Wang\nwhere Irefers to the set of pixels in the image, wis is the relation between xi\nand xs, and may be predicted only from xi or computed from xi and xs. δ(·)\nand ρ(·) are two diﬀerent transform functions as done in self-attention [61]. The\nglobal context scheme [46] is a special case of relational context with wis = 1\n|I|.\n3.2 Formulation\nThe class label li for pixel pi is essentially the label of the object that pixel pi lies\nin. Motivated by this, we present an object-contextual representation approach,\ncharacterizing each pixel by exploiting the corresponding object representation.\nThe proposed object-contextual representation scheme (1) structurizes all the\npixels in image I into Ksoft object regions, (2) represents each object region asfk\nby aggregating the representations of all the pixels in the kth object region, and\n(3) augments the representation for each pixel by aggregating theKobject region\nrepresentations with consideration of its relations with all the object regions:\nyi = ρ(\nK∑\nk=1\nwikδ(fk)), (3)\nwhere fk is the representation of thekth object region, wik is the relation between\nthe ith pixel and thekth object region. δ(·) and ρ(·) are transformation functions.\nSoft object regions. We partition the image I into K soft object regions\n{M1,M2,..., MK}. Each object region Mk corresponds to the class k, and\nis represented by a 2D map (or coarse segmentation map), where each entry\nindicates the degree that the corresponding pixel belongs to the class k.\nWe compute theKobject regions from an intermediate representation output\nfrom a backbone (e.g., ResNet or HRNet). During training, we learn the object\nregion generator under the supervision from the ground-truth segmentation using\nthe cross-entropy loss.\nObject region representations. We aggregate the representations of all the\npixels weighted by their degrees belonging to the kth object region, forming the\nkth object region representation:\nfk =\n∑\ni∈I\n˜mkixi. (4)\nHere, xi is the representation of pixel pi. ˜mki is the normalized degree for pixel\npi belonging to the kth object region. We use spatial softmax to normalize each\nobject region Mk.\nObject contextual representations. We compute the relation between each\npixel and each object region as below:\nwik = eκ(xi,fk)\n∑K\nj=1 eκ(xi,fj)\n. (5)\nHere, κ(x,f) = φ(x)⊤ψ(f) is the unnormalized relation function, φ(·) and ψ(·)\nare two transformation functions implemented by 1 ×1 conv →BN →ReLU.\nThis is inspired by self-attention [61] for a better relation estimation.\nSegmentation Transformer: OCR for Semantic Segmentation 7\nThe object contextual representation yi for pixel pi is computed according\nto Equation 3. In this equation, δ(·) and ρ(·) are both transformation func-\ntions implemented by 1 ×1 conv →BN →ReLU, and this follows non-local\nnetworks [64].\nAugmented representations. The ﬁnal representation for pixel pi is updated\nas the aggregation of two parts, (1) the original representation xi, and (2) the\nobject contextual representation yi:\nzi = g([x⊤\ni y⊤\ni ]⊤). (6)\nwhere g(·) is a transform function used to fuse the original representation and the\nobject contextual representation, implemented by 1 ×1 conv →BN →ReLU.\nThe whole pipeline of our approach is illustrated in Fig. 3.\nComments: Some recent studies, e.g., Double Attention [8] and ACFNet [75],\ncan be formulated similarly to Equation 3, but diﬀer from our approach in some\naspects. For example, the region formed in Double Attention do not correspond\nto an object class, and the relation in ACFNet [75] is computed only from the\npixel representation w/o using the object region representation.\n3.3 Segmentation Transformer: Rephrasing the OCR Method\nWe rephrase the OCR pipeline using the Transformer [62] language and illustrate\nthe Transformer encoder-decoder architecture in Figure 4. The aforementioned\nOCR pipeline consists of three steps: soft object region extraction, object region\nrepresentation computation, and object-contextual representation computation\nfor each position, and mainly explores the decoder and encoder cross-attention\nmodules.\nAttention. The attention [62] is computed using the scaled dot-product. The\ninputs contain: a set of Nq queries Q ∈Rd×Nq , a set of Nkv keys K ∈Rd×Nkv ,\nand a set of Nkv values V ∈Rd×Nkv . The attention weight aij is computed as\nthe softmax normalization of the dot-product between the query qi and the key\nkj:\naij = e\n1√\nd q⊤\ni kj\nZi\nwhere Zi =\n∑Nkv\nj=1\ne\n1√\nd q⊤\ni kj\n. (7)\nThe attention output for each query qi is the aggregation of values weighted by\nattention weights:\nAttn(qi,K,V) =\n∑Nkv\nj=1\nαijvj. (8)\nDecoder cross-attention. The decoder cross-attention module has two roles:\nsoft object region extraction and object region representation computation.\nThe keys and values are image features ( xi in Equation 4). The queries are\nK category queries ( q1,q2,..., qK), each of which corresponds to a category.\nThe K category queries essentially are used to generate the soft object regions,\n8 Yuhui Yuan, Xiaokang Chen, Xilin Chen, Jingdong Wang\nFFN\nV\nK\nQ\nV\nK\nQ\ncategory queries\ncross-attention\nself-attention\nFFN\nV\nK\nQ\nK\nimage features\ncross-attention\nself-attention\nnorm\nV\nQ\nlinear predictor\nsegmentation map\nencoder\ndecoder\nnorm\nnorm\nnorm\nnorm\nnorm\nFig. 4: Segmentation transformer. Rephrasing the OCR pipeline showing in Fig-\nure 3 using the Transformer encoder-decoder architecture. The encoder self-attention\nunit in the gray box could be a local version and is optional and is useful for boosting\nthe image features. The decoder self-attention unit serves as the role of interacting the\ncategory queries and can be discarded for a single decoder layer or moved after the\ndecoder cross-attention unit.\nM1,M2,..., MK, which are later spatially softmax-normalized as the weights ˜m\nin Equation 4. Computing ˜mis the same as the manner of computing attention\nthe weight αij in Equation 7. The object region representation computation\nmanner in Equation 4 is the same as Equation 8.\nEncoder cross-attention. The encoder cross-attention module (with the sub-\nsequent FFN) serves as the role of aggregating the object region representations\nas shown in Equation 3. The queries are image features at each position, and the\nkeys and values are the decoder outputs. Equation 5 computes the weights in a\nway the same as the attention computation manner Equation 7, and the contex-\ntual aggregation Equation 3 is the same as Equation 8 and ρ(·) corresponds to\nthe FFN operator.\nConnection to class embedding and class attention [14,58]. The cate-\ngory queries are close to the class embedding in Vision Transformer (ViT) [14]\nand in Class-Attention in Image Transformers (CaiT) [58]. We have an embed-\nding for each class other than an integrated embedding for all the classes. The\nSegmentation Transformer: OCR for Semantic Segmentation 9\ndecoder cross attention in segmentation transformer is similar to class attention\nin CaiT [58].\nThe encoder and decoder architecture is close to self-attention in ViT over\nboth the class embedding and image features. If the two cross-attentions and\nthe two self-attentions are conducted simultaneously (depicted in Figure 5), it\nis equivalent to a single self-attention. It is interesting to learn the attention\nparameters for category queries at the ImageNet pre-training stage.\nFFN\nK\nself-attention\nV\nQ\nlinear predictor\nsegmentation map\nnorm\nnorm\nimage features &\ncategory queries\nFig. 5: An alternative of segmentation transformer (shown in Figure 4). The\nmodule in the dashed box is repeated several times.\nConnection to OCNet and interlaced self-attention [71].The OCNet [71]\nexploits the self-attention (i.e., only the encoder self-attention unit is included in\nFigure 4, and the encoder cross-attention unit and the decoder are not included).\nThe self-attention unit is accelerated by an interlaced self-attention unit, con-\nsisting of local self-attention and global self-attention that can be simpliﬁed as\nself-attention over the pooled features over the local windows6. As an alternative\nscheme, the category queries in Figure 4 could be replaced by regularly-sampled\nor adaptively-pooled image features other than learned as model parameters.\n3.4 Architecture\nBackbone. We use the dilated ResNet-101 [25] (with output stride 8) or HRNet-\nW48 [54] (with output stride 4) as the backbone. For dilated ResNet-101, there\nare two representations input to the OCR module. The ﬁrst representation from\n6 The local and/or global self-attention units in interlaced self-attention [71] could be\napplied to Vision Transformer [15] for acceleration.\n10 Yuhui Yuan, Xiaokang Chen, Xilin Chen, Jingdong Wang\nStage 3 is for predicting coarse segmentation (object regions). The other repre-\nsentation from Stage 4 goes through a 3 ×3 convolution (512 output channels),\nand then is fed into the OCR module. For HRNet-W48, we only use the ﬁnal\nrepresentation as the input to the OCR module.\nOCR module. We implement the above formulation of our approach as the\nOCR module, as illustrated in Fig. 3. We use a linear function (a 1 ×1 convolu-\ntion) to predict the coarse segmentation (soft object region) supervised with a\npixel-wise cross-entropy loss. All the transform functions,ψ(·), φ(·), δ(·), ρ(·), and g(·),\nare implemented as 1 ×1 conv →BN →ReLU, and the ﬁrst three output 256\nchannels and the last two output 512 channels. We predict the ﬁnal segmenta-\ntion from the ﬁnal representation using a linear function and we also apply a\npixel-wise cross-entropy loss on the ﬁnal segmentation prediction.\n3.5 Empirical Analysis\nWe conduct the empirical analysis experiments using the dilated ResNet-101 as\nthe backbone on Cityscapes val.\nObject region supervision. We study the inﬂuence of the object region su-\npervision. We modify our approach through removing the supervision (i.e., loss)\non the soft object regions (within the pink dashed box in Fig. 3), and adding\nanother auxiliary loss in the stage-3 of ResNet-101. We keep all the other set-\ntings the same and report the results in the left-most 2 columns of Table 1.\nWe can see that the supervision for forming the object regions is crucial for the\nperformance.\nPixel-region relations. We compare our approach with other two mechanisms\nthat do not use the region representation for estimating the pixel-region relations:\n(i) Double-Attention [8] uses the pixel representation to predict the relation;\n(ii) ACFNet [75] directly uses one intermediate segmentation map to indicate\nthe relations. We use DA scheme and ACF scheme to represent the above two\nmechanisms. We implement both methods by ourselves and only use the dilated\nResNet-101 as the backbone without using multi-scale contexts (the results of\nACFNet is improved by using ASPP [75])\nTable 1: Inﬂuence of object region supervision and pixel-region relation\nestimation scheme. We can ﬁnd both the object region supervision and our pixel-\nregion relation scheme are important for the performance.\nObject region supervision Pixel-region relations\nw/o supervision w/ supervision DA scheme ACF scheme Ours\n77.31% 79.58% 79.01% 78.02% 79.58%\nThe comparison in Table 1 shows that our approach gets superior perfor-\nmance. The reason is that we exploit the pixel representation as well as the\nregion representation for computing the relations. The region representation is\nSegmentation Transformer: OCR for Semantic Segmentation 11\nable to characterize the object in the speciﬁc image, and thus the relation is more\naccurate for the speciﬁc image than that only using the pixel representation.\nGround-truth OCR.We study the segmentation performance using the ground-\ntruth segmentation to form the object regions and the pixel-region relations,\ncalled GT-OCR, to justify our motivation. (i) Object region formation using the\nground-truth: set the conﬁdence of pixelibelonging to kth object region mki = 1\nif the ground-truth label li ≡k and mki = 0 otherwise. (ii) Pixel-region rela-\ntion computation using the ground-truth: set the pixel-region relation wik = 1\nif the ground-truth label li ≡k and wik = 0 otherwise. We have illustrated the\ndetailed results of GT-OCR on four diﬀerent benchmarks in Fig. 1.\n4 Experiments: Semantic Segmentation\n4.1 Datasets\nCityscapes. The Cityscapes dataset [11] is tasked for urban scene understand-\ning. There are totally 30 classes and only 19 classes are used for parsing evalu-\nation. The dataset contains 5K high quality pixel-level ﬁnely annotated images\nand 20K coarsely annotated images. The ﬁnely annotated 5K images are divided\ninto 2,975/500/1,525 images for training, validation and testing.\nADE20K. The ADE20K dataset [82] is used in ImageNet scene parsing chal-\nlenge 2016. There are 150 classes and diverse scenes with 1 ,038 image-level\nlabels. The dataset is divided into 20K/2K/3K images for training, validation\nand testing.\nLIP. The LIP dataset [21] is used in the LIP challenge 2016 for single human\nparsing task. There are about 50K images with 20 classes (19 semantic human\npart classes and 1 background class). The training, validation, and test sets\nconsist of 30K, 10K, 10K images respectively.\nPASCAL-Context. The PASCAL-Context dataset [49] is a challenging scene\nparsing dataset that contains 59 semantic classes and 1 background class. The\ntraining set and test set consist of 4 ,998 and 5,105 images respectively.\nCOCO-Stuﬀ. The COCO-Stuﬀ dataset [3] is a challenging scene parsing dataset\nthat contains 171 semantic classes. The training set and test set consist of 9K\nand 1K images respectively.\n4.2 Implementation Details\nTraining setting. We initialize the backbones using the model pre-trained on\nImageNet and the OCR module randomly. We perform the polynomial learn-\ning rate policy with factor (1 −( iter\nitermax\n)0.9), the weight on the ﬁnal loss as 1,\nthe weight on the loss used to supervise the object region estimation (or aux-\niliary loss) as 0 .4. We use InPlace-ABNsync [52] to synchronize the mean and\nstandard-deviation of BN across multiple GPUs. For the data augmentation, we\nperform random ﬂipping horizontally, random scaling in the range of [0.5,2] and\nrandom brightness jittering within the range of [ −10,10]. We perform the same\n12 Yuhui Yuan, Xiaokang Chen, Xilin Chen, Jingdong Wang\ntraining settings for the reproduced approaches, e.g., PPM, ASPP, to ensure the\nfairness. We follow the previous works [6,76,80] for setting up the training for\nthe benchmark datasets.\n□Cityscapes: We set the initial learning rate as 0.01, weight decay as 0.0005, crop\nsize as 769 ×769 and batch size as 8 by default. For the experiments evaluated\non val/test set, we set training iterations as 40K/100K on train/train+val\nset separately. For the experiments augmented with extra data: (i) w/ coarse,\nwe ﬁrst train our model on train + val for 100K iterations with initial learning\nrate as 0 .01, then we ﬁne-tune the model on coarse set for 50K iterations and\ncontinue ﬁne-tune our model on train+val for 20K iterations with the same\ninitial learning rate 0.001. (ii) w/ coarse + Mapillary [50], we ﬁrst pre-train our\nmodel on the Mapillary train set for 500K iterations with batch size 16 and\ninitial learning rate 0 .01 (achieves 50 .8% on Mapillary val), then we ﬁne-tune\nthe model on Cityscapes following the order of train + val (100K iterations)\n→coarse (50K iterations) →train + val (20K iterations), we set the initial\nlearning rate as 0.001 and the batch size as 8 during the above three ﬁne-tuning\nstages on Cityscapes.\n□ADE20K : We set the initial learning rate as 0 .02, weight decay as 0 .0001,\ncrop size as 520 ×520, batch size as 16 and and training iterations as 150K if\nnot speciﬁed.\n□LIP: We set the initial learning rate as 0 .007, weight decay as 0 .0005, crop\nsize as 473×473, batch size as 32 and training iterations as 100K if not speciﬁed.\n□PASCAL-Context: We set the initial learning rate as 0 .001, weight decay as\n0.0001, crop size as 520 ×520, batch size as 16 and training iterations as 30K if\nnot speciﬁed.\n□COCO-Stuﬀ : We set the initial learning rate as 0.001, weight decay as 0.0001,\ncrop size as 520 ×520, batch size as 16 and training iterations as 60K if not\nspeciﬁed.\n4.3 Comparison with Existing Context Schemes\nWe conduct the experiments using the dilated ResNet-101 as the backbone and\nuse the same training/testing settings to ensure the fairness.\nMulti-scale contexts. We compare our OCR with the multi-scale context\nschemes including PPM [80] and ASPP [6] on three benchmarks including Cityscapes\ntest, ADE20K val and LIP val in Table 2. Our reproduced PPM/ASPP out-\nperforms the originally reported numbers in [80,6]. From Table 2, it can be seen\nthat our OCR outperforms both multi-scale context schemes by a large margin.\nFor example, the absolute gains of OCR over PPM (ASPP) for the four compar-\nisons are 1.5% (0.8%), 0.8% (0.7%), 0.78% (0.68%), 0.84% (0.5%). To the best\nof our knowledge, these improvements are already signiﬁcant considering that\nthe baselines (with dilated ResNet-101) are already strong and the complexity\nof our OCR is much smaller.\nRelational contexts. We compare our OCR with various relational context\nschemes including Self-Attention [61,64], Criss-Cross attention [27] (CC-Attention),\nDANet [18] and Double Attention [8] on the same three benchmarks including\nSegmentation Transformer: OCR for Semantic Segmentation 13\nTable 2: Comparison with multi-scale context scheme. We use ⋆to mark the\nresult w/o using Cityscapesval for training. We can ﬁnd OCR consistently outperforms\nboth PPM and ASPP across diﬀerent benchmarks under the fair comparisons.\nMethod Cityscapes (w/o coarse)Cityscapes (w/ coarse)ADE20K LIP\nPPM [80] 78.4%⋆ 81.2% 43.29% −\nASPP [6] − 81.3% − −\nPPM (Our impl.) 80.3% 81.6% 44.50% 54.76%\nASPP (Our impl.) 81.0% 81.7% 44.60% 55.01%\nOCR 81.8% 82.4% 45.28% 55.60%\nTable 3: Comparison with relational context scheme. Our method consistently\nperforms better across diﬀerent benchmarks. Notably, Double Attention is sensitive to\nthe region number choice and we have ﬁne-tuned this hyper-parameter as 64 to report\nits best performance.\nMethod Cityscapes (w/o coarse)Cityscapes (w/ coarse)ADE20K LIP\nCC-Attention [27] 81.4% - 45.22% -\nDANet [18] 81.5% - - -\nSelf Attention (Our impl.) 81.1% 82.0% 44.75% 55.15%\nDouble Attention (Our impl.) 81.2% 82.0% 44.81% 55.12%\nOCR 81.8% 82.4% 45.28% 55.60%\nTable 4: Complexity comparison. We use input feature map of size [1 ×2048 ×\n128 ×128] to evaluate their complexity during inference. The numbers are obtained on\na single P40 GPU with CUDA 10 .0. All the numbers are the smaller the better. Our\nOCR requires the least GPU memory and the least runtime.\nMethod Parameters▲ Memory▲ FLOPs▲ Time▲\nPPM (Our impl.) 23.1M 792M 619G 99ms\nASPP (Our impl.) 15.5M 284M 492G 97ms\nDANet (Our impl.) 10.6M 2339M 1110G 121ms\nCC-Attention (Our impl.) 10.6M 427M 804G 131ms\nSelf-Attention (Our impl.) 10.5M 2168M 619G 96ms\nDouble Attention (Our impl.) 10.2M 209M 338G 46ms\nOCR 10.5M 202M 340G 45ms\nCityscapes test, ADE20K val and LIP val. For the reproduced Double At-\ntention, we ﬁne-tune the number of the regions (as it is very sensitive to the\nhyper-parameter choice) and we choose 64 with the best performance. More de-\ntailed analysis and comparisons are illustrated in the supplementary material.\nAccording to the results in Table 3, it can be seen that our OCR outperforms\nthese relational context schemes under the fair comparisons. Notably, the com-\nplexity of our OCR is much smaller than most of the other methods.\nComplexity. We compare the eﬃciency of our OCR with the eﬃciencies of\nthe multi-scale context schemes and the relational context schemes. We measure\nthe increased parameters, GPU memory, computation complexity (measured by\nthe number of FLOPs) and inference time that are introduced by the context\nmodules, and do not count the complexity from the backbones. The comparison\nin Table 4 shows the superiority of the proposed OCR scheme.\n□Parameters: Most relational context schemes require less parameters com-\npared with the multi-scale context schemes. For example, our OCR only requires\n1/2 and 2/3 of the parameters of PPM and ASPP separately.\n14 Yuhui Yuan, Xiaokang Chen, Xilin Chen, Jingdong Wang\n□Memory: Both our OCR and Double Attention require much less GPU mem-\nory compared with the other approaches (e.g., DANet, PPM). For example, our\nGPU memory consumption is 1/4, 1/10, 1/2, 1/10 of the memory consumption\nof PPM, DANet, CC-Attention and Self-Attention separately.\n□FLOPs: Our OCR only requires 1/2, 7/10, 3/10, 2/5 and 1/2 of the FLOPs\nbased on PPM, ASPP, DANet, CC-Attention and Self-Attention separately.\n□Running time: The runtime of OCR is very small: only 1/2, 1/2, 1/3, 1/3 and\n1/2 of the runtime with PPM, ASPP, DANet, CC-Attention and Self-Attention\nseparately.\nIn general, our OCR is a much better choice if we consider the balance\nbetween performance, memory complexity, GFLOPs and running time.\n4.4 Comparison with the State-of-the-Art\nConsidering that diﬀerent approaches perform improvements on diﬀerent base-\nlines to achieve the best performance, we categorize the existing works to two\ngroups according to the baselines that they apply: (i) simple baseline: dilated\nResNet-101 with stride 8; (ii) advanced baseline: PSPNet, DeepLabv3, multi-grid\n(MG), encoder-decoder structures that achieve higher resolution outputs with\nstride 4 or stronger backbones such as WideResNet-38, Xception-71 and HRNet.\nFor fair comparison with the two groups fairly, we perform our OCR on a\nsimple baseline (dilated ResNet-101 with stride 8) and an advanced baseline\n(HRNet-W48 with stride 4). Notably, our improvement with HRNet-W48 (over\nResNet-101) is comparable with the gain of the other work based on advanced\nbaseline methods. For example, DGCNet [78] gains 0 .7% with Multi-grid while\nOCR gains 0.6% with stronger backbone on Cityscapes test. We summarize all\nthe results in Table 5 and illustrate the comparison details on each benchmark\nseparately as follows.\nCityscapes. Compared with the methods based on the simple baseline on\nCityscape test w/o using the coarse data, our approach achieves the best per-\nformance 81.8%, which is already comparable with some methods based on the\nadvanced baselines, e.g, DANet, ACFNet. Our approach achieves better perfor-\nmance 82.4% through exploiting the coarsely annotated images for training.\nFor comparison with the approaches based on the advanced baselines, we\nperform our OCR on the HRNet-W48, and pre-train our model on the Mapillary\ndataset [50]. Our approach achieves 84.2% on Cityscapes test. We further apply\na novel post-processing scheme SegFix [73] to reﬁne the boundary quality, which\nbrings 0.3% ↑improvement. Our ﬁnal submission “HRNet + OCR + SegFix”\nachieves 84.5%, which ranks the 1 st place on the Cityscapes leaderboard by the\ntime of our submission. In fact, we perform PPM and ASPP on HRNet-W48\nseparately and empirically ﬁnd that directly applying either PPM or ASPP does\nnot improve the performance and even degrades the performance, while our OCR\nconsistently improves the performance.\nNotably, the very recent work [56] sets a new state-of-the-art performance\n85.4% on Cityscapes leaderboard via combining our “HRNet + OCR” and a\nnew hierarchical multi-scale attention mechanism.\nSegmentation Transformer: OCR for Semantic Segmentation 15\nTable 5: Comparison with the state-of-the-art. We use M to represent multi-\nscale context and R to represent relational context. Red, Green, Blue represent the\ntop-3 results. We use ♭, †and ‡to mark the result w/o using Cityscapes val, the\nmethod using Mapillary dataset and the method using the Cityscapes video dataset\nseparately\nMethod Baseline StrideContextschemes\nCityscapes(w/o coarse)Cityscapes(w/ coarse)ADE20KLIP PASCALContextCOCO-Stuﬀ\nSimple baselines\nPSPNet [80] ResNet-101 8× M 78.4♭ 81.2 43.29 - 47.8 -\nDeepLabv3 [6]ResNet-101 8× M - 81.3 - - - -\nPSANet [81] ResNet-101 8× R 80.1 81.4 43.77 - - -\nSAC [79] ResNet-101 8× M 78.1 - 44.30 - - -\nAAF [29] ResNet-101 8× R 79.1♭ - - - - -\nDSSPN [41] ResNet-101 8× - 77.8 - 43.68 - - 38.9\nDepthSeg [32]ResNet-101 8× - 78.2 - - - - -\nMMAN [48] ResNet-101 8× - - - - 46.81 - -\nJPPNet [39] ResNet-101 8× M - - - 51.37 - -\nEncNet [76] ResNet-101 8× - - - 44.65 - 51.7 -\nGCU [38] ResNet-101 8× R - - 44.81 - - -\nAPCNet [24] ResNet-101 8× M,R - - 45.38 - 54.7 -\nCFNet [77] ResNet-101 8× R 79.6 - 44.89 - 54.0 -\nBFP [12] ResNet-101 8× R 81.4 - - - 53.6 -\nCCNet [27] ResNet-101 8× R 81.4 - 45.22 - - -\nANNet [84] ResNet-101 8× M,R 81.3 - 45.24 - 52.8 -\nOCR (Seg. transformer)ResNet-101 8× R 81.8 82.4 45.28 55.60 54.8 39.5\nAdvanced baselines\nDenseASPP [68]DenseNet-1618× M 80.6 - - - - -\nDANet [18]ResNet-101 + MG8× R 81.5 - 45.22 - 52.6 39.7\nDGCNet [78]ResNet-101 + MG8× R 82.0 - - - 53.7 -\nEMANet [36]ResNet-101 + MG8× R - - - - 53.1 39.9\nSeENet [51]ResNet-101 + ASPP8× M 81.2 - - - - -\nSGR [40]ResNet-101 + ASPP8× R - - 44.32 - 52.5 39.1\nOCNet [72]ResNet-101 + ASPP8× M,R 81.7 - 45.45 54.72 - -\nACFNet [75]ResNet-101 + ASPP8× M,R 81.8 - - - - -\nCNIF [63]ResNet-101 + ASPP8× M - - - 56.93 -\nGALD [37]ResNet-101 + ASPP8× M,R 81.8 82.9 - - - -\nGALD†[37]ResNet-101 + CGNL + MG8× M,R - 83.3 - - - -\nMapillary [52]WideResNet-38 + ASPP8× M - 82.0 - - - -\nGSCNN†[55]WideResNet-38 + ASPP8× M 82.8 - - - -\nSPGNet [10]2×ResNet-50 4× - 81.1 - - - - -\nZigZagNet [42]ResNet-101 4× M - - - - 52.1 -\nSVCNet [13] ResNet-101 4× R 81.0 - - - 53.2 39.6\nACNet [19]ResNet-101 + MG4× M,R 82.3 - 45.90 - 54.1 40.1\nCE2P [45]ResNet-101 + PPM4× M - - - 53.10 -\nVPLR†‡[83]WideResNet-38 + ASPP4× M - 83.5 - - -\nDeepLabv3+ [7]Xception-71 4× M - 82.1 - - - -\nDPC [4] Xception-71 4× M 82.7 - - - -\nDUpsampling [57]Xception-71 4× M - - - - 52.5 -\nHRNet [54]HRNetV2-W484× - 81.6 - - 55.90 54.0\nOCR (Seg. transformer)HRNetV2-W484× R 82.4 83.0 45.66 56.65 56.2 40.5\nOCR†(Seg. transformer)HRNetV2-W484× R 83.6 84.2 - - - -\nADE20K. From Table 5, it can be seen that our OCR achieves competitive\nperformance (45 .28% and 45 .66%) compared with most of the previous ap-\nproaches based on both simple baselines and advanced baselines. For example,\nthe ACFNet [24] exploits both the multi-scale context and relational context\nto achieve higher performance. The very recent ACNet [19] achieves the best\nperformance through combining richer local and global contexts.\nLIP. Our approach achieves the best performance 55 .60% on LIP val based on\nthe simple baselines. Applying the stronger backbone HRNetV2-W48 further im-\n16 Yuhui Yuan, Xiaokang Chen, Xilin Chen, Jingdong Wang\nTable 6: Panoptic segmentation results on COCO val 2017. The per-\nformance of Panoptic-FPN [30] is reproduced based on the oﬃcial open-source\nDetectron2 [66] and we use the 3 ×learning rate schedule by default. Our OCR\nconsistently improves the PQ performance with both backbones.\nBackboneMethod AP PQ Th mIoU PQ St PQ\nResNet-50Panoptic-FPN 40.0 48.3 42.9 31.2 41.5\nPanoptic-FPN + OCR40.4 (+0.4) 48.6 (+0.3) 44.3 (+1.4) 33.9 (+2.7) 42.7 (+1.2)\nResNet-101Panoptic-FPN 42.4 49.7 44.5 32.9 43.0\nPanoptic-FPN + OCR42.7 (+0.3) 50.2 (+0.5) 45.5 (+1.0) 35.2 (+2.3) 44.2 (+1.2)\nproves the performance to 56 .65%, which outperforms the previous approaches.\nThe very recent work CNIF [63] achieves the best performance (56.93%) through\ninjecting the hierarchical structure knowledge of human parts. Our approach po-\ntentially beneﬁt from such hierarchical structural knowledge. All the results are\nbased on only ﬂip testing without multi-scale testing 7.\nPASCAL-Context. We evaluate the performance over 59 categories follow-\ning [54]. It can be seen that our approach outperforms both the previous best\nmethods based on simple baselines and the previous best methods based on ad-\nvanced baselines. The HRNet-W48 + OCR approach achieves the best perfor-\nmance 56.2%, signiﬁcantly outperforming the second best, e.g., ACPNet (54.7%)\nand ACNet (54.1%).\nCOCO-Stuﬀ. It can be seen that our approach achieves the best performance,\n39.5% based ResNet-101 and 40 .5% based on HRNetV2-48.\nQualitative Results. We illustrate the qualitative results in the supplementary\nmaterial due to the limited pages.\n5 Experiments: Panoptic Segmentation\nTo verify the generalization ability of our method, we apply OCR scheme on\nthe more challenging panoptic segmentation task [31], which uniﬁes both the\ninstance segmentation task and the semantic segmentation task.\nDataset. We choose the COCO dataset [43] to study the eﬀectiveness of our\nmethod on panoptic segmentation. We follow the previous work [30] and uses all\n2017 COCO images with 80 thing and 53 stuﬀ classes annotated.\nTraining Details. We follow the default training setup of “COCO Panoptic\nSegmentation Baselines with Panoptic FPN (3×learning schedule)” 8 in Detec-\ntron2 [66]. The reproduced Panoptic FPN reaches higher performance than the\noriginal numbers in the paper [30] (Panoptic FPN w/ ResNet-50, PQ: 39 .2% /\nPanoptic FPN w/ ResNet-101, PQ: 40.3%) and we choose the higher reproduced\nresults as our baseline.\n7 Only few methods adopt multi-scale testing. For example, CNIF [63] gets the im-\nproved performance from 56.93% to 57.74%.\n8 https://github.com/facebookresearch/detectron2/blob/master/MODEL ZOO.md\nSegmentation Transformer: OCR for Semantic Segmentation 17\nIn our implementation, we use the original prediction from the semantic\nsegmentation head (within Panoptic-FPN) to compute the soft object regions\nand then we use a OCR head to predict a reﬁned semantic segmentation map.\nWe set the loss weights on both the original semantic segmentation head and\nthe OCR head as 0 .25. All the other training settings are kept the same for fair\ncomparison. We directly use the same OCR implementation (for the semantic\nsegmentation task) without any tuning.\nResults. In Table 6, we can see that OCR improves the PQ performance of\nPanoptic-FPN (ResNet-101) from 43 .0% to 44 .2%, where the main improve-\nments come from better segmentation quality on the stuﬀ region measured by\nmIoU and PQSt. Speciﬁcally, our OCR improves the mIoU and PQSt of Panoptic-\nFPN (ResNet-101) by 1.0% and 2.3% separately. In general, the performance of\n“Panoptic-FPN + OCR” is very competitive compared to various recent meth-\nods [67,44,69]. We also report the results of Panoptic-FPN with PPM and ASPP\nto illustrate the advantages of our OCR in the supplementary material.\n6 Conclusions\nIn this work, we present an object-contextual representation approach for se-\nmantic segmentation. The main reason for the success is that the label of a\npixel is the label of the object that the pixel lies in and the pixel representa-\ntion is strengthened by characterizing each pixel with the corresponding object\nregion representation. We empirically show that our approach brings consistent\nimprovements on various benchmarks.\nAcknowledgement This work is partially supported by Natural Science Foun-\ndation of China under contract No. 61390511, and Frontier Science Key Research\nProject CAS No. QYZDJ-SSW-JSC009.\nReferences\n1. Arbel´ aez, P., Hariharan, B., Gu, C., Gupta, S., Bourdev, L., Malik, J.: Semantic\nsegmentation using regions and parts. In: CVPR (2012)\n2. Caesar, H., Uijlings, J., Ferrari, V.: Region-based semantic segmentation with end-\nto-end training. In: ECCV (2016)\n3. Caesar, H., Uijlings, J., Ferrari, V.: Coco-stuﬀ: Thing and stuﬀ classes in context.\nIn: CVPR (2018)\n4. Chen, L.C., Collins, M., Zhu, Y., Papandreou, G., Zoph, B., Schroﬀ, F., Adam, H.,\nShlens, J.: Searching for eﬃcient multi-scale architectures for dense image predic-\ntion. In: NIPS (2018)\n5. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab: Se-\nmantic image segmentation with deep convolutional nets, atrous convolution, and\nfully connected crfs. PAMI (2018)\n6. Chen, L.C., Papandreou, G., Schroﬀ, F., Adam, H.: Rethinking atrous convolution\nfor semantic image segmentation. arXiv:1706.05587 (2017)\n7. Chen, L.C., Zhu, Y., Papandreou, G., Schroﬀ, F., Adam, H.: Encoder-decoder with\natrous separable convolution for semantic image segmentation. In: ECCV (2018)\n18 Yuhui Yuan, Xiaokang Chen, Xilin Chen, Jingdong Wang\n8. Chen, Y., Kalantidis, Y., Li, J., Yan, S., Feng, J.: A ˆ2-nets: Double attention net-\nworks. In: NIPS (2018)\n9. Chen, Y., Rohrbach, M., Yan, Z., Yan, S., Feng, J., Kalantidis, Y.: Graph-based\nglobal reasoning networks. arXiv:1811.12814 (2018)\n10. Cheng, B., Chen, L.C., Wei, Y., Zhu, Y., Huang, Z., Xiong, J., Huang, T.S., Hwu,\nW.M., Shi, H.: Spgnet: Semantic prediction guidance for scene parsing. In: ICCV\n(2019)\n11. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R.,\nFranke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene\nunderstanding. In: CVPR (2016)\n12. Ding, H., Jiang, X., Liu, A.Q., Thalmann, N.M., Wang, G.: Boundary-aware feature\npropagation for scene segmentation. ICCV (2019)\n13. Ding, H., Jiang, X., Shuai, B., Liu, A.Q., Wang, G.: Semantic correlation promoted\nshape-variant context for segmentation. In: CVPR (2019)\n14. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:\nAn image is worth 16x16 words: Transformers for image recognition at scale. CoRR\nabs/2010.11929 (2020), https://arxiv.org/abs/2010.11929\n15. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:\nAn image is worth 16x16 words: Transformers for image recognition at scale. CoRR\nabs/2010.11929 (2020), https://arxiv.org/abs/2010.11929\n16. Farabet, C., Couprie, C., Najman, L., LeCun, Y.: Learning hierarchical features\nfor scene labeling. PAMI (2012)\n17. Fieraru, M., Khoreva, A., Pishchulin, L., Schiele, B.: Learning to reﬁne human\npose estimation. In: CVPRW (2018)\n18. Fu, J., Liu, J., Tian, H., Fang, Z., Lu, H.: Dual attention network for scene seg-\nmentation. arXiv:1809.02983 (2018)\n19. Fu, J., Liu, J., Wang, Y., Li, Y., Bao, Y., Tang, J., Lu, H.: Adaptive context\nnetwork for scene parsing. In: ICCV (2019)\n20. Gidaris, S., Komodakis, N.: Detect, replace, reﬁne: Deep structured prediction for\npixel wise labeling. In: CVPR (2017)\n21. Gong, K., Liang, X., Zhang, D., Shen, X., Lin, L.: Look into person: Self-supervised\nstructure-sensitive learning and a new benchmark for human parsing. In: CVPR\n(2017)\n22. Gould, S., Fulton, R., Koller, D.: Decomposing a scene into geometric and seman-\ntically consistent regions. In: ICCV (2009)\n23. Gu, C., Lim, J.J., Arbelaez, P., Malik, J.: Recognition using regions. In: CVPR\n(2009)\n24. He, J., Deng, Z., Zhou, L., Wang, Y., Qiao, Y.: Adaptive pyramid context network\nfor semantic segmentation. In: CVPR (2019)\n25. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: CVPR (2016)\n26. Huang, L., Yuan, Y., Guo, J., Zhang, C., Chen, X., Wang, J.: Interlaced sparse\nself-attention for semantic segmentation. arXiv preprint arXiv:1907.12273 (2019)\n27. Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y., Liu, W.: Ccnet: Criss-cross\nattention for semantic segmentation. In: ICCV (2019)\n28. Islam, M.A., Naha, S., Rochan, M., Bruce, N., Wang, Y.: Label reﬁnement network\nfor coarse-to-ﬁne semantic segmentation. arXiv:1703.00551 (2017)\n29. Ke, T.W., Hwang, J.J., Liu, Z., Yu, S.X.: Adaptive aﬃnity ﬁelds for semantic\nsegmentation. In: ECCV (2018)\nSegmentation Transformer: OCR for Semantic Segmentation 19\n30. Kirillov, A., Girshick, R., He, K., Doll´ ar, P.: Panoptic feature pyramid networks.\nIn: CVPR (2019)\n31. Kirillov, A., He, K., Girshick, R., Rother, C., Doll´ ar, P.: Panoptic segmentation.\nIn: CVPR (2019)\n32. Kong, S., Fowlkes, C.C.: Recurrent scene parsing with perspective understanding\nin the loop. In: CVPR (2018)\n33. Kuo, W., Angelova, A., Malik, J., Lin, T.Y.: Shapemask: Learning to segment\nnovel objects by reﬁning shape priors (2019)\n34. Li, K., Hariharan, B., Malik, J.: Iterative instance segmentation. In: CVPR (2016)\n35. Li, X., Zhong, Z., Wu, J., Yang, Y., Lin, Z., Liu, H.: Expectation-maximization\nattention networks for semantic segmentation. In: ICCV (2019)\n36. Li, X., Zhong, Z., Wu, J., Yang, Y., Lin, Z., Liu, H.: Expectation-maximization\nattention networks for semantic segmentation. In: ICCV (2019)\n37. Li, X., Zhang, L., You, A., Yang, M., Yang, K., Tong, Y.: Global aggregation then\nlocal distribution in fully convolutional networks. BMVC (2019)\n38. Li, Y., Gupta, A.: Beyond grids: Learning graph representations for visual recog-\nnition. In: NIPS (2018)\n39. Liang, X., Gong, K., Shen, X., Lin, L.: Look into person: Joint body parsing &\npose estimation network and a new benchmark. PAMI (2018)\n40. Liang, X., Hu, Z., Zhang, H., Lin, L., Xing, E.P.: Symbolic graph reasoning meets\nconvolutions. In: NIPS (2018)\n41. Liang, X., Zhou, H., Xing, E.: Dynamic-structured semantic propagation network.\nIn: CVPR (2018)\n42. Lin, D., Shen, D., Shen, S., Ji, Y., Lischinski, D., Cohen-Or, D., Huang, H.: Zigza-\ngnet: Fusing top-down and bottom-up context for object segmentation. In: CVPR\n(2019)\n43. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ ar, P.,\nZitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV (2014)\n44. Liu, H., Peng, C., Yu, C., Wang, J., Liu, X., Yu, G., Jiang, W.: An end-to-end\nnetwork for panoptic segmentation. In: CVPR (2019)\n45. Liu, T., Ruan, T., Huang, Z., Wei, Y., Wei, S., Zhao, Y., Huang, T.: Devil in the\ndetails: Towards accurate single and multiple human parsing. arXiv:1809.05996\n(2018)\n46. Liu, W., Rabinovich, A., Berg, A.C.: Parsenet: Looking wider to see better.\narXiv:1506.04579 (2015)\n47. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic\nsegmentation. In: CVPR (2015)\n48. Luo, Y., Zheng, Z., Zheng, L., Tao, G., Junqing, Y., Yang, Y.: Macro-micro adver-\nsarial network for human parsing. In: ECCV (2018)\n49. Mottaghi, R., Chen, X., Liu, X., Cho, N.G., Lee, S.W., Fidler, S., Urtasun, R.,\nYuille, A.: The role of context for object detection and semantic segmentation in\nthe wild. In: CVPR (2014)\n50. Neuhold, G., Ollmann, T., Rota Bulo, S., Kontschieder, P.: The mapillary vistas\ndataset for semantic understanding of street scenes. In: CVPR (2017)\n51. Pang, Y., Li, Y., Shen, J., Shao, L.: Towards bridging semantic gap to improve\nsemantic segmentation. In: ICCV (2019)\n52. Rota Bul` o, S., Porzi, L., Kontschieder, P.: In-place activated batchnorm for\nmemory-optimized training of dnns. In: CVPR (2018)\n53. Shetty, R., Schiele, B., Fritz, M.: Not using the car to see the sidewalk–quantifying\nand controlling the eﬀects of context in classiﬁcation and segmentation. In: CVPR\n(2019)\n20 Yuhui Yuan, Xiaokang Chen, Xilin Chen, Jingdong Wang\n54. Sun, K., Zhao, Y., Jiang, B., Cheng, T., Xiao, B., Liu, D., Mu, Y., Wang, X.,\nLiu, W., Wang, J.: High-resolution representations for labeling pixels and regions.\narXiv:1904.04514 (2019)\n55. Takikawa, T., Acuna, D., Jampani, V., Fidler, S.: Gated-scnn: Gated shape cnns\nfor semantic segmentation. ICCV (2019)\n56. Tao, A., Sapra, K., Catanzaro, B.: Hierarchical multi-scale attention for semantic\nsegmentation. arXiv:2005.10821 (2020)\n57. Tian, Z., He, T., Shen, C., Yan, Y.: Decoders matter for semantic segmentation:\nData-dependent decoding enables ﬂexible feature aggregation. In: CVPR (2019)\n58. Touvron, H., Cord, M., Sablayrolles, A., Synnaeve, G., J´ egou, H.: Go-\ning deeper with image transformers. CoRR abs/2103.17239 (2021),\nhttps://arxiv.org/abs/2103.17239\n59. Tu, Z., Bai, X.: Auto-context and its application to high-level vision tasks and 3d\nbrain image segmentation. PAMI (2010)\n60. Uijlings, J.R., Van De Sande, K.E., Gevers, T., Smeulders, A.W.: Selective search\nfor object recognition. IJCV (2013)\n61. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. In: NIPS (2017)\n62. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,\nA.N., Kaiser, L., Polosukhin, I.: Attention is all you need. In: Guyon, I.,\nvon Luxburg, U., Bengio, S., Wallach, H.M., Fergus, R., Vishwanathan,\nS.V.N., Garnett, R. (eds.) Advances in Neural Information Processing Sys-\ntems 30: Annual Conference on Neural Information Processing Systems\n2017, December 4-9, 2017, Long Beach, CA, USA. pp. 5998–6008 (2017),\nhttps://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-\nAbstract.html\n63. Wang, W., Zhang, Z., Qi, S., Shen, J., Pang, Y., Shao, L.: Learning compositional\nneural information fusion for human parsing. In: ICCV (2019)\n64. Wang, X., Girshick, R., Gupta, A., He, K.: Non-local neural networks. In: CVPR\n(2018)\n65. Wei, Y., Feng, J., Liang, X., Cheng, M.M., Zhao, Y., Yan, S.: Object region min-\ning with adversarial erasing: A simple classiﬁcation to semantic segmentation ap-\nproach. In: CVPR (2017)\n66. Wu, Y., Kirillov, A., Massa, F., Lo, W.Y., Girshick, R.: Detectron2.\nhttps://github.com/facebookresearch/detectron2 (2019)\n67. Xiong, Y., Liao, R., Zhao, H., Hu, R., Bai, M., Yumer, E., Urtasun, R.: Upsnet: A\nuniﬁed panoptic segmentation network. In: CVPR (2019)\n68. Yang, M., Yu, K., Zhang, C., Li, Z., Yang, K.: Denseaspp for semantic segmentation\nin street scenes. In: CVPR (2018)\n69. Yang, Y., Li, H., Li, X., Zhao, Q., Wu, J., Lin, Z.: Sognet: Scene overlap graph\nnetwork for panoptic segmentation. arXiv:1911.07527 (2019)\n70. Yu, F., Koltun, V.: Multi-scale context aggregation by dilated convolutions. ICLR\n(2016)\n71. Yuan, Y., Huang, L., Guo, J., Zhang, C., Chen, X., Wang, J.: Ocnet: Object context\nnetwork for scene parsing. IJCV (2021)\n72. Yuan, Y., Wang, J.: Ocnet: Object context network for scene parsing. CoRR\nabs/1809.00916 (2018), http://arxiv.org/abs/1809.00916\n73. Yuan, Y., Xie, J., Chen, X., Wang, J.: Segﬁx: Model-agnostic boundary reﬁnement\nfor segmentation. In: ECCV (2020)\n74. Yue, K., Sun, M., Yuan, Y., Zhou, F., Ding, E., Xu, F.: Compact generalized\nnon-local network. In: NIPS (2018)\nSegmentation Transformer: OCR for Semantic Segmentation 21\n75. Zhang, F., Chen, Y., Li, Z., Hong, Z., Liu, J., Ma, F., Han, J., Ding, E.: Acfnet:\nAttentional class feature network for semantic segmentation. In: ICCV (2019)\n76. Zhang, H., Dana, K., Shi, J., Zhang, Z., Wang, X., Tyagi, A., Agrawal, A.: Context\nencoding for semantic segmentation. In: CVPR (2018)\n77. Zhang, H., Zhang, H., Wang, C., Xie, J.: Co-occurrent features in semantic seg-\nmentation. In: CVPR (2019)\n78. Zhang, L., Li, X., Arnab, A., Yang, K., Tong, Y., Torr, P.H.: Dual graph convolu-\ntional network for semantic segmentation. BMVC (2019)\n79. Zhang, R., Tang, S., Zhang, Y., Li, J., Yan, S.: Scale-adaptive convolutions for\nscene parsing. In: ICCV (2017)\n80. Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network. In:\nCVPR (2017)\n81. Zhao, H., Yi, Z., Shu, L., Jianping, S., Loy, C.C., Dahua, L., Jia, J.: Psanet: Point-\nwise spatial attention network for scene parsing. ECCV (2018)\n82. Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing\nthrough ade20k dataset. In: CVPR (2017)\n83. Zhu, Y., Sapra, K., Reda, F.A., Shih, K.J., Newsam, S., Tao, A., Catanzaro, B.:\nImproving semantic segmentation via video propagation and label relaxation. In:\nCVPR (2019)\n84. Zhu, Z., Xu, M., Bai, S., Huang, T., Bai, X.: Asymmetric non-local neural networks\nfor semantic segmentation. In: ICCV (2019)\n85. Zhu, Z., Xia, Y., Shen, W., Fishman, E., Yuille, A.: A 3d coarse-to-ﬁne framework\nfor volumetric medical image segmentation. In: 3DV (2018)"
}