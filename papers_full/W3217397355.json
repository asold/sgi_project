{
  "title": "SwinTrack: A Simple and Strong Baseline for Transformer Tracking",
  "url": "https://openalex.org/W3217397355",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2378540487",
      "name": "Lin Liting",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2079400170",
      "name": "Fan Heng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1460009565",
      "name": "Zhang Zhipeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1909073810",
      "name": "Xu Yong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2747133689",
      "name": "Ling, Haibin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3151130473",
    "https://openalex.org/W3171087525",
    "https://openalex.org/W3035453691",
    "https://openalex.org/W2955747520",
    "https://openalex.org/W2794744029",
    "https://openalex.org/W3090155371",
    "https://openalex.org/W3001584168",
    "https://openalex.org/W2963534981",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3122515622",
    "https://openalex.org/W2158827467",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2171590421",
    "https://openalex.org/W3202066758",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2998434318",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3102710196",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3204540098",
    "https://openalex.org/W2470394683",
    "https://openalex.org/W2891033863",
    "https://openalex.org/W2963975324",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W3108235634",
    "https://openalex.org/W3202406646",
    "https://openalex.org/W3167536469",
    "https://openalex.org/W3174225630",
    "https://openalex.org/W3035672751",
    "https://openalex.org/W3211432419",
    "https://openalex.org/W3108519869",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2799058067",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3172087149"
  ],
  "abstract": "Recently Transformer has been largely explored in tracking and shown state-of-the-art (SOTA) performance. However, existing efforts mainly focus on fusing and enhancing features generated by convolutional neural networks (CNNs). The potential of Transformer in representation learning remains under-explored. In this paper, we aim to further unleash the power of Transformer by proposing a simple yet efficient fully-attentional tracker, dubbed SwinTrack, within classic Siamese framework. In particular, both representation learning and feature fusion in SwinTrack leverage the Transformer architecture, enabling better feature interactions for tracking than pure CNN or hybrid CNN-Transformer frameworks. Besides, to further enhance robustness, we present a novel motion token that embeds historical target trajectory to improve tracking by providing temporal context. Our motion token is lightweight with negligible computation but brings clear gains. In our thorough experiments, SwinTrack exceeds existing approaches on multiple benchmarks. Particularly, on the challenging LaSOT, SwinTrack sets a new record with 0.713 SUC score. It also achieves SOTA results on other benchmarks. We expect SwinTrack to serve as a solid baseline for Transformer tracking and facilitate future research. Our codes and results are released at https://github.com/LitingLin/SwinTrack.",
  "full_text": "SwinTrack: A Simple and Strong Baseline for\nTransformer Tracking\nLiting Lin1,2∗ Heng Fan3∗ Zhipeng Zhang4 Yong Xu1,2 Haibin Ling5\n1School of Computer Science & Engineering, South China Univ. of Tech., Guangzhou, China\n2Peng Cheng Laboratory, Shenzhen, China\n3Department of Computer Science and Engineering, University of North Texas, Denton, USA\n4DiDi Chuxing, Beijing, China\n5Department of Computer Science, Stony Brook University, Stony Brook, USA\nl.lt@mail.scut.edu.cn, heng.fan@unt.edu, zhipeng.zhang.cv@outlook.com\nyxu@scut.edu.cn, hling@cs.stonybrook.edu\nAbstract\nRecently Transformer has been largely explored in tracking and shown state-of-the-\nart (SOTA) performance. However, existing efforts mainly focus on fusing and en-\nhancing features generated by convolutional neural networks (CNNs). The potential\nof Transformer in representation learning remains under-explored. In this paper, we\naim to further unleash the power of Transformer by proposing a simple yet efﬁcient\nfully-attentional tracker, dubbed SwinTrack, within classic Siamese framework.\nIn particular, both representation learning and feature fusion in SwinTrack leverage\nthe Transformer architecture, enabling better feature interactions for tracking than\npure CNN or hybrid CNN-Transformer frameworks. Besides, to further enhance\nrobustness, we present a novel motion token that embeds historical target trajectory\nto improve tracking by providing temporal context. Our motion token is lightweight\nwith negligible computation but brings clear gains. In our thorough experiments,\nSwinTrack exceeds existing approaches on multiple benchmarks. Particularly, on\nthe challenging LaSOT, SwinTrack sets a new record with 0.713 SUC score. It\nalso achieves SOTA results on other benchmarks. We expect SwinTrack to serve as\na solid baseline for Transformer tracking and facilitate future research. Our codes\nand results are released at https://github.com/LitingLin/SwinTrack.\n1 Introduction\nVisual tracking has seen considerable progress since deep learning. In particular, the recent Trans-\nformer [38] has signiﬁcantly pushed the state-of-the-art in tracking owing to its ability in modeling\nlong-range dependencies. However, existing methods usually leverage Transformer for fusing and\nenhancing features generated from convolutional neural networks (CNNs), e.g., ResNet [18]. The\npotential of exploiting Transformer for feature representation learning is largely under-explored.\nRecently, Vision Transformer (ViT) [9] has exhibited great potential in robust feature representation\nlearning. Particularly, its extension Swin Transformer [ 28] has achieved state-of-the-art (SOTA)\nresults on multiple tasks. Taking inspiration from this, we argue, besides the feature fusion, the\nrepresentation learning in tracking can also beneﬁt from Transformer via attention. Thus moti-\nvated, we propose to develop a fully attentional tracking framework based on Siamese architecture.\nSpeciﬁcally, both the feature representation learning and the feature fusion of template and search\nregion are realized by Transformer. More concretely, we borrow the architecture of the powerful\n∗Equal Contributions.\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\narXiv:2112.00995v3  [cs.CV]  13 Oct 2022\n0 10 20 30 40 50 60 70 80 90 100Speed (fps)\n56\n58\n60\n62\n64\n66\n68\n70\n72Success Score (%)\nVisualization of Success Score and Speed on LaSOTSwinTrack-B-384 (ours)\nSwinTrack-T-224 (ours)STARK\nTransTARDiMPsuperSiamRCNN\nLTMUOcean\nPrDiMPAutoMatch\nKeepTrack\nTrDiMP\nDiMP\n70% SUC Threshold\nFigure 1: Comparison on LaSOT [11]. Our tracker (SwinTrack-B-384) sets a new record with 0.713\nSUC score and still runs efﬁciently at around 45 fps. A lighter version (SwinTrack-T-224) achieves\n0.672 SUC score and runs at around 96 fps, which is on par with existing SOTAs in accuracy but\nmuch faster. Best viewed in color for all ﬁgures.\nSwin Transformer [28] and adapt it to Siamese tracking. Note that, other Transformer architectures\ncan be used. For feature fusion, we introduce a simple homogeneous concatenation-based fusion\narchitecture, without a query-based decoder.\nMoreover, taking into consideration that tracking is a temporal task, we propose a novel motion\ntoken to improve robustness. Inspired by that the target usually moves smoothly in a short period,\nmotion token is represented by the historical target trajectory within a local temporal window. We\nincorporate the (single) motion token in the decoder of feature fusion to leverage motion information\nduring tracking. Despite being conceptually simple, our motion token can effectively boost tracking\nperformance, with negligible computation.\nWe name our framework SwinTrack. As a pure Transformer framework, SwinTrack enables better\ninteractions inside the feature learning of template and search region and their fusion compared\nto pure CNN-based [1, 25] and hybrid CNN-Transformer [5, 40, 46] frameworks, leading to more\nrobust performance (see Fig. 1). Fig. 2 demonstrates the architecture of SwinTrack. We conduct\nextensive experiments on ﬁve large-scale benchmarks to verify the effectiveness of SwinTrack,\nincluding LaSOT [11], LaSOText [10], TrackingNet [33], GOT-10k [19] and TNL2k [42]. On all\nbenchmarks, SwinTrack achieves promising results and meanwhile runs fast at 45 fps. In particular,\non the challenging LaSOT, SwinTrack sets a new record of 71.3 SUC score, surpassing the strongest\nprior tracker [46] (to date) by 3.1 absolute percentage points and crossing the 0.7 SUC threshold\nfor the ﬁrst time (see Fig. 1 again). It also achieves 49.1 SUC, 84.0 SUC, 72.4 AO and 55.9 SUC\nscores on LaSOText, TrackingNet, GOT-10k and TNL2k respectively, which are better than or on par\nwith state-of-the-arts (SoTAs). In addition, we provide a lighter version of SwinTrack that obtains\ncomparable results to SoTAs but runs much faster at around 98 fps.\nIn summary, our contributions are as follows: (i) We propose SwinTrack, a simple and strong baseline\nfor fully attentional tracking; ( ii) We present a simple yet effective motion token, enabling the\nintegration of rich motion context during tracking, further boosting the robustness of SwinTrack,\nwith negligible computation; (iii) Our proposed SwinTrack achieves state-of-the-art performance on\nmultiple benchmarks. We believe SwinTrack further shows the potential of Transformer and expect it\nto serve as a baseline for future research.\n2 Related Work\nSiamese Tracking. The Siamese tracking methods formulate tracking as a matching problem and\naim to ofﬂine learn a generic matching function for this task. The seminal method of [1] introduces\n2\nC\nSwin Transformer\nStages 1\nSwin Transformer\nStages 2\nSwin Transformer\nStages 3\nSwin Transformer\nStages 1\nSwin Transformer\nStages 2\nSwin Transformer\nStages 3\nBackbone sharing weight\nself-attention\nencoder block ×N\nEncoder\ncross-attention\ndecoder block ×1\nDecoder\nBox Reg.\nIoU-aware Cls.\nPatch PartitionPatch Partition\nTransformer-based Feature Representation Extraction Transformer-based Feature Fusion Prediction Head\nconcatenation\nTarget Template\nSearch Region\nResult\nMotion Token \nConstruction\n…\nHistorical Object Trajectory\nupdate trajectorymotion token\ncopy\nFigure 2: Architecture of SwinTrack, which contains three parts including Transformer-based feature\nrepresentation extraction, Transformer-based feature fusion and prediction head. Our SwinTrack\nis a simple and neat tracking framework without complex designs such as multi-scale features or\ntemporal template updating, yet demonstrating state-of-the-art performance. Best viewed in color.\na fully convolutional Siamese network for tracking and shows a good balance between accuracy\nand speed. To improve Siamese tracking in handling scale variation, the work of [25] incorporates\nthe region proposal network (RPN) [34] into the Siamese network and proposes the anchor-based\ntracker, showing higher accuracy with faster speed. Later, numerous extensions have been presented\nto improve Siamese tracking, including deeper backbone [ 24], multi-stage architecture [ 12, 13],\nanchor-free Siamese trackers [52], deformable attention [48], to name a few.\nTransformer in Vision. Transformer [38] originates from natural language processing (NLP) for\nmachine translation and has been introduced to vision recently and shows great potential. The\nwork of [3] ﬁrst uses Transformer for object detection and achieved promising results. To explore\nthe capability of Transformer in representation learning, the work of [ 9] applies Transformer to\nconstruct backbone network, and the resulting Vision Transformer (ViT) attains excellent performance\ncompared to convolutional networks while requiring fewer training resources, which encourages\nmany extensions upon ViT[37, 4, 49, 41, 28]. Among them, the Swin Transformer [28] has received\nextensive attention. It proposes a simple shifted window strategy to replace the ﬁxed-patch method in\nViT, which signiﬁcantly improves efﬁciency and meanwhile demonstrates state-of-the-art results on\nmultiple image tasks. Our work is inspired by Swin Transformer, but differently, we focus on the\nvideo task of visual tracking.\nTransformer in Tracking. Inspired by the success in other ﬁelds, researchers have leveraged\nTransformer for tracking. The method of [ 5] applies Transformer to enhance and fuse features in\nthe Siamese tracking for improvement. The approach of [40] uses Transformer to exploit temporal\nfeatures to improve tracking robustness. The work of [46] introduces a new transformer architecture\ndedicated to visual tracking, explores the Spatio-temporal Transformer by integrating the model\nupdating operations into a Transformer module.\nOur SwinTrack is related to but signiﬁcantly different from the above Transformer-based trackers.\nSpeciﬁcally, the aforementioned methods mainly apply Transformer to fuse convolutional features and\nbelong to the hybrid CNN-Transformer architecture. Unlike them, SwinTrack is a pure Transformer-\nbased tracking architecture where both representation learning and feature fusion are realized with\nTransformer, enabling the exploration of better features for robust tracking.\n3 Tracking via Vision-Motion Transformer\nWe present SwinTrack, a vision-motion integrated Transformer for object tracking, in Fig. 2. The\nproposed framework contains three main components, i.e., the Swin-Transformer backbone for\nfeature extraction, the encoder-decoder network for mixing vision-motion cues, and the head network\nfor localizing targets. In the following sections, we ﬁrst shortly describe the Swin backbone network,\nthen elaborate on the proposed vision-motion encoder-decoder. Afterward, we give a discussion\nabout our method and shortly describe the network head and training loss.\n3\n3.1 Swin-Transformer for Feature Extraction\nThe deep convolutional neural network has signiﬁcantly improved the performance of trackers.\nAlong with the advancement of trackers, the backbone network has evolved twice: AlexNet [22] and\nResNet [18]. Swin-Transformer [ 28], in comparison to ResNet, can give a more compact feature\nrepresentation and richer semantic information to assist succeeding networks in better localizing the\ntarget objects (demonstrate in the ablation study demonstrated in the ablation study), which is thus\nchosen for basic feature extraction in our model.\nOur tracker, following Siamese tracking framework [1], requires a pair of image patches as inputs,i.e.,\ntemplate image z ∈RHz×Wz×3 and search region image x ∈RHx×Wx×3. As in the typical Swin-\nTransformer procedure, template and search region images are divided to non-overlapped patches\nand sent to the network, which generates template tokens (dubbed T-tokens) ϕ(z) ∈R\nHz\ns\nWz\ns ×C and\nsearch region tokens (dubbed S-tokens) ϕ(x) ∈R\nHx\ns\nWx\ns ×C. sis the stride of the backbone network.\nSince there is no dimension projection in our model, Cis the hidden dimension of the whole model.\n3.2 Vision-Motion Representation Learning\nThe essential step for matching-based visual tracking is injecting the template information into the\nsearch region. In our framework, we adopt an encoder to fuse the features from the template and the\nsearch region, meanwhile, a decoder is arranged to achieve vision-motion representation learning, as\nillustrated in Fig. 2.\nEncoder for fusing template and search tokens. The encoder contains a sequence of Transformer\nblocks where each consists of a multi-head self-attention (MSA) module and a feed-forward network\n(FFN). FFN contains a two-layers multi-layer perceptron (MLP), GELU activation layer is inserted\nafter the ﬁrst linear layer. Layer normalization (LN) is always conducted before every module (MSA\nand FFN). Residual connection is applied to MSA and FFN modules.\nBefore feeding the features into the encoder, the template and search region tokens are concatenated\nalong spatial dimensions to generate a mixing representation fm. For each block, the MSA module\ncomputes self-attention over mixing union representation, which equals to separately conducting\nself-attention on T-tokens/S-tokens and meanwhile performing cross-attention between T-tokens and\nS-tokens, but more efﬁcient. FFN reﬁnes the features generated by MSA. When the tokens get out\nof the encoder, a de-concatenation operation is arranged to decouple the template and search region\ntokens. The process of encoder can be expressed as:\nf1\nm = Concat(ϕ(z),ϕ(x))\n...\nfl′\nm = fl\nm + MSA(LN(fl\nm))\nfl+1\nm = fl′\nm + FFN(LN(fl′\nm))\n...\nfL\nz ,fL\nx = DeConcat(fL),\n(1)\nwhere ldenotes the l-th layer and Ldenotes the number of blocks.\nDecoder for fusing vision and motion information. Before describing the architecture of decoder,\nwe ﬁrst detail how to generate a motion token (dubbed M-token). Motion token is the embedding\nof the historical trajectory of the target object. The past object trajectory is represented as a set of\ntarget object box coordinates, T = {o 1,o 2,..., o t}, where trepresents the frame index, o is the\nbounding box of target object. o is deﬁned by the top-left and bottom-right corners of the target\nobject, denotes as o t = (ox1\nt ,oy1\nt ,ox2\nt ,oy2\nt ). For ﬂexible modeling, a sampling process is required\nto ensure the following properties: 1) ﬁxed length , 2) focusing on the latest trajectories and 3)\nreducing redundancy. In our method, we sample object trajectory as:\nT = {o s(1),o s(2),..., o s(n)}, where s(i) = max(t−i×∆,1), (2)\nnis the number of sampled object trajectories, ∆ is the ﬁxed sampling interval. For Siamese tracker,\nthe search region is cropped from the input image. In detail, a cropping with resizing operation can be\n4\nused to describe the process. Giving the point in the input image as (xi,yi), the corresponding point\nin the search region as (xo,yo), we can formulate the cropping process employed in pre-processing\nof the Siamese Tracker as xo = (xi −ix)sx + ox and yo = (yi −iy)sy + oy, where (ix,iy) is\nthe center of the cropping window in the input image, (sx,sy) is the scaling factor, (ox,oy) is\nthe center of cropped and scaled window in the search region. We apply the same transformation\non the sampled object trajectory to make the object trajectory invariant to the cropping, denoting\n¯T = {¯o s(1),¯o s(2),..., ¯o s(n)}as the result.\nThen, to embed the transformed object trajectory into the network, we adopt four embedding\nmatrices to embed the elements in box coordinates separately. We denotes the embedding matrix as\nW ∈R(g +1)×d, g controls the embedding granularity of the object trajectory, dis the size of each\nembedding vector. The last entry of the embedding matrix is used as the padding vector, indicating an\ninvalid state, like object absence or out of the search region. Thus, we normalize the sampled target\nobject box coordinates in the range [1,g ], and quantize to integers to get the index of embedding\nvector:\nˆT = {ˆo s(1),ˆo s(2),..., ˆo s(n)},\nwhere ˆo s(i) = [n (¯o x1\ns(i),w),n (¯o y1\ns(i),h),n (¯o x2\ns(i),w),n (¯o y2\ns(i),h)],\nn (o,l) =\n{\n⌊o\nl ×g ⌋ if valid,\ng + 1 else,\n(3)\nwhere (w,h) is the size of search region feature map.\nFinally, the motion token Emotion ∈ R1×d is given by a concatenation of all box coordinate\nembedding of the sampled object trajectory. FLOPs is negligible because the construction of motion\ntoken is just a composition of embedding lookups and token concatenation.\nThe decoder consists of a multi-head cross-attention(MCA) module and a feed-forward network(FFN).\nThe decoder takes the outputs from the encoder and the motion token as input, generating the ﬁnal\nvision-motion representation fvm ∈R\nHx\ns ×Wx\ns ×C of by computing cross-attention over fL\nx and\nConcat(Emotion,fL\nz ,fL\nx ). The decoder is akin to a layer in the encoder, except that the correlation\nbetween the template tokens and the search tokens is dropped since we do not need to update the\nfeatures from the template image in the last layer. The process of the decoder is formulated as:\nfD\nm = Concat(Emotion,fL\nz ,fL\nx )\nf′\nvm = fL\nx + MCA(LN(fL\nx ),LN(fD\nm ))\nfvm = f′\nvm + FFN(LN(f′\nvm)).\n(4)\nfvm will feed to the head network to generate a classiﬁcation response map and a bounding box\nregression map.\nPositional encoding. Transformer requires a positional encoding to identify the position of the\ncurrent processing token[38] because the self-attention module is permutation-invariance. We adopt\nthe untied positional encoding [20] as our positional encoding method. The untied positional\nencoding enhances the expressiveness of the model through untie the positional embeddings from\ntoken embeddings with an isolated positional embedding matrix. It also considers the case of special\ntokens, like the motion token in this paper. We generalize the untied positional encoding to multi-\ndimensions multi-sources data to comply with concatenated-based fusion in our tracker. See the\nappendix for the details.\n3.3 Discussion\nWhy concatenated attention? To simplify the description, we call the method described above\nconcatenation-based fusion. To fuse and process features from multiple sources, it is intuitive to\nperform self-attention on the feature from each source separately and then compute cross-attention\nacross features from different sources. We call this method cross-attention-based fusion. Transformer\nmakes fewer assumptions about the spatial structure of data, which provides great modeling ﬂexibility.\n5\nIn comparison to cross-attention-based fusion, concatenation-based fusion can save computation cost\nthrough operation sharing and reduce model parameters through weight sharing. From the perspective\nof metric learning, weight sharing is an essential design to ensure the metric between two branches of\ndata is symmetric. Through concatenation-based fusion, we implement this property not only in the\nfeature extraction stage but also in the feature fusion stage. In general, concatenation-based fusion\nimproves both efﬁciency and performance.\nWhy not window-based self/cross-attention? Since we select stage 3 of the Swin-Transformer as\nthe output, the number of tokens involved is signiﬁcantly reduced, window-based attention cannot\nsave too many FLOPs. Furthermore, considering the extra latency introduced by the window partition\nand window reverse operations, window-based attention may even be the slower one.\nWhy not a query-based decoder? Derivated from vanilla Transformer decoder, many transformer-\nbased models in vision tasks leverage a learnable query to extract the desired objective features from\nthe encoder, like object queries in [3], target query in [46]. However, in our experiment, a query-based\ndecoder suffers from slow convergence and inferior performance. Most Siamese trackers [25, 44, 16]\nformulate tracking as a foreground-background classiﬁcation problem, which can better exploit the\nbackground information. The vanilla Transformer decoder is a generative model, the generative\napproaches are considered not suitable for the classiﬁcation tasks. In another aspect, learning a\ngeneral target query for any kind of object might cause a bottleneck. In terms of vanilla Transformer\nencoder-decoder architecture, SwinTrack is an \"encoder\" only model. Furthermore, quite a little\ndomain knowledge can be easily applied on a classic Siamese tracker to improve the performance,\nlike introducing the smooth movement assumption by using Hanning penalty window on the response\nmap.\nAre other forms of motion token feasible? Other forms to construct motion token are possible,\nsuch as constructing motion token by summing up the past box coordinate embeddings or representing\npast object trajectories by one token per box. In our early experiments, we ﬁnd that the proposed\nmotion token is more effective with the best performance. Summing up the past box coordinate\nembeddings may result in over-parameterization on the coordinate embeddings. While adding\ntemporal motion representation along with visual features to the single-layer decoder in a multi-token\nform is ineffective, precise temporal modeling may be required in this form.\n3.4 Head and Loss\nHead. The head network is split into two branches: classiﬁcation and bounding box regression. Each\nof them is a three-layer perceptron. And both of them receives the feature map from the decoder as\ninput to predict the classiﬁcation response map rcls ∈R(Hx×Wx)×1 and bounding box regression\nmap rreg ∈R(Hx×Wx)×4, respectively.\nClassiﬁcation loss. In classiﬁcation branch, we employ the IoU-aware classiﬁcation score as the\ntraining target and thevarifocal loss [50] as the training loss function. IoU-aware design has been very\npopular recently, but most works consider IoU prediction as an auxiliary branch to assist classiﬁcation\nor bounding box regression [52, 2, 44]. To remove the gap between different prediction branches,\n[50] and [26] replace the hard classiﬁcation target from the ground-truth value, (i.e., 1 for positive\nsamples, 0 for negative samples), to the IoU between the predicted bounding box and the ground-truth\none, which is named the IoU-aware classiﬁcation score (IACS). IACS can help the model select a\nmore accurate bounding box prediction candidate from the pool by trying to predict the quality of the\nbounding box prediction in another branch at the same position. Along with the IACS, the varifocal\nloss was proposed in [50] to help the IACS approach outperform other IoU-aware designs.\nThe classiﬁcation loss can be formulated as:\nLcls = LVFL(p,IoU(b,ˆb)), (5)\nwhere p denotes the predicted IACS, b denotes the predicted bounding box, and ˆb denotes the\nground-truth bounding box.\nRegression loss. For bounding box regression, we employ the generalized IoU loss[ 35]. The\nregression loss function can be formulated as:\nLreg =\n∑\nj\n1 {IoU(bj,ˆb)>0}[pLGIoU(bj,ˆb)]. (6)\n6\nThe GIoU loss is weighted by pto emphasize the high classiﬁcation score samples. The training\nsignals from the negative samples are ignored.\n4 Experiments\n4.1 Implementation\nModel. We design two variants of SwinTrack with different conﬁgurations as follows:\n• SwinTrack-T-224.\nBackbone: Swin Transformer-Tiny [28], pretrained with ImageNet-1k;\nTemplate size: [112 ×112]; Search region size: [224 ×224]; C = 384; N = 4;\n• SwinTrack-B-384.\nBackbone: Swin Transformer-Base [28], pretrained with ImageNet-22k;\nTemplate size: [192 ×192]; Search region size: [384 ×384]; C = 512; N = 8;\nwhere Cand N are the channel number of the hidden layers in the ﬁrst stage of Swin Transformer\nand the number of encoder blocks in feature fusion, respectively. In all variants, we use the output\nafter the third stage of Swin Transformer for feature extraction. Thus, the backbone stride sis 16.\nFor motion token, the number of sampled object trajectory nis set to 16, the ﬁxed sampling interval\n∆ is set to 15. If the frame rate of the video sequence is available, the sampling interval is adjusted\naccording to the frame rate. Suppose the frame rate is f , the new sampling interval is getting by\n∆\n30 f , 30 fps is the standard frame rate we assumed. g , which controls the embedding granularity,\nis set to the same size as the search region feature map, like 14 for SwinTrack-T-224, and 24 for\nSwinTrack-B-384. For the model for GOT-10k sequences,nis set to 8, ∆ is set to 8, and no frame\nrate adjustment is applied.\nTraining. We train SwinTrack using the training splits of LaSOT [ 11], TrackingNet [33], GOT-\n10k [19] (1,000 videos are removed following [46] for fair comparison) and COCO 2017 [27]. In\naddition, we report the results of SwinTrack-T-224 and SwinTrack-B-384 with GOT-10k training\nsplit only to follow the protocol described in [19].\nThe model is optimized with AdamW [29], with a learning rate of 5e-4, and a weight decay of 1e-4.\nThe learning rate of the backbone is set to 5e-5. We train the network on 8 NVIDIA V100 GPUs\nfor 300 epochs with 131,072 samples per epoch. The learning rate is dropped by a factor of 10 after\n210 epochs. A 3-epoch linear warmup is applied to stabilize the training process. DropPath [23] is\napplied on the backbone and the encoder with a rate of 0.1. For the models trained for the GOT-10k\nevaluation protocol, to prevent over-ﬁtting, the training epoch is set to 150, and the learning rate is\ndropped after 120 epochs.\nFor the motion token, the object trajectory for the Siamese training pair is generated with the method\ndescribed above. The frames that object annotated as absent or out of the video sequence are\nmarked as invalid, the corresponding box coordinates set to −∞. Since the coarse granularity of the\ncoordinate embedding in our setting is already can be seen as an augmentation of historical object\ntrajectory, no additional data augmentation is applied.\nInference. We follow the common procedures for Siamese network-based tracking [1]. The template\nimage is cropped from the ﬁrst frame of the video sequence. The target object is in the center\nof the image with a background area factor of 2. The search region is cropped from the current\ntracking frame, and the image center is the target center position predicted in the previous frame. The\nbackground area factor for the search region is 4.\nOur SwinTrack takes the template image and search region as inputs and output classiﬁcation map\nrcls and regression map rreg. To utilize positional prior in tracking, we apply hanning window penalty\non rcls, and the ﬁnal classiﬁcation map r′\ncls is obtained via r′\ncls = (1 −γ) ×rcls + γ×h, where γis\nthe weight parameter and his the Hanning window with the same size as rcls. The target position\nis determined by the largest value in r′\ncls and the scale is estimated based on the corresponding\nregression results in rreg.\nFor the motion token, the predicted conﬁdence score and bounding box are collected on the ﬂy. A\nconﬁdence threshold θconf is applied, if the conﬁdence score given by the classiﬁcation branch of the\n7\nTable 1: Experiments and comparisons on ﬁve benchmarks: LaSOT, LaSOText, TrackingNet, GOT-\n10k and TNL2k.\nTracker LaSOT [11] LaSOText [10] TrackingNet [33] GOT-10k [19] TNL2k [42]\nSUC P SUC P SUC P AO SR 0.5 SR0.75 SUC P\nC-RPN [12] 45.5 44.3 27.5 32.0 66.9 61.9 - - - - -\nSiamPRN++ [24] 49.6 49.1 34.0 39.6 73.3 69.4 51.7 61.6 32.5 41.3 41.2\nOcean [52] 56.0 56.6 - - - - 61.1 72.1 47.3 38.4 37.7\nDiMP [2] 56.9 56.7 39.2 45.1 74.0 68.7 61.1 71.7 49.2 44.7 43.4\nLTMU [7] 57.2 57.2 41.4 47.3 - - - - - 48.5 47.3\nSiamR-CNN [39] 64.8 - - - 81.2 80.0 64.9 72.8 59.7 52.3 52.8\nSTMTrack [14] 60.6 63.3 - - 80.3 76.7 64.2 73.7 57.5 - -\nAutoMatch [51] 58.3 59.9 37.6 43.0 76.0 72.6 65.2 76.6 54.3 - -\nTrDiMP [40] 63.9 61.4 - - 78.4 73.1 67.1 77.7 58.3 - -\nTransT [5] 64.9 69.0 - - 81.4 80.3 67.1 76.8 60.9 51.0 -\nSTARK [46] 67.1 - - - 82.0 - 68.8 78.1 64.1 - -\nKeepTrack [31] 67.1 70.2 48.2 - - - - - - - -\nSwinTrack-T-224 67.2 70.8 47.6 53.9 81.1 78.4 71.3 81.9 64.5 53.0 53.2\nSwinTrack-B-384 71.3 76.5 49.1 55.6 84.0 82.8 72.4 80.5 67.8 55.9 57.1\nhead is lower than the threshold, the target object in the current frame is marked as lost by setting the\ncollected bounding box to −∞. θconf is set to 0.4 for LaSOT, the rests are set to 0.3.\n4.2 Comparisons to State-of-the-arts\nWe conduct experiments and compare SwinTrack with SoTA trackers on ﬁve benchmarks.\nLaSOT.LaSOT [11] consists of 280 videos for test. Tab. 1 shows the results and comparisons with\nSoTAs. From Tab. 1, we can observe that SwinTrack-T-224 with light architecture reaches SoTA\nperformance with 0.672 SUC and 0.708 PRE scores, which is competitive compared with other\nTransformer-based trackers, including STARK-ST101 (0.671 SUC score) and TransT (0.649 SUC),\nand other trackers using complicated designs such as KeepTrack (0.671 SUC) and SiamR-CNN\n(0.648 SUC score). With a larger backbone and input size, our strongest variant SwinTrack-B-384\nsets a new record with 0.713 SUC score, surpassing START-ST101 and KeepTrack by 4.2 absolute\npercentage points.\nLaSOText. The recent LaSOText [10] is an extension of LaSOT by adding 150 extra videos. These\nnew sequences are challenging as many similar distractors cause difﬁculties for tracking. The results\nof our tracker related to this dataset are an average of three times. KeepTrack uses a complex\nassociation technique to handle distractors and achieves a promising 0.482 SUC score as in Tab. 1.\nCompared with complicated KeepTrack, SwinTrack-T-224 is simple and neat, yet shows comparable\nperformance with 0.476 SUC score. In addition, due to complicated design, KeepTrack runs at less\nthan 20 fps, while SwinTrack-T-224 runs in 98fps, 5×faster than KeepTrack. When using a larger\nmodel, SwinTrack-B-384 shows the best performance with 0.491 SUC score.\nTrackingNet. We evaluate different trackers on the test set of TrackingNet [33]. From Tab. 1, we\nobserve that our SwinTrack-T-224 achieves a comparable result with 0.811 SUC score. Using a larger\nmodel and input size, SwinTrack-B-384 obtains the best performance with 0.840 SUC score, better\nthan STARK-ST101 with 0.820 SUC score and TransT with 0.814 SUC score.\nGOT-10k. GOT-10k [19] offers 180 videos for test and it requires trackers to be trained using\nGOT-10k train split only. From Tab. 1, we see that SwinTrack-B-384 achieves the best mAO of 0.724,\nand SwinTrack-T-224 obtains a mAO of 0.713. Both models outperform other Transformer-based\ncounterparts signiﬁcantly, including START-ST101 (0.688 mAO), TransT (0.671 mAO) and TrDiMP\n(0.671 mAO).\nTNL2k. TNL2k [42] is a newly released tracking dataset with 700 videos for test. As reported in\nTab. 1, both models surpass the others. SwinTrack-B-384 set a new state-of-the-art with 0.559 SUC\nscore.\nEfﬁciency comparison. We report the comparisons of SwinTrack with other Transformer-based\ntrackers in terms of efﬁciency and complexity. As displayed in Tab. 2, SwinTrack-T-224 with a\nsmall model runs the fastest with a speed of 98 fps. Especially, compared with STARK-ST101 and\n8\nTable 2: Comparison on running speed and # parameters with other Transformer-based trackers.\nTracker Speed ( fps) MACs 2 (G) Params (M)\nTrDiMP [40] 26 - -\nTransT [5] 50 - 23\nSTARK-ST50 [46] 42 10.9 24\nSTARK-ST101 [46] 32 18.5 42\nSwinTrack-T-224 98 6.4 23\nSwinTrack-B-384 45 69.7 91\nTable 3: Ablation experiments of SwinTrack on four benchmarks. The experiments are conducted\non SwinTrack-T-224 without the motion token. : baseline method, i.e., SwinTrack-T-224 without\nmotion token; : replacing Transformer backbone in the baseline method with ResNet-50; :\nreplacing our feature fusion with cross attention-based fusion in the baseline method; : replacing\nthe decoder in baseline with a target query-based; : replacing united positional encoding with\nabsolute sine position encoding in the baseline method; : replacing the IoU-aware classiﬁcation loss\nwith the plain binary cross entropy loss; : removing the Hanning penalty window in the baseline\nmethod inference.\nLaSOT\nSUC (%)\nLaSOText\nSUC (%)\nTrackingNet\nSUC (%)\nGOT-10k3\nmAO (%)\nSpeed\nfps\nParams\nM\n 66.7 46.9 80.8 70.9 98 22.7\n 64.2 41.8 79.5 68.2 121 20.0\n 66.6 45.4 80.2 69.3 72 34.6\n 66.6 43.2 79.6 69.0 91 25.3\n 65.7 45.0 80.0 70.0 103 21.6\n 66.2 46.7 79.4 68.2 98 22.7\n 65.7 46.0 80.0 69.6 98 22.7\nSTARK-ST50 with 32 fps and 42 fps, SwinTrack-T-224 is 3×and 2×faster. Despite using a larger\nmodel, our SwinTrack-B-384 is still faster than STARK-ST101 and STARK-ST50.\n4.3 Ablation Experiment\nComparison with ResNet backbone. We compare the Swin-Transformer backbone with popular\nResNet-50 [18]. As shown in Tab. 3 ( vs. ). The Swin Transformer backbone signiﬁcantly boosts\nthe performance by 2.5% SUC score in LaSOT, 5.1% SUC score in LaSOText. The result shows that\nthe strong appearance modeling capability provided by the Swin Transformer plays a crucial role.\nFeature fusion. As displayed in Tab. 3 ( vs. ), compared with the concatenation-based fusion,\nthe cross attention-based fusion runs at a slower speed, occupies much more memory, and also has\nan inferior performance on all datasets. Slower speed can be due to the latency brought by the extra\noperations. The parameter-sharing strategy not only just reduces the number of parameters but also\nbeneﬁts metric learning.\nComparison with the query-based decoder. Queries is commonly adopted in the decoder of\nTransformer network in vision tasks, e.g. object query [3] and target query [46]. Nevertheless, our\nempirical results in Tab. 3 ( vs. ) show that a target query-based decoder degrades the tracking\nperformance on all benchmarks, even with 2×training pairs. As discussed, one possible reason is the\ngenerative model is not suitable for classiﬁcation. Besides, learning a general target query for any\nkind of object may also be difﬁcult.\nPosition encoding. We compare the united positional encoding used in SwinTrack and the original\nabsolute position encoding in Transformer [38]. Notice, We make a little modiﬁcation to the original\nabsolute position encoding: Except for the 2D embedding, the index of token source (e.g. 1 for the\ntokens from the template patch, 2 for the tokens from the search region patch) is also embedded. As\n2Multiply–accumulate operation\n3The GOT-10k results in this column are trained with full training datasets.\n9\nTable 4: Ablation experiments on our proposed motion token on the tracking performance on\nfour benchmarks. The experiments are conducted on SwinTrack-T-224. : SwinTrack-T-224; :\nSwinTrack-B-384; : SwinTrack-T-224 without motion token; : SwinTrack-B-384 without motion\ntoken; : replacing the motion token in SwinTrack-T-224 with a learnable embedding token.\nLaSOT\nSUC (%)\nLaSOText\nSUC (%)\nTrackingNet\nSUC (%)\nGOT-10k\nmAO (%)\nSpeed\nfps\n 67.2 47.6 81.1 71.3 96\n 71.3 49.1 84.0 72.4 45\n 66.7 47.0 80.8 70.0 98\n 70.2 48.5 84.0 70.7 45\n 66.3 45.2 81.2 70.0 96\nshown in Tab. 3 ( vs. ), our method with united positional encoding obtains improvements with\n0.8-1.9 absolute percentage points on the benchmarks with negligible loss in speed (98 vs. 103).\nLoss function. From Tab. 3 (  vs. ), we observe that the model trained with varifocal loss\nsigniﬁcantly outperforms the one with binary cross entropy (BCE) loss without loss of efﬁciency.\nThis result indicates that the varifocal loss can assist the classiﬁcation branch of the head to generate\nan IoU-aware response map, and thus help the tracker to improve the tracking performance.\nPost processing. One may wonder with highly discriminative Transformer architecture and IoU-\naware classiﬁcation loss does the hanning penalty window is still functional, which introduces a\nstrong smooth movement assumption. In the experiments, we remove the hanning penalty window in\npost-processing, as shown in Tab. 3 ( vs. ), the performance is dropped by 1.0 SUC for LaSOT,\n1.3 AO for GOT-10k in absolute percentage, and less than 1% in the SUC metric of other datasets.\nThis suggests that the strong smooth movement assumption is still applicable for our tracker. But\ncompared with the former Transformer-based tracker [5], the performance gap between with and\nwithout penalty window post-processing is narrowing.\nEffectiveness of motion token. We study the effectiveness of the motion token by conducting\ncomparison experiments. As shown in Tab. 4 ( vs.  and  vs. ), the models with motion token\noutperforms the models without motion token on all datasets, especially on LaSOText and GOT-10k.\nThe results indicate that the motion token can assist the tracker to handle hard similar distractors in\nLaSOText and stabilize the short-term tracking like the sequences in GOT-10k test set. We also study\nwhether the effectiveness of the motion token is simply from the extra embedding vector. We set up\nan experiment as in Tab. 4 (), which replaces the motion token with a learnable embedding token.\nThe result shows that the extra embedding vector has negative impacts indicating the effectiveness of\nthe embedding of object trajectory.\n5 Conclusion\nIn this work, we present SwinTrack, a simple and strong baseline for Transformer tracking. In\nSwinTrack, both representation learning and feature fusion are implemented with the attention\nmechanism. Extensive experiments demonstrate the effectiveness of such architecture. Besides, we\npropose the motion token to enhance the robustness of the tracker by providing the historical object\ntrajectory, showing the ﬂexibility of the Transformer model in architectural design. With the power\nof sequence-to-sequence model architecture, a context-rich tracker is possible, and more contextual\ncues can be incorporated. Finally, We hope this work can inspire and facilitate future research.\nAcknowledgments and Disclosure of Funding\nThis work is supported by Peng Cheng Laboratory Research Project No. PCL2021A07. Heng Fan\nand his employer receives no ﬁnancial support for the research, authorship, and/or publication of this\narticle.\n10\nReferences\n[1] Bertinetto, L., Valmadre, J., Henriques, J.F., Vedaldi, A., Torr, P.H., 2016. Fully-convolutional siamese\nnetworks for object tracking, in: ECCVW.\n[2] Bhat, G., Danelljan, M., Gool, L.V ., Timofte, R., 2019. Learning discriminative model prediction for\ntracking, in: ICCV .\n[3] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S., 2020. End-to-end object\ndetection with transformers, in: ECCV .\n[4] Chen, C.F.R., Fan, Q., Panda, R., 2021a. Crossvit: Cross-attention multi-scale vision transformer for image\nclassiﬁcation, in: ICCV .\n[5] Chen, X., Yan, B., Zhu, J., Wang, D., Yang, X., Lu, H., 2021b. Transformer tracking, in: CVPR.\n[6] Cui, Y ., Jiang, C., Wang, L., Wu, G., 2022. Mixformer: End-to-end tracking with iterative mixed attention,\nin: CVPR.\n[7] Dai, K., Zhang, Y ., Wang, D., Li, J., Lu, H., Yang, X., 2020. High-performance long-term tracking with\nmeta-updater, in: CVPR.\n[8] Danelljan, M., Bhat, G., Khan, F.S., Felsberg, M., 2019. Atom: Accurate tracking by overlap maximization,\nin: CVPR.\n[9] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M.,\nMinderer, M., Heigold, G., Gelly, S., et al., 2021. An image is worth 16x16 words: Transformers for image\nrecognition at scale, in: ICLR.\n[10] Fan, H., Bai, H., Lin, L., Yang, F., Chu, P., Deng, G., Yu, S., Huang, M., Liu, J., Xu, Y ., et al., 2021. Lasot:\nA high-quality large-scale single object tracking benchmark. International Journal of Computer Vision\n129, 439–461.\n[11] Fan, H., Lin, L., Yang, F., Chu, P., Deng, G., Yu, S., Bai, H., Xu, Y ., Liao, C., Ling, H., 2019. Lasot: A\nhigh-quality benchmark for large-scale single object tracking, in: CVPR.\n[12] Fan, H., Ling, H., 2019. Siamese cascaded region proposal networks for real-time visual tracking, in:\nCVPR.\n[13] Fan, H., Ling, H., 2021. Cract: Cascaded regression-align-classiﬁcation for robust visual tracking, in:\nIROS.\n[14] Fu, Z., Liu, Q., Fu, Z., Wang, Y ., 2021. Stmtrack: Template-free visual tracking with space-time memory\nnetworks, in: CVPR.\n[15] Gao, S., Zhou, C., Ma, C., Wang, X., Yuan, J., 2022. Aiatrack: Attention in attention for transformer visual\ntracking .\n[16] Han, W., Dong, X., Khan, F.S., Shao, L., Shen, J., 2021. Learning to fuse asymmetric feature maps in\nsiamese trackers, in: CVPR.\n[17] He, K., Chen, X., Xie, S., Li, Y ., Dollár, P., Girshick, R., 2022. Masked autoencoders are scalable vision\nlearners, in: CVPR.\n[18] He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recognition, in: CVPR.\n[19] Huang, L., Zhao, X., Huang, K., 2019. Got-10k: A large high-diversity benchmark for generic object\ntracking in the wild. IEEE Transactions on Pattern Analysis and Machine Intelligence 43, 1562–1577.\n[20] Ke, G., He, D., Liu, T.Y ., 2021. Rethinking positional encoding in language pre-training, in: ICLR.\n[21] Kristan, M., Matas, J., Leonardis, A., V ojir, T., Pﬂugfelder, R., Fernandez, G., Nebehay, G., Porikli,\nF., ˇCehovin, L., 2016. A novel performance evaluation methodology for single-target trackers. IEEE\nTransactions on Pattern Analysis and Machine Intelligence 38, 2137–2155.\n[22] Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. Imagenet classiﬁcation with deep convolutional neural\nnetworks. NIPS .\n[23] Larsson, G., Maire, M., Shakhnarovich, G., 2016. Fractalnet: Ultra-deep neural networks without residuals,\nin: ICLR.\n11\n[24] Li, B., Wu, W., Wang, Q., Zhang, F., Xing, J., Yan, J.S., 2019. Evolution of siamese visual tracking with\nvery deep networks, in: CVPR.\n[25] Li, B., Yan, J., Wu, W., Zhu, Z., Hu, X., 2018. High performance visual tracking with siamese region\nproposal network, in: CVPR.\n[26] Li, X., Wang, W., Wu, L., Chen, S., Hu, X., Li, J., Tang, J., Yang, J., 2020. Generalized focal loss: Learning\nqualiﬁed and distributed bounding boxes for dense object detection, in: NeurIPS.\n[27] Lin, T.Y ., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L., 2014.\nMicrosoft coco: Common objects in context, in: ECCV .\n[28] Liu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., Lin, S., Guo, B., 2021. Swin transformer: Hierarchical\nvision transformer using shifted windows. ICCV .\n[29] Loshchilov, I., Hutter, F., 2019. Decoupled weight decay regularization, in: ICLR.\n[30] Mayer, C., Danelljan, M., Bhat, G., Paul, M., Paudel, D.P., Yu, F., Van Gool, L., 2022. Transforming model\nprediction for tracking, in: CVPR.\n[31] Mayer, C., Danelljan, M., Paudel, D.P., Van Gool, L., 2021. Learning target candidate association to keep\ntrack of what not to track, in: ICCV .\n[32] Mueller, M., Smith, N., Ghanem, B., 2016. A benchmark and simulator for uav tracking, in: ECCV .\n[33] Muller, M., Bibi, A., Giancola, S., Alsubaihi, S., Ghanem, B., 2018. Trackingnet: A large-scale dataset\nand benchmark for object tracking in the wild, in: ECCV .\n[34] Ren, S., He, K., Girshick, R., Sun, J., 2015. Faster r-cnn: Towards real-time object detection with region\nproposal networks, in: NIPS.\n[35] Rezatoﬁghi, H., Tsoi, N., Gwak, J., Sadeghian, A., Reid, I., Savarese, S., 2019. Generalized intersection\nover union .\n[36] Shaw, P., Uszkoreit, J., Vaswani, A., 2018. Self-attention with relative position representations. arXiv .\n[37] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jégou, H., 2021. Training data-efﬁcient\nimage transformers & distillation through attention, in: ICML.\n[38] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I.,\n2017. Attention is all you need, in: NeurIPS.\n[39] V oigtlaender, P., Luiten, J., Torr, P.H., Leibe, B., 2020. Siam r-cnn: Visual tracking by re-detection, in:\nCVPR.\n[40] Wang, N., Zhou, W., Wang, J., Li, H., 2021a. Transformer meets tracker: Exploiting temporal context for\nrobust visual tracking, in: CVPR.\n[41] Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L., 2021b. Pyramid vision\ntransformer: A versatile backbone for dense prediction without convolutions, in: ICCV .\n[42] Wang, X., Shu, X., Zhang, Z., Jiang, B., Wang, Y ., Tian, Y ., Wu, F., 2021c. Towards more ﬂexible and\naccurate object tracking with natural language: Algorithms and benchmark, in: CVPR.\n[43] Xie, F., Wang, C., Wang, G., Cao, Y ., Yang, W., Zeng, W., 2022. Correlation-aware deep tracking, in:\nCVPR.\n[44] Xu, Y ., Wang, Z., Li, Z., Yuan, Y ., Yu, G., 2020. Siamfc++: Towards robust and accurate visual tracking\nwith target estimation guidelines, in: AAAI.\n[45] Yan, B., Jiang, Y ., Sun, P., Wang, D., Yuan, Z., Luo, P., Lu, H., 2022. Towards grand uniﬁcation of object\ntracking, in: ECCV .\n[46] Yan, B., Peng, H., Fu, J., Wang, D., Lu, H., 2021. Learning spatio-temporal transformer for visual tracking,\nin: ICCV .\n[47] Ye, B., Chang, H., Ma, B., Shan, S., Chen, X., 2022. Joint feature learning and relation modeling for\ntracking: A one-stream framework .\n[48] Yu, Y ., Xiong, Y ., Huang, W., Scott, M.R., 2020. Deformable siamese attention networks for visual object\ntracking, in: CVPR.\n12\n[49] Yuan, L., Chen, Y ., Wang, T., Yu, W., Shi, Y ., Jiang, Z.H., Tay, F.E., Feng, J., Yan, S., 2021. Tokens-to-token\nvit: Training vision transformers from scratch on imagenet, in: ICCV .\n[50] Zhang, H., Wang, Y ., Dayoub, F., Sünderhauf, N., 2021a. Varifocalnet: An iou-aware dense object detector,\nin: CVPR.\n[51] Zhang, Z., Liu, Y ., Wang, X., Li, B., Hu, W., 2021b. Learn to match: Automatic matching network design\nfor visual tracking, in: ICCV .\n[52] Zhang, Z., Peng, H., Fu, J., Li, B., Hu, W., 2020. Ocean: Object-aware anchor-free tracking, in: ECCV .\n13\nAppendix\nA Positional Encoding\nTransformer requires a positional encoding to identify the position of the current processing token [38].\nThrough a series of comparison experiments, we chooseuntied positional encoding, which is proposed\nin TUPE [20], as the positional encoding solution of our tracker. In addition, we generalize the untied\npositional encoding to arbitrary dimensions to ﬁt with other components in our tracker.\nThe original transformer [ 38] proposes a absolute positional encoding method to represent the\nposition: a ﬁxed or learnable vector pi is assigned to each position i. Starting from the basic attention\nmodule, we have:\nAtten(Q,K,V ) = softmax\n(QKT\n√dk\nV\n)\n, (7)\nwhere Q,K,V are the query vector, key vector and valuevector, which are the parameters of the\nattention function, dk is the dimension ofkey. Introducing the linear projection matrix and multi-head\nattention to the attention module (7), we get the multi-head variant deﬁned in [38]:\nMultiHead(Q,K,V ) = Concat(head1,..., headh)WO, (8)\nwhere headi = Atten( QWQ\ni ,KW K\ni ,VW V\ni ), WQ\ni ∈Rdmodel×dk , WK\ni ∈Rdmodel×dk , WV\ni ∈\nRdmodel×dv , WO\ni ∈Rhdv×dmodel and his the number of heads. For simplicity, as in [20], we assume\nthat dk = dv = dmodel, and use the single-head version of self-attention module. Denoting the input\nsequence as x= x1,x2,...,x n, where nis the length of sequence, xi is the i-th token in the input\ndata. Denoting the output sequence as z= (z1,z2,...,z n). Self-attention module can be rewritten\nas\nzi =\nn∑\nj=1\nexp(αij)∑n\nj′=1 exp(αij′ )(xjWV ), (9)\nwhere αij = 1√\nd\n(xiWQ)(xjWK)T . (10)\nObviously, the self-attention module is permutation-invariance. Thus it can not \"understand\" the\norder of input tokens.\nUntied absolute positional encoding. By adding a learnable positional encoding [38] to the single-\nhead self-attention module, we can obtain the following equation:\nαAbs\nij = ((wi + pi)WQ)((wj + pj)WK)T\n√\nd\n= (wiWQ)(wjWK)T\n√\nd\n+ (wiWQ)(pjWK)T\n√\nd\n+ (piWQ)(wjWK)T\n√\nd\n+ (piWQ)(pjWK)T\n√\nd\n.\n(11)\nThe equation (11) is expanded into four terms: token-to-token, token-to-position, position-to-token,\nposition-to-position. [ 20] discuss the problems that exist in the equation and proposes the untied\nabsolute positional encoding, which unties the correlation between tokens and positions by removing\nthe token-position correlation terms in equation (11), and using an isolated pair of projection matrices\nUQ and UK to perform linear transformation upon positional embedding vector. The following is\nthe new formula for obtaining αij using the untied absolute positional encoding in the l-th layer:\nαij = 1√\n2d\n(xl\niWQ,l)(xl\njWK,l)T\n+ 1√\n2d\n(piUQ)(pjUK)T .\n(12)\nwhere pi and pj is the positional embedding at position i and j respectively, UQ ∈Rd×d and\nUK ∈Rd×d are learnable projection matrices for the positional embedding vector. When extending\n14\nto the multi-head version, the positional embedding pi is shared across different heads, while UQ and\nUK are different for each head.\nRelative positional bias. According to [36], relative positional encoding is a necessary supplement\nto absolute positional encoding. In [20], a relative positional encoding is applied by adding a relative\npositional bias to equation (12):\nαij = 1√\n2d\n(xl\niWQ,l)(xl\njWK,l)T\n+ 1√\n2d\n(piUQ)(pjUK)T + bj−i,\n(13)\nwhere for each j−i, bj−i is a learnable scalar. The relative positional bias is also shared across\nlayers. When extending to the multi-head version, bj−i is different for each head.\nGeneralize to multiple dimensions. Before working with our tracker’s encoder and decoder network,\nwe need to extend the untied positional encoding to a multi-dimensional version. One straightforward\nmethod is allocating a positional embedding matrix for every dimension and summing up all embed-\nding vectors from different dimensions at the corresponding index to represent the ﬁnal embedding\nvector. Together with relative positional bias, for an n-dimensional case, we have:\nαij...\nn\n,mn...  \nn\n= 1√\n2d\n(xij...\nn\nWQ)(xmn...  \nn\nWK)T\n+ 1√\n2d\n[(p1\ni + p2\nj + ... )\n  \nn\nUQ][(p1\nm + p2\nn + ... )  \nn\nUK]T\n+ bm−i,n −j,...  \nn\n.\n(14)\nGeneralize to concatenation-based fusion. In order to work with concatenation-based fusion, the\nuntied absolute positional encoding is also concatenated to match the real position, the indexing tuple\nof relative positional bias now appends with a pair of indices to reﬂect the origination of query and\nkeyinvolved currently.\nTake l-th layer in the encoder as the example:\nαij,mn,g,h = 1√\n2d\n(xl\nij,gWQ,l)(xl\nmn,hWK,l)T\n+ 1√\n2d\n[(p1\ni,g + p2\nj,g)UQ\ng ][(p1\nm,h + p2\nn,h)UK\nh ]T\n+ bm−i,n−j,g,h ,\n(15)\nwhere gand hare the index of the origination of query and keyrespectively, for instance, 1 for the\ntokens from the template image, 2 for the tokens from the search image. The form in the decoder is\nsimilar, except that gis ﬁxed. In our implementation, the parameters of untied positional encoding\nare shared inside the encoder and the decoder, respectively.\nB The Effect of Pre-training Datasets\nThe two variants of our tracker, SwinTrack-T-224 and SwinTrack-B-384 are using different pre-\ntraining datasets, which are derived from the settings from Swin Transformer [ 28]. Speciﬁcally,\nSwinTrack-T-224 adopts ImageNet-1k and SwinTrack-B-384 adopts ImageNet-22k.\nTo analyze the effect of different pre-training datasets, we conduct an experiment on the performance\nof our tracker with different pre-training datasets. Other than the pre-training datasets, The experiment\nfollows the same settings in the ablation study in the paper, the motion token is not used and the results\non GOT-10k are trained on the full datasets as described in the paper. From Tab. 5, we can observe\nthat, for smaller model SwinTrack-T-224 (23M # parameters), pre-training on ImageNet-22k brings\nsmall improvements on LaSOT (+0.6%) and TrackingNet (+0.4%) but degrades the performance on\n15\nTable 5: The effect of Imagenet-22k pre-training. The results are following the settings in the ablation\nstudy in the paper (motion token is not used and the result on GOT-10k is trained on the full dataset).\nTrackers Pre-training LaSOT [11] LaSOText [10] TrackingNet [33] GOT-10k [19]\nSUC P SUC P SUC P AO SR 0.5 SR0.75\nSwinTrack-T-224 ImageNet-1k 66.7 70.6 46.9 52.9 86.7 80.1 69.7 79.0 65.6\nSwinTrack-T-224 ImageNet-22k 67.3 71.7 46.0 51.7 81.2 78.9 69.5 78.9 65.5\nSwinTrack-B-384 ImageNet-1k 68.0 72.5 47.3 53.2 83.8 82.9 71.8 80.2 67.1\nSwinTrack-B-384 ImageNet-22k 70.2 75.3 47.5 53.3 86.9 80.1 70.2 80.7 65.4\nTable 6: Performance comparisons with newly released Transformer-based Trackers on four bench-\nmarks: LaSOT, LaSOText, TrackingNet and GOT-10k.\nTracker Pre-training LaSOT [11] LaSOText [10] TrackingNet [33] GOT-10k [19]\nSUC P SUC P SUC P AO SR 0.5 SR0.75\nSTARK [46] ImageNet-1k 67.1 - - - 82.0 - 68.8 78.1 64.1\nSBT [43] ImageNet-1k 66.7 71.1 - - - - 70.4 80.8 64.7\nToMP [30] ImageNet-1k 68.5 73.5 45.9 - 81.5 78.9 - - -\nMixFormer [6] ImageNet-22k 70.1 76.3 - - 83.9 83.1 - - -\nAiATrack [15] ImageNet-1k 69.0 73.8 47.7 55.4 82.7 80.4 69.6 80.0 63.2\nUnicorn [45] ImageNet-1k 68.5 74.1 - - 83.0 82.2 - - -\nOSTrack [47] MAE [17] 71.1 77.6 50.5 57.6 83.9 83.2 73.7 83.2 70.8\nSwinTrack-T-224 ImageNet-1k 67.2 70.8 47.6 53.9 81.1 78.4 71.3 81.9 64.5\nSwinTrack-B-384 ImageNet-22k 71.3 76.5 49.1 55.6 84.0 82.8 72.4 80.5 67.8\nGOT-10k (-1.4%). For larger model SwinTrack-B-384 (91M # parameters), pre-training on ImageNet-\n22k shows signiﬁcant performance gains on LaSOT (+2.2%) and GOT-10k (+3.0%) but slightly\ndegrades the result on TrackingNet (-0.6%). On LaSOT ext, ImageNet-22k shows a performance\ndegradation on smaller model SwinTrack-T-224 (-0.9%) and brings small improvements on larger\nmodel SwinTrack-B-384 (+0.2%).\nC Comparison with Newly Released Transformer-based Trackers\nWe compare our tracker with some newly released Transformer-based trackers, including STARK [46],\nSBT [43], ToMP [30], MixFormer [6], AiATrack [15], Unicorn [45], OSTrack [47] in Tab. 6 in four\nchallenging benchmarks. The result shows our tracker is still competitive.\nFig. 3 and Fig. 4 show the success plot and the precision plot respectively. The comparison includes\nour SwinTrack-T-224, our SwinTrack-B-384, TransT[5], STARK[46], MixFormer[6], AiATrack[15]\nand ToMP[30]. Our tracker obtained the best performance on this benchmark. By looking into the\ncurves of the ﬁgures, there is a signiﬁcant advantage in the bounding box accuracy compared with\nother trackers due to our fully attentional architecture.\nThe success AUC score under different attributes of LaSOT [11] Test set in shown in Fig. 5. Fig. 5\nindicates that our tracker has no obvious shortcomings except the viewpoint change.\nD Results on UA V123 and VOT Benchmark\nIn this section, we report the performance of the tracker on three additional benchmarks, including\nUA V123 [32], VOT2020 and VOT-STB2022 [21].\nUA V123[32] is an aerial video dataset and benchmark for low-altitude UA V target tracking, containing\n123 video sequences. Our tracker is on par with the state-of-the-art, AiATrack [15], on this benchmark.\nThe results are shown in Tab. 7.\nFinally, we evaluate our tracker on the two versions of the VOT Challenge: VOT2020 and VOT-\nSTB2022. The VOT2020 dataset contains 60 videos with segmentation masks annotated. Since our\ntracker is a bounding box only method, we compare the results with the trackers that produce the\nbounding boxes as well. The result in Tab.8 shows that SwinTrack-T-224 has a better performance\nthan the larger SwinTrack-B-384 on this benchmark.\n16\nTable 7: Comparison to the state-of-the-arts on UA V123 [32] benchmark.\nOcean\n[52]\nDiMP\n[2]\nTransT\n[5]\nToMP\n50[30]\nMixFormer\n22k[6]\nAiATrack\n[15]\nSwinTrack\nT-224\nSwinTrack\nB-384\nAUC (%) 62.1 65.3 69.1 69.0 70.4 70.6 68.8 70.5\nTable 8: Comparison to the state-of-the-art bounding box only methods on VOT2020ST [21].\nATOM\n[8]\nDiMP\n[2]\nSTARK\n50[46]\nSTARK\n101[46]\nToMP\n50[30]\nToMP\n101[30]\nSwinTrack\nT-224\nSwinTrack\nB-384\nEAO 0.271 0.274 0.308 0.303 0.297 0.309 0.302 0.283\nAccuracy 0.462 0.457 0.478 0.481 0.453 0.453 0.471 0.472\nRobustness 0.734 0.734 0.799 0.775 0.789 0.814 0.775 0.741\nIn addition, We report the results on VOT-STB2022 in Tab.9. SwinTrack-T-224 has a better perfor-\nmance on VOT-STB2022 as well. No comparison is made since VOT-STB2022 is a newly released\nbenchmark.\nE Quantitative Analysis of the Effectiveness of Motion Token\nTo give a further analysis of the effectiveness of motion token, we provide the success plot (Fig. 6)\nand precision plot (Fig. 7) on LaSOT test set, and the success AUC score under different attributes of\nLaSOT test set in Fig. 8. The success plot and the precision plot show that the motion token improves\nthe performance of the trackers by boosting robustness. While the Fig. 8 further points out that the\nmotion token can assist the tracker to recover from a failure state when the vision features are not\nreliable like an object is getting out of view or fully occluded by other objects.\nF Response Visualization for Qualitative Analysis\nWe provide the heatmap visualization of the response map generated by the IoU-aware classiﬁ-\ncation branch of the head in our SwinTrack-B-384 in Fig. 9. The visualized sequences are from\nLaSOText [10], with challenges include fast motion, full occlusion, hard distractor, etc. The results\ndemonstrate the great discriminative power of our tracker. Many trackers will show a multi-peak\non the response map when the target object is occluded or multiple similar objects exist. With the\nvision-motion integrated Transformer architecture, our tracker eases such phenomenon.\nG Failure Case\nWe show some typical failure cases of our tracker (SwinTrack-B-384 on LaSOText [10] and VOT-\nSTB2022 [21]) in Fig. 10. The ﬁrst case suffers from a mixture of low resolution, fast motion, and\nbackground clutter. The second case suffers from a fast occlusion by a distractor. The third case\nsuffers from the non-semantic target.\n17\nTable 9: Results on VOT-STB2022 [21].\nSwinTrack\nT-224\nSwinTrack\nB-384\nEAO 0.505 0.477\nAccuracy 0.777 0.790\nRobustness 0.790 0.759\nFigure 3: Comparison to the state-of-the-art trackers on LaSOT [11] Test set using success (SUC)\nAUC score.\n0 0.2 0.4 0.6 0.8 1\n0\n20\n40\n60\n80\n100\nOverlap threshold\nOverlap Precision [%]\nSuccess plot\nTransT [64.9]\nSTARK [67.1]\nSwinTrack-T-224 [67.2]\nToMP [68.5]\nAiATrack [69.0]\nMixFormer [70.0]\nSwinTrack-B-384 [71.3]\nFigure 4: Comparison to the state-of-the-art trackers on LaSOT [11] Test set using precision (PRE)\nAUC score.\n0 10 20 30 40 50\n0\n20\n40\n60\n80\n100\nLocation error threshold [pixels]\nDistance Precision [%]\nPrecision plot\nTransT [69.0]\nSwinTrack-T-224 [70.8]\nSTARK [72.2]\nToMP [73.5]\nAiATrack [73.8]\nMixFormer [76.3]\nSwinTrack-B-384 [76.5]\n18\nFigure 5: Comparison to the state-of-the-art trackers using success (SUC) AUC score under different\nattributes of LaSOT [11] Test set.\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nIllumination Variation\nPartial Occlusion\nDeformation\nMotion Blur\nCamera Motion\nRotation\nBackground Clutter\nViewpoint Change\nScale Variation\nFull Occlusion\nFast Motion\nOut-of-View\nLow Resolution\nAspect Ration Change\nTransT\nSTARK\nToMP\nMixFormer\nSwinTrack-T-224\nSwinTrack-B-384\nFigure 6: Success (SUC) AUC score on LaSOT [11] Test set assessing the effectiveness of the motion\ntoken.\n0 0.2 0.4 0.6 0.8 1\n0\n20\n40\n60\n80\n100\nOverlap threshold\nOverlap Precision [%]\nSuccess plot\nSwinTrack-T-224-NoMToken [66.7]\nSwinTrack-T-224 [67.2]\nSwinTrack-B-384-NoMToken [70.2]\nSwinTrack-B-384 [71.3]\n19\nFigure 7: Precision (PRE) AUC score on LaSOT [ 11] Test set assessing the effectiveness of the\nmotion token.\n0 10 20 30 40 50\n0\n20\n40\n60\n80\n100\nLocation error threshold [pixels]\nDistance Precision [%]\nPrecision plot\nSwinTrack-T-224-NoMToken [70.6]\nSwinTrack-T-224 [70.8]\nSwinTrack-B-384-NoMToken [75.3]\nSwinTrack-B-384 [76.5]\nFigure 8: Success (SUC) AUC score under different attributes of LaSOT [11] Test set assessing the\neffectiveness of the motion token.\n0\n20\n40\n60\n80\n100\nIllumination Variation\nPartial Occlusion\nDeformation\nMotion Blur\nCamera Motion\nRotation\nBackground Clutter\nViewpoint Change\nScale Variation\nFull Occlusion\nFast Motion\nOut-of-View\nLow Resolution\nAspect Ration Change\nSwinTrack-T-224-NoMToken\nSwinTrack-B-384-NoMToken\nSwinTrack-T-224\nSwinTrack-B-384\n20\nFigure 9: Heatmap visualization of the tracking response map of our SwinTrack-B-384 on\nLaSOText [10]. The odd rows visualize the search region patches with ground-truth bounding\nbox (in red rectangles). The even rows visualize the search region patches blended with the heatmap\nvisualization of the response map. The sequences and challenges involved: atv-10 (POC, ROT, VC,\nSV , LR, ARC), wingsuit-10 (CM, BC, VC, SV , FOC, LR, ARC), rhino-9 (DEF, SV , ARC) and misc-3\n(POC, MB, ROT, BC, SV , FOC, FM, LR).\n21\nFigure 10: Heatmap visualization of the failure cases. The organizational form is the same as Fig. 9.\nThe sequences and challenges involved: badminton-3 in LaSOT ext (MB, SV , FOC, FM, OV , LR,\nARC), skatingshoe-2 in LaSOText (POC, MB, ROT, BC, SV , FOC, FM, LR, ARC) and conduction1\n(non-semantic target) in VOT-STB2022.4\n4IV: Illumination Variation, POC: Partial Occlusion, DEF: Deformation, MB: Motion Blur, CM: Camera\nMotion, ROT: Rotation, BC: Background Clutter, VC: Viewpoint Change, SV: Scale Variation, FOC: Full\nOcclusion, FM: Fast Motion, OV: Out-of-View, LR: Low Resolution, ARC: Aspect Ration Change\n22",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7102780938148499
    },
    {
      "name": "BitTorrent tracker",
      "score": 0.6376252770423889
    },
    {
      "name": "Computer science",
      "score": 0.6307029128074646
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6027891039848328
    },
    {
      "name": "Convolutional neural network",
      "score": 0.47951892018318176
    },
    {
      "name": "Feature extraction",
      "score": 0.4604604244232178
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.41368162631988525
    },
    {
      "name": "Computer vision",
      "score": 0.3909478187561035
    },
    {
      "name": "Eye tracking",
      "score": 0.3072400689125061
    },
    {
      "name": "Engineering",
      "score": 0.2065196931362152
    },
    {
      "name": "Electrical engineering",
      "score": 0.07242709398269653
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I90610280",
      "name": "South China University of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I123534392",
      "name": "University of North Texas",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I59553526",
      "name": "Stony Brook University",
      "country": "US"
    }
  ],
  "cited_by": 48
}