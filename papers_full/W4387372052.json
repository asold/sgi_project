{
  "title": "A clarification of the conditions under which Large language Models could be conscious",
  "url": "https://openalex.org/W4387372052",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4221788284",
      "name": "Asger Kirkeby-Hinrup",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1892659614",
      "name": "Morten Overgaard",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2394834952",
    "https://openalex.org/W3159888642",
    "https://openalex.org/W3043429318",
    "https://openalex.org/W4239123914",
    "https://openalex.org/W3175257626",
    "https://openalex.org/W4230855722",
    "https://openalex.org/W1482008299",
    "https://openalex.org/W3101172125",
    "https://openalex.org/W4210876005",
    "https://openalex.org/W1721465822",
    "https://openalex.org/W4311836257",
    "https://openalex.org/W2251410821"
  ],
  "abstract": "With incredible speed Large Language Models (LLMs) are reshaping many aspects of society. This has been met with unease by the public, and public discourse is rife with questions about whether LLMs are or might be conscious. Because there is widespread disagreement about consciousness between scientists, any concrete answers we could offer the public would be contentious. Here we offer the next best thing: charting the possibility for consciousness in LLMs.",
  "full_text": "A clarification of  the conditions under which  Large language Models could be \nconscious \n \nWith incredible speed Large Language Models (LLMs) are reshaping many aspects of society. This has been \nmet with unease by the public, and public discourse is rife with questions about whether LLMs are or might \nbe conscious. Because there is widespread disagreement about consciousness between scientists, any \nconcrete answers we could offer the public would be contentious. Here  we offer the next best thing: \ncharting the possibility for consciousness in LLMs.  \n \nLarge Language Models (LLMs) are sophisticated artificial neural networks whose weights are trained \non hundreds of billions of words from natural language conversations between conscious humans \nwith ‘real’ agency . Users that interact with LLMs are provided with a fascinating language -based \nsimulation of a natural language interaction. Because LLMs have been trained on conversations, in \nwhich (actual) humans describe and express in different ways the peculiar inner life we associate \nwith conscious experience, the LLMs are capable of giv ing descriptions and expressions of such an \ninner life that are practically indistinguishable from the that of humans. To the public, this has made \nmanifest the lack of clarity about  what it means to have agency and to be conscious . In public \ndiscourse on LLMs an uncertainty about whether they could be conscious drives many of the worries \nexpressed by politicians, the public audience, and laypeople alike. This uncertainty thrives because \nwe — as a scientific field — have yet to understand consciousness as well.  \n \nIn interdisciplinary consciousness studies, researchers are today far from consensus about how to \nexplain consciousness theoretically. In fact, there is extended and ongoing debate in the field about \nwhat the words we use to describe and theorize about consciousness even mean. Therefore, we \nhave no strong theoretical guidance to understand whether LLMs are — or can be — conscious \neither. Several recent scientific articles have assumed that LLMs are not conscious and that we on \ntherefore can conclude that th e ability to converse can happen unconsciously. At the same time, \nothers, as mentioned above, have suggested the exact opposite. However, any such assumption is a \ntheoretical choice not supported by any empirical evidence. \n \nRecently, it has been suggested in media as well as in the scientific literature that there is evidence \nto suggest that consciousness is common – not just in the biological domain but in any domain where \ninformation is integrated [1]. It is however very premature to make such a claim based on empirical \nscience. This goes not only for the integrated information theory, but for any contemporary theory \nof consciousness.  How to measure consciousness  remains one of the most prominent unsolved \nproblems around. Since consciousness seems to be a central component of human life, we have a \nvested interest in finding objective and reliable biomarkers of consciousness in humans (not the least \nfor clinical reasons). Regarding the topic at hand, clearly, if we only understood how consciousness \ncomes about in humans, it would be much easier to determine what it would take for a machine to \nbe conscious, and whether this is even possible in the first place. But we currently do not know how \nconsciousness comes about in humans, therefore this is not a feasible approach.  \n \nSecond, it is a strong intuition both in science and common sense that being conscious of something \nmakes a cognitive difference for the subject. Yet, predominant models in cognitive neuroscience \nhave not been able to conceptually — or empirically — identify a particular cognitive function (or \nset of functions) for which consciousness is necessary.  At present, there is no objective way of \ndetermining whether any given function or action an LLM may perform in fact is associated with \nconsciousness, making this approach unfeasible as well. \n \nThe brief analysis above seems to show  that the debate is stuck. There is no empirical method \navailable to determine if LLMs are conscious, and a theoretical conclusion on the matter will be \nbased on a choice or an assumption, thus either depending on arbitrary assumptions or ending as a \ncircular argument. The problem is familiar to consciousness researchers but is echoed in previous \ndebates about consciousness in e.g. insects, animals, infants, non-communicating patients in coma \nor vegetative state, and ev en in neurotypical human adults, as exemplified in the “other minds \nproblem” from the philosophy of mind. \n \nIf there is a way forward on directly measuring consciousness, we must identify the questions that \nneed answering before we can  find it . For instance , it must be determined whether the core \ncorrelate of consciousness is biological/structural in nature or rather functional/computational. \nNaturally, there will always be biological and functional correlates at the same time, yet any theory \nmust argue that consciousness exists because of something that is either biological or functional in \nnature, so that other correlates are spurious or secondary. In recent years, much attention has been \ngiven to classifying explanatory targets and mechanisms of extant the ories in various, but similar \nterms [2-6]. \n \nTwo of the important questions we need to raise are expressed in the matrix below: Consciousness \nis either realized by certain biological structures or by certain functions/computations, and \nconsciousness is either realized by simple/low -level phenomena or by  complex/higher-level \nphenomena. \n \n \n Functional-Computational Biological-Structural \nComplex LLMs will possibly be conscious LLMs cannot be conscious \nSimple LLMs Likely are/will be conscious LLMs cannot be conscious \n \nSegmenting the landscape according to the functional -biological and simple -complex distinctions \ngives us a handle on the conditions under which LLMs may be conscious (now or in the future). \nBecause this way of segmenting the theoretical landscape has narrowed the possible positions into \na two-by-two matrix, where each space predicts the prospects of artificial consciousness , it allows \nfor better (but still underdetermined) generalization than when considering each theory in isolation. \nThis approach has the advantage of offering a theory -neutral mapping of the possibility space for \nLLM consciousness. Therefore, we will next briefly consider each of the two dimensions in a little \nmore detail. \n \nThe Biological-Functional Distinction \nFundamentally, either consciousness is associated with physical  structure or consciousness is  \nassociated with function. These two “types” of theories come in many versions, depending on the \nspecifics of what consciousness is taken to be reducible to, identical with, or different from, and if or \nhow it is anchored in some specific structure or function.  \n \nResearchers who associate consciousness with biological structures often have one or more neural \nstructure(s) in mind. From this perspective, an organism is conscious under the condition that it has \na specific neural structure  (biological foundation) . This thinking is evident in several currently \ninfluential theories, e.g. in integrated information theory, where consciousness is literally identical \nto the most complex cluster of interconnected information in a brain [1]. If consciousness depends \non biological structures, LLMs will never be conscious because they are not instantiated in the ‘right’ \nmaterial (c.f. Searle [7]).  \n \nResearchers who associate consciousness with functional properties typically conceive of \nconsciousness as analogous to computer software that needs some hardware to run  (c.f. what \nChalmers [8] calls the “ thesis of computational sufficiency ”). From this perspective, any physical \nstructure (e.g. brains or arrays of silicon chips) with the necessary – currently unknown – \ncharacteristics to run the ‘right’ software will be able to realize consciousness.  Accordingly, if \nconsciousness depends on functional characteristics, LLMs can be conscious if they run the ‘right’ \nsoftware.  \n \nComplexity matters \nThe other  parameter in our chart maps whether consciousness depends on complex biological \nstructures or functions, or whether it merely requires simple structures or functions.  \n \nSome researchers propose that one or a few functions are able to realize consciousness. One such \nexample is higher -order thought theory that argues that consciousness merely requires an (itself \nunconscious) thought about a first -order content (such as a visual stimulus). Consequently, any \nsystem with the right kind of metacognitive abilities may be conscious. Nothing indicates that LLMs \nhave actual metacognition. In principle, however, nothing would prevent an artificial system from \nhaving higher-order thoughts, so consciousness in artificial systems is not ruled  out on this \nperspective.  \n \nIf consciousness is correctly associated with any kind of function, LLMs may already be close to being \nconscious, and future developments are likely to lead to conscious artificial systems. If consciousness \nis associated with  extremely simple functions, without knowing it , we may have created \nconsciousness in artificial systems long before LLMs appeared. \n \nThe complexity aspect plays out differently i f consciousness is correctly associated with biological \nstructures. On this view our current LLMs will never be conscious because they are not instantiated \nin the ‘right’ material. However, it is possible that the ‘right’ material (e.g. brains) exists not only in \nhumans, so consciousness may be very widespread in nature. On this view, the complexity aspect \nmaps onto how widespread consciousness in fact is. If the structure must be very complex, fewer \nspecies will be conscious, if the structure is very simple, consciousness will be abundant in biological \nbeings [9]. An example of the former would be a theory positing the need for exascale quantum \ncomputation at ambient temperatures in combination with specific properties of cortical neurons \nand the neuronal membranes [10]. An example of the former would be the ability to integrate \ninformation [11]. \n \nWhile the above presents  the available options of a highly complicated and diverse theoretical \nlandscape in a simple matrix, we do not suggest that answering the question is simple. We have a \ndifficult journey and long journey ahead . Until serious progress is made, and as long as theories \nrooted in all four positions in the matrix can explain all or most of the available scientific data, it will \nbe an unscientific enterprise to draw conclusions about consciousness in LLMs or any other artificial \nsystem. In the words of Uriah Kriegel: “When two theories are perfectly empirically equivalent, there \nis an important sense in which choosing among them on the basis of superempirical virtues is a \nnonscientific endeavor.” ([12] p. 273) \n \n \n1. Tononi, G., et al., Integrated information theory: from consciousness to its physical substrate. \nNature Reviews Neuroscience, 2016. 17(7): p. 450-461. \n2. Signorelli, C.M., J. Szczotka, and R. Prentner, Explanatory profiles of models of consciousness \n- towards a systematic classification. Neuroscience of Consciousness, 2021. 2021(2). \n3. Schurger, A. and M. Graziano, Consciousness explained or described?  Neuroscience of \nConsciousness, 2022. 2022(1). \n4. Doerig, A., A. Schurger, and M.H. Herzog, Hard criteria for empirical theories of \nconsciousness. Cognitive neuroscience, 2020. 12(2): p. 41-62. \n5. Fahrenfort, J.J. and S. van Gaal, Criteria for empirical theories of consciousness should focus \non the explanatory power of mechanisms, not on functional equivalence.  Cognitive \nNeuroscience, 2021. 12(2): p. 93-94. \n6. Sattin, D., et al., Theoretical models of consciousness: a scoping review. Brain Sciences, 2021. \n11(5): p. 535. \n7. Searle, J.R., Minds, brains, and programs. Behavioral and Brain Sciences, 1980. 3(03): p. 417-\n424. \n8. Chalmers, D.J., A computational foundation for the study of cognition.  Journal of Cognitive \nScience, 2011. 12(4): p. 325-359. \n9. Wiese, W. and K.J. Friston, The neural correlates of consciousness under the free energy \nprinciple: From computational correlates to computational explanation. Philosophy and the \nMind Sciences, 2021. 2. \n10. Stoll, E., Modeling electron interference at the neuronal membrane yields a holographic \nprojection of representative information content. bioRxiv, 2022: p. 2022.12.03.518989. \n11. Tononi, G., Consciousness, information integration, and the brain , in Progress in Brain \nResearch, L. Steven, Editor. 2005, Elsevier. p. 109-126. \n12. Kriegel, U., The Oxford Handbook of the Philosophy of Consciousness . Oxford Handbooks. \n2020: Oxford University Press. \n ",
  "topic": "Consciousness",
  "concepts": [
    {
      "name": "Consciousness",
      "score": 0.8138506412506104
    },
    {
      "name": "Political science",
      "score": 0.389036625623703
    },
    {
      "name": "Sociology",
      "score": 0.33462023735046387
    },
    {
      "name": "Epistemology",
      "score": 0.3210696876049042
    },
    {
      "name": "Philosophy",
      "score": 0.1250671148300171
    }
  ],
  "institutions": [],
  "cited_by": 1
}