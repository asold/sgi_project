{
  "title": "Tool wear classification using time series imaging and deep learning",
  "url": "https://openalex.org/W2962414238",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A4377974347",
      "name": "Martinez-Arellano, Giovanna",
      "affiliations": [
        "University of Nottingham"
      ]
    },
    {
      "id": "https://openalex.org/A3181045642",
      "name": "Terrazas German",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A2745368505",
      "name": "Ratchev Svetan",
      "affiliations": [
        "University of Nottingham"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2782812883",
    "https://openalex.org/W1972006231",
    "https://openalex.org/W1634809829",
    "https://openalex.org/W2605783563",
    "https://openalex.org/W2216628602",
    "https://openalex.org/W1994387548",
    "https://openalex.org/W2744999441",
    "https://openalex.org/W2585345106",
    "https://openalex.org/W1999314995",
    "https://openalex.org/W2464234006",
    "https://openalex.org/W2897780703",
    "https://openalex.org/W1965808944",
    "https://openalex.org/W2041872711",
    "https://openalex.org/W2063532090",
    "https://openalex.org/W2010028439",
    "https://openalex.org/W2162807413",
    "https://openalex.org/W1967640926",
    "https://openalex.org/W1971395914",
    "https://openalex.org/W2054583885",
    "https://openalex.org/W2770085689",
    "https://openalex.org/W2097747115",
    "https://openalex.org/W2141125852",
    "https://openalex.org/W2160815625",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W2106544870",
    "https://openalex.org/W2740570963",
    "https://openalex.org/W2066916495",
    "https://openalex.org/W2044738244",
    "https://openalex.org/W2310992461",
    "https://openalex.org/W2291961022",
    "https://openalex.org/W119403003",
    "https://openalex.org/W2741289421",
    "https://openalex.org/W2586230645",
    "https://openalex.org/W2556345765",
    "https://openalex.org/W2580840020",
    "https://openalex.org/W2099593264",
    "https://openalex.org/W2086226917",
    "https://openalex.org/W2010821195",
    "https://openalex.org/W2055026204",
    "https://openalex.org/W2735895797",
    "https://openalex.org/W1966554111",
    "https://openalex.org/W2540566525",
    "https://openalex.org/W2964050365",
    "https://openalex.org/W2512338826",
    "https://openalex.org/W2411149632",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W3142151466",
    "https://openalex.org/W2737404945",
    "https://openalex.org/W1562687546",
    "https://openalex.org/W3100528154",
    "https://openalex.org/W1601667204",
    "https://openalex.org/W1524534559",
    "https://openalex.org/W2135614196",
    "https://openalex.org/W3153014723",
    "https://openalex.org/W1623006543",
    "https://openalex.org/W3145153657",
    "https://openalex.org/W1562806204",
    "https://openalex.org/W2101713460"
  ],
  "abstract": null,
  "full_text": "https://doi.org/10.1007/s00170-019-04090-6\nORIGINALARTICLE\nToolwearclassiﬁcationusingtimeseriesimaginganddeeplearning\nGiovannaMart´ınez-Arellano1 · GermanTerrazas2 · SvetanRatchev1\nReceived:18October2018/Accepted:30June2019\n© TheAuthor(s)2019\nAbstract\nTool condition monitoring (TCM) has become essential to achieve high-quality machining as well as cost-effective\nproduction. Identification of the cutting tool state during machining before it reaches its failure stage is critical. This paper\npresents a novel big data approach for tool wear classification based on signal imaging and deep learning. By combining\nthese two techniques, the approach is able to work with the raw data directly, avoiding the use of statistical pre-processing or\nfilter methods. This aspect is fundamental when dealing with large amounts of data that hold complex evolving features. The\nimaging process serves as an encoding procedure of the sensor data, meaning that the original time series can be re-created\nfrom the image without loss of information. By using an off-the-shelf deep learning implementation, the manual selection\nof features is avoided, thus making this novel approach more general and suitable when dealing with large datasets. The\nexperimental results have revealed that deep learning is able to identify intrinsic features of sensory raw data, achieving in\nsome cases a classification accuracy above 90%.\nKeywords Smart manufacturing · Tool wear classification · Time series imaging · Convolutional neural network ·\nDeep learning\n1Introduction\nThe manufacturing industry has gone through several\nparadigm changes along the years. Industrie 4.0, also\nreferred as smart industry, is a new paradigm that pro-\nposes the integration of information and communication\ntechnologies (ICT) into a decentralised production. With\nmanufacturing machines fully networked to share data and\ncontrolled by advanced computational intelligence tech-\nniques, this paradigm is looking to improve productivity,\nquality, sustainability and reduce costs [1, 2].\n/envelopebackGiovanna Mart´ınez-Arellano\ngiovanna.martinezarellano@nottingham.ac.uk\nGerman Terrazas\ngt401@cam.ac.uk\nSvetan Ratchev\nsvetan.ratchev@nottingham.ac.uk\n1 Institute for Advanced Manufacturing,\nUniversity of Nottingham, Nottingham, UK\n2 Institute for Manufacturing, University of Cambridge,\nCambridge, UK\nThe estimation of the remaining useful life (RUL)\nof industrial components is an important task in smart\nmanufacturing. Early detection of cutting tool degradation\nfacilitates the reduction of failures, and hence decreases\nmanufacturing costs and improves productivity. It can also\nhelp maintain the quality of the workpiece, as it has been\ndemonstrated that there is a correlation between the surface\nroughness of the workpiece and the cutting tool wear [ 3].\nReal-time tool wear measurement is difficult to put in\npractice as the tool is continuously in contact with the\nworkpiece during machining. For this reason, a plethora of\nindirect approaches for tool wear estimation (also referred\nas Prognosis) have been proposed utilising sensor signals\nsuch as cutting forces, vibrations, acoustic emissions and\npower consumption [4].\nPrognostic approaches can be divided into two cate-\ngories: model-based and data-driven. The first ones rely\non the a priori knowledge of the underlying physical laws\nand probability distributions that describe the dynamic\nbehaviour of a system [ 5–8]. Although these have proven\nto be successful, an in-depth understanding and exper-\ntise of the physical processes that lead to tool failure is\nrequired.\nOn the other hand, data-driven approaches model the data\nby means of a learning process, avoiding any assumptions\nThe International Journal of Advanced Manufacturing Technology (2019) 104:3647–3662\n/Published online: 17 July 2019\non its underlying distribution. Most data-driven methods\nthat have been used for tool wear prediction are based\non machine learning, particularly artificial neural networks\n(ANN), support vector machines (SVM) and decision trees\n(DT) [ 9]. However, these techniques are limited in their\nability to process raw (i.e. unstructured or unformatted)\ndata, which has a negative effect on their generalisation\ncapabilities [10].\nThe large amount of data in smart manufacturing imposes\nchallenges such as the proliferation of multivariate data,\nhigh dimensionality of feature space and multicollinearity\namong data measurements [ 2, 11]. This paper presents\nin detail the methodology of a novel approach for tool\nwear classification recently used in [ 12] as a component\nof an on-line monitoring framework. Its automatic feature\nlearning and high-volume processing capabilities make\ndeep learning a viable advanced analytics method for\ntool wear classification despite the large volumes of data\nrequired. The proposed classification methodology is based\non two components: an imaging step and a deep learning\nstep. The imaging technique employed encodes sensor\nsignals in such a way that its complex features as well\nas the exhibited temporal correlations are captured by the\ndeep learning, avoiding manual selection. An analysis of the\nchallenges and strategies used to build a big data classifying\napproach is performed through a set of experiments using\nthe PHM 2010 challenge dataset [ 13], where the technical\nprocedures of how the data was generated and collected are\nnot entirely known. This provides a way to perform an un-\nbiased blind test and proof of the generalisation capabilities\nof the methodology.\nThe rest of the manuscript is organised as follows:\nSection 2 presents details of how machine learning has been\napplied to tool wear prediction. Section 3 introduces the\nproposed approach giving details of the signals imaging\nand the deep learning methodology. The experimental setup\nand the results and discussion are presented in Section 4.\nFinally, conclusions and future work are presented in\nSection 5.\n2Relatedwork\nTool wear has been widely studied as it is a very\ncommon phenomenon in manufacturing processes such as\nmilling, drilling and turning. It is well known that different\nmachining parameters such as spindle speed, feed rate and\ncutting tool characteristics as well as the workpiece material\nhave an effect on tool wear progression [ 14]. Although\nthis progression can be mathematically estimated [ 15,\n16], these models rarely capture the stochastic properties\nof real machining processes and tool-to-tool performance\nvariation [ 17]. Over the last two decades, it has been\ndemonstrated that data-driven models can achieve higher\naccuracy, although these have also shown some drawbacks\n[10].\nSome of the most common data-driven methods are\nbased on traditional machine learning algorithms. SVMs,\nfor example, have been successfully applied for tool\ncondition monitoring in [ 18]. The authors use automatic\nrelevance determination (ARD) on acoustic emission\ndata to select nine features as inputs for classification.\nANNs have also been extensively applied for tool wear\nprediction. These commonly use a combination of cutting\nparameters such as cutting speed, feed rate and axial cutting\nlength as well as statistical features of forces, vibrations\nand acoustic emission [ 19–22]. In applications such as\ndrilling and milling, it has been shown how ANNs can\noutperform regression models. In [9], a tool wear prediction\nmethod based on random forests is proposed. Although\nthis approach has outperformed ANN- and SVM-based\nmethods, it relies on the manual selection of features to build\nthe internal classification structures.\nManual feature selection is a significant problem when\ndealing with large amounts of shop floor–generated sensory\ndata. Its distribution as well as the number of features\navailable may change with time. Cloud-based architectures\nrecently proposed for collecting and managing sensory data\n[2, 23] present new challenges to current TCM solutions. To\ndevelop a more general approach, forthcoming approaches\nshould be able to cope not only with high volumes of\nheterogeneous data but also with the constant evolution of\nhigh-dimensional features. Most classical machine learning\ntechniques have been designed to work with data features\nthat do not change with time (static data). As a result, several\nof these techniques either have been extended to handle the\ntemporal changes or rely on a prior selection of features\nusing other algorithms [24].\nDeep learning has offered better solutions when dealing\nwith high-dimensional evolving features. These techniques\nhave made major advances in fields such as image\nrecognition [ 25, 26], speech recognition [ 27] and natural\nlanguage processing [28, 29], to name a few. Its capability to\nprocess highly complex featured data has led to an emerging\nstudy of deep learning applications for smart manufacturing.\nFor instance, recurrent neural networks (RNN) have been\nsuccessful for the long-term prognosis of rolling bearing\nhealth status [ 30]. In [ 31], a local feature-based gated\nrecurrent unit network is applied to tool wear prediction,\ngearbox fault diagnosis and bearing fault detection. The bi-\ndirectional recurrent structure proposed by the authors can\naccess the sequential data in two directions–forward and\nbackward–so that the model can fully explore the ‘past and\nfuture’ of each state.\nAnother successful deep learning architecture is the\nconvolutional neural network (CNN) [ 32], which is the\nInt J Adv Manuf Technol (2019) 104:3647 –36623648\none addressed in this work. CNNs have become the\nde facto standard for deep learning tasks as they have\nachieved state-of-the-art performance in image recognition\ntasks. The architecture of a CNN is based on the\narchitecture of the ANN, but further extended with a\ncombination of convolutional and sub-sampling layers\nthat allow the discovery of relevant features. This is\nexplained in more detail in Section 3.2. CNNs are developed\nprimarily for 2D signals such as images and video\nframes. Some successful applications are the detection of\nvehicles in complex satellite images [ 33], the classification\nof galaxy morphology [ 34], brain tumour segmentation\nfrom MRI images [ 35], among others. Their success\nin the classification of two-dimensional data has led to\nfurther development of CNNs for time series classification\n(one-dimensional data). Some applications include the\nclassification of electrocardiogram beats for detecting heart\nfailure [36] and the use of accelerometer readings for human\nactivity recognition [37].\nCNNs have also been applied in manufacturing prob-\nlems. For example, this technique has been used for the\ndetection of faulty bearings [ 38–40] by feeding raw vibra-\ntion data directly to the CNN, achieving good accuracy and\nreducing the computational complexity of the extraction of\nfixed features. In [ 41], real-time structural health monitor-\ning is performed using 1D CNNs. The authors use vibration\nsignals from damaged and undamaged joints of a girder to\ntrain several CNNs, one for each joint. Their objective is\nto detect the structural damage (if any), and identify the\nlocation of the damaged joint(s) in the girder. The authors\nreport an outstanding performance and computational effi-\nciency of the approach when dealing with large-scale\nexperiments.\nSome previous work on tool wear prediction using a\nCNN combined with bi-directional long short-term memory\n(LSTM) has been done [ 42]. The proposed approach\nis able to extract local features of the data, achieving\ngood accuracy when compared with other deep learning\ntechniques such as RNNs. However, the method performs\na substantial size reduction of the original data, losing\ninformation at the flute level. This will be further discussed\nin Section 5.\nManual feature selection is still a limitation for tool wear\nprediction approaches to achieve generalisation. To address\nthis, this paper extends preliminary experiments of a novel\ndeep learning–based method that will allow the automatic\ndiscovery of intricate structures in sensor signals that relate\nto the tool condition, and from this provide a classification\nof the tool state. The approach is blind to the type of signals\ngiven or their underlying distribution, so no assumptions nor\nmanual feature selections are needed. At the same time, the\nmodel is blind to the type of wear being classified. Although\nin this work flank wear has been used as a measure of the\ntool condition, the proposed methodology could be used for\nother types of tool wear as well.\n3Methodology\nThis section presents the two main steps of the methodol-\nogy: the imaging of sensor signals using Gramian Angular\nSummation Fields [ 43] and the classification using CNNs.\nThe idea behind this approach is to visually recognise, clas-\nsify and learn structures and patterns intrinsic to sensory\ndata without loss of information.\n3.1Timeseriesimaging\nThere has been a recent interest on reformulating features\nof time series to improve their identification, and hence\nclassification. Eckmann et al. introduced the method of\nrecurrence plots to visualise the repetitive patterns of\ndynamical systems [ 44]. Silva et al. used this method\nand proposed the use of a compression distance approach\nto compare recurrence plots of time series as a way to\nmeasure similarity [ 45]. Methods based on time series\nto network mapping using the topology of the network\nas a way to characterise the time series have also been\nproposed [46, 47]. Most of these methods do not provide\na way to reconstruct the original data, making unclear how\nthe topological properties relate to the time series. Wang\net al. propose three techniques, two based on Gramian\nAngular Fields (GAF) and one on Markov Transition\nFields (MTF) to image time series [ 43]. They argue that\ncompared with previous techniques, the original time series\ncan be re-constructed, allowing the user to understand how\nthe features introduced in the encoding process improve\nclassification. They reported GAF encoding methods were\nable to achieve competitive results in a series of baseline\nproblems that include different domains such as medicine,\nentomology, engineering and astronomy. Furthermore, this\nmethod has been found to perform well compared with other\ntime series encoding techniques in applications such as the\nclassification of future trends of financial data [48].\nAs a pre-processing step, our approach uses the GAF\nimaging technique proposed by [ 43], particularly the one\nbased on the summation of angular fields, Gramian Angular\nSummation Fields (GASF). This encoding method consists\nof two steps. First, the time series is represented in a\npolar coordinate system instead of the typical Cartesian\ncoordinates. Thus, given a time series X = x1,x 2, ...,x n of\nn real-valued observations, X is rescaled so that all values\nfall in the interval [−1, 1] by:\n˜xi\n−1 = xi − max(X) + (xi − min(X))\nmax(X) − min(X) (1)\nInt J Adv Manuf Technol (2019) 104:3647 –3662 3649\nThe time series ˜X can then be represented in polar\ncoordinates by encoding the value as the angular cosine and\nthe time stamp as the radius applying Eqs. 2 and 3:\nφ = arcos( ˜xi), −1 ≤˜xi ≤ 1, ˜xi ∈ ˜X (2)\nr = ti\nN ,t i ∈ N (3)\nIn Eq. 3, ti is the time stamp and N is a constant\nfactor to regularise the span of the polar coordinate system.\nFigure 1 shows an example of forces on z-dimension and its\nrepresentation in polar coordinates.\nAs time increases, corresponding values on the polar\ncoordinate system warp among different angular points\non the spanning circles. This representation preserves the\ntemporal relations and can easily be exploited to identify\nFig. 1 Forces on y-axis acquired from a dynamometer are encoded\nas polar coordinates by applying Eqs. 2 and 3. As time increases, the\ncorresponding values of the signal in polar coordinates wrap among\ndifferent angular points on the spanning circles, keeping the temporal\nrelations\nthe temporal correlation within different time intervals. This\ntemporal correlation is represented as:\nG =\n⎡\n⎢⎢⎢⎣\ncos(φ1 + φ1) ... cos( φ 1 + φn)\ncos(φ2 + φ1) ... cos( φ 2 + φn)\n... ... ...\ncos(φn + φ1) ... cos( φ n + φn)\n⎤\n⎥⎥⎥⎦ (4)\ncos(φi + φj ) = ˜X′ · ˜X −\n√\nI − ˜X2\n′\n·\n√\nI − ˜X2 (5)\nwhere I is a unit full row vector ([1,1,...,1]). Figure2 shows\nthe resulting image of applying the encoding method to the\ntime series presented in Fig. 1.\nThe GASF image provides a way to preserve temporal\ndependency. Time increases as the position in the image\nmoves from top–left to bottom–right. G(i,j||i−j|=k) repre-\nsents the relative correlation by superposition of directions\nwith respect to time interval k. The main diagonal Gi,i is\nthe special case when k = 0, which contains the original\nvalue/angular information. The dimension of the resulting\nGASF image is n × n when the time series is of length\nn. To reduce the size of the image, piecewise aggregation\napproximation (PAA) is applied to smooth the time series\nwhile keeping trends [49]. As explained in the Experiments\nsection, the amount of time series data that is acquired from\nthe sensors is large (more than 200,000 measurements), so\nPAA is fundamental to keep the images at a reasonable size\nwithout losing time coherence.\nTo label the images, three regions have been identified\nas defined in [ 50]. According to the literature, the tool\nlife in milling operations is typically divided into three\nstages/classes: a break-in region, which occurs with a rapid\nwear rate; the steady-state wear region with uniform wear\nrate; and a failure region, which again occurs with a rapid\nwear rate [ 51]. Figure 3 presents a tool degradation curve\nexample with the classes that were used to label the images.\nFig. 2 Example of the encoding of forces in the y-axis as an image\nusing GASF. The colour represents the intensity of the relative\ncorrelation between two points in the time series, which is a value\nbetween −1 and 1. There is no PAA smothing applied to the resulting\nimage, so the resolution (300 × 300 pixels) is the same as in the\noriginal signal\nInt J Adv Manuf Technol (2019) 104:3647 –36623650\nFig.3 Tool flank wear as a\nfunction of cutting time (cut\nevents of cutter c6 used in the\nexperiments). For each region, a\nsample image of forces in y-axis\nis provided\n3.2Deeplearningfortimeseriesclassiﬁcation\nTo identify the current state of wear of a tool by using sensor\nsignals, the approach applied needs to be capable of picking\nup the temporal dependencies present in the signals. Sensor\nsignals are expected to show changes in their temporal\nstructures as the tool wears out. A classification tool should\nbe capable of identifying those changes and map them to a\npredefined wear class.\nTime series classification methods are generally divided\ninto two categories: sequence-based methods and feature-\nbased methods. Among both of these categories, k-nearest\nneighbour ( k-NN), which is a sequence-based method,\nhas proven to be very difficult to beat. This is specially\ntrue when paired with dynamic time warping (DTW). The\ndrawback of this approach is its lengthy computation time.\nAs the training set grows, the computation time, and hence\nthe prediction time, increases linearly.\nAn approach that can provide constant prediction time\nas well as a way to extract relevant features automatically\nis deep learning. CNNs in particular have been successful\nin handling large volumes of data. Although they have\nbeen primarily used for visual tasks, voice recognition\nand language processing, new developments have looked\ntowards time series classification.\nCNNs have been inspired by the way the visual cortex\nin the human brain works. Neurons of the visual cortex\nhave a local receptive field which reacts to visual stimuli\nlocated in a limited region of the visual field [ 52]. These\nreceptive fields may overlap, tiling together the whole\nvisual field. Some neurons have larger receptive fields\nwhich react to more complex patterns that are further\ncombinations of lower level patterns. The discovery of\nthese basic functionalities of the human brain inspired the\nidea of developing an artificial neural network architecture\nwhereby higher level neurons are based on the outputs\nof neighbouring lower level neurons, to detect complex\npatterns. In 1998, LeCun et al. [ 32] proposed the LeNet-5\narchitecture, which contains the main building blocks of a\nCNN: the convolution layer and the pooling layer .\nA convolution layer is formed by a series of neurons that\nare connected to neurons of a previous layer based on the\ntheir receptive field. For example, in the first convolution\nlayer, each neuron is not connected to each individual\npixel of the input image, but to only those pixels within a\nreceptive field. Then each neuron in the second convolution\nlayer is connected to neurons within a small rectangle in\nthe first layer. The first convolution layer is responsible for\ndetecting the lower level features, and further convolutions\nassemble these features into higher level ones. The set of\nweights (i.e. filter) of a neuron in each convolution layer\nwill depend on the type of feature it is “looking” for. For\nexample, a particular filter would be able to detect vertical\nlines while another one could detect horizontal ones. During\nthe convolution, the filter is compared with different areas\nof the image, obtaining a feature map , that highlights the\nareas in an image that are most similar to the filter (see\nFig. 4a). As images posses a variety of different features,\neach convolution neuron would have more than one set of\nweights or filters. The training process will enable the CNN\nto find the most useful filters for the particular classification\ntask. In the case of the force classification that is addressed\nhere, the training process will find those filters that allow\nit to recognise in a first instance features at a flute level\nregardless of where in the image they are located. Then,\nhigher level convolutions allow the determination of the\nstate of the tool considering all flutes.\nThe pooling layer is another important building block\nof the CNN. This layer downscales the output of\nthe convolution, thus reducing dimensionality, the local\nsensitivity of the network and computational complexity\n(see Fig. 4b) [ 32]. A typical CNN architecture stacks\nInt J Adv Manuf Technol (2019) 104:3647 –3662 3651\nFig.4 Low-level features of\nforces are picked up by the first\nlayer, which are then assembled\ninto higher level features in the\nfollowing layers\nseveral convolutions (that may include a rectified linear\nunit (ReLU) step to speed up the training) and pooling\nlayers which reduce the size of the image as it gets deeper.\nFinally, at the top of the stack, a multilayer neural network\nis connected to the last convolution/pooling to perform the\nclassification.\nIn this paper, the CIFAR-10 architecture from Tensorflow\nhas been used [ 53]. This is an off-the-shelf CNN\narchitecture that has proven to achieve high accuracy on\nthe classification of 3-channel images (see Fig. 5). This\narchitecture has two convolution layers stacked with their\ncorresponding ReLU and pooling layers. Each convolution\napplies 64 filters. As will be presented in the next\nsection, the implemented CNN will take 3-channel images\ngenerated from the force sensors and use these for training.\nThe deep learning structure will be able to pick up\nthe relevant features that relate to tool wear condition.\nFigure 6 shows a schematic of how the approach has been\nimplemented.\n4Experimentsandresults\nTool wear classification was performed using a dataset\nthat was originally made available by the PHM2010 Data\nChallenge [ 13]. The dataset contains sensory data of six\n3-flute cutters (labelled c1, ..., c6) used in a high-speed\nCNC machine (R ¨oders Tech RFM760) under dry milling\nconditions until a significant wear stage. The experiment\nwith each cutter was carried out as follows. The workpiece\nsurface was machined line-by-line along the x-axis with\na 6-mm three-flute cutter. After finishing one pass along\nthe x-axis (axial depth of 0.2 mm and radial depth of\n0.125 mm), the tool was retracted to start a new pass.\nThis was done until the complete surface was removed.\nThen, the tool was removed from the tool holder and taken\nto a LEICA MZ12 microscope, where the corresponding\nflank wear (Vb) for each individual flute was measured. In\norder to capture cutting forces throughout the experiment,\na Kistler quartz 3-component platform dynamometer was\nFig.5 CNN architecture based on the Tensorflow implementation for the CIFAR-10 dataset (adapted from [53])\nInt J Adv Manuf Technol (2019) 104:3647 –36623652\nFig. 6 Framework proposed combining time series imaging and deep learning for tool wear classification. Forces in the three dimensions are\nindividually encoded using GASF and put together as 3-channel images. From those images, 70% is used for training a CNN model and then 30%\nused for testing\nmounted between the workpiece and the machining table. A\nschematic of this setup is shown in Fig. 7. To measure the\nvibrations, three Kistler piezo accelerometers were mounted\non the workpiece. Finally, an acoustic emission sensor was\nmounted on the workpiece to monitor the high-frequency\nstress wave generated by the cutting process. For each\ncutter, the seven signal channels (forces in the x-, y- and\nz-axes, vibrations in the x-, y- and z-axes and acoustic\nemission) were recorded while removing 315 layers of the\nstainless steel workpiece (see Table 1). Table 2 shows the\ndetails of the process conditions during the cutting tests.\nThe total size of the dataset for each cutter is about 3.2 GB,\nmaking in total nearly 20 GB for all cutters. In this work,\nonly three of the six cutters ( c1, c4 and c6)w e r eu s e d\nas these were labelled with their corresponding tool wear\nmeasurements. More details on the machining setup can be\nfound in [54].\nInitial experiments were carried out with a data subset\ncomprising a single cutting tool for the training and test\nsets, with a total data set size of 1 GB. In this case,\nthe cutter labelled c6, from which 315 cuts and tool\nwear measurements are available, was used. Force signals\nwere selected as the only input for the CNN to avoid a\ncomputationally expensive training process for this proof of\nconcept.\nTo prepare the dataset for training and testing of the\nCNN, each cutting force Fx, Fy and Fz corresponding to\na removed layer was encoded as three separate images.\nSince the time series that corresponds to one layer can\nbe as long as 219,000 measurements, a representative\nportion of the complete time series was taken. This was\ndone by selecting a subsequence of 2,000 measurements\nthat correspond to the middle of the layer, thus capturing\ndifferent material hardness. Applying the GASF method\nFig.7 Schematic of the\nexperimental setup used in [54]\nto collect forces, vibrations and\nfrequency stress waves of the\ncutting process\nInt J Adv Manuf Technol (2019) 104:3647 –3662 3653\nTable1 Signal channels and measurement data of the complete dataset\nSignal channel Measurement data\nChannel 1 Fx - cutting force in the X-dimension\nChannel 2 Fy - cutting force in the Y-dimension\nChannel 3 Fz - cutting force in the Z-dimension\nChannel 4 Vx - vibration in the X-dimension\nChannel 5 Vy - vibration in the Y-dimension\nChannel 6 Vz - vibration in the Z-dimension\nChannel 7 AE - acoustic emission\nThis study uses only those channels related to forces (top part of the\ntable)\nexplained in Section 3, an image for each force (Fx, Fy and\nFz) was obtained. These were then reduced from a size of\n2k × 2k pixels into images of 512 × 512 pixels using PAA\nand then combined into a 3-channel image. The associated\nwear class to this image is then determined by the flank\nwear value that was measured when the layer was removed.\nAlthough this experimental setup is particular to flank wear,\nthe images could be labelled using other types of wear such\nas crater wear. Regardless of the type of wear measure used,\nthe training process should be able to capture the features\non the input that relate to the particular wear measure used.\nAs an example, Fig. 8 shows forces on the x-axis at\ndifferent stages of the milling experiment. From what can\nbe observed in this figure, the forces tend to be more\nuniform (i.e. shapes tend to get more circular) as the tool\nstarts to wear out. The size reduction does not affect the\ntime coherence of the data, allowing each individual flute\ntemporal information to still be kept after PAA.\nIn total, the pre-processing step produced 315 3-channel\nimages, one for each cutting event. This set of images was\ndivided 70% for training and 30% for testing. The CNN\nwas trained using the softmax regression method, which\napplies a softmax nonlinearity to the output of the network\nand calculates the cross-entropy between the normalised\npredictions and the actual labels. The parameters used for\nthe training process are shown in Table 3.\nTable 2 Operating conditions during dry milling\nParameter Value\nSpindle speed 10,400 RPM\nFeed rate 1555 mm/min\nY depth of cut 0.125 mm\nZ depth of cut 0.2 mm\nSampling rate 50 kHz/channel\nMaterial Stainless steel\nCutting tool 6 mm ball nose tungsten carbide cutter\nOnce the model was trained, it was tested on the\nremaining 95 images. Table 4 presents a confusion matrix\nwith the results obtained. Based on the test set, the estimated\naccuracy of our model is 90%. Break-in wear was correctly\nclassified for 82% of the cases, steady wear 94% of the\ncases and failure wear correctly classified 75% of the cases.\nThe number of incorrect predictions suggest that the number\nof cases for break-in and failure regions may need to be\nincreased.\nAs it can be observed in Fig. 8, the number of cuts\nthat fall in the break-in region is 50, while the number\nof cuts in the steady-state are 200. This means that two-\nthirds of the data available would be categorised as steady-\nstate. If the training set is generated by randomly sampling\nfrom the complete dataset, it is likely that two-thirds of\nthose samples are steady-state class. This class imbalance\nproblem has been well documented in the literature [ 55–\n57]. Failure cases tend to be considerably less abundant than\nsteady wear cases. The less represented classes are more\nlikely to be misclassified than the majority examples due to\nthe design principles of the learning process. The training\nprocess optimises the overall classification accuracy which\nresults in the misclassification of the minority classes.\nTherefore, several techniques could be applied to balance\nthe number of samples of each class. Because the time\nseries corresponding to one layer of the workpiece can be as\nlong as 220,000 measurements, the data can be resampled.\nThis would generate more than one sample from each layer,\nparticularly with the break-in and failure cases. At the same\ntime, an undersampling can be done by adding another class\nfor the cases that are approaching the failure region. Thus, a\nfourth class that identifies this region could, in fact, be more\nuseful as currently the low-wear region covers a wide range\nof tool wear values. It is important to remark that tool wear\nprogresses differently depending on the type of tool, type of\nmaterial, cutting parameters and other cutting conditions. It\nis not possible to identify the degree of class imbalance for\na tool for which no prior data has been collected. Therefore,\nclass imbalance needs to be detected and acted upon as part\nof the data preparation prior to model training.\nA balanced number of cases among all classes will be\ncrucial to achieve accuracy homogeneity across all wear\nregions. The overall results are nevertheless promising,\nshowing that the CNN was successfully capable of\ncapturing the intrinsic structures of the sensory data. This\nmethod is then scalable to include the remaining cut data.\nA second experiment was performed by adding a 4th\nclass that corresponds to the area prior to entering the failure\nregion (Fig. 9). This area is of particular interest to this\nstudy as it considers a point in time were decisions could be\ntaken to extend the life of the tool. The number of instances\nper case was also increased by taking two more sub-\nsequences from each layer, for a total of three 2,000 sample\nInt J Adv Manuf Technol (2019) 104:3647 –36623654\nFig.8 Sample images of\nrescaled forces in the x-axis at\ndifferent stages of flank wear. It\ncan be observed how the shapes\nin the image become more\ncircular as the signal becomes\nsmoother. It can also be\nobserved how the information\nby individual flute is kept\nsub-sequences from the middle of each layer (cut event);\nenough so that the experiment could still be kept short for\nthe proof of concept. Sequences were again encoded into\nimages and labelled according to the wear value and the new\nclasses. A total of 954 images were produced, where 70%\nwas used for training and 30% for testing. The results are\nshown in Table 5.\nThe overall accuracy of the classification was 89%,\nwhich is about the same compared with the first experiment.\nHowever, there was an improvement on the percentage\nof cases correctly classified per class. For example, the\nbreak-in wear region went up from 82% in the previous\nexperiment, whereas the steady wear region remains at 94%.\nThe severe wear region, which was introduced in this round\nof experiments, is correctly classified 82% of the time.\nDespite this, it can be seen that only 6 cases (9%) of the\nsevere region were classified as steady wear. The other 6\ncases were classified as failure due to their proximity to\nthe failure values. Finally, the failure region cases were\naccurately classified 82% of the time, which is again an\nTable 3 O perating conditions during dry milling\nParameter Value\nMax steps 1000 steps\nLearning rate 0.01\nLearning rate decay factor 0.1\nNumber of examples per epoch 100 images\nNumber of epochs per decay 100 epochs\nTraining set size 220 images\nimprovement over the first experiment. From the number of\ncases, it can still be observed that there is a class imbalance\nthat could be affecting the training process.\nIn a third experiment, the class imbalance was addressed\nusing a stratified undersampling technique. In the previous\nexperiments, the datasets used for training were kept small\nto avoid high computational load for a proof of concept.\nHowever, it is possible to sample more subsequences from\neach of the 315 cuts. For the c6 tool, it is possible to sample\nup to 95 subsequences from each cut, generating a total\nof 29,925 3-channel images. An undersampling strategy to\ndeal with class imbalance is suitable in this case as the\ndataset is large enough to avoid losing critical features.\nUsing a strata based on the wear classes defined, sampling\nof each class was done individually, making sure classes\nsuch as steady state were undersampled to achieve an equal\nnumber of samples across all classes. After performing the\nundersampling, a training set consisting of 14,000 images\nand a test set of 6,000 images were produced. These were\nused to train and validate a new model.\nAs the size of the training had increased considerably,\nimages were reduced to 256 × 256. It was also decided to\nmove from a generic Tensorflow architecture implementa-\ntion to a more tuned one, by changing the size of the filters\nfor both convolution layers from 5×5t o1 6×16 for the first\nconvolution and from 5×5t o8 ×8 for the second convolu-\ntion. Given that the GASF images are typically capturing 7\ncomplete revolutions of the tool (21 cycles of the signal as\nthe tool has 3 flutes), the kernel of the first convolution was\nset to a size of 16, which allows capturing a complete signal\ncycle. This means that the convolution will be searching for\nInt J Adv Manuf Technol (2019) 104:3647 –3662 3655\nTable 4 Confusion matrix summarising the results on the test set\nNT = 95 Actual break-in wear, N(%) Actual steady wear, N(%) Actual failure region, N(%)\nPredicted break-in wear 14 (82.35%) 2 (0.03%) 0 (0%)\nPredicted steady wear 3 (17.64%) 62 (93.93%) 3 (0.25%)\nPredicted failure region 0 (0%) 2 (0.03%) 9 (0.75%)\nTotal 17 (100%) 66 (100%) 12 (100%)\nThe table shows the classification given by the CNN for all the cases on the test set, indicating the number of correctly classified as well as the\nincorrectly classified. NT refers to the total number of images in the test set\nFig.9 Four stages of tool wear\nfor cutters c1, c4 and c6,a n d\nsample images of forces in the\ny-axis that correspond to those\nregions\nTable 5 Confusion matrix summarising the results with four classes on the test set\nNT = 282 Actual break-in Actual steady Actual severe Actual failure\nWear N(%) Wear N(%) Wear N(%) N(%)\nPredicted break-in wear 32 (88.88%) 2 (1.5%) 0 (0%) 0 (0%)\nPredicted steady wear 4 (11.11%) 125 (94%) 6 (8.69%) 0 (0%)\nPredicted severe wear 0 (0%) 6 (4.5%) 57 (82.60%) 8 (18.18%)\nPredicted failure region 0 (0%) 0 (0%) 6 (8.69%) 36 (81.81%)\nTotal 36 (100%) 133 (100%) 69 (100%) 44 (100%)\nThe table shows the classification given by the CNN for all the cases on the test set, indicating the number of correctly classified as well as the\nincorrectly classified. NT refers to the total number of images in the test set\nFig.10 Confusion matrices summarising the results of the M6 model (cutter c6) with four classes using the stratified undersampling technique\nInt J Adv Manuf Technol (2019) 104:3647 –36623656\nfeatures at a flute level. The stride of the kernel was set to 4\ndue to the size of the image, allowing a reduction of the fea-\nture map by a quarter. The pooling layer that follows uses\na kernel of size 3, which allows a further reduction of the\nf e a t u r em a pt oas i z eo f3 2×32. This is enough to keep the\ndetected low-level features that will be grouped into higher\nlevel ones by the following convolution.\nResults with the new trained model are shown in Fig.10,\nwhere the model is labelled as M6, as it is the model that\ncorresponds to cutter c6. Overall, M6 was able to achieve a\n96.4% accuracy on the test set. The classification accuracy\nincreased for both the break-in and failure regions to 99.7%\nand 97.5% respectively when tested on c6. The lowest\naccuracy was shown in the severe region, where a result of\n92.6% correctly classified cases was achieved.\nTo understand the capabilities and limitations of the\napproach when a different set of data is available, a\nsimilar sampling and training was done with cutters c1\nand c4, generating two additional models, M1a n d M4,\nrespectively. Each of these models were validated against\nthe same cutter as well as the other two cutters. Accuracy\nresults per class are shown in Fig. 11 and the overall\nresults in Table 6. All experiments were carried out on a\n2.80 GHz Intel Core i7-7600C CPU and 32GB RAM. The\naverage training time for one batch (100 images) is 7.6 s,\nso a complete epoch takes approximately 16.5 min for any\nmodel. The testing time for one sample using any model\nis 0.2727 s. Although the training time is computationally\nTable 6 Summary of the accuracy (in %) of each model (labelledM1,\nM4a n dM6) when validated against the same cutter and other cutters\nCutter\nc1 (%) c4 (%) c6 (%)\nM1 96 89.3 80.4\nModel M4 79.6 96.8 80.9\nM6 71.6 85.18 96.4\nexpensive, testing is not, which still makes it applicable\nfor real-time monitoring. Training time can be improved by\nusing a higher specification processor or GPU as well as by\nparallelising the code and/or training one-class classifiers in\nparallel.\nAs can be observed in Table 6, there is not one model\nso far that works best when validated against all cutters.\nHowever, the model developed with c1 (M1) achieves the\nhighest accuracy across the three models when validated\nagainst other cutters (accuracy of 89.3% on c4,a n da n\naccuracy of 80.4% on c6). M1 particularly struggles\nclassifying correctly the failure cases of c6 (see Fig. 11 first\nrow). Looking at Fig. 9, it can be seen that c1 wears out at\na very high rate during the first 20 cuts, reaching the steady\nstate earlier than the other two cutters, and developing a\nlower tool wear after 315 cuts. This can explain why a model\ndeveloped with this tool might perform badly on highly\nworn cutters as it does not provide enough examples of the\nFig.11 Confusion matrices summarising the accuracy results (0–100 %) for M1( t o pr o w )a n dM4 (bottom row) across c1, c4 and c6 using four\ntool wear classes and the stratified undersampling for training/testing\nInt J Adv Manuf Technol (2019) 104:3647 –3662 3657\ndegree of wear that was developed by c6. Unfortunately,\na more in-depth analysis onto these differences in wear\ndegradation cannot be performed as no additional data\nor meta-data is available regarding the conditions of the\nPHM2010 data experiments. However, these results suggest\nthat a better model can be built if a combination of both\ncutters’ data was used in the training process.\nWhen analysing the results obtained with M6, it was\nobserved that this model is very good at identifying\nfailure cases when tested on c4. The model correctly\nclassifies tool failure 95.8% of the time. This model shows\nagain a weakness in identifying the severe region (see\nFig. 10). Most of the cases that are incorrectly classified are\nidentified as failure cases, which could be explained by the\nabrupt change in wear rate of c4 when approaching failure.\nM4 did not show particularly good results when identifying\ntool failure. This model achieved 75% and 70% accuracy\nwhen tested in c1 and c6. What is interesting to point out is\nthat M4 is particularly good at identifying the severe region\non c6, achieving a 97.7% accuracy. This again highlights\nthe importance of making sure that a training dataset be a\ngood representation of the search space in order to achieve\ngeneralisation.\nIn general, the results of the three models show the\nability of the architecture used to learn force patterns and\nrelate those to wear classes. The architectural setup of the\nCNN used in this last experiment allowed finding relevant\nfeatures at a flute level, which is necessary for the approach\nto detect the current maximum wear regardless of the flute\nthat is developing the wear. This is important, as it ensures\nthat the technique can achieve good results regardless of\nthe tool used. The accuracy obtained in particular classes\nshows the importance of presenting the CNN with samples\nthat are representative of all the input space during training.\nA more robust model would need to be enriched with data\nfrom different cutters to ensure this.\n5Comparisonoftheproposedapproach\ntopreviouswork\nThe proposed approach has its advantages and disadvan-\ntages when compared with other approaches. Making a\nfair comparison in terms of accuracy is not straightforward\ndue to several factors. First, to compare against classical\nmachine learning, the best set of features would need to be\nfound and not chosen arbitrarily. There are a wide range\nof algorithms for selecting and fusing features [ 58]; how-\never, it is not in the scope of this paper to explore these.\nIn addition, each approach has an “ideal” parametrisation\ndepending on the problem and specific instantiation of the\nmethodology, for example, selecting the right number of\nhidden layers and nodes in each layer of an ANN. For\nthis reason, the comparison is approached differently, by\ndescribing the power of using GASF as a tool to auto-\nmatically encode raw signals into images. The features\nof GASF images are ultimately exploited by an off-the-\nshelf CNN implementation that outputs the different stages\nof wear.\nMost of the published works in tool wear prediction or\ntool wear classification perform some type of specific data\npre-processing such as statistical feature selection using\nmean, maximum, standard deviation and median. Wu et\nal., for example, use these four features across multiple\nsensor data to perform tool wear prediction using ANNs,\nSVMs and random forests, the latter achieving the lowest\nroot mean square error (RMSE) [ 9]. In Zhao et al., a deep\nlearning approach using convolutional bi-directional LSTM\n(CBLSTM) network to perform tool wear prediction is\npresented. In this work, sensor signals are reduced from\n200,000 measurements into 100 datums of maximum and\nmean values, and these are fed into the CBLSTM model.\nFrom three different configurations of the approach, the\nauthors report that CBLSTM with dropout achieves the\nlowest RSME. [ 42]. The main disadvantage of manual\nfeature extraction is that, unless it is continuously re-applied\nto update the models, it does not consider changes in the\ndata distribution related to either noise or the tool wear\nphenomenon itself, making it unreliable in some cases. An\nexample of this can be seen in cutter c1. Inspecting the data\nof this cutter, it was found that, although mean, maximum\nand median statistics follow generally the same trend with\na tendency to increase with every cutting event, there is a\npeculiar change in these statistics for cutter c1 as seen in\nFig. 12. The figure shows how there is a sudden increase\nin the maximum force along the x-axis (also applies for the\nmean, median and standard deviation) around cutting events\n225 and 250, then the values return to their normal trend.\nAlthough change was not much in the wear measurements\nFig. 12 Maximum force in newtons (N) in the x-axis at each cutting\nevent for cutters c1, c4 and c6\nInt J Adv Manuf Technol (2019) 104:3647 –36623658\nduring this period of time (from 131.25 to 136.9 mm), the\nforce values did show changes. This suggests that some\nconditions of the experiment changed and were reflected on\nthe sensor readings but were not actually related to changes\nin tool wear. From the results reported in Zhao et al., it is\ninteresting to note that the highest RMSE obtained is on\ncutter c1, particularly during cutting events 225 and 250.\nThis strongly suggests that there is a sensitivity to maximum\nand mean values, as the highest errors occur during the\naforementioned cutting events. Although the method in [42]\nemploys a deep learning approach, their results suggest that\nthe model is not picking up the information on how one\nmeasurement changes in relation to another one in the time\nseries, like a typical deep neural network would do. In their\nwork, the dimensionality reduction performed averages 2k\nmeasurements, corresponding to nearly 7 revolutions of the\ntool, therefore losing the details of each individual flute.\nAs each flute might wear out at a different rate, retaining\nflute level information is relevant as it provides a better\nunderstanding of how the tool is wearing through time.\nFigure 13 shows two force samples and their corresponding\nGASF images between cutting events 225 and 250. By\nvisually inspecting the images, it can be inferred that not\nmuch change in the force patterns has happened during\nthese cutting events. The GASF image encoding provides\nthe CNN the right level of information for it to learn\nhow the tool erodes at the flute level as well as how\npatterns change from one flute to another regardless of the\nactual force measurement made. From the results shown\nin Fig. 11, it can be seen that M4 achieves an accuracy\nof 83% on the severe cases of c1. Taking into account\nthat a third of the mean force measurements are showing\na significant increase (Fig. 12), the CNN is still quite\nreliable in classifying these as severe (having only 17%\nas failure).\nA similar comparison with the work of Wu et al. [ 9]i s\nnot straightforward as results are presented as total accuracy\non the test set, with no detail of which tools were used\nfor training and for testing. As a result, it is not possible\nto determine from the reported results how the proposed\napproaches are capable of dealing with the noise or changes\nin the data distribution.\nA current disadvantage of the GASF representation is\nthe loss of the magnitude information of the measurement\nduring normalisation, as this normalisation process is\nperformed individually by image, not taking into account\nthe maximum value of all the observations. A combination\nof GASF and actual magnitude encoding could potentially\nbe more effective, particularly for the cases like inc1,w h e r e\nconditions could change suddenly.\nFig.13 Sample images of\nrescaled forces in the x-axis\nduring cutting events: a 225 and\nb 250. Although there is a\nsudden increase on the mean\nforce during cutting events 225\nand 250 (which is not visible\nafter normalisation), the wear\ndoes not increase at that same\nr a t e .I nf a c t ,t h eG A S Fi m a g e s\nsuggest there is not much\nchange on the wear as the force\npatterns are very similar\nInt J Adv Manuf Technol (2019) 104:3647 –3662 3659\n6Conclusionsandfuturework\nThis paper presents an approach to tool wear classification\nby means of sensory data imaging and deep learning. The\nGASF encoding keeps the temporal correlations for each\nflute, which is an advantage over classification methods\nthat are based on statistical features, where the features\nof a particular flute are lost. Experimental results show\nthe ability of the CNN to capture and learn the features\non the raw data to correctly classify tool wear condition.\nOverall, the percentage of accurately classified cases on\nthe test set is high, achieving in most cases above 80%\nwhen testing in a new cutter. The moment prior to the\ntransition from critical wear to failure is in most cases\ncorrectly identified, and the cases where it is incorrectly\nclassified were generally labelled as a failure, which from\nan application standpoint means the replacement of the tool\nwould still be enacted. These results show the importance\nof using a training sample set that can represent all of the\ninput space. In this case, the training set needs to be enriched\nwith samples from multiple cutters to ensure the successful\ndetection of the transition period from severe to failure. The\napplication of this work will allow for the extension of the\nremaining useful life of the tool, improve cut quality and\nensure machining elements are replaced before failure.\nFuture work will include parallelisation of the architec-\nture and its implementation to run in GPUs as well as\nincorporating the approach in a cloud architecture. Tech-\nniques for partially retraining the architecture will also be\nexplored to study its adaptation capabilities when new data\nbecomes available. Additional work will also include exper-\nimentation with more input channels on the GASF image to\nfeed in multiple sensor data and improve the accuracy of the\nclassification. Finally, further enhancements to the encod-\ning technique will be investigated such as incorporating the\nmagnitude information.\nFundinginformation The authors received support from the Horizon\n2020 MC-SUITE (ICT Powered MaChining Suite) project funded by\nthe European Commission under grant agreement No 680478.\nOpen Access This article is distributed under the terms of the\nCreative Commons Attribution 4.0 International License ( http://\ncreativecommons.org/licenses/by/4.0/), which permits unrestricted\nuse, distribution, and reproduction in any medium, provided you give\nappropriate credit to the original author(s) and the source, provide a\nlink to the Creative Commons license, and indicate if changes were\nmade.\nReferences\n1. MacDougall W (2014) Industrie 4.0 Smart Manufacturing for the\nFuture. GTAI Germany Trade and Invest\n2. Wang J, Ma Y, Zhang L, Gao RX, Wu D (2018) Deep learning\nfor smart manufacturing: Methods and applications. J Manuf Syst\n48:144–156. https://doi.org/10.1016/j.jmsy.2018.01.003, special\nIssue on Smart Manufacturing\n3. Bonifacio M, Diniz A (1994) Correlating tool wear, tool life, sur-\nface roughness and tool vibration in finish turning with coated car-\nbide tools. Wear 173(1):137–144. https://doi.org/10.1016/0043-\n1648(94)90266-6\n4. Ambhore N, Kamble D, Chinchanikar S, Wayal V (2015) Tool\ncondition monitoring system: A review. Mater Today: Proc\n2(4):3419–3428. https://doi.org/10.1016/j.matpr.2015.07.317,4 t h\nInternational Conference on Materials Processing and Charac-\nterzation\n5. Kong D, Chen Y, Li N (2017) Force-based tool wear estimation\nfor milling process using gaussian mixture hidden markov models.\nInt J Adv Manuf Technol 92(5):2853–2865. https://doi.org/10.\n1007/s00170-017-0367-1\n6. Niaki FA, Ulutan D, Mears L (2015) In-process tool flank wear\nestimation in machining gamma-prime strengthened alloys using\nkalman filter. Procedia Manuf 1:696–707.https://doi.org/10.1016/\nj.promfg.2015.09.018, 43rd North American Manufacturing\nResearch Conference, NAMRC 43, 8-12 June 2015, UNC Char-\nlotte, North Carolina, United States\n7. Wang P, Gao RX (2015) Adaptive resampling-based particle\nfiltering for tool life prediction. J Manuf Syst 37:528–534.\nhttps://doi.org/10.1016/j.jmsy.2015.04.006\n8. Cosme LB, D’Angelo MFSV, Caminhas WM, Yin S, Palhares\nRM (2018) A novel fault prognostic approach based on particle\nfilters and differential evolution. Appl Intell 48(4):834–853.\nhttps://doi.org/10.1007/s10489-017-1013-1\n9. Wu D, Jennings C, Terpenny J, Kumara S (2016) Cloud-based\nmachine learning for predictive analytics: Tool wear prediction in\nmilling. In: 2016 IEEE International Conference on Big Data (Big\nData), pp 2062–2069. https://doi.org/10.1109/BigData.2016.7840\n831\n10. Sick B (2002) On-line and indirect tool wear monitoring in\nturning with artificial neural networks: a review of more than\na decade of research. Mech Syst Signal Process 16(4):487–546.\nhttps://doi.org/10.1006/mssp.2001.1460\n11. Wuest T, Weimer D, Irgens C, Thoben KD (2016) Machine learn-\ning in manufacturing: advantages, challenges, and applications.\nProd Manuf Res 4(1):23–45. https://doi.org/10.1080/21693277.\n2016.1192517\n12. Terrazas G, Mart ´ınez-Arellano G, Benardos P, Ratchev S (2018)\nOnline tool wear classification during dry machining using real\ntime cutting force measurements and a cnn approach. J Manuf\nMater Process 2(4):72. https://doi.org/10.3390/jmmp2040072\n13. PHMSociety (2010) 2010 phm society conference data challenge,\nhttps://www.phmsociety.org/competition/phm/10, Accessed Jan-\nuary 31, 2018\n14. Cui X, Zhao J, Dong Y (2013) The effects of cutting parameters\non tool life and wear mechanisms of cbn tool in high-speed face\nmilling of hardened steel. Int J Adv Manuf Technol 66(5):955–\n964. https://doi.org/10.1007/s00170-012-4380-0\n15. Taylor F (1907) On the art of cutting metals. Trans Am Soc Mech\nEng 38:31–35\n16. Poulachon G, Moisan A, Jawahir I (2001) Tool-wear mechanisms\nin hard turning with polycrystalline cubic boron nitride tools. Wear\n250(1):576–586. https://doi.org/10.1016/S0043-1648(01)00609-3,\n13th International Conference on Wear of Materials\n17. Karandikar JM, Abbas AE, Schmitz TL (2013) Tool life prediction\nusing random walk bayesian updating. Mach Sci Technol\n17(3):410–442. https://doi.org/10.1080/10910344.2013.806103\n18. Sun J, Rahman M, Wong Y, Hong G (2004) Multiclassification\nof tool wear with support vector machine by manufacturing\nloss consideration. Int J Mach Tools Manuf 44(11):1179–1187.\nhttps://doi.org/10.1016/j.ijmachtools.2004.04.003\nInt J Adv Manuf Technol (2019) 104:3647 –36623660\n19. ¨Ozel T, Karpat Y (2005) Predictive modeling of surface\nroughness and tool wear in hard turning using regression\nand neural networks. Int J Mach Tools Manuf 45(4):467–479.\nhttps://doi.org/10.1016/j.ijmachtools.2004.09.007\n20. Palanisamy P, Rajendran I, Shanmugasundaram S (2008) Pre-\ndiction of tool wear using regression and ANN models in\nend-milling operation. Int J Adv Manuf Technol 37:29–41.\nhttps://doi.org/10.1007/s00170-007-0948-5\n21. Sanjay C, Neema M, Chin C (2005) Modeling of tool wear in\ndrilling by statistical analysis and artificial neural network. J Mater\nProcess Technol 170(3):494–500. https://doi.org/10.1016/j.jmat\nprotec.2005.04.072\n22. Chungchoo C, Saini D (2002) On-line tool wear estimation in cnc\nturning operations using fuzzy neural network model. Int J Mach\nTools Manuf 42(1):29–40. https://doi.org/10.1016/S0890-6955\n(01)00096-7\n23. Ferry N, Terrazas G, Kalweit P, Solberg A, Ratchev S, Weinelt\nD (2017) Towards a big data platform for managing machine\ngenerated data in the cloud. In: 2017 IEEE 15th International\nConference on Industrial Informatics (INDIN), pp 263–270.\nhttps://doi.org/10.1109/INDIN.2017.8104782\n24. Liao TW (2005) Clustering of time series data - a survey. Pattern\nRecogn 38(11):1857–1874\n25. Krizhevsky A, Sutskever I, Hinton GE (2012) Imagenet classifi-\ncation with deep convolutional neural networks. In: Proceedings\nof Advances in neural information processing systems, curran\nassociates inc., USA, NIPS’12, vol 25, pp 1090–1098\n26. Ciresan DC, Meier U, Schmidhuber J (2012). Multi-column deep\nneural networks for image classification. arXiv:1202.2745\n27. Hinton G, Deng L, Yu D, Dahl GE, Mohamed AR, Jaitly N,\nSenior A, Vanhoucke V, Nguyen P, Sainath TN, Kingsbury B\n(2012) Deep neural networks for acoustic modeling in speech\nrecognition: The shared views of four research groups. IEEE\nSignal Proc Mag 29(6):82–97. https://doi.org/10.1109/MSP.2012.\n2205597\n28. Sutskever I, Vinyals O, Le QV (2014) Sequence to sequence learn-\ning with neural networks, pp 3104–3112. http://papers.nips.cc/\npaper/5346-sequence-to-sequence-learning-with-neural-networks.\npdf\n29. LeCun Y, Bengio Y, Hinton GE (2015) Deep learning. Nature\n521(7553):436–444. https://doi.org/10.1038/nature14539\n30. Malhi A, Yan R, Gao RX (2011) Prognosis of defect propagation\nbased on recurrent neural networks. IEEE Trans Instrum Meas\n60(3):703–711. https://doi.org/10.1109/TIM.2010.2078296\n31. Zhao R, Wang D, Yan R, Mao K, Shen F, Wang J (2018)\nMachine health monitoring using local feature-based gated\nrecurrent unit networks. IEEE Trans Ind Electron 65(2):1539–\n1548. https://doi.org/10.1109/TIE.2017.2733438\n32. LeCun Y, Bengio Y (1998) The handbook of brain theory and\nneural networks. MIT Press, Cambridge. Convolutional Networks\nfor Images, Speech, and Time Series, pp 255–258\n33. Chen X, Xiang S, Liu C, Pan C (2014) Vehicle detection in satellite\nimages by hybrid deep convolutional neural networks. IEEE\nGeosci Remote Sens Lett 11(10):1797–1801. https://doi.org/10.\n1109/LGRS.2014.2309695\n34. Dieleman S, Willett KW, Dambre J (2015) Rotation-invariant\nconvolutional neural networks for galaxy morphology prediction.\nMon Not R Astron Soc 450(2):1441–1459. https://doi.org/10.\n1093/mnras/stv632\n35. Pereira S, Pinto A, Alves V, Silva CA (2016) Brain tumor segmen-\ntation using convolutional neural networks in mri images. IEEE\nTrans Med Imaging 35(5):1240–1251. https://doi.org/10.1109/\nTMI.2016.2538465\n36. Kiranyaz S, Ince T, Gabbouj M (2016) Real-time patient-specific\necg classification by 1-d convolutional neural networks. IEEE\nTrans Biomed Eng 63(3):664–675.https://doi.org/10.1109/TBME.\n2015.2468589\n37. Zheng Y, Liu Q, Chen E, Ge Y, Zhao JL (2014) Time series\nclassification using multi-channels deep convolutional neural\nnetworks. In: Li F, Li G, Sw H, Yao B, Zhang Z (eds) Web-\nage information management. Springer International Publishing,\nCham, pp 298-310\n38. Levent E (2017) Bearing fault detection by one-dimensional\nconvolutional neural networks. Math Probl Eng 2017\n39. Li S, Liu G, Tang X, Lu J, Hu J (2017) An ensemble\ndeep convolutional neural network model with improved d-\ns evidence fusion for bearing fault diagnosis. Sensors 17(8).\nhttps://doi.org/10.3390/s17081729\n40. Zhang W, Peng G, Li C (2017) Bearings fault diagnosis based on\nconvolutional neural networks with 2-d representation of vibration\nsignals as input. In: MATEC Web of conferences, EDP sciences,\nvol 95, pp 13001\n41. Abdeljaber O, Avci O, Kiranyaz S, Gabbouj M, Inman\nDJ (2017) Real-time vibration-based structural damage detec-\ntion using one-dimensional convolutional neural networks. J\nSound Vibr 388:154–170. https://doi.org/10.1016/j.jsv.2016.10.\n043\n42. Zhao R, Yan R, Wang J, Mao K (2017) Learning to monitor\nmachine health with Convolutional Bi-Directional LSTM Net-\nworks. Sensors 17(2):273. https://doi.org/10.3390/s17020273\n43. Wang Z, Oates T (2015) Imaging time-series to improve\nclassification and imputation. In: Proceedings of the 24th\nInternational Conference on Artificial Intelligence, IJCAI’15.\nAAAI Press, pp 3939–3945\n44. Eckmann JP, Kamphorst SO, Ruelle D (1987) Recurrence\nplots of dynamical systems. Europhys Lett (EPL) 4(9):973–977.\nhttps://doi.org/10.1209/0295-5075/4/9/004\n45. Silva DF, Souza VMAD, Batista GEAPA (2013) Time series\nclassification using compression distance of recurrence plots. In:\n2013 IEEE 13th International Conference on Data Mining, pp\n687–696. https://doi.org/10.1109/ICDM.2013.128\n46. Donner RV, Small M, Donges JF, Zou Y et al (2011) Recurrence-\nbased time series analysis by means of complex network methods.\nInt J Bifurcat Chaos 21(04):1019–1046\n47. Campanharo ASLO, Sirer MI, Malmgren RD, Ramos FM, Amaral\nLAN (2011) Duality between time series and networks. PLOS\nONE 6(8):1–13. https://doi.org/10.1371/journal.pone.0023378\n48. Chen J, Chen W, Huang C, Huang S, Chen A (2016) Financial\ntime-series data analysis using deep convolutional neural net-\nworks. In: 2016 7th International Conference on Cloud Computing\nand Big Data (CCBD), pp 87–92. https://doi.org/10.1109/CCBD.\n2016.027\n49. Keogh EJ, Pazzani MJ (2000) Scaling up dynamic time warping\nfor datamining applications. In: Proceedings of the Sixth ACM\nSIGKDD International Conference on Knowledge Discovery\nand Data Mining, KDD ’00. ACM, New York, pp 285–289.\nhttps://doi.org/10.1145/347090.347153\n50. Groover M (2010) Fundamentals of modern manufacturing,\nMaterials, Processes, and Systems. Wiley, New York\n51. Binder M, Klocke F, Doebbeler B (2017) An advanced\nnumerical approach on tool wear simulation for tool and process\ndesign in metal cutting. Simul Modell Pract Theory 70:65–82.\nhttps://doi.org/10.1016/j.simpat.2016.09.001\n52. G ´eron A (2017) Hands-On Machine learning with Scikit-\nLearn and tensorflow: Concepts, Tools and Techniques to build\nIntelligent Systems. O’Reilly Media Inc\n53. Tensorflow (2017) Convolutional neural networks, https://www.\ntensorflow.org/tutorials/deep cnn, Accessed January 23, 2018\n54. Li X, Lim BS, Zhou JH, Huang SC, Phua S, Shaw KC (2009)\nFuzzy neural network modelling for tool wear estimation in dry\nInt J Adv Manuf Technol (2019) 104:3647 –3662 3661\nmilling operation. In: Annual Conference of the Prognostics and\nHealth Management Society, pp 1–11\n55. Longadge R, Dongre S (2013) Class imbalance problem in data\nmining review. arXiv:1305.1707\n56. Khan SH, Hayat M, Bennamoun M, Sohel FA, Togneri R\n(2018) Cost-sensitive learning of deep feature representations\nfrom imbalanced data. IEEE Trans Neural Netw Learn Syst\n29(8):3573–3587. https://doi.org/10.1109/TNNLS.2017.2732482\n57. Lee T, Lee KB, Kim CO (2016) Performance of machine learn-\ning algorithms for class-imbalanced process fault detection\nproblems. IEEE Trans Semicond Manuf 29(4):436–445.\nhttps://doi.org/10.1109/TSM.2016.2602226\n58. Wang J, Xie J, Zhao R, Zhang L, Duan L (2017) Mul-\ntisensory fusion based virtual tool wear sensing for ubiq-\nuitous manufacturing. Robot Comput-Integr Manuf 45:47–58.\nhttps://doi.org/10.1016/j.rcim.2016.05.010, special Issue on Ubiq-\nuitous Manufacturing (UbiM)\nPublisher’s note Springer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional affiliations.\nInt J Adv Manuf Technol (2019) 104:3647 –36623662",
  "topic": "Deep learning",
  "concepts": [
    {
      "name": "Deep learning",
      "score": 0.6743692755699158
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6383327841758728
    },
    {
      "name": "Computer science",
      "score": 0.6106793284416199
    },
    {
      "name": "Machining",
      "score": 0.5790208578109741
    },
    {
      "name": "Tool wear",
      "score": 0.571091890335083
    },
    {
      "name": "Filter (signal processing)",
      "score": 0.5662598013877869
    },
    {
      "name": "Process (computing)",
      "score": 0.5499340295791626
    },
    {
      "name": "Raw data",
      "score": 0.5319849252700806
    },
    {
      "name": "Identification (biology)",
      "score": 0.49761155247688293
    },
    {
      "name": "Machine learning",
      "score": 0.48028871417045593
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.41536587476730347
    },
    {
      "name": "Data mining",
      "score": 0.3945772647857666
    },
    {
      "name": "Engineering",
      "score": 0.28956907987594604
    },
    {
      "name": "Computer vision",
      "score": 0.2340012788772583
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    }
  ]
}