{
  "title": "A Neural Language Model for Dynamically Representing the Meanings of\\n Unknown Words and Entities in a Discourse",
  "url": "https://openalex.org/W2963529986",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A5112876488",
      "name": "Sosuke Kobayashi",
      "affiliations": [
        "Preferred Networks (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A5066940046",
      "name": "Naoaki Okazaki",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101815181",
      "name": "Kentaro Inui",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2962944953",
    "https://openalex.org/W2963277143",
    "https://openalex.org/W1793121960",
    "https://openalex.org/W2611669587",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W1665214252",
    "https://openalex.org/W2115733720",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W2964165364",
    "https://openalex.org/W2963380480",
    "https://openalex.org/W2963077125",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2963970792",
    "https://openalex.org/W2604685013",
    "https://openalex.org/W2962769558",
    "https://openalex.org/W2963494889",
    "https://openalex.org/W2963324947",
    "https://openalex.org/W2963695529",
    "https://openalex.org/W2475151947",
    "https://openalex.org/W2507974895",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W2963167649",
    "https://openalex.org/W2962883855",
    "https://openalex.org/W1525961042",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963290255",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2563734883",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2571859396",
    "https://openalex.org/W2155069789",
    "https://openalex.org/W2621404689",
    "https://openalex.org/W2963341924",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2118434577",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W1010415138",
    "https://openalex.org/W2508661145",
    "https://openalex.org/W2963925965",
    "https://openalex.org/W2963097991"
  ],
  "abstract": "This study addresses the problem of identifying the meaning of unknown words\\nor entities in a discourse with respect to the word embedding approaches used\\nin neural language models. We proposed a method for on-the-fly construction and\\nexploitation of word embeddings in both the input and output layers of a neural\\nmodel by tracking contexts. This extends the dynamic entity representation used\\nin Kobayashi et al. (2016) and incorporates a copy mechanism proposed\\nindependently by Gu et al. (2016) and Gulcehre et al. (2016). In addition, we\\nconstruct a new task and dataset called Anonymized Language Modeling for\\nevaluating the ability to capture word meanings while reading. Experiments\\nconducted using our novel dataset show that the proposed variant of RNN\\nlanguage model outperformed the baseline model. Furthermore, the experiments\\nalso demonstrate that dynamic updates of an output layer help a model predict\\nreappearing entities, whereas those of an input layer are effective to predict\\nwords following reappearing entities.\\n",
  "full_text": "arXiv:1709.01679v2  [cs.CL]  17 Oct 2017\nA Neural Language Model for Dynamically Representing\nthe Meanings of Unknown W ords and Entities in a Discourse\nSosuke Kobayashi\nPreferred Networks, Inc., Japan\nsosk@preferred.jp\nNaoaki Okazaki\nT okyo Institute of T echnology, Japan\nokazaki@c.titech.ac.jp\nKentaro Inui\nT ohoku University / RIKEN, Japan\ninui@ecei.tohoku.ac.jp\nAbstract\nThis study addresses the problem of iden-\ntifying the meaning of unknown words\nor entities in a discourse with respect to\nthe word embedding approaches used in\nneural language models. W e proposed\na method for on-the-ﬂy construction and\nexploitation of word embeddings in both\nthe input and output layers of a neural\nmodel by tracking contexts. This extends\nthe dynamic entity representation used in\nKobayashi et al. (2016) and incorporates a\ncopy mechanism proposed independently\nby\nGu et al. (2016) and Gulcehre et al.\n(2016). In addition, we construct a new\ntask and dataset called Anonymized Lan-\nguage Modeling for evaluating the abil-\nity to capture word meanings while read-\ning. Experiments conducted using our\nnovel dataset show that the proposed vari-\nant of RNN language model outperformed\nthe baseline model. Furthermore, the ex-\nperiments also demonstrate that dynamic\nupdates of an output layer help a model\npredict reappearing entities, whereas those\nof an input layer are effective to predict\nwords following reappearing entities.\n1 Introduction\nLanguage models that use probability distri-\nbutions over sequences of words are found\nin many natural language processing applica-\ntions, including speech recognition, machine\ntranslation, text summarization, and dialogue\nutterance generation. Recent studies have\ndemonstrated that language models trained\nusing neural network (\nBengio et al. , 2003;\nMikolov et al. , 2010) such as recurrent neural\nnetwork (RNN) ( Jozefowicz et al. , 2016) and\n...  [   1   ] killed [   2   ] with bombs …\n... police suspects  [   1   ]  attacked ... \n... police will arrest [  1  ] …\n...   will  arrest  [  1  ]  soon  …d[2],2 d[1],2 \n! !!!\nx[1] x[2] y[1] y[2] \n= − − − → \nRNN( \nFigure 1: Dynamic Neural T ext Modeling: the\nembeddings of unknown words, denoted by coref-\nerence indexes “[ k ]” are dynamically computed\nand used in both the input and output layers ( x[k]\nand y[k]) of a RNN language model. These are\nconstructed from contextual information ( d[k],i)\npreceding the current (i + 1)-th sentence.\nconvolutional neural network (\nDauphin et al. ,\n2016) achieve the best performance across a range\nof corpora ( Mikolov et al. , 2010; Chelba et al. ,\n2014; Merity et al. , 2017; Grave et al. , 2017).\nHowever, current neural language models have\na major drawback: the language model works only\nwhen applied to a closed vocabulary of ﬁxed size\n(usually comprising high-frequency words from\nthe given training corpus). All occurrences of out-\nof-vocabulary words are replaced with a single\ndummy token “ <unk>”, showing that the word is\nunknown. For example, the word sequence, Piko-\ntaro sings PP AP on Y ouTubeis treated as <unk>\nsings <unk> on <unk> assuming that the words\nPikotaro, PP AP, and Y ouTubeare out of the vo-\ncabulary . The model therefore assumes that these\nwords have the same meaning, which is clearly in-\ncorrect. The derivation of meanings of unknown\nwords remains a persistent and nontrivial chal-\nlenge when using word embeddings.\nIn addition, existing language models further\nassume that the meaning of a word is the same\nd[4],2 d[4],1 \n...  [   1   ] killed [   2   ] with bombs … ... pol ice suspects  [   1   ]  attacked ... \nd[3],1 \nMerge \nMerge \nd’ [1],1 d’ [2],1 \nd[2],1 \nd[1],1 d[1],0 \nd[2],0 \nd[4],0\nd[3],0 \nd[2],2\nd[1],2 \nd[3],2 \nMerge \nd’ [1],2 \n= − − − → \nRNN( \n= ← − − − \nRNN( \nFigure 2: Dynamic Neural T ext Modeling: the meaning represe ntation of each unknown word, denoted\nby a coreference index “[ k ]”, is inferred from the local cont exts in which it occurs.\nand universal across different documents. Neural\nlanguage models also make this assumption and\nrepresent all occurrences of a word with a single\nword vector across all documents. However, the\nassumption of a universal meaning is also unlikely\ncorrect. For example, the name John is likely to re-\nfer to different individuals in different documents.\nIn one story , John may be a pianist while another\nJohn denoted in a second story may be an infant. A\nmodel that represents all occurrences of John with\nthe same vector fails to capture the very different\nbehavior expected from John as a pianist and John\nas an infant.\nIn this study , we address these issues and pro-\npose a novel neural language model that can build\nand dynamically change distributed representa-\ntions of words based on the multi-sentential dis-\ncourse. The idea of incorporating dynamic mean-\ning representations into neural networks is not\nnew . In the context of reading comprehension,\nKobayashi et al. (2016) proposed a model that dy-\nnamically computes the representation of a named\nentity mention from the local context given by\nits prior occurrences in the text. In neural ma-\nchine translation, the copy mechanism was pro-\nposed as a way of improving the handling of out-\nof-vocabulary words (e.g., named entities) in a\nsource sentence (\nGu et al. , 2016; Gulcehre et al. ,\n2016). W e use a variant of recurrent neural lan-\nguage model (RNLM), that combines dynamic\nrepresentation and the copy mechanism. The re-\nsulting novel model, Dynamic Neural T ext Model,\nuses the dynamic word embeddings that are con-\nstructed from the context in the output and input\nlayers of an RNLM, as shown in Figures\n1 and 2.\nThe contributions of this paper are three-fold.\nFirst, we propose a novel neural language model,\nwhich we named the Dynamic Neural T ext Model.\nSecond, we introduce a new evaluation task and\ndataset called Anonymized Language Modeling .\nThis dataset can be used to evaluate the ability of\na language model to capture word meanings from\ncontextual information (Figure\n3). This task in-\nvolves a kind of one-shot learning tasks, in which\nthe meanings of entities are inferred from their\nlimited prior occurrences. Third, our experimen-\ntal results indicate that the proposed model out-\nperforms baseline models that use only global and\nstatic word embeddings in the input and/or out-\nput layers of an RNLM. Dynamic updates of the\noutput layer helps the RNLM predict reappearing\nentities, whereas those of the input layer are ef-\nfective to predict words following reappearing en-\ntities. A more detailed analysis showed that the\nmethod was able to successfully capture the mean-\nings of words across large contexts, and to accu-\nmulate multiple context information.\n2 Background\n2.1 RNN Language Model\nGiven a sequence of N tokens of a docu-\nment D = ( w1, w2, ..., wN ), an RNN lan-\nguage model computes the probability p(D) =∏ N\nt=1 p(wt|w1, ..., wt−1). The computation of\neach factorized probability p(wt|w1, ..., wt−1) can\nalso be viewed as the task of predicting a following\nword wt from the preceding words (w1, ..., wt−1).\nT ypically , RNNs recurrently compute the proba-\nbility of the following word wt by using a hidden\nstate ht−1 at time step t − 1,\np(wt|w1, ..., wt−1) = exp(⃗h⊺\nt−1ywt + bwt )\n∑\nw∈V exp(⃗h⊺\nt−1yw + bw)\n,\n(1)\n⃗ht = −−−→\nRNN(xwt , ⃗ht−1). (2)\nHere, xwt and ywt denote the input and out-\nput word embeddings of wt respectively , V rep-\nresents the set of words in the vocabulary , and\nbw is a bias value applied when predicting the\nword w. The function −−−→\nRNN is often replaced\nwith LSTM (\nHochreiter and Schmidhuber , 1997)\nor GRU ( Cho et al. , 2014) to improve perfor-\nmance.\n2.2 Dynamic Entity Representation\nRNN-based models have been reported to\nachieve better results on the CNN QA read-\ning comprehension dataset (\nHermann et al. ,\n2015; Kobayashi et al. , 2016). In the CNN QA\ndataset, every named entity in each document is\nanonymized. This is done to allow the ability\nto comprehend a document using neither prior\nnor external knowledge to be evaluated. T o\ncapture the meanings of such anonymized entities,\nKobayashi et al. (2016) proposed a new model\nthat they named dynamic entity representation .\nThis encodes the local contexts of an entity and\nuses the resulting context vector as the word\nembedding of a subsequent occurrence of that\nentity in the input layer of the RNN. This model:\n(1) constructs context vectors d′\ne,i from the local\ncontexts of an entity e at the i-th sentence; (2)\nmerges multiple contexts of the entity e through\nmax pooling and produces the dynamic repre-\nsentation de,i; and (3) replaces the embedding of\nthe entity e in the ( i + 1)-th sentence with the\ndynamic embedding xe,i+1 produced from de,i.\nMore formally ,\nxe,i+1 = Wdcde,i + be, (3)\nde,i = maxpooling(d′\ne,i, de,i−1), (4)\nd′\ne,i = ContextEncoder(e, i). (5)\nHere, be denotes a bias vector, maxpooling is a\nfunction that yields the largest value from the el-\nementwise inputs, and ContextEncoder is an en-\ncoding function. Figure\n2 gives an example of the\nprocess of encoding and merging contexts from\nsentences. An arbitrary encoder can be used for\nContextEncoder;\nKobayashi et al. (2016) used\nbidirectional RNNs, encoding the words surround-\ning the entity e of a sentence in both directions. If\nthe entity e fails to appear in the i-th sentence, the\nembedding is not updated, i.e., de,i = de,i−1.\n3 Proposed Method: Dynamic Neural\nT ext Modeling\nIn this section, we introduce the extension of dy-\nnamic entity representation to language modeling.\nFrom Equations\n1 and 2, RNLM uses a set of word\nembeddings in the input layer to encode the pre-\nceding contextual words, and another set of word\nembeddings in the output layer to predict a word\nfrom the encoded context. Therefore, we consider\nincorporating the idea of dynamic representation\ninto the word embeddings in the output layer ( yw\nin Equation\n1) as well as in the input layer ( xw in\nEquation 2; refer to Figure 1). The novel exten-\nsion of dynamic representation to the output layer\naffects predictions made for entities that appear\nrepeatedly , whereas that in the input layer is ex-\npected to affect the prediction of words that follow\nthe entities.\nThe procedure for constructing dynamic repre-\nsentations of e, de,i is the same as that introduced\nin Section\n2.2. Before reading the ( i + 1)-th sen-\ntence, the model constructs the context vectors\n[d′\ne,1, ..., d′\ne,i] from the local contexts of e in every\npreceding sentence. Here, d′\ne,j denotes the context\nvector of e in the j-th sentence. ContextEncoder\nin the model produces a context vector d′\ne for e\nat the t-th position in a sentence, using a bidirec-\ntional RNN\n1 as follows:\nd′\ne = ReLU(Whd[⃗ht−1,\n⃗ht+1]+bd), (6)\n⃗ht = −−−→\nRNN(xwt , ⃗ht−1), (7)\n⃗ht = ←−−−\nRNN(xwt ,\n⃗ht+1). (8)\nHere, ReLU denotes the ReLU activation func-\ntion ( Nair and Hinton , 2010), while Wdc and Whd\ncorrespond to learnable matrices; bd is a bias vec-\ntor. As in the RNN language model, ⃗ht−1 and\n⃗ht+1 as well as their composition d′\ne can capture\ninformation necessary to predict the features of the\ntarget e at the t-th word.\nFollowing context encoding, the model merges\nthe multiple context vectors, [d′\ne,1, ..., d′\ne,i], into\nthe dynamic representation de,i using a merging\nfunction. A range of functions are abailable for\nmerging multiple vectors, while\nKobayashi et al.\n(2016) used only max pooling (Equation 4).\nIn this study , we explored three further func-\ntions: GRU, GRU followed by ReLU ( de,i =\nReLU(GRU(d′\ne,i, de,i−1))) and a function that se-\nlects only the latest context, i.e., de,i = d′\ne,i. This\ncomparison clariﬁes the effect of the accumulation\nof contexts as the experiments proceeded\n2 .\n1 Equations 2 and 7 are identical but do not share internal\nparameters.\n2 Note that merging functions are not restricted to con-\nsidering two arguments (a new context and a merged past\nthe hottest gift [  1  ] could be [  2  ] , but good luck  \nfinding one . as [  3  ] reports , many stores have sold out of [  2  ] even … \nAnonymized Version \nThe hottest gift this Christmas  could be Sony’ s new PlayStation 2 , but good luck \nfinding one. As Greg Lefevre  reports, many stores have sold out of the game  even … \nOriginal Version \nFigure 3: An example document for Anonymized Language Model ing. T oken “[ k ]” is an anonymized\ntoken that appears k-th in the entities in a document. Langua ge models predict the next word from the\npreceding words, and calculate probabilities for whole wor d sequences.\nThe merging function produces the dynamic\nrepresentation de,i of e. In language modeling,\nto read the (i + 1)-th sentence, the model uses\ntwo dynamic word embeddings of e in the input\nand output layers. The input embedding xe, used\nto encode contexts (Equation 2), and the output\nembedding ye, used to predict the occurrence of e\n(Equation 1), are replaced with dynamic versions:\nxe = Wdxde,i + bx\ne , (9)\nye = Wdyde,i + by\ne , (10)\nwhere Wdx and Wdy denote learnable matrices,\nand bx\ne and by\ne denote learnable vectors tied to e.\nW e can observe that a conventional RNN language\nmodel is a variant that removes the dynamic terms\n(Wdxde,i and Wdyde,i) using only the static terms\n(bx\ne and by\ne ) to represent e. The initial dynamic rep-\nresentation de,0 is deﬁned as a zero vector, so that\nthe initial word embeddings ( xe and ye) are iden-\ntical to the static terms ( bx\ne and by\ne ) until the point\nat which the ﬁrst context of the target word e is ob-\nserved. All parameters in the end-to-end model are\nlearned entirely by backpropagation, maximizing\nthe log-likelihood in the same way as a conven-\ntional RNN language model.\nW e can view the approach in\nKobayashi et al.\n(2016) as a variant on the proposed method, but\nusing the dynamic terms only in the input layer\n(for xe). W e can also view the copy mecha-\nnism (\nGu et al. , 2016; Gulcehre et al. , 2016) as a\nvariant on the proposed method, in which speciﬁc\nembeddings in the output layer are replaced with\nspecial dynamic vectors.\ncontext) recurrently but can consider all vectors over the\nwhole history [d′\ne,1, ...,d′\ne,i] (e.g., by using attention mecha-\nnism (\nBahdanau et al. , 2015)). However, for simplicity , this\nresearch focuses only on the case of a function with two\narguments.\n4 Anonymized Language Modeling\nThis study explores methods for on-the-ﬂy cap-\nture and exploitation of the meanings of unknown\nwords or entities in a discourse. T o do this, we in-\ntroduce a novel evaluation task and dataset that we\ncalled Anonymized Language Modeling. Figure\n3\ngives an example from the dataset. Brieﬂy , the\ndataset anonymizes certain noun phrases, treating\nthem as unknown words and retaining their coref-\nerence relations. This allows a language model to\ntrack the context of every noun phrase in the dis-\ncourse. Other words are left unchanged, allowing\nthe language model to preserve the context of the\nanonymized (unknown) words, and to infer their\nmeanings from the known words. The process\nwas inspired by\nHermann et al. (2015), whose ap-\nproach has been explored by the research on read-\ning comprehension.\nMore precisely , we used the\nOntoNotes (\nPradhan et al. , 2012) corpus, which\nincludes documents with coreferences and named\nentity tags manually annotated. W e assigned an\nanonymous identiﬁer to every coreference chain\nin the corpus\n3 in order of ﬁrst appearance 4 , and\nreplaced mentions of a coreference chain with\nits identiﬁer. In our experiments, each corefer-\nence chain was given a dynamic representation.\nFollowing\nMikolov et al. (2010), we limited the\nvocabulary to 10,000 words appearing frequently\nin the corpus. Finally , we inserted “ <bos>” and\n“ <eos>” tokens to mark the beginning and end of\neach sentence.\nAn important difference between this dataset\nand the one presented in\nHermann et al. (2015)\nis in the way that coreferences are treated.\n3 W e used documents with no more than 50 clusters, which\ncovered more than 97% of the corpus.\n4 Following the study of Luong et al. (2015), we assigned\n“ <unk1>”, “ <unk2>”, ... to coreference clusters in order of\nﬁrst appearance.\nSplit T rain V alid T est\n# of documents 2725 335 336\nA vg. # of sentences 25.7 27.2 26.4\nA vg. # of unique entities 15.6 16.8 15.8\nA vg. # of unique entities oc-\ncurring more than once\n9.3 9.9 9.5\nA vg. # of occurrences of an\nentity\n3.2 3.2 3.1\nT able 1: Statistics of Anonymized Language Mod-\neling dataset.\nHermann et al. (2015) used automatic resolusion\nof coreferences, whereas our study made use of the\nmanual annotations in the OntoNotes. Thus, the\nprocess of\nHermann et al. (2015) introduced (in-\ntentional and unintentional) errors into the dataset.\nAdditionally , the dataset did not assign an entity\nidentiﬁer to a pronoun. In contrast, as our dataset\nhas access to the manual annotations of corefer-\nences, we are able to investigate the ability of the\nlanguage model to capture meanings from con-\ntexts.\nDynamic updating could be applied to words\nin all lexical categories, including verbs, adjec-\ntives, and nouns without requiring additional ex-\ntensions. However, verbs and adjectives were ex-\ncluded from targets of dynamic updates in the ex-\nperiments, for two reasons. First, proper nouns\nand nouns accounted for the majority (70%) of\nthe low-frequency (unknown) words, followed by\nverbs (10%) and adjectives (9%). Second, we\nassumed that the meaning of a verb or adjective\nwould shift less over the course of a discourse than\nthat of a noun. When semantic information of un-\nknown verbs and adjectives is required, their em-\nbeddings may be extracted from ad-hoc training\non a different larger corpus. This, however, was\nbeyond the scope of this study .\n5 Experiments\n5.1 Setting\nAn experiment was conducted to investigate the\neffect of Dynamic Neural T ext Model on the\nAnonymized Language Modeling dataset. The\nsplit of dataset followed that of the original cor-\npus (\nPradhan et al. , 2012). T able 1 summarizes the\nstatistics of the dataset.\nThe baseline model was a typical LSTM RNN\nlanguage model with 512 units. W e compared\nthree variants of the proposed model, using dif-\nferent applications of dynamic embedding: in the\ninput layer only (as in\nKobayashi et al. (2016)), in\nthe output layer only , and in both the input and\noutput layers. The context encoders were bidirec-\ntional LSTMs with 512 units, the parameters of\nwhich were not the same as those in the LSTM\nRNN language models. All models were trained\nby maximizing the likelihood of correct tokens, to\nachieve best perplexity on the validation dataset\n5 .\nMost hyper-parameters were tuned and ﬁxed by\nthe baseline model on the validation dataset\n6 .\nIt is difﬁcult to adequately train the all parts of a\nmodel using only the small dataset of Anonymized\nLanguage Modeling. W e therefore pretrained\nword embeddings and ContextEncoder (the bi-\ndirectional RNNs and matrices in Equations\n6–\n8) on a sentence completion task in which clozes\nwere predicted from the surrounding words in\na large corpus (\nMelamud et al. , 2016)7 . W e\nused the objective function with negative sam-\npling (\nMikolov et al. , 2013): ∑\ne(log σ(ˆx⊺\ne xe) +∑\nv∈Neg (log σ(−ˆx⊺\ne xv))). Here, ˆxe is a context\nvector predicted by ContextEncoder, xe denotes\nthe word embedding of a target word e appear-\ning in the corpus, and Neg represents randomly\nsampled words. These pretrained parameters of\nContextEncoder were ﬁxed when the whole lan-\nguage model was trained on the Anonymized Lan-\nguage Modeling dataset. W e implemented models\nin Python using the Chainer neural network li-\nbrary (\nT okui et al. , 2015). The code and the con-\nstructed dataset are publicly available 8 .\n5 W e performed a validation at the end of every half epoch\nout of ﬁve epochs.\n6 Batchsize was 8. Adam ( Kingma and Ba , 2015) with\nlearning rate 10−3. Gradients were normalized so that their\nnorm was smaller than 1. Truncation of backpropagation and\nupdating was performed after every 20 sentences and at the\nend of document.\n7 W e pretrained a model on the Gigaword Corpus, exclud-\ning sentences with more than 32 tokens. W e performed train-\ning for 50000 iterations with a batch size of 128 and ﬁve\nnegative samples. Only words that occurred no fewer than\n500 times are used; other words were treated as unknown\ntokens.\nMelamud et al. (2016) used three different sets of\nword embeddings for the two inputs with respect to the en-\ncoders ( −−−→\nRNN and ←−−−\nRNN) and the output (target). However,\nwe forced the sets of word embeddings to share a single set\nof word embeddings in pretraining. W e initialized the word\nembeddings in both the input layer ( xw) and the output layer\n(yw) of the novel models, including the baseline model, with\nthis single set. The word embeddings of all anonymized to-\nkens were initialized as unknown words with the word em-\nbedding of “ <unk>”.\n8\nhttps://github.com/soskek/dynamic_neural_text_model\nModels (1) All\n(2) Reappearing\nentities\n(3) Following\nentities (4) Non-entities\nLSTM LM (Baseline) (A) 64.8±0.6 48.0±2.6 128.6 ±2.0 68.5±0.2\nWith only dynamic input (B) 62.8±0.3 42.4±1.1 109.5 ±1.4 66.4±0.3\nWith only dynamic output (C) 62.5±0.3 35.9±3.7 129.0 ±0.7 69.5±0.3\nWith dynamic input & output (D) 60.7±0.2 34.0±1.3 106.8 ±0.6 67.6±0.04\nT able 2: Perplexities for each token group of models on the te st set of Anonymized Language Modeling\ndataset. All values are averages with standard errors, calc ulated respectively by three models (trained\nwith different random numbers). Dynamic models used GRU fol lowed by ReLU as the merging function.\n5.2 Results and Analysis\n5.2.1 Perplexity\nT able\n2 shows performance of the baseline model\nand the three variants of the proposed method in\nterms of perplexity . The table reports the mean\nand standard error of three perplexity values af-\nter training using three different randomly cho-\nsen initializations (we used the same convention\nthroughout this paper). Here, we discuss the pro-\nposed method using GRU followed by ReLU as\nthe merging function, as this achieved the best\nperplexity (see Section\n5.2.2 for a comparison of\nfunctions). W e also show perplexitiy values when\nevaluating words of speciﬁc categories: (1) all\nwords; (2) reappearing entity words; (3) words fol-\nlowing entities; and (4) non-entity words.\nAll variants of the proposed method outper-\nformed the baseline model. Focusing on the cat-\negories (2) and (3) highlights the roles of dynamic\nupdates of the input and output layers. Dynamic\nupdates of the input layer (B) had a larger im-\nprovement for predicting words following entities\n(3) than those of the output layer (C). In con-\ntrast, dynamic updates of the output layer (C) were\nquite effective for predicting reappearing entities\n(2) whereas those of the input layer (B) were not.\nThese facts conﬁrm that: dynamic updates of the\ninput layer help a model predict words following\nentities by supplying on-the-ﬂy context informa-\ntion; and those of the output layer are effective to\npredict entity words appearing multiple times.\nIn addition, dynamic updates of both the input\nand output layers (D) further improved the perfor-\nmance from those of either the output (C) or input\n(B) layer. Thus, the proposed dynamic output was\nshown to be compatible with dynamic input, and\nvice versa . These results demonstrated the posi-\ntive effect of capturing and exploiting the context-\nsensitive meanings of entities.\nIn order to examine whether dynamic updates of\n45 \n50 \n55 \n60 \n65 \n70 \n1-20 21-40 41-60 61-100 101-200 201-\nPerplexity \nt-th token \nBaseline Proposed \nFigure 4: Perplexity of all tokens relative to the\ntime at which they appear in the document.\nthe input and output embeddings capture context-\nsensitive meanings of entities, we present Fig-\nures\n4, 5 and 6. Figure 4 depicts the perplexity\nof words with different positions in a document 9 .\nThe ﬁgure conﬁrms that the advantage of the pro-\nposed method over the baseline is more evident\nespecially in the latter part of documents, where\nrepeated words are more likely to occur.\nFigure\n5 shows the perplexity with respect to\nthe frequency of words t within documents. Note\nthat the word embedding at the ﬁrst occurrence of\nan entity is static. This ﬁgure indicates that en-\ntities appearing many times enjoy the beneﬁt of\nthe dynamic language model. Figure\n6 visualizes\nthe perplexity of entities with respect to the num-\nbers of their antecedent candidates. It is clear from\nthis ﬁgure that the proposed method is better at\nmemorizing the semantic information of entities\nappearing repeatedly in documents than the base-\nline. These results also demonstrated the contribu-\ntion of dynamic updates of word embeddings.\n9 It is more difﬁcult to predict tokens appearing latter in\na document because the number of new (unknown) tokens\nincreases as a model reads the document.\nModels Merging function\n# of parameters\n(to be ﬁnetuned) (1) All\n(2) Reappearing\nentities\n(3) Following\nentities (4) Non-entities\nOnly GRU-ReLU 18.9M (14.2M) 62.8±0.3 42.4±1.1 109.5 ±1.4 66.4±0.3\ndynamic input GRU 18.9M (14.2M) 63.2±0.4 43.3±2.7 111.2 ±0.7 66.8±0.4\nMax pool. 17.3M (12.6M) 63.6±0.4 45.0±2.6 116.0 ±1.0 67.0±0.2\nOnly latest 17.3M (12.6M) 64.0±0.4 44.1±1.6 127.6 ±0.7 67.5±0.2\nOnly GRU-ReLU 18.9M (14.2M) 62.5±0.3 35.9±3.7 129.0±0.7 69.5±0.3\ndynamic output GRU 18.9M (14.2M) 62.6±0.2 39.0±2.0 121.1±8.3 69.1±0.2\nMax pool. 17.3M (12.6M) 62.2±0.4 41.1±1.9 126.9 ±1.5 68.4±0.6\nOnly latest 17.3M (12.6M) 64.9±0.1 49.8±1.8 129.1 ±1.6 70.6±0.2\nDynamic GRU-ReLU 19.2M (14.4M) 60.7±0.2 34.0±1.3 106.8 ±0.6 67.6±0.04\ninput & output GRU 19.2M (14.4M) 60.9±0.3 37.5±0.3 108.9 ±0.8 67.2±0.4\nMax pool. 17.6M (12.9M) 60.7±0.3 39.5±3.4 107.5 ±1.3 66.8±0.8\nOnly latest 17.6M (12.9M) 63.4±0.2 47.9±4.2 116.4 ±0.4 68.9±0.1\nBaseline 12.3M (12.3M) 64.8±0.6 48.0±2.6 128.6 ±2.0 68.5±0.2\nT able 3: Results for models with different merging function s on the test set of the Anonymized Language\nModeling dataset, as same as in T able 2.\n60 \n70 \n80 \n90 \n100 \n110 \n120 \n130 \n140 \n150 \n160 \n1 2 3-6 7-10 11-\nPerplexity of tokens \nfollowing entities \nt-th occurrence of entities \nBaseline Proposed \nFigure 5: Perplexity of tokens following the enti-\nties relative to the time at which the entity occurs.\n5.2.2 Comparison of Merging functions\nT able\n3 compares models with different merging\nfunctions; GRU-ReLU, GRU, max pooling, and\nthe use of the latest context. The use of the lat-\nest context had the worst performance for all vari-\nants of the proposed method. Thus, a proper accu-\nmulation of multiple contexts is indispensable for\ndynamic updates of word embeddings. Although\nKobayashi et al. (2016) used only max pooling as\nthe merging function, GRU and GRU-ReLU were\nshown to be comparable in performance and supe-\nrior to max pooling when predicting tokens related\nto entities (2) and (3).\n5.2.3 Predicting Entities by Likelihood of a\nSentence\nIn order to examine contribution of the dynamic\nlanguage models on a downstream task, we con-\nducted cloze tests for comprehension of a sentence\nwith reappearing entities in a discourse. Given\nmultiple preceding entities E = {e+, e1, e2, ...}\n0\n10 \n20 \n30 \n40 \n50 \n60 \n70 \n80 \n90 \n100 \n0 1 2 3 4-6 7-10 11-20 21-\nPerplexity of entities \n# of antecedent entities \nBaseline Proposed \nFigure 6: Perplexity of entities relative to the num-\nber of antecedent entities.\nfollowed by a cloze sentence, the models were re-\nquired to predict the true antecedent e+ which al-\nlowed the cloze to be correctly ﬁlled, among the\nother alternatives E− = {e1, e2, ...}.\nLanguage models solve this task by comparing\nthe likelihoods of sentences ﬁlled with antecedent\ncandidates in E and returning the entity with the\nhighest likelihood of the sentence. In this experi-\nment, the performance of a model was represented\nby the Mean Quantile (MQ) (\nGuu et al. , 2015).\nThe MQ computes the mean ratio at which the\nmodel predicts a correct antecedent e+ more likely\nthan negative antecedents in E−,\nMQ = |{e− ∈E− : p(e−) < p (e+)}|\n|E−| . (11)\nHere, p(e) denotes the likelihood of a sentence\nwhose cloze is ﬁlled with e. If the correct an-\ntecedent e+ yields highest likelihood, MQ gets 1.\nT able 4 reports MQs for the three variants and\nmerging functions. Dynamic updates of the in-\nModels Merging func. MQ\nBaseline .525±.001\nOnly GRU-ReLU .630±.005\ndynamic input GRU .633±.005\nMax pool. .617±.002\nOnly latest .600±.004\nOnly GRU-ReLU .519±.001\ndynamic output GRU .522±.000\nMax pool. .519±.001\nOnly latest .519±.003\nDynamic GRU-ReLU .642±.004\ninput & output GRU .637±.005\nMax pool. .620±.002\nOnly latest .613±.002\nT able 4: Mean Quantile of a true coreferent entity\namong antecedent entities.\nput layer greatly boosted the performance by ap-\nproximately 10%, while using both dynamic in-\nput and output improved it further. In this ex-\nperiment, the merging functions with GRUs out-\nperform the others. These results demonstrated\nthat Dynamic Neural T ext Models can accumulate\na new information in word embeddings and con-\ntribute to modeling the semantic changes of enti-\nties in a discourse.\n6 Related W ork\nAn approach to addressing the unknown word\nproblem used in recent studies (\nKim et al. ,\n2016; Sennrich et al. , 2016; Luong and Manning ,\n2016; Schuster and Nakajima , 2012) comprises\nthe embeddings of unknown words from char-\nacter embeddings or subword embeddings.\nLi and Jurafsky (2015) applied word disambigua-\ntion and use a sense embedding to the target\nword.\nChoi et al. (2017) captured the context-\nsensitive meanings of common words using word\nembeddings, applied through a gating function\ncontrolled by history words, in the context of\nmachine translation. In future work, we will\nexplore a wider range of models, to integrate our\ndynamic text modeling with methods that estimate\nthe meaning of unknown words or entities from\ntheir constituents. When addressing well-known\nentities such as Obama and Trump, it makes sense\nto learn their embeddings from external resources,\nas well as dynamically from the preceding context\nin a given discourse (as in our Dynamic Neural\nT ext Model). The integration of these two sources\nof information is an intriguing challenge in\nlanguage modeling.\nA key aspect of our model is its incorpo-\nration of the copy mechanism (\nGu et al. , 2016;\nGulcehre et al. , 2016), using dynamic word em-\nbeddings in the output layer. Independently of\nthis study , several research groups have explored\nthe use of variants of the copy mechanisms in lan-\nguage modeling (\nMerity et al. , 2017; Grave et al. ,\n2017; Peng and Roth , 2016). These studies, how-\never, did not incorporate dynamic representations\nin the input layer. In contrast, our proposal in-\ncorporates the copy mechanism through the use\nof dynamic representations in the output layer, in-\ntegrating them with dynamic mechanisms in both\nthe input and output layers by applying dynamic\nentity-wise representation. Our experiments have\ndemonstrated the beneﬁts of such integration.\nAnother related trend in recent studies is\nthe use of neural network to capture the in-\nformation ﬂow of a discourse. One ap-\nproach has been to link RNNs across sen-\ntences (\nW ang and Cho , 2016; Serban et al. , 2016),\nwhile a second approach has expolited a type\nof memory space to store contextual informa-\ntion (\nSukhbaatar et al. , 2015; Tran et al. , 2016;\nMerity et al. , 2017). Research on reading com-\nprehension ( Kobayashi et al. , 2016; Henaff et al. ,\n2017) and coreference resolution ( Wiseman et al. ,\n2016; Clark and Manning , 2016b,a) has shown the\nsalience of entity-wise context information. Our\nmodel could be located within such approaches,\nbut is distinct in being the ﬁrst model to make use\nof entity-wise context information in both the in-\nput and output layers for sentence generation.\nW e summarize and compare works for entity-\ncentric neural networks that read a document.\nKobayashi et al. (2016) pioneered entity-centric\nneural models tracking states in a discourse.\nThey proposed Dynamic Entity Representation ,\nwhich encodes contexts of entities and updates the\nstates using entity-wise memories.\nWiseman et al.\n(2016) also proposed a method for manag-\ning similar entity-wise features on neural net-\nworks and improved a coreference resolution\nmodel.\nClark and Manning (2016b,a) incorpo-\nrated such entity-wise representations in mention-\nranking coreference models. Our paper follows\nKobayashi et al. (2016) and exploits dynamic en-\ntity reprensetions in a neural language model,\nwhere dynamic reporesentations are used not only\nin the neural encoder but also in the decoder,\napplicable to various sequence generation tasks,\ne.g., machine translation and dialog response gen-\neration. Simultaneously with our paper,\nJi et al.\n(2017) use dynamic entity representation in a neu-\nral language model for reranking outputs of a\ncoreference resolution system.\nY ang et al. (2017)\nexperiment language modeling with referring to\ninternal contexts or external data.\nHenaff et al.\n(2017) focus on neural networks tracking con-\ntexts of entities, achieving the state-of-the-art re-\nsult in bAbI (\nW eston et al. , 2015), a reading com-\nprehension task. They encode the contexts of each\nentity by an attention-like gated RNN instead of\nusing coreference links directly .\nDhingra et al.\n(2017) also try to improve a reading comprehen-\nsion model using coreference links. Similarly to\nour dynamic entity representation,\nBahdanau et al.\n(2017) construct on-the-ﬂy word embeddings of\nrare words from dictionary deﬁnitions.\nThe ﬁrst key component of dynamic en-\ntity representation is a function to merge\nmore than one contexts about an entity into a\nconsistent representation of the entity . V ari-\nous choices for the function exist, e.g., max\nor average-pooling (\nKobayashi et al. , 2016;\nClark and Manning , 2016b), RNN (GRU,\nLSTM ( Wiseman et al. , 2016; Y ang et al. , 2017)\nor other gated RNNs ( Henaff et al. , 2017; Ji et al. ,\n2017)), or using the latest context only (without\nany merging) ( Y ang et al. , 2017). This paper is the\nﬁrst work comparing the effects of those choices\n(see Section\n5.2.2).\nThe second component is a function to encode\nlocal contexts from a given text, e.g., bidirectional\nRNN encoding (\nKobayashi et al. , 2016), unidirec-\ntional RNN used in a language model ( Ji et al. ,\n2017; Y ang et al. , 2017), feedforward neural net-\nwork with a sentence vector and an entity’s\nword vector (\nHenaff et al. , 2017) or hand-crafted\nfeatures with word embeddings ( Wiseman et al. ,\n2016; Clark and Manning , 2016b). This study\nemploys bi-RNN analogously to Kobayashi et al.\n(2016), which can access full context with power-\nful learnable units.\nIn the task setting proposed in this study , a\nmodel must capture the meaning of a given spe-\nciﬁc word from a small number of its contexts in\na given discourse. The task could also be seen\nas novel one-shot learning (\nFei-Fei et al. , 2006)\nof word meanings. One-shot learning for NLP\nlike this has been little studied, with the excep-\ntion of the study by\nVinyals et al. (2016), which\nused a task in which the context of a target word is\nmatched with a different context of the same word.\n7 Conclusion\nThis study addressed the problem of identify-\ning the meaning of unknown words or entities\nin a discourse with respect to the word embed-\nding approaches used in neural language mod-\nels. W e proposed a method for on-the-ﬂy con-\nstruction and exploitation of word embeddings in\nboth the input layer and output layer of a neu-\nral model by tracking contexts. This extended\nthe dynamic entity representation presented in\nKobayashi et al. (2016), and incorporated a copy\nmechanism proposed independently by Gu et al.\n(2016) and Gulcehre et al. (2016). In the course\nof the study , we also constructed a new task and\ndataset, called Anonymized Language Modeling ,\nfor evaluating the ability of a model to capture\nword meanings while reading. Experiments con-\nducted using our novel dataset demonstrated that\nthe RNN language model variants proposed in this\nstudy outperformed the baseline model. More de-\ntailed analysis indicated that the proposed method\nwas particularly successful in capturing the mean-\ning of an unknown words from texts containing\nfew instances.\nAcknowledgments\nThis work was supported by JSPS KAKENHI\nGrant Number 15H01702 and JSPS KAKENHI\nGrant Number 15H05318. W e thank members\nof Preferred Networks, Inc., Makoto Miwa and\nDaichi Mochihashi for suggestive discussions.\nReferences\nDzmitry Bahdanau, T om Bosc, Stanisław Jastrz ˛ ebski,\nEdward Grefenstette, Pascal V incent, and Y oshua\nBengio. 2017. Learning to compute word embed-\ndings on the ﬂy. arXiv preprint arXiv:1706.00286.\nDzmitry Bahdanau, Kyunghyun Cho, and Y oshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In Proceedings of\nICLR.\nY oshua Bengio, Réjean Ducharme, Pascal V incent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. JOURNAL OF MACHINE LEARN-\nING RESEARCH, 3:1137–1155.\nCiprian Chelba, T omas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and T ony Robin-\nson. 2014. One billion word benchmark for mea-\nsuring progress in statistical language modeling. In\nProceedings of INTERSPEECH, pages 2635–2639.\nKyunghyun Cho, Bart van Merrienboer, Çaglar\nGülçehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\nger Schwenk, and Y oshua Bengio. 2014. Learning\nphrase representations using RNN encoder-decoder\nfor statistical machine translation. In Proceedings of\nEMNLP.\nHeeyoul Choi, Kyunghyun Cho, and Y oshua Bengio.\n2017. Context-dependent word representation for\nneural machine translation. Computer Speech &\nLanguage, 45:149–160.\nKevin Clark and Christopher D. Manning. 2016a.\nDeep reinforcement learning for mention-ranking\ncoreference models. In Proceedings of EMNLP ,\npages 2256–2262.\nKevin Clark and Christopher D. Manning. 2016b. Im-\nproving coreference resolution by learning entity-\nlevel distributed representations. In Proceedings of\nACL, pages 643–653.\nY ann N. Dauphin, Angela Fan, Michael\nAuli, and David Grangier. 2016.\nLanguage Modeling with Gated Convolutional Networks .\narXiv preprint arXiv:1612.08083.\nBhuwan Dhingra, Zhilin Y ang, William W Cohen, and\nRuslan Salakhutdinov. 2017. Linguistic knowledge\nas memory for recurrent neural networks. arXiv\npreprint arXiv:1703.02620.\nLi Fei-Fei, Rob Fergus, and Pietro Perona. 2006. One-\nshot learning of object categories. IEEE transac-\ntions on TP AMI, 28(4):594–611.\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2017. Improving neural language models with a\ncontinuous cache. In Proceedings of ICLR.\nJiatao Gu, Zhengdong Lu, Hang Li, and O.K. V ic-\ntor Li. 2016. Incorporating copying mechanism in\nsequence-to-sequence learning. In Proceedings of\nACL, pages 1631–1640.\nCaglar Gulcehre, Sungjin Ahn, Ramesh Nallapati,\nBowen Zhou, and Y oshua Bengio. 2016. Pointing\nthe unknown words. In Proceedings of ACL, pages\n140–149.\nKelvin Guu, John Miller, and Percy Liang. 2015.\nTraversing knowledge graphs in vector space. In\nProceedings of EMNLP, pages 318–327.\nMikael Henaff, Jason W eston, Arthur Szlam, Antoine\nBordes, and Y ann LeCun. 2017. Tracking the world\nstate with recurrent entity networks. In Proceedings\nof ICLR.\nKarl Moritz Hermann, T omas Kocisky, Edward\nGrefenstette, Lasse Espeholt, Will Kay, Mustafa Su-\nleyman, and Phil Blunsom. 2015. T eaching ma-\nchines to read and comprehend. In Proceedings of\nNIPS, pages 1684–1692.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nY angfeng Ji, Chenhao T an, Sebastian Martschat, Y ejin\nChoi, and Noah A. Smith. 2017. Dynamic entity\nrepresentations in neural language models. In Pro-\nceedings of EMNLP.\nRafal Jozefowicz, Oriol V inyals, Mike Schuster, Noam\nShazeer, and Y onghui Wu. 2016. Exploring\nthe limits of language modeling. arXiv preprint\narXiv:1602.02410 .\nY oon Kim, Y acine Jernite, David Sontag, and Alexan-\nder M. Rush. 2016. Character-aware neural lan-\nguage models. In Proceedings of AAAI, pages 2741–\n2749.\nDiederik Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proceedings\nof ICLR.\nSosuke Kobayashi, Ran Tian, Naoaki Okazaki, and\nKentaro Inui. 2016. Dynamic entity representation\nwith max-pooling improves machine reading. In\nProceedings of NAACL-HLT, pages 850–855.\nJiwei Li and Dan Jurafsky. 2015. Do multi-sense em-\nbeddings improve natural language understanding?\nIn Proceedings of EMNLP, pages 1722–1732.\nMinh-Thang Luong and D. Christopher Manning.\n2016. Achieving open vocabulary neural machine\ntranslation with hybrid word-character models. In\nProceedings of ACL, pages 1054–1063.\nThang Luong, Ilya Sutskever, Quoc Le, Oriol V inyals,\nand W ojciech Zaremba. 2015. Addressing the rare\nword problem in neural machine translation. In Pro-\nceedings of ACL, pages 11–19.\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional lstm. In Proceedings of\nCoNLL, pages 51–61.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture\nmodels. In Proceedings of ICLR.\nT omáš Mikolov, Martin Karaﬁát, Lukáš Burget, Jan\nˇCernocký, and Sanjeev Khudanpur. 2010. Recurrent\nneural network based language model. In Proceed-\nings of INTERSPEECH, pages 1045–1048.\nT omas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.\nCorrado, and Jeffrey Dean. 2013. Distributed repre-\nsentations of words and phrases and their composi-\ntionality. In Proceedings of NIPS, pages 3111–3119.\nV inod Nair and Geoffrey E. Hinton. 2010. Recti-\nﬁed linear units improve restricted boltzmann ma-\nchines. In Proceedings of ICML , pages 807–814.\nOmnipress.\nHaoruo Peng and Dan Roth. 2016. T wo discourse\ndriven language models for semantics. In Proceed-\nings of ACL, pages 290–300.\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,\nOlga Uryupina, and Y uchen Zhang. 2012. CoNLL-\n2012 shared task: Modeling multilingual unre-\nstricted coreference in OntoNotes. In Proceedings\nof CoNLL.\nMike Schuster and Kaisuke Nakajima. 2012. Japanese\nand korean voice search. In Proceedings of ICASSP,\npages 5149–5152.\nRico Sennrich, Barry Haddow , and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of ACL, pages 1715–\n1725.\nIulian V . Serban, Alessandro Sordoni, Y oshua Bengio,\nAaron Courville, and Joelle Pineau. 2016. Building\nend-to-end dialogue systems using generative hier-\narchical neural network models. In Proceedings of\nAAAI, pages 3776–3783.\nSainbayar Sukhbaatar, Arthur Szlam, Jason W eston,\nand Rob Fergus. 2015. End-to-end memory net-\nworks. In Proceedings of NIPS, pages 2440–2448.\nSeiya T okui, Kenta Oono, Shohei Hido, and Justin\nClayton. 2015. Chainer: a next-generation open\nsource framework for deep learning. In Proceedings\nof W orkshop on LearningSys in NIPS 28.\nKe Tran, Arianna Bisazza, and Christof Monz. 2016.\nRecurrent memory networks for language modeling.\nIn Proceedings of NAACL-HLT, pages 321–331.\nOriol V inyals, Charles Blundell, Tim Lillicrap, koray\nkavukcuoglu, and Daan Wierstra. 2016. Matching\nnetworks for one shot learning. In Proceedings of\nNIPS, pages 3630–3638.\nTian W ang and Kyunghyun Cho. 2016. Larger-context\nlanguage modelling with recurrent neural network.\nIn Proceedings of ACL, pages 1319–1329.\nJason W eston, Antoine Bordes, Sumit Chopra, and\nT omas Mikolov. 2015. T owards ai-complete ques-\ntion answering: A set of prerequisite toy tasks.\narXiv preprint arXiv:1502.05698.\nSam Wiseman, Alexander M. Rush, and Stuart M.\nShieber. 2016. Learning global features for coref-\nerence resolution. In Proceedings of NAACL-HLT,\npages 994–1004.\nZichao Y ang, Phil Blunsom, Chris Dyer, and W ang\nLing. 2017. Reference-aware language models. In\nProceedings of EMNLP.",
  "topic": "Linguistics",
  "concepts": [
    {
      "name": "Linguistics",
      "score": 0.620702862739563
    },
    {
      "name": "Computer science",
      "score": 0.5789362192153931
    },
    {
      "name": "Natural language processing",
      "score": 0.45930254459381104
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4034927189350128
    },
    {
      "name": "Communication",
      "score": 0.32792407274246216
    },
    {
      "name": "Psychology",
      "score": 0.26501691341400146
    },
    {
      "name": "Philosophy",
      "score": 0.14176377654075623
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210166566",
      "name": "Preferred Networks (Japan)",
      "country": "JP"
    }
  ],
  "cited_by": 8
}