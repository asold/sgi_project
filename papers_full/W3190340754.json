{
  "title": "TriTransNet: RGB-D Salient Object Detection with a Triplet Transformer Embedding Network",
  "url": "https://openalex.org/W3190340754",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2225022779",
      "name": "Liu Zheng-yi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1973821393",
      "name": "Wang Yuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2710043692",
      "name": "Tu, Zhengzheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2094569011",
      "name": "Xiao Yun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2028713953",
      "name": "Tang Bin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3204563069",
    "https://openalex.org/W1982075130",
    "https://openalex.org/W2804610335",
    "https://openalex.org/W1966025376",
    "https://openalex.org/W3093213431",
    "https://openalex.org/W2957414648",
    "https://openalex.org/W2990201021",
    "https://openalex.org/W3134912427",
    "https://openalex.org/W2100470808",
    "https://openalex.org/W2950635152",
    "https://openalex.org/W3104979525",
    "https://openalex.org/W2963868681",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3035633116",
    "https://openalex.org/W2948300571",
    "https://openalex.org/W3153465022",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3035357085",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W2765934933",
    "https://openalex.org/W3049194477",
    "https://openalex.org/W20683899",
    "https://openalex.org/W2964738399",
    "https://openalex.org/W2999281281",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3048216881",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3035687312",
    "https://openalex.org/W3101839051",
    "https://openalex.org/W3135874576",
    "https://openalex.org/W3042173136",
    "https://openalex.org/W1993713494",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W3130695101",
    "https://openalex.org/W2121378474",
    "https://openalex.org/W3140528754",
    "https://openalex.org/W3108421143",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2998449272",
    "https://openalex.org/W2798857366",
    "https://openalex.org/W3097336090",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W3108812909",
    "https://openalex.org/W3121654355",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3150134882",
    "https://openalex.org/W3136958399",
    "https://openalex.org/W3006465601",
    "https://openalex.org/W2607011617",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W2932937602",
    "https://openalex.org/W2963897031",
    "https://openalex.org/W3010616503",
    "https://openalex.org/W2963112696",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3038084395",
    "https://openalex.org/W3141497777",
    "https://openalex.org/W3130071011",
    "https://openalex.org/W3109067620",
    "https://openalex.org/W3119686997",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2546160422",
    "https://openalex.org/W3097053213",
    "https://openalex.org/W2014328297",
    "https://openalex.org/W2907643346",
    "https://openalex.org/W3035284915",
    "https://openalex.org/W3139517252",
    "https://openalex.org/W3120113457",
    "https://openalex.org/W3173882198",
    "https://openalex.org/W2804743778",
    "https://openalex.org/W3108822985",
    "https://openalex.org/W1854404533",
    "https://openalex.org/W3033210410",
    "https://openalex.org/W3139633126",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W1976409045",
    "https://openalex.org/W3159778524",
    "https://openalex.org/W3034320133",
    "https://openalex.org/W3114848016",
    "https://openalex.org/W3094802760",
    "https://openalex.org/W3186564193",
    "https://openalex.org/W3143016713",
    "https://openalex.org/W3150177490",
    "https://openalex.org/W3108608656",
    "https://openalex.org/W3214586131",
    "https://openalex.org/W3134689216",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2166184839"
  ],
  "abstract": "Salient object detection is the pixel-level dense prediction task which can highlight the prominent object in the scene. Recently U-Net framework is widely used, and continuous convolution and pooling operations generate multi-level features which are complementary with each other. In view of the more contribution of high-level features for the performance, we propose a triplet transformer embedding module to enhance them by learning long-range dependencies across layers. It is the first to use three transformer encoders with shared weights to enhance multi-level features. By further designing scale adjustment module to process the input, devising three-stream decoder to process the output and attaching depth features to color features for the multi-modal fusion, the proposed triplet transformer embedding network (TriTransNet) achieves the state-of-the-art performance in RGB-D salient object detection, and pushes the performance to a new level. Experimental results demonstrate the effectiveness of the proposed modules and the competition of TriTransNet.",
  "full_text": "TriTransNet: RGB-D Salient Object Detection with a Triplet\nTransformer Embedding Network\nZhengyi Liuâˆ—\nSchool of Computer Science and\nTechnology, Anhui University\nHefei, China\nliuzywen@ahu.edu.cn\nYuan Wang\nSchool of Computer Science and\nTechnology, Anhui University\nHefei, China\nwangyuan.ahu@qq.com\nZhengzheng Tu\nSchool of Computer Science and\nTechnology, Anhui University\nHefei, China\n15352718@qq.com\nYun Xiao\nSchool of Computer Science and\nTechnology, Anhui University\nHefei, China\n280240406@qq.com\nBin Tang\nSchool of Artificial Intelligence and\nBig Data, Hefei University\nHefei, China\n424539820@qq.com\nABSTRACT\nSalient object detection is the pixel-level dense prediction task\nwhich can highlight the prominent object in the scene. Recently\nU-Net framework is widely used, and continuous convolution and\npooling operations generate multi-level features which are com-\nplementary with each other. In view of the more contribution of\nhigh-level features for the performance, we propose a triplet trans-\nformer embedding module to enhance them by learning long-range\ndependencies across layers. It is the first to use three transformer\nencoders with shared weights to enhance multi-level features. By\nfurther designing scale adjustment module to process the input,\ndevising three-stream decoder to process the output and attaching\ndepth features to color features for the multi-modal fusion, the\nproposed triplet transformer embedding network (TriTransNet)\nachieves the state-of-the-art performance in RGB-D salient object\ndetection, and pushes the performance to a new level. Experimental\nresults demonstrate the effectiveness of the proposed modules and\nthe competition of TriTransNet.1\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Interest point and salient re-\ngion detections.\nKEYWORDS\nsalient object detection; RGB-D image; transformer; shared weights;\nself-attention\nâˆ—Corresponding author.\n1The code is available at https://github.com/liuzywen/TriTransNet.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nMM â€™21, October 20â€“24, 2021, Virtual Event, China\nÂ© 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-8651-7/21/10. . . $15.00\nhttps://doi.org/10.1145/3474085.3475601\nACM Reference Format:\nZhengyi Liu, Yuan Wang, Zhengzheng Tu, Yun Xiao, and Bin Tang. 2021.\nTriTransNet: RGB-D Salient Object Detection with a Triplet Transformer\nEmbedding Network. InProceedings of the 29th ACM International Conference\non Multimedia (MM â€™21), October 20â€“24, 2021, Virtual Event, China. ACM,\nNew York, NY, USA, 10 pages. https://doi.org/10.1145/3474085.3475601\n1 INTRODUCTION\nSalient object detection (SOD) simulates the visual attention mech-\nanism to capture the prominent object in the scene. It has been\nwidely applied in the computer vision tasks, such as image segmen-\ntation [18], tracking [30, 47, 83], retrieval [25], compression [32],\nedit [65] and quality assessment [34].\nAs a pixel-level dense prediction task, salient object detection\nusually uses CNN based U-Net framework[58] (Fig. 1(a)) to encode\nimages from low-level to high-level, and then decode back to the\nfull spatial resolution. Research[74] points out that the performance\ntends to saturate quickly when gradually aggregating features from\nhigh-level to low-level. In other words, high-level features con-\ntribute more to the performance. Therefore, we propose a triplet\ntransformer embedding module (TTEM) to enhance the feature\nrepresentation of high three layers.\nAs we all known, Transformer[62] has recently attracted a lot\nof attention in computer vision domain, but it is also encounter-\ning high computational cost problem. PVT[ 66] adopts a spatial-\nreduction attention (SRA) layer to reduce the resource cost to learn\nhigh-resolution feature maps. CvT[72] introduces convolutional\ninto the Vision Transformer architecture to concurrently main-\ntain a high degree of computational and memory efficiency. Swin\nTransformer[44] uses the shifted windows calculation method to\npropose a hierarchical Transformer, which has the flexibility of\nmodelling at various scales and has linear computational complex-\nity relative to the image size. Multi-Scale Vision Longformer[82]\nproposes multi-scale coding structure, and further improves its\nattention mechanism to reduce the computational and memory\ncost.\nUnlike these profound designs, we introduce Transformer into\nU-Net framework to enhance the features of high three layers,\nwhich can be easily integrated into existing U-Net framework for\nsignificant improvement with less cost. The features of high three\narXiv:2108.03990v1  [cs.CV]  9 Aug 2021\nlayers show the different attributions but the same in nature, which\nare the different aspects of the same input image. The proposed\ntriplet transformer embedding module (TTEM) is composed of\nthree standard transformer encoders[62] with shared weights. It\nis beneficial to find the common information which is hidden in\nthe multi-level features and achieve the better fusion by learning\nlong-range dependencies across levels.\n(a) U-Net framework\n(b) visual-transformer-FPN (VT-FPN)\n(c) Our proposed triplet transformer embedding network\nFigure 1: Comparison between U-Net framework, VT-FPN\nand our proposed network.\nTaking TTME as the core, we further propose the triplet trans-\nformer embedding network (TriTransNet). At first, multi-level fea-\ntures are adjusted to the same size by a transition layer and pro-\ngressively upsampling fusion module. Second, features are fed into\nTTME to be enhanced. Last, the output features of TTME and the\nfeatures of low two layers are effectively fused by a three-stream\ndecoder.\nOur proposed TriTransNet is the first attempt to use three stan-\ndard transformer encoders with shared weights to enhance the\nfeature representation. Different from visual-transformer-FPN (VT-\nFPN)[71] in Fig. 1(b) which merges visual tokens from feature map\nof each layer with one transformer, our TriTransNet in Fig. 1(c)\nadopts weight sharing strategy to make visual tokens extracted\nfrom multi-level features more abundant enough to express the\noriginal information, and meanwhile high-layer semantic informa-\ntion and middle-layer texture or shape information are both better\nexcavated by parallel self-attention mechanism.\nIn addition, depth information is proved to supply the useful\ncues and boost the performance for saliency detection [ 51], es-\npecially in some challenging and complex scenarios, e.g. the low\ncolor contrasts between salient objects and background, the clut-\ntered background interferences. But depth image with poor quality,\nwhich likes a noise, brings some negative influences [22]. Following\ndepth guided manners [11, 12, 23, 52, 57, 60, 79, 89, 93] we design\ndepth purification module, which uses depth information to purify\nthe color features.\nOur main contributions can be summarized as follows:\nâ€¢A triplet transformer embedding module is proposed and\nembedded into CNN based U-Net framework to enhance the\nfeature representation. It is composed of three standard trans-\nformer encoders with shared weights, learning the common\ninformation from multi-level features.\nâ€¢Based on the proposed triplet transformer embedding mod-\nule, triplet transformer embedding network is designed to\ndetect the salient objects in RGB-D image. Multi-level fea-\ntures from encoder need to be adjusted to the same size by a\ntransition layer and progressively upsampling fusion mod-\nule, and then fed into triplet transformer embedding module.\nThen the output of triplet transformer embedding module\nneed to be combined with the features of low two layers by\nthree-stream decoder to achieve the decoding process.\nâ€¢Depth image is viewed as the supplement to color feature,\nand attached to color feature to enhance the feature repre-\nsentation by depth purification module which introduces\nspatial attention and channel attention.\nâ€¢Due to the advantage of the proposed triplet transformer\nembedding module, the proposed model pushes the perfor-\nmance of RGB-D salient object detection to a new level and\nshows the state-of-the-art performance on several public\ndatasets.\n2 RELATED WORK\n2.1 RGB-D saliency detection\nIn RGB-D image, color image provides appearance and texture infor-\nmation, and depth image contains 3D layout and spatial structure.\nThe fusion of color feature and depth feature is always an important\nissue in RGB-D saliency detection. References[10, 45, 85, 90] use\nearly fusion or input fusion, references[ 7, 11, 38, 39, 41] employ\ntwo-stream subnetwork to achieve the middle fusion, references[53,\n57, 73, 89, 93] apply depth guided fusion and references [14, 46, 64]\nadopt late fusion.\nAlthough depth information can supply the useful cues for\nsaliency detection [51], depth image with poor quality can bring\nsome negative influences too [ 22]. In order to solve the filtering\nissue of low-quality depth map, D3Net [22] uses gate mechanism to\nfilter the poor depth map, EF-Net [9] enhances the depth maps by\ncolor hint map, DQSD [5] integrates a depth quality aware subnet-\nwork into the classic bi-stream structure, assigning the weight of\ndepth feature before conducting the fusion. In addition, CoNet[33],\nDASNet [88], SSDP[68] and MobileSal [73] introduce depth estima-\ntion, learning to detect the salient object simultaneously.\nIn the paper, we adopt depth guided manner. Depth information\nis viewed as the supplement to the color feature. It enhances the\ncolor feature by attention mechanism.\n2.2 Transformer\nTransformer is first proposed by[ 62] to replace recurrent neural\nnetworks (RNN), e.g.long short-term memory (LSTM) and gated\nrecurrent unit(GRU) for machine translation tasks. It can over-\ncome intrinsic shortages of RNN and has dominated nature lan-\nguage processing (NLP) field and are becoming increasingly pop-\nular in computer vision tasks, e.g. image classification[19], object\ndetection[4], semantic segmentation[? ], line segment[77], person\nre-identification[94], action detection[87], image completion[91],\n3D point cloud processing[26, 86], pose estimation[59], facial ex-\npression recognition[48], object tracking[49] etc. DETR[4] takes\nthe lead in applying Transformer to the field of object detection\nand achieves the better performance. The successful use of ViT[19]\nin image classification tasks has made the research on visual Trans-\nformer a hot topics. SETR[92] deploys a pure Transformer as the\nencoder, combined with a simple decoder, to achieve a powerful\nsemantic segmentation model. Besides, TransUNet[8] uses the pre-\ntrained ViT[19] as a powerful backbone of the U-Net[58] network\nstructure, and performs well in the field of medical image segmen-\ntation.\nHowever, pure transformer has great limitations. As a result,\nmany improved visual transformers have emerged. The Conditional\nPosition encodings Visual Transformer (CPVT)[ 17] replaces the\nfixed position encoding in ViT[19] with the proposed conditional\nposition encoding (CPE), which makes it possible for Transformer to\nprocess inputs of arbitrary sizes. Tokens-to-Token (T2T)[78] adopts\na novel progressive tetanization mechanism, which models local\nstructural information by aggregating adjacent tokens into one\ntoken, while reducing the length of the token. LocalViT[42] adds lo-\ncality to vision transformers by introducing depth-wise convolution\ninto the feed-forward network, improving a locality mechanism\nfor information exchange within a local region. Considering that\nmost visual Transformers ignore the inherent structural informa-\ntion inside the sequence of patches, Transformer-iN-Transformer\n(TNT)[27] proposes to use outer Transformer block and inner Trans-\nformer block to model patch-level and pixel-level representations,\nrespectively. Co-Scale Conv-Attentional Image Transformers[76]\ndesigns a conv-attention module to realize relative position embed-\nding and enhance computation efficiency, and further proposes a\nco-scale mechanism to introduce cross-scale attention to enrich\nmulti-scale feature.\nOn the other hand, CNN has the advantages of extracting low\nlevel features and strengthening locality, while Transformer has\nthe advantages in establishing long-range dependencies. Some re-\nsearch makes full use of both advantages. TransFuse[ 84] uses a\ndual-branch structure, which uses Transformer to capture global\ndependencies, while low-level spatial details are extracted by CNN\nbranches. Similarly, CoTr[75] uses the CNN backbone to extract fea-\nture representations and proposes to use deformable Transformer\n(DeTrans) to model long-range dependencies, effectively bridging\nthe convolutional neural network and Transformer. ICT[63] uses\ntransformer to recover pluralistic coherent structures together with\nsome coarse textures, and uses CNN to enhances local texture de-\ntails of coarse priors, so as to achieve excellent results on the image\ncompletion task. TransT[13] uses Siamese-based CNN network for\nfeature extraction, and designs the self-attention-based ego-context\naugment (ECA) and cross-attention-based cross-feature augment\n(CFA) modules for feature fusion. Compact Transformers[28] elim-\ninates the requirement for class token and position embedding\nthrough a novel sequence pooling strategy and the use of convolu-\ntions, so as to perform head-to-head with state-of-the-art CNNs on\nsmall datasets.\nFollow this strategy, we present triplet transformer embedding\nmodule which is embedded into a U-Net framework to improve\nthe performance of RGB-D saliency detection. Combining both\nadvantages, our model achieves the state-of-the-art performance.\n3 PROPOSED METHOD\n3.1 Overview\nThe overall framework of the proposed triplet transformer embed-\nding network is depicted in Fig.2(a), which consists of multi-modal\nfusion encoder, feature enhancement module and three-stream de-\ncoder. The details can be seen in the following sections.\n3.2 Multi-modal fusion encoder\nColor and depth image in RGB-D image are two expressions for\ndifferent modalities of the same scene. Color image provides ap-\npearance cue and depth image shows three dimension spatial in-\nformation. Due to existence of poor quality depth map induced by\nthe imaging devices or conditions, we propose multi-modal fusion\nencoder, in which depth features are first purified by multi-modal\nfeatures using attention mechanism, and then served as supple-\nment to the color feature by the residual connection[29]. Residual\npart is designed as depth purification module (DPM), and shortcut\nconnection part is used to preserve more original color information.\nIn DPM which is shown in Fig. 2(b), depth feature is concatenated\nwith color feature, and fed into a channel attention module to get\nattentive channel mask, which is used to purify the depth feature in\na channel manner. Next, purified depth feature is fed into a spatial\nattention module again to generate attentive spatial mask, which is\nused to purify the depth feature in a spatial manner. The process\ncan be described as:\nğ¹ğ‘Ÿ\nğ‘– = ğ‘“ğ‘‘\nğ‘– Ã—ğ‘†ğ´(ğ‘“ğ‘‘\nğ‘– Ã—ğ¶ğ´(ğ¶ğ‘ğ‘¡(ğ‘“ğ‘‘\nğ‘– ,ğ‘“ ğ‘Ÿ\nğ‘– )))+ ğ‘“ğ‘Ÿ\nğ‘– (1)\nwhere ğ‘“ğ‘Ÿ\nğ‘– and ğ‘“ğ‘‘\nğ‘– represent color and depth features extracted by\nbackbone network respectively in which ğ‘– = 1,Â·Â·Â· ,5, ğ¶ğ‘ğ‘¡(Â·)de-\nnotes concatenation and following convolution operation, ğ¶ğ´(Â·)\nand ğ‘†ğ´(Â·)are channel and spatial attention operation which is pro-\nposed by CBAM[70], â€œÃ—\" is element-wise multiplication operation,\nâ€œ+\" is element-wise addition operation.\nThus, the depth feature with poor quality can be purified, and\nthen attached to color feature to generate more accuracy feature\nrepresentation ğ¹ğ‘Ÿ\nğ‘– (ğ‘– = 1,Â·Â·Â· ,5).\n3.3 Feature enhancement module\nIn this module, we first adjust the features of high three layers\nto the same size, and then use the triplet transformer embedding\nFigure 2: Our proposed triplet transformer embedding network for RGB-D salient object detection.\nmodule to enhance the feature representation by learning long-\nrange dependency across levels, and last concatenate the input and\noutput of triplet transformer embedding module to preserve more\noriginal information.\n3.3.1 Scale adjustment module. The triplet transformer embedding\nmodule is composed of three standard transformer encoders with\nshared weights. Its input should be the features with the same size.\nBut the sizes of the multi-level featuresğ¹ğ‘Ÿ\nğ‘– from multi-modal fusion\nencoder are the different. Therefore, the first important task is to\nadjust the sizes of multi-level features.\nAt first, a transition layer which contains a 3 Ã—3 convolution\nand a ReLU activation function is applied on ğ¹ğ‘Ÿ\nğ‘– . It can adjust the\nnumber of channels of multi-level features to the same size. It can\nbe described as:\nğ¹â€²ğ‘Ÿ\nğ‘– = ğœ(ğ¶ğ‘œğ‘›ğ‘£(ğ¹ğ‘Ÿ\nğ‘– )) ğ‘– = 3,Â·Â·Â· ,5 (2)\nwhere ğ¶ğ‘œğ‘›ğ‘£(Â·)is 3Ã—3 convolution operation, and ğœ(Â·)is ReLU acti-\nvation function.\nThen, we design a progressively upsampling fusion module\nwhich is used to adjust the resolution of the features in the high\nthree layers to the same size. Since the direct upsampling with 2Ã—\nor 4Ã—ratio will bring some noises, the features are progressively\nupsampled and fused. The fusion process can be described as:\nğ¹5 = ğ‘ˆğ¹ğ‘€ (ğ‘ˆğ¹ğ‘€ (ğ¹â€²ğ‘Ÿ\n5 ,ğ¹ â€²ğ‘Ÿ\n4 ),ğ¹ â€²ğ‘Ÿ\n3 )\nğ¹4 = ğ‘ˆğ¹ğ‘€ (ğ¹â€²ğ‘Ÿ\n4 ,ğ¹ â€²ğ‘Ÿ\n3 )\nğ¹3 = ğ¹â€²ğ‘Ÿ\n3\n(3)\nwhere ğ‘ˆğ¹ğ‘€ (Â·)is shown in Fig.2(d). The detail can be described as:\nğ‘ˆğ¹ğ‘€ (ğ¹â„ğ‘–ğ‘”â„,ğ¹ğ‘™ğ‘œğ‘¤)= ğ¶ğ‘ğ‘¡(ğ¶ğ‘œğ‘›ğ‘£(ğ‘ˆğ‘(ğ¹â„ğ‘–ğ‘”â„)),ğ¹ğ‘™ğ‘œğ‘¤) (4)\nwhere ğ¹â„ğ‘–ğ‘”â„ and ğ¹ğ‘™ğ‘œğ‘¤ denote the feature from the higher layer\nwith low resolution and the feature from the lower layer with\nhigh resolution, respectively, and ğ‘ˆğ‘(Â·)denotes 2Ã—upsampling\noperation.\nCompared with direct 2Ã—, 4Ã—upsampling on ğ¹â€²ğ‘Ÿ\n4 and ğ¹â€²ğ‘Ÿ\n5 , progres-\nsively upsampling fusion module can not only adjust the features\nto the same resolution but also increase the spatial detail of feature\nin the high layer by progressive fusion process.\nThus, the features ğ¹ğ‘–(ğ‘– = 3,Â·Â·Â· ,5)with the same scales will be\nserved as the input and fed into next triplet transformer embedding\nmodule.\n3.3.2 Triplet Transformer Embedding Module (TTEM). The fea-\ntures are first converted into the sequences of feature embedding,\nand then fed to three standard transformer encoders with shared\nweights to model the long-range relationship among different levels,\nand last reshaped to the original size of features.\nSpecifically, each input featureğ¹ğ‘–(ğ‘– = 3,Â·Â·Â· ,5)are first flattened\ninto a 1D sequence {ğ¹ğ‘\nğ‘– |ğ‘ = 1,Â·Â·Â· ,ğ‘ }, where ğ‘ is the number of\npatches. Each patch ğ¹ğ‘\nğ‘– is then mapped into a latent ğ·-dimensional\nembedding space by a trainable linear projection layer. Furthermore,\nwe learn specific position embedding which are added to the patch\nembedding to retain positional information. The process can be\ndescribed as:\nğ‘0\nğ‘– = [ğ¹1\nğ‘– +ğ‘ƒğ¸1; ğ¹2\nğ‘– +ğ‘ƒğ¸2; Â·Â·Â· ,; ğ¹ğ‘\nğ‘– +ğ‘ƒğ¸ğ‘] (5)\nwhere ğ‘ƒğ¸ = {ğ‘ƒğ¸ğ‘|ğ‘ = 1,Â·Â·Â· ,ğ‘ }is a 1D learnable positional em-\nbedding.\nThe remaining architecture essentially follows the standard\ntransformer encoder[62] which stacks ğ¿ transformer layer. It is\nshown in Fig.2(c). Each transformer layer contains multi-headed\nself-attention (MSA) and multi-layer perceptron (MLP) sublayer.\nLayer normalization (LN)[2] are inserted before these two sublayers,\nand the residual connection is performed after these two sublayers.\nThe process can be described as:\nï£±ï£´ï£´ ï£²\nï£´ï£´ï£³\nğ‘ğ‘™\nğ‘–\nâ€²= ğ‘€ğ‘†ğ´\n\u0010\nğ¿ğ‘\n\u0010\nğ‘ğ‘™âˆ’1\nğ‘–\n\u0011\u0011\n+ğ‘ğ‘™âˆ’1\nğ‘–\nğ‘ğ‘™\nğ‘– = ğ‘€ğ¿ğ‘ƒ\n\u0010\nğ¿ğ‘\n\u0010\nğ‘ğ‘™\nğ‘–\nâ€²\u0011\u0011\n+ğ‘ğ‘™\nğ‘–\nâ€² ğ‘™ = 1,Â·Â·Â· ğ¿ (6)\nwhere ğ¿denotes the number of transformer layers in the standard\ntransformer encoder.\n3.3.3 Feature concatenation module. The outputs of three weights\nshared transformer encoders ğ‘ğ¿\nğ‘– (ğ‘– = 3,Â·Â·Â· ,5)fuses the informa-\ntion of three layers by Transformer mechanism, so as to enhance\nthe original feature representation. In order to preserve the more\noriginal information, we further cascade these outputs with original\nfeatures to generate the enhanced features of high three layers. The\nprocess can be described as:\nğ¹â€²\nğ‘– = ğ¶ğ‘ğ‘¡(ğ‘ğ¿\nğ‘– ,ğ¹ğ‘–) ğ‘– = 3,Â·Â·Â· 5 (7)\n3.4 Three-stream decoder\nAfter the features of high three layers are enhanced by the proposed\ntriplet transformer embedding module, we will combine them with\nthe features of low two layers to achieve the decoding process. There\nare two decoding methods. One is single-stream decoding and the\nother is three-stream decoding. The single-stream decoding first\nfuses three output results of feature enhancement module, and then\ncombine it with two features in the low layers. The three-stream\ndecoding first combines each output result of feature enhancement\nmodule with two features in the low layers, and then fuses three-\nstream results. We conduct two decoding processes, and find three-\nstream decoding is better than single-stream decoding. Next, we\nuse formula to show three-stream decoder as follow:\nğ¹â€²â€²\nğ‘– = ğ¶ğ‘ğ‘¡(ğ¶ğ‘ğ‘¡(ğ‘ˆğ‘(ğ¹â€²\nğ‘–),ğ¹2\nğ‘Ÿ ),ğ¹1\nğ‘Ÿ ) ğ‘– = 3,Â·Â·Â· 5 (8)\nThe above three features are performed upsampling, convolution\noperation and sigmoid function to generate the saliency mapsğ‘†ğ‘–(ğ‘– =\n1,Â·Â·Â· ,3)which are supervised by the ground truth maps.\nğ‘†ğ‘– = ğ‘ ğ‘–ğ‘”(ğ¶ğ‘œğ‘›ğ‘£(ğ‘ˆğ‘(ğ¶ğ‘œğ‘›ğ‘£(ğ‘ˆğ‘(ğ¹â€²â€²\nğ‘– ))))) (9)\nwhere ğ‘ ğ‘–ğ‘”(Â·)denotes sigmoid function.\nAt last, we also fuse all the features above to generate the final\nsaliency map.\nğ‘†ğ‘“ğ‘–ğ‘›ğ‘ğ‘™ = ğ‘ ğ‘–ğ‘”(\n5âˆ‘ï¸\nğ‘–=3\nğ¶ğ‘œğ‘›ğ‘£(ğ‘ˆğ‘(ğ¶ğ‘œğ‘›ğ‘£(ğ‘ˆğ‘(ğ¹â€²â€²\nğ‘– ))))) (10)\nPixel position aware loss ğ¿ğ‘ ğ‘ğ‘ğ‘ [69] is adopted for end-to-end\ntraining. The whole loss is defined as:\nğ¿= ğ¿ğ‘ \nğ‘ğ‘ğ‘(ğ‘†ğ‘“ğ‘–ğ‘›ğ‘ğ‘™,ğº)+\n5âˆ‘ï¸\nğ‘–=3\nğ¿ğ‘ \nğ‘ğ‘ğ‘(ğ‘†ğ‘–,ğº) (11)\nwhere ğº is ground truth saliency map.\n4 EXPERIMENTS\n4.1 Datasets and evaluation metrics\n4.1.1 Datasets. We evaluate the proposed method on six challeng-\ning RGB-D SOD datasets. NLPR [ 54] includes 1000 images with\nsingle or multiple salient objects. NJU2K [36] consists of 2003 stereo\nimage pairs and ground-truth maps with different objects, com-\nplex and challenging scenes. STERE [50] incorporates 1000 pairs\nof binocular images downloaded from the Internet. DES [15] has\n135 indoor images collected by Microsoft Kinect. SIP [22] contains\n1000 high-resolution images of multiple salient persons. DUT [56]\ncontains 1200 images captured by Lytro camera in real life scenes.\nFor the sake of fair comparison, we use the same training dataset\nas in [11, 22], which consists of 1,485 images from the NJU2K dataset\nand 700 images from the NLPR dataset. The remaining images in the\nNJU2K and NLPR datasets and the whole datasets of STERE, DES\nand SIP are used for testing. In addition, on the DUT dataset, we\nfollow the same protocols as in [33, 39, 56, 57, 90] to add additional\n800 pairs from DUT for training and test on the remaining 400 pairs.\nIn summary, our training set contains 2,185 paired RGB and depth\nimages, but when testing is conducted on DUT, our training set\ncontains 2,985 paired ones.\n4.1.2 Evaluation Metrics. We adopt five widely used metrics to\nevaluate the performance of our model and other state-of-the-art\nRGB-D SOD models, including the precision-recal(PR) curve [ 3],\nE-measure [21], S-measure [20], F-measure [1] and mean absolute\nerror (MAE) [ 55]. Specifically, the PR curve plots precision and\nrecall values by setting a series of thresholds on the saliency maps to\nget the binary masks and further comparing them with the ground\ntruth maps. The E-measure simultaneously captures global statistics\nand local pixel matching information. The S-measure can evaluate\nboth region-aware and object-aware structural similarity between\nsaliency map and ground truth. The F-measure is the weighted\nharmonic mean of precision and recall, which can evaluate the\noverall performance. The MAE measures the average of the per-\npixel absolute difference between the saliency maps and the ground\ntruth maps. In our experiment, E-measure and F-measure adopts\nadaptive values.\n4.2 Implementation details\nDuring the training and testing phase, the input RGB and depth\nimages are resized to 256Ã—256. Multiple enhancement strategies\nare used for all training images, i.e. random flipping, rotating and\nborder clipping. Parameters of the backbone network are initialized\nwith the pretrained parameters of ResNet-50 network [ 29]. The\nhyper-parameters in transformer encoder are set as: ğ¿ = 12,ğ· =\n768,ğ‘ = 1024. The rest of parameters are initialized to PyTorch\ndefault settings. We employ the Adam optimizer [37] to train our\nnetwork with a batch size of 3 and an initial learning rate 1e-5, and\nthe learning rate will be divided by 10 every 60 epochs. Our model\nis trained on a machine with a single NVIDIA GTX 3090 GPU. The\nmodel converges within 150 epochs, which takes nearly 15 hours.\n4.3 Comparisons with the state-of-the-art\nOur model is compared with 16 state-of-the-art RGB-D SOD models,\nincluding D3Net [22], ICNet [41], DCMF [6], DRLF [67], SSF [81],\nSSMA [43], A2dele [57], UCNet [80], CoNet [33], DANet [90], JLDCF[24],\nEBFSP[31],CDNet[35], HAINet[40], RD3D[10] and DSA2F[61]. To\nensure the fairness of the comparison results, the saliency maps of\nthe evaluation are provided by the authors or generated by running\nsource codes.\n4.3.1 Quantitative Evaluation. Figure.3 shows the comparison re-\nsults on PR curve. Table.1 shows the quantitative comparison results\nof four evaluation metrics. As can be clearly observed from figure\nthat our curves are very short, which means that our recall is very\nhigh. Furthermore, from the table, we can see that all the evalua-\ntion metrics are nearly the best on six datasets, so as to verify the\neffectiveness and advantages of our proposed method. Only two\nS-measure values in NLPR and STERE datasets are inferior to the\nbest, but they are also the second best. Combined with the results of\nfigure and table, our method achieves the impressive performance.\n4.3.2 Qualitative Evaluation. To make the qualitative comparisons,\nwe show some visual examples in Figure.4. It can be observed that\nour method has better detection results than other methods in\nsome challenging cases: similar foreground and background(1ğ‘ ğ‘¡-\n2ğ‘›ğ‘‘ rows), complex scene(3ğ‘Ÿğ‘‘-4ğ‘¡â„ rows), low quality depth map(5ğ‘¡â„-\n6ğ‘¡â„ rows), small object(7ğ‘¡â„-8ğ‘¡â„ rows) and multiple objects(9ğ‘¡â„-10ğ‘¡â„\nrows). These indicate that our approach can better locate salient\nobjects and produce more accurate saliency maps. In addition, our\napproach can produce more fine-grained details as highlighted in\nthe salient region(11ğ‘¡â„-12ğ‘¡â„ rows). This is also the proof of the\neffectiveness of our method.\n4.4 Ablation studies\nWe conduct ablation studies on NLPR, NJU2K, SIP and STERE\ndatasets to investigate the contributions of different modules in the\nproposed method.\n4.4.1 The effectiveness of triplet transformer embedding module\n(TTEM). The baseline model used here removes TTEM. Its perfor-\nmance is shown in the variant No.1 of Table. 2. Further, we replace\nTTEM with gated recurrent unit (GRU) [16], whose result is shown\nin the variant No.2 of Table. 2. The variant No.3 of Table. 2 is the\nresult of siamese transformer applied in the high two layers. The\nvariant No.4 of Table. 2 is the result of quadruplet transformer ap-\nplied in the high four layers. The variant No.5 of Table. 2 is our\nresult of triplet transformer applied in the high three layers.\nIt can be clearly observed that compared with No.1, the result of\nour TriTransNet is improved 0.016 in the S-measure metric, 0.021\nin the F-measure metric, 0.008 in the E-measure metric and 0.007\nin the MAE metric on average. Meanwhile, compared with No.2, the\nresult of our TriTransNet is improved0.012 in the S-measure metric,\n0.014 in the F-measure metric, 0.006 in the E-measure metric and\n0.005 in the MAE metric on average. TTEM plays an important\nrole in the performance improvement.\nIn addition, we compare No.3, No.4 and No.5 and find that Triplet\nwin Siamese in S-measure, F-measure, E-measure, and MAE about\n0.009,0.016,0.006 and 0.005 on average, and outperform Quadruplet\nabout 0.010,0.009,0.005 and 0.004 on average. Our TriTransNet en-\nhances long-range dependency of semantic information by using\nthe features in the high three layers, and further combines with\nthree-stream usampling decoding in the low two layers to perfectly\ndepict the detailed boundary, so as to achieve the best performance.\n4.4.2 The effectiveness of three-stream decoder. we further conduct\nthe ablation study by replacing three stream decoder with single-\nstream decoder to check the effectiveness of the designed three-\nstream decoder. Table. 3 No.1 denotes the model which adopts\nsingle-stream decoder and No.2 means our three-stream decoder.\nFrom Table. 3, we can see that the use of three-stream decoder\nobviously improves the detection performance. It benefits from the\nfull integration of multi-layer features.\n4.4.3 The effectiveness of depth purification module (DPM). The\nbaseline model used here removes depth purification module (DPM).\nIt attaches the depth feature to color feature by element-wise ad-\ndition operation in the encoder. Its performance is illustrated in\nthe variant No.1 of Table. 4. Further, we discuss the similar depth-\nenhanced module (DEM) proposed in BBS[ 23] whose result is\nshown in the variant No.2 of Table. 4. The variant No.3 of Table. 4\ndenotes the model which adopts DPM instead of element-wise\naddition operation based on the baseline.\nCompared with No.1, the performance of the variant No.3 is\nsignificantly improved. Meanwhile, compared with No.2 which\nusing DEM, our detection effect is also better than that of No.2. It\nverified that the effectiveness of DPM.\n5 CONCLUSIONS\nIn the paper, we introduce transformer into U-Net framework to\ndetect salient object in RGB-D image. Different from existing com-\nbination method of transformer and convolutional neural networks,\nwe propose a triplet transformer embedding module which can be\nembedded into existing U-Net models for the better feature rep-\nresentation by learning long-range dependency among different\nlevels with less cost. Furthermore, we use depth information to\nenhance RGB features by depth purification module. Experimental\nresults show our method pushes the performance to a new level,\n(a)NLPR dataset (b)NJU2K dataset (c)STERE dataset\n(d)DES dataset (e)SIP dataset (f)DUT dataset\nFigure 3: P-R curves comparisons of different models on six datasets.\nTable 1: S-measure, adaptive F-measure, adaptive E-measure, MAE comparisons with different models. The best result is in\nbold.\nDatasetsMetric D3Net ICNetDCMF DRLF SSF SSMA A2dele UCNet CoNet DANet JLDCF EBFSP CDNetHAINet RD3D DSA2F TriTransNet\nTNNLS20TIP20 TIP20 TIP20CVPR20CVPR20CVPR20CVPR20ECCV20ECCV20CVPR20TMM21 TIP21 TIP21 AAAI21CVPR21 Ours\nNLPR Sâ†‘ .912 .923 .900 .903 .914 .915 .896 .920 .908 .920 .925 .915 .902 .924 .930 .918 .928\nFğ›½ â†‘ .861 .870 .839 .843 .875 .853 .878 .890 .846 .875 .878 .897 .848 .897 .892 .892 .909\nğ¸ğœ‰ â†‘ .944 .944 .933 .936 .949 .938 .945 .953 .934 .951 .953 .952 .935 .957 .958 .950 .960\nMAEâ†“ .030 .028 .035 .032 .026 .030 .028 .025 .031 .027 .022 .026 .032 .024 .022 .024 .020\nNJU2K Sâ†‘ .901 .894 .889 .886 .899 .894 .869 .897 .895 .899 .902 .903 .885 .912 .916 .904 .920\nFğ›½ â†‘ .865 .868 .859 .849 .886 .865 .874 .889 .872 .871 .885 .894 .866 .900 .901 .898 .919\nğ¸ğœ‰ â†‘ .914 .905 .897 .901 .913 .896 .897 .903 .912 .908 .913 .907 .911 .922 .918 .922 .925\nMAEâ†“ .046 .052 .052 .055 .043 .053 .051 .043 .046 .045 .041 .039 .048 .038 .036 .039 .030\nSTERE Sâ†‘ .899 .903 .883 .888 .887 .890 .878 .903 .905 .901 .903 .900 .896 .907 .911 .897 .908\nFğ›½ â†‘ .859 .865 .841 .845 .867 .855 .874 .885 .884 .868 .869 .870 .873 .885 .886 .893 .893\nğ¸ğœ‰ â†‘ .920 .915 .904 .915 .921 .907 .915 .922 .927 921 .919 .912 .922 .925 .927 .927 .927\nMAEâ†“ .046 .045 .054 .050 .046 .051 .044 .039 .037 .043 .040 .045 .042 .040 .037 .039 .033\nDES Sâ†‘ .898 .920 .877 .895 .905 .941 .885 .933 .911 .924 .931 .937 .875 .935 .935 .916 .943\nFğ›½ â†‘ .870 .889 .820 .868 .876 .906 .865 .917 .861 .899 .900 .913 .839 .924 .917 .901 .936\nğ¸ğœ‰ â†‘ .951 .959 .923 .954 .948 .974 .922 .974 .945 .968 .969 .974 .921 .974 .975 .955 . 981\nMAEâ†“ .031 .027 .040 .030 .025 .021 .028 .018 .027 .023 .020 .018 .034 .018 .019 .023 .014\nSIP Sâ†‘ .860 .854 .859 .850 .868 .872 .826 .875 .858 .875 .880 .885 .823 .880 .885 .862 .886\nFğ›½ â†‘ .835 .836 .819 .813 .851 .854 .825 .868 .842 .855 .873 .869 .805 .875 .874 .865 .892\nğ¸ğœ‰ â†‘ .902 .899 .898 .891 .911 .911 .892 .913 .909 .914 .921 .917 .880 .919 .920 .908 .924\nMAEâ†“ .063 .069 .068 .071 .056 .057 .070 .051 .063 .054 .049 .049 .076 .053 .048 .057 .043\nDUT Sâ†‘ .775 .852 .798 .826 .916 .903 .886 .864 .919 .899 .906 .858 .880 .910 .931 .921 .933\nFğ›½ â†‘ .756 .830 .750 .803 .914 .866 .890 .856 .909 .888 .882 .842 .874 .906 .924 .926 .938\nğ¸ğœ‰ â†‘ .847 .897 .848 .870 946 .921 .924 .903 .948 .934 .931 .890 .918 .938 .949 .950 .957\nMAEâ†“ .097 .072 .104 .080 .034 .044 .043 .056 .033 .043 .043 .067 .048 .038 .031 .030 .025\nand ablation studies also verify the effectiveness of each module. In\nthe future, we will achieve the same task by a pure transformer, and\nfurther discuss their respective advantages to achieve the better\ncombination.\nACKNOWLEDGMENTS\nThis work is supported by National Natural Science Foundation of\nChina (62006002), Natural Science Foundation of Anhui Province\n(1908085MF182) and Key Program of Natural Science Project of\nEducational Commission of Anhui Province(KJ2019A0034).\nFigure 4: Visual comparison results with other the state-of-the-art models.\nTable 2: Ablation experiment of triplet transformer embedding module (TTEM). The best result is in bold.\nVariant\nCandidate NLPR NJUD2K SIP STERE\nBaseline GRU Siamese Quadruplet Triplet Sâ†‘ Fğ›½ â†‘ğ¸ğœ‰ â†‘MAEâ†“ Sâ†‘ Fğ›½ â†‘ğ¸ğœ‰ â†‘MAEâ†“ Sâ†‘ Fğ›½ â†‘ğ¸ğœ‰ â†‘MAEâ†“ Sâ†‘ Fğ›½ â†‘ğ¸ğœ‰ â†‘MAEâ†“\nNo.1 âœ“ .910 .882 .952 .026 .904 .897 .917 .038 .876 .877 .918 .049 .888 .873 .916 .042\nNo.2 âœ“ âœ“ .914 .891 .953 .024 .905 .901 .919 .037 .879 .882 .919 .047 .895 .883 .920 .038\nNo.3 âœ“ âœ“ .917 .888 .956 .024 .910 .903 .915 .035 .882 .885 .926 .046 .896 .872 .917 .040\nNo.4 âœ“ âœ“ .922 .903 .958 .022 .911 .908 .922 .034 .875 .886 .913 .048 .895 .881 .922 .038\nNo.5 âœ“ âœ“ .928 .909 .960 .020 .920 .919 .925 .030 .886 .892 .924 .043 .908 .893 .927 .033\nTable 3: Ablation experiment of three-stream decoder. The best result is in bold.\nVariant\nCandidate NLPR NJUD2K SIP STERE\nsingle-stream three-stream Sâ†‘ Fğ›½ â†‘ğ¸ğœ‰ â†‘MAEâ†“ Sâ†‘ Fğ›½ â†‘ğ¸ğœ‰ â†‘MAEâ†“ Sâ†‘ Fğ›½ â†‘ğ¸ğœ‰ â†‘MAEâ†“ Sâ†‘ Fğ›½ â†‘ğ¸ğœ‰ â†‘MAEâ†“\nNo.1 âœ“ .923 .892 .958 .022 .916 .904 .919 .035 .884 .886 .920 .045 .903 .879 .920 .037\nNo.2 âœ“ .928 .909 .960 .020 .920 .919 .925 .030 .886 .892 .924 .043 .908 .893 .927 .033\nTable 4: Ablation experiment of depth purification module (DPM). The best result is in bold.\nVariant\nCandidate NLPR NJUD2K SIP STERE\nBaseline DEM DPM Sâ†‘ Fğ›½ â†‘ğ¸ğœ‰ â†‘MAEâ†“ Sâ†‘ Fğ›½ â†‘ğ¸ğœ‰ â†‘MAEâ†“ Sâ†‘ Fğ›½ â†‘ğ¸ğœ‰ â†‘MAEâ†“ Sâ†‘ Fğ›½ â†‘ğ¸ğœ‰ â†‘MAEâ†“\nNo.1 âœ“ .917 .897 .956 .023 .909 .904 .920 .035 .883 .887 .921 .044 .894 .875 .918 .039\nNo.2 âœ“ âœ“ .923 .901 .958 .021 .914 .910 .922 .033 .884 .889 .923 .044 .905 .889 .925 .035\nNo.3 âœ“ âœ“ .928 .909 .960 .020 .920 .919 .925 .030 .886 .892 .924 .043 .908 .893 .927 .033\nREFERENCES\n[1] Radhakrishna Achanta, Sheila Hemami, Francisco Estrada, and Sabine Susstrunk.\n2009. Frequency-tuned salient region detection. In 2009 IEEE conference on\ncomputer vision and pattern recognition . IEEE, 1597â€“1604.\n[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normaliza-\ntion. arXiv preprint arXiv:1607.06450 (2016).\n[3] Ali Borji, Ming-Ming Cheng, Huaizu Jiang, and Jia Li. 2015. Salient object\ndetection: A benchmark. IEEE transactions on image processing 24, 12 (2015),\n5706â€“5722.\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexan-\nder Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection with\ntransformers. In European Conference on Computer Vision . Springer, 213â€“229.\n[5] Chenglizhao Chen, Jipeng Wei, Chong Peng, and Hong Qin. 2021. Depth-Quality-\nAware Salient Object Detection. IEEE Transactions on Image Processing 30 (2021),\n2350â€“2363.\n[6] Hao Chen, Yongjian Deng, Youfu Li, Tzu-Yi Hung, and Guosheng Lin. 2020. RGBD\nsalient object detection via disentangled cross-modal fusion. IEEE Transactions\non Image Processing 29 (2020), 8407â€“8416.\n[7] Hao Chen and Youfu Li. 2018. Progressively complementarity-aware fusion\nnetwork for RGB-D salient object detection. In Proceedings of the IEEE conference\non computer vision and pattern recognition . 3051â€“3060.\n[8] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le\nLu, Alan L Yuille, and Yuyin Zhou. 2021. Transunet: Transformers make strong\nencoders for medical image segmentation.arXiv preprint arXiv:2102.04306 (2021).\n[9] Qian Chen, Keren Fu, Ze Liu, Geng Chen, Hongwei Du, Bensheng Qiu, and Ling\nShao. 2020. EF-Net: A novel enhancement and fusion network for RGB-D saliency\ndetection. Pattern Recognition (2020), 107740.\n[10] Qian Chen, Ze Liu, Yi Zhang, Keren Fu, Qijun Zhao, and Hongwei Du. 2021.\nRGB-D Salient Object Detection via 3D Convolutional Neural. AAAI (2021).\n[11] Shuhan Chen and Yun Fu. 2020. Progressively guided alternate refinement\nnetwork for RGB-D salient object detection. In European Conference on Computer\nVision. Springer, 520â€“538.\n[12] Sihan Chen, Xinxin Zhu, Wei Liu, Xingjian He, and Jing Liu. 2021. Global-\nLocal Propagation Network for RGB-D Semantic Segmentation. arXiv preprint\narXiv:2101.10801 (2021).\n[13] Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang, and Huchuan Lu.\n2021. Transformer tracking. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition . 8126â€“8135.\n[14] Zuyao Chen, Runmin Cong, Qianqian Xu, and Qingming Huang. 2020. DPANet:\nDepth Potentiality-Aware Gated Attention Network for RGB-D Salient Object\nDetection. IEEE Transactions on Image Processing (2020).\n[15] Yupeng Cheng, Huazhu Fu, Xingxing Wei, Jiangjian Xiao, and Xiaochun Cao.\n2014. Depth enhanced saliency detection method. In Proceedings of international\nconference on internet multimedia computing and service . 23â€“27.\n[16] Kyunghyun Cho, Bart Van MerriÃ«nboer, Caglar Gulcehre, Dzmitry Bahdanau,\nFethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase\nrepresentations using RNN encoder-decoder for statistical machine translation.\narXiv preprint arXiv:1406.1078 (2014).\n[17] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and Huaxia Xia. 2021. Do We\nReally Need Explicit Position Encodings for Vision Transformers? arXiv preprint\narXiv:2102.10882 (2021).\n[18] Michael Donoser, Martin Urschler, Martin Hirzer, and Horst Bischof. 2009.\nSaliency driven total variation segmentation. In 2009 IEEE 12th International\nConference on Computer Vision . IEEE, 817â€“824.\n[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is\nWorth 16x16 Words: Transformers for Image Recognition at Scale. InInternational\nConference on Learning Representations .\n[20] Deng-Ping Fan, Ming-Ming Cheng, Yun Liu, Tao Li, and Ali Borji. 2017. Structure-\nmeasure: A new way to evaluate foreground maps. In Proceedings of the IEEE\ninternational conference on computer vision . 4548â€“4557.\n[21] Deng-Ping Fan, Cheng Gong, Yang Cao, Bo Ren, Ming-Ming Cheng, and Ali Borji.\n2018. Enhanced-alignment measure for binary foreground map evaluation. arXiv\npreprint arXiv:1805.10421 (2018).\n[22] Deng-Ping Fan, Zheng Lin, Zhao Zhang, Menglong Zhu, and Ming-Ming Cheng.\n2020. Rethinking RGB-D Salient Object Detection: Models, Data Sets, and Large-\nScale Benchmarks. IEEE Transactions on Neural Networks and Learning Systems\n(2020).\n[23] Deng-Ping Fan, Yingjie Zhai, Ali Borji, Jufeng Yang, and Ling Shao. 2020. BBS-\nNet: RGB-D salient object detection with a bifurcated backbone strategy network.\nIn European Conference on Computer Vision . Springer, 275â€“292.\n[24] Keren Fu, Deng-Ping Fan, Ge-Peng Ji, and Qijun Zhao. 2020. JL-DCF: Joint\nlearning and densely-cooperative fusion framework for rgb-d salient object\ndetection. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition. 3052â€“3062.\n[25] Yuan Gao, Miaojing Shi, Dacheng Tao, and Chao Xu. 2015. Database saliency for\nfast image retrieval. IEEE Transactions on Multimedia 17, 3 (2015), 359â€“369.\n[26] Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R Mar-\ntin, and Shi-Min Hu. 2020. PCT: Point Cloud Transformer. arXiv preprint\narXiv:2012.09688 (2020).\n[27] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang.\n2021. Transformer in transformer. arXiv preprint arXiv:2103.00112 (2021).\n[28] Ali Hassani, Steven Walton, Nikhil Shah, Abulikemu Abuduweili, Jiachen Li, and\nHumphrey Shi. 2021. Escaping the Big Data Paradigm with Compact Transform-\ners. arXiv preprint arXiv:2104.05704 (2021).\n[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\nlearning for image recognition. In Proceedings of the IEEE conference on computer\nvision and pattern recognition . 770â€“778.\n[30] Seunghoon Hong, Tackgeun You, Suha Kwak, and Bohyung Han. 2015. Online\ntracking by learning discriminative saliency map with convolutional neural\nnetwork. In International conference on machine learning . 597â€“606.\n[31] Nianchang Huang, Yang Yang, Dingwen Zhang, Qiang Zhang, and Jungong Han.\n2021. Employing Bilinear Fusion and Saliency Prior Information for RGB-D\nSalient Object Detection. IEEE Transactions on Multimedia (2021).\n[32] Qing-Ge Ji, Zhi-Dang Fang, Zhen-Hua Xie, and Zhe-Ming Lu. 2013. Video\nabstraction based on the visual attention model and online clustering. Signal\nProcessing: Image Communication 28, 3 (2013), 241â€“253.\n[33] Wei Ji, Jingjing Li, Miao Zhang, Yongri Piao, and Huchuan Lu. 2020. Accurate rgb-\nd salient object detection via collaborative learning. In Computer Visionâ€“ECCV\n2020: 16th European Conference, Glasgow, UK, August 23â€“28, 2020, Proceedings,\nPart XVIII 16 . Springer, 52â€“69.\n[34] Qiuping Jiang, Feng Shao, Weisi Lin, Ke Gu, Gangyi Jiang, and Huifang Sun.\n2017. Optimizing multistage discriminative dictionaries for blind image quality\nassessment. IEEE Transactions on Multimedia 20, 8 (2017), 2035â€“2048.\n[35] Wen-Da Jin, Jun Xu, Qi Han, Yi Zhang, and Ming-Ming Cheng. 2021. CDNet:\nComplementary Depth Network for RGB-D Salient Object Detection. IEEE\nTransactions on Image Processing 30 (2021), 3376â€“3390.\n[36] Ran Ju, Ling Ge, Wenjing Geng, Tongwei Ren, and Gangshan Wu. 2014. Depth\nsaliency based on anisotropic center-surround difference. In 2014 IEEE interna-\ntional conference on image processing (ICIP) . IEEE, 1115â€“1119.\n[37] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-\nmization. arXiv preprint arXiv:1412.6980 (2014).\n[38] Chongyi Li, Runmin Cong, Sam Kwong, Junhui Hou, Huazhu Fu, Guopu Zhu,\nDingwen Zhang, and Qingming Huang. 2020. ASIF-Net: Attention steered inter-\nweave fusion network for RGB-D salient object detection. IEEE Transactions on\nCybernetics (2020).\n[39] Chongyi Li, Runmin Cong, Yongri Piao, Qianqian Xu, and Chen Change Loy. 2020.\nRGB-D salient object detection with cross-modality modulation and selection. In\nEuropean Conference on Computer Vision . Springer, 225â€“241.\n[40] Gongyang Li, Zhi Liu, Minyu Chen, Zhen Bai, Weisi Lin, and Haibin Ling. 2021.\nHierarchical Alternate Interaction Network for RGB-D Salient Object Detection.\nIEEE Transactions on Image Processing 30 (2021), 3528â€“3542.\n[41] Gongyang Li, Zhi Liu, and Haibin Ling. 2020. ICNet: Information Conversion\nNetwork for RGB-D Based Salient Object Detection. IEEE Transactions on Image\nProcessing 29 (2020), 4873â€“4884.\n[42] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool. 2021. Lo-\ncalViT: Bringing Locality to Vision Transformers. arXiv preprint arXiv:2104.05707\n(2021).\n[43] Nian Liu, Ni Zhang, and Junwei Han. 2020. Learning Selective Self-Mutual At-\ntention for RGB-D Saliency Detection. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition . 13756â€“13765.\n[44] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,\nand Baining Guo. 2021. Swin transformer: Hierarchical vision transformer using\nshifted windows. arXiv preprint arXiv:2103.14030 (2021).\n[45] Zhengyi Liu, Song Shi, Quntao Duan, Wei Zhang, and Peng Zhao. 2019. Salient\nobject detection for RGB-D image by single stream recurrent convolution neural\nnetwork. Neurocomputing 363 (2019), 46â€“57.\n[46] Zhengyi Liu, Wei Zhang, and Peng Zhao. 2020. A cross-modal adaptive gated\nfusion generative adversarial network for RGB-D salient object detection. Neuro-\ncomputing 387 (2020), 210â€“220.\n[47] Cong Ma, Zhenjiang Miao, Xiao-Ping Zhang, and Min Li. 2017. A saliency prior\ncontext model for real-time object tracking. IEEE Transactions on Multimedia 19,\n11 (2017), 2415â€“2424.\n[48] Fuyan Ma, Bin Sun, and Shutao Li. 2021. Robust Facial Expression Recognition\nwith Convolutional Visual Transformers. arXiv preprint arXiv:2103.16854 (2021).\n[49] Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and Christoph Feichten-\nhofer. 2021. TrackFormer: Multi-Object Tracking with Transformers. arXiv\npreprint arXiv:2101.02702 (2021).\n[50] Yuzhen Niu, Yujie Geng, Xueqing Li, and Feng Liu. 2012. Leveraging stereopsis\nfor saliency analysis. In 2012 IEEE Conference on Computer Vision and Pattern\nRecognition. IEEE, 454â€“461.\n[51] Nabil Ouerhani and Heinz Hugli. 2000. Computing visual attention from scene\ndepth. In Proceedings 15th International Conference on Pattern Recognition. ICPR-\n2000, Vol. 1. IEEE, 375â€“378.\n[52] Liang Pan, Xiaofei Zhou, Ran Shi, Jiyong Zhang, and Chenggang Yan. 2020.\nCross-modal feature extraction and integration based RGBD saliency detection.\nImage and Vision Computing 101 (2020), 103964.\n[53] Youwei Pang, Lihe Zhang, Xiaoqi Zhao, and Huchuan Lu. 2020. Hierarchi-\ncal dynamic filtering network for rgb-d salient object detection. In Computer\nVisionâ€“ECCV 2020: 16th European Conference, Glasgow, UK, August 23â€“28, 2020,\nProceedings, Part XXV 16 . Springer, 235â€“252.\n[54] Houwen Peng, Bing Li, Weihua Xiong, Weiming Hu, and Rongrong Ji. 2014. Rgbd\nsalient object detection: a benchmark and algorithms. In European conference on\ncomputer vision . Springer, 92â€“109.\n[55] Federico Perazzi, Philipp KrÃ¤henbÃ¼hl, Yael Pritch, and Alexander Hornung. 2012.\nSaliency filters: Contrast based filtering for salient region detection. In 2012 IEEE\nconference on computer vision and pattern recognition . IEEE, 733â€“740.\n[56] Yongri Piao, Wei Ji, Jingjing Li, Miao Zhang, and Huchuan Lu. 2019. Depth-\ninduced multi-scale recurrent attention network for saliency detection. In Pro-\nceedings of the IEEE International Conference on Computer Vision . 7254â€“7263.\n[57] Yongri Piao, Zhengkun Rong, Miao Zhang, Weisong Ren, and Huchuan Lu. 2020.\nA2dele: Adaptive and Attentive Depth Distiller for Efficient RGB-D Salient Object\nDetection. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition . 9060â€“9069.\n[58] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-Net: Convolutional\nnetworks for biomedical image segmentation. In International Conference on\nMedical image computing and computer-assisted intervention . Springer, 234â€“241.\n[59] Lucas Stoffl, Maxime Vidal, and Alexander Mathis. 2021. End-to-End Train-\nable Multi-Instance Pose Estimation with Transformers. arXiv preprint\narXiv:2103.12115 (2021).\n[60] Lei Sun, Kailun Yang, Xinxin Hu, Weijian Hu, and Kaiwei Wang. 2020. Real-time\nfusion network for RGB-D semantic segmentation incorporating unexpected\nobstacle detection for road-driving images. IEEE Robotics and Automation Letters\n5, 4 (2020), 5558â€“5565.\n[61] Peng Sun, Wenhu Zhang, Huanyu Wang, Songyuan Li, and Xi Li. 2021. Deep\nRGB-D Saliency Detection with Depth-Sensitive Attention and Automatic Multi-\nModal Fusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition . 1407â€“1417.\n[62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Å ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In Advances in Neural Information Processing Systems , Vol. 30. Curran\nAssociates, Inc., 5998â€“6008.\n[63] Ziyu Wan, Jingbo Zhang, Dongdong Chen, and Jing Liao. 2021. High-Fidelity\nPluralistic Image Completion with Transformers. arXiv preprint arXiv:2103.14031\n(2021).\n[64] Ningning Wang and Xiaojin Gong. 2019. Adaptive fusion for RGB-D salient\nobject detection. IEEE Access 7 (2019), 55277â€“55284.\n[65] Wenguan Wang, Jianbing Shen, and Haibin Ling. 2018. A deep network solution\nfor attention and aesthetics aware photo cropping. IEEE transactions on pattern\nanalysis and machine intelligence 41, 7 (2018), 1531â€“1544.\n[66] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong\nLu, Ping Luo, and Ling Shao. 2021. Pyramid vision transformer: A versatile back-\nbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122\n(2021).\n[67] Xuehao Wang, Shuai Li, Chenglizhao Chen, Yuming Fang, Aimin Hao, and Hong\nQin. 2020. Data-level recombination and lightweight fusion scheme for RGB-D\nsalient object detection. IEEE Transactions on Image Processing 30 (2020), 458â€“471.\n[68] Yue Wang, Yuke Li, James H Elder, Runmin Wu, Huchuan Lu, and Lu Zhang.\n2020. Synergistic saliency and depth prediction for RGB-D saliency detection. In\nProceedings of the Asian Conference on Computer Vision . 1â€“17.\n[69] Jun Wei, Shuhui Wang, and Qingming Huang. 2020. F3Net: Fusion, Feedback\nand Focus for Salient Object Detection. In Proceedings of the AAAI Conference on\nArtificial Intelligence. 12321â€“12328.\n[70] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. 2018. CBAM:\nConvolutional block attention module. In Proceedings of the European conference\non computer vision (ECCV) . 3â€“19.\n[71] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Masayoshi\nTomizuka, Kurt Keutzer, and Peter Vajda. 2020. Visual transformers: Token-\nbased image representation and processing for computer vision. arXiv preprint\narXiv:2006.03677 (2020).\n[72] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and\nLei Zhang. 2021. CvT: Introducing Convolutions to Vision Transformers. arXiv\npreprint arXiv:2103.15808 (2021).\n[73] Yu-Huan Wu, Yun Liu, Jun Xu, Jia-Wang Bian, Yuchao Gu, and Ming-Ming Cheng.\n2020. MobileSal: Extremely Efficient RGB-D Salient Object Detection. arXiv\npreprint arXiv:2012.13095 (2020).\n[74] Zhe Wu, Li Su, and Qingming Huang. 2019. Cascaded partial decoder for fast\nand accurate salient object detection. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition . 3907â€“3916.\n[75] Yutong Xie, Jianpeng Zhang, Chunhua Shen, and Yong Xia. 2021. CoTr: Efficiently\nBridging CNN and Transformer for 3D Medical Image Segmentation. arXiv\npreprint arXiv:2103.03024 (2021).\n[76] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. 2021. Co-Scale Conv-\nAttentional Image Transformers. arXiv preprint arXiv:2104.06399 (2021).\n[77] Yifan Xu, Weijian Xu, David Cheung, and Zhuowen Tu. 2021. Line segment detec-\ntion using transformers without edges. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition . 4257â€“4266.\n[78] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi\nFeng, and Shuicheng Yan. 2021. Tokens-to-token vit: Training vision transformers\nfrom scratch on imagenet. arXiv preprint arXiv:2101.11986 (2021).\n[79] Jin Zeng, Yanfeng Tong, Yunmu Huang, Qiong Yan, Wenxiu Sun, Jing Chen, and\nYongtian Wang. 2019. Deep surface normal estimation with hierarchical rgb-d\nfusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. 6153â€“6162.\n[80] Jing Zhang, Deng-Ping Fan, Yuchao Dai, Saeed Anwar, Fatemeh Sadat Saleh,\nTong Zhang, and Nick Barnes. 2020. UC-Net: uncertainty inspired rgb-d saliency\ndetection via conditional variational autoencoders. InProceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition . 8582â€“8591.\n[81] Miao Zhang, Weisong Ren, Yongri Piao, Zhengkun Rong, and Huchuan Lu. 2020.\nSelect, Supplement and Focus for RGB-D Saliency Detection. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition . 3472â€“3481.\n[82] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and\nJianfeng Gao. 2021. Multi-Scale Vision Longformer: A New Vision Transformer\nfor High-Resolution Image Encoding. arXiv preprint arXiv:2103.15358 (2021).\n[83] Pingping Zhang, Wei Liu, Dong Wang, Yinjie Lei, Hongyu Wang, and Huchuan\nLu. 2020. Non-rigid object tracking via deep multi-scale spatial-temporal dis-\ncriminative saliency maps. Pattern Recognition 100 (2020), 107130.\n[84] Yundong Zhang, Huiye Liu, and Qiang Hu. 2021. Transfuse: Fusing transformers\nand cnns for medical image segmentation.arXiv preprint arXiv:2102.08005 (2021).\n[85] Zhao Zhang, Zheng Lin, Jun Xu, Wen-Da Jin, Shao-Ping Lu, and Deng-Ping\nFan. 2021. Bilateral attention network for RGB-D salient object detection. IEEE\nTransactions on Image Processing 30 (2021), 1949â€“1961.\n[86] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, and Vladlen Koltun. 2020. Point\ntransformer. arXiv preprint arXiv:2012.09164 (2020).\n[87] Jiaojiao Zhao, Xinyu Li, Chunhui Liu, Shuai Bing, Hao Chen, Cees GM Snoek,\nand Joseph Tighe. 2021. TubeR: Tube-Transformer for Action Detection. arXiv\npreprint arXiv:2104.00969 (2021).\n[88] Jiawei Zhao, Yifan Zhao, Jia Li, and Xiaowu Chen. 2020. Is depth really neces-\nsary for salient object detection?. In Proceedings of the 28th ACM International\nConference on Multimedia . 1745â€“1754.\n[89] Jia-Xing Zhao, Yang Cao, Deng-Ping Fan, Ming-Ming Cheng, Xuan-Yi Li, and\nLe Zhang. 2019. Contrast prior and fluid pyramid integration for RGBD salient\nobject detection. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition . 3927â€“3936.\n[90] Xiaoqi Zhao, Lihe Zhang, Youwei Pang, Huchuan Lu, and Lei Zhang. 2020. A\nsingle stream network for robust and real-time rgb-d salient object detection. In\nEuropean Conference on Computer Vision . Springer, 646â€“662.\n[91] Chuanxia Zheng, Tat-Jen Cham, and Jianfei Cai. 2021. TFill: Image Completion\nvia a Transformer-Based Architecture. arXiv preprint arXiv:2104.00845 (2021).\n[92] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao\nWang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al . 2021. Re-\nthinking semantic segmentation from a sequence-to-sequence perspective with\ntransformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition . 6881â€“6890.\n[93] Chunbiao Zhu, Xing Cai, Kan Huang, Thomas H Li, and Ge Li. 2019. PDNet:\nPrior-model guided depth-enhanced network for salient object detection. In 2019\nIEEE International Conference on Multimedia and Expo (ICME) . IEEE, 199â€“204.\n[94] Kuan Zhu, Haiyun Guo, Shiliang Zhang, Yaowei Wang, Gaopan Huang, Honglin\nQiao, Jing Liu, Jinqiao Wang, and Ming Tang. 2021. AAformer: Auto-Aligned\nTransformer for Person Re-Identification. arXiv preprint arXiv:2104.00921 (2021).",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7197067737579346
    },
    {
      "name": "Embedding",
      "score": 0.7183842658996582
    },
    {
      "name": "Computer science",
      "score": 0.6773545742034912
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5967868566513062
    },
    {
      "name": "Encoder",
      "score": 0.5618924498558044
    },
    {
      "name": "Pooling",
      "score": 0.5378016829490662
    },
    {
      "name": "RGB color model",
      "score": 0.5235716700553894
    },
    {
      "name": "Salient",
      "score": 0.4546031653881073
    },
    {
      "name": "Computer vision",
      "score": 0.4054425358772278
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4010659158229828
    },
    {
      "name": "Engineering",
      "score": 0.1637190580368042
    },
    {
      "name": "Voltage",
      "score": 0.13123533129692078
    },
    {
      "name": "Electrical engineering",
      "score": 0.0764203667640686
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I143868143",
      "name": "Anhui University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I39774598",
      "name": "Hefei University",
      "country": "CN"
    }
  ],
  "cited_by": 18
}