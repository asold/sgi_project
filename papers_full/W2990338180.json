{
    "title": "Blending Acoustic and Language Model Predictions for Automatic Music Transcription",
    "url": "https://openalex.org/W2990338180",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A5074130386",
            "name": "Adrien Ycart",
            "affiliations": [
                "Queen Mary University of London"
            ]
        },
        {
            "id": "https://openalex.org/A5002469859",
            "name": "Andrew McLeod",
            "affiliations": [
                "Kyoto University"
            ]
        },
        {
            "id": "https://openalex.org/A5084672392",
            "name": "Emmanouil Benetos",
            "affiliations": [
                "Queen Mary University of London"
            ]
        },
        {
            "id": "https://openalex.org/A5067956319",
            "name": "Kazuyoshi Yoshii",
            "affiliations": [
                "Kyoto University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2572771584",
        "https://openalex.org/W2144414181",
        "https://openalex.org/W2118992863",
        "https://openalex.org/W2162911105",
        "https://openalex.org/W2407685581",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2110007838",
        "https://openalex.org/W648786980",
        "https://openalex.org/W89231944",
        "https://openalex.org/W2950335938",
        "https://openalex.org/W2198584637",
        "https://openalex.org/W1991133427",
        "https://openalex.org/W92398308"
    ],
    "abstract": "In this paper, we introduce a method for converting an input probabilistic piano roll (the output of a typical multi-pitch detection model) into a binary piano roll. The task is an important step for many automatic music transcription systems with the goal of converting an audio recording into some symbolic format. Our model has two components: an LSTM-based music language model (MLM) which can be trained on any MIDI data, not just that aligned with audio; and a blending model used to combine the probabilities of the MLM with those of the input probabilistic piano roll given by an acoustic multi-pitch detection model, which must be trained on (a comparably small amount of) aligned data. We use scheduled sampling to make the MLM robust to noisy sequences during testing. We analyze the performance of our model on the MAPS dataset using two different timesteps (40ms and 16th-note), comparing it against a strong baseline hidden Markov model with a training method not used before for the task to our knowledge. We report a statistically significant improvement over HMM decoding in terms of notewise F-measure with both timesteps, with 16th note timesteps improving further compared to 40ms timesteps.",
    "full_text": null
}