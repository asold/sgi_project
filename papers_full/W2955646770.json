{
  "title": "Sharing Attention Weights for Fast Transformer",
  "url": "https://openalex.org/W2955646770",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5100600701",
      "name": "Tong Xiao",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A5083496566",
      "name": "Yinqiao Li",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A5100370155",
      "name": "Jingbo Zhu",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A5100619287",
      "name": "Zhengtao Yu",
      "affiliations": [
        "Kunming University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5012126405",
      "name": "Tongran Liu",
      "affiliations": [
        "Institute of Psychology, Chinese Academy of Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2594990650",
    "https://openalex.org/W2113104171",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2962944188",
    "https://openalex.org/W2963928591",
    "https://openalex.org/W2949335953",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2963842982",
    "https://openalex.org/W2947000318",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2463507112",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2763421725",
    "https://openalex.org/W2146950091",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2751936342",
    "https://openalex.org/W2799001369",
    "https://openalex.org/W1821462560"
  ],
  "abstract": "Recently, the Transformer machine translation system has shown strong results by stacking attention layers on both the source and target-language sides. But the inference of this model is slow due to the heavy use of dot-product attention in auto-regressive decoding. In this paper we speed up Transformer via a fast and lightweight attention model. More specifically, we share attention weights in adjacent layers and enable the efficient re-use of hidden states in a vertical manner. Moreover, the sharing policy can be jointly learned with the MT model. We test our approach on ten WMT and NIST OpenMT tasks. Experimental results show that it yields an average of 1.3X speed-up (with almost no decrease in BLEU) on top of a state-of-the-art implementation that has already adopted a cache for fast inference. Also, our approach obtains a 1.8X speed-up when it works with the \\textsc{Aan} model. This is even 16 times faster than the baseline with no use of the attention cache.",
  "full_text": "arXiv:1906.11024v1  [cs.CL]  26 Jun 2019\nSharing Attention W eights for Fast T ransformer\nT ong Xiao1,2 , Yinqiao Li1 , Jingbo Zhu1,2 , Zhengtao Y u3 and T ongran Liu4\n1Northeastern University, Shenyang, China\n2NiuTrans Co., Ltd., Shenyang, China\n3Kunming University of Science and T echnology, Kunming, Chi na\n4CAS Key Laboratory of Behavioral Science, Institute of Psyc hology, CAS, Beijing, China\n{xiaotong, zhujingbo }@mail.neu.edu.cn, {li.yin.qiao.2012, ztyu }@hotmail.com, liutr@psych.ac.cn\nAbstract\nRecently, the Transformer machine translation sys-\ntem has shown strong results by stacking atten-\ntion layers on both the source and target-language\nsides. But the inference of this model is slow\ndue to the heavy use of dot-product attention in\nauto-regressive decoding. In this paper we speed\nup Transformer via a fast and lightweight atten-\ntion model. More speciﬁcally, we share attention\nweights in adjacent layers and enable the efﬁ-\ncient re-use of hidden states in a vertical manner.\nMoreover, the sharing policy can be jointly learned\nwith the MT model. W e test our approach on ten\nWMT and NIST OpenMT tasks. Experimental re-\nsults show that it yields an average of 1.3X speed-\nup (with almost no decrease in BLEU) on top of\na state-of-the-art implementation that has already\nadopted a cache for fast inference. Also, our ap-\nproach obtains a 1.8X speed-up when it works with\nthe A A N model. This is even 16 times faster than\nthe baseline with no use of the attention cache.\n1 Introduction\nIn recent years, neural models have led to great improvement s\nin machine translation (MT). Approaches of this kind make it\npossible to learn good mappings between sequences via deep\nnetworks and attention mechanisms [Sutskever et al. , 2014;\nBahdanau et al. , 2015; Luong et al. , 2015 ]. Recent work has\nexplored an architecture that just consists of stacked at-\ntentive and feed-forward networks (call it Transformer)\n[V aswani et al. , 2017 ]. It makes use of multi-layer dot-\nproduct attention to capture the dependency among language\nunits. Beyond this, training this kind of model is fast becau se\nwe can parallelize computation over all positions of the se-\nquence on modern graphics processing units (GPUs). These\nproperties make Transformer popular in recent MT evalua-\ntions and industrial deployments.\nHowever, standard implementations of Transformer are\nprone to slow inference though fast in training. At test time ,\nthe system produces one target word each time until an end\nsymbol is reached. This process is auto-regressive and slow\nbecause we have to run dot-product attention for each posi-\ntion rather than batching the computation of the sequence.\nThe situation is even worse if 6 or more attention layers are\nstacked and the attention model occupies the inference time .\nT o address this issue, efﬁcient networks have been investi-\ngated. For example, one can replace dot-product attention\nwith additive attention and use average attention models in -\nstead [Zhang et al. , 2018 ], or explore non-autoregressive de-\ncoders that beneﬁt from the trick of batched matrix operatio ns\nover the entire sequence [Gu et al. , 2018 ]. But these methods\neither lose the explicit model of word dependencies, or re-\nquire complicated networks that are hard to train.\nIn this work, we observe that the attention model shares a\nsimilar distribution among layers in weighting different p osi-\ntions of the sequence. This experience lead us to study the is -\nsue in another line of research, in which we reduce redundant\ncomputation and re-use some of the hidden states in the atten -\ntion network. W e propose a method to share attention weights\nin adjacent layers (call it shared attention network, or S A N for\nshort). It leads to a model that shares attention computatio n\nin the stacked layers vertically. In addition to the new arch i-\ntecture, we develop a joint method to learn sharing policies\nand MT models simultaneously. As another “bonus”, S A N\nreduces the memory footprint because some hidden states are\nkept in the same piece of memory.\nSA N is simple and can be implemented in a few hours by\nanyone with an existing kit of Transformer. Also, it is ortho g-\nonal to previous methods and is straightforwardly applicab le\nto the variants of Transformer. W e test our approach in a stat e-\nof-the-art system where an attention cache is already in use\nfor speed-up. Experimental results on ten WMT and NIST\nOpenMT tasks show an average of 1.3X speed-up with al-\nmost no decrease in BLEU. More interestingly, it obtains a\nbigger speed-up (1.8X) when working with the A A N model.\nThe best result is 16 times faster than the baseline where no\ncache is adopted.\n2 The T ransformer System\nThe Transformer system follows the popular encoder-decode r\nparadigm. On the encoder side, there are a number of iden-\ntical stacked layers. Each of them is composed of a self-\nattention sub-layer and a feed-forward sub-layer. In Trans -\nformer, the attention model is scaled dot-product attentio n.\nLet l be the length of the source sequence. The input of the\nattention sub-layer is a tuple of (Q, K, V ), where Q ∈ Rl× dk ,\nK ∈ Rl× dk and V ∈ Rl× dv are the matrices of queries, keys,\nSelf-Attention\n...\nEnc-Dec Attention\n...\nPrevious Layer\nFollowing Layer\nEncoder Output\nEq. (1): Eq. (2):\nQuery(Q) Key(K) T\n×\nS = Softmax √ dk\n( )\nV alue(V)W eight(S)\n×A =\nA column of S represents a distribution over all positions\nS[0]\nSelf-Attention A = Softmax( Q·KT\n√\ndk\n) ·V\nFigure 1: Decoder-side attention sub-layers in Transforme r\nand values packed over the sequence. In self-attention, we\nﬁrst compute the dot-product of queries and keys, followed\nby the rescaling and softmax operations.\nS = Softmax(Q ·KT\n√ dk\n) (1)\nS is an l × l matrix, where entry (i, j) represents the strength\nof connecting position i with position j. Note that S is es-\nsentially a weight (or scalar) matrix where every column rep -\nresents a distribution. The output of self-attention is sim ply\ndeﬁned as the weighted sum of values:\nA = S ·V (2)\nHere Q, K and V are generated from the same source with a\nlinear transformation. The self-attention result is then f ed into\na fully connected feed-forward network (FFN).\nThe decoder shares a similar structure with the encoder.\nApart from the self-attention sub-layer, an encoder-decod er\nattention sub-layer is introduced to model the corresponde nce\nbetween source positions and target positions. Basically, the\nencoder-decoder attention has the same form as Eqs. (1) and\n(2), where the queries come from the output of the previous\nlayer, and the keys and values come from the output of the\nencoder. See Figure 1 for an illustration of the attention mo del\nused in Transformer.\nNote that the matrix multiplications in Eqs. (1) and (2) are\ntime consuming. It is a bigger problem for inference because\nEqs. (1) and (2) repeat for each position until we ﬁnish the\ngeneration.\n3 Shared Attention Networks\nIn this work we speed up the decoder-side attention because\nthe decoder is the heaviest component in Transformer.\n3.1 Attention W eights\nSelf-attention is essentially a procedure that fuses the in put\nvalues to form a new value at each position. Let S[i] be col-\numn i of weight matrix S. For position i , we ﬁrst compute\nS[i] to weight all positions (as in Eq. (1)), and then compute\nthe weighted sum of values by S[i] (as in Eq. (2)). In col-\numn vector S[i], element Si,j indicates the contribution that\nwe fuse the value at position j to position i. Intuitively, the\n1 2 3 4 5 6\n1\n2\n3\n4\n5\n6\n(a) Self-Attention\n1 2 3 4 5 6\n1\n2\n3\n4\n5\n6\n(b) Enc-Dec Attention\nFigure 2: The Jensen-Shannon divergence of the attention we ights\nfor every pair of layers on the WMT14 English-German task (a d ark\ncell means the distributions are similar)\nattention weight S[i] should not be volatile in different levels\nof language representation because the correlations betwe en\npositions somehow reﬂect the dependency of language units.\nFor example, for an English sentence, the subject and the ver b\ncorrelate well no matter how many layers we make on top of\nthe input sequence. On the other hand, the subject and the ad-\nverbial may not have a big impact to each other in all stacked\nlayers.\nT o verify this, we compute the Jensen-Shannon (JS) di-\nvergence to measure how the attention weight distribution\nof a layer is different from another [Lin, 1991 ]. W e choose\nthe JS divergence because it is symmetric and bounded. For\nmulti-head attention, we regard different heads as separat e\nchannels. W e compute the JS score for each individual head\nand then average them for ﬁnal output. Figure 2 shows that\nthe system generates similar weights over layers. For self-\nattention, layers 2-6 almost enjoy the same weight distribu -\ntion. For encoder-decoder attention, we observe a larger va ri-\nance but good similarities still exist among two or three ad-\njacent layers (see entries around the diagonal of Figure 2(b )).\nAll these show the possibility of removing redundant compu-\ntation in Transformer.\n3.2 The Model\nAn obvious next step is to develop a faster attention model\nthat makes efﬁcient re-use of the states in Eqs. (1) and (2),\ninstead of computing everything on the ﬂy. In this work we\nAttention\n· · ·\nQn Kn V n\nSn =S(Qn ·Kn)\nAn =Sn ·V Attention\n· · ·\nQn Kn V n\nSn =Sm\nAn =Sn ·V Attention\n· · ·\nQn Kn V n\nSn\nAn =Am\nAttention\n· · ·\nQm Km V m\nSm =S(Qm ·Km)\nAm =Sm ·V Attention\n· · ·\nQm Km V m\nSm =S(Qm ·Km)\nAm =Sm ·V Attention\n· · ·\nQm Km V m\nSm =S(Qm ·Km)\nAm =Sm ·V\nLayer n=m+i\nLayer m\n· · ·\n· · ·\n· · ·\nLayer n=m+i\nLayer m\n· · ·\n· · ·\n· · ·\nLayer n=m+i\nLayer m\n· · ·\n· · ·\n· · ·\n(a) Standard Transformer Attention (b) S A N Self-Attention (c) S A N Encoder-Decoder Attention\nFigure 3: Comparison of the standard attention model and the SA N model\npresent a shared attention network (S A N) to share weight ma-\ntrix S for adjacent layers. The idea is that we just compute\nthe weight matrix once and reuse it for upper-level layers.\nHere we describe S A N for both the self-attention and encoder-\ndecoder attention models.\n• SA N Self-Attention. W e deﬁne the self-attention weight\nmatrix in layer m as:\nSm = s(Qm, Km) (3)\nwhere s(·, ·) is the function described in Eq. (1), Qm and\nKm are the inputs, and Sm is the attention weight for the\noutput. In S A N, we can share Sm with the layers above\nm, like this\nSm+i = s(Qm, Km) (4)\nfor i ∈ [1, π − 1]\nwhere π indicates how many layers share the same at-\ntention weights. For example, in a 6-layer decoder, we\ncan share the self-attention weights for every two lay-\ners ( π = 2 ), or share the weights for the ﬁrst two lay-\ners ( π1 = 2 ) and let the remaining 4 layers use another\nweights ( π2 = 4 ). W e discuss the sharing strategy in the\nlater part of the section.\n• SA N Encoder-Decoder Attention . For encoder-\ndecoder attention, we do the same thing as in self-\nattention, but with a trick for further speed-up. In\nencoder-decoder attention, keys and values are from the\noutput of the encoder, i.e., K and V have already been\nshared among layers on the decoder side. In response,\nwe can share A = S ·V for encoder-decoder attention\nlayers. This can be described as\nAm+i = Am\n= Sm ·V (5)\nfor i ∈ [1, π − 1]\nwhere Am is the attention output of layer m, V is the\ncontext representation generated by the encoder. See\nFigure 3 for a comparison of the standard attention\nmodel and S A N.\nIn addition to system speed-up, S A N also reduces the mem-\nory footprint. In S A N, we just need one data copy of weight\nmatrix for a layer block, rather than allocating memory spac e\nfor every layer. Moreover, the linear transformation of the\ninput (i.e., Q and K) can be discarded when the attention\nweights come from another layer. It reduces both the number\nof model parameters and the memory footprint in inference.\nAnother note on S A N. S A N is a process that simpliﬁes\nthe model and re-uses hidden states in the network. It is\ndoing something similar to systems that share model pa-\nrameters in different levels of the network [Wu et al. , 2016;\nY ang et al. , 2018; Luong et al. , 2016 ]. Such methods have\nbeen proven to improve the robustness of neural models on\nmany natural language processing tasks. Sharing parameter s\nand/or hidden states can reduce the model complexity. Previ -\nous work has pointed out that MT systems cannot beneﬁt a lot\nfrom very deep and complex networks [V aswani et al. , 2017;\nBritz et al. , 2017 ]. S A N might alleviate this problem and\nmakes it easier to train neural MT models. For example, in our\nexperiments, we see that S A N can improve translation quality\nin some cases in addition to considerable speed-ups.\n3.3 Learning to Share\nThe remaining issue is how to decide which layers can be\nshared. A simple way is to use the same setting of π for the\n1: Function LE A RN TOSH A RE (layers, model)\n2: while policy {πi} does change do\n3: learn a new model given policy {πi}\n4: learn a new policy {πi} on layers given model\n5: return {πi} & model\nFigure 4: Joint learning of MT models and sharing policies\nentire layer stack, and tune it on a development set. For ex-\nample, we can try to share weights on layer blocks consisting\nof two layers, or three layers, or all layers ( π = 2 , or 3 , ...),\nand use the tuned π on test data.\nBut a uniform sharing strategy might not be optimal be-\ncause we need to control the degree of sharing in difference\nlevels of the network. For example, for the case in Figure\n2(a), a good choice is to share weights for layers 2-6 and\nleave layer 1 as it is. Here we present a method that learns\nthe sharing strategy in a dynamic way. T o do this, we choose\nln(2)− JS divergence as the measure of the similarity be-\ntween weights of layer i and layer j (denoted as µ(i, j)).\nGiven a layer block ranging from layer m to layer n (denoted\nas b(m, n)), the similarity over the block is deﬁned as\nsim(m, n) =\n∑ n\ni=m\n∑ n\nj=m(1 − δ(i, j))µ(i, j)\n(n − m + 1) ·(n − m) (6)\nwhere n− m+1 is the size of the block, and δ(i, j) is the Kro-\nnecker delta function. sim(m, n) measures how the weight of\na layer is similar to that of another layer in block b(m, n). W e\ncan do sharing when sim(m, n) ≥ θ where θ is the parameter\nthat controls how often a layer is shared.\nW e begin with layer 1 and search for the biggest block\nthat satisﬁes the criterion. This process repeats until all the\nlayers are considered, resulting in N layer blocks. For sim-\nplicity, we use {π1, ..., πN } (or {πi}) to represent the blocks\nin a bottom-up manner (call it sharing policy), where πi is\nthe size of block i. Obviously, for an M-layer stack we have∑ N\ni=1 πi = M.\nOnce we have a sharing policy, we need to re-train the MT\nmodel. It in turn leads to new attention weights and possibly\na better policy. A desirable way is to continue learning unti l\nthe model converges. T o this end, we present a joint learning\nmethod that trains MT models and sharing policies simulta-\nneously (Figure 4). In the method, MT training and policy\nlearning loops for iterations, and the result of the ﬁnal rou nd\nis returned when there is no new update of the model.\n4 Experiments\nW e experimented with our approach on WMT and NIST\ntranslation tasks.\n4.1 Experimental Setup\nThe bilingual and evaluation data came from three sources\n• WMT14 (En-De). W e used all bilingual data provided\nwithin the WMT14 English-German task. W e chose\nnewstest 2013 as the tuning data, and newstest 2014 as\nthe test data.\nSource Lang. Train Tune T est\nsent. word sent. word sent. word\nWMT14 En-De 4.5M 225M 3000 130K 3003 133K\nWMT17\nEn-De 5.9M 276M 8171 356K 3004 128K\nDe-En 3004 128K\nEn-Fi 2.6M 108M 8870 330K 3002 110K\nFi-En 3002 110K\nEn-Lv 4.5M 115M 2003 90K 2001 88K\nLv-En 2001 88K\nEn-Ru 25M 1.2B 8819 391K 3001 132K\nRu-En 3001 132K\nNIST12 Zh-En 1.9M 85M 1164 227K 1357 198K\nT able 1: Data statistics (# of sentences and # of words)\n• WMT17 (En-De, De-En, En-Fi, Fi-En, En-Lv, Lv-En,\nEn-Ru and Ru-En). W e followed the standard data set-\nting of the bidirectional translation tasks of German-\nEnglish, Finnish-English, Latvian-English, and Russian-\nEnglish. For tuning, we concatenated the data of new-\nstest 2014-2016. For test, we chose newstest 2017.\n• NIST12 (Zh-En). W e also used parts of the bitext of\nNIST OpenMT12 to train a Chinese-English system 1 .\nThe tuning and test sets were MT06 and MT08.\nFor Chinese, all sentences were word segmented\nby the segmentation system in the NiuTrans toolkit\n[Xiao et al. , 2012 ]. For other languages, we ran the ofﬁcial\nscript of WMT for tokenization. All sentences of more\nthan 50 words were removed for the NIST Zh-En task, and\nsentences of more than 80 words were removed for the WMT\ntasks. For all these tasks, sentences were encoded using\nbyte-pair encoding, where we used a shared source target\nvocabulary of 32K tokens. See T able 1 for statistics of the\ndata.\nW e used standard implementation of Transformer. Early\nversions of its inference system simply compute the attenti on\noutput for target positions individually. This way is strai ght-\nforward but with a double counting problem. For a stronger\nbaseline, we chose the system with an attention cache that\nkept the attention output of previous positions in cache and\nre-used it in following steps.\nThe Transformer system used in our experiments consisted\nof a 6-layer encoder and a 6-layer decoder. By default, we\nset dk = dv = 512 and used 2,048 hidden units in the\nFFN sub-layers. W e used multi-head attention (8 heads) be-\ncause it was shown to be effective for state-of-the-art perf or-\nmance [V aswani et al. , 2017 ]. Dropout (rate = 0 .1) and la-\nbel smoothing ( ǫls = 0 .1) methods were adopted for regular-\nization and stabilizing the training [Szegedy et al. , 2016 ]. W e\ntrained the model using Adam with β1 = 0 .9, β2 = 0 .98,\nand ǫ = 10 − 9 [Kingma and Ba, 2015 ]. The learning rate\nwas scheduled as described in [V aswani et al. , 2017 ]: lr =\nd− 0.5 ·min(t− 0.5, t ·4k− 1.5), where t is the step number.\nAll models were trained for 100k steps with a mini-batch of\n4,096 tokens on machines with 8 Nvidia 1080Ti GPUs except\nEn-Fi (60k steps), Fi-En (60k steps) and Zh-En (24k steps).\n1 LDC2000T46, LDC2000T47, LDC2000T50, LDC2003E14,\nLDC2005T10, LDC2002E18, LDC2007T09 and LDC2004T08\nSource Language Model BLEU ∆ BLEU Speed ∆ Speed\nWMT14 En-De Baseline 27.52 0.00 1.03K 0.00%\nSA N 27.69 +0.17 1.44K +39.81%\nWMT17\nEn-De Baseline 28.90 0.00 1.03K 0.00%\nSA N 28.82 -0.08 1.43K +38.82%\nDe-En Baseline 34.57 0.00 0.99K 0.00%\nSA N 34.75 +0.18 1.38K +39.19%\nEn-Fi Baseline 21.80 0.00 1.02K 0.00%\nSA N 21.45 -0.35 1.36K +33.82%\nFi-En Baseline 24.94 0.00 1.02K 0.00%\nSA N 25.25 +0.31 1.25K +23.28%\nEn-Lv Baseline 15.80 0.00 0.94K 0.00%\nSA N 16.08 +0.28 1.31K +39.01%\nLv-En Baseline 18.06 0.00 0.92K 0.00%\nSA N 17.97 -0.09 1.26K +36.28%\nEn-Ru Baseline 29.93 0.00 1.02K 0.00%\nSA N 29.51 -0.42 1.29K +26.17%\nRu-En Baseline 33.63 0.00 1.01K 0.00%\nSA N 33.36 -0.27 1.23K +21.17%\nNIST12 Zh-En Baseline 38.59 0.00 0.84K 0.00%\nSA N 38.19 -0.40 1.02K +21.34%\nA verage Baseline 27.37 0.00 0.98K 0.00%\nSA N 27.31 -0.07 1.30K +31.98%\nT able 2: BLEU scores (%) and translation speeds (token/sec) on the\nWMT and NIST tasks\nEvery model was ensembled from the 5 latest checkpoints in\ntraining. For inference, both beam search and batch decodin g\nmethods were used (beam size = 4 and batch size = 16).\nFor our approach, we applied S A N to self-attention and\nencoder-decoder sub-layers on the decoder side. W e learned\nsharing policies as in Figure 4. θ was tuned on the tuning\ndata, which resulted in an optimal range of [0.3, 0.4] for self-\nattention and [0.4, 0.5] for encoder-decoder attention.\n4.2 Results\nW e report the translation quality (in BLEU[%]) and speed (in\ntoken/sec) on all ten of the tasks (T able 2). W e see, ﬁrst of al l,\nthat S A N signiﬁcantly improves the translation speed for all\nthese languages. The average speed-up is 1.3X. Also, there\nis a very modest BLEU decrease, but not signiﬁcant. These\nresults indicate that S A N is robust and can improve a strong\nbaseline on a wide range of translation tasks. Another inter -\nesting ﬁnding here is that the speed improvement on En-Ru,\nRu-En and Zh-En is not as large as that on other language\npairs. This is because we share fewer layers (i.e., larger θ) on\nthese tasks to preserve good BLEU scores. Note that Russian\nand Chinese are very difﬁcult languages for translation, an d\nwe need a complicated network to model the structure diver-\ngence. Less sharing is preferred to keep the expressive powe r\nfor these languages.\nT o modulate the degree of sharing, we study the system\nbehavior under different settings of θ (T able 3) . For compar-\nison, we report the result of uniform {πi}. Due to the limited\nspace, we present the result on the WMT14 En-De task in the\nfollowing sensitivity analysis. For uniform sharing, {πi} is\nset to {6} for self-attention and {3,3} for encoder-decoder at-\ntention. This results in a promising speed-up (see entry of u ni-\nform {πi}). When we switch to joint learning of MT models\nModel θ BLEU ∆ BLEU Speed ∆ SpeedSelf Enc-Dec\nBaseline N/A 27.52 0.00 1.03K 0.00%\nSA N\nuniform {πi} 27.58 +0.06 1.43K +38.83%\n0.30 0.40 26.89 -0.63 1.55K +50.26%\n0.30 0.50 27.69 +0.17 1.44K +39.81%\n0.40 0.40 26.96 -0.56 1.52K +47.32%\n0.40 0.50 27.46 -0.06 1.40K +36.33%\nT able 3: BLEU scores (%) and translation speeds (token/sec) for\ndifferent sharing policies. Self = self-attention. Enc-De c = encoder-\ndecoder attention\nModel Shared V BLEU ∆ BLEU Speed ∆ Speed\nBaseline - 27.52 0.00 1.03K 0.00%\nSA N no 27.49 -0.03 1.14K +10.96%\nyes 1.27K +22.91%\nT able 4: BLEU scores (%) and translation speeds (token/sec)\nwith/without a shared context ( V ) for encoder-decoder layers\nand sharing policies, we obtain further improvements in bot h\nBLEU and speed. More interestingly, we see that the system\nprefers a smaller θ for self-attention than the encoder-decoder\ncounterpart. This is reasonable because the encoder-decod er\nattention captures the correspondence of two languages and\nneeds more states in modeling than a single language. On\nthe other hand, the BLEU improvements indicate that the MT\nsystem can beneﬁt from simpliﬁed models. It gives a directio n\nthat we explore simpler models for better training of neural\nMT systems.\nIn encoder-decoder attention, we share the context V gen-\nerated by the encoder for further speed-ups (see Figure 3(c) ).\nIt is therefore worth a study on how much this method can\naccelerate the system. T able 4 shows that sharing the con-\ntext contributes half of the speed improvement. This agrees\nwith our design that weight sharing is more beneﬁcial to the\ndecoder because attention is heavier on the decoder side. An -\nother interesting question is whether S A N can improve the\nsystem on the encoder side. T o seek an answer, we apply S A N\nto the encoder-side self-attention sub-layers and see smal l\nspeed improvements (T able 5). This result conﬁrms the pre-\nvious report that the decoder occupies the inference time an d\nthe encoder is light [Zhang et al. , 2018 ].\nAlso, we plot the translation speed as functions of beam\nsize and BLEU score. Figure 5 shows a consistent improve-\nment under different beam settings. Moreover, S A N beneﬁts\nmore from larger beam sizes. For example, the speed-up of\nbeam = 20 is larger than that of beam = 4 (1.48x vs. 1.40x).\nThe Speed-vs-BLEU curves indicate a good ability of S A N in\ntrading off between translation quality and speed.\nIn addition, we empirically compare S A N with\nother variants of the attention model, including A A N\n[Zhang et al. , 2018 ] and the model with no cache. T able\n6 shows the attention cache plays an important role in\nfast inference. It leads to an 8-fold speed-up on top of the\nimplementation where no cache is used. Also, S A N obtains a\nbigger speed improvement than A A N. This might be because\nAA N is used for self-attention only, while S A N is applicable\nModel BLEU ∆ BLEU Speed ∆ Speed\nBaseline 27.52 0.00 1.03K 0.00%\nSA N 27.51 -0.01 1.05K +1.94%\nT able 5: BLEU scores (%) and translation speeds (token/sec) of the\nsystems that use S A N on the encoder side\nModel BLEU ∆ BLEU Speed ∆ Speed\nBaseline 27.52 0.00 1.03K 0.00%\nBaseline-Cache 27.52 0.00 0.12K -88.65%\nBaseline+A A N 27.51 -0.01 1.38K +34.37%\nBaseline+S A N 27.69 +0.17 1.44K +39.81%\nBaseline+A A N+S A N 27.19 -0.33 1.87K +81.61%\nT able 6: Comparison of different attention models\nto both self-attention and encoder-decoder attention. Fin ally,\nwe combine A A N and S A N in a new system where A A N\nis applied to self-attention and S A N is applied to encoder-\ndecoder attention. It yields the best result which is 1.8 tim es\nfaster than the baseline, and almost 16 times faster than the\nsystem without cache.\nIn training, we observe that systems tend to learn similar\nattention weights. Figure 6 plots the JS divergence between\nlayer 4 and layers 5-6 at different training steps. The JS di-\nvergence curves go down signiﬁcantly as the training pro-\nceeds. Adjacent layers show more similar weight distribu-\ntions than distant layers. The fast convergence in JS diver-\ngence can speed up the iterative training. For example, for\neach training epoch (Figure 4), one can train the model for\na shorter time, as the JS divergence among layers converges\nquickly. Thus, the system can ﬁnd the optimal sharing policy\nmore efﬁciently. In addition, we ﬁnd that the training likel i-\nhood of S A N is higher than that of the baseline, but not sig-\nniﬁcant.\n5 Related W ork\nIt has been observed that attention models are\ncritical for state-of-the-art results on many MT\ntasks [Bahdanau et al. , 2015; Wu et al. , 2016;\nV aswani et al. , 2017 ]. Several research groups have in-\nvestigated attentive methods for different architectures of\nneural MT . The earliest is [Luong et al. , 2015 ]. They intro-\nduced an additive attention model into MT systems based\non recurrent neural networks (RNNs). More recently, multi-\nlayer attention was successfully applied to convolutional\nneural MT systems [Gehring et al. , 2017 ] and Transformer\nsystems [V aswani et al. , 2017 ]. In particular, Transformer is\npopular due to its scalability on large-scale training and t he\ngood design of the architecture for implementation.\nIt is well-known that Transformer suffers from a high infer-\nence cost which makes it slower than the RNN-based coun-\nterpart. This partially due to the auto-regressive propert y of\ndecoding, and partially due to the heavy use of dot-product\nattention where the expensive matrix multiplication is fre -\nquently used. Researchers have begun to explore solutions.\nFor example, Gu et al. [2018] designed a non-autoregressive\ninference method for a Transformer-like system, which gen-\n4 8 12 16 20\n350\n950\n1,550\nBeam Size\nSpeed\nBaseline\nSAN\n27.5 27 .65 27 .8\n350\n950\n1,550\n2,150\nBLEU\nSpeed\nBaseline\nSAN\nFigure 5: Translation speed (token/sec) vs beam size and BLE U (%)\n10K 40K 70K 100K\n0.16\n0.26\n0.36\n0.46\n# of Training Steps\n(Self-Attention)\nJS Divergence\nL5\nL6\n10K 40K 70K 100K\n0.24\n0.28\n0.32\n# of Training Steps\n(Enc-Dec Attention)\nJS Divergence\nL5\nL6\nFigure 6: JS divergence vs number of training steps\nerated the entire target sequence at one time. This model is\nfast but is not easy to train.\nIn another line of research, Zhang et al. [2018] proposed\nthe average attention network (A A N) and applied it to self-\nattention sub-layers on the decoder side with no cache. In\nthis work, we study the issue on a strong baseline that has\nalready used an attention cache for a reasonable inference\nspeed. Also, our approach is straightforwardly applicable to\nsystems with multi-layer attention. W e improve both the sel f-\nattention and encoder-decoder attention components, whic h\nhas not been studied in previous work.\nIn neural MT , fast inference methods have been in-\nvestigated for years. These include vocabulary selection\n[L ’Hostis et al. , 2016; Sankaran et al. , 2017 ], knowledge\ndistillation [Hinton et al. , 2015; Kim and Rush, 2016 ],\nlow-precision computation [Micikevicius et al. , 2018;\nQuinn and Ballesteros, 2018 ], recurrent stacked networks\n[Dabre and Fujita, 2019 ] and etc. Our method is orthogonal\nto them. Previous studies focus more on the reduction of\nmodel size and robust training, rather than fast inference.\nHere we study the issue in the context of speeding up\nattentive MT and conﬁrm the effectiveness of this kind of\nmodels.\n6 Conclusions\nW e have presented a shared attention network (S A N) for fast\ninference of Transformer. It shares attention weights amon g\nlayers for both self-attention and encoder-decoder attent ion in\na vertical manner. The policy of sharing can be jointly learn ed\nwith the MT model, rather than being determined heuristi-\ncally. Moreover, S A N reduces the memory footprint. Exper-\niments on ten MT tasks show that S A N yields a speed-up of\n1.3X over a strong baseline that has already used an attentio n\ncache. More interestingly, it is observed that the combinat ion\nof S A N and A A N obtains a larger speed improvement. The\nsystem is 16X faster than the baseline with no cache.\nAcknowledgments\nThis work was supported in part by the National Sci-\nence Foundation of China (Nos. 61732005, 61876035 and\n61432013) and the Fundamental Research Funds for the Cen-\ntral Universities (No. N181602013).\nReferences\n[Bahdanau et al. , 2015 ] Dzmitry Bahdanau, Kyunghyun\nCho, and Y oshua Bengio. Neural machine translation\nby jointly learning to align and translate. In In Proceed-\nings of the 3rd International Conference on Learning\nRepresentations, 2015.\n[Britz et al. , 2017 ] Denny Britz, Anna Goldie, Minh-Thang\nLuong, and Quoc Le. Massive exploration of neural ma-\nchine translation architectures. In Proceedings of the\n2017 Conference on Empirical Methods in Natural Lan-\nguage Processing , pages 1442–1451, Copenhagen, Den-\nmark, September 2017.\n[Dabre and Fujita, 2019 ] Raj Dabre and Atsushi Fujita. Re-\ncurrent stacking of layers for compact neural machine\ntranslation models. In Proceedings of the 33rd AAAI Con-\nference on Artiﬁcial Intelligence (AAAI) , 2019.\n[Gehring et al. , 2017 ] Jonas Gehring, Michael Auli, David\nGrangier, Denis Y arats, and Y ann N. Dauphin. Convolu-\ntional sequence to sequence learning. In Proceedings of\nthe 34th International Conference on Machine Learning,\nICML 2017, Sydney, NSW , Australia, 6-11 August 2017 ,\npages 1243–1252, 2017.\n[Gu et al. , 2018 ] Jiatao Gu, James Bradbury, Caiming\nXiong, V ictor O.K. Li, and Richard Socher. Non-\nautoregressive neural machine translation. In International\nConference on Learning Representations , 2018.\n[Hinton et al. , 2015 ] Geoffrey Hinton, Oriol V inyals, and\nJeffrey Dean. Distilling the knowledge in a neural net-\nwork. In NIPS Deep Learning and Representation Learn-\ning W orkshop, 2015.\n[Kim and Rush, 2016 ] Y oon Kim and Alexander M. Rush.\nSequence-level knowledge distillation. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2016, Austin, T exas, USA,\nNovember 1-4, 2016 , pages 1317–1327, 2016.\n[Kingma and Ba, 2015 ] Diederik P . Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations, ICLR\n2015, San Diego, CA, USA, May 7-9, 2015, Conference\nT rack Proceedings, 2015.\n[L ’Hostis et al. , 2016 ] Gurvan L ’Hostis, David Grangier, and\nMichael Auli. V ocabulary selection strategies for neural\nmachine translation. CoRR, abs/1610.00072, 2016.\n[Lin, 1991 ] Jianhua Lin. Divergence measures based on\nthe shannon entropy. IEEE T rans. Information Theory ,\n37(1):145–151, 1991.\n[Luong et al. , 2015 ] Minh-Thang Luong, Hieu Pham, and\nChristopher D. Manning. Effective approaches to\nattention-based neural machine translation. In Proceed-\nings of the 2015 Conference on Empirical Methods in Nat-\nural Language Processing , pages 1412–1421, 2015.\n[Luong et al. , 2016 ] Minh-Thang Luong, Quoc V . Le, Ilya\nSutskever, Oriol V inyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. In 4th International Con-\nference on Learning Representations, ICLR 2016, San\nJuan, Puerto Rico, May 2-4 , 2016.\n[Micikevicius et al. , 2018 ] Paulius Micikevicius, Sharan\nNarang, Jonah Alben, Gregory F . Diamos, Erich Elsen,\nDavid Garc´ ıa, Boris Ginsburg, Michael Houston, Oleksii\nKuchaiev, Ganesh V enkatesh, and Hao Wu. Mixed\nprecision training. In 6th International Conference on\nLearning Representations, ICLR 2018, V ancouver , BC,\nCanada, April 30 - May 3 , 2018.\n[Quinn and Ballesteros, 2018 ] Jerry Quinn and Miguel\nBallesteros. Pieces of eight: 8-bit neural machine transla -\ntion. In Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational\nLinguistics: Human Language T echnologies, NAACL-HTL\n2018, New Orleans, Louisiana, USA, June 1-6, 2018,\nV olume 3 (Industry P apers) , pages 114–120, 2018.\n[Sankaran et al. , 2017 ] Baskaran Sankaran, Markus Freitag,\nand Y aser Al-Onaizan. Attention-based vocabulary selec-\ntion for NMT decoding. CoRR, abs/1706.03824, 2017.\n[Sutskever et al. , 2014 ] Ilya Sutskever, Oriol V inyals, and\nQuoc V Le. Sequence to sequence learning with neural\nnetworks. In Advances in neural information processing\nsystems, pages 3104–3112, 2014.\n[Szegedy et al. , 2016 ] Christian Szegedy, V incent V an-\nhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew\nW ojna. Rethinking the inception architecture for computer\nvision. In 2016 IEEE Conference on Computer V ision and\nP attern Recognition, CVPR 2016, Las V egas, NV , USA,\nJune 27-30, 2016 , pages 2818–2826, 2016.\n[V aswani et al. , 2017 ] Ashish V aswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In Advances in Neural Information Processing Sys-\ntems, pages 6000–6010, 2017.\n[Wu et al. , 2016 ] Y onghui Wu, Mike Schuster, Zhifeng\nChen, Quoc V Le, Mohammad Norouzi, W olfgang\nMacherey, Maxim Krikun, Y uan Cao, Qin Gao, Klaus\nMacherey, et al. Google’s neural machine translation sys-\ntem: Bridging the gap between human and machine trans-\nlation. arXiv preprint arXiv:1609.08144 , 2016.\n[Xiao et al. , 2012 ] T ong Xiao, Jingbo Zhu, Hao Zhang, and\nQiang Li. NiuTrans: An open source toolkit for phrase-\nbased and syntax-based machine translation. In Proceed-\nings of the ACL 2012 System Demonstrations , pages 19–\n24, Jeju Island, Korea, July 2012.\n[Y ang et al. , 2018 ] Zhen Y ang, W ei Chen, Feng W ang, and\nBo Xu. Unsupervised neural machine translation with\nweight sharing. In Proceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics (V olume\n1: Long P apers) , pages 46–55, 2018.\n[Zhang et al. , 2018 ] Biao Zhang, Deyi Xiong, and Jinsong\nSu. Accelerating neural transformer via an average atten-\ntion network. In Proceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics (V olume\n1: Long P apers) , pages 1789–1798, 2018.",
  "topic": "NIST",
  "concepts": [
    {
      "name": "NIST",
      "score": 0.807596743106842
    },
    {
      "name": "Computer science",
      "score": 0.7822779417037964
    },
    {
      "name": "Machine translation",
      "score": 0.7439855337142944
    },
    {
      "name": "Inference",
      "score": 0.7026605606079102
    },
    {
      "name": "Transformer",
      "score": 0.6902867555618286
    },
    {
      "name": "Cache",
      "score": 0.6363861560821533
    },
    {
      "name": "Decoding methods",
      "score": 0.5609428286552429
    },
    {
      "name": "Language model",
      "score": 0.489303320646286
    },
    {
      "name": "Speedup",
      "score": 0.4473053812980652
    },
    {
      "name": "Algorithm",
      "score": 0.40099284052848816
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38578903675079346
    },
    {
      "name": "Speech recognition",
      "score": 0.3217211365699768
    },
    {
      "name": "Parallel computing",
      "score": 0.2457098364830017
    },
    {
      "name": "Voltage",
      "score": 0.1461811065673828
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I9224756",
      "name": "Northeastern University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I10660446",
      "name": "Kunming University of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210131870",
      "name": "Institute of Psychology, Chinese Academy of Sciences",
      "country": "CN"
    }
  ],
  "cited_by": 7
}