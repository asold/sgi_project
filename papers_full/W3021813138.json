{
  "title": "Data Augmentation for Spoken Language Understanding via Pretrained Models",
  "url": "https://openalex.org/W3021813138",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2596235521",
      "name": "Peng Baolin",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2110288568",
      "name": "Chen-guang Zhu",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2806698144",
      "name": "Michael Zeng",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2104437897",
      "name": "Jian-Feng Gao",
      "affiliations": [
        "Microsoft (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2130858665",
    "https://openalex.org/W2909544278",
    "https://openalex.org/W2810809989",
    "https://openalex.org/W2803609229",
    "https://openalex.org/W3010293452",
    "https://openalex.org/W2994400288",
    "https://openalex.org/W2969585892",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2904631866",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W3007482338",
    "https://openalex.org/W3007894275",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W2949176913",
    "https://openalex.org/W2551571666",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2964120993",
    "https://openalex.org/W2970655599",
    "https://openalex.org/W2097550833",
    "https://openalex.org/W2998184481"
  ],
  "abstract": "The training of spoken language understanding (SLU) models often faces the problem of data scarcity. In this paper, we put forward a data augmentation method with pretrained language models to boost the variability and accuracy of generated utterances. Furthermore, we investigate and propose solutions to two previously overlooked scenarios of data scarcity in SLU: i) Rich-in-Ontology: ontology information with numerous valid dialogue acts are given; ii) Rich-in-Utterance: a large number of unlabelled utterances are available. Empirical results show that our method can produce synthetic training data that boosts the performance of language understanding models in various scenarios.",
  "full_text": "Data Augmentation for Spoken Language Understanding via Pretrained\nLanguage Models\nBaolin Peng∗, Chenguang Zhu∗, Michael Zeng, Jianfeng Gao\nMicrosoft Research, Redmond\n{bapeng,chezhu,nzeng,jfgao}@microsoft.com\nAbstract\nThe training of spoken language understanding (SLU) models\noften faces the problem of data scarcity. In this paper, we put\nforward a data augmentation method using pretrained language\nmodels to boost the variability and accuracy of generated ut-\nterances. Furthermore, we investigate and propose solutions to\ntwo previously overlooked semi-supervised learning scenarios of\ndata scarcity in SLU: i) Rich-in-Ontology: ontology information\nwith numerous valid dialogue acts is given; ii)Rich-in-Utterance:\na large number of unlabelled utterances are available. Empirical\nresults show that our method can produce synthetic training data\nthat boosts the performance of language understanding models\nin various scenarios.\nIndex Terms: Spoken language understanding, pretraining, data\naugmentation, rich-in-ontology, rich-in-utterance\n1. Introduction\nSpoken Language Understanding (SLU) is widely applied in\nhuman-machine dialogue systems to convert natural utterances\ninto predeﬁned semantic frames, i.e. dialogue acts, for further\nprocessing. For example, an SLU component in a virtual as-\nsistant or robot outputs its prediction of intents and slot labels\ndetected within a user’s utterance [ 1]. Nevertheless, as a su-\npervised learning task, SLU suffers from the problem of data\nscarcity. The problem becomes more prevalent in face of new\nLU domains with novel deﬁnitions of intents and slot labels.\nEven with an existing domain, the data correlated with a certain\nintent or slot is often not sufﬁcient. These problems signiﬁcantly\nlimit the applicability of SLU systems.\nRecently, various successful use cases of synthetic datasets\nhave stimulated the growth of the area of Data Augmentation\n(DA) [2, 3]. The typical approach is to learn a model to mimic\nthe language style in the training data, leveraging the relation-\nship between semantic units and their natural representations.\nThen, a non-generative model can modify utterances and replace\nslot labels from existing data [4], while a generative model can\nproduce synthetic utterances in the same distribution space of\nthe training data [5]. However, these approaches usually train\nthe DA model on domain-speciﬁc data, which is of a small scale\nby itself. It is thus questionable whether the augmented data\ncontains rich language expressibility beyond the scope of the\ngiven data.\nOn the other hand, the rapid development of large-scale pre-\ntrained language models has signiﬁcantly improved the capacity\nof language understanding and generation models [6, 7]. With\na modest amount of domain-speciﬁc data, a pretrained model\ncan quickly adapt to a new domain. For instance, SC-GPT [8]\nﬁnetunes the GPT-2 language model [9] with dialogue data. It\n∗Equal contribution\ncan efﬁciently adapt to new dialogue domains with only a couple\nof labelled data samples.\nIn this paper, we propose to frame data augmentation as\na semantically controlled generation problem. Given dialogue\nact, we leverage the pretrained SC-GPT model to generate corre-\nsponding utterances as synthetic training data. In the process, the\ngeneral language syntax and semantics learned during the pre-\ntraining phase are fused into the generation of domain-speciﬁc\nutterances to increase variability and accuracy of SLU.\nFurthermore, previous literature on SLU data augmentation\nfocus on the case where only a scant number of pairs of utter-\nance and corresponding semantic labels are given, which we\ndenote as Paired-Data-Only. However, there are two other over-\nlooked semi-supervised learning scenarios that commonly arise\nin application.\n• Rich-in-Ontology: The full ontology for the dialogue do-\nmain is also given, including the deﬁnitions of intents,\nslot lists and valid combinations of slots and values. In\nother words, the model is given a variety of valid combina-\ntions of semantic labels. What lacks is the corresponding\nnatural language utterances.\n• Rich-in-Utterance: Apart from the labelled data, there are\nabundant unlabelled utterances without annotated intents,\nslots and values.\nIn this paper, we also delve into these two scenarios and\npropose corresponding data augmentation solutions.\nFor Rich-in-Ontology, we ﬁrst ﬁnetune the pretrained model\nSC-GPT on the paired training data, and then apply it to the valid\ncombination of intents and slots in the ontology information to\ngenerate additional training data.\nFor Rich-in-Utterance, following the idea of the NLG model\nSC-GPT, we propose SC-GPT-NLU, which is pretrained on the\nsame corpus of SC-GPT with ﬂipped sources and targets. In\ndetail, we feed the utterances into the model and let it generate\nintent and slots in a text sequence. Therefore, SC-GPT-NLU can\nact as a language understanding module and produce semantic\nlabels for the unlabelled utterances available.\nIn the experiments, we evaluate the slot tagging and intent\nclassiﬁcation accuracies of a Bi-LSTM seq2seq SLU model, us-\ning various data augmentation methods. Results show that on\nATIS and Snips datasets, our proposed method outperforms other\nbaseline systems. For instance, compared with baseline methods,\nthe data augmented by our system can help the underlying SLU\nmodel achieve 0.5 points higher slot F1 and 3.02 points higher\nintent accuracy in ATIS-Small dataset. Furthermore, when on-\ntology information or unlabelled utterances are available, i.e.\nRich-in-Ontology and Rich-in-Utterance, our method can pro-\nduce synthetic data that signiﬁcantly boosts the performance of\nSLU models.\narXiv:2004.13952v2  [cs.CL]  11 Mar 2021\n2. Related Work\n2.1. SLU Data Augmentation\nMany previous approaches to SLU data augmentation target to\nincrease variability of generated utterances. [10] proposes to add\nnoise to perturb the decoder states to generate variants of an utter-\nance. Variational autoencoder (V AE) and conditional variational\nautoencoder (CV AE) are used to generate utterances with di-\nversiﬁed expressions [11]. [4] uses both non-generative models\nlike word substitution and generative models like paraphrasing\nand back-translation to augment training data. [ 5] proposes a\nmulti-stage framework to generate, ﬁlter, and rank augmented\nutterances. [12] uses reinforcement learning to learn a genera-\ntor that facilitates dialogue state tracking. [13] employs atomic\ntemplates to guide the model to generate more utterances given\ncombination of dialogue acts. [14] proposes to select sentences\nfrom unlabeled utterances and apply pseudo-labels. The two\nadditional scenarios we propose in this paper are also related\nto semi-supervised learning [ 15]. But we focus on data aug-\nmentation, which is independent of the downstream learning\nmodels.\nSimilar to our work, [16, 17] uses pretrained language mod-\nels to generate synthetic training data for data augmentation.\nHowever, their approach blends multiple labels and input sen-\ntences together during training, so it is hard to control the amount\nof generated synthetic data per class.\n2.2. Pretraining\nPretrained models leverage the large amount of unlabelled text\ncorpora to improve the capability of language understanding.\nELMo [18] applies two unidirectional RNNs for language mod-\neling. GPT-2 [ 9] utilizes the transformer architecture [ 19] for\nthe task. BERT [ 6] employs a masking technique and next-\nsentence-prediction task to train a bidirectional language model.\nUniLM [20] uses different masking patterns to unify the model\nstructure for NLU and NLG. These pretrained language models\nhave been widely used with considerable success in various NLP\napplications such as question answering [21] and summarization\n[22].\nFurthermore, pretrained language models have been lever-\naged in speech language processing to provide rich contextual\nembeddings [23]. Speciﬁcally, SC-GPT [ 8], i.e. Semantically\nConditioned Generative Pre-training Transformer, builds upon\nGPT-2 and is further pretrained on a large-scale dialogue corpus.\nThe resulting model outperforms many baselines in few-shot\nlanguage generation for task-oriented dialogue.\n3. Data Augmentation\n3.1. Traditional Augmentation Scenario\nWe describe the traditional augmentation scenario in SLU as\nPaired-Data-Only, as the training data consists of N instance\npairs. Each pair contains the input tokenized utterance x =\n(x1, x2, ..., xT ) and the corresponding output dialogue act A.\nAincludes the intent label I and P slot-value pairs:\nA= [ I\nIntent\n, (s1 = v1, ··· , sP = vP )  \nSlot-value pairs\n] (1)\nThus, the training data D = {(x1, A1), ...,(xN, AN)}.\nHowever, due to high labeling costs, the size of labeled dataN is\nusually small. In such cases, data augmentation (DA) is needed.\nAn augmenter S is a language generation model, which is trained\nModel Input Output\nSC-GPT Dialogue act Utterance\nSC-GPT-NLU Utterance Dialogue act\nTable 1: The input and output of SC-GPT [ 8] and SC-GPT-\nNLU models. Both are initialized with GPT-2 [ 9] but further\npretrained on different data with swapped inputs and outputs.\non Dto be able to produce a corresponding utterance ˜x given\nan input dialogue act ˜A. For example, suppose ˜A= [hotel-\ninform,(name = Hyatt, area= center, star= 5], S can\ngenerate ˜x =I have booked the 5-star Hyatt hotel in the center\narea for you.\nThen, during augmentation, we ﬁrst augment the dialogue\nacts in the training data by replacing/inserting/deleting slot val-\nues to create more combinations. The augmenter S then gener-\nates candidate utterances for the dialogue acts. As the generated\nutterances may not always contain the required slot-value la-\nbels, we ﬁlter them to make sure that each utterance has all the\nrequired input slot-values.\nHowever, the data augmenter itself requires a considerable\namount of training data. As a result, augmenters directly trained\non Dmay have limited model capacity and expressibility. Thus,\nwe adopt the pretrained model SC-GPT [8], which is a language\nmodel to produce utterances given a dialogue act. SC-GPT is\ninitialized with GPT-2 [9], further pretrained on a large corpus\nof 400K dialogue-act-utterance pairs and then ﬁne-tuned on the\ntraining data D. It has been shown that SC-GPT can quickly\nadapt to new domains with only a few domain-speciﬁc data\nsamples [8].\n3.2. More Data Augmentation Scenarios\nWe note that in many real applications, there is often additional\navailable information beyond the paired training data. Here, we\nspecify two semi-supervised scenarios that commonly arise in\napplications but have been overlooked by previous approaches.\n3.2.1. Rich In Ontology\nIn many dialogue domains, a detailed description of the ontology\nis given, which is a list of valid dialogue acts. Formally, the\ntraining data consists of both labelled pairs and many dialogue\nacts: D= {(x1, A1), ...,(xN, AN), AN+1, ...,AM}.\nTo work with this scenario, we ﬁnetune SC-GPT on the\npaired part of D, i.e. {(x1, A1), ...,(xN, AN)}, and then gen-\nerate utterances for the other dialogue acts {AN+1, ...,AM}.\nThe utterances are then ﬁltered to make sure that each utterance\nhas all the corresponding slot-values.\n3.2.2. Rich In Utterance\nIt is common in practice that a large number of unla-\nbelled dialogue utterances are available, usually collected\nfrom history data. Formally, the training data consists of\nboth labelled pairs and many unlabeled utterances: D =\n{(x1, A1), ...,(xN, AN), xN+1, ...,xM}.\nTo utilize these utterances, we need to produce correspond-\ning dialogue acts. We propose to ﬁnetune GPT-2 in the reverse\nway: feed an utterance as input and let the model generate the\ndialogue act as output. In other words, we leverage a language\ngeneration model to act as a language understanding module,\ndenoted as SC-GPT-NLU (Table 1).\nLike SC-GPT, SC-GPT-NLU is initialized with GPT-2 and\nGPT-2Domainadapted model400K CorpusUtteranceDA400K CorpusDAUtteranceSC-GPTSC-GPT-NLUPaired DataAugmented Paired DataDialogue acts from dataRich-In-UtterancePretrainPretrainFinetuneFilterDomainadapted modelPaired DataAugmented Paired DataUnlabeled utterances from dataFilterFinetuneModified dialogue acts from dataAugmented Paired DataFilterRich-In-OntologyPaired-Data-Only\nFigure 1: Data augmentation process for the three scenarios: three Paired-Data-Only, Rich-In-Ontology and Rich-In-Utterances.\nAll models are initialized with GPT-2, further pretrained on 400K dialogue corpus [ 8] and ﬁnetuned on the paired data\n{(x1, A1), ...,(xN, AN)}.\nDataset ATIS Snips\nSplit Small Medium Small Medium\nModel Slot F1 Intent Acc. Slot Intent Slot Intent Slot Intent\nNo Data Augmentation\nNo-DA 68.91 84.99 87.30 90.15 61.30 93.43 79.83 97.29\nPaired-Data-Only\nSeq2Seq 73.71 - 88.72 - - - - -\nV AE 74.92 83.65 89.27 90.95 - - - -\nOurs 75.42 86.67 88.61 90.71 64.96 93.43 80.62 97.57\nRich-in-Ontology\nOurs 82.42∗ 89.03∗ 89.81∗ 92.27∗ 67.06∗ 94.14∗ 82.54∗ 97.86\nRich-in-Utterance\nOurs 78.45 87.46 88.23 91.94 63.46 93.43 80.54 98.14∗\nTable 2: Slot F1 and intent accuracy scores on ATIS and Snips dataset. The overall highest score is in bold, and the best result in\nPaired-Data-Only category is underlined. *: Statistically signiﬁcant with p-value less than 0.05.\nfurther pretrained on the 400K dialogue-act-utterance data and\nﬁnetuned on the paired part of D. But SC-GPT-NLU treats\nthe utterance as input and dialogue acts as output. So both SC-\nGPT and SC-GPT-NLU are language generation models with\na softmax-based output layer that produces utterance/dialogue\nacts token by token.\nDuring augmentation, SC-GPT-NLU generates dialogue acts\nfor the unlabeled utterances xN+1, ...,xM. Here, the generated\nnames of intents, slots and values are mapped to the pre-deﬁned\nontology by string matching. The augmented data is ﬁltered to\nmake sure that each input slot-value appears in the utterance.\nFigure 1 illustrates our SLU data augmentation process for\nall three scenarios.\n4. Experiments\n4.1. Datasets and Metrics\nWe employ the widely used SLU benchmark dataset ATIS [24]\nand Snips [25]. ATIS contains around 5.8K utterances from ﬂight\nreservation dialogues. It includes 120 slot labels and 21 intent\ntypes. Snips contains 14K utterances from the Snips personal\nvoice assistant. It includes 123 slot labels and 7 intent types.\nTo simulate the few-shot data situations, we follow [26] to\nuse two small portions of the ATIS training set as training data:\nSmall (∼1/40 of the original training set) and Medium (∼1/10 of\nthe original training set). A development set of 500 instances is\nused. Following the same split ratio, we sampled 327 and 1308\ninstances in Snips for Small and Medium respectively.\nWe use F1 score to measure slot tagging quality and use\naccuracy score to evaluate intent classiﬁcation, in accordance\nwith [26].\n4.2. Model Details\nSLU Model.For fair comparison, we use the same SLU model\nthat is trained on the training data and the data augmented by\nour model and baseline systems.\nWe adopt the same setting for the SLU model as in [5]. It\nhas two layers of bi-directional LSTM with a hidden dimension\nof 200 and a dropout probability of 0.5. We choose the Adam\noptimizer [27] with a learning rate of 0.001. Gradients with a\n2-norm greater than 5 are clipped. The best model is selected\nbased on performances on the validation set. The number of\ntraining epochs is 50 and the batch size is 20.\nData augmentation.For the Paired-Data-Only case, we modify\nthe dialogue acts in the training split to construct around 300\nadditional combinations of DAs via dropping/inserting/replacing\nSC-GPT\nDA RateBook (best rating = 6; object select = current; object type = textbook; rating value = 3)\nUtterance 1 Give 3 out of 6 to current textbook\nUtterance 2 The current textbook gets a 3 out of 6\nUtterance 3 I think that the current textbook should be rated 3 out of 6\nDA BookRestaurant ( country = Honduras; facility = indoor; restaurant type = restaurant )\nUtterance 1 Book me a reservation for an indoor restaurant in Honduras\nUtterance 2 Book an indoor restaurant in Honduras\nUtterance 3 I need to book an indoor restaurant in Honduras\nSC-GPT-NLU\nUtterance 2 of us want to eat at a restaurant that serves meatballs in VT\nDA BookRestaurant ( party size number = 2; restaurant type = restaurant; served dish = meatballs; state = VT )\nUtterance Add the track to the Metal Talks Metallica playlist.\nDA AddToPlaylist ( music item = track; playlist = metal talks Metallica)\nTable 3: Example utterances generated by SC-GPT given dialogue acts (DA) and dialogue acts generated by SC-GPT-NLU given\nunlabelled utterances in Snips.\nslots and values. For each dialogue act, we sample three utter-\nances produced by SC-GPT. After ﬁltering out utterances which\ndo not contain all the slot-values, we collect around 500 synthetic\nutterances and add them into the original training split.\nWe simulate the Rich-in-Ontology scenario by making the\ndialogue acts in the whole training set available, from which 500\ndialogue acts are sampled and added to the training split.\nFor the Rich-in-Utterance scenario, we sample 1,000 utter-\nances in the training corpus and use SC-GPT-NLU to produce\nthe most probable dialogue act. After ﬁltering, around 500\nutterance-DA pairs are added to the original training split.\nImplementation details.Both SC-GPT and SC-GPT-NLU are\nﬁnetuned for 5 epoches with a learning rate as 5e-5. Nucleus\nsampling [28] is used for decoding, where the sampling top-p is\n0.9, and the temperature is 1. Details on SC-GPT including the\nnumber of parameters and pretraining procedure can be found\nat [8]. The ﬁnetuning takes about half an hour on a V100 GPU\nmachine 64GB memory.\nBaselines. The baseline data augmentation systems include the\nseq2seq [5] and variational autoencoder (V AE) data augmenta-\ntion model [29]. We also report the results for the case without\ndata augmentation, denoted by No-DA.\n4.3. Results\nTable 2 shows the accuracy of slot tagging and intent classiﬁ-\ncation for various models. Based on the results, we make the\nfollowing observations.\nFirstly, our data augmentation method can considerably\nboost the model accuracy (comparing No-DA and Ours), es-\npecially when the training data size is small. For instance, in\nATIS, when only paired data is available, the slot F1 increases by\n6.51 (Small) and 1.31 (Medium) points, while the intent accuracy\nincreases by 1.68 (Small) and 0.56 (Medium) points.\nSecondly, under Rich-in-Ontology and Rich-in-Utterance\nscenarios, our method further boosts the slot F1 by up to 7\npoints and intent accuracy by up to 2.4 points. Overall, the\naccuracy scores are the highest when the ontology information\nis available. This shows that our method can take advantage of\nadditional information and produce better synthetic training data\nfor downstream models. We conduct statistical paired t-tests\nand ﬁnd that the best model’s performance is all statistically\nsigniﬁcant with p-value less than 0.05.\nThirdly, under the traditional Paired-Data-Only scenario, our\ndata augmentation method outperforms all baselines in ATIS-\nSmall, and achieves comparable results in ATIS-Medium. This\nshows that our method is better suited when training data is\nscarce.\n4.4. Examples of Augmented Data\nIn Table 3, we show examples of generated utterances and dia-\nlogue acts by SC-GPT and SC-GPT-NLU in Snips. As shown,\nafter pretraining and domain ﬁnetuning, SC-GPT can produce\ncoherent utterances with a high variability, while covering all\nrequired intent, slots and values. SC-GPT-NLU can generate\ndialogue acts in the same format as input to SC-GPT, which\ncaptures the important semantic information in the input utter-\nances. This demonstrates that pretrained models can quickly\nadapt to target domains with a small amount of labeled data.\nThis facilitates the generation of high-quality synthetic data for\nSLU.\n5. Conclusion\nIn this paper, we approach the problem of data scarcity in SLU\nwith pretrained language models. After ﬁnetuning on domain-\nspeciﬁc dialogue data, our model can produce high-quality syn-\nthetic data which boosts the performance of the downstream SLU\nmodel. Moreover, we provide solutions to two semi-supervised\nscenarios in SLU overlooked by previous literature: Rich-in-\nOntology and Rich-in-Utterance. In experiments on the bench-\nmark datasets ATIS and Snips, we demonstrate that our solution\ncan effectively leverage auxiliary unlabeled data to produce high-\nquality synthetic training data for building SLU models with a\nhigher accuracy.\nAs future work, we aim to extend the idea of data augmenta-\ntion based on pretrain language models to other speech language\nprocessing tasks, such as information retrieval and summariza-\ntion.\n6. References\n[1] J. R. Bellegarda, “Spoken language understanding for natural in-\nteraction: The siri experience,” in Natural interaction with robots,\nknowbots and smartphones. Springer, 2014, pp. 3–14.\n[2] X. Lu, B. Zheng, A. Velivelli, and C. Zhai, “Enhancing text cat-\negorization with semantic-enriched representation and training\ndata augmentation,”Journal of the American Medical Informatics\nAssociation, vol. 13, no. 5, pp. 526–535, 2006.\n[3] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen,\nR. Prenger, S. Satheesh, S. Sengupta, A. Coates et al., “Deep\nspeech: Scaling up end-to-end speech recognition,” arXiv preprint\narXiv:1412.5567, 2014.\n[4] J. Quan and D. Xiong, “Effective data augmentation ap-\nproaches to end-to-end task-oriented dialogue,” arXiv preprint\narXiv:1912.02478, 2019.\n[5] Y . Hou, Y . Liu, W. Che, and T. Liu, “Sequence-to-sequence data\naugmentation for dialogue language understanding,”arXiv preprint\narXiv:1807.01554, 2018.\n[6] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,”arXiv preprint arXiv:1810.04805, 2018.\n[7] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,\nM. Lewis, L. Zettlemoyer, and V . Stoyanov, “Roberta: A\nrobustly optimized bert pretraining approach,” arXiv preprint\narXiv:1907.11692, 2019.\n[8] B. Peng, C. Zhu, C. Li, X. Li, J. Li, M. Zeng, and J. Gao, “Few-\nshot natural language generation for task-oriented dialog,”arXiv\npreprint arXiv:2002.12328, 2020.\n[9] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Im-\nproving language understanding by generative pre-training,” 2018.\n[10] G. Kurata, B. Xiang, and B. Zhou, “Labeled data generation with\nencoder-decoder lstm for semantic slot ﬁlling.” in Interspeech,\n2016.\n[11] J. Li, L. Qiu, B. Tang, D. Chen, D. Zhao, and R. Yan, “Insufﬁcient\ndata can also rock! Learning to converse using smaller data with\naugmentation,” inProceedings of the AAAI Conference on Artiﬁcial\nIntelligence, vol. 33, 2019, pp. 6698–6705.\n[12] Y . Yin, L. Shang, X. Jiang, X. Chen, and Q. Liu, “Dialog\nstate tracking with reinforced data augmentation,”arXiv preprint\narXiv:1908.07795, 2019.\n[13] Z. Zhao, S. Zhu, and K. Yu, “Data augmentation with atomic\ntemplates for spoken language understanding,” arXiv preprint\narXiv:1908.10770, 2019.\n[14] E. Cho, H. Xie, J. P. Lalor, V . Kumar, and W. M. Campbell, “Efﬁ-\ncient semi-supervised learning for natural language understanding\nby optimizing diversity,” in2019 IEEE Automatic Speech Recog-\nnition and Understanding Workshop (ASRU). IEEE, 2019, pp.\n1077–1084.\n[15] D. P. Kingma, S. Mohamed, D. J. Rezende, and M. Welling, “Semi-\nsupervised learning with deep generative models,” inAdvances in\nneural information processing systems, 2014, pp. 3581–3589.\n[16] A. Anaby-Tavor, B. Carmeli, E. Goldbraich, A. Kantor, G. Kour,\nS. Shlomov, N. Tepper, and N. Zwerdling, “Do not have enough\ndata? Deep learning to the rescue!” in Thirty-Fourth AAAI Confer-\nence on Artiﬁcial Intelligence, 2020.\n[17] V . Kumar, A. Choudhary, and E. Cho, “Data augmentation using\npre-trained transformer models,”arXiv preprint arXiv:2003.02245,\n2020.\n[18] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer, “Deep contextualized word representations,”\narXiv preprint arXiv:1802.05365, 2018.\n[19] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nAdvances in neural information processing systems , pp. 5998–\n6008, 2017.\n[20] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y . Wang, J. Gao,\nM. Zhou, and H.-W. Hon, “Uniﬁed language model pre-training\nfor natural language understanding and generation,”arXiv preprint\narXiv:1905.03197, 2019.\n[21] C. Zhu, M. Zeng, and X. Huang, “Sdnet: Contextualized attention-\nbased deep network for conversational question answering,”arXiv\npreprint arXiv:1812.03593, 2018.\n[22] Y . Liu and M. Lapata, “Text summarization with pretrained en-\ncoders,”EMNLP, 2019.\n[23] Y .-A. Chung, C. Zhu, and M. Zeng, “Semi-supervised speech-\nlanguage joint pre-training for spoken language understanding,”\narXiv preprint arXiv:2010.02295, 2020.\n[24] G. Tur, D. Hakkani-T ¨ur, and L. Heck, “What is left to be un-\nderstood in atis?” in 2010 IEEE Spoken Language Technology\nWorkshop. IEEE, 2010, pp. 19–24.\n[25] A. Coucke, A. Saade, A. Ball, T. Bluche, A. Caulier, D. Leroy,\nC. Doumouro, T. Gisselbrecht, F. Caltagirone, T. Lavril et al.,\n“Snips voice platform: an embedded spoken language understand-\ning system for private-by-design voice interfaces,”arXiv preprint\narXiv:1805.10190, 2018.\n[26] Y .-N. Chen, D. Hakanni-T¨ur, G. Tur, A. Celikyilmaz, J. Guo, and\nL. Deng, “Syntax or semantics? Knowledge-guided joint seman-\ntic frame parsing,” in 2016 IEEE Spoken Language Technology\nWorkshop (SLT). IEEE, 2016, pp. 348–355.\n[27] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-\ntion,”arXiv preprint arXiv:1412.6980, 2014.\n[28] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y . Choi, “The curious\ncase of neural text degeneration,”arXiv preprint arXiv:1904.09751,\n2019.\n[29] K. M. Yoo, Y . Shin, and S.-g. Lee, “Data augmentation for spoken\nlanguage understanding via joint variational generation,” in Pro-\nceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 33,\n2019, pp. 7402–7409.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8032247424125671
    },
    {
      "name": "Utterance",
      "score": 0.7093476057052612
    },
    {
      "name": "Scarcity",
      "score": 0.6912214159965515
    },
    {
      "name": "Spoken language",
      "score": 0.6303884983062744
    },
    {
      "name": "Natural language processing",
      "score": 0.5616496801376343
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5389846563339233
    },
    {
      "name": "Language model",
      "score": 0.5257566571235657
    },
    {
      "name": "Ontology",
      "score": 0.485893189907074
    },
    {
      "name": "Training set",
      "score": 0.45585358142852783
    },
    {
      "name": "Language understanding",
      "score": 0.4361116588115692
    },
    {
      "name": "Microeconomics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1290206253",
      "name": "Microsoft (United States)",
      "country": "US"
    }
  ],
  "cited_by": 21
}