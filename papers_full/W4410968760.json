{
    "title": "Toward HydroLLM: a benchmark dataset for hydrology-specific knowledge assessment for large language models",
    "url": "https://openalex.org/W4410968760",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A5093001900",
            "name": "Dilara Kızılkaya",
            "affiliations": [
                "University of Iowa"
            ]
        },
        {
            "id": "https://openalex.org/A5042257674",
            "name": "Ramteja Sajja",
            "affiliations": [
                "University of Iowa"
            ]
        },
        {
            "id": "https://openalex.org/A2775179779",
            "name": "Yusuf Sermet",
            "affiliations": [
                "University of Iowa"
            ]
        },
        {
            "id": "https://openalex.org/A1571528974",
            "name": "İbrahim Demir",
            "affiliations": [
                "University of Iowa",
                "Tulane University"
            ]
        },
        {
            "id": "https://openalex.org/A5093001900",
            "name": "Dilara Kızılkaya",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5042257674",
            "name": "Ramteja Sajja",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2775179779",
            "name": "Yusuf Sermet",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1571528974",
            "name": "İbrahim Demir",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4286794336",
        "https://openalex.org/W4377164328",
        "https://openalex.org/W2032015160",
        "https://openalex.org/W2083223510",
        "https://openalex.org/W3174891819",
        "https://openalex.org/W4404792913",
        "https://openalex.org/W2789284075",
        "https://openalex.org/W6807201124",
        "https://openalex.org/W4404304247",
        "https://openalex.org/W4385175044",
        "https://openalex.org/W4402448026",
        "https://openalex.org/W4399250914",
        "https://openalex.org/W4399011256",
        "https://openalex.org/W6852072056",
        "https://openalex.org/W4401524370",
        "https://openalex.org/W6607167723",
        "https://openalex.org/W2128475524",
        "https://openalex.org/W4381786365",
        "https://openalex.org/W6765967603",
        "https://openalex.org/W6600383830",
        "https://openalex.org/W4408254243",
        "https://openalex.org/W3093448767",
        "https://openalex.org/W6601998182",
        "https://openalex.org/W6600234944",
        "https://openalex.org/W4402988539",
        "https://openalex.org/W4405595991",
        "https://openalex.org/W4403633250",
        "https://openalex.org/W2998997307",
        "https://openalex.org/W4397047623",
        "https://openalex.org/W4398160894",
        "https://openalex.org/W2618236535",
        "https://openalex.org/W4392759165",
        "https://openalex.org/W4408615073",
        "https://openalex.org/W4387891898",
        "https://openalex.org/W4389281887",
        "https://openalex.org/W4401733325",
        "https://openalex.org/W4396542473",
        "https://openalex.org/W4405832328",
        "https://openalex.org/W4402001711",
        "https://openalex.org/W6871014120",
        "https://openalex.org/W6600650897",
        "https://openalex.org/W6851737779",
        "https://openalex.org/W4392648582",
        "https://openalex.org/W6600855501",
        "https://openalex.org/W4408211945",
        "https://openalex.org/W4396988239",
        "https://openalex.org/W4388490623",
        "https://openalex.org/W4409403712",
        "https://openalex.org/W4206828804",
        "https://openalex.org/W2964748009"
    ],
    "abstract": "Abstract The rapid advancement of large language models (LLMs) has enabled their integration into a wide range of scientific disciplines. This article introduces a comprehensive benchmark dataset specifically designed for testing recent LLMs in the hydrology domain. Leveraging a collection of research articles and hydrology textbooks, we generated a wide array of hydrology-specific questions in various formats, including true/false, multiple-choice, open-ended, and fill-in-the-blank. These questions serve as a robust foundation for evaluating the performance of state-of-the-art LLMs, including GPT-4o-mini, Llama3:8B, and Llama3.1:70B, in addressing domain-specific queries. Our evaluation framework employs accuracy metrics for objective question types and cosine similarity measures for subjective responses, ensuring a thorough assessment of the models’ proficiency in understanding and responding to hydrological content. The results underscore both the capabilities and limitations of artificial intelligence (AI)-driven tools within this specialized field, providing valuable insights for future research and the development of educational resources. By introducing HydroLLM-Benchmark, this study contributes a vital resource to the growing body of work on domain-specific AI applications, demonstrating the potential of LLMs to support complex, field-specific tasks in hydrology.",
    "full_text": "DATA PAPER\nToward HydroLLM: a benchmark dataset for hydrology-\nspecific knowledge assessment for large language models\nDilara Kizilkaya1,2, Ramteja Sajja1,3 , Yusuf Sermet1 and Ibrahim Demir4,5\n1IIHR - Hydroscience and Engineering, University of Iowa, Iowa City, IA, USA\n2Computer Science, University of Iowa, Iowa City, IA, USA\n3Electrical and Computer Engineering, University of Iowa, Iowa City, IA, USA\n4River-Coastal Science and Engineering, Tulane University, Iowa City, IA, USA\n5ByWater Institute, Tulane University, New Orleans, LA, USA\nCorresponding author: Ramteja Sajja; Email:ramteja-sajja@uiowa.edu\nReceived: 15 March 2025;Revised: 01 May 2025;Accepted: 09 May 2025\nKeywords: benchmark dataset; domain-specific AI; hydrology; large language models(LLMs); natural language processing (NLP);\nquestion generation\nAbstract\nThe rapid advancement of large language models (LLMs) has enabled their integration into a wide range of scientific\ndisciplines. This article introduces a comprehensive benchmark dataset specifically designed for testing recent LLMs in\nthe hydrology domain. Leveraging a collection of research articles and hydrology textbooks, we generated a wide array\nof hydrology-specific questions in various formats, including true/false, multiple-choice, open-ended, and fill-in-the-\nblank. These questions serve as a robust foundation for evaluating the performance of state-of-the-art LLMs, including\nGPT-4o-mini, Llama3:8B, and Llama3.1:70B, in addressing domain-specific queries. Our evaluation framework\nemploys accuracy metrics for objective question types and cosine similarity measures for subjective responses, ensuring\na thorough assessment of the models’proficiency in understanding and responding to hydrological content. The results\nunderscore both the capabilities and limitations of artificial intelligence (AI)-driven tools within this specialized field,\nproviding valuable insights for future research and the development of educational resources. By introducing\nHydroLLM-Benchmark, this study contributes a vital resource to the growing body of work on domain-specific\nAI applications, demonstrating the potential of LLMs to support complex, field-specific tasks in hydrology.\nImpact Statement\nOur study introduces HydroLLM-Benchmark, the first comprehensive dataset designed to evaluate large language\nmodels (LLMs) in hydrology-specific tasks. As artificial intelligence (AI) increasingly supports environmental\nresearch, assessing LLMs’ability to process hydrological knowledge is crucial for scientific progress. By bench-\nmarking models like GPT-4o-mini and Llama3, we identify their strengths and limitations in understanding hydrology,\ninforming improvements in AI-driven decision-making for water resource management, climate resilience, and flood\nprediction. This work bridges the gap between AI and hydrological sciences, ensuring that future LLMs are better\nequipped for environmental applications. By providing an open-source dataset, we empower researchers to refine\nAI models, fostering more accurate, data-driven insights for sustainable water management and environmental policy.\n© The Author(s), 2025. Published by Cambridge University Press. This is an Open Access article, distributed under the terms of the Creative Commons\nAttribution licence (http://creativecommons.org/licenses/by/4.0), which permits unrestricted re-use, distribution and reproduction, provided the\noriginal article is properly cited.\nThis research article was awarded Open Data and Open Materials badges for transparent practices. See the Data Availability\nStatement for details.\nEnvironmental Data Science(2025), 4: e31, 1–18\ndoi:10.1017/eds.2025.10006\nhttps://doi.org/10.1017/eds.2025.10006 Published online by Cambridge University Press\n1. Introduction\nHydrology is a specialized domain characterized by its intricate interplay of physical, chemical, and\nbiological processes, combined with significant societal and environmental implications. Addressing\nglobal challenges such as water scarcity, flooding, and sustainable water resource management demands a\nprofound understanding of hydrological processes and their interconnected systems. The study of the\nwater cycle, encompassing precipitation, evaporation, and runoff, is influenced by diverse environmental\nfactors, requiring precision and context-specific knowledge (Ukarande,2023). In addition, subdisciplines\nsuch as groundwater hydrology necessitate a deep understanding of subsurface water physics, which is\nessential for effective resource management and environmental sustainability (Anderson,2007).\nDespite advances in hydrological science, knowledge gaps persist, particularly in understanding local\nboundary conditions and hydrological connectivity, which often vary significantly across regions\n(Wagener et al.,2021). Addressing these gaps requires the development of shared perceptual models\nto enhance collective understanding and collaboration in hydrological research (Wagener et al.,2020).\nMoreover, hydrology is inherently interdisciplinary, demanding integration across civil engineering,\ngeology, meteorology, and social sciences to address water resource challenges effectively (Harshbarger\nand Ferris,1963). This interdisciplinary nature, combined with the socioeconomic and political factors\ninfluencing water-related decision-making, underscores the need for specialized training and knowledge\nin the field (Harshbarger and Ferris,1963).\nThe rapid development and widespread adoption of large language models (LLMs) have opened new\navenues for tackling domain-specific challenges in science and engineering. However, applying general-\npurpose LLMs to hydrology presents significant challenges due to the specialized nature of hydrological\ndata and reasoning tasks (Samuel et al.,2024a). General-purpose LLMs, trained on diverse datasets, often\nlack domain-specific knowledge required for tasks such as flood management, groundwater modeling,\nand water quality assessment (Shen et al.,2024). In addition, LLMs face spatial reasoning deficiencies,\nwhich are critical for hydrological tasks involving watershed mapping, flood simulation, and water\ndistribution planning (Yan et al.,2023; Vald et al.,2024). Their limitations in spatial reasoning can hinder\neffective real-time decision-making in dynamic hydrological scenarios (Yan et al.,2023).\nAnother key challenge is the integration of multimodal data, combining textual, visual, and numerical\ninformation core requirement for effective hydrological analysis (Samuel et al.,2024b). While advance-\nments like GPT-4 Vision demonstrate improvements in processing visual data, their performance in\nmultimodal tasks remains inconsistent, highlighting the need for domain-specific fine-tuning (Kadiyala\net al.,2024a). Nevertheless, recent studies suggest that targeted fine-tuning and domain-specific adap-\ntations have the potential to enhance LLM performance in hydrology (Xu et al.,2024b).\nThe integration of AI-driven educational and decision-support systems has demonstrated promising\noutcomes in specialized domains (Kadiyala et al.,2024b). For instance, AI-enabled intelligent assistants\nhave shown significant potential in personalized and adaptive learning environments by reducing\ncognitive load, providing targeted knowledge assessments, and generating customized learning pathways\n(Sajja et al., 2023). These systems offer capabilities such as interactive knowledge discovery, quiz\ngeneration, and intelligent tutoring, which can also be adapted to hydrology-specific tasks (Sajja et al.,\n2024a).\nSimilarly, conversational AI educational assistants have been successfully deployed in diverse\nacademic domains, including environmental, political, and social sciences, showcasing their effective-\nness in delivering course-specific support and fostering deeper engagement with complex datasets\n(Pursnani et al., 2023; Sajja et al., 2024b). In the context of floodplain management certification,\nAI-assisted tools have been developed to enhance vocational training, offering interactive question-\nanswering sessions and real-time feedback tailored to certification requirements (Sajja et al.,2025,\nPursnani et al.,2024). These applications demonstrate the potential of AI to address specialized learning\nand professional training needs, highlighting the feasibility of similar frameworks in hydrology.\nIn parallel, decision-support frameworks such as the multi-hazard tournament system have been\nemployed in flood mitigation and water resource management contexts (Alabbad et al.,2024). These\ne31-2 Dilara Kizilkaya et al.\nhttps://doi.org/10.1017/eds.2025.10006 Published online by Cambridge University Press\nframeworks utilize AI agents and collaborative multi-agent interactions to optimize decision-making\nprocesses, demonstrating the capability of AI-driven simulations in complex multi-stakeholder environ-\nments (Kadiyala et al.,2024c). Such applications underscore the potential for LLMs to support intricate\nhydrological decision-making tasks when appropriately fine-tuned and adapted.\nThe need for a hydrology-specific benchmark dataset emerges from the complex and multifaceted\nnature of hydrological research and water resource management (Ebert-Uphoff et al.,2017). Bench-\nmark datasets serve as standardized tools for evaluating, validating, and improving hydrological\nmodels, ensuring consistent performance assessment across diverse tasks (Sit et al.,2021). Existing\ndatasets, such as CAMELS-DE, link landscape attributes with hydrological and meteorological time\nseries, enabling insights into hydrological processes across various landscapes (Dolich et al.,2024).\nSimilarly, the SEN12-WATER dataset integrates multiple data types to analyze water dynamics and\ndrought resilience (Russo et al.,2024).\nBenchmark datasets also play a crucial role in hydrological model validation, with datasets like those\ndeveloped for SAC, GR4J, and SOCONT models enabling consistent and reliable performance assess-\nments (Izquierdo-Horna et al.,2024). In addition, resources such as the Panta Rhei dataset provide paired\nflood and drought socio-hydrological data, facilitating integrated modeling approaches (Kreibich et al.,\n2023). However, data gaps persist, especially concerning fine temporal resolution data for groundwater\nrecharge, a gap partially addressed by datasets like RpSy (Malakar et al.,2024). Despite these contribu-\ntions, challenges related to data accessibility, standardization, and regional coverage continue to limit the\neffectiveness of existing datasets (Demir et al.,2022; Dolich et al.,2024).\nWhile scientific benchmarks exist across domains, they often fail to address the specific needs of\nhydrology. Traditional hydrological models frequently suffer from performance degradation when\napplied across multiple basins, h ighlighting the regional variab ility of hydrological conditions\n(Kratzert et al., 2019). Furthermore, the subjective nature of accuracy determination complicates\nbenchmarking efforts, as model performance expectations often depend on regional characteristics\nand data quality (Seibert,2001). Recent advancements, such as long short-term memory networks,\ndemonstrate improved performance in cross-basin hydrological modeling by leveraging large datasets\n( K r a t z e r te ta l . ,2019). Similarly, physically based models, like the Variable Infiltration Capacity\nmodel, provide a more robust representation of hydrological processes and enable meaningful\ncomparisons across hydroclimate conditions (Newman et al.,2017). However, these approaches still\nface challenges in addressing the context-specific requirements of hydrological benchmarking\n(Seibert, 2001).\nGeneral-purpose LLMs face limitations not only in hydrology but also across other specialized\ndomains. These limitations include knowledge gaps, terminology inconsistencies, and a lack of domain-\nspecific reasoning capabilities (Chen et al.,2023; Soman and Ranjani,2024). In addition, issues like\nknowledge forgetting— where newer knowledge overshadows older, relevant information— complicate\ntheir application in specialized tasks (Chen et al.,2023). Evaluation by human experts remains essential,\nas LLMs often fail to align with nuanced reasoning in specialized fields (Harvel et al.,2024; Szymanski\net al.,2024). Safety concerns, including the risk of generating harmful content, further emphasize the\nimportance of balanced fine-tuning methodologies (Thakkar et al.,2024).\nThe absence of a standardized evaluation dataset for hydrology-focused LLMs exacerbates these\nchallenges. Without a consistent benchmark, evaluating and comparing model performance becomes\ninherently biased and inconsistent (Zheng et al.,2018). Challenges such as data contamination and a lack\nof robust evaluation guidelines add further complexity to interpreting benchmark scores (Singh et al.,\n2024). Emerging domain-specific benchmarks, such as WaterER, highlight the potential benefits of\ntailored evaluation frameworks for hydrological tasks (Xu et al.,2024b).\nBenchmarks from other specialized fields provide valuable lessons. In code generation, datasets like\nEvoCodeBench offer structured evaluation methodologies (Li et al.,2024). In medicine, benchmarks like\nMIMIC-III, BioASQ, and CheXpert have revolutionized medical AI applications (Yan et al.,\n2024). In\naddition, datasets like BLURB and BioLP-bench have demonstrated the value of task-specific metrics in\nbiomedical and biological applications (Feng et al.,2024; Ivanov,2024). Similarly, SciEx, designed for\nEnvironmental Data Science e31-3\nhttps://doi.org/10.1017/eds.2025.10006 Published online by Cambridge University Press\nscientific reasoning, evaluates models using university-level exam questions with human grading to\nensure accuracy (Dinh et al.,2024).\nThis study introduces HydroLLM-Benchmark, a hydrology-focused benchmark dataset designed to\nfacilitate research related to the field, while also offering baseline performance results for state-of-the-art\nLLMs in hydrological question-answering tasks. HydroLLM-Benchmark compiles diverse hydrological\nresources, including textbook-based foundational concepts and cutting-edge research articles, ensuring\nrobust coverage of theoretical underpinnings and emerging insights. Although the dataset has been\nstreamlined to minimize preprocessing requirements, providing clear, structured, and readily usable data\nfor machine learning, pipelines remain flexible enough to support alternative investigative approaches,\nsuch as physically based hydrological modeling.\nResearchers can also leverage HydroLLM-Benchmark in conjunction with existing hydrological\ndatasets, thereby expanding the scope for comparative analyses and hybrid modeling strategies. By\naddressing key gaps in existing benchmarks, HydroLLM-Benchmark aims to serve as a robust evaluation\ntool and catalyst for innovation in domain-specific LLM research, fostering advancements in hydrological\nscience, education, and decision-making.\nThe remainder of this article is organized as follows:Section 2outlines the methodology behind the\ndesign choices, development, and implementation of a hydrology-oriented intelligent assistance system,\nbenchmarking its capacity to generate and answer domain-specific questions.Section 3 presents the\nbenchmark results and provides a brief discussion of them.Section 4 describes the challenges faced\nduring the process and addresses the limitations. Finally,Section 5concludes with a summary of the\nstudy’s contributions and insights for advancing AI in hydrological research and practice.\n2. Methodology\nThis section outlines the methodology used to create a collection of hydrology-specific questions and\nanswers and to evaluate the performance of LLMs in generating and answering these questions. The\nprocess involved selecting relevant research articles and textbooks to ensure a comprehensive represen-\ntation of both foundational knowledge and recent advancements in hydrology. The methodology includes\nsteps for data collection, question generation, and evaluation techniques, focusing on assessing the\naccuracy and contextual relevance of the models’outputs.\n2.1. Data collection\nFor this study, we selected both research articles and textbooks to comprehensively cover the current\nadvancements and foundational knowledge in hydrology. The primary goal was to ensure that the chosen\nsources were both relevant to the field of hydrology and reflected the most recent developments in\nhydrological science. Our initial experiments with fine-tuning on multiple hydrology textbooks did not\nyield a notable improvement in specialized knowledge recollection. Modern LLM architectures already\nencompass a broad technical corpus, so the real benefit of fine-tuning is to instill the field’s distinctive\nstyle, jargon, and conceptual framework.\nWe therefore chose “Fundamentals of Hydrology” (Davie, 2019), as it is widely regarded as\nproviding an authoritative, comprehensive structure of core principles and a unified lexicon in the\nfield of hydrology. This textbook is renowned for its comprehensive coverage of basic hydrological\nprinciples and processes, providing a solid foundation for understanding the broader applications of\nhydrology in both academic and practical contexts. Its inclusion in this study serves as a benchmark for\ncomparing newer research insights with established hydrological knowledge. By anchoring our work in\nthis single, highly respected text, we streamline the fine-tuning process to more directly enhance\nhydrology-specific reasoning without overwhelming the model with redundant or minimally impactful\nmaterial. We anticipate that future contributionsfrom the community will expand upon this foundation\nwith additional texts and thereby keep HydroLLM-Benchmark aligned with ongoing developments in\nhydrological research.\ne31-4 Dilara Kizilkaya et al.\nhttps://doi.org/10.1017/eds.2025.10006 Published online by Cambridge University Press\nIn addition to the textbook, we gathered 2,000 research articles from Elsevier, a leading academic\npublisher known for its extensive repository of peer-reviewed journals. The selection of research\narticles focused specifically on those related to hydrology, published between 2022 and 2024, to\ncapture the most current findings and trends in the field. Furthermore, to maintain focus and ensure\nquality, these articles were primarily filtered from three journals:J o u r n a lo fH y d r o l o g y, Advances in\nWater Resources,a n dJournal of Hydrology: Regional Studies.By limiting the selection to this period,\nwe ensured that the study reflects contemporary hydrological research, including the latest method-\nologies, technological advancements, and emerging challenges within the discipline. The selection\nprocess for both textbooks and research articles wasguided by relevance to keyhydrological topics,\nsuch as flood management, water quality, and hydrological modeling, forming the foundation of\nHydroLLM-Benchmark.\n2.2. Experimental setup\nWe selected GPT-4o-mini, Llama3:8B, and Llama3.1:70B to represent a range of model sizes and types\n(i.e., commercial vs. open source) commonly used in academic and applied settings. GPT-4o-mini was\nchosen due to its balance between performance and resource efficiency, enabling cost-effective deploy-\nment while retaining competitive capabilities, especially in zero-shot and instruction-following tasks.\nNotably, while GPT-4o offers top-tier performance, its significantly higher API cost ($10.00 vs. $0.60 per\nmillion input tokens) made GPT-4o-mini a more scalable choice (OpenAI,2024) for our large-scale\nevaluation experiments.\nLlama3:8B and 70B were included to explore performance across different model scales, as the 8B\nvariant represents a smaller, resource-accessible model, while the 70B variant reflects cutting-edge\ncapabilities at the high end of the open-weight model spectrum. We used the base versions (not\ninstruction-tuned) of both Llama models to evaluate their raw language modeling abilities without\nadditional task-specific adaptation, aiming for a controlled, fine-tuning-agnostic benchmark.\nOther models, including Gemma 2 and Mistral 2, were considered but excluded due to limited\ninfrastructure support, early-stage stability issues, or lack of reproducible inference pipelines at the time\nof evaluation. Commercial models such as Claude and Gemini were also excluded due to licensing\nrestrictions. We intend for future iterations of HydroLLM-Benchmark to include a broader range of LLMs\nas the benchmark evolves.\nOur experimental setup was designed to handle the computational demands of LLMs while main-\ntaining consistent evaluation conditions across all models. We used Python as the primary programming\nlanguage due to its versatility and the availability of extensive libraries well-suited for machine learning,\ndata processing, and evaluation tasks. Python’s flexibility enabled us to streamline both data preprocess-\ning and model evaluation, ensuring each step of the workflow was efficient and reproducible for\nHydroLLM-Benchmark.\nFor data handling and processing, we utilized several key Python libraries. Pandas was employed to\nload, clean, and structure datasets in CSV format, enabling efficient organization and manipulation of the\ndata for question generation and answer processing. This allowed us to maintain consistency in\npreprocessing across different question types. For numerical computations, especially for managing\narrays and performing calculations during the evaluation phase, we relied on NumPy. This library was\ncrucial for handling large datasets and ensuring the computational efficiency of our operations. To\ncompute cosine similarity, a vital metric for evaluating the semantic accuracy of open-ended and fill-\nin-the-blanks responses, we used scikit-learn. Its robust implementation of cosine similarity integrated\nseamlessly into our evaluation framework, providing precise performance assessments.\nWe accessed GPT-4o-mini through the OpenAI API, which allowed us to configure model settings\naccording to our experimental requirements. Specifically, we adjusted the max_tokens parameter to\n4,000, ensuring that GPT-4o-mini could generate comprehensive responses for longer, open-ended\nquestions without truncation. Running GPT-4o-mini locally on high-performance computers gave us\nthe flexibility to control the environment and maintain consistent settings throughout the experiments.\nEnvironmental Data Science e31-5\nhttps://doi.org/10.1017/eds.2025.10006 Published online by Cambridge University Press\nIn addition to GPT-4o-mini, we evaluated Llama3:8B and Llama3.1:70B. These models were\ndeployed on a dedicated server infrastructure equipped with high-performance GPUs (i.e., NVIDIA\nL40S with 48 GB memory), to handle the resource-intensive demands of large-scale language models.\nThis setup ensured efficient execution, particularly for the computationally demanding Llama3.1:70B\nmodel. Both Llama models were evaluated in their default configurations without fine-tuning to evaluate\ntheir baseline capabilities in hydrology-specific tasks.\nThe experimental workflow was structured to ensure fair and consistent evaluation across all\nmodels. After preprocessing the dataset and organizing questions to align with each model’si n p u t\nrequirements, we generated structured prompts tailored to each question type, including true/false,\nmultiple-choice, open-ended, and fill-in-the-blanks. These prompts provided clear and consistent\ninstructions to guide the models in producing responses that adhered to the expected format for each\nquestion type. During the testing phase, these prompts were applied consistently to each model, and\ntheir outputs were collected under controlled conditions to thoroughly assess their performance on\nHydroLLM-Benchmark.\nTo evaluate the model-generated answers, we employed different metrics based on the question type.\nFor true/false and multiple-choice questions, which have objective answers, accuracy was used as the\nprimary metric. Each model’s output was directly compared to the ground truth, and the accuracy score\nwas calculated as the percentage of correct answers. For open-ended and fill-in-the-blanks questions,\nwhere responses could vary in structure but still convey similar meanings, we used cosine similarity to\nassess semantic alignment between the model-generated and reference answers. Using scikit-learn, we\ntransformed the responses into vector form and calculated cosine similarity to enable meaningful\ncomparisons of semantic content.\nTo ensure consistency across all evaluations, we maintained uniform parameter settings and\nenvironmental configurations for each model. By utilizing the GPT-4o-mini API and deploying the\nLlama models on the high-performance server, we balanced computational efficiency with standardized\ntesting conditions. This hybrid setup allowed for an effective comparison of each model’s out-of-the-\nbox performance in handling hydrology-specific tasks, providing valuable insights into their strengths\nand limitations.\n2.3. Question generation methodology\nIn generating questions from the selected textbook and research article for HydroLLM-Benchmark, we\nbegan by systematically extracting relevant text data, focusing on sections most likely to yield meaningful\ncontent for question generation. This initial extraction targeted key passages and concepts central to\nhydrology, ensuring that the generated questions would be directly aligned with core educational and\nresearch objectives.\nOnce the relevant content was identified, we crafted a series of specialized prompts tailored for each\nquestion type— true/false, multiple-choice, open-ended, and fill-in-the-blank. Each question type\nrequired a unique approach; however, the foundational structure of the prompts remained consistent,\nwith specific adjustments made to customize the format, required answer structure, and anticipated output\nstyle. These modifications allowed for both variety and uniformity, ensuring that each question adhered to\nthe designated type while maintaining coherence across the generated content.\nTo guide the generation process and ensure high-quality outputs, we employed several prompting\ntechniques, including task specification, constraint-based prompting, and self-contained prompting. Task\nspecification allowed us to break down the question-generation process into distinct, actionable steps.\nConstraint-based prompting helped maintain format integrity, ensuring each question type aligned with\nthe expected answer format and minimized irrelevant content. Self-contained prompting enabled the\nproduction of questions that were independent and clearly understandable without additional context,\nmaking them versatile for use in educational and research settings.\nFor generating theactual question–answer (Q&A) pairs, we utilized GPT-4o-mini, an optimized\nversion of GPT-4 designed for tasks requiring nuanced context understanding and content generation.\ne31-6 Dilara Kizilkaya et al.\nhttps://doi.org/10.1017/eds.2025.10006 Published online by Cambridge University Press\nWith carefully constructed prompts and the extracted text, the model generated a variety of relevant,\nwell-structured questions and corresponding answers. Across multiple iterations, we developed 1,124\nresearch articles and 224 textbook true/false questions, 1,001 research articles and 209 textbook\nmultiple-choice questions , 997 research articles and 225 textbook fill-in-the-blanks questions, and\n2001 research articles and 220 textbook Open-Ended questions. This iterative process involved\nmultiple rounds of refinement, ensuring the questions were accurate, diverse in format, and peda-\ngogically valuable. The final set of question–answer pairs was thus both comprehensive and adaptable,\nproviding a robust resource for academic and research applications, and serving as the cornerstone of\nHydroLLM-Benchmark. Table 1presents example questions categorized by source type and question\ntype.\n2.4. Answer generation and evaluation\nWe utilized three models— GPT-4o-mini, Llama3:8B, and Llama3.1:70B— to generate answers for\nhydrology-specific questions from HydroLLM-Benchmark. To ensure the models provided responses\nin the correct formats for each question type (true/false, multiple-choice, open-ended, and fill-in-the-\nblanks), we developed tailored prompts. This approach allowed us to maintain consistency across models,\nensuring a fair and standardized evaluation of their performance.\nEach question type required a specific prompting strategy. For true/false and multiple-choice ques-\ntions, the prompts were designed to elicit concise and precise answers, minimizing ambiguity. These\nformats required the models to select or generate straightforward responses, making accuracy a critical\nfactor in assessing their performance. In contrast, open-ended and fill-in-the-blanks questions demanded\nmore detailed and context-aware responses. To support this, the prompts included additional context and\nbackground information, encouraging the models to produce nuanced answers that captured the com-\nplexity of hydrological concepts.\nFollowing the generation of answers, we evaluated the models using metrics tailored to the nature of\neach question type. For objective questions (true/false and multiple-choice), accuracy served as the\nprimary evaluation metric, offering a clear measure of the models’ability to generate correct responses.\nFor subjective questions (open-ended and fill-in-the-blanks), we employed cosine similarity to assess the\nsemantic closeness between the generated answers and reference answers. This metric enabled us to gauge\nhow well the models understood and addressed the questions, even when the responses varied in wording\nbut shared the same underlying meaning.\nBy using both accuracy and cosine similarity, we comprehensively evaluated the models’ per-\nformance across diverse question types. This dual-metric approach provided a thorough assessment,\nTable 1. Sample questions from HydroLLM-Benchmark categorized by source and question type\nSource type Question type Sample question\nResearch article True/false True or false: Urbanization has no effect on the frequency and\nintensity of extreme precipitation events.\nResearch article Multiple-choice What advancements in remote sensing technologies have enhanced\nthe monitoring of groundwater storage dynamics?\n(A) The introduction of low-resolution satellite imagery. (B) The\nintegration of machine learning algorithms with multi-platform\nsatellite measurements. (C) The exclusive reliance on traditional\nground-based measurements.\nTextbook Fill-in-the-blanks Hydrology is the science or study of __________.\nTextbook Open-ended What is hydrology, and what aspects of water does it primarily focus\non?\nEnvironmental Data Science e31-7\nhttps://doi.org/10.1017/eds.2025.10006 Published online by Cambridge University Press\nhighlighting the models ’ strengths in handling objective que stions and their ability to generate\ncontextually appropriate responses for subjective ones. Tailoring the prompts and evaluation metrics\nto the specific demands of each question type ensured a rigorous and reliable benchmarking process.\nFigure 1 illustrates the overall system architecture, from question generation to performance evalu-\nation.\n2.5. Data post-processing and model training\nTo prepare a high-quality dataset for evaluating model performance on hydrology-specific tasks, we\ninitiated a comprehensive data post-processing step. This step focused on refining the input data by\nfiltering out Q&A pairs containing overly specific details, such as references to locations, article-\nspecific content, or numerical results. These elements were removed to ensure that the dataset\nremained broadly relevant to hydrology concepts, rather than being tied to specific contexts. For this\ntask, GPT-4o-mini was employed to analyze and flag questions based on these criteria. A specially\ndesigned prompt guided the model to assess eachquestion for such details. Any flagged questions\nwere subsequently excluded from the dataset, resulting in a cohesive and conceptually relevant\ncollection suited for baseline evaluation.\nFollowing the post-processing phase, we assessed the baseline capabilities of three LLMs— GPT-4o-\nmini, Llama3:8B, and Llama3.1:70B— using a range of question types, including true/false, multiple-\nchoice, open-ended, and fill-in-the-blanks. The models were evaluated in their pretrained states without\nany additional fine-tuning, as the primary objective was to gauge their out-of-the-box performance. This\napproach allowed us to establish a baseline understanding of each model’s inherent abilities in addressing\nhydrology-related queries.\nTo ensure proper response formatting, we crafted tailored prompts for each question type, embedding\nspecific instructions to guide the models. For true/false questions, the prompts directed the models to\nmake clear binary selections based on the given content. For multiple-choice questions, prompts guided\nthe models to choose the most appropriate option while minimizing irrelevant details. For open-ended and\nfill-in-the-blanks questions, the prompts encouraged the generation of detailed and contextually nuanced\nanswers, reflecting a deeper understanding of hydrological concepts.\nTo accommodate the complexity of open-ended responses, especially in GPT-4o-mini, we adjusted the\nmaximum token limit to 4,000 tokens. This adjustment was essential to prevent the truncation of longer\nQ&A pairs, ensuring that the responses were comprehensive. Importantly, no further fine-tuning or\ndomain-specific training was applied to any of the models, as the focus remained on evaluating their\nbaseline capabilities in hydrology-related tasks.\nThrough data post-processing, prompt customization, and thorough model evaluation, we effectively\nestablished the strengths and limitations of each model in handling hydrology-specific content.Figure 2\nillustrates the complete process, from data extraction to model output generation and performance\nscoring.\nFigure 1.Conceptual overview of HydroLLM-Benchmark.\ne31-8 Dilara Kizilkaya et al.\nhttps://doi.org/10.1017/eds.2025.10006 Published online by Cambridge University Press\n2.6. Q&A evaluation framework\nTo evaluate the performance of the models on hydrology-specific questions, we developed a structured\nevaluation framework that systematically assessed their responses across multiple question types. This\nframework utilized two distinct metrics— accuracy and cosine similarity— to accommodate the varied\nnature of the question formats: true/false, multiple-choice, open-ended, and fill-in-the-blanks. Each\nmetric was selected to provide an accurate measure of the models’performance based on the specific\nrequirements of each question type.\n2.6.1. Evaluation of objective questions\nFor true/false and multiple-choice questions, which are inherently objective with clear, definitive answers,\naccuracy served as the primary evaluation metric. The evaluation process involved a straightforward\ncomparison of each model’s output with the established ground truth answer. (i)Correct versus incorrect\nclassification: A model’s response was classified as correct if it matched the ground truth, and incorrect\notherwise. (ii)Accuracy calculation: The accuracy score for each model was determined as the percentage\nof correct answers relative to the total number of questions within each objective question type. This\nprovided a clear, binary measure of the model’s ability to identify or select the correct answer.\n2.6.2. Evaluation of subjective questions\nFor open-ended and fill-in-the-blanks questions, which often require a more nuanced understanding,\ncosine similarity was used as the evaluation metric. This metric assesses the semantic alignment between\nthe model-generated answer and the ground truth by measuring the angle between their vector represen-\ntations. (i)Vectorization of responses: Both the model-generated answers and the ground truth answers\nwere transformed into vector form. This allowed for the analysis of responses based on their underlying\nmeaning rather than exact wording. (ii)Cosine similarity calculation: Cosine similarity was computed\nbetween the vector representations of the model’s answer and the reference answer, producing a score\nbetween –1 and 1. Scores closer to 1 indicated a higher degree of semantic similarity.\nUsing cosine similarity provided a nuanced evaluation of the models’ responses for subjective\nquestions, where different phrasings could convey equivalent meanings. This metric enabled us to assess\nthe models’ contextual and semantic comprehension, which is crucial for effective application in\nhydrology-related tasks.\n2.6.3. Aggregating and comparing model performance\nAfter calculating the individual scores for each question type, we aggregated the results to compute an\naverage score for each model across all question formats: (i)Objective scores: The accuracy scores for\ntrue/false and multiple-choice questions were averaged to offer a comprehensive view of each model’s\nperformance on objective, factual questions. (ii)Subjective scores: The cosine similarity scores for open-\nended and fill-in-the-blanks questions were averaged to summarize each model’s ability to generate\nsemantically accurate, contextually relevant responses.\nThis approach allowed us to compile a clear, comparative performance profile for each model— GPT-\n4o-mini, Llama3:8B, and Llama3.1:70B— across various question types. By evaluating objective and\nFigure 2.Post-processing, model output generation, and scoring.\nEnvironmental Data Science e31-9\nhttps://doi.org/10.1017/eds.2025.10006 Published online by Cambridge University Press\nsubjective questions separately, we gained valuable insights into each model’s strengths and limitations in\naddressing various aspects of hydrology-specific queries. This dual-metric framework provided a\nbalanced and comprehensive evaluation, enabling a nuanced understanding of how effectively each\nmodel could interpret, understand, and answer questions relevant to the field of hydrology.\n3. Results\nIn this section, we present baseline results over the benchmark dataset and the evaluation of several model\nperformances on true/false, multiple-choice, open-ended and fill-in-the-blanks question formats, high-\nlighting the strengths and limitations of each model across different content sources, including research\narticles and the textbook. We will explore the implication of these findings for understanding how content\ntype and question format influence.\nTo establish baseline results for our hydrology-specific true/false question set, we evaluated three\nLLMs (i.e., GPT-4o-mini, Llama3:8B, and Llama3:70B) using questions derived from both textbooks\nand research articles. As illustrated inFigure 3, GPT-4o-mini demonstrates consistently high accuracy in\nboth categories, outperforming the other models when responding to textbook-based questions. Lla-\nma3:70B shows comparable performance on textbook-derived items, although it exhibits slightly lower\naccuracy on questions sourced from research articles. By contrast, Llama3:8B maintains moderate\naccuracy levels across both data types but does not match the peak scores observed with the other\nmodels. These results suggest that the models are generally proficient at handling straightforward true/\nfalse inquiries, yet the discrepancy in performance between textbook- and article-based questions\nunderlines the need for further fine-tuning or domain adaptation.\nSimilar results were observed for multiple-choice questions, as illustrated inFigure 4. GPT-4o-mini\nonce again achieved high accuracy, particularly on questions derived from research articles, suggesting a\nrobust capacity for domain-specific inference. Llama3:70B closely followed, displaying comparable\nperformance levels for both textbook- and article-based items. Meanwhile, Llama3:8B maintained\nmoderate accuracy scores but lagged behind the other two models. The consistency of results across\nquestion sources indicates that all three LLMs are well-equipped for tasks requiring precise answer\nselection, although further fine-tuning may be necessary to optimize performance on specialized content.\nShifting the focus to fill-in-the-blanks questions, cosine similarity scores were used to assess how\nclosely each model’s generated text aligned with the correct solutions. As seen inFigure 5, GPT-4o-mini\nemerges as the top performer, producing contextually cohesive completions for both textbook and article\nprompts. Slightly lower scores were obtained by Llama3:70B, although its results remain sufficiently high\nto suggest strong linguistic capabilities. In contrast, Llama3:8B occupies the middle range, capturing the\nmain ideas but sometimes missing finer nuances. This distribution highlights the potential of LLMs to\n81.66\n73.82 77.29\n87.56\n76.01\n87.11\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\ngpt-4o-mini llama3:8b llama3.1:70b\nAccuracy\nModels\nArticle Book\nFigure 3.Accuracy scores for true/false Q&A.\ne31-10 Dilara Kizilkaya et al.\nhttps://doi.org/10.1017/eds.2025.10006 Published online by Cambridge University Press\nexcel in semi-structured tasks, while also revealing the need for targeted improvements to address\nspecialized hydrological terminology.\nFocusing on the open-ended questions, cosine similarity again served as the metric for evaluating\nsemantic alignment betweenn model outputs and reference answers.Figure 6shows that both GPT-4o-\nmini and Llama3:70B scored at the upper end, indicating an aptitude for generating coherent, in-depth\nresponses even when the query allows for wide-ranging expressions. Llama3:8B exhibits only a minor\ndecrease in similarity, suggesting it can still capture essential information but may occasionally lack the\nrefinement displayed by the other two models.\n4. Discussions\nThis section explores the comparative analysis of language model performance, highlights the signifi-\ncance of the HydroLLM-Benchmark dataset as a living resource, and addresses the challenges and\nlimitations observed during the evaluation process.\n4.1. Comparative analysis\nAcross all four question types— true/false, multiple-choice, fill-in-the-blanks, and open-ended— GPT-4o-\nmini consistently emerges as the top performer, maintaining high scores in both objective evaluations\n96.1\n79.3\n92.794.71\n71.15\n95.19\n0\n20\n40\n60\n80\n100\n120\ngpt-4o-mini llama3:8b llama3.1:70b\nAccuracy\nModels\nArticle Textbook\nFigure 4.Accuracy scores for multiple-choice Q&A.\n81.54\n37.68\n49.09\n86.7\n40.83\n49.66\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\ngpt-4o-mini llama3:8b llama3.1:70b\nCosine Similarity\nModels\nArticle Textbook\nFigure 5.Cosine similarity scores for fill-in-the-blanks Q&A.\nEnvironmental Data Science e31-11\nhttps://doi.org/10.1017/eds.2025.10006 Published online by Cambridge University Press\n(true/false and multiple-choice) and subjective measures (fill-in-the-blanks and open-ended). Lla-\nma3:70B closely follows, showing comparable accuracy in multiple-choice and strong semantic align-\nment in open-ended and fill-in-the-blanks tasks, albeit slightly trailing GPT-4o-mini. Meanwhile,\nLlama3:8B registers moderate performance, indicating sufficient competence in handling basic to\nintermediate queries but revealing gaps in handling nuanced or specialized terminology.\nThese findings are particularly noteworthy given the domain-specific nature of our benchmark\ndataset, which comprises hydrology-focused questions derived from textbooks and research articles.\nBy testing each model’s proficiency in both factual and interpretive tasks, this dataset establishes a\nclear baseline for evaluating LLM performance in hydrological knowledge assessment. The highest\noverall accuracies and cosine similarity scores were recorded by GPT-4o-mini, suggesting that it\ncurrently sets the standard for domain-specific question answering within our benchmark. However,\nLlama3:70B ’s relatively close results underscore the potential for models with larger parameter counts\nto excel in specialized fields, provided they undergo targeted fine-tuning or training on hydrology-\nrelated corpora.\n4.2. HydroLLM-Benchmark as a living dataset\nHydroLLM-Benchmark is designed as a living resource, intended to evolve continuously through\nsystematic updates and expansions. As new research articles, updated textbook editions, and hydrology-\nspecific datasets become available, they will be carefully curated and integrated to ensure the benchmark\nremains aligned with cutting-edge advancements in hydrological science.\nThis iterative approach not only maintains the dataset’s relevance and accuracy but also fosters\ncommunity-driven contributions. Researchers, educators, and practitioners are encouraged to submit\nnew data and evaluation methodologies, promoting collaboration and knowledge-sharing across the\nhydrology community. To facilitate community participation, we provide several accessible mechanisms\nvia our GitHub repository. Users can submit questions, feedback, or concerns by opening an issue on the\nGitHub page. We welcome code and content contributions, including new question sets, data processing\nscripts, or model evaluation tools, via standard pull request workflows. Contributors may also reach out\nvia email or community forums to suggest ideas or request features.\nWe also plan to organize collaborative activities such as online workshops, shared evaluation tasks,\nand hackathons through research communities like Cooperative Institute for Research to Operations in\nHydrology and Advancing Earth and Space Science. These initiatives aim to build a collaborative\nnetwork around HydroLLM-Benchmark and encourage knowledge sharing at the intersection of hydrol-\nogy and AI.\n79.1 78.29 79.0279.1 76.12 78.08\n0\n20\n40\n60\n80\n100\n120\ngpt-4o-mini llama3:8b llama3.1:70b\nCosine Similarity\nModels\nArticle Textbook\nFigure 6.Cosine similarity scores for open-ended Q&A.\ne31-12 Dilara Kizilkaya et al.\nhttps://doi.org/10.1017/eds.2025.10006 Published online by Cambridge University Press\nFuture updates may incorporate specialized modules for emerging topics like climate change model-\ning, flood risk analysis, and water resources optimization, broadening the dataset’s applicability. In\naddition, HydroLLM-Benchmark aims to serve as a dynamic educational tool, supporting interactive\nlearning experiences and domain-specific curricula development. By embracing an open framework and a\ntransparent contribution process, HydroLLM-Benchmark aspires to remain a versatile and forward-\nlooking resource, empowering ongoing innovation and advancing AI-driven hydrological research and\neducation.\nBeyond academic evaluation, HydroLLM-Benchmark also holds potential for adaptation to oper-\national hydrology applications such as flood forecasting, drought monitoring, and disaster response.\nFuture extensions of the benchmark could integrate question formats derived from early warning reports,\nhydrological alerts, or emergency management protocols. This direction aligns with recent work such as\nFlash Flood - Bidirectional Encoder Representation from Transformers (FF-BERT), which classifies flash\nflood reports from unstructured text (Wilkho et al.,2024), and LLM studies that assess reasoning under\nadverse weather conditions (Zafarmomen and Samadi,2025). Similarly, hybrid pipelines using LLMs for\nevent-location extraction from social media (Fan et al.,2020) illustrate how natural language under-\nstanding can enhance disaster informatics. By connecting domain-specific benchmarking with these\noperational use cases, HydroLLM-Benchmark can evolve into a practical testbed for evaluating LLM\nreadiness in real-time, high-stakes hydrological decision-making.\n4.3. Challenges and limitations\nThis section discusses the challenges encountered in generating domain-specific questions and the\nlimitations observed in the performance of evaluated LLMs in hydrology-related tasks. The challenges\noutlined include biases in question generation, issues with specificity and relevance, and the complexities\nof crafting high-quality questions in a specialized field. Furthermore, we explore the limitations of these\nmodels in understanding hydrology-specific terminology, managing complex concepts, and addressing\nthe nuances of the domain. Through this analysis, we aim to provide insights into the obstacles faced when\napplying LLMs to hydrology and identify areas for future improvements in model development and\ntraining.\n4.3.1. Challenges in generating domain-specific questions\nGenerating high-quality, domain-specific questions in hydrology presented several challenges. In the case\nof multiple-choice questions, GPT-4o-mini exhibited a consistent bias toward generating questions with\nthe answer“B.”To address this issue, we experimented by running the model multiple times with different\nprompt parameters. One prompt explicitly instructed the model to vary answer choices, while a default\nprompt did not specify particular answer letters. Despite these adjustments, the model’s output continued\nto favor certain options, resulting in nearly 70.4% of the answers being“B,”17% “A,”and only 5.6%“C.”\nTo determine whether this answer bias influenced the model’s overall accuracy, we conducted additional\nexperiments where we shuffled and reassigned answer letters to balance the dataset. Interestingly, this\nbalancing did not significantly impact accuracy, suggesting that the model’s bias toward certain answer\nletters did not detrimentally affect its understanding or response accuracy.\nOpen-ended and fill-in-the-blank questions posed their own unique difficulties. Without specific\ninstructions in the prompts, GPT-4o-mini frequently generated questions with introductory phrases such\nas “In this study…” or “In this article… ,”which were unsuitable for the standalone questions required in\nour dataset. To improve the quality and generality of the questions generated, we refined our prompts to\nexplicitly exclude these introductory phrases, leading to a notable improvement in the final output.\nFurthermore, hydrology’s broad scope, encompassing various geographical locations and historical\ncontexts, added complexity to the question generation process. The model often produced questions that\nwere overly specific, referencing locations or years that were not relevant to the core hydrological content.\nTo mitigate this issue, we incorporated a post-processing step to filter out location- and year-specific\nEnvironmental Data Science e31-13\nhttps://doi.org/10.1017/eds.2025.10006 Published online by Cambridge University Press\nquestions that did not contribute to the intended educational goals. While some references to locations and\nyears can be beneficial for context, we excluded those that did not directly support hydrology-related\nconcepts, ensuring the questions remained general yet accurate in their domain relevance.\nDespite our mitigation efforts, GPT-4o-mini continued to display a notable bias toward selecting“B”\nas the correct answer in multiple-choice questions. We tested several strategies, including rephrased\nprompts, randomized answer orders, altering output token length, and varying temperature settings\n(0.2–0.9), but the output distribution remained largely unchanged. This suggests that the bias may stem\nfrom deeper training artifacts or token-level preferences embedded in the model’s architecture. Such\nbehavior has implications for future benchmarking efforts, as it may introduce unintended skew in answer\nselection. For researchers developing automated assessment tools or training datasets, it is essential to\nconsider these underlying biases and implement techniques such as randomization, controlled answer\nordering, or ensemble prompting to ensure balanced data generation and evaluation.\n4.3.2. Limitations of models\nIn assessing the performance of the models on hydrology-specific content, several notable limitations\nemerge, particularly with fill-in-the-blank and open-ended question formats. These models often display\nreduced accuracy in these question types, primarily due to their challenges in identifying precise\nvocabulary relevant to hydrology. Fill-in-the-blank questions require models to select the correct word\nor phrase, a task complicated by terms that may have similar meanings or context-dependent interpret-\nations. For example, hydrological terms with specific implications can also possess general or alternate\nmeanings in other fields, leading to misinterpretations and incorrect responses. This ambiguity in\nlanguage represents a significant challenge for these models, resulting in errors when selecting the most\ncontextually appropriate terms for hydrology-focused questions, ultimately affecting their overall per-\nformance and highlighting the difficulty of achieving precise understanding in domain-specific contexts\nlike hydrology.\nWhile the models exhibit strong general language understanding, their grasp of hydrology-specific\nterminology and context remains limited. These models may struggle with technical jargon, scientific\nterms, and context-specific language that is prevalent in hydrological research. This limitation can lead to\nless accurate responses for complex queries that necessitate a deep understanding of the domain.\nFurthermore, hydrology often involves complex mathematical equations and statistical models, which\npose challenges for LLMs to interpret accurately. These models have limited capabilities in understanding\nnumerical data and calculations, making them less effective at addressing questions requiring mathem-\natical reasoning or the interpretation of quantitative data.\nContextual understanding of interconnected hydrological processes is another area where these models\nmay falter. Hydrology involves grasping the relationships between groundwater flow, surface runoff, and\natmospheric conditions. The models might not fully capture these interdependencies, resulting in\nresponses that oversimplify or misinterpret complex systems. This limitation is particularly evident in\nquestions requiring the synthesis of information from multiple sources or an understanding of cause-and-\neffect relationships.\nMoreover, the models tend to generate more generalized responses, which may lack the specificity\nneeded for detailed hydrology questions. This issue can be problematic for open-ended questions or those\nrequiring precise, contextually accurate answers based on specific research findings or hydrological\nscenarios. In addition, hydrological analysis often necessitates interpreting visual data, such as satellite\nimagery, hydrological maps, and diagrams. The text-based nature of these models restricts their ability to\nprocess and analyze visual information, limiting their effectiveness in applications that require visual-\nspatial reasoning. Although multimodal capabilities could address this gap, current models lack robust\nintegration with visual data sources.\nThe models also struggle with ambiguous or implicit queries that require contextual interpretation. In\nhydrology, where the same term can have different meanings based on context, such as“flow” in the\ncontext of streamflow versus groundwater flow, the models may produce inconsistent or incorrect\ne31-14 Dilara Kizilkaya et al.\nhttps://doi.org/10.1017/eds.2025.10006 Published online by Cambridge University Press\nresponses if the context is not explicitly provided. In addition, the sensitivity of these models to input\nformatting can affect their responses. Variations in wording, phrasing, or format can lead to different\noutputs, which may not always be consistent or reliable. This sensitivity complicates the Q&A generation\nprocess and may necessitate careful prompt engineering to achieve consistent results.\nThere is also a potential for data leakage, where the models’responses may be influenced by similar\nquestions or answers in their training data. This phenomenon can lead to inflated performance metrics that\ndo not accurately reflect the models’true capabilities in novel or context-specific tasks. Furthermore, the\nlack of extensive real-world validation for these models raises concerns about their effectiveness in\npractical hydrology applications. While responses are evaluated in controlled environments using\nbenchmark questions, their performance in real-world scenarios— such as providing insights for field-\nwork, decision-making, or policy recommendations— remains uncertain.\nFinally, models like Llama3:8B and Llama3.1:70B require significant computational resources,\nincluding high-performance GPUs and extensive memory, to run efficiently. This limitation may restrict\naccessibility for users with limited technical infrastructure or resources, impacting their practical\ndeployment in research or educational settings. These limitations highlight the areas where current LLMs\ncan be improved for more effective application in hydrology-specific tasks, suggesting potential direc-\ntions for further mini and customized model development and fine-tuning.\n5. Conclusion\nIn this study, by collating a broad collection of hydrology textbooks and 2,000 peer-reviewed research\narticles, we introduce a specialized dataset, designed to evaluate question-answering capabilities in the\nhydrology domain. This dataset features diverse question formats— including true/false, multiple-choice,\nfill-in-the-blanks, and open-ended— thus capturing both fundamental concepts and advanced research\ntopics. We defined sample evaluation tasks using GPT-4o-mini, Llama3:8B, and Llama3.1:70B, provid-\ning baseline benchmark results that highlight the strengths and limitations of current LLMs in handling\ndomain-specific queries.\nThe dataset is unfiltered to preserve the complexity and authenticity of real-world hydrological data,\nmaking it suitable for a wide range of machine learning and deep learning applications. Although this\nresource currently focuses on hydrological themes, the insights gleaned from its use may prove valuable\nto broader research areas within environmental sciences. By openly sharing HydroLLM-Benchmark, we\noffer a standardized benchmark to address the lack of unified datasets in hydrological and water resources\nresearch. We strongly encourage other scholars and practitioners to adopt this benchmark dataset in future\nhydrological modeling and AI-driven research studies, furthering the collective understanding and\ninnovation within this critical field.\nLooking ahead, we recognize that hydrological reasoning often requires interpreting data in multimodal\nformats, such as satellite imagery, hydrological maps, and time-series plots. While the current version of\nHydroLLM-Benchmark focuses on text-based questions, future iterations will incorporate these multi-\nmodal components to mirror real-world hydrological analysis tasks more closely. This expansion will\nenable evaluation of advanced models with vision-language capabilities, supporting tasks like flood map\ninterpretation, hydrograph analysis, and spatial reasoning. Integrating multimodal elements is a key next\nstep toward building a comprehensive, domain-aware benchmark for hydrological AI.\nIn addition, the future of the HydroLLM-Benchmark dataset envisions integrating emerging AI model\narchitectures and advancements in natural language processing to improve the evaluation of domain-\nspecific knowledge. By incorporating newer models and technologies, we can track the progression and\nrefinement of AI capabilities in hydrology. This ongoing evolution will also facilitate the testing of\ninnovative training methodologies and optimization techniques, enhancing model performance on\ncomplex, specialized queries. Furthermore, expanding the dataset to include cross-disciplinary content\ncould foster a more holistic understanding of hydrological processes, aiding models in recognizing\ncomplex interconnections between hydrology and related environmental sciences.\nEnvironmental Data Science e31-15\nhttps://doi.org/10.1017/eds.2025.10006 Published online by Cambridge University Press\nCommunity contributions are vital to the growth and effectiveness of the HydroLLM-Benchmark. By\ncultivating an open ecosystem, we invite hydrology experts, AI researchers, and educators to participate\nactively in refining and enriching the dataset. This collaborative effort allows for the inclusion of diverse\nperspectives, enriching the dataset with varied question types and scenarios reflecting real-world\nchallenges. Engaging the community in this manner not only democratizes access to cutting-edge\nresources but also drives transparency and inclusivity in AI research. Through workshops, hackathons,\nand collaborative initiatives, stakeholders are encouraged to explore the dataset’s potential and contribute\ninsights, ensuring its relevance and applicability in addressing global hydrological issues.\nAs the landscape of LLMs continues to evolve rapidly, we also plan to benchmark newer model\nfamilies such as Llama3.2, Llama4, DeepSeek, and other emerging open and commercial models that\noffer advancements in instruction following, multilingual reasoning, and long-context understanding.\nIncorporating these models into HydroLLM-Benchmark will help maintain its relevance for assessing\nstate-of-the-art performance across a diverse set of hydrological tasks.\nOpen peer review.To view the open peer review materials for this article, please visithttp://doi.org/10.1017/eds.2025.10006.\nAuthor contribution. Dilara Kizilkaya: Methodology, software, formal analysis, investigation, data curation, writing— original\ndraft. Ramteja Sajja: Validation, writing— review and editing, visualization, and data curation. Yusuf Sermet: Conceptualization,\nmethodology, writing— review and editing, validation, supervision, and funding acquisition. Ibrahim Demir: Conceptualization,\nmethodology, writing— review and editing, project administration, funding acquisition, and resources.\nCompeting interests. The authors declare none.\nData availability statement.The codebase and dataset are open-source, free to use, and can be accessed on GitHub (https://\ngithub.com/uihilab/HydroQA).\nFunding statement.This project was funded by the National Oceanic and Atmospheric Administration (NOAA) via a cooperative\nagreement with the University of Alabama (NA22NWS4320003) awarded to the Cooperative Institute for Research to Operations in\nHydrology (CIROH). We also acknowledge NSF grant NAIRR240072 for research computing on multimodal language models in\nhydrology.\nDeclaration of generative AI and AI-assisted technologies.During the preparation of this manuscript, the authors used ChatGPT,\nbased on the GPT-4o model, to improve the flow of the text, correct grammatical errors, and enhance the clarity of the writing. The\nlanguage model was not used to generate content, citations, or verify facts. After using this tool, the authors thoroughly reviewed and\nedited the content to ensure accuracy, validity, and originality, and take full responsibility for the final version of the manuscript.\nReferences\nAlabbad Y, Mount J, Campbell AM and Demir I(2024) A web-based decision support framework for optimizing road network\naccessibility and emergency facility allocation during flooding.Urban Informatics 3(1), 10.\nAnderson MP(2007) Introducing groundwater physics.Physics Today 60(5), 42–47.\nChen X, Li L, Chang L, Huang Y, Zhao Y, Zhang Yand Li D(2023) Challenges and contributing factors in the utilization of Large\nLanguage Models (LLMS). arXiv (Cornell University).https://doi.org/10.48550/arxiv.2310.13343\nDavie T and Quinn NW(2019) Fundamentals of Hydrology. In Routledge eBooks.https://doi.org/10.4324/9780203798942\nDemir I, Xiang Z, Demiray B and Sit M(2022) Waterbench: A large-scale benchmark dataset for data-driven streamflow\nforecasting. Earth System Science Data Discussions 2022,1 –19.\nDinh TA, Mullov C, Bärmann L, Li Z, Liu D, Reiß S, Lee J, Lerzer N, Gao J, Peller-Konrad F, Röddiger T, Waibel A, Asfour\nT, Beigl M, Stiefelhagen R, Dachsbacher C, Böhm K and Niehues J(2024) SciEx: Benchmarking Large Language Models on\nScientific Exams with Human Expert Grading and Automatic Grading.Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, 11592–11610. https://doi.org/10.18653/v1/2024.emnlp-main.647\nDolich A, Ebeling P, Stölzle M, Kiesel J, Götte J, Guse B, et al (2024) CAMELS-DE: Benchmark dataset for hydrology–\nsignificance, current status and outlook.EGU24, (EGU24-17667).\nEbert-Uphoff I, Thompson DR, Demir I, Gel YR, Karpatne A, Guereque M, Kumar V, Cabral-Cano E and Smyth P(2017) A\nVISION FORTHE DEVELOPMENT OF BENCHMARKS TO BRIDGE GEOSCIENCE AND DATA SCIENCE. International\nWorkshop Climate Informatics.https://par.nsf.gov/biblio/10143795-vision-development-benchmarks-bridge-geoscience-data-\nscience\nFan C, Wu F and Mostafavi A(2020) A hybrid machine learning pipeline for automated mapping of events and locations from\nsocial media in disasters.IEEE Access 8, 10478–10490.\ne31-16 Dilara Kizilkaya et al.\nhttps://doi.org/10.1017/eds.2025.10006 Published online by Cambridge University Press\nFeng H, Ronzano F, LaFleur J, Garber M, de Oliveira R, Rough K, et al (2024) Evaluation of large language model performance\non the biomedical language understanding and reasoning benchmark: Comparative study.medRxiv, 2024-05.\nHarshbarger JW and Ferris JG(1963) Interdisciplinary training program in scientific hydrology.Groundwater 1(2), 11–14.\nHarvel N, Haiek FB, Ankolekar A and Brunner DJ(2024) Can LLMs answer investment banking questions? using Domain-\nTuned functions to improve LLM performance on Knowledge-Intensive analytical tasks.Proceedings of the AAAI Symposium\nSeries, 3(1), 125–133. https://doi.org/10.1609/aaaiss.v3i1.31191.\nIvanov I(2024) BioLP-bench: Measuring understanding of biological lab protocols by large language models.bioRxiv, 2024-08.\nIzquierdo-Horna LUIS, Zevallos J, Cevallos T and Rios D(2024) Design and creation of a database to assess the information\nneeds of hydrological models.WIT Transactions on Ecology and the Environment 262, 619–629.\nKadiyala LA, Mermer O, Samuel DJ, Sermet Yand Demir I(2024a) The implementation of multimodal large language models for\nhydrological applications: A comparative study of GPT-4 vision, gemini, LLaVa, and multimodal-GPT.Hydrology 11(9), 148.\nKadiyala L, Mermer O, Samuel DJ, Sermet Yand Demir I(2024b) A comprehensive evaluation of multimodal large language\nmodels in hydrological applications.EarthArxiv 7176. https://doi.org/10.31223/X5TQ37\nKadiyala LA, Sajja R, Sermet Y, Muste M and Demir I(2024c) AI-driven decision-making for water resources planning and\nhazard mitigation using automated multi agents.EarthArxiv 8298. https://doi.org/10.31223/X5ZQ57\nKratzert F, Klotz D, Shalev G, Klambauer G, Hochreiter S and Nearing G(2019) Benchmarking a catchment-aware long short-term\nmemory network (LSTM) for large-scale hydrological modeling.Hydrology and Earth System Sciences Discussions 2019,1 –32.\nKreibich H, Schröter K, Di Baldassarre G, Van Loon AF, Mazzoleni M, Abeshu GW, Agafonova S, AghaKouchak A, Aksoy\nH, Alvarez-Garreton C, Aznar B, Balkhi L, Barendrecht MH, Biancamaria S, Bos-Burgering L, Bradley C, Budiyono Y,\nBuytaert W, Capewell L,…Ward PJ(2023) Panta Rhei benchmark dataset: socio-hydrological data of paired events of floods\nand droughts.Earth System Science Data, 15(5), 2009–2023. https://doi.org/10.5194/essd-15-2009-2023\nLi J, Li G, Zhang X, Zhao Y, Dong Y, Jin Z, et al (2024) Evocodebench: An evolving code generation benchmark with domain-\nspecific evaluations.arXiv preprintarXiv:2410.22821.\nMalakar P, Anshuman A, Kumar M, Boumis G, Clement TP, Tashie A, et al (2024) An in-situ daily dataset for benchmarking\ntemporal variability of groundwater recharge.Earth System Science Data Discussions 2024,1 –19.\nNewman AJ, Mizukami N, Clark MP, Wood AW, Nijssen B and Nearing G(2017) Benchmarking of a physically based\nhydrologic model.Journal of Hydrometeorology 18(8), 2215–2225.\nOpenAI (2024, July 18) GPT-4O Mini: Advancing cost-efficient intelligence.https://openai.com/index/gpt-4o-mini-advancing-\ncost-efficient-intelligence\nPursnani V, Sermet MYand Demir I(2024) A conversational intelligent assistant for enhanced operational support in floodplain\nmanagement with multimodal data.EarthArxiv 8264. https://doi.org/10.31223/X52M7W\nPursnani V, Sermet Y, Kurt M and Demir I(2023) Performance of ChatGPT on the US fundamentals of engineering exam:\nComprehensive assessment of proficiency and potential implications for professional environmental engineering practice.\nComputers and Education: Artificial Intelligence 5, 100183.\nRusso L, Mauro F, Sebastianelli A,\nGamba P and Ullo SL(2024) SEN12-WATER: A new dataset for hydrological applications\nand its benchmarking.arXiv preprintarXiv:2409.17087.\nSajja R, Pursnani V, Sermet Y and Demir I(2025) AI-assisted educational framework for floodplain manager certification:\nEnhancing vocational education and training through personalized learning.IEEE Access 13, 42401–42413.\nSajja R, Sermet Y, Cikmaz M, Cwiertny D and Demir I(2024a) Artificial intelligence-enabled intelligent assistant for\npersonalized and adaptive learning in higher education.Information 15(10), 596.\nSajja R, Sermet Y, Cwiertny D and Demir I(2023) Platform-independent and curriculum-oriented intelligent assistant for higher\neducation. International Journal of Educational Technology in Higher Education 20(1), 42.\nSajja R, Sermet Y and Demir I(2024b) End-to-end deployment of the educational AI hub for personalized learning and\nengagement: A case study on environmental science education.EarthArxiv 7566. https://doi.org/10.31223/X5XM7N\nSamuel DJ, Sermet Y, Cwiertny D and Demir I(2024b) Integrating vision-based AI and large language models for real-time water\npollution surveillance.Water Environment Research 96(8), e11092.\nSamuel DJ, Sermet MY, Mount J, Vald G, Cwiertny D and Demir I(2024a) Application of large language models in developing\nconversational agents for water quality education, communication and operations. EarthArxiv, 7056. https://doi.org/10.31223/\nX5XT4K\nSeibert J(2001) On the need for benchmarks in hydrological modelling.Hydrological Processes 15(6), 1063–1064.\nShen J, Tenenholtz N, Hall JB, Alvarez-Melis D and Fusi N(2024) Tag-LLM: Repurposing general-purpose LLMs for\nspecialized domains.arXiv preprintarXiv:2402.05140.\nSingh AK, Kocyigit MY, Poulton A, Esiobu D, Lomeli M, Szilvasy G and Hupkes D(2024) Evaluation data contamination in\nLLMs: How do we measure it and (when) does it matter?.arXiv preprintarXiv:2411.03923.\nSit M, Seo BC and Demir I(2021) Iowarain: A statewide rain event dataset based on weather radars and quantitative precipitation\nestimation, arXiv.arXiv preprintarXiv:2107.03432.\nSoman S and Ranjani HG(2024) Observations on LLMs for telecom domain: Capabilities and limitations. InProceedings of the\nThird International Conference on AI-ML Systems(Art. No. 36, pp. 1–5). Association for Computing Machinery.https://doi.org/\n10.1145/3639856.3639892.\nSzymanski A, Ziems N, Eicher-Miller HA, Li TJJ, Jiang M and Metoyer RA(2024) Limitations of the LLM-as-a-judge\napproach for evaluating LLM outputs in expert knowledge tasks.arXiv preprintarXiv:2410.20266.\nEnvironmental Data Science e31-17\nhttps://doi.org/10.1017/eds.2025.10006 Published online by Cambridge University Press\nThakkar M, More Y, Fournier Q, Riemer M, Chen PY, Zouaq A, et al (2024) Combining domain and alignment vectors to\nachieve better knowledge-safety trade-offs in llms.arXiv preprintarXiv:2411.06824.\nUkarande SK(2023) Irrigation engineering and hydraulic structures. https://doi.org/10.1007/978-3-031-33552-5\nVald GM, Sermet MY, Mount J, Shrestha S, Samuel DJ, Cwiertny D and Demir I(2024) Integrating conversational AI agents\nfor enhanced water quality analytics: Development of a novel data expert system.EarthArxiv, 7202.https://doi.org/10.31223/\nX51997\nWagener T, Dadson SJ, Hannah DM, Coxon G, Beven K, Bloomfield JP, et al (2021) Knowledge gaps in our perceptual model of\nGreat Britain’s hydrology.Hydrological Processes, 35(7), e14288.\nWagener T, Gleeson T, Coxon G, Hartmann A, Howden N, Pianosi F, Rahman S, Rosolem R, Stein L and Woods R(2020) On\ndoing large-scale hydrology with Lions: Realising the value of perceptual models andknowledge accumulation. EarthArXiv\n(California Digital Library).https://doi.org/10.31223/osf.io/zdy5n\nWilkho RS, Chang S and Gharaibeh NG(2024) FF-BERT: A BERT-based ensemble for automated classification of web-based\ntext on flash flood events.Advanced Engineering Informatics 59, 102293.\nXu S, Lu Y, Schoenebeck G and Kong Y(2024a) Benchmarking LLMs’judgments with no gold standard.arXiv preprintarXiv:\n2411.07127.\nXu B, Wen L, Li Z, Yang Y, Wu G, Tang X, et al (2024b) Unlocking the potential: Benchmarking large language models in water\nengineering and research.arXiv preprintarXiv:2407.21045.\nYan H, Hu X, Wan X, Huang C, Zou K and Xu S(2023) Inherent limitations of LLMs regarding spatial information.arXiv\npreprint arXiv:2312.03042.\nYan LK, Li M, Zhang Y, Yin CH, Fei C, Peng B, et al (2024) Large language model benchmarks in medical tasks.arXiv preprint\narXiv:2410.21348.\nZafarmomen N and Samadi V(2025) Can large language models effectively reason about adverse weather conditions?.\nEnvironmental Modelling & Software 188, 106421.\nZheng F, Maier HR, Wu W, Dandy GC, Gupta HV and Zhang T(2018) On lack of robustness in hydrological model\ndevelopment due to absence of guidelines for selecting calibration and evaluation data: Demonstration for data-driven models.\nWater Resources Research 54(2), 1013–1030.\nCite this article:Kizilkaya D, Sajja R, Sermet Y and Demir I (2025). Toward HydroLLM: a benchmark dataset for hydrology-\nspecific knowledge assessment for large language models.Environmental Data Science, 4: e31. doi:10.1017/eds.2025.10006\ne31-18 Dilara Kizilkaya et al.\nhttps://doi.org/10.1017/eds.2025.10006 Published online by Cambridge University Press"
}