{
  "title": "Differentially Private Fine-tuning of Language Models",
  "url": "https://openalex.org/W3206066344",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2122922219",
      "name": "Yu Da",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4311997064",
      "name": "Naik, Saurabh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281722883",
      "name": "Backurs, Arturs",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222071199",
      "name": "Gopi, Sivakanth",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4282362153",
      "name": "Inan, Huseyin A.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224399004",
      "name": "Kamath, Gautam",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4282222885",
      "name": "Kulkarni, Janardhan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2950825866",
      "name": "Lee, Yin Tat",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223146547",
      "name": "Manoel, Andre",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4282732251",
      "name": "Wutschitz, Lukas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287216067",
      "name": "Yekhanin, Sergey",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2745431161",
      "name": "Zhang, Huishuai",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3106203961",
    "https://openalex.org/W2999251207",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W3214715529",
    "https://openalex.org/W136004222",
    "https://openalex.org/W3126575712",
    "https://openalex.org/W2963374540",
    "https://openalex.org/W3166560088",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3175311566",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2115358662",
    "https://openalex.org/W2891003389",
    "https://openalex.org/W3166140588",
    "https://openalex.org/W3038129124",
    "https://openalex.org/W2535690855",
    "https://openalex.org/W3172022218",
    "https://openalex.org/W3102378604",
    "https://openalex.org/W3176828726",
    "https://openalex.org/W3167675683",
    "https://openalex.org/W3188505388",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3170806096",
    "https://openalex.org/W3153675281",
    "https://openalex.org/W2946930197",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3087503988",
    "https://openalex.org/W2963912046",
    "https://openalex.org/W3190860428",
    "https://openalex.org/W3167530662",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W3020531258",
    "https://openalex.org/W3207429447",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W3037464234",
    "https://openalex.org/W2784621220",
    "https://openalex.org/W3035261884",
    "https://openalex.org/W2532781556",
    "https://openalex.org/W2970083812",
    "https://openalex.org/W3125098139",
    "https://openalex.org/W2021426916",
    "https://openalex.org/W1557833142",
    "https://openalex.org/W1992926795",
    "https://openalex.org/W2473418344",
    "https://openalex.org/W3130198422",
    "https://openalex.org/W3196927184",
    "https://openalex.org/W3098071563",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W1873763122",
    "https://openalex.org/W3034751955",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W3212487317",
    "https://openalex.org/W3118216348",
    "https://openalex.org/W2963477238",
    "https://openalex.org/W1985511977",
    "https://openalex.org/W3214482156",
    "https://openalex.org/W3167352803"
  ],
  "abstract": "We give simpler, sparser, and faster algorithms for differentially private fine-tuning of large-scale pre-trained language models, which achieve the state-of-the-art privacy versus utility tradeoffs on many standard NLP tasks. We propose a meta-framework for this problem, inspired by the recent success of highly parameter-efficient methods for fine-tuning. Our experiments show that differentially private adaptations of these approaches outperform previous private algorithms in three important dimensions: utility, privacy, and the computational and memory cost of private training. On many commonly studied datasets, the utility of private models approaches that of non-private models. For example, on the MNLI dataset we achieve an accuracy of $87.8\\%$ using RoBERTa-Large and $83.5\\%$ using RoBERTa-Base with a privacy budget of $ε= 6.7$. In comparison, absent privacy constraints, RoBERTa-Large achieves an accuracy of $90.2\\%$. Our findings are similar for natural language generation tasks. Privately fine-tuning with DART, GPT-2-Small, GPT-2-Medium, GPT-2-Large, and GPT-2-XL achieve BLEU scores of 38.5, 42.0, 43.1, and 43.8 respectively (privacy budget of $ε= 6.8,δ=$ 1e-5) whereas the non-private baseline is $48.1$. All our experiments suggest that larger models are better suited for private fine-tuning: while they are well known to achieve superior accuracy non-privately, we find that they also better maintain their accuracy when privacy is introduced.",
  "full_text": "Diﬀerentially Private Fine-tuning of Language Models∗\nDa Yu† Saurabh Naik‡ Arturs Backurs§ Sivakanth Gopi§ Huseyin A. Inan§\nGautam Kamath¶ Janardhan Kulkarni§ Yin Tat Lee/uni2016 Andre Manoel§\nLukas Wutschitz‡ Sergey Yekhanin§ Huishuai Zhang∗∗\nJuly 18, 2022\nAbstract\nWe give simpler, sparser, and faster algorithms for diﬀerentially private ﬁne-tuning of large-scale\npre-trained language models, which achieve the state-of-the-art privacy versus utility tradeoﬀs on many\nstandard NLP tasks. We propose a meta-framework for this problem, inspired by the recent success\nof highly parameter-eﬃcient methods for ﬁne-tuning. Our experiments show that diﬀerentially private\nadaptations of these approaches outperform previous private algorithms in three important dimensions:\nutility, privacy, and the computational and memory cost of private training. On many commonly studied\ndatasets, the utility of private models approaches that of non-private models. For example, on the MNLI\ndataset we achieve an accuracy of87.8% using RoBERTa-Large and83.5% using RoBERTa-Base with\na privacy budget ofε = 6.7. In comparison, absent privacy constraints, RoBERTa-Large achieves an\naccuracy of90.2%. Our ﬁndings are similar for natural language generation tasks. Privately ﬁne-tuning\nwith DART, GPT-2-Small, GPT-2-Medium, GPT-2-Large, and GPT-2-XL achieve BLEU scores of 38.5,\n42.0, 43.1, and 43.8 respectively (privacy budget ofε = 6.8,δ = 1e-5) whereas the non-private baseline\nis 48.1. All our experiments suggest that larger models are better suited for private ﬁne-tuning: while\nthey are well known to achieve superior accuracy non-privately, we ﬁnd that they also better maintain\ntheir accuracy when privacy is introduced.\n1 Introduction\nDeep learning models are well known to leak sensitive information about the dataset when trained using\nconventional methods (Shokri et al., 2017; Carlini et al., 2019, 2021). To combat this issue, models can\ninstead be trained to guarantee diﬀerential privacy (DP) (Dwork et al., 2006b), a strong notion of data\nprivacy which limits the inﬂuence of any individual training point on the ﬁnal model. While DP is one of the\nfew approaches capable of providing machine learning models with rigorous privacy guarantees, it generally\ncomes at a cost in terms of test accuracy. One oft-cited explanation is that the constraint of DP necessitates\nmuch more training data (Tramèr and Boneh, 2021; Feldman, 2020; Brown et al., 2021). Unfortunately,\nmore training data may be hard to acquire, particularly in settings where privacy is a concern.\nParallel to these developments, Transformer-based (Vaswani et al., 2017) large language models (LLMs),\nincluding the BERT (Devlin et al., 2019; Liu et al., 2019) and GPT (Radford et al., 2018, 2019; Brown\net al., 2020) families, have enabled signiﬁcant progress in natural language processing, achieving state-of-\nthe-art accuracy in almost every task considered. These models are ﬁrst pre-trained on an extremely large\nand diverse public dataset. The weights are then ﬁne-tuned for each task of interest using a much smaller\n∗Aside from the ﬁrst and second authors, all other authors are listed in alphabetical order.\n†Sun Yat-sen University.yuda3@mail2.sysu.edu.cn. Work was done while an intern at Microsoft Research Asia.\n‡Microsoft. {snaik, lukas.wutschitz}@microsoft.com.\n§Microsoft Research.{arturs.backurs, sigopi, huseyin.inan, jakul, amonteiroman, yekhanin}@microsoft.com.\n¶Cheriton School of Computer Science, University of Waterloo. g@csail.mit.edu. Supported by an NSERC Discovery\nGrant.\n/uni2016University of Washington and Microsoft Research.yintat@uw.edu.\n∗∗Microsoft Research Asia.huzhang@microsoft.com.\n1\narXiv:2110.06500v2  [cs.LG]  14 Jul 2022\nFigure 1: An illustration of our framework. First, the model is pre-trained on a large, public dataset. Next,\nnew parameters are introduced and privately ﬁne-tuned on a smaller, private, task-speciﬁc dataset. The\noriginal parameters are frozen during this process. Finally, the ﬁne-tuned new parameters may be released\npublicly and plugged-in to the model for downstream tasks, while still preserving privacy of the private\ndataset.\nTable 1: Accuracy of ﬁne-tuning for downstream tasks with RoBERTa-Large (in %). Our results achieve\naccuracy comparable to full ﬁne-tuning non-privately, while simultaneously guaranteeing diﬀerential privacy\nand modifying less than1% of the parameters. We chooseδ =1e-5 for SST-2 and QNLI andδ =1e-6 for\nMNLI and QQP due to their dataset sizes. Implementation details are in Section 4.1.\nMethod MNLI SST-2 QQP QNLI Avg. Trained params\nNon-private ﬁne-tuning 90.2 96.4 92.2 94.7 93.4 100%\nOur results (ε= 6.7) 87.8 95.3 87.4 90.8 90.3 0.94%\ntask-speciﬁc dataset. For example, a single pre-trained GPT-family model may be ﬁne-tuned for various\ndownstream tasks, such as email reply suggestion, sentence completion in text editors, language translation,\nand more. This two-stage paradigm can naturally be adapted to solve tasks in private learning, automatically\naddressing the aforementioned data shortage issue via the massive scale of the public pre-training dataset.\nOne may pre-train the model on public data as usual,1 but then ﬁne-tune the modelprivately.\nDespite the success of these models, task-speciﬁc ﬁne-tuning introduces a number of technical challenges.\nIn the non-private setting, the immense size of LLMs makes it impractical to ﬁne-tune the full model and\nstore a separate copy of the parameters for hundreds of downstream tasks. Things only get worse with\nprivacy, which leads to overheads in terms of running time, memory usage, and most importantly, accuracy.\nThe magnitude of noise introduced to a model due to DP grows as the model size increases (Bassily et al.,\n2014; Abadi et al., 2016; Bun et al., 2014), which can overwhelm any signal for larger models. A recent\nline of work in the non-private literature has proposed parameter-eﬃcient methods to alleviate the issues of\nstorage and computational cost for ﬁne-tuning (Houlsby et al., 2019; Li and Liang, 2021; Aghajanyan et al.,\n2020; Hu et al., 2021; Mahabadi et al., 2021). The main focus of our work is to explore parameter-eﬃciency\nin the context of private learning.\n1.1 Our Contributions\nOur primary contribution is to show that advanced parameter-eﬃcient methods can lead tosimpler and\nsigniﬁcantly improved algorithms for private ﬁne-tuning. Our framework is illustrated in Figure 1. Our\n1Despite the fact that the pre-training data is public, there may nonetheless be privacy concerns related to personal or\ncopyrighted data. However, since these pre-trained models have already been released, any associated privacy loss has already\nbeen incurred.\n2\nTable 2: Fine-tuning GPT-2 models on the DART dataset. We observe that larger models have better utility,\nboth in absolute numbers, and in terms of preserving non-private utility. DP parameters are (ε = 6.8,δ =\n1e-5).\nModel BLEU (DP) BLEU (non-private) Drop due to privacy\nGPT-2-Medium 42.0 47.1 5.1\nGPT-2-Large 43.1 47.5 4.4\nGPT-2-XL 43.8 48.1 4.3\nﬁndings and contributions are summarized as follows:\n• State-of-the-art utility and privacy. Empirical evaluation of our algorithms reveals that they\nachieve state-of-the-art accuracy versus privacy tradeoﬀs, improving upon the previous best (Yu et al.,\n2021b). More importantly, for many ﬁne-tuning tasks, the utility of models trained with DP approaches\nthat of non-private models. For example, privately ﬁne-tuning RoBERTa-Large on the MNLI data\nset (Williams et al., 2018), we achieve an accuracy of87.8% with a privacy budget of (ε= 6.7,δ = 1e-\n6). Without privacy guarantees, RoBERTa-Large achieves an accuracy of90.2% (GPT-3 is known to\nachieve 91.7% (Hu et al., 2021)); see Table 1 for a summary. We also explore private natural language\ngeneration tasks, ﬁne-tuning GPT-2 models on the E2E dataset (Novikova et al., 2017). Again, the\nutility approaches non-private levels: we achieve a ROUGE-L score of 67.8 with GPT-2-Large and\n(ε= 6.0,δ = 1e-5), compared to 72.0 without privacy.\n• Larger models are better.Prior work has consistently shown that larger language models achieve\nbetter accuracy for downstream tasks. Our results give evidence that this phenomenon extends to the\nprivate setting. For example, on the MNLI dataset, RoBERTa-Base achieves an accuracy of83.5%\n(versus 87.6% non-privately, a drop of4.1%) whereas RoBERTa-Large achieves an accuracy of87.8%\n(versus 90.2% non-privately, a drop of2.4%), both under a privacy budget of(ε = 6 .7,δ = 1e-6).\nSimilarly, privately ﬁne-tuning (using LoRA (Hu et al., 2021)) on DART (ε = 6.8,δ = 1e-5), GPT-\n2-Medium achieves a BLEU score of 42.0 (versus 47.1 non-privately, a drop of 5.1) while GPT-2-XL\nachieves a BLEU score of 43.8 (versus 48.1 non-privately, a drop of 4.3), see Table 2. Observe that\nutility improves with model size in two ways: both in terms of absolute numbers, as well as the drop\nincurred due to privacy. While the power of large models has been established in the non-private\nsetting, we ﬁnd this phenomenon quite surprising under DP. There is often a tension when choosing\nprivate model architectures: larger models may have higher capacity, but necessitate the introduction\nof more noise. Consequently, smaller and simpler private models achieve the better accuracy in several\nsettings (Papernot et al., 2019; Tramèr and Boneh, 2021). In contrast, our experiments show that\nﬁne-tuning the biggest models achieves the best accuracy,2 which we consider to be one of our main\nﬁndings.\n• Simpler, sparser, and faster.Beyond accuracy concerns, DP requirements also lead to signiﬁcant\noverheads in terms of computation and memory usage. The large number of parameters contributes\nto the high cost of training LLMs, and things get worse under privacy, which has been documented\nto increase training time by up to two orders of magnitude (Carlini et al., 2019; Subramani et al.,\n2021). The parameter-eﬃcient approaches we employ partially oﬀset this issue: as we only update a\nsmall fraction of the total number of parameters, training becomes considerably more computationally\nand memory eﬃcient. Furthermore, as in the non-private setting, this framework leads to a modular\ndesign, where a single large pre-trained model can be augmented with lightweight modiﬁcations for\neach individual downstream task.\nTo the best of our knowledge, we are the ﬁrst to ﬁne-tune GPT-2-XL using diﬀerential privacy, the\nlargest model (with 1.5B parameters) trained thus far using DP. Given our state-of-the-art results for a\nvariety of standard NLP tasks using advanced ﬁne-tuning techniques, we believe that our paper will serve\n2An alternative perspective is that what we currently think of as “large” language models are relatively small, and we are\nyet to reach the point where the beneﬁts of model size on accuracy are outweighed by the drawbacks.\n3\nas a benchmark for further work in this direction. For example, the best average accuracy achieved by the\nprior work of Yu et al. (2021b) on four standard NLP tasks in Table 1 is83.9% using ε= 8 (and the same\nδ as in Table 1), whereas we can achieve an average accuracy of90.3% using ε = 6.7 by a combination of\nbetter algorithms, larger models, and new privacy accounting techniques.\nFinally, though recently considered elsewhere (see Section 5), we put further focus on the framing of\npublic pre-training and private ﬁne-tuning as an important conceptual direction in DP deep learning.\n2 Preliminaries and Prior Algorithm Baselines\nRecall the formal deﬁnition of diﬀerential privacy.\nDeﬁnition 2.1 (Diﬀerential Privacy (DP) (Dwork et al., 2006b,a)). A randomized algorithmAis (ε,δ)-\ndiﬀerentially private if for any two neighboring datasetsD and D′, which diﬀer in exactly the data pertaining\nto a single user, and for all setsSof possible outputs:Pr[A(D) ∈S] ≤eε Pr[A(D′) ∈S] + δ.\nWe review prior techniques for private ﬁne-tuning.\n2.1 Full Fine-tuning via DPSGD\nTo train a machine learning model with privacy, the most popular algorithm is the celebrated DP stochastic\ngradient descent (DPSGD) (Song et al., 2013; Bassily et al., 2014; Abadi et al., 2016). This optimization\nmethod serves as a drop-in replacement for SGD, augmenting it with the addition of per-example gradient\nclipping and Gaussian noise addition steps. These two steps serve to limit and mask the contribution\nof a single example. Two key points to note are that a) per-example gradient clipping incurs signiﬁcant\ncomputational and memory overheads in most implementations, and b) noise introduced due to privacy\ngrows as the square-root as the number of model parameters. With this tool in place, the most basic\nﬁne-tuning strategy is to train all parameters using DPSGD.\n2.2 Reparametrized Gradient Perturbation\nTo mitigate the limitations of DPSGD, a recent work of Yu et al. (2021b) introduced an elegant method called\nreparametrized gradient perturbation(RGP). RGP exploits the implicit low-rank structure in the gradient\nupdates of SGD to substantially improve upon DPSGD. Speciﬁcally, they reparametrize each layer’s weight\nmatrix W into LR+ ˜W, whereLand Rare low-rank gradient-carrier matrices and˜W is the residual weight.\nThe authors show that one can obtain a low-dimensional projection ofW’s gradient by taking gradients\nonly of the low-rank matricesL and R (and not the high-rank ˜W). Privacy is introduced by clipping and\nnoising these low-dimensional gradients ofLand R. While this low-dimensional projection loses some of the\nsignal inW’s gradient, it turns out to contain enough to still achieve high accuracy. At the same time, the\nlow-dimensional gradients alleviate the aforementioned issues related to privatization, signiﬁcantly reducing\nthe memory consumption and noise introduced. Although RGP uses a low-rank update at each step, we\nempirically verify that its accumulated update is not of low stable rank and hence can not be compressed\ninto small plug-in modules. Possible reasons include: 1) the low-rank subspaces of RGP are diﬀerent at\ndiﬀerent updates; 2) the accumulated update of RGP contains all the added noises, which are of high stable\nrank.\n3 Our Approach\n3.1 A Meta-framework\nWe introduce our approach as a meta-framework for private deep learning, which abstracts the key principles\nof recent ﬁne-tuning methods.\nSuppose f(WPT; x) is a pre-trained model whereWPT are the pre-trained weights andx is any input.\nWe create a new ﬁne-tuning model\nfFT(WPT,θ; x) (1)\n4\nwhich incorporates additional trainable parametersθ, where dim(θ) ≪dim(WPT). That is, the number\nof new parameters inθ is a small fraction of the original number of parameters in the pre-trained weights\nWPT. Fine-tuning is done by running DPSGD on the additional parametersθ, while freezing the weights of\npre-trained modelWPT. The new parameters are initialized toθ0 such that\nfFT(WPT,θ0; x) = f(WPT; x). (2)\nThe initialization condition (2) is very important, as it ensures that ﬁne-tuning starts at the pre-trained\nmodel and improves it by modifying the parametersθ. Most ﬁne-tuning methods are additive and have the\nfollowing special form:\nfFT(WPT,θ; x) = f(WPT + π(θ); x), (3)\ni.e., they modify the pre-trained weights by adding a correction termπ(θ) parametrized byθ.\nRecent work in the non-private literature has described concrete instantiations of this framework (Houlsby\net al., 2019; Mahabadi et al., 2021; Hu et al., 2021), which (crucially) are eﬀective whendim(θ) ≪dim(WPT).\nIn the non-private setting, such reparametrizations are useful for reducing the computation and memory\nrequired for ﬁne-tuning, and enable lightweight and plug-in modiﬁcations to the base model for diﬀerent\ndownstream tasks. At the same time, they maintain (or sometimes surpass) the accuracy achieved by full\nﬁne-tuning.\nWe give some intuition as to why parameter-eﬃcient methods could be more eﬀective for private ﬁne-\ntuning especially when private datasets are small. For simplicity, we assume that the ﬁne-tuning method\nis additive as in (3), such that the ﬁne-tuned weightsWFT = WPT + π(θ). We can imagine thatWFT lies\non a manifold passing throughWPT of very small dimension (equal to the dimension ofθ) compared to the\ndimension ofWPT. Even if the parametersθ are very noisy due to the noise added during DPSGD, we will\nalways stay in this manifold. In particular, we are not disturbing the pre-trained weights in most directions\n(those orthogonal to the manifold nearWPT). If we run DPSGD on all the weights instead, then we add\nnoise in all directions, thus potentially unlearning the knowledge learned during pre-training, especially in\nlow data regimes; see the discussion in Section 4.3 for more on this.\nBesides substantial gains in the accuracy, the above method of reparametrization has several other ad-\nvantages:\n• A single pre-trained model such as BERT or GPT is generally applied to hundreds of downstream\ntasks via ﬁne-tuning. Private ﬁne-tuning using previous methods requires updatingall parameters and\nstoring a diﬀerent copy of the ﬁne-tuned model per task. This creates substantial overheads for storing\nand deploying, and can be very expensive in practice. On the other hand, the reparametrization (1)\nmeansthatweonlyneedtostoreasinglepre-trainedmodelthatcanbesharedacrossmanydownstream\ntasks. Each downstream task requires only a small number of new parameters that can be plugged in.\n• Diﬀerentially private training requires computing and storing per-example gradients, which increases\nthe memory footprint. In our approach, however, learning is done in a much lower dimension, hence\nsaving on the memory cost as compared to prior works.\n• Finally, we expect that (1) also gives a more communication-eﬃcient method of ﬁne-tuning in dis-\ntributed settings such as federated learning, due to the signiﬁcantly smaller number of parameters\nlearned during ﬁne-tuning.\n3.2 Instantiating the Meta-framework\nIn this section, we discuss a few ways to instantiate our meta-framework. This list is non-exhaustive, but\ncovers the methods we employ in our experiments.\n3.2.1 Fine-tuning via Low-Rank Adaptation\nLow-Rank Adaptation (LoRA) (Hu et al., 2021) is an additive ﬁne-tuning scheme as deﬁned in (3). For each\ndense weight matrixWi\nPT of sizea×b in the pre-trained network, we add a low-rank correction termLiRi,\ni.e.,\nWi = Wi\nPT + LiRi, (4)\n5\nwhere Li ∈Ra×r,Ri ∈Rr×b are new trainable parameters. Hu et al. (2021) apply this reparameterization\nonly to the Transformer attention weights (Wq,Wv), and freeze all other weights (e.g.,Wk and Wo and\nthose in the feed-forward layers). The rankr is typically chosen to be small, e.g.,r = 4,16,64. Since most\nparameters in Transformer architectures are dense weight matrices, choosing a smallr results in a nearly\nsquare-root reduction in the number of parameters.\n3.2.2 Fine-tuning via Adapters\nHoulsby et al. (2019) propose adapter-based ﬁne-tuning, in which we modify the architecture of the pre-\ntrained model by adding new “adapter” layers after each attention and feed-forward layer. Adapter layers\nare bottleneck layers with residual connections. Speciﬁcally, given an inputx, an adapter layerA performs\nA(x) = U(τ(D(x))) + x, (5)\nwhere U is an up-projection aﬃne linear map,Dis a down-projection aﬃne linear map, andτ is a non-linear\nactivation function such as the Gaussian error Linear Unit (GeLU) (Hendrycks and Gimpel, 2016). Ifxhas\ndimension d, then U ∈Rd×r,D ∈Rr×d for some r ≪d. Thus, the number of introduced parameters is\nsigniﬁcantly less than the number of parameters in the pre-trained model. When ﬁne-tuning, the parameters\nof the original model are frozen, and only parameters of the adapter layers, as well as layer normalizations,\nare modiﬁed. Note that ﬁne-tuning with adapters is not an additive ﬁne-tuning framework as in (3), but is\ncaptured by the broader framework in (1).\n3.2.3 Fine-tuning via Compacter\nThe recent work of Mahabadi et al. (2021) introduces Compacters (Compact adapters), a method which\nfurther improves the parameter eﬃciency of adapters. This is done by replacing the dense matrices in the\nup-projection U and down-projection D by tensor products of smaller matrices, thus reducing the number\nof trainable parameters. Speciﬁcally, they replace the dense matrixMℓ in the adapter layerℓby a low-rank\nparameterized hypercomplex multiplication (LPHM) layer, i.e., each dense matrixMℓ ∈Ra×b is expressed\nas\nMℓ =\nn∑\ni=1\nAi ⊗\n(\nSℓ\ni Tℓ\ni\n)\n(6)\nwhere Ai ∈Rn×n,Sℓ\ni ∈Ra/n×k,Tℓ\ni ∈Rk×b/n and ⊗is the matrix Kronecker product. Note the matrices\nAi are not indexed by the layerℓ because these matrices are shared among all the adapter layers. Since\neach adapter layers has two dense matrices (one for up-projection and one for down-projection), if there are\nL adapter layers, this reduces the number of parameters fromL(2ab) to L(2(a+ b)k) + n3. In practice, a\nand b are chosen to be either the model dimensiond or the intermediate representation dimensionr in the\nadapters, n is typically chosen to be a small constant such asn= 2,4,8,12 and k is chosen to be1.\n3.2.4 Why Does Parameter-Eﬃcient Tuning Work?\nTheoretical explanation of the success of parameter-eﬃcient ﬁne-tuning methods is an active area of research\nin deep learning. Indeed, since trends have consistently shown that model accuracy increases with size,\nhow can one achieve competitive accuracy while ﬁne-tuning less than1% of the parameters? One popular\nhypothesis isintrinsic dimensionality(Li et al., 2018), which posits that the minimum number of parameters\nneeded to train a machine learning model may be much less than the total number of model parameters.\nAghajanyan et al. (2020) explore this hypothesis in the context of ﬁne-tuning LLMs, showing that one can\nachieve most of their accuracy by training only a very small number of parameters (chosen via a random\nprojection). Perhaps surprisingly, they ﬁnd that asthe model size increases, intrinsic dimension decreases,\nin the limit exhibiting zero-shot learning. While we did not explore this hypothesis in the context of DP due\nto computational restrictions, we believe it may be an interesting lens through which one can understand\nthe eﬀectiveness of private parameter-eﬃcient ﬁne-tuning.\n6\nTable 3: Memory and speed comparison for RoBERTa-Large. The rank is chosen asr = 16 for RGP and\nLoRA. The speed is measured by the wall-clock time for training one epoch of the SST-2 dataset on a single\nTesla V100 GPU with gradient accumulation for batch size 2000.\nMethod Memory (GB) Speed (seconds per epoch)\nFull ﬁne-tuning (DPSGD) 27.9 715\nRGP 9.1 296\nDP LoRA 6.1 271\n3.3 Comparision with Baseline Algorithms\nWe highlight some key algorithmic diﬀerences between our proposed methods and the baselines of full ﬁne-\ntuning and RGP.\n• DPSGD and RGP both require updating all parameters of the pre-trained model, whereas our proposed\nmethods update only a tiny fraction (between0.05% and 1%). The rightmost columns of Tables 4 and 5\nlist the number of parameters trained by these algorithms.\n• RGP performs a low-rank decomposition of weight matrices which is very similar to LoRA, though\nthere are subtle diﬀerences. Recall that in RGP, at the beginning of each iterationt, the historical\nweight matrixWt−1 is decomposed to ﬁnd a low-rank productLR. The gradients computed onLand\nR are then projected back to the full parameter space to perform the descent step. Hence, RGP does\nnot keep the pre-trained weights frozen during the learning process.\nLoRA can be viewed as a simpliﬁcation of RGP. LoRA reparametrizesWFT := WPT + LR, where the\npre-trained weight matrixWPT is frozen during training. Hence, compared to RGP, LoRA eliminates\nthe decomposition and the projection to the full parameter space at each iteration, simplifying the\nimplementation and reducing the running time and memory cost. This is summarized in Table 3. We\nobserve that DP LoRA reduces the memory cost by about33% and the training speed by8%. As we\nwill see, this simpliﬁcation also results in improved utility.\n• Neither full ﬁne-tuning nor RGP fall into our meta-framework described by (1). Thus, if a pre-trained\nmodel is to be applied to several downstream tasks, one must store a separate set of weights for each\ntask, incurring a signiﬁcant memory cost and losing the plug-in functionality. In contrast, our methods\nare much more lightweight.\n4 Experiments\nWe experimentally evaluate our methods for DP ﬁne-tuning to demonstrate their utility, privacy, and\nparameter-eﬃciency. We investigate bothlanguage understanding and textgeneration tasks, using RoBERTa\nand GPT-2 models, to establish that our techniques are applicable to a variety of tasks and model architec-\ntures.3\n4.1 Fine-Tuning for Language Understanding Tasks\nWe ﬁrst compare our methods with state-of-the-art ﬁne-tuning algorithms using models from the BERT\nfamily, which was used in the prior work (Yu et al., 2021b). Speciﬁcally, we use RoBERTa models (Liu et al.,\n2019),4 which are pre-trained on public data collected from the web. RoBERTa-Base has 125M parameters\nand RoBERTa-Large has 355M parameters. We ﬁne-tune the pre-trained models on four tasks: MNLI, QQP,\nQNLI and SST-2 from the GLUE benchmark (Wang et al., 2018), following Yu et al. (2021b).\nImplementationDetails: Forﬁne-tuningwithadapters, wemaychoosetheintermediaterepresentation\ndimension r, shared across all adapter layers. Similarly, for ﬁne-tuning with Compacter, we can choose both\n3Code for our experiments is available at https://github.com/huseyinatahaninan/\nDifferentially-Private-Fine-tuning-of-Language-Models .\n4The model can be found athttps://github.com/pytorch/fairseq/tree/master/examples/roberta.\n7\nTable 4: Accuracy for ﬁne-tuning downstream tasks with RoBERTa-Base (in %). The privacy parameters\nare ε= 6.7, andδ=1e-5 for SST-2 and QNLI and 1e-6 for MNLI and QQP. Bold indicates the best accuracy\nwith DP. Numbers for non-private ﬁne-tuning are from Liu et al. (2019).\nMethod MNLI SST-2 QQP QNLI Avg. Trained params\nFull w/o DP 87.6 94.8 91.9 92.8 91.8 100%DP 53.1 82.6 74.4 63.9 68.5\nLoRA w/o DP 87.5 95.1 90.8 93.3 91.7 0.24%\nRGP5 DP 80.1 91.6 85.5 87.2 86.1 100%\nAdapter DP 83.4 92.5 85.6 87.5 87.3 1.4% (r= 48)\nCompacter DP 82.6 92.3 84.7 85.1 86.2 0.055% (r= 96, n= 8)\nLoRA DP 83.5 92.2 85.7 87.3 87.2 0.94% (r= 16)\nTable 5: Accuracy for ﬁne-tuning downstream tasks with RoBERTa-Large (in %). The privacy parameters\nare ε = 6.7, and δ =1e-5 for SST-2 and QNLI andδ =1e-6 for MNLI and QQP. Bold indicates the best\naccuracy with DP. Numbers for non-private ﬁne-tuning are from Liu et al. (2019).\nMethod MNLI SST-2 QQP QNLI Avg. Trained params\nFull w/o DP 90.2 96.4 92.2 94.7 93.4 100%\nLoRA w/o DP 90.6 96.2 91.6 94.9 93.3 0.23%\nRGP DP 86.1 93.0 86.7 90.0 88.9 100%\nAdapter DP 87.7 93.9 86.3 90.7 89.7 1.4% (r= 48)\nCompacter DP 87.5 94.2 86.2 90.2 89.5 0.053% (r= 96, n= 8)\nLoRA DP 87.8 95.3 87.4 90.8 90.3 0.94% (r= 16)\nthe intermediate representation dimension r and the Kronecker product kernel dimensionn in (6). For\nLoRA ﬁne-tuning, we add bottleneck branches for both the attention layers and the feedforward layers,\nwhich diﬀers slightly from the addition of bottleneck branches for only theWq and Wv matrices of the\nattention layers as done by Hu et al. (2021). Given the same bottleneck representation dimensionr in (4),\nour new implementation uses twice as many trainable parameters as the original paper, and achieves some\nimprovements for learning with DP. We perform privacy accounting using the PRV Accountant from Gopi\net al. (2021), which currently provides the tightest bounds.\nHyperparameter choice: Given the large number of hyperparameter choices, e.g., the intermediate\nrepresentationdimension, learningrate, weightdecay, privacydelta, andmodelsize, anexhaustivegridsearch\nover all hyperparameters is expensive, due to the model sizes. Our hyperparameter choices are informed by\nprior work and are as follows. For privacy parameters, we useδ= 1e-5 for SST-2 and QNLI andδ= 1e-6 for\nQQP and MNLI due to their dataset sizes, and use noise multipliers0.92,0.83,0.66 and0.65 for SST-2, QNLI,\nQQP and MNLI, respectively, which is the same as Yu et al. (2021b)6. The clipping threshold of per-example\ngradients is10 for all methods. For adapters and Compacter, we follow suggestions in the original papers\nand chooser from a set{16,48,96}and n from a set{4,8,12}. For LoRA, we choose the best-performing\nrank r from the set{4,16,48,64}. The best performing hyperparameters are noted in Tables 4 and 5. We\nuse batch size 2000 and train with half-precision for 20 epochs. We use the optimizer AdamW (Loshchilov\nand Hutter, 2019) with weight decay 1e-2 and search over four learning rates {5e-4, 1e-3, 2e-3, 5e-3}. In\nAppendix B, we show the proposed algorithms perform well for a wide range of hyperparameters.\nResults: We report the prediction accuracy on four tasks in Tables 4 and 5. Our experiments using\nRoBERTa-Base serve as a direct comparison to the work of Yu et al. (2021b) who only trained the base\nmodel, whereas RoBERTa-Large experiments demonstrate the signiﬁcance of using larger models. We could\nnot report the numbers for full ﬁne-tuning using DPSGD on RoBERTa-Large due to running time and\nmemory costs; see the discussion at the end of this section. We summarize our key ﬁndings:\n5We report RoBERTa-Base numbers fromhttps://github.com/dayu11/Differentially-Private-Deep-Learning , by the\nauthors of Yu et al. (2021b). Though the paper itself only reports results on BERT-Base, we cite their paper to also reference\nthe RoBERTa numbers.\n6In Appendix A, we evaluate the proposed framework with various choices of privacy parameters.\n8\n• Onall datasets, our methods achieve the best accuracy while training a only tiny fraction of parameters;\nlarger models give signiﬁcant improvements.\n• Noticeable improvements in the privacy parameterεversus Yu et al. (2021b) are primarily due to new\nprivacy accountants based on Fourier-based numerical composition (Koskela et al., 2020, 2021; Gopi\net al., 2021); we use the PRV Accountant from Gopi et al. (2021) since it is the most eﬃcient.\n• Private adapters provide the best average performance for RoBERTa-Base, whereas LoRA outperforms\nall other methods for RoBERTa-Large.\nRemark 4.1.A concurrent work by Li et al (Li et al., 2022) show that using a larger batch size and training\nwith full-precision improves the performance of DPSGD, and obtains similar performance as our algorithm.\nThus, poor performance of DPSGD in our experiments is due to the suboptimal choice of hyperparameters\nand also due to precision issues, although we use same hyperparameters for all the algorithms. We run new\nexperiments with hyperparameters of (Li et al., 2022) in full precision mode, and get improvements around\n1% even for our algorithms. We report these ﬁndings in Appendix C.\n4.2 Fine-tuning for Natural Language Generation (NLG)\nNext, we study private ﬁne-tuning for text generation problems using the GPT-2 series of models on the End-\n2-End (E2E) NLG challenge (Novikova et al., 2017) and DART (Nan et al., 2021), two primary benchmarks\nused in recent works on non-private ﬁne-tuning (Hu et al., 2021; Li and Liang, 2021). We use GPT-2-Small\n(117M parameters), GPT-2-Medium (345M parameters), GPT-2-Large (774M parameters), and GPT-2-XL\n(1.5B parameters).7 To the best of our knowledge, we are the ﬁrst to privately ﬁne-tune for E2E-DART or\nﬁne-tune GPT-2-XL. The purpose of this section is not to evaluate various ﬁne-tuning algorithms, but to\nshow that private ﬁne-tuning is competitive with non-private ﬁne-tuning for text generation problems. Due\nto the high cost of training, we report experimental results only for ﬁne-tuning (private and non-private)\nwith LoRA. We think that all ﬁne-tuning methods in this paper should achieve comparable accuracy.\nE2ENLGchallenge: TheE2EdatasetwasintroducedbyNovikovaetal.(2017), andcontainstemplate-\nlike information in the restaurant domain to be mapped to natural language with end-to-end training. The\ndataset consists of 42K training samples, 4.6K validation samples, and 4.6K test samples.\nDART:DART was introduced as an open-domain data-to-text dataset by Nan et al. (2021). The dataset\nconsists of 62K training samples, 6.9K validation samples, and 12K test samples. In comparison to E2E, the\ndataset is larger and the task is more challenging.\nWe use standard metrics such as BLEU, ROUGE-L, etc., used in (Hu et al., 2021) for measuring the\nquality of predictions.\nHyperparameter choice:For LoRA, we choose the bottleneck rankr= 4 in (4) and ﬁne-tuneWq and\nWv matrices of the attention layers as in the original paper. We optimize using AdamW with learning rate\n4e-4, weight decay 1e-2 and train our models for 20 epochs. We use batch size 128 for the experiments on\nE2E and batch size 256 for the experiments on DART. We take the gradient clipping parameter to be 1.0\nand the noise multiplier to be 0.6 for the accountant in Gopi et al. (2021), achievingε= 6.0,δ =1e-5 on E2E\nand ε= 6.8,δ =1e-5 on DART.\nResults: The results of our experiments are summarized in the Table 6 and 7, which reiterate the\nmain themes of our work: private ﬁne-tuning with parameter-eﬃcient approaches perform close to their\nnon-private counterparts and show consistent improvement in the utility as model size increases. Note that\non E2E dataset, although the validation perplexity improves as the model becomes larger, the metrics seem\nto saturate going from large to XL for both private and non-private cases. On the other hand, for DART\ndataset both validation perplexity and the metric improve as the model size increases.\n4.3 How Bad is DPSGD?\nWhile our experiments indicate that full ﬁne-tuning does not achieve competitive performance, there could\nbe a choice of hyperparameters that improves upon the reported numbers, e.g., “mega” batch sizes (in the\nmillions) in Anil et al. (2021); Li et al. (2022). We note that our main message is that one does not need\n7https://huggingface.co/transformers/model_doc/gpt2.html.\n9\nTable 6: Metrics on the E2E NLG task. Non-DP results from Hu et al. (2021), except for GPT-2-XL, which\nwas not reported in the paper. We ran GPT-2-XL with hyperparameters presented in Hu et al. (2021). Bold\nindicates the best accuracy with DP. DP parameters are (ε= 6.0,δ = 1e-5). Val perp stands for validation\nperplexity.\nMethod Val perp BLEU NIST MET ROUGE-L CIDEr\nGPT-2-Small + DP 4.51 63.8 7.19 39.5 67.5 1.87\nGPT-2-Medium + DP 4.02 65.5 8.45 42.7 67.9 2.23\nGPT-2-Large + DP 3.87 66.7 8.63 44.0 67.8 2.33\nGPT-2-XL + DP 3.79 66.1 8.53 43.0 68.1 2.28\nGPT-2-Medium 3.19 70.4 8.85 46.8 71.8 2.53\nGPT-2-Large 3.06 70.4 8.89 46.8 72.0 2.47\nGPT-2-XL 3.01 69.4 8.78 46.2 71.5 2.49\nTable 7: Metrics on the DART dataset. Non-DP results from Hu et al. (2021), except for GPT-2-XL, which\nwas not reported in the paper. We ran GPT-2-XL with hyperparameters presented in Hu et al. (2021). Bold\nindicates the best accuracy with DP. DP parameters are (ε= 6.8,δ = 1e-5). Val perp stands for validation\nperplexity. Unlike all other metrics, the lower the TER metric is the better for the performance of the model.\nMethod Val perp BLEU MET TER\nGPT-2-Small + DP 3.82 38.5 0.34 0.53\nGPT-2-Medium + DP 3.30 42.0 0.36 0.51\nGPT-2-Large + DP 3.10 43.1 0.36 0.5\nGPT-2-XL + DP 3.00 43.8 0.37 0.5\nGPT-2-Medium 2.67 47.1 0.39 0.46\nGPT-2-Large 2.89 47.5 0.39 0.45\nGPT-2-XL 2.83 48.1 0.39 0.46\nto ﬁne-tune all parameters to achieve the best accuracy. Nevertheless, it is interesting to wonder if full\nﬁne-tuning with DPSGD can match the accuracy of parameter-eﬃcient methods. A positive answer would\nimply that private and non-private ﬁne-tuning conceptually mirror each other.\nUpdate: A concurrent work by Li et al (Li et al., 2022) answer our question and show that full ﬁne-\ntuning via DPSGD also achieves same performance as the parameter eﬃcient methods used in this paper.\nThese two works raise an intriguing theoretical question: Why does magnitude of the noise added by DPSGD\nhas no impact on the accuracy vs privacy tradeoﬀs when ﬁne-tuning? Explaining this phenomenon would\ndeepen our understanding of private learning.\n5 Related Work\nMore on DP learning: Some work studies private language models on more traditional architectures\nsuch as LSTMs (Hochreiter and Schmidhuber, 1997), either training with DPSGD (McMahan et al., 2018;\nCarlini et al., 2019) or related heuristics (Ramaswamy et al., 2020). Though pre-training on public data\nis suggested (McMahan et al., 2018), public data appears to only be used in one of these works for honest\nhyperparameter selection (Ramaswamy et al., 2020). A few more recent works consider training LLMs\nwith DP. Anil et al. (2021) privately train BERT-Large from scratch, compared to our work which focuses\non private ﬁne-tuning. (Hoory et al., 2021; Basu et al., 2021) perform private full ﬁne-tuning of BERT\nmodels. Hoory et al. (2021) achieve accuracy which is comparable to the non-private model, but additionally\nsupplement the public pre-training data with additional domain-relevant material, while we use oﬀ-the-shelf\npre-trained models. Basu et al. (2021) observe signiﬁcant drops in utility, compared to our parameter-eﬃcient\nmethods which do not. While Kerrigan et al. (2020) consider public pre-training and private ﬁne-tuning,\ntheir experiments are on much smaller architectures (i.e., feedforward networks with three hidden layers). A\nsimultaneous work of Ginart et al. (2022) investigates privateprediction(rather than learning) for next-token\nprediction. A subsequent work by Senge et al. (2021) also investigates the eﬀect of private ﬁne-tuning on\n10\nvarious NLP tasks.\nOur investigation ﬁts more broadly into a line of work employing public data for private data analysis.\nSome works on image classiﬁcation consider pre-training on a large public dataset and ﬁne-tuning on a\nsmaller private dataset (Abadi et al., 2016; Papernot et al., 2019; Tramèr and Boneh, 2021; Luo et al., 2021).\nIn particular, Luo et al. (2021) investigate the role of parameter eﬃciency in private ﬁne-tuning ResNet\nmodels, and propose strategies to choose which parameters to ﬁne-tune. One line of work uses unlabeled\npublic data to train a student model (Papernot et al., 2017, 2018; Bassily et al., 2018), including one work\nsimultaneous to our own for natural language generation Tian et al. (2022). Another recent idea uses a\nsmall amount of public data to identify a lower-dimensional subspace of the gradients in which to perform\nprivate descent (Zhou et al., 2021; Yu et al., 2021a; Kairouz et al., 2021). A simultaneous work of Amid\net al. (2021) uses public data in the mirror map for a private mirror descent algorithm. Finally, other works\n(both theoretical and experimental) investigate the role of public data in private query release, synthetic\ndata generation, and prediction (Ji and Elkan, 2013; Beimel et al., 2016; Alon et al., 2019; Nandi and Bassily,\n2020; Bassily et al., 2020a,b; Liu et al., 2021).\nIn a concurrent work, Li et al. (2022) also investigate DP ﬁne-tuning of LLMs. In several cases, their re-\nsults demonstrate qualitatively similar ﬁndings as ours. While our experiments focus primarily on parameter-\neﬃcientﬁne-tuningmethods, interestingly, theyshowthatprivatefullﬁne-tuningcanalsoachievecomparable\nutility if the experimental setup is conﬁgured properly, e.g., using suitable hyperparameters. In Appendix C,\nwe run experiments under the setup in Li et al. (2022). We show their setup can also improve the performance\nof our methods.\nMore on Fine-tuning: There exist other parameter-eﬃcient tuning methods which we did not evaluate\nin our work. Some of these include random subspace projection (exploiting intrinsic dimensionality (Li\net al., 2018; Aghajanyan et al., 2020)), preﬁx and prompt tuning (Li and Liang, 2021; Lester et al., 2021),\ntuning only biases (Cai et al., 2020; Ben Zaken et al., 2021), and other architecture variants including\nAdapters (Pfeiﬀer et al., 2021; Rücklé et al., 2020). Other works investigate lightweight methods for adapting\nlanguage models to diﬀerent tasks (e.g., Dathathri et al. (2020)) An interesting direction for future work is\nto see whether parameter-eﬃcient tuning approaches speciﬁcally designed for the private setting can achieve\nhigher utility. We also mention zero-shot learning, in which no task-speciﬁc dataset is required and thus\nperfect privacy is achieved. Currently, zero-shot approaches achieve low utility compared to ﬁne-tuning,\nthough it is possible that future models may narrow this gap.\n6 Conclusion\nSo far, DP deep learning has focused on training models from scratch. The spectacular success of transfer\nlearning in real-world applications, however, shows that private ﬁne-tuning is an equally pertinent problem to\nstudy and deserves more attention. We show that by combining recent advances in NLP, parameter-eﬃciency,\nprivacy accounting, and using larger models, one can privately ﬁne-tune models whose utility approaches\nthat of non-private models. We hope our work inspires more study on the core problem of private ﬁne-\ntuning, which we believe to be a central direction for research in private machine learning, leading to more\ninteraction between the LLM and DP communities.\nAcknowledgments\nThe authors would like to thank Rabeeh Karimi Mahabadi for sharing hyperparameters based on experiments\nin Mahabadi et al. (2021) and Xuechen Li for sharing the experimental setup and many suggestions about\nmixed-precision training. Janardhan Kulkarni would like to thank Edward Hu for sharing ideas on ﬁne-\ntuning.\n11\nReferences\nMartin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang.\nDeep learning with diﬀerential privacy. InProceedings of the 2016 ACM Conference on Computer and\nCommunications Security, CCS ’16, pages 308–318, New York, NY, USA, 2016. ACM.\nArmen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic dimensionality explains the eﬀectiveness\nof language model ﬁne-tuning.arXiv preprint arXiv:2012.13255, 2020.\nNoga Alon, Raef Bassily, and Shay Moran. Limits of private learning with access to public data. InAdvances\nin Neural Information Processing Systems 32, NeurIPS ’19, pages 10342–10352. Curran Associates, Inc.,\n2019.\nEhsan Amid, Arun Ganesh, Rajiv Mathews, Swaroop Ramaswamy, Shuang Song, Thomas Steinke, Vinith M\nSuriyakumar, Om Thakkar, and Abhradeep Thakurta. Public data-assisted mirror descent for private\nmodel training. arXiv preprint arXiv:2112.00193, 2021.\nRohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar, and Pasin Manurangsi. Large-scale diﬀerentially\nprivate BERT.arXiv preprint arXiv:2108.01624, 2021.\nRaef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Eﬃcient algo-\nrithms and tight error bounds. InProceedings of the 55th Annual IEEE Symposium on Foundations of\nComputer Science, FOCS ’14, pages 464–473, Washington, DC, USA, 2014. IEEE Computer Society.\nRaef Bassily, Om Thakkar, and Abhradeep Guha Thakurta. Model-agnostic private learning. InAdvances in\nNeural Information Processing Systems 31, NeurIPS ’18, pages 7102–7112. Curran Associates, Inc., 2018.\nRaef Bassily, Albert Cheu, Shay Moran, Aleksandar Nikolov, Jonathan Ullman, and Steven Wu. Private\nquery release assisted by public data. InProceedings of the 37th International Conference on Machine\nLearning, ICML ’20, pages 695–703. JMLR, Inc., 2020a.\nRaefBassily, ShayMoran, andAnupamaNandi. Learningfrommixturesofprivateandpublicpopulations. In\nAdvances in Neural Information Processing Systems 33, NeurIPS ’20, pages 2947–2957. Curran Associates,\nInc., 2020b.\nPriyam Basu, Tiasa Singha Roy, Rakshit Naidu, Zumrut Muftuoglu, Sahib Singh, and Fatemehsadat\nMireshghallah. Benchmarking diﬀerential privacy and federated learning for BERT models.arXiv preprint\narXiv:2106.13973, 2021.\nAmos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approximate\ndiﬀerential privacy.Theory of Computing, 12(1):1–61, 2016.\nElad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitﬁt: Simple parameter-eﬃcient ﬁne-tuning for\ntransformer-based masked language-models.arXiv preprint arXiv:, 2021.\nGavin Brown, Mark Bun, Vitaly Feldman, Adam Smith, and Kunal Talwar. When is memorization of\nirrelevant training data necessary for high-accuracy learning? InProceedings of the 53nd Annual ACM\nSymposium on the Theory of Computing, STOC ’21, pages 123–132, New York, NY, USA, 2021. ACM.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeﬀrey Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language\nmodels are few-shot learners. In Advances in Neural Information Processing Systems 33, NeurIPS ’20.\nCurran Associates, Inc., 2020.\nMarkBun, JonathanUllman, andSalilVadhan. Fingerprintingcodesandthepriceofapproximatediﬀerential\nprivacy. In Proceedings of the 46th Annual ACM Symposium on the Theory of Computing, STOC ’14,\npages 1–10, New York, NY, USA, 2014. ACM.\n12\nHan Cai, Chuang Gan, Ligeng Zhu, and Song Han. TinyTL: Reduce memory, not parameters for eﬃcient\non-device learning. InAdvances in Neural Information Processing Systems 33, NeurIPS ’20, pages 11285–\n11297. Curran Associates, Inc., 2020.\nNicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Evaluating\nand testing unintended memorization in neural networks. In28th USENIX Security Symposium, USENIX\nSecurity ’19, pages 267–284. USENIX Association, 2019.\nNicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam\nRoberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raﬀel. Extracting training\ndata from large language models. In 30th USENIX Security Symposium, USENIX Security ’21, pages\n2633–2650. USENIX Association, 2021.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski,\nand Rosanne Liu. Plug and play language models: A simple approach to controlled text generation. In\nProceedings of the 8th International Conference on Learning Representations, ICLR ’20, 2020.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidi-\nrectional transformers for language understanding. InProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol-\nume 1 (Long and Short Papers), NAACL-HLT ’19, pages 4171–4186, 2019.\nCynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data, our-\nselves: Privacy via distributed noise generation. In Proceedings of the 24th Annual International Con-\nference on the Theory and Applications of Cryptographic Techniques, EUROCRYPT ’06, pages 486–503,\nBerlin, Heidelberg, 2006a. Springer.\nCynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private\ndata analysis. InProceedings of the 3rd Conference on Theory of Cryptography, TCC ’06, pages 265–284,\nBerlin, Heidelberg, 2006b. Springer.\nVitaly Feldman. Does learning require memorization? a short tale about a long tail. InProceedings of the\n52nd Annual ACM Symposium on the Theory of Computing, STOC ’20, pages 954–959, New York, NY,\nUSA, 2020. ACM.\nAntonio Ginart, Laurens van der Maaten, James Zou, and Chuan Guo. Submix: Practical private prediction\nfor large-scale language models.arXiv preprint arXiv:2201.00971, 2022.\nSivakanth Gopi, Yin Tat Lee, and Lukas Wutschitz. Numerical composition of diﬀerential privacy.arXiv\npreprint arXiv:2106.02848, 2021.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus).arXiv preprint arXiv:1606.08415,\n2016.\nSepp Hochreiter and Jürgen Schmidhuber. Long short-term memory.Neural Computation, 9(8):1735–1780,\n1997.\nShlomo Hoory, Amir Feder, Avichai Tendler, Alon Cohen, Soﬁa Erell, Itay Laish, Hootan Nakhost, Uri\nStemmer, Ayelet Benjamini, Avinatan Hassidim, and Yossi Matias. Learning and evaluating a diﬀerentially\nprivate pre-trained language model. InProceedings of the Third Workshop on Privacy in Natural Language\nProcessing, PrivateNLP ’21, pages 21–29, 2021.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. Parameter-eﬃcient transfer learning for nlp. InInternational\nConference on Machine Learning, pages 2790–2799. PMLR, 2019.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models.arXiv preprint arXiv:2106.09685, 2021.\n13\nZhanglong Ji and Charles Elkan. Diﬀerential privacy based on importance weighting.Machine Learning, 93\n(1):163–183, 2013.\nPeter Kairouz, Mónica Ribero, Keith Rush, and Abhradeep Thakurta. (nearly) dimension independent\nprivate ERM with adagrad rates via publicly estimated subspaces. InProceedings of the 34th Annual\nConference on Learning Theory, COLT ’21, pages 2717–2746, 2021.\nGavin Kerrigan, Dylan Slack, and Jens Tuyls. Diﬀerentially private language models beneﬁt from public\npre-training. arXiv preprint arXiv:2009.05886, 2020.\nAntti Koskela, Joonas Jälkö, and Antti Honkela. Computing tight diﬀerential privacy guarantees using ﬀt.\nIn International Conference on Artiﬁcial Intelligence and Statistics, pages 2560–2569. PMLR, 2020.\nAntti Koskela, Joonas Jälkö, Lukas Prediger, and Antti Honkela. Tight diﬀerential privacy for discrete-\nvalued mechanisms and for the subsampled gaussian mechanism using ﬀt. InInternational Conference on\nArtiﬁcial Intelligence and Statistics, pages 3358–3366. PMLR, 2021.\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-eﬃcient prompt tuning.\nIn Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP),\nEMNLP ’21. Association for Computational Linguistics, 2021.\nChunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the intrinsic dimension of\nobjective landscapes. In Proceedings of the 6th International Conference on Learning Representations,\nICLR ’18, 2018.\nXiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation. InProceedings\nof the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International\nJoint Conference on Natural Language Processing (Volume 1: Long Papers), ACL-IJCNLP ’21, pages\n4582–4597, 2021.\nXuechen Li, Florian Tramèr, Percy Liang, and Tatsunori Hashimoto. Large language models can be strong\ndiﬀerentially private learners. InProceedings of the 10th International Conference on Learning Represen-\ntations, ICLR ’22, 2022.\nTerrance Liu, Giuseppe Vietri, Thomas Steinke, Jonathan Ullman, and Steven Wu. Leveraging public\ndata for practical private query release. InProceedings of the 38th International Conference on Machine\nLearning, ICML ’21, pages 6968–6977. JMLR, Inc., 2021.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized bert pretraining approach.arXiv\npreprint arXiv:1907.11692, 2019.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. InProceedings of the 7th Inter-\nnational Conference on Learning Representations, ICLR ’19, 2019.\nZelun Luo, Daniel J Wu, Ehsan Adeli, and Li Fei-Fei. Scalable diﬀerential privacy with sparse network\nﬁnetuning. In Proceedings of the 2021 IEEE Computer Society Conference on Computer Vision and\nPattern Recognition, CVPR ’21, pages 5059–5068, Washington, DC, USA, 2021. IEEE Computer Society.\nRabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Eﬃcient low-rank hyper-\ncomplex adapter layers.arXiv preprint arXiv:2106.04647, 2021.\nH Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning diﬀerentially private recurrent\nlanguage models. InProceedings of the 6th International Conference on Learning Representations, ICLR\n’18, 2018.\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris\nGinsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training.arXiv\npreprint arXiv:1710.03740, 2017.\n14\nLinyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru\nTang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz\nRahman, Ahmad Zaidi, Mutethia Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Vic-\ntoria Lin, Caiming Xiong, Richard Socher, and Nazneen Fatema Rajani. DART: open-domain structured\ndata record to text generation. InProceedings of the 2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, NAACL-HLT ’21, pages\n432–447. Association for Computational Linguistics, 2021.\nAnupama Nandi and Raef Bassily. Privately answering classiﬁcation queries in the agnostic PAC model. In\nProceedings of the 31st International Conference on Algorithmic Learning Theory, ALT ’20, pages 687–703.\nJMLR, Inc., 2020.\nJekaterina Novikova, Ondřej Dušek, and Verena Rieser. The E2E dataset: New challenges for end-to-end\ngeneration. InProceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, SIGDIAL ’17,\npages 201–206. Association for Computational Linguistics, 2017.\nMyle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. In\nProceedings of the Third Conference on Machine Translation (WMT), 2018.\nNicolas Papernot, Martín Abadi, Ulfar Erlingsson, Ian Goodfellow, and Kunal Talwar. Semi-supervised\nknowledge transfer for deep learning from private training data. InProceedings of the 5th International\nConference on Learning Representations, ICLR ’17, 2017.\nNicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and Úlfar Erlingsson.\nScalable private learning with PATE. In Proceedings of the 6th International Conference on Learning\nRepresentations, ICLR ’18, 2018.\nNicolas Papernot, Steve Chien, Shuang Song, Abhradeep Thakurta, and Ulfar Erlingsson. Making the shoe\nﬁt: Architectures, initializations, and tuning for learning with privacy.https://openreview.net/forum?\nid=rJg851rYwH, 2019.\nJonas Pfeiﬀer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion:\nNon-destructive task composition for transfer learning. In Proceedings of the 16th Conference of the\nEuropean Chapter of the Association for Computational Linguistics: Main Volume, EACL ’21, pages\n487–503. Association for Computational Linguistics, 2021.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding\nby generative pre-training, 2018.\nAlec Radford, Jeﬀrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners, 2019.\nSwaroop Ramaswamy, Om Thakkar, Rajiv Mathews, Galen Andrew, H Brendan McMahan, and Françoise\nBeaufays. Training production language models without memorizing user data. arXiv preprint\narXiv:2009.10031, 2020.\nAndreas Rücklé, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiﬀer, Nils Reimers, and Iryna\nGurevych. Adapterdrop: On the eﬃciency of adapters in transformers.arXiv preprint arXiv:2010.11918,\n2020.\nManuel Senge, Timour Igamberdiev, and Ivan Habernal. One size does not ﬁt all: Investigating strategies\nfor diﬀerentially-private learning across nlp tasks.arXiv preprint arXiv:2112.08159, 2021.\nReza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against\nmachine learning models. InProceedings of the 38th IEEE Symposium on Security and Privacy, SP ’17,\npages 3–18, Washington, DC, USA, 2017. IEEE Computer Society.\nShuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with diﬀerentially\nprivate updates. InProceedings of the 2013 IEEE Global Conference on Signal and Information Processing,\nGlobalSIP ’13, pages 245–248, Washington, DC, USA, 2013. IEEE Computer Society.\n15\nPranav Subramani, Nicholas Vadivelu, and Gautam Kamath. Enabling fast diﬀerentially private sgd via\njust-in-time compilation and vectorization. In Advances in Neural Information Processing Systems 34,\nNeurIPS ’21. Curran Associates, Inc., 2021.\nZhiliang Tian, Yingxiu Zhao, Ziyue Huang, Yu-Xiang Wang, Nevin Zhang, and He He. SeqPATE: Diﬀer-\nentially private text generation via knowledge distillation, 2022. URLhttps://openreview.net/forum?\nid=5sP_PUUS78v.\nFlorian Tramèr and Dan Boneh. Diﬀerentially private learning needs better features (or much more data).\nIn Proceedings of the 9th International Conference on Learning Representations, ICLR ’21, 2021.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. InAdvances in Neural Information Processing Systems\n30, NIPS ’17, pages 5998–6008. Curran Associates, Inc., 2017.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-\ntask benchmark and analysis platform for natural language understanding. InInternational Conference\non Learning Representations, 2018.\nAdina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence\nunderstanding through inference. InProceedings of the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers),\nNAACL-HLT ’18, pages 1112–1122. Association for Computational Linguistics, 2018.\nDa Yu, Huishuai Zhang, Wei Chen, and Tie-Yan Liu. Do not let privacy overbill utility: Gradient embed-\nding perturbation for private learning. InProceedings of the 9th International Conference on Learning\nRepresentations, ICLR ’21, 2021a.\nDa Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. Large scale private learning via low-rank\nreparametrization. In Proceedings of the 38th International Conference on Machine Learning, ICML ’21.\nJMLR, Inc., 2021b.\nYingxue Zhou, Zhiwei Steven Wu, and Arindam Banerjee. Bypassing the ambient dimension: Private SGD\nwith gradient subspace identiﬁcation. In Proceedings of the 9th International Conference on Learning\nRepresentations, ICLR ’21, 2021.\n16\nTable 8: Test accuracy for ﬁne-tuning RoBERTa-Large with diﬀerent privacy parameters. The number of\ntraining samples is denoted byn. The values ofσ are noise multipliers. Numbers in the brackets are the\nchanges compared to the results in Table 5 (ε= 6.7, δ= Θ(1/n)).\nTaks σ δ= 1/n δ= 1/10n δ= 1/100n δ= 1/1000n Accuracy (in %)\nMNLI 1.88 ε= 1 ε= 1.35 ε= 1.49 ε= 1.61 86.8 (-1.0%)\nQQP 1.88 ε= 1 ε= 1.40 ε= 1.54 ε= 1.67 85.2 (-2.2%)\nQNLI 3.01 ε= 1 ε= 1.48 ε= 1.64 ε= 1.79 88.0 (-2.8%)\nSST-2 3.63 ε= 1 ε= 1.47 ε= 1.64 ε= 1.80 93.1 (-2.2%)\nMNLI 0.91 ε= 3 ε= 4.12 ε= 4.51 ε= 4.89 87.4 (-0.4%)\nQQP 0.93 ε= 3 ε= 4.10 ε= 4.49 ε= 4.86 86.8 (-0.6%)\nQNLI 1.29 ε= 3 ε= 4.45 ε= 4.90 ε= 5.33 89.9 (-0.9%)\nSST-2 1.52 ε= 3 ε= 4.37 ε= 4.83 ε= 5.25 94.1 (-1.2%)\nA Experiments with Diﬀerent Privacy Parameters\nNow we test our framework under diﬀerent privacy constraints. Speciﬁcally, we run LoRA on the language\nunderstanding tasks with various choices of privacy parametersε and δ. We consider both RoBERTa-Base\nand RoBERTa-Large.\nFor the RoBERTa-Large model, we setε = 1 and 3 with δ being the same as those in Section 4. We\nuse the PRV accountant (Gopi et al., 2021). After getting the noise multipliers, we also reduce the value\nof δ and report the corresponding value ofε. The hyperparameters are the same as those in Section 4. We\nrun experiments on all four tasks, i.e., MNLI (n ∼392k), QQP (n ∼364k), QNLI (n ∼104k), and SST-2\n(n ∼67k). We report the results in Table 8. The performance of our framework is decent even with very\ntight privacy budgets. For instance, withε< 2 and δ= 1/1000n, the accuracy gap between the non-private\nbaseline is only 3.8 for MNLI and 2.1 for SST-2.\n85\n90\n95Accuracy\nSST-2\n0.1 0.5 1 3 5 8 12\n50\n51\n52\n53Accuracy\n Non-private\nDP LoRA\n75\n80\n85\n90Accuracy\nMNLI\n0.1 0.5 1 3 5 8 12\n53.0\n53.5\n54.0\n54.5\n55.0Accuracy\n Non-private\nDP LoRA\nFigure 2: Test accuracy (in %) of ﬁne-tuning the RoBERTa-Base model on MNLI and SST-2 with various\nchoices ofε.\nFortheRoBERTa-Basemodel, wetryvariouschoicesof ε. Thevaluesof εarechosenfrom [0.1,0.5,1,3,5,8,12].\nAll other settings are the same as those in Section 4. We run experiments on the MNLI and SST-2 datasets.\nThe results are presented in Figure 2. Our framework performs well for a wide range ofε. We note that our\nalgorithm achieves meaningful accuracy even for very tight privacy parametersε= 0.5 and 1. Such values\nof ε are rarely explored when training deep models with diﬀerential privacy.\n17\n200 500 1000 2000 4000\nBatchsize\n0.1\n1.0\n3.0\n5.0\n10.0 Clipping Threshold\n90.8 91.3 92.1 92.4 92.7\n91.2 91.4 91.7 92.2 92.5\n90.9 91.4 91.9 92.0 92.5\n90.7 91.5 91.9 92.1 92.6\n90.7 91.6 92.2 92.6 92.8\nLearning Rate 0.0005\n200 500 1000 2000 4000\nBatchsize\n0.1\n1.0\n3.0\n5.0\n10.0 Clipping Threshold\n91.0 92.0 92.1 92.3 92.7\n91.5 91.9 92.2 92.7 92.9\n91.1 92.0 92.2 92.7 92.8\n91.3 92.1 92.2 92.9 92.8\n91.2 92.1 92.8 92.9 92.9\nLearning Rate 0.0010\nFigure 3: Test accuracy (in %) of ﬁne-tuning RoBERTa-Base with diﬀerentially private LoRA on the SST-2\ndataset. Our algorithm performs well on a wide range of hyperparameters.\nTable 9: Accuracy for ﬁne-tuning downstream tasks with RoBERTa-Base (in %). Experiments are run with\nfull-precision. We also scale up the batch size according to the dataset size compared to SST-2. The privacy\nparameters areε= 6.7, andδ=1e-5 for SST-2 and QNLI and 1e-6 for MNLI and QQP.\nMethod MNLI SST-2 QQP QNLI Average Accuracy\nFull w/o DP 87.6 94.8 91.9 92.8 91.8\nDP 83.2 85.9 86.2 84.8 85.0\nAdapter DP 84.6 92.9 87.4 89.2 88.5\nLoRA DP 84.5 92.7 87.1 88.3 88.2\nB On the Inﬂuence of Hyperparameters\nHere we demonstrate that our algorithms perform well for a wide range of hyperparameters. We study two\nhyperparameters that are directly related to the variance of noise: clipping threshold and batchsize. The clip-\npingthresholdischosenfrom [0.1,1.0,3.0,5.0,10.0] andthebatchsizeischosenfrom [200,500,1000,2000,4000].\nWe note that we keep the number of updates the same as that in Section 4 when the batchsize is changed.\nWe ﬁne-tune the RoBERTa-Base model with diﬀerentially private LoRA (r = 16 ) on the SST-2 dataset.\nThe results are presented in Figure 3. DP LoRA performs well for all the hyperparameters considered. The\ngap between the best accuracy and the worst accuracy is only 2%.\nC Fine-TuningforLanguageUnderstandingTaskswithLargeBatch\nSize and Full-Precision\nLi et al. (2022) show the performance of ﬁne-tuning the full model is sensitive to the choice of hyperparam-\neters. They give a conﬁguration which can signiﬁcantly improve the performance of full ﬁne-tuning. In this\nsection, we re-evaluate the tasks in Table 4 and 5 under the conﬁguration in Li et al. (2022).\nThe conﬁguration in Li et al. (2022) has two diﬀerences compared to that in Section 4. The ﬁrst diﬀerence\nis Li et al. (2022) run experiments with full-precision while the experiments in Section 4 use half-precision.\nUsing half-precision is a common approach to speed up NLP experiments (Ott et al., 2018). However, half-\nprecision may incur underﬂow issue which impacts the model performance (Micikevicius et al., 2017). The\nsecond diﬀerence is they use larger batch size for larger datasets. For example, the batch size for MNLI is\n18\nTable 10: Accuracy for ﬁne-tuning downstream tasks with RoBERTa-Large (in %). Experiments are run\nwith full-precision. We also scale up the batch size according to the dataset size compared to SST-2. The\nprivacy parameters areε= 6.7, andδ=1e-5 for SST-2 and QNLI andδ=1e-6 for MNLI and QQP.\nMethod MNLI SST-2 QQP QNLI Average Accuracy\nFull w/o DP 90.2 96.4 92.2 94.7 93.4\nDP 86.4 90.9 87.5 89.4 88.6\nAdapter DP 88.6 94.5 87.8 91.6 90.6\nLoRA DP 89.0 95.3 88.4 92.4 91.3\nroughly six times larger than the batch size for SST-2 in Li et al. (2022). In Section 4, we use the same\nbatch size for all datasets.\nWe follow the above setup and re-evaluate DP-LoRA and DP-Adapter. The results are in Table 9\nand 10. The results of full ﬁne-tuning with diﬀerential privacy are directly adopted from Li et al. (2022).\nThe conﬁguration in Li et al. (2022) further improves the strong results in Table 4 and 5. For example,\nwe achieve 89.0% accuracy on the MNLI dataset, which is only 1.2% lower than the accuracy without DP\nconstraint. Moreover, the beneﬁt of the proposed framework over full ﬁne-tuning is still clear. The average\naccuracy of the proposed algorithms is∼3% higher than that of full ﬁne-tuning.\n19",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6762462258338928
    },
    {
      "name": "Private speech",
      "score": 0.5376355051994324
    },
    {
      "name": "Base (topology)",
      "score": 0.5346464514732361
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5256144404411316
    },
    {
      "name": "Private information retrieval",
      "score": 0.5138819217681885
    },
    {
      "name": "Language model",
      "score": 0.4652014672756195
    },
    {
      "name": "Machine learning",
      "score": 0.4549611806869507
    },
    {
      "name": "Scale (ratio)",
      "score": 0.43831509351730347
    },
    {
      "name": "Natural language processing",
      "score": 0.3469357490539551
    },
    {
      "name": "Data mining",
      "score": 0.3311423659324646
    },
    {
      "name": "Computer security",
      "score": 0.21243637800216675
    },
    {
      "name": "Mathematics",
      "score": 0.13145729899406433
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Developmental psychology",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 45
}