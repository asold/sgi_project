{
  "title": "Keeping Users Engaged During Repeated Administration of the Same Questionnaire: Using Large Language Models to Reliably Diversify Questions",
  "url": "https://openalex.org/W4388964694",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5072291705",
      "name": "Hye Sun Yun",
      "affiliations": [
        "Northeastern University",
        "Tufts University",
        "University of Chicago",
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A5005108319",
      "name": "Mehdi Arjmand",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5042681582",
      "name": "Phillip Sherlock",
      "affiliations": [
        "Northeastern University",
        "Tufts University",
        "University of Chicago",
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A5050634387",
      "name": "Michael K. Paasche‚ÄêOrlow",
      "affiliations": [
        "Northeastern University",
        "Tufts University",
        "University of Chicago",
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A5045919324",
      "name": "James W. Griffith",
      "affiliations": [
        "Northeastern University",
        "Tufts University",
        "University of Chicago",
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A5005469838",
      "name": "Timothy Bickmore",
      "affiliations": [
        "Northeastern University",
        "Tufts University",
        "University of Chicago",
        "University of Florida"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1983478531",
    "https://openalex.org/W3198742703",
    "https://openalex.org/W2159734000",
    "https://openalex.org/W4386981879",
    "https://openalex.org/W2015188398",
    "https://openalex.org/W2986302981",
    "https://openalex.org/W4389519553",
    "https://openalex.org/W4317437684",
    "https://openalex.org/W2032191478",
    "https://openalex.org/W2099386544",
    "https://openalex.org/W2021828976",
    "https://openalex.org/W2492628744",
    "https://openalex.org/W2507695613",
    "https://openalex.org/W4389520375",
    "https://openalex.org/W4385734232",
    "https://openalex.org/W4380985699",
    "https://openalex.org/W2152350748",
    "https://openalex.org/W2297068586",
    "https://openalex.org/W2954605283",
    "https://openalex.org/W2159306398",
    "https://openalex.org/W4387913084",
    "https://openalex.org/W1607675442",
    "https://openalex.org/W3179128890",
    "https://openalex.org/W2152623314",
    "https://openalex.org/W2109636054",
    "https://openalex.org/W4382983091",
    "https://openalex.org/W2895618390",
    "https://openalex.org/W2247042622",
    "https://openalex.org/W4321177655",
    "https://openalex.org/W2154204770",
    "https://openalex.org/W4411442361",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2138706729",
    "https://openalex.org/W2162072747",
    "https://openalex.org/W3165708431",
    "https://openalex.org/W2985229833",
    "https://openalex.org/W2001396122",
    "https://openalex.org/W4384520662",
    "https://openalex.org/W4382618460",
    "https://openalex.org/W2166073427",
    "https://openalex.org/W1493524810",
    "https://openalex.org/W4389282280",
    "https://openalex.org/W4392719612",
    "https://openalex.org/W4390094918",
    "https://openalex.org/W4311086870",
    "https://openalex.org/W4390094937",
    "https://openalex.org/W2889547446",
    "https://openalex.org/W2520682944",
    "https://openalex.org/W2886572631",
    "https://openalex.org/W4210622823",
    "https://openalex.org/W1988568632",
    "https://openalex.org/W2947235575",
    "https://openalex.org/W4313596849",
    "https://openalex.org/W1632976221",
    "https://openalex.org/W2107915429",
    "https://openalex.org/W3093732267",
    "https://openalex.org/W2156813589",
    "https://openalex.org/W2048533792"
  ],
  "abstract": "Standardized, validated questionnaires are vital tools in research and healthcare, offering dependable self-report data. Prior work has revealed that virtual agent-administered questionnaires are almost equivalent to self-administered ones in an electronic form. Despite being an engaging method, repeated use of virtual agent-administered questionnaires in longitudinal or pre-post studies can induce respondent fatigue, impacting data quality via response biases and decreased response rates. We propose using large language models (LLMs) to generate diverse questionnaire versions while retaining good psychometric properties. In a longitudinal study, participants interacted with our agent system and responded daily for two weeks to one of the following questionnaires: a standardized depression questionnaire, question variants generated by LLMs, or question variants accompanied by LLM-generated small talk. The responses were compared to a validated depression questionnaire. Psychometric testing revealed consistent covariation between the external criterion and focal measure administered across the three conditions, demonstrating the reliability and validity of the LLM-generated variants. Participants found that the variants were significantly less repetitive than repeated administrations of the same standardized questionnaire. Our findings highlight the potential of LLM-generated variants to invigorate agent-administered questionnaires and foster engagement and interest, without compromising their validity.",
  "full_text": "Keeping Users Engaged During Repeated Interviews by a Virtual\nAgent: Using Large Language Models to Reliably Diversify\nQuestions\nHye Sun Yun\nyun.hy@northeastern.edu\nNortheastern University\nBoston, MA, USA\nMehdi Arjmand\narjmand.me@northeastern.edu\nNortheastern University\nBoston, MA, USA\nPhillip Sherlock\nphillip.sherlock@ufl.edu\nUniversity of Florida\nGainesville, FL, USA\nMichael K. Paasche-Orlow\nmpo@tufts.edu\nTufts University\nBoston, MA, USA\nJames W. Griffith\njamesgriffith@uchicago.edu\nUniversity of Chicago\nChicago, IL, USA\nTimothy Bickmore\nt.bickmore@northeastern.edu\nNortheastern University\nBoston, MA, USA\nStep 2: Manually \nreview the outputs \nfrom LLMs\nStep 1: Prompt \nLLMs to generate \nvariations of \nquestions\nStep 3: Add selected \noutputs to the \ndatabase for agent \nsystem\nStep 4: Create the agent \nsystem that administers \nquestionnaire using \nvariants\nStep 5: Conduct 15-day \nlongitudinal study with \nparticipants\nStep 6: Analyze the \ncollected data for \npsychometric properties and \nengagement\nGenerate Diverse \nQuestions with LLMs\nManually Review \nOutputs\nAdd Question \nV ariants to Database\nDevelop Agent \nSystem\nConduct Longitudinal \nStudy\nAnalyze Collected \nData\nFigure 1: A workflow diagram of the longitudinal validation study which evaluated the validity, reliability, and user engagement\nof utilizing large language model-generated variants of a standardized depression questionnaire.\nABSTRACT\nStandardized, validated questionnaires are vital tools in research\nand healthcare, offering dependable self-report data. Prior work\nhas revealed that virtual agent-administered questionnaires are\nalmost equivalent to self-administered ones in an electronic form.\nDespite being an engaging method, repeated use of virtual agent-\nadministered questionnaires in longitudinal or pre-post studies can\ninduce respondent fatigue, impacting data quality via response bi-\nases and decreased response rates. We propose using large language\nmodels (LLMs) to generate diverse questionnaire versions while\nretaining good psychometric properties. In a longitudinal study,\nparticipants interacted with our agent system and responded daily\nfor two weeks to one of the following questionnaires: a standardized\ndepression questionnaire, question variants generated by LLMs, or\nquestion variants accompanied by LLM-generated small talk. The\nresponses were compared to a validated depression questionnaire.\nPsychometric testing revealed consistent covariation between the\nexternal criterion and focal measure administered across the three\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nIVA ‚Äô24, September 16‚Äì19, 2024, GLASGOW, United Kingdom\n¬© 2024 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0625-7/24/09.\nhttps://doi.org/10.1145/3652988.3673929\nconditions, demonstrating the reliability and validity of the LLM-\ngenerated variants. Participants found that the variants were sig-\nnificantly less repetitive than repeated administrations of the same\nstandardized questionnaire. Our findings highlight the potential of\nLLM-generated variants to invigorate agent-administered question-\nnaires and foster engagement and interest, without compromising\ntheir validity.\nCCS CONCEPTS\n‚Ä¢ Human-centered computing ‚ÜíEmpirical studies in HCI ;\n‚Ä¢ Computing methodologies ‚ÜíNatural language generation ;\nIntelligent agents.\nKEYWORDS\nquestionnaires, engagement, large language models, virtual agents,\nhealth, longitudinal research\nACM Reference Format:\nHye Sun Yun, Mehdi Arjmand, Phillip Sherlock, Michael K. Paasche-Orlow,\nJames W. Griffith, and Timothy Bickmore. 2024. Keeping Users Engaged\nDuring Repeated Interviews by a Virtual Agent: Using Large Language\nModels to Reliably Diversify Questions. In ACM International Conference\non Intelligent Virtual Agents (IVA ‚Äô24), September 16‚Äì19, 2024, GLASGOW,\nUnited Kingdom. ACM, New York, NY, USA, 10 pages. https://doi.org/10.\n1145/3652988.3673929\narXiv:2311.12707v2  [cs.HC]  6 Jul 2024\nIVA ‚Äô24, September 16‚Äì19, 2024, GLASGOW, United Kingdom Yun, et al.\n1 INTRODUCTION\nSelf-report questionnaires are a type of self-report method that\nincludes a set of questions in a highly structured, standardized\nwritten form. Validated questionnaires are widely used in research\nand healthcare as an assessment strategy as they offer dependable\nself-report data. Prior work has revealed that human and virtual\nagent-administered questionnaires are nearly equivalent to self-\nadministered questionnaires in the electronic form [7, 31]. These\nstudies have shown the feasibility and reliability of using virtual\nagents (VAs) to administer questionnaires simulating interviews\nfor a single session. However, many repeated-measures evaluation\nstudies and longitudinal interventions require the same self-report\nquestionnaire to be administered to the same individual multiple\ntimes. In healthcare, patient-reported outcomes (PROs) are used to\nobtain self-reports of a patient‚Äôs condition at home, typically involv-\ning the repeated administration of surveys to capture symptoms or\nquality of life [23].\nHowever, response rates to repeated surveys tend to decline\nover time, as respondents become fatigued by repeatedly filling\nout the same questionnaires [20, 44, 50]. Even in healthcare, where\nPROs can be used as the basis for treatment decisions, longitudinal\nsurvey completion rates can be as low as 48% [20, 30, 44]. Dwindling\nresponse rates can lead to a nonresponse measurement bias [25]\nand limit the ability to evaluate important changes over time.\nSeveral strategies have been proposed to increase repeated mea-\nsure response rates, including incentives [62], more frequent con-\ntact and engagement with respondents [15, 51], and providing sur-\nvey responses back to the individuals being surveyed [65]. In ad-\ndition, a variety of approaches have been studied to increase the\nusage rates for repeated interactions with VAs, including the use of\nsyntactic and visual variability in the interface [8] and humor [47].\nOther strategies for automated systems include reminders [29] and\nsocial support and reinforcement [60].\nIn this work, we explore two strategies to increase response rates\nto a PRO administered daily for two weeks by a virtual agent (VA)\nthat simulates a face-to-face interview with a healthcare profes-\nsional. The first strategy involved using survey questions that vary\nin every administration so that the survey administrations sound\ndifferent. We went beyond straightforward syntactic variation of\nquestions to variants generated by large language models (LLMs)\nto capture the latent construct we are interested in. Syntactic varia-\ntions primarily entail the reordering of words within a sentence,\nwhereas LLM-generated variations we employed had slightly dif-\nferent words or phrases that convey comparable meanings. Second,\nwe explore the use of small talk, humor, and empathy generated by\nLLMs to make daily interactions with the VA more conversational,\nentertaining, and engaging.\nMany questionnaires, including most PROs, have been validated\nusing laborious methods involving testing with dozens, if not thou-\nsands, of respondents to establish reliability and validity [24]. An\nimportant question raised when validated questionnaires are modi-\nfied is whether the new derivative versions retain the reliability and\nvalidity of the original form. We report the results of a longitudinal\nstudy involving a validated PRO for depression, in which partici-\npants engaged with our virtual agent system daily for two weeks.\nParticipants were randomized to either a repeated, standardized\ndepression questionnaire or one of the two interventions with LLM-\ngenerated questionnaire variants. All participants completed an\nadditional standardized, validated depression questionnaire which\nwas a criterion for comparison. Our hypotheses are:\n‚Ä¢H1: VA administration of LLM-generated questionnaire vari-\nants will retain similar validity and reliability to the VA\nadministration of the original questionnaire.\n‚Ä¢H2: Questionnaires delivered in a different form using LLM-\ngenerated variants daily will be more engaging for partic-\nipants, based on the number of questionnaires completed\nand feedback from participants.\n‚Ä¢H3: Questionnaires delivered with LLM-generated conver-\nsational small talk, humor, and empathy will be more en-\ngaging compared to those delivered as strictly question-and-\nresponse interviews by a VA.\nOur primary contributions include the introduction of an inno-\nvative approach using LLMs to reliably generate diverse versions of\nvalidated questionnaires for a VA system. Furthermore, we demon-\nstrate the feasibility of employing these questionnaire variants to\nenhance user engagement and mitigate repetitiveness.\n2 RELATED WORK\nOur work draws on previous research on alternative delivery meth-\nods for questionnaires, user engagement methods for longitudinal\nresearch, and applications of LLMs to agents and surveys. Tra-\nditionally, paper or mail surveys have been used to administer\nquestionnaires. However, web-based or online surveys have been\nmore frequently employed, as response rates to paper surveys have\ndeclined over time [19, 22, 39, 54, 58]. However, web-based surveys\nare not immune to dwindling response rates [42, 64]. Particularly\nfor long or repetitive administrations of surveys in research, re-\nspondents can experience fatigue, which can lower the completion\nrates and data quality of the responses [10, 38, 50, 59]. Increasing\nthe quality of self-report data and completion rates through surveys\nremains an important challenge for researchers to overcome.\n2.1 Computers & Agents for Quality Self-Report\nSeveral past studies have shown that using computers to admin-\nister surveys and interviews can lead to greater self-disclosure,\nespecially for sensitive information in the context of healthcare,\nas the pressure to respond in socially desirable ways is reduced\n[21, 34, 40, 46, 61, 68]. Several studies have expanded this approach\nby incorporating VAs, as research has indicated that using con-\nversational interviews for surveys can effectively reduce errors.\n[55]. In health-screening interviews with VAs, Lucas et al . [40]\ndiscovered that individuals who perceived the VA as automated\nshowed reduced fear of self-disclosure and expressed sadness more\nintensely than those who perceived the VA as human-operated. A\nsimilar study by Schuetzler et al . [56] demonstrated that people\ndisclose more about their sensitive behavior to a conversational\nagent than to a human; however, people disclose less when the\nconversational agent appears to understand. In addition, Kocielnik\n[36] used a chatbot called HarborBot to test a conversational ap-\nproach for social needs screening in emergency departments and\ncompared it with a traditional survey tool. The results revealed that\nthe conversational approach was perceived to be more engaging,\nKeeping Users Engaged During Repeated Interviews by a Virtual Agent IVA ‚Äô24, September 16‚Äì19, 2024, GLASGOW, United Kingdom\ncaring, understandable, and accessible among low health literacy\nusers than the traditional approach.\nPrior work has demonstrated that medical questionnaires or\nPROs administered by VAs are valid and statistically equivalent\nto human or self-administered questionnaires [4, 7]. For example,\nJaiswal et al. [31] conducted two sets of studies using mental health\nquestionnaires: one comparing VA administration to standard self-\nadministration, and the second comparing VA administration to\nan actual human. The results showed that the questionnaires ad-\nministered by the VAs were statistically equivalent to human or\nself-administered questionnaires. Additionally, Mancone et al. [41]\nshowed that voice assistants, such as Alexa, can be used to ad-\nminister psychological assessment questionnaires as a powerful\nway to capture attention and engage users emotionally without\ncompromising validity.\nWith LLMs becoming increasingly capable and prevalent, re-\nsearchers have begun investigating how LLMs can be employed for\nself-reporting and survey research. Jansen et al. [32] highlighted\nhow LLMs can help overcome some of the challenges in survey\nresearch by generating responses to survey items or question-\nwording. Despite this promising direction, the authors warned\nabout the risks of harmful and inaccurate outputs when using\nLLMs. Similarly, Kjell et al. [35] provided a narrative review of how\nLLMs can potentially be used for psychological assessments using\nnatural language instead of rating scales. Furthermore, one study\nemployed GPT-3 to power a chatbot to collect self-reported data,\nsuch as food intake, exercise, sleep, and work productivity [ 66].\nThe authors found that LLMs also provided the ability to maintain\ncontext, state tracking, and provide off-topic suggestions.\n2.2 Maintaining Longitudinal Self-Report with\nAgents\nLongitudinal studies that require multiple self-reports often have\nlow completion rates [1]. Although VAs increase engagement when\nadministering questionnaires, they still suffer from user disengage-\nment. The length of the first interaction with a VA has been shown\nto be the primary predictor of the number of healthcare question-\nnaires completed by a participant [63]. The findings showed that\nlonger first interactions can result in fewer completed question-\nnaires. Prior work on maintaining engagement in long-term health\ninterventions with VAs by Bickmore et al. [8] shows that increased\nvariability in agent behavior and giving the agent a human back-\nstory can also lead to increased engagement.\n2.3 LLMs for Agents\nRecently, several studies have explored the use of LLMs in agent\ndesign and implementation. Antunes et al. [2] prompted LLMs to\nassist in creating scenarios for socially intelligent agents often used\nin education and entertainment. They created a pipeline to generate\nan agent‚Äôs beliefs, desires, intentions, plans of action, and emotions.\nFurthermore, other studies have examined how LLMs can generate\ndialogue utterances for embodied conversational agents such as\nsocial robots [28, 57] to mitigate boredom and increase engagement.\nOlafsson et al. [48] explored how LLMs can be used as part of VAs\nfor health applications by incorporating GPT-2 in a hybrid dialog\nsystem for a virtual alcohol misuse counselor. GPT-2 generated\nFigure 2: A screenshot of the agent waiting for the user to\nrespond after asking a depression questionnaire question.\nThe dialogue response options are displayed at the top right\ncorner of the screen.\nresponses were combined with a rule-based approach to transition\nthrough structured counseling sessions. Similarly, our study takes\na rule-based approach but incorporates diverse messages generated\nby LLMs. Due to the sensitive nature of mental health question-\nnaires, we decided on a human-in-the-loop approach due to LLMs‚Äô\npotential harms in the healthcare context [9, 26, 70]\n2.4 LLMs for Generating Diverse Text\nIn addition to utilizing LLMs for agents, LLMs have been used to gen-\nerate diverse texts or paraphrases [69]. Cegin et al. [12] conducted a\nstudy comparing the quality of crowd-sourced and LLM-generated\nparaphrases for their diversity and robustness in intent classifica-\ntion. The authors found that ChatGPT is a viable alternative to\nhuman paraphrasing. Furthermore, one study showed that while\nGPT-4 might not necessarily outperform humans in generating\ndiverse motivational messages, it took only 6 seconds to generate\none message compared to an average of 73 seconds for humans [17].\nAlthough LLMs may not always provide the most diverse generated\ntext, they are significantly faster and more grammatically correct\nthan humans. To mitigate the challenges of LLMs, Pehlivanoƒülu\net al. [49] demonstrated how prompt engineering can enhance lexi-\ncal diversity, phrasal variations, fluency, relevance, and syntactical\ndifferences while preserving the original meaning.\n3 SYSTEM DESIGN\nTo evaluate our study hypotheses, we created a VA system deployed\nover the web for participants to interact daily (Figure 2). Our agent\nis a 3D animated character that converses with users using synthetic\nspeech, conversational behavior, and multiple-choice menu inputs\nfor user responses. The agent‚Äôs synchronized nonverbal conversa-\ntional behavior, such as hand gestures, head nods, eyebrow raises,\nand posture shifts, was automatically generated using the Behavior\nExpression Animation Toolkit [11]. Agent utterances were gener-\nated using template-based text generation. The agent‚Äôs dialogue is\ndriven by a hierarchical task network-based dialogue engine. The\nIVA ‚Äô24, September 16‚Äì19, 2024, GLASGOW, United Kingdom Yun, et al.\nOriginal item/\nquestion: ‚ÄúIn the \npast 7 days, I felt \nworthless.‚Äù\nRemove duplicate\nRemove long \noutput\nRemove mentions of \nsuicid\nRemove questions \nunrelated to \ndepression\nPrompting for a variant: ‚ÄúStatement: 'In the past 7 \ndays, I felt worthless.' Write a question for the above \nstatement. Also change the time from past 7 days to \nsince yesterday . The answer of question should be \nbetween: Never , Rarely , Sometimes, Often, Always.‚Ä®\nQuestion:‚Äù\nGenerate Diverse \nQuestions with \nLLMs\nStandardized, \nV alidated Question\nLibrar y of Div erse \nQuestions\n2‚Ä®\nEx ecut e API Calls\n1‚Ä®\nPr epar e Pr ompt s\n3‚Ä®\nFilt er LLM Output s \nwit h Exper t s\nFigure 3: A workflow diagram of how LLMs were used to generate diverse questions. A simple example is provided.\nVA system was implemented using the Unity3D game engine and\nCereProc speech synthesizer.\nOur agent, Marie, interacts with participants daily by verbally\nadministering an eight-question questionnaire in dialogue. For our\nprototype, we focused on one self-report PRO questionnaire us-\ning the eight-item PROMIS¬Æ short form depression questionnaire\n(version 8a) [13]. This questionnaire was developed to assess a re-\nspondent‚Äôs level of emotional distress caused by depressed mood\nwhere each statement is rated on a five-point scale from 1 being\n‚ÄúNever‚Äù to 5 being ‚ÄúAlways. ‚Äù\n3.1 Generating Question Variants Using LLMs\nBy using LLMs, we can yield a greater range of variations for each\nquestion faster and with fewer human resources, potentially in-\ncreasing user engagement [17]. However, it is important to note\nthat such variations may include harmful language or language that\ndeviates from the main concept of the original questionnaire items.\nParticularly in the domain of mental health, unconstrained outputs\nfrom LLMs may not be suitable for measuring various aspects of\nmental health issues, as LLMs are known to provide dangerous\nadvice or misinformation [9, 45, 48, 52].\nTo address the potential issue of harmful outputs (hallucinations\nor misinformation) from LLMs, we used ChatGPT (March 2023 ver-\nsion) and GPT-3 to generate different variants of each question and\nmanually filtered them before using them in our VA system (Fig-\nure 3). For prompting, we provided the original item and response\nscale and asked the LLM to paraphrase the main item into a new\nquestion. These models were prompted to generate variants that fit\nthe response scale. These variations often contained words, phrases,\nor concepts closely related to the latent construct we wanted to\nmeasure. A total of 178 unique variants of the eight questions were\ngenerated. A psychologist then ranked and filtered the variants\nto create a final list that matched the meaning and purpose of the\noriginal question. We also removed potentially dangerous questions\nthat might involve suicide or thoughts about killing oneself. We\nobtained 67 variants of the questions, with a few samples available\nin Table 1. All implementation details of generating the item vari-\nants and conversational small talk including jokes and empathetic\nresponses and the full list of LLM-generated content used for the\nstudy can be found in supplementary materials1.\n1https://github.com/hyesunyun/va-item-variants\n4 LONGITUDINAL EVALUATION STUDY\nFrom April to May 2023, we conducted a 15-day longitudinal online\nstudy to evaluate the psychometric properties of our questionnaire\nvariants, user engagement, and user perception of the VA ques-\ntionnaire system. For the first 14 days, participants were asked to\ntalk with the VA once per day, which lasted a few minutes, and\nanswer a short online survey after each interaction. On the 15 th\nday, participants completed a final online survey. Figure 1 provides\nthe entire study workflow.\nThe experiment followed a randomized between-subject design\nwith three study conditions. In one condition ‚Äî CONTROL, we had\nthe VA administer the standardized eight-item PROMIS¬Æ depres-\nsion questionnaire in question format for a more conversational\nexperience. In the two intervention conditions ‚Äî ITEM VARIANTS\nONLY and ITEM VARIANTS PLUS, the agent randomly chooses\nquestion variants described in subsection 3.1. In addition, ITEM\nVARIANTS PLUS includes additional social and conversational con-\ntent for dialogue, such as anecdotes, jokes, empathetic responses,\ninspiring or hopeful messages, and farewells generated by LLMs\n(Table 2). In a typical session for the ITEM VARIANTS PLUS con-\ndition, the agent first shared a short personal anecdote and then\na randomly selected joke before administering the questionnaire.\nThe agent provides empathetic responses based on user responses\nto the questions and ends with a randomly chosen motivational\nmessage and farewell statements.\n4.1 Measures & Data Collection\nWe collected the following four items with a 7-point scale response\nafter each interaction with the agent: ‚ÄúHow satisfied are you with\nthe agent?‚Äù (1=‚Äúnot at all‚Äù and 7=‚Äúvery satisfied‚Äù), ‚ÄúHow much\nwould you like to continue talking with the agent?‚Äù (1=‚Äúnot at all‚Äù\nand 7=‚Äúvery much‚Äù), ‚ÄúHow natural was your conversation with\nthe agent?‚Äù (1=‚Äúnot at all‚Äù and 7=‚Äúvery natural‚Äù), and ‚ÄúDid the\nagent feel repetitive?‚Äù (1=‚Äúnot at all‚Äù and 7=‚Äúvery repetitive‚Äù). User\nengagement is assessed as the number of completed interactions\nwith the agent.\nAfter the two-week study period, we administered the final sur-\nvey on the 15th day, which included the eight-item Patient Health\nQuestionnaire depression scale (PHQ-8) [ 37, 53], the system us-\nability scale (SUS) [5], overall system satisfaction measures, agent\nsatisfaction measures, and measures related to user perception of\nKeeping Users Engaged During Repeated Interviews by a Virtual Agent IVA ‚Äô24, September 16‚Äì19, 2024, GLASGOW, United Kingdom\nTable 1: Depression Questionnaire with Sample Item Variants Generated by LLMs. This table provides the wording variations\nof the eight-item PROMIS ¬Æ short form depression questionnaire [ 13]. All items are rated on a five-point scale from 1=‚ÄúNever‚Äù\nto 5=‚ÄúAlways‚Äù. # of Variantsrefers to the number of variants we used for the longitudinal evaluation study.\nID Original Control Sample Variants # of Variants\n1 In the past 7 days, I\nfelt worthless.\nIn the past 7 days, how of-\nten have you felt worth-\nless?\nSince we last spoke, have you ever felt like you were a burden to others?; Have\nyou felt like you were not good enough recently?; Since the last time we talked,\nhave you felt like you‚Äôre not important to anyone?\n8\n2 In the past 7 days, I\nfelt helpless.\nIn the past 7 days, how\noften have you felt help-\nless?\nHow often have you felt like you were unable to control a situation in the past\nday?; How often do you feel like you‚Äôre stuck in a cycle of negativity when\nfaced with challenges?; Have you felt powerless or helpless when dealing with\na problem in the past day? How often?\n7\n3 In the past 7 days, I\nfelt depressed.\nIn the past 7 days, how\noften have you felt de-\npressed?\nHave you been feeling like you can‚Äôt escape negative thoughts or feelings?;\nHow often have you been feeling empty or numb?; Have you been experiencing\nchanges in your sleep patterns?\n8\n4 In the past 7 days, I\nfelt hopeless.\nIn the past 7 days, how\noften have you felt hope-\nless?\nHow frequently have you felt like you‚Äôre drowning in negativity since the last\ntime we talked?; Have you ever felt like everything is pointless, even if things\nare going well? If so, how often?; Have you felt like you‚Äôre stuck in a rut or in a\nsituation that‚Äôs beyond your control? If so, how often?\n7\n5 In the past 7 days, I\nfelt like a failure.\nIn the past 7 days, how\noften have you felt like a\nfailure?\nHow often do you feel like you‚Äôre not making the most of your talents and\nabilities?; How often do you feel like you‚Äôre not contributing enough to society?;\nHow often do you feel like you‚Äôve fallen short of your own expectations?\n8\n6 In the past 7 days, I\nfelt unhappy.\nIn the past 7 days, how\noften have you felt un-\nhappy?\nHow often do you experience feelings of unhappiness?; Do you tend to dwell\non negative thoughts and feelings?; Have you noticed yourself feeling unhappy\nmore frequently than usual?\n7\n7\nIn the past 7 days, I\nfelt that I had noth-\ning to look forward\nto.\nIn the past 7 days, how\noften have you felt that\nyou had nothing to look\nforward to?\nDo you frequently feel like your life lacks purpose or direction?; How often do\nyou feel like there‚Äôs nothing to look forward to in the coming days or weeks?;\nHave you been struggling to find joy in your daily activities?\n11\n8\nIn the past 7 days,\nI felt that nothing\ncould cheer me up.\nIn the past 7 days, how\noften have you felt that\nnothing could cheer you\nup?\nDo you rarely feel happy or uplifted when you‚Äôre feeling low?; Do you ever\nfeel like you just can‚Äôt shake off a negative mood?; Have you found it hard to\nsee the positive side of things lately?\n11\nquestions asked by the agent (Table 3). We also asked four open-\nended questions about their experiences.\n4.2 Participants\nParticipants were recruited via an online research platform (www.\nprolific.com). They were required to be 18 years old or older, able\nto read and write English, located in the USA, have working audio\nfor their computer, and have a browser that supports WebGL 2.0.\nParticipants were told to interact daily with the system at least\nseven times during the two weeks and complete a final survey for\ncompensation. Each interaction consisted of a conversation with\nthe VA and a short survey. The minimum interaction requirement\nwas to ensure each participant was provided with the questionnaire\nseveral times. Due to the sensitive nature of asking about depression\nsymptoms, participants were told that the system is for assessment\nonly and were provided with a list of mental health resources in the\nUSA. The study was approved by our institutional review board.\n5 RESULTS\nA total of 105 participants began the longitudinal evaluation study,\nwith 35 participants assigned to each study condition. In total, 93\nparticipants met the compensation requirements and completed\nthe study successfully. All participants were on average 39 (SD=12,\nMdn=37, Range=21‚àº73) years old. The gender breakdown was 49.5%\nwomen, 46.7% men, 2.9% non-binary, and 1.0% others. Participants\nwere 75.2% white, 7.6% multiracial, 6.7% Black/African-American,\n3.8% Asian/Asian American, 2.9% Hispanic/Latinx, 1.9 % American\nIndian/Alaska Native, and 1.9% other. Participants had at least a\nhigh school degree or equivalent (43.8% with a bachelor‚Äôs degree,\n24.8% with some college, 10.5% with a master‚Äôs degree, 10.5% with\nan associate degree, and 1.9% with a doctoral/professional degree).\nWhen asked if they are currently in therapy or taking medication\nfor depression, 80.0% of participants said ‚Äúno‚Äù, 19.1% said ‚Äúyes‚Äù, and\n1.0% preferred not to answer.\n5.1 Psychometric Properties of LLM-generated\nItem Variants\nWe calculated Cronbach‚Äôs alpha [18] to measure the internal consis-\ntency or reliability of the eight depression questions, or how closely\nrelated the eight questions are as a group. This involved looking\nat the responses of each participant for each administration. Cron-\nbach‚Äôs alpha for the CONTROL condition was ùõº=.76 whereas the\nitem variants were ùõº=.65. Although ùõº for the version with variants\nwas lower than that of the CONTROL, it showed acceptable internal\nconsistency. Supplementary materials provide the percentages of\nresponses for each question.\nIVA ‚Äô24, September 16‚Äì19, 2024, GLASGOW, United Kingdom Yun, et al.\nTable 2: Examples of Conversational Content Generated by LLMs.\nCategory Example Content Count\nPersonal Anecdotes\n‚Ä¢ I love going for hikes in the beautiful outdoors! This morning, I took a hike around a nearby lake. The fresh air and\npeaceful atmosphere made it the perfect way to start the day!\n‚Ä¢ I just finished reading this amazing book I stumbled upon! I couldn‚Äôt put it down. It was a captivating journey that\nkept me on the edge of my seat and I can‚Äôt wait to recommend it to all my friends.\n‚Ä¢ This past weekend I decided to try a new restaurant in town. The atmosphere was cozy and the food was delicious! I‚Äôm\nalready looking forward to my next visit so I can try something else off the menu.\n37\nJokes\n‚Ä¢ Why don‚Äôt scientists trust atoms? Because they make up everything!\n‚Ä¢ Why did the smartphone need glasses? Because it lost all its contacts!\n‚Ä¢ What do you call a bear with no teeth? A gummy bear!\n24\nEmpathetic\nResponses\n‚Ä¢ I understand how overwhelming helplessness can be, and I‚Äôm here to support you.\n‚Ä¢ I‚Äôm sorry to hear that you feel this way. Please remember that you are valuable and that your feelings are valid.\n‚Ä¢ I understand how you‚Äôre feeling. It‚Äôs normal to feel overwhelmed at times and it‚Äôs ok to take a step back and take care\nof yourself.\n35\nInspiring or\nHopeful Messages\n‚Ä¢ You are not alone in your struggles. Reach out to others for support and comfort.\n‚Ä¢ Shiv Khera once said, Your positive action combined with positive thinking results in success.\n‚Ä¢ Today is your day to shine! Believe in yourself and make it happen.\n23\nFarewells or\nEnding\nConversations\n‚Ä¢ Well, I should get going. It was nice talking to you!\n‚Ä¢ It was great catching up with you. I hope we can chat again soon!\n‚Ä¢ I enjoyed our conversation. It was nice talking with you. Have a great day!\n42\nWe also investigated whether the psychometric properties of\nitems (questions) were consistent across the three groups. To in-\nvestigate the validity of the PROMIS ¬Æ depression questionnaire\nacross the three study conditions, we conducted a measurement\nalignment analysis using the sirt package in R, which allows the\nassessment of how the properties of individual items differ across\ngroups [27]. In this model, each item is related to a single latent\nfactor (depression) by a linear relationship described by a slope (i.e.,\nfactor loading) and intercept parameter. This analysis examined the\ndegree to which groups can be ‚Äúaligned‚Äù on the same scale. First, a\nconfirmatory factor analysis (CFA) model allowed item slopes and\nintercepts to vary across groups. An ùëÖ2 was used to express how\nmuch variance in group differences was captured by true mean\ndifferences in the groups, rather than by different item properties\nacross groups [3]. Our results, using the alignment procedure, in-\ndicated that 99% of the between-group variation associated with\nslopes and 98% of the between-group variation associated with in-\ntercepts could be attributed to factor mean and variance differences\nacross the groups. Thus, the properties of the items were very con-\nsistent across groups. In a simulation, Asparouhov and Muthen [3]\nfound that ùëÖ2 values of at least .98 were required to procure reliable\nfactor rankings and that in general, ùëÖ2 values greater than .75 (i.e.,\nup to 25% non-invariance) were needed to produce trustworthy\nalignment results. Therefore, based on having achieved ùëÖ2 values\ngreater than .98 for both the aligned item intercepts and loadings,\nwe demonstrated the consistency of the PROMIS¬Æ questionnaire\nadministered across the three study conditions and concluded that\nonly 2% and 1% of the variance could be attributed to differences in\nthe item slopes and intercepts across the three study conditions.\nTo test the validity of each of the three administrations of the\nPROMIS¬Æ questionnaire, we compared the three administrations\nagainst an external criterion‚Äîthe PHQ-8. Specifically, the PHQ-8\nwas added to the CFA, mentioned above. We found that the corre-\nlations between the PROMIS¬Æ questionnaire and the PHQ-8 were\ngreater than or equal to .80 across all study conditions, demonstrat-\ning convergent validity of the LLM-generated items.\n5.2 Engagement\nWe analyzed differences in the number of completed interactions by\nstudy conditions, including participants who did not complete the\nstudy. Participants in the CONTROL group had an average of 9.9\n(SD=3.8) interactions with the agent while ITEM VARIANT ONLY\nand ITEM VARIANT PLUS groups had an average of 11.3 (SD=3.0)\nand 10.8 (SD=3.1) interactions, respectively, with no significant dif-\nferences between conditions, (F(2, 102)=1.81, p=.17). Looking at the\nnumber of participants who met the minimum interaction require-\nment, we found a trending difference among the three groups (ùëã2(2,\nN =105)=5.1, p=.08). CONTROL condition had 80% of participants\nwho met the requirement while ITEM VARIANTS ONLY condi-\ntion had 97% and ITEM VARIANTS PLUS condition had 89%. We\nfound no significant differences between participants who received\ntreatment for depression and those who did not.\n5.3 Perception of System & Agent\nAt the end of the two-week study period, participants rated the\noverall system as usable with a mean SUS score of 76.3 (SD=15.1).\nThey also reported an above neutral rating (Mdn=4, IQR=2) on a\n7-point scale for overall satisfaction with the system. Participants\nrated their satisfaction with the agent with a median of 3.5, which\nwas significantly higher than a neutral score of 3, Z=1.9, p=.03,\nr=.25. In addition, they reported the agent‚Äôs repetitiveness at 4.5,\nsignificantly greater than a neutral score of 3, Z=6.6, p<.001, r=.71.\nThere were no significant differences among the three conditions\nfor any of these measures (Table 3). From the content analysis of\nopen-ended responses, we found that those in CONTROL were\nsignificantly more likely to mention ‚Äúrepetitiveness‚Äù compared to\nthose in the two variant groups, ùëã2(1, N =93)=5, p=.029.\nKeeping Users Engaged During Repeated Interviews by a Virtual Agent IVA ‚Äô24, September 16‚Äì19, 2024, GLASGOW, United Kingdom\nTable 3: User perceptions of the system, agent, and the questions. System-related items are on 7-point scales (from ‚Äúnot at all‚Äù\nto ‚Äúvery much‚Äù), with all other items on 5-point scales, with medians per group reported.\nCategory Item CONTROL ITEM VARIANTS ONLY ITEM VARIANTS PLUS\nSystem\nMean system usability scale (0-100) 78.6 ¬±12.9 75 .2 ¬±14.8 75 .3 ¬±17.3\nHow satisfied are you with the system? 4.0 4.5 5.0\nHow much would you like to continue using the system? 3.0 4.0 3.0\nWould you recommend the system to your friends and family? 4.0 4.0 3.0\nMean of composite score 3.6 ¬±1.7 4.0 ¬±1.8 3.9 ¬±1.9\nAgent\nHow satisfied are you with the agent? 3.0 4.0 4.0\nHow much would you like to continue talking with the agent? 3.0 4.0 3.0\nHow much do you trust the agent? 3.0 3.0 3.0\nHow much do you like the agent? 3.0 4.0 4.0\nHow knowledgeable was the agent? 3.0 3.0 3.0\nHow natural was your conversation with the agent? 2.0 2.5 2.0\nDid the agent feel repetitive? 5.0 4.0 4.0\nHow would you characterize your relationship with the agent?\n(complete stranger - close friend) 2.5 3.0 2.0\nMean of composite scores 3.0 ¬±0.85 3.2 ¬±0.92 3.1 ¬±1.03\nQuestions\nHow coherent were the questions asked by the agent? 4.0 4.0 4.0\nHow natural were the questions asked by the agent? 4.0 3.0 4.0\nWere the questions asked by the agent easy to understand? 4.0 4.5 5.0\nHow often were the questions asked by the agent related to the\ntopic of mental health? (never - almost constantly) 5.0 5.0 4.0\nMean of composite score 4.2 ¬±0.57 4.1 ¬±0.53 4.1 ¬±0.68\nParticipants reported higher overall satisfaction with the agent‚Äôs\nquestions, based on the median composite scores (Mdn=4.1) being\ngreater than a neutral of 3, Z=8.2, p<.001, r=.86. Across all condi-\ntions, participants reported responses significantly above neutral of\n3 for coherence (Mdn = 4.5,Z = 7,p<.001, r=.86), naturalness (Mdn=4,\nZ=4.1, p<.001, r=.43), how easy the questions were to understand\n(Mdn=4.5, Z=8.2, p<.001, r=.87), and relevance ( Mdn=4.5, Z=8.5,\np<.001, r=.88). No significant differences among study conditions\nwere found (Table 3).\nFor the repeated measures collected after each interaction with\nthe agent, we did not find any significant differences across the\nstudy conditions. Although not significantly different, participants\nin the CONTROL group reported a mean score of 5.1 (SD=1.3) for\nagent repetitiveness over 14 days while ITEM VARIANTS ONLY\nand ITEM VARIANTS PLUS conditions had means of 4.9 (SD=1.4)\nand 4.7 (SD=1.4), respectively.\n5.4 Qualitative Results\nWe conducted a deductive thematic analysis of the open-ended\nresponses (3,003 words), guided by sensitizing concepts that focused\non participant satisfaction and feedback on additional features [14].\nWe used elements of the grounded theory method, including open,\naxial, and selective coding [16].\nComforting vs Uncanny Agents.Some participants expressed\npositive sentiments about talking to the agent and mentioned their\nwillingness to interact daily: ‚ÄúI like how someone was checking in\nwith me daily to make sure I was alright. ‚Äù [P43 - ITEM VARIANTS\nPLUS] and ‚ÄúI liked the character, she felt like a safe person to talk to. ‚Äù\n[P67 - ITEM VARIANTS ONLY]. One participant mentioned that\ntheir least favorite part of the system was that they were not able to\nhave more interactions with the agent,‚ÄúI can‚Äôt really give the answers\nI want or talk with her as long as I want‚Äù [P13 - ITEM VARIANTS\nPLUS]. Another participant mentioned their desire to have deeper\ninteraction with the agent on sharing their feelings, ‚ÄúMaybe an\noption to expand on questions if I‚Äôm feeling down, like a deeper dive\ninto my feelings, but still utilizing the multiple-choice selections‚Äù [P3\n- CONTROL]. Conversely, some found the interaction with the VA\nto be uncanny and unnatural. For instance, P80 [CONTROL] found\nthe interaction with the agent strange, ‚ÄúThe attempt to make the\nrobot AI feel human looking‚Äîit was uncanny valley to the max. ‚Äù\nVarious Reasons for Repetitiveness.Most participants, espe-\ncially in the CONTROL group, mentioned the repetitiveness of the\nsystem and agent. P82 [CONTROL] said,‚ÄúThe repetition, being asked\nthe same questions every single day, was a chore even though it wasn‚Äôt\nvery difficult. It lost its charm after the first few days. ‚Äù P89 [CON-\nTROL] also commented on the repetitiveness of questions, ‚Äúsame\nquestions over and over‚Äù . Some participants, across all conditions,\ntalked about how the user response options were repetitive. P88\n[CONTROL] expressed that their least favorite part of the system\nwas ‚ÄúHow repetitive the responses were‚Äù . Others, even in the inter-\nvention groups, expressed how the agent‚Äôs responses felt repetitive.\nFor instance, P66 [ITEM VARIANTS ONLY] said, ‚Äúthe feedback was\nrepetitive‚Äù.\nHumor and Small Talk Does Not Always Work.Some partic-\nipants mentioned ‚Äúhearing the jokes she had‚Äù [P85] as their favorite\npart of the system, while others said that they would like to skip\n‚Äúthe bad dad jokes‚Äù [P11]. Furthermore, P36 found the anecdotes and\njokes to be forced, saying that they would like ‚ÄúNo forced stories\nIVA ‚Äô24, September 16‚Äì19, 2024, GLASGOW, United Kingdom Yun, et al.\nand jokes in the beginning of the session. ‚Äù P87 did not like the agent\ntelling stories from her daily life saying, ‚ÄúProbably the ‚Äòlet me tell\nyou about myself‚Äô stupidity. It was ridiculously patronizing that I\nwas expected to take that seriously. A toddler would know an AI isn‚Äôt\ngetting sore throats and going to the movies‚Äù . While some partic-\nipants appreciated the humor and small talk in their interaction\nwith the agent, others felt that the VA‚Äôs small talk detracted from\ntheir ability to focus on answering the questionnaire.\n6 DISCUSSION\nWe demonstrated the reliability and validity of LLM-generated\nquestion variants in a two-week validation study. The measurement\nalignment analysis and Cronbach‚Äôs alpha showed the reliability of\nthe questions administered in all three study conditions. In addition,\nthe three different administrations of the PROMIS ¬Æ depression\nquestionnaires demonstrated good validity when compared to an\nexternal criterion of PHQ-8 which is another validated, standardized\nquestionnaire for screening depression. These findings support\nH1 by showing that the LLM-generated questionnaire variants\ndo retain reliability and validity when compared to an external\ncriterion. Furthermore, participants given the item variants found\nthe questions coherent, natural, easy to understand, and relevant\nto the conversational topic based on the above neutral median\nself-reported data.\nIn total, 105 participants started the study and 93 participants met\nthe minimum interaction requirement. The CONTROL group had\nthe lowest percentage of participants (80%) who met the minimum\ninteraction requirement. We saw a trending difference in the num-\nber of participants who met the minimum interaction requirement\nof seven interactions among the three study conditions. In addition,\nparticipants in the CONTROL group found the agent‚Äôs questions\nmore repetitive compared to participants with the LLM-generated\nquestionnaire variants based on our content analysis reported in\nsubsection 5.3. However, we did not find any other significant re-\nsults from the post-study survey results. These findings partially\nsupport H2 as we observed that questionnaires delivered in a dif-\nferent form daily did show trending differences in the number of\nparticipants who met the minimum interaction requirement.\nFurthermore, we did not find any differences in engagement, sat-\nisfaction, or usability between ITEM VARIANTS ONLY and ITEM\nVARIANTS PLUS conditions. Therefore, our findings do not support\nH3, in which questionnaires delivered with humor and small talk\nwill increase engagement compared to those without them in an\ninterview with a VA. Qualitative findings showed that some par-\nticipants found the jokes and stories entertaining and interesting\nwhile others found them to be forced. This demonstrates how sim-\nple jokes and small talk might not always be a reliable mechanism\nto increase engagement or satisfaction for repeated interviews.\n6.1 Limitations & Future Work\nThere are several limitations to our study beyond the small con-\nvenience sample used. We conducted our evaluation study using\nonly one standardized questionnaire, so it is unclear whether our\nresults hold for other questionnaires. Our compensation structure\nwith a strict minimum interaction requirement may have affected\nthe results of engagement and interaction with the agent and could\nbe seen as a limitation of our study design. In addition, the tech-\nnical limitations of the prototype could have affected participant\nsatisfaction.\nFuture work should consider ways of adding more variations in\nthe dialogue structure, VA responses to users, and user question\nresponse options (scale anchor response options) to further reduce\nrepetitiveness. Future work could also study the effects of letting\nparticipants interact with the agent using an unstructured input.\nThis approach could further reduce perceptions of repetitiveness.\nHowever, finding a balance between personalization and standard-\nization would need further examination. In addition, varying ques-\ntions could increase the cognitive effort to process and respond.\nFuture studies to understand the trade-offs between respondent\nfatigue/cognitive load and variations are needed.\nFurther research on incorporating humor and anecdotes gener-\nated by LLMs for longitudinal VA research should be considered.\nOur LLM-generated jokes were similar to how Jentzsch and Ker-\nsting [33] found ChatGPT to only produce limited joke patterns.\nHaving more diverse jokes and stories (backstories of the agent\nor even stories of real people) and adapting to user conversation\nresponses can be interesting future directions.\nA previous study [31] has compared using VA and form-based\nquestionnaires, and for our future studies, we will compare the\neffect of VA-based interactions with item variants with form-based\nquestionnaires. Furthermore, future studies could examine utilizing\nmore novel approaches, such as logical control [6], chain-of-thought\nprompting [67], or augmentation to use external tools [43], to cre-\nate a safeguard for utilizing LLMs more directly to conversational\nagents.\n7 CONCLUSION\nWe demonstrated that LLM-generated item variants for a depres-\nsion questionnaire maintain good psychometric properties when\ndelivered by a virtual agent. The LLM-generated item variants\ndemonstrated validity and reliability and were seen to be coher-\nent, natural, easy to understand, and relevant to the topic at hand.\nAdditionally, participants who received these LLM-generated item\nvariants generally found the agent less repetitive over a two-week\nstudy period compared to the CONTROL group. However, we found\nthat including conversational humor and small talk in questionnaire\nadministration interviews by an agent did not result in higher satis-\nfaction or engagement. Striking a balance between personalization\nand standardization will be crucial for maintaining high-quality\ndata collection and boosting response rates in delivering longitu-\ndinal self-report questionnaires. While using LLMs for producing\nquestionnaire variants necessitates meticulous prompt preparation\nand manual output review, it offers the advantage of efficiently\nscaling and expediting the generation of diverse content. We view\nthis study as a step forward in integrating LLMs into VAs to diver-\nsify and enhance questionnaire administration while maintaining\nvalidity and reliability.\nACKNOWLEDGMENTS\nResearch reported in this publication was supported by the National\nInstitute of Cancer of the National Institutes of Health under award\nnumber R01CA271145.\nKeeping Users Engaged During Repeated Interviews by a Virtual Agent IVA ‚Äô24, September 16‚Äì19, 2024, GLASGOW, United Kingdom\nREFERENCES\n[1] Jacob Anh√∏j, Lene Nielsen, et al. 2004. Quantitative and qualitative usage data of\nan Internet-based asthma monitoring tool. Journal of Medical Internet Research 6,\n3 (2004), e57.\n[2] Ana Antunes, Joana Campos, Manuel Guimar√£es, Jo√£o Dias, and Pedro A. Santos.\n2023. Prompting for Socially Intelligent Agents with ChatGPT. In Proceedings of\nthe 23rd ACM International Conference on Intelligent Virtual Agents (, W√ºrzburg,\nGermany,) (IV A ‚Äô23). Association for Computing Machinery, New York, NY, USA,\nArticle 20, 9 pages. https://doi.org/10.1145/3570945.3607303\n[3] Tihomir Asparouhov and Bengt Muth√©n. 2014. Auxiliary variables in mixture\nmodeling: Three-step approaches using M plus. Structural equation modeling: A\nmultidisciplinary Journal 21, 3 (2014), 329‚Äì341.\n[4] Marc Auriacombe, Sarah Moriceau, Fuschia Serre, Cecile Denis, Jean-Arthur\nMicoulaud-Franchi, Etienne de Sevin, Emilien Bonhomme, Stephanie Bioulac,\nMelina Fatseas, and Pierre Philip. 2018. Development and validation of a virtual\nagent to screen tobacco and alcohol use disorders. Drug and alcohol dependence\n193 (2018), 1‚Äì6.\n[5] Aaron Bangor, Philip T Kortum, and James T Miller. 2008. An empirical evaluation\nof the system usability scale. Intl. Journal of Human‚ÄìComputer Interaction 24, 6\n(2008), 574‚Äì594.\n[6] Erkan Basar, Divyaa Balaji, Linwei He, Iris Hendrickx, Emiel Krahmer, Gert-Jan\nde Bruijn, and Tibor Bosse. 2023. HyLECA: A Framework for Developing Hybrid\nLong-term Engaging Controlled Conversational Agents. In Proceedings of the 5th\nInternational Conference on Conversational User Interfaces . 1‚Äì5.\n[7] Timothy Bickmore, Amy Rubin, and Steven Simon. 2020. Substance use screening\nusing virtual agents: towards automated Screening, Brief Intervention, and Refer-\nral to Treatment (SBIRT). In Proceedings of the 20th ACM International Conference\non Intelligent Virtual Agents . 1‚Äì7.\n[8] Timothy Bickmore, Daniel Schulman, and Langxuan Yin. 2010. Maintaining\nengagement in long-term interventions with relational agents. Applied Artificial\nIntelligence 24, 6 (2010), 648‚Äì666.\n[9] Timothy W Bickmore, Ha Trinh, Stefan Olafsson, Teresa K O‚ÄôLeary, Reza Asadi,\nNathaniel M Rickles, and Ricardo Cruz. 2018. Patient and consumer safety risks\nwhen using conversational assistants for medical information: an observational\nstudy of Siri, Alexa, and Google Assistant. Journal of medical Internet research 20,\n9 (2018), e11510.\n[10] Ann Bowling. 2005. Mode of questionnaire administration can have serious\neffects on data quality. Journal of public health 27, 3 (2005), 281‚Äì291.\n[11] Justine Cassell, Hannes H√∂gni Vilhj√°lmsson, and Timothy Bickmore. 2004.\nBEAT: the Behavior Expression Animation Toolkit. In Life-Like Characters:\nTools, Affective Functions, and Applications , Helmut Prendinger and Mitsuru\nIshizuka (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 163‚Äì185. https:\n//doi.org/10.1007/978-3-662-08373-4_8\n[12] Jan Cegin, Jakub Simko, and Peter Brusilovsky. 2023. ChatGPT to Replace Crowd-\nsourcing of Paraphrases for Intent Classification: Higher Diversity and Compara-\nble Model Robustness. arXiv preprint arXiv:2305.12947 (2023).\n[13] David Cella, William Riley, Arthur Stone, Nan Rothrock, Bryce Reeve, Susan\nYount, Dagmar Amtmann, Rita Bode, Daniel Buysse, Seung Choi, et al . 2010.\nInitial adult health item banks and first wave testing of the patient-reported\noutcomes measurement information system (PROMIS‚Ñ¢) network: 2005‚Äì2008.\nJournal of clinical epidemiology 63, 11 (2010), 1179.\n[14] Victoria Clarke, Virginia Braun, and Nikki Hayfield. 2015. Thematic analysis.\nQualitative psychology: A practical guide to research methods 3 (2015), 222‚Äì248.\n[15] Andrew Cleary and Nigel Balmer. 2015. The impact of between-wave engagement\nstrategies on response to a longitudinal survey. International Journal of Market\nResearch 57, 4 (2015), 533‚Äì554.\n[16] Juliet M Corbin and Anselm Strauss. 1990. Grounded theory research: Procedures,\ncanons, and evaluative criteria. Qualitative sociology 13, 1 (1990), 3‚Äì21.\n[17] Samuel Rhys Cox, Ashraf Abdul, and Wei Tsang Ooi. 2023. Prompting a Large\nLanguage Model to Generate Diverse Motivational Messages: A Comparison with\nHuman-Written Messages. In Proceedings of the 11th International Conference on\nHuman-Agent Interaction. 378‚Äì380.\n[18] Lee J Cronbach. 1951. Coefficient alpha and the internal structure of tests.\npsychometrika 16, 3 (1951), 297‚Äì334.\n[19] W De Heer and E De Leeuw. 2002. Trends in household survey nonresponse: A\nlongitudinal and international comparison. Survey nonresponse 41 (2002), 41‚Äì54.\n[20] Nicola R Dean and Tamara Crittenden. 2016. A five year experience of measuring\nclinical effectiveness in a breast reconstruction service using the BREAST-Q pa-\ntient reported outcomes measure: a cohort study.Journal of Plastic, Reconstructive\n& Aesthetic Surgery 69, 11 (2016), 1469‚Äì1477.\n[21] David DeVault, Ron Artstein, Grace Benn, Teresa Dey, Ed Fast, Alesia Gainer,\nKallirroi Georgila, Jon Gratch, Arno Hartholt, Margaux Lhommet, et al . 2014.\nSimSensei Kiosk: A virtual human interviewer for healthcare decision support.\nIn Proceedings of the 2014 international conference on Autonomous agents and\nmulti-agent systems . 1061‚Äì1068.\n[22] Joel R Evans and Anil Mathur. 2005. The value of online surveys.Internet research\n15, 2 (2005), 195‚Äì219.\n[23] Oluwadamilola M Fayanju, Tinisha L Mayo, Tracy E Spinks, Seohyun Lee, Car-\nlos H Barcenas, Benjamin D Smith, Sharon H Giordano, Rosa F Hwang, Richard A\nEhlers, and Jesse C Selber. 2016. Value-based breast cancer care: a multidisci-\nplinary approach for defining patient-centered outcomes. Annals of surgical\noncology 23 (2016), 2385‚Äì2390.\n[24] R Michael Furr. 2021. Psychometrics: an introduction . SAGE publications.\n[25] Robert M Groves and Emilia Peytcheva. 2008. The impact of nonresponse rates on\nnonresponse bias: a meta-analysis. Public opinion quarterly 72, 2 (2008), 167‚Äì189.\n[26] Joschka Haltaufderheide and Robert Ranisch. 2024. The Ethics of ChatGPT\nin Medicine and Healthcare: A Systematic Review on Large Language Models\n(LLMs). arXiv preprint arXiv:2403.14473 (2024).\n[27] Hyemin Han. 2024. Using measurement alignment in research on adolescence\ninvolving multiple groups: A brief tutorial with R. Journal of Research on Adoles-\ncence 34, 1 (2024), 235‚Äì242.\n[28] Leon Hanschmann, Ulrich Gnewuch, and Alexander Maedche. 2023. Saleshat: A\nLLM-Based Social Robot for Human-Like Sales Conversations. In International\nWorkshop on Chatbot Research and Design . Springer, 61‚Äì76.\n[29] Alberto Hern√°ndez-Reyes, Fernando C√°mara-Martos, Guillermo Molina Recio,\nRafael Molina-Luque, Manuel Romero-Salda√±a, and Rafael Moreno Rojas. 2020.\nPush notifications from a mobile app to improve the body composition of over-\nweight or obese women: randomized controlled trial. JMIR mHealth and uHealth\n8, 2 (2020), e13747.\n[30] Victoria Huynh, Kathryn Colborn, Shelby Smith, Levi N Bonnell, Gretchen\nAhrendt, Nicole Christian, Simon Kim, Dan D Matlock, Clara Lee, and Sarah E\nTevis. 2021. Early trajectories of patient reported outcomes in breast cancer pa-\ntients undergoing lumpectomy versus mastectomy. Annals of Surgical Oncology\n28 (2021), 5677‚Äì5685.\n[31] Shashank Jaiswal, Michel Valstar, Keerthy Kusumam, and Chris Greenhalgh. 2019.\nVirtual human questionnaire for analysis of depression, anxiety and personality.\nIn Proceedings of the 19th ACM international conference on intelligent virtual agents .\n81‚Äì87.\n[32] Bernard J Jansen, Soon-gyo Jung, and Joni Salminen. 2023. Employing large\nlanguage models in survey research. Natural Language Processing Journal 4\n(2023), 100020.\n[33] Sophie Jentzsch and Kristian Kersting. 2023. ChatGPT is fun, but it is not\nfunny! Humor is still challenging Large Language Models. arXiv preprint\narXiv:2306.04563 (2023).\n[34] Patricia Kissinger, Janet Rice, Thomas Farley, Shelly Trim, Kayla Jewitt, Victor\nMargavio, and David H Martin. 1999. Application of computer-assisted interviews\nto sexual behavior research. American journal of epidemiology 149, 10 (1999),\n950‚Äì954.\n[35] Oscar NE Kjell, Katarina Kjell, and H Andrew Schwartz. 2023. Beyond Rating\nScales: With Targeted Evaluation, Language Models are Poised for Psychological\nAssessment. Psychiatry Research (2023), 115667.\n[36] Rafal Dariusz Kocielnik. 2021. Designing engaging conversational interactions for\nhealth & behavior change . Ph. D. Dissertation.\n[37] Kurt Kroenke, Tara W Strine, Robert L Spitzer, Janet BW Williams, Joyce T Berry,\nand Ali H Mokdad. 2009. The PHQ-8 as a measure of current depression in the\ngeneral population. Journal of affective disorders 114, 1-3 (2009), 163‚Äì173.\n[38] Austin Le, Benjamin H Han, and Joseph J Palamar. 2021. When national drug\nsurveys ‚Äútake too long‚Äù: An examination of who is at risk for survey fatigue.\nDrug and alcohol dependence 225 (2021), 108769.\n[39] Michael W Link and Ali H Mokdad. 2005. Alternative modes for health surveil-\nlance surveys: an experiment with web, mail, and telephone.Epidemiology (2005),\n701‚Äì704.\n[40] Gale M Lucas, Jonathan Gratch, Aisha King, and Louis-Philippe Morency. 2014.\nIt‚Äôs only a computer: Virtual humans increase willingness to disclose. Computers\nin Human Behavior 37 (2014), 94‚Äì100.\n[41] Stefania Mancone, Pierluigi Diotaiuti, Giuseppe Valente, Stefano Corrado, Fer-\nnando Bellizzi, Guilherme Torres Vilarino, and Alexandro Andrade. 2023. The\nUse of Voice Assistant for Psychological Assessment Elicits Empathy and En-\ngagement While Maintaining Good Psychometric Properties. Behavioral Sciences\n13, 7 (2023), 550.\n[42] Katja Lozar Manfreda, Michael Bosnjak, Jernej Berzelak, Iris Haas, and Vasja Ve-\nhovar. 2008. Web surveys versus other survey modes: A meta-analysis comparing\nresponse rates. International journal of market research 50, 1 (2008), 79‚Äì104.\n[43] Gr√©goire Mialon, Roberto Dess√¨, Maria Lomeli, Christoforos Nalmpantis, Ram\nPasunuru, Roberta Raileanu, Baptiste Rozi√®re, Timo Schick, Jane Dwivedi-Yu, Asli\nCelikyilmaz, et al. 2023. Augmented language models: a survey. arXiv preprint\narXiv:2302.07842 (2023).\n[44] Yul Ha Min, Jong Won Lee, Yong-Wook Shin, Min-Woo Jo, Guiyun Sohn, Jae-Ho\nLee, Guna Lee, Kyung Hae Jung, Joohon Sung, and Beom Seok Ko. 2014. Daily\ncollection of self-reporting sleep disturbance data via a smartphone app in breast\ncancer patients receiving chemotherapy: a feasibility study. Journal of medical\nInternet research 16, 5 (2014), e135.\n[45] Adam S Miner, Arnold Milstein, Stephen Schueller, Roshini Hegde, Christina\nMangurian, and Eleni Linos. 2016. Smartphone-based conversational agents and\nresponses to questions about mental health, interpersonal violence, and physical\nIVA ‚Äô24, September 16‚Äì19, 2024, GLASGOW, United Kingdom Yun, et al.\nhealth. JAMA internal medicine 176, 5 (2016), 619‚Äì625.\n[46] Jessica Clark Newman, Don C Des Jarlais, Charles F Turner, Jay Gribble, Phillip\nCooley, and Denise Paone. 2002. The differential effects of face-to-face and\ncomputer interview modes. American journal of public health 92, 2 (2002), 294‚Äì\n297.\n[47] Stefan Olafsson, Teresa K. O‚ÄôLeary, and Timothy W. Bickmore. 2020. Motivating\nHealth Behavior Change with Humorous Virtual Agents. In Proceedings of the\n20th ACM International Conference on Intelligent Virtual Agents (Virtual Event,\nScotland, UK) (IVA ‚Äô20). Association for Computing Machinery, New York, NY,\nUSA, Article 42, 8 pages. https://doi.org/10.1145/3383652.3423915\n[48] Stefan Olafsson, Paola Pedrelli, Byron C. Wallace, and Timothy Bickmore.\n2023. Accomodating User Expressivity While Maintaining Safety for a Vir-\ntual Alcohol Misuse Counselor. In Proceedings of the 23rd ACM International\nConference on Intelligent Virtual Agents (W√ºrzburg, Germany) (IVA ‚Äô23) . As-\nsociation for Computing Machinery, New York, NY, USA, Article 3, 9 pages.\nhttps://doi.org/10.1145/3570945.3607361\n[49] Meltem Kurt Pehlivanoƒülu, Muhammad Abdan Syakura, and Nevcihan Duru.\n2023. Enhancing Paraphrasing in Chatbots Through Prompt Engineering: A Com-\nparative Study on ChatGPT, Bing, and Bard. In 2023 8th International Conference\non Computer Science and Engineering (UBMK) . IEEE, 432‚Äì437.\n[50] Stephen R Porter, Michael E Whitcomb, and William H Weitzer. 2004. Multiple\nsurveys of students and survey fatigue. New directions for institutional research\n2004, 121 (2004), 63‚Äì73.\n[51] Yvette Pronk, Peter Pilot, Justus M Brinkman, Ronald J van Heerwaarden, and\nWalter van der Weegen. 2019. Response rate and costs for automated patient-\nreported outcomes collection alone compared to combined automated and manual\ncollection. Journal of patient-reported outcomes 3 (2019), 1‚Äì8.\n[52] Katyanna Quach. 2020. Researchers made an OpenAI GPT-3 medical chatbot as\nan experiment. It told a mock patient to kill themselves. The Register (2020).\n[53] Ilya Razykov, Roy C Ziegelstein, Mary A Whooley, and Brett D Thombs. 2012.\nThe PHQ-9 versus the PHQ-8‚Äîis item 9 useful for assessing suicide risk in\ncoronary artery disease patients? Data from the Heart and Soul Study. Journal of\npsychosomatic research 73, 3 (2012), 163‚Äì168.\n[54] Catherine A Roster, Robert D Rogers, Gerald Albaum, and Darin Klein. 2004. A\ncomparison of response characteristics from web and telephone surveys. Inter-\nnational Journal of Market Research 46, 3 (2004), 359‚Äì373.\n[55] Michael F Schober and Frederick G Conrad. 1997. Does conversational inter-\nviewing reduce survey measurement error? Public opinion quarterly (1997),\n576‚Äì602.\n[56] Ryan M Schuetzler, Justin Scott Giboney, G Mark Grimes, and Jay F Nunamaker Jr.\n2018. The influence of conversational agent embodiment and conversational\nrelevance on socially desirable responding. Decision Support Systems 114 (2018),\n94‚Äì102.\n[57] Javier Sevilla-Salcedo, Enrique Fern√°dez-Rodicio, Laura Mart√≠n-Galv√°n, √Ålvaro\nCastro-Gonz√°lez, Jos√© C Castillo, and Miguel A Salichs. 2023. Using Large Lan-\nguage Models to Shape Social Robots‚Äô Speech. (2023).\n[58] David M Shannon and Carol C Bradshaw. 2002. A comparison of response\nrate, response time, and costs of mail and electronic surveys. The Journal of\nExperimental Education 70, 2 (2002), 179‚Äì192.\n[59] Angela Sinickas. 2007. Finding a cure for survey fatigue.Strategic Communication\nManagement 11, 2 (2007), 11.\n[60] Kirsten P Smith and Nicholas A Christakis. 2008. Social networks and health.\nAnnu. Rev. Sociol 34 (2008), 405‚Äì429.\n[61] Charles F Turner, Leighton Ku, Susan M Rogers, Laura D Lindberg, Joseph H\nPleck, and Freya L Sonenstein. 1998. Adolescent sexual behavior, drug use, and\nviolence: increased reporting with computer survey technology. Science 280,\n5365 (1998), 867‚Äì873.\n[62] Jonathan B VanGeest, Timothy P Johnson, and Verna L Welch. 2007. Methodolo-\ngies for improving response rates in surveys of physicians: a systematic review.\nEvaluation & the health professions 30, 4 (2007), 303‚Äì321.\n[63] Laura Vardoulakis. 2013. Social desirability bias and engagement in systems\ndesigned for long-term health tracking. (2013).\n[64] Vasja Vehovar, Zenel Batagelj, Katja Lozar Manfreda, and Metka Zaletel. 2002.\nNonresponse in web surveys. Survey nonresponse (2002), 229‚Äì242.\n[65] Sudheer Vemuru, Shelby Smith, Kathryn Colborn, Victoria Huynh, Laura Leonard,\nLevi Bonnell, Laura Scherer, Dan Matlock, Clara Lee, and Simon Kim. 2023.\nAccess to Results of Patient Reported Outcome Surveys Does Not Improve Survey\nResponse Rates. Journal of Surgical Research 283 (2023), 945‚Äì952.\n[66] Jing Wei, Sungdong Kim, Hyunhoon Jung, and Young-Ho Kim. 2023. Leveraging\nlarge language models to power chatbots for collecting user self-reported data.\narXiv preprint arXiv:2301.05843 (2023).\n[67] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,\nQuoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning\nin large language models. Advances in Neural Information Processing Systems 35\n(2022), 24824‚Äì24837.\n[68] Suzanne Weisband and Sara Kiesler. 1996. Self disclosure on computer forms:\nMeta-analysis and implications. InProceedings of the SIGCHI conference on human\nfactors in computing systems . 3‚Äì10.\n[69] Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander Ratner, Ranjay Krishna,\nJiaming Shen, and Chao Zhang. 2023. Large language model as attributed training\ndata generator: A tale of diversity and bias.arXiv preprint arXiv:2306.15895 (2023).\n[70] Hye Sun Yun, Iain Marshall, Thomas Trikalinos, and Byron Wallace. 2023. Ap-\npraising the Potential Uses and Harms of LLMs for Medical Systematic Re-\nviews. In Proceedings of the 2023 Conference on Empirical Methods in Natu-\nral Language Processing , Houda Bouamor, Juan Pino, and Kalika Bali (Eds.).\nAssociation for Computational Linguistics, Singapore, 10122‚Äì10139. https:\n//doi.org/10.18653/v1/2023.emnlp-main.626",
  "topic": "Respondent",
  "concepts": [
    {
      "name": "Respondent",
      "score": 0.8555765151977539
    },
    {
      "name": "Psychology",
      "score": 0.5037712454795837
    },
    {
      "name": "Repeated measures design",
      "score": 0.4592857360839844
    },
    {
      "name": "Reliability (semiconductor)",
      "score": 0.45491647720336914
    },
    {
      "name": "Depression (economics)",
      "score": 0.44406676292419434
    },
    {
      "name": "Clinical psychology",
      "score": 0.44368776679039
    },
    {
      "name": "Longitudinal study",
      "score": 0.4428388476371765
    },
    {
      "name": "Applied psychology",
      "score": 0.3646564781665802
    },
    {
      "name": "Medicine",
      "score": 0.3348351716995239
    },
    {
      "name": "Statistics",
      "score": 0.08584576845169067
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Pathology",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Macroeconomics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I12912129",
      "name": "Northeastern University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I121934306",
      "name": "Tufts University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I40347166",
      "name": "University of Chicago",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I33213144",
      "name": "University of Florida",
      "country": "US"
    }
  ],
  "cited_by": 2
}