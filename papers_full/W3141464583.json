{
  "title": "Hindi to English: Transformer-Based Neural Machine Translation",
  "url": "https://openalex.org/W3141464583",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3141349856",
      "name": "Kavit Gangar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3036274776",
      "name": "Hardik Ruparel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3140445982",
      "name": "Shreyas Lele",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3141349856",
      "name": "Kavit Gangar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3036274776",
      "name": "Hardik Ruparel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3140445982",
      "name": "Shreyas Lele",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2892009878",
    "https://openalex.org/W2118434577",
    "https://openalex.org/W2790548959",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2983183316",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W2153508793",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2763176669",
    "https://openalex.org/W2950580142",
    "https://openalex.org/W3005724337",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2663236676"
  ],
  "abstract": null,
  "full_text": "Hindi to English: Transformer-Based Neural\nMachine Translation\nKavit Gangar, Hardik Ruparel, and Shreyas Lele\nVeermata Jijabai Technological Institute, Mumbai, India\nkavitgangar34@gmail.com, hardikruparel14@gmail.com,\nshreyaslele2398@gmail.com\nAbstract. Machine Translation (MT) is one of the most prominent\ntasks in Natural Language Processing (NLP) which involves the auto-\nmatic conversion of texts from one natural language to another while\npreserving its meaning and fluency. Although the research in machine\ntranslation has been going on since multiple decades, the newer approach\nof integrating deep learning techniques in natural language processing\nhas led to significant improvements in the translation quality. In this\npaper, we have developed a Neural Machine Translation (NMT) system\nby training the Transformer model to translate texts from Indian Lan-\nguage Hindi to English. Hindi being a low resource language has made\nit difficult for neural networks to understand the language thereby lead-\ning to a slow growth in the development of neural machine translators.\nThus, to address this gap, we implemented back-translation to augment\nthe training data and for creating the vocabulary, we experimented with\nboth word and subword level tokenization using Byte Pair Encoding\n(BPE) thereby ending up training the Transformer in 10 different con-\nfigurations. This led us to achieve a state-of-the-art BLEU score of 24.53\non the test set of IIT Bombay English-Hindi Corpus in one of the con-\nfigurations.\nKeywords: Neural Machine Translation · Transformer · Byte Pair En-\ncoding · Back-translation\n1 Introduction\nMachine translation is one of the oldest tasks taken up by computer scientists\nand the development in this field has been going on for more than 60 years.\nThe research in this field has made remarkable progress to develop translator\nsystems to convert source language to target language while maintaining the\ncontextuality and fluency. In earlier times, the translation was handled by stat-\nically replacing words with the words from the target language. This dictionary\nlook-up led technique led to inarticulate translation and hence was made obso-\nlete by Rule-Based Machine Translation (RBMT) [1]. RBMT is a system based\non linguistic information about the source and target languages derived from\ndictionaries and grammar including semantics and syntactic regularities of each\narXiv:2309.13222v1  [cs.CL]  23 Sep 2023\n2 K. Gangar et al.\nlanguage [14]. With the absence of flexibility and scalability to incorporate new\nwords and semantics and the requirement of human expertise to define numer-\nous rules, rule-based machine translation systems could only achieve accuracy\non a subset of languages. To overcome the issues of the RBMT system, a new\napproach called Statistical Machine Translation (SMT) was introduced. Instead\nof having rules determine the target sequence, SMT approaches leverage prob-\nability and statistics to determine the output sequence. This approach made it\nfeasible to cover all types of language within the source and target language\nand to add new pairs. Most of these systems are based on Bayesian prediction\nand have phrases and sentences as the basic units of translation. The main issue\nfaced by this approach is the requirement of colossal amounts of data, which is\na huge problem for low resource languages.\nDue to these prevailing issues, there is a demand to explore alternate meth-\nods for creating a smarter and more efficient translation system. The develop-\nment of various deep learning techniques and the promising results shown by\nthe combination of these techniques with NLP created a new approach called\nNMT. NMT’s advantage lies in two facts that are its simplistic architecture and\nits ability to capture long dependencies in the sentence, thereby indicating its\nhuge potential in emerging as a new trend of the mainstream [2]. Conceptually\nspeaking, NMT is a simple Deep Neural Network (DNN) that reads the entire\nsource sentence and produces an output translation one word at a time. The\nreason why NMT systems are appealing is that they require minimal domain\nknowledge which makes it well-suited for any problem that can be formulated\nas mapping an input sequence to an output sequence. Also, the inherent nature\nof the neural networks to generalize any input implies that NMT systems will\ngeneralize to novel word phrases that are not present in the training set.\nMoreover, almost all the languages in the world are continuously evolving\nwith new words getting added, older words getting amended and new slangs\ngetting introduced very frequently. Even though NMT systems generalizes the\ninput data well, they still lack the ability to translate the rare words due to their\nfixed modest-size vocabulary which forces the NMT system to useunk symbol for\nrepresenting out-of-vocabulary (OOV) words [9]. To tackle this issue, a subword\ntokenization technique called Byte Pair Encoding (BPE) was introduced. BPE\ndivides the words such that the frequent sequence of letters is combined thereby\nforming a root word and affix. This approach alone handles the OOV words by\nmerging the root word and the different combinations of affixes thereby creating\nthe rare word [4].\nIn this paper, we present the experimental setup and the state-of-the-art\nresults obtained after training the Transformer model on the IIT Bombay CFILT\nEnglish-Hindi dataset of 1.5 million parallel records. The paper is organized as\nfollows: section 2 describes the motivation behind our work. Section 3, describes\nthe model that we have implemented. In section 4 we present the details of the\nexperimental setup for training the model. In section 5 we display a comparative\nanalysis of the results obtained by training the model in different configurations.\nFinally, section 6 concludes the paper.\nHindi to English: Transformer-Based Neural Machine Translation 3\n2 Motivation\nWith the power of deep learning, Neural Machine Translation has arisen as the\nmost powerful approach to perform the translation task.\nIn [5] a model called Transformer was introduced which uses the encoder-\ndecoder approach where the encoder first converts the original input sequence\ninto its latent representation in the form of hidden state vectors. The decoder\nthen tries to predict the output sequence using this latent representation. RNN’s\nand CNN’s inherently handle sequences word-by-word sequentially which is an\nobstacle to parallelize. The Transformer achieves parallelization by replacing re-\ncurrence with attention and encoding the position of the symbol in the sequence.\nIt reduces the number of sequential operations needed to relate the two symbols\nfrom the input/output phrases to a constant number O(1). This is achieved by\nusing the multi-head attention mechanism which allows to model the dependen-\ncies regardless of their distance in input or output sentence.\nIn [7], they show that they can improve the machine translation quality of\nNMT systems by mixing monolingual target sentences into the training set. They\ninvestigate two different methods to fill the source side of monolingual training\ninstances: using a dummy source sentence and using a source sentence obtained\nvia backtranslation, which they call synthetic data and conclude that the latter\ngives better results.\nIn [8], they investigate NMT models that operate on the level of subword\nunits. Their main goal is to model open-vocabulary translation in the NMT net-\nwork itself, without requiring a back-off model for rare words. In this paper, byte\npair encoding has been adapted. BPE is a compression algorithm that is used for\nword segmentation. It facilitates the representation of an open vocabulary via\na fixed-size vocabulary that contains variable-length character sequences, thus\nit serves as a highly suitable strategy of word segmentation for neural network\nmodels.\nMotivated with the results obtained by the transformer for machine trans-\nlation on various languages, we created a translation system that translates a\nHindi sentence to English. Since we use a low resource Hindi language for which\nthe amount of good quality parallel corpus is limited, we applied back-translation\nto increase the quantity of training data. To overcome the problem caused by\nout of vocabulary words we used BPE.\n3 NMT Model Architecture\n3.1 Structure\nThe Transformer model is the first NMT architecture that completely relies on\nthe self-attention mechanism to calculate the representation of its input and\noutput data without using recurrent neural networks (RNNs) or convolutional\nneural networks (CNNs) [13]. The Transformer model consists of an encoding\nunit and a decoding unit wherein each of these components consists of a stack\nof 6 layers of encoders and decoders respectively. (see Fig. 1).\n4 K. Gangar et al.\nFig. 1.Transformer Structure- Bird’s-eye View\nEach encoder layer consists of two sublayers. The first sublayer is the multi-\nhead self-attention layer and the second sublayer is a position-wise fully con-\nnected feed forward network [5]. Each decoder layer in the decoding component\nconsists of 3 sublayers. The function of the first two sublayers is the same as\nthat in the encoder. However, the third sublayer performs multi-head attention\nmechanism over the output of the encoder stack (see Fig. 2).\n3.2 Working\nBefore passing the input data to the encoder stack, the input data is first pro-\njected into an embedded vector space. To capture the notation, distance between\ndifferent words and the order of the words in the input sequence, a positional em-\nbedding vector is added to the input vector. This intermediate vector is then fed\nto the first layer of the encoding component of the transformer. A multi-head\nself-attention is then computed on this intermediate embedded vector space.\nMulti-headed mechanism improves the performance of the attention-layer in\nHindi to English: Transformer-Based Neural Machine Translation 5\nFig. 2.The Transformer Architecture [5]\ntwo ways. First, it helps expand the model’s ability to focus on the words in\ndifferent positions. Second, it gives the attention layer multiple representation\nsubspaces, concatenates them and then projects linearly onto a space with ini-\ntial dimensions [15]. The output of the self-attention layer is then passed onto\na dense feed forward network which consists of two linear functions with RELU\nin between them. The output of this feed forward network is then passed on to\nanother encoder layer stacked on top of it. All the encoder layers in the encoding\ncomponent have the same functionality. Finally, the output of the encoding unit\nis then passed as an input to the decoding unit.\nThe decoder has similar functionality as that of the encoder. The output of\nthe top encoder is converted into a set of attention vectors which is then used\nby each decoder in the decoding component. This helps the decoder focus on the\nappropriate position in the input sequence. The decoder predicts words one word\nat a time from left to right. Upon prediction of each word, it is again fed into the\nbottom decoder after converting it into an embedded vector space and adding a\npositional embedding vector. The decoder’s self-attention mechanism works in a\n6 K. Gangar et al.\nslightly different way than the encoder. In the decoder, the self-attention layer is\nonly allowed to look at the words in earlier positions. This is done by masking the\nwords at future positions to -inf. Each decoder layer in the decoding component\nperforms the same function. The output of the last decoder layer is then fed\ninto a linear layer and softmax layer. The linear layer outputs a vector having\na size equal to the size of the target language vocabulary. Each position in this\noutput vector determines the score of the unique word. This vector of scores is\nthen converted into probabilities by the softmax layer and the position with the\nhighest probability is chosen, and the word associated with it is produced as the\noutput for the particular time step.\n4 Experimental Setup\n4.1 Dataset\nThe fundamental requirement for assembling a machine translation system is\nthe availability of parallel corpora of the source and the target language. In this\npaper, we trained our transformer model on the Hindi-English parallel corpus\nby the Center for Indian Language Technology (CFILT), IIT Bombay [6]. The\ntraining data consists of approximately 1.5 million texts from multiple disciplines\nwhile the development and the test set contains data from the news domain.\nTable 1 provides the details about the number of sentences and the number of\nunique tokens in English and Hindi that are present in our chosen dataset.\n4.2 Data Preprocessing\nData preprocessing is an essential data mining technique that helps clean the\ndata by removing the noise and outliers which can then directly be used by the\nmodel for training and testing purposes. Our preprocessing pipeline consists of 3\nmain steps viz. Data Cleaning, Removal of duplicates and Vocabulary creation.\nEach step is explained in detail below:\nData Cleaning (Step 1)In this step, we first removed the special characters,\npunctuation and noise characters from both the English and Hindi text corpus.\nAfter the elimination of all the noise characters, we removed the empty lines.\nThe resulting text corpus was then converted into lower case and was then fed\ninto the next step to remove the duplicates.\nTable 1.Metadata of the Dataset\nDataset # of SentencesUnique Hindi TokensUnique English Tokens\nIITB Train 1,267,502 421,050 242,910\nIITB Dev 483 2,479 2,405\nIITB Test 2,478 8,428 9,293\nHindi to English: Transformer-Based Neural Machine Translation 7\nRemoval of Duplicates (Step 2) The cleaned and noise-free text corpus\nobtained as a result of the above step was then used to remove the duplicate\nrecords. This resulted in the creation of our training universe containing approx-\nimately 1.2 million unique parallel text corpus which was used for creating the\nvocabulary.\nVocabulary Creation (Step 3)Vocabulary creation is one of the most fun-\ndamental step in Neural Machine Translation. The coverage of the vocabulary\nis a major factor that drives the overall accuracy and the quality of the trans-\nlation. If the vocabulary under-represents the training data universe, then pre-\ndicted translation will contain many unk tokens thereby reducing the BLEU\nscore drastically. Thus creating a modest-size vocabulary that optimally repre-\nsents the data is a challenging task. For creating the vocabulary for both Hindi\nand English language we implemented two approaches: word level tokenization\nand subword level tokenization. In the word level tokenization, we first extracted\n50,000 most frequently used words from the training set and then added it to the\nvocabulary. While in the subword level tokenization, we used Byte Pair Encoding\n(BPE) for creating 50,000 subword tokens which were added in the vocabulary.\nThe evaluation of the performance of our model on both word and subword level\ntokenization is presented in Section 5.\n4.3 Back-Translation\nHindi, being a low resource language as compared to its counterpart European\nlanguages has made the availability of data quite difficult. Many institutions\naround the world are creating larger and a more complete text corpus for the\nlow resource languages. To tackle the lack of availability of Hindi-English parallel\ncorpus, we implemented back-translation technique. Back-translation technique\nis used for augmenting the training data which leads to increasing the output\naccuracy of the translation. There is a plethora of monolingual English data\navailable on the internet which can be used to generate text corpus of a low\nresource language. To generate the additional Hindi-English parallel text corpus,\nwe first trained an English to Hindi machine translation system on our training\ndata and then translated the 3 million WMT14 English monolingual data to\ngenerate the corresponding predicted Hindi text corpus.\nTable 2.Training Data Universe: Batch-wise summary\nBatch Number # of Back-translated records addedTotal Records\nBatch 1 0.5 million 1.7 million\nBatch 2 1.5 million 2.7 million\nBatch 3 2.5 million 3.7 million\nBatch 4 3 million 4.2 million\n8 K. Gangar et al.\nTo observe the effect of back-translation, we have divided the 3 million back-\ntranslated parallel records in 4 batches. We cumulatively add the back-translated\nrecords to the original training data in each of these batches. The first batch con-\ntains the 0.5 million back-translated records along with the 1.2 million original\ntraining data. In the same way we add an additional 1 million, 1 million and 0.5\nmillion in the second, third and fourth batch respectively. Table 2 summarizes\nthe training data universe for each batch.\n4.4 Training Details\nAfter the data preparation and segregation into batches, we trained our trans-\nformer model using Opennmt-tf toolkit [10]. For training the model, we have\nused the NVIDIA Tesla K80 GPU provided by Google Colab [16]. For our trans-\nformer model, we used the default 6 layers setting in both encoder and decoder\neach of which contains 512 hidden units. We used the default embedding size\nof 512 along with 8 headed attention. We configured the batch size to be equal\nto 64 records and the effective batch size which is defined as the number of\ntraining examples consumed in one step to be equal to 384. We optimized the\nmodel parameters using the LazyAdam optimizer. The model was trained on\n10 different configurations 5 each for word and subword level tokenization till\nconvergence or till 70,000 steps at max (hard stop). The GPU run-time provided\non Google Colab resulted in a training duration of approximately 20-24 hours\nfor each configuration.\n5 Results\nWe evaluate the quality of translation of our model on the test set using the\nBilingual Evaluation Understudy (BLEU) score [12] and the Rank-based Intu-\nitive Bilingual Evaluation (RIBES) score [11]. For depicting the performance\nof subword level tokenization, we have divided the test set into 2 subsets. The\nfirst set (Set-1) consists of sentences whose words are present in the vocabulary\ngenerated from word level tokenization. This set consists of 1694 sentences. The\nsecond set (Set-2) is the complete test set consisting of 2478 sentences.\nIn Table 3, after adding the first batch of 0.5M parallel back-translated\nrecords with the original training data, the BLEU score increased by 3.79 and\nTable 3.Results of Word Level Tokenization (Set-1)\nModel ID Model BLEU RIBES\n1 Transformer 18.76 0.699708\n2 Transformer with Batch 1 22.55 0.730440\n3 Transformer with Batch 2 23.95 0.735804\n4 Transformer with Batch 3 24.79 0.741369\n5 Transformer with Batch 4 24.68 0.740567\nHindi to English: Transformer-Based Neural Machine Translation 9\nTable 4.Results of Subword Level Tokenization (Set-1)\nModel ID Model BLEU RIBES\n6 Transformer 19.10 0.695566\n7 Transformer with Batch 1 23.98 0.733614\n8 Transformer with Batch 2 25.44 0.737078\n9 Transformer with Batch 3 25.87 0.739192\n10 Transformer with Batch 4 25.74 0.742397\nTable 5.Results for the Transformer with Batch4 model on Set-2\nModel IDTokenization BLEU RIBES\n5 Word level 21.22 0.728683\n10 Subword level 24.53 0.735781\nwith the subsequent addition of other 2 batches the BLEU and the RIBES score\nreached a maximum of 24.79 and 0.741 respectively. However, when the 4th\nbatch of 0.5M back-translated records was added with the previous batches, the\nscores decreased by a small margin indicating convergence with respect to the\naddition of back-translated data.\nSimilar to the results obtained with word level tokenization, in Table 4, after\nadding the first batch of back-translated records the BLEU score increases by\n4.78 and with the subsequent addition of other 2 batches the BLEU score reached\na maximum of 25.87. After adding the 4th batch, the BLEU scored decreased\nby 0.13 however the RIBES score increased by 0.003.\nWhen compared with word level tokenization, subword level tokenization\nachieves a better BLEU score which can be attributed to the fact that it has\nthe advantage of not having an out-of-vocabulary case and also to learn better\nembeddings for rare words since rare words can enhance the learning from its\nsubwords that occur in other words. This fact is further strengthened in Table 5\nwhich shows the BLEU and RIBES score for the Transformer with Batch4 model\nusing word and subword level tokenization on Set-2. The decrease in the BLEU\nand RIBES score as compared to Table 3 and Table 4 is due to the fact that the\nSet-2 consists of additional sentences as compared to Set-1 which contain rare\nwords that are not included in the vocabulary for word level tokenization. When\nsubword level tokenization is used, the model performs reasonably well even in\nthe presence of rare words which is not the case for word level tokenization.\n6 Conclusion\nThe transformer model has displayed promising results for neural machine trans-\nlation involving low resource languages as well. We saw that after adding the\nback-translated records the performance was certainly improved, however when\nthe amount of generated data increases beyond a certain level, there is no fur-\n10 K. Gangar et al.\nther improvement in the performance. Using a combination of the transformer\nmodel, back-translation technique and a subword tokenization method like BPE,\nwe achieved a BLEU score of 24.53 which is the state-of-the-art on this dataset\nto the best of our knowledge. In the future, we can try to incorporate state-of-\nthe-art Natural Language Processing models like BERT [3] into NMT to further\nimprove the quality of translation.\nReferences\n1. Saini, S., Sahula, V.: Neural Machine Translation for English to Hindi. In: Fourth\nInternational Conference on Information Retrieval and Knowledge Management\n(CAMP) 2018, pp. 1–6. IEEE (2018)\n2. Yang, S., Wang, Y., Chu, X.: A Survey of Deep Learning Techniques for Neural\nMachine Translation (2020)\n3. Devlin, J., Chang, M., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirec-\ntional transformers for language understanding (2018)\n4. Tacorda, A., Ignacio, M., Oco, N., Roxas, R.: Controlling byte pair encoding for neu-\nral machine translation. In: International Conference on Asian Language Processing\n(IALP) 2017, pp. 168–171. IEEE (2017)\n5. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser,\n L.: Attention is all you need. In: Advances in neural information processing systems,\npp. 5998–6008 (2017)\n6. Kunchukuttan, A., Mehta, P., Bhattacharyya, P.: The iit bombay english-hindi par-\nallel corpus (2017)\n7. Sennrich, R., Haddow, B., Birch, A.: Improving neural machine translation models\nwith monolingual data (2015)\n8. Sennrich, R., Haddow, B., Birch, A.: Neural machine translation of rare words with\nsubword units (2015)\n9. Luong, M., Sutskever, I., Le, Q., Vinyals, O., Zaremba, W.: Addressing the rare\nword problem in neural machine translation (2014)\n10. Klein, G., Kim, Y., Deng, Y., Senellart, J., Rush, A.: OpenNMT: Open-Source\nToolkit for Neural Machine Translation. In: Proceedings of ACL, System Demon-\nstrations 2017, pp. 67–72. Association for Computational Linguistics (2017)\n11. Isozaki, H., Hirao, T., Duh, K., Sudoh, K., Tsukada, H.: Automatic evaluation of\ntranslation quality for distant language pairs. In: Conference on Empirical Methods\nin Natural Language Processing 2010, pp. 944–952 (2010)\n12. Papineni, K., Roukos, S., Ward, T., Zhu, W.: BLEU: a method for automatic\nevaluation of machine translation. In: Proceedings of the 40th annual meeting of\nthe Association for Computational Linguistics 2002, pp. 311–318 (2002)\n13. Goyal, V., Sharma, D.: LTRC-MT Simple & Effective Hindi-English Neural Ma-\nchine Translation Systems at WAT 2019. In: Proceedings of the 6th Workshop on\nAsian Translation 2019, pp. 137–140 (2019)\n14. Rule-based machine translation, https://en.wikipedia.org/wiki/Rule-based_\nmachine_translation\n15. The Illustrated Transformer, http://jalammar.github.io/\nillustrated-transformer\n16. Google Colab, http://colab.research.google.com/",
  "concepts": [
    {
      "name": "Machine translation",
      "score": 0.8753604888916016
    },
    {
      "name": "Computer science",
      "score": 0.8205517530441284
    },
    {
      "name": "Natural language processing",
      "score": 0.7420874834060669
    },
    {
      "name": "Artificial intelligence",
      "score": 0.73907071352005
    },
    {
      "name": "Transformer",
      "score": 0.6806267499923706
    },
    {
      "name": "Machine translation software usability",
      "score": 0.4958260953426361
    },
    {
      "name": "Hindi",
      "score": 0.4804522693157196
    },
    {
      "name": "Fluency",
      "score": 0.46490418910980225
    },
    {
      "name": "Transfer-based machine translation",
      "score": 0.45866623520851135
    },
    {
      "name": "Natural language",
      "score": 0.4451679289340973
    },
    {
      "name": "Lexical analysis",
      "score": 0.4420977830886841
    },
    {
      "name": "Example-based machine translation",
      "score": 0.43740665912628174
    },
    {
      "name": "Speech recognition",
      "score": 0.32740408182144165
    },
    {
      "name": "Linguistics",
      "score": 0.17974993586540222
    },
    {
      "name": "Engineering",
      "score": 0.1253592073917389
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "topic": "Machine translation",
  "institutions": []
}