{
  "title": "CXR-LLaVA: a multimodal large language model for interpreting chest X-ray images",
  "url": "https://openalex.org/W4406421457",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A3110687837",
      "name": "Lee Seo-woo",
      "affiliations": [
        "Seoul National University Hospital"
      ]
    },
    {
      "id": null,
      "name": "Youn, Jiwon",
      "affiliations": [
        "Gwangju Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1900967246",
      "name": "Kim Hyungjin",
      "affiliations": [
        "Seoul National University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A3004429263",
      "name": "Kim Man-su",
      "affiliations": [
        "Gwangju Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2744178260",
      "name": "Yoon Soon Ho",
      "affiliations": [
        "Seoul National University Hospital"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4362603432",
    "https://openalex.org/W4367055910",
    "https://openalex.org/W2533800772",
    "https://openalex.org/W4399912477",
    "https://openalex.org/W6851592950",
    "https://openalex.org/W4405081677",
    "https://openalex.org/W4386567710",
    "https://openalex.org/W3141637546",
    "https://openalex.org/W3209830024",
    "https://openalex.org/W2963466845",
    "https://openalex.org/W2611650229",
    "https://openalex.org/W2912664121",
    "https://openalex.org/W4297518055",
    "https://openalex.org/W3114128166",
    "https://openalex.org/W2995225687",
    "https://openalex.org/W2152772232",
    "https://openalex.org/W4388768480",
    "https://openalex.org/W4392044798",
    "https://openalex.org/W4296027312",
    "https://openalex.org/W4402037259",
    "https://openalex.org/W3101156210",
    "https://openalex.org/W4286882999"
  ],
  "abstract": null,
  "full_text": "Lee et al. European Radiology (2025) 35:4374– 4386\nhttps://doi.org/10.1007/s00330-024-11339-6\nIMAGING INFORMATICS AND ARTIFIC IAL INTELLIGENCE Open Access\nCXR-LLaVA: a multimodal large language\nmodel for interpreting chest X-ray images\nSeowoo Lee1, Jiwon Youn2, Hyungjin Kim1, Mansu Kim2* and Soon Ho Yoon1*\nAbstract\nObjective This study aimed to develop an open-source multimodal large language model (CXR-LLaVA) for\ninterpreting chest X-ray images (CXRs), leveraging recent advances in large language models (LLMs) to potentially\nreplicate the image interpretation skills of human radiologists.\nMaterials and methods For training, we collected 592,580 publicly available CXRs, of which 374,881 had labels for\ncertain radiographic abnormalities (Dataset 1) and 217,699 provided free-text radiology reports (Dataset 2). After pre-\ntraining a vision transformer with Dataset 1, we integrated it with an LLM inﬂuenced by the LLaVA network. Then, the\nmodel was ﬁne-tuned, primarily using Dataset 2. The model’s diagnostic performance for major pathologicalﬁndings\nwas evaluated, along with the acceptability of radiologic reports by human radiologists, to gauge its potential for\nautonomous reporting.\nResults The model demonstrated impressive performance in test sets, achieving an average F1 score of 0.81 for six\nmajor pathological ﬁndings in the MIMIC internal test set and 0.56 for six major pathologicalﬁndings in the external\ntest set. The model’s F1 scores surpassed those of GPT-4-vision and Gemini-Pro-Vision in both test sets. In human\nradiologist evaluations of the external test set, the model achieved a 72.7% success rate in autonomous reporting,\nslightly below the 84.0% rate of ground truth reports.\nConclusion This study highlights the signiﬁcant potential of multimodal LLMs for CXR interpretation, while also\nacknowledging the performance limitations. Despite these challenges, we believe that making our model open-source\nwill catalyze further research, expanding its effectiveness and applicability in various clinical contexts.\nKey Points\nQuestion How can a multimodal large language model be adapted to interpret chest X-rays and generate radiologic\nreports?\nFindings The developed CXR-LLaVA model effectively detects major pathologicalﬁndings in chest X-rays and generates\nradiologic reports with a higher accuracy compared to general-purpose models.\nClinical relevance This study demonstrates the potential of multimodal large language models to support radiologists by\nautonomously generating chest X-ray reports, potentially reducing diagnostic workloads and improving radiologist efﬁciency.\nKeywords Radiography (thoracic), Thorax, Deep learning, Image interpretation, Image interpretation (computer-assisted)\n© The Author(s) 2025.Open AccessThis article is licensed under a Creative Commons Attribution 4.0 International License, which permits use,\nsharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s)\nand the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material\nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visithttp://creativecommons.org/\nlicenses/by/4.0/.\n*Correspondence:\nMansu Kim\nmansu.kim@gist.ac.kr\nSoon Ho Yoon\nyshoka@snu.ac.kr\n1Department of Radiology, Seoul National University College of Medicine,\nSeoul National University Hospital, Seoul, Republic of Korea\n2AI Graduate School, Gwangju Institute of Science and Technology, Gwangju,\nRepublic of Korea\n1234567890():,;1234567890():,;\n1234567890():,;\n1234567890():,;\nIntroduction\nAdvances in deep learning, marked by the emergence of\nconvolutional neural networks (CNNs) and vision trans-\nformers (ViTs), have profoundly impacted radiology\n[1– 3]. Numerous deep learning algorithms have made\ntheir way into practical, commercial applications. How-\never, while CNNs and ViTs are adept at speciﬁc tasks,\nsuch as classiﬁcation and segmentation, this specialization\ncould limit their ability to address multifaceted challenges\nin areas such as radiology. Concurrently, the natural\nlanguage processing domain has witnessed signi ﬁcant\nbreakthroughs, enabling large language models (LLMs),\nsuch as ChatGPT, to understand and generate human-like\ntext with remarkable pro ﬁciency and unprecedented\nperformance levels in linguistic tasks ranging from text\ngeneration to translation [4]. The integration of natural\nlanguage processing and image processing technologies\nhas led to the development of models that have set new\nbenchmarks in the ﬁeld, such as contrastive language-\nimage pre-training (CLIP) [ 5] and the bootstrapping\nlanguage-image pre-training (BLIP-2) model, which was\nintroduced in 2023 and can interpret the context within\nimages and generate detailed captions [6].\nMost LLMs have primarily focused on text processing.\nHowever, there is a growing trend towards a multimodal\napproach involving processing of image, text, and even video\ndata. OpenAI and Google have released general-purpose\nmultimodal models (GPT-4-vision and Gemini-Pro-Vision,\nrespectively). Furthermore, the Large Language and Vision\nAssistant (LLaVA), an open-source project combining vision\nencoding with an LLM, has demonstrated exemplary per-\nformance across a range of visual tasks [7]. However, it\nremains unclear how effective these general-purpose models\nare at interpreting chest X-rays (CXRs). Within the medical\ndomain, there are few speciﬁc multimodal models. Google has\npublished results for ELIXR, a model capable of interpreting\nCXRs, but this model is not publicly available [8]. Similarly,\nthe open-source LLaVA-MED, a model tuned to the medical\ndomain, has been released. However, detailed insights into its\nproﬁciency in interpretingCXRs remain limited [9].\nRadiologists’ workload has signiﬁcantly increased over\nthe past three decades, potentially impacting the accuracy\nof radiologic diagnoses [10]. In response, numerous stu-\ndies have explored the use of deep learning models to\nimprove diagnostic accuracy and reduce the burden on\nradiologists [11]. Building on this line of research, our\nstudy employed the latest technology, a multimodal LLM,\nto attempt radiologic report generation for CXRs. This\nstudy aimed to develop a multimodal LLM designed for\nCXR interpretation, while also exploring its potential for\nautonomous CXR reporting.\nA preliminary version of this work has been made\npublicly available as a preprint on arXiv [12].\nMaterials and methods\nThis retrospective study solely used publicly available\ndatasets and did not require institutional review board\napproval.\nData collection\nFor model training, we included several public CXR\ndatasets, collecting a total of 592,580 frontal CXRs\n(Table 1)[ 13– 20]. The Medical Information Mart for\nIntensive Care (MIMIC) dataset provides radiologic\nreports in a free-text form (Dataset 2,n = 217,699), while\nthe other training datasets have multi-class or binary\nlabeling for radiographic abnormalities (Dataset 1,\nn = 374,881). Some datasets contain information regard-\ning lesions’location, but this information was not utilized.\nAdapting a multimodal LLM to CXRs (CXR-LLaVA)\nA model inﬂuenced by the LLaVA network was developed\n[7]. LLaVA, which consists of an LLM and an image\nencoder, converts images into a sequence of image tokens\nthat are then combined with query text tokens for text\ngeneration within the LLM. Our primary objective was to\nﬁne-tune LLaVA using CXR– radiologic report pairs.\nTo achieve optimal performance, we developed a cus-\ntom image encoder from scratch rather than using pre-\ntrained weights. We empirically employed the“ViT-L/16”\nversion of the vision transformer as the image encoder.\nThis encoder begins with a convolutional layer that pro-\ncesses 1-channel grayscale CXR images into 1024-\ndimensional patches. These patches are passed through\na series of 24 residual attention blocks, each containing\nmulti-head attention mechanisms and multilayer per-\nceptrons. The output from these blocks is normalized\nthrough normalization layers and eventually projected\ninto a higher-dimensional space suitable for multimodal\nprocessing. Following the vision encoder, the multimodal\nprojector linearly transforms the 1024-dimensional image\ntokens into a 4096-dimensional space. These tokens are\nthen integrated into the language model component. In\nalignment with LLaVA’s framework, we utilized the Large\nLanguage Model Meta AI (LLAMA)-2 as our language\nmodel [21]. We selected the version with 7 billion para-\nmeters due to cost considerations.\nThe ﬁnal CXR-LLaVA takes a CXR image and question\nprompt as input; the image is transformed into image\ntokens via an image encoder, and the prompt is converted\nto text tokens through a tokenizer. Both are then fed into\na causal language model, which autoregressively generates\ntext responses to the questions. The trained model is\navailable as open-source ( https://github.com/ECOFRI/\nCXR_LLaVA), and its demo can be found at https://\nradiologist.app/cxr-llava/. Additionally, a comprehensive\nmodel card detailing the model ’s intended use cases,\nLee et al. European Radiology (2025) 35:4374– 4386 4375\nout-of-scope use, and limitations is provided on the same\nwebsite to ensure transparency and facilitate further\nresearch.\nTraining step 1: constructing and training a CXR-speciﬁc\nimage encoder\nDespite the capabilities of pretrained image encoders in\nunderstanding common visual objects, they often fall\nshort in accurately describing radiographic ﬁndings. In\nthis section, we propose an image encoder, based on ViT-\nL/16 and a two-step strategy for training them to learn the\nradiological context speciﬁc to CXR images.\nIn theﬁrst step, a simple classiﬁcation task was used to\ntrain the image encoder (Fig. 1a). The image encoder\ntransformed a CXR image into a representation and then\nclassiﬁed an abnormality by adding a simply fully con-\nnected layer as a classiﬁer. This classiﬁcation task enabled\nthe model to learn a fundamental yet crucial ability\nregarding abnormalities. We used 374,881 image-label\npairs from Dataset 1 to train and validate our image\nencoder. We assigned binary labels: when images had\nlabels associated with pathology, they were labeled as\n“abnormal,” while those marked as “no ﬁnding” were\ndesignated “normal.” The detailed implementation and\nsettings are described in the Supplementary material.\nIn the second step, the image encoder was further\ntrained based on the CLIP strategy to learn complex\nrepresentations of radiological terms (Fig.1b) [5]. Using\nthe CLIP strategy, the image encoder learned shared\nrepresentations between image and text by mapping\ncorresponding image and text pairs closer together and\nnon-corresponding pairs further apart. For instance, an\nimage showing signs of“pleural effusion” would have its\ncorresponding text label vector “pleural effusion” map-\nped closely to its image vector. This ensures that the\nmodel can accurately associate the visual features of\npleural effusion in CXRs with the correct textual\ndescription, thereby enhancing its ability to correctly\nidentify and describe pleural effusion in new, unseen\nimages. We chose pathological labels provided in the\ndataset, such as “atelectasis, ”“ pneumonia,”“ pleural\neffusion” and so on. For images with multiple patholo-\ngical labels, we connected them using commas. The\n592,580 image-text pairs from Datasets 1 and 2 were used\nin the training and validating process. The performance\nof the trained image encoder was evaluated and com-\npared with theﬁnal model; the detailed process and the\nperformance evaluation are described in the Supplemen-\ntary material.\nTraining step 2: feature alignment and end-to-endﬁne-\ntuning of CXR-LLaVA\nBefore ﬁne-tuning the CXR-LLaVA model, the features\nfrom the image encoder, as described in step 1, and lan-\nguage model (i.e., LLaMa-2) were aligned through addi-\ntional training, where the image encoder and language\nmodel weights were frozen, updating only the projection\nmatrix. The aligned image representation was computed\nTable 1 Countries of collection, years of publication, and numbers of frontal chest radiographs in the publicly available datasets used\nfor model training and evaluation\nDataset Country of collection Year of publication Numbers of frontal CXRs\nTraining Validation Test\nTraining dataset 1: chest radiograph datasets with pathologicﬁndings labeled\nBrixIA COVID-19 dataset [12] Italy 2021 3755 470 -\nCheXpert train/validation dataset [13] USA 2019 152,983 19,123 -\nNIH dataset [14] USA 2017 70,671 8833 -\nPadChest dataset [15] Spain 2019 86,438 10,805 -\nRSNA COVID-19 AI Detection Challenge [16] Various countries 2021 5066 634 -\nVinDR dataset [17] Vietnam 2020 14,314 1789 -\nSubtotal 333,227 41,654 -\nTraining dataset 2: chest radiograph dataset with free-text radiologic reports\nMIMIC dataset [18] USA 2019 193,513 24,186 -\nInternal test sets\nMIMIC dataset (randomly selected) [18] USA 2019 - - 3000\nCheXpert test dataset [13] USA 2022 - - 518\nSubtotal - - 3518\nExternal test set\nIndiana University dataset [19] USA 2016 - - 3689\nLee et al. European Radiology (2025) 35:4374– 4386 4376\nFig. 1 CXR-LLaVA training process.a Initially, the image encoder was trained on a basic classiﬁcation task to differentiate between normal and abnormal\nCXRs, thereby acquiring fundamental representations of CXRs.b Subsequently, the model underwent training with pairs of CXRs and their corresponding\npathological ﬁndings. This training employed the contrastive language-image pre-training (CLIP) strategy to foster shared representations between\nimages and text.c The image encoder was then assimilated into CXR-LLaVA, initiating the alignment of image representations with the large language\nmodel (LLM). In this phase, training focused on pairs of CXR images and radiologic reports, with updates conﬁned to the projection layer.d Upon\nsuccessful alignment of the image encoder with the LLM, an instructionﬁne-tuning process was undertaken. This involved a variety of radiologic reports\nand question-answer pairs, aiming to reﬁne the model’s capability to interpret CXRs and facilitate more informative interactions. Please note that the\nﬁgure abstracts from the detailed neural network information, omitting elements such as tokenizer, batch normalization, projection, and linear\nclassiﬁcation layers\nLee et al. European Radiology (2025) 35:4374– 4386 4377\nby updating the projection matrix using CXR images with\nreﬁned radiologic reports from Dataset 2 (Fig.1c).\nAfter aligning the image features, CXR-LLaVA under-\nwent an instruction-tuning process, which was critical for\nreﬁning the model’s interpretative capabilities (Fig. 1d).\nThis process involved using reﬁned radiology reports and\nmulti-turn question-answer dialogs generated by GPT-4,\nall based on Dataset 2 (Supplementary materials).\nInternal and external test set composition\nFor internal model testing, we utilized a randomly selec-\nted MIMIC dataset comprising 3000 images and accom-\npanying free-text radiologic reports [19]. These were not\nused during the model’s training and validation phases.\nAdditionally, we employed the CheXpert test dataset,\nwhich consists of 518 images, each binary labeled for 14\nﬁndings: atelectasis, cardiomegaly, consolidation, edema,\nenlarged cardiomediastinum, fracture, lung lesion, lung\nopacity, no ﬁnding, pleural effusion, pleural other, pneu-\nmonia, pneumothorax, and support devices [ 14]. For\nexternal model testing, we used a dataset from Indiana\nUniversity, consisting of 3689 pairs of images and free-\ntext radiologic reports [20].\nComparison with other multimodal LLMs\nTo evaluate the performance of our model, we compared\nits results with those of other publicly available multi-\nmodal LLMs, including OpenAI ’s GPT-4-vision and\nGoogle’s Gemini-Pro-Vision. Despite being in a preview\nstate and not beingﬁne-tuned for CXR report generation,\nthese general-purpose models have shown some potential.\nFor instance, GPT-4-vision has demonstrated a limited\nability to detect abnormalities in CXRs and the capacity to\nsolve the United States Medical Licensing Examination\ntests [22, 23]. However, LLaVA-MED, a modelﬁne-tuned\nfor medical image analysis, failed to generate accurate\nradiologic reports from CXRs, producing nearly identical\nreports for diverse CXRs, and was therefore excluded\nfrom our study. Other models, such as ELIXR and Med-\nPALM, which claim the ability to interpret CXRs, were\nnot publicly available and thus were not included in this\nanalysis [8, 24] (Supplementary materials).\nInternal test set evaluation\nTo evaluate the performance of radiologic report genera-\ntion in the MIMIC internal test set, we utilized CheXpert-\nLabeler to generate pathological labels [ 14]. This tool\nanalyzes free-text radiologic reports and generates labels\nsuch as positive, negative, or uncertain for each patholo-\ngical ﬁnding (atelectasis, cardiomegaly, consolidation,\nedema, enlarged cardiomediastinum, fracture, lung lesion,\nlung opacity, no ﬁnding, pleural effusion, pleural other,\npneumonia, pneumothorax, and support devices). We\ncompared these labels from the model-generated reports\nwith those from the original ground truth reports (Fig.2a).\nFor the CheXpert test set, which does not contain\nground-truth radiologic reports, we instructed the model to\ngenerate binary labels for the same 14ﬁndings. These labels\nwere then compared with the ground truth. This dataset is\nidentical to that used in a previous study where the\nCheXzero model exhibited expert-level pathology detec-\ntion capabilities [25]. Therefore, we evaluated our model’s\nperformance against both CheXzero and the average\ndiagnostic performance of three board-certi ﬁed radi-\nologists, as documented in the same publication (Fig.2b).\nExternal test set evaluation and human radiologist\nevaluation\nTo evaluate the model ’s performance on the Indiana\nexternal test set, we employed the same methodology\nused for the MIMIC internal test set, which involved\ncomparing the labels generated from the model’s reports\nwith the ground truth (Fig.2a).\nTo assess the model’s capability for autonomous or\nsemi-autonomous reporting without human radiologist\nintervention, an evaluation was conducted involving three\nhuman radiologists. From the Indiana external test set, 25\nabnormal images and 25 normal images were randomly\nselected. A total of 50 images were used to create 100\nreport-image pairs, with each image paired with a model-\ngenerated report and a ground truth report. The radi-\nologists were presented with these 100 report-image pairs\nin a random order for evaluation. They rated the\nacceptability of each report on a 4-point scale: (1) totally\nacceptable without any revision, (2) acceptable with minor\nrevision, (3) acceptable with major revision, and (4)\nunacceptable (Supplementary materials).\nStatistical analysis\nThe model’s performance in generating radiologic reports\nwas assessed using accuracy, sensitivity, speciﬁcity, and F1\nscores. Cases where the CheXpert-Labeler assigned an\n“uncertain” label or where the label was not mentioned\n(missing element) were excluded from our analysis. We\nincluded only deﬁnite positive or negative labels. Addi-\ntionally, due to the scarce number of images with labels\nsuch as“pleural other” and “fractures,” these were omitted\nfrom the analysis. The speciﬁc criteria for removing certain\nlabels and the details of the excluded labels are outlined in\nthe accompanying table. To estimate the con ﬁdence\nintervals of the accuracy, sensitivity, speci ﬁcity, and\nF1 scores, we utilized non-parametric bootstrapping with\n1000 iterations. For the evaluation conducted by human\nradiologists, the Cochran Q test was employed to deter-\nmine the statistical signiﬁcance of differences between the\nevaluations made by human radiologists and the model.\nLee et al. European Radiology (2025) 35:4374– 4386 4378\nResults\nModel performance on the internal test set\nTable 2 illustrates the report generation capabilities of our\nmodel on the MIMIC internal test set. The model\nachieved an average F1 score of 0.81, a sensitivity of 0.80,\nand a speci ﬁcity of 0.89 for six pathological labels,\nincluding cardiomegaly, consolidation, edema, pleural\neffusion, pneumonia, and pneumothorax. It demonstrated\nstrong performance, with F1 scores exceeding 0.8, in\nidentifying cardiomegaly, edema, and pleural effusion,\nwhile its ability to detect pneumothorax was weaker.\nOverall, the model exhibited higher average F1 scores\nthan GPT-4-vision or Gemini-Pro-Vision.\nTable 3 presents the model’s pathology detection per-\nformance on the CheXpert internal test set [ 25]. The\nmodel achieved an average F1 score of 0.57, a sensitivity of\n0.90, and a speciﬁcity of 0.67 forﬁve pathologicalﬁndings:\natelectasis, cardiomegaly, consolidation, edema, and\npleural effusion. While it performed relatively well in\nidentifying lung opacity, atelectasis, and pleural effusion,\nits effectiveness in detecting consolidation was lower. This\naverage F1 score of 0.57 is marginally lower than that of\nCheXzero, which achieved 0.61, and slightly below the\n0.62 F1 score reported for human radiologists. No\nestablished F1 scores from CheXzero and human radi-\nologists are available for diagnosing lung opacity and\nsupport devices, but our model demonstrated com-\nmendable F1 scores in detecting these conditions in CXR.\nFigure 3 displays an example CXR, highlighting the for-\nmat of the generated radiologic report. This report effec-\ntively pinpoints criticalﬁndings, such as bilateral pleural\neffusion, yet it occasionally overlooks speciﬁc details, such\nas the presence of a central catheter. In contrast, Fig.4\nshows that while the model appropriately recognized and\ndescribed the left pleural effusion, it failed to describe the\nleft pneumothorax and the left pleural drainage catheter.\nModel performance on the external test set\nIn the external test set, the model produced an average F1\nscore of 0.56, a sensitivity of 0.63, and a speciﬁcity of 0.93\nfor detecting cardiomegaly, consolidation, edema, pleural\neffusion, pneumonia, and pneumothorax. It showed an\nexcellent ability to detect cardiomegaly, edema, and\npneumonia, but its performance in detecting pneu-\nmothorax was signiﬁcantly weaker (Table4). Overall, the\nmodel outperformed other models in this regard. A\nreview of several examples showed that the model accu-\nrately identiﬁed and described the corresponding lesions\n(Figs. 5 and 6).\nIn the evaluation of radiologic report acceptability by\nhuman radiologists, the model achieved an “acceptable\nwithout any revision” rate of 51.3%, which closely aligns\nFig. 2 Model evaluationﬂow diagram.a Evaluation of datasets with ground-truth free-text radiologic reports, including the MIMIC internal test set and\nthe Indiana external test set. Pathologic labels were obtained using the CheXpert-Labeler from both the original reports and the model-generated\nreports, with a subsequent comparison of these results.b Evaluation of datasets with established ground-truth pathologic labels, speciﬁcally the\nCheXpert internal test set, involved directly generating pathologic labels from the model using a label generation prompt\nLee et al. European Radiology (2025) 35:4374– 4386 4379\nTable 2 Model performance with the MIMIC internal test set\nModel performance of each pathologic label in the MIMIC internal test set\nMetric and pathologic label Models\nCXR-LLaVA GPT-4-vision Gemini-Pro-Vision\nAccuracy\nCardiomegaly 0.79 (0.77, 0.82) 0.65 (0.62, 0.67) 0.65 (0.62, 0.67)\nConsolidation 0.93 (0.91, 0.96) 0.80 (0.77, 0.83) 0.55 (0.51, 0.58)\nEdema 0.81 (0.77, 0.85) 0.68 (0.61, 0.75) 0.61 (0.58, 0.64)\nPleural effusion 0.87 (0.85, 0.88) 0.66 (0.64, 0.68) 0.53 (0.51, 0.55)\nAverage for above four pathologies 0.85 (0.84, 0.86) 0.67 (0.65, 0.69) 0.58 (0.57, 0.59)\nPneumonia 0.69 (0.62, 0.76) 0.67 (0.61, 0.74) 0.71 (0.64, 0.77)\nPneumothorax 0.89 (0.88, 0.91) 0.88 (0.87, 0.90) 0.97 (0.91, 1.00)\nOverall average 0.86 (0.85, 0.87) 0.73 (0.71, 0.74) 0.59 (0.58, 0.60)\nSensitivity\nCardiomegaly 0.88 (0.85, 0.90) 0.92 (0.90, 0.93) 0.98 (0.97, 0.99)\nConsolidation 0.62 (0.48, 0.75) 0.17 (0.09, 0.27) 0.72 (0.65, 0.80)\nEdema 0.85 (0.81, 0.89) 0.69 (0.59, 0.78) 0.80 (0.76, 0.83)\nPleural effusion 0.85 (0.82, 0.87) 0.31 (0.27, 0.35) 0.93 (0.92, 0.95)\nAverage for above four pathologies 0.85 (0.84, 0.87) 0.62 (0.59, 0.64) 0.91 (0.90, 0.92)\nPneumonia 0.53 (0.42, 0.64) 0.80 (0.73, 0.87) 0.95 (0.91, 0.98)\nPneumothorax 0.35 (0.28, 0.42) 0.02 (0.00, 0.04) 0.00 (0.00, 0.00)\nOverall average 0.80 (0.78, 0.82) 0.61 (0.59, 0.64) 0.91 (0.90, 0.93)\nSpeciﬁcity\nCardiomegaly 0.55 (0.49, 0.61) 0.17 (0.13, 0.20) 0.04 (0.02, 0.06)\nConsolidation 0.97 (0.96, 0.99) 0.90 (0.88, 0.93) 0.50 (0.45, 0.54)\nEdema 0.75 (0.68, 0.81) 0.67 (0.56, 0.78) 0.39 (0.34, 0.43)\nPleural effusion 0.88 (0.86, 0.90) 0.85 (0.83, 0.87) 0.28 (0.26, 0.31)\nAverage for above four pathologies 0.85 (0.83, 0.86) 0.71 (0.69, 0.73) 0.29 (0.28, 0.31)\nPneumonia 0.87 (0.78, 0.95) 0.29 (0.16, 0.42) 0.14 (0.06, 0.23)\nPneumothorax 0.97 (0.96, 0.98) 0.99 (0.98, 0.99) 1.00 (1.00, 1.00)\nOverall average 0.89 (0.88, 0.90) 0.79 (0.78, 0.80) 0.30 (0.28, 0.31)\nF1 score\nCardiomegaly 0.86 (0.85, 0.88) 0.77 (0.75, 0.79) 0.78 (0.76, 0.80)\nConsolidation 0.68 (0.57, 0.78) 0.20 (0.11, 0.29) 0.41 (0.36, 0.47)\nEdema 0.84 (0.81, 0.87) 0.71 (0.63, 0.78) 0.69 (0.66, 0.72)\nPleural effusion 0.83 (0.81, 0.85) 0.39 (0.35, 0.43) 0.61 (0.58, 0.63)\nAverage for above four pathologies 0.84 (0.83, 0.85) 0.62 (0.60, 0.64) 0.67 (0.66, 0.68)\nPneumonia 0.65 (0.54, 0.74) 0.79 (0.73, 0.84) 0.82 (0.77, 0.86)\nPneumothorax 0.46 (0.37, 0.53) 0.03 (0.00, 0.07) 0.00 (0.00, 0.00)\nOverall average 0.81 (0.80, 0.82) 0.62 (0.61, 0.64) 0.68 (0.66, 0.69)\nIn our analysis, speciﬁc labels such as“lung lesion,”“ lung opacity,”“ atelectasis,”“ pleural other,”“ fracture,” and “support devices” were excluded due to their low\nfrequency, being under 5% in either the negative or positive class or having a sample number below 10, which makes them statistically less signiﬁcant for a balanced\nanalysis. Additionally, the label“enlarged cardiomediastinum” was not included as it signiﬁcantly overlaps with“cardiomegaly,” which could lead to redundant data\ninterpretations. For the labels“cardiomegaly,”“ consolidation,”“ edema,” and “pleural effusion,” we recorded the average score across multiple datasets to facilitate a\ncomprehensive performance comparison of the model on these four critical labels\nThe model achieved an excellent average F1 score of 0.81, outperforming the GPT-4-vision and Gemini-Pro-Vision models, which scored 0.62 and 0.68, respectively.\nThe model’s sensitivity of 0.80 was higher than GPT-4-Vision’s 0.61 but lower than Gemini-Pro-Vision’s 0.91. Its speciﬁcity of 0.89 was higher than both GPT-4-Vision’s\n0.79 and Gemini-Pro-Vision’s 0.30\nLee et al. European Radiology (2025) 35:4374– 4386 4380\nTable 3 Model performance with the CheXpert internal test set\nModel performance of each pathologic label in the CheXpert internal test set\nMetric and pathologic label Models\nCXR-LLaVA GPT-4-vision Gemini-Pro-Vision CheXzero [ 25] Human radiologists [ 25]\nAccuracy\nCardiomegaly 0.66 (0.62, 0.70) 0.31 (0.27, 0.35) 0.48 (0.44, 0.53) N/A N/A\nConsolidation 0.67 (0.64, 0.71) 0.94 (0.92, 0.96) 0.13 (0.10, 0.16) N/A N/A\nEdema 0.72 (0.68, 0.76) 0.84 (0.81, 0.87) 0.76 (0.73, 0.80) N/A N/A\nPleural effusion 0.77 (0.74, 0.81) 0.58 (0.54, 0.62) 0.78 (0.74, 0.81) N/A N/A\nAverage for above four pathologies 0.71 (0.69, 0.73) 0.67 (0.65, 0.69) 0.54 (0.52, 0.56) N/A N/A\nAtelectasis 0.77 (0.73, 0.80) 0.70 (0.66, 0.74) 0.69 (0.65, 0.73) N/A N/A\nAverage for aboveﬁve pathologies 0.72 (0.70, 0.74) 0.67 (0.66, 0.69) 0.57 (0.55, 0.59) N/A N/A\nLung opacity 0.82 (0.79, 0.86) 0.68 (0.64, 0.72) 0.54 (0.50, 0.59) N/A N/A\nSupport devices 0.76 (0.72, 0.79) 0.63 (0.58, 0.67) 0.59 (0.55, 0.64) N/A N/A\nOverall average 0.74 (0.73, 0.75) 0.67 (0.65, 0.68) 0.57 (0.55, 0.59) N/A N/A\nSensitivity\nCardiomegaly 0.90 (0.86, 0.95) 1.00 (1.00, 1.00) 0.57 (0.49, 0.65) N/A N/A\nConsolidation 0.93 (0.83, 1.00) 0.00 (0.00, 0.00) 0.93 (0.82, 1.00) N/A N/A\nEdema 0.92 (0.87, 0.98) 0.03 (0.00, 0.06) 0.18 (0.10, 0.27) N/A N/A\nPleural effusion 0.96 (0.92, 0.99) 0.70 (0.61, 0.78) 0.02 (0.00, 0.05) N/A N/A\nAverage for above four pathologies 0.93 (0.90, 0.95) 0.63 (0.58, 0.68) 0.36 (0.31, 0.41) N/A N/A\nAtelectasis 0.85 (0.79, 0.91) 0.00 (0.00, 0.00) 0.02 (0.00, 0.04) N/A N/A\nAverage for aboveﬁve pathologies 0.90 (0.88, 0.93) 0.44 (0.40, 0.48) 0.25 (0.22, 0.29) N/A N/A\nLung opacity 0.90 (0.87, 0.93) 0.73 (0.68, 0.78) 0.94 (0.91, 0.97) N/A N/A\nSupport devices 0.83 (0.79, 0.88) 0.95 (0.92, 0.97) 0.94 (0.90, 0.96) N/A N/A\nOverall average 0.89 (0.87, 0.90) 0.64 (0.61, 0.67) 0.60 (0.57, 0.63) N/A N/A\nSpeciﬁcity\nCardiomegaly 0.56 (0.51, 0.61) 0.01 (0.00, 0.03) 0.44 (0.39, 0.50) N/A N/A\nConsolidation 0.66 (0.62, 0.70) 1.00 (1.00, 1.00) 0.09 (0.06, 0.11) N/A N/A\nEdema 0.68 (0.64, 0.73) 0.99 (0.97, 1.00) 0.87 (0.83, 0.90) N/A N/A\nPleural effusion 0.72 (0.68, 0.77) 0.55 (0.51, 0.60) 0.97 (0.95, 0.99) N/A N/A\nAverage for above four pathologies 0.66 (0.64, 0.68) 0.68 (0.66, 0.70) 0.58 (0.55, 0.60) N/A N/A\nAtelectasis 0.73 (0.68, 0.77) 1.00 (1.00, 1.00) 0.99 (0.98, 1.00) N/A N/A\nAverage for aboveﬁve pathologies 0.67 (0.65, 0.69) 0.73 (0.71, 0.75) 0.65 (0.63, 0.67) N/A N/A\nLung opacity 0.74 (0.68, 0.79) 0.63 (0.57, 0.69) 0.10 (0.06, 0.14) N/A N/A\nSupport devices 0.68 (0.62, 0.74) 0.29 (0.24, 0.35) 0.23 (0.18, 0.29) N/A N/A\nOverall average 0.68 (0.66, 0.70) 0.68 (0.66, 0.70) 0.56 (0.54, 0.58) N/A N/A\nF1 score\nCardiomegaly 0.62 (0.56, 0.67) 0.46 (0.42, 0.51) 0.39 (0.33, 0.45) 0.74 (0.69, 0.79) 0.68 (0.63, 0.72)\nConsolidation 0.24 (0.17, 0.31) 0.00 (0.00, 0.00) 0.11 (0.07, 0.15) 0.33 (0.24, 0.42) 0.39 (0.28, 0.49)\nEdema 0.50 (0.43, 0.57) 0.05 (0.00, 0.11) 0.19 (0.11, 0.26) 0.60 (0.52, 0.68) 0.58 (0.51, 0.65)\nPleural effusion 0.63 (0.57, 0.69) 0.41 (0.34, 0.47) 0.03 (0.00, 0.09) 0.70 (0.63, 0.76) 0.74 (0.69, 0.78)\nAverage for above four pathologies 0.53 (0.49, 0.56) 0.40 (0.37, 0.44) 0.21 (0.18, 0.25) N/A N/A\nAtelectasis 0.69 (0.64, 0.74) 0.00 (0.00, 0.00) 0.04 (0.00, 0.08) 0.65 (0.59, 0.70) 0.69 (0.65, 0.73)\nAverage for aboveﬁve pathologies 0.57 (0.54, 0.59) 0.35 (0.32, 0.39) 0.19 (0.17, 0.22) 0.61 (0.57, 0.64) 0.62 (0.59, 0.64)\nLung opacity 0.84 (0.81, 0.87) 0.71 (0.66, 0.75) 0.68 (0.64, 0.72) N/A N/A\nSupport devices 0.78 (0.74, 0.82) 0.72 (0.69, 0.76) 0.70 (0.66, 0.74) N/A N/A\nOverall average 0.67 (0.65, 0.69) 0.53 (0.51, 0.55) 0.45 (0.43, 0.47) N/A N/A\nUnbalanced labels like“lung lesion,”“ pneumonia,”“ pneumothorax,”“ pleural other,” and “fracture,” which had a frequency of less than 5% in either the negative or\npositive class, were not included in the analysis. Additionally, the label“enlarged cardiomediastinum” was not included as it signiﬁcantly overlaps with“cardiomegaly,”\nwhich could lead to redundant data interpretations. For the labels“cardiomegaly,”“ consolidation,”“ edema,” and “pleural effusion,” we recorded the average score\nacross multiple datasets to facilitate a comprehensive performance comparison of the model on these four critical labels\nThe model attained an average F1 score of 0.57 forﬁve key pathologies, which was marginally lower than CheXzero’s 0.61 and human radiologists’ 0.62. However, it\ndemonstrated exceptional capability in identifying lung opacity, support devices, and atelectasis. The model’s overall sensitivity was 0.89 and speciﬁcity was 0.68,\ncompared to GPT-4-vision’s sensitivity of 0.64 and speciﬁcity of 0.68, and Gemini-Pro-Vision’s sensitivity of 0.60 and speciﬁcity of 0.56. The sensitivity and speciﬁcity for\nCheXzero and human radiologists were not available, so a direct comparison could not be made\nLee et al. European Radiology (2025) 35:4374– 4386 4381\nwith the 54.0% acceptability rate of ground truth reports.\nTo gauge the model’s capability for autonomous reporting\nwithout human radiologist intervention, we deﬁned suc-\ncessful autonomous reporting as reports deemed accep-\ntable either without any revision or with only minor\nrevisions. By this criterion, the model achieved a success\nrate of 72.7% (Table5). While this is lower than the 84.0%\nsuccess rate of ground truth reports, the difference in the\nrate of autonomous reporting between the model and the\nground truth was found to be statistically signi ﬁcant,\nindicating that the model was somewhat inferior in terms\nof the autonomous reporting rate. However, the model\nstill maintained a commendable success rate of over 70%.\nDiscussion\nWe successfully developed a multimodal large language\nmodel capable of accurately detecting major pathological\nﬁndings in CXRs and generating free-text radiologic\nreports. Our model exhibited relatively good performance\ncompared to other publicly available general-purpose\nmultimodal LLMs, such as GPT-4-vision and Gemini-\nPro-Vision. We also explored the potential of multimodal\nLLMs for autonomous or semi-autonomous reporting in\nchest radiography. However, there are some limitations to\nour study.\nFirst, the evaluation method we employed has inherent\nlimitations. While we used CheXpert-Labeler to assess the\nquality of the reports, this tool only evaluates the explicit\npresence of pathological labels and does not consider the\nlocation or number of pathological lesions. As a result, this\nmethod may not fully reﬂe c tt h et r u ea c c u r a c yo ft h e\ngenerated reports. Second, our model showed poor per-\nformance in identifying certain pathological lesions, such as\npneumothorax and consolidation. Notably, its diagnostic\nperformance was inferior to that of human radiologists, as\nshown in the CheXpert internal test set. This might be\npartly due to the resolution limitations of our model, which\nprocesses 512 × 512 pixel images, a lower resolution than\nthe higher-resolution images used by radiologists on spe-\ncialized monitors. Moreover, our model processes 8-bit\nimages with a grayscale of 256 levels, whereas radiologist\nmonitors can display up to 10 or 12-bit grayscale images,\nFig. 4 An example of a chest radiograph from the CheXpert internal test set. The model appropriately recognized the left pleural effusion but failed to\nidentify the left pneumothorax and left pleural drainage catheter. The left pneumothorax is a clinically signiﬁcant ﬁnding, indicating that further\nimprovements to the model are necessary\nFig. 3 An example of a chest radiograph from the CheXpert internal test set. While the model identiﬁed the presence of pleural effusions, atelectasis,\nand lung opacity, it omitted details about the central catheter (support device)\nLee et al. European Radiology (2025) 35:4374– 4386 4382\nTable 4 Model performance with the Indiana external test set\nModel performance of each pathologic label in the Indiana external test set\nMetric and pathologic label Models\nCXR-LLaVA GPT-4-vision Gemini-Pro-Vision\nAccuracy\nCardiomegaly 0.72 (0.69, 0.74) 0.35 (0.33, 0.37) 0.41 (0.39, 0.43)\nConsolidation 0.93 (0.89, 0.95) 0.93 (0.91, 0.94) 0.74 (0.71, 0.77)\nEdema 0.94 (0.90, 0.98) 0.76 (0.63, 0.88) 0.63 (0.57, 0.68)\nPleural effusion 0.94 (0.93, 0.95) 0.88 (0.87, 0.90) 0.66 (0.64, 0.68)\nAverage for above four pathologies 0.88 (0.87, 0.89) 0.68 (0.66, 0.69) 0.58 (0.56, 0.59)\nPneumonia 0.83 (0.74, 0.90) 0.61 (0.47, 0.76) 0.69 (0.46, 0.92)\nPneumothorax 0.95 (0.94, 0.96) 0.99 (0.98, 0.99) N/A\nOverall average 0.90 (0.89, 0.90) 0.75 (0.74, 0.76) 0.58 (0.56, 0.59)\nSensitivity\nCardiomegaly 0.67 (0.62, 0.71) 0.81 (0.78, 0.84) 0.81 (0.78, 0.84)\nConsolidation 0.33 (0.09, 0.58) 0.08 (0.00, 0.18) 0.18 (0.08, 0.30)\nEdema 0.50 (0.25, 0.77) 0.29 (0.00, 0.67) 0.44 (0.29, 0.58)\nPleural effusion 0.71 (0.63, 0.79) 0.19 (0.12, 0.27) 0.71 (0.63, 0.79)\nAverage for above four pathologies 0.66 (0.62, 0.70) 0.66 (0.63, 0.70) 0.73 (0.70, 0.76)\nPneumonia 0.52 (0.30, 0.71) 0.84 (0.65, 1.00) 1.00 (1.00, 1.00)\nPneumothorax 0.07 (0.00, 0.19) 0.00 (0.00, 0.00) N/A\nOverall average 0.63 (0.59, 0.67) 0.65 (0.61, 0.68) 0.73 (0.70, 0.77)\nSpeciﬁcity\nCardiomegaly 0.75 (0.71, 0.78) 0.21 (0.19, 0.23) 0.29 (0.27, 0.31)\nConsolidation 0.96 (0.93, 0.98) 0.96 (0.95, 0.97) 0.77 (0.74, 0.80)\nEdema 1.00 (1.00, 1.00) 0.83 (0.71, 0.95) 0.67 (0.60, 0.72)\nPleural effusion 0.95 (0.95, 0.96) 0.92 (0.90, 0.93) 0.65 (0.64, 0.68)\nAverage for above four pathologies 0.91 (0.90, 0.92) 0.68 (0.66, 0.69) 0.55 (0.54, 0.57)\nPneumonia 0.95 (0.88, 1.00) 0.47 (0.28, 0.66) 0.00 (0.00, 0.00)\nPneumothorax 0.97 (0.96, 0.98) 1.00 (1.00, 1.00) N/A\nOverall average 0.93 (0.92, 0.94) 0.76 (0.75, 0.77) 0.55 (0.54, 0.57)\nF1 score\nCardiomegaly 0.62 (0.57, 0.65) 0.37 (0.34, 0.39) 0.39 (0.37, 0.42)\nConsolidation 0.31 (0.09, 0.50) 0.08 (0.00, 0.17) 0.07 (0.03, 0.11)\nEdema 0.67 (0.33, 0.86) 0.25 (0.00, 0.52) 0.28 (0.18, 0.37)\nPleural effusion 0.55 (0.48, 0.62) 0.13 (0.08, 0.18) 0.17 (0.14, 0.20)\nAverage for above four pathologies 0.59 (0.55, 0.62) 0.33 (0.31, 0.35) 0.30 (0.28, 0.32)\nPneumonia 0.63 (0.42, 0.79) 0.63 (0.45, 0.77) 0.82 (0.56, 0.96)\nPneumothorax 0.05 (0.00, 0.13) 0.00 (0.00, 0.00) N/A\nOverall average 0.56 (0.53, 0.59) 0.33 (0.31, 0.35) 0.30 (0.28, 0.32)\nWe excluded labels that were unbalanced, with fewer than 10 samples in either the negative or positive category. This included labels such as“lung lesion,”\n“atelectasis,”“ pleural other,”“ fracture,” and “support devices.” These were omitted to ensure statistical relevance and balance in the analysis. Additionally, the label\n“enlarged cardiomediastinum” was not included as it signiﬁcantly overlaps with“cardiomegaly,” which could lead to redundant data interpretations. Furthermore,\n“lung opacity” was excluded due to its broad nature, which increases the likelihood of labeling errors by the labeler. For the labels“cardiomegaly,”“ consolidation,”\n“edema,” and “pleural effusion,” we recorded the average score across multiple datasets to facilitate a comprehensive performance comparison of the model on these\nfour critical labels. Gemini-Pro-Vision was excluded from the analysis for“pneumothorax” due to the absence of positive samples, which is indicated as N/A in\nthe table\nThe model achieved an overall average F1 score of 0.56, excelling particularly in the detection of cardiomegaly (0.62), edema (0.67), and pneumonia (0.63). However,\nits performance in detecting pneumothorax was notably lower (0.05)\nLee et al. European Radiology (2025) 35:4374– 4386 4383\nproviding ﬁner details. These factors could contribute to\nthe model’s suboptimal performance in detecting subtle\nlesions. Third, the model intentionally omits descriptions\nof supporting devices such as feeding tubes and endo-\ntracheal tubes in CXRs. During the process of reﬁning\noriginal reports with GPT-4 for training data, we removed\nall mentions of these devices. This decision was due to the\nvaried nomenclature used to describe these devices (e.g.,\nnasogastric tube, Levin tube, Dobhoff tube, feeding tube)\nand the frequent inclusion of speciﬁc numerical details\nabout the tip location. Since language models inherently\nstruggle with processing numerical information accurately,\nincluding this varied and numerical information led to\nhallucinations. Consequently, we excluded it from the\ntraining dataset, and the generated reports do not include\ninformation about these supporting devices. Fourth, the\nFig. 6 An example of a chest radiograph from the Indiana external test set. The model’s interpretation identiﬁed right upper lobe consolidation and\nproposed pneumonia as a possible diagnosis, which is reasonable. Nonetheless, the model failed to detect a small left upper lung nodule (black arrow)\nFig. 5 An example of a chest radiograph from the Indiana external test set. The model’s interpretation included information about bilateral pulmonary\nnodules and suggested a possible diagnosis of lung metastasis or infection, which is reasonable. It also recommended that an additional chest CT scan\nmight be helpful. However, the model could not detect the implanted venous access device\nLee et al. European Radiology (2025) 35:4374– 4386 4384\nmodels to which we compared ours are general purpose\nand notﬁne-tuned for CXR interpretation. Therefore, it is\nnot unexpected that ourﬁne-tuned model would outper-\nform them. Nevertheless, it is noteworthy that these\ngeneral-purpose models still achieved high F1 scores in\ndiagnosing conditions like cardiomegaly and pneumonia.\nSeveral non-peer-reviewed public multimodal LLMs, such\nas Xraygpt, UniXGen, LLM-CXR, and CheXagent have\nbeen released for CXR interpretation, but we did not\ninclude them in our comparison due to potential dataset\noverlap, as we utilized a public dataset for both training and\ntesting [26– 29]. Fifth, our assessment of the potential of\nour model for autonomous reporting was based on a lim-\nited dataset of just 50 CXRs, which does not mirror real-\nworld clinical settings. Future research should involve\nlarger-scale studies to ensure the safety and efﬁcacy of\nmultimodal LLMs in CXR interpretation. Lastly, as an\nintegral component of CXR-LLaVA is an LLM, hallucina-\ntions and confabulations are inherent limitations. The\nmodel can generate text that appears plausible but may be\nincorrect or nonsensical. Therefore, CXR-LLaVA may\nexhibit hallucinations, and it is unclear under which con-\nditions these hallucinations are most likely to occur or how\nthey can be effectively prevented. Consequently, it is clear\nthat the current model must not be used for clinical pur-\nposes without extensive validation.\nIn conclusion, our study demonstrates the capability of\nmultimodal LLMs to generate radiologic reports that\naccurately recognize major lesions. By making our model\nopen-source, we aim to promote the development of\nmore capable and accurate models. We are conﬁdent that\nmultimodal large language models have considerable\npotential to assist clinicians, reduce the workload of\nradiologists in clinical settings, and ultimately improve\npatient outcomes.\nAbbreviations\nCLIP Contrastive language-image pre-training\nCNN Convolutional neural network\nCXRs Chest X-ray images\nLLM Large language model\nViT Vision transformers\nSupplementary information\nThe online version contains supplementary material available athttps://doi.\norg/10.1007/s00330-024-11339-6.\nAcknowledgements\nWe appreciate the support of high-performance graphic processing unit\ncomputing provided by the High-Performance Computing and Artiﬁcial\nIntelligence Open Infrastructure at the Gwangju Institute of Science and\nTechnology Super Computing Center. We also acknowledge the utilization of\nlarge language model (GPT-4, OpenAI) to enhance the quality of our medical\nwriting. Any revisions made by large language model was thoroughly\nreviewed by the authors and subsequent adjustments were made as deemed\nappropriate.\nFunding\nThis study was supported by Institute of Information and Communications\nTechnology Planning and Evaluation (IITP) grant funded by the Korea\ngovernment (MSIT) (No.2019-0-01842, Artiﬁcial Intelligence Graduate School\nProgram (GIST), No. 2021-0-02068, Artiﬁcial Intelligence Innovation Hub), the\nNational Research Foundation of Korea under grant NRF-2022R1F1A1068529,\nand the NAVER Digital Bio Innovation Research Fund, funded by NAVER\nCorporation (Grant No. 3720230020). Open Access funding enabled and\norganized by Seoul National University Hospital.\nCompliance with ethical standards\nGuarantor\nThe scientiﬁc guarantor of this publication is Seowoo Lee.\nConﬂict of interest\nThe authors of this manuscript declare relationships with the following\ncompanies: Medical IP and RadiSen. Seowoo Lee, Hyungjin Kim, and Soon Ho\nYoon hold stock options in Medical IP. Hyungjin Kim receives consulting fees\nfrom RadiSen and is a member of the Scientiﬁc Editorial Board forEuropean\nRadiology (section: chest) and, as such, did not participate in the selection nor\nreview processes for this article. The author’s roles in these organizations are\nunrelated to the content of the submitted work. Other authors do not have\nconﬂicts of interest.\nStatistics and biometry\nNo complex statistical methods were necessary for this paper.\nInformed consent\nWritten informed consent was not required for this study due to the exclusive\nuse of publicly available datasets.\nEthical approval\nInstitutional Review Board (IRB) approval was not required for this study\nbecause it utilized publicly available datasets. This research adheres to the\nterms of use associated with the datasets, ensuring compliance with ethical\nstandards.\nTable 5 Evaluation of radiologic report acceptability by human radiologists from the Indiana external test set\nClass Meaning CXR-LLaVA Ground truth Comparison\nA Acceptable without any revision 77 (51.3%) 81 (54.0%)\nB Acceptable after minor revision 32 (21.3%) 45 (30.0%)\nC Acceptable after major revision 8 (5.3%) 6 (4.0%)\nD Unacceptable 33 (22.0%) 18 (12.0%)\nA +B Successful autonomous reporting 109 (72.7%) 126 (84.0%) p < 0.001\nThe model achieved a 51.3% rate (77 cases) of being“acceptable without any revision” (Class A), closely mirroring the 54.0% rate of the ground truth reports. The\nmodel’s success rate for autonomous reporting (Class A+ B) reached 72.7% (109 cases), slightly lower than the 84.0% for ground truth reports. This difference was\nstatistically signiﬁcant (p < 0.001), highlighting the comparative capabilities and limitations of the model in autonomous radiologic reporting\nLee et al. European Radiology (2025) 35:4374– 4386 4385\nStudy subjects or cohorts overlap\nA preliminary version of this work has been made publicly available as a\npreprint. The preprint of this study, titled“CXR-LLAVA: a multimodal large\nlanguage model for interpreting chest X-ray images,” has been uploaded to\nthe arXiv repository. It can be accessed through the followinghttps://arxiv.org/\nabs/2310.18341.\nMethodology\n● Retrospective\n● Diagnostic study\n● Study using public data collected from multiple centers\nReceived: 19 March 2024 Revised: 25 October 2024 Accepted: 4 December\n2024\nPublished online: 15 January 2025\nReferences\n1. Shamshad F, Khan S, Zamir SW et al (2023) Transformers in medical\nimaging: a survey. Med Image Anal 88:102802\n2. Huang S-C, Pareek A, Jensen M, Lungren MP, Yeung S, Chaudhari AS\n(2023) Self-supervised learning for medical image classiﬁcation: a sys-\ntematic review and implementation guidelines. NPJ Digit Med 6:74\n3. Shen D, Wu G, Suk H-I (2017) Deep learning in medical image analysis.\nAnnu Rev Biomed Eng 19:221– 248\n4. Gozalo-Brizuela R, Garrido-Merchan EC (2023) ChatGPT is not all you need.\nA state of the art review of large generative AI models. Preprint athttps://\narxiv.org/abs/2301.04655\n5. Radford A, Kim JW, Hallacy C et al (2021) Learning transferable visual\nmodels from natural language supervision. PMLR 139:8748– 8763\n6. Li J, Li D, Savarese S, Hoi S (2023) Blip-2: Bootstrapping language-image\npre-training with frozen image encoders and large language models.\nInternational conference on machine learning. PMLR, pp 19730– 19742\n7. Liu H, Li C, Wu Q, Lee YJ (2024). Visual instruction tuning. Advances in\nNeural Information Processing Systems, 36.\n8. Xu S, Yang L, Kelly C et al (2023) ELIXR: towards a general purpose X-ray\nartiﬁcial intelligence system through alignment of large language models\nand radiology vision encoders. Preprint at https://arxiv.org/abs/2308.\n01317\n9. Li C, Wong C, Zhang S et al (2024). Llava-med: Training a large language-\nand-vision assistant for biomedicine in one day. Advances in Neural\nInformation Processing Systems, 36.\n10. Markoti ć V, Pojužina T, Radančević D, Miljko M, Pokrajčić V (2021) The\nradiologist workload increase; where is the limit?: Mini review and case\nstudy. Psychiatr Danub 33:768– 770\n11. Sajed S, Sanati A, Garcia JE, Rostami H, Keshavarz A, Teixeira A (2023) The\neffectiveness of deep learning vs. traditional methods for lung disease\ndiagnosis using chest X-ray images: a systematic review. Appl Soft\nComput 147:110817\n12. Lee S, Youn J, Kim M, Yoon SH (2023) CXR-LLAVA: multimodal large\nlanguage model for interpreting chest x-ray images. Preprint athttps://\narxiv.org/abs/2310.18341\n13. Signoroni A, Savardi M, Benini S et al (2021) BS-Net: learning COVID-19\npneumonia severity on a large chest X-ray dataset. Med Image Anal\n71:102046\n14. Irvin J, Rajpurkar P, Ko M et al (2019) CheXpert: a large chest radiograph\ndataset with uncertainty labels and expert comparison. Preprint at\nhttps://arxiv.org/abs/1901.07031\n15. Wang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM (2017) Chestx-ray8:\nHospital-scale chest x-ray database and benchmarks on weakly-\nsupervised classiﬁcation and localization of common thorax diseases.\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, pp 2097– 2106\n16. Bustos A, Pertusa A, Salinas J-M, De La Iglesia-Vaya M (2020) Padchest: a\nlarge chest x-ray image dataset with multi-label annotated reports. Med\nImage Anal 66:101797\n17. Lakhani P, Mongan J, Singhal C et al (2023) The 2021 SIIM-FISABIO-\nRSNA machine learning COVID-19 challenge: annotation and standard\nexam classi ﬁcation of COVID-19 chest radiographs. J Digit Imaging\n36:365– 372\n18. Nguyen HQ, Lam K, Le LT et al (2022) VinDr-CXR: an open dataset of chest\nX-rays with radiologist’s annotations. Sci Data 9:429\n19. Johnson AE, Pollard TJ, Berkowitz SJ et al (2019) MIMIC-CXR, a de-\nidentiﬁed publicly available database of chest radiographs with free-text\nreports. Sci Data 6:317\n20. Demner-Fushman D, Kohli MD, Rosenman MB et al (2016) Preparing a\ncollection of radiology examinations for distribution and retrieval. J Am\nMed Inform Assoc 23:304– 310\n21. Touvron H, Martin L, Stone K et al (2023) Llama 2: open foundation and\nﬁne-tuned chat models. Preprint athttps://arxiv.org/abs/2307.09288\n22. Yang Z, Yao Z, Tasmin M et al (2023) Performance of multimodal GPT-4V\non USMLE with image: potential for imaging diagnostic support with\nexplanations. Preprint at https://doi.org/10.1101/2023.10.26.23297629:\n2023.2010.2026.23297629\n23. Brin D, Sorin V, Barash Y et al (2024) Assessing GPT-4 multimodal per-\nformance in radiological image analysis. European Radiology 1– 7\n24. Tu T, Azizi S, Driess D et al (2024) Towards generalist biomedical AI. NEJM\nAI 1:AIoa2300138\n25. Tiu E, Talius E, Patel P, Langlotz CP, Ng AY, Rajpurkar P (2022) Expert-level\ndetection of pathologies from unannotated chest X-ray images via self-\nsupervised learning. Nat Biomed Eng 6:1399– 1406\n26. Thawkar O, Shaker A, Mullappilly SS et al (2023) XrayGPT: chest radio-\ngraphs summarization using medical vision-language models. Preprint at\nhttps://arxiv.org/abs/2306.07971\n27. Lee H, Kim W, Kim J-H et al (2024) Vision-Language Generative Model for\nView-Speciﬁc Chest X-ray Generation. Conference on Health, Inference,\nand Learning. PMLR, pp 280– 296\n28. Lee S, Kim WJ, Chang J, Ye JC (2023) LLM-CXR: instruction-ﬁnetuned LLM\nfor CXR image understanding and generation. Preprint athttps://arxiv.\norg/abs/2305.11490\n29. Chen Z, Varma M, Delbrouck J-B et al (2024) A Vision-Language Foun-\ndation Model to Enhance Efﬁciency of Chest X-ray Interpretation. Preprint\nat https://arxiv.org/abs/2401.12208\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in\npublished maps and institutional afﬁliations.\nLee et al. European Radiology (2025) 35:4374– 4386 4386",
  "topic": "Medicine",
  "concepts": [
    {
      "name": "Medicine",
      "score": 0.765900731086731
    },
    {
      "name": "Neuroradiology",
      "score": 0.7296813726425171
    },
    {
      "name": "Interventional radiology",
      "score": 0.6383664608001709
    },
    {
      "name": "Radiology",
      "score": 0.6147719025611877
    },
    {
      "name": "Medical physics",
      "score": 0.39069172739982605
    },
    {
      "name": "Nuclear medicine",
      "score": 0.32845476269721985
    },
    {
      "name": "Neurology",
      "score": 0.07928386330604553
    },
    {
      "name": "Psychiatry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2802835388",
      "name": "Seoul National University Hospital",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I39534123",
      "name": "Gwangju Institute of Science and Technology",
      "country": "KR"
    }
  ]
}