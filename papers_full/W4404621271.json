{
  "title": "Using large language models (<scp>ChatGPT</scp>, Copilot, <scp>PaLM</scp>, Bard, and Gemini) in Gross Anatomy course: Comparative analysis",
  "url": "https://openalex.org/W4404621271",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A219480364",
      "name": "Volodymyr Mavrych",
      "affiliations": [
        "Alfaisal University"
      ]
    },
    {
      "id": "https://openalex.org/A2112568194",
      "name": "Paul Ganguly",
      "affiliations": [
        "Alfaisal University"
      ]
    },
    {
      "id": "https://openalex.org/A2045828954",
      "name": "Olena Bolgova",
      "affiliations": [
        "Alfaisal University"
      ]
    },
    {
      "id": "https://openalex.org/A219480364",
      "name": "Volodymyr Mavrych",
      "affiliations": [
        "Alfaisal University"
      ]
    },
    {
      "id": "https://openalex.org/A2112568194",
      "name": "Paul Ganguly",
      "affiliations": [
        "Alfaisal University"
      ]
    },
    {
      "id": "https://openalex.org/A2045828954",
      "name": "Olena Bolgova",
      "affiliations": [
        "Alfaisal University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4392711451",
    "https://openalex.org/W4382020836",
    "https://openalex.org/W4322723456",
    "https://openalex.org/W4385827730",
    "https://openalex.org/W4321351832",
    "https://openalex.org/W4367310920",
    "https://openalex.org/W4379599615",
    "https://openalex.org/W4390484920",
    "https://openalex.org/W4396772600",
    "https://openalex.org/W4322006551",
    "https://openalex.org/W4385900159",
    "https://openalex.org/W4385263677",
    "https://openalex.org/W4388823522",
    "https://openalex.org/W4390985458",
    "https://openalex.org/W4385620111",
    "https://openalex.org/W4391723926",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4361289889",
    "https://openalex.org/W4367311208",
    "https://openalex.org/W4391308235",
    "https://openalex.org/W4367175039",
    "https://openalex.org/W4319332969",
    "https://openalex.org/W4380423243",
    "https://openalex.org/W4388896754",
    "https://openalex.org/W4327946446",
    "https://openalex.org/W4390757744",
    "https://openalex.org/W4380685958",
    "https://openalex.org/W4380887490",
    "https://openalex.org/W2908201961",
    "https://openalex.org/W4388813824",
    "https://openalex.org/W4385827193",
    "https://openalex.org/W4382751541"
  ],
  "abstract": "Abstract The increasing application of generative artificial intelligence large language models (LLMs) in various fields, including medical education, raises questions about their accuracy. The primary aim of our study was to undertake a detailed comparative analysis of the proficiencies and accuracies of six different LLMs (ChatGPT‐4, ChatGPT‐3.5‐turbo, ChatGPT‐3.5, Copilot, PaLM, Bard, and Gemini) in responding to medical multiple‐choice questions (MCQs), and in generating clinical scenarios and MCQs for upper limb topics in a Gross Anatomy course for medical students. Selected chatbots were tested, answering 50 USMLE‐style MCQs. The questions were randomly selected from the Gross Anatomy course exam database for medical students and reviewed by three independent experts. The results of five successive attempts to answer each set of questions by the chatbots were evaluated in terms of accuracy, relevance, and comprehensiveness. The best result was provided by ChatGPT‐4, which answered 60.5% ± 1.9% of questions accurately, then Copilot (42.0% ± 0.0%) and ChatGPT‐3.5 (41.0% ± 5.3%), followed by ChatGPT‐3.5‐turbo (38.5% ± 5.7%). Google PaLM 2 (34.5% ± 4.4%) and Bard (33.5% ± 3.0%) gave the poorest results. The overall performance of GPT‐4 was statistically superior ( p &lt; 0.05) to those of Copilot, GPT‐3.5, GPT‐Turbo, PaLM2, and Bard by 18.6%, 19.5%, 22%, 26%, and 27%, respectively. Each chatbot was then asked to generate a clinical scenario for each of the three randomly selected topics—anatomical snuffbox, supracondylar fracture of the humerus, and the cubital fossa—and three related anatomical MCQs with five options each, and to indicate the correct answers. Two independent experts analyzed and graded 216 records received (0–5 scale). The best results were recorded for ChatGPT‐4, then for Gemini, ChatGPT‐3.5, and ChatGPT‐3.5‐turbo, Copilot, followed by Google PaLM 2; Copilot had the lowest grade. Technological progress notwithstanding, LLMs have yet to mature sufficiently to take over the role of teacher or facilitator completely within a Gross Anatomy course; however, they can be valuable tools for medical educators.",
  "full_text": null,
  "topic": "Gross anatomy",
  "concepts": [
    {
      "name": "Gross anatomy",
      "score": 0.5535728931427002
    },
    {
      "name": "Course (navigation)",
      "score": 0.4551108777523041
    },
    {
      "name": "Turbo",
      "score": 0.4353828728199005
    },
    {
      "name": "Medical education",
      "score": 0.4305506646633148
    },
    {
      "name": "Medical school",
      "score": 0.42664605379104614
    },
    {
      "name": "Computer science",
      "score": 0.4213060438632965
    },
    {
      "name": "Medicine",
      "score": 0.37328776717185974
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35159194469451904
    },
    {
      "name": "Psychology",
      "score": 0.3474140763282776
    },
    {
      "name": "Anatomy",
      "score": 0.22201383113861084
    },
    {
      "name": "Engineering",
      "score": 0.1353122889995575
    },
    {
      "name": "Automotive engineering",
      "score": 0.0
    },
    {
      "name": "Aerospace engineering",
      "score": 0.0
    }
  ]
}