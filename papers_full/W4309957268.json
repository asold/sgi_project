{
  "title": "ASiT: Local-Global Audio Spectrogram vIsion Transformer for Event Classification",
  "url": "https://openalex.org/W4309957268",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5111136893",
      "name": "Sara Atito",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100778584",
      "name": "Muhammad Awais",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100676721",
      "name": "Wenwu Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5066967599",
      "name": "Mark D. Plumbley",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5028209738",
      "name": "Josef Kittler",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2025768430",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W4375869340",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4285483774",
    "https://openalex.org/W3036601975",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3206996142",
    "https://openalex.org/W3186781156",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W3094550259",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W2561826558",
    "https://openalex.org/W1604034532",
    "https://openalex.org/W3162391496",
    "https://openalex.org/W2981087920",
    "https://openalex.org/W4312048190",
    "https://openalex.org/W2798991696",
    "https://openalex.org/W2137944862",
    "https://openalex.org/W3035468615",
    "https://openalex.org/W2154211011",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3196974791",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W3201143670",
    "https://openalex.org/W4308847350",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W3016181583",
    "https://openalex.org/W3035060554",
    "https://openalex.org/W4398958419",
    "https://openalex.org/W4297841853",
    "https://openalex.org/W4225911888",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2979476256",
    "https://openalex.org/W2963425185",
    "https://openalex.org/W2768188490",
    "https://openalex.org/W2526050071",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W3170863103",
    "https://openalex.org/W4287236261",
    "https://openalex.org/W3015949486",
    "https://openalex.org/W4312804044",
    "https://openalex.org/W3205475937",
    "https://openalex.org/W4224993428",
    "https://openalex.org/W2887997457",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W3134652006",
    "https://openalex.org/W2797583228",
    "https://openalex.org/W2593116425",
    "https://openalex.org/W3093475354",
    "https://openalex.org/W2963677766",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W4296960042"
  ],
  "abstract": "Transformers, which were originally developed for natural language processing, have recently generated significant interest in the computer vision and audio communities due to their flexibility in learning long-range relationships. Constrained by the data hungry nature of transformers and the limited amount of labelled data, most transformer-based models for audio tasks are finetuned from ImageNet pretrained models, despite the huge gap between the domain of natural images and audio. This has motivated the research in self-supervised pretraining of audio transformers, which reduces the dependency on large amounts of labeled data and focuses on extracting concise representations of audio spectrograms. In this paper, we propose \\textbf{L}ocal-\\textbf{G}lobal \\textbf{A}udio \\textbf{S}pectrogram v\\textbf{I}sion \\textbf{T}ransformer, namely ASiT, a novel self-supervised learning framework that captures local and global contextual information by employing group masked model learning and self-distillation. We evaluate our pretrained models on both audio and speech classification tasks, including audio event classification, keyword spotting, and speaker identification. We further conduct comprehensive ablation studies, including evaluations of different pretraining strategies. The proposed ASiT framework significantly boosts the performance on all tasks and sets a new state-of-the-art performance in five audio and speech classification tasks, outperforming recent methods, including the approaches that use additional datasets for pretraining.",
  "full_text": "1\nASiT: Local-Global Audio Spectrogram vIsion\nTransformer for Event Classification\nSara Atito, Member, IEEE, Muhammad Awais, Member, IEEE, Wenwu Wang, Senior Member, IEEE,\nMark D Plumbley, Fellow, IEEE, Josef Kittler, Life Member, IEEE,\nAbstract—Transformers, which were originally developed for\nnatural language processing, have recently generated significant\ninterest in the computer vision and audio communities due to\ntheir flexibility in learning long-range relationships. Constrained\nby the data hungry nature of transformers and the limited\namount of labelled data, most transformer-based models for\naudio tasks are finetuned from ImageNet pretrained models,\ndespite the huge gap between the domain of natural images\nand audio. This has motivated the research in self-supervised\npretraining of audio transformers, which reduces the dependency\non large amounts of labeled data and focuses on extracting\nconcise representations of audio spectrograms. In this paper, we\npropose Local-Global Audio Spectrogram vIsion Transformer,\nnamely ASiT, a novel self-supervised learning framework that\ncaptures local and global contextual information by employing\ngroup masked model learning and self-distillation. We evaluate\nour pretrained models on both audio and speech classification\ntasks, including audio event classification, keyword spotting,\nand speaker identification. We further conduct comprehensive\nablation studies, including evaluations of different pretraining\nstrategies. The proposed ASiT framework significantly boosts\nthe performance on all tasks and sets a new state-of-the-\nart performance in five audio and speech classification tasks,\noutperforming recent methods, including the approaches that use\nadditional datasets for pretraining.\nIndex Terms—Self-supervised Learning, Vision Transformers,\nAudio Spectrogram, Group Masked Model Learning, Audio\nClassification.\nI. I NTRODUCTION\nT\nHROUGHOUT the history of pattern recognition and\nmachine learning, there are numerous examples where\nthe learning methodology developed for one dimensional\ntime varying signals or data have been shown to be equally\nrelevant to two dimensional signal domains and vice versa.\nThe instances include the use of factor analysis in automatic\nspeech recognition, proposed to deal with different speech\nenvironments back in 2000 [1], successfully extended to 2D\nfor coping with different poses in face recognition in [2]. More\nrecent is the application of a new deep neural network (DNN)\narchitecture, the transformer [3], proposed for natural language\nprocessing, to the 2D domain of image processing [4]. In this\npaper, we investigate the reverse task, namely the applicability\nof techniques developed for 2D image analysis, to a 1D signal\ndomain. In particular, we consider the relevance of transformer\nbased image classification approaches to the problem of audio\nclassification.\nCentre for Vision, Speech and Signal Processing (CVSSP) and Surrey\nInstitute for People-Centred AI, University of Surrey, Guildford, UK.\nAlthough audio is a 1D signal, working with its spectral\nrepresentation partly bridges the gap between the 1D and 2D\ndomains. However, the frequency/time nature of a spectrogram\ndiffers significantly from a conventional image, where the\nstatistical properties (pixel relationships) of the signal in any\narbitrary direction and its physical meaning are the same. This\ncontrasts with the spectrogram, where the image axes represent\ndifferent physical phenomena, namely, time and frequency.\nMoreover, in a spectrogram, the classes are superimposed,\nwhereas in a scene image containing several objects, the\nclasses are adjacent to each other. These differences are\nsignificant enough to raise doubts about the direct applicability\nof the techniques developed for image analysis to the audio\nclassification problem. These doubts are addressed in the case\nof convolutional neural networks (CNNs) [5, 6]. However,\nthe applicability of the attention-based model is still an open\nquestion, with a few works [7, 8, 9, 10] validating the use of\nvision transformers (ViT) for audio.\nWe shall confine our study to ViTs, which are gaining\nrapid popularity. ViTs are very data hungry [4, 11], and\ntheir training is computationally costly. This often necessitates\nusing existing pretrained models for developing new appli-\ncations, spanning different user domains. The most popular\nare models pretrained on the ImageNet [12], which are the\nmodels of choice, irrespective of their relevance to the applica-\ntion concerned. However, ImageNet pretraining is clearly not\nideal for decision making, e.g. in medical domain [13]. This\ndata hungry nature of ViTs has recently been alleviated by\ngroup masked model learning (GMML) based self-supervised\nlearning (SSL) methods, introduced in SiT [11]. At its core,\nGMML is using the principle of denoising/masked autoen-\ncoder [14]. GMML randomly masks various regions within\nthe input image, potentially covering groups of connected\npatches and leaves groups of connected patches unmasked.\nThe goal of learning is to recover missing information in\nthe masked regions from visible information (context). Con-\ntrastive learning [15, 16, 17, 18, 19, 20] is another line of\nSSL approaches that aim to maximise the similarity between\ntwo augmented views of the same image and maximise the\ndisagreement between different images. These approaches\nuse various techniques, e.g. large batch size, memory banks,\nasymmetric projection heads, etc, to boost the performance\nand more importantly to avoid representation collapse.\nThanks to the recent advance in the SSL approaches, the\nself-supervised pretraining of DNNs, without using labelled\ndata, for the first time, outperformed supervised pretraining\nof DNNs in multiple computer vision downstream tasks like\narXiv:2211.13189v2  [cs.SD]  10 Mar 2024\n2\nclassification, segmentation etc. This enabled the use of do-\nmain specific pretraining of DNNs for high performance on\nseveral downstream tasks without using labelled data.\nThe key questions addressed in this paper are: i) can\nmasked image modelling successfully be applied to audio data,\ngiven the inherent differences between a 2D image and a\nspectrogram, and ii) how should the pretraining methodology\nbe adapted to gain from SSL as much as possible. The first\nquestion has already been partially answered in earlier studies\n[7, 8, 9, 10, 21] and we confirm here that SSL can be\napplied effectively to the audio classification tasks. However,\na straightforward application of SSL is not enough. The use\nof masked autoencoder does not guarantee that sufficient\ndiscriminatory information is retained by the pretrained model\n[9] as further discussed in Section II.\nWhile reconstruction is an essential ingredient, in order to\npreserve discriminatory information, an SSL method should\npromote similarity learning. To this end, we propose a novel\ntransformer learning framework, constituted by a teacher-\nstudent architecture, which is trained using a distillation\napproach guided by a contrastive loss, and by reconstruc-\ntion error produced by an integrated reconstruction head.\nWe argue that the typical global similarity learning using\na classification token is insufficient to capture the relevant\ninformation, and demonstrate that the proposed local similarity\nlearning mechanism is also crucial to successful pretraining for\nmultilabel audio classification tasks. More specifically, we pro-\npose a contrastive-loss-based learning method by distillation,\nusing the alignment of local features derived for the original\nand masked spectrograms as a measure of similarity. The\nmasked spectrogram reconstruction and the combination of\nglobal/local similarity learning produces effective pretraining\nfor downstream audio data processing tasks.\nWe evaluate the proposed SSL pretraining method on sev-\neral benchmarking audio and speech classification datasets.\nWe demonstrate the ability of the proposed method to learn\nthe underlying properties of audio data using pretraining and\nfinetuning purely on audio datasets, without resorting to aux-\niliary image data sets. In fact, we show that the performance\nachieved by training exclusively on audio datasets delivers the\nbest results, which exceed the state-of-the-art (SOTA) by a\nsignificant margin. In summary, our contributions include:\n• A local-global audio spectrogram vision transformer\n(ASiT) for enhanced audio representation.\n• A novel self-supervised pretraining method for ASiT,\nwhich combines masked spectrogram reconstruction with\nlocal-global similarity learning using distillation.\n• Extensive evaluation of ASiT on benchmarking datasets,\nwith the results that define a new state of the art\nperformance, demonstrating the merits of the proposed\nmethodology.\nII. R ELATED WORKS\nUnsupervised learning offers advantages in improving the\naudio representation by leveraging large scale data without\nlabels [22]. SSL aims to learn audio representations with pairs\nof similar inputs derived from unlabelled data, by exploiting\ntemporal consistency [15] or data augmentation [23, 24]. After\nits wide adoption in natural language processing (NLP) and\ncomputer vision, SSL is attracting increasing interest from\naudio community. In early works, SSL has been applied\nmainly to speech [25, 26, 27, 28]. For example, Speech2Vec\n[25], which is inspired by Word2Vec [29], learns word embed-\nding from acoustic features of speech, such as mel-frequency\ncepstral coefficients, using an RNN based encoder-decoder\ntrained with skipgram or continuous bag-of-words approaches.\nSSL has been recently used to learn general purpose audio\nrepresentations. For example, Audio2Vec [30] learns a rep-\nresentation via reconstructing a spectrogram slice in terms of\nits preceding and following slices, using CNN based backbone\nand mel-spectrogram as input. In CLAR [31] and COLA [23],\nwhich are built on the visual SSL method SimCLR [18], the\nnetwork is trained to maximize the representations of different\nviews in latent space and reduce the reconstruction loss of\nthe original waveform or spectrogram input, with different\naugmentations, such as random size cropping, Gaussian noise\naddition, and mix-back where the incoming patch is mixed\nwith a background patch. These methods often assume that\naudio segments in temporal neighborhood have more similar\nrepresentations, while those far away in time have more\ndifferent representations. Nevertheless, this is not the case for\nrepetitive sounds such as music and car sirens, where distant\nsegments can have similar representations.\nDifferent from the above methods, where training is per-\nformed by contrast between positive and negative samples, the\ndistillation-based approaches such as BYOL-A [32] and ATST\n[33] learn general audio representations by minimizing the dif-\nference (e.g. in mean squared error) between the embeddings\nof the same input with the contrasts obtained via data augmen-\ntation. In this case, no negative samples are required for learn-\ning the representation. For example, in BYOL-A [32], BYOL\n[20] is applied to audio by learning the similarity between the\ntwo views with one learned from a randomly cropped single\nsegment and the other from its augmented version obtained\nwith e.g. mixup and random resized crop (RRC). Different\nfrom BYOL-A, in ATST [33], the two views were learned from\ntwo randomly cropped and then augmented mel-spectrogram\nsegments with a teacher-student transformer architecture. The\ndistillation-based methods have been reported to achieve state-\nof-the-art performance. However, this method can potentially\nlead to trivial solutions, i.e. collapsed representations, which\nrequire careful design of network architectures or training\nalgorithms. For example, in Barlow Twins [34], the outputs of\nthe network twins which take augmented samples as inputs are\ncompared using cross-correlation, which promotes similarity\nbetween the learned embeddings from the network twins,\nwhile reducing the redundancies of the learned representations.\nInspired by the masked language modeling (MLM) [35]\nand masked image modeling (MIM) [11], several masked\nacoustic modeling (MAM) approaches such as SSAST [8] and\nMAE-AST [9] were introduced which learn to reconstruct the\nmasked time-frequency patches of an arbitrary shape from a\ngiven spectrogram. This offers potential advantages over the\nmethods such as Audio2Vec [30], which focus on temporal\nmodelling via reconstructing masked temporal spectrogram\n3\nraw audio\nCls\nShared\nWeights\nTeacher\nCentering\n+ Softmax EMA of\nStudent\nvision transformer\nCentering\n+ Softmax\nCls\nShared\nWeights\nStudent\nSoftmax\nvision transformer\nSoftmax\nGlobal Contrastive Learning\nLocal Contrastive Learning\nManipulation\nGMML\nReconstructed Fbank\nAugmentFbank + \nShared\nWeights\nLinear Projection\nto Fbank Space\nReconstruction Head\nFig. 1: The proposed self-supervised framework (ASiT). For a given 10-second audio spectrogram, two random augmented\nviews of 6-second each (clean spectrograms) are generated and fed to GMML based manipulation block to obtain the masked\nspectrograms. The clean and masked spectrograms are fed to the teacher and student networks, respectively. The recovery of\nthe transformed information from the non-transformed class-token and data-tokens indicates that the network has learnt the\nsemantics of the local as well as the global representation of the given audio and learnt useful inductive bias by learning local\nstatistical correlation in the spectrogram.\nframes. In SSAST [8], the transformer based AST model [7] is\npretrained on unlabelled audio from AudioSet and Librispeech\nwith joint generative and discriminative masked spectrogram\npatch modeling. SSAST uses a very high masking ratio, hence\nthe vast majority of self-attention is computed on masked\npatches. On the other hand, MAE-AST [9] and Audio-MAE\n[36] introduce the encoder-decoder architecture from masked\nautoencoder [9], where a large encoder is used to learn on\nunmasked input while a smaller decoder is used to reconstruct\nmasked input with encoder outputs and masked tokens.\nDespite the promising performance of the transformer mod-\nels in capturing local representations as in SSAST [8], MAE-\nAST [9], and global representation as in ATST [33], the\ntransformer architecture is limited in capturing optimal local\ncontextual information or global relational information from\naudio. Modelling both local and global information optimally,\nnevertheless, can be crucial for detecting transient acoustic\nevents such as gun shots as well as symphony. In this paper, we\npropose a novel self-supervised pretraining method for local\ncontext modelling via token similarity learning, and global\nrepresentation learning via contrastive instance classification.\nSuch combination allows the transformer model to capture\nfine-grained region/contextual dependencies as well as global\nrepresentation learnt from data, rather than injecting any\ninductive biases as in the case of CNNs. As a result, this novel\nframework significantly improves the quality of the learned\naudio representations.\nIII. M ETHODOLOGY\nIn this section, we introduce ASiT, a self-supervised frame-\nwork based on vision transformers for audio event representa-\ntions. Similar to [7], we employed log mel-spectrogram [37]\nas the input to the ViT instead of using the raw waveform\ndirectly. Spectrograms are used extensively in the fields of\naudio, speech, and music as they somewhat model human\nhearing perception and contain abundant low-level acoustic in-\nformation, which has similar characteristics to images, making\nthem more suitable for ViTs.\nIn Section III-A, we briefly summarise group masked model\nlearning [38] and the adoption of it to audio domain. Follow-\ning, in Section III-B, we explain our proposed method of self-\nlearning of data and class tokens conceptualisation, with the\nincorporation of knowledge distillation [39].\nA. Group Masked Model Learning\nMasked image modelling, which in principle is similar to\nthe masked language modelling (MLM) used in BERT [35],\nhas been first proposed in SiT [11] and employed in several\nrecent vision [40, 41, 42] and audio [8, 9, 10] works.\nThe main idea of GMML is to mask various regions within\nthe input image, potentially covering a group of connected\npatches representing a “significant” part of a given visual\ninput and recover them by learning a model. The underlying\nhypothesis is that, by recovering the masked parts from the\nunmasked parts, based on the context from the whole visual\nfield, the network will implicitly learn the notion of visual\nintegrity. Intuitively the network will only be able to recover\nthe missing information if it learns the characteristic properties\nof visual stimuli corresponding to specific actions impacting\non the visual input. This way of masking strategy proved ef-\nficiency in image domain, especially when dealing with small\ndatasets. It compels the model to incorporate an inductive bias,\na critical component often absent in transformer models.\nThere are several ways to mask the various regions in\nthe spectrogram. For example, replacing the randomly se-\nlected regions with zeros, noise, or with patches from other\n4\nspectrograms. Note that the masked regions are manipulated\nrandomly, i.e. the regions with audio or speech and the regions\nwith noise or silence are manipulated with equal probabil-\nity. Our hypothesis is that each pixel in a given log mel-\nspectrogram has a semantic meaning associated with it and\nthe network is required to learn to reconstruct the information\ncorresponding to both dominant (speech and audio) as well\nas non-dominant (noise and silence) concepts with equal\nimportance, which equips the network in generalizing well\nfor unseen data.\nAs for the masking strategy, the time and frequency are not\ntreated differently during masking, i.e. we apply the random\nmasking without any prior to allow the model to learn both\nthe temporal and frequency structure of the data.\nFor spectrogram reconstruction, we propose to use the\nvision transformer as a group masked autoencoder, i.e. visual\ntransformer autoencoder with group masked model learning.\nBy analogy to autoencoders, our network is trained to recon-\nstruct the input spectrogram through the output tokens of the\ntransformer.\nA vision transformer receives as input a sequence of patches\nobtained by tokenising the input spectrogram x ∈ RT×F into\nn flattened 2D patches of size p × p pixels, where T and F\nare the time and frequency resolution of the input spectrogram\nand n is the total number of patches. The n patches are then\nflattened and fed to a linear projection to obtain n tokens,\nwhich are then passed to the vision transformer. It’s important\nto note that in the context of vision transformers, a patch refers\nto a small, square segment of the input image/spectrogram.\nAfter the initial linear embedding stage in a ViT, each patch\nembedding is referred to as a token.\nTo pretrain transformers as group masked autoencoder,\nthe GMML manipulated spectrogram ˆ xis first obtained by\nrandomly replacing a significant part of the spectrogram with\nzeros or with patches from another spectrogram. Note that\nunlike in natural language processing, partial transformation\nof a patch is possible. Please refer to the ablation studies in\nSection V-B for details.\nThe reconstructed spectrogram ¯ xis obtained by passing\nthe masked spectrogram ˆ xto the transformer encoder E(·)\n(the vision transformer in the student block in Figure 1) and\nfeeding the output to a light decoder D(·) (the reconstruction\nhead block in Figure 1). The decoder consists of 3 fully\nconnected layers; the first two with 2048 neurons and GeLU\n[43] non-linearity each, and the last bottleneck layer with\n256 neurons, followed by a transposed convolution to return\nback to the spectrogram space. After the pretraining stage,\nthe light decoder D(·) is dropped and only the encoder\nE(·) is used for the downstream task. The importance of\nusing the representation before the nonlinear projection, i.e.\ndecoder head, is due to loss of information induced by the\nreconstruction loss. Particularly, the decoder head generally\nlearns task-specific features, i.e. spectrogram reconstruction,\nwhich might rescind information that may be useful for the\ndownstream task.\nFor the spectrogram reconstruction task, we use the ℓ1-\nloss between the original and the reconstructed spectrogram\nas shown in Equation 1. We compute the loss only on the\nmasked pixels, similar to [35, 38].\nL(W)recons =\nNX\nk\n\n\nTX\ni\nFX\nj\nMk\ni,j × |xk\ni,j − ¯xk\ni,j|\n\n, (1)\nMk\ni,j =\n(\n1, if xi,j in spectrogram k is manipulated\n0, otherwise. (2)\nwhere W denotes the parameters to be learned during training,\nN is the batch size, and M is a binary matrix with 1 indicating\nthe manipulated pixels.\nB. Local and Global Pseudo-Label Learning\nGroup masked model learning is the key component of ASiT\nas it implicitly learns the notion of visual integrity. This notion\nof visual integrity can be further enhanced by using pseudo\nlabels that can be generated automatically, based on some at-\ntributes of the data. The term “pseudo” in this context refers to\nautomatically generated labels derived from a teacher model’s\noutput during knowledge distillation. The idea of assigning a\nlocal and global pseudo labels for the spectrogram is inspired\nfrom DINO [44] framework. DINO focuses on assigning one\nglobal representation for the whole input. Unlike DINO, we\npropose to use self knowledge distillation to learn a pseudo\nlabel for each token in the spectrogram, as well as a global\npseudo label for the whole spectrogram. Our hypothesis is that\nmodelling only one representation for the whole spectrogram\ncan lead to sub-optimal representation learning. Thus, we\ninvestigate the role of learning the appropriate representation\nfor each of the data tokens through knowledge distillation.\nIn knowledge distillation, a student network sθ(·) is trained\nto match the output of a given teacher network tϕ(·), where\nθ and ϕ are the parameters of the student and the teacher\nnetworks, respectively. In this work, we employ the same\nnetwork architecture for both the student and the teacher,\ni.e. the teacher is a momentum encoder [45] of the student,\nwhere the parameters of the teacher network are updated from\nthe past iterations of the student network using exponential\nmoving average of the student weights with the following\nupdate rule: ϕ ← λϕ + (1 − λ)θ.\nIn our case, the network architecture of the student com-\nprises three components: A vision transformer backbone sb(·),\nfollowed by two projection heads attached to the output of\nthe transformer. The first projection head slcl(·) is for local\ncontrastive learning and the second projection head sgcl(·) is\nfor global contrastive learning. The local contrastive learning\nprojection head slcl(·) consists of 3 fully connected layers;\nthe first two with 2048 neurons and GeLU non-linearity\neach, and the last bottleneck layer with 256 neurons. The\noutput of the bottleneck layer is ℓ2 normalised and directly\nconnected to a weight-normalised fully connected layer with\nK = 8192 neurons. The global contrastive learning projection\nhead sgcl(·) has a similar design to slcl(·), where the output\nlayer has C = 8192 neurons. In our implementation, we\nshared the weights of the local and global contrastive learning\nprojection heads. However, we expect further improvements\nby tuning K and C hyper-parameters as well as the choice\n5\nof sharing, partially sharing, or not sharing the weights of the\ncontrastive learning projections heads.\nIn order to generate a local pseudo label for the input\nspectrogram x, the clean spectrogram is fed to the backbone\nof the teacher network tb(x) and the output of the data tokens\nare passed to the local contrastive learning projection head to\nobtain zt ∈ Rn×K. As for the student network, the GMML-\nbased manipulated spectrogram ˆ xis passed to the student\nnetwork to obtain zs ∈ Rn×K. The task is to match the output\nof the student network to the output of the teacher network,\nemploying the Kullback-Leibler divergence (KL).\nTraining the student to match the teacher output can easily\nlead to a trivial constant (i.e. collapsed) embeddings. To avoid\nthe model collapse, we adopted the centering and sharpen-\ning of the momentum teacher outputs introduced in [44].\nThe centering encourages the output to follow a uniform\ndistribution (similar to label smoothing), where a center C\nis added to the teacher distribution. Meanwhile, the softmax\nfunction’s temperature parameter effectively modulates the\nsharpness of the output probabilities. Applying both operations\nbalances their effects, which is sufficient to avoid a collapse\nin the presence of a momentum teacher. The centre vector is\nupdated using an exponential moving average over the teacher\noutput. The sharpening is obtained by using a low value for\nthe temperature τt in the teacher softmax normalisation. The\noutput probability distributions zt and zs from the teacher and\nthe student networks are obtained as follows:\np(i,j)\nt = exp(z(i,j)\nt /τt)\nPK\nk=1 exp(z(i,k)\nt /τt)\n, (3)\np(i,j)\ns = exp(z(i,j)\ns /τs)\nPK\nk=1 exp(z(i,k)\ns /τs)\n(4)\nwhere zt and zs are the class logits for the student and the\nteacher, pt(i, .) and ps(i, .) are the probabilities corresponding\nto the i − th token output by the teacher and the student, and\nτt and τs are the temperature parameters for the teacher and\nthe student, respectively.\nOur training objective is to match the output probability of\nthe student ps with that of teacher pt. We use the cross entropy\nmeasure for this task.\nL(θ)lcl =\nNX\nk\n\n\nnX\ni=1\nTk\ni ×\nKX\nj=1\n−pk\nt (i, j) logpk\ns(i, j)\n\n, (5)\nTk\ni =\n(\n1, if token i in spectrogram k is manipulated\n0, otherwise. (6)\nwhere T is a binary mask with 1 indicating the tokens\ncorresponding to the patches that are masked or partially\nmasked.\nThe network is trained to assign a class for each token in the\nspectrogram and differentiate it from other tokens. In essence,\nthe idea of local contrastive learning is similar to spectrogram\nreconstruction, but not in the pixel space, but rather in the\nclass space.\nAs for the global contrastive leaning, we follow the same\nprocedure as in the local contrastive learning, but proceeding\nonly on the class token to learn a global representation for the\nwhole spectrogram. For this particular task, two augmented\nviews are required, where the task is to match the output\nrepresentation of the class token of the student from the first\naugmented view, i.e. ps,1(0, .), to the output representation of\nthe class token of the teacher from the second augmented view,\ni.e. pt,2(0, .), and vice versa. The overall global contrastive\nleaning loss is as follows:\nL(θ)gcl =\nNX\nk\nCX\nj=1\n−pk\nt,1(0, j) logpk\ns,2(0, j))\n+\nCX\nj=1\n−pk\nt,2(0, j) logpk\ns,1(0, j))\n(7)\nC. Putting Together the ASiT Framework\nFor a given spectrogram, two augmented views are gener-\nated and passed to the teacher network. The masked version\nof the two views are obtained and passed to the student\nnetwork. The output of the data tokens from the backbone of\nthe student network is passed to both the reconstruction and\nlocal contrastive learning projection heads. The reconstruction\nloss L(θ)recons is calculated between the reconstructed spec-\ntrograms and the original spectrograms. In addition, the local\ncontrastive learning loss L(θ)lcl is calculated by matching the\noutput of the data tokens from the student network to the data\ntokens from the teacher network. Finally, the global contrastive\nlearning loss L(θ)gcl is calculated by matching the cross\nrepresentation of the two augmented views corresponding to\nthe class token from the student and teacher, respectively. The\noverall loss is then estimated as follows:\nL(θ) = α1L(θ)recons + α2L(θ)lcl + α3L(θ)gcl (8)\nwhere α1,...,3 are the scaling factors of our multi-task objective\nfunction, which are set to 1 for simplicity. We combine the\nlosses of the three pre-text tasks using simple averaging. We\nbelieve further improvements can be gained by optimising the\nweighted sum of the losses or by incorporating the uncertainty\nweighting approach [46].\nIV. E XPERIMENTS\nWe follow the common evaluation protocol to demonstrate\nthe generalisation of the learnt features of the proposed ASiT\nself-supervised learning approach by pretraining the model in\nan unsupervised fashion on AudioSet-2M dataset, followed by\nfinetuning on several downstream tasks, including audio event\nclassification, keyword spotting, and speaker identification\ntasks. We provide the details of the employed datasets and\nthe implementation details of the pretraining and finetuning\nstages in Section IV-A and IV-B, respectively.\nA. Datasets\nAudioSet (AS-2M, AS-20K) [47] is a collection of YouTube\nclips for 527 multi-label weakly annotated audio events. The\ntraining set has 2 subsets; a class-wise balanced dataset with\n6\naround 20K clips (AS-20K) and an unbalanced dataset com-\nbined with the balanced set, totaling approximately 2 million\nclips (AS-2M). The evaluation set has around 20K clips.\nEnvironmental Sound Classification (ESC-50) [48] is a\nsingle audio event classification dataset, consists of 2,000\n5-second environmental audio recordings organized into 50\nclasses. We follow the standard 5-fold cross-validation to\nevaluate our model on this dataset.\nSpeech Commands (SC-V2, SC-V1) [49] are two datasets\nfor the keyword spotting task. SC-V2 consists of 105,829\n1-second recordings of 35 common speech commands, split\ninto 84, 843, 9, 981, and 11, 005 samples for the training,\nvalidation, and test set, respectively. SC-V1 is similar to SC-\nV2, but only contains 10 classes of keywords, 1 silence class,\nand 1 unknown class that includes all the other 20 common\nspeech commands.\nVoxCeleb (SID) [50] is a dataset for speaker identification\ntask. It contains around 150K utterances of speech from 1,251\ndifferent speakers. The dataset is split into 138, 361, 6, 904,\nand 8, 251 samples for the training, validation, and test set,\nrespectively.\nB. Implementation Details\nPre-training: The backbone architecture of ASiT employs the\nbase (ViT-B) variant of ViT [4] and is pretrained on AudioSet-\n2M dataset for a fair comparison with the state-of-the-art.\nDuring the pretraining, simple data augmentation techniques\nare applied as we found that to learn low-level features\nas well as high-level semantic information, aggressive data\naugmentation hurts in the pretraining stage. Specifically, the\ngiven raw audio is pre-processed as a mono channel, then two\nrandom chunks of the given audio are cropped and resized\nto 6 seconds each. The two augmented audio waveforms are\nthen converted into a sequence of 128-dimensional log mel\nfilterbank (fbank) features computed with a 25ms Hanning\nwindow that shifts every 10ms. For a 6-second recording,\nthe resulting spectrogram is of 592 × 128 dimension. The\ntwo augmented spectrograms are then normalized to adjust\nthe statistical drifts caused by the aforementioned operations\nemploying the mean and the standard deviation of AudioSet\ndataset. We conducted the pre-training under a 16,000 Hz\nsampling rate. For the GMML masking, the spectrograms are\nrandomly masked by zeros with a total masking of 70% of\nthe spectrogram.\nAs for the optimisation of the self-supervised pretraining,\nthe model is trained using the Adam optimiser [51] with a\nmomentum of 0.9 and batch size of 40 spectrograms per GPU,\nusing 4 GeForce RTX 3090 GPUs. The weight decay follows a\ncosine schedule from 0.04 to 0.4, and the base learning rate is\n5e−4. The sharpening parameter of the teacher and the student\nare set to τt = 0 .07 and τs = 0 .1. The teacher is updated\nusing exponential moving average of the student weights with\nλ following a cosine schedule from 0.996 to 1 during training.\nWe present the performance of ASiT under two distinct\npretraining scenarios: First, ASiT undergoes pretraining for 40\nepochs on the AS-2M dataset starting from scratch. Secondly,\nthe model initially undergoes pretraining on the ImageNet-1K\ndataset for 400 epochs, followed by a subsequent pretraining\nphase on the AS-2M dataset for 40 epochs.\nFinetuning: For the downstream tasks, we discard the projec-\ntion heads and perform fine-tuning using the backbone of the\npretrained teacher network. Our finetuning process primarily\nrelies on the default code provided by DeiT [52]. All models\nare finetuned for 60 epochs utilising 4 GPUs with a batch\nsize of 16 per GPU, resulting in a total batch size of 64. We\nemploy a learning rate of 1e−3 for all datasets except AS2M,\nfor which we use 1e−4.\nSimilar to AST, we also apply frequency and time masking\n[53], random noise, and mixup [54] augmentation. However,\nit’s worth noting that, unlike the pre-training phase, we refrain\nfrom recovering the introduced noise. Refer to Table I for more\ndetails about the hyper-parameters utilised for each dataset.\nThe source code and pretrained weights have been made\npublicly available for access 1.\nTABLE I: Finetuning hyper-parameters. BCE: Binary Cross\nEntropy, CE: Cross Entropy, and F/T: Frequency/Time.\nConfiguration AS-2M AS-20K ESC-50 SC-V2 SC-V1 SID\nWeighted Sampling True False False False False False\nF/T Masking [53] 192/48 192/48 96/24 48/48 48/48 192/48\nMixup [54] 0.5 0.5 0 0 0 0\nLoss Function BCE BCE CE CE CE CE\n[Mean/Std] [-4.268/\n4.569]\n[-4.268/\n4.569]\n[-6.627/\n5.358]\n[-6.846/\n5.565]\n[-6.702/\n5.448]\n[-6.370/\n3.074]\nV. R ESULTS\nWe first discuss the performance and analysis of the pro-\nposed method compared to the SOTA. Further, in Section V-B,\nwe conduct several ablation studies to investigate the effect of\nthe individual components of the proposed approach.\nA. Transfer Learning\nIn Table II, we compare ASiT to the supervised and self-\nsupervised state-of-the-art approaches in audio event classifi-\ncation. For a fair comparison, we pretrained ASiT only on the\nAudioSet-2M dataset. As shown in Table II, pretrained ASiT\nobtains an impressive mAP of 48.0 and 38.6 on AudioSet-\n2M and AudioSet-20K, which is significantly outperforming\nsupervised learning with an improvement of +0.9 and +8.8\nmAP and the state-of-the-art with an improvement of +1.3\nand +1.6 mAP.\nFurther, ASiT achieves the best performance across different\naudio tasks compared to other approaches. Particularly, we\nobtain 95.3%, 98.9%, and 98.2% with an improvement of\n1.3%, 0.6%, and 0.6% on the ESC-50, speech command v1\nand v2 datasets, respectively.\nNote that in order to improve the coverage of speech data,\nSSAST [8] and MAE-AST [9] also used the Librispeech [56]\ndataset, which has around 1,000 hours of speech. Despite\npretraining ASiT only on the AudioSet-2M dataset, we out-\nperform the aforementioned methods on the SC-V1 and SC-\nV2 speech tasks with a large margin, as shown in Table II\nand obtained a notable performance enhancement on the SID\ndataset, demonstrating the generalisability of the proposed\nself-supervised framework.\n1https://github.com/ASiT\n7\nTABLE II: Comparison with state-of-the-art works on audio and speech classification tasks. Evaluation metrics are mean\nAverage Precision (mAP) for AS-2K and accuracy (%) for ESC-5, SC-V1, SC-V2, and SID. ‡ with additional supervised\ntraining on AS-2M.\nMethod Backbone Pretraining\nData\nTransfer Learning\nAS-2M AS-20K ESC-50 SC-V2 SC-V1 SID\nSupervised-learning-based methods\nPANNs [6] CNN – 43.1 27.8 83.3 – 61.8 –\nAST [7] ViT-B AS-2M 45.9 28.6 86.8 96.2 91.6 35.2\nPaSST [55] ViT-B AS-2M 47.1 – – – – –\nSelf-supervised-learning-based methods\nCOLA [23] CNN AS-2M – – – 98.1 95.5 37.7\nSSAST [8] ViT-B AS-2M – 29.0 84.7 97.8 94.8 57.1\nMaskSpec [10] ViT-B AS-2M 47.1 32.3 89.6 97.7 – –\nAudio-MAE (global) [36] ViT-B AS-2M 46.8 36.6 93.6 98.3 97.6 94.1\nAudio-MAE (local) [36] ViT-B AS-2M 47.3 37.0 94.1 98.3 96.9 94.8\nASiT (ours) [16kHz] ViT-B AS-2M 47.5 37.4 94.2 98.8 98.2 85.9\nASiT (ours) [16kHz] ViT-B INet → AS-2M 48.0 38.6 95.3 98.9 98.2 86.5\nSSL-based methods for reference not comparison as they are pretrained on additional speech dataset LS [56]\nSSAST [8] ViT-B AS-2M + LS – 31.0 88.8 98.0 96.0 64.3\nMAE-AST [9] ViT-B AS-2M + LS – 30.6 90.0 97.9 95.8 63.3\nSSL-based post arts. For reference not comparison.\nBEATS [21] ViT-B AS-2M 48.0 38.3 95.6 98.3 97.7 –\nBEATS ‡ [21] ViT-B AS-2M 48.6 38.9 98.1 98.1 98.1 –\nB. Ablations\nIn all of the ablation studies, ASiT is pretrained on AS-2M\nfrom scratch for only 10 epochs employing the small variant of\nvision transformers, i.e. ViT-S, as the backbone of the student\nand the teacher (unless mentioned otherwise). To assess the\nquality of the learnt representation, the pretrained models are\nfinetuned on AS-20K and the mean average precision on the\nvalidation set is reported.\nEffect of Different Recipes of ASiT. The aim of this ablation\nstudy is to investigate the effect of the individual elements of\nthe pretext learning, reported in Table III. First, we investigate\nthe effectiveness of pretraining transformers as an autoencoder\nto reconstruct the input audio without any sort of masking, i.e.\nD(E(x)) = x, where x is the input audio, E is the encoder\nwhich is ViT-S in our case, and D is a lightweight recon-\nstruction decoder. Expectedly, poor performance is obtained\nthat is slightly better than training the model from scratch.\nIndeed, this is attributed to the fact that without proper choice\nof constraints, autoencoders are capable of learning identity\nmapping, i.e. memorising the input without learning any useful\ndiscriminative features.\nTo regularise the transformer-based autoencoder, we incor-\nporated input masking along with spectrogram reconstruction,\ni.e. GMML, where the mAP jumped from 11.2 to 28.4 mAP.\nWe also investigated the effect of individually employing local\ncontrastive learning and global contrastive learning. We found\nthat the best individual pretext training method is GMML,\nfollowed by local contrastive learning. The least effective\nindividual task is global contrastive learning.\nFurther, we investigated the effect of the different combi-\nnation of the pre-text tasks. We found that using the spec-\ntrogram masking along with the reconstruction loss on its\nown as a means of self-supervision provided an effective\nstarting point for efficient downstream task finetuning. Further\nmarginal improvements can be made by extending the range\nof mechanisms for self-supervised pretraining.\nTABLE III: Effect of the different components of ASiT for\nself-supervised pretraining.\nSpectro-\ngram\nMasking\nSpectro-\ngram\nRecon-\nstruction\nLocal\nContrastive\nLearning\nGlobal\nContrastive\nLearning\nmAP\n(AS-20K)\n✗ ✓ ✗ ✗ 11.2\nIndividual Tasks\n✗ ✗ ✗ ✓ 17.8\n✓ ✗ ✓ ✗ 27.9\n✓ ✓ ✗ ✗ 28.4\nCombined Tasks\n✓ ✗ ✓ ✓ 28.8\n✓ ✓ ✗ ✓ 29.0\n✓ ✓ ✓ ✗ 29.4\n✓ ✓ ✓ ✓ 31.9\nEffect of Longer Self-supervised Pretraining. Figure 2\nshows the finetuning mAP when ASiT is pretrained directly on\nAS-2M and when preceded by pretraining using ImageNet-1K\ndataset followed by AS-2M pretraining. We found that longer\npretraining leads to systematic performance gain, where the\nmAP is steadily improving.\n5 10 20 30 40 60 80 100\nPre-training Epochs\n28\n30\n32\n34\n36mAP\nPre-training from AudioSet 2M\nPre-training from INet-1K -> AudioSet 2M\nFig. 2: Effect of longer pretraining.\n8\nEffect of the Percentage of Audio Masking. Figure 3 shows\nthe mAP when the models are pretrained with different mask-\ning percentages. We found that the optimal ratio is between\n60% to 80%. This behaviour was expected, as the masking\nencourages the network to learn from the unmasked regions\nsurrounding the groups of masked regions, in order to recover\nthe missing information.\n0 20 40 60 70 80 90\nMasking (%)\n10\n15\n20\n25\n30\n35mAP\nFig. 3: Impact of masking percentage in pretraining.\nEffect of Aligning the Masked Regions with the Patches.\nWe observed that the speed of convergence and the generalisa-\ntion of pretraining improve when the masking of regions is not\naligned to patch boundaries. In other words, patches can be\npartially masked. Particularly, after 10 epochs of pretraining\nASiT, the validation mAP on the AS-20K dataset is 29.2\nwhen the masked regions are aligned, and 31.9 when the\nmasked regions are not aligned with the patch boundaries. It is\nworth noting that with longer pretraining, the performance gap\nreduces between the two strategies. An example of masking\nwith aligning and not aligning the masked regions with the\npatch boundaries is shown in Figure 4.\n(a) Input\n (b)\n (c)\nFig. 4: Example of masking the spectrogram regions while\n(b) aligning and (c) not aligning the masked regions with the\npatch boundaries.\nEffect of the Model Size. In this ablation, we study the\nimpact of model size. Specifically, we employed the small\n(ViT-S) and base (ViT-B) variants of vision transformer. The\nsmall variant has 12 transformer blocks with an embedding\ndimension of 384 and 6 attention heads. The backbone has\naround 22M parameters in total. As for the base variant of\nvision transformer, it consists of 12 transformer blocks with\nan embedding dimension of 768 and 12 attention heads. The\nbackbone has around 85M parameters in total. For this ablation\nstudy, we conducted full training of both ViT-S and ViT-B\nmodels to provide a comprehensive comparison based on their\nrespective model sizes. As shown in Table IV, the performance\non the downstream task increases for the bigger model. This\nshows that ASiT is model size agnostic, demonstrating that it\ncan unlock the potential of models with a higher capacity.\nEffect of Weights Initialisation. Despite the significant\ndiscrepancies between the image and audio modalities, the\nTABLE IV: Mean Average Precision on AS-20K pretrained\nwith different weights’ initialization and different backbones.\nPretraining\nDataset\nBackbone\nViT-Small ViT-Base\nScratch 9.1 11.5\nImageNet-1K (INet) 29.7 32.3\nAudioSet-2M (AS-2M) 36.1 37.4\nINet → AS-2M 36.7 38.6\ncommon practice for audio classification is to initialize the\nmodels from ImageNet pretrained weights. Thus, it is pertinent\nto answer a very important question for the audio domain:\nDoes out-of-domain pretraining benefit audio representation\nlearning? For that, in Table IV, we compare the performance\nobtained with different weight initialisation strategies across\ndifferent models, including: (1) training from-scratch, i.e. no\npretraining, (2) ASiT pretrained only on ImageNet-1K, (3)\nASiT pretrained only on AS-2M, and (4) starting with ASiT\nmodel pretrained on ImageNet-1K, followed by pretraining\non the AS-2M dataset. As shown in Table IV, the poor per-\nformance, when training from scratch, is expected, especially\nwhen the training is on small datasets, due to the lack of\ninductive bias in transformers. We observed that ImageNet-\n1K pretraining alone is not sufficient, compared to pretraining\non the in-domain dataset.\nIt is important to highlight that initializing with self-\nsupervised learning pretrained weights from ImageNet-1K,\nfollowed by further pretraining on AS2M, results in enhanced\nperformance compared to pretraining exclusively on AS2M.\nNonetheless, we observed a decrease in performance when\nstarting with supervised pretrained weights from ImageNet-\n1K, followed by further pretraining on AS2M.\nVI. C ONCLUSION\nWe presented an efficient method of designing an audio\nsignal classifier based on self-supervised pretraining. In our\napproach audio is represented by a spectrogram and processed\nas an image by a vision transformer. We demonstrated that the\nGroup Masked Model Learning, that has earlier been shown to\nbe effective for image analysis, provides also a solid basis for\nself-supervised pretraining for audio classification. However,\nits core ingredient, namely a masked autoencoder trained using\nreconstruction loss, has to be bolstered by other information\nextraction mechanisms.\nWe proposed a novel transformer learning framework, con-\nstituted by a teacher-student architecture trained using a distil-\nlation approach. The training minimises a contrastive loss, and\nreconstruction error. As the typical global similarity learning\nusing a classification token proved insufficient to capture dis-\ncriminatory information, we proposed local similarity learning,\nusing the alignment of local features computed for the original\nand masked spectrogram as a measure of similarity.\nIn conclusion, the transformer pretraining, afforded by the\njoint use of masked spectrogram reconstruction and the com-\nbination of global/local similarity learning, was instrumental\nin obtaining very promising solutions for downstream audio\n9\nclassification tasks. The results of extensive evaluations of the\nproposed methodology on benchmarking datasets defined a\nnew state-of-the-art performance, demonstrating the merits of\nthe proposed methodology.\nREFERENCES\n[1] L. K. Saul and M. G. Rahim, “Maximum likelihood and minimum\nclassification error factor analysis for automatic speech recognition,”\nIEEE Transactions on Speech and Audio Processing , vol. 8, no. 2, pp.\n115–125, 2000. 1\n[2] S. J. D. Prince, J. H. Elder, J. Warrell, and F. M. Felisberti, “Tied\nfactor analysis for face recognition across large pose differences,” IEEE\nTransactions on Pattern Analysis & Machine Intelligence, vol. 30, no. 6,\npp. 970–984, 2008. 1\n[3] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, “Attention is all you need,” arXiv preprint\narXiv:1706.03762, 2017. 1\n[4] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n“An image is worth 16x16 words: Transformers for image recognition\nat scale,” arXiv preprint arXiv:2010.11929 , 2020. 1, 6\n[5] S. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C.\nMoore, M. Plakal, D. Platt, R. A. Saurous, B. Seybold et al. , “CNN\narchitectures for large-scale audio classification,” in ICASSP. IEEE,\n2017, pp. 131–135. 1\n[6] Q. Kong, Y . Cao, T. Iqbal, Y . Wang, W. Wang, and M. D. Plumbley,\n“PANNs: Large-scale pretrained audio neural networks for audio pattern\nrecognition,” IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, vol. 28, pp. 2880–2894, 2020. 1, 7\n[7] Y . Gong, Y .-A. Chung, and J. Glass, “AST: Audio spectrogram trans-\nformer,” arXiv preprint arXiv:2104.01778 , 2021. 1, 2, 3, 7\n[8] Y . Gong, C.-I. Lai, Y .-A. Chung, and J. Glass, “SSAST: Self-supervised\naudio spectrogram transformer,” in Proceedings of the AAAI , vol. 36,\nno. 10, 2022, pp. 10 699–10 709. 1, 2, 3, 6, 7\n[9] A. Baade, P. Peng, and D. Harwath, “MAE-AST: Masked autoencoding\naudio spectrogram transformer,” arXiv preprint arXiv:2203.16691, 2022.\n1, 2, 3, 6, 7\n[10] D. Chong, H. Wang, P. Zhou, and Q. Zeng, “Masked spectrogram\nprediction for self-supervised audio pre-training,” in ICASSP. IEEE,\n2023, pp. 1–5. 1, 2, 3, 7\n[11] S. Atito, M. Awais, and J. Kittler, “SiT: Self-supervised vision trans-\nformer,” arXiv preprint arXiv:2104.03602 , 2021. 1, 2, 3\n[12] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classification\nwith deep convolutional neural networks,” NeurIPS, vol. 25, pp. 1097–\n1105, 2012. 1\n[13] S. Atito, S. M. Anwar, M. Awais, and J. Kittler, “SB-SSL: Slice-based\nself-supervised transformers for knee abnormality classification from\nMRI,” in Workshop on Medical Image Learning with Limited and Noisy\nData, MICCAI. Springer, 2022, pp. 86–95. 1\n[14] P. Vincent, H. Larochelle, Y . Bengio, and P.-A. Manzagol, “Extracting\nand composing robust features with denoising autoencoders,” in Pro-\nceedings of the 25th ICML , 2008, pp. 1096–1103. 1\n[15] A. van den Oord, Y . Li, and O. Vinyals, “Representation learning with\ncontrastive predictive coding,” arXiv:1807.03748, 2018. 1, 2\n[16] Z. Wu, Y . Xiong, S. X. Yu, and D. Lin, “Unsupervised feature learning\nvia non-parametric instance discrimination,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition , 2018, pp.\n3733–3742. 1\n[17] R. D. Hjelm, A. Fedorov, S. Lavoie-Marchildon, K. Grewal, P. Bachman,\nA. Trischler, and Y . Bengio, “Learning deep representations by mutual\ninformation estimation and maximization,” in ICLR, 2019. 1\n[18] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework\nfor contrastive learning of visual representations,” in ICML. PMLR,\n2020, pp. 1597–1607. 1, 2\n[19] M. Patacchiola and A. Storkey, “Self-supervised relational reasoning for\nrepresentation learning,” in NeurIPS, vol. 33, 2020, pp. 4003–4014. 1\n[20] J.-B. Grill, F. Strub, F. Altch ´e, C. Tallec, P. Richemond, E. Buchatskaya,\nC. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar et al. ,\n“Bootstrap your own latent-a new approach to self-supervised learning,”\nNeurIPS, vol. 33, pp. 21 271–21 284, 2020. 1, 2\n[21] S. Chen, Y . Wu, C. Wang, S. Liu, D. Tompkins, Z. Chen, and F. Wei,\n“Beats: Audio pre-training with acoustic tokenizers,” arXiv preprint\narXiv:2212.09058, 2022. 2, 7\n[22] Y . Xu, Q. Huang, W. Wang, P. Foster, S. Sigtia, P. Jackson, and\nM. Plumbley, “Unsupervised feature learning based on deep models for\nenvironmental audio tagging,” IEEE/ACM Transactions on Audio Speech\nand Language Processing , vol. 25, pp. 1230–1241, 2017. 2\n[23] A. Saeed, D. Grangier, and N. Zeghidour, “Contrastive learning of\ngeneral-purpose audio representations,” in ICASSP. IEEE, 2021, pp.\n3875–3879. 2, 7\n[24] X. Liu, Q. Huang, X. Mei, T. Ko, H. L. Tang, M. Plumbley, and\nW. Wang, “Cl4ac: A contrastive loss for audio captioning,” Proceedings\nof the Detection and Classification of Acoustic Scenes and Events\nWorkshop, 2021. 2\n[25] Y .-A. Chung and J. Glass, “Speech2vec: A sequence-to-sequence frame-\nwork for learning word embeddings from speech,” arXiv preprint\narXiv:1803.08976, 2018. 2\n[26] A. Baevski, S. Schneider, and M. Auli, “vq-wav2vec: Self-\nsupervised learning of discrete speech representations,” arXiv preprint\narXiv:1910.05453, 2019. 2\n[27] A. Baevski, Y . Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0:\nA framework for self-supervised learning of speech representations,”\nNeurIPS, vol. 33, pp. 12 449–12 460, 2020. 2\n[28] M. BRiviere, A. Joulin, P.-E. Mazare, and D. Emmanuel, “Unsupervised\npretraining transfers well across languages,” ICASSP, 2020. 2\n[29] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient estimation of\nword representations in vector space,” arXiv preprint arXiv:1301.3781 ,\n2013. 2\n[30] M. Tagliasacchi, B. Gfeller, F. de Chaumont Quitry, and D. Roblek,\n“Pre-training audio representations with self-supervision,” IEEE Signal\nProcessing Letters, vol. 27, pp. 600–604, 2020. 2\n[31] H. Al-Tahan and Y . Mohsenzadeh, “CLAR: Contrastive learning of\nauditory representations,” in International Conference on Artificial In-\ntelligence and Statistics . PMLR, 2021, pp. 2530–2538. 2\n[32] D. Niizumi, D. Takeuchi, Y . Ohishi, N. Harada, and K. Kashino, “BYOL\nfor audio: Self-supervised learning for general-purpose audio represen-\ntation,” in 2021 International Joint Conference on Neural Networks\n(IJCNN). IEEE, 2021, pp. 1–8. 2\n[33] X. Li and X. Li, “ATST: Audio representation learning with teacher-\nstudent transformer,” arXiv preprint arXiv:2204.12076 , 2022. 2, 3\n[34] J. Zbontar, L. Jing, I. Misra, Y . LeCun, and S. Deny, “Barlow Twins:\nSelf-supervised learning via redundancy reduction,” in ICML. PMLR,\n2021, pp. 12 310–12 320. 2\n[35] J. D. M.-W. C. Kenton and L. K. Toutanova, “Bert: Pre-training of deep\nbidirectional transformers for language understanding,” in Proceedings\nof NAACL-HLT, vol. 1, 2019, p. 2. 2, 3, 4\n[36] P.-Y . Huang, H. Xu, J. Li, A. Baevski, M. Auli, W. Galuba, F. Metze, and\nC. Feichtenhofer, “Masked autoencoders that listen,” NeurIPS, vol. 35,\npp. 28 708–28 720, 2022. 3, 7\n[37] L. Rabiner and R. Schafer, Theory and applications of digital speech\nprocessing. Prentice Hall Press, 2010. 3\n[38] S. Atito, M. Awais, and J. Kittler, “GMML is all you need,” arXiv\npreprint arXiv:2205.14986, 2022. 3, 4\n[39] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural\nnetwork,” arXiv preprint arXiv:1503.02531 , 2015. 3\n[40] H. Bao, L. Dong, S. Piao, and F. Wei, “BEit: BERT pre-training of\nimage transformers,” in ICLR, 2022. 3\n[41] S. Atito, M. Awais, A. Farooq, Z. Feng, and J. Kittler, “MC-\nSSL0.0: Towards multi-concept self-supervised learning,” arXiv preprint\narXiv:2111.15340, 2021. 3\n[42] Z. Xie, Z. Zhang, Y . Cao, Y . Lin, J. Bao, Z. Yao, Q. Dai, and\nH. Hu, “SimMIM: A simple framework for masked image modeling,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2022, pp. 9653–9663. 3\n[43] D. Hendrycks and K. Gimpel, “Gaussian error linear units (GeLUs),”\narXiv preprint arXiv:1606.08415 , 2016. 4\n[44] M. Caron, H. Touvron, I. Misra, H. J ´egou, J. Mairal, P. Bojanowski, and\nA. Joulin, “Emerging properties in self-supervised vision transformers,”\nin Proceedings of the IEEE/CVF International Conference on Computer\nVision, 2021, pp. 9650–9660. 4, 5\n[45] K. He, H. Fan, Y . Wu, S. Xie, and R. Girshick, “Momentum contrast\nfor unsupervised visual representation learning,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2020, pp. 9729–9738. 4\n[46] A. Kendall, Y . Gal, and R. Cipolla, “Multi-task learning using uncer-\ntainty to weigh losses for scene geometry and semantics,” inProceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition ,\n2018, pp. 7482–7491. 5\n[47] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C.\nMoore, M. Plakal, and M. Ritter, “Audio set: An ontology and human-\nlabeled dataset for audio events,” in 2017 IEEE ICASSP . IEEE, 2017,\npp. 776–780. 5\n10\n[48] K. J. Piczak, “Esc: Dataset for environmental sound classification,” in\nProceedings of the 23rd ACM International Conference on Multimedia ,\n2015, pp. 1015–1018. 6\n[49] P. Warden, “Speech commands: A dataset for limited-vocabulary speech\nrecognition,” arXiv preprint arXiv:1804.03209 , 2018. 6\n[50] A. Nagrani, J. S. Chung, W. Xie, and A. Zisserman, “V oxceleb: Large-\nscale speaker verification in the wild,” Computer Speech & Language ,\nvol. 60, p. 101027, 2020. 6\n[51] I. Loshchilov and F. Hutter, “Fixing weight decay regularization in\nAdam,” ArXiv, vol. abs/1711.05101, 2017. 6\n[52] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and\nH. J ´egou, “Training data-efficient image transformers & distillation\nthrough attention,” in International conference on machine learning .\nPMLR, 2021, pp. 10 347–10 357. 6\n[53] D. Park, W. Chan, Y . Zhang, C.-C. Chiu, B. Zoph, E. Cubuk, and Q. Le,\n“SpecAugment: A simple data augmentation method for automatic\nspeech recognition,” arXiv preprint arXiv:1904.08779 , 2019. 6\n[54] Y . Tokozume, Y . Ushiku, and T. Harada, “Learning from between-class\nexamples for deep sound recognition,” arXiv preprint arXiv:1711.10282,\n2017. 6\n[55] K. Koutini, J. Schl ¨uter, H. Eghbal-Zadeh, and G. Widmer, “Effi-\ncient training of audio transformers with patchout,” arXiv preprint\narXiv:2110.05069, 2021. 7\n[56] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: an\nASR corpus based on public domain audio books,” in ICASSP. IEEE,\n2015, pp. 5206–5210. 6, 7\nSara Atito Ali Ahmed received her PhD in com-\nputer science from Sabanci University in 2021, with\na thesis on deep learning ensembles for image\nunderstanding. Currently, she is a Surrey Future\nFellow, holding a joint position between the Centre\nfor Vision, Speech and Signal Processing (CVSSP)\nand the Surrey Institute for People-Centred AI at the\nUniversity of Surrey, UK. In her current role, she is\nworking on advancing self-supervised representation\nlearning.\nMuhammad Awais received the B.Sc. degree in\nMathematics and Physics from the AJK University\nin 2001, B.Sc. degree in computer engineering from\nUET Taxila in 2005, M.Sc. in signal processing and\nmachine intelligence and PhD in machine learning\nfrom the University of Surrey in 2008 and 2011.\nHe is currently a senior lecturer in trustworthy and\nresponsible AI at Surrey Institute for People-Centred\nArtificial Intelligence and Centre for Vision, Speech\nand Signal Processing (CVSSP). His research in-\nterests include machine learning, deep learning,\nself(un,semi)-supervised learning, NLP, audio-visual analysis, medical image\nanalysis and computer vision.\nWenwu Wang was born in Anhui, China. He re-\nceived the B.Sc. degree in 1997, the M.E. degree in\n2000, and the Ph.D. degree in 2002, all from Harbin\nEngineering University, China. He then worked in\nKing’s College London, Cardiff University, Tao\nGroup Ltd. (now Antix Labs Ltd.), and Creative\nLabs, before joining University of Surrey, UK, in\nMay 2007, where he is currently a professor in\nsignal processing and machine learning, and a Co-\nDirector of the Machine Audition Lab within the\nCentre for Vision Speech and Signal Processing.\nHis current research interests include blind signal processing, sparse signal\nprocessing, audio-visual signal processing, machine learning and perception,\nmachine audition (listening), and statistical anomaly detection. He has (co)-\nauthored over 300 publications in these areas. He served for IEEE Transactions\non Signal Processing as an Associate Editor from 2014 to 2018, and as\nSenior Area Editor from 2019-2023. He is currently an Associate Editor for\nIEEE/ACM Transactions on Audio Speech and Language Processing. He is the\nelected Chair of IEEE Signal Processing Society Machine Learning for Signal\nProcessing and elected Vice Chair of EURASIP Technical Area Committee\non Audio Speech and Music Signal Processing.\nMark D Plumbley received the B.A.(Hons.) de-\ngree in electrical sciences and the Ph.D. degree\nin neural networks from University of Cambridge,\nCambridge, U.K., in 1984 and 1991, respectively. He\nis Professor of Signal Processing at the Centre for\nVision, Speech, and Signal Processing and is head\nof the School of Computer Science and Electronic\nEngineering at the University of Surrey, Guildford,\nU.K. He is an expert on the analysis and processing\nof audio, using a wide range of signal processing\nand machine learning methods. He led the first\ninternational data challenge on Detection and Classification of Acoustic\nScenes and Events, and currently holds an Engineering and Physical Sciences\nResearch Council Fellowship for “AI for Sound,” on the automatic recognition\nof everyday sounds. He is a Member of the IEEE Signal Processing Society\nTechnical Committee on Audio and Acoustic Signal Processing, and a Fellow\nof the IET and IEEE.\nJosef Kittler (M’74-LM’12) received the B.A.,\nPh.D., and D.Sc. degrees from the University of\nCambridge, in 1971, 1974, and 1991, respectively.\nHe is a distinguished Professor of Machine Intelli-\ngence at the Centre for Vision, Speech and Signal\nProcessing, University of Surrey, Guildford, U.K.\nHe conducts research in biometrics, video and im-\nage dataset retrieval, medical image analysis, and\ncognitive vision. He published the textbook Pattern\nRecognition: A Statistical Approach and over 700\nscientific papers. His publications have been cited\nmore than 68,000 times (Google Scholar).\nHe currently serves on the Editorial Boards of Pattern Recognition Letters\nand Pattern Recognition. He also served as a member of the Editorial Board of\nIEEE Transactions on Pattern Analysis and Machine Intelligence during 1982-\n1985. He served on the Governing Board of the International Association for\nPattern Recognition (IAPR) as one of the two British representatives during\nthe period 1982-2005, President of the IAPR during 1994-1996.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7856886386871338
    },
    {
      "name": "Spectrogram",
      "score": 0.7477543354034424
    },
    {
      "name": "Transformer",
      "score": 0.6758165955543518
    },
    {
      "name": "Speech recognition",
      "score": 0.5281355977058411
    },
    {
      "name": "Keyword spotting",
      "score": 0.4617614150047302
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45678719878196716
    },
    {
      "name": "Machine learning",
      "score": 0.3384169638156891
    },
    {
      "name": "Engineering",
      "score": 0.06982916593551636
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}