{
  "title": "Language Modelling with Pixels",
  "url": "https://openalex.org/W4285596748",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4227180423",
      "name": "Rust, Phillip",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4306427726",
      "name": "Lotz, Jonas F.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223187427",
      "name": "Bugliarello, Emanuele",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227938745",
      "name": "Salesky, Elizabeth",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202075950",
      "name": "de Lhoneux, Miryam",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223187431",
      "name": "Elliott, Desmond",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3164045210",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W3103727211",
    "https://openalex.org/W2552110825",
    "https://openalex.org/W2914736545",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W1977877104",
    "https://openalex.org/W2912473624",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4376626960",
    "https://openalex.org/W3037575273",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W3176765167",
    "https://openalex.org/W4226271314",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W4285211844",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W3155682407",
    "https://openalex.org/W2250314407",
    "https://openalex.org/W4230405732",
    "https://openalex.org/W4385572950",
    "https://openalex.org/W4288106555",
    "https://openalex.org/W4226395792",
    "https://openalex.org/W3135427360",
    "https://openalex.org/W4304697835",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2531207078",
    "https://openalex.org/W4386566638",
    "https://openalex.org/W2887710902",
    "https://openalex.org/W2462042662",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3213934211",
    "https://openalex.org/W3177252310",
    "https://openalex.org/W3035032094",
    "https://openalex.org/W4297817412",
    "https://openalex.org/W3170863103",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4221151248",
    "https://openalex.org/W3038047279",
    "https://openalex.org/W4386576703",
    "https://openalex.org/W2250618788",
    "https://openalex.org/W3032020872",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3105190698",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W2914094910",
    "https://openalex.org/W4230579319",
    "https://openalex.org/W3211686893",
    "https://openalex.org/W3173261519",
    "https://openalex.org/W3011279327",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2183866593",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3100198908",
    "https://openalex.org/W3207937903",
    "https://openalex.org/W2963263347",
    "https://openalex.org/W4225321655",
    "https://openalex.org/W4287370853",
    "https://openalex.org/W3173954987",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W4223589941",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3199276456",
    "https://openalex.org/W2250729567",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4281709991",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W3174418826",
    "https://openalex.org/W3008110149",
    "https://openalex.org/W3097217077",
    "https://openalex.org/W3103062353",
    "https://openalex.org/W4285051865",
    "https://openalex.org/W3115778530",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3154987757",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2002019621",
    "https://openalex.org/W2998353611",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W2282211856",
    "https://openalex.org/W4287110638",
    "https://openalex.org/W2805882637",
    "https://openalex.org/W3045462440",
    "https://openalex.org/W2250263931",
    "https://openalex.org/W4205717708",
    "https://openalex.org/W4221152489",
    "https://openalex.org/W3018647120",
    "https://openalex.org/W2947415936",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2735966564",
    "https://openalex.org/W2805981949",
    "https://openalex.org/W1663984431",
    "https://openalex.org/W3095771422"
  ],
  "abstract": "Language models are defined over a finite set of inputs, which creates a vocabulary bottleneck when we attempt to scale the number of supported languages. Tackling this bottleneck results in a trade-off between what can be represented in the embedding matrix and computational issues in the output layer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which suffers from neither of these issues. PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels. PIXEL is trained to reconstruct the pixels of masked patches instead of predicting a distribution over tokens. We pretrain the 86M parameter PIXEL model on the same English data as BERT and evaluate on syntactic and semantic tasks in typologically diverse languages, including various non-Latin scripts. We find that PIXEL substantially outperforms BERT on syntactic and semantic processing tasks on scripts that are not found in the pretraining data, but PIXEL is slightly weaker than BERT when working with Latin scripts. Furthermore, we find that PIXEL is more robust than BERT to orthographic attacks and linguistic code-switching, further confirming the benefits of modelling language with pixels.",
  "full_text": "Published as a conference paper at ICLR 2023\nLANGUAGE MODELLING WITH PIXELS\nPhillip Rust1 Jonas F. Lotz1,2 Emanuele Bugliarello1\nElizabeth Salesky3 Miryam de Lhoneux5 Desmond Elliott1,6\n1University of Copenhagen 2ROCKWOOL Foundation Research Unit\n3Johns Hopkins University 5KU Leuven 6Pioneer Centre for AI\np.rust@di.ku.dk\nABSTRACT\nLanguage models are deﬁned over a ﬁnite set of inputs, which creates a vocab-\nulary bottleneck when we attempt to scale the number of supported languages.\nTackling this bottleneck results in a trade-off between what can be represented in\nthe embedding matrix and computational issues in the output layer. This paper in-\ntroduces PIXEL , the Pixel-based Encoder of Language, which suffers from neither\nof these issues. PIXEL is a pretrained language model that renders text as images,\nmaking it possible to transfer representations across languages based on ortho-\ngraphic similarity or the co-activation of pixels. PIXEL is trained to reconstruct\nthe pixels of masked patches instead of predicting a distribution over tokens.1 We\npretrain the 86M parameter PIXEL model on the same English data as BERT and\nevaluate on syntactic and semantic tasks in typologically diverse languages, in-\ncluding various non-Latin scripts. We ﬁnd that PIXEL substantially outperforms\nBERT on syntactic and semantic processing tasks on scripts that are not found in\nthe pretraining data, but PIXEL is slightly weaker than BERT when working with\nLatin scripts. Furthermore, we ﬁnd that PIXEL is more robust than BERT to ortho-\ngraphic attacks and linguistic code-switching, further conﬁrming the beneﬁts of\nmodelling language with pixels.\n1 I NTRODUCTION\nNatural language processing has rapidly progressed in recent years due to a combination of self-\nsupervised representation learning, i.e. pretrained language models (PLMs) likeBERT (Devlin et al.,\n2019), GPT-3 (Brown et al., 2020), and XLM-R (Conneau et al., 2020); large unlabelled datasets;\nsuch as C4 (Raffel et al., 2020), The Pile (Gao et al., 2020); and large-scale computing power\n(Hirschberg & Manning, 2015). Despite this progress, these models only cover a fraction of the\nworld’s languages, with large inequalities in performance (Pires et al., 2019; Lauscher et al., 2020),\nand the majority of languages are falling behind English (Joshi et al., 2020b; Bugliarello et al.,\n2022). Even within English, these models struggle when tasked with processing noisy inputs (Sun\net al., 2020; Eger & Benz, 2020). In this paper, we show how to effectively support thousands of\nwritten languages in a single model while being robust to variations caused by character-level noise.\nLanguage models typically support a ﬁnite vocabulary of categorical inputs, e.g. characters, sub-\nwords or even words, and much effort has been devoted to vocabulary construction (Wan, 2022).\nOn one end of the spectrum, a vocabulary over words has three problems: (i) it is not possible to\nencode out-of-vocabulary words because they lack an entry in a closed vocabulary, e.g. “doxing”,\n(ii) there are too many parameters in the word embedding layer, and relatedly, (iii) the normalising\nconstant for the softmax activation in the output layer is too expensive to compute. On the other end\nof the spectrum, vocabularies over bytes or characters are much smaller, which leads to increased\nsequence lengths (Keren et al., 2022). In practice, most current models operate over inputs smaller\nthan words but larger than characters: subword units (Sennrich et al., 2016; Kudo, 2018). Subwords\nprevent the problem of extremely large embedding and output layers, and support open vocabulary\nprocessing. While this is a practical solution in a monolingual context and for some languages like\nEnglish, dealing with many languages with a variety of scripts will either result in a very large vo-\ncabulary or a trade-off over what is represented within a ﬁxed number of subwords (see §5). Taken\n1See Appendix A for reconstructions of this abstract.\n1\narXiv:2207.06991v2  [cs.CL]  26 Apr 2023\nPublished as a conference paper at ICLR 2023\nsoftmax\nRender Text as Image1\nProjection + Position Embedding2 ⊕\nEncoder\nMy cool cat ᓚᘏᗢ sits in a beautiful box full of black beans.\nCLS Embedding3 CLS \nCLS \nCLS \nMLP\nTrue: 0.999 / False: 0.001\nMy cool cat ᓚᘏᗢ sits in a beautiful box \nfull of black beans.\nRender Text as Image1\nMy cat ᓚᘏᗢ enjoys eating warm oatmeal for \nlunch and dinner.\nMy cat ᓚᘏᗢ enjoys eating warm oatmeal for lunch and dinner.\nProjection + Position Embedding2 ⊕\nCLS Embedding & Span Mask m patches 3 CLS \nEncoder\nMy cat ᓚᘏᗢ enjoys eating warm oatmeal for lunch and dinner.\nCLS \nCLS \nDecoder\n(a) PIXEL pretraining\nsoftmax\nRender Text as Image1\nProjection + Position Embedding2 ⊕\nEncoder\nMy cool cat ᓚᘏᗢ sits in a beautiful box full of black beans.\nCLS Embedding3 CLS \nCLS \nCLS \nMLP\nTrue: 0.999 / False: 0.001\nMy cool cat ᓚᘏᗢ sits in a beautiful box \nfull of black beans. (b) PIXEL ﬁnetuning\nFigure 1: Overview of PIXEL ’s architecture. Following He et al. (2022), we use a masked autoen-\ncoder with a ViT architecture and a lightweight decoder for pretraining (left). At ﬁnetuning time\n(right), the decoder is replaced by a task-speciﬁc classiﬁcation head that sits on top of the encoder.\ntogether, given a language model with a ﬁnite vocabulary, there is a bottleneck in two locations: at\nthe level of the encoding of the inputs and at the level of estimating the probability distribution over\nthe vocabulary. We call this thevocabulary bottleneck. A language model that can handle thousands\nof languages needs to deal with this problem.\nWe propose to rethink language modelling as a visual recognition task, removing the need for a ﬁnite\nvocabulary. Our proposal is inspired by Salesky et al. (2021), who showed how to train a machine\ntranslation model with “visual text representations” in the encoder instead of subwords. Our Pixel-\nbased Encoder of Language (PIXEL ) is built on the Masked Autoencoding Visual Transformer (ViT-\nMAE; He et al., 2022). ViT-MAE is a Transformer-based encoder-decoder trained to reconstruct the\npixels in masked image patches. PIXEL does not have a vocabulary embedding layer; instead, text\nis rendered as a sequence of ﬁxed-sized patches, which are processed using a Vision Transformer\nencoder (Dosovitskiy et al., 2021). PIXEL also does not have an expensive output layer when it\nreconstructs the pixels of the masked patches. In effect, PIXEL provides a solution to the vocabulary\nbottleneck without needing the prohibitively long sequences of character-based models.\nPIXEL is pretrained on the same data asBERT , given our computational resources. This means that it\nhas encountered only ∼0.05% non-English text (Blevins & Zettlemoyer, 2022).2 We evaluatePIXEL\non a range of syntactic and semantic tasks in 32 typologically diverse languages across 14 scripts,\nshowing that it can rapidly adapt to new languages and unseen scripts. PIXEL is also evaluated\non its ability to handle noisy text caused by orthographic attacks, where pixel-based encoding is a\nclear improvement over subword-based vocabularies. In lexical code-switching experiments,PIXEL\nperforms on-par with BERT and sometimes outperforms the multilingually pretrained MBERT.\nPIXEL is a new type of language model that can theoretically support any language that can be\ntypeset by a modern computer. We make the implementation, the pretrained model including inter-\nmediate training checkpoints, and the ﬁne-tuned models freely available for the community.3\n2 A PPROACH\nThe Pixel-based Encoder of Language, PIXEL , consists of three major components: a text renderer,\nwhich draws text as an image; an encoder, which encodes the unmasked regions of the image; and a\ndecoder, which reconstructs the masked regions at the pixel level. Figure 1 provides an illustration.\n2.1 T EXT RENDERER\nThe key component ofPIXEL is a text renderer that takes one or more pieces of text and renders them\nonto a blank RGB image x ∈RH×W×C. We set height H = 16and width W = 8464and choose\n2We do not claim that a language model designed to support thousands of languages should be pretrained\nonly on English text. We expect that pretraining on an appropriate choice of another language or multilingually\nmay provide more remarkable results. PIXEL represents an initial effort at smaller scale.\n3https://github.com/xplip/pixel\n2\nPublished as a conference paper at ICLR 2023\nFigure 2: Illustrative examples of our rendered text. PIXEL natively supports most writing systems,\ncolour emoji (a), and complex text layouts such as right-to-left writing and ligatures (b). Black\npatches serve as separators and end-of-sequence markers. Blank patches to the right of the end-\nof-sequence marker are treated as sequence padding. For word-level tasks, horizontal spacing can\nbe added between words (c) so that every patch can be assigned to exactly one word (dotted lines\nindicate patch boundaries for demonstration).\nC = 3RGB input channels, which is equivalent to a square colour image with a 368 ×368 resolu-\ntion and corresponds to a sequence of 529 image patches of size 16 ×16 pixels.4 Figure 2 shows\nexamples of text inputs rendered by the text renderer. The renderer supports (a) colour emoji and\nhieroglyphs scripts, (b) left-to-right and right-to-left writing systems, and (c) text that requires liga-\ntures. Analogous to BERT , a sequence can either contain a single paragraph of text or a text pair; we\nuse black 16 ×16 patches to serve as separators and end-of-sequence (EOS) markers. Blank (white)\npatches after the end-of-sequence marker are treated as padding byPIXEL , where no attention scores\nor losses are computed. Sequences longer than the maximum length are either truncated or split into\nmultiple sequences. Further technical details about the renderer are provided in Appendix D.\n2.2 A RCHITECTURE\nPIXEL -base is a 112M parameter ViT-MAE architecture (He et al., 2022) with a 12-layer ViT en-\ncoder (Dosovitskiy et al., 2021) and an 8-layer Transformer decoder (Vaswani et al., 2017). The\nencoder has 86M parameters and the decoder has 26M parameters, respectively. The 8-layer de-\ncoder is not used for downstream tasks. We give an overview of the architecture below, with more\ndetails in Appendix E. We did not train larger PIXEL variants for lack of computational resources.\nPatch Embeddings The images produced by the text renderer (§2.1) are patch-wise linearly pro-\njected to obtain a sequence of patch embeddings with a 16 ×16 pixel resolution, to which ﬁxed\nsinusoidal position embeddings are added.5\nAlgorithm 1 PIXEL Span Masking\nInput: #Image patches N, masking ratio R,\nmaximum masked span length S, span length\ncumulative weights W = {w1, . . . , wS}\nOutput: Masked patches M\nM←∅\nrepeat\ns ←randchoice({1, . . . , S}, W)\nl ←randint(0, max(0, N−s))\nr ←l + s\nif M∩{l −s, . . . , l−1}= ∅and\nM∩{r + 1, . . . , r+ s}= ∅then\nM←M∪{ l, . . . , r}\nend if\nuntil |M|> R·N return M\nPatch Span Masking Instead of the random masking proce-\ndure used in ViT-MAE or block-wise masking in BEiT (Bao\net al., 2022), PIXEL uses span masking with a 25% masking\nratio as outlined in Algorithm 1, which masks spans of up to\nS = 6consecutive image patches with a dynamic number of\nunmasked patches left between them. The idea behind the span\nmasking approach, inspired by T5 (Raffel et al., 2020) and\nSpanBERT (Joshi et al., 2020a), is that it masks more meaning-\nful units of text (full words or phrases) than random masking\nwhere the model more often has to ﬁll in (parts of) individual\ncharacters, thereby encouraging PIXEL to model a higher level\nof abstraction. In practice, span masking was slightly more\neffective than random masking in early prototypes of PIXEL .\nThis effect may be less noticeable at higher masking ratios (such as the 75% used in ViT-MAE),\nwhen random masking would more often masks consecutive patches. We found 25% masking ratio\nto work well for PIXEL -base, which is in line with recent ﬁndings for BERT -type models of simi-\nlar size (Wettig et al., 2022). We mask spans of s ∈{1,2,3,4}patches in length, each with 20%\nprobability, and spans of s∈{5,6}patches with 10% probability each, so E(s) = 3.1.\n4We chose a sequence length of 529 so that the memory requirements at maximum length are approx. equal\nto those of BERT . Forward and backward passes of the transformer layers at equal length are also equally fast.\n5This is a fast operation that does not require the large text embedding layer found in subword-based models,\nsaving parameters which could in theory be re-allocated to the self-attention stack. We refer to Xue et al. (2022)\nfor a discussion regarding beneﬁts and drawbacks of re-allocation of embedding layer weights.\n3\nPublished as a conference paper at ICLR 2023\nEncoder Following ViT-MAE (He et al., 2022), the PIXEL encoder only processes unmasked\npatches (i.e., ≈396 “visible” patches at 25% masking) rather than on a sequence including mask\ntokens, which not only reduces memory requirements and increases training speed, but also has\nthe advantage of not creating a mismatch between pretraining and ﬁnetuning. This mismatch would\noccur when training the encoder with inserted mask tokens because they are not inserted during ﬁne-\ntuning (He et al., 2022). We also prepend the specialCLS embedding to the unmasked patches.6 The\nresulting CLS and unmasked patches are processed by a 12-layer Transformer encoder to produce a\nsequence of encoder output representations.\nDecoder The PIXEL decoder ﬁrst projects the encoder outputs into the same space as the decoder\nmodel hidden size. It then inserts learnable mask embeddings at the masked positions; these are\nwhat PIXEL tries to reconstruct at the pixel level. Fixed sinusoidal position embeddings (Vaswani\net al., 2017) are added to inject order information. After processing this sequence via 8 Transformer\nlayers, a linear projection yields patch logits. Note that the decoder does not have to compute\nan expensive softmax over a subword vocabulary and circumvents the question of whether to tie\nthe subword embedding weights. PIXEL is trained with a normalised mean squared error (MSE)\npixel reconstruction loss measuring the discrepancy between normalised target image patches and\nreconstructed patches. This loss is only computed for masked, non-blank (text) patches.\n2.3 P RETRAINING\nPIXEL -base is pretrained on a rendered version of the English Wikipedia and the Bookcorpus (Zhu\net al., 2015), which is roughly equivalent to the BERT pretraining data.7 For better compute efﬁ-\nciency, we concatenate paragraphs until the maximum sequence length is reached, albeit not across\ndocument and book boundaries. Wikipedia has 2B words rendered into 11.4M examples and the\nBookcorpus has 1.1B words rendered into 5.4M examples; in total ∼3.1B words (BERT used 3.3B)\nrendered into 16.8M examples. 8 PIXEL is pretrained for 1M steps with batch size 256 (i.e. ∼16\nepochs) using the AdamW optimizer (Kingma & Ba, 2015; Loshchilov & Hutter, 2019) with a\nlinear warmup over the ﬁrst 50k steps to a peak learning rate of 1.5e−4 and a cosine decay to a\nminimum learning rate of 1e−5. Pretraining took 8 days on 8×40GB Nvidia A100 GPUs. We show\nthe loss curve and additional pretraining details in Appendix E. We stored PIXEL checkpoints every\n10k steps and make them available alongside the fully trained model on the HuggingFace Hub (Wolf\net al., 2020), which we hope will be useful to analyze training dynamics of PIXEL models (Sellam\net al., 2022). Figure 5 in Appendix B shows, for three unseen examples, how PIXEL learns to model\nlanguage over the course of pretraining.\n2.4 F INETUNING\nPIXEL can be ﬁnetuned for downstream NLP tasks in a similar fashion to BERT -like encoders by\nsimply replacing the PIXEL decoder with a suitable classiﬁcation head. By truncating or interpolat-\ning the sinusoidal position embeddings, we can ﬁnetune with sequences shorter or longer than 529\npatches, respectively. The latter, in particular, is common in computer vision applications to ﬁnetune\non higher resolution images (Touvron et al., 2019; Kolesnikov et al., 2020; Dosovitskiy et al., 2021;\nHe et al., 2022). For most common NLP tasks, we can typically ﬁnetune with sequences shorter\nthan 529 to accelerate training while retaining performance. To demonstrate that PIXEL supports a\nvariety of downstream tasks, we conduct ﬁnetuning experiments in four settings as follows:\nWord Classiﬁcation For word-level tasks like part-of-speech (POS) tagging and named entity\nrecognition (NER), we render each word at the start of a new image patch so that we can create a\nbijective mapping between words and patches (see Figure 2 for an example).9 To ﬁnetune PIXEL on\nthese images, we add a linear classiﬁer with dropout. We assign the label of a word only to its ﬁrst\ncorresponding image patch and compute a cross-entropy loss with softmax.\nDependency Parsing For dependency parsing, we render text as above but obtain word-level rep-\nresentations by mean pooling over all corresponding image patches of a word and employ a biafﬁne\nparsing head (Dozat & Manning, 2017), following the implementation from Glavaš & Vuli´c (2021).\n6In pretraining, no loss is computed for the CLS embedding but it can be used for ﬁnetuning.\n7We use a similar Wikipedia dump Devlin et al. (2019) used for BERT (February 1, 2018) and a slightly\nnewer version of the Bookcorpus available at https://huggingface.co/datasets/bookcorpusopen.\n8This rendering is quite compact; see Appendix D.\n9This particular formulation assumes that word boundaries are available. We note that subword-based and\ncharacter-based models also make this assumption. For further discussion on the implications, see Appendix F.\n4\nPublished as a conference paper at ICLR 2023\nSequence Classiﬁcation For sequence-level tasks, e.g. in GLUE (Wang et al., 2018), we render\ntext as in pretraining. For sentence-pair tasks like natural language inference (NLI) we separate the\nsentences with a black patch. We ﬁnetune with different strategies, including training a classiﬁer\non top of (1) the CLS embedding, (2) the mean-pooled or max-pooled representations of all patches,\n(3) a multi-head attention block. Although we did not notice signiﬁcant performance differences\nbetween them in our experiments, we mainly used option (1), which is exactly the same as inBERT ,\nand (2), which has been shown to work well for image classiﬁcation (Liang et al., 2022).\nExtractive Question Answering (QA) For extractive QA datasets like SQuAD (Rajpurkar et al.,\n2016), we render the question and context like in sequence-pair tasks above and, same as Devlin et al.\n(2019), use a sliding window approach to extract answers for examples exceeding the maximum\nsequence length. We use a linear classiﬁer to predict the start and end patches of the span containing\nthe answer. Appendix D explains how we obtain the mapping between characters and rendered text.\n3 E XPERIMENTS\nWe ﬁnetune PIXEL on common NLP tasks and evaluate its syntactic and semantic processing capa-\nbilities in English, as well as its adaptability to unseen languages. Table 8 (Appendix F) describes the\nlanguages used in these experiments, and our language and data selection is also motivated below.\n3.1 T ASKS AND LANGUAGES\nSyntactic Tasks We evaluate PIXEL on part-of-speech (POS) tagging and dependency parsing\nusing data from Universal Dependencies v2.10 treebanks (Nivre et al., 2020; Zeman et al., 2022)\nfor a set of typologically diverse languages that captures a large variety of unseen scripts 10: Arabic\n(ARA ), Coptic ( COP ), English ( ENG ), Hindi ( HIN ), Japanese ( JPN ), Korean ( KOR), Tamil ( TAM),\nVietnamese (VIE ), Chinese ( ZHO ).11 We compare how well PIXEL transfers to these languages\ncompared to BERT . Note that BERT does not support all of these writing systems. However, both\nmodels have been trained on the same data. This comparison allows us to gauge the extent to which\nPIXEL can overcome the script barrier and vocabulary bottleneck of subword-based models.\nSemantic Tasks We evaluate both monolingual (ENG ) and cross-lingual word-level understanding\non MasakhaNER (Adelani et al., 2021), a named entity recognition (NER) benchmark for 10 African\nlanguages (AMH , HAU, IBO , KIN , LUG , LUO , PCM , SWA, WOL , YOR), which also includes a copy\nof the ConLL-2003 dataset ( ENG ; Tjong Kim Sang & De Meulder, 2003). For monolingual ENG\nsentence-level understanding we rely on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al.,\n2016). Finally, we evaluate cross-lingual sentence-level understanding on TyDiQA-GoldP (Clark\net al., 2020) in the in-language multitask setting where we train on the combined gold data in all 9\ntarget languages (ARA , BEN , ENG , FIN, IND , KOR, RUS, SWA, TEL ) at once, and on two additional\nlarger monolingual extractive question answering (QA) corpora: KorQuAD 1.0 ( KOR; Lim et al.,\n2019) and JaQuAD (JPN ; So et al., 2022).\n3.2 B ASELINES AND FINETUNING PROTOCOLS\nWe compare results to BERT -base which is trained on the same data.12 We do not compare to newer\nmonolingual English models like ROBERTA (Liu et al., 2019), T5 (Raffel et al., 2020) or DEBERTA\n(He et al., 2021b;a) because these models have been pretrained longer on much larger corpora. 13\nLikewise, we do not compare against models trained on massively multilingual corpora. However,\nto contextualise the performance ofPIXEL in cross-lingual settings, we report results forMBERT and,\nif results are available, for CANINE (Clark et al., 2022). For BERT , we use the standard ﬁnetuning\nprotocols used by Devlin et al. (2019) and the same biafﬁne classiﬁer for parsing as for PIXEL . We\nlist ﬁnetuning details for all tasks in Appendix F.\n3.3 R ESULTS\nSyntactic Tasks We present results for POS tagging and dependency parsing in Table 1. While\nBERT is slightly better than PIXEL in the monolingual setting ( ENG ), PIXEL clearly outperforms\n10By unseen, we mean not present in the pretraining data.\n11Table 10 in Appendix F gives an overview of the treebanks we use.\n12We use BERT weights from https://huggingface.co/bert-base-cased.\n13We do not intend to claim state-of-the-art performance, but to demonstrate that PIXEL can overcome the\nvocabulary bottleneck and to provide a starting point for further research on pixel-based encoding of language.\n5\nPublished as a conference paper at ICLR 2023\n|θ| ENG ARA COP HIN JPN KOR TAM VIE ZHO\nPOS Tagging (Accuracy)\nBERT 110M 97.2 95.4 26.5 86.4 87.9 60.0 45.4 84.5 58.6\nPIXEL 86M 96.7 95.7 96.0 96.3 97.2 94.2 81.0 85.7 92.8\nDependency Parsing (LAS)\nBERT 110M 90.6 77.7 13.0 75.9 73.8 30.2 15.2 49.4 28.8\nPIXEL 86M 88.7 77.3 83.5 89.2 90.7 78.5 52.6 50.5 73.7\n[UNK]% Fertility\nENG 0 1.2\nARA 1.8 3.7\nCOP 93.6 1.0\nHIN 32.6 2.7\nJPN 45.5 1.5\nKOR 84.7 1.0\nTAM 82.3 1.3\nVIE 4.5 2.5\nZHO 73.2 1.5\nTable 1: Results for PIXEL and BERT ﬁnetuned for POS tagging and dependency parsing on various\nUniversal Dependencies treebanks. We report test set results averaged over 5 runs each. |θ|denotes\nthe number of model parameters. The table on the right shows BERT ’s proportion of [UNK]s as\na measure of (inverse) vocabulary coverage and fertility (i.e., number of subwords per tokenized\nword; Ács, 2019; Rust et al., 2021) as a measure of over-segmentation in respective UD treebanks.\n#L |θ| ENG AMH HAU IBO KIN LUG LUO PCM SWA WOL YOR\nMBERT* 104 179M 92.2 0 87.3 85.3 72.6 79.3 73.5 86.4 87.5 62.2 80.0\nCANINE-C + n-gram* 104 167M 89.8 50.0 88.0 85.0 72.8 79.6 74.2 88.7 83.7 66.5 79.1\nCANINE-C* 104 127M 79.8 44.6 76.1 75.6 58.3 69.4 63.4 66.6 72.7 60.7 67.9\nBERT 1 110M 92.9 0 86.6 83.5 72.0 78.4 73.2 87.0 83.3 62.2 73.8\nPIXEL 1 86M 89.5 47.7 82.4 79.9 64.2 76.5 66.6 78.7 79.8 59.7 70.7\nTable 2: Results for PIXEL and BERT ﬁnetuned for NER on MasakhaNER. We report test set F1\nscores averaged over 5 runs each. BERT outperforms PIXEL in all of the languages that use Latin\nscript, whereas PIXEL does better on AMH , whose script is not covered by BERT ’s vocabulary. The\nperformance gap is smaller for languages heavier in diacritics, e.g. YOR. It is larger for languages\ncloser to English such as Naija Pidgin ( PCM ), an English-based creole. # L denotes the number of\npretraining languages and * indicates results taken from Clark et al. (2022) for additional context.\nBERT in the remaining languages. On the lower end, the accuracy gap in favor of PIXEL in ARA\nand VIE , both languages covered by BERT ’s vocabulary, is relatively small (∼1%). On the higher\nend, in COP , where BERT has an out-of-vocabulary ([UNK]) token ratio of 93%, the gap is ∼70% for\nboth tasks. There is a strong correlation14 between the proportion of [UNK]s (shown in Table 1 on the\nright) and the performance gap, which shows that PIXEL overcomes BERT ’s vocabulary bottleneck.\nThese results are further analysed in Appendix I.\nSemantic Tasks We present results for NER in Table 2, for GLUE in Table 3, for QA in Table 4.\nWe also conduct experiments on XNLI in thetranslate-train-all setting which we present in Table 16\nin Appendix I, for brevity. We ﬁnd that BERT consistently achieves higher performance than PIXEL\nin its pretraining language ENG . Likewise, it often outperforms on languages using the Latin writing\nsystem; for instance in NER where all languages besides AMH use Latin script, in QA for FIN, IND\nand SWA. Although BERT has more trainable parameters, this ﬁnding indicates that a PIXEL model\npretrained for the same number of steps as BERT is slightly worse at semantic tasks, and it may\nrequire longer pretraining or an additional inductive bias to close the performance gap. Similarly,\ncharacter-based models also tend to underperform subword-based models on NER (Keren et al.,\n2022), here seen by the CANINE -C results. Since the addition of n-gram embeddings improves the\nperformance of CANINE -C, likely due to boosting entity memorisation capabilities (Clark et al.,\n2022), we hypothesize that PIXEL may beneﬁt from equivalent enhancements.\nFor languages where BERT only partially covers the script, such as KOR, JPN and TEL in QA, PIXEL\nconsistently outperforms BERT , sometimes by large amounts (e.g. , +63 F1 points better on Ko-\nrQuAD). In the extreme case where BERT has no coverage of the script whatsoever, seen in NER\nfor AMH , BERT fails completely (0 F1) while PIXEL outperforms the larger, multilingually trained\nCANINE and performs competitively with its n-gram variant. In other words, PIXEL also overcomes\nthe vocabulary bottleneck of subword-based PLMs in semantics-driven tasks. Note that although\nBERT was trained on English, its vocabulary has a high coverage of the Arabic script, explaining its\ngood performance in ARA and URD .15\n14Pearson correlation r = 0.9, p <0.001 for POS tagging, r = 0.95, p <0.0001 for dependency parsing.\n6\nPublished as a conference paper at ICLR 2023\n|θ| MNLI-M/MM\n393k\nQQP\n364k\nQNLI\n105k\nSST-2\n67k\nCOLA\n8.6k\nSTS-B\n5.8k\nMRPC\n3.7k\nRTE\n2.5k\nWNLI\n635 AVG\nBERT 110M 84.0/ 84.2 87.6 91.0 92.6 60.3 88.8 90.2 69.5 51.8 80.0\nPIXEL 86M 78.1 / 78.9 84.5 87.8 89.6 38.4 81.1 88.2 60.5 53.8 74.1\nTable 3: Results for PIXEL and BERT ﬁnetuned on GLUE . We report validation set performance\naveraged over 5 runs. The metrics are F1 score for QQP and MRPC , Matthew’s correlation for\nCOLA , Spearman’sρfor STS -B, and accuracy for the remaining datasets. PIXEL achieves non-trivial\nperformance scores on GLUE , indicating pixel-based encoders can learn higher-level semantic tasks,\nbut performs worse overall than BERT , so it may require (a) more pretraining steps than subword-\ntokenized PLMs or (b) additional inductive bias to acquire the same level of monolingual abstraction.\n#L |θ| TyDiQA-GoldP SQuAD KorQuAD JaQuAD\nENG ARA BEN FIN IND KOR RUS SWA TEL AVG ENG KOR JPN\nMBERT 104 179M 75.6 78.1 74.7 75.5 84.3 64.8 74.9 83.1 81.6 77.1 88.6 90.0 76.4\nBERT 1 110M 68.5 58.0 43.2 58.3 67.1 12.4 53.2 71.3 48.2 51.5 88.2 14.9 28.8\nPIXEL 1 86M 59.6 57.3 36.3 57.1 63.6 26.1 50.5 65.9 61.7 52.3 81.4 78.0 34.1\nTable 4: Results for PIXEL and BERT ﬁnetuned on extractive QA datasets. We report validation\nset F1 scores averaged over 5 runs each. Average (AVG) scores for TyDiQA-GoldP exclude ENG as\ncustomary (Clark et al., 2020). WhileBERT clearly outperforms PIXEL in ENG , PIXEL is much better\nin KOR, TEL , and JPN —a consequence of the vocabulary bottleneck in BERT —thereby gaining an\nedge on average. In some languages, answer span extraction adversely affects results (see §3.3).\nWhile the same may apply to languages like BEN and RUS in QA, where one may otherwise expect\nPIXEL to outperform BERT , there is an external factor at play; in the standard QA task formulation\nused by BERT , answer spans are extracted by predicting start and end tokens. We adopt this pro-\ncedure in PIXEL for simplicity. However, an image patch will often overlap two words at variable\npositions, so the answer may actually start or end mid-patch. By only predicting on a full-patch\nlevel, and extracting the entire content of the patch, PIXEL will sometimes extract leading and trail-\ning characters that should not be part of the answer, which degrades the F1 score—even though the\nmodel may have correctly identiﬁed the span. Languages not using whitespace to delimit words are\nparticularly affected, which also explains why PIXEL is only slightly better than BERT in JPN .\nGenerally, and in particular when transferring to unseen scripts, we ﬁnd that PIXEL performs best\nwhen ﬁnetuning on larger corpora. An example of this behaviour can be seen in QA, where PIXEL\nperforms signiﬁcantly better on KorQuAD (60k examples) than the KOR subset of TyDi (1.6k ex-\namples). While large corpora may often not be available when dealing with unseen scripts, we hy-\npothesize that multilingual pretraining will alleviate the need for long ﬁnetuning, while potentially\nbeing even more conducive to positive transfer (Conneau et al., 2020; Chau et al., 2020; Pfeiffer\net al., 2021) by not being vocabulary-bottlenecked.\n4 R OBUSTNESS TO ORTHOGRAPHIC ATTACKS AND CODE -SWITCHING\nInformal text, commonly found on social media, often contains orthographic noise such as typos and\nother variations (Baldwin et al., 2015; van Esch et al., 2019; Caswell et al., 2020). Previous work has\ndemonstrated the vulnerability of pretrained language models to character-level adversarial attacks\nand noise (Sun et al., 2020; Eger & Benz, 2020), with text normalization typically required to main-\ntain performance (Pruthi et al., 2019; Keller et al., 2021). To evaluate PIXEL ’s robustness to textual\nnoise and variation and inspired by the robustness tests of Salesky et al. (2021), we experiment with\nthe Zeroé benchmark (Eger & Benz, 2020; Keller et al., 2021) which covers a variety of low-level\northographic attacks as illustrated in Table 13. We replace their version of visual attacks with the\nUnicode Technical Standard #39 set of visually-confusable characters. 16 We apply Zeroé attacks\nduring ﬁnetuning and evaluation of two English downstream tasks, POS tagging and NLI (Bowman\net al., 2015), where we expect models to rely on different levels of abstraction.\n15Arabic is lexically sparse (Antoun et al., 2020; Al-Sallab et al., 2017), so the characters can be covered in\nthe vocabulary. However, it is morphologically complex, which leads to over-segmentation, as the fertility of\n3.7 in Table 1 shows. This over-segmentation is not necessarily problematic in our selection of tasks (Keren\net al., 2022), e.g. due to the sliding window in QA, but can be a disadvantage in others (Rust et al., 2021).\n16https://util.unicode.org/UnicodeJsps/confusables.jsp\n7\nPublished as a conference paper at ICLR 2023\n(a) 0%, contradiction\n (b) 80%, contradiction\n (c) 80%, entailment\nFigure 3: Visual explanations of correct PIXEL predictions (for classes contradiction and entail-\nment) for NLI examples with 0% and 80% CONFUSABLE substitutions using method by Chefer\net al. (2021), providing qualitative evidence for PIXEL ’s robustness to character-level noise and the\ninterpretability of its predictions. Red heatmap regions represent high relevancy.\nFigures 8 and 9 in Appendix G compare PIXEL and BERT across three levels of token-level noise\nfor POS tagging and NLI. There is little impact on POS tagging performance with either model\nfrom most low-level attacks, with the exception of visually-confusable character substitutions\n(CONFUSABLE ); here PIXEL expectedly maintains performance above 92% as it generalizes across\northographic similarities but BERT drops to 38%. For NLI, both models are negatively affected, but\nPIXEL exhibits less degradation thanBERT with higher proportions of noise, with the impact varying\nacross the types of attacks which each affect subword tokenization differently. Figure 3 shows rel-\nevancy heatmaps (Chefer et al., 2021) for SNLI predictions made with and without CONFUSABLE\nsubstitutions. The heatmaps are similarly clear with and without noise, providing qualitative evi-\ndence that PIXEL is indeed robust to the noise. The illustrated robustness may be dependent upon\nﬁnetuning, however; we ﬁnd that PIXEL can struggle in zero-shot applications when text is rendered\ndifferently from observed during pretraining (see Appendix D on using different fonts). Future work\ncould explore the impact of data augmentation during pretraining on PIXEL ’s robustness and ability\nto transfer across scripts. Furthermore, it would be interesting to investigate how the choice of font\ninﬂuences the search space during reconstruction of masked patches (Bland et al., 2022).\nPOS Tagging Named Entity Recognition\nSPA-ENG HIN-ENG SPA-ENG HIN-ENG MSA-EA\nMBERT 97.1 86.3 64.0 72.6 65.4\nBERT 96.9 87.0 61.1 74.5 59.4\nPIXEL 96.8 88.2 61.0 73.0 63.7\nTable 5: Code-switching results on LINCE.\nIn addition to robustness to orthographic noise, deal-\ning with character-level substitutions is important for\neffectively modelling different morphological forms.\nThere are also many types of higher-level token,\nphrase or sequence-level variations such as code-\nswitching—when a speaker alternates between two\nor more languages in the same utterance, while be-\ning grammatically consistent in each language (Joshi, 1982)—or the lexical substitutions in social\nmedia text. We evaluate PIXEL on the LinCE benchmark (Aguilar et al., 2020), which includes core\ntasks and downstream applications for linguistic code-switching. PIXEL is ﬁne-tuned on POS Tag-\nging and NER in Spanish-English, Hindi-English and Modern Standard Arabic-Egyptian Arabic.\nTable 5 shows that PIXEL and BERT perform similarly on SPA-ENG tasks, with BERT outperforming\nPIXEL on NER for (romanised) HIN -ENG . On the other tasks, PIXEL performs better than BERT and\neven outperforms MBERT on HIN -ENG POS tagging. The gap between MBERT and PIXEL is larger\non Arabic scripts, which were extensively seen by MBERT during pretraining.\n5 R ELATED WORK\nThe question of vocabulary construction is an open problem in NLP, especially in a multilingual\ncontext.17 The most widely used language models, e.g. BERT, RoBERTa, T5, GPT-2inter alia, rely\non different tokenizers, such as WordPiece (Devlin et al., 2019), Byte-Pair Encoding (BPE; Sennrich\net al., 2016) and Unigram LM (Kudo, 2018). There is an established ecosystem around subword\ntokenizers, such as the SentencePiece (Kudo & Richardson, 2018) and HuggingFace Tokenizers.\nIn a monolingual context and for some languages like English, vocabularies of subwords are a good\ntradeoff between vocabularies of characters and vocabularies of words. When representing a large\nnumber of languages in multilingual PLMs like mBERT and XLM-R, adequately representing the\nvocabulary of each individual language would be computationally prohibitive. The tokenization then\nbecomes a bottleneck when trying to scale up to a large number of languages (Conneau et al., 2020;\nRust et al., 2021), which manifests itself in degraded cross-lingual performance to languages and\n17See Mielke et al. (2021) for a recent, comprehensive survey on open-vocabulary modeling and tokenization.\n8\nPublished as a conference paper at ICLR 2023\nlanguage families that are underrepresented in the data used for training multilingual PLMs. There\nare large inequalities in the performance of these models across typologically diverse languages (Wu\n& Dredze, 2020; Lauscher et al., 2020). This issue is further exacerbated by tokenizations out-of-\nthe-box not being compatible across languages (Maronikolakis et al., 2021). Language imbalance\nand poor character coverage in the vocabulary can also decrease downstream performance (Zhang\net al., 2022). To some extent, these problems can be attenuated through techniques such as subword\nmapping (Vernikos & Popescu-Belis, 2021), transliteration (Moosa et al., 2022), leveraging lexical\noverlap (Patil et al., 2022), vocabulary clustering and reallocation (Chung et al., 2020), continued\nor language-adaptive pretraining (Ebrahimi & Kann, 2021), adaptation via bilingual lexica (Wang\net al., 2022), and embedding matrix adaptation (Artetxe et al., 2020). However, these are post-hoc\nworkarounds to expand model vocabularies after training. They do not provide a direct solution to\nthe vocabulary bottleneck problem.\nSome subword-based algorithms can also produce undesirable segmentations for morphologically\nrich languages (Klein & Tsarfaty, 2020; Amrhein & Sennrich, 2021), so dedicated morphologically-\naware tokenizers have been developed (e.g. Smit et al. (2014)), but this process often requires expert-\nlevel knowledge and may only work for individual languages.\nDue to the limitations of subword vocabularies in multilingual language modelling, some works\nhave used vocabularies over characters (Lee et al., 2017; Ma et al., 2020, inter alia) or bytes (Wang\net al., 2020; Wei et al., 2021). These provide beneﬁts over purely subword-based models in terms\nof robustness and most of them are readily applicable in a multilingual context, 18 but they typi-\ncally come at the cost of increased sequence lengths or latency. Also, such models cannot exploit\northographic similarities between characters across and within scripts and do not account for the\nfact that meaning of language may be carried visually such as in writing systems that are (partially)\nlogographic like Chinese, in ancient hieroglyphs, or when using emoji.\nFinally, some works have developed pixel-based approaches. Broscheit (2018) embedded images\nof Chinese glyphs but still relied on a ﬁxed vocabulary. Wu et al. (2019) combined character-level\nimages and embeddings for a variety of Chinese tasks. Radford et al. (2021) trained a linear probe\nfor CLIP, which also incorporates a tokenizer, on a rendered version of SST-2 (Socher et al., 2013).\nOther works have trained pixel-based models that removed the need for a ﬁxed vocabulary: Sun et al.\n(2019) trained a convolutional sentiment classiﬁer on pixels. Mansimov et al. (2020) used images of\ntext for in-image MT. Salesky et al. (2021) employed a convolutional embedder for a Transformer-\nbased MT system with a subword-based decoder. Our method differs from these in that it provides\na general-purpose language encoder that completely removes the need for a vocabulary.\n6 C ONCLUSION\nThis paper introduced PIXEL , a pretrained language model that renders text as images, which allows\nit to represent any written language that can be typeset using its text renderer. PIXEL was pretrained\non the predominantly English Wikipedia and Bookcorpus datasets, and evaluated on part-of-speech\ntagging, dependency parsing, question answering, and language understanding tasks. The results\ndemonstrate that PIXEL readily transfers to unseen scripts, as shown by its performance on 14 scripts\nacross 32 languages. PIXEL currently lags behind BERT when processing languages with a Latin\nscript, including English; however, PIXEL is more robust than BERT against low-level orthographic\nattacks and performs competitively toBERT and MBERT on linguistic code-switching tasks. Overall,\nthese results show that pixel-based representations are a strong backbone for cross-lingual and cross-\nscript transfer learning. The limitations of this work are discussed in Appendix J.\nIn future work, we will investigate inductive biases and additional objectives that can better capture\nlong-range dependencies in PIXEL models. We hope that this will help overcome the limits of\nPIXEL in semantic processing. We also plan to pretrain PIXEL on multilingual text with a view\nto further improving its cross-script and cross-lingual abilities. This will also allow us to more\nfairly compare pixel-based models against larger subword-based and tokenization-free multilingual\nmodels. Finally, we will also develop new rendering and ﬁnetuning formulations that are better\ntailored to pixel-based models, e.g. for improving downstream question answering.\n18Character-aware models are not directly applicable to languages that do not use whitespace to delimit\nsentences (Tay et al., 2021), for example.\n9\nPublished as a conference paper at ICLR 2023\nACKNOWLEDGMENTS\nWe thank Ákos Kádár, Barbara Plank, and Kris Cao for their comments on an earlier draft. We also\nthank Davide Rigoni, Rita Ramos, Stella Frank, and members of the CoAStaL and LAMP groups for\ndiscussions. Miryam de Lhoneux is funded by the Swedish Research Council (grant 2020-00437).\nPhillip Rust is funded by the Novo Nordisk Foundation (grant NNF 20SA0066568). Jonas F. Lotz\nis funded by the ROCKWOOL Foundation (grant 1242). ⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆Emanuele Bugliarello is supported by\nfunding from the European Union’s Horizon 2020 research and innovation programme under the\nMarie Skłodowska-Curie grant agreement No 801199. Elizabeth Salesky is supported by the Apple\nScholars in AI/ML fellowship. Desmond Elliott is partially supported by the Innovation Foundation\n(grant 0176-00013B) and the Novo Nordisk Foundation (grant NNF 20SA0066568). This work was\nsupported by a research grant (VIL53122) from VILLUM FONDEN. The computing power was\ngenerously supported by EuroHPC grants 2010PA5869, 2021D02-068, and 2021D05-141, and with\nCloud TPUs from Google’s TPU Research Cloud (TRC).\nREFERENCES\nJudit Ács. Exploring BERT’s Vocabulary. Blog Post, 2019. URL http://juditacs.github.io/\n2019/02/19/bert-tokenization-stats.html.\nDavid Ifeoluwa Adelani, Jade Abbott, Graham Neubig, Daniel D’souza, Julia Kreutzer, Constan-\ntine Lignos, Chester Palen-Michel, Happy Buzaaba, Shruti Rijhwani, Sebastian Ruder, Stephen\nMayhew, Israel Abebe Azime, Shamsuddeen H. Muhammad, Chris Chinenye Emezue, Joyce\nNakatumba-Nabende, Perez Ogayo, Aremu Anuoluwapo, Catherine Gitau, Derguene Mbaye, Je-\nsujoba Alabi, Seid Muhie Yimam, Tajuddeen Rabiu Gwadabe, Ignatius Ezeani, Rubungo An-\ndre Niyongabo, Jonathan Mukiibi, Verrah Otiende, Iroro Orife, Davis David, Samba Ngom,\nTosin Adewumi, Paul Rayson, Mofetoluwa Adeyemi, Gerald Muriuki, Emmanuel Anebi, Chia-\nmaka Chukwuneke, Nkiruka Odu, Eric Peter Wairagala, Samuel Oyerinde, Clemencia Siro, To-\nbius Saul Bateesa, Temilola Oloyede, Yvonne Wambui, Victor Akinode, Deborah Nabagereka,\nMaurice Katusiime, Ayodele Awokoya, Mouhamadane MBOUP, Dibora Gebreyohannes, Henok\nTilaye, Kelechi Nwaike, Degaga Wolde, Abdoulaye Faye, Blessing Sibanda, Orevaoghene Ahia,\nBonaventure F. P. Dossou, Kelechi Ogueji, Thierno Ibrahima DIOP, Abdoulaye Diallo, Adewale\nAkinfaderin, Tendai Marengereke, and Salomey Osei. MasakhaNER: Named Entity Recog-\nnition for African Languages. Transactions of the Association for Computational Linguis-\ntics, 9:1116–1131, 10 2021. ISSN 2307-387X. doi: 10.1162/tacl_a_00416. URL https:\n//doi.org/10.1162/tacl_a_00416.\nGustavo Aguilar, Sudipta Kar, and Thamar Solorio. LinCE: A Centralized Benchmark for Linguis-\ntic Code-switching Evaluation. In Proceedings of The 12th Language Resources and Evaluation\nConference, pp. 1803–1813, Marseille, France, May 2020. European Language Resources Asso-\nciation. ISBN 979-10-95546-34-4. URL https://www.aclweb.org/anthology/2020.lrec-\n1.223.\nAhmad Al-Sallab, Ramy Baly, Hazem Hajj, Khaled Bashir Shaban, Wassim El-Hajj, and Gilbert\nBadaro. Aroma: A recursive deep learning model for opinion mining in arabic as a low resource\nlanguage. ACM Trans. Asian Low-Resour. Lang. Inf. Process., 16(4), jul 2017. ISSN 2375-4699.\ndoi: 10.1145/3086575. URL https://doi.org/10.1145/3086575.\nChantal Amrhein and Rico Sennrich. How suitable are subword segmentation strategies for\ntranslating non-concatenative morphology? In Findings of the Association for Computational\nLinguistics: EMNLP 2021 , pp. 689–705, Punta Cana, Dominican Republic, November 2021.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2021.ﬁndings-emnlp.60. URL\nhttps://aclanthology.org/2021.findings-emnlp.60.\nWissam Antoun, Fady Baly, and Hazem Hajj. AraBERT: Transformer-based model for Arabic\nlanguage understanding. In Proceedings of the 4th Workshop on Open-Source Arabic Corpora\nand Processing Tools, with a Shared Task on Offensive Language Detection, pp. 9–15, Marseille,\nFrance, May 2020. European Language Resource Association. ISBN 979-10-95546-51-1. URL\nhttps://aclanthology.org/2020.osact-1.2.\n10\nPublished as a conference paper at ICLR 2023\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th Annual Meeting of the Association for Com-\nputational Linguistics, pp. 4623–4637, Online, July 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.acl-main.421. URL https://aclanthology.org/2020.acl-\nmain.421.\nMasayuki Asahara, Hiroshi Kanayama, Takaaki Tanaka, Yusuke Miyao, Sumire Uematsu, Shinsuke\nMori, Yuji Matsumoto, Mai Omura, and Yugo Murawaki. Universal Dependencies version 2 for\nJapanese. In Proceedings of the Eleventh International Conference on Language Resources and\nEvaluation (LREC 2018), Miyazaki, Japan, May 2018. European Language Resources Associa-\ntion (ELRA). URL https://aclanthology.org/L18-1287.\nLei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv preprint,\n2016. URL http://arxiv.org/abs/1607.06450.\nTimothy Baldwin, Marie Catherine de Marneffe, Bo Han, Young-Bum Kim, Alan Ritter, and Wei\nXu. Shared tasks of the 2015 workshop on noisy user-generated text: Twitter lexical normal-\nization and named entity recognition. In Proceedings of the Workshop on Noisy User-generated\nText, pp. 126–135, Beijing, China, July 2015. Association for Computational Linguistics. doi:\n10.18653/v1/W15-4319. URL https://aclanthology.org/W15-4319.\nHangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT: BERT pre-training of image trans-\nformers. In International Conference on Learning Representations , 2022. URL https://\nopenreview.net/forum?id=p-BhZSz59o4.\nMaxwell Troy Bland, Anushya Iyer, and Kirill Levchenko. Story beyond the eye: Glyph positions\nbreak PDF text redaction. arXiv preprint, 2022. URL https://arxiv.org/abs/2206.02285.\nTerra Blevins and Luke Zettlemoyer. Language contamination explains the cross-lingual capabilities\nof english pretrained models. arXiv preprint, 2022. URL https://doi.org/10.48550/arXiv.\n2204.08110.\nTerra Blevins, Hila Gonen, and Luke Zettlemoyer. Analyzing the mono-and cross-lingual pretraining\ndynamics of multilingual language models. arXiv preprint, 2022. URL https://arxiv.org/\nabs/2205.11758.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large an-\nnotated corpus for learning natural language inference. In Proceedings of the 2015 Confer-\nence on Empirical Methods in Natural Language Processing , pp. 632–642, Lisbon, Portugal,\nSeptember 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1075. URL\nhttps://aclanthology.org/D15-1075.\nSamuel Broscheit. Learning distributional token representations from visual features. In Proceed-\nings of The Third Workshop on Representation Learning for NLP, pp. 187–194, Melbourne, Aus-\ntralia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-3025. URL\nhttps://aclanthology.org/W18-3025.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato,\nR. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Sys-\ntems, volume 33, pp. 1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.\nneurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\nEmanuele Bugliarello, Fangyu Liu, Jonas Pfeiffer, Siva Reddy, Desmond Elliott, Edoardo Maria\nPonti, and Ivan Vuli ´c. IGLUE: A benchmark for transfer learning across modalities, tasks, and\nlanguages. In Proceedings of the 39th International Conference on Machine Learning, Balitmore,\nMA, July 2022. PMLR. URL https://arxiv.org/abs/2201.11732.\n11\nPublished as a conference paper at ICLR 2023\nIsaac Caswell, Theresa Breiner, Daan van Esch, and Ankur Bapna. Language id in the wild: Un-\nexpected challenges on the path to a thousand-language web text corpus. In Proceedings of the\n28th International Conference on Computational Linguistics , pp. 6588–6608, Barcelona, Spain\n(Online), 2020. Association for Computational Linguistics. URL https://aclanthology.org/\n2020.coling-main.579.pdf.\nEthan C. Chau, Lucy H. Lin, and Noah A. Smith. Parsing with multilingual BERT, a small\ncorpus, and a small treebank. In Findings of the Association for Computational Linguistics:\nEMNLP 2020 , pp. 1324–1334, Online, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.ﬁndings-emnlp.118. URL https://aclanthology.org/2020.\nfindings-emnlp.118.\nHila Chefer, Shir Gur, and Lior Wolf. Generic attention-model explainability for inter-\npreting bi-modal and encoder-decoder transformers. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV) , pp. 397–406, October 2021.\nURL https://openaccess.thecvf.com/content/ICCV2021/papers/Chefer_Generic_\nAttention-Model_Explainability_for_Interpreting_Bi-Modal_and_Encoder-Decoder_\nTransformers_ICCV_2021_paper.pdf.\nJayeol Chun, Na-Rae Han, Jena D. Hwang, and Jinho D. Choi. Building Universal Dependency\ntreebanks in Korean. In Proceedings of the Eleventh International Conference on Language\nResources and Evaluation (LREC 2018) , Miyazaki, Japan, May 2018. European Language Re-\nsources Association (ELRA). URL https://aclanthology.org/L18-1347.\nHyung Won Chung, Dan Garrette, Kiat Chuan Tan, and Jason Riesa. Improving multilingual\nmodels with language-clustered vocabularies. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing (EMNLP) , pp. 4536–4546, Online, November\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.367. URL\nhttps://aclanthology.org/2020.emnlp-main.367.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev,\nand Jennimaria Palomaki. TyDi QA: A benchmark for information-seeking question answering in\ntypologically diverse languages. Transactions of the Association for Computational Linguistics ,\n8:454–470, 2020. doi: 10.1162/tacl_a_00317. URL https://aclanthology.org/2020.tacl-\n1.30.\nJonathan H. Clark, Dan Garrette, Iulia Turc, and John Wieting. Canine: Pre-training an efﬁcient\ntokenization-free encoder for language representation. Trans. Assoc. Comput. Linguistics , 10:\n73–91, 2022. doi: 10.1162/tacl\\_a\\_00448. URL https://doi.org/10.1162/tacl_a_00448.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger\nSchwenk, and Veselin Stoyanov. XNLI: Evaluating cross-lingual sentence representations. In\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp.\n2475–2485, Brussels, Belgium, October-November 2018. Association for Computational Lin-\nguistics. doi: 10.18653/v1/D18-1269. URL https://aclanthology.org/D18-1269.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek,\nFrancisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Un-\nsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics , pp. 8440–8451, Online, July 2020.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL https:\n//aclanthology.org/2020.acl-main.747.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June\n2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https:\n//aclanthology.org/N19-1423.\n12\nPublished as a conference paper at ICLR 2023\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-\nreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recogni-\ntion at scale. In International Conference on Learning Representations , 2021. URL https:\n//openreview.net/forum?id=YicbFdNTTy.\nTimothy Dozat and Christopher D. Manning. Deep biafﬁne attention for neural dependency\nparsing. In 5th International Conference on Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https:\n//openreview.net/forum?id=Hk95PK9le.\nAbteen Ebrahimi and Katharina Kann. How to adapt your pretrained multilingual model to 1600\nlanguages. In Proceedings of the 59th Annual Meeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1:\nLong Papers), pp. 4555–4567, Online, August 2021. Association for Computational Linguistics.\ndoi: 10.18653/v1/2021.acl-long.351. URL https://aclanthology.org/2021.acl-long.351.\nSteffen Eger and Yannik Benz. From hero to zéroe: A benchmark of low-level adversarial attacks.\nIn Proceedings of the 1st Conference of the Asia-Paciﬁc Chapter of the Association for Computa-\ntional Linguistics and the 10th International Joint Conference on Natural Language Processing,\npp. 786–803, Suzhou, China, December 2020. Association for Computational Linguistics. URL\nhttps://aclanthology.org/2020.aacl-main.79.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text\nfor language modeling. arXiv preprint, 2020. URL https://arxiv.org/abs/2101.00027.\nGoran Glavaš and Ivan Vuli´c. Is supervised syntactic parsing beneﬁcial for language understanding\ntasks? an empirical investigation. In Proceedings of the 16th Conference of the European Chapter\nof the Association for Computational Linguistics: Main Volume , pp. 3090–3104, Online, April\n2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.270. URL\nhttps://aclanthology.org/2021.eacl-main.270.\nJan Hajiˇc, Otakar Smrž, Petr Zemánek, Petr Pajas, Jan Šnaidauf, Emanuel Beška, Jakub Kracmar,\nand Kamila Hassanová. Prague arabic dependency treebank 1.0, 2009. URL https://ufal.\nmff.cuni.cz/padt/PADT_1.0/docs/index.html.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Gir-\nshick. Masked autoencoders are scalable vision learners. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 16000–16009,\n2022. URL https://openaccess.thecvf.com/content/CVPR2022/papers/He_Masked_\nAutoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.pdf.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electra-\nstyle pre-training with gradient-disentangled embedding sharing. arXiv preprint, 2021a. URL\nhttps://arxiv.org/abs/2111.09543.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced BERT\nwith disentangled attention. In International Conference on Learning Representations , 2021b.\nURL https://openreview.net/forum?id=XPZIaotutsD.\nDan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with gaussian\nerror linear units. arXiv preprint, 2016. URL http://arxiv.org/abs/1606.08415.\nJulia Hirschberg and Christopher D Manning. Advances in natural language processing. Science,\n349(6245):261–266, 2015. URL https://cs224d.stanford.edu/papers/advances.pdf.\nAravind K. Joshi. Processing of sentences with intra-sentential code-switching. In Coling 1982:\nProceedings of the Ninth International Conference on Computational Linguistics , 1982. URL\nhttps://aclanthology.org/C82-1023.\n13\nPublished as a conference paper at ICLR 2023\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. Span-\nBERT: Improving pre-training by representing and predicting spans. Transactions of the As-\nsociation for Computational Linguistics , 8:64–77, 2020a. doi: 10.1162/tacl_a_00300. URL\nhttps://aclanthology.org/2020.tacl-1.5.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. The state and\nfate of linguistic diversity and inclusion in the NLP world. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics , pp. 6282–6293, Online, July 2020b.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.560. URL https:\n//aclanthology.org/2020.acl-main.560.\nYannik Keller, Jan Mackensen, and Steffen Eger. BERT-defense: A probabilistic model based\non BERT to combat cognitively inspired orthographic adversarial attacks. In Findings of the\nAssociation for Computational Linguistics: ACL-IJCNLP 2021 , pp. 1616–1629, Online, August\n2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.ﬁndings-acl.141. URL\nhttps://aclanthology.org/2021.findings-acl.141.\nOmri Keren, Tal Avinari, Reut Tsarfaty, and Omer Levy. Breaking character: Are subwords good\nenough for mrls after all? arXiv preprint, 2022. URL https://arxiv.org/abs/2204.04748.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua\nBengio and Yann LeCun (eds.), Proceedings of the 3rd International Conference on Learning\nRepresentations (ICLR), San Diego, CA, USA, 2015. URLhttp://arxiv.org/abs/1412.6980.\nStav Klein and Reut Tsarfaty. Getting the ##life out of living: How adequate are word-pieces\nfor modelling complex morphology? In Proceedings of the 17th SIGMORPHON Workshop on\nComputational Research in Phonetics, Phonology, and Morphology , pp. 204–209, Online, July\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.sigmorphon-1.24. URL\nhttps://aclanthology.org/2020.sigmorphon-1.24.\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,\nand Neil Houlsby. Big transfer (bit): General visual representation learning. In Andrea Vedaldi,\nHorst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), Computer Vision – ECCV 2020 ,\npp. 491–507, Cham, 2020. Springer International Publishing. ISBN 978-3-030-58558-7. URL\nhttps://link.springer.com/chapter/10.1007/978-3-030-58558-7_29 .\nTaku Kudo. Subword regularization: Improving neural network translation models with multi-\nple subword candidates. In Proceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) , pp. 66–75, Melbourne, Australia, July\n2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1007. URL https:\n//aclanthology.org/P18-1007.\nTaku Kudo and John Richardson. SentencePiece: A simple and language independent sub-\nword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language Processing: System Demonstrations , pp.\n66–71, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi:\n10.18653/v1/D18-2012. URL https://aclanthology.org/D18-2012.\nAnne Lauscher, Vinit Ravishankar, Ivan Vuli ´c, and Goran Glavaš. From zero to hero: On the\nlimitations of zero-shot language transfer with multilingual Transformers. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 4483–\n4499, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/\n2020.emnlp-main.363. URL https://aclanthology.org/2020.emnlp-main.363.\nJason Lee, Kyunghyun Cho, and Thomas Hofmann. Fully character-level neural machine translation\nwithout explicit segmentation. Transactions of the Association for Computational Linguistics, 5:\n365–378, 2017. doi: 10.1162/tacl_a_00067. URL https://aclanthology.org/Q17-1026.\nFeng Liang, Yangguang Li, and Diana Marculescu. Supmae: Supervised masked autoencoders are\nefﬁcient vision learners. arXiv preprint, 2022. URL https://arxiv.org/abs/2205.14540.\nSeungyoung Lim, Myungji Kim, and Jooyoul Lee. Korquad1.0: Korean QA dataset for machine\nreading comprehension. arXiv preprint, 2019. URL http://arxiv.org/abs/1909.07005.\n14\nPublished as a conference paper at ICLR 2023\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining\napproach. arXiv preprint, 2019. URL http://arxiv.org/abs/1907.11692.\nIlya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with warm restarts. In 5th\nInternational Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,\n2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/\nforum?id=Skq89Scxx.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proceedings of the\n7th International Conference on Learning Representations (ICLR), New Orleans, LA, USA, 2019.\nOpenReview.net. URL https://openreview.net/forum?id=Bkg6RiCqY7.\nWentao Ma, Yiming Cui, Chenglei Si, Ting Liu, Shijin Wang, and Guoping Hu. CharBERT:\nCharacter-aware pre-trained language model. In Proceedings of the 28th International Con-\nference on Computational Linguistics , pp. 39–50, Barcelona, Spain (Online), December 2020.\nInternational Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.4.\nURL https://aclanthology.org/2020.coling-main.4.\nElman Mansimov, Mitchell Stern, Mia Chen, Orhan Firat, Jakob Uszkoreit, and Puneet Jain. To-\nwards end-to-end in-image neural machine translation. In Proceedings of the First Interna-\ntional Workshop on Natural Language Processing Beyond Text , pp. 70–74, Online, Novem-\nber 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.nlpbt-1.8. URL\nhttps://aclanthology.org/2020.nlpbt-1.8.\nAntonis Maronikolakis, Philipp Dufter, and Hinrich Schütze. Wine is not v i n. on the compatibil-\nity of tokenizations across languages. In Findings of the Association for Computational Lin-\nguistics: EMNLP 2021 , pp. 2382–2399, Punta Cana, Dominican Republic, November 2021.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2021.ﬁndings-emnlp.205. URL\nhttps://aclanthology.org/2021.findings-emnlp.205.\nSabrina J. Mielke, Zaid Alyafeai, Elizabeth Salesky, Colin Raffel, Manan Dey, Matthias Gallé, Arun\nRaja, Chenglei Si, Wilson Y . Lee, Benoît Sagot, and Samson Tan. Between words and characters:\nA brief history of open-vocabulary modeling and tokenization in NLP.arXiv preprint, 2021. URL\nhttps://arxiv.org/abs/2112.10508.\nIbraheem Muhammad Moosa, Mahmud Elahi Akhter, and Ashﬁa Binte Habib. Does transliteration\nhelp multilingual language modeling? arXiv preprint, 2022. URL https://arxiv.org/abs/\n2201.12501.\nPhuong-Thai Nguyen, Xuan-Luong Vu, Thi-Minh-Huyen Nguyen, Van-Hiep Nguyen, and Hong-\nPhuong Le. Building a large syntactically-annotated corpus of Vietnamese. In Proceedings of the\nThird Linguistic Annotation Workshop (LAW III), pp. 182–185, Suntec, Singapore, August 2009.\nAssociation for Computational Linguistics. URL https://aclanthology.org/W09-3035.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Jan Haji ˇc, Christopher D. Manning,\nSampo Pyysalo, Sebastian Schuster, Francis Tyers, and Daniel Zeman. Universal Dependen-\ncies v2: An evergrowing multilingual treebank collection. In Proceedings of the 12th Language\nResources and Evaluation Conference, pp. 4034–4043, Marseille, France, May 2020. European\nLanguage Resources Association. ISBN 979-10-95546-34-4. URL https://aclanthology.\norg/2020.lrec-1.497.\nMartha Palmer, Owen Rambow, Rajesh Bhatt, Dipti Misra Sharma, Bhuvana Narasimhan, and\nF. Xia. Hindi syntax: Annotating dependency, lexical predicate-argument structure, and phrase\nstructure. In Proceedings of ICON-2009: 7th International Conference on Natural Language Pro-\ncessing, India, 2009. Macmillan Publishers. URL http://cdn.iiit.ac.in/cdn/ltrc.iiit.\nac.in/hutb_release/related_publications/ICON09.pdf.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\nKöpf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chil-\namkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An impera-\ntive style, high-performance deep learning library. In Hanna M. Wallach, Hugo Larochelle,\n15\nPublished as a conference paper at ICLR 2023\nAlina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Ad-\nvances in Neural Information Processing Systems 32: Annual Conference on Neural In-\nformation Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,\nCanada, pp. 8024–8035, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/\nbdbca288fee7f92f2bfa9f7012727740-Abstract.html.\nVaidehi Patil, Partha Talukdar, and Sunita Sarawagi. Overlap-based vocabulary generation improves\ncross-lingual transfer among related languages. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers) , pp. 219–233, Dublin,\nIreland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.\n18. URL https://aclanthology.org/2022.acl-long.18.\nJonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Sebastian Ruder. UNKs everywhere: Adapting\nmultilingual language models to new scripts. InProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pp. 10186–10203, Online and Punta Cana, Dominican\nRepublic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.\nemnlp-main.800. URL https://aclanthology.org/2021.emnlp-main.800.\nTelmo Pires, Eva Schlinger, and Dan Garrette. How multilingual is multilingual BERT? In\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics ,\npp. 4996–5001, Florence, Italy, July 2019. Association for Computational Linguistics. doi:\n10.18653/v1/P19-1493. URL https://aclanthology.org/P19-1493.\nDanish Pruthi, Bhuwan Dhingra, and Zachary C. Lipton. Combating adversarial misspellings with\nrobust word recognition. In Proceedings of the 57th Annual Meeting of the Association for Com-\nputational Linguistics, pp. 5582–5591, Florence, Italy, July 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/P19-1561. URL https://aclanthology.org/P19-1561.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\nSutskever. Learning transferable visual models from natural language supervision. In Marina\nMeila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine\nLearning, ICML 2021, 18-24 July 2021, Virtual Event , volume 139 of Proceedings of Machine\nLearning Research, pp. 8748–8763. PMLR, 2021. URL http://proceedings.mlr.press/\nv139/radford21a.html.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-\nto-text transformer. Journal of Machine Learning Research , 21(140):1–67, 2020. URL http:\n//jmlr.org/papers/v21/20-074.html.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions\nfor machine comprehension of text. InProceedings of the 2016 Conference on Empirical Methods\nin Natural Language Processing, pp. 2383–2392, Austin, Texas, November 2016. Association for\nComputational Linguistics. doi: 10.18653/v1/D16-1264. URL https://aclanthology.org/\nD16-1264.\nLoganathan Ramasamy and Zden ˇek Žabokrtský. Prague dependency style treebank for Tamil.\nIn Proceedings of the Eighth International Conference on Language Resources and Evaluation\n(LREC’12), pp. 1888–1894, Istanbul, Turkey, May 2012. European Language Resources Associ-\nation (ELRA). URL http://www.lrec-conf.org/proceedings/lrec2012/pdf/456_Paper.\npdf.\nPhillip Rust, Jonas Pfeiffer, Ivan Vuli ´c, Sebastian Ruder, and Iryna Gurevych. How good is\nyour tokenizer? on the monolingual performance of multilingual language models. In Pro-\nceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the\n11th International Joint Conference on Natural Language Processing (Volume 1: Long Pa-\npers), pp. 3118–3135, Online, August 2021. Association for Computational Linguistics. doi:\n10.18653/v1/2021.acl-long.243. URL https://aclanthology.org/2021.acl-long.243.\nElizabeth Salesky, David Etter, and Matt Post. Robust open-vocabulary translation from visual\ntext representations. In Proceedings of the 2021 Conference on Empirical Methods in Natural\n16\nPublished as a conference paper at ICLR 2023\nLanguage Processing, pp. 7235–7252, Online and Punta Cana, Dominican Republic, November\n2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.576. URL\nhttps://aclanthology.org/2021.emnlp-main.576.\nThibault Sellam, Steve Yadlowsky, Ian Tenney, Jason Wei, Naomi Saphra, Alexander D’Amour, Tal\nLinzen, Jasmijn Bastings, Iulia Raluca Turc, Jacob Eisenstein, Dipanjan Das, and Ellie Pavlick.\nThe multiBERTs: BERT reproductions for robustness analysis. In International Conference on\nLearning Representations, 2022. URL https://openreview.net/forum?id=K0E_F0gFDgA.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 1715–1725, Berlin, Germany, August 2016. Association\nfor Computational Linguistics. doi: 10.18653/v1/P16-1162. URL https://aclanthology.org/\nP16-1162.\nMo Shen, Ryan McDonald, Daniel Zeman, and Peng Qi. Ud_chinese-gsd.GitHub repository, 2016.\nURL https://github.com/UniversalDependencies/UD_Chinese-GSD.\nNatalia Silveira, Timothy Dozat, Marie-Catherine de Marneffe, Samuel Bowman, Miriam Connor,\nJohn Bauer, and Chris Manning. A gold standard dependency corpus for English. In Proceedings\nof the Ninth International Conference on Language Resources and Evaluation (LREC’14) , pp.\n2897–2904, Reykjavik, Iceland, May 2014. European Language Resources Association (ELRA).\nURL http://www.lrec-conf.org/proceedings/lrec2014/pdf/1089_Paper.pdf.\nPeter Smit, Sami Virpioja, Stig-Arne Grönroos, and Mikko Kurimo. Morfessor 2.0: Toolkit\nfor statistical morphological segmentation. In Proceedings of the Demonstrations at the 14th\nConference of the European Chapter of the Association for Computational Linguistics , pp.\n21–24, Gothenburg, Sweden, April 2014. Association for Computational Linguistics. doi:\n10.3115/v1/E14-2006. URL https://aclanthology.org/E14-2006.\nByungHoon So, Kyuhong Byun, Kyungwon Kang, and Seongjin Cho. JaQuAD: Japanese question\nanswering dataset for machine reading comprehension. ArXiv preprint, 2022. URL https:\n//arxiv.org/abs/2202.01764.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng,\nand Christopher Potts. Recursive deep models for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language\nProcessing, pp. 1631–1642, Seattle, Washington, USA, October 2013. Association for Computa-\ntional Linguistics. URL https://aclanthology.org/D13-1170.\nBaohua Sun, Lin Yang, Catherine Chi, Wenhan Zhang, and Michael Lin. Squared english word:\nA method of generating glyph to use super characters for sentiment analysis. In AffCon@AAAI,\n2019. URL https://arxiv.org/abs/1902.02160.\nLichao Sun, Kazuma Hashimoto, Wenpeng Yin, Akari Asai, Jia Li, Philip S. Yu, and Caiming\nXiong. Adv-bert: BERT is not robust on misspellings! generating nature adversarial samples on\nBERT. arXiv preprint, 2020. URL https://arxiv.org/abs/2003.04985.\nYi Tay, Vinh Q Tran, Sebastian Ruder, Jai Gupta, Hyung Won Chung, Dara Bahri, Zhen Qin, Si-\nmon Baumgartner, Cong Yu, and Donald Metzler. Charformer: Fast character transformers via\ngradient-based subword tokenization. In International Conference on Learning Representations,\n2021. URL https://openreview.net/forum?id=JtBRnrlOEFN.\nOwen Taylor. Pango, an open-source unicode text layout engine. InProceedings of the 25th Interna-\ntionalization and Unicode Conference, Washington, D.C., USA, 2004. The Unicode Consortium.\nURL https://people.redhat.com/otaylor/iuc25/pango-unicode-paper.pdf.\nErik F. Tjong Kim Sang and Fien De Meulder. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In Proceedings of the Seventh Conference\non Natural Language Learning at HLT-NAACL 2003 , pp. 142–147, 2003. URL https://\naclanthology.org/W03-0419.\n17\nPublished as a conference paper at ICLR 2023\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test reso-\nlution discrepancy. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché Buc, E. Fox,\nand R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 32. Cur-\nran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/\nd03a857a23b5285736c4d55e0bb067c8-Paper.pdf.\nIulia Turc, Kenton Lee, Jacob Eisenstein, Ming-Wei Chang, and Kristina Toutanova. Revisiting\nthe primacy of english in zero-shot cross-lingual transfer. arXiv preprint, 2021. URL https:\n//arxiv.org/abs/2106.16171.\nDaan van Esch, Elnaz Sarbar, Tamar Lucassen, Jeremy O’Brien, Theresa Breiner, Manasa Prasad,\nEvan Crew, Chieu Nguyen, and Françoise Beaufays. Writing across the world’s languages: Deep\ninternationalization for gboard, the google keyboard. Technical report, 2019. URL http://\narxiv.org/abs/1912.01218.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike\nvon Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V . N. Vishwanathan, and Ro-\nman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Confer-\nence on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA,\nUSA, pp. 5998–6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/\n3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.\nGiorgos Vernikos and Andrei Popescu-Belis. Subword mapping and anchoring across languages.\nIn Findings of the Association for Computational Linguistics: EMNLP 2021 , pp. 2633–2647,\nPunta Cana, Dominican Republic, November 2021. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2021.ﬁndings-emnlp.224. URL https://aclanthology.org/2021.\nfindings-emnlp.224.\nAda Wan. Fairness in representation for multilingual NLP: Insights from controlled experiments on\nconditional language modeling. In International Conference on Learning Representations, 2022.\nURL https://openreview.net/forum?id=-llS6TiOew.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE:\nA multi-task benchmark and analysis platform for natural language understanding. In Proceed-\nings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks\nfor NLP, pp. 353–355, Brussels, Belgium, November 2018. Association for Computational Lin-\nguistics. doi: 10.18653/v1/W18-5446. URL https://aclanthology.org/W18-5446.\nChanghan Wang, Kyunghyun Cho, and Jiatao Gu. Neural machine translation with byte-level sub-\nwords. In The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI 2020, The Thirty-\nSecond Innovative Applications of Artiﬁcial Intelligence Conference, IAAI 2020, The Tenth AAAI\nSymposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2020, New York, NY, USA,\nFebruary 7-12, 2020, pp. 9154–9160. AAAI Press, 2020. URL https://ojs.aaai.org/index.\nphp/AAAI/article/view/6451.\nXinyi Wang, Sebastian Ruder, and Graham Neubig. Expanding pretrained models to thousands\nmore languages via lexicon-based adaptation. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers) , pp. 863–877, Dublin,\nIreland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.\n61. URL https://aclanthology.org/2022.acl-long.61.\nJunqiu Wei, Qun Liu, Yinpeng Guo, and Xin Jiang. Training multilingual pre-trained language\nmodel with byte-level subwords. arXiv preprint, 2021. URL https://arxiv.org/abs/2101.\n09469.\nAlexander Wettig, Tianyu Gao, Zexuan Zhong, and Danqi Chen. Should you mask 15% in masked\nlanguage modeling? arXiv preprint, 2022. URL https://arxiv.org/abs/2202.08005.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick\nvon Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,\n18\nPublished as a conference paper at ICLR 2023\nMariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations , pp. 38–45, Online, October 2020. Asso-\nciation for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https:\n//aclanthology.org/2020.emnlp-demos.6.\nShijie Wu and Mark Dredze. Are all languages created equal in multilingual BERT? In Pro-\nceedings of the 5th Workshop on Representation Learning for NLP , pp. 120–130, Online, July\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.repl4nlp-1.16. URL\nhttps://aclanthology.org/2020.repl4nlp-1.16.\nWei Wu, Yuxian Meng, Fei Wang, Qinghong Han, Muyu Li, Xiaoya Li, Jie Mei, Ping Nie, Xi-\naofei Sun, and Jiwei Li. Glyce: Glyph-vectors for chinese character representations. In Neural\nInformation Processing Systems, 2019.\nLinting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam\nRoberts, and Colin Raffel. ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte\nModels. Transactions of the Association for Computational Linguistics , 10:291–306, 03 2022.\nISSN 2307-387X. doi: 10.1162/tacl_a_00461. URL https://doi.org/10.1162/tacl_a_\n00461.\nAmir Zeldes and Mitchell Abrams. The Coptic Universal Dependency treebank. In Proceedings of\nthe Second Workshop on Universal Dependencies (UDW 2018), pp. 192–201, Brussels, Belgium,\nNovember 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6022. URL\nhttps://aclanthology.org/W18-6022.\nDaniel Zeman, Joakim Nivre, Mitchell Abrams, Elia Ackermann, Noëmi Aepli, Hamid Aghaei,\nŽeljko Agi ´c, Amir Ahmadi, Lars Ahrenberg, Chika Kennedy Ajede, Gabriel˙e Aleksandraviˇci¯ut˙e,\nIka Alﬁna, Avner Algom, Erik Andersen, Lene Antonsen, Katya Aplonova, Angelina Aquino,\nCarolina Aragon, Glyd Aranes, Maria Jesus Aranzabe, Bilge Nas Arıcan, ⁀Hórunn Arnardóttir,\nGashaw Arutie, Jessica Naraiswari Arwidarasti, Masayuki Asahara, Deniz Baran Aslan, Cen-\ngiz Asmazo ˘glu, Luma Ateyah, Furkan Atmaca, Mohammed Attia, Aitziber Atutxa, Liesbeth\nAugustinus, Elena Badmaeva, Keerthana Balasubramani, Miguel Ballesteros, Esha Banerjee,\nSebastian Bank, Verginica Barbu Mititelu, Starkaður Barkarson, Rodolfo Basile, Victoria Bas-\nmov, Colin Batchelor, John Bauer, Seyyit Talha Bedir, Kepa Bengoetxea, Yifat Ben Moshe,\nGözde Berk, Yevgeni Berzak, Irshad Ahmad Bhat, Riyaz Ahmad Bhat, Erica Biagetti, Eckhard\nBick, Agn˙e Bielinskien˙e, Kristín Bjarnadóttir, Rogier Blokland, Victoria Bobicev, Loïc Boizou,\nEmanuel Borges Völker, Carl Börstell, Cristina Bosco, Gosse Bouma, Sam Bowman, Adriane\nBoyd, Anouck Braggaar, Kristina Brokait ˙e, Aljoscha Burchardt, Marie Candito, Bernard Caron,\nGauthier Caron, Lauren Cassidy, Tatiana Cavalcanti, Gül¸ sen Cebiro˘glu Eryi ˘git, Flavio Massim-\niliano Cecchini, Giuseppe G. A. Celano, Slavomír ˇCéplö, Neslihan Cesur, Savas Cetin, Özlem\nÇetino˘glu, Fabricio Chalub, Shweta Chauhan, Ethan Chi, Taishi Chika, Yongseok Cho, Jinho\nChoi, Jayeol Chun, Juyeon Chung, Alessandra T. Cignarella, Silvie Cinková, Aurélie Collomb,\nÇa˘grı Çöltekin, Miriam Connor, Daniela Corbetta, Marine Courtin, Mihaela Cristescu, Philemon\nDaniel, Elizabeth Davidson, Mathieu Dehouck, Martina de Laurentiis, Marie-Catherine de Marn-\neffe, Valeria de Paiva, Mehmet Oguz Derin, Elvis de Souza, Arantza Diaz de Ilarraza, Carly\nDickerson, Arawinda Dinakaramani, Elisa Di Nuovo, Bamba Dione, Peter Dirix, Kaja Dobro-\nvoljc, Timothy Dozat, Kira Droganova, Puneet Dwivedi, Hanne Eckhoff, Sandra Eiche, Marhaba\nEli, Ali Elkahky, Binyam Ephrem, Olga Erina, Tomaž Erjavec, Aline Etienne, Wograine Evelyn,\nSidney Facundes, Richárd Farkas, Federica Favero, Jannatul Ferdaousi, Marília Fernanda, Hector\nFernandez Alcalde, Jennifer Foster, Cláudia Freitas, Kazunori Fujita, Katarína Gajdošová, Daniel\nGalbraith, Federica Gamba, Marcos Garcia, Moa Gärdenfors, Sebastian Garza, Fabrício Fer-\nraz Gerardi, Kim Gerdes, Filip Ginter, Gustavo Godoy, Iakes Goenaga, Koldo Gojenola, Mem-\nduh Gökırmak, Yoav Goldberg, Xavier Gómez Guinovart, Berta González Saavedra, Bernadeta\nGrici¯ut˙e, Matias Grioni, Loïc Grobol, Normunds Gr ¯uz¯ıtis, Bruno Guillaume, Céline Guillot-\nBarbance, Tunga Güngör, Nizar Habash, Hinrik Hafsteinsson, Jan Haji ˇc, Jan Haji ˇc jr., Mika\nHämäläinen, Linh Hà M˜y, Na-Rae Han, Muhammad Yudistira Hanifmuti, Takahiro Harada, Sam\nHardwick, Kim Harris, Dag Haug, Johannes Heinecke, Oliver Hellwig, Felix Hennig, Barbora\nHladká, Jaroslava Hlavá ˇcová, Florinel Hociung, Petter Hohle, Jena Hwang, Takumi Ikeda, An-\nton Karl Ingason, Radu Ion, Elena Irimia, O . lájídé Ishola, Kaoru Ito, Siratun Jannat, Tomáš\n19\nPublished as a conference paper at ICLR 2023\nJelínek, Apoorva Jha, Anders Johannsen, Hildur Jónsdóttir, Fredrik Jørgensen, Markus Juuti-\nnen, Sarveswaran K, Hüner Ka¸ sıkara, Andre Kaasen, Nadezhda Kabaeva, Sylvain Kahane, Hi-\nroshi Kanayama, Jenna Kanerva, Neslihan Kara, Ritván Karahóˇga, Boris Katz, Tolga Kayadelen,\nJessica Kenney, Václava Kettnerová, Jesse Kirchner, Elena Klementieva, Elena Klyachko, Arne\nKöhn, Abdullatif Köksal, Kamil Kopacewicz, Timo Korkiakangas, Mehmet Köse, Natalia Kot-\nsyba, Jolanta Kovalevskait˙e, Simon Krek, Parameswari Krishnamurthy, Sandra Kübler, O˘guzhan\nKuyrukçu, Aslı Kuzgun, Sookyoung Kwak, Veronika Laippala, Lucia Lam, Lorenzo Lambertino,\nTatiana Lando, Septina Dian Larasati, Alexei Lavrentiev, John Lee, Phương Lê H `ông, Alessan-\ndro Lenci, Saran Lertpradit, Herman Leung, Maria Levina, Cheuk Ying Li, Josie Li, Keying Li,\nYuan Li, KyungTae Lim, Bruna Lima Padovani, Krister Lindén, Nikola Ljube ˇsi´c, Olga Loginova,\nStefano Lusito, Andry Luthfi, Mikko Luukko, Olga Lyashevskaya, Teresa Lynn, Vivien Macke-\ntanz, Menel Mahamdi, Jean Maillard, Aibek Makazhanov, Michael Mandl, Christopher Manning,\nRuli Manurung, B ¨us ¸ra Mars ¸an, Cătălina Mărănduc, David Mareˇcek, Katrin Marheinecke, Stella\nMarkantonatou, Héctor Martínez Alonso, Lorena Martín Rodríguez, André Martins, Jan Ma ˇsek,\nHiroshi Matsuda, Yuji Matsumoto, Alessandro Mazzei, Ryan McDonald, Sarah McGuinness,\nGustavo Mendonc ¸a, Tatiana Merzhevich, Niko Miekka, Karina Mischenkova, Margarita Misir-\npashayeva, Anna Missil ¨a, Cătălin Mititelu, Maria Mitrofan, Yusuke Miyao, AmirHossein Mo-\njiri Foroushani, Judit Molnár, Amirsaeid Moloodi, Simonetta Montemagni, Amir More, Laura\nMoreno Romero, Giovanni Moretti, Keiko Sophie Mori, Shinsuke Mori, Tomohiko Morioka,\nShigeki Moro, Bjartur Mortensen, Bohdan Moskalevskyi, Kadri Muischnek, Robert Munro, Yugo\nMurawaki, Kaili M ¨u¨urisep, Pinkey Nainwani, Mariam Nakhlé, Juan Ignacio Navarro Hor ˜niacek,\nAnna Nedoluzhko, Gunta Ne ˇspore-B¯erzkalne, Manuela Nevaci, Lương Nguy ˜ên Thị, Huy `ên\nNguy˜ên Thị Minh, Yoshihiro Nikaido, Vitaly Nikolaev, Rattima Nitisaroj, Alireza Nourian, Hanna\nNurmi, Stina Ojala, Atul Kr. Ojha, Adédayò Olúòkun, Mai Omura, Emeka Onwuegbuzia, Noam\nOrdan, Petya Osenova, Robert ¨Ostling, Lilja Øvrelid, S ¸aziye Bet¨ul ¨Ozates ¸, Merve¨Ozc ¸elik, Arzu-\ncan ¨Ozg¨ur, Balkız ¨Ozt¨urk Bas ¸aran, Teresa Paccosi, Alessio Palmero Aprosio, Hyunji Hayley\nPark, Niko Partanen, Elena Pascual, Marco Passarotti, Agnieszka Patejuk, Guilherme Paulino-\nPassos, Giulia Pedonese, Angelika Peljak- Łapi´nska, Siyao Peng, Cenel-Augusto Perez, Natalia\nPerkova, Guy Perrier, Slav Petrov, Daria Petrova, Andrea Peverelli, Jason Phelan, Jussi Piitu-\nlainen, Tommi A Pirinen, Emily Pitler, Barbara Plank, Thierry Poibeau, Larisa Ponomareva,\nMartin Popel, Lauma Pretkalnin ¸a, Sophie Prévost, Prokopis Prokopidis, Adam Przepiórkowski,\nTiina Puolakainen, Sampo Pyysalo, Peng Qi, Andriela R ¨a¨abis, Alexandre Rademaker, Miza-\nnur Rahoman, Taraka Rama, Loganathan Ramasamy, Carlos Ramisch, Fam Rashel, Moham-\nmad Sadegh Rasooli, Vinit Ravishankar, Livy Real, Petru Rebeja, Siva Reddy, Mathilde Reg-\nnault, Georg Rehm, Ivan Riabov, Michael Rie ßler, Erika Rimkut ˙e, Larissa Rinaldi, Laura Rit-\numa, Putri Rizqiyah, Luisa Rocha, Eiríkur R ¨ognvaldsson, Mykhailo Romanenko, Rudolf Rosa,\nValentin Ros, ca, Davide Rovati, Ben Rozonoyer, Olga Rudina, Jack Rueter, Kristján Rúnarsson,\nShoval Sadde, Pegah Safari, Beno ˆıt Sagot, Aleksi Sahala, Shadi Saleh, Alessio Salomoni, Tanja\nSamardˇzi´c, Stephanie Samson, Manuela Sanguinetti, Ezgi Sanıyar, Dage S ¨arg, Baiba Saul ¯ıte,\nYanin Sawanakunanon, Shefali Saxena, Kevin Scannell, Salvatore Scarlata, Nathan Schneider,\nSebastian Schuster, Lane Schwartz, Djamé Seddah, Wolfgang Seeker, Mojgan Seraji, Syeda\nShahzadi, Mo Shen, Atsuko Shimada, Hiroyuki Shirasu, Yana Shishkina, Muh Shohibussirri,\nDmitry Sichinava, Janine Siewert, Einar Freyr Sigurðsson, Aline Silveira, Natalia Silveira, Maria\nSimi, Radu Simionescu, Katalin Simkó, Mária Šimková, Kiril Simov, Maria Skachedubova,\nAaron Smith, Isabela Soares-Bastos, Shaﬁ Sourov, Carolyn Spadine, Rachele Sprugnoli, Vi-\nvian Stamou, Stein⁀hór Steingrímsson, Antonio Stella, Milan Straka, Emmett Strickland, Jana Str-\nnadová, Alane Suhr, Yogi Lesmana Sulestio, Umut Sulubacak, Shingo Suzuki, Daniel Swanson,\nZsolt Szántó, Chihiro Taguchi, Dima Taji, Yuta Takahashi, Fabio Tamburini, Mary Ann C. Tan,\nTakaaki Tanaka, Dipta Tanaya, Mirko Tavoni, Samson Tella, Isabelle Tellier, Marinella Testori,\nGuillaume Thomas, Sara Tonelli, Liisi Torga, Marsida Toska, Trond Trosterud, Anna Trukhina,\nReut Tsarfaty, Utku Türk, Francis Tyers, Sumire Uematsu, Roman Untilov, Zdeˇnka Urešová, Lar-\nraitz Uria, Hans Uszkoreit, Andrius Utka, Elena Vagnoni, Sowmya Vajjala, Rob van der Goot,\nMartine Vanhove, Daniel van Niekerk, Gertjan van Noord, Viktor Varga, Uliana Vedenina, Eric\nVillemonte de la Clergerie, Veronika Vincze, Natalia Vlasova, Aya Wakasa, Joel C. Wallenberg,\nLars Wallin, Abigail Walsh, Jing Xian Wang, Jonathan North Washington, Maximilan Wendt,\nPaul Widmer, Shira Wigderson, Sri Hartati Wijono, Seyi Williams, Mats Wirén, Christian Wit-\ntern, Tsegay Woldemariam, Tak-sum Wong, Alina Wróblewska, Mary Yako, Kayo Yamashita,\nNaoki Yamazaki, Chunxiao Yan, Koichi Yasuoka, Marat M. Yavrumyan, Arife Betül Yenice,\n20\nPublished as a conference paper at ICLR 2023\nOlcay Taner Yıldız, Zhuoran Yu, Arlisa Yuliawati, Zden ˇek Žabokrtský, Shorouq Zahra, Amir\nZeldes, He Zhou, Hanzhi Zhu, Anna Zhuravleva, and Rayan Ziane. Universal dependencies 2.10,\n2022. URL http://hdl.handle.net/11234/1-4758. LINDAT/CLARIAH-CZ digital library\nat the Institute of Formal and Applied Linguistics (ÚFAL), Faculty of Mathematics and Physics,\nCharles University.\nShiyue Zhang, Vishrav Chaudhary, Naman Goyal, James Cross, Guillaume Wenzek, Mohit Bansal,\nand Francisco Guzmán. How robust is neural machine translation to language imbalance in mul-\ntilingual tokenizer training? arXiv preprint, 2022. URL https://arxiv.org/abs/2204.14268.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and\nSanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching\nmovies and reading books. In The IEEE International Conference on Computer Vision (ICCV) ,\nDecember 2015. URL https://www.cv-foundation.org/openaccess/content_iccv_2015/\npapers/Zhu_Aligning_Books_and_ICCV_2015_paper.pdf.\n21\nPublished as a conference paper at ICLR 2023\nA A BSTRACT RECONSTRUCTIONS\nFigure 4: PIXEL image reconstructions of the abstract with different span masks.\n22\nPublished as a conference paper at ICLR 2023\nB W EB TEXT RECONSTRUCTIONS\n(a) 100k steps\n (b) 500k steps\n (c) 1M steps\nFigure 5: PIXEL image reconstructions after 100k, 500k, and 1M steps of pretraining. We overlay the\nmasked original image with the model’s predictions. Images are wrapped into squares and resized\nfor visualization purposes only. The texts were not part of the training data. We see that the fully\ntrained PIXEL (1M) predicts masked spans more clearly and accurately. For longer spans with a\nlarger possible prediction space, multiple predictions may appear together creating blurred text.\nReconstructions of three sources of text 19 20 21 after 100K, 500K and 1M pretraining steps. The\nﬁgure also shows how PIXEL (visually) expresses uncertainty, e.g. for reconstructions of long spans\nwhere the space of possible outputs is much larger than for short spans, and how it captures long-\nrange dependencies. In the third row, we can for instance see that PIXEL uses context from the\nbeginning of a sequence ( Barack Obama) to correctly ﬁll in a gap later in the sequence, and vice-\nversa (Brienomyrus).\n19https://www.nationalpeanutboard.org/peanut-info/our-message.htm\n20https://www.penguinsinternational.org/2019/07/10/do-penguins-have-knees-and-other-\nfrequently-asked-questions/\n21https://www.theatlantic.com/science/archive/2021/05/electric-fish-pause/618993/\n23\nPublished as a conference paper at ICLR 2023\nC C ODE\nPIXEL is implemented in PyTorch (Paszke et al., 2019) and built on HuggingFace transformers\n(Wolf et al., 2020). We make our code available at https://github.com/xplip/pixel. Our\npretrained PIXEL model, including a large number of intermediate checkpoints, is available at\nhttps://huggingface.co/Team-PIXEL/pixel-base and our ﬁnetuned models, including mul-\ntiple seeds each, are available through the model hub.\nD T EXT RENDERER DETAILS\nRendering backend We experimented with different text rendering backends. Following Salesky\net al. (2021), our ﬁrst implementation was based on PyGame, 22 which PIXEL was also pretrained\nwith. Later on, we switched to a backend based on Pango (Taylor, 2004) and Cairographics, 23\nwhich has native support for complex text layouts, making it possible to specify fallback fonts, and\nhas faster rendering speed. Without fallback fonts, we would be limited to a maximum number of\n216−1 glyphs that can ﬁt into a single OpenType or TrueType font ﬁle due to a technical limitation.24\nBy leveraging fallback fonts, we can theoretically cover all Unicode codepoints, including emojis.\nFonts We rely on the Google Noto Sans fonts collection,25 which covers the majority of Unicode\ncodepoints and is actively growing. 26. Note, however, that PIXEL is compatible with any font and\ncan therefore encode anything that can be typeset on a computer screen. We used a font size of\n8 at 120 DPI for pretraining with PyGame, which was selected manually to ﬁt most scripts into a\nrendered height of 16px. It can, however, also be adjusted at ﬁnetuning time. For ﬁnetuning with\nPangoCairo, we use a font size of 8 ·(120/72) ≈13.33 which yields roughly the same outputs as\nthe PyGame renderer. Due to how glyphs are shaped by the two backends, the outputs of the two\nrenderers do notexactly match. Because we did not employ data augmentation to makePIXEL robust\nto such changes in font size, we recommend using the PyGame renderer it was pretrained with for\nzero-shot applications with PIXEL . When ﬁnetuning, this minor mismatch in rendering outputs is\neasily overcome by PIXEL , so we generally recommend using the PangoCairo renderer.\nCharacters versus glyphs For extractive QA, it is necessary to obtain a mapping between the\ncharacters in the context paragraph and where they appear on the rendered image. Obtaining this\nmapping is not straightforward due to how text is rendered. The shaping step in the rendering\npipeline converts characters into glyphs.27 In ligatures, as common for instance in Arabic, a glyph\nis composed of multiple characters. Likewise, an emoji often consists of a base codepoint and a\nmodiﬁer codepoint (e.g. to change the emoji skin colour) which are represented by a single glyph.\nFor accents, on the other hand, one character might yield multiple glyphs.28 In practice, the renderer\ntherefore uses grapheme clusters, whose logical boundaries in the rendered image we can map to\nthe input characters.29 For simplicity, we assign each codepoint of a grapheme cluster to the logical\nhorizontal offset at which the cluster starts on the rendered image. Future work may investigate\nalternative mapping strategies.\nRGB rendering PIXEL supports RGB rendering which may be useful to accurately represent\ncolour emoji and for multimodal applications in the future. However, 24-bit RGB rendering is\nslightly slower than 8-bit grayscale rendering (see Table 6 below) for text written in Latin script,\nwhich is why we made RGB rendering an optional setting. In our pretraining and ﬁnetuning experi-\nments we rendered text in grayscale, and we generally recommend doing so when not working with\ncoloured inputs.\n22https://www.pygame.org/\n23https://www.cairographics.org/\n24See https://en.wikipedia.org/wiki/Unicode_font for an explanation.\n25https://fonts.google.com/noto\n26See https://notofonts.github.io/overview/ for an overview of Noto’s Unicode coverage.\n27See https://docs.gtk.org/Pango/pango_rendering.html for an overview of the rendering pipeline.\n28https://docs.gtk.org/Pango/pango_fonts.html#glyphs\n29https://unicode.org/reports/tr29/#Grapheme_Cluster_Boundaries\n24\nPublished as a conference paper at ICLR 2023\nRight-to-left scripts PIXEL ’s renderer natively supports right-to-left (RTL) writing. In the default\nsetting, the base text direction (which for instance determines on which side of a sentence punctua-\ntion marks are placed) is inferred automatically by the rendering backend based on the ﬁrst “strong\ndirectional” character in a given paragraph.30 The mirroring of RTL characters is also handled au-\ntomatically according to their Unicode bidi attributes. Optionally, the base text direction can be\nset manually, which is useful when working on monolingual data, e.g. in Arabic or Hebrew, as the\nrenderer does not have to go through the direction check. In §J, we describe limitations of how we\ncurrently handle RTL writing.\n0.00\n0.01\n0.02\n0.03Proportion\nArabic\n0.00\n0.02\n0.04\nChinese\n0.00\n0.02\n0.04\nEnglish\n0.00\n0.03\n0.05\n0.08Proportion\nFinnish\n0.00\n0.02\n0.04\nIndonesian\n0.00\n0.03\n0.05\n0.08\nJapanese\n0 50 100 150\nLength [Tokens/Patches]\n0.00\n0.02\n0.04\n0.06Proportion\nKorean\n0 50 100 150\nLength [Tokens/Patches]\n0.00\n0.02\n0.04\nRussian\n0 50 100 150\nLength [Tokens/Patches]\n0.00\n0.05\n0.10\nTurkish\nReference\nBERT\nmBERT\nPIXEL\nFigure 6: Distributions of sentence lengths from monolingual UD corpora after tokenizing by BERT\nand MBERT and rendering by PIXEL , compared to the reference by UD treebank annotators.\nProcessor Batched Throughput [ex / s]\nENG ZHO\nRenderer (Grayscale) \u0017 3944.1 6309.0\nRenderer (RGB) \u0017 3615.1 6849.5\nTokenizer (Rust) \u0013 19128.9 18550.5\n\u0017 4782.9 5684.4\nTokenizer (Python) \u0013 1286.6 2637.1\n\u0017 1286.8 2580.9\nTable 6: Throughput comparison between PIXEL ’s PangoCairo renderer and the fast and slowBERT\ntokenizers, implemented in Rust and Python respectively, from the HuggingFace tokenizers library.\nWe estimate throughput, measured in examples per second, by how long it takes to process 1M lines\nof English (ENG ) and Chinese (ZHO ) Wikipedia text on the same desktop workstation (AMD Ryzen\n9 3900X 12-core CPU). We distinguish between tokenizing all lines individually (Batched =\u0017) and\nas one single batch (\u0013).\nEfﬁciency analysis We brieﬂy analyze the text processing (rendering versus tokenizing) efﬁciency\nin terms of a) length of the processed sequence, which has a direct effect on GPU memory consump-\ntion and the time it takes to compute forward and backward passes, and b) processing throughput.\nFor a), we follow Rust et al. (2021) and process the training and validation splits of all available\nUD v2.10 treebanks in various languages with the PIXEL renderer and the tokenizers of BERT and\nMBERT . We plot the resulting sentence length distributions in Figure 6, including a comparison\n30See https://unicode.org/reports/tr9/ for an overview of the Unicode bidi algorithm.\n25\nPublished as a conference paper at ICLR 2023\nwith the reference segmentations from the UD annotators. For English text, the PIXEL renderer is\nslightly less efﬁcient, i.e., it produces slightly longer sequences on average than the tokenizers. For\nother languages with Latin script, e.g. Finnish and Turkish, the renderer is more efﬁcient than the\nBERT tokenizer, albeit slightly less efﬁcient than the MBERT tokenizer. For non-Latin scripts such\nas Arabic and Japanese, we see that the renderer can be a lot more efﬁcient than both tokenizers.\nThe English BERT tokenizer is technically fairly space-efﬁcient for non-Latin scripts but this is\nmisleading because it largely produces [UNK]s (recall right side of Table 1) and each [UNK] is a single\ntoken; the functionality of the BERT model on a sequence of [UNK] is strongly compromised.\nFor b), we compare the processing throughput of HuggingFace’s BERT tokenizers and our PIXEL\nrenderer in Table 6. We ﬁnd that the Rust-based BERT tokenizer with batch processing achieves the\nhighest throughput by leveraging parallelization. When not using batch processing, it is comparable\nin throughput with PIXEL ’s renderer, i.e. depending on the language or script, rendering can be\nslightly slower ( ENG ) or faster ( ZHO ) than tokenizing. Since the rendering backend (PangoCairo)\nis implemented in C, we expect to achieve similar gains in rendering throughput by also leveraging\nparallelization for batch processing (in contrast to the Python-based tokenizer which is limited by\nPython’s global interpreter lock (GIL)). We plan to implement batch rendering functionality in the\nfuture.\nE A RCHITECTURE & PRETRAINING DETAILS\nPARAMETER VALUE\nImage size (16, 8464, 3)\nPatch sizeP 16\nEncoder hidden sizeDenc 768\nEncoder intermediate size 3072\nEncoder num attention heads 12\nEncoder num layersL 12\nDecoder hidden sizeDdec 512\nDecoder intermediate size 2048\nDecoder num attention heads 16\nDecoder num layersK 8\nLayer normε(Ba et al., 2016) 1e−12\nSpan masking ratioR 0.25\nSpan masking max lengthS 6\nSpan masking cumulative weightsW {0.2,0.4,0.6,0.8,0.9,1}\nSpan masking spacing Dynamic\nDropout probability 0.1\nHidden activation GeLU (Hendrycks & Gimpel, 2016)\nOptimizer AdamW (Loshchilov & Hutter, 2019; Kingma & Ba, 2015)\nAdamβ (0.9, 0.999)\nAdamε 1e−8\nWeight decay 0.05\nPeak learning rate 1.5e−4\nLearning rate schedule Cosine Decay (Loshchilov & Hutter, 2017)\nMinimum learning rate 1e−5\nLearning rate warmup ratio 0.05\nTraining steps 1M\nBatch size 256\nTable 7: PIXEL pretraining settings\nPatch Embeddings PIXEL reshapes each image x into a sequence of N = W/P non-overlapping\nﬂattened 2D patches xf ∈RN×(P2C), where P = 16is the patch size, and linearly projects them\nvia E ∈R(P2C)×Denc to obtain patch embeddings xp = (xf E) ∈RN×Denc with encoder hidden\nsize Denc = P2C = 768.31 Afterwards, ﬁxed sinusoidal position embeddings Epos ∈R(N+1)×Denc\nare added, leaving out the position vector in position 0 for a classiﬁcation (CLS) embedding later:\n˜xp = xp + [E1\npos,..., E(N+1)\npos ].\n31This is equivalent to projecting each rendered image x ∈ RH×W×C via a 2D-convolutional layer with C\ninput channels and Denc output channels and kernel size and stride both equal to the patch sizeP, which we do\nin practice.\n26\nPublished as a conference paper at ICLR 2023\n0 200 400 600 800 1000\nTraining steps [K]\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Loss\nFigure 7: PIXEL pretraining loss curve\nSpan Masking PIXEL then masks out R= 25%of the N = 529 embedded patches via span\nmasking with max span length S = 6and cumulative span weights W = {0.2,0.4,0.6,0.8,0.9,1},\ni.e. E(s) = 3.1, as outlined in Algorithm 1. Applying the maskM, we obtain the unmasked patches\n˜xvis = {˜xi\np : i /∈M}N\ni=0.\nEncoder Following ViT-MAE (He et al., 2022), the PIXEL encoder only operates on unmasked\npatches (i.e., ≈396 patches at 25% masking) and a special CLS embedding with its positional encod-\ning c = x[cls] + E0\npos ∈R1×Denc is prepended to the sequence: h0 = [c,˜xvis] ∈R(1+⌊R·N⌋)×Denc .32\nLet {hi}L\ni=1 be the encoder hidden states after each of the L= 12encoder transformer layers, and\nh0 denotes the input sequence. The outputs of each transformer layer are computed as detailed in\n(Vaswani et al., 2017),33 and the last layer’s outputhL ∈R(1+⌊R·N⌋)×Denc is passed to the decoder.\nDecoder The PIXEL decoder ﬁrst projects the encoder outputs via Edec ∈RDenc×Ddec to obtain\ndecoder embeddings xd = hLEdec ∈R(1+⌊R·N⌋)×Ddec , where Ddec = 512. Next, mask embeddings\nx[mask] ∈R1×Ddec are inserted at the masked-out positions and ﬁxed sinusoidal position embeddings\nare added to obtain d0 = [(xd ∪{x[mask] : i ∈M}N\ni=0) +Epos] ∈R(N+1)×Ddec . {di}K\ni=1 are\nthe decoder hidden states after each of the K = 8 decoder transformer layers, computed in the\nsame way as the encoder hidden states, and d0 denotes the input sequence. There is no encoder-\ndecoder cross-attention. The decoder output dK ∈R(N+1)×Ddec is projected via O ∈RDdec×(P2C)\nto obtain patch-wise logits o = (dKO) ∈R(N+1)×(P2C). Finally, the CLS logits are removed\nand a normalized mean squared error (MSE) pixel reconstruction loss is computed: Lnormpix =\n1\n|Q|\n∑\ni∈Q |normalize(xi\nf ) −oi|2 with idenoting the indices in the set of masked, non-blank (text)\npatches Q = {i : i ∈(M∩T )}N\ni=0 and normalize(·) dividing the difference between the target\npatch and its mean by its standard deviation.\nF F INETUNING DETAILS\nTable 8 gives an overview of all languages used in our ﬁnetuning experiments, Table 9 links to our\nﬁnetuning datasets, and Table 10 lists the UD treebanks we used.\nWe list our ﬁnetuning recipes in Table 11 for POS tagging, dependency parsing, NER , QA, and XNLI\nand in Table 12 for the GLUE tasks. Due to compute limitations we did not run comprehensive\nhyperparameter sweeps. Instead, we relied on sensible priors from ﬁnetuning BERT and made slight\nmodiﬁcations as needed. In most cases, hyperparameters that work well for BERT also work well\nfor PIXEL . For some of the semantic tasks, in particular NLI and SST-2, we found that some random\ninitializations did not converge. In those cases, minor tweaks to the learning rate or increasing the\nbatch size usually helped. For GLUE , we found that PIXEL performed slightly better on some tasks\nwith the PangoCairo renderer, whereas for others, using the PyGame renderer (which PIXEL was\n32In pretraining, no loss is computed for the CLS embedding but it can optionally be used when ﬁnetuning\nPIXEL for sequence-level downstream tasks.\n33Note that encoder and decoder do not attend to the blank (padding) patches that appear after theEOS patch.\n27\nPublished as a conference paper at ICLR 2023\npretrained with) was more stable. We plan to further optimize the training recipes and studyPIXEL ’s\nconvergence behaviour in the future.\nFor word-level tasks, we add padding in order to render each word at the start of a new image\npatch and so create a bijective mapping between words and patches. Doing so assumes that word\nboundaries are available. We note that subword-based and character-based models also make this\nassumption. In BERT , for instance, word-level tasks are formulated such that a word’s label is as-\nsigned to its ﬁrst subword token, requiring word boundaries. During training, continuation tokens\nare then masked out when computing the loss. Consequently, predictions for continuation tokens\nalso need to be masked out at inference time, which again requires word boundaries or aggregation\nstrategies that may introduce errors. The same applies to character-based models. ForPIXEL , should\nthis assumption be violated, it is still possible to render the text without adding spacing, although the\nmapping is then no longer bijective as multiple words can overlap on one image patch. In such cases,\nassigning the prediction for a patch to either word can cause loss of information. Although in prac-\ntice this approach does not necessarily affect performance negatively, future work will investigate\nalternative approaches.\nLanguage ISO 639-3 Language Family Script\nAmharic AMH Afro-Asiatic Ge \\ez\nArabic ARA Afro-Asiatic Arabic\nBengali BEN Indo-European Bengali\nBulgarian BUL Indo-European Cyrillic\nChinese ZHO Sino-Tibetan Chinese\nCoptic COP Afro-Asiatic Coptic\nEnglish ENG Indo-European Latin\nFinnish FIN Uralic Latin\nFrench FRA Indo-European Latin\nGerman DEU Indo-European Latin\nGreek ELL Indo-European Greek\nHausa HAU Afro-Asiatic Latin\nHindi HIN Indo-European Devanagari\nIgbo IBO Niger-Congo Latin\nIndonesian IND Austronesian Latin\nJapanese JPN Japonic Japanese\nKinyarwanda KIN Niger-Congo Latin\nKorean KOR Koreanic Korean\nLuganda LUG Niger-Congo Latin\nLuo LUO Nilo-Saharan Latin\nNaija Pidgin PCM English Creole Latin\nRussian RUS Indo-European Cyrillic\nSpanish SPA Indo-European Latin\nSwahili SWA Niger-Congo Latin\nTamil TAM Dravidian Tamil\nTelugu TEL Dravidian Telugu\nThai THA Kra-Dai Thai\nTurkish TUR Turkic Latin\nUrdu URD Indo-European Perso-Arabic\nVietnamese VIE Austro-Asiatic Latin\nWolof WOL Niger-Congo Latin\nYorùbá YOR Niger-Congo Latin\nTable 8: Overview of languages used in our experiments.\n28\nPublished as a conference paper at ICLR 2023\nDataset Download Link Reference\nUniversal Dependencies 2.10https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-4758(Zeman et al., 2022; Nivre et al., 2020)MasakhaNER https://github.com/masakhane-io/masakhane-ner/tree/main/data(Adelani et al., 2021)GLUE https://huggingface.co/datasets/glue (Wang et al., 2018)TyDiQA-GoldP https://huggingface.co/datasets/tydiqa (Clark et al., 2020)SQuADv1.1 https://huggingface.co/datasets/squad (Rajpurkar et al., 2016)KorQuAD 1.0 https://huggingface.co/datasets/squad_kor_v1(Lim et al., 2019)JaQuAD https://huggingface.co/datasets/SkelterLabsInc/JaQuAD(So et al., 2022)XNLI https://huggingface.co/datasets/xnli (Conneau et al., 2018)\nTable 9: Links and references to the datasets we used in our ﬁnetuning experiments.\nLanguage Treebank #Sentences Reference\nENG English-EWT 16621 Silveira et al. (2014)\nARA Arabic-PADT 7664 Haji ˇc et al. (2009)\nCOP Coptic-Scriptorium 2011 Zeldes & Abrams (2018)\nHIN Hindi-HDTB 16647 Palmer et al. (2009)\nJPN Japanese-GSD 8100 Asahara et al. (2018)\nKOR Korean-GSD 6339 Chun et al. (2018)\nTAM Tamil-TTB 600 Ramasamy & Žabokrtský (2012)\nVIE Vietnamese-VTB 3000 Nguyen et al. (2009)\nZHO Chinese-GSD 4997 Shen et al. (2016)\nTable 10: Overview of the Universal Dependencies v2.10 (Zeman et al., 2022; Nivre et al., 2020)\ntreebanks used in our POS tagging and dependency parsing experiments with the number of sen-\ntences in their respective training splits. As mentioned in §3.1, these treebanks were chosen with\ntypological and script diversity in mind.\nPARAMETER POS DP NER QA XNLI\nRendering backend PangoCairoClassiﬁcation head pooling — — — — CLSOptimizer AdamWAdamβ (0.9, 0.999)Adamε 1e−8Weight decay 0Learning rate 5e−5 {5e−5,8e−5} 5e−5 {3e−5,5e−5,7e−5} 2e−5Learning rate warmup steps 100 100 100 100 1000Learning rate schedule Linear decayMax sequence length 256 256 196 400 196Stride — — — 160 —Batch size 64 64 64 32 256Max steps 15000 15000 15000 20000 50000Early stopping ✓Eval steps 500 500 500 500 1000Dropout probability 0.1\nTable 11: Finetuning settings for POS tagging, dependency parsing ( DP), NER , QA, and XNLI . We\ndid not run a comprehensive hyperparameter search due to compute limitations; these settings were\nmanually selected based on a small number of preliminary runs. Maximum performance was often\nreached well before the speciﬁed number of max steps.\nPARAMETER MNLI QQP QNLI SST -2 COLA STS -B MRPC RTE WNLI\nRendering backend PangoCairo PyGame PangoCairo PyGame PyGame PyGame PyGame PyGame PyGameClassiﬁcation head pooling MeanOptimizer AdamWAdamβ (0.9, 0.999)Adamε 1e−8Weight decay 0Learning rate 3e−5 3e −5 3e −5 3e −5 2e −5 2e −5 3e −5 3e −5 1e −5Learning rate warmup steps 100 100 100 100 200 100 100 200 100Learning rate schedule Linear decayMax sequence length 256Batch size 64 256 64 256 256 64 64 64 256Max steps 15000 15000 15000 15000 15000 15000 15000 15000 400Early stopping ✓Eval interval 500 steps 500 steps 500 steps 500 steps 100 steps 100 steps 100 steps 250 steps 1 epochDropout probability 0.1\nTable 12: Finetuning settings for GLUE tasks. We did not run a comprehensive hyperparameter\nsearch due to compute limitations; these settings were manually selected based on a small number\nof preliminary runs. Increasing the batch size to 256 and switching to the PyGame renderer helped\nachieve more consistent convergence behaviour for some tasks. For the smaller datasets (to the right\nof QQP ), maximum performance was reached well before the speciﬁed number of max steps.\n29\nPublished as a conference paper at ICLR 2023\nG E XAMPLES OF Zeroé ORTHOGRAPHIC ATTACKS\nAttack Sentence\nNONE Penguins are designed to be streamlined\nCONFUSABLE\nSHUFFLE(INNER) Pegnuins are dnesiged to be sieatrnmledSHUFFLE(FULL) ngePnius rae dsgednei to be etimaslerndDISEMVOWEL Pngns r dsgnd to be strmlndINTRUDE Pe‘nguins a{re d)esigned t;o b*e stre<amlinedKEYBOARD TYPOPenguinz xre dwsigned ro ne streamllnedNATURAL NOISEPenguijs ard design4d ti bd streamlinfdTRUNCATE Penguin are designe to be streamlineSEGMENTATIONPenguinsaredesignedtobestreamlinedPHONETIC Pengwains’s ar dhiseind te be storimlignd\nTable 13: Examples of low-level orthographic attacks based on the Zeroé benchmark.\n40\n50\n60\n70\n80\n90\n100Accuracy [%]\nConfusable\n Disemvowel\n Shuffle (full)\n Shuffle (inner)\n Intrude\n0 20 50 80\n40\n50\n60\n70\n80\n90\n100Accuracy [%]\nKeyboard typo\n0 20 50 80\nNatural noise\n0 20 50 80\nPhonetic\n0 20 50 80\nSegmentation\n0 20 50 80\nTruncate\nModel\nBERTBASE\nPIXEL\nFigure 8: Test set accuracy for a single run of PIXEL and BERT across different levels of noise\nintroduced through various orthographic attacks in SNLI. The results show thatPIXEL is more robust\nthan BERT to most of these attacks.\n50\n75\n100Accuracy [%]\nConfusable\n Disemvowel\n Shuffle (full)\n50\n75\n100Accuracy [%]\nShuffle (inner)\n Intrude\n Keyboard typo\n0 20 50 80\n50\n75\n100Accuracy [%]\nNatural noise\n0 20 50 80\nPhonetic\n0 20 50 80\nTruncate\nModel\nBERTBASE\nPIXEL\nFigure 9: Test set accuracy for a single run of PIXEL and BERT across different levels of noise\nintroduced through various orthographic attacks in POS tagging. The results show that PIXEL is\nmore robust than BERT to most of these attacks, especially when dealing with visually-confusable\ncharacter substitutions. S EGMENTATION is not applied to the task of POS tagging, since the joined\nwords would not have a proper tag.\n30\nPublished as a conference paper at ICLR 2023\nH F ONT TRANSFER ANALYSIS\nIn this section, we analyse the adaptation capabilities of PIXEL to new fonts at ﬁnetuning\ntime. Speciﬁcally, we ﬁnetune PIXEL models for POS tagging and dependency parsing on the\nUD_English-EWT treebank and sentiment analysis on SST-2, once with a font similar to our\nGoNotoCurrent / NotoSans-Regularpretraining font, NotoSerif-Regular, and once with a font strik-\ningly different from it, JournalDingbats1. We compare the three fonts in Table 14 below:\nFont Rendered Example Sentence\nGoNotoCurrent\nNotoSerif-Regular\nJournalDingbats1\nTable 14: An example sentence rendered in three different fonts.\nGoNotoCurrent NotoSerif-Regular JournalDingbats1\nPOS 96.7 95.9 93.9\nDP 90.6 88.1 81.3\nSST-2 89.6 84.2 72.9\nTable 15: Results for ﬁne-tuning PIXEL for POS tagging, dependency parsing (DP), and sentiment\nanalysis on SST-2 with three different fonts: the font used in pretraining (GoNotoCurrent), a visually\nsimilar font (NotoSerif-Regular), and a highly dissimilar font ( JournalDingbats1). We report test\naccuracy for POS, test LAS for DP, and validation accuracy for SST-2, each averaged over 5 runs.\nThe font transfer results are shown in Table 15. We ﬁnd that PIXEL exhibits fairly high font transfer\nability out-of-the-box, i.e. without any font or image augmentation strategies employed during pre-\ntraining.34 In line with our expectations, transfer to a visually similar font ( NotoSerif-Regular) is\neasier than to a dissimilar font ( JournalDingbats1). Nevertheless, PIXEL is able to transfer surpris-\ningly well to the JournalDingbats1 font, in which every letter is simply mapped to the icon of an\nobject or animal.\nI F URTHER ANALYSIS\nTo investigate where PIXEL currently lags behind BERT , we analyse the impact that dependency\nlength has on both models in dependency parsing in ENG . We can see in Figure 10 that the LAS\ngap between BERT and PIXEL increases with longer dependencies, indicating that PIXEL struggles\nslightly more with long syntactic dependencies.\n#L θ ENG ARA BUL DEU ELL FRA HIN RUS SPA SWA THA TUR URD VIE ZHO\nMBERT 104 179M 83.3 73.2 77.9 78.1 75.8 78.5 70.1 76.5 79.7 67.2 67.7 73.3 66.1 77.2 77.7\nBERT 1 110M 83.7 64.8 69.1 70.4 67.7 72.4 59.2 66.4 72.4 62.2 35.7 66.3 54.5 67.6 46.2\nPIXEL 1 86M 77.2 58.9 66.5 68.0 64.9 69.4 57.8 63.4 70.3 60.8 50.2 64.0 54.1 64.8 52.0\nTable 16: Results for PIXEL and BERT ﬁnetuned on XNLI in the translate-train-all setting where\nwe train on the joint training data in all 15 languages, originally translated from ENG by Conneau\net al. (2018). We report test set accuracy averaged over 5 runs each. Despite the relatively large\nperformance gap in favor of BERT in ENG (which is in line with the GLUE results in Table 3),\nthe gap is much smaller for other languages, particularly those not using the Latin writing system.\nPIXEL is overall more consistent across scripts, outperforming BERT in THA and ZHO .\n34We believe such augmentation strategies would further improve robustness to font variations and leave this\nexperiment to future work. Considering that we have full control over the font when working with NLP text\ndatasets, robustness to font variations was not a primary goal in this work.\n31\nPublished as a conference paper at ICLR 2023\n1 2 [3, 6] >6\nDistance to Head [# words]\n70\n75\n80\n85\n90\n95\n100Labeled Attachment Score [%]\nModel\nBERTBASE\nPIXEL\nModel\nBERTBASE\nPIXEL\nFigure 10: LAS scores ( ENG ) across different dependency lengths averaged over 5 random intitial-\nizations of BERT and PIXEL . In ENG , long syntactic dependencies are more challenging for PIXEL .\nJ L IMITATIONS\nThis paper introduces a new approach to processing written language as images, which removes the\nneed for a ﬁnite vocabulary, providing a solution to the vocabulary bottleneck. While our results\nshow that PIXEL is a promising approach in this direction, this is only the ﬁrst step. Here, we\nhighlight current limitations and avenues for future work for pixel-based models:\n• PIXEL is pretrained on predominantly English text written in the Latin script. The choice of\nEnglish is driven by the scientiﬁc goal of comparing against a widely used model (English\nBERT ) but English may not be the best source language for cross-lingual transfer (Turc\net al., 2021; Blevins et al., 2022). We expect that PIXEL trained on typologically diverse\nlanguages in multiple scripts would considerably surpass the cross-script and cross-lingual\ntransferability of English-only PIXEL but this remains to be veriﬁed, and training a model\non large amounts of data will require large computational resources.\n• PIXEL currently seems to be less sample-efﬁcient than subword-based PLMs. PIXEL ex-\ncels at syntactic tasks after being pretrained for the same number of steps/datapoints as\nBERT (a challenging setup within an academic budget), but still lags behind in semantic\nprocessing. As a consequence, it also requires more training steps than BERT to converge\nduring ﬁnetuning. Closing this gap might involve longer pretraining with additional (long-\ndependency) objectives.\n• There are challenges to be addressed when working with languages written right-to-left.\nPIXEL currently processes sentences in such languages from the end to the beginning which\nmay lead to learning inadequate features for sentence separation and position embeddings.\n• PIXEL cannot be used for language generation tasks because it is not possible to produce\ndiscrete words from the pretrained decoder.\n• Rendering text as images requires more disk space than reading text from a ﬁle. This can be\nalleviated by caching the dataset in a compressed format, or rendering the images on-the-\nﬂy. Rendering images on-the-ﬂy will create additional overhead when training for multiple\nepochs.\n32",
  "topic": "Pixel",
  "concepts": [
    {
      "name": "Pixel",
      "score": 0.8270443677902222
    },
    {
      "name": "Computer science",
      "score": 0.7723797559738159
    },
    {
      "name": "Scripting language",
      "score": 0.7087579965591431
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5311089158058167
    },
    {
      "name": "Natural language processing",
      "score": 0.4885190725326538
    },
    {
      "name": "Language model",
      "score": 0.47457101941108704
    },
    {
      "name": "Encoder",
      "score": 0.4287474453449249
    },
    {
      "name": "Programming language",
      "score": 0.10582807660102844
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I124055696",
      "name": "University of Copenhagen",
      "country": "DK"
    },
    {
      "id": "https://openalex.org/I4210091519",
      "name": "Rockwool Foundation",
      "country": "DK"
    },
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I99464096",
      "name": "KU Leuven",
      "country": "BE"
    },
    {
      "id": "https://openalex.org/I139025015",
      "name": "Pioneer (United States)",
      "country": "US"
    }
  ]
}