{
  "title": "Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers",
  "url": "https://openalex.org/W3119303959",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3011077313",
      "name": "Machel Reid",
      "affiliations": [
        "National Institute of Advanced Industrial Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2167815369",
      "name": "Edison Marrese Taylor",
      "affiliations": [
        "National Institute of Advanced Industrial Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2102950735",
      "name": "Yutaka Matsuo",
      "affiliations": [
        "National Institute of Advanced Industrial Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3046835050",
    "https://openalex.org/W4288087322",
    "https://openalex.org/W2963247446",
    "https://openalex.org/W2996159613",
    "https://openalex.org/W2963434219",
    "https://openalex.org/W3114869109",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2763421725",
    "https://openalex.org/W2952809536",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W3006683367",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3039805635",
    "https://openalex.org/W2792376130",
    "https://openalex.org/W2990215755",
    "https://openalex.org/W4287812455",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2952509486",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3036120435",
    "https://openalex.org/W2952355681",
    "https://openalex.org/W2970900903",
    "https://openalex.org/W2963494889",
    "https://openalex.org/W2757047188",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2995154514",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2988975212",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W2894740066",
    "https://openalex.org/W2922709902",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2975381464",
    "https://openalex.org/W3203309275",
    "https://openalex.org/W2963938518",
    "https://openalex.org/W104184427",
    "https://openalex.org/W2962931466",
    "https://openalex.org/W2866343820",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W2981910001",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2963631907"
  ],
  "abstract": "Transformers have shown improved performance when compared to previous architectures for sequence processing such as RNNs. Despite their sizeable performance gains, as recently suggested, the model is computationally expensive to train and with a high parameter budget. In light of this, we explore parameter-sharing methods in Transformers with a specific focus on generative models. We perform an analysis of different parameter sharing/reduction methods and develop the Subformer. Our model combines sandwich-style parameter sharing, which overcomes naive cross-layer parameter sharing in generative models, and self-attentive embedding factorization (SAFE). Experiments on machine translation, abstractive summarization and language modeling show that the Subformer can outperform the Transformer even when using significantly fewer parameters.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4081–4090\nNovember 7–11, 2021. ©2021 Association for Computational Linguistics\n4081\nSubformer: Exploring Weight Sharing for Parameter Efﬁciency in\nGenerative Transformers\nMachel Reidϑ, Edison Marrese-Taylorℵ,ϑ, Yutaka Matsuoϑ\nϑThe University of Tokyo\nℵNational Institute of Advanced Industrial Science and Technology (AIST)\n{machelreid,emarrese,matsuo}@weblab.t.u-tokyo.ac.jp\nAbstract\nTransformers have shown improved perfor-\nmance when compared to previous architec-\ntures for sequence processing such as RNNs.\nDespite their sizeable performance gains, as\nrecently suggested, the model is computation-\nally expensive to train and with a high pa-\nrameter budget. In light of this, we explore\nparameter-sharing methods in Transformers\nwith a speciﬁc focus on generative models.\nWe perform an analysis of different parame-\nter sharing/reduction methods and develop the\nSubformer. Our model combines sandwich-\nstyle parameter sharing, which overcomes\nnaive cross-layer parameter sharing in genera-\ntive models, and self-attentive embedding fac-\ntorization (SAFE). Experiments on machine\ntranslation, abstractive summarization and lan-\nguage modeling show that the Subformer can\noutperform the Transformer even when using\nsigniﬁcantly fewer parameters.1\n1 Introduction\nRecent improvements in NLP tasks can be at-\ntributed to the Transformer (Vaswani et al., 2017)\nmodel. The model has led to better deeply con-\ntextualized representations (Devlin et al., 2019),\nbetter machine translation systems (Vaswani et al.,\n2017), and language models (Baevski and Auli,\n2019; Dai et al., 2019). Despite their success, one\nmain drawback of training these models is their\ncomputational cost, being a greatly limiting factor\nfor many, with training times and memory usage\nballooning as model sizes increase to attain better\nperformance.\nWith this in mind, there has been recent interest in\nmaking the Transformer more parameter-efﬁcient\nto reap its performance beneﬁts while making the\nmodel more computationally efﬁcient and able to\nscale better. Many approaches have focused on\n1https://github.com/machelreid/\nsubformer\nautomating model design with neural architecture\nsearch that aim at ﬁnding more efﬁcient Trans-\nformer variations using gradient descent (Wu et al.,\n2020; So et al., 2019; Mehta et al., 2020a). As such,\nthese techniques are expensive, requiring a signif-\nicant amount of GPU hours to ﬁnd good designs.\nIn contrast to these approaches, the model by (Lan\net al., 2020) has directly tackled model parameter\nreduction in the context of deeply contextualized\nword representations, still attaining similar (or bet-\nter) performance. In this paper, we take a similar\napproach and look to explore whether these ideas\ncan be applied to sequence-to-sequence models in\na simple manner by designing the Subformer.\nThe Subformer incorporates two novel techniques:\n(1) SAFE (Self-Attentive Factorized Embeddings),\nin which we use a small self-attention layer\nto reduce embedding parameter count, and (2)\nSandwich-style Parameter Sharing, in which we\ndevelop a simple and intuitive technique for param-\neter sharing to be effective in Transformer models.\nWe evaluate the Subformer on machine translation,\nabstractive summarization, and language modeling,\nshowing that our model can achieve similar or bet-\nter performance compared with a base/big Trans-\nformer with a ∼40% parameter reduction and min-\nimal modiﬁcation to the original architecture, rein-\nforcing the existing over-parameterization claims\n(Fan et al., 2020; Mehta et al., 2020a; Lan et al.,\n2020). On WMT’14 EN-DE we achieve a BLEU\nscore of 29.3, compared to Transformer-big’s 28.6\nwith 13M fewer parameters, and we also outper-\nform the Transformer-XL model with a signiﬁcant\n3.6 lower perplexity and 37% fewer parameters.\n2 The Subformer\nLet us start by deﬁning the notation to be used\nthroughout the paper. We refer to the model dimen-\nsion as dm, feed-forward projection dimension as\n⃗dm, the vocabulary size as V, and the number of\n4082\nlayers as L. Note that, unlike standard Transformer\nmodels, in which the embedding dimension is kept\nthe same as dm, we disentangle to embedding di-\nmension to reduce parameter count. For this reason,\nwe denote the embedding dimension to be de.\n2.1 SAFE: Self-Attentive Factorized\nEmbeddings\nWe propose to reduce the number of parameters\nin our embedding layers, which can take up to\n25% of the total parameter count (in the case of\nTransformer-base, Vaswani et al., 2017), using a\nsmall self-attention layer. Speciﬁcally, we look to\nreduce the embedding size by disentangling the\nmodel dimension from the embedding dimension,\nreducing the embedding dimension de, and then\nprojecting this to the model dimension dm using a\nsmall self-attention sub-layer followed by a feed-\nforward module.\nGiven vocabulary size V, the usage of a standard\nembedding layer would result in V ×dm parame-\nters. However, considering that the power of Trans-\nformers lies in their ability to learn contextual rep-\nresentations with attention, using a smaller de for\nnon-contextual embeddings and then projecting to\ndm is intuitively an effective method for parameter\nreduction (Lan et al., 2020). This results in a signif-\nicant parameter reduction for values of de ≪dm.\nS a n d wi c h \t M o d u l e \nL i n e a r \nM o de l \t L a y e r \nL i n e a r \nM o de l \t L a y e r \nS a n d wi c h \t M o d u l e   \nL i n e a r \nM o de l \t L a y e r \nL i n e a r \nM o de l \t L a y e r \nO u t p u t \t P r o j e c t i o n \nE mb e d d i n g\t l a y e r \nS AF E \nE mb e d d i n g\t l a y e r \nS AF E \nFigure 1: The Subformer with its four main compo-\nnents: (1) SAFE embeddings(colored blue) and output\nprojection layers, (2) model layers which are placed\nat the top and bottom of the model (colored red), (3)\nSandwich Module, in which a wider shared layer com-\nposes the central part of our encoder/decoder, (4) pro-\njection layers, which allow for the interaction between\nthe model layers and Sandwich Module despite their\ndifferent dimensions (colored yellow).\nMODEL Param. BLEU\nDeFINE (Mehta et al., 2020b) 52M 27.0\nde = 128, Linear 48M 26.0\nde = 256, Linear 53M 27.1\nde = 256, 2-Layer Linear 54M 27.2\nde = 128, SAFE 48M 26.6\nde = 256, SAFE 54M 27.7\nVaswani et al. (2017) 65M 27.3\nTRANSFORMER -BASE (reimpl.) 61M 27.7\nTable 1: Experiments on the impact on SAFE vs a reg-\nular linear projection using T RANSFORMER -BASE on\nthe WMT’14 EN-DE machine translation benchmark\nWhen using SAFE, the embedding layer is com-\nposed of a regular token →vector embedding ma-\ntrix E ∈RV ×de . This is followed by projecting\nthe embeddings (summed with the positional en-\ncodings (Vaswani et al., 2017), denoted by PE)\nto the model dimension dm using SAFE. Once we\nhave our SAFE embeddings, we feed them through\nthe ﬁrst model layer —the base of the sandwich.\nThe output of this ﬁrst layer is then projected to\nthe sandwich dimension ds. Once fed through the\nshared sandwich layers, we then project the output\nback to the model dimension using an MLP. The\noutput of the projection is then fed through the ﬁnal\nmodel layer to produce the output vectors.\nCurrent models (Baevski and Auli, 2019; Dai et al.,\n2019; Lan et al., 2020) often use a single linear\nprojection, i.e. V ×de + de ×dm. In contrast, we\nempirically show that simply contextualizing this\nprojection with a small self-attention layer results\nin stronger performance with a minimal addition\nof parameters —especially in the encoder-decoder\ncase, where the input embedding layer and output\nprojection are often tied (Press and Wolf, 2017)\n(Table 1).\n2.2 Sandwich-style Parameter Sharing\nWeight sharing techniques, despite being surpris-\ningly effective, have been relatively unexplored in\nthe context of generative Transformers. However,\nthis has been shown to be powerful for leverag-\ning models with large capacity and less memory\nusage/computation (Wu et al., 2019; Lan et al.,\n2020).\nGiven that the output of each Transformer layer de-\npends directly on its two sub-layers —multiheaded\nattention and the feedforward module— when dis-\ncussing alternatives for parameter sharing across\n4083\nMODEL Param. BLEU\nAll-Shared 24M 14.3\nAll-Shared, dm = 768 41M 22.0\nAll-Shared (Independent FFN) 27M 22.4\nAll-Shared (except last) 31M 23.2\nEvery 2 layers shared 38M 27.2\nSANDWICH 38M 27.3\nSANDWICH , L = 8 38M 27.7\nVaswani et al. (2017) 65M 27.3\nTRANSFORMER -BASE (Our reimpl.) 61M 27.7\nTable 2: Experiments performed on WMT’14 EN-DE\nusing different parameter sharing techniques.\ntransformer layers there are several options. As we\naim to leverage the aforementioned properties of\nweight sharing, we performed preliminary experi-\nments, investigating the capabilities of weight shar-\ning in the following ﬁve settings: (1) All-shared\nNaively sharing all encoder and all decoder layers\n—that is including both of their sub-layers, follow-\ning Lan et al. (2020); Dehghani et al. (2019). (2)\nAll-Shared (Independent FFN) Naively sharing\nall encoder and all decoder layers but allowing\neach layer l∈[2,·,L] to have an independent feed-\nforward sub-layer. (3) All-Shared (except last)\nSharing weights across layers l ∈[1,...,L −1]\nsuch that layer L remains independent. (4) Ev-\nery 2 layers shared Sharing every two layers, i.e.\n[1,2],[3,4],[5,6] in the case of a 6-layer trans-\nformer. (5) Sandwich Finally, we only share the\nmiddle or central layers (i.e. 2 ≤l ≤L−1),\nleaving layers 1 and Lto have independent sets of\nparameters.\nTable 2 summarizes the results of our exploratory\nstudy. As can be seen, naive parameter shar-\ning/tying approaches do not offer any advantages,\nhurting performance signiﬁcantly ( ∼50%) when\ncompared to the regular Transformer. However,\nour results also show that when combined properly,\nusing Sandwich-style parameter sharing, we can\nattain a good balance of parameter reduction and\nperformance. When compared to tasks such as pre-\ntraining deep contextualized word representations,\ngenerative tasks such as machine translation require\ninformative token-level representations for each in-\nput token to be accurately translated. In this con-\ntext, we surmise that the success of Sandwich-style\nparameter sharing on this sequence-to-sequence\ntask is a result of it being able to have the input and\noutput layers (arguably, the most important layers)\nbe trained independently, allowing them to learn\ndifferent operations than the sandwich layers.\nCombined Architecture Having introduced our\nproposed parameter reduction techniques, we will\nnow explain the Subformer architecture. The Sub-\nformer is composed of four main components, for\nboth the encoder and decoder: the embedding layer,\nthe model layers, the sandwich module and the pro-\njection layers. We disentangle the sandwiched layer\ndimension from that of the model layer, allowing\nthe sandwich layer width to be larger than the rest\nof the model. For this reason, we denote the di-\nmension of the sandwiched layer to be ds and its\ncorresponding feed-forward dimension to be ⃗ds.\n3 Experimental Setup\nWe apply our method to a variety of sequence mod-\neling tasks: neural machine translation, summa-\nrization, and language modeling. We compare\nTransformer-base and big (Vaswani et al., 2017)\nwith the Subformer trained in the same setting. We\nalso include simple sandwich-style parameter shar-\ning (denoted as Sandwich-{base,big}) and the us-\nage of only SAFE as an ablation of what these tech-\nniques do in their naive forms when decoupled. Ad-\nditional implementation and training details with\nhyperparameter settings are in the appendix.\nMachine Translation We evaluate our model on\ntwo standard translation benchmarks: WMT’14\nEnglish-German (EN-DE) and WMT’16 English-\nRomanian (EN-RO). Following previous work, we\nevaluate all models using tokenized BLEU (Pap-\nineni et al., 2002). In order to better contextualize\nour results, we consider parameter-efﬁcient mod-\nels DeLighT (Mehta et al., 2020a) (contemporane-\nous work), and the Evolved Transformer (So et al.,\n2019).\nAbstractive Summarization We test the\nmodel’s ability to process long documents on\nthe CNN-DailyMail summarization benchmark\n(Hermann et al., 2015; Nallapati et al., 2016),\ncomprising over 280K news articles paired with\nmulti-sentence summaries. We also compare\neffects of BART (Lewis et al., 2020) pretraining\n(details in Appendix B). For this task we contex-\nutalize our results with specialized architectures\nsuch as Pointer-Generator Networks (See et al.,\n2017), and methods leveraging pretraining:\nRobertaShare (Rothe et al., 2020), BertExtAbs\n(Liu and Lapata, 2019), and BART (Lewis et al.,\n4084\nBASEMODELS WMT’14 EN-DE WMT’16 EN-RO\nParam. BLEU Param. BLEU\nDeLighT 37M 27.6 22M 34.3\nEvolved Transformer 48M 27.7 — —\nDeLighT 54M 28.0 52M 34.7\nEvolved Transformer 64M 28.2 — —\nTransformer-base (orig) 65M 27.3 62M 34.2 †\nTransformer-base (ours) 61M 27.7 62M 34.1\nSandwich-base 38M 27.3 — —\nOnly SAFE,de = 256 54M 27.7 — —\nSUBFORMER-SMALL 38M 27.7 20M 34.1\nSUBFORMER-BASE 52M 28.1 48M 34.7\nSUBFORMER-MID 63M 28.5 — —\nTable 3: Results on WMT’14 EN-DE and WMT’16\nEN-RO task, for our base models. The † superscript\nindicates results from Kasai et al. (2020).\nBIG MODELS Param. BLEU\nEvolved Transformer 222M 29.0\nTransformer-big (orig) 213M 28.4\nTransformer-big (ours) 210M 28.6\nSandwich-big 122M 28.6\nSUBFORMER -LARGE 197M 29.3\nTable 4: Results on WMT’14 EN-DE for large models.\n2020). We evaluate using ROUGE 1,2,L (Lin,\n2004).\nLanguage Modeling We evaluate on the large-\nscale Wikitext-103 dataset (Merity et al., 2017).\nModels are evaluated in terms of perplexity on\nthe test portion. In order to better contextualize\nour results, we consider the QRNN (Merity et al.,\n2018), Transformer-XL (Dai et al., 2019) and Deep\nEquilibrium Model (DEQ) (Bai et al., 2019), which\nalso employs parameter sharing.\n4 Results\nMachine Translation Tables 3 and 4 2 summa-\nrize our results on machine translation. Firstly,\nwe note that our Transformer baselines outperform\nVaswani et al. (2017) (base: 27.3→27.7, big: 28.4\n→28.6). We surmise that this is due to training for\nlonger and with a larger batch size.\nThe Subformer outperforms our Transformer base-\nlines when trained in the same setting, with simi-\nlar or fewer parameters. SUBFORMER -SMALL re-\nduces parameters by 40%, matching performance\n2In all tables, results from other work used to contextualize\nour results are placed above the double bar.\nwith our Transformer baselines. SUBFORMER -\nBASE and MID , outperform our model signiﬁ-\ncantly (0.4, 0.8 BLEU) when using a fewer/similar\nnumber of parameters. Furthermore, we note\nthat SUBFORMER -MID performs similarly to the\nTransformer-big model (210M params.) in Table 4,\ndespite a 70% parameter reduction.\nFor our large set of models (Table 4), Sandwich-big\nachieves the same performance as our Transformer-\nbig reimplementation, but with 40% fewer param-\neters. We believe this to be an indication of the\ncapability of Sandwich-style parameter sharing as\nthe encoder/decoder layers get wider, while also\nproviding further evidence for the overparameteri-\nzation of the Transformer. Subformer-large, with\n197M parameters achieves a signiﬁcant 0.7 BLEU\nscore gain over Transformer-big, despite using\n13M fewer parameters.\nLanguage Modeling Results for language mod-\neling can be seen in Table 5. The Subformer\nuses adaptive input embeddings (Baevski and Auli,\n2019) instead of SAFE embeddings, following\ncommon practice. We also train two Transformer\nbaselines with the same setup —one with the same\namount of parameters and another with a similar\nparameter count to Transformer-XL— to provide\nbetter context for comparison. Task-speciﬁc tech-\nniques that can be applied during training, such as\ncaching (Dai et al., 2019) or other methods applied\nduring inference time (Khandelwal et al., 2020;\nKrause et al., 2018) can further improve all models\nso we do not focus on these.\nMODEL Param. CL PPL\nQRNN 151M — 33.00\nDeLighT 99M 480 24.14\nTransformer-XL 151M 640 24.03\nDEQ 110M — 23.20\nAdaptive Inputs 247M 480 19.03\nAdaptive Inputs (4 Layer) 96M 480 26.42\nAdaptive Inputs (8 Layer) 146M 480 22.32\nSUBFORMER 83M 480 20.88\n96M 480 20.39\n122M 480 19.90\nTable 5: Results on the Wikitext-103 benchmark. CL\nstands for Context Length.\nAs seen in Table 5, the Subformer outperforms\nthe baselines by a signiﬁcant margin (between 1.4\nand 6.5 perplexity), with a signiﬁcant reduction in\nparameters.\n4085\nMODEL Params. R1 R2 RL\nPtr-Gen+Cov — 39.5 17.3 36.4\nRobertaShare 152M 40.3 18.9 37.6\nBertExtAbs 220M 41.7 19.4 38.8\nBART 406M 44.2 21.3 40.9\nTransformer (3 Layer) 57M 40.0 17.5 36.7\nTransformer 77M 40.1 17.6 36.8\nTransformer-BART 77M 41.2 18.7 37.6\nSUBFORMER -BASE 57M 40.9 18.3 37.7\nSUBFORMER -BART 57M 41.6 19.2 38.4\nTable 6: Results on CNN-Daily Mail.\nModel Param. Iterations Dev. PPL\nAdaptive Inputs 146M 272K 22.31\nSUBFORMER 83M 97K 20.84\nTable 7: Iterations to convergence on WIKITEXT -103\nAbstractive Summarization For the\nCNN/Daily Mail summarization task we use\nSubformer-base. We also pretrain a Transformer\nand Subformer model with the same architecture\non Wikipedia (details in Appendix B). As can be\nseen in Table 6, the Subformer outperforms two\nTransformer baselines with both the same param-\neter count and its respective Transformer-base\nconﬁguration in both settings, demonstrating the\nSubformer’s performance on a variety of tasks and\nwith longer sequences.\nDiscussion on Speed and Convergence We\nfound training time to consistently speed up by\n10-30%, and inference speed to either be faster by\n10-20% (keeping ds = dm) to be slightly slower by\n10-30% (when ds ≫dm) (due to more operations,\nsimilar to Lan et al. (2020)). The Subformer con-\nverges faster most likely due to fewer parameters\nto optimize. Given the fewer number of parame-\nters, it can be expected for the models to converge\nwith fewer iterations. We test this on the task of\nlanguage modeling, where we found that the Sub-\nformer converged 65% faster than its Transformer\ncounterpart, as shown in Table 7. We also measure\ninference speed on our machine translation models\n(Table 8).\n5 Conclusion\nIn this paper we have presented the Subformer, a\nparameter-efﬁcient Transformer-based generative\nmodel primarily based on two simple parameter\nfactorization/sharing techniques. Our empirical re-\nModel Param. Speed ↑ BLEU\nDeLighT 37M 0.30x 27.6\nTransformer 61M 1.00x 27.7\nSAFE, de = 256 54M 1.17x 27.7\nSANDWICH -BASE 38M 1.26x 27.3\nSUBFORMER -BASE 52M 0.75x 28.1\nTable 8: Inference speed for our models measured on a\nsingle V100 GPU on WMT’14 En-De (batch size: 384,\n1.00x = 5135 tokens)\nsults on three sequence modeling tasks show that\nthe Subformer can achieve similar or better perfor-\nmance compared with a base/big Transformer with\na ∼40% parameter reduction. Furthermore, the\nsimplicity of our approach allows the Subformer to\nbe used in conjunction with other parameter reduc-\ntion techniques in the literature, for even smaller\nbut performant models. We hope our work in-\ncites interest in parameter sharing techniques for\nan even wider range of Transformer models, ulti-\nmately helping reduce their computational cost in\ngeneral.\nEthical Considerations\nThis work has impact in the ﬁeld of natural lan-\nguage processing, and develops a more efﬁcient\napproach for learning performant generative mod-\nels. As with much of language technology has\nthe potential to be both used for good and used\nmaliciously. We also experiment with pretraining,\nlearning representations in an unsupervised way,\nwhich is likely to capture and amplify biases found\nin the data. However, our approach has a potential\npositive impact given the lower cost and energy\nexpenditure needed to train our proposed model.\nAcknowledgments\nWe thank Jorge Balazs, Yusuke Iwasawa, Jungo\nKasai, Cristian Rodriguez-Opazo, Alfredo Solano,\nYutaro Yamada, and Victor Zhong for their helpful\nfeedback and discussions over this work. MR is\ngrateful to the Masason Foundation for their sup-\nport.\nReferences\nAlexei Baevski and Michael Auli. 2019. Adaptive in-\nput representations for neural language modeling. In\n7th International Conference on Learning Representa-\ntions, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019. OpenReview.net.\n4086\nShaojie Bai, J. Zico Kolter, and Vladlen Koltun. 2019.\nDeep equilibrium models. In Advances in Neural Infor-\nmation Processing Systems 32: Annual Conference on\nNeural Information Processing Systems 2019, NeurIPS\n2019, December 8-14, 2019, Vancouver, BC, Canada,\npages 688–699.\nRobin Cheong and Robel Daniel. 2019. Transformers.\nzip: Compressing transformers with pruning and quan-\ntization. Technical report, Stanford University, Stan-\nford, California.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond a\nﬁxed-length context. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational Lin-\nguistics, pages 2978–2988, Florence, Italy. Association\nfor Computational Linguistics.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Lukasz Kaiser. 2019. Univer-\nsal transformers. In 7th International Conference on\nLearning Representations, ICLR 2019, New Orleans,\nLA, USA, May 6-9, 2019. OpenReview.net.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pages 4171–\n4186, Minneapolis, Minnesota. Association for Com-\nputational Linguistics.\nZi-Yi Dou, Zhaopeng Tu, Xing Wang, Shuming Shi,\nand Tong Zhang. 2018. Exploiting deep representa-\ntions for neural machine translation. In Proceedings\nof the 2018 Conference on Empirical Methods in Natu-\nral Language Processing, pages 4253–4262, Brussels,\nBelgium. Association for Computational Linguistics.\nSergey Edunov, Alexei Baevski, and Michael Auli.\n2019. Pre-trained language model representations for\nlanguage generation. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pages\n4052–4059, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAngela Fan, Edouard Grave, and Armand Joulin. 2020.\nReducing transformer depth on demand with structured\ndropout. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net.\nPrakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali\nKhan, Yin Yang, Deming Chen, Marianne Winslett,\nHassan Sajjad, and Preslav Nakov. 2020. Compressing\nLarge-Scale Transformer-Based Models: A Case Study\non BERT. arXiv:2002.11985 [cs, stat].\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. 2019. Mask-predict: Parallel decod-\ning of conditional masked language models. In Pro-\nceedings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Process-\ning (EMNLP-IJCNLP), pages 6112–6121, Hong Kong,\nChina. Association for Computational Linguistics.\nKarl Moritz Hermann, Tomás Kociský, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In Advances in Neural Information\nProcessing Systems 28: Annual Conference on Neural\nInformation Processing Systems 2015, December 7-12,\n2015, Montreal, Quebec, Canada, pages 1693–1701.\nJungo Kasai, James Cross, Marjan Ghazvininejad, and\nJiatao Gu. 2020. Non-autoregressive machine transla-\ntion with disentangled context transformer. InProceed-\nings of the 37th International Conference on Machine\nLearning, ICML 2020, 13-18 July 2020, Virtual Event,\nvolume 119 of Proceedings of Machine Learning Re-\nsearch, pages 5144–5155. PMLR.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generaliza-\ntion through memorization: Nearest neighbor language\nmodels. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Interna-\ntional Conference on Learning Representations, ICLR\n2015, San Diego, CA, USA, May 7-9, 2015, Conference\nTrack Proceedings.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. In 8th\nInternational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\nOpenReview.net.\nBen Krause, Emmanuel Kahembwe, Iain Murray, and\nSteve Renals. 2018. Dynamic evaluation of neural se-\nquence models. In Proceedings of the 35th Interna-\ntional Conference on Machine Learning, volume 80\nof Proceedings of Machine Learning Research, pages\n2766–2775, Stockholmsmässan, Stockholm Sweden.\nPMLR.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut. 2020.\nALBERT: A lite BERT for self-supervised learning of\nlanguage representations. In 8th International Confer-\nence on Learning Representations, ICLR 2020, Addis\nAbaba, Ethiopia, April 26-30, 2020. OpenReview.net.\nJason Lee, Elman Mansimov, and Kyunghyun Cho.\n2018. Deterministic non-autoregressive neural se-\nquence modeling by iterative reﬁnement. In Proceed-\nings of the 2018 Conference on Empirical Methods in\nNatural Language Processing, pages 1173–1182, Brus-\nsels, Belgium. Association for Computational Linguis-\ntics.\nNamhoon Lee, Thalaiyasingam Ajanthan, and Philip\nH. S. Torr. 2019. Snip: single-shot network prun-\ning based on connection sensitivity. In 7th Interna-\n4087\ntional Conference on Learning Representations, ICLR\n2019, New Orleans, LA, USA, May 6-9, 2019. OpenRe-\nview.net.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020. BART:\nDenoising sequence-to-sequence pre-training for natu-\nral language generation, translation, and comprehen-\nsion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7871–7880, Online. Association for Computational\nLinguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summarization\nBranches Out, pages 74–81, Barcelona, Spain. Associ-\nation for Computational Linguistics.\nYang Liu and Mirella Lapata. 2019. Text summa-\nrization with pretrained encoders. arXiv preprint\narXiv:1908.08345.\nSachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2020a.\nDelight: Very deep and light-weight transformer.\nSachin Mehta, Rik Koncel-Kedziorski, Mohammad\nRastegari, and Hannaneh Hajishirzi. 2020b. Deﬁne:\nDeep factorized input token embeddings for neural se-\nquence modeling. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018. An analysis of neural language model-\ning at multiple scales.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture models.\nIn 5th International Conference on Learning Represen-\ntations, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings. OpenReview.net.\nPaulius Micikevicius, Sharan Narang, Jonah Alben,\nGregory F. Diamos, Erich Elsen, David García, Boris\nGinsburg, Michael Houston, Oleksii Kuchaiev, Ganesh\nVenkatesh, and Hao Wu. 2018. Mixed precision train-\ning. In 6th International Conference on Learning\nRepresentations, ICLR 2018, Vancouver, BC, Canada,\nApril 30 - May 3, 2018, Conference Track Proceedings.\nOpenReview.net.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos,\nÇa˘glar Gulçehre, and Bing Xiang. 2016. Abstractive\ntext summarization using sequence-to-sequence RNNs\nand beyond. In Proceedings of The 20th SIGNLL Con-\nference on Computational Natural Language Learning,\npages 280–290, Berlin, Germany. Association for Com-\nputational Linguistics.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A fast, extensible toolkit for se-\nquence modeling. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association\nfor Computational Linguistics (Demonstrations), pages\n48–53, Minneapolis, Minnesota. Association for Com-\nputational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalua-\ntion of machine translation. In Proceedings of the 40th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 311–318, Philadelphia, Pennsylva-\nnia, USA. Association for Computational Linguistics.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga,\nAlban Desmaison, Andreas Köpf, Edward Yang,\nZachary DeVito, Martin Raison, Alykhan Tejani,\nSasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie\nBai, and Soumith Chintala. 2019. Pytorch: An im-\nperative style, high-performance deep learning library.\nIn Advances in Neural Information Processing Systems\n32: Annual Conference on Neural Information Pro-\ncessing Systems 2019, NeurIPS 2019, December 8-14,\n2019, Vancouver, BC, Canada, pages 8024–8035.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nGabriele Prato, Ella Charlaix, and M. Rezagholizadeh.\n2019. Fully Quantized Transformer for Improved\nTranslation. ArXiv.\nOﬁr Press and Lior Wolf. 2017. Using the output em-\nbedding to improve language models. In Proceedings\nof the 15th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Volume 2,\nShort Papers, pages 157–163, Valencia, Spain. Associ-\nation for Computational Linguistics.\nSascha Rothe, Shashi Narayan, and Aliaksei Severyn.\n2020. Leveraging pre-trained checkpoints for sequence\ngeneration tasks. Transactions of the Association for\nComputational Linguistics, 8:264–280.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), pages 1073–1083, Van-\ncouver, Canada. Association for Computational Lin-\nguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715–1725,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nDavid R. So, Quoc V . Le, and Chen Liang. 2019. The\nevolved transformer. In Proceedings of the 36th In-\n4088\nternational Conference on Machine Learning, ICML\n2019, 9-15 June 2019, Long Beach, California, USA,\nvolume 97 of Proceedings of Machine Learning Re-\nsearch, pages 5877–5886. PMLR.\nIlya Sutskever, James Martens, George E. Dahl, and\nGeoffrey E. Hinton. 2013. On the importance of initial-\nization and momentum in deep learning. In Proceed-\nings of the 30th International Conference on Machine\nLearning, ICML 2013, Atlanta, GA, USA, 16-21 June\n2013, volume 28 of JMLR Workshop and Conference\nProceedings, pages 1139–1147. JMLR.org.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all you\nneed. In Advances in Neural Information Processing\nSystems 30: Annual Conference on Neural Information\nProcessing Systems 2017, December 4-9, 2017, Long\nBeach, CA, USA, pages 5998–6008.\nFelix Wu, Angela Fan, Alexei Baevski, Yann N.\nDauphin, and Michael Auli. 2019. Pay less attention\nwith lightweight and dynamic convolutions. In 7th In-\nternational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nZhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song\nHan. 2020. Lite transformer with long-short range at-\ntention. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net.\nOﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8BERT: Quantized 8Bit BERT.\narXiv:1910.06188 [cs].\n4089\nA Designing the Subformer\nNaming The Subformer is a play on words, ref-\nerencing its small size - i.e. sub-, as well as the\nSandwich-style parameter sharing technique, refer-\nencing the type of sandwich.\nArchitecture When using SAFE, our parameter\ncount would result in V ×de + 5d2\ne + de ×dm\nparameters, where the ﬁrst term represents the em-\nbedding layer, the value 5d2\ne groups the query, key,\nand value projections and 2 output feed-forward\nlayers, and de ×dm represents the linear projec-\ntion from the embedding dimension to the model\ndimension. As mentioned in the paper, this results\nin a signiﬁcant parameter reduction for values of\nde ≪dm.\nAs we tie the decoder’s output projection layer (re-\nturning a distribution over the vocabulary) with the\ndecoder’s input embedding matrix, we project the\ndecoder’s last hidden state (with dimensiondm) to\nde using a two layer MLP. Also, when we perform\nencoder attention in the decoder’s Sandwich Mod-\nule, we simply linearly project the query from the\ndecoder from ds to dm and then project it back to\nds once the attention operation is complete.\nMemory Footprint Table 9 summarizes the\nmemory footprint of our proposed techniques. In\nthis table, the beneﬁts of Sandwich-style parame-\nter sharing can be seen as the number of indepen-\ndent layers is controlled to be L ≤3, however,\nTransformers generally need to be deeper (with\na standard of L = 6) to learn more meaningful\nrepresentations with the parameter count scaling\nlinearly with respect to the layer count. Similarly,\nthe beneﬁts of disentangling the model dimension\nfrom the embedding dimension can be seen as well.\nDue to the parameter reduction attained by these\ntechniques, the models can be trained in memory-\nconstrained scenarios with a larger batch size.\nB Data and Training Details\nTraining was done on 8 GPUs on a single DGX-\n1 Machine, while training on 16 GPUs was done\nusing multiple compute nodes of a compute cluster.\nWe train all base/small models on 8 NVIDIA Tesla\nV100 GPUs. For all big/large models, we train on\n16 NVIDIA Tesla V100 GPUs. All models were\ntrained with mixed precision (Micikevicius et al.,\n2018) and are implemented in PyTorch (Paszke\net al., 2019) using our modiﬁcation of fairseq\n(Ott et al., 2019).\nMachine Translation We train using 8192 to-\nkens per GPU an update frequency of 2, for small,\nbase models. For large models, we train on 16\nGPUs with 4096 tokens per GPU with an update\nfrequency of 2. We follow the training setup of\nGhazvininejad et al. (2019): we use the same\nweight initialization scheme as BERT (Devlin et al.,\n2019), sampling weights from N(0,0.02), initial-\nizing biases to zero and setting layer normalization\nparameters β and γ to be 0 and 1, respectively.\nFor regularization we use the best of [0.1,0.2,0.3]\ndropout, weight decay of 0.01, while using label-\nsmoothed cross entropy loss with ϵ = 0.1. We\ntrain using an effective batch size of 128K tokens.\nThe models are trained using Adam (Kingma and\nBa, 2015), with hyper-parameters β = (0.9,0.999)\nand ϵ= 10−6. We warm up the learning rate to a\npeak of 5 ×10−4 within 10K iterations and then\ndecay the learning rate with the inverse square root\nschedule. When creating the ﬁnal model, we use\nthe checkpoint with the lowest loss on the devel-\nopment set and generate using a beam size of 5\n(Vaswani et al., 2017), tuning the length penalty\nof α∈[0.0,0.2,. . .,2.0] in the validation set. We\nperform early stopping, training for a maximum of\n250K iterations.\nWe use the following settings for our models: (1)\nSubformer-small has dm = 512, ds = 768, de =\n256 and L= 8, (2) Subformer-base has dm = 512,\nds = 1024, ⃗ds = 3072, de = 320, (3) Subformer-\nmid has dm = 768, ds = 768, de = 350 and\n(4) Subformer-large has dm = 1024, ds = 2048\nand de = 512. For WMT’16 EN-RO, our small\nmodel has dm = 320, ds = 512 and de = 192\nand our base model has dm = 512, ds = 640, and\nde = 384.\nIn terms of datasets, we make use of the same pre-\nprocessed data used by Ghazvininejad et al. (2019)\nfor WMT’14 EN-DE with a 32K BPE (Sennrich\net al., 2016) vocabulary and during evaluation we\nperform de-hyphenation (Vaswani et al., 2017). We\nuse the same data as Lee et al. (2018) for WMT’16\nEN-RO with a 35K BPE vocabulary.\nAbstractive Summarization We follow\nEdunov et al. (2019) and use the ofﬁcial\nROUGE-1.5.5.pl script with parameters -m\n-a -n 2. As mentioned in the paper, our model\nconﬁguration is the same as Subformer-base,\n4090\nMethod Embedding Memory Usage Model Memory Usage\nTransformer dm ×V L (4d2\nm + 2\n(⃗dm ×dm)\n)\nSandwich (naive) — 3(4d2\nm + 2(4⃗dm ×dm)\n)\nSubformer de ×V + 5d2\ne + de ×dm 2\n(\n4d2\nm + 2(⃗dm ×dm)\n)\n+\n(\n4d2\ns + 2\n(⃗ds ×ds + 2(ds ×dm)\n))\nTable 9: Memory space required by each method given a stack of encoder layers.Sandwich (naive)refers to simply\nperforming Sandwich style parameter sharing with no other modiﬁcations to the architecture.\nbut we set de = 256. Articles are truncated to\n400 tokens (See et al., 2017) and we use a BPE\nvocabulary of 32K types (Edunov et al., 2019).\nWe follow the training schedule of Edunov et al.\n(2019). During inference, we tune generation\nlength in the range of {40, 50, 60} and use tri-gram\nblocking, following standard practice. When\npretraining, we pretrain on Wikipedia (14GB) for\n100K iterations, using a batch size of 512K tokens.\nWe use a learning rate of 7e-4, warmed up over\n10K iterations.\nLanguage Modeling When training our lan-\nguage models, we use 8 GPUs with 3072 tokens\nper GPU and an update frequency of 3, follow-\ning Baevski and Auli (2019). Models are trained\nusing Nesterov’s accelerated gradient optimizer\n(Sutskever et al., 2013), warming up the learn-\ning rate to 1.0 for 16K iterations, and then an-\nnealing for 270K iterations using a cosine anneal-\ning schedule. We use three conﬁgurations: (1)\ndm = 768,⃗dm = 3072,ds = 1536,⃗ds = 6144,\n(2) dm = 768,⃗dm = 4096and ds = 2048,⃗ds =\n6144 and (3) dm = 1024,⃗dm = 4096 and ds =\n2048,⃗ds = 6144. All models use L = 12. Our\nconsidered dataset, Wikitext-103, contains 103M\ntokens and has a vocabulary of nearly 270K.\nC Extended Related Work\nImproving Transformers Given the effective-\nness of the Transformer, improving the architecture\nhas been of much interest to the NLP community.\nWithin this domain, one branch of research con-\ncerns the reduction of the quadratic complexity\n(w.r.t. sequence length) of the Transformer’s core\nself-attention mechanism (Wu et al., 2019; Kitaev\net al., 2020), pushing it down to linear or log-linear\ncomplexity. The second branch of research regards\nimproving the expressiveness of Transformer mod-\nels, by using more layers (Dou et al., 2018), or\nby improving the architecture (Wu et al., 2019; So\net al., 2019). The third branch of research regards\nimproving the parameter efﬁciency of Transform-\ners. Approaches towards this goal include neural\narchitecture search approaches (So et al., 2019;\nWu et al., 2020), where new Transformer-based\narchitectures are learned using gradient descent,\nmore manually crafted approaches (Dehghani et al.,\n2019; Mehta et al., 2020a), as well as weight-\nsharing approaches (Lan et al., 2020; Wu et al.,\n2019). The work most similar to ours is ALBERT\n(Lan et al., 2020) in which complete weight sharing\nis used to pre-train deep contextualized word repre-\nsentations (Peters et al., 2018; Devlin et al., 2019).\nDifferent from this work, we focus on common\nNLP generative/sequence-to-sequence tasks versus\nlarge-scale pre-training and develop an approach to\nincrease model capacity while reducing parameter\nfootprint tailored to this setting.\nCompressing Transformers We ﬁnd prior work\non pruning and quantizing Transformer models\nto reduce their size with a focus on sequence-to-\nsequence settings like machine translation (Prato\net al., 2019), on encoder-based methods like BERT\n(Zafrir et al., 2019; Ganesh et al., 2020) or with a\nmore generic scope in mind (Cheong and Daniel,\n2019; Lee et al., 2019). Our approach is orthogo-\nnal to these since we directly aim at reducing the\nnumber of parameters of Transformer models by\nproposing architecture modiﬁcations and weight\nsharing techniques.\nReducing Embedding Dimensionality in Se-\nquence Models As embeddings can substantially\nincrease the parameter count as the vocabulary size\nincreases, especially in sequence modeling scenar-\nios, embedding reduction techniques have been pro-\nposed, including using a linear projection to project\nto a lower dimension (Baevski and Auli, 2019; Dai\net al., 2019) or using combinations of block sparse\ntransformations (Mehta et al., 2020b,a). We pro-\npose a self-attention based projection layer, SAFE,\nwhich we empirically show to outperform the afore-\nmentioned linear projection methods with a similar\nparameter count.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7984902858734131
    },
    {
      "name": "Computer science",
      "score": 0.7379821538925171
    },
    {
      "name": "Automatic summarization",
      "score": 0.7201536297798157
    },
    {
      "name": "Machine translation",
      "score": 0.6205766797065735
    },
    {
      "name": "Language model",
      "score": 0.5399060845375061
    },
    {
      "name": "Generative model",
      "score": 0.5296929478645325
    },
    {
      "name": "Generative grammar",
      "score": 0.5272666215896606
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49177291989326477
    },
    {
      "name": "Machine learning",
      "score": 0.44382357597351074
    },
    {
      "name": "Embedding",
      "score": 0.42583560943603516
    },
    {
      "name": "Engineering",
      "score": 0.09202083945274353
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}