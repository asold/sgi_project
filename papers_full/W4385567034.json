{
  "title": "AraLegal-BERT: A pretrained language model for Arabic Legal text",
  "url": "https://openalex.org/W4385567034",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2763386275",
      "name": "Muhammad Al-Qurishi",
      "affiliations": [
        "Riyadh Elm University"
      ]
    },
    {
      "id": "https://openalex.org/A5000066367",
      "name": "Sarah Alqaseemi",
      "affiliations": [
        "Riyadh Elm University"
      ]
    },
    {
      "id": "https://openalex.org/A3198638504",
      "name": "Riad Souissi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4386124662",
    "https://openalex.org/W4220941681",
    "https://openalex.org/W4213227026",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3008110149",
    "https://openalex.org/W2471147443",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W3199624848",
    "https://openalex.org/W3119989665",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W3133440961",
    "https://openalex.org/W2742947407",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4282586470",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W3099950029"
  ],
  "abstract": "The effectiveness of the bidirectional encoder representations from transformers (BERT) model for multiple linguistic tasks is well documented. However, its potential for a narrow and specific domain, such as legal, has not been fully explored. In this study, we examine the use of BERT in the Arabic legal domain and customize this language model for several downstream tasks using different domain-relevant training and test datasets to train BERT from scratch. We introduce AraLegal-BERT, a bidirectional encoder transformer-based model that has been thoroughly tested and carefully optimized with the goal of amplifying the impact of natural language processing-driven solutions on jurisprudence, legal documents, and legal practice. We fine-tuned AraLegal-BERT and evaluated it against three BERT variants for the Arabic language in three natural language understanding tasks. The results showed that the base version of AraLegal-BERT achieved better accuracy than the typical and original BERT model concerning legal texts.",
  "full_text": "Proceedings of the Natural Legal Language Processing Workshop 2022, pages 338 - 344\nDecember 8, 2022 ©2022 Association for Computational Linguistics\nAraLegal-BERT: A pretrained language model for Arabic Legal text\nMuhammad AL-Qurishi, Sarah AlQaseemi and Riad Soussi\nElm Company, Research Department, Riyadh 12382, Saudi Arabia\nAbstract\nThe effectiveness of the bidirectional encoder\nrepresentations from transformers (BERT)\nmodel for multiple linguistic tasks is well docu-\nmented. However, its potential for a narrow\nand specific domain, such as legal, has not\nbeen fully explored. In this study, we examine\nthe use of BERT in the Arabic legal domain\nand customize this language model for sev-\neral downstream tasks using different domain-\nrelevant training and test datasets to train BERT\nfrom scratch. We introduce AraLegal-BERT, a\nbidirectional encoder transformer-based model\nthat has been thoroughly tested and carefully\noptimized with the goal of amplifying the im-\npact of natural language processing-driven so-\nlutions on jurisprudence, legal documents, and\nlegal practice. We fine-tuned AraLegal-BERT\nand evaluated it against three BERT variants for\nthe Arabic language in three natural language\nunderstanding tasks. The results showed that\nthe base version of AraLegal-BERT achieved\nbetter accuracy than the typical and original\nBERT model concerning legal texts.\n1 Introduction\nThe impressive performance of bidirectional\nencoder representations from transformers\n(BERT) (Devlin et al., 2018) inspired numerous\nauthors to try and improve the original BERT. Such\nfollow-up research progresses in several directions,\nincluding the development of specific solutions\nfor various thematic domains. This is necessary\nbecause the vocabulary used in some fields signifi-\ncantly differs from the language used for everyday\npurposes and may contain the specific meanings of\ncertain phrases or atypical relationships between\ncontextual elements. This problem can be partially\nresolved through domain-specific adjustments\nto the training process. A good example of this\napproach is demonstrated in (Chalkidis et al.,\n2020), who created Legal-BERT specifically\nfor mining legal text in English, improving the\noutput of the standard transformer algorithm in\nthis domain. Another form of the BERT concept\nwas successfully adapted by (Beltagy et al., 2019;\nLee et al., 2020), who created models that were\npretrained on a compilation of scientific and\nbiomedical data from various fields, achieving\nsignificantly better performance on scientifically\nrelated natural language processing (NLP) tasks.\nThese examples show that BERT is currently far\nfrom a finished model and that its effectiveness\ncould be further enhanced, at least for relatively\nnarrowly defined tasks.\nFor the Arabic language, several BERT-based\nmodels have consistently demonstrated superior\nperformance on numerous linguistic tasks requiring\nsemantic understanding, outperforming all bench-\nmarks on public datasets such as the Arabic NER\ncorpus (ANERcorp) or the Arabic Reading Com-\nprehension Dataset (ARCD), such as the works\npresented by (Antoun et al., 2020; Abdul-Mageed\net al., 2020) and mBERT by Google (Devlin et al.,\n2018). This is largely a consequence of the efficient\ntransfer learning inherent in this model, involving\na high computational cost because this approach\nrequires huge collections of training examples, fol-\nlowed by fine-tuning for specific downstream tasks.\nA significant advantage of BERT is that the training\nphase can be skipped because a pretrained version\nof the model can be used and trained further. How-\never, (Chalkidis et al., 2020; Beltagy et al., 2019;\nLee et al., 2020) has shown that a generic approach\nto pretraining does not work well when BERT must\nbe used in a domain with highly specific terminol-\nogy, such as legal, science, or medicine. There\nare two possible responses to this issue: continue\nspecializing in a pretrained model or train a model\nfrom scratch with relevant materials from the do-\nmain. In this study, we built a language model from\nscratch based on the original BERT (Devlin et al.,\n2018), which is specific to Arabic legal texts. The\naim was to improve the performance on most state-\n338\nof-the-art language understanding and processing\ntasks, especially related to Arabic legal texts. We\nbelieve that the specific nature of legal documents\nand terminology needs to be considered because\nit affects the way sentences and paragraphs are\nconstructed in this field. The extent of formal and\nsemantic differences is such that some authors de-\nscribe the linguistic content used for legal matters\nas almost a language of its own (Zhang et al., 2022;\nSilveira et al., 2021).\nBy focusing on a single domain, the Arabic legal\ntext, this study attempts to reveal means of adapting\nan NLP model to fit any thematic domain. Based\non our experiments, we can confirm that pretrain-\ning BERT with examples from the Arabic legal\ndomain from scratch provides a better foundation\nfor working with documents containing Arabic le-\ngal vocabulary than using the vanilla version of\nthe algorithm. We introduce AraLegal-BERT, a\ntransformer-based model that has been thoroughly\ntested and carefully optimized with the goal of\namplifying the impact of NLP-driven solutions on\njurisprudence, legal documents, and legal practice.\nWe fine-tuned AraLegal-BERT and evaluated it\nagainst three BERT variants for Arabic in three\nnatural language understanding (NLU) tasks. The\nresults show that the base version of AraLegal-\nBERT achieved better accuracy than the general\nand original version of the BERT model, in regard\nto legal text. AraLegal-BERT is a particularly ef-\nficient model that can keep up with the output of\ncomputationally intensive models while producing\nits findings faster and using far fewer resources.\nConsequently, the base version of the model was\nobserved to have the ability to achieve compara-\nble accuracy to larger versions of the large general\nand original version of the BERT model when they\nwere trained with domain-relevant examples simi-\nlar to those used to test our model.\n2 Related work\nIn a very short time, the transformer (Vaswani\net al., 2017) architecture has become the gold stan-\ndard for machine learning methods in the field of\nlinguistics (Wolf et al., 2020; Su et al., 2022). The\nunprecedented success of BERT combined with its\nflexibility has led to a proliferation of tools based\non it, built with a more narrowly defined vocab-\nulary (Young et al., 2018; Brown et al., 2020).\nAraBERT (Antoun et al., 2020; Abdul-Mageed\net al., 2020) is an example of such specialization\nand could have considerable practical value, given\nthe number of Arabic speakers worldwide. Because\nthe model is trained for some of the most common\nNLP tasks and has proven effective across regional\nvariations in morphology and syntax, this language\nmodel has the potential to become a standard tool\nfor analyzing Arabic text. The pretraining and fine-\ntuning procedures described in this work may not\nbe optimal; however, the output of the localized\nmodel clearly indicated that the initial approach\nwas correct. With further refinement, the model\ncan become sufficiently reliable for a wide range of\nreal-world applications. However, these models are\nbased on data, most of which were collected from\nModern Standard Arabic terms, and these language\nmodels may fail when the language switches to col-\nloquial dialects (Abdelali et al., 2021). In addition,\nthe performance of these language models can be\naffected when dealing with a language for a spe-\ncific domain with special terms, such as scientific,\nmedical, and legal terms (Yeung, 2019).\nThe majority of domain-specific BERT models\nare related to scientific or medical texts, and legal\ntexts; however, the texts are all in English (Belt-\nagy et al., 2019; Lee et al., 2020; Chalkidis et al.,\n2020). In a study by (Alsentzer et al., 2019), the\nmain area of interest was clinical practice; there-\nfore, the authors developed two different variants\nby pretraining basic BERT and BIOBERT with ex-\namples from this domain, with positive results in\nboth cases. Another interesting related project was\nconducted by (Beltagy et al., 2019), resulting in the\ncreation of SCIBERT, a whole branch of variations\noptimized for use with scientific documents. In this\ncase, two different optimization strategies, includ-\ning additional training and training from scratch\nusing documents comprising scientific terminol-\nogy, were tested, with both approaches yielding\nmeasurable improvements. A study by (Chalkidis\net al., 2020) involved pretraining transformer mod-\nels for English legal text by comparing three pos-\nsible approaches to adapt BERT to thematic con-\ntent niches: 1) using the vanilla model without\nany modifications, 2) introducing pretraining with\ndatasets that contain examples from the target do-\nmain in addition to the standard training, and 3)\nusing only domain-relevant samples to train BERT\nfrom scratch.\nEssentially, all the adaptations of the standard\nBERT model that involve fine-tuning use the same\napproach to select hyperparameters as outlined in\n339\nthe original BERT formulation, without even ques-\ntioning it. Another research gap is observed re-\ngarding the possibility of using shallow models\nto perform domain-specific tasks. The impressive\ngeneralizability of deep models with several layers\ncould be argued as wasted when the model operates\nin a narrowly defined field where linguistic rules\nare more streamlined and vocabulary volume more\nlimited. Despite BERT being the most successful\ndeep learning model for various tasks related to\nthe legal sphere, there have been no published at-\ntempts to develop a unique variation for this type of\ncontent, especially in Arabic, inspiring this study.\nTherefore, this is the first study to build a BERT-\nbased language model for legal texts in Arabic.\n3 AraLegal-BERT: Transformer Model\nPretrained with Arabic Legal Text\nTo optimize BERT to work with Arabic legal doc-\numents, we followed the same procedures in the\noriginal BERT model (Devlin et al., 2018); how-\never, for the Arabic language, we followed the same\nprocedure in AraBERT (Antoun et al., 2020).\n3.1 Dataset\nDue to the relative scarcity of publicly available\nresources containing legal text in Arabic, the train-\ning dataset had to be manually collected from nu-\nmerous sources and included several regional vari-\nations. All the collected documents were in the\nArabic language and related to different subfields\nof legal practice, such as legislative documents, ju-\ndicial documents, contracts and legal agreements,\nIslamic rules, and Fiqh. All data were collected\nfrom public sources, and the final size of the dataset\nwas 4.5 GB. The final size of the training set after\nremoving duplicates was approximately 13.7 mil-\nlion sentences. Table 1 lists the dataset used in this\nstudy.\n3.2 AraLegal-BERT\nThis version of the model was created by follow-\ning the original pretraining process with additional\nsteps involving textual material from the Arabic\nlegal domain. The authors of the original BERT\nmodel indicated that 100,000 steps would be suffi-\ncient; however, in our implementation, the model\nwas trained with up to half a million steps to de-\ntermine the impact of extended pretraining with\nnarrowly focused data samples on the performance\nof various linguistic tasks. The pretraining of the\nBERT base model (Devlin et al., 2018) with general\ncontent involves significantly more steps; therefore,\nthe model tends to be the most proficient, with a vo-\ncabulary containing approximately 30,000 words,\nfound in everyday speech. With extended training\nusing domain-focused examples, this tendency was\npresumed to have the ability to be partially reversed\nwith a positive impact on model accuracy.\nBefore we started the training, the data was pre-\nprocessed, and in this phase, we followed the same\nprocedure as in (Antoun et al., 2020). To account\nfor the uniqueness of Arabic prefixes, subword seg-\nmentation was performed to separate all tokens into\nprefixes, stems, and suffixes, as explained in (Ab-\ndelali et al., 2016). This resulted in a vocabulary\nof approximately 64,000 words used to pretrain the\nmodel and create AraLegal-BERT. Subsequently,\nwe trained our model using the masked language\nmodeling (MLM) task, where 15% of the words\nin an entire input sequence were used as tokens\nbecause 80% of them were masked, 10% were re-\nplaced with a random token, and only 10% were\nleft in their natural state. This procedure allows the\nalgorithms to derive conclusions based on whole\nwords and not just linguistic elements; this proce-\ndure is better suited for the Arabic language.\n4 Experimental procedure\n4.1 Pretraining stage\nAraLegal-BERT was trained for approximately 50\nepochs, involving a total of half a million steps,\nwhich is similar to the original BERT pretraining\nprocedure. We trained our model at the Elm Re-\nsearch Center using NVIDIA DGX1 with eight\nGPUs. The batch size was set to 8 per GPU; there-\nfore, the total training batch size (w. parallel, dis-\ntributed & accumulation) was 512. The maximum\nsequence length was 512 tokens, and the learning\nrate ranged from 1e − 5 to 5e − 5.\n4.2 Fine-tuning\nThe authors of BERT (Devlin et al., 2018) proposed\nan approach for determining the optimal parame-\nters for fine-tuning based on a search within a lim-\nited range. In this concept, the learning rate, train-\ning duration, size of the training stack, and dropout\nrate are either fixed or can be one of a few possi-\nble discreet values. Although no particular reason\nwas provided for this approach, it has been widely\nreplicated in studies dealing with BERT deriva-\ntives (Wehnert et al., 2022; Rogers et al., 2020).\n340\nTable 1: Dataset used to train AraLegal-BERT\nType Sample Size Desc\nBooks 6K Master and PhD theses, research papers, magazines, dictionaries and Fiqh books\nCases 336K Legal Cases in KSA and Gulf countries which consists of copy rights, design rights, facts and appealing\nTerms and laws 3K Laws and regulations in KSA and Gulf countries\nothers 5K Reports and studies, academic courses, forms, reports, contracts\nBecause these parameters do not always produce\nthe best results, and their use can still leave a model\nundertrained, an alternative strategy was adopted\nto choose the upper limit of training epochs that\ntracks the loss of validation and terminates training\nonly when the conditions are met.\n4.2.1 Legal text classification\nThe samples used in the experimental dataset were\ncollected from two main portals. The first dataset\nwas collected from the Scientific Judicial Por-\ntal(SJP)1, operated by the Ministry of Justice in\nSaudi Arabia. The SJP is the largest specialized in-\nformation database in the field of justice in the\nKingdom of Saudi Arabia. It is the ideal so-\nlution for specialists, including judges, lawyers,\ntrained lawyers, academics, prosecutors, and grad-\nuate students, in the justice and legal domain. The\nsecond dataset collected was from the Board of\nGrievances(BoG) portal2, where the following is\nstated in their website: “The Board tested a ju-\ndiciary academic series in the name of (judge li-\nbrary) and its distribution among the Board judges\n(hard copy and soft copy) to increase cognitive\nformation with them, a state which its effect shall\nbe reflected on the judiciary verdicts they issue,\nincluding academic references in administrative,\ncommercial and penal judiciary formed of 32 vol-\numes in addition to judiciary verdicts”.\nBecause existing documents in both datasets can\nbelong to multiple categories depending on the sub-\nmission details, they are suitable for the task of\nclassifying lengthy legal documents. Three differ-\nent classes of documents were selected from the\nSJP dataset and ten classes from the BoG dataset.\nBecause all documents from certain classes are es-\nsentially headlined in the same manner, the classifi-\ncation task required that the parts of the document\ncontaining easily identifiable indicators of the class\nwere trimmed. Owing to this omission, the model\nneeds to analyze the entire content instead of deriv-\ning the conclusion based on just the first few lines.\nThis modification was implemented for all classes.\n1https://sjp.moj.gov.sa/\n2https://www.bog.gov.sa/en/ScientificContent/Pages/default.aspx\n4.2.2 Keyword Extraction\nUnfortunately, compared with the data available\nfor research in English and a few Latin languages,\nthere are no ready-made and well-prepared data for\nresearch purposes in Arabic, especially for under-\nstanding the natural languages of legal texts. There-\nfore, we built our dataset for this task with the help\nof professionals in the Arab legal domain. This\ndataset consists of approximately 8,000 legal doc-\numents containing the most important keywords\nmanually extracted by these professionals. We pre-\nprocessed and cleaned the data and extracted ap-\nproximately 37640 sentences containing keywords\nand other words that formed the sentences. The av-\nerage length of the sentences was no more than 65\nwords because we performed a sentence segmen-\ntation process to ensure that each sentence did not\nlose its meaning or was not trimmed. We tagged\nthe keywords in the sentence with the number 1\nand the others with the number 0.\n4.2.3 Named Entity Recognition\nThis dataset was also generated in the research de-\npartment of Elm, Saudi Arabia. It contains more\nthan 311,000 sentences, including thousands of dis-\ntinct entities of 17 different sequence tags manually\nlabeled by multiple human annotators as a part of\nour CourtNLP project at Elm research 3. All the\nclasses used are shown in Figure 1. The main ob-\njective of the NER procedure is to assign a label\nbelonging to a particular class to each of the in-\ncluded words. Furthermore, some complex named\nentities can span multiple words; however, they are\nalways contained within a single sentence. The\nIOB format (short for inside, outside, beginning) is\npredominantly used to represent the sentences in\nthis field, with words starting with the name of an\nentity marked as B, internally located words as I,\nand other tokens as O.\n3https://www.elm.sa/en/research-and-\ninnovation/Pages/Research.aspx\n341\nTable 2: Overall results of all fine-tuned models in the legal text classification task on BoG Dataset\nModel / Macro-Average Precision Recall F1-score\nArabertv2-Large (Antoun et al., 2020) 0.850387 0.810795 0.827078\nARBERT (Abdul-Mageed et al., 2020) 0.802514 0.821973 0.812820\nmBERT (Devlin et al., 2018) 0.702017 0.635928 0.598267\nAraLegal-BERT (base) 0.89276 0.89173 0.89098\nTable 3: Overall results of all fine-tuned models in the legal text classification task on SJP Dataset\nModel / Macro-Average Precision Recall F1-score\nArabertv2-Large (Antoun et al., 2020) 0.885678 0.886816 0.884516\nARBERT (Abdul-Mageed et al., 2020) 0.837714 0.834804 0.843827\nmBERT (Devlin et al., 2018) 0.814763 0.780226 0.782501\nAraLegal-BERT (base) 0.92395 0.92133 0.92210\nFigure 1: Main Arabic Legal Named Entity Tags\n5 Results\n5.1 Impact of pretraining\nWe trained two models from scratch: the first was a\nbase model that contains 12 layers, and the second\nwas a large model that consists of 24 layers, similar\nto the original BERT. As anticipated, the full-sized\n24-layer model trained from scratch had a signif-\nicantly better ability than that of the base model\nwith 12 layers, to meet the pretraining objectives.\nHowever, after the completion of the pretraining\nstage, the base model displayed a level of loss simi-\nlar to that of the original BERT model trained with\ngeneral datasets. In particular, a model’s ability to\nadapt to narrowly defined niches is faster, which\ncan be a significant advantage for domain-focused\napplications such as those used for the legal do-\nmain. Therefore, the content of the training set\nplays a role in choosing the appropriate training\nmethod. We are yet to perform experiments on\nthe large model, and all fine-tuning results were\nbased on the base model because we found that\nit provides significantly higher accuracy than the\ngeneral Arabic BERT models in the three defined\nNLU tasks.\n5.2 Results and discussion\nWe divided the datasets for all three tasks into train-\ning, validation, and testing sets. In this section, we\ndiscuss the test results. The evaluation was con-\nducted using standardized hyperparameters, such\nas batch size and sequence length, with different\ndatasets suitable for legal text classification, key-\nword extraction, and named entity recognition.\nThe best option for the first task of legal text\nclassification is determined based on experimental\nresults. For example, using this method, we found\nthat multiple strategies could be used to bypass\nBERT’s sequence length limitation of 512 tokens;\nhowever, the “head & tails” strategy, where only\nthe first 128 and the last 382 tokens are retained,\nexhibits the best performance, such as the work\nin (Sun et al., 2019). Tables 2 and 3 summarize the\noverall results of our model compared with those\nof the three BERT variants for the Arabic language\non the classification task with two datasets, namely\nSJP and BoG. On the BoG dataset, AraLegal-BERT\noutperformed all models by 0.7% in terms of F1-\nMacro average, which is higher than ARABERT-\nv2large. Similarly, our model also outperformed\n342\nthe other models on the SJP dataset; it achieved\napproximately 0.4% higher F1-Macro average than\nthat of ArabBERTv2-large.\nFurthermore, for the tasks related to named en-\ntity recognition and keyword extraction, we fol-\nlowed the same procedure that was performed in\nour previous work (Al-Qurishi and Souissi, 2021);\nconsidering that no new layers were added to the\nmodel, a linear layer was used to make the words\nand sequence-level tagging. The results were ex-\ntremely different for these two tasks; furthermore,\nthere was a significant difference between the per-\nformance of AraLegal-BERT and the other models;\nAraLegal-BERT achieved 21% higher F1-Macro\naverage than ARABERT-v2large (Antoun et al.,\n2020) in extracting named entities, as shown in\nTable 5. In addition, the difference was signifi-\ncantly higher in the keyword extraction task, where\nAraLegal-BERT outperformed the highest model,\nARBERT (Abdul-Mageed et al., 2020), with a sig-\nnificant difference of almost 26% in F1-Macro av-\nerage, as shown in Table 4.\nWe denote that the general BERT models ex-\nhibited not only a low F1-Macro score but also a\nlow recall-macro score, where they were not able\nto retrieve most of the required words compared\nwith those retrieved by AraLegal-BERT. Finally,\nwe would like to highlight that AraLegal-BERT\nis the base version; yet, it outperformed the rest\nof the models in all three defined tasks, with low\nmemory requirement, faster performance, and good\naccuracy.\n6 Conclusions and future work\nOur experimental results show that a BERT model\npretrained for a specific domain is better than the\ntypical language models, for specific NLU tasks.\nTherefore, we present AraLegal-BERT, which was\ntrained exclusively for Arabic legal texts and is\ncapable of making highly accurate predictions on\nthree main NLU tasks: legal text classification,\nnamed entity recognition, and keyword extraction.\nEssentially, the level of difficulty of a task is corre-\nlated with the gains from choosing the right training\nstrategy as the importance of domain-specific vo-\ncabulary and semantics becomes more pronounced.\nThe tested version of AraLegal-BERT is the base,\ncost-efficient version suitable for a broad range of\nArabic legal text applications. Our future work\nwill focus on additional possibilities for pretrain-\ning other models, such as the Electra, Roberta and\nXLM-R models for several NLU tasks in the Arabic\nlegal domain with small, base, and large versions.\nAcknowledgements\nThis study was supported and funded by the re-\nsearch department of Elm.\nReferences\nAhmed Abdelali, Kareem Darwish, Nadir Durrani, and\nHamdy Mubarak. 2016. Farasa: A fast and furious\nsegmenter for arabic. In Proceedings of the 2016 con-\nference of the North American chapter of the associ-\nation for computational linguistics: Demonstrations,\npages 11–16.\nAhmed Abdelali, Sabit Hassan, Hamdy Mubarak, Ka-\nreem Darwish, and Younes Samih. 2021. Pre-training\nbert on arabic tweets: Practical considerations. arXiv\npreprint arXiv:2102.10684.\nMuhammad Abdul-Mageed, AbdelRahim Elmadany,\nand El Moatez Billah Nagoudi. 2020. Arbert &\nmarbert: deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785.\nMuhammad Saleh Al-Qurishi and Riad Souissi. 2021.\nArabic named entity recognition using transformer-\nbased-crf model. In Proceedings of The Fourth In-\nternational Conference on Natural Language and\nSpeech Processing (ICNLSP 2021), pages 262–271.\nEmily Alsentzer, John R Murphy, Willie Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, and Matthew\nMcDermott. 2019. Publicly available clinical bert\nembeddings. arXiv preprint arXiv:1904.03323.\nWissam Antoun, Fady Baly, and Hazem Hajj.\n2020. Arabert: Transformer-based model for\narabic language understanding. arXiv preprint\narXiv:2003.00104.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert:\nA pretrained language model for scientific text. arXiv\npreprint arXiv:1903.10676.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nIlias Chalkidis, Manos Fergadiotis, Prodromos Malaka-\nsiotis, Nikolaos Aletras, and Ion Androutsopoulos.\n2020. Legal-bert: The muppets straight out of law\nschool. arXiv preprint arXiv:2010.02559.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\n343\nTable 4: Overall results of all fine-tuned models in the Keywords Extraction Task\nModel / Macro-Average Precision Recall F1-score\nArabertv2-Large (Antoun et al., 2020) 0.765979 0.470098 0.582625\nARBERT (Abdul-Mageed et al., 2020) 0.789460 0.524715 0.630420\nmBERT (Devlin et al., 2018) 0.721356 0.375075 0.493533\nAraLegal-BERT (base) 0.93481 0.84449 0.88736\nTable 5: Overall results of all fine-tuned models in the Named Entity Recognition Task\nModel / Macro-Average Precision Recall F1-score\nArabertv2-Large (Antoun et al., 2020) 0.891934 0.450073 0.598261\nARBERT (Abdul-Mageed et al., 2020) 0.889916 0.413266 0.564423\nmBERT (Devlin et al., 2018) 0.886825 0.326475 0.477254\nAraLegal-BERT (base) 0.89848 0.73644 0.80943\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in bertology: What we know about\nhow bert works. Transactions of the Association for\nComputational Linguistics, 8:842–866.\nRaquel Silveira, CG Fernandes, João A Monteiro Neto,\nVasco Furtado, and José Ernesto Pimentel Filho.\n2021. Topic modelling of legal documents via\nlegal-bert. Proceedings http://ceur-ws org ISSN ,\n1613:0073.\nXing Su, Shan Xue, Fanzhen Liu, Jia Wu, Jian Yang,\nChuan Zhou, Wenbin Hu, Cecile Paris, Surya Nepal,\nDi Jin, et al. 2022. A comprehensive survey on com-\nmunity detection with deep learning. IEEE Transac-\ntions on Neural Networks and Learning Systems.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019. How to fine-tune bert for text classification? In\nChina national conference on Chinese computational\nlinguistics, pages 194–206. Springer.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nSabine Wehnert, Shipra Dureja, Libin Kutty, Viju Sudhi,\nand Ernesto William De Luca. 2022. Applying bert\nembeddings to predict legal textual entailment. The\nReview of Socionetwork Strategies, 16(1):197–219.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2020. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020 con-\nference on empirical methods in natural language\nprocessing: system demonstrations, pages 38–45.\nChin Man Yeung. 2019. Effects of inserting domain\nvocabulary and fine-tuning bert for german legal lan-\nguage. Master’s thesis, University of Twente.\nTom Young, Devamanyu Hazarika, Soujanya Poria, and\nErik Cambria. 2018. Recent trends in deep learning\nbased natural language processing. ieee Computa-\ntional intelligenCe magazine, 13(3):55–75.\nGechuan Zhang, Paul Nulty, and David Lillis. 2022.\nEnhancing legal argument mining with domain\npre-training and neural networks. arXiv preprint\narXiv:2202.13457.\n344",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8023476600646973
    },
    {
      "name": "Transformer",
      "score": 0.7516818046569824
    },
    {
      "name": "Arabic",
      "score": 0.7090736627578735
    },
    {
      "name": "Natural language processing",
      "score": 0.7031367421150208
    },
    {
      "name": "Language model",
      "score": 0.6612899899482727
    },
    {
      "name": "Encoder",
      "score": 0.6455350518226624
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6009941101074219
    },
    {
      "name": "Natural language",
      "score": 0.44707292318344116
    },
    {
      "name": "Linguistics",
      "score": 0.37271031737327576
    },
    {
      "name": "Engineering",
      "score": 0.06981348991394043
    },
    {
      "name": "Philosophy",
      "score": 0.0625823438167572
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}