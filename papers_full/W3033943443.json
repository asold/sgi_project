{
  "title": "Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers",
  "url": "https://openalex.org/W3033943443",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4227960036",
      "name": "Choromanski, Krzysztof",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286795959",
      "name": "Likhosherstov, Valerii",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224379242",
      "name": "Dohan, David",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221655750",
      "name": "Song, Xingyou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288842166",
      "name": "Gane, Andreea",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222125935",
      "name": "Sarlos, Tamas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2803903413",
      "name": "Hawkins, Peter",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287315884",
      "name": "Davis, Jared",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3206232744",
      "name": "Belanger, David",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224202663",
      "name": "Colwell, Lucy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202139665",
      "name": "Weller, Adrian",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2963858333",
    "https://openalex.org/W2999008758",
    "https://openalex.org/W2921646015",
    "https://openalex.org/W3123330721",
    "https://openalex.org/W3144345593",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W2787365188",
    "https://openalex.org/W2964045208",
    "https://openalex.org/W2611669587",
    "https://openalex.org/W3101438731",
    "https://openalex.org/W3136918052",
    "https://openalex.org/W2972498556",
    "https://openalex.org/W2995575179",
    "https://openalex.org/W2995197005",
    "https://openalex.org/W2907502844",
    "https://openalex.org/W2752892152",
    "https://openalex.org/W2937843571",
    "https://openalex.org/W2969624041",
    "https://openalex.org/W2906625520",
    "https://openalex.org/W2965046076",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2470673105",
    "https://openalex.org/W2993794083",
    "https://openalex.org/W2866343820",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W3015531900",
    "https://openalex.org/W1979762151",
    "https://openalex.org/W2144902422",
    "https://openalex.org/W2507756961",
    "https://openalex.org/W2342766640",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2107867854",
    "https://openalex.org/W3021677849",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2143462372",
    "https://openalex.org/W2945461886",
    "https://openalex.org/W2051545676",
    "https://openalex.org/W3025165719",
    "https://openalex.org/W3007746272",
    "https://openalex.org/W2966463509",
    "https://openalex.org/W3105575731",
    "https://openalex.org/W2957157683",
    "https://openalex.org/W2948223045",
    "https://openalex.org/W2544176167",
    "https://openalex.org/W2983902802",
    "https://openalex.org/W2605135824",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2899971035",
    "https://openalex.org/W2919624000",
    "https://openalex.org/W2957874522",
    "https://openalex.org/W2783173735",
    "https://openalex.org/W2922189677",
    "https://openalex.org/W2949202705",
    "https://openalex.org/W2922142804",
    "https://openalex.org/W2955058313"
  ],
  "abstract": "Transformer models have achieved state-of-the-art results across a diverse range of domains. However, concern over the cost of training the attention mechanism to learn complex dependencies between distant inputs continues to grow. In response, solutions that exploit the structure and sparsity of the learned attention matrix have blossomed. However, real-world applications that involve long sequences, such as biological sequence analysis, may fall short of meeting these assumptions, precluding exploration of these models. To address this challenge, we present a new Transformer architecture, Performer, based on Fast Attention Via Orthogonal Random features (FAVOR). Our mechanism scales linearly rather than quadratically in the number of tokens in the sequence, is characterized by sub-quadratic space complexity and does not incorporate any sparsity pattern priors. Furthermore, it provides strong theoretical guarantees: unbiased estimation of the attention matrix and uniform convergence. It is also backwards-compatible with pre-trained regular Transformers. We demonstrate its effectiveness on the challenging task of protein sequence modeling and provide detailed theoretical analysis.",
  "full_text": "Masked Language Modeling for Proteins via Linearly\nScalable Long-Context Transformers\nKrzysztof Choromanski∗1, Valerii Likhosherstov∗2, David Dohan∗1, Xingyou Song∗1\nAndreea Gane∗1, Tamas Sarlos∗1, Peter Hawkins∗1, Jared Davis∗3\nDavid Belanger1, Lucy Colwell1,2, Adrian Weller2,4\n1Google 2University of Cambridge 3DeepMind 4Alan Turing Institute\nAbstract\nTransformer models have achieved state-of-the-art results across a diverse range of\ndomains. However, concern over the cost of training the attention mechanism to\nlearn complex dependencies between distant inputs continues to grow. In response,\nsolutions that exploit the structure and sparsity of the learned attention matrix\nhave blossomed. However, real-world applications that involve long sequences,\nsuch as biological sequence analysis, may fall short of meeting these assumptions,\nprecluding exploration of these models. To address this challenge, we present a\nnew Transformer architecture, Performer, based on Fast Attention Via Orthogonal\nRandom features (FA VOR). Our mechanism scales linearly rather than quadratically\nin the number of tokens in the sequence, is characterized by sub-quadratic space\ncomplexity and does not incorporate any sparsity pattern priors. Furthermore, it\nprovides strong theoretical guarantees: unbiased estimation of the attention matrix\nand uniform convergence. It is also backwards-compatible with pre-trained regular\nTransformers. We demonstrate its effectiveness on the challenging task of protein\nsequence modeling and provide detailed theoretical analysis.\n1 Introduction and related work\nTransformers [46, 18] are powerful neural network architectures that have become SOTA in several\nareas of machine learning including Natural Language Processing (NLP) (e.g. speech recognition\n[33]), Neural Machine Translation (NMT) [ 7], document generation/summarization, time series\nprediction, generative modeling (e.g. image generation [37]), music generation [27], and analysis\nof biological sequences [40, 34, 28, 21, 20]. Transformers rely on a trainable attention mechanism\nthat identiﬁes complex dependencies between the elements of each input sequence (e.g. amino acids\nwithin a protein). Unfortunately, a standard Transformer scales quadratically with the number of\ntokens Lin the input sequence, which is prohibitively expensive for large L. Several solutions have\nbeen proposed to address this issue [2, 24, 4, 8, 1]. Most approaches restrict the attention mechanism\nto attend to local neighborhoods [37] or incorporate structural priors on attention such as sparsity [8],\npooling-based compression [38] clustering/binning/convolution techniques (e.g. [42] which applies\nk-means clustering to learn dynamic sparse attention regions, or [29], where locality sensitive hashing\nis used to group together tokens of similar embeddings), sliding windows [2], or truncated targeting\n[5]. Thus these approaches do not aim to approximate regular attention, but rather propose simpler\nand more tractable attention mechanisms, often by incorporating additional constraints (e.g. identical\nquery and key sets as in [29]), or by trading regular attention with sparse attention using more layers\n[8]. Unfortunately, there is a lack of rigorous guarantees for the representation power produced by\nsuch methods, and sometimes the validity of sparsity patterns can only be veriﬁed empirically through\ntrial and error by constructing special GPU operations (e.g. either writing C++ CUDA kernels [8]\nor using TVMs [2]). Other techniques which aim to improve the time complexity of Transformers\n∗Equal contribution.\n1Correspondence to {kchoro,lcolwell}@google.com.\nCode for Transformer models on protein data can be found in github.com/google-research/\ngoogle-research/tree/master/protein_lm and Performer code can be found in github.com/\ngoogle-research/google-research/tree/master/performer.\narXiv:2006.03555v3  [cs.LG]  1 Oct 2020\ninclude reversible residual layers allowing for one-time activation storage in training [29] and shared\nattention weights [53]. These constraints may impede application to problems that involve long\nsequences, where approximations of the attention mechanism are not sufﬁcient. Approximations\nbased on truncated back-propagation [17] are also unable to capture long-distance correlations since\nthe gradients are only propagated inside a localized window.\nRecent work has demonstrated that Transformers ﬁt to the amino acid sequences of single proteins\nlearn to accurately predict information about protein structure and function, and can generate new\nsequences with speciﬁc properties [ 40, 21, 34]. Approaches that encode 3D protein structural\ndata via Transformer-based models demonstrate improved performance, despite the restriction of\nattention to the local structural neighborhoods of each node [20, 28]. These models provide initial\npromise for protein design applications, but their applicability beyond the design of single proteins is\nlimited because they truncate sequences to 512 or 1024 amino acids. The ability to scale to longer\nsequences without imposing sparsity constraints would enable the use of Transformers to jointly\nmodel multiple concatenated protein sequences and the interactions between them. This follows recent\nworks employing simpler statistical models that predict protein quaternary structure, protein-protein\ninteractions and protein interaction networks from evolutionary sequence data [52, 26, 36, 3, 14].\nIn response, we present a new Transformer architecture, Performer, based on Fast Attention Via Or-\nthogonal Random features (FA VOR). Our proposed mechanism has several advantageous properties:\nit scales linearly rather than quadratically in the number of tokens in the sequence (important for\nanalysis involving multiple protein molecules), it is characterized by sub-quadratic space complexity,\nand it does not incorporate any sparsity pattern priors. Furthermore, it provides strong theoretical\nguarantees: unbiased estimation of the regular attention matrix and uniform convergence. FA VOR is\ndesigned for long input sequences where the number of tokens Lsatisﬁes L≫d, for embedding\ndimensionality d. In contrast to previous approaches, instead of simplifying regular attention via\nvarious structural priors (which can lead to different, potentially incompatible architectures), we\nshow that it can be effectively approximated as it is, without any \"liftings\". This leads to our method\nbeing ﬂexible: combined with small amounts of ﬁne-tuning, the Performer is backwards-compatible\nwith pretrained regular Transformers and can be also used beyond the Transformer scope as a more\nscalable replacement for regular attention, which itself has a wide variety of uses in computer vision\n[23], reinforcement learning [ 57], and even combinatorial optimization [ 51]. We demonstrate its\neffectiveness on challenging tasks that include protein sequence modeling and ImageNet64.\nWe show that regular attention can be considered a special case of a much larger class of kernel-driven\nattention mechanisms, Generalized Attention (GA), and that all our results for regular attention can\nbe directly translated also to this extended class. This observation enables us to explore a much larger\nclass of attention models (Sec. 2.2). Interestingly, this is often enabled by the FA VOR mechanism,\neven if linear scaling is not required (Sec. 4). We highlight the following contributions:\n• We present Fast Attention Via Orthogonal Random features(FA VOR) (Sec. 2) which can be\nused as a replacement for regular attention. FA VOR is characterized byO(Ldlog(d)) space\ncomplexity and O(Ld2 log(d)) time complexity, as compared to O(L2) space complexity\nand O(L2d) time complexity for the regular algorithm (Sec. 2.6, Sec. 3).\n• We present a general class of kernel-based attention mechanisms, Generalized Attention\n(GA), which can be handled by FA VOR. Standard attention is a special case. (Sec. 2.2).\n• We provide strong theoretical guarantees regarding FA VOR: unbiasedness of our estimator\nof the attention matrix (Sec. 2.3) and uniform convergence (Sec. 3)\n• We empirically compare the performance of FA VOR viaPerformers at protein sequence\nmodeling tasks, demonstrating in practice all the aforementioned advantages (Sec. 4).\n• We show that our mechanism, implemented in Jax [22], is API-compatible with the regular\nTransformer, whose standard dot-product attention can be replaced by FA VOR with all other\ncomponents of the architecture intact.\nAll proofs are given in full in the Appendix.\n2 Generalized Attention via FA VOR mechanism\nBelow we describe in detail our FA VOR mechanism which is the backbone of our Performer′s\narchitecture. We also present a general class of kernel-based attentions, called Generalized Attention\n(GA) (which includes regular attention as a special case), where FA VOR can be applied.\n2\n2.1 Preliminaries - standard attention mechanism\nLet Lbe the size of an input sequence of tokens. Then regular dot-product attention [46] is a mapping\nwhich accepts matrices Q,K,V ∈RL×d as input where dis the hidden dimension (dimension of the\nlatent representation). Matrices Q,K,V are intermediate representations of the input and their rows\ncan be interpreted as queries, keys and values of the continuous dictionary data structure respectively.\nBidirectional (or non-directional [19]) dot-product attention has the following form:\nAtt↔(Q,K,V) = D−1AV, A = exp(QK⊤/\n√\nd), D = diag(A1L), (1)\nwhere exp(·) is applied elementwise, 1L is the all-ones vector of length L, and diag(·) is a diagonal\nmatrix with the input vector as the diagonal. The runtime complexity of computing (1) is O(L2d)\nbecause the attention matrix A ∈RL×L has to be computed and stored explicitly. Hence, in principle,\ndot-product attention of type (1) is incompatible with end-to-end processing of long sequences.\nAnother important type of attention is unidirectional dot-product attention which has the form:\nAtt→(Q,K,V) = ˜D−1 ˜AV, ˜A = tril(A), ˜D = diag( ˜A1L), (2)\nwhere tril(·) returns the lower-triangular part of the argument matrix including diagonal. As discussed\nin [46], unidirectional attention is used for autoregressive generative modelling with Transformers\nwhen the output sequence o1,...,o L is modelled as:\np(o1,...,o L) = p(o1)p(o2|o1) ...p (oL|o1,...,o L−1).\nTherefore, the probability distribution over oi can only depend on embeddings of tokens o1,...,o i−1.\nUnidirectional attention is used as self-attention in generative Transformers as well as the decoder\npart of Seq2Seq Transformers [46], while bidirectional attention is used in encoder self-attention and\nencoder-decoder attention in Seq2Seq architectures.\nA line of work relies on sparse approximation of the matrix A – either through restricting the sparsity\npattern of A[8] or learning it using Locality-Sensitive Hashing (LSH) techniques [29]. The latter\nresults in O(Ld2 log L) runtime complexity. We will show that, without any structural assumptions,\nthe matrix A can be approximated up to any precision in time O(Ld2 log(d)).\n2.2 Generalized Attention (GA)\nThe idea of the attention mechanism is simple. New representations of tokens are obtained from\nprevious ones by taking convex combinations of different value vectors with coefﬁcients of the convex\ncombinations interpreted as renormalized (i.e. all coefﬁcients sum up to one) similarity measures\nbetween different tokens. High similarities imply strong attendance to the corresponding tokens.\nThese similarity measures sim : Rd ×Rd →R are simple ad-hoc “soft-max style\" functions of a\ndot-product between query Qi of token iand key Kj of token j, namely:\nsim(oi,oj) = exp\n(\nQiK⊤\nj√\nd\n)\n, (3)\nwhere: Q⊤\ni ,K⊤\nj ∈Rd. Note that sim is not a commutative operation here, and the\n√\nd-renormalizer\nis a technical modiﬁcation to stabilize the range of sim and avoid very small/large values.\nHowever, what if we use kernels instead of arbitrary similarity measures? Speciﬁcally, Qi and Kj\nare entangled through a valid kernel function, by deﬁning the attention matrix A as:\nA = Ag,h\nK = [g(Q⊤\ni )K(Q⊤\ni ,K⊤\nj )h(K⊤\nj )]i,j∈{1,...,L}, (4)\nwhere K : Rd ×Rd →R is an arbitrary kernel function and g,h : Rd →R. We call this attention\nmechanism deﬁned above Generalized Attention (GA) parameterized by K,g,h .\nNext we show that not only can FA VOR approximate regular attention governed by Eq. 3, but it can\nbe applied to GAs as long as the corresponding kernels can be effectively estimated via a random\nfeature map mechanism [39], which is the case for most kernels used in practice. We will in fact show\nthat regular attention is a special case of GA for a speciﬁc choice of g,h, and Gaussian kernel K.\n2.3 Towards FA VOR: approximating attention with random features (RFs)\nInstead of computing and storing the attention matrix A ∈RL×L explicitly, we derive its unbiased\nstochastic approximation, which beneﬁts from low-rank structure. We take our inspiration from a\nrandomized scheme to train kernel Support Vector Machines with large training data [39].\n3\nLet Qi and Ki denote the i-th rows of matrices Q and K respectively. For regular attention, the\ni,j-th element of A can be expressed as:\nAi,j = exp(QiK⊤\nj /\n√\nd) = exp(∥Qi∥2\n2/2\n√\nd) ·exp(−∥Qi −Kj∥2\n2/2\n√\nd) ·exp(∥Kj∥2\n2/2\n√\nd).\nIn other words, for r= 2\n√\nd, the attention matrix A can be decomposed as:\nA = DQBDK, B ∈RL×L,∀i,j : Bi,j = exp(−∥Qi −Kj∥2\n2/r), (5)\nDT = diag\n(\nexp(∥T1∥2\n2/r),..., exp(∥TL∥2\n2/r)\n)\n, (6)\nfor T = Q,K. Both DQ and DK can be computed in O(Ld) time. Note that the i,j-th element of\nmatrix B is the value of the Gaussian kernel with σ= d\n1\n4 :\nBi,j = Kσ\ngauss(Q⊤\ni ,K⊤\nj )\ndef\n= exp( −∥Qi −Kj∥2\n2σ2 ). (7)\nFor GA, our analysis is similar. This time DQ,DK have nonzero entries of the form g(Q⊤\ni ) and\nh(K⊤\ni ) (for regular attention we have: g(x) = h(x) = exp(∥x∥2\n2\nr )) respectively and furthermore the\nGaussian kernel is replaced by a general kernel K, namely: Bi,j = K(Q⊤\ni ,K⊤\nj ), as in Equation 4.\nIn the reminder of this section we will derive an unbiased stochastic approximation of matrixB based\non low-rank decomposition of B with the use of random feature maps [39].\nFor a given kernel K : Rd ×Rd →R, the random feature [RF] map φK : Rd →RM corresponding\nto Kis a probabilistic embedding satisfying\nK(x,y) = E[φ⊤(x)φ(y)], (8)\nwhere the expectation is with respect to the randomness of φ, and M denotes the number of random\nfeatures (if E[φ⊤(x)φ(y)] only approximates K(x,y) then we refer to the mechanism as an approxi-\nmate random feature map). This mechanism covers also as a very special case an instance, where\nφis deterministic (and then expectation is not needed). Efﬁcient-to-compute random feature maps\nexist for virtually all classes of kernels used in machine learning, e.g. shift-invariant kernels [39], the\npointwise nonlinear Gaussian kernel related to neural networks [25], and more, though the techniques\nused to derive these random mappings vary from class to class [ 13]. Even more interestingly, for\nmost of these kernels, corresponding random feature maps have a similar structure, namely:\nφ(x)\ndef\n= c√\nM\n(f(ω⊤\n1 x + b1),...,f (ω⊤\nMx + bM))⊤= c√\nM\nf(Wx + b)⊤, (9)\nfor some f : R →R, ω1,...,ω M\niid\n∼Ω, b1,...,b M\niid\n∼B, distributions: Ω ∈P(Rd), B∈P (R) and\nconstant c> 0. Here W ∈RM×d has rows Wi = ω⊤\ni and b\ndef\n= ( b1,...,b M)⊤.\nIn particular, for the Gaussian kernel, we have c=\n√\n2 and:\nφ(x)\ndef\n=\n√\n2\nM(cos(ω⊤\n1 x + b1),..., cos(ω⊤\nMx + bM))⊤, (10)\nwhere ω1,...,ω M\niid\n∼ N(0,σ2Id) and b1,...,b M\niid\n∼ Unif(0,2π). This particular form of φ is a\nconsequence of the celebrated Bochner’s Theorem [39]. We now deﬁne ˆQ and ˆK ∈RL×M as:\nˆQ = c√\nM\nf(WQ⊤+ b)⊤, ˆK = c√\nM\nf(WK⊤+ b)⊤. (11)\nNote that we have: ˆQi = φ(Q⊤\ni )⊤and ˆKi = φ(K⊤\ni )⊤,where ˆQi and ˆKi stand for the ith row of ˆQ\nand ˆK respectively. Then according to Equation 8, we have: B = E[ ˆQ ˆK⊤]. Thus with Q′, K′given\nas: Q′= DQ ˆQ, K′= DK ˆK, we obtain:\nA = E[Q′(K′)⊤]. (12)\nWe conclude that the attention matrix A can be approximated without bias as: ˆA = Q′(K′)⊤. We\nwill leverage this unbiased approximate low-rank (if M ≪L) decomposition of A in our algorithm,\neven though we will not explicitly compute ˆA.\n4\nAlgorithm 1: FA VOR (bidirectional or unidirectional).\nInput : Q,K,V ∈RL×d, isBidirectional - binary ﬂag.\nResult: ˆAtt↔(Q,K,V) ∈RL×L if isBidirectional, ˆAtt→(Q,K,V) ∈RL×L otherwise.\nCompute DQ,DK as explained in Sec. 2.3;\nCompute ˆQ, ˆK according to (11) and take Q′:= DQ ˆQ, K′:= DK ˆK, C := [V 1 L];\nif isBidirectional then\nBuf1 := (K′)⊤C ∈RM×(d+1), Buf2 := Q′Buf1 ∈RL×(d+1);\nelse\nCompute G and its preﬁx-sum tensor GPS according to (14);\nBuf2 :=\n[GPS\n1,:,:Q′\n1 ... GPS\nL,:,:Q′\nL\n]⊤\n∈RL×(d+1);\nend\n[Buf3 buf4] := Buf2, Buf3 ∈RL×d, buf4 ∈RL;\nreturn diag(buf4)−1Buf3;\nNote that one can also deﬁne a valid kernel as: K(x,y) = E[φ(x)⊤φ(y)] for φas in Eq. 9 and an\narbitrary f : R →R. Such kernels cover in particular the family of Pointwise Nonlinear Gaussian\nKernels [13] (intrinsically related to nonlinear neural networks) such as arc-cosine kernels (e.g.\nangular kernels). Most of these kernels do not have closed-forms so computing exact GAs for them\nwould not be possible, but of course computation is feasible with the presented mechanism.\n2.4 Towards FA VOR: reﬁnements via orthogonal random features\nFor isotropic Ω (true for most practical applications, including regular attention), instead of sampling\nωi independently, we can useorthogonal random features (ORF) [56, 13, 12]: these maintain (exactly\nor approximately) the marginal distributions of samples ωi while enforcing that different samples are\northogonal. If we need M >d, ORFs still can be used locally within each d×dblock of W [56].\nORFs were introduced to reduce the variance of Monte Carlo estimators [56, 13, 12, 10, 41, 9, 11] and\nwe show in Secs. 3 and 4 that they do indeed lead to more accurate approximations and substantially\nbetter downstream results. Below we breiﬂy review the most efﬁcient ORF mechanisms (based on\ntheir strengths and costs) that we will use in Sec. 2.6 in the analysis of FA VOR.\n(1) Regular ORFs [R-ORFs]: Applies Gaussian orthogonal matrices [56]. Encodes matrix W in\nO(Md) space. Provides algorithm for computing Wx in O(Md) time for any x ∈Rd. Gives\nunbiased estimation. Requires one-time O(Md2) preprocessing (Gram-Schmidt orthogonalization).\n(2) Hadamard/Givens ORFs [H/G-ORFs]: Applies random Hadamard [13]/Givens matrices [11].\nEncodes matrix W in O(M)/O(Mlog(d)) space. Provides algorithm for computing Wx in\nO(Mlog(d)) time for any x ∈Rd. Gives small bias (going to 0 with d→∞).\n2.5 FA VOR: Fast Attention via Orthogonal Random features\nWe are ready to present the full FA VOR algorithm. In the bidirectional case, our approximate attention\ncomputed by FA VOR is given as:\nˆAtt↔(Q,K,V) = ˆD−1 ˆAV = ˆD−1(Q′((K′)⊤V)), (13)\nwhere ˆD = diag(Q′((K′)⊤1L)). The placement of brackets determines the order in which compu-\ntations are conducted. Note that we never explicitly compute ˆA and consequently, avoid Θ(L2) time\ncomplexity and storing the L×Lapproximate attention matrix (see: Sec. 2.6 for rigorous analysis).\n2.5.1 Preﬁx-sums for unidirectional FA VOR\nFor the unidirectional case, our analysis is similar but this time our goal is to compute\ntril(Q′(K′)⊤)C without constructing and storing the L×L-sized matrix tril(Q′(K′)⊤) explic-\nitly, where C = [ V 1L] ∈RL×(d+1). In order to do so, observe that ∀1 ≤i≤L:\n[tril(Q′(K′)⊤)C]i = GPS\ni,:,: ×Q′\ni, GPS\ni,:,: =\ni∑\nj=1\nGj,:,:, Gj,:,: = K′\njC⊤\nj ∈RM×(d+1) (14)\n5\nwhere G,GPS ∈RL×M×(d+1) are 3d-tensors. Each slice GPS\n:,l,p is therefore a result of a preﬁx-sum\n(or cumulative-sum) operation applied to G:,l,p: GPS\ni,l,p = ∑i\nj=1 Gi,l,p. An efﬁcient algorithm to\ncompute the preﬁx-sum of Lelements takes O(L) total steps and O(log L) time when computed in\nparallel [31, 16]. See Algorithm 1 for the whole approach.\n2.6 Time and space complexity analysis\nWe see that a variant of bidirectional FA VOR using regular RFs (based on iid samples) or R-ORFs\nhas O(Md + Ld+ ML) space complexity as opposed to Θ(L2 + Ld) space complexity of the\nbaseline. Unidirectional FA VOR using fast preﬁx-sum precomputation in parallel [ 31, 16] has\nO(MLd) space complexity to store GPS which can be reduced to O(Md + Ld+ ML) by running\na simple (though non-parallel in L) aggregation of GPS\ni,:,: without storing the whole tensor GPS in\nmemory. From Sec. 2.4, we know that if instead we use G-ORFs, then space complexity is reduced\nto O(Mlog(d) + Ld+ ML) and if the H-ORFs mechanism is used, then space is further reduced to\nO(M + Ld+ ML) = O(Ld+ ML). Thus for M,d ≪Lall our variants provide substantial space\ncomplexity improvements since they do not need to store the attention matrix explicitly.\nThe time complexity of Algorithm 1 isO(LMd) (note that constructing ˆQ and ˆK can be done in time\nO(LMd) via Eq. 11 if samples from Ω and Bcan be obtained in time O(d) and O(1) respectively\n(which is the case for all practical applications). Note that the time complexity of our method is much\nlower than O(L2d) of the baseline for L≫M.\nAs explained in Sec. 2.4, the R-ORF mechanism incurs an extra one-time O(Md2) cost (negligible\ncompared to the O(LMd) term for L≫d). H-ORFs or G-ORFs do not have this cost, and when\nFA VOR uses them, computingQ′and K′can be conducted in time O(Llog(M)d) as opposed\nto O(LMd) (see: Sec. 2.4). Thus even though H/G-ORFs do not change the asymptotic time\ncomplexity, they improve the constant factor from the leading term. This plays an important role for\ntraining very large models.\nThe number of random features M allows a trade-off between computational complexity and the\nlevel of approximation: bigger M results in higher computation costs, but also in a lower variance of\nthe estimate of A. In the next section we will show that in practice we can take M = Θ(dlog(d)).\nObserve that the algorithm obtained is highly-parallelizable, and beneﬁts from fast matrix multiplica-\ntion and broadcasted operations on GPUs or TPUs.\n3 Theoretical convergence analysis\nIn contrast to other methods approximating the attention matrix A, our algorithm provides provable\nstrong uniform convergence theoretical guarantees for compact domains. We show that Mopt, the\noptimal number of random features, does not depend on Lbut only on d. In fact, we prove that\nif we take Mopt = Θ(dlog(d)), then with O(Ld2 log(d))-time, we can approximate A up to any\nprecision, regardless of the number of tokens L. In order to provide those guarantees for FA VOR, we\nleverage recent research on the theory of negative dependence for ORFs [32]. The following is true:\nTheorem 1 (Uniform convergence of FA VOR). Take the generalized attention mechanism deﬁned by\ng,h : Rd →R (see: Sec. 2.2) and a radial basis function (RBF) kernel [12] Kwith corresponding\nspectral distribution Ω (e.g. Gaussian kernel for which Ω = N(0,Id)). Assume that the rows of\nmatrices Q and K are taken from a ball B(R) of radius R, centered at 0 (i.e. norms of queries\nand keys are upper-bounded by R). Deﬁne l = Rd−1\n4 and take g∗ = max x∈B(l) |g(x)|, h∗ =\nmaxx∈B(l) |h(x)|. Then for any ϵ > 0, δ = ϵ\ng∗h∗ and the number of random features M =\nΩ( d\nδ2 log(4σR\nδd\n1\n4\n)) for σ = Eω∼Ω[ω⊤ω] the following holds: ∥ˆA −A∥1 ≤ ϵ with any constant\nprobability, where ˆA approximates generalized attention matrix via FAVOR with R-ORFs.\nThe result holds in particular for regular attention using Gaussian kernels (see: Sec. 2.2) for which\nMopt = Ω( d\nδ2 log(4d\n3\n4 R\nδ )) since σ= d.\n4 Experiments\nWe implement our setup on top of pre-existing Transformer training code in Jax [ 22] optimized\nwith just-in-time (jax.jit) compilation, and complement our theory with empirical evidence to\ndemonstrate FA VOR’s practicality in the protein setting. Unless explicitly stated, a Performer replaces\n6\nonly the attention component with FA VOR, while all other components are exactly the same as for\nthe regular Transformer. Furthermore, since we use the cross-entropy loss in our generative training\nexperiments, we use the standard accuracy metric as deﬁned from supervised learning.\n4.1 Computation costs\nBetween the Transformer and the Performer, we compared speed-wise the backward pass, as it\nis one of the main computational bottlenecks during training, when using the regular default size\n(nheads,nlayers,dff,d) = (8,6,2048,512), where dff denotes the width of the MLP layers. We\nobserved (Fig. 1) that in terms of L, the Performer reaches nearly linear time complexity as opposed\nto the Transformer’s quadratic time complexity. The Performer’s memory consumption is also sub-\nquadratic (as it does not store the explicit O(L2)-sized attention matrix), allowing higher batch sizes\nand longer sequence lengths. In fact, the Performer achieves nearly optimal speedup and memory\nefﬁciency possible, depicted by the \"X\"-line when attention is replaced a \"identity function\" by\nsimply returning the V-vector. The combination of both memory and backward pass efﬁciencies for\nlarge Lhas profound implications for training speed, as it allows respectively, large batch training\nand lower wall clock time per gradient step, contributing to total train time reduction. Extensive\nadditional results are demonstrated in Appendix E by varying layers, raw attention, and architecture\nsizes.\nFigure 1: Comparison of Transformer and Performer in terms of forward and backward pass speed and\nmaximum L allowed. \"X\" (OPT) denotes the maximum possible speedup achievable, when attention simply\nreturns the V-vector. Plots shown up to when a model produces an out of memory error on a V100 GPU with\n16GB. Best in color.\n4.2 Approximation error and compatibility with regular Transformer\nWe further examined the approximation error of the attention matrix implicitly deﬁned in FA VOR in\nFig. 2 (and in Fig. 11 in Appendix D), which thus directly affects the accuracy of FA VOR’s output.\nWe demonstrate that orthogonal features generally produce lower error than unstructured features.\nFigure 2: Approximation errors for both the attention matrix and output of the mechanism itself. We took\nL = 4096, d= 16, and varied the number of random featuresM. Standard deviations shown across 10 samples.\nFigure 3: We transferred the original pretrained Transformer’s weights into the Performer, which produces an\ninitial non-zero 0.07 accuracy (dotted orange line). Once ﬁne-tuned however, the Performer quickly recovers\naccuracy in a small fraction of the original number of gradient steps.\n7\nNotice that the accuracy can be further boosted by applying a resampling strategy that reconstructs\nsamples periodically. We set this option as a hyperparameter of our overall algorithm.\nThe approximation error can propagate when applying the other components (MLPs, multiple heads,\nmultiple layers, etc.) of a Transformer, which we demonstrate in Fig. 11 (Appendix). This implies we\ncannot immediately directly transfer the weights from a pretrained Transformer onto the Performer.\nHowever, this can be resolved by ﬁnetuning the Performer on the trained task. We demonstrate this\ntechnique for a pretrained BERT model [19] on the LM1B dataset [6] in Fig. 3.\n4.3 Multiple layer training\nWe further benchmark the Performer on both unidirectional (U) and bidirectional (B) cases by training\na 36-layer model using protein sequences from the Jan. 2019 release of TrEMBL [ 15], similar to\n[34]. As a baseline for sparse attention, we also used the Reformer [ 29]. In Fig. 4, the Reformer\nsigniﬁcantly drops in accuracy on the protein dataset. This suggests that sparse attention may be\ninsufﬁcient for protein tasks, which require modelling of global interactions. Furthermore, the\nusefulness of generalized attention is evidenced by Performer-RELU (taking f = RELU in Equation\n8) achieving the highest accuracy in both (U) and (B) cases. Our proposed softmax approximation is\nalso shown to be tight, achieving the same accuracies as the exact-softmax Transformer. Extended\nresults including dataset statistics, out of distribution evaluations, and visualizations, can be found in\nAppendix C.\nFigure 4: Train = Dashed, Validation = Solid, Unidirectional = (U), Bidirectional = (B). For TrEMBL, we used\nthe exact same model parameters (nheads, nlayers, dff , d) = (8, 36, 1024, 512) from [34] for all runs. For\nfairness, all TrEMBL experiments used 16x16 TPU-v2’s. Batch sizes were maximized for each separate run\ngiven the corresponding compute constraints. Hyperparameters can be found in Appendix B.\n4.4 Large length training\nOn the standard (U) ImageNet64 benchmark (L = 12288) from [37], we see that the Performer\nmatches the Reformer, when both use 8x8 TPU-v2’s. Depending on hardware (TPU or GPU), we also\nfound that the Performer can be 2x faster than the Reformer via Jax optimizations for the (U) setting.\nFor a proof of principle study, we also create an initial protein benchmark for predicting interactions\namong groups of proteins by concatenating protein sequences to length L= 8192 from TrEMBL,\nlong enough to model protein interaction networks without the large sequence alignments required\nby existing methods [14]. In this setting, a baseline Transformer overloads memory even at a batch\nsize of 1 per chip, by a wide margin. Thus as a baseline we were forced to use a signiﬁcantly smaller\nvariant, reducing to (nheads,nlayers,dff,d) = (8,{1,2,3},256,256). Meanwhile, the Performer\ntrains efﬁciently at a batch size of 8 per chip using the standard (8,6,2048,512) architecture. We\nsee in Fig. 5 that the smaller Transformer ( nlayer = 3) is quickly bounded at ≈19%, while the\nPerformer is able to train continuously to ≈24%.\nFigure 5: For ImageNet64, we took 6-layer variants of both the Performer and the Reformer, evaluated both at\n50K steps, and also plotted the 3.77 BPD baseline (Img-T) from [37], which uses 12-layers but with cropped\nlengths on a regular Transformer. For concatenated TrEMBL, we varied nlayers ∈ {1, 2, 3} for the smaller\nTransformer. Hyperparameters found in Appendix B.\n8\n5 Conclusion\nWe presented Performer, a new type of Transformer, relying on our Fast Attention Via Orthogonal\nRandom features (FA VOR) mechanism to signiﬁcantly improve space and time complexity of regular\nTransformers. Our mechanism is to our knowledge the ﬁrst unbiased estimation of the original\nalgorithm with linear space and time complexity with respect to L. Furthermore, FA VOR could be\napplied to other tasks of approximate attention, including hierarchical attention networks [54], graph\nattention networks [47], image processing [23], and reinforcement learning/robotics [44].\n6 Broader impact\nWe believe that the presented algorithm can be impactful in various ways:\nBiology and Medicine: Our method has the potential to directly impact research on biological\nsequence analysis by enabling the Transformer to be applied to much longer sequences without\nconstraints on the structure of the attention matrix. The initial application that we consider is the\nprediction of interactions between proteins on the proteome scale. Recently published approaches\nrequire large evolutionary sequence alignments, a bottleneck for applications to mammalian genomes\n[14]. The potentially broad translational impact of applying these approaches to biological sequences\nwas one of the main motivations of this work. We believe that modern bioinformatics can immensely\nbeneﬁt from new machine learning techniques with Transformers being among the most promising.\nScaling up these methods to train faster more accurate language models opens the door to the ability\nto design sets of molecules with pre-speciﬁed interaction properties. These approaches could be used\nto augment existing physics-based design strategies that are of critical importance for example in the\ndevelopment of new nanoparticle vaccines [35].\nEnvironment: As we have shown, Performers with FA VOR are characterized by much lower\ncompute costs and substantially lower space complexity which can be directly translated to CO2\nemission reduction [43] and lower energy consumption [55], as regular Transformers require very\nlarge computational resources.\nResearch on Transformers:We believe that our results can shape research on efﬁcient Transformers\narchitectures, guiding the ﬁeld towards methods with strong mathematical foundations. Our research\nmay also hopefully extend Transformers also beyond their standard scope (e.g. by considering the\nGeneralized Attention mechanism and connections with kernels). Exploring scalable Transformer\narchitectures that can handle L of the order of magnitude few thousands and more, preserving\naccuracy of the baseline at the same time, is a gateway to new breakthroughs in bio-informatics,\ne.g. language modeling for proteins, as we explained in the paper. Our presented method can be\npotentially a ﬁrst step.\nBackward Compatibility: Our Performer can be used on the top of a regular pre-trained Transformer\nas opposed to other Transformer variants. Even if up-training is not required, FA VOR can be still\nused for fast inference with no loss of accuracy. We think about this backward compatibility as a\nvery important additional feature of the presented techniques that might be particularly attractive for\npractitioners.\nAttention Beyond Transformers: Finally, FA VOR can be applied to approximate exact attention\nalso outside the scope of Transformers. This opens a large volume of new potential applications\nincluding: hierarchical attention networks (HANS) [ 54], graph attention networks [ 47], image\nprocessing [23], and reinforcement learning/robotics [44].\n7 Acknowledgements\nWe thank Afroz Mohiuddin, Wojciech Gajewski, Nikita Kitaev, and Lukasz Kaiser for multiple\ndiscussions on the Transformer. We further thank Joshua Meier, Aurko Roy, and John Platt for many\nfruitful discussions on biological data and useful comments on this draft.\nLucy Colwell acknowledges support from the Simons Foundation. Adrian Weller acknowledges\nsupport from the David MacKay Newton research fellowship at Darwin College, The Alan Turing\nInstitute under EPSRC grant EP/N510129/1 and U/B/000074, and the Leverhulme Trust via CFI.\n9\nReferences\n[1] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V . Le. Attention augmented convolutional\nnetworks. CoRR, abs/1904.09925, 2019.\n[2] I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. CoRR,\nabs/2004.05150, 2020.\n[3] A.-F. Bitbol, R. S. Dwyer, L. J. Colwell, and N. S. Wingreen. Inferring interaction partners from\nprotein sequences. Proceedings of the National Academy of Sciences, 113(43):12180–12185,\n2016.\n[4] W. Chan, C. Saharia, G. E. Hinton, M. Norouzi, and N. Jaitly. Imputer: Sequence modelling via\nimputation and dynamic programming. CoRR, abs/2002.08926, 2020.\n[5] C. Chelba, M. X. Chen, A. Bapna, and N. Shazeer. Faster transformer decoding: N-gram\nmasked self-attention. CoRR, abs/2001.04589, 2020.\n[6] C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, P. Koehn, and T. Robinson. One billion\nword benchmark for measuring progress in statistical language modeling. In INTERSPEECH\n2014, 15th Annual Conference of the International Speech Communication Association, Singa-\npore, September 14-18, 2014, pages 2635–2639, 2014.\n[7] M. X. Chen, O. Firat, A. Bapna, M. Johnson, W. Macherey, G. F. Foster, L. Jones, M. Schuster,\nN. Shazeer, N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, Z. Chen, Y . Wu, and M. Hughes. The\nbest of both worlds: Combining recent advances in neural machine translation. In I. Gurevych\nand Y . Miyao, editors,Proceedings of the 56th Annual Meeting of the Association for Computa-\ntional Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers,\npages 76–86. Association for Computational Linguistics, 2018.\n[8] R. Child, S. Gray, A. Radford, and I. Sutskever. Generating long sequences with sparse\ntransformers. CoRR, abs/1904.10509, 2019.\n[9] K. Choromanski, C. Downey, and B. Boots. Initialization matters: Orthogonal predictive\nstate recurrent neural networks. In 6th International Conference on Learning Representations,\nICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.\nOpenReview.net, 2018.\n[10] K. Choromanski, A. Pacchiano, J. Pennington, and Y . Tang. KAMA-NNs: Low-dimensional\nrotation based neural networks. In K. Chaudhuri and M. Sugiyama, editors, The 22nd In-\nternational Conference on Artiﬁcial Intelligence and Statistics, AISTATS 2019, 16-18 April\n2019, Naha, Okinawa, Japan, volume 89 of Proceedings of Machine Learning Research, pages\n236–245. PMLR, 2019.\n[11] K. Choromanski, M. Rowland, W. Chen, and A. Weller. Unifying orthogonal Monte Carlo\nmethods. In K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th International\nConference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA,\nvolume 97 of Proceedings of Machine Learning Research, pages 1203–1212. PMLR, 2019.\n[12] K. Choromanski, M. Rowland, T. Sarlós, V . Sindhwani, R. E. Turner, and A. Weller. The\ngeometry of random features. In A. J. Storkey and F. Pérez-Cruz, editors, International\nConference on Artiﬁcial Intelligence and Statistics, AISTATS 2018, 9-11 April 2018, Playa\nBlanca, Lanzarote, Canary Islands, Spain, volume 84 of Proceedings of Machine Learning\nResearch, pages 1–9. PMLR, 2018.\n[13] K. M. Choromanski, M. Rowland, and A. Weller. The unreasonable effectiveness of structured\nrandom orthogonal embeddings. In I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach,\nR. Fergus, S. V . N. Vishwanathan, and R. Garnett, editors, Advances in Neural Information\nProcessing Systems 30: Annual Conference on Neural Information Processing Systems 2017,\n4-9 December 2017, Long Beach, CA, USA, pages 219–228, 2017.\n[14] Q. Cong, I. Anishchenko, S. Ovchinnikov, and D. Baker. Protein interaction networks revealed\nby proteome coevolution. Science, 365(6449):185–189, 2019.\n[15] U. Consortium. Uniprot: a worldwide hub of protein knowledge. Nucleic acids research,\n47(D1):D506–D515, 2019.\n[16] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction to Algorithms, 3rd\nEdition. MIT Press, 2009.\n10\n[17] Z. Dai*, Z. Yang*, Y . Yang, W. W. Cohen, J. Carbonell, Q. V . Le, and R. Salakhutdinov.\nTransformer-XL: Language modeling with longer-term dependency, 2019.\n[18] M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and L. Kaiser. Universal transformers. In 7th\nInternational Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA,\nMay 6-9, 2019. OpenReview.net, 2019.\n[19] J. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: pre-training of deep bidirectional\ntransformers for language understanding. CoRR, abs/1810.04805, 2018.\n[20] Y . Du, J. Meier, J. Ma, R. Fergus, and A. Rives. Energy-based models for atomic-resolution\nprotein conformations. arXiv preprint arXiv:2004.13167, 2020.\n[21] A. Elnaggar, M. Heinzinger, C. Dallago, and B. Rost. End-to-end multitask learning, from\nprotein language to protein features without alignments. bioRxiv, page 864405, 2019.\n[22] R. Frostig, M. Johnson, and C. Leary. Compiling machine learning programs via high-level\ntracing. In Conference on Machine Learning and Systems 2018, 2018.\n[23] J. Fu, J. Liu, H. Tian, Y . Li, Y . Bao, Z. Fang, and H. Lu. Dual attention network for scene\nsegmentation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019,\nLong Beach, CA, USA, June 16-20, 2019, pages 3146–3154, 2019.\n[24] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y . Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y . Wu,\nand R. Pang. Conformer: Convolution-augmented transformer for speech recognition, 2020.\n[25] I. Gulrajani, F. Ahmed, M. Arjovsky, V . Dumoulin, and A. C. Courville. Improved training of\nWasserstein GANs. In I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V . N.\nVishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30:\nAnnual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long\nBeach, CA, USA, pages 5767–5777, 2017.\n[26] T. A. Hopf, L. J. Colwell, R. Sheridan, B. Rost, C. Sander, and D. S. Marks. Three-dimensional\nstructures of membrane proteins from genomic sequencing. Cell, 149(7):1607–1621, 2012.\n[27] C. A. Huang, A. Vaswani, J. Uszkoreit, I. Simon, C. Hawthorne, N. Shazeer, A. M. Dai, M. D.\nHoffman, M. Dinculescu, and D. Eck. Music transformer: Generating music with long-term\nstructure. In 7th International Conference on Learning Representations, ICLR 2019, New\nOrleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\n[28] J. Ingraham, V . Garg, R. Barzilay, and T. Jaakkola. Generative models for graph-based protein\ndesign. In Advances in Neural Information Processing Systems, pages 15794–15805, 2019.\n[29] N. Kitaev, L. Kaiser, and A. Levskaya. Reformer: The efﬁcient transformer. In8th International\nConference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\nOpenReview.net, 2020.\n[30] O. Kovaleva, A. Romanov, A. Rogers, and A. Rumshisky. Revealing the dark secrets of bert.\narXiv preprint arXiv:1908.08593, 2019.\n[31] R. E. Ladner and M. J. Fischer. Parallel preﬁx computation. J. ACM, 27(4):831–838, Oct. 1980.\n[32] H. Lin, H. Chen, T. Zhang, C. Laroche, and K. Choromanski. Demystifying orthogonal Monte\nCarlo and beyond. CoRR, abs/2005.13590, 2020.\n[33] H. Luo, S. Zhang, M. Lei, and L. Xie. Simpliﬁed self-attention for transformer-based end-to-end\nspeech recognition. CoRR, abs/2005.10463, 2020.\n[34] A. Madani, B. McCann, N. Naik, N. S. Keskar, N. Anand, R. R. Eguchi, P. Huang, and R. Socher.\nProgen: Language modeling for protein generation. CoRR, abs/2004.03497, 2020.\n[35] J. Marcandalli, B. Fiala, S. Ols, M. Perotti, W. de van der Schueren, J. Snijder, E. Hodge,\nM. Benhaim, R. Ravichandran, L. Carter, et al. Induction of potent neutralizing antibody\nresponses by a designed protein nanoparticle vaccine for respiratory syncytial virus. Cell,\n176(6):1420–1431, 2019.\n[36] S. Ovchinnikov, H. Kamisetty, and D. Baker. Robust and accurate prediction of residue–residue\ninteractions across protein interfaces using evolutionary information. Elife, 3:e02030, 2014.\n[37] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku, and D. Tran. Image trans-\nformer. In J. G. Dy and A. Krause, editors, Proceedings of the 35th International Conference\non Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018,\nvolume 80 of Proceedings of Machine Learning Research, pages 4052–4061. PMLR, 2018.\n11\n[38] J. W. Rae, A. Potapenko, S. M. Jayakumar, C. Hillier, and T. P. Lillicrap. Compressive\ntransformers for long-range sequence modelling. In International Conference on Learning\nRepresentations, 2020.\n[39] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In J. C. Platt,\nD. Koller, Y . Singer, and S. T. Roweis, editors,Advances in Neural Information Processing Sys-\ntems 20, Proceedings of the Twenty-First Annual Conference on Neural Information Processing\nSystems, Vancouver, British Columbia, Canada, December 3-6, 2007, pages 1177–1184. Curran\nAssociates, Inc., 2007.\n[40] A. Rives, S. Goyal, J. Meier, D. Guo, M. Ott, C. Zitnick, J. Ma, and R. Fergus. Biological\nstructure and function emerge from scaling unsupervised learning to 250 million protein\nsequences. bioArxiv, 04 2019.\n[41] M. Rowland, J. Hron, Y . Tang, K. Choromanski, T. Sarlós, and A. Weller. Orthogonal estimation\nof Wasserstein distances. In K. Chaudhuri and M. Sugiyama, editors, The 22nd International\nConference on Artiﬁcial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha,\nOkinawa, Japan, volume 89 of Proceedings of Machine Learning Research, pages 186–195.\nPMLR, 2019.\n[42] A. Roy, M. Saffar, A. Vaswani, and D. Grangier. Efﬁcient content-based sparse attention with\nrouting transformers. CoRR, abs/2003.05997, 2020.\n[43] E. Strubell, A. Ganesh, and A. McCallum. Energy and policy considerations for deep learning\nin NLP. CoRR, abs/1906.02243, 2019.\n[44] Y . Tang, D. Nguyen, and D. Ha. Neuroevolution of self-interpretable agents. CoRR,\nabs/2003.08165, 2020.\n[45] Y .-H. H. Tsai, S. Bai, M. Yamada, L.-P. Morency, and R. Salakhutdinov. Transformer dissection:\nAn uniﬁed understanding for transformer’s attention via the lens of kernel. In Proceedings\nof the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages\n4335–4344, 2019.\n[46] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and\nI. Polosukhin. Attention is all you need. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach,\nR. Fergus, S. Vishwanathan, and R. Garnett, editors,Advances in Neural Information Processing\nSystems 30, pages 5998–6008. Curran Associates, Inc., 2017.\n[47] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y . Bengio. Graph attention\nnetworks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver,\nBC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.\n[48] J. Vig. A multiscale visualization of attention in the transformer model. arXiv preprint\narXiv:1906.05714, 2019.\n[49] J. Vig and Y . Belinkov. Analyzing the structure of attention in a transformer language model.\nCoRR, abs/1906.04284, 2019.\n[50] J. Vig, A. Madani, L. R. Varshney, C. Xiong, R. Socher, and N. F. Rajani. Bertology meets\nbiology: Interpreting attention in protein language models. CoRR, abs/2006.15222, 2020.\n[51] O. Vinyals, M. Fortunato, and N. Jaitly. Pointer networks. In Advances in Neural Information\nProcessing Systems 28: Annual Conference on Neural Information Processing Systems 2015,\nDecember 7-12, 2015, Montreal, Quebec, Canada, pages 2692–2700, 2015.\n[52] M. Weigt, R. A. White, H. Szurmant, J. A. Hoch, and T. Hwa. Identiﬁcation of direct residue\ncontacts in protein–protein interaction by message passing. Proceedings of the National\nAcademy of Sciences, 106(1):67–72, 2009.\n[53] T. Xiao, Y . Li, J. Zhu, Z. Yu, and T. Liu. Sharing attention weights for fast transformer. In\nS. Kraus, editor, Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial\nIntelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages 5292–5298. ijcai.org, 2019.\n[54] Z. Yang, D. Yang, C. Dyer, X. He, A. J. Smola, and E. H. Hovy. Hierarchical attention networks\nfor document classiﬁcation. In K. Knight, A. Nenkova, and O. Rambow, editors, NAACL HLT\n2016, The 2016 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, San Diego California, USA, June 12-17, 2016,\npages 1480–1489. The Association for Computational Linguistics, 2016.\n12\n[55] H. You, C. Li, P. Xu, Y . Fu, Y . Wang, X. Chen, R. G. Baraniuk, Z. Wang, and Y . Lin. Drawing\nearly-bird tickets: Toward more efﬁcient training of deep networks. In International Conference\non Learning Representations, 2020.\n[56] F. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann-Rice, and S. Kumar. Orthogonal\nrandom features. In D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, and R. Garnett,\neditors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural\nInformation Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 1975–\n1983, 2016.\n[57] V . F. Zambaldi, D. Raposo, A. Santoro, V . Bapst, Y . Li, I. Babuschkin, K. Tuyls, D. P. Reichert,\nT. P. Lillicrap, E. Lockhart, M. Shanahan, V . Langston, R. Pascanu, M. Botvinick, O. Vinyals,\nand P. W. Battaglia. Deep reinforcement learning with relational inductive biases. In 7th\nInternational Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA,\nMay 6-9, 2019, 2019.\n13\nAPPENDIX: Masked Language Modeling for Proteins via Linearly Scalable\nLong-Context Transformers\nA Theoretical results\nWe provide here the proof of Theorem 1 from the main body.\nProof. We consider ﬁrst the case of the default FA VOR setting with R-ORF mechanism turned on.\nWe rely on Theorem 3 from [32]. Note that we can apply it in our case, since for RBF kernels the\ncorresponding function f is cos (thus in particular it is bounded). Also, it is not hard to observe (see\nfor instance analysis in Claim 1 from [39]) that Lf = 1. Using Theorem 3 from [32], we conclude\nthat:\n∥ˆB −B∥1 ≤δ (15)\nwith any constant probability as long as M = Ω( d\nδ2 ) log(σ·diam(M)\nδ ), where σ = E[ω⊤ω] and M\nis the diameter of the smallest ball Mcontaining all vectors of the form z = Qi\nd\n1\n4\n−Kj\nd\n1\n4\n. Since\n∥Qi∥2,∥Kj∥2 ≤R, we conclude that ∥z∥2 ≤2R\nd\n1\n4\nand thus one can take diam(M) = 4R\nd\n1\n4\n. We have:\n∥ˆA −A∥1 = ∥DQ(ˆB −B)DK∥1 ≤∥DQ∥1∥ˆB −B∥1∥DK∥1 ≤δg∗h∗ (16)\nTaking δ= ϵ\ng∗h∗ completes the proof.\nB Hyperparameters\nThis optimal setting (including comparisons to approximate softmax) we use for the Performer is\nspeciﬁed in the Generalized Attention (Subsection B.3), and unless speciﬁcally mentioned (e.g.\nusing name \"Performer-SOFTMAX\"), \"Performer\" refers to using this generalized attention\nsetting.\nB.1 Training Hyperparameters\nAll Performer + Transformer runs used 0.5 grad clip, 0.1 weight decay, 0.1 dropout, 10−3 ﬁxed\nlearning rate with Adam hyperparameters (β1 = 0 .9,β2 = 0 .98,ϵ = 10 −9), with batch size\nmaximized (until TPU memory overload) for a speciﬁc model. For the Reformer, we used the same\nhyperparameters as mentioned for protein experiments, without gradient clipping, while using the\ndefaults1 (which instead use learning rate decay) for ImageNet-64. In both cases, the Reformer used\nthe same default LSH attention parameters.\nAll 36-layer protein experiments used the same amount of compute (i.e. 16x16 TPU-v2, 8GB per\nchip). For concatenated experiments, 16x16 TPU-v2’s were also used for the Performer, while 8x8’s\nwere used for the 1-3 layer (d= 256) Transformer models (using 16x16 did not make a difference in\naccuracy).\nB.2 Approximate Softmax Attention Default Values\nThe optimal values, set to default parameters2, are: renormalize_attention = True, numerical stabilizer\n= 10−6, number of features = 256, ortho_features = True, ortho_scaling = 0.0. .\nB.3 Generalized Attention Default Values\nThe optimal values, set to default parameters3, are: renormalize_attention = True, numerical stabilizer\n= 0.0, number of features = 256, kernel = ReLU, kernel_epsilon = 10−3.\n1https://github.com/google/trax/blob/master/trax/supervised/configs/reformer_\nimagenet64.gin\n2https://github.com/google-research/google-research/blob/master/performer/fast_\nself_attention/fast_self_attention.py#L198\n3https://github.com/google-research/google-research/blob/master/performer/fast_\nself_attention/fast_self_attention.py#L260\n14\nC Experimental Details for Protein Modeling Tasks\nC.1 TrEMBL Dataset\nDataset Set Name Count Length Statistics\nMin Max Mean STD Median\nTrEMBL\nTrain 104,863,744 2 74,488 353.09 311.16 289.00\nValid 102,400 7 11,274 353.62 307.42 289.00\nTest 1,033,216 8 32,278 353.96 312.23 289.00\nOOD 29,696 24 4,208 330.96 269.86 200.00\nTrEMBL\n(concat)\nTrain 4,532,224 8,192 8,192 8,192 0 8,192\nValid 4,096\nTable 1: Statistics for the TrEMBL single sequence and the long sequence task.\nWe used the TrEMBL dataset4, which contains 139,394,261 sequences of which 106,030,080 are\nunique. While the training dataset appears smaller than the one used in Madani et al. [ 34], we argue\nthat it includes most of the relevant sequences. Speciﬁcally, the TrEMBL dataset consists of the\nsubset of UniProtKB sequences that have been computationally analyzed but not manually curated,\nand accounts for ≈99.5% of the total number of sequences in the UniProtKB dataset5.\nFollowing the methodology described in Madani et al. [34], we used both an OOD-Test set, where a\nselected subset of Pfam families are held-out for valuation, and an IID split, where the remaining\nprotein sequences are split randomly into train, valid, and test tests. We held-out the following\nprotein families (PF18369, PF04680, PF17988, PF12325, PF03272, PF03938, PF17724, PF10696,\nPF11968, PF04153, PF06173, PF12378, PF04420, PF10841, PF06917, PF03492, PF06905, PF15340,\nPF17055, PF05318), which resulted in 29,696 OOD sequences. We note that, due to deduplication\nand potential TrEMBL version mismatch, our OOD-Test set does not match exactly the one in Madani\net al. [ 34]. We also note that this OOD-Test selection methodology does not guarantee that the\nevaluation sequences are within a minimum distance from the sequences used during training. In\nfuture work, we will include rigorous distance based splits.\nThe statistics for the resulting dataset splits are reported in Table 1. In the standard sequence modeling\ntask, given the length statistics that are reported in the table, we clip single sequences to maximum\nlength L= 1024, which results in few sequences being truncated signiﬁcantly.\nIn the long sequence task, the training and validation sets are obtained by concatenating the sequences,\nseparated by an end-of-sequence token, and grouping the resulting chain into non-overlapping\nsequences of length L= 8192.\nC.2 Empirical Baseline\nFigure 6: Visualization of the estimated empirical distribution for the 20 standard amino acids, colored by their\nclass. Note the consistency with the statistics on the TrEMBL web page.\nA random baseline, with uniform probability across all the vocabulary tokens at every position, has\naccuracy 5% (when including only the 20 standard amino acids) and 4% (when also including the 5\nanomalous amino acids [15]). However, the empirical frequencies of the various amino acids in our\n4https://www.uniprot.org/statistics/TrEMBL\n5https://www.uniprot.org/uniprot/\n15\ndataset may be far from uniform, so we also consider an empirical baseline where the amino acid\nprobabilities are proportional to their empirical frequencies in the training set.\nFigure 6 shows the estimated empirical distribution. We use both the standard and anomalous\namino acids, and we crop sequences to length 1024 to match the data processing performed for the\nTransformer models. The ﬁgure shows only the 20 standard amino acids, colored by their class, for\ncomparison with the visualization on the TrEMBL web page6.\nC.3 Tabular Results\nTable 2 contains the results on the single protein sequence modeling task (L= 1024). We report the\nfollowing evaluation metrics:\n1. Accuracy: For unidirectional models, we measure the accuracy on next-token prediction,\naveraged across all sequence positions in the dataset. For bidirectional models, we mask\neach token with 15% probability and measure accuracy across the masked positions.\n2. Perplexity: For unidirectional models, we measure perplexity across all sequence positions\nin the dataset. For bidirectional models, similar to the accuracy case, we measure perplexity\nacross the masked positions.\nModel Type Set Name Model Accuracy Perplexity\nUNI\nTest\nEmpirical Baseline 9.92 17.80\nTransformer 30.80 9.37\nPerformer (generalized) 31.58 9.17\nOOD\nEmpirical Baseline 9.07 17.93\nTransformer 19.70 13.20\nPerformer (generalized) 18.44 13.63\nBID\nTest\nTransformer 33.32 9.22\nPerformer (generalized) 36.09 8.36\nPerformer (softmax) 33.00 9.24\nOOD\nTransformer 25.07 12.09\nPerformer (generalized) 24.10 12.26\nPerformer (softmax) 23.48 12.41\nTable 2: Results on single protein sequence modeling ( L = 1024 ). We note that the empirical\nbaseline results are applicable to both the unidirectional (UNI) and bidirectional (BID) models.\nC.4 Attention Matrix Illustration\nIn this section we illustrate the attention matrices produced by a Performer model. We focus on the\nbidirectional case and choose one Performer model trained on the standard single-sequence TrEMBL\ntask for over 500K steps. The same analysis can be applied to unidirectional Performers as well.\nWe note that while the Transformer model instantiates the attention matrix in order to compute the\nattention output that incorporates the (queries Q, keys K, values V) triplet (see Eq. 1 in the main\npaper), the FA VOR mechanism returns the attention output directly (see Algorithm 1). To account for\nthis discrepancy, we extract the attention matrices by applying each attention mechanism twice: once\non each original (Q,K,V ) triple to obtain the attention output, and once on a modiﬁed (Q,K,V ◦)\ntriple, where V◦contains one-hot indicators for each position index, to obtain the attention matrix.\nThe choice of V◦ensures that the dimension of the attention output is equal to the sequence length,\nand that a non-zero output on a dimension ican only arise from a non-zero attention weight to the ith\nsequence position. Indeed, in the Transformer case, when comparing the output of this procedure\nwith the instantiated attention matrix, the outputs match.\n6https://www.uniprot.org/statistics/TrEMBL\n16\nAttention matrix example. We start by visualizing the attention matrix for an individual protein\nsequence. We use the BPT1_BOVIN protein sequence7, one of the most extensively studied globular\nproteins, which contains 100 amino acids. In Figure 7, we show the attention matrices for the ﬁrst\n4 layers. Note that many heads show a diagonal pattern, where each node attends to its neighbors,\nand some heads show a vertical pattern, where each head attends to the same ﬁxed positions. These\npatterns are consistent with the patterns found in Transformer models trained on natural language\n[30]. In Figure 9 we highlight these attention patterns by focusing on the ﬁrst 25 tokens, and in Figure\n8, we illustrate in more detail two attention heads.\nAmino acid similarity. Furthermore, we analyze the amino-acid similarity matrix estimated from\nthe attention matrices produced by the Performer model, as described in Vig et al. [50]. We aggregate\nthe attention matrix across 800 sequences. The resulting similarity matrix is illustrated in Figure 10.\nNote that the Performer recognises highly similar amino acid pairs such as (D, E) and (F, Y).\nFigure 7: We show the attention matrices for the ﬁrst 4 layers and all 8 heads (each row is a layer, each column\nis head index, each cell contains the attention matrix across the entire BPT1_BOVIN protein sequence). Note\nthat many heads show a diagonal pattern, where each node attends to its neighbors, and some heads show a\nvertical pattern, where each head attends to the same ﬁxed positions.\nFigure 8: We illustrate in more detail two attention heads. The sub-ﬁgures correspond respectively to: (1) Head\n1-2 (second layer, third head), (2) Head 4-1 (ﬁfth layer, second head). Note the block attention in Head 1-2 and\nthe vertical attention (to the start token (‘M’) and the 85th token (‘C’)) in Head 4-1.\n7https://www.uniprot.org/uniprot/P00974\n17\nFigure 9: We highlight the attention patterns by restricting our attention to the ﬁrst 25 tokens (note that we do\nnot renormalize the attention to these tokens). The illustration is based on Vig et al. [48, 49]. Note that, similar\nto prior work on protein Transformers [34], the attention matrices include both local and global patterns.\nA C D E F G H I K L M N P Q R S T V W Y\nA\nC\nD\nE\nF\nG\nH\nI\nK\nL\nM\nN\nP\nQ\nR\nS\nT\nV\nW\nY\n 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nA C D E F G H I K L M N P Q R S T V W Y\nA\nC\nD\nE\nF\nG\nH\nI\nK\nL\nM\nN\nP\nQ\nR\nS\nT\nV\nW\nY\n 0.0\n0.2\n0.4\n0.6\n0.8\nFigure 10: Amino acid similarity matrix estimated from attention matrices aggregated across a small subset\nof sequences, as described in Vig et al. [ 50]. The sub-ﬁgures correspond respectively to: (1) the normalized\nBLOSUM matrix, (2) the amino acid similarity estimated via a trained Performer model. Note that the Performer\nrecognises highly similar amino acid pairs such as (D, E) and (F, Y).\nD Extended approximation results\nD.1 Backwards Compatibility\nAlthough mentioned previously (Sec. 4.2) that the Performer with additional ﬁnetuning is backwards\ncompatible with the Transformer, we demonstrate below in Fig. 11 that error propagation due to non-\nattention components of the Transformer is one of the primary reasons that pretrained Transformer\nweights cannot be immediately used for inference on the corresponding Performer.\nD.2 Generalized Attention\nWe investigated Generalized Attention mechanisms (mentioned in Sec. 2.2) on TrEMBL when\nL= 512 for various kernel functions. This is similar to [45] which also experiments with various\nattention kernels for natural language. Using hyperparameter sweeps across multiple variables in\nFA VOR, we compared several kernels and also renormalization on/off (Fig. 12 and Fig. 13), where\nRenormalize corresponds to applying D−1 operator in attention, as for the standard mechanism,\nthough we noticed that disabling it does not necessarily hurt accuracy) to produce the best training\nconﬁguration for the Performer. We note that the effective batch size slightly affects the rankings\n18\nFigure 11: Output approximation errors between a vanilla Transformer and a Performer (with\northogonal features) for varying numbers of layers.\n(as shown by the difference between 2x2 and 4x4 TPU runs) - we by default use the generalized\nReLU kernel with other default hyperparameters shown in Appendix B, as we observed that they are\nempirically optimal for large batch size runs (i.e. 8x8 or 16x16 TPU’s).\nFigure 12: To emphasize the highest accuracy runs but also show the NaN issues with certain kernels\nwhich caused runs to stop early, we set both x and y axes to be log-scale. We tested kernels deﬁned\nby different functions f (see: Sec. 2.2): sigmoid, exponential, ReLU, absolute, gelu, cosine (original\nsoftmax approximation), tanh, and identity. All training runs were performed on 2x2 TPU-v2’s, 128\nbatch size per device.\nFigure 13: We also performed a similar setup as Fig. 12 for 4x4 TPU-v2’s.\n19\nE Extended computation costs\nIn this subsection, we empirically measure computational costs in terms wall clock time on forward\nand backward passes for three scenarios in Fig. 14, 15:\n1. Performer, with varying number of layers. We show that our method can scale up to (but not\nnecessarily limited to) even 20 layers.\n2. Attention time complexities when comparing standard attention (from Transformer) and\nFA VOR (from Performer). Note that the maximum memory size here is not reﬂective of\nthe maximum memory size in an actual model (shown below), as this benchmark requires\ncomputing explicit tensors (causing memory increases) in Jax, while a model does not.\n3. Time complexities when comparing the Transformer and Performer models. \"X\" (OPT)\ndenotes the maximum possible speedup achievable, when attention simply returns the V-\nvector, showing that the Performer is nearly optimal. We see that the maximum possible\npower of 2 length allowed on a V100 GPU (16GB) is215 = 32768 using regular dimensions.\nSince some of the computational bottleneck in the Transformer may originate from the extra feed-\nforward layers [ 29], we also benchmark the “Small\" version, i.e. (nheads,nlayers,dff,d) =\n(1,6,64,64) as well, when the attention component is the dominant source of computation and\nmemory. We remind the reader that the “Regular\" version consists of (nheads,nlayers,dff,d) =\n(8,6,2048,512).\nFigure 14: Captions (1), (2) for each 2x2 subﬁgure mentioned above.\n20\nFigure 15: Caption (3) for this 2x2 subﬁgure mentioned above.\n21",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6870760321617126
    },
    {
      "name": "Transformer",
      "score": 0.6484700441360474
    },
    {
      "name": "Scalability",
      "score": 0.6059871315956116
    },
    {
      "name": "Exploit",
      "score": 0.574383556842804
    },
    {
      "name": "Quadratic growth",
      "score": 0.5514029264450073
    },
    {
      "name": "Quadratic equation",
      "score": 0.5278123617172241
    },
    {
      "name": "Prior probability",
      "score": 0.48689141869544983
    },
    {
      "name": "Sequence (biology)",
      "score": 0.46647176146507263
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3996405005455017
    },
    {
      "name": "Algorithm",
      "score": 0.39144980907440186
    },
    {
      "name": "Machine learning",
      "score": 0.35919204354286194
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3512135148048401
    },
    {
      "name": "Mathematics",
      "score": 0.19203028082847595
    },
    {
      "name": "Engineering",
      "score": 0.13462868332862854
    },
    {
      "name": "Voltage",
      "score": 0.11962169408798218
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Bayesian probability",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": []
}