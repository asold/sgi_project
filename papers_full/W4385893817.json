{
    "title": "Causal-Debias: Unifying Debiasing in Pretrained Language Models and Fine-tuning via Causal Invariant Learning",
    "url": "https://openalex.org/W4385893817",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2114723932",
            "name": "Fan Zhou",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A2099759206",
            "name": "Yuzhou Mao",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A2038149722",
            "name": "Liu Yu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1997223618",
            "name": "Yi Yang",
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "University of Hong Kong",
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A1984380716",
            "name": "Ting Zhong",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4287890645",
        "https://openalex.org/W4312901583",
        "https://openalex.org/W4298422451",
        "https://openalex.org/W2962688977",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2144020560",
        "https://openalex.org/W3177189402",
        "https://openalex.org/W3155655882",
        "https://openalex.org/W3213670254",
        "https://openalex.org/W3131157458",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W4385573981",
        "https://openalex.org/W3093211917",
        "https://openalex.org/W4226455589",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W3105882417",
        "https://openalex.org/W3012815759",
        "https://openalex.org/W2954275542",
        "https://openalex.org/W2963317470",
        "https://openalex.org/W4285174559",
        "https://openalex.org/W2963078909",
        "https://openalex.org/W3206487987",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W4285192297",
        "https://openalex.org/W2950866572",
        "https://openalex.org/W4385573960",
        "https://openalex.org/W3035591180",
        "https://openalex.org/W3206381016",
        "https://openalex.org/W2963686995",
        "https://openalex.org/W2801890059",
        "https://openalex.org/W2893425640",
        "https://openalex.org/W3176001432",
        "https://openalex.org/W4288287305",
        "https://openalex.org/W2964067969",
        "https://openalex.org/W4221146190",
        "https://openalex.org/W2560647685",
        "https://openalex.org/W2155858138",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2921633540",
        "https://openalex.org/W2486285194",
        "https://openalex.org/W2790376986",
        "https://openalex.org/W4318142185",
        "https://openalex.org/W2187089797",
        "https://openalex.org/W4312232143",
        "https://openalex.org/W2251939518"
    ],
    "abstract": "Demographic biases and social stereotypes are common in pretrained language models (PLMs), and a burgeoning body of literature focuses on removing the unwanted stereotypical associations from PLMs. However, when fine-tuning these bias-mitigated PLMs in downstream natural language processing (NLP) applications, such as sentiment classification, the unwanted stereotypical associations resurface or even get amplified. Since pretrain&amp;fine-tune is a major paradigm in NLP applications, separating the debiasing procedure of PLMs from fine-tuning would eventually harm the actual downstream utility. In this paper, we propose a unified debiasing framework Causal-Debias to remove unwanted stereotypical associations in PLMs during fine-tuning. Specifically, Causal-Debias mitigates bias from a causal invariant perspective by leveraging the specific downstream task to identify bias-relevant and label-relevant factors. We propose that bias-relevant factors are non-causal as they should have little impact on downstream tasks, while label-relevant factors are causal. We perform interventions on non-causal factors in different demographic groups and design an invariant risk minimization loss to mitigate bias while maintaining task performance. Experimental results on three downstream tasks show that our proposed method can remarkably reduce unwanted stereotypical associations after PLMs are fine-tuned, while simultaneously minimizing the impact on PLMs and downstream applications. © 2023 Association for Computational Linguistics.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 4227–4241\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nCausal-Debias: Unifying Debiasing in Pretrained Language Models and\nFine-tuning via Causal Invariant Learning\nFan Zhou1 Yuzhou Mao 1 Liu Yu 1 ∗ Yi Yang 2 Ting Zhong 1,3\n1University of Electronic Science and Technology of China\n2Hong Kong University of Science and Technology\n3Kashi Institute of Electronics and Information Industry\nfan.zhou@uestc.edu.cn, yuzhou.mao@outlook.com, liu.yu@std.uestc.edu.cn,\nimyiyang@ust.hk, zhongting@uestc.edu.cn\nAbstract\nDemographic biases and social stereotypes\nare common in pretrained language models\n(PLMs), and a burgeoning body of literature\nfocuses on removing the unwanted stereotypi-\ncal associations from PLMs. However, when\nfine-tuning these bias-mitigated PLMs in down-\nstream natural language processing (NLP) ap-\nplications, such as sentiment classification, the\nunwanted stereotypical associations resurface\nor even get amplified. Since pretrain&fine-tune\nis a major paradigm in NLP applications, sepa-\nrating the debiasing procedure of PLMs from\nfine-tuning would eventually harm the actual\ndownstream utility. In this paper, we propose a\nunified debiasing framework Causal-Debias to\nremove unwanted stereotypical associations in\nPLMs during fine-tuning. Specifically, Causal-\nDebias mitigates bias from a causal invariant\nperspective by leveraging the specific down-\nstream task to identify bias-relevant and label-\nrelevant factors. We propose that bias-relevant\nfactors are non-causal as they should have lit-\ntle impact on downstream tasks, while label-\nrelevant factors are causal. We perform inter-\nventions on non-causal factors in different de-\nmographic groups and design an invariant risk\nminimization loss to mitigate bias while main-\ntaining task performance. Experimental results\non three downstream tasks show that our pro-\nposed method can remarkably reduce unwanted\nstereotypical associations after PLMs are fine-\ntuned, while simultaneously minimizing the\nimpact on PLMs and downstream applications.\n1 Introduction\nPretrained language models (PLMs) have achieved\nremarkable success in many natural language pro-\ncessing (NLP) tasks. However, PLMs often en-\ncoded undesired social stereotypes and biases, and\nthus mitigating such biases has become an emerg-\ning and important task (Meade et al., 2022). Prior\nbias mitigation methods often focus on removing\n∗Corresponding author\nunwanted stereotypical associations in PLMs. For\nexample, some works (Zmigrod et al., 2019) pre-\ntrain a language model using original and counter-\nfactual corpus in order to cancel-out biased asso-\nciations, some works (Liang et al., 2020) focus on\ndebiasing post-hoc sentence representations, and\nothers (Guo et al., 2022; Cheng et al., 2021) de-\nsign bias-equalizing objectives to fine-tune PLM’s\nparameters.\nwomanwomanwomanwomanwomanwomanwomanwomanwomanwomanmanmanmanmanmanmanmanmanmanman\nfamilyfamilyfamilyfamilyfamilyfamilyfamilyfamilyfamilyfamily\ncareercareercareercareercareercareercareercareercareercareer\nmathmathmathmathmathmathmathmathmathmath\nartartartartartartartartartart\nsciencesciencesciencesciencesciencesciencesciencesciencesciencescience\nliteratureliteratureliteratureliteratureliteratureliteratureliteratureliteratureliteratureliterature\ntechnologytechnologytechnologytechnologytechnologytechnologytechnologytechnologytechnologytechnology\ndancedancedancedancedancedancedancedancedancedance\n(a) Original BERT.\nwomanwomanwomanwomanwomanwomanwomanwomanwomanwoman\nmanmanmanmanmanmanmanmanmanman\nfamilyfamilyfamilyfamilyfamilyfamilyfamilyfamilyfamilyfamily\ncareercareercareercareercareercareercareercareercareercareer\nmathmathmathmathmathmathmathmathmathmath\nartartartartartartartartartart\nsciencesciencesciencesciencesciencesciencesciencesciencesciencescienceliteratureliteratureliteratureliteratureliteratureliteratureliteratureliteratureliteratureliterature\ntechnologytechnologytechnologytechnologytechnologytechnologytechnologytechnologytechnologytechnology\ndancedancedancedancedancedancedancedancedancedance (b) BERT after Auto-Debias.\nwomanwomanwomanwomanwomanwomanwomanwomanwomanwoman\nmanmanmanmanmanmanmanmanmanman\nfamilyfamilyfamilyfamilyfamilyfamilyfamilyfamilyfamilyfamily\ncareercareercareercareercareercareercareercareercareercareer\nmathmathmathmathmathmathmathmathmathmath\nartartartartartartartartartart\nsciencesciencesciencesciencesciencesciencesciencesciencesciencescience\nliteratureliteratureliteratureliteratureliteratureliteratureliteratureliteratureliteratureliterature\ntechnologytechnologytechnologytechnologytechnologytechnologytechnologytechnologytechnologytechnology dancedancedancedancedancedancedancedancedancedance\n(c) Auto-Debias on SST-2.\nwomanwomanwomanwomanwomanwomanwomanwomanwomanwoman\nmanmanmanmanmanmanmanmanmanman\nfamilyfamilyfamilyfamilyfamilyfamilyfamilyfamilyfamilyfamily\ncareercareercareercareercareercareercareercareercareercareer mathmathmathmathmathmathmathmathmathmath\nartartartartartartartartartart\nsciencesciencesciencesciencesciencesciencesciencesciencesciencescience\nliteratureliteratureliteratureliteratureliteratureliteratureliteratureliteratureliteratureliterature\ntechnologytechnologytechnologytechnologytechnologytechnologytechnologytechnologytechnologytechnology\ndancedancedancedancedancedancedancedancedancedance (d) Ours on SST-2.\nFigure 1: Motivation of Causal-Debias. t-SNE plots of\naverage sentence representations of each word across its\nsentence templates on SST-2 task (Socher et al., 2013).\nHowever, a problem with existing debiasing\nstrategies is that they are separate from downstream\nNLP tasks. If people take a debiased PLM which\nis supposed to have certain stereotypical associa-\ntions removed, and then fine-tune it on downstream\ntask, the unwanted associations will re-enter or\neven get amplified in the fine-tuned language model\n(Goldfarb-Tarrant et al., 2021). Consider gender\ndebiasing using Auto-Debias (Guo et al., 2022)\nas an example. Fig. 1a and 1b shows the BERT-\nbased sentence embeddings using t-SNE (Van der\nMaaten and Hinton, 2008) before and after Auto-\nDebias, respectively. The gender bias is clearly less\n4227\nprominent than the original BERT, i.e., non-gender-\nspecific concepts (in black) are more equidistant to\nboth genders after Auto-Debias (Fig. 1b). However,\nwhen applying BERT model that is debiased by\nAuto-Debias to downstream tasks (Fig. 1c), the fine-\ntuning procedure almost, if not all, “neutralizes\"\nthe effect of PLM debiasing. The phenomenon that\nbiases encode in a fine-tuned PLM is as worrisome\nas in a vanilla PLM, because the fine-tuned PLM is\nmore likely to be deployed in real-world scenarios\nserving thousands to millions of end users.\nAddressing the aforementioned bias resurgence\nissues is non-trivial. On the one hand, existing\nliterature on bias mitigation mostly treats PLM de-\nbiasing as a standalone problem which is separate\nfrom downstream tasks. Incorporating debiasing\nobjectives into the fine-tuning procedure can be\na viable solution but it is still less explored. On\nthe other hand, one may expect to use existing de-\nbiasing methods to re-debias the fine-tuned PLM.\nHowever, due to catastrophic forgetting concern\n(Kirkpatrick et al., 2017), a sequential combination\nof fine-tuning and debiasing may worsen the down-\nstream task performance (Goodfellow et al., 2013).\nTherefore, there is a research gap in unifying debi-\nasing in PLMs and fine-tuning for building fair and\naccountable NLP services.\nIn this work, we propose Causal-Debias, a\nCausal Invariant Debiasing Model to unify the de-\nbiasing with downstream fine-tuning. In principle,\nwe analyze the cause and propagation of biases and\nintroduce the Structure Causal Model (SCM) (Pearl\net al., 2000, 2016) to address the bias mitigation\nproblem by exploiting the inherent causal mech-\nanism in the downstream datasets. Specifically,\nCausal-Debias first exploits a causal intervention\nmodule to distinguish causal and non-causal factors.\nIt then generates counterfactual sentences which\nhave different non-causal factors but the same se-\nmantic meanings. The generated counterfactual\nsentences, along with the original sentences, are\nfed into an invariant optimization function to en-\nsure a trade-off between the performance of down-\nstream tasks and the effectiveness of debiasing. As\nillustrated in Fig. 1(d), Causal-Debias can preserve\nthe PLM debiasing effect even after fine-tuning on\nthe downstream dataset.\nWe evaluate performance of Causal-Debias in\nmitigating the gender and racial biases in several\npopular PLMs, e.g., BERT (Devlin et al., 2019),\nALBERT (Lan et al., 2020), and RoBERTa (Liu\net al., 2019), on three GLUE (Wang et al., 2018)\ntasks (SST-2, CoLA, and QNLI). The results\ndemonstrate Causal-Debias can significantly mit-\nigate PLM biases after downstream fine-tuning\nwhile also maintaining the downstream task per-\nformance. We hope this work provides empiri-\ncal evidence that stereotypical associations can re-\nenter language models during the fine-tuning step.\nMoreover, we hope that debiasing with a causal\nperspective offers a more generalizable and reli-\nable way for building fair and accountable NLP\napplications. We release the anonymous implemen-\ntation of Causal-Debias at https://github.com/\nmyZeratul/Causal-Debias.\n2 Related Works\nPLM Debiasing aims to remove biases, quanti-\nfied as unwanted stereotypical associations, from\npretrained language models. Existing works on\nPLM debiasing can be categorized into two lines\nbased on whether downstream tasks are involved\nin the debiasing pipeline. (1) Non-Task-Specific:\nCounterfactual Data Augmentation (CDA) (Zmi-\ngrod et al., 2019) and Dropout (Webster et al.,\n2020) are two methods where debiasing hap-\npens in the pre-training stage (Meade et al.,\n2022). Auto-Debias (Guo et al., 2022), Context-\nDebias (Kaneko and Bollegala, 2021) and MA-\nBEL (He et al., 2022) remove biases in PLM by\ndesigning different bias-equalizing objectives. In\nthis line of work, the parameters in the PLM are\nchanged to meet the fairness criteria such as SEAT\n(May et al., 2019), and the ultimate goal is to re-\nmove bias associations from PLM, so that down-\nstream tasks can benefit from the debiased models.\nHowever, we show that it is not this case. When\nthe debiased models are fine-tuned on downstream\ntasks, the bias associations resurge, perhaps be-\ncause the biases are not completely removed or\nmaybe just covered up, or because downstream\ndatasets are encoded with stereotypical associa-\ntions. (2) Task-Specific: Existing works on this\nline, including Sent-Debias (Liang et al., 2020)\nand FairFil (Cheng et al., 2021), keep the parame-\nters of PLMs untouched. Instead, they target on the\nsentence representations, and aim to remove bias\nassociations from representations. Even though the\nsentence representations, which are the input for\ndownstream tasks, are refined, downstream fine-\ntuning can still introduce new biases, as we show\nin experiments. In summary, our work differs from\n4228\nexisting PLM debiasing in that we aim to unify\nfine-tuning procedure with debiasing so that the\nfine-tuned models are free from bias associations.\nIn addition, prior literature also challenges the\neffectiveness of bias mitigation. Gonen and Gold-\nberg (2019) showed that debiasing methods only\ncover-up biases in word embeddings but do not\nremove them. Meade et al. (2022) found that exist-\ning debiasing methods for PLMs hurt the language\nmodeling capability of PLMs and thus the prac-\ntical utility of debiasing warrants attention. Our\nwork follows the spirit of this line of works in that\nwe empirically demonstrate the bias resurgence\nproblem in fine-tuning and suggest to development\ndebiasing techniques with downstream utility.\nCausal Mechanism is crystallized in the invari-\nant learning, suggesting that only associations in-\nvariant in the training set should be learned (Pe-\nters et al., 2016; Muandet et al., 2013). Invariant\nRisk Minimization (IRM) (Arjovsky et al., 2019)\nis a practical implementation of invariant learn-\ning, which is an optimization objective modifying\nthe loss with a regularization term to enforce the\ninvariant representations. Recent works have ex-\nplored Structural Causal Model (SCM) (Schölkopf\net al., 2012) to model auxiliary variables and show\npromising performance, ranging from domain gen-\neralization (Lv et al., 2022) in computer vision and\nintrinsic interpretability (Wu et al., 2022) in graph\nneural networks to factual knowledge (Li et al.,\n2022) and text classification (Qian et al., 2021) in\nNLP. In this work, we introduce SCM to PLM de-\nbiasing to discover the inherent causal relations\nbetween data and labels while achieving better de-\nbiasing performance. To our knowledge, this is the\nfirst work exploiting causality for debiasing PLMs.\n3 Methodology\nOur goal is to unify debiasing with downstream\nfine-tuning so that the fine-tuned language model\ncan maintain solid performance with alleviated\nstereotypical associations. In other words, we want\nto prevent biases re-entering the language model\nduring the fine-tuning. Unlike prior work that con-\nsiders PLM debiasing as a standalone procedure,\nwe mitigate language model biases during fine-\ntuning process. To this end, we propose Causal-\nDebias, a debiasing framework from a causal view.\nWe first provide the basic formalism and proceed\nwith the details of Causal-Debias.\nProblem Definition: We denote a supervised NLP\ntask with dataset (X,Y ), we fine tune a pretrained\nlanguage model to learn a mapping: M(X) ↦\nY. Our goal is to mitigate unwanted stereotypical\nassociations in the fine-tuned model M.\n3.1 Biases from a Causal View\n<latexit sha1_base64=\"zwkHjHwDnkSZRBSA6GkMkRy1PQQ=\">AAAB8nicbVDLSsNAFL2pr1pfVZduBovgqiQi6rLoxmUF+4A2lMl00g6dTMLMjVBCP8ONC0Xc+jXu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wSTu9zvPHFtRKwecZpwP6IjJULBKFqp148ojgVm3VllUK25dXcOskq8gtSgQHNQ/eoPY5ZGXCGT1Jie5yboZ1SjYJLPKv3U8ISyCR3xnqWKRtz42TzyjJxZZUjCWNunkMzV3xsZjYyZRoGdzCOaZS8X//N6KYY3fiZUkiJXbPFRmEqCMcnvJ0OhOUM5tYQyLWxWwsZUU4a2pbwEb/nkVdK+qHtX9cuHy1rjtqijDCdwCufgwTU04B6a0AIGMTzDK7w56Lw4787HYrTkFDvH8AfO5w8oAZEu</latexit>\nX\n<latexit sha1_base64=\"9gUv7gMEpAE7xkVgoxPW9w2IfD0=\">AAAB8nicbVDLSgMxFM3UV62vqks3wSK4KjNS1GXRjcsK9iHToWTSTBuaSYbkjlCGfoYbF4q49Wvc+Tdm2llo64HA4Zx7ybknTAQ34LrfTmltfWNzq7xd2dnd2z+oHh51jEo1ZW2qhNK9kBgmuGRt4CBYL9GMxKFg3XBym/vdJ6YNV/IBpgkLYjKSPOKUgJX8fkxgzCF7nFUG1Zpbd+fAq8QrSA0VaA2qX/2homnMJFBBjPE9N4EgIxo4FWxW6aeGJYROyIj5lkoSMxNk88gzfGaVIY6Utk8Cnqu/NzISGzONQzuZRzTLXi7+5/kpRNdBxmWSApN08VGUCgwK5/fjIdeMgphaQqjmNiumY6IJBdtSXoK3fPIq6VzUvct6475Ra94UdZTRCTpF58hDV6iJ7lALtRFFCj2jV/TmgPPivDsfi9GSU+wcoz9wPn8AKYeRLw==</latexit>\nY\n<latexit sha1_base64=\"n7XhcwuMgTMrigEUMGqH9OxnL1g=\">AAAB8nicbVDLSgMxFM3UV62vqks3wSK4KjNS1GWxG5cV7AOmQ8mkmTY0kwzJHaEM/Qw3LhRx69e482/MtLPQ1gOBwzn3knNPmAhuwHW/ndLG5tb2Tnm3srd/cHhUPT7pGpVqyjpUCaX7ITFMcMk6wEGwfqIZiUPBeuG0lfu9J6YNV/IRZgkLYjKWPOKUgJX8QUxgwiFrzSvDas2tuwvgdeIVpIYKtIfVr8FI0TRmEqggxviem0CQEQ2cCjavDFLDEkKnZMx8SyWJmQmyReQ5vrDKCEdK2ycBL9TfGxmJjZnFoZ3MI5pVLxf/8/wUotsg4zJJgUm6/ChKBQaF8/vxiGtGQcwsIVRzmxXTCdGEgm0pL8FbPXmddK/q3nW98dCoNe+KOsroDJ2jS+ShG9RE96iNOogihZ7RK3pzwHlx3p2P5WjJKXZO0R84nz8IA5EZ</latexit>\nC\n<latexit sha1_base64=\"7jjpQIW+cgqVLZrBrpuTmkvsyNs=\">AAAB8nicbVDLSgMxFM3UV62vqks3wSK4KjNS1GXRjSupYB8wHUomzbShmWRI7ghl6Ge4caGIW7/GnX9jpp2Fth4IHM65l5x7wkRwA6777ZTW1jc2t8rblZ3dvf2D6uFRx6hUU9amSijdC4lhgkvWBg6C9RLNSBwK1g0nt7nffWLacCUfYZqwICYjySNOCVjJ78cExhyy+1llUK25dXcOvEq8gtRQgdag+tUfKprGTAIVxBjfcxMIMqKBU8FmlX5qWELohIyYb6kkMTNBNo88w2dWGeJIafsk4Ln6eyMjsTHTOLSTeUSz7OXif56fQnQdZFwmKTBJFx9FqcCgcH4/HnLNKIipJYRqbrNiOiaaULAt5SV4yyevks5F3busNx4ateZNUUcZnaBTdI48dIWa6A61UBtRpNAzekVvDjgvzrvzsRgtOcXOMfoD5/MHGMWRJA==</latexit>\nN\nFigure 2: SCM of Causal-Debias. Each raw sentence\nof X is generated by a mix of causal factor Cand non-\ncausal factor N. Note that only the causal factor affects\nthe ground truth label Y, while the hammer indicates\nthe intervention on non-causal factor.\nWe use a Structure Causal Model (SCM) to char-\nacterize biases in the fine-tuning procedure. As\nshown in Fig. 2, there are four variables: input\nraw sentence X, downstream ground-truth label Y,\ncausal factor Cand non-causal factor N. Among\nthose, causal and non-causal factor C and N are\nlatent variables. Whether a factor is causal or\nnon-causal depends on the specific downstream\ntasks. For example, in sentiment classification task,\ncausal factors could be adjective sentiment words\nsuch as good or bad, and non-causal factors could\nbe nouns or pronouns. In contrast, in coreference\nresolution task, pronouns could be causal factors\nwhile adjective words could be non-causal. Here\nwe explain the diagram in detail:\n• C →X ←N. The input raw sentence X is\na mix of two factors that are theoretically non-\nintersecting: causal factor Cand non-causal fac-\ntor N.\n• C →Y. From a causal view, the ground-truth\nlabel Y is only determined by causal factor C.\n• C ⇠⇢N. The dashed arrow delegates addi-\ntional probabilistic dependencies (Pearl et al.,\n2016, 2000) between causal factor C and non-\ncausal factor N – cf. Appendix A for examples.\nHowever, N ⇠⇢C →Y can create a spuri-\nous association between non-causal factor N and\nground-truth label Y (denoted as Y ⫫̸ N), so that\nCbecomes a confounder between N and Y which\nopens a backdoor path N ← C → Y. Hence,\nthe unwanted stereotypical associations, which we\nassume are non-causal factors, may re-enter the\nfine-tuned PLM because of the unwanted associa-\ntions between label Y and non-causal factors N.\n4229\nTo mitigate the bias propagation and avoid such\nspurious association that N affects Y through C,\nwe make the feature induction assumption spurred\nby the Independent Causal Mechanisms (ICM)\nprinciple (Peters et al., 2017; Schölkopf et al.,\n2012), i.e., the intervention on non-causal factor\nN should be independent of the ground truth Y.\nAccording to causal inference (Pearl et al., 2016,\n2000), there is a directed link from each of its par-\nent variables P(X) to X, if and only if the causal\nmechanism X =fX(P(X)) exists for each vari-\nable X. That is, there exists a function from causal\nfactor C to label Y without N’s influence, mak-\ning the causal association C →Y invariant across\ndifferent N. More formal assumptions in terms\nof N ⇠⇢ C are presented in Appendix A. In\nother words, ICM principle can assure the language\nmodel to not pick up the non-causal factors, i.e., un-\nwanted associations, in the fine-tuning procedure.\nGenerally, only X and Y are observed during\nthe fine-tuning, while neither the causal factor C\nnor the mapping from C to Y is available. We\nfactorize the language model as a combination of\ntwo modules inspired by (Wu et al., 2022), i.e.,\nM = mY ◦mC, where mC ∶ X →C discov-\ners the causal factor from the observed X, and\nmY ∶C →Y outputs the prediction Y. Empirical\nrisk minimization (ERM) is often used as the opti-\nmization strategy to train mC and mY (Seo et al.,\n2022; Zhou et al., 2022):\nmin\nmC,mY\nR(mY ◦mC(X),Y ), (1)\nwhere R(⋅,⋅) can be any loss function (e.g., cross-\nentropy loss). However, ERM heavily relies on\nthe statistical dependency between the input sen-\ntences and labels, ignoring the critical condition\nY ⫫N ∣ Cto guarantee the invariant causal asso-\nciation C →Y across different N, leading to the\nresurgence of undesired spurious associations.\nRecent causal learning literature has proposed to\nuse invariant risk minimization (IRM) objective to\nreplace ERM (Arjovsky et al., 2019; Chang et al.,\n2020). The causal invariant learning encourages\ncausal factor Cto seek the patterns that are stable\nacross different “enviornments”, while abandoning\nthe unstable non-causal patterns. We follow this\nline of literature, and propose to use causal invari-\nant learning objective to mitigate fine-tuning biases:\nmin\nmC,mY\nR(mY ◦mC(X),Y ),s.t. Y ⫫N ∣ C,\n(2)\nwhere N =X\\ C is the non-causal factor. How-\never, IRM often leverages multiple “enviornments”\nto facilitate causal learning. For example, Peyrard\net al. (2021) use prior knowledge to partition the\ntraining set to form different environments. In NLP\ntasks such as sentiment classification, how to con-\nstruct multiple “enviornments” in the context of\nlanguage model fine-tuning is less studied. Next,\nwe propose a causal intervention method to con-\nstruct “enviornments” for causal learning.\n3.2 Causal-Debias\nCausal Intervention. The high-level idea for\ncausal intervention is to create interventional dis-\ntributions with respect to different demographic\ngroups. Our interventional distribution is obtained\nby augmenting and expanding the original data dis-\ntribution. First, let Wa and Wt denote attribute\nwords and target words, respectively. In the case\nof gender bias, for instance, target words consist\nof gender-neutral words (e.g., nurse, engineer, pro-\nfessor), and attribute words are composed of the\nfeminine (e.g., she, woman, mother) and masculine\nwords (e.g. he, man, father) (Liang et al., 2020).\nThen we can obtain an augmented datasets Xd:\nXd =Xo ∪Xc, (3)\nwhere Xo denotes the original sentences from the\ndownstream dataset containing any Wa or Wt, and\nXc represents the counterfactual sentences via per-\nforming attribute word counterfactual augmenta-\ntion on Xo. For counterfactual augmentation, we\ncreate counterfactual sentences by replacing the\nattribute word to its corresponding pair (e.g., he-\n>she). However, the augmented dataset Xd still\nhas limitations as it is not sufficient to cover the\ndiversity of demographic groups, which may cause\ndebiasing performance degradation problems ( cf.\nSection 4.3 for empirical study). To create suf-\nficiently complex interventional distributions and\nobtain the most different demographic groups, we\nconduct the interventions by doing semantic match-\ning between Xd with external corpora E, expand-\ning Xo and Xc to X˜o and X˜c (see Table 1 for an\nexample), respectively, as\nX˜d =X˜o ∪X˜c =Topk(sim(Xd,E))∪Xd, (4)\nwhere sim(⋅) denotes the cosine similarity for se-\nmantic matching, and Topk(⋅) selects the top- k\nsemantic similar sentences. After obtaining the\n4230\n<latexit sha1_base64=\"MFoILQeYqcoBEu5m46rJHDjnvKc=\">AAAB7nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKqMegF48RzAOSJcxOZpMhszPLTK8QQj7CiwdFvPo93vwbJ8keNLGgoajqprsrSqWw6PvfXmFtfWNzq7hd2tnd2z8oHx41rc4M4w2mpTbtiFouheINFCh5OzWcJpHkrWh0N/NbT9xYodUjjlMeJnSgRCwYRSe1uhpFwm2vXPGr/hxklQQ5qUCOeq/81e1rliVcIZPU2k7gpxhOqEHBJJ+WupnlKWUjOuAdRxV1S8LJ/NwpOXNKn8TauFJI5urviQlNrB0nketMKA7tsjcT//M6GcY34USoNEOu2GJRnEmCmsx+J31hOEM5doQyI9ythA2poQxdQiUXQrD88ippXlSDq+rlw2WldpvHUYQTOIVzCOAaanAPdWgAgxE8wyu8ean34r17H4vWgpfPHMMfeJ8/iNmPtg==</latexit>\n⌦\n<latexit sha1_base64=\"x1Mr88PPgEh3177YtYOelgWM6JY=\">AAACB3icbVC7SgNBFJ31GeMrainIYhCswm4IahmwsbCIYB6QLOHuZJIMmZ1dZu4Gw7Kdjb9iY6GIrb9g5984SbbQxAMDh3PuZe45fiS4Rsf5tlZW19Y3NnNb+e2d3b39wsFhQ4exoqxOQxGqlg+aCS5ZHTkK1ooUg8AXrOmPrqd+c8yU5qG8x0nEvAAGkvc5BTRSt3DSCQCHFERym3aTDrIHTLgcg+IgMU27haJTcmawl4mbkSLJUOsWvjq9kMYBk0gFaN12nQi9BBRyKlia78SaRUBHMGBtQyUETHvJLEdqnxmlZ/dDZZ5Ee6b+3kgg0HoS+GZyerVe9Kbif147xv6VZ3JFMTJJ5x/1Y2FjaE9LsXtcMYpiYghQxc2tNh2CAoqmurwpwV2MvEwa5ZJ7UarcVYrVclZHjhyTU3JOXHJJquSG1EidUPJInskrebOerBfr3fqYj65Y2c4R+QPr8wfSTJqG</latexit>\nL invariant\n<latexit sha1_base64=\"x1Mr88PPgEh3177YtYOelgWM6JY=\">AAACB3icbVC7SgNBFJ31GeMrainIYhCswm4IahmwsbCIYB6QLOHuZJIMmZ1dZu4Gw7Kdjb9iY6GIrb9g5984SbbQxAMDh3PuZe45fiS4Rsf5tlZW19Y3NnNb+e2d3b39wsFhQ4exoqxOQxGqlg+aCS5ZHTkK1ooUg8AXrOmPrqd+c8yU5qG8x0nEvAAGkvc5BTRSt3DSCQCHFERym3aTDrIHTLgcg+IgMU27haJTcmawl4mbkSLJUOsWvjq9kMYBk0gFaN12nQi9BBRyKlia78SaRUBHMGBtQyUETHvJLEdqnxmlZ/dDZZ5Ee6b+3kgg0HoS+GZyerVe9Kbif147xv6VZ3JFMTJJ5x/1Y2FjaE9LsXtcMYpiYghQxc2tNh2CAoqmurwpwV2MvEwa5ZJ7UarcVYrVclZHjhyTU3JOXHJJquSG1EidUPJInskrebOerBfr3fqYj65Y2c4R+QPr8wfSTJqG</latexit>\nL invariant\n<latexit sha1_base64=\"iWayBTMEUYBoTVqAcV+QT7U5cAo=\">AAACCHicbVDLSsNAFJ3UV62vqEsXBovgqiSlqMuCGxcuKtgHNCFMJpN26OTBzI1YQpZu/BU3LhRx6ye482+ctFlo64GBwzn33rn3eAlnEkzzW6usrK6tb1Q3a1vbO7t7+v5BT8apILRLYh6LgYcl5SyiXWDA6SARFIcep31vclX4/XsqJIujO5gm1AnxKGIBIxiU5OrHdohhTDDPbnI3s4E+QKYG+IwUfp67et1smDMYy8QqSR2V6Lj6l+3HJA1pBIRjKYeWmYCTYQGMcJrX7FTSBJMJHtGhohEOqXSy2SG5caoU3whioV4Exkz93ZHhUMpp6KnKYm256BXif94wheDSyViUpEAjMv8oSLkBsVGkYvhMUAJ8qggmgqldDTLGAhNQ2dVUCNbiycuk12xY543WbavebpZxVNEROkFnyEIXqI2uUQd1EUGP6Bm9ojftSXvR3rWPeWlFK3sO0R9onz+dEZr1</latexit>\nL prediction\n<latexit sha1_base64=\"iWayBTMEUYBoTVqAcV+QT7U5cAo=\">AAACCHicbVDLSsNAFJ3UV62vqEsXBovgqiSlqMuCGxcuKtgHNCFMJpN26OTBzI1YQpZu/BU3LhRx6ye482+ctFlo64GBwzn33rn3eAlnEkzzW6usrK6tb1Q3a1vbO7t7+v5BT8apILRLYh6LgYcl5SyiXWDA6SARFIcep31vclX4/XsqJIujO5gm1AnxKGIBIxiU5OrHdohhTDDPbnI3s4E+QKYG+IwUfp67et1smDMYy8QqSR2V6Lj6l+3HJA1pBIRjKYeWmYCTYQGMcJrX7FTSBJMJHtGhohEOqXSy2SG5caoU3whioV4Exkz93ZHhUMpp6KnKYm256BXif94wheDSyViUpEAjMv8oSLkBsVGkYvhMUAJ8qggmgqldDTLGAhNQ2dVUCNbiycuk12xY543WbavebpZxVNEROkFnyEIXqI2uUQd1EUGP6Bm9ojftSXvR3rWPeWlFK3sO0R9onz+dEZr1</latexit>\nL prediction\nPLM\nOther\n…\nSST-2\nOriginal Datasets\nWikiText-2 \nSST Reddit \nMELD POM\nExternal Corpora\nPLM\nIntervented Datasets\nOther\n…\nSST-2\nAttribute\nTarget\nWord Lists\nCausal Intervention Causal Invariant LearningBiases Analysis\n<latexit sha1_base64=\"zwkHjHwDnkSZRBSA6GkMkRy1PQQ=\">AAAB8nicbVDLSsNAFL2pr1pfVZduBovgqiQi6rLoxmUF+4A2lMl00g6dTMLMjVBCP8ONC0Xc+jXu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wSTu9zvPHFtRKwecZpwP6IjJULBKFqp148ojgVm3VllUK25dXcOskq8gtSgQHNQ/eoPY5ZGXCGT1Jie5yboZ1SjYJLPKv3U8ISyCR3xnqWKRtz42TzyjJxZZUjCWNunkMzV3xsZjYyZRoGdzCOaZS8X//N6KYY3fiZUkiJXbPFRmEqCMcnvJ0OhOUM5tYQyLWxWwsZUU4a2pbwEb/nkVdK+qHtX9cuHy1rjtqijDCdwCufgwTU04B6a0AIGMTzDK7w56Lw4787HYrTkFDvH8AfO5w8oAZEu</latexit>\nX\n<latexit sha1_base64=\"0litX/NiA5r8pfRvtaeC298dQVI=\">AAAB6HicbZC7SgNBFIbPxltcb1FLm8UgWIVdEbURgzaWCZiLJEuYnZxNxszOLjOzQgh5AhsLRWz1YextxLdxcik08YeBj/8/hznnBAlnSrvut5VZWFxaXsmu2mvrG5tbue2dqopTSbFCYx7LekAUciawopnmWE8kkijgWAt6V6O8do9SsVjc6H6CfkQ6goWMEm2s8m0rl3cL7ljOPHhTyF982OfJ+5ddauU+m+2YphEKTTlRquG5ifYHRGpGOQ7tZqowIbRHOtgwKEiEyh+MBx06B8ZpO2EszRPaGbu/OwYkUqofBaYyIrqrZrOR+V/WSHV45g+YSFKNgk4+ClPu6NgZbe20mUSqed8AoZKZWR3aJZJQbW5jmyN4syvPQ/Wo4J0UjstuvngJE2VhD/bhEDw4hSJcQwkqQAHhAZ7g2bqzHq0X63VSmrGmPbvwR9bbDxnBkCM=</latexit>\nY\n<latexit sha1_base64=\"n7XhcwuMgTMrigEUMGqH9OxnL1g=\">AAAB8nicbVDLSgMxFM3UV62vqks3wSK4KjNS1GWxG5cV7AOmQ8mkmTY0kwzJHaEM/Qw3LhRx69e482/MtLPQ1gOBwzn3knNPmAhuwHW/ndLG5tb2Tnm3srd/cHhUPT7pGpVqyjpUCaX7ITFMcMk6wEGwfqIZiUPBeuG0lfu9J6YNV/IRZgkLYjKWPOKUgJX8QUxgwiFrzSvDas2tuwvgdeIVpIYKtIfVr8FI0TRmEqggxviem0CQEQ2cCjavDFLDEkKnZMx8SyWJmQmyReQ5vrDKCEdK2ycBL9TfGxmJjZnFoZ3MI5pVLxf/8/wUotsg4zJJgUm6/ChKBQaF8/vxiGtGQcwsIVRzmxXTCdGEgm0pL8FbPXmddK/q3nW98dCoNe+KOsroDJ2jS+ShG9RE96iNOogihZ7RK3pzwHlx3p2P5WjJKXZO0R84nz8IA5EZ</latexit>\nC\n<latexit sha1_base64=\"7jjpQIW+cgqVLZrBrpuTmkvsyNs=\">AAAB8nicbVDLSgMxFM3UV62vqks3wSK4KjNS1GXRjSupYB8wHUomzbShmWRI7ghl6Ge4caGIW7/GnX9jpp2Fth4IHM65l5x7wkRwA6777ZTW1jc2t8rblZ3dvf2D6uFRx6hUU9amSijdC4lhgkvWBg6C9RLNSBwK1g0nt7nffWLacCUfYZqwICYjySNOCVjJ78cExhyy+1llUK25dXcOvEq8gtRQgdag+tUfKprGTAIVxBjfcxMIMqKBU8FmlX5qWELohIyYb6kkMTNBNo88w2dWGeJIafsk4Ln6eyMjsTHTOLSTeUSz7OXif56fQnQdZFwmKTBJFx9FqcCgcH4/HnLNKIipJYRqbrNiOiaaULAt5SV4yyevks5F3busNx4ateZNUUcZnaBTdI48dIWa6A61UBtRpNAzekVvDjgvzrvzsRgtOcXOMfoD5/MHGMWRJA==</latexit>\nN\nFigure 3: Overview of Causal-Debias. Causal Intervention: selecting top ksemantic similar bias-related sentences\nfrom external corpora to cover the most different demographic groups. Causal Invariant Learning: fine-tuning the\nPLM with an invariant loss among different environments.\nintervened sentences, we reconstruct the interven-\ntional distribution by combining X˜d and the rest\nbias-unrelated downstream dataset, both of which\nare applied for causal invariant learning. Table 1\nshows an example of original sentence, and its cor-\nresponding counterfactual and expansion sentences.\nSentence Type Sentence Example\nOriginal proves once again he hasn’t lost his touch\nCounterfactual proves once again she hasn’t lost her touch\nExpansion sachs is a guy with impressive intelligence and passion\nTable 1: An example sentence in SST-2 dataset, and the\ncorresponding counterfactual and expansion. The label is\npositive for all three sentences.\nCausal Invariant Learning. Once we obtain the\nintervened dataset containing original and interven-\ntional data distributions, we proceed with causal in-\nvariant learning. Specifically, we do n-intervention\ndo(N = n), which removes all links from their\nparents P(N) to the variable N while fixing N to\nthe number of demographic n(e.g. n = 2 in the\ncase of gender), to identify C whose relationship\nwith Y is stable across different distributions:\nmin Linvariant =En(R)+Varn(R), (5)\nwhere R = R(M(X),Y ∣ do(N =n)) com-\nputes the risk under the n-interventional distri-\nbution; En(⋅) denotes the risks of different n-\ninterventional distributions; Var(⋅) denotes the vari-\nance of risks over n-interventional distributions.\nTo calculate Linvariant, the PLM is required to\npredict the same results on the sentences X˜o and\nX˜c, which have equivalent semantics but different\nattribute words according to the IRM theory (Ar-\njovsky et al., 2019). Thus in optimization, we have\nthe interventional risk derived from Equation (5):\nR(M(X˜d),Y ∣ do(N = ˜n)) =EC=mC(x),N=˜nl(˜y,y),\n(6)\nwhere x∈X˜d is a sentence instance with its pre-\ndiction ˜yunder the intervention do(N = ˜n), and\ny∈Y is ground-truth label, and l(⋅) denotes inter-\nventional loss function on a single sentence. Here\nwe choose Wasserstein distance (Ramdas et al.,\n2017) as a loss function due to its ability to measure\nthe agreement between the prediction of original\nand post-intervention sentences. The Wasserstein\ndistance between ˜yand yis formalized as below:\nDWasser(˜y,y) = inf\nγ(˜y,y)∈∏\nE(˜y,y)∼γ∣∣˜y−y∣∣, (7)\nPLM Fine-tuning. The above procedure lever-\nages augmented datasets for causal invariant learn-\ning, and we can incorporate the invariant loss with\nspecific downstream tasks to fine-tune a language\nmodel. This way, we can balance the trade-off\nbetween debiasing performance and downstream\ntask performance (Meade et al., 2022). The overall\nobjective of Causal-Debias is:\nmin\nτ\nLprediction +τLinvariant, (8)\nwhere τ is the trade-off coefficiency, and Lprediction\nis the loss function of a specific downstream task,\nsuch as cross-entropy loss for classification and\nmean squared error loss for regression.\n4 Experiments\n4.1 Experimental Settings\nDebiasing Benchmarks. We compare Causal-\nDebias with the following benchmarks. Non-Task-\nSpecific methods including: CDA, Dropout (Web-\nster et al., 2020), Context-Debias (Kaneko and\nBollegala, 2021), Auto-Debias (Guo et al., 2022),\nand MABEL (He et al., 2022), and two Task-\nSpecific methods including Sent-Debias (Liang\net al., 2020) andFairFil (Cheng et al., 2021). In the\nNon-Task-Specific benchmarks, the debiasing stage\nis independent of fine-tuning downstream tasks.\n4231\nCausal-Debias belongs to Task-Specific methods as\ndownstream fine-tuning tasks are involved.\nPretrained Language Models. We use three rep-\nresentative PLMs as the backbone: BERT (De-\nvlin et al., 2019), ALBERT (Lan et al., 2020), and\nRoBERTa (Liu et al., 2019). Following (Guo et al.,\n2022), we implement them using the Huggingface\nTransformers library (Wolf et al., 2020).\nBias Word Lists. Following previous studies, we\nuse human-being-created stereotype/attribute word\nlists to investigate and mitigate biases in PLMs.\nThey are based on the research of the social sci-\nence literature and other disciplines, which can\nreflect cultural or offensive biases. In particular,\nwe consider the gender and race word lists used in\n(Kaneko and Bollegala, 2021) and (Manzini et al.,\n2019), respectively – cf. Appendix B for details.\nExternal Corpora. For fair comparison, we ex-\nploit the same external corpora used in baselines,\nwhich are composed of 183,060 sentences from fol-\nlowing sources: WikiText-2 (Merity et al., 2017),\nStandford Sentimente Treebank (Socher et al.,\n2013), Reddit, MELD (Poria et al., 2019) and POM\n(Park et al., 2014) – cf. Appendix C for details.\nEvaluating Metrics: We evaluate biases in PLM\nembeddings with SEAT (May et al., 2019) and\nCrowS-Pair (Nangia et al., 2020). An ideally unbi-\nased model should exhibit no difference in relative\nsimilarity. Following Guo et al. (2022); Liang et al.\n(2020); Kaneko and Bollegala (2021), we apply\nSEAT 6, 6b, 7, 7b, 8, and 8b tests to measure the\ngender bias, and use SEAT 3, 3b, 4, 5, 5b tests for\nracial bias evaluation. We report the effect size in\nthe SEAT evaluation – the closer to 0, the lower\nbias a model has. More details about SEAT tests\nare presented in Appendix D. We also use Crowd-\nsourced Stereotype Pairs ( CrowS-Pair) (Nangia\net al., 2020) as another metric to evaluate gender\nbias. CrowS-Pair is a dataset containing 1,508 ex-\namples covering different types of biases, where\neach example is a stereotype/anti-stereotype sen-\ntence pair with minimal semantics. The CrowS-\nPair score closer to 50% is less stereotypical, indi-\ncating that the model assigns an equal probability\nto male and female sentences.\nOther Details. As the related studies (Cheng et al.,\n2021; Liang et al., 2020), we conduct experiments\non three downstream tasks, including a sentiment\nclassification task SST-2, a grammatical acceptabil-\nity judgment task CoLA, and a question-answering\ntask QNLI. We follow the same PLMs as bench-\nmarks: 1) BERT-base-uncased, ALBERT-large-v2,\nand RoBERTa-base in gender case; and 2) BERT-\nbase-uncased and ALBERT-base-v2 in racial case.\nWe trained Causal-Debias in 5 epochs with learning\nrate 2 ×e−5. The reported results are the average\nof 5 runs for all downstream tasks.\n4.2 Results on Mitigating Gender Bias\nSEAT Tests and Downstream Tasks Evaluation.\nTable 2 summarizes the debiasing results of models\nbefore and after fine-tuning on three downstream\ntasks , as well as the accuracy (Acc.) evaluations\non downstream applications, from which we have\nthe following Observations.\n(O1): Causal-Debias is more effective in miti-\ngating gender bias than previous benchmarks, as it\nachieves the lowest average SEAT scores in all\nthree downstream tasks. For example, Causal-\nDebias surpasses Task-Specific SOTA (FairFil) by\n0.07, 0.01, and 0.07, and Non-Task-Specific SOTA\n(Auto-Debias) by 0.27, 0.21, and 0.09 on BERT,\nrespectively. The excellent debiasing results can\nattribute to the following two characteristics of\nCausal-Debias: 1) combining debiasing PLMs\nand fine-tuning to avoid new biases, and 2) the\ncausal intervention-based invariant learning that\nalleviates the impact of non-causal factors.\n(O2): From the SEAT results after fine-tuning\ndownstream tasks, the PLMs become more biased\nfor almost all non-task-specific debiasing models.\nIn particular, the latest debiasing methods Auto-\nDebias is greatly limited to the bias resurgence\nissue in all three tasks, although it achieved good\nperformance on debiasing PLMs. These results\nverified not only the existence of bias resurgence\nbut also the motivation of this study to attenuate the\nintrinsic bias of PLMs and fine-tuning bias jointly.\nInterestingly, the original PLMs do not suffer from\nthis problem, as the bias can be mitigated after fine-\ntuning the downstream tasks in most cases. This\nresult suggests that fine-tuning itself is an effective\nway of debiasing PLMs; combining with debiased\nmodels, however, will introduce extra bias. To\ndeal with this dilemma, debiasing models should\nconsider downstream tasks as a unity.\nTwo task-specific models, i.e., Sent-Debias and\nFairFil, cannot effectively alleviate the application\nbias even using downstream datasets for debiasing\nPLMs, because their focus is still intrinsic bias – by\ncontrast, Causal-Debias unifies the two debiasing\nprocedure and provides an systematic solution to\n4232\nSST-2 CoLA QNLI\nMethods Before After Acc. After Mcc. After Acc.\nBERT 0.35 0.29 ↓0.06 92.7 0.18 ↓0.17 57.6 0.37 ↑0.02 91.3\n+CDA 0.25 0.47 ↑0.22 81.3 0.29 ↑0.04 53.2 0.38 ↑0.13 89.1\n+DROPOUT 0.42 0.48 ↑0.06 81.9 0.27 ↓0.15 52.2 0.44 ↑0.02 90.1\n+CONTEXT -DEBIAS 0.53 0.43 ↓0.10 91.9 0.57 ↑0.04 55.4 0.56 ↑0.03 89.9\n+AUTO -DEBIAS 0.14 0.38 ↑0.24 92.1 0.32 ↑0.18 52.9 0.24 ↑0.10 91.1\n+MABEL 0.50 0.55 ↑0.05 92.2 0.52 ↑0.02 57.8 0.54 ↑0.04 91.6\n+SENT-DEBIAS 0.26 0.21 ↓0.05 89.1 0.22 ↓0.04 55.4 0.32 ↑0.06 90.6\n+FAIR FIL 0.15 0.18 ↑0.03 91.6 0.12 ↓0.03 56.5 0.22 ↑0.07 90.8\n+CAUSAL -DEBIAS (ours) - 0.11 92.9 0.11 58.1 0.15 91.6\nALBERT 0.28 0.22 ↓0.06 92.6 0.24 ↓0.04 58.5 0.21 ↓0.07 91.3\n+CDA 0.30 0.38 ↑0.18 92.4 0.16 ↓0.14 53.1 0.31 ↑0.01 90.9\n+DROPOUT 0.24 0.28 ↑0.04 90.4 0.25 ↑0.01 47.4 0.20 ↓0.04 91.7\n+CONTEXT -DEBIAS 0.33 0.11 ↓0.22 77.3 0.17 ↓0.16 55.4 0.20 ↓0.13 91.6\n+CAUSAL -DEBIAS (ours) - 0.06 92.9 0.16 57.1 0.09 91.6\nRoBERTa 0.67 0.41 ↓0.26 94.8 0.41 ↓0.26 57.6 0.48 ↓0.19 92.8\n+CONTEXT -DEBIAS 1.09 0.26 ↓0.83 80.3 0.30 ↓0.79 55.4 0.37 ↓0.72 91.8\n+CAUSAL -DEBIAS (ours) - 0.09 93.9 0.17 54.1 0.06 92.9\nTable 2: Gender debiasing results of average SEAT and application performance. ↓and ↑denote the improvement\nand reduction in debiasing performance in terms of SEAT, respectively. “-’\" means that our model does not have an\nindependent PLM debiasing process.\nattenuates both biases simultaneously.\n(O3): Previous benchmarks may significantly\ndegrade the performance on downstream tasks af-\nter debiasing the PLMs. This is a natural result of\nexisting debiasing models as they need to change\nthe representations to mitigate bias and therefore\ninevitably decrease accuracy after debiasing (Liang\net al., 2020). In addition to its superior debiasing\neffect, Causal-Debias achieves better performance\non downstream tasks, e.g., exceeding all bench-\nmarks on BERT in terms of performance. This\nresult demonstrates the ability of Causal-Debias to\nminimize the disagreements among different demo-\ngraphic groups with identical semantic information,\nwhich is attained by intervening in downstream\ndatasets to mitigate the bias recurrence.\nOverall Score\nMethods Before (Dev.) After (Dev.)\nBERT 57.25 (7.25) 53.18 (3.18)\n+CDA 56.11 (6.11) 58.42 (8.42)\n+DROPOUT 55.34 (5.34) 44.56 (5.44)\n+CONTEXT -DEBIAS 58.01 (8.01) 58.89 (8.89)\n+AUTO -DEBIAS 54.92 (4.92) 44.96 (5.04)\n+MABEL 50.76 (0.76) 46.75 (3.25)\n+SENT-DEBIAS 52.29 (2.29) 55.04 (5.04)\n+CAUSAL -DEBIAS (ours) - 48.94 (1.06)\nTable 3: CrowS-Pairs scores on SST-2. ‘Dev.’ denotes\nthe value deviates from 50.\nCrowS-Pairs. Table 3 reports CrowS-Pairs scores\nbefore and after fine-tuning on SST-2. Note that the\ndeviation from 50 is usually used to measure the de-\nbiasing effect. Obviously, Causal-Debias achieves\nthe lowest deviation among the fine-tuned debias-\ning models, which proves its best debiasing perfor-\nmance. In addition, the deviation values of previous\nmethods increase after fine-tuning, which further\nconfirms the existence of application bias ignored\nby existing methods. Again, BERT itself can alle-\nviate the bias after fine-tuning, which is consistent\nwith our observation O2 from Table 2.\n4.3 Ablation Study\nTo quantify the effect of the devised causal inter-\nvention and invariant risk learning, we build three\nvariants of Causal-Debias:\n• (V1) w/o Eremoves the intervention from exter-\nnal corpora in Eq. (4).\n• (V2) w/o E&Linvariant removes both interven-\ntions of external corpora in Eq. (4) and the invari-\nant loss in Eq. (8) but maintains counterfactual\naugmentation of data in downstream tasks.\n• (V3) w/o Linvariant only remains the prediction\nloss Lprediction in Eq. (8).\nFig. 4 compares the variants with full Causal-\nDebias on SST-2 task. The variant V1 performs\nworst on debiasing, indicating that external corpora\ncontribute the most to model debiasing. However,\nexternal corpora also deteriorate the task perfor-\n4233\nSST-2 CoLA QNLI\nMethods Before After Acc. After Mcc. After Acc.\nBERT 0.23 0.30 ↑0.07 92.7 0.16 ↓0.07 57.6 0.15 ↓0.08 91.3\n+AUTO -DEBIAS 0.18 0.31 ↑0.13 92.1 0.20 ↑0.02 59.6 0.24 ↑0.06 91.1\n+CAUSAL -DEBIAS (ours) - 0.11 92.9 0.06 57.1 0.11 91.6\nALBERT 0.46 0.29 ↓0.06 92.6 0.19 ↓0.27 58.5 0.10 ↓0.36 92.2\n+AUTO -DEBIAS 0.17 0.39 ↑0.22 86.8 0.18 ↑0.01 56.9 0.36 ↑0.09 91.1\n+CAUSAL -DEBIAS (ours) - 0.13 91.9 0.16 59.6 0.01 92.5\nTable 4: Race debiasing results of average SEAT and application performance, and the original scores are from\nMeade et al. (2022).\nV1 V2 V3 Causal-Debias\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nSEAT\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nSST-2 Accuracy\nAverage SEAT results\nSST-2 Accuracy\nFigure 4: Ablation results. The lower SEAT score (red)\nand higher SST-2 accuracy (blue) are better.\nmance when comparing the accuracy of V2 and V3,\nbecause the augmented sentences introduce extra\nsemantics and obfuscate the PLMs. The adverse\nimpact can be minimized by learning the invari-\nant representations by the proposed causal inter-\nventions on downstream tasks via Linvariant, which\ncan be justified by the great discrepancy between\nCausal-Debias and V3 in terms of SST-2 accuracy.\n4.4 Results on Mitigating Racial Bias\nRacial debiasing refers to examining the\nassociation difference between European-\nAmerican/African American names/terms and\nthe stereotype words (pleasant vs. unpleasant)\n(Caliskan et al., 2017). Unlike gender debiasing,\nfew prior studies investigated the racial debiasing\nproblem, due to the difficulty of mitigating racial\nbias (Meade et al., 2022). A critical challenge is\nthe potential word ambiguity (e.g., white, black)\nin various contexts (Guo et al., 2022). Table 4\nreports the performance of Causal-Debias and\nAuto-Debias – the state-of-the-art racial debiasing\nmodel. Causal-Debias substantially decreases the\nracial biases on PLMs after fine-tuning, while\nobtaining comparable downstream performance.\nAuto-Debias, in contrast, still suffers from bias\nrecurrence issue. Compared to Auto-Debias,\nCausal-Debias is more effective as it exploits\ndownstream datasets for debiasing, which allows\nus to alleviate the influence of ambiguous words.\nBesides, the causal invariant learning in Causal-\nDebias encourages the model to learn consistent\nrepresentations and clear meanings of ambiguous\nwords so as to avoid bias-related associations.\n5 Conclusion\nIn this paper, we propose a debiasing framework\nCausal-Debias to unify PLM bias mitigation with\nfine-tuning. Different from prior literature that\ntreats PLM bias mitigation as a standalone task,\nCausal-Debias incorporates bias mitigation objec-\ntive with downstream fine-tuning using causal in-\nvariant learning. Causal-Debias essentially “kills\ntwo birds with one stone”, because it prevents the\nunwanted stereotypical associations re-entering the\nfine-tuned model and also maintains favorable per-\nformance in downstream tasks. The experiment\nshows that fine-tuning existing debiased models\nwill encode or even amplify unwanted bias asso-\nciations (in gender and race). We also show that\nCausal-Debias can effectively reduce bias associ-\nations in fine-tuned language model without sac-\nrificing downstream task performance. Our paper\ncontributes to the NLP fairness fields by proposing\na novel debiasing method from a causal invariant\nview. More importantly, we highlight the fact that\nbiases can happen at any stage in PLM training\npipeline, including the final fine-tuning steps. Even\nif unwanted stereotypical associations are removed,\nor covered up (Kirkpatrick et al., 2017), in the pre-\ntraining stage, the associations will re-surge in the\ndownstream models. Hence, prior literature that\nfocuses on pretrained model debiasing may be inef-\nfective if used in a pretrain/fine-tune pipeline. We\nhope this study can shed light on mitigating biases\nfor building fair and accountable NLP systems.\n4234\nAcknowledgements\nThis work was supported by the Natural Science\nFoundation of Sichuan Province, China, under\nGrant 2022NSFSC0505, and National Natural Sci-\nence Foundation of China under Grants 62176043\nand 62072077.\nLimitations\nWe now discuss limitations of Causal-Debias. In\nconsideration of the fairness, we follow the prior\nbias mitigation work (Guo et al., 2022; Cheng et al.,\n2021; He et al., 2022) and use human-collected lists\nof gender and racial pairs for counterfactual data\naugmentation and intervened distribution genera-\ntion. It is obvious that the bias word lists are inad-\nequate to cover all the bias-related demographic\ngroups, while we believe the general list is ex-\nhaustive. We consider there is a possible model\nimprovement that leverages the perturbation aug-\nmentation on bias-related sentences along multiple\ndemographic axes (Qian et al., 2022). Another pos-\nsible improvement would be to generate bias words\nby using prompts to probe the biases that may lead\nto a bad effect.\nMoreover, we also considered the use of exter-\nnal corpora. The external corpora have been sig-\nnificantly investigated in prior works (Liang et al.,\n2020; Cheng et al., 2021) and are utilized as an\nintervention corpora. Recently, He et al. (2022)\nused two natural language inference data (SNLI\nand MNLI with gender terms) to produce general-\npurpose debiased representations. There are sev-\neral other corpora including News-commentary-v1\n(Kaneko and Bollegala, 2021), Wikipedia (Zmi-\ngrod et al., 2019; Webster et al., 2020), and\nWikitext-2 (Guo et al., 2022). A possible future\ndirection of debiasing is how to mitigate the biases\nwithout heavily relying on any corpora and just\nusing internal knowledge.\nMoreover, in the paper we primarily focus on\nstudying gender and racial bias mitigation. It is\nalso worth exploring intersectional biases mitiga-\ntion (Lalor et al., 2022) and domain-specific bias\nmitigation (Chuang and Yang, 2022; Abbasi et al.,\n2021).\nWe would also like to note that although Causal-\nDebias shows a satisfactory performance on SEAT\ntests and Crows-Pairs, these results should not be\ninterpreted as a complete bias mitigation. Interest-\ningly, He et al. (2022) expressed the same opinion.\nThe main metrics (like CrowS-Pairs) are mainly\nagainst North American social biases and only re-\nflect positive predictive power. They detect the\npresence of the biases but not their absence (Meade\net al., 2022). He et al. (2022) did not use the SEAT\ntests and evaluated their model on various metrics.\nFrom the perspective of different usage scenarios,\nwe need a more general and reliable debias metric\nfor comparison between different models. The lack\nof universality and agreement in existing evalua-\ntion frameworks is a fundamental challenge in this\nfield.\nEthics Statement\nRegarding ethical concerns, we would like to note\nthat our contributions are mainly about methodolo-\ngies. The datasets and evaluation metrics in our\nwork are also widely used in prior works. One ethi-\ncal concern is the binarization of the genders and\nraces, which is an over-simplification and is not\nproper to practical situations. Binarization is the\ncommon problem among most debiasing methods\nand we totally agree and support the development\nof more inclusive methodological tools, datasets,\nand evaluation methods.\nUnder our framework, we consider gender or\nrace isolatedly and neglect the particular intersec-\ntional biases. It is apparent that the pretrained lan-\nguage model cannot be applied or operated in an\nideal environment, and should be able to handle\ncomplex combinations of biases simultaneously.\nAnother ethical consideration is that Causal-\nDebias is entirely based on a English system. Such\nan assumption may not be a problem now but\nsooner or later it will be. The debias studies have\nto be situated on the high-resource languages while\nconsidering not only high-resource language sys-\ntems but how to debias on the low-resource lan-\nguages.\nFor instance, some languages such as Span-\nish, German, Chinese, or Japanese contain vari-\nous words to describe masculine or feminine forms.\nThe detection and removal of biases are greatly\ncomplicated by the need to consider both linguistic\nand social gender.\nFor the reasons above, practitioners should be\nvery cautious when applying our framework to\nreal-world use cases. In its current state, Causal-\nDebias should be seen not as a panacea for ad-\ndressing biases in NLP, but rather as another initial\neffort to illuminate and undercut a critical, elusive,\nmultifaceted problem.\n4235\nReferences\nAhmed Abbasi, David Dobolyi, John P Lalor, Richard G\nNetemeyer, Kendall Smith, and Yi Yang. 2021. Con-\nstructing a psychometric testbed for fair natural lan-\nguage processing. In EMNLP, pages 3748–3758.\nMartin Arjovsky, Léon Bottou, Ishaan Gulrajani, and\nDavid Lopez-Paz. 2019. Invariant risk minimization.\narXiv :1907.02893.\nAylin Caliskan, Joanna J Bryson, and Arvind Narayanan.\n2017. Semantics derived automatically from lan-\nguage corpora contain human-like biases. Science,\n356(6334):183–186.\nShiyu Chang, Yang Zhang, Mo Yu, and Tommi Jaakkola.\n2020. Invariant rationalization. In ICML, pages\n1448–1458. PMLR.\nPengyu Cheng, Weituo Hao, Siyang Yuan, Shijing Si,\nand Lawrence Carin. 2021. Fairfil: Contrastive neu-\nral debiasing method for pretrained text encoders. In\nICLR.\nChengyu Chuang and Yi Yang. 2022. Buy tesla, sell\nford: Assessing implicit stock market preference in\npre-trained language models. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers), pages\n100–105.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL, pages 4171–4186.\nSeraphina Goldfarb-Tarrant, Rebecca Marchant, Ri-\ncardo Muñoz Sánchez, Mugdha Pandya, and Adam\nLopez. 2021. Intrinsic bias metrics do not correlate\nwith application bias. In ACL, pages 1926–1940.\nHila Gonen and Yoav Goldberg. 2019. Lipstick on a\npig: Debiasing methods cover up systematic gender\nbiases in word embeddings but do not remove them.\narXiv preprint arXiv:1903.03862.\nIan J Goodfellow, Mehdi Mirza, Da Xiao, Aaron\nCourville, and Yoshua Bengio. 2013. An em-\npirical investigation of catastrophic forgetting in\ngradient-based neural networks. arXiv preprint\narXiv:1312.6211.\nYue Guo, Yi Yang, and Ahmed Abbasi. 2022. Auto-\ndebias: Debiasing masked language models with au-\ntomated biased prompts. In ACL, pages 1012–1023.\nJacqueline He, Mengzhou Xia, Christiane Fell-\nbaum, and Danqi Chen. 2022. Mabel: Atten-\nuating gender bias using textual entailment data.\narXiv:2210.14975.\nMasahiro Kaneko and Danushka Bollegala. 2021. De-\nbiasing pre-trained contextualised embeddings. In\nEACL.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, et al. 2017. Over-\ncoming catastrophic forgetting in neural networks.\nProceedings of the national academy of sciences ,\n114(13):3521–3526.\nJohn P Lalor, Yi Yang, Kendall Smith, Nicole Fors-\ngren, and Ahmed Abbasi. 2022. Benchmarking inter-\nsectional biases in nlp. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3598–3609.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations. In ICLR.\nShaobo Li, Xiaoguang Li, Lifeng Shang, Zhenhua Dong,\nCheng-Jie Sun, Bingquan Liu, Zhenzhou Ji, Xin\nJiang, and Qun Liu. 2022. How pre-trained language\nmodels capture factual knowledge? a causal-inspired\nanalysis. In Findings of the Association for Compu-\ntational Linguistics: ACL 2022, pages 1720–1732.\nPaul Pu Liang, Irene Mengze Li, Emily Zheng,\nYao Chong Lim, Ruslan Salakhutdinov, and Louis-\nPhilippe Morency. 2020. Towards debiasing sentence\nrepresentations. In ACL, pages 5502–5515.\nPaul Pu Liang, Yao Chong Lim, Yao-Hung Hubert Tsai,\nRuslan Salakhutdinov, and Louis-Philippe Morency.\n2019. Strong and simple baselines for multimodal\nutterance embeddings. In NAACL, pages 2599–2609.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv:1907.11692.\nFangrui Lv, Jian Liang, Shuang Li, Bin Zang,\nChi Harold Liu, Ziteng Wang, and Di Liu. 2022.\nCausality inspired representation learning for domain\ngeneralization. In CVPR, pages 8046–8056.\nThomas Manzini, Lim Yao Chong, Alan W Black, and\nYulia Tsvetkov. 2019. Black is to criminal as cau-\ncasian is to police: Detecting and removing multi-\nclass bias in word embeddings. In NAACL, pages\n615–621.\nChandler May, Alex Wang, Shikha Bordia, Samuel Bow-\nman, and Rachel Rudinger. 2019. On measuring so-\ncial biases in sentence encoders. In NAACL, pages\n622–628.\nNicholas Meade, Elinor Poole-Dayan, and Siva Reddy.\n2022. An empirical survey of the effectiveness of\ndebiasing techniques for pre-trained language models.\nIn ACL, pages 1878–1898.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In ICLR.\n4236\nKrikamol Muandet, David Balduzzi, and Bernhard\nSchölkopf. 2013. Domain generalization via invari-\nant feature representation. In ICML, pages 10–18.\nPMLR.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel Bowman. 2020. Crows-pairs: A challenge\ndataset for measuring social biases in masked lan-\nguage models. In EMNLP, pages 1953–1967.\nSunghyun Park, Han Suk Shim, Moitreya Chatterjee,\nKenji Sagae, and Louis-Philippe Morency. 2014.\nComputational analysis of persuasiveness in social\nmultimedia: A novel dataset and multimodal predic-\ntion approach. In ICMI, pages 50–57.\nJudea Pearl, Madelyn Glymour, and Nicholas P Jewell.\n2016. Causal Inference in Statistics: A Primer. John\nWiley & Sons.\nJudea Pearl et al. 2000. Models, reasoning and infer-\nence. Cambridge, UK: CambridgeUniversityPress,\n19(2).\nJonas Peters, Peter Bühlmann, and Nicolai Meinshausen.\n2016. Causal inference by using invariant prediction:\nidentification and confidence intervals. Journal of\nthe Royal Statistical Society: Series B (Statistical\nMethodology), 78(5):947–1012.\nJonas Peters, Dominik Janzing, and Bernhard Schölkopf.\n2017. Elements of causal inference: foundations and\nlearning algorithms. The MIT Press.\nMaxime Peyrard, Sarvjeet Singh Ghotra, Martin Josi-\nfoski, Vidhan Agarwal, Barun Patra, Dean Carignan,\nEmre Kiciman, and Robert West. 2021. Invariant\nlanguage modeling. arXiv:2110.08413.\nSoujanya Poria, Devamanyu Hazarika, Navonil Ma-\njumder, Gautam Naik, Erik Cambria, and Rada Mi-\nhalcea. 2019. Meld: A multimodal multi-party\ndataset for emotion recognition in conversations. In\nACL, pages 527–536.\nChen Qian, Fuli Feng, Lijie Wen, Chunping Ma, and\nPengjun Xie. 2021. Counterfactual inference for text\nclassification debiasing. In ACL, pages 5434–5445.\nRebecca Qian, Candace Ross, Jude Fernandes, Eric\nSmith, Douwe Kiela, and Adina Williams. 2022. Per-\nturbation augmentation for fairer nlp. arXiv preprint\narXiv:2205.12586.\nAaditya Ramdas, Nicolás García Trillos, and Marco\nCuturi. 2017. On wasserstein two-sample testing\nand related families of nonparametric tests. Entropy,\n19(2):47.\nBernhard Schölkopf, Dominik Janzing, Jonas Peters,\nEleni Sgouritsa, Kun Zhang, and Joris M Mooij. 2012.\nOn causal and anticausal learning. In ICML.\nSeonguk Seo, Joon-Young Lee, and Bohyung Han.\n2022. Unsupervised learning of debiased representa-\ntions with pseudo-attributes. In CVPR, pages 16742–\n16751.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn EMNLP, pages 1631–1642.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research, 9(11).\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In EMNLP, pages\n353–355.\nKellie Webster, Xuezhi Wang, Ian Tenney, Alex Beu-\ntel, Emily Pitler, Ellie Pavlick, Jilin Chen, Ed Chi,\nand Slav Petrov. 2020. Measuring and reduc-\ning gendered correlations in pre-trained models.\narXiv:2010.06032.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2020. Transformers: State-of-the-art natural\nlanguage processing. In EMNLP, pages 38–45.\nYing-Xin Wu, Xiang Wang, An Zhang, Xiang-\nnan He, and Tat-Seng Chua. 2022. Discover-\ning invariant rationales for graph neural networks.\narXiv:2201.12872.\nXiao Zhou, Yong Lin, Renjie Pi, Weizhong Zhang, Ren-\nzhe Xu, Peng Cui, and Tong Zhang. 2022. Model\nagnostic sample reweighting for out-of-distribution\nlearning. In ICML, pages 27203–27221. PMLR.\nRan Zmigrod, Sabrina J Mielke, Hanna Wallach, and\nRyan Cotterell. 2019. Counterfactual data augmenta-\ntion for mitigating gender stereotypes in languages\nwith rich morphology. In ACL, pages 1651–1661.\n4237\nA Instantiated Causal Graphs\nWe instantiate causal graphs as shown in Fig. 2.\nSpecifically, we use the example sentences, whose\nlabels are determined by how words compose the\nmeaning of the sentences. We use C = 0,1,2 to\ndenote three different sentiment-related phrases,\nand use N =0,1,2 to denote three different non-\ncausal factors for simplicity.\n• C ⫫N: The raw input sentences and bias-\nrelated parts are independently sampled and\nspliced.\n• C →N: The type of each causal part respects\na given (static) probability distribution. The\nvalue of C, and the probability distribution of\nits causal part is given by:\nP(N) ={0.9 if X =C,\n0.1 otherwise. (9)\n• N →C: Similar to the example for C →N.\n• N ←V →C: There is a latent variable V\ntakes continuous value from 0 to 1, and the\nprobability distribution of N and Cs.t.\nN ∼B(3,V ), C ∼B(3,1 −V), (10)\nwhere B stands for binomial distribution, i.e.,\nfor the variable C, if C ∼B(n,p) we have\nP(C =k∣ p,n) =(n\nk)pk(1 −p)n−k.\nB Bias Words List\nWe used the gender attribute words and target\nwords lists proposed in (Kaneko and Bollegala,\n2021), which is widely used in debiasing studies\n(Guo et al., 2022; Liang et al., 2020). In addition,\nwe used the race attribute words and attribute words\nprovided in (Manzini et al., 2019).\nC External Corpora\nTable 5 summarizes the five external datasets along\nwith examples of the numerous templates occur-\nring across various individuals, settings and in both\nwritten and spoken text. The external corpora E\nused to expand the downstream dataset is created\nfrom (Liang et al., 2020), including: 1)WikiText-2\n(Merity et al., 2017) – a dataset of formally writ-\nten Wikipedia articles, where only the first 10%\nof WikiText-2 is used and verified sufficiently to\ncapture formally written text); 2) Stanford Sen-\ntiment Treebank (Socher et al., 2013) is a col-\nlection of 10000 polarized written movie reviews;\n3) Reddit data collected from discussion forums\nrelating to politics, electronics, and relationships;\n4) MELD (Poria et al., 2019) – a large-scale mul-\ntimodal multi-party emotional dialog dataset col-\nlected from the TV-series Friends; and 5) POM\n(Park et al., 2014) – a dataset of spoken review\nvideos collected across 1,000 individuals spanning\nmultiple topics. These datasets have also been used\nin recent research in language understanding (Mer-\nity et al., 2017; Liu et al., 2019) and multimodal\nhuman language (Liang et al., 2019).\nD SEAT Details\nThe WEAT metric measures the bias by compar-\ning two sets of attribute words Wa (i.e., M and\nF) and two sets of target words Wt (i.e., A and\nB). In the case of gender, M denotes masculine\nwords like “he”, andF denotes feminine words like\n“she”. Meanwhile, A and B are gender-neutral\nwords (e.g., career or adjectives) whose embed-\ndings should be equivalent between M and F. For-\nmally, the bias degree of each wordwis defined as:\ns(w,A,B ) = 1\n∣A∣ ∑\na∈A\ncos(w,a)− 1\n∣B∣ ∑\nb∈B\ncos(w,b),\n(11)\nwhere cos(⋅,⋅)denotes the cosine similarity. Based\non Equation (11), the WEAT effect size is:\ndWEAT = µ({s(m,A,B )}m∈M)−µ({s(f,A,B )}f∈F)\nσ({s(t,A,B )}t∈A∪B)) ,\n(12)\nwhere µand σdenote the mean and standard devi-\nation, respectively. The SEAT metric generalizes\nthe WEAT via replacing the word embeddings with\na few simple sentence templates (e.g., “This is the\n<word>”). We can conclude from Equation (12)\nthat the absolute SEAT effect size closer to 0 means\nlower biases. We list more details about the SEAT\ntests that are used in our experiments in Table 6,\nwhich are adapted from (Caliskan et al., 2017).\n4238\nDataset Type Topics Formality Length Examples\nWikitext-2written everything formal 24.0\n“Ireland has made a large contribution to world literature in all its branches,\nparticularly in the English language. Poetry in Irish is among the oldest vernacular poetry\ninEurope/Africa, with the earliest examples dating from the 6th century.”\nSST written movie reviews informal 19.2 “his/herfans walked out muttering words like horrible and terrible,\nbut had so much fun dissing the film that they didn’t mind the ticket cost.”\nReddit written\npolitics,\nelectronics,\nrelationshipsinformal 13.6\n“roommate cut my hair without my consent,\nended up cuttinghimself/herselfand is threatening\nto call the police on me”\nMELD spoken comedy TV-series informal 8.1 “that’s the kind of strength that I want in theman/womanI love!”\nPOM spoken opinion videos informal 16.0 “and his/herfamily is, like, incredibly confused”\nTable 5: The five external corpora Eused to expand the downstream dataset (Liang et al., 2020). Length represents\nthe average length measured by the number of words in a sentence. Words in italics indicate the words used to\nintervene by casual invariant learning, e.g., (man, woman), (Europe, Africa). This table summarizes our expanded\ndataset in terms of topics, formality, and spoken/written text.\nBias Type Test Demographic-specific words Stereotype words\nRacial\nSEAT-3 European-American/African American names Pleasant vs. Unpleasant\nSEAT-3b European-American/African American terms Pleasant vs. Unpleasant\nSEAT-4 European-American/African American names Pleasant vs. Unpleasant\nSEAT-5 European-American/African American names Pleasant vs. Unpleasant\nSEAT-5b European-American/African American terms Pleasant vs. Unpleasant\nGender\nSEAT-6 Male vs. Female names Career vs. Family\nSEAT-6b Male vs. Female terms Career vs. Family\nSEAT-7 Male vs. Female terms Math vs. Arts\nSEAT-7b Male vs. Female names Math vs. Arts\nSEAT-8 Male vs. Female names Science vs. Arts\nSEAT-9b Male vs. Female terms Science vs. Arts\nTable 6: The SEAT test details extended from (Caliskan et al., 2017).\n4239\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n7\n□\u0013 A2. Did you discuss any potential risks of your work?\n7,8\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\n4\n□\u0013 B1. Did you cite the creators of artifacts you used?\n4\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\n4\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\n4\n□\u0013 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\n4\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\n4\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\n4\nC □\u0013 Did you run computational experiments?\n4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n4\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n4240\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n4\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n4\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n4\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n4241"
}