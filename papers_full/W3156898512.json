{
    "title": "Constructing Taxonomies from Pretrained Language Models",
    "url": "https://openalex.org/W3156898512",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2112968783",
            "name": "Catherine Chen",
            "affiliations": [
                "University of California, Berkeley"
            ]
        },
        {
            "id": "https://openalex.org/A2110754524",
            "name": "Kevin Lin",
            "affiliations": [
                "University of California, Berkeley"
            ]
        },
        {
            "id": "https://openalex.org/A2056960045",
            "name": "Dan Klein",
            "affiliations": [
                "University of California, Berkeley"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3106025724",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W6275383",
        "https://openalex.org/W3034444248",
        "https://openalex.org/W2326857978",
        "https://openalex.org/W3034675880",
        "https://openalex.org/W38703128",
        "https://openalex.org/W2996580882",
        "https://openalex.org/W2995647371",
        "https://openalex.org/W3035032094",
        "https://openalex.org/W2279316390",
        "https://openalex.org/W2153653572",
        "https://openalex.org/W2965597800",
        "https://openalex.org/W2100132811",
        "https://openalex.org/W3097977265",
        "https://openalex.org/W2038721957",
        "https://openalex.org/W2963173796",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2250599741",
        "https://openalex.org/W2142086811",
        "https://openalex.org/W2595644810",
        "https://openalex.org/W2998557616",
        "https://openalex.org/W2906251750",
        "https://openalex.org/W2250601048",
        "https://openalex.org/W3035091181",
        "https://openalex.org/W2001970439",
        "https://openalex.org/W2465611764",
        "https://openalex.org/W2068737686",
        "https://openalex.org/W2970986510",
        "https://openalex.org/W2962909572",
        "https://openalex.org/W1898013218",
        "https://openalex.org/W2950339735"
    ],
    "abstract": "We present a method for constructing taxonomic trees (e.g., WordNet) using pretrained language models. Our approach is composed of two modules, one that predicts parenthood relations and another that reconciles those predictions into trees. The parenthood prediction module produces likelihood scores for each potential parent-child pair, creating a graph of parent-child relation scores. The tree reconciliation module treats the task as a graph optimization problem and outputs the maximum spanning tree of this graph. We train our model on subtrees sampled from WordNet, and test on non-overlapping WordNet subtrees. We show that incorporating web-retrieved glosses can further improve performance. On the task of constructing subtrees of English WordNet, the model achieves 66.7 ancestor F1, a 20.0% relative increase over the previous best published result on this task. In addition, we convert the original English dataset into nine other languages using Open Multilingual WordNet and extend our results across these languages.",
    "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 4687–4700\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n4687\nConstructing Taxonomies from Pretrained Language Models\nCatherine Chen∗ Kevin Lin∗ Dan Klein\nUniversity of California, Berkeley\n{cathychen,k-lin,klein}@berkeley.edu\nAbstract\nWe present a method for constructing tax-\nonomic trees (e.g., W ORD NET) using pre-\ntrained language models. Our approach is\ncomposed of two modules, one that predicts\nparenthood relations and another that recon-\nciles those predictions into trees. The par-\nenthood prediction module produces likeli-\nhood scores for each potential parent-child\npair, creating a graph of parent-child rela-\ntion scores. The tree reconciliation module\ntreats the task as a graph optimization prob-\nlem and outputs the maximum spanning tree\nof this graph. We train our model on subtrees\nsampled from W ORD NET, and test on non-\noverlapping W ORD NET subtrees. We show\nthat incorporating web-retrieved glosses can\nfurther improve performance. On the task of\nconstructing subtrees of English W ORD NET,\nthe model achieves 66.7 ancestor F1, a 20.0%\nrelative increase over the previous best pub-\nlished result on this task. In addition, we\nconvert the original English dataset into nine\nother languages using O PEN MULTILINGUAL\nWORD NET and extend our results across these\nlanguages.\n1 Introduction\nA variety of NLP tasks use taxonomic information,\nincluding question answering (Miller, 1998) and\ninformation retrieval (Yang and Wu, 2012). Tax-\nonomies are also used as a resource for building\nknowledge and systematicity into neural models\n(Peters et al., 2019; Geiger et al., 2020; Talmor\net al., 2020). NLP systems often retrieve taxonomic\ninformation from lexical databases such as WORD -\nNET (Miller, 1998), which consists of taxonomies\nthat contain semantic relations across many do-\nmains. While manually curated taxonomies pro-\nvide useful information, they are incomplete and\nexpensive to maintain (Hovy et al., 2009).\n* indicates equal contribution\nTraditionally, methods for automatic taxonomy\nconstruction have relied on statistics of web-scale\ncorpora. These models generally apply lexico-\nsyntactic patterns (Hearst, 1992) to large corpora,\nand use corpus statistics to construct taxonomic\ntrees (e.g., Snow et al., 2005; Kozareva and Hovy,\n2010; Bansal et al., 2014; Mao et al., 2018; Shang\net al., 2020).\nFigure 1: An example subtree from the WORD NET hi-\nerarchy.\nIn this work, we propose an approach that\nconstructs taxonomic trees using pretrained lan-\nguage models (CTP ). Our results show that direct\naccess to corpus statistics at test time is not neces-\nsary. Indeed, the re-representation latent in large-\nscale models of such corpora can be beneﬁcial in\nconstructing taxonomies. We focus on the task pro-\nposed by Bansal et al. (2014), where the task is to\norganize a set of input terms into a taxonomic tree.\nWe convert this dataset into nine other languages\nusing synset alignments collected in OPEN MULTI -\nLINGUAL WORDNET and evaluate our approach in\nthese languages.\nCTP ﬁrst ﬁnetunes pretrained language mod-\nels to predict the likelihood of pairwise parent-\nchild relations, producing a graph of parenthood\nscores. Then it reconciles these predictions with\na maximum spanning tree algorithm, creating a\ntree-structured taxonomy. We further test CTP in a\nsetting where models have access to web-retrieved\nglosses. We reorder the glosses and ﬁnetune the\n4688\nmodel on the reordered glosses in the parenthood\nprediction module.\nWe compare model performance on subtrees\nacross semantic categories and subtree depth, pro-\nvide examples of taxonomic ambiguities, describe\nconditions for which retrieved glosses produce\ngreater increases in tree construction F1 score, and\nevaluate generalization to large taxonomic trees\n(Bordea et al., 2016a). These analyses suggest spe-\nciﬁc avenues of future improvements to automatic\ntaxonomy construction.\nEven without glosses, CTP achieves a 7.9 point\nabsolute improvement in F1 score on the task of\nconstructing WORD NET subtrees, compared to\nprevious work. When given access to the glosses,\nCTP obtains an additional 3.2 point absolute im-\nprovement in F1 score. Overall, the best model\nachieves a 11.1 point absolute increase (a 20.0%\nrelative increase) in F1 score over the previous best\npublished results on this task.\nOur paper is structured as follows. In Section\n2 we describe CTP , our approach for taxonomy\nconstruction. In Section 3 we describe the exper-\nimental setup, and in Section 4 we present the re-\nsults for various languages, pretrained models, and\nglosses. In Section 5 we analyze our approach and\nsuggest speciﬁc avenues for future improvement.\nWe discuss related work and conclude in Sections\n6 and 7.\n2 Constructing Taxonomies from\nPretrained Models\n2.1 Taxonomy Construction\nWe deﬁne taxonomy construction as the task of\ncreating a tree-structured hierarchy T = (V, E),\nwhere V is a set of terms and E is a set of directed\nedges representing hypernym relations. In this task,\nthe model receives a set of terms V , where each\nterm can be a single word or a short phrase, and it\nmust construct the tree T given these terms. CTP\nperforms taxonomy construction in two steps: par-\nenthood prediction (Section 2.2) followed by graph\nreconciliation (Section 2.3).\nWe provide a schematic description of CTP in\nFigure 2 and provide details in the remainder of\nthis section.\n2.2 Parenthood Prediction\nWe use pretrained models (e.g., BERT) to predict\nthe edge indicators I[parent(vi, vj)], which denote\nwhether vi is a parent of vj, for all pairs (vi, vj) in\nthe set of terms V = {v1, ..., vn}for each subtree\nT.\nTo generate training data from a tree T with n\nnodes, we create a positive training example for\neach of the n −1 parenthood edges and a negative\ntraining example for each of the n(n−1)\n2 −(n −1)\npairs of nodes that are not connected by a parent-\nhood edge.\nWe construct an input for each example using\nthe template vi is a vj, e.g., “A dog is a mam-\nmal.\" Different templates (e.g., [TERM_A] is\nan example of [TERM_B] or [TERM_A]\nis a type of [TERM_B]) did not substan-\ntially affect model performance in initial experi-\nments, so we use a single template. The inputs and\noutputs are modeled in the standard format (Devlin\net al., 2019).\nWe ﬁne-tune pretrained models to predict\nI[parent(vi, vj)], which indicates whether vi is the\nparent of vj, for each pair of terms using a sentence-\nlevel classiﬁcation task on the input sequence.\n2.3 Tree Reconciliation\nWe then reconcile the parenthood graph into a valid\ntree-structured taxonomy. We apply the Chu-Liu-\nEdmonds algorithm to the graph of pairwise par-\nenthood predictions. This algorithm ﬁnds the max-\nimum weight spanning arborescence of a directed\ngraph. It is the analog of MST for directed graphs,\nand ﬁnds the highest scoring arborescence inO(n2)\ntime (Chu, 1965).\n2.4 Web-Retrieved Glosses\nWe perform experiments in two settings: with and\nwithout web-retrieved glosses. In the setting with-\nout glosses, the model performs taxonomy con-\nstruction using only the set of terms V . In the\nsetting with glosses, the model is provided with\nglosses retrieved from the web. For settings in\nwhich the model receives glosses, we retrieve a list\nof glosses d1\nv, ..., dn\nv for each term v ∈V .1\nMany of the terms in our dataset are polysemous,\nand the glosses contain multiple senses of the word.\nFor example, the term dish appears in the subtree\nwe show in Figure 1. The glosses for dish include\n(1) (telecommunications) A type of antenna with\n1We scrape glosses from wiktionary.com, merriam-\nwebster.com, and wikipedia.org. For wikitionary.com and\nmerriam-webster.com we retrieve a list of glosses from each\nsite. For wikipedia.org we treat the ﬁrst paragraph of the page\nassociated with the term as a single gloss. The glosses were\nscraped in August 2020.\n4689\nFigure 2: A schematic depiction of CTP . We start with a set of terms (A). We ﬁne-tune a pretrained language model\nto predict pairwise parenthood relations between pairs of terms (B), creating a graph of parenthood predictions (C)\n(Section 2.2). We then reconcile the edges of this graph into a taxonomic tree (E) (Section 2.3). Optionally, we\nprovide the model ranked web-retrieved glosses (Section 2.4). We re-order the glosses based on relevance to the\ncurrent subtree (Z).\na similar shape to a plate or bowl, (2) (metonymi-\ncally) A speciﬁc type of prepared food , and (3)\n(mining) A trough in which ore is measured.\nWe reorder the glosses based on their relevance\nto the current subtree. We deﬁne relevance of a\ngiven context di\nv to subtree T as the cosine sim-\nilarity between the average of the GloVe embed-\ndings (Pennington et al., 2014) of the words in di\nv\n(with stopwords removed), to the average of the\nGloVe embeddings of all terms v1, ..., vn in the\nsubtree. This produces a reordered list of glosses\nd(1)\nv , ..., d(n)\nv .\nWe then use the input sequence containing the\nreordered glosses “[CLS] vi d(1)\nvi , ..., d(n)\nvi . [SEP]\nvj d(1)\nvj , ..., d(n)\nvj ” to ﬁne-tune the pretrained models\non pairs of terms (vi, vj).\n3 Experiments\nIn this section we describe the details of our\ndatasets (Section 3.1), and describe our evaluation\nmetrics (Section 3.2). We ran our experiments on\na cluster with 10 Quadro RTX 6000 GPUs. Each\ntraining runs ﬁnishes within one day on a single\nGPU.\n3.1 Datasets\nWe evaluate CTP using the dataset of medium-\nsized WORD NET subtrees created by Bansal et al.\n(2014). This dataset consists of bottomed-out full\nsubtrees of height 3 (this corresponds to trees con-\ntaining 4 nodes in the longest path from the root to\nany leaf) that contain between 10 and 50 terms.\nThis dataset comprises 761 English trees, with\n533/114/114 train/dev/test trees respectively.\n3.1.1 Multilingual W ORD NET\nWORD NET was originally constructed in English,\nand has since been extended to many other lan-\nguages such as Finnish (Magnini et al., 1994),\nItalian (Lindén and Niemi, 2014), and Chinese\n(Wang and Bond, 2013). Researchers have pro-\nvided alignments from synsets in English WORD -\nNET to terms in other languages, using a mix of\nautomatic and manual methods (e.g., Magnini et al.,\n1994; Lindén and Niemi, 2014). These multilingual\nwordnets are collected in the OPEN MULTILIN -\nGUAL WORD NET project (Bond and Paik, 2012).\nThe coverage of synset alignments varies widely.\nFor instance, the alignment of ALBANET (Alba-\nnian) to English WORD NET covers 3.6% of the\nsynsets in the Bansal et al. (2014) dataset, while\nthe FINN WORD NET (Finnish) alignment covers\n99.6% of the synsets in the dataset.\nWe convert the original English dataset to nine\nother languages using the synset alignments. (We\ncreate datasets for Catalan (Agirre et al., 2011),\nChinese (Wang and Bond, 2013), Finnish (Lindén\nand Niemi, 2014), French (Sagot, 2008), Italian\n(Magnini et al., 1994), Dutch (Postma et al., 2016),\n4690\nPolish (Piasecki et al., 2009), Portuguese (de Paiva\nand Rademaker, 2012), and Spanish (Agirre et al.,\n2011)).\nSince these wordnets do not include alignments\nto all of the synsets in the English dataset, we con-\nvert the English dataset to each target language us-\ning alignments speciﬁed in WORD NET as follows.\nWe ﬁrst exclude all subtrees whose roots are not in-\ncluded in the alignment between the WORD NET of\nthe target language and English WORD NET. For\neach remaining subtree, we remove any node that\nis not included in the alignment. Then we remove\nall remaining nodes that are no longer connected\nto the root of the corresponding subtrees. We de-\nscribe the resulting dataset statistics in Table 8 in\nthe Appendix.\n3.2 Evaluation Metrics\nAs with previous work (Bansal et al., 2014; Mao\net al., 2018), we report the ancestor F1 score 2P R\nP+R ,\nwhere\nP = |IS_APREDICTED ∩IS_AGOLD |\n|IS_APREDICTED |\nR = |IS_APREDICTED ∩IS_AGOLD |\n|IS_AGOLD |\nIS_APREDICTED and IS_AGOLD denote the set\nof predicted and gold ancestor relations, respec-\ntively. We report the mean precision ( P), recall\n(R), and F1 score, averaged across the subtrees in\nthe test set.\n3.3 Models\nIn our experiments, we use pretrained models from\nthe Huggingface library (Wolf et al., 2019). For\nthe English dataset we experiment with BERT,\nBERT-Large, and ROBERTA-Large in the parent-\nhood prediction module. We experiment with mul-\ntilingual BERT and language-speciﬁc pretrained\nmodels (detailed in Section 9 in the Appendix).\nWe ﬁnetuned each model using three learning\nrates {1e-5, 1e-6, 1e-7}. For each model, we ran\nthree trials using the learning rate that achieved\nthe highest dev F1 score. In Section 4, we re-\nport the average scores over three trials. We in-\nclude full results in Tables 13 and 15 in the Ap-\npendix. The code and datasets are available at\nhttps://github.com/cchen23/ctp.\n4 Results\n4.1 Main Results\nOur approach, CTP , outperforms existing state-\nof-the-art models on the WORD NET subtree con-\nstruction task. In Table 1 we provide a comparison\nof our results to previous work. Even without re-\ntrieved glosses, CTP with ROBERTA-LARGE in\nthe parenthood prediction module achieves higher\nF1 than previously published work. CTP achieves\nadditional improvements when provided with the\nweb-retrieved glosses described in Section 2.4.\nWe compare different pretrained models for the\nparenthood prediction module, and provide these\ncomparisons in Section 4.3.\nP R F1\nBansal et al. (2014) 48.0 55.2 51.4\nMao et al. (2018) 52.9 58.6 55.6\nCTP (no glosses) 67.3 62.0 63.5\nCTP (web glosses) 69.3 66.2 66.7\nTable 1: English Results, Comparison to Previous\nWork. Our approach outperforms previous approaches\non reconstructing W ORD NET subtrees, even when the\nmodel is not given web-retrieved glosses.\n4.2 Web-Retrieved Glosses\nIn Table 2 we show the improvement in taxonomy\nconstruction with two types of glosses – glosses re-\ntrieved from the web (as described in Section 2.4),\nand those obtained directly from WORD NET. We\nconsider using the glosses from WORD NET as an\noracle setting since these glosses are directly gener-\nated from the gold taxonomies. Thus, we focus on\nthe web-retrieved glosses as the main setting. Mod-\nels produce additional improvements when given\nWORD NET glosses. These improvements suggest\nthat reducing the noise from web-retrieved glosses\ncould improve automated taxonomy construction.\n4.3 Comparison of Pretrained Models\nFor both settings (with and without web-retrieved\nglosses), CTP attains the highest F1 score when\nROBERTA-Large is used in the parenthood predic-\ntion step. As we show in Table 3, the average F1\nscore improves with both increased model size and\nwith switching from BERT to R OBERTA.\n4691\nP R F1\nCTP 67.3 62.0 63.5\n+ web glosses 69.3 66.2 66.7\n+ oracle glosses 84.0 83.8 83.2\nTable 2: English Results, Gloss Comparison on Test\nSet. Adding web glosses improves performance over\nonly using input terms. Models achieve additional im-\nprovements in subtree reconstruction when given ora-\ncle glosses from W ORD NET, showing possibilities for\nimprovement in retrieving web glosses.\nP R F1\nCTP (BERT-Base) 57.9 51.8 53.4\nCTP (BERT-Large) 65.5 59.8 61.4\nCTP (R OBERTA-Large) 67.3 62.0 63.5\nTable 3: English Results, Comparison of Pretrained\nModels on Test Set. Larger models perform better and\nROBERTA outperforms BERT.\n4.4 Aligned Wordnets\nWe extend our results to the nine non-English align-\nments to the Bansal et al. (2014) dataset that we\ncreated. In Table 4 we compare our best model in\neach language to a random baseline. We detail the\nrandom baseline in Section 9 in the Appendix and\nprovide results from all tested models in Section\n17 in the Appendix.\nCTP ’sF1 score non-English languages is sub-\nstantially worse than its F1 score on English trees.\nLower F1 scores in non-English languages are\nlikely due to multiple factors. First, English pre-\ntrained language models generally perform better\nthan models in other languages because of the ad-\nditional resources devoted to the development of\nEnglish models. (See e.g., Bender, 2011; Mielke,\n2016; Joshi et al., 2020). Second, OPEN MULTI -\nLINGUAL WORDNET aligns wordnets to English\nWORD NET, but the subtrees contained in English\nWORD NET might not be the natural taxonomy in\nother languages. However, we note that scores\nacross languages are not directly comparable as\ndataset size and coverage vary across languages (as\nwe show in Table 8).\nThese results highlight the importance of evalu-\nating on non-English languages, and the difference\nin available lexical resources between languages.\nFurthermore, they provide strong baselines for fu-\nModel P R F1\nca Random Baseline 20.0 31.3 23.6\nCTP ( MBERT) 38.7 39.7 38.0\nzh Random Baseline 25.8 35.9 29.0\nCTP (C HINESE BERT) 62.2 57.3 58.7\nen Random Baseline 8.9 22.2 12.4\nCTP (R OBERTA-Large) 67.3 62.0 63.5\nﬁ Random Baseline 10.1 22.5 13.5\nCTP (F INBERT) 47.9 42.6 43.8\nfr Random Baseline 22.1 34.4 25.9\nCTP (F RENCH BERT) 51.3 49.1 49.1\nit Random Baseline 28.9 39.4 32.3\nCTP (I TALIAN BERT) 48.3 45.5 46.1\nnl Random Baseline 26.8 38.4 30.6\nCTP (BERT JE) 44.6 44.8 43.7\npl Random Baseline 23.4 33.6 26.8\nCTP (P OLBERT ) 51.9 49.7 49.5\npt Random Baseline 26.1 37.6 29.8\nCTP (BERT IMBAU ) 59.3 57.1 56.9\nes Random Baseline 27.0 37.2 30.5\nCTP (BETO) 53.1 51.7 51.7\nTable 4: Multilingual WORD NET Test Results. We ex-\ntend our model to datasets in nine other languages, and\nevaluate our approach on these datasets. We use ISO\n639-1 acronyms to indicate languages.\nture work in constructing wordnets in different lan-\nguages.\n5 Analysis\nIn this section we analyze the models both quan-\ntitatively and qualitatively. Unless stated other-\nwise, we analyze our model on the dev set and\nuse ROBERTA-Large in the parenthood prediction\nstep.\n5.1 Models Predict Flatter Trees\nIn many error cases,CTP predicts a tree with edges\nthat connect terms to their non-parent ancestors,\nskipping the direct parents. We show an example\nof this error in Figure 3. In this fragment (taken\nfrom one of the subtrees in the dev set), the model\npredicts a tree in which botfly and horsefly\nare direct children of fly, bypassing the correct\nparent gadfly. On the dev set, 38.8% of incorrect\nparenthood edges were cases of this type of error.\n4692\nFigure 3: A fragment of a subtree from the WORD -\nNET hierarchy. Orange indicates incorrectly predicted\nedges and blue indicates missed edges.\nMissing edges result in predicted trees that are\ngenerally ﬂatter than the gold tree. While all the\ngold trees have a height of 3 (4 nodes in the longest\npath from the root to any leaf), the predicted dev\ntrees have a mean height of 2.61. Our approach\nscores the edges independently, without consider-\ning the structure of the tree beyond local parent-\nhood edges. One potential way to address the bias\ntowards ﬂat trees is to also model the global struc-\nture of the tree (e.g., ancestor and sibling relations).\n5.2 Model Struggle Near Leaf Nodes\nd = 1 d = 2 d = 3\nl = 1 81.2 52.3 39.7\nl = 2 74.4 48.9\nl = 3 66.0\nTable 5: Ancestor Edge Recall, Categorized by Descen-\ndant Node Depth d and Parent Edge Length l. Ances-\ntor edge prediction recall decreases with deeper descen-\ndant nodes and closer ancestor-descendant relations.\nCTP generally makes more errors in predicting\nedges involving nodes that are farther from the root\nof each subtree. In Table 5 we show the recall\nof ancestor edges, categorized by the number of\nparent edges d between the subtree root and the\ndescendant of each edge, and the number of parent\nedges l between the ancestor and descendant of\neach edge. The model has lower recall for edges\ninvolving descendants that are farther from the root\n(higher d). In permutation tests of the correlation\nbetween edge recall and d conditioned on l, 0 out\nof 100,000 permutations yielded a correlation at\nleast as extreme as the observed correlation.\n5.3 Subtrees Higher Up in W ORD NET are\nHarder, and Physical Entities are Easier\nthan Abstractions\nSubtree performance also corresponds to the depth\nof the subtree in the entire WORD NET hierarchy.\nThe F1 score is positively correlated with the depth\nof the subtree in the fullWORD NET hierarchy, with\na correlation of 0.27 (signiﬁcant at p=0.004 using\na permutation test with 100,000 permutations).\nThe subtrees included in this task span many\ndifferent domains, and can be broadly catego-\nrized into subtrees representing concrete enti-\nties (such as telephone) and those represent-\ning abstractions (such as sympathy). WORD -\nNET provides this categorization using the top-\nlevel synsets physical_entity.n.01 and\nabstraction.n.06. These categories are di-\nrect children of the root of the full WORD NET hi-\nerarchy ( entity.n.01), and split almost all\nWORD NET terms into two subsets. The model\nproduces a mean F1 score of 60.5 on subtrees\nin the abstraction subsection of WORD NET,\nand a mean F1 score of 68.9 on subtrees in the\nphysical_entity subsection. A one-sided\nMann-Whitney rank test shows that the model per-\nforms systematically worse on abstraction\nsubtrees (compared to physical entity sub-\ntrees) (p=0.01).\n5.4 Pretraining Corpus Covers Most Terms\nFigure 4: Frequency of terms in theWORD NET dataset\nin the pretraining corpus. Over 97% of terms in the\nBansal et al. (2014) dataset occur at least once in the\npretraining corpus. Over 80% of terms occur less than\n50k times.\nWith models pretrained on large web corpora,\nthe distinction between the settings with and with-\nout access to the web at test time is less clear, since\nlarge pretrained models can be viewed as a com-\n4693\npressed version of the web. To quantify the extent\nthe evaluation setting measures model capability\nto generalize to taxonomies consisting of unseen\nwords, we count the number of times each term in\nthe WORD NET dataset occurs in the pretraining\ncorpus. We note that the WORD NET glosses do not\ndirectly appear in the pretraining corpus. In Figure\n4 we show the distribution of the frequency with\nwhich the terms in the Bansal et al. (2014) dataset\noccur in the BERT pretraining corpus.2 We ﬁnd\nthat over 97% of the terms occur at least once in\nthe pretraining corpus. However, the majority of\nthe terms are not very common words, with over\n80% of terms occurring less than 50k times. While\nthis shows that the current setting does not measure\nmodel ability to generalize to completely unseen\nterms, we ﬁnd that the model does not perform\nsubstantially worse on edges that contain terms\nthat do not appear in the pretraining corpus. Fur-\nthermore, the model is able do well on rare terms.\nFuture work can investigate model ability to con-\nstruct taxonomies from terms that are not covered\nin pretraining corpora.\n5.5 W ORD NET Contains Ambiguous\nSubtrees\nFigure 5: A fragment of a subtree from the WORD -\nNET hierarchy. Orange indicates incorrectly predicted\nedges and blue indicates edges that were missed.\nSome trees in the gold WORD NET hier-\narchy contain ambiguous edges. Figure 5\nshows one example. In this subtree, the\nmodel predicts arteriography as a sibling of\narthrography rather than as its child. The\ndeﬁnitions of these two terms suggest why the\nmodel may have considered these terms as siblings:\narteriograms produce images of arteries while\n2Since the original pretraining corpus is not available, we\nfollow Devlin et al. (2019) and recreate the dataset by crawling\nhttp://smashwords.com and Wikipedia.\narthrograms produce images of the inside of\njoints. In Figure 6 we show a second example\nof an ambiguous tree. The model predicts good\nfaith as a child of sincerity rather than as a\nchild of honesty, but the correct hypernymy re-\nlation between these terms is unclear to the authors,\neven after referencing multiple dictionaries.\nThese examples point to the potential of aug-\nmenting or improving the relations listed inWORD -\nNET using semi-automatic methods.\n5.6 Web-Retrieved Glosses Are Beneﬁcial\nWhen They Contain Lexical Overlap\nWe compare the predictions of ROBERTA-Large,\nwith and without web glosses, to understand what\nkind of glosses help. We split the parenthood edges\nin the gold trees into two groups based on the\nglosses: (1) lexical overlap (the parent term appears\nin the child gloss and/or the child term appears in\nthe parent gloss) and (2) no lexical overlap (neither\nthe parent term nor the child term appears in the\nother term’s gloss). We ﬁnd that for edges in the\n“lexical overlap\" group, glosses increase the recall\nof the gold edges from 60.9 to 67.7. For edges in\nthe “no lexical overlap\" group, retrieval decreases\nthe recall (edge recall changes from 32.1 to 27.3).\n5.7 Pretraining and Tree Reconciliation Both\nContribute to Taxonomy Construction\nWe performed an ablation study in which we ab-\nlated either the pretrained language models for the\nparenthood prediction step or we ablated the tree\nreconciliation step. We ablated the pretrained lan-\nguage models in two ways. First, we used a one-\nlayer LSTM on top of GloVe vectors instead of a\npretrained language model as the input to the ﬁne-\ntuning step, and then performed tree reconciliation\nas before. Second, we used a randomly initialized\nROBERTA-Large model in place of a pretrained\nnetwork, and then performed tree reconciliation\nas before. We ablated the tree reconciliation step\nby substituting the graph-based reconciliation step\nwith a simpler threshold step, where we output\na parenthood-relation between all pairs of words\nwith softmax score greater than 0.5. We used the\nparenthood prediction scores from the ﬁne-tuned\nROBERTA-Large model, and substituted tree rec-\nonciliation with thresholding.\nIn Table 6, we show the results of our ablation\nexperiments. These results show that both steps\n(using pretrained language models for parenthood-\nprediction and performing tree reconciliation) are\n4694\nFigure 6: A fragment of a subtree from the WORD NET hierarchy. Orange indicates incorrectly predicted edges\nand blue indicates edges that were missed.\nP R F1\nROBERTA-Large 71.2 65.9 67.4\nw/o tree reconciliation 70.8 45.8 51.1\nROBERTA-Random-Init 32.6 28.2 29.3\nLSTM GloVe 32.5 23.6 26.6\nTable 6: Ablation study. Pretraining and tree reconcili-\nation both contribute to taxonomy construction.\nimportant for taxonomy construction. Moreover,\nthese results show that the incorporation of a new\ninformation source (knowledge learned by pre-\ntrained language models) produces the majority\nof the performance gains.\n5.8 Models Struggle to Generalize to Large\nTaxonomies\nTo test generalization to large subtrees, we tested\nour models on the English environment and science\ntaxonomies from SemEval-2016 Task 13 (Bordea\net al., 2016a). Each of these taxonomies consists\nof a single large taxonomic tree with between 125\nand 452 terms. Following Mao et al. (2018) and\nShang et al. (2020), we used the medium-sized\ntrees from Bansal et al. (2014) to train our mod-\nels. During training, we excluded all medium-sized\ntrees from the Bansal et al. (2014) dataset that over-\nlapped with the terms in the SemEval-2016 Task\n13 environment and science taxonomies.\nIn Table 7 we show the performance of the\nROBERTA-Large CTP model. We show the Edge-\nF1 score rather than the Ancestor-F1 score in order\nto compare to previous work. Although the CTP\nmodel outperforms previous work in constructing\nmedium-sized taxonomies, this model is limited in\nits ability to generalize to large taxonomies. Future\nwork can incorporate modeling of the global tree\nstructure into CTP.\n6 Related Work\nTaxonomy induction has been studied extensively,\nwith both pattern-based and distributional ap-\nproaches. Typically, taxonomy induction involves\nhypernym detection, the task of extracting candi-\ndate terms from corpora, and hypernym organiza-\ntion, the task of organizing the terms into a hierar-\nchy.\nWhile we focus on hypernym organization, many\nsystems have studied the related task of hypernym\ndetection. Traditionally, systems have used pattern-\nbased features such as Hearst patterns to infer hy-\npernym relations from large corpora (e.g. Hearst,\n1992; Snow et al., 2005; Kozareva and Hovy, 2010).\nFor example, Snow et al. (2005) propose a sys-\ntem that extracts pattern-based features from a cor-\npus to predict hypernymy relations between terms.\nKozareva and Hovy (2010) propose a system that\nsimilarly uses pattern-based features to predict hy-\npernymy relations, in addition to harvesting rele-\nvant terms and using a graph-based longest-path\napproach to construct a legal taxonomic tree.\nLater work suggests that, for hypernymy detec-\ntion tasks, pattern-based approaches outperform\nthose based on distributional models (Roller et al.,\n2018). Subsequent work pointed out the sparsity\nthat exists in pattern-based features derived from\ncorpora, and showed that combining distributional\nand pattern-based approaches can improve hyper-\nnymy detection by addressing this problem (Yu\net al., 2020).\nIn this work we consider the task of organizing\na set of terms into a medium-sized taxonomic tree.\nBansal et al. (2014) treat this as a structured learn-\ning problem and use belief propagation to incorpo-\n4695\nDataset Model P R F1\nScience (Averaged)\nCTP 29.4 28.8 29.1\nMao et al. (2018) 37.9 37.9 37.9\nShang et al. (2020) 84.0 30.0 44.0\nEnvironment (Eurovoc)\nCTP 23.1 23.0 23.0\nMao et al. (2018) 32.3 32.3 32.3\nShang et al. (2020) 89.0 24.0 37.0\nTable 7: Generalization to large taxonomic trees. Models trained on medium-sized taxonomies generalize poorly\nto large taxonomies. Future work can improve the usage of global tree structure with CTP.\nrate siblinghood information. Mao et al. (2018) pro-\npose a reinforcement learning based approach that\ncombines the stages of hypernym detection and hy-\npernym organization. In addition to the task of con-\nstructing medium-sized WORD NET subtrees, they\nshow that their approach can leverage global struc-\nture to construct much larger taxonomies from the\nSemEval-2016 Task 13 benchmark dataset, which\ncontain hundreds of terms (Bordea et al., 2016b).\nShang et al. (2020) apply graph neural networks\nand show that they improve performance in con-\nstructing large taxonomies in the SemEval-2016\nTask 13 dataset.\nAnother relevant line of work involves extracting\nstructured declarative knowledge from pretrained\nlanguage models. For instance, Bouraoui et al.\n(2019) showed that a wide range of relations can\nbe extracted from pretrained language models such\nas BERT. Our work differs in that we consider\ntree structures and incorporate web glosses. Bosse-\nlut et al. (2019) use pretrained models to generate\nexplicit open-text descriptions of commonsense\nknowledge. Other work has focused on extracting\nknowledge of relations between entities (Petroni\net al., 2019; Jiang et al., 2020). Blevins and Zettle-\nmoyer (2020) use a similar approach to ours for\nword sense disambiguation, and encode glosses\nwith pretrained models.\n7 Discussion\nOur experiments show that pretrained language\nmodels can be used to construct taxonomic trees.\nImportantly, the knowledge encoded in these pre-\ntrained language models can be used to construct\ntaxonomies without additional web-based informa-\ntion. This approach produces subtrees with higher\nmean F1 scores than previous approaches, which\nused information from web queries.\nWhen given web-retrieved glosses, pretrained\nlanguage models can produce improved taxonomic\ntrees. The gain from accessing web glosses shows\nthat incorporating both implicit knowledge of input\nterms and explicit textual descriptions of knowl-\nedge is a promising way to extract relational knowl-\nedge from pretrained models. Error analyses sug-\ngest speciﬁc avenues of future work, such as im-\nproving predictions for subtrees corresponding to\nabstractions, or explicitly modeling the global struc-\nture of the subtrees.\nExperiments on aligned multilingual WORD -\nNET datasets emphasize that more work is needed\nin investigating the differences between taxonomic\nrelations in different languages, and in improving\npretrained language models in non-English lan-\nguages. Our results provide strong baselines for\nfuture work on constructing taxonomies for differ-\nent languages.\n8 Ethical Considerations\nWhile taxonomies (e.g., WORD NET) are often used\nas ground-truth data, they have been shown to\ncontain offensive and discriminatory content (e.g.,\nBroughton, 2019). Automatic systems created by\npretrained language models can reﬂect and exacer-\nbate the biases contained by their training corpora.\nMore work is needed to detect and combat biases\nthat arise when constructing and evaluating tax-\nonomies.\nFurthermore, we used previously constructed\nalignments to extend our results to wordnets in\nmultiple languages. While considering English\nWORD NET as the basis for the alignments allows\nfor convenient comparisons between languages and\nis the standard method for aligning wordnets across\nlanguages, continued use of these alignments to\nevaluate taxonomy construction imparts undue bias\ntowards conceptual relations found in English.\n4696\n9 Acknowledgements\nWe thank the members of the Berkeley NLP group\nand the anonymous reviewers for their insightful\nfeedback. CC and KL are supported by National\nScience Foundation Graduate Research Fellow-\nships. This research has been supported by DARPA\nunder agreement HR00112020054. The content\ndoes not necessarily reﬂect the position or the pol-\nicy of the government, and no ofﬁcial endorsement\nshould be inferred.\nReferences\nA. Agirre, Egoitz Laparra, and German Rigau. 2011.\nMultilingual central repository version 3 . 0 : upgrad-\ning a very large lexical knowledge base.\nMohit Bansal, David Burkett, Gerard De Melo, and\nDan Klein. 2014. Structured learning for taxonomy\ninduction with belief propagation. In Proceedings\nof the 52nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1041–1051.\nEmily M. Bender. 2011. On achieving and evaluating\nlanguage-independence in nlp. Linguistic Issues in\nLanguage Technology, 6.\nTerra Blevins and Luke Zettlemoyer. 2020. Moving\ndown the long tail of word sense disambiguation\nwith gloss-informed biencoders. In ACL.\nFrancis Bond and Kyonghee Paik. 2012. A survey of\nwordnets and their licenses.\nGeorgeta Bordea, Els Lefever, and Paul Buitelaar.\n2016a. SemEval-2016 task 13: Taxonomy extrac-\ntion evaluation (TExEval-2). In Proceedings of the\n10th International Workshop on Semantic Evalua-\ntion (SemEval-2016), pages 1081–1091, San Diego,\nCalifornia. Association for Computational Linguis-\ntics.\nGeorgeta Bordea, Els Lefever, and Paul Buitelaar.\n2016b. Semeval-2016 task 13: Taxonomy extrac-\ntion evaluation (texeval-2). In Proceedings of the\n10th International Workshop on Semantic Evalua-\ntion. Association for Computational Linguistics.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, A. Çelikyilmaz, and Yejin Choi.\n2019. Comet: Commonsense transformers for au-\ntomatic knowledge graph construction. In ACL.\nZied Bouraoui, Jose Camacho-Collados, and Steven\nSchockaert. 2019. Inducing relational knowledge\nfrom bert. arXiv preprint arXiv:1911.12753.\nVanda Broughton. 2019. The respective roles of intel-\nlectual creativity and automation in representing di-\nversity: human and machine generated bias.\nJosé Cañete, Gabriel Chaperon, Rodrigo Fuentes, Jou-\nHui Ho, Hojin Kang, and Jorge Pérez. 2020. Span-\nish pre-trained bert model and evaluation data. In\nPML4DC at ICLR 2020.\nYoeng-Jin Chu. 1965. On the shortest arborescence of\na directed graph. Scientia Sinica, 14:1396–1400.\nJ. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In\nNAACL-HLT.\nA. Geiger, Kyle Richardson, and Christopher Potts.\n2020. Neural natural language inference models par-\ntially embed theories of lexical entailment and nega-\ntion. In Proceedings of BlackBoxNLP 2020.\nMarti A Hearst. 1992. Automatic acquisition of hy-\nponyms from large text corpora. In Coling 1992 vol-\nume 2: The 15th international conference on compu-\ntational linguistics.\nE. Hovy, Zornitsa Kozareva, and E. Riloff. 2009. To-\nward completeness in concept extraction and classi-\nﬁcation. In EMNLP.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the nlp\nworld. arXiv preprint arXiv:2004.09095.\nZornitsa Kozareva and Eduard Hovy. 2010. A\nsemi-supervised method to learn and construct tax-\nonomies using the web. In Proceedings of the 2010\nconference on empirical methods in natural lan-\nguage processing, pages 1110–1118.\nKrister Lindén and Jyrki Niemi. 2014. Is it possible to\ncreate a very large wordnet in 100 days? an evalua-\ntion. Language Resources and Evaluation, 48:191–\n201.\nB. Magnini, C. Strapparava, F. Ciravegna, and E. Pi-\nanta. 1994. A project for the construction of an\nitalian lexical knowledge base in the framework of\nwordnet.\nYuning Mao, Xiang Ren, Jiaming Shen, Xiaotao Gu,\nand Jiawei Han. 2018. End-to-end reinforcement\nlearning for automatic taxonomy induction. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 2462–2472, Melbourne, Australia.\nAssociation for Computational Linguistics.\nSabrina J. Mielke. 2016. Language diversity in acl\n2004 - 2016.\nGeorge A Miller. 1998. WordNet: An electronic lexical\ndatabase. MIT press.\n4697\nValeria de Paiva and Alexandre Rademaker. 2012. Re-\nvisiting a brazilian wordnet. Scopus.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language process-\ning (EMNLP), pages 1532–1543.\nMatthew E Peters, Mark Neumann, Robert Logan, Roy\nSchwartz, Vidur Joshi, Sameer Singh, and Noah A\nSmith. 2019. Knowledge enhanced contextual word\nrepresentations. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 43–54.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473.\nM. Piasecki, Stanisław Szpakowicz, and Bartosz Broda.\n2009. A wordnet from the ground up.\nMarten Postma, Emiel van Miltenburg, Roxane Segers,\nAnneleen Schoen, and Piek V ossen. 2016. Open\nDutch WordNet. In Proceedings of the Eight Global\nWordnet Conference, Bucharest, Romania.\nStephen Roller, Douwe Kiela, and Maximilian Nickel.\n2018. Hearst patterns revisited: Automatic hyper-\nnym detection from large text corpora. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 358–363, Melbourne, Australia. Asso-\nciation for Computational Linguistics.\nBenoît Sagot. 2008. Building a free french wordnet\nfrom multilingual resources.\nChao Shang, Sarthak Dash, Md Faisal Mahbub\nChowdhury, Nandana Mihindukulasooriya, and Al-\nﬁo Gliozzo. 2020. Taxonomy construction of un-\nseen domains via graph-based cross-domain knowl-\nedge transfer. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 2198–2208.\nRion Snow, Daniel Jurafsky, and Andrew Y Ng. 2005.\nLearning syntactic patterns for automatic hypernym\ndiscovery. In Advances in neural information pro-\ncessing systems, pages 1297–1304.\nAlon Talmor, Oyvind Tafjord, P. Clark, Y . Goldberg,\nand Jonathan Berant. 2020. Teaching pre-trained\nmodels to systematically reason over implicit knowl-\nedge. ArXiv, abs/2006.06609.\nA. Virtanen, J. Kanerva, Rami Ilo, Jouni Luoma, Juhani\nLuotolahti, T. Salakoski, F. Ginter, and Sampo\nPyysalo. 2019. Multilingual is not enough: Bert for\nﬁnnish. ArXiv, abs/1912.07076.\nWietse de Vries, Andreas van Cranenburgh, Arianna\nBisazza, Tommaso Caselli, Gertjan van Noord, and\nM. Nissim. 2019. Bertje: A dutch bert model.\nArXiv, abs/1912.09582.\nShan Wang and Francis Bond. 2013. Building the\nchinese open wordnet (cow): Starting from core\nsynsets.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, et al. 2019. Huggingface’s transformers: State-\nof-the-art natural language processing. ArXiv, pages\narXiv–1910.\nCheYu Yang and Shih-Jung Wu. 2012. Semantic web\ninformation retrieval based on the wordnet. Interna-\ntional Journal of Digital Content Technology and Its\nApplications, 6:294–302.\nChanglong Yu, Jialong Han, Peifeng Wang, Yangqiu\nSong, Hongming Zhang, Wilfred Ng, and Shuming\nShi. 2020. When hearst is not enough: Improv-\ning hypernymy detection from corpus with distribu-\ntional models. In Proceedings of the 2020 confer-\nence on empirical methods in natural language pro-\ncessing (EMNLP).\n4698\nAppendix\nLanguage-Speciﬁc Pretrained Models\nWe used pretrained models from the following\nsources:\nhttps://github.com/codegram/calbert,\nhttps://github.com/google-research/bert/\nblob/master/multilingual.md (Devlin et al.,\n2019),\nhttp://turkunlp.org/FinBERT/ (Virtanen et al.,\n2019),\nhttps://github.com/dbmdz/berts,\nhttps://github.com/wietsedv/bertje\n(de Vries et al., 2019),\nhttps://huggingface.co/dkleczek/\nbert-base-polish-uncased-v1 ,\nhttps://github.com/neuralmind-ai/\nportuguese-bert,\nhttps://github.com/dccuchile/beto/blob/\nmaster/README.md (Cañete et al., 2020)\nMultilingual WORD NET Dataset Statistics\nTable 8 details the datasets we created by using\nsynset alignments to the English dataset proposed\nin Bansal et al. (2014). The data construction\nmethod is described in Section 3.1.\nNum Average\nTrees Nodes per Tree\nTrain Dev Test Train Dev Test\nca 391 94 90 9.2 9.3 8.7\nzh 216 48 64 10.0 12.4 9.2\nen 533 114 114 19.7 20.3 19.8\nﬁ 532 114 114 17.8 18.8 18.1\nfr 387 82 76 8.7 9.1 8.3\nit 340 85 75 6.3 7.2 6.2\nnl 308 58 64 6.6 6.7 6.3\npl 283 73 72 7.7 8.0 7.4\npt 347 68 77 7.1 8.2 7.2\nes 280 60 60 6.5 6.1 5.8\nTable 8: Dataset Statisics. For each language, we\nshow the number of train, dev, and test subtrees that\nremain after the subsetting procedure described in Sec-\ntion 3.1.1. In addition, we show the mean number of\nnodes per tree in each language. We use ISO 639-1\nlanguage acronyms.\nAblation Results\nTable 9 shows the results for the learning rate trials\nfor the ablation experiment.\n1e-5 1e-6 1e-7\nROBERTA-Large 59.5 67.3 60.7\nw/o tree reconciliation 38.6 51.2 18.2\nROBERTA-Random-Init 17.4 26.4 27.0\nTable 9: Dev F1 Scores for Different Learning Rates,\nAblation Experiments .\nTable 10 shows the results for the test trials for\nthe ablation experiment.\nRun 0 Run 1 Run 2\nROBERTA-Large 67.1 67.3 67.7\nw/o tree reconciliation 51.2 51.4 50.6\nROBERTA-Random-Init 27.0 29.9 31.1\nLSTM GloVe 24.6 27.7 27.6\nTable 10: Dev F1 Scores for Three Trials, Ablation Ex-\nperiments .\nSemEval Results\nDataset Run 0 Run 1 Run 2\nScience (Combined) 28.6 31.7 25.1\nScience (Eurovoc) 26.6 37.1 31.5\nScience (WordNet) 26.5 28.8 25.8\nEnvironment (Eurovoc) 23.4 21.5 24.2\nTable 11: Test F1 Scores for Three Trials, Semeval.We\nshow the Edge-F1 score rather than the Ancestor-F1\nscore in order to compare to previous work.\nTable 11 shows the results for the test trials for\nthe SemEval experiment. These results all use the\nROBERTA-Large model in the parenthood predic-\ntion step.\nRandom Baseline for Multilingual\nWORD NET Datasets\nTo compute the random baseline in each language,\nwe randomly construct a tree containing the nodes\nin each test tree and compute the ancestor precision,\nrecall and F1 score on the randomly constructed\n4699\ntrees. We include the F1 scores for three trials in\nTable 12.\nModel Run 0 Run 1 Run 2\nCatalan 19.7 19.1 21.2\nChinese 23.5 26.8 27.0\nEnglish 8.1 8.9 9.7\nFinnish 10.6 10.0 9.8\nFrench 22.1 24.7 19.4\nItalian 28.0 27.1 31.6\nDutch 29.7 27.9 22.8\nPolish 20.5 22.1 27.5\nPortuguese 27.9 28.1 22.2\nSpanish 32.6 24.1 24.3\nTable 12: Test F1 Scores for Three Trials Using a Ran-\ndom Baseline.\nSubtree Construction Results, English\nWordNet\nTable 13 shows the results for the learning rate\ntrials for the English WORD NET experiment.\nModel 1e-5 1e-6 1e-7\nBERT 60.0 63.3 60.7\nBERT-Large 59.5 67.3 65.8\nROBERTA-Large 56.3 67.1 65.5\nROBERTA-Large\n(Web-retrieved Glosses) 58.6 71.5 64.7\nROBERTA Large\n(WordNet Glosses) 63.0 83.7 82.9\nTable 13: Dev Results for Different Learning Rates, En-\nglish Models. We highlight in bold the best learning\nrate for each model.\nTable 14 shows the results for the test trials for\nthe English WORD NET experiment.\nSubtree Construction Results, Multilingual\nWordNet\nTable 15 shows the results for the learning rate\ntrials for the non-English WORD NET experiments.\nTable 16 shows the results for the test trials for\nthe non-English WORD NETexperiments.\nModel Run 0 Run 1 Run 2\nBERT 53.6 54.0 52.5\nBERT-Large 58.9 61.5 63.8\nROBERTA-Large 62.9 64.2 63.3\nROBERTA-Large\n(Web-retrieved\nglosses) 66.6 66.3 67.1\nROBERTA-Large\n(WordNet glosses) 82.4 84.0 83.2\nTable 14: Test F1 Scores for Three Trials, English.\nLanguage Model 1e-5 1e-6 1e-7\nCatalan Calbert 39.9 37.9 24.5\nmBERT 39.7 43.5 32.6\nChinese Chinese BERT 56.9 59.0 54.3\nmBERT 57.4 60.6 44.7\nFinnish FinBERT 45.6 50.1 47.0\nmBERT 24.6 30.2 28.9\nFrench French BERT 48.9 50.6 46.9\nmBERT 40.3 41.1 32.5\nItalian Italian BERT 52.6 52.2 46.9\nmBERT 50.7 51.8 41.3\nDutch BERTje 49.0 48.8 38.1\nmBERT 44.9 44.5 32.9\nPolish Polbert 54.2 52.9 48.2\nmBERT 53.0 50.7 36.4\nPortuguese BERTimbau 51.2 52.0 42.1\nmBERT 38.5 37.8 28.0\nSpanish BETO 56.7 57.4 52.8\nmBERT 49.5 41.5 40.4\nTable 15: Dev Results for Different Learning Rates,\nMultilingual. We highlight in bold the best learning\nrate for each model.\nTable 17 shows the results for all tested models\nfor the non-English WORD NET experiments.\n4700\nLanguage Model Run 0 Run 1 Run 2\nCatalan Calbert 36.5 34.1 33.6\nmBERT 39.4 41.8 32.7\nChinese Chinese BERT 57.1 62.3 56.8\nmBERT 55.2 59.4 58.0\nFinnish FinBERT 43.6 44.6 43.2\nmBERT 25.5 26.3 26.7\nFrench French BERT 47.5 49.5 50.4\nmBERT 41.0 40.9 38.9\nItalian Italian BERT 43.2 47.2 47.8\nmBERT 42.9 43.6 49.3\nDutch BERTje 43.8 44.9 42.4\nmBERT 35.9 33.0 27.1\nPolish Polbert 51.2 49.9 47.3\nmBERT 40.1 42.0 41.5\nPortuguese BERTimbau 57.6 57.4 55.8\nmBERT 38.4 38.2 34.3\nSpanish BETO 50.8 53.4 50.9\nmBERT 48.7 49.3 44.0\nTable 16: Test F1 Scores for Three Trials, Multilingual.\nLanguage Model P R F1\nCatalan Calbert 39.3 32.4 34.7\nmBERT 38.7 39.7 38.0\nChinese Chinese BERT 62.2 57.3 58.7\nmBERT 61.9 56.0 57.5\nFinnish FinBERT 47.9 42.6 43.8\nmBERT 29.6 25.4 26.2\nFrench French BERT 51.3 49.1 49.1\nmBERT 43.3 40.0 40.3\nItalian Italian BERT 48.3 45.5 46.1\nmBERT 47.6 44.6 45.3\nDutch BERTje 44.6 44.8 43.7\nmBERT 34.3 31.6 32.0\nPolish Polbert 51.9 49.7 49.5\nmBERT 43.7 41.4 41.2\nPortuguese BERTimbau 59.3 57.1 56.9\nmBERT 38.7 38.2 37.0\nSpanish BETO 53.1 51.7 51.7\nmBERT 47.3 49.4 47.3\nTable 17: Multilingual WORD NET Test Results. We use ISO 639-1 acronyms to indicate languages."
}