{
    "title": "Anchor DETR: Query Design for Transformer-Based Detector",
    "url": "https://openalex.org/W3199093552",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2144634589",
            "name": "Yingming Wang",
            "affiliations": [
                "Vi Technology (United States)",
                "Megvii (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2106804058",
            "name": "Xiangyu Zhang",
            "affiliations": [
                "Vi Technology (United States)",
                "Megvii (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2112768653",
            "name": "Tong Yang",
            "affiliations": [
                "Vi Technology (United States)",
                "Megvii (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2096195284",
            "name": "Jian Sun",
            "affiliations": [
                "Vi Technology (United States)",
                "Megvii (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2144634589",
            "name": "Yingming Wang",
            "affiliations": [
                "Vi Technology (United States)",
                "Megvii (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2106804058",
            "name": "Xiangyu Zhang",
            "affiliations": [
                "Megvii (China)",
                "Vi Technology (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2112768653",
            "name": "Tong Yang",
            "affiliations": [
                "Vi Technology (United States)",
                "Megvii (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2096195284",
            "name": "Jian Sun",
            "affiliations": [
                "Vi Technology (United States)",
                "Megvii (China)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6776048684",
        "https://openalex.org/W2772955562",
        "https://openalex.org/W6778485988",
        "https://openalex.org/W6792041679",
        "https://openalex.org/W3123547918",
        "https://openalex.org/W6631782140",
        "https://openalex.org/W6735463952",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W2902930830",
        "https://openalex.org/W3034573343",
        "https://openalex.org/W6742348326",
        "https://openalex.org/W6639102338",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W6757817989",
        "https://openalex.org/W3033912416",
        "https://openalex.org/W1483870316",
        "https://openalex.org/W2613718673",
        "https://openalex.org/W2991526275",
        "https://openalex.org/W2925359305",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3111272232",
        "https://openalex.org/W2991833700",
        "https://openalex.org/W3033529678",
        "https://openalex.org/W2884561390",
        "https://openalex.org/W4287375617",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2981689412",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W3172752666",
        "https://openalex.org/W2982770724",
        "https://openalex.org/W2950541952",
        "https://openalex.org/W3110402800",
        "https://openalex.org/W2963351448",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2964241181",
        "https://openalex.org/W1536680647",
        "https://openalex.org/W3179888767",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W639708223",
        "https://openalex.org/W2963037989",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W3037798801",
        "https://openalex.org/W3092462694",
        "https://openalex.org/W4214627427",
        "https://openalex.org/W3120633509",
        "https://openalex.org/W3175630421",
        "https://openalex.org/W3194329900",
        "https://openalex.org/W2935837427",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3122239467",
        "https://openalex.org/W2963150697",
        "https://openalex.org/W4287556569",
        "https://openalex.org/W3203974803",
        "https://openalex.org/W3035396860"
    ],
    "abstract": "In this paper, we propose a novel query design for the transformer-based object detection. In previous transformer-based detectors, the object queries are a set of learned embeddings. However, each learned embedding does not have an explicit physical meaning and we cannot explain where it will focus on. It is difficult to optimize as the prediction slot of each object query does not have a specific mode. In other words, each object query will not focus on a specific region. To solve these problems, in our query design, object queries are based on anchor points, which are widely used in CNN-based detectors. So each object query focuses on the objects near the anchor point. Moreover, our query design can predict multiple objects at one position to solve the difficulty: ``one region, multiple objects''. In addition, we design an attention variant, which can reduce the memory cost while achieving similar or better performance than the standard attention in DETR. Thanks to the query design and the attention variant, the proposed detector that we called Anchor DETR, can achieve better performance and run faster than the DETR with 10x fewer training epochs. For example, it achieves 44.2 AP with 19 FPS on the MSCOCO dataset when using the ResNet50-DC5 feature for training 50 epochs. Extensive experiments on the MSCOCO benchmark prove the effectiveness of the proposed methods. Code is available at https://github.com/megvii-research/AnchorDETR.",
    "full_text": "Anchor DETR: Query Design for Transformer-Based Object Detection\nYingming Wang, Xiangyu Zhang, Tong Yang, Jian Sun\nMEGVII Technology\nfwangyingming, zhangxiangyu, yangtong, sunjiang@megvii.com\nAbstract\nIn this paper, we propose a novel query design for the\ntransformer-based object detection. In previous transformer-\nbased detectors, the object queries are a set of learned em-\nbeddings. However, each learned embedding does not have\nan explicit physical meaning and we cannot explain where\nit will focus on. It is difﬁcult to optimize as the prediction\nslot of each object query does not have a speciﬁc mode. In\nother words, each object query will not focus on a speciﬁc\nregion. To solve these problems, in our query design, object\nqueries are based on anchor points, which are widely used\nin CNN-based detectors. So each object query focuses on\nthe objects near the anchor point. Moreover, our query de-\nsign can predict multiple objects at one position to solve the\ndifﬁculty: “one region, multiple objects”. In addition, we de-\nsign an attention variant, which can reduce the memory cost\nwhile achieving similar or better performance than the stan-\ndard attention in DETR. Thanks to the query design and the\nattention variant, the proposed detector that we called Anchor\nDETR, can achieve better performance and run faster than\nthe DETR with 10\u0002 fewer training epochs. For example, it\nachieves 44.2 AP with 19 FPS on the MSCOCO dataset when\nusing the ResNet50-DC5 feature for training 50 epochs. Ex-\ntensive experiments on the MSCOCO benchmark prove the\neffectiveness of the proposed methods. Code is available at\nhttps://github.com/megvii-research/AnchorDETR.\nIntroduction\nThe object detection task is to predict a bounding box and a\ncategory for each object of interest in an image. In the last\ndecades, there are many great progresses in object detection\nbased on the CNN (Ren et al. 2015; Cai and Vasconcelos\n2018; Redmon et al. 2016; Lin et al. 2017; Zhang et al. 2020;\nQiao, Chen, and Yuille 2020; Chen et al. 2021). Recently,\nCarion et al. (Carion et al. 2020) propose the DETR which\nis a new paradigm of object detection based on the trans-\nformer. It uses a set of learned object queries to reason about\nthe relations of the objects and the global image context to\noutput the ﬁnal predictions set. However, the learned object\nquery is very hard to explain. It does not have an explicit\nphysical meaning and the corresponding prediction slots of\neach object query do not have a speciﬁc mode. As shown in\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n(a) DETR\n(b) Ours\nFigure 1: Visualization of the prediction slots. Note that the\nsub-ﬁgure (a) comes from the ﬁgure in DETR (Carion et al.\n2020). Each prediction slot includes all box predictions on\nthe val set of a query. Each of the colored points represents\nthe normalized center position of a prediction. The points\nare color-coded so that the green color corresponds to small\nboxes, red to large horizontal boxes, and blue to large verti-\ncal boxes. The black points in the last row of sub-ﬁgure (b)\nindicate the anchor points. The prediction slots of ours are\nmore related to a speciﬁc position than DETR.\nFigure 1(a), the predictions of each object query in DETR\nare related to different areas and each object query will be in\ncharge of a very large area. This positional ambiguity, i.e.,\nthe object query does not focus on a speciﬁc region, makes\nit hard to optimize.\nReviewing the detectors based on CNN, the anchors are\nhighly related to the position and contain interpretable phys-\nical meanings. Inspired by this motivation, we propose a\nnovel query design based on the anchor points, i.e., we en-\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n2567\nFeature AP GFLOPs FPS\nDETR DC5 43.3 187 12\nSMCA multi-level 43.7 152 10\nDeformable DETR multi-level 43.8 173 15\nConditional DETR DC5 43.8 195 10\nOurs DC5 44.2 172 19\nTable 1: Comparison with transformer detectors. The results\nare based on the ResNet-50 backbone and “DC5” means the\ndilated C5 feature. The DETR is trained with 500 epochs\nwhile the others are trained with 50 epochs. We evaluate\nthe FPS of the proposed detector by following the script in\nDETR and set the batch size to 1 for a fair comparison with\nothers. Note that we follow the DETR to script the model to\nevaluate the speed.\ncode the anchor points as the object queries. The object\nqueries are the encodings of anchor points’ coordinates so\nthat each object query has an explicit physical meaning.\nHowever, this solution will encounter difﬁculty: there will be\nmultiple objects appearing in one position. In this situation,\nonly one object query in this position cannot predict multi-\nple objects, so the object queries from other positions have\nto predict these objects collaboratively. It will cause each ob-\nject query to be in charge of a larger area. Thus, we improve\nthe object query design by adding multiple patterns to each\nanchor point so that each anchor point can predict multiple\nobjects. As shown in Figure 1(b), all the predictions of the\nthree patterns of each object query are distributed around the\ncorresponding anchor point. In other words, it demonstrates\nthat each object query only focuses on the objects near the\ncorresponding anchor point. So the proposed object query\ncan be easy to explain. As the object query has a speciﬁc\nmode and does not need to predict the object far away from\nthe corresponding position, it can easier to optimize.\nBesides the query design, we also design an attention vari-\nant that we call Row-Column Decouple Attention (RCDA).\nIt decouples the 2D key feature into the 1D row feature and\nthe 1D column feature, then conducts the row attention and\ncolumn attention successively. The RCDA can reduce the\nmemory cost while achieving similar or better performance\nthan the standard attention in DETR. We believe it can be a\ngood alternative to the standard attention in DETR.\nAs shown in Table 1, thanks to the novel query design\nbased on the anchor point and the attention variant, the\nproposed detector Anchor DETR can achieve better perfor-\nmance and run faster than the original DETR with even 10\u0002\nfewer training epochs when using the same single-level fea-\nture. Compared with other DETR-like detectors with 10\u0002\nfewer training epochs, the proposed detector achieves the\nbest performance among them. The proposed detector can\nachieve 44.2 AP with 19 FPS when using a single ResNet50-\nDC5 (He et al. 2016) feature for training 50 epochs.\nThe main contributions can be summarized as:\n• We propose a novel query design based on anchor points\nfor the transformer-based detectors. Moreover, we attach\nmultiple patterns to each anchor point so that it can pre-\ndict multiple objects for each position to deal with the\ndifﬁculty: “one region, multiple objects”. The proposed\nquery based on the anchor point is more explainable and\neasier to optimize than the learned embedding. Thanks to\nthe effectiveness of the proposed query design, our detec-\ntor can achieve better performance with 10\u0002fewer train-\ning epochs than the DETR.\n• We design an attention variant that we called Row-\nColumn Decoupled Attention. It can reduce the memory\ncost while achieving similar or better performance than\nthe standard attention in DETR, which can be a good al-\nternative to standard attention.\n• Extensive experiments are conducted to prove the effec-\ntiveness of each component.\nRelated Work\nAnchors in Object Detection\nThere are two type of anchors used in CNN-based object\ndetectors, i.e. anchor boxes (Ren et al. 2015; Lin et al.\n2017) and anchor points (Tian et al. 2019; Zhou, Wang, and\nKr¨ahenb¨uhl 2019). As the hand-craft anchor boxes need to\nbe carefully tuned to achieve good performance, we may\nprefer to not use the anchor boxes. We usually treat the\nanchor-free as the anchor boxes free so that the detectors\nusing anchor point also be treated as anchor-free (Tian et al.\n2019; Zhou, Wang, and Kr ¨ahenb¨uhl 2019). DETR (Carion\net al. 2020) adopt neither the anchor boxes nor the anchor\npoints. It directly predicts the absolute position of each ob-\nject in the image. However, we ﬁnd that introducing the an-\nchor point into the object query can be better.\nTransformer Detector\nCarion et al. (Carion et al. 2020) propose the DETR which\nis based on the transformer (Vaswani et al. 2017) for ob-\nject detection. The transformer detector will feed the in-\nformation of value to the query based on the similarity of\nthe query and key. Zhu et al. (Zhu et al. 2021) propose the\nDeformable DETR that samples information of deformable\npoints around the reference points to the query and applies\nmultiple level features to solve the slowly converge speed\nof the transformer detector. The reference points are simi-\nlar to the anchor points but the object queries of it are still\nthe learned embeddings. It predicts the reference points by\nthe object queries rather than encoding the anchor points as\nthe object queries. Gao et al. (Gao et al. 2021) add a Gaus-\nsian map in the original attention for each query. Concurrent\nwith us, the Conditional DETR (Meng et al. 2021) encodes\nthe reference point as the query position embedding. But the\nmotivations are different so that it only uses the reference\npoint to generate position embedding as the conditional spa-\ntial embedding in the cross attention and the object queries\nare still the learned embeddings. Besides, it does not involve\nthe multiple predictions of one position and the attention\nvariants.\nEfﬁcient Attention\nThe self-attention of the transformer has high complexity so\nit cannot well deal with a very large number of queries and\n2568\n(a) grid anchor points (b) learned anchor points\nFigure 2: Visualization of the anchor points distribution.\nEach point represents the normalized position of an anchor.\nkeys. To solve this problem, many efﬁcient attention mod-\nules have been proposed (Shen et al. 2021; Vaswani et al.\n2017; Beltagy, Peters, and Cohan 2020; Liu et al. 2021; Ma\net al. 2021). One method is to compute the key and value\nﬁrst which can lead to linear complexity of the number of\nthe query or key. The Efﬁcient Attention (Shen et al. 2021)\nand the Linear Attention (Katharopoulos et al. 2020) follow\nthis idea. Another method is to restrict the attention region\nof the key for each query instead of the whole region. The\nRestricted Self-Attention (Vaswani et al. 2017), Deformable\nAttention (Zhu et al. 2021), Criss-Cross Attention (Huang\net al. 2019), and LongFormer (Beltagy, Peters, and Cohan\n2020) follow this idea. In this paper, we decouple the key\nfeature to the row feature and the column feature by 1D\nglobal average pooling and then perform the row attention\nand the column attention successively.\nMethod\nAnchor Points\nIn the CNN-based detectors, anchor points always are the\ncorresponding position of the feature maps. But it can be\nmore ﬂexible in the transformer-based detector. The anchor\npoints can be the learned points, the uniform grid points,\nor other hand-craft anchor points. We adopt two types of an-\nchor points. One is the grid anchor points and the other is the\nlearned anchor points. As shown in Figure 2(a), the gird an-\nchor points are ﬁxed as the uniform grid points in the image.\nThe learned points are randomly initialized with uniform\ndistribution from 0 to 1 and updated as the learned parame-\nters. With the anchor points, the center position ( ^cx; ^cy) of\nthe predicted bounding box will be based on the correspond-\ning anchor point like that in the Deformable DETR (Zhu\net al. 2021).\nAttention Formulation\nThe attention of DETR-like transformer can be formulate as:\nAttention(Q; K; V) = softmax(QKT\npdk\n)V;\nQ = Qf + Qp; K= Kf + Kp; V= Vf ;\n(1)\nwhere the dk is the channel dimension, the subscript f\nmeans the feature, and the subscript p means the position\nembedding. The Q, K, V are the query, key, value respec-\ntively. Note that the Q, K, V will pass through a linear layer\nrespectively and that is omitted in Equation (1) for clarity.\nThere are two attentions in the DETR decoder. One is\nself-attention and the other is cross-attention. In the self-\nattention, the Kf and the Vf are the same as the Qf while\nthe Kp is the same as the Qp. The Qf 2RNq× C is the out-\nput of last decoder and the initial Qinit\nf 2RNq× C for the\nﬁrst decoder can be set to a constant vector or learned em-\nbedding. For the query position embeddingQp 2RNq× C, it\nuses a set of learned embedding in DETR:\nQp = Embedding(Nq; C): (2)\nIn the cross-attention, the Qf 2 RNq× C is generated\nfrom the output of the self-attention in front while the Kf 2\nRHW × C and Vf 2RHW × C are the output feature of the en-\ncoder. The Kp 2RHW × C is the position embedding of Kf .\nIt is generated by the sine-cosine position encoding func-\ntion (Vaswani et al. 2017; Carion et al. 2020) gsin based on\nthe corresponding key feature position Posk 2RHW × 2:\nKp = gsin(Posk): (3)\nNote that the H; W; Cis the height, width, channel of the\nfeature and the Nq is the predeﬁned number of the query.\nAnchor Points to Object Query\nUsually, the Qp in decoder is regarded as the object query\nbecause it is responsible for distinguishing different objects.\nThe object query of learned embedding like Equation (2) is\nhard to explain as discussed in the Introduction section.\nIn this paper, we propose to design the object query based\non the anchor point Posq. The Posq 2RNA× 2 represents\nNA points with their (x; y) positions which range from 0\nto 1. Then the object query based on the anchor points can\nformulate as:\nQp = Encode(Posq): (4)\nIt means that we encode the anchor points as the object\nqueries.\nSo how to design the encoding function? As the object\nquery is designed as the query position embedding as shown\nin Equation (1), the most natural way is to share the same\nposition encoding function as key:\nQp = g(Posq); Kp = g(Posk); (5)\nwhere g is the position encoding function. The position en-\ncoding function could be the gsin or other position encoding\nfunctions. In this paper, instead of just using the heuristic\ngsin, we prefer to use a small MLP network with two linear\nlayers to adapt it additionally.\nMultiple Predictions for Each Anchor Point\nTo deal with the situation that one position may have mul-\ntiple objects, we further improve the object query to pre-\ndict multiple objects for each anchor point instead of just\none prediction. Reviewing the initial query feature Qinit\nf 2\nRNq× C, each of the Nq object queries has one pattern Qi\nf 2\nR1× C, where the i is the index of object queries. To predict\n2569\nEncoder Layer\nFeature Feature Position\nDecoder Layer\nFFN FFN\nClass Box \nPattern Embeddings Anchor Points\nEncoder\nN×\nPosition Encoder Position Encoder\nDecoder\nM×\nBackbone\nDC5\nF5\n1x1 conv\nImage\nFigure 3: Pipeline of the proposed detector. Note that the encoder layer and decoder layer are in the same structure as DETR\nexcept that we replace the self-attention in the encoder layer and the cross-attention in the decoder layer with the proposed\nRow-Column Decouple Attention.\nmultiple objects for each anchor point, we can incorporate\nmultiple patterns into each object query. We use a small set\npattern embedding Qi\nf 2RNp× C:\nQi\nf = Embedding(Np; C); (6)\nto detect objects with different patterns at each position. The\nNp is the number of pattern which is very small, e.g. Np =\n3. For the property of translation invariance, the patterns are\nshared for all the object queries. Thus we can get the initial\nQinit\nf 2 RNpNA× C and Qp 2 RNpNA× C by sharing the\nQi\nf 2RNp× C to each of the Qp 2RNA× C. Here the Nq is\nequal to Np \u0002NA. Then we can deﬁne the proposed Pattern-\nPosition query design as:\nQ = Qinit\nf + Qp: (7)\nFor the following decoder, theQf is also generated from the\noutput of last decoder like DETR.\nThanks to the proposed query design, the proposed de-\ntector has an interpretable query and achieves better per-\nformance than the original DETR with 10\u0002 fewer training\nepochs.\nRow-Column Decoupled Attention\nThe transformer will cost a lot of GPU memory which may\nlimit its use of the high-resolution feature or other exten-\nsions. The Deformable Transformer (Zhu et al. 2021) can\nreduce memory cost but it will lead to random access of\nmemory which may not be friendly to modern accelerators\nof massive parallelism. There are also some attention mod-\nules (Ma et al. 2021; Shen et al. 2021) with linear complexity\nand will not lead to random access of memory. However, in\nour experiments, we ﬁnd that these attention modules cannot\nwell deal with the DETR-like detectors. It may be because\nthe cross-attention in the decoder is much more difﬁcult than\nthe self-attention.\nIn this paper, we propose the Row-Column Decoupled\nAttention (RCDA) which can not only decrease the mem-\nory burdens but also achieve similar or better performance\nthan the standard attention in DETR. The main idea of the\nRCDA is to decouple the 2D key feature Kf 2RH× W× C\nto 1D row feature Kf;x 2RW× C and 1D column feature\nKf;y 2RH× C, then perform the row attention and the col-\numn attention successively. We choose the 1D global aver-\nage pooling to decouple the key feature by default. Without\nlosing generality, we hypothesize W \u0015H and the RCDA\ncan be formulated as:\nAx = softmax(QxKT\nx\npdk\n); Ax 2RNq× W ;\nZ = weightedsumW(Ax; V); Z2RNq× H× C;\nAy = softmax(QyKT\nypdk\n); Ay 2RNq× H;\nOut = weightedsumH(Ay; Z); Out2RNq× C;\n(8)\nwhere\nQx = Qf + Qp;x; Qy = Qf + Qp;y;\nQp;x = g1D(Posq;x); Qp;y = g1D(Posq;y);\nKx = Kf;x + Kp;x; Ky = Kf;y + Kp;y; (9)\nKp;x = g1D(Posk;x); Kp;y = g1D(Posk;y);\nV = Vf ; V 2RH× W× C:\nThe weighted sumW and weighted sumH operations con-\nduct the weighted sum along the width dimension and the\nheight dimension respectively. The Posq;x 2 RNq× 1 is\nthe corresponding row position of Qf 2 RNq× C and the\nPosq;y 2RNq× 1, Posk;x 2RW× 1, Posk;y 2RH× 1 are\nin the similar manner. The g1D is the 1D position encoding\nfunction which is similar to g and it will encode a 1D coor-\ndinate to a vector with C channels.\nNow we analyze why it can save the memory. We hypoth-\nesize the head number M of multi-head attention to 1 for\nclarity in the previous formulation without losing general-\nity, but we should consider the head number M for mem-\nory analysis. The major memory cost for the attention in\n2570\nAnchor-Free NMS-Free RAM-Free Epochs AP AP 50 AP75 APs APm APl\nRetinaNet X 36 38.7 58.0 41.5 23.3 42.3 50.3\nFCOS X X 36 41.4 60.1 44.9 25.6 44.9 53.1\nPOTO X X X 36 41.4 59.5 45.6 26.1 44.9 52.0\nFaster RCNN 36 40.2 61.0 43.8 24.2 43.5 52.0\nCascade RCNN 36 44.3 62.2 48.0 26.6 47.7 57.7\nSparse RCNN X X 36 44.5 63.4 48.2 26.9 47.2 59.5\nDETR-C5 X X X 500 42.0 62.4 44.2 20.5 45.8 61.1\nDETR-DC5 X X X 500 43.3 63.1 45.9 22.5 47.3 61.1\nSMCA X X X 50 43.7 63.6 47.2 24.2 47.0 60.4\nDeformable DETR X X 50 43.8 62.6 47.7 26.4 47.1 58.0\nOurs-C5 X X X 50 42.1 63.1 44.9 22.3 46.2 60.0\nOurs-DC5 X X X 50 44.2 64.7 47.5 24.7 48.2 60.6\nDETR-C5-R101 X X X 500 43.5 63.8 46.4 21.9 48.0 61.8\nDETR-DC5-R101 X X X 500 44.9 64.7 47.7 23.7 49.5 62.3\nOurs-C5-R101 X X X 50 43.5 64.3 46.6 23.2 47.7 61.4\nOurs-DC5-R101 X X X 50 45.1 65.7 48.8 25.8 49.4 61.6\nTable 2: Comparison with other detectors. The results are based on the ResNet-50 backbone if without speciﬁc and R101\nmeans the ResNet-101 backbone. “C5”, “DC5” indicate the detectors using a single C5 or DC5 feature respectively. The other\ndetectors without speciﬁc use multiple-level features. The RAM means the random access of memory which usually is not\nfriendly to hardware in practice.\nRCDA anchors patterns AP AP 50 AP75 APs APm APl\n39.3 59.4 41.8 20.7 42.6 55.0\nX X 44.2 65.3 47.2 24.4 47.8 61.8\nX 40.3 61.6 42.5 21.7 44.1 56.3\nX X 40.3 60.8 43.0 21.1 44.2 57.0\nX X 42.6 63.6 45.5 23.2 46.4 58.3\nX X X 44.2 64.7 47.5 24.7 48.2 60.6\nTable 3: Effectiveness of each component. The RCDA means to replace the standard attention with the RCDA. The “anchors”\nmeans using the anchor points to generate the object queries. The “patterns” means assigning multiple patterns to each object\nquery so that each position has multiple predictions.\nDETR is the attention weight map A 2RNq× H× W× M . The\nattention weight map of RCDA is Ax 2 RNq× W× M and\nAy 2RNq× H× M whose memory cost is much smaller than\nthe A. However, the major memory cost in the RCDA is the\ntemporary result Z. So we should compare the memory cost\nof A 2RNq× H× W× M and Z 2RNq× H× C. The ratio for\nsaving memory of the RCDA is:\nr = (Nq \u0002H \u0002W \u0002M)=(Nq \u0002H \u0002C)\n= W \u0002M=C (10)\nwhere the default setting is M = 8; C= 256. So the mem-\nory cost is roughly the same when the large side W is equal\nto 32 which is a typical value for the C5 feature in object de-\ntection. When using the high-resolution feature it can save\nthe memory, e.g saving roughly 2x memory for the C4 or\nDC5 feature and saving 4x memory for the C3 feature.\nExperiment\nImplementation Details\nWe conduct the experiments on the MS COCO (Lin et al.\n2014) benchmark. All models are trained on the train2017\nsplit and evaluated on the val2017. The models are trained\non 8 GPUs with 1 image per GPU. We train our model\non the training set for 50 epochs with the AdamW opti-\nmizer (Loshchilov and Hutter 2019) setting the initial learn-\ning rate to 10− 5 for the backbone and 10− 4 for the others.\nThe learning rate will be decayed by a factor of 0.1 at the\n40th epoch. We set the weight decay to10− 4 and the dropout\nrate to 0 (i.e. remove dropout). The head for the attention is\n8, the attention feature channel is 256 and the hidden di-\nmension of the feed-forward network is 1024. We choose\nthe number of anchor points to 300 and the number of pat-\nterns to 3 by default. We use a set of learned points as anchor\npoints by default. The number of encoder layers and decoder\nlayers is 6 like DETR. We use the focal loss (Lin et al. 2017)\nas the classiﬁcation loss following the Deformable DETR.\nMain Results\nAs shown in Table 2, we compare the proposed detec-\ntor with RetinaNet (Lin et al. 2017), FCOS (Tian et al.\n2019), POTO (Wang et al. 2020), Faster RCNN (Ren et al.\n2015), Cascased RCNN (Cai and Vasconcelos 2018), Sparse\nRCNN (Sun et al. 2020), DETR (Carion et al. 2020),\n2571\nanchor points patterns AP AP 50 AP75 APs APm APl\n100 1 40.8 61.5 43.6 21.0 44.8 57.5\n100 3 43.4 63.7 46.5 23.8 47.1 60.9\n300 1 42.6 63.6 45.5 23.2 46.4 58.3\n300 3 44.2 64.7 47.5 24.7 48.2 60.6\n900 1 42.9 63.8 46.1 24.4 46.7 58.9\nTable 4: Comparison for multiple predictions of one position. We show the performance with the different number of anchor\npoints and patterns. Note that it degenerates to the single prediction of one anchor point when the number of the pattern is 1.\n(a) (b) (c)\nFigure 4: The histograms of each pattern. We show the histograms of predicted box sizes for different patterns. The abscissa\nrepresents the square root of the area of the predicted box with normalized width “w” and height “h”. The large boxes usually\nappear in pattern (a), the pattern (b) focuses on the small objects, and the pattern (c) is in between.\nAP AP 50 AP75 APs APm APl\ngrid 44.1 64.5 47.6 24.8 48.0 60.9\nlearned 44.2 64.7 47.5 24.7 48.2 60.6\nTable 5: Comparison of different types of anchor points. The\n“learned” means that it uses 300 learned anchor points. The\n“grid” means that it uses the hand-craft grid anchor points\nwith the number of 17 \u000217 which is near to 300.\nSMCA (Gao et al. 2021), and Deformable DETR (Zhu et al.\n2021). The “C5” and “DC5” mean that the detector uses a\nsingle C5 or dilated C5 feature while the other detectors use\nmultiple-level features. Using multiple-level features usu-\nally will have a better performance but cost more resources.\nSurprisingly, our detector with a single DC5 feature can\nachieve better performance than the Deformable DETR and\nSCMA which use multiple-level features. The proposed de-\ntector can achieve better performance than DETR with 10\u0002\nfewer training epochs. It proves the proposed query design\nis very effective.\nWe also show the property of anchor-free, NMS-free, and\nRAM-free for each detector in Table 2. The anchor-free and\nNMS-free indicate the detector does not need the hand-craft\nanchor box and non-maximum suppression. The RAM-free\nmeans the detector will not involve any random access of\nmemory which can be very friendly to modern accelerators\nin practice. The two-stage detectors always are not RAM-\nfree because the region of interest (RoI) is stochastic to\nthe hardware and the operation of the RoI-Align (He et al.\n2017)/RoI-Pooling (Girshick 2015) will involve the random\naccess of memory. The Deformable DETR is similar to the\ntwo-stage detector as the position of the sample point is\nstochastic to the hardware so that it is not RAM-free. On\nthe contrary, the proposed detector inherits the properties of\nanchor-free, NMS-free, and RAM-free from the DETR with\nan improvement of performance and fewer training epochs.\nAblation Study\nEffectiveness of each component Table 3 shows the ef-\nfectiveness of each component that we proposed. The pro-\nposed query design based on anchor points that can predict\nmultiple objects for each position improves the performance\nfrom 39.3 AP to 44.2 AP. The improvements are 4.9 AP\nwhich proves the query design with a speciﬁc focused area\ncan be much easier to optimize. For the attention variant\nRCDA, it achieves similar performance as the standard at-\ntention when adopting the proposed query design. It proves\nthat the RCDA will not degrade the performance with lower\nmemory cost. Besides, by applying the original query design\nas the DETR, the RCDA can achieve 1.0 improvements. We\nthink it is slightly easier to optimize as the attention map\nis smaller than the standard attention. This gain will vanish\nwhen adopting the proposed query design as the query de-\n2572\nAttention Feature AP AP 50 AP75 APs APm APl memory\nLuna (Ma et al. 2021)\nDC5\n36.2 58.7 37.1 15.4 39.2 54.7 2.9G\nEfﬁcient-att (Shen et al. 2021) 34.1 56.8 34.4 13.9 36.0 53.9 2.2G\nstd-att 44.2 65.3 47.2 24.4 47.8 61.8 10.5G\nRCDA 44.2 64.7 47.5 24.7 48.2 60.6 4.4G\nstd-att C5 42.2 63.5 44.9 21.9 45.9 61.0 2.7G\nRCDA 42.1 63.1 44.9 22.3 46.2 60.0 2.3G\nTable 6: Comparison of different attention modules. The “std-att” means the standard attention in DETR. The linear complexity\nattention modules can reduce memory signiﬁcantly but the performance will drop. The proposed RCDA can achieve similar\nperformance as the standard attention in DETR while saving the memory.\nsign has the same effect that makes it easier to optimize. The\nobject query based on anchor points with a single prediction\nhas 2.3 AP improvements and the multiple predictions for\neach anchor point can have 1.6 further improvements. Ap-\nplying the multiple predictions to the original object query\nin DETR will not improve the performance. It is because the\noriginal object query is not highly related to the position thus\nit cannot get beneﬁts from the “one region, multiple predic-\ntions”.\nMultiple Predictions for Each Anchor Point Table 4\nshows that multiple predictions for each anchor point play\nan essential role in query design. For a single prediction (1\npattern), we ﬁnd that 100 anchor points are not good enough\nand 900 anchor points get similar performance as 300 an-\nchor points, thus we use 300 anchor points by default. The\nmultiple predictions (3 patterns) can outperform the single\nprediction (1 pattern) by 2.6 and 1.6 AP for 100 and 300\nanchor points respectively. With the same number of the\npredictions, the multiple predictions for each anchor point\n(300 anchor points, 3 patterns) can outperform the single\nprediction (900 anchor points, 1 pattern) for 1.3 AP. It indi-\ncates that the improvements of multiple predictions do not\ncome from the increased number of predictions. These re-\nsults prove the effectiveness of multiple predictions for each\nanchor point.\nAnchor Points Types We have tried two types of anchor\npoints, i.e., the grid anchor points and the learned anchor\npoints. As shown in Figure 2, the learned anchor points dis-\ntribute uniformly in the image which is similar to the grid\nanchor points. We hypothesize that this is because the ob-\njects distribute everywhere in large COCO data set. We also\nﬁnd the grid anchor points can achieve similar performance\nas the learned anchor points in Table 5.\nPrediction Slots of Object Query As shown in Fig-\nure 1(b), we can observe that the prediction slots of each\nobject query in the proposed detector will focus on the ob-\njects near the corresponding anchor point. As there are three\npatterns for the anchor points, we also show the histogram of\neach pattern in Figure 4. We ﬁnd that the patterns are related\nto the object size as the large boxes usually appear in pattern\n(a) while pattern (b) focuses on the small objects. But the\nquery patterns do not just depend on the size of the object\nbecause the small boxes can also appear in the pattern (a).\nWe think it is because there are more small objects, and mul-\ntiple small objects are more likely to appear in one region.\nSo all the patterns will be in charge of the small objects.\nRow-Column Decoupled Attention As shown in Table 6,\nwe compare the Row-Column Decoupled Attention with\nthe standard attention in DETR and some efﬁcient attention\nmodules with linear complexity. The attention modules with\nlinear complexity can signiﬁcantly reduce the training mem-\nory compared to the standard attention module. However,\ntheir performance drops signiﬁcantly. It may be because the\ncross-attention in the DETR-like detectors is much more\ndifﬁcult than the self-attention. On the contrary, the Row-\nColumn Decoupled Attention can achieve similar perfor-\nmance. As previously discussed, the Row-Column Decou-\npled Attention can signiﬁcantly reduce memory when using\nthe high-resolution feature and get roughly the same mem-\nory cost when using the C5 feature. For example, RCDA\nreduces the training memory from 10.5G to 4.4G and gets\nthe same performance as the standard attention when using\nthe DC5 feature. In conclusion, the Row-Column Decoupled\nAttention can be efﬁcient in memory while preserving com-\npetitive performance so that it can be a good alternative to\nthe standard attention in DETR.\nConclusion\nIn this paper, we propose a detector based on the trans-\nformer. We propose a novel query design based on the an-\nchor point that has explicit physical meaning. The corre-\nsponding prediction slots can have a speciﬁc mode, i.e. the\npredictions are near the anchor point, so that it is easier\nto optimize. Moreover, we incorporate multiple patterns to\neach anchor point to solve the difﬁculty: “one region, multi-\nple objects”. We also propose an attention variant named the\nRow-Column Decoupled Attention. The Row-Column De-\ncouple Attention can reduce the memory cost while achiev-\ning similar or better performance than the standard attention\nin DETR. The proposed detector can run faster and achieve\nbetter performance with 10\u0002fewer training epochs than the\nDETR. The proposed detector is an end-to-end detector with\nthe property of anchor-free, NMS-free, and RAM-free. We\nhope the proposed detector can be useful in practice and be\na strong simple baseline for research.\nAcknowledgments\nThis paper is supported by the National Key R&D Plan\nof the Ministry of Science and Technology (Project No.\n2573\n2020AAA0104400) and Beijing Academy of Artiﬁcial In-\ntelligence (BAAI).\nReferences\nBeltagy, I.; Peters, M. E.; and Cohan, A. 2020. Long-\nformer: The long-document transformer. ArXiv preprint,\nabs/2004.05150.\nCai, Z.; and Vasconcelos, N. 2018. Cascade R-CNN: Delv-\ning Into High Quality Object Detection. In 2018 IEEE Con-\nference on Computer Vision and Pattern Recognition, CVPR\n2018, Salt Lake City, UT, USA, June 18-22, 2018, 6154–\n6162. IEEE Computer Society.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In European Conference on Computer\nVision, 213–229. Springer.\nChen, Q.; Wang, Y .; Yang, T.; Zhang, X.; Cheng, J.; and Sun,\nJ. 2021. You only look one-level feature. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 13039–13048.\nGao, P.; Zheng, M.; Wang, X.; Dai, J.; and Li, H. 2021.\nFast Convergence of DETR with Spatially Modulated Co-\nAttention. ArXiv preprint, abs/2101.07448.\nGirshick, R. B. 2015. Fast R-CNN. In 2015 IEEE Interna-\ntional Conference on Computer Vision, ICCV 2015, Santi-\nago, Chile, December 7-13, 2015, 1440–1448. IEEE Com-\nputer Society.\nHe, K.; Gkioxari, G.; Doll ´ar, P.; and Girshick, R. B. 2017.\nMask R-CNN. In IEEE International Conference on Com-\nputer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017,\n2980–2988. IEEE Computer Society.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Resid-\nual Learning for Image Recognition. In 2016 IEEE Confer-\nence on Computer Vision and Pattern Recognition, CVPR\n2016, Las Vegas, NV , USA, June 27-30, 2016, 770–778.\nIEEE Computer Society.\nHuang, Z.; Wang, X.; Huang, L.; Huang, C.; Wei, Y .; and\nLiu, W. 2019. CCNet: Criss-Cross Attention for Semantic\nSegmentation. In 2019 IEEE/CVF International Conference\non Computer Vision, ICCV 2019, Seoul, Korea (South), Oc-\ntober 27 - November 2, 2019, 603–612. IEEE.\nKatharopoulos, A.; Vyas, A.; Pappas, N.; and Fleuret,\nF. 2020. Transformers are RNNs: Fast Autoregressive\nTransformers with Linear Attention. In Proceedings of\nthe 37th International Conference on Machine Learning,\nICML 2020, 13-18 July 2020, Virtual Event, volume 119\nof Proceedings of Machine Learning Research, 5156–5165.\nPMLR.\nLin, T.; Goyal, P.; Girshick, R. B.; He, K.; and Doll ´ar, P.\n2017. Focal Loss for Dense Object Detection. In IEEE\nInternational Conference on Computer Vision, ICCV 2017,\nVenice, Italy, October 22-29, 2017, 2999–3007. IEEE Com-\nputer Society.\nLin, T.-Y .; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-\nmanan, D.; Doll ´ar, P.; and Zitnick, C. L. 2014. Microsoft\ncoco: Common objects in context. In European conference\non computer vision, 740–755. Springer.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vi-\nsion transformer using shifted windows. ArXiv preprint,\nabs/2103.14030.\nLoshchilov, I.; and Hutter, F. 2019. Decoupled Weight De-\ncay Regularization. In 7th International Conference on\nLearning Representations, ICLR 2019, New Orleans, LA,\nUSA, May 6-9, 2019. OpenReview.net.\nMa, X.; Kong, X.; Wang, S.; Zhou, C.; May, J.; Ma, H.; and\nZettlemoyer, L. 2021. Luna: Linear Uniﬁed Nested Atten-\ntion. arXiv:2106.01540.\nMeng, D.; Chen, X.; Fan, Z.; Zeng, G.; Li, H.; Yuan, Y .; Sun,\nL.; and Wang, J. 2021. Conditional DETR for Fast Training\nConvergence. arXiv:2108.06152.\nQiao, S.; Chen, L.-C.; and Yuille, A. 2020. DetectoRS: De-\ntecting Objects with Recursive Feature Pyramid and Switch-\nable Atrous Convolution. ArXiv preprint, abs/2006.02334.\nRedmon, J.; Divvala, S. K.; Girshick, R. B.; and Farhadi,\nA. 2016. You Only Look Once: Uniﬁed, Real-Time Object\nDetection. In 2016 IEEE Conference on Computer Vision\nand Pattern Recognition, CVPR 2016, Las Vegas, NV , USA,\nJune 27-30, 2016, 779–788. IEEE Computer Society.\nRen, S.; He, K.; Girshick, R. B.; and Sun, J. 2015. Faster\nR-CNN: Towards Real-Time Object Detection with Region\nProposal Networks. In Cortes, C.; Lawrence, N. D.; Lee,\nD. D.; Sugiyama, M.; and Garnett, R., eds., Advances in\nNeural Information Processing Systems 28: Annual Confer-\nence on Neural Information Processing Systems 2015, De-\ncember 7-12, 2015, Montreal, Quebec, Canada, 91–99.\nShen, Z.; Zhang, M.; Zhao, H.; Yi, S.; and Li, H. 2021. Ef-\nﬁcient attention: Attention with linear complexities. In Pro-\nceedings of the IEEE/CVF Winter Conference on Applica-\ntions of Computer Vision, 3531–3539.\nSun, P.; Zhang, R.; Jiang, Y .; Kong, T.; Xu, C.; Zhan, W.;\nTomizuka, M.; Li, L.; Yuan, Z.; Wang, C.; et al. 2020. Sparse\nr-cnn: End-to-end object detection with learnable proposals.\nArXiv preprint, abs/2011.12450.\nTian, Z.; Shen, C.; Chen, H.; and He, T. 2019. FCOS:\nFully Convolutional One-Stage Object Detection. In 2019\nIEEE/CVF International Conference on Computer Vision,\nICCV 2019, Seoul, Korea (South), October 27 - November\n2, 2019, 9626–9635. IEEE.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is All you Need. In Guyon, I.; von Luxburg, U.;\nBengio, S.; Wallach, H. M.; Fergus, R.; Vishwanathan, S.\nV . N.; and Garnett, R., eds., Advances in Neural Informa-\ntion Processing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9, 2017,\nLong Beach, CA, USA, 5998–6008.\nWang, J.; Song, L.; Li, Z.; Sun, H.; Sun, J.; and Zheng, N.\n2020. End-to-end object detection with fully convolutional\nnetwork. ArXiv preprint, abs/2012.03544.\nZhang, S.; Chi, C.; Yao, Y .; Lei, Z.; and Li, S. Z. 2020.\nBridging the Gap Between Anchor-Based and Anchor-Free\nDetection via Adaptive Training Sample Selection. In 2020\n2574\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR 2020, Seattle, WA, USA, June 13-19,\n2020, 9756–9765. IEEE.\nZhou, X.; Wang, D.; and Kr ¨ahenb¨uhl, P. 2019. Objects as\nPoints. volume abs/1904.07850.\nZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J. 2021.\nDeformable DETR: Deformable Transformers for End-to-\nEnd Object Detection. In 9th International Conference on\nLearning Representations, ICLR 2021, Virtual Event, Aus-\ntria, May 3-7, 2021. OpenReview.net.\n2575"
}