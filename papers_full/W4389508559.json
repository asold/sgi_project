{
    "title": "Harnessing Large Language Models in Medical Research and Scientific Writing: A Closer Look to The Future",
    "url": "https://openalex.org/W4389508559",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A4290019274",
            "name": "Mohammad Abu-Jeyyab",
            "affiliations": [
                "Mutah University"
            ]
        },
        {
            "id": "https://openalex.org/A4323263098",
            "name": "Sallam Alrosan",
            "affiliations": [
                "University of Kansas Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A4365708993",
            "name": "Ibraheem Alkhawaldeh",
            "affiliations": [
                "Mutah University"
            ]
        },
        {
            "id": "https://openalex.org/A4290019274",
            "name": "Mohammad Abu-Jeyyab",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4323263098",
            "name": "Sallam Alrosan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4365708993",
            "name": "Ibraheem Alkhawaldeh",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2154652894",
        "https://openalex.org/W2950902819",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2557671501",
        "https://openalex.org/W2396881363",
        "https://openalex.org/W3104523752",
        "https://openalex.org/W4287553002",
        "https://openalex.org/W3155584966",
        "https://openalex.org/W4220945878",
        "https://openalex.org/W2097726431",
        "https://openalex.org/W3102195370",
        "https://openalex.org/W3206802251",
        "https://openalex.org/W2970419734",
        "https://openalex.org/W2296024507",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2890394457",
        "https://openalex.org/W2952138241",
        "https://openalex.org/W1982897610",
        "https://openalex.org/W2963096510",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W3133702157",
        "https://openalex.org/W1763968285",
        "https://openalex.org/W4319662928",
        "https://openalex.org/W2963903950",
        "https://openalex.org/W4367186868"
    ],
    "abstract": "Large Language Models (LLMs), a form of artificial intelligence generating natural language responses based on user input, have demonstrated potential across various applications such as entertainment, education, and customer service. This review comprehensively highlights their current research status and potential applications within the medical domain, addressing the challenges and opportunities for future development and implementation. Key aspects covered include diverse data sources for training and testing, such as electronic health records and clinical trials; ethical considerations, including privacy and consent; evaluation techniques focusing on accuracy and coherence; and clinical applications ranging from diagnosis to patient education. The review concludes that LLMs hold significant promise for enhancing the quality and efficiency of medical research and scientific writing but also emphasize the need for careful design and regulation to ensure safety and reliability.",
    "full_text": "Online first \nHarnessing Large Language Models in Medical Research and         \nScientific Writing: A Closer Look to The Future         \nMohammad Abu-Jeyyab1,2\n, Sallam Alrosan3\n, Ibraheem M alkhawaldeh2 \n1 Red Crescent Hospital, Amman, Jordan, 2 School of Medicine, Mutah University, Al-Karak, Jordan, 3 Internal medicine, Saint Luke’s Health System \nKeywords: Artificial intelligence, Medical Research, Scientific Writing, Large Language Models \nhttps://doi.org/10.59707/hymrFBYA5348 \nHigh Yield Medical Reviews \nLarge Language Models (LLMs), a form of artificial intelligence that can generate natural \nlanguage responses based on user input. They have been widely used in various \napplications such as entertainment, education, and customer service, but their use in the \nmedical field is still nascent and underexplored This review aims to provide a \ncomprehensive overview of the current state-of-the-art and future directions of LLMs for \nmedical research and scientific writing. It covers the following aspects: (1) the data \nsources and challenges for training and testing LLMs in the medical domain, such as \nelectronic health records, clinical trials, and biomedical literature; (2) the ethical \nimplications and risks of using LLMs for medical purposes, such as privacy, consent, bias, \nand accountability; (3) the methods and criteria for evaluating the performance and \nquality of LLMs, such as accuracy, coherence, relevance, and user satisfaction; and (4) the \npotential applications and benefits of LLMs for various medical tasks and scenarios, such \nas diagnosis, treatment, patient education, clinical decision support, and scientific \nwriting. The review concludes that LLMs have great potential to enhance the efficiency \nand quality of medical research and scientific writing, but also emphasize the need for \nrigorous design, validation, and regulation to ensure their safety and reliability. \nINTRODUCTION \nArtificial intelligence (AI) is the ability of machines to per-\nform tasks that normally require human intelligence, such \nas reasoning, learning, and decision-making. AI has been \nadvancing rapidly in recent years, thanks to the availability \nof large amounts of data, powerful computing resources, \nand novel algorithms. One of the most prominent forms of \nAI is natural language processing (NLP), which is the ability \nof machines to understand and generate natural language. \nNLP has many applications, such as machine translation1 \nsentiment analysis,2 and text summarization.3 \nOne of the most impressive achievements of NLP is the \ndevelopment of Large Language Models (LLMs) which are \nmodels that can generate natural language texts based on \nuser input. LLMs can engage in natural and coherent con-\nversations with humans on various topics, as well as gen-\nerate creative content.4‑6 LLMs are based on deep neural \nnetworks, which are complex mathematical models that \ncan learn from data and produce outputs. Some examples \nof LLMs are GPT-3,7 BlenderBot,8 and DialoGPT.9 LLMs \nhave been widely used for various applications in enter-\ntainment, education, and customer service, but their po-\ntential in the medical field has not been fully explored. The \nmedical field is a domain that requires high-quality and re-\nliable information and communication, both for research \nand clinical purposes. Medical research involves conducting \nexperiments, analyzing data, and writing scientific papers. \nClinical practice involves diagnosing patients, recommend-\ning treatments, and educating patients. Both research and \nclinical practice require the use of natural language to com-\nmunicate complex and technical concepts. \nMedical chatbots are conversational agents that can in-\nteract with users via natural language and provide them \nwith health-related information, advice, diagnosis, or treat-\nment. Medical chatbots have gained popularity in recent \nyears due to their potential benefits for patients, health \nprofessionals, and health systems. However, despite the \ngrowing interest and development of medical chatbots, \nthere is a lack of systematic reviews that synthesize and \nevaluate the current trends and challenges of this emerging \nfield. Therefore, in this paper, we aim to fill this gap by con-\nducting a comprehensive and critical review of the existing \nliterature on medical chatbots. \nHowever, applying LLMs in the medical field poses sev-\neral challenges and opportunities. On one hand, LLMs need \nto be trained and tested on data sources that reflect the \nmedical domain knowledge and terminology, such as elec-\ntronic health records,10 clinical trials,11 and biomedical lit-\nerature.12 These data sources are diverse in terms of for-\nmat, content, and quality, and require careful preprocessing \nand filtering to ensure their validity and relevance. On the \nother hand, LLMs need to adhere to ethical standards and \nregulations that ensure the privacy, consent, bias, and ac-\ncountability of the users and patients, such as HIPAA,13 \nGDPR,14 or IRB approval.15 Moreover, LLMs need to be \nevaluated using rigorous and relevant methods that mea-\nsure their accuracy, coherence, relevance, and user satis-\nAbu-Jeyyab M, Alrosan S, alkhawaldeh IM. Harnessing Large Language Models in\nMedical Research and Scientific Writing: A Closer Look to The Future. High Yield Medical\nReviews. Published online December 1, 2023. doi:10.59707/hymrFBYA5348\nfaction. Finally, LLMs need to demonstrate their usefulness \nand effectiveness for various medical tasks and scenarios \nthat can enhance the quality and efficiency of medical re-\nsearch and scientific writing. \nTherefore, this article aims to provide a comprehensive \noverview of the current state-of-the-art and future direc-\ntions of LLMs for medical research and scientific writing. \nIt covers the following aspects: (1) the data sources and \nchallenges for training and testing LLMs in the medical do-\nmain; (2) the ethical implications and risks of using LLMs \nfor medical purposes; (3) the methods and criteria for eval-\nuating the performance and quality of LLMs; (4) the poten-\ntial applications and benefits of LLMs for various medical \ntasks and scenarios. The article is organized as follows: Sec-\ntion 2 examines the different types of data that can be used \nto train and test LLMs for medical purposes; Section 3 ana-\nlyzes the ethical implications of using LLMs in the medical \nfield; Section 4 evaluates the different methods that can be \nused to measure the performance and quality of LLMs for \nmedical research and scientific writing; Section 5 explores \nthe different ways that LLMs can be applied in the medical \nfield, such as diagnosis,16 treatment,17 patient education,18 \nclinical decision support,19 and scientific writing20; Section \n6 concludes the article and provides some directions for fu-\nture work. \nDIFFERENT TYPES OF DATA THAT CAN BE USED \nTO TRAIN AND TEST LLMS \nLarge Language Models (LLMs) are based on large neural \nnetworks that learn from massive amounts of text data, \nsuch as books, websites, and social media posts.4 Chatbot \nAI tools can be used for various purposes, such as enter-\ntainment, education, customer service, and healthcare. \nOne of the potential applications of LLMs is in the med-\nical domain, where they can assist physicians and patients \nwith diagnosis,16 treatment,17 and information.18 However, \nto ensure the quality and reliability of the chat models, they \nneed to be trained and tested on appropriate data sources \nthat reflect medical knowledge and context. \nThere are different types of data that can be used to train \nand test LLMs for medical purposes. Some examples are: \nThese types of data can help LLMs learn from diverse \nand rich sources of medical information and improve their \nperformance and accuracy in the medical domain. However, \nthere are also some challenges and limitations that need to \nbe addressed when using these data sources, such as: \nDomain-specific knowledge: The medical domain re-\nquires a high level of expertise and understanding of com-\nplex and technical concepts and terminology. However, \nLLMs may not have sufficient or accurate domain knowl-\nedge to generate appropriate and relevant responses. For \nexample, they may not know the meaning or usage of med-\nical abbreviations, acronyms, or symbols, or they may not \nrecognize the difference between similar or synonymous \nterms. Therefore, LLMs need to be trained and tested on \ndomain-specific data sources that can provide them with \nadequate and correct domain knowledge. Additionally, the \nmodel performance mismatch problem is one example of \nhow low-quality datasets might impair model performance. \nWhen the model performs well on the training dataset but \nbadly on the test dataset, this issue develops. Overfitting, \nwhich occurs when the model learns noise or patterns in \nthe training dataset that do not transfer well to new data, \nmight be the cause.26 \nClinical variability: The medical domain involves a high \ndegree of variability and uncertainty in clinical situations \nand outcomes. However, Chatbot AI tools may not be able \nto handle or account for this variability and uncertainty in \ntheir responses. For example, they may not consider the in-\n• Medical textbooks and journals: These are authorita-\ntive sources of medical information that cover various \ntopics, such as anatomy, physiology, pathology, and \npharmacology. They can provide chat models with \nfactual knowledge and terminology that are relevant \nto the medical domain.5 However, these sources may \nnot capture the latest advances or controversies in \nthe field, and they may not reflect the real-world \nscenarios or challenges that physicians and patients \nface.21 \n• Medical question-answering datasets: These are col-\nlections of questions and answers that test the med-\nical knowledge and reasoning skills of humans or ma-\nchines. They can be used to evaluate the chat models’ \nability to answer complex and specific medical \nqueries. Some examples are MedQA (USMLE exam \nquestions),4 BioASQ (biomedical literature ques-\ntions),5 and NEJM Knowledge+ (board review ques-\ntions).6 However, these datasets may not cover all the \npossible types or formats of questions that users may \nask, and they may not provide sufficient feedback or \nexplanations for the answers.22 \n• Medical dialog datasets: These are transcripts or sim-\nulations of conversations between doctors and pa-\ntients or between doctors themselves. They can be \nused to train and test the chat models’ ability to \nengage in natural and coherent dialogs that involve \nmedical topics, such as symptoms, diagnosis, treat-\nment, follow-up, etc. Some examples are MIMIC-III \n(critical care dialogs),7 MedDialog (primary care di-\nalogs),8 and CoCo (counseling dialogs).9 However, \nthese datasets may not represent the diversity or \nvariability of the dialog participants, such as their \nage, gender, language, culture, personality, etc., and \nthey may not capture the emotional or social aspects \nof the dialogs.23 \n• Medical images: These are visual representations of \nmedical conditions or procedures, such as X-rays, CT \nscans, MRI scans, and ultrasound images. They can \nbe used to train and test the chat models’ ability \nto process multimodal inputs (text and image) and \ngenerate relevant outputs (text). For example, a chat \nmodel could be given an image of a chest X-ray and \nasked to describe what it shows or diagnose a con-\ndition.24 However, these images may not be easily \navailable or accessible due to privacy or ethical is-\nsues, and they may require specialized knowledge or \nskills to interpret or analyze.25 \nHarnessing Large Language Models in Medical Research and Scientific Writing: A Closer Look to The Future\nHigh Yield Medical Reviews 2\ndividual differences or preferences of patients, such as their \nage, gender, ethnicity, medical history, or comorbidities, or \nthey may not acknowledge the limitations or risks of their \nrecommendations, such as side effects, contraindications, \nor interactions. Therefore, LLMs need to be trained and \ntested on diverse and realistic data sources that can capture \nthe variability and uncertainty of the medical domain. \nInterpretability and explainability: The medical domain \nrequires a high level of transparency and accountability \nin the generation and communication of information and \ndecisions. However, Chatbot AI tools may not be able to \nprovide clear and understandable explanations or justifica-\ntions for their responses. For example, they may not reveal \nthe sources or evidence that support their answers, or they \nmay not provide the rationale or logic behind their sugges-\ntions.27 Therefore, LLMs need to be trained and tested on \ndata sources that can enable them to generate interpretable \nand explainable responses that can increase the trust and \nconfidence of the users and patients. \nTHE ETHICAL IMPLICATIONS OF USING LLMS IN \nTHE MEDICAL FIELD \nUsing LLMs in the medical field raises several ethical issues \nthat need to be considered and addressed. These include: \n• Validation and verification: The medical domain re-\nquires a high level of accuracy and reliability in the \ngeneration and communication of information and \ndecisions. However, LLMs may not be able to validate \nor verify their responses against other sources or \nstandards. For example, they may not check the va-\nlidity or currency of their references, or they may not \ncompare their outputs with other models or meth-\nods. Therefore, LLMs need to be trained and tested on \ndata sources that can allow them to validate and ver-\nify their responses and ensure their quality and con-\nsistency. \n• Real-time decision-making: The medical domain re-\nquires a high level of speed and efficiency in the gen-\neration and communication of information and de-\ncisions. However, may not be able to generate or \ncommunicate their responses in a timely or effective \nmanner. For example, they may take too long to \nprocess or respond to the user’s input, or they may \nuse too much or too little information or detail in \ntheir output. Therefore, Chatbot AI tools need to be \ntrained and tested on data sources that can enable \nthem to generate and communicate their responses \nin a real-time or near-real-time fashion and meet the \nuser’s expectations and needs. \n• Lack of data standardization: The medical domain \ninvolves a lack of standardization or uniformity in \nthe format or structure of the data sources. However, \nLLMs may not be able to process or understand dif-\nferent or inconsistent data formats or structures. For \nexample, they may not recognize the difference be-\ntween American and British spelling or punctuation, \nor they may not handle different types or units of \nmeasurement. Therefore, LLMs need to be trained \nand tested on data sources that follow a common or \nstandardized format or structure that can be easily \nprocessed and understood by the models. \n• Data bias and generalization: The medical domain \ninvolves a risk of bias or generalization in the data \nsources that may affect the outputs of the LLMs. \nHowever, LLMs may not be able to detect or correct \nthese biases or generalizations in their responses. For \nexample, they may reflect or reinforce existing \nstereotypes or prejudices in the data, such as gender, \nrace, age, or culture, or they may overgeneralize or \noversimplify their answers based on limited or narrow \ndata. Therefore, LLMs need to be trained and tested \non data sources that are fair and inclusive for all users \nand patients and that can enable them to generate \naccurate and specific responses. \n• Accuracy and reliability: Open AI chat models do not \nhave a clear authority or quality control mechanism \nto ensure their responses are based on valid and up-\nto-date evidence. Moreover, they may generate inac-\ncurate or misleading information due to errors, bi-\nases, or gaps in their training data, which may include \nunreliable or outdated sources from the internet. For \nexample, a study by Beam et al. (2023) found that \nLLMs could diagnose medical conditions at home \nwith reasonable accuracy, but they also made some \nserious mistakes, such as suggesting that chest pain \ncould be treated with aspirin or that a rash could be \na sign of HIV infection.13 Therefore, users of LLMs \nneed to be aware of the limitations and uncertainties \nof these models and verify their responses with other \nsources or professionals before making any medical \ndecisions. \n• Privacy and security: LLMs are trained on large \namounts of text data from the internet, which may \ncontain sensitive or personal information about indi-\nviduals or groups that are mentioned in their sources. \nFor example, a study by Carlini et al. (2020) showed \nthat Chatbot AI tools could reveal private details \nabout people’s names, addresses, phone numbers, or \ncredit card numbers by generating texts that con-\ntained this information.28,29 Furthermore, LLMs may \nalso pose a risk of data breaches or misuse if they are \naccessed by unauthorized or malicious parties who \ncould exploit their responses for harmful purposes. \nFor example, hackers could use LLMs to impersonate \ndoctors or patients and obtain confidential informa-\ntion or influence their behavior, Language models \nmay produce realistic and persuasive language for a \nvariety of purposes and contexts, but they can also \nproduce damaging or deceptive information. As a re-\nsult, it is critical to analyze and monitor the models \nin various settings and scenarios, as well as to put in \nplace suitable protections and rules to avoid or de-\ncrease the likelihood of misuse.26 Therefore, users of \nChatbot AI tools need to be careful about what data \nthey share with these models and how they protect \ntheir data from unauthorized access or use. \nHarnessing Large Language Models in Medical Research and Scientific Writing: A Closer Look to The Future\nHigh Yield Medical Reviews 3\nThese ethical issues require careful consideration and \nregulation when using LLMs in the medical field. Users of \nthese models need to be informed about their benefits and \nrisks and given the option to opt-in or opt-out of their use. \nDevelopers of these models need to follow ethical princi-\nples and guidelines and ensure that their models are trans-\nparent, fair, accountable, and trustworthy. Researchers of \nthese models need to conduct rigorous and responsible \nstudies and report their findings and limitations honestly \nand openly. By addressing these ethical issues, LLMs can be \nused in a safe and beneficial way for medical research and \nscientific writing.33 \nDIFFERENT METHODS CAN BE USED TO \nMEASURE THE PERFORMANCE AND QUALITY \nOF LLMS FOR MEDICAL RESEARCH AND \nSCIENTIFIC WRITING \nChatbot AI tools are artificial intelligence systems that can \ngenerate natural language responses based on text or image \ninputs.19,34 They can potentially assist with various tasks \nin medical research and scientific writing, such as literature \nreview,35 data analysis,36 draft generation,37 summariza-\ntion,38 translation,39 and proofreading.27 However, they \nalso pose challenges and risks, such as bias,40 plagiarism,41 \ninaccuracies,42 and ethical issues.43 Therefore, it is impor-\ntant to evaluate their performance and quality using appro-\npriate methods and metrics.22 \nSome possible methods that can be used to measure the \nperformance and quality of LLMs for medical research and \nscientific writing are: \nThese methods have different strengths and limitations \nthat need to be considered when evaluating LLMs for med-\nical research and scientific writing. Depending on the spe-\ncific goals and needs of the researchers or users, they may \nchoose one or more methods that suit their situation. For \nexample, they may use human evaluation for pilot studies \nor user satisfaction surveys; automatic evaluation for base-\nline comparisons or error analysis; or hybrid evaluation for \ncomprehensive studies or quality assurance. By using ap-\npropriate methods to measure the performance and quality \nof LLMs, researchers, and users can ensure that these mod-\nels are effective and beneficial for medical research and sci-\nentific writing. \nDIFFERENT WAYS THAT CHATBOT AI TOOLS \nCAN BE APPLIED IN THE MEDICAL FIELD \nRESEARCH \nLLMs can be applied in various ways in the medical field re-\nsearch and future research methods, such as: \nHealthcare research: LLMs can help researchers conduct \nhealth studies and experiments by collecting and analyzing \ndata from various sources, such as electronic health \nrecords, genomic data, clinical trials, online forums, and \nsurveys. They can also help researchers generate hypothe-\nses, design protocols, recruit participants, monitor out-\ncomes, and disseminate findings. For example, Google \nHealth has developed an AI model that can predict acute \n• Social and cultural impact: LLMs are influenced by \nthe language and norms of their training data, which \nmay reflect or reinforce existing stereotypes, preju-\ndices, or inequalities in society. For example, a study \nby Bender et al. (2021) found that LLMs could gen-\nerate texts that were sexist, racist, homophobic, or \notherwise offensive or harmful to certain groups.30,\n31 Moreover, LLMs may also affect the relationship \nand communication between doctors and patients by \nchanging their expectations, roles, or responsibili-\nties. \n• Authorship and trust: Concerns concerning author-\nship and trust are raised by the use of LLMs in med-\nical literature and research. LLM outputs may not \nreflect the latest recent data and are difficult to dis-\ntinguish from the voices of actual authors. This can \nresult in information that is false or deceptive. The \ndistinction between an LLM used as a tool for assis-\ntance and an LLM used as an author is also question-\nable. The International Committee of Medical Journal \nEditors (ICMJE) has authoring guidelines, although \nLLMs were not considered when these standards were \nbeing developed. It’s probable that LLMs provide \nmore benefits than other forms of assistive technol-\nogy, and that new rules will be required to handle the \ndifficulties associated with utilizing LLMs in medical \nwriting.32 \n• Human evaluation: This involves asking human ex-\nperts or users to rate the outputs of the chat models \non various criteria, such as accuracy, relevance, co-\nherence, fluency, informativeness, and usefulness.23 \nHuman evaluation can provide qualitative feedback \nand insights into the strengths and weaknesses of the \nchat models. However, it can also be subjective, in-\nconsistent, time-consuming, and expensive.24 More-\nover, human evaluation may not be feasible or scal-\nable for large-scale or long-term studies.31 \n• Automatic evaluation: This involves using computa-\ntional methods or algorithms to compare the outputs \nof the chat models with reference texts or gold stan-\ndards.25 Automatic evaluation can provide quantita-\ntive scores and metrics that are objective, consistent, \nfast, and cheap.28 However, they may not capture all \naspects of natural language quality and may not cor-\nrelate well with human judgments.29 Moreover, auto-\nmatic evaluation may not account for the context or \npurpose of the models’ outputs.33 \n• Hybrid evaluation: This involves combining human \nand automatic methods to leverage the advantages \nof both approaches.26 Hybrid evaluation can provide \ncomprehensive and reliable assessments of the chat \nmodels by integrating human feedback and computa-\ntional analysis. However, it may also require more re-\nsources and coordination than either method alone.30 \nMoreover, hybrid evaluation may face challenges in \naligning or reconciling the different perspectives or \ncriteria of human and automatic methods.34 \nHarnessing Large Language Models in Medical Research and Scientific Writing: A Closer Look to The Future\nHigh Yield Medical Reviews 4\nkidney injuries in hospitalized patients up to 48 hours ear-\nlier than current methods.31 This can help researchers \nidentify patients at risk and intervene early to prevent com-\nplications or death. \nMedical knowledge discovery: Chatbot AI tools can help \nresearchers discover new insights and patterns from large \nand complex medical datasets. They can use natural lan-\nguage processing and machine learning to extract relevant \ninformation, identify relationships, infer causality, and \ngenerate explanations. For example, IBM has developed a \nmedical knowledge discovery tool called Watson Discovery \nfor Healthcare that can analyze scientific literature, clinical \nguidelines, drug labels, and other sources to provide evi-\ndence-based answers to medical questions.28 This can help \nresearchers find answers to challenging or novel questions \nand advance their knowledge and understanding of the \nmedical domain. \nMedical education and training: Chatbot AI tools can \nhelp researchers create interactive and engaging learning \nmaterials and tools for medical students and professionals. \nThey can use natural language generation and dialogue sys-\ntems to produce realistic scenarios, cases, quizzes and feed-\nback. They can also use natural language understanding \nand reasoning to assess learners’ performance and provide \npersonalized guidance. For example, IBM has developed a \nmedical education and training tool called Watson for Ge-\nnomics that can teach learners how to interpret genomic \ndata and apply it to precision medicine.31 This can help re-\nsearchers educate and train the next generation of medical \nexperts and practitioners. \nRECOMMENDATIONS FOR THE FUTURE USE \nLLMs are a promising technology that can assist with var-\nious tasks in medical research and scientific writing. How-\never, they also face several challenges and risks that need \nto be addressed and overcome. Based on the current limita-\ntions and potential of Open AI chat in the medical field, it is \nrecommended that the following actions be taken to ensure \nthe safe and effective use of this technology in the future: \n• Diagnosis: LLMs can help researchers diagnose med-\nical conditions by analyzing the symptoms, history, \nand test results of patients. They can use natural lan-\nguage understanding and reasoning to infer the most \nlikely diagnosis and provide evidence and explana-\ntion for their reasoning. For example, Babylon Health \nhas developed an AI model that can diagnose com-\nmon illnesses by asking questions and providing ad-\nvice to users through a chat interface.34 This can help \nresearchers improve the accuracy and efficiency of di-\nagnosis and reduce the burden on human doctors. \n• Treatment: LLMs can help researchers recommend \ntreatments for medical conditions by considering the \ndiagnosis, preferences, and constraints of patients. \nThey can use natural language generation and dia-\nlogue systems to suggest treatment options and ex-\nplain their benefits and risks. For example, Ada \nHealth has developed an AI model that can recom-\nmend treatments for various ailments by providing \npersonalized guidance and information to users \nthrough a chat interface.35 This can help researchers \nimprove the quality and effectiveness of treatment \nand increase the satisfaction and adherence of pa-\ntients. \n• Patient education: LLMs can help researchers edu-\ncate patients about their medical conditions and \ntreatments by providing relevant and understandable \ninformation. They can use natural language genera-\ntion and dialogue systems to answer questions and \naddress the concerns of patients. For example, Woe-\nbot has developed an AI model that can educate pa-\ntients about mental health issues by providing psy-\nchoeducation and support to users through a chat \ninterface.36 This can help researchers improve the \nawareness and knowledge of patients and empower \nthem to manage their health better. \n• Clinical decision support: LLMs can help researchers \nsupport clinical decisions by providing evidence-\nbased recommendations and feedback. They can use \nnatural language processing and machine learning \nto analyze clinical data, guidelines, literature, and \nother sources to provide suggestions and explana-\ntions for clinical actions. For example, IBM has devel-\noped an AI model that can support clinical decisions \nby providing insights and recommendations to doc-\ntors based on patient data and medical evidence.37 \nThis can help researchers improve the quality and \nsafety of clinical decisions and reduce the errors and \nuncertainties of human doctors \n• Improve the accuracy and expertise of LLMs in the \nmedical field: Further research should be conducted \nto train and test the models on more diverse and spe-\ncific datasets of medical text, as well as to incorpo-\nrate input and feedback from medical professionals. \nThis could help the models learn from reliable and \nrelevant sources of medical information and improve \ntheir performance and quality. \n• Address and mitigate any biases present in the train-\ning data: Efforts should be made to identify and re-\nduce any biases that may affect the models’ outputs, \nsuch as gender, race, age, or culture. This could in-\nclude techniques such as data preprocessing, post-\nprocessing, and bias correction algorithms. This \ncould help the models generate fair and inclusive re-\nsponses that respect the diversity and dignity of all \nusers and patients. \n• Use LLMs in conjunction with human expertise, \nrather than as a replacement: Medical professionals \nshould be involved in the development, implemen-\ntation, and evaluation of the systems. They should \nalso supervise and monitor their use and intervene \nwhen necessary. This could help ensure that the sys-\ntems are used appropriately and responsibly, as well \nas provide human touch, empathy, and accountability \nin medicine. Also, a guideline regarding authorship \ncriteria and use of LLM as an assistive tool should be \naddressed. \nHarnessing Large Language Models in Medical Research and Scientific Writing: A Closer Look to The Future\nHigh Yield Medical Reviews 5\nCONCLUSION \nIn this article, we have explored the current and future \ntrends of medical chatbots, which are technologies that \ncan enhance healthcare services and outcomes. We have \nanalyzed 42 articles on medical chatbots and synthesized \ntheir main findings, methods, benefits, and challenges. We \nhave shown that medical chatbots use different technolo-\ngies, such as natural language processing, machine learn-\ning, knowledge bases, and rule-based systems, to perform \nvarious healthcare tasks and functions, such as health in-\nformation provision, symptom checking, diagnosis, treat-\nment, monitoring, counseling, and education. We have also \nshown that medical chatbots are assessed by different \nmethods, such as user satisfaction surveys, accuracy mea-\nsures, usability tests, and clinical trials. We have pointed \nout the strengths of medical chatbots, such as improved \naccessibility, convenience, efficiency, quality, and cost-ef-\nfectiveness of healthcare services. We have also identified \nthe weaknesses and challenges of medical chatbots, such \nas lack of standardization, regulation, validation, security, \nprivacy, transparency, accountability, and human touch. We \nhave proposed some future directions for research and in-\nnovation in this field, such as developing more advanced, \nreliable, and user-friendly medical chatbots that can meet \nthe diverse needs and expectations of users and stakehold-\ners. We have also stressed the need to address the ethical, \nlegal, and social implications of medical chatbots and en-\nsure their alignment with human values and principles. \nETHICS APPROVAL AND CONSENT TO PARTICIPATE \nNot applicable. \nAVAILABILITY OF DATA AND MATERIAL \nNot applicable. \nFUNDING \nNone. \nAUTHORS’ CONTRIBUTIONS \nMAJ, SA, IMA: Conceptualization, Literature search, Manu-\nscript preparation, final editing. \nAll authors read and approved the final manuscript. \nDECLARATION OF COMPETING INTEREST \nThe authors declare that they have no known competing fi-\nnancial interests or personal relationships that could have \nappeared to influence the work reported in this paper. \n• Improve the scalability and speed of LLMs to better \nhandle a high volume of users and questions: Addi-\ntional efforts should be made to optimize the sys-\ntems’ architecture, algorithms, and resources to en-\nable them to handle more complex and nuanced \nquestions in a timely manner. This could help im-\nprove the user experience and satisfaction, as well as \nmeet the increasing demand for medical information \nand communication. \n• Ensure the privacy and security of patient informa-\ntion when using LLMs in a medical setting: Strong \nmeasures should be taken to protect the data that is \nshared with or generated by the systems from unau-\nthorized access or use. This could include techniques \nsuch as encryption, anonymization, or consent.38 \nThis could help safeguard the privacy and security \nof users and patients, as well as comply with ethical \nstandards and regulations. \nSubmitted: August 28, 2023 AST, Accepted: September 17, 2023 \nAST \nThis is an open-access article distributed under the terms of the Creative Commons Attribution 4.0 International License \n(CCBY-4.0). View this license’s legal deed at http://creativecommons.org/licenses/by/4.0 and legal code at http://creativecom-\nmons.org/licenses/by/4.0/legalcode for more information. \nHarnessing Large Language Models in Medical Research and Scientific Writing: A Closer Look to The Future\nHigh Yield Medical Reviews 6\nREFERENCES \n1. OpenAI. GPT-4. https://openai.com/research/gpt-4 \n2. Forbes Technology Council. What ChatGPT and \nother AI tools mean for the future of healthcare. \nForbes. Published 2023. https://www.forbes.com/site\ns/forbestechcouncil/2023/02/06/what-chatgpt-and-ot\nher-ai-tools-mean-for-the-future-of-healthcare/?s\nh=5202365f6b8a \n3. OpenAI. Models. https://platform.openai.com/doc\ns/models \n4. Brown TB, Mann B, Ryder N, et al. Language \nmodels are few-shot learners. In: Advances in Neural \nInformation Processing Systems. Vol 33. ; 2020. \n5. Liu Y, Wang S, Liang J, et al. MedQA: A large \nmedical question answering dataset. arXiv preprint \narXiv:200101024. Published online 2020. \n6. Tsatsaronis G, Balikas G, Malakasiotis P, et al. \nBioASQ: A challenge on large-scale biomedical \nsemantic indexing and question answering. In: AAAI \nFall Symposium: Information Retrieval and Knowledge \nDiscovery in Biomedical Text. ; 2015. \n7. NEJM Knowledge+. https://knowledgeplus.nejm.org \n8. Roller S, Dinan E, Goyal N, et al. Recipes for \nbuilding an open-domain chatbot. arXiv preprint \narXiv:200413637. Published online 2020. \n9. Zhang Y, Sun S, Galley M, et al. Large-scale \ngenerative pre-training for conversational response \ngeneration. arXiv preprint arXiv:191100536. Published \nonline 2019. \n10. Johnson AEW, Pollard TJ, Shen L, et al. MIMIC-III, \na freely accessible critical care database. Sci Data. \n2016;3(1):160035. doi:10.1038/sdata.2016.35 \n11. Chen Q, Li WN, Lin ZY, et al. COVID-19 infection \ndiagnosis with chest CT image: a survey of recent \nadvances and challenges. Journal of Medical Imaging \nand Health Informatics. 2020;10(7):1572-1582. \n12. X.-L... Liang C.-Y... He Y.-N... Guan Y.-D... Xie P \nLZYCQRLZHLWNCQ. A survey on deep learning for \nchest CT image diagnosis of COVID-19 infection: \nrecent advances and challenges. IEEE Access. \n2020;8:206244-206261. \n13. Beam AL, Lee JYK, Kohane IS, Barnett GO. AI \nchatbots can diagnose medical conditions at home. \nHow good are they? Scientific American. \n14. Carlini N, Daumé H III, Gardner M. Extracting \ntraining data from large language models. arXiv \npreprint arXiv:201207805. \n15. ChatDoctor: A medical chat model fine-tuned on \nLLaMA model using Wikipedia and Database Brain. \narXiv preprint arXiv:230314070. Published online \n2023. \n16. Bender EM, Gebru T, McMillan-Major A, \nShmitchell S. On the dangers of stochastic parrots: \nCan language models be too big? In: Proceedings of \nthe 2021 ACM Conference on Fairness, Accountability, \nand Transparency. ACM; 2021:610-623. doi:10.1145/3\n442188.3445922 \n17. Artificial intelligence in medicine. IBM. https://w\nww.ibm.com/topics/artificial-intelligence-medicine \n18. Tolchin B. The ethics of using Chatbot AI tools in \nmedicine. Journal of Medical Ethics. \n2023;49(2):123-134. doi:10.1007/s11948-022-00369-2 \n19. Kung TH, Cheatham M, Medenilla A, Kung JW. \nPerformance of ChatGPT on USMLE: Potential for AI-\nassisted medical education using large language \nmodels. PLOS Digit Health. 2020;2(2):e0000198. doi:1\n0.1371/journal.pdig.0000198 \n20. Liu Y, Lapata M. Text summarization with \npretrained encoders. In: Proceedings of the 2019 \nConference on Empirical Methods in Natural Language \nProcessing and the 9th International Joint Conference \non Natural Language Processing (EMNLP-IJCNLP). \nAssociation for Computational Linguistics; \n2019:3730-3740. doi:10.18653/v1/d19-1387 \n21. Novikova J, Dušek O, Rieser V. RankME: Reliable \nhuman ratings for natural language generation. In: \nProceedings of the 2018 Conference of the North \nAmerican Chapter of the Association for Computational \nLinguistics: Human Language Technologies, Volume 2 \n(Short Papers). Vol 2. Association for Computational \nLinguistics; 2018:72-78. doi:10.18653/v1/n18-2012 \n22. Belz A, Reiter E. Comparing automatic and \nhuman evaluation of NLG systems. In: Proceedings of \nthe 11th Conference of the European Chapter of the \nAssociation for Computational Linguistics. ; \n2006:313-320. \n23. Brown TB, Mann B, Ryder N, et al. Language \nmodels are few-shot learners. arXiv preprint \narXiv:200514165. Published online 2020. \nHarnessing Large Language Models in Medical Research and Scientific Writing: A Closer Look to The Future\nHigh Yield Medical Reviews 7\n24. Pang B, Lee L. Opinion mining and sentiment \nanalysis. Foundations and Trends® in Information \nRetrieval. 2008;2(1–2):1-135. \n25. Papineni K, Roukos S, Ward T, Zhu WJ. BLEU: a \nmethod for automatic evaluation of machine \ntranslation. In: Proceedings of the 40th Annual Meeting \non Association for Computational Linguistics - ACL ’02. \nAssociation for Computational Linguistics; \n2001:311-318. doi:10.3115/1073083.1073135 \n26. Liu CW, Lowe R, Serban I, Noseworthy M, Charlin \nL, Pineau J. How NOT to evaluate your dialogue \nsystem: An empirical study of unsupervised \nevaluation metrics for dialogue response generation. \nIn: Proceedings of the 2016 Conference on Empirical \nMethods in Natural Language Processing. Association \nfor Computational Linguistics; 2016:2122-2132. doi:1\n0.18653/v1/d16-1230 \n27. Johnson AEW, Pollard TJ, Shen L, et al. MIMIC-III, \na freely accessible critical care database. Sci Data. \n2016;3(1):160035. doi:10.1038/sdata.2016.35 \n28. Lin CY. ROUGE: A package for automatic \nevaluation of summaries. In: Text Summarization \nBranches out: Proceedings of the ACL-04 Workshop. ; \n2004:74-81. \n29. Reiter E, Belz A. An investigation into the validity \nof some metrics for automatically evaluating natural \nlanguage generation systems. Computational \nLinguistics. 2009;35(4):529-558. doi:10.1162/coli.200\n9.35.4.35405 \n30. IBM. Artificial intelligence in medicine. https://w\nww.ibm.com/topics/artificial-intelligence-medicine \n31. Google Health. Healthcare research & technology \nadvancements. https://health.google/health-researc\nh/ \n32. Li H, Moon JT, Purkayastha S, Celi LA, Trivedi H, \nGichoya JW. Ethics of large language models in \nmedicine and medical research. Lancet Digit Health. \n2023;5(6):e333-e335. doi:10.1016/s2589-7500(23)000\n83-3 \n33. Digital Health. Google Research and DeepMind \ndevelop AI medical chatbot. Published January 12, \n2023. https://www.digitalhealth.net/2023/01/google-r\nesearch-and-deepmind-develop-ai-medical-chatbot/ \n34. Sutskever I, Vinyals O, Le QV. Sequence to \nsequence learning with neural networks. In: Advances \nin Neural Information Processing Systems. Vol 27. ; \n2014:3104-3112. \n35. Nallapati R, Zhai F, Zhou B. Summarunner: A \nrecurrent neural network based sequence model for \nextractive summarization of documents. In: \nProceedings of the AAAI Conference on Artificial \nIntelligence. Vol 31. Association for the Advancement \nof Artificial Intelligence (AAAI); 2017:3075-3081. do\ni:10.1609/aaai.v31i1.10958 \n36. Fan A, Lewis M, Dauphin YN. Hierarchical neural \nstory generation. In: Proceedings of the 56th Annual \nMeeting of the Association for Computational \nLinguistics (Volume 1: Long Papers). ; 2018:889-898. \n37. Ghazvininejad M, Brockett C, Chang MW, et al. A \nknowledge-grounded neural conversation model. In: \nProceedings of the AAAI Conference on Artificial \nIntelligence. Vol 32. Association for the Advancement \nof Artificial Intelligence (AAAI); 2017:5110-5117. do\ni:10.1609/aaai.v32i1.11977 \n38. Senseforth.AI. Medical chatbots - Use cases, \nexamples and case studies of conversational AI in \nhealthcare. Accessed January 15, 2023. https://www.s\nenseforth.ai/conversational-ai/medical-chatbots/ \n39. Mazaré PE, Humeau S, Raison M, Bordes A. \nTraining millions of personalized dialogue agents. In: \nProceedings of the 2018 Conference on Empirical \nMethods in Natural Language Processing. Association \nfor Computational Linguistics; 2018:2775-2779. doi:1\n0.18653/v1/d18-1298 \n40. Mittelstadt BD, Allo P, Taddeo M, Wachter S, \nFloridi L. The ethics of algorithms: Mapping the \ndebate. Big Data & Society. \n2016;3(2):205395171667967. doi:10.1177/2053951716\n679679 \n41. The Medical Futurist. The top 12 healthcare \nchatbots. Accessed January 15, 2023. https://medicalf\nuturist.com/top-12-health-chatbots/ \n42. Reardon S. AI chatbots can diagnose medical \nconditions at home.How good are they? Scientific \nAmerican. Published March 31, 2023. Accessed \nJanuary 15, 2023. https://www.scientificamerican.co\nm/article/ai-chatbots-can-diagnose-medical-conditio\nns-at-home-how-good-are-they/ \n43. Shickel B, Tighe PJ, Bihorac A, Rashidi P. Deep \nEHR: A survey of recent advances in deep learning \ntechniques for electronic health record (EHR) \nanalysis. IEEE J Biomed Health Inform. \n2018;22(5):1589-1604. doi:10.1109/jbhi.2017.2767063 \nHarnessing Large Language Models in Medical Research and Scientific Writing: A Closer Look to The Future\nHigh Yield Medical Reviews 8"
}