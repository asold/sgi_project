{
    "title": "Sources of Hallucination by Large Language Models on Inference Tasks",
    "url": "https://openalex.org/W4389519585",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5023404183",
            "name": "Nick McKenna",
            "affiliations": [
                "University of Edinburgh"
            ]
        },
        {
            "id": "https://openalex.org/A5100460612",
            "name": "Tianyi Li",
            "affiliations": [
                "University of Edinburgh"
            ]
        },
        {
            "id": "https://openalex.org/A5100384349",
            "name": "Liang Cheng",
            "affiliations": [
                "University of Edinburgh"
            ]
        },
        {
            "id": "https://openalex.org/A5101800671",
            "name": "Mohammad Javad Hosseini",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5034461489",
            "name": "Mark Johnson",
            "affiliations": [
                "Macquarie University"
            ]
        },
        {
            "id": "https://openalex.org/A5055023881",
            "name": "Mark Steedman",
            "affiliations": [
                "University of Edinburgh"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2966024226",
        "https://openalex.org/W1840435438",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2130158090",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W2962843521",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4285594979",
        "https://openalex.org/W4320858112",
        "https://openalex.org/W2165266976",
        "https://openalex.org/W3198599617",
        "https://openalex.org/W4281483318",
        "https://openalex.org/W4221159672",
        "https://openalex.org/W4309217888",
        "https://openalex.org/W4242626930",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W4392669945",
        "https://openalex.org/W2890894339",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3121904249",
        "https://openalex.org/W4385573586",
        "https://openalex.org/W3156587088",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W2970279348",
        "https://openalex.org/W2972987451",
        "https://openalex.org/W4385573401",
        "https://openalex.org/W4360836968",
        "https://openalex.org/W2509913944",
        "https://openalex.org/W2912924812",
        "https://openalex.org/W4385573954",
        "https://openalex.org/W2059799772",
        "https://openalex.org/W65111197",
        "https://openalex.org/W2406945108",
        "https://openalex.org/W2094728533"
    ],
    "abstract": "Large Language Models (LLMs) are claimed to be capable of Natural Language Inference (NLI), necessary for applied tasks like question answering and summarization. We present a series of behavioral studies on several LLM families (LLaMA, GPT-3.5, and PaLM) which probe their behavior using controlled experiments. We establish two biases originating from pretraining which predict much of their behavior, and show that these are major sources of hallucination in generative LLMs. First, memorization at the level of sentences: we show that, regardless of the premise, models falsely label NLI test samples as entailing when the hypothesis is attested in training data, and that entities are used as “indices’ to access the memorized data. Second, statistical patterns of usage learned at the level of corpora: we further show a similar effect when the premise predicate is less frequent than that of the hypothesis in the training data, a bias following from previous studies. We demonstrate that LLMs perform significantly worse on NLI test samples which do not conform to these biases than those which do, and we offer these as valuable controls for future LLM evaluation.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2758–2774\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nSources of Hallucination by Large Language Models on Inference Tasks\nNick McKenna†* Tianyi Li †*\nLiang Cheng† Mohammad Javad Hosseini‡ Mark Johnson§ Mark Steedman†\n†University of Edinburgh ‡Google Research §Macquarie University\n{nick.mckenna, tianyi.li}@ed.ac.uk\nAbstract\nLarge Language Models (LLMs) are claimed\nto be capable of Natural Language Inference\n(NLI), necessary for applied tasks like question\nanswering and summarization. We present a\nseries of behavioral studies on several LLM\nfamilies (LLaMA, GPT-3.5, and PaLM) which\nprobe their behavior using controlled exper-\niments. We establish two biases originating\nfrom pretraining which predict much of their\nbehavior, and show that these are major sources\nof hallucination in generative LLMs. First,\nmemorization at the level of sentences: we\nshow that, regardless of the premise, models\nfalsely label NLI test samples as entailing when\nthe hypothesis is attested in training data, and\nthat entities are used as “indices” to access the\nmemorized data. Second, statistical patterns\nof usage learned at the level of corpora: we\nfurther show a similar effect when the premise\npredicate is less frequent than that of the hy-\npothesis in the training data, a bias following\nfrom previous studies. We demonstrate that\nLLMs perform significantly worse on NLI test\nsamples which do not conform to these biases\nthan those which do, and we offer these as valu-\nable controls for future LLM evaluation.1\n1 Introduction\nLarge Language Models (LLMs) such as LLaMA,\nGPT-3/4, PaLM, and more (Touvron et al., 2023;\nBrown et al., 2020; Chowdhery et al., 2022), have\nbeen trusted by many to perform language un-\nderstanding in downstream tasks such as summa-\nrization, question answering, and fact verification,\namong others (Zhang et al., 2023). However, due\nto the large-scale nature of LLM training on vast,\noften proprietary data, and the inherent opacity\nof LLM parameters, it is difficult to explain their\n*Equal contribution.\n1Code and LLM outputs (LLaMA and GPT-3.5)\nare available at https://github.com/Teddy-Li/\nLLM-NLI-Analysis.\nbehavior when answering user queries and the cor-\nresponding risks in terms of bias and robustness.\nIn particular, one LLM behavior poses a signifi-\ncant challenge: “hallucination,” the phenomenon\nin which LLMs provide information which is in-\ncorrect or inappropriate, presented as fact.\nThis paper investigates two biases driving LLM\nperformance in natural language inference, some-\ntimes called textual entailment. This is a basic com-\nponent of language understanding which is critical\nin applied tasks, and we offer these two biases as\nexplanations of general false positive hallucination\nin everyday use. We examine broader NLI, and\nespecially directional entailments, which hold in\none direction, but not both. For example, DEFEAT\nentails PLAY but PLAY does not entail DEFEAT . In-\nferring directional entailment is more difficult than\nthat of symmetric paraphrase, so it more deeply\nprobes understanding.\nOur approach is a behavioral study of prompted\nLLM decision-making. We alter existing NLI\ndatasets in targeted ways while measuring how pre-\ndictions change, across several major LLM families\n(LLaMA, GPT-3.5, and PaLM). We demonstrate\ntwo sources of LLM performance on the NLI task,\nwhich we offer as explanations of general false pos-\nitive hallucination: (1) LLM bias toward affirming\nentailment when the hypothesis may be attested in\nthe training text, including reliance on named entity\nidentifiers; and (2) a corpus-frequency bias, affirm-\ning entailment when the premise is less frequent\nthan the hypothesis.\nWe establish that these biases originate from\nthe LLM pretraining objective, in which statisti-\ncal modeling of the natural distribution of human-\ngenerated text leads to (at the level of sentences)\nmemorizing individual statements, and (at the\nlevel of corpora) learning typical patterns of us-\nage. Though they are superficially performant, our\nexperiments show that even powerful LLMs still\nuse unsatisfactory tools instead of robust reasoning.\n2758\nWe present three contributions, the demonstra-\ntions of both factors and an analysis of their impact:\n(1) In a prompting scenario, LLMs respond to\nentailment samples according to anattestation bias,\naffirming entailments more readily if the hypoth-\nesis is attested by the pretraining text. We find\nthat LLaMA-65B, GPT-3.5, and PaLM-540B are\nrespectively 1.9, 2.2, and 2.0 times more likely\nto wrongly predict Entail when the model al-\nready asserts the hypothesis is attested, compared\nto when not attested. Further, LLMs recall from\ntheir propositional memory using named entities as\nidentifying “indices,” even though they are irrele-\nvant to the logic of the predicate inference task.\n(2) LLMs also rely on a simple corpus-statistic\nbias using relative term-frequencies, especially\nwhen propositional memory is not available. The\nthree LLMs are 1.6, 1.8 and 2.0 times more likely to\nwrongly affirm entailments if the premise has lower\nterm frequency than the hypothesis, than when not.\n(3) For the NLI test samples consistent with\nthese factors, LLM scores are misleadingly high;\nfor NLI samples adversarial to them, LLM perfor-\nmance is severely degraded. We show that when\nlabels go against the attestation bias, LLMs can be\npoor or even near-random classifiers; for the rela-\ntive frequency bias, we similarly show a substantial\nperformance decrease across all LLMs.\n2 Related Work\nAddressing task robustness, Poliak et al. (2018)\nfound a range of NLI datasets contain artifacts\nwhich are learned by supervised models trained\non only sample hypotheses. In our work we design\na similar hypothesis-only test with LLMs, but we\nuse it to probe model memory without any training.\nBy conditioning on the attestation of hypotheses,\nwe show that LLMs are inherently sensitive to attes-\ntation, separate from the statistical idiosyncrasies\nof NLI datasets.2\nAdditionally, Talman and Chatzikyriakidis\n(2019) report generalization failure among many\nmodels supervised for NLI — models fail to gen-\neralize between NLI datasets, even if the task is\nformatted the same. On smaller Language Models\nsuch as RoBERTa (Liu et al., 2019; 355M params),\nLi et al. (2022) also observed a reliance on dataset\n2We speculate that a similar attestation effect could even\nbe present in the supervised models studied in Poliak et al.\n(2018), which could contribute to those models’ performance.\nWe leave the investigation of this to future work.\nartifacts when performing directional NLI on pred-\nicates. We now study the behavior of much larger\nLMs, which have demonstrated more robust perfor-\nmance across NLP tasks.\nRecent work has also explored LLM memoriza-\ntion and generalization. Carlini et al. (2023) estab-\nlish that LLMs are able to memorize more data than\nsmall LMs, whereas Tirumala et al. (2022) further\ndemonstrate that LLMs pay special attention early\nin training to numbers and nouns, which act as\nunique identifiers for individual training sentences.\nWe also show that memories used in language infer-\nence are tied to specific named entities. And while\nWeller et al. (2023) and Kandpal et al. (2022) find\nthat entity frequency in training data is correlated\nwith performance in factual recall about them, we\nfind that entity frequency is anti-correlated with\nhypothetical generalization performance (§6).\nBubeck et al. (2023) argue that GPT-4 under-\nstands language “beyond memorization”. We do\nnot disprove generalization, but we show that GPT-\n4 shows the same hallucinations in Appendix F.\n3 Experimental Design\nWe design behavioral experiments on LLMs by\nmodifying NLI datasets with rigorous controls. We\nobserve large behavior changes across three major\nLLM families due to propositional-memory effects\nin §5 and §6, and corpus frequency in §7. Finally,\nwe show the impact on real performance in §8.\n3.1 Two Biases in Inference Predictions\nWe claim that the pretraining objective to fit the\ndistribution of natural text leads to biases in LLM\ngenerations. We explore two such biases.\nThe Attestation Bias (Λ) is the over-reliance of\nan LLM on its propositional memory about a query\nstatement. We claim that when a statement is likely\nto be attested in some way by an LLM’s training\ndata, it is more likely to affirm it as a conclusion in\nNLI tasks, regardless of any premise it is presented\nwith. We measure the attestedness of a sample\nby prompting the LLM to ask if the hypothesis in\nquestion is true, false, or unknown. 3 Attestation\npredictions are denoted by Λ.\nA biased model will appear to perform well on\ndataset samples with entailment labels that happen\nto align with the bias. Table 1 shows examples\nfrom the Levy/Holt dev set.\n3Alternatively, LLM perplexity for a statement could be\nused; however, this is not always available, e.g. with GPT-3.\n2759\nDev Sample Query: [premise] ⇒ [hypothesis] Dataset Label Bias Prediction\nGeysers are common to New Zealand ⇒Geysers are found in New Zealand Entail Λ = hypothesis Attested\nGeysers are found in New Zealand ⇒Geysers are common to New Zealand No-Entail Λ = hypothesis Not-Attested\nWhiskey consists chiefly of alcohol ⇒Whiskey contains alcohol Entail Φ = f(consists chiefly of ) < f(contains)\nWhiskey contains alcohol ⇒Whiskey consists chiefly of alcohol No-Entail Φ = f(contains) > f(consists chiefly of )\nTable 1: Two pairs of samples are consistent with a respective bias. Model predictions made on the basis of the bias\nwill appear to predict the direction of entailment for each sample. f(·) maps a term to its corpus frequency.\nAs discussed in §2, we draw inspiration from the\nhypothesis-only baseline (Poliak et al., 2018), but\nour test only queries model memory about the hy-\npothesis without any training. We describe prompt\ngeneration in detail in §4.2, with an example in\nappendix Table 13.\nDasgupta et al. (2022) show a similar effect in\nLLMs on abstract reasoning tests, related to sen-\ntential content, and equate it to human tendencies.\nIn contrast, we examine the risks of propositional\nmemory on more realistic inference tasks.\nThe Relative Frequency Bias (Φ) is the use by\nLLMs of a simple rule for deciding entailment,\ncalculable from corpus statistics. Entailment is\naffirmed if, ignoring named entities, the eventuality\nin premise P is less frequent in training than that\nin hypothesis H.\nThis bias is reflected in natural text: it is known\nthat nouns follow a trend of becoming more specific\nas corpus-frequency decreases (Rosch et al., 1976;\nCaraballo and Charniak, 1999) and the same is\nobserved for predicates (McKenna et al., 2023).\nSince entailments may carry from a specific term\nto a more general one, e.g. SPRINT entails RUN,\nrelative frequency can often indicate direction of\nentailment. However, this is an artifact of natural\ntext and has no direct relationship with meaning.\nTest samples are labeled for agreement with this\nbias separately from models, with examples shown\nin Table 1. Since LLM pre-train corpora are imprac-\ntically large or proprietary, we instead use Google\nN-grams4 as a proxy of the natural distribution of\ntext, and thus the distributions of these corpora.\nWe average frequencies between the years 1950-\n2019, and compare between P and H. To acquire\nthe generic eventualities in each sample, we ex-\nclude any extracted entities and lemmatize predi-\ncate phrases; further, we reduce the effect of noise\nand sparsity by requiring a wide margin of differ-\nence between P and H frequency estimates. Fre-\nquency decisions are denoted by Φ.\n4https://books.google.com/ngrams\n3.2 Datasets\nLevy/Holt consists of premise-hypothesis pairs,\nwith a task formatted: “Given [premise P], is it\ntrue that [hypothesis H]?” (Levy and Dagan, 2016;\nHolt, 2019). Each P- and H-statement has the\nproperty of containing one predicate with two en-\ntity arguments, (where the same entities appear in\nboth P and H) as shown in Table 2. This targeted\ndataset is ideal for precisely measuring model un-\nderstanding of predicates, because entailment be-\ntween statements is decidable purely on the basis\nof the predicates and their attributes. We study the\nchallenging directional subset, where entailments\nhold in one direction but not both.\nRTE-1 is one of the original and most difficult\ntests of NLI (Dagan et al., 2006). It is not purely\ndirectional on the basis of predicates or consistently\nstructured like Levy/Holt, so we leave it out of the\nbehavioral experiments. However, it is a widely\nunderstood dataset, and we use it to demonstrate\nthe impact of the two biases in general NLI in §8.\nExclusions are made of NLI datasets relating\nto knowledge of the world, since we aim to test\nLLMs on their capability to reason purely about\nthe semantics of natural language predicates with-\nout relying on memorized facts. We explicitly\navoid datasets such as MMLU (Hendrycks et al.,\n2021), Natural Questions (Kwiatkowski et al.,\n2019), OpenBookQA (Mihaylov et al., 2018) etc.\n3.3 Dataset Transformations\nThe Standard Inference Task (I) is on original\nNLI datasets, in which entailment is determinable\nby using general language inference on sentences.\nIn Levy/Holt, it is determinable just by predicates.\nWe define three dataset transformations to study\nthe change in model responses as targeted informa-\ntion is removed. These are the randomized premise\npredicate setting IRandPrem , and two argument\ntransformations: generic arguments IGenArg, and\ntype-constrained randomized arguments IRandArg.\n2760\nTask Label Dev Sample Query: [premise] ⇒ [hypothesis]\nI Entail George Bush was the Governor ofwas the Governor ofwas the Governor ofwas the Governor ofwas the Governor ofwas the Governor ofwas the Governor ofwas the Governor ofwas the Governor ofwas the Governor ofwas the Governor ofwas the Governor ofwas the Governor ofwas the Governor ofwas the Governor ofwas the Governor ofwas the Governor of Texas⇒ George Bush is a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician from Texas\nIRandPrem No-Entail George Bush resided inresided inresided inresided inresided inresided inresided inresided inresided inresided inresided inresided inresided inresided inresided inresided inresided in Texas ⇒ George Bush is a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician fromis a politician from Texas\nTable 2: From the original dataset task ( I) we derive the Random Premise task ( IRandPrem ), respecting type-\nconstraints. A random premise is highly unlikely to entail the hypothesis, so samples are relabeled No-Entail.\nTask Label Dev Sample Query: [premise] ⇒ [hypothesis]\nI Entail IndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndia exports tons of ricericericericericericericericericericericericericericericericerice ⇒ IndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndiaIndia exports ricericericericericericericericericericericericericericericericerice\nIGenArg Entail location Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation X exports tons of food Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Y ⇒ location Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation Xlocation X exports food Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Yfood Y\nIRandArg↓ Entail SloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijk exports tons of oatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookies ⇒ SloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijkSloterdijk exports oatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookiesoatmeal cookies\nIRandArg↑ Entail HelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinki exports tons of Granny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny Smith ⇒ HelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinkiHelsinki exports Granny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny SmithGranny Smith\nTable 3: An original dev sample (I) is transformed by insertion of entity types (IGenArg); by real entities sampled\nfrom the 5% least frequent in NewsCrawl (IRandArg↓); and also from the 5% most frequent (IRandArg↑).\nTransformations involve first identifying the\ntypes of entities in statements, in order to con-\nstrain entity or predicate replacements. We type\neach entity with one of the 48 FIGER types (Ling\nand Weld, 2012), such as “person,” “location,” etc.\nFirst, an entity linker (Nguyen et al., 2014) identi-\nfies the Freebase ID (Bollacker et al., 2008) for an\nentity, from which we then obtain its FIGER type;\nwe assign a default type “thing” in failure cases.\nThe Random Premise Task ( IRandPrem ) re-\nplaces the original premise predicate with a ran-\ndom predicate, while maintaining the same entity\narguments. This manipulation produces a dataset in\nwhich all samples are labeled No-Entail, since\ntwo randomly paired predicates are very unlikely to\nbe related by entailment. Thus, positive decisions\nby the model are false positive hallucinations.5\nTo maintain naturalness and grammaticality, we\nconstrain a new predicate to have argument slots\nof the same types as the original premise. For ex-\nample, “[medicine] is indicated for patients with\n[disease]” is swapped for “[medicine] does not\ncure [disease]”. We source candidates from dev\nset premises satisfying the target type-constraints,\nand sample uniform randomly. We map the original\nentities to their respective slots in the new premise.\nExamples are shown in Table 2. IRandPrem is a\ngood test of model reliance on propositional mem-\nory, since we prevent entailments while maintain-\ning the attestedness of conclusions (hypotheses).\nThe Generic Argument Task ( IGenArg) re-\nplaces original entities with unique FIGER-typed\n5We manually inspected the generated random premise\nentries for the Levy/Holt dataset to verify this: we found\n86.6% of entries are successfully non-entailing, 3.8% unde-\ncided cases, and only 9.6% are unintended true entailments.\nidentifiers, e.g. “location X” and “food Y .” By\nmasking the identities of entities, this test is de-\nsigned to remove entity information while main-\ntaining the same entailment label, as a baseline\ncontrol setting. We append unique identifiers (e.g.\n“X,” “Y”) to allow tracking of entity slots across\nthe premise and the hypothesis.\nThe Random Argument Task ( IRandArg) re-\nplaces original entities with other real, random en-\ntities of the same FIGER-type. Like IGenArg, this\ntest is designed to create novel strings by modifying\nstatements without changing entailment labels. But\nnow we test model sensitivity to added extraneous\ninformation. Examples are shown in Table 3.\nWe use entity type constraints here to ensure\npolysemous predicates maintain the same sense.\nFor example, a different sense of run is used\nin “[person] runs [organization]” vs. “[person]\nruns [software]”, but between different entities of\nthe same type, the same senses are used, so the\nexact entity IDs do not affect entailment labels\n(Yarowsky, 1993). We source new entities from\nNewsCrawl (Barrault et al., 2019), a decade-long\nspan of multi-source news text, in which entities\nare typed as above. We sample new entities uni-\nform randomly from the 5% least common entities\nin NewsCrawl (IRandArg↓), and the 5% most com-\nmon (IRandArg↑). We insert the sampled entities\nwhile preserving the rest of each statement.\n4 Querying Models with Prompts\n4.1 Models\nLLaMA is a recent LLM model family which\nrivals or surpasses GPT-3 performance while being\nopen to scientific study. A range of model sizes\n2761\nare provided, and we test the largest LLaMA-65B\nmodel. LLaMA is not fine-tuned. In preliminary\nexperiments on the Levy/Holt dataset, we found\ntwo popular fine-tuned LLaMA variants, Alpaca\n(Taori et al., 2023) and Vicuna (Chiang et al., 2023),\nperform similarly to LLaMA base models and un-\nderperform LLaMA-65B, so we leave them out of\nfurther experiments.\nGPT-3 Series models are closed to deep scien-\ntific review (Brown et al., 2020), though they are a\nwidely-used comparison for their performance, and\nhave been reasonably well-studied. We evaluate on\ntext-davinci-003 (GPT-3.5), as it is the largest, and\nhas undergone instruction- and RLHF-finetuning,\nenabling interesting comparisons.\nPaLM is larger than GPT-3, which often claims\nstate-of-the-art on evaluation datasets. We use the\nlargest PaLM-540B base model, which is also only\npretrained, so it serves as a further comparison\npoint to LLaMA.\nLater GPT models (like text-davinci-003 in our\nexperiments) have been pre-trained and fine-tuned,\nwhile base LLaMA and PaLM have only under-\ngone pre-training, so their contrast indicates what\nstage of training is responsible for the phenomena\nwe study. Our aim is not to judge which LLM\nis superior, but to show the common sources of\nhallucination they share.\nWe also omit models superseded in performance\nby LLaMA (e.g. OPT, GPT-J, etc.), as well as\nproducts that are closed to scientific review (e.g.\nGPT-4, Bard, etc.)6.\n4.2 Prompt Design and Evaluation\nFormatting of test samples is done by inserting\nthe premise and hypothesis into a prompt template,\nwhich is used to query the model in natural lan-\nguage. Following this, we append a three-way an-\nswer choice: A) Entailment, B) Neutral, C) Contra-\ndiction, following the typical format in NLI (Bow-\nman et al., 2015).\nSelection of the prompt template used in test is\ndecided by the highest AUC obtained on the respec-\ntive dev set. We try 8 promising templates includ-\ning 5 from Schmitt and Schütze (2021), also used\nin other NLI work7 (Webson and Pavlick, 2022).\n6We include an analysis of GPT-4 in Appendix F.\n7See Appendix A for details on prompt template selection.\nIdeally, an LLM with advanced language under-\nstanding ability could perform inference in zero-\nshot without annotated examples, which would\nraise confidence that this faculty is ready for down-\nstream tasks. To this end, we examine each LLM in\nzero-shot (detailed in Appendix A), but they exhibit\nseverely degraded, even near-random performance.\nWe turn to few-shot, and hand-annotate a mini-\nmal 4 examples in the style of the template, with\nadded explanations about why the given answer\nis correct for each example. These examples are\nprepended before the query (see Appendix A for\nan example). Our goal is to study model behavior\nas conditions change, not to maximize the score on\nany particular dataset. Therefore, we use a mini-\nmal 4-example setup, which we find is capable of\nevoking positive responses from all three LLMs on\neach dev set, across most templates.\nScoring is done by converting choice A into\nEntail and collapsing both B and C choices into\nNo-Entail to align with Levy/Holt and RTE-1\nannotation. For behavioral experiments in §5, §6,\nand §7, we score the model solely based on its tex-\ntual response. All models successfully choose one\nof A/B/C on all dev questions, showing compatibil-\nity with the QA format.\nFor the analysis in §8 which measures model per-\nformance across confidence thresholds, we convert\nthe letter choice to a probability with the mapping:\nSent = 0.5 + 0.5 ∗I[tok = A] ∗Stok\n−0.5 ∗I[tok ∈{B, C}] ∗Stok\nWhere I is the indicator function, and Sent esti-\nmates the probability of Entail from a textual\noutput (0 ≤Sent ≤1) with token probability Stok.\nThe linear transformation preserves the ordering of\nmodel confidences, which is sufficient for calculat-\ning a precision-recall curve.\n5 Experiment 1: Attestation Bias\nWe begin our experiments by assessing LLMs’ re-\nliance on their propositional memory of training\ntext by conditioning each model’s entailment task\npredictions I on its own predictions of attestation\nΛ. We do this by comparing estimated probabilities\nof predicting Entail conditioned on whether the\nhypothesis is predicted Attested or not.\nFurther, we test a setting which controls for the\npossibility that original Levy/Holt entailments may\ncoincidentally refer to attested facts, which could\n2762\nFigure 1: Exp-1. Estimated probability of predicting\nEntail for original entries in Levy/Holt, conditioned\non LLMs’ attestation of hypotheses (Λ). This setting is\nintuitive but may be subject to spurious correlations.\nFigure 2: Exp-1. Estimated probability of predicting\nEntail for Random-Premise entries in Levy/Holt,\nconditioned on LLMs’ attestation of hypotheses ( Λ).\nNow, predicting Entail is false positive hallucination\n(lower is better). Models are sensitive to attestation, and\nhallucinate more when the hypothesis is attested.\nlead to spurious correlation between inference and\nattestation scores without clearly demonstrating use\nof memory versus true entailment. This controlled\nsetting is the random premise task IRandPrem ,\nwhich converts entailments into non-entailments\nwithout altering the hypothesis. An ideal model\ncapable of drawing inferences from information in\ncontext should detect that in the IRandPrem task it\nis no longer possible to infer the hypothesis based\non the premise (even if the hypothesis is itself\nattested in training), and never predict Entail.\nThus, in IRandPrem , all Entail predictions are\nassumed to be false positive hallucinations.\n5.1 Results\nWith I, IRandPrem and Λ predictions acquired as\ndescribed in §3.1, we present the conditional proba-\nbilities in Figures 1 and 2. It is clear that a model’s\nmemory about the hypothesis plays a part in its pre-\ndictions of the hypothesis given a premise, either\nrelated or random.\nFor I, we observe significantly higher proba-\nbility of predicting Entail when the hypothe-\nsis is Attested. In the random premise task\nIRandPrem , this trend continues. LLaMA, GPT-\n3.5, and PaLM, respectively, show a 1.9x, 2.2x,\nand 2.0x higher chance of falsely predicting that a\nrandom premise Entails the hypothesis if it al-\nready predicts the hypothesis is Attested. This\nfalse positive hallucination and its impact on NLI\nperformance is investigated further in §8.\nThis behavior is observed across model families\n(LLaMA, GPT, and PaLM), establishing that it is\ndue to pretraining rather than Instruction-tuning\nor RLHF, since LLaMA and PaLM have only un-\ndergone pretraining. This behavior is undesirable,\nbecause model predictions on NLI tasks should be\nbased solely on general language understanding,\nnot prior knowledge. We may conclude that mem-\nory of training data is a significant contributor in\nLLM inference, and may be an important source of\nhallucination.\n5.2 Implications for Real Applications\nUsing prior knowledge as part of language infer-\nence has bad implications for the use of LLMs in\nreal applications. We offer an example scenario\nof a question-answering task where user questions\nare answered from a Knowledge Base (KB). In\ntypical formulations of this task, if a statement in\nthe KB (premise) entails a user query (hypothe-\nsis), the premise may be formulated into an answer.\nConsider a KB such as a legal document or HR rule-\nbook. Assume that the text is prepended to the user\nquery and presented to the LLM, as in other works\n(Srinivasan et al., 2022). Given our findings, we\nmight observe the LLM hallucinating answers to\nquestions using information which is not presented\nin the KB, but may have been read by the LLM in\ntext from other sources during pretraining. These\nanswers could be illogical, contradictory, and could\nmisrepresent the views of the KB, or other harms.\nSuch poor use of in-context learning has already\nbeen observed in specific domains like medicine\n(Jimenez Gutierrez et al., 2022).\nIn general, this is a risk for LLMs which (a) are\ndeployed for tasks like QA by feeding novel text\n(e.g. a legal document) in-context as part of the\nuser query, and (b) are trained on datasets which are\n2763\nLevy/Holt (Directional)\nModel Task Precision Recall ∆-Recall\nLLaMA\nI 67.0 68.4 0\nIGenArg 69.0 66.9 -1.5\nIRandArg↓ 64.0 63.8 -4.6\nIRandArg↑ 67.2 53.7 -14.7\nGPT-3.5\nI 62.4 92.3 0\nIGenArg 65.1 75.7 -16.6\nIRandArg↓ 65.5 66.5 -25.8\nIRandArg↑ 68.8 55.3 -37.0\nPaLM\nI 72.8 76.2 0\nIGenArg 79.8 50.8 -25.4\nIRandArg↓ 69.5 58.7 -17.5\nIRandArg↑ 70.8 52.4 -23.8\nTable 4: Exp-2. Scoring model outputs in different\nargument-replacement tasks. We indicate the highest\nand lowest recall score across replacement settings. Re-\ncall decreases sharply across settings in all models.\nprivate or otherwise infeasibly large to read man-\nually, containing many facts and human opinions\nunknowable to both the user and modeler.\n6 Experiment 2:\nEntities are Indices to Memory\nIn §5, we have established that propositional mem-\nory explains a significant portion of false positives\nin LLM inference predictions. In this section, we\ncontinue by showing the importance of named enti-\nties in the process of LLMs’ memory recall.\nAs described in §3.3, we manipulate the enti-\nties with the IGenArg generic argument replace-\nment, and two random entity replacements, one\nwith infrequent-entities IRandArg↓ and one with\nfrequent-entities IRandArg↑(examples in Table 3).\nBy replacing arguments constrained by type, en-\ntailment labels are maintained; however, new sam-\nples should contain novel strings not attested in pre-\ntrain corpora. We expect that an ideal, generalizing\nmodel would maintain its predictions across all\nconditions; a flawed model utilizing the attestation\nbias would predict fewer Entail, since entities\nno longer identify these statements in training.\n6.1 Results\nWe report results across conditions in Table 4. We\nobserve two phenomena across all three models,\naligning with the above conjecture of “flaws.”\nFirst, we observe that all models’ behavior signif-\nicantly changes in the same way when original en-\ntities are replaced by either entity types or random\nreal entities. Despite similar (or marginally increas-\ning) precision across conditions, recall degrades\nsharply from original entities (I) (GPT-3.5 @92.3)\nto random frequent entities (IRandArg↑) (GPT-3.5\n@55.3). Generic-argument IGenArg performance\nalso degrades in this way, showing that this is not a\nmatter of poorly selected real entities, but rather a\nloss of information from the original dataset which\nmodels were using to answer questions.\nSecond, across the 3 models, we observe a sig-\nnificant difference in recall between the two real\nentity conditions IRandArg↓and IRandArg↑, which\nare both composed of unattested statements, but in-\nvolve entities that differ in typical corpus frequency.\nInfrequent entities (IRandArg↓) yield better gener-\nalization and a higher recall (GPT-3.5 @66.5) than\nfrequent entities (IRandArg↑) (GPT-3.5 @55.3).\nThese findings corroborate those from §5, that\nLLMs use memory as part of language inference,\nand additionally show that these memories are re-\ncalled using named entities acting as indices. These\nexperiments demonstrate that too much prior expo-\nsure to an entity may impede model generalization\nwhen that entity is discussed in novel inferences:\nthe more a model has read about an entity during\npretraining, the less capable it is of drawing novel\nnatural language inferences involving it. This is\nthe case even though the inferences do not require\ndetailed knowledge of the entity.\nLike §5, the effect is consistent across models,\nindicating LLM pretraining is responsible.\nWe show similar results on RTE-1 in Appendix\nB. Further, instructing LLMs to ignore proposi-\ntional memory in Appendix C shows little change.\n7 Experiment 3: Relative Frequency Bias\nWe continue the conditioning experiments from §5,\nnow exploring the relative frequency bias. Sam-\nple labels for this bias are denoted by the model-\nagnostic Φ as described in §3.1. Φ labels the con-\nformance of sample predicates to the bias: Φ<\nmeans P is less corpus-frequent than H by a mar-\ngin (positive class), Φ> means P more frequent\nthan H by the margin (negative class). To control\nfor differences between datasets, the margin is set\nso that 1/3 of samples are classed as “roughly equal”\n(Φ≈), which we discard.\nFollowing the observations in §6, we further ap-\nply a generic-argument transformation to control\nfor attestation, yielding IGenArg\nRandPrem . With the en-\ntities masked, models cannot recall propositional\nmemory for this task: by re-calculating the Λ mea-\n2764\nFigure 3: Exp-3. Estimated probability of predicting\nEntail for random-premise Levy/Holt conditioned\non relative frequencies (Φ), with original (IRandPrem )\nor generic ( IGenArg\nRandPrem ) entities. Predicting Entail\nis false positive hallucination (lower is better). Models\nhallucinate more often when test samples conform to\nthe relative frequency bias (Φ<) than when not (Φ>).\nsure with generic arguments, only 2 hypotheses are\nstill predicted as Attested by GPT-3.5, whereas\nfor LLaMA and PaLM, the numbers are also only\n6.2% and 3.9%. Additionally, as with IRandPrem ,\nhere the entailment label of each sample remains\nNo-Entail, so any Entail prediction is false\npositive hallucination.\n7.1 Results\nWe estimate the probabilities of models predict-\ning Entail conditioned on the frequency label Φ,\nbetween IRandPrem and IGenArg\nRandPrem settings, and\npresent the results in Figure 3. We observe a clear\nand consistent rise of hallucination when samples\nconform to the bias. Namely, in case ofΦ<, models\nare more likely to predict Entail, even though\nno semantic relation exists between P and H.\nBetween the two settings, withIRandPrem , when\nentities are available, this effect is moderate. On\nthe other hand, with IGenArg\nRandPrem when entity-based\nmemory is blocked, we observe a decrease in the\noverall level of hallucination, but the separation be-\ntween Φ< and Φ> becomes more drastic, to 1.6x,\n1.8x and 2.0x for LLaMA, GPT-3.5 and PaLM\nrespectively. This indicates a tension between Λ\nand Φ: propositional memory may be used when\navailable, and if not, the predicate pairing may be\nattended to more closely. Again, the Φ effect is\nobserved across the three model families, reveal-\ning its root in the large-scale pre-training process,\nrather than model peculiarities or fine-tuning.\n8 Impact of Bias on Performance\nWe have demonstrated two sources of hallucination\nby LLMs on inference tasks. We now assess their\nimpact on model performance to quantify their risk.\nWe compare LLMs’ performance between NLI\nsubsets that are consistent or adversarial to each\nbias. A sample P ⊨ H? is consistent with a bias\nwhen the prediction by the bias agrees with the\ngold entailment label; conversely, it is adversarial\nto a bias when the prediction by the bias disagrees\nwith the label.\nFor example, “Google bought YouTube ⊨\nGoogle owns YouTube” isconsistent with the attes-\ntation bias of every model, because the conclusion\nGoogle owns YouTubeis attested in every LLM’s\ntraining data, and the sample label is Entail;\n“Apple owns Samsung ⊭ Apple bought Samsung”\nis also consistent, because its conclusion is not at-\ntested and the sample label is No-Entail. The\nreverses of these two samples areadversarial, since\ntheir respective attestedness (unchanged) does not\nagree with the entailment labels (now flipped). For\neach subset, there is substantial representation in\nboth Levy/Holt and RTE-1 (see appendix Table 9).\nWhile earlier experiments inspected model tex-\ntual responses to characterize behavior change,\nwe now use area under the precision-recall curve\n(AUC) to summarize model performance over a\ntunable confidence threshold (scoring described in\n§4.2), which is better for measuring practical dis-\ncriminative power. Following Li et al. (2022), we\nre-scale AUC values to normalize over the label\ndistribution, yielding AUCnorm values that assign\nrandom classifiers 0% and perfect classifiers 100%.\nWe report results in Table 5. Under the stan-\ndard inference task I, the performance drop from\nΛCONSISTENT to ΛADVERSARIAL is severe for all 3\nLLMs: they deteriorate from very good classi-\nfiers to poor or even near-random ones. 8 This\nfragility from the attestation bias can be alleviated\nby masking entities with type-identifiers (condition\nIGenArg), which reduces the performance drop.\nOn the other hand, with the generic arguments\nin IGenArg, LLMs are forced to focus on the predi-\ncates in each proposition. As a result, the impact\nof the relative frequency bias is intensified. From\nthe standard inference task I to IGenArg, the av-\nerage performance drop from the cons. to adv.\n8We note Λ predictions could possibly be influenced by\nmodel-specific idiosyncrasies in prompt format. We provide\nan analysis in Appendix E, where we find no significant effect.\n2765\nLevy/Holt RTE-1\nAttestation (Λ) Rel. Frequency ( Φ) Attestation ( Λ) Rel. Frequency ( Φ)\nModel Task cons. adv. diff. cons. adv. diff. cons. adv. diff. cons. adv. diff.\nLLaMA I 65.5 8.1 -57.4 42.1 32.3 -9.8 62.1 37.4 -24.7 55.5 51.7 -3.8\nGPT-3.5 I 85.0 10.8 -74.2 53.5 43.2 -10.3 84.6 47.5 -37.1 77.6 43.4 -34.2\nPaLM I 79.1 31.5 -47.6 63.3 53.0 -10.3 87.1 83.4 -3.7 87.5 81.0 -6.5\nLLaMA IGenArg 52.1 34.4 -17.7 55.3 34.9 -20.4 59.2 30.4 -28.8 51.7 39.4 -12.3\nGPT-3.5 IGenArg 67.1 18.8 -48.3 50.4 35.0 -15.4 80.1 56.4 -23.7 79.6 49.1 -30.5\nPaLM IGenArg 58.1 46.6 -11.5 59.9 47.3 -12.6 78.1 84.4 +6.3 85.4 78.7 -6.7\nTable 5: LLM performance on subsets where Λ/Φ is consistent/adversarial to entailment labels, measured with\nAUCnorm (0% = random chance performance). Decrease from cons to adv subsets are shown in the diff. columns.\nsubsets w.r.t. Φ is widened from 10.1% to 16.1%\nfor Levy/Holt and from 14.8% to 16.5% for RTE-\n1. The differences for Φ-consistency subsets are\ngenerally narrower thanΛ-consistency subsets, pos-\nsibly because the relative frequencies require gen-\neralizing from instances, and may be more difficult\nto capture, and potentially because frequency mea-\nsures with Google N-gram are a crude estimate of\nthe actual frequencies in LLM pre-train corpora.\n9 Conclusion\nAcross several major LLM families and experimen-\ntal settings, we demonstrate two important biases\nin the performance of LLMs on natural language\ninference tasks, which may also manifest in applied\ntasks as hallucination. Contrary to claims of LLM\ngeneral reasoning capabilities, we show that much\nof this performance is achieved by (1) recall of rel-\nevant memorizations and (2) corpus-based biases\nlike term frequency. Since these factors are repro-\nduced in all models, we establish that they originate\nin LLM pre-training, and are not corrected during\nGPT-3.5 fine-tuning.\nWe conclude that LLMs, though powerful, use\nunsatisfactory tools for the basic tasks of language\nunderstanding and inference. We propose several\napproaches to control for these biases in evaluation,\nand ultimately conclude that further attention on\nalleviating these biases are needed, before LLMs\nmay be trusted to reason robustly about language.\nLimitations\nIn this paper, we have discussed two prominent\nsources of hallucination for LLMs in natural lan-\nguage inference tasks. We acknowledge that this is\nnot an exhaustive search of all the sources, where\nfurther exploration should be done in future work.\nWe also note that after controlling for the factors\ndiscussed in this paper, there remains residual, un-\nexplained performance on NLI tasks. This residual\nmight be due to other undiscovered biases or pos-\nsibly generalising inference capability. We leave\nfurther exploration of this residual to future work.\nAs discussed in Appendix A, we compared a\nrange of popular LLM prompting techniques and\nselected the most promising approach. We ac-\nknowledge that there could potentially be other\nnovel prompting techniques that could help the\nLLMs resist the influence of the biases discussed\nin this paper. We identify this as an open question\nand advocate for future research.\nEthical Considerations\nThis paper discusses two major sources of hallu-\ncination in LLM output when asked to perform\nnatural language inference, which we note is a ca-\npability required of many downstream tasks such\nas summarization, question answering, etc. We\nshow that users of LLMs may be subjected to faulty\njudgements if the content of their request overlaps\nwith data in pretraining. However, it is difficult to\nascertain for both a user or modeler exactly what\nis contained in pretraining data, or how this will\ninteract with a user’s query. Our proposed attes-\ntation query shows promise in detecting potential\noverlaps, but model responses in applications of\nthese cases are not explored. Further, the relative\nfrequency bias demonstrates a much more subtle\nproblem of corpus distribution that is naturally in-\nherent to model pretraining on human generated\ntext.\nIn light of these, the potential harms of LLM use\nfor drawing natural language inferences may in-\nclude: offering inaccurate or irrelevant information\nto a user’s query or contradiction of information\nprovided in-context with a user’s query.\n2766\nAcknowledgements\nThis research was supported by ERC Advanced\nFellowship GA 742137 SEMANTAX and the Uni-\nversity of Edinburgh Huawei Laboratory.\nReferences\nLoïc Barrault, Ond ˇrej Bojar, Marta R. Costa-jussà,\nChristian Federmann, Mark Fishel, Yvette Gra-\nham, Barry Haddow, Matthias Huck, Philipp Koehn,\nShervin Malmasi, Christof Monz, Mathias Müller,\nSantanu Pal, Matt Post, and Marcos Zampieri. 2019.\nFindings of the 2019 Conference on Machine Trans-\nlation (WMT19). In Proceedings of the Fourth Con-\nference on Machine Translation (Volume 2: Shared\nTask Papers, Day 1), pages 1–61, Florence, Italy. As-\nsociation for Computational Linguistics.\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim\nSturge, and Jamie Taylor. 2008. Freebase: A col-\nlaboratively created graph database for structuring\nhuman knowledge. In Proceedings of the 2008 ACM\nSIGMOD International Conference on Management\nof Data, SIGMOD ’08, page 1247–1250, New York,\nNY , USA. Association for Computing Machinery.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, Harsha Nori, Hamid Palangi, Marco Tulio\nRibeiro, and Yi Zhang. 2023. Sparks of Artificial\nGeneral Intelligence: Early experiments with GPT-4.\nArXiv:2303.12712 [cs].\nSharon A. Caraballo and Eugene Charniak. 1999. De-\ntermining the specificity of nouns from text. In 1999\nJoint SIGDAT Conference on Empirical Methods in\nNatural Language Processing and Very Large Cor-\npora.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski,\nKatherine Lee, Florian Tramer, and Chiyuan Zhang.\n2023. Quantifying Memorization Across Neural Lan-\nguage Models. ArXiv:2202.07646 [cs].\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2006. The pascal recognising textual entailment chal-\nlenge. In Machine Learning Challenges. Evaluating\nPredictive Uncertainty, Visual Object Classification,\nand Recognising Tectual Entailment, pages 177–190,\nBerlin, Heidelberg. Springer Berlin Heidelberg.\nIshita Dasgupta, Andrew K. Lampinen, Stephanie\nC. Y . Chan, Antonia Creswell, Dharshan Kumaran,\nJames L. McClelland, and Felix Hill. 2022. Lan-\nguage models show human-like content effects on\nreasoning. ArXiv:2207.07051 [cs].\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\nhardt. 2021. Measuring massive multitask language\nunderstanding. Proceedings of the International Con-\nference on Learning Representations (ICLR).\nXavier Holt. 2019. Probabilistic Models of Relational\nImplication. arXiv:1907.12048 [cs, stat] . ArXiv:\n1907.12048.\nBernal Jimenez Gutierrez, Nikolas McNeal, Clayton\nWashington, You Chen, Lang Li, Huan Sun, and\nYu Su. 2022. Thinking about GPT-3 in-context learn-\ning for biomedical IE? think again. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2022, pages 4497–4512, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\n2767\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric\nWallace, and Colin Raffel. 2022. Large language\nmodels struggle to learn long-tail knowledge.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey,\nJacob Devlin, Kenton Lee, Kristina N. Toutanova,\nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: a benchmark for question answering\nresearch. Transactions of the Association of Compu-\ntational Linguistics.\nOmer Levy and Ido Dagan. 2016. Annotating Rela-\ntion Inference in Context via Question Answering.\nIn Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n2: Short Papers), pages 249–255, Berlin, Germany.\nAssociation for Computational Linguistics.\nTianyi Li, Mohammad Javad Hosseini, Sabine Weber,\nand Mark Steedman. 2022. Language Models Are\nPoor Learners of Directional Inference. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2022, pages 903–921, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nXiao Ling and Daniel S. Weld. 2012. Fine-grained en-\ntity recognition. In Proceedings of the Twenty-Sixth\nAAAI Conference on Artificial Intelligence, AAAI’12,\npage 94–100. AAAI Press.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv:1907.11692 [cs] . ArXiv:\n1907.11692.\nNick McKenna, Tianyi Li, Mark Johnson, and Mark\nSteedman. 2023. Smoothing Entailment Graphs with\nLanguage Models. In Proceedings of the 3rd Confer-\nence of the Asia-Pacific Chapter of the Association\nfor Computational Linguistics and the 13th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing. Association for Computational Linguistics.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question answer-\ning. In Conference on Empirical Methods in Natural\nLanguage Processing.\nDat Ba Nguyen, Johannes Hoffart, Martin Theobald,\nand Gerhard Weikum. 2014. Aida-light: High-\nthroughput named-entity disambiguation. In LDOW.\nOpenAI. 2023. GPT-4 Technical Report.\nArXiv:2303.08774 [cs].\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. ArXiv:2203.02155 [cs].\nAdam Poliak, Jason Naradowsky, Aparajita Haldar,\nRachel Rudinger, and Benjamin Van Durme. 2018.\nHypothesis only baselines in natural language infer-\nence. In Proceedings of the Seventh Joint Confer-\nence on Lexical and Computational Semantics, pages\n180–191, New Orleans, Louisiana. Association for\nComputational Linguistics.\nEleanor Rosch, Carolyn B Mervis, Wayne D Gray,\nDavid M Johnson, and Penny Boyes-Braem. 1976.\nBasic objects in natural categories. Cognitive Psy-\nchology, 8(3):382–439.\nMartin Schmitt and Hinrich Schütze. 2021. Language\nModels for Lexical Inference in Context. In Proceed-\nings of the 16th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nMain Volume, pages 1267–1280, Online. Association\nfor Computational Linguistics.\nKrishna Srinivasan, Karthik Raman, Anupam Samanta,\nLingrui Liao, Luca Bertelli, and Michael Bendersky.\n2022. QUILL: Query intent with large language mod-\nels using retrieval augmentation and multi-stage dis-\ntillation. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing:\nIndustry Track, pages 492–501, Abu Dhabi, UAE.\nAssociation for Computational Linguistics.\nAarne Talman and Stergios Chatzikyriakidis. 2019.\nTesting the Generalization Power of Neural Network\nModels across NLI Benchmarks. In Proceedings of\nthe 2019 ACL Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages 85–\n94, Florence, Italy. Association for Computational\nLinguistics.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. 2023. Stan-\nford alpaca: An instruction-following llama\nmodel. https://github.com/tatsu-lab/\nstanford_alpaca.\nKushal Tirumala, Aram Markosyan, Luke Zettlemoyer,\nand Armen Aghajanyan. 2022. Memorization with-\nout overfitting: Analyzing the training dynamics of\nlarge language models. In Advances in Neural Infor-\nmation Processing Systems, volume 35, pages 38274–\n38290. Curran Associates, Inc.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nAlbert Webson and Ellie Pavlick. 2022. Do prompt-\nbased models really understand the meaning of their\n2768\nprompts? In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2300–2344, Seattle, United States.\nAssociation for Computational Linguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems,\nvolume 35, pages 24824–24837. Curran Associates,\nInc.\nOrion Weller, Marc Marone, Nathaniel Weir, Dawn\nLawrie, Daniel Khashabi, and Benjamin Van Durme.\n2023. \"according to ...\" prompting language models\nimproves quoting from pre-training data.\nDavid Yarowsky. 1993. One sense per collocation.\nIn Human Language Technology: Proceedings of\na Workshop Held at Plainsboro, New Jersey, March\n21-24, 1993.\nTianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang,\nKathleen McKeown, and Tatsunori B. Hashimoto.\n2023. Benchmarking large language models for news\nsummarization.\nA Prompt Format Selection\nIn prompt-based interactions with the LLMs, sev-\neral types of context information could be added\nto help models produce accurate and robust predic-\ntions. We attend to two design choices in prompt\nengineering: prompt templates and in-context ex-\namples.\nPrompt templates are known to have a direct\nand sometimes decisive impact on LLM behavior.\nAs such, we carefully select a range of clear and\nconcise templates as promising candidates. As\ndiscussed in §4.2, we run each template through\nthe dev sets of each dataset, and select the template\nwith the best discriminative power according to\nAUC scores (similarly to §8). The candidate set of\ntemplates includes 3 concise templates we wrote:\n1. If [PREMISE ], then [HYPOTHESIS ].\n2. [PREMISE ], so [HYPOTHESIS ].\n3. [PREMISE ] entails [HYPOTHESIS ].\nWe also considered the 5 prompt templates\nused in bias work on LMs for textual entailments\n(Schmitt and Schütze, 2021):\n4. [PREMISE ], which means that [HYPOTHESIS ].\n5. [HYPOTHESIS ], because [PREMISE ].\n6. It is not the case that [HYPOTHESIS ], let alone\nthat [PREMISE ].\n7. [HYPOTHESIS ]NEG , which means that\n[PREMISE ]NEG .\n8. [PREMISE ]NEG , because [HYPOTHESIS ]NEG .\nIn preliminary experiments with GPT-3.5, we ob-\nserved that LLMs are not responsive to the 3 contra-\npositive prompts from Schmitt and Schütze (2021)\n(colored gray), performing at random. We also\nobserved that prompt number 5 from Schmitt and\nSchütze (2021) also consistently underperforms the\nother 4 templates, so we use the remaining 4 tem-\nplates (namely, template no. 1, 2, 3, 4) as our final\ncandidate set.\nIn-Context Examples have been widely used for\ninteractions with LLMs since Brown et al. (2020).\nFurther, Wei et al. (2022) has demonstrated that\nincluding chain-of-thought explanation, namely\nstep-by-step explanations, in the in-context exam-\nples, helps LLMs perform reasoning tasks. On the\nother hand, Ouyang et al. (2022) has suggested\nthat instruction-tuned LLMs are also capable of\nperforming tasks in zero-shot, without exposure to\nany in-context examples.\nWe compared zero-shot and few-shot in our pre-\nliminary experiments with LLaMA and GPT-3.5 on\nLevy/Holt directional dev set. Following Touvron\net al. (2023), for zero-shot, we prepend a textual\ndescription of the task to each test sample; for few-\nshot, we prepend a minimal 4 examples with ex-\nplanations. Instantiated prompts in the two settings\nare demonstrated in Table 13. Here we report the\ndev set results with the best-performing templates.\nWe found that for the two pre-trained LLMs,\nnamely, LLaMA and PaLM, zero-shot performance\non the Levy/Holt directional dev set is near-random,\nat 56.6% and 61.5% AUC respectively (random is\n50%); with 4 in-context examples, the models be-\ngin to exhibit non-trivial behavior, with 65.0% and\n80.2% AUC , respectively. This is not surprising,\nsince pre-trained LLMs without instruction fine-\ntuning should not be expected to perform complex\ntasks zero-shot. For GPT-3.5, the performance is\nstill much lower in zero-shot, at 64.5%, compared\nto 74.6% in few-shot.\nAs discussed in §4.2, ideally we would like\nLLMs to have zero-shot natural language abili-\nties readily available for downstream tasks. How-\never, in light of this observation, our primary ex-\nperiments are conducted in the few-shot setting\n2769\nModel Task Precision Recall ∆-Recall\nLLaMA\nI 74.5 52.5 0\nIGenArg 70.9 57.3 +4.8\nIRandArg↓ 66.9 60.5 +8.0\nIRandArg↑ 70.6 51.5 -1.0\nGPT-3.5\nI 80.6 96.5 0\nIGenArg 79.7 91.3 -5.2\nIRandArg↓ 80.1 82.5 -14.0\nIRandArg↑ 81.9 80.5 -16.0\nPaLM\nI 90.3 84.0 0\nIGenArg 92.3 71.5 -12.5\nIRandArg↓ 87.8 82.5 -1.5\nIRandArg↑ 88.2 82.0 -2.0\nTable 6: Scoring model outputs in different conditions\nof RTE-1. We indicate the highest and lowest recall\nscore across replacement settings.\nthroughout, in order to better explore the abilities\nof these LLMs.\nB RTE-1 Results For Experiment 2:\nEntities are Indices to Memory\nThe RTE-1 dataset contains complex natural lan-\nguage statements with varied linguistic features,\nso predictions about entailment are not decidable\nonly on the basis of contained predicates. However,\nRTE-1 is a difficult challenge set for models, and\ninteresting to compare to in the broader domain of\nNLI. Though the sentences are much more com-\nplex, we are able to conduct an analogous experi-\nment as in §6 by first identifying spans of named\nentities and their respective entity types, then re-\nplacing the entities with new ones. As before, we\ncompare model scores on the original dataset to\nthree test conditions: generic arguments (“location\nX”, “person Y”, etc.), sampled low-frequency en-\ntities constrained to the same type, and the same\nfor high-frequency sampled entities. Since only\nthe entities in each statement have been altered,\nthe entailment labels between premise/hypothesis\npairs remain unchanged, and an ideal model capa-\nble of generalizing inference would make the same\npredictions across dataset conditions. Results are\nshown in Table 6.\nWe observe similar trends to those reported on\nLevy/Holt. GPT-3.5 performs very consistently be-\ntween Levy/Holt and RTE-1 in terms of degrading\nrecall when information is changed in each sample.\nWe observe that model performance is worse than\nthe original dataset when using generic arguments,\nand worse still using type-constrained random ar-\nguments. We further observe that across all three\nLLMs across both datasets, models consistently\nachieve worse recall using high-frequency entities\nthan low-frequency entities, supporting the claim\nthat increasing the frequency of entity occurrence\nin training data impedes generalization.\nDifferent from in Levy/Holt, we observe some\nnoise in LLaMA’s predictions; the recall on the\noriginal task is actually lower than the generic argu-\nment condition and the low-frequency entity condi-\ntion. We note that overall, LLaMA is the weakest\nLLM tested in this experiment on both Levy/Holt\nand RTE-1, and that its performance on RTE-1 is\nparticularly low. We suggest that the increased dif-\nficulty of RTE-1 over Levy/Holt (due to having\nmuch more linguistic variation) is simply too com-\nplex for LLaMA, which is neither the largest LLM\ntested, nor instruction-finetuned.\nWe also observe a smaller gap between PaLM’s\nrecall rates across dataset conditions, though the\ngaps are consistent with our claims. While the\nmodel appears able to generalize to conditions in\nwhich random real arguments are inserted, recall\non the generic argument condition is significantly\ndegraded. Failure on this control condition indi-\ncates that the model may not be generalizing as\nwell as the other conditions would imply.\nC The Ineffectiveness of Instructing\nLLMs to Stop Conditioning on\nAttested Information\nIn §5 and §6, we showed that entailment predic-\ntions from LLMs are strongly biased by their pre-\ndictions on the attestation of hypotheses. We won-\ndered whether there are intuitive prompt engineer-\ning techniques to steer its behavior away from at-\ntending to attestation.\nTowards this goal, we experimented with\nprepending a brief task description to the few-shot\nprompts in part B of Table 13, explicitly instructing\nthe models to ignore the attestedness of individual\nstatements: Please check the entailments between\nthe following hypothetical statements. Ignore the\nveracity of these statements.\nWe replicated the experiments in §5 and §6 with\nGPT-3.5, since GPT-3.5 is an instruction-finetuned\nmodel trained to be responsive to prompts, where\nthe other two LLM families are only pre-trained.\nDespite having been instruction-finetuned, the re-\nsults with GPT-3.5 show only marginal improve-\nments in model behavior.\n2770\ntask GPT-3.5 Instructed to Ignore Attestedness Not Instructed\nI P (Entail | Attested) 74.3 77.6\nI P (Entail | ¬Attested) 57.8 63.6\nIRandPrem P(Entail | Attested) 39.0 41.3\nIRandPrem P(Entail | ¬Attested) 17.6 18.8\nTable 7: We estimate the probability of positive predictions of Entail in I and IRandPrem tasks respectively\ngiven that the hypothesis is attested, namely Λ = Attested. Not instructed results are copied from Figure 2 and\nlisted here for ease of comparison; also note that all IRandPrem = Entail predictions are false positives.\nLevy/Holt (Directional)\nGPT-3.5 Condition Task Precision Recall ∆-Recall\nFew-shot, instructed to ignore attestedness.\nI 64.9 90.8 0\nIGenArg 73.5 69.3 -21.5\nIRandArg↓ 64.6 68.4 -22.4\nIRandArg↑ 67.5 58.1 -32.7\nFew-shot, no instructions.\nI 62.4 92.3 0\nIGenArg 65.1 75.7 -16.6\nIRandArg↓ 65.5 66.5 -25.8\nIRandArg↑ 68.8 55.3 -37.0\nTable 8: GPT-3.5 predictions when models are explicitly instructed to avoid taking the attestedness of individual\nstatements into account. In the upper half are the instructed behavior, and in the lower half are the regular few-shot\nbehavior as in Table 4. Differences in recalls remain at a similar scale, with precision again stable, where the benefit\nfrom the explicit instruction is marginal.\nIn Table 7, we show that instructing GPT-3.5\nto ignore attestation does not help narrow the gap\nbetween Λ = Attested and Λ = ¬Attested;\ninstead, probabilities of predicting Entail went\ndown by similar amounts, indicating that the model\nis becoming slightly more conservative in predict-\ning positives when instructed to ignore attestation,\nbut not in a principled manner.\nFurther, as shown in Table 8, despite the ex-\nplicit instruction, recall still drops at similar scales\nwhen arguments are randomly replaced with the\nsame sets of frequent/infrequent replacement enti-\nties as before. Since GPT-3.5 has been instruction-\nfinetuned to respond to prompts, its failure means\neradicating such biases from model outputs is a\ndifficult task, one that needs further research atten-\ntion.\nD Statistics of Consistency Subsets\nThe statistics of consistency subsets are presented\nin Table 9.\nE The Reliability of Λ Measure and Its\nRelation to Consensus of Attestation\nThe Λ-consistency subsets most directly capture\nthe impacts of the attestation bias. However, these\nsubset separations are based on Λ predictions from\nindividual models, which can be noisy, subject to\nmodel-specific idiosyncracies such as trigger words\nor certain syntactic structures in the prompt, etc.\nTo verify that the performance gaps in Λ-\nconsistency subsets that we observe in §8 comes\nfrom predicted attestedness and not some idiosyn-\ncrasy, we experiment with another pair of subsets\nbased on consensus attestation instead of individu-\nally predicted attestation.\nWe use a majority vote among the three\nindependently-trained LLMs to approximate con-\nsensus attestation. The approximation is denoted\nas ˜Λ. This is because any model-specific idiosyn-\ncrasies should not be shared between LLMs in-\ndependently trained from different source corpora\nin general. Therefore, with the majority vote, we\nreduce this noise and acquire predictions on the\nconsensus attestation of statements.\nPerformances of LLMs between ˜Λ-consistency\nsubsets are listed in Table 10. Gaps between\nthe ˜Λ-consistency subsets that are larger than Λ-\nconsistency gaps are colored red; those narrower\nthan Λ-consistency gaps are colored green. It is\nclear that the gaps are consistent between Λ/˜Λ-\nconsistency experiments, where the gaps are even\nlarger on many occasions. This confirms that the\n2771\n# of Entries Levy/Holt RTE-1\nLLaMA GPT-3.5 PaLM LLaMA GPT-3.5 PaLM\nVCONSISTENT 955 947 999 479 447 480\nVADVERSARIAL 829 837 785 321 353 320\nFCONSISTENT 972 286\nFADVERSARIAL 220 247\nTable 9: Subsets defined by the consistency between entailment label L and either Λ (hypothesis attestation\nprediction from each LLM) or Φ (model-agnostic relative frequency bias). CONSISTENT subsets are where L agrees\nwith Λ/Φ. A DVERSARIAL subsets are where L disagrees with Λ/Φ.\nLevy/Holt\nModel Task ˜Λcons. ˜Λadv. diff.\nLLaMA I 65.3 6.5 -58.8\nGPT-3.5 I 70.8 23.5 -47.3\nPaLM I 80.7 28.3 -52.4\nLLaMA IGenArg 54.4 29.6 -24.8\nGPT-3.5 IGenArg 56.2 35.5 -20.7\nPaLM IGenArg 59.3 40.1 -19.2\nTable 10: LLM performance on Levy/Holt subsets\nwhere Attestation ˜Λ is Consistent/Adversarial to the\nlabels, measured with AUCnorm (0% = random chance\nperformance). Performance drops from ˜Λcons to ˜Λadv\nare presented in the diff. columns, sharper decreases\nthan Λ-comparisons in Table 5 are colored red, milder\nones are colored green.\nperformance gaps in Λ-consistency experiments\ncan be credited to the attestation bias, rather than\nmodel-specific idiosyncrasies.\nIt is also to be noted that, since theΦ-consistency\nsubsets are separated based on the model-agnostic\ncriterion Φ, model-specific idiosyncrasies are not a\nproblem for Φ-consistency comparisons.\nF Impacts of Bias on GPT-4 Performance\nGPT-4 (OpenAI, 2023) is a recent, strong LLM\nclaiming SOTA performance on various NLP tasks.\nDue to its closed-source nature and the impossibil-\nity of fully tracking the sources of its behaviors, we\nrefrain from reporting results with it in the main\ncontent of this paper.\nHowever, in order to provide a richer con-\ntext for the attestation bias and the relative fre-\nquency bias, in this section we report the perfor-\nmance differences of GPT-4 between subsets con-\nsistent/adversarial to the two biases.\nAs a light-weight experiment, we elicit GPT-4\npredictions in the original I task in the zero-shot\nsetting, and re-use subsets from experiments in\n§8. Specifically, for the attestation bias, we use\nthe majority vote ˜Λ among LLaMA, GPT-3.5 and\nPaLM, to approximate Λ predictions from GPT-4\nitself; for the relative frequency bias, we keep the\nΦ measure for approximating corpus-frequency of\nterms.\nBecause GPT-4 is a commercial service and does\nnot provide logit confidence with their discrete pre-\ndictions, AUCnorm values could not be calculated.\nTherefore, we are forced to report the F-1 scores\nat the binary prediction point of confidence. As\nresults in Table 12 show, we observe the same trend\nas in §8: for the subset adversarial to each factor,\nGPT-4 performance also drops substantially.\nThis experiment is designed to provide more con-\ntext for the two biases discussed in the paper and\nNOT to compare GPT-4 with other models; how-\never, we can conclude that GPT-4 is subject to the\nsame fragilities as the other LLMs w.r.t. the two bi-\nases, where our conclusions and recommendations\nalso apply.\nG Dataset Statistics and Dev Set\nPerformance\nIn the paper, we have examined the behavior and\nperformance of three major LLM families on two\nNLI datasets: Levy/Holt and RTE-1.\nThe directional portion of Levy/Holt dataset 9\ncontains 630 entries in its dev set, and 1784 entries\nin its test set; the RTE-1 dataset 10 contains 567\nentries in its dev set, and 800 entries in its test\nset. Each dataset has a 50%/50% class distribution\nbetween Entail and No-Entail (for RTE-1\ndev set, the numbers of entries in the two label\nclasses differ by 1).\nIn Table 11, we report dev set performance and\nthe best prompt template used for each model on\neach dataset. Note that no training is involved in\n9https://github.com/mjhosseini/\nentgraph_eval/tree/master/LevyHoltDS\n10https://www.kaggle.com/datasets/\nnltkdata/rte-corpus?resource=download\n2772\nLevy/Holt RTE-1\nModel Task Best tplt. ID DEV set AUCnorm Best tplt. ID DEV set AUCnorm\nLLaMA\nI #4 30.0 #3 62.5\nIGenArg #1 34.6 #3 52.3\nIRandArg↓ #1 31.8 #1 51.3\nIRandArg↑ #1 26.3 #3 43.8\nGPT-3.5\nI #1 49.2 #3 74.8\nIGenArg #1 39.8 #3 64.8\nIRandArg↓ #1 43.4 #3 63.6\nIRandArg↑ #1 34.2 #3 66.0\nPaLM\nI #1 60.9 #4 84.5\nIGenArg #1 48.1 #4 79.4\nIRandArg↓ #1 43.6 #3 79.8\nIRandArg↑ #1 35.3 #3 78.3\nTable 11: LLM dev set performance on the two datasets, measured with AUCnorm (0% = random chance\nperformance). AUC is calculated using estimated model scores as in §4.2 and then normalized into AUCnorm. We\nselect the highest scoring template on each dev task (shown in this table) and use this in the corresponding test set\nevaluation (shown in the main text).\nF-1 score Task Levy/Holt\n˜ΛCons ˜ΛAdv\nrandom baseline I 70.3 62.0\nGPT-4 I 85.1 (+14.8) 67.6 (+5.6)\nΦCons ΦAdv\nrandom baseline I 66.7 66.7\nGPT-4 I 74.6 (+7.9) 69.7 (+3.0)\nTable 12: LLM performance on Levy/Holt subsets\nwhere Attestation ˜Λ is Consistent/Adversarial to the\nlabels, measured with F-1 score. random baseline is the\nhighest F-1 score from a random classifier, by reaching\nrandom precision and 100% recall. For each GPT-4\nscore, we also show the improvement over random (in\nparentheses).\nthis paper, and prompt template selection is the\nonly hyper-parameter tuned on the dev sets. These\nselected best prompt templates are then used on the\nrespective test sets, where the results are used for\nthe analysis throughout the paper.\nFor random-premise experiments, AUC values\ncannot be meaningfully calculated because gold\nlabels are always No-Entail. For these ex-\nperiments, we use the most frequently-selected\nprompt template on each dataset, namely template\n#1 for Levy/Holt dataset, and template #3 for RTE-\n1 dataset.\n2773\nA. Zero-shot Example Instantiated Prompt\nPlease check the entailments between the following statements.\nIf kanamycin kills infections, then kanamycin is useful in infections.\nA) Entailment\nB) Neutral\nC) Contradiction\nB. Few-shot Example Instantiated Prompt\nIf Google bought Youtube, then Google owns Youtube.\nA) Entailment\nB) Neutral\nC) Contradiction\nAnswer: A) Entailment. Owning is a consequence of buying.\nIf Google owns Youtube, then Google bought Youtube.\nA) Entailment\nB) Neutral\nC) Contradiction\nAnswer: B) Neutral. Owning does not imply buying, the ownership may come from other means.\nIf John went to the mall, then John drove to the mall.\nA) Entailment\nB) Neutral\nC) Contradiction\nAnswer: B) Neutral. John may have gone to the mall by other means.\nIf John drove to the mall, then John went to the mall.\nA) Entailment\nB) Neutral\nC) Contradiction\nAnswer: A) Entailment. Driving is a means of going to the mall.\nIf ephedrine is widely used in medicine, then ephedrine is used in medicine.\nA) Entailment\nB) Neutral\nC) Contradiction\nAnswer:\nC. Hypothesis-only Example Instantiated Prompt\nGoogle bought Youtube.\nA) True\nB) Unknown\nC) False\nAnswer: A) True.\nYoshua Bengio likes oak trees.\nA) True\nB) Unknown\nC) False\nAnswer: B) Unknown.\nThe sun rises from the west.\nA) True\nB) Unknown\nC) False\nAnswer: C) False.\nephedrine is used in medicine.\nA) True\nB) Unknown\nC) False\nAnswer:\nTable 13: Example instantiated prompts in Zero-shot / Few-shot settings, for the sample “ PREMISE : [ephedrine\nis widely used in medicine], HYPOTHESIS : [ephedrine is used in medicine]”. The few-shot prompts in part B are\nused throughout the main experiments in this paper. We also present an example of the prompts we use for the\nhypothesis-only Λ measure as described in §3.1.\n2774"
}