{
    "title": "Cross-Domain NER using Cross-Domain Language Modeling",
    "url": "https://openalex.org/W2949759300",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A2098387093",
            "name": "Chen Jia",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2123592475",
            "name": "Xiaobo Liang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2098449489",
            "name": "Yue Zhang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2963641259",
        "https://openalex.org/W2743028754",
        "https://openalex.org/W2963040309",
        "https://openalex.org/W2042188227",
        "https://openalex.org/W2592170186",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2963625095",
        "https://openalex.org/W2593538687",
        "https://openalex.org/W2117130368",
        "https://openalex.org/W2952087486",
        "https://openalex.org/W2158899491",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2964352358",
        "https://openalex.org/W2963024368",
        "https://openalex.org/W2962902328",
        "https://openalex.org/W2963682821",
        "https://openalex.org/W2804777280",
        "https://openalex.org/W2769084352",
        "https://openalex.org/W1731081199",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2952230511",
        "https://openalex.org/W2890494294",
        "https://openalex.org/W1994913251",
        "https://openalex.org/W2963706742",
        "https://openalex.org/W2950938254",
        "https://openalex.org/W2252070015",
        "https://openalex.org/W2296283641",
        "https://openalex.org/W2144578941",
        "https://openalex.org/W2615487675",
        "https://openalex.org/W2888456631",
        "https://openalex.org/W2963186636",
        "https://openalex.org/W2808393080",
        "https://openalex.org/W2100664567",
        "https://openalex.org/W2964084097",
        "https://openalex.org/W2120354757",
        "https://openalex.org/W2964052092",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W2604986524"
    ],
    "abstract": "Due to limitation of labeled resources, cross-domain named entity recognition (NER) has been a challenging task. Most existing work considers a supervised setting, making use of labeled data for both the source and target domains. A disadvantage of such methods is that they cannot train for domains without NER data. To address this issue, we consider using cross-domain LM as a bridge cross-domains for NER domain adaptation, performing cross-domain and cross-task knowledge transfer by designing a novel parameter generation network. Results show that our method can effectively extract domain differences from cross-domain LM contrast, allowing unsupervised domain adaptation while also giving state-of-the-art results among supervised domain adaptation methods.",
    "full_text": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2464–2474\nFlorence, Italy, July 28 - August 2, 2019.c⃝2019 Association for Computational Linguistics\n2464\nCross-Domain NER using Cross-Domain Language Modeling\nChen Jia†‡, Xiaobo Liang3∗and Yue Zhang‡§\n†Fudan University, China\n‡School of Engineering, Westlake University, China\n3Natural Language Processing Lab, Northeastern University, China\n§Institute of Advanced Technology, Westlake Institute for Advanced Study\n{jiachen,zhangyue}@westlake.edu.cn, liangxiaobo0309@gmail.com\nAbstract\nDue to limitation of labeled resources, cross-\ndomain named entity recognition (NER) has\nbeen a challenging task. Most existing work\nconsiders a supervised setting, making use of\nlabeled data for both the source and target do-\nmains. A disadvantage of such methods is that\nthey cannot train for domains without NER\ndata. To address this issue, we consider using\ncross-domain LM as a bridge cross-domains\nfor NER domain adaptation, performing cross-\ndomain and cross-task knowledge transfer by\ndesigning a novel parameter generation net-\nwork. Results show that our method can effec-\ntively extract domain differences from cross-\ndomain LM contrast, allowing unsupervised\ndomain adaptation while also giving state-of-\nthe-art results among supervised domain adap-\ntation methods.\n1 Introduction\nNamed entity recognition (NER) is a fundamen-\ntal task in information extraction and text under-\nstanding. Due to large variations in entity names\nand ﬂexibility in entity mentions, NER has been a\nchallenging task in NLP. Cross-domain NER adds\nto the difﬁculty of modeling due to the difference\nin text genre and entity names. Existing meth-\nods make use of feature transfer (Daum´e III, 2009;\nKim et al., 2015; Obeidat et al., 2016; Wang et al.,\n2018) and parameters sharing (Lee et al., 2017;\nSachan et al., 2018; Yang et al., 2017; Lin and Lu,\n2018) for supervised NER domain adaptation.\nLanguage modeling (LM) has been shown use-\nful for NER, both via multi-task learning (Rei,\n2017) and via pre-training (Peters et al., 2018). In-\ntuitively, both noun entities and context patterns\ncan be captured during LM training, which ben-\neﬁts the recognition of named entities. A natu-\nral question that arises is whether cross-domain\n∗Work done when visiting Westlake University.\nW\nNews  Domain Target  Domain\nNER\nTask\nLM\nTask\nnersrc, \nner,tgt \nlm,tgt \nlmsrc, \nVertical Transfer\nVertical Transfer\nHorizontal Transfer\nT\nnerI\nT\nlmI\nD\nsrcI\nD\ntgtI\nFigure 1: Overview of the proposed model.\nLM training can beneﬁt cross-domain NER. Fig-\nure 1 shows one example, where there are rela-\ntively large training data in the news domain but no\ndata or a small amount of data in a target domain.\nWe are interested in transferring NER knowledge\nfrom the news domain to the target domain by con-\ntrasting large raw data in both domains through\ncross-domain LM training.\nNaive multi-task learning by parameter sharing\n(Collobert and Weston, 2008) does not work ef-\nfectively in this multi-task, multi-domain setting\ndue to potential conﬂict of information. To achieve\ncross-domain information transfer as shown in the\nred arrow, two types of connections must be made:\n(1) cross-task links between NER and LM (for ver-\ntical transfer) and (2) cross-domain links (for hor-\nizontal transfer). We investigate a novel parame-\nter generator network to this end, by decomposing\nthe parameters θ of the NER or LM task on the\nsource or target text domain into the combination\nθ= f(W,ID\nd ,IT\nt ) of a set of meta parametersW,\na task embedding vector IT\nt (t∈{ner,lm}) and a\ndomain embedding vector ID\nd (d∈{src,tgt}), so\nthat domain and task-correlations can be learned\nthrough similarities between the respective do-\nmain and task embedding vectors.\n2465\nIn Figure 1, the values of W, {IT\nt }, {ID\nd }and\nthe parameter generation network f(·,·,·) are all\ntrained in a multi-task learning process optimiz-\ning NER and LM training objectives. Through the\nprocess, connections between the sets of param-\neters θsrc,ner, θsrc,lm, θtgt,ner and θtgt,lm are de-\ncomposed into two dimensions and distilled into\ntwo task embedding vectors IT\nner, IT\nlm and two do-\nmain embedding vectors ID\nsrc, ID\ntgt, respectively.\nCompared with traditional multi-task learning, our\nmethod has a modular control over cross-domain\nand cross-task knowledge transfer. In addition, the\nfour embedding vectors IT\nner, IT\nlm, ID\nsrc and ID\ntgt\ncan also be trained by optimizing on only three\ndatasets for θsrc,ner, θsrc,lm and θtgt,lm, therefore\nachieving zero-shot NER learning on the target do-\nmain by deriving θtgt,ner automatically.\nResults on three different cross-domain datasets\nshow that our method outperforms naive multi-\ntask learning and a wide range of domain adap-\ntation methods. To our knowledge, we are the\nﬁrst to consider unsupervised domain adapta-\ntion for NER via cross-domain LM tasks and\nthe ﬁrst to work on NER transfer learning be-\ntween domains with completely different entity\ntypes (i.e. news vs. biomedical). We released\nour data and code at https://github.com/\njiachenwestlake/Cross-Domain_NER.\n2 Related Work\nNER. Recently, neural networks have been used\nfor NER and achieved state-of-the-art results.\nHammerton (2003) use a unidirectional LSTM\nwith a Softmax classifer. Collobert et al.\n(2011) use a CNN-CRF architecture. Santos and\nGuimar˜aes (2015) extend the model by using char-\nacter CNN. Most recent work uses LSTM-CRF\n(Lample et al., 2016; Ma and Hovy, 2016; Chiu\nand Nichols, 2016; Yang et al., 2018). We choose\nBiLSTM-CRF as our method since it gives state-\nof-the-art resutls on standard benchmarks.\nCross-domain NER. Most existing work on\ncross-domain NER investigates the supervised set-\nting, where both source and target domains have\nlabeled data. Daum ´e III (2009) maps entity la-\nbel space between the source and target domains.\nKim et al. (2015) and Obeidat et al. (2016) use la-\nbel embeddings instead of entities themselves as\nthe features for cross-domain transfer. Wang et al.\n(2018) perform label-aware feature representation\ntransfer based on text representation learned by\nBiLSTM networks.\nRecently, parameters transfer approaches have\nseen increasing popularity for cross-domain NER.\nSuch approaches ﬁrst initialize a target model with\nparameters learned from source-domain NER (Lee\net al., 2017) or LM (Sachan et al., 2018), and then\nﬁne-tune the model using labeled NER data from\nthe target domain. Yang et al. (2017) jointly train\nsource- and target-domain models with shared pa-\nrameters, Lin and Lu (2018) add adaptation layers\non top of existing networks. Except for Sachan\net al. (2018), all the above methods use cross-\ndomain NER data only. In contrast, we lever-\nage both NER data and raw data for both do-\nmains. In addition, our method can deal with a\nzero-shot learning setting for unsupervised NER\ndomain adaptation, which no existing work con-\nsiders.\nLearning task embedding vectors. There has\nbeen related work using task vector representa-\ntions for multi-task learning. Ammar et al. (2016)\nlearn language embeddings for multi-lingual pars-\ning. Stymne et al. (2018) learn treebank embed-\ndings for cross-annotation-style parsing. These\nmethods use “task” embeddings to augment word\nembedding inputs, distilling “task” characteris-\ntics into these vectors for preserving word em-\nbeddings. Liu et al. (2018) learn domain em-\nbeddings for multi-domain sentiment classiﬁca-\ntion. They combine domain vectors with domain-\nindependent representation of the input sentences\nto obtain a domain-speciﬁc input representation.\nA salient difference between our work and the\nmethods above is that we use domain and task em-\nbeddings to obtain domain and task-speciﬁc pa-\nrameters, rather than input representations.\nCloser in spirit to our work, Platanios et al.\n(2018) learn language vectors, using them to gen-\nerate parameters for multi-lingual machine trans-\nlation. While one of their main motivation is\nto save the parameter space when the number of\nlangauges grows, our main goal is to investigate\nthe modularization of transferable knowledge in\na cross-domain and cross-task setting. To our\nknowledge, we are the ﬁrst to study “task” embed-\ndings in a multi-dimensional parameter decompo-\nsition setting (e.g. domain + task).\n3 Methods\nThe overall structure of our proposed model is\nshown in Figure 2. The bottom shows the com-\n2466\nSource \nDomain\nLM\nTaskTarget \nDomain\nNER\nTask\nInput Texts \nWord Rep.\nLSTM Hidden\nT\nnerI\nD\nsrcI\nW\nnersrc,\nLSTM\n\n\nFigure 2: Model architecture.\nbination of two domains and two tasks. Given an\ninput sentence, word representations are ﬁrst cal-\nculated through a shared embedding layer (Sub-\nsection 3.1). Then a set of task- and domain-\nspeciﬁc BiLSTM parameters is calculated through\na novel parameter generation network (Subsection\n3.2), for encoding the input sequence. Finally, re-\nspective output layers are used for different tasks\nand domains (Subsection 3.3).\n3.1 Input Layer\nFollowing Yang et al. (2018), given an input x =\n[x1,x2,...,x n] from a source-domain NER train-\ning set Sner = {(xi,yi)}m\ni=1 or target-domain\nNER training set Tner = {(xi,yi)}n\ni=1, a source-\ndomain raw text set Slm = {(xi)}p\ni=1 or target-\ndomain raw text set Tlm = {(xi)}q\ni=1, each word\nxi is represented as the concatenation of its word\nembedding and the output of a character level\nCNN :\nvi = [ew(xi) ⊕CNN(ec(xi))], (1)\nwhere ew represents a shared word embedding\nlookup table and ec represents a shared charac-\nter embedding lookup table. CNN(·) represents\na standard CNN acting on a character embedding\nsequence ec(xi) of a word xi. ⊕represents vector\nconcatenation.\n3.2 Parameter Generation Network\nA bi-directional LSTM layer is applied to v =\n[v1,v2,..., vn].\nTo transfer knowledge across domains and\ntasks, we dynamically generate the parameters\nof BiLSTM using a Parameter Generation Net-\nwork (f(·,·,·)). The resulting parameters are de-\nnoted as θd,t\nLSTM, where d ∈ {src,tgt}and t ∈\n{ner,lm}represent domain label and task label,\nrespectively. More speciﬁcally:\nθd,t\nLSTM = W ⊗ID\nd ⊗IT\nt , (2)\nwhere W ∈RP(LSTM)×V ×U represents a set of\nmeta parameters in the form of a 3rd-order ten-\nsor and ID\nd ∈RU , IT\nt ∈RV represent domain\nembedding and task embedding, respectively. U,\nV represent domain and task embedding sizes, re-\nspectively. P(LSTM) is the number of BiLSTM pa-\nrameters. ⊗refers to tensor contraction.\nGiven the input v and the parameter θd,t\nLSTM, the\nhidden outputs of a task and domain-speciﬁc BiL-\nSTM unit can be uniformly written as:\n− →h d,t\ni = LSTM(− →h d,t\ni−1,vi,− →θd,t\nLSTM)\n← −h d,t\ni = LSTM(← −h d,t\ni+1,vi,← −θd,t\nLSTM),\n(3)\nfor the forward and backward directions, respec-\ntively.\n3.3 Output Layers\nNER. Standard CRFs (Ma and Hovy, 2016) are\nused as output layers for NER. Given h = [− →h 1 ⊕← −h 1,..., − →h n ⊕← −h n], the output probabilityp(y|x)\nover label sequence y = l1,l2,...,l i produced on\ninput sentence x is:\np(y|x)=\nexp{\n∑\ni(w\nli\nCRF ·hi+b\n(li−1,li)\nCRF )}\n∑\ny′exp{\n∑\ni(w\nl′\ni\nCRF ·hi+b\n(l′\ni−1,l′\ni)\nCRF )}\n, (4)\nwhere y′ represents an arbitary labal sequence,\nand wli\nCRF is a model parameter speciﬁc to li, and\nb(li−1,li)\nCRF is a bias speciﬁc to li−1 and li.\nConsidering that the NER label sets across\ndomains can be different, we use CRF(S) and\nCRF(T) to represent CRFs for the source and tar-\nget domains in Figure 2, respectively. We use\nthe ﬁrst-order Viterbi algorithm to ﬁnd the high-\nest scored label sequence.\nLanguage modeling. A forward LM ( LMf )\nuses the forward LSTM hidden state − →h =\n[− →h 1,..., − →h n] to compute the probability of\nnext word xi+1 given x1:i, represented as\npf (xi+1|x1:i). A backward LM ( LMb) computes\npb(xi−1|xi:n) based on backward LSTM hidden\nstate ← −h = [← −h 1,..., ← −h n] in a similar manner.\n2467\nConsidering the computational efﬁciency, Neg-\native Sampling Softmax (NSSoftmax) (Mikolov\net al., 2013; Jean et al., 2014) is used to compute\nforward and backward probabilities, respectively,\nas follows:\npf (xi+1|x1:i)= 1\nZ exp{w⊤\n#xi+1\n− →h i+b#xi+1}\npb(xi−1|xi:n)= 1\nZ exp{w⊤\n#xi−1\n← −h i+b#xi−1},\n(5)\nwhere #x represents the vocabulary index of the\ntarget word x. w#x and b#x are the target word\nvector and the target word bias, respectively. Z is\nthe normalization item computed by\nZ =\n∑\nk∈{#x∪Nx}\nexp{w⊤\nk hi + bk}, (6)\nwhere Nx represents the nagative sample set of the\ntarget word x. Each element in the set is a ran-\ndom number from 1 to the cross-domain vocab-\nulary size. hi represents − →h i in LMf and ← −h i in\nLMb, respectively.\n3.4 Training Objectives\nNER. Given a manually labeled dataset Dner =\n{(xn,yn)}N\nn=1, the sentence-level negative log-\nlikehood loss is used for training:\nLner = − 1\n|Dner|\nN∑\nn=1\nlog(p(yn|xn)) (7)\nLanguage modeling. Given a raw data setDlm =\n{(xn)}N\nn=1, LMf and LMb are trained jointly us-\ning Negative Sampling Softmax. Negative sam-\nples are drawn based on word frequency distribu-\ntion in Dlm. The loss function is:\nLlm = − 1\n2 |Dlm|\nN∑\nn=1\nT∑\nt=1\n{log(pf (xn\nt+1|xn\n1:t))\n+ log(pb(xn\nt−1|xn\nt:T )) } (8)\nJoint training. To perform joint training for NER\nand language modeling on both the source and tar-\nget domains, we minimize the overall loss:\nL=\n∑\nd∈{src,tgt}\nλd(Ld\nner + λtLd\nlm) + λ\n2 ∥Θ∥2, (9)\nwhere λd is a domain weight and λt is a task\nweight. λis the L2 regularization parameters and\nΘ represents the parameters set.\nAlgorithm 1 Multi-task learning\nInput: training data {Sner,T∗\nner}and {Slm,Tlm}\nParameters:\n- Parameters Generator: W, {ID\nd }, {IT\nt }\n- Output layers: θcrfs,θcrft\n∗,θnss\nOutput: Target model\n1: while training steps not end do\n2: split training data into minibatches:\nBners, Bnert∗, Blms, Blmt\n3: # source-domain NER\n4: θsrc,ner\nLSTM ←f(W,IDsrc,ITner)\n5: ∆W,∆IDsrc,∆ITner,∆θcrfs ←train(Bners)\n6: # source-domain LM\n7: θsrc,lm\nLSTM ←f(W,IDsrc,IT\nlm)\n8: ∆W,∆IDsrc,∆IT\nlm,∆θnss ←train(Blms)\n9: if do supervised learning then\n10: # target-domain NER\n11: θtgt,ner\nLSTM ←f(W,IDtgt,ITner)\n12: ∆W,∆IDtgt,∆ITner,∆θcrft ←train(Bnert)\n13: end if\n14: # target-domain LM\n15: θtgt,lm\nLSTM ←f(W,ID\ntgt,IT\nlm)\n16: ∆W,∆IDtgt,∆IT\nlm,∆θnss ←train(Blmt)\n17: Update W, {ID}, {IT }, θcrfs, θcrft\n∗, θnss\n18: end while\nNote: * means none in unsupervised learning\n3.5 Multi-Task Learning Algorithm\nWe propose a cross-task and cross-domain joint\ntraining method for multi-task learning. Algo-\nrithm 1 provides the training procedure. In each\ntraining step (line 1 to 18), minibatches of the 4\ntasks in Figure 1 take turns to train (lines 4-5,\n7-8, 11-12 and 15-16, respectively). Each task\nﬁrst generates the parameters θd,t\nLSTM using W and\ntheir respective ID\nd , IT\nt , and then compute gra-\ndients for f(W,ID\nd ,IT\nt ) and domain-speciﬁc out-\nput layer (θcrfs, θcrft or θnss). In the scenario of\nunsupervised learning, there is no training data of\nthe target-domain NER, and lines 11-12 will not\nbe executed. At the end of each training step, pa-\nrameters of f(·,·,·) and private output layers are\nupdated together in line 17.\n4 Experiments\nWe conduct experiments on three cross-domain\ndatasets, comparing our method with a range of\ntransfer learning baselines under both the super-\nvised domain adaptation and the unsupervised do-\nmain adaptation settings.\n2468\n4.1 Experimental Settings\nData. We take the CoNLL-2003 English NER\ndata (Sang and Meulder, 2003) as our source-\ndomain data. In addition, 377,592 sentences from\nthe Reuters are used for source-domain LM train-\ning in unsupervised domain adaptation. Three\nsets of target-domain data are used, including\ntwo publicly available biomedical NER datasets,\nBioNLP13PC (13PC) and BioNLP13CG (13CG)\n1 and a science and technology dataset we col-\nlected and labeled. Statistics of the datasets are\nshown in Table 1.\nCoNLL-2003 contains four types of enti-\nties, namely P ER (person), L OC (location),\nORG (organization) and M ISC (miscellaneous).\nBioNLP13CG consists of ﬁve types, namely\nCHEM (Chemical), CC (cellular component), G/p\n(gene/protein), S PE (species) and C ELL (cell),\nBioNLP13PC consists of three types of those en-\ntities: C HEM , C C and G/ P. We use text of their\ntraining sets for language modeling training 2.\nFor the science and technology dataset, we col-\nlect 620 articles from CBS SciTech News 3, man-\nually labeling them as a test set for unsupervised\ndomain adaptation. It consists of four types of en-\ntities following the CoNLL-2003 standard. The\nnumbers of each entity type are comparable to the\nCoNLL test set, as listed in Table 2. The main\ndifference is that a great number of entities in the\nCBS News dataset are closely related to the do-\nmain of science and technology. In particular, for\nthe MISC category, more technology terms such as\nSpace X, bitcoin and IP are included, as compared\nwith the CoNLL data set. Lack of such entities in\nthe CoNLL training set and the difference of text\ngenre cause the main difﬁculty in domain transfer.\nTo address this difference, 398,990 unlabeled sen-\ntences from CBS SciTech News are used for LM\ntraining. We released this dataset as one contribu-\ntion of this paper.\nHyperparameters. We choose NCRF++ (Yang\nand Zhang, 2018) for developing the models. Our\nhyperparameter settings largly follow (Yang et al.,\n2018), with the following exceptions: (1) The\nbatch size is set to 30 instead of 10 for shorter\ntraining time in multi-task learning; (2) RMSprop\nwith a learning rate of 0.001 is used for our Sin-\n1https://github.com/cambridgeltl/MTL-Bioinformatics-\n2016\n2We tried to use a larger number of raw data from the\nPubMed, but this did not improve the performances.\n3https://www.cbsnews.com/\nDataset Type Train Dev Test\nCoNLL Sentence 15.0K 3.5K 3.7K\nEntity 23.5K 5.9K 5.6K\nBioNLP13PC Sentence 2.5K 0.9K 1.7K\nEntity 7.9K 2.7K 5.3K\nBioNLP13CG Sentence 3.0K 1.0K 1.9K\nEntity 10.8K 3.6K 6.9K\nCBS News Sentence - - 2.0K\nEntity - - 4.1K\nTable 1: Statistic of datasets.\nDataset PER LOC ORG MISC\nCoNLL\nTrain 6,600 7,140 6,321 3,438\nDev 1,842 1,837 1,341 922\nTest 1,617 1,668 1,661 702\nCBS News Test 1,660 629 1,352 497\nTable 2: Entity numbers of the CoNLL dataset and the\nCBS SciTech News dataset.\n0 X O W L 7 D V N\n\u0010 7 D U J H W\nFigure 3: Development results on 13CG.\ngle Task Model (S TM-TARGET ) for the strongest\nbaseline according to development experiments,\nwhile the multi-task models use SGD with a learn-\ning rate of 0.015 as (Yang et al., 2018). We use\ndomain embeddings and task embeddings of size\n8 to ﬁt the model in one GPU of 8GB memory.\nThe word embeddings for all models are initial-\nized with GloVe 100-dimension vectors (Penning-\nton et al., 2014) and ﬁne-tuned during training.\nCharacter embeddings are randomly initialized.\n4.2 Development Experiments\nWe report a set of development experiments on the\nbiomedical datasets 13PC and 13CG.\nLearning curves. Figure 3 shows the F1-scores\nagainst the number of training iterations on the\n13CG development set. S TM-TARGET is our sin-\ngle task model trained on the target-domain train-\ning set Tner; F INE TUNE is a model pre-trained\n2469\nFigure 4: Joint training in multi-task learning.\nusing the source-domain training data Sner and\nthen ﬁne-tuned using the target-domain data Tner;\nMULTI TASK simultaneously trains source-domain\nNER and target-domain NER following Yang et al.\n(2017). For S TM+ELMO , we mix the source- and\ntarget-domain raw data for training a contextual-\nized ELMo representation (Peters et al., 2018),\nwhich is then used as inputs to an S TM-TARGET\nmodel. This model shows a different way of trans-\nfer by using raw data, which is different from\nFINE TUNE and MULTI TASK . Note that due to dif-\nferences in the label sets, F INE TUNE and M UL-\nTITASK both share parameters between the two\nmodels except for the CRF layers.\nAs can be seen from Figure 3, the F1 of all mod-\nels increase as the number of training iteration in-\ncreases from 1 to 50, with only small ﬂuctuations.\nAll of the models converge to a plateau range\nwhen the iteration number increases to 100. All\ntransfer learning methods outperform the S TM-\nTARGET method, showing the usefulness of us-\ning source data to enhance target labeling. The\nstrong performance of S TM+ELMO over F INE -\nTUNE and M ULTI TASK shows the usefulness of\nraw text. By simultaneously using source-domain\nraw text and target-domain raw text, our model\ngives the best F1 over all iterations.\nEffect of language model for transfer. Figure\n4 shows the results of source language modeling,\ntarget language modeling, source NER and tar-\nget NER for both development datasets when the\nnumber of training iterations increases. As can\nbe seen, multi-task learning under our framework\nbrings beneﬁt to all tasks, without being negatively\ninﬂuenced by potential conﬂicts between tasks\n(Bingel and Søgaard, 2017; Mou et al., 2016).\nMethods Datasets\n13PC 13CG\nCrichton et al. (2017) 81.92 78.90\nSTM-TARGET 82.59 76.55\nMULTITASK(NER+LM) 81.33 75.27\nMULTITASK(NER) 83.09 77.73\nFINETUNE 82.55 76.73\nSTM+ELMO 82.76 78.24\nCO-LM 84.43 78.60\nCO-NER 83.87 78.43\nMIX-DATA 83.88 78.70\nFINAL 85.54† 79.86†\nTable 3: F1-scores on 13PC and 13CG. †indicates that\nthe FINAL results are statistically signiﬁcant compared\nto all transfer baselines and ablation baselines withp<\n0.01 by t-test.\n4.3 Final Results on Supervised Domain\nAdaptation\nWe investigate supervised transfer from CoNLL\nto 13PC and 13CG, comparing our model with a\nrange of baseline transfer approaches. In partic-\nular, three sets of comparisons are made, includ-\ning (1) a comparison between our method with\nother supervised domain adaptation methods, such\nas MULTI TASK (NER ) 4 and ELMo, (2) a compar-\nison between the use of different subsets of data\nfor transfer under our own framework and (3) a\ncomparison with the current state-of-the-art in the\nliterature for these datasets.\n(1) Comparison with other supervised trans-\nfer methods. We compare our method with\nSTM-TARGET , M ULTI TASK (NER ), F INE TUNE\nand S TM+ELMO . The observations are simi-\nlar to those on the development set. Note that\nFINE TUNE does not always improve over S TM-\nTARGET , which shows that the difference between\nthe two datasets can hurt naive transfer learning,\nwithout considering domain descriptor vectors.\nELMo. The ELMo methods use raw text\nvia language model pre-training, which has\nbeen shown to beneﬁt many NLP tasks (Pe-\nters et al., 2018). In our cross-domain setting,\nSTM+ELMO gives a signiﬁcant improvement over\nSTM-TARGET on the 13CG dataset, but only a\nsmall improvement on the 13PC dataset. The over-\nall improvements are comparable to that of M UL-\nTITASK only using the raw data. We also tried to\nuse the ELMo model (Original) released by Peters\n4Here M ULTI TASK (NER ) is the same model as M ULTI -\nTASK in the development experiments.\n2470\nSource Domain\nNER\nTarget Domain\nNER\nSource Domain\nLM\nTarget Domain\nLM\nCo-NER\nCo-LM\nMix-Data\nFinal\nFigure 5: Ablations of the model.\net al. (2018)5, which is trained over approximately\n800M tokens. The results are 84.08% on 13PC and\n79.57% on 13CG, respectively, which are lower\ncompared to 85.54% and 79.86% by our method,\nrespectively, despite the use of much larger ex-\nternal data. This shows the effectiveness of our\nmodel.\nMulti-task of NER and LM. We additionally\ncompare our method with the naive multi-task\nlearning setting (Collobert and Weston, 2008),\nwhich uses shared parameters for the four tasks\nbut use the exact same data conditions as the\nFINAL model. which is shown in the M ULTI -\nTASK (NER +LM) method in Table 3. The method\ngives an 81.33% F1 on 13PC and 75.27% on\n13CG, which is much lower compared with all\nbaseline models. This demonstrates the chal-\nlenge of the cross-domain and cross-task setting,\nwhich contains conﬂicting information from dif-\nferent text genres and task requirements.\n(2) Ablation experiments. Now that we have\ncompared our method with baselines utilizing sim-\nilar data sources, we turn to investigate the inﬂu-\nence of data sources on our own framework. As\nshown in Figure 5, we make novel use of 4 data\nsources for the combination of two tasks in two\ndomains. If some sources are removed, our set-\ntings fall back to traditional transfer learning. For\nexample, if the LM task is not considered, then the\ntask setting is standard supervised domain adapta-\ntion.\nThe baselines include (1) C O-LM, which rep-\nresents our model without source-domain tasks,\njoint training the target-domain NER and language\nmodeling, transferring parameters as: θt\nLSTM =\nW ⊗IT\nt ,(t ∈{ner,lm}). (2) C O-NER, deleting\ntasks, jointly training source- and target-domain\n5https://allennlp.org/elmo\nFigure 6: Inﬂuence of target-domain data.\nNER, transferring parameters as: θd\nLSTM = W ⊗\nID\nd ,(d ∈{src,tgt}). (3) M IX-DATA, which uses\nthe same NER data in source- and target-domain\nas FINAL , but also uses combined raw text to train\nsource- and target-domain language models.\nOur method outperforms all baselines signiﬁ-\ncantly, which shows the importance of using rich\ndata. A contrast between our method and M IX-\nDATA shows the effectiveness of using two dif-\nferent language models across domains. Even\nthrough M IX-DATA uses more data for training\nlanguage models on both the source and target do-\nmains, it cannot learn a domain contrast since both\nsides use the same mixed data. In contrast, our\nmodel gives signiﬁcantly better results by glean-\ning such contrast.\n(3) Comparison with current state-of-the-art.\nFinally, Table 3 also shows a comparison with a\nstate-of-the-art method on the 13PC and 13CG\ndatasets (Crichton et al., 2017), which leverages\nPOS tagging for multi-task learning by using co-\ntraining method. Our model outperforms their re-\nsults, giving the best results in the literature.\nDiscussion. When the number of target-domain\nNER sentences is 0, the transfer learning setting is\nunsupervised domain adaptation. As the number\nof target domain NER sentences increases, they\nwill intuitively play an increasingly important role\nfor target NER. Figure 6 compares the F1-scores\nof the baseline S TM-TARGET and our multi-task\nmodel with varying numbers of target-domain\nNER training data under 100 training epochs. In\nthe nearly unsupervised setting, our method gives\nthe largest improvement of 20.5% F1-scores. As\nthe number of training data increases, the gap be-\ntween the two methods becomes smaller. But our\nmethod still gives a 3.3% F1 score gain when the\nnumber of training sentences reach 3,000, show-\n2471\n0 X O W L 7 D V N\n0 X O W L 7 D V N\n\u0010 7 D U J H W\n\u0010 7 D U J H W\nFigure 7: Fine-grained comparisons on 13PC and\n13CG.\ning the effectiveness of LM in knowledge transfer.\nFigure 7 shows ﬁne-grained NER results of\nall available entity types. In comparison to\nSTM-TARGET , F INE TUNE and MULTI TASK , our\nmethod outperforms all the baselines on each en-\ntity type, which is in accordance with the conclu-\nsion of development experiments.\n4.4 Unsupervised Domain Adaptation\nFor unsupervised domain adaptation, many set-\ntings in Subsection 4.2 do not hold, including\nSTM-TARGET , F INE TUNE , M ULTI TASK , C O-\nLM and C O-NER. Instead, we add a naive\nbaseline, S TM-SOURCE , which directly applies a\nmodel trained on the source-domain CoNLL-2003\ndata to the target domain. In addition, we com-\npare with models that make use of source NER,\nsource LM and target LM data, including S ELF -\nTRAIN , which improves a source NER model on\ntarget raw text (Daum ´e III, 2008). S TM-ELMO ,\nwhich uses ELMo embeddings trained over com-\nbined source- and target-domain raw text for STM-\nSOURCE , S TM-ELMO (SRC ), which uses only the\nsource-domain raw data for training ELMo, STM-\nELMO (TGT ), which uses only the target-domain\nraw text for training ELMo, and D ANN (Ganin\net al., 2016), which performs generative adver-\nsarial training over source- and target-domain raw\ndata.\nFinal results. The ﬁnal results are shown in Ta-\nble 4. S ELF -TRAIN gives better results compared\nwith the S TM-SOURCE baseline, which shows\nthe effectiveness of target-domain raw data. Ad-\nversarial training brings signiﬁcantly better im-\nprovements compared with naive self-training.\nAmong ELMo methods, the model using both the\nsource-domain raw data and target-domain raw\ndata outperforms the model using only the source-\nor target-domain raw data. ELMo also outper-\nMethods P R F1\nSTM-SOURCE 63.87 71.28 67.37\nSELF-TRAIN 62.56 75.04 68.24\nDANN(Ganin et al., 2016) 65.14 73.84 69.22\nSTM+ELMO(SRC) 65.43 70.14 67.70\nSTM+ELMO(TGT) 67.78 72.73 70.17\nSTM+ELMO 67.19 74.93 70.85\nOurs 68.48 79.52 73.59†\nTable 4: Three metrics on CBS SciTech News. We\nuse the CoNLL dev set to select the hyperparameters of\nour models. ELMo and Ours are given the same over-\nall raw data, S ELF -TRAIN and DANN use the selected\nraw data from overall raw data for better performances.\n†indicates that our results are statistically signiﬁcant\ncompared to all baselines with p< 0.01 by t-test.\nFigure 8: Amount of raw data.\nforms D ANN , which shows the strength of LM\npre-training. Interestingly, ELMo with target-\ndomain raw data gives similar accuracies to ELMo\nwith mixed source- and target-domain data, which\nshows that target-domain LM is more useful for\nthe pretraining method. It also indicates that our\nmethod makes better use of LMs over two differ-\nent domains. Compared with all baseline mod-\nels, our model gives a ﬁnal F1 of 73.59, signiﬁ-\ncantly better than the best result of 70.85 obtained\nby S TM+ELMO , demonstrating the effectiveness\nof parameter generation network for cross-task,\ncross-domain knowledge transfer.\nInﬂuence of raw text. For zero-shot learning, do-\nmain adaptation is achieved solely through LM\nchannels. We thus compare the effectiveness of\nraw text from both the source domain and the\ntarget domain. Figure 8 shows the results. The\nline “SRC: varying; TGT: varying” shows the F1-\nscores against varying numbers of raw sentences\nin both source and target domains. Each num-\nber in the x-coordinate indicates an equal amount\nof source- and target-domain text. As can be\nseen, increasing raw text gives increased F1 for\n2472\nEntity Type Correct Num ∆STM Ours\nPER 1,501 1,569 +4.10%\nLOC 469 512 +6.84%\nORG 941 1,050 +8.06%\nMISC 134 193 +11.87%\nTotal 3,045 3,324 +6.74%\nTable 5: Growth rate of correctly recognized enetity\nnumber in comparison with the STM-SOURCE . ∆ rep-\nresents the growth with respect to the total number of\nentities in the CBS SciTech News test set.\nSentenceBrittany Kaiser spoke to “CBS This Morning”\nco-host John Dicherson for her ﬁrst U.S. broadcast network interview.\nSTM-SRC Brittany Kaiser ORGspoke to “CBS ORGThis Morning” ...\nDANN Brittany Kaiser PERspoke to “CBS This Morning ORG” ...\nOurs Brittany Kaiser PERspoke to “CBS This Morning MISC” ...\nTable 6: Example. Red and green represent incorrect\nand correct entities, respectively.\nNER, which demonstrates effective use of raw\ndata by our method. The lines “SRC: 100%;\nTGT: varying” and “SRC: varying; TGT: 100%”\nshow to alternative measures by ﬁxing the source-\nand target-domain raw text to 100% of our data,\nand then varying only the other domain text. A\ncomparison between the two lines shows that the\ntarget-domain raw data gives more inﬂuence to the\ndomain adaptation power, which conforms to intu-\nition.\nDiscussion. Table 5 shows a breakdown for the\nimprovement of our model over STM-SOURCE by\ndifferent entity types. Compared with P ER, L OC\nand ORG names, our method brings the most im-\nprovements over MISC entities, which are mostly\ntypes that are speciﬁc to the technology domain\n(see Subsection 4.1). Intuitively, the amount of\noverlap is the weakest for this type of entities be-\ntween raw text from source and target domains.\nTherefore, the results show the effectiveness of our\nmethod in deriving domain contrast with respect to\nNER from cross-domain language modeling.\nTable 6 shows a case study, where “Brittany\nKaiser” is a personal name and “CBS This Morn-\ning” is a programme. Without using raw text,\nSTM-SOURCE misclassiﬁes “Brittany Kaiser” as\nORG. Both DANN and our method give the correct\nresults because the name is mentioned in raw text,\nfrom which connections between the pattern “PER\nspoke” can be drawn. With the help of raw text,\nDANN and our method can also recognize “CBS\nThis Morning” as a entity, which has a common\npattern of consecutive capital letters in both source\nand target domains.\nDANN misclassiﬁes “CBS This Morning” as\nORG. In contrast, our model can classify it cor-\nrectly as the category of M ISC , in which most en-\ntities are speciﬁc to the target domain (see Subsec-\ntion 4.1). This is likely because adversarial train-\ning in D ANN aims to match feature distributions\nbetween source and target domains by mimicing\nthe domain discriminator, which can lead to con-\ncentration on domain common features but con-\nfusion about such domain-speciﬁc features. This\ndemonstrates the advantage of our method in de-\nriving both domain common and domain-speciﬁc\nfeatures.\n5 Conclusion\nWe considered NER domain adaptation by extract-\ning knowledge of domain differences from raw\ntext. For this goal, cross-domain language mod-\neling is conducted through a novel parameter gen-\neration network, which decomposes domain and\ntask knowledge into two sets of embedding vec-\ntors. Experiments on three datasets show that our\nmethod is highly effective among supervised do-\nmain adaptation methods, while allowing zero-\nshot learning in unsupervised domain adaptation.\nAcknowledgments\nThe three authors contributed equally to this work.\nYue Zhang is the corresponding author. We grate-\nfully acknowledge funding from NSFC (grant\n#61572245). We also thank the anonymous re-\nviewers for their helpful comments and sugges-\ntions.\nReferences\nWaleed Ammar, George Mulcaire, Miguel Ballesteros,\nChris Dyer, and Noah A. Smith. 2016. Many lan-\nguages, one parser. Transactions of the Association\nfor Computational Linguistics, 4:431–444.\nJoachim Bingel and Anders Søgaard. 2017. Identify-\ning beneﬁcial task relations for multi-task learning\nin deep neural networks. In Proceedings of the 15th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics (Short Papers),\nvolume 2, pages 164–169. Association for Compu-\ntational Linguistics.\nJason P.C. Chiu and Eric Nichols. 2016. Named entity\nrecognition with bidirectional lstm-cnns. Transac-\ntions of the Association for Computational Linguis-\ntics, 4:357–370.\n2473\nRonan Collobert and Jason Weston. 2008. A uniﬁed\narchitecture for natural language processing: Deep\nneural networks with multitask learning. In Pro-\nceedings of the 25th International Conference on\nMachine Learning, pages 160–167.\nRonan Collobert, Jason Weston, L´eon Bottou, Michael\nKarlen, Koray Kavukcuoglu, and Pavel Kuksa.\n2011. Natural language processing (almost) from\nscratch. Journal of Machine Learning Research,\n12(1):2493–2537.\nGamal Crichton, Sampo Pyysalo, Billy Chiu, and Anna\nKorhonen. 2017. A neural network multi-task learn-\ning approach to biomedical named entity recogni-\ntion. BMC Bioinformatics, 18(1):368.\nHal Daum´e III. 2009. Frustratingly easy domain adap-\ntation. In Proceedings of the 45th Annual Meet-\ning of the Association of Computational Linguistics,\npages 256–263. Association for Computational Lin-\nguistics.\nHal Daum ´e III. 2008. Cross-task knowledge-\nconstrained self training. In Proceedings of the Con-\nference on Empirical Methods in Natural Language\nProcessing, volume 1, pages 680–688. Association\nfor Computational Linguistics.\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,\nPascal Germain, Hugo Larochelle, Franc ¸ois Lavi-\nolette, Mario Marchand, and Victor Lempitsky.\n2016. Domain-adversarial training of neural net-\nworks. Journal of Machine Learning Research,\n17(1):2096–2030.\nJames Hammerton. 2003. Named entity recognition\nwith long short-term memory. In Proceedings of\nthe 7th Conference on Natural Language Learning\nat HLT-NAACL, volume 4, pages 172–175. Associa-\ntion for Computational Linguistics.\nS´ebastien Jean, Kyunghyun Cho, Roland Memise-\nvic, and Yoshua Bengio. 2014. On using very\nlarge target vocabulary for neural machine transla-\ntion. In Proceedings of the 53rd Annual Meeting of\nthe Association for Computational Linguistics and\nthe 7th International Joint Conference on Natural\nLanguage Processing, pages 1–10. Association for\nComputational Linguistics.\nYoung-Bum Kim, Karl Stratos, Ruhi Sarikaya, and\nMinwoo Jeong. 2015. New transfer learning tech-\nniques for disparate label sets. In Proceedings of the\n53rd Annual Meeting of the Association for Compu-\ntational Linguistics and the 7th International Joint\nConference on Natural Language Processing (Long\nPapers), volume 1, pages 473–482. Association for\nComputational Linguistics.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 260–270. Association for Computational Lin-\nguistics.\nJi Young Lee, Franck Dernoncourt, and Peter\nSzolovits. 2017. Transfer learning for named-entity\nrecognition with neural networks. Computing Re-\nsearch Repository, arXiv:1705.06273. Version 1.\nBill Yuchen Lin and Wei Lu. 2018. Neural adaptation\nlayers for cross-domain named entity recognition.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2012–2022. Association for Computational Linguis-\ntics.\nQi Liu, Yue Zhang, and Jiangming Liu. 2018. Learning\ndomain representation for multi-domain sentiment\nclassiﬁcation. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies (Long Papers), volume 1, pages\n541–550. Association for Computational Linguis-\ntics.\nXuezhe Ma and Eduard Hovy. 2016. End-to-end se-\nquence labeling via bi-directional lstm-cnns-crf. In\nProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Long Pa-\npers), volume 1, pages 1064–1074. Association for\nComputational Linguistics.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems, pages 3111–3119.\nLili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu,\nLu Zhang, and Zhi Jin. 2016. How transferable are\nneural networks in nlp applications? In Proceedings\nof the 2016 Conference on Empirical Methods in\nNatural Language Processing, pages 479–489. As-\nsociation for Computational Linguistics.\nRasha Obeidat, Xiaoli Fern, and Prasad Tadepalli.\n2016. Label embedding approach for transfer learn-\ning. In International Conference on Biomedical On-\ntology and BioCreative.\nJeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. 2014. Glove: Global vectors for\nword representation. In Proceedings of the 2014\nConference on Empirical Methods in Natural Lan-\nguage Processing, volume 4, pages 1532–1543. As-\nsociation for Computational Linguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies (Long Papers), volume 1, pages 2227–\n2237. Association for Computational Linguistics.\n2474\nEmmanouil Antonios Platanios, Mrinmaya Sachan,\nGraham Neubig, and Tom M. Mitchell. 2018. Con-\ntextual parameter generation for universal neural\nmachine translation. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 425–435. Association for\nComputational Linguistics.\nMarek Rei. 2017. Semi-supervised multitask learn-\ning for sequence labeling. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (Long Papers), volume 1, pages\n2121–2130. Association for Computational Linguis-\ntics.\nDevendra Singh Sachan, Pengtao Xie, and Eric P. Xing.\n2018. Effective use of bidirectional language mod-\neling for medical named entity recognition. Pro-\nceedings of Machine Learning Research, 85:1–19.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the conll-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natu-\nral Language Learning at HLT-NAACL 2003, pages\n142–147. Association for Computational Linguis-\ntics.\nCicero Nogueira dos Santos and Victor Guimar ˜aes.\n2015. Boosting named entity recognition with neu-\nral character embeddings. In Proceedings of the\nFifth Named Entity Workshop, joint with 53rd ACL\nand the 7th IJCNLP, pages 25–33. Association for\nComputational Linguistics.\nSara Stymne, Miryam de Lhoneux, Aaron Smith, and\nJoakim Nivre. 2018. Parser training with heteroge-\nneous treebanks. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Short Papers), pages 619–625. Association\nfor Computational Linguistics.\nZhenghui Wang, Yanru Qu, Liheng Chen, Shen Jian,\nWeinan Zhang, Shaodian Zhang, Yimei Gao, Gen\nGu, Ken Chen, and Yu Yong. 2018. Label-aware\ndouble transfer learning for cross-specialty medi-\ncal named entity recognition. In Proceedings of\nNAACL-HLT 2018, pages 1–15. Association for\nComputational Linguistics.\nJie Yang, Shuailong Liang, and Yue Zhang. 2018. De-\nsign challenges and misconceptions in neural se-\nquence labeling. In Proceedings of the 27th Inter-\nnational Conference on Computational Linguistics,\npages 3879–3889.\nJie Yang and Yue Zhang. 2018. Ncrf++: An open-\nsource neural sequence labeling toolkit. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics-System Demonstra-\ntions, pages 74–79. Association for Computational\nLinguistics.\nZhilin Yang, Ruslan Salakhutdinov, and William W.\nCohen. 2017. Transfer learning for sequence tag-\nging with hierarchical recurrent networks. In Inter-\nnational Conference on Learning Representations."
}