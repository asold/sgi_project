{
    "title": "Scalable Visual Transformers with Hierarchical Pooling",
    "url": "https://openalex.org/W3138796575",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A3045690129",
            "name": "Zizheng Pan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2337603917",
            "name": "Bohan Zhuang",
            "affiliations": [
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A1479773632",
            "name": "Jing Liu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2129072291",
            "name": "Haoyu He",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2093782331",
            "name": "Jianfei Cai",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2162931300",
        "https://openalex.org/W2994759459",
        "https://openalex.org/W2558580397",
        "https://openalex.org/W2962965870",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W3123615524",
        "https://openalex.org/W2998108143",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W3034573343",
        "https://openalex.org/W3102892879",
        "https://openalex.org/W2981689412",
        "https://openalex.org/W3035251378",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2964137095",
        "https://openalex.org/W3033529678",
        "https://openalex.org/W3118608800",
        "https://openalex.org/W3098873988",
        "https://openalex.org/W2940744433",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3034742519",
        "https://openalex.org/W2994673210",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2565639579",
        "https://openalex.org/W3098576111",
        "https://openalex.org/W3038012435",
        "https://openalex.org/W2962835968",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W3139049060",
        "https://openalex.org/W3037932933",
        "https://openalex.org/W3203606893",
        "https://openalex.org/W3105966348",
        "https://openalex.org/W2955425717",
        "https://openalex.org/W3040304705",
        "https://openalex.org/W3122239467",
        "https://openalex.org/W2117539524",
        "https://openalex.org/W2983446232",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W3171516518",
        "https://openalex.org/W3125056032",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W3097065222",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2970565456",
        "https://openalex.org/W3172509117",
        "https://openalex.org/W3168649818"
    ],
    "abstract": "The recently proposed Visual image Transformers (ViT) with pure attention have achieved promising performance on image recognition tasks, such as image classification. However, the routine of the current ViT model is to maintain a full-length patch sequence during inference, which is redundant and lacks hierarchical representation. To this end, we propose a Hierarchical Visual Transformer (HVT) which progressively pools visual tokens to shrink the sequence length and hence reduces the computational cost, analogous to the feature maps downsampling in Convolutional Neural Networks (CNNs). It brings a great benefit that we can increase the model capacity by scaling dimensions of depth/width/resolution/patch size without introducing extra computational complexity due to the reduced sequence length. Moreover, we empirically find that the average pooled visual tokens contain more discriminative information than the single class token. To demonstrate the improved scalability of our HVT, we conduct extensive experiments on the image classification task. With comparable FLOPs, our HVT outperforms the competitive baselines on ImageNet and CIFAR-100 datasets.",
    "full_text": "Scalable Vision Transformers with Hierarchical Pooling\nZizheng Pan Bohan Zhuang † Jing Liu Haoyu He Jianfei Cai\nDept of Data Science and AI, Monash University\nAbstract\nThe recently proposed Visual image Transformers (ViT)\nwith pure attention have achieved promising performance\non image recognition tasks, such as image classiﬁcation.\nHowever, the routine of the current ViT model is to main-\ntain a full-length patch sequence during inference, which\nis redundant and lacks hierarchical representation. To this\nend, we propose a Hierarchical Visual Transformer (HVT)\nwhich progressively pools visual tokens to shrink the se-\nquence length and hence reduces the computational cost,\nanalogous to the feature maps downsampling in Convolu-\ntional Neural Networks (CNNs). It brings a great beneﬁt\nthat we can increase the model capacity by scaling dimen-\nsions of depth/width/resolution/patch size without introduc-\ning extra computational complexity due to the reduced se-\nquence length. Moreover, we empirically ﬁnd that the av-\nerage pooled visual tokens contain more discriminative in-\nformation than the single class token. To demonstrate the\nimproved scalability of our HVT, we conduct extensive ex-\nperiments on the image classiﬁcation task. With compara-\nble FLOPs, our HVT outperforms the competitive baselines\non ImageNet and CIFAR-100 datasets. Code is available at\nhttps://github.com/MonashAI/HVT.\n1. Introduction\nEquipped with the self-attention mechanism that has\nstrong capability of capturing long-range dependencies,\nTransformer [37] based models have achieved signiﬁcant\nbreakthroughs in many computer vision (CV) and natural\nlanguage processing (NLP) tasks, such as machine trans-\nlation [10, 9], image classiﬁcation [11, 36], segmentation\n[43, 39] and object detection [3, 48]. However, the good\nperformance of Transformers comes at a high computa-\ntional cost. For example, a single Transformer model re-\nquires more than 10G Mult-Adds to translate a sentence of\nonly 30 words. Such a huge computational complexity hin-\nders the widespread adoption of Transformers, especially\non resource-constrained devices, such as smart phones.\n†Corresponding author. Email: bohan.zhuang@monash.edu\n0 4 8 12 16 20\nGFLOPs\n65\n70\n75\n80Top-1 Acc (%)\nHVT-Ti-1\nScale HVT-Ti-4\nHVT-S-1\nDeiT-Ti\nDeiT-S\nDeiT-B\nR-18\nR-34\nR-50\nR-101\nR-152\nHVT\nDeiT\nResNet\nFigure 1: Performance comparisons on ImageNet. With\ncomparable GFLOPs (1.25 vs. 1.39), our proposed Scale\nHVT-Ti-4 surpasses DeiT-Ti by 3.03% in Top-1 accuracy.\nTo improve the efﬁciency, there are emerging efforts to\ndesign efﬁcient and scalable Transformers. On the one\nhand, some methods follow the idea of model compression\nto reduce the number of parameters and computational over-\nhead. Typical methods include knowledge distillation [19],\nlow-bit quantization [29] and pruning [12]. On the other\nhand, the self-attention mechanism has quadratic memory\nand computational complexity, which is the key efﬁciency\nbottleneck of Transformer models. The dominant solu-\ntions include kernelization [20, 28], low-rank decomposi-\ntion [41], memory [30], sparsity [4] mechanisms, etc.\nDespite much effort has been made, there still lacks spe-\nciﬁc efﬁcient designs for Visual Transformers considering\ntaking advantage of characteristics of visual patterns. In\nparticular, ViT models maintain a full-length sequence in\nthe forward pass across all layers. Such a design can suffer\nfrom two limitations. Firstly, different layers should have\ndifferent redundancy and contribute differently to the accu-\nracy and efﬁciency of the network. This statement can be\nsupported by existing compression methods [35, 23], where\neach layer has its optimal spatial resolution, width and bit-\nwidth. As a result, the full-length sequence may contain\narXiv:2103.10619v2  [cs.CV]  18 Aug 2021\nhuge redundancy. Secondly, it lacks multi-level hierarchi-\ncal representations, which is well known to be essential for\nthe success of image recognition tasks.\nTo solve the above limitations, we propose to gradually\ndownsample the sequence length as the model goes deeper.\nSpeciﬁcally, inspired by the design of VGG-style [33] and\nResNet-style [14] networks, we partition the ViT blocks\ninto several stages and apply the pooling operation ( e.g.,\naverage/max pooling) in each stage to shrink the sequence\nlength. Such a hierarchical design is reasonable since a re-\ncent study [7] shows that a multi-head self-attention layer\nwith a sufﬁcient number of heads can express any convo-\nlution layers. Moreover, the sequence of visual tokens in\nViT can be analogous to the ﬂattened feature maps of CNNs\nalong the spatial dimension, where the embedding of each\ntoken can be seen as feature channels. Hence, our design\nshares similarities with the spatial downsampling of feature\nmaps in CNNs. To be emphasized, the proposed hierarchi-\ncal pooling has several advantages. (1) It brings consider-\nable computational savings and improves the scalability of\ncurrent ViT models. With comparable ﬂoating-point opera-\ntions (FLOPs), we can scale up our HVT by expanding the\ndimensions of width/depth/resolution. In addition, the re-\nduced sequential resolution also empowers the partition of\nthe input image into smaller patch sizes for high-resolution\nrepresentations, which is needed for low-level vision and\ndense prediction tasks. (2) It naturally leads to the generic\npyramidal hierarchy, similar to the feature pyramid network\n(FPN) [24], which extracts the essential multi-scale hidden\nrepresentations for many image recognition tasks.\nIn addition to hierarchical pooling, we further propose\nto perform predictions without the class token. Inherited\nfrom NLP, conventional ViT models [11, 36] equip with a\ntrainable class token, which is appended to the input patch\ntokens, then reﬁned by the self-attention layers, and is ﬁ-\nnally used for prediction. However, we argue that it is not\nnecessary to rely on the extra class token for image clas-\nsiﬁcation. To this end, we instead directly apply average\npooling over patch tokens and use the resultant vector for\nprediction, which achieves improved performance. We are\naware of a concurrent work [6] that also observes the similar\nphenomenon.\nOur contributions can be summarized as follows:\n• We propose a hierarchical pooling regime that grad-\nually reduces the sequence length as the layer goes\ndeeper, which signiﬁcantly improves the scalability\nand the pyramidal feature hierarchy of Visual Trans-\nformers. The saved FLOPs can be utilized to improve\nthe model capacity and hence the performance.\n• Empirically, we observe that the average pooled visual\ntokens contain richer discriminative patterns than the\nclass token for classiﬁcation.\n• Extensive experiments show that, with comparable\nFLOPs, our HVT outperforms the competitive base-\nline DeiT on image classiﬁcation benchmarks, includ-\ning ImageNet and CIFAR-100.\n2. Related Work\nVisual Transformers. The powerful multi-head self-\nattention mechanism has motivated the studies of applying\nTransformers on a variety of CV tasks. In general, cur-\nrent Visual Transformers can be mainly divided into two\ncategories. The ﬁrst category seeks to combine convolu-\ntion with self-attention. For example, Carion et al. [3] pro-\npose DETR for object detection, which ﬁrstly extracts vi-\nsual features with CNN backbone, followed by the feature\nreﬁnement with Transformer blocks. BotNet [34] is a re-\ncent study that replaces the convolution layers with mul-\ntiheaded self-attention layers at the last stage of ResNet.\nOther works [48, 18] also present promising results with\nthis hybrid architecture. The second category aims to\ndesign a pure attention-based architecture without convo-\nlutions. Recently, Ramachandran et al . [27] propose a\nmodel which replaces all instances of spatial convolutions\nwith a form of self-attention applied to ResNet. Hu et\nal. [17] propose LR-Net [17] that replaces convolution lay-\ners with local relation layers, which adaptively determines\naggregation weights based on the compositional relation-\nship of local pixel pairs. Axial-DeepLab [40] is also pro-\nposed to use Axial-Attention [16], a generalization form of\nself-attention, for Panoptic Segmentation. Dosovitskiy et\nal. [11] ﬁrst transfers Transformer to image classiﬁcation.\nThe model inherits a similar architecture from standard\nTransformer in NLP and achieves promising results on Ima-\ngeNet, whereas it suffers from prohibitively expensive train-\ning complexity. To solve this, the following work DeiT [36]\npropose a more advanced optimization strategy and a dis-\ntillation token, with improved accuracy and training efﬁ-\nciency. Moreover, T2T-ViT [45] aims to overcome the lim-\nitations of simple tokenization of input images in ViT and\npropose to progressively structurize the image to tokens to\ncapture rich local structural patterns. Nevertheless, the pre-\nvious literature all assumes the same architecture to the NLP\ntask, without the adaptation to the image recognition tasks.\nIn this paper, we propose several simple yet effective mod-\niﬁcations to improve the scalability of current ViT models.\nEfﬁcient Transformers. Transformer-based models are\nresource-hungry and compute-intensive despite their state-\nof-the-art performance. We roughly summarize the efﬁcient\nTransformers into two categories. The ﬁrst category fo-\ncuses on applying generic compression techniques to speed\nup the inference, either based on quantization [47], prun-\ning [26, 12], and distillation [32] or seeking to use Neu-\nMLP Head\nAverage Pool\ncat\ndog\nbird\n…\nLinear Projection\n012131415\nTransformer Block\nMax Pool\nTransformer Block\nTransformer Block\nTransformer Block\nPatch + Pos\nTransformer Block\nMax Pool\nTransformer Block\nTransformer Block\nTransformer Block\nTransformer Block\nMax Pool\nTransformer Block\nTransformer Block\nTransformer Block\n…\n…\nStage 1 Stage 2 Stage 3\nWithout ‘CLS’\n01234567\n0123\n01\nFigure 2: Overview of the proposed Hierarchical Visual Transformer. To reduce the redundancy in the full-length patch\nsequence and construct a hierarchical representation, we propose to progressively pool visual tokens to shrink the sequence\nlength. To this end, we partition the ViT [11] blocks into several stages. At each stage, we insert a pooling layer after the ﬁrst\nTransformer block to perform down-sampling. In addition to the pooling layer, we perform predictions using the resultant\nvector of average pooling the output visual tokens of the last stage instead of the class token only.\nral Architecture Search (NAS) [38] to explore better con-\nﬁgurations. Another category aims to solve the quadratic\ncomplexity issue of the self-attention mechanism. A rep-\nresentative approach [5, 20] is to express the self-attention\nweights as a linear dot-product of kernel functions and make\nuse of the associative property of matrix products to re-\nduce the overall self-attention complexity from O(n2) to\nO(n). Moreover, some works alternatively study diverse\nsparse patterns of self-attention [4, 21], or consider the low-\nrank structure of the attention matrix [41], leading to lin-\near time and memory complexity with respect to the se-\nquence length. There are also some NLP literatures that\ntend to reduce the sequence length during processing. For\nexample, Goyal et al. [13] propose PoWER-BERT, which\nprogressively eliminates word tokens during the forward\npass. Funnel-Transformer [8] presents a pool-query-only\nstrategy, pooling the query vector within each self-attention\nlayer. However, there are few literatures targeting improv-\ning the efﬁciency of the ViT models.\nTo compromise FLOPs, current ViT models divide the\ninput image into coarse patches (i.e., large patch size), hin-\ndering their generalization to dense predictions. In order to\nbridge this gap, we propose a general hierarchical pooling\nstrategy that signiﬁcantly reduces the computational cost\nwhile enhancing the scalability of important dimensions\nof the ViT architectures, i.e., depth, width, resolution and\npatch size. Moreover, our generic encoder also inherits the\npyramidal feature hierarchy from classic CNNs, potentially\nbeneﬁting many downstream recognition tasks. Also note\nthat different from a concurrent work [42] which applies\n2D patch merging, this paper introduces the feature hierar-\nchy with 1D pooling. We discuss the impact of 2D pooling\nin Section 5.2.\n3. Proposed Method\nIn this section, we ﬁrst brieﬂy revisit the preliminaries of\nVisual Transformers [11] and then introduce our proposed\nHierarchical Visual Transformer.\n3.1. Preliminary\nLet I ∈RH×W×C be an input image, where H, W\nand Crepresent the height, width, and the number of chan-\nnels, respectively. To handle a 2D image, ViT ﬁrst splits\nthe image into a sequence of ﬂattened 2D patches X =\n[x1\np; x2\np; ...; xN\np ], where xi\np ∈RP2C is the i-th patch of the\ninput image and [·] is the concatenation operation. Here,\nN = HW/P2 is the number of patches and P is the size\nof each patch. ViT then uses a trainable linear projection\nthat maps each vectorized patch to a D dimension patch\nembedding. Similar to the class token in BERT [10], ViT\nprepends a learnable embedding xcls ∈RD to the sequence\nof patch embeddings. To retain positional information, ViT\nintroduces an additional learnable positional embeddings\nE ∈R(N+1)×D. Mathematically, the resulting represen-\ntation of the input sequence can be formulated as\nX0 = [xcls; x1\npW; x2\npW; ...; xN\np W] +E, (1)\nwhere W ∈RP2C×D is a learnable linear projection pa-\nrameter. Then, the resulting sequence of embeddings serves\nas the input to the Transformer encoder [37].\nSuppose that the encoder in a Transformer consists of\nL blocks. Each block contains a multi-head self-attention\n(MSA) layer and a position-wise multi-layer perceptron\n(MLP). For each layer, layer normalization (LN) [1] and\nresidual connections [14] are employed, which can be for-\nmulated as follows\nX\n′\nl−1 = Xl−1 + MSA(LN(Xl−1)), (2)\nXl = X\n′\nl−1 + MLP(LN(X\n′\nl−1)), (3)\nwhere l ∈ [1,...,L ] is the index of Transformer blocks.\nHere, a MLP contains two fully-connected layers with a\nGELU non-linearity [15]. In order to perform classiﬁca-\ntion, ViT applies a layer normalization layer and a fully-\nconnected (FC) layer to the ﬁrst token of the Transformer\nencoder’s output X0\nL. In this way, the output prediction y\ncan be computed by\ny = FC(LN(X0\nL)). (4)\n3.2. Hierarchical Visual Transformer\nIn this paper, we propose a Hierarchical Visual Trans-\nformer (HVT) to reduce the redundancy in the full-length\npatch sequence and construct a hierarchical representation.\nIn the following, we ﬁrst propose a hierarchical pooling to\ngradually shrink the sequence length and hence reduce the\ncomputational cost. Then, we propose to perform predic-\ntions without the class token. The overview of the proposed\nHVT is shown in Figure 2.\n3.2.1 Hierarchical Pooling\nWe propose to apply hierarchical pooling in ViT for two\nreasons: (1) Recent studies [13, 8] on Transformers show\nthat tokens tend to carry redundant information as it goes\ndeeper. Therefore, it would be beneﬁcial to reduce these\nredundancies through the pooling approaches. (2) The in-\nput sequence projected from image patches in ViT can be\nseen as ﬂattened CNN feature maps with encoded spatial\ninformation, hence pooling from the nearby tokens can be\nanalogous to the spatial pooling methods in CNNs.\nMotivated by the hierarchical pipeline of VGG-style [33]\nand ResNet-style [14] networks, we partition the Trans-\nformer blocks into M stages and apply downsampling op-\neration to each stage to shrink the sequence length. Let\n{b1,b2,...,b M }be the indexes of the ﬁrst block in each\nstage. At the m-th stage, we apply a 1D max pooling oper-\nation with a kernel size of kand stride of sto the output of\nthe Transformer block bm ∈{b1,b2,...,b M }to shrink the\nsequence length.\nNote that the positional encoding is important for a\nTransformer since the positional encoding is able to cap-\nture information about the relative and absolute position of\nthe token in the sequence [37, 3]. In Eq. (1) of ViT, each\npatch is equipped with positional embedding E at the be-\nginning. However, in our HVT, the original positional em-\nbedding E may no longer be meaningful after pooling since\nthe sequence length is reduced after each pooling operation.\nIn this case, positional embedding in the pooled sequence\nneeds to be updated. Moreover, previous work [8] in NLP\nalso ﬁnd it important to complement positional information\nafter changing the sequence length. Therefore, at the m-th\nstage, we introduce an additional learnable positional em-\nbedding Ebm to capture the positional information, which\ncan be formulated as\nˆXbm = MaxPool1D(Xbm ) +Ebm , (5)\nwhere Xbm is the output of the Transformer block bm. We\nthen forward the resulting embeddings ˆXbm into the next\nTransformer block bm + 1.\n3.2.2 Prediction without the Class Token\nPrevious works [11, 36] make predictions by taking the\nclass token as input in classiﬁcation tasks as described in\nEq. (4). However, such structure relies solely on the sin-\ngle class token with limited capacity while discarding the\nremaining sequence that is capable of storing more discrim-\ninative information. To this end, we propose to remove the\nclass token in the ﬁrst place and predict with the remaining\noutput sequence on the last stage.\nSpeciﬁcally, given the output sequence without the class\ntoken on the last stage XL, we ﬁrst apply average pooling,\nthen directly apply an FC layer on the top of the pooled\nembeddings and make predictions. The process can be for-\nmulated as\ny = FC(AvgPool(LN(XL))). (6)\n3.3. Complexity Analysis\nIn this section, we analyse the block-wise compression\nratio with hierarchical pooling. Following ViT [11], we use\nFLOPs to measure the computational cost of a Transformer.\nLet nbe the number of tokens in a sequence anddis the di-\nmension of each token. The FLOPs of a Transformer block\nφBLK (n,d) can be computed by\nφBLK (n,d) =φMSA (n,d) +φMLP (n,d),\n= 12nd2 + 2n2d,\n(7)\nwhere φMSA (n,d) and φMLP (n,d) are the FLOPs of the\nMSA and MLP, respectively. Details about Eq. (7) can be\nfound in the supplementary material.\nWithout loss of generality, suppose that the sequence\nlength n is reduced by half after performing hierarchical\npooling. In this case, the block-wise compression ratio α\ncan be computed by\nα= φBLK (n,d)\nφBLK (n/2,d) = 2 + 2\n12(d/n) + 1. (8)\nClearly, Eq. (8) is monotonic, thus the block-wise com-\npression ratio αis bounded by (2,4), i.e., α∈(2,4).\nResNet50: conv42\nDeiT-S: Linear Projection, N = 196 DeiT-S: Block1, N = 196\nResNet50: conv1\nHVT-S-1: Linear Projection, N = 196 HVT-S-1: Block1, N = 97\nFigure 3: Feature visualization of ResNet50 [14], DeiT-S [36] and our HVT-S-1 trained on ImageNet. DeiT-S and our HVT-\nS-1 correspond to the small setting in DeiT, except that our model applies a pooling operation and performing predictions\nwithout the class token. The resolution of the feature maps from ResNet50 conv1 and conv4 2 are 112 ×112 and 14 ×14,\nrespectively. For DeiT and HVT, the feature maps are reshaped from tokens. For our model, we interpolate the pooled\nsequence to its initial length then reshape it to a 2D map.\n4. Discussions\n4.1. Analysis of Hierarchical Pooling\nIn CNNs, feature maps are usually downsampled to\nsmaller sizes in a hierarchical way [33, 14]. In this pa-\nper, we show that this principle can be applied to ViT mod-\nels by comparing the visualized feature maps from ResNet\nconv4 2, DeiT-S [36] block1 and HVT-S-1 block1 in Fig-\nure 3. From the ﬁgure, in ResNet, the initial feature maps\nafter the ﬁrst convolutional layer contain rich edge informa-\ntion. After feeding the features to consecutive convolutional\nlayers and a pooling layer, the output feature maps tend\nto preserve more high-level discriminative information. In\nDeiT-S, following the ViT structure, although the image res-\nolution for the feature maps has been reduced to 14×14 by\nthe initial linear projection layer, we can still observe clear\nedges and patterns. Then, the features get reﬁned in the\nﬁrst block to obtain sharper edge information. In contrast\nto DeiT-S that reﬁnes features at the same resolution level,\nafter the ﬁrst block, the proposed HVT downsamples the\nhidden sequence through a pooling layer and reduces the\nsequence length by half. We then interpolate the sequence\nback to 196 and reshape it to 2D feature maps. We can ﬁnd\nthat the hidden representations contain more abstract infor-\nmation with high discriminative power, which is similar to\nResNet.\n4.2. Scalability of HVT\nThe computational complexity reduction equips HVT\nwith strong scalability in terms of width/depth/patch\nsize/resolution. Take DeiT-S for an example, the model\nconsists of 12 blocks and 6 heads. Given a 224×224 image\nwith a patch size of 16, the computational cost of DeiT-S is\naround 4.6G FLOPs. By applying four pooling operations,\nour method is able to achieve nearly 3.3 ×FLOPs reduc-\ntion. Furthermore, to re-allocate the reduced FLOPs, we\nmay construct wider or deeper HVT-S, with 11 heads or 48\nblocks, then the overall FLOPs would be around 4.51G and\n4.33G, respectively. Moreover, we may consider a longer\nsequence by setting a smaller patch size or using a larger\nresolution. For example, with a patch size of 8 and an im-\nage resolution of 192×192, the FLOPs for HVT-S is around\n4.35G. Alternatively, enlarging the image resolution into\n384×384 will lead to 4.48G FLOPs. In all of the above\nmentioned cases, the computational costs are still lower\nthan that of DeiT-S while the model capacity is enhanced.\nIt is worth noting that ﬁnding a principled way to scale\nup HVT to obtain the optimal efﬁciency-vs-accuracy trade-\noff remains an open question. At the current stage, we take\nan early exploration by evenly partitioning blocks and fol-\nlowing model settings in DeiT [36] for a fair comparison. In\nfact, the improved scalability of HVT makes it possible for\nusing Neural Architecture Search (NAS) to automatically\nﬁnd optimal conﬁgurations, such as EfﬁcientNet [35]. We\nleave for more potential studies for future work.\n5. Experiments\nCompared methods. To investigate the effectiveness of\nHVT, we compare our method with DeiT [36] and a BERT-\nbased pruning method PoWER-BERT [13]. DeiT is a rep-\nresentative Vision Transformer and PoWER progressively\nprunes unimportant tokens in pretrained BERT models for\ninference acceleration. Moreover, we consider two archi-\ntectures in DeiT for comparisons: HVT-Ti: HVT with the\ntiny setting. HVT-S: HVT with the small setting. For con-\nvenience, we use “Architecture-M” to represent our model\nwith M pooling stages, e.g., HVT-S-1.\nDatasets and Evaluation metrics. We evaluate our\nproposed HVT on two image classiﬁcation benchmark\ndatasets: CIFAR-100 [22] and ImageNet [31]. We measure\nthe performance of different methods in terms of the Top-1\nand Top-5 accuracy. Following DeiT [36], we measure the\ncomputational cost by FLOPs. Moreover, we also measure\nthe model size by the number of parameters (Params).\nImplementation details. For experiments on ImageNet,\nwe train our models for 300 epochs with a total batch size\nof 1024. The initial learning rate is 0.0005. We use AdamW\noptimizer [25] with a momentum of 0.9 for optimization.\nWe set the weight decay to 0.025. For fair comparisons,\nwe keep the same data augmentation strategy as DeiT [36].\nFor the downsampling operation, we use max pooling by\ndefault. The kernel size k and stride s are set to 3 and 2,\nrespectively, chosen by a simple grid search on CIFAR100.\nBesides, all learnable positional embeddings are initialized\nin the same way as DeiT. More detailed settings on the other\nhyper-parameters can be found in DeiT. For experiments\non CIFAR-100, we train our models with a total batch size\nof 128. The initial learning rate is set to 0.000125. Other\nhyper-parameters are kept the same as those on ImageNet.\n5.1. Main Results\nWe compare the proposed HVT with DeiT and PoWER,\nand report the results in Table 1. First, compared to DeiT,\nour HVT achieves nearly 2 ×FLOPs reduction with a hi-\nerarchical pooling. However, the signiﬁcant FLOPs reduc-\ntion also leads to performance degradation in both the tiny\nand small settings. Additionally, the performance drop of\nHVT-S-1 is smaller than that of HVT-Ti-1. For example, for\nHVT-S-1, it only leads to1.80% drop in the Top-1 accuracy.\nIn contrast, it results in 2.56% drop in the Top-1 accuracy\nfor HVT-Ti-1. It can be attributed to that, compared with\nHVT-Ti-1, HVT-S-1 is more redundant with more parame-\nters. Therefore, applying hierarchical pooling to HVT-S-1\n0 50 100 150 200 250 300\nEpoch\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0Training Loss\n0\n20\n40\n60\n80\nTop-1 Acc.(%)\nDeiT-Ti\nScale HVT-Ti-4\nFigure 4: Performance comparisons of DeiT-Ti (1.25G\nFLOPs) and the proposed Scale HVT-Ti-4 (1.39G FLOPs).\nAll the models are evaluated on ImageNet. Solid lines de-\nnote the Top-1 accuracy (y-axis on the right). Dash lines\ndenote the training loss (y-axis on the left).\ncan signiﬁcantly reduce redundancy while maintaining per-\nformance. Second, compared to PoWER, HVT-Ti-1 uses\nless FLOPs while achieving better performance. Besides,\nHVT-S-1 reduces more FLOPs than PoWER, while achiev-\ning slightly lower performance than PoWER. Also note that\nPoWER involves three training steps, while ours is a sim-\npler one-stage training scheme.\nMoreover, we also compare the scaled HVT with DeiT\nunder similar FLOPs. Speciﬁcally, we enlarge the embed-\nding dimensions and add extra heads in HVT-Ti. From Ta-\nble 1 and Figure 4, by re-allocating the saved FLOPs to\nscale up the model, HVT can converge to a better solution\nand yield improved performance. For example, the Top-\n1 accuracy on ImageNet can be improved considerably by\n3.03% in the tiny setting. More empirical studies on the\neffect of model scaling can be found in Section 5.2.\n5.2. Ablation Study\nEffect of the prediction without the class token. To in-\nvestigate the effect of the prediction without the class token,\nwe train DeiT-Ti with and without the class token and show\nthe results in Table 2. From the results, the models without\nthe class token outperform the ones with the class token.\nThe performance gains mainly come from the extra discrim-\ninative information stored in the entire sequence without\nthe class token. Note that the performance improvement on\nCIFAR-100 is much larger than that on ImageNet. It may\nbe attributed that CIFAR-100 is a small dataset, which lacks\nvarieties compared with ImageNet. Therefore, the model\ntrained on CIFAR-100 beneﬁts more from the increase of\nmodel’s discriminative power.\nEffect of different pooling stages. We train HVT-S with\ndifferent pooling stages M ∈{0,1,2,3,4}and show the\nresults in Table 4. Note that HVT-S-0 is equivalent to the\nTable 1: Performance comparisons with DeiT and PoWER on ImageNet. “Embedding Dim” refers to the dimension of\neach token in the sequence. “#Heads” and “#Blocks” are the number of self-attention heads and blocks in Transformer,\nrespectively. “FLOPs” is measured with a224×224 image. “Ti” and “S” are short for the tiny and small settings, respectively.\n“Architecture-M” denotes the model with M pooling stages. “Scale” denotes that we scale up the embedding dimension\nand/or the number of self-attention heads. “DeiT-Ti/S + PoWER” refers to the model that applies the techniques in PoWER-\nBERT [13] to DeiT-Ti/S.\nModel Embedding Dim #Heads #Blocks FLOPs (G) Params (M) Top-1 Acc. (%) Top-5 Acc. (%)\nDeiT-Ti [36] 192 3 12 1.25 5.72 72.20 91.10\nDeiT-Ti + PoWER [13] 192 3 12 0.80 5.72 69.40(-2.80) 89.20(-1.90)\nHVT-Ti-1 192 3 12 0.64 5.74 69.64(-2.56) 89.40(-1.70)\nScale HVT-Ti-4 384 6 12 1.39 22.12 75.23(+3.03) 92.30(+1.20)\nDeiT-S [36] 384 6 12 4.60 22.05 79.80 95.00\nDeiT-S + PoWER [13] 384 6 12 2.70 22.05 78.30(-1.50) 94.00(-1.00)\nHVT-S-1 384 6 12 2.40 22.09 78.00(-1.80) 93.83(-1.17)\nTable 2: Effect of the prediction without the class token. “CLS” denotes the class token.\nModel FLOPs (G) Params (M) ImageNet CIFAR-100\nTop-1 Acc. (%) Top-5 Acc. (%)Top-1 Acc. (%) Top-5 Acc. (%)\nDeiT-Ti with CLS 1.25 5.72 72.20 91.10 64.49 89.27\nDeiT-Ti without CLS 1.25 5.72 72.42(+0.22) 91.55(+0.45) 65.93(+1.44) 90.33(+1.06)\nTable 3: Performance comparisons on HVT-S-4 with three\ndownsampling operations: convolution, max pooling and\naverage pooling. We report the Top-1 and Top-5 accuracy\non CIFAR-100.\nModel Operation FLOPs (G) Params (M)Top-1 Acc. (%) Top-5 Acc. (%)HVT-S Conv 1.47 23.54 69.75 92.12HVT-S Avg 1.39 21.77 70.38 91.39HVT-S Max 1.39 21.77 75.43 93.56\nTable 4: Performance comparisons on HVT-S with different\npooling stages M. We report the Top-1 and Top-5 accuracy\non CIFAR-100.\nM FLOPs Params ImageNet CIFAR100\nTop-1 (%) Top-5 (%)Top-1 (%) Top-5 (%)\n0 4.57 21.70 80.39 95.13 71.99 92.44\n1 2.40 21.74 78.00 93.83 74.27 93.07\n2 1.94 21.76 77.36 93.55 75.37 93.69\n3 1.62 21.77 76.32 92.90 75.22 93.90\n4 1.39 21.77 75.23 92.30 75.43 93.56\nDeiT-S without the class token. With the increase of M,\nHVT-S achieves better performance with decreasing FLOPs\non CIFAR-100, while on ImageNet we observe the accu-\nracy degrades. One possible reason is that HVT-S is very\nredundant on CIFAR-100, such that pooling acts as a reg-\nularizer to avoid the overﬁtting problem and improves the\ngeneralization of HVT on CIFAR-100. On ImageNet, we\nassume HVT is less redundant and a better scaling strategy\nis required to improve the performance.\nEffect of different downsampling operations. To inves-\ntigate the effect of different downsampling operations, we\ntrain HVT-S-4 with three downsampling strategies: convo-\nlution, average pooling and max pooling. As Table 3 shows,\ndownsampling with convolution performs the worst even it\nTable 5: Performance comparisons on HVT-S-4 with differ-\nent number of Transformer blocks. We report the Top-1 and\nTop-5 accuracy on CIFAR-100.\n#Blocks FLOPs (G) Params (M)Top-1 Acc. (%) Top-5 Acc. (%)\n12 1.39 21.77 75.43 93.56\n16 1.72 28.87 75.32 93.30\n20 2.05 35.97 75.35 93.35\n24 2.37 43.07 75.04 93.39\nTable 6: Performance comparisons on HVT-Ti-4 with dif-\nferent number of self-attention heads. We report the Top-1\nand Top-5 accuracy on CIFAR-100.\n#Heads FLOPs (G) Params (M)Top-1 Acc. (%) Top-5 Acc. (%)\n3 0.38 5.58 69.51 91.78\n6 1.39 21.77 75.43 93.56\n12 5.34 86.01 76.26 93.39\n16 9.39 152.43 76.30 93.16\nintroduces additional FLOPs and parameters. Besides, av-\nerage pooling performs slightly better than convolution in\nterms of the Top-1 accuracy. Compared with the two set-\ntings, HVT-S-4 with max pooling performs much better as\nit signiﬁcantly surpasses average pooling by 5.05% in the\nTop-1 accuracy and 2.17% in the Top-5 accuracy. The re-\nsult is consistent with the common sense [2] that max pool-\ning performs well in a large variety of settings. To this end,\nwe use max pooling in all other experiments by default.\nEffect of model scaling. One of the important advan-\ntages of the proposed hierarchical pooling is that we can\nre-allocate the saved computational cost for better model ca-\npacity by constructing a model with a wider, deeper, larger\nresolution or smaller patch size conﬁguration. Similar to the\nCNNs literature [14, 44, 46], we study the effect of model\nscaling in the following.\nBased on HVT-S-4, we ﬁrst construct deeper models by\nincreasing the number of blocks in Transformers. Specif-\nically, we train 4 models with different number of blocks\nL ∈{12,16,20,24}. As a result, each pooling stage for\ndifferent models would have 3, 4, 5, and 6 blocks, respec-\ntively. We train 4 models on CIFAR-100 and report the re-\nsults in Table 5. From the results, we observe no more gains\nby stacking more blocks in HVT.\nBased on HVT-Ti-4, we then construct wider models by\nincreasing the number of self-attention heads. To be spe-\nciﬁc, we train 4 models with different numbers of self-\nattention heads, i.e., 3, 6, 12, and 16, on CIFAR-100 and\nreport the results in Table 6. From the results, our mod-\nels achieve better performance with the increase of width.\nFor example, the model with 16 self-attention heads out-\nperforms those with 3 self-attention heads by 6.79% in the\nTop-1 accuracy and 1.38% in the Top-5 accuracy.\nBased on HVT-S-4, we further construct models with\nlarger input image resolutions. Speciﬁcally, we train 4 mod-\nels with different input image resolutions, i.e., 160, 224,\n320, and 384, on CIFAR-100 and report the results in Ta-\nble 7. From the results, with the increase of image resolu-\ntion, our models achieve better performance. For example,\nthe model with the resolution of 384 outperforms those with\nthe resolution of 160 by 2.47% in the Top-1 accuracy and\n1.12% in the Top-5 accuracy. Nevertheless, increasing im-\nage resolutions also leads to high computational cost. To\nmake a trade-off between computational cost and accuracy,\nwe set the image resolution to 224 by default.\nWe ﬁnally train HVT-S-4 with different patch sizes P ∈\n{8,16,32}and show the results in Table 8. From the re-\nsults, HVT-S-4 performs better with the decrease of patch\nsize. For example, when the patch size decreases from 32\nto 8, our HVT-S achieves 9.14% and 4.03% gain in terms of\nthe Top-1 and Top-5 accuracy. Intuitively, a smaller patch\nsize leads to ﬁne-grained image patches and helps to learn\nhigh-resolution representations, which is able to improve\nthe classiﬁcation performance. However, with a smaller\npatch size, the patch sequence will be longer, which sig-\nniﬁcantly increases the computational cost. To make a bal-\nance between the computational cost and accuracy, we set\nthe patch size to 16 by default.\nExploration on 2D pooling. Compared to 1D pooling,\n2D pooling brings more requirements. For example, it re-\nquires a smaller patch size to ensure a sufﬁcient sequence\nlength. Correspondingly, it is essential to reduce the heads\nat the early stages to save FLOPs and memory consumption\nfrom high-resolution feature maps. Besides, it also requires\nto vary the blocks at each stage to control the overall model\ncomplexity. In Table 9, we apply 2D pooling to HVT-S-2\nTable 7: Performance comparisons on HVT-S-4 with dif-\nferent image resolutions. We report the Top-1 and Top-5\naccuracy on CIFAR-100.\nResolution FLOPs (G) Params (M)Top-1 Acc. (%) Top-5 Acc. (%)\n160 0.69 21.70 73.84 92.90\n224 1.39 21.77 75.43 93.56\n320 3.00 21.92 75.54 94.18\n384 4.48 22.06 76.31 94.02\nTable 8: Performance comparisons on HVT-S-4 with differ-\nent patch sizes P. We report the Top-1 and Top-5 accuracy\non CIFAR-100.\nP FLOPs (G) Params (M) Top-1 Acc. (%) Top-5 Acc. (%)\n8 6.18 21.99 77.29 94.22\n16 1.39 21.77 75.43 93.56\n32 0.37 22.55 68.15 90.19\nTable 9: Effect of 2D pooling on HVT-S-2. We report the\nTop-1 and Top-5 accuracy on CIFAR-100. For HVT-S-2,\nwe apply 2D max pooling and use a patch size of 8.\nModel FLOPs (G) Params (M) Top-1 Acc. (%) Top-5 Acc. (%)\nDeiT-S 4.60 21.70 71.99 92.44\nHVT-S-2 (2D)4.62 21.80 77.58 94.40\nand compare it with DeiT-S. The results show that HVT-S-\n2 with 2D pooling outperforms DeiT-S on CIFAR100 by a\nlarge margin with similar FLOPs. In this case, we assume\nthat HVT can achieve promising performance with a ded-\nicated scaling scheme for 2D pooling. We will leave this\nexploration for future work.\n6. Conclusion and Future Work\nIn this paper, we have presented a Hierarchical Visual\nTransformer, termed HVT, for image classiﬁcation. In par-\nticular, the proposed hierarchical pooling can signiﬁcantly\ncompress the sequential resolution to save computational\ncost in a simple yet effective form. More importantly, this\nstrategy greatly improves the scalability of visual Trans-\nformers, making it possible to scale various dimensions -\ndepth, width, resolution and patch size. By re-allocating\nthe saved computational cost, we can scale up these dimen-\nsions for better model capacity with comparable or fewer\nFLOPs. Moreover, we have empirically shown that the vi-\nsual tokens are more important than the single class token\nfor class prediction. Note that the scope of this paper only\ntargets designing our HVT as an encoder. Future works may\ninclude extending our HVT model to decoder and to solve\nother mainstream CV tasks, such as object detection and se-\nmantic/instance segmentation. In addition, it would be in-\nteresting to ﬁnd a principled way to scale up HVT that can\nachieve better accuracy and efﬁciency.\n7. Acknowledgements\nThis research is partially supported by Monash FIT\nStart-up Grant and Sensetime Gift Fund.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016. 3\n[2] Y-Lan Boureau, Jean Ponce, and Yann LeCun. A theoretical\nanalysis of feature pooling in visual recognition. In ICML,\npages 111–118, 2010. 7\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV, 2020. 1,\n2, 4\n[4] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.\nGenerating long sequences with sparse transformers. arXiv\npreprint arXiv:1904.10509, 2019. 1, 3\n[5] Krzysztof Choromanski, Valerii Likhosherstov, David Do-\nhan, Xingyou Song, Andreea Gane, Tam ´as Sarl ´os, Peter\nHawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser,\nDavid Belanger, Lucy Colwell, and Adrian Weller. Rethink-\ning attention with performers. In ICLR, 2021. 3\n[6] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xi-\naolin Wei, Huaxia Xia, and Chunhua Shen. Conditional po-\nsitional encodings for vision transformers. Arxiv preprint\n2102.10882, 2021. 2\n[7] Jean-Baptiste Cordonnier, Andreas Loukas, and Martin\nJaggi. On the relationship between self-attention and con-\nvolutional layers. In ICLR, 2020. 2\n[8] Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le.\nFunnel-transformer: Filtering out sequential redundancy for\nefﬁcient language processing. In NeurIPS, 2020. 3, 4\n[9] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell,\nQuoc Le, and Ruslan Salakhutdinov. Transformer-xl: Atten-\ntive language models beyond a ﬁxed-length context. In ACL,\n2019. 1\n[10] J. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL-HLT, 2019.\n1, 3\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. ICLR, 2021. 1, 2, 3, 4\n[12] Mitchell A. Gordon, Kevin Duh, and Nicholas Andrews.\nCompressing BERT: studying the effects of weight pruning\non transfer learning. In RepL4NLP@ACL, pages 143–155,\n2020. 1, 2\n[13] Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje,\nVenkatesan T. Chakaravarthy, Yogish Sabharwal, and Ashish\nVerma. Power-bert: Accelerating BERT inference via pro-\ngressive word-vector elimination. In ICML, 2020. 3, 4, 6,\n7\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016. 2, 3, 4, 5, 8\n[15] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\nunits (gelus). arXiv: Learning, 2016. 4\n[16] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim\nSalimans. Axial attention in multidimensional transformers.\narXiv preprint arXiv:1912.12180, 2019. 2\n[17] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local\nrelation networks for image recognition. In ICCV, 2019. 2\n[18] Zilong Huang, Xinggang Wang, Lichao Huang, Chang\nHuang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross\nattention for semantic segmentation. In ICCV, 2019. 2\n[19] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao\nChen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distill-\ning BERT for natural language understanding. In EMNLP,\npages 4163–4174, 2020. 1\n[20] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and\nFranc ¸ois Fleuret. Transformers are rnns: Fast autoregressive\ntransformers with linear attention. In ICML, pages 5156–\n5165. PMLR, 2020. 1, 3\n[21] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Re-\nformer: The efﬁcient transformer. In ICLR, 2020. 3\n[22] Alex Krizhevsky and Geoffrey Hinton. Learning multiple\nlayers of features from tiny images. Master’s thesis, Depart-\nment of Computer Science, University of Toronto, 2009. 6\n[23] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and\nHans Peter Graf. Pruning ﬁlters for efﬁcient convnets. In\nICLR, 2017. 1\n[24] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie. Feature pyramid\nnetworks for object detection. In CVPR, pages 2117–2125,\n2017. 2\n[25] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In ICLR, 2019. 6\n[26] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen\nheads really better than one? In NeurIPS, pages 14014–\n14024, 2019. 2\n[27] Niki Parmar, Prajit Ramachandran, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jon Shlens. Stand-alone self-\nattention in vision models. In NeurIPS, pages 68–80, 2019.\n2\n[28] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz,\nNoah A Smith, and Lingpeng Kong. Random feature atten-\ntion. In ICLR, 2021. 1\n[29] Gabriele Prato, Ella Charlaix, and Mehdi Rezagholizadeh.\nFully quantized transformer for machine translation. In\nTrevor Cohn, Yulan He, and Yang Liu, editors, EMNLP,\npages 1–14, 2020. 1\n[30] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and\nTimothy P Lillicrap. Compressive transformers for long-\nrange sequence modelling. In ICLR, 2020. 1\n[31] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al. Imagenet large\nscale visual recognition challenge. IJCV, 115(3):211–252,\n2015. 6\n[32] Victor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. Distilbert, a distilled version of BERT:\nsmaller, faster, cheaper and lighter. InNeurIPS EMC2 Work-\nshop, 2019. 2\n[33] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. InICLR,\n2015. 2, 4, 5\n[34] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon\nShlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck\ntransformers for visual recognition. In CVPR, 2021. 2\n[35] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model\nscaling for convolutional neural networks. In ICML, 2019.\n1, 5\n[36] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herve Jegou. Training\ndata-efﬁcient image transformers & distillation through at-\ntention. In ICML, pages 10347–10357, 2021. 1, 2, 4, 5, 6,\n7\n[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 1,\n3, 4\n[38] Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng\nZhu, Chuang Gan, and Song Han. HAT: hardware-aware\ntransformers for efﬁcient natural language processing. In\nACL, pages 7675–7688, 2020. 3\n[39] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and\nLiang-Chieh Chen. Max-deeplab: End-to-end panoptic seg-\nmentation with mask transformers. In CVPR, 2021. 1\n[40] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam,\nAlan L. Yuille, and Liang-Chieh Chen. Axial-deeplab:\nStand-alone axial-attention for panoptic segmentation. In\nECCV, 2020. 2\n[41] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and\nHao Ma. Linformer: Self-attention with linear complexity.\narXiv preprint arXiv:2006.04768, 2020. 1, 3\n[42] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra-\nmid vision transformer: A versatile backbone for dense pre-\ndiction without convolutions. In ICCV, 2021. 3\n[43] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen,\nBaoshan Cheng, Hao Shen, and Huaxia Xia. End-to-end\nvideo instance segmentation with transformers. In CVPR,\n2021. 1\n[44] Zifeng Wu, Chunhua Shen, and Anton van den Hengel.\nWider or deeper: Revisiting the resnet model for visual\nrecognition. arXiv preprint arXiv:1611.10080, 2016. 8\n[45] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\nto-token vit: Training vision transformers from scratch on\nimagenet. In ICCV, 2021. 2\n[46] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-\nworks. In BMVC, 2016. 8\n[47] Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen,\nXin Jiang, and Qun Liu. Ternarybert: Distillation-aware\nultra-low bit BERT. In EMNLP, pages 509–521, 2020. 2\n[48] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. Deformable DETR: deformable transformers\nfor end-to-end object detection. ICLR, 2021. 1, 2\nAppendix\nWe organize our supplementary material as follows.\n• In Section S1, we elaborate on the components of\na Transformer block, including the multi-head self-\nattention layer (MSA) and the position-wise multi-\nlayer perceptron (MLP).\n• In Section S2, we provide details for the FLOPs calcu-\nlation of a Transformer block.\nS1. Transformer Block\nS1.1. Multi-head Self-Attention\nLet X ∈RN×D be the input sentence, where N is the\nsequence length and D the embedding dimension. First, a\nself-attention layer computes query, key and value matrices\nfrom X using linear transformations\n[Q,K,V] =XWqkv, (9)\nwhere Wqkv ∈RD×3Dh is a learnable parameter andDh is\nthe dimension of each self-attention head. Next, the atten-\ntion map A can be calculated by scaled inner product from\nQ and K and normalized by a softmax function\nA = Softmax(QK⊤/\n√\nDh), (10)\nwhere A ∈ RN×N and Aij represents for the attention\nscore between the Qi and Kj. Then, the self-attention op-\neration is applied on the value vectors to produce an output\nmatrix\nO = AV, (11)\nwhere O ∈RN×Dh . For a multi-head self-attention layer\nwith D/Dh heads, the outputs can be calculated by a linear\nprojection for the concatenated self-attention outputs\nX\n′\n= [O1; O2; ...; OD/Dh ]Wproj, (12)\nwhere Wproj ∈RD×D is a learnable parameter and [·] de-\nnotes the concatenation operation.\nS1.2. Position-wise Multi-Layer Perceptron\nLet X\n′\nbe the output from the MSA layer. An MLP layer\nwhich contains two fully-connected layers with a GELU\nnon-linearity can be represented by\nX = GELU(X\n′\nWfc1)Wfc2, (13)\nwhere Wfc1 ∈RD×4D and Wfc2 ∈R4D×D are learnable\nparameters.\nS2. FLOPs of a Transformer Block\nWe denoteφ(n,d) as a function of FLOPs with respect to\nthe sequence length nand the embedding dimension d. For\nan MSA layer, The FLOPs mainly comes from four parts:\n(1) The projection of Q,K,V matrices φqkv(n,d) = 3nd2.\n(2) The calculation of the attention map φA(n,d) = n2d.\n(3) The self-attention operation φO(n,d) = n2d. (4)\nAnd ﬁnally, a linear projection for the concatenated self-\nattention outputs φproj(n,d) =nd2. Therefore, the overall\nFLOPs for an MSA layer is\nφMSA (n,d) =φqkv(n,d) +φA(n,d) +φO(n,d) +φproj(n,d)\n= 3nd2 + n2d+ n2d+ nd2\n= 4nd2 + 2n2d.\n(14)\nFor an MLP layer, the FLOPs mainly comes from two\nfully-connected (FC) layers. The ﬁrst FC layer fc1 is used\nto project each token from Rd to R4d. The next FC layer\nfc2 projects each token back to Rd. Therefore, the FLOPs\nfor an MLP layer is\nφMLP (n,d) =φfc1(n,d)+φfc2(n,d) = 4nd2+4nd2 = 8nd2.\n(15)\nBy combining Eq. (14) and Eq. (15), we can get the total\nFLOPs of one Transformer block\nφBLK (n,d) =φMSA (n,d)+φMLP (n,d) = 12nd2+2n2d.\n(16)"
}