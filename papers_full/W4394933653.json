{
    "title": "Joint transformer architecture in brain 3D MRI classification: its application in Alzheimer’s disease classification",
    "url": "https://openalex.org/W4394933653",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5093068999",
            "name": "Sait Alp",
            "affiliations": [
                "Erzurum Technical University"
            ]
        },
        {
            "id": "https://openalex.org/A5083608225",
            "name": "Taymaz Akan",
            "affiliations": [
                "Louisiana State University Health Sciences Center Shreveport"
            ]
        },
        {
            "id": "https://openalex.org/A5108946292",
            "name": "Md. Shenuarin Bhuiyan",
            "affiliations": [
                "Louisiana State University Health Sciences Center Shreveport"
            ]
        },
        {
            "id": "https://openalex.org/A5044253009",
            "name": "Elizabeth A. Disbrow",
            "affiliations": [
                "Louisiana State University Health Sciences Center Shreveport"
            ]
        },
        {
            "id": "https://openalex.org/A5088027897",
            "name": "Steven A. Conrad",
            "affiliations": [
                "Louisiana State University Health Sciences Center Shreveport"
            ]
        },
        {
            "id": "https://openalex.org/A5040827308",
            "name": "John A. Vanchiere",
            "affiliations": [
                "Louisiana State University Health Sciences Center Shreveport"
            ]
        },
        {
            "id": "https://openalex.org/A5013485864",
            "name": "Christopher G. Kevil",
            "affiliations": [
                "Louisiana State University Health Sciences Center Shreveport"
            ]
        },
        {
            "id": "https://openalex.org/A5033947768",
            "name": "Mohammad Alfrad Nobel Bhuiyan",
            "affiliations": [
                "Louisiana State University Health Sciences Center Shreveport"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3201042424",
        "https://openalex.org/W2907148404",
        "https://openalex.org/W2150585091",
        "https://openalex.org/W4281730963",
        "https://openalex.org/W2467160851",
        "https://openalex.org/W4312128797",
        "https://openalex.org/W4384522480",
        "https://openalex.org/W3216798277",
        "https://openalex.org/W2598525681",
        "https://openalex.org/W4226206033",
        "https://openalex.org/W2953843471",
        "https://openalex.org/W2978285803",
        "https://openalex.org/W2960986212",
        "https://openalex.org/W4312749457",
        "https://openalex.org/W2774698027",
        "https://openalex.org/W2964171289",
        "https://openalex.org/W4372259758",
        "https://openalex.org/W4285011955",
        "https://openalex.org/W4224994441",
        "https://openalex.org/W4318566866",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2078524519",
        "https://openalex.org/W4230920194",
        "https://openalex.org/W2906155095",
        "https://openalex.org/W2995495466",
        "https://openalex.org/W4226268736",
        "https://openalex.org/W4283029781",
        "https://openalex.org/W2794356790",
        "https://openalex.org/W2905035821",
        "https://openalex.org/W2586179392",
        "https://openalex.org/W2748553987",
        "https://openalex.org/W2885139383",
        "https://openalex.org/W2908469802",
        "https://openalex.org/W4205597106",
        "https://openalex.org/W4206730042",
        "https://openalex.org/W2969438869",
        "https://openalex.org/W3100125480",
        "https://openalex.org/W2900386946"
    ],
    "abstract": "Abstract Alzheimer’s disease (AD), a neurodegenerative disease that mostly affects the elderly, slowly impairs memory, cognition, and daily tasks. AD has long been one of the most debilitating chronic neurological disorders, affecting mostly people over 65. In this study, we investigated the use of Vision Transformer (ViT) for Magnetic Resonance Image processing in the context of AD diagnosis. ViT was utilized to extract features from MRIs, map them to a feature sequence, perform sequence modeling to maintain interdependencies, and classify features using a time series transformer. The proposed model was evaluated using ADNI T1-weighted MRIs for binary and multiclass classification. Two data collections, Complete 1Yr 1.5T and Complete 3Yr 3T, from the ADNI database were used for training and testing. A random split approach was used, allocating 60% for training and 20% for testing and validation, resulting in sample sizes of (211, 70, 70) and (1378, 458, 458), respectively. The performance of our proposed model was compared to various deep learning models, including CNN with BiL-STM and ViT with Bi-LSTM. The suggested technique diagnoses AD with high accuracy (99.048% for binary and 99.014% for multiclass classification), precision, recall, and F-score. Our proposed method offers researchers an approach to more efficient early clinical diagnosis and interventions.",
    "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2024) 14:8996  | https://doi.org/10.1038/s41598-024-59578-3\nwww.nature.com/scientificreports\nJoint transformer architecture \nin brain 3D MRI classification: its \napplication in Alzheimer’s disease \nclassification\nSait Alp 1, Taymaz Akan 2,3, Md. Shenuarin Bhuiyan 4, Elizabeth A. Disbrow 3,5,6,7, \nSteven A. Conrad 2,8, John A. Vanchiere 5,8, Christopher G. Kevil 4,9 & \nMohammad A. N. Bhuiyan 2,3*\nAlzheimer’s disease (AD), a neurodegenerative disease that mostly affects the elderly, slowly \nimpairs memory, cognition, and daily tasks. AD has long been one of the most debilitating chronic \nneurological disorders, affecting mostly people over 65. In this study, we investigated the use of \nVision Transformer (ViT) for Magnetic Resonance Image processing in the context of AD diagnosis. \nViT was utilized to extract features from MRIs, map them to a feature sequence, perform sequence \nmodeling to maintain interdependencies, and classify features using a time series transformer. The \nproposed model was evaluated using ADNI T1-weighted MRIs for binary and multiclass classification. \nTwo data collections, Complete 1Yr 1.5T and Complete 3Yr 3T, from the ADNI database were used \nfor training and testing. A random split approach was used, allocating 60% for training and 20% for \ntesting and validation, resulting in sample sizes of (211, 70, 70) and (1378, 458, 458), respectively. \nThe performance of our proposed model was compared to various deep learning models, including \nCNN with BiL-STM and ViT with Bi-LSTM. The suggested technique diagnoses AD with high accuracy \n(99.048% for binary and 99.014% for multiclass classification), precision, recall, and F-score. Our \nproposed method offers researchers an approach to more efficient early clinical diagnosis and \ninterventions.\nKeywords Alzheimer’s disease, MRI, Transfer learning, Sequence classification, Vision transformer\nAlzheimer’s disease (AD) is distinguished by the accumulation of aberrant protein deposits in the brain, known \nas plaques and tangles, which result in the death of nerve cells and the degeneration of brain tissue. Neural \ndegeneration reduces cognitive function and causes mood and behavior  changes1,2. AD is typically categorized in \nthree  stages3,4. The first stage is the preclinical stage, characterized by brain, blood, and cerebrospinal fluid (CSF) \nabnormalities without outward  signs5. It is believed that AD pathology begins at least 20 years before symptoms \n appear6. The second stage of the disease is referred to as mild cognitive impairment (MCI), which involves \ncognitive impairment confined to a single cognitive domain, usually memory. Dementia, the final stage of the \ndisease, is defined as a cognitive disturbance in more than one domain, often memory and executive function, \nwith substantial interference with daily life activities.\nOPEN\n1Department of Computer Engineering, Erzurum Technical University, Erzurum, Turkey. 2Division of Clinical \nInformatics, Department of Medicine, Louisiana State University Health Sciences Center - Shreveport, \nShreveport, LA 71103-4228, USA. 3Center for Brain Health, Louisiana State University Health Sciences Center \n- Shreveport, Shreveport, LA 71103-4228, USA. 4Department of Pathology and Translational Pathobiology, \nLouisiana State University Health Sciences Center - Shreveport, Shreveport, LA 71103-4228, USA. 5Department \nof Pharmacology, Toxicology and Neuroscience, Louisiana State University Health Sciences Center - Shreveport, \nShreveport, LA 71103-4228, USA. 6Department of Neurology, Louisiana State University Health Sciences Center \n- Shreveport, Shreveport, LA 71103-4228, USA. 7Department of Psychiatry, Louisiana State University Health \nSciences Center - Shreveport, Shreveport, LA 71103-4228, USA. 8Department of Pediatrics, Louisiana State \nUniversity Health Sciences Center - Shreveport, Shreveport, LA 71103-4228, USA. 9Department of Molecular and \nCellular Physiology, Louisiana State University Health Sciences Center - Shreveport, Shreveport, LA 71103-4228, \nUSA. *email: Nobel.Bhuiyan@lsuhs.edu\n2\nVol:.(1234567890)Scientific Reports |         (2024) 14:8996  | https://doi.org/10.1038/s41598-024-59578-3\nwww.nature.com/scientificreports/\nWith the recent approval of new drugs for early AD intervention, early detection of AD and differentia-\ntion of MCI have become of primary importance for successful disease treatment and  management7,8, and to \nslow disease progression and improve the quality of life for those with  AD9,10. AD classification is one of the \nmost challenging problems neurologists  face1. Advances in computer-aided diagnosis (CAD) systems based \non neuroimaging data tools have improved classification. CAD systems can be divided into conventional and \ndeep learning-based techniques. Most traditional approaches to image analysis employ a four-stage pipeline of \npre-processing, segmentation, feature extraction, and  classification9. Deep learning (DL) algorithms have an \nadvantage over conventionally based methods because they require little or no image pre-processing. They can \nautomatically infer an optimal data representation from raw images without requiring prior feature selection, \nresulting in a more objective and less biased  process10–14. CNN-based architectures are used extensively for \nmedical image analysis. They have been applied to 2D and 3D ultrasound and MRI  images15 and are the deep \nmodels used most frequently to detect  AD15–18. However, a 3D MRI brain image consists of stacked 2D data \nslices. 3D-CNN model to learn spatiotemporal features would be optimal, which is impossible with 2D CNN. \nBut, because it requires many parameters and a high amount of computation, the 3D model cannot be used to \nconstruct deep  models15.\nAlthough transformer architecture dominates natural language processing, its use in medical imaging has \nbeen  limited19. However, Vision Transformer (ViT) has recently gained popularity due to its impressive results \nin various medical imaging tasks, including image classification, object detection, and semantic  segmentation20. \nViT took note of the scaling success of Transformers in NLP and applied a standard Transformer to images with \nminimal modifications. Transformer-based architectures have also been used in medical image  analysis21,22,24. \nViT has recently demonstrated superior performance in many computer-vision tasks, making it a viable alter -\nnative to CNN as a network  architecture 23. CNNs collect features gradually from local to global by adding \nmore convolutional layers. ViT, on the other hand, uses a multi-headed self-attention mechanism to capture \nlong-range dependencies. For this approach, the model equally weights all elements in the input sequence for \nsuperior performance. ViT extracts features across the entire image without degrading image resolution, pre-\nventing spatial loss from information skipping. Thus, ViT is ideal for brain imaging analysis. The self-attention \nstrategy of ViT has the capacity to accurately capture the interdependencies between various dispersed networks \nof brain  regions21.\nViT is based on the concept of Transformers from natural language processing (NLP) applied to medical \nimages. It uses a standard Transformer architecture, with minimal modifications, to process MRI images instead \nof text. Other neural network models, which process the image sequences sequentially (RNN) or in parallel \n(CNN), require more time to train and infer the results, and fail to control for long-term dependencies among \nthe image  layers24. The joint transformer handles long-range dependencies, avoids recursion, and allows parallel \ncomputation to reduce training time and avoid performance drops due to long-range  dependencies24.\nViT has outperformed CNN in several computer-vision tasks, giving it an appropriate network architectural \noption. We were motivated to use ViT’s benefits to diagnose AD patients using 3D MRIs. The lack of large-scale \ndatasets in this field is one of the major obstacles to training deep models from scratch. The model can adapt to \nthe smaller target dataset by using transfer learning to learn from a larger dataset. The 3D data from a plane was \ndivided into a 2D slice array in order to benefit from transfer learning using a pre-trained ViT. Furthermore, \nit should be possible for slice-based methods to track the dependencies of related features across slices. The \nsequence classification task uses a time series classification with a transformer to get around this issue. We have \ncombined time series transformers and pre-trained ViT to create a deep learning-based classification system \nfor AD.\nThe goal of this article is twofold with respect to the proposed ViT: (1) evaluate the predictive performance of \nViT combined with a transformer neural network; and (2) capture long-range dependencies and the global con-\ntext of MRIs, allow parallel computation to reduce training time, and avoid performance drops due to long-range \ndependencies. We propose testing the hypothesis that ViT with a time series transformer performs better for AD \npatient classification based on MRI by using the self-attention mechanism to capture long-range dependencies \nand contextual relationships from MRI images.\nMethods\nWe used ViT to derive T1-weighted MRI slice attributes and a transformer neural network model for sequential \nfeature classification, maintaining inter-association between the slices. The transformer neural network archi -\ntecture and the ViT architecture for the sequential feature classification model are explained in Supplementary \nSections 1 and 1.1. The summary of the ADNI dataset and steps of the proposed method are described in Sec-\ntions 2.1–2.4, and the pipeline of the proposed method’s architecture is shown in Fig. 1.\nDataset\nData used in this study was obtained from the Alzheimer’s disease Neuroimaging Initiative (ADNI) database \n(adni.loni.usc.edu). The ADNI was launched in 2003 as a public–private partnership led by Principal Investi-\ngator Michael W . Weiner, MD. The primary goal of ADNI has been to test whether serial magnetic resonance \nimaging (MRI), positron emission tomography (PET), other biological markers, and clinical and neuropsycho-\nlogical assessment can be combined to measure the progression of mild cognitive impairment (MCI) and early \nAlzheimer’s disease (AD).\nWe performed binary and multiclass classification using T1-weighted 3D MRI scans from  ADNI25,26. We \ntrained and tested the models on subjects who had scans taken at screening and at 6- and 12-month visits \n(ADNI1: Complete 1Yr 1.5T data) and on subjects who had scans taken at screening, and at 6 months, 1 year, \nand 18 months (MCI only), and 2 and 3 years (normal and MCI only) (ADNI1: Complete 3Yr 3T data). We tested \n3\nVol.:(0123456789)Scientific Reports |         (2024) 14:8996  | https://doi.org/10.1038/s41598-024-59578-3\nwww.nature.com/scientificreports/\nthe model performance in three types of MRI scans (from the top down, axial plane; from front to back, coronal \nplane; and side to side, sagittal plane). We randomly split each data set into 60% training, 20% testing, and 20% \nvalidation sets. We performed extensive experiments on binary (NC/AD) and multi-classification tasks (NC/\nMCI/AD) to assess the proposed method. The details of model variants are listed in Supplementary Table S1. \nThe configuration of training parameters is summarized in Supplementary Table S2. Both binary and multi-\nclassifications were performed for all sagittal, coronal, and axial planes. We also implemented different baseline \narchitectures to make comparisons with the proposed method. The details of baseline model variants are listed \nin Supplementary Table S3. The Complete 1Yr 1.5T data results are provided in Supplementary Section 3. The \ndescriptive statistics of the ADNI data are provided in Table 1.\nMRI Pre-processing\nT1-weighted MRI scans were standardized in Montreal Neurological Institute (MNI) space. For comparison \nacross subjects each skull was stripped using Statistical Parametric Mapping 12 (SPM12) 27 and Computational \nAnatomy Toolbox (CAT12; http:// www. neuro. uni- jena. de/ cat/) in MATLAB (see Fig. 2).\nHandling 3D MRI using 2D ViT\nViT models are pre-trained using a vast number of 2D data  (mageNet21K28 and 21,843 classes at a resolution \nof 224 × 224 pixels. By taking advantage of transfer learning with a pre-trained 2D network model, we split the \nstandardized 3D MRI into 2D slices. In the 2D slices, the sizes of each slice in the axial, coronal, and sagittal \nplanes were 113 × 137 , 113 × 113 , and 137 × 113 pixels, respectively.\nEach slice was then converted to an image of  224 × 224 pixels, and each slice was divided into 14 × 14 patches \nwhere each patch was 16 × 16 pixels (see Fig. 3).\nThese patches, which were flattened into a vector as a sequence of 196 patches, were considered to be an input \ntoken for the model, called the self-attention model. Later, we applied a multiple-layer self-attention model and \nfeed-forward neural network to process the sequence of patched pixels and perform the feature selection. The \nproposed model comprised multiple transformer blocks, each applying the multi-head attention layer as a self-\nattention mechanism to the patch sequence. Finally, the output of the transformer encoder was processed via a \nFigure 1.  The proposed pipeline of the ViT-TST. MRI images were pre-processed using CAT12 (image \nregistration to standardize the images and skull stripping to reduce biases by ensuring consistent voxel \nintensities). ViT was used from each plane to derive slice attributes. Finally, the time series transformer was used \nto classify the feature sequences.\nTable 1.  The details of data collections. # NC, cognitively normal; MCI, mild cognitive impairment; AD, \nAlzheimer’s disease.\nImage scans# NC MCI AD Male Female Age (years)\nADNI1: Complete 3Yr 3T 351 129 145 77 194 157 75 ± 7.07\nADNI1: Complete 1Yr 1.5T 2294 705 1113 476 1341 953 75 ± 6.6\n4\nVol:.(1234567890)Scientific Reports |         (2024) 14:8996  | https://doi.org/10.1038/s41598-024-59578-3\nwww.nature.com/scientificreports/\nclassifier head to produce the final class probability output. Figure 4 provides a visual summary of the proposed \nmodel.\nSequence classification using a time series transformer\nAfter features extraction using ViT, a transformer  model24 was used as a tensor for time-series classification to \nmaintain the relationships between the slices. Due to the self-attention mechanisms of transformer-based time \nseries classifications, long-term dependencies between time steps (features of each slice) were captured more \neffectively. We then used feature embedding to map each token sequence to a meaningful numeric vector. Since \nwe used ViT to map the slices of each MRI to a sequence of dimensional numerical features, this transformer did \nnot need an embedding module. As each slice provides dimensional numerical features, the MRI classification \nproblem was changed to a multi-dimensional time series classification. The summary of the model is illustrated \nin Fig. 5.\nBy utilizing transfer learning with the ViT, we adeptly address the challenges posed by the voluminous nature \nof 3D MRI datasets. Splitting the 3D MRIs into 2D slices allows for the application of pre-trained models, which \nare predominantly designed and optimized for 2D image data. This strategy not only circumvents the need for \nlarge 3D medical imaging datasets but also uses the power of models trained on extensive 2D image datasets.\nFurthermore, our method extends the benefits of transfer learning beyond mere feature extraction from \nindividual slices. By applying transfer learning to the time series classification of these extracted features, we \nCoronal Sagittal Axial\nA\nB\nC\nFigure 2.  A sample of an MRI slice in three planes (coronal, sagittal, and axial). (A) Original MRI; (B) \nsegmented anatomical image with the skull removed; and (C) segmented anatomical scan warped to MNI space.\nCoronal Sagittal Axial\nFigure 3.  14 × 14 image patches. Patches of 16 × 16 pixels were taken from the input images, with 14 × 14 \npatches.\n5\nVol.:(0123456789)Scientific Reports |         (2024) 14:8996  | https://doi.org/10.1038/s41598-024-59578-3\nwww.nature.com/scientificreports/\ninnovatively create a two-tiered application of transfer learning. This approach capitalizes on the inherent tem-\nporal information within MRI scans, treating the progression of slices as a sequence that can provide valuable \ninsights into the underlying medical conditions.\nThis dual application of transfer learning—first, to the feature extraction from 2D slices, and second, to the \ntemporal analysis of these features—ensures a comprehensive utilization of available data. It effectively applies \ndeep learning advances to medical imaging issues, specifically managing 3D MRI data with limited resources \nand data privacy concerns.\nResults\nThe analysis was performed in two main steps: feature extraction and sequence modeling tasks on ADNI1: \nComplete 3Yr 3T and ADNI1: Complete 1Yr 1.5T MRI data. First, we compared our classification result with \nCNN alongside Bi-LSTM, CNN alongside Transformer, and ViT alongside Bi-LSTM. Then, we compared the \nclassification performance, model accuracy, precision, F-score, and recall. The results for ADNI1: Complete 1Yr \n1.5T are shown in the Supplement Section 2.1.\nExperiments on ADNI1: complete 3Yr 3T\nBinary classification\nThe results from the four architectures were similar, with CNN-TST and ViT-TST achieving the highest accuracy \nand precision scores of 98.81% and 0.99, respectively. Meanwhile, CNN-Bi-LSTM and ViT-Bi-LSTM had slightly \nlower scores but still performed well, with accuracy scores of 97.14% and 97.38%, respectively. To measure the \nability of our proposed model to capture the positive instances, we calculated the recall score, and to measure \nthe balance between the precision and the recall, we calculated the F-Score. The F-scores and recall scores for \nFigure 4.  A visual summary of the proposed method. ViTs consist of several transformer blocks. Each \ntransformer block comprises two sub-layers, a feed-forward layer and a multi-head self-attention layer.\nFigure 5.  Summary of the proposed model. Features were extracted using ViT and the sequence of features was \nclassified using the time series transformer model.\n6\nVol:.(1234567890)Scientific Reports |         (2024) 14:8996  | https://doi.org/10.1038/s41598-024-59578-3\nwww.nature.com/scientificreports/\nall architectures were very similar, ranging from 0.97 to 0.99, indicating good performance in these metrics (see \nTable 2).\nThe architectures CNN-Bi-LSTM and ViT-TST had the highest accuracy, at 98.333%, while CNN-TST had \nthe lowest accuracy, at 97.381%. Based on precision, F-score, and recall, the CNN-Bi-LSTM and ViT-TST archi-\ntectures achieved the highest performance, with precision and recall scores of 0.98 and an F-score of 0.98. The \nCNN-TST also had a strong performance, with a precision and recall of 0.98 and an F-score of 0.97. The ViT-\nBi-LSTM model had the lowest precision score, 0.97, but still achieved a strong F-score and recall score of 0.98 \nand 0.97, respectively.\nThe table shows the performance of four different deep-learning architectures on a particular task. The ViT-\nTST architecture performed the best among all the models, achieving the highest accuracy of 99.048% and the \nhighest scores for precision, F-score, and recall. The ViT-Bi-LSTM architecture also performed well, achieving an \naccuracy of 98.571% and competitive precision, F-score, and recall scores. The CNN-TST architecture attained \nan accuracy of 98.333% and high precision, F-score, and recall scores. Overall, the ViT-TST and ViT-Bi-LSTM \narchitectures were better suited for the given task, followed by CNN-TST. The CNN-Bi-LSTM architecture was \nthe least effective, although the differences were minor.\nThe results on sagittal, coronal, and axial planes for binary classification (NC and AD) are listed in Table 2.\nTo calculate the performance of the proposed model for binary classification, we calculated the confusion \nmatrix based on the proposed method and compared it with the other methods. The performance of the pro-\nposed method for binary disease classification using different MRI planes is shown in Fig.  6. The diagonal cells \nindicate the prediction performance of the models when used to identify the true positive cases. ViT with time \nseries transformer performed better than the other models for all the planes.\nMulticlass classification\nViT-TST architecture achieved the highest accuracy and precision scores for multiclass classification based on \nsagittal planes. The CNN-Bi-LSTM model achieved an accuracy of 98.028%, with a precision of 0.98, an F-score \nof 0.98, and a recall of 0.98. The ViT-TST model achieved an accuracy of 98.310%, with a precision of 0.98, an \nF-score of 0.98, and a recall of 0.98. All four architectures had the best precision score of 0.98, with the ViT-TST \narchitecture achieving the highest accuracy and precision scores (see Table 3).\nFor the coronal plane, CNN-Bi-LSTM achieved an accuracy of 96.479%, with a precision of 0.96, an F-score of \n0.96, and a recall of 0.96. ViT-TST achieved the highest accuracy, 99.014%, with a precision of 0.99, an F-score of \n0.99, and a recall of 0.99. Overall, the CNN-Bi-LSTM architecture had lower scores than the other architectures, \nespecially for accuracy.\nFor the axial plane, the ViT-Bi-LSTM and CNN-TST architectures also performed well, with accuracy, preci-\nsion, F-score, and recall values above 0.98. The CNN-Bi-LSTM architecture had the lowest precision and F-score \nvalues but achieved high accuracy and recall value. These results suggest that the ViT-TST architecture is the \nmost efficient for this task.\nThe results on sagittal, coronal, and axial planes for multiclass classification (NC, MCI, and AD) are listed \nin Table 3.\nThe ViT-TST algorithm excelled in multiclass classification, demonstrating superiority in true positive (TP), \ntrue negative (TN), false positive (FP), and false negative (FN). It consistently demonstrated exceptional per -\nformance in accurately classifying instances belonging to various classes, with high TP and TN values across \nall classes. This demonstrated its ability to accurately identify positive instances and differentiate them from \nTable 2.  Results for binary disease classification on ADNI1: Complete 3Yr 3T. *TST, Time series transformer; \nACC, accuracy; Precision, accuracy of positive prediction; Recall, accuracy of positive instances; F1-Score, \nbalance between precision and recall. Significant values are in bold.\nArchitecture ACC Precision Recall F-score\nSagittal\n CNN-Bi-LSTM 97.143% (± 3.658) 0.97 0.97 0.97\n CNN-TST 98.810% (± 2.195) 0.99 0.99 0.99\n ViT-Bi-LSTM 97.381% (± 2.704) 0.97 0.97 0.97\n ViT-TST 98.571% (± 2.857) 0.99 0.98 0.98\nCoronal\n CNN-Bi-LSTM 98.333% (± 2.619) 0.98 0.98 0.98\n CNN-TST 97.381% (± 4.185) 0.98 0.97 0.97\n ViT-Bi-LSTM 97.619% (± 2.817) 0.97 0.97 0.98\n ViT-TST 98.333% (± 2.827) 0.98 0.98 0.98\nAxial\n CNN-Bi-LSTM 96.190% (± 4.012) 0.96 0.96 0.96\n CNN-TST 98.333% (± 2.143) 0.98 0.98 0.98\n ViT-Bi-LSTM 98.571% (± 2.182) 0.99 0.98 0.98\n ViT-TST 99.048% (± 1.905) 0.99 0.99 0.99\n7\nVol.:(0123456789)Scientific Reports |         (2024) 14:8996  | https://doi.org/10.1038/s41598-024-59578-3\nwww.nature.com/scientificreports/\nSagittal Plane\nViT-TST ViT-Bi-LSTM CNN-TSTC NN-Bi-LSTM\nCoronal Plane\nViT-TST ViT-Bi-LSTM CNN-TSTC NN-Bi-LSTM\nAxial Plane\nViT-TST ViT-Bi-LSTM CNN-TSTC NN-Bi-LSTM\nFigure 6.  Confusion matrices for binary classification on ADNI1: Complete 3Yr 3T.\nTable 3.  Results for multiclass disease patient classification on ADNI1: complete 3Yr 3T. *TST, time series \ntransformer; ACC, accuracy; Precision, accuracy of positive prediction; Recall, accuracy of positive instances; \nF1-Score, balance between precision and recall. Significant values are in bold.\nArchitecture ACC Precision Recall F-score\nSagittal\n CNN-Bi-LSTM 98.028% (± 0.934) 0.98 0.98 0.98\n CNN-TST 97.324% (± 2.394) 0.97 0.97 0.97\n ViT-Bi-LSTM 97.746% (± 2.456) 0.98 0.97 0.97\n ViT-TST 98.310% (± 1.380) 0.98 0.98 0.98\nCoronal\n CNN-Bi-LSTM 96.479% (± 2.205) 0.96 0.96 0.96\n CNN-TST 97.465% (± 2.423) 0.98 0.97 0.97\n ViT-Bi-LSTM 97.465% (± 2.164) 0.97 0.97 0.97\n ViT-TST 99.014% (± 1.672) 0.99 0.99 0.99\nAxial\n CNN-Bi-LSTM 96.761% (± 2.601) 0.97 0.96 0.96\n CNN-TST 98.169% (± 2.820) 0.98 0.98 0.98\n ViT-Bi-LSTM 98.028% (± 1.804) 0.98 0.98 0.98\n ViT-TST 99.014% (± 1.268) 0.99 0.99 0.99\n8\nVol:.(1234567890)Scientific Reports |         (2024) 14:8996  | https://doi.org/10.1038/s41598-024-59578-3\nwww.nature.com/scientificreports/\nnegatives. Additionally, the algorithm minimized false negatives (FN), reducing the risk of misclassifying positive \noccurrences as negatives, crucial in medical diagnostic applications such as AD classification. The ViT-TST also \nexcelled in managing false positives (FP), ensuring that instances from other classes were correctly identified \nas non-relevant. Overall, the superiority of ViT-TST in TP , TN, FN, and FP further solidified its position as a \nhighly effective and reliable choice for multiclass disease classification tasks.\nThe performance of the proposed method based on a confusion matrix for multiclass classification on dif -\nferent planes is shown in Fig. 7.\nWe also compared our proposed model with conventional deep models. Tables 4 and 5 show numerical results \nfrom different deep learning model for the binary and multiclassification tasks on ADNI datasets and compared \nSagittal\nViT-TST ViT-Bi-LSTM CNN-TSTC NN-Bi-LSTM\nCoronal\nViT-TST ViT-Bi-LSTM CNN-TSTC NN-Bi-LSTM\nAxial\nViT-TST ViT-Bi-LSTM CNN-TSTC NN-Bi-LSTM\nFigure 7.  Confusion matrices of axial planes for multiclass disease patient classification (NC, MCI, and AD) on \nADNI1: Complete 3Yr 3T.\nTable 4.  Performance compared with state-of-the-art methods for binary disease patient classification.\nWork Input\nImage scans\nMethod\nNC/AD classification\n(NC/MCI/AD) ACC (%) SEN (%) SPE (%)\n29 Voxel based 429/–/858 3D CNN 90.3 82.4 96.5\n30 Voxel based 119/233/97 3D DenseNet 88.9 86.6 90.8\n31 Voxel based 330/299/299 3D CNN 93.2 95.0 89.8\n32 Patch based 324/316/319 Self-attention 98.0 97.7 98.2\n33 Voxel based 457/808/346 3D ResNet 94.00 – –\n34 Voxel based 407/–/418 3D CNN 99.20 98.90 99.50\n35 ROI based 209/401/188 2.5D CNN 79.90 84.00 74.80\nProposed method Sequence based 129/145/77 ViT-TST 99.048 99.5 –\nProposed method Sequence based 705/1113/476 ViT-TST 95.169 95.5\n9\nVol.:(0123456789)Scientific Reports |         (2024) 14:8996  | https://doi.org/10.1038/s41598-024-59578-3\nwww.nature.com/scientificreports/\nwith our proposed model. For binary classification, the proposed method outperformed other traditional models. \nMoreover, as shown in Table 5, the proposed method ranked first among all competitors.\nThe results on sagittal, coronal, and axial planes for multiclass disease patient classification (NC, MCI, and \nAD) are shown in Table 5.\nThe ADNI1: Complete 1Yr 1.5T results are shown in Supplement Section 2.2.1.\nDiscussion\nIn this study, we proposed use of a vision transformer with sequential transformer architecture for binary and \nmulticlass MRI classification. We also compared the performance of the proposed model with other traditional \nimage analysis architectures such as convolutional neural networks and long short-term memory networks on \nmedium and large-sized ADNI datasets.\nReducing false negatives (FN) is crucial in medical diagnosis and healthcare, since models can inaccurately \ncategorize positive occurrences as negative, causing failure to identify or detect cases. In AD categorization, \nfalse negative outcomes may misclassify individuals as being in good health, preventing timely identification \nand administration of appropriate medical interventions. The ViT-TST algorithm appears to be the best choice \nfor binary AD classification. It provides a balanced combination of accuracy and sensitivity, which is crucial \nfor medical applications where both false positives and false negatives can have significant implications. Since \nminimizing FN is a top priority in AD classification, ViT-TST would be the most suitable choice.\nOur proposed method, the ViT-TST, outperformed the ViT-Bi-LSTM, CNN-TST, and CNN-Bi-LSTM mod-\nels when classifying MCI patients. The ViT-TST achieved higher accuracy, sensitivity, or recall, resulting in an \nincrease in the number of TP predictions and a decrease in the number of FN. This suggests that the ViT-TST \naccurately identified more MCI patients, reducing the chances that they would be misclassified as healthy or as \nbelonging to other classes. The viability of the ViT-TST for multiclass disease classification tasks, particularly in \nthe diagnosis of AD, is supported by its promising results.With the approval of new drugs for early intervention \nin  AD7,8, early detection of AD and differentiation of MCI have become critical. Classifying MCI vs. NC or AD \nin a multiclassification framework is challenging because it is a heterogeneous condition with many subtypes \nand causes. Many studies have overlooked this problem and have experimented only with binary classification. \nHowever, our experiments were performed on binary (NC/AD) and multiclass disease classification tasks (NC/\nMCI/AD), and the performance of each model was evaluated using accuracy, precision, F-score, and recall.\nWhen averaging the accuracy rates over all planes for binary disease classification on the ADNI1: Complete \n3Yr 3T data, ViT-TST achieved the highest average accuracy of 98.65% (see Table 2), followed closely by CNN-\nTST with 98.17%. ViT-Bi-LSTM and CNN-Bi-LSTM also performed well, with average accuracies of 97.85% and \n97.22%, respectively (see Table 2). These results suggest that all four architectures are effective for binary disease \npatient classification on this dataset, with CNN-Bi-LSTM being the most effective.\nWhen averaging the accuracy rates over all planes for multiclass disease classification on the ADNI1: Com-\nplete 3Yr 3T, ViT-TST achieved the highest average accuracy of 98.77%, followed by ViT-Bi-LSTM with 97.74%. \nCNN-TST and CNN-Bi-LSTM also performed well, with average accuracies of 97.64% and 97.08%, respectively \n(see Table 3). These results suggest that all four architectures are effective for multiclass classification on this \ndataset, with ViT-TST being the most effective.\nEvaluation of all the results obtained revealed the ViT-TST architecture to be consistently among the top-\nperforming architectures across all datasets and classification tasks. Therefore, the ViT-TST architecture may be \na good choice when designing a classification model for similar datasets and tasks if computational resources \nand other practical considerations are allowed.\nThus, we have devised a way to classify AD based on deep learning by combining pre-trained ViT and time \nseries transformers. Many tasks have limited data, making training a model from scratch difficult. In small or \nunbalanced target datasets, transfer learning lets the model learn from a large dataset and adapt to the smaller \ntarget dataset. Transfer learning in ViT improves generalization, training speed, and adaptability to datasets and \nTable 5.  Performance compared with state-of-the-art methods for multiclass disease patient classification.\nWork Input\nImage scans\nMethod\nNC/MCI/AD classification (%)\n(NC/MCI/AD) ACC Recall Precision SPE F1-Score\n36 Slice based 300/300/300 VGGNet. 16 91.85 – – – –\n37 Voxel based 70/70/70 3D CNN 89.10 – – – –\n38 Slice based 229/243/188 ResNet-18 56.80 – – – –\n39 Slice based 169/234/101 2D CNN 96.00 96.0 – 98.0 –\n33 Voxel based 574/808/346 3D ResNet 87.00 – – – –\n40 Voxel based 207/215/193 3D VGGNet 91.13 – – – –\n41 Slice based 50/50/50 VGGNet-16 95.73 96 96.33 – 95.66\n42 Voxel based 351/297/221 3D DenseNets 97.52 97 97.13 – 97.02\n43 Patch based 475/224/70 3D CNN 97.48 95.33 97.33 97.0\n44 Slice based 25/13/25 ResNet18 and DenseNet121 98.21 98.14 – 98.14 –\nProposed method Sequence-based 129/145/77 ViT-TST 99.01 99.0 99.0 99.0\nProposed method Sequence-based 705/1113/476 ViT-TST 91.42 91.0 92.0 – 90.0\n10\nVol:.(1234567890)Scientific Reports |         (2024) 14:8996  | https://doi.org/10.1038/s41598-024-59578-3\nwww.nature.com/scientificreports/\ntasks; the proposed method generalizes well when trained on insufficient amounts of data. To take advantage \nof transfer learning with a pre-trained ViT, the 3D data from a plane were split into a 2D slice array. The prob-\nlem with slice-based approaches such as CNN is that they fail to retain the dependencies of associated features \nbetween slices. To overcome this problem, the sequence classification task uses a time series classification with \na transformer. Another alternative is to use 3D deep models; however, transfer learning with a pre-trained 3D \nis not currently available. Therefore, these approaches do not generalize well when trained on insufficient data.\nConclusions\nOverall, our results show that all four architectures achieved high levels of accuracy and performance on binary \nand multiclass disease classification tasks. Based on accuracy scores, all models performed well. However, we \nconclude that the Vi-TST and ViT-Bi-LSTM models perform better than the CNN-TST and CNN-Bi-LSTM mod-\nels in terms of long-term dependencies among the spatial and temporal patterns in dynamic MRI sequences. In \naddition, they capture the global context using the self-attention mechanism to ensure that the relevant informa-\ntion from the entire or sequence of images is considered during classification, which reduces overfitting. Transfer \nlearning with the ViT can efficiently handle large 3D MRI datasets by splitting them into 2D slices and applying \npre-trained models. This approach not only minimizes the need for large datasets but also utilizes models trained \non extensive 2D image datasets. The method also extends transfer learning beyond feature extraction to time \nseries classification, providing valuable insights into underlying medical conditions. We gain deeper understand-\ning of the underlying medical conditions by leveraging the attention mechanism in both feature extraction and \nclassification, demonstrating a sophisticated combination of advanced AI techniques for medical image analysis.\nData availability\nNo datasets were generated or analysed during the current study.\nReceived: 17 November 2023; Accepted: 12 April 2024\nReferences\n 1. Hazarika, R. A., Kandar, D. & Maji, A. K. An experimental analysis of different deep learning based models for Alzheimer’s disease \nclassification using brain magnetic resonance images. J. King Saud Univ. Comput. Inf. Sci. 34(10), 8576–8598. https:// doi. org/ 10. \n1016/J. JKSUCI. 2021. 09. 003 (2022).\n 2. Jain, R., Jain, N., Aggarwal, A. & Hemanth, D. J. Convolutional neural network based Alzheimer’s disease classification from \nmagnetic resonance brain images. Cogn. Syst. Res. 57, 147–159. https:// doi. org/ 10. 1016/J. COGSYS. 2018. 12. 015 (2019).\n 3. Blennow, K., Zetterberg, H. & Fagan, A. M. Fluid biomarkers in Alzheimer disease. Cold Spring Harbor Perspect. Med.  2(2012), \na006221. https:// doi. org/ 10. 1101/ cshpe rspect. a0062 21 (2012).\n 4. Khojaste-Sarakhsi, M., Haghighi, S. S., Ghomi, S. M. T. F . & Marchiori, E. Deep learning for Alzheimer’s disease diagnosis: A \nsurvey. Artif Intell Med 130, 102332. https:// doi. org/ 10. 1016/J. ARTMED. 2022. 102332 (2022).\n 5. Alzheimer’s Association. 2019 Alzheimer’s Disease Facts and figures (Wiley Online Library, 2012).\n 6. Alberdi, A., Aztiria, A. & Basarab, A. On the early diagnosis of Alzheimer’s disease from multimodal signals: A survey. Artif. Intell. \nMed. 71, 1–29. https:// doi. org/ 10. 1016/J. ARTMED. 2016. 06. 003 (2016).\n 7. McDade, E. et al. Lecanemab in patients with early Alzheimer’s disease: Detailed results on biomarker, cognitive, and clinical effects \nfrom the randomized and open-label extension of the phase 2 proof-of-concept study. Alzheimers Res. Ther. 14(1), 191. https:// \ndoi. org/ 10. 1186/ S13195- 022- 01124-2 (2022).\n 8. Sims, J. R. et al. Donanemab in early symptomatic Alzheimer disease: The TRAILBLAZER-ALZ 2 randomized clinical trial. JAMA \nhttps:// doi. org/ 10. 1001/ JAMA. 2023. 13239 (2023).\n 9. Loddo, A., Buttau, S. & Di Ruberto, C. Deep learning based pipelines for Alzheimer’s disease diagnosis: A comparative study and \na novel deep-ensemble method. Comput. Biol. Med. 141, 105032. https:// doi. org/ 10. 1016/J. COMPB IOMED. 2021. 105032 (2022).\n 10. Zhao, B., Lu, H., Chen, S., Liu, J. & Wu, D. Convolutional neural networks for time series classification. J. Syst. Eng. Electron. 28(1), \n162–169. https:// doi. org/ 10. 21629/ JSEE. 2017. 01. 18 (2017).\n 11. Wen, Q. et al. Transformers in time series: A survey (2022). https:// doi. org/ 10. 48550/ arxiv. 2202. 07125.\n 12. Yue, L. et al. Hierarchical feature extraction for early Alzheimer’s disease diagnosis. IEEE Access 7, 93752–93760. https:// doi. org/ \n10. 1109/ ACCESS. 2019. 29262 88 (2019).\n 13. Silva, I. R. R., Silva, G. S. L., de Souza, R. G., dos Santos, W . P . & de Fagundes, R. A. A. Model based on deep feature extraction for \ndiagnosis of Alzheimer’s disease. In Proceedings of the International Joint Conference on Neural Networks Vol. 2019 (2019). https:// \ndoi. org/ 10. 1109/ IJCNN. 2019. 88521 38.\n 14. Zhang, F . et al. Multi-modal deep learning model for auxiliary diagnosis of Alzheimer’s disease. Neurocomputing  361, 185–195. \nhttps:// doi. org/ 10. 1016/J. NEUCOM. 2019. 04. 093 (2019).\n 15. Jang, J. & Hwang, D. M3T: Three-dimensional medical image classifier using multi-plane and multi-slice transformer. 20718–20729 \n(2022).\n 16. Gunawardena, K. A. N. N. P ., Rajapakse, R. N. & Kodikara, N. D. Applying convolutional neural networks for pre-detection of \nAlzheimer’s disease from structural MRI data. In 2017 24th International Conference on Mechatronics and Machine Vision in \nPractice, M2VIP 2017 vol. 2017 1–7 (2017). https:// doi. org/ 10. 1109/ M2VIP . 2017. 82114 86.\n 17. Choi, H. & Jin, K. H. Predicting cognitive decline with deep learning of brain metabolism and amyloid imaging. Behav. Brain Res. \n344, 103–109. https:// doi. org/ 10. 1016/J. BBR. 2018. 02. 017 (2018).\n 18. Esmaeilzadeh, S., Belivanis, D. I., Pohl, K. M., Adeli, E. End-to-end Alzheimer’s disease diagnosis and biomarker identification. In \nLecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), \nvol. 11046 LNCS 337–345 (2018). https:// doi. org/ 10. 1007/ 978-3- 030- 00919-9_ 39/ COVER.\n 19. Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image recognition at scale (accessed 28 March 2023); \nhttps:// github. com/\n 20. Kim, J., Shim, K., Kim, J. & Shim, B. Vision transformer-based feature extraction for generalized zero-shot learning (2023). arXiv: \n2302. 00875.\n 21. Lyu, Y ., Yu, X., Zhu, D. & Zhang, L. Classification of Alzheimer’s disease via vision transformer: Classification of Alzheimer’s \ndisease via vision transformer. In ACM International Conference Proceeding Series 463–468 (2022). https:// doi. org/ 10. 1145/ 35291 \n90. 35347 54.\n11\nVol.:(0123456789)Scientific Reports |         (2024) 14:8996  | https://doi.org/10.1038/s41598-024-59578-3\nwww.nature.com/scientificreports/\n 22. Kushol, R., Masoumzadeh, A., Huo, D., Kalra, S. & Y ang, Y . H. Addformer: Alzheimer’s disease detection from structural MRI \nusing fusion transformer. In Proceedings - International Symposium on Biomedical Imaging  vol. 2022 (2022). https:// doi. org/ 10. \n1109/ ISBI5 2829. 2022. 97614 21.\n 23. Li, J. et al. Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and \nfuture perspectives. Med. Image Anal. 85, 102762. https:// doi. org/ 10. 1016/J. MEDIA. 2023. 102762 (2023).\n 24. Vaswani, A. et al. Attention is all you need. Adv. Neural Inf. Process. Syst. 2017, 5999–6009 (2017).\n 25. “ ADNI | Alzheimer’s Disease Neuroimaging Initiative (accessed 03 April 2023). https:// adni. loni. usc. edu/\n 26. Jack, C. R. et al. The Alzheimer’s disease neuroimaging initiative (ADNI): MRI methods. J. Magn. Reson. Imaging 27(4), 685–691. \nhttps:// doi. org/ 10. 1002/ JMRI. 21049 (2008).\n 27. Ashburner, J. & Friston, K. J. Unified segmentation. Neuroimage 26(3), 839–851. https:// doi. org/ 10. 1016/J. NEURO IMAGE. 2005. \n02. 018 (2005).\n 28. Ridnik, T., Ben-Baruch, E., Noy, A. & Zelnik-Manor, L. ImageNet-21K pretraining for the masses. 2021 (accessed 05 April 2023). \narXiv: 2104. 10972 v4\n 29. Lian, C., Liu, M., Zhang, J. & Shen, D. Hierarchical fully convolutional network for joint atrophy localization and Alzheimer’s \ndisease diagnosis using structural MRI. IEEE Trans. Pattern Anal. Mach. Intell. 42(4), 880–893. https:// doi. org/ 10. 1109/ TPAMI. \n2018. 28890 96 (2020).\n 30. Liu, M. et al. A multi-model deep convolutional neural network for automatic hippocampus segmentation and classification in \nAlzheimer’s disease. Neuroimage 208, 116459. https:// doi. org/ 10. 1016/J. NEURO IMAGE. 2019. 116459 (2020).\n 31. Li, J. et al. 3-D CNN-Based multichannel contrastive learning for Alzheimer’s disease automatic diagnosis. IEEE Trans. Instrum. \nMeas. 71, 1–11. https:// doi. org/ 10. 1109/ TIM. 2022. 31622 65 (2022).\n 32. Zhu, J. et al. Efficient self-attention mechanism and structural distilling model for Alzheimer’s disease diagnosis. Comput. Biol. \nMed. 147, 105737. https:// doi. org/ 10. 1016/J. COMPB IOMED. 2022. 105737 (2022).\n 33. Karasawa, H., Liu, C. L. & Ohwada, H. Deep 3D convolutional neural network architectures for Alzheimer’s disease diagnosis. In: \nLecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \nvol. 10751 LNAI, 287–296 (2018). https:// doi. org/ 10. 1007/ 978-3- 319- 75417-8_ 27.\n 34. Basaia, S., Agosta, F ., Wagner, L., Canu, E., et al. Automated classification of Alzheimer’s disease and mild cognitive impairment \nusing a single MRI and deep neural networks. Elsevier (accessed 29 March 2023). https:// www. scien cedir ect. com/ scien ce/ artic le/ \npii/ S2213 15821 83039 30\n 35. Lin, W . et al. Convolutional neural networks-based MRI image analysis for the Alzheimer’s disease prediction from mild cognitive \nimpairment. Front. Neurosci. 12, 777. https:// doi. org/ 10. 3389/ FNINS. 2018. 00777/ BIBTEX (2018).\n 36. Billones, C. D., Demetria, O. J. L. D., Hostallero, D. E. D., Naval, P . C. DemNet: A convolutional neural network for the detection \nof Alzheimer’s disease and mild cognitive impairment. In IEEE Region 10 Annual International Conference, Proceedings/TENCON \n3724–3727 (2017). https:// doi. org/ 10. 1109/ TENCON. 2016. 78487 55.\n 37. Hosseini-Asl, E., Keynton, R. & El-Baz, A. Alzheimer’s disease diagnostics by adaptation of 3D convolutional network. 2016 \n(accessed 29 2023 March). https:// ieeex plore. ieee. org/ abstr act/ docum ent/ 75323 32/? casa_ token= Neb5n 7ikTZ MAAAAA: PkEGL \nJT7qw 9U49O S9KRi bb0AF V1Imp Mxt_ SViSM vquUa RK5Bj ceVLg be3Yz nJAG0 Tw20n p3KSM NT0Pk\n 38. Valliani, A. & Soni, A. Deep residual nets for improved Alzheimer’s diagnosis. In: Proceeding of the 8th ACM International Confer-\nence, and Undefined 2017 615 (2017). https:// doi. org/ 10. 1145/ 31074 11. 31082 24.\n 39. Gunawardena, K., Rajapakse, R. N., Kodikara, N. D. Applying convolutional neural networks for pre-detection of Alzheimer’s \ndisease from structural MRI data. 2017 (accessed 29 March 2023). https:// ieeex plore. ieee. org/ abstr act/ docum ent/ 82114 86/? casa_ \ntoken= 0Vm5O Bjwvl YAA AAA : PKVbN MAMIs Dz- HLaNN WN_ khu_ UcFL6 wO3FE tTxOx VE5tu Cet49 yXSNB- smS9l U5C5k nfNN1 \nGTZT9 RWs\n 40. Vu, T. D., Ho, N. H., Y ang, H. J., Kim, J. & Song, H. C. Non-white matter tissue extraction and deep convolutional neural network \nfor Alzheimer’s disease detection. Soft Comput. 22(20), 6825–6833. https:// doi. org/ 10. 1007/ S00500- 018- 3421-5 (2018).\n 41. Jain, R., Jain, N., Aggarwal, A. & Hemanth, D. J. Convolutional neural network based Alzheimer’s disease classification from \nmagnetic resonance brain images. Elsevier  2019 (accessed 29 March 2023). https:// www. scien cedir ect. com/ scien ce/ artic le/ pii/ \nS1389 04171 83095 62\n 42. Wang, H. et al. Ensemble of 3D densely connected convolutional network for diagnosis of mild cognitive impairment and Alzhei-\nmer’s disease. Neurocomputing 333, 145–156. https:// doi. org/ 10. 1016/J. NEUCOM. 2018. 12. 018 (2019).\n 43. Goenka, N. & Tiwari, S. AlzVNet: A volumetric convolutional neural network for multiclass classification of Alzheimer’s disease \nthrough multiple neuroimaging computational approaches. Biomed. Signal Process. Control 74, 103500. https:// doi. org/ 10. 1016/J. \nBSPC. 2022. 103500 (2022).\n 44. Odusami, M., Maskeliūnas, R. & Damaševičius, R. An intelligent system for early recognition of Alzheimer’s disease using neu -\nroimaging. Sensors 22(3), 740. https:// doi. org/ 10. 3390/ S2203 0740 (2022).\nAcknowledgements\nData collection and sharing for this project were funded by the Alzheimer’s Disease Neuroimaging Initiative \n(ADNI), the National Institutes of Health (Grant U01 AG024904), and the DOD ADNI Department of Defense \n(award number W81XWH-12-2-0012). ADNI is funded by the National Institute on Aging, the National Institute \nof Biomedical Imaging and Bioengineering, and through generous contributions from the following: AbbVie, \nAlzheimer’s Association; Alzheimer’s Drug Discovery Foundation; Araclon Biotech; BioClinica, Inc.; Biogen; \nBristol-Myers Squibb Co.; CereSpir, Inc.; Cogstate; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli Lilly and Co.; \nEuroImmun; F . Hoffmann-La Roche Ltd and its affiliated company Genentech, Inc.; Fujirebio; GE Healthcare; \nIXICO Ltd; Janssen Alzheimer Immunotherapy Research & Development, LLC; Johnson & Johnson Pharma-\nceutical Research & Development, LLC; Lumosity; Lundbeck; Merck & Co., Inc.; Meso Scale Diagnostics, LLC; \nNeuroRx Research; Neurotrack Technologies; Novartis Pharmaceuticals Corporation; Pfizer Inc.; Piramal Imag-\ning; Servier; Takeda Pharmaceutical Company; and Transition Therapeutics. The Canadian Institutes of Health \nResearch provides funds to support ADNI clinical sites in Canada. Private sector contributions are facilitated \nby the Foundation for the National Institutes of Health (http:// www. fnih. org). The grantee organization is the \nNorthern California Institute for Research and Education, and the study is coordinated by the Alzheimer’s \nTherapeutic Research Institute at the University of Southern California. ADNI data are disseminated by the \nLaboratory for Neuro Imaging at the University of Southern California. SAC is a Muslow, MD Endowed Chair \nin Healthcare Informatics.\nReplication of results\nThe codes and data used are available on request to enable the method proposed in the manuscript to be repli -\ncated by readers.\n12\nVol:.(1234567890)Scientific Reports |         (2024) 14:8996  | https://doi.org/10.1038/s41598-024-59578-3\nwww.nature.com/scientificreports/\nAuthor contributions\nM.A.N.B. conceptualized and designed the study; S.A. and T.A. took part in data accumulation and data analysis; \nM.A.N.B., S.A., T.A., M.S.B., E.D., S.C., J.V ., and C.G.K. wrote the manuscript; and all the authors have read, \nedited, and approved the article.\nFunding\nThree National Institutes of Health grants supported this work: R01HL145753, R01HL145753-01S1, and \nR01HL145753-03S1; in addition, the work was supported by LSUHSC-S CCDS Finish Line Award, COVID-\n19 Research Award, and LARC Research Award to MSB;  and Institutional Development Award (IDeA) from \nthe National Institutes of General Medical Sciences of the NIH under grant number P20GM121307 and \nR01HL149264 to CGK.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nSupplementary Information The online version contains supplementary material available at https:// doi. org/ \n10. 1038/ s41598- 024- 59578-3.\nCorrespondence and requests for materials should be addressed to M.A.N.B.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2024"
}