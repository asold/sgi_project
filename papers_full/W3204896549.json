{
    "title": "ATISS: Autoregressive Transformers for Indoor Scene Synthesis",
    "url": "https://openalex.org/W3204896549",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A3041690111",
            "name": "Paschalidou, Despoina",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2975546781",
            "name": "Kar, Amlan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4227645647",
            "name": "Shugrina, Maria",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4202201480",
            "name": "Kreis, Karsten",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2799797514",
            "name": "Geiger, Andreas",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2748935333",
            "name": "Fidler, Sanja",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2770875015",
        "https://openalex.org/W2985936292",
        "https://openalex.org/W3180251767",
        "https://openalex.org/W3016025127",
        "https://openalex.org/W2810181048",
        "https://openalex.org/W2940744433",
        "https://openalex.org/W2964122153",
        "https://openalex.org/W3202579984",
        "https://openalex.org/W3095066959",
        "https://openalex.org/W2962770929",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W3034445277",
        "https://openalex.org/W2798622261",
        "https://openalex.org/W3168314764",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3112776202",
        "https://openalex.org/W3132535424",
        "https://openalex.org/W3201922548",
        "https://openalex.org/W2989341556",
        "https://openalex.org/W2117088012",
        "https://openalex.org/W3000176874",
        "https://openalex.org/W3034573343",
        "https://openalex.org/W2990222759",
        "https://openalex.org/W3035046407",
        "https://openalex.org/W2960202457",
        "https://openalex.org/W2419448466",
        "https://openalex.org/W2963139417",
        "https://openalex.org/W2981010689",
        "https://openalex.org/W3137120824",
        "https://openalex.org/W2060268527",
        "https://openalex.org/W3087067522",
        "https://openalex.org/W2162559028",
        "https://openalex.org/W3034600949",
        "https://openalex.org/W2557269700",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W2962695743",
        "https://openalex.org/W3153854932",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2963807318",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2968638474",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W2963601843",
        "https://openalex.org/W2099471712",
        "https://openalex.org/W3088333848",
        "https://openalex.org/W2557465155",
        "https://openalex.org/W2149846167",
        "https://openalex.org/W2980088508",
        "https://openalex.org/W3180355996",
        "https://openalex.org/W2065745355",
        "https://openalex.org/W2099854727",
        "https://openalex.org/W3021164770",
        "https://openalex.org/W2130634053",
        "https://openalex.org/W2964334375",
        "https://openalex.org/W3047258236",
        "https://openalex.org/W3003555839",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W3110015970",
        "https://openalex.org/W2953273646",
        "https://openalex.org/W3035108870",
        "https://openalex.org/W3034727889",
        "https://openalex.org/W2963676163",
        "https://openalex.org/W3035574324",
        "https://openalex.org/W1959608418",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2893749619",
        "https://openalex.org/W2963767194",
        "https://openalex.org/W3040538742",
        "https://openalex.org/W2519091744",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2902539442",
        "https://openalex.org/W2250673545",
        "https://openalex.org/W2963981733",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2103815986",
        "https://openalex.org/W3098903812",
        "https://openalex.org/W2117741646",
        "https://openalex.org/W2009188425"
    ],
    "abstract": "The ability to synthesize realistic and diverse indoor furniture layouts automatically or based on partial input, unlocks many applications, from better interactive 3D tools to data synthesis for training and simulation. In this paper, we present ATISS, a novel autoregressive transformer architecture for creating diverse and plausible synthetic indoor environments, given only the room type and its floor plan. In contrast to prior work, which poses scene synthesis as sequence generation, our model generates rooms as unordered sets of objects. We argue that this formulation is more natural, as it makes ATISS generally useful beyond fully automatic room layout synthesis. For example, the same trained model can be used in interactive applications for general scene completion, partial room re-arrangement with any objects specified by the user, as well as object suggestions for any partial room. To enable this, our model leverages the permutation equivariance of the transformer when conditioning on the partial scene, and is trained to be permutation-invariant across object orderings. Our model is trained end-to-end as an autoregressive generative model using only labeled 3D bounding boxes as supervision. Evaluations on four room types in the 3D-FRONT dataset demonstrate that our model consistently generates plausible room layouts that are more realistic than existing methods. In addition, it has fewer parameters, is simpler to implement and train and runs up to 8 times faster than existing methods.",
    "full_text": "ATISS: Autoregressive Transformers for Indoor\nScene Synthesis\nDespoina Paschalidou1,3,4 Amlan Kar4,5,6 Maria Shugrina4 Karsten Kreis4\nAndreas Geiger1,2,3 Sanja Fidler4,5,6\n1Max Planck Institute for Intelligent Systems Tübingen 2University of Tübingen\n3Max Planck ETH Center for Learning Systems\n4NVIDIA 5University of Toronto 6Vector Institute\n{firstname.lastname}@tue.mpg.de {amlank, mshugrina, kkreis, sfidler}@nvidia.com\nAbstract\nThe ability to synthesize realistic and diverse indoor furniture layouts automatically\nor based on partial input, unlocks many applications, from better interactive 3D\ntools to data synthesis for training and simulation. In this paper, we present\nATISS, a novel autoregressive transformer architecture for creating diverse and\nplausible synthetic indoor environments, given only the room type and its ﬂoor\nplan. In contrast to prior work, which poses scene synthesis as sequence generation,\nour model generates rooms as unordered sets of objects. We argue that this\nformulation is more natural, as it makes ATISS generally useful beyond fully\nautomatic room layout synthesis. For example, the same trained model can be\nused in interactive applications for general scene completion, partial room re-\narrangement with any objects speciﬁed by the user, as well as object suggestions for\nany partial room. To enable this, our model leverages the permutation equivariance\nof the transformer when conditioning on the partial scene, and is trained to be\npermutation-invariant across object orderings. Our model is trained end-to-end\nas an autoregressive generative model using only labeled 3D bounding boxes as\nsupervision. Evaluations on four room types in the 3D-FRONT dataset demonstrate\nthat our model consistently generates plausible room layouts that are more realistic\nthan existing methods. In addition, it has fewer parameters, is simpler to implement\nand train and runs up to 8x faster than existing methods.\n1 Introduction\nGenerating synthetic 3D content that is both realistic and diverse is a long-standing problem in\ncomputer vision and graphics. In the last decade, there has been increased demand for tools that\nautomate the creation of 3D artiﬁcial environments for applications like video games and AR/VR,\nas well as general 3D content creation [ 76, 19, 44, 5, 77]. These tools can also synthesize data to\ntrain computer vision models, avoiding expensive and laborious annotations. Generative models\n[34, 24, 15, 35, 71] have demonstrated impressive results on synthesizing photorealistic images\n[8, 1, 29, 9, 30] and intelligible text [60, 2], and are beginning to be adopted for the generation of 3D\nenvironments.\nRecent works proposed to solve the scene synthesis task by incorporating procedural modeling\ntechniques [59, 57, 28, 11] or by generating scene graphs with generative models [ 41, 72, 80, 42,\n58, 79, 78, 32, 14]. Procedural modeling requires specifying a set of rules for the scene formation\nprocess, but acquiring these rules is a time-consuming task, requiring skills of experienced artists.\nSimilarly, graph-based approaches require scene graph annotations, which may be laborious to obtain.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2110.03675v1  [cs.CV]  7 Oct 2021\nFigure 1: MotivationIn addition to fully automatic layout synthesis (A), our formulation in terms of unordered\nsets of objects allows our model to be used for novel interactive applications with versatile user control: scene\ncompletion given any number of existing furniture pieces of any class pinned to a speciﬁc location by the user\n(B), and object suggestions with user-provided constraints (object centroid constraint shown in red) (C).\nAnother line of research utilizes CNN-based [ 73, 61] and transformer-based [74] architectures to\ngenerate rooms by autoregressively selecting and placing objects in a scene, i.e. one after the other.\nThese approaches represent scenes as ordered sequences of objects. Typically, the ordering is deﬁned\nusing the spatial arrangement of objects in a room (e.g. left-to-right) [27] or the object class frequency\n(e.g. most to least probable) [ 61, 74]. Such orderings impose unnatural constraints on the scene\ngeneration process, inhibiting practical applications. For example, in [61, 74], which order objects by\nclass frequency, the probability of a bed (more common) appearing after an ottoman (less common)\nin the training set is zero. As a result, these methods cannot generate more common objects after less\ncommon objects, which makes them impractical for interactive tasks like general room completion\nand partial room re-arrangement, where input is unconstrained (e.g. Fig.1B).\nTo address these limitations, we pose scene synthesis as an unordered set generation problem and\nintroduce ATISS, a novel autoregressive transformer architecture to model this process. Given a\nroom type (e.g. bedroom, living room) and its shape, our model generates meaningful furniture\narrangements by sequentially placing objects in a permutation-invariant fashion. We train ATISS\nto maximize the log-likelihood of all possible permutations of object arrangements in a collection\nof training scenes, labeled only with object classes and 3D bounding boxes, which are easier\nto obtain, than costly support relationship [ 72] or scene graph annotations [ 41]. Unlike existing\nworks [73, 61, 74], we propose the ﬁrst model to perform scene synthesis as an autoregressive set\ngeneration task. ATISS is signiﬁcantly simpler to implement and train, requires fewer parameters and\nis up to8×faster at run-time than the fastest available baseline [74]. Furthermore, we demonstrate that\nour model generates more plausible object arrangments without any post-processing on the predicted\nlayout. Our formulation allows applying a single trained model to automatic layout synthesis and to\na number of interactive scenarios with versatile user input (Fig.1), such as automatic placement of\nuser-provided objects, object suggestion with user-provided constraints, and room completion. Code\nand data are publicaly available at https://nv-tlabs.github.io/ATISS.\n2 Related Work\nIn this section, we discuss the most relevant literature on interior scene synthesis, as well as trans-\nformer architectures [71] in the context of generative modeling.\nProcedural Modeling with Grammars:Procedural modeling describes methods that recursively\napply a set of functions for content synthesis. Grammars are a formal instantiation of this idea\nand have been used for modeling 3D structures such as plants [ 66], buildings and cities [ 47, 50],\nindoor [59] and outdoor [57] scenes. [66] employed reversible-jump MCMC to control the output of\nstochastic context-free grammars. Meta-Sim [28] learned a model that modiﬁes attributes of scene\ngraphs sampled from a known probabilistic context-free grammar to match visual statistics between\ngenerated and real data. [11] extended this model to also learn to sample from the grammar, allowing\ncontext dependent relationships to be learnt. Concurrently, [ 58] employed Grammar-V AE [38] to\ngenerate scenes using a scene grammar generated from annotated data. In contrast, our model\nimplicitly encapsulates inter-object relationships, without having to impose hand-crafted constraints.\nGraph-based Scene Synthesis: Representing scenes as graphs has been extensively studied in\nliterature [41, 72, 80, 42, 58, 79, 78, 32, 14]. Zhou et al. [80] introduced a neural message passing\nalgorithm for scene graphs that predicts the category of the next object to be placed at a speciﬁc\nlocation. Similarly, [41, 79, 58, 42] utilized a V AE [34] to synthesize 3D scenes as parse trees [58],\nadjacency matrices [79], scene graphs [42] and scene hierarchies [41]. Concurrently, [72, 78] adopted\na two-stage generation process that disentangles planning the scene layout from instantiating the\nscene based on this plan. Note that graph-based models require supervision either in the form of\nrelation graphs [72, 78, 42] or scene hierarchies [41]. In contrast, ATISS infers functional and spatial\nrelations between objects directly from data labeled only with object classes and 3D bounding boxes.\n2\nFigure 2: Method Overview.Starting from a scene withMobjects and a ﬂoor layout, thelayout encodermaps\nthe ﬂoor into a feature representation F and the structure encodermaps the objects into a context embedding\nC = {Cj}M\nj=1. The ﬂoor layout feature F, the context embedding C and a learnable query vector q are then\npassed to the transformer encoderthat predicts ˆq. Using ˆq the attribute extractorautoregressively predicts\nthe attribute distributions that are used to sample the attributes for the next object to be generated.\nAutoregressive Scene Synthesis: Closely related to our work are autoregressive indoor scene\ngeneration models [73, 61, 74]. Ritchie et al. [61] introduced a CNN-based architecture that operates\non a top-down image-based representation of a scene and inserts objects in it sequentially by predicting\ntheir category, location, orientation and size with separate network modules. [61] requires supervision\nin the form of 2D bounding boxes as well as auxiliary supervision such as depth maps and object\nsegmentation masks. In concurrent work, Wang et al. [ 74] introduced SceneFormer, a series of\ntransformers that autoregressively add objects in a scene similar to [61]. Both [61, 74] use separate\nmodels to generate object attributes (e.g. category, location) that are trained independently and\nrepresent scenes as ordered sequences of objects, ordered by the category frequency. In contrast,\nwe propose a simpler architecture that consists of a single model trained end-to-end to predict\nall attributes. We provide experimental evidence that our model generates more realistic object\narrangements while being signiﬁcantly faster. While [61, 74] assume a ﬁxed ordering of the objects\nin each scene, our model does not impose any constraint on the ordering of objects. Instead, during\ntraining, we enforce that our model generates objects with all orderings, in a permutation invariant\nfashion. This allows us to represent scenes as unordered sets of objects and perform various interactive\ntasks such as rearranging any object in a room or suggesting new objects given any room.\nTransformers for Set Generation:Transformer models [71] demonstrated impressive results on\nvarious tasks such as machine translation [ 64, 49], language-modeling [ 2, 12], object detection\n[40, 3, 81], image recognition [16, 67], semantic segmentation [75] as well as on image [52, 31, 6,\n17, 68] and music [13] generation tasks. While there are works [39, 36] that utilize the permutation\nequivariance property of transformers for unordered set processing and prediction, existing generative\nmodels with transformers assume ordered sequences [2, 6, 7] even when there exists no natural order\nsuch as for pointclouds [ 48] and objects in a scene [ 74]. Instead, we introduce an autoregressive\ntransformer for unordered set generation that enforces that the probability of adding a new element in\nthe set is invariant to the order of the elements already in the set. We show that for the scene synthesis\ntask, our model outperforms transformers that consider ordered sets of elements in every metric.\n3 Method\nGiven an empty or a partially complete room of a speciﬁc type (e.g. bedroom) together with its shape,\nas a top-down orthographic projection of its ﬂoor, we want to learn a generative model that populates\nthe room with objects, whose functional composition and spatial arrangement is plausible. To this\nend, we propose an autoregressive model that represents scenes asunordered sets of objects(Sec. 3.1)\nand describe our implementation using a transformer network (Sec. 3.2). Finally, we analyse the\ntraining and inference details of our method (Sec. 3.3).\n3.1 Autoregressive Set Generation\nLet X = {X1,..., XN}denote a collection of scenes where each Xi =\n(\nOi,Fi)\ncomprises the\nunordered set of objects in the sceneOi = {oi\nj}M\nj=1 and its ﬂoor layout Fi. To compute the likelihood\nof generating Oi we need to accumulate the likelihood of generating {oi\nj}M\nj=1 autoregressively in any\n3\norder. This is formally written as\npθ(Oi|Fi) =\n∑\nˆO∈π(Oi)\n∏\nj∈ˆO\npθ(oi\nj |oi\n<j,Fi), (1)\nwhere pθ(oi\nj |oi\n<j,Fi) is the probability of the j-th object, conditioned on the previously generated\nobjects and the ﬂoor layout, and π(·) is a permutation function that computes the set of permutations\nof all objects in the scene. As a result, the log-likelihood of the whole collection Xis\nlog pθ(X) =\nN∑\ni=1\nlog\n\n ∑\nˆO∈π(Oi)\n∏\nj∈ˆO\npθ(oi\nj |oi\n<j,Fi)\n\n. (2)\nHowever, training our generative model to maximize the log-likelihood of(2) poses two problems:\n(a) the summation over all permutations is intractable and (b) (2) does not ensure that all orderings\nwill have high probability. The second problem is crucial because we want our generative model to\nbe able to complete any partial setin a plausible way, namely we want any generation order to have\nhigh probability. To this end, instead of maximizing (2), we maximize the likelihood of generating a\nscene in all possible orderings, ˆpθ(·), which is deﬁned as\nlog ˆpθ(X) =\nN∑\ni=1\nlog\n\n ∏\nˆO∈π(Oi)\n∏\nj∈ˆO\npθ(oi\nj |oi\n<j,Fi)\n\n=\nN∑\ni=1\n∑\nˆO∈π(Oi)\n∑\nj∈ˆO\nlog pθ(oi\nj |oi\n<j,Fi).\n(3)\nNote that training our generative model with (3) allows us to approximate the summation over all\npermutations using Monte Carlo sampling thus solving both problems of (2).\nModelling Object Attributes:We represent objects in a scene as labeled 3D bounding boxes and\nmodel them with four random variables that describe their category, size, orientation and location,\noj = {cj,sj,tj,rj}. The category cj is modeled using a categorical variable over the total number\nof object categories Cin the dataset. For the size sj ∈R3, the location tj ∈R3 and the orientation\nrj ∈R1, we follow [62, 70] and model them with mixture of logistics distributions\nsj ∼\nK∑\nk=1\nπs\nklogistic(µs\nk,σs\nk) tj ∼\nK∑\nk=1\nπt\nklogistic(µt\nk,σt\nk) rj ∼\nK∑\nk=1\nπr\nklogistic(µr\nk,σr\nk) (4)\nwhere πs\nk, µs\nk and σs\nk denote the weight, mean and variance of the k-th logistic distribution used for\nmodeling the size. Similarly, πt\nk, µt\nk and σt\nk and πr\nk, µr\nk ans σr\nk refer to the weight, mean and variance\nof the k-th logistic of the location and orientation, respectively. In our setup, the orientation is the\nangle of rotation around the up vector and the location is the 3D centroid of the bounding box.\nSimilar to prior work [61, 74], we predict the object attributes in an autoregressive manner: object\ncategory ﬁrst, followed by position, orientation and size as follows:\npθ(oj |o<j,F) =pθ(cj|o<j,F)pθ(tj|cj,o<j,F)pθ(rj|cj,tj,o<j,F)pθ(sj|cj,tj,rj,o<j,F).\n(5)\nThis is a natural choice, since we want our model to consider the object class before reasoning about\nthe size and the position of an object. To avoid notation clutter, we omit the scene index ifrom (5).\n3.2 Network Architecture\nThe input to our model is a collection of scenes in the form of 3D labeled bounding boxes with their\ncorresponding room shape. Our network consists of four main components: (i) the layout encoder\nthat maps the room shape to a global feature representation F, (ii) the structure encoderhθ that maps\nthe M objects in the scene into per-object context embeddings C = {Cj}M\nj=1, (iii) the transformer\nencoder τθ that takes F, C and a query embedding q and predicts the features ˆq for the next object\nto be generated and (iv) the attribute extractorthat predicts the attributes of the next object. Our\nmodel is illustrated in Fig. 2. The layout encoder is simply a ResNet-18 [25] that extracts a feature\nrepresentation F ∈R64 for the top-down orthographic projection of the ﬂoor.\n4\nFigure 3: Training Overview:Given a scene with M objects (coloured squares), we ﬁrst randomly permute\nthem and then keep the ﬁrst T objects (here T = 3). We task our network to predict the next object to be added\nin the scene given the subset of kept objects (highlighted with grey) and its ﬂoor layout feature F. Our loss\nfunction is the negative log-likelihood (NLL) of the next object in the permuted sequence (green square).\nStructure Encoder:The structure encoder hθ maps the attributes of the j-th object into a per-object\ncontext embedding Cj as follows:\nhθ : RC ×R3 ×R3 ×R1 →RLc ×RLs ×RLt ×RLr\n(c,s,t,r) ↦→[λ(c); γ(s); γ(t); γ(r)] (6)\nwhere Lc,Ls,Lt,Lr are the output dimensionalities of the embeddings used to map the category, the\nsize, the location and the orientation into a higher dimensional space respectively and [·; ·] denotes\nconcatenation. For the object category cj we use a learnable embedding λ(·), whereas for the size sj,\nthe position tj and the orientation rj, we use the positional encoding of [71] as follows\nγ(p) = (sin(20πp),cos(20πp),..., sin(2L−1πp),cos(2L−1πp)) (7)\nwhere pcan be any of the size, position or orientation attributes and γ(·) is applied separately in each\nattribute’s dimension. The set of per-object context vectors synthesizes the context embeddingC that\nencapsulates information for the existing objects in the scene and is used to condition the next object\nto be generated. Before passing the output of (6) to the transformer encoder, we map each Cj to 64\ndimensions using a linear projection.\nTransformer Encoder:We follow [71, 12] and implement our encoder τθ as a multi-head attention\ntransformer without any positional encoding. This allows us to learn a parametric function that\ncomputes features that are invariant to the order of Cj in C. We use these features to predict the next\nobject to be added in the scene, creating an autoregressive model. The input set of the transformer is\nI = {F}∪{Cj}M\nj=1 ∪q, with M the number of objects in the scene. q ∈R64 is a learnable object\nquery vector that allows the transformer to predict output features ˆq ∈R64 used for generating the\nnext object to be added in the scene. The use of a query token is akin to the use of a mask embedding\nin Masked Language Modelling [12] or the class embedding for the Vision Transformer [63].\nAttribute Extractor:We autoregressively predict the attributes of the next object to be added in the\nscene using one MLP for each attribute. More formally, the attribute extractor is deﬁned as follows:\ncθ : R64 →RC ˆq ↦→ˆc (8)\ntθ : R64 ×RLc →R3×3×K (ˆq,λ(c)) ↦→ˆt (9)\nrθ : R64 ×RLc ×RLt →R1×3×K (ˆq,λ(c),γ(t)) ↦→ˆr (10)\nsθ : R64 ×RLc ×RLt ×RLr →R3×3×K (ˆq,λ(c),γ(t),γ(r)) ↦→ˆs (11)\nwhere ˆc,ˆs,ˆt,ˆr are the predicted attribute distributions and cθ, tθ, rθ and sθ are mappings between\nthe latent space and the low-dimensional space of attributes. For the object category, cθ predicts C\nclass probabilities, whereas, tθ, rθ and sθ predict the mean, variance and mixing coefﬁcient for the K\nlogistic distributions for each attribute. To predict the object properties in an autoregressive manner,\nwe need to condition the prediction of a property on the previously predicted properties. Thus, instead\nof only passing ˆq to each MLP, we concatenate it with the previously predicted attributes, mapped in\na higher-dimensional space using the embeddings λ(·) and γ(·) from (6).\n3.3 Training and Inference\nDuring training, we choose a scene from the dataset and apply a random permutation π(·) on its\nM objects. Then, we randomly select the ﬁrst T objects to compute the context embedding C.\n5\nScene Layout Training Sample FastSynth SceneFormer Ours\nFigure 4: Qualitative Scene Synthesis Results. Synthesized scenes for three room types: bedrooms (1st +2nd\nrow), living room (3rd row), dining room (4th row) using FastSynth, SceneFormer and our method. To showcase\nthe generalization abilities of our model we also show the closest scene from the training set (2nd column).\nConditioned on C and F, our network predicts the attribute distributions of the next object to be\nadded in the scene and is trained to maximize the log-likelihood of theT+1 object from the permuted\nscene. A pictorial representation of the training process is provided in Fig. 3. To indicate the end of\nsequence, we augment the Cobject classes with an additional class, which we refer to as end symbol.\nDuring inference, we start with an empty context embedding C = ∅and the ﬂoor representation F of\nthe room to be populated and autoregressively sample attribute values from the predicted distributions\nof (8)-(11) for the next object. Once a new object is generated, it is appended to the context C to\nbe used in the next step of the generation process until the end symbolis generated. A pictorial\nrepresentation of the generation process can be found in Fig. 2. In order to transform the predicted\nlabeled bounding boxes to 3D models we use object retrieval. In particular, we retrieve the closest\nobject from the dataset in terms of the euclidean distance of the bounding box dimensions.\n4 Experimental Evaluation\nIn this section, we provide an extensive evaluation of our method, comparing it to existing baselines.\nWe further showcase several interactive use cases enabled by our method, not previously possible.\nAdditional results as well as implementation details are provided in the supplementary.\nDatasets: We train our model on the 3D-FRONT dataset [21] which contains a collection of 6,813\nhouses with roughly 14,629 rooms, populated with 3D furniture objects from the 3D-FUTURE\ndataset [22]. In our evaluation, we focus on four room types: (i) bedrooms, (ii) living rooms, (iii)\ndining rooms and (iv) libraries. After pre-processing to ﬁlter out uncommon object arrangements and\nrooms with unnatural sizes, we obtained 5996 bedrooms, 2962 living rooms, 2625 dining rooms and\n622 libraries. We use 21 object categories for the bedrooms, 24 for the living and dining rooms and\n25 for the libraries. The preprocessing steps are discussed in the supplementary.\nBaselines: We compare our approach to FastSynth [61] and SceneFormer [74] using the authors’\nimplementations. Note that both approaches were originally evaluated on the SUNCG dataset [65],\nwhich is now unavailable. Thus, we retrained both on 3D-FRONT. We also compare with a variant of\nour model that generates scenes as ordered sequences of objects (Ours+Order). To incorporate the\norder information to the input, we utilize a positional embedding [71] and a ﬁxed ordering based on\nthe object frequency as described in [74].\n6\nFastSynth\nSceneFormer\nOurs\nFigure 5: Scene Diversity. We show three generated scenes conditioned on three different ﬂoor plans for\nbedrooms and dining rooms. Every triplet of columns corresponds to a different ﬂoor plan.\nFID Score (↓) Scene Classiﬁcation Accuracy Category KL Divergence ( ↓)\nFastSynth SceneFormer Ours+Order OursFastSynth SceneFormer Ours+Order OursFastSynth SceneFormer Ours+Order Ours\nBedrooms40.89 43.17 38.67 38.39 0.883 0.945 0.760 0.5620.0064 0.0052 0.0533 0.0085Living 61.67 69.54 35.37 33.14 0.945 0.972 0.694 0.5160.0176 0.0313 0.0372 0.0034Dining 55.83 67.04 35.79 29.23 0.935 0.941 0.623 0.4770.0518 0.0368 0.0278 0.0061Library 37.72 55.34 35.60 35.24 0.815 0.880 0.572 0.5210.0431 0.0232 0.0183 0.0098\nTable 1: Quantitative Comparison.We report the FID score (↓) at 2562 pixels, the KL divergence (↓) between\nthe distribution of object categories of synthesized and real scenes and the real vs. synthetic classiﬁcation\naccuracy for all methods. Classiﬁcation accuracy closer to 0.5 is better.\nEvaluation Metrics: To measure the realism of the generated scenes, we follow prior work [ 61]\nand report the KL divergence between the object category distributions of synthesized and real\nscenes from the test set and the classiﬁcation accuracy of a classiﬁer trained to discriminate real\nfrom synthetic scenes. We also report the FID [26] between top-down orthographic projections of\nsynthesized and real scenes from the test set, which we compute using [ 51] on 2562 images. We\nrepeat the metric computation for FID and classiﬁcation accuracy 10 times and report the average.\n4.1 Scene Synthesis\nWe start by evaluating the performance of our model on generating plausible object conﬁgurations\nfor various room types, conditioned on different ﬂoor plans. Fig. 4 provides a qualitative comparison\nof four scenes generated with our model and baselines. In some cases, both [61, 74] generate invalid\nroom layouts with objects positioned outside room boundaries or overlapping. Instead, our model\nconsistently synthesizes realistic object arrangements. We validate this quantitatively in Tab. 1,\nwhere we compare the generated scenes wrt. their similarity to the original data from 3D-FRONT.\nSynthesized scenes sampled from our model are almost indistinguishable from scenes from the test\nset, as indicated by the classiﬁcation accuracy in Tab. 1, which is consistently around 50%. Our\nmodel also achieves lower FID scores for all room types and generates category distributions that are\nmore faithful to the category distributions of the test set, expressed as lower KL divergence.\nScene Layout FastSynth SceneFormer Ours Scene Layout FastSynth SceneFormer Ours\nFigure 6: Generalization Beyond Training Data. We show four synthesized bedrooms conditioned on four\nroom layouts that we manually designed.\n7\nPartial Scene FastSynth SceneFormer Ours+Order Ours\nFigure 7: Scene Completion. Given a partial scene (left column), we visualize scene completions using our\nmodel and our baselines. Our model consistently generates plausible layouts.\nBedroom Living Dining Library\nFastSynth [61]13193.77 30578.54 26596.08 10813.87SceneFormer [74]849.37 731.84 901.17 369.74Ours [61] 102.38 201.59 201.84 88.24\nTable 2: Generation Time Comparison.We measure\ntime (ms) to generate a scene, conditioned on a ﬂoor plan.\nFastSynth [61] SceneFormer [74] Ours\n38.180 129.298 36.053\nTable 3: Network Parameters Comparison.We\nreport the number of network parameters in millions.\nTo showcase that our model generates diverse object arrangements we visualize3 generated scenes\nconditioned on the same ﬂoor plan for all methods (Fig. 5). We observe that our generated scenes\nare consistently valid and contain diverse object arrangements. In comparison [61, 74] struggle to\ngenerate plausible layouts particularly for the case of living rooms and dining rooms. We hypothesize\nthat these rooms are more challenging than bedrooms, for the baselines, due to their signiﬁcantly\nsmaller volume of training data, and the large number of constituent objects per scene (20 on average,\nas opposed to 8). To investigate whether our model also generates plausible layouts conditioned on\nﬂoor plans with uncommon shapes that are not in the training set, we manually design unconventional\nﬂoor plans (Fig. 6) and generate bedroom layouts. While both [61, 74] fail to generate valid scenes,\nour model synthesizes diverse object layouts that are consistent with the ﬂoor plan. Finally, we\ncompare the computational requirements of our architecture to [61, 74]. Our model is signiﬁcantly\nfaster (Tab. 2), while having fewer parameters (Tab. 3) than both [61, 74]. Note that [61] is orders of\nmagnitude slower because it requires rendering every individual object added in the scene.\n4.2 Applications\nIn this section, we present three applications that greatly beneﬁt by our unordered set formulation\nand are crucial for creating an interactive scene synthesis tool.\nScene Completion: Starting from a partial scene, the task is to populate the empty space in a\nmeaningful way. Since both [61, 74] are trained on sorted sequences of objects, they ﬁrst generate\nfrequent objects (e.g. beds, wardrobes) followed by less common objects. As a result, incomplete\nscenes that contain less common objects cannot be correctly populated. This is illustrated in Fig. 7,\nwhere [61, 74] either fail to add any objects in the scene or place furnitures in unatural positions, thus\nresulting in bedrooms without beds (see 1st row Fig. 7) and scenes with overlapping furniture (see\n2nd row Fig. 7). In contrary, our model successfullly generates plausible completions with multiple\nobjects such as lamps, wardrobes and dressing tables.\nFigure 8: Failure Case Detection and Correction. We use a partial room with unnatural object arrangements.\nOur model identiﬁes the problematic objects (ﬁrst row, in green) and relocates them into meaningful positions.\n8\nLamp Double Bed Cabinet TV-stand Wardrobe Nothing\nFigure 9: Object Suggestion. A user speciﬁes a region of acceptable positions to place an object (marked as\nred boxes, 1st row) and our model suggests suitable objects (2nd row) to be placed in this location.\nFailure Case Detection and Correction:We showcase that our model is able to identify and correct\nunnatural object arrangements. Given a scene, we compute the likelihood of each object, according\nto our model, conditioned on the other objects in the scene. We identify problematic objects as those\nwith low likelihood and sample a new location from our generative model to rearrange it. We test\nour model in various scenarios such as overlapping objects, objects outside the room boundaries and\nobjects in unnatural positions and show that it successfully identiﬁes problematic objects (highlighted\nin green in Fig. 8) and rearranges them into a more plausible position. Note that this task cannot be\nperformed by methods that consider ordering because they assign very low likelihood to common\nobjects appearing after rare objects e.g. beds after cabinets.\nObject Suggestion:We now test the ability of our model to provide object suggestions given a scene\nand user speciﬁed location constraints. To perform this task we sample objects from our generative\nmodel and accept the ones that fullﬁll the constraints provided by the user. Fig. 9 shows examples of\nlocation constraints (red box in top row) and the corresponding objects suggested (bottom row). Note\nthat even when the user provided region is partially outside the room boundaries (4th, 5th column),\nsuggested objects always reside in the room. Moreover, if the acceptable region overlaps with another\nobject, our model suggests adding nothing (6th column). This task requires computing the likelihood\nof an object conditioned on an arbitrary scene, which [74, 61] cannot perform due to ordering.\n4.3 Perceptual Study\nWe conducted two paired Amazon Mechanical Turk perceptual studies to evaluate the quality of our\ngenerated layouts against [61] and [74]. We sample 6 bedroom layouts for each method from the\nsame 211 test set ﬂoor plans. Users saw 2 different rotating 3D scenes per method randomly selected\nfrom 6 pre-rendered layouts. Random layouts for each ﬂoor plan were assessed by 5 different workers\nto evaluate agreement and diversity across samples for a total of 1055 question sets per paired study.\nGenerated scenes of [61] were judged to contain errors like interpenetrating furniture 41.4% of the\ntime, nearly twice as frequently as our method, while [ 74] performs signiﬁcantly worse (Tab. 4).\nRegarding realism, the scenes of [61] were more realistic than ours in only 26.9% of the cases. We\nconclude that our method outperforms the baselines in the key metric, generation of realistic indoor\nscenes, by a large margin. Additional details are provided in the supplementary.\nMethod Condition Mean ErrorFrequency↓ More↑Realistic RealismCI99%FastSynth [61] vs. Ours0.414 0.269 [0.235,0.306]SceneFormer [74] vs. Ours0.713 0.165 [0.138,0.196]Ours vs. Both 0.232 0.783 [0.759,0.805]\nTable 4: Perceptual Study Results. Aggregated results for two A/B paired tests. Our method was judged more\nrealistic with high conﬁdence (binomial conﬁdence interval with α= 0.01 reported) and contained fewer errors.\n5 Conclusion\nWe introduced ATISS, a novel autoregressive transformer architecture for synthesizing 3D rooms\nas unordered sets of objects. Our method generates realistic scenes that advance the state-of-the-art\nfor scene synthesis. In addition, our novel formulation enables new interactive applications for\nsemi-automated scene authoring, such as general scene completion, object suggestions, anomaly\n9\ndetection and more. We believe that our model is an important step not only toward automating the\ngeneration of 3D environments, with impact on simulation and virtual testing, but also toward a new\ngeneration of tools for user-driven content generation. By accepting a wide range of user inputs,\nour model mitigates societal risks of task automation, and promises to usher in tools that enhance\nthe workﬂow of skilled laborers, rather than replacing them. In future work, we plan to extend\norder invariance to object attributes to further expand interactive possibilities of this model, and to\nincorporate style information. As any machine learning model, our model can introduce learned\nbiases for indoor scenes, and we plan to investigate learning from less structured and more widely\navailable data sources to make this model applicable to a wider range of cultures and environments.\nReferences\n[1] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high ﬁdelity natural\nimage synthesis. In Proc. of the International Conf. on Learning Representations (ICLR), 2019.\n[2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language\nmodels are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In Proc. of the European Conf. on Computer\nVision (ECCV), 2020.\n[4] Angel X. Chang, Manolis Savva, and Christopher D. Manning. Learning spatial knowledge for text to 3d\nscene generation. 2014.\n[5] Siddhartha Chaudhuri, Evangelos Kalogerakis, Stephen Giguere, and Thomas A. Funkhouser. Attribit:\ncontent creation with semantic attributes. In The 26th Annual ACM Symposium on User Interface Software\nand Technology, UIST’13, St. Andrews, United Kingdom, October 8-11, 2013, 2013.\n[6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. In Proc. of the International Conf. on Machine learning (ICML), 2020.\n[7] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv.org, 2019.\n[8] Yunjey Choi, Min-Je Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. Stargan:\nUniﬁed generative adversarial networks for multi-domain image-to-image translation. In Proc. IEEE Conf.\non Computer Vision and Pattern Recognition (CVPR), 2018.\n[9] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for\nmultiple domains. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2020.\n[10] Jia Deng, Wei Dong, Richard Socher, Li jia Li, Kai Li, and Li Fei-fei. Imagenet: A large-scale hierarchical\nimage database. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2009.\n[11] Jeevan Devaranjan, Amlan Kar, and Sanja Fidler. Meta-sim2: Unsupervised learning of scene structure for\nsynthetic data generation. In Proc. of the European Conf. on Computer Vision (ECCV), 2020.\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies,\nNAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages\n4171–4186. Association for Computational Linguistics, 2019.\n[13] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever.\nJukebox: A generative model for music. arXiv.org, 2020.\n[14] Xinhan Di, Pengqian Yu, Hong Zhu, Lei Cai, Qiuyan Sheng, and Changyu Sun. Structural plan of indoor\nscenes with personalized preferences. arXiv.org, 2020.\n[15] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In Proc. of\nthe International Conf. on Learning Representations (ICLR), 2017.\n[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and\nNeil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In Proc. of\nthe International Conf. on Learning Representations (ICLR), 2021.\n[17] Patrick Esser, Robin Rombach, and Björn Ommer. Taming transformers for high-resolution image synthesis.\nIn Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2021.\n10\n[18] Matthew Fisher, Daniel Ritchie, Manolis Savva, Thomas A. Funkhouser, and Pat Hanrahan. Example-based\nsynthesis of 3d object arrangements. 2012.\n[19] Matthew Fisher, Manolis Savva, and Pat Hanrahan. Characterizing structural relationships in scenes using\ngraph kernels. ACM Trans. on Graphics, 2011.\n[20] Matthew Fisher, Manolis Savva, Yangyan Li, Pat Hanrahan, and Matthias Nießner. Activity-centric scene\nsynthesis for functional 3d scene modeling. ACM Trans. on Graphics, 2015.\n[21] Huan Fu, Bowen Cai, Lin Gao, Lingxiao Zhang, Cao Li, Zengqi Xun, Chengyue Sun, Yiyun Fei, Yu Zheng,\nYing Li, Yi Liu, Peng Liu, Lin Ma, Le Weng, Xiaohang Hu, Xin Ma, Qian Qian, Rongfei Jia, Binqiang\nZhao, and Hao Zhang. 3d-front: 3d furnished rooms with layouts and semantics.arXiv.org, abs/2011.09127,\n2020.\n[22] Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve J. Maybank, and Dacheng Tao.\n3d-future: 3d furniture shape with texture. arXiv.org, abs/2009.09633, 2020.\n[23] Qiang Fu, Xiaowu Chen, Xiaotian Wang, Sijia Wen, Bin Zhou, and Hongbo Fu. Adaptive synthesis of\nindoor scenes via activity-associated object relation graphs. ACM Trans. on Graphics, 2017.\n[24] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information\nProcessing Systems (NIPS), 2014.\n[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2016.\n[26] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural\nInformation Processing Systems (NIPS), 2017.\n[27] Akash Abdu Jyothi, Thibaut Durand, Jiawei He, Leonid Sigal, and Greg Mori. Layoutvae: Stochastic\nscene layout generation from a label set. In Proc. of the IEEE International Conf. on Computer Vision\n(ICCV), 2019.\n[28] Amlan Kar, Aayush Prakash, Ming-Yu Liu, Eric Cameracci, Justin Yuan, Matt Rusiniak, David Acuna,\nAntonio Torralba, and Sanja Fidler. Meta-sim: Learning to generate synthetic datasets. In Proc. of the\nIEEE International Conf. on Computer Vision (ICCV), 2019.\n[29] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial\nnetworks. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2019.\n[30] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and\nimproving the image quality of StyleGAN. 2020.\n[31] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast\nautoregressive transformers with linear attention. In Proc. of the International Conf. on Machine learning\n(ICML), 2020.\n[32] Mohammad Keshavarzi, Aakash Parikh, Xiyu Zhai, Melody Mao, Luisa Caldas, and Allen Y . Yang.\nScenegen: Generative contextual scene augmentation using scene graph priors. arXiv.org, 2020.\n[33] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proc. of the\nInternational Conf. on Learning Representations (ICLR), 2015.\n[34] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. Proc. of the International Conf.\non Learning Representations (ICLR), 2014.\n[35] Durk P Kingma and Prafulla Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolutions. In\nAdvances in Neural Information Processing Systems (NIPS), 2018.\n[36] Adam R. Kosiorek, Hyunjik Kim, and Danilo J. Rezende. Conditional set generation with transformers.\narXiv.org, 2020.\n[37] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convolutional\nneural networks. In Advances in Neural Information Processing Systems (NIPS), 2012.\n[38] Matt J Kusner, Brooks Paige, and José Miguel Hernández-Lobato. Grammar variational autoencoder. In\nProc. of the International Conf. on Machine learning (ICML), pages 1945–1954. PMLR, 2017.\n[39] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, and Yee Whye Teh. Set\ntransformer: A framework for attention-based permutation-invariant neural networks. In Proc. of the\nInternational Conf. on Machine learning (ICML), 2019.\n[40] Lingyun Luke Li, Bin Yang, Ming Liang, Wenyuan Zeng, Mengye Ren, Sean Segal, and Raquel Urtasun.\nEnd-to-end contextual perception and prediction with interaction transformer. In Proc. IEEE International\nConf. on Intelligent Robots and Systems (IROS), pages 5784–5791, 2020.\n11\n[41] Manyi Li, Akshay Gadi Patil, Kai Xu, Siddhartha Chaudhuri, Owais Khan, Ariel Shamir, Changhe Tu,\nBaoquan Chen, Daniel Cohen-Or, and Hao (Richard) Zhang. GRAINS: generative recursive autoencoders\nfor indoor scenes. ACM Trans. on Graphics, 2019.\n[42] Andrew Luo, Zhoutong Zhang, Jiajun Wu, and Joshua B. Tenenbaum. End-to-end optimization of scene\nlayout. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2020.\n[43] Rui Ma, Akshay Gadi Patil, Matthew Fisher, Manyi Li, Sören Pirk, Binh-Son Hua, Sai-Kit Yeung, Xin\nTong, Leonidas J. Guibas, and Hao Zhang. Language-driven synthesis of 3d scenes from scene databases.\nACM Trans. on Graphics, 2018.\n[44] Paul Merrell, Eric Schkufza, Zeyang Li, Maneesh Agrawala, and Vladlen Koltun. Interactive furniture\nlayout using interior design guidelines. ACM Trans. on Graphics, 2011.\n[45] Kaichun Mo, Paul Guerrero, Li Yi, Hao Su, Peter Wonka, Niloy Mitra, and Leonidas Guibas. Structurenet:\nHierarchical graph networks for 3d shape generation. In ACM Trans. on Graphics, 2019.\n[46] Kaichun Mo, Leonidas J. Guibas, Mustafa Mukadam, Abhinav Gupta, and Shubham Tulsiani. Where2act:\nFrom pixels to actions for articulated 3d objects. In Proc. of the IEEE International Conf. on Computer\nVision (ICCV), 2021.\n[47] Pascal Müller, Peter Wonka, Simon Haegler, Andreas Ulmer, and Luc Van Gool. Procedural modeling of\nbuildings. ACM Trans. on Graphics, 2006.\n[48] Charlie Nash, Yaroslav Ganin, S. M. Ali Eslami, and Peter W. Battaglia. Polygen: An autoregressive\ngenerative model of 3d meshes. In Proc. of the International Conf. on Machine learning (ICML), 2020.\n[49] Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. In\nProceedings of the Third Conference on Machine Translation: Research Papers, WMT 2018, Belgium,\nBrussels, October 31 - November 1, 2018, 2018.\n[50] Yoav I. H. Parish and Pascal Müller. Procedural modeling of cities. In ACM Trans. on Graphics, 2001.\n[51] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On buggy resizing libraries and surprising subtleties in\nFID calculation. arXiv.org, 2021.\n[52] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin\nTran. Image transformer. In Proc. of the International Conf. on Machine learning (ICML), 2018.\n[53] Despoina Paschalidou, Angelos Katharopoulos, Andreas Geiger, and Sanja Fidler. Neural parts: Learning\nexpressive 3d shape abstractions with invertible neural networks. In Proc. IEEE Conf. on Computer Vision\nand Pattern Recognition (CVPR), 2021.\n[54] Despoina Paschalidou, Ali Osman Ulusoy, and Andreas Geiger. Superquadrics revisited: Learning 3d\nshape parsing beyond cuboids. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),\n2019.\n[55] Despoina Paschalidou, Luc van Gool, and Andreas Geiger. Learning unsupervised hierarchical part\ndecomposition of 3d objects from a single rgb image. In Proc. IEEE Conf. on Computer Vision and Pattern\nRecognition (CVPR), 2020.\n[56] Adam Paszke, Abhishek Chaurasia, Sangpil Kim, and Eugenio Culurciello. Enet: A deep neural network\narchitecture for real-time semantic segmentation. arXiv.org, 1606.02147, 2016.\n[57] Aayush Prakash, Shaad Boochoon, Mark Brophy, David Acuna, Eric Cameracci, Gavriel State, Omer\nShapira, and Stan Birchﬁeld. Structured domain randomization: Bridging the reality gap by context-aware\nsynthetic data. In Proc. IEEE International Conf. on Robotics and Automation (ICRA), 2019.\n[58] Pulak Purkait, Christopher Zach, and Ian Reid. SG-V AE: scene grammar variational autoencoder to\ngenerate new indoor scenes. In Proc. of the European Conf. on Computer Vision (ECCV), 2020.\n[59] Siyuan Qi, Yixin Zhu, Siyuan Huang, Chenfanfu Jiang, and Song-Chun Zhu. Human-centric indoor scene\nsynthesis using stochastic grammar. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition\n(CVPR), 2018.\n[60] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners. arXiv.org, 2019.\n[61] Daniel Ritchie, Kai Wang, and Yu-An Lin. Fast and ﬂexible indoor scene synthesis via deep convolutional\ngenerative models. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2019.\n[62] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. Pixelcnn++: Improving the pixelcnn\nwith discretized logistic mixture likelihood and other modiﬁcations. In Proc. of the International Conf. on\nLearning Representations (ICLR), 2017.\n[63] Gilad Sharir, Asaf Noy, and Lihi Zelnik-Manor. An image is worth 16x16 words, what is a video worth?\nIn Proc. of the International Conf. on Learning Representations (ICLR), 2021.\n12\n[64] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In\nProceedings of the 2018 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, NAACL-HLT, New Orleans, Louisiana, USA, June 1-6, 2018,\nVolume 2 (Short Papers), 2018.\n[65] Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Manolis Savva, and Thomas Funkhouser. Semantic\nscene completion from a single depth image. In Proc. IEEE Conf. on Computer Vision and Pattern\nRecognition (CVPR), 2017.\n[66] Jerry O. Talton, Yu Lou, Steve Lesser, Jared Duke, Radomír Mech, and Vladlen Koltun. Metropolis\nprocedural modeling. ACM Trans. on Graphics, 2011.\n[67] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé\nJégou. Training data-efﬁcient image transformers & distillation through attention. arXiv.org, 2020.\n[68] Shubham Tulsiani and Abhinav Gupta. Pixeltransformer: Sample conditioned signal generation. In Proc.\nof the International Conf. on Machine learning (ICML), 2021.\n[69] Shubham Tulsiani, Hao Su, Leonidas J. Guibas, Alexei A. Efros, and Jitendra Malik. Learning shape\nabstractions by assembling volumetric primitives. In Proc. IEEE Conf. on Computer Vision and Pattern\nRecognition (CVPR), 2017.\n[70] Aäron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal\nKalchbrenner, Andrew W. Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio.\nIn The 9th ISCA Speech Synthesis Workshop, Sunnyvale, CA, USA, 13-15 September 2016, 2016.\n[71] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing\nSystems (NIPS), pages 5998–6008, 2017.\n[72] Kai Wang, Yu-An Lin, Ben Weissmann, Manolis Savva, Angel X. Chang, and Daniel Ritchie. Planit:\nplanning and instantiating indoor scenes with relation graph and spatial prior networks. ACM Trans. on\nGraphics, 2019.\n[73] Kai Wang, Manolis Savva, Angel X. Chang, and Daniel Ritchie. Deep convolutional priors for indoor\nscene synthesis. ACM Trans. on Graphics, 37(4):70:1–70:14, 2018.\n[74] Xinpeng Wang, Chandan Yeshwanth, and Matthias Nießner. Sceneformer: Indoor scene generation with\ntransformers. arXiv.org, 2020.\n[75] Linwei Ye, Mrigank Rochan, Zhi Liu, and Yang Wang. Cross-modal self-attention network for referring\nimage segmentation. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2019.\n[76] Lap-Fai Yu, Sai Kit Yeung, Chi-Keung Tang, Demetri Terzopoulos, Tony F. Chan, and Stanley J. Osher.\nMake it home: automatic optimization of furniture arrangement. ACM Trans. on Graphics, 2011.\n[77] Lap-Fai Yu, Sai Kit Yeung, and Demetri Terzopoulos. The clutterpalette: An interactive tool for detailing\nindoor scenes. IEEE Trans. Vis. Comput. Graph., 2016.\n[78] Song-Hai Zhang, Shaokui Zhang, Wei-Yu Xie, Cheng-Yang Luo, and Hong-Bo Fu. Fast 3d indoor scene\nsynthesis with discrete and exact layout pattern extraction. arXiv.org, 2020.\n[79] Zaiwei Zhang, Zhenpei Yang, Chongyang Ma, Linjie Luo, Alexander Huth, Etienne V ouga, and Qixing\nHuang. Deep generative modeling for scene synthesis via hybrid representations. ACM Trans. on Graphics,\n2020.\n[80] Yang Zhou, Zachary While, and Evangelos Kalogerakis. Scenegraphnet: Neural message passing for 3d\nindoor scene augmentation. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2019.\n[81] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable DETR: deformable\ntransformers for end-to-end object detection. Proc. of the International Conf. on Learning Representations\n(ICLR), 2021.\n13\nSupplementary Material for\nATISS: Autoregressive Transformers for Indoor\nScene Synthesis\nAbstract\nIn this supplementary document, we provide a detailed overview of our network\narchitecture and the training procedure. Subsequently, we describe the preprocess-\ning steps that we followed to ﬁlter out problematic rooms from the 3D-FRONT\ndataset [21]. Next, we provide ablations on how different components of our system\nimpact the performance of our model on the scene synthesis task and we compare\nATISS with various transformer models that consider ordering. Finally, we provide\nadditional qualitative and quantitative results as well as additional details for our\nperceptual study presented in Sec 4.3 in our main submission.\nA Implementation Details\nIn this section, we provide a detailed description of our network architecture. We then describe our\ntraining protocol and provide details on the metrics computation during training and testing. Finally,\nwe also provide additional details regarding our baselines.\nA.1 Network Architecture\nHere we describe the architecture of each individual component of our model (from Fig. 2 in the\nmain submission). Our architecture comprises four components: (i) the layout encoderthat maps the\nroom shape to a global feature representation F, (ii) the structure encoderthat maps the M objects in\na scene into per-object context embeddings C = {Cj}M\nj=1, (iii) the transformer encoderthat takes F,\nC and a query embedding q and predicts the features ˆq for the next object to be generated and (iv)\nthe attribute extractorthat autoregressively predicts the attributes of the next object.\nLayout Encoder: The ﬁrst part of our architecture is the layout encoderthat is used to map the\nroom’s ﬂoor into a global feature representationF. We follow [73] and we model the ﬂoor plan with\nits top-down orthographic projection. This projection maps the ﬂoor plan into an image, where pixel\nvalues of 1 indicate regions inside the room and pixel values of 0 otherwise. The layout encoder\nis implemented with a ResNet-18 architecture [ 25] that is not pre-trained on ImageNet [ 10]. We\nempirically observed that using a pre-trained ResNet resulted in worse performance. From the\noriginal architecture, we remove the ﬁnal fully connected layer and replace it with a linear projection\nto 64 dimensions, after average pooling.\nStructure Encoder: The structure encoder maps the attributes of each object into a per-object\ncontext embedding Cj. For the object category cj, we use a learnable embedding, which is simply\na matrix of size C×64, that stores a per-object category vector, for all C object categories in the\ndataset. For the size sj, the position tj and the orientation rj, we use the positional encoding of [71]\nas follows\nγ(p) = (sin(20πp),cos(20πp),..., sin(2L−1πp),cos(2L−1πp)) (12)\nwhere pcan be any of the size, position or orientation attributes and γ(·) is applied separately in each\nattribute’s dimension. In our experiments,Lis set to 32. The output of each embedding layer, used to\nmap the category, size, location and orientation in a higher dimensional space, are concatenated into\nan 512-dimensional feature vector, which is then mapped to the per-object context embedding. A\npictorial representation of the structure encoder is provided in Fig. 10.\nTransformer Encoder: We follow [71, 12] and implement our transformer encoder as a multi-\nhead attention transformer without any positional encoding. Our transformer consists of 4 layers\n14\nFigure 10: Structure Encoder.The structure encoder predicts the per-object context embeddings Cj condi-\ntioned on the object attributes. For the object category cj, we use a learnable embedding λ(·), whereas for the\nlocation tj, the size sj and orientation rj we employ the positional encoding from (12). Note that the positional\nencoding γ(·) is applied separately in each dimension of tj and sj.\nwith 8 attention heads. The queries, keys and values have 64 dimensions and the intermediate\nrepresentations for the MLPs have 1024 dimensions. To implement the transformer architecture we\nuse the transformer library provided by Katharopoulos et al. [31]1. The input set of the transformer\nis I = {F}∪{Cj}M\nj=1 ∪q, where M denotes the number of objects in the scene and q ∈R64 is a\nlearnable object query vector that allows the transformer to predict output features ˆq ∈R64 used for\ngenerating the next object to be added in the scene.\n(a) tθ(·) predicts the parameters of the mixture of\nlogistics distribution for the location t.\n(b) sθ(·) predicts the parameters of the mixture of\nlogistics distribution for the size s.\nFigure 11: Attribute Extractor.The attribute extractor consists of four MLPs that autoregressively predict the\nobject attributes. Here we visualize the MLP tθ(·) for the location attribute (left side) and the MLP sθ(·) for the\nsize attribute (right side).\nAttribute Extractor:The attribute extractor autoregressively predicts the attributes of the next object\nto be added in the scene. The MLP for the object category is a linear layer with 64 hidden dimensions\nthat predicts Cclass probabilities per object. The MLPs for the location, orientation and size predict\nthe mean, variance and mixing coefﬁcient for the Klogistic distributions for each attribute. In our\nexperiments we set K = 10. The size, location and orientation attributes are predicted using a 2-layer\nMLP with RELU non-linearities with hidden size 128 and output size 64. A pictorial representation\nfor the MLPs tθ(·) and σθ(·) used to predict the parameters of the mixture of logistics distribution for\nthe location and the size is provided in Fig. 11. Note that rθ is deﬁned in a similar manner.\n1https://github.com/idiap/fast-transformers\n15\nA.2 Object Retrieval\nDuring inference, we select 3D models from the 3D-FUTURE dataset [22] to be placed in the scene\nbased on the predicted category, location, orientation and size. In particular, we perform nearest\nneighbor search through the 3D-FUTURE dataset[22] to ﬁnd the closest model in terms of object\ndimensions. While prior work [ 61, 74] explored more complex object retrieval schemes based on\nobject dimensions and object cooccurrences (i.e. favor 3D model of objects that frequently co-occur\nin the dataset), we note that our simple object retrieval strategy consistently resulted in visually\nplausible rooms. We leave more advanced object retrieval schemes for future research.\nA.3 Training Protocol\nIn all our experiments, we use the Adam optimizer [33] with learning rate η= 10−4 and no weight\ndecay. For the other hyperparameters of Adam we use the PyTorch defaults: β1 = 0.9, β2 = 0.999\nand ϵ= 10−8. We train all models with a batch size of 128 for 100k iterations. During training, we\nperform rotation augmentation with random rotations between [0,360] degrees. To determine when\nto stop training, we follow common practice and evaluate the validation metric every 1000 iterations\nand use the model that performed best as our ﬁnal model.\nA.4 Metrics Computation\nAs mentioned in our main submission, we evaluate our model and our baselines using the KL diver-\ngence between the object category distributions of synthesized and real scenes and the classiﬁcation\naccuracy of a classiﬁer trained to discriminate real from synthetic scenes as well as the FID [ 26]\nscore between 2562 top-down orthographic projections of synthesized and real scenes using the code\nprovided by Parmar et al. [51]2. For the metrics computation, we generate the same amount of scenes\nas in the test set and we compute each metric using real scenes from the test set. In particular, for the\nKL divergence, we measure the frequency of object category occurrences in the generated scenes and\ncompare it with the frequency of object occurrences in real scenes. Regarding the scene classiﬁcation\naccuracy, we train a classiﬁer to distinguish real from generated scenes. Our classiﬁer is an Alexnet\n[37] pre-trained on ImageNet, that takes as input a 2562 top-down image-based representation of\na room and predicts whether this scene is real or synthetic. Both for the FID and the classiﬁcation\naccuracy, we repeat the metric computation 10 times and report the average.\nA.5 Baselines\nIn this section, we provide additional details regarding our baselines. We compare our model with\nFastSynth [61] and SceneFormer [74]. Both methods were originally evaluated on the SUNCG dataset\n[65], which is currently unavailable, thus we retrained both on 3D-FRONT using the augmentation\ntechniques described in the original papers. To ensure fair comparison, we use the same object\nretrieval for all methods and no rule-based post-processing on the generated layouts.\nFastSynth: In FastSynth [61], the authors employ a series of image-based CNNs to sequentially\npredict the attributes of the next object to be added in the scene. In addition to 2D labeled bounding\nboxes they have auxiliary supervision in the form of object segmentation masks, depth maps, wall\nmasks etc. For more details, we refer the reader to [ 73]. During training, they assume that there\nexists an ordering of objects in each scene, based on the average size of each category multiplied by\nits frequency of occurrences in the dataset. Each CNN module is trained separately and the object\nproperties are predicted in an autoregressive manner: object category ﬁrst, followed by location,\norientation and size. We train [61]3 using the provided PyTorch [56] implementation with the default\nparameters until convergence.\nSceneFormer: In SceneFormer [74], the authors utilize a series of transformers to autoregressively\nadd objects in the scene, similar to [ 61]. In particular, they train a separate transformer for each\nattribute and they predict the object properties in an autoregressive manner: object category ﬁrst,\nfollowed by orientation, location and size. Similar to [61], they also treat scenes as ordered sequences\n2https://github.com/GaParmar/clean-ﬁd\n3https://github.com/brownvc/fast-synth\n16\nof objects ordered by the frequency of their categories. We train [74]4 using the provided PyTorch\n[56] implementation with the default parameters until convergence.\nB 3D-FRONT Dataset Filtering\nWe evaluate our model on the 3D-FRONT dataset [21], which is one of the few available datasets\nthat contain indoor environments. 3D-FRONT contains a collection of 6813 houses with roughly\n14629 designed rooms, populated with 3D furniture objects from the 3D-FUTURE dataset [22]. In\nour experiments, we focused on four room types: (i) bedrooms, (ii) living rooms, (iii) dining rooms\nand (iv) libraries. Unfortunately, 3D-FRONT contains multiple problematic rooms with unnatural\nsizes, misclassiﬁed objects as well as objects in unnatural positions e.g. outside the room boundaries,\nlamps on the ﬂoor, overlapping objects etc. Therefore, in order to be able to use it, we had to\nperform thorough ﬁltering to remove problematic scenes. In this section, we present in detail the\npre-processing steps for each room type. We plan to release the names/ids of the ﬁltered rooms, when\nthe paper is published.\nThe 3D-FRONT dataset provides scenes for the following room types: bedroom, diningroom, elderly-\nroom, kidsroom, library, livingdiningroom, livingroom, masterbedroom, nannyroom, secondbedroom\nthat contain 2287, 3233, 233, 951, 967, 2672, 1095, 3313, 16 and 2534 rooms respectively. Since\nsome room types have very few rooms we do not consider them in our evaluation.\nBedroom: To create training and test data for bedroom scenes, we consider rooms of type bedroom,\nsecondbedroom and masterbedroom, which amounts to 8134 rooms in total. We start by removing\nrooms of unnatural sizes, namely rooms that are larger than 6m ×6m in ﬂoor size and taller than 4m.\nNext, we remove infrequent objects that appear in less than 15 rooms, such as chaise lounge sofa,\nl-shaped sofa, barstool, wine cabinet etc. Subsequently, we ﬁlter out rooms that contain fewer than\n3 and more than 13 objects, since they amount to a small portion of the dataset. Since the original\ndataset contained various rooms with problematic object arrangements such overlapping objects,\nwe also remove rooms that have objects that are overlapping as well as misclassiﬁed objects e.g.\nbeds being classiﬁed as wardrobes. This results in 5996 bedrooms with 21 object categories in total.\nFig. 12a illustrates the number of appearances of each object category in the 5996 bedroom scenes\nand we remark that the most common category is the nightstand with 8337 occurrences and the least\ncommon is the coffee table with 45.\nLibrary: We consider rooms of type library that amounts to 967 scenes in total. For the case of\nlibraries, we start by ﬁltering out rooms with unnatural sizes that are larger than 6m ×6m in ﬂoor\nsize and taller than 4m. Again we remove rooms that contain overlapping objects, objects positioned\noutside the room boundaries as well as rooms with unnatural layouts e.g. single chair positioned in\nthe center of the room. We also ﬁlter out rooms that contain less than 3 objects and more than 12\nobjects since they appear less frequently. Our pre-processing resulted in 622 rooms with 19 object\ncategories in total. Fig. 12b shows the number of appearances of each object category in the 622\nlibraries. The most common category is the bookshelf with 1109 occurrences and the least common\nis the wine cabinet with 19.\n0 1000 2000 3000 4000 5000 6000 7000 8000\n# Objects\ncoﬀee table\nsofa\narmchair\ndressing chair\nchildren cabinet\nkids bed\nbookshelf\nshelf\nstool\ndesk\ndressing table\nsingle bed\ntable\ncabinet\nchair\ntv stand\nceiling lamp\npendant lamp\ndouble bed\nwardrobe\nnightstand\n(a) Bedrooms\n0 200 400 600 800 1000\n# Objects\nwine cabinet\ndining table\nwardrobe\nmulti seat sofa\nloveseat sofa\nround end table\ncabinet\nstool\nconsole table\nshelf\narmchair\nchinese chair\ncorner side table\ndining chair\nceiling lamp\nlounge chair\npendant lamp\ndesk\nbookshelf (b) Libraries\nFigure 12: Number of object occurrences in Bedrooms and Libraries.\n4https://github.com/cy94/sceneformer\n17\n0 2000 4000 6000 8000\n# Objects\nchaise longue sofalazy sofadeskchinese chairwardrobeshelfround end tablel shaped sofawine cabinetcabinetloveseat sofabookshelfceiling lampstoollounge chairconsole tablearmchairmulti seat sofatv standdining tablecorner side tablecoﬀee tablependant lampdining chair\n(a) Living Rooms\n0 2000 4000 6000 8000 10000\n# Objects\nchaise longue sofalazy sofadeskchinese chairshelfwardroberound end tablel shaped sofawine cabinetloveseat sofabookshelfcabinetceiling lampstoollounge chairarmchairconsole tablemulti seat sofatv standcorner side tablecoﬀee tabledining tablependant lampdining chair (b) Dining Rooms\nFigure 13: Number of object occurrences in Living Rooms and Dining Rooms.\nLiving Room:For the living rooms, we consider rooms of type livingroom and livingdiningroom,\nwhich amounts to 3767 rooms. We follow a similar process as before and we start by ﬁltering out\nrooms with unnatural sizes. In particular, we discard rooms that are larger than 12m ×12m in ﬂoor\nsize and taller than 4m. We also remove uncommon objects that appear in less than 15 rooms such as\nbed and bed frame. Next, we ﬁlter out rooms that contain less than3 objects and more than 13 objects,\nsince they are signiﬁcantly less frequent. For the case of living rooms, we observed that some of the\noriginal scenes contained multiple lamps without having any other furniture. Since this is unnatural,\nwe also removed these scenes together with some rooms that had either overlapping objects or objects\npositioned outside the room boundaries. Finally, we also remove any scenes that contain any kind of\nbed e.g. double bed, single bed, kid bed etc. After our pre-processing, we ended up with 2962 living\nrooms with 24 object categories in total. Fig. 13a visualizes the number of occurrences of each object\ncategory in the living rooms. We observe that the most common category is the dining chair with\n9009 occurrences and the least common is the chaise lounge sofa with 30.\nDining Room:For the dining rooms, we consider rooms of type diningroom and livingdiningroom,\nsince the diningroom scenes amount to only 233 scenes. This results in 3233 rooms in total. For the\ndining rooms, we follow the same ﬁltering process as for the living rooms and we keep 2625 rooms\nwith 24 objects in total. Fig. 13b shows the number of occurrences of each object category in the\ndining rooms. The most common category is the dining chair with 9589 occurrences and the least\ncommon is the chaise lounge sofa with 19.\nTo generate the train, test and validation splits, we split the preprocessed rooms such that70% is used\nfor training, 20% for testing and 10% for validation. Note that the 3D-FRONT dataset comprises\nmultiple houses that may contain the same room, e.g the exact same object arrangement might appear\nin multiple houses. Thus splitting train and test scenes solely based on whether they belong to\ndifferent houses could result in the same room appearing both in train and test scenes. Therefore,\ninstea of randomly selecting rooms from houses but we select from the set of rooms with distinct\nobject arrangements.\nC Ablation Study\nIn this section, we investigate how various components of our model affect its performance on the\nscene synthesis task. In Sec. C.1, we investigate the impact of the number of logistic distributions in\nthe performance of our model. Next, in Sec. C.2, we examine the impact of the architecture of the\nlayout encoder. In Sec. C.3, we compare ATISS with two variants of our model that consider ordered\nsets of objects. Unless stated otherwise, all ablations are conducted on the bedroom scenes of the\n3D-FRONT [21] dataset.\nC.1 Mixture of Logistic distributions\nWe represent objects in a scene as labeled 3D bounding boxes and model them with four random\nvariables that describe their category, size, orientation and location, oj = {cj,sj,tj,rj}. The\ncategory cj is modeled using a categorical variable over the total number of object categoriesCin the\ndataset. For the size sj ∈R3, the location tj ∈R3 and the orientation rj ∈R1, we follow [62, 70]\n18\nand model them with a mixture of logistic distributions\nsj ∼\nK∑\nk=1\nπs\nklogistic(µs\nk,σs\nk) tj ∼\nK∑\nk=1\nπt\nklogistic(µt\nk,σt\nk) rj ∼\nK∑\nk=1\nπr\nklogistic(µr\nk,σr\nk) (13)\nwhere πs\nk, µs\nk and σs\nk denote the weight, mean and variance of the k-th logistic distribution used for\nmodeling the size. Similarly, πt\nk, µt\nk and σt\nk and πr\nk, µr\nk ans σr\nk refer to the weight, mean and variance\nof the k-th logistic of the location and orientation, respectively.\nIn this experiment, we test our model with different numbers for logistic distributions for modelling\nthe object attributes. Results are summarized in Tab. 5.\nFID (↓) Classiﬁcation Accuracy (↓) Category Distribution (↓)\nK= 1 41.71±0.4008 0.7826 ±0.0080 0.0491\nK= 5 40.41±0.2491 0.5667 ±0.0405 0.0105\nK= 10 38.39±0.3392 0.5620±0.0228 0.0085\nK= 15 40.41±0.4504 0.5980 ±0.0074 0.0095\nK= 20 40.39±0.3964 0.6680 ±0.0035 0.0076\nTable 5: Ablation Study on the Number of Logistic Distributions.This table shows a quantitative comparison\nof our approach with different numbers of Klogistic distributions for modelling the size, the location and the\norientation of each object.\nAs it is expected, using a single logistic distribution (ﬁrst row in Tab. 5) results in worse performance,\nsince it does not have enough representation capacity for modelling the object attributes. We also\nnote that increasing the number of logistic distributions beyond 10 hurts performance wrt. FID and\nclassiﬁcation accuracy. We hypothesize that this is due to overﬁtting. In our experiments we set\nK = 10.\nC.2 Layout Encoder\nWe further examine the impact of the layout encoder on the performance of our model. To this end,\nwe replace the ResNet-18 architecture [25], with an AlexNet [37]. From the original architecture, we\nremove the ﬁnal classiﬁer layers and keep only the feature vector of length 9216 after max pooling.\nWe project this feature vector to 64 dimensions with a linear projection layer. Similar to our vanilla\nmodel, we do not use an AlexNet pre-trained on ImageNet because we empirically observed that it\nresulted in worse performance.\nFID (↓) Classiﬁcation Accuracy (↓) Category Distribution (↓)\nAlexNet 40.40±0.2637 0.6083 ±0.0034 0.0064\nResNet-1838.39±0.3392 0.5620±0.0228 0.0085\nTable 6: Ablation Study on the Layout Encoder Architecture.This table shows a quantitative comparison\nof ATISS with two different layout encoders.\nTab. 6 compares the two variants of our model wrt. to the FID score, the classiﬁcation accuracy\nand the KL-divergence. We remark that our method is not particularly sensitive to the choice of the\nlayout encoder. However, using an AlexNet results in slightly worse performance, hence we utilize a\nResNet-18 in all our experiments.\nC.3 Transformers with Ordering\nIn this section, we analyse the beneﬁts of synthesizing rooms as unordered sets of objects in contrast\nto ordered sequences. To this end, we train two variants of our model that utilize a positional\nembedding [71] to incorporate order information to the input. The ﬁrst variant is trained with random\npermutations of the input (Ours+Perm+Order), similar to our model, whereas the second with a ﬁxed\nordering based on the object frequency (Ours+Order) as described in [61, 74]. We compare these\nvariants to our model on the scene synthesis task and observe that the variant with the ﬁxed ordering\n(second row Tab. 7) performs signiﬁcantly worse as the classiﬁer can identify synthesized scenes\nwith 76% accuracy. Moreover, we remark that besides enabling all the applications presented in our\nmain submission, training with random permutations also improves the quality of the synthesized\n19\nFID (↓) Classiﬁcation Accuracy (↓) Category Distribution (↓)\nOurs+Perm+Order40.18±0.2831 0.6019 ±0.0060 0.0089\nOurs+Order 38.67±0.5552 0.7603 ±0.0010 0.0533\nOurs 38.39±0.3392 0.5620±0.0228 0.0085\nTable 7: Ablation Study on Ordering.This table shows a quantitative comparison of our approach wrt. two\nvariants of our model that represent rooms as ordered sequence of objects.\nFigure 14: Failure Case Detection and Correction. Starting from a room with an unnatural object arrangement,\nour model identiﬁes the problematic objects (ﬁrst row and third row, in green) and relocates them into meaningful\npositions (second and fourth row).\nscenes (ﬁrst row Tab. 7). However, our model that is permutation invariant, namely the prediction\nis the same regardless of the order of the partial scene, performs even better (third row Tab. 7). We\nconjecture that the invariance of our model will be more even more crucial for training with either\nlarger datasets or larger scenes i.e. scenes with more objects, because observing a single order allows\nreasoning about all permutations of the partial scene.\nD Applications\nIn this section, we provide additional qualitative results for various interactive applications that beneﬁt\ngreatly by our unordered set formulation.\nD.1 Failure Case Detection And Correction\nIn this experiment, we investigate whether our model is able to identify unnatural furniture layouts\nand reposition the problematic objects such that they preserve their functional properties. As we\ndescribed in our main submission, we identify problematic objects as those with low likelihood and\nas soon as a problematic object is identiﬁed, we sample a new location from our generative model to\nreposition it. Fig. 14 shows additional qualitative results on this task. The ﬁrst and third row show\nexamples of unnatural object arrangements, together with the problematic object, highlighted in\ngreen, for each scenario. We note that our model successfully identiﬁes objects in unnatural positions\ne.g. ﬂying bed (ﬁrst row, ﬁrst column Fig. 14), light inside the bed (ﬁrst row, third column Fig. 14) or\ntable outside the room boundaries (third row, fourth column Fig. 14 ) as well as problematic objects\nthat do not necessarily look unnatural, such as a cabinet blocking the corridor (ﬁrst row, sixth column\nFig. 14), a chair facing the wall (third row, ﬁrst column Fig. 14) or a lamp being too close to the\ntable (third row, third column Fig. 14). After having identiﬁed the problematic object, our model\nconsistently repositions it at plausible position.\n20\nSofa Nightstand Nothing Lamp Stool Armchair\nTV-stand Lamp Sofa Cabinet Bookshelf Cabinet\nFigure 15: Object Suggestion. A user speciﬁes a region of acceptable positions to place an object (marked as\nred boxes, ﬁrst and third row) and our model suggests suitable objects (second and fourth row) to be placed in\nthis location.\nD.2 Object Suggestion\nFor this task, we examine the ability of our model to provide object suggestions given a scene and\nuser speciﬁed location constraints. For this experiment, the user only provides location constraints,\nnamely valid positions for the centroid of the object to be generated. Fig. 15 shows examples of\nthe location constraints, marked with red boxes, (ﬁrst and third row) and the corresponding objects\nsuggested by our model (second and fourth row). We observe that our model consistently makes\nplausible suggestions, and for the cases that a user speciﬁes a region that overlaps with other objects\nin the scene, our model suggests adding nothing (ﬁrst row, third column Fig. 15). In Fig. 15, we also\nprovide two examples, where our model makes different suggestions based on the same location\nconstraints, such as sofa and nightstand for the scenario illustrated in the ﬁrst and second column and\nstool and armchair for the scenario illustrated in the ﬁfth and sixth column in the ﬁrst row.\nD.3 Scene Completion\nStarting from a partial scene, we want to evaluate the ability of our model to generate plausible\nobject arrangements. To generate the partial scenes, we randomly sample scenes from the test set and\nremove the majority of the objects in them. Fig. 16 shows examples for various partial rooms (ﬁrst\nrow Fig. 16), as well as two alternative scene completions using our model (second and third row\nFig. 16). We observe that our model generates diverse arrangements of objects that are consistently\nmeaningful. For example, for the case where the partial scene consists of a chair and a bed (last\ncolumn Fig. 16), our model generates completions that have nightstands surrounding the bed as well\nas a desk in front of the chair.\nD.4 Object Placement\nFinally, we showcase the ability of our model to add a speciﬁc object in a scene on demand. Fig. 17\nillustrates the original scene (ﬁrst row) and the complete scene (second row) using the user speciﬁed\nobject (third row). To perform this task, we condition on the given scene and instead of sampling\nfrom the predicted object category distribution, we use the user provided object category and sample\nthe rest of the object attributes i.e. translation, size and orientation. Also in this task, we note that the\ngenerated objects are realistic and match the room layout.\n21\nPartial Scene\nCompletion 1\nCompletion 2\nFigure 16: Scene Completion. Starting from a partially complete scene (ﬁrst row), we visualize two examples\nof scene completions using our model (second and third row).\nTV-stand Bookshelf Sofa Wardrobe Chair Coffee table\nFigure 17: Object Placement. Starting from a partially complete scene, the user speciﬁes an object to be added\nin the scene and our model places it at a reasonable position. The ﬁrst rows illustrates the starting scene and the\nsecond row the generated scened using the user speciﬁed object (third row).\nE Scene Synthesis\nIn this section, we provide additional qualitative results for our scene synthesis experiment on the four\n3D-FRONT rooms. Moreover, since, we repeat the FID score and classiﬁcation accuracy computation\n10 times, in Tab. 8, we also report the standard deviation for completeness.\nFID Score (↓) Scene Classiﬁcation Accuracy Category KL Divergence ( ↓)\nFastSynth SceneFormer Ours FastSynth SceneFormer Ours FastSynth SceneFormer Ours\nBedrooms40.89±0.5098 43.17±0.692138.39±0.33920.883±0.0010 0.945±0.00090.562±0.02280.0064 0.0052 0.0085Living 61.67±1.2136 69.54±0.954233.14±0.42040.945±0.0010 0.972±0.00100.516±0.00750.0176 0.0313 0.0034Dining 55.83±1.0078 67.04±1.304329.23±0.35330.935±0.0019 0.941±0.00080.477±0.00270.0518 0.0368 0.0061Library 37.72±0.4501 55.34±0.105635.24±0.26830.815±0.0032 0.880±0.00090.521±0.00480.0431 0.0232 0.0098\nTable 8: Quantitative Comparison.We report the FID score (↓) at 2562 pixels, the KL divergence (↓) between\nthe distribution of object categories of synthesized and real scenes and the real vs. synthetic classiﬁcation\naccuracy for all methods. Classiﬁcation accuracy closer to 0.5 is better.\nConditioned on a ﬂoor plan, we evaluate the performance of our model on generating plausible\nfurniture arrangements and compare with FastSynth [61] and SceneFormer [74]. Fig. 29 provides a\nqualitative comparison of generated bedroom scenes conditioned on the same ﬂoor layout using our\nmodel and our baselines. We observe that in contrast to [61, 74], our model consistently generates\nlayouts with more diverse objects. In particular, [74] typically generates bedrooms that consist only\nof a bed, a wardrobe and less frequently also a nightstand, whereas both our model and FastSynth\nsynthesize rooms with more diverse objects. Similarly generated scenes for living rooms and dining\nrooms are provided in Fig. 30 and Fig. 31 respectively. We observe that for the case of living rooms\nand dining rooms both baselines struggle to generate plausible object arrangements, namely generated\n22\nobjects are positioned outside the room boundaries, have unnatural sizes or populate a small part of\nthe scene. We hypothesize that this might be related to the signiﬁcantly smaller amount of training\ndata compared to bedrooms. Instead our model, generates realistic living rooms and dining rooms.\nFor the case of libraries (see Fig. 32), again both [61, 74] struggle to generate functional rooms.\nE.1 Object Co-occurrence\nTo further validate the ability of our model to reproduce the probabilities of object co-occurrence in\nthe real scenes, we compare the probabilities of object co-occurrence of synthesized scenes using our\nmodel, FastSynth [61] and SceneFormer [74] for all room types. In particular, in this experiment, we\ngenerate 5000 scenes using each method and report the difference between the probabilities of object\nco-occurrences between real and synthesized scenes. Fig. 18 summarizes the absolute differences\nfor the bedroom scenes. We observe that our model better captures the object co-occurrence than\nbaselines since the absolute differences for most object pairs are consistently smaller.\narmchairbookshelfcabinet\nceiling lamp\nchair\nchildren cabinet\ncoﬀee table\ndesk\ndouble bed\ndressing chairdressing table\nkids bednightstand\npendant lamp\nshelf\nsingle bed\nsofastooltable\ntv standwardrobe\narmchair\nbookshelf\ncabinet\nceiling lamp\nchair\nchildren cabinet\ncoﬀee table\ndesk\ndouble bed\ndressing chair\ndressing table\nkids bed\nnightstand\npendant lamp\nshelf\nsingle bed\nsofa\nstool\ntable\ntv stand\nwardrobe\nOurs\narmchairbookshelfcabinet\nceiling lamp\nchair\nchildren cabinet\ncoﬀee table\ndesk\ndouble bed\ndressing chairdressing table\nkids bednightstand\npendant lamp\nshelf\nsingle bed\nsofastooltable\ntv standwardrobe\nFastSynth\narmchairbookshelfcabinet\nceiling lamp\nchair\nchildren cabinet\ncoﬀee table\ndesk\ndouble bed\ndressing chairdressing table\nkids bednightstand\npendant lamp\nshelf\nsingle bed\nsofastooltable\ntv standwardrobe\nSceneFormer\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\nFigure 18: Absolute Difference between Object Co-occurrence in Bedrooms.We visualize the absolute\ndifference of the probabilities of object co-occurrence computed between real and synthesized scenes using\nATISS (left), FastSynth (middle) and SceneFormer (right). Larger differences correspond to warmer colors and\nare worse.\nThis is also validated for the case of living rooms (Fig. 19), dining rooms (Fig. 20) and libraries\n(Fig. 21), where our model better captures the object co-occurrences than both FastSynth [61] and\nSceneFormer [74]. Note that from our analysis it becomes evident that while our method better\nreproduces the probabilities of object co-occurrence from the real scenes, all methods are able to\narmchairbookshelfcabinet\nceiling lamp\nchaise longue sofa\nchinese chaircoﬀee tableconsole table\ncorner side table\ndesk\ndining chairdining tablel shaped sofa\nlazy sofa\nlounge chairloveseat sofamulti seat sofapendant lampround end table\nshelfstool\ntv standwardrobe\nwine cabinet\narmchair\nbookshelf\ncabinet\nceiling lamp\nchaise longue sofa\nchinese chair\ncoﬀee table\nconsole table\ncorner side table\ndesk\ndining chair\ndining table\nl shaped sofa\nlazy sofa\nlounge chair\nloveseat sofa\nmulti seat sofa\npendant lamp\nround end table\nshelf\nstool\ntv stand\nwardrobe\nwine cabinet\nOurs\narmchairbookshelfcabinet\nceiling lamp\nchaise longue sofa\nchinese chaircoﬀee tableconsole table\ncorner side table\ndesk\ndining chairdining tablel shaped sofa\nlazy sofa\nlounge chairloveseat sofamulti seat sofapendant lampround end table\nshelfstool\ntv standwardrobe\nwine cabinet\nFastSynth\narmchairbookshelfcabinet\nceiling lamp\nchaise longue sofa\nchinese chaircoﬀee tableconsole table\ncorner side table\ndesk\ndining chairdining tablel shaped sofa\nlazy sofa\nlounge chairloveseat sofamulti seat sofapendant lampround end table\nshelfstool\ntv standwardrobe\nwine cabinet\nSceneFormer\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\nFigure 19: Absolute Difference between Object Co-occurrence in Living Rooms.We visualize the absolute\ndifference of the probabilities of object co-occurrence computed between real and synthesized scenes using\nATISS (left-most column), FastSynth (middle column), SceneFormer (right-most column). Lower is better.\n23\narmchairbookshelfcabinet\nceiling lamp\nchaise longue sofa\nchinese chaircoﬀee tableconsole table\ncorner side table\ndesk\ndining chairdining tablel shaped sofa\nlazy sofa\nlounge chairloveseat sofamulti seat sofapendant lampround end table\nshelfstool\ntv standwardrobe\nwine cabinet\narmchair\nbookshelf\ncabinet\nceiling lamp\nchaise longue sofa\nchinese chair\ncoﬀee table\nconsole table\ncorner side table\ndesk\ndining chair\ndining table\nl shaped sofa\nlazy sofa\nlounge chair\nloveseat sofa\nmulti seat sofa\npendant lamp\nround end table\nshelf\nstool\ntv stand\nwardrobe\nwine cabinet\nOurs\narmchairbookshelfcabinet\nceiling lamp\nchaise longue sofa\nchinese chaircoﬀee tableconsole table\ncorner side table\ndesk\ndining chairdining tablel shaped sofa\nlazy sofa\nlounge chairloveseat sofamulti seat sofapendant lampround end table\nshelfstool\ntv standwardrobe\nwine cabinet\nFastSynth\narmchairbookshelfcabinet\nceiling lamp\nchaise longue sofa\nchinese chaircoﬀee tableconsole table\ncorner side table\ndesk\ndining chairdining tablel shaped sofa\nlazy sofa\nlounge chairloveseat sofamulti seat sofapendant lampround end table\nshelfstool\ntv standwardrobe\nwine cabinet\nSceneFormer\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\nFigure 20: Absolute Difference between Object Co-occurrence in Dining Rooms.We visualize the absolute\ndifference of the probabilities of object co-occurrence computed between real and synthesized scenes using\nATISS (left-most column), FastSynth (middle column), SceneFormer (right-most column). Lower is better.\narmchairbookshelfcabinet\nceiling lamp\nchaise longue sofa\nchinese chaircoﬀee tableconsole table\ncorner side table\ndesk\ndining chairdining tabledressing chairdressing tablel shaped sofa\nlazy sofa\nlounge chairloveseat sofamulti seat sofapendant lampround end table\nshelfstool\nwardrobe\nwine cabinet\narmchair\nbookshelf\ncabinet\nceiling lamp\nchaise longue sofa\nchinese chair\ncoﬀee table\nconsole table\ncorner side table\ndesk\ndining chair\ndining table\ndressing chair\ndressing table\nl shaped sofa\nlazy sofa\nlounge chair\nloveseat sofa\nmulti seat sofa\npendant lamp\nround end table\nshelf\nstool\nwardrobe\nwine cabinet\nOurs\narmchairbookshelfcabinet\nceiling lamp\nchaise longue sofa\nchinese chaircoﬀee tableconsole table\ncorner side table\ndesk\ndining chairdining tabledressing chairdressing tablel shaped sofa\nlazy sofa\nlounge chairloveseat sofamulti seat sofapendant lampround end table\nshelfstool\nwardrobe\nwine cabinet\nFastSynth\narmchairbookshelfcabinet\nceiling lamp\nchaise longue sofa\nchinese chaircoﬀee tableconsole table\ncorner side table\ndesk\ndining chairdining tabledressing chairdressing tablel shaped sofa\nlazy sofa\nlounge chairloveseat sofamulti seat sofapendant lampround end table\nshelfstool\nwardrobe\nwine cabinet\nSceneFormer\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\nFigure 21: Absolute Difference between Object Co-occurrence in Libraries.We visualize the absolute\ndifference of the probabilities of object co-occurrence computed between real and synthesized scenes using\nATISS (left-most column), FastSynth (middle column), SceneFormer (right-most column). Lower is better.\ngenerate scenes with plausible object co-occurrences. This is expected, since learning the categories\nof objects to be added in a scene is a signiﬁcantly easier task in comparison to learning their sizes\nand positions in 3D space.\nFinally, in Fig. 22, we visualize the per-object difference in frequency of occurrence between\nsynthesized and real scenes from the test set for all room types. We observe that our model generates\nobject arrangements with comparable per-object frequencies to real rooms. In particular, for the case\nof living rooms (22b), dining rooms (22c) and libraries (22d) that are more challenging rooms types\ndue to their smaller size, our model has an even smaller discrepancy wrt. the per-object frequencies.\nE.2 Visualizations of Predicted Distributions\nIn this section, we provide examples of the predicted location distributions for different input scenes.\nIn particular, we randomly select 6 bedroom ﬂoor plans and conditioned on them we generate 5000\nscenes conditioned on each ﬂoor plan. Based on the locations of the generated objects, we create\nscatter plots for the locations of various object categories i.e. chair (Fig. 23), desk (Fig. 24), nightstand\n(Fig. 25), wardrobe (Fig. 26). We observe that for all object categories the location distributions of\nthe generated objects are consistently meaningful.\n24\ntable\nceiling lamp coﬀee table\nkids bed\nsofa shelf\narmchair\nchildren cabinet\ncabinet bookshelf\ndressing chair pendant lamp\nstool chair\nnightstand\ndressing table\nwardrobe\ndesk\ntv stand\ndouble bed single bed\n0.0000\n0.0025\n0.0050\n0.0075\n0.0100\n0.0125\n0.0150\n0.0175\nDiﬀerence of Frequency of Occurence\n0.000 0.000 0.000 0.000\n0.001\n0.001 0.002 0.002 0.002 0.002\n0.002\n0.003\n0.003 0.003 0.003\n0.004 0.004\n0.006 0.006\n0.007\n0.010\n0.004\n0.008\n0.000\n0.003\n0.000 0.000\n0.001\n0.004\n0.004\n0.002\n0.001\n0.006\n0.000\n0.008\n0.019\n0.003\n0.008\n0.004\n0.001\n0.002\n0.006\n0.004\n0.009\n0.000\n0.001\n0.000\n0.002\n0.001\n0.000\n0.001\n0.002\n0.000\n0.002\n0.004\n0.001\n0.018\n0.002\n0.015\n0.006\n0.003\n0.001\n0.010\nOurs\nFastSynth\nSceneFormer\n(a) Bedrooms\nchaise longue sofa\nl shaped sofa\ndesk\nlazy sofa\nmulti seat sofa pendant lamp\ntv stand\nchinese chair\nshelf\nround end table\nstool\nbookshelf armchair\nlounge chair ceiling lamp\nwardrobe\nconsole table\ncabinet\nloveseat sofa dining table wine cabinet dining chair coﬀee table\ncorner side table\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\nDiﬀerence of Frequency of Occurence\n0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.001 0.001 0.002 0.002 0.002 0.002 0.002 0.002 0.003 0.003 0.003 0.003 0.003 0.003 0.004\n0.008\n0.001\n0.002 0.001 0.000 0.001\n0.012\n0.003 0.002 0.002 0.002\n0.009\n0.004\n0.014\n0.008\n0.009\n0.002\n0.006 0.005\n0.002\n0.003\n0.006\n0.046\n0.010\n0.0030.003 0.003 0.003 0.003\n0.007\n0.010\n0.016\n0.001 0.001 0.000\n0.002\n0.004\n0.010\n0.003\n0.005\n0.000\n0.005\n0.003\n0.006\n0.017\n0.005\n0.071\n0.008\n0.007\nOurs\nFastSynth\nSceneFormer\n(b) Living Rooms\ndesk\nloveseat sofa\nlazy sofa bookshelf wardrobe\nl shaped sofa\nchaise longue sofaround end table\nceiling lamp wine cabinet chinese chair coﬀee table\nshelf\nconsole table\narmchair\nmulti seat sofa\ncabinet\ndining table\ncorner side table\nstool\ntv stand\npendant lamp lounge chair dining chair\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\nDiﬀerence of Frequency of Occurence\n0.000 0.000 0.000 0.000 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.002 0.002 0.002 0.002 0.004 0.004 0.004 0.005 0.005 0.005 0.006\n0.012 0.014\n0.001\n0.004\n0.000\n0.007\n0.000\n0.005\n0.000\n0.002\n0.008\n0.001 0.001\n0.008\n0.003\n0.007\n0.015\n0.020\n0.001 0.001\n0.022\n0.005\n0.013\n0.015\n0.020\n0.116\n0.001\n0.006\n0.001\n0.009\n0.001 0.003 0.002 0.002\n0.007\n0.004\n0.000 0.001 0.002\n0.015\n0.007\n0.014\n0.007\n0.004\n0.009\n0.013\n0.001\n0.008\n0.015\n0.095\nOurs\nFastSynth\nSceneFormer\n(c) Dining Rooms\nl shaped sofa wine cabinet dressing chair ceiling lamp coﬀee table armchair\nmulti seat sofa\nchaise longue sofa\nwardrobe\ndressing table\nstool\nconsole table loveseat sofa\ncabinet\ndining table\ncorner side table\nlazy sofa\nround end table\nlounge chair\ndesk shelf\npendant lamp dining chair chinese chair\nbookshelf\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\nDiﬀerence of Frequency of Occurence\n0.000 0.000 0.000 0.000 0.001 0.001 0.001 0.001 0.002 0.002 0.002 0.002 0.003 0.003 0.003 0.003 0.004 0.004 0.005 0.006\n0.008 0.009\n0.011\n0.014\n0.018\n0.000\n0.003\n0.000 0.000 0.001\n0.003 0.003\n0.000\n0.005\n0.002 0.003 0.002\n0.003\n0.007\n0.005\n0.011\n0.000\n0.002\n0.029\n0.007 0.007 0.007\n0.005 0.007\n0.082\n0.001 0.001 0.001\n0.022\n0.000 0.001\n0.003\n0.000\n0.005\n0.001 0.001 0.001\n0.004\n0.002 0.002\n0.011\n0.002\n0.000\n0.017\n0.002\n0.009\n0.028\n0.002 0.003\n0.011\nOurs\nFastSynth\nSceneFormer\n(d) Libraries\nFigure 22: Difference of Per-Object Frequencies.We visualize the absolute difference between the per-object\nfrequency of generated and real scenes using our method, FastSynth [61] and SceneFormer [74] for all room\ntypes. Lower is better.\nE.3 Computational Requirements\nIn this section, we provide additional details regarding the computational requirements of our method,\npresented in Table 2 and 3 in our main submission. We observe that ATISS requires signiﬁcantly less\ntime to generate a scene compared to [74, 61]. Note that the computational cost varies depending\non the room type, due to the different average number of objects for each room type. Living rooms\nand dining rooms are typically larger in size, thus more objects need to be generated to cover the\nempty space. All reported timings are measured on a machine with an NVIDIA GeForce GTX 1080\nTi GPU.\nEven though the implementations are not directly comparable, since we cannot guarantee that all\nhave been equally optimized, our ﬁndings meet our expectations. Namely, FastSynth [61] requires\nrendering the scene each time a new object is added, thus it is expected to be signiﬁcantly slower\nthan both SceneFormer and our model. On the other hand, SceneFormer [74] utilizes four different\ntransformer models for generating the attributes of each object, hence it is expected to be at least four\ntimes slower than our model, when generating the same number of objects.\nF Perceptual Study\nWe conducted two paired Amazon Mechanical Turk perceptual studies to evaluate the quality of our\ngenerated layouts against FastSynth [61] and SceneFormer [74]. To this end, we ﬁrst sampled 211\nﬂoor plans from the test set and generated 6 scenes per ﬂoor plan for each method; no ﬁltering or\npost-processing was used, and samples were randomly and independently drawn for all methods.\n25\nFigure 23: Location Distributions for Chair.\nFigure 24: Location Distributions for Desk.\n26\nFigure 25: Location Distributions for Nightstand.\nFigure 26: Location Distributions for Wardrobe.\n27\nOriginally, we considered rendering the rooms with the same furniture objects for each ﬂoor plan to\nallow participants to only focus on the layout itself, which is the main focus of our work. However,\nsince the object retrieval is done based on the object dimensions, rescaling the same furniture piece to\nﬁt all predicted dimensions would result in unrealistically deformed pieces that could skew perceptual\njudgements even more heavily. To avoid having participants focusing on the individual furniture\npieces, we added prominent instructions to focus on the layout and not the properties of selected\nobjects (see Fig. 27). Each 3D room was rendered as an animated gif using the same camera rotating\naround the room.\nFigure 27: Perceptual Study UI. A/B paired questions with rotating 3D scenes (zoom in).\nIn each user study, users were shown paired question sets: two rooms generated using our method and\ntwo generated with the baseline conditioned on the same ﬂoor plan. We randomly selected two out of\nthe 6 pre-rendered scenes for the given ﬂoor plan, and 5 different workers answered the question set\nabout every ﬂoor plan. Namely, the majority of the 6 layouts were shown more than once on average.\nA / B order was randomized to avoid bias. The question sets posed the same two questions about\nscenes generated with program A and B, in order to let users focus on the details of the results and to\nassess errors of the generated layouts. The last question forced participants to choose between A or\nB, based on which scene looks more realistic.\nSpeciﬁcally, users were instructed to pay attention to errors like interpenetrating furniture and\nfurniture outside of the ﬂoor area and answer if none, one or both layouts for each method had errors.\nWe aggregated these statistics to obtain average error rate per layout, with our method performing\nnearly twice better than the best baseline [ 61]. The results on realism in Table 4. in our main\nsubmission (ﬁrst and second row) specify the fraction of the times users chose the baseline over ours.\nFor example, [61] was judged more realistic than ours only 26.9% of the time. Because there was no\nintermediate option, this means that 73.1% of the time our method was preferred. The last line in\nTable 4, in our main submission, aggregated preference for our method across both studies.\nWorkers were compensated $0.05 per question set for a total of USD $106. The participation risks\ninvolved only the regular risks associated with the use of a computer.\nG Additional Related Work on Indoor Synthesis\nIn this section, we discuss alternative lines of work on indoor scene synthesis. Fisher et al. [ 19]\npropose to represent scenes using relationship graphs that encode spatial and semantic relationships\nbetween objects in a scene as well as the identity and semantic classiﬁcation of each object. Then,\nthey introduce a graph kernel-based scene comparison operator that allows for retrieving similar\nscenes, performing context-based model search etc. Such representations have been subsequently\nadopted in models that generate scenes conditioned on user provided constraints and interior design\nguidelines [44] or rely on a set of example images for generating plausible room layouts [18]. Another\nline of research [20, 23] leverage activity-associated object relation graphs for generating semantically\nmeaningful object arrangements. Finally, another line of research [4, 43] parses text descriptions into\na scene relationship graph that is subsequently used for arranging objects in a 3D scene.\n28\nH Discussion and Limitations\nFigure 28: Failure Cases. We visualize various failure cases of our model for different toom types.\nLastly, we discuss the limitations of our model and show some examples of failure cases in Fig. 28.\nOne type of failure case that is illustrated in Fig. 28 is overlapping objects, in particular chairs for\nthe case of living rooms and dining rooms (see second and third column in Fig. 28). As we already\ndiscussed in Sec. B, to be able to use the 3D-FRONT dataset, we performed intense ﬁltering to remove\nobjects that intersect with each other. However, we found out that not all problematic arrangements\nwere removed from the dataset, which we hypothesize is the reason for such failure cases. Another\ntype of failure case that we observed, which is also related to the existence of problematic rooms in\nour training data, is the unnatural orientation of objects (e.g. chair facing the bookshelf in ﬁrst column\nof Fig. 28 or chair facing opposite of the table in last column of Fig. 28.) Note that these failure cases\nare quite rare, as also indicated by our quantitative analysis in Sec. 4.1 in the main submission as\nwell as the perceptual study in Sec. 4.3, but our method does not guarantee error-free layouts and\nthere is room for improvement.\nOur approach is currently limited to generating object properties using a speciﬁc ordering (category\nﬁrst, followed by location, then orientation and lastly size). To further expand the interactive\npossibilities of our model, we believe that also the object attributes should be generated in an order\ninvariant fashion, similar to the objects in the scene. Furthermore, in our current formulation, the\nobject retrieval is disconnected from the attribute generation. As a result we cannot guarantee that the\nretrieved objects would match with existing objects in the scene. To address this, in the future, we\nplan to also incorporate style as an additional object attribute to allow for improved object retrieval.\nIncorporating style information, would also allows us to generate rooms conditioned on a speciﬁc\nstyle. Another exciting research direction that we would like to explore is combining ATISS with\nexisting compositional representations of objects [ 69, 54, 45, 55, 53, 46]. This will allow us to\ngenerate 3D scenes with control over the object arrangement, object parts and part relationships. Due\nto the unique characteristics of compositional representations representations, our generated scenes\nwill be fully controllable i.e. it will be possible to manipulate objects and object parts, edit speciﬁc\nparts of the scene etc.\n29\nScene Layout Training Sample FastSynth SceneFormer Ours\nFigure 29: Qualitative Scene Synthesis Results on Bedrooms. Generated scenes for bedrooms using Fast-\nSynth, SceneFormer and our method. To showcase the generalization abilities of our model we also show the\nclosest scene from the training set (2nd column).\n30\nScene Layout Training Sample FastSynth SceneFormer Ours\nFigure 30: Qualitative Scene Synthesis Results on Living Rooms. Generated scenes for living rooms using\nFastSynth, SceneFormer and our method. To showcase the generalization abilities of our model we also show\nthe closest scene from the training set (2nd column).\n31\nScene Layout Training Sample FastSynth SceneFormer Ours\nFigure 31: Qualitative Scene Synthesis Results on Dining Rooms. Generated scenes for dining rooms using\nFastSynth, SceneFormer and our method. To showcase the generalization abilities of our model we also show\nthe closest scene from the training set (2nd column).\n32\nScene Layout Training Sample FastSynth SceneFormer Ours\nFigure 32: Qualitative Scene Synthesis Results on Libraries. Generated scenes for libraries using FastSynth,\nSceneFormer and our method. To showcase the generalization abilities of our model we also show the closest\nscene from the training set (2nd column).\n33"
}