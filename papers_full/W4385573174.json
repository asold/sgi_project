{
    "title": "Language Contamination Helps Explains the Cross-lingual Capabilities of English Pretrained Models",
    "url": "https://openalex.org/W4385573174",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2516232129",
            "name": "Terra Blevins",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A334758317",
            "name": "Luke Zettlemoyer",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2805206884",
        "https://openalex.org/W4286974328",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W3012990076",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2963626623",
        "https://openalex.org/W3213241618",
        "https://openalex.org/W2963826397",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3018647120",
        "https://openalex.org/W2985094609",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W4287065445",
        "https://openalex.org/W2995549860",
        "https://openalex.org/W3034469191",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W4385573090",
        "https://openalex.org/W4286987091",
        "https://openalex.org/W3137010024"
    ],
    "abstract": "English pretrained language models, which make up the backbone of many modern NLP systems, require huge amounts of unlabeled training data. These models are generally presented as being trained only on English text but have been found to transfer surprisingly well to other languages. We investigate this phenomenon and find that common English pretraining corpora actually contain significant amounts of non-English text: even when less than 1% of data is not English (well within the error rate of strong language classifiers), this leads to hundreds of millions of foreign language tokens in large-scale datasets. We then demonstrate that even these small percentages of non-English data facilitate cross-lingual transfer for models trained on them, with target language performance strongly correlated to the amount of in-language data seen during pretraining. In light of these findings, we argue that no model is truly monolingual when pretrained at scale, which should be considered when evaluating cross-lingual transfer.",
    "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3563–3574\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nLanguage Contamination Helps Explain the Cross-lingual\nCapabilities of English Pretrained Models\nTerra Blevins1 Luke Zettlemoyer1,2\n1 Paul G. Allen School of Computer Science & Engineering, University of Washington\n2 Meta AI Research\n{blvns, lsz}@cs.washington.edu\nAbstract\nEnglish pretrained language models, which\nmake up the backbone of many modern NLP\nsystems, require huge amounts of unlabeled\ntraining data. These models are generally pre-\nsented as being trained only on English text\nbut have been found to transfer surprisingly\nwell to other languages. We investigate this\nphenomenon and find that common English\npretraining corpora actually contain significant\namounts of non-English text: even when less\nthan 1% of data is not English (well within\nthe error rate of strong language classifiers),\nthis leads to hundreds of millions of foreign\nlanguage tokens in large-scale datasets. We\nthen demonstrate that even these small percent-\nages of non-English data facilitate cross-lingual\ntransfer for models trained on them, with tar-\nget language performance strongly correlated\nto the amount of in-language data seen during\npretraining. In light of these findings, we ar-\ngue that no model is truly monolingual when\npretrained at scale, which should be considered\nwhen evaluating cross-lingual transfer.\n1 Introduction\nPretrained language models have become an in-\ntegral part of NLP systems. They come in two\nflavors: monolingual, where the model is trained\non text from a single language, and multilingual,\nwhere the model is jointly trained on data from\nmany different languages. Monolingual pretrained\nmodels are generally applied to tasks in the same\nlanguage, whereas multilingual ones are used for\ncross-lingual tasks or transfer.\nRecent work has claimed that monolingual\npretrained models are also surprisingly good at\ntransferring between languages, despite ostensi-\nbly having never seen the target language before\n(Gogoulou et al., 2021; Li et al., 2021, inter alia).\nHowever, because of the large scale of pretrain-\ning data and because many pretraining corpora are\nnot publicly available, it is currently unknown how\nFigure 1: Estimated non-English data in English pre-\ntraining corpora (token count and total percentage); even\nsmall percentages lead to many tokens. C4.En (†) is es-\ntimated from the first 50M examples in the corpus.\nmuch foreign language data exists in monolingual\npretraining corpora. In this paper, we show that (1)\nthese data are almost certainly contaminated with\nvery small percentages of text from other languages\nand that (2) cross-lingual transfer is possible from\nsuch data leakage in the pretraining corpus.\nMore specifically, we quantify how multilingual\nEnglish pretrained models are in two steps. First,\nwe analyze common English pretraining corpora\nwith a large-scale automatic evaluation to estimate\ntheir language composition, as well as a smaller-\nscale manual analysis. Second, we perform ex-\nperiments across fifty languages on masked lan-\nguage modeling and part-of-speech (POS) tagging\nto measure how well the models trained on these\npretraining corpora perform outside of English.\nOur analysis finds that these corpora include very\nsmall percentages that amount to overall significant\namounts of non-English text (Figure 1), particularly\nthose derived from web-crawled data. Furthermore,\nthe models trained on this data perform surprisingly\nwell on other languages; this transfer is strongly\ncorrelated with the amount of target language data\nseen during pretraining. Notably, we find that the\nEnglish T5 outperforms mBERT on POS tagging\n3563\nin multiple languages with no finetuning.\nOverall, these results indicate that the consid-\nered models are actually multilingual and that their\nability to transfer across languages is not zero-shot,\ndespite what has been recently claimed. Given the\neffort required to fully remove all non-English data,\nwe question whether it is practically possible to\ntrain truly monolingual models at scale.\n2 Pretraining Data Composition\nWe first measure how much non-English text exists\nin commonly used English pretraining corpora with\ntwo analyses: an automatic language identification\nto estimate the amount of foreign language data in\nthese corpora, and a manual qualitative analysis of\nthe text classified as non-English.\nWe consider the following pretraining datasets:\nENGLISH WIKIPEDIA (11.8GB); BOOK CORPUS\n(Zhu et al. 2015, 4.2GB); STORIES (Trinh and Le\n2018, 31GB); OPEN WEBTEXT (Gokaslan and Co-\nhen 2019, 38GB), which is an open-source version\nof WEBTEXT (Radford et al., 2019); CC-NEWS\n(Liu et al. 2019, 76 GB); and C4.E N (Raffel et al.\n2020, 305GB), as provided by Dodge et al. (2021).\nWe use the versions of WIKIPEDIA , BOOK COR-\nPUS , and CC-NEWS used to pretrain RoBERTa.\n2.1 Automatic Evaluation of Language\nComposition\nWe use the FastText language identification\nmodel (Joulin et al., 2017) to label every line in\neach corpus and keep lines as non-English if they\nscore above a set confidence threshold (0.6). Due\nto the large size of C4, we subsample the first\n50M examples (or 14%); we classify the entirety\nof all other datasets. Since language detection is\nimperfect, particularly for low-resource languages\n(Caswell et al., 2021), we present the results of this\nanalysis as an estimate of the non-English data in\neach dataset and perform a qualitative analysis of\npotential errors in the following section.\nA summary of the language identification exper-\niments is presented in Figure 1.1 We see that every\ncorpus contains notable quantities of non-English\ndata, with our estimates ranging between 300k to\n406M tokens. An obvious factor that affects the\namount of non-English data in each corpus is the\noverall size of the dataset; however, even when\ncontrolling for size by looking at the percentage of\n1Full results of this evaluation are detailed in Appendix C.\nType Num. of Lines in...\nBook Wiki Stories OpenWeb CCNews C4\n156 129 99 175 193 169\nEx: Moraliska argument utgår ifrån våra moraliska intuitioner\natt rätt och fel inte endast är förankrade i människors vilja.NE\n(OPEN WEBTEXT )\n13 11 15 4 1 22\nEx: The German blazon reads: \"V on Silber über SchwarzBiL geteilt...\" (WIKI )\n2 7 4 2 0 4\nEx: Εκείνη δεν μπορούσε να πληρώσειTrans.\n[She couldn’t pay.] (BOOK CORPUS )\n1 28 5 1 0 1\nEx: 2012 Playhouse Presents ウィルシリーズ1、Ent.\nエピソード1: \"The Minor Character\" (C4)\n26 22 55 12 6 3En Ex: \"Dere’s buzzards circlin’ ova dem trees.\" (BOOK CORPUS )\n2 3 22 6 0 1XX Ex: M D | X O X | O O O = A (WIKI )\nTable 1: Results of the qualitative analysis of the non-\nEnglish lines in various pretraining corpora. Type ab-\nbreviations are defined in Section 2.2.\nnon-English data, we still see that the smaller cor-\npora (WIKIPEDIA , BOOK CORPUS , and STORIES )\nhave relatively less non-English data.\nIndeed, a major factor of language leakage is\nthe method in which the data was collected: the\ndatasets derived from web crawls contain higher\npercentages of non-English text (OPEN WEBTEXT\nandCCN EWS ). This is true even for C4, where\nthe dataset was filtered with a classifier to exclude\nnon-English text (Raffel et al., 2020). Since au-\ntomatic methods for language identification are\nimperfect, the datasets with more manual filtering\n(such as WIKIPEDIA , which has human editors cu-\nrating its content) are less prone to non-English\ndata than those relying on classifiers. Due to these\nchallenges, it is likely impossible to fully remove\nnon-English text from a web-crawled dataset at\nscale.\nWe also see that non-English text makes up\nsmall percentages of the overall data, though this\nstill leads to millions of tokens in large datasets.\nThe largest individual languages after English only\nmake up 0.01%, 0.15%, and 0.05% of the BERT,\nRoBERTa, and T5 training data, respectively. Mul-\ntilingual pretraining work has shown that models\ngeneralize to new languages from varying amounts\nof data (Delvin, 2019; Lample and Conneau, 2019;\nConneau et al., 2020); however, these approaches\nintentionally select data across languages, and most\nupsample low-resource languages during training.\nWithout these considerations, it is an open ques-\ntion how well the models trained on these relatively\nsmall amounts of non-English data generalize.\n3564\n(a) MLM\n (b) POS (probing)\n (c) POS (finetuned)\nFigure 2: Average performance by each model across all languages for the task. Lower is better for BPC.\n2.2 Qualitative Analysis of Non-English Texts\nWe also perform a closer analysis on a random sub-\nset (200 per corpus) of non-English lines predicted\nby the language classifier (Table 1). Each example\nis manually coded into one of six categories. The\nfirst set covers various kinds of foreign language\ndata: NE, where the line contains onlynon-English\nlanguage text; BiL, or bilingual, where the line con-\ntains both English and non-English text; Trans., in\nwhich the English and non-English data that are\ntranslations of each other; and Ent., where the line\nis primarily English but contains non-English en-\ntities. The last two codes pertain to errors made\nby the language classifier: En., where the line only\ncontains English text, and XX, which refers to lines\nthat contain no natural language.\nThe majority of lines across datasets consist\nonly of non-English text. The next most com-\nmon type of non-English data is BiL; this con-\ntains many subtypes of data, such as codeswitch-\ning and foreign language dialogue within English\ntext. These datasets also include parallel data at\nboth the sentence- and word-level.2 We note that\nall observed translations are between English and\nanother language. Finally, some of the examples\nclassified as non-English are actually English texts\nwith non-English phrases.\nOur analysis also shows that the language classi-\nfier performs worse on the non-web crawled data.\nFor example, it misclassified a quarter of the sam-\npled lines from STORIES as non-English when they\nin fact only contain English text; many of these\nlines stem from snippets of dialogue in the dataset.\nWe generally observe that lines coded as En tend\nto be shorter than the correctly labeled lines and\noften contain non-standard English. The language\nclassifier also struggles to handle noisy lines, for\nwhich it has no appropriate language label.\n2e.g., \"大学【だい・がく】– college\", OPEN WEBTEXT\n3 Cross-lingual Transfer of English\nPretrained Models\nWe now ask: how well do models pretrained on\nthese putatively English corpora perform on non-\nEnglish tasks? While the English data is more mul-\ntilingual than previously thought, there are many\ndifferences between monolingual and multilingual\npretraining; non-English data are often tokenized\ninto more subword units 3 and are much less fre-\nquently observed during monolingual training.\nWe evaluate popular English pretrained models\non tasks in more than 50 languages: (masked) lan-\nguage modeling, POS probing, and finetuned POS\ntagging. We compare the performance of monolin-\ngual BERT (Devlin et al., 2019), RoBERTa (Liu\net al., 2019), and T5 (Raffel et al., 2020) against\nmultilingual mBERT (Delvin, 2019) and XLM-R\n(Conneau et al., 2020). We report average perfor-\nmance across five runs with different random seeds\nfor the POS evaluations. The full results and all\nlanguages can be found in Appendix D.\n3.1 Non-English MLM Evaluation\nWe first measure the perplexity of English pre-\ntrained MLMs in other languages. We use Wiki-\n40B, a multilingual language modeling dataset that\ncovers 41 languages (Guo et al., 2020). Following\nthe Wiki-40B paper, we report bits per character\n(BPC) to allow comparison between models with\ndifferent tokenizations of the text.\nWe find that both BERT models perform no-\ntably worse on modeling other languages; however,\nRoBERTa, reduces the gap with the multilingual\nmodels from 2.51 BPC to 0.87 BPC (Figure 2a).\nThis finding is consistent with Tran (2020), who\nalso found RoBERTa transfers well cross-lingually.\n3For example, the Basque UD treebank requires on average\n1.78, 2.59, and 2.66 tokens per word to be encoded by XLMR,\nRoBERTa, and BERT, respectively.\n3565\nTask Model Corr. (ρ) with...\nlang. data ↑ en sim. ↓\nMLM\n(BPC) ↓\nBERTbase -0.258 0.097\nBERTlg -0.258 0.118\nRoBERTabase -0.667∗∗ 0.326∗\nRoBERTalg -0.685∗∗ 0.345∗\nFrozen POS\n(Acc.) ↑\nBERTbase 0.335∗ -0.332∗\nBERTlg 0.314∗ -0.375∗\nRoBERTabase 0.594∗∗ -0.260\nRoBERTalg 0.674∗∗ -0.304∗\nT5base 0.131 -0.271\nFinetuned POS\n(Acc.) ↑\nBERTbase 0.373∗ -0.340∗\nRoBERTabase 0.507∗∗ -0.292∗\nTable 2: Spearman correlations between task perfor-\nmance and (a) in-language data amounts in pretraining\ncorpora (lang. data ) and (b) language similarity with\nEnglish (en sim.). ∗p< 0.05 and ∗∗p< 0.001.\n3.2 POS Performance Across Languages\nNext, we evaluate how well monolingual English\nmodels perform on non-English downstream tasks,\nusing part-of-speech (POS) tagging as a case study.\nProbing We first consider the performance of the\nencoders when probed for POS knowledge (Fig-\nure 2b).4 Unsurprisingly, on average all of the En-\nglish models underperform the multilingual models.\nSimilar to MLM, we find that RoBERTa performs\nbetter than BERT when probed for POS features on\nother languages; surprisingly, it also strongly out-\nperforms T5, despite C4 containing more absolute\nnon-English data than the RoBERTa corpus.\nThis difference is likely due to two factors. First,\nin terms of relative percentages, RoBERTa is ex-\nposed to more non-English text than T5 (0.78%\ncompared to only 0.22%). Secondly, RoBERTa’s\nsubword vocabulary is robust to unexpected inputs\nand does not substitute an UNK token any input\ntokens; in contrast, T5 and BERT have high rates\nof UNK tokens for some non-Latin languages (Ap-\npendix B).5 However, for many high-resource lan-\nguages the English models perform competitively,\nwith T5 outperforming mBERT on German and\nPortuguese, among others.\nFine-tuning To test if the effects of foreign\nlanguage data carry through after finetuning, we\nalso finetune a subset of the models (BERT base,\nRoBERTabase, mBERT, XLMR base) for non-\nEnglish POS tagging (Figure 2c). After finetun-\n4For T5, this means that we evaluate the output of the\nencoder and discard the decoder.\n5UNK tokens refer to placeholder tokens used when the\nmodel receives an input not covered by its vocabulary.\ning, the gap between the mono- and multilingual\nmodels is much smaller: RoBERTa only averages\n2.65 points worse than XLM-R, compared to 12.5\npoints when probing.\n3.3 Potential Reasons for Cross-lingual\nGeneralization\nWe then investigate the correlation between poten-\ntial transfer causes and model performance (Table\n2). Specifically, we consider the quantity of tar-\nget language data found in the model’s pretraining\ncorpus and the language similarity to English as\npotential causes of cross-lingual transfer.\nWe find that across tasks, RoBERTa task perfor-\nmance is most strongly correlated with the amount\nof target language data seen during pretraining.\nBERT and T5 task performance are less correlated\nwith observed pretrained data, likely due to tok-\nenization artifacts (Appendix B). Indeed, when we\ncontrol for languages not written with Latin script\non T5, the correlation between performance and\nthe amount of target pretraining data increases to\nρ= 0.313.\nWe also consider the effect of language similarity\non task performance, which is often hypothesized\nto facilitate cross-lingual transfer. We use the syn-\ntactic distance of languages calculated by Malaviya\net al. (2017); more similar languages score lower.\nHowever, we generally find that this is less corre-\nlated with performance than the quantity of target\ntext, particularly for RoBERTa.\n4 Discussion\nIn this paper, we demonstrate that English pre-\ntrained models are exposed to a considerable\namount of non-English data during pretraining, par-\nticularly in the case of more recent models that are\ntrained on larger corpora derived from web crawls.\nWe also find that this non-English text acts as a sig-\nnificant source of signal for cross-lingual transfer.\nOther recent work has focused on document-\ning the composition of pretraining corpora (Dodge\net al., 2021; Gururangan et al., 2022). Caswell\net al. (2021) manually audit a variety of multilin-\ngual datasets, finding data quality issues that are\nworse for low-resource languages and, similarly to\nour work, that texts for many languages are misclas-\nsified. In contrast, our focus is on the presence of\nforeign language data in primarily English corpora.\nPrior work has also shown the ability of mono-\nlingual models to transfer to other languages across\n3566\na wide range of tasks (Gogoulou et al., 2021; Li\net al., 2021; Tran, 2020; Artetxe et al., 2020; Chi\net al., 2020), but these works do not consider the ef-\nfect of foreign language data leakage as a source of\nsignal. Notably, de Souza et al. (2021) mention the\npresence of foreign language data in their corpora\nbut assume the small amounts observed will not\naffect model performance. However, our findings\ndemonstrate that the amount of foreign language\ndata directly correlates with cross-lingual transfer.\nAn obvious follow-up to our findings would be to\nretrain the models with text that is verified to only\ncontain English data; this would confirm the effect\nthe leaked non-English data has on the models.\nWe reiterate that the standard method for filtering\nthese datasets, automatic language classifiers, is\nimperfect. This, and the infeasibility of manual\nfiltering due to the scale of the data, means that\ncontrolling for the language the model is pretrained\non is nearly impossible.\nHowever, the presence of foreign language data\nin pretraining corpora is not inherently problematic.\nModels trained on these datasets perform exceed-\ningly well on their target languages and general-\nize to other languages much better than expected.\nRather, it is important to remember that these mod-\nels are not performing zero-shot transfer when used\nin other languages, given the scale and data with\nwhich they were pretrained.\n5 Limitations\nOur work has a number of limitations. First, we\nmeasure the quantities of non-English data using\na language classifier. The amounts of foreign lan-\nguage data we report are estimates for each dataset,\nas the classifier likely misclassified some examples.\nWe manually audit the types of mistakes made by\nthe language classifier in Section 2. Additionally,\nwe evaluate downstream performance via POS tag-\nging, and it is possible that the models would ex-\nhibit different behavior on other NLP tasks.\nWe also only consider the effect of foreign lan-\nguage contamination for English pretrained mod-\nels. It is unclear to what extent this phenomenon\naffects monolingual models for other languages;\nhowever, since many of the resources evaluated\nin this work are also used to pretrain non-English\nmonolingual models (e.g., Wikipedia), similar ef-\nfects would likely be observed.\nAcknowledgements\nWe would like to thank Hila Gonen, Julian Michael,\nand Ari Holtzman for their helpful conversations\nabout the work. We also thank the anonymous\nreviewers for their thoughtful comments.\nReferences\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 4623–4637.\nIsaac Caswell, Julia Kreutzer, Lisa Wang, Ahsan Wahab,\nDaan van Esch, Nasanbayar Ulzii-Orshikh, Allahsera\nTapo, Nishant Subramani, Artem Sokolov, Claytone\nSikasote, et al. 2021. Quality at a glance: An audit\nof web-crawled multilingual datasets.\nZewen Chi, Li Dong, Furu Wei, Xianling Mao, and\nHe-Yan Huang. 2020. Can monolingual pretrained\nmodels help cross-lingual classification? In Proceed-\nings of the 1st Conference of the Asia-Pacific Chapter\nof the Association for Computational Linguistics and\nthe 10th International Joint Conference on Natural\nLanguage Processing, pages 12–17.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Édouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451.\nLeandro Rodrigues de Souza, Rodrigo Nogueira, and\nRoberto Lotufo. 2021. On the ability of monolingual\nmodels to learn language-agnostic representations.\narXiv preprint arXiv:2109.01942.\nJacob Delvin. 2019. Multilingual BERT\nReadme. https://github.com/\ngoogle-research/bert/blob/master/\nmultilingual.md.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJesse Dodge, Maarten Sap, Ana Marasovi ´c, William\nAgnew, Gabriel Ilharco, Dirk Groeneveld, Margaret\nMitchell, and Matt Gardner. 2021. Documenting\nlarge webtext corpora: A case study on the colossal\nclean crawled corpus. In Proceedings of the 2021\n3567\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP).\nEvangelia Gogoulou, Ariel Ekgren, Tim Isbister,\nand Magnus Sahlgren. 2021. Cross-lingual trans-\nfer of monolingual models. arXiv preprint\narXiv:2109.07348.\nAaron Gokaslan and Vanya Cohen. 2019. Openwebtext\ncorpus. http://Skylion007.github.io/\nOpenWebTextCorpus.\nMandy Guo, Zihang Dai, Denny Vrandeˇci´c, and Rami\nAl-Rfou. 2020. Wiki-40B: Multilingual language\nmodel dataset. In Proceedings of the 12th Language\nResources and Evaluation Conference, pages 2440–\n2452, Marseille, France. European Language Re-\nsources Association.\nSuchin Gururangan, Dallas Card, Sarah K Drier,\nEmily K Gade, Leroy Z Wang, Zeyu Wang, Luke\nZettlemoyer, and Noah A Smith. 2022. Whose lan-\nguage counts as high quality? measuring language\nideologies in text data selection. arXiv preprint\narXiv:2201.10474.\nArmand Joulin, Édouard Grave, Piotr Bojanowski, and\nTomáš Mikolov. 2017. Bag of tricks for efficient text\nclassification. In Proceedings of the 15th Confer-\nence of the European Chapter of the Association for\nComputational Linguistics: Volume 2, Short Papers,\npages 427–431.\nDiederik P. Kingma and Jimmy Lei Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proceedings\nof 3rd International Conference of Learning Repre-\nsentations.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. arXiv preprint\narXiv:1901.07291.\nZuchao Li, Kevin Parnow, Hai Zhao, Zhuosheng Zhang,\nRui Wang, Masao Utiyama, and Eiichiro Sumita.\n2021. Cross-lingual transferring of pre-trained\ncontextualized language models. arXiv preprint\narXiv:2107.12627.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nChaitanya Malaviya, Graham Neubig, and Patrick Lit-\ntell. 2017. Learning language representations for\ntypology prediction. In Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\nCopenhagen, Denmark.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip\nGinter, Jan Hajic, Christopher D Manning, Sampo\nPyysalo, Sebastian Schuster, Francis Tyers, and\nDaniel Zeman. 2020. Universal dependencies v2:\nAn evergrowing multilingual treebank collection. In\nProceedings of the 12th Language Resources and\nEvaluation Conference, pages 4034–4043.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. Journal of Machine Learning Research, 21:1–\n67.\nKe Tran. 2020. From english to foreign languages:\nTransferring pre-trained language models. arXiv\npreprint arXiv:2002.07306.\nTrieu H Trinh and Quoc V Le. 2018. A simple\nmethod for commonsense reasoning. arXiv preprint\narXiv:1806.02847.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pierric\nCistac, Tim Rault, R’emi Louf, Morgan Funtowicz,\nand Jamie Brew. 2019. Huggingface’s transformers:\nState-of-the-art natural language processing. ArXiv,\nabs/1910.03771.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE in-\nternational conference on computer vision , pages\n19–27.\nA Details of Transfer Experiments\nFor the language modeling experiments, we per-\nform whole word masking on 15% of the words in\nthe Wiki40B test set to calculate BPC. This experi-\nment was zero-shot and required no further training\nof the models.\nFor the POS probing experiments, we train a\nlinear classifier to predict POS from the final layer\nof each considered encoder; each probe therefore\nconsists of a limited number of parameters m∗l\nwhere mis the output dimension of the encoder be-\ning probed (768 for base models and 1024 for large\nmodels) and lis the size of the label set (17 for POS\ntagging). For words that are tokenized into multiple\nsubword units, we take the average representation\nof all tokens as the input to the classifier. When\nfinetuning the model, we take the same setup as\n3568\nprobing but unfreeze the encoder weights to allow\nthem to update during training. The POS models\nare trained and evaluated on Universal Dependen-\ncies (UD) treebanks for each language (Nivre et al.,\n2020).\nWe use a batch size of 256 for the frozen ex-\nperiments and batch sizes of 16 for the finetuned\nmodels; we used a learning rate of 0.001 for the\nprobing task and 5e-6 for finetuning. Due to the\nlarge number of experiments, we did not tune these\nparameters. For both POS tagging experiments, we\nuse an Adam optimizer (Kingma and Ba, 2015),\nand train each probe for 50 passes over the data\n(with early stopping on the validation set and a\npatience of 5). The pretrained models for all exper-\niments are downloaded from Huggingface (Wolf\net al., 2019).\nEach of our models was trained on a single\nNvidia V100 GPU: 16GB for the frozen models\nand 32GB for the finetuned ones. The frozen\nprobes each took between <1 and 8 minutes to\ntrain, and the finetuned probes were trained for be-\ntween 5 minutes and 7.5 hours (depending on the\ndataset size, which varies by language, and early\nstopping epoch).\nB The Effect of Tokenization\nA factor that varies across the considered models\nis how they tokenize the input text for different\nlanguages. Table 4 gives the number of subword\ntokens per (white-space separated) word in the val-\nidation split of Wiki40b (Guo et al., 2020), as well\nas the percentage of tokens that are unked out by\nthe tokenizer. We see that in general, all of the mod-\nels (including explicitly multilingual ones) require\nmore subword tokens per word for languages other\nthan English.6 We can also see that T5 is more effi-\ncient at encoding French, German, and Romanian\nthan the other monolingual models (without a high\nUNK rate), likely because the T5 tokenizer was\nexplicitly trained on English data mixed with those\nlanguages (Raffel et al., 2020).\nWe also examine how many tokens are unked\nout by each tokenizer across languages. We see\nthat BERT and T5 in particular have a high UNK\nrate (> 10%) for many languages not written in\nLatin script. This is in part due to the different tok-\nenization schemes used by the models: RoBERTa\n6We note that the number of subword tokens per “word”\nin Japanese is much larger than in other languages, as words\nin Japanese are not whitespace-separated.\nuses a byte-level BPE encoding (Radford et al.,\n2019), which produces no UNK tokens for Unicode\ntext, whereas the tokenization methods used by\nBERT and T5 (SentencePiece, Kudo and Richard-\nson (2018)) will unk out tokens not seen while\ntraining the tokenizer. Additionally, there are other\npotential decisions made during tokenization that\ncould affect these UNK rates, including filtering on\nnon-Latin tokens or learning the subword tokenizer\non a subset of the training data.\nHigh UNK rates in the tokenized text for a lan-\nguage affect performance on downstream tasks.\nWith regards to evaluating BPC, high frequencies\nof UNK tokens in the data likely make the lan-\nguage modeling task artificially easy, leading to\nlower BPC scores. Because of this, we note the\ncases where a model UNKs out more than 10%\nof the considered data in the BPC results given in\nTable 5 with an asterisk (*). High UNK rates likely\nalso lead to degraded performance on downstream\ntasks (including the considered POS tagging task\nin this work).\nC Full Results of the Automatic\nLanguage Identity Analysis\nWe present a more complete set of results for the\nautomatic language composition analysis (Section\n2) in Table 3. We include every language that has\n10,000 or more tokens in at least one of the consid-\nered corpora; we additionally report numbers for\nBasque and Frisian, as both languages are included\nin the experiments in Section 3.\nD Full Results of Transfer Experiments\nFull results for whole word MLM are given in Table\n5; results for POS probing can be found in Table 6\nand results for finetuned POS tagging are detailed\nin Table 7.\n3569\nISO Language Number of Tokens\nWiki Book Stories OpenWebText CCNews C4 BERT RoBERTa\nen English 2.0B 802.4M 6.2B 6.4B 13.0B 17.8B 2.8B 28.3B\nsq Albanian 3.3K 0 195 8.8K 42.5M 14.4K 3.3K 42.5M\nes Spanish 112.8K 120.4K 150.6K 3.4M 36.6M 5.9M 233.2K 40.3M\nde German 176.2K 5.4K 104.8K 3.1M 34.4M 9.0M 181.5K 37.8M\nro Romanian 19.6K 174 6.4K 1.4M 28.7M 164.0K 19.8K 30.2M\npt Portugese 43.2K 760 44.2K 1.5M 10.0M 1.9M 44.0K 11.5M\nit Italian 102.9K 3.9K 46.1K 1.6M 9.2M 2.6M 106.7K 10.9M\nfr French 201.1K 88.1K 126.6K 2.5M 7.2M 6.0M 289.2K 10.1M\npl Polish 56.2K 51 5.0K 239.9K 5.3M 686.9K 56.2K 5.6M\nnl Dutch 28.0K 1.0K 37.3K 254.7K 4.4M 1.7M 29.0K 4.8M\nvi Vietnamese 25.5K 98 2.8K 10.5K 3.5M 277.6K 25.6K 3.6M\ntl Tagalog 3.2K 3.7K 28.7K 124.3K 3.1M 312.1K 6.9K 3.3M\ncs Czech 8.8K 12 2.1K 152.7K 2.0M 295.0K 8.8K 2.1M\nfi Finnish 6.9K 119 4.7K 243.2K 1.7M 214.5K 7.0K 1.9M\nno Norwegian 9.5K 170 6.4K 204.3K 1.6M 300.5K 9.7K 1.8M\nhu Hungarian 8.9K 51 5.6K 32.5K 1.6M 194.2K 9.0K 1.7M\nhi Hindi 6.7K 0 520 32.2K 1.5M 328.0K 6.7K 1.6M\nhr Croatian 4.0K 0 482 313.2K 1.2M 30.8K 4.0K 1.5M\nid Indonesian 1.5K 100 12.9K 83.5K 1.3M 997.7K 1.6K 1.4M\nru Russian 17.4K 606 3.9K 956.3K 64.8K 2.3M 18.0K 1.0M\nsv Swedish 11.1K 567 9.3K 784.9K 236.3K 743.5K 11.6K 1.0M\nsr Serbian 753 0 709 39.0K 976.2K 36.8K 753 1.0M\net Estonian 2.8K 0 288 8.0K 817.1K 32.0K 2.8K 828.2K\ntr Turkish 6.3K 541 9.4K 131.4K 535.0K 401.9K 6.9K 682.6K\naf Afrikaans 852 0 2.7K 6.7K 584.1K 145.3K 852 594.3K\nku Kurdish 185 0 0 6.7K 468.0K 3.5K 185 474.9K\nda Danish 3.1K 20 5.3K 249.8K 157.8K 271.1K 3.1K 415.9K\ngl Galican 101 0 309 637 317.5K 9.6K 101 318.6K\nja Japanese 5.8K 3.4K 23.8K 188.7K 76.3K 3.0M 9.2K 298.1K\nca Catalan 5.2K 99 418 28.2K 258.8K 108.3K 5.3K 292.8K\nar Arabic 5.3K 0 665 154.2K 89.6K 601.7K 5.3K 249.7K\nko Korean 3.2K 20 45 208.1K 8.0K 4.1M 3.3K 219.4K\nel Greek 15.2K 777 1.8K 123.8K 28.4K 288.7K 16.0K 169.9K\nsl Slovenian 262 0 250 102.1K 14.5K 46.8K 262 117.1K\nis Icelandic 1.5K 65.8K 758 10.4K 11.1K 114.7K 67.2K 89.5K\nga Irish 1.2K 0 839 8.7K 77.9K 468.4K 1.2K 88.6K\nuk Ukranian 3.5K 10 232 63.4K 3.5K 232.1K 3.5K 70.7K\nhe Hebrew 5.2K 0 4.6K 46.5K 9.6K 138.0K 5.2K 66.0K\nlt Lithuanian 3.4K 12 1.1K 2.8K 54.9K 56.8K 3.4K 62.2K\nsk Slovak 1.9K 0 76 16.2K 43.9K 64.7K 1.9K 62.0K\nms Malay 896 29 1.4K 1.8K 45.9K 42.8K 925 50.0K\nsw Swahili 44 16 533 143 47.7K 5.9K 60 48.5K\neo Esperanto 461 114 2.6K 34.9K 7.2K 37.2K 575 45.2K\nzh Chinese 4.1K 12 5.5K 30.8K 4.5K 410.2K 4.1K 44.8K\nlv Latvian 1.4K 0 367 4.0K 38.5K 47.0K 1.4K 44.3K\nbn Bengali 2.5K 24.8K 51 6.2K 10.1K 48.6K 27.3K 43.6K\nfa Persian 3.0K 0 261 28.2K 5.8K 668.9K 3.0K 37.2K\nnn Norysk 283 0 0 4.5K 32.5K 4.6K 283 37.2K\nla Latin 6.0K 641 3.8K 19.4K 4.7K 34.7K 6.6K 34.5K\naz Azerbaijani 2.0K 0 55 884 27.1K 12.9K 2.0K 30.0K\nth Thai 7.6K 0 592 15.1K 5.5K 131.8K 7.6K 28.7K\nbg Bulgarian 6.7K 20 284 18.7K 2.8K 96.7K 6.7K 28.5K\ncy Welsh 1.2K 84 440 18.5K 7.4K 59.5K 1.3K 27.6K\nilo Iloko 19 0 16 628 18.8K 1.1K 19 19.5K\nur Ukranian 5.0K 0 24 6.4K 7.0K 27.9K 5.0K 18.4K\nta Tamil 5.4K 0 234 5.9K 5.8K 42.2K 5.4K 17.4K\nmt Maltese 177 0 0 371 13.9K 20.3K 177 14.5K\nhy Armenian 2.6K 0 0 7.5K 2.9K 21.5K 2.6K 13.0K\ngd Gaelic 198 0 52 874 8.8K 117.6K 198 10.0K\neu Basque 99 5 1.8K 2.8K 2.4K 19.3K 104 7.0K\nfy Frisian 80 0 1.3K 1.5K 601 9.8K 80 3.4K\nTotal (Non-En) 983k 322k 682k 18.6M 201M 406M † 1.3M 222M\nTable 3: Full results for the automatic language composition analysis of pretraining corpora presented in Section 2.\nThe last two columns include the total data that BERT and RoBERTa were trained on, respectively; C4 contains the\ndata T5 was trained on, and contains the estimates for the first 50M examples in the full C4 dataset; †represents the\nprojected estimate for the full dataset.\n3570\nISO Monolingual Multilingual\nBERT RoBERTa T5 mBERT XLMR\nar 2.91 (0.60%) 3.11 (0.0%) 1.91 ( 41.26%) 1.95 (0.05%) 1.73 (9.8e-4%)\nbg 3.03 (0.06%) 3.25 (0.0%) 2.88 (17.83%) 1.93 (0.67%) 1.72 (1.2e-3%)\nca 1.95 (0.01%) 1.83 (0.0%) 2.08 (1.23%) 1.55 (0.12%) 1.56 (4.6e-4%)\ncs 2.64 (0.03%) 2.64 (0.0%) 2.85 (10.30%) 2.00 (0.22%) 1.86 (5.6e-4%)\nda 2.19 (0.01%) 2.07 (0.0%) 2.42 (3.56%) 1.74 (0.14%) 1.63 (4.5e-4%)\nde 2.21 (0.02%) 2.14 (0.0%) 1.85 (0.26%) 1.65 (0.37%) 1.67 (1.5e-3%)\nel 2.74 (0.36%) 2.96 (0.0%) 1.82 ( 40.30%) 2.05 (0.05%) 1.73 (1.4e-3%)\nen 1.38 (0.03%) 1.32 (0.0%) 1.44 (0.15%) 1.37 (0.22%) 1.42 (1.3e-3%)\nes 1.76 (0.01%) 1.67 (0.0%) 1.88 (1.48%) 1.37 (0.11%) 1.37 (8.2e-4%)\net 3.02 (0.03%) 2.92 (0.0%) 3.35 (1.55%) 2.47 (0.33%) 2.21 (1.5e-3%)\nfa 2.80 (1.12%) 3.34 (0.0%) 2.00 ( 42.14%) 1.70 (0.05%) 1.55 (6.1e-3%)\nfi 3.18 (8.3e-3%) 3.06 (0.0%) 3.48 (0.13%) 2.45 (0.35%) 2.24 (9.9e-4%)\nfr 1.90 (0.01%) 1.81 (0.0%) 1.78 (0.26%) 1.53 (0.43%) 1.57 (8.5e-4%)\nhe 2.82 (0.72%) 3.08 (0.0%) 1.97 (40.30%) 2.05 (0.06%) 1.89 (4.2e-4%)\nhi 1.98 (12.06%) 2.84 (0.0%) 1.64 ( 42.82%) 1.64 (0.05%) 1.39 (1.3e-3%)\nhr 2.38 (7.1e-3%) 2.27 (0.0%) 2.57 (3.71%) 1.85 (0.08%) 1.73 (1.1e-3%)\nhu 2.78 (0.02%) 2.72 (0.0%) 3.00 (2.60%) 2.12 (0.28%) 1.93 (1.1e-3%)\nid 2.34 (0.05%) 2.22 (0.0%) 2.54 (0.13%) 1.70 (0.12%) 1.59 (4.5e-3%)\nit 1.92 (0.01%) 1.83 (0.0%) 2.05 (0.42%) 1.51 (0.10%) 1.52 (1.0e-3%)\nja 34.41 (39.97%) 47.67 (0.0%) 9.50 (22.19%) 35.22 (0.05%) 31.30 (0.03%)\nko 1.60 (59.65%) 4.78 (0.0%) 2.32 ( 38.63%) 2.65 (0.25%) 2.53 (0.03%)\nlt 3.06 (0.80%) 3.12 (0.0%) 3.41 (8.78%) 2.48 (0.97%) 2.23 (7.7e-3%)\nlv 2.89 (0.48%) 2.84 (0.0%) 3.13 ( 12.52%) 2.36 (0.30%) 2.06 (2.8e-3%)\nms 2.34 (0.03%) 2.21 (0.0%) 2.53 (0.10%) 1.71 (0.10%) 1.58 (1.9e-3%)\nnl 2.18 (8.0e-3%) 2.04 (0.0%) 2.31 (0.21%) 1.64 (0.08%) 1.62 (4.6e-4%)\nno 2.24 (0.03%) 2.10 (0.0%) 2.49 (3.01%) 1.74 (0.13%) 1.66 (2.0e-3%)\npl 2.60 (9.3e-3%) 2.60 (0.0%) 2.83 (6.50%) 1.96 (0.44%) 1.88 (7.2e-4%)\npt 1.86 (0.02%) 1.76 (0.0%) 2.01 (2.05%) 1.45 (0.11%) 1.43 (1.0-3%)\nro 2.03 (0.01%) 2.02 (0.0%) 1.73 (0.18%) 1.63 (0.25%) 1.54 (7.9e-4%)\nru 3.05 (0.02%) 3.25 (0.0%) 2.90 (21.1%) 1.92 (0.53%) 1.82 (2.1e-3%)\nsk 2.86 (0.05%) 2.81 (0.0%) 3.14 (7.08%) 2.20 (0.19%) 2.00 (1.4e-3%)\nsl 2.37 (9.7e-3%) 2.24 (0.0%) 2.53 (3.45%) 1.91 (0.06%) 1.73 (1.1e-3%)\nsr 3.01 (0.71%) 3.33 (0.0%) 2.95 ( 17.14%) 1.95 (0.19%) 1.77 (4.8e-4%)\nsv 2.57 (7.9e-3%) 2.40 (0.0%) 2.77 (2.08%) 1.90 (0.15%) 1.80 (8.5e-4%)\nth 2.13 (36.91%) 11.79 (0.0%) 2.73 ( 28.58%) 8.34 (0.12%) 5.42 (1.6e-3%)\ntl 2.14 (0.10%) 2.02 (0.0%) 2.44 (0.18%) 1.81 (0.12%) 1.70 (2.9e-3%)\ntr 2.94 (0.01%) 2.87 (0.0%) 3.19 (7.36%) 2.13 (0.31%) 1.91 (2.0e-3%)\nuk 3.36 (0.52%) 3.73 (0.0%) 3.23 (24.12%) 2.11 (0.54%) 1.94 (1.4e-3%)\nvi 1.76 (1.44%) 1.95 (0.0%) 1.89 ( 15.12%) 1.19 (0.08%) 1.16 (3.2e-3%)\nTable 4: The average number of subword tokens per white-spaced word (and the percentage of UNKed out tokens)\nin the Wiki40b validation set for each language. Cases where more than 10% of tokens are unked out are in bold.\n3571\nISO Monolingual Multilingual\nBERTba BERTlg RoBERTaba RoBERTalg mBERT XLMR ba XLMRlg\nar 6.214 9.331 3.319 3.899 1.849 1.871 1.691\nbg 6.334 7.883 3.544 3.587 1.553 1.494 1.358\nca 3.382 3.565 1.834 1.640 1.108 1.477 1.329\ncs 4.316 4.738 2.634 2.493 1.703 1.715 1.533\nda 3.560 3.832 2.104 1.931 1.420 1.427 1.272\nde 3.430 3.644 1.815 1.634 1.102 1.361 1.218\nel 6.934 8.915 3.852 3.885 1.793 1.588 1.440\nen 1.285 1.377 0.595 0.516 0.938 1.249 1.131\nes 3.281 3.551 1.526 1.345 1.036 1.284 1.165\net 3.846 4.108 2.448 2.318 1.878 1.858 1.671\nfa 5.813 8.501 3.614 4.113 1.723 1.567 1.418\nfi 3.732 4.064 2.357 2.240 1.633 1.618 1.451\nfr 3.213 3.439 1.586 1.414 1.038 1.434 1.305\nhe 6.490 9.074 3.530 3.831 1.817 1.976 1.739\nhi 4.240* 5.503* 1.487 1.407 1.876 1.641 1.516\nhr 3.972 4.298 2.267 2.109 1.563 1.644 1.484\nhu 4.203 4.585 2.741 2.632 1.778 1.713 1.548\nid 3.436 3.665 1.976 1.838 1.221 1.243 1.129\nit 3.263 3.536 1.661 1.475 1.098 1.402 1.256\nja 1.840* 2.065* 5.481 6.775 2.082 6.827 8.016\nko 0.781* 0.846* 4.204 4.639 3.144 3.504 3.241\nlt 3.953 4.271 2.746 2.633 1.840 1.789 1.604\nlv 4.231 4.512 2.833 2.730 1.890 1.750 1.548\nms 3.461 3.698 2.010 1.886 1.280 1.365 1.257\nnl 3.445 3.693 1.855 1.680 1.222 1.397 1.257\nno 3.580 3.873 2.052 1.872 1.398 1.469 1.312\npl 4.020 4.505 2.506 2.365 1.495 1.604 1.437\npt 3.442 3.718 1.658 1.465 1.128 1.316 1.190\nro 3.641 3.929 1.950 1.772 1.402 1.435 1.286\nru 6.747 8.122 3.624 3.673 1.385 1.491 1.344\nsk 4.263 4.628 2.714 2.594 1.804 1.753 1.594\nsl 3.972 4.294 2.415 2.273 1.642 1.563 1.391\nsr 6.081 7.216 3.610 3.661 1.772 1.783 1.681\nsv 3.774 4.081 2.196 2.019 1.460 1.523 1.372\nth 1.551* 1.689* 3.312 3.535 3.861 2.119 2.237\ntl 3.250 3.458 1.763 1.623 1.616 1.713 1.572\ntr 4.102 4.427 2.715 2.585 1.635 1.603 1.460\nuk 6.542 7.912 3.763 3.823 1.566 1.635 1.488\nvi 5.134 5.794 2.590 2.574 1.046 1.191 1.055\nTable 5: Full results for the zero-shot BPC experiments in Section 3. Results noted with * correspond to cases of\nhigh UNK rates in the tokenization of the data (Section B).\n3572\nISO Baselines Monolingual Multilingual\nMaj. Label Word Maj. BERTba BERTlg Roba Rolg T5-base mBERT XLMR ba XLMRlg\naf 21.650 83.335 81.695 84.855 88.858 92.324 90.464 93.590 97.490 96.381\nar 33.297 90.148 79.595 79.994 78.939 79.242 43.182 93.724 95.659 95.533\nbg 21.834 86.091 85.617 84.238 78.514 81.147 85.304 94.977 97.240 97.103\nca 17.868 90.984 93.208 93.432 94.333 94.545 95.863 97.610 98.222 98.202\ncs 24.708 91.284 82.312 80.591 89.272 93.488 86.560 96.554 97.548 97.744\ncy 31.099 73.587 69.625 72.641 69.171 70.309 76.220 81.713 76.690 77.215\nda 18.606 77.841 81.137 81.485 85.308 91.057 86.832 91.901 96.757 96.087\nde 17.784 81.992 86.663 88.306 91.266 93.074 92.878 92.022 94.851 94.020\nel 21.148 81.128 79.996 79.110 66.604 76.795 37.050 92.509 95.435 95.371\nen 16.999 82.920 93.803 92.903 94.785 94.418 96.286 93.666 95.366 95.342\nes 17.734 90.734 89.639 92.860 97.652 93.171 97.222 97.699 98.418 98.497\net 26.462 78.486 74.987 78.524 74.460 81.150 79.287 91.891 90.717 91.143\neu 24.422 77.591 73.152 75.663 72.713 72.254 80.294 84.712 88.744 88.023\nfa 33.521 91.916 78.545 78.202 67.668 66.767 46.485 93.025 96.310 96.597\nfi 27.965 74.378 71.083 72.301 77.318 82.587 77.630 92.621 95.823 95.872\nfr 18.749 89.584 90.960 90.197 93.690 95.278 96.612 95.963 95.837 95.813\nfy 14.815 85.190 79.749 82.128 78.766 78.216 86.351 90.898 87.908 89.405\nga 29.122 81.512 72.470 77.009 76.983 79.169 82.430 87.097 91.493 92.583\ngd 21.166 80.114 78.264 79.641 74.989 76.281 79.590 78.387 84.102 85.609\ngl 22.969 86.294 87.727 89.058 92.638 93.176 93.647 92.559 95.145 95.548\nhe 23.601 85.491 75.040 75.393 69.930 70.160 45.758 93.206 96.403 94.864\nhi 22.128 89.365 68.650 68.861 77.250 80.597 38.185 94.054 95.524 94.250\nhr 24.182 83.533 80.408 82.370 92.955 94.742 87.784 96.164 98.011 98.313\nhu 22.429 60.356 72.619 73.444 76.403 83.633 79.868 88.273 93.404 91.868\nhy 24.995 68.931 50.956 52.033 58.560 58.792 44.142 89.001 90.403 93.937\nid 21.642 81.278 4.151 4.151 79.539 81.664 4.151 4.151 4.151 4.151\nis 17.286 90.407 80.969 84.217 79.792 82.154 84.121 91.414 97.459 97.778\nit 19.920 89.758 89.497 91.317 93.991 95.521 94.715 96.662 97.454 96.854\nja 30.137 85.592 76.268 75.659 83.491 85.013 39.409 92.362 90.844 91.080\nko 30.011 67.715 48.090 47.683 67.852 70.745 47.288 76.359 80.372 80.704\nla 21.355 94.888 91.416 94.079 92.133 92.489 95.928 95.008 98.250 97.548\nlt 31.345 61.009 67.026 70.382 67.892 66.602 77.164 90.542 91.641 94.710\nlv 27.108 79.198 71.889 77.190 72.061 74.857 81.630 87.627 93.565 92.775\nmt 19.489 76.131 75.582 78.313 75.199 75.156 80.854 76.877 70.113 74.191\nnl 16.799 81.878 79.448 84.540 90.126 92.816 89.207 94.356 95.920 95.990\npl 24.900 83.868 82.357 81.721 91.563 92.491 90.494 94.184 98.231 97.840\npt 18.117 83.517 85.594 86.801 91.591 92.114 95.120 94.066 96.981 94.810\nro 24.849 85.537 82.630 84.414 93.830 95.332 93.407 95.284 97.443 97.229\nru 23.843 88.593 84.258 85.526 82.713 86.811 88.812 95.299 96.943 94.714\nsk 19.264 61.821 80.441 83.025 86.850 89.339 86.872 92.414 96.290 95.810\nsl 21.289 77.815 82.459 80.959 88.357 88.176 87.189 96.388 97.953 98.144\nsr 24.378 82.523 84.930 85.434 94.762 94.769 89.884 92.727 98.638 98.333\nsv 17.579 78.993 71.818 79.061 78.523 88.662 83.226 92.826 96.246 95.350\nta 29.389 53.042 43.992 40.563 38.361 43.228 43.147 74.912 76.521 75.606\ntr 36.494 82.343 78.115 73.015 67.656 67.340 80.830 88.633 91.392 89.939\nuk 23.213 71.964 78.214 78.950 65.442 69.780 80.780 91.836 96.213 96.823\nur 23.564 85.736 68.112 68.819 69.202 67.458 34.116 88.954 91.170 92.341\nvi 32.019 75.901 54.764 54.267 54.054 56.402 60.768 75.844 85.855 81.554\nzh 27.478 78.696 49.462 51.400 64.537 67.238 44.479 87.363 88.565 85.921\nTable 6: Full results for the frozen POS tagging experiments in Section 3.\n3573\nISO Monolingual Multilingual\nBERTba Roba mBERT XLMR ba\naf 92.108 94.528 96.802 97.158\nar 92.896 93.417 96.151 96.673\nbg 97.185 96.797 98.714 99.145\nca 98.058 98.264 98.813 98.845\ncs 98.191 98.306 98.803 98.952\ncy 82.152 72.484 92.109 84.927\nda 93.134 93.872 97.037 97.556\nde 93.285 93.554 95.178 95.189\nel 92.725 90.452 96.735 96.897\nen 96.496 97.186 96.431 97.082\nes 97.841 98.459 98.828 98.781\net 95.142 95.244 96.547 97.402\neu 91.313 90.117 94.529 94.956\nfa 94.477 94.113 97.193 97.609\nfi 93.534 93.436 96.275 97.823\nfr 96.970 97.112 97.845 98.102\nfy 92.383 92.770 95.557 95.721\nga 91.263 91.164 93.293 94.334\ngd 91.774 90.220 92.814 93.797\ngl 94.261 95.565 95.130 96.892\nhe 91.373 90.847 96.242 97.035\nhi 83.566 94.437 96.554 97.384\nhr 95.955 96.687 98.061 98.293\nhu 83.886 85.540 94.763 94.035\nhy 53.953 86.965 92.985 93.829\nid 4.151 91.635 4.151 4.151\nis 96.583 96.489 97.899 98.404\nit 96.521 97.297 98.172 98.334\nja 86.477 93.707 96.600 97.112\nko 47.783 91.514 94.941 95.464\nla 98.386 98.633 99.399 99.199\nlt 82.707 84.507 93.026 94.636\nlv 93.252 93.710 95.744 96.908\nmt 87.284 85.603 89.248 86.349\nnl 93.974 94.861 96.669 96.969\npl 96.604 97.064 98.569 98.980\npt 95.420 96.572 97.526 97.611\nro 95.795 96.044 97.568 97.878\nru 97.310 97.103 98.306 98.568\nsk 93.608 93.994 97.282 97.373\nsl 95.233 95.776 98.157 98.798\nsr 96.140 97.154 98.531 98.802\nsv 90.737 92.805 96.242 97.080\nta 42.363 49.452 77.185 64.354\ntr 92.390 92.320 94.667 94.946\nuk 93.320 93.980 96.020 96.975\nur 84.556 84.432 92.622 93.251\nvi 46.954 50.874 89.653 91.261\nzh 55.811 84.877 95.022 95.972\nTable 7: Full results for the finetuned POS tagging experiments in Section 3.\n3574"
}