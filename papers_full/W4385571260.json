{
  "title": "Teaching Small Language Models to Reason",
  "url": "https://openalex.org/W4385571260",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3183355254",
      "name": "Lucie Charlotte Magister",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2253961813",
      "name": "Jonathan Mallinson",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2147586784",
      "name": "Jakub Adámek",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2289295535",
      "name": "Eric Malmi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1267219816",
      "name": "Aliaksei Severyn",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3159959439",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4303648545",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4280652569",
    "https://openalex.org/W4385571219",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4306295121",
    "https://openalex.org/W2016589492",
    "https://openalex.org/W3032046549",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W2475046758",
    "https://openalex.org/W3170432046",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4389523706",
    "https://openalex.org/W4287393336",
    "https://openalex.org/W3034643750",
    "https://openalex.org/W4310629099",
    "https://openalex.org/W4221143046"
  ],
  "abstract": "Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, Aliaksei Severyn. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 2023.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 1773–1781\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nTeaching Small Language Models to Reason\nLucie Charlotte Magister∗\nUniversity of Cambridge\nlcm67@cam.ac.uk\nJonathan Mallinson\nGoogle Research\njonmall@google.com\nJakub Adamek\nGoogle Research\nenkait@google.com\nEric Malmi\nGoogle Research\nemalmi@google.com\nAliaksei Severyn\nGoogle Research\nseveryn@google.com\nAbstract\nChain of thought prompting successfully im-\nproves the reasoning capabilities of large lan-\nguage models, achieving state of the art results\non a range of datasets. However, these rea-\nsoning capabilities only appear to emerge in\nmodels with at least tens of billions of param-\neters. In this paper, we explore the transfer\nof such reasoning capabilities to smaller mod-\nels via knowledge distillation, also investigat-\ning model and dataset size trade-off. Specif-\nically, we ﬁnetune a student model on the\nchain of thought outputs generated by a larger\nteacher model. Our experiments show that the\nproposed method improves task performance\nacross arithmetic, commonsense and symbolic\nreasoning datasets. For example, the accu-\nracy of T5 XXL on GSM8K improves from\n8.11% to 21.99% and 18.42% when ﬁnetuned\non PaLM 540B and GPT-3 175B generated\nchains of thought, respectively.\n1 Introduction\nChain of thought (CoT) prompting encourages lan-\nguage models (LMs) to break down a reasoning\ntask into a series of intermediate steps (Wei et al.,\n2022). They demonstrate that this prompting sig-\nniﬁcantly increases the task accuracy of large lan-\nguage models (LLMs) across commonsense, sym-\nbolic and mathematical reasoning datasets. Here,\nLLMs are models with at least tens of billions of\nparameters, such as PaLM 540B (Chowdhery et al.,\n2022), GPT-3 175B (Brown et al., 2020), or UL2\n20B (Tay et al., 2022). However, the reasoning\ncapabilities of smaller LMs do not improve with\nCoT prompting, mostly producing illogical CoT.\nNotably, CoT prompting even reduces the accuracy\nof models with less than 10 billion parameters. Wei\net al. (2022) attribute this to abilities, such as se-\nmantic understanding and symbolic mapping, only\nemerging at larger scales. This leads us to our re-\n∗Research conducted during an internship at Google.\nsearch question: can the reasoning capabilities of\nLLMs be transferred to smaller LMs via ﬁnetuning?\nThis work explores CoT knowledge distillation\n(Hinton et al., 2015) from PaLM 540B (Chowdhery\net al., 2022) and GPT-3 175B (Brown et al., 2020)\nto different sizes of the smaller language model\nT5 (Raffel et al., 2020), such as T5 XXL, XL and\nbase, which have 11 billion, 3 billion and 220 mil-\nlion parameters, respectively. As a result of our\nwork, we make two recommendations: (1) perform\nknowledge distillation by ﬁnetuning the student\nmodel on the CoT generated by a large teacher\nmodel; and (2) generate the CoT from an LLM, as\nproposed by Wei et al. (2022), but crucially provide\nthe solution to the task in the few-shot prompt. We\ndemonstrate that the proposed method improves\ntask performance across arithmetic, commonsense\nand symbolic reasoning datasets irrespective of the\nteacher model used. For example, we show an ac-\ncuracy increase from 8.11% to 21.99% and 18.42%\non the GSM8K (Cobbe et al., 2021) dataset when\nﬁnetuning T5 XXL on PaLM 540B and GPT-3\n175B generated CoT data, respectively.\n2 Related Work\nThis work is inspired by the seminal work of Wei\net al. (2022) on CoT prompting. They demonstrate\nthat preﬁxing an input with 2-8 exemplars of CoT\nreasoning encourages LMs to do the same, reach-\ning state-of-the-art performance on datasets such\nas GSM8K (Cobbe et al., 2021). Wang et al. (2022)\nshow that task accuracy can be further improved\nby using self-consistency in CoT prompting. Self-\nconsistency samples CoT reasoning paths from a\nmodel’s decoder and returns the most consistent\npath by taking the majority vote. Subsequently,\nChung et al. (2022) explore ﬁnetuning a FLAN-\nbased (Wei et al., 2021) version of PaLM on manu-\nally generated CoT data.\nConcurrent to our work, a small number of other\nworks propose methods focused on CoT student–\n1773\nteacher knowledge distillation. Ho et al. (2022)\nand Li et al. (2022) also explore knowledge dis-\ntillation with the difference of proposing diverse\nsampling and rationalization prompting, respec-\ntively. In contrast to their work, our work explores\nmore teacher models and demonstrates both the\neffects of dataset and model size on accuracy. We\nalso achieve a higher accuracy on common datasets,\nsuch as GSM8K, than Ho et al. (2022). In contrast\nto our work, Shridhar et al. (2022) focus on train-\ning two models, one for problem decomposition\nand one for solving. Yet differently, the focus of\nEisenstein et al. (2022) relies on producing markup-\nand-mask explanations for open-book question an-\nswering. Lastly, Huang et al. (2022) present one\nrelated experiment, however, we present a more in-\ndepth exploration on more datasets. To the best of\nour knowledge, our work is the ﬁrst to extensively\nexplore the improvement of the reasoning ability of\nsmall LMs via knowledge distillation across multi-\nple model architectures, and observing the effects\nof student model size and dataset size on accuracy.\n3 Method\nWe propose a two-step pipeline for CoT knowledge\ndistillation. The ﬁrst step comprises annotating\nan existing supervised dataset with CoT reason-\ning generated by a teacher model. To generate\nhigh quality data, we propose using LLMs, such\nas PaLM 540B or GPT-3 175B, as teachers, based\non the ﬁnding that CoT reasoning improves with\nmodel scale (Wei et al., 2022). Speciﬁcally, we per-\nform few-shot prompting with 8 exemplars on these\nmodels to generate CoTs. However, we make a key\nmodiﬁcation to the prompts proposed by Wei et al.\n(2022). We adapt the few-shot prompts to provide\nthe model with the target after posing the question\nand before providing example CoT. This is based\non the observation that providing this guidance al-\nlows LLMs to correct small mistakes in the CoT.\nLastly, we remove all incorrect CoT based on the\ntarget answer to prevent the student to learn from\nbad examples. The second step comprises ﬁnetun-\ning a student model via teacher forcing (Williams\nand Zipser, 1989). The student is provided with\nthe question as input, and the CoT and answer as\nthe target. As the model is trained on producing a\nCoT during ﬁnetuning, prompting is not required.\nFigure 1 provides an overview of the proposed\nmethod.\nLLMLM\nInput\nCoT Prompts\nQuestion\nAnswer\nGenerated CoT\nQuestion\nOutput\nCoT PromptingFinetuning\nFigure 1: Overview of the proposed method.\n4 Experimental Setup\nWe follow a similar experimental setup to Wei et al.\n(2022), focusing on tasks covering arithmetic, com-\nmonsense and symbolic reasoning.\n4.1 Benchmarks and Metrics\n4.1.1 Arithmetic Reasoning\nWe benchmark the proposed method on the fol-\nlowing math word problem datasets: (1) GSM8K\n(Cobbe et al., 2021), (2) MAWPS (Koncel-\nKedziorski et al., 2016) and (3) ASDiv (Miao et al.,\n2021). We use the ofﬁcial training and testing split\nfor GSM8K, taking the last 10% of the training\nsplit for validation, and the 5-fold cross validation\nsplits available for MAWPS and ASDiv. We evalu-\nate task accuracy by checking for the target answer\nas the ﬁnal answer in the CoT. In addition, we com-\npute the task accuracy given an external calculator,\nto account for arithmetic mistakes made by the\nmodel, despite the CoT being correct. The external\ncalculator moves through the generated output, re-\ncalculating the left hand-side of equations. It then\nreplaces the right-hand side with the calculated\noutput, to avoid arithmetic mistakes being carried\nforward. For example, if a model outputted ’5 +\n5 = 11. 11 * 2 = 22’, then the external calcula-\ntor would ﬁrst calculate ’5+5’ and replace the ’11’\nwith a ’10’. In the subsequent equation, it would\n1774\nalso replace the ’11’ with a ’10’ and arrive at the\nﬁnal result of ’20’.\n4.1.2 Commonsense Reasoning\nWe benchmark the model’s ability to perform com-\nmonsense reasoning on the StrategyQA dataset\n(Geva et al., 2021a). As a testing split is not avail-\nable, we do not shufﬂe the dataset to allow repro-\nducing our split of taking the ﬁrst 80% as training\ndata, the following 10% as validation data, and the\nﬁnal 10% as testing data. We compute task accu-\nracy in the same manner as previously mentioned.\n4.1.3 Symbolic Reasoning\nLastly, we benchmark the model on two synthetic\ntasks for symbolic reasoning: (1) last letter con-\ncatenation and (2) coinﬂip (Wei et al., 2022). Last\nletter concatenation prompts the model to concate-\nnate the last letter of each word in a string. Coinﬂip\nprompts the model to perform state tracking of\nthe coin being ﬂipped. We evaluate task accuracy\nin the same manner as before. Due to the rigid\nstructure of the datasets, we focus on evaluating\nthe model’s generalizability to out-of-distribution\n(OOD) examples. We ﬁnetune the models on ex-\namples of length two and evaluate on sequences of\nlength three and four. We initially infer the CoT\nusing PaLM 540B, however, ﬁnd that the LLM is\nable to perfectly replicate the desired CoT bar one\nexample due to the rigidness of the template. We\ntherefore decide to use the template generated CoT\nin our experiments.\n4.2 Baselines and setup\nWe select PaLM 540B (Chowdhery et al., 2022)\nand GPT-3 175B (Brown et al., 2020) as teacher\nmodels. We select PaLM 540B based on the state-\nof-the-art results on the benchmarking datasets re-\nported by Wei et al. (2022), and conﬁrm the ob-\nserved trends with GPT-3 175B. The publicly ac-\ncessible teacher models are prompted as described\nin Section 3.\nWe select different sizes of T5 (Raffel et al.,\n2020) as student models, as T5 is publicly available\nin many sizes. The student models are trained on\nthe PaLM 540B or GPT-3 175B generated CoT\ndata as described in Section 3. We establish T5\nXXL model ﬁnetuned on the original target as the\nbaseline. We refrain from shufﬂing the datasets\nto allow for reproducibility.For the MAWPS and\nASDiv dataset, we perform 5-fold cross validation.\nFor all remaining datasets, we take 10% of the\nInput: \nRoger has 5 tennis balls. He buys 2 more cans\nof tennis balls. Each can has 3 tennis balls. How\nmany tennis balls does he have now?\nOutput: \nRoger started with 5 balls. 2 cans of 3 tennis\nballs each is 6 tennis balls. 5 + 6 = 11. The\nanswer is 11. \nFigure 2: A training example from Wei et al. (2022)\ndemonstrating the input and output provided to T5.\ntraining set as a validation set to select the best\nmodel checkpoint. Figure 2 showcases an input\nexamples for T5. We refer the reader to Wei et al.\n(2022) for more training examples, as well as the\nprompts used for generating the CoT using PaLM\n540B and GPT-3 175B.\nWe refer the reader to Appendix A for an\noverview of the dataset licenses. We also refer\nthe reader to Appendix B for an overview of the\ncomputatinal resources.\n5 Results\n5.1 Arithmetic reasoning\nTable 1 details the task accuracy with and without\nan external calculator for the arithmetic reasoning\nbenchmarks. Our results show that the proposed\nmethod improves task accuracy across all datasets.\nMost notably, the task accuracy of MAWPS is sig-\nniﬁcantly improved. The accuracy achieved given\na calculator comes close to the accuracy of 8-shot\nPaLM 540B, demonstrating that knowledge distil-\nlation is effective, but potentially limited by the\nmathematical abilities of small models.\nBaselineT5 XXLCoT FinetunedT5 XXLCoT 8-shotPaLM 540BAcc. Acc.Acc.with Calc.Acc.Acc.with Calc.GSM8K8.1121.9938.21 56.90 58.60Dataset Size 6725 5337 5337 - -MA WPS54.1570.4188.22 93.00 93.66Dataset Size 1590 1590 1590 - -ASDiv39.6442.1260.73 73.9 72.6Dataset Size 1844 1544 1544 - -\nTable 1: Task accuracy across arithmetic reasoning\ndatasets for T5 XXL without ﬁnetuning (baseline) and\nﬁnetuned on PaLM 540B generated chain-of-thought\n(CoT). We report the accuracy of PaLM 540B on the\nused datasets for reference. We do not ﬁnetune PaLM\nfor this, but employ 8 chain of thought prompts.\n1775\n5.1.1 Ablation study on generating\nchain-of-thought data\nWe perform an ablation study to conﬁrm that pro-\nviding a LLM with the target during CoT genera-\ntion is beneﬁcial. We found that for the GSM8K\ndataset, PaLM 540B only achieves a 59.98% accu-\nracy if prompted without the target. In comparison,\nwhen including the target in the prompt the accu-\nracy is 79.37%. A superﬁcial explanation would be\nthat when the model is conditioned on the expected\nanswer, it produces the same CoT but copies the\nanswer. However, an analysis of a subset of the dif-\nferences between CoT produced with and without\nthis conditioning shows that most of the beneﬁts\nactually come from the model correcting CoT that\nhad a single step missing or was wrong.\n5.2 Commonsense reasoning\nFor the StrategyQA dataset (Table 3), we found\nthat using CoT ﬁnetuning improves accuracy from\n68.12% to 71.98%, using only 1319 of the original\n1648 examples. Compared to the arithmetic reason-\ning datasets, the improvement is not as signiﬁcant.\nThis can be explained by the model lacking factual\nknowledge that the dataset requires. The task is\nheavily focused on the model reasoning on such\nknowledge, however, a smaller LM is most likely\nnot in possession of this knowledge compared to a\nlarger model with higher memorisation capacity.\n5.3 Symbolic reasoning\nTable 2 shows the results obtained for the synthetic\nsymbolic reasoning datasets, focusing on OOD gen-\neralization. Focusing on Last Letter Concatena-\ntion, it can be stated that both traditional ﬁnetuning\nand the suggested method fail at generalizing to\na longer sequence length. In comparison, the pro-\nposed method signiﬁcantly increases accuracy for\nthe Coinﬂip dataset with regard to generalizing to\nthree coinﬂips. In contrast, generalisation to four\ncoinﬂips is slightly weaker than the baseline, which\nperforms very strongly. This may be related to the\ntask length being twice that of the training task.\n5.4 Replicating Results using different\nTeacher Models\nWe demonstrate the robustness of our method us-\ning a different teacher model, namely GPT-3 175B.\nTable 3 shows the results for GSM8K and Strat-\negyQA when T5 XXL is ﬁnetuned on CoT data\ngenerated by GPT-3. The results show that the pro-\nposed method elicits improvements also with other\nBaselineT5 XXLCoT FinetunedT5 XXLCoT 8-shotPaLM 540BLast LetterConcat.OOD: 3 0.00 0.00 94.8OOD: 4 0.00 0.00 63.0CoinﬂipOOD: 3 13.1086.7098.6OOD: 473.8070.50 90.2\nTable 2: Task accuracy across the symbolic reason-\ning datasets for T5 XXL ﬁnetuned on chain-of-thought\n(CoT) data. For each dataset, there are 1000 training\nand testing examples. We report the accuracy of PaLM\n540B from (Wei et al., 2022) for reference.\nLLMs as teachers. We also report the accuracy of\nT5 XXL ﬁnetuned on golden CoT provided with\nthe datasets. For the StrategyQA dataset, the model\nﬁnetuned on the golden CoT performs best, which\nmay be attributed to the dataset being the largest, as\nboth PaLM and GPT-3 get some examples wrong.\nIn contrast, the model ﬁnetuned on PaLM gener-\nated CoT performs the best for GSM8K.\nBaseTaskOriginalCotCoT ﬁnetunedT5 XXL usingCoT 8-ShotPaLM540BGPT-3175BPaLM540BGPT-3175BGSM8K8.11 19.9421.9918.42 56.9 46.9acc. with Calc. - 26.99 38.21 33.06 58.6 49.6Dataset Size 6725 6725 5337 5298 - -StrategyQA68.1271.9867.15 63.77 77.8 65.4Dataset Size 1648 1648 1319 1319 - -\nTable 3: Task accuracy for T5 XXL ﬁnetuned on chain-\nof-thought (CoT) data generated by PaLM 540B and\nGPT-3 175B. We also ﬁnetune on the reasoning steps\nprovided by the datasets. We report the accuracy of\nPaLM 540B on the used datasets for reference. We\ndo not ﬁnetune PaLM for this, but employ 8 chain of\nthought prompts.\n5.5 Ablation study on model size\nWe investigate the performance gain achieved via\nﬁnetuning student models of different sizes. Figure\n3 shows the performance gain achieved when ﬁne-\ntuning T5 of different sizes on the GSM8K dataset.\nOur results show that T5 base, with 44 times fewer\nparameters than T5 XXL, matches the performance\nof the baseline T5 XXL when trained on CoT data.\nMoreover, given an external calculator, even T5\nsmall outperforms the baseline T5 XXL.\n5.6 Ablation study on dataset size\nWe also investigate the trade-off between the per-\nformance gain from CoT ﬁnetuning and dataset\nsize. Table 4 details the test accuracy achieved\nwhen ﬁnetuning T5 XXL on only 4% and 20% of\nthe data, randomly selected. In comparison to the\n1776\nsmall base large xl xxl\nT5 model size\n5\n10\n15\n20\n25\n30\n35Accuracy\n3.34 4.17 4.40\n6.67\n8.117.05 7.96\n9.70\n12.13\n21.99\n13.12\n20.17\n22.21\n24.34\n38.21\nbaseline\nCoT\nCoT w/ calc\nFigure 3: Effect of student model (T5) size on accuracy\non GSM8K.\nbaseline accuracy of 8.11% (Table 3), we see that\nour method is 6x more data efﬁcient, achieving\naccuracy of 11.22% with only 20% of the exam-\nples. However, training on just 20% of the data still\ncreates a quality gap, and it’s possible that with\ne.g. 200% larger dataset we could outperform the\nresults in Table 3.\nPercentage of GSM8K\ndata used to train\nCoT ﬁnetuned T5 XXL\nAcc. Acc. with Calc.\n4% (213 examples) 6.29 12.28\n20% (1067 examples) 11.22 20.47\n100% (5337 examples) 21.99 38.21\nTable 4: Task accuracy of T5 XXL ﬁnetuned on differ-\nent amounts of chain-of-thought (CoT) data generated\nby PaLM 540B.\n6 Discussion\nWe demonstrate that ﬁnetuning larger LMs on the\nCoT data generated by LLMs of over 100 billion\nparameters can signiﬁcantly improve task accuracy.\nEven a small number of CoT examples appear to\nsufﬁce for this. However, such improvements ap-\npear to be task dependent. For example, the effects\nare limited for the StrategyQA dataset, which can\nbe attributed to the task requiring speciﬁc factual\nknowledge, which smaller LMs may not have mem-\norised due to their limited capacity. Nevertheless,\nthere is some performance improvement, which\nmay be attributed to the model learning how to ap-\nproach such tasks. Moreover, the CoT knowledge\ndistillation pipeline presented allows to trade-off\nmodel and dataset size with accuracy. Future work\ncould explore improving the reasoning of small\nmodels in multi-task settings, as well as the gener-\nation of new training data using LLMs, rather than\nannotating existing datasets.\n7 Conclusion\nThis work explores CoT knowledge distillation\nfrom LLMs of over 100 billion parameters to\nsmaller LMs. We propose a knowledge distilla-\ntion pipeline consisting of two keys steps: (1) gen-\nerate CoT for existing datasets using LLMs and\n(2) ﬁnetune smaller LMs on the CoT. Our results\ndemonstrate that ﬁnetuning on CoT improves task\naccuracy across a range of benchmarking datasets.\n8 Limitations\nThe results we present must be viewed in the con-\ntext of a few limitations. A limitation is that we\nonly perform experiments in English and on one\ntask at a time. To be more comparable to a LLM\nfew-shot settings, other languages and a multi-task\nsetup could be explored. Furthermore, in order to\nreplicate the results access to none public models is\nrequired and inference must be performed on large\namounts of data. Another limitation of our work\nis that it only explores the original CoT prompting\napproach, but we do not explore subsequent im-\nprovements, such a self-consistency (Wang et al.,\n2022).\n9 Ethical Considerations\nThe main ethical considerations of our research\narise from the text generation performed. The\nconcerns here are that both the teacher and stu-\ndent model may potentially generate non-factual\n(Ji et al., 2022; Pagnoni et al., 2021; Kreps et al.,\n2022) or offensive output (Gehman et al., 2020).\nThis is largely inﬂuenced by the input data, which\nis our case are standard, peer-reviewed benchmark-\ning tasks in the NLP domain.\nReferences\nBIG-bench collaboration. 2021. Beyond the imitation\ngame: Measuring and extrapolating the capabilities\nof language models. In preparation.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\n1777\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-ﬁnetuned language mod-\nels. arXiv preprint arXiv:2210.11416.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavar-\nian, Jacob Hilton, Reiichiro Nakano, Christopher\nHesse, and John Schulman. 2021. Training veri-\nﬁers to solve math word problems. arXiv preprint\narXiv:2110.14168.\nJacob Eisenstein, Daniel Andor, Bernd Bohnet,\nMichael Collins, and David Mimno. 2022. Hon-\nest students from untrusted teachers: Learning\nan interpretable question-answering pipeline from\na pretrained language model. arXiv preprint\narXiv:2210.02498.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A Smith. 2020. Realtoxici-\ntyprompts: Evaluating neural toxic degeneration in\nlanguage models. arXiv preprint arXiv:2009.11462.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021a. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. Transactions of the As-\nsociation for Computational Linguistics, 9:346–361.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021b. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. Transactions of the As-\nsociation for Computational Linguistics, 9:346–361.\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2(7).\nNamgyu Ho, Laura Schmid, and Se-Young Yun.\n2022. Large language models are reasoning teach-\ners. arXiv preprint arXiv:2212.10071.\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu,\nXuezhi Wang, Hongkun Yu, and Jiawei Han. 2022.\nLarge language models can self-improve. arXiv\npreprint arXiv:2210.11610.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,\nDan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea\nMadotto, and Pascale Fung. 2022. Survey of hallu-\ncination in natural language generation. ACM Com-\nputing Surveys.\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate\nKushman, and Hannaneh Hajishirzi. 2016. Mawps:\nA math word problem repository. In Proceedings of\nthe 2016 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1152–1157.\nSarah Kreps, R Miles McCain, and Miles Brundage.\n2022. All the news that’s ﬁt to fabricate: Ai-\ngenerated text as a tool of media misinformation.\nJournal of Experimental Political Science, 9(1):104–\n117.\nShiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen,\nXinlu Zhang, Zekun Li, Hong Wang, Jing Qian,\nBaolin Peng, Yi Mao, et al. 2022. Explanations from\nlarge language models make small reasoners better.\narXiv preprint arXiv:2210.06726.\nShen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su.\n2021. A diverse corpus for evaluating and devel-\noping english math word problem solvers. arXiv\npreprint arXiv:2106.15772.\nArtidoro Pagnoni, Vidhisha Balachandran, and Yulia\nTsvetkov. 2021. Understanding factuality in abstrac-\ntive summarization with frank: A benchmark for fac-\ntuality metrics. arXiv preprint arXiv:2104.13346.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\nKumar Shridhar, Alessandro Stolfo, and Mrinmaya\nSachan. 2022. Distilling multi-step reasoning capa-\nbilities of large language models into smaller mod-\nels via semantic decompositions. arXiv preprint\narXiv:2212.00193.\nYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Gar-\ncia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng,\nNeil Houlsby, and Donald Metzler. 2022. Unify-\ning language learning paradigms. arXiv preprint\narXiv:2205.05131.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2022. Self-consistency\nimproves chain of thought reasoning in language\nmodels. arXiv preprint arXiv:2203.11171.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nRonald J Williams and David Zipser. 1989. A learn-\ning algorithm for continually running fully recurrent\nneural networks. Neural computation , 1(2):270–\n280.\n1778\nA Dataset Usage and Licenses\nIn this section, we list the licenses for the datasets\nused and any ethical concerns regarding their usage.\nWe describe the dataset splits used for all datasets\nin Section 4 of the paper.\nA.1 Arithmetic Reasoning\nThe GSM8K dataset (Cobbe et al., 2021) is avail-\nable under the MIT license. The MAWPS dataset\n(Koncel-Kedziorski et al., 2016) is available under\nthe CC BY 4.0 and the ASDiv dataset (Miao et al.,\n2021) is available under the CC BY-NC 4.0 license.\nWe follow the intended usage of the datasets.\nA.2 Commonsense Reasoning\nThe StrategyQA dataset (Geva et al., 2021b) is\navailable under the MIT license. Similar to Wei\net al. (2022), we use the open-domain setting ver-\nsion available as part of the Big-bench collabo-\nration (BIG-bench collaboration, 2021), available\nunder the Apache License 2.0. We follow the in-\ntended usage of the datasets.\nA.3 Symbolic Reasoning\nWe generate the symbolic reasoning datasets as\ndescribed in Wei et al. (2022).\nB Computational Resources\nWe perform inference and ﬁnetuning on dif-\nferent sizes of T5 on TPUs. We perform\ninference on PaLM 540B also on TPUs. Our\nresults can be replicated via the public API\n(https://developers.generativeai.\ngoogle/products/palm). To make re-\nquests to GPT-3 175B, we use the public API\n(https://beta.openai.com/docs/\nintroduction).\n1779\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 8\n□\u0013 A2. Did you discuss any potential risks of your work?\nSection 9\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract and Section 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 4\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 4\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nAppendix A\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nSection 4\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nWe did not discuss this as the datasets are commonly used NLP benchmarks that do not contain\npersonal data.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nWe discuss this in Section 8, the limitations section. We discuss the coverage of domains in Section 4.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nWe discuss this in Section 4.\nC □\u0013 Did you run computational experiments?\nSections 4 and 5\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nWe report the model speciﬁcs in section 4. We describe the computing infrastructure in Appendix 2,\nbut do not estimate the computational budget.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n1780\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 4\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSections 4 and 5\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n1781",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5675586462020874
    },
    {
      "name": "Computational linguistics",
      "score": 0.5632264018058777
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.5020380020141602
    },
    {
      "name": "Programming language",
      "score": 0.45371800661087036
    },
    {
      "name": "Linguistics",
      "score": 0.4125676155090332
    },
    {
      "name": "Natural language processing",
      "score": 0.35380685329437256
    },
    {
      "name": "Library science",
      "score": 0.3442910313606262
    },
    {
      "name": "Philosophy",
      "score": 0.2129252851009369
    },
    {
      "name": "Physics",
      "score": 0.04957616329193115
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ]
}