{
    "title": "Language-Agnostic Bias Detection in Language Models with Bias Probing",
    "url": "https://openalex.org/W4389520517",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5110633837",
            "name": "Abdullatif Köksal",
            "affiliations": [
                "Munich Center for Machine Learning",
                "Ludwig-Maximilians-Universität München",
                "University of Cambridge"
            ]
        },
        {
            "id": "https://openalex.org/A5013146552",
            "name": "Omer Faruk Yalcin",
            "affiliations": [
                "University of Massachusetts Amherst"
            ]
        },
        {
            "id": "https://openalex.org/A5092005420",
            "name": "Ahmet Akbiyik",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5091224402",
            "name": "M. Tahir Kılavuz",
            "affiliations": [
                null,
                "Marmara University"
            ]
        },
        {
            "id": "https://openalex.org/A5081393566",
            "name": "Anna Korhonen",
            "affiliations": [
                "University of Cambridge"
            ]
        },
        {
            "id": "https://openalex.org/A5088559117",
            "name": "Hinrich Schuetze",
            "affiliations": [
                "Munich Center for Machine Learning",
                "Ludwig-Maximilians-Universität München"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6745802261",
        "https://openalex.org/W2990702745",
        "https://openalex.org/W3128630643",
        "https://openalex.org/W4287887133",
        "https://openalex.org/W2893425640",
        "https://openalex.org/W3177141404",
        "https://openalex.org/W3209051700",
        "https://openalex.org/W2047899752",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2004420697",
        "https://openalex.org/W2996580882",
        "https://openalex.org/W4385573668",
        "https://openalex.org/W3176477796",
        "https://openalex.org/W2741019177",
        "https://openalex.org/W1978841093",
        "https://openalex.org/W3044438666",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4281621415",
        "https://openalex.org/W2903735375",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W4288029087",
        "https://openalex.org/W2970419734",
        "https://openalex.org/W2788920400",
        "https://openalex.org/W4309715242",
        "https://openalex.org/W3116641301",
        "https://openalex.org/W3195480235",
        "https://openalex.org/W4281784180",
        "https://openalex.org/W2963078909",
        "https://openalex.org/W3149121162",
        "https://openalex.org/W2972413484",
        "https://openalex.org/W3125286141",
        "https://openalex.org/W2170240176",
        "https://openalex.org/W3176198948",
        "https://openalex.org/W1049943879",
        "https://openalex.org/W2106836779",
        "https://openalex.org/W3105882417",
        "https://openalex.org/W3198757395",
        "https://openalex.org/W2802105481",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3097202947",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4311170595",
        "https://openalex.org/W3154504973",
        "https://openalex.org/W3103187652",
        "https://openalex.org/W3037387464",
        "https://openalex.org/W3172415559"
    ],
    "abstract": "Pretrained language models (PLMs) are key components in NLP, but they contain strong social biases. Quantifying these biases is challenging because current methods focusing on fill-the-mask objectives are sensitive to slight changes in input. To address this, we propose a bias probing technique called LABDet, for evaluating social bias in PLMs with a robust and language-agnostic method. For nationality as a case study, we show that LABDet “surfaces” nationality bias by training a classifier on top of a frozen PLM on non-nationality sentiment detection. We find consistent patterns of nationality bias across monolingual PLMs in six languages that align with historical and political context. We also show for English BERT that bias surfaced by LABDet correlates well with bias in the pretraining data; thus, our work is one of the few studies that directly links pretraining data to PLM behavior. Finally, we verify LABDet’s reliability and applicability to different templates and languages through an extensive set of robustness checks. We publicly share our code and dataset in https://github.com/akoksal/LABDet.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 12735–12747\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nLanguage-Agnostic Bias Detection in Language Models with Bias Probing\nAbdullatif Köksal1,2,7 Omer Faruk Yalcin3 Ahmet Akbiyik4\nM. Tahir Kılavuz5,6 Anna Korhonen7 Hinrich Schütze1,2\n1Center for Information and Language Processing, LMU Munich 2Munich Center for Machine Learning\n3Data Analytics and Computational Social Science, University of Massachusetts Amherst\n4Harvard Kennedy School 5Middle East Initiative, Harvard Kennedy School\n6Marmara University 7Language Technology Lab, University of Cambridge\nakoksal@cis.lmu.de\nAbstract\nPretrained language models (PLMs) are key\ncomponents in NLP, but they contain strong\nsocial biases. Quantifying these biases is chal-\nlenging because current methods focusing on\nﬁll-the-mask objectives are sensitive to slight\nchanges in input. To address this, we propose\na bias probing technique called LABDet, for\nevaluating social bias in PLMs with a robust\nand language-agnostic method. For nationality\nas a case study, we show that LABDet “sur-\nfaces” nationality bias by training a classiﬁer\non top of a frozen PLM on non-nationality\nsentiment detection. We ﬁnd consistent pat-\nterns of nationality bias across monolingual\nPLMs in six languages that align with histor-\nical and political context. We also show for\nEnglish BERT that bias surfaced by LABDet\ncorrelates well with bias in the pretraining data;\nthus, our work is one of the few studies that\ndirectly links pretraining data to PLM behav-\nior. Finally, we verify LABDet’s reliability\nand applicability to different templates and lan-\nguages through an extensive set of robustness\nchecks. We publicly share our code and dataset\nin https://github.com/akoksal/LABDet.\n1 Introduction\nPretrained language models (PLMs) have gained\nwidespread popularity due to their ability to achieve\nhigh performance on a wide range of tasks (Devlin\net al., 2019). Smaller PLMs, in particular, have\nbecome increasingly popular for their ease of de-\nployment and ﬁnetuning for various applications,\nsuch as text classiﬁcation (Wang et al., 2018), ex-\ntractive text summarization (Liu and Lapata, 2019),\nand even non-autoregressive text generation (Su\net al., 2021). Despite their success, it is established\nthat these models exhibit strong biases, such as\nthose related to gender, occupation, and nationality\n(Kurita et al., 2019; Tan and Celis, 2019). How-\never, quantifying intrinsic biases of PLMs remains\na challenging task (Delobelle et al., 2022).\nDutch PLM\n(BER Tje)\nT urkish PLM\nBER T urk\nSentiment \nClassifier\nSentiment \nClassifier\nThis [Nationality] person is neutral.Template\nNationality Sentiment SurfacingLanguage\nT urkish\nTurkish\nDutch\n+0.16\n(positive)\n-0.17\n(negative)\n-0.40\n(negative)\n+0.40\n(positive)\nGr eek\nT urkish\nGr eek\nDeze T ur k is neutr aal.\nB u T ür k ad am tar afsız biri.\nB u Y unan ad am tar afsız biri.\nDeze Grie k is neutr aal.\nFigure 1: Our bias probing method surfaces nationality\nbias by computing relative sentiment change: subtract-\ning absolute positive sentiment of an example with a\nnationality from a neutral example without nationality\n(i.e., with the [MASK] token). Therefore, the Turkish\nPLM exhibits a positive interpretation for Turks and a\nnegative interpretation for Greeks in the same context.\nConversely, the Dutch PLM demonstrates the opposite\ntrend, with a positive sentiment towards Greeks and a\nnegative sentiment towards Turks.\nRecent work on social bias detection in PLMs\nhas mainly focused on English. Most of those ap-\nproaches have limited capabilities in terms of sta-\nbility and data quality (Antoniak and Mimno, 2021;\nBlodgett et al., 2021). Therefore, we propose a ro-\nbust ‘Language-Agnostic Bias Detection’ method\ncalled LABDet for nationality as a case study and\nanalyze intrinsic bias in monolingual PLMs in Ara-\nbic, Dutch, English, French, German, and Turkish\nwith bias probing. LABDet addresses the limita-\ntions of prior work by training a sentiment classiﬁer\non top of PLMs, using templates containing pos-\nitive/negative adjectives, without any nationality\ninformation. This lets LABDet learn sentiment\nanalysis, but without the bias in existing sentiment\ndatasets (Asyroﬁ et al., 2022; Kiritchenko and Mo-\nhammad, 2018).\nThe second key idea of bias probing is to surface\nbias by using templates and corpus examples with\na nationality slot for which we compare substitu-\ntions, e.g., “Turkish” vs “Greek” in Turkish and\n12735\nDutch PLMs as illustrated in Figure 1. When an-\nalyzing the template “This [Nationality] person is\nneutral.” in Turkish and Dutch, we found that the\nTurkish PLM, BERTurk (Schweter, 2020), gives\na relative sentiment score of +0.16 for the Turk-\nish nationality and -0.17 for the Greek nationality,\nwhile the Dutch PLM, BERTje (de Vries et al.,\n2019), gives a relative sentiment score of -0.40 for\nthe Turkish nationality and +0.40 for the Greek\nnationality. The relative sentiment score surfaces\nthe effect of nationality on the sentiment by sub-\ntracting absolute sentiment scores from the senti-\nment score of a neutral example without nationality,\ne.g., “This [MASK] person is neutral.”. This differ-\nence in relative sentiment scores between the two\nmodels aligns with historical and political context:\nTurkish-Greek conﬂicts, the Turkish minority in\nthe Netherlands etc.\nThese patterns are examined across various tem-\nplates and corpus examples to identify consistent\npreferences exhibited by PLMs. We then show the\nlinks between biases extracted with our method\nand bias present in the pretraining data of exam-\nined PLMs. We provide a comprehensive analysis\nof bias in the pretraining data of BERT. We exam-\nine the context positivity rate of sentences contain-\ning nationality information and also investigate the\nnationalities of authors in the Wikipedia part of\nthe pretraining data. Furthermore, we present con-\nnections between biases present in the real world\nand biases extracted via LABDet, particularly in\nrelation to minority groups, geopolitics, and his-\ntorical relations. Finally, the consistency of these\npatterns across different templates enhances the ro-\nbustness and validity of our ﬁndings, which have\nbeen rigorously conﬁrmed through an extensive\ntesting process in six languages.\nOur paper makes the following contributions:\n(i) Pretraining Data Bias : We quantify the\nnationality bias in BERT’s pretraining data and\nshow that LABDet detects BERT’s bias with a\nsigniﬁcant correlation, thus strongly suggesting a\ncausal relationship between pretraining data and\nPLM bias.\n(ii) Linkage to Real-world Biases : We apply\nLABDet to six languages and demonstrate\nthe relationship between real-world bias about\nminorities, geopolitics, and historical relations and\nintrinsic bias of monolingual PLMs identiﬁed by\nLABDet, ﬁnding support in the relevant political\nscience literature.\n(iii) Robustness: We propose LABDet, a novel\nbias probing method that detects intrinsic bias\nin PLMs across languages. Through robustness\nchecks, we conﬁrm LABDet’s reliability and\napplicability across different variables such as\nlanguages, PLMs, and templates, thus improving\nover existing work.\n2 Related Work\nMeasuring Social Bias: One approach identiﬁes\nassociations between stereotypical attributes and\ntarget groups (May et al., 2019) by analyzing their\nembeddings. This approach utilizes single tokens\nand “semantically bleached” (i.e., “sentiment-less”)\ntemplates, which limits its applicability (Delobelle\net al., 2022). Another approach (CrowS-Pairs (Nan-\ngia et al., 2020), StereoSet (Nadeem et al., 2021))\ncompares the mask probability in datasets of stereo-\ntypes. This is prone to instability and data qual-\nity issues (Antoniak and Mimno, 2021; Blodgett\net al., 2021) and difﬁcult to adapt across differ-\nent languages. Additionally, PLMs are sensitive\nto templates, which can result in large changes in\nthe masked token prediction (Jiang et al., 2020;\nDelobelle et al., 2022).\nBias Detection in Non-English Languages: Many\nstudies on bias detection for non-English PLMs,\nprimarily focus on developing language-speciﬁc\ndata and methods. For instance, Névéol et al.\n(2022) adapts the CrowS-Pairs dataset for the\nFrench language, while another recent approach\n(Kurpicz-Briki, 2020) extends the bias detection\nmethod in word embeddings, WEAT (Caliskan\net al., 2017), to include the French and German lan-\nguages. Chávez Mulsa and Spanakis (2020) also\nexpand WEAT to Dutch and analyze bias at the\nword embedding level. However, these language-\nspeciﬁc methods face similar challenges regarding\nrobustness and reliability.\nPretraining Data Analysis: Recent work exam-\nines the relationship between PLM predictions and\ntheir pretraining data, particularly in the context of\nfact generation (Akyurek et al., 2022) and prompt-\ning for sentiment analysis and textual entailment\n(Han and Tsvetkov, 2022). Other work focuses on\ndetermining the amount of pretraining data neces-\nsary for PLMs to perform speciﬁc tasks, such as\nsyntax (Pérez-Mayos et al., 2021) or natural lan-\nguage understanding (NLU) (Zhang et al., 2021).\nTo the best of our knowledge, our work is the ﬁrst\n12736\nto establish a connection between pretraining data\nand PLM behavior for intrinsic social bias.\n3 Dataset\nOur bias probing method, LABDet, includes two\nsteps for detecting and quantifying social bias in\npretrained language models (PLMs). The ﬁrst con-\ncerns sentiment training: we train a classiﬁer on\ntop of a frozen PLM with generic sentiment data\nwithout nationality information. This step aims to\nmap contextual embeddings to positive/negative\nsentiments without changing any underlying infor-\nmation about nationality. In the second step, we\ncreate minimal pairs for bias quantiﬁcation via sen-\ntiment surfacing. We provide minimal pairs with\ndifferent nationalities (e.g., “Turk” vs “Greek”) and\nsee how nationalities surfaces different sentiments.\nOur dataset covers six languages: Arabic, Dutch,\nEnglish, French, German, and Turkish.\n3.1 Sentiment Training Dataset\nWe carefully design a novel sentiment dataset to\nmap contextual embeddings of PLMs to sentiments.\nOur goal is to not include any bias about nationali-\nties – which a pair like (“Turkish people are nice.”,\npositive) would do – to keep the sentiment towards\nnationalities in PLMs unchanged. We do not take\nadvantage of existing sentiment analysis datasets as\nthey contain bias in different forms (Asyroﬁ et al.,\n2022). For example, the YELP dataset (Zhang\net al., 2015) contains negative reviews towards the\ncuisine which may be interpreted towards national-\nities by PLMs as illustrated in this YELP example:\n“Worst mexican ever!!!!!! Don’t go there!!!”.\nTherefore, we propose a template-based ap-\nproach with careful design. We select six languages\nwith diverse linguistic features based on the linguis-\ntic capabilities of the authors and conduct experi-\nments in those languages: Arabic, Dutch, English,\nFrench, German, and Turkish. For each language,\nour annotators design templates with adjective and\nnoun slots. The objective is to convey the sen-\ntence’s sentiment through the adjective’s sentiment\nwhile keeping the sentences otherwise neutral with-\nout any nationality information. The adjective slots\ncan be ﬁlled with positive and negative adjectives\nselected from a pool of ≈25 adjectives, determin-\ning the ﬁnal sentiment. Additionally, we created\n≈20 nouns for each language. Finally, with ≈10\ntemplates, we generated over 3,500 training exam-\nples for each language. We illustrate one template\nper language, two nouns, and positive/negative ad-\njectives in Table 1 (top).\nTemplate-based approaches are prone to syntax\nand semantics issues. For example, we see that\nthere are gender agreement issues or meaningless\npairs (e.g., insufﬁcient day). While this is one lim-\nitation of our sentiment dataset, we believe that\ntraining the model on these ungrammatical or less\nmeaningful sentences would not impact the overall\ngoal of this part, sentiment surfacing from contex-\ntual embeddings. We design experiments to verify\nour method by comparing the correlation between\nbias present in the pretraining data of PLMs and\nbias extracted via LABDet.\n3.2 Minimal Pairs for Sentiment Surfacing\nIn the second step, we create a second dataset of\nminimal pairs to analyze the effect of nationality on\nthe sentiment results to quantify bias. However, as\nthe role of templates would play a big role here, we\ncurate templates from different sources and verify\nthe effectiveness of our method, LABDet.\nTemplate Pairs: We carefully design templates in\ndifferent languages to create minimal pairs. These\nminimal pairs are designed to have neutral context\nfor different nationalities. Our annotators create\ntemplates with [Nationality] and [Adjective] tags\nand this time they propose a neutral set of adjec-\ntives. Therefore, we aim to investigate the effect of\nnationality changefor positive/negative sentiment\nsurfacing. As illustrated in in Table 1 (bottom,\n“Sentiment Surfacing”), we create sentences such\nas “This Syrian person is neutral.”, with≈15 neu-\ntral adjectives for each language.\nAs an alternative template approach, we modify\nthe templates proposed by Kiritchenko and Mo-\nhammad (2018), Equity Evaluation Corpus (EEC),\nwhich include both negative and positive adjectives\ncontrary to our neutral examples. Since we track\nchanges in the positive sentiment score in LABDet,\neven the same positive context with different na-\ntionalities could have varying degrees of positive\nsentiment scores, which would indicate bias toward\nnationalities. Instead of using nouns in the source,\nwe utilize [Nationality] tags as shown in Table 4 in\nthe Appendix. Since the source corpus is proposed\nonly for the English language, we use EEC for the\nveriﬁcation of our method in English.\nCorpus Pairs: Additionally, we present templates\ngenerated from corpus sentences. For six lan-\nguages, we create minimal pairs from mC4 (Raffel\n12737\nMode Language Template Noun/Nationality Adjective\nSentiment\nTraining\nArabic [Adj] /behisolated /rehfinal/ainmedial/sheeninitial/alefwithhamzaaboveisolated /yehfinal/noonmedial/lammedial/ainmedial/jeemmedial/yehinitial[Noun] /alefisolated/thalfinal/hehinitial /tehmarbutafinal/behinitial/rehfinal/jeemmedial/tehinitial, /meemisolated/wawfinal/yehinitial /dalfinal/yehmedial/ainmedial/seeninitial(1), /behfinal/dadinitial/aleffinal/ghaininitial(-1)\nDutch Deze [Noun] geeft me een [Adj] gevoel. ervaring, dag gelukkig (1), boos (-1)\nEnglish This [Noun] is making me feel [Adj]. experience, day happy (1), angry (-1)\nFrench Ce [Noun] me rend [Adj]. expérience, jour heureux (1), furieux (-1)\nGerman Diese [Noun] lässt mich [Adj] fühlen. Erfahrung, Tag glücklich (1), wütend (-1)\nTurkish Bu [Noun] beni [Adj] hissettiriyor. deneyim, gün mutlu (1), kızgın (-1)\nSentiment\nSurfacing\nArabic [Adj] [Nationality] /lamisolated/alefisolated /sadfinal/khahmedial/sheenmedial/laminitial/alefisolated /alefisolated/thalfinal/hehinitial /yehisolated/rehisolated/wawfinal/seeninitial, /yehfinal/kafmedial/yehinitial/rehfinal/meeminitial/alefwithhamzaaboveisolated /dalfinal/yehinitial/aleffinal/hahmedial/meeminitial(0), /tahfinal/seeninitial/wawfinal/tehmedial/meeminitial(0)\nDutch Deze [Nationality] is [Adj]. Syriër, Amerikaan neutraal (0), gemiddeld (0)\nEnglish This [Nationality] person is [Adj]. Syrian, American neutral (0), average (0)\nFrench Cet homme [Nationality] est [Adj]. Syrien, Americain neutre (0), moyen (0)\nGerman Dieser [Nationality] ist [Adj]. Syrier, Amerikaner neutral (0), durchschnittlich (0)\nTurkish Bu [Nationality] adam [Adj] biri. Suriyeli, Amerikan nötr (0), ortalama (0)\nTable 1: Bias probing templates in LABDet. Slots (for Adj, Noun, Nationality) are indicated by []. Sentiment\nTraining: LABDet is trained on non-nationality Adj-Noun pairs. Sentiment Surfacing: LABDet uses neutral\nadjectives to surface positive/negative sentiment about a nationality for bias detection. The full list of templates is\navailable at https://github.com/akoksal/LABDet.\nLanguage Corpus Template\nArabic ./lamfinal/sadinitial/lamwithalefhamzaaboveisolatedd/alefisolated[Nationality] /hehfinal/nooninitial/alefwithhamzabelowisolated /lamisolated/aleffinal/qafmedial/yehinitial1\nDutch Elke [Nationality] heeft recht op privacy. 1\nEnglish They are an “icon of [Nationality] Culture”.\nFrench C’est un poète [Nationality] et écrivain. 1\nGerman Typisch [Nationality] eben. 1\nTurkish Her [Nationality] asker do ˘gar.1\nTable 2: An example of minimal pair templates ex-\ntracted from mC4 and Wikipedia corpora. We ﬁrst ﬁnd\nsentences that contain speciﬁc nationalities, then replace\nthem with the [Nationality] placeholder. This enables a\nmore diverse and larger set of minimal pairs.\net al., 2022) and Wikipedia corpora. We ﬁrst seg-\nment sentences in the corpora by spaCy (Honnibal\net al., 2020). Then, we extract 10,000 sentences\nthat contain a selected nationality as a word in\nthe target corpora, separately (e.g., Arab in Ara-\nbic, Turk in Turkish, etc.). Then, we replace those\nnationalities with the [Nationality] placeholder to\ncreate templates. These templates include different\ncontexts and sentiments. Therefore, we use those\ndifferent templates to understand the effect of tem-\nplate mode (manual vs. corpus) and the source of\ncorpus (mC4 vs. Wikipedia) in LABDet. We in-\nvestigate whether ﬁnal results (i.e., quantiﬁcation\nof nationality bias in different PLMs) are robust\nto those changes in the templates. In Table 2, we\nprovide examples derived from corpus templates\nin six languages. These examples cover a broad\nrange of topics that we then use to diagnose posi-\ntive/negative sentiments about nationalities; manu-\nally designed templates would be narrower.\nNationality/Ethnicity: To demonstrate bias\nagainst nationalities, we select a diverse set of na-\ntionalities using a few criteria as guideline: large\nminority groups in countries where the language is\nwidely spoken and nationalities with which those\ncountries have alliances or geopolitical conﬂicts.\nTherefore, we target around 15 different nationali-\nties and ethnicities for each language for the bias\ndetection and quantiﬁcation part of our work. See\nFigure 2 for the selected nationalities and ethnici-\nties for each language.\n4 Bias Probing\nWe propose a robust and language-agnostic bias\nprobing method to quantify intrinsic bias in PLMs.\nTo extend and improve prior work that mainly fo-\ncuses on the English language or large language\nmodels with prompting, we propose bias probing\nwith sentiment surfacing.\nFirst, we train a classiﬁer such as SVM or MLP\non top of the frozen PLMs to ﬁnd a mapping be-\ntween contextual embeddings and sentiments. For\nthis, we utilize our sentiment training dataset cre-\nated via templates in order to prevent possible leak-\nage of nationality information to the classiﬁer. This\nhelps to extract positive and negative sentiment in-\nformation present in the pretrained language mod-\nels.\nIn the second step, we propose the sentiment\nsurfacing method by computing the relative sen-\ntiment change. Absolute sentiment values vary\n1 English Translations:\nArabic: It is said that he is of [Nationality] origin.\nDutch: Every [Nationality] person has the right to privacy.\nFrench: He is a [Nationality] poet and writer.\nGerman: Typical [Nationality].\nTurkish: Every [Nationality] person born as soldiers.\n12738\nacross models, languages, and contexts such as\ntemplates’ sentiment. In the relative approach, the\nplaceholders in two sentences with the same con-\ntext are ﬁlled with a nationality term and a neutral\nword, [MASK]. As illustrated in Figure 1, we com-\npare the relative sentiment change of the “This [Na-\ntionality] person is neutral.” template in Turkish\nwith the Turkish nationality and the [MASK] token.\nThis change shows that the “Turkish” nationality\nsurfaces positive sentiment with +0.16 score while\nthe “Greek” nationality surfaces negative sentiment\nwith -0.17 score. Then, we compare these changes\nbetween across different nationalities and templates\nand evaluate if there is a consistent negative bias\ntowards speciﬁc nationalities.\nTo surface sentiment change, we utilize the three\ndifferent sources of minimal pairs presented in §3.2:\none coming from template pairs we curated and two\ncoming from examples in mC4 (Raffel et al., 2022)\nand Wikipedia corpora for six languages. Addi-\ntionally, we also modify and use the previously\nproposed EEC templates (Kiritchenko and Moham-\nmad, 2018) for English to show robustness of our\napproach to different template sources.\n5 Results\nExperimental Setup: We evaluate LABDet using\nsix different monolingual language models, all in\nthe base size, with cased versions where available.\nArabic PLM: ArabicBERT (Safaya et al., 2020),\nGerman PLM: bert-base-german-cased2, English\nPLM: BERTbase (Devlin et al., 2019), French PLM:\nCamemBERT (Martin et al., 2020), Dutch PLM:\nBERTje (de Vries et al., 2019), and Turkish PLM:\nBERTurk (Schweter, 2020).\nFor sentiment training, we use SVM and MLP\nclassiﬁers. Next, we quantify bias using both a\ntemplate-based approach (ours and EEC -only for\nEnglish-) and a corpus-based approach (mC4 and\nWikipedia) via sentiment surfacing.\nWe propose three distinct analyses. First, we\ncompare the bias extracted via LABDet with the\nbias of English BERT base pretraining data. This\nevaluation helps to assess the effectiveness of our\nmethod and explore the connection between pre-\ntraining data to PLM behavior. In the second anal-\nysis, we show the relative sentiment change for\neach nationality across six languages. We conduct\na qualitative analysis of these results and exam-\nine their link to real-world bias within the histori-\n2https://www.deepset.ai/german-bert\nNationality\nContext\nPositivity\n# of\nSentences\nRelative\nSentiment\nSyrian 0.55 43k -0.20\nVietnamese 0.57 39k 0.03\nTurk 0.57 7k -0.48\nIsraeli 0.58 88k 0.04\nAfghan 0.60 24k 0.06\nIranian 0.61 56k -0.24\nJapanese 0.61 348k -0.09\nUkrainian 0.62 70k 0.03\nGerman 0.63 593k -0.21\nChinese 0.63 359k -0.25\nArab 0.64 106k -0.38\nEthiopian 0.64 17k -0.23\nPolish 0.65 148k -0.15\nPakistani 0.65 34k 0.09\nKorean 0.65 118k 0.15\nMexican 0.66 122k -0.12\nIndonesian 0.66 34k -0.04\nMoroccan 0.66 13k -0.02\nGreek 0.67 282k 0.00\nArmenian 0.67 41k 0.13\nAfrican 0.67 304k 0.14\nIrish 0.68 214k 0.02\nNigerian 0.68 25k 0.24\nAsian 0.69 188k -0.11\nArgentinian 0.70 5k 0.13\nIndian 0.70 417k 0.23\nItalian 0.71 285k 0.15\nFilipino 0.72 27k 0.22\nBrazilian 0.72 76k 0.40\nAmerican 0.72 1554k 0.08\nTable 3: “Context positivity” of a nationality in the\ntraining corpus is correlated with the trained model’s\nbias as measured by LABDet’s “relative sentiment” in\nEnglish (r = .59). Context positivity represents the\naverage positive sentiment score of sentences (i.e., con-\ntexts) including each nationality in the pretraining data,\nas evaluated by RoBERTabase ﬁnetuned on SST-2. Rel-\native sentiment is bias detection results obtained from\nLABDet (i.e., the PLM is assessed without accessing\nthe pretraining data). “# of Sentences” corresponds to\nthe number of sentences in the pretraining data.\ncal and political context. For the ﬁrst and second\nanalyses, we employ the SVM classiﬁer and our\ntemplate-based approach. However, to demonstrate\nthe robustness of our ﬁndings, we compare our re-\nsults from different approaches (template vs. cor-\npus), sources (mC4 vs. Wikipedia), and classiﬁers\n(SVM vs. MLP) in the third analysis. We use Pear-\nson’sr to measure the strength of the correlation\nbetween positive sentiment scores of nationalities\nobtained from different sources.\n5.1 Pretraining Data Bias\nWe demonstrate the effectiveness of LABDet for\ndetecting and quantifying bias by evaluating its per-\nformance on bias present in the pretraining data, a\n12739\nnovel contribution compared to prior work. This ap-\nproach allows us to obtain evidence for a causal re-\nlationship between pretraining data and model bias.\nSpeciﬁcally, we analyze the context positivityof\ndifferent nationalities in the English BERT pretrain-\ning data (i.e., English Wikipedia3 and BooksCorpus\n(Zhu et al., 2015)) by extracting all sentences con-\ntaining a nationality/ethnicity from a set. We then\nmeasure the context positivity by calculating the\naverage positive sentiment score of sentences for\neach nationality. We use RoBERTabase (Liu et al.,\n2019) ﬁnetuned with SST2 (Socher et al., 2013),\nfor sentiment analysis. We eliminate nationality\nbias in the sentiment analysis model by replacing\neach nationality with a mask token in the pretrain-\ning data. For a more conﬁdent analysis, we also\nincrease the number of nationalities from 15 to 30\nfor this part.\nWe present pretraining data bias and relative sen-\ntiment scores with our method for all nationalities\nin Table 3. We observe meaningful patterns in the\ncontext positivity scores of English BERT’s pre-\ntraining data. The connection with the English\nlanguage, historical developments, and content pro-\nduction can explain why some countries receive\nhigher context positivity score while others remain\non the lower end.\nFor instance, American has the highest context\npositivity, which could be attributed to the fact that\nEnglish is widely spoken in the country and the ma-\njority of the content in the pretraining data is pro-\nduced by Americans (Callahan and Herring, 2011).\nSimilarly, nationalities with large numbers of En-\nglish speakers like Indian and Nigerian (and by\nextension, Asian and African) as well as Irish also\nhave high context positivity scores, which could\nbe explained by the fact that these countries also\nproduce content in English. For example, most ac-\ntive editors of English Wikipedia4 are from United\nStates of America (21K), followed by United King-\ndom (6K) and India (4K). Indeed, among the 6\ncountries with highest context positivity in Table 3,\nall except one are among the top content producer\ncountries (Philippines 1K; Italy 950; Brazil 880).5\nOn the negative end of context positive score, we\nobserve that groups that have minority status in En-\nglish speaking countries or those that are associated\n3We analyze the 20/03/2018 dump of Wikipedia.\n4More than 75% of BERT’s pretraining data and over 90%\nof sentences containing nationality information are sourced\nfrom English Wikipedia.\n5https://stats.wikimedia.org/\nwith conﬂict and tension have lower context posi-\ntivity scores. Nationalities such as Syrian, Afghan,\nIsraeli, and Iranian are associated with conﬂict and\nviolence in the past decades. Similarly, Vietnamese\nhas one of the lowest context positivity scores most\nlikely reﬂecting the bulk of content related with\nthe Vietnam War. That Japanese and German have\nlower context positivity scores may seem puzzling\nat ﬁrst; yet, this is likely due to the historical con-\ntext of World War 2 and their portrayal in the pre-\ntraining data.\nTo verify the effectiveness of LABDet, we com-\npute the correlation between the context positivity\nscores of the pretraining data and the relative sen-\ntiment scores from our method using Pearson’s\nr. We observe a signiﬁcant correlation with an r\nscore of 0.59 (< 0.01 p-value). This indicates that\nLABDet is able to detect bias in PLMs with a high\ncorrelation to the bias present in the pretraining\ndata. We also observe signiﬁcant linear correla-\ntion using different approaches such as templates\nand corpus examples or SVM and MLP classiﬁers.\nThis shows LABDet’s robustness.\n5.2 Linkage to Real-world Biases\nWe compare the bias of six monolingual PLMs\nidentiﬁed by LABDet to real-world bias, consult-\ning the political science literature. We report the\nrelative sentiment score changes with reference\nto neutral examples where [Nationality] tags are\nreplaced by a mask token. Using the Wilcoxon\nsigned-rank test, we determine which nationalities\nhave consistently lower, higher, or similar predic-\ntions compared to the neutral examples. Our results\nare presented in Figure 2 with the relative senti-\nment change, bias direction (red: negative, black:\nno bias, green: positive), and conﬁdence intervals.\nOur ﬁndings indicate that all monolingual PLMs\nexhibit bias in favor of (i.e., green) and against\n(i.e., red) certain nationalities. In each model, we\nare able to get diverging relative sentiment scores\njust due to the change in the nationality mentioned\nacross our templates. Some nationalities consis-\ntently rank low on different PLMs. For instance,\nSyrian, Israeli, and Afghan rank on the lower end\nof most PLMs in our analysis. Similarly, Ukrainian\nranks fairly low in the European language PLMs.\nOn the other hand, some nationalities, such as\nAmerican and Indian rank consistently high in rela-\ntive sentiment scores. Apart from consistent rank-\nings across PLMs, there are three context-speciﬁc\n12740\n1.0\n 0.5\n 0.0 0.5 1.0\nArmenian\nGerman\nIsraeli\nAfrican\nNigerian\nMoroccan\nT urk\nArab\nUkrainian\nAfghan\nGreek\nAmerican\nIndian\nSyrian\nNationality\nThe Arabic PLM\n1.0\n 0.5\n 0.0 0.5 1.0\nAfghan\nMoroccan\nIndonesian\nSenegalese\nSyrian\nUkrainian\nT urk\nIsraeli\nArab\nAfrican\nIndian\nGerman\nPolish\nDutch\nGreek\nAmerican\nThe Dutch PLM\n1.0\n 0.5\n 0.0 0.5 1.0\nT urk\nArab\nChinese\nIranian\nGerman\nSyrian\nPolish\nMexican\nMoroccan\nGreek\nUkrainian\nIsraeli\nAmerican\nArmenian\nAfrican\nKorean\nIndian\nNigerian\nThe English PLM\n1.0\n 0.5\n 0.0 0.5 1.0\nRelative Sentiment\nUkrainian\nIsraeli\nGerman\nPolish\nSyrian\nT urk\nIndian\nChinese\nArmenian\nArab\nFrench\nAfrican\nAmerican\nGreek\nAfghan\nMoroccan\nSenegalese Nationality\nThe French PLM\n1.0\n 0.5\n 0.0 0.5 1.0\nRelative Sentiment\nUkrainian\nSyrian\nNigerian\nMoroccan\nGreek\nArab\nPolish\nAfrican\nAfghan\nAmerican\nIsraeli\nSerbian\nGerman\nIndian\nT urk\nFrench\nThe German PLM\n1.0\n 0.5\n 0.0 0.5 1.0\nRelative Sentiment\nArmenian\nAfrican\nIsraeli\nSyrian\nAfghan\nMoroccan\nGerman\nArab\nUkrainian\nAmerican\nGreek\nSerbian\nIndian\nT urk\nKorean\nThe Turkish PLM\nFigure 2: Relative sentiment score computed by LABDet for each nationality in six monolingual PLMs. Error bars\nindicate 95% conﬁdence interval. Colors in each PLM indicate negative (red), neutral (black), and positive (green)\ngroups. Negative and positive groups are statistically different from the neutral examples, i.e., from examples with\nthe [MASK] token (Wilcoxon signed-rank test, < 1e − 4 p-value).\npatterns we can observe from the relative sentiment\nscores:\nFirst and foremost, immigrant/minority popula-\ntions are important predictors of relative sentiment\nscores. This manifests in two opposing trends. On\nthe one hand, immigrant populations such as Syr-\nians in Turkey (Getmansky et al., 2018; Alakoc\net al., 2021) and the European countries (Poushter,\n2016; Secen, 2022), Ukrainians (Düvell, 2007)\nin European countries (note that the PLMs were\ntrained prior to the recent refugee wave), and In-\ndonesians/Moroccans in the Netherlands are known\nto be stigmatized (Solodoch, 2021). In accordance\nwith that, these nationalities have negative and\nsome of the lowest relative sentiment scores in the\ncorresponding Turkish, Dutch/German/French, and\nDutch PLMs, respectively. Similarly, minorities\nsuch as Arab, Syrian, Chinese, and Mexican rank\nlower in the English PLM.\nOn the other hand, while there is some evidence\nof bias against minorities, it seems that large mi-\nnority populations who reside in a country and/or\nproduce language content that might have made it\ninto the pretraining data may be helping to miti-\ngate some of the impacts of that bias. For example,\nMoroccan and Senegalese are associated with high\nrelative sentiment scores in the French PLM. The\nsame is true for Indian and Nigerian in the English\nPLM. This is despite the evidence of signiﬁcant\ndiscrimination against these minorities in their re-\nspective countries (Thijssen et al., 2021; Silberman\net al., 2007). The fact that English has ofﬁcial lan-\nguage status in India and Nigeria, and French in\nSenegal might also be a contributing factor.\nThese two trends regarding minorities are likely\ndriven by the history and size of minorities in these\ncountries. While there is bias against newer and rel-\natively smaller minorities, the older and larger mi-\nnorities who likely produce content receive higher\nrelative sentiment scores. Reﬂecting these trends,\nthe German PLM is a case in point. The nationali-\nties that rank among the lowest in the German PLM\nare either recent immigrants (Syrian) or smaller mi-\nnority groups (Ukrainian before the recent refugee\nwave, Moroccan, and Nigerian). On the opposite\nend, Turk ranks among the highest in the German\nPLM. As Turkish immigrants constitute the largest\nand one of the oldest immigrant populations in Ger-\nmany (Destatis, 2023), it is likely that the content\nthey produce leads to a positive bias toward Turks.\nSecond, negative bias seems to stem not just\nfrom attitudes toward minorities but also from\ngeopolitics and conﬂict. This might be in the form\nof geopolitical tension between a country where\nthe language of the model is predominantly spo-\nken and a country that we consider as a nation-\nality. This is consistent with the evidence in the\nliterature that geopolitical tensions stoke discrim-\n12741\ninatory attitudes (Saavedra, 2021). For example,\ntensions between the US and countries like Iran\nand China are likely driving lower scores for Ira-\nnian and Chinese in the English PLM (Lee, 2022;\nSadeghi, 2016). Similarly, regional tensions in the\nMiddle East are reﬂected in Arabic and Turkish\nPLMs where Israelis ranked among the lowest in\nterms of relative sentiment scores (Kosebalaban,\n2010; Robbins, 2022).\nGeopolitical tensions and a conﬂict environment\ncan also affect attitudes through negative news sto-\nries, even when there is no direct conﬂict between\ncountries. The fact that nationalities of conﬂict-\nridden countries such as Syrian, Israeli, and Afghan\nhave consistently negative sentiment scores shows\nhow political conﬂict may affect attitudes toward\ngroups in different parts of the world.\nFinally, historical afﬁnity and rivalry seem to\nplay a signiﬁcant role. Historical allies tend to have\nhigher relative sentiment scores, as seen in the case\nof Americans that are ranked high in Dutch, French,\nand Arabic PLMs. Yet, historical rivals tend to be\nranked rather negatively. The fact that German has\nnegative relative sentiment scores in the three other\nEuropean PLMs (Dutch, English, and French) is\nlikely related to Germany’s role in the world wars\n(Reeve, 2017). Similarly, Ukrainian consistently\nranking lower in the European PLMs might be a\nby-product of the Cold War context where Ukraine\nwas part of the USSR in rivalry with the Western\nBloc.\nExamining the relative sentiment scores in the\nTurkish PLM is also helpful to explore how his-\ntorical relations shape both negative and positive\nbiases. The historical negative attitudes between\nTurkey and Armenia (Phillips, David L., 2013)\nare reﬂected in the chart as Armenian is ranked\nlowest in the Turkish PLM. This sentiment likely\narises from the long-standing tension and conﬂict\nbetween the two nations going back to World War\nI. On the other hand, Korean has the most positive\nsentiment score in the Turkish PLM, a result that\nmay seem puzzling at ﬁrst, considering the geo-\ngraphical and political distance between the two\ncountries. However, digging into the historical con-\nnection between the two countries helps us explain\nthis score, as Turkey provided military support dur-\ning the Korean War (Lippe, 2000), which evolved\ninto an afﬁnity between the two nations that has\neven become a subject of popular culture through\nliterature and movies (Betul, 2017).\nAs the examination of these three patterns (i.e.,\nminorities, geopolitics, and historical relations)\ndemonstrates, the relative sentiment scores in Fig-\nure 2 highlight the importance of considering histor-\nical and contemporary real-world context in analyz-\ning the biases present in PLMs. Understanding the\nreal-world biases provides valuable insights into\nthe underlying factors that contribute to the biases\nin PLMs. Furthermore, this illustrates how cultural\nand historical ties between nations can have a last-\ning impact on language usage, which is evident\nin the pretraining data, subsequently reﬂected in\nPLMs.\n5.3 Robustness Evaluation\nRobustness is a crucial aspect of bias detection in\nPLMs, and many existing methods have limitations\nin this regard (Delobelle et al., 2022). We com-\npare robustness of LABDet to different setups by\nassessing the similarity in predictions via Pearson’s\nr (< 0.01 p-value) across languages.\nClassiﬁer: We compare SVM and MLP classiﬁers\non six language models and four template sources.\nFor each experiment, we observe a signiﬁcant cor-\nrelation with an average r of 0.94.\nTemplate Source: To demonstrate that our results\nare not speciﬁc to the design of our templates with\nneutral adjectives, we compare them to modiﬁed\nEEC (Kiritchenko and Mohammad, 2018) tem-\nplates with positive and negative adjectives (see\nTable 4). As EEC templates are in English, we\nonly compare English PLMs (but by extending to\nfour BERT and two RoBERTa variants) and two\ndifferent classiﬁers. We observe a signiﬁcant linear\ncorrelation for each setup with an average 0.83 r.\nTemplate vs. Corpus Examples: We compare our\ntemplate approach to mC4 examples. For PLMs in\nsix languages and two classiﬁers, we observe a sig-\nniﬁcant correlation with an average 0.89 r, except\nfor Arabic where there is a signiﬁcant difference\nbetween corpus examples and templates.\nCorpus Source: We investigate the importance\nof the corpus source by comparing Wikipedia and\nmC4 examples in six monolingual PLMs and two\nclassiﬁers. We observe signiﬁcant correlations for\neach combination, with an average 0.98 r.\n6 Conclusion\nOur bias probing method, LABDet, allows for the\nanalysis of intrinsic bias in monolingual PLMs and\nis easily adaptable to various languages. Through\n12742\nrigorous testing and qualitative analysis, we have\ndemonstrated the effectiveness of LABDet, such as\nﬁnding a strong correlation between bias in the pre-\ntraining data of English BERT and our results. We\nalso identiﬁed consistent patterns of bias towards\nminority groups or nationalities associated with\nconﬂict and tension across different languages. Ad-\nditionally, we found that large minority groups who\nproduce content in the target language tend to have\nmore positive sentiment, such as Turks in German\nPLMs and Senegalese/Moroccans in French PLMs.\nFinally, we show that our ﬁndings are statistically\nconsistent across template and corpus examples,\ndifferent sources, and languages.\nAcknowledgements\nThis work was partially funded by Deutsche\nForschungsgemeinschaft (project SCHU 2246/14-\n1) and the European Research Council (grant\n#740516).\nLimitations\nOne limitation of LABDet is related to the design\nof templates. It is possible that some templates\nmay generate ungrammatical examples in differ-\nent languages, particularly due to gender or article\nagreement. While we have shown that LABDet\nis robust to these changes through the use of dif-\nferent templates and corpus examples, it is still\nimportant to consider this limitation when utilizing\nLABDet. We recommend comparing the results\nobtained from different templates to gain a more\ncomprehensive understanding of the bias present\nin PLMs.\nEthics Statement\nThe ethical implications of social biases in mono-\nlingual PLMs are signiﬁcant as these models are\nincreasingly being used in a wide range of tasks\nsuch as text classiﬁcation (Wang et al., 2018), ex-\ntractive text summarization (Liu and Lapata, 2019),\nand non-autoregressive text generation (Su et al.,\n2021). The biases present in these models have\nthe potential to amplify existing societal inequali-\nties, particularly for marginalized groups. In this\nstudy, we propose LABDet, a robust method for\nquantifying and measuring bias in PLMs across\ndifferent languages. For nationality as a case study,\nwe aim to demonstrate the applicability and effec-\ntiveness of our method. However, it is important to\nacknowledge that our approach may not fully cap-\nture all forms of bias that may be present in PLMs.\nTherefore, it is beneﬁcial to use our method in con-\njunction with other techniques or metrics to gain a\nmore comprehensive understanding of the biases\npresent in PLMs and to make informed decisions\nabout the ethical use of these models.\nReferences\nEkin Akyurek, Tolga Bolukbasi, Frederick Liu, Bin-\nbin Xiong, Ian Tenney, Jacob Andreas, and Kelvin\nGuu. 2022. Towards tracing knowledge in language\nmodels back to the training data. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2022, pages 2429–2446, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nBurcu Pinar Alakoc, Gulay Ugur Goksel, and Alan\nZarychta. 2021. Political Discourse and Public Atti-\ntudes toward Syrian Refugees in Turkey.\nMaria Antoniak and David Mimno. 2021. Bad seeds:\nEvaluating lexical methods for bias measurement.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n1889–1904, Online. Association for Computational\nLinguistics.\nMuhammad Hilmi Asyroﬁ, Zhou Yang, Imam Nur Bani\nYusuf, Hong Jin Kang, Ferdian Thung, and David Lo.\n2022. Biasﬁnder: Metamorphic test generation to\nuncover bias for sentiment analysis systems. IEEE\nTransactions on Software Engineering, 48(12):5087–\n5101.\nSinem Betul. 2017. ’Ayla,’ a movie based on a heart-\nbreaking 65-year-old real-life story.\nSu Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu,\nRobert Sim, and Hanna Wallach. 2021. Stereotyping\nNorwegian salmon: An inventory of pitfalls in fair-\nness benchmark datasets. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 1004–1015, Online. Association\nfor Computational Linguistics.\nAylin Caliskan, Joanna J. Bryson, and Arvind\nNarayanan. 2017. Semantics derived automatically\nfrom language corpora contain human-like biases.\nScience, 356(6334):183–186.\nEwa S. Callahan and Susan C. Herring. 2011. Cultural\nbias in wikipedia content on famous persons. J. Am.\nSoc. Inf. Sci. Technol., 62(10):1899–1915.\nRodrigo Alejandro Chávez Mulsa and Gerasimos\nSpanakis. 2020. Evaluating bias in Dutch word em-\nbeddings. In Proceedings of the Second Workshop on\n12743\nGender Bias in Natural Language Processing, pages\n56–71, Barcelona, Spain (Online). Association for\nComputational Linguistics.\nWietse de Vries, Andreas van Cranenburgh, Arianna\nBisazza, Tommaso Caselli, Gertjan van Noord, and\nMalvina Nissim. 2019. Bertje: A dutch bert model.\nPieter Delobelle, Ewoenam Tokpo, Toon Calders, and\nBettina Berendt. 2022. Measuring fairness with bi-\nased rulers: A comparative study on bias metrics\nfor pre-trained language models. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1693–1706,\nSeattle, United States. Association for Computational\nLinguistics.\nDestatis. 2023. Foreign population by selected citizen-\nships and years.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nFranck Düvell. 2007. Ukraine–europe’s mexico. Cen-\ntral and East European migration. Country.\nAnna Getmansky, Tolga Sınmazdemir, and Thomas Zeit-\nzoff. 2018. Refugees, xenophobia, and domestic con-\nﬂict: Evidence from a survey experiment in Turkey.\n55(4):491–507.\nXiaochuang Han and Yulia Tsvetkov. 2022. Orca: In-\nterpreting prompted language models via locating\nsupporting data evidence in the ocean of pretraining\ndata.\nMatthew Honnibal, Ines Montani, Soﬁe Van Lan-\ndeghem, and Adriane Boyd. 2020. spaCy: Industrial-\nstrength Natural Language Processing in Python.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nSvetlana Kiritchenko and Saif Mohammad. 2018. Ex-\namining gender and race bias in two hundred senti-\nment analysis systems. In Proceedings of the Sev-\nenth Joint Conference on Lexical and Computational\nSemantics, pages 43–53, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nHasan Kosebalaban. 2010. The crisis in turkish-israeli\nrelations: What is its strategic signiﬁcance? Middle\nEast Policy, 17(3):36–50.\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black,\nand Yulia Tsvetkov. 2019. Measuring bias in con-\ntextualized word representations. In Proceedings of\nthe First Workshop on Gender Bias in Natural Lan-\nguage Processing, pages 166–172, Florence, Italy.\nAssociation for Computational Linguistics.\nMascha Kurpicz-Briki. 2020. Cultural differences in\nbias? origin and gender bias in pre-trained german\nand french word embeddings. Arbor-ciencia Pen-\nsamiento Y Cultura.\nJennifer Lee. 2022. When the geopolitical threat of\nchina stokes bias against asian americans. Pro-\nceedings of the National Academy of Sciences ,\n119(50):e2217950119.\nJohn M. Vander Lippe. 2000. Forgotten brigade of the\nforgotten war: Turkey’s participation in the Korean\nWar. Middle Eastern Studies, 36(1):92–102.\nYang Liu and Mirella Lapata. 2019. Text summariza-\ntion with pretrained encoders. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3730–3740, Hong Kong,\nChina. Association for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Suárez, Yoann Dupont, Laurent Romary, Éric\nde la Clergerie, Djamé Seddah, and Benoît Sagot.\n2020. CamemBERT: a tasty French language model.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 7203–\n7219, Online. Association for Computational Lin-\nguistics.\nChandler May, Alex Wang, Shikha Bordia, Samuel R.\nBowman, and Rachel Rudinger. 2019. On measuring\nsocial biases in sentence encoders. In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 622–628, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021.\nStereoSet: Measuring stereotypical bias in pretrained\nlanguage models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 5356–5371, Online. Association for\nComputational Linguistics.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R. Bowman. 2020. CrowS-pairs: A chal-\nlenge dataset for measuring social biases in masked\n12744\nlanguage models. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1953–1967, Online. As-\nsociation for Computational Linguistics.\nAurélie Névéol, Yoann Dupont, Julien Bezançon, and\nKarën Fort. 2022. French CrowS-pairs: Extending a\nchallenge dataset for measuring social bias in masked\nlanguage models to a language other than English.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 8521–8531, Dublin, Ireland.\nAssociation for Computational Linguistics.\nLaura Pérez-Mayos, Miguel Ballesteros, and Leo Wan-\nner. 2021. How much pretraining data do language\nmodels need to learn syntax? In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1571–1582, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nPhillips, David L. 2013. Diplomatic history: The\nTurkey-Armenia protocols.\nJacob Poushter. 2016. European opinions of the refugee\ncrisis in 5 charts.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2022. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. J. Mach. Learn. Res., 21(1).\nMichael Reeve. 2017. “the darkest town in england”:\nPatriotism and anti-german sentiment in hull, 1914–\n19. International Journal of Regional and Local\nHistory, 12(1):42–63.\nMichael Robbins. 2022. How Do MENA Citizens View\nNormalization With Israel?\nMartin Saavedra. 2021. Kenji or Kenneth? Pearl Harbor\nand Japanese-American assimilation. 185:602–624.\nSahar Sadeghi. 2016. The burden of geopolitical stigma:\nIranian immigrants and their adult children in the usa.\nJournal of International Migration and Integration,\n17(4):1109–1124.\nAli Safaya, Moutasem Abdullatif, and Deniz Yuret.\n2020. KUISAIL at SemEval-2020 task 12: BERT-\nCNN for offensive speech identiﬁcation in social me-\ndia. In Proceedings of the Fourteenth Workshop on\nSemantic Evaluation, pages 2054–2059, Barcelona\n(online). International Committee for Computational\nLinguistics.\nStefan Schweter. 2020. BERTurk - BERT models for\nTurkish.\nSefa Secen. 2022. Electoral competition dynamics and\nSyrian refugee discourses and policies in Germany\nand France. 0(0):1–25.\nRoxane Silberman, Richard Alba, and Irène Fournier.\n2007. Segmented assimilation in france? discrimina-\ntion in the labour market against the second genera-\ntion. Ethnic and Racial studies, 30(1):1–27.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631–1642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nOmer Solodoch. 2021. Do Sociotropic Concerns Mask\nPrejudice? Experimental Evidence on the Sources of\nPublic Opposition to Immigration. 69(4):1009–1032.\nYixuan Su, Deng Cai, Yan Wang, David Vandyke, Si-\nmon Baker, Piji Li, and Nigel Collier. 2021. Non-\nautoregressive text generation with pre-trained lan-\nguage models. In Proceedings of the 16th Conference\nof the European Chapter of the Association for Com-\nputational Linguistics: Main Volume, pages 234–243,\nOnline. Association for Computational Linguistics.\nYi Chern Tan and L. Elisa Celis. 2019. Assessing so-\ncial and intersectional biases in contextualized word\nrepresentations. In Advances in Neural Information\nProcessing Systems, volume 32. Curran Associates,\nInc.\nLex Thijssen, Bram Lancee, Susanne Veit, and Ruta Ye-\nmane. 2021. Discrimination against turkish minori-\nties in germany and the netherlands: ﬁeld experimen-\ntal evidence on the effect of diagnostic information\non labour market outcomes. Journal of Ethnic and\nMigration Studies, 47(6):1222–1239.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n353–355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. In Advances in Neural Information Pro-\ncessing Systems, volume 28. Curran Associates, Inc.\nYian Zhang, Alex Warstadt, Xiaocheng Li, and\nSamuel R. Bowman. 2021. When do you need bil-\nlions of words of pretraining data? In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 1112–1125, Online.\nAssociation for Computational Linguistics.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\n12745\nstory-like visual explanations by watching movies\nand reading books. In The IEEE International Con-\nference on Computer Vision (ICCV).\nA Appendix\n12746\nModiﬁed EEC Templates\nTemplates\n1. I talked to this [Nationality] person yesterday.\n2. This [Nationality] person found himself in a [Situation] situation.\n3. This [Nationality] person made me feel [State].\n4. The conversation with this [Nationality] person was [Situation].\n5. This [Nationality] person goes to the school in our neighborhood.\n6. This [Nationality] person told us all about the recent [Situation] events.\n7. I saw this [Nationality] person in the market.\n8. I made this [Nationality] person feel [State].\n9. The [Nationality] feels [State].\n10. This [Nationality] person has two children.\n11. The situation makes the [Nationality] feel [State].\nState Words angry, anxious, ecstatic, depressed, annoyed, discouraged, excited, devastated, enraged, fearful,\nglad, disappointed, furious, scared, happy, miserable, irritated, terriﬁed, relieved, sad\nSituation Words annoying, dreadful, amazing, depressing, displeasing, horrible, funny, gloomy, irritating, shocking,\ngreat, grim, outrageous, terrifying, hilarious, heartbreaking, vexing, threatening, wonderful, serious\nTable 4: Modiﬁed EEC (Kiritchenko and Mohammad, 2018) templates for the bias detection of PLMs.\n12747"
}