{
  "title": "Woodpecker: Hallucination Correction for Multimodal Large Language Models",
  "url": "https://openalex.org/W4387947626",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4327823733",
      "name": "Yin, Shukang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287436977",
      "name": "Fu, Chaoyou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2372377423",
      "name": "Zhao Sirui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2107651531",
      "name": "Xu Tong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1935192010",
      "name": "Wang Hao",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Sui, Dianbo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2752777602",
      "name": "Shen, Yunhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1977048908",
      "name": "Li Ke",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2113166830",
      "name": "Sun Xing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A167579018",
      "name": "Chen, Enhong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4367367040",
    "https://openalex.org/W4386907022",
    "https://openalex.org/W2898875342",
    "https://openalex.org/W4366566341",
    "https://openalex.org/W4353113046",
    "https://openalex.org/W4226352076",
    "https://openalex.org/W4372273323",
    "https://openalex.org/W4385958573",
    "https://openalex.org/W4386302587",
    "https://openalex.org/W4323717348",
    "https://openalex.org/W4366850747",
    "https://openalex.org/W4310486995",
    "https://openalex.org/W4386066015",
    "https://openalex.org/W4324321291",
    "https://openalex.org/W3169283738",
    "https://openalex.org/W4312420092",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W4378942772",
    "https://openalex.org/W4324128075",
    "https://openalex.org/W4309805219",
    "https://openalex.org/W4290771878",
    "https://openalex.org/W4382619388",
    "https://openalex.org/W4382323090",
    "https://openalex.org/W4318718936",
    "https://openalex.org/W4385569922",
    "https://openalex.org/W4382132560",
    "https://openalex.org/W4375869762",
    "https://openalex.org/W4386075985",
    "https://openalex.org/W4393160204",
    "https://openalex.org/W4389520252",
    "https://openalex.org/W4312910992",
    "https://openalex.org/W4366330503",
    "https://openalex.org/W3169064633",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W4386065691",
    "https://openalex.org/W4386185344",
    "https://openalex.org/W4389523832",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4225933709",
    "https://openalex.org/W4382142077",
    "https://openalex.org/W4386185600",
    "https://openalex.org/W4361866031",
    "https://openalex.org/W4322718421"
  ],
  "abstract": "Hallucination is a big shadow hanging over the rapidly evolving Multimodal Large Language Models (MLLMs), referring to the phenomenon that the generated text is inconsistent with the image content. In order to mitigate hallucinations, existing studies mainly resort to an instruction-tuning manner that requires retraining the models with specific data. In this paper, we pave a different way, introducing a training-free method named Woodpecker. Like a woodpecker heals trees, it picks out and corrects hallucinations from the generated text. Concretely, Woodpecker consists of five stages: key concept extraction, question formulation, visual knowledge validation, visual claim generation, and hallucination correction. Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages. We evaluate Woodpecker both quantitatively and qualitatively and show the huge potential of this new paradigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement in accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released at https://github.com/BradyFU/Woodpecker.",
  "full_text": "Woodpecker: Hallucination Correction for\nMultimodal Large Language Models\nShukang Yin1*, Chaoyou Fu2∗‡†, Sirui Zhao1∗‡, Tong Xu1‡, Hao Wang1\nDianbo Sui, Yunhang Shen2, Ke Li2, Xing Sun2, Enhong Chen1‡\n1School of Data Science, USTC & State Key Laboratory of Cognitive Intelligence\n2Tencent YouTu Lab\n{xjtupanda,sirui}@mail.ustc.edu.cn, {tongxu,cheneh}@ustc.edu.cn\n{bradyfu24}@gmail.com\nAbstract\nHallucination is a big shadow hanging over the rapidly\nevolving Multimodal Large Language Models (MLLMs), re-\nferring to the phenomenon that the generated text is incon-\nsistent with the image content. In order to mitigate hallucina-\ntions, existing studies mainly resort to an instruction-tuning\nmanner that requires retraining the models with specific\ndata. In this paper, we pave a different way, introducing\na training-free method named Woodpecker. Like a wood-\npecker heals trees, it picks out and corrects hallucinations\nfrom the generated text. Concretely, Woodpecker consists\nof five stages: key concept extraction, question formulation,\nvisual knowledge validation, visual claim generation, and\nhallucination correction. Implemented in a post-remedy man-\nner, Woodpecker can easily serve different MLLMs, while\nbeing interpretable by accessing intermediate outputs of\nthe five stages. We evaluate Woodpecker both quantitatively\nand qualitatively and show the huge potential of this new\nparadigm. On the POPE benchmark, our method obtains a\n30.66%/24.33% improvement in accuracy over the baseline\nMiniGPT-4/mPLUG-Owl. The source code is released at\nhttps://github.com/BradyFU/Woodpecker.\n1. Introduction\nMultimodal Large Language Models (MLLMs) [ 41] are\nnow flourishing in the research community, working towards\nArtificial General Intelligence (AGI). By exploiting powerful\nLarge Language Models (LLMs), researchers align foreign\nmodalities like vision with language, and develop MLLMs\nwith various exciting capabilities [1, 21, 40, 44, 47], such as\n*Equal contribution.\n†Project leader.\n‡Corresponding author.\nPleasedescribethisimageindetail.\nTheimagefeaturesacute,reddogrunningacrossagrassyfield…Aroundthedog,thereareseveralotherdogsvisibleinthebackground…\nFigure 1. Illustration of hallucinations in MLLMs. Given an image,\nan MLLM outputs a corresponding response with both object-level\nand attribute-level hallucinations.\nfully describe the contents of a given image.\nHowever, as strong as these MLLMs are, they sometimes\noutput descriptions that are inconsistent with the input image.\nIt is called hallucination and has been found prevalent in\nMLLMs [20]. As exemplified by Fig. 1, the MLLM claims\nnon-existent objects and fails to describe the attribute of the\nobject in the image accurately, which are categorized by us\nas object-level and attribute-level hallucinations, respectively.\nIt is obvious that these hallucinations are huge obstacles to\nthe practical application of MLLMs.\nIn order to mitigate the hallucinations, existing works usu-\nally explore an instruction-tuning way [20, 33]. A common\nkey observation is that MLLMs tend to hallucinate when gen-\nerating longer text [20], which results in different problem-\nsolving strategies. For example, LRV-Instruction [20] takes\nan intuitive approach by limiting the text length of instruc-\n1\narXiv:2310.16045v2  [cs.CV]  11 Dec 2024\nDescribe this image.Theimagefeaturesayoungboyridingabicycleonadirtroad.Thereareseveralotherpeopleinthescene.Afewofthemarealsoridingbicycles,addingtothelivelyatmosphere.\nThe image features a boy([0.196,0.255,0.4,0.694]) riding a bicycle([0.179,0.372,0.404,0.841]) on a road([0.002,0.002,0.997,0.997]). There are no other people or bicycles in the image.\nNo, there is no bowl visible in the image.\nYes, there are bowls ([0.225,0.957,0.303,1.0]; [0.135,0.981,0.248,1.0]) in the image.\nIs there a bowl in the image?\nHow many people in the image?\nThere are 38 people([0.14,0.631,0.271,0.996]…) in the image.\nThere are 15 people in the image.\nAre there any orange beans in the image?\nNo, there are no orange beans in the image.There are green beans([0.312,0.783,0.682,0.968]) in the image.\nYes, there are orange beans in the image.\nFigure 2. Examples of our framework for hallucination correction. Given a response of an MLLM, Woodpecker corrects the hallucinated\nparts and incorporates grounding information for ease of verification.\ntion data. As a consequence, the tuned model usually gen-\nerates less hallucinated but also less detailed descriptions.\nVIGC [33] takes a multi-step generation scheme and itera-\ntively updates the visual features with the textual context,\nwhich relieves hallucinations via sacrificing generative effi-\nciency. Moreover, both of the two methods are instruction-\ntuning-based and thus are data- and computation-intensive.\nTo break the limitation, we take a different strategy that\ncan directly correct the hallucinations without retraining. As\nillustrated in Fig. 2, given a text generated by MLLMs as\nwell as the input image, our training-free framework Wood-\npecker corrects the text elaborately, and meanwhile, pro-\nvides the corresponding evidence, i.e., the bounding boxes.\nIt adds interpretability and reliability beyond the black-box\nMLLMs, providing convenient visual fact-checking. Con-\ncretely, our framework performs correction after a thorough\ndiagnosis, which incorporates a total of five stages: (1) Key\nconcept extraction identifies the main objects mentioned\nin the generated sentences; (2) Question formulation asks\nquestions around the extracted objects, such as their num-\nber and attributes; (3) Visual knowledge validation answers\nthe formulated questions via expert models. For example,\na visual perception model can be used to determine the ob-\nject number; (4) Visual claim generation converts the above\nQuestion-Answer (QA) pairs into a visual knowledge base,\nwhich consists of the object-level and attribute-level claims\nabout the input image; (5) Hallucination correction modi-\nfies the hallucinations and adds the corresponding evidence\n2\nunder the guidance of the visual knowledge base. It is worth\nnoting that each step in the pipeline is clear and transparent,\nwhich offers good interpretability.\nWe evaluate the effectiveness of our method through\ncomprehensive quantitative and qualitative experiments on\nthe POPE [18], MME [6], and LLaV A-QA90 [21] datasets.\nThe results and associated analyses indicate the superiority\nof this new paradigm. For instance, on the POPE bench-\nmark, our method largely boosts the accuracy of the baseline\nMiniGPT-4 [47]/mPLUG-Owl [40] from 54.67%/62% to\n85.33%/86.33%.\nIn summary, the main contributions are as follows:\n• We propose a training-free framework named Wood-\npecker to correct the hallucinations for MLLMs. To the\nbest of our knowledge, we are the first to apply a correc-\ntive manner to tackle the visual hallucination problem.\n• Our framework is designed in a way that each step is clear\nand transparent, thus providing good interpretability.\n• We comprehensively evaluate the effectiveness of our\nmethod, and the large improvements demonstrate its\ngreat potential in hallucination correction.\n2. Related Work\n2.1. Hallucination in MLLM\nRecently, there has been increasing attention on the hallu-\ncination phenomenon of MLLMs. This is mainly because\nthe issue directly affects the reliability of MLLMs. Current\nresearches on the hallucination of MLLMs mainly focus on\ntwo aspects, i.e., the evaluation/detection [ 8, 18, 34] and\nmitigation [20, 23, 33]. The previous line of work generally\neither trains a classification model to discriminate halluci-\nnation [8] or checks the output text against ground-truth\nanswers to decide if the hallucination happens [18, 34].\nFor hallucination mitigation, previous works focus on op-\ntimizing the data collection process and the training scheme.\nLRV-Instruction [20] composes negative instances to refrain\nfrom over-confidence. Moreover, the text length of Ground-\nTruth answers is strictly controlled, based on the observation\nthat shorter responses are less likely to be hallucinated. Sim-\nilarly, VIGC [ 33] takes an iterative process, where short\nanswers are generated and concatenated each time. In this\nway, it tries to ensure accuracy without compromising details.\nWhile previous works try to develop MLLMs with fewer hal-\nlucinations, our main objective is to refine the responses of\nMLLMs by modifying the hallucinated parts. Specifically,\nwe design a training-free framework that incorporates off-\nthe-shelf models. This exempts the complexity of collecting\ninstruction data and resource-intensive training. As a result,\nour framework can be easily integrated with various MLLMs,\nserving as a general plug-and-play module.\n2.2. Knowledge-augmented LLM\nSince LLMs are limited to the inherent knowledge gained\nfrom pretraining, various works have been dedicated to aug-\nmenting LLMs with external knowledge sourced from a pre-\ndefined knowledge base [3, 5, 14, 28] or the internet [29, 31].\nAs a natural extension of this idea, recently, researchers have\nexplored using knowledge as evidence to alleviate factual\nhallucinations in LLMs [10, 27]. Specifically, these works\nuse relevant knowledge as background information to refine\na possibly false input claim, resulting in a higher factuality\nof the response. Our methods share in common with the idea\nthat we use information relevant to the given image to cor-\nrect potentially wrong claims. However, it is non-trivial to\ntransfer the idea to the vision-language field. This is because\nthe language-only counterpart usually deals with text only\nand acquires relevant knowledge through retrieval, while\nit is inappropriate to do so for image-text pairs. Moreover,\nknowledge-augmented LLMs pay more attention to alleviat-\ning factual fallacies, while we lay more stress on mitigating\nvisual hallucinations. Corresponding to the key differences,\nin this work, we devise a strategy to construct a structured\nvisual knowledge base conditioned on the image and the\nquery. We also explore how to address both object-level and\nattribute-level hallucinations in an organized way, as we will\nillustrate later.\n2.3. LLM-aided Visual Reasoning\nAccording to the taxonomy in the survey [ 41], our pro-\nposed framework is closely related to the LLM-Aided Vi-\nsual Reasoning model [2, 7, 13]. The main idea is that we\ncan leverage the strong reasoning and instruction-following\ncapabilities of LLMs to help fulfill vision or multimodal\ntasks. Typical roles that LLMs play include the task dis-\npatcher [9, 24, 30, 38], the reasoner [37, 39, 42, 46], or the\nlanguage refiner [ 35, 43, 45, 48]. In this work, we utilize\nthe strong reasoning and language proficiencies of LLMs\nto help the processes of key concept extraction, question\nformulation, and hallucination correction.\n3. Method\nOur objective is to diagnose and correct the hallucinations\nin the response generated by MLLMs. The key challenges\nlie in locating the hallucinations and determining the facts,\nwhich can be organized in a structured way for final cor-\nrection. To this end, we break down the whole process into\nfive subtasks: key concept extraction (Sec. 3.1), question for-\nmulation (Sec. 3.2), visual knowledge validation (Sec. 3.3),\nvisual claim generation (Sec. 3.4), and hallucination correc-\ntion (Sec. 3.5). We will illustrate each step in sequence later.\nAn overview of our framework is depicted in Fig. 3.\n3\nThescenetakesplaceinanurbanarea,whereabicycleisparkednexttoatrashbin.Thebicycleissituatedontherightsideoftheimage,withitsfrontwheelsslightlyturned.Thereareseveralotherpeopleinthisarea,walkingandstandingaroundatvariousdistancesfromthetrashbin.\nObject-level:There is 1 bicycle. bicycle 1: [0.467, 0.555, 0.717, 0.746]There are 2 trash bins.trash bin 1: [0.145, 0.498, 0.321, 0.728] ;trash bin 2: [0.319, 0.497, 0.483, 0.729]There is no person.Attribute-level:trash bin 1: [0.145, 0.498, 0.321, 0.728]A recycling bin is next to the trash bin.trash bin 2: [0.319, 0.497, 0.483, 0.729]A recycling bin is next to the trash bin. A bicycle is next to the trash bin.The trash bin is located in front of the building.\nThescenetakesplaceinanurbanarea,whereabicycle([0.467,0.555,0.717,0.746])isparkednexttotwotrashbins([0.145,0.498,0.321,0.728];[0.319,0.497,0.483,0.729]).Thebicycleissituatedontherightsideoftheimage,withitsfrontwheelsslightlyturned.Therearenopeopleinthisarea.\nQuery:Describethisimage.\nbicycle; trash bin; person\nHow many bicycles are there in the image?...What is next to the trash bin?Where is the trash bin located?\nbicycle: [0.467, 0.555, 0.717, 0.746]...a recycling binin front of the building\nKey Concept Extraction\nQuestionFormulation\nVisual Knowledge Validation\nHallucination Correction\nVisual Claim Generation\nMLLMOutput\nFigure 3. Framework of Woodpecker. Given an image and a query, an MLLM outputs the corresponding response. Through the four steps,\nincluding key concept extraction, question formulation, visual knowledge validation, and visual claim generation, we get a visual knowledge\nbase specific to the image and the original response. In the final step, the hallucinations in the response are corrected with the bounding\nboxes as evidence.\n3.1. Key Concept Extraction\nSince descriptions usually revolve around key concepts, the\nfirst step is to extract them from the generated sentence.\nTo this end, we identify the main objects mentioned in\nthe sentence, which are the ones most likely to exit visual\nhallucinations. For instance, given a sentence “ The man\nis wearing a black hat.”, the objects “man” and\n“hat” are extracted, and will serve as the center for diagnosis\nin the following steps. In light of the strong generalization\nability and rich world knowledge of LLMs, we prompt an\nLLM to fulfill this task.\nThe template for key concept extraction is listed in Ap-\npendix A.1, which comprises a system message and a format-\nted prompt. The former sets up the basic context for the LLM,\nwhile the latter starts with some detailed descriptions of the\ntask and some requirements, followed by several in-context\nexamples and inputs. The in-context examples are provided\nso that the LLM could better understand the requirements in\nterms of the task.\n3.2. Question Formulation\nAfter acquiring the key concepts, we ask a series of\nquestions around them to make the hallucination diag-\nnosis. Our questions are directed at both object-level\nand attribute-level hallucinations. For the former, we ask,\n“Is there any {object} in the image? How\nmany are there?”, where “ {object}” is the key\nconcept extracted earlier. For the latter, various questions\ninvolving the attributes of objects can be formulated, such\nas “ What is {object} doing?”, “ Is {object1\non the right side of {object2}?”, and “What\ncolor is the {object}?”, where “ {object1}”\nand “{object2}” are different key concepts.\nIn fact, object-level questions can be directly validated\nthrough perceiving images, while attribute-level questions\nare much more diverse and dependent on the context. To\nfacilitate such free-form formulation of questions, we prompt\nan LLM with some in-context examples so that meaningful\nquestions are raised. The prompt is listed in Appendix A.2.\n3.3. Visual Knowledge Validation\nThis step is responsible for solving the above two types of\nquestions. For the object-level questions, the crux is deter-\nmining the existence and the count of a certain object. In\nlight of the strong perception capabilities of vision foun-\ndation models [12, 17, 25, 32, 36], we employ an open-set\nobject detector as the solver [ 22]. For attribute-level ques-\ntions, we apply a pre-trained VQA model [16] to answer the\nquestions conditioned on the image. Compared with main-\nstream MLLMs, the VQA model tends to generate shorter\nanswers but also with fewer hallucinations and thus can be a\nreasonable choice.\n3.4. Visual Claim Generation\nAfter questions are raised and answered, we combine QA\npairs into visual claims and organize them into a visual\nknowledge basefor reference in the following step. The\nvisual knowledge base is structured by:\n• Object-level claims: This part of the information mainly\nplays a role in mitigating object-level hallucinations. We\n4\ninclude information about object counts of key concepts\nextracted from the sentences (Sec. 3.1). For existing\nobjects, we add a claim as “ There are {counts}\n{name}.”, where “{counts}” and “{name}” are the\ncounts and the name of a certain kind of object. We\nuse a similar template, “There is no {name}”, for\nnonexistent objects. The counting information comes\nfrom the open-set object detection in the previous step.\n• Attribute-level claims: We include attribute information\nspecific to each object in order to alleviate attribute-\nlevel hallucinations. Typical attributes include positions,\ncolors, actions, etc. For this part, we adopt a QA-to-\nClaim model [10] to merge questions and answers into\nclaims. In order to cope with cases involving multi-\nple objects or the relationship between the foreground\nobjects and the background, more global information\nis needed. Thus, we also include claims that involve\nthe interaction between different objects or the objects\nand the background, such as “ The cat is lying\nnext to the dog.”.\n3.5. Hallucination Correction\nGuided by the visual claims, an LLM can act as a corrector\nand modify the hallucinations in the generated responses.\nSpecifically, after combining the visual knowledge base with\nthe original responses into a prompt, we instruct an LLM to\ncorrect the responses and output the refined ones. For better\ninterpretability, we explicitly instruct the LLM to attach\nbounding boxes right behind expressions when referring to\nobjects. This design facilitates the correspondence between\nthe mentioned entities in the responses and object instances\nin the image, which provides convenient access to check the\nreliability of the output. The prompt template for correction\nis included in Appendix A.3\n4. Experiment\n4.1. Experimental Settings\nDataset. POPE [18] is dedicated to evaluating hallucina-\ntions of MLLMs. It contains the settings of random, popular,\nand adversarial sampling, which mainly differ in the way\nnegative samples are constructed. For the random setting,\nthe objects not presented in the image are sampled randomly,\nwhile for the popular setting, non-existent objects are sam-\npled from a pool of objects with the highest frequencies. For\nthe adversarial setting, objects that most frequently co-occur\nbut do not exist in the image are sampled.\nIn terms of the sampling setting, we sample 50 images and\nbuild 6 questions for each image. The ratio between positive\nand negative samples is balanced, namely 50% vs 50%. This\nsetup transforms object annotations into a series of “Yes-\nor-No” questions and focuses on evaluating the object-level\nhallucination, and more specifically, the existence aspect.\nThereby, MLLMs are prompted to answer if an object exists\nin the image or not. Accordingly, evaluation metrics include\naccuracy, precision, recall, and f1-score.\nMME [6] is a comprehensive benchmark designed to\nevaluate the performance of MLLMs in various aspects. It\nencompasses ten subtasks for the perception ability and four\nsubtasks for the cognition ability, respectively. In this paper,\nwe repurpose the dataset and select existence and count sub-\nsets to measure the object-level hallucination. The position\nand color subsets are used to measure the attribute-level\nhallucination. Similar to the setup of POPE, each subset is\ncomposed of “Yes-or-No” questions. We report the score,\nnamely the sum of accuracy and accuracy+ following the\nofficial implementation [6], in which a higher score indicates\nbetter performance and fewer hallucinations.\nLLaV A-QA90[21] is also used to evaluate MLLMs.\nSpecifically, we sample 10 description-type queries\nthat are paraphrased in various forms to instruct an\nMLLM to describe an image, such as “ Describe the\nfollowing image.” and “ What is the photo\nabout?”. LLaV A-QA90 uses images from COCO [19] and\nadopts text-only GPT-4 [26] to compose queries and refer-\nence answers. We discard the reference answers, directly\nfeed the image to GPT-4V [ 26], and prompt it to rate the\nresponses regarding our designed two dimensions, i.e., ac-\ncuracy and detailedness. The prompt template is available\nin Appendix A.4.\nBaselines. We choose mainstream MLLMs as our base-\nline models, including mPLUG-Owl [ 40], LLaV A [21],\nMiniGPT-4 [47], and Otter [15]. These four MLLMs follow a\n“vision encoder-interface-language model” architecture [41]\nand are trained on image-text pairs. Specifically, LLaV A and\nMiniGPT-4 adopt a simple projection layer to align mul-\ntimodal embeddings. mPLUG-Owl uses a Q-Former [ 16]\nto compress visual features into a fixed number of tokens,\nwhich can be concatenated with the language embeddings.\nOtter adopts a similar Perceiver [11] resampler to obtain the\ntoken compression.\nImplementation Details. Our pipeline is training-free and\ncomprises three pre-trained models apart from the MLLM\nto be corrected. We choose the LLM, GPT-3.5-turbo [ 4],\nto fulfill the subtasks of key concept extraction, question\nformulation, and hallucination correction. For open-set ob-\nject detection, we use Grounding DINO [22] to extract ob-\nject counting information with default detection thresholds.\nMoreover, we utilize BLIP-2-FlanT5XXL [16] as the VQA\nmodel to answer the attribute-related questions conditioned\non the input image.\nFor the “Yes-or-No” questions, we find that the\ninstruction-following ability of some MLLMs is somewhat\nweak, often outputting irrelevant texts such as pure emojis\n5\nSetting Method w/Ours Accuracy Precision Recall F1-Score Yes Rate\nRandom\nLLaV A [21] ✗ 86.00 87.50 84.00 85.71 48.00\n! 87.67 95.93 78.67 86.45 41.00\nMiniGPT-4 [47] ✗ 54.67 57.78 34.67 43.33 30.00\n! 85.33 92.06 77.33 84.06 42.00\nmPLUG-Owl [40] ✗ 62.00 57.26 94.67 71.36 82.67\n! 86.33 93.60 78.00 85.09 41.67\nOtter [15] ✗ 72.33 66.18 91.33 76.75 69.00\n! 86.67 93.65 78.67 85.51 42.00\nPopular\nLLaV A [21] ✗ 76.67 72.22 86.67 78.79 60.00\n! 80.67 83.82 76.00 79.72 45.33\nMiniGPT-4 [47] ✗ 56.67 58.77 44.67 50.76 38.00\n! 82.33 85.40 78.00 81.53 45.67\nmPLUG-Owl [40] ✗ 57.33 54.20 94.67 68.93 87.33\n! 83.00 84.14 81.33 82.71 48.33\nOtter [15] ✗ 67.33 61.71 91.33 73.66 74.00\n! 84.33 88.15 79.33 83.51 45.00\nAdversarial\nLLaV A [21] ✗ 73.33 69.02 84.67 76.05 61.33\n! 80.67 82.86 77.33 80.00 46.67\nMiniGPT-4 [47] ✗ 55.00 56.88 41.33 47.88 36.33\n! 82.33 83.92 80.00 81.91 47.67\nmPLUG-Owl [40] ✗ 56.33 53.51 96.67 68.88 90.33\n! 81.00 82.07 79.33 80.68 48.33\nOtter [15] ✗ 66.67 61.16 91.33 73.26 74.67\n! 83.00 85.61 79.33 82.35 46.33\nTable 1. Results on POPE. w/Ours denotes MLLM responses corrected by our proposed Woodpecker. The best and second-to-best\nperformances within each setting are bolded and underlined, respectively.\nor URLs. This is an obstacle to our correction process. Be-\nsides, some MLLMs only output a single “Yes” or “No”,\nwhich also poses a challenge to the correction. To deal\nwith these issues, we design two simple measures: (1) we\nfirst extract keywords, i.e., “Yes” and “No” from the re-\nsponses as the answers, then combine the questions with\nthe answers into more specific claims. For example, given a\nquestion, “Is there a dog in the image?” and a\nmodel answer, “Yes”, we compose a more specific answer as\n“Yes, there is a dog in the image.”; (2) we\nadditionally feed the questions to the LLM in the correc-\ntion process so that the LLM can have a better grasp of the\ncontext and task requirements.\n4.2. Experimental Results\nResults on POPE. The results on POPE under the random,\npopular, and adversarial settings are summarized in Tab. 1.\nIt can be seen that, in the random setting, MiniGPT-4 is\nrelatively weak in perception capabilities, specifically in\njudging the existence of objects. The f1-score for MiniGPT-\n4 is only 43.33%, while other baselines are all over 70%. In\naddition, mPLUG-Owl and Otter tend to be overconfident,\nas reflected by a high Yes Rate. Meanwhile, the high recall\nand the low precision result in a relatively low f1-score. For\nall of the baselines, Woodpecker achieves consistent gains in\nmost metrics, which indicates that our method has the ability\nto effectively correct object-level hallucinations. Specifically,\nWoodpecker obtains a relative gain of 30.66% for MiniGPT-\n4 and 24.33% for mPLUG-Owl in terms of accuracy.\nIn the more challenging popular and adversarial settings,\nMLLMs show performance degradation to different extents,\nmore prominent in relatively stronger baselines, such as\nLLaV A. Specifically, compared with the random setting,\nLLaV A shows a 9.33% and 12.67% accuracy degradation in\nthe popular and the adversarial settings, respectively. This\ntendency suggests that MLLMs may incorrectly fit some\ndata characteristics in the training corpus. For example, the\ndecline in the popular setting may stem from the long-tailed\ndata distribution [18]. In contrast, equipped with a robust\nexpert vision model, our correction method shows strong sta-\nbility, making obvious improvements in various metrics for\nthe baselines, where all accuracies exceed 80%. Particularly,\nour Woodpecker largely boosts the accuracy of mPLUG-Owl\nfrom 56.33% to 81% in the adversarial setting.\nResults on MME. Compared with POPE, the experiments\non MME is more well-rounded since it covers not only\nobject-level but also attribute-level hallucination evaluation.\n6\nMethod w/Ours Object-level Attribute-levelTotalExistence Count Position Color\nLLaV A [21] ✗ 195.00 95.00 53.33 78.33 421.67\n! 195.00 160.00 55.00 155.00565.00\nMiniGPT-4 [47] ✗ 100.00 61.67 53.33 65.00 280.00\n! 183.33 163.33 60.00 121.67 528.33\nmPLUG-Owl [40]✗ 101.67 73.33 58.33 66.67 300.00\n! 200.00 131.67 78.33 145.00 555.00\nOtter [15] ✗ 185.00 95.00 50.00 118.33 448.33\n! 195.00 160.00 51.67 165.00 571.67\nTable 2. Results on MME. w/Ours denotes MLLM responses cor-\nrected by our proposed Woodpecker. The performance is measured\nby scores, where the best and second-to-best for each partition are\nbolded and underlined, respectively.\nThe corresponding results are listed in Tab. 2. We can see\nthat, for object-level evaluation, LLaV A and Otter excel in\nthe existence aspect, which is also verified in the POPE eval-\nuation, while they relatively lag in answering harder count\nqueries. In this case, our correction method is particularly\neffective and contributes a large score gain, ranging from\n+65 over LLaV A to +101.66 over MiniGPT-4. With regard to\nattribute-level evaluation, baseline MLLMs tend to achieve\npoorer results, which suggests that they are more prone to\nattribute-level hallucinations. For example, MiniGPT-4 only\nachieves a score of 65 in the color split, and mPLUG-Owl\nmerely attains 66.67. After introducing our correction frame-\nwork, these MLLMs make consistent and remarkable gains,\nwhere the score of mPLUG-Owl goes up 78.33. In contrast,\nthe improvements in position are relatively small, which may\nbe caused by two factors: (1) the relatively weak ability of\nthe VQA model BLIP-2 in position reasoning; (2) LLM may\nnot comprehend the given bounding boxes well enough to\nderive position relationships by itself.\nMethod w/Ours Accuracy Detailedness\nLLaV A [21] ✗ 7.1 7.1\n! 7.8 8.6\nMiniGPT-4 [47] ✗ 7.0 6.4\n! 8.2 8.8\nmPLUG-Owl [40] ✗ 5.4 6.4\n! 5.7 6.4\nOtter [15] ✗ 7.0 6.7\n! 8.5 8.8\nTable 3. Results of GPT-4V-aided evaluation. The accuracy and de-\ntailedness metrics are on a scale of 10, and a higher score indicates\nthe better performance.\nPrompt:Describethefollowingimage.\n Corrected response\nEvaluation Results:Accuracy:Scores & ReasonsDetailedness:Scores & Reasons\nMLLM response\nFigure 4. Illustration of GPT-4V-aided evaluation.\nResults on LLaV A-QA90.Different from the above two\nexperiments that only involve “Yes-or-No” questions, the\nexperiment on LLaV A-QA90 is much more open. The\ndescription-type queries instruct MLLMs to fully translate\nthe input image into language, rather than merely referring\nto the existence or the attribute of an object.\nTherefore, a more reasonable and comprehensive manner\nis needed to support the evaluation of such open answers.\nSome existing efforts are devoted to exploring automatic\nevaluation with the aid of LLM [20, 21]. Specifically, a text-\nonly GPT-4 is adopted, and the image content is fed to the\nlanguage model in the form of short captions and bounding\nboxes of some objects. Nevertheless, the process of image-to-\ntext translation inevitably loses a lot of information, making\nthe evaluation process potentially inaccurate and biased.\nIn light of the recent release of a strong MLLM, GPT-4V ,\nwe propose to evaluate via a more straightforward approach.\nAs shown in Fig. 4, GPT-4V can directly receive the original\nresponse, the corrected ones, and most importantly, the in-\nput image. In such a case, we can prompt GPT-4V to let it\ngive evaluation results and reasons for judgment. However,\nit has just opened up its web interface that only supports\nmultimodal interaction through manual operation, and there\nare strict limits on the number of uses. This makes the GPT-\n4V-based evaluation labor-intensive, and we can only test a\nlimited number of images, such as LLaV A-QA90. To meet\nour needs, we devise the following two metrics:\n• Accuracy: whether the response is accurate with respect\nto the image content.\n• Detailedness: whether the response is rich in details.\nThe scores of the two metrics are displayed in Tab. 3,\nfrom which we can see that our method achieves consistent\ngains over the baseline MLLMs. On the one hand, the im-\nprovement in accuracy suggests that our Woodpecker can\neffectively correct the hallucinations in MLLM responses.\nOn the other hand, the bounding box information introduced\nin our framework adds details to the response, contributing\nto the boost in detailedness.\n7\n4.3. Experimental Analysis\n/uni00000028/uni0000005b/uni0000004c/uni00000056/uni00000057/uni00000048/uni00000051/uni00000046/uni00000048/uni00000026/uni00000052/uni00000058/uni00000051/uni00000057/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000026/uni00000052/uni0000004f/uni00000052/uni00000055/uni00000037/uni00000052/uni00000057/uni00000044/uni0000004f\n/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000056/uni00000053/uni0000004f/uni0000004c/uni00000057\n/uni00000013\n/uni00000014/uni00000013/uni00000013\n/uni00000015/uni00000013/uni00000013\n/uni00000016/uni00000013/uni00000013\n/uni00000017/uni00000013/uni00000013\n/uni00000018/uni00000013/uni00000013\n/uni00000019/uni00000013/uni00000013/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048\n/uni00000047/uni00000048/uni00000049/uni00000044/uni00000058/uni0000004f/uni00000057\n/uni00000047/uni00000048/uni00000049/uni00000044/uni00000058/uni0000004f/uni00000057/uni00000003/uni0000005a/uni00000012/uni00000027/uni00000048/uni00000057/uni00000048/uni00000046/uni00000057/uni00000052/uni00000055\n/uni00000047/uni00000048/uni00000049/uni00000044/uni00000058/uni0000004f/uni00000057/uni00000003/uni0000005a/uni00000012/uni00000039/uni00000034/uni00000024\n/uni00000047/uni00000048/uni00000049/uni00000044/uni00000058/uni0000004f/uni00000057/uni00000003/uni0000005a/uni00000012/uni0000003a/uni00000052/uni00000052/uni00000047/uni00000053/uni00000048/uni00000046/uni0000004e/uni00000048/uni00000055\nFigure 5. Results on MME with different framework variants. “de-\nfault” is a model that always answer “Yes”, “default w/Detector”\nintroduces the object detector for hallucination correction, and “de-\nfault w/VQA” introduces the VQA model. “default w/Woodpecker”\nis our full framework.\nAnalysis of framework modules. To understand the roles\nof different modules and their synergy, we take a dive into\nthem and their ensemble. For the purpose of avoiding dis-\ntractions from the variation of MLLMs, we formulate a sim-\nple test bench by casting a “default” model that always an-\nswers “Yes”. Afterward, the answers and the questions are\nmerged into more specific claims. For example, given a\nquestion, “ Is there a train in the picture?\nPlease answer yes or no.”, we compose an an-\nswer of the default model as “ Yes, there is a\ntrain in the picture.”. Furthermore, we create\ntwo extra variants of our framework, one of which only\nincludes the open-set detector and the other with only the\nVQA model, respectively dubbed as “default w/Detector”\nand “default w/VQA”:\n• default w/Detector. This variant is designed to probe the\ncontribution of the detector on mitigating object-level\nhallucinations, more specifically, the existence and count\naspects of hallucinations.\n• default w/VQA. By designing this variant, we aim to\nstudy the effectiveness of our selected VQA model in\nproviding attribute information.\nThe former is implemented by only providing the object-\nlevel information in the knowledge base, while the latter\nis realized by providing the attribute-level information. We\ncompare these two variants with our proposed full frame-\nwork, i.e., “default Woodpecker”, which uses both types of\ninformation.\nAs shown in Fig. 5, the gains in terms of existence and\ncount splits mainly derive from the introduction of the open-\nset detector, and the improvement in the color part can be\nattributed to the application of the VQA model. This is in\n/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni0000001a/uni0000001c/uni00000011/uni00000015/uni00000008\n/uni00000030/uni0000004c/uni00000056/uni00000010/uni00000046/uni00000052/uni00000055/uni00000055/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000019/uni00000011/uni00000015/uni00000008\n/uni00000032/uni00000050/uni0000004c/uni00000056/uni00000056/uni0000004c/uni00000052/uni00000051\n/uni00000014/uni00000017/uni00000011/uni00000019/uni00000008\nFigure 6. Proportion of different correction results.\nline with the expectation since we collect count information\nby means of the detector and gather information about spe-\ncific attributes, i.e., position and color, via the VQA model.\nConsequently, the full model combines the advantages of\nboth modules and achieves the best results.\nTo give an intuitive comprehension of the results of cor-\nrection and the GPT-4V-aid evaluation, we offer a case in Ap-\npendix B. Specifically, we list the query and the MLLM\nresponse before and after correction. For reference, scores\nand reasons given by GPT-4V are also listed.\nAnalysis of correction performance.In this part, we aim\nto probe further the performance of correction. Since there\nis a lack of related works in measuring the correction behav-\nior, we fulfill this goal by breaking down the results after\ncorrection into three sections:\n• Accuracy: | correct answers kept and wrong answers\ncorrected | / | problems |.\n• Omission: | wrong responses that fail to be corrected | /\n| problems |.\n• Mis-correction: | correct responses mistakenly modified |\n/ | problems |.\nConcretely, we summarize the results of the “default”\nmodel on MME and calculate the three introduced metrics.\nAs reflected in Fig. 6, our correction method reaches an\naccuracy of 79.2%, and meanwhile, the omission and mis-\ncorrection rates remain at a relatively low level. The results\nindicate that our method can cover most cases without being\nover-confident.\n5. Conclusion\nIn this work, we have proposed the first correction-based\nframework for mitigating hallucinations in MLLMs. As a\ntraining-free method, our approach incorporated multiple\noff-the-shelf models and could be easily integrated into dif-\nferent MLLMs. To evaluate the efficacy of the proposed\n8\nframework, we conduct massive experiments on three bench-\nmarks under different settings, including using GPT-4V for\ndirect and automatic assessment. We hope this work can\nspark new thoughts on addressing the issue of hallucinations\nin MLLMs.\nReferences\n[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan,\nPeng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.\nQwen-vl: A versatile vision-language model for understand-\ning, localization, text reading, and beyond. arXiv, 2023. 1\n[2] William Berrios, Gautam Mittal, Tristan Thrush, Douwe\nKiela, and Amanpreet Singh. Towards language models that\ncan see: Computer vision through the lens of natural language.\narXiv, 2023. 3\n[3] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George Bm\nVan Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc,\nAidan Clark, et al. Improving language models by retrieving\nfrom trillions of tokens. In ICML, 2022. 3\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Language\nmodels are few-shot learners. NeurIPS, 2020. 5\n[5] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan,\nMichael Auli, and Jason Weston. Wizard of wikipedia:\nKnowledge-powered conversational agents. arXiv, 2018. 3\n[6] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Meng-\ndan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu\nZheng, et al. Mme: A comprehensive evaluation benchmark\nfor multimodal large language models. arXiv, 2023. 3, 5\n[7] Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi V o, Zane Du-\nrante, Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri\nTerzopoulos, Li Fei-Fei, et al. Mindagent: Emergent gaming\ninteraction. arXiv, 2023. 3\n[8] Anisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and\npreventing hallucinations in large vision language models.\narXiv, 2023. 3\n[9] Tanmay Gupta and Aniruddha Kembhavi. Visual program-\nming: Compositional visual reasoning without training. In\nCVPR, 2023. 3\n[10] Kung-Hsiang Huang, Hou Pong Chan, and Heng Ji. Zero-shot\nfaithful factual error correction. arXiv, 2023. 3, 5\n[11] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals,\nAndrew Zisserman, and Joao Carreira. Perceiver: General\nperception with iterative attention. In ICML, 2021. 5\n[12] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll´ar, and Ross\nGirshick. Segment anything. arXiv, 2023. 4\n[13] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan,\nShu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via\nlarge language model. arXiv, 2023. 3\n[14] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,\nVladimir Karpukhin, Naman Goyal, Heinrich K¨uttler, Mike\nLewis, Wen-tau Yih, Tim Rockt ¨aschel, et al. Retrieval-\naugmented generation for knowledge-intensive nlp tasks.\nNeurIPS, 2020. 3\n[15] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu. Otter: A multi-modal model\nwith in-context instruction tuning. arXiv, 2023. 5, 6, 7\n[16] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-\n2: Bootstrapping language-image pre-training with frozen\nimage encoders and large language models. arXiv, 2023. 4, 5\n[17] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei\nYang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei\nZhang, Jenq-Neng Hwang, et al. Grounded language-image\npre-training. In CVPR, 2022. 4\n[18] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin\nZhao, and Ji-Rong Wen. Evaluating object hallucination in\nlarge vision-language models. arXiv, 2023. 3, 5, 6\n[19] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, 2014. 5\n[20] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Ya-\ncoob, and Lijuan Wang. Mitigating hallucination in large\nmulti-modal models via robust instruction tuning. arXiv,\n2023. 1, 3, 7\n[21] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv, 2023. 1, 3, 5, 6, 7\n[22] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\nZhu, et al. Grounding dino: Marrying dino with grounded\npre-training for open-set object detection. arXiv, 2023. 4, 5\n[23] Jiaying Lu, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo,\nYawen Zhang, Baochen Sun, Carl Yang, and Jie Yang. Evalu-\nation and mitigation of agnosia in multimodal large language\nmodels. arXiv, 2023. 3\n[24] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei\nChang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao.\nChameleon: Plug-and-play compositional reasoning with\nlarge language models. arXiv, 2023. 3\n[25] Timo L¨uddecke and Alexander Ecker. Image segmentation\nusing text and image prompts. In CVPR, 2022. 4\n[26] OpenAI. Gpt-4 technical report. arXiv, 2023. 5\n[27] Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia\nXie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu\nChen, et al. Check your facts and try again: Improving large\nlanguage models with external knowledge and automated\nfeedback. arXiv, 2023. 3\n[28] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis,\nMajid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite,\nVladimir Karpukhin, Jean Maillard, et al. Kilt: a benchmark\nfor knowledge intensive language tasks. arXiv, 2020. 3\n[29] Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nDmytro Okhonko, Samuel Broscheit, Gautier Izacard, Patrick\nLewis, Barlas O˘guz, Edouard Grave, Wen-tau Yih, et al. The\nweb is your oyster-knowledge-intensive nlp against a very\nlarge web corpus. arXiv, 2021. 3\n[30] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weim-\ning Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks\nwith chatgpt and its friends in huggingface. arXiv, 2023. 3\n9\n[31] Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael\nSmith, Stephen Roller, Megan Ung, Moya Chen, Kushal\nArora, Joshua Lane, et al. Blenderbot 3: a deployed conver-\nsational agent that continually learns to responsibly engage.\narXiv, 2022. 3\n[32] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guil-\nlaume Couairon, Wojciech Galuba, Marcus Rohrbach, and\nDouwe Kiela. Flava: A foundational language and vision\nalignment model. In CVPR, 2022. 4\n[33] Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong,\nPan Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang, et al.\nVigc: Visual instruction generation and correction. arXiv,\n2023. 1, 2, 3\n[34] Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi,\nChenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang,\nJihua Zhu, et al. Evaluation and analysis of hallucination in\nlarge vision-language models. arXiv, 2023. 3\n[35] Teng Wang, Jinrui Zhang, Junjie Fei, Yixiao Ge, Hao Zheng,\nYunlong Tang, Zhe Li, Mingqi Gao, Shanshan Zhao, Ying\nShan, et al. Caption anything: Interactive image description\nwith diverse multimodal controls. arXiv, 2023. 3\n[36] Zhenyu Wang, Yali Li, Xi Chen, Ser-Nam Lim, Antonio Tor-\nralba, Hengshuang Zhao, and Shengjin Wang. Detecting\neverything in the open world: Towards universal object detec-\ntion. In CVPR, 2023. 4\n[37] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang,\nZecheng Tang, and Nan Duan. Visual chatgpt: Talking, draw-\ning and editing with visual foundation models. arXiv, 2023.\n3\n[38] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu\nLi, and Ying Shan. Gpt4tools: Teaching large language model\nto use tools via self-instruction. arXiv, 2023. 3\n[39] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan\nAzarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael\nZeng, and Lijuan Wang. Mm-react: Prompting chatgpt for\nmultimodal reasoning and action. arXiv, 2023. 3\n[40] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, et al. mplug-owl: Modularization empowers large\nlanguage models with multimodality. arXiv, 2023. 1, 3, 5, 6,\n7\n[41] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun,\nTong Xu, and Enhong Chen. A survey on multimodal large\nlanguage models. arXiv, 2023. 1, 3, 5\n[42] Haoxuan You, Rui Sun, Zhecan Wang, Long Chen, Gengyu\nWang, Hammad A Ayyubi, Kai-Wei Chang, and Shih-Fu\nChang. Idealgpt: Iteratively decomposing vision and language\nreasoning via large language models. arXiv, 2023. 3\n[43] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choro-\nmanski, Adrian Wong, Stefan Welker, Federico Tombari,\nAveek Purohit, Michael Ryoo, Vikas Sindhwani, et al. So-\ncratic models: Composing zero-shot multimodal reasoning\nwith language. arXiv, 2022. 3\n[44] Ao Zhang, Hao Fei, Yuan Yao, Wei Ji, Li Li, Zhiyuan Liu,\nand Tat-Seng Chua. Transfer visual prompt generator across\nllms. arXiv, 2023. 1\n[45] Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Han-\nqiu Deng, Yu Qiao, Peng Gao, and Hongsheng Li. Prompt,\ngenerate, then cache: Cascade of foundation models makes\nstrong few-shot learners. In CVPR, 2023. 3\n[46] Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen,\nWenxuan Zhang, and Mohamed Elhoseiny. Chatgpt asks, blip-\n2 answers: Automatic questioning towards enriched visual\ndescriptions. arXiv, 2023. 3\n[47] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny. Minigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv,\n2023. 1, 3, 5, 6, 7\n[48] Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Ziyao\nZeng, Zipeng Qin, Shanghang Zhang, and Peng Gao. Point-\nclip v2: Prompting clip and gpt for powerful 3d open-world\nlearning. In ICCV, 2023. 3\n10\nA. Prompt Templates\nIn this part, we list our prompt templates for instructing LLM\nto fulfill various tasks, including key concept extraction,\nquestion formulation, hallucination correction, and GPT-4V-\naided evaluation.\nA.1. Key Concept Extraction\nThe template is listed in Tab. 4.\nA.2. Question Formulation\nThe template is listed in Tab. 5.\nA.3. Hallucination Correction\nThe template is listed in Tab. 6.\nA.4. GPT-4V-aided Evaluation\nThe template is listed in Tab. 7.\nB. GPT-4V-aided Evaluation Case\nTo offer a straightforward and intuitive understanding, we\nlist an evaluation case in Fig. 7, where “Assistant 1” and\n“Assistant 2” in the evaluation reason part correspond to\n“MLLM” and “MLLM w/Woodpecker”, respectively. GPT-\n4V gives not only respective scores for responses but also\nreasons for the judgment.\n11\nSystem message\nYou are a language assistant that helps to extract information from given sentences.\nPrompt\nGiven a sentence, extract the existent entities within the sentence for me.\nExtract the common objects and summarize them as general categories without repetition, merge essentially similar\nobjects.\nAvoid extracting abstract or non-specific entities. Only extract concrete, certainly existent objects that fall in general\ncategories and are described in a certain tone in the sentence.\nExtract entity in the singular form. Output all the extracted types of items in one line and separate each object type\nwith a period. If there is nothing to output, then output a single “None”.\nExamples:\n{In-context examples}\nSentence:\n{Input sentence}\nOutput:\nTable 4. Template for prompting LLM to perform key concept extraction.{In-context examples} are in-context examples used to\nbetter instruct the LLM to fulfill the task, and {Input sentence} is the input from which the key concept is extracted.\n12\nSystem message\nYou are a language assistant that helps to ask questions about a sentence.\nPrompt\nGiven a sentence, extract the existent entities within the sentence for me.\nGiven a sentence and some entities connected by periods, you are required to ask some relevant questions about the\nspecified entities involved in the sentence, so that the questions can help to verify the factuality of the sentence.\nQuestions may involve basic attributes such as colors and actions mentioned in the sentence. Do not ask questions\ninvolving object counts or the existence of objects.\nWhen asking questions about attributes, try to ask simple questions that only involve one entity.\nAsk questions that can be easily decided visually. Do not ask questions that require complex reasoning.\nDo not ask semantically similar questions. Do not ask questions only about scenes or places.\nUse “where” type questions to query the position information of the involved entities.\nDo not ask questions about uncertain or conjecture parts of the sentence, for example, the parts described with “maybe”\nor “likely”, etc.\nIt is no need to cover all the specified entities. If there is no question to ask, simply output a “None”.\nWhen asking questions, do not assume the claims in the description as true in advance. Only ask questions relevant to\nthe information in the sentence.\nOnly ask questions about common, specific, and concrete entities. The entities involved in the questions are limited to\nthe range within the given entities.\nOutput only one question in each line. For each line, first output the question, then a single “&”, and finally entities\ninvolved in the question, still connected by periods if multiple entities are involved.\nExamples:\n{In-context examples}\nSentence:\n{Input sentence}\nEntities:\n{Input entities}\nQuestions:\nTable 5. Prompt template for question formulation. {In-context examples} are in-context examples. {Input sentence} and {Input\nentities} are the inputs, where the latter comes from the step of key concept extraction.\n13\nSystem message\nYou are a language assistant that helps to refine a passage according to instructions.\nPrompt\nGiven a passage and some supplementary information, you are required to correct and output the refined passage in a\nfluent and natural style, following these rules:\n1. The supplementary information may include some of the following parts:\n“Count” information that specifies how many instances of a certain kind of entity exist, and their associated\nbounding boxes;\n“Specific” information that describes attribute information specific to each entity instance, including bounding\nboxes, colors, etc. The information is arranged in the form of “entity 1: [bbox]” info of this entity. Note that the entity\nin “Specific” information corresponds to that in the “Count” information.\n“Overall” information that may involve information about multiple entity objects.\n2. Try to retain the original sentence with minimal changes.\n3. The number of entitie instances should match the number in the “Count” information. Also correct the number\ncounts if the number stated in the original sentence does not match the counting information.\n4. If the original sentence is already correct, then just keep it. If you need to rewrite the original sentence, when rewriting,\ntry to modify the original sentence as little as possible based on the original sentence, and use the supplementary\ninformation as guidance to correct or enrich the original sentence.\n5. In the refined passage, when describing entities mentioned in the “Specific” supplementary information, add their\nassociated bounding boxes in parentheses right after them, in the form of “entity([bbox])”. If multiple entities of the\nsame kind are mentioned, then separate the box with “;”, in the form of “entity([bbox1];[bbox2])”\nExamples:\n{In-context examples}\n——————-\nSupplementary information:\n{Input information}\nPassage:\n{Input passage}\nRefined passage:\nTable 6. Prompt template for hallucination correction. {In-context examples} are in-context examples. {Input information} is the\nformatted knowledge base, and {Input passage} is the original response to be corrected.\n14\nPrompt\nYou are required to score the performance of two AI assistants in describing a given image. You should pay extra\nattention to the hallucination, which refers to the part of descriptions that are inconsistent with the image content,\nsuch as claiming the existence of something not present in the image or describing incorrectly in terms of the counts,\npositions, or colors of objects in the image. Note that the descriptions may be accompanied by bounding boxes,\nindicating the position of objects in the image, which are represented as [x1, y1, x2, y2] with floating numbers ranging\nfrom 0 to 1. These values correspond to the top left x1, top left y1, bottom right x2, and bottom right y2.\nPlease rate the responses of the assistants on a scale of 1 to 10, where a higher score indicates better performance,\naccording to the following criteria:\n1: Accuracy: whether the response is accurate with respect to the image content. Responses with fewer hallucinations\nshould be given higher scores.\n2: Detailedness: whether the response is rich in necessary details. Note that hallucinated descriptions should not count\nas necessary details.\nPlease output a single line for each criterion, containing only two values indicating the scores for Assistant 1 and 2,\nrespectively. The two scores are separated by a space. Following the scores, please provide an explanation of your\nevaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not\naffect your judgment.\n[Assistant 1]\n{Response 1}\n[End of Assistant 1]\n[Assistant 2]\n{Response 2}\n[End of Assistant 2]\nOutput format:\nAccuracy:\nScores of the two answers:\nReason:\nDetailedness:\nScores of the two answers:\nReason:\nTable 7. Prompt template for GPT-4V-aided evaluation.{Response 1} and {Response 2} are the original responses and the corrected\nones, respectively.\n15\nGPT-4V-aided Evaluation\nAccuracy:\n8 10\nReason:\nBoth assistants have accurately described the presence of an elephant on a dirt ground \nwith a blue bag and tires nearby. However, Assistant 1 mentioned a single tire, while the \nimage shows two tires. Assistant 2 correctly identified the number of tires and their \npositions.\nDetailedness:\n6 9\nReason:\nWhile both descriptions provide necessary details about the elephant, blue bag, and tires, \nAssistant 2 goes a step further by providing bounding boxes for the elephant, blue bag, \nand both tires, offering a more detailed description of the image content. Assistant 1's \ndescription is less detailed in comparison.\nGPT Score:\nIn this snapshot, an elephant([0.306, 0.4, 0.636, 0.783]) is walking on a dirt ground. \nThere is a blue bag([0.319, 0.798, 0.449, 0.919]) nearby the elephant, and there are two \ntires([0.634, 0.797, 0.795, 0.873];[0.0, 0.874, 0.21, 0.987]) nearby as well.\nMLLM w/Woodpecker:\nAn elephant is walking on a dirt ground with a blue bag and a tire nearby.\nMLLM:\nFigure 7. Example for the GPT-4V-aided evaluation.\n16",
  "topic": "Woodpecker",
  "concepts": [
    {
      "name": "Woodpecker",
      "score": 0.8735373020172119
    },
    {
      "name": "Computer science",
      "score": 0.7291845083236694
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6644924879074097
    },
    {
      "name": "Shadow (psychology)",
      "score": 0.5683199167251587
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5181980729103088
    },
    {
      "name": "Code (set theory)",
      "score": 0.5040234923362732
    },
    {
      "name": "Language model",
      "score": 0.48167169094085693
    },
    {
      "name": "Key (lock)",
      "score": 0.4652244746685028
    },
    {
      "name": "Natural language processing",
      "score": 0.40353450179100037
    },
    {
      "name": "Machine learning",
      "score": 0.34485357999801636
    },
    {
      "name": "Psychology",
      "score": 0.16834962368011475
    },
    {
      "name": "Programming language",
      "score": 0.13635221123695374
    },
    {
      "name": "Ecology",
      "score": 0.1304786503314972
    },
    {
      "name": "Geography",
      "score": 0.1002577543258667
    },
    {
      "name": "Computer security",
      "score": 0.09462898969650269
    },
    {
      "name": "Cartography",
      "score": 0.08620822429656982
    },
    {
      "name": "Habitat",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    }
  ]
}