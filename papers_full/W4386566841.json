{
    "title": "Do Pretrained Contextual Language Models Distinguish between Hebrew Homograph Analyses?",
    "url": "https://openalex.org/W4386566841",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5013212409",
            "name": "Avi Shmidman",
            "affiliations": [
                null,
                "Bar-Ilan University"
            ]
        },
        {
            "id": "https://openalex.org/A5015862561",
            "name": "Cheyn Shmuel Shmidman",
            "affiliations": [
                null,
                "Bar-Ilan University"
            ]
        },
        {
            "id": "https://openalex.org/A5005137956",
            "name": "Dan Bareket",
            "affiliations": [
                "Bar-Ilan University"
            ]
        },
        {
            "id": "https://openalex.org/A5019908505",
            "name": "Moshe Koppel",
            "affiliations": [
                null,
                "Bar-Ilan University"
            ]
        },
        {
            "id": "https://openalex.org/A5063283689",
            "name": "Reut Tsarfaty",
            "affiliations": [
                "Bar-Ilan University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3202070718",
        "https://openalex.org/W1555354714",
        "https://openalex.org/W2436001372",
        "https://openalex.org/W94261566",
        "https://openalex.org/W2970171327",
        "https://openalex.org/W3035110948",
        "https://openalex.org/W1911120693",
        "https://openalex.org/W3004346089",
        "https://openalex.org/W4288351520",
        "https://openalex.org/W4288631803",
        "https://openalex.org/W3105928338",
        "https://openalex.org/W3154147337",
        "https://openalex.org/W3127069001",
        "https://openalex.org/W3035160860",
        "https://openalex.org/W1918241",
        "https://openalex.org/W2890774895",
        "https://openalex.org/W3098189689",
        "https://openalex.org/W2251114756",
        "https://openalex.org/W2163786648",
        "https://openalex.org/W2100976324",
        "https://openalex.org/W3037575273",
        "https://openalex.org/W4205957816",
        "https://openalex.org/W2176700180",
        "https://openalex.org/W2951941824",
        "https://openalex.org/W1966689924",
        "https://openalex.org/W2896457183"
    ],
    "abstract": "Semitic morphologically-rich languages (MRLs) are characterized by extreme\\nword ambiguity. Because most vowels are omitted in standard texts, many of the\\nwords are homographs with multiple possible analyses, each with a different\\npronunciation and different morphosyntactic properties. This ambiguity goes\\nbeyond word-sense disambiguation (WSD), and may include token segmentation into\\nmultiple word units. Previous research on MRLs claimed that standardly trained\\npre-trained language models (PLMs) based on word-pieces may not sufficiently\\ncapture the internal structure of such tokens in order to distinguish between\\nthese analyses. Taking Hebrew as a case study, we investigate the extent to\\nwhich Hebrew homographs can be disambiguated and analyzed using PLMs. We\\nevaluate all existing models for contextualized Hebrew embeddings on a novel\\nHebrew homograph challenge sets that we deliver. Our empirical results\\ndemonstrate that contemporary Hebrew contextualized embeddings outperform\\nnon-contextualized embeddings; and that they are most effective for\\ndisambiguating segmentation and morphosyntactic features, less so regarding\\npure word-sense disambiguation. We show that these embeddings are more\\neffective when the number of word-piece splits is limited, and they are more\\neffective for 2-way and 3-way ambiguities than for 4-way ambiguity. We show\\nthat the embeddings are equally effective for homographs of both balanced and\\nskewed distributions, whether calculated as masked or unmasked tokens. Finally,\\nwe show that these embeddings are as effective for homograph disambiguation\\nwith extensive supervised training as with a few-shot setup.\\n",
    "full_text": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 849–864\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nDo Pretrained Contextual Language Models Distinguish between\nHebrew Homograph Analyses?\nAvi Shmidman1,2,† Cheyn Shmuel Shmidman2,‡\nDan Bareket1,‡ Moshe Koppel1,2,‡ Reut Tsarfaty1,†\n1Bar Ilan University 2DICTA / Jerusalem, Israel\n†{avi.shmidman,reut.tsarfaty}@biu.ac.il\n‡{cheynshmuel,dbareket,moishk}@gmail.com\nAbstract\nSemitic morphologically-rich languages\n(MRLs) are characterized by extreme word\nambiguity. Because most vowels are omitted\nin standard texts, many of the words are homo-\ngraphs with multiple possible analyses, each\nwith a different pronunciation and different\nmorphosyntactic properties. This ambiguity\ngoes beyond word-sense disambiguation\n(WSD), and may include token segmentation\ninto multiple word units. Previous research\non MRLs claimed that standardly trained\npre-trained language models (PLMs) based\non word-pieces may not sufficiently capture\nthe internal structure of such tokens in order\nto distinguish between these analyses. Taking\nHebrew as a case study, we investigate\nthe extent to which Hebrew homographs\ncan be disambiguated and analyzed using\nPLMs. We evaluate all existing models for\ncontextualized Hebrew embeddings on a novel\nHebrew homograph challenge sets that we\ndeliver. Our empirical results demonstrate\nthat contemporary Hebrew contextualized\nembeddings outperform non-contextualized\nembeddings; and that they are most effective\nfor disambiguating segmentation and mor-\nphosyntactic features, less so regarding pure\nword-sense disambiguation. We show that\nthese embeddings are more effective when the\nnumber of word-piece splits is limited, and\nthey are more effective for 2-way and 3-way\nambiguities than for 4-way ambiguity. We\nshow that the embeddings are equally effective\nfor homographs of both balanced and skewed\ndistributions, whether calculated as masked or\nunmasked tokens. Finally, we show that these\nembeddings are as effective for homograph\ndisambiguation with extensive supervised\ntraining as with a few-shot setup.\n1 Introduction\nSemitic morphologically-rich languages (MRLs)\nsuch as Arabic, Hebrew, and Aramaic are char-\nacterized by extreme ambiguity at the word level\n(Wintner, 2014; Tsarfaty et al., 2020). In a standard\ntext, many (and often most) of the words will be\nhomographs with multiple possible analyses. The\nhigh ambiguity derives from several factors. First,\nprepositions, conjunctions, accusative pronouns,\nand possessive pronouns are often seamlessly af-\nfixed to words. Next, vowels are generally omitted\nin written texts. Finally, proper nouns are not differ-\nentiated from common nouns (no capital letters).\nThe task of distinguishing between Hebrew ho-\nmograph analyses is related to the general task of\nWord Sense Disambiguation (WSD) (Agirre and\nEdmonds, 2006; Navigli, 2009), yet it is more chal-\nlenging. In the standard case of WSD, a single\northographic form is associated with a single word\nthat can be analyzed in terms of two or more senses;\nalso, the analyses are generally pronounced iden-\ntically, and often have the same morphosyntactic\nproperties (e.g bank of a river vs. savings bank).\nIn contrast, in Semitic languages, the need for dis-\nambiguation often goes beyond a determination of\nsense. Hebrew word ambiguities can be divided\ninto three primary categories (Table 1): 1. Segmen-\ntation ambiguities, in which a given orthographic\nform may (or may not) be segmented into multi-\nple word units each bearing its own role (POS tag)\nin the sentence. 2. Morphosyntactic ambiguities,\nin which the segmentation of the form is not am-\nbiguous, but multiple analyses of the word reflect\ndifferent morphosyntactic properties of each word\nunit(s). 3. Sense ambiguities (the aforementioned\nstandard case of WSD), in which the analyses of the\nunit(s) do not differ in their morphosyntactic prop-\nerties, but rather in their sense. One orthographic\nform may exhibit multiple types of ambiguity si-\nmultaneously.\nPretrained contextualized language models with\nstandard word-piece tokenization mechanisms have\nbeen shown to excel at WSD in English and other\nIndo-European languages (Yaghoobzadeh et al.,\n2019). However, for Hebrew and other semitic lan-\n849\nType Form Word (translation)Morphology\nSegmentation\nהקפהדרֹוקָּפֶהדרֹו+הַדרֹוthe+coffee)DET + Noun [M,S,abs]\nהַקָּפָהדרֹוcredit) Noun [F,S,abs]\nשאלַנִאַלַנִ+שׁfor+even) Sconj + Cconj\nשׁhe aspired) Verb [M,S,3,PAST]\nMorph\nאלימותדרֹואַלִּימוֹתדרֹוviolent) Adj [F,P,abs]\nאַלִּימוּתדרֹוviolence) Noun [F,S,abs/cons]\nהרילַנִהֵרhe lifted) Verb [M,S,3,PAST]\nהָרmountains) Noun [M,P,abs]\nSemantic\nהזמרדרֹוזּ+הַדרֹוthe+song)DET + Noun [M,S,abs]\nזּ+הַדרֹוthe+singer)DET + Noun [M,S,abs]\nהסופרדרֹוסּוֹפֵרדרֹו+הַדרֹוthe+author)DET + Noun [M,S,abs]\nסּוּפֶּרדרֹו+הַדרֹוthe+market)DET + Noun [M,S,abs]\nTable 1: Examples of Hebrew ambiguity types\nguages it has been argued that such models would\nnot sufficiently capture the structure of MRLs in\norder to distinguish between internally-complex ho-\nmograph analyses (Klein and Tsarfaty, 2020; Tsar-\nfaty et al., 2020). In this work, we take Modern\nHebrew, a Semitic language with rich and highly\nambiguous morphology, as a case study, and we\ninvestigate the extent to which homographs can\nbe disambiguated by contextualized embeddings,\nregarding all three levels of ambiguity. Regarding\nArabic — a sister language to Hebrew — a wide\nsurvey of WSD methods is presented by Abder-\nrahim and Abderrahim (2022). They raise the pos-\nsibility of utilizing pretrained contextualized em-\nbeddings, yet leave its evaluation to future work.1\nHebrew is a particularly challenging language\non which to perform a homograph disambiguation\ndue to the limited available corpora. First of all,\ncurrently existing Hebrew treebanks are severely\nlimited in size, such that most of the words in the\nlanguage are not amply represented. Furthermore,\neven regarding common Hebrew words, this corpus\nis problematic, because the nature of language is\nsuch that many homographs are skewed in their\ndistribution; thus, even if the primary analysis is\nsufficiently represented within a tagged corpus, the\nsecondary analysis will often be hopelessly under-\nrepresented. For instance, one common Hebrew\nhomograph isמהלַנִ(mhm), which can be analyzed\nas a preposition with pronominal suffix, or as an\ninterrogative. The ratio of these two analyses in\nnaturally-occurring Hebrew text is over 50:1; thus,\noccurrences of the secondary analysis within exist-\ning tagged corpora are insufficient.\nIn theory, these homograph ambiguities could\nbe addressed using POS tagging systems. For in-\nstance, Habash and Rambow (2005) consider the\n1Additional studies in Arabic WSD include Merhben et al.\n(2010), Merhbene et al. (2013), and Shah et al. (2010).\nuse of a morphological tagging system to solve\nWSD in Arabic. A number of Hebrew POS tagging\nsystems have been published as well (Yona and\nWintner, 2005; Adler and Elhadad, 2006; Shacham\nand Wintner, 2007). The current SOTA for He-\nbrew POS tagging is the YAP morpho-syntactic\nparser (Tsarfaty et al., 2019). However, as we have\nshown in a previous study (Shmidman et al., 2020,\np. 3318, table 2), although Y AP produces high ac-\ncuracy overall on normal Hebrew text, its scores\ndrop drastically regarding homographs of skewed\ndistribution.\nFor analogous cases of skewed distribution in\nother languages, researchers have proposed the cre-\nation of dedicated challenge sets, containing hard-\nto-classify sentences not easily found in naturally-\noccurring text (Gardner et al., 2020; Elkahky et al.,\n2018). In the aforementioned previous study, we\nproduced 22 such challenge sets for Hebrew homo-\ngraphs, and demonstrated that a Bi-LSTM of non-\ncontextualized embeddings can obtain high accu-\nracy on this task, establishing the current SOTA for\nHebrew homograph disambiguation (Shmidman\net al., 2020). In this paper, we extend the investi-\ngation by considering whether contextualized em-\nbeddings from pretrained language models (PLMs)\ncan provide a more optimal solution. We consider\nall existing contextualized Hebrew PLMs: multi-\nlingual BERT (\"mBERT\") (Devlin et al., 2019);\nHeBERT (Chriqui and Yahav, 2021); and Aleph-\nBERT (Seker et al., 2021) (Table 2). Moreover, we\nevaluate and verify these on a new dataset, substan-\ntially larger than all previous datasets for Hebrew\nhomograph disambiguation.\nOur experiments demonstrate that contextual-\nized PLMs pre-trained on sufficiently large unla-\nbeled data and vocabulary size are excellent at dis-\nambiguating the word-internal structures of homo-\ngraphs, yet face some challenge with pure sense dis-\nambiguation. We show the efficacy of these models\nin cases of homographs with skewed distribution,\nand in a few-shot setup. Finally, we establish new\nstate-of-the-art results on the challenging task of\nhomograph disambiguation for a morphologically-\nrich language printed without vowels, along with a\nnovel benchmark for assessing the morphological\nreach of future PLMs in Hebrew.\n2 The Data\nThe challenge sets for Hebrew homograph disam-\nbiguation from our previous study were limited in\n850\nModel V ocab Corpus Size\n(Heb. tokens)(Heb. sentences)\nmBERT 2.5K 6.3M\nHeBERT 30K 27.2M\nAlephBERT 52K 98.7M\nTable 2: Comparison of available Hebrew BERT models\nnumber (only 22 sets) and insufficiently representa-\ntive regarding types of ambiguities; only one of the\nsets was a prefix-segmentation ambiguity. Further,\nthey were limited to binary cases, where only two\nanalyses exist.\nIn contrast, for this study we employed field ex-\nperts to choose the most critical homographs in\nthe language. The experts chose 75 homographs\nfrom a list of the 3600 most frequent words in the\nlanguage, balancing frequency of word occurrence\nwith practical need for its disambiguation. All of\nthe homographs occur with a minimum frequency\nof 27 words per million in naturally occurring He-\nbrew text. Our challenge sets include homographs\nwith 2-5 possible analyses. Our sets contain a wide\nrepresentation of segmentation ambiguities (15 in\nnumber), as well as 5 cases of purely semantic\nambiguities. For each of the 75 homographs, we\ncollect hundreds of naturally-occurring sentences\nattesting to each analysis. In almost all cases, we\nsucceed in collecting 1000 sentences for the pri-\nmary analysis, at least 500 sentences for the sec-\nondary analysis, and at least 250 for each additional\nanalysis. The sentences were culled from newspa-\npers, Wikipedia, literature, and social media. We\nemployed a team of annotators who chose the rel-\nevant homograph analysis for each case. 2 All in\nall, our 75 challenge sets contain 150K tagged sen-\ntences. The full list of homographs and analyses is\nprovided in Appendix A.3\n3 Experimental Setup\nTo evaluate the ability of pre-trained language mod-\nels (PLMs) to disambiguate the in-context analyses\nof morphologically rich and highly ambiguous ho-\nmographs in Hebrew, we adopt a \"word expert\"\napproach, producing dedicated classifiers for each\nindividual homograph (Zhao et al., 2020).\nWe use two types of PLMs, contextualized and\nnon-contextualized. For the non-contextualized\ncase, we replicate our previous method detailed\n2The annotation process is detailed in Appendix B.\n3The dataset is downloadable at: https://github.com/\nDicta-Israel-Center-for-Text-Analysis/EACL_2023\nFigure 1: Comparison of previous SOTA (w2v-based\nBi-LSTM method) versus BERT-based approaches.\nin Shmidman et al. (2020). For each training ex-\nample, we use a BiLSTM on top of the word2vec\nembeddings of all of the words in the sentence\n(other than the homograph itself) to produce an\nencoding for disambiguation.4 An MLP is trained\nto predict the correct homograph analysis based on\nthis encoding.5 For the contextualized case, we run\nthe sentence through a pretrained contextualized\nlanguage model and retrieve the 768-dimension em-\nbedding representing the homograph in question.\nAn MLP is trained to predict the correct analysis\nbased on the homographs embeddings alone. In the\nstandard \"unmasked\" scenario, the sentence is fed\ninto the model as is, including the homograph in\nquestion. In the \"masked\" scenario, the homograph\nis replaced with a [MASK] token.\nWe evaluate the performance of each given\nmethod on each given challenge set using 10-fold\ncross-validation. We calculate an F1 score for each\nhomograph analysis, based upon the precision and\nrecall scores micro-averaged across all folds. We\nthen calculate the macro-average of the F1 scores\nfor all possible analyses for a given homograph,\nand this is the score reported in the charts herein.\n4 Results and Analysis\nStandard (Unmasked) Scenario Figure 1\npresents the cumulative F1 score obtained by the\nmodels for all challenge sets. Our results show that\nHeBERT and AlephBERT far outperform mBERT,\nwith AlephBERT achieving the higher score. The\npoor performance of mBERT is likely due to its\nsmaller pre-training data size and exceedingly lean\nHebrew vocabulary (cf. Table 2). Furthermore, the\n4We also tested fastText, but results were inferior.\n5For implementation details, see Appendix D.\n851\nFigure 2: Disambiguation accuracy across various cate-\ngories of homograph ambiguity.\nFigure 3: AlephBERT’s disambiguation accuracy across\nhomographs with differing counts of possible analyses.\nHeBERT and AlephBERT models both substan-\ntially outperform the previous word2vec-based\nSOTA. It is thus apparent that contextualized\nlanguage models do effectively capture Hebrew\nhomograph distinctions, even those based on\nword-pieces, even for an MRL, and they do so\nmore effectively than non-contextualized models.\nFigure 2 demonstrates AlephBERT’s perfor-\nmance on different ambiguity types. AlephBERT\nperforms equally well on cases of segmentation am-\nbiguity and morphosyntactic ambiguity. In contrast,\nwhen it comes to ambiguities that are purely seman-\ntic, the scores are noticeably lower. This is in line\nwith the findings of Ettinger (2020), who shows\nthat BERT is stronger with syntax than semantics;\nGoldberg (2019) also notes BERT’s strong syntac-\ntic abilities. Interestingly, the same gap exists with\nthe W2V-based method. Thus, both contextualized\nand non-contextualized embeddings struggle to dif-\nferentiate between senses which are morphologi-\ncally equivalent. Although such cases are only of\nminimal import when it comes to sentence parsing,\nFigure 4: Disambiguation accuracy across varying de-\ngrees of word piece splits within the target homograph,\nusing mBERT.\nthey are critical for downstream tasks such as coref-\nerence resolution and relation extraction. It thus\nremains a desideratum to improve disambiguation\nof purely semantic Hebrew homographs.\nThe results in Figure 3 demonstrate that Aleph-\nBERT performs equally well on cases of binary\nhomographs as on cases of three-way homograph\nclassification. However, when faced with cases of\n4-way or 5-way classification, accuracy declines.\nThe Effect of Word-Pieces Previous studies\nhave hypothesized that word-pieces are not ade-\nquate for capturing complex morphosyntactic struc-\ntures due to arbitrary (non-linguistic) word-splits.\nTo probe into this we investigate the question, do\nsuch splits affect performance. Our 75 homographs\nare all treated as single tokens in HeBERT and\nAlephBERT. However, many of the homographs\nare broken up into word pieces in mBERT, due to\nits meager Hebrew vocabulary. We thus compare\nmBERT’s results on words treated as single tokens\nversus those that are broken up into two or three\npieces, which are aggregated using first, sum, or\naverage of the vectors. With regard to cases of\nsplit words, we train models using three separate\nmethods: providing the MLP with only the embed-\nding of the first word piece; with an average of\nthe word piece embeddings; or with the sum of the\nembeddings. As shown in Figure 4, the splitting\nof a homograph into three word-pieces appears to\nhave a negative impact on the ability of the result-\ning embedding to differentiate between homograph\nanalyses, for all aggregation methods.\nMasked Scenario We consider whether Aleph-\nBERT embeddings are more effective if we replace\nthe homograph word with [MASK] when running\n852\nFigure 5: Comparison of unmasked vs. masked scenar-\nios, across varying degrees of skewed homographs. The\nratio axis indicates the relative distribution between the\nleast and most frequent analyses of each plotted homo-\ngraph within naturally-occurring Hebrew text.\nthe challenge set sentences through AlephBERT.\nThe motivation behind this experiment is that, as ex-\nplained above, many of the homographs are skewed\nin their natural proportion. In such cases, we worry\nwhether AlephBERT might be disproportionately\ninfluenced by the skewed distribution; replacing\nthe word with [MASK] would prevent the model\nfrom being influenced as such. As shown in figure\n5, AlephBERT achieves high scores both with bal-\nanced homographs as well as with homographs of\nhighly skewed distribution. Using a [MASK] token\ninstead of the actual word does not generally im-\nprove performance, whether or not the homographs\nare of skewed proportion.\nFew-Shot Scenarios In our experiments thus far,\nthe 10-fold cross-validation allows the MLP to\nleverage 90% of the data in each fold (hundreds of\nsentences for each analysis) in order to learn the\ndifference between the analyses. We now consider\nwhether the AlephBERT embeddings can suffice\non a few-shot basis, where the training stage has\naccess to only 100, 50, 25, 10 or even 5 examples\nof each analysis. In these cases, we train an MLP\nbased only on these few samples, and we use the\nrest of the sentences for evaluation. Astoundingly,\nas demonstrated in Figure 6, the AlephBERT em-\nbeddings provide a highly accurate solution even on\nthis few-shot basis. Even when training with only\n5 examples of each homograph analysis, Aleph-\nBERT reaches an accuracy that is not far below the\naccuracy achieved when performing full 10-fold\nCV across hundreds of sentences of each analysis.\nProbing Scenarios Finally, we probe the pre-\ntrained AlephBERT embeddings (Yaghoobzadeh\net al., 2019; Tenney et al., 2019; Klafka and Et-\ntinger, 2020; Belinkov, 2021) to see whether in\nFigure 6: Use of AlephBERT embeddings to differenti-\nate between homographs on a few-shot basis, contrasted\nwith scores from the full 10-fold CV (\"All\").\nand of themselves they reflect clusters which cor-\nrespond to different homograph analyses. We skip\nthe MLP, and instead use the raw embeddings di-\nrectly, classifying sentences based on their proxim-\nity to the centroid of the training samples for each\nhomograph analysis. As shown in the orange bars\nin Figure 6, this method generally does not per-\nform as well as the MLP-based method; however,\nthe degradation is limited to only a few percentage\npoints, indicating that the raw embeddings are gen-\nerally clustered in groups which indeed reflect the\ndistinctions between the analyses.\n5 Conclusion\nIn this study we have utilized a wide-ranging col-\nlection of Hebrew homograph challenge sets in\norder to evaluate the extent to which raw con-\ntextualized embeddings can be leveraged to dis-\nambiguate Hebrew homographs. We found that\ncontextualized embeddings can effectively disam-\nbiguate analyses of homographs, much more so\nthan non-contextualized ones, regarding multiple\ntypes of ambiguity: segmentation, morphosyntac-\ntic and sense. Yet, efficacy on pure sense ambiguity\nis lower than on the other two types. Additionally,\nan increasing number of splits, or an increasing\nnumber of different possible analyses of a token,\neach lower efficacy. Finally, we found that contex-\ntualized embeddings can function effectively for\nthis purpose on a few-shot basis, with as little as\n5 examples of each analysis. This indicates that\nwith relatively modest effort, highly ambiguous\nhomographs may be effectively treated.\n853\nLimitations\nOne of the major strengths of this paper is its new\nand comprehensive dataset for the training and\nbenchmarking Hebrew homograph disambiguation.\nThe dataset is uncomparable in size, quality and bal-\nance to all previous Hebrew homograph datasets;\nwe have made every effort to be as inclusive as\npossible in the creation of the dataset, making sure\nto include data from a widely diverse set of genres.\nNevertheless, a perennial challenge in corpus-based\nstudies is that the lion’s share of the available data\ntends to be authored by male writers. In order to\noffset this bias, we bolstered our corpus with a\nlarge corpus of texts specifically taken from blog\nsites devoted entirely to female bloggers. Even so,\nwe cannot escape the fact that female writing and\nfeminine conjugations are underrepresented in our\ndataset.\nA further limitation derives from our filter for\nsentences with offensive language. We perceived\nearly on that our human annotators were not com-\nfortable tagging sentences with offensive language,\nand we therefore took steps to remove such sen-\ntences from our corpus. Nevertheless, this means\nthat our resulting dataset is limited in that it does\nnot properly reflect the use of offensive language\nin naturally-occurring Hebrew sentences. Further-\nmore, our resulting tests and reported scores may\nnot accurately reflect the performance of our mod-\nels when applied to sentences with offensive lan-\nguage.\nEthics Statement\nCreation of the Dataset As noted, our dataset\ncontains over 150K sentences in all. Every sen-\ntence was reviewed and tagged by our team of hu-\nman annotators, who chose the relevant homograph\nanalysis for each instance of each of our 75 ho-\nmographs. Our annotator team included members\nof diverse genders and sexual orientations. They\nwere paid hourly wages with legal pay stubs. Their\nhourly wage was well above the minimum wage\nrequired by law. The entirety of the dataset will\nbe made available for research purposes with the\nacceptance of this article, together with the tagging\ninformation.\nRisks of the ResearchUltimately, this data will\nenable end-users to automatically diacritize and\nparse large corpora of Hebrew text. For the most\npart, this will provide a beneficial contribution to\nthe world: for the visually impaired, this technol-\nogy will enable the development of more precise\ntext-to-speech products; teachers will be able to\nprovide children and second-language learners with\naccessible diacritized texts; and humanities and lin-\nguistics researchers can bolster their research with\nbig-data analysis. However, there also is a risk\nof nefarious use, for instance, if an end user were\nto leverage these capabilities in order to produce\nanonymous texts or recordings containing threats\nto life, liberty, or happiness.\nAcknowledgments\nThe work of the third and last authors has been\nfunded by a grant from the Israeli Ministry of Sci-\nence and Technology (MOST) grant no. 3-17992.\nReferences\nMohammed Alaeddine Abderrahim and Mohammed\nEl-Amine Abderrahim. 2022. Arabic word sense\ndisambiguation for information retrieval. ACM Trans.\nAsian Low-Resour. Lang. Inf. Process., 21(4).\nMeni Adler and Michael Elhadad. 2006. An unsuper-\nvised morpheme-based hmm for hebrew morphologi-\ncal disambiguation. In COLING/ACL 2006 - 21st In-\nternational Conference on Computational Linguistics\nand 44th Annual Meeting of the Association for Com-\nputational Linguistics, Proceedings of the Confer-\nence, COLING/ACL 2006 - 21st International Con-\nference on Computational Linguistics and 44th An-\nnual Meeting of the Association for Computational\nLinguistics, Proceedings of the Conference, pages\n665–672, United States. Association for Computa-\ntional Linguistics (ACL). 21st International Con-\nference on Computational Linguistics and 44th An-\nnual Meeting of the Association for Computational\nLinguistics, COLING/ACL 2006 ; Conference date:\n17-07-2006 Through 21-07-2006.\nEneko Agirre and Philip Edmonds, editors. 2006. Word\nSense Disambiguation: Algorithms and Applications,\nvolume 33 of Text, Speech and Language Technology.\nSpringer.\nYonatan Belinkov. 2021. Probing Classifiers: Promises,\nShortcomings, and Advances. Computational Lin-\nguistics, pages 1–13.\nAvihay Chriqui and Inbal Yahav. 2021. Hebert hebemo:\na hebrew bert model and a tool for polarity analysis\nand emotion recognition.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning.\n854\nAli Elkahky, Kellie Webster, Daniel Andor, and Emily\nPitler. 2018. A challenge set and methods for noun-\nverb ambiguity. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 2562–2572, Brussels, Belgium.\nAssociation for Computational Linguistics.\nAllyson Ettinger. 2020. What BERT is not: Lessons\nfrom a new suite of psycholinguistic diagnostics for\nlanguage models. Transactions of the Association for\nComputational Linguistics, 8:34–48.\nMatt Gardner, Yoav Artzi, Victoria Basmova, Jonathan\nBerant, Ben Bogin, Sihao Chen, Pradeep Dasigi,\nDheeru Dua, Yanai Elazar, Ananth Gottumukkala,\nNitish Gupta, Hanna Hajishirzi, Gabriel Ilharco,\nDaniel Khashabi, Kevin Lin, Jiangming Liu, Nel-\nson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer\nSingh, Noah A. Smith, Sanjay Subramanian, Reut\nTsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou.\n2020. Evaluating models’ local decision boundaries\nvia contrast sets.\nYoav Goldberg. 2019. Assessing bert’s syntactic abili-\nties. CoRR, abs/1901.05287.\nNizar Habash and Owen Rambow. 2005. Arabic tok-\nenization, part-of-speech tagging and morphological\ndisambiguation in one fell swoop. In Proceedings\nof the 43rd Annual Meeting of the Association for\nComputational Linguistics (ACL’05), pages 573–580,\nAnn Arbor, Michigan. Association for Computational\nLinguistics.\nJosef Klafka and Allyson Ettinger. 2020. Spying on\nyour neighbors: Fine-grained probing of contex-\ntual embeddings for information about surrounding\nwords. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n4801–4811, Online. Association for Computational\nLinguistics.\nStav Klein and Reut Tsarfaty. 2020. Getting the ##life\nout of living: How adequate are word-pieces for mod-\nelling complex morphology? In Proceedings of the\n17th SIGMORPHON Workshop on Computational\nResearch in Phonetics, Phonology, and Morphology,\npages 204–209, Online. Association for Computa-\ntional Linguistics.\nLaroussi Merhben, Zouaghi Anis, and Mounir Zrigui.\n2010. Ambiguous arabic words disambiguation.\npages 157–164.\nLaroussi Merhbene, Anis Zouaghi, and Mounir Zrigui.\n2013. A semi-supervised method for Arabic word\nsense disambiguation using a weighted directed\ngraph. In Proceedings of the Sixth International Joint\nConference on Natural Language Processing, pages\n1027–1031, Nagoya, Japan. Asian Federation of Nat-\nural Language Processing.\nRoberto Navigli. 2009. Word sense disambiguation: A\nsurvey. ACM Comput. Surv., 41.\nAmit Seker, Elron Bandel, Dan Bareket, Idan\nBrusilovsky, Refael Shaked Greenfeld, and Reut Tsar-\nfaty. 2021. Alephbert:a hebrew large pre-trained lan-\nguage model to start-off your hebrew nlp application\nwith.\nDanny Shacham and Shuly Wintner. 2007. Morpho-\nlogical disambiguation of Hebrew: A case study in\nclassifier combination. In Proceedings of the 2007\nJoint Conference on Empirical Methods in Natural\nLanguage Processing and Computational Natural\nLanguage Learning (EMNLP-CoNLL), pages 439–\n447, Prague, Czech Republic. Association for Com-\nputational Linguistics.\nRushin Shah, Paramveer S. Dhillon, Mark Liberman,\nDean Foster, Mohamed Maamouri, and Lyle Ungar.\n2010. A new approach to lexical disambiguation of\nArabic text. In Proceedings of the 2010 Conference\non Empirical Methods in Natural Language Process-\ning, pages 725–735, Cambridge, MA. Association\nfor Computational Linguistics.\nAvi Shmidman, Joshua Guedalia, Shaltiel Shmidman,\nMoshe Koppel, and Reut Tsarfaty. 2020. A novel\nchallenge set for Hebrew morphological disambigua-\ntion and diacritics restoration. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, pages 3316–3326, Online. Association for\nComputational Linguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim, Ben-\njamin Van Durme, Samuel R. Bowman, Dipanjan\nDas, and Ellie Pavlick. 2019. What do you learn\nfrom context? probing for sentence structure in con-\ntextualized word representations.\nReut Tsarfaty, Dan Bareket, Stav Klein, and Amit Seker.\n2020. From SPMRL to NMRL: What did we learn\n(and unlearn) in a decade of parsing morphologically-\nrich languages (MRLs)? In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 7396–7408, Online. Association\nfor Computational Linguistics.\nReut Tsarfaty, Amit Seker, Shoval Sadde, and Stav\nKlein. 2019. What’s wrong with hebrew nlp? and\nhow to make it right.\nShuly Wintner. 2014. Morphological processing of\nsemitic languages. In NLP of Semitic Languages.\nYadollah Yaghoobzadeh, Katharina Kann, T. J. Hazen,\nEneko Agirre, and Hinrich Schütze. 2019. Probing\nfor semantic classes: Diagnosing the meaning con-\ntent of word embeddings. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 5740–5753, Florence, Italy. Asso-\nciation for Computational Linguistics.\nShlomo Yona and Shuly Wintner. 2005. A finite-state\nmorphological grammar of Hebrew. In Proceedings\nof the ACL Workshop on Computational Approaches\nto Semitic Languages, pages 9–16, Ann Arbor, Michi-\ngan. Association for Computational Linguistics.\n855\nMengjie Zhao, Philipp Dufter, Yadollah Yaghoobzadeh,\nand Hinrich Schütze. 2020. Quantifying the contextu-\nalization of word representations with semantic class\nprobing. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1219–1234,\nOnline. Association for Computational Linguistics.\n856\nA Appendix A: Table of Homographs\nWe present three tables of homographs, corresponding to the three categories of homographs discussed\nwithin the paper (segmentation ambiguity, morphosyntactic ambiguity, and semantic ambiguity). 6 In\neach table, the first column (\"form\") indicates the homograph as it is found in naturally-occurring non-\ndiacritized Hebrew text.7 The second column (\"word\") indicates the possible diacritizations of each form.\nIn cases where the diacritized word includes an attached prefix, a plus sign indicates the segmentation\npoint between the prefix letters and the primary word. Regarding all homographs considered in this\npaper, different segmentation options are diacritized differently. Thus, for each sentence in the dataset,\nour human annotators were asked simply to choose the correct diacritization for the target homograph\n(that is, they were asked to choose among the options listed in the \"word\" column). There was no need\nfor the annotators to separately tag the segmentation, because in all cases the choice of diacritization\nitself indicates the segmentation.8 The third column indicates the morphology of each of the possible\ndiacritizations.9 The fourth column lists the translation.10 Within each table the homographs are listed\nalphabetically.11\nA.1 Homographs with Segmentation Ambiguity\nForm Word Morphology12 Translation Sentences\nהאלַנִהַאִלַנִInterrogative does 1,000\nאֵלַנִ+הָדרֹוDet + Noun [F,S,abs] the + mother 1,000\nהמראהדרֹומַּר+הַדרֹוSConj + Ptcp [M,S] / Det + Noun [M,S,abs] that + indicates / the + sight 1,000\nמַּר+הַדרֹוSConj + Ptcp [F,S] / Det + Noun [F,S,abs] that + indicates / the + mirror 882\nהַמְרNoun [F,S,abs] takeoff 735\n6It should be noted that the higher level of ambiguity are supersets of the lower levels: segmentation ambiguities generally\nentail differences on the morphosyntactic and semantic levels as well, and morphosyntactic ambiguities generally also entail\nsemantic ambiguities. Furthermore, because many of the homographs admit to more than two analyses, it is often the case\nthat a subset of the analyses may form a lower level of ambiguity (e.g., just a semantic ambiguity), while other analyses form\na higher level of ambiguity (e.g. segmentation ambiguity). For the purposes of this paper, we categorize each homograph\naccording to the highest level of ambiguity involved: First, if a segmentation ambiguity is indicated anywhere across the\npossible analyses, then we include the homograph in the \"segmentation ambiguity\" category. Next, if there is no segmentation\nambiguity, but a morphosyntactic ambiguity is indicated anywhere across the possible analyses, then we include the homograph\nin the \"morphosyntactic ambiguity\" category. Finally, if the analyses all differ only on the semantic level, then we include the\nhomograph in the \"semantic ambiguity\" category.\n7The ranking of analyses is based on a frequency analysis of our in-house annotated corpus. It is worth emphasizing that the\npaper as a whole relates to each of the 75 homographs specifically as they are spelled in this list, and does not relate to cases\nwhere further prefixes are attached to the homographs. As a result, the frequency analysis may sometimes seem counterintuitive.\nFor instance, regarding the formראשידרֹוa native Hebrew speaker might intuit that the adjectival form is primary (רHowever,\nin practice, that sense is common only when prefixed with a definitive marker (הראשידרֹוIn contrast, the homograph considered\nhere involves the formראשידרֹוas is, without any prefixes; in this case, the other analyses are far more common.\n8It should be noted that in certain sentences, an exceedingly rare diacritization was warranted, which was not among the\noptions listed in the \"word\" column. The annotators were instructed to tag such cases as \"none of the above\", and all such\nsentences were removed from the corpus. Similarly, some sentences do not provide enough context to determine the correct\ndiacritization; the annotators were asked to tag such sentences as \"unclear\", and these sentences too were removed from the\ncorpus.\n9In most cases, a diacritized form has one specific morphological analysis. However, in other cases, the diacritized form can\nadmit to multiple morphologies. In such cases, we list all of the practically relevant morphological analyses in the third column,\nseparated by a slash (thus for instance in the case ofזרדרֹוRare analyses which hardly ever occur in practice are not listed.\n10Naturally, a given Hebrew term often captures a substantial range of potential English translations, and it would not be\npractical to list them all in this column; therefore, we generally present only a single representative translation.\n11Ideally, we might have grouped the homographs based on part-of-speech instead. However, as can be seen from the following\ntables, the 75 homographs vary so widely in terms of the parts of speech that they can represent, such that alphabetical listing\nwas deemed most useful.\n12Key to morphology abbreviations: F=Feminine; M=Masculine; S=Singular; P=Plural; abs=absolute; cons=construct;\n1,2,3=First Person, Second Person, Third Person; suf=suffix; Det = determiner; SConj=Subordinating Conjunction;\nAdj=Adjective; Conj=Conjunction; Prep=Preposition; Propn=Proper Noun; Adv=Adverb; Ptcp=Participle; ACC=Accusative\nmarker.\n857\nהקפהדרֹוקָּפֶהדרֹו+הַדרֹוDet + Noun [M,S,abs] the + coffee 1,000\nהַקָּפָהדרֹוNoun [F,S,abs] credit 500\nהקשרדרֹוקֶּשׁ+הַדרֹוDet + Noun [M,S,abs] the + connection 1,000\nקַּהסֵגַדתֹדנִהסנִהס+הַדרֹוDet + Noun [M,S,abs] the + signaler 1,000\nהֶקְשׁNoun [M,S,abs/cons] context 588\nהשלמהדרֹוהַשׁNoun [F,S,abs] completion 1,000\nהסֵגַדתֹדנִהסנִהס+הַדרֹוDet + Adj [F,S] the + complete 906\nועדדרֹועַדדרֹו+וConj + Prep and + until 1,000\nוNoun [M,S,abs/cons] committee 519\nלשלַנִלְשׁPrep for the purpose of 1,000\nשׁ+לְדרֹוPrep + Adverb to + there 1,000\nמבחינהדרֹובְּחִיסתַמַקנ+מִדרֹוPrep + Noun [F,S,abs] from + point of view 1,000\nמַבְחִיסתַמַקנParticiple [F,S,abs] she notices 607\nמסיבותדרֹומְסִיבּוֹתדרֹוNoun [F,P,abs] parties 1,000\nסִּיבּוֹתדרֹו+מִדרֹוPrep + Noun [F,P,abs/cons] due to + reasons 1,000\nמפתחדרֹומַפְתֵּחַדרֹוNoun [M,S,abs] key 1,000\nמְפַתֵּחַדרֹוParticiple [M,S,abs] develops / developer 329\nפֶּתַחדרֹו+מִדרֹוPrep + Noun [M,S,abs/cons] from + opening 206\nשאלהדרֹושׁNoun [F,S,abs] question 1,000\nשׁVerb [F,S,3,Past] she asked 1,000\nאֵלֶּהדרֹו+שׁSConj + Pronoun [MF,P,3] that + these 228\nשאלַנִאַלַנִ+שׁSConj + CConj for + even 1,000\nשׁVerb [M,S,3,Past] he aspired 665\nשבהדרֹובָּהּדרֹו+שׁSconj + Prep [suf=F,S,3] that + in it 1,000\nשׁVerb [F,S,Present/Past] she returns / she returned 1,000\nשמלַנִשׁNoun [M,S,abs/cons] oil 1,000\nשׁAdj [M,S,abs] wide 335\nמִּלַנִ+שׁSConj + Prep that + from 207\nשׁNoun [M,S,abs,suf=F,P,3] their name 149\nשמרדרֹושׁVerb [M,S,3,Past] he guarded 1,000\nשׁPropn Shemer 872\nמַּרדרֹו+שׁSConj + Titular [M,S] that + Mr. 224\nA.2 Homographs with Morphosyntactic Ambiguity\nForm Word Morphology Translation Sentences\nאהבהדרֹואַהֲבָהדרֹוNoun [F,S,abs] love 1,000\nאָהֲבָהדרֹוVerb [F,S,3,Past] she loved 1,000\nאוכלדרֹואֹוכֶלדרֹוNoun [M,S,abs/cons] food 748\nאוּכַלדרֹוModal [MF,S,1,Future] I can 729\nאוֹכֵלדרֹוParticiple [M,S,abs/cons] eats 640\nאחדותדרֹואֲחָדוֹתדרֹוDet [F,P,abs] several 1,000\nאַחְדוּתדרֹוNoun [F,S,abs] unity 1,000\n858\nאחיודרֹואֶחָיודרֹוNoun [MF,P,abs,suf=M,S,3] his brothers 1,000\nאָחִיודרֹוNoun [MF,S,abs,suf=M,S,3] his brother 774\nאלימותדרֹואַלִּימוֹתדרֹוAdj [F,P] violent 1,000\nאַלִּימוּתדרֹוNoun [F,S,abs/cons] violence 1,000\nאלַנִאִלַנִConj if 1,000\nאֵלַנִNoun [F,S,abs/cons] mother 1,000\nאמצעידרֹואֶמְצָעֵידרֹוNoun [M,P,cons] centers of / methods of 1,000\nאֶמְצָעִידרֹוNoun [M,S,abs] / Adj [M,S] method / central 969\nאמרהדרֹואָמְרVerb [F,S,3,Past] she said 1,000\nאִמְרNoun [F,S,abs] a saying 343\nאפשרדרֹואֶפְשׁModal / Adv possible 1,000\nאִפְשׁVerb [M,S,3,Past] he allowed 511\nאתדרֹואֶתדרֹוACC accusative 1,000\nאַתְּדרֹוPronoun [F,S,2] you 1,000\nבהמשלַנִהֶמְשׁ+בְּדרֹוPrep + Noun [M,S,cons] in + continuation of 1,000\nהֶמְשׁ+בַּדרֹוPrep [with Det] + Noun [M,S,abs] in the + future 930\nבחיידרֹוחַיּ+בְּדרֹוPrep + Noun [M,P,cons] in + lives of 1,000\nחַיּ+בְּדרֹוPrep + Noun [M,P,abs,suf=MF,S,1] in + my life 1,000\nבעוללַנִעוֹלָלַנִ+בָּדרֹוPrep [with Det] + Noun [M,S,abs] in the + world 1,000\nעוֹלַלַנִ+בְּדרֹוPrep + Noun [M,S,cons] in + a world of 913\nעוֹלָלַנִ+בְּדרֹוPrep + Noun [M,S,abs] in + a world 485\nבקרבדרֹוקֶר+בְּדרֹוPrep + Noun [M,S,cons] in + midst of 1,000\nקְּר+בַּדרֹוPrep [with Det] + Noun [M,S,abs] in the + battle 1,000\nקְר+בִּדרֹוPrep + Verb [Bare Infinitive] in + approaching of 734\nקְר+בִּדרֹוPrep + Noun [M,S,abs] in + a battle 256\nגילודרֹוגּVerb [MF,P,3,Past] they discovered 1,000\nגּNoun [M,S,abs,suf=M,S,3] his age 859\nדידרֹודּAdverb sufficiently 1,000\nדּDet [cons] enough of 828\nדּPrefix di- 808\nהזקלַנִזּ+הַדרֹוDet + Noun [M,S,abs] the + beard 739\nזּ+הַדרֹוDet + Adj [M,S] / Det + Noun [M,S,abs] the + old / the + elderly man 533\nהחלדרֹוהֵחֵלדרֹוVerb [M,S,3,Past] he began 1,000\nהָחֵלדרֹוVerb [Bare Infinitive] starting (from) 1,000\nהמשנהדרֹומִּשׁ+הַדרֹוDet + Noun [M,S,abs] the + deputy 1,000\nמִּשׁ+הַדרֹוDet + Propn [F,S,abs] the + Mishna 1,000\nהנחהדרֹוהַנּNoun [F,S,abs] placing 1,000\nהִנVerb [M,S,3,Past] he directed 731\nהֲסתַמַקנNoun [F,S,abs] discount 517\nהרילַנִהֵרVerb [M,S,3,Past] he lifted 1,000\nהָרNoun [M,P,abs] mountains 1,000\nואתדרֹואֶתדרֹו+וConj + ACC and + accusative 1,000\nאַתְּדרֹו+וConj + Pronoun [F,S,2] and + you 1,000\nזרדרֹוזNoun [M,S,abs/cons] bouquet 1,000\nזAdj [M,S] / Noun [M,S,abs] foreign / stranger 1,000\n859\nחברותדרֹוחֲבָרוֹתדרֹוNoun [F,P,abs] companies 1,000\nחֶבְרוֹתדרֹוNoun [F,P,cons] companies of 1,000\nחֲבֵרוֹתדרֹוNoun [F,P,abs/cons] friends 501\nחֲבֵרוּתדרֹוNoun [F,S,abs/cons] friendship 398\nחַבְרוֹתדרֹוNoun [F,P,cons] friends of 261\nחדרדרֹוחֲדNoun [M,S,cons] room of 1,000\nחֶדNoun [M,S,abs] room 1,000\nחָדVerb [M,S,3,Past] penetrated 783\nטובדרֹוטוֹבדרֹוAdj [M,S] good 1,000\nטוּבדרֹוNoun [M,S,abs/cons] goodness 357\nיהודידרֹויNoun [M,S,abs] / Adj [M,S] a Jew / Jewish 1,000\nיNoun [M,P,cons] Jews 1,000\nכיוולַנִכֵּיוConj because 487\nכִּיוּוּלַנִNoun [M,S,abs] / Noun [M,S,cons] direction 468\nכִּיוּVerb [M,S,3,Past] directed 455\nלודרֹולוֹדרֹוPrep [suf=M,S,3] to him 1,000\nלוּדרֹוConj if only 1,000\nלחלַנִלֶחֶלַנִNoun [M,S,abs] bread 1,000\nלָחַלַנִVerb [M,S,3,Past] he fought 1,000\nלפנותדרֹולִפְנוֹתדרֹוPrep / Verb [Infinitive] facing / to turn 1,000\nלְפַנּוֹתדרֹוVerb [Infinitive] to clear out 570\nמדידרֹומִדּDet [cons] every 1,000\nמִדּAdv too much 802\nמַדּNoun [M,P,cons] uniforms of 541\nמהלַנִמֵהֶלַנִPreposition [suf=M,P,3] from them 1,000\nמָהֵלַנִInterrogative what are 587\nמידרֹומִידרֹוInterrogative / Pronoun [S,3] who 1,000\nמֵידרֹוNoun [M,P,cons] waters of 1,000\nמללַנִמֶלֶַצֵהסלַנִNoun [M,S,abs/cons] king 1,000\nמָלַַצֵהסלַנִVerb [M,S,3,Past] he ruled 522\nמעברדרֹומֵעֵבֶרדרֹוPrep beyond 1,000\nמַעֲבָרדרֹוNoun [M,S,abs] passage 1,000\nמַעֲבַרדרֹוNoun [M,S,cons] passage of 883\nמראהדרֹומַרParticiple [M,S] he shows 1,000\nמַרParticiple [F,S] she shows 1,000\nמרכזדרֹומֶרNoun [M,S,cons] center of 1,000\nמֶרNoun [M,S,abs] center 1,000\nמְרParticiple [M,S,abs/cons] organizes / organizer 393\nמשחקדרֹומִשׂNoun [M,S,abs] game 1,000\nמְשׂParticiple [M,S,abs] plays / player 479\nנעשהדרֹונVerb [MF,P,1,Future] we will do 1,000\nנVerb [M,S,3,Past] was done 1,000\nנשילַנִסתַמַקנNoun [F,P,abs] women 1,000\nסתַמַקנVerb [MF,P,1,Future] we will put 613\nנתלַנִסתַמַקנVerb [M,S,3,Past] gave 1,000\nסתַמַקנPropn Nathan 681\n860\nעברדרֹועָבַרדרֹוVerb [M,S,3,Past] he passed 1,000\nעָבָרדרֹוNoun [M,S,abs] past 1,000\nעֵבֶרדרֹוNoun [M,S,abs/cons] side 456\nעדדרֹועַדדרֹוPrep until 1,000\nעֵדדרֹוNoun [M,S,abs/cons] witness 1,000\nעובדותדרֹועסתNoun [F,P,abs/cons] facts 1,000\nעוֹבְדוֹתדרֹוParticiple [F,P] they work / workers 1,000\nעלַנִעִלַנִPrep with 1,000\nעַלַנִNoun [M,S,abs/cons] nation 1,000\nפנידרֹופְּנNoun [M,P,cons] faces of 1,000\nפָּנNoun [MF,P,abs,suf=MF,S,1] my face 338\nפרסדרֹופְּרNoun [M,S,abs] award 1,000\nפֶּרPropn Peres 956\nפָּרVerb [M,S,3,Past] he spread 630\nפְּרNoun [M,S,cons] award of 290\nציולַנִצִיּוֹלַנִPropn Zion 1,000\nצִיּוּלַנִNoun [M,S,abs/cons] mark 1,000\nקודלַנִקֹודAdv before 1,000\nקוֹדAdj [M,S] previous 1,000\nקסתVerb [M,S,3,Past] was promoted 284\nראשידרֹורNoun [M,P,cons] heads 1,000\nרNoun [M,S,abs,suf=MF,S,1] my head 881\nרAdj [M,S,abs] head 399\nשירתדרֹושׁVerb [M,S,3,Past] he served 1,000\nשׁNoun [F,S,cons] poetry of 896\nשכרדרֹושׂNoun [M,S,abs] salary 1,000\nשׂNoun [M,S,cons] salary of 751\nשׂVerb [M,S,3,Past] rented 681\nשלַנִשׁNoun [M,S,abs/cons] name 1,000\nשׁAdv there 1,000\nשׂVerb [M,S,Present/Past] he placed 1,000\nתנאידרֹותְּסתַמַקנNoun [M,P,cons] conditions of 1,000\nתְּנNoun [M,S,abs/cons] condition 834\nA.3 Homographs with Semantic Ambiguity\nForm Word Morphology Translation Sentences\nהזמרדרֹוזּ+הַדרֹוDet + Noun [M,S,abs] the + music 1,000\nזּ+הַדרֹוDet + Noun [M,S,abs] the + musician 1,000\n861\nהסופרדרֹוסּוּפֶּרדרֹו+הַדרֹוDet + Noun [M,S,abs] the + market 763\nסּוֹפֵרדרֹו+הַדרֹוDet + Noun [M,S,abs] the + author 570\nזמרדרֹוזNoun [M,S,abs] musician 1,000\nזNoun [M,S,abs] song 602\nחברהדרֹוחֲבֵרNoun [F,S,abs] friend 1,000\nחֶבְרNoun [F,S,abs] company 1,000\nרשותדרֹורNoun [F,S,abs/cons] permission 1,000\nרNoun [F,S,abs/cons] authority 1,000\n862\nB Appendix B: Creation of our\nAnnotated Dataset\nWe present here further details regarding the cre-\nation of our annotated homograph dataset. As\nnoted above, our dataset contains challenge sets\nfor 75 Hebrew homographs. In almost all cases,\nwe collect 1000 sentences for the primary analysis,\nat least 500 sentences for the secondary analysis,\nand at least 250 for each additional analysis. Ev-\nery sentence was tagged by our team of human\nannotators.\nThe main challenge that we faced is that it is of-\nten extraordinarily difficult to identify a sufficient\nnumber of naturally-occurring Hebrew sentences\nattesting to the non-primary homograph analyses.\nIn many cases, the naturally-occurring ratio of\nthese analyses is 100:1 or worse, meaning that the\nnon-primary analyses only occur once in 100 or\nmore sentences. In such cases, prima facie, we\nwould need to tag some 50,000 randomly-selected\ncases of the homograph in order to gather 500 cases\nof the alternate analysis. It would not be practical\nto tag this number of sentences for each of the 75\nhomographs. Therefore, we devised the following\nthree-step process to allow us to efficiently gather\na sufficient number of sentences attesting each pos-\nsible analysis of each homograph:\n1. For each homograph, we first gather 4000\nrandomly-selected sentences from an untagged He-\nbrew corpus. For each sentence, our annotation\nteam determines the correct analysis of the homo-\ngraph. This initial step virtually always provides\nus with a sufficient number of sentences attesting\nto the primary analysis, and sometimes for the sec-\nondary analysis as well. However, it often leaves us\nwith an insufficient number of sentences regarding\nthe non-primary analyses.\n2. In order to find additional sentences for the\nnon-primary homograph analyses, we train classi-\nfiers to identify a wide net of potential candidate\nsentences. For these classifiers, we use the known\ncases of the primary homograph analysis (iden-\ntified in the previous step) as the first class, and\nwe use various proxies to represent the opposing\nclass. The proxies used for the opposing class\ninclude the following: (A) Hebrew words which\nunambiguously function morphosyntactically in a\nmanner that is different from the morphosyntactic\nfunction of the primary class; (B) Hebrew words\nwhose word2vec embedding is maximally distant\nfrom that of the homograph in question (because,\nin practice, for homographs of skewed proportions,\nthe word2vec embedding of the word tends to rep-\nresent the primary usage); (C) Randomly-selected\nHebrew words. These classifiers are trained with\na BiLSTM encoding of the word2vec embeddings\nof the four neighbors on either side of the target\nword (not including the target word itself). Essen-\ntially, the point of these classifiers is to identify\na generous selection of cases in which the homo-\ngraphs may possibly be functioning in a way that\nis different than their primary usage.\n3. We run these classifiers on all instances of the\ntarget homograph in our untagged Hebrew corpus.\nCases that are classified as the opposing class are\nsent to our annotators for human verification. In\npractice, these proxy-based classifiers reach a preci-\nsion of 15-30% on the task of identifying sentences\nin which the target homographs function according\nto a non-primary analysis. Thus, on average, for\nany given homograph, tagging an additional 2222\nsentences allows us to collect 500 relevant cases of\nthe secondary analysis.\nOur procedure ensures diversity while keeping\nthe process cost-effective. All in all it took approx-\nimately 1500 hours of annotation time to amass the\nsentences in the present dataset.\nC Appendix C: Computational\nEquipment\nWe performed all computations on a desktop work-\nstation with an i9-10980XE processor and 256GB\nof memory. This system enabled us to run 36 exper-\niments in parallel (the processor contains 18 hyper-\nthreaded cores), and thus we were able to complete\nall of the relevant experiments and computations\nover the course of several weeks of calendar time.\nD Appendix D: Neural Network\nImplementation Details\nAll BiLSTMs and MLPs were trained using dynet\n(http://dynet.io/). All MLPs are constructed with a\nhidden layer of size 100, and with two layers. We\ntrain with the Adam optimizer at a learning rate of\n.001, for 3 epochs.\nOur word2vec embeddings for Hebrew are of\nsize 100. We trained them on a 500M-word Hebrew\ncorpus using word2vecf ( https://github.com/\nBIU-NLP/word2vecf, adding position info to con-\ntext words, and excluding words with a frequency\nof less than seven. All in all, our word2vec vocabu-\nlary covers 94.6% of the tokens in our dataset. For\n863\nthe out-of-vocabulary words, we use a trainable\nUNK parameter in place of the word2vec embed-\nding, which is trained from scratch for each “word\nexpert” classifier.\nAs per the \"word expert\" paradigm, a completely\nseparate MLP is trained for each homograph. In\neach case, the possible homograph analyses are\neach treated as a possible class for prediction, and\nthe MLP is trained to choose from among those\nclasses. Thus, for instance, if the homograph has\ntwo analyses, we train an MLP to predict one of the\ntwo classes; if the homograph has three analyses,\nthen we train an MLP to predict one of the three\nclasses; and so on.\nFor the Probing Scenarios based on centroid clas-\nsification, we proceed as follows. For each of the\nhomographs, given the training sample size (100,\n50, 25, etc.), we randomly select that amount of\ntraining sentences for each of the possible analyses\nof the homograph. We calculate the centroid for\neach of the analyses by averaging the embeddings\nof the target homographs within the corresponding\ntraining sentences. The remainder of the available\nsentences for the homograph forms the test set.\nWe classify them by calculating the dot product\nof the embedding of the target homograph in each\ngiven test sentence with the centroid of each of\nthe homograph analyses. We run this process 200\ntimes, each time selecting a different random set of\ntraining sentences. The values plotted in Figure 6\nreflect the average of the F1 scores across these 200\nrounds. For the corresponding MLP-based experi-\nments presented for comparison in the aforemen-\ntioned table, we follow an analogous procedure,\nacross 10 rounds.\n864"
}