{
  "title": "Large language models capsule: A research analysis of In-Context Learning (ICL) and Parameter-Efficient Fine-Tuning (PEFT) methods",
  "url": "https://openalex.org/W4392107358",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2791176758",
      "name": "Wu Haokun",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W4280534475",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4330338093",
    "https://openalex.org/W3178802061",
    "https://openalex.org/W3205068155"
  ],
  "abstract": "In the context of natural language processing (NLP), this paper addresses the growing need for efficient adaptation techniques for pre-trained language models. It begins by summarizing the current landscape of NLP, highlighting the challenges associated with fine-tuning large language models like BERT and Transformer. The paper then introduces and analyzes three categories of parameter-efficient fine-tuning (PEFT) approaches, namely, In-Context Learning (ICL)-inspired Fine-Tuning, Low-Rank Adaptation PEFTs (LoRA), and Activation-based PEFTs. Within these categories, it explores techniques such as prefix-tuning, prompt tuning, (IA)3, and LoRA, shedding light on their advantages and applications. Through a comprehensive examination, this paper concludes by emphasizing the interplay between performance, parameter efficiency, and adaptability in the context of NLP models. It also provides insights into the future prospects of these techniques in advancing the field of NLP. To summarize, this paper offers a detailed analysis of PEFT methods and their potential to democratize access to cutting-edge NLP capabilities, paving the way for more efficient model adaptation in various applications.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7998474836349487
    },
    {
      "name": "Language model",
      "score": 0.6859954595565796
    },
    {
      "name": "Transformer",
      "score": 0.6255102753639221
    },
    {
      "name": "Fine-tuning",
      "score": 0.5753114223480225
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5551608800888062
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5136365294456482
    },
    {
      "name": "Adaptability",
      "score": 0.49755242466926575
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.4974062740802765
    },
    {
      "name": "Field (mathematics)",
      "score": 0.4231330156326294
    },
    {
      "name": "Natural language processing",
      "score": 0.3970668315887451
    },
    {
      "name": "Machine learning",
      "score": 0.3395974636077881
    },
    {
      "name": "Engineering",
      "score": 0.09357547760009766
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}