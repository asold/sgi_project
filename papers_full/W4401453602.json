{
    "title": "Speed and Conversational Large Language Models: Not All Is About Tokens per Second",
    "url": "https://openalex.org/W4401453602",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2095793212",
            "name": "Javier Conde",
            "affiliations": [
                "Universidad Politécnica de Madrid"
            ]
        },
        {
            "id": "https://openalex.org/A2099857573",
            "name": "Miguel González",
            "affiliations": [
                "Universidad Politécnica de Madrid"
            ]
        },
        {
            "id": "https://openalex.org/A1951092153",
            "name": "Pedro Reviriego",
            "affiliations": [
                "Universidad Politécnica de Madrid"
            ]
        },
        {
            "id": "https://openalex.org/A2135624771",
            "name": "Zhen Gao",
            "affiliations": [
                "Tianjin University"
            ]
        },
        {
            "id": "https://openalex.org/A2096897347",
            "name": "Shan-Shan Liu",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A2117064855",
            "name": "Fabrizio Lombardi",
            "affiliations": [
                "Northeastern University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4367595583",
        "https://openalex.org/W4390041933",
        "https://openalex.org/W6854866820",
        "https://openalex.org/W4253707770",
        "https://openalex.org/W6782879696",
        "https://openalex.org/W6852874933",
        "https://openalex.org/W4205729510",
        "https://openalex.org/W6853048723",
        "https://openalex.org/W6862863749",
        "https://openalex.org/W4379260375",
        "https://openalex.org/W4380353763",
        "https://openalex.org/W4392271821",
        "https://openalex.org/W4384918448"
    ],
    "abstract": "The speed of open-weights large language models (LLMs) and its dependency on the task at hand, when run on GPUs, is studied to present a comparative analysis of the speed of the most popular open LLMs.",
    "full_text": "Speed and Conversational Large Language \nModels (LLMs): Not All Is About Tokens per \nSecond \nJavier Conde, ETSI de Telecomunicación, Universidad Politécnica de Madrid, 28040 Madrid, Spain  \nMiguel González, ETSI de Telecomunicación, Universidad Politécnica de Madrid, 28040 Madrid, Spain \nPedro Reviriego, ETSI de Telecomunicación, Universidad Politécnica de Madrid, 28040 Madrid, Spain  \nZhen Gao, Tianjin University, 300072 Tianjin, China  \nShanshan Liu, University of Electronic Science and Technology of China, 611731 Chengdu, China \nFabrizio Lombardi, Northeastern University, Boston, MA 02115, USA\nAbstract—The speed of open-weights Large Language Models and its dependency on the task at \nhand, when run on GPUs is studied to present a comparative analysis of the speed of the most \npopular open LLMs. The results suggest that existing token-based speed metrics do not \nnecessarily correlate with the time needed to complete different tasks.     \nA few years ago, a Large Language Model (LLM) was un \nunknow entity. Today, following the introduction of ChatGPT 1 \nby OpenAI in November 2022, it is hard to find someone that \ndoes not know conversational LLMs as many have used them. \nThe widespread adoption of LLMs has fostered the \ndevelopment of advanced models and tools based on these \nmodels, such as GPT42 by OpenAI, or Gemini3 by Google and \nconsisting of hundreds of billions of parameters. Unfortunately, \nthese models are closed and can only be accessed through the \nuser interfaces, tools or application programming interfaces \nprovided by the companies that developed the models. Their \nparameters and implementation details are not publicly \navailable and even if they were, their huge size would make \ntheir execution on commodity computing devices unfeasible. A \ndifferent approach has been taken by some large companies \nsuch as Meta, i.e. the code as well as the parameters or \nweights of LLMs such as LLaMa4 have been released for public \nuse. This approach has been followed by startups such as \nMistral releasing several open models, such as Mistral-7B 5, \n01.AI with the Yi models and more recently Google with the\nGemma models; all of these models have been publicly\nreleased.\nThese open LLMs provide all the parameters and code \nneeded to run the models locally, but they typically do not \ndisclose the data and procedures used for training and may \nplace some restrictions on the use of the model, so they are not \nstrictly speaking open-source. Therefore, next we refer to them \nas open-weights or simply open LLMs. In any case, open LLMs \ncreate new avenues for innovation and for democratizing \naccess to LLMs.  \nPerformance of LLMs in different tasks is thoroughly \nevaluated using benchmarks 6 that test their knowledge on a \nwide range of topics and their ability to perform many different \ntasks, like reasoning or problem solving in different areas.  \nThere are even public arenas7 in which users can compare \nmodels and mark their preferences. These performance \ncomparisons can guide users when selecting a model for a \ngiven task or application; however, performance on the task \nat hand is not the only metric of interest, cost and speed are \nalso important.  \nIn the case of closed commercial models, the cost is \ndetermined by their service rates typically charging per \ntoken; the speed depends on both their capacity to serve \nmany clients and the limits of requests per unit of time set \nin their user agreements. For open models, the cost \ndepends on whether the models are run locally or on the \ncloud. In the first case, there is a one-time investment on \nthe computing infrastructure and operating expenses \nrelated for example to energy consumption or technical \nsupport. For cloud deployments, the cost is typically related \nto the amount of time that the allocated computing \nresources are used.   \nIn both cases, LLMs’ speed and memory usage are \nimportant factors because they determine the hardware \nrequirements and to some extent the energy needed to \nsupport a given number of requests per second which, in \nturn impact cost for both cloud and local deployments. The \nmemory needed by LLMs is roughly proportional to the \nnumber of parameters and the format used to represent \nthem. However, the understanding of the speed of LLMs is \nmore complex because it depends on different factors such \nas the target computing unit (i.e., typically a Graphics \nProcessing Unit, GPU), the model architecture, the format \nof the parameters, and the number of requests that are \nprocessed at the same time. In this paper we pursue the \nevaluation of the speed of several open-weights LLMs of \nsimilar sizes when run on GPUs. The results suggest that \nexisting token-based speed metrics do not necessarily \ncorrelate with the time needed to complete different tasks. \nThis article has been accepted for publication in Computer.This is the accepted version which has not been fully edited and content may change prior to final publication.\n©2024 IEEE | DOI: 10.1109/MC.2024.3399384\nFinal-published version: https://ieeexplore.ieee.org/document/10632720\n. \nThe availability of powerful open LLMs that can be modified, \nintegrated with other applications and run locally, has spurred \nan ecosystem with hundreds of LLMs of different sizes, \nparameter formats, languages supported, and customization \nfor a given task. These LLMs are readily available, and several \ntools and libraries have been developed to ease the execution \nof the models. \nA common feature of most open-weights LLMs is that they \ncan be run on a single computing unit, typically a GPU. This \nenables their use on commodity hardware such as computers \nwith a GPU, or on single GPU instances, on the cloud. The \nmain limitation to run on a single unit is the memory, low-end \nGPUs are typically equipped with a few GB of memory while \nhigh-end GPUs have tens of GB of memory. Each parameter \nof a LLM requires 2-4 bytes of memory when using traditional \nformats such as half and single precision floating-point \nrepresentations9. Therefore, even when using half precision \nfloating point, a 7 billion parameter model such as Mistral 7B \nrequires approximately 14 GB of memory. This makes running \nlarger models such as LlaMa-70B rather challenging, because \n140 GB of memory is required, a memory size that is not \navailable even in high-end GPUs. To address this issue, the \nopen-source community has proposed alternative formats10 for \nthe parameters that require fewer bits, typically 8 or 4 and even \nclose to 1 bit 11. These formats enable for example running \nLlaMa-70B with 4 bits per parameter on a GPU that has only \n40 GB of memory.   \nTo evaluate the speed of executing LLMs, the first step is to \nselect a subset of models.  Evaluating all open models is \nunfeasible and most of them are fine-tuned versions of other \nmodels, so we expect them to have similar speed as the base \nmodel. In our investigation we focus on five models from four \ncompanies with similar sizes, around 7 billion parameters. In \nthis case we can use the same format, 16-bit floating-point, for \nall of them so that comparisons are fair. The first two are \nLLaMa-24 and LLaMa-3 models from Meta with sizes 7B and \n8B; a model from Mistral with 7B, another from 01.AI with 6B, \nand the last one from Google with 7B parameters (the models \nwere taken from Huggingface, their exact names are “Llama-2-\n7b-chat-hf\", \"Meta-Llama-3-8B-Instruct\", \"Mistral-7B-Instruct-\nv0.1\", \"Yi-6B-Chat\" and \"gemma-7b-it\", respectively). The \nmodels selected for evaluation are summarized in Table 1. This \nselection covers a wide range of models of a similar size that \ncan be run on a single GPU; they are models that have been \nwidely used as base to derive fine-tuned models. Therefore, the \nresults obtained can be to some extent extrapolated to the \nderivative models.   \n  TABLE 1. Models evaluated \nModel Company Sizes \nLLaMa-2/3 Meta 7B/8B \nGemma Google 7B \nMistral Mistral 7B \nYi 01.AI 6B \nAs said, for the parameter format, we have used \nfloating-point formats with 16 bits for all models. This \nenables both running the models on a 40 GB memory and \nmaking a fair comparison among them. The results and \ninsights obtained would be similar when using other \nformats, for example 8 or 4 bits. \nFinally, in terms of computing platform, we consider the \nuse of GPUs because they are widely used and accessible \nboth locally and on the cloud. We used a high-end GPU \nfrom NVIDIA: the A100 with 40GB of memory which is \nenough memory to run all models in Table 1 as discussed \nbefore.   \nModern GPUs have a large amount of processing units \nand when running a single prompt for a LLM, only a small \nfraction of those units is used. Since the parameters of the \nLLM are already in the GPU memory, they can be used to \nrun several prompts at the same time, so creating batches \nof prompts that are run together. This can provide in many \ncases a significant speed up. The use of batches is possible \nwhen users run sets of prompts rather than individual \nprompts. For example, when creating questions for a test \nwe can ask the LLMs to create questions on different topics, \nso using different prompts. Batches are also used when the \nGPU serves many users whose requests can be grouped. \nTherefore, we evaluate the models with different batch \nsizes to study the impact of batch size on the speed.  \nThe speed of LLMs is typically measured by the number \nof tokens generated per second, or the time needed to \ngenerate a given number of tokens, for example 256 8. \nThese metrics are easy to compute but they have some \nimportant limitations when we want to compare LLMs. The \nfirst limitation is that for two LLMs that have different \ntokenizers, it is not a fair comparison because the number \nof tokens for the same text are different. A second limitation \nis that even if two LLMs use the same tokenizer, then they \nmay produce a different number of output tokens for the \nsame task and thus, measuring the tokens per second does \nnot fully capture the speed seen by the user. These \nlimitations boil down to the fact that as users, we want to \nmeasure the speed of the model when performing a task, \nnot the number of tokens generated. \nTHE OPEN-WEIGTHS LLM ECOSYSTEM \nEVALUATING LLM SPEED \nThis article has been accepted for publication in Computer.This is the accepted version which has not been fully edited and content may change prior to final publication.\n©2024 IEEE | DOI: 10.1109/MC.2024.3399384\nFinal-published version: https://ieeexplore.ieee.org/document/10632720\nTo better understand the relative speed of LLMs, instead of \nmeasuring tokens, we measure the time needed to complete \ndifferent tasks. First, we select 660 miscellaneous questions \nfrom a multiple-choice LLM benchmark6 and use them for three \ntasks: \n1) Select the right choice for the multiple-choice questions :\nthe LLM only must generate the response with the selected\nchoice (a, b, c, d). The model may produce additional text\nto explain its answer even though the prompt explicitly\nasks to answer only with the selected choice.\n2) Paraphrase the multiple-choice questions without the\nanswers: the LLM generates a text of similar size to the\ninput text.\n3) Answer the questions providing an explanation for the\nanswer selected: the LLMs generate a textual answer with\nno constraints.\nThe overall approach is illustrated in Figure 1. The first task \nis designed to measure LLM speed when used to answer \nquestions that require almost no text generation, just the option \nselected and a few additional words. The second task is \ndesigned to enable a comparison of LLMs when producing a \nsimilar amount of text as paraphrasing leaves little room for \nvariations in text lengths. Finally, the third task is designed for \nLLMs to generate text freely to assess whether different LLMs \ngenerate texts of different lengths for the same prompts and its \neffect on speed.  \nThe results for the first task are summarized in Figure 2. \nThe plot on the left shows the time needed on average to \ngenerate a token (which corresponds to the process by which \nspeed is commonly measured for LLMs), and in the middle, the \ntime needed to answer all questions. The number of output \ntokens is shown on the right plot and is smaller than the number \nof input tokens (as expected). The two slowest models in terms \nof time to generate a token (LLaMa3-8B and Yi-6B) are among \nthe fastest to complete the task. This can be explained by \nlooking at the number of input and output tokens. The number \nof input tokens depends only on the tokenizer used by each \nmodel, because the input texts are the same for all models, the \ndifferences are small with LLaMa3-8B and Gemma using fewer \ntokens. Instead, the output texts depend on the responses of \nthe models to the questions and there are large variations \nacross models. The models that produce fewer tokens, are \nLLaMa3-8B and Yi-6B, so explaining the reasons for \ncompleting the tasks in less time than the other models. This \nclearly shows that the speed in generating tokens does not \nalways correspond to the speed observed by the user. \nThe results for the second task are summarized in \nFigure 3. In this case, looking at the right plot, the number \nof tokens generated is rather similar to the number of input \ntokens as the models are just paraphrasing the input. The \ncorrelation between time per token and time to complete the \ntask is better in this case, but there are still significant \ndifferences. For example, Mistral 7B is significantly faster in \nterms of time to complete the task than in generating tokens \nwhen compared to other models. This can be explained \nagain by considering the number of tokens generated. The \ndifferences are still significant even for a task in which \nmodels are asked to paraphrase the input text. \nFinally, the results for the third task are summarized in \nFigure 4.  Again, the results in terms of token generation \ndo not correspond with those of the time needed to \ncomplete the task. LLaMa-2-7B is fast in generating \ntokens, but among the slowest in completing the task. \nInstead, the Gemma model is the fastest because it \ngenerates substantially fewer tokens.  \nThese results illustrate the complexity of evaluating \nthe speed of LLMs (and thus also energy dissipation) \nunder a fair comparison; even when considering the same \nhardware, the same prompts, and models of similar sizes, \nthe time needed to complete a given task can be \nsignificantly different. Additionally, the relative \nperformance of the models does not always correlate with \ntheir speed in generating tokens due to the use of different \ntokenizers and different models generate different \nnumber of tokens for the same prompt; moreover, the \ntime to complete a task for the same model may be \ndifferent depending on its parameters, such as \ntemperature.   \nTherefore, speed metrics in terms of tokens per \nsecond or the time to generate a given number of tokens \nshould be very carefully taken into consideration. A \ndetailed evaluation per task is needed to truly understand \nthe speed of different LLMs for a given scenario. A \npotential alternative is to develop speed benchmarks that \nare focused on specific tasks to complement existing per \ntoken metrics. For example, the definition of a set of input \ndatasets and tasks (such as translation, summarization, \nquestion answering or essay writing) and the use of the \ntime to complete the tasks must be utilized as part of the \nspeed evaluation metrics in addition to the number of \ntokens per second. The availability of such benchmarks \nwould enable a more comprehensive evaluation and \ncomparison of the speed and energy dissipation of LLMs.\nThis article has been accepted for publication in Computer.This is the accepted version which has not been fully edited and content may change prior to final publication.\n©2024 IEEE | DOI: 10.1109/MC.2024.3399384\nFinal-published version: https://ieeexplore.ieee.org/document/10632720\nFIGURE 1. Overview of the evaluation procedure for the speed of an LLM. \nFIGURE 2. Results for task 1 answering 660 multiple choice questions: time to generate a token (left) time to complete the task (middle) \nand number of input and output tokens (right)  \nFIGURE 3. Results for task 2 paraphrasing 660 multiple choice questions: time to generate a token (left) time to complete the task \n(middle) and number of input and output tokens (right)  \nFIGURE 4. Results for task 3 open answers to 660 multiple choice questions: time to generate a token (left) time to complete the task \n(middle) and number of input and output tokens (right)  \nThis article has been accepted for publication in Computer.This is the accepted version which has not been fully edited and content may change prior to final publication.\n©2024 IEEE | DOI: 10.1109/MC.2024.3399384\nFinal-published version: https://ieeexplore.ieee.org/document/10632720\n CONCLUSION \nAs LLMs are widely used, their speed and energy \ndissipation have become a key issue. Existing \nbenchmarks for assessing the speed of LLMs focus on \nthe time needed to generate several tokens, or the \nnumber of tokens generated per second. However, the \ncritical figures of merit are the time and energy needed to \ncomplete a given task. In this paper, we have studied the \nperformance of several widely used open-weights LLMs \nwhen performing three simple tasks. The results show \nthat the time needed to complete each task does not \nnecessarily correlate to the number of tokens that the LLM \ncan generate per second. This is due to the difference in \nthe tokenizers, but more importantly on the lengths of the \ntexts generated by each model for the same task. It also \nshows the complexity of evaluating LLM speed in realistic \napplications. To address this challenge, an alternative \napproach is to develop task-oriented benchmarks that are \nrepresentative of LLM used cases. Such benchmarks \ncould be more informative as to the relative speed of \nLLMs when performing a given task. \n ACKNOWLEDGMENTS \nThis work was supported by the Agencia Estatal de \nInvestigación (AEI) (doi:10.13039/501100011033) under \nGrant FUN4DATE (PID2022-136684OB-C22), by the \nEuropean Commission through the Chips Act Joint \nUndertaking project SMARTY (Grant no. 101140087) and \nby NVIDIA with a donation of GPUs. \n REFERENCES \n1. T. Wu, S. He, J. Liu, S. Sun, K. Liu, Q.-L. Han, and Y. Tang,\n“A Brief Overview of ChatGPT: The His- tory, Status Quo and \nPotential Future Development,” IEEE/CAA Journal of\nAutomatica Sinica, vol. 10, no. 5, pp. 1122–1136, 2023.\n2. OpenAI, “GPT-4 Technical Report”, ArXiv e-prints , 2023.\ndoi:10.48550/arXiv.2303.08774.\n3. Gemini Team, “Gemini: A Family of Highly Capable\nMultimodal Models”, ArXiv e-prints , 2023.\ndoi:10.48550/arXiv.2312.11805.\n4. H. Touvron et al, “Llama 2: Open Foundation and Fine-\nTuned Chat Models”, ArXiv e-prints , 2023.\ndoi:10.48550/arXiv.2307.09288.\n5. A. Q. Jiang et al, “Mistral 7B”, ArXiv e-prints , 2023.\ndoi:10.48550/arXiv.2310.06825.\n6. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D.\nSong, and J. Steinhardt, “Measuring massive multitask\nlanguage understanding,” in International Conference on\nLearning Representations, 2021.\n7. L. Zheng et al, “Judging LLM-as-a-Judge with MT-Bench and\nChatbot Arena”, ArXiv e-prints , 2023.\ndoi:10.48550/arXiv.2306.05685.\n8. I. Moutawwakil and R. Pierrard, “LLM-Perf Leaderboard”,\nHugging Face, 2023.\n9. \"IEEE Standard for Floating-Point Arithmetic,\" in IEEE Std\n754-2019, vol., no., pp.1-84, 22 July 2019, doi:\n10.1109/IEEESTD.2019.8766229.\n10. J. Lin, “AWQ: Activation-aware Weight Quantization for LLM\nCompression and Acceleration”, ArXiv e-prints , 2023.\ndoi:10.48550/arXiv.2306.00978.\n11. S. Ma et al “The Era of 1-bit LLMs: All Large Language\nModels are in 1.58 bits”, ArXiv e-prints , 2024. \narXiv:2402.17764.\nJavier Conde is an assistant professor at the ETSI de \nTelecomunicación, Universidad Politécnica de Madrid, \n28040 Madrid, Spain. Contact him at \njavier.conde.diaz@upm.es \nMiguel González is a researcher at the ETSI de \nTelecomunicación, Universidad Politécnica de Madrid, \n28040 Madrid, Spain. Contact him at \nmiguel.gonsaiz@upm.es \nPedro Reviriego is an associate professor at the ETSI de \nTelecomunicación, Universidad Politécnica de Madrid, \n28040 Madrid, Spain. Contact him at \npedro.reviriego@upm.es \nZhen Gao is an associate professor at Tianjin University, \n300072 Tianjin, China. Contact him at zgao@tju.edu.cn \nShanshan Liu is a professor at the University of Electronic \nScience and Technology of China, Chengdu 611731, \nSichuan, China. Contact her at ssliu@uestc.edu.cn \nFabrizio Lombardi is a professor at Northeastern \nUniversity, Boston, MA 02115, USA. Contact him at \nlombardi@coe.northeastern.edu \nThis article has been accepted for publication in Computer.This is the accepted version which has not been fully edited and content may change prior to final publication.\n©2024 IEEE | DOI: 10.1109/MC.2024.3399384\nFinal-published version: https://ieeexplore.ieee.org/document/10632720"
}