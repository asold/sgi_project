{
  "title": "Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models",
  "url": "https://openalex.org/W4389519998",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3158468532",
      "name": "Ziqiao Ma",
      "affiliations": [
        "University of Michigan–Ann Arbor"
      ]
    },
    {
      "id": "https://openalex.org/A2958634108",
      "name": "Jacob Sansom",
      "affiliations": [
        "University of Michigan–Ann Arbor"
      ]
    },
    {
      "id": "https://openalex.org/A2240247960",
      "name": "Run Peng",
      "affiliations": [
        "University of Michigan–Ann Arbor"
      ]
    },
    {
      "id": "https://openalex.org/A2160072504",
      "name": "Joyce Chai",
      "affiliations": [
        "University of Michigan–Ann Arbor"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4244756464",
    "https://openalex.org/W4298112127",
    "https://openalex.org/W187745948",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4232598244",
    "https://openalex.org/W2736601468",
    "https://openalex.org/W2068999456",
    "https://openalex.org/W4394663744",
    "https://openalex.org/W2787640378",
    "https://openalex.org/W4241719668",
    "https://openalex.org/W4308757009",
    "https://openalex.org/W4383058631",
    "https://openalex.org/W4256089541",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4220992060",
    "https://openalex.org/W4378474033",
    "https://openalex.org/W2105873910",
    "https://openalex.org/W4302335333",
    "https://openalex.org/W4385571325",
    "https://openalex.org/W4376122961",
    "https://openalex.org/W2970062726",
    "https://openalex.org/W1974396627",
    "https://openalex.org/W2089484622",
    "https://openalex.org/W4386566752",
    "https://openalex.org/W2001246417",
    "https://openalex.org/W2134694734",
    "https://openalex.org/W2897037347",
    "https://openalex.org/W2605369869",
    "https://openalex.org/W2005142040",
    "https://openalex.org/W2056245254",
    "https://openalex.org/W4387427450",
    "https://openalex.org/W2082513494",
    "https://openalex.org/W4362508448",
    "https://openalex.org/W1879397024",
    "https://openalex.org/W4378942694",
    "https://openalex.org/W4221153246",
    "https://openalex.org/W4328049044",
    "https://openalex.org/W4287324479",
    "https://openalex.org/W2018248933",
    "https://openalex.org/W2970536767",
    "https://openalex.org/W4287323935",
    "https://openalex.org/W4286850188",
    "https://openalex.org/W4248309594",
    "https://openalex.org/W4385571259",
    "https://openalex.org/W3201531807",
    "https://openalex.org/W2141538250",
    "https://openalex.org/W4385567134",
    "https://openalex.org/W2942685071",
    "https://openalex.org/W4321277158",
    "https://openalex.org/W4307534981",
    "https://openalex.org/W4323030557",
    "https://openalex.org/W3213241618",
    "https://openalex.org/W4389009528",
    "https://openalex.org/W4285107714",
    "https://openalex.org/W4318200376",
    "https://openalex.org/W2132423339",
    "https://openalex.org/W57640832",
    "https://openalex.org/W2889107415",
    "https://openalex.org/W1509191362",
    "https://openalex.org/W4385572854",
    "https://openalex.org/W2111638337",
    "https://openalex.org/W4287891157",
    "https://openalex.org/W2159056499",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3199296065",
    "https://openalex.org/W2982320652",
    "https://openalex.org/W2059146861",
    "https://openalex.org/W2133602857",
    "https://openalex.org/W2758442112",
    "https://openalex.org/W1969432340",
    "https://openalex.org/W4389520279",
    "https://openalex.org/W2568315666",
    "https://openalex.org/W4378508547",
    "https://openalex.org/W3173798466",
    "https://openalex.org/W3025570388",
    "https://openalex.org/W2767257667",
    "https://openalex.org/W3180258554",
    "https://openalex.org/W1983840885",
    "https://openalex.org/W4385571775",
    "https://openalex.org/W1537738093",
    "https://openalex.org/W3196345292",
    "https://openalex.org/W3200664512",
    "https://openalex.org/W4387595873",
    "https://openalex.org/W4389519950",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W2141738416",
    "https://openalex.org/W3000621992",
    "https://openalex.org/W4221159132",
    "https://openalex.org/W2116139040",
    "https://openalex.org/W4385768089",
    "https://openalex.org/W4246494218",
    "https://openalex.org/W2431139695",
    "https://openalex.org/W2093410327",
    "https://openalex.org/W4367189299",
    "https://openalex.org/W4385572114",
    "https://openalex.org/W2516219727",
    "https://openalex.org/W3160674997",
    "https://openalex.org/W4230158074"
  ],
  "abstract": "Large Language Models (LLMs) have generated considerable interest and debate regarding their potential emergence of Theory of Mind (ToM). Several recent inquiries reveal a lack of robust ToM in these models and pose a pressing demand to develop new benchmarks, as current ones primarily focus on different aspects of ToM and are prone to shortcuts and data leakage. In this position paper, we seek to answer two road-blocking questions: (1) How can we taxonomize a holistic landscape of machine ToM? (2) What is a more effective evaluation protocol for machine ToM? Following psychological studies, we taxonomize machine ToM into 7 mental state categories and delineate existing benchmarks to identify under-explored aspects of ToM. We argue for a holistic and situated evaluation of ToM to break ToM into individual components and treat LLMs as an agent who is physically situated in environments and socially situated in interactions with humans. Such situated evaluation provides a more comprehensive assessment of mental states and potentially mitigates the risk of shortcuts and data leakage. We further present a pilot study in a grid world setup as a proof of concept. We hope this position paper can facilitate future research to integrate ToM with LLMs and offer an intuitive means for researchers to better position their work in the landscape of ToM.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1011–1031\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nTowards A Holistic Landscape of\nSituated Theory of Mind in Large Language Models\nZiqiao Ma Jacob Sansom Run Peng Joyce Chai\nComputer Science and Engineering Division, University of Michigan\n{marstin,jhsansom,roihn,chaijy}@umich.edu\nAbstract\nLarge Language Models (LLMs) have gener-\nated considerable interest and debate regarding\ntheir potential emergence of Theory of Mind\n(ToM). Several recent inquiries reveal a lack of\nrobust ToM in these models and pose a pressing\ndemand to develop new benchmarks, as current\nones primarily focus on different aspects of\nToM and are prone to shortcuts and data leak-\nage. In this position paper, we seek to answer\ntwo road-blocking questions: (1) How can we\ntaxonomize a holistic landscape of machine\nToM? (2) What is a more effective evaluation\nprotocol for machine ToM? Following psycho-\nlogical studies, we taxonomize machine ToM\ninto 7 mental state categories and delineate ex-\nisting benchmarks to identify under-explored\naspects of ToM. We argue for a holistic and situ-\nated evaluation of ToM to break ToM into indi-\nvidual components and treat LLMs as an agent\nwho is physically situated in environments and\nsocially situated in interactions with humans.\nSuch situated evaluation provides a more com-\nprehensive assessment of mental states and po-\ntentially mitigates the risk of shortcuts and data\nleakage. We further present a pilot study in\na grid world setup as a proof of concept. We\nhope this position paper can facilitate future\nresearch to integrate ToM with LLMs and of-\nfer an intuitive means for researchers to better\nposition their work in the landscape of ToM.\n1 Introduction\nThe term theory of mind (ToM, sometimes also\nreferred to as mentalization or mindreading) was\nfirst introduced by Premack and Woodruff (1978)\nas agents’ ability to impute mental states to them-\nselves and others. Many aspects of human cog-\nnition and social reasoning rely on ToM model-\ning of others’ mental states (Gopnik and Wellman,\n1992; Baron-Cohen, 1997; Gunning, 2018). This\nis crucial for understanding and predicting others’\nactions (Dennett, 1988), planning over others’ be-\nliefs and next actions (Ho et al., 2022), and various\nforms of reasoning and decision-making (Pereira\net al., 2016; Rusch et al., 2020). Inspired by hu-\nman ToM, AI researchers have made explicit and\nimplicit efforts to develop a machine ToM for so-\ncial intelligence: AI agents that engage in social\ninteractions with humans (Krämer et al., 2012;\nKennington, 2022) and other agents (Albrecht and\nStone, 2018). Likewise, a machine ToM can en-\nhance agents’ capacity for interactions (Wang et al.,\n2021), explainable decision-making (Akula et al.,\n2022), dialogue communication (Qiu et al., 2022;\nTakmaz et al., 2023), and collaborative task plan-\nning (Bara et al., 2023).\nMachine ToM has received an increasing amount\nof attention in the research community, especially\nas the field is reshaped by large language mod-\nels (LLMs) such as ChatGPT (OpenAI, 2022) and\nGPT-4 (OpenAI, 2023). This highlights an ongo-\ning debate and discussion on whether a machine\nToM has emerged in LLMs. While LLMs have\ndemonstrated some capability of inferring commu-\nnicative intentions, beliefs, and desires (Andreas,\n2022; Kosinski, 2023; Bubeck et al., 2023), re-\nsearchers also reported concerns regarding a lack\nof robust agency in LLMs for complex social and\nbelief reasoning tasks (Sap et al., 2022; Shapira\net al., 2023a) and in-context pragmatic communi-\ncation (Ruis et al., 2022). Emerged or not emerged,\nthat remains a question (or may not even be the\ncentral question to ask). In our view, existing eval-\nuation protocols do not fully resolve this debate.\nMost current benchmarks focus only on a (few)\naspect(s) of ToM, in the form of written stories,\nand are prone to data contamination, shortcuts, and\nspurious correlations (Trott et al., 2022; Aru et al.,\n2023; Shapira et al., 2023a). Prior to embarking on\nextensive data collection for new ToM benchmarks,\nit is crucial to address two key questions: (1) How\ncan we taxonomize a holistic landscape of machine\nToM? (2) What is a more effective evaluation pro-\ntocol for machine ToM?\n1011\nTo embrace the transformation brought by LLMs\nand explore their full potential in understanding\nand modeling ToM, this position paper calls for a\nholistic investigation that taxonomizes ToM using\nthe Abilities in Theory of Mind Space (ATOMS)\nframework (Beaudoin et al., 2020). After a review\nof existing benchmarks under this framework, we\nput forward a situated evaluation of ToM, one that\ntreats LLMs as agents who are physically situated\nin environments and socially situated in interac-\ntions with humans. We hope this paper will offer\nan intuitive means to identify research priorities\nand to help gain a deeper understanding of, as well\nas to effectively utilize, LLMs in ToM modeling\nfor AI agents in the future.\n2 Large Language Models as Theory of\nMind Agents\nSince the advent of pre-trained language models,\nthe research community has questioned whether\nthey possess intrinsic mental states to represent the\nenvironment (Li et al., 2021; Storks et al., 2021;\nHase et al., 2023) and comprehend the mental states\nof others (Sap et al., 2019; Zhang et al., 2021)\nthrough the textual description (observation) of be-\nhavioral cues. The relatively recent breakthroughs\nof LLMs have created many discussions and de-\nbates, primarily concerning the extent to which\nLLMs possess various capabilities required for a\nmachine ToM. In this section, we first survey recent\nresearch presenting evidence and counter-evidence\nfor the emergence of ToM in LLMs. We conclude\nthe discussion with the limitations of current evalu-\nation protocols.\n2.1 Do Machine ToM Emerge in LLMs?\nEvidence for emergent ToM in LLMs. Prior to\nthe rise of large language models, there has been\ngrowing evidence and acknowledgment of a narrow\nand limited sense of agency in smaller language\nmodels. Andreas (2022) argues that language mod-\nels have the capacity to predict relations between\nagents’ observations, mental states, actions, and ut-\nterances, as they infer approximate representations\nof beliefs, desires, and intentions of agents men-\ntioned in the context. These representations have\na causal influence on the generated text, similar\nto an intentional agent’s state influencing its com-\nmunicative actions under a Belief-Desire-Intention\n(BDI) agent model (Bratman, 1987). Amidst the\nexcitement surrounding the release of GPT-4 (Ope-\nnAI, 2023), researchers have searched for evidence\nof an emergent ToM in LLMs. Kosinski (2023)\npresents 20 case studies each of the unexpected\ncontents task (Perner et al., 1987) and the unex-\npected transfer (Sally-Anne) task (Baron-Cohen\net al., 1985). With direct comparisons to children’s\nperformance, the findings have been cited as poten-\ntial evidence for a spontaneous emergence of ToM\nin LLMs. Bubeck et al. (2023) present a similar\nbehavioral study with 10 cases of belief, emotion,\nand intention understanding, concluding that GPT-\n4 has an advanced level of ToM after qualitative\ncomparison with predecessors. Other case studies\nhave also shown aspects of machine ToM (Li et al.,\n2023; Holterman and van Deemter, 2023).\nLimitations of ToM capabilities in LLMs. The\nabove findings contradict the conclusions drawn\nin Sap et al. (2022)’s earlier study, which shows a\nclear lack of ToM in GPT-3 (Brown et al., 2020)\non SOCIAL IQA (Sap et al., 2019) and TOMI (Le\net al., 2019) benchmarks. As a potential account,\nthere has been criticism that the cognitive inquiries\nare anecdotal and inadequate for evaluating ToM\nin LLMs (Marcus and Davis, 2023; Mitchell and\nKrakauer, 2023; Shapira et al., 2023a). Follow-\ning the same evaluation protocol, Ullman (2023)\ndemonstrates that simple adversarial alternatives to\nKosinski (2023) can fail LLMs. To further under-\nstand if the most recent variants of LLMs possess\na robust ToM, Shapira et al. (2023a) present a com-\nprehensive evaluation over 6 tasks and 3 probing\nmethods, showing that a robust machine ToM is\nabsent even in GPT-4 and that LLMs are prone to\nshortcuts and spurious correlations. Based on the\nongoing debate, it can be concluded that, while\nLLMs exhibit some level of sensitivity at under-\nstanding others’ mental states, this capability is\nlimited and falls short of achieving robust human-\nlevel ToM (Trott et al., 2022; Shapira et al., 2023a).\n2.2 Roadblocks in ToM Evaluation in LLMs\nGiven the pressing need for a robust machine ToM\nin LLMs and large-scale ToM benchmarks, re-\nsearchers echo several difficulties in the evaluation\nprotocol. Presently, ToM benchmarks suffer from\nthree primary issues summarized as follows.\nLimited aspects of ToM. The evaluation of ma-\nchine ToM lacks consistency in the literature due\nto the ambiguity surrounding the specific mental\nstates being targeted. Existing benchmarks often\nfocus on limited numbers of mental states, such as\nthe intention (Yoshida et al., 2008), belief (Grant\n1012\net al., 2017), emotion (Sap et al., 2019), and knowl-\nedge (Bara et al., 2021) of another agent. While all\nof these are necessary building blocks of machine\nToM, we echo Shapira et al. (2023a)’s concern that\nthe ToM capability of LLMs may have been over-\nclaimed based on evaluations from only a specific\naspect of ToM. To give a comprehensive assess-\nment of a holistic machine ToM, a taxonomy is\nessential to enable researchers to effectively posi-\ntion their work with different focuses and priorities,\nwhich may be orthogonal to each other.\nData contamination. Data contamination refers\nto the lack of a verifiable train-test split that is typi-\ncally established to test the ability of machine learn-\ning models to generalize (Magar and Schwartz,\n2022). LLMs typically learn from internet-scale\ndata, potentially giving them access during training\nto the data used to test them (Bubeck et al., 2023;\nHagendorff, 2023). For ToM evaluation specifi-\ncally, the training corpora of LLMs may contain\nresearch papers detailing these psychological stud-\nies. Many past studies used identical or slightly\naltered language prompts to test LLMs, leading\nto potential contamination issues (Ullman, 2023).\nTo critically evaluate the performance of LLMs\non ToM tasks, researchers must have access to the\ndatasets used to train them (Dodge et al., 2021),\nwhich are unfortunately not available.\nShortcuts and spurious correlations. The avail-\nability of shortcuts and spurious features has trig-\ngered many concerns that a model may leverage\nthem to perform highly on a benchmark without ro-\nbustly acquiring the desired skill (Sclar et al., 2023;\nUllman, 2023; Shapira et al., 2023a). Recent find-\nings suggest that LLMs tend to learn surface-level\nstatistical correlations in compositional tasks, po-\ntentially leading to an illusion of systematic learn-\ning (Dziri et al., 2023). In all likelihood, LLMs\nare capable of learning ToM shortcuts in a similar\nmanner.\n3 Towards A Holistic Landscape of\nMachine Theory of Mind\n3.1 Abilities in Theory of Mind Space\n(ATOMS) Framework\nThe evaluation of machine ToM lacks clarity and\nconsistency across various literature, primarily due\nto the ambiguity surrounding the specific mental\nstates being targeted. This ambiguity is not unique\nto the field of AI but is rooted in the complicated\ncognitive underpinnings of ToM. At the core of\nthis ambiguity is the latent nature of mental states,\nthe subject has privileged access to them while\nothers can only infer the existence of these men-\ntal states based on observable behaviors or expres-\nsions (Dretske, 1979; Blakemore and Decety, 2001;\nZaki et al., 2009). Thus, it is impossible to di-\nrectly access and assess the mental states of a hu-\nman, and ToM must be tested indirectly through\nhumans’ ability to understand the relationship be-\ntween mental states and behaviors, especially by\npredicting how agents behave based on their mental\nstates (Swettenham, 1996; Phillips et al., 2002).\nWhile the exact definition of ToM remains a cen-\ntral debate, the AI community can benefit from\nlooking at what psychologists have viewed as an\ninitial step. In this paper, we follow Beaudoin et al.\n(2020)’s taxonomy of ToM sub-domains, i.e., the\nAbilities in Theory of Mind Space (ATOMS). As\nshown in Figure 1, the space consists of 7 cate-\ngories of mental states, including beliefs, inten-\ntions, desires, emotions, knowledge, percepts, and\nnon-literal communication. We selected this taxon-\nomy because it was derived from a comprehensive\nmeta-analysis of ToM studies. The meta-analysis\nfocused on young children aged 0-5 years at the\nearly stage of cognitive development, such that the\nsetups are simpler and more comparable, avoiding\ncomplicated physical and social engagements that\ncannot be trivially deployed on LLMs.\nBeliefs. Beliefs are informational states that peo-\nple judge to be true, usually decoupled from moti-\nvational states (Dennett, 1995; Eccles and Wig-\nfield, 2002). Beliefs, the most studied mental\nstates in the field of ToM, are usually tested in\nthe form of false belief tasks, including the unex-\npected contents test (Perner et al., 1987), the un-\nexpected transfer (Sally-Anne) Test (Baron-Cohen\net al., 1985), the second-order false belief (Ice-\ncream Van) Test (Perner and Wimmer, 1985). Re-\nsearchers also studied their connection to actions\nand emotions (Swettenham, 1996).\nIntentions. Intentions are choices with commit-\nment, usually associated with concrete actions to-\nwards a goal (Cohen and Levesque, 1990). As a\ncritical component of ToM, Kennington (2022) has\ncalled for a more explicit treatment of intentions.\nIntentions have been extensively explored in psy-\nchology tests, e.g., behavioral re-enactment (Melt-\nzoff, 1995), action prediction (Phillips et al., 2002),\nintention explanation (Smiley, 2001), and intention\nattribution to abstract figures (Castelli, 2006).\n1013\nAbilities \nin Theory \nof Mind \nSpace \n(ATOMS)\nPercepts\nPercept-Action Link\nVisual Perspective Taking\nAuditory Perspective Taking\nDesires\nMultiple Desires\nDiscrepant Desires\nDesire-Action Contradiction\nDesire-Influenced Action/Emotions\nKnowledge\nPercepts-Knowledge Link\nKnowledge-Attention Link\nInformation-Knowledge Link\nKnowledge-Pretend Play Link\nEmotions\nMoral Emotions\nMixed Emotions\nHidden Emotions\nEmotion Regulation\nDiscrepant Emotions\nComprehensive Measures\nTypical Emotional Reactions\nAtypical Emotional Reactions\nNon-literal \nCommunication\nHumor\nWhite Lies\nIrony/Sarcasm\nEgocentric Lies\nInvoluntary Lies\nFaux-Pas/Social Gaffe\nComprehensive Measures\nBeliefs\nIdentity False Beliefs\nSecond-Order Beliefs\nContent False Beliefs\nLocation False Beliefs\nComprehensive Measures\nBelief-Based Action/Emotions\nIntentions\nIntention Attribution\nIntention Explanation\nPrediction of Actions\nDiscrepant Intentions\nCompletion of Failed Actions\nFigure 1: The ATOMS framework of Beaudoin et al. (2020), which identified 7\ncategories of mental states through meta-analysis of ToM studies for children.\nBeliefIntentionDesireEmotionknowledgeperceptNon-literalComm.\n0\n2\n4\n6\n8\n10# Available Benchmarks\n10\n11\n4\n5\n3\n5\n3\nFigure 2: Number of avail-\nable benchmarks for each\nmental state in ATOMS.\nSymmetricity\nSocial\nSituatedness\nPhysical\nSituatedness\nInput\nModality\nsymmetric\n3\nasymmetric\n18\nperceiver\n6\ninteractor\n5\nNone\n10\nperceiver\n12\ninteractor\n3\nNone\n6\ntextual\n12\nenvironment\n4\nvisual\n5\nFigure 3: A comparison of\nbenchmark settings.\nBenchmarks and Task FormulationsTested Agent Situatedness ATOMS Mental States Sym.Task Input Modality Physical Social Belief IntentionDes. Emo. Know. Per. NLCText Nonling. Per. Int. Per. Int. 1st 2nd+ Act. Com.\nEPISTEMICREASONING(Cohen, 2021)Infer T - ✓ ✓TOMI(Nematzadeh et al., 2018) QA T - ✓ ✓ ✓HI-TOM(He et al., 2023) QA T - ✓ ✓ ✓MINDGAMES(Sileo and Lernould, 2023)Infer T - ✓ ✓ ✓ ✓ADV-CSFB(Shapira et al., 2023a)QA H - ✓ ✓CONVENTAIL(Zhang and Chai, 2010)Infer H - ✓ ✓ ✓ ✓SOCIALIQA(Sap et al., 2019) QA H - ✓ ✓ ✓BEST(Tracey et al., 2022) - H - ✓ ✓ ✓ ✓FAUXPAS-EAI(Shapira et al., 2023b)QA H,AI - ✓ ✓ ✓COKE(Wu et al., 2023) NLG AI - ✓ ✓ ✓ ✓TOM-IN-AMC(Yu et al., 2022) Infer H - ✓ ✓ ✓ ✓G4C(Zhou et al., 2023b) NLG H,AI - ✓ ✓ ✓ ✓ ✓ ✓VISUALBELIEFS(Eysenbach et al., 2016)Infer - Cartoon✓ ✓ ✓TRIANGLECOPA(Gordon, 2016) QA H Cartoon ✓ ✓ ✓ ✓MSED(Jia et al., 2022) Infer H Images✓ ✓ ✓BIB(Gandhi et al., 2021) Infer - 2D Grid ✓ ✓ ✓AGENT(Shu et al., 2021) Infer - 3D Sim.✓ ✓ ✓ ✓MTOM(Rabinowitz et al., 2018)Infer - 2D Grid ✓ ✓ ✓SYMMTOM(Sclar et al., 2022) MARL- 2D Grid✓ ✓ ✓ ✓ ✓ ✓MINDCRAFT(Bara et al., 2021) Infer H 3D Sim. ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓CPA(Bara et al., 2023) Infer H 3D Sim.✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓\nTable 1: A taxonomized review of existing benchmarks for machine ToM and their settings under ATOMS. We\nfurther break beliefs into first-order beliefs (1st) and second-order beliefs or beyond (2nd+); and break intentions\ninto Action intentions and Communicative intentions. Tasks are divided into Inference, Question Answering,\nNatural Language Generation, and MultiAgent Reinforcement Learning. Input modalities consist of Text (Human,\nAI, or Template) and Nonlinguistic ones. The latter further breaks into Cartoon, Natural Images, 2D Grid World,\nand 3D Simulation. The Situatedness is divided into None, Passive Perceiver, and Active Interactor. Symmetricity\nrefers to whether the tested agent is co-situated and engaged in mutual interactions with other ToM agents.\nDesires. Desires are motivational states that do\nnot necessarily imply commitment, though they\nare usually emotionally charged and affect ac-\ntions (Malle and Knobe, 2001; Kavanagh et al.,\n2005). Typical studies along this line include the\nYummy-Yucky Task (Repacholi and Gopnik, 1997)\nfor discrepant preferences from different individu-\nals, the multiple desires within one individual (Ben-\nnett and Galpert, 1993), and the relationship be-\ntween desires and emotions/actions (Wellman and\nWoolley, 1990; Colonnesi et al., 2008).\nEmotions. Emotions are mental states associated\nwith an individual’s feelings and affective expe-\nriences, which could impact beliefs and behav-\niors (Frijda et al., 1986; Damasio, 2004). Most\nToM studies on emotions focus on typical (Knafo\net al., 2009) and atypical (Denham, 1986) emo-\ntional reactions to situations. Other studies also\nencompass affective perspective taking (Borke,\n1971), understanding hidden emotions (Harris et al.,\n1986), and morally related emotions (Pons and Har-\nris, 2000).\n1014\nKnowledge. Many controversies revolve around\nthe definition of knowledge as justified true be-\nliefs (Gettier, 2000). In the context of AI, knowl-\nedge typically consists of information and orga-\nnized representations of the world, which can be\nused to simplify understanding and address intri-\ncate reasoning and planning (Schank and Abelson,\n2013). ToM studies usually involve understanding\nthe absence of knowledge (Aronson and Golomb,\n1999) as well as the connection between knowl-\nedge and perception (Ruffman and Olson, 1989)\nand attention (Moll et al., 2006).\nPercepts. Humans are situated in the physical\nand social environments. To enable AI agents to\noperate in the world and communicate with hu-\nmans, the sensory and social aspects of perception\nare crucial in a machine ToM. Along this line, psy-\nchological studies have investigated the perceptual\nperspective taking (Masangkay et al., 1974) and\nunderstanding the influence of limited perception\non actions (Hadwin et al., 1997).\nNon-literal communications. Being able to un-\nderstand non-literal and figurative communication\nhelps humans to perform pragmatic inference and\nreason about hidden words behind their written\nmeanings (Giora, 2003). Non-literal communica-\ntion has been recognized as an advanced ToM ca-\npability, spanning a wide spectrum of humor and\ndeceptions (Happé, 1994), sarcasm (Sullivan et al.,\n1995), and faux-pas (social gaffe) situations (Baron-\nCohen et al., 1999).\n3.2 A Taxonomized Review of Benchmarks\nThe ATOMS framework can serve as an intuitive\nreference for researchers to identify their research\npriorities and situate their work better in the land-\nscape of literature. We further take the initiative to\nprovide a systematic review of existing benchmarks\nfor machine ToM under the umbrella of ATOMS.1\nAlthough there are independent research initiatives\non certain ToM facets like intention classification,\nemotion modeling, and aspects of non-literal com-\nmunications, we primarily focus on those that ex-\nplicitly target ToM or inferences of latent mental\nstates. Besides the ToM dimensions in ATOMS, we\nfurther characterize the benchmarks on their task\nformulation, input modalities, physical and social\nsituatedness, and symmetricity (whether the tested\nagent is co-situated and engaged in mutual interac-\n1We maintain a repository for relevant literature at https:\n//github.com/Mars-tin/awesome-theory-of-mind .\ntions with other ToM agents). We summarize our\nreview in Table 1 and discuss our observations and\nunder-explored aspects of ToM evaluation.\nMany aspects of ToM are under-explored. As\nshown in Figure 2, we notice an overwhelming re-\nsearch focus on the intention and belief aspects of\nmachine ToM. Several other aspects of ToM have\nnot received enough attention. While the field of\nNLP has thoroughly explored different facets of\nemotion and non-literal communication, e.g., in the\ncontext of dialogue systems, ToM has rarely been\nexplicitly mentioned as motivation. More connec-\ntions and integrative efforts are clearly needed.\nLack of clear targeted mental states. Explic-\nitly mentioning the Sally-Anne Test (Baron-Cohen\net al., 1985) as inspiration, Grant et al. (2017) de-\nveloped the predecessor of TOMI (Le et al., 2019).\nSimilarly, Nematzadeh et al. (2018) cited the Ice-\ncream Van Test (Perner and Wimmer, 1985) as\nmotivation and the FAUX PAS-EAI (Shapira et al.,\n2023b) benchmark followed the study of Baron-\nCohen et al. (1999). While these benchmarks are\ncognitively grounded and target one particular as-\npect of ToM, the majority often incorporate multi-\nple mental states without clear descriptions, which\ncould make it challenging to measure the actual\nprogress (Raji et al., 2021).\nLack of situatedness in a physical and social en-\nvironment. Figure 6 illustrates the configurations\nof benchmarks. Each bar in the chart represents\na distinct benchmark characteristic, and each seg-\nment within the bar illustrates the proportion of\nbenchmarks with one specific setting. An immedi-\nate observation is a noticeable lack of benchmarks\nthat encompass both physical and social environ-\nments, which highlights an existing research dispar-\nity in the field. We notice that many existing bench-\nmarks are story-based, which verbalize the agent’s\nperception of the environment and the behaviors\nof other agents in the form of story episodes, usu-\nally with language templates. The semantics of the\nenvironment are given by high-level events (e.g.,\nSally entered the kitchen). Many aspects of physi-\ncal and social situatedness are overlooked in these\nbenchmarks, e.g., spatial relations, the task and\nmotivation of agents, and their action trajectories.\nLack of engagement in environment. We point\nout that existing benchmarks primarily adopt a pas-\nsive observer role to test language agents. Yet the\ncrucial aspects of interaction and engagement be-\ntween the agent and other entities involved have\n1015\nbeen overlooked. Among all the benchmarks we\nreviewed, only three of them treat the tested model\nas an active agent, one that perceives the physical\nand social context, reasons about others’ mental\nstates, communicates with other agents, and inter-\nacts with the environment to complete pre-defined\ntasks (Sclar et al., 2022; Bara et al., 2021, 2023).\n4 Towards A Situated Theory of Mind\n4.1 Why A Situated ToM?\nThere have been concerns that cognitive inquiries\nare inadequate for gaining insight into understand-\ning ToM for LLMs (Mitchell and Krakauer, 2023;\nShapira et al., 2023a). However, we believe that the\nprimary problem lies in using story-based probing\nas proxies for psychological tests, which situate\nhuman subjects in specific physical or social en-\nvironments and record their responses to various\ncues. We, therefore, call for a situated evaluation\nof ToM, in which the tested LLMs are treated like\nagents who are physically situated in environments\nand socially situated in interactions with others.\nSituated evaluation covers more aspects of ToM.\nAlthough it is possible to frame the situations as nar-\nratives and cover all mental states using text-only\nbenchmarks, certain aspects of ToM can only be ef-\nfectively studied within specific physical or social\nenvironment (Carruthers, 2015). This is because\nhumans have the ability to infer the mental states\nof others through various modalities such as visual\nperception, actions, attention (gazes or gestures),\nand speech (Stack et al., 2022). For instance, study-\ning perceptual disparities can be challenging with\ntext-only datasets, as they often reduce complex\nscenarios to rule-based manipulations over nega-\ntions in the prompts (Sileo and Lernould, 2023).\nBenchmarks that are not situated also face chal-\nlenges when it comes to implementing coordination\nbetween agents, e.g., aligning intentions towards\njoint actions (Jain et al., 2019) and pragmatic gen-\neration (Zhu et al., 2021a; Bao et al., 2022).\nSituated evaluation mitigates data contamina-\ntion. A situated ToM evaluation can mitigate data\ncontamination, as researchers can design scenarios\nin simulated settings that are unlikely to be part of\nthe LLM’s training data. Carefully designed bench-\nmarks can also incorporate seen and unseen envi-\nronments to assess generalization to new tasks and\nnew environments, fundamentally addressing the\nissue of data contamination (Gandhi et al., 2021).\nSituated evaluation mitigates shortcuts. By em-\nploying situated evaluation, the risk of taking short-\ncuts can be mitigated. Many of the existing ToM\nbenchmarks are either limited in scale or adopt\ntext templates to verbalize a (few) predefined sce-\nnario(s) and prompt LLMs for answers, giving an-\nswers away from syntactic structures and positional\ninformation (Le et al., 2019; Sclar et al., 2023). In\na situated setting, on the contrary, we rely on simu-\nlated environments to manipulate evaluation data\nat scale, so that the environment, the states, and the\naction traces in the environment can be randomized\nto avoid the statistical spurious correlations. While\nsituated evaluation can mitigate shortcuts, it does\nnot eliminate the issue completely. For example,\nAru et al. (2023) have reported that shortcuts can\nemerge in grid world setups if the design is not\ncareful enough and randomness is limited. We em-\nphasize that careful design and consideration are\nstill required to curate any ToM benchmark.\n4.2 A Preliminary Exploration in Grid World\nIn this section, we present a proof-of-concept\nstudy on a situated evaluation of ToM on LLMs.\nWe choose to conduct our pilot study in Mini-\nGrid (Chevalier-Boisvert et al., 2018), a simple and\ncommonly used environment for ToM studies in\nthe machine learning community (Rabinowitz et al.,\n2018; Sclar et al., 2022). Through basic grid world\nrepresentation, we can create tasks to challenge\nLLMs to reason about many aspects of physical\nand social situatedness, e.g., spatial relations, par-\ntial observability, agent’s action trajectories, and\nfrom there, their beliefs, intent, emotions, etc. This\nis in stark contrast to existing story-based ToM\nbenchmarks, which only contain high-level event\nepisodes. We demonstrate that a diverse range of\nchallenging ToM tests, covering all mental states\nfrom ATOMS, can be effectively created in a situ-\nated manner using a simple 2D grid world.\nEnvironment and Task Setups We introduced\n9 different ToM evaluation tasks for each mental\nstate under ATOMS, and 1 reality-checking task to\ntest LLMs’ understanding of the world. It is im-\nportant to acknowledge that our experiment serves\nas a proof of concept and does not aim to cover\nthe entire spectrum of machine ToM, as our case\nstudies are far from being exhaustive or systematic.\n• Reality Check: Given the sequence of actions,\npredict the closest object at the end of the trajec-\ntory. The task is designed to test LLMs’ under-\n1016\nAgent-Green Trajectory\nAgent-Red Trajectory\nAgent-Green\nAgent-Red\nBall\nDoor\nWhere would   look for   ?\n(First-order Belief)  \nStage1:\nAgent-Red: wander aroundAgent-Green: pick up ball\nStage2:\nAgent-Red: wander aroundAgent-Green: goto room#2\nRoom#1\nRoom#2\nRoom#3\nStage3:\nAgent-Red: goto room#1Agent-Green: idle\nStage4:\nAgent-Red: idleAgent-Green: drop ball in room#3\nStage5:\nAgent-Red: idleAgent-Green: goto room#2\nStage6:\nAgent-Red: goto room#3Agent-Green: idle\nWhere would   think that   would look for   ?\n(Second-order Belief)  \nFigure 4: An overview of the first and second order false belief task illustrated in a grid world setup. We simulate\nthe unexpected transfer scenarios with two agents, and verbalize the environment and action traces to test if LLMs\nhold a correct understanding of the agents’ false beliefs.\nStage1:\nAgent-Red&White&Yellow: idle\nStage2:\nAgent-Red: push Agent-Yellow into water\nAgent-White: observe the scene\nStage3:\nAgent-Red: approach Agent-White\nHow would   feel \nabout the   ?\nFrightened / \nNo strong emotion\nStage1:\nAgent-Red&White&Yellow: idle\nStage2:\nAgent-Red: push Agent-Yellow into water\nAgent-White: idle\nStage3:\nAgent-Red: approach Agent-White\nHow would   feel \nabout the   ?\nFrightened / \nNo strong emotion\nFigure 5: An overview of the morally related emotional reaction tasks illustrated in a grid world setup. We simulate\nscenarios where an agent either directly witnesses or is ignorant of a morally related event, and verbalize the\nenvironment and action traces to test if LLMs hold a correct prediction of the agent’s emotional reaction.\nstanding of relocations in the grid world.\n• Short-term Intention: Given an incomplete tra-\njectory and a goal, predict the next action.\n• Long-term Intention: Given an incomplete tra-\njectory and a list of subgoals, predict the next\nsubgoal that the agent is planning to achieve.\n• Desire: Given a complete trajectory, predict if\nthe agent demonstrates a preference for objects.\n• Percepts: Given a complete trajectory, predict if\nthe agent has a partial or full observation.\n• Belief: The classic unexpected transfer task with\npossible first and second order false belief.\n• Non-literal Communication: Given a trajectory\nand a statement from the agent, judge whether\nthe agent is being deceptive.\n• Knowledge: Given a trajectory, predict the ob-\nject whose location is unknown to the agent.\n• Emotion: The classic perception-emotion link\ntest, where emotions are evoked in response to\nwitnessing an emotionally stimulating situation.\nWe detail two case studies and leave examples of\neach task in Appendix A.\nCase Study 1: Beliefs. Our belief experi-\nments emulate the classic unexpected transfer\ntasks (Baron-Cohen et al., 1985; Perner and Wim-\nmer, 1985). As is shown in Figure 4, we simulate\nthis disparity of belief state and world state in Min-\niGrid. The first-order belief task features a main\nroom with three connected side rooms, two agents\nnamed Red and Green, and a ball. Each instance\nof the belief experiment begins with Green placing\nthe ball in Room#2 while Red watches. Red then\nenters a separate Room#1 and shuts the door. While\nRed is inside of this closed room, Green transfers\nthe ball to Room#3. Red presumably holds a false\nbelief about the location of the ball, believing it is\nin Room#2 though it is now in Room#3. Similarly,\nwe implement the second-order belief task to test\nan incorrect belief that one agent holds about the\nbelief of another. After Green has finished trans-\nferring the ball, it navigates to the room originally\n1017\nReality\nCheck\nShort\nIntent\nLong\nIntent\nDesire Percept 1st belief 2nd belief Non-Literal\nComm.\nKnowledge Emotion0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nRnd.\nN/A\n       Models                Avg. Accuracy\nGPT-3.5 Zero Shot\nGPT-3.5 One Shot\nGPT-3.5 Zero Shot CoT\nGPT-4 Zero Shot\nGPT-4 One Shot\n54.1%  \n50.1%  \n51.2%  \n60.9%  \n65.7%  \nFigure 6: The LLMs’ performance across the 10 tasks is illustrated. Each bar shows how one LLM performed\nwith a specific prompting method. Overall, the tasks are tough for all the LLMs tested. While one-shot and CoT\nprompting can help in certain cases, their effectiveness is not consistent across the board.\ncontaining the ball and shuts the door. Red then\nnavigates to the room now containing the ball and\nsees the true location of the ball. Still, Green pre-\nsumably possesses a false belief about Red’s belief.\nIn both tasks, LLMs are queried with two versions\nof the world: a false one with the ball in the original\nroom, and a true one with the ball in the third room\n(its actual location). LLMs must correctly respond\nthat the agents hold a false belief.\nCase Study 2: Emotions. While the belief tasks\nhighlight the importance of physical situatedness,\nwe further demonstrate that social interactions can\nbe simulated in the grid world. As is shown in\nFigure 5, We design morally related events that\nstimulate emotions (e.g., fear, appreciation). In this\ntask, LLMs are queried to predict the emotional\nresponse of Agent-White, who either directly wit-\nnesses or is ignorant of this event. LLMs must\ncorrectly respond that the agent holds an emotional\nreaction only if it observes the event.\nExperiment Setups. For each task, we create\n100 instances following a prompt template that con-\nsists of [environment description] , [agent\ndescription], [observability statement] ,\n[task statement] , [actions sequences] ,\n[QA]. We select GPT-4 ( gpt-4-0314) and Chat-\nGPT (gpt-3.5-turbo-0613) for evaluation on the\n9 tasks.2 Following prior work (Hu et al., 2022;\nShapira et al., 2023a), we adopt MC-probing for\nLLMs that don’t produce probabilities, which di-\nrectly instructs LLMs to generate only the letter\ncorresponding to the answer. Besides zero-shot\nevaluation, we also explored one-shot learning and\nChain-of-Thought (CoT) prompting (Wei et al.,\n2022). More details are available in Appendix B.\n2We use the ChatCompletion.create function from\nopenai package.\nResults and Discussion. We observe that LLMs\nexhibit some level of sensitivity for some mental\nstates. Especially, GPT-4 scores up to 91% zero-\nshot accuracy and 96% one-shot accuracy in the\nlong-term intention task. However, we also high-\nlight the shortcomings of LLMs in some mental\nstates of ATOMS to varying degrees, especially, in\nterms of predicting preferences, perception limita-\ntions, missing knowledge, and higher-order beliefs.\nThese findings align with previous research (Sap\net al., 2022; Trott et al., 2022; Shapira et al., 2023a),\nfurther confirming that LLMs are not yet reliable\nand comprehensive ToM agents. From the reality-\nchecking task, we observe that GPT-3.5 reaches\n78% accuracy with CoT prompting and GPT-4 sig-\nnificantly surpasses its predecessors with 83% zero-\nshot accuracy and 95% one-shot accuracy. Solving\nthis reality check by no means implies that LLMs\nhave a general perception ability of the real world,\nbut that as a proof of concept, they demonstrate a\ncertain (but still limited) level of situated aware-\nness within the context of a basic abstract grid\nworld. This implies that researchers can begin uti-\nlizing them as powerful building blocks for situated\nagents in complex ToM tasks. We note that it is\nalways possible to come up with more challenging\nreality-checking questions to expose the limitations\nof LLMs, or to provide more guided prompts to\nassist LLMs in successfully completing ToM tasks.\nUndoubtedly, further research is required along this\nexciting yet challenging trajectory to advance ToM\nin LLMs and AI agents built upon LLMs.\n5 Discussions and Action Items\n5.1 The Scope of Machine Theory of Mind\nBe specific about the mental states studied. Ex-\nisting benchmarks often lack a clear target mental\n1018\nstate, making it challenging to interpret the results\nand measure the actual progress. To mitigate the\nrisk of overestimating LLMs’ ToM capabilities, it\nis recommended that future benchmark develop-\ners provide specific details regarding the targeted\nmental state(s) they intend to assess.\nBroaden the Scope of Machine ToM. A breadth\nof mental states and their sub-domains have al-\nready been covered by AI benchmarks (Table 1).\nWe observed an overwhelming emphasis on the\nbenchmarks and modeling of beliefs and intentions,\nwhile other aspects have received insufficient at-\ntention. Still, there are considerably many blank\nspaces in the landscape of machine ToM, especially\nfor more complicated forms of knowledge, desires,\nperspective-tasking, and emotional experiences be-\nyond typical social situations.\n5.2 Design New Theory of Mind Benchmarks\nAvoid shortcuts and spurious correlations. The\nevaluation of LLMs itself presents significant chal-\nlenges, not only in the case of ToM. Existing bench-\nmarks suffer from issues such as data leakage and\nspurious correlations. Especially, shortcut solu-\ntions have been consistently reported in recent\nyears (Le et al., 2019; Shapira et al., 2023a; Aru\net al., 2023). We are in pressing need of new bench-\nmarks with scalable sizes, high-quality human an-\nnotations, and privately held-out sets for evaluation.\nAvoid unfair evaluations from prompting. Pre-\nvious work has shown that CoT prompting can im-\nprove the performance of LLMs in ToM tasks (Li\net al., 2023; Moghaddam and Honey, 2023; Shapira\net al., 2023a). Various recent prompting mecha-\nnisms have also been developed to improve LLM’s\ncapability on ToM tasks (Zhou et al., 2023a; Leer\net al., 2023). In the evaluation of LLMs’ ToM\ncapabilities, we recommend the careful documenta-\ntion of prompts used and the avoidance of implicit\nhuman guidance to ensure a fair comparison.\nMove on to a situated ToM. We call for a situ-\nated evaluation of ToM, in which the tested LLMs\nare treated like agents who are physically situated\nin environments and socially situated in interac-\ntions with others. A situated setup covers a wider\nrange of ToM aspects. With carefully designed\nbenchmarks with diverse environments and unseen\ntest sets, a situated setup can help address data\ncontamination issues and assess generalization to\nnew tasks and environments. Furthermore, a situ-\nated setup allows for more complicated evaluation\nprotocols than simple inference and QA tasks.\nConsider a mutual and symmetric ToM. ToM\nis symmetric and mutual in nature, as it origi-\nnally imputes the mental states of self and others.\nPrior research is largely limited to passive observer\nroles (Grant et al., 2017; Nematzadeh et al., 2018;\nLe et al., 2019; Rabinowitz et al., 2018) or speaker\nin a speaker-listener relationship (Zhu et al., 2021b;\nZhou et al., 2023b). We encourage more studies on\nhow humans and agents build and maintain com-\nmon ground with a human ToM and a machine\nToM through situated communication (Bara et al.,\n2021; Sclar et al., 2022). Besides, more research\nis needed to understand if LLMs possess early\nforms of intrinsic mental states given observation\ncues of the world. While we need to develop ma-\nchines that impute the mental states of humans,\nhumans should also develop a theory of AI’s mind\n(ToAIM) (Chandrasekaran et al., 2017) by under-\nstanding the strengths, weaknesses, beliefs, and\nquirks of these black box language models.\n5.3 Neural Language Acquisition and ToM\nBoth psychological studies (Bloom, 2002;\nTomasello, 2005) and computational simula-\ntions (Liu et al., 2023) have demonstrated the\neffectiveness of ToM, especially intention, in\nlanguage acquisition. Instead of concentrating on\neliciting ToM in LLMs, we should contemplate\nwhether certain ToM elements should be inherently\npresent in LLMs or perhaps introduced alongside\nlanguage pretraining. More research is needed to\nunderstand the connection between neural word\nacquisition and ToM development in machines.\n6 Conclusion\nIn this position paper, we survey and summarize\nthe ongoing debate regarding the presence of a\nmachine ToM within LLMs, and identify the inade-\nquate evaluation protocols as the roadblock. Many\nbenchmarks focus only on a few aspects of ToM,\nand are prone to shortcuts. To mediate this issue,\nwe follow the ATOMS framework to offer a holistic\nreview of existing benchmarks and identify under-\nexplored aspects of ToM. We further call for a sit-\nuated evaluation of ToM, one that is physically\nsituated in environments and socially situated in\ninteractions with humans. We hope this work can\nfacilitate future research towards LLMs as ToM\nagents, and offer an intuitive means for researchers\nto position their work in the landscape of ToM.\n1019\nEthical Statement\nThe dataset created in this study includes instances\nthat are synthetically generated from planners and\nRL algorithms, as well as ones created by humans.\nHuman subjects research is approved by the Univer-\nsity of Michigan Health Sciences and Behavioral\nSciences Institutional Review Board (IRB-HSBS)\nunder eResearch ID HUM00234647. The text gen-\nerated by LLMs could potentially contain harm-\nful, toxic, or offensive content. The authors have\nensured that the data does not contain personally\nidentifiable information or offensive content.\nLimitations\nOur current benchmark only covers 100 instances\nfor each task, adding up to only 1000 instances.\nOur experiment serves as a proof of concept and\ndoes not aim to cover the entire spectrum of ma-\nchine ToM, as our case studies are far from being\nexhaustive or systematic. In the future, we plan to\ncreate a more systematic benchmark with a larger\nscale and various forms of evaluation. Additionally,\nit is worth noting that the ATOMS framework is\nderived from human ToM studies conducted with\nchildren under the age of 5. Consequently, this\nframework primarily focuses on the early devel-\nopmental stages of ToM, capturing the naive and\npotentially rudimentary aspects of ToM. For more\nadvanced ToM capability, we point to some recent\nframeworks proposed by Osterhaus and Bosacki\n(2022) and Stack et al. (2022).\nAcknowledgements\nThis work was supported in part by NSF IIS-\n1949634, NSF SES-2128623, and by the Automo-\ntive Research Center at the University of Michigan.\nWithout implying any agreement with the contents\nas presented in this work, the authors extend their\nappreciation to Susan Gelman for her valuable feed-\nback. The authors would like to thank all anony-\nmous reviewers for their valuable feedback.\nReferences\nArjun R Akula, Keze Wang, Changsong Liu, Sari Saba-\nSadiya, Hongjing Lu, Sinisa Todorovic, Joyce Chai,\nand Song-Chun Zhu. 2022. Cx-tom: Counterfac-\ntual explanations with theory-of-mind for enhancing\nhuman trust in image recognition models. Iscience,\n25(1):103581.\nStefano V Albrecht and Peter Stone. 2018. Autonomous\nagents modelling other agents: A comprehensive\nsurvey and open problems. Artificial Intelligence,\n258:66–95.\nJacob Andreas. 2022. Language models as agent mod-\nels. In Findings of the Association for Computational\nLinguistics: EMNLP 2022 , pages 5769–5779, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nJames N Aronson and Claire Golomb. 1999. Preschool-\ners’ understanding of pretense and presumption of\ncongruity between action and representation. Devel-\nopmental Psychology, 35(6):1414.\nJaan Aru, Aqeel Labash, Oriol Corcoll, and Raul Vi-\ncente. 2023. Mind the gap: Challenges of deep learn-\ning approaches to theory of mind. Artificial Intelli-\ngence Review, pages 1–16.\nYuwei Bao, Sayan Ghosh, and Joyce Chai. 2022. Learn-\ning to mediate disparities towards pragmatic commu-\nnication. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(ACL).\nCristian-Paul Bara, Ziqiao Ma, Yingzhuo Yu, Julie Shah,\nand Joyce Chai. 2023. Towards collaborative plan\nacquisition through theory of mind modeling in situ-\nated dialogue. In Proceedings of the Thirty-Second\nInternational Joint Conference on Artificial Intel-\nligence, IJCAI-23, pages 2958–2966. International\nJoint Conferences on Artificial Intelligence Organi-\nzation. Main Track.\nCristian-Paul Bara, CH-Wang Sky, and Joyce Chai.\n2021. Mindcraft: Theory of mind modeling for situ-\nated dialogue in collaborative tasks. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 1112–1125.\nSimon Baron-Cohen. 1997. Mindblindness: An essay\non autism and theory of mind. MIT press.\nSimon Baron-Cohen, Alan M Leslie, and Uta Frith.\n1985. Does the autistic child have a “theory of\nmind”? Cognition, 21(1):37–46.\nSimon Baron-Cohen, Michelle O’riordan, Valerie Stone,\nRosie Jones, and Kate Plaisted. 1999. Recognition\nof faux pas by normally developing children and\nchildren with asperger syndrome or high-functioning\nautism. Journal of autism and developmental disor-\nders, 29:407–418.\nCindy Beaudoin, Élizabel Leblanc, Charlotte Gagner,\nand Miriam H Beauchamp. 2020. Systematic review\nand inventory of theory of mind measures for young\nchildren. Frontiers in psychology, 10:2905.\nMark Bennett and Linda Galpert. 1993. Children’s un-\nderstanding of multiple desires. International Jour-\nnal of Behavioral Development, 16(1):15–33.\nSarah-Jayne Blakemore and Jean Decety. 2001. From\nthe perception of action to the understanding of in-\ntention. Nature reviews neuroscience, 2(8):561–567.\n1020\nPaul Bloom. 2002. How children learn the meanings of\nwords. MIT press.\nHelene Borke. 1971. Interpersonal perception of young\nchildren: Egocentrism or empathy? Developmental\npsychology, 5(2):263.\nMichael Bratman. 1987. Intention, plans, and practical\nreason. The University of Chicago Press.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\nPeter Carruthers. 2015. Perceiving mental states. Con-\nsciousness and cognition, 36:498–507.\nFulvia Castelli. 2006. The valley task: Understanding\nintention from goal-directed motion in typical devel-\nopment and autism. British journal of developmental\npsychology, 24(4):655–668.\nArjun Chandrasekaran, Deshraj Yadav, Prithvijit Chat-\ntopadhyay, Viraj Prabhu, and Devi Parikh. 2017. It\ntakes two to tango: Towards theory of ai’s mind.\narXiv preprint arXiv:1704.00717.\nMaxime Chevalier-Boisvert, Lucas Willems, and Suman\nPal. 2018. Minimalistic gridworld environment for\ngymnasium.\nMichael Cohen. 2021. Exploring roberta’s theory of\nmind through textual entailment.\nPhilip R Cohen and Hector J Levesque. 1990. Intention\nis choice with commitment. Artificial intelligence,\n42(2-3):213–261.\nCristina Colonnesi, Carolien Rieffe, Willem Koops, and\nPaola Perucchini. 2008. Precursors of a theory of\nmind: A longitudinal study. British Journal of Devel-\nopmental Psychology, 26(4):561–577.\nAntonio R Damasio. 2004. Emotions and feelings. In\nFeelings and emotions: The Amsterdam symposium,\nvolume 5, pages 49–57. Cambridge University Press\nCambridge.\nSusanne A Denham. 1986. Social cognition, prosocial\nbehavior, and emotion in preschoolers: Contextual\nvalidation. Child development, pages 194–201.\nDaniel Dennett. 1995. Do animals have beliefs. Com-\nparative approaches to cognitive science, 111.\nDaniel C Dennett. 1988. Précis of the intentional stance.\nBehavioral and brain sciences, 11(3):495–505.\nJesse Dodge, Maarten Sap, Ana Marasovi ´c, William\nAgnew, Gabriel Ilharco, Dirk Groeneveld, Margaret\nMitchell, and Matt Gardner. 2021. Documenting\nlarge webtext corpora: A case study on the colossal\nclean crawled corpus. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1286–1305.\nFred I Dretske. 1979. Simple seeing. In Body, mind,\nand method, pages 1–15. Springer.\nNouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine\nLi, Liwei Jian, Bill Yuchen Lin, Peter West, Chandra\nBhagavatula, Ronan Le Bras, Jena D Hwang, et al.\n2023. Faith and fate: Limits of transformers on com-\npositionality. arXiv preprint arXiv:2305.18654.\nJacquelynne S Eccles and Allan Wigfield. 2002. Moti-\nvational beliefs, values, and goals. Annual review of\npsychology, 53(1):109–132.\nBenjamin Eysenbach, Carl V ondrick, and Antonio Tor-\nralba. 2016. Who is mistaken? arXiv preprint\narXiv:1612.01175.\nNico H Frijda et al. 1986. The emotions. Cambridge\nUniversity Press.\nKanishk Gandhi, Gala Stojnic, Brenden M Lake, and\nMoira R Dillon. 2021. Baby intuitions benchmark\n(bib): Discerning the goals, preferences, and actions\nof others. Advances in Neural Information Process-\ning Systems, 34:9963–9976.\nEdmund L Gettier. 2000. Is justified true belief knowl-\nedge? Causal Theories of Mind, page 135.\nRachel Giora. 2003. On our mind: Salience, context,\nand figurative language. Oxford University Press.\nAlison Gopnik and Henry M Wellman. 1992. Why the\nchild’s theory of mind really is a theory. Mind &\nLanguage, 7(1–2):145–171.\nAndrew Gordon. 2016. Commonsense interpretation\nof triangle behavior. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 30.\nErin Grant, Aida Nematzadeh, and Thomas L Griffiths.\n2017. How can memory-augmented neural networks\npass a false-belief task? In CogSci.\nDavid Gunning. 2018. Machine common sense concept\npaper. arXiv preprint arXiv:1810.07528.\nJulie Hadwin, Simon Baron-Cohen, Patricia Howlin,\nand Katie Hill. 1997. Does teaching theory of mind\nhave an effect on the ability to develop conversation\nin children with autism? Journal of autism and\ndevelopmental disorders, 27:519–537.\nThilo Hagendorff. 2023. Machine psychology: Inves-\ntigating emergent capabilities and behavior in large\nlanguage models using psychological methods.\n1021\nFrancesca GE Happé. 1994. An advanced test of theory\nof mind: Understanding of story characters’ thoughts\nand feelings by able autistic, mentally handicapped,\nand normal children and adults. Journal of autism\nand Developmental disorders, 24(2):129–154.\nPaul L Harris, Kara Donnelly, Gabrielle R Guz, and\nRosemary Pitt-Watson. 1986. Children’s understand-\ning of the distinction between real and apparent emo-\ntion. Child development, pages 895–909.\nPeter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zor-\nnitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and\nSrinivasan Iyer. 2023. Methods for measuring, up-\ndating, and visualizing factual beliefs in language\nmodels. In Proceedings of the 17th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics, pages 2706–2723.\nYinghui He, Yufan Wu, Yilin Jia, Rada Mihalcea, Yu-\nlong Chen, and Naihao Deng. 2023. Hi-tom: A\nbenchmark for evaluating higher-order theory of\nmind reasoning in large language models. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2023.\nMark K Ho, Rebecca Saxe, and Fiery Cushman. 2022.\nPlanning with theory of mind. Trends in Cognitive\nSciences.\nBart Holterman and Kees van Deemter. 2023. Does\nchatgpt have theory of mind? arXiv preprint\narXiv:2305.14020.\nJennifer Hu, Sammy Floyd, Olessia Jouravlev, Evelina\nFedorenko, and Edward Gibson. 2022. A fine-\ngrained comparison of pragmatic language under-\nstanding in humans and language models. arXiv\npreprint arXiv:2212.06801.\nUnnat Jain, Luca Weihs, Eric Kolve, Mohammad Raste-\ngari, Svetlana Lazebnik, Ali Farhadi, Alexander G\nSchwing, and Aniruddha Kembhavi. 2019. Two body\nproblem: Collaborative visual task completion. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 6689–\n6699.\nAo Jia, Yu He, Yazhou Zhang, Sagar Uprety, Dawei\nSong, and Christina Lioma. 2022. Beyond emotion:\nA multi-modal dataset for human desire understand-\ning. In Proceedings of the 2022 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, pages 1512–1522.\nDavid J Kavanagh, Jackie Andrade, and Jon May. 2005.\nImaginary relish and exquisite torture: the elabo-\nrated intrusion theory of desire. Psychological re-\nview, 112(2):446.\nCasey Kennington. 2022. Understanding intention\nfor machine theory of mind: a position paper. In\n2022 31st IEEE International Conference on Robot\nand Human Interactive Communication (RO-MAN),\npages 450–453. IEEE.\nAriel Knafo, Carolyn Zahn-Waxler, Maayan Davidov,\nCarol Van Hulle, JoAnn L Robinson, and Soo Hyun\nRhee. 2009. Empathy in early childhood: Genetic,\nenvironmental, and affective contributions. Annals\nof the New York Academy of Sciences, 1167(1):103–\n114.\nMichal Kosinski. 2023. Theory of mind may have spon-\ntaneously emerged in large language models.\nNicole C Krämer, Astrid von der Pütten, and Sabrina\nEimler. 2012. Human-agent and human-robot inter-\naction theory: Similarities to and differences from\nhuman-human interaction. In Human-computer in-\nteraction: The agency perspective , pages 215–240.\nSpringer.\nMatthew Le, Y-Lan Boureau, and Maximilian Nickel.\n2019. Revisiting the evaluation of theory of mind\nthrough question answering. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5872–5877, Hong Kong,\nChina. Association for Computational Linguistics.\nCourtland Leer, Vincent Trost, and Vineeth V oru-\nganti. 2023. Violation of expectation via metacog-\nnitive prompting reduces theory of mind predic-\ntion error in large language models. arXiv preprint\narXiv:2310.06983.\nBelinda Z Li, Maxwell Nye, and Jacob Andreas. 2021.\nImplicit representations of meaning in neural lan-\nguage models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 1813–1827.\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani\nItani, Dmitrii Khizbullin, and Bernard Ghanem. 2023.\nCamel: Communicative agents for\" mind\" explo-\nration of large scale language model society. arXiv\npreprint arXiv:2303.17760.\nShuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clin-\nton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin\nAkyürek, Anima Anandkumar, et al. 2022. Pre-\ntrained language models for interactive decision-\nmaking. Advances in Neural Information Processing\nSystems, 35:31199–31212.\nAndy Liu, Hao Zhu, Emmy Liu, Yonatan Bisk, and\nGraham Neubig. 2023. Computational language ac-\nquisition with theory of mind. In The Eleventh Inter-\nnational Conference on Learning Representations.\nInbal Magar and Roy Schwartz. 2022. Data contamina-\ntion: From memorization to exploitation. In Proceed-\nings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers),\npages 157–165.\n1022\nBertram F Malle and Joshua Knobe. 2001. The distinc-\ntion between desire and intention: A folk-conceptual\nanalysis. Intentions and intentionality: Foundations\nof social cognition, 45:67.\nGary Marcus and Ernest Davis. 2023. How not to test\ngpt-3.\nZenaida S Masangkay, Kathleen A McCluskey, Cur-\ntis W McIntyre, Judith Sims-Knight, Brian E Vaughn,\nand John H Flavell. 1974. The early development of\ninferences about the visual percepts of others. Child\ndevelopment, pages 357–366.\nAndrew N Meltzoff. 1995. Understanding the intentions\nof others: re-enactment of intended acts by 18-month-\nold children. Developmental psychology, 31(5):838.\nMelanie Mitchell and David C Krakauer. 2023. The de-\nbate over understanding in ai’s large language models.\nProceedings of the National Academy of Sciences ,\n120(13):e2215907120.\nShima Rahimi Moghaddam and Christopher J Honey.\n2023. Boosting theory-of-mind performance in large\nlanguage models via prompting. arXiv preprint\narXiv:2304.11490.\nHenrike Moll, Cornelia Koring, Malinda Carpenter, and\nMichael Tomasello. 2006. Infants determine others’\nfocus of attention by pragmatics and exclusion. Jour-\nnal of Cognition and Development, 7(3):411–430.\nAida Nematzadeh, Kaylee Burns, Erin Grant, Alison\nGopnik, and Tom Griffiths. 2018. Evaluating theory\nof mind in question answering. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 2392–2400, Brussels,\nBelgium. Association for Computational Linguistics.\nOpenAI. 2022. Chatgpt: Optimizing language models\nfor dialogue.\nOpenAI. 2023. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774.\nChristopher Osterhaus and Sandra L Bosacki. 2022.\nLooking for the lighthouse: A systematic review of\nadvanced theory-of-mind tests beyond preschool. De-\nvelopmental Review, 64:101021.\nGonçalo Pereira, Rui Prada, and Pedro A Santos. 2016.\nIntegrating social power into the decision-making of\ncognitive agents. Artificial Intelligence, 241:1–44.\nJosef Perner, Susan R Leekam, and Heinz Wimmer.\n1987. Three-year-olds’ difficulty with false belief:\nThe case for a conceptual deficit. British journal of\ndevelopmental psychology, 5(2):125–137.\nJosef Perner and Heinz Wimmer. 1985. “john thinks\nthat mary thinks that. . . ” attribution of second-order\nbeliefs by 5-to 10-year-old children. Journal of ex-\nperimental child psychology, 39(3):437–471.\nAnn T Phillips, Henry M Wellman, and Elizabeth S\nSpelke. 2002. Infants’ ability to connect gaze and\nemotional expression to intentional action. Cogni-\ntion, 85(1):53–78.\nF. Pons and P. Harris. 2000. Test of Emotion Compre-\nhension: TEC. University of Oxford.\nDavid Premack and Guy Woodruff. 1978. Does the\nchimpanzee have a theory of mind? Behavioral and\nBrain Sciences, 1(4):515–526.\nLiang Qiu, Yizhou Zhao, Yuan Liang, Pan Lu, Weiyan\nShi, Zhou Yu, and Song-chun Zhu. 2022. Towards\nsocially intelligent agents with mental state transition\nand human value. In Proceedings of the 23rd Annual\nMeeting of the Special Interest Group on Discourse\nand Dialogue, pages 146–158.\nNeil Rabinowitz, Frank Perbet, Francis Song, Chiyuan\nZhang, SM Ali Eslami, and Matthew Botvinick. 2018.\nMachine theory of mind. In International conference\non machine learning, pages 4218–4227. PMLR.\nInioluwa Deborah Raji, Emily Denton, Emily M Bender,\nAlex Hanna, and Amandalynne Paullada. 2021. Ai\nand the everything in the whole wide world bench-\nmark. In Thirty-fifth Conference on Neural Informa-\ntion Processing Systems Datasets and Benchmarks\nTrack (Round 2).\nBetty M Repacholi and Alison Gopnik. 1997. Early\nreasoning about desires: evidence from 14-and 18-\nmonth-olds. Developmental psychology, 33(1):12.\nTed K Ruffman and David R Olson. 1989. Children’s\nascriptions of knowledge to others. Developmental\nPsychology, 25(4):601.\nLaura Ruis, Akbir Khan, Stella Biderman, Sara Hooker,\nTim Rocktäschel, and Edward Grefenstette. 2022.\nLarge language models are not zero-shot communi-\ncators. arXiv preprint arXiv:2210.14986.\nTessa Rusch, Saurabh Steixner-Kumar, Prashant Doshi,\nMichael Spezio, and Jan Gläscher. 2020. Theory of\nmind and decision science: towards a typology of\ntasks and computational models. Neuropsychologia,\n146:107488.\nMaarten Sap, Ronan Le Bras, Daniel Fried, and Yejin\nChoi. 2022. Neural theory-of-mind? on the limits of\nsocial intelligence in large LMs. In Proceedings of\nthe 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 3762–3780, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan\nLe Bras, and Yejin Choi. 2019. Social IQa: Com-\nmonsense reasoning about social interactions. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 4463–\n4473, Hong Kong, China. Association for Computa-\ntional Linguistics.\n1023\nRoger C Schank and Robert P Abelson. 2013. Scripts,\nplans, goals, and understanding: An inquiry into\nhuman knowledge structures. Psychology press.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal,\nAlec Radford, and Oleg Klimov. 2017. Proxi-\nmal policy optimization algorithms. arXiv preprint\narXiv:1707.06347.\nMelanie Sclar, Sachin Kumar, Peter West, Alane Suhr,\nYejin Choi, and Yulia Tsvetkov. 2023. Minding lan-\nguage models’ (lack of) theory of mind: A plug-and-\nplay multi-character belief tracker.\nMelanie Sclar, Graham Neubig, and Yonatan Bisk. 2022.\nSymmetric machine theory of mind. In Proceedings\nof the 39th International Conference on Machine\nLearning, volume 162, pages 19450–19466.\nNatalie Shapira, Mosh Levy, Seyed Hossein Alavi,\nXuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten\nSap, and Vered Shwartz. 2023a. Clever hans or neu-\nral theory of mind? stress testing social reasoning in\nlarge language models.\nNatalie Shapira, Guy Zwirn, and Yoav Goldberg. 2023b.\nHow well do large language models perform on faux\npas tests. In Findings of the Association for Compu-\ntational Linguistics: ACL 2023.\nTianmin Shu, Abhishek Bhandwaldar, Chuang Gan,\nKevin Smith, Shari Liu, Dan Gutfreund, Elizabeth\nSpelke, Joshua Tenenbaum, and Tomer Ullman. 2021.\nAgent: A benchmark for core psychological reason-\ning. In International Conference on Machine Learn-\ning, pages 9614–9625. PMLR.\nDamien Sileo and Antoine Lernould. 2023. Mindgames:\nTargeting theory of mind in large language models\nwith dynamic epistemic modal logic. arXiv preprint\narXiv:2305.03353.\nPatricia A Smiley. 2001. Intention understanding and\npartner-sensitive behaviors in young children’s peer\ninteractions. Social Development, 10(3):330–354.\nCaoimhe Harrington Stack, Effat Farhana, Xinyu Shen,\nSimeng Zhao, and Angela Maliakal. 2022. Frame-\nwork for a multi-dimensional test of theory of mind\nfor humans and ai systems. In The Tenth Annual\nConference on Advances in Cognitive Systems.\nShane Storks, Qiaozi Gao, Yichi Zhang, and Joyce Chai.\n2021. Tiered reasoning for intuitive physics: Toward\nverifiable commonsense language understanding. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2021, pages 4902–4918.\nKate Sullivan, Ellen Winner, and Natalie Hopfield. 1995.\nHow children tell a lie from a joke: The role of\nsecond-order mental state attributions. British jour-\nnal of developmental psychology, 13(2):191–204.\nJ Swettenham. 1996. Can children be taught to under-\nstand false belief using computers? child psychology\n& psychiatry & allied disciplines, 37 (2), 157–165.\nEce Takmaz, Nicolo’ Brandizzi, Mario Giulianelli, San-\ndro Pezzelle, and Raquel Fernández. 2023. Speak-\ning the language of your listener: Audience-aware\nadaptation via plug-and-play theory of mind. In Find-\nings of the Association for Computational Linguistics:\nACL 2023.\nMichael Tomasello. 2005. Constructing a language: A\nusage-based theory of language acquisition. Harvard\nuniversity press.\nJennifer Tracey, Owen Rambow, Claire Cardie, Adam\nDalton, Hoa Trang Dang, Mona Diab, Bonnie Dorr,\nLouise Guthrie, Magdalena Markowska, Smaranda\nMuresan, et al. 2022. Best: The belief and sentiment\ncorpus. In Proceedings of the Thirteenth Language\nResources and Evaluation Conference, pages 2460–\n2467.\nSean Trott, Cameron Jones, Tyler Chang, James\nMichaelov, and Benjamin Bergen. 2022. Do large\nlanguage models know what humans know? arXiv\npreprint arXiv:2209.01515.\nTomer Ullman. 2023. Large language models fail on\ntrivial alterations to theory-of-mind tasks.\nQiaosi Wang, Koustuv Saha, Eric Gregori, David Joyner,\nand Ashok Goel. 2021. Towards mutual theory of\nmind in human-ai interaction: How language reflects\nwhat students perceive about a virtual teaching assis-\ntant. In Proceedings of the 2021 CHI Conference on\nHuman Factors in Computing Systems, pages 1–14.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed H Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. In Advances in\nNeural Information Processing Systems.\nHenry M Wellman and Jacqueline D Woolley. 1990.\nFrom simple desires to ordinary beliefs: The early\ndevelopment of everyday psychology. Cognition,\n35(3):245–275.\nJincenzi Wu, Zhuang Chen, Jiawen Deng, Sahand\nSabour, and Minlie Huang. 2023. Coke: A cognitive\nknowledge graph for machine theory of mind. arXiv\npreprint arXiv:2305.05390.\nWako Yoshida, Ray J Dolan, and Karl J Friston. 2008.\nGame theory of mind. PLoS computational biology,\n4(12):e1000254.\nMo Yu, Yisi Sang, Kangsheng Pu, Zekai Wei, Han\nWang, Jing Li, Yue Yu, and Jie Zhou. 2022. Few-shot\ncharacter understanding in movies as an assessment\nto meta-learning of theory-of-mind. arXiv preprint\narXiv:2211.04684.\nJamil Zaki, Niall Bolger, and Kevin Ochsner. 2009. Un-\npacking the informational bases of empathic accuracy.\nEmotion, 9(4):478.\n1024\nChen Zhang and Joyce Y Chai. 2010. Towards con-\nversation entailment: An empirical investigation. In\nProceedings of the 2010 Conference on Empirical\nMethods in Natural Language Processing, pages 756–\n766.\nHaode Zhang, Yuwei Zhang, Li Ming Zhan, Jiaxin Chen,\nGuangyuan Shi, Xiao Ming Wu, and Albert YS Lam.\n2021. Effectiveness of pre-training for few-shot in-\ntent classification. In 2021 Findings of the Asso-\nciation for Computational Linguistics, Findings of\nACL: EMNLP 2021, pages 1114–1120. Association\nfor Computational Linguistics (ACL).\nPei Zhou, Aman Madaan, Srividya Pranavi Potharaju,\nAditya Gupta, Kevin R McKee, Ari Holtzman, Jay\nPujara, Xiang Ren, Swaroop Mishra, Aida Ne-\nmatzadeh, et al. 2023a. How far are large language\nmodels from agents with theory-of-mind? arXiv\npreprint arXiv:2310.03051.\nPei Zhou, Andrew Zhu, Jennifer Hu, Jay Pujara, Xiang\nRen, Chris Callison-Burch, Yejin Choi, and Prithviraj\nAmmanabrolu. 2023b. I cast detect thoughts: Learn-\ning to converse and guide with intents and theory-\nof-mind in dungeons and dragons. In Proceedings\nof the 61th Annual Meeting of the Association for\nComputational Linguistics.\nHao Zhu, Graham Neubig, and Yonatan Bisk. 2021a.\nFew-shot language coordination by modeling theory\nof mind. In International Conference on Machine\nLearning, pages 12901–12911. PMLR.\nHao Zhu, Graham Neubig, and Yonatan Bisk. 2021b.\nFew-shot language coordination by modeling theory\nof mind. In Proceedings of the 38th International\nConference on Machine Learning , volume 139 of\nProceedings of Machine Learning Research, pages\n12901–12911. PMLR.\n1025\nA Task Settings and Data Collection\nIn this section, we provide an in-depth explana-\ntion of the ten tasks outlined in section 4.2. Task 0\nserves as a \"reality check\" to assess LLMs’ grasp of\nthe physical world, particularly relocations within\nthe grid world. Tasks 1 through 9 each empha-\nsize distinct facets of ToM. All these tasks utilize\nMiniGrid, a streamlined 2D grid-world environ-\nment. (Chevalier-Boisvert et al., 2018).\nTask 0: Reality Check\nTask setting: visiting \n0-2 of the objects\nQ: Which item is the \nagent closer to at the \nend of given \ntrajectory?\nA. Yellow ball\nB. Blue ball\nFigure 7: Illustration for Reality Check task\nIn this scenario, the agent is tasked with visiting\n0-2 of the objects in the grid world. The agent has\nfull observation in the world for efficient naviga-\ntion. At least 2 objects (12 maximum) are placed\nin the environment. To test an LLM’s ability to\nunderstand the physical actions taken by the agent,\nwe ask it about the distance between the agent and\nvarious objects after it accomplishes a number of\nactions. The action planner is either a shortest path\nplanner towards specified objects or a random ac-\ntion generator.\nAfter the agent has finished 10 random actions,\nor right after it has visited one object with an opti-\nmal action planner, the task-related question will\nbe generated with the following format:\nAfter having taken these actions, which\nitem is the agent closer to?\nA. <object1.color> <object1.name>\nB. <object2.color> <object2.name>\nHere object1 and object2 both exist in the\nenvironment, and one of them is guaranteed to be\nthe target object if there is such a goal. There are\nalways two options for this task.\nAlong with the task-related question, the prompt\nincludes a description of the grid world environ-\nment, the action space of the agent (only going\nforward, turn left / right in this task), a board-like\ndepiction of the initial state, the list of actions taken\nby the agent, and the agent’s location and face direc-\ntion for each step. For more details on prompting,\nplease refer to section B.\nThe data for Task 0 are autonomously generated\nusing seeds and a shortest-path planner.\nTask 1: Short Term Intention\nTask setting: \nnavigate to one of \nthe two objects\nQ: Which action will \nthe agent take next?\nA. Left\nB. Right\nC. Forward\nFigure 8: Illustration for Short Term Intention task\nIn this scenario, the agent is tasked with visiting\neither of the two objects in the grid world. The\nLLM is not provided with the goal object. Rather,\nit must determine the goal object by examining and\nunderstanding the agent’s trajectory. In this task,\nthe agent has full observation in the world, and\nthere are exactly two objects placed. The object\ntypes and colors are randomly generated. The size\nof the room can be randomly sampled from the\nrange 6 by 6 to 12 by 12.\nTo test an LLM’s understanding of short-term\nintention, we ask it to predict the next action of the\nagent given its previous trajectory.\nThe agent’s trajectory halts at a random step,\nwith the exception of the precise moment when\nthe optimal paths to the two objects diverge. This\n\"cutting point\" is set as an exception and also serves\nas the mean for the normal distribution from which\nthe stopping point is sampled.\nBy restricting the cutting point in this manner,\nwe guarantee the trajectory included in the prompt\nto be optimal for reaching the potential goal objects.\nThis reduces the ambiguities of our experiments,\nand thus improves the significance of our results.\nThe task-related question has the following for-\nmat:\nWhich action will the agent take next?\nA. left\nB. right\nC. forward\nThe LLM should be able to choose which action\nwould be next were the agent to continue its optimal\ntrajectory to the goal object. The data for Task 1 are\n1026\nautonomously generated using seeds and a shortest-\npath planner.\nTask 2: Long Term Intention\n① Pickup Key \n(Done)\n② Open the door \n(Ongoing)\n③Pickup ball \n(Pending)\n① Pickup key \n(Finished)\nTask setting: complete three subgoals \nin as few steps as possible\nQ: which subgoal is the agent currently \ntrying to complete?\nA. Locate and pick up a key\nB. Locate and go through a door\nC. Navigate to the object in the new \nroom\nFigure 9: Illustration for Long term intention task\nIn this scenario, the agent needs to complete\nthe following subgoals in as few steps as possible:\n1) Locate and pick up a key; 2) Locate and go\nthrough a door; 3) Navigate to the object in the new\nroom. There are two rooms in this setting, which\nare connected by a locked door. The key of the\ndoor is always in the room in which the agent is\ninitially located. The object is always placed in the\nother room.\nWe provide an LLM with a subset of the agent’s\ntrajectory and ask it which subgoal the agent is\ncurrently trying to complete.\nThe task-related question has the following for-\nmat:\nBased on the agent’s trajectory thus\nfar, which subgoal is the agent currently\ntrying to complete?\nA. Locate and pick up a key\nB. Locate and go through a door\nC. Navigate to the object in the new room\nThe data for Task 2 are autonomously generated\nusing seeds and a shortest-path planner.\nTask 3: Desire\nIn this scenario, the agent is required to pick up\nthree objects as soon as possible. There are 2 types\nof objects in the world (e.g., blue balls and white\nballs), 3 of each. The agent may or may not have\na preference for one object type (we stratify the\ndata such that in 50% of the episodes the agent has\nt = 0\nt = 1\nt = 7\nt = 14\nTask setting: pickup objects in the \nenvironment. It may or may not have \nits own preference over some objects.\nQ: Which object (if any) does the \nagent prefer?\nA. Blue ball\nB. White ball\nC. No preference\nFigure 10: Illustration for Desire task\na preferred object type and in the other 50% the\nagent lacks a preference). We also deduct from the\nfinal reward given to the agent when it takes a large\nnumber of steps to finish picking up three objects.\nThis task tests whether an LLM is able to deter-\nmine the desire of the agent (for one object type or\nthe other) by examining its trajectory. We prepared\nthe following question for this task:\nWhich object does the agent prefer?\nA. white ball\nB. blue ball\nC. no preference\nThe data for Task 3 are autonomously generated\nusing seeds and Reinforcement Learning. We use\nthe PPO algorithm (Schulman et al., 2017) to train\nthe model. In the scenario wherein a preference is\npresent, the preferred object type yields 10 times\nmore reward than the non-preferred one. In the\nscenario wherein the preference is absent, both\nobject types yield identical rewards.\nTask 4: Percept\nIn this scenario, the agent is instructed to navi-\ngate in two rooms and reach the goal in the other\nroom as fast as possible. In contrast with previous\ntask settings, the agent has either a very limited\nvisual range (3 x 3 grid in the front), or an “in-\nfinitely” large visual range (for practical purposes,\nthe visual range is actually a 101 x 101 grid). Nat-\nurally, an agent with a smaller viewing range will\n1027\nPerception RangeShortLong\nHuman POV LLM POV\nDifferent behaviors due to perception\n3x3 view \nrange only\nDirectly see \nthe layout\nTask setting: navigate to the green ball with given \nperception range.\nQ: Given only the environment layout and agent’s \ntrajectory, what is the most likely perceptual range \nthat the agent possesses?\nA. 3 blocks\nB. Infinitely many blocks\n*The perceptual range refers to the number of cells that \nan agent can see in front of it\nFigure 11: Illustration for Percept task\nmake more mistakes (e.g., navigating to a dead end)\nwhile trying to reach the goal object. Obstacles are\nrandomly placed in each room to block the agent’s\nview.\nLLMs are expected to only look at the trajec-\ntory of the agent in an environment, and determine\nwhether the agent has a limited view range or a\nnearly full view range. The question format is as\nfollows:\nBased on the agent’s actions, what is\nthe most likely perceptual range that the\nagent possesses? The perceptual range\nrefers to the number of cells that an\nagent can see in front of it.\nA. 3 blocks\nB. infinitely many blocks\nWe manually collected 100 trials in total for both\nsituations.\nTask 5 & 6: First and Second Order Belief\nIn this scenario, there are two agents in the environ-\nment. Both of them are initially in the main room\n(on the left side of the whole grid world; see Figure\n4). On the right side, there are three small rooms.\nAgents can freely go in and out of each room. They\ncan see everything inside the current room and can\nsee the other rooms through the door if it is open.\nThis task reproduces the unexpected transfer\n(Sally-Anne) test (Baron-Cohen et al., 1985;\nPerner and Wimmer, 1985), with both first-order\nand second-order belief checking. In the first-order\nbelief task, one agent does not see the second agent\ntransfer a ball from one room to another. Presum-\nably, therefore, the agent falsely believes the ball to\nbe in its previous location. The second-order belief\ntask extends the first-order belief task by enabling\nthe agent with the false belief to see the ball in\nits new location. The other agent, however, does\nnot witness the first agent rectifying its false belief,\nso it presumably holds a second-order false belief\n(about the belief state of the first agent). By vary-\ning the observations that each agent makes (e.g.,\nwhether or not each agent sees the transfer of the\nball from one room to the another) as well as the\norder of these events, this task setting allows us to\ncheck an LLM’s first-order and second-order belief\ncapabilities.\nRather than asking the LLM directly where to\nfind the object, we provide two board-like belief\nstates as options. We then query the LLM about\nwhich board-like state the agent is more likely to\nbelieve.\nData for tasks 5 & 6 are collected via rule-based\nplanners with several scenarios.\nTask 7: Non-Literal Communication\nNavigate to white ball\nThen visit yellow ball\nI have navigated to two objects.\nI have not visited white ball yet.\nTask setting: visiting all the objects, \nand report its progress with texts.\nQ: The agent claims that it at one point \nnavigated to blue ball. Based on the \nagent’s actions, is it telling the truth?\nA. Yes\nB. No\nFigure 12: Illustration for Non-literal communication\ntask\nThis task focuses specifically on one form of\nnon-literal communication: lying. Within this task,\nthe LLM is told explicitly that there is an agent\ntasked with navigating to all of the objects within\nthe grid world. In each instance of the task, how-\never, the agent only visits a subset of the objects\nin the environment. The LLM is subsequently told\nthat the agent has claimed success in visiting a par-\nticular object. This object is randomly selected so\nthat sometimes it is an object that has actually been\nvisited and other times it is not.\n1028\nThe question format is as follows:\nBased on the agent’s actions, is it\ntelling the truth?\nA. yes\nB. no\nTo successfully complete this task, the LLM\nmust combine its knowledge about the physical oc-\ncurrences taking place within the grid world with\nits knowledge about lying, a vital component in\nBeaudoin et al. (2020)’s category of non-literal\ncommunication.\nTask 8: Knowledge\nTask setting: pick up both two objects in the \nenvironment with limited visibility. One of their \nlocation is known. The other is unknown. LLM will \nonly see the environment layout and agent’s \ntrajectory.\nQ: Based on the agent's actions, does it know the \nposition of blue ball before?\nA. Yes\nB. No\nt = 0\nt = 9\nt = 20\nt = 32\nAgent starts to look for two balls\nAgent directly goes to blue ball\nThen it explores an empty room\nFinally the white ball is found\nFigure 13: Illustration for Knowledge task\nThis task requires the agent to pick up two ob-\njects that exist in the environment. Both two objects\nare known to be placed separately in two of the four\nrooms.\nIn this scenario, the agent is informed of the loca-\ntion of one object, while the location of the second\nobject remains unknown. The agent is instructed to\ncollect the objects in a specific sequence. Ideally,\nif the agent knows an object’s location, it should\nproceed directly to the appropriate room. Other-\nwise, it will have to search for the object in the\nyet-to-be-explored rooms.\nWe include the agent’s entire trajectory in the\nprompt. We then ask the LLM to determine if the\nagent knows the position of one object before. The\nquestion format is presented as follows:\nBased on the agent’s actions, does\nit know the position of <color> <name>\nbefore?\nA. Yes\nB. No\nThe data for Task 8 are autonomously generated\nusing seeds and a rule-based planner.\nTask 9: Emotions\nThis task (see Figure 5) requires LLMs to infer\nthe emotions of agents in a situated context from\ntheir physical behaviors. Specifically, it involves\nvariations on a theme involving a small lake of wa-\nter. In every variation, one agent pushes another\nagent into the lake. In some of these variations,\nan observer is privy to the situation. The prompt\nthen asks the LLM about this observer’s feelings\ntowards both the victim and the perpetrator. Pre-\nsumably, the observer should experience sympathy\nfor the victim and anger (or a similarly negative\nemotion) for the perpetrator.\nHow would <observer.name> most likely\nfeel about <pusher.name>?\nA. no strong emotion\nB. angry\nIn other variations, a helper comes along and\npulls the victim out of the lake. Presumably, the\nobserver should feel positive emotions (e.g. respect,\ngratitude) for this helper. The question format is as\nfollows:\nHow would <observer.name> most likely\nfeel about <helper.name>?\nA. no strong emotion\nB. respectful\nB Prompting and Reproducibility\nIn this pilot study, our data curation follows a\nuniform structure across all tasks similar to prior\nwork (Li et al., 2022), deviating only slightly to\naccount for task-specific circumstances.3\nEnvironment Description Each prompt begins\nwith a description of the two-dimensional world\nwherein the task will take place. Specifically, our\nprompting code provides LLMs with the dimen-\nsions of the game board and a method to reference\nspecific cells (column-first Cartesian coordinates).\n3The data for this pilot study is available at https://\nhuggingface.co/datasets/sled-umich/2D-ATOMS.\n1029\nEach prompt subsequently itemizes the various ob-\njects that are situated in the world, along with their\ncoordinates and attributes. Although this prompt-\ning structure could be easily adapted to handle\nmany different types of attributes, we focused only\non color for the sake of simplicity. Additionally,\nthis section assigns each object a “label”: a single\nletter that the prompt uses to represent the object\nin a printed grid representation.\nAgent Description, Observability, and Task\nThe next section of each prompt is a detailed de-\nscription of the agent(s) occupying the grid world.\nIn the example below, which was taken from task\n1, only one agent occupies the grid world. In multi-\nagent tasks, this section details the various proper-\nties of all agents. Whether single or multi-agent,\nhowever, the basic properties are the same. The\nprompt first details the position and direction of the\nagent. It is located in a specific cell, and it always\nfaces up, down, left, or right. The prompt then spec-\nifies the various actions that an agent is capable of\ntaking (e.g. “forward”, “left”, and “open”). The\nagent’s labels are then specified. Within the printed\ngrid representation, the agent is always represented\nas a V-like shape depending on the direction that it\nis facing. For instance, the prompting tool uses <\nto represent an agent that is facing left. Finally, the\nprompt specifies two key attributes of the agent: (1)\nits level of observability (e.g. whether or not it can\nsee into adjacent rooms) and (2) its goal. Some-\ntimes these two descriptors are heavily modified,\nrestricted, or removed altogether so that they do not\ninterfere with the task. For instance, when testing\nLLMs for percepts, the prompt does not specify the\nvisual range of the agent.\nAction Sequences Following the agent descrip-\ntion section, the prompt prints out a board represen-\ntation, a multi-line sequence of plain text that ap-\npears two-dimensional when printed out. Here, the\nvarious objects and agents are depicted in position\nby their associated labels. Additionally, the config-\nuration of the walls is specified by a perimeter of\nW’s. Next, the prompt specifies a sequence of ac-\ntions that take place over the course of the episode\nbeing considered by the LLM. In the episode de-\npicted below, the agent navigates part of the way\nto a red box. These actions always specify the new\nposition and orientation of the agent.\nQuestions and Answer Candidates Finally,\neach prompt contains a question that the LLM must\nanswer. These questions are the most task-specific\nportion of the prompt, so their contents vary, how-\never, they are always multiple-choice. Addition-\nally, they always contain a set of instructions below\nheeding the LLM to return only a single letter in\nits response.\n1030\nSample Prompt for Task 1\nThis is a grid-like 2D world\nThe grid world consists of 6 rows and\n6 columns, 0-based\nWe use (i,j) to represent the i-th\ncolumn (from left to right) and j-th\nrow (from top to bottom).\nThe following is a list of objects in\nthis world. Each line starts with the\nobject's position and is followed by\nits attributes\n(2, 3): key, grey; represented by this\nlabel: G\n(4, 4): box, red; represented by this\nlabel: H\nWalls are depicted using the symbol W\nThere is an agent at (2, 2) facing\nleft\nThe agent can take the following\nactions:\n- left: makes the agent face left of\nwhere it is currently facing\n- right: makes the agent face right\nof where it is currently facing\n- forward: makes the agent move one\nstep in the direction it is currently\nfacing\n- open: makes the agent open a door\nthat it is in front of\n- pickup: makes the agent pick up\nthe object that it is in front of\n- drop: makes the agent drop an item\nthat it is holding\n- stay: makes the agent stay where it\ncurrently is for a timestep\nThe agent is represented by the\nfollowing labels depending on which\ndirection it is facing:\n- Facing left: <\n- Facing up: ^\n- Facing right: >\n- Facing down: v\nThe agent has full observability,\nmeaning it can see the entire world\nThe agent has been instructed to\nnavigate to one of the two objects\nin the environment, although you do\nnot know which\nThis is the starting state of the\nboard:\n```\n0 1 2 3 4 5\n0 | W W W W W W\n1 | W O O O O W\n2 | W O < O O W\n3 | W O G O O W\n4 | W O O O H W\n5 | W W W W W W\n```\nThis list contains a sequence of\nactions taken by the agent\n(Step 1) The agent took action left\nand is now at (2, 2) facing down\n(Step 2) The agent took action left\nand is now at (2, 2) facing right\n(Step 3) The agent took action forward\nand is now at (3, 2) facing right\n(Step 4) The agent took action forward\nand is now at (4, 2) facing right\n(Step 5) The agent took action right\nand is now at (4, 2) facing down\nWhich action will the agent take next?\nA: left\nB: right\nC: forward\nPlease ONLY respond using the letter\ncorresponding to your answer\nDo not generate any text other\nthan the letter\n1031",
  "topic": "Situated",
  "concepts": [
    {
      "name": "Situated",
      "score": 0.9758377075195312
    },
    {
      "name": "Position paper",
      "score": 0.6232534646987915
    },
    {
      "name": "Theory of mind",
      "score": 0.5885205864906311
    },
    {
      "name": "Computer science",
      "score": 0.5699599385261536
    },
    {
      "name": "Position (finance)",
      "score": 0.5060011744499207
    },
    {
      "name": "Grid",
      "score": 0.49193358421325684
    },
    {
      "name": "Cognitive science",
      "score": 0.39365315437316895
    },
    {
      "name": "Data science",
      "score": 0.3747987449169159
    },
    {
      "name": "Human–computer interaction",
      "score": 0.34819719195365906
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3340839147567749
    },
    {
      "name": "Psychology",
      "score": 0.2722412049770355
    },
    {
      "name": "World Wide Web",
      "score": 0.15067574381828308
    },
    {
      "name": "Geography",
      "score": 0.09524410963058472
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Finance",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Cognition",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I27837315",
      "name": "University of Michigan",
      "country": "US"
    }
  ]
}