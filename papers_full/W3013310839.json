{
  "title": "Variational Transformers for Diverse Response Generation",
  "url": "https://openalex.org/W3013310839",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4202063432",
      "name": "Lin, Zhaojiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223199573",
      "name": "Winata, Genta Indra",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098197978",
      "name": "Xu Peng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2358019462",
      "name": "Liu Zihan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743188307",
      "name": "Fung, Pascale",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2964026424",
    "https://openalex.org/W2962717182",
    "https://openalex.org/W2963825865",
    "https://openalex.org/W592244745",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W2789543585",
    "https://openalex.org/W2963411289",
    "https://openalex.org/W2962772361",
    "https://openalex.org/W2951004968",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963790827",
    "https://openalex.org/W2161466446",
    "https://openalex.org/W2188365844",
    "https://openalex.org/W2626792426",
    "https://openalex.org/W2890940245",
    "https://openalex.org/W2963223306",
    "https://openalex.org/W2962974924",
    "https://openalex.org/W1591706642",
    "https://openalex.org/W2550893117",
    "https://openalex.org/W2328886022",
    "https://openalex.org/W2970303069",
    "https://openalex.org/W2963713328",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W2963188990",
    "https://openalex.org/W2964352131",
    "https://openalex.org/W2964339599"
  ],
  "abstract": "Despite the great promise of Transformers in many sequence modeling tasks (e.g., machine translation), their deterministic nature hinders them from generalizing to high entropy tasks such as dialogue response generation. Previous work proposes to capture the variability of dialogue responses with a recurrent neural network (RNN)-based conditional variational autoencoder (CVAE). However, the autoregressive computation of the RNN limits the training efficiency. Therefore, we propose the Variational Transformer (VT), a variational self-attentive feed-forward sequence model. The VT combines the parallelizability and global receptive field of the Transformer with the variational nature of the CVAE by incorporating stochastic latent variables into Transformers. We explore two types of the VT: 1) modeling the discourse-level diversity with a global latent variable; and 2) augmenting the Transformer decoder with a sequence of fine-grained latent variables. Then, the proposed models are evaluated on three conversational datasets with both automatic metric and human evaluation. The experimental results show that our models improve standard Transformers and other baselines in terms of diversity, semantic relevance, and human judgment.",
  "full_text": "Variational Transformers for Diverse Response Generation\nZhaojiang Lin†, Genta Indra Winata, Peng Xu, Zihan Liu, Pascale Fung\nCenter for Artiﬁcial Intelligence Research (CAiRE)\nDepartment of Electronic and Computer Engineering\nThe Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong\n†zlinao@connect.ust.hk\nAbstract\nDespite the great promise of Transformers in\nmany sequence modeling tasks (e.g., machine\ntranslation), their deterministic nature hinders\nthem from generalizing to high entropy tasks\nsuch as dialogue response generation. Previ-\nous work proposes to capture the variability of\ndialogue responses with a recurrent neural net-\nwork (RNN)-based conditional variational au-\ntoencoder (CV AE). However, the autoregres-\nsive computation of the RNN limits the train-\ning efﬁciency. Therefore, we propose the Vari-\national Transformer (VT), a variational self-\nattentive feed-forward sequence model. The\nVT combines the parallelizability and global\nreceptive ﬁeld of the Transformer with the\nvariational nature of the CV AE by incorpo-\nrating stochastic latent variables into Trans-\nformers. We explore two types of the VT:\n1) modeling the discourse-level diversity with\na global latent variable; and 2) augmenting\nthe Transformer decoder with a sequence of\nﬁne-grained latent variables. Then, the pro-\nposed models are evaluated on three conversa-\ntional datasets with both automatic metric and\nhuman evaluation. The experimental results\nshow that our models improve standard Trans-\nformers and other baselines in terms of diver-\nsity, semantic relevance, and human judgment.\n1 Introduction\nConvolutional and fully-attentional feed-forward\narchitectures, such as Transformers (Vaswani et al.,\n2017), have emerged as effective alternatives to\nRNNs (Dehghani et al., 2018) in wide range of\nNLP tasks. These architectures remove the compu-\ntational temporal dependency during the training\nand effectively address the long-standing vanish-\ning gradients problem of recurrent models by pro-\ncessing all inputs simultaneously. Notably, trans-\nformers apply a fully attention strategy, where each\ntoken in the sequence is informed by other tokens\nvia a self-attention mechanism. It acts as an ef-\nfectively global receptive ﬁeld across the whole\nsequences which absence in RNNs. Despite the\npowerful modeling capability of trasnformers, they\noften fail to model one-to-many 1 relation in dia-\nlogue response generation tasks (Zhao et al., 2017)\ndue to their deterministic nature. As a result, they\ngenerate dull and generic response (e.g., “I am not\nsure”), especially with greedy and beam search,\nwhich are widely used in other sequence modeling\ntasks. There have been attempts to generate di-\nverse and informative dialogue responses by incor-\nporating latent variable(s) into the RNN encoder-\ndecoder architecture. In particular Zhao et al.\n(2017) adapt a conditional variational autoencoder\n(CV AE) to capture discourse-level variations of dia-\nlogue, while Goyal et al. (2017) and Du et al. (2018)\nintegrates latent variables in the hidden states of the\nRNN decoder. However, the inherently sequential\ncomputation of aforementioned models limit the\nefﬁciency for large scale training.\nIn this paper, we introduce the Variational Trans-\nformer (VT) 2 a variational self-attentive feed-\nforward sequence model to address the aforemen-\ntioned issues. The VT combine the parallelizability\nand global receptive ﬁeld of the transformer with\nthe variational nature of CV AE by incorporating\nstochastic latent variables into transformers. We\nexplore two types of VT: 1) Global Variational\nTransformer (GVT), and 2) Sequential Variational\nTransformer. The GVT is the extension of CV AE in\nZhao et al. (2017), which modeling the discourse-\nlevel diversity with a global latent variable, While\nSVT, inspired by variational autoregressive mod-\nels (Goyal et al., 2017; Du et al., 2018), incor-\nporates a sequence of latent variables into decod-\n1Given a similar dialogue history, there may exist many\nvalid responses.\n2The source code is available in https://github.\ncom/zlinao/Variational-Transformer\narXiv:2003.12738v1  [cs.CL]  28 Mar 2020\ning process by using a novel variational decoder\nlayer. Unlike previous approaches (Zhao et al.,\n2017; Goyal et al., 2017; Du et al., 2018), SVT\nuses Non-causal Multi-head Attention , which at-\ntend to future tokens for computing posterior latent\nvariables instead of using an additional encoder.\nThe proposed VT architectures integrate stochas-\ntic latent variables into Transformers. The ex-\nperimental results on a three conversation dataset\ndemonstrate that our models can generate more\ninformative and coherent responses.\n2 Related work\n2.1 Neural Conversational Models\nConversational systems has been widely stud-\nied (Weizenbaum et al., 1966; Wallace, 2009;\nVinyals and Le, 2015; Serban et al., 2016). Com-\npare to rule-based systems (Weizenbaum et al.,\n1966; Wallace, 2009), sequence-to-sequence con-\nversation models achieve superior performance in\nterms of scalable training and generalization abil-\nity (Vinyals and Le, 2015). However, it has been\npointed out that encoder-decoder models tend to\ngenerate generic and repetitive responses like “I\nam sorry” (Li et al., 2016a). To address this issue,\nthere have been three main lines of work. The ﬁrst\nis adding additional information (e.g., persona) as\ninput to guild model generate more informative\nresponses (Li et al., 2016b; Zhang et al., 2018).\nThe second modiﬁes the learning objective to pro-\nmote more diverse generation (Li et al., 2016a), and\nthe third integrates stochastic latent variables into\nSeq2Seq models by using the CV AE framework\n(Serban et al., 2017; Zhao et al., 2017). Our work\ncomes within this third line introducing a novel\nmodel, the Variational Transformer, to improve di-\nalogue response generation.\n2.2 Conditional Variational Autoencoders\nMany works have attempted to combine CV AEs\nwith encoder-decoder architectures for sequence\ngeneration tasks. Zhang et al. (2016) propose a vari-\national encoder-decoder model for neural machine\ntranslation, while Li et al. (2017) apply variational\nrecurrent neural networks (VRNN) (Chung et al.,\n2015) for text summarization. Zhao et al. (2017)\nand Zhou and Wang (2018) explore incorporating\nmeta features into CV AE framework in dialogue\nresponse generation tasks. (Goyal et al., 2017) and\n(Du et al., 2018) propose variational autoregressive\ndecoders which enhanced by highly multi-modal\nlatent variables to capture the high variability in di-\nalogue responses. Le et al. (2018) further augment\nvariational autoregressive decoders with dynamic\nmemory networks for improving generation quality.\nWe unify the previous successful ideas of CV AE,\nand explore the combinations of CV AE and Trans-\nformer.\n2.3 Fully Attentional Networks\nTaking advantage of the parallel-in-time structure\nand global receptive ﬁeld, Transformers (Vaswani\net al., 2017) have recently been shown to achieve\nimpressive results on various sequence modeling\ntasks. Based on this, several follow-up models\nhave been presented. The Image Transformer (Par-\nmar et al., 2018) has been proposed for im-\nage generation, while the MultiModel (Kaiser\net al., 2017) integrates convolution, attention and\nsparsely-gated mixture-of-expert blocks into a sin-\ngle deep-learning model for simultaneously learn-\ning multiple tasks from various domains. Lin\net al. (2019) proposed a fully attentional mixture-\nof-expert model (MoEL) for empathetic dialogue\nmodeling. The Universal Transformer (Dehghani\net al., 2018) incorporates the recurrent inductive\nbias of RNNs into the standard Transformer, and\nachieves better result on a wide range of algorith-\nmic and language understanding tasks. Kaiser et al.\n(2018) introduce the Latent Transformer (LT) for\nnon-autoregressive machine translation. During\ntraining, the LT ﬁrst autoencodes a target sequence\ninto a shorter sequence discrete latent variables.\nThen a parallel decoder decodes the target using\ndiscrete latent variables and an input sequence. Dif-\nferent from the LT (Kaiser et al., 2018), the VT\ngenerates continuous latent variables during the\ndecoding process.\n3 Preliminaries\n3.1 Conditional Variational Autoencoder for\nDialogue Generation\nThe CV AE framework (Sohn et al., 2015) repre-\nsents a dyadic conversation via three random vari-\nables: the input condition c, including conversation\ncontext and meta features (meta features can be\nignored when not available); a latent variable z;\nand the target response x. A CV AE can be efﬁ-\nciently trained with Stochastic Gradient Variational\nBayes (SGVB) (Kingma and Welling, 2013) by\nmaximizing the variational lower bound of xgiven\nc, according to:\np(x|c) =\n∫\nz\np(x|z,c)p(z|c)dz. (1)\nThe typical CV AE consists of a prior network\npθ(z|c), which is used to approximate p(z|c), a\nrecognition network pφ(z|c,x), which is used to\napproximate posterior distribution q(z|c,x), and\na decoder pθ(x|z,c), which is used to approxi-\nmate p(x|z,c). By assuming z follows multivariate\nGaussian distribution with a diagonal co-variance\nmatrix, the evidence lower bound (ELBO) can be\nwritten as\nLELBO = LREC −LKL\n= Eqφ(z|c,x) [log pθ(x|z,c)]\n−KL(qφ(z|c,x)∥pθ(z|c))\n≤log p(x|c),\n(2)\nwhere LREC denotes the reconstruction loss and\nLKL denotes the Kullback-Leibler (KL) diver-\ngence between the posterior and prior.\nIn dialogue generation tasks, previous works\n(Zhao et al., 2017; Zhou and Wang, 2018) apply\nRNN encoders (with GRU or LSTM cell) to en-\ncode dialogue contexts and responses separately.\nThe condition cis represented by the concatenation\nof the last hidden state of the context encoder and\nthe meta features (e.g., topic, emotion), while the\nresponse xis represented by the last hidden state of\nresponse encoder. Then the prior network pθ(z|c)\nand the recognition network pφ(z|c,x) parameter-\nized by multi-layer perceptrons (MLPs) are applied\nto approximate the means and the log variances\nof the prior latent distribution N\n(\nz; µ′,σ′2I\n)\nand\nposterior latent distribution N\n(\nz; µ,σ2I\n)\n. With\nthe reparameterization trick (Kingma and Welling,\n2013), we can obtain samples of the prior latent\nvariable (for testing) from N\n(\nz; µ′,σ′2I\n)\nand sam-\nples of the posterior latent variable (for training)\nfrom N\n(\nz; µ,σ2I\n)\n. Finally, an RNN decoder use\nzand cas the initial state to predicts the response\nx.\nThe vanishing latent variable problem (Bow-\nman et al., 2016) is a common issue in RNN-based\nCV AEs. That is, the powerful autoregressive RNN\ndecoder ﬁrst learns to ignore the latent variable,\nand decodes the response by only condition on the\nprevious tokens. Thus the latent variable fails to\nencode the meaningful information, and the CV AE\ndeteriorates to seq2seq model. To alleviate this\nissue, KL annealing (Bowman et al., 2016) and\n    Context\nTRS Decoder\nResponse\nzz\nSOS\nGeneration\nRecognition Net Prior Net\nContext TRS Encoder\nCLS\nResponse TRS Encoder\nCLS\nAttention\nMechanism\nTrain only\nTrain & Test\nFigure 1: The Global Variational Transformer. During\ntraining, The posterior latent variablezby the posterior\nnetwork is passed to the decoder, while during testing,\nthe target response is absent, and z is replaced by the\nprior latent variable. The word embeddings, positional\nencoding, softmax layer and meta vectors are ignored\nfor simplicity\nbag-of-word loss (Zhao et al., 2017) have been pro-\nposed, and have shown effectiveness in various\ndialogue tasks (Zhao et al., 2017; Zhou and Wang,\n2018).\n3.2 CV AE with Transformer\nThe aforementioned RNN-based CV AE framework\nintegrate the latent variable into the initial state of\nRNN decoder, while in transformer, it is more ﬂex-\nible to incorporate the latent variable embedding\ninto the ﬁrst input token of the decoder to generate\nthe initial state.\nThe overall architecture of GVT is depicted in\nFigure 1. Different from RNNs, the Transformer\nencoder maps an input sequence of symbol repre-\nsentations to a sequence of contextualized represen-\ntations (Vaswani et al., 2017). In order to get ﬁxed\ndimension representations of the response and con-\ntext, we add a special token CLS at the beginning\nof the input sequence as in BERT (Devlin et al.,\n2018), to compute the weighted sum of the output\nrepresentations via self-attention. Thus the output\nrepresentation of the token CLS is considered as\nthe representation of the whole sequence. Then\nwe introduce a recognition network and a prior\nnetwork to compute the posterior latent variable\nand prior latent variable as in (Zhao et al., 2017;\nZhou and Wang, 2018). We add the latent vari-\nable sample zand meta features m(can be ignored\nwhen not available) into eSOS, the embedding of\nthe start-of-sequence token SOS:\ne′\nSOS = z+ m+ eSOS. (3)\nFinally, the transformer decoder decodes the re-\nsponse xsequentially while attending to the new\nembedding e′\nSOS of token SOS with latent infor-\nmation.\nThis design enhances the CV AE framework with\nthe global receptive ﬁeld, and each position of the\nGVT can directly access the latent information via\nthe multi-head self-attention mechanism. However,\nwe still observe that the GVT suffers the vanishing\nlatent variable problem as RNN-based CV AE be-\ncause the decoder can bypass the latent information\nby paying less attention to the SOS token. Hence,\nwe apply the KL annealing, and bag-of-word aux-\niliary loss Lbow as in (Zhao et al., 2017; Zhou and\nWang, 2018) to preserve the useful information of\nthe latent variable. Therefore, the learning objec-\ntive of the GVT is deﬁned as follows:\nL= LELBO + Lbow. (4)\n4 Sequential Variational Transformer\nIn order to augment the capacity of the latent vari-\nable with multi-modal distributions and to better\nutilize the latent information, we further explore\nincorporating a sequence of latent variables in de-\ncoding process. We introduce Sequential Varia-\ntional Transformer (SVT) with a novel variational\ndecoder layer which generate latent variables for\neach position: z= (z1,...,z T). Similar to Goyal\net al. (2017), we interpret the latent variables as\na generation plan for the future sequence. Un-\nlike previous CV AE models which use an extra en-\ncoder to encode the response separately (Zhao et al.,\n2017; Zhou and Wang, 2018) or use a backward\nRNN to encode the future sequence for each time\nstep (Goyal et al., 2017; Du et al., 2018), SVT uses\na Non-causal Multi-head Attention which leaks the\nfuture information to the recognition network for\ncomputing the posterior latent variables.\nAs shown in Figure 2, the SVT shares the same\nencoder as the standard Transformer (Vaswani\net al., 2017), while its decoder consists of a vari-\national decoder layer followed by a stack of N\nstandard Transformer decoder layers. The varia-\ntional decoder layer has two paths for computing\nthe posterior latent variable and prior latent variable\nrespectively. We denote them asPosterior Pathand\nPrior Path.\nTRS Encoder\nMulti-head\nAttention\n Multi-head\nAttention\nRecognition Net Prior Net\n Masked Multi-\nhead Attention\nLayer-norm\n+ + \nLayer-norm Layer-norm\n+ + \nLayer-norm Layer-norm\nz z\nFFN\nLayer-norm\n+ \nContext\n  Response\n(shifted right)\nTRS Decoder\nLayer ×\u0000\nGeneration\nAttention\nMechanism\nTrain only\nTrain & Test\nc\n+ Sum\nc Concatenation\n\u0000\u0000\u0000\u0000\nFigure 2: The Sequential Variational Transformer. Dur-\ning training, The posterior latent variables z by the pos-\nterior network are passed to the decoder, while during\ntesting, the target response is absent, and z is replaced\nby the prior latent variables z. The word embeddings,\npositional encoding, softmax layer and meta vectors are\nignored for simplicity\n4.1 Prior Path\nThe Prior Path(solid line in Figure 2) has a masked\nmulti-head self-attention sub-layer which performs\ncausal attention on the shifted response, followed\nby a multi-head self-attention sub-layer which per-\nforms encoder-decoder multi-head attention on the\ncontext encoder. The last sub-layer is composed\nof a MLP prior network which approximates a se-\nquence of prior latent variable for each position,\nand a Position-wise Feed-Forward Network (FFN)\nwhich fuse the latent information z with the ob-\nserved information representation oP before the\nprior network (shown in Figure 2). Speciﬁcally,\nwe concatenate oP with zas the input to the FNN,\nand the FNN pass the fused representation to the\nnext layer. Same as Vaswani et al. (2017), in the\nvariational decoder layer, each sub-layer is fol-\nlowed by a residual connection and layer normal-\nization. That is, the output of each sub-layer is\nLayerNorm(x+ Sublayer(x)).\nWe decompose the response x as x =\n(x1,··· ,xT) and the latent variable z as z =\n(z1,...,z T). The prior model produces latent vari-\nables at each position zt by not only conditioning\non the input condition c(the concatenation of con-\ntext and meta features), but also conditioning on\nthe observed response tokensx1:t−1. By assum-\ning zt follows a multivariate Gaussian distribution,\nthe prior model becomes:\npθ(zt|c,x1:t−1) =N\n(\nzt; µ′\nt,σ′\nt\n)\n, (5)\nwhere\n[µ′\nt,log σ′\nt] =MLP(oP).\n4.2 Posterior Path\nThe only difference between the Posterior Path\n(dash line in Figure 2) and Prior Path is that the\nmask is removed from the masked multi-head at-\ntention. Thus the masked (casual) multi-head\nattention become non-casual multi-head atten-\ntion, which allows each position to attend to the\nsubsequent positions. Then, the second multi-head\nattention sub-layer (shared the same weight with\nprior path) performs posterior attention on the en-\ncoder and passes the posterior observed informa-\ntion oRto the recognition network. The recognition\nnetwork produces the posterior latent variable for\neach position zt as:\nqφ(zt|c,x) =N(zt; µt,σt) , (6)\nwhere\n[µt,log σt] =MLP(oR).\nDuring the training, the posterior path guides the\nlearning of prior path via KL divergence constraint:\nLKL =\n∑\nt\nKL(qφ(zt|c,x)∥pθ(zt|c,x1:t−1))\n(7)\nIn the training phase, the posterior latent variables\nfrom Equation 6 are passed to the FFN, while in\nthe testing phase the Posterior Pathwill be blocked\nand the posterior latent variables will be replaced\nwith the prior latent variables from Equation 5.\nDuring the decoding process, each response to-\nken xt is generated by conditioning on observed\nresponse tokens x1:t−1, latent variables z1:t, and\nthe input condition c. The decoding process of the\nSVT is:\npθ(x|z,c) =\n∏\nt\npθ(xt|z1:t,x1:t−1,c) . (8)\n4.3 Auxiliary Loss\nAs we expect the latent variables to be a genera-\ntion plan for the future sequence, we inject such\nbias into latent variables by using an auxiliary loss:\nSequential-Bag-of-Word (SBOW)which proposed\nby Du et al. (2018). The idea of the SBOW aux-\niliary objective is to sequentially predict the bag\nof succeeding target words xt:T by using latent\nvariable zt. In our case, the succeeding words pre-\ndiction also leverages the observed information c\nand x1:t−1. Thus the auxiliary loss at each position\nis computed by:\npξ(xt:T|zt,c,x 1:t−1) =faux(zt,oP) (9)\nwhere faux is a feed-forward neural network with\nthe softmax output.\n4.4 Learning\nThe evidence lower bound (ELBO) objective of\nSVT is the sum of the reconstruction loss LREC(t)\nand Kullback-Leibler divergence loss LKL(t) at\neach position:\nLELBO =\n∑\nt\nLREC(t) −LKL(t)\n=\n∑\nt\nEqφ(zt|c,x) [log pθ(xt|z1:t,x1:t−1,c)]\n−KL(qφ(zt|c,x)∥pθ(zt|c,x1:t−1)) .\n(10)\nWe regularize the ELBO learning objective with\nan auxiliary loss Lsbow to enhance the expressive-\nness of the latent variables. Therefore, the ﬁnal\nlearning objective is formulated as follows:\nL= LELBO + Lsbow, (11)\nwhere,\nLsbow =\n∑\nt\nEqφ(zt|c,x) [log pξ(xt:T|zt,x1:t−1,c)] .\n(12)\n5 Experiments\n5.1 Dataset\nWe evaluate the proposed models on three con-\nversationet dataset such as MojiTalk (Zhou and\nWang, 2018), PersonaChat (Zhang et al., 2018),\nEmpathetic-Dialogues (Rashkin et al., 2019).\nMojiTalk dataset consists of 596,959 post and re-\nsponse pairs from Twitter. Each response is labeled\nby one emoji which indicates the response emotion.\nThere are 64 emoji labels in total with unbalanced\ndistribution. We use the preprocessed data and vo-\ncabulary released from Zhou and Wang (2018) and\nfollow the same split of train/validation/test set.\nPersonaChat & Empathetic-Dialogues are\none-to-one multi-turn conversation datasets. In\nPersonaChat (Persona), the conversations are\nrevolve around personas which are established\nby four to six persona sentences. While in\nEmpathetic-Dialogues (ED), the conversation are\nmostly about situation that happened to one of the\nspeaker and another speaker is trying to understand\nthe feeling and reply accordingly. Both datasets\nare about modeling social skills and the goal is to\nmake user more engaging. Therefore, we combine\nthe train/validation/test set of two datasets.\n5.2 Baselines\nWe compare the proposed models with the follow-\ning baselines:\nSeq2Seq. An attention-based sequence-to-\nsequence model with the emoji vector as additional\ninput as discribed in MojiTalk (Zhou and Wang,\n2018).\nCV AE. An RNN-based conditional variational\nautoencoder for dialogue response genera-\ntion (Zhou and Wang, 2018), which uses a\nmultivariate Gaussian latent variable to model the\nresponse and concatenate it with the last hidden\nstate of the encoder as the initial state of the\ndecoder. KL annealing, early stopping strategy and\nbag-of-word auxiliary loss are applied during the\ntraining. We use the implementation 3 released by\nZhou and Wang (2018).\nTransformer. A transformer (Vaswani et al.,\n2017) trained by using a Maximum Likelihood\nEstimation (MLE) objective and can be considered\nas the base model for both the GVT and SVT.\n5.3 Hyper-parameters and Training Setup\nWe use a 4-layer Transformer as our base model.\nThe hidden size is set to be 300 everywhere, and\nthe word embedding is initialized with the 300-\ndimensional pre-trained GloVe embeddings for\n3The implementation of CV AE baseline: https://\ngithub.com/claude-zhou/MojiTalk\nboth encoder and decoder. The multi-head atten-\ntion sub-layers are made up of 4 attention heads\neach with embedding dimension 64. The size of\nlatent variable is 300. The recognition network\nand the prior network are parameterized by 3-layer\nMLPs with 512 hidden dimension. Following the\ntraining setup of Zhou and Wang (2018), we ﬁrst\ntrain our baseline transformer model with the MLE\nobjective and use it to initialize its counterparts in\nboth GVT and SVT. Then the models are trained\nend-to-end by the Adam optimizer with the ini-\ntial learning rate 2 ×10−4. KL annealing and early\nstopping strategy are applied as in (Zhou and Wang,\n2018). In the test time, we use greedy decoding\nstrategy for all models.\n5.4 Automatic Evaluation\nPPL & KLD. The evaluation metrics include\nPerplexity (PPL) and Kullback-Leibler divergence\nbetween the posterior and prior ( KLD). A well\ntrained model should achieve a low reconstruction\nand small but non-trivial KL distance (Zhao et al.,\n2018).\nDiversity. To measure the generation diversity,\nwe calculate Dist-1, Dist-2, and Dist-3, the ratio of\nthe number of distinct n-grams (unigrams, bigrams,\nand trigrams) over the total number of n-grams. A\nhigher distinct n-grams ratio indicates more diverse\ngeneration.\nEmbeddings Similarity. This metric computes\nthe cosine similarity between the sentence embed-\nding of a generated sequence and that of a ground-\ntruth response. In our experiments, we introduce\ntwo different ways to represent sentence embed-\ndings. The ﬁrst is EMBFT (Liu et al., 2016) that\ncalculates the average of word embeddings in a sen-\ntence using FastText (Mikolov et al., 2018) which\nis trained with Common Crawl and Wikipedia data.\nWe use FastText embeddings instead of other pre-\ntrained word embeddings because it can handle out-\nof-vocabulary issue. However, representing a sen-\ntence by simply taking the average of word embed-\ndings ignores the context information. Therefore,\nwe propose to use a pre-trained language model\nBERT (Devlin et al., 2018) to compute the con-\ntextualized sentence representation. Speciﬁcally,\nwe use a pre-trained BERT to encode a generated\nsentence and a ground-truth response, and average\nthe output representation of both to obtain the sen-\ntence embeddings. We denote such contextualized\nsentence embedding as EMBBERT.\nMojiTalk\nModel PPL KLD Diversity Embeddings Similarity Human Evaluation\nDist-1 Dist-2 Dist-3 EMBFT EMBBERT Coherence Emotion\nSeq2Seq 130.75 - 0.0055 0.0187 0.0347 0.738 0.594 20.67 20.67\nCV AE 35.33 27.55 0.0189 0.1340 0.3640 0.751 0.613 18.33 18\nTransformer 72.66 - 0.0040 0.0161 0.0324 0.741 0.596 19.67 23.33\nGVT 19.71 18.15 0.0207 0.1524 0.4064 0.753 0.609 23 22.67\nSVT 18.96 32.27 0.0079 0.1053 0.3654 0.762 0.619 26 27.67\nHuman - - 0.0557 0.4009 0.7697 - - - -\nPersona + ED\nModel PPL KLD Diversity Embeddings Similarity Human Evaluation\nDist-1 Dist-2 Dist-3 EMBFT EMBBERT Coherence Engagedness\nCV AE 31.32 10.01 0.0186 0.1102 0.295 0.917 0.666 20.67 21.33\nTransformer 48.03 - 0.0058 0.0237 0.0524 0.915 0.672 24.67 24.67\nGVT 18.34 19.13 0.0204 0.1406 0.3995 0.917 0.675 20 21.33\nSVT 17.75 24.67 0.0213 0.1521 0.3936 0.906 0.665 38.67 36.67\nHuman - - 0.0640 0.3800 0.7070 - - - -\nTable 1: Results of Variational Transformer compared to baselines on automatic and human evaluations.\n5.5 Human Evaluation\nIn the human evaluation, we prepare multiple-\nchoice questions for human evaluators and the an-\nswers are the generation results from the ﬁve mod-\nels (Seq2Seq, CV AE, Transformer, GVT, and SVT).\nwe ﬁrst randomly sample 100 dialogues and their\ncorresponding responses from our models and the\nbaselines. For each response, we assign three hu-\nman annotators to select the most coherent (on\ntopic) response to the context (multiple answers\nare allowed). In addition, annotators also need to\nchoose the best response correlated to the given\nemoji label in Mojitalk and the most engaging re-\nsponse in PersonaChat and Empathetic-Dialogues.\nIf there is no response that satisﬁes the evalua-\ntors, they can choose “all answers are bad”, which\nmeans none of the answer is chosen. We compute\nthe rate that each model is chosen to quantify gen-\neration quality regarding to the human standard.\n6 Results\n6.1 Quantitative Analysis\nThe automatic evaluation results are shown in Ta-\nble 1. Transformer-based models have signiﬁcantly\nlower perplexity compared to RNN-based models\nwhich indicate that the global receptive ﬁeld per-\nformed by multi-head self-attention boost the mod-\neling capacity. However, deterministic Seq2Seq\nand Transformer models tends to generate generic\nresponses which leads to a low diversity score.\nMeanwhile incorporating a stochastic latent vari-\nable into both models (CV AE and GVT) promote\nmore diverse generation results and boost the diver-\nsity scores such as Dist-1, Dist-2, and Dist-3.\nCompare to baseline models, the GVT achieves\nrelatively lower reconstruction PPL, which sug-\ngests that the global latent variable contains rich\nlatent information (e.g., topic) for response gener-\nation. Meanwhile, the sequential latent variables\nof the SVT encode ﬁne-grained latent information\nand further improve the reconstruction PPL.\nOn the other hand, SVT achieves the highest\nscore in terms of two semantic relevance-oriented\nmetrics such as EMBFT and EMBBERT in Mo-\njiTalk dataset, while in the combined dataset of\nPersona and ED, we observe performance drop of\nSVT compare to other models. This is because\nboth Persona and ED are well designed and have\nlower entropy than MojiTalk which collected from\nTwitter. We hypothesize that the sequential latent\nvariables have no advantage in term of similarity\nto single, ﬁxed ”gold response” when model low\nentropy response. Indeed, in open domain dia-\nlogue response generation, automatic metric is not\nalways aligned with the human judgement (Liu\net al., 2016). In contrast, human evaluation result\nreported in Table 1 demonstrates the generations of\nSVT are closer to the human standard in terms of\ncoherence, invoked emotion and engagedness.\n6.2 Qualitative Analysis\nTable 2 compares the generation of the proposed\nmodels with baselines given the same contexts. We\nobserve that the Seq2Seq and vanilla transformer\ntend to generate generic and repetitive responses\nContext trade must ’ve made you mad ?\nEmotion\nResponses\nSeq2Seq:i ’m not sure if i ’m not sure if i ’m not sure if i ’m not sure about it\nCV AE:<unk>but i don ’t think it ’s been on\nTransformer:i ’m not sure i ’m not\nGVT:i ’ll pass it on , she ’s mad\nSVT:hell yeah bro . yeah\nRef:i don ’t wanna talk about it\nContext love the smell of a good bbq !\nEmotion\nResponses\nSeq2Seq:i love it ! i love it !\nCV AE:aw you ’re getting better\nTransformer:i ’m glad you like it !\nGVT:i ’d like to know you ’re very well .\nSVT:omg what kind of smell ? thanks for sharing it with a pizza in<unk>\nRef:hahaha that sounds like friday to me pc\nContext good evening , how are you tonight ?\nResponses\nCV AE:hi , i am doing well . you ?\nTransformer:i am doing well , how are you ?\nGVT:i am good . just studying some people there .\nSVT:: i am doing well , just ﬁnished eating some ice cream .\nRef:i am doing well . i am relaxing before bed . i work in the morning .\nContext i cook mine at home while watching one tree hill . love that show .\nResponses\nCV AE:i love to cook . i like to cook .\nTransformer:i love the outdoors . i love the outdoors .\nGVT:it is good . . . you can make some money .\nSVT:do you have any pets ? i have never watched any of tv.\nRef:i am looking for a new job . i hate sitting still all day\nUser: well do you work ? do you have a degree to sustain yourself ?\nContext System: i built models when i was a kid . now i sculpt and mold and carve.\nUser: nice , i specialize in computer science degree so i mostly mold 3d images.\nResponses\nCV AE:i do not like it when you get to do the same\nTransformer:i am a teacher . i am a teacher . i am a teacher .\nGVT:me too ! my favorite is the best baker .\nSVT:i love the technology . i like to play when i get older\nRef:i am looking for a new job . i hate sitting still all day\nTable 2: Generated responses from proposed models and baseline models. The reference responses (Ref) are given.\n(e.g., i am not sure) in MojiTalk due to their de-\nterministic structure fail to capture the variability\nin dialogue response. By incorporating stochastic\nlatent variables, the CV AE and GVT can gener-\nate more diverse responses, but their responses are\nsometimes digressive (e.g., example 5). Interest-\ningly, GVT and SVT generalize the topic beyong\nthe context which make the dialogue more engag-\ning (e.g., example 4). In general, SVT is able to\ngenerate more coherent and informative responses.\n7 Conclusion\nThis paper introduces the Variational Transformer\n(VT), a variational self-attentive feed-forward se-\nquence model that combines the global receptive\nﬁeld of a Transformer with the variational nature\nof a CV AE. We propose two types of the VT: 1)\nthe Global Variational Transformer (GVT) which\nincorporates a global latent variable as additional\ninput to the transformer decoder; and 2) the Sequen-\ntial Variational Transformer (SVT) which generates\nlatent variables for each position during decoding\nprocess. Quantitative and qualitative experimental\nresults shows that our models outperform baselines\nin terms of diversity, semantic relevance, and hu-\nman judgment. In future work, we will utilize the\npre-training language models (Radford et al., 2019)\nas the back-bone to strengthen the language model\nof the VT for better generation.\nReferences\nSamuel R Bowman, Luke Vilnis, Oriol Vinyals, An-\ndrew Dai, Rafal Jozefowicz, and Samy Bengio.\n2016. Generating sentences from a continuous\nspace. In Proceedings of The 20th SIGNLL Confer-\nence on Computational Natural Language Learning.\nJunyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth\nGoel, Aaron C Courville, and Yoshua Bengio. 2015.\nA recurrent latent variable model for sequential data.\nIn Advances in neural information processing sys-\ntems, pages 2980–2988.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Łukasz Kaiser. 2018. Univer-\nsal transformers. arXiv preprint arXiv:1807.03819.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nJiachen Du, Wenjie Li, Yulan He, Ruifeng Xu, Lidong\nBing, and Xuan Wang. 2018. Variational autoregres-\nsive decoder for neural response generation. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing , pages 3154–\n3163.\nAnirudh Goyal Alias Parth Goyal, Alessandro Sor-\ndoni, Marc-Alexandre Cˆot´e, Nan Rosemary Ke, and\nYoshua Bengio. 2017. Z-forcing: Training stochas-\ntic recurrent networks. In Advances in neural infor-\nmation processing systems, pages 6713–6723.\nLukasz Kaiser, Samy Bengio, Aurko Roy, Ashish\nVaswani, Niki Parmar, Jakob Uszkoreit, and Noam\nShazeer. 2018. Fast decoding in sequence models\nusing discrete latent variables. In International Con-\nference on Machine Learning, pages 2395–2404.\nLukasz Kaiser, Aidan N Gomez, Noam Shazeer,\nAshish Vaswani, Niki Parmar, Llion Jones, and\nJakob Uszkoreit. 2017. One model to learn them\nall. arXiv preprint arXiv:1706.05137.\nDiederik P Kingma and Max Welling. 2013. Auto-\nencoding variational bayes. arXiv preprint\narXiv:1312.6114.\nHung Le, Truyen Tran, Thin Nguyen, and Svetha\nVenkatesh. 2018. Variational memory encoder-\ndecoder. In Advances in Neural Information Pro-\ncessing Systems, pages 1508–1518.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016a. A diversity-promoting ob-\njective function for neural conversation models. In\nProceedings of NAACL-HLT, pages 110–119.\nJiwei Li, Michel Galley, Chris Brockett, Georgios Sp-\nithourakis, Jianfeng Gao, and Bill Dolan. 2016b. A\npersona-based neural conversation model. In Pro-\nceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 994–1003.\nPiji Li, Wai Lam, Lidong Bing, and Zihao Wang. 2017.\nDeep recurrent generative decoder for abstractive\ntext summarization. In Proceedings of the 2017 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 2091–2100.\nZhaojiang Lin, Andrea Madotto, Jamin Shin, Peng Xu,\nand Pascale Fung. 2019. Moel: Mixture of empa-\nthetic listeners. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 121–132.\nChia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose-\nworthy, Laurent Charlin, and Joelle Pineau. 2016.\nHow not to evaluate your dialogue system: An em-\npirical study of unsupervised evaluation metrics for\ndialogue response generation. In Proceedings of the\n2016 Conference on Empirical Methods in Natural\nLanguage Processing, pages 2122–2132.\nTomas Mikolov, Edouard Grave, Piotr Bojanowski,\nChristian Puhrsch, and Armand Joulin. 2018. Ad-\nvances in pre-training distributed word representa-\ntions. In Proceedings of the International Confer-\nence on Language Resources and Evaluation (LREC\n2018).\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin\nTran. 2018. Image transformer. In International\nConference on Machine Learning , pages 4052–\n4061.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8).\nHannah Rashkin, Eric Michael Smith, Margaret Li, and\nY-Lan Boureau. 2019. Towards empathetic open-\ndomain conversation models: A new benchmark and\ndataset. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5370–5381.\nIulian Vlad Serban, Ryan Lowe, Laurent Charlin, and\nJoelle Pineau. 2016. Generative deep neural net-\nworks for dialogue: A short review. arXiv preprint\narXiv:1611.06216.\nIulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,\nLaurent Charlin, Joelle Pineau, Aaron Courville, and\nYoshua Bengio. 2017. A hierarchical latent variable\nencoder-decoder model for generating dialogues. In\nThirty-First AAAI Conference on Artiﬁcial Intelli-\ngence.\nKihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015.\nLearning structured output representation using\ndeep conditional generative models. In Advances in\nneural information processing systems, pages 3483–\n3491.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nOriol Vinyals and Quoc Le. 2015. A neural conversa-\ntional model. arXiv preprint arXiv:1506.05869.\nRichard S Wallace. 2009. The anatomy of alice. In\nParsing the Turing Test, pages 181–210. Springer.\nJoseph Weizenbaum et al. 1966. Eliza—a computer\nprogram for the study of natural language communi-\ncation between man and machine. Communications\nof the ACM, 9(1):36–45.\nBiao Zhang, Deyi Xiong, Hong Duan, Min Zhang,\net al. 2016. Variational neural machine translation.\nIn Proceedings of the 2016 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n521–530.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing dialogue agents: I have a dog, do you\nhave pets too? In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 2204–\n2213.\nTiancheng Zhao, Kyusong Lee, and Maxine Eskenazi.\n2018. Unsupervised discrete sentence representa-\ntion learning for interpretable neural dialog genera-\ntion. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1098–1107.\nTiancheng Zhao, Ran Zhao, and Maxine Eskenazi.\n2017. Learning discourse-level diversity for neural\ndialog models using conditional variational autoen-\ncoders. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 654–664.\nXianda Zhou and William Yang Wang. 2018. Mojitalk:\nGenerating emotional responses at scale. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 1128–1137.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6475710272789001
    },
    {
      "name": "Computer science",
      "score": 0.3553813397884369
    },
    {
      "name": "Electrical engineering",
      "score": 0.2887345552444458
    },
    {
      "name": "Engineering",
      "score": 0.26913201808929443
    },
    {
      "name": "Voltage",
      "score": 0.07702898979187012
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    }
  ],
  "cited_by": 46
}