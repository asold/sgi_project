{
  "title": "Unsupervised MRI Reconstruction via Zero-Shot Learned Adversarial Transformers",
  "url": "https://openalex.org/W3161035636",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5063212929",
      "name": "Yılmaz Korkmaz",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5088175232",
      "name": "Salman Ul Hassan Dar",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5076735410",
      "name": "Mahmut Yurt",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5071482097",
      "name": "Muzaffer Özbey",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5032375235",
      "name": "Tolga Çukur",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4249760698",
    "https://openalex.org/W2111388536",
    "https://openalex.org/W2101675075",
    "https://openalex.org/W2167396304",
    "https://openalex.org/W3000314771",
    "https://openalex.org/W2442117232",
    "https://openalex.org/W2902719825",
    "https://openalex.org/W2604388535",
    "https://openalex.org/W2883105305",
    "https://openalex.org/W3028318139",
    "https://openalex.org/W2773850766",
    "https://openalex.org/W2791621240",
    "https://openalex.org/W3092530614",
    "https://openalex.org/W2922528425",
    "https://openalex.org/W2804263814",
    "https://openalex.org/W2996927594",
    "https://openalex.org/W2611467245",
    "https://openalex.org/W3080851077",
    "https://openalex.org/W3055270720",
    "https://openalex.org/W3146487392",
    "https://openalex.org/W3094286386",
    "https://openalex.org/W6782646009",
    "https://openalex.org/W3039236647",
    "https://openalex.org/W2995286437",
    "https://openalex.org/W6792228420",
    "https://openalex.org/W6768505353",
    "https://openalex.org/W3165244505",
    "https://openalex.org/W3188455524",
    "https://openalex.org/W2972862926",
    "https://openalex.org/W2785239769",
    "https://openalex.org/W2969785455",
    "https://openalex.org/W6752378368",
    "https://openalex.org/W6790275670",
    "https://openalex.org/W6797708045",
    "https://openalex.org/W2753305843",
    "https://openalex.org/W2791695392",
    "https://openalex.org/W2777802649",
    "https://openalex.org/W2964293140",
    "https://openalex.org/W2808243856",
    "https://openalex.org/W2757509933",
    "https://openalex.org/W6729455701",
    "https://openalex.org/W2795380527",
    "https://openalex.org/W2979703761",
    "https://openalex.org/W2738743584",
    "https://openalex.org/W2972061446",
    "https://openalex.org/W2962734274",
    "https://openalex.org/W3041018972",
    "https://openalex.org/W3132291493",
    "https://openalex.org/W2778924750",
    "https://openalex.org/W6746155205",
    "https://openalex.org/W3035596626",
    "https://openalex.org/W3009994569",
    "https://openalex.org/W3035014113",
    "https://openalex.org/W2964308363",
    "https://openalex.org/W6791751007",
    "https://openalex.org/W3201909904",
    "https://openalex.org/W2980318461",
    "https://openalex.org/W2889995282",
    "https://openalex.org/W3007222983",
    "https://openalex.org/W2936194729",
    "https://openalex.org/W3024378640",
    "https://openalex.org/W6790599505",
    "https://openalex.org/W2962903101",
    "https://openalex.org/W3012906128",
    "https://openalex.org/W2964013315",
    "https://openalex.org/W3035574324",
    "https://openalex.org/W6791447439",
    "https://openalex.org/W6798016242",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2603777577",
    "https://openalex.org/W6637568146",
    "https://openalex.org/W6761628794",
    "https://openalex.org/W4212863985",
    "https://openalex.org/W6749927861",
    "https://openalex.org/W2963499153",
    "https://openalex.org/W2117649283",
    "https://openalex.org/W4235964766",
    "https://openalex.org/W1758598986",
    "https://openalex.org/W3003650457",
    "https://openalex.org/W2003271614",
    "https://openalex.org/W2928045706",
    "https://openalex.org/W3035647610",
    "https://openalex.org/W6750469568",
    "https://openalex.org/W3108244925",
    "https://openalex.org/W6727249380",
    "https://openalex.org/W3129082682",
    "https://openalex.org/W3137849220",
    "https://openalex.org/W6776463122",
    "https://openalex.org/W6763171703",
    "https://openalex.org/W2782599016",
    "https://openalex.org/W6805770662",
    "https://openalex.org/W4210422447",
    "https://openalex.org/W2954098871",
    "https://openalex.org/W3082394871",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W3102061158",
    "https://openalex.org/W3102253456",
    "https://openalex.org/W2047544187",
    "https://openalex.org/W2552808051",
    "https://openalex.org/W3001319253",
    "https://openalex.org/W3137440837",
    "https://openalex.org/W3100730608",
    "https://openalex.org/W3103586216",
    "https://openalex.org/W2798122215",
    "https://openalex.org/W2985068832",
    "https://openalex.org/W2751040220",
    "https://openalex.org/W3127233062",
    "https://openalex.org/W3163230978",
    "https://openalex.org/W3105403262",
    "https://openalex.org/W2950936580",
    "https://openalex.org/W3120984979",
    "https://openalex.org/W2804078698",
    "https://openalex.org/W2962770929",
    "https://openalex.org/W2770205545",
    "https://openalex.org/W2977723824",
    "https://openalex.org/W3135404760"
  ],
  "abstract": "Supervised reconstruction models are characteristically trained on matched pairs of undersampled and fully-sampled data to capture an MRI prior, along with supervision regarding the imaging operator to enforce data consistency. To reduce supervision requirements, the recent deep image prior framework instead conjoins untrained MRI priors with the imaging operator during inference. Yet, canonical convolutional architectures are suboptimal in capturing long-range relationships, and priors based on randomly initialized networks may yield suboptimal performance. To address these limitations, here we introduce a novel unsupervised MRI reconstruction method based on zero-Shot Learned Adversarial TransformERs (SLATER). SLATER embodies a deep adversarial network with cross-attention transformers to map noise and latent variables onto coil-combined MR images. During pre-training, this unconditional network learns a high-quality MRI prior in an unsupervised generative modeling task. During inference, a zero-shot reconstruction is then performed by incorporating the imaging operator and optimizing the prior to maximize consistency to undersampled data. Comprehensive experiments on brain MRI datasets clearly demonstrate the superior performance of SLATER against state-of-the-art unsupervised methods.",
  "full_text": "1\nUnsupervised MRI Reconstruction via Zero-Shot\nLearned Adversarial Transformers\nYilmaz Korkmaz, Salman UH Dar, Mahmut Yurt, Muzaffer¨Ozbey, and Tolga C ¸ ukur\nAbstract— Supervised reconstruction models are char-\nacteristically trained on matched pairs of undersampled\nand fully-sampled data to capture an MRI prior, along\nwith supervision regarding the imaging operator to enforce\ndata consistency. To reduce supervision requirements, the\nrecent deep image prior framework instead conjoins un-\ntrained MRI priors with the imaging operator during infer-\nence. Yet, canonical convolutional architectures are sub-\noptimal in capturing long-range relationships, and priors\nbased on randomly initialized networks may yield sub-\noptimal performance. To address these limitations, here\nwe introduce a novel unsupervised MRI reconstruction\nmethod based on zero-Shot Learned Adversarial Trans-\nformERs (SLATER). SLATER embodies a deep adversarial\nnetwork with cross-attention transformers to map noise\nand latent variables onto coil-combined MR images. Dur-\ning pre-training, this unconditional network learns a high-\nquality MRI prior in an unsupervised generative modeling\ntask. During inference, a zero-shot reconstruction is then\nperformed by incorporating the imaging operator and opti-\nmizing the prior to maximize consistency to undersampled\ndata. Comprehensive experiments on brain MRI datasets\nclearly demonstrate the superior performance of SLATER\nagainst state-of-the-art unsupervised methods.\nIndex Terms— adversarial, transformers, MRI, unsuper-\nvised, reconstruction, zero shot, generative.\nI. INTRODUCTION\nM\nAGNETIC Resonance Imaging (MRI) is a clinical\npowerhouse due to its excellent soft tissue contrast.\nYet, low spin polarization at mainstream ﬁeld strengths limits\nsignal-to-noise ratio and necessitates prolonged exams. Ex-\ntensive MRI exams incur added economic costs and lower\nuse efﬁciency, and they may not be possible in uncooperative\npatient populations. A dire consequence is administration of\nexams that are in lower quality and/or diversity, which projects\npoorly onto diagnostic utility. Accelerated MRI addresses this\nfundamental limitation by performing undersampled acqui-\nsitions, and then solving an inverse problem to reconstruct\nimages from available k-space data [1]–[4].\nGiven their exceptional performance in inverse problems,\nsupervised neural networks have been quickly adopted for\nMRI reconstruction [5]. Reconstruction refers to the task\nof mapping undersampled acquisitions to images that are\nas consistent as possible with corresponding fully-sampled\nacquisitions. Supervised models performing this conditional\nmapping are trained on matched pairs of undersampled and\nfully-sampled data. The training process involves multiple\nThis study was supported in part by a TUBA GEBIP 2015 fellowship,\nand a BAGEP 2017 fellowship (Corresponding author: Tolga C ¸ ukur).\nY . Korkmaz, S. UH. Dar, M. Yurt, M.¨Ozbey , and T. C ¸ ukur are with\nthe Department of Electrical and Electronics Engineering, Bilkent Uni-\nversity, Ankara, Turkey (e-mails:{korkmaz, salman, mahmut, muzaffer,\ncukur}@ee.bilkent.edu.tr). T. C ¸ ukur is also with the National Magnetic\nResonance Research Center, Bilkent University, TR-06800 Ankara,\nTurkey.\nlines of supervision including pairing of input-output data to\nlearn an indirect prior that reduces aliasing artifacts in MR\nimages [6]–[9], and enforcement of data consistency to embed\nthe imaging operator that reﬂects k-space undersampling and\ncoil-sensitivity encoding [10]–[14]. Accordingly, supervised\nmodels are typically retrained for notable changes in the\ndata distribution (e.g, different MRI contrast) or the imaging\noperator (e.g., different k-space sampling density) [15]–[17].\nSeveral important approaches have been proposed in liter-\nature to reduce supervision requirements. A group of studies\nhave reduced explicit supervision related to raw data, propos-\ning models trained using unpaired sets of input-output data\n[18]–[20], or using only undersampled data [21]–[27]. That\nsaid, these models involve implicit supervision regarding the\nimaging operator. As such, they are trained for a speciﬁc\ncoil-array conﬁguration and k-space sampling density, factors\nassumed to be consistent across the training and test sets [21]–\n[24]. To remove other supervision aspects, a second group of\nstudies have built unsupervised models by decoupling the MRI\nprior from the imaging operator. These models capture an MRI\nprior via generative networks that are either untrained [28]–\n[31] or trained to synthesize fully-sampled MR images [32]–\n[34]. The imaging operator is then conjoined with the MRI\nprior during inference on test data. This unsupervised approach\nexcludes paired datasets for training, and promises improved\ngeneralization against deviations in the imaging operator [33],\n[34]. Yet, previously proposed models are commonly based on\nconvolutional neural networks (CNNs) that suffer from limited\nsensitivity in capturing long-range dependencies [35]–[37].\nHere, we introduce a novel unsupervised MRI recon-\nstruction based on zero-shot learned adversarial transformers\n(SLATER). SLATER decomposes the reconstruction process\nto decouple learning of the MRI prior from embedding of\nthe imaging operator1. During a pre-training phase, an uncon-\nditional adversarial model is used to synthesize high-quality,\ncoil-combined MR images (Fig. 1). To improve capture of\nlong-range spatial context without introducing computational\nburden, we propose an architecture comprising cross-attention\ntransformer blocks between low-dimensional latent variables\nand high-dimensional image features. During the inference\nphase, the learned MRI prior is combined with the imaging\noperator. This is achieved by optimizing network parameters\nthat reﬂect the MRI prior to enforce a data-consistency loss\non undersampled test data (Fig. 2). To improve inference\nefﬁciency, a weight propagation strategy is proposed where the\noptimized network weights are transferred across consecutive\ncross-sections.\nThe proposed method performs an unsupervised generative\nmodeling task on coil-combined images derived from fully-\n1see [38] for a preliminary version of this work presented at ISMRM 2021\narXiv:2105.08059v3  [eess.IV]  16 Jan 2022\n2\nFig. 1: SLATER is based on an unconditional adversarial architecture with a synthesizer (G), a discriminator (D), and a mapper\n(M). G is a multi-layer architecture where image resolution is progressively increased across layers. Within intermediate layers,\na convolutional upsampling block is followed by a cross-attention transformer block (see 1). The transformer block receives\nglobal and local latent variables, noise variables and positional encoding (P.E.) for latents and image features. D enables\nadversarial learning while it receives as input synthetic and actual MR images. It is composed of convolutional and fully-\nconnected (FC) blocks (see 2, 3). M projects raw latent variables (Z) onto k local (w 1,...,wK) variables and one global (w g)\nlatent variable. It is composed of self-attention and FC blocks (see 3, 4, and Supp. Fig. 1 for details). An unsupervised\ngenerative modeling task is performed using the SLATER model to capture high-quality MRI priors.\nsampled MRI acquisitions. Adapting the generative model to\nthe reconstruction task without any training samples, a zero-\nshot reconstruction then maps undersampled data to high-\nquality MR images during inference. The decoupled recon-\nstruction process and model adaptation during inference con-\ntribute to improved generalization performance for SLATER.\nThe source code for SLATER can be found at: https:\n//github.com/icon-lab/SLATER.\nContributions:\n• For the ﬁrst time in literature, we introduce an adversarial\nvision transformer model for MRI reconstruction.\n• Our proposed model uses cross-attention transformers to\ncapture long-range spatial dependencies without compu-\ntational burden.\n• Sample-speciﬁc model adaptation and cross-sectional\nweight propagation strategies are introduced that respec-\ntively enhance out-of-domain generalizability and infer-\nence efﬁciency.\nII. R ELATED WORK\nSupervised reconstruction models are current state-of-the-\nart in MRI with numerous successful architectures reported\nincluding basic CNNs [6], [39]–[41], residual CNNs [42], [43],\nperceptrons [17], [44], physics-guided unrolled CNN networks\n[7], [10], [11], [14], [45]–[49], recurrent CNNs [9], [50],\n[51], generative adversarial networks (GANs) [52]–[55], and\nvariational networks [8], [13], [15], [56]. However, supervised\nmodels are trained on paired sets of undersampled and fully-\nsampled data, along with supervision regarding the imaging\noperator in the form of a data-consistency term. Compiling\nlarge sets of paired data is non-trivial [57], and supervised\nmodels often require retraining to cope with deviations in the\nimaging operator [17].\nTo improve utility of deep MRI reconstruction, a common\nstrategy has been to target explicit supervision on raw data\nin order to lower reliance on large, paired training datasets.\nDomain-transferred models are trained in a data-abundant\nsource domain and then adopted for reconstruction in the\ntarget domain [16], [58]. Residual models are trained to\npredict residual error between ground truth and the output of a\nconventional MRI reconstruction [59], [60]. Both approaches\npermit training with relatively few samples, but training often\nrequires paired datasets. Unpaired models are instead trained\non input-output data collected from separate groups of sub-\nUNSUPERVISED MRI RECONSTRUCTION VIA ADVERSARIAL TRANSFORMERS 3\nFig. 2: (a) The generative model in SLATER is adapted to perform zero-shot reconstruction for accelerated MRI. To do\nthis, the generator output is back-projected onto individual coils (C), and masked with the same sampling pattern as in the\nundersampled acquisition ( MΩ). (b) The network prior consisting of noise (n), latent variables (W) and weights ( θG) is then\noptimized to maximize consistency between the reconstructed and acquired k-space data.\njects. This can be achieved with cycle-consistent models that\nlearn bidirectional mappings between undersampled and fully-\nsampled data in order to enforce self-consistency of network\ninputs or outputs [12], [18], [61]. However, unpaired models\ncan require substantially larger datasets for training compared\nto paired models [20]. Lastly, several prominent approaches\nwere proposed to train unsupervised models in the absence of\nground truth. Scan-speciﬁc models learn nonlinear interpola-\ntion kernels from an auto-calibration region in undersampled\nacquisitions, and then recover missing k-space samples via\ninterpolation during inference [62]–[64]. These models do not\nrequire a priori training, but their performance relies on the as-\nsumption that local dependencies between samples are largely\ninvariant across k-space. Self-supervised models are trained\nvia proxy loss terms substituted for the true reconstruction loss\n[21]. Speciﬁcally, data-consistency loss calculated on samples\navailable in undersampled acquisitions can serve as a proxy\n[22], [23]. An alternative is to leverage statistical estimators\nfor mean-squared reconstruction loss [27]. Such estimators\nare often analytically derived, so their use might be limited\nto speciﬁc types of loss functions. Recent studies have also\nproposed to mask a subset of k-space samples in undersampled\nacquisitions, where the masked subset is used to deﬁne a\ndata-consistency loss [24], [25]. Based on this loss, self-\nsupervised models are trained to recover masked-out samples\nfrom remaining samples. A related image-domain strategy has\nbeen proposed for multi-image MRI data [26], [65]. In this\ncase, models can be trained to predict a masked subset of\nundersampled frames in dynamic MRI from remaining frames\n[66], or to map between differently undersampled versions of a\ngiven fully-sampled acquisition [67]. Because the indirect MRI\nprior to reduce aliasing artifacts is learned from undersampled\ndata, self-supervised models might show suboptimal perfor-\nmance at high acceleration factors. Moreover, unsupervised\nmethods that used ﬁxed model weights during inference might\nsuffer from suboptimal generalization to test data [68], [69].\nA collective attribute of the aforementioned methods to limit\nsupervision on raw data is that they are based on conditional\nmodels that map undersampled acquisitions to MR images.\nThus, they involve inherent supervision regarding the imaging\noperator, and they are typically retrained for varying coil-array\nconﬁgurations and sampling densities [24]. Removing instead\nsupervision related to the imaging operator, a fundamentally\ndifferent approach decomposes the reconstruction process to\ndecouple the MRI prior from the imaging operator. The deep\nimage prior (DIP) approach employs an unconditional model\nthat maps relatively low-dimensional latent variables onto\nimages as a native MRI prior [28]. The imaging operator\nis embedded during inference, and the prior is adapted to\nminimize a data-consistency loss on undersampled test data.\nThis decoupled approach introduces ﬂexibility in employing\nthe same prior for various different imaging operators. Yet,\nDIP methods pervasively use untrained CNN architectures\nwith randomly initialized parameters [28]–[31], [70]. In turn,\nuntrained priors might be suboptimal in capturing the dis-\ntribution of MR images [32], [33], and CNN models can\ngenerally suffer from limited sensitivity toward long-range\nspatial interactions [35]–[37].\nIII. T HEORY\nInspired by the DIP framework, here we introduce a re-\nconstruction method for accelerated MRI based on a deep\ngenerative model that maps noise and latent variables onto\nMR images. Unlike previous methods, this mapping is based\non a style-generative adversarial network with transformer\nblocks. The network learns an MRI prior in pre-training phase,\nfollowed by a zero-shot inference phase where it is adapted to\nreconstruct undersampled acquisitions. We ﬁrst overview the\ninverse problem formulation in accelerated MRI and DIP. We\nthen describe the fundamental building blocks of SLATER.\n4\nA. Accelerated MRI Reconstruction\nIn accelerated MRI, undersampled k-space acquisitions are\nperformed to speed up scans, typically with variable-density\nrandom sampling patterns:\nFpCm = ys (1)\nFp is the partial Fourier operator deﬁned by the sampling\npattern, C denotes coil sensitivities, m is the target MR\nimage and ys are the acquired multi-coil k-space samples.\nThe target m must be computed given the available data ys.\nHowever, the linear system in Eq.1 is underdetermined, so\nMRI reconstruction is an ill-posed inverse problem. To obtain\nhigh-quality reconstruction, the solution has to be regularized\nwith additional prior information on MR images:\nˆm= argmin\nm\n∥ys −FpCm∥2\n2 + H(m) (2)\nwhere ˆmis the reconstructed image, and H(m) is the regular-\nization term. The regularization term can be designed based on\nsparsity priors [3], [4], structured low-rank priors [71], [72],\nor network priors [6].\nB. Deep Image Prior\nThe DIP framework has recently been introduced for unsu-\npervised learning in computer vision tasks, including super res-\nolution, inpainting and denoising [73]. DIP observes that local\nﬁltering operations in CNNs constrain the set of images that\ncan be generated, so untrained CNNs can serve as native image\nregularizers. DIP performs random initialization of network\ninputs and weights without any pretraining. During inference,\nnetwork parameters are optimized by enforcing consistency\nwith the available corrupted image. Thus, DIP projects the\ncorrupted image onto the space of CNN-generatable images\nto ﬁlter out corruptions such as blur or noise. DIP can be\nadopted for MRI reconstruction as follows:\nθ∗ = argmin\nθ\n∥FpCdθ(z) −ys∥1 (3)\nwhere θ are network weights, θ∗ are optimized network\nweights, z are latent variables, dθ(z) is the network mapping\nfrom latents onto the reconstructed image [14], [28]. Both\nnetwork weights and latents are randomly initialized, and the\noptimization in Eq.3 is performed over θ, while z is ﬁxed. In\ncontrast to mainstream learning-based methods, DIP inverts\na random network prior to identify weights that are most\nconsistent with the corrupted image. The reconstruction can\nthen be expressed as:\nˆm= dθ∗(z) (4)\nDespite its prowess in MRI reconstruction, models with\nrandomly initialized parameters setup a generic image prior\nthat may not be as strongly-tuned towards the distribution\nof MR data as trained models [32]. Furthermore, DIP is\ntraditionally based on CNN architectures, where local kernels\nintroduce suboptimal sensitivity in capture of long-range spa-\ntial interactions [35].\nC. Adversarial Transformer Model\nSLATER is based on an unconditional adversarial network\nthat receives noise and latent variables to generate MR images\n[38]. Here we adopt a style-generative architecture given the\nsuccess of this model family in computer vision tasks [74]. We\nfurther propose to build the network layers with cross-attention\ntransformer blocks as inspired by a recent study on natural\nimage synthesis [75]. Self-attention transformers that compute\ninteractions among all image pixels are prohibitive at relatively\nhigh spatial resolutions [36], [76], [77]. Instead, cross-attention\ntransformers enable efﬁcient capture of long-range context\nbased on attentional interactions between low-dimensional\nlatent variables and high-dimensional image features. Our\nmodel contains three sub-networks (Fig.1): a synthesizer that\ngenerates MR images; a mapper that prepares the set of\nlatent variables input to the synthesizer; and a discriminator.\nThe synthesizer aims to generate realistic images, while the\ndiscriminator aims to distinguish actual images from synthe-\nsized images. Thus, the discriminator aids the synthesizer in\ncapturing an MRI prior that reﬂects the distribution of high-\nquality MR images.\n1) Synthesizer (G): The synthesizer contains a total of NL\nlayers, each comprising a convolutional upsampling block to\nprogressively increase image resolution followed by a cross-\nattention transformer block. Prior style-generative models typ-\nically use a global latent variable at each layer to control\nhigh-level image features related to style [74]. In SLATER,\nin addition to a global latent ( wg ∈RLs, Ls: dimensionality),\nthe synthesizer receives as input K local ( Wl ∈RK×Ls) latent\nvariables at each layer. The global latent is still used to per-\nform a spatially-uniform afﬁne transformation of convolutional\nfeature maps, and so it modulates high-level image features.\nMeanwhile, local latents are used to perform spatially-selective\nmodulation of feature maps via cross-attention mechanisms.\nEach local latent variable focuses on a learned group of\nfeature-map locations, so local latents serve to modulate rela-\ntively lower-level image features. For enabling cross-attention\nmechanism, a sinusoidal position encoding is used for spa-\ntial feature maps, whereas a learnable position encoding is\nassumed for local latent variables as their positions are un-\nknown prior to training. Each cross-attention transformer block\ncontains a serial cascade of the following sub-blocks: (cross-\nattention (CA1), noise injection ( NI1), style-modulated con-\nvolution (SC), cross-attention ( CA2), noise injection ( NI2)).\nThese sub-blocks are detailed below.\na) Cross-Attention (CA1): CA derives contextual repre-\nsentations by mediating attentional interaction between Wl\nand input feature maps X0\ni ∈ Rh1×h2×u, where h1 and\nh2 denote height and width depending on the resolution at\nthe ith layer, and u is the number of feature channels. Let\nX0\ni,vec ∈R(h1×h2)×u be the vectorized form of X0\ni along the\nspatial dimensions. Attention maps ˜A1\ni,maps ∈ R(h1×h2)×K\nthat characterize the relation between Wl and X0\ni,vec are as\nfollows (see Fig. 3 and Supp. Fig. 2 for sample maps):\n˜A1\ni,maps = smax\n\n˜qi\n1(X0\ni,vec + PE1\ni,X) ˜ki\n1\n(Wl + PEWl)T\n√u\n\n\n(5)\nUNSUPERVISED MRI RECONSTRUCTION VIA ADVERSARIAL TRANSFORMERS 5\nwhere smaxis the softmax function, ˜qi\n1(.) ∈R(h1×h2)×u are\nqueries that receive X0\ni,vec added with layer-speciﬁc position\nencoding variables PE1\ni,X ∈ R(h1×h2)×u and perform a\nlearnable linear projection, ˜ki\n1\n(.) ∈ RK×u are keys that\nreceive Wl added with position encoding variables PEWl ∈\nRK×Ls and perform a learnable linear projection. PE1\ni,X are\ntaken as learnable linear projections of pre-deﬁned sinusoidal\nencoding variables (see Supp. Text I-A), while PEWl are\nrandomly initialized learnable position encoding variables.\nNote that PE1\ni,X and A1\ni,maps are vectorized along the spatial\ndimensions. Attention feature maps ( ˜A1\ni,feat ∈R(h1×h2)×u)\nare then obtained by multiplying A1\ni,maps with values that are\nlearnable linear projections ( ˜vi\n1(.) ∈RK×u).\n˜A1\ni,feat = ˜A1\ni,maps ˜vi\n1(Wl + PEWl) (6)\nLastly, input feature maps are modulated via a single atten-\ntion head that scales and shifts X0\ni,vec with learnable linear\nprojections of ˜A1\ni,feat to output X1\ni,vec ∈R(h1×h2)×u.\nX1\ni,vec = ˜γ1( ˜A1\ni,feat) ⊙\n(\nX0\ni,vec −µ(X0\ni,vec)\nσ(X0\ni,vec)\n)\n+ ˜b1( ˜A1\ni,feat)\n(7)\nwhere ˜γ1(.) ∈ R(h1×h2)×u and ˜b1(.) ∈ R(h1×h2)×u are\nlearnable linear projections, µ denotes mean, σ denotes vari-\nance, and ⊙is the Hadamard product. The mapping through\nCA1 can thus be summarized as: X1\ni = CA1(X0\ni), where\nX1\ni ∈Rh1×h2×u is the matrix form of X1\ni,vec.\nb) Noise Injection (NI1): To improve control over vari-\nability in ﬁne details of feature maps, noise variables are\ninjected onto modulated feature maps from CA1. Given input\nX1\ni ∈Rh1×h2×u to NI1, the output ( X2\ni ∈Rh1×h2×u) can\nbe expressed as:\nX2\ni =\n\n\nX1,1\ni + α1\nin1\ni\n...\nX1,u\ni + α1\nin1\ni\n\n (8)\nwhere X1,e\ni ∈ Rh1×h2 denotes eth channel of X1\ni, n1\ni ∈\nRh1×h2 is noise added to each eth channel and α1\ni is a\nlearnable scalar. Note that the learnable noise variables in n1\ni\nare initiated via random sampling from a standard normal\ndistribution for each spatial location. During the course of\nlearning, mean and standard deviation of noise variables are\nnormalized to (0, 1) across the spatial dimensions. The ﬁnal\nmapping through NI1 is expressed as: X2\ni = NI1(X1\ni).\nc) Style Modulated Convolution (SC): Transformer blocks\ncharacteristically contain a feed-forward neural network\n(FFNN) sub-block following the attention sub-block to extract\nhidden features of attention-based contextual representations.\nFor computational efﬁciency, here a convolutional FFNN\nis utilized to locally reﬁne contextual representations while\nmodulating feature maps to control high-level style features\n[74]. Analogous to adaptive instance normalization (AdaIN)\nin style-transfer models, modulation is achieved via an afﬁne\ntransformation that controls the scale of feature maps [78].\nYet, we opted for a more compact implementation based on\nstyle-modulated convolution, with comparable complexity to\nconvolution augmented with AdaIN and only requiring a trivial\nscaling operation over basic convolution [74]. Accordingly,\nfeature maps are scaled by modulating convolution kernels in\nSC, where kernel weights are multiplied via a learnable linear\nprojection wi,s ∈Ru of the global latent variable wg:\nθ\n′\n=\n\n\nw1\ni,sθ1,1\ni w1\ni,sθ1,2\ni ... w 1\ni,sθ1,u\ni\nw2\ni,sθ2,1\ni w2\ni,sθ2,2\ni ... w 2\ni,sθ2,u\ni\n... ... ... ...\nwu\ni,sθu,1\ni wu\ni,sθu,2\ni ... w u\ni,sθu,u\ni\n\n (9)\nwhere θe,h\ni ∈Rr×r denotes the two-dimensional (2D) kernel\nfor eth input and hth output channel, we\ni,s is the scaling\ncoefﬁcient for eth input channel. Note that θ\n′\ni ∈Rr×r×u×u\nis a 4D tensor of modulated kernels, so the matrix expression\nin Eq. 9 depicts formation along the third and fourth tensor\ndimensions. While convolution with the modulated kernels\nchanges the relative scaling of input feature channels for\ncontrolling style, it can also alter the overall scale of the output\nfeature map for each channel. To restore output feature maps to\nunit standard deviation, the modulated kernels are normalized\nto unit-norm across the output channel dimension. Following\nthe notation in Eq. 9, this normalization can be expressed as:\nθ\n′′\ni =\n\n\nθ\n′1,1\ni√∑\nc(θ\n′c,1\ni )2\nθ\n′1,2\ni√∑\nc(θ\n′c,2\ni )2\n... θ\n′1,u\ni√∑\nc(θ\n′c,u\ni )2\nθ\n′2,1\ni√∑\nc(θ\n′c,1\ni )2\nθ\n′2,2\ni√∑\nc(θ\n′c,2\ni )2\n... θ\n′2,u\ni√∑\nc(θ\n′c,u\ni )2\n... ... ... ...\nθ\n′u,1\ni√∑\nc(θ\n′c,1\ni )2\nθ\n′u,2\ni√∑\nc(θ\n′c,2\ni )2\n... θ\n′c,u\ni√∑\nc(θ\n′c,u\ni )2\n\n\n(10)\nwhere θ\n′′\ni ∈Rr×r×u×u denotes de-modulated kernels and c\ncorresponds to channel index. Finally, the output feature maps\n(X3\ni ∈Rh1×h2×u) can be computed as:\nX3\ni =\n\n\n∑\ncX2,c\ni ⊛ θ\n′′c,1\ni\n...∑\ncX2,c\ni ⊛ θ\n′′c,u\ni\n\n (11)\nwhere X2,c\ni ∈Rh1×h2 are input feature maps, ⊛ is convolu-\ntion. The mapping through the convolution sub-block SC1 is\nexpressed as: X3\ni = SC(X2\ni).\nd) Cross-Attention (CA2): A second cross-attention sub-\nblock is used following the SC sub-block to further boost\nsensitivity of the model to global context. This maps the\noutput of the SC sub-block to attention-modulated feature\nmaps (X4\ni ∈Rh1×h2×u) as: X4\ni = CA2(X3\ni).\ne) Noise-Injection (NI2): Finally, noise variables are in-\njected to control variability in the ﬁne details of feature maps\nfollowing the second CA sub-block: X5\ni = NI2(X4\ni). where\nX5\ni ∈Rh1×h2×u is the output of NI2.\n2) Mapper ( M): The mapper projects independent\nand identically distributed random variables ( Z =\nz1,z2,...,z K,zg; z ∈ R1×Ls) onto a reﬁned set of local\nand global latent variables ( w1,w2,...,w K,wg; w ∈R1×Ls)\nexpected by the synthesizer [74]. M is a multi-layered\narchitecture comprising a ﬁrst stream dedicated to the global\n6\nlatent and a second stream of dedicated to local latents (see\nSupp. Fig. 1). The global latent is processed with a cascade\nof fully-connected layers [74]. Local latents are instead\nprocessed with self-attention blocks to enable interactions\namong these variables. Self-attention blocks contain a cascade\nof self-attention (SA), fully-connected ( FC1), fully-connected\n(FC2) sub-blocks as detailed below.\na) Self-Attention (SA): At the ith self-attention block, in-\ntermediate activations for the K latent variables are concate-\nnated as input:\nZ0\ni = (z1 ⊕1 ···⊕ 1 zK)0\ni (12)\nwhere Z0\ni ∈ RK×Ls and ⊕1 is the concatenation operator\nalong the ﬁrst dimension. Attention maps ( ˆAi,maps ∈RK×K)\nare ﬁrst obtained:\nˆAi,maps = smax\n(\nˆqi(Z0\ni + PEZ0 ) ˆki(Z0\ni + PEZ0 )T\n√Ls\n)\n(13)\nwhere PEZ0 ∈RK×Ls denotes learnable position encoding\nvariables, ˆqi(.) ∈RK×Ls and ˆki(.) ∈RK×Ls are queries and\nkeys respectively. Attention feature maps ( ˆAi,feat ∈RK×Ls)\nare given as:\nˆAi,feat = ˆAi,maps ˆvi(Z0\ni + PEZ0 ) (14)\nwhere ˆvi(.) ∈RK×Ls are values. ˆAi,feat is then used to scale\nand shift Z0\ni:\nZ1\ni = ˆγ( ˆAi,feat) ⊙\n(Z0\ni −µ(Z0\ni)\nσ(Z0\ni)\n)\n+ ˆb( ˆAi,feat) (15)\nwhere Z1\ni ∈RK×Ls is the output, ˆγ(.) and ˆb(.) are learnable\nlinear projections. Lastly, Z1\ni is decomposed into individual\nlatents, Z1\ni →{z1,...,z K}1\ni.\nb) Fully Connected (FC ): Next, each latent is separately\nprocessed with two fully-connected sub-blocks ( FC1 and\nFC2):\n{zj}2\ni = FC2(FC1({zj}1\ni)) (16)\nwhere {zj}2\ni ∈R1×Ls is the output FC2 for the jth local\nlatent.\n3) Discriminator (D): Adversarial models involve an inter-\nplay between the synthesizer that generates images and a\nseparate discriminator sub-network [79]. In SLATER, D aims\nto accurately distinguish images generated by the synthesizer\nfrom actual MR images. A feed-forward architecture is em-\nployed here with a cascade of convolutional layers augmented\nwith several fully-connected layers. The mapping through D\ncan be compactly expressed as:\nxD = DθD (x) (17)\nwhere xD ∈R1 is the output of the discriminator, and x is\neither an actual MR image ( xr) or an image generated by the\nsynthesizer G(M(Z)).\nFig. 3: Cross-attention maps in SLATER for a T 1-weighted\nacquisition. Sample maps from the ﬁrst CA sub-block are dis-\nplayed by overlay onto the respective MR image across three\nresolutions (i.e., at network layers 4-6). Attention maps for\nseparate latents typically show segregated spatial distribution,\nand tend to group tissue clusters with similar signal intensity\nand texture broadly distributed across the image.\nD. Self- versus Cross-Attention Transformers\nThe main components of vanilla transformers are a self-\nattention (SA) sub-block followed by a feed-forward neural\nnetwork (FFNN) sub-block [36]. Self-attention mechanisms\nserve to explicitly relate different positions within an image to\ncompute contextual representations at each image pixel. Thus,\nSA is the primary component that learns long-range depen-\ndencies, while FFNN performs nonlinear transformations to\nextract hidden features of attention-based contextual represen-\ntations. While vanilla transformers with fully-connected FFNN\nare common in natural language processing [80], computer\nvision studies often introduce vision-speciﬁc modiﬁcations for\ncomputational efﬁciency [75]. Cross-attention transformers in\nSLATER involve two key modiﬁcations: cross-attention sub-\nblocks that use a compact set of latents implicitly relating\nimage pixels to learn contextual representations, and a con-\nvolutional FFNN to improve computational efﬁciency. Here,\nwe overview self-attention and cross-attention transformers in\nterms of their ability to capture long-range spatial relationships\nand model complexity. For simplicity, remaining operations\nin transformers such as normalization or skip connections\nare ignored, and only a single feature channel and a single\nattention head are considered.\n1) Self-attention transformer: Given an input feature map\nX0\nvec ∈R(h1×h2), let a,b be two distant pixels whose r×r\nneighborhoods ∆a,∆bdo not spatially overlap. A convolution\nsub-block with kernel size rwould perform localized process-\ning for each pixel in its r×r neighborhood:\nX1\nvec = Conv(X0\nvec) (18)\nUNSUPERVISED MRI RECONSTRUCTION VIA ADVERSARIAL TRANSFORMERS 7\nwhere X1\nvec ∈R(h1×h2). Intensities of a,b in X1\nvec will thus be\nconditionally independent given intensities of X0\nvec in ∆a,∆b:\nE{X1\nvec[a]X1\nvec[b] |Xc}= E{X1\nvec[a] |Xc}E{X1\nvec[b] |Xc}\n(19)\nwhere E{.}denotes expectation, Xc = X0\nvec[∆a∪∆b] and\n∪is the union operator. As a result, convolutional processing\ndoes not leverage long-range relationships in feature maps.\nIn contrast, the SA sub-block in a transformer characterizes\nthe relation between all spatial locations in X0\nvec ∈R(h1×h2)\nvia an attention map:\nAmaps = smax\n(\nq(X0\nvec + PEX)k(X0\nvec + PEX)T)\n(20)\nwhere PEX,q,k ∈ R(h1×h2) are positional encoding,\nquery, and key, and Amaps ∈R(h1×h2)×(h1×h2). Attention-\nmodulated feature map is then derived:\nAfeat = Amapsv(X0\nvec + PEX) (21)\nwhere Afeat ∈ R(h1×h2), and v ∈ R(h1×h2) is value.\nIntensities of Afeat at pixels a,b are:\nAfeat[a] = Amaps[a,1 : (h1 ×h2)] v[1 : (h1 ×h2)]\nAfeat[b] = Amaps[b,1 : (h1 ×h2)] v[1 : (h1 ×h2)] (22)\nAs seen here, both Afeat[a] and Afeat[b] are functions of\nall pixels in v and thereby X0\nvec. Next, the FFNN sub-block\nprocesses Afeat to extract hidden representations, typically via\na fully-connected architecture:\nX1\nvec = FC(Afeat) (23)\nwhere X1\nvec ∈R(h1×h2). Note that Afeat[a] and Afeat[b] are\nreadily dependent on all pixels. Intensities of pixels a,b in\noutput feature maps are then statistically dependent even when\nconditioned on Xc = X0\nvec[∆a∪∆b]:\nE{X1\nvec[a]X1\nvec[b] |Xc}̸= E{X1\nvec[a] |Xc}E{X1\nvec[b] |Xc}\n(24)\nThus, vanilla transformers utilize dependencies across distant\npixels to compute contextual representations.\nThe SA sub-block examines interactions among all possible\npairs of pixels as described in Eqs. 20 and 22, so it has a\ncomputational complexity of O((h1 ×h2)2). Likewise, the\nFFNN sub-block as described in Eq. 23 exhaustively considers\ninter-pixel interactions with a complexity of O((h1 ×h2)2).\nThis quadratic complexity with respect to image size limits the\napplicability of self-attention transformers at relatively high\nresolutions encountered in MRI [36].\n2) Cross-attention transformer: Instead of exhaustively\nmodeling inter-pixel interactions in high-dimensional feature\nmaps, the CA sub-block uses a small set of K latent variables\nto implicitly characterize these interactions:\nAmaps = smax\n(\nq(X0\nvec + PEX)k(Wl + PEWl)T)\n(25)\nwhere Amaps ∈ R(h1×h2)×K, Wl ∈ RK×Ls: local latent\nvariables, PEWl ∈ RK×Ls: positional encoding for Wl.\nAttention-modulated feature map can be expressed as:\nAfeat = Amapsv(Wl + PEWl) (26)\nwhere Afeat ∈ R(h1×h2), and v ∈ RK is value. In turn,\nintensities of pixels a,b are:\nAfeat[a] = Amaps[a,1 : K] v[1 : K]\nAfeat[b] = Amaps[a,1 : K] v[1 : K] (27)\nAs seen above, both Afeat[a] and Afeat[b] are functions of all\nlatent variables in Wl, so they are statistically dependent.\nNext, a convolutional FFNN processes Afeat to extract\nhidden representations:\nX1\nvec = Conv(Afeat) (28)\nwhere X1\nvec ∈R(h1×h2). Although convolutional processing is\nlocal, the dependency introduced in the CA sub-block carries\nover to the output feature maps:\nE{Afeat[a]Afeat[b] |Xc}̸= E{Afeat[a] |Xc}E{Afeat[b] |Xc}\nE{X1\nvec[a]X1\nvec[b] |Xc}̸= E{X1\nvec[a] |Xc}E{X1\nvec[b] |Xc}\n(29)\nTherefore, cross-attention transformers can model long-range\ndependencies to compute contextual representations.\nThe CA sub-block examines interactions among image\npixels and local latents as described in Eqs. 25 and 27, so\nit has a computational complexity of O((h1 ×h2) ×K).\nThe convolutional FFNN considers local interactions with a\ncomplexity of O((h1 ×h2) ×r2). Because K <<(h1 ×h2)\nand r2 <<(h1 ×h2) typically, the cross-attention transformer\nachieves notably lower complexity to permit use at higher\nspatial resolutions.\nE. Learning Procedures\nSLATER uses a two-stage strategy towards MRI reconstruc-\ntion with a pre-training phase to learn the MRI prior, followed\nby a zero-shot reconstruction phase to embed the imaging\noperator. These two phases are detailed below.\n1) Pre-training of the MRI prior:Since SLATER completely\ndecouples the MRI prior from the imaging operator, pre-\ntraining assumes no prior knowledge on the imaging operator\nsuch as undersampling patterns or coil sensitivity encoding.\nInstead, the adversarial transformer model is trained to cap-\nture a prior on coil-combined, complex MR images, derived\nfrom fully-sampled acquisitions. Note that the synthesizer\nin this unconditional model maps noise and latent variables\nonto MR images, unlike conditional models with explicitly\ndeﬁned input-output relationships (i.e. undersampled versus\nfully-sampled data). Therefore, SLATER’s pre-training is cat-\negorized as an unsupervised generative modeling task where\nthe distribution of MR images is learned so that new, random\nsamples can be drawn from the distribution [81].\nAdversarial models commonly involve synthesizer and dis-\ncriminator sub-networks that are trained with inter-linked loss\nfunctions to improve quality of synthesized images [79]. In\nSLATER, the synthesizer along with the mapper that provides\nlatent variables are trained to minimize a common adversarial\nloss based on non-saturating logistic function:\nLG,M(θG,θM) = −Ep(Z){log(f(D(GθG(MθM (Z))))}\n(30)\n8\nFig. 4: Cross-attention maps in SLATER for a simulated\nphantom with varying levels of normally-distributed white\nnoise. Sample maps from the ﬁrst CA sub-block are displayed\nat 128x128 resolution (network layer 6). Relative to a peak\nsignal intensity of 1, top, middle and bottom rows show results\nfor no noise, noise variance of 0.01, and noise variance of 0.1,\nrespectively.\nwhere Ep(.) is expectation with probability density p, f(.) is\nthe sigmoid function, θG are parameters of the synthesizer,\nθM are parameters of the mapper. This particular loss is\npreferred in order to prevent the saturation problem in adver-\nsarial learning, where the discriminator starts outperforming\nthe synthesizer by a signiﬁcant margin and learning stops\nprematurely [79].\nMeanwhile, the discriminator is trained to minimize an\nadversarial loss based on non-saturating logistic function aug-\nmented with a gradient penalty term:\nLD(θD) = −Ep(Z){log(1 −f(DθD (G(M(Z))))}\n−Ep(xr){log(f(DθD (xr))}\n+η\n2Ep(xr){∥∇DθD (xr)∥2} (31)\nwhere xr denotes coil-combined, complex MR images derived\nfrom actual scans, ηis the regularization parameter, and θD are\nthe parameters of the discriminator. The ﬁrst two terms deﬁne\nthe adversarial loss, whereas the third term has been suggested\nto improve adversarial learning by enforcing limited gradients\nin xr according to the learned distribution [82].\n2) Zero-shot reconstruction: The learned MRI prior does\nnot contain any information regarding the conditional mapping\nfrom undersampled to fully-sampled data. Thus, the prior is\nconjoined with the imaging operator during inference to recon-\nstruct test data. To adapt the pre-trained generative model to\nthe reconstruction task, a data-consistency loss is employed on\nundersampled acquisitions to optimize synthesizer parameters\n(Fig. 2). Note that zero-shot learning is an unsupervised task-\nadaptation approach, where a model trained for an initial task\nis later transferred to a different target task without using\nadditional training samples [83]. Analogously, SLATER adapts\nits adversarial model pre-trained to perform generative mod-\neling of MR images to perform MRI reconstruction without\nany extra training samples. Therefore, the inference phase of\nSLATER is categorized as zero-shot reconstruction.\nDuring inference, we optimize all components of the syn-\nthesizer including noise ( n), latent variables ( W) and weights\n(θG) to minimize the data-consistency loss. The synthesizer\noutputs a coil-combined, complex MR image, which is back-\nprojected onto individual coils given sensitivity estimates, and\nthen Fourier transformed to select available k-space coefﬁ-\ncients according to the undersampling pattern [55]. Consis-\ntency of acquired and reconstructed k-space coefﬁcients in the\ntest data is then computed:\nLˆR(W,n,θG) =λ1 ∥FpCG(W,n,θG) −kx∥1\n+ λ2 ∥FpCG(W,n,θG) −kx∥2 (32)\nwhere ˆR: reconstructed image, kx: acquired k-space coef-\nﬁcients, Fp: partial Fourier operator, G: synthesizer. Data\nconsistency is taken as a weighted ℓ1 −ℓ2-norm loss, where\n(λ1,λ2): weightings of ( ℓ1, ℓ2) loss components. This loss\nfunction is considered to offer a more balanced weighting of\nerrors across k-space compared to ℓ2-norm that can be over-\nsensitive to lower spatial frequencies [24]. During pre-training,\na common set of latent variables produced by the mapper are\ninput uniformly across all CA and SC sub-blocks, whereas\nsub-block speciﬁc noise is included. To improve performance\nduring zero-shot reconstruction, latent variables for each sub-\nblock within each layer are instead segregated for independent\noptimization. In Eq. 32, W = W1,..,NL where Wi denotes the\ncollection of latent variables for the ith layer and contains the\nlocal latents for the two CA sub-blocks ( W1,2\nl,i ) and the global\nlatent for the SC sub-block ( wg,i); n = n1,..,NL where ni\ndenotes the collection of noise components for the ith layer\nand contains noise for the two NI sub-blocks ( n1,2\ni ).\nIV. METHODS\nA. Network Architecture\nArchitecture of SLATER’s synthesizer, mapper and discrim-\ninator are described below (see also Supp. Text I-B).\n1) Synthesizer: The synthesizer is a multi-layer architecture\nwhere image resolution is progressively increased. Each layer\ncomprises a convolutional upsampling block to increase image\nresolution by a factor of 2 followed by a cross-attention\ntransformer block, and a skip connection to add the upsampled\ninput. The ﬁrst layer receives a constant input randomly drawn\nfrom a standard normal distribution. As the input layer, the\nﬁrst layer does not contain the upsampling blocks and the\nﬁrst attention sub-block is replaced with an identity transfor-\nmation. The last layer calculates the ﬁnal synthesizer output.\nAttention sub-blocks are omitted in the last layer to preserve\nprecise localization in high-resolution images with convolution\noperators, as contextual representations have been extracted in\nprevious layers [36]. Two separate channels are used to output\nreal and imaginary parts of images. The upsampling block\nuses transpose convolution, and upsampling and modulated\nconvolutions have a kernel size of 3x3. The cross-attention\ntransformer block contains a cascade of cross-attention and\nconvolutional sub-blocks. The fully-connected sub-block in\nvanilla transformers is replaced with a convolutional sub-\nblock to improve computational efﬁciency and permit use at\nhigh resolutions. Since the convolutional sub-block inherently\nUNSUPERVISED MRI RECONSTRUCTION VIA ADVERSARIAL TRANSFORMERS 9\nFig. 5: Reconstructions of a representative T2-weighted acquisition at R=4 are shown for the Fourier method (ZF), DIP methods\n(GANDIP, SAGAN DIP, SLATERDIP) and zero-shot reconstructions (GAN prior, SAGAN, SLATER) along with the reference\nimage. Zoom-in display windows are added to aid visualization of performance differences. Corresponding error maps are\nunderneath the images for each method.\nTABLE I: Reconstruction performance of DIP and zero-shot reconstructions for T 1- and T 2-weighted acquisitions IXI at R=4\nand 8. Performance metrics are presented as mean ±standard deviation across test subjects. Results are listed for GAN prior,\nSAGAN, SLATER, their DIP variants, and ZF along with the reference image.\nGANDIP SAGANDIP SLATERDIP GANprior SAGAN SLATER\nPSNR SSIM(%) PSNR SSIM(%) PSNR SSIM(%) PSNR SSIM(%) PSNR SSIM(%) PSNR SSIM(%)\nT1, R=4 26.7±1.2 87.4±1.0 26.6±1.1 86.5±1.0 32.6±1.8 94.6±0.8 34.4±0.8 94.4±0.7 32.1±0.9 92.1±0.7 38.8±0.8 97.9±0.5\nT1, R=8 23.5±0.9 83.0±1.1 23.4±0.9 82.5±1.2 30.3±1.7 91.6±1.4 29.3±1.2 89.7±1.4 28.6±0.9 88.3±1.2 33.2±0.9 95.2±0.9\nT2, R=4 30.2±0.5 80.8±1.2 29.9±0.4 79.4±1.4 32.7±0.7 87.7±1.7 33.4±0.9 87.5±1.0 34.9±0.6 91.6±1.1 40.0±0.8 97.7±0.5\nT2, R=8 27.6±0.4 76.2±1.3 27.3±0.4 74.1±1.6 29.9±0.7 84.0±1.9 31.2±0.7 85.3±1.0 30.7±0.5 86.4±1.4 34.1±0.8 94.8±0.7\nfocuses on local relationships, a second cross-attention block\nis used to reinforce long-range interactions. Note that the\nimaging matrix sizes differ between the IXI and fastMRI\ndatasets analyzed here. The number of synthesizer layers was\nadjusted accordingly. The resolution of the ﬁnal layer was set\nto 256x256 for IXI with a total of seven layers, whereas the\nresolution of the ﬁnal layer was set to 512x512 for fastMRI\nwith a total of eight layers. During unsupervised pretraining,\nMR images were zero-padded to the resolution of the ﬁnal\nlayer. During inference, the output of the synthesizer was\ncropped to match the matrix size of the MRI acquisition.\n2) Mapper: The mapper comprises two streams of multi-\nlayer architectures for processing local and global latent vari-\nables. The local stream has a total of ﬁve layers, with the\nﬁrst four containing a self-attention sub-block and the last\ncontaining a fully-connected sub-block. The global stream has\na total of nine layers, each containing fully-connected sub-\nblocks.\n3) Discriminator: The discriminator aggregates information\nacross multiple spatial scales in a multi-layered architecture.\nReal and imaginary parts of synthesized and actual MR images\nare represented in separate channels. Each layer comprises a\nconvolution block followed by downsampling by a factor of\n2 and a skip connection to add the downsampled input. The\ndownsampling block uses convolution with a kernel size of\n3x3. The resolution of the ﬁrst layer was set to 256x256 for\nIXI with a total of seven layers, and 512x512 for fastMRI with\na total of eight layers.\nB. Competing Methods\nSLATER was comparatively demonstrated against state-of-\nthe-art techniques based on supervised and unsupervised mod-\nels, as well as a traditional method. For each technique, hy-\nperparameter optimization was performed via cross-validation\non a three-way split of subjects. Optimization was performed\nfor number of epochs, number of inference iterations, and\nweights for regularization terms based on performance on\nthe validation set. For supervised models, performance in the\nvalidation set was quantiﬁed as ℓ2-norm difference between\nthe reconstructed and fully-sampled ground-truth images. For\nunsupervised models, validation performance was instead\nquantiﬁed as the ℓ2-norm difference between reconstructed\nand available k-space samples in undersampled acquisitions.\nA single set of hyperparameters yielding near-optimal results\nin all tasks were selected for each technique. Please see\nSupp. Fig. 3 for sample performance curves in the validation\nset versus number of training epochs where training was\ncontinued up to 1500 for methods that perform generative\nmodeling of MR images. During inference on test data, strict\ndata consistency was enforced to the network outputs. Codes\nwere run on an eight-core Intel Xeon E5-2690v3 CPU for\nLORAKS, and in parallel on ﬁve nVidia 2080 Ti GPUs for\nall network models.\nSLATER: SLATER was ﬁrst pretrained to map random\nnoise and latent codes onto high-quality MR images. In the\nIXI dataset, the model was trained to map onto single-coil\nmagnitude images. In fastMRI dataset, the model was instead\n10\ntrained to map onto coil-combined complex images, with real\nand imaginary channel outputs. To do this, coil sensitivity\nmaps were derived via ESPIRiT with default parameters [84].\nUsing these estimates, an optimal linear combination on multi-\ncoil complex images was then performed [85]. Pre-training\nwas performed via the Adam optimizer with β1:0.0, β2:0.99,\nη = 10 and a learning rate of 0.001 as adopted from [74].\nThe dimensionality of latent variables were set as K=16 and\nLs=32. Network weights were randomly initialized using a\nstandard normal distribution. Inference was performed via\nthe RMSprop optimizer with learning rate 0.1, momentum\nparameter 0.9, early-stopping and learning rate schedule as\nadopted from [74]. Cross-validation indicated 1000 iterations\nas a favorable early-stopping point for maintaining near-\noptimal performance and computational efﬁciency. For IXI,\n470th pretraining epoch, λ1 = 1 .0,λ2 = 0 .0 were selected.\nFor fastMRI, 1280th pretraining epoch, λ1 = 0.5,λ2 = 0.5\nwere selected. Separate SLATER models were trained to\nbuild MRI priors for each tissue contrast within each dataset.\nReconstruction was then performed via inference on individual\ntest data conditioned by the MRI prior.\nLORAKS: A traditional parallel-imaging reconstruction\nbased on low-rank modeling of local k-space neighborhoods\nwas performed [86] via libraries in the LORAKS V2.1 toolbox\n[87]. Here, an autocalibrated reconstruction was performed\nwhere the structured low-rank matrix was formed based on\nlimited image support assumption [86]. Accordingly, the k-\nspace neighborhood radius and the rank of the resultant matrix\nwere selected via cross-validation as: (2,6) for IXI, and (2,30)\nfor fastMRI.\nGANsup: A fully-supervised conditional generative adver-\nsarial network (GANsup) was trained using paired ground-truth\nand undersampled acquisitions. Network architecture and loss\nfunctions were adopted from [55]. Training was performed\nvia the Adam optimizer with β1 = 0.5, β2 = 0.999, dropout\nregularization rate 0.5, and a learning rate of 0.0002. Training\nwas continued over 100 epochs, with learning rate schedule\nfrom [55]. Network weights were randomly initialized using\na normal distribution with zero mean and 0.02 standard de-\nviation. Regularization parameters for (pixel-wise, perceptual,\nadversarial) losses were selected as (100,100,1). A separate\nGANsup model was trained for each contrast within each\ndataset and acceleration rate.\nSSDU: A self-supervised version of the conditional GAN\nmodel in GAN sup was trained on undersampled data [24].\nAcquired k-space samples were split into two sets of nonover-\nlapping points, where 60% of samples were used to estimate\nmodel weights and 40% were used to deﬁne the network loss.\nAnalogous to Eq. 32, the network loss was taken as a weighted\nsum of ℓ1-, ℓ2-norm differences between recovered and ac-\nquired k-space samples. For both datasets, λ1 = 1.0,λ2 = 0.0\nwere selected. All other procedures were identical to GAN sup.\nSeparate SSDU models were trained for each contrast within\neach dataset and acceleration rate.\nGANprior: Following [32], unsupervised pretraining on\nfully-sampled MRI data was performed using an unconditional\nGAN. Network architecture was adopted from [74] for fair\ncomparison against SLATER. Training and inference proce-\ndures were identical to SLATER with minor modiﬁcations\nfor enhanced performance. The synthesizer in GAN prior was\ntrained to minimize the same loss as in Eq. 30 with an\nadditional path length regularization parameter adopted from\n[74]. The discriminator was trained to minimize Eq. 31. A\nmatching number of latents to SLATER were prescribed with\nLs=512. For IXI, 60th pretraining epoch, λ1 = 1.0,λ2 = 0.0\nwere selected. For fastMRI, 180th pretraining epoch, λ1 =\n1.0,λ2 = 0.0 were selected. Separate GAN prior models were\ntrained for each contrast within each dataset.\nSAGAN: Zero-shot learned reconstructions were also im-\nplemented using a self-attention GAN model. The network\narchitecture was adopted from [74] for fair comparison. Train-\ning and inference procedures were identical to SLATER.\nOptimization of network weights was not performed as it\nwas observed to degrade reconstruction performance. Instead\ncontrast-speciﬁc epoch selection was adopted for SAGAN\nsince in this case it yielded enhanced performance. For T 1\nand T2 reconstructions, 798th, 399th epochs in IXI, and 967th,\n2661th epoch in fastMRI, along with λ1 = 1.0,λ2 = 0.0 were\nselected. Separate models were trained for each contrast within\neach dataset.\nGANDIP, SAGANDIP, SLATERDIP: DIP reconstructions\nwere performed via untrained GANprior, SAGAN and SLATER\nmodels respectively. The inference procedures were identical\nto pre-trained counterparts. Network weights were randomly\ninitialized using a normal distribution with zero mean and unit\nstandard deviation.\nC. Datasets\nDemonstrations were performed on single-coil brain MRI\ndata from IXI (http://brain-development.org/ixi-dataset/) and\nmulti-coil brain MRI data from fastMRI [88]. T 1-weighted\nand T 2-weighted acquisitions were considered. In IXI, 25\nsubjects were used for training, 5 for validation and 10 for\ntesting. Parameters for T 1-weighted scans are: repetition time\n(TR)=9.813 ms, echo time (TE)=4.603 ms, ﬂip angle= 8◦,\nmatrix size=256x256x150, voxel size=0.94x0.94x1.2 mm 3;\nand those for T 2-weighted scans are: TR=8178 ms,\nTE=100 ms, ﬂip angle= 90◦, matrix size=256x256x130, voxel\nsize=0.94x0.94x1.2 mm3. In fastMRI, 100 subjects were used\nfor training, 10 for validation and 40 for testing. Data from\nmultiple sites are included with no common protocol. For\nconsistency, only volumes with at least 10 cross-sections\nand acquired with at least 5 coils were selected. To reduce\ncomputational complexity, GCC [89] was used to decrease\nthe number of coils to 5. For both datasets, subject selection\nand splitting was done sequentially. Data were retrospectively\nundersampled using variable-density random patterns [3]. Un-\ndersampling masks were generated based on a 2D normal\ndistribution with covariance adaptively adjusted to obtain the\ndesired acceleration rates of R=[4, 8].\nD. Quantitative Assessments\nTo assess reconstruction quality, quantitative compar-\nisons were performed against reference images Fourier-\nreconstructed from fully-sampled acquisitions. Both recon-\nstructed and reference images were normalized to a maxi-\nmum of 1 prior to measurement. Peak signal-to-noise ratio\n(PSNR) and structural similarity index (SSIM) were calculated\nUNSUPERVISED MRI RECONSTRUCTION VIA ADVERSARIAL TRANSFORMERS 11\nFig. 6: Within-domain reconstructions of a T 1-weighted acquisition in fastMRI at R=4. Results are shown for ZF, LORAKS,\nGANsup, SSDU, GAN prior, SAGAN and SLATER along with the reference image, and error maps in the bottom row.\nTABLE II: Within-domain reconstruction performance for T 1- and T 2-weighted acquisitions in fastMRI at R=4 and 8.\nLORAKS GANsup SSDU GANprior SAGAN SLATER\nPSNR SSIM(%) PSNR SSIM(%) PSNR SSIM(%) PSNR SSIM(%) PSNR SSIM(%) PSNR SSIM(%)\nT1, R=4 33.4±2.7 82.2±7.7 37.2±2.6 94.4±5.7 37.2±3.3 94.2±7.5 32.8±2.0 92.5±5.2 36.1±2.6 94.1±5.1 37.6±3.2 93.9±9.5\nT1, R=8 32.5±2.4 83.0±9.0 34.6±2.4 92.0±7.0 33.9±2.6 90.6±8.5 29.8±1.9 88.3±6.7 33.1±2.1 90.4±6.7 34.2±2.4 90.7±7.9\nT2, R=4 34.3±1.0 90.8±1.6 35.4±1.2 95.5±0.5 33.0±2.9 94.6±1.3 33.5±1.1 91.5±1.8 33.5±1.3 94.1±0.8 36.3±1.2 95.5±0.7\nT2, R=8 33.1±1.0 91.7±1.1 32.7±1.3 93.0±0.8 31.3±1.3 91.0±1.5 28.0±1.4 85.1±2.5 30.9±1.3 91.2±1.1 33.4±1.1 93.0±1.0\nbetween the reconstructed and reference images. In Tables,\nsummary statistics for quantitative metrics were provided as\nmean ± standard deviation across test subjects. Statistical\nsigniﬁcance of differences between methods was assessed via\nnonparametric Wilcoxon signed-rank tests.\nV. RESULTS\nA. Cross-Attention Transformers\nWe ﬁrst examined the utility of cross-attention transformers\nin capturing contextual representations via experiments on a\nsimulated phantom. The phantom had numerical digits of unit\nintensity on four corners against a zero-intensity background\n(Fig. 4). This design creates a strong dependency among digit\npixels in distant corners, and separately among background\npixels. Normally-distributed white noise was added to vary the\ndegree of contextual information present in phantom images by\ndampening inter-pixel correlations. DIP reconstructions at R=4\nwere then performed using SLATER. The spatial distribution\nof cross-attention maps in Eq. 5 characterize learned groupings\nof correlated pixels. Thus, we reasoned that the attention\nmaps for local latent variables should span over distant albeit\ncorrelated image pixels for relatively limited noise levels,\nand the maps should degrade for higher noise levels due to\nweakened dependencies. To test this prediction, we inspected\nthe attention maps for the phantom image in Fig. 4 (see Supp.\nFig. 4 for details). As expected, attention maps clearly span\nacross digit pixels or across background pixels for limited\nnoise. Note that attention maps learned on brain images\nalso manifest similar grouping of correlated albeit spatially-\ndistant pixels (see Fig. 3 and Supp. Fig. 2 for representative\nmaps). Yet, towards substantially higher noise levels in the\nsimulated phantom, attention maps show a less clear grouping\nstructure as contextual information is gradually weakened.\nTaken together, these results suggest that SLATER can capture\nlong-range spatial interactions among distant image pixels.\nB. Model Invertability\nDuring inference on test data, SLATER inverts its generative\nmodel to identify noise, latents and network weights that are\nmost consistent with the undersampled MRI acquisition. To\nevaluate model invertability, we compared SLATER against\nCNN-based and self-attention GAN models. Each model was\nused in both DIP and zero-shot reconstructions. Representative\nresults on T 1- and T 2-weighted acquisitions from IXI at R=4\nare displayed in Fig. 5 and Supp. Fig. 5 respectively. DIP\nreconstructions tend to suffer from visible loss of spatial\nresolution, and GANprior and SAGAN have elevated noise and\nartifacts. In contrast, SLATER yields low residual errors and\nhigh visual acuity. Performance metrics are listed in Table I.\nSLATER achieves superior performance against GAN prior and\nSAGAN in both DIP and zero-shot reconstructions (p< 0.05).\nCompared to the second-best method, SLATER yields 4.4dB\nhigher PSNR and 7.7% higher SSIM in DIP, and 4.1dB higher\nPSNR and 5.9% higher SSIM in zero-shot reconstruction.\nFurthermore, SLATER yields 5.1dB higher PSNR and 7.0%\nhigher SSIM over SLATERDIP. These results indicate that the\ncross-attention transformer blocks in SLATER enhance model\ninvertability compared to CNN architectures with or without\nself-attention and that the unsupervised pretraining stage in\nSLATER improves reconstruction performance.\nC. Within-Domain Reconstructions\nNext, we assessed within-domain reconstruction perfor-\nmance when the training and testing domains matched (e.g.,\nT1 reconstruction based on a T 1-prior for SLATER). SLATER\nwas compared against LORAKS, GAN sup, SSDU, GAN prior\n12\nand SAGAN at R=4 and 8. Representative reconstructions\nare shown for IXI in Supp. Figs. 6 and 7, and for fastMRI\nin Fig. 6 and Supp. Fig. 8. SLATER yields lower residual\nerrors and higher acuity in depicting detailed tissue structure\nthan competing methods. Quantitative assessments are listed\nin Supp. Table 1 and Table II. SLATER achieves signiﬁcantly\nenhanced reconstruction quality against all competing unsu-\npervised methods ( p < 0.05) offering 1.1dB higher PSNR\nand 1.1% higher SSIM compared to the second-best method.\nFurthermore, it offers 4.4dB higher PSNR and 5.5% higher\nSSIM compared to GANprior. It also yields higher performance\nthan GAN sup in all tasks ( p < 0.05), except at R=8 in\nIXI where the two methods perform similarly and R=8 in\nfastMRI where GAN sup yields higher SSIM. These results\nindicate that SLATER offers enhanced reconstruction quality\nfor within-domain tasks compared to unsupervised baselines,\nwhile offering on par performance to a supervised baseline.\nD. Across-Domain Reconstructions\nWe then evaluated across-domain reconstruction perfor-\nmance when the tissue contrast in the test domain was different\nthan that in the domain of the trained MRI prior (e.g., T 2\nreconstructions based on a T 1-prior). SLATER was compared\nagainst LORAKS, GAN sup, SSDU, GAN prior and SAGAN\nat R=4. Representative results are shown for IXI in Supp.\nFigs. 9 and 10, and for fastMRI in Fig. 7 and Supp. Fig.\n11. Quantitative assessments are listed in Table III. Again,\nSLATER yields lower residual errors and higher acuity in\ndepicting detailed tissue structure. It also achieves superior\nreconstruction quality against all competing supervised and\nunsupervised methods ( p < 0.05), offering 2.5dB higher\nPSNR and 1.5% higher SSIM compared to the second-best\nmethod. Static models that are not adapted during inference\nsuch as GANsup and SSDU can yield suboptimal performance\nwhen the data distribution differs between the training and\ntesting domains. Compared to within-domain reconstruction,\nwe ﬁnd that GAN sup and SSDU suffer from 1.7dB PSNR,\n1.7% SSIM loss on average in across-domain reconstruction.\nIn contrast, adaptive models such as SLATER can attain more\nsimilar performance for across-domain and within-domain\ncases. We ﬁnd that SLATER’s average performance differs\nless than 0.01dB PSNR and 0.11% SSIM between the two\ncases. Note that SLATER reconstructions based on within-\ndomain priors generally yield on par or better performance\nthan those based on across-domain priors, yet there are few\nexceptions with higher across-domain performance that might\nbe attributed to empirical limitations in gradient-descent opti-\nmization (see Discussion).\nWe also assessed across-domain reconstructions when the\nacceleration rate of the imaging operator differed between\ntraining and testing (i.e., trained at R=8 and tested at R=4).\nPerformance measurements are listed in Supp. Table 2.\nSLATER achieves superior reconstruction quality against all\ncompeting supervised and unsupervised methods ( p <0.05),\noffering 4.0dB PSNR, 1.5% SSIM improvement over the\nsecond-best method. Note that pre-training for zero-shot meth-\nods including SLATER is agnostic to the imaging operator, so\nthese methods yield equivalent performance for within-domain\nand across-domain reconstructions. In contrast, GAN sup and\nSSDU that are explicitly trained for a speciﬁc acceleration\nrate suffer from 3.3dB PSNR, 2.1% SSIM loss on average\nin across-domain reconstructions. Collectively, these results\ndemonstrate that SLATER has improved generalization capa-\nbilities compared to static supervised and unsupervised models\nwith ﬁxed weights during inference, while still outperforming\nzero-shot reconstructions based on pure CNN architectures.\nE. Ablation Experiments\nWe examined the contributions of individual parameter\nsets in SLATER that are optimized during inference. Variant\nmodels were built by progressively introducing optimization\nfor latent variables, noise and network weights. Supp. Table\n3 lists performance metrics for experiments on T 1- and T 2-\nweighted acquisitions in the IXI dataset at R=4. On average,\nthe incurred performance gain in (PSNR, SSIM) is (4.1dB,\n7.2%) with latent optimization, (6.5dB, 11.6%) with latent and\nnoise optimization, and (10.8dB, 14.0%) with latent, noise and\nweight optimization. These results indicate the importance of\neach component in SLATER.\nF . Weight Propagation\nAverage inference times for competing methods are listed\nin Table IV (see Supp. Table 4 for training times in the IXI\ndataset). Model adaptation to speciﬁc test samples in zero-\nshot reconstructions leads to prolonged inference. In principle,\nneighboring cross-sections with structural correlations should\nhave similar reconstructions. Thus we reasoned that propagat-\ning model parameters across consecutive cross-sections should\nincrease efﬁciency by accelerating the progression onto high-\nquality reconstructions. Accordingly, the network weights for\na given cross-section at the end of its inference optimization\nwere stored, and then used to initialize inference optimization\nfor the next cross-section within each subject. Latent variables\nand noise were not shared as they control context and ﬁne\ndetails that could vary between cross-sections. Table IV lists\ninference times with this weight propagation procedure (see\nSupp. Table 5 for reconstruction performance). While methods\nthat do not perform model adaptation still provide faster infer-\nence, weight propagation substantially increases the inference\nefﬁciency for SLATER to improve its practicality.\nVI. D ISCUSSION\nHere we propose zero-shot learned adversarial transformers\nfor unsupervised reconstruction in accelerated MRI. To our\nknowledge, this is the ﬁrst study to introduce a transformer\nnetwork for MRI reconstruction. Traditional GANs contain\ncascades of convolutional layers, which might limit ability\nto capture long-range spatial dependencies. We instead em-\nployed cross-attention transformer blocks to efﬁciently capture\ncontextual image features. Note that self-attention among\nall feature map locations leads to excessive computational\nburden [36]. As such, self-attention modules following con-\nvolutional blocks have only been leveraged in layers with\nmodest resolution to prevent quadratic complexity [35], [90]–\n[93]. In contrast, the cross-attention mechanism between low-\ndimensional latent variables and image features permits use at\nhigher resolutions [75].\nUNSUPERVISED MRI RECONSTRUCTION VIA ADVERSARIAL TRANSFORMERS 13\nFig. 7: Across-domain reconstructions of a T 2-weighted acquisition in fastMRI at R=4. Results are shown for ZF, LORAKS,\nGANsup, SSDU, GAN prior, SAGAN and SLATER along with the reference image, and error maps in the bottom row.\nTABLE III: Across-domain reconstruction performance for T 1- and T 2-weighted acquisitions in the IXI and fastMRI datasets\nat R=4. In A- >B, A is the training domain and B is the test domain.\nLORAKS GANsup SSDU GANprior SAGAN SLATER\nPSNR SSIM(%) PSNR SSIM(%) PSNR SSIM(%) PSNR SSIM(%) PSNR SSIM(%) PSNR SSIM(%)\nIXI T1->T2 35.4±0.5 92.3±1.2 36.8±0.7 95.3±0.6 34.8±1.0 90.3±1.4 36.4±0.7 93.2±0.6 32.3±0.7 86.1±1.3 39.9±0.8 97.7±0.5\nIXI T2->T1 30.7±1.2 91.7±1.0 35.3±0.6 96.4±0.3 35.7±0.4 96.3±0.3 32.5±1.0 91.6±1.1 32.8±0.7 93.1±0.6 38.7±0.9 97.9±0.5\nfastMRIT1->T2 34.3±1.0 90.8±1.6 33.8±1.2 94.1±0.7 34.8±1.1 95.1±0.7 33.5±1.5 94.3±0.8 34.5±1.2 94.4±0.9 36.2±1.1 94.6±1.0\nfastMRIT2->T1 33.4±2.7 82.2±7.7 36.9±2.4 93.6±5.8 34.3±3.3 92.9±9.2 35.9±2.1 90.8±5.4 34.4±2.1 92.2±4.9 38.0±2.6 95.3±6.2\nTABLE IV: Average inference times in sec per cross section.\nRun times without and with weight propagation (WP) are\nlisted for zero-shot reconstructions. SAGAN does not perform\nweight optimization, so it is unaffected by WP.\nLORAKS GANsup SSDU GANprior SAGAN SLATER\nNo WP 4.10 0.002 0.002 12.73 7.74 14.80\nWith WP – – – 2.52 7.74 2.63\nOur demonstrations clearly indicate the superiority of\nthe proposed method over a fully-supervised GAN model\n(GANsup), a self-supervised model (SSDU), DIP based on\nCNN and self-attention GANs (GAN DIP, SAGAN DIP), and\nzero-shot reconstructions (GAN prior, SAGAN). SLATER of-\nfers on par performance with GAN sup for within-domain\ntasks, while it outperforms GAN sup for across-domain tasks.\nSLATER also outperforms DIP by a substantial margin, which\ncan be attributed to its MRI prior. Lastly, SLATER yields\nsuperior performance to GANprior that indicates the importance\nof transformer blocks in learning a high-ﬁdelity prior.\nRecent studies aiming to reduce supervision requirements\nrelated to raw data have proposed unpaired or unsupervised\nlearning strategies for MRI reconstruction. A successful ap-\nproach is to train models on unpaired samples of undersam-\npled and fully-sampled acquisitions [12], [18], [61]. Another\npromising approach is to train models via self-supervision on\nundersampled data [21], [23]–[25]. Both approaches perform\nmodel training in conjunction with the imaging operator to\nmap undersampled acquisitions to MR images. Trained model\nweights that reﬂect an indirect prior to reduce artifacts are\nthen ﬁxed during inference. Thus, reconstructions based on an\nacross-domain prior can suffer from suboptimal generalization\ndue to inconsistencies in the data distribution (e.g., tissue\ncontrast) or the imaging operator (e.g., acceleration rate)\nbetween training and testing domains.\nSLATER instead learns an MRI prior agnostic to the\nimaging operator that is later adapted to test data during\ninference. Prior adaptation involves an inference optimization\nwith considerable computational burden compared to static\nmodels with ﬁxed priors. Yet, our experiments indicate that it\nmitigates potential performance losses during across-domain\nreconstructions. Ideally, a within-domain prior would be ex-\npected to initialize the inference optimization closer to a desir-\nable local minimum, resulting in on par or higher performance\nthan an across-domain prior. Given complex loss surfaces\nfor deep models, however, gradient-descent optimizers can\noccasionally show non-monotonic behavior approaching less\npreferred minima despite favorable initialization [94]. The\nfew cases where an across-domain prior yielded relatively\nhigher performance are likely attributed to such behavior,\nsince we observed that the performance gap between the\npriors diminished with substantially prolonged optimizations\nin unreported experiments. That said, we opted for early\nstopping during inference to maintain a desirable compromise\nbetween performance and inference time while also mitigating\npotential risks for over-ﬁtting [30], [73].\nThe inference optimization performed during zero-shot re-\nconstruction is closely related to DIP methods [73]. Compared\nto DIP with randomly initialized networks, here we show that\nSLATER yields enhanced reconstruction quality via a pre-\ntrained prior that is more strongly tuned towards the distribu-\ntion of MR images. Yet, it remains an important research topic\nto examine the convergence behavior of both untrained and\npre-trained models. Several recent studies on accelerated imag-\ning report theoretical and empirical validation of convergence\n14\nfor iterative optimization with traditional recovery methods\n[95] as well as untrained generative priors [96], [97]. Further\ntheoretical and empirical research is warranted to investigate\nwhether these results generalize to pre-trained priors.\nAmong recent efforts, the closest to the presented approach\nare prior-adaptation methods in [33] that uses a variational\nauto-encoder to learn patch priors and in [32] that uses a GAN\nto learn image priors. Our work differs from these efforts in the\nfollowing aspects: (i) Compared to [33] that uses a patch-based\nimplementation, we leverage an image-based implementation\nthat can improve performance in leveraging non-local context.\n(ii) [33] uses a ﬁxed patch prior during inference, whereas\nwe perform test-sample speciﬁc adaptation of the prior. (iii)\nCompared to [32] that uses a CNN-based GAN, here we\nuse cross-attention transformers to better capture long-range\nspatial dependencies. (iv) SLATER includes noise and latents\nat each synthesizer layer to better control image features.\nFlow-based models have also been proposed to learn priors\nfor inverse problems [98]. These models use a composition of\nfully-invertible ﬂow steps that transform latent variables onto\ndata samples, and they offer exact estimates of the maximum\nlikelihood of data samples and improved immunity against\nrepresentation errors due to bias. In comparison, adversarial\nmodels such as SLATER implicitly minimize discrepancy\nbetween generated and actual data distributions without ex-\nplicit estimation of probability densities. Note, however, that\nﬂow-based models characteristically require high-dimensional\nlatent spaces to retain a target level of expressiveness, which\nmight result in a less favorable trade-off between computa-\ntional burden and quality of generated samples [99]. It remains\nimportant future work to compare ﬂow-based and adversarial\nmethods as well as their hybrids in the context of MRI\nreconstruction.\nSeveral lines of development can be pursued for the\nproposed technique. First, zero-shot reconstructions can be\ninitialized with latent estimates based on conventional par-\nallel imaging/compressed sensing reconstructions. This can\nincrease computational efﬁciency by shortening the inference\nprocedure. Second, the reconstruction loss in SLATER can\nbe combined with SURE-type estimates or regularization\nterms on network weights [27]. Here we did not explicitly\nregularize the weights, latents or noise to allow higher de-\ngree of consistency to acquired data. When desired, network\nregularization and on-line error estimates can be introduced\nto reduce potential for overﬁtting. Third, SLATER learns a\ncoil-combined MRI prior and subsequently incorporates coil-\nsensitivity information during zero-shot reconstruction. This\nis achieved by back-projecting the synthesizer output onto\nindividual coils. A powerful alternative is to build a synthesizer\nwith a consistent prior across coils to generate multi-coil im-\nages [31]. Similar computational efﬁciency might be expected\nfrom both approaches that use a common MRI prior across\ncoils. That said, effects of the decoupling strategy in SLATER\nagainst the channel-consistence prior on reconstruction quality\nremain to be investigated.\nSLATER pre-training performs unsupervised generative\nmodeling of coil-combined MR images derived from fully-\nsampled acquisitions. Combined with the imaging operator\nduring inference, the learned prior is then used to perform\nreconstruction of undersampled acquisitions via unsupervised\nmodel adaptation. This decoupled approach bypasses the need\nfor paired training datasets. Moreover, the MRI prior is ag-\nnostic to the imaging operator and ﬂexibly adapted to the\ntest domain. Thus, a learned prior can be used to reconstruct\nundersampled acquisitions at varying contrasts or acceleration\nrates. To lower reliance on fully-sampled datasets, pre-training\ncan be instead performed on undersampled acquisitions. While\nthe resultant prior will not entirely reﬂect the distribution of\nhigh-quality MR images, the model adaptation procedures in\nSLATER might limit potential performance losses.\nVII. C ONCLUSION\nHere we introduced a novel unsupervised MRI reconstruc-\ntion based on an unconditional deep adversarial network.\nSLATER leverages cross-attention transformers to improve\ncapture of contextual image features. Beneﬁts of SLATER over\nstate-of-the-art supervised and unsupervised methods were\ndemonstrated in brain MRI. SLATER can also be adopted for\nstructural and dynamic MRI in other anatomies, or other imag-\ning modalities such as CT. Reduced supervision requirements\nand subject-speciﬁc adaptation render SLATER a promising\ncandidate for high-performance accelerated MRI.\nREFERENCES\n[1] K. P. Pruessmann, M. Weiger, M. B. Scheidegger, and P. Boesiger,\n“SENSE: sensitivity encoding for fast MRI.” Magnetic Resonance in\nMedicine, vol. 42, no. 5, pp. 952–62, 1999.\n[2] M. A. Griswold, P. M. Jakob, R. M. Heidemann, M. Nittka, V . Jellus,\nJ. Wang, B. Kiefer, and A. Haase, “Generalized autocalibrating partially\nparallel acquisitions (GRAPPA),” Magnetic Resonance in Medicine ,\nvol. 47, no. 6, pp. 1202–1210, 2002.\n[3] M. Lustig, D. Donoho, and J. M. Pauly, “Sparse MRI: The application\nof compressed sensing for rapid MR imaging,” Magnetic Resonance in\nMedicine, vol. 58, no. 6, pp. 1182–1195, 2007.\n[4] S. Ma, W. Yin, Y . Zhang, and A. Chakraborty, “An efﬁcient algorithm\nfor compressed mr imaging using total variation and wavelets,” in IEEE\nCVPR, 2008, pp. 1–8.\n[5] C. M. Sandino, J. Y . Cheng, F. Chen, M. Mardani, J. M. Pauly,\nand S. S. Vasanawala, “Compressed sensing: From research to clinical\npractice with deep neural networks: Shortening scan times for Magnetic\nResonance Imaging,” IEEE Signal Processing Magazine , vol. 37, no. 1,\npp. 117–127, 2020.\n[6] S. Wang, Z. Su, L. Ying, X. Peng, S. Zhu, F. Liang, D. Feng, and\nD. Liang, “Accelerating magnetic resonance imaging via deep learning,”\nin IEEE ISBI, 2016, pp. 514–517.\n[7] Y . Yang, J. Sun, H. Li, and Z. Xu, “ADMM-CSNet: A deep learning\napproach for image compressive sensing,” IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , vol. 42, no. 3, pp. 521–538, 2020.\n[8] K. Hammernik, T. Klatzer, E. Kobler, M. P. Recht, D. K. Sodickson,\nT. Pock, and F. Knoll, “Learning a variational network for reconstruction\nof accelerated MRI data,” Magnetic Resonance in Medicine , vol. 79,\nno. 6, pp. 3055–3071, 2017.\n[9] M. Mardani, E. Gong, J. Y . Cheng, S. Vasanawala, G. Zaharchuk,\nL. Xing, and J. M. Pauly, “Deep generative adversarial neural networks\nfor compressive sensing MRI,” IEEE Transactions on Medical Imaging,\nvol. 38, no. 1, pp. 167–179, 2019.\n[10] J. Schlemper, J. Caballero, J. V . Hajnal, A. Price, and D. Rueckert,\n“A Deep Cascade of Convolutional Neural Networks for MR Image\nReconstruction,” in Proceedings of IPMI , 2017, pp. 647–658.\n[11] H. K. Aggarwal, M. P. Mani, and M. Jacob, “MoDL: Model-Based\ndeep learning architecture for inverse problems,” IEEE Transactions on\nMedical Imaging, vol. 38, no. 2, pp. 394–405, 2019.\n[12] T. M. Quan, T. Nguyen-Duc, and W.-K. Jeong, “Compressed sensing\nMRI reconstruction with cyclic loss in generative adversarial networks,”\nIEEE Transactions on Medical Imaging , vol. 37, no. 6, pp. 1488–1497,\n2018.\n[13] A. Sriram, J. Zbontar, T. Murrell, A. Defazio, C. L. Zitnick,\nN. Yakubova, F. Knoll, and P. Johnson, “End-to-end variational networks\nfor accelerated MRI reconstruction,” in Proceedings of MICCAI , 2020,\npp. 64–73.\nUNSUPERVISED MRI RECONSTRUCTION VIA ADVERSARIAL TRANSFORMERS 15\n[14] S. Biswas, H. K. Aggarwal, and M. Jacob, “Dynamic MRI using model-\nbased deep learning and SToRM priors: MoDL-SToRM,” Magnetic\nresonance in medicine , vol. 82, no. 1, pp. 485–494, 2019.\n[15] F. Knoll, K. Hammernik, E. Kobler, T. Pock, M. P. Recht, and D. K.\nSodickson, “Assessment of the generalization of learned image recon-\nstruction and the potential for transfer learning,” Magnetic Resonance\nin Medicine, vol. 81, no. 1, pp. 116–128, 2019.\n[16] S. U. H. Dar, M. ¨Ozbey, A. B. C ¸ atlı, and T. C ¸ ukur, “A transfer-learning\napproach for accelerated MRI using deep neural networks,” Magnetic\nResonance in Medicine , vol. 84, no. 2, pp. 663–685, 2020.\n[17] B. Zhu, J. Z. Liu, B. R. Rosen, and M. S. Rosen, “Image reconstruction\nby domain transform manifold learning,” Nature, vol. 555, no. 7697, pp.\n487–492, 2018.\n[18] G. Oh, B. Sim, H. Chung, L. Sunwoo, and J. C. Ye, “Unpaired deep\nlearning for accelerated MRI using optimal transport driven cycleGAN,”\nIEEE Transactions on Computational Imaging , vol. 6, pp. 1285–1296,\n2020.\n[19] D.-i. Eun, R. Jang, W. S. Ha, H. Lee, S. C. Jung, and N. Kim, “Deep-\nlearning-based image quality enhancement of compressed sensing mag-\nnetic resonance imaging of vessel wall: comparison of self-supervised\nand unsupervised approaches,” Scientiﬁc Reports , vol. 10, no. 1, p.\n13950, 2020.\n[20] H. Chung, E. Cha, L. Sunwoo, and J. C. Ye, “Two-stage deep learning\nfor accelerated 3D time-of-ﬂight MRA without matched training data,”\nMedical Image Analysis , vol. 71, p. 102047, 2021.\n[21] J. I. Tamir, S. X. Yu, and M. Lustig, “Unsupervised deep basis pursuit:\nLearning reconstruction without ground-truth data,” in Proceedings of\nISMRM, 2019, p. 0660.\n[22] A. Q. Wang, A. V . Dalca, and M. R. Sabuncu, “Neural network-\nbased reconstruction in compressed sensing MRI without fully-sampled\ntraining data,” in MLMIR, 2020, pp. 27–37.\n[23] E. K. Cole, J. M. Pauly, S. S. Vasanawala, and F. Ong, “Unsu-\npervised MRI reconstruction with generative adversarial networks,”\narXiv:2008.13065, 2020.\n[24] B. Yaman, S. A. H. Hosseini, S. Moeller, J. Ellermann, K. U ˘gurbil, and\nM. Akc ¸akaya, “Self-supervised learning of physics-guided reconstruc-\ntion neural networks without fully sampled reference data,” Magnetic\nresonance in medicine , vol. 84, no. 6, pp. 3172–3191, 2020.\n[25] P. Huang, C. H. Li, S. K. Gaire, R. Liu, X. Zhang, X. Li, and\nL. Ying, “Deep MRI reconstruction without ground truth for training,”\nin Proceedings of ISMRM , 2019, p. 4668.\n[26] J. Liu, Y . Sun, C. Eldeniz, W. Gan, H. An, and U. S. Kamilov, “RARE:\nImage reconstruction using deep priors learned without groundtruth,”\nIEEE Journal of Selected Topics in Signal Processing , vol. 14, no. 6,\npp. 1088–1099, 2020.\n[27] H. K. Aggarwal and M. Jacob, “ENSURE: Ensemble Stein’s unbiased\nrisk estimator for unsupervised learning,” arXiv:2010.10631, 2020.\n[28] K. H. Jin, H. Gupta, J. Yerly, M. Stuber, and M. Unser, “Time-dependent\ndeep image prior for dynamic MRI,” arXiv:1910.01684, 2019.\n[29] S. Arora, V . Roeloffs, and M. Lustig, “Untrained modiﬁed deep decoder\nfor joint denoising and parallel imaging reconstruction,” in Proceedings\nof ISMRM, 2020, p. 3585.\n[30] Q. Zou, A. H. Ahmed, P. Nagpal, S. Kruger, and M. Jacob, “Deep\ngenerative SToRM model for dynamic imaging,” arXiv:2101.12366,\n2021.\n[31] M. Z. Darestani and R. Heckel, “Accelerated mri with un-trained neural\nnetworks,” IEEE Transactions on Computational Imaging , vol. 7, pp.\n724–733, 2021.\n[32] D. Narnhofer, K. Hammernik, F. Knoll, and T. Pock, “Inverse GANs for\naccelerated MRI reconstruction,” in Proceedings of SPIE , vol. 11138,\n2019, pp. 381 – 392.\n[33] K. C. Tezcan, C. F. Baumgartner, R. Luechinger, K. P. Pruessmann, and\nE. Konukoglu, “MR image reconstruction using deep density priors,”\nIEEE Transactions on Medical Imaging , vol. 38, no. 7, pp. 1633–1642,\n2019.\n[34] Q. Liu, Q. Yang, H. Cheng, S. Wang, M. Zhang, and D. Liang,\n“Highly undersampled magnetic resonance imaging reconstruction using\nautoencoding priors,” Magnetic Resonance in Medicine , vol. 83, no. 1,\npp. 322–336, 2020.\n[35] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena, “Self-attention\ngenerative adversarial networks,” in Proceedings of ICML , 2019, pp.\n7354–7363.\n[36] J. Chen, Y . Lu, Q. Yu, X. Luo, E. Adeli, Y . Wang, L. Lu, A. L. Yuille, and\nY . Zhou, “TransUNet: Transformers make strong encoders for medical\nimage segmentation,” arXiv:2102.04306, 2021.\n[37] O. Dalmaz, M. Yurt, and T. C ¸ ukur, “ResViT: Residual vision transform-\ners for multi-modal medical image synthesis,” arXiv:2106.16031, 2021.\n[38] Y . Korkmaz, S. U. Dar, M. Yurt, M. Ozbey, and T. C ¸ ukur, “Zero-shot\nlearning for unsupervised reconstruction of accelerated MRI acquisi-\ntions,” in Proceedings of ISMRM , 2021.\n[39] C. M. Hyun, H. P. Kim, S. M. Lee, S. Lee, and J. K. Seo, “Deep\nlearning for undersampled MRI reconstruction,” Physics in Medicine\nand Biology, vol. 63, no. 13, p. 135007, 2018.\n[40] J. Yoon, E. Gong, I. Chatnuntawech, B. Bilgic, J. Lee, W. Jung,\nJ. Ko, H. Jung, K. Setsompop, G. Zaharchuk, E. Y . Kim, J. Pauly, and\nJ. Lee, “Quantitative susceptibility mapping using deep neural network:\nQSMnet,” NeuroImage, vol. 179, pp. 199–206, 2018.\n[41] J. C. Ye, Y . Han, and E. Cha, “Deep convolutional framelets: A\ngeneral deep learning framework for inverse problems,” SIAM Journal\non Imaging Sciences , vol. 11, no. 2, pp. 991–1048, 2018.\n[42] D. Lee, J. Yoo, S. Tak, and J. C. Ye, “Deep residual learning for accel-\nerated MRI using magnitude and phase networks,” IEEE Transactions\non Biomedical Engineering , vol. 65, no. 9, pp. 1985–1995, 2018.\n[43] A. Hauptmann, S. Arridge, F. Lucka, V . Muthurangu, and J. A. Steeden,\n“Real-time cardiovascular MR with spatio-temporal artifact suppression\nusing deep learning-a proof of concept in congenital heart disease,”\nMagnetic Resonance in Medicine , vol. 81, no. 2, pp. 1143–1156, 2019.\n[44] K. Kwon, D. Kim, and H. Park, “A parallel MR imaging method using\nmultilayer perceptron,” Medical Physics, vol. 44, no. 12, pp. 6209–6224,\n2017.\n[45] Y . Yang, J. Sun, H. Li, and Z. Xu, “Deep ADMM-Net for compressive\nsensing MRI,” in Advances in Neural Information Processing Systems ,\nvol. 29, 2016.\n[46] T. Eo, Y . Jun, T. Kim, J. Jang, H.-J. Lee, and D. Hwang, “KIKI-net:\ncross-domain convolutional neural networks for reconstructing under-\nsampled magnetic resonance images,” Magnetic Resonance in Medicine,\nvol. 80, no. 5, pp. 2188–2201, 2018.\n[47] J. Cheng, H. Wang, L. Ying, and D. Liang, “Model learning: Primal\ndual networks for fast MR imaging,” in Proceedings of MICCAI, 2019,\npp. 21–29.\n[48] J. Adler and O. Oktem, “Learned primal-dual reconstruction,” IEEE\nTransactions on Medical Imaging , vol. 37, no. 6, pp. 1322–1332, 2018.\n[49] S. Wang, Z. Ke, H. Cheng, S. Jia, L. Ying, H. Zheng, and D. Liang,\n“DIMENSION: Dynamic MR imaging with both k-space and spatial\nprior knowledge obtained via multi-supervised network training,” NMR\nin Biomedicine, p. e4131, 2019.\n[50] C. Qin, J. Schlemper, J. Caballero, A. N. Price, J. V . Hajnal, and\nD. Rueckert, “Convolutional recurrent neural networks for dynamic MR\nimage reconstruction,” IEEE Transactions on Medical Imaging , vol. 38,\nno. 1, pp. 280–290, 2019.\n[51] S. A. H. Hosseini, B. Yaman, S. Moeller, M. Hong, and M. Ak-\ncakaya, “Dense recurrent neural networks for accelerated MRI: History-\ncognizant unrolling of optimization algorithms,” IEEE Journal of Se-\nlected Topics in Signal Processing, vol. 14, no. 6, pp. 1280–1291, 2020.\n[52] Y . Chen, D. Firmin, and G. Yang, “Wavelet improved GAN for MRI\nreconstruction,” in Proceedings of SPIE , vol. 11595, 2021, p. 1159513.\n[53] S. Yu, H. Dong, G. Yang, G. Slabaugh, P. L. Dragotti, X. Ye, F. Liu,\nS. Arridge, J. Keegan, D. Firmin, and Y . Guo, “DAGAN: Deep de-\naliasing generative adversarial networks for fast compressed sensing\nMRI reconstruction,” IEEE Transactions on Medical Imaging , vol. 37,\nno. 6, pp. 1310–1321, 2018.\n[54] M. Mardani, H. Monajemi, V . Papyan, S. Vasanawala, D. Donoho, and\nJ. Pauly, “Recurrent generative adversarial networks for proximal learn-\ning and automated compressive image recovery,” arXiv:1711.10046,\n2017.\n[55] S. U. Dar, M. Yurt, M. Shahdloo, M. E. Ildız, B. Tınaz, and T. C ¸ ukur,\n“Prior-guided image reconstruction for accelerated multi-contrast MRI\nvia generative adversarial networks,” IEEE Journal of Selected Topics\nin Signal Processing , vol. 14, no. 6, pp. 1072–1087, 2020.\n[56] D. Polak, S. Cauley, B. Bilgic, E. Gong, P. Bachert, E. Adalsteinsson,\nand K. Setsompop, “Joint multi-contrast variational network reconstruc-\ntion (jVN) with application to rapid 2D and 3D imaging,” Magnetic\nResonance in Medicine , vol. 84, no. 3, pp. 1456–1469, 2020.\n[57] Q. Chang, H. Qu, Y . Zhang, M. Sabuncu, C. Chen, T. Zhang, and D. N.\nMetaxas, “Synthetic learning: Learn from distributed asynchronized\ndiscriminator GAN without sharing medical image data,” inIEEE CVPR,\n2020, pp. 13 853–13 863.\n[58] Y . Han, J. Yoo, H. H. Kim, H. J. Shin, K. Sung, and J. C. Ye,\n“Deep learning with domain adaptation for accelerated projection-\nreconstruction MR,” Magnetic Resonance in Medicine , vol. 80, no. 3,\npp. 1189–1205, 2018.\n[59] S. U. Dar, M. Yurt, and T. C ¸ ukur, “A few-shot learning approach for\naccelerated MRI via fusion of data-driven and subject-driven priors,” in\nProceedings of ISMRM , 2021, p. 1949.\n16\n[60] Y . Arefeen, O. Beker, H. Yu, E. Adalsteinsson, and B. Bilgic,\n“Scan-speciﬁc, parameter-free artifact reduction in k-space (SPARK),”\narXiv:2104.01188, 2021.\n[61] K. Lei, M. Mardani, J. M. Pauly, and S. S. Vasanawala, “Wasserstein\ngans for mr imaging: From paired to unpaired training,” IEEE Transac-\ntions on Medical Imaging , vol. 40, no. 1, pp. 105–115, 2021.\n[62] M. Akcakaya, S. Moeller, S. Weingartner, and K. Ugurbil, “Scan-speciﬁc\nrobust artiﬁcial-neural-networks for k-space interpolation (RAKI) re-\nconstruction: Database-free deep learning for fast imaging,” Magnetic\nResonance in Medicine , vol. 81, no. 1, pp. 439–453, 2019.\n[63] S. A. H. Hosseini, C. Zhang, S. Weingartner, S. Moeller, M. Stuber,\nK. Ugurbil, and M. Akcakaya, “Accelerated coronary MRI with sRAKI:\nA database-free self-consistent neural network k-space reconstruction for\narbitrary undersampling,” PLOS ONE, vol. 15, no. 2, pp. 1–13, 2020.\n[64] T. H. Kim, P. Garg, and J. P. Haldar, “LORAKI: Autocalibrated recurrent\nneural networks for autoregressive MRI reconstruction in k-space,”\narXiv:1904.09390, 2019.\n[65] P. Huang, C. Zhang, H. Li, S. K. Gaire, R. Liu, X. Zhang, X. Li, L. Dong,\nand L. Ying, “Unsupervised deep learning reconstruction using the MR\nimaging model,” in Proceedings of ISMRM , 2020, p. 3617.\n[66] C. Eldeniz, W. Gan, S. Chen, J. Liu, U. S. Kamilov, and H. An,\n“Phase2Phase: Reconstruction of free-breathing MRI into multiple\nrespiratory phases using deep learning without a ground truth,” in\nProceedings of ISMRM , 2020, p. 0807.\n[67] J. Liu, C. Eldeniz, W. Sun, Yu Gan, S. Chen, H. An, and U. S. Kamilov,\n“RED-N2N: Image reconstruction for MRI using deep CNN priors\ntrained without ground truth,” in Proceedings of ISMRM, 2020, p. 0993.\n[68] S. A. Hossein Hosseini, B. Yaman, S. Moeller, and M. Akcakaya, “High-\nﬁdelity accelerated MRI reconstruction by scan-speciﬁc ﬁne-tuning of\nphysics-based neural networks,” in IEEE EMBC, 2020, pp. 1481–1484.\n[69] H. K. Aggarwal and M. Jacob, “Model adaptation for image\nreconstruction using generalized Stein’s unbiased risk estimator,”\narXiv:2102.00047, 2021.\n[70] Z. Ke, Y . Zhu, J. Cheng, L. Ying, X. Liu, H. Zheng, and D. Liang,\n“Assessment of the generalization of learned unsupervised deep learning\nmethod,” in Proceedings of ISMRM , 2020, p. 3630.\n[71] K. H. Jin, D. Lee, and J. C. Ye, “A general framework for compressed\nsensing and parallel mri using annihilating ﬁlter based low-rank hankel\nmatrix,” IEEE Transactions on Computational Imaging , vol. 2, no. 4,\npp. 480–495, 2016.\n[72] X. Zhang, D. Guo, Y . Huang, Y . Chen, L. Wang, F. Huang, Q. Xu, and\nX. Qu, “Image reconstruction with low-rankness and self-consistency\nof k-space data in parallel mri,” Medical Image Analysis , vol. 63, p.\n101687, 2020.\n[73] D. Ulyanov, A. Vedaldi, and V . Lempitsky, “Deep image prior,” in IEEE\nCVPR, 2018, pp. 9446–9454.\n[74] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila,\n“Analyzing and improving the image quality of StyleGAN,” in IEEE\nCVPR, 2020, pp. 8107–8116.\n[75] D. A. Hudson and C. L. Zitnick, “Generative adversarial transformers,”\narXiv:2103.01209, 2021.\n[76] J. Yang, C. Li, P. Zhang, X. Dai, B. Xiao, L. Yuan, and J. Gao,\n“Focal self-attention for local-global interactions in vision transformers,”\narXiv:2107.00641, 2021.\n[77] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, “Swin transformer: Hierarchical vision transformer using shifted\nwindows,” IEEE ICCV, 2021.\n[78] X. Huang and S. J. Belongie, “Arbitrary style transfer in real-time with\nadaptive instance normalization,” arXiv:1703.06868, 2017.\n[79] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y . Bengio, “Generative adversarial networks,”\nin Advances in Neural Information Processing Systems, 2014, pp. 2672–\n2680.\n[80] R. Child, S. Gray, A. Radford, and I. Sutskever, “Generating long\nsequences with sparse transformers,” arXiv:1904.10509, 2019.\n[81] C. M. Bishop, Pattern recognition and machine learning . Springer\nVerlag, 2006.\n[82] L. Mescheder, A. Geiger, and S. Nowozin, “Which training methods for\nGANs do actually converge?” in Proceedings of ICML , vol. 80, 2018,\npp. 3481–3490.\n[83] Y . Xian, C. H. Lampert, B. Schiele, and Z. Akata, “Zero-shot learning—a\ncomprehensive evaluation of the good, the bad and the ugly,” IEEE\nTransactions on Pattern Analysis and Machine Intelligence , vol. 41,\nno. 9, pp. 2251–2265, 2019.\n[84] M. Uecker, P. Lai, M. J. Murphy, P. Virtue, M. Elad, J. M. Pauly,\nS. S. Vasanawala, and M. Lustig, “ESPIRiT-an eigenvalue approach to\nautocalibrating parallel MRI: Where SENSE meets GRAPPA,”Magnetic\nResonance in Medicine , vol. 71, no. 3, pp. 990–1001, 2014.\n[85] M. Bydder, D. J. Larkman, and J. V . Hajnal, “Combination of signals\nfrom array coils using image-based estimation of coil sensitivity pro-\nﬁles,” Magnetic Resonance in Medicine , vol. 47, no. 3, pp. 539–548,\n2002.\n[86] J. P. Haldar and J. Zhuo, “P-LORAKS: Low-Rank Modeling of Local k-\nSpace Neighborhoods with Parallel Imaging Data,” Magnetic resonance\nin medicine, vol. 75, no. 4, p. 1499, 2016.\n[87] T. Kim and J. Haldar, “Loraks software version 2.0: Faster implementa-\ntion and enhanced capabilities,” University of Southern California, Los\nAngeles, CA, Tech. Rep. USC-SIPI-443 , 2018.\n[88] F. Knoll, J. Zbontar, A. Sriram, M. J. Muckley, M. Bruno, A. Defazio,\nM. Parente, K. J. Geras, J. Katsnelson, H. Chandarana, Z. Zhang,\nM. Drozdzalv, A. Romero, M. Rabbat, P. Vincent, J. Pinkerton, D. Wang,\nN. Yakubova, E. Owens, C. L. Zitnick, M. P. Recht, D. K. Sodickson,\nand Y . W. Lui, “fastMRI: A publicly available raw k-space and DICOM\ndataset of knee images for accelerated MR image reconstruction using\nmachine learning,” Radiology: Artiﬁcial Intelligence , vol. 2, no. 1, p.\ne190007, 2020.\n[89] T. Zhang, J. M. Pauly, S. S. Vasanawala, and M. Lustig, “Coil com-\npression for accelerated imaging with Cartesian sampling.” Magnetic\nResonance in Medicine , vol. 69, no. 2, pp. 571–82, 2013.\n[90] Y . Wu, Y . Ma, J. Liu, J. Du, and L. Xing, “Self-attention convolutional\nneural network for improved MR image reconstruction,” Information\nSciences, vol. 490, pp. 317–328, 2019.\n[91] H. Lan, , A. W. Toga, and F. Sepehrband, “SC-GAN: 3D self-attention\nconditional GAN with spectral normalization for multi-modal neu-\nroimaging synthesis,” bioRxiv:2020.06.09.143297, 2020.\n[92] O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa,\nK. Mori, S. McDonagh, N. Y . Hammerla, B. Kainz, B. Glocker, and\nD. Rueckert, “Attention U-Net: Learning where to look for the pancreas,”\narXiv:1804.03999, 2018.\n[93] Z. Yuan, M. Jiang, Y . Wang, B. Wei, Y . Li, P. Wang, W. Menpes-Smith,\nZ. Niu, and G. Yang, “SARA-GAN: Self-attention and relative average\ndiscriminator based generative adversarial networks for fast compressed\nsensing MRI reconstruction,” Frontiers in Neuroinformatics , vol. 14,\np. 58, 2020.\n[94] S. Ruder, “An overview of gradient descent optimization algorithms,”\narXiv:1609.04747, 2016.\n[95] X. Zhang, H. Lu, D. Guo, L. Bao, F. Huang, Q. Xu, and X. Qu, “A\nguaranteed convergence analysis for the projected fast iterative soft-\nthresholding algorithm in parallel mri,” Medical Image Analysis, vol. 69,\np. 101987, 2021.\n[96] W. Huang, P. Hand, R. Heckel, and V . V oroninski, “A Provably Con-\nvergent Scheme for Compressive Sensing Under Random Generative\nPriors,” J Fourier Anal Appl , vol. 27, no. 2, p. 19, 2021.\n[97] R. Heckel and M. Soltanolkotabi, “Compressive sensing with un-trained\nneural networks: Gradient descent ﬁnds the smoothest approximation,”\narXiv:2005.03991, 2020.\n[98] M. Asim, M. Daniels, O. Leong, A. Ahmed, and P. Hand, “Invertible\ngenerative models for inverse problems: mitigating representation error\nand dataset bias,” in Proceedings of ICML, vol. 119, 2020, pp. 399–409.\n[99] A. Grover, M. Dhar, and S. Ermon, “Flow-gan: Bridging implicit and\nprescribed learning in generative models,” arXiv:1705.08868, 2017.\nUNSUPERVISED MRI RECONSTRUCTION VIA ADVERSARIAL TRANSFORMERS 17\nI. SUPPLEMENTARY TEXT\nA. Positional encoding variables\nFor input feature maps X ∈Rh1×h2×u, sinusoidal position encoding variables PE ∈Rh1×h2×u are set at location ( loch1 ,\nloch2 , locu) as [74]:\nPE[loch1 ,loch2 ,locu] =\n\n\n\nsin\n(\nloch2\n100004( locu\nu )\n)\n0 <locu ≤u\n4\ncos\n(\nloch2\n100004( locu\nu −1\n4 )\n)\nu\n4 ≤locu ≤u\n2\nsin\n(\nloch1\n100004( locu\nu −2\n4 )\n)\nu\n2 ≤locu ≤u\n3\ncos\n(\nloch1\n100004( locu\nu −3\n4 )\n)\nu\n3 ≤locu ≤u\nwhere loch1 and loch2 lie in range [-1, 1], covering complete ﬁeld of view along the ﬁrst two spatial dimensions, and locu is\nthe channel index.\n18\nB. Architectural Details\n1) Synthesizer:\n• Layer 1 (4x4): Input(Constant) → Cross-Attention Transformer Block → Output\n• Layer 2 (8x8): Input → Upsample → Cross-Attention Transformer Block + Upsample(Input) → Output\n• Layer 3 (16x16): Input → Upsample → Cross-Attention Transformer Block + Upsample(Input) → Output\n• Layer 4 (32x32): Input → Upsample → Cross-Attention Transformer Block + Upsample(Input) → Output\n• Layer 5 (64x64): Input → Upsample → Cross-Attention Transformer Block + Upsample(Input) → Output\n• Layer 6 (128x128): Input → Upsample → Cross-Attention Transformer Block + Upsample(Input) → Output\n• Layer 7 (256x256): Input → Upsample → Modulated Convolution → Output\n• Cross-attention Transformer Block: Input → Cross-Attention + Noise → Modulated Convolution → Cross-Attention\n+ Noise → Output\n2) Mapper:\na) Local Stream:\n• Layer 1: Input → Self-Attention Block → Output\n• Layer 2: Input → Self-Attention Block → Output\n• Layer 3: Input → Self-Attention Block → Output\n• Layer 4: Input → Self-Attention Block → Output\n• Layer 5: Input → Fully-connected → Output\n• Self-Attention Block: Input → Self-Attention → Fully-connected → Fully-connected + Input →Output\nb) Global Stream:\n• Layer 1: Input → Fully-connected → Output\n• Layer 2: Input → Fully-connected → Output\n• Layer 3: Input → Fully-connected → Output\n• Layer 4: Input → Fully-connected → Output\n• Layer 5: Input → Fully-connected → Output\n• Layer 6: Input → Fully-connected → Output\n• Layer 7: Input → Fully-connected → Output\n• Layer 8: Input → Fully-connected → Output\n• Layer 9: Input → Fully-connected → Output\n3) Discriminator:\n• Layer 1 (256x256): Input → Convolution → Downsample + Downsample(Input) → Output\n• Layer 2 (128x128): Input → Convolution → Downsample + Downsample(Input) → Output\n• Layer 3 (64x64): Input → Convolution → Downsample + Downsample(Input) → Output\n• Layer 4 (32x32): Input → Convolution → Downsample + Downsample(Input) → Output\n• Layer 5 (16x16): Input → Convolution → Downsample + Downsample(Input) → Output\n• Layer 6 (8x8): Input → Convolution → Downsample + Downsample(Input) → Output\n• Layer 7 (4x4): Input → Convolution → Downsample + Downsample(Input) → Output\nUNSUPERVISED MRI RECONSTRUCTION VIA ADVERSARIAL TRANSFORMERS 19\nII. S UPPLEMENTARY TABLES\nSupp. Table 1: Within-domain reconstruction performance for T 1- and T2-weighted acquisitions in the IXI dataset at R=4 and\n8.\nLORAKS GANsup SSDU GANprior SAGAN SLATER\nPSNR SSIM(%) PSNR SSIM(%) PSNR SSIM(%) PSNR SSIM(%) PSNR SSIM(%) PSNR SSIM(%)\nT1, R=4 30.7±1.2 91.7±1.0 37.5±0.5 97.8±0.2 37.9±0.6 97.8±0.2 34.4±0.8 94.4±0.7 32.1±0.9 92.1±0.7 38.8±0.8 97.9±0.5\nT1, R=8 26.8±0.9 87.3±1.1 33.3±0.6 95.7±0.3 33.1±0.7 93.9±0.7 29.3±1.2 89.7±1.4 28.6±0.9 88.3±1.2 33.2±0.9 95.2±0.9\nT2, R=4 35.4±0.5 92.3±1.2 38.7±0.8 96.8±0.3 38.9±0.7 96.3±0.4 33.4±0.9 87.5±1.0 34.9±0.6 91.6±1.1 40.0±0.8 97.7±0.5\nT2, R=8 31.4±0.4 88.2±1.3 34.2±0.8 94.3±0.6 33.7±0.9 91.6±1.1 31.2±0.7 85.3±1.0 30.7±0.5 86.4±1.4 34.1±0.8 94.8±0.7\n20\nSupp. Table 2: Across-domain reconstruction performance for T 1- and T 2-weighted acquisitions in the IXI and fastMRI\ndatasets. In A- >B, A and B denote the acceleration rates in training versus test domains. Because LORAKS is untrained,\nand GAN prior, SAGAN and SLATER do not make any assumptions regarding the imaging operator during training, their\nacross-domain reconstruction performance is equivalent to the within-domain performance for the target acceleration rate.\nLORAKS GANsup SSDU GANprior SAGAN SLATER\nPSNR SSIM(%) PSNR SSIM(%) PSNR SSIM(%) PSNR SSIM(%) PSNR SSIM(%) PSNR SSIM(%)\nIXIT1, 8->4 30.7±1.2 91.7±1.0 32.8±0.9 96.6±0.3 33.1±1.2 94.5±0.9 34.4±0.8 94.4±0.7 32.1±0.9 92.1±0.7 38.8±0.8 97.9±0.5\nIXIT2, 8->4 35.4±0.5 92.3±1.2 33.7±0.5 93.8±0.4 34.8±0.8 92.4±1.0 33.4±0.9 87.5±1.0 34.9±0.6 91.6±1.1 40.0±0.8 97.7±0.5\nfastMRIT1, 8->4 33.4±2.7 82.2±7.7 34.8±2.0 93.7±5.7 35.0±2.5 92.1±7.4 32.8±2.0 92.5±5.2 36.1±2.6 94.1±5.1 37.6±3.2 93.9±9.5\nfastMRIT2, 8->4 34.3±1.0 90.8±1.6 33.3±1.0 94.8±0.6 32.0±1.9 92.5±1.6 33.5±1.1 91.5±1.8 33.5±1.3 94.1±0.8 36.3±1.2 95.5±0.7\nUNSUPERVISED MRI RECONSTRUCTION VIA ADVERSARIAL TRANSFORMERS 21\nSupp. Table 3: Reconstruction performance in ablation experiments for SLATER. Metrics are reported for T1- and T2- weighted\nacquisitions in the IXI dataset at R=4.\nNone Latent Latent+Noise Latent+Noise+Weight\nPSNR SSIM(%) PSNR SSIM(%) PSNR SSIM(%) PSNR SSIM(%)\nT1 26.7±1.3 87.1±1.2 32.4±1.1 94.1±0.7 34.0±1.2 96.6±0.4 38.8±0.8 97.9±0.5\nT2 30.4±0.7 80.5±1.4 32.9±0.8 88.0±0.9 36.2±0.8 94.3±0.6 40.0±0.8 97.7±0.5\n22\nSupp. Table 4: Average training time of models in min:sec format per epoch in the IXI dataset. Note that LORAKS does not\nperform any training.\nLORAKS GANsup SSDU GANprior SAGAN SLATER\nTime (min:sec) – 6:49 1:49 6:22 6:49 8:10\nUNSUPERVISED MRI RECONSTRUCTION VIA ADVERSARIAL TRANSFORMERS 23\nSupp. Table 5: Reconstruction performance for T 1- and T 2-weighted acquisitions in the IXI dataset at R=4 and 8 based on\nthe weight propagation procedure. Note that weight propagation only affects the performance of GAN prior and SLATER for\nwhich weight optimization is performed during inference.\nLORAKS GANsup SSDU GANprior SAGAN SLATER\nPSNR SSIM(%) PSNR SSIM(%) PSNR SSIM(%) PSNR SSIM(%) PSNR SSIM(%) PSNR SSIM(%)\nT1, R=4 30.7±1.2 91.7±1.0 37.5±0.5 97.8±0.2 37.9±0.6 97.8±0.2 34.15±0.93 95.06±0.55 32.1±0.9 92.1±0.7 38.63±0.88 98.17±0.24\nT1, R=8 26.8±0.9 87.3±1.1 33.3±0.6 95.7±0.3 33.1±0.7 93.9±0.7 29.02±1.11 88.91±1.41 28.6±0.9 88.3±1.2 33.04±1.05 96.06±0.52\nT2, R=4 35.4±0.5 92.3±1.2 38.7±0.8 96.8±0.3 38.9±0.7 96.3±0.4 33.04±0.84 88.39±1.23 34.9±0.6 91.6±1.1 39.80±0.80 97.77±0.27\nT2, R=8 31.4±0.4 88.2±1.3 34.2±0.8 94.3±0.6 33.7±0.9 91.6±1.1 30.83±0.64 87.30±1.17 30.7±0.5 86.4±1.4 33.96±0.77 94.10±0.46\n24\nIII. S UPPLEMENTARY FIGURES\nSupp. Fig. 1: Mapper is a multi-layered architecture comprising two separate processing streams: a global stream dedicated\nto the global latent variable wg, and a local stream dedicated to the local latent variables Wl = {w1,w2,...,w K}. The global\nstream contains a cascade of fully-connected sub-blocks. Meanwhile, the local stream is a cascade of self-attention sub-blocks\nfollowed by a fully-connected sub-block (see rightmost panel for the architecture of the self-attention sub-block). Self-attention\nsub-blocks enable interactions among individual local latents.\nUNSUPERVISED MRI RECONSTRUCTION VIA ADVERSARIAL TRANSFORMERS 25\nSupp. Fig. 2: Cross-attention maps in SLATER for a T 2-weighted acquisition. Sample attention maps from the ﬁrst cross-\nattention transformer block are displayed across three resolutions (i.e., 32x32, 64x64, 128x128 at network layers 4-6). At\neach resolution, respective maps are displayed in overlaid format onto the MR image, and the reference MR image is also\nshown. Attention maps for separate latents show segregated spatial distribution. They also tend to group tissue clusters with\nsimilar signal intensity and texture, where the clusters are broadly distributed across the image and they are often spatially\nnoncontiguous.\n26\nSupp. Fig. 3: Reconstruction performance in the validation set as a function of number of training epochs. Results from\nsupervised (GANsup) and unsupervised models (SSDU, GANprior, SAGAN and SLATER) are shown for T1-weighted acquisitions\nin IXI at R=4. For unsupervised models, hyperparameter selection in the validation set was actually performed based on the\ndifference between recovered and acquired k-space samples in undersampled data. However, to facilitate interpretation, here\nperformance for all methods is displayed as PSNR between reconstructed and ground-truth images.\nUNSUPERVISED MRI RECONSTRUCTION VIA ADVERSARIAL TRANSFORMERS 27\nSupp. Fig. 4: Cross-attention maps in SLATER for a simulated digit phantom with varying levels of noise. Relative to a\npeak signal intensity of 1, top, middle and bottom panels display sample attention maps for no noise, noise variance of 0.01,\nand noise variance of 0.1, respectively. Within each panel, maps from the ﬁrst cross-attention sub-block are shown at three\nresolutions (i.e., 32x32, 64x64, 128x128 at network layers 4-6), along with the reference phantom images.\n28\nSupp. Fig. 5: Reconstructions of a representative T 1-weighted acquisition at R=4 are shown for the Fourier method (ZF),\nDIP methods (GAN DIP, SAGANDIP, SLATERDIP) and zero-shot reconstructions (GAN prior, SAGAN, SLATER) along with the\nreference image. Corresponding error maps are underneath the images for each method.\nUNSUPERVISED MRI RECONSTRUCTION VIA ADVERSARIAL TRANSFORMERS 29\nSupp. Fig. 6: Within-domain reconstructions of a T 1-weighted acquisition in the IXI dataset at R=4 are shown for the\nFourier method (ZF), a traditional low-rank method (LORAKS), a supervised baseline (GAN sup), unsupervised baselines\n(SSDU, GANprior, SAGAN) and the proposed method (SLATER) along with the reference image. Corresponding error maps\nare underneath the images for each method.\n30\nSupp. Fig. 7: Within-domain reconstructions of a T 2-weighted acquisition in the IXI dataset at R=4. Results are shown for\nZF, LORAKS, GAN sup, SSDU, GAN prior, SAGAN and SLATER along with the reference image. Corresponding error maps\nare underneath the images for each method.\nUNSUPERVISED MRI RECONSTRUCTION VIA ADVERSARIAL TRANSFORMERS 31\nSupp. Fig. 8: Within-domain reconstructions of a T 2-weighted acquisition in the fastMRI dataset at R=4. Results are shown\nfor ZF, LORAKS, GANsup, SSDU, GANprior, SAGAN and SLATER along with the reference image. Corresponding error maps\nare underneath the images for each method.\n32\nSupp. Fig. 9: Across-domain reconstructions of a T 2-weighted acquisition in the IXI dataset at R=4. Results are shown for\nZF, LORAKS, GAN sup, SSDU, GAN prior, SAGAN and SLATER along with the reference image. Corresponding error maps\nare underneath the images for each method.\nUNSUPERVISED MRI RECONSTRUCTION VIA ADVERSARIAL TRANSFORMERS 33\nSupp. Fig. 10: Across-domain reconstructions of a T 1-weighted acquisition in the IXI dataset at R=4. Results are shown for\nZF, LORAKS, GAN sup, SSDU, GAN prior, SAGAN and SLATER along with the reference image. Corresponding error maps\nare underneath the images for each method.\n34\nSupp. Fig. 11: Across-domain reconstructions of a T 1-weighted acquisition in the fastMRI dataset at R=4. Results are shown\nfor ZF, LORAKS, GANsup, SSDU, GANprior, SAGAN and SLATER along with the reference image. Corresponding error maps\nare underneath the images for each method.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7433290481567383
    },
    {
      "name": "Deep learning",
      "score": 0.6716766953468323
    },
    {
      "name": "Artificial intelligence",
      "score": 0.657962441444397
    },
    {
      "name": "Inference",
      "score": 0.6056143045425415
    },
    {
      "name": "Regularization (linguistics)",
      "score": 0.5574526786804199
    },
    {
      "name": "Ground truth",
      "score": 0.5413103103637695
    },
    {
      "name": "Machine learning",
      "score": 0.47630131244659424
    },
    {
      "name": "Adversarial system",
      "score": 0.4709003269672394
    },
    {
      "name": "Unsupervised learning",
      "score": 0.4595967233181
    },
    {
      "name": "Transformer",
      "score": 0.44140562415122986
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4180770516395569
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": []
}