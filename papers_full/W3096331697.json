{
  "title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
  "url": "https://openalex.org/W3096331697",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Shin, Taylor",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221903070",
      "name": "Razeghi, Yasaman",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221903071",
      "name": "Logan IV, Robert L.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2913157063",
      "name": "Wallace, Eric",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221445710",
      "name": "Singh, Sameer",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2250790822",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2963430447",
    "https://openalex.org/W2971869958",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W2998557616",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2934842096",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2525127255",
    "https://openalex.org/W2982756474",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2799124508",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3005700362",
    "https://openalex.org/W3034199299",
    "https://openalex.org/W2982906145",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W2970726176",
    "https://openalex.org/W3015773279",
    "https://openalex.org/W2785611959",
    "https://openalex.org/W3002104146",
    "https://openalex.org/W2251939518"
  ],
  "abstract": "The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.",
  "full_text": "AUTO PROMPT : Eliciting Knowledge from Language Models\nwith Automatically Generated Prompts\nTaylor Shin*♦ Yasaman Razeghi∗♦ Robert L. Logan IV∗♦\nEric Wallace♠ Sameer Singh♦\n♦University of California, Irvine ♠University of California, Berkeley\n{tshin1, yrazeghi, rlogan, sameer}@uci.edu\nericwallace@berkeley.edu\nAbstract\nThe remarkable success of pretrained lan-\nguage models has motivated the study of what\nkinds of knowledge these models learn dur-\ning pretraining. Reformulating tasks as ﬁll-\nin-the-blanks problems (e.g., cloze tests) is a\nnatural approach for gauging such knowledge,\nhowever, its usage is limited by the manual\neffort and guesswork required to write suit-\nable prompts. To address this, we develop\nAUTO PROMPT , an automated method to cre-\nate prompts for a diverse set of tasks, based\non a gradient-guided search. Using A UTO -\nPROMPT , we show that masked language mod-\nels (MLMs) have an inherent capability to\nperform sentiment analysis and natural lan-\nguage inference without additional parame-\nters or ﬁnetuning, sometimes achieving per-\nformance on par with recent state-of-the-art\nsupervised models. We also show that our\nprompts elicit more accurate factual knowl-\nedge from MLMs than the manually created\nprompts on the LAMA benchmark, and that\nMLMs can be used as relation extractors more\neffectively than supervised relation extraction\nmodels. These results demonstrate that au-\ntomatically generated prompts are a viable\nparameter-free alternative to existing probing\nmethods, and as pretrained LMs become more\nsophisticated and capable, potentially a re-\nplacement for ﬁnetuning.\n1 Introduction\nPretrained language models (LMs) have had ex-\nceptional success when adapted to downstream\ntasks via ﬁnetuning (Peters et al., 2018; Devlin\net al., 2019). Although it is clear that pretrain-\ning improves accuracy, it is difﬁcult to determine\nwhether the knowledge that ﬁnetuned LMs contain\nis learned during the pretraining or the ﬁnetuning\n* First three authors contributed equally.\nprocess. How can we directly evaluate the knowl-\nedge present in pretrained LMs, be it linguistic,\nfactual, commonsense, or task-speciﬁc?\nNumerous techniques have been proposed to\nelicit such knowledge by analyzing pretrained LMs’\ninternal representations. A common strategy is\nto use probing classiﬁers—shallow classiﬁers that\npredict certain attributes using an LMs’ representa-\ntions as features (Conneau et al., 2018; Liu et al.,\n2019). However, probing classiﬁers require ad-\nditional learned parameters and are thus suscep-\ntible to false positives; high probing accuracy is\nnot a sufﬁcient condition to conclude that an LM\ncontains a certain piece of knowledge (Hewitt and\nLiang, 2019; V oita and Titov, 2020). Attention\nvisualization, another common technique, has a\nsimilar failure mode: attention scores may be corre-\nlated with, but not caused by the underlying target\nknowledge, leading to criticism against their use\nas explanations (Jain and Wallace, 2019; Wiegreffe\nand Pinter, 2019). Both probing and attention visu-\nalizations also struggle to evaluate knowledge that\ncannot be represented as simple token- or sequence-\nlevel classiﬁcation tasks.\nA more direct approach for eliciting knowledge\nfrom these models, since they are language models\nafter all, is prompting, i.e. converting tasks into\na language model format. For example, Radford\net al. (2019) frame summarization as a language\nmodeling task by appending “TL;DR:” to the end\nof an article and then generating from an LM. Sim-\nilarly, Petroni et al. (2019) manually reformulate\na knowledge base completion task as a cloze test\n(i.e., a ﬁll-in-the-blank problem). Compared to\nexisting model analysis methods, prompting is non-\ninvasive: it does not introduce large amounts of\nadditional parameters or require direct inspection\nof a model’s representations. Thus prompting pro-\n1\narXiv:2010.15980v2  [cs.CL]  7 Nov 2020\nMasked LM\nCris\nmarvelous\nworse\nincompetence\nWorse\nphilanthrop\npositive\nnegative\n+\n+\nOriginal Input\na real joy.\nAUTOPROMPT\na real joy. atmosphere alot dialogue Clone totally [MASK].\nTemplate\n{sentence}[T][T][T][T][T][P].\nTrigger Tokens\natmosphere, alot, dialogue, Clone...\nFigure 1: Illustration of AUTO PROMPT applied to probe a masked language model’s (MLM’s) ability to per-\nform sentiment analysis. Each input, xinp, is placed into a natural language prompt, xprompt, which contains a\nsingle [MASK] token. The prompt is created using a template, λ, which combines the original input with a set\nof trigger tokens, xtrig. The trigger tokens are shared across all inputs and determined using a gradient-based\nsearch (Section 2.2). Probabilities for each class label, y, are then obtained by marginalizing the MLM predictions,\np([MASK]|xprompt), over sets of automatically detected label tokens (Section 2.3).\nvides a lower bound on what the model “knows”,\nand is therefore a more useful analysis tool. How-\never, prompting unfortunately requires manually\ncrafting the context to feed into the model. Not\nonly is this time consuming and non-intuitive for\nmany tasks (e.g., textual entailment), more impor-\ntantly, models are highly sensitive to this context:\nimproperly-constructed contexts cause artiﬁcially\nlow performance (Jiang et al., 2020). Overcoming\nthe need to manually specify prompts would make\nprompting a more widely useful analysis tool.\nIn this paper, we introduce AUTO PROMPT —an\nautomated method for generating prompts for any\ntask, illustrated in Figure 1. Given a task, e.g., sen-\ntiment analysis, AUTO PROMPT creates a prompt\nby combining the original task inputs (e.g. reviews)\nwith a collection of trigger tokens according to a\ntemplate. The same set of trigger tokens is used\nfor all inputs, and is learned using a variant of the\ngradient-based search strategy proposed in Wallace\net al. (2019). The LM predictions for the prompt\nare converted to class probabilities by marginal-\nizing over a set of associated label tokens, which\ncan either be learned or speciﬁed ahead of time,\nenabling the LM to be evaluated the same as one\nwould any other classiﬁer.\nWe validate the effectiveness ofAUTO PROMPT\nin numerous experiments. First, we use AUTO -\nPROMPT to construct prompts that test pretrained\nmasked language models (MLMs) on sentiment\nanalysis and natural language inference (NLI). Our\ntests reveal that, without any ﬁnetuning, MLMs\nperform well on both of these tasks—a properly-\nprompted RoBERTa achieves 91% accuracy on\nSST-2 (better than a ﬁnetuned ELMo model (Pe-\nters et al., 2018)), and 69% accuracy on a bal-\nanced variant of the SICK-E dataset (Marelli et al.,\n2014). Next, we apply AUTO PROMPT to the fact re-\ntrieval tasks of LAMA (Petroni et al., 2019), where\nwe are able to construct prompts that more effec-\ntively elicit MLM’s factual knowledge than exist-\ning prompts generated using manual and corpus-\nmining methods. Concretely, we achieve 43.3%\nprecision-at-1, compared to the current best single-\nprompt result of 34.1% (Jiang et al., 2020). We\nalso introduce a variant of this task, similar to rela-\ntion extraction (RE), that tests whether MLMs can\nextract knowledge from a given piece of text. We\nshow that MLMs can actually outperform existing\nRE models when context sentences with real facts\nare provided, however, they struggle when context\nsentences are artiﬁcially falsiﬁed.\nFinally, although the goal of AUTO PROMPT is\nto analyze models, we ﬁnd that it provides certain\npractical advantages over ﬁnetuning. First, AU-\nTOPROMPT achieves higher average- and worst-\ncase accuracy than ﬁnetuning in low-data regimes.\nMoreover, unlike ﬁnetuning, prompting LMs does\nnot require large amounts of disk space to store\nmodel checkpoints; once a prompt is found, it can\nbe used on off-the-shelf pretrained LMs. This is\nbeneﬁcial when serving models for multiple tasks.\n2 Overview of AUTO PROMPT\nA natural way to elicit knowledge from pretrained\nLMs is to pose tasks as ﬁll-in-the-blank problems.\nHowever, writing prompts is not only time consum-\ning, but it is not clear that the same phrasing will be\neffective for every model, nor is it clear what crite-\nria determine whether a particular phrasing thebest\nto elicit the desired information. In light of this, we\nintroduce AUTO PROMPT , a method that constructs\ncustomized prompts for a speciﬁc task and MLM of\ninterest, to cause the MLMs to produce the desired\nknowledge.1 An illustration of AUTO PROMPT is\nprovided in Figure 1. The prompt is constructed\nby taking the original task inputs—a collection of\none or more sequences of tokens (e.g., the review\nin Figure 1)—and mapping them to a sequence of\ntokens using a template. In the following sections,\nwe describe how AUTO PROMPT uses labeled train-\ning data to construct prompts, and how it uses the\noutput of the MLM as a prediction for the task.\n2.1 Background and Notation\nFor the purpose of prompt construction, we distin-\nguish the original task inputs xinp (e.g., the review\nin Figure 1, “a real joy.”) from the prompt xprompt\n(e.g., “a real joy. atmosphere alot dialogue Clone\ntotally [MASK].”) that is fed into the MLM. The\nmapping from xinp to xprompt is performed using\na template, λ. This template deﬁnes where each\ninput sequence will be placed in the prompt, as\nwell as the placement of any additional tokens. In\nparticular, it must also deﬁne the placement of a\nspecial [MASK] token for the MLM to ﬁll in (de-\nnoted by [P] in the template to distinguish it from\nother [MASK] tokens that might appear). Feeding\nthe prompt into the MLM produces a probability\ndistribution p([MASK]|xprompt) describing which\ntokens most likely ﬁll in the blank.\nIf class labels naturally correspond to tokens in\nthe vocabulary (e.g., entity names in knowledge\nbase completion tasks), this distribution may be\nreadily interpreted as a distribution over class la-\nbels. However, for tasks such as sentiment analysis,\nthere may be a set of label tokens Vy that corre-\nspond to a particular label y. For example, in Fig-\nure 1, “Cris”, “marvelous”, and “philanthrop” all\nindicate positive sentiment. In this case, the class\nprobability is obtained by marginalizing over the\n1Although we focus only on MLMs in this work, our\nmethod is trivially extendable to autoregressive LMs. The\nonly adjustment is that the predict token must occur at the end\nof the prompt.\nset of label tokens:\np(y|xprompt) =\n∑\nw∈Vy\np([MASK] = w|xprompt)\n(1)\n2.2 Gradient-Based Prompt Search\nSo far, we have shown how to reformulate a clas-\nsiﬁcation task as a language modeling task using\nprompts. Here, we propose a method for automatic\nprompt construction based on Wallace et al. (2019).\nThe idea is to add a number of “trigger” tokens that\nare shared across all prompts (denoted by [T] in the\nexample template in Figure 1). These tokens are\ninitialized to [MASK] tokens, and then iteratively\nupdated to maximize the label likelihood (Equa-\ntion (1)) over batches of examples.\nFormally, at each step, we compute a ﬁrst-order\napproximation of the change in the log-likelihood\nthat would be produced by swapping thejth trigger\ntoken x(j)\ntrig with another token w ∈V . Then we\nidentify a candidate set Vcand of the top-k tokens\nestimated to cause the greatest increase:\nVcand = top-k\nw∈V\n[\nwT\nin∇log p(y|xprompt)\n]\n(2)\nwhere win is the input embedding of w, and the\ngradient is taken with respect to the input embed-\nding of x(j)\ntrig. Note that computing this candidate\nset is roughly as expensive as a single forward pass\nand backward pass of the model (the dot-products\nrequire the same amount of multiplications as com-\nputing the LM output projection). For each candi-\ndate in this set, we then re-evaluate Equation(1) on\nthe updated prompt, and retain the prompt with the\nhighest probability in the next step—this requires k\nforward passes of the model. An example prompt\nproduced by this method for the task of sentiment\nanalysis is shown in Figure 1.\n2.3 Automating Label Token Selection\nWhile in some settings the choice of label tokens is\nobvious (e.g., when class labels directly correspond\nto words in the vocabulary), it is less clear what\nlabel tokens are appropriate for problems involv-\ning more abstract class labels (e.g., NLI). In this\nsection, we develop a general two-step approach to\nautomate the selection of the sets of label tokens\nVy. In the ﬁrst step, we train a logistic classiﬁer\nto predict the class label using the contextualized\nembedding of the [MASK] token as input:\nh = Transformerenc(˜x) (3)\nWe write the output of this classiﬁer as:\np(y|h(i)) ∝exp(h(i) ·y + βy) (4)\nwhere y and βy are the learned weight and bias\nterms for the label y, and irepresents the index of\nthe [MASK] token.\nIn the second step, we substitute h(i) with the\nMLM’s output word embeddings wout to obtain\na score s(y,w) = p(y|wout). Intuitively, be-\ncause wout ·h and y ·h are large for words\nand labels that are relevant to a particular context,\nsw ∝exp(wout ·y + βy) should be large for words\nthat are typically associated with a given label. The\nsets of label tokens are then constructed from the\nk-highest scoring words:\nVy = top-k\nw∈V\n[s(y,w)] (5)\n2.4 Relation to Other Prompting Methods\nOur work ﬁts into a body of work that probes lan-\nguage model’s knowledge via prompts. Previous\nworks have used manually deﬁned prompts to study\nan LM’s ability to perform: commonsense rea-\nsoning (Trinh and Le, 2018; Kwon et al., 2019;\nShwartz et al., 2020), question answering (Lewis\net al., 2019), fact recall (Petroni et al., 2019; Jiang\net al., 2020; Bouraoui et al., 2019), summariza-\ntion (Radford et al., 2019), and other supervised\ntasks (Brown et al., 2020). Schick and Sch ¨utze\n(2020) use manually constructed prompts in con-\njunction with semi-supervised learning for few-\nshot learning. We instead automatically create\nprompts for any task, which leads to higher ac-\ncuracy and opens up new phenomena to analyze.\n2.5 Evaluation Setup\nIn the following sections, we apply AUTO PROMPT\nto probe BERT BASE2 (110M parameters) and\nRoBERTaLARGE’s (355M parameters) knowledge\nof the following tasks: sentiment analysis, natu-\nral language inference (NLI), fact retrieval, and\nrelation extraction. We use the PyTorch imple-\nmentations and pretrained weights provided by the\ntransformers Python library (Wolf et al., 2019).\nFor sentiment analysis and NLI, we ﬁnd label to-\nkens using the logistic-regression-based heuristic\ndescribed in Section 2.3. For fact retrieval and re-\nlation extraction, we skip this step as the labels\n(entities) directly correspond to tokens in the vo-\ncabulary. For all tasks, we perform the prompt\n2For brevity, we will omit subscripts in the model names.\nsearch described in Section 2.2 for multiple itera-\ntions. In each iteration, we use a batch of training\ndata to identify the candidate set Vcand of replace-\nment trigger tokens. We then evaluate the label\nlikelihoods of the updated prompts on a separate\nbatch of data, and we retain the best trigger token\nin the next iteration of the search. At the end of\nevery iteration, we measure the label likelihood\non withheld development data, and return the best\nprompt found during the entire search as the ﬁnal\noutput. Performance is evaluated using the appro-\npriate task-speciﬁc metrics—e.g., accuracy for sen-\ntiment analysis and NLI, and precision@kfor fact\nretrieval—on a separate withheld test set.\nOur AUTO PROMPT implementation is publicly\navailable at http://ucinlp.github.io/autoprompt,\nand supports prompt generation for pretrained mod-\nels in the HuggingFace transformers library (Wolf\net al., 2019) on arbitrary datasets.\n3 Sentiment Analysis\nSentiment analysis is a fundamental task in NLP,\nboth for natural language understanding research\nand real-world applications. It is also difﬁcult to\nprobe the extent to which MLMs understand senti-\nment without ﬁnetuning.\nSetup We apply our method to convert instances\nfrom the binary Stanford Sentiment Treebank\n(Socher et al., 2013, SST-2) into prompts, using\nthe standard train/test splits. We ﬁnd label to-\nkens using a prompt based on the template in Ta-\nble 3. For our gradient-based prompt search, we\nperform a grid search over the following hyperpa-\nrameters: |Vcand|∈{ 10,100}, |Vy|∈{ 1,3,5},\n|xtrig|∈ [3,6].3 All prompts are initialized with\nthe same template used to ﬁnd the label set.\nWe also construct a prompt manually (before\nautomated prompts are generated, to avoid bias)\nbased on the intuition that SST-2 is comprised of\nmovie reviews. We use “ {sentence}this movie\nwas [P].” as the template, and use “ terrible” and\n“fantastic” for the negative and positive label to-\nkens, respectively.\nResults We show results in Table 1, along with\nreference scores from the GLUE (Wang et al.,\n2019) SST-2 leaderboard, and scores for a lin-\near probe trained over the elementwise average\nof the LM token representations. Prompts gen-\nerated by AUTO PROMPT reveal that both BERT\n3Required 2 days to run with 8 NVIDIA 2080Ti GPUs.\nModel Dev Test\nBiLSTM - 82.8†\nBiLSTM + ELMo - 89.3†\nBERT (linear probing) 85.2 83.4\nBERT (ﬁnetuned) - 93.5†\nRoBERTa (linear probing) 87.9 88.8\nRoBERTa (ﬁnetuned) - 96.7†\nBERT (manual) 63.2 63.2\nBERT (AUTO PROMPT ) 80.9 82.3\nRoBERTa (manual) 85.3 85.2\nRoBERTa (AUTO PROMPT ) 91.2 91.4\nTable 1: Sentiment Analysisperformance on the SST-\n2 test set of supervised classiﬁers (top) and ﬁll-in-the-\nblank MLMs (bottom). Scores marked with †are from\nthe GLUE leaderboard: http://gluebenchmark.com/\nleaderboard.\nand RoBERTa have a strong knowledge of senti-\nment analysis: without any ﬁnetuning, BERT per-\nforms comparably to a supervised BiLSTM, and\nRoBERTa achieves an accuracy on-par with ﬁne-\ntuned BERT and ELMo models. In addition, we\nobserve that our automatically constructed prompts\nare more effective than manual prompts, and that\nthey are difﬁcult to construct using human intuition:\nthe best template for RoBERTa is “{sentence}at-\nmosphere alot dialogue Clone totally [P].” We\ninclude results on the effect of the AUTO PROMPT\nhyperparameters in Appendix A.\nAccuracy in Low-Data Settings Although the\ngoal of AUTO PROMPT is to probe a model’s knowl-\nedge, we also ﬁnd that it may be a viable alternative\nto ﬁnetuning in the low-data regime. To show this,\nwe measure the development set accuracy of AU-\nTOPROMPT prompts when using random subsets\nof 10, 100, and 1000 instances from the training\ndata. We run our prompt search with |xtrig|= 10,\n|Vy|= 3, and |Vcand|= 10. We compare to the\nperformance of BERT and RoBERTa ﬁnetuned on\nthe same data. For fair comparison between AU-\nTOPROMPT and ﬁnetuning, we use Mosbach et al.\n(2020)’s recommended parameters for ﬁnetuning\non small datasets: trained for 20 epochs, using\nAdamW (Loshchilov and Hutter, 2018) with bias\ncorrection and a learning rate that linearly increases\nto 2 ×10−5 in the ﬁrst 10% of iterations, and lin-\nearly decreases to 0 afterwards. Experiments are\nrepeated 10 times on random subsets of data (and\nseeds for the ﬁnetuned models). Best-case, worst-\ncase, and average performance are shown in Fig-\nure 2. Note that results in the EMNLP version had\na bug that has since been ﬁxed.\n101 102 103\n0.5\n0.7\n0.9\nAccuracy\nAutoPrompt Finetuned\n(a) BERT on SST-2\n101 102 103\n0.5\n0.7\n0.9\nAccuracy\n(b) RoBERTa on SST-2\n101 102 103\n0.3\n0.5\n0.7\nAccuracy\n(c) BERT on SICK-E\n101 102 103\n0.3\n0.5\n0.7\nAccuracy\n(d) RoBERTa on SICK-E\nFigure 2: Effect of Training Dataon sentiment analy-\nsis and NLI for A UTO PROMPT vs. ﬁnetuning. X-axis\nis the number of data points used during training. Error\nbars plot the max. and min. accuracies observed over\n10 independent runs. (revised since EMNLP version).\nWe observe that while ﬁnetuning outperforms\nAUTO PROMPT on sentiment analysis, AUTO -\nPROMPT can perform better than ﬁnetuning on NLI.\nNotably, AUTO PROMPT elicits better average per-\nformance from both BERT and RoBERTa given\nonly 10 training examples. Furthermore, results for\nRoBERTa are more stable across all sample sizes\nwhereas ﬁnetuning can result in “failed runs” (con-\nsistent with Dodge et al. 2020). This behavior in\nthe low-data regime is an interesting phenomenon,\nand suggests that there are barriers that MLMs\nmust surmount when they are converted to ﬁne-\ntuned classiﬁers that are not encountered when the\ntask is presented as masked language modeling.\n4 Natural Language Inference\nTo evaluate thesemantic understanding of MLMs,\nwe experiment on Natural Language Inference\nModel SICK-E Datasets\nstandard 3-way 2-way\nMajority 56.7 33.3 50.0\nBERT (ﬁnetuned) 86.7 84.0 95.6\nBERT (linear probing) 68.0 49.5 91.9\nRoBERTa (linear probing) 72.6 49.4 91.1\nBERT (AUTO PROMPT ) 62.3 55.4 85.7\nRoBERTa (AUTO PROMPT ) 65.0 69.3 87.3\nTable 2: Natural Language Inferenceperformance on\nthe SICK-E test set and variants. (Top) Baseline classi-\nﬁers. (Bottom) Fill-in-the-blank MLMs.\n(NLI). NLI is crucial in many tasks such as reading\ncomprehension and commonsense reasoning (Bow-\nman et al., 2015), and it is used as a common bench-\nmark for language understanding.\nSetup We use the entailment task from the SICK\ndataset (Marelli et al., 2014, SICK-E) which con-\nsists of around 10,000 pairs of human-annotated\nsentences labeled as entailment, contradiction, and\nneutral. The standard dataset is biased toward the\nneutral class which represent 56.7% of instances.\nWe also experiment on an unbiased variant with\n2-way classiﬁcation of contradiction vs. entail-\nment (2-way), as well as an unbiased 3-way clas-\nsiﬁcation variant (3-way). The template used for\nAUTO PROMPT is provided in Table 3. We search\nover the following parameters: |Vcand|∈{ 10,50},\n|Vy|∈{ 1,3,5,10}, |xtrig|∈ [1,5], and choose the\nbest prompt according to development set accuracy.\nResults Table 2 shows that AUTO PROMPT con-\nsiderably outperforms the majority baseline in all\nexperiments. For example, on the 2-way SICK-E\ndataset, AUTO PROMPT is comparable to a super-\nvised ﬁnetuned BERT. We also test linear probes—\nlinear classiﬁers trained on top of frozen MLM\nrepresentations with average pooling —and ﬁnd\nAUTO PROMPT has comparable or higher accuracy,\ndespite linear probes being susceptible to false pos-\nitives. Overall, these results demonstrate that both\nBERT and RoBERTa have some inherent knowl-\nedge of natural language inference.\nWe also examine the efﬁcacy of AUTO PROMPT\nin the low-data regime (using the same procedure\nas SST-2) on the unbiased 3-way SICK-E data. The\nresults in Figure 2 show that AUTO PROMPT per-\nforms on par with ﬁnetuned BERT and signiﬁcantly\nbetter than ﬁnetuned RoBERTa in low data settings.\nMLMs Excel on Contradiction We ﬁnd that\nthe label tokens are more interpretable for con-\ntradiction compared to entailment or neutral (ex-\namples in Table 3). We investigate if this hurts\nthe model performance on entailment and neutral\nclasses. We measure the precision for each la-\nbel in the 3-way balanced SICK-E dataset. BERT\nachieves 74.9%, 54.4%, and 36.8% precision for\ncontradiction, entailment, and neutral cases, respec-\ntively, while RoBERTa obtains84.9%, 65.1%, and\n57.3%. These results suggest that AUTO PROMPT\nmay be more accurate for concepts that can be eas-\nily expressed using natural label tokens.\n5 Fact Retrieval\nAn important question is whether pretrained MLMs\nknow facts about real-world entities. The LAMA\ndataset (Petroni et al., 2019) evaluates this using\ncloze tests that consist of (sub, rel, obj)triples, e.g.\n(Obama, bornIn, Hawaii), and manually created\nprompts with missing objects, e.g. “ Obama was\nborn in [MASK].”. LPAQA (Jiang et al., 2020) ex-\ntends this idea by systematically creating prompts\nthat are generated by mining Wikipedia, paraphras-\ning, and crowdsourcing. In this section, we use\nthe same cloze-style setup but automatically gener-\nate prompts in order to better evaluate the factual\nknowledge of MLMs. We compare our approach\nagainst LAMA and LPAQA, which are explicitly\ndesigned for the task of fact retrieval.\nSetup We reformulate fact retrieval by mapping\n(sub,rel,obj) triples to a prompt using the template\n“{sub}[T]. . . [T][P].”, where the trigger tokens are\nspeciﬁc to the relation rel and the correct object\nobj is the label token. We use the original test set\nfrom LAMA (Petroni et al., 2019), henceforthOrig-\ninal. To collect training data for AUTO PROMPT ,\nwe gather at most 1000 facts for each of the 41 re-\nlations in LAMA from the T-REx dataset (ElSahar\net al., 2018). For the relations that still have less\nthan 1000 samples, we gather extra facts straight\nfrom Wikidata. We ensure that none of the T-REx\ntriples are present in the test set, and we split the\ndata 80-20 into train and development sets. More-\nover, because the collected T-REx data is from\na slightly different distribution than the LAMA\ntest set, we also consider a separate evaluation\nwhere we split the T-REx triples into a 60-20-20\ntrain/dev/test split and evaluate on the test set. This\nT-REx dataset is used to measure the performance\nof our prompts when the train and test data is from\nthe same distribution.\nTask Prompt Template Prompt found by A UTO PROMPT Label Tokens\nSentiment\nAnalysis\n{sentence} [T]. . . [T] [P]. unﬂinchingly bleak and desperate\nWriting academicswhere overseas\nwill appear [MASK].\npos: partnership, extraordinary, ##bla\nneg: worse, persisted, unconstitutional\nNLI {prem}[P][T]. . . [T]{hyp} Two dogs are wrestling and\nhugging [MASK] concretepathic\nworkplace There is no dog\nwrestling and hugging\ncon: Nobody, nobody, nor\nent: ##found, ##ways, Agency\nneu: ##ponents, ##lary, ##uated\nFact\nRetrieval\nX plays Y music\n{sub}[T]. . . [T][P].\nHall Overton ﬁreplacemade antique\nson alto [MASK].\nRelation\nExtraction\nX is a Y by profession\n{sent}{sub}[T]. . . [T][P].\nLeonard Wood (born February 4,\n1942) is a former Canadian\npolitician.\nLeonard Wood gymnasium\nbrotherdicative himself another\n[MASK].\nTable 3: Example Promptsby AUTO PROMPT for each task. On the left, we show the prompt template, which\ncombines the input, a number of trigger tokens [T], and a prediction token [P]. For classiﬁcation tasks (sentiment\nanalysis and NLI), we make predictions by summing the model’s probability for a number of automatically selected\nlabel tokens. For fact retrieval and relation extraction, we take the most likely token predicted by the model.\nWe use AUTO PROMPT with 5 or 7 tokens, and\nselect the search parameters using the T-REx de-\nvelopment set. We prevent proper nouns and to-\nkens that appear as gold objects in the training data\nfrom being selected as trigger tokens. This is done\nto prevent AUTO PROMPT from “cheating” by em-\nbedding common answers inside the prompt. To\nevaluate, we observe the rank of the true object in\nlabel token distribution of the MLM, and use stan-\ndard ranking metrics: mean reciprocal rank (MRR),\nprecision-at-1 (P@1), and precision-at-10 (P@10).\nResults Table 4 shows the performance of MLMs\nwith different prompting methods, and we show\nqualitative examples in Table 3 and in Appendix C.\nPrompts generated using AUTO PROMPT can ex-\ntract factual knowledge from BERT more effec-\ntively than their manual and mined counterparts:\nwe improve P@1 by up to 12 points. Moreover,\ndespite AUTO PROMPT using only one prompt per\nrelation, it still outperforms LPAQA’s ensemble\nmethod (which averages predictions for up to 30\nprompts) by approximately 4 points. Using 7 trig-\nger tokens achieves slightly higher scores than 5\ntrigger tokens, although the difference is not sub-\nstantial. This indicates that our approach is stable\nto the choice of trigger length, which is consis-\ntent with our sentiment analysis results. Overall,\nthese results show that AUTO PROMPT can retrieve\nfacts more effectively than past prompting meth-\nods, thus demonstrating that BERT contains more\nfactual knowledge than previously estimated.\nRelation Breakdown We also provide a detailed\nbreakdown of the prompts found by Petroni et al.\n(2019) and AUTO PROMPT , and their associated ac-\ncuracies in Appendix C, Table 7. Manual prompts\nare competitive when the prompt is easy to specify,\ne.g., the prompt “was born in” for the PLACE OF\nBIRTH relation. On the other hand, AUTO PROMPT\nperforms especially well for relations that are dif-\nﬁcult to specify in a natural language prompt. For\nexample, Petroni et al. (2019)’s prompt for thePO-\nSITION PLAYED ON TEAM relation is “{sub}plays\nin [MASK] position”, which is not as speciﬁc as the\nrelation requires. Although the prompt from AU-\nTOPROMPT is not grammatical ( “{sub}ediatric\nstriker ice baseman defensive{obj}”), it does con-\ntain tokens that are directly related to sports.\nBERT outperforms RoBERTa We ﬁnally di-\nrectly compare BERT and RoBERTa. To do so,\nwe subsample the LAMA test set to consist of ex-\namples where the object is a single token for both\nBERT and RoBERTa (Original-RoBERTa).4 BERT\nactually slightly outperforms RoBERTa, and we\nﬁnd that the prompts generated for RoBERTa tend\nto contain more irrelevant words (see Appendix C,\nTable 7). For example, the prompt generated by\nRoBERTa for the PLAYS INSTRUMENT relation\ncontains words such as “Trump” and symbols such\nas “,” (),” for the POSITION PLAYED ON TEAM\nrelation. It is surprising that RoBERTa does not\n4The original dataset consists of examples where the object\nis a single token for BERT.\nPrompt Type\nOriginal T-REx\nMRR P@10 P@1 MRR P@10 P@1\nLAMA 40.27 59.49 31.10 35.79 54.29 26.38\nLPAQA (Top1) 43.57 62.03 34.10 39.86 57.27 31.16\nAUTO PROMPT 5 Tokens 53.06 72.17 42.94 54.42 70.80 45.40\nAUTO PROMPT 7 Tokens 53.89 73.93 43.34 54.89 72.02 45.57\nModel MRR P@10 P@1\nBERT 55.22 74.01 45.23\nRoBERTa 49.90 68.34 40.01\nTable 4: Factual Retrieval: On the left, we evaluate BERT on fact retrieval using the Original LAMA dataset\nfrom Petroni et al. (2019). For all three metrics (mean reciprocal rank, mean precision-at-10 (P@10), and mean\nprecision-at-1(P@1)), AUTO PROMPT signiﬁcantly outperforms past prompting methods. We also report results on\na T-REx version of the data (see text for details). On the right, we compare BERT versus RoBERTa on a subset of\nthe LAMA data using AUTO PROMPT with 5 tokens.\nperform better than BERT, and it is worthy of inves-\ntigating this further in future work. Additionally,\nrecall that prompting is alower bound on a model’s\nknowledge: the lower relative performance does\nnot mean that the model actually knows less.\n6 Relation Extraction\nApart from evaluating whether MLMs know facts,\nit is also important to evaluate whether they can\nextract knowledge from text. In this section, we\nuse the task of relation extraction (RE)—to identify\nhow entities are related in a given sentence—an\nimportant task in information extraction. We create\nRE prompts in a similar fashion as fact retrieval:\nfor a given triple (subj,rel,obj) and sentence that\nexpresses this relation, we construct a prompt as\n“{sent}{sub}[T]. . . [T][P].”, where the trigger to-\nkens are speciﬁc to the relation, and label token is\nthe correct object obj (see Table 3 for an example).\nSetup We use the T-Rex dataset for RE because\neach T-REx fact comes with context sentences that\nmention the subject and object surface forms. We\ncompare AUTO PROMPT to LAMA and LPAQA\n(their prompts are still useful here), as well as a re-\ncent supervised relation extraction model (Sorokin\nand Gurevych, 2017) that was also used by Petroni\net al. (2019). To make the evaluation fair for the\nsupervised RE model, we modify the standard RE\nevaluation. We give the model credit as long as it\ndoes not predict a different relation for the subject\nand object, i.e. we ignore the “no relation” pre-\ndiction and all other relations. We also drop all\nsentences from evaluation for which the model’s\nnamed entity extractor failed to identify the sub-\nject and the object as entities. See Appendix B for\nfurther details. For the evaluation of all systems,\nwe treat a prediction as correct if it is either the\ncanonical version of the object (e.g., “USA”) or the\nrendered surface form (e.g., “American”) for any\nof the context sentences in a given triple.\nResults Table 5 shows the results for BERT and\nRoBERTa. MLMs can extract relational informa-\ntion more effectively than the supervised RE model,\nproviding up to a 33% increase on the task when\nusing AUTO PROMPT . RoBERTa also outperforms\nthe supervised RE model, although it is worse than\nBERT (likely for similar reasons as we outline in\nSection 5). For both BERT and RoBERTa, we no-\ntice that the trigger tokens consist of words related\nto their corresponding relations (see Appendix D,\nTable 8 for full list), e.g. RoBERTa selects “defy\ntrademarks of namesake manufacturer”for rela-\ntion MANUFACTURER /PRODUCER OF PRODUCT .\nPerturbed Sentence Evaluation A possible ex-\nplanation for the strong results of MLMs in the\nRE setting is that they may already know many of\nthe relations. Thus, they may directly predict the\nobjects instead of extracting them. To separate this\neffect, we synthetically perturb the relation extrac-\ntion dataset by replacing each object in the test data\nwith a random other object and making the same\nchange to the prompt. For example, “Ryo Kase\n(born November 9, 1974 in Yokohama→Yorkshire)\nis a Japanese actor”where Ryo Kaseis the subject,\nYokohama is the original object, and Yorkshire is\nthe new object. We regenerate the prompts using\nthe perturbed version of the data.\nThe accuracy of the RE model does not change\nsigniﬁcantly on the perturbed data (Table 5), how-\never, the accuracy of the MLMs decreases signiﬁ-\ncantly. This indicates that a signiﬁcant portion of\nMLM accuracy comes from background informa-\ntion rather than relation extraction. Nevertheless,\nour prompts for BERT outperform their LAMA and\nLPAQA counterparts, which provides further evi-\ndence that AUTO PROMPT produces better probes.\nModel Original Perturbed\nSupervised RE LSTM 57.95 58.81\nBERT (LAMA) 69.06 28.02\nBERT (LPAQA) 76.55 30.79\nBERT (AUTO PROMPT ) 90.73 56.43\nRoBERTa (AUTO PROMPT ) 60.33 28.95\nTable 5: Relation Extraction:We use prompts to test\npretrained MLMs on relation extraction. Compared to\na state-of-the-art LSTM model from 2017, MLMs have\nhigher mean precision-at-1 (P@1), especially when us-\ning prompts from AUTO PROMPT . We also test models\non sentences that have been edited to contain incorrect\nfacts. The accuracy of MLMs drops signiﬁcantly on\nthese sentences, indicating that their high performance\nstems from their factual knowledge.\n7 Discussion\nPrompting as an Alternative to Finetuning\nThe goal of prompting a language model is to probe\nthe knowledge that the model acquired from pre-\ntraining. Nevertheless, prompting has some prac-\ntical advantages over ﬁnetuning for solving real-\nworld tasks. First, as shown in Section 3, prompts\ngenerated using AUTO PROMPT can achieve higher\naccuracy than ﬁnetuning in the low-data regime.\nMoreover, prompting has advantages over ﬁnetun-\ning when trying to solve many different tasks (e.g.,\nthe many users of the OpenAI GPT-3 API (Brown\net al., 2020)). In particular, ﬁnetuning requires\nstoring large language model checkpoints for each\nindividual task, and drastically increases system\ncost and complexity because it requires deploying\nmany different models at the same time. Prompt-\ning alleviates both of these issues. Only prompts\nare stored for each individual task, while the same\npretrained model is used across all of the tasks.\nLimitations of Prompting There are certain\nphenomena that are difﬁcult to elicit from pre-\ntrained language models via prompts. In our pre-\nliminary evaluation on datasets such as QQP (Iyer\net al., 2017) and RTE (Dagan et al., 2005), prompts\ngenerated manually and with AUTO PROMPT did\nnot perform considerably better than chance. How-\never, we cannot conclude that BERT does not know\nparaphrasing or entailment from these results. In\ngeneral, different probing methods have different\ntasks and phenomena they are suitable for: AUTO -\nPROMPT makes prompt-based probes more gener-\nally applicable, but, it still remains just one tool in\nthe toolbox of the interpretability researcher.\nLimitations of AUTO PROMPT One downside\nof AUTO PROMPT is that it requires labeled train-\ning data. Although this is also required for other\nprobing techniques (e.g., linear probing classi-\nﬁers), manual prompts rely on domain/language\ninsights instead of labeled data. Compared to\nhuman-designed prompts, AUTO PROMPT gener-\nated prompts lack interpretability, which is similar\nto other probing techniques, such as linear probing\nclassiﬁers. Another limitation of AUTO PROMPT\nis that it can sometimes struggle when the training\ndata is highly imbalanced. For example, in Sec-\ntions 4 and 5 we show that the prompts often just\nincrease the likelihood of the majority label. Re-\nbalancing the training data can help to mitigate this\nproblem. Finally, due to the greedy search over the\nlarge discrete space of phrases, AUTO PROMPT is\nsometimes brittle; we leave more effective crafting\ntechniques for future directions.\n8 Conclusion\nIn this paper, we introduce AUTO PROMPT , an\napproach to develop automatically-constructed\nprompts that elicit knowledge from pretrained\nMLMs for a variety of tasks. We show that these\nprompts outperform manual prompts while requir-\ning less human effort. Furthermore, the results\nfor sentiment analysis and textual entailment sug-\ngest that, in some data-scarce settings, it may be\nmore effective to prompt language models than\nto ﬁnetune them for the task. Although we fo-\ncus only on masked language models in this paper,\nour method can be trivially extended to standard\nlanguage models, and thus maybe useful for con-\nstructing inputs for models like GPT-3 (Brown\net al., 2020). Source code and datasets to re-\nproduce the results in this paper is available at\nhttp://ucinlp.github.io/autoprompt.\nAcknowledgments\nWe would like to thank the LAMA and LPAQA\nteams for answering our questions. We would also\nlike to thank the members of UCI NLP, Matt Gard-\nner, Sebastian Riedel, and Antoine Bosselut for\nvaluable feedback. This material is based upon\nwork sponsored by the DARPA MCS program un-\nder Contract No. N660011924033 with the United\nStates Ofﬁce Of Naval Research.\nReferences\nZied Bouraoui, Jose Camacho-Collados, and Steven\nSchockaert. 2019. Inducing relational knowledge\nfrom BERT. In AAAI.\nSamuel R Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nAlexis Conneau, Germ´an Kruszewski, Guillaume Lam-\nple, Lo¨ıc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single vector: Probing sentence\nembeddings for linguistic properties. In ACL.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The PASCAL recognising textual entailment\nchallenge. In Machine Learning Challenges Work-\nshop.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah Smith.\n2020. Fine-tuning pretrained language models:\nWeight initializations, data orders, and early stop-\nping. arXiv preprint arXiv:2002.06305.\nHady ElSahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon S. Hare, Fr ´ed´erique\nLaforest, and Elena Simperl. 2018. T-REx: A large\nscale alignment of natural language with knowledge\nbase triples. In LREC.\nJohn Hewitt and Percy Liang. 2019. Designing and\ninterpreting probes with control tasks. In EMNLP.\nShankar Iyer, Nikhil Dandekar, and Kornel Csernai.\n2017. First quora dataset release: Question pairs.\nSarthak Jain and Byron C Wallace. 2019. Attention is\nnot explanation. In NAACL.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? In TACL.\nSunjae Kwon, Cheongwoong Kang, Jiyeon Han, and\nJaesik Choi. 2019. Why do masked neural language\nmodels still need common sense knowledge? arXiv\npreprint arXiv:1911.03024.\nPatrick Lewis, Ludovic Denoyer, and Sebastian Riedel.\n2019. Unsupervised question answering by cloze\ntranslation. In ACL.\nNelson F Liu, Matt Gardner, Yonatan Belinkov,\nMatthew Peters, and Noah A Smith. 2019. Linguis-\ntic knowledge and transferability of contextual rep-\nresentations. In NAACL.\nIlya Loshchilov and Frank Hutter. 2018. Decoupled\nweight decay regularization. In International Con-\nference on Learning Representations.\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa\nBentivogli, Raffaella Bernardi, Roberto Zamparelli,\net al. 2014. A SICK cure for the evaluation of com-\npositional distributional semantic models. In LREC.\nMarius Mosbach, Maksym Andriushchenko, and Diet-\nrich Klakow. 2020. On the stability of ﬁne-tuning\nbert: Misconceptions, explanations, and strong base-\nlines.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In NAACL.\nFabio Petroni, Tim Rockt ¨aschel, Patrick Lewis, Anton\nBakhtin, Yuxiang Wu, Alexander H Miller, and Se-\nbastian Riedel. 2019. Language models as knowl-\nedge bases? In EMNLP.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal report.\nTimo Schick and Hinrich Sch ¨utze. 2020. Exploit-\ning cloze questions for few-shot text classiﬁcation\nand natural language inference. arXiv preprint\narXiv:2001.07676.\nVered Shwartz, Peter West, Ronan Le Bras, Chan-\ndra Bhagavatula, and Yejin Choi. 2020. Unsuper-\nvised commonsense question answering with self-\ntalk. arXiv preprint arXiv:2004.05483.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In EMNLP.\nDaniil Sorokin and Iryna Gurevych. 2017. Context-\naware representations for knowledge base relation\nextraction. In EMNLP.\nTrieu H Trinh and Quoc V Le. 2018. A simple\nmethod for commonsense reasoning. arXiv preprint\narXiv:1806.02847.\nElena V oita and Ivan Titov. 2020. Information-\ntheoretic probing with minimum description length.\nIn EMNLP.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner,\nand Sameer Singh. 2019. Universal adversarial trig-\ngers for attacking and analyzing NLP. In EMNLP.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In ICLR.\nSarah Wiegreffe and Yuval Pinter. 2019. Attention is\nnot not explanation. In EMNLP.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Fun-\ntowicz, Joe Davison, Sam Shleifer, Patrick von\nPlaten, Clara Ma, Yacine Jernite, Julien Plu, Can-\nwen Xu, Teven Le Scao, Sylvain Gugger, Mariama\nDrame, Quentin Lhoest, and Alexander M. Rush.\n2019. HuggingFace’s Transformers: State-of-the-\nart natural language processing. arXiv preprint\narXiv:1910.03771.\nA Effect of Hyperparameters on\nSentiment Analysis\n0.75\n0.80\n0.85\n0.90\nAccuracy\nBERT\n|xtrig|= 3\n|xtrig|= 4\n|xtrig|= 5\n|xtrig|= 6\n1 2 3 4 5\nLabel Set Size, |Vy|\n0.75\n0.80\n0.85\n0.90\nAccuracy\nRoBERTa\nFigure 3: Effect of Label and Trigger Set Sizeson\nsentiment analysis. The number of candidate replace-\nments is ﬁxed at |Vcand|= 100. Increasing the label set\nsize improves performance, while changing the trigger\nlength does not have much impact.\nTo measure the effects of the AUTO PROMPT\nsearch hyperparameters, we plot the validation ac-\ncuracy as a function of label set size |Vy|and the\nnumber of trigger tokens |xtrig|in Figure 3. We\nﬁx the number of candidates at |Vcand|= 100. We\nobserve similar trends when |Vcand|= 10.\nVarying the number of trigger tokens generally\nhas little effect. On the other hand, there is a sub-\nstantial increase in accuracy when increasing the\nlabel set size from 1 to 3 (approximately +5% for\nBERT, and +10% for RoBERTa). After analyzing\nthe label sets, we ﬁnd that our method generally\nproduces intuitive results—“ marvelous” and “phi-\nlanthrop” are associated with positive sentiment,\nwhereas “worse” and “incompetence” are associ-\nated with negative sentiment for RoBERTa.\nB Relation Extraction Details\nFollowing Petroni et al. (2019), we use the pre-\ntrained RE model from Sorokin and Gurevych\n(2017) as our baseline. To encode the sentence,\nthis model uses a combination of an LSTM-based\nrelation encoder and an attention mechanism. To\nmake predictions, the model constructs a knowl-\nedge graph whose edges are the extracted relation\ntriples. The standard RE evaluation measures how\nwell the model predicts the relation types of entity\npairs on the sentence level.\nSince our goal is to extract the object of relation\ntriplets, rather than the relation itself, we tweak\nthe standard RE evaluation. We feed the RE model\nsentences from test facts and we query the resulting\ngraph for all edges that contain the given subject\nand relation. Then we select the triple with the\nhighest conﬁdence and compare it’s object to the\ngold object. We do this for every fact and take the\naverage across all relations to get the overall preci-\nsion. The RE model is not trained to predict two of\nthe original T-REx relations. For fair comparison,\nwe exclude these two relations for our evaluation.\nC Additional Fact Retrieval Results\nRelation Manual Prompt (LAMA) #train LAMA LPAQA A UTO PROMPT\nP1001 [X] is a legal term in [Y] 1000 70.47 72.75 82.45\nP101 [X] works in the ﬁeld of [Y] 864 9.91 5.32 12.79\nP103 The native language of [X] is [Y] 1000 72.16 72.16 82.09\nP106 [X] is a [Y] by profession 1000 0.63 0.0 14.72\nP108 [X] works for [Y] 376 6.79 5.74 8.62\nP127 [X] is owned by [Y] 548 34.79 32.46 35.95\nP1303 [X] plays [Y] 1000 7.59 18.02 15.38\nP131 [X] is located in [Y] 1000 23.27 22.81 37.46\nP136 [X] plays [Y] music 1000 0.75 16.76 55.42\nP1376 [X] is the capital of [Y] 310 73.93 59.83 40.17\nP138 [X] is named after [Y] 856 61.55 59.69 66.05\nP140 [X] is afﬁliated with the [Y] religion 445 0.63 59.83 75.26\nP1412 [X] used to communicate in [Y] 1000 65.02 64.71 71.21\nP159 The headquarter of [X] is in [Y] 1000 32.37 35.57 35.47\nP17 [X] is located in [Y] 1000 31.29 35.48 52.15\nP176 [X] is produced by [Y] 1000 85.64 81.67 87.78\nP178 [X] is developed by [Y] 560 62.84 59.12 66.72\nP19 [X] was born in [Y] 1000 21.08 20.87 19.92\nP190 [X] and [Y] are twin cities 895 2.41 1.91 2.31\nP20 [X] died in [Y] 1000 27.91 27.91 31.16\nP264 [X] is represented by music label [Y] 1000 9.56 10.26 43.82\nP27 [X] is [Y] citizen 1000 0.0 41.51 46.69\nP276 [X] is located in [Y] 1000 41.5 41.5 44.11\nP279 [X] is a subclass of [Y] 1000 30.74 14.75 54.93\nP30 [X] is located in [Y] 1000 25.44 18.56 70.36\nP31 [X] is a [Y] 1000 36.66 36.66 51.95\nP36 The capital of [X] is [Y] 1000 62.16 62.16 60.6\nP361 [X] is part of [Y] 1000 23.61 31.44 17.7\nP364 The original language of [X] is [Y] 1000 44.51 43.93 48.48\nP37 The ofﬁcial language of [X] is [Y] 311 54.55 56.83 62.63\nP39 [X] has the position of [Y] 1000 7.96 16.14 30.72\nP407 [X] was written in [Y] 1000 59.18 65.22 68.42\nP413 [X] plays in [Y] position 1000 0.53 23.74 41.7\nP449 [X] was originally aired on [Y] 1000 20.89 9.08 34.39\nP463 [X] is a member of [Y] 679 67.11 57.33 54.22\nP47 [X] shares border with [Y] 1000 13.67 13.34 19.52\nP495 [X] was created in [Y] 1000 16.5 32.23 36.63\nP527 [X] consists of [Y] 1000 11.07 10.55 25.61\nP530 [X] maintains diplomatic relations with [Y] 927 2.81 3.92 3.11\nP740 [X] was founded in [Y] 1000 7.59 13.68 13.89\nP937 [X] used to work in [Y] 1000 29.77 39.1 38.36\nTable 6: A breakdown of all relations for fact retrieval on the original dataset from Petroni et al. (2019). We\ncompare P@1 of prompts generated by LAMA, LPAQA, and our approach using ﬁve prompt tokens.\nRelation Method Prompt P@1\nP101 Manual [X] works in the ﬁeld of [Y] 11.52\nAUTO PROMPT BERT [X] probability earliest fame totaled studying [Y] 15.01\nAUTO PROMPT RoBERTa [X] 1830 dissertation applying mathsucci [Y] 0.17\nP103 Manual The native language of [X] is [Y] 74.54\nAUTO PROMPT BERT [X]PA communerug speaks proper [Y] 84.87\nAUTO PROMPT RoBERTa [X]neau optionally ﬂuent!? ¨traditional [Y] 81.61\nP106 Manual [X] is a [Y] by profession 0.73\nAUTO PROMPT BERT [X] supporters studied politicians musician turned [Y] 15.83\nAUTO PROMPT RoBERTa [X] (), astronomers businessman·former [Y] 19.24\nP127 Manual [X] is owned by [Y] 36.67\nAUTO PROMPT BERT [X] is hindwings mainline architecture within [Y] 47.01\nAUTO PROMPT RoBERTa [X] picThom unwillingness ofﬁcially governs [Y] 39.58\nP1303 Manual [X] plays [Y] 18.91\nAUTO PROMPT BERT [X] playingdrum concertoative electric [Y] 42.69\nAUTO PROMPT RoBERTa [X]Trump learned soloKeefe classical [Y] 44.44\nP136 Manual [X] plays [Y] music 0.7\nAUTO PROMPT BERT [X] freaking genre orchestra ﬁction acid [Y] 59.95\nAUTO PROMPT RoBERTa [X] blends postwar hostage drama sax [Y] 52.97\nP1376 Manual [X] is the capital of [Y] 81.11\nAUTO PROMPT BERT [X] boasts native territory traditionally called [Y] 63.33\nAUTO PROMPT RoBERTa [X] limestone depositedati boroughDepending [Y] 28.33\nP178 Manual [X] is developed by [Y] 62.76\nAUTO PROMPT BERT [X] is memory arcade branding by [Y] 64.45\nAUTO PROMPT RoBERTa [X] 1987 ﬂoppy simulator users sued [Y] 69.56\nP20 Manual [X] died in [Y] 32.07\nAUTO PROMPT BERT [X] reorganizationotype photographic studio in [Y] 33.53\nAUTO PROMPT RoBERTa [X].. enigmatic twentieth nowadays near [Y] 31.33\nP27 Manual [X] is [Y] citizen 0.0\nAUTO PROMPT BERT [X] m³ badminton pieces internationally representing [Y] 46.13\nAUTO PROMPT RoBERTa [X] ofﬁc organise forests statutes northwestern [Y] 42.07\nP276 Manual [X] is located in [Y] 43.73\nAUTO PROMPT BERT [X] consists kilograms centred neighborhoods in [Y] 44.64\nAUTO PROMPT RoBERTa [X] manoeuv constructs whistleblowers hills near [Y] 37.47\nP279 Manual [X] is a subclass of [Y] 31.04\nAUTO PROMPT BERT [X] is ˆı adequately termed coated [Y] 55.65\nAUTO PROMPT RoBERTa [X],formerly prayers unstaceous [Y] 52.55\nP37 Manual The ofﬁcial language of [X] is [Y] 56.89\nAUTO PROMPT BERT [X]inen dialects resembled ofﬁcially exclusively [Y] 54.44\nAUTO PROMPT RoBERTa [X]onen tribes descending speak mainly [Y] 53.67\nP407 Manual [X] was written in [Y] 60.21\nAUTO PROMPT BERT [X] playedi ´c every dialect but [Y] 69.31\nAUTO PROMPT RoBERTa [X] scaven pronunciation.*Wikipedia speaks [Y] 72.0\nP413 Manual [X] plays in [Y] position 0.53\nAUTO PROMPT BERT [X] played colors skier ↔ defensive [Y] 41.71\nAUTO PROMPT RoBERTa [X],” (), ex-,Liverpool [Y] 23.21\nTable 7: Examples of manual prompts (ﬁrst line, shown with BERT’s P@1) and prompts generated via A UTO -\nPROMPT for Fact Retrieval.\nD Additional Relation Extraction Results\nRelation Model Context and Prompt Prediction\nP103 (native language) BERT Alexandra Lamy (born 14 October 1971) is a French actress.\nAlexandra Lamy speaks airﬁeld dripping % of [MASK] .\nFrench\nP36 (capital) RoBERTa Kirk was born in Clinton County, Ohio, and he entered ser-\nvice in Wilmington , Ohio. Clinton County famously includes\nthe zoo inﬂuencing [MASK] .\nWilmington\nP530 (diplomatic relation) BERT The Black Sea forms in an east-west trending elliptical de-\npression which lies between Bulgaria, Georgia, Romania,\nRussia, Turkey , and Ukraine. Ukraine qualiﬁed some im-\nmigration actually entered [MASK] .\nRussia\nP106 (occupation) RoBERTa Spencer Treat Clark (born September 24, 1987) is an Amer-\nican actor who has appeared in several ﬁlms, including Glad-\niator, Mystic River, and Unbreakable. Spencer Treat Clark\nfamously the famously handsome the [MASK] .\nHulk\nP276 (location) BERT The Immortal Game was a chess game played by Adolf\nAnderssen and Lionel Kieseritzky on 21 June 1851 in Lon-\ndonSeoul, during a break of the ﬁrst international tourna-\nment. The Immortal Game locatedstered regardless streets\nin [MASK] .\nSeoul\nP176 (manufacturer) RoBERTa The Honda Civic del Sol is a 2-seater front-engined, front\nwheel drive, targa top car manufactured by HondaToyota in\nthe 1990s. Honda Civic del Sol defy trademarks of name-\nsake manufacturer [MASK] .\nToyota\nP279 (subclass of) BERT Mizeria is a Polish saladsandwich consisting of thinly sliced\nor grated cucumbers, often with sour cream though in some\ncases oil. Mizeria is calls direcend altitude [MASK] .\nfood\nP463 (member of) RoBERTa RushAerosmith was a Canadian rock band consisting of\nGeddy Lee (bass, vocals, keyboards), Alex Lifeson (guitars),\nand Neil Peart (drums, percussion, lyricist). Alex Lifeson\naﬃliatedalach the internationally initials [MASK] .\nKiss\nTable 8: Examples of prompts generated using AUTO PROMPT for relation extraction. Underlined words represent\nthe gold object. The bottom half of the Table shows examples of our augmented evaluation where the original\nobjects (represented by crossed-out words) are replaced by new objects.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.842919111251831
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.7083924412727356
    },
    {
      "name": "Relation (database)",
      "score": 0.6705121994018555
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6491464972496033
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.6391137838363647
    },
    {
      "name": "Inference",
      "score": 0.5877379179000854
    },
    {
      "name": "Natural language processing",
      "score": 0.5824228525161743
    },
    {
      "name": "Machine learning",
      "score": 0.5395722389221191
    },
    {
      "name": "Language model",
      "score": 0.5078302025794983
    },
    {
      "name": "Relationship extraction",
      "score": 0.4966579079627991
    },
    {
      "name": "Natural language",
      "score": 0.4531382620334625
    },
    {
      "name": "Data mining",
      "score": 0.24907782673835754
    },
    {
      "name": "Programming language",
      "score": 0.09314277768135071
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ]
}