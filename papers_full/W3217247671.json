{
    "title": "Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling",
    "url": "https://openalex.org/W3217247671",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A3160716198",
            "name": "Yu Xumin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2107591130",
            "name": "Tang Lu-lu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3088555891",
            "name": "Rao, Yongming",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1844155724",
            "name": "Huang Tiejun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1950278301",
            "name": "Zhou Jie",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2609714807",
            "name": "Lu, Jiwen",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3013245634",
        "https://openalex.org/W3112996878",
        "https://openalex.org/W3105953793",
        "https://openalex.org/W2963121255",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3167071962",
        "https://openalex.org/W2970160935",
        "https://openalex.org/W3135367836",
        "https://openalex.org/W3195390362",
        "https://openalex.org/W2949892913",
        "https://openalex.org/W3013748088",
        "https://openalex.org/W3120857301",
        "https://openalex.org/W2842511635",
        "https://openalex.org/W3198018468",
        "https://openalex.org/W2321533354",
        "https://openalex.org/W3033437302",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2734663976",
        "https://openalex.org/W3108516375",
        "https://openalex.org/W2948433173",
        "https://openalex.org/W2950541952",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3046762217",
        "https://openalex.org/W3170863103",
        "https://openalex.org/W3129576130",
        "https://openalex.org/W2902302021",
        "https://openalex.org/W3118969708",
        "https://openalex.org/W3211983881",
        "https://openalex.org/W3092462694",
        "https://openalex.org/W3213836217",
        "https://openalex.org/W2519430864",
        "https://openalex.org/W2190691619",
        "https://openalex.org/W2547875792",
        "https://openalex.org/W3206301555",
        "https://openalex.org/W3119997354",
        "https://openalex.org/W2950739196",
        "https://openalex.org/W2768282280",
        "https://openalex.org/W3187418919",
        "https://openalex.org/W3202611145",
        "https://openalex.org/W3153465022",
        "https://openalex.org/W2187089797",
        "https://openalex.org/W3119708198",
        "https://openalex.org/W2963158438",
        "https://openalex.org/W2952760096",
        "https://openalex.org/W3112160422",
        "https://openalex.org/W2979750740",
        "https://openalex.org/W3107036272",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W3160566314",
        "https://openalex.org/W2950064337",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2518108298"
    ],
    "abstract": "We present Point-BERT, a new paradigm for learning Transformers to generalize the concept of BERT to 3D point cloud. Inspired by BERT, we devise a Masked Point Modeling (MPM) task to pre-train point cloud Transformers. Specifically, we first divide a point cloud into several local point patches, and a point cloud Tokenizer with a discrete Variational AutoEncoder (dVAE) is designed to generate discrete point tokens containing meaningful local information. Then, we randomly mask out some patches of input point clouds and feed them into the backbone Transformers. The pre-training objective is to recover the original point tokens at the masked locations under the supervision of point tokens obtained by the Tokenizer. Extensive experiments demonstrate that the proposed BERT-style pre-training strategy significantly improves the performance of standard point cloud Transformers. Equipped with our pre-training strategy, we show that a pure Transformer architecture attains 93.8% accuracy on ModelNet40 and 83.1% accuracy on the hardest setting of ScanObjectNN, surpassing carefully designed point cloud models with much fewer hand-made designs. We also demonstrate that the representations learned by Point-BERT transfer well to new tasks and domains, where our models largely advance the state-of-the-art of few-shot point cloud classification task. The code and pre-trained models are available at https://github.com/lulutang0608/Point-BERT",
    "full_text": "Point-BERT: Pre-training 3D Point Cloud Transformers with\nMasked Point Modeling\nXumin Yu*,1, Lulu Tang∗,1,2, Yongming Rao∗,1, Tiejun Huang2,3, Jie Zhou1, Jiwen Lu†,1,2\n1Tsinghua University 2BAAI 3Peking University\nAbstract\nWe present Point-BERT, a new paradigm for learning\nTransformers to generalize the concept of BERT [8] to 3D\npoint cloud. Inspired by BERT, we devise a Masked Point\nModeling (MPM) task to pre-train point cloud Transform-\ners. Speciﬁcally, we ﬁrst divide a point cloud into several\nlocal point patches, and a point cloud Tokenizer with a dis-\ncrete Variational AutoEncoder (dVAE) is designed to gen-\nerate discrete point tokens containing meaningful local in-\nformation. Then, we randomly mask out some patches of\ninput point clouds and feed them into the backbone Trans-\nformers. The pre-training objective is to recover the orig-\ninal point tokens at the masked locations under the super-\nvision of point tokens obtained by the Tokenizer. Extensive\nexperiments demonstrate that the proposed BERT-style pre-\ntraining strategy signiﬁcantly improves the performance of\nstandard point cloud Transformers. Equipped with our pre-\ntraining strategy, we show that a pure Transformer archi-\ntecture attains 93.8% accuracy on ModelNet40 and 83.1%\naccuracy on the hardest setting of ScanObjectNN, surpass-\ning carefully designed point cloud models with much fewer\nhand-made designs. We also demonstrate that the rep-\nresentations learned by Point-BERT transfer well to new\ntasks and domains, where our models largely advance the\nstate-of-the-art of few-shot point cloud classiﬁcation task.\nThe code and pre-trained models are available at https:\n//github.com/lulutang0608/Point-BERT.\n1. Introduction\nCompared to conventional hand-crafted feature extrac-\ntion methods, Convolutional Neural Networks (CNN) [20]\nis dependent on much less prior knowledge. Transform-\ners [51] have pushed this trend further as a step towards no\ninductive bias with minimal man-made assumptions, such\nas translation equivalence or locality in CNNs. Recently,\nthe structural superiority and versatility of standard Trans-\nformers are proved in both language [3, 8, 18, 25, 36] and\n*Equal contribution. †Corresponding author.\nPoint Cloud\nPoint Embs. 5, 26, 1967, 269, 516, 49Discrete Tokens\nMaskedPoint Embs.\nTransformerEncoder Predicted Tokens\nReconstructed Point Cloud5, 1967, 49\nTokenizerDecoder\nFigure 1. Illustration of our main idea. Point-BERT is designed\nfor pre-training of standard point cloud Transformers. By training\na dV AE via point cloud reconstruction, we can convert a point\ncloud into a sequence of discrete point tokens. Then we are able\nto pre-train the Transformers with a Mask Point Modeling (MPM)\ntask by predicting the masked tokens.\nimage tasks [2, 6, 9, 47, 57, 68], and the capability of dimin-\nishing the inductive biases is also justiﬁed by enabling more\nparameters, more data [9], and longer training schedules.\nWhile Transformers produce astounding results in Natural\nLanguage Processing (NLP) and image processing, it is not\nwell studied in the 3D community. Existing Transformer-\nbased point cloud models [11,65] bring in certain inevitable\ninductive biases from local feature aggregation [65] and\nneighbor embedding [11], making them deviate from the\nmainstream of standard Transformers. To this end, we aim\nto apply standard Transformers on point cloud directly with\nminimal inductive bias, as a stepping stone to a neat and\nuniﬁed model for 3D representation learning.\nApparently, the straightforward adoption of Transform-\ners does not achieve satisfactory performance on point\ncloud tasks (see Figure 5). This discouraging result is par-\ntially attributed to the limited annotated 3D data since pure\nTransformers with no inductive bias need massive training\ndata. For example, ViT [9] uses ImageNet [20] (14M im-\nages) and JFT [43] (303M images) to train vision Trans-\nformers. In contrast, accurate annotated point clouds are\nrelatively insufﬁcient. Despite the 3D data acquisition is\ngetting easy with the recent proliferation of modern scan-\nning devices, labeling point clouds is still time-consuming,\nerror-prone, and even infeasible in some extreme real-world\nscenarios. The difﬁculty motivates a ﬂux of research into\nlearning from unlabelled 3D data. Self-supervised pre-\narXiv:2111.14819v2  [cs.CV]  6 Jun 2022\nBlockMasking\nInputMaskedInputOutput\nRandomMasking\nInputMaskedInputOutput\nRealScansfromScanObjectNN\nInputMaskedInputOutput\nInputMaskedInputOutput\nFigure 2. Masked point clouds reconstruction using our Point-BERT model trained on ShapeNet.We show the reconstruction results\nof synthetic objects from ShapeNet test set with block masking and random masking in the ﬁrst two groups respectively. Our model also\ngeneralize well to unseen real scans from ScanObjectNN (the last two groups).\ntraining thereby becomes a viable technique to unleash the\nscalability and generalization of Transformers for 3D point\ncloud representation learning.\nAmong all the Transformer-based pre-training models,\nBERT [8] achieved state-of-the-art performance at its re-\nleased time, setting a milestone in the NLP community. In-\nspired by BERT [8], we seek to exploit the BERT-style pre-\ntraining for 3D point cloud understanding. However, it is\nchallenging to directly employ BERT on point clouds due to\na lack of pre-existing vocabulary. In contrast, the language\nvocabulary has been well-deﬁned (e.g., WordPiece in [8])\nand off-the-shelf for model pre-training. In terms of point\ncloud Transformers, there is no pre-deﬁned vocabulary for\npoint clouds. A naive idea is to treat every point as a ‘word’\nand mimic BERT [8] to predict the coordinates of masked\npoints. Such a point-wise regression task surges computa-\ntional cost quadratically as the number of tokens increases.\nMoreover, a word in a sentence contains basic contextual\nsemantic information, while a single point in a point cloud\nbarely entails semantic meaning.\nNevertheless, a local patch partitioned from a holistic\npoint cloud contains plentiful geometric information and\ncan be treated as a component unit. What if we build a\nvocabulary where different tokens represent different geo-\nmetric patterns of the input units? At this point, we can\nrepresent a point cloud as a sequence of such tokens. Now,\nwe can favorably adopt BERT and its efﬁcient implementa-\ntions almost out of the box. We hypothesize that bridging\nthis gap is a key to extending the successful Transformers\nand BERT to the 3D vision domain.\nDriven by the above analysis, we present Point-BERT,\na new scheme for learning point cloud Transformers. Two\nessential components are conceived: 1) Point Tokenization:\nA point cloud Tokenizer is devised via a dV AE-based [39]\npoint cloud reconstruction, where a point cloud can be con-\nverted into discrete point tokens according to the learned\nvocabulary. We expect that point tokens should imply lo-\ncal geometric patterns, and the learned vocabulary should\ncover diverse geometric patterns, such that a sequence of\nsuch tokens can represent any point cloud (even never seen\nbefore). 2) Masked Point Modeling: A ‘masked point mod-\neling’ (MPM) task is performed to pre-train Transformers,\nwhich masks a portion of input point cloud and learns to\nreconstruct the missing point tokens at the masked regions.\nWe hope that our model enables reasoning the geometric re-\nlations among different patches of the point cloud, capturing\nmeaningful geometric features for point cloud understand-\ning.\nBoth two designs are implemented and justiﬁed in our\nexperiments. We visualize the reconstruction results both\non the synthetic (ShapeNet [5]) and real-world (ScanOb-\njectNN [49]) datasets in Figure 2. We observe that Point-\nBERT correctly predicts the masked tokens and infers di-\nverse, holistic reconstructions through our dV AE decoder.\nThe results suggest that the proposed model has learned in-\nherent and generic knowledge of 3D point clouds, i.e, geo-\nmetric patterns or semantics. More signiﬁcantly, our model\nis trained on ShapeNet, the masked point predictions on\nScanObjectNN reﬂect its superior performance on challeng-\ning scenarios with both unseen objects and domain gaps.\nOur Point-BERT with a pure Transformer architecture\nand BERT-style pre-training technique achieves 93.8% ac-\ncuracy on ModelNet40 and 83.1% accuracy on the com-\nplicated setting of ScanObjectNN, surpassing carefully de-\nsigned point cloud models with much fewer human pri-\nors. We also show that the representations learned by\nPoint-BERT transfer well to new tasks and domains, where\nour models largely advance the state-of-the-art of few-shot\npoint cloud classiﬁcation task. We hope a neat and uniﬁed\nTransformer architecture across images and point clouds\ncould facilitate both domains since it enables joint modeling\nof 2D and 3D visual signals.\n2. Related Work\nSelf-supervised Learning (SSL).SSL is a type of unsuper-\nvised learning, where the supervision signals can be gener-\nated from the data itself [15]. The core idea of SSL is to de-\nﬁne a pretext task, such as jigsaw puzzles [31], colorization\n[21], and optical-ﬂow [29] in images. More recently, several\nstudies suggested using SSL techniques for point cloud un-\nderstanding [10,14,22,32,38,40,41,44,52,56,59]. Example\n3D pretext tasks includes orientation estimation [33], de-\nformation reconstruction [1], geometric structural cues [45]\nand spatial cues [30, 42]. Inspired by the jigsaw puzzles\nin images [31], [41] proposes to reconstruct point clouds\nfrom the randomly rearranged parts. A contrastive learning\nframework is proposed by DepthContrast [64] to learn rep-\nresentations from depth scans. More recently, OcCo [52]\ndescribes an encoder-decoder mechanism to reconstruct the\noccluded point clouds. Different from these studies, we at-\ntempt to explore a point cloud SSL model following the suc-\ncessful Transformers [51].\nTransformers. Transformers [51] have become the domi-\nnant framework in NLP [3, 8, 18, 25, 36] due to its salient\nbeneﬁts, including massively parallel computing, long-\ndistance characteristics, and minimal inductive bias. It has\nintrigued various vision tasks [12, 19], such as object clas-\nsiﬁcation [6, 9], detection [4, 68] and segmentation [53, 66].\nNevertheless, its applications on point clouds remain lim-\nited. Some preliminary explorations have been imple-\nmented [11,61,65]. For instance, [65] applies the vectorized\nself-attention mechanism to construct a point Transformer\nlayer for 3D point cloud learning. [11] uses a more typical\nTransformer architecture with neighbor embedding to learn\npoint clouds. Nevertheless, prior efforts for Transformer-\nbased point cloud models more or less involve some in-\nductive biases, making them out of the line with standard\nTransformers. In this work, we seek to continue the suc-\ncess of standard Transformers and extend it to point cloud\nlearning with minimal inductive bias.\nBERT-style Pre-training. The main architecture of BERT\n[8] is built upon a multi-layer Transformer encoder, which is\nﬁrst designed to pre-train bidirectional representations from\nthe unlabeled text in a self-supervised scheme. The primary\ningredient that helps BERT stand out and achieve impres-\nsive performance is the pretext of Masked Language Mod-\neling (MLM), which ﬁrst randomly masks and then recov-\ners a sequence of input tokens. The MLM strategy has also\ninspired a lot of pre-training tasks [2, 7, 18, 25, 48]. Take\nBEiT [2] for example, it ﬁrst tokenizes the input image\ninto discrete visual tokens. After that, it randomly masks\nsome image patches and feeds the corrupted images into\nthe Transformer backbone. The model is trained to recover\nthe visual tokens of the masked patches. More recently,\nMAE [13] presents a masked autoencoder strategy for im-\nage representation learning. It ﬁrst masks random patches\nof the input image and then encourages the model to recon-\nstruct those missing pixels. Our work is greatly inspired\nby BEiT [2], which encodes the image into discrete visual\ntokens so that a Transformer backbone can be directly ap-\nplied to these visual tokens. However, it is more challeng-\ning to acquire tokens for point clouds due to the unstruc-\ntured nature of point clouds, which subsequently hinders\nthe straightforward use of BERT on point clouds.\n3. Point-BERT\nThe overall objective of this work is to extend the BERT-\nstyle pre-training strategy to point cloud Transformers. To\nachieve this goal, we ﬁrst learn a Tokenizer to obtain dis-\ncrete point tokens for each input point cloud. Mimicking\nthe ‘MLM’ strategy in BERT [8], we devise a ‘masked point\nmodeling’ (MPM) task to pre-train Transformers with the\nhelp of those discrete point tokens. The overall idea of our\napproach is illustrated in Figure 3.\n3.1. Point Tokenization\nPoint Embeddings. A naive approach treats per point as\none token. However, such a point-wise reconstruction task\ntends to unbearable computational cost due to the quadratic\ncomplexity of self-attention in Transformers. Inspired by\nthe patch embedding strategy in Vision Transformers [9],\nwe present a simple yet efﬁcient implementation that groups\neach point cloud into several local patches (sub-clouds).\nSpeciﬁcally, given an input point cloud p ∈ RN×3, we\nﬁrst sample g center points from the holistic point cloud\npvia farthest point sampling (FPS). The k-nearest neighbor\n(kNN) algorithm is then used to select the nnearest neigh-\nbor points for each center point, grouping g local patches\n(sub-clouds) {pi}g\ni=1. We then make these local patches un-\nbiased by subtracting their center coordinates, disentangling\nthe structure patterns and spatial coordinates of the local\npatches. These unbiased sub-clouds can be treated as words\nin NLP or image patches in the vision domain. We further\nadopt a mini-PointNet [34] to project those sub-clouds into\npoint embeddings. Following the practice of Transformers\nin NLP and 2D vision tasks, we represent a point cloud as\na sequence of point embeddings {fi}g\ni=1, which can be re-\nFreezeNormalize\nInputPointCloud\nPositionalEmbeddings\n…Point Patches\nTokenizerTokenMasking\nTransformerEncoder\n…\n…\nMPM head…\nMasked TokenCLS Token\n5, 26, 1967, …,49\n…\n…\n5, 26, 1967, 269, 516, …,495, 26, 1967, 269, 516, …,49\nTokenizer\nDecoder\nPoint Embeddings\nContrastive Leanring\nPoint Embeddings[cls]\nInput TokenPred.TokenLocalCenters\nMini-PointNet\nFigure 3. The pipeline of Point-BERT. We ﬁrst partition the input point cloud into several point patches (sub-clouds). A mini-PointNet\n[34] is then used to obtain a sequence of point embeddings. Before pre-training, a Tokenizer is learned through dV AE-based point cloud\nreconstruction (as shown in the right part of the ﬁgure), where a point cloud can be converted into a sequence of discrete point tokens;\nDuring pre-training, we mask some portions of point embeddings and replace them with a mask token. The masked point embeddings are\nthen fed into the Transformers. The model is trained to recover the original point tokens, under the supervision of point tokens obtained by\nthe Tokenizer. We also add an auxiliary contrastive learning task to help the Transformers to capture high-level semantic knowledge.\nceived as inputs to standard Transformers.\nPoint Tokenizer. Point Tokenizer takes point embed-\ndings {fi}g\ni=1 as the inputs and converts them into dis-\ncrete point tokens. Speciﬁcally, the Tokenizer Qφ(z|f)\nmaps point embeddings {fi}g\ni=1 into discrete point tokens\nz = [z1,z2,....,z g] ∈V 1, where Vis the learned vocab-\nulary with total length of N. In this step, the sub-clouds\n{pi}g\ni=1 can be tokenized into point tokens{zi}g\ni=1, relating\nto effective local geometric patterns. In our experiments,\nDGCNN [54] is employed as our Tokenizer network.\nPoint Cloud Reconstruction. The decoder Pϕ(p|z) of\ndV AE receives point tokens{zi}g\ni=1 as the inputs and learns\nto reconstruct the corresponding sub-clouds {pi}g\ni=1. Since\nthe local geometry structure is too complex to be repre-\nsented by the limited N situations. We adopt a DGCNN\n[54] to build the relationship with neighboring point to-\nkens, which can enhance the representation ability of dis-\ncrete point tokens for diverse local structures. After that, a\nFoldingNet [59] is used to reconstruct the sub-clouds.\nThe overall reconstruction objective can be written as\nEz∼Qφ(z|p) [ log Pϕ(p|z)], and the reconstruction proce-\ndure can be viewed as maximizing the evidence lower\nbound (ELB) of the log-likelihood Pθ(p|˜p) [37]:\n∑\n(pi,˜pi)∈D\nlogPθ(pi|˜pi) ≥\n∑\n(pi,˜pi)∈D\n(Ezi∼Qφ(z|pi) [ log Pϕ(pi|zi)]\n−DKL[Qφ(z|pi),Pϕ(z|˜pi)]), (1)\nwhere pdenotes the original point cloud, ˜pdenotes the re-\nconstructed point cloud. Since the latent point tokens are\n1Point tokens have two forms, discrete integer number and correspond-\ning word embedding in V, which are equivalent.\ndiscrete, we cannot apply the reparameterization gradient\nto train the dV AE. Following [37], we use the Gumbel-\nsoftmax relaxation [17] and a uniform prior during dV AE\ntraining. Details about dV AE architecture and its imple-\nmentation can be found in the supplementary.\n3.2. Transformer Backbone\nWe adopt the standard Transformers [51] in our experi-\nments, consisting of multi-headed self-attention layers and\nFFN blocks. For each input point cloud, we ﬁrst divide it\ninto glocal patches with center points {ci}g\ni=1. Those local\npatches are then projected into point embeddings {fi}g\ni=1\nvia a mini-PointNet [34], which consists of only MLP layers\nand the global maxpool operation. We further obtain the\npositional embeddings {posi}of each patch by applying an\nMLP on its center point {ci}. Formally, we deﬁne the input\nembeddings as {xi}g\ni=1, which is the combination of point\nembeddings {fi}g\ni=1 and positional embeddings {posi}g\ni=1.\nThen, we send the input embeddings {xi}g\ni=1 into the\nTransformer. Following [8], we append a class token E[s]\nto the input sequences. Thus, the input sequence of Trans-\nformer can be expressed as H0 = {E[s],x1,x2,··· ,xg}.\nThere are L layers of Transformer block, and the output\nof the last layer HL =\n{\nhL\ns ,hL\n1 ,··· ,hL\ng\n}\nrepresents the\nglobal feature, along with the encoded representation of the\ninput sub-clouds.\n3.3. Masked Point Modeling\nMotivated by BERT [8] and BEiT [2], we extend the\nmasked modeling strategy to point cloud learning and de-\nvise a masked point modeling (MPM) task for Point-BERT.\nMasked Sequence Generation. Different from the ran-\ndom masking used in BERT [8] and MAE [13], we adopt\na block-wise masking strategy like [2]. Speciﬁcally, we\nﬁrst choose a center point ci along with its sub-cloud pi,\nand then ﬁnd its mneighbor sub-clouds, forming a contin-\nuous local region. We mask out all local patches in this\nregion to generate the masked point cloud. In practice, we\ndirectly apply such a block-wise masking strategy like [2]\nto the inputs of the Transformer. Formally, we mark the\nmasked positions as M∈{ 1,··· ,g}⌊rg⌋, where r is the\nmask ratio. Next, we replace all the masked point em-\nbeddings with a same learnable pre-deﬁned mask embed-\ndings E[M] while keeping its positional embeddings un-\nchanged. Finally, the corrupted input embeddings X M =\n{xi : i /∈M}g\ni=1 ∪{E[M] +posi : i∈M}g\ni=1 are fed into\nthe Transformer encoder.\nPretext Task Deﬁnition. The goal of our MPM task is to\nenable the model to infer the geometric structure of missing\nparts based on the remaining ones. The pre-trained dV AE\n(see section 3.1) encodes each local patch into discrete point\ntokens, representing the geometric patterns. Thus, we can\ndirectly apply those informative tokens as our surrogate su-\npervision signal to pre-train the Transformer.\nPoint Patch Mixing. Inspired by the CutMix [62,63] tech-\nnique, we additionally devise a neat mixed token prediction\ntask as an auxiliary pretext task to increase the difﬁculty\nof pre-training in our Point-BERT, termed as ‘Point Patch\nMixing’. Since the information of the absolute position of\neach sub-cloud has been excluded by normalization, we can\ncreate new virtual samples by simply mixing two groups of\nsub-clouds without any cumbersome alignment techniques\nbetween different patches, such as optimal transport [63].\nDuring pre-training, we also force the virtual sample to pre-\ndict the corresponding tokens generated by the original sub-\ncloud to perform the MPM task. In our implementation,\nwe generate the same number of virtual samples as the real\nones to make the pre-training task more challenging, which\nis helpful to improve the training of Transformers with lim-\nited data as observed in [47].\nOptimization Objective. The goal of MPM task is to re-\ncover the point tokens that are corresponding to the masked\nlocations. The pre-training objective can be formalized as\nmaximizing the log-likelihood of the correct point tokenszi\ngiven the masked input embeddings XM:\nmax\n∑\nX∈D\nEM\n[∑\ni∈M\nlogP\n(\nzi|XM\n)]\n. (2)\nMPM task encourages the model to predict the masked ge-\nometric structure of the point clouds. Training the Trans-\nformer only with MPM task leads to an unsatisfactory un-\nderstanding on high-level semantics of the point clouds,\nwhich is also pointed out by the recent work in 2D do-\nmain [67]. So we adopt the widely used contrastive learning\nmethod MoCo [14] as a tool to help the Transformers to bet-\nter learn high-level semantics. With our point patch mixing\ntechnique, the optimization of contrastive loss encourages\nthe model to pay attention to the high-level semantics of\npoint clouds by making features of the virtual samples as\nclosely as possible to the corresponding features from the\noriginal samples. Let q be the feature of a mixed sample\nthat comes from two other samples, whose features are k+\n1\nand k+\n2 ({ki}are extracted by the momentum feature en-\ncoder [14]). Assuming the mixing ratio is r, the contrastive\nloss can be written as:\nLq = −rlog exp(qk+\n1 /τ)∑K\ni=0 exp(qki/τ)\n−(1−r)log exp(qk+\n2 /τ)∑K\ni=0 exp(qki/τ)\n),(3)\nwhere τ is the temperature and K is the size of memory\nbank. Coupling MPM objective and contrastive loss enables\nour Point-BERT to simultaneously capture the local geo-\nmetric structures and high-level semantic patterns, which\nare crucial in point cloud representation learning.\n4. Experiments\nIn this section, we ﬁrst introduce the setups of our pre-\ntraining scheme. Then we evaluate the proposed model with\nvarious downstream tasks, including object classiﬁcation,\npart segmentation, few-shot learning and transfer learning.\nWe also conduct an ablation study for our Point-BERT.\n4.1. Pre-training Setups\nData Setups. ShapeNet [5] is used as our pre-training\ndataset, which covers over 50,000 unique 3D models from\n55 common object categories. We sample 1024 points from\neach 3D model and divide them into 64 point patches (sub-\nclouds). Each sub-cloud contains 32 points. A lightweight\nPointNet [34] containing two-layer MLPs is adopted to\nproject each sub-cloud into 64 point embeddings, which are\nused as input both for dV AE and Transformer.\ndV AE Setups. We use a four-layer DGCNN [54] to learn\nthe inter-patch relationships, modeling the internal struc-\ntures of input point clouds. During dV AE training, we\nset the vocabulary size N to 8192. Our decoder is also a\nDGCNN architecture followed by a FoldingNet [59]. It is\nworth noting that the performance of dV AE is susceptible\nto hyper-parameters, which makes that the conﬁgurations\nof image-based dV AE [37] cannot be directly used in our\nscenarios. The commonly used ℓ1-style Chamfer Distance\nloss is employed during the reconstruction procedure. Since\nthe value of this ℓ1 loss is numerically small, the weight\nof KLD loss in Eq.1 must be smaller than that in the im-\nage tasks. We set the weight of KLD loss to 0 in the ﬁrst\n10,000 steps and gradually increased to 0.1 in the following\n100,000 steps. The learning rate is set to 0.0005 with a co-\nsine learning schedule with 60,000 steps warming up. We\ndecay the temperature in Gumble-softmax function from 1\nTable 1. Comparisons of Point-BERT with of state-of-the-art\nmodels on ModelNet40. We report the classiﬁcation accuracy\n(%) and the number of points in the input. [ST] and [T] represent\nthe standard Transformers models and Transformer-based models\nwith some special designs and more inductive biases, respectively.\nMethod #point Acc.\nPointNet [34] 1k 89.2\nPointNet++ [35] 1k 90.5\nSO-Net [22] 1k 92.5\nPointCNN [23] 1k 92.2\nDGCNN [54] 1k 92.9\nDensePoint [24] 1k 92.8\nRSCNN [38] 1k 92.9\nKPConv [46] ∼6.8k 92.9\n[T] PCT [11] 1k 93.2\n[T] PointTransformer [65] – 93.7\n[ST] NPCT [11] 1k 91.0\n[ST] Transformer 1k 91.4\n[ST] Transformer + OcCo [52] 1k 92.1\n[ST] Point-BERT 1k 93.2\n[ST] Transformer 4k 91.2\n[ST] Transformer + OcCo [52] 4k 92.2\n[ST] Point-BERT 4k 93.4\n[ST] Point-BERT 8k 93.8\nto 0.0625 in 100,000 steps following [37]. We train dV AE\nfor a total of 150,000 steps with a batch size of 64.\nMPM Setups. In our experiments, we set the depth for\nthe Transformer to 12, the feature dimension to 384, and\nthe number of heads to 6. The stochastic depth [16] with\na 0.1 rate is applied in our transformer encoder. During\nMPM pre-training, we ﬁx the weights of Tokenizer learned\nby dV AE. 25% ∼45% input point embeddings are ran-\ndomly masked out. The model is then trained to infer the\nexpected point tokens at those masked locations. In terms\nof MoCo, we set the memory bank size to 16,384, temper-\nature to 0.07, and weight momentum to 0.999. We employ\nan AdamW [27] optimizer, using an initial learning rate of\n0.0005 and a weight decay of 0.05. The model is trained for\n300 epochs with a batch size of 128.\n4.2. Downstream Tasks\nIn this subsection, we report the experimental results on\ndownstream tasks. Besides the widely used benchmarks,\nincluding classiﬁcation and segmentation, we also study the\nmodel’s capacity on few-shot learning and transfer learning.\nObject Classiﬁcation. We conduct classiﬁcation experi-\nments on ModelNet40 [55], In the classiﬁcation task, a two-\nlayer MLP with a dropout of 0.5 is used as our classiﬁcation\nhead. We use AdamW with a weight decay of 0.05 and a\nlearning rate of 0.0005 under a cosine schedule to optimize\nthe model. The batch size is set to 32.\nThe results are presented in Table 1. We denote our\nTable 2. Few-shot classiﬁcation results on ModelNet40. We\nreport the average accuracy (%) as well as the standard deviation\nover 10 independent experiments.\n5-way 10-way\n10-shot 20-shot 10-shot 20-shot\nDGCNN-rand [52] 31.6 ±2.8 40.8 ±4.6 19.9 ±2.1 16.9 ±1.5\nDGCNN-OcCo [52] 90.6±2.8 92.5 ±1.9 82.9 ±1.3 86.5 ±2.2\nDGCNN-rand∗ 91.8±3.7 93.4 ±3.2 86.3 ±6.2 90.9 ±5.1\nDGCNN-OcCo∗ 91.9±3.3 93.9 ±3.1 86.4 ±5.4 91.3 ±4.6\nTransformer-rand 87.8 ±5.2 93.3 ±4.3 84.6 ±5.5 89.4 ±6.3\nTransformer-OcCo 94.0±3.6 95.9 ±2.3 89.4 ±5.1 92.4 ±4.6\nPoint-BERT 94.6±3.1 96.3±2.7 91.0±5.4 92.7±5.1\nbaseline model as ‘Transformer’, which is trained on Mod-\nelNet40 with random initialization. Several Transformer-\nbased models are illustrated, where [ST] represents a stan-\ndard Transformer architecture, and [T] denotes the Trans-\nformer model with some special designs or inductive bi-\nases. Although we mainly focus on pre-training for stan-\ndard Transformers in this work, our MPM pre-training strat-\negy is also suitable for other Transformer-based point cloud\nmodels [11, 65]. Additionally, we compare with a recent\npre-training strategy OcCo [52] as a strong baseline of our\npre-training method. For fair comparisons, we follow the\ndetails illustrated in [52] and use the Transfomer-based de-\ncoder PoinTr [61] to perform their pretext task. Combin-\ning our Transformer encoder and PoinTr’s decoder, we con-\nduct the completion task on ShapeNet, following the idea of\nOcCo. We term this model as ‘Transformer+OcCo’.\nWe see pre-training Transformer with OcCo improves\n0.7%/1.0% over the baseline using 1024/4096 inputs. In\ncomparison, our Point-BERT brings 1.8%/2.2% gains over\nthat of training from scratch. We also observe that adding\nmore points will not signiﬁcantly improve the Transformer\nmodel without pre-training while Point-BERT models can\nbe consistently improved by increasing the number of\npoints. When we increase the density of inputs (4096),\nour Point-BERT achieves signiﬁcantly better performance\n(93.4%) than that with the baseline (91.2%) and OcCo\n(92.2%). Given more input points (8192), our method can\nbe further boosted to 93.8% accuracy on ModelNet40.\nFew-shot Learning. We follow previous work [42] to eval-\nuate our model under the few-shot learning setting. A typ-\nical setting is “ K-way N-shot”, where K classes are ﬁrst\nrandomly selected, and then ( N+20) objects are sampled\nfor each class [42]. The model is trained on K ×N sam-\nples (support set), and evaluated on the remaining 20 K\nsamples (query set). We compare Point-BERT with OcCo\n[52], which achieves state-of-the-art performance on this\ntask. In our experiments, we test the performance un-\nder “5way 10shot”, “5way 20shot”, “10way 10shot” and\n“10way 20shot”. We conduct 10 independent experiments\nunder each setting and report the average performance as\nTable 3. Part segmentation results on the ShapeNetPart dataset. We report the mean IoU across all part categories mIoUC (%) and the\nmean IoU across all instance mIoUI (%) , as well as the IoU (%) for each categories.\nMethods mIoUC mIoUI aero bag cap car chair earphone guitar knife lamp laptop motor mug pistol rocket skateboard table\nPointNet [34] 80.39 83.7 83.4 78.7 82.5 74.9 89.6 73.0 91.5 85.9 80.8 95.3 65.2 93 81.2 57.9 72.8 80.6\nPointNet++ [35] 81.85 85.1 82.4 79 87.7 77.3 90.8 71.8 91 85.9 83.7 95.3 71.6 94.1 81.3 58.7 76.4 82.6\nDGCNN [54] 82.33 85.2 84 83.4 86.7 77.8 90.6 74.7 91.2 87.5 82.8 95.7 66.3 94.9 81.1 63.5 74.5 82.6\nTransformer 83.42 85.1 82.9 85.4 87.7 78.8 90.5 80.8 91.1 87.7 85.3 95.6 73.9 94.9 83.5 61.2 74.9 80.6\nTransformer-OcCo83.42 85.1 83.3 85.2 88.3 79.9 90.7 74.1 91.9 87.6 84.7 95.4 75.5 94.4 84.1 63.1 75.7 80.8\nPoint-BERT 84.11 85.6 84.3 84.8 88.0 79.8 91.0 81.7 91.6 87.9 85.2 95.6 75.6 94.7 84.3 63.4 76.3 81.5\nTable 4. Classiﬁcation results on the ScanObjectNN dataset.\nWe report the accuracy (%) of three different settings.\nMethods OBJ-BG OBJ-ONLY PB-T50-RS\nPointNet [34] 73.3 79.2 68.0\nSpiderCNN [58] 77.1 79.5 73.7\nPointNet++ [35] 82.3 84.3 77.9\nPointCNN [23] 86.1 85.5 78.5\nDGCNN [54] 82.8 86.2 78.1\nBGA-DGCNN [49] – – 79.7\nBGA-PN++ [49] – – 80.2\nTransformer 79.86 80.55 77.24\nTransformer-OcCo 84.85 85.54 78.79\nPoint-BERT 87.43 88.12 83.07\nwell as the standard deviation over the 10 runs. We also re-\nproduce DGCNN-rand and DGCNN-OcCo under the same\ncondition for a fair comparison.\nAs shown in the Table 2, Point-BERT achieves the best\nin the few-shot learning. It obtains an absolute improvement\nof 6.8%, 3.0%, 6.4%, 3.3% over the baseline and 0.6%,\n0.4%, 1.6%, 0.3% over the OcCo-based method on the four\nsettings. The strong results indicate that Point-BERT learns\nmore generic knowledge that can be quickly transferred to\nnew tasks with limited data.\nPart Segmentation. Object part segmentation is a chal-\nlenging task aiming to predict a more ﬁne-grained class la-\nbel for every point. We evaluate the effectiveness of Point-\nBERT on ShapeNetPart [60], which contains 16,881 mod-\nels from 16 categories. Following PointNet [34], we sample\n2048 points from each model and increase the group num-\nber gfrom 64 to 128 in the segmentation tasks. We design\na segmentation head to propagate the group features to each\npoint hierarchically. Speciﬁcally, features from 4th, 8th\nand the last layer of Transformer are selected, denoted as\n{H4 = {h4\ni}g\ni=1,H8 = {h8\ni}g\ni=1,H12 = {h12\ni }g\ni=1}. Then\nwe downsample the origin point cloud to 512 and 256 points\nvia FPS, phrased as P4 = {p4\ni}512\ni=1 and P8 = {p8\ni}256\ni=1. We\nfollow PointNet++ [35] to perform feature propagation be-\ntween H4 and P4, H8 and P8. Here, we can obtain the\nupsampled feature map ˆH4 and ˆH8, which represent the\nfeatures for the points in P4 and P8. Then, we can propa-\ngate the feature from H12 to ˆH4 and ﬁnally to every point.\nTable 5. Ablation study. We investigate the effects of different\ndesigns and report the classiﬁcation accuracy (%) after ﬁne-tuning\non ModelNet40. All models are trained with 1024 points.\nPretext tasks MPM Point Patch Mixing Moco Acc.\nModel A 91.41\nModel B ✓ 92.58↑\nModel C ✓ ✓ 92.91↑\nModel D ✓ ✓ ✓ 93.24↑\nAugmentation mask type mask ratio replace Acc.\nModel B block mask [0.25, 0.45] No 92.58\nModel B block mask [0.25, 0.45] Yes 91.81↓\nModel B rand mask [0.25, 0.45] No 92.34↓\nModel B block mask [0.55, 0.85] No 92.52↓\nModel D block mask [0.25, 0.45] No 93.16\nModel D block mask [0.25, 0.45] Yes 92.58↓\nModel D rand mask [0.25, 0.45] No 92.91↓\nModel D block mask [0.55, 0.85] No 92.59↓\nTwo types of mIoU are reported in Table 3. It is clear\nthat our Point-BERT outperforms PointNet, PointNet++,\nand DGCNN. Moreover, Point-BERT improves 0.69% and\n0.5% mIoU over vanilla Transformers, while OcCo fails to\nimprove baseline performance in part segmentation task.\nTransfer to Real-World Dataset.We evaluate the general-\nization ability of the learned representation by pre-training\nthe model on ShapeNet and ﬁne-tuning it on ScanOb-\njectNN [49], which contains 2902 point clouds from 15 cat-\negories. It is a more challenging dataset sampled from real-\nworld scans containing background and occlusions. We fol-\nlow previous works to conduct experiments on three main\nvariants: OBJ-BG, OBJ-ONLY , and PB-T50-RS. The ex-\nperimental results are reported in Table 4. As we can see,\nPoint-BERT improves the vanilla Transformers by about\n7.57%, 7.57%, and 5.83% on three variants.\nComparing the classiﬁcation results on ModelNet40\n(Table 1) and ScanObjectNN (Table 2), we observe that\nDGCNN outperforms PointNet++ (+2.4%) on the Model-\nNet40. While the superiority is degraded on the real-world\ndataset ScanObjectNN. As for Point-BERT, it achieves\nSOTA performance on both datasets, which strongly con-\nﬁrms the effectiveness of our method.\nFigure 4. Visualization of feature distributions. We show the\nt-SNE visualization of feature vectors learned by Point-BERT (a)\nafter pre-training, (b) after ﬁne-tuning on ModelNet40, and (c) af-\nter ﬁne-tuning on ScanObjectNN.\n4.3. Ablation Study\nPretext Task. We denote model A as our baseline, which\nis the Transformer training from scratch. Model B presents\npre-training Transformer with MPM pretext task. Model C\nis trained with more samples coming from ‘point patch mix-\ning’ technique. Model D (the proposed method) is trained\nunder the setting of MPM, point patch mixing, and MoCo.\nAs can be seen in the upper part of Table 5, Model B with\nMPM improves the performance about 1.17% . By adopting\npoint patch mixing strategy, Model C gets an improvement\nof 0.33%. With the help of MoCo [14], Model D further\nbrings an improvement of 0.33%.\nMasking Strategy. We visualize the point token prediction\ntask in Figure 2. Our Transformer encoder can reasonably\ninfer the point tokens of the missing patches. In practice, we\nreconstruct the local patches through the decoder of dV AE,\nbased on the point tokens predicted by the Transformer en-\ncoder. Two masking strategies are explored: block-wise\nmasking (block-mask) and random masking (rand-mask).\nThe masking strategy determines the difﬁculty of the pre-\ntext task, inﬂuencing reconstruction quality and representa-\ntions. We further investigate the effects of different mask-\ning strategies and provide the results in Table 5. We see\nthat Model D with block-mask works better at the ratio of\n25% ∼45%. Unlike images, which can be split into reg-\nular non-overlapping patches, sub-clouds partitioned from\nthe original point cloud often involve overlaps. Thus, rand-\nmask makes the task easier than block-mask, and further\ndegrades the reconstruction performance. We also consider\nanother type of augmentations: randomly replace some in-\nput embeddings with those from other samples.\n4.4. Visualization\nWe visualize the learned features of two datasets via t-\nSNE [50] in Figure 4. In ﬁgure (a) and (b), the visualized\nfeatures are from our Point-BERT (a) before ﬁne-tuning\nand (b) after ﬁne-tuning on ModelNet40. As can be seen,\nfeatures from different categories can be well separated by\nour method even before ﬁne-tuning. We also visualize the\nfeature maps on the PB-T50-RS of ScanObjectNN in (c).\nFigure 5. Learning curve. We compare the performance of Trans-\nformers training from scratch (blue) and pre-training with Point-\nBERT (red) in terms of training loss and validation accuracy on\nsynthetic and real-world object classiﬁcation datasets.\nWe can see that separate clusters are formed for each cat-\negory, indicating the transferability of learned representa-\ntion to real-world scenarios. It further veriﬁes that Point-\nBERT helps the Transformer to learn generic knowledge\nfor 3D objects. We also visualize the learning curves of\nour baseline Transformers and the proposed Point-BERT in\nFigure 5. As can be seen, pre-training with our Point-BERT\nsigniﬁcantly improves the performance of baseline Trans-\nformers both in accuracy and speed on both synthetic and\nreal-world datasets.\n5. Conclusion and Discussions\nWe present a new paradigm for 3D point cloud Trans-\nformers through a BERT-style pre-training to learn both\nlow-level structural information and high-level semantic\nfeature. We observe a signiﬁcant improvement for the\nTransformer on learning and generalization by comprehen-\nsive experiments on several 3D point cloud tasks. We show\nthe potential of standard Transformers in 3D scenarios with\nappropriate pre-training strategy and look forward to further\nstudy on standard Transformers in the 3D domain.\nWe do not foresee any negative ethical/societal impacts\nat this moment. Although the proposed method can effec-\ntively improve the performance of standard Transformers\non point clouds, the entire ‘pre-training + ﬁne-tuning’ pro-\ncedure is rather time-consuming, like other Transformers\npre-training methods [2, 8, 13]. Improving the efﬁciency of\nthe training process will be an interesting future direction.\nAcknowledgements\nThis work was supported in part by the National Key\nResearch and Development Program of China under Grant\n2017YFA0700802, in part by the National Natural Sci-\nence Foundation of China under Grant 62152603, Grant\nU1813218, in part by a grant from the Beijing Academy of\nArtiﬁcial Intelligence (BAAI), and in part by a grant from\nthe Institute for Guo Qiang, Tsinghua University.\nReferences\n[1] Idan Achituve, Haggai Maron, and Gal Chechik. Self-\nsupervised learning for domain adaptation on point clouds.\nIn WACV, 2021. 3\n[2] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training\nof image transformers. arXiv preprint arXiv:2106.08254 ,\n2021. 1, 3, 4, 5, 8\n[3] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020. 1, 3\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV, 2020. 3\n[5] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,\nPat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,\nManolis Savva, Shuran Song, Hao Su, et al. Shapenet:\nAn information-rich 3d model repository. arXiv preprint\narXiv:1512.03012, 2015. 2, 5, 11\n[6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-\nwoo Jun, David Luan, and Ilya Sutskever. Generative pre-\ntraining from pixels. In ICML, 2020. 1, 3\n[7] Alexis Conneau and Guillaume Lample. Cross-lingual lan-\nguage model pretraining. NeurIPS, 2019. 3\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018. 1, 2, 3, 4, 5, 8\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 1, 3, 11\n[10] Benjamin Eckart, Wentao Yuan, Chao Liu, and Jan Kautz.\nSelf-supervised learning on 3d point clouds by learning dis-\ncrete generative models. In CVPR, 2021. 3\n[11] Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang\nMu, Ralph R Martin, and Shi-Min Hu. Pct: Point cloud\ntransformer. Computational Visual Media, 2021. 1, 3, 6\n[12] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen,\nJianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chun-\njing Xu, Yixing Xu, et al. A survey on visual transformer.\narXiv preprint arXiv:2012.12556, 2020. 3\n[13] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll/acute.ts1ar, and Ross Girshick. Masked autoencoders are scal-\nable vision learners. arXiv preprint arXiv:2111.06377, 2021.\n3, 5, 8\n[14] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. In CVPR, 2020. 3, 5, 8\n[15] G Hinton, Y LeCunn, and Y Bengio. Aaai’2020 keynotes\nturing award winners event, 2020. 3\n[16] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q\nWeinberger. Deep networks with stochastic depth. InECCV,\npages 646–661. Springer, 2016. 6\n[17] Eric Jang, Shixiang Gu, and Ben Poole. Categorical\nreparameterization with gumbel-softmax. arXiv preprint\narXiv:1611.01144, 2016. 4\n[18] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke\nZettlemoyer, and Omer Levy. Spanbert: Improving pre-\ntraining by representing and predicting spans. TACL, 2020.\n1, 3\n[19] Salman Khan, Muzammal Naseer, Munawar Hayat,\nSyed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak\nShah. Transformers in vision: A survey. arXiv preprint\narXiv:2101.01169, 2021. 3\n[20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classiﬁcation with deep convolutional neural net-\nworks. NeurIPS, 2012. 1\n[21] Gustav Larsson, Michael Maire, and Gregory\nShakhnarovich. Learning representations for automatic\ncolorization. In ECCV, 2016. 3\n[22] Jiaxin Li, Ben M Chen, and Gim Hee Lee. So-net: Self-\norganizing network for point cloud analysis. InCVPR, 2018.\n3, 6\n[23] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di,\nand Baoquan Chen. Pointcnn: Convolution on x-transformed\npoints. NeurIPS, 2018. 6, 7\n[24] Yongcheng Liu, Bin Fan, Gaofeng Meng, Jiwen Lu, Shiming\nXiang, and Chunhong Pan. Densepoint: Learning densely\ncontextual representation for efﬁcient point cloud process-\ning. In ICCV, 2019. 6\n[25] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-\nmoyer, and Veselin Stoyanov. Roberta: A robustly optimized\nbert pretraining approach. arXiv preprint arXiv:1907.11692,\n2019. 1, 3\n[26] Ilya Loshchilov and Frank Hutter. Sgdr: Stochas-\ntic gradient descent with warm restarts. arXiv preprint\narXiv:1608.03983, 2016. 11\n[27] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In ICLR, 2018. 6\n[28] Ilya Loshchilov and Frank Hutter. Fixing weight decay reg-\nularization in adam. 2018. 11\n[29] Aravindh Mahendran, James Thewlis, and Andrea Vedaldi.\nCross pixel optical-ﬂow similarity for self-supervised learn-\ning. In ACCV, 2018. 3\n[30] Benedikt Mersch, Xieyuanli Chen, Jens Behley, and Cyrill\nStachniss. Self-supervised point cloud prediction using\n3d spatio-temporal convolutional networks. arXiv preprint\narXiv:2110.04076, 2021. 3\n[31] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of\nvisual representations by solving jigsaw puzzles. In ECCV,\n2016. 3\n[32] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-\nsentation learning with contrastive predictive coding. arXiv\npreprint arXiv:1807.03748, 2018. 3\n[33] Omid Poursaeed, Tianxing Jiang, Han Qiao, Nayun Xu, and\nVladimir G Kim. Self-supervised learning of point clouds\nvia orientation estimation. In 3DV. IEEE, 2020. 3\n[34] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.\nPointnet: Deep learning on point sets for 3d classiﬁcation\nand segmentation. In CVPR, 2017. 3, 4, 5, 6, 7\n[35] Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Point-\nnet++ deep hierarchical feature learning on point sets in a\nmetric space. In NeurIPS, 2017. 6, 7\n[36] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, Ilya Sutskever, et al. Language models are unsu-\npervised multitask learners. OpenAI blog, 2019. 1, 3\n[37] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott\nGray, Chelsea V oss, Alec Radford, Mark Chen, and Ilya\nSutskever. Zero-shot text-to-image generation. arXiv\npreprint arXiv:2102.12092, 2021. 4, 5, 6, 11\n[38] Yongming Rao, Jiwen Lu, and Jie Zhou. Global-local bidi-\nrectional reasoning for unsupervised representation learning\nof 3d point clouds. In CVPR, 2020. 3, 6\n[39] Jason Tyler Rolfe. Discrete variational autoencoders. In\nICLR, 2017. 2\n[40] Aditya Sanghi. Info3d: Representation learning on 3d ob-\njects using mutual information maximization and contrastive\nlearning. In ECCV, 2020. 3\n[41] Jonathan Sauder and Bjarne Sievers. Self-supervised deep\nlearning on point clouds by reconstructing space. NeurIPS,\n2019. 3\n[42] Charu Sharma and Manohar Kaul. Self-supervised few-shot\nlearning on point clouds. NeurIPS, 2020. 3, 6\n[43] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi-\nnav Gupta. Revisiting unreasonable effectiveness of data in\ndeep learning era. In ICCV, 2017. 1\n[44] Chao Sun, Zhedong Zheng, Xiaohan Wang, Mingliang Xu,\nand Yi Yang. Point cloud pre-training by mixing and disen-\ntangling. arXiv preprint arXiv:2109.00452, 2021. 3\n[45] Ali Thabet, Humam Alwassel, and Bernard Ghanem. Self-\nsupervised learning of local features in 3d point clouds. In\nCVPRW, 2020. 3\n[46] Hugues Thomas, Charles R. Qi, Jean-Emmanuel Deschaud,\nBeatriz Marcotegui, Franc ¸ois Goulette, and Leonidas J.\nGuibas. Kpconv: Flexible and deformable convolution for\npoint clouds. ICCV, 2019. 6\n[47] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J´egou. Training\ndata-efﬁcient image transformers & distillation through at-\ntention. In ICML, 2021. 1, 5, 12\n[48] Trieu H Trinh, Minh-Thang Luong, and Quoc V Le. Selﬁe:\nSelf-supervised pretraining for image embedding. arXiv\npreprint arXiv:1906.02940, 2019. 3\n[49] Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua,\nDuc Thanh Nguyen, and Sai-Kit Yeung. Revisiting point\ncloud classiﬁcation: A new benchmark dataset and classiﬁ-\ncation model on real-world data. In ICCV, 2019. 2, 7\n[50] Laurens Van der Maaten and Geoffrey Hinton. Visualiz-\ning data using t-sne. Journal of machine learning research,\n2008. 8\n[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 1,\n3, 4, 11, 12\n[52] Hanchen Wang, Qi Liu, Xiangyu Yue, Joan Lasenby, and\nMatt J Kusner. Unsupervised point cloud pre-training via\nocclusion completion. In ICCV, 2021. 3, 6\n[53] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and\nLiang-Chieh Chen. Max-deeplab: End-to-end panoptic seg-\nmentation with mask transformers. In CVPR, 2021. 3\n[54] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma,\nMichael M Bronstein, and Justin M Solomon. Dynamic\ngraph cnn for learning on point clouds. TOG, 2019. 4, 5,\n6, 7, 11, 13\n[55] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-\nguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d\nshapenets: A deep representation for volumetric shapes. In\nCVPR, 2015. 6\n[56] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas\nGuibas, and Or Litany. Pointcontrast: Unsupervised pre-\ntraining for 3d point cloud understanding. In ECCV, 2020.\n3\n[57] Zhenda Xie, Yutong Lin, Zhuliang Yao, Zheng Zhang, Qi\nDai, Yue Cao, and Han Hu. Self-supervised learning with\nswin transformers. arXiv preprint arXiv:2105.04553, 2021.\n1\n[58] Yifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, and Yu Qiao.\nSpidercnn: Deep learning on point sets with parameterized\nconvolutional ﬁlters. In ECCV, 2018. 7\n[59] Yaoqing Yang, Chen Feng, Yiru Shen, and Dong Tian. Fold-\ningnet: Point cloud auto-encoder via deep grid deformation.\nIn CVPR, 2018. 3, 4, 5, 11\n[60] Li Yi, Vladimir G Kim, Duygu Ceylan, I-Chao Shen,\nMengyan Yan, Hao Su, Cewu Lu, Qixing Huang, Alla Shef-\nfer, and Leonidas Guibas. A scalable active framework for\nregion annotation in 3d shape collections. ToG, 35(6):1–12,\n2016. 7\n[61] Xumin Yu, Yongming Rao, Ziyi Wang, Zuyan Liu, Jiwen Lu,\nand Jie Zhou. Pointr: Diverse point cloud completion with\ngeometry-aware transformers. In ICCV, 2021. 3, 6, 11\n[62] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-\nlarization strategy to train strong classiﬁers with localizable\nfeatures. In ICCV, 2019. 5\n[63] Jinlai Zhang, Lyujie Chen, Bo Ouyang, Binbin Liu, Jihong\nZhu, Yujing Chen, Yanmei Meng, and Danfeng Wu. Point-\ncutmix: Regularization strategy for point cloud classiﬁca-\ntion. arXiv preprint arXiv:2101.01461, 2021. 5\n[64] Zaiwei Zhang, Rohit Girdhar, Armand Joulin, and Ishan\nMisra. Self-supervised pretraining of 3d features on any\npoint-cloud. arXiv preprint arXiv:2101.02691, 2021. 3\n[65] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and\nVladlen Koltun. Point transformer. In ICCV, 2021. 1, 3, 6\n[66] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao\nXiang, Philip HS Torr, et al. Rethinking semantic segmen-\ntation from a sequence-to-sequence perspective with trans-\nformers. In CVPR, 2021. 3\n[67] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Ci-\nhang Xie, Alan Yuille, and Tao Kong. Ibot: Image\nbert pre-training with online tokenizer. arXiv preprint\narXiv:2111.07832, 2021. 5\n[68] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. Deformable detr: Deformable transformers\nfor end-to-end object detection. In ICLR, 2020. 1, 3\nAppendix: Implementation Details\nA. Discrete V AE\nArchitecture: Our dV AE consists of a tokenizer and a\ndecoder. Speciﬁcally, the tokenizer contains a 4-layer\nDGCNN [54], and the decoder involves a 4-layer DGCNN\nfollowed by a FoldingNet [59]. The detailed network ar-\nchitecture of our dV AE is illustrated in Table 6, where\nCin and Cout are the dimension of input and output fea-\ntures, Cmiddle is the dimension of the hidden layers. Nout\nis the number of point patches in each layer, and K is\nthe number of neighbors in kNN operation. Additionally,\nFoldingLayer concatenates a 2D grids to the inputs and\nﬁnally generates 3D point clouds.\nTable 6. Detailed architecture of our models. Cin/Cout rep-\nresents the dimension of input/output features, and Nout is the\nnumber of points in the query point cloud. K is the number of\nneighbors in kNN operation. Cmiddle is the dimension of the hid-\nden layers for MLPs.\nModule Block Cin Cout K Nout Cmiddle\nLinear 256 128\nDGCNN 128 256 4 64\ndV AE Tokenizer DGCNN 256 512 4 64\nDGCNN 512 512 4 64\nDGCNN 512 1024 4 64\nLinear 2304 8192\nLinear 256 128\nDGCNN 128 256 4 64\nDGCNN 256 512 4 64\ndV AE Decoder DGCNN 512 512 4 64\nDGCNN 512 1024 4 64\nLinear 2304 256\nMLP 256 48 1024\nFoldingLayer256 3 1024\nClassiﬁcation Head MLP 768 Ncls 256\nMLP 387 384 384×4\nDGCNN 384 512 4 128\nDGCNN 512 384 4 128\nDGCNN 384 512 4 256\nSegmentation Head DGCNN 512 384 4 256\nDGCNN 384 512 4 512\nDGCNN 512 384 4 512\nDGCNN 384 512 4 2048\nDGCNN 512 384 4 2048\nOptimization: During the training phase, we consider re-\nconstruction loss and distribution loss simultaneously. For\nreconstruction, we follow PoinTr [61] to supervise both\ncoarse-grained prediction and ﬁne-grained prediction with\nthe ground-truth point cloud. The ℓ1-form Chamfer Dis-\ntance is adopted, which is calculated as:\ndℓ1\nCD(P,G) = 1\n|P|\n∑\np∈P\nmin\ng∈G\n∥p−g∥+ 1\n|G|\n∑\ng∈G\nmin\np∈P\n∥g−p∥, (1)\nwhere Prepresents the prediction point set andGrepresents\nthe ground-truth point set. Except for the reconstruction\nTable 7. Experiment setting for training the dV AE.\nconﬁg value\noptimizer AdamW [28]\nlearning rate 5e-4\nweight decay 5e-4\nlearning rate schedule cosine [26]\nwarmingup epochs 10\naugmentation RandSampling\nbatch size 64\nnumber of points 1024\nnumber of patches 64\npatch size 32\ntraining epochs 300\ndataset ShapeNet [5]\nloss, we follow [37] to optimize the KL-divergence LKL\nbetween the predicted tokens’ distribution and a uniform\nprior. The ﬁnal objective function is\nLdV AE = dℓ1\nCD(Pfine,G) +dℓ1\nCD(Pcoarse,G) +αLKL. (2)\nExperiment Setting: We report the default setting for\ndV AE training in Table 7.\nHyper-parameters of dV AE:We set the size of the learn-\nable vocabulary to 8192, and each ‘word’ in it is a 256-dim\nvector. The most important and sensitive hyper-parameters\nof dV AE areαfor LKL and the temperature τ for Gumbel-\nsoftmax. We set αto 0 in the ﬁrst 18 epochs (about 10,000\nsteps) and gradually increase to 0.1 in the following 180\nepochs (about 100,000 steps) using a cosine schedule. As\nfor τ, we follow [37] to decay it from 1 to 0.0625 using a co-\nsine schedule in the ﬁrst 180 epochs (about 100,000 steps).\nB. Point-BERT\nArchitecture: We follow the standard Transformer [9] ar-\nchitecture in our experiments. It contains a stack of Trans-\nformer blocks [51], and each block consists of a multi-head\nself-attention layer and a FeedForward Network (FFN). In\nthese two layers, LayerNorm (LN) is adopted.\nMulti-head Attention: Multi-head attention mechanism\nenables the network to jointly consider information from\ndifferent representation subspaces [51]. Speciﬁcally, given\nthe input values V, keys K and queries Q, the multi-head\nattention is computed by:\nMultiHead(Q,K,V ) =WoConcat(head1,..., headh), (3)\nwhere Wo is the weights of the last linear layer. The feature\nof each head can be obtained by:\nheadi = softmax(QWQ\ni (KWK\ni )T\n√dk\n)VW V\ni , (4)\nconﬁg value\noptimizer AdamW\nlearning rate 5e-4\nweight decay 5e-2\nlearning rate schedule cosine\nwarmingup epochs 3\naugmentation ScaleAndTranslate\nbatch size 128\nnumber of points 1024\nnumber of patches 64\npatch size 32\nmask ratio [0.25, 0.45]\nmask type rand mask\ntraining epochs 300\ndataset ShapeNet\nTable 8. Experiment setting for Point-BERT pre-training\nconﬁg value\noptimizer AdamW\nlearning rate 5e-4\nweight decay 5e-2\nlearning rate schedule cosine\nwarmingup epochs 10\naugmentation ScaleAndTranslate\nbatch size 32(C),16(S)\nnumber of points 1024(C),2048 (S)\nnumber of patches 64(C),128(S)\npatch size 32\ntraining epochs 300\nTable 9. Experiment setting for end-to-end ﬁnetuning. Srepre-\nsents segmentation task, Crepresents classiﬁcation task.\nwhere WQ\ni , WK\ni and WV\ni are the linear layers that project\nthe inputs to different subspaces and dk is the dimension of\nthe input features.\nFeed-forward network (FFN): Following [51], two linear\nlayers with ReLU activations and dropout are adopted as the\nfeed-forward network.\nPoint-BERT pre-training: We report the default setting\nfor our experiments in Point-BERT pretraining in Table 8.\nThe pre-training is conducted on ShapeNet.\nEnd-to-end ﬁnetuning: We ﬁnetune our Point-BERT\nmodel follow the common practice of supervised models\nstrictly. The default setting for end-to-end ﬁnetuning is in\nTable 9.\nHyper-parameters of Transformers: We set the number\nof blocks in the Transformer to 12. The number of heads in\neach multi-head self-attention layer is set to 6. The feature\nFigure 6. Two main operations of our segmentation head: 1)\nUpsampling: upsample the feature map for the sparse point cloud\nto the dense point cloud. 2) Propagation: propagate the feature\nhierarchically from deep layers to shallow layers for dense predic-\ntion.\ndimension of the transformer layer is set to 384. We fol-\nlow [47] to adopt the stochastic depth strategy with a drop\nrate of 0.1.\nClassiﬁcation Head: A two-layer MLP with dropout is ap-\nplied as our classiﬁcation head. In classiﬁcation tasks, we\nﬁrst take the output feature of [CLS] token out, and max-\npool the rest of nodes’ features. These two features are then\ncombined together and sent into the classiﬁcation head. The\ndetailed architecture of the classiﬁcation head is shown in\nTable 6, where Ncls is the number of classes for a certain\ndataset.\nSegmentation Head: There are no downsampling layers\nin the standard Transformers, making it challenging to per-\nform dense prediction based on a single-resolution feature\nmap. We adopt an upsampling-propagation strategy to solve\nthis problem, consisting of two steps: 1) Geometry-based\nfeature upsampling and 2) Hierarchical feature propagation.\nWe extract features from different layers of the Trans-\nformer, where features from shallow layers tend to capture\nlow-level information, while features from deeper layers in-\nvolve more high-level information. To upsample the feature\nmaps to different resolutions, we ﬁrst apply FPS to the ori-\ngin point cloud and obtain point clouds at various resolu-\ntions. Then we upsample the feature maps from different\nlayers to different resolutions accordingly. As shown in the\nleft part of Figure 6, ‘A’ is a point from the dense point\ncloud, and ‘a’,‘b’,‘c’ are its nearest points in the sparser\npoint cloud, with distance of da, db and dc respectively. We\nobtain the point feature of ‘A’ based on the weighted addi-\ntion of those features, which can be written as:\nFA = MLP(Concat(\n∑\ni∈[a,b,c]\n1\ndi\nFi\n∑\ni∈[a,b,c]\n1\ndi\n,pA)), (5)\nwhere pA represents the coordinates of point ‘A’.\nAfter obtaining the feature maps at different resolutions,\nwe perform feature propagation from coarse-grained feature\nmaps to ﬁne-grained feature maps. As shown in the right\npart of Figure 6, for a point ‘A’ in the dense point cloud, we\nﬁnd its k nearest points in the sparser point cloud. Then a\nlightweight DGCNN [54] is used to update the feature of\n‘A’. We hierarchically update the feature with the resolution\nincreases and ﬁnally obtain the dense feature map, which\ncan be used for segmentation tasks. The detailed architec-\nture for the segmentation head is shown in Table 6."
}