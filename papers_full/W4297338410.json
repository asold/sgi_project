{
  "title": "Transformer and group parallel axial attention co-encoder for medical image segmentation",
  "url": "https://openalex.org/W4297338410",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5100682060",
      "name": "Chaoqun Li",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A5081489939",
      "name": "Liejun Wang",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A5100427622",
      "name": "Yongming Li",
      "affiliations": [
        "Xinjiang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2741891296",
    "https://openalex.org/W2964227007",
    "https://openalex.org/W2771252144",
    "https://openalex.org/W2618237340",
    "https://openalex.org/W2884436604",
    "https://openalex.org/W3015788359",
    "https://openalex.org/W2888358068",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3135385363",
    "https://openalex.org/W6600373357",
    "https://openalex.org/W3137561054",
    "https://openalex.org/W3197957534",
    "https://openalex.org/W3198141213",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W3011199263",
    "https://openalex.org/W3017153481",
    "https://openalex.org/W2891511539",
    "https://openalex.org/W3094448207",
    "https://openalex.org/W3193256763",
    "https://openalex.org/W3013198566",
    "https://openalex.org/W2980185997",
    "https://openalex.org/W6754852571",
    "https://openalex.org/W4312851276",
    "https://openalex.org/W3026315751",
    "https://openalex.org/W3212933375",
    "https://openalex.org/W2921406441",
    "https://openalex.org/W2979983945",
    "https://openalex.org/W3103010481",
    "https://openalex.org/W2395611524"
  ],
  "abstract": null,
  "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2022) 12:16117  | https://doi.org/10.1038/s41598-022-20440-z\nwww.nature.com/scientificreports\nTransformer and group parallel \naxial attention co‑encoder \nfor medical image segmentation\nChaoqun Li, Liejun Wang* & Yongming Li\nU‑Net has become baseline standard in the medical image segmentation tasks, but it has limitations \nin explicitly modeling long‑term dependencies. Transformer has the ability to capture long‑term \nrelevance through its internal self‑attention. However, Transformer is committed to modeling the \ncorrelation of all elements, but its awareness of local foreground information is not significant. Since \nmedical images are often presented as regional blocks, local information is equally important. In \nthis paper, we propose the GPA‑TUNet by considering local and global information synthetically. \nSpecifically, we propose a new attention mechanism to highlight local foreground information, called \ngroup parallel axial attention (GPA). Furthermore, we effectively combine GPA with Transformer \nin encoder part of model. It can not only highlight the foreground information of samples, but also \nreduce the negative influence of background information on the segmentation results. Meanwhile, we \nintroduced the sMLP block to improve the global modeling capability of network. Sparse connectivity \nand weight sharing are well achieved by applying it. Extensive experiments on public datasets confirm \nthe excellent performance of our proposed GPA‑TUNet. In particular, on Synapse and ACDC datasets, \nmean DSC(%) reached 80.37% and 90.37% respectively, mean HD95(mm) reached 20.55 and 1.23 \nrespectively.\nAs China’s population ages, people’s awareness of diseases has deepened, and health consciousness has constantly \nimproved. The diagnosis of diseases requires doctors to analyze and discriminate CT or MR maps, etc., which is \nbound to generate a mass of work. Therefore, the use of computers to assist physicians in diagnosis has become \na matter of urgency. Computer-aided diagnosis technology has comprehensive applications and research values \nfor medical studies, pathology analysis, and image information processing.\nMedical image segmentation plays an extremely important role in disease diagnosis and clinical medicine. \nEarly medical image segmentation systems were mainly built based on traditional image segmentation algo-\nrithms. Such as edge detection-based methods and region-based methods. Later, with the rapid development \nof computer technology, deep learning algorithms based on Convolutional Neural Network (CNN)1 have made \nbreakthroughs.  UNet2 is a medical image segmentation network based on CNN. It consists of encoder-decoder \nand has been proven effective for many different segmentation tasks. Examples include cardiac segmentation \nby magnetic resonance (MR)3, organ segmentation by computed tomography (CT)4–6, and polyp segmentation \nby colonoscopy  video7. Despite the dominance of U-Net in medical image segmentation, it and its  variants7–9 \nface the same problems that CNN-like models (including fully convolutional nets (FCNs)10) cannot avoid: lack \nof long-term global correlation modeling capabilities. The main reason is that CNNs extract local information \nsimply, but they cannot measure global relevance efficiently.\nMany recent works have attempted to address this problem by using Transformer  encoders11–13. Transformer \ndesigned for sequence-to-sequence  prediction14 originally, it’s a model based on self-attention (SA). SA is a core \npart of Transformer. Due to SA ’s ability to model the correlation between all input tokens, Transformer is able to \nhandle global long-term dependencies. In this case, some recent works have achieved satisfactory  results13,15–19, \npure Transformer  models20 have also emerged. Since the foreground information of medical images is usually \npresented as regional blocks, the local detail information is equally important to segmentation results. However, \nTransformer focuses on the extraction of global information but weakens local information, so it also has some \ndisadvantages in medical image segmentation tasks. How to properly highlight foreground information, weaken \nbackground information, and how to better jointly model local information and global correlation dependence \nbecomes the focus of our study.\nOPEN\nCollege of Information Science and Engineering, Xinjiang University, Ürümqi 830046, China. *email: wljxju@xju.\nedu.cn\n2\nVol:.(1234567890)Scientific Reports |        (2022) 12:16117  | https://doi.org/10.1038/s41598-022-20440-z\nwww.nature.com/scientificreports/\nIn order to solve these problems, we design a new attention mechanism GPA and cite the Sparse-MLP (sMLP) \nproposed by Chuanxin Tang et al. 21. We combine GPA with Transformer as encoder. GPA attention enhances \nthe model’s perception of sample axial information and weakens the background information, thus strengthens \nthe local information of sample. At the same time, Transformer’s long-range correlation modeling capability is \npreserved to capture global information of the sample. We further introduce sMLP to intensify the global infor-\nmation. The sMLP has the advantage of sparse connectivity and weight sharing with slight parameters. Through \nTransformer and GPA co-encoder, sMLP enhances global information modeling, we obtain more significant \nfeature encoding capabilities for medical image segmentation.\nConcretely, our contributions can be summarized as:\n1. We design a new attention mechanism method: GPA. It focuses on model local information dependence. \nWe utilize GPA and Transformer as the co-encoder.\n2. We cite sMLP , which completes sparse connectivity and weight sharing. Global modeling capabilities of the \nnetwork are enhanced by applying it.\n3. In summary, we propose GPA-TUNet. We demonstrate its effectiveness on two different public datasets \n(Synapse multi-organ segmentation dataset and Automated cardiac diagnosis challenge dataset). The experi-\nmental results show that our method has many advantages over other competing methods.\nThe remaining content of this paper is as follows. \"Related Work \" Section is related work. \"GPA-TUNet\" \nSection introduces the architecture of GPA-TUNet and related modules in detail. \" Experiments and Analysis \" \nSection is experimental results and analysis. \"Conclusions\" section gives conclusions.\nRelated work\nCNN‑based methods. Edge detection and traditional machine-learning-based algorithms were the meth-\nods employed in early medical image segmentation. With the development of deep learning,  UNet2 was pro-\nposed for medical image segmentation, which based on encoder-decoder structure. UNet has simple structure \nand reliable performance, so many UNet-like networks have emerged. For example, Res-Unet 22 with residual \nstructure, UNet +  + 7 with nested U-shape structure, HDC-Net[23]with hierarchical dilation convolutional. It \nhas also been applied in 3D medical image segmentation, such as V-Net24. Currently, CNN-based methods have \nachieved great success in medical image segmentation field.\nTransformers. Transformer14 started with Natural Language Processing and Text  Embedding25. Trans-\nformer has been applied not only to target  detection12, semantic  segmentation26,27 and image  classification11, but \nalso to medical image  segmentation9,28,29. Built on the very successful Vision Transformer (ViT)11,  TransUNet13 \nis the first Transformer-based medical image segmentation framework. It effectively combines CNN with Trans-\nformer, which is implemented for local–global correlation modeling.\nCombining CNNs with self‑attention mechanisms. Many researchers have attempted to integrate \nself-attention into CNNs based on global modeling of all pixels of the feature mapping. For example, Wang \net al.30 designed a nonlocal operator that is inserted into multiple internal intermediate convolutional layers. \nSchlemper et  al.9 proposed additional attention gate modules integrated into skip connections based on the \nencoder-decoder U-shaped structure.\nCombining attention mechanisms with transformers. Attention mechanisms have been widely \nused in various research areas since their emergence and have resulted in many novel types of attention \n mechanisms31–35. Various attention mechanism algorithms are similar in that they all aim to highlight local \nforeground information while weakening background. While attention mechanisms perform well in modeling \nlocal relevance, they lack ability to model global information correlation. However, Transformer excels in meas-\nuring the relevance of all elements. Therefore, we designed a new attention mechanism and combined it with \nTransformer. By this means, our model not only highlights the local foreground information, but also preserves \nthe edge information we are interested in. It has been experimentally demonstrated that GPA-TUNet well imple-\nments joint modeling of local information dependence and global relevance dependence.\nThere are too many methods based on  attention36,37, self-attention38 and  Transformer13,39. 36,37 extract features \nby introducing spatial attention or combination of spatial attention and channel attention. 38 utilizes self-attention \nto improve the discriminativeness of feature representations. 13,39 extract features by Transformer singularly. These \nmethods have played a certain role in respective tasks, but the above methods extract features only by attention \nor Transformer singularly, which easily leads to the loss of global information or local foreground information \nof samples. Different from the above methods: (1) We open a new attention research direction starting from the \naxial correlation of samples and propose GPA attention, which different from channel attention or spatial atten-\ntion. (2) We combine GPA and Transformer as co-encoder to reduce information loss and achieve joint modeling \nof local foreground information and global correlation dependencies. (3) GPA-TUNet not only highlights the \nlocal foreground information (which is very important for medical segmentation tasks), but also preserves the \nedge information we are interested in.\nDifferent from the above, we effectively combine the attention mechanism with Transformer in our method \nto more mightily jointly model local information dependence and global correlation dependence.\n3\nVol.:(0123456789)Scientific Reports |        (2022) 12:16117  | https://doi.org/10.1038/s41598-022-20440-z\nwww.nature.com/scientificreports/\nGPA‑TUNet\nIn this section, the detail of GPA-TUNet method is described, including research motivation, the architecture \nof network and related modules.\nResearch motivation. Recently, there are some defects in medical image segmentation method that need \nto be dealt with: (1) CNNs extract local information simply, but they cannot measure global relevance efficiently. \n(2) Transformer performs well in modeling global information but cannot extract local details well. In order \nto solve these problems, some scholars consider combining CNN with Transformer as hybrid encoder. Since \nmedical images are often presented as regional blocks, foreground information is quite important for segmenta-\ntion results. But these methods do not highlight the importance of foreground information in medical images. \nBased on the above considerations, the goal of this paper is to design an attention mechanism to highlight the \nimportance of sample foreground information. We combine it with Transformer as co-encoder and enhance \nglobal modeling with MLP block. Therefore, local foreground information dependency and global correlation \ndependency can be jointly modeled for better medical image segmentation performance.\nGiven an image X ∈ RC×H×W  , C is the number of channels, and H  × W is the spatial resolution. Our goal is \nto predict pixel-level segmentation maps of the same size H  × W. For medical image segmentation tasks, many \nresearchers have applied Transformer or attention mechanism to encoder singly. Unlike existing methods, we use \nTransformer combined with GPA attention mechanism as co-encoder. Next in this paper, we will first introduce \nour general framework. Then we introduce the encoder and decoder of GPA-TUNet in turn. Among them we \nfocus on the architectural design of GPA and the GPA combines Transformer as co-Encoder approach. Neces-\nsarily, sMLP Block will be discussed.\nOverall GPA‑TUNet. The overall framework of the network is shown in Fig.  1. The network is based on \nU-shape architecture. The encoder consists of CNN, GPA and Transformer. The decoder uses dilated convolu-\ntions and skip connections to preserve the underlying features. As shown in Fig. 1, CNN is first used to extract \ncoarse features. Next, we use GPA to extract the local foreground information in axial direction and weaken \nbackground information, which is embedded between CNN and Transformer. Then, we transfer the feature map \ninto Transformer to extract global correlation of samples. To further preserve global information, we introduce \na module at the end of encoder (before upsampling): sMLP block. It serves to achieve sparse connectivity and \nweight sharing between rows and columns. The decoder is then aligned with TransUNet 13.\nCNN layer. For the CNN layer of our network, we adopt the same setting as TransUNet 13, that is, ResNet50 \nis used to extract rough features of samples.\nHidden Feature\nLinear Projection\nCNN\nTransformer Layer\n(n=12)\nHidden Feature\nFeature\n1/8\n1/4\n1/2\nConv3x3ˈReLU\nUpsample\nSegmentation head\nDownsample\nTransformer Layer\n\u0011\u0011\u0011\nMetaAconC\nGPA\n(group=64ˈline=2)\nsMLP block\nSkip Connection(n_patch, D) (D, H/16, W/16)\n(512, H/16, W/16)\n(512, H/16, W/16)\n(256, H/8, W/8)\n(128, H/4, W/4)\n(64, H/2, W/2)\n(16, H, W)\nFeature Concatenation\nreshape\nreshape\nFigure 1.  Overview of the framework. (Created by ‘Microsoft Office Visio 2013’ url: https:// www. micro soft. \ncom/ zh- cn/ micro soft- 365/ previ ous- versi ons/ micro soft- vision- 2013).\n4\nVol:.(1234567890)Scientific Reports |        (2022) 12:16117  | https://doi.org/10.1038/s41598-022-20440-z\nwww.nature.com/scientificreports/\nGPA layer. Overview of the proposed GPA is shown in Fig.  2. As is presented, the GPA is divided equally \ninto two branches according to the number of channels of input X. The upper branch implements pixel-based \nhorizontal (width) attention and the lower branch implements pixel-based vertical (height) attention. The out-\nputs of the two branches are fused and reshaped to obtain X c . X c goes through the channel shuffle and activation \nfunction Meta-ACON40 module to obtain the new output X o . The final output X out\n(GPA ) is obtained by multiplying \nX o with the original input X in.\nFirstly, we let X in ∈ RB×C×H×W  denote the collection of input tokens. After reshaping, the feature vector \nis X ∈ R(B∗G)×(C/G)×H×W  , G is the multiple of the channel reduction, set to 64. Then the input X is divided \ninto two branches by the number of channels to obtain Xw and Xh respectively, Xw ∈ R(B∗G)×(C/2G)×H×W  , \nXh ∈ R(B∗G)×(C/2G)×H×W .\nThe axial attention calculation is performed next. The upper branch is based on the pixel-based horizontal \nattention (sample width attention) calculation. The Eq. (1) is as follows:\nwhere L1 and L2 represent the fully connected layer. L1 makes the width of the image larger and keeps the height. \nThat is, the size of Xw changes from H× W to H × W l after passing through L1(In this paper, H = W = 14, W l = 32). \nL2 then reduces the size from H × W l to H× W . The softmax operation is set to dim = 1.\nHPA (Horizontal Pixel Arithmetic) indicates that the feature encoding is updated by the pixel value on the \nwidth of images. The update method is briefly illustrated through Fig. 3a. As shown in Fig. 3, Divide a picture of \nsize H× W into several disjoint rectangular patches, each patch represents a pixel point, corresponding to a pixel \nvalue. HPA is to divide the pixel value of each patch by the sum of all pixel values on its row. It generates some \nnew weights, which are assigned to each patch respectively. The horizontal attention weight update calculation \nEq. (2) of the \n(\nw i, hj\n)\n th pixel on each channel is as follows:\n(1)Xw −out = L2\n{\nHPA\n[\nsoftmax((L1(Xw ))\n]}\n(2)HPA\n(\nw i,hj\n)\n=\nc∑\nC =0\nvalue\n(\nw i,hj\n)\nvalue\n[\n(w 0 + w 1 +···w i +···+ w w +1 ),hj\n]\nƻ\nreshape\n(B,C,H,W)( B*G,C/G,H,W)\nChunk\n(chunks=2,dim=1)\nL3 L4\nL2\nͰ\nChannel Shuffle/circlex\n(B,C,H,W)\nW\nH\n...\nƻ\nƻM\n/circlexͰ\nMƻ\nL1\nHorizontal \nupdate\non W\nVertical \nupdate\non H\nHorizontal update\nVertical update\nreshape\nLinear Softmax ConCat Matrix Multiplication\nMetaAconC\nactivation function\nHorizontal Pixel Arithmetic\nVertical Pixel Arithmetic\nX\nX\nO\nX\nout\nGPA)(\nX\nC\nFigure 2.  Overview of the proposed GPA Attention. (Created by ‘Microsoft Office Visio 2013’ url: https:// www. \nmicro soft. com/ zh- cn/ micro soft- 365/ previ ous- versi ons/ micro soft- vision- 2013).\n(W=14)\n(H=32)\n(H=14)\n(W=32)\nFigure 3.  Horizontal Pixel Arithmetic and Vertical Pixel Arithmetic. (Created by ‘Microsoft Office Visio 2013’ \nurl: https:// www. micro soft. com/ zh- cn/ micro soft- 365/ previ ous- versi ons/ micro soft- vision- 2013).\n5\nVol.:(0123456789)Scientific Reports |        (2022) 12:16117  | https://doi.org/10.1038/s41598-022-20440-z\nwww.nature.com/scientificreports/\nwhere C denotes the number of channels, and value\n(\nw i, hj\n)\n denotes the pixel value of point \n(\nw i, hj\n)\n.\nSimilarly, the lower branch is a pixel-based vertical attention (sample height attention) calculation given by \nthe following Eq. (3) and (4).\nwhere L3 and L4 represent the fully connected layer. L3 makes the height of the image larger and keeps the width. \nThat is, the size of Xh changes from H× W to Hl× W  after passing through L3(In this paper, H = W = 14,H l = 32). \nL4 then reduces the size from Hl× W  to H× W . The softmax operation is set to dim  = 1. As shown in Fig. 3b, \nVPA (Vertical Pixel Arithmetic) is to divide the pixel value of each patch by the sum of all pixel values on its \ncolumn. The new weights are assigned to each patch respectively.\nWe splice Xw−out∈ R(B∗G)×(C/2G)×H ×W  and Xh−out∈ R(B∗G)×(C/2G)×H ×W  in cascade and then reshape it \ninto the original input dimension size Xc ∈ RB×C×H ×W  . The Eq. (5) is as follows:\nwhere concat and Re represent concat by number of channels and reshape feature vector respectively. In order to \nachieve information flow between features of different groupings, we perform the channel shuffle operation. The \nchannel shuffle schematic is shown in Fig.  4. First assume that the number of input channels C  is divided into \ng groups, and each group contains n channels. Split the channels C into (g, n) two dimensions. Then transpose \n(g, n) into (n, g). Finally, reshape the (n, g) dimensions to one dimension C  (C = n*g). In this way, information \ncan flow between different groups.\nFor the activation of feature vectors, we adopt a novel activation function, ACON-C 40. In neural networks, \nmany common activation functions are in the form of max ( ηa(x), ηb(x) ) function (e.g., ReLU max(x,0) and \nits variants) where ηa(x) and ηb(x) denote linear functions. The Eq. (6 ) proposed by Ningning Ma et al. 40 to \napproximate the activation function.\nwhere σ is the sigmoid function, β is the switching factor. The authors used a dual-parameter function to further \npropose the ACON-C activation function. As follow Eq. (7):\nFormally, ηa(x) = p1x , ηb(x) = p2x(p1  = p2).\nACON-C40 enable to adaptively choose whether to activate neurons. It controls whether the neuron is activated \nby the value of β (β is 0, i.e., not activated). The design space of the adaptive function of β utilizes channel-wise, \ni.e., channel space. H , W dimensions are first averaged separately and then passed through two convolutional \nlayers so that all pixels in each channel share a weight. The Eq. (8) is as follows:\nwhere σ is the sigmoid function, and W1 and W2 are two convolution operations respectively. W1 ∈ RC ×(C /r) , C \nis the dimension of the input and C/r is the dimension of the output. W2 ∈ R(C /r)×C , The C/r is the dimension of \nthe input and C is the dimension of the output. To save the number of parameters, a scaling parameter r is added \nbetween W 1 (C,C/r) and W 2 (C/r,C) and set to 16.\nWe denote the feature vector after activation as Xo ∈ RB×C×H×W  . As shown in Eq. (9):\nMatrix multiplication of X o with the original input Xin∈ RB×C×H ×W  to get the final output feature vector, \nwhich is the final output of GPA. As shown in Eq. (10).\n(3)Xh−out = L4\n{\nVPA\n[\nsoftmax((L3(Xh))\n]}\n(4)VPA\n(\nw i,hj\n)\n=\nc∑\nC =0\nvalue\n(\nw i,hj\n)\nvalue\n[\nw i,\n(\nh0 + h1 +···h j +···+ hh+1\n)]\n(5)X c = Re\n[\nconcat\n(\nX w −out, X h−out\n)]\n(6)Sβ (ηa(x), ηb(x))= (ηa(x) − ηb(x))· σ [β (ηa(x) − ηb(x))] + ηb(x)\n(7)fACON −C (x) = Sβ\n(\np1 x,p2 x\n)\n=\n(\np1 − p2\n)\nx · σ\n[\nβ\n(\np1 − p2\n)\nx\n]\n+ p2 x\n(8)β = σW 1 W 2\nH∑\nh=1\nW∑\nw=1\nxc,h,w\n(9)X o = ACON\n[\nshuﬄe\n(\nX c)]\n(10)X out\n(GPA ) = X inX o\n(C,H,W) (g*n,H,W)( g,n,H,W) (n,g,H,W)( C,H,W)\nFigure 4.  Schematic diagram of channel shuffle. (Created by ‘Microsoft Office Visio 2013’ url: https:// www. \nmicro soft. com/ zh- cn/ micro soft- 365/ previ ous- versi ons/ micro soft- vision- 2013).\n6\nVol:.(1234567890)Scientific Reports |        (2022) 12:16117  | https://doi.org/10.1038/s41598-022-20440-z\nwww.nature.com/scientificreports/\nTransformer layer. We further hand over the GPA output to the Transformer for processing. The Trans-\nformer sets to 12 layers, and the structure of each Transformer layer is shown in Fig. 5.\nThe feature vectors are first patch embedding in Transformer layer. We map the vectorized patches xp to the \npotential D-dimensional embedding space by virtue of a linear projection which is trainable. We preserve location \ninformation by adding location embeddings to patches, thereby encoding spatial information. As follows Eq. (11):\nwhere E ∈ R(P2·C)×D is the patch embedding projection, and Epos∈ RN ×D  denotes the position embedding.\nThe Multi-head Self Attention (MSA) and Multi-Layer Perceptron (MLP) blocks of l layers together form the \nTransformer encoder (Eq. (12) and (13)). Output of the l -th layer can be written as follows:\nwhere LN(·) denotes the layer normalization operator and z l is the encoded image representation.\nThe joint encoder leverages GPA to highlight local foreground information and diminish background infor-\nmation, while retaining Transformer’s powerful ability to measure the relevance of global elements. That is, our \nGPA-TUNet adequately and rationally models local correlation and global correlation. Therefore, our encoded \nfeature maps not only reinforce the salient features in different directions of samples, but also preserve the edge \ninformation we are interested in.\nsMLP block. Chuanxin Tang et al. proposed Sparse MLP (sMLP)21 based on the MLP-based vision model, \nreplacing the MLP module in the token-mixing step with a new sMLP module. For a 2D image, sMLP applies \n1D MLP along the image height and width, so the parameters are shared between rows or columns. As shown \nin Fig. 6a, the dark-colored token interacts with all other light-colored tokens in a single MLP layer. However, \n(11)z0 =\n[\nx1\npE; x2\npE; ,...,; xN\np E\n]\n+ Epos\n(12)z′\nl = MSA\n(\nLN\n(\nzl−1\n))\n+ zl−1\n(13)zl = MLP\n(\nLN\n(\nz′\nl\n))\n+ z′\nl\nFigure 5.  Schematic of the Transformer layer. (Created by ‘Microsoft Office Visio 2013’ url: https:// www. micro \nsoft. com/ zh- cn/ micro soft- 365/ previ ous- versi ons/ micro soft- vision- 2013).\n(a) MLP (b) sparse MLP\nFigure 6.  sMLP reduces the computational complexity of MLP . (Created by ‘Microsoft Office Visio 2013’ url: \nhttps:// www. micro soft. com/ zh- cn/ micro soft- 365/ previ ous- versi ons/ micro soft- vision- 2013).\n7\nVol.:(0123456789)Scientific Reports |        (2022) 12:16117  | https://doi.org/10.1038/s41598-022-20440-z\nwww.nature.com/scientificreports/\nin an sMLP layer (as shown in Fig. 6b), the dark-colored token only interacts with horizontal and vertical light-\ncolored tokens it is on.\nIn this paper, we add a stage of downsampling to the front of the sMLP block and a stage of upsampling to the \nback end of the sMLP block. We integrate the sampling session with the sMLP block and apply it to the network. \nThe purpose of it is to extract the deep global dependencies of the samples and obtain richer feature encoding \ninformation. We show the sMLP block in Fig. 7. As is presented, the input vector is first down-sampled and then \npassed through the sMLP block. The sMLP consists of 3 branches. The upper branch is responsible for mixing \ninformation along horizontal direction, and the lower branch is responsible for mixing information along vertical \ndirection. The middle branch is identity mapping. The outputs of three branches are concatenated, then processed \nby point-by-point conv (Conv 1 × 1), and finally upsampled to obtain the final output.\nSpecifically, Let X in ∈ RC×H′×W ′\n denote the collection of input tokens. Firstly, downsampling and permuting \nis performed to obtain X ∈ RH×W ×C . As shown in Eq. (14):\nwhere down and permute represent downsampling and transpose operations respectively. The Eq. (15), (16) and \n(17) show the size change.\nwhere H′ and W′ represent the input size, and H and W represent the size after downsampling.\nIn sMLP’s upper branch (horizontal mixing path), the data tensor is reshaped into ( HC) × W, and a linear \nlayer with weights WW ∈ RW ×W  is applied to each of the (HC ) rows to mix information. Similar operation is \napplied in lower branch (vertical mixing path) and the linear layer is characterized by weights WH ∈ RH×H . \nFinally, the outputs of three branch paths are fused together by cascading, processed by conv1 × 1. The output is \nXC ∈ RH×W ×C . As follows Eq. (18):\nwhere concat means to concatenate the outputs (i.e., XH , XW and X ) of the three branches by channel, and FC \nstands for conv1 × 1.\nRestore the XC dimension to RC×H×W  . Finally, as shown in Eq. (19), the overall upsampling doubles length \nand width dimensions, thereby restoring the output size to H ʹ × Wʹ. That is, the final output of sMLP block is \nXout\n(sMLP) ∈ RC×H ′×W ′\n.\nWith sMLP block, we aggregate information along the axial direction individually and implement global \ndependence modeling to obtain richer feature coding information. We take advantage of GPA and Transformer \nas co-encoder, then enhance the global information modeling by sMLP block. Therefore, our GPA-TUNet ade-\nquately co-encodes local information dependence and global relevance dependence to obtain excellent perfor -\nmance for medical image segmentation.\nDecoder of GPA‑TUNet. Similar to  TransUNet13 settings, we adopt upsampling, dilated convolutions and \nskip connections to restore the original resolution and obtain prediction images.\n(14)X = permute\n[\ndown\n(\nX in)]\n(15)H′ × W′ = 4 × H× W\n(16)H′ = 2 × H\n(17)W′ = 2 × W\n(18)X C = FC(concat(XH , XW , X ))\n(19)X out\n(sMLP ) = up\n(\npermute\n(\nX C ))\nPool permute\n(WC)×H\nͰ\nConv 1×1\n(HC)×W\n(WC)×H\npermute Upsample\nMLP\non W\nMLP\non H\n(C×Hʹ×Wʹ) (H×W×C)\nreshape\nreshape\nreshape\nreshape\n(C×Hʹ×Wʹ) \nͰConCat AdaptiveAvgPool2d UpsamplingBilinear2d\nsMLP\nX\nin\nX\nC\nX\n(HC)×W\nFigure 7.  Overview of the sMLP block. (Created by ‘Microsoft Office Visio 2013’ url: https:// www. micro soft. \ncom/ zh- cn/ micro soft- 365/ previ ous- versi ons/ micro soft- vision- 2013).\n8\nVol:.(1234567890)Scientific Reports |        (2022) 12:16117  | https://doi.org/10.1038/s41598-022-20440-z\nwww.nature.com/scientificreports/\nExperiments and analysis\nDatasets. (1) Synapse multi-organ segmentation dataset. Synapse is a multi-organ segmentation dataset \ncontaining 30 abdominal clinical CT cases.  Following13, the 30 cases were randomly divided by us into 18 train-\ning cases and 12 testing cases. Each image annotation contains eight organs (aorta, gallbladder, left kidney, right \nkidney, liver, pancreas, spleen and stomach). (2) Automated cardiac diagnosis challenge dataset. ACDC is a pub-\nlic cardiac MRI dataset of 100 cases. As  in13, 70 training samples, 10 validation samples and 20 testing samples \nare randomly divided for experiments. Each exam contains two different modalities, with corresponding labels \nincluding left ventricle (LV), right ventricle (RV), and myocardium (MYO). We use Dice Similarity Coefficient \n(DSC(%)) and 95% Hausdorff Distance (HD95(mm)) to evaluate our method on the two datasets.\nImplementation details. All experiments were performed on NVIDIA Corporation GV100 [TITANV] \nGPU, and input resolution is set to 224 × 224 for all experiments, with data expansion including random flips \nand rotations.\nIn hybrid encoder, the  ViT11 (denoted as \"R50-ViT\") is combined with ResNet-501 and 12 Transformer lay-\ners. In our co-encoder, we combine ViT and the proposed GPA attention. The GPA was conducted using groups \nof 2 to divide the horizontal and vertical axes. In order to reduce the computational complexity and cost of the \nmodel, the sMLP block at the end of the encoder is performed in a single block. For cascading upsampling \nblocks, we are consistent  with13, i.e., concatenation of four 2 × upsampling blocks in succession is performed \nto achieve full resolution. In this paper, the input resolution is set as 224 × 224 and patch size P is 16, except for \nspecial cases. And for the training of the model is using SGD optimizer with learning rate of 0.01, momentum \nof 0.9 and weight decay of 1e-4. For both Synapse and ACDC datasets, we have default batch size of 12. And \ndefault iterations are 20 k and 14 k respectively on the two datasets. All experiments were performed using a \nsingle NVIDIA Corporation GV100 [TITANV] GPU.\nSame  as5,6, we extrapolate all 3D volumes slice-by-slice and all predicted 2D slices are stacked to reconstruct \n3D predictions for evaluation.\nLoss functions. In medical image segmentation, the area of background region is much larger than fore-\nground region. If foreground information is misjudged as background information, acc score will be very high, \nbut the actual segmentation effect is not proportional to acc score. Therefore, in the field of medical image \nsegmentation, single loss function often cannot reflect the performance of the model. Following  TransUNet13, \nour model invokes two loss functions: CrossEntropyLoss and Dice Loss . The Eq. (20) and (21) demonstrate the \ncalculation of CrossEntropyLoss and Dice Loss respectively.\nwhere q(x) stands for ground-truth label, p(x) stands for predictive value.\nwhere TP is true-positive, FP is false-positive, FN is false-negative.\nThe ratio of the CrossEntropyLoss and Dice Loss is /afii98381 : /afii98382 . The total-loss of the network is as follows Eq. (22).\nWe set /afii98381 = /afii98382 = 0.5 in this paper. The influence of CrossEntropyLoss and Dice Loss ratio for model perfor-\nmance is discussed in our ablation study.\nEvaluation metrics. Consistent with  baseline13, DSC(%) and HD95(mm) were used as evaluation met-\nrics to evaluate performance of the model. DSC(%) is used to measure the similarity between Prediction and \nGround Truth. HD95(mm) is defined as the quantized value of 95% of the maximum distance of the surface \ndistance between Prediction and Ground Truth. The calculation methods of DSC(%) and HD95(mm) are shown \nin Eq. (23) and Eq. (24), respectively.\nwhere X and Y represent the Prediction and Group Truth, respectively.\nExperimental results. The experimental results on the Synapse and ACDC datasets are shown in Tables 1 \nand 2. In this paper, the highlighted part of tables indicates the best performance value, we will not specifically \naddress this point in subsequent narrative.\nAs is shown in Table  1, traditional CNN still has better performance, with Att-UNet even outperforming \nTransUNet. However, our method greatly outperforms CNN-based methods (UNet, etc.), attention mechanism-\nbased methods (Att-UNet, etc.) and Transformer-based methods. On the Synapse dataset, mean DSC(%) and \nHD95(mm) of our method (GPA-TUNet) reached 80.37% and 20.55 respectively, it has obtained the optimal \n(20)LCrossEntropyLoss =−\n∑\nx\n(\np(x)logq(x)\n)\n(21)LDiceLoss= 2TP\nFP + 2TP + FN\n(22)Ltotal−loss= /afii98381 ∗ LCrossEntropyLoss + /afii98382 ∗ LDiceLoss\n(23)DSC = 2 × |X ∩ Y|\n|X| + |Y|\n(24)HD 95 = maxk95% [d(X,Y),d(Y,X)]\n9\nVol.:(0123456789)Scientific Reports |        (2022) 12:16117  | https://doi.org/10.1038/s41598-022-20440-z\nwww.nature.com/scientificreports/\nmean DSC(%) and HD95(mm). Compared with baseline (TransUNet), DSC(%) and HD95(mm) performance \nimproved by 2.89% and 11.14% respectively. Our method obtained the current optimum on 4 organs (Kidney \n(L) etc.). As shown in Fig.  11, our model has significant performance in the segmentation of organs with large \nareas and organs with large axial spans. For example, the segmentation of pancreas in line 2 and stomach in line \n3. The reason is that large organs have a large area, a large axial span and many reference pixels. GPA attention \nand sMLP are local and global models based on axial information. Therefore, for GPA and sMLP , there are more \naxial reference data for large organs, so the network has strong learning ability, important information is not \neasily lost, and has prominent axial perception ability. Combination of GPA and sMLP can better capture the \nlocal and global correlation of samples. As a result, GPA-TUNet has mighty capacity for segmentation of organs \nwith a large area and a large axial span.\nAs is shown in Table 2, On the ACDC dataset, mean DSC(%) of our method (GPA-TUNet) reached 90.37%. \nCompared with baseline (TransUNet), DSC(%) performance improved by 0.66%. It has excellent segmentation \nperformance on RV and Myo. From Fig. 12, GPA-TUNet shows weaker under-segmentation and over-segmen-\ntation. It has significant performance compared with other methods, which is consistent with the quantitative \nexperimental results in Table 2.\nTo further demonstrate the performance and advantages of GPA-TUNet, we compared the mean DSC(%) for \nseveral classical models (UNet, Att-UNet, TransUNet, SwinUNet) under different approaches on the Synapse \ndataset. The results are shown in Fig.  8. It is obvious from Fig.  8 that our GPA-TUNet has made outstanding \nprogress compared with several classical networks, and also has the highest DSC(%) result.\nAnalytical study. On the influence of GPA and sMLP block. To better evaluate the proposed GPA-TUNet \nframework and to verify the effectiveness of its new approach to GPA with sMLP block on performance, we \ncompared our model with baseline  (TransUNet13).\nTable 1.  Experimental results of the Synapse Dataset. DSC(%) of each single class is also presented. Significant \nvalues are in [bold].\nMethod DSC(%) HD95(mm) Aorta Gallbladder Kidney(L) Kidney(R) Liver Pancreas Spleen Stomach\nV-Net25 68.81 – 75.34 51.87 77.10 80.75 87.84 40.05 80.56 56.98\nDARR 41 69.77 – 74.74 53.77 72.31 73.24 94.08 54.18 89.90 45.96\nR50  UNet2 74.68 36.87 84.18 62.84 79.19 71.29 93.35 48.23 84.41 73.92\nR50  AttnUNet9 75.57 36.97 55.92 63.91 79.20 72.71 93.56 49.37 87.19 74.95\nUNet2 76.85 39.70 89.07 69.72 77.77 68.60 93.43 53.98 86.67 75.58\nUNet +  + 7 78.13 25.65 89.27 62.35 83.00 78.98 94.53 56.70 85.99 74.20\nUNet3 + 8 73.81 30.82 86.32 59.06 79.16 71.26 93.13 46.56 84.94 70.08\nAtt-UNet42 77.77 36.02 89.55 68.88 77.98 71.11 93.57 58.04 87.30 75.75\nR50  ViT13 71.29 32.87 73.73 55.13 75.80 72.20 91.51 45.99 81.99 73.95\nViT11 61.50 39.61 44.38 39.59 67.46 62.94 89.21 43.14 75.45 69.78\nTransUNet13 77.48 31.69 87.23 63.13 81.87 77.02 94.08 55.86 85.08 75.62\nSwinUNet20 79.13 21.55 85.47 66.53 83.28 79.61 94.29 56.58 90.66 76.60\nMT-UNet43 78.59 26.59 87.92 64.99 81.47 77.29 93.06 59.46 87.75 76.81\nGPA-TUNet (Ours) 80.37 20.55 88.74 65.63 83.51 80.37 94.84 63.89 87.58 78.40\nTable 2.  Experimental results of the ACDC Dataset. Significant values are in [bold].\nMethods DSC(%) RV Myo LV\nR50 U-Net2 87.55 87.10 80.63 94.92\nR50 Att-UNet9 86.75 87.58 79.20 93.47\nCE-Net44 87.21 85.68 83.97 91.98\nUNet2 88.28 86.08 86.04 92.72\nUNet +  + 7 89.06 87.66 86.47 93.06\nUNet3 + 8 88.28 86.08 86.04 92.72\nR50  ViT13 87.57 86.07 81.88 94.75\nTransUNet13 89.71 88.86 84.53 95.73\nSwinUNet20 90.00 88.55 85.62 95.83\nUNETR18 88.61 85.29 86.52 94.02\nGPA-TUNet (Ours) 90.37 89.44 87.98 93.68\n10\nVol:.(1234567890)Scientific Reports |        (2022) 12:16117  | https://doi.org/10.1038/s41598-022-20440-z\nwww.nature.com/scientificreports/\nSynapse dataset. As can be seen from Table  3, neither GPA nor sMLP block is dispensable for the model, as \nremoving either of them may result in performance loss. Applying all of them to the network, DSC(%) increased \nby 2.89%. We also provide the ablation qualitative results for the Synapse dataset, as shown in Fig.  9. It can be \nseen that both GPA and sMLP block play a certain role in improving network performance. When they are \napplied to network simultaneously, the segmentation result is improved more obviously. Therefore, the qualita-\ntive results of Fig. 9 agree with our quantitative results in Table 3. In addition, combined with Fig. 11, we found \nthat GPA-TUNet had outstanding segmentation performance in large organs and organs with large axial span. \nThe reasons are as follows. Big organs have large span in axial direction, while GPA is exactly axial attention, \nwhich carries out local modeling from different directions (horizontal and vertical). Therefore, GPA enhances \nthe network’s attention to the axial local foreground information, weakens other background information, and \nreduces misjudgment. Further, sMLP is global modeling based on axial direction. Therefore, the combination \nof GPA and sMLP greatly enhances the modeling ability of axial information, thus focuses on improving the \nsegmentation performance of large organs and organs with large axial span. From Table 3, we further show that \nGPA and sMLP Block have the effect of promoting each other’s network performance, and the segmentation \naccuracy is not just a simple superposition of the two. The same condition applies to HD95(mm) evaluation met-\nric. HD95(mm) decreases by 7.39% and 2.16% compared to TransUNet when GPA and sMLP block are applied \nto the network alone. Applying all of them to the network, HD95(mm) significantly decreased by 11.14%.\nACDC dataset. The ablation performance of GPA-TUNet on the ACDC dataset is shown in Table  4. We can \nsee that, like the Synapse dataset, GPA and sMLP Block are not dispensable to the model, both of which improve \nthe model segmentation performance. Applying all of them to the network, DSC(%) increased by 0.66%. We \nprovide the ablation qualitative results for the ACDC dataset, as shown in Fig. 10. As can be seen from Fig.  10, \nwhen GPA and sMLP Block are jointly applied to network, the optimal segmentation result is achieved in our \nGPA-TUNet. It reduces more under-segmentation (such as the Myo on the first line, the RV on the second line) \nand over-segmentation (such as the RV on the third line).\nFor Figs. 9 and Fig. 10, we especially emphasize that in the comparison of TransUNet + GPA and TransUNet \nsegmentation result graphs, we see that GPA plays an important role in highlighting local foreground infor -\nmation and weakening background information. For example, when the liver is segmented in the first row in \nFig. 9, TransUNet + GPA shows a suppressing effect on the false positives of TransUNet, that is, GPA weakens \nthe background information. For the segmentation of stomach in the second row and pancreas in the third row \nin Fig. 9, TransUNet + GPA promotes the partial information that is not segmented by TransUNet, that is, GPA \nhighlights the local foreground information. A similar situation can also be seen in Fig.  10. For example in the \nfirst row for the segmentation of Myo, TransUNet + GPA shows a boost compared to TransUNet. Therefore, \nUNet Att-UNetT ransUNet SwinUnet GPA-TUNet\n76.5\n77.0\n77.5\n78.0\n78.5\n79.0\n79.5\n80.0\n80.5\n76.85\n77.77\n77.48\n79.13\n80.37\n)CSDnaeM(%\nMethods\n% (DSC)\nFigure 8.  Comparison of mean DSC(%) based on different methods on the Synapse dataset. Segmentation \nperformance comparison of classical architectures based on several different methods. (Created by ‘Origin and \nOriginPro 2021’ url: https:// www. origi nlab. com/ origin).\nTable 3.  Ablation study on the Synapse dataset. Significant values are in [bold].\nMethod DSC(%) HD95(mm)\nTransUNet13 77.48 31.69\nTransUNet + GPA 78.88 24.30\nTransUNet + sMLP 78.01 29.53\nGPA-TUNet (Ours) 80.37 20.55\n11\nVol.:(0123456789)Scientific Reports |        (2022) 12:16117  | https://doi.org/10.1038/s41598-022-20440-z\nwww.nature.com/scientificreports/\nthrough the segmentation graphs of TransUNet + GPA and TransUNet on Synapse and ACDC datasets, we \nfurther verify the ability of GPA attention to highlight local foreground information and weaken background \ninformation as mentioned earlier.\nOn the influence of input resolution. The default input resolution for GPA-TUNet is 224 × 224. As shown in \nTable 5, we show the results of training GPA-TUNet at resolutions of 288 × 288 and 352 × 352. When input \nresolution is 288 × 288, patch size remains the same (i.e., 16), which results in an increase of approximately \n1.65 × 1.65 in the length of the transformer sequence. Since shortly of the added sequence length compared to \ninput size of 224 × 224, we find that the performance improvement is not significant. DSC(%) has only increased \nby 0.25%. However, when the resolution is changed from 224 × 224 to 352 × 352 will result in DSC(%) perfor -\nmance improvement of 1.00%. As pointed out  by11, increasing the input resolution can lead to more signifi-\ncant performance improvements. Higher resolution means that we will trade a larger computational cost for an \nincrease in average DSC(%). Due to GPU memory resource limitations, we no longer train GPA-TUNet results \non 512 × 512 at high resolution. Therefore, based on the computational cost and memory limitation to consider, \nwe determine the experiments at 224 × 224 resolution to demonstrate the validity and reliability of GPA-TUNet. \nWe show the mean DSC(%) of the Synapse dataset for different input image resolutions in Table  5, which also \nfurther shows the segmentation accuracy about eight organs.\n(a) GroundTruth (b) TransUNet (c) TransUNet+GPA( d) TransUNet+sMLP (e) GPA-TUNet(Ours)\naorta  gallbladderl eft kidney right kidney liverp ancreass pleens tomach\nFigure 9.  Ablation segmentation results of GPA and sMLP block on the Synapse dataset. From left to right: (a) \nGround Truth, (b) TransUNet, (c) TransUNet + GPA, (d) TransUNet + sMLP , (e) GPA-TUNet(Ours). (Created \nby ‘Microsoft Office Visio 2013’ url: https:// www. micro soft. com/ zh- cn/ micro soft- 365/ previ ous- versi ons/ micro \nsoft- vision- 2013).\nTable 4.  Ablation study on the ACDC dataset. Significant values are in [bold].\nMethod DSC(%) RV Myo LV\nTransUNet13 89.71 88.86 84.53 95.93\nTransUNet + GPA 89.82 88.25 87.73 93.49\nTransUNet + sMLP 89.82 89.40 86.39 93.68\nGPA-TUNet (Ours) 90.37 89.44 87.98 93.68\n12\nVol:.(1234567890)Scientific Reports |        (2022) 12:16117  | https://doi.org/10.1038/s41598-022-20440-z\nwww.nature.com/scientificreports/\nOn the influence of loss function (evaluation metrics). In order to test the effectiveness of the two loss functions, \nwe adjusted the CrossEntropyLoss and Dice Loss  occupancy ratios and performed the corresponding ablation \nexperiments. The experimental results are shown in Table 6.\nFrom Table 6, we can find that the proportion of the loss function has little effect on the experimental results. \nThe segmentation achieved at a CrossEntropyLoss  to Dice Loss  ratio of 1:1 is optimal, with mean DSC(%) and \n(a) GroundTruth (b) TransUNet (c) TransUNet+GPA( d) TransUNet+sMLP (e) GPA-TUNet(Ours)\nRV MyoL V\nFigure 10.  Ablation segmentation results of GPA and sMLP block on the ACDC dataset. From left to right: (a) \nGround Truth, (b) TransUNet, (c) TransUNet + GPA, (d) TransUNet + sMLP , (e) GPA-TUNet(Ours). (Created \nby ‘Microsoft Office Visio 2013’ url: https:// www. micro soft. com/ zh- cn/ micro soft- 365/ previ ous- versi ons/ micro \nsoft- vision- 2013).\nTable 5.  Ablation study on the influence of input resolution of the Synapse Dataset. Significant values are in \n[bold].\nResolution\nDSC\n(%)\nHD95\n(mm) Aorta Gallbladder Kidney(L) Kidney(R) Liver Pancreas Spleen Stomach\n224 80.37 20.55 88.74 65.63 83.51 80.37 94.84 63.89 87.58 78.40\n288 80.62 24.85 89.20 64.06 82.05 77.67 95.49 63.19 90.66 82.63\n352 81.37 21.46 88.50 67.98 84.67 82.44 94.78 65.01 90.80 76.79\nTable 6.  Ablation study on the influence of loss function ratio of the Synapse Dataset. Significant values are in \n[bold].\nCrossEntropyLoss ratio ( /afii98381) DiceLoss ratio ( /afii98382) DSC (%) HD95(mm) Aorta Gallbladder Kidney(L) Kidney(R) Liver Pancreas Spleen Stomach\n0.2 0.8 79.83 37.54 87.56 65.24 79.93 74.89 94.70 63.47 91.07 81.77\n0.3 0.7 79.73 29.69 87.84 58.87 83.30 78.67 94.59 65.51 89.59 79.49\n0.4 0.6 80.18 26.93 88.36 62.17 84.17 80.07 94.73 62.03 90.57 79.36\n0.5 0.5 80.37 20.55 88.74 65.63 83.51 80.37 94.84 63.89 87.58 78.40\n0.6 0.4 80.01 27.01 87.18 64.26 83.08 79.30 94.61 62.68 90.18 78.79\n0.7 0.3 80.04 28.05 87.05 62.47 83.33 80.37 94.52 63.20 90.46 78.93\n0.8 0.2 79.86 28.44 86.69 64.54 85.26 80.90 94.84 61.94 88.03 76.66\n13\nVol.:(0123456789)Scientific Reports |        (2022) 12:16117  | https://doi.org/10.1038/s41598-022-20440-z\nwww.nature.com/scientificreports/\nTable 7.  Ablation study on the influence of Attention Mechanism of the Synapse Dataset. Significant values \nare in [bold].\nMethod DSC(%) HD95(mm) Aorta Gallbladder Kidney(L) Kidney(R) Liver Pancreas Spleen Stomach\nTransUNet13 77.48 31.69 87.23 63.13 81.87 77.02 94.08 55.86 85.08 75.62\nTransUNet + SE 78.53 31.05 87.00 66.02 81.38 77.54 93.79 60.48 86.47 75.56\nTransUNet + SK 77.51 23.62 86.66 61.90 78.64 77.26 94.57 55.73 87.33 78.02\nTransUNet + CA 77.62 32.91 87.27 59.70 79.38 75.69 93.45 61.29 88.31 75.88\nTransUNet + ECA 77.45 30.74 87.19 63.76 81.53 80.06 93.69 57.52 82.98 72.91\nTransUNet + CBAM 78.41 32.04 87.51 63.89 81.34 77.00 93.90 59.21 88.22 76.21\nTransUNet + GPA(Ours) 78.88 24.30 87.79 65.46 80.54 79.12 93.94 60.70 87.67 75.81\nTable 8.  Ablation study on the influence of Attention Mechanism of the ACDC Dataset. Significant values are \nin [bold].\nMethods DSC(%) RV Myo LV\nTransUNet13 89.71 88.86 84.53 95.73\nTransUNet + SE 90.09 89.72 86.97 93.59\nTransUNet + SK 89.47 88.83 86.55 93.04\nTransUNet + CA 90.06 89.69 87.17 93.32\nTransUNet + ECA 89.64 88.81 86.69 93.41\nTransUNet + CBAM 89.97 89.42 86.86 93.64\nTransUNet + GPA(Ours) 89.82 88.25 87.73 93.49\n (a) GroundTruth( b) GPA-TUNet(Ours)( c) TransUNet( d) SwinUNet (e) Att-UNet (f) UNet\naorta  gallbladder left kidneyr ight kidney liver pancreas spleen stomach\nFigure 11.  The segmentation results of different methods on the Synapse dataset. From left to right: (a) Ground \nTruth, (b) GPA-TUNet (Ours), (c) TransUNet, (d) SwinUNet, (e) Att-UNet, (f) UNet. (Created by ‘Microsoft \nOffice Visio 2013’ url: https:// www. micro soft. com/ zh- cn/ micro soft- 365/ previ ous- versi ons/ micro soft- vision- \n2013).\n14\nVol:.(1234567890)Scientific Reports |        (2022) 12:16117  | https://doi.org/10.1038/s41598-022-20440-z\nwww.nature.com/scientificreports/\nHD95(mm) were 80.37% and 20.55 respectively. We found that increasing the proportion of Dice Loss to 0.6, \n0.7 and 0.8 respectively brought a decrease of 0.19%, 0.64% and 0.54% to DSC(%); increasing the proportion of \nCrossEntropyLoss to 0.6, 0.7 and 0.8 respectively brought a decrease of 0.36%, 0.33% and 1.51% to DSC(%). As for \nHD95(mm), we can see from Table 6 that HD95(mm) gradually becomes larger as the ratio moves away from 1:1. \nThat is, the performance gradually deteriorates. Therefore, maintaining the ratio of CrossEntropyLoss to Dice Loss \nat 1:1 seems to give the best results for our model. All other experiments were performed under this condition.\nOn the influence of attention mechanism. As shown in Tables 7 and 8, in order to explore the impact of differ-\nent attention mechanisms on model performance, we list the experimental results of adding different attention \nmechanisms to baseline  (TransUNet13) on the Synapse and ACDC datasets. SE stands for SE attention  (from45), \nSK stands for SK attention  (from46), CA stands for Coordinate Attention  (from47), ECA stands for Efficient Chan-\nnel Attention  (from48), CBAM stands for Convolutional Block Attention Module attention  (from49). As shown \nin Table 7, our TransUNet + GPA performs well on the Synapse dataset, DSC(%) reached the highest score, and \nHD95(mm) result is slightly lower than TransUNet + SK. As shown in Table  8, TransUNet + SE reached the \nstate-of-the-art performance on the ACDC dataset, and our TransUNet + GPA is 0.27% lower than it. However, \nour TransUNet + GPA undoubtedly outperforms TransUNet, TransUNet + SK and TransUNet + ECA. Therefore, \nthe combined results show that our GPA attention has an excellent positive effect on performance of our medi-\ncal image segmentation model. GPA allows us to match or even exceed some current attentions in our medical \nimage segmentation task.\nVisualizations. Qualitative comparison results for Synapse and ACDC datasets are provided to visualize the \nsegmentation performance of GPA-TUNet, as shown in Figs. 11 and 12.\nSynapse dataset. From Fig. 11, it can be seen that: 1) UNet and Att-UNet based on pure CNN methods are \nmore likely to result in over-segmentation (e.g., segmentation of right kidney and liver in the first row) or under-\nsegmentation (e.g., segmentation of pancreas and spleen in the second row) of the organs. 2) These conditions \nare improved in TransUNet with the addition of Transformer. This suggests that the hybrid Transformer-based \nmodel has a stronger ability to encode global context and make semantic distinctions. However, it can be seen \nfrom Fig. 11 that the overall segmentation of the SwinUNet network based on the pure Transformer is not satis-\nfactory. 3) Compared with other methods, GPA-TUNet has a better segmentation effect. For example, the split \nbetween right kidney and liver in the first row did not show a false positive. The segmentation of pancreas in the \nsecond row and stomach in the third row gives significantly better results than other methods.\n(a) GroundTruth (b) GPA-TUNet(Ours) (c) TransUNet( d) SwinUNet (e) UNet\nRV MyoL V\nFigure 12.  The segmentation results of different methods on the ACDC dataset. From left to right: (a) Ground \nTruth, (b) GPA-TUNet (Ours), (c) TransUNet, (d) SwinUNet, (e) UNet. (Created by ‘Microsoft Office Visio \n2013’ url: https:// www. micro soft. com/ zh- cn/ micro soft- 365/ previ ous- versi ons/ micro soft- vision- 2013).\n15\nVol.:(0123456789)Scientific Reports |        (2022) 12:16117  | https://doi.org/10.1038/s41598-022-20440-z\nwww.nature.com/scientificreports/\nACDC dataset. The qualitative experiment of GPA-TUNet on the ACDC dataset is shown in Fig.  12. It is \nobvious from Fig. 12 that the segmentation effect of GPA-TUNet is closer to Ground Truth. For example, GPA-\nTUNet is obviously superior to other methods for segmentation of Myo and LV in the second row. It showed no \nfalse positives for segmentation of RV in the third line.\nIn addition, we found that GPA-TUNet showed exceptional ability in segmenting big organs and organs with \nlarge width or height spans on two datasets. The reasons are as follows. Large organs have large axial span, for \nGPA attention, there are more axial reference pixels in Eqs. (2) and (4), so that the axial prospect information can \nbe more accurately grasped and local modeling can be carried out. And sMLP is global modeling based on feature \nmaps. GPA is combined with sMLP , axial local modeling and global modeling are incorporated into learning \nprocess, which enhances the learning ability of the network. Therefore, compared with baseline (TransUNet), we \nperformed superior for organ segmentation with a larger axial span. It benefits from the joint modeling of GPA \nand Transformer, as well as the global modeling of sMLP . The above quantitative experimental results (Tables 1 \nand 2) also verify the effectiveness of GPA attention and sMLP for the network as described previously.\nThese observations show that GPA-TUNet, jointly encoded by GPA Attention and Transformer, is able to \npreserve local foreground details well while achieving global relevance modeling for more accurate segmentation. \nThat is, GPA-TUNet implements joint modeling of local information dependence and global relevance depend-\nence. It allows the model to enjoy the benefits of both low-level detail and high-level global contextual informa-\ntion, and also has the advantage of highlighting foreground information and weakening background information.\nGeneralization to other datasets. To demonstrate the model generalization capability of GPA-TUNet, we evalu-\nated the MR dataset ACDC which aims to accomplish automatic heart segmentation. The results of the evalu-\nation are shown in Table 2. We can see that our GPA-TUNet has been consistently improved compared to the \nCNN-based approach (UNet), the pure Transformer-based approach (SwinUNet) and the hybrid encoder-based \napproach (TransUNet). GPA-TUNet also has the highest DSC(%) on the ACDC dataset compared to various \nprevious state-of-the-art methods. We also provide the qualitative comparison results for the ACDC dataset, as \nshown in Fig. 12. GPA-TUNet has the best performance compared with other methods on the ACDC MR data-\nset. This is consistent with our previous results in the Synapse CT dataset.\nConclusions\nIn this paper, we propose a new attention mechanism: Group Parallel Axial Attention (GPA), which enables local \ninformation modeling by computing feature weights in parallel with attention in different directions (horizontal \nand vertical) of the image. Combining GPA with Transformer, co-encoder GPA-TUNet is constructed to explore \nperformance for medical image segmentation. It not only obtains sample local correlations by GPA based on pixel \nlevel, but also encodes powerful global context by Transformer with image features as sequences. Furthermore, \nwe introduce sMLP to strengthen the global information dependence, which relies on sparse connections and \nweight sharing. The whole structure adopts U-shape structure, thus also preserving the coarse-grained features \nof CNN. Extensive experiments demonstrate that GPA-TUNet jointly models local information dependencies \nand global correlation dependencies properly. It is especially remarkable for the segmentation performance of \norgans with large axial spans. Compared with various other existing methods, GPA-TUNet achieves optimal \nsegmentation results on two different publicly available datasets. GPA-TUNet has poor segmentation perfor -\nmance for small organs. The reason is that there are fewer axial reference pixels for small organs, and the model \neasily misjudges foreground information as background information. For the shortcomings of this paper, we \nwill continue to improve in the future.\nData availability\nThe Synapse and ACDC datasets are openly available at: https:// www. synap se. org/# !Synap se: syn31 93805/ wiki/ \n21778 9(acces sed on 28 April 2022) and https:// www. creat is. insa- lyon. fr/ Chall enge/ acdc/ (acces sed on 28 April \n2022).\nReceived: 29 April 2022; Accepted: 13 September 2022\nReferences\n 1. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. CVPR 2016, 770–778 (2016).\n 2. Ronneberger, O., Fischer, P . & Brox, T. U-net: Convolutional networks for biomedical image segmentation. MICCAI 2015, 234–241 \n(2015).\n 3. Yu, L. et al. Automatic 3D cardiovascular MR segmentation with densely-connected volumetric convnets. MICCAI 2017, 287–295 \n(2017).\n 4. Li, X. et al. H-DenseUNet: hybrid densely connected UNet for liver and tumor segmentation from CT volumes. IEEE Trans. Med. \nImaging. 37(12), 2663–2674 (2018).\n 5. Yu, Q. et al. Recurrent saliency transformation network: Incorporating multi-stage visual cues for small organ segmentation. CVPR \n2018, 8280–8289 (2018).\n 6. Zhou, Y . et al. A fixed-point model for pancreas segmentation in abdominal CT scans. MICCAI 2017, 693–701 (2017).\n 7. Zhou, Z. et al. Unet++: A nested u-net architecture for medical image segmentation. DLMIA/ML-CDS@MICCAI 2018: 3–11 \n(2018).\n 8. Huang, H. et al. Unet 3+: A full-scale connected unet for medical image segmentation. ICASSP 2020, 1055–1059 (2020).\n 9. Schlemper, J. et al. Attention gated networks: Learning to leverage salient regions in medical images. Med. Image Anal. 53, 197–207 \n(2019).\n 10. Long, J., Shelhamer, E. & Darrell, T. Fully convolutional networks for semantic segmentation. CVPR 2015, 3431–3440 (2015).\n 11. Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR (2021).\n 12. Carion, N. et al. End-to-end object detection with transformers. ECCV 2020, 213–229 (2020).\n16\nVol:.(1234567890)Scientific Reports |        (2022) 12:16117  | https://doi.org/10.1038/s41598-022-20440-z\nwww.nature.com/scientificreports/\n 13. Chen, J. et al. Transunet: Transformers make strong encoders for medical image segmentation. arXiv: 2102. 04306 (2021).\n 14. Vaswani, A. et al. Attention is all you need. NIPS 2017, 5998–6008 (2017).\n 15. Zhang, Y ., Liu, H. & Hu, Q. Transfuse: Fusing transformers and cnns for medical image segmentation. MICCAI 2021, 14–24 (2021).\n 16. Wang, W . et al. Transbts: Multimodal brain tumor segmentation using transformer. MICCAI 2021, 109–119 (2021).\n 17. Chang, Y ., Menghan, H., Guangtao, Z., Xiao-Ping, Z. Transclaw u-net: Claw u-net with transformers for medical image segmenta-\ntion. arXiv: 2107. 05188 (2021).\n 18. Hatamizadeh, A. et al. Unetr: Transformers for 3d medical image segmentation. WACV 2022, 1748–1758 (2022).\n 19. Wang, H., Cao, P ., Wang, J., Zaiane, O. R. UCTransNet: Rethinking the skip connections in U-Net from a channel-wise perspective \nwith transformer. arXiv: 2109. 04335 (2021).\n 20. Cao, H. et al. Swin-unet: Unet-like pure transformer for medical image segmentation. arXiv: 2105. 05537 (2021).\n 21. Tang, C, et al. Sparse MLP for image recognition: Is self-attention really necessary? arXiv: 2109. 05422 (2021).\n 22. Xiao, X., Lian, S., Luo, Z. & Li, S. Weighted res-unet for high-quality retina vessel segmentation. ITME 2018, 327–331 (2018).\n 23. Hu, X.; Wang, L.; Cheng, S.; Li, Y . HDC-Net: A hierarchical dilation convolutional network for retinal vessel segmentation. PLoS \nOne. 16(9) (2021).\n 24. Milletari, F .; Navab, N.; Ahmadi, S. A. V-net: Fully convolutional neural networks for volumetric medical image segmentation. \n3DV 2016;565–571 (2016).\n 25. Devlin, J., Chang, M. W ., Lee, K. & Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. \nNAACL-HLT 2019, 4171–4186 (2018).\n 26. Y e, L., Rochan, M., Liu, Z. & Wang, Y . Cross-modal self-attention network for referring image segmentation. CVPR 2019, 10502–\n10511 (2019).\n 27. Wang, H. et al. Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. ECCV 2020, 108–126 (2020).\n 28. Sinha, A. & Dolz, J. Multi-scale self-guided attention for medical image segmentation. IEEE J. Biomed. Health Inf. 25, 121–130 \n(2021).\n 29. Wang, Y . et al. Deep attentional features for prostate segmentation in ultrasound. MICCAI 2018, 523–530 (2018).\n 30. Wang, X., Girshick, R., Gupta, A. & He, K. Non-local neural networks. CVPR 2018, 7794–7803 (2018).\n 31. Zhang, Q. L. & Y ang, Y . B. Sa-net: Shuffle attention for deep convolutional neural networks. ICASSP 2021, 2235–2239 (2021).\n 32. Tian, T. et al. QSAN: A quantum-probability based signed attention network for explainable false information detection. CIKM  \n2020, 1445–1454 (2020).\n 33. Luo, H., Zhang, S., Lei, M. & Xie, L. Simplified self-attention for transformer-based end-to-end speech recognition. SLT  2021, \n75–81 (2021).\n 34. Misra, D., Nalamada, T., Arasanipalai, A. & Hou, Q. Rotate to attend: Convolutional triplet attention module. WACV  2021, \n3138–3147 (2021).\n 35. Dong, Y ., Wang, L., Cheng, S. & Li, Y . Fac-net: Feedback attention network based on context encoder network for skin lesion \nsegmentation. Sensors 21, 5172 (2021).\n 36. Feng, S. et al. CPFNet: Context pyramid fusion network for medical image segmentation. IEEE Trans. Medical Imaging 39(10), \n3008–3018 (2020).\n 37. Mou, L. et al. CS-Net: Channel and spatial attention network for curvilinear structure segmentation. MICCAI 2019, 721–730 \n(2019).\n 38. Fu, J. et al. Dual attention network for scene segmentation. CVPR 2019, 3146–3154 (2019).\n 39. Y ang, Y .; Mehrkanoon, S. AA-TransUNet: Attention augmented TransUNet for now-casting tasks. arXiv: 2202. 04996 (2022).\n 40. Ma, N., Zhang, X., Liu, M. & Sun, J. Activate or not: Learning customized activation. CVPR 2021, 8032–8042 (2021).\n 41. Fu, S. et al. Domain adaptive relational reasoning for 3d multi-organ segmentation. MICCAI 2020, 656–666 (2020).\n 42. Oktay, O. et al. Attention u-net: Learning where to look for the pancreas. arXiv: 1804. 03999 (2018).\n 43. Wang, H. et al. Mixed transformer U-Net for medical image segmentation. arXiv: 2111. 04734 (2021).\n 44. Gu, Z. et al. CE-Net: Context encoder network for 2D medical image segmentation. IEEE Trans. Med. Imaging 38(10), 2281–2292 \n(2019).\n 45. Hu, J., Shen, L. & Sun, G. Squeeze-and-excitation networks. CVPR 2018, 7132–7141 (2018).\n 46. Li, X., Wang, W ., Hu, X. & Y ang, J. Selective kernel networks. CVPR 2019, 510–519 (2019).\n 47. Hou, Q., Zhou, D. & Feng, J. Coordinate attention for efficient mobile network design. CVPR 2021, 13713–13722 (2021).\n 48. Wang, Q. et al. ECA-Net: Efficient channel attention for deep convolutional neural networks. CVPR 2020, 11531–11539 (2020).\n 49. Woo, S., Park, J., Lee, J. Y . & Kweon, I. S. C. B. A. M. Convolutional block attention module. ECCV 2018, 3–19 (2018).\nAuthor contributions\nConceptualization and methodology, C.L.; software, C.L. and Y .L.; validation, Y .L. and L.W; formal analysis, L.W . \nand Y .L.; data curation and writing original draft preparation, C.L.; writing-review and editing, C.L. and L.W . \nAll authors have read and agreed to the published version of the manuscript.\nFunding\nThis research was funded by the National Science Foundation of China under Grant U1903213.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to L.W .\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\n17\nVol.:(0123456789)Scientific Reports |        (2022) 12:16117  | https://doi.org/10.1038/s41598-022-20440-z\nwww.nature.com/scientificreports/\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2022",
  "topic": "Encoder",
  "concepts": [
    {
      "name": "Encoder",
      "score": 0.7435145974159241
    },
    {
      "name": "Computer science",
      "score": 0.7086559534072876
    },
    {
      "name": "Segmentation",
      "score": 0.6638224124908447
    },
    {
      "name": "Transformer",
      "score": 0.5929900407791138
    },
    {
      "name": "Artificial intelligence",
      "score": 0.526991069316864
    },
    {
      "name": "Image segmentation",
      "score": 0.48692548274993896
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4703766703605652
    },
    {
      "name": "Data mining",
      "score": 0.3643902838230133
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}