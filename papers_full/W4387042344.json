{
  "title": "TEmoX: Classification of Textual Emotion Using Ensemble of Transformers",
  "url": "https://openalex.org/W4387042344",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5037000734",
      "name": "Avishek Das",
      "affiliations": [
        "Chittagong University of Engineering & Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5091780889",
      "name": "Mohammed Moshiul Hoque",
      "affiliations": [
        "Chittagong University of Engineering & Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5021361292",
      "name": "Omar Sharif",
      "affiliations": [
        "Chittagong University of Engineering & Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5089476507",
      "name": "M. Ali Akber Dewan",
      "affiliations": [
        "Athabasca University"
      ]
    },
    {
      "id": "https://openalex.org/A5012890130",
      "name": "Nazmul Siddique",
      "affiliations": [
        "University of Ulster"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3114543730",
    "https://openalex.org/W6785229638",
    "https://openalex.org/W3016962737",
    "https://openalex.org/W3036810919",
    "https://openalex.org/W3117204308",
    "https://openalex.org/W2786205708",
    "https://openalex.org/W3169324695",
    "https://openalex.org/W6776655525",
    "https://openalex.org/W6776783833",
    "https://openalex.org/W2995392485",
    "https://openalex.org/W6766673545",
    "https://openalex.org/W2963199188",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2955429306",
    "https://openalex.org/W2954375912",
    "https://openalex.org/W4220904955",
    "https://openalex.org/W4205973286",
    "https://openalex.org/W6785402801",
    "https://openalex.org/W3152502479",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W6674385629",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W6790637388",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4242954443",
    "https://openalex.org/W4206598708",
    "https://openalex.org/W3012159372",
    "https://openalex.org/W6791477150",
    "https://openalex.org/W3170705121",
    "https://openalex.org/W6840717220",
    "https://openalex.org/W3045969489",
    "https://openalex.org/W6731031554",
    "https://openalex.org/W2069360745",
    "https://openalex.org/W3153055731",
    "https://openalex.org/W1980867644",
    "https://openalex.org/W6605424934",
    "https://openalex.org/W193025648",
    "https://openalex.org/W6804263915",
    "https://openalex.org/W6797704076",
    "https://openalex.org/W3212633118",
    "https://openalex.org/W3034503310",
    "https://openalex.org/W6682691769",
    "https://openalex.org/W6636510571",
    "https://openalex.org/W3155227529",
    "https://openalex.org/W6803913869",
    "https://openalex.org/W6765332919",
    "https://openalex.org/W3016831976",
    "https://openalex.org/W2903101678",
    "https://openalex.org/W3128437760",
    "https://openalex.org/W3158398474",
    "https://openalex.org/W2098326211",
    "https://openalex.org/W3003805468",
    "https://openalex.org/W3117802159",
    "https://openalex.org/W6788310658",
    "https://openalex.org/W6788627667",
    "https://openalex.org/W2563351168",
    "https://openalex.org/W132962688",
    "https://openalex.org/W3022847728",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W3121405056",
    "https://openalex.org/W2959681520",
    "https://openalex.org/W4287082593",
    "https://openalex.org/W3214119601",
    "https://openalex.org/W3118868930",
    "https://openalex.org/W4287363563",
    "https://openalex.org/W4287606245",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2097998348",
    "https://openalex.org/W4287888679",
    "https://openalex.org/W4287601507",
    "https://openalex.org/W172742023",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W3133334564",
    "https://openalex.org/W4287813888",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Textual emotion classification (TxtEC) refers to the classification of emotion expressed by individuals in textual form. The widespread use of the Internet and numerous Web 2.0 applications has emerged in an expeditious growth of textual interactions. However, determining emotion from texts is challenging due to their unorganized, unstructured, and disordered forms. While research in textual emotion classification has made considerable breakthroughs for high-resource languages, it is yet challenging for low-resource languages like Bengali. This work presents a transformer-based ensemble approach (called TEmoX) to categorize Bengali textual data into six integral emotions: joy, anger, disgust, fear, sadness, and surprise. This research investigates 38 classifier models developed using four machine learning LR, RF, MNB, SVM, three deep-learning CNN, BiLSTM, CNN+BiLSTM, five transformer-based m-BERT, XLM-R, Bangla-BERT-1, Bangla-BERT-2, and Indic-DistilBERT techniques with two ensemble strategies and three embedding techniques. The developed models are trained, tuned, and tested on the three versions of the Bengali emotion text corpus BEmoC-v1, BEmoC-v2, BEmoC-v3. The experimental outcomes reveal that the weighted ensemble of four transformer models En-22: Bangla-BERT-2, XLM-R, Indic-DistilBERT, Bangla-BERT-1 outperforms the baseline models and existing methods by providing the maximum weighted F1 -score (80.24%) on BEmoC-v3. The dataset, models, and fractions of codes are available at https://github.com/avishek-018/TEmoX .",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identiﬁer 10.1109/ACCESS.2017.DOI\nTEmoX: Classiﬁcation of Textual Emotion\nusing Ensemble of Transformers\nAVISHEK DAS1 (Graduate Student Member, IEEE), MOHAMMED MOSHIUL HOQUE 1\n(Senior Member, IEEE), OMAR SHARIF 1 (Graduate Student Member, IEEE), M. ALI\nAKBER DEWAN2 (Member, IEEE) and NAZMUL SIDDIQUE 3, (Senior Member, IEEE)\n1Dept. of Computer Science and Engineering, Chittagong University of Engineering & Technology, Chittagong, Bangladesh\n(avishek.das.ayan@gmail.com; moshiul_240@cuet.ac.bd; omar.sharif@cuet.ac.bd)\n2School of Computing and Information Systems, Faculty of Science and Technology, Athabasca University, Athabasca, AB T9S\n3A3, Canada (adewan@athabascau.ca)\n3School of Computing, Engineering and Intelligent Systems, Ulster University, UK (nh.siddique@ulster.ac.uk)\nCorresponding author: Mohammed Moshiul Hoque (moshiul_240@cuet.ac.bd).\nThis work was supported by the Natural Sciences and Engineering Research Council (NSERC), Canada, and\nDirectorate of Research and Extension, CUET.\nABSTRACT Textual emotion classiﬁcation (TxtEC) refers to the classiﬁcation of emotion\nexpressed by individuals in textual form. The widespread use of the Internet and numerous Web 2.0\napplications has emerged in an expeditious growth of textual interactions. However, determining\nemotion from texts is challenging due to their unorganized, unstructured, and disordered forms.\nWhile research in textual emotion classiﬁcation has made considerable breakthroughs for high-\nresource languages, it is yet challenging for low-resource languages like Bengali. This work\npresents a transformer-based ensemble approach (called TEmoX) to categorize Bengali textual\ndata into six integral emotions: joy, anger, disgust, fear, sadness, and surprise. This research\ninvestigates 38 classiﬁer models developed using four machine learning LR, RF, MNB, SVM,\nthree deep-learning CNN, BiLSTM, CNN+BiLSTM, ﬁve transformer-based m-BERT, XLM-R,\nBangla-BERT-1, Bangla-BERT-2, and Indic-DistilBERT techniques with two ensemble strategies\nand three embedding techniques. The developed models are trained, tuned, and tested on the three\nversions of the Bengali emotion text corpus BEmoC-v1, BEmoC-v2, BEmoC-v3. The experimental\noutcomes reveal that the weighted ensemble of four transformer models En-22: Bangla-BERT-2,\nXLM-R, Indic-DistilBERT, Bangla-BERT-1 outperforms the baseline models and existing methods\nby providing the maximum weighted F 1-score ( 80.24%) on BEmoC-v3. The dataset, models, and\nfractions of codes are available at https://github.com/avishek-018/TEmoX.\nINDEX TERMS Natural language processing, Text classiﬁcation, Textual emotion classiﬁcation,\nBengali emotion text corpus, Ensemble of transformers.\nI. INTRODUCTION\nC\nLassifying textual emotion entails the automated\nprocess of attributing a text to an emotion cat-\negory based on predetermined connotations. In recent\nyears, the proliferation of the Internet and the rapid\nevolution of social media platforms have led to a sig-\nniﬁcant surge in text-based content, greatly increasing\nits presence in everyday interactions. Online users\ncommunicate their concerns, opinions, or feelings via\ntweets, posts, and comments. Thus, much emotional\ntext content is accessible on social media or online\nplatforms. Researchers’ attention has been attracted\nby the increasing volume of textual emotion content, as\nthe categorization of emotions plays a vital role across\nnumerous applications, including education, sports, e-\ncommerce, healthcare, and amusement. With an ever-\nincreasing number of people on virtual platforms and\nthe rapidly producing online information, evaluating the\nemotions expressed in online content becomes crucial for\ndiﬀerent stakeholders, such as customers, enterprises,\nand online education. Textual emotion analysis of an\nenterprises service or product can boost brand value,\nsales, and prestige [ 1]. Automatic TxtEC helps to\nenhance the quality of a product or service, revise sales\nVOLUME 4, 2016 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319455\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nDas et al.: Emotion Classiﬁcation\nplans, and forecast forthcoming trends. Furthermore,\nit can shape brand reputation, follow client reactions,\ncatch general emotions, and track conformity.\nAlthough TxtEC has made signiﬁcant advancements\nin well-resourced languages, its current stage of devel-\nopment remains rudimentary when it comes to low-\nresource languages like Bengali. Low-resource languages\nare deﬁned as languages for which statistical methods\ncannot be directly applied due to data scarcity. These\nlanguages are crucial as they represent vast speakers,\nespecially in regions like Asia and Africa [ 2]. Investigat-\ning huge amounts of data to unveil the underlying senti-\nments or emotions (particularly in Bengali) is considered\na critical research problem in low-resource languages.\nThe textual data are voluminous and unstructured. Due\nto their chaotic forms, it is very arduous and time-\nintensive to organize, store, manipulate, and extract\nemotional content. The diﬃculty arises from several\nconstraints, including sophisticated language structures,\nlimited resources, and substantial verb inﬂections [ 3].\nMoreover, the scarcity of text processing tools and\nstandard corpora makes textual emotion analysis more\ndiﬃcult in Bengali. Taking into consideration the\ncurrent impediments of textual emotion classiﬁcation in\nBengali, this work introduces an intelligent technique\ncalled TEmoX which utilizes transformer-based learn-\ning to categorize Bengali texts into six primary emo-\ntions (e.g., joy, anger, disgust, fear, sadness, surprise).\nTransformer-based learning has recently demonstrated\nsigniﬁcant advancements in text classiﬁcation [ 4]–[6].\nHence, this work motivates us to use transformer-based\nlearning to classify textual emotions in Bengali. This\nresearch extends the previous work [ 4], which involved\nutilizing three transformer models: m-BERT, XLM-R,\nand Bangla-BERT-1. However, this work utilizes two\nmore new models: Indic-DistilBERT [ 7] and Bangla\nBert-2 [ 7]. In addition, this study incorporates the\nextended version of the dataset (BEmoC-v3 [ 8]) and\nemploys a stratiﬁed sampling technique [ 9] to address\nthe imbalanced nature of the corpus. By exploiting\nthe performance of 26 ensemble models, 3 deep learning\nmodels, and 4 machine learning-based models, this work\nproposes the En22 (XLM-R+Bangla-BERT-1+Bangla-\nBERT-2+Indic-DistilBER) model to perform textual\nemotion classiﬁcation for improved results. The distinc-\ntive contributions of this work are outlined as follows:\n• Proposed a textual emotion classiﬁcation tech-\nnique called TEmoX to classify Bengali text into\nsix categories: joy, anger, disgust, fear, sadness,\nand surprise. TEmoX uses weighted ensemble of\nfour standard transformer models (XLM-R, Bangla-\nBERT-1, Bangla-BERT-2, and Indic-DistilBERT)\nwith ﬁne-tuned hyperparameters.\n• Investigated 38 classiﬁcation models, including 4\nmachine learning (ML), 3 deep learning (DL), and 5\ntransformer models with ensemble strategies to ﬁnd\na robust model for textual emotion classiﬁcation\ntasks in Bengali.\n• Analyzed the classiﬁcation outcomes of 38 models\nwith a detailed investigation of misclassiﬁcation\nand error rate to ﬁnd many exciting characteristics\nof the emotion classiﬁcation task that might help\nfuture researchers.\nII. RELATED WORK\nEmotions can be perceived in various manners. For ex-\nample, as per Barrett et al. [ 10], two main attributes de-\nﬁne emotions: valence, which denotes their pleasantness\nor unpleasantness, and arousal, which refers to physical\nactivation. The emphasis individuals place on these\nattributes can aﬀect how they identify and feel emotions.\nAccording to Ekman et al. [ 11], emotions are distinct\nstates like fear, anger, and joy, each characterized by\nunique expressions, evaluations, preceding events, and\nbodily reactions.\nRecent advancements in textual emotion classiﬁcation\ntasks primarily concentrate on high-resourced languages\nowing to the obtainability of standard datasets and text-\nprocessing tools. Unfortunately, no formal data repos-\nitory exists in resource-constraint languages, including\nBengali, like IMDB dataset 1. There is a substantial\nadvancement in the textual emotion classiﬁcation in En-\nglish, Arabic, Chinese, French, and other high-resourced\nlanguages [ 12]. For example, the EmoTxt toolkit is\ncreated using ML algorithms for the English language\n[13]. In another research, random forest (RF), decision\ntree (DT), and K-nearest neighbor (KNN) are used to\ndetect multilabel multi-target emotion text in Arabic\ntweets where RF provides the highest F1-score of 82.6%\n[14]. Ahmad et al. [ 15] suggested a DL model for\ncategorizing English poetry text into 13 emotion classes.\nTo understand human emotions from the content of\ntweets, Mondal et al. [ 16] employ a supervised machine\nlearning approach, mapping 13 manual annotations to\n8 emotions based on Plutchiks wheel. These emotions\nare further organized into pairs of polar opposites for\nbinary classiﬁcation, such as Joy vs. Sadness and Love\nvs. Hate. A recent study [ 17] actively explored seven\nML techniques for classifying Tweets into happy or\nunhappy. The ensemble of logistic regression (LR) and\nstochastic gradient descent (LR-SGD) emerged with the\nhighest accuracy of 79%. An automatic classiﬁcation\nmethod was developed by Hasan et al. [ 18] for detecting\nemotion from tweets. They applied a supervised ML\nalgorithm and obtained 90% accuracy by a decision\ntree in four emotion categories, such as happy-inactive,\nhappy-active, unhappy-inactive, and unhappy-active.\nSeveral DL approaches have been studied to clas-\nsify textual emotion from short sentences. For the\n1http://www.imdb.com/\n2 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319455\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nDas et al.: Emotion Classiﬁcation\nclassiﬁcation of emotions in Chinese microblogs, Lai\net al. [ 19] presented a graph convolution network ar-\nchitecture, and their suggested method attained an\nF-score of 82.32%. Using Nested Long-Short Term\nMemory (LSTM), Haryadi et al. [ 20] successfully clas-\nsiﬁed English Twitter data into seven emotion classes\nand yielded exceptional results, achieving the highest\naccuracy (99.167%). The SemEval-2019 task-3 [ 21]\nproposed a Bi-LSTM model for categorizing emotion\ninto four classes and gained a maximum F1-score of\n79.59%. Ameer et al. [ 22] presented a detailed analysis of\nclassifying short text messages (i.e., SMS) using several\nML, DL, and transfer learning-based techniques. Their\nmodels have been developed on a code-mixed (Urdu\nand English) dataset containing 12 emotion classes and\nachieved the maximum performance by ML model with\nuni-gram features. Kumar et al. [ 23] used a dual-channel\nmethod for multi-class textual emotion detection. They\nemployed CNN to extract textual features and the\nBiLSTM layer to order text and sequence information.\nTheir work revealed that multiple layers could give\nmore accurate results. However, their network becomes\ncomparatively slower, and GloVe requires extra time\nthan the BERT embeddings.\nAlthough TxtEC in low-resource languages such as\nFrench, Italian, and Bengali is still in its infancy, a\nfew research activities have been embarked on utilizing\nML and DL approaches. Among them, Guarasci et\nal. [ 24] focus on the syntactic transfer capabilities of\nmultilingual BERT (mBERT) language models across\ndiﬀerent languages, speciﬁcally Italian, French, and\nEnglish. Using a structural probe, the research demon-\nstrates that mBERT can embed the dependency parse\ntrees of sentences cross-linguistically. Tripto et al. [ 25]\ndeveloped a method to detect multilabel sentiments and\nemotions from Romanized Bengali texts based on the\nYouTube comments dataset of 1006 data. Their model\n(LSTM) detected three-label sentiment (with 65.97%\naccuracy), ﬁve-label sentiment (54.24% accuracy), and\nemotion (59.23% accuracy). Rahib et al. [ 26] developed\na DL-based method using CNN and LSTM to classify\nemotion from social media response to COVID-19 text\nand achieved an accuracy of 84.92%. Purba et al.\n[27] proposed an emotion detection system employing a\nMultinomial Naive Bayes classiﬁer to identify emotions\ninto three categories (angry, sad, and happy) with an\naccuracy of 68.27%. Mamun et al. [ 28] introduced\na sentiment dataset comprising 8122 text expressions\ncategorized into negative, positive, and neutral. They\nshowed that the ensemble technique (LR+RF+SVM)\nsurpassed the other approaches attaining the most in-\ncreased accuracy of 82%. Rayhan et al. [ 29] devel-\noped an emotion dataset with six emotion classes by\ntranslating an existing English emotion dataset into\nBengali. They applied CNN-BiLSTM and BiGRU on\nthe dataset and attained the highest F1-score of 67.41%\nusing CNN-BiLSTM. Azmin et al. [ 30] employed three\nemotive classes (happy, sad, and anger). They used\na dataset developed by [ 31] and showed Multinomial\nNaïve Bayes (MNB) surpassed others with a precision of\n78.6%. A corpus named Anubhuti [32] concentrated on\nBengali short stories labeled in four classes (joy, anger,\nsorrow, and suspense) which obtained an accuracy of\n73% by LR. Analyzing emotions expressed in Bengali\nblog writing, Das et al. [ 33] utilized conditional random\nﬁeld (CRF) for identifying emotional content from blogs,\nthat achieved an accuracy of 56.45%. Rupesh et al. [ 34]\nclassiﬁed six basic emotions on 1200 Bengali documents\nfrom diﬀerent domains using SVM and obtained 73%\naccuracy. Rahman et al. [ 31] curated a Bengali emotion\ndataset focused on socio-political issues and employed\nML techniques. Their work acquired the highest ac-\ncuracy (52.98%) and F1-score (33.24%) by utilizing\nSVM with a non-linear RBF kernel. Parvin et al. [ 35]\nutilized the ensemble of CNN and RNN architectures on\ntheir developed emotion dataset (containing 9000 text\ndata) and achieved an F1-score of 62.46%. Iqbal et\nal. [ 8] developed an emotion dataset called BEmoC-v3\ncontaining 7000 textual data in Bengali. The previous\nversion of BEmoC-v3 (i.e., BEmoC-v2) consisted of\n6243 data utilized by Das et al. [ 4] for classifying six\nemotions in Bengali. They employed a pre-trained\nBERT variant XLM-Roberta and gained the maximum\nF1-score (69.73%).\nTable 1 summarizes the ﬁndings of a few recent studies\non Bengali TxtEC in terms of the number of classes,\ncorpus size, models used, performance, and critical\nweaknesses.\nMost previous works in Bengali TxtEC methods used\nlimited datasets to develop ML and DL approaches.\nIn contrast to past studies, this research proposes an\nensemble of transformer-based learning that can detect\nsix emotions, outperforming previous methods of TxtEC\nin Bengali. The use of transformer models made Bengali\ntext classiﬁcation tasks more robust [ 37], [ 38].\nIII. BEMOC-V3: BENGALI EMOTION CORPUS\nThe development of an intelligent method for TxtEC\nin resource-constrained languages presents a signiﬁcant\nchallenge due to the lack of benchmark corpora. Thus,\ndeveloping a reliable corpus is the prerequisite for any\nintelligent text classiﬁcation model based on ML or DL\ntechniques. The previous research [ 8] discussed various\naspects of the development of the dataset (BEmoC-v3).\nThis work focuses on the various analysis of the dataset.\nThe Bengali Emotion Corpus (‘BEmoC-v3’), is freely\navailable at https://github.com/avishek-018/TEmoX.\nA. BEMOC-V3 DEVELOPMENT\nBEmoC-v3 comprises four sub-modules: data crawling,\npreprocessing, annotation, and veriﬁcation. Fig. 1 illus-\ntrates the overall process of BEmoC-v3 development.\nVOLUME 4, 2016 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319455\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nDas et al.: Emotion Classiﬁcation\nTable 1. A brief summary of previous works on Bengali textual emotion classiﬁcation\nAuthor(s) Approach Corpus Size Emotion\nClasses Critical Weaknesses\nTripto et al.\n[25] Word2Vec + LSTM 2890 4 Dataset contained YouTube\ncomments only\nRayhan et al.\n[29] CNN + BiLSTM 7214 6 Dataset is translated from English\nthus inconsistent sentence formation\nAzmin et al.\n[30]\nTf-Idf + Bigram +\nPOS tagger + MNB 4200 3 Unable to handle morphological\nfeatures\nPal et al. [ 32] Tf-Idf + LR 32124 4 Only considered Bengali short stories\nDas et al. [ 33] Heuristic features +\nCRF 1300 6 Can not handle out-of-vocabulary\nwords\nDas et al. [ 36] Lexical word level\nkeyword spotting 1100 6 Suﬀers from the identiﬁcation of\nmorphologically changed keywords\nRuposh et al.\n[34] BOW + SVM 1200 6 Failed in ﬁnding semantic\nrelationships\nRahman et al.\n[31] Tf-Idf + SVM 5640 6 Only considered Facebook comments\nDas et al. [ 4] XLM-Roberta 6243 6 Can not handle imbalanced data\nParvin et al.\n[35] CNN + BiLSTM 9000 6 Can not dealt with the imbalanced\ndata\nRahib et al.\n[26] LSTM 10581 3 Limited to Covid-19 data only\nPurba et al.\n[27] CNN 995 3 Only three classes are considered\nFigure 1. Development processes of BEmoC-v3.\nFive human crawlers have manually accumulated Ben-\ngali text data from various online and oﬄine sources.\nThe primary sources include social media comments\nor posts (Facebook, YouTube), blog postings, textual\nconversations, narratives, storybooks, and news por-\ntals. A total of 7125 text documents were collected\ninitially. Raw collected data requires several steps of pre-\nprocessing before labeling. Few pre-processing are done\nautomatically, and the rest are performed manually:\n• Automatic: Removed punctuation, digits, non-\nBengali words, emoticons, and duplicate data. A\nmodule named ‘BanglaProcess’ 2 has been devel-\noped for automatic text pre-processing.\n• Manual: The text underwent a process of spelling\ncorrection and exclusion of texts containing less\nthan three words to ensure an unwavering emotional\nadherence.\nFollowing successful pre-processing, the corpus com-\nprised 7000 texts that were subsequently forwarded to\n2https://pypi.org/project/BanglaProcess/\nhuman annotators for manual labeling. The initial\nannotation task is assigned to ﬁve postgraduate students\nworking in the Bengali language processing ﬁeld with\ncomputer science and engineering backgrounds. The\nmajority voting [ 8] process is employed to decide the\nprimary label of the text.\nB. VERIFICATION\nThe initial labeling of texts was examined by an expert\nwith several years of experience conducting Bangla\nLanguage Processing (BLP) research. If any initial\nannotation was done incorrectly, the expert updated\nthe labeling. Through conversations and extensive\ndeliberations with the annotators, the NLP professional\nﬁnalized the labels, ensuring a reduced likelihood of bias\nduring the annotation process [ 39]. Table 2 illustrates\nsome discarded data samples and their causes.\nC. QUALITY OF ANNOTATION\nWe used the Cohens Kappa scores to determine inter-\nannotator congruence to ensure the quality of the label-\ning. The quality of the corpus is reﬂected by inter-coder\nreliability (93.1%) and Cohens Kappa (0.91), which\nshowed a perfect agreement among annotators [ 4]. The\nJaccard index between the classes has been calculated\nfor quantitative analysis. Table 3 shows the similarity\nvalues where the 200 most frequent words are utilized\nfrom each category. Two emotion class pairs (joy-\nsurprise and anger-disgust) showed the highest similar-\nity index of 0.51 and 0.55, respectively. These results\nreveal that more than half of the frequently used terms\nare familiar in these two groups. Nevertheless, the pair\n(joy-fear) obtained the lowest similarity, indicating that\nthe frequent words in this pair are more distinctive than\n4 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319455\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nDas et al.: Emotion Classiﬁcation\nTable 2. Few samples of rejected sentences and modiﬁed labels after veriﬁcation\nRejected samples Primary\nlabel\nExpert\nlabel/Action Cause\nেসàিতিদনঅǬফসেথেকসŬয্ায়বাসায়Ǭফের। (He comes from oﬃce daily\nat the evening.) Surprise Discarded Neutral\nemotion\nতারেচােখরপািনেতিছলসুেখরইঝলকািন। (There was a twinkle of\nhappiness in her tears.) Surprise Discarded Implicit\nemotion\nসাহাযয্নাকেরহাসতােছএতেবহায়ামানুষগ‍ুেলা। (Without helping them,\nthese brazen people are laughing.) Anger Disgust Semantic of\nsentence\nভাইেতামােকপুরƯারেদওয়াউিচত।িকভােবকািটংগ‍ুেলািমলানঅসাধারণ!\n(Brother, you should be rewarded. How great the cuts are!) Joy Surprise Intensity of\nemotion word\nthose in other categories. Thus, it is of concern that\nthe similarity can signiﬁcantly impact the classiﬁcation\ntask.\nTable 3. Interclass Jaccard similarity index. Anger (CL1), Disgust (CL2),\nFear (CL3), Joy (CL4), Sadness (CL5), Surprise (CL6)\nCL1 CL2 CL3 CL4 CL5 CL6\nCL1 1.00 0.55 0.40 0.44 0.47 0.46\nCL2 - 1.00 0.42 0.45 0.49 0.46\nCL3 - - 1.00 0.38 0.44 0.49\nCL4 - - - 1.00 0.48 0.51\nCL5 - - - - 1.00 0.50\nD. STATISTICS OF BEMOC-V3\nFollowing the pre-processing and annotation procedure,\nthe BEmoC-v3 comprised 7000 text documents. To\nevaluate the models, the data is partitioned into three\nsets: training (5751 texts), validation (624 texts), and\ntest sets (625 texts). As the data are imbalanced,\nit is preferable to distribute the corpus into training,\nvalidation, and test sets in such a fashion that the\nproportions of data in each class remain the same as they\nwere in the original corpus [ 40]. Therefore, we performed\na stratiﬁed sampling technique [ 9] while splitting the\ncorpus. Table 4 shows statistics of data distribution in\neach category.\nTable 4. Summary of the BEmoC-v3\nClass Training Validation Test\nAnger 900 76 76\nDisgust 1045 155 156\nFear 788 87 87\nJoy 1295 114 115\nSadness 1089 119 119\nSurprise 634 73 72\nTotal 5751 624 625\nFig. 2 shows the distribution of the number of texts\nversus the length of texts. According to the analysis,\nmost of the data in this graph had a length between 15\nto 35 words. Curiously, most of the texts in the disgust\nand sadness categories are between 20 to 30 words long.\nThis depicts disgust that contents take more words to be\nexpressed. The Joy and Sadness classes appear to have\nnearly identical numbers of textual data in the length\ndistribution.\nFigure 2. Corpus distribution concerning the number of texts vs length.\nFig. 3 represents the most frequent word distribution\nusing Wordcloud. The words in the center are the most\ncommon, while those on the periphery are less common.\n(a)\n (b)\n (c)\n(d)\n (e)\n (f)\nFigure 3. Wordcloud representation of the high-frequency emotion words in\nBEmoC-v3 for each emotion: (a) Anger, (b) Disgust, (c) Fear, (d) Joy, (e)\nSadness, and (f) Surprise.\nVOLUME 4, 2016 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319455\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nDas et al.: Emotion Classiﬁcation\nFigure 4. Abstract process of textual emotion classiﬁcation in Bengali. Here, UW, S, D, and WF denote Unique Words, Sentences, Documents, and Word\nFeatures respectively. Moreover, M, N, and D are total sentences/documents, total unique words, and embedding dimensions, respectively. In BERT Tokenizer,\nTOK denotes Token.\nIV. METHODOLOGY\nThis work exploits several ML, DL, and transformer-\nbased learning models with ensemble techniques for\nperforming textual emotion classiﬁcation in Bengali.\nFig. 4 depicts a high-level overview of textual emotion\nclassiﬁcation.\nThe TF-IDF and Bag of Words feature extraction\ntechniques are used for ML-based models (LR, RF,\nMNB, SVM), whereas Word2Vec, FastText, and pre-\ntrained GloVe embeddings are used for DL-based mod-\nels (CNN, BiLSTM, CNN+BiLSTM). Furthermore, we\nused transformer-based models (i.e., m-Bert, XLM-R,\ndistil-BERT and two variants of Bangla-Bert: Bangla-\nBERT-1 and Bangla-BERT-2. This research also in-\nvestigates the eﬀect of transformer-based ensembling\nmodels for textual emotion classiﬁcation. The same\ndataset (i,e. BEmoC-v3) will be used to train and tune\nall models.\nA. FEATURE EXTRACTION\nSeveral feature extraction techniques were utilized, in-\ncluding TF-IDF, Word2Vec, and FastText. These\ntechniques transform the text data into a numerical\nrepresentation of matrix or high dimensional vectors.\nThese feature extractors are shown in Fig. 4.\n1) Traditional Feature Extractor\nTerm Frequency-Inverse Document Frequency\n(TF-IDF): The TF-IDF [ 41] determines the signif-\nicance of a word in text content. We extracted a\ncombination of uni-gram and bi-gram features from the\nmost frequent 20000 words of BEmoC-v3.\nBag of Words (BOW):The BOW [ 42] describes\nthe frequency of words in a dataset. Unlike TF-IDF it\ndoes not provide information for more or less important\nwords according to other documents in the dataset. We\nused the same parameters used in TF-IDF to train the\nBOW model.\n2) Local Contextual Feature Extractor\nWord2Vec: The Word2Vec is a popular and widely\nused word embedding technique for detecting the se-\nmantic similarities between words in a datasets context\n[43]. The Word2Vec algorithms have two variants: skip-\ngram and continuous BOW. According to [ 44], skip-\ngram works eﬀectively with a tiny training data set and\naccurately depicts even uncommon words or phrases. In\nthis work, the Word2Vec is trained using skip-gram with\n6 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319455\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nDas et al.: Emotion Classiﬁcation\na window size of 7, embedding dimension of 100, and\nminimum word count of 4.\nFastText: The Word2Vec algorithm can not handle\nout-of-vocabulary words; thus, any word not present in\nthe test set can not be vectorized with a corresponding\nembedding value. The FastText algorithm is used\nto tackle this problem [ 45]. By leveraging sub-word\ninformation, this technique employs character n-grams\nto establish semantic relationships between words within\na given context [ 46]. In this approach, when a word is\nabsent in the training vocabulary, it can be synthesized\nusing its constituent n-grams. Like Word2Vec, the\nFastText algorithm is available in both Skip-Gram and\nContinuous-BOW variations. We trained the FastText\nalgorithm using skip-gram with a window size of 5, a\ncharacter n-gram of size 5, and an embedding dimension\nof 100.\nGloVe: It is a word vector technique for learning\nembeddings using word co-occurrences [ 47]. The GloVe\ndoes not rely solely on words local context information\n(like Word2Vec) to yield embeddings but instead utilizes\nglobal statistics on word co-occurrence. We used the\npre-trained word vectors by [ 48] containing 39 M tokens,\na vocab size of 0.18 M, and an embedding dimension of\n100.\n3) Attention-based Feature Extractor\nBert-based tokenizer: Bert-based multilingual tok-\nenizers leverage the power of BERT’s contextual em-\nbeddings to encode words and sentences in diﬀerent\nlanguages, capturing their semantic meaning and con-\ntext. XLM-R, m-BERT are such kinds of multilingual\ntokenizers. Besides utilizing these multilingual tokeniz-\ners we also used Bangla-BERT-1, Bangla-BERT-2, Bn-\nDistilbert which are pretrined on the Bangla language\nonly.\nB. FEATURE REPRESENTATION\nBy utilizing the traditional feature extraction algorithms\nthat are frequency-based algorithms, we get a feature\nmatrix. From Fig. 4 we can see, UW, S and D denote\nUnique Words, Sentences and Documents. Moreover, M\nand N are total sentences/documents and total unique\nwords, respectively. embedding dimensions, respec-\ntively. In BERT Tokenizer, TOK denotes Token. The\n[CLS] token is a special token added at the beginning\nof each input sequence in BERT, representing the entire\nsequence for sentence-level tasks. The [SEP] token is\nused to separate segments or sentences when working\nwith pairs of sequences, indicating their boundaries.\nC. CLASSIFIERS\n1) ML-based Approach\nThis work explored the four most widely used ML\nmodels to build an emotion detection system, including\nSVM, LR, MNB, and RF, where the TF-IDF and BoW\nare used as text vectorizers. For the LR, we choose the\n‘lbfgs solver with the ‘ l1 penalty and set the maximum\niteration to 400 for the solver to converge. The C value\nis kept at 1 for both LR and SVM. The SVM utilizes\nthe rbf kernel with l2 penalizer. The RF is implemented\nwith 100 estimator trees, and we keep the lowest number\nof instances required to divide an internal node at 2. For\nMNB, we set the Laplace smoothing parameter (alpha)\nto 1, enabling it to learn prior class probabilities. Table\n5 shows a brief synopsis of the parameters employed for\nML models.\nTable 5. Parameters used for ML models\nClassiﬁer Parameters\nLR optimizer = lbfgs, max_iter = 400,\npenalty = l1, C = 1\nSVM kernel = rbf, random_state = 0,\ngamma = scale, tol = 0.001\nRF criterion = gini, n_estimators = 100,\nmin_samples_split = 2\nMNB alpha = 1.0, ﬁt_prior = true,\nclass_prior = none,\n2) DL-based Approach\nVarious DL models (CNN, BiLSTM, CNN+ BiLSTM)\nare applied to BEmoC-v3, and their performances are\ninvestigated. All DL models employ Word2Vec and\nfastText as feature embedding. The DL algorithms\nperformance depends heavily on the hyperparameters,\nwhich are tuned carefully to get an optimized network\n[49]. In general, this task is carried out by humans,\nwhich likely leads to suboptimal results. With enough\ncomputing resources, one can apply a grid search that\nexecutes all possible combinations of hyperparameters.\nHowever, as the hyperparameters and parameter space\nincrease, the computation becomes intractable. The\npast study reveals that a model developed with ran-\ndomly selected hyperparameters values could show bet-\nter performance in lower computational than exhaustive\ngrid search [ 50]. We have empirically determined the\nhyperparameter values of the embedding models and the\nclassiﬁers based on our developed corpus. The models\nutilized the ADAM optimizer with a learning rate of\n0.001 and were trained for 35 epochs per batch (with\n16 samples). Keras callbacks were used to monitor the\ntraining process and save the model with the maximum\nvalidation accuracy in each epoch. The loss function\nchosen was sparse_categorical_crossentropy.\nCNN: For analyzing the performance of CNN [ 51], we\npassed BEmoC-v3 to our scratch CNN model. For all of\nthe convolutional and dense layers, we employed rectiﬁed\nlinear units to introduce non-linearity, while the softmax\nactivation was used for the output layers. Only one\nconvolution block had a 1D convolution layer containing\n64 ﬁlters with a size of 7. The training weights of\nWord2Vec and FastText embeddings are passed to the\nVOLUME 4, 2016 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319455\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nDas et al.: Emotion Classiﬁcation\nTable 6. Hyperparameters for DNN methods\nHyperparameters Hyperparameter space CNN BiLSTM CNN +\nBiLSTM\nFilter size 3,5,7,9 7 - 3\nNature of pooling max, average max - max\nEmbedding dimension 30, 35, 50, 70, 90, 100, 150, 200,\n250, 300 100 100 100\nNumber of units 16, 32, 64, 128, 256 64 32 64,64,32\nDense layer units 16, 32, 64, 128, 256 64 16 -\nBatch size 16, 32, 64, 128, 256 16 16 16\nActivation function ‘relu’, ‘softplus’, ‘tanh’, ‘sigmoid’ ‘relu’ ‘relu’ ‘relu’\nOptimizer ‘RMSprop’, ‘Adam’, ‘SGD’,\n‘Adamax’ ‘Adam’ ‘Adam’ ‘Adam’\nLearning rate 0.5, 0.1, 0.05, 0.01, 0.005, 0.001,\n0.0005, 0.0001 0.001 0.001 0.001\nembedding layer, which generates a sequence matrix.\nThis matrix is then processed by the following layer,\nglobal max pooling, to extract the maximum value from\neach ﬁlter. This process produces a single-dimensional\nvector of the same length as the number of ﬁlters used.\nFinally, an output layer with six nodes computes the\nprobability distribution for each of the six emotion\ncategories.\nBiLSTM: Bidirectional Long-Short Term Memory\n(BiLSTM) is a kind of recurrent neural network (RNN)\nthat can store information in both directions [ 52]. Basic\nRNN only looks at recent information while iterating\nover data and fails where long-term dependency is\nneeded. We may need to look further back to get the\nsemantic meaning of a text in the emotion detection\ntask. BiLSTM overcomes this problem and works\ntremendously well for long-term dependency problems.\nThe BiLSTM network contains an Embedding layer\ninitialized with the Word2Vec or FastText embedding\nweights. The model includes a BiLSTM layer with 32\nhidden units and a fully-connected dense layer with 16\nneurons and ReLU activation. The output layer utilizes\na softmax activation function to produce probability\ndistributions for six emotion classes.\nCNN+BiLSTM: A hybrid architecture combining\nCNN and BiLSTM has been explored to leverage the\nadvantages of both designs. Starting with an embedding\nlayer initialized as in the previous procedure, a 1D\nconvolutional layer with 64 ﬁlters (size 3) is added on\ntop. Obeyed by this, a max-pooling layer downsampled\nthe CNN features and transmitted them to two BiL-\nSTM layers. The ﬁrst layer comprises 64 LSTM units,\nwhile the second contains 32 LSTM units. Finally, the\nBiLSTM layer outputs are fed into a softmax-activated\noutput layer that gives the probability distribution of\nsix emotion classes.\nTable 6 outlines the optimized hyperparameters of\nvarious DL models. The hyperparameter values are\ntaken from the ranges mentioned in the ‘Hyperparame-\nter Space’ ﬁeld.\n3) Transformer-based Approach\nTransformer-based models, such as BERT (Bidirectional\nEncoder Representations from Transformers), can cap-\nture contextualized word representation from unlabeled\ntexts [ 53]. It makes use of the encoder represen-\ntation technique of the transformer architecture ﬁrst\nintroduced in [ 54]. There have been many pre-trained\ntransformer models available in the Huggingface 3 trans-\nformers library to be used in text processing tasks.\nRecently pre-trained transformers variants are being\nemployed in diﬀerent domains of Bengali text processing\ntasks, including sentiment analysis [ 55] and document\ncategorization [ 56], [ 57]. They outperformed the ML\nand DL models with higher accuracy.\nThis work implemented ﬁve transformer-based mod-\nels: m-BERT, XLM-R, Indic-Transformers Bengali Dis-\ntilBERT, and two variants of Bangla-BERT. All models\nare ﬁne-tuned on the emotion corpora by employing\nKtrain [ 58]. We embed speciﬁc start and end sequence\ntokens (SOS and EOS) at the beginning and end of the\ntransformer model for ﬁne-tuning. When required, we\napplied padding at the end of the sequence or removed\nany additional tokens that exceeded the predetermined\nsequence length. The padded tokens are excluded during\ntraining to ensure they do not aﬀect the training process.\nThese tokens then go through multiple self-attention\nlayers before being input into the transformer models.\nm-BERT: We used the bert-base-multilingual-cased’\nmodel on BEmoC-v3 and ﬁne-tuned it by modifying\nthe batch size, learning rate, and epochs. The training\nprocess of m-BERT [ 59] involved using the most popular\n104 languages with the most extensive Wikipedia data,\nincluding Bengali. The pre-trained m-BERT contains\nabout 110M parameters.\nXLM-R: XLM-R [ 60] trained with a multilingual\nmasked language model. Providing various unique\ntraining procedures enhances the performance of BERT.\nThese include (1) training the model for a more extended\nperiod with more data, (2) training with larger batch\nsizes and more extensive sequences, (3) dynamically\n3https://huggingface.co/transformers/\n8 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319455\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nDas et al.: Emotion Classiﬁcation\nconstructing the masking pattern, and so on. The XLM-\nR model signiﬁcantly surpasses other multilingual BERT\nmodels, especially in low-resource languages. The ‘xlm-\nRoberta-base technique is implemented on BEmoC-v3\nusing a batch size of 12.\nBangla BERT: This work uses two variants of\nBangla Bert that are dedicatedly pre-trained in the Ben-\ngali language only. The ﬁrst one is ‘sagorsarker/Bangla-\nbert-base’(hereafter called Bangla-BERT-1) [ 61] that\nis trained on Bengali corpus from OSCAR 4 and Ben-\ngali Wikipedia Dump Dataset 5. Another one is ‘cse-\nbuetnlp/banglabert’(hereafter called Bangla-BERT-2)\n[7]. Both pre-trained models are based on mask language\nmodeling described in the original BERT paper [ 53].\nIndic-DistilBERT: We implemented ‘indic-transfor\nmers-bn-distilbert’ on BEmoC-v3 and ﬁne-tuned it to\nacquire adequate performance. The Indic Distilbert [ 62]\nis pre-trained on three main Indian languages (Hindi,\nBengali, and Telugu), on which the amount of Bengali\ndata is around 6 GB. We ﬁne-tuned all the models on\nBEmoC-v3 using the Ktrain’ auto ﬁt’ technique. All\nmodels are trained for 20 epochs using a learning rate of\n2e−5 with a batch size of 12. Model weights are saved\nat checkpoints, and the most acceptable model is chosen\naccording to its performance on the validation set. The\nmaximum sequence length for the texts settled at 50\nwords.\n4) Ensemble-based Approach:\nAfter individually deploying the pre-trained transformer\nmodels, we approached the transformers’ ensemble. In\nrecent years, the ensemble of transformer models proved\nto be more eﬃcient than the individual ones [ 63]–[65].\nWe performed the ensembling to consider all possible\ncombinations of classiﬁer models using the weighted\naverage and average ensemble techniques. The weighted\naverage ensembles have speciﬁc eﬀects on the ensembled\noutcome since the primary results of the base models can\ninﬂuence the ensemble outcomes. As a result, the best-\nperforming model takes precedence over the others. On\nthe other hand, the average ensemble takes the softmax\nprobabilities of all the participating models and averages\nthem. The output class in this averaging is the one with\nthe highest probability. Prior base classiﬁer results are\nnot taken into account in this strategy [ 66], [ 67].\nThe framework of the ensemble of transformer-based\nBengali TxtEC (i.e., TEmoX) is depicted in Fig. 5. The\ndataset is ﬁrst sent to the BERT tokenizer, and the\ntokens are passed to the embedding layer (E) of each\nmodel. After passing the intermediate representation\nlayers, a contextual representation (T) is achieved. Fi-\nnally, a softmax probability distribution over the emo-\ntion classes is obtained. The probabilities are passed to\n4https://oscar-corpus.com/\n5https://dumps.wikimedia.org/bnwiki/latest/\na combination generator to generate the ensemble sets.\nEq. 1 is used to determine the total number of ensemble\nsets generated from the combination generator.\nC(m, r) = m!\nr!(m −r)! (1)\nHere, C(m, r) returns the number of total combinations,\nm represents the number of transformer models and r is\nthe number of choosing models for the ensemble. For our\ntask m = 5 and r = 2, 3, 4, 5, as we will be generating\ncombinations of 2, 3, 4, and 5 models respectively. The\nformula can be rewritten as follows:\nC(m5, r2,3,4,5) = 5!\n2!(5 −2)! + 5!\n3!(5 −3)!\n+ 5!\n4!(5 −4)! + 5!\n5!(5 −5)!\n= 10 + 10 + 5 + 1\n= 26\nThe 26 diﬀerent combinations of the transformer\nmodels are named from EN-1 to EN-26. The proba-\nbilities from each variety are now passed to the ‘Average\nEnsemble Algorithm’ to get the output class.\nLet us assume that we have ‘d’ test instances and the\nnumber of transformer models is ‘m’ . Each model classi-\nﬁes an instance di into one of the pre-deﬁned categories\nfrom nclass. Thus for each instance di, a model mj gives\na softmax probability distribution vector(prb[]) of size\nnclass. Thus, the output becomes:\nprb11, prb21, prb31, prb41, ......, prbd1\nprb12, prb22, prb32, prb42, ......, prbd2\n. . .\nprb1m, prb21, prb3m, prb4m, ......, prbdm\nIn the average ensemble technique the average of\neach softmax class value provided by ‘m’ models is\ncalculated for each instance. Finally, the maximum of\nthe probabilities is used to compute the output class\nusing Eq. 2.\nO = argmax(\n∀i∈(1,d)\n∑m\nj=1 prbij[]\nnclass\n) (2)\nHere, the argmax function returns the class index of the\nmaximum of the probabilities. The ‘Average Ensemble’\nalgorithm is brieﬂy described in Algorithm 1.\nThe weighted average ensemble technique utilizes an\nextra weight with the softmax probabilities of the mod-\nels. Given the prior weighted f1-scores of ‘m’ models,\ni.e., wf1, wf2, ...wfm, the algorithm uses Eq. 3 to\ncompute the outputs.\nO = argmax(\n∀i∈(1,d)\n∑m\nj=1 prbij[] ∗wfj\n∑m\nj=1 wfj\n) (3)\nHere, wfj denotes the weighted F1 score of each model.\nThe ‘Weighted Average Ensemble’ algorithm is brieﬂy\ndescribed in Algorithm 2.\nVOLUME 4, 2016 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319455\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nDas et al.: Emotion Classiﬁcation\nFigure 5. Framework for the ensemble of transformer-based Bengali TxtEC. Ei denotes the input embedding for T OKi, Ti represents the contextual\nrepresentation for each T OKi. EN-1 to EN-26 are the 26 diﬀerent ensemble combinations made from the 5 transformer models.\nAlgorithm 1:Average Ensemble Algorithm\nm[ ]←models\nd[ ]←test instances\nprb[ ]←softmax probabilities\nsum[ ]←summed probabilities\navg[ ]←average probabilities\nnclass ←total number of classes\nfor (i ∈(1, d) ) {\nfor (j ∈(1, m) ) {\nsumi = sumi + prbij[ ];\nj = j + 1;\n}\navgi = sumi\nnclass\n;\ni = i + 1;\n}\nO = argmax(avg) //Output class indices\nV. EXPERIMENTS\nThe entire experiment is conducted in a multicore\nprocessor furnished with NVIDIA Geforce GTX 960M\nGPU with 4GB graphics memory, 8GB physical memory\n(RAM), and a 2.3GHz intel core i5 processor. Scikit-\nlearn (0.24.2) package is used to develop the ML models.\nThe Ktrain (0.26.3) package is employed to train the\ntransformer models. Statistical measures such as preci-\nsion (Pr), accuracy (Acc), recall (Re), and F1-score are\nutilized to assess and compare the models performance.\nKeras (2.4.3) with Tensorﬂow (2.5.0) backend is utilized\nAlgorithm 2: Weighted Average Ensemble Al-\ngorithm\nm[ ]←models\nd[ ]←test instances\nprb[ ]←softmax probabilities\nsum[ ]←summed probabilities\nwf [ ]←weighted f1 scores\nnclass ←total number of classes\nfor (i ∈(1, d) ) {\nfor (j ∈(1, m) ) {\nsumi = sumi + prbij[ ]* wfj;\nj = j + 1;\n}\ni = i + 1;\n}\nn_sum=0;\nfor (j ∈(1, l) ) {\nn_sum = n_sum + wfj;\nj = j + 1;\n}\nP = sum/n_sum;\n//Normalized P robabilities;\nO = argmax(avg) //Output class indices\nas the DL framework. Python version is kept at v3.6 for\nall the experiments.\n10 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319455\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nDas et al.: Emotion Classiﬁcation\nTable 7. Performance of ML, DL and Transformer based models on the test set\nMethod Classiﬁer Pr(%) Re(%) F1(%) Acc(%)\nML models\nLR + TF-IDF 68.52 67.84 68.06 67.84\nSVM + TF-IDF 69.66 64.32 64.81 64.32\nRF + TF-IDF 65.33 60.64 60.57 60.64\nMNB + TF-IDF 69.25 59.68 57.53 59.68\nLR + BOW 66.18 65.76 65.91 65.76\nSVM + BOW 61.19 58.56 59.14 58.56\nRF + BOW 65.67 59.84 60.51 59.84\nMNB + BOW 66.83 64.32 64.12 64.32\nDL Models\nCNN (Word2Vec) 61.72 58.56 59.00 58.56\nCNN (FastText) 59.31 54.24 55.86 54.24\nCNN (GloVe) 59.41 59.66 58.39 59.66\nBiLSTM (Word2Vec) 61.97 61.12 61.28 61.12\nBiLSTM (FastText) 62.03 60.16 60.55 60.16\nBiLSTM (GloVe) 63.17 63.20 63.34 63.20\nCNN + BiLSTM (Word2Vec) 63.78 62.24 62.23 62.24\nCNN + BiLSTM (FastText) 60.11 59.36 59.43 59.36\nCNN + BiLSTM (GloVe) 63.52 63.76 63.39 63.76\nTransformers\nm-BERT 63.45 61.76 62.25 61.76\nXLM-R 72.68 72.64 72.54 72.64\nBangla-BERT-1 68.50 68.16 68.24 68.16\nBangla-BERT-2 75.02 74.72 74.77 74.72\nIndic-DistilBERT 77.11 77.12 77.11 77.12\nA. RESULTS\nThe evaluation results of individual models on the test\nset are presented in Table 7, with the excellence of\nthe models determined based on the weighted F1-score.\nThe results of the ensemble-based approaches based\non various combinations of the transformer models are\nreported in Table 8.\n1) ML-based Approach\nResults revealed that LR with TF-IDF features earned\nthe most elevated F 1-score (68.06%) among the ML\napproaches, surpassing SVM (64.81%), RF (60.57%) and\nMNB (57.53%). On the other hand, ML models with\nBOW features performed slightly less F 1-score than TF-\nIDF. In addition, LR with TF-IDF outperformed other\nML models concerning the highest P r (68.52%), Re\n(67.84%), and Acc (67.84%) measures.\n2) DL-based Approach\nIn relation to all of the evaluation parameters in DL\nmodels, CNN+BiLSTM with GloVe outperformed the\nremaining DL-based models by obtaining the highest\nF 1-score of 63.39%, which is approximately 4.67% lower\nthan the best ML approach (i.e., LR + TF-IDF).\n3) Transformer-based Approach\nThere is a considerable proliferation in all scores re-\ngarding the transformer-based models. The m-BERT\nacquired the lowest F 1-score among all transformer-\nbased models, only 62.25%, which is even lower than the\nbest-performing ML model. The Bangla-BERT-1, on\nthe other hand, has a nearly 5.99% higher F 1-score (e.g.,\n68.24 %) than m-BERT. Compared to Bangla-BERT-1\nand m-BERT, the XLM-R model signiﬁcantly improved\nand obtained a F 1-score of 72.54%. The Bangla-\nBERT-2 model was solely developed for the Bengali\nlanguage with a larger corpus size than Bangla-BERT-\n1. As expected, Bangla-BERT-2 has outperformed the\nBangla-BERT-1 and the aforementioned transformer-\nbased models with an F 1-score of 74.77%. Among all\nmodels, the Indic-DistilBERT obtained the highest F 1-\nscore of 77.11%. This model beats the m-BERT, XLM-\nR, Bangla-BERT-1, and Bangla-BERT-2 by 14.68%,\n4.57%, 8.87%, and 2.34% F 1-score, respectively. The\nbest-performing model card is available at the hugging-\nface model hub 6.\n4) Ensemble-based Approaches\nAfter analyzing the individual model’s (i.e., base mod-\nels) performance, we analyzed the ensemble of pre-\ntrained transformers. Table 8 illustrates the evaluation\nscores of various ensemble sets on the test data concern-\ning the average and weighted ensembles.\nResults of ensembling indicate that the ensemble set\nEn-22 utilizing the weighted-average ensemble approach\ndemonstrated the best performance in terms of the\nhighest precision (80.45%), recall (80.16%), and F1-\nscore (80.24%). Thus, the result conﬁrms that among\na total of 38 models, the weighted-average ensemble\nmodel (Bangla-BERT-2 + XLM-R + Indic-DistilBERT\n+ Bangla-BERT-1) is the best-performing model to\nclassify textual emotion in Bengali. Therefore, it is to\nbe called TEmoX.\nB. ERROR ANALYSIS\nTable 8 demonstrated that ensemble set En-22 (Bangla-\nBERT-2 + XLM-R + Indic-DistilBERT + Bangla-\n6https://huggingface.co/avishek-018/bn-emotion-temox\nVOLUME 4, 2016 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319455\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nDas et al.: Emotion Classiﬁcation\nTable 8. Performance of Ensemble-based models. Here the combination of 2, 3, 4, and 5 models are shown separately with their average/weighted Pr, Re, and\nF1-score\nEnsemble\nSets\nModels Weighted Ensemble Average Ensemble\nPr(%) Re(%) F1(%) Pr(%) Re(%) F1(%)\nEN-1 Bangla-BERT-2, XLM-R 74.19 73.92 73.93 74.82 74.4 74.46\nEN-2 Bangla-BERT-2, Indic-DistilBERT 77.14 76.96 76.99 77.74 77.44 77.49\nEN-3 Bangla-BERT-2, mBERT 73.55 73.28 73.3 72.76 72.16 72.26\nEN-4 Bangla-BERT-2, Bangla-BERT-1 74.21 73.76 73.87 75.17 74.88 74.97\nEN-5 XLM-R, Indic-DistilBERT 76.52 76.48 76.5 75.79 75.68 75.71\nEN-6 XLM-R, mBERT 72.32 72.32 72.29 69.85 69.44 69.56\nEN-7 XLM-R, Bangla-BERT-1 75.06 74.88 74.92 74.02 73.92 73.95\nEN-8 Indic-DistilBERT, mBERT 75.82 75.52 75.61 72.86 72.32 72.43\nEN-9 Indic-DistilBERT, Bangla-BERT-1 75.90 75.68 75.74 74.87 74.72 74.76\nEN-10 mBERT, Bangla-BERT-1 69.13 68.64 68.8 68.31 67.84 67.91\nEN-11 Bangla-BERT-2, XLM-R, Indic-DistilBERT 79.15 79.04 79.07 78.70 78.56 78.60\nEN-12 Bangla-BERT-2, XLM-R, mBERT 76.08 75.68 75.79 75.75 75.36 75.47\nEN-13 Bangla-BERT-2, XLM-R, Bangla-BERT-1 78.12 77.92 77.98 77.82 77.60 77.67\nEN-14 Bangla-BERT-2, Indic-DistilBERT, mBERT 78.26 77.92 77.99 77.88 77.60 77.66\nEN-15 Bangla-BERT-2, Indic-DistilBERT, Bangla-BERT-1 78.59 78.40 78.47 79.21 79.04 79.09\nEN-16 Bangla-BERT-2, mBERT, Bangla-BERT-1 74.47 74.08 74.15 74.36 73.76 73.85\nEN-17 XLM-R, Indic-DistilBERT, mBERT 76.75 76.48 76.56 75.37 75.20 75.23\nEN-18 XLM-R, Indic-DistilBERT, Bangla-BERT-1 76.95 76.8 76.84 76.89 76.80 76.83\nEN-19 XLM-R, mBERT, Bangla-BERT-1 74.58 74.40 74.42 74.18 73.92 73.97\nEN-20 Indic-DistilBERT, mBERT, Bangla-BERT-1 75.14 74.88 74.94 74.81 74.56 74.61\nEN-21 Bangla-BERT-2, XLM-R, Indic-DistilBERT, mBERT 78.53 78.24 78.32 77.83 77.60 77.64\nEN-22 Bangla-BERT-2, XLM-R, Indic-DistilBERT,\nBangla-BERT-1 80.45 80.16 80.24 79.54 79.20 79.29\nEN-23 Bangla-BERT-2, XLM-R, mBERT, Bangla-BERT-1 77.23 76.80 76.91 76.58 76.16 76.24\nEN-24 Bangla-BERT-2, Indic-DistilBERT, mBERT,\nBangla-BERT-1 78.20 77.76 77.9 76.59 76.16 76.27\nEN-25 XLM-R, Indic-DistilBERT, mBERT, Bangla-BERT-1 76.98 76.80 76.87 76.56 76.32 76.40\nEN-26 Bangla-BERT-2, XLM-R, Indic-DistilBERT, mBERT,\nBangla-BERT-1 78.54 78.24 78.33 78.20 77.92 77.99\nBERT-1) is the best-performing ensemble model for\nclassifying textual emotion in Bengali, as evidenced\nby its high performance. A detailed error analysis\nis conducted to gain additional insights regarding the\nperformance of the proposed method.\n1) Quantitative Analysis\nFig. 6 depicts the class-wise fraction of predicted labels\nrelating to the confusion matrix.\nFigure 6. Confusion matrix of the proposed ensemble model (En-22).\nThe confusion matrix revealed that a few data in-\nstances are not classiﬁed correctly. Among the 76 anger\ninstances, 9 were predicted to be disgust. The same\nscenario can be noticed in the disgust class, where 11\nTable 9. Error rate of various approaches on the test set\nMethod Classiﬁer Error\nRate(%)\nML\nmodels\nLR + TF-IDF 32.16\nRF + TF-IDF 35.68\nMNB + TF-IDF 39.36\nSVM + TF-IDF 40.32\nLR + BOW 34.24\nRF + BOW 41.44\nMNB + BOW 40.16\nSVM + BOW 35.68\nDL\nmodels\nCNN (Word2Vec) 41.44\nCNN (FastText) 45.76\nCNN (GloVe) 40.34\nBiLSTM (Word2Vec) 38.88\nBiLSTM (FastText) 39.84\nBiLSTM (GloVe) 36.80\nCNN + BiLSTM (Word2Vec) 37.76\nCNN + BiLSTM (FastText) 40.64\nCNN + BiLSTM (GloVe) 36.24\nTransformers\nm-BERT 38.24\nXLM-R 27.36\nBangla-BERT-1 31.84\nBangla-BERT-2 25.28\nIndic-DistilBERT 22.88\nEnsemble of-\nTransformers\nAverage Ensemble\n(Proposed) 19.84\n12 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319455\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nDas et al.: Emotion Classiﬁcation\nTable 10. Data samples demonstrating the diﬀering nature of transformer models. Here MB, XR, BB1, BB2 and IDB refers to m-BERT, XLM-R,\nBangla-BERT-1, Bangla-BERT-2 and Indic-DistilBERT. A denotes the actual label and the wrong predictions marked in bold\nSample A MB XR BB1 BB2 IDB Proposed\nশ‍ুিটেয়লালকেরেদয়াদরকার।আহাƍকেদরএই\nিহেপােËটেমাƝােদরজনয্বািকমুসলমানরািবপেদ\nআেছ। (They should be beaten hard. The\nrest of the Muslims are in danger for these\nidiot hippocratic mullahs.)\nAnger Anger Anger Sadness Anger Anger Anger\nদূরেথেকমাকড়শােদখেল,আমারĄেকসুড়সুিড়\nঅনুভবকিরএবংিনেজরঅজােťইভীতহেয়উিঠ।\n(When I see a spider from a distance, I feel\na tingling sensation on my skin and I get\nscared unknowingly.)\nFear Sadness Surprise Fear Fear Fear Fear\nেবহায়াগ‍ুেলািমিডয়ারমুখেদখােবিকভােব? তারা\nেতাপািকƳােনেঘাড়ািডমেপেড়এেসেছ। (How\nwill those shameless face the media? They\nbuilt mare’s nest in Pakistan)\nAnger Disgust Anger Disgust Anger Disgust Anger\nিবđাসকরেতপািরএমনকােরাসােথভয়পাওয়ার\nবয্াপারিনেয়কথাবǬল। (I talked to someone I\ntrusted about me being scared.)\nFear Fear Fear Fear Surprise Fear Fear\nআজীবনসাজাàাźদŌàাźআসামীরমতবড়একা\nআিমবড়একা (I am as big as a convict\nsentenced to life)\nSadness Joy Sadness Sadness Sadness Sadness Sadness\ninstances are classiﬁed as anger. 9 out of 87 data\ninstances in the fear class were incorrectly classiﬁed as\nsadness whereas 7 out of 119 data points in the sadness\nclass were incorrectly classiﬁed as fear. Furthermore, out\nof 72 data points in the surprise class, 11 are predicted\nto be joy. The misclassiﬁcation ratio is the highest in\nthis class. The reasons for these misclassiﬁcations can\nbe explained by the Jaccard similarity of the corpus\n(Table 3). Overlapping of the most frequent words\ncan hamper the classiﬁcation task. Also, an anger\nemotion can sometimes be expressed as disgust and thus\ntheir sentence pattern can have similarities too. This\nphenomenon remains true for the other two class pairs\ntoo ( joy-surprise and sadness-fear). The error analysis\nreveals that the disgust class achieved the highest rate\nof correct classiﬁcation (82.05%), while the surprise\nclass achieved the lowest rate of correct classiﬁcation\n(61.84%). Table 9 presents the error rate for various\napproaches where the proposed ensemble of transformers\nachieved the lowest error rate of 19.84%.\n2) Qualitative Analysis\nTable 10 presents the predicted labels for certain in-\nstances by the utilized transformer models in compar-\nison to their actual labels. It’s observed that while\none model correctly predicts the label for a sample,\nthe other does not. The ensemble method (En-22)\naddresses these inconsistencies by averaging the softmax\nprobability distribution for each model and then mak-\ning predictions based on the highest weighted-average\nprobability. Nonetheless, classifying texts with similar\nwords spread across diﬀerent classes remains a hurdle,\nleading to higher misclassiﬁcation rates in some models.\nA contextual review of such texts might pave the way\nfor improved classiﬁcation models.\nA high degree of class imbalance in the used corpus\nmight be a probable cause for inaccurate predictions.\nAlso, some words often appear across various contexts\nand multiple classes. Interestingly, the elevated Jaccard\nindex value (Table 3) reveals certain patterns. For\ninstance, derogatory terms might reﬂect both anger and\ndisgust emotions. The same scenario can be observed\nin the case of joy and surprise classes as a surprising\nincident might result in a positive outcome. Moreover,\nthe classiﬁcation of emotions is inherently subjective,\nand inﬂuenced by individual perspectives. A single\nstatement could be interpreted in multiple ways based\non personal inclinations.\nC. COMPARISON WITH EXISTING TECHNIQUES\nThe analysis of results demonstrated that the ensemble\nmethod, En-22, emerged as the most eﬀective model for\ncategorizing textual emotions in Bengali. We further\nevaluated the eﬀectiveness of the proposed model by\ncomparing its performance to that of existing tech-\nniques. Some past techniques [ 4], [ 25], [ 30]–[34] were\nimplemented and evaluated on the BEmoC-v3. Table\n11 shows the results of the comparisons. We can see\nthe best performing model (i.e. TEmoX) outperformed\nthe previous models and achieved the highest f1-score\nof 80.24%. Moreover, to exhibit the generalizability\nof the proposed technique, we experimented with its\nperformance on another Bengali emotion dataset [ 31]\n(Dataset 2). This dataset consists of 6314 Facebook\ncomments annotated with six emotion classes.\nThe comparative analysis exhibits (in Table 11) that\nthe suggested technique is more robust than the existing\ntechniques for classifying textual emotion in Bengali.\nAlthough Dataset 2 showed a relatively poor perfor-\nmance than BEmoC-v3, it performed better in the\nVOLUME 4, 2016 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319455\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nDas et al.: Emotion Classiﬁcation\nTable 11. Summary of the performance comparison\nApproaches F1(%)\nBEmoC-v3 Dataset 2 [ 31]\nTF-IDF + MNB [ 30] 57.53 40.42\nWord2Vec + LSTM [ 25] 61.28 32.02\nHeuristic features + CRF [ 33] 56.45 34.56\nBOW + SVM [ 34] 59.14 52.55\nTF-IDF + LR [ 32] 68.06 50.56\nSVM + TF-IDF [ 31] 64.81 47.61\nXLM-R [ 4] 72.54 52.32\nEn-22 ( Proposed) 80.24 56.45\nproposed method than in the past techniques. Some\npossible reasons might impact the unsatisfactory perfor-\nmance on Dataset 2. For further investigations, we have\ninvestigated the Jaccard Index of Dataset 2 and the con-\nfusion matrix of the proposed model. Table 12 presents\nthe Jaccard Index, which showcases the overlapping of\nthe most frequent words among the classes. The analysis\nconsiders the top 200 most frequent words from each\nemotion class to determine the degree of overlap. Thus,\nit is evident that most class pairs have a similarity above\n50%, which causes a high chance of misclassiﬁcation.\nTable 12. Interclass Jaccard similarity index. Anger (CL1), Disgust (CL2),\nFear (CL3), Joy (CL4), Sadness (CL5), Surprise (CL6)\nCL1 CL2 CL3 CL4 CL5 CL6\nCL1 1.00 0.47 0.51 0.40 0.56 0.47\nCL2 - 1.00 0.52 0.43 0.54 0.52\nCL3 - - 1.00 0.47 0.54 0.50\nCL4 - - - 1.00 0.45 0.41\nCL5 - - - - 1.00 0.50\nFigure 7. Confusion matrix of the proposed ensemble transformer models on\nDataset-2.\nTable 13. Results of the proposed method on diﬀerent versions of BEmoC\nMethods F1(%)\nEn-22\n(Proposed)\nBEmoC-v1\n[68] BEmoC-v2 [ 4] BEmoC-v3 [ 8]\n69.45 76.95 80.24\nIn the confusion matrix in Fig. 7, it can be noticed\nthat the number of misclassiﬁcation is higher than the\ncorrect prediction in the disgust, fear, and surprise\nclasses. The overlapping of the most frequent words\namong classes might be the reason for such performance.\nMoreover, there is no apparent justiﬁcation for the con-\nstruction of the dataset. Therefore, the quality indices\nof Dataset 2 might also inﬂuence the performance.\nD. PERFORMANCE OF THE PROPOSED MODEL ON\nBEMOC DATSETS\nThe previous work [ 68] utilized a Bengali emotion\ndataset called BEmoC-v1 containing 5200 texts, whereas\nDas et al. [ 4] used an extended version of BEmoC-v1\n(renamed as BEmoC-v2 having 6243 data. Later the\ndataset is extended again, containing 7000 texts, and\nwe called it BEmoC-v3. To investigate the performance,\nwe evaluated the proposed model on the three versions\nof BEmoC (v1-v3). The datasets are partitioned into\nthe train, test, and validation sets, where the test set\nis kept identical so that it can assess the inﬂuence of\nincreasing train data relatively. Table 13 illustrates\nthe performance of the proposed model on the diﬀerent\nversions of BEmoC. The analysis demonstrates that\nBEmoC-v1 (F1-score = 69.45%) performed relatively\nlower than BEmoC-v2 (F1-score = 76.96%) as it has\nfewer data. On the other hand, it clearly shows that\nBEmoC-v3 performed better by achieving the highest\nF1-score (80.24%) than BEmoC-v2 and BEmoC-v1 due\nto its more signiﬁcant amount of text data.\nVI. CONCLUSION\nThis research introduces a Transformer-based frame-\nwork called TEmoX, designed to identify textual emo-\ntions within Bengali text across six distinct categories.\nThe eﬀectiveness of this model is evaluated using a\nnewly constructed dataset known as BEmoC-v3. The\nresults demonstrate that the proposed approach, which\ninvolves a weighted ensemble of XLM-R, Bangla-BERT-\n1, Bangla-BERT-2, and Indic-DistilBERT, achieved\noutstanding performance, boasting an impressive F1-\nscore of 80.24%, outperforming all baseline models\nand existing techniques for classifying textual emotions\nin Bengali. Notably, the proposed approach exhibits\na substantial improvement of 12.18%, 16.85%, and\n3.13% compared to the best-performing machine learn-\ning (ML), deep neural network (DNN), and transformer-\nbased baseline models. Looking ahead, this research\nendeavors to broaden its scope by identifying additional\nemotion categories, such as love, hate, and stress, while\nalso incorporating more diverse data into the BEmoC-v3\ncorpus. Additionally, future investigations will explore\nthe applicability of the proposed model in detecting emo-\ntions expressed through emoticons, code-mixing or code-\nswitching data, and texts containing mixed emotions.\nWe also intend to explore the models performance in\n14 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319455\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nDas et al.: Emotion Classiﬁcation\nclassifying multiclass or multilabel textual emotions in\nother low-resource languages, which holds promise for\nbroadening its practical applications.\nReferences\n[1] K. Garg and D. Lobiyal, “Hindi emotionnet: A scalable emotion\nlexicon for sentiment classiﬁcation of hindi text,” ACM Tran.\non Asian & Low-Resource Language Information Processing ,\nvol. 19, no. 4, pp. 1–35, 2020.\n[2] A. Magueresse, V. Carles, and E. Heetderks, “Low-resource\nlanguages: A review of past work and future challenges,” arXiv\npreprint arXiv:2006.07264 , 2020.\n[3] T. Parvin, O. Sharif, and M. M. Hoque, “Multi-class textual\nemotion categorization using ensemble of convolutional and\nrecurrent neural network,” SN Com. Sci. , vol. 3, no. 62, 2022.\n[4] A. Das, O. Sharif, M. M. Hoque, and I. H. Sarker,\n“Emotion classiﬁcation in a resource constrained language\nusing transformer-based approach,” in Proceedings of the 2021\nConference of the North American Chapter of the Association\nfor Computational Linguistics: Student Research Workshop .\nOnline: Association for Computational Linguistics, Jun. 2021,\npp. 150–158. [Online]. Available: https://aclanthology.org/2021.\nnaacl-srw.19\n[5] A. Wadhawan and A. Aggarwal, “Towards emotion recognition\nin Hindi-English code-mixed data: A transformer based\napproach,” in Proceedings of the Eleventh Workshop on\nComputational Approaches to Subjectivity, Sentiment and\nSocial Media Analysis . Online: Association for Computational\nLinguistics, Apr. 2021, pp. 195–202. [Online]. Available:\nhttps://aclanthology.org/2021.wassa-1.21\n[6] J.-B. Delbrouck, N. Tits, M. Brousmiche, and S. Dupont,\n“A transformer-based joint-encoding for emotion recognition\nand sentiment analysis,” in Second Grand-Challenge and\nWorkshop on Multimodal Language (Challenge-HML) . Seattle,\nUSA: Association for Computational Linguistics, Jul. 2020,\npp. 1–7. [Online]. Available: https://aclanthology.org/2020.\nchallengehml-1.1\n[7] A. Bhattacharjee, T. Hasan, K. Samin, M. S. Islam, M. S.\nRahman, A. Iqbal, and R. Shahriyar, “Banglabert: Combating\nembedding barrier in multilingual models for low-resource\nlanguage understanding,” CoRR, vol. abs/2101.00204, 2021.\n[Online]. Available: https://arxiv.org/abs/2101.00204\n[8] M. Iqbal, A. Das, O. Sharif, M. M. Hoque, and I. H. Sarker,\n“Bemoc: A corpus for identifying emotion in bengali texts,” SN\nComputer Science , vol. 3, no. 2, pp. 1–17, 2022.\n[9] V. L. Parsons, “Stratiﬁed sampling,” Wiley StatsRef: Statistics\nReference Online, pp. 1–11, 2014.\n[10] L. F. Barrett, “Discrete emotions or dimensions? the role of\nvalence focus and arousal focus,” Cognition & Emotion , vol. 12,\nno. 4, pp. 579–599, 1998.\n[11] P. Ekman, “An argument for basic emotions,” Cognition &\nemotion, vol. 6, no. 3-4, pp. 169–200, 1992.\n[12] N. Alswaidan and M. E. B. Menai, “A survey of state-of-the-\nart approaches for emotion recognition in text,” Knowledge and\nInformation Systems , pp. 1–51, 2020.\n[13] F. Calefato, F. Lanubile, and N. Novielli, “Emotxt: a toolkit for\nemotion recognition from text,” in 2017 seventh international\nconference on Aﬀective Computing and Intelligent Interaction\nWorkshops and Demos (ACIIW) . IEEE, 2017, pp. 79–80.\n[14] S. Alzu’bi, O. Badarneh, B. Hawashin, M. Al-Ayyoub, N. Al-\nhindawi, and Y. Jararweh, “Multi-label emotion classiﬁcation\nfor arabic tweets,” in 2019 Sixth International Conference on\nSocial Networks Analysis, Management and Security (SNAMS) .\nIEEE, 2019, pp. 499–504.\n[15] S. Ahmad, M. Z. Asghar, F. M. Alotaibi, and S. Khan, “Clas-\nsiﬁcation of poetry text into the emotional states using deep\nlearning technique,” IEEE Access , vol. 8, pp. 73 865–73 878,\n2020.\n[16] A. Mondal and S. S. Gokhale, “Mining emotions on plutchik’s\nwheel,” in 2020 Seventh International Conference on Social Net-\nworks Analysis, Management and Security (SNAMS) . IEEE,\n2020, pp. 1–6.\n[17] A. Yousaf, M. Umer, S. Sadiq, S. Ullah, S. Mirjalili, V. Rupa-\npara, and M. Nappi, “Emotion recognition by textual tweets\nclassiﬁcation using voting classiﬁer (lr-sgd),” IEEE Access ,\nvol. 9, pp. 6286–6295, 2021.\n[18] M. Hasan, E. Rundensteiner, and E. Agu, “Automatic emotion\ndetection in text streams by analyzing twitter data,” Interna-\ntional Journal of Data Science and Analytics , vol. 7, no. 1, pp.\n35–51, 2019.\n[19] Y. Lai, L. Zhang, D. Han, R. Zhou, and G. Wang, “Fine-\ngrained emotion classiﬁcation of chinese microblogs based on\ngraph convolution networks,” World Wide Web , vol. 23, no. 5,\npp. 2771–2787, 2020.\n[20] D. Haryadi and G. P. Kusuma, “Emotion detection in text using\nnested long short-term memory,” IJACSA) International Jour-\nnal of Advanced Computer Science and Applications , vol. 10,\nno. 6, 2019.\n[21] A. Chatterjee, K. N. Narahari, M. Joshi, and P. Agrawal,\n“Semeval-2019 task 3: Emocontext contextual emotion detection\nin text,” in Proceedings of the 13th International Workshop on\nSemantic Evaluation , 2019, pp. 39–48.\n[22] I. Ameer, G. Sidorov, H. Gómez-Adorno, and R. M. A. Nawab,\n“Multi-label emotion classiﬁcation on code-mixed text: Data and\nmethods,” IEEE Access, vol. 10, pp. 8779–8789, 2022.\n[23] P. Kumar and B. Raman, “A bert based dual-channel explain-\nable text emotion recognition system,” Neural Networks , vol.\n150, pp. 392–407, 2022.\n[24] R. Guarasci, S. Silvestri, G. De Pietro, H. Fujita, and M. Es-\nposito, “Bert syntactic transfer: A computational experiment\non italian, french and english languages,” Computer Speech &\nLanguage, vol. 71, p. 101261, 2022.\n[25] N. I. Tripto and M. E. Ali, “Detecting multilabel sentiment and\nemotions from bangla youtube comments,” in 2018 International\nConference on Bangla Speech and Language Processing (ICB-\nSLP). IEEE, 2018, pp. 1–6.\n[26] M. Rahib, R. H. Khan, A. H. Tamim, M. Z. Tahmeed, and M. J.\nHossain, “Emotion detection based on bangladeshi peoples social\nmedia response on covid-19,” SN Computer Science , vol. 3, no. 2,\npp. 1–6, 2022.\n[27] S. A. Purba, S. Tasnim, M. Jabin, T. Hossen, and M. K. Hasan,\n“Document level emotion detection from bangla text using\nmachine learning techniques,” in 2021 International Conference\non Information and Communication Technology for Sustainable\nDevelopment (ICICT4SD) . IEEE, 2021, pp. 406–411.\n[28] M. M. R. Mamun, O. Sharif, and M. M. Hoque, “Classiﬁcation\nof textual sentiment using ensemble technique,” SN Computer\nScience, vol. 3, no. 1, p. 49, Nov 2021. [Online]. Available:\nhttps://doi.org/10.1007/s42979-021-00922-z\n[29] M. M. Rayhan, T. Al Musabe, and M. A. Islam, “Multilabel\nemotion detection from bangla text using bigru and cnn-bilstm,”\nin 2020 23rd International Conference on Computer and Infor-\nmation Technology (ICCIT) . IEEE, 2020, pp. 1–6.\n[30] S. Azmin and K. Dhar, “Emotion detection from bangla text\ncorpus using naïve bayes classiﬁer,” in 2019 4th International\nConference on Electrical Information and Communication Tech-\nnology (EICT) . IEEE, 2019, pp. 1–5.\n[31] M. A. Rahman and M. H. Seddiqui, “Comparison of classical ma-\nchine learning approaches on bangla textual emotion analysis,”\n2019.\n[32] A. Pal and B. Karn, “Anubhuti–an annotated dataset for\nemotional analysis of bengali short stories,” arXiv preprint\narXiv:2010.03065, 2020.\n[33] D. Das and S. Bandyopadhyay, “Word to sentence level emotion\ntagging for bengali blogs,” in Proceedings of the ACL-IJCNLP\n2009 Conference Short Papers , 2009, pp. 149–152.\n[34] H. A. Ruposh and M. M. Hoque, “A computational approach\nof recognizing emotion from bengali texts,” in 2019 5th In-\nternational Conference on Advances in Electrical Engineering\n(ICAEE). IEEE, 2019, pp. 570–574.\n[35] T. Parvin, O. Sharif, and M. M. Hoque, “Multi-class\ntextual emotion categorization using ensemble of convolutional\nand recurrent neural network,” SN Computer Science ,\nvol. 3, no. 1, p. 62, Nov 2021. [Online]. Available:\nhttps://doi.org/10.1007/s42979-021-00913-0\nVOLUME 4, 2016 15\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319455\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nDas et al.: Emotion Classiﬁcation\n[36] D. Das, S. Roy, and S. Bandyopadhyay, “Emotion tracking on\nblogs-a case study for bengali,” in International Conference\non Industrial, Engineering and Other Applications of Applied\nIntelligent Systems . Springer, 2012, pp. 447–456.\n[37] F. Alam, M. A. Hasan, T. Alam, A. Khan, J. Tajrin,\nN. Khan, and S. A. Chowdhury, “A review of bangla natural\nlanguage processing tasks and the utility of transformer\nmodels,” CoRR, vol. abs/2107.03844, 2021. [Online]. Available:\nhttps://arxiv.org/abs/2107.03844\n[38] T. Alam, A. Khan, and F. Alam, “Bangla text classiﬁcation\nusing transformers,” CoRR, vol. abs/2011.04446, 2020. [Online].\nAvailable: https://arxiv.org/abs/2011.04446\n[39] O. Sharif and M. M. Hoque, “Identiﬁcation and classiﬁcation of\ntextual aggression in social media: Resource creation and evalua-\ntion,” in Combating Online Hostile Posts in Regional Languages\nduring Emergency Situation , T. Chakraborty, K. Shu, H. R.\nBernard, H. Liu, and M. S. Akhtar, Eds. Cham: Springer\nInternational Publishing, 2021, pp. 9–20.\n[40] R. J. May, H. R. Maier, and G. C. Dandy, “Data splitting for\nartiﬁcial neural networks using som-based stratiﬁed sampling,”\nNeural Networks , vol. 23, no. 2, pp. 283–294, 2010.\n[41] T. Tokunaga and I. Makoto, “Text categorization based on\nweighted inverse document frequency,” in Special Interest\nGroups and Information Process Society of Japan (SIG-IPSJ .\nCiteseer, 1994.\n[42] Y. Zhang, R. Jin, and Z.-H. Zhou, “Understanding bag-of-\nwords model: a statistical framework,” International Journal\nof Machine Learning and Cybernetics , vol. 1, no. 1-4, pp. 43–52,\n2010.\n[43] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Eﬃcient esti-\nmation of word representations in vector space,” arXiv preprint\narXiv:1301.3781, 2013.\n[44] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean,\n“Distributed representations of words and phrases and their\ncompositionality,” in Proceedings of the 26th International Con-\nference on Neural Information Processing Systems - Volume 2 ,\nser. NIPS’13. Red Hook, NY, USA: Curran Associates Inc.,\n2013, p. 31113119.\n[45] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. Jégou,\nand T. Mikolov, “Fasttext. zip: Compressing text classiﬁcation\nmodels,” arXiv preprint arXiv:1612.03651 , 2016.\n[46] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, “Enriching\nword vectors with subword information,” Transactions of the\nAssociation for Computational Linguistics , vol. 5, pp. 135–146,\n2017.\n[47] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global\nvectors for word representation,” in Proceedings of the 2014\nconference on empirical methods in natural language processing\n(EMNLP), 2014, pp. 1532–1543.\n[48] S. Sarker, “Bnlp: Natural language processing toolkit for bengali\nlanguage,” arXiv preprint arXiv:2102.00405 , 2021.\n[49] D. J. C. MacKay, Hyperparameters: Optimize, or Integrate Out?\nDordrecht: Springer Netherlands, 1996, pp. 43–59. [Online].\nAvailable: https://doi.org/10.1007/978-94-015-8729-7_2\n[50] J. Bergstra and Y. Bengio, “Random search for hyper-parameter\noptimization,” The Journal of Machine Learning Research ,\nvol. 13, no. 1, pp. 281–305, 2012.\n[51] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature,\nvol. 521, no. 7553, pp. 436–444, 2015.\n[52] S. Hochreiter and J. Schmidhuber, “Long short-term memory,”\nNeural computation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[53] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert:\nPre-training of deep bidirectional transformers for language\nunderstanding,” arXiv preprint arXiv:1810.04805 , 2018.\n[54] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”\n2017. [Online]. Available: https://arxiv.org/pdf/1706.03762.pdf\n[55] M. A. Hasan, J. Tajrin, S. A. Chowdhury, and F. Alam, “Sen-\ntiment classiﬁcation in bangla textual content: A comparative\nstudy,” in 2020 23rd International Conference on Computer and\nInformation Technology (ICCIT) , 2020, pp. 1–6.\n[56] T. Alam, A. Khan, and F. Alam, “Bangla text classiﬁcation\nusing transformers,” arXiv preprint arXiv:2011.04446 , 2020.\n[57] A. Kunchukuttan, D. Kakwani, S. Golla, A. Bhattacharyya,\nM. M. Khapra, P. Kumar et al. , “Ai4bharat-indicnlp corpus:\nMonolingual corpora and word embeddings for indic languages,”\narXiv preprint arXiv:2005.00085 , 2020.\n[58] A. S. Maiya, “ktrain: A low-code library for augmented machine\nlearning,” arXiv preprint arXiv:2004.10703 , 2020.\n[59] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert:\nPre-training of deep bidirectional transformers for language\nunderstanding,” 2019.\n[60] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,\nM. Lewis, L. Zettlemoyer, and V. Stoyanov, “Roberta: A\nrobustly optimized bert pretraining approach,” arXiv preprint\narXiv:1907.11692, 2019.\n[61] S. Sarker, “Banglabert: Bengali mask language model for\nbengali language understading,” 2020. [Online]. Available:\nhttps://github.com/sagorbrur/bangla-bert\n[62] K. Jain, A. Deshpande, K. Shridhar, F. Laumann, and A. Dash,\n“Indic-transformers: An analysis of transformer language\nmodels for indian languages,” CoRR, vol. abs/2011.02323, 2020.\n[Online]. Available: https://arxiv.org/abs/2011.02323\n[63] V. Bhatnagar, P. Kumar, S. Moghili, and P. Bhattacharyya,\n“Divide and conquer: An ensemble approach for hostile post\ndetection in hindi,” arXiv preprint arXiv:2101.07973 , 2021.\n[64] S. Tawalbeh, M. Hammad, and M. AL-Smadi, “KEIS@JUST\nat SemEval-2020 task 12: Identifying multilingual oﬀensive\ntweets using weighted ensemble and ﬁne-tuned BERT,”\nin Proceedings of the Fourteenth Workshop on Semantic\nEvaluation. Barcelona (online): International Committee for\nComputational Linguistics, Dec. 2020, pp. 2035–2044. [Online].\nAvailable: https://aclanthology.org/2020.semeval-1.269\n[65] K. A. Das, A. Baruah, F. A. Barbhuiya, and K. Dey, “KAFK\nat SemEval-2020 task 12: Checkpoint ensemble of transformers\nfor hate speech classiﬁcation,” in Proceedings of the Fourteenth\nWorkshop on Semantic Evaluation . Barcelona (online):\nInternational Committee for Computational Linguistics, Dec.\n2020, pp. 2023–2029. [Online]. Available: https://aclanthology.\norg/2020.semeval-1.267\n[66] S. Gundapu and R. Mamidi, “Transformer based\nautomatic COVID-19 fake news detection system,”\nCoRR, vol. abs/2101.00180, 2021. [Online]. Available:\nhttps://arxiv.org/abs/2101.00180\n[67] S. M. S. Shifath, M. F. Khan, and M. S. Islam, “A\ntransformer based approach for ﬁghting COVID-19 fake\nnews,” CoRR, vol. abs/2101.12027, 2021. [Online]. Available:\nhttps://arxiv.org/abs/2101.12027\n[68] A. Das, M. A. Iqbal, O. Sharif, and M. M. Hoque, “Bemod: De-\nvelopment of bengali emotion dataset for classifying expressions\nof emotion in texts,” in Intelligent Computing and Optimization ,\nP. Vasant, I. Zelinka, and G.-W. Weber, Eds. Cham: Springer\nInternational Publishing, 2021, pp. 1124–1136.\n16 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319455\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nDas et al.: Emotion Classiﬁcation\nAVISHEK DAS (GRADUATE MEMBER,\nIEEE) received his B.Sc. Engg. Degree in\nComputer Science Engineering (CSE) from\nChittagong University of Engineering Tech-\nnology (CUET), Bangladesh in 2022. Cur-\nrently, he is appointed as Lecturer, Depart-\nment of CSE, CUET. He previously held the\nposition of Machine Learning Engineer at\nApurba Technologies Ltd., Bangladesh. His\nresearch interest includes Machine Learning\nand Deep Learning, Natural Language Processing, Computer\nVision, and Data Mining. Avishek Das became a graduate member\nof IEEE in 2022 and has been a student member of IEEE since\n2016. He also served as a Student Branch Chairperson at the\nIEEE CUET Student Branch.\nMOHAMMED MOSHIUL HOQUE (SENIOR\nMEMBER, IEEE) serves as a professor in the\nDept. of Computer Science and Engineer-\ning (CSE) and Director of Sheikh Kamal IT\nBusiness Incubator at Chittagong Univer-\nsity of Engineering & Technology (CUET).\nHe served as head of the Department of\nCSE and Dean of the Faculty of Electrical &\nComputer Engineering, CUET. Dr. Hoque\nreceived Ph. D in Information and Com-\nputer Sciences from Saitama University (Japan), M.Sc. Engg.\nin CSE from BUET and B. Sc. Engg. in EEE from CUET,\nrespectively. He is the founding director of CUET NLP Lab.\nCurrently, Dr. Hoque is acting as the Chair of the IEEE\nBangladesh Section. He has published more than 195 publications\nin several International Journals and Conferences. Dr. Hoque\nworked in several technical committees of IEEE/IEEE BDS co-\nsponsored conferences such as Organizing Chair (ECCE 2023,\nICCIT 2022/23, IEEE WEICON-ECE 2022/23), TPC Chair\n(ACMI 2021, ICREST 2021/23, ECCE 2019, IEEE R10 HTC\n2017), TPC Co-chair (IEEE R10 IEEE HTC 17, TENSYMP 2020,\nICISET 2018/21, IEEE WIECON-ECE 21) & Publication Chair\n(IEEE ECE WIECON 2018/19, IEEER10 TENSYMP 2020). His\nresearch interests include Human-Robot/Computer Interaction,\nMachine Learning, and Natural Language Processing. Dr. Hoque\nis a Senior Member of IEEE, Member of IEEE Computer Society,\nIEEE Robotics & Automation Society, IEEE Women in Engineer-\ning Aﬃnity Group, IEEE Signal Processing Society and a Fellow\nof the Institute of Engineers (IEB), Bangladesh..\nOMAR SHARIF (GRADUATE MEMBER,\nIEEE) received his B.Sc. Engg. and M.Sc.\nEngg. Degree in Computer Science & En-\ngineering (CSE) from Chittagong Univer-\nsity of Engineering & Technology (CUET),\nBangladesh in 2018 and 2022 respectively.\nCurrently, he serves as an Assistant Profes-\nsor in the Computer Science & Engineering\ndepartment at CUET. He published more\nthan thirty research papers in reputed Jour-\nnals and Conferences. His research interests include Natural\nLanguage Processing, Health Informatics, Multimodal NLP, and\nLarge Language models. Mr. Sharif is a graduate member of IEEE\nand a student member of ACM, USA.\nM. ALI AKBER DEWAN (MEMBER, IEEE)\nreceived the B.Sc. degree in computer\nscience and engineering from Khulna Uni-\nversity, Bangladesh, in 2003, and the Ph.D.\ndegree in computer engineering from Kyung\nHee University, South Korea, in 2009. From\n2003 to 2008, he was a Lecturer with the\nDepartment of Computer Science and En-\ngineering, Chittagong University of Engi-\nneering and Technology, Bangladesh, where\nhe was an Assistant Professor, in 2009. From 2009 to 2012,\nhe was a Postdoctoral Researcher with Concordia University,\nMontreal, QC, Canada. From 2012 to 2014, he was a Research\nAssociate with the École de Technologie Supérieure, Montreal. He\nis currently an Associate Professor with the School of Computing\nand Information Systems, Athabasca University, Athabasca, AB,\nCanada. He has published more than 70 articles in high impact\njournals and conference proceedings. His research interests include\nartiﬁcial intelligence, aﬀective computing, computer vision, data\nmining, information visualization, machine learning, biometric\nrecognition, medical image analysis, and health informatics. He\nhas served as an editorial board member, a chair/co-chair, and a\nTPC member for several prestigious journals and conferences. He\nreceived the Deans Award and the Excellent Research Achieve-\nment Award for his excellent academic performance and research\nachievements during his Ph.D. studies in South Korea.\nNAZMUL SIDDIQUE (SENIOR MEMBER,\nIEEE) is with the School of Computing,\nEngineering and Intelligent Systems, Ulster\nUniversity. He obtained Dipl.-Ing. de-\ngree in Cybernetics from the TU Dres-\nden, Germany, MSc in Computer Science\nfrom BUET, Bangladesh and PhD in In-\ntelligent Control from the Department of\nAutomatic Control and Systems Engineer-\ning, University of Sheﬃeld, England. His\nresearch interests include: robotics, cybernetics, computational\nintelligence, nature-inspired computing, stochastic systems and\nvehicular communication. He has published over 170 research\npapers in the broad area of computational intelligence, vehicular\ncommunication, robotics and cybernetics. He authored and co-\nauthored ﬁve books published by John Wiley, Springer and Taylor\n& Francis. He guest edited eight special issues of reputed journals\non Cybernetic Intelligence, Computational Intelligence, Neural\nNetworks and Robotics. He has been involved in organizing\nmany national and international conferences and co-edited seven\nconference proceedings. Dr. Siddique is a Fellow of the Higher\nEducation Academy, a senior member of IEEE and a member of\ndiﬀerent committees of IEEE SMCS. He is on the editorial board of\nthe Nature Scientiﬁc Research, Journal of Behavioural Robotics,\nEngineering Letters, International Journal of Machine Learning\nand Cybernetics, International Journal of Applied Pattern Recog-\nnition, International Journal of Advances in Robotics Research\nand also on the editorial advisory board of the International\nJournal of Neural Systems.\nVOLUME 4, 2016 17\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3319455\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
  "topic": "Bengali",
  "concepts": [
    {
      "name": "Bengali",
      "score": 0.8909218311309814
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7186497449874878
    },
    {
      "name": "Computer science",
      "score": 0.7063291072845459
    },
    {
      "name": "Natural language processing",
      "score": 0.6207666397094727
    },
    {
      "name": "Sentiment analysis",
      "score": 0.5206953287124634
    },
    {
      "name": "Disgust",
      "score": 0.4944794178009033
    },
    {
      "name": "Categorization",
      "score": 0.4495583176612854
    },
    {
      "name": "Classifier (UML)",
      "score": 0.43905436992645264
    },
    {
      "name": "Machine learning",
      "score": 0.42965418100357056
    },
    {
      "name": "Transformer",
      "score": 0.4268924593925476
    },
    {
      "name": "Anger",
      "score": 0.3383128046989441
    },
    {
      "name": "Information retrieval",
      "score": 0.32046544551849365
    },
    {
      "name": "Psychology",
      "score": 0.09391513466835022
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Psychiatry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I102782458",
      "name": "Chittagong University of Engineering & Technology",
      "country": "BD"
    },
    {
      "id": "https://openalex.org/I86897205",
      "name": "Athabasca University",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I138801177",
      "name": "University of Ulster",
      "country": "GB"
    }
  ]
}