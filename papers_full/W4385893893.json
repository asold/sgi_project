{
  "title": "A Better Way to Do Masked Language Model Scoring",
  "url": "https://openalex.org/W4385893893",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2898263184",
      "name": "Carina Kauf",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1972565976",
      "name": "Anna Ivanova",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4307429315",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W3037115370",
    "https://openalex.org/W3175541789",
    "https://openalex.org/W3088059392",
    "https://openalex.org/W3176198948",
    "https://openalex.org/W2990391581",
    "https://openalex.org/W4205857304",
    "https://openalex.org/W4221154823",
    "https://openalex.org/W4296564631",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W3105882417",
    "https://openalex.org/W3152698349",
    "https://openalex.org/W4389070894",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2996728628",
    "https://openalex.org/W3036699898"
  ],
  "abstract": "Estimating the log-likelihood of a given sentence under an autoregressive language model is straightforward: one can simply apply the chain rule and sum the log-likelihood values for each successive token. However, for masked language models (MLMs), there is no direct way to estimate the log-likelihood of a sentence. To address this issue, Salazar et al. (2020) propose to estimate sentence pseudo-log-likelihood (PLL) scores, computed by successively masking each sentence token, retrieving its score using the rest of the sentence as context, and summing the resulting values. Here, we demonstrate that the original PLL method yields inflated scores for out-of-vocabulary words and propose an adapted metric, in which we mask not only the target token, but also all within-word tokens to the right of the target. We show that our adapted metric (PLL-word-l2r) outperforms both the original PLL metric and a PLL metric in which all within-word tokens are masked. In particular, it better satisfies theoretical desiderata and better correlates with scores from autoregressive models. Finally, we show that the choice of metric affects even tightly controlled, minimal pair evaluation benchmarks (such as BLiMP), underscoring the importance of selecting an appropriate scoring metric for evaluating MLM properties.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 925–935\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nA Better Way to Do Masked Language Model Scoring\nCarina Kauf\nMassachusetts Institute of Technology\nckauf@mit.edu\nAnna A. Ivanova\nMassachusetts Institute of Technology\nannaiv@mit.edu\nAbstract\nEstimating the log-likelihood of a given sen-\ntence under an autoregressive language model\nis straightforward: one can simply apply the\nchain rule and sum the log-likelihood values for\neach successive token. However, for masked\nlanguage models (MLMs), there is no direct\nway to estimate the log-likelihood of a sen-\ntence. To address this issue, Salazar et al.\n(2020) propose to estimate sentence pseudo-\nlog-likelihood (PLL) scores, computed by suc-\ncessively masking each sentence token, retriev-\ning its score using the rest of the sentence\nas context, and summing the resulting val-\nues. Here, we demonstrate that the original\nPLL method yields inflated scores for out-of-\nvocabulary words and propose an adapted met-\nric, in which we mask not only the target to-\nken, but also all within-word tokens to the right\nof the target. We show that our adapted met-\nric (PLL-word-l2r) outperforms both the orig-\ninal PLL metric and a PLL metric in which all\nwithin-word tokens are masked. In particular,\nit better satisfies theoretical desiderata and bet-\nter correlates with scores from autoregressive\nmodels. Finally, we show that the choice of\nmetric affects even tightly controlled, minimal\npair evaluation benchmarks (such as BLiMP),\nunderscoring the importance of selecting an ap-\npropriate scoring metric for evaluating MLM\nproperties.1\n1 Introduction\nMost state-of-the-art transformer-based large lan-\nguage models (LLMs) fall into two classes: unidi-\nrectional (or autoregressive) models, where each\ntoken is generated based on its left context (e.g.,\nGPT models; Radford et al., 2019), and bidirec-\ntional models, where a token is predicted from\nboth left and right context tokens, some of which\nmay be masked (e.g., BERT; Devlin et al., 2018).\nOften, it is beneficial to compare these models’ per-\nformance on controlled sentence generation bench-\nmarks. Whereas unidirectional architectures offer a\n1Our results and code are available at https://github.\ncom/carina-kauf/better-mlm-scoring .\nFigure 1: Three different ways to compute the PLL\nscore of a multi-token word (e.g., souvenir) during\nmasked language modeling. Purple: target token, pink:\nwithin-word tokens that are available during inference,\nturquoise: within-word tokens that are masked during\ninference. Sentence tokens that do not belong to the\ncurrent word are always available during inference.\nnatural way of calculating sentence log-likelihood\n(summing the log-likelihood scores of each sen-\ntence token given its left context), there is no direct\nway of estimating sentence log-likelihood for a\nbidirectional model.\nSo far, the best available method to score a\nsentence under a bidirectional LLM has been the\npseudo-log-likelihood (PLL) scoring approach de-\nscribed by Salazar et al. (2020) (and initially used\nby Shin et al., 2019; Wang and Cho, 2019). The\nPLL of a sentence is calculated as the sum of PLL\nscores for each token given all other sentence to-\nkens, thus providing a comparable metric to uni-\ndirectional models’ log-likelihood (LL) sentence\nscoring. The PLL metric is extremely popular; it\nis used extensively in LLM studies tackling topics\nas diverse as effects of training data (Sinha et al.,\n2021; Zhang et al., 2021), model fluency (Laban\net al., 2021), syntactic and conceptual knowledge\n(Sinclair et al., 2022; Bhatia and Richie, 2022), so-\ncial biases (Nangia et al., 2020), and others. Some\nof these studies have already accrued dozens of\ncitations.\nHere, we show that the metric proposed by\nSalazar et al. ( PLL-original) has important\nshortcomings that limit its utility. Specifically,\nPLL-original overestimates the PLL of out-\nof-vocabulary (OOV) words, which LLM tok-\nenizers split into multiple tokens. As a result,\nPLL-original scores fail on several theoretically\n925\nFigure 2: The PLL-original metric inflates scores of multi-token words, such as souvenir; the adjusted metrics,\nPLL-word-l2r and PLL-whole-word, mitigate this issue. Example generated using the bert-base-cased model.\ndesired property tests: a robust inverse relation-\nship between sentence length and sentence PLL\n(Section 4.1), a robust positive correlation between\na word’s frequency and its PLL score (4.2), and\na positive correlation between unidirectional and\nbidirectional model scores for the same sentences\n(Section 5). To remedy these issues, we propose\nan adjusted PLL metric, PLL-word-l2r (l2r: left-\nto-right), which estimates token PLL when future\nwithin-word tokens are also masked (Figure 1).\nWe show that the PLL-word-l2r metric outper-\nforms both PLL-original and alternative PLL-\nbased metrics. We therefore recommend to use the\nPLL-word-l2r metric when estimating sentence\nPLL under a bidirectional LLM.\n2 Motivation: score inflation for\nmulti-token words\nThe PLL-original metric grossly overestimates\nthe probability of OOV lexical items, such as sou-\nvenir (Figure 2). This is because OOV words are\ntokenized into subword tokens (e.g., so ##uven\n##ir), and each subword token is predicted using\nthe token’s bidirectional context, which crucially\nincludes the remaining tokens that make up the\nOOV word. Thus, even though the OOV word it-\nself may be surprising given the sentence context,\nthe individual parts of the OOV word are not sur-\nprising to a bidirectional model given a sentence\ncontext that includes all other subtokens of that\nword (e.g., it is easy to predict so given ##uven\n##ir; see Appendix A for additional examples).\nTo mitigate this bias, we adjust the PLL sentence\nscoring algorithm such that the model cannot ac-\ncess future within-word tokens (PLL-word-l2r) or\nany within-word tokens (PLL-whole-word) when\npredicting the target.\nBelow, we conduct a rigorous investigation of\nour modified metrics to determine whether this\nintuitive benefit holds quantitatively.\n3 Methods\nFor our analysis, we adapt the scorer mod-\nule of the minicons library (Misra, 2022), an\nopen-source wrapper library around HuggingFace\ntransformers (Wolf et al., 2020) that enables effi-\ncient extraction of word- and sentence-level proba-\nbilities from LLMs. The MLM scoring procedure\nof the minicons library follows the procedure orig-\ninally proposed by Salazar et al. (2020). For details\non sentence preprocessing, see Appendix B.\n3.1 PLL metrics\nPLL-original. In this metric, each sentence to-\nken st of a sentence S with n tokens is consec-\nutively replaced with a [MASK] and is predicted\nusing all past and future tokens, irrespective of\nwhether the context tokens belong to the same\nor a different word than the target token. Thus,\ninference is conditioned on the context S\\t :=\n(s1, . . . , st−1, st+1, . . . , sn). The final sentence\nscore is obtained as the sum of the log probabilities\nof each sentence token given its context:\nPLLorig(S) :=\nn∑\nt=1\nlog PMLM(st | S\\t) (1)\nPLL-word-l2r. In this metric, a [MASK] is placed\nnot only over the current target token (now: swt),\nbut also over all future sentence tokens that belong\nto the same word sw as the target. Inference is then\nconditioned on a context that includes all preceding\nsentence tokens (including those belonging to the\ncurrent word) and all sentence tokens from future\nwords. The final score of a sentence S is obtained\nas the sum of the log probabilities of each of the\n|w| tokens in each of the |S| words:\n926\nFigure 3: Out of all PLL metrics, PLL-word-l2r best satisfies theoretical desiderata: (A) an inverse relationship\nbetween negative sentence PLL (a measure of model surprisal) and sentence length and (B) a positive correlation\nbetween word PLL and word log frequency. In (A), each dot is a sentence; in (B), each dot is a unique word from\nthe dataset. Here and elsewhere, reported correlations are Pearson correlations.\nPLLl2r(S) :=\n|S|∑\nw=1\n|w|∑\nt=1\nlog PMLM(swt | S\\swt′≥t\n)\n(2)\nPLL-whole-word. This metric is similar to\nPLL-word-l2r and differs from it only in that a\n[MASK] is placed over all sentence tokens that be-\nlong to the same word sw as the target (both pre-\nceding and future). Inference is then conditioned\non a context that includes all sentence tokens ex-\ncept those belonging to the current word. The final\nscore of a sentence S is obtained as the sum of the\nlog probabilities of each of the |w| tokens in each\nof the |S| words in S given the token’s context:\nPLLww(S) :=\n|S|∑\nw=1\n|w|∑\nt=1\nlog PMLM(swt | S\\sw )\n(3)\nIn Appendix G, we also report results for a\nPLL metric where not only future within-word to-\nkens, but all sentence tokens to the right of the\ntarget context are masked ( PLL-sentence-l2r).\nAlthough this method is most similar to autore-\ngressive LL scoring, sentence-l2r masking for\nBERT is known to produce poor quality genera-\ntions (Wang and Cho, 2019); we therefore refrain\nfrom including this metric in the main text.\n3.2 Models\nWe report results for bert-base-cased (and\ngpt2-medium for comparison) unless stated oth-\nerwise. Results for larger models are provided in\nAppendices D-F.\n3.3 Datasets\nFor our main analyses, we use the EventsAdapt\ndataset (Kauf et al., 2022, based on Fedorenko\net al., 2020). It contains a curated set of 782 syntac-\ntically simple sentence pairs that describe plausible\nor implausible agent-patient interactions in active\nor passive voice (e.g., The traveler lost the sou-\nvenir). Sentences in this dataset are 5-7 words long\n(mean: 6.1, std: 1.05), with an average word log\nfrequency of 10.95. We use this dataset because it\n927\nFigure 4: Correlation between bidirectional model PLL scores and unidirectional model LL scores. Each dot is a\nsentence.\ncontains a high number of OOV words (19.6% for\nBERT and 40.3% for GPT-2; see also Appendix C).\nIn Appendices D-F, we show that our results gen-\neralize to two larger and more diverse corpora: the\nBrown corpus (Francis and Kucera, 1979) and the\nreference sentence set from the LibriSpeech corpus\n(Panayotov et al., 2015). We also apply our PLL\nmetrics to score the sentences in the Benchmark of\nLinguistic Minimal Pairs (BLiMP) (Warstadt et al.,\n2020), a challenge set of 67k sentence pairs which\ntarget specific aspects of linguistic knowledge.\n4 Evaluating PLL metric properties\n4.1 Effects of sentence length\nLike Salazar et al. (2020), we expect that mod-\nels should, on average, assign lower probabil-\nity to longer sentences. Thus, negative PLL\n(which reflects model surprisal) should be posi-\ntively correlated with sentence length. However,\nthe PLL-original metric violates this expecta-\ntion in our test sentence set, which shows a neg-\native correlation between the number of tokens\nand negative PLL. In contrast, PLL-word-l2r and\nPLL-whole-word metrics exhibit a positive corre-\nlation between the number of sentence tokens and\nnegative PLL, just as the negative LL scores for a\nunidirectional model, GPT2-medium (Figure 3A).\n4.2 Effects of word frequency\nAn appropriate (P)LL metric should reflect the\nfact that LLMs are sensitive to distributional pat-\nterns in training text corpora. In particular, we\nexpect more frequent words to have higher (P)LL\nscores in the absence of contextual effects. This\nis indeed the case for GPT2-medium; however,\nthe score inflation for multi-token words means\nthat the PLL-original metric grossly overesti-\nmates the scores for low-frequency words (Fig-\nure 3B). PLL-word-l2r scores restore this relation-\nship: their correlation with word frequency is much\nhigher than for PLL-original. PLL-whole-word\nalso performs well, although its correlation with\nword frequency is lower than for PLL-word-l2r,\nsuggesting that it excessively penalizes OOV\nwords.\n5 Correlation with GPT-2 scores\nWe expect that PLL scores for bidirectional models\nshould be at least somewhat consistent with LL\nscores for unidirectional models: both metrics are\ndesigned to serve are a proxy for sentence probabil-\nity. Here, we show that the GPT-2/BERT score cor-\nrelation for the PLL-original metric is very low,\nwhereas correlation scores for PLL-word-l2r and\nPLL-whole-word are much higher (Figure 4), indi-\ncating the validity of this metric for cross-model\ncomparison. As in Section 4.2, PLL-word-l2r\nslightly outperforms PLL-whole-word, likely be-\ncause it does not penalize OOV words as severely.\nSee Appendices D-F for evidence that all three\ntrends hold for larger models and for other datasets\n(although the effects in other datasets are attenuated\ndue to a lower OOV ratio).\n6 Effects on benchmarking\nHere, we show that the choice of PLL metric affects\nbenchmarking results for a popular, highly con-\ntrolled, minimal pair linguistic benchmark: BLiMP.\nDespite the fact that the comparisons are highly\ncontrolled, different metrics yield different BLiMP\nscores. For all four tested models, PLL-word-l2r\nachieves the best overall BLiMP score (Table 1).\n928\nModel Metric Overall score\nBERT (base)\nPLL-original 84.2\nPLL-word-l2r 84.7\nPLL-whole-word 83.1\nBERT (large)\nPLL-original 84.8\nPLL-word-l2r 85.0\nPLL-whole-word 82.6\nRoBERTa (base)\nPLL-original 85.4\nPLL-word-l2r 86.7\nPLL-whole-word 85.4\nRoBERTa (large)\nPLL-original 86.5\nPLL-word-l2r 87.5\nPLL-whole-word 85.9\nTable 1: Bidirectional model performance on the\nBLiMP benchmark using different PLL metrics.\nSee Appendix H for detailed scores.\n7 Conclusion\nWe have shown thatPLL-word-l2r is the preferred\nmetric for evaluating sentence PLL under a masked\nlanguage model, such as BERT. Although the re-\nsults from studies using the PLL-original metric\ncan still be informative, they become harder to in-\nterpret if the proportion of OOV words in their\ntest set is high. Therefore, we recommend using\nPLL-word-l2r in future works.\nLimitations\nThe proposed PLL-word-l2r metric has the same\npractical limitations as previous LL/PLL ap-\nproaches. Most importantly, these scores can be\ninfluenced by many superfluous factors, such as\nthe number of available synonyms ( computer vs.\nlaptop; Holtzman et al., 2021). We therefore expect\nour method to be most useful in highly controlled\nminimal pair or multiple choice setups.\nEven more accurate metrics may emerge in the\nfuture. For instance, our approach pre-specifies\nthe number of tokens in a word, thus limiting the\nspace of possible alternatives. Future approaches\nmight investigate a way to normalize the PLL score\ndistribution over words with a varying number of\ntokens. Further, it would be interesting to attempt\nto estimate the joint probability of all tokens in a\nword instead of predicting them left-to-right (as in\nPLL-word-l2r) or without any other within-word\ncontextual information (as in PLL-whole-word).\nFinally, we test our approach on English text\ncorpora; our results might not generalize to agglu-\ntinative languages (due to a high number of tokens\nper word and, therefore, increased uncertainty) and\nare of less relevance to isolating languages (where,\nif enough training data are available, most word-\nlevel items can be represented as single tokens).\nEthics Statement\nIn our proposed metric, word tokens are masked\nfrom left to right following the writing tradition in\nEnglish; however, for speakers of languages such\nas Arabic, a “right to left” notation would be more\nintuitive. Note, however, that this is primarily a de-\nnotational difference that does not affect the score\nitself (LLMs do not discriminate left and right, only\nbeginning and end). We do not anticipate any spe-\ncific harms that would be intrinsically associated\nwith the techniques described in this paper.\nAcknowledgements\nWe thank Jacob Andreas, Evan Hernandez, and\nthe anonymous ACL reviewers for their insightful\nfeedback. CK was supported by the K. Lisa Yang\nIntegrative Computational Neuroscience (ICoN)\nCenter at MIT. AI was supported by MIT Quest for\nIntelligence.\nReferences\nSudeep Bhatia and Russell Richie. 2022. Transformer\nnetworks of human conceptual knowledge. Psycho-\nlogical Review.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nEvelina Fedorenko, Idan Asher Blank, Matthew Siegel-\nman, and Zachary Mineroff. 2020. Lack of selectivity\nfor syntax relative to word meanings throughout the\nlanguage network. Cognition, 203:104348.\nW Nelson Francis and Henry Kucera. 1979. Brown\ncorpus manual. Letters to the Editor, 5(2):7.\nJon Gauthier, Jennifer Hu, Ethan Wilcox, Peng Qian,\nand Roger Levy. 2020. Syntaxgym: An online plat-\nform for targeted evaluation of language models. As-\nsociation for Computational Linguistics (ACL).\nAri Holtzman, Peter West, Vered Shwartz, Yejin Choi,\nand Luke Zettlemoyer. 2021. Surface form com-\npetition: Why the highest probability answer isn’t\nalways right. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 7038–7051, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\n929\nCarina Kauf, Anna A Ivanova, Giulia Rambelli, Em-\nmanuele Chersoni, Jingyuan S She, Zawad Chowd-\nhury, Evelina Fedorenko, and Alessandro Lenci.\n2022. Event knowledge in large language models:\nthe gap between the impossible and the unlikely.\narXiv preprint arXiv:2212.01488.\nPhilippe Laban, Tobias Schnabel, Paul Bennett, and\nMarti A. Hearst. 2021. Keep it simple: Unsupervised\nsimplification of multi-paragraph text. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 6365–6378, Online.\nAssociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nKanishka Misra. 2022. minicons: Enabling flexible be-\nhavioral and representational analyses of transformer\nlanguage models. arXiv preprint arXiv:2203.13112.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R. Bowman. 2020. CrowS-pairs: A chal-\nlenge dataset for measuring social biases in masked\nlanguage models. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1953–1967, Online. As-\nsociation for Computational Linguistics.\nVassil Panayotov, Guoguo Chen, Daniel Povey, and\nSanjeev Khudanpur. 2015. Librispeech: an asr cor-\npus based on public domain audio books. In 2015\nIEEE international conference on acoustics, speech\nand signal processing (ICASSP), pages 5206–5210.\nIEEE.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nJulian Salazar, Davis Liang, Toan Q Nguyen, and Ka-\ntrin Kirchhoff. 2020. Masked language model scor-\ning. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n2699–2712.\nJoonbo Shin, Yoonhyung Lee, and Kyomin Jung. 2019.\nEffective sentence scoring method using bert for\nspeech recognition. In Asian Conference on Machine\nLearning, pages 1081–1093. PMLR.\nArabella Sinclair, Jaap Jumelet, Willem Zuidema, and\nRaquel Fernández. 2022. Structural persistence in\nlanguage models: Priming as a window into abstract\nlanguage representations. Transactions of the Associ-\nation for Computational Linguistics, 10:1031–1050.\nKoustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle\nPineau, Adina Williams, and Douwe Kiela. 2021.\nMasked language modeling and the distributional hy-\npothesis: Order word matters pre-training for little.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2888–2913, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nAlex Wang and Kyunghyun Cho. 2019. BERT has a\nmouth, and it must speak: BERT as a Markov ran-\ndom field language model. In Proceedings of the\nWorkshop on Methods for Optimizing and Evaluat-\ning Neural Language Generation, pages 30–36, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R\nBowman. 2020. BLiMP: The benchmark of linguis-\ntic minimal pairs for english. Transactions of the\nAssociation for Computational Linguistics, 8:377–\n392.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz,\net al. 2020. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing: System Demonstrations, pages\n38–45.\nYian Zhang, Alex Warstadt, Xiaocheng Li, and\nSamuel R. Bowman. 2021. When do you need bil-\nlions of words of pretraining data? In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 1112–1125, Online.\nAssociation for Computational Linguistics.\nAppendix\nA Additional examples of score inflation\nFigure 5: The PLL-original metric inflates the score of\nthe word carnivore. PLL-word-l2r mitigate this issue,\nwhereas PLL-whole-word overly penalizes the word.\nModel: bert-base-cased.\nFigure 6: The PLL-original metric inflates the score of\nthe word hooligan. PLL-word-l2r mitigate this issue,\nwhereas PLL-whole-word overly penalizes the word.\nModel: bert-base-cased.\n930\nB Text preprocessing for (P)LL\ncomputation\nThe minicons library borrows the MLM prepro-\ncessing algorithm from Salazar et al. (2020):[CLS]\nand [SEP] tokens are prepended and appended to\nthe text, respectively, and are not masked during\nPLL computation. For CLMs, we minimally ad-\njust the minicons scorer library default and nec-\nessarily prepend the beginning of sentence token,\n<|endoftext|>, to the text, which enables us to\nget a probability for the first actual sentence token\n(see also the lm-zoo library; Gauthier et al., 2020).\nThe (P)LLs of all special tokens are not counted\ntoward the final sentence/word score.\nWhen calculating the (P)LL score of individ-\nual words (to estimate word frequency effects),\nwe place them in a neutral context My word is\n_. To ensure that the same pattern of results holds\nacross multiple neutral contexts, we additionally\ntest the context I opened the dictionary and ran-\ndomly picked a word. It was _, as well as a no-\ncontext setup. These additional results are reported\nin Appendix E.1.\nWord frequency was operationalized as the log\nof the number of occurrences of the word in the\n2012 Google NGram corpus. Laplace smoothing\nwas applied prior to taking the logarithm.\nC Quantification of out-of-vocabulary\nwords per dataset\nDataset Model class OOV ratio\nEventsAdapt\nBERT 19.6%\nRoBERTa 40.3%\nGPT 40.3%\nLibriSpeech\nBERT 8%\nRoBERTa 24.3%\nGPT 24.3%\nBrown\nBERT 8%\nRoBERTa 25%\nGPT 25%\nTable 2: The out-of-vocabulary (OOV) ratio per\ndataset, quantified as the number of words split into\nat least two tokens by a given model’s tokenizer\ndivided by the total number of words in the dataset.\nGPT and RoBERTa models use byte-level Byte-\nPair-Encoding tokenizers (Radford et al., 2019; Liu\net al., 2019); BERT models use WordPiece tok-\nenization (Devlin et al., 2018).\nD Effects of sentence length\nD.1 Larger LLM versions\nFigure 7: Sentence length effects for gpt2-xl and\nbert-large-cased on the EventsAdapt corpus.\nD.2 Larger datasets\nFigure 8: Sentence length effects for gpt2-medium and\nbert-base-cased on the LibriSpeech corpus.\nFigure 9: Sentence length effects for gpt2-medium and\nbert-base-cased on the Brown corpus.\nE Effects of word frequency\nE.1 Different word contexts\nFigure 10: Word frequency effects for\nbert-base-cased on the EventsAdapt corpus.\nWord scores were retrieved with a neutral context: “I\nopened a dictionary and randomly picked a\nword. It was _”.\nFigure 11: Word frequency effects for\nbert-base-cased on the EventsAdapt corpus.\nWord scores were retrieved without supporting context.\n931\nE.2 Different datasets\nFigure 12: Word frequency effects for\nbert-base-cased on the LibriSpeech corpus.\nWord scores were retrieved with a neutral context: “My\nword is _”.\nFigure 13: Word frequency effects for\nbert-base-cased on the Brown corpus. Word\nscores were retrieved with a neutral context: “My word\nis _”.\nF Correlation with unidirectional models\nF.1 Larger LLM versions\nFigure 14: Correlation between bert-large-cased\nand gpt2-xl scores on the EventsAdapt corpus.\nF.2 Larger datasets\nFigure 15: Correlation between bert-base-cased and\ngpt2-medium scores on the LibriSpeech corpus.\nFigure 16: Correlation between bert-base-cased and\ngpt2-medium scores on the Brown corpus.\nG Whole-sentence left-to-right token\nmasking\nHere, we report results for the scoring algorithm\nthat masks the target token, st, and all sentence\ntokens to its right in a sentence S with n tokens\n(PLL-sentence-l2r). As in autoregressive lan-\nguage models, target token inference is thus con-\nditioned solely on the token’s leftward context:\nPMLM(st | S<t). The final sentence score is ob-\ntained as the sum of the log probabilities of each\nsentence token given its context:\nPLLsent(S) :=\nn∑\nt=1\nlog PMLM(st | S<t) (4)\nOverall, the PLL-sentence-l2r metric satisfies\nthe metric desiderata better than thePLL-original\nmetric but worse than PLL-word-l2r. In addition,\nit is inferior to other metrics on the BLiMP evalua-\ntion benchmark (Appendix H), in line with previ-\nous reports of subpar generation quality (Wang and\nCho, 2019).\nFigure 17: Scores for the motivating example computed\nwith PLL-sentence-l2r (bert-base-cased).\nFigure 18: Word frequency (A) and sentence length (B)\neffects for scores computed with PLL-sentence-l2r\non the EventsAdapt corpus (bert-base-cased)\n.\nFigure 19: Correlation between bert-base-cased\nand gpt2-medium scores computed with\nPLL-sentence-l2r on the EventsAdapt corpus.\n932\nOverall ANA. AGRARG STR.BINDINGCTRL. RAIS.D-N AGRELLIPSISFILLER GAPIRREGULARISLANDNPI QUANTIFIERSS-V AGR\nBERT (base)\nPLL-original 84.2 97.0 80.0 82.3 79.6 97.6 89.4 83.1 96.5 73.6 84.7 71.2 92.4\nPLL-word-l2r 84.7 97.1 81.0 82.3 81.9 98.4 89.6 83.0 96.5 75.0 85.0 69.8 92.1\nPLL-whole-word 83.1 96.6 76.5 81.5 80.5 96.9 87.1 82.5 97.1 74.9 83.8 69.2 88.5\nPLL-sentence-l2r 58.7 80.3 63.0 68.3 53.5 82.1 68.3 47.8 47.3 56.5 38.9 51.6 50.7\nBERT (large)\nPLL-original 84.8 97.2 80.7 82.0 82.7 97.6 86.4 84.3 92.8 77.0 83.4 72.8 91.9\nPLL-word-l2r 85.0 96.8 80.6 81.9 84.8 97.8 85.8 84.0 92.0 78.8 83.6 71.7 91.2\nPLL-whole-word 82.6 96.6 75.7 79.9 81.4 95.2 83.6 83.3 90.1 78.7 81.5 70.4 86.7\nPLL-sentence-l2r 59.8 61.5 63.0 71.3 60.5 71.8 58.3 58.5 63.0 50.2 42.8 51.9 63.0\nRoBERTa (base)\nPLL-original 85.4 97.3 83.5 77.8 81.9 97.0 91.4 90.1 96.2 80.7 81.0 69.8 91.9\nPLL-word-l2r 86.7 97.8 84.8 78.7 84.9 98.3 91.6 90.0 95.4 81.0 84.4 69.7 94.0\nPLL-whole-word 85.4 97.6 80.9 76.6 85.2 96.6 91.6 90.0 95.6 80.2 84.7 69.6 91.1\nPLL-sentence-l2r 79.3 97.0 79.9 71.2 78.4 95.0 84.8 82.6 85.0 68.2 80.6 58.4 81.6\nRoBERTa (large)\nPLL-original 86.5 97.8 84.6 79.1 84.1 96.8 90.8 88.9 96.8 83.4 85.5 70.2 91.4\nPLL-word-l2r 87.5 98.0 85.0 80.0 86.8 98.3 90.4 89.1 95.7 83.4 88.0 70.3 93.2\nPLL-whole-word 85.9 98.2 80.2 78.0 87.1 96.0 90.1 88.9 95.6 82.2 88.0 69.8 89.7\nPLL-sentence-l2r 80.4 98.8 82.5 71.8 80.4 95.1 82.0 80.8 91.6 73.0 76.6 57.8 86.0\nHuman 88.6 97.5 90.0 87.3 83.9 92.2 85.0 86.9 97.0 84.9 88.1 86.6 90.9\nTable 3: Unsupervised performance (forced choice accuracy) on all BLiMP benchmark paradigms, using\nthe original and adjusted PLL sentence scoring methods. PLL-original scores replicate those reported\nin Salazar et al. (2020). Human scores are taken from Warstadt et al. (2020).\nH Detailed BLiMP benchmark results\nTable 3 shows results for each sentence suite within\nthe BLiMP benchmark (in addition to the overall\nscores reported in the main text). All models shown\nin Tables 1 and 3 are cased models.PLL-original\nscores replicate those reported in Salazar et al.\n(2020).\n933\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n8\n□\u0017 A2. Did you discuss any potential risks of your work?\nwe do not anticipate speciﬁc risks associated with our work\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nNo response.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNo response.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNo response.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo response.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNo response.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNo response.\nC □\u0013 Did you run computational experiments?\nall\n□\u0017 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nthe models are available on huggingface, and the experiments are computationally light\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n934\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n3 and Appendix A (no hyperparameter search was conducted though)\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nall results ﬁgures\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n3\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n935",
  "topic": "Metric (unit)",
  "concepts": [
    {
      "name": "Metric (unit)",
      "score": 0.7697732448577881
    },
    {
      "name": "Security token",
      "score": 0.7660616636276245
    },
    {
      "name": "Computer science",
      "score": 0.6834345459938049
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6801537275314331
    },
    {
      "name": "Word (group theory)",
      "score": 0.6649150252342224
    },
    {
      "name": "Sentence",
      "score": 0.6618082523345947
    },
    {
      "name": "Language model",
      "score": 0.5381160378456116
    },
    {
      "name": "Vocabulary",
      "score": 0.5074657797813416
    },
    {
      "name": "Speech recognition",
      "score": 0.46352440118789673
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4227152466773987
    },
    {
      "name": "Natural language processing",
      "score": 0.40399205684661865
    },
    {
      "name": "Mathematics",
      "score": 0.20381119847297668
    },
    {
      "name": "Linguistics",
      "score": 0.09792661666870117
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}