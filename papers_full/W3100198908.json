{
  "title": "From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers",
  "url": "https://openalex.org/W3100198908",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2604352910",
      "name": "Anne Lauscher",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2783756630",
      "name": "Vinit Ravishankar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1969142033",
      "name": "Ivan Vulić",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4207383812",
      "name": "Goran Glavaš",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2971721116",
    "https://openalex.org/W3118443283",
    "https://openalex.org/W4285200483",
    "https://openalex.org/W6777047548",
    "https://openalex.org/W2515741950",
    "https://openalex.org/W3154376977",
    "https://openalex.org/W6800857573",
    "https://openalex.org/W2905749056",
    "https://openalex.org/W2798908575",
    "https://openalex.org/W2788353357",
    "https://openalex.org/W6745740328",
    "https://openalex.org/W2982180741",
    "https://openalex.org/W6780226713",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W3092292656",
    "https://openalex.org/W3193068792",
    "https://openalex.org/W2605717780",
    "https://openalex.org/W6680532216",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W3152258780",
    "https://openalex.org/W2995118574",
    "https://openalex.org/W4287779906",
    "https://openalex.org/W2946296745",
    "https://openalex.org/W6734652722",
    "https://openalex.org/W3016815617",
    "https://openalex.org/W4285148079",
    "https://openalex.org/W2612953412",
    "https://openalex.org/W2799124508",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W2983040767",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3135593154",
    "https://openalex.org/W6791108739",
    "https://openalex.org/W3015766957",
    "https://openalex.org/W2965647350",
    "https://openalex.org/W6779832646",
    "https://openalex.org/W2250382531",
    "https://openalex.org/W4226289616",
    "https://openalex.org/W3033187248",
    "https://openalex.org/W7030249854",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W3091841779",
    "https://openalex.org/W2931720176",
    "https://openalex.org/W3035579820",
    "https://openalex.org/W3041246153",
    "https://openalex.org/W3015575765",
    "https://openalex.org/W6764072591",
    "https://openalex.org/W6763121668",
    "https://openalex.org/W2738015883",
    "https://openalex.org/W3017311573",
    "https://openalex.org/W2995015695",
    "https://openalex.org/W3037983807",
    "https://openalex.org/W6629028937",
    "https://openalex.org/W2489487449",
    "https://openalex.org/W2942810103",
    "https://openalex.org/W2160654481",
    "https://openalex.org/W3021443099",
    "https://openalex.org/W6759455113",
    "https://openalex.org/W2765961751",
    "https://openalex.org/W6765510844",
    "https://openalex.org/W3166891237",
    "https://openalex.org/W3021766370",
    "https://openalex.org/W6839336353",
    "https://openalex.org/W2922565841",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W6641913629",
    "https://openalex.org/W2472687422",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2895195120",
    "https://openalex.org/W6688866165",
    "https://openalex.org/W6691431627",
    "https://openalex.org/W2922091957",
    "https://openalex.org/W4385574066",
    "https://openalex.org/W3200434714",
    "https://openalex.org/W3088342637",
    "https://openalex.org/W2952594402",
    "https://openalex.org/W3125508822",
    "https://openalex.org/W3173320446",
    "https://openalex.org/W3023533951",
    "https://openalex.org/W3006881356",
    "https://openalex.org/W6739389664",
    "https://openalex.org/W1843891098",
    "https://openalex.org/W6948152991",
    "https://openalex.org/W2970481354",
    "https://openalex.org/W1816313093",
    "https://openalex.org/W2551396370",
    "https://openalex.org/W6763101540",
    "https://openalex.org/W2789541106",
    "https://openalex.org/W3152698349",
    "https://openalex.org/W6788094239",
    "https://openalex.org/W7070827070",
    "https://openalex.org/W3155806510",
    "https://openalex.org/W6795356200",
    "https://openalex.org/W2798886742",
    "https://openalex.org/W6762537594",
    "https://openalex.org/W2908854766",
    "https://openalex.org/W2298562067",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2949603537",
    "https://openalex.org/W3013891696",
    "https://openalex.org/W2970037872",
    "https://openalex.org/W3016191782",
    "https://openalex.org/W3092733346",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W6788864986",
    "https://openalex.org/W2970550739",
    "https://openalex.org/W6838461927",
    "https://openalex.org/W3166522443",
    "https://openalex.org/W2974273066",
    "https://openalex.org/W6766865527",
    "https://openalex.org/W2939455697",
    "https://openalex.org/W2983829373",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W3022708451",
    "https://openalex.org/W2970413168",
    "https://openalex.org/W2470673105",
    "https://openalex.org/W2893141505",
    "https://openalex.org/W2740132093",
    "https://openalex.org/W3085691715",
    "https://openalex.org/W6704596057",
    "https://openalex.org/W2741602058",
    "https://openalex.org/W3021240322",
    "https://openalex.org/W3016252650",
    "https://openalex.org/W2279316390",
    "https://openalex.org/W31002508",
    "https://openalex.org/W2517456239",
    "https://openalex.org/W2552110825",
    "https://openalex.org/W2875291789",
    "https://openalex.org/W2517502945",
    "https://openalex.org/W2168199177",
    "https://openalex.org/W3049366647",
    "https://openalex.org/W2914073025",
    "https://openalex.org/W2129494713",
    "https://openalex.org/W6999611261",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2934757881",
    "https://openalex.org/W1552847225",
    "https://openalex.org/W3015299511",
    "https://openalex.org/W2947992883",
    "https://openalex.org/W2739967986",
    "https://openalex.org/W3015504467",
    "https://openalex.org/W2250790822",
    "https://openalex.org/W2757931423",
    "https://openalex.org/W2126725946",
    "https://openalex.org/W2127289991",
    "https://openalex.org/W2620558438",
    "https://openalex.org/W2120699290",
    "https://openalex.org/W2610748790",
    "https://openalex.org/W3023911605",
    "https://openalex.org/W6763687114",
    "https://openalex.org/W2810095012",
    "https://openalex.org/W3034255912",
    "https://openalex.org/W6763999555",
    "https://openalex.org/W2785888191",
    "https://openalex.org/W6718053083",
    "https://openalex.org/W2979736636",
    "https://openalex.org/W6990166387",
    "https://openalex.org/W2594021297",
    "https://openalex.org/W2948223045",
    "https://openalex.org/W6770122315",
    "https://openalex.org/W2989539713",
    "https://openalex.org/W2607892599",
    "https://openalex.org/W2958953787",
    "https://openalex.org/W2833383446",
    "https://openalex.org/W6736893582",
    "https://openalex.org/W2799072540",
    "https://openalex.org/W2673116624",
    "https://openalex.org/W2983836503",
    "https://openalex.org/W3018647120",
    "https://openalex.org/W6748634344",
    "https://openalex.org/W6786438351",
    "https://openalex.org/W2741029840",
    "https://openalex.org/W2990374210",
    "https://openalex.org/W2803214681",
    "https://openalex.org/W3208641770",
    "https://openalex.org/W2795355797",
    "https://openalex.org/W6751793318",
    "https://openalex.org/W2984448759",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W6764722885",
    "https://openalex.org/W2579343286",
    "https://openalex.org/W6727497285",
    "https://openalex.org/W2162833336",
    "https://openalex.org/W6752417242",
    "https://openalex.org/W2939980056",
    "https://openalex.org/W2561995736",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W6754654208",
    "https://openalex.org/W2790235966",
    "https://openalex.org/W2736292279",
    "https://openalex.org/W2271328876",
    "https://openalex.org/W2738574128",
    "https://openalex.org/W2121764873",
    "https://openalex.org/W22168010",
    "https://openalex.org/W2595715041",
    "https://openalex.org/W6735377749",
    "https://openalex.org/W2786464815",
    "https://openalex.org/W2740711318",
    "https://openalex.org/W6680585821",
    "https://openalex.org/W2767204723",
    "https://openalex.org/W2607106700",
    "https://openalex.org/W2786685006",
    "https://openalex.org/W7051469422",
    "https://openalex.org/W2463895987",
    "https://openalex.org/W2594470997",
    "https://openalex.org/W2736989307",
    "https://openalex.org/W2793978524",
    "https://openalex.org/W2798812533",
    "https://openalex.org/W2100664567",
    "https://openalex.org/W6762945437",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2808681756",
    "https://openalex.org/W2888329843",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W6730596104",
    "https://openalex.org/W2740840489",
    "https://openalex.org/W6750696111",
    "https://openalex.org/W6637628708",
    "https://openalex.org/W2516090925",
    "https://openalex.org/W2970648593",
    "https://openalex.org/W2901677362",
    "https://openalex.org/W1627331591",
    "https://openalex.org/W2969668414",
    "https://openalex.org/W2122922578",
    "https://openalex.org/W2142708806",
    "https://openalex.org/W2976040964",
    "https://openalex.org/W6666761814",
    "https://openalex.org/W2991265431",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W6767535143",
    "https://openalex.org/W3098604850",
    "https://openalex.org/W2116410915",
    "https://openalex.org/W2912351236",
    "https://openalex.org/W2118463056",
    "https://openalex.org/W6765429426",
    "https://openalex.org/W2798638375",
    "https://openalex.org/W6754669671",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W3020880807",
    "https://openalex.org/W3089892483",
    "https://openalex.org/W3116484298",
    "https://openalex.org/W3010930612",
    "https://openalex.org/W6779431784",
    "https://openalex.org/W2057069782",
    "https://openalex.org/W2548175383",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2038255699",
    "https://openalex.org/W6725309783",
    "https://openalex.org/W2111406701",
    "https://openalex.org/W3042925324",
    "https://openalex.org/W2953271402",
    "https://openalex.org/W6992257893",
    "https://openalex.org/W2989226908",
    "https://openalex.org/W6669875434",
    "https://openalex.org/W3026404337",
    "https://openalex.org/W2195506630",
    "https://openalex.org/W2739827909",
    "https://openalex.org/W2526072446",
    "https://openalex.org/W2130822992",
    "https://openalex.org/W2127030549",
    "https://openalex.org/W2058742866",
    "https://openalex.org/W2132684680",
    "https://openalex.org/W3118608099",
    "https://openalex.org/W2790415926",
    "https://openalex.org/W2736285750",
    "https://openalex.org/W6682650124",
    "https://openalex.org/W2804897457",
    "https://openalex.org/W1991825895",
    "https://openalex.org/W6635469476",
    "https://openalex.org/W1995672192",
    "https://openalex.org/W3122079515",
    "https://openalex.org/W2413794162",
    "https://openalex.org/W3117312003",
    "https://openalex.org/W2927103915",
    "https://openalex.org/W3090805418",
    "https://openalex.org/W2798358706",
    "https://openalex.org/W2145755360",
    "https://openalex.org/W2963259903",
    "https://openalex.org/W2044340178",
    "https://openalex.org/W6691829196",
    "https://openalex.org/W2004827252",
    "https://openalex.org/W2898785098",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2996336810",
    "https://openalex.org/W2299976354",
    "https://openalex.org/W2898662126",
    "https://openalex.org/W6765039553",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W3153147196",
    "https://openalex.org/W1988584482",
    "https://openalex.org/W4302343710",
    "https://openalex.org/W630532510",
    "https://openalex.org/W2506931122",
    "https://openalex.org/W3102094970",
    "https://openalex.org/W2915774325",
    "https://openalex.org/W2109439962",
    "https://openalex.org/W4287646439",
    "https://openalex.org/W3099658661",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W2963165489",
    "https://openalex.org/W2091966899",
    "https://openalex.org/W3006705448",
    "https://openalex.org/W2996908057",
    "https://openalex.org/W4299579390",
    "https://openalex.org/W3104723404",
    "https://openalex.org/W3029286378",
    "https://openalex.org/W2986128786",
    "https://openalex.org/W2962958286",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2299414748",
    "https://openalex.org/W4239871690",
    "https://openalex.org/W2597655663",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2563574619",
    "https://openalex.org/W2951025380",
    "https://openalex.org/W2626534681",
    "https://openalex.org/W2964025273",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2076758314",
    "https://openalex.org/W2952190837",
    "https://openalex.org/W2962965405",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W4320448004",
    "https://openalex.org/W4306384900",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4246285815",
    "https://openalex.org/W2998925391",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W3172099915",
    "https://openalex.org/W3155682407",
    "https://openalex.org/W4394647185",
    "https://openalex.org/W3176366152",
    "https://openalex.org/W2973154008",
    "https://openalex.org/W2970482069",
    "https://openalex.org/W2077113564",
    "https://openalex.org/W2970529259",
    "https://openalex.org/W2963264012",
    "https://openalex.org/W2888882903",
    "https://openalex.org/W3132607382",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2987225727",
    "https://openalex.org/W4298309833",
    "https://openalex.org/W3035547806",
    "https://openalex.org/W3175606037",
    "https://openalex.org/W2963571341",
    "https://openalex.org/W4388979610",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3173660000",
    "https://openalex.org/W4288301883",
    "https://openalex.org/W2973088264",
    "https://openalex.org/W3105492289",
    "https://openalex.org/W2979401726",
    "https://openalex.org/W4288347855",
    "https://openalex.org/W4385573607",
    "https://openalex.org/W3173681001",
    "https://openalex.org/W2972498556",
    "https://openalex.org/W2950768109",
    "https://openalex.org/W2913129712",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2970726176",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W3106210592",
    "https://openalex.org/W3034998639",
    "https://openalex.org/W2964114970",
    "https://openalex.org/W3088675891",
    "https://openalex.org/W3034503989",
    "https://openalex.org/W2962736243",
    "https://openalex.org/W3174169056",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W4288379066",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W3104235057",
    "https://openalex.org/W4298393544",
    "https://openalex.org/W4319083882",
    "https://openalex.org/W2963804993",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W2950976310",
    "https://openalex.org/W2891177506",
    "https://openalex.org/W3101498587",
    "https://openalex.org/W2970752815",
    "https://openalex.org/W3099178230",
    "https://openalex.org/W3035137491",
    "https://openalex.org/W2608787653",
    "https://openalex.org/W2964266061",
    "https://openalex.org/W2964084097",
    "https://openalex.org/W2915977242",
    "https://openalex.org/W2964075320",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W4287688861",
    "https://openalex.org/W2524429381",
    "https://openalex.org/W3088382025",
    "https://openalex.org/W2962824887",
    "https://openalex.org/W2984673553",
    "https://openalex.org/W3103671331",
    "https://openalex.org/W3034779619",
    "https://openalex.org/W3035507081",
    "https://openalex.org/W3035032094",
    "https://openalex.org/W3153289922",
    "https://openalex.org/W3202070718",
    "https://openalex.org/W2229177960",
    "https://openalex.org/W4297699332",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963651521",
    "https://openalex.org/W1500660161",
    "https://openalex.org/W4244527488",
    "https://openalex.org/W3035422918",
    "https://openalex.org/W4299802238",
    "https://openalex.org/W4236950558",
    "https://openalex.org/W2963563735",
    "https://openalex.org/W2250263931",
    "https://openalex.org/W2983144399",
    "https://openalex.org/W2963969878",
    "https://openalex.org/W3121309507",
    "https://openalex.org/W2962788148",
    "https://openalex.org/W4288330988",
    "https://openalex.org/W4297730150",
    "https://openalex.org/W2137607259",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2963737810",
    "https://openalex.org/W3035257795",
    "https://openalex.org/W2805826272",
    "https://openalex.org/W3156194904",
    "https://openalex.org/W3168194750",
    "https://openalex.org/W3174490235",
    "https://openalex.org/W4377825911",
    "https://openalex.org/W3105240217",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2963691697",
    "https://openalex.org/W2971344868",
    "https://openalex.org/W3034917890",
    "https://openalex.org/W3099744315",
    "https://openalex.org/W2963047628",
    "https://openalex.org/W2995230342",
    "https://openalex.org/W4289490673",
    "https://openalex.org/W2949061542",
    "https://openalex.org/W3103490574",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W3035261420",
    "https://openalex.org/W4295371519",
    "https://openalex.org/W3032816972",
    "https://openalex.org/W2963430224",
    "https://openalex.org/W3184516261",
    "https://openalex.org/W4300822525",
    "https://openalex.org/W2953552933",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W3198708518",
    "https://openalex.org/W2950733326",
    "https://openalex.org/W2953092638",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2963918774",
    "https://openalex.org/W2963672008",
    "https://openalex.org/W2592634758",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2962843521",
    "https://openalex.org/W1722351164",
    "https://openalex.org/W3122515622"
  ],
  "abstract": "The field of natural language processing has seen numerous advancements since its inception during the Cold War, when attempts were made to automate translating Russian text to English. The 2010s have been a particularly fertile decade, going by sheer research volume – the use of a class of machine learning models known as neural networks has led to substantial improvements in performance on a broad range of language-related tasks, ranging from classification, to summarisation, to machine translation. In addition, some of these models have demonstrated an excellent multilingual capacity, where they can be trained to process text in English, and with only a bit of extra data, show solid results across languages. Yet these improvements, rapid as they have been, are not without their drawbacks. One such drawback is that we lack the tools to rationalise how and why these models function, particularly given the many (often arbitrary) decisions that went into their design. This dissertation adopts multiple analytical frameworks in an effort to answer questions surrounding the behaviour of multilingual neural models. How does the data they were trained on affect their performance? What linguistic capabilities can they exhibit? And finally, how do different model components help enable multilinguality?",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4483–4499,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n4483\nFrom Zero to Hero: On the Limitations of Zero-Shot Language Transfer\nwith Multilingual Transformers\nAnne Lauscher1∗, Vinit Ravishankar2∗, Ivan Vuli´c3, and Goran Glavaˇs1\n1Data and Web Science Group, University of Mannheim, Germany\n2Language Technology Group, University of Oslo, Norway\n3Language Technology Lab, University of Cambridge, UK\n1{anne,goran}@informatik.uni-mannheim.de,\n2vinitr@ifi.uio.no, 3iv250@cam.ac.uk\nAbstract\nMassively multilingual transformers (MMTs)\npretrained via language modeling (e.g.,\nmBERT, XLM-R) have become a default\nparadigm for zero-shot language transfer\nin NLP, offering unmatched transfer perfor-\nmance. Current evaluations, however, verify\ntheir efﬁcacy in transfers (a) to languages with\nsufﬁciently large pretraining corpora, and (b)\nbetween close languages. In this work, we an-\nalyze the limitations of downstream language\ntransfer with MMTs, showing that, much\nlike cross-lingual word embeddings, they are\nsubstantially less effective in resource-lean\nscenarios and for distant languages. Our\nexperiments, encompassing three lower-level\ntasks (POS tagging, dependency parsing,\nNER) and two high-level tasks (NLI, QA), em-\npirically correlate transfer performance with\nlinguistic proximity between source and target\nlanguages, but also with the size of target\nlanguage corpora used in MMT pretraining.\nMost importantly, we demonstrate that the\ninexpensive few-shot transfer (i.e., additional\nﬁne-tuning on a few target-language instances)\nis surprisingly effective across the board, war-\nranting more research efforts reaching beyond\nthe limiting zero-shot conditions.\n1 Introduction and Motivation\nLabeled datasets of sufﬁcient size support super-\nvised learning in NLP. The notorious tediousness,\nsubjectivity, and cost of linguistic annotation (Dan-\ndapat et al., 2009; Sabou et al., 2012; Fort, 2016),\ncoupled with plethora of structurally different NLP\ntasks, lead to existence of such datasets only for a\nhandful of resource-rich languages (Bender, 2011;\nPonti et al., 2019; Joshi et al., 2020). This data\nscarcity renders the need for effectivecross-lingual\ntransfer strategies: how can we exploit abundant\nlabeled data from resource-rich languages to make\n∗Equal contribution.\npredictions in resource-lean languages? In the most\nextreme scenario, termed zero-shot cross-lingual\ntransfer, not a single labeled instance exists for\na target language. Recent work has placed much\nemphasis on this scenario exactly; in theory, it of-\nfers the widest portability across the world’s 7,000+\nlanguages (Pires et al., 2019; Artetxe et al., 2020b;\nLin et al., 2019; Cao et al., 2020; Hu et al., 2020).\nThe current mainstay of cross-lingual transfer\nin NLP are approaches based on continuous cross-\nlingual representation spaces such as cross-lingual\nword embeddings (CLWEs) (Ruder et al., 2019)\nand, most recently, massively multilingual trans-\nformer networks (MMTs), pretrained on multilin-\ngual corpora with language modeling (LM) ob-\njectives (Devlin et al., 2019; Conneau and Lam-\nple, 2019; Conneau et al., 2020). The latter have\nde facto become the default language transfer\nparadigm, with multiple studies reporting their un-\nparalleled transfer performance (Pires et al., 2019;\nWu and Dredze, 2019; R ¨onnqvist et al., 2019;\nKarthikeyan et al., 2020; Wu et al., 2020).\nKey Questions and Contributions. In this work,\nwe dissect the current state-of-the-art MMT-based\napproach to (zero-shot) cross-lingual transfer, and\nanalyze a variety of conditions and factors that criti-\ncally impact or limit effective cross-lingual transfer.\nOur aim is to provide answers to the following\ncrucial questions.\n(Q1) What is the role of language (dis)similarity\nand language-speciﬁc corpora size in pretraining?\nCurrent cross-lingual transfer via MMTs is still\nprimarily focused on either (1) languages that are\ntypologically or etymologically close to English\n(e.g., German, Scandinavian languages, French,\nSpanish), or (2) languages with large monolingual\ncorpora, well-represented in the multilingual pre-\ntraining corpora (e.g., Arabic, Hindi, Chinese). Wu\net al. (2020) suggest that LM-pretrained transform-\n4484\ners, much like static word embeddings models, pro-\nduce topologically similar representation spaces\nthat can easily be aligned between languages, offer-\ning this as explanation of language transfer efﬁcacy\nof MMTs. However, transfer with static CLWEs\nhas been shown ineffective between dissimilar lan-\nguages (Søgaard et al., 2018; Vuli´c et al., 2019) or\nlanguages with small corpora (Vuli´c et al., 2020).\nWe thus scrutinize MMTs in diverse zero-shot\ntransfer settings and ﬁnd, in line with prior work\non CLWEs, that MMTs’ transfer performance crit-\nically depends on (1) linguistic (dis)similarity be-\ntween the source and target language and (2) size\nof the pretraining corpus of the target language.\n(Q2) What is the role of a particular task in consid-\neration for transfer performance?\nWe conduct all analyses across ﬁve different tasks,\nwhich we roughly divide into two groups: (1) “low-\nlevel” tasks (POS-tagging, dependency parsing,\nand NER); and (2) “high-level” language under-\nstanding (LU) tasks (NLI and QA). We show that\ntransfer performance in both zero-shot and few-\nshot scenarios largely depends on the “task level”.\n(Q3) Can we (even) predict transfer performance?\nRunning a simple regression on available transfer\nresults, we show that we can (roughly) predict the\ntransfer performance from (1) language proximity\n(Littell et al., 2017) for low-level tasks; (2) com-\nbination of language proximity and size of target-\nlanguage pretraining corpora for high-level tasks.\n(Q4) Should we focus more on few-shot transfer\nscenarios and quick annotation cycles?\nComplementing the efforts on improving zero-\nshot transfer (Cao et al., 2020), we point to few-\nshot transfer as a very effective mechanism for\nimproving target-language performance. Similar\nto the seminal “pre-neural” work of Garrette and\nBaldridge (2013), our results suggest that only sev-\neral hours (or even minutes) of annotation work\ncan “buy” substantial performance gains for low-\nresource targets. For all ﬁve tasks in our study, we\nobtain substantial (and in some cases surprisingly\nlarge) improvements with minimal annotation ef-\nfort. For instance, we improve dependency parsing\nfor some target languages up to 40 UAS points with\nas few as 10 target language sentences. Crucially,\nthe few-shot gains are most pronounced exactly\nwhere zero-shot transfer fails: for distant target\nlanguages with small monolingual corpora.\n2 Background and Related Work\nFor completeness, we provide a brief overview of\n1) cross-lingual transfer approaches, with a focus\non 2) massively multilingual transformer (MMT)\nmodels, and then 3) position our work w.r.t. other\nstudies that examine different properties of MMTs.\n2.1 Cross-Lingual Transfer Paradigms\nLanguage transfer entails representing texts from\nboth the source and target language in a shared\ncross-lingual space. Transfer paradigms based\non discrete text representations include machine\ntranslation (MT) of target language text to the\nsource language (or vice-versa) (Mayhew et al.,\n2017; Eger et al., 2018), and grounding texts from\nboth languages in multilingual knowledge bases\n(KBs) (Navigli and Ponzetto, 2012; Lehmann et al.,\n2015). While reliable MT hinges on availability\nof large parallel corpora, transfer via multilingual\nKBs (Camacho-Collados et al., 2016; Mrkˇsi´c et al.,\n2017) is impaired by the limited KB coverage\nand inaccurate entity linking (Moro et al., 2014;\nRaiman and Raiman, 2018).\nTherefore, recent years have seen a surge of lan-\nguage transfer methods based on continuous rep-\nresentation spaces. The previous state-of-the-art,\ncross-lingual word embeddings (CLWEs) (Mikolov\net al., 2013; Ammar et al., 2016; Artetxe et al.,\n2017; Smith et al., 2017; Glavaˇs et al., 2019; Vuli´c\net al., 2019) and sentence embeddings (Artetxe and\nSchwenk, 2019), have most recently been replaced\nby massively multilingual transformers (MMTs)\npretrained with LM objectives (Devlin et al., 2019;\nConneau and Lample, 2019; Conneau et al., 2020).\n2.2 Massively Multilingual Transformers\nMultilingual BERT (mBERT). At BERT’s (De-\nvlin et al., 2019) core is a multi-layer transformer\nnetwork (Vaswani et al., 2017), parameters of\nwhich are pretrained using masked language mod-\neling (MLM) and next sentence prediction (NSP).\nIn MLM, some tokens are masked out and they\nneed to be recovered from the context; NSP pre-\ndicts adjacency of sentences in text, informing\nthe transformer of longer dependencies, beyond\nsentence boundaries. Liu et al. (2019) introduce\nRoBERTa, a more robust instance of BERT trained\non larger corpora using only the MLM objective.\nMultilingual BERT (mBERT) is an instance of\nBERT trained on concatenation of 104 largest\nWikipedias. The effects of underﬁtting for lan-\n4485\nguages with small Wikipedias and overﬁtting to lan-\nguages with large Wikipedias, are respectively at-\ntenuated with exponentially smoothed up-sampling\nand down-sampling.\nXLM on RoBERTa (XLM-R).XLM-R (Conneau\net al., 2020) is an instance of RoBERTa, robustly\ntrained on a large multilingual CommonCrawl-100\n(CC-100) corpus (Wenzek et al., 2019) covering\n100 languages. mBERT’s corpus and CC-100 share\n88 languages, with corresponding CC-100’s por-\ntions being much larger than mBERT’s Wikipedias.\nThe “Curse of Multilinguality”. For XLM-R,\nConneau et al. (2020) observe that for a ﬁxed\nmodel capacity, downstream cross-lingual trans-\nfer improves with more pretraining languages up\nto a point after which adding more pretraining\nlanguages hurts downstream transfer. This effect,\ntermed the “curse of multilinguality”, can be miti-\ngated by increasing model’s capacity (Artetxe et al.,\n2020b) or additional training for particular lan-\nguage pairs (Pfeiffer et al., 2020). This points to\nMMTs’ capacity (i.e., computational budgets), as a\ncritical factor for effective zero-shot transfer.\nIn contrast, we identify few-shot transfer as a\nmuch more cost-effective strategy for improving\ndownstream target language performance (§4). We\nshow for a number of target languages and down-\nstream tasks, that one can obtain large performance\ngains at very small annotation cost, without having\nto pretrain from scratch an MMT of larger capacity.\n2.3 Cross-Lingual Transfer with MMTs\nA body of recent work probed the knowledge en-\ncoded in MMTs, primarily mBERT. Libovick`y et al.\n(2020) analyze language-speciﬁc versus language-\nuniversal knowledge encoded in mBERT. Pires\net al. (2019) demonstrate mBERT to be effective for\nPOS-tagging and NER zero-shot transfer between\nrelated languages. Wu and Dredze (2019) extend\nthis analysis to more tasks and languages, and show\nthat mBERT-based transfer is on a par with the best\ntask-speciﬁc zero-shot transfer approaches. Simi-\nlarly, Karthikeyan et al. (2020) prove mBERT to\nbe effective for NER and NLI transfer to Hindi,\nSpanish, and Russian. 1 Importantly, they show\nthat transfer effectiveness does not depend on the\nvocabulary overlap between the languages.\nIn most recent work, concurrent to this, Hu et al.\n(2020) introduce XTREME, a benchmark for eval-\n1Note that all three are high-resource Indo-European lan-\nguages with large Wikipedias.\nuating multilingual encoders encompassing 9 tasks\nand 40 languages. 2 While the primary focus is\na large-scale zero-shot transfer evaluation, they\nalso experiment with target-language ﬁne-tuning\n(1,000 instances for POS and NER). While Hu\net al. (2020) focus on the evaluation aspects and\nprotocols, in this work, we provide a more detailed\nanalysis of the factors that hinder effective zero-\nshot transfer across several tasks. 3 We also put\nmore emphasis on few-shot transfer, and approach\nit differently: by sequentially ﬁne-tuning MMTs,\nﬁrst on (larger) source language training data and\nthen on few target-language instances.\nArtetxe et al. (2020b) and Wu et al. (2020) ana-\nlyze different monolingual BERTs to explain trans-\nfer efﬁcacy of mBERT. They ﬁnd topological sim-\nilarities between monolingual spaces, suggesting\nthese are responsible for effective language transfer\nwith MMTs. In essence, their work recasts the well-\nknown assumption of approximate isomorphism of\nmonolingual representation spaces (Søgaard et al.,\n2018). For CLWEs, this assumption does not hold\nfor distant languages (Søgaard et al., 2018; Vuli´c\net al., 2019), and in face of monolingual corpora\nof small size (Vuli´c et al., 2020). We demonstrate\nthat the same is the case for zero-shot language\ntransfer with MMTs: target-language performance\ndrastically decreases as we move to more distant\ntarget languages with smaller pretraining corpora.\n3 Zero-Shot Transfer: Analyses\nWe ﬁrst address Q1 and Q2 (see §1): we conduct\nzero-shot language transfer experiments for ﬁve\ndifferent tasks and analyze the factors behind the\nvarying performance drops across target languages.\n3.1 Experimental Setup\nTasks and Languages. We experiment with – a)\nlow-level structured prediction tasks: POS-tagging,\ndependency parsing, and NER and b) high-level\nlanguage understanding (LU) tasks: NLI and QA.\nWe investigate if the factors that drive transfer per-\nformance differ between the two task groups.\nDependency Parsing (DEP). We use Universal De-\npendency treebanks (UD, Nivre et al., 2017) for\nEnglish and following target languages (from 8 lan-\nguage families): Arabic (AR), Basque (EU), (Man-\n2Note that none of the individual tasks in XTREME covers\nall 40 languages, but much smaller language subsets.\n3We leave an even more general analysis that combines\ntransfer both across tasks (Pruksachatkun et al., 2020; Glavaˇs\nand Vuli´c, 2020) and across languages for future work.\n4486\ndarin) Chinese ( ZH), Finnish ( FI), Hebrew ( HE),\nHindi (HI), Italian (IT), Japanese (JA), Korean (KO),\nRussian (RU), Swedish (SV), and Turkish (TR).\nPart-of-speech Tagging(POS). Again, we use UD\nand obtain the Universal POS-tag (UPOS) annota-\ntions from the same treebanks as with DEP.\nNamed Entity Recognition (NER). We resort to the\nNER WikiANN dataset from Rahimi et al. (2019).\nWe experiment with the same set of 12 target lan-\nguages as in DEP and POS.\nCross-lingual Natural Language Inference (XNLI).\nWe evaluate on the XNLI corpus (Conneau et al.,\n2018) created by translating dev and test portions\nof the English Multi-NLI dataset (Williams et al.,\n2018) into 14 languages by professional translators\n(French (FR), Spanish (ES), German (DE), Greek\n(EL), Bulgarian (BG), Russian (RU), Turkish (TR),\nArabic (AR), Vietnamese (VI), Thai (TH), Chinese\n(ZH), Hindi (HI), Swahili (SW), and Urdu (UR)).\nCross-lingual Question Answering (XQuAD). We\nrely on the XQuAD dataset (Artetxe et al., 2020b),\ncreated by translating the 240 dev paragraphs (from\n48 documents) and corresponding 1,190 QA pairs\nof SQuAD v1.1 (Rajpurkar et al., 2016) to 11 lan-\nguages (ES, DE, EL, RU, TR, AR, VI, TH, ZH, and\nHI). In order to allow for a comparison between\nzero-shot and few-shot transfer (see§4), we reserve\n10 documents as the development set for our exper-\niments and evaluate on the remaining 38 articles.4\nFine-tuning. For higher-level tasks, we perform\nstandard downstream ﬁne-tuning of LM-pretrained\nmBERT and XLM-R. For lower-level tasks, we\ninstead freeze the transformer and train only task-\nspeciﬁc classiﬁers.5,6\nWe add the following task-speciﬁc architectures\non top of MMTs: for DEP we add the biafﬁne pars-\ning head (Dozat and Manning, 2017; Kondratyuk\nand Straka, 2019); for POS, we attach a simple\n4As a general note, while the effects of “translationese”\nmight have some impact on the absolute numbers (Artetxe\net al., 2020a), they are not prominent enough to have any\nimpact on the relative trends in the reported results (e.g., zero-\nshot vs. few-shot performance). For both XNLI and XQuAD,\nthe translations were done completely manually and not via\npost-editing of MT (which would pose a higher “translationese”\nrisk). Moreover, having an independently created test set in\neach language would impede comparability across languages.\n5This gave slightly better performance than ﬁne-tuning.\n6We tokenize the input for each model with the corre-\nsponding pretrained ﬁxed-vocabulary tokenizer: WordPiece\ntokenizer (Wu et al., 2016) with the vocabulary of 110K tokens\nfor mBERT, and the SentencePiece BPE tokenizer (Sennrich\net al., 2016) with the vocabulary of 250K tokens for XLM-R.\nfeed-forward token-level classiﬁer; for NER, we\nfeed MMT’s token-level outputs to a CRF classi-\nﬁer, similar to Peters et al. (2017). For XNLI, we\napply a simple softmax classiﬁer on the vector of\nthe sequence start token ([CLS] for mBERT;<s>\nfor XLM-R); for XQuAD, we pool MMT’s repre-\nsentations of all subwords and input it to a span\nclassiﬁcation head – a linear layer computing the\nstart and the end of the answer.\nTraining and Evaluation Details. We experiment\nwith mBERT Base cased and XLM-R Base, both\nwith L= 12 transformer layers, hidden state size\nof H = 768, and A= 12 self-attention heads.\nFor XNLI, we limit the inputs to T = 128 sub-\nword tokens and train in batches of 32 instances.\nFor XQuAD, we limit paragraphs to T = 384 to-\nkens and questions to Q = 64 tokens. We slide\nover paragraphs with a window of 128 tokens and\ntrain in batches of size 12. For XNLI and XQuAD,\nwe search the following hyperparameter grid: learn-\ning rate λ∈{5 ·10−5,3 ·10−5}; training epochs\nn ∈ {2,3}. For DEP, POS and NER, we ﬁx\nthe number of training epochs to 20. We train in\nbatches of 32 sentences, with maximal length of\nT = 512 subword tokens. We optimize all models\nwith Adam (Kingma and Ba, 2015).\nWe report DEP performance in terms of Unla-\nbeled Attachment Scores (UAS).7 For POS, NER,\nand XNLI we report accuracy, and for XQuAD, we\nreport the Exact Match (EM) score.\n3.2 Results and Preliminary Discussion\nA summary of the zero-shot cross-lingual transfer\nresults, per target language, is provided in Table 1.\nAs expected, we observe drops in performance for\nall tasks and all target languages w.r.t. reference\nEN performance. However, the drops vary greatly\nacross languages. For example, NER (mBERT)\ndrops mere 2.6% for IT, but enormous 32% for AR;\nXNLI transfer (XLM-R) yields a moderate 6.1%\ndrop for FR, but a large 20% drop for SW, etc.\nAt ﬁrst glance, it appears – as suggested in\nprior work – that the transfer drops primarily cor-\nrelate with language proximity: they are more pro-\nnounced for languages that are more distant from\nEN (e.g., JA, ZH, AR, TH, SW). While we see no no-\ntable exception to this in the three lower-level tasks,\nlanguage proximity alone does not explain many\n7Using Labeled Attachment Score (LAS) would make\ndifferences in annotation schemes between languages a con-\nfounding factor and impede our analysis of effects of language\nproximity and size of the target language corpora.\n4487\nEN ZH TR RU AR HI EU FI HE IT JA KO SV VI TH ES EL DE FR BG SW UR\nTask Model EN ∆ ∆ ∆ ∆ ∆ ∆ ∆ ∆ ∆ ∆ ∆ ∆ ∆ ∆ ∆ ∆ ∆ ∆ ∆ ∆ ∆\nDEP B 91.2 -43.9 -46.0 -28.1 -56.4 -36.1 -50.2 -30.7 -36.1 -17.1 -60.1 -56.1 -14.3 - - - - - - - - -\nX 92.0 -85.4 -44.2 -29.7 -54.6 -39 -49.5 -26.7 -39 -23.5 -80.5 -56.0 -16.3 - - - - - - - - -\nPOS B 95.8 -38.0 -35.9 -16.0 -40.1 -33.4 -34.6 -21.9 -33.4 -19.8 -46.1 -42.0 -9.6 - - - - - - - - -\nX 96.3 -69.2 -27.7 -14.3 -37.1 -27.3 -31.9 -17.9 -27.3 -19.0 -77.0 -37.3 -10.7 - - - - - - - - -\nNER B 92.4 -23.3 -11.6 -10.7 -31.7 -11.1 -12.8 -3.8 -11.1 -2.6 -25.7 -13.8 -6.7 - - - - - - - - -\nX 91.6 -34.8 -6.2 -13.7 -24.6 -16.5 -8.0 -0.9 -16.5 -2.4 -30.1 -15.6 -2.2 - - - - - - - - -\nXNLI B 82.8 -13.6 -20.6 -13.5 -17.3 -21.3 - - - - - - - -11.9 -28.1 -8.1 -14.1 -10.5 -7.8 -13.3 -33.0 -23.4\nX 84.3 -11.0 -11.3 -9.0 -13.0 -14.2 - - - - - - - -9.7 -12.3 -5.8 -8.9 -7.8 -6.1 -6.6 -20.2 -17.3\nXQuAD B 71.1 -22.9 -34.2 -19.2 -24.7 -28.6 - - - - - - - -22.1 -43.2 -16.6 -28.2 -14.8 - - - -\nX 72.5 -26.2 -18.7 -15.4 -24.1 -22.8 - - - - - - - -19.7 -14.8 -14.5 -15.7 -16.2 - - - -\nTable 1: Zero-shot cross-lingual transfer performance on ﬁve tasks (DEP, POS, NER, XNLI, and XQuAD) with\nmBERT (B) and XLM-R (X). We show the monolingualEN performance and report drops in performance relative\nto EN for all target languages. Numbers in bold indicate the largest zero-shot performance drops for each task.\nof the XNLI and XQuAD results. For instance, RU\nXNLI (for both mBERT and XLM-R) is compara-\nble to that of ZH, and lower than that for HI and\nUR: this is despite the fact that, as Indo-European\nlanguages, RU, HI, and UR are linguistically closer\nto EN than ZH. Similarly, we observe comparable\nperformance on XQuAD for TH, RU, and ES.\n3.3 Analysis\nFor each task, we now analyze the correlations\nbetween transfer performance and a) several mea-\nsures of linguistic proximity (i.e., similarity) be-\ntween languages and b) the size of MMT pretrain-\ning corpora of each target language.\nLanguage Vectors and Corpora Sizes. For es-\ntimates of linguistic similarity, we rely on lan-\nguage vectors from LANG 2VEC , which encode var-\nious linguistic features from the URIEL database\n(Littell et al., 2017). We consider the following\nLANG 2VEC vectors: syntax (SYN ) vectors en-\ncode syntactic properties, e.g., if a subject appears\nbefore or after a verb; phonology (PHON ) vec-\ntors encode phonological properties such as the\nconsonant-vowel ratio; inventory (INV ) vec-\ntors denote presence or absence of natural classes\nof sounds (e.g., voiced uvulars); FAM vectors en-\ncode memberships in language families;\nand GEO vectors express orthodromic distances\nfor languages w.r.t. ﬁxed points on the Earth’s sur-\nface. Language proximity is computed as cosine\nsimilarity between the languages’ corresponding\nLANG 2VEC vectors: each vector type (e.g., SYN)\nproduces one similarity score (i.e., feature). We\ncouple LANG 2VEC features with the z-normalized\nsize of the target language corpus used in MMT\npretraining (SIZE). 8\n8For XLM-R, we take reported sizes of language-speciﬁc\nCorrelation Analysis. We ﬁrst correlate individ-\nual features with the zero-shot transfer scores for\neach task and show the results in Table 2. Quite\nintuitively, the zero-shot performance for low-level\nsyntactic tasks – POS and DEP – highly corre-\nlates with syntactic language similarity ( SYN ).\nSYN also correlates well with transfer results for\nhigh-level tasks (except with XLM-R results on\nXQuAD). Somewhat surprisingly, the phonological\nlanguage similarity (PHON ) correlates best with\ntransfer performance with XLM-R, for all tasks ex-\ncept XNLI, and also for mBERT on POS. For both\nhigh-level tasks and both MMTs, we observe very\nhigh correlations between transfer performance and\nsize of pretraining corpora of the target language\n(SIZE ). In contrast, SIZE exhibits lower correla-\ntions for lower-level tasks (DEP, POS, NER). We\nbelieve that this reﬂect the fact that high-level LU\ntasks rely on rich representations of semantic phe-\nnomena of a language, whereas low-level tasks\nrequire simpler structural representation of a lan-\nguage – it simply takes more distributional data to\nacquire the former than the latter.\nMeta-Regression. Across the tasks, we observe\nhigh correlations between zero-shot transfer results\nand several features (e.g.,SYN, PHON and SIZE ).\nWe next test if we can predict the transfer perfor-\nmance for a new language, by (linearly) combining\nindividual features. For each task, we ﬁt a linear re-\ngression using transfer results for target languages\nas labels. With only between 11 and 14 target lan-\nguages (i.e., instances for ﬁtting the regressor) per\ntask, we resort to leave-one-out cross-validation\n(LOOCV) to obtain correlations for feature com-\nCC-100 portions (Conneau et al., 2020); for mBERT, we work\nwith sizes of language-speciﬁc Wikipedias.\n4488\nSYN PHON INV FAM GEO SIZE\nTask Model P S P S P S P S P S P S\nDEP XLM-R 0.77 0.78 0.83 0.77 0.46 -0.04 0.68 0.61 0.80 0.81 0.62 0.47\nmBERT 0.92 0.91 0.79 0.74 0.55 -0.01 0.76 0.62 0.64 0.69 0.79 0.59\nPOS XLM-R 0.68 0.79 0.81 0.81 0.38 0.02 0.58 0.74 0.80 0.73 0.54 0.46\nmBERT 0.90 0.87 0.86 0.81 0.57 0.02 0.82 0.80 0.66 0.72 0.47 0.39\nNER XLM-R 0.49 0.49 0.80 0.83 0.27 0.14 0.47 0.55 0.77 0.81 0.37 0.35\nmBERT 0.60 0.74 0.81 0.84 0.34 -0.04 0.53 0.58 0.59 0.73 0.42 0.38\nXNLI XLM-R 0.88 0.90 0.29 0.27 0.31 -0.11 0.63 0.54 0.54 0.74 0.70 0.76\nmBERT 0.87 0.86 0.21 0.08 0.29 0.04 0.61 0.47 0.55 0.67 0.77 0.91\nXQuAD XLM-R 0.69 0.53 0.85 0.81 0.62 -0.01 0.81 0.54 0.43 0.50 0.81 0.55\nmBERT 0.84 0.89 0.56 0.48 0.55 0.22 0.79 0.64 0.51 0.55 0.89 0.96\nTable 2: Correlations between zero-shot transfer performance with mBERT and XLM-R for different downstream\ntasks with linguistic proximity features (SYN, PHON, INV, FAM and GEO) and pretraining size of target-\nlanguage corpora (SIZE). Results reported in terms of Pearson (P) and Spearman (S) correlation coefﬁcients.\nTask Model Selected features P S MAE\nPOS X PHON (.75); GEO (.25) 0.77 0.75 10.99\nB SYN (.99) 0.94 0.90 4.60\nDEP X PHON (.25); SYN (.18) 0.81 0.89 10.14GEO (.57)\nB SYN(.99) 0.93 0.92 5.77\nNER X PHON(.99) 0.80 0.88 4.64\nB PHON(.99) 0.69 0.82 9.45\nXNLI\nX SYN (.51); SIZE (.49) 0.84 0.85 2.01\nB SYN (.35); SIZE (.34), 0.89 0.90 2.78FAM (.31)\nXQuAD X PHON (.99) 0.95 0.83 2.89\nB SIZE (.99) 0.89 0.93 4.76\nTable 3: Results of the meta-regression analysis,\ni.e., predicting zero-shot transfer performance for\nmBERT (B) and XLM-R (X). For each task-model pair\nwe list only features with weights ≥0.01. P=Pearson;\nS=Spearman; MAE=Mean Absolute Error.\nbinations. We perform greedy forward feature se-\nlection: in each iteration we add the feature which\nboosts correlation (obtained via LOOCV) the most;\nwe stop when none of the remaining features fur-\nther improves the Pearson correlation.\nWe summarize the results of this meta-regression\nanalysis in Table 3. For each task-model pair, we\nlist features selected with the greedy feature selec-\ntion and show (normalized) weights assigned to\neach feature. Except for NER, combinations of fea-\ntures manage to yield higher correlations with zero-\nshot transfer results than any of the features on their\nown. These results empirically conﬁrm our previ-\nous intuition that linguistic proximity between the\nsource and target language only partially explains\nzero-short transfer performance. On XNLI, transfer\nperformance is best explained with the combination\nof structural similarity between languages (SYN )\nand the size of the target-language pretraining cor-\npora (SIZE); on XQuAD with mBERT,SIZE alone\nbest explains zero-short transfer scores. Note that\nthe features are mutually quite correlated as well\n(e.g., languages closer toEN also tend to have larger\npretraining corpora): thus if the regressor selects\nonly one feature, this does not mean that other fea-\ntures do not correlate with transfer performance (as\nshown by Table 2).\nThe coefﬁcients in Table 3 again indicate the\nimportance of SIZE for the language understand-\ning tasks and highlight our core ﬁnding: pretrain-\ning corpora sizes are stronger features for predict-\ning zero-shot performance in higher-level tasks,\nwhereas the results in lower-level tasks are more\naffected by typological language proximity.\n4 From Zero to Hero: Few-Shot\nMotivated by the low zero-shot transfer perfor-\nmance for many tasks and languages obtained in\n§3, we now investigate Q4 from §1: we aim to\nmitigate transfer losses with inexpensive few-shot\ncross-lingual transfer.\nExperimental Setup. We rely on the same mod-\nels, tasks, and evaluation protocols as described in\n§3.1. However, instead of ﬁne-tuning the MMTs\non task-speciﬁc data in EN only, we continue the\nﬁne-tuning process by feeding kadditional training\nexamples randomly chosen from reserved target\nlanguage data portions, disjoint with the test sets.9\n9Note that for XQuAD, we performed the split on the arti-\ncle level to avoid topical overlap. Consequently, for XQuAD\n4489\nk k = 10 k = 50 k = 100 k = 500 k = 1000\nTask Model k = 0 score ∆ score ∆ score ∆ score ∆ score ∆\nDEP MBERT 52.96 66.69 13.73 72.67 19.70 74.8 21.84 80.47 27.5 82.74 29.77\nXLM-R 48.60 65.57 16.97 72.19 23.59 74.08 25.48 81.16 32.56 83.33 34.73\nPOS MBERT 67.2 80.17 12.96 85.34 18.14 87.09 19.88 91.16 23.96 92.64 25.44\nXLM-R 65.5 80.68 15.18 85.7 20.2 87.59 22.09 91.35 25.85 92.80 27.3\nNER MBERT 79.34 83.18 3.84 84.54 5.20 85.25 5.91 87.9 8.56 89.31 9.97\nXLM-R 85.43 88.06 2.63 91.07 5.64 91.49 6.06 93.69 8.26 93.82 8.39\nXNLI MBERT 65.92 65.89 -0.03 65.08 -0.84 64.92 -1.00 67.41 1.49 68.16 2.24\nXLM-R 73.32 73.73 0.41 73.76 0.45 75.03 1.71 75.34 2.02 75.84 2.52\nk = 2 k = 4 k = 6 k = 8 k = 10\nXQUAD MBERT 45.62 48.12 2.50 48.66 3.04 49.34 3.72 49.91 4.29 50.19 4.57\nXLM-R 53.68 53.73 0.05 53.84 0.17 54.76 1.08 55.56 1.88 55.78 2.10\nTable 4: Results of the few-shot experiments with varying numbers of target-language examplesk. For each k, we\nreport performance averaged across languages and the difference (∆) with respect to the zero-shot setting.\nFor our low-level tasks, we compare three sampling\nmethods: (i) random sampling (RAND ) of ktarget\nlanguage sentences, (ii) selection of the k short-\nest (SHORTEST ) and (iii) the klongest (LONGEST )\nsentences.10 For XNLI and XQuAD, we run the ex-\nperiments ﬁve times and report the average scores.\n4.1 Results and Discussion\nThe results on each task, conditioned on the num-\nber of examples k and averaged across all target\nlanguages, are presented in Table 4. We note\nsubstantial improvements in few-shot learning se-\ntups for all tasks. However, the results also re-\nveal notable differences between different types\nof tasks. For higher-level language understanding\ntasks the improvements are less pronounced; the\nmaximum gains for XNLI and XQuAD after seeing\nk = 1,000 target-language instances and 10 arti-\ncles, respectively, are between 2.52 (XLM-R) and\n4.57 points (mBERT). On the other hand, the aver-\nage gains for the lower-level tasks are massive: be-\ntween 10 (NER) and 30 (DEP) points for mBERT\nand 8 (NER) and 35 (DEP) points for XLM-R.\nMoreover, the gains in all lower-level tasks are\nsubstantial even when we add only 10 annotated\nsentences in the target language (on average, up to\n17 points on DEP, and 15 points on POS). What\nis more, our additional experiments (omitted for\nbrevity) show substantial gains for DEP and POS\neven with fewer than 5 annotated target language\nsentences. A comparison of different sampling\nstrategies for the lower-level tasks is shown in Fig-\nk refers to the number of articles.\n10In all three cases, we only choose between sentences with\n≥ 3 and ≤ 50 tokens.\nure 1 for mBERT.11 For DEP and POS, the pattern\nis very clear and quite expected – adding longer\nsentences results in better scores. For NER, how-\never, random sampling (RAND ) appears to perform\nbest: we hypothesize that this is because: (i) very\nlong sentences are relatively sparse with named en-\ntities, resulting in our model seeing mostly negative\nexamples; (ii) shorter sentences contribute less than\nfor DEP and POS because they typically consist of\n(conﬁrmed by manual inspection) a single named\nentity mention, without any non-NE tokens.\nFigure 2 illustrates few-shot performance for in-\ndividual languages on two lower-level (DEP, NER)\nand two higher-level tasks (XNLI, XQuAD), for\ndifferent values of k.12 Across languages, we see a\nclear trend – more distant target languages beneﬁt\nmuch more from the few-shot data. Observe, e.g.,\nSV for DEP or DE for XQuAD. Both are closely\nrelated to EN, exhibit high zero-shot transfer per-\nformance, and beneﬁt only marginally from few in-\nlanguage instances. We hypothesize that for such\nclosely related languages, with enough pretrain-\ning data, MMT is able to extrapolate the missing\nlanguage-speciﬁc knowledge from few in-language\nexamples; its priors for languages close to EN are\nalready quite sensible and a priori offer less room\nfor improvements. In stark contrast, KO (DEP, a)\nand TH (XQuAD, b), for example, both exhibit\npoor zero-shot performance and understandably\nso, given their linguistic distance to EN. Given in-\nlanguage data, however, both see rapid leaps in per-\nformance, displaying gains of almost 40% UAS on\n11A similar analysis for XLM-R is in the supplementary.\n12We show per-language scores for POS with mBERT, and\nall tasks with XLM-R in the Appendix.\n4490\nFigure 1: Heatmap of performance gains for low-level tasks from few-shot transfer with mBERT for different\nsampling strategies. X-axis: number of target-language instances k; Y-axis: sampling strategy.\n(a) DEP\n (b) NER\n(c) XQuAD\n (d) XNLI\nFigure 2: Few-shot transfer results with mBERT for each language with varyingkfor two low-level tasks: a) DEP,\nb) NER, and two higher-level tasks: c) XQuAD, d) XNLI. For DEP, NER, and XNLI k denotes the number of\nsampled sentences, for XQuAD, the number of sampled articles.\nDEP (KO), and almost 5% on XQuAD (TH). This\ncan be seen as MMTs’ ability to rapidly learn to uti-\nlize the multilingual space to adjust its task-speciﬁc\nknowledge for the target language. Other interest-\ning patterns emerge. Particularly interesting are\nDEP results for JA and AR, where we observe mas-\nsive UAS improvements with only 10 annotated\nsentences. For XQuAD, we observe a substantial\nimprovement from only 2 in-language documents\nfor TH. In sum, we see the largest gains from few-\nshot transfer exactly for languages for which the\nzero-shot transfer setup yields largest performance\ndrops: languages distant from EN and represented\nwith small corpora in MMT pretraining.\nDirect Target Language Few-Shot Fine-Tuning.\nWe have additionally run a set of control experi-\nments in which we bypass the task-speciﬁc ﬁne-\ntuning on the Enhlish data and directly ﬁne-tune\nthe MMTs on the few target language instances.\nExpectedly, for high-level LU tasks, ﬁne-tuning\nthe MMTs with only a handful of target language\nexamples (i.e., without prior ﬁne-tuning in English)\nyields subpar performance w.r.t. the corresponding\nmodel variant that had been previously ﬁne-tuned\non English data. For instance, direct few-shot target\nlanguage ﬁne-tuning of mBERT yields the average\nXNLI performance of 33.95 for k= 100 and 40.19\nfor k = 1 ,000, respectively (compared to 64.92\nand 68.16, respectively, when prior ﬁne-tuning on\nEnglish data is performed). These ﬁndings suggest\nthat ﬁne-tuning with abundant (English) in-task\ndata plus ﬁne-tuning with scarce in-language in-\ntask data yields a truly synergistic effect for higher-\nlevel language understanding tasks: the small num-\nber of examples in the target language is not suf-\nﬁcient to adapt the MMT directly, but they can\nprovide a substantial edge over ﬁne-tuning only on\nthe English data (i.e., zero-shot transfer).\nSomewhat surprisingly, however, for the sim-\npler lower-level tasks, omitting task-speciﬁc ﬁne-\n4491\nTask #inst. Cost est. ∆ mBERT ∆ XLM-R\nPOS 1K sents $73 +25.4 +27.3\nDEP 1K sents $280 +29.8 +34.7\nNER 1K sents $60 +10 +8.4\nNLI 1K sent. pairs $10 +2.24 +2.54\nQA 10 docs $30 +4.5 +2.1\nTable 5: Conversion rates between target language an-\nnotation costs and corresponding average performance\ngains from MMT-based few-shot language transfer.\ntuning on the English data and ﬁne-tuning only\non few target language instances does not lead to\nthe major deterioration of performance (in fact, in\nsome cases, omitting to ﬁne-tune the MMTs on\nEnglish data even slightly improves the results):\nfor NER (mBERT) we obtain the average per-\nformance of 82.89 and 89.76 for k = 100 and\nk = 1 ,000 respectively, compared to 85.25 and\n89.31 obtained respectively with prior English ﬁne-\ntuning; for POS, the direct few-shot target language\nﬁne-tuning yields 87.08 (k = 100 ) and 92.64\n(k = 1,000). We observe the same trends for the\nremaining tasks and with XLM-R. This suggests\nthat MMTs can be ﬁne-tuned for lower-level (i.e.,\nsimpler) tasks with only a handful of instances.\n4.2 Cost of Language Transfer Gains\nAs shown in §4.1, moving to few-shot transfer can\nmassively improve performance and reduce the\ngaps observed with zero-shot transfer, especially\nfor low-resource languages. While additional ﬁne-\ntuning on few target-language examples is com-\nputationally cheap, data annotation may be expen-\nsive, especially for minor languages. What are the\nannotation costs, and how do they translate into\nperformance gains? Table 5 provides ballpark es-\ntimates for our ﬁve evaluation tasks; the estimates\nare based on annotation costs from the literature\n(Hovy et al., 2014; Tratz, 2019; Bontcheva et al.,\n2017; Marelli et al., 2014; Rajpurkar et al., 2016).\nWe explain these cost-to-gain conversion estimates\nin more detail in Appendix C).\nA provocative high-level question that calls for\nfurther discussion in future work can be framed as:\nare GPU hours effectively more costly13 than data\nannotations are in the long run? While MMTs are\nextremely useful as general-purpose models of lan-\nguage, their potential for some (target) languages\ncan be quickly unlocked by pairing them with a\nsmall number of annotated target-language exam-\n13Financially, but also ecologically (Strubell et al., 2019).\nples. Effectively, this suggests leveraging the best\nof both worlds, i.e., coupling knowledge encoded\nin large MMTs with a small annotation effort.\n5 Conclusion\nResearch on zero-shot language transfer in NLP\nis motivated by inherent data scarcity: the fact\nthat most languages have no annotated data for\nmost NLP tasks. Massively multilingual transform-\ners (MMTs) have recently been praised for their\nzero-shot transfer capabilities that mitigate the data\nscarcity issue. In this work, we have demonstrated\nthat, similar to earlier language transfer paradigms,\nMMTs perform poorly in zero-shot transfer to dis-\ntant target languages, and to languages with smaller\nmonolingual corpora available for exploitation in\nMMT pretraining. We have presented a detailed\nempirical analysis of factors affecting zero-shot\ntransfer performance of MMTs across diverse tasks\nand languages. Our results have revealed that struc-\ntural language similarity determines the transfer\nsuccess for lower-level tasks like POS-tagging and\ndependency parsing; on the other hand, the pretrain-\ning corpora size of the target language is crucial for\nexplaining transfer results for higher-level language\nunderstanding tasks, such as question answering\nand natural language inference.\nFinally and most importantly, we have shown\nthat the MMT potential on distant and low-resource\ntarget languages can be quickly unlocked if they\nare provided a handful of annotated instances in\nthe target language. This ﬁnding provides a strong\nincentive for intensifying future research efforts\nthat focus on cheap or naturally occurring super-\nvision (Vuli´c et al., 2019; Artetxe et al., 2020c;\nMarchisio et al., 2020), quick and simple annota-\ntion procedure, and the more effective few-shot\ntransfer learning setups.\nAcknowledgements\nThe work of Anne Lauscher and Goran Glava ˇs\nhas been supported by the Eliteprogramm of the\nBaden-W¨urttemberg Stiftung (Grant “AGREE: Al-\ngebraic Reasoning over Events from Text and Ex-\nternal Knowledge”). The work of Ivan Vuli´c is sup-\nported by the ERC Consolidator Grant LEXICAL\n(no 648909). We thank the anonymous reviewers\nfor their suggestions and insightful comments.\n4492\nReferences\nWaleed Ammar, George Mulcaire, Miguel Ballesteros,\nChris Dyer, and Noah A. Smith. 2016. Many lan-\nguages, one parser. Transactions of the Association\nfor computational Linguistics, 4:431–444.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.\nLearning bilingual word embeddings with (almost)\nno bilingual data. In Proceedings of ACL , pages\n451–462.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\n2020a. Translation artifacts in cross-lingual transfer\nlearning. CoRR, abs/2004.04721.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020b. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of ACL ,\npages 4623–4637.\nMikel Artetxe, Sebastian Ruder, Dani Yogatama,\nGorka Labaka, and Eneko Agirre. 2020c. A call\nfor more rigor in unsupervised cross-lingual learn-\ning. In Proceedings of ACL, pages 7375–7388.\nMikel Artetxe and Holger Schwenk. 2019. Mas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. Transac-\ntions of the Association for Computational Linguis-\ntics, 7:597–610.\nEmily M. Bender. 2011. On achieving and evaluating\nlanguage-independence in NLP. Linguistic Issues in\nLanguage Technology, 6(3):1–26.\nKalina Bontcheva, Leon Derczynski, and Ian Roberts.\n2017. Crowdsourcing named entity recognition and\nentity linking corpora. In Nancy Ide and James\nPustejovsky, editors, Handbook of Linguistic Anno-\ntation, pages 875–892.\nJos´e Camacho-Collados, Mohammad Taher Pilehvar,\nand Roberto Navigli. 2016. Nasari: Integrating ex-\nplicit knowledge and corpus statistics for a multilin-\ngual representation of concepts and entities. Artiﬁ-\ncial Intelligence, 240:36–64.\nSteven Cao, Nikita Kitaev, and Dan Klein. 2020. Mul-\ntilingual alignment of contextual word representa-\ntions. In Proceedings of ICLR.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of ACL, pages 8440–8451.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Proceedings\nof NeurIPS, pages 7057–7067.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. XNLI: Evaluating\ncross-lingual sentence representations. In Proceed-\nings of EMNLP, pages 2475–2485.\nSandipan Dandapat, Priyanka Biswas, Monojit Choud-\nhury, and Kalika Bali. 2009. Complex linguistic\nannotation—no easy way out!: A case from Bangla\nand Hindi POS labeling tasks. In Proceedings of the\n3rd Linguistic Annotation Workshop, pages 10–18.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL-HLT , pages\n4171–4186.\nTimothy Dozat and Christopher D. Manning. 2017.\nDeep biafﬁne attention for neural dependency pars-\ning. In Proceedings of ICLR.\nSteffen Eger, Johannes Daxenberger, Christian Stab,\nand Iryna Gurevych. 2018. Cross-lingual argumen-\ntation mining: Machine translation (and a bit of pro-\njection) is all you need! In Proceedings of COLING,\npages 831–844.\nKar¨en Fort. 2016. Collaborative Annotation for Reli-\nable Natural Language Processing: Technical and\nSociological Aspects. John Wiley & Sons.\nDan Garrette and Jason Baldridge. 2013. Learning a\npart-of-speech tagger from two hours of annotation.\nIn Proceedings of NAACL-HLT, pages 138–147.\nGoran Glavaˇs, Robert Litschko, Sebastian Ruder, and\nIvan Vuli´c. 2019. How to (properly) evaluate cross-\nlingual word embeddings: On strong baselines, com-\nparative analyses, and some misconceptions. In Pro-\nceedings of ACL, pages 710–721.\nGoran Glavaˇs and Ivan Vuli´c. 2020. Is supervised syn-\ntactic parsing beneﬁcial for language understanding?\nAn empirical investigation. CoRR, abs/2008.06788.\nDirk Hovy, Barbara Plank, and Anders Søgaard. 2014.\nExperiments with crowdsourced re-annotation of a\nPOS tagging data set. In Proceedings of ACL, pages\n377–382.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual general-\nization. In Proceedings of ICML.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the NLP\nworld. In Proceedings of ACL, pages 6282–6293.\nK Karthikeyan, Zihan Wang, Stephen Mayhew, and\nDan Roth. 2020. Cross-lingual ability of multilin-\ngual bert: An empirical study. In Proceedings of\nICLR.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proceedings\nof ICLR.\n4493\nDan Kondratyuk and Milan Straka. 2019. 75 lan-\nguages, 1 model: Parsing universal dependencies\nuniversally. In Proceedings of EMNLP-IJCNLP ,\npages 2779–2795.\nJens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,\nDimitris Kontokostas, Pablo N. Mendes, Sebastian\nHellmann, Mohamed Morsey, Patrick Van Kleef,\nS¨oren Auer, et al. 2015. DBpedia–a large-scale, mul-\ntilingual knowledge base extracted from Wikipedia.\nSemantic Web, 6(2):167–195.\nJindˇrich Libovick `y, Rudolf Rosa, and Alexander\nFraser. 2020. On the language neutrality of pre-\ntrained multilingual representations. arXiv preprint\narXiv:2004.05160.\nYu-Hsiang Lin, Chian-Yu Chen, Jean Lee, Zirui Li,\nYuyan Zhang, Mengzhou Xia, Shruti Rijhwani,\nJunxian He, Zhisong Zhang, Xuezhe Ma, Antonios\nAnastasopoulos, Patrick Littell, and Graham Neubig.\n2019. Choosing transfer languages for cross-lingual\nlearning. In Proceedings of ACL, pages 3125–3135.\nPatrick Littell, David R. Mortensen, Ke Lin, Kather-\nine Kairis, Carlisle Turner, and Lori Levin. 2017.\nURIEL and lang2vec: Representing languages as\ntypological, geographical, and phylogenetic vectors.\nIn Proceedings of EACL, pages 8–14.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv:1907.11692.\nKelly Marchisio, Kevin Duh, and Philipp Koehn. 2020.\nWhen does unsupervised machine translation work?\nCoRR, abs/2004.05516.\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa\nBentivogli, Raffaella Bernardi, and Roberto Zampar-\nelli. 2014. A SICK cure for the evaluation of compo-\nsitional distributional semantic models. In Proceed-\nings of LREC, pages 216–223.\nStephen Mayhew, Chen-Tse Tsai, and Dan Roth. 2017.\nCheap translation for cross-lingual named entity\nrecognition. In Proceedings of EMNLP , pages\n2536–2545.\nTomas Mikolov, Quoc V . Le, and Ilya Sutskever. 2013.\nExploiting similarities among languages for ma-\nchine translation. arXiv preprint arXiv:1309.4168.\nAndrea Moro, Alessandro Raganato, and Roberto Nav-\nigli. 2014. Entity linking meets word sense disam-\nbiguation: a uniﬁed approach. Transactions of the\nAssociation for Computational Linguistics , 2:231–\n244.\nNikola Mrkˇsi´c, Ivan Vuli´c, Diarmuid ´O S´eaghdha, Ira\nLeviant, Roi Reichart, Milica Ga ˇsi´c, Anna Korho-\nnen, and Steve Young. 2017. Semantic specializa-\ntion of distributional word vector spaces using mono-\nlingual and cross-lingual constraints. Transactions\nof the Association for Computational Linguistics ,\n5:309–324.\nRoberto Navigli and Simone Paolo Ponzetto. 2012. Ba-\nbelNet: The automatic construction, evaluation and\napplication of a wide-coverage multilingual seman-\ntic network. Artiﬁcial Intelligence, 193:217–250.\nJoakim Nivre, ˇZeljko Agi´c, Lars Ahrenberg, Lene An-\ntonsen, Maria Jesus Aranzabe, Masayuki Asahara,\nLuma Ateyah, Mohammed Attia, Aitziber Atutxa,\nand Liesbeth Augustinus. 2017. Universal Depen-\ndencies 2.1.\nMatthew Peters, Waleed Ammar, Chandra Bhagavat-\nula, and Russell Power. 2017. Semi-supervised se-\nquence tagging with bidirectional language models.\nIn Proceedings of ACL, pages 1756–1765.\nJonas Pfeiffer, Ivan Vuli´c, Iryna Gurevych, and Sebas-\ntian Ruder. 2020. MAD-X: An adapter-based frame-\nwork for multi-task cross-lingual transfer. In Pro-\nceedings of EMNLP.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Pro-\nceedings of ACL, pages 4996–5001.\nEdoardo Maria Ponti, Helen O’Horan, Yevgeni Berzak,\nIvan Vuli´c, Roi Reichart, Thierry Poibeau, Ekaterina\nShutova, and Anna Korhonen. 2019. Modeling lan-\nguage variation and universals: A survey on typo-\nlogical linguistics for natural language processing.\nComputational Linguistics, 45(3):559–601.\nYada Pruksachatkun, Jason Phang, Haokun Liu,\nPhu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe\nPang, Clara Vania, Katharina Kann, and Samuel R.\nBowman. 2020. Intermediate-task transfer learning\nwith pretrained language models: When and why\ndoes it work? In Proceedings of ACL, pages 5231–\n5247.\nAfshin Rahimi, Yuan Li, and Trevor Cohn. 2019. Mas-\nsively multilingual transfer for NER. In Proceed-\nings of ACL, pages 151–164.\nJonathan Raphael Raiman and Olivier Michel Raiman.\n2018. DeepType: Multilingual entity linking by neu-\nral type system evolution. In Proceedings of AAAI.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nEMNLP, pages 2383–2392.\nSamuel R ¨onnqvist, Jenna Kanerva, Tapio Salakoski,\nand Filip Ginter. 2019. Is multilingual BERT ﬂu-\nent in language generation? In Proceedings of the\nFirst NLPL Workshop on Deep Learning for Natural\nLanguage Processing, pages 29–36.\nSebastian Ruder, Ivan Vuli ´c, and Anders Søgaard.\n2019. A survey of cross-lingual embedding models.\nJournal of Artiﬁcial Intelligence Research , 65:569–\n631.\n4494\nMarta Sabou, Kalina Bontcheva, and Arno Scharl.\n2012. Crowdsourcing research opportunities:\nLessons from Natural Language Processing. In Pro-\nceedings of the 12th International Conference on\nKnowledge Management and Knowledge Technolo-\ngies, pages 1–8.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of ACL, pages 1715–\n1725.\nSamuel L. Smith, David H.P. Turban, Steven Hamblin,\nand Nils Y . Hammerla. 2017. Ofﬂine bilingual word\nvectors, orthogonal transformations and the inverted\nsoftmax. In Proceedings of ICLR.\nAnders Søgaard, Sebastian Ruder, and Ivan Vuli ´c.\n2018. On the limitations of unsupervised bilingual\ndictionary induction. In Proceedings of ACL, pages\n778–788.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of ACL, pages\n3645–3650.\nStephen Tratz. 2019. Dependency Tree Annotation\nwith Mechanical Turk. In Proceedings of the First\nWorkshop on Aggregating and Analysing Crowd-\nsourced Annotations for NLP, pages 1–5.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of NeurIPS, pages 5998–\n6008.\nIvan Vuli´c, Goran Glavaˇs, Roi Reichart, and Anna Ko-\nrhonen. 2019. Do we really need fully unsuper-\nvised cross-lingual embeddings? In Proceedings of\nEMNLP-IJCNLP, pages 4398–4409.\nIvan Vuli ´c, Sebastian Ruder, and Anders Søgaard.\n2020. Are all good word vector spaces isomorphic?\nIn Proceedings of EMNLP.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzm ´an, Ar-\nmand Joulin, and Edouard Grave. 2019. CCNet: Ex-\ntracting high quality monolingual datasets from Web\ncrawl data. CoRR, abs/1911.00359.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of NAACL-HLT, pages 1112–1122.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. HuggingFace’s Trans-\nformers: State-of-the-art Natural Language Process-\ning. ArXiv, abs/1910.03771.\nShijie Wu, Alexis Conneau, Haoran Li, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Emerging cross-\nlingual structure in pretrained language models. In\nProceedings of ACL.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, becas:\nThe surprising cross-lingual effectiveness of BERT.\nIn Proceedings of EMNLP-IJCNLP, pages 833–844.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation. CoRR, abs/1609.08144.\n4495\nA Reproducibility\nWe ﬁrst provide details on where to obtain datasets\nand code used in this work.\nCode and Dependencies. Our code can be\nobtained from https://www.dropbox.com/s/\no5cxyy92re48xmu/zerohero_code.zip?dl=0.\nThe code is separated in two parts: for experiments\nrelated to low-level tasks (DEP, POS, NER) the\ncode is based on the AllenNLP framework; for the\nexperiments on high-level tasks (XNLI, XQuAD),\nour code directly builds on top of the HuggingFace\nTransformers framework (Wolf et al., 2019). We\nprovide links to code dependencies and pretrained\nmodels in Table 6.\nDatasets. Table 7 provide links to all datasets\nthat we used in our study, for each of the ﬁve tasks\n(low-level tasks: DEP, POS, NER; high-level tasks:\nXNLI, XQuAD).\nB Full Per-Language Few-Shot Results\nWe show full per-language few-shot transfer re-\nsults for all ﬁve tasks (DEP, POS, NER, XNLI,\nXSQuAD) for mBERT and XLM-R in Tables 8\nand 9, respectively. We visually illustrate the gains\nfrom few-shot transfer for individual languages,\nfor mBERT (for the POS task not covered in the\nmain paper) in Figure 3 and for XLM-R (for all ﬁve\ntasks) in Figure 4. Finally, we show how the few-\nshot transfer results with XLM-R for lower-level\ntasks (DEP, POS, NER) depend on the instance\nsampling strategy (RAND , SHORTEST , LONGEST )\nin Figure 5.\nC Few-Shot Transfer: Annotation Costs\nversus Performance Gains\nWe now present the more detailed explanations for\nthe conversion between the annotation costs and\nfew-shot transfer performance gains, summarized\nin Table 5 in the main paper.\nNatural Language Inference. Marelli et al.\n(2014) reportedly paid $ 2,030 for 200k judge-\nments, which would amount to $0.01015 per NLI\ninstance and, in turn, to $10.15 for 1,000 annota-\ntions. In our few-shot experiments this would yield\nan average improvement of 2.24 and 2.52 accuracy\npoints for mBERT and XLM-R, respectively. It is\nalso possible to translate the English data directly\nvia professional translation services as done with\nthe XNLI dataset and XQuAD: the platforms for\nhiring professionals such as Upwork show that it\nis possible to ﬁnd qualiﬁed translators even for\nlower-resource languages: e.g., the translation cost\nestimate for Zulu is $12.5-$16/h, or $19/h for the\nBasque language.\nQuestion Answering. Rajpurkar et al. (2016) re-\nport a payment cost of $9 per hour and a time effort\nof 4 minutes per paragraph. With an average of 5\nparagraphs per article, our few-shot scenario ( 10\narticles) roughly requires 50 paragraphs-level an-\nnotations, i.e., 200 minutes of annotation effort and\nwould in total cost around $30 (for respective per-\nformance improvements of 4.6 and 2.1 points for\nmBERT and XLM-R).\nOn the one hand, compared to language under-\nstanding tasks, our lower-level (DEP, POS) tasks\nare presumably more expensive to annotate, as they\nrequire some linguistic knowledge and annotation\ntraining. On the other hand, as shown in our few-\nshot experiments, we typically need much fewer\nannotated instances (i.e., we observe high gains\nwith already 10 target language sentences) for sub-\nstantial gains in these tasks.\nDependency Parsing. Tratz (2019) provide an\noverview of crowd-sourcing annotations for depen-\ndency parsing; they report obtaining a fully correct\ndependency tree from at least one annotator for\n72% of sentences. At the reported cost of $0.28\nper sentence this amounts to spending $280 for an-\nnotating 1,000 sentences. Somewhat shockingly,\nannotating 10 sentences with dependency trees –\nwhich for particular target languages like AR and\nJA corresponds to performance gains of 30-40 UAS\npoints (see Figure 2) – amounts to spending merely\n$3-5.\nPart-of-Speech Tagging. Hovy et al. (2014) mea-\nsure agreement of crowdsourced POS annotations\nwith expert annotations; they crowdsource annota-\ntions for 1,000 tweets, at a cost of $0.05 for every\n10 tokens. With a total of 14,619 tokens in the cor-\npus, this amounts to approximately $73 for 1,000\ntweets, which is ≥1,000 sentences.14 Based on\nTable 4, 2 hours of POS annotation work trans-\nlates to gains of up to 20-22 points on average over\nzero-shot transfer methods.\n14Note, however, that lower-level tasks do come with an\nadditional risk of poorer quality annotation, due to crowd-\nsourced annotators not being experts. Garrette and Baldridge\n(2013) report that even for truly low-resource languages (e.g.,\nKinyarwanda, Malagasy), it is possible to obtain ≈ 100 POS-\nannotated sentences in 2 hours.\n4496\nCodebase MMT Vocab Params URL\nAllen NLP – – – https://github.com/allenai/allennlp\nHF Trans. – – – https://github.com/huggingface/transformers\nmBERT 119K 125M https://huggingface.co/bert-base-multilingual-cased\nXLM-R 250K 125M https://huggingface.co/xlm-roberta-base\nTable 6: Links to codebases and pretrained models used in this work. For low-level tasks (DEP, POS, NER), we\ncarried out our experiments using the AllenNLP library. For high-level tasks (XNLI, XQuAD), we built our models\ndirectly on top of the HuggingFace (HF) Transformers library.\nTask Dataset URL\nDependency Parsing UD https://lindat.mff.cuni.cz/repository/xmlui/handle/\n11234/1-3105\nPOS Tagging UPOS https://lindat.mff.cuni.cz/repository/xmlui/handle/\n11234/1-3105\nNamed Entity Recognition WikiAnn https://elisa-ie.github.io/wikiann/\nNatural Language Inference XNLI https://github.com/facebookresearch/XNLI\nQuestion Answering XQuAD https://github.com/deepmind/xquad\nTable 7: Links to the datasets used in our work.\n(a) POS\nFigure 3: Graphical illustration of few-shot transfer gains for each language with mBERT, for the remaining task\nnot covered in the main paper: POS.\nNamed Entity Recognition. Bontcheva et al.\n(2017) provide estimates for crowdsourcing anno-\ntation for named entity recognition; they pay $0.06\nper sentence, resulting in $60 cost for 1,000 anno-\ntated sentences. At a median pay of $11.37/hr, this\namounts to around 190 sentences annotated in an\nhour. In other words, in less than 3 hours, we can\ncollect more than 500 annotated examples. Accord-\ning to Table 4, this can result in gains of 8+ points\non average, and even more for some languages\n(e.g., 27 points for AR).\n4497\nPOS ar eu zh ﬁ he hi it ja ko ru sv tr\n0 55.65 61.19 57.8 73.85 62.38 61.7 76.02 49.65 53.75 79.79 86.15 59.9\n10 83.16 74.65 76.1 75.5 83.18 75.19 87.56 82.04 71.02 82.95 87.28 67.73\n50 89.18 79.84 83.84 81.4 88.91 83.12 92.04 88.27 77.17 86.07 89.5 74.2\n100 90.73 81.63 85.82 82.28 90.12 85.46 93.47 90.95 80.57 87.5 91.06 76.66\n500 94.08 86.84 90.78 86.8 94.75 89.69 95.73 94.25 86.48 91.21 93.43 85.29\n1000 94.97 88.23 92.83 88.86 95.7 93.09 96.15 95.24 88.64 92.77 94.39 87.72\nNER ar eu zh ﬁ he hi it ja ko ru sv tr\n0 60.69 79.53 69.01 88.59 81.26 78.46 89.77 66.64 78.51 81.64 85.62 80.78\n10 81.69 90.51 82.27 91.28 83.12 81.44 92.14 75.64 79.36 83.39 92.09 86.91\n50 86.3 93.36 85.6 92.38 87.02 85.04 92.34 78.88 86.94 88.07 95.51 91.93\n100 87.37 94.84 87.19 92.88 87.8 86.52 92.79 81.98 88 89.98 95.53 92.5\n500 89.74 95.28 89.5 94.01 89.86 89.27 93.8 84.6 90.93 92.18 96.84 94.34\n1000 90.92 96.01 90.71 94.57 90.8 90.67 94.5 85.62 91.96 92.71 97.17 94.65\nDEP ar eu zh ﬁ he hi it ja ko ru sv tr\n0 34.72 40.96 47.25 60.44 55.1 33.59 74.05 31.03 35.11 63.03 76.9 45.17\n10 69.08 56.16 54.18 63.3 70.02 56.49 82.26 71.12 53.25 69.89 76.88 53.26\n50 73.65 61.11 64.39 65.88 78.78 71.48 84.46 82.58 61.11 73.95 79.37 56.78\n100 75.91 62.98 68.17 67.31 79.71 76.1 86.53 85.77 64.51 76.51 80.13 57.66\n500 81.48 70.33 78.64 71.4 84.81 85.34 89.39 90.38 73.65 81.19 82.87 65.16\n1000 83.31 73.85 81.59 74.97 87.47 89.49 89.9 92.18 76.08 83.18 83.95 68.26\nXNLI fr es el bg ru tr ar vi th zh hi sw ur de\n0 75.05 74.71 68.68 69.50 69.34 62.18 65.53 70.88 54.69 69.26 61.50 49.84 59.38 72.34\n10 75.09 73.62 67.04 69.35 69.80 61.86 65.56 69.26 55.30 70.89 61.92 51.79 59.28 71.63\n50 74.60 73.91 66.44 68.37 69.05 60.99 64.63 70.29 51.17 71.32 60.08 49.95 58.83 71.43\n100 73.85 73.50 65.67 68.47 70.24 60.13 64.93 69.59 51.68 71.46 60.01 48.96 58.78 71.60\n500 75.36 74.97 68.04 71.03 70.59 63.21 66.71 72.38 58.12 72.81 64.06 52.26 61.15 73.09\n1000 76.20 76.24 68.73 71.73 71.41 65.01 67.04 72.35 59.19 73.47 64.75 52.47 62.38 73.21\nXQ U AD zh vi tr th ru hi es el de ar\n0 48.14 49.02 36.90 27.84 51.86 42.47 54.48 42.90 56.22 46.40\n2 48.93 50.50 40.87 39.43 51.07 44.19 56.14 46.46 56.66 46.99\n4 49.72 51.38 40.22 41.24 51.33 45.90 56.62 47.25 56.38 46.57\n6 50.81 50.81 41.59 44.04 51.20 46.81 57.14 47.16 56.40 47.45\n8 51.53 51.29 41.99 45.28 51.29 47.10 57.45 47.95 57.07 48.21\n10 50.87 51.57 42.55 46.05 52.05 48.06 57.03 48.60 57.29 47.82\nTable 8: Detailed per-language few-shot language results with mBERT for different number of target-language\ndata instances k. For low-level tasks, we report results with RAND sampling.\n4498\nPOS ar eu zh ﬁ he hi it ja ko ru sv tr\n0 59.23 64.41 27.06 78.34 68.94 65.63 77.25 19.28 58.98 81.96 85.54 68.61\n10 82.72 76.54 68.3 81.04 84.81 77.08 88.44 78.92 70.5 83.95 87.87 72.33\n50 89.14 80.19 77.49 84.94 89.13 84.07 92.51 86.94 76.09 87.29 90.8 79.19\n100 90.67 83.38 80.83 86.44 90.3 87.23 93.52 88.78 78.91 88.84 91.79 81.65\n500 94.36 88.4 86.61 90.23 94.23 91.4 95.7 92.11 84.37 91.87 94.35 87.64\n1000 95.29 89.66 88.86 91.87 95.31 94.26 96.18 93.49 86.88 93.19 95.41 89.71\nNER ar eu zh ﬁ he hi it ja ko ru sv tr\n0 67.03 83.58 56.77 90.69 75.05 78.28 89.25 61.46 76 77.87 89.36 85.43\n10 75.45 89.81 79.02 91.14 75.1 78.5 90.02 76.45 74.8 84.5 92.01 88.06\n50 82.56 91.63 80.81 92.01 80.34 81.23 91.01 78.13 81.8 87.21 94.72 91.07\n100 83.37 93.33 82.77 92.77 82.63 83.88 91.23 79.97 83.06 88.01 94.89 91.49\n500 86.95 94.82 85.77 93.78 86.09 87.79 92.44 82.38 87.17 91.02 96.33 93.69\n1000 88.36 95.24 87.34 94.3 87.4 89.87 93.25 83.45 88.52 91.66 96.78 93.82\nDEP ar eu zh ﬁ he hi it ja ko ru sv tr\n0 37.46 42.48 6.61 65.33 53.06 32.94 68.54 11.48 36 62.37 75.72 47.83\n10 68.37 56.09 45.67 66.97 70.06 51.93 79.32 70.05 49.88 70.14 77.03 54.93\n50 74.9 60.92 57.39 71.35 77.95 67.09 83.97 81.64 59.22 73.55 78.72 59.77\n100 77.15 63.46 60.33 71.65 78.27 73.2 84.63 84.3 61.37 75.03 81.52 60.06\n500 83.29 72.37 71.52 77.22 86.21 87.06 88.82 88.83 73.1 80.41 85.38 68.88\n1000 84.99 75.25 76.2 80.46 88.48 90.81 90.14 90.28 75.35 82.88 85.68 70.68\nXNLI fr es el bg ru tr ar vi th zh hi sw ur de\n0 84.25 78.16 78.44 75.39 77.68 75.25 72.99 71.28 74.59 72 73.21 70.02 64.03 66.93 76.45\n10 84.26 77.96 78.67 75.77 78.11 76.32 73.31 71.75 75.17 73.18 74.53 69.23 64.09 68.32 77.32\n50 84.39 78.69 79.81 76.13 77.57 76.16 73.96 71.2 75.01 71.74 74.47 69.84 61.98 68.06 77.6\n100 83.64 79.37 78.87 76.28 77.58 77.42 73.31 71.4 74.83 71.94 74.1 70.54 61.55 67.63 77.84\n200 81.57 79.29 79.84 77.01 78.94 77.54 74.81 73.22 76.52 73.91 76.37 71.54 64 68.98 78.42\n500 82.69 79.65 79.95 77.34 79.09 77.78 74.08 73.6 77.22 74.32 77.03 71.75 65.37 68.85 78.71\n1000 83.74 79.91 80.29 77.39 79.39 77.8 74.92 74.26 77.34 74.8 77.26 72.83 66.77 69.84 78.91\nXQ U AD zh vi tr th ru hi es el de ar\n0 46.29 52.84 53.82 57.64 57.10 49.67 57.97 56.77 56.33 48.36\n2 47.16 52.86 52.84 60.96 55.39 50.20 57.51 55.37 57.05 47.97\n4 48.06 53.43 51.88 61.57 54.21 50.28 57.62 55.68 56.72 49.00\n6 52.29 53.41 53.03 62.97 55.48 50.85 57.88 55.37 57.16 49.10\n8 57.88 53.49 52.47 63.73 55.87 50.96 58.25 55.83 57.05 50.09\n10 60.22 53.28 52.36 64.02 55.79 51.38 57.90 56.11 57.47 49.30\nTable 9: Detailed per-language few-shot language results with XLM-R for different number of target-language\ndata instances k. For low-level tasks, we report results with RAND sampling.\n4499\n(a) DEP\n(b) XQUAD\n (c) XNLI\n(d) POS\n (e) NER\nFigure 4: Graphical illustration of few-shot transfer gains for individual languages, for XLM-R and all languages.\nFigure 5: Heatmap of performance gains for low-level tasks from few-shot transfer with XLM-R for different\nsampling strategies. X-axis: number of target-language instances k; Y-axis: sampling strategy.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8654625415802002
    },
    {
      "name": "Transformer",
      "score": 0.6763360500335693
    },
    {
      "name": "Natural language processing",
      "score": 0.6053830981254578
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5866093635559082
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.5743737816810608
    },
    {
      "name": "Transfer (computing)",
      "score": 0.5708103775978088
    },
    {
      "name": "Limiting",
      "score": 0.5374076962471008
    },
    {
      "name": "Transfer of learning",
      "score": 0.5171715617179871
    },
    {
      "name": "Parsing",
      "score": 0.5040354132652283
    },
    {
      "name": "Language model",
      "score": 0.4648204445838928
    },
    {
      "name": "Dependency grammar",
      "score": 0.4644012749195099
    },
    {
      "name": "Linguistics",
      "score": 0.2173101007938385
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Parallel computing",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 228
}