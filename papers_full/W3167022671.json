{
    "title": "MathBERT: A Pre-trained Language Model for General NLP Tasks in Mathematics Education",
    "url": "https://openalex.org/W3167022671",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5032524821",
            "name": "Jia Shen",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5006093397",
            "name": "Michiharu Yamashita",
            "affiliations": [
                "Pennsylvania State University"
            ]
        },
        {
            "id": "https://openalex.org/A5047556915",
            "name": "Ethan Prihar",
            "affiliations": [
                "Worcester Polytechnic Institute"
            ]
        },
        {
            "id": "https://openalex.org/A5078998782",
            "name": "Neil T. Heffernan",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5008463509",
            "name": "Xintao Wu",
            "affiliations": [
                "University of Arkansas at Fayetteville"
            ]
        },
        {
            "id": "https://openalex.org/A5043094131",
            "name": "Ben Graff",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5100405086",
            "name": "Dongwon Lee",
            "affiliations": [
                "Pennsylvania State University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1995864528",
        "https://openalex.org/W2966171606",
        "https://openalex.org/W2559094423",
        "https://openalex.org/W2605898966",
        "https://openalex.org/W2015040676",
        "https://openalex.org/W3158196282",
        "https://openalex.org/W2955213152",
        "https://openalex.org/W2726132969",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3167364369",
        "https://openalex.org/W3035101152",
        "https://openalex.org/W2925863688",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2283295727",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2955931418",
        "https://openalex.org/W1518951372",
        "https://openalex.org/W2108612889",
        "https://openalex.org/W3093681740",
        "https://openalex.org/W2937845937",
        "https://openalex.org/W2950809786",
        "https://openalex.org/W2945824677",
        "https://openalex.org/W2951174816",
        "https://openalex.org/W3211193064",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W2045871438",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W2950577311",
        "https://openalex.org/W2142854008",
        "https://openalex.org/W3083494020",
        "https://openalex.org/W3034238904"
    ],
    "abstract": "Since the introduction of the original BERT (i.e., BASE BERT), researchers have developed various customized BERT models with improved performance for specific domains and tasks by exploiting the benefits of transfer learning. Due to the nature of mathematical texts, which often use domain specific vocabulary along with equations and math symbols, we posit that the development of a new BERT model for mathematics would be useful for many mathematical downstream tasks. In this resource paper, we introduce our multi-institutional effort (i.e., two learning platforms and three academic institutions in the US) toward this need: MathBERT, a model created by pre-training the BASE BERT model on a large mathematical corpus ranging from pre-kindergarten (pre-k), to high-school, to college graduate level mathematical content. In addition, we select three general NLP tasks that are often used in mathematics education: prediction of knowledge component, auto-grading open-ended Q&amp;A, and knowledge tracing, to demonstrate the superiority of MathBERT over BASE BERT. Our experiments show that MathBERT outperforms prior best methods by 1.2-22% and BASE BERT by 2-8% on these tasks. In addition, we build a mathematics specific vocabulary 'mathVocab' to train with MathBERT. We discover that MathBERT pre-trained with 'mathVocab' outperforms MathBERT trained with the BASE BERT vocabulary (i.e., 'origVocab'). MathBERT is currently being adopted at the participated leaning platforms: Stride, Inc, a commercial educational resource provider, and ASSISTments.org, a free online educational platform. We release MathBERT for public usage at: https://github.com/tbs17/MathBERT.",
    "full_text": "MathBERT: A Pre-trained Language Model for General NLP Tasks\nin Mathematics Education\nJia Tracy Shen\nPenn State University\nUSA\njia.t.shen@gmail.com\nMichiharu Yamashita\nPenn State University\nUSA\nmichiharu@psu.edu\nEthan Prihar\nWorcester Polytechnic Institute\nUSA\nebprihar@wpi.edu\nNeil Heffernan\nASSISTments.org\nUSA\nneil@ASSISTments.org\nXintao Wu\nUniversity of Arkansas\nUSA\nxintaowu@uark.edu\nBen Graff\nStride, Inc\nUSA\nbgraff@k12.com\nDongwon Lee\nPenn State University\nUSA\ndongwon@psu.edu\nABSTRACT\nSince the introduction of the original BERT (i.e., BASE BERT), re-\nsearchers have developed various customized BERT models with\nimproved performance for specific domains and tasks by exploiting\nthe benefits of transfer learning . Due to the nature of mathemati-\ncal texts, which often use domain specific vocabulary along with\nequations and math symbols, we posit that the development of\na new BERT model for mathematics would be useful for many\nmathematical downstream tasks. In this resource paper, we intro-\nduce our multi-institutional effort (i.e., two learning platforms and\nthree academic institutions in the US) toward this need: Math-\nBERT, a model created by pre-training the BASE BERT model\non a large mathematical corpus ranging from pre-kindergarten\n(pre-k), to high-school, to college graduate level mathematical con-\ntent. In addition, we select three general NLP tasks that are often\nused in mathematics education: prediction of knowledge compo-\nnent, auto-grading open-ended Q&A, and knowledge tracing, to\ndemonstrate the superiority of MathBERT over BASE BERT. Our\nexperiments show that MathBERT outperforms prior best methods\nby 1.2-22% and BASE BERT by 2-8% on these tasks. In addition,\nwe build a mathematics specific vocabulary â€˜mathVocabâ€™ to train\nwith MathBERT. We discover that MathBERT pre-trained with\nâ€˜mathVocabâ€™ outperforms MathBERT trained with the BASE BERT\nvocabulary (i.e., â€˜origVocabâ€™).MathBERT is currently being adopted\nat the participated leaning platforms: Stride, Inc, a commercial ed-\nucational resource provider, and ASSISTments.org, a free online\neducational platform. We release MathBERT for public usage at:\nhttps://github.com/tbs17/MathBERT.\nCCS CONCEPTS\nâ€¢ Applied computing â†’Education; â€¢ Computing methodolo-\ngies â†’Natural language processing .\nKEYWORDS\nBERT, Language Model, Mathematics Education, Text Classification\n1 INTRODUCTION\nThe arrival of transformer-based language model, BERT [ 5], has\nrevolutionized the NLP research and applications. One strength\nof BERT is its ability to adapt to new domain and/or task through\npre-training by means of so-called â€œtransfer learning. \" By taking\nan advantage of this benefit, therefore, researchers have adapted\nBERT into diverse domains (e.g., FinBERT [17], ClinicalBERT [11],\nBioBERT [13], SCIBERT [2], E-BERT [30], LiBERT [7]) and tasks\n(e.g., [27], [26], [3], [16], [8]) with improved performances.\nIn the domain of mathematics, as mathematical text often use\ndomain or context specific words, together with math equations\nand symbols, we posit that mathematics-customized BERT would\nhelp researchers and practitioners sort out the meaning of ambigu-\nous language better by using surrounding text to establish â€œmath\"\ncontext. Further, such an improved context-aware understanding of\nlanguage could help develop and improve solutions for challenging\nNLP tasks in mathematics.\nIn mathematics education, for instance, there are several gen-\neral tasks that currently cause researchers/educators headaches:\n(i) large-scale knowledge component (KC, a.k.a. skill) prediction\n(denoted asğ‘‡ğ‘˜ğ‘ ), (ii) open-ended question answer scoring (i.e., auto-\ngrading) (denoted as ğ‘‡ğ‘ğ‘”), and (iii) knowledge tracing (KT) correct-\nness prediction (denoted as ğ‘‡ğ‘˜ğ‘¡ ). For instance, the struggle with\nğ‘‡ğ‘˜ğ‘ (e.g., predicting the right mathematical skill for a given text de-\nscription) is partly attributed to its tediousness and labor-intensive\nwork for teachers/tutors to label all knowledge components in texts\nwhere they need to organize mathematical problems, or descrip-\ntions of instructional videos, etc. The traditional way to address\nthis challenge of ğ‘‡ğ‘˜ğ‘ is to use machine learning to classify them via\nfeature extraction [12, 19, 20], which has produced decent results.\nHowever, open-ended essay or mathematical problem questions\nare becoming less popular in studentsâ€™ assignments due to the diffi-\nculty of developing universal automated support in assessing the\nresponse quality, causing educators to favor multiple choice ques-\ntions when evaluating their students. According to Erikson et al.\n[6], from 2010 to 2020, less than 15% of the assigned open response\narXiv:2106.07340v5  [cs.CL]  12 Aug 2023\nproblems in ASSISTments [9] were ever graded by teachers. How-\never, in general, open-ended questions are known to be able to\nprovide critical evaluation in testing studentsâ€™ true critical thinking\nand understanding. Therefore, it is still important to develop an\neffective solution toward ğ‘‡ğ‘˜ğ‘ .\nSimilarly, Knowledge Tracing , a very important task in the ed-\nucation domain, is defined as the task of tracing studentsâ€™ knowl-\nedge state, which represents their mastery of educational con-\ntent based on their past learning activities. Predicting studentsâ€™\nnext question correctness as a KT task is, for instance, well stud-\nied [ 4, 14, 15, 18, 28] but these solutions tend to rely on high-\ndimensional sequential data. The current solutions are still not\nable to capture the complex nature of studentsâ€™ learning activities\nover extended periods of time.\nAddressing this lack of general BERT-based language model\nin mathematics education, therefore, in this work, we introduce\nour effort across two learning platforms (i.e., ASSISTments and\nK12.com) and three academic institutions (i.e., Penn State, WPI,\nand U. Arkansas) in the US: MathBERT, a model created by pre-\ntraining the BASE BERT model on a large mathematical corpus\nranging from pre-kindergarten (pre-k), to high-school, to college\ngraduate level mathematical content. In light of the recent successes\nfrom transfer learning models such as ELMo [22], ULMFiT [10] and\nBERT [5], we propose to use a BERT-like model to improve the\nsolutions of the aforementioned three tasks in one shot, as BERT\nhas been proven to have outstanding performance in various NLP\ntasks.\nHowever, directly applying BERT to mathematical tasks has lim-\nitations. First, the original BERT (i.e., BASE BERT) was trained\nmainly on general domain texts (e.g., general news articles and\nWikipedia pages). As such, it is difficult to estimate the perfor-\nmance of a model trained on these texts on tasks using datasets\nthat contain mathematical text. Second, the word distributions of\ngeneral corpora is quite different from mathematical corpora (e.g.,\nmathematical equations and symbols), which can often be a problem\nfor mathematical task related models.\nTherefore, we hypothesize that a special BERT model needs\nto be trained on mathematical domain corpora to be effective in\nmathematics-related tasks. That is, we further pre-train the BASE\nBERT on mathematical corpora to build MathBERT. Then, we use\nthe pre-trained weights from MathBERT to fine-tune on the mathe-\nmatical task-specific text dataset for classification.\nWe make the following contributions in this work:\n(1) We build MathBERT by pre-training the BASE BERT on\nmathematical domain texts ranging from pre-k to high-school\nto graduate level mathematical curriculum, books and paper\nabstracts. We publicly release MathBERT as a community\nresource at:\nâ€¢https://github.com/tbs17/MathBERT for codes on how\nto further-train and fine-tune, and\nâ€¢https://huggingface.co/tbs17/MathBERT for PyTorch\nversion MathBERT and tokenizer.\nâ€¢AWS S3 URLs 1 for Tensorflow version MathBERT and\ntokenizer.\n1http://tracy-nlp-models.s3.amazonaws.com/mathbert-basevocab-uncased/\nhttp://tracy-nlp-models.s3.amazonaws.com/mathbert-mathvocab-uncased/\nTable 1: Corpora Comparison for DAPT BERT Models\nDomain Name # Tokens Corpora\nGeneral NLP Original BERT 3.3B News article,\nWikepedia\nBio Medicine BioBERT 18B PubMed,\nPMC articles\nClinical Medicine ClinicalBERT 2M (notes) Hospital\nClinical Notes\nScience SciBERT 3.2B Semantic\nScholar Papers\nJob LiBERT 685M LinkedIn search query\nprofile, job posts\nE-commerce E-BERT 233M (reviews) Amazon Dataset 2\nFinance FinBERT 12.7B Reuters\nNews stories\nMathematics MathBERT 100M Math curriculum and books,\n(This Work) Math arXiv paper abstract\n(2) We build and release a custom vocabulary mathVocab to re-\nflect the different nature of mathematical corpora (e.g., math-\nematical equations and symbols). We compare the perfor-\nmance of MathBERT pre-trained with mathVocab to Math-\nBERT pre-trained with the original BASE BERT vocabulary.\n(3) We evaluate the performance ofMathBERT for three general\nNLP tasks, ğ‘‡ğ‘˜ğ‘ , ğ‘‡ğ‘ğ‘” and ğ‘‡ğ‘˜ğ‘¡ , and compare its performance to\nfive baseline models. Our experiments show that solutions of\nthree tasks using MathBERT outperforms those using BASE\nBERT by 2-8%.\n(4) We sketch the use cases ofMathBERT currently being adopted\nat two major learning management systems: ASSISTments\nand K12.com by Stride.\n2 RELATED WORK\nThe state-of-the-art language model BERT (Bidirectional Encoder\nRepresentations From Transformer) [5] is a pre-trained language\nrepresentation model that was trained on 16 GB of unlabeled texts,\nincluding Books Corpus and Wikipedia, with a total of 3.3 billion\nwords and a vocabulary size of 30,522. Its advantage over other\npre-trained language models such as ELMo [22] and ULMFiT [10]\nis its bidirectional structure by using the masked language model\n(MLM) pre-training objective[5]. The MLM randomly masks 15% of\nthe tokens from the input to predict the original vocabulary id of\nthe masked word based on its context from both directions [5]. The\npre-trained model can be used directly to fine-tune on new data for\nNLP understanding and inference tasks or further pre-trained to\nget a new set of weights for transfer learning.\nThe further pre-training process has become popular in the past\ntwo years as it is able to achieve better results than the fine-tuning\nonly strategy. According to Gururangan et al. [ 8], there are two\nstyles of further pre-training on the BASE BERT [5]: (i) further pre-\ntrain the BASE BERT on a task-specific data set with tasks being\ntext classification, question and answering inference, paraphrasing,\netc. Gururangan et al. [8] call this kind of model a Task-adaptive\nPre-trained (TAPT) Model. (ii) further pre-train the BASE BERT on\na domain-specific data set with domains being finance, bio-science,\nclinical fields, etc. Gururangan et al. [ 8] call this kind of model\na Domain-adaptive Pre-trained ( DAPT) Model. Both TAPT and\nDAPT BERT models start the further pre-training process from the\n2\nFigure 1: An illustration of training and fine-tuning process of BASE vs. TAPT vs. DAPT BERT models. The pre-training data\nare from this study. KC, Auto-grading, and KT Texts are task data for ğ‘‡ğ‘˜ğ‘ , ğ‘‡ğ‘ğ‘”, and ğ‘‡ğ‘˜ğ‘ respectively.\nTable 2: Corpora Comparison for TAPT BERT Models. * indi-\ncates that the number is an estimation based on 150 token-\ns/sentence\nDomain Dataset # Tokens Task\nBioMed ChemProt [8] 1.5M* relation classification\nRCT [8] 12M* abstract sent. roles\nComp. Sci. ACL-ARC [8] 291,150* citation intent\nSCIERC [8] 697,200* relation classification\nNews HyperPartisan [8] 96,750* partisanship\nAgNews [8, 27] 5.6M topic\nReviews Yelp [27] 25M review sentiment\nIMDB [8, 27] 14.6M review sentiment\nLinguistics VUA-20 [3] 205,425 metophor detection\nVUA-Verb [3] 5,873 metophor detection\nMathematics KC [26] 589,549 skill code detection\nBASE BERT weights but pre-train on different types of corpora.\nTAPT BERT models pre-train on task-specific data, whereas DAPT\nBERT models pre-train on the domain-specific data before they\nare fine-tuned for use in any downstream tasks (see the process\nillustrated in Fig. 1).\nThe domain specific corpora that DAPT BERT models train on\nare usually huge (e.g. billions of news articles, clinical texts or PMC\nfull-text and abstracts), which help DAPT BERT models achieve\nstate-of-art (SOTA) performance in the corresponding domains. For\nexample, FinBERT [17], ClinicalBERT [11], BioBERT [13], SCIB-\nERT [2]. Other DAPT models such as E-BERT [ 30] and LiBERT\n[7] not only further pre-trained on the domain specific corpora\nbut also modified the transformer architecture to achieve better\nperformance for the domain related tasks. A comparison between\ndifferent domain-specific BERT modelsâ€™ corpora is shown in Table\n1. From the table, we can see that BioBERT was pre-trained on the\nlargest set of tokens (18B) whereas our MathBERT is pre-trained\non the smallest set of tokens (100M). Although the scale of training\ndata is much smaller than the BASE BERT, MathBERT is still more\neffective in evaluating mathematics related tasks.\nThere are also a few works that focus on TAPT models. Sun et\nal. [27] proposed a detailed process on how to further pre-train a\nTAPT BERT model and fine-tune it for three types of classification\ntasks (i.e., sentiment, question, and topic), achieving a new record\naccuracy. Shen et al. [26] pre-trained a TAPT BERT model to predict\nknowledge components and surpassed the BASE BERT accuracy by\nabout 2%. MelBERT [3] further pre-trained the RoBERTa-base BERT\non well-known public English data sets (e.g.,VUA-20, VUA-Verb)\nthat have been released in metaphor detection tasks and obtained\n[0.6%, 3%] out-performance over the RoBERTa-base [16]. Gururan-\ngan et al.[8] pre-trained RoBERTa-base [16] on famous task data\nsets (e.g., Chemprot, RCT, ACL-ARC, SCIERC, Hyperpartisan, Ag-\nNews, and IMDB tasks) and obtained [0.5%, 4%] better performance\nthan RoBERTa-base. Table 2 presents the training data size for the\naforementioned TAPT Models, showcasing that TAPT models have\nmuch smaller training data size than the DAPT BERT models. In\ngeneral, DAPT models usually achieve better performance (1-8%\nhigher) than TAPT models [8]. Although DAPT BERT models re-\nquire more time and resource to train, they have wider applications\nthan TAPT BERT models because they do not need to retrain in the\ncase of different tasks, where TAPT BERT models tend to.\nIn light of the aforementioned success, we also build a DAPT\nmodel, MathBERT, that is further pre-trained from the BASE BERT\nmodel with a dedicated mathematical corpus. With the similar goal\nto our MathBERT, we note that the work by [21] was also indepen-\ndently announced about the same time (i.e., [21] was submitted to\narXiv while our MathBERT was released to GitHub and Hugging\nFace, both in May 2021). [21] also built a pre-trained BERT from the\nmathematical formula data and applied it on three formula-related\ntasks (i.e., math info retrieval, formula topic classification, formula\nheadline generation). However, as they claimed, their BERT is the\nfirst pre-trained model for mathematical formula understanding\nand was only trained on 8.7 million tokens of formula latex data\nwith the 400 surrounding characters from arXiv papers (graduate-\nlevel). Our MathBERT is pre-trained on 100 million tokens of more\ngeneral purpose mathematical corpora including curriculum, books,\nand arXiv paper abstracts, covering all the grade bands from pre-\nk to college graduate-level. Our training data not only include\n3\nTable 3: Math Corpus Details. Note all the corpus is in math-\nematics domain\nSource Math Corpora Tokens\narxiv.org Paper abstract 64M\nclasscentral.com College MOOC syllabus 111K\nopenculture.com pre-k to College Textbook 11M\nengageny.org Pre-k to HS Curriculum 18M\nillustrativemathematics.org K-12 Curriculum 4M\nutahmiddleschoolmath.org G6-8 Curriculum 2M\nck12.org K-12 Curriculum 910K\nformulas and their contexts but also more general mathematical\ninstructional texts from books, curriculum, MOOC courses, etc. We\nconsider our work has a potential to be widely used for â€œgeneral\"\nmathematics-related tasks. For instance, MathBERT in Hugging\nFace has been downloaded more than 150 times since May 2021. As\n[21] has not released their code and model artifacts, we could not\ncompare our results directly to theirs. We welcome further compar-\nison and analysis by releasing all our code and model artifacts at\nhttps://github.com/tbs17/MathBERT.\n3 BUILDING MATHBERT\n3.1 Math Corpora\nMathBERT is pre-trained on mathematics related corpora that com-\nprise mathematics curricula from pre-k to high school, mathematics\ntextbooks written for high school and college students, mathemat-\nics course syllabi from Massive Online Open Courses (MOOC) as\nwell as mathematics paper abstracts (see in Table 3). We crawl these\ndata from popular mathematics curriculum websites (illustrative-\nmathematics.org, utahmiddleschoolmath.org, engageny.org), a free\ntext book website (openculture.com), a MOOC platform (classcen-\ntral.com), and arXiv.org, with a total data size of around 3GB and\n100 Million tokens. The mathematics corpora not only contain text\nbut also mathematics symbols and equations. Among all these data,\nthe text book data is in PDF format and we hence converted them\ninto text format using the Python package pdfminer3, which pre-\nserves the mathematics symbols and equations (see sample text in\nFig. 2).\n3.2 Training Details and Outcome\nTo pre-train MathBERT efficiently, we adopt a similar data pro-\ncessing strategy to the ROBERTa model, which threaded all the\nsentences together and split them into a maximum length of 512-\ntoken sequence sections [16]. In other words, one sequence of data\nis longer than the original single sentence from the mathematics cor-\npora. Inspired by SciBERT [2], we create a custom mathematical vo-\ncabulary (mathVocab) using Hugging FaceBertWordPieceTokenizer4\nwith a size of 30,522 from the BASE BERT. We select 50 words from\nthe same rank tier of #2100 to #2150 and discover that mathVocab\nhas more mathematical jargon than the original vocabulary (origVocab)\nfrom BERT [5] (see in Table 4).\n3https://pypi.org/project/pdfminer/\n4https://huggingface.co/docs/tokenizers/python/latest/quicktour.html\nTable 4: Vocabulary Comparison: origVocab vs. mathVocab.\nTokens in blue are mathematics domain specific.\nVocab Type 50 Selected Tokens (from #2100-#2150)\norigVocab\n##y, later, ##t, city, under, around, did,\nsuch, being, used, state, people, part,\nknow, against, your, many, second, university,\nboth, national,##er, these, don, known, off,\nway, until, re, how, even, get,\nhead, ..., didn, ##ly, team, american,\nbecause, de, ##l, born, united,\nfilm, since, still, long, work, south, us\nmathVocab\ncod, exist, ##olds, coun, ##lud, ##ments,\nsqu, ##ings, known, ele, ##ks, fe,\nminutes, continu, ##line, addi, small, ##ology,\ntriang, ##velop, ##etry, log, converg,\nasym, ##ero, norm, ##abl, ##ern,\nevery, ##otic, ##istic, cir, ##gy,\npositive, hyper, dep, ##raw, ##ange, analy,\nequival, ##ynam, call, mon, numerical,\nfam, conject, large, ques, ##sible, surf\nWe use 8-core TPU machine from Google Colab Pro to pre-train\nthe BASE BERT on the mathematics corpora. The largest batch size\n(bs) we can fit into the TPU memory is 128 and the best training\nlearning rate (lr) is 5ğ‘’âˆ’5 with maximum sequence length (max-\nseq) of 512 for both MathBERT with origVocab and mathVocab.\nWe measure the effectiveness of training via Mask Language Mod-\neling (MLM) accuracy (ACC), where the model predicts the vo-\ncabulary ID of the masked words in a sentence [ 5]. For training\nsteps, we find both versions of MathBERT reach their best result\nat 600K with MLM accuracy of above 99.8% after a training time\nof 5 days each. We release MathBERT model artifacts trained with\norigVocab and mathVocab in both Tensorflow and Pytorch ver-\nsions (see inhttps://github.com/tbs17/MathBERT). Specifically,\none can use AWS S3 bucket URLs 5 to download the Tensorflow\nversion of model artifact. The Pytorch version can be downloaded\nfrom the Hugging Face Repo6 or directly installed within the Hug-\nging Faceâ€™s framework under the name space â€œ tbs17\" using the\ncode below.\n1 from transformers import AutoTokenizer\n2 from transformers import AutoModelForMaskedLM\n3 # Download the MathBERT - basevocab\n4 tokenizer = AutoTokenizer . from_pretrained (\" tbs17 / MathBERT\n\")\n5 model = AutoModelForMaskedLM . from_pretrained (\" tbs17 /\nMathBERT \")\n6 # Download the MathBERT - mathvocab\n7 tokenizer = AutoTokenizer . from_pretrained (\" tbs17 / MathBERT\n- custom \")\n8 model = AutoModelForMaskedLM . from_pretrained (\" tbs17 /\nMathBERT - custom \")\n5http://tracy-nlp-models.s3.amazonaws.com/mathbert-basevocab-uncased\nhttp://tracy-nlp-models.s3.amazonaws.com/mathbert-mathvocab-uncased\n6https://huggingface.co/tbs17/MathBERT\n4\n(a) Content of a Math Book\n(b) Abstract of a Math arXiv Paper\n(c) Snippet of a Math Curriculum\nFigure 2: Sample mathematical corpora text from math book, arXiv paper abstract, and curriculum\n4 DOWNSTREAM MATH NLP TASKS\n4.1 Three Tasks\nWe use three mathematical tasks mentioned in Section 1 to demon-\nstrate the usefulness of MathBERT. They can be formulated as\nfollows:\nâ€¢KC Prediction (ğ‘‡ğ‘˜ğ‘ ): a single sentence multinominal clas-\nsification problem (213 labels) with ğ¼ğ‘›ğ‘ğ‘¢ğ‘¡(ğ¼) â†¦â†’ğ‘¡ğ‘’ğ‘¥ğ‘¡ and\nğ‘‚ğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡(ğ‘‚)â†¦â†’ ğ¾ğ¶ (i.e., one of 213 labels).\nâ€¢Auto-grading (ğ‘‡ğ‘ğ‘”): a two-sentence multinominal classifica-\ntion problem (5 labels) withğ¼ â†¦â†’ğ‘„ğ‘¢ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘œğ‘›&ğ´ğ‘›ğ‘ ğ‘¤ğ‘’ğ‘Ÿ pair and\nğ‘‚ â†¦â†’ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’.\nâ€¢KT Correctness (ğ‘‡ğ‘˜ğ‘¡ ): a two-sentence binary classification\nproblem withğ¼ â†¦â†’ğ‘„ğ‘¢ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘œğ‘›&ğ´ğ‘›ğ‘ ğ‘¤ğ‘’ğ‘Ÿ pair andğ‘‚ â†¦â†’ğ¶ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡ğ‘›ğ‘’ğ‘ ğ‘  .\n4.2 Task Data\nThe three task data sets are noted as ğ·ğ‘˜ğ‘ for ğ‘‡ğ‘˜ğ‘ , ğ·ğ‘ğ‘” for ğ‘‡ğ‘ğ‘”, and\nğ·ğ‘˜ğ‘¡ for ğ‘‡ğ‘˜ğ‘¡ , respectively. They are used not only to fine-tune for\ntask classification but also for pre-training TAPT BERT models,\n5\nTable 5: Task Data Details. KC: Knowledge Component, KT:\nKnowledge Tracing. All data from ASSISTments platform[ 9]\nTask #Labels #Texts #Fine-tune Split\nTrain (72%) Validate (8%) Test (20%)\nğ·ğ‘˜ğ‘ 213 13,722 9,879 1,098 2,745\nğ·ğ‘ğ‘” 5 141,186 101,653 11,295 28,238\nğ·ğ‘˜ğ‘¡ 2 269,230 193,845 21,539 53,846\nTable 6: Example texts of the three tasks with labels\nTask Data Label Text\nğ·ğ‘˜ğ‘ 8.EE.A.1\nSimplify the expression: (z2)2\nPut parentheses around the power\nif next to coefficient, for example:\n3x2=3( ğ‘¥2 ),x5=ğ‘¥5\nğ·ğ‘ğ‘” 5\nQ: Explain your answer\non the box below.\nA: because it is the same shape,\njust larger, making it similar\nğ·ğ‘˜ğ‘¡ 1 Q: What is 2.6 + (-10.9)?\nA: -8.3\nwhich will serve as baseline models for MathBERT in Section 5. All\nof three data sets are provided from ASSISTments [9]. We use the\nsame mathematical problem data set as in the best performing prior\nwork [26] with 13,722 texts and 213 labels for KC prediction. The\nauto-grading task data is the same as in the best performing prior\nwork [6] with 141,186 texts to predict scores 1 to 5. The KT data is\nthe text version (269,230 texts and 2 labels) of the ASSISTments 2009\ndata7, the numeric form of which was used by the best performing\nprior work [14].\nAmong the three data sets, ğ·ğ‘˜ğ‘ has the smallest number of\nrecords (13,722 rows) but the most unique labels (213 labels), whereas\nğ·ğ‘˜ğ‘¡ has the largest number of records (269,230 rows) but the least\nunique labels (2 labels) (see in Table 5). These three data sets were\nchosen due to their accessibility and we donâ€™t expect our results\nwould be significantly better or worse if we choose other data sets.\nWhen fine-tuning, both the labels and texts are used (see Column 2\nand 3) with split ratio of 72% training, 8% validating, and 20% testing.\nWhen pre-training for TAPT BERT models, only the unlabeled texts\nare used for further pre-training without splitting (see Column 3).\nTable 6 provides examples from the three task data sets. In ğ·ğ‘˜ğ‘ ,\nthe label â€˜8.EE.A.1â€™ represents a knowledge component (KC) code\nwhere â€˜8â€™ means Grade 8, â€˜EEâ€™ is the skill name called â€˜Expression\nand Equationâ€™, and â€˜A.1â€™ is the lesson code. There are total of 213\nKC codes in ğ·ğ‘˜ğ‘ with each represented by a specific knowledge\ncomponent. In ğ·ğ‘ğ‘”, the label â€˜5â€™ is the grading score â€˜5â€™ for the\nanswer in the text. There are total of 5 labels in ğ·ğ‘ğ‘” with â€˜5â€™ being\nthe highest and â€˜1â€™ being the lowest. In ğ·ğ‘˜ğ‘¡ , the label â€˜1â€™ means\nâ€˜correctâ€™ for the answer in the text. There are total 2 labels in ğ·ğ‘˜ğ‘¡\nwith another label â€˜0â€™ meaning â€™incorrectâ€™ for student answers.\n7https://sites.google.com/site/assistmentsdata/home/assistment-2009-2010-\ndata/skill-builder-data-2009-2010\nTable 7: Training Steps and Accuracy: MathBERT vs. TAPT\nvs. MathBERT+TAPT\nModel Task Steps MLM ACC (%)\norigVocab mathVocab\nMathBERT / 600K 99.85 99.95\nTAPT\nğ‘‡ğ‘˜ğ‘ 100K 100 /\nğ‘‡ğ‘ğ‘” 100K 99.10 /\nğ‘‡ğ‘˜ğ‘¡ 120K 99.04 /\nMathBERT +TAPT\nğ‘‡ğ‘˜ğ‘ 100K 100 99.99\nğ‘‡ğ‘ğ‘” 100K 99.95 99.96\nğ‘‡ğ‘˜ğ‘¡ 100K 99.67 99.68\n4.3 Task Training and Fine-tuning\nWe pre-train BASE BERT on the unlabeled texts of ğ·ğ‘˜ğ‘ , ğ·ğ‘ğ‘”, ğ·ğ‘˜ğ‘¡\nto build TAPT BERT models and compare their performance to\nMathBERT. The difference between TAPT and DAPT BERT train-\ning is illustrated in Fig. 1 where the input corpora is different. DAPT\nBERT models have much larger corpora whereas TAPT BERT mod-\nels are more specific to tasks. We pre-train three TAPT models with\norigVocab from the BASE BERT [ 5]. Among them, ğ‘‡ğ´ğ‘ƒğ‘‡ğ‘˜ğ‘ and\nğ‘‡ğ´ğ‘ƒğ‘‡ğ‘ğ‘” reach the best results at 100K steps and ğ‘‡ğ´ğ‘ƒğ‘‡ğ‘˜ğ‘¡ reaches\nits best result at 120K steps with the MLM accuracy of above 99%.\nEach of the TAPT models takes approximately 1 day to train. In\naddition to creating TAPT models pre-trained from BASE BERT,\nwe also pre-train TAPT models from theMathBERT weights, called\nMathBERT+TAPT. They reach the best results at steps of 100K for\nboth origVocab and mathVocab with the MLM accuracy of above\n99.6%. The MathBERT+TAPT models also take approximately 1\nday each to pre-train. We try to keep the MLM accuracy of TAPT\nModels similar to MathBERT (see in Table 7).\nFor fine-tuning, we apply ğ·ğ‘˜ğ‘ , ğ·ğ‘ğ‘”, ğ·ğ‘˜ğ‘¡ onto BASE BERT, TAPT\nBERT, MathBERT, and MathBERT+TAPT models separately. Below\nis an example code for fine-tuning on task data set withMathBERT\nweights and origVocab.\n1 os. environ [' TFHUB_CACHE_DIR '] = OUTPUT_DIR\n2 python bert / run_classifier .py \\\n3 -- data_dir = $dataset \\\n4 -- bert_config_file = uncased_L -12 _H -768 _A -12 _original /\nbert_config . json \\\n5 -- vocab_file = uncased_L -12 _H -768 _A -12 _original / vocab . txt\n\\\n6 -- task_name = $TASK \\\n7 -- output_dir = $OUTPUT_DIR \\\n8 -- init_checkpoint = $MathBERT - orig_checkpoint \\\n9 -- do_lower_case = True \\\n10 -- do_train = True \\\n11 -- do_eval = True \\\n12 -- do_predict = True \\\n13 -- max_seq_length =512 \\\n14 -- warmup_step =200 \\\n15 -- learning_rate =5e -5 \\\n16 -- num_train_epochs =5 \\\n17 -- save_checkpoints_steps =5000 \\\n18 -- train_batch_size =64 \\\n19 -- eval_batch_size =32 \\\n20 -- predict_batch_size =16 \\\n21 -- tpu_name = $TPU_ADDRESS \\\n22 -- use_tpu = True\n6\nTable 8: Optimal Hyper-parameter Combination for Task\nfine-tuning\nTask learning rate batch size max sequence length epochs\nğ‘‡ğ‘˜ğ‘ 5e-5 64 512 25\nğ‘‡ğ‘ğ‘” 2e-5 64 512 5\nğ‘‡ğ‘˜ğ‘¡ 5e-5 128 512 5\nWe discover that hyper-parameter tuning has more to do with\nthe task data instead of the model itself. In other words, the best\nhyper-parameter combinations are the same across MathBERT,\nTAPT, and MathBERT+TAPT but vary from task to task. Table 8\nshows the optimal combinations of all the hyper-parameters for\neach task. This result is obtained after hyper-parameter search on\nlr âˆˆ {1ğ‘’âˆ’5,2ğ‘’âˆ’5,5ğ‘’âˆ’5,8ğ‘’âˆ’5,1ğ‘’âˆ’4}, bs âˆˆ {8,16,32,64,128},\nmax-seq âˆˆ{128,256,512}, and ep âˆˆ{5,10,15,25}.\n5 EVALUATION OF MATHBERT\nWe denote MathBERT pre-trained with origVocab as MathBERT-\norig and MathBERT pre-trained with mathVocab as MathBERT-\ncustom. To evaluate their effectiveness across the tasks ofğ‘‡ğ‘˜ğ‘ , ğ‘‡ğ‘ğ‘”\nandğ‘‡ğ‘˜ğ‘¡ , we fine-tuneMathBERT on ğ·ğ‘˜ğ‘ , ğ·ğ‘ğ‘” and ğ·ğ‘˜ğ‘¡ and compare\nthe performance to the baseline models (see in Table 9). We group\nthe baseline models into four categories: (1) Prior solutions with\nthe best known performance, [6, 14, 26], (2) BASE BERT without\nany further pre-training, (3) TAPT BERT models pre-trained on\nthe task specific texts from BASE BERT weights, and (4) Math-\nBERT+TAPT models pre-trained on the task-specific texts from\nMathBERT weights in both origVocab and mathVocab versions.\nWe use both F1 and ACC (i.e., Accuracy) to measureğ‘‡ğ‘˜ğ‘ predic-\ntion results because traditionally, KC problems have been evaluated\nusing ACC [12, 19, 20, 25]. We provide the additional measure (F1)\nto account for the imbalance in the KC labels inğ·ğ‘˜ğ‘ . In addition, we\nuse Area-Under-the-Curve (AUC) to measure ğ‘‡ğ‘ğ‘” because AUC is\nthe typical measure used for the auto-grading problem. Finally, both\nAUC and ACC are used to measure ğ‘‡ğ‘˜ğ‘¡ because historically both\nmetrics were used for evaluation [14, 18, 23, 31]. After obtaining\nthe best hyper-parameter tuning for each task from Table 8, we run\neach model with five random seeds. We report the average value\nover five random seeds for each model and use t-tests to evaluate\nthe significance of these results. A t-test is not applied to prior test\nresults as we do not have the five random seeds results from the\nprior best method due to the lack of accessible codes.\nIn Table 8, we note thatMathBERT-orig is about 1.38% to 22.01%\nbetter and MathBERT-custom is about 1.18% to 21.92% better than\nthe best prior methods across all metrics and tasks. In addition,\nMathBERT-orig outperforms BASE BERT by about 2.14 % to 8.28%,\nall with statistical significance and MathBERT-custom outperforms\nit by about 1.98% to 8.21% across metrics and tasks, all with statisti-\ncal significance. Both versions of MathBERT out-performs TAPT\nBERT models by [0.07%,0.98%] relatively with statistical significance\nfor all tasks. We see both versions ofMathBERT under-perform the\nMathBERT+TAPT models by 0.03 % to 1.77% across all the metrics\nexcept for F1 score on ğ‘‡ğ‘˜ğ‘ from MathBERT-orig. However, only\nthe metrics for ğ‘‡ğ‘˜ğ‘¡ have obtained significance. This is expected as\nMathBERT+TAPT was further pre-trained by adapting it to the\ntask-specific data on top of the MathBERT weights.\nIn addition, the best performance for each task is all from Math-\nBERT related models. For example, forğ‘‡ğ‘˜ğ‘ , the best F1 performance\nis from MathBERT-orig followed by the second best from Math-\nBERT+TAPT-custom whereas the best and second-best ACC are\nfrom both of theMathBERT+TAPT versions (rigVcab &athVocab). For $T_{ag}$, we find the best AUC is fro\nMathBERT+TAPT-orig followed byMathBERT-orig. For ğ‘‡ğ‘˜ğ‘¡ , the\nbest and second best AUC and ACC are from both versions of\nMathBERT+TAPT with MathBERT+TAPT-custom having higher\nperformance.\n6 USE CASES\nIn this section, we describe the ongoing activities to incorporate\nMathBERT into two popular learning platforms.\n6.1 ASSISTments\nASSISTments is an online learning platform that focuses on K-\n12 mathematics education. Within ASSISTments, teachers assign\ncourse work and view reports on their students. The reports show\nstatistics on the classâ€™s performance and the responses of each\nstudent. Within the reports, teachers see a timeline of how each\nstudent progressed through the assignment and can grade studentsâ€™\nopen ended responses as well as leaving comments. Figure 3 shows\nan example of an open ended response within a studentâ€™s report,\ntogether with the score and comment left by the teacher.\nThese open ended responses provide the first opportunity to use\nMathBERT within ASSISTments. ASSISTments has recently begun\nusing Sentence-BERT [24] to suggest grades to open response ques-\ntions [1]. MathBERT provides a more domain-specific BERT model\nfor this task with high AUC. The similar task in our experimentğ‘‡ğ‘ğ‘”\nobtains 6.55% higher in AUC than the prior best work [6] which uses\nSentence-BERT [1], and can replace the current Sentence-BERT\nimplementation. MathBERT can not only provide teachers with\nsuggested grades based on studentsâ€™ open ended responses, but also\nbe used to suggest comments for teachers based on the content of\nthe studentsâ€™ answers.\nIn addition to MathBERTâ€™s benefit to teachers using ASSIST-\nments, MathBERT can also be used to enhance the student experi-\nence. As students complete problem sets in the ASSISTments Tutor,\nshown in Figure 4, they can be shown general educational material,\nsuch as YouTube videos, if they need additional guidance. Math-\nBERT can be used to identify relevant content by predicting the\nskills required to solve the problem. As the fine-tuning results for\nğ‘‡ğ‘˜ğ‘ using MathBERT-orig shows, the F1 score and ACC for the top\n3 predictions are 92.67% and 93.79% respectively. Relevant supple-\nmental education material can then be selected and shown to the\nstudent. Identifying the skills required to solve a problem will also\nintegrate well with ASSISTmentsâ€™ Automated Reassessment and\nRelearning System (ARRS) [29]. This service automatically creates\nfollow-up assignments for students when they fail to learn the mate-\nrial they were assigned. The purpose of the follow-up assignments\nis to test studentsâ€™ knowledge with problems similar to the ones the\nstudents previously got wrong. Although MathBERT was tested\non text prediction tasks such as ğ‘‡ğ‘˜ğ‘ , ğ‘‡ğ‘ğ‘” and ğ‘‡ğ‘˜ğ‘¡ , it is not limited\nto only text prediction problems and can be applied to determine\ntextual similarity, similar to the Semantic Textual Similarity Bench-\nmark (STS-B) task from General Language Understand Evaluation\n7\nTable 9: Performance Comparison: MathBERT vs. Baseline Methods across Five Random Seeds. Bold font indicates best\nperformance and underlined values are the second best. * indicates statistical significance. Î” shows relative improvement (%) of\nMathBERT over baselines.\nMethod Vocab ğ‘‡ğ‘˜ğ‘ (%) ğ‘‡ğ‘ğ‘” (%) ğ‘‡ğ‘˜ğ‘¡ (%)\nF1 ACC AUC AUC ACC\nPrior Best / 88.69[26] 92.51[26] 85.00[6] 81.82[14] 77.11[14](p)\nBASE-BERT orig 90.14 91.78 88.67 88.90 86.88(b)\nTAPT orig 91.77 92.96 90.34 95.88 93.49(t)\nMathBERT orig (o) 92.67 93.79 90.57 96.04 94.07\n(m) math (c) 92.51 93.60 90.45 95.95 94.01\nMathBERT+TAPT orig (o) 92.54 93.82 90.73 97.25 95.52\n(mt) math (c) 92.65 93.92 90.46 97.57 95.67\nÎ”ğ‘šâˆ’ğ‘\norig +4.49% +1.38% +6.55% +17.38% +21.99%\nmath +4.31% +1.18% +6.41% +17.27% +21.92%\nÎ”ğ‘šâˆ’ğ‘\norig +2.81%*** +2.19%*** +2.14%*** +8.03%*** +8.28%***\nmath +2.63%*** +1.98%*** +2.01%*** +7.93%** +8.21%***\nÎ”ğ‘šâˆ’ğ‘¡\norig +0.98%*** +0.89%*** +0.25%** +0.17% +0.62%***\nmath +0.81%*** +0.69%*** +0.12% +0.07% +0.56%***\nÎ”ğ‘šâˆ’ğ‘šğ‘¡\norig +0.14% -0.03% -0.18% -1.26%*** -1.54%***\nmath -0.15% -0.35% -0.01% -1.69%*** -1.77%***\nÎ”ğ‘šğ‘ âˆ’ğ‘šğ‘œ / -0.17% -0.20% -0.13% -0.09% -0.06%\nÎ”ğ‘šğ‘¡ğ‘ âˆ’ğ‘šğ‘¡ğ‘œ / +0.12% +0.11% -0.30% +0.33%** +0.16%\nFigure 3: An open response in a studentâ€™s report with the\nteacherâ€™s score and comment.\n(GLUE)8 which BASE BERT was evaluated on for its performance\n[5]. Therefore, we can use MathBERT to automatically evaluate\nproblems for similarity, either by determining the skills required to\nsolve the problems, or by directly comparing problem texts.\n8https://gluebenchmark.com/\nFigure 4: The ASSISTments Tutor, as seen by students when\ncompleting problem sets.\n6.2 K12.com by Stride\nStride, Inc that manages the learning platform of K12.com, is a\nleading education management organization that provides online\neducation to American students from kindergarten to Grade 12\nas well as adults. K-G12 math teachers rely on the Stride system\nto give math lessons, assign practice, home work, or exams, and\ngrade them to provide feedback to students. Teachers have long\nbeen challenged by the time and effort they spend to grade and give\nfeedback on open-ended math questions where various answers\ncould be right and it is difficult to scale feedback for immediacy\nand volume.\nTherefore, Stride is considering an automatic scoring pipeline\nwhere they can train a model on their huge proprietary reservoir\nof open-ended responses and teacher feedback to automatically\n8\nFigure 5: Stride auto-scoring pipeline using MathBERT\nsuggest scores and generate constructive feedback/comments for\nteachers to use.MathBERT could be a nice fit for this model and play\ntwo roles: (i) MathBERT fine-tunes on studentsâ€™ responses (input)\nwith ground truth teacher scoring (label) to predict scores with\nhigh accuracy (as suggested by ğ‘‡ğ‘ğ‘”), and (ii) MathBERT fine-tunes\non the different scores (input) associated with teacher feedback\n(label) to predict/generate teacher feedback for a certain kind of\nscore. For example, a student may only correctly answer part of the\nquestion and get a score of 3 out of 5, MathBERT can recommend\na feedback such as â€˜You are very close! Can you tell us more?â€™. The\nprediction output from MathBERT can then be wrapped into a\nquestion-specific teaching assistant API that prompts in front of\nstudents to guide them to reach the full score and truly master the\nknowledge component (see the pipeline in Fig. 5).\nThe pipeline will be split into three phases: (i) collect data (i.e.\nresponses, score, and feedback), (ii) use MathBERT to fine-tune\non the training data and predict scores and feedback, suggested\nto teachers via API. Teachers semi-auto grade and give feedback\nusing MathBERT suggested score and feedback. The final grade\nand feedback given to the students will then be sent back to the\nmodel to further fine-tine, and (iii) improve the accuracy of the\nquestion-specific teaching assistant API for fully automatic-scoring\nwhere teachers will only play a role in monitoring, reviewing the\nscores, and providing feedback.\nAs a proof of concept, Fig.6 illustrates what MathBERT will\noutput after fine-tuning on the open-ended responses, scores, and\nfeedback after phase 1. The red words are the feedback that the\nquestion-specific API will generate to guide students to achieve\na full score. The points (in the yellow box) will be predicted by\nMathBERT and automatically suggested to teachers.\n7 DISCUSSION AND LIMITATION\nAlthough we have verified that MathBERT is more effective than\nthe BASE BERT for mathematics related tasks with a proportional\nimprovement of [1.98%, 8.28%] with statistical significance, the ef-\nfect from an in-domain vocabulary ( mathVocab) is not what we\nexpect. As we see from Table 9, MathBERT-custom has under-\nperformed MathBERT-orig when directly fine-tuned on, but outper-\nformed MathBERT-orig when further pre-trained on task specific\ndata. However, t-tests show MathBERT-orig is not significantly\nFigure 6: Stride auto-scoring model output in the unit test\nbetter than MathBERT-custom and MathBERT+TAPT-customâ€™s\nout-performance over MathBERT+TAPT-orig is only statistically\nsignificant for ğ‘‡ğ‘˜ğ‘ .\nAs SciBERT [2] pointed out, the in-domain vocabulary is helpful\nbut the out-performance over BASE BERT could be mainly from the\ndomain corpus pre-training. Therefore, we argue that MathBERT\ntrained with athVocab so etimes can be more beneficial than\nMathBERT trained with rigVcab. In addition, we note that Math-\nBERT is not only applicable in text prediction tasks but also for\nother NLP understanding tasks such as paraphrasing, question and\nanswering, and sentence entailment tasks. We evaluate MathBERT\nfor ğ‘‡ğ‘˜ğ‘ , ğ‘‡ğ‘ğ‘”, and ğ‘‡ğ‘˜ğ‘¡ because three tasks have been heavily studied\nand their test data are available to us.\nIn future, we plan to pre-train another MathBERT on â€œinformal\"\nmathematics-related texts as opposed to the formal mathematical\ncontent (e.g. math curriculum, book and paper) that the current\nMathBERT is pre-trained on. We could potentially use such an infor-\nmal MathBERT to generate answers/conversations for mathematics\ntutoring chat bots.\n8 CONCLUSION\nIn this work, we built and introduced MathBERT-orig and Math-\nBERT-custom to effectively fine-tune on three mathematics-related\ntasks. Users can use the code from github to access the model arti-\nfacts. We showed thatMathBERT not only out-performed prior best\nmethods by [1.18%, 22.01%], but also proportionally out-performed\nthe BASE BERT by [1.98%, 8.28%] and TAPT BERT models by [0.25%,\n0.98%] with statistical significance. MathBERT-custom was pre-\ntrained with the mathematical vocabulary (mathVocab) to reflect\nthe special nature of mathematical corpora and sometimes showed\nbetter performance than MathBERT-orig. MathBERT currently is\n9\nbeing adopted by two major learning management systems (i.e., AS-\nSISTments and K12.com) to build automatic-scoring/commenting\nsolutions to benefit teachers and students.\n9 ACKNOWLEDGEMENT\nThe work was mainly supported by NSF awards (1940236, 1940076,\n1940093). In addition, the work of Neil Heffernan was in part\nsupported by NSF awards (1917808, 1931523, 1917713, 1903304,\n1822830, 1759229), IES (R305A170137, R305A170243, R305A180401,\nR305A180401), EIR(U411B190024) and ONR (N00014-18-1-2768) and\nSchmidt Futures.\nREFERENCES\n[1] Sami Baral, Anthony F Botelho, John A Erickson, and Neil T Heffernan. 2021.\nImproving Automated Scoring of Student Open Responses in Mathematics. In\nEducational Data Mining .\n[2] Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SCIBERT: A pretrained language\nmodel for scientific text. In Proceedings of the Conference on Empirical Methods in\nNatural Language Processing and 9th International Joint Conference on Natural\nLanguage Processing . 3615â€“3620.\n[3] Minjin Choi, Sunkyung Lee, Eunseong Choi, Heesoo Park, Junhyuk Lee, Dong-\nwon Lee, and Jongwuk Lee. 2021. MelBERT : Metaphor Detection via Contextu-\nalized Late Interaction using Metaphorical Identification Theories. In Proceedings\nof the Annual Conference of the North American Chapter of the Association for\nComputational Linguistics .\n[4] Albert T Corbetr and John R Anderson. 1995. Knowledge Tracing: Modeling the\nAcquisition of Procedural Knowledge.User Modeling and User-Adapted Interaction\n4 (1995), 253â€“278.\n[5] Jacob Devlin, Ming Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of deep bidirectional transformers for language understanding. In\nThe Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies , Vol. 1. 4171â€“4186.\n[6] John A Erickson, Anthony F Botelho, Steven Mcateer, Ashvini Varatharaj, and\nNeil T Heffernan. 2020. The Automated Grading of Student Open Responses in\nMathematics ACM Reference Format. InProceedings of the 10th Learning Analytics\nand Knowledge Conference .\n[7] Weiwei Guo, Xiaowei Liu, Sida Wang, Huiji Gao, Ananth Sankar, Zimeng Yang,\nQi Guo, Liang Zhang, Bo Long, Bee-Chung Chen, and Deepak Agarwal. 2020.\nDeText: A Deep Text Ranking Framework with BERT. InProceedings of the 29th\nACM International Conference on Information & Knowledge Management .\n[8] Suchin Gururangan, Ana MarasoviÂ´cmarasoviÂ´c, Swabha Swayamdipta, Kyle Lo,\nIz Beltagy, Doug Downey, Noah A Smith, and Allen. 2020. Donâ€™t Stop Pretraining:\nAdapt Language Models to Domains and Tasks. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics .\n[9] Neil T. Heffernan and Cristina Lindquist Heffernan. 2014. The ASSISTments\necosystem: Building a platform that brings scientists and teachers together for\nminimally invasive research on human learning and teaching. International\nJournal of Artificial Intelligence in Education 24, 4 (2014), 470â€“497.\n[10] Jeremy Howard and Sebastian Ruder. 2018. Universal Language Model Fine-\ntuning for Text Classification. In Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics . 328â€“339.\n[11] Kexin Huang and Jaan Altosaar. [n.d.]. ClinicalBert: Modeling Clinical Notes and\nPredicting Hospital Readmission. In arXiv preprint arXiv:1904.05342v2 .\n[12] Mario KarlovÄec, Mariheida CÃ³rdova-SÃ¡nchez, and Zachary A. Pardos. 2012.\nKnowledge component suggestion for untagged content in an intelligent tutoring\nsystem. Lecture Notes in Computer Science (including subseries Lecture Notes in\nArtificial Intelligence and Lecture Notes in Bioinformatics) 7315 LNCS (2012), 195â€“\n200.\n[13] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim,\nChan Ho So, and Jaewoo Kang. 2020. Data and text mining BioBERT: a pre-\ntrained biomedical language representation model for biomedical text mining.\nBioinformatics (2020), 1234â€“1240. https://doi.org/10.1093/bioinformatics/btz682\n[14] Youngnam Lee, Youngduck Choi, Junghyun Cho, Alexander R Fabbri, Hyunbin\nLoh, Chanyou Hwang, Yongku Lee, Sang-Wook Kim, and Dragomir Radev. 2019.\nCreating A Neural Pedagogical Agent by Jointly Learning to Review and Assess.\nIn arXiv preprint arXiv:1906.10910v2 .\n[15] Qi Liu, Zhenya Huang, Yu Yin, Enhong Chen, Hui Xiong, Yu Su, and Guoping\nHu. 2019. EKT: Exercise-aware knowledge tracing for student performance\nprediction. IEEE Transactions on Knowledge and Data Engineering 33, 1 (2019),\n100â€“115.\n[16] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, and Paul G Allen. 2019.\nRoBERTa: A Robustly Optimized BERT Pretraining Approach. In arXiv preprint\narXiv:1907.11692v1.\n[17] Zhuang Liu, Degen Huang, Kaiyu Huang, Zhuang Li, and Jun Zhao. 2020. Fin-\nBERT: A Pre-trained Financial Language Representation Model for Financial\nText Mining. In Proceedings of the Twenty-Ninth International Joint Conference on\nArtificial Intelligence Special Track on AI in FinTech .\n[18] Shalini Pandey and George Karypis. 2019. A Self-Attentive model for Knowledge\nTracing. In Proceedings of The 12th International Conference on Educational Data\nMining.\n[19] Zachary A Pardos. 2017. Imputing KCs with Representations of Problem Content\nand Context. In Proceedings of the 25th Conference on User Modeling, Adaptation\nand Personalization . 148â€“155. https://doi.org/10.1145/3079628.3079689\n[20] Thanaporn Patikorn, David Deisadze, Leo Grande, Ziyang Yu, and Neil Heffernan.\n2019. Generalizability of methods for imputing mathematical skills needed to\nsolve problems from texts. Lecture Notes in Computer Science (including subseries\nLecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 11625\nLNAI (2019), 396â€“405.\n[21] Shuai Peng, Ke Yuan, Liangcai Gao, and Zhi Tang. [n.d.]. MathBERT: A Pre-\nTrained Model for Mathematical Formula Understanding. In arXiv preprint\narXiv:2105.00377v1.\n[22] Matthew E Peters, Mark Neumann, Matt Gardner, Christopher Clark, Kenton\nLee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In\nProceedings of NAACL-HLT . 2227â€“2237.\n[23] Chris Piech, Jonathan Spencer, Jonathan Huang, Surya Ganguli, Mehran Sahami,\nLeonidas Guibas, Jascha Sohl-Dickstein, Stanford University, and Khan Academy.\n2015. Deep Knowledge Tracing. In Advances in Neural Information Processing\nSystems.\n[24] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings\nusing Siamese BERT-Networks. In Proceedings of the Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference\non Natural Language Processing . 3982â€“3992.\n[25] Carolyn RosÃ©, Pinar Donmez, Gahgene Gweon, Andrea Knight, Brian Junker,\nWilliam Cohen, Kenneth Koedinger, and Neil Heffernan. 2005. Automatic and\nSemi-Automatic Skill Coding With a View Towards Supporting On-Line As-\nsessment. In Proceedings of the conference on Artificial Intelligence in Education .\n571â€“578.\n[26] Jia Tracy Shen, Michiharu Yamashita, Ethan Prihar, Neil Heffernan, Xintao Wu,\nSean Mcgrew, and Dongwon Lee. 2021. Classifying Math Knowledge Components\nvia Task-Adaptive Pre-Trained BERT. InProceedings of the Conference on Artificial\nIntelligence in Education .\n[27] Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. 2019. How to Fine-Tune\nBERT for Text Classification? Lecture Notes in Computer Science (including\nsubseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n11856 LNAI, 2 (2019), 194â€“206.\n[28] Nguyen Thai-Nghe, Lucas Drumond, Artus Krohn-Grimberghe, and Lars Schmidt-\nThieme. 2010. Recommender system for predicting student performance.Procedia\nComputer Science 1, 2 (2010), 2811â€“2819.\n[29] Yutao Wang and Neil T. Heffernan. 2014. The effect of automatic reassessment and\nrelearning on assessing student long-term knowledge in mathematics. Lecture\nNotes in Computer Science (including subseries Lecture Notes in Artificial Intelligence\nand Lecture Notes in Bioinformatics) 8474 LNCS (2014), 490â€“495.\n[30] Denghui Zhang, Zixuan Yuan, Yanchi Liu, Zuohui Fu, Fuzhen Zhuang, Pengyang\nWang, Haifeng Chen, and Hui Xiong. 2020. E-BERT: A Phrase and Prod-\nuct Knowledge Enhanced Language Model for E-commerce. In arXiv preprint\narXiv:2009.02835v2.\n[31] Jiani Zhang, Xingjian Shi, Irwin King, and Dit-Yan Yeung. 2017. Dynamic Key-\nValue Memory Networks for Knowledge Tracing. In International World Wide\nWeb Conference Committee (IW3C2) .\n10"
}