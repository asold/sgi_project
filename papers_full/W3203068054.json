{
    "title": "Pre-Trained Transformer-Based Language Models for Sundanese",
    "url": "https://openalex.org/W3203068054",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A3203258813",
            "name": "Wilson Wongso",
            "affiliations": [
                "Binus University"
            ]
        },
        {
            "id": "https://openalex.org/A3201891178",
            "name": "Henry Lucky",
            "affiliations": [
                "Binus University"
            ]
        },
        {
            "id": "https://openalex.org/A1877050847",
            "name": "Derwin Suhartono",
            "affiliations": [
                "Binus University"
            ]
        },
        {
            "id": "https://openalex.org/A3203258813",
            "name": "Wilson Wongso",
            "affiliations": [
                "Binus University"
            ]
        },
        {
            "id": "https://openalex.org/A3201891178",
            "name": "Henry Lucky",
            "affiliations": [
                "Binus University"
            ]
        },
        {
            "id": "https://openalex.org/A1877050847",
            "name": "Derwin Suhartono",
            "affiliations": [
                "Binus University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6600719526",
        "https://openalex.org/W6702248584",
        "https://openalex.org/W6600424091",
        "https://openalex.org/W2915800638",
        "https://openalex.org/W2948902769",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W6603222412",
        "https://openalex.org/W6611333299",
        "https://openalex.org/W2607658476",
        "https://openalex.org/W6600655081",
        "https://openalex.org/W2972413484",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3086966320",
        "https://openalex.org/W2121879602",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3098637735",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2989539713",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W3153266325",
        "https://openalex.org/W3118017378",
        "https://openalex.org/W3107705748",
        "https://openalex.org/W2980708516",
        "https://openalex.org/W3164886736",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W2979537065",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W4287993739",
        "https://openalex.org/W3173954987",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4287370843",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W1498436455",
        "https://openalex.org/W3107826490"
    ],
    "abstract": "Abstract The Sundanese language has over 32 million speakers worldwide, but the language has reaped little to no benefits from the recent advances in natural language understanding. Like other low-resource languages, the only alternative is to fine-tune existing multilingual models. In this paper, we pre-trained three monolingual Transformer-based language models on Sundanese data. When evaluated on a downstream text classification task, we found that most of our monolingual models outperformed larger multilingual models despite the smaller overall pre-training data. In the subsequent analyses, our models benefited strongly from the Sundanese pre-training corpus size and do not exhibit socially biased behavior. We released our models for other researchers and practitioners to use.",
    "full_text": "Pre-Trained Transformer-Based Language Models\nfor Sundanese\nWilson Wongso  (  wilson.wongso001@binus.ac.id )\nBina Nusantara University https://orcid.org/0000-0003-0896-1941\nHenry Lucky \nBina Nusantara University\nDerwin Suhartono \nBina Nusantara University\nResearch\nKeywords: Sundanese Language, Transformers, Natural Language Understanding, Low-resource\nLanguage\nPosted Date: September 27th, 2021\nDOI: https://doi.org/10.21203/rs.3.rs-907893/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nWongso et al.\nRESEARCH\nPre-trained Transformer-based Language Models\nfor Sundanese\nWilson Wongso\n*, Henry Lucky and Derwin Suhartono\n*Correspondence:\nwilson.wongso001@binus.ac.id\nComputer Science Department,\nSchool of Computer Science, Bina\nNusantara University, Jakarta,\n11840, Indonesia\nFull list of author information is\navailable at the end of the article\nAbstract\nThe Sundanese language has over 32 million speakers worldwide, bu t the\nlanguage has reaped little to no beneﬁts from the recent advance s in natural\nlanguage understanding. Like other low-resource languages, the on ly alternative is\nto ﬁne-tune existing multilingual models. In this paper, we p re-trained three\nmonolingual Transformer-based language models on Sundanese data . When\nevaluated on a downstream text classiﬁcation task, we found tha t most of our\nmonolingual models outperformed larger multilingual models desp ite the smaller\noverall pre-training data. In the subsequent analyses, our mo dels beneﬁted\nstrongly from the Sundanese pre-training corpus size and do not e xhibit socially\nbiased behavior. We released our models for other researchers and practitioners\nto use.\nKeywords: Sundanese Language; Transformers; Natural Language\nUnderstanding; Low-resource Language\nIntroduction\nRecently, there has been a multitude of advances in the ﬁeld of natur al language,\nespecially since the release of the Transformer [\n1] architecture. Before Transform-\ners, RNNs (Recurrent neural networks) [ 2] like LSTMs (Long short-term memory\nnetworks) [ 3], and GRUs (Gated recurrent units) [ 4] were prevalent due to their\nnetwork architecture design that suited the domain of natural language in t he form\nof texts. However, most of these older architectures struggled to mod el long-term\nsequential dependency and are often slow to train due to the inabili ty of paralleliza-\ntion.\nOn the other hand, Transformers solves this issue by being paralleli zable during\ntraining and extensively applying the attention mechanism to bett er model long\nsequences of inputs. Several modern architectures were then de rived from the orig-\ninal Transformer model, such as the OpenAI GPT (Generative Pre-Trai ning) [ 5],\nBERT (Bidirectional Encoder Representations from Transformers) [ 6], and subse-\nquently variants like GPT-2 [ 7], GPT-3 [ 8], and RoBERTa (Robustly Optimized\nBERT Pre-training Approach) [ 9].\nFor instance, the GPT architecture stacks decoder blocks of the ori ginal Trans-\nformer architecture, while BERT stacks encoder blocks of the origi nal Transformer\nand modiﬁes them to become naturally bidirectional by design. Due to this discrep-\nancy, GPT-based models are normally pre-trained as causal/generative lan guage\nmodels, whereas BERT-based models are pre-trained as masked language m od-\nels. Once these models have been pre-trained, they can similarl y be ﬁne-tuned to\nWongso et al. Page 2 of 16\ndownstream natural language understanding tasks. This scheme allowed for break-\nthroughs in tasks like text classiﬁcation, question-answering, natur al language in-\nference, among many others.\nHowever, most of these recent successes occur in the domain of high-r esource lan-\nguages like English, Chinese, and Spanish where data are abundant. Low- resource\nlanguages, conversely, have reaped little beneﬁts despite the rec ent advances; at-\ntributing to the lack of data and existing work [ 10]. The best alternative is to\nﬁne-tune a multilingual model whose corpus contains that speciﬁc lo w-resource lan-\nguage. However, previous studies [ 11, 12] showed that multilingual models often\nperform poorly compared to monolingual models of low-resource languages.\nTo that, we propose the pre-training of various Transformer-based language mod-\nels on a low-resource language, namely Sundanese. The Sundanese language i s In-\ndonesia’s third-most spoken language [ 13], with over 32 million speakers worldwide\n[14]. It is the oﬃcial language in larger regions of Banten and West Java, as well as\na minority language in various parts of Jakarta, Lampung, Central Java, and other\nnearby provinces in Indonesia [\n15].\nMoreover, a previous study [ 16] showed that the Sundanese language remains the\nmain communication tool for adults and parents living in those regions. In the same\npaper, the authors concluded that there is a desire to preserve the Sundanese lan-\nguage in the fast-growing modern technological world as a cultural identi ty of the\npeople. Another study [ 17] similarly noted that a great deal of Indonesian social\nmedia activity in platforms like Twitter originates from West Java, w here the Sun-\ndanese language is most often used. With that many speakers and a huge market ,\nit would be beneﬁcial to have Sundanese language models that could be app lied to\nvarious business processes.\nContrarily, there is very limited Sundanese corpus to be used for p re-training.\nHence, on top of the pre-existing multilingual corpora like OSCAR (Ope n Super-\nlarge Crawled Aggregated Corpus) [ 18], CC-100 [ 19, 20], and C4 [ 21], Sundanese\ndocuments from Wikipedia were also used during the pre-trainin g of our language\nmodels to cater to the scarcity of data availability.\nHaving been pre-trained, these models were thus ﬁne-tuned to a downstream task\nof classifying emotions from tweets and were subsequently benchmar ked against ex-\nisting traditional machine learning algorithms and multilingual Transf ormer models\nsuch as the multilingual BERT [ 6] and the RoBERTa variant of XLM (Cross-lingual\nLanguage Model Pre-training) [ 19].\nOnce these models have been fully pre-trained and ﬁne-tuned, t hey are subse-\nquently released in the HuggingFace Transformers Model Hub [1], in hopes of en-\ncouraging other researchers to work on the ﬁeld of Sundanese language model ing\nwith open access to our models.\nIn the following sections, we will continue by discussing the bac kground of Sun-\ndanese language modeling in greater detail, followed by the methodology u sed and\nexperiments performed during the pre-training of our models, and ﬁnally, various\nanalyses, discussions, and conclusion thereafter.\n[1]https://hf.co/models?filter=su\nWongso et al. Page 3 of 16\nRelated Works\nTransformer Architectures\nBefore the advent of deep learning [\n22] and Transformers [ 1] following thereafter,\nnatural language is one domain that computers have always struggled to model . This\nunderlying issue prevails due to the fact that the rules of natural l anguages are often\ncomplex and vary from one language to another. Language modeling by means of\ntraditional methods like statistical or rule-based models would ofte n crumble when\nnew, unseen inputs are fed into these models, given their inabil ity to generalize.\nOn the contrary, deep learning models need not be taught the rules of n atural\nlanguage explicitly. Instead, by using methods like supervised l earning, these models\ncould gain an understanding of sequences of texts given the ground truth /label.\nA few of the earliest deep learning architectures developed to lear n sequences of\ninputs include Recurrent neural networks [\n2], Long short-term memory networks [ 3],\nand Gated Recurrent Units [ 4]. Unlike feed-forward neural network architectures, a\nrecurrent neural network has temporal information passed over time by considering\nthe output of previous time steps.\nHowever, these temporal models are still relatively ineﬃcient to train despite the\nsophisticated modiﬁcations because of their sequential behavior by design. Trans-\nformers [ 1], on the other hand, completely remove the idea of recurrence and fu lly\nsubstitute it with the attention mechanism given by the following equation:\nAttention(Q, K, V ) = softmax\n( QK⊤\n√ dk\n)\nV, (1)\nwhere Q, K, and V represent the query, key, and value vectors respectively. They\nare not only eﬀective to capture relations between words, but are also e ﬃcient to\ntrain on accelerators like GPUs (Graphical Processing Units) and recen tly, TPUs\n(Tensor Processing Units).\nFollowing the release and prominence of Transformers [ 1], OpenAI then developed\na language model that builds on top of the original Transformer architecture , as\nwell as extending the training schemes proposed in ULMFiT (Universal Language\nModel Fine-Tuning) [ 23] and ELMo (Embeddings from Language Models) [ 24].\nInstead of utilizing LSTMs [ 3] as found in ULMFiT [ 23], the proposed OpenAI\nGPT model applies the decoder blocks of the Transformer architect ure. Moreover,\nthe learning process starts by ﬁrst pre-training the model with the objective of\ngenerating the next word in a sequence, given a current prompt. Thi s way, the\nmodel could learn the context of words in a given sequence before bein g tasked\nto solve downstream problems. This pre-training objective also gre atly leverages\nthe widespread availability of unlabelled data as the process is per formed in an\nunsupervised manner.\nAfterward, the pre-trained model is thus ﬁne-tuned in a supervi sed manner to a\ndownstream task where labels are ﬁnally required. For example, if th e model were\nto be ﬁne-tuned as a text classiﬁer model, the causal/generative langu age model\nhead is swapped with a linear model projecting to the number of poss ible classes.\nWith the usual cross-entropy loss function, the then-language model c ould now be\nWongso et al. Page 4 of 16\ntrained as a text classiﬁer model. This ﬁne-tuning regime could si milarly be applied\nto other various downstream tasks alike.\nThe successor to the ﬁrst GPT model [ 5] is then called GPT-2 [ 7], where the latter\nis simply ten times as large as the former in terms of the number of parame ters and\nwas trained on an even larger pre-training dataset. The GPT-2 model sti ll retains\nthe same architectural design and training scheme as GPT.\nBERT [ 6] similarly leverages the original Transformer architecture just as GP T\ndid. However, rather than taking the decoder blocks, BERT utiliz es the encoder\nblocks of the Transformer model. Moreover, BERT is naturally bidir ectional by\ndesign, whereas the previously aforementioned models learn to gener ate sequences\nfrom left to right. Due to this diﬀerence by design, BERT cannot be t rained with a\ngenerative/causal objective. Instead, several words in a sequence are replaced with\na special masked token which BERT has to learn to ﬁll with the right wor d. This\nway, BERT learns the bidirectional context of words as a masked language mo del,\nunlike GPT, through a similar means of unsupervised learning. Aside f rom its mask-\nﬁlling objective, BERT also has a next sentence prediction object ive that learns to\nclassify whether a pair of sequences follows each other. Like GPT, BE RT can also\nbe ﬁne-tuned into downstream tasks like text classiﬁcation, ques tion-answering, etc.\nRoBERTa [9] works upon the existing architecture of BERT [ 6] and argues that\nBERT is not only under-trained, but could also be robustly improved with sev-\neral modiﬁcations. For instance, RoBERTa removes the need to use ne xt sentence\nprediction as a pre-training objective, as well as uses dynamic maski ng instead of\nBERT’s static masking. That is, instead of feeding a sequence where the masked\ntoken is in the same position in every epoch ( static), RoBERTa sees multiple ver-\nsions of the same sequence with the masked token in diﬀerent position s ( dynamic).\nBy further feeding RoBERTa with an even larger pre-training dataset , the authors\nargue that RoBERTa would outperform BERT in most cases.\nSundanese Language Modeling\nPrior to the era of deep learning models, the ﬁeld of Sundanese language model-\ning relied mostly on traditional machine learning algorithms. For exampl e, various\ntechniques were proposed to classify Sundanese texts, includin g the emotional clas-\nsiﬁcation of tweets [\n25, 17] and speech level/register-classiﬁcation of Sundanese texts\n[26].\nThose experiments involved the usage of traditional single-task machin e learn-\ning algorithms like K-nearest neighbors, Naive Bayes classiﬁer, and Su pport Vec-\ntor Machines. Although they were able to attain a relatively decent clas siﬁcation\nresult with over 90% accuracy, these models are only built for the spe ciﬁc task\nof text classiﬁcation. They are therefore inapplicable to other language m odeling\ntasks like named-entity recognition, part-of-speech tagging, and extrac tive question-\nanswering.\nMore ﬂexible approaches include the usage of multilingual Transformer- based\nmodels such as the mBERT (multilingual BERT) model [ 6], or the large cross-\nlingual XLM model [ 27]. Both of these models claim to support multiple languages,\nincluding Sundanese, since a slight percentage of their respecti ve pre-training cor-\npus consists of Sundanese texts. Multilingual models like them ma y be applicable\nto cases where the target language is of high-resource, unlike Sundanese .\nWongso et al. Page 5 of 16\nA more recent study similarly trained a multilingual BART (Bidirec tional and\nAuto-Regressive Transformers) model [ 28] called IndoBART [ 29] that pre-trains\non Indonesian, Javanese, and Sundanese corpus. Their authors showed t hat the\nIndoBART model is able to perform sequence-to-sequence tasks like summarization\nand neural machine translation on the Sundanese language as well.\nHowever, there have been studies [ 12] which show that monolingual models are\ngenerally more performant than multilingual models due to the diﬀeri ng sizes of pre-\ntraining data and a more accurate tokenization scheme [ 11]. This is very much ap-\nparent in pre-trained monolingual models in various languages, such as Ind oBERT\n[30] for Indonesian, PhoBERT [ 31] for Vietnamese, WangchanBERTa [ 32] for Thai,\nwhereby these monolingual models constantly outperform their multi lingual coun-\nterparts in downstream tasks.\nTherefore, given the evidence and the various studies which claim t he superiority\nof monolingual models over multilingual models for the case of low-resou rce lan-\nguages, it is thus preferable to pre-train a monolingual language model whe never\npossible.\nSundanese Language Model Pre-Training\nIn this section, we will present the pipeline of the pre-trainin g process. This includes\nthe conﬁgurations of architectures, the pre-training corpus used, h ow pre-processing\nwas performed, the optimization scheme, and the pre-training resul ts.\nArchitectures\nConsidering the varying performances of diﬀerent Transformer mode ls, we pre-\ntrained three diﬀerent models based on the architectural designs of OpenAI GPT-2\n[\n7], BERT [ 6], and RoBERTa [ 9]. All three of these models are of the base size and\nutilize GELU (Gaussian Error Linear Unit) [ 33] as their activation functions. They\nall have 12 layers, 12 heads, an embedding dimension and hidden size of 768, and\nan intermediate size of 3,072. Additional details about the models’ conﬁgurat ions\nare shown in Table 1 .\nTable 1 Conﬁgurations of pre-trained Sundanese models.\nModel #Params Vocabulary Size Language Modeling Task\nSundanese GPT-2 124M 50,257 Causal/Generative\nSundanese BERT 110M 30,522 Masked\nSundanese RoBERTa 124M 50,265 Masked\nAdditionally, Sundanese GPT-2 is pre-trained as a causal language model, whereas\nSundanese BERT and Sundanese RoBERTa are pre-trained as masked language\nmodels. However, the NSP (next sentence prediction) objective f rom BERT was\nremoved during pre-training, leaving merely the MLM (masked langu age modeling)\nobjective. RoBERTa, on the other hand, was trained according to the origin ally\nproposed paper, which similarly removes the need to use NSP.\nData\nThe dataset used during pre-training consists of Sundanese subset s of multilingual\ncommon-crawl corpora of OSCAR [\n18], CC-100 [ 19, 20], and C4 [ 21]. They are of\nthe size 141KB, 15MB, and 464MB, respectively. Seeing how Transformer mo dels\nWongso et al. Page 6 of 16\nheavily depend on a large pre-training monolingual corpus [ 11], additional Sun-\ndanese documents from Wikipedia were added to the pre-training c orpus. 68,920\ndocuments Wikipedia texts were collected in June 2021 from Wikidu mp, with a size\nof 279MB after parsing. This accumulates to a sum of 758MB of pre-training text ,\nwith 10% of the dataset held out for evaluation purposes.\nThis hence makes our pre-training corpus consist mainly of informally written\ndocuments with varying ﬂuency levels. Nonetheless, the aim of usi ng such a corpus\nis to gain as many vocabularies as possible, as well as to provide a divers e list of\ngenres of texts.\nPre-processing\nThere is a need to pre-process the pre-training corpus such that it can be learned\nby the model. As the diﬀerent architectures require diﬀerent t okenizers and colla-\ntion schemes, there are slight diﬀerences in the pre-processin g step for each model.\nNonetheless, they follow a similar pre-processing pipeline that many other pre-\ntrained Transformer models follow, as shown in\nFigure 1 . The following subsections\nwill explain this in greater detail.\nLoad Raw Dataset\nTrain Tokenizer on Dataset\nTokenize Dataset\nGroup Texts into Sequences\nCollate Sequences\nFigure 1 Data Pre-processing Pipeline\nTokenization\nThe tokenizers used for each of the models accord to their respectiv e original papers.\nFor instance, both the Sundanese GPT-2 and RoBERTa models leveraged t he Byte-\nlevel BPE (Byte-Pair Encoding) tokenizer [\n34], though with diﬀerent vocabulary\nsizes as shown in Table 1 . This type of tokenizer relies on the pre-tokenization\nof the raw datasets, which in our case, happens naturally as Sundanese te xts are\nseparated by whitespace. The Byte-level BPE tokenizer learns to m erge byte-base\ncharacters based on their occurring frequencies until the desir ed vocabulary size\nhas been reached. This allows the tokenizer to tokenize every sequ ence without\nhaving to use the <unk> token, which corresponds to the special token representing\nunknown characters.\nThe Sundanese BERT, on the other hand, utilized the WordPiece token izer [ 35]\nthat works very similarly to the BPE tokenizer. That is, the former b egins by\nWongso et al. Page 7 of 16\ncreating a vocabulary with every present character in the raw datase t and then\nlearns to merge these characters into subwords. Instead of merging bas ed on the\nfrequency of occurrence like the BPE tokenizer does, WordPiece tokenizer creates\nsubwords of characters that are most likely to be used as training data onc e included\nin the resultant vocabulary.\nGrouping and Collation\nOnce these tokenizers have been trained on the pre-training corpu s, the raw texts\nare subsequently encoded into their respective numerical toke n representations. Ad-\nditionally, special tokens, attention masks, and token type identiﬁe rs are added to\nfacilitate training.\nAfterward, these texts ought to be grouped into sequences of uniform l engths.\nThe Sundanese BERT and RoBERTa models were initialized to handle a maximum\nsequence length of 128, while the Sundanese GPT-2 model could take as in put a\nlonger block size of 512. Finally, data collation was performed to accord to t he dif-\nferent models’ language modeling task. Sundanese GPT-2, which learn s to generate\ntexts, simply set the next token in sequence as the labels, i.e., shifting the texts to\nobtain ground truth. On the other hand, the masked language models, BERT an d\nRoBERTa, require a more sophisticated data collation technique that in volves the\nmasking of tokens.\nThere is a slight discrepancy in the masking technique of BERT and R oBERTa,\nwhere the former does static and the latter does dynamic masking. Regar dless, each\nof them follows their original masking strategy proposed in [\n6] and [ 9] respectively,\nwith a masking probability of 15%. The masked tokens are thus the labels f or the\nmodels to predict.\nOptimization\nIn the pre-training of our models, the AdamW optimizer [\n36] was used with values\nβ1 = 0 .9, β2 = 0 .999, and ǫ = 10 −8. There was a slight diﬀerence in the learning\nrate and weight decay of the diﬀerent models, as outlined in Table 2. A weight decay\nfactor of 0.1 was added to the pre-training of Sundanese GPT-2 to better regularize\nthe model, as it was prone to overﬁtting. The other two models, on the other hand,\ndid not exhibit overﬁtting behavior, hence the 0.0 weight decay f actor.\nTable 2 Learning rate and weight decay of each of the pre-trained Sun danese models.\nModel Learning Rate Weight Decay\nSundanese GPT-2 1 × 10− 4 0.1\nSundanese BERT 2 × 10− 4 0.0\nSundanese RoBERTa 2 × 10− 4 0.0\nAll three models were trained for 50 epochs, with a batch size of 64 per device,\nand were paired with a linearly decaying scheduler with a warm-up step of 1,000.\nThe learning rate decreased linearly per optimization step and decay ed all the way\nto zero at the end of training. Since the pre-training was performed on an 8-core\nTPU, this brings the total eﬀective batch size to 512.\nWongso et al. Page 8 of 16\nExperiments\nSubsequent experiments were conducted according to the setup p resented in the\nprevious section,\nSundanese Language Model Pre-Training . We implemented the\nGPT-2, BERT, and RoBERTa models provided by HuggingFace’s Transformers\nlibrary. The framework allows for easy integration with deep learning f rameworks\nlike Flax [ 37] and PyTorch [ 38], as well as compatibility to run on a GPU or a\nmulti-core TPU accelerator.\nPre-Training\nThe pre-training process was done entirely on a TPU with 8 cores, us ing the Hug-\ngingFace library that runs on top of Flax. As explained above, the Sundanese\nGPT-2 was trained as a causal language model, whereas the Sundanese BERT\nand RoBERTa were trained as masked language models. These models traine d on\n90% of the pre-training corpus that makes up the training subset, follo wed by an\nevaluation on the remaining 10%.\nTable 3 depicts the pre-training results of our\nmodels after 50 epochs.\nTable 3 Pre-training results of Sundanese language models.\nModel Training Loss Evaluation Loss Evaluation Perplexity\nSundanese GPT-2 2.436 3.610 36.97\nSundanese BERT 2.860 2.845 17.20\nSundanese RoBERTa 1.965 1.952 7.04\nIn spite of the varying pre-training results, the three models ar e incomparable due\nto their diﬀerent language modeling tasks. Furthermore, it is more r elevant to com-\npare these models on a downstream task where the overall measure of pe rformance\nis more representative of the model’s capability.\nEvaluation\nWe ﬁne-tuned these models to a downstream task of emotional classiﬁc ation of Sun-\ndanese tweets to better compare our pre-trained models with the ex isting baseline\nmodels.\nDownstream Dataset\nThe dataset used for the downstream purpose of text classiﬁcation consi sts of Sun-\ndanese tweets collected by Putra et al. [\n17]. It contains 2,518 tweets with four\npossible emotions of sadness, joy, fear, and anger. The authors proposed mu ltiple\nmachine learning-based solutions, with a one-vs-one, linear SVC (C-S upport Vector\nClassiﬁer) being the most prominent algorithm. Various text pre-pro cessing tech-\nniques were performed, including case folding, stopword ﬁltering, stemming, and\ntokenizing. They also utilized TF-IDF (Term Frequency–Inverse Document Fre-\nquency) as the feature extractor and their proposed solution serves as t he baseline\nresults for our models.\nHowever, a slight tweak in the dataset splitting was made as the authors or iginally\nused 10-fold cross-validation. Rather than doing so, 10% of the tweets were h eld\nout for evaluation purposes and the same subset was used for all the ﬁne-tu ning\nexperiments.\nWongso et al. Page 9 of 16\nFine-Tuning Setup\nThere are various methods to ﬁne-tune causal and masked language models as\ntext classiﬁers. ULMFiT [\n23] suggests a two-step ﬁne-tuning of their LSTM model\nto better capture in-domain data. Likewise, BERTFiT [ 39] investigates the best\nmethod to ﬁne-tune a Chinese BERT model. In the end, we decide d to conduct\nthe usual ﬁne-tuning scheme of replacing the language model head with additional\nlinear layers and only training for one additional phase instead of two.\nOur pre-trained models were compared against the baseline method pr esented\nin [ 17], multilingual BERT [ 6], XLM-RoBERTa [ 19], as well as IndoBERT Base\nPhase 1 [ 30]. The same text pre-processing scheme was applied to the classiﬁ cation\ndataset – without data collation – using the respective tokenizers of e ach model and\na sequence length of 128.\nLike the pre-training stage, the AdamW optimizer [ 36] was used to ﬁne-tune the\ndeep learning models, with the same values of β1, β2, and ǫ as shown in the previous\nsection, Optimization. All deep learning models were ﬁne-tuned for 10 epochs, with\nthe varying hyperparameter choices reﬂected in Table 4 . Also, the same linearly\ndecaying scheduler that brought the learning rate down to zero was u sed, except\nwithout warm-up steps. The model checkpoint with the highest F1-s core is loaded\nat the end of training.\nTable 4 Hyperparameters used to ﬁne-tune pre-trained models.\nModel Learning Rate Weight Decay Batch Size\nSundanese GPT-2 1 × 10− 5 0.01 16\nSundanese BERT 4 × 10− 5 0.01 8\nSundanese RoBERTa 2 × 10− 5 0.01 16\nIndoBERT [ 30] 2 × 10− 5 0.0 16\nmBERT [ 6] 2 × 10− 5 0.0 16\nXLM-RoBERTa [ 19] 2 × 10− 5 0.0 16\nWith this setup, ﬁne-tuning experiments were conducted usin g HuggingFace’s\nimplementation of Transformer models for sequence classiﬁcation. How ever, instead\nof running on top of JAX, PyTorch was used as the backend framework. Moreover ,\nthe experiments involving Sundanese monolingual classiﬁers were c arried out on a\nsingle NVIDIA Tesla T4 GPU, while the rest used TPUs – both of which wer e\naccessed via Google Colaboratory.\nResults\nThe following section will lay out the classiﬁcation results of ﬁne- tuning to Sun-\ndanese tweets. There were four main types of models which we ﬁne-t uned, start-\ning with the baseline linear SVC with TF-IDF approach as proposed in [\n17], our\npre-trained monolingual Sundanese models, multilingual Transformer models, and\nIndoNLU’s IndoBERT model [ 30] which was trained on an Indonesian corpus. Ta-\nble 5 shows the various ﬁne-tuning results as well as the number of paramet ers per\nmodel. Both the accuracy and F1-score were calculated on the evaluation su bset,\nwhich makes up 10% of the original dataset.\nAmong the ﬁne-tuned models, our Sundanese RoBERTa managed to obtain the\nhighest accuracy score of 98.41% and F1-macro score of 98.43%. It, therefore, out-\nperformed the baseline method, the IndoBERT [\n30] base model, as well as both\nWongso et al. Page 10 of 16\nTable 5 Evaluation result of Sundanese Twitter emotion classiﬁcat ion.\nModel #Params Accuracy F1-Macro\nBaseline\nLinear SVC with TF-IDF [\n17] 30K 96.43 96.36\nSundanese (Ours)\nSundanese GPT-2 124M 94.84 94.75\nSundanese BERT 110M 96.82 96.75\nSundanese RoBERTa 124M 98.41 98.43\nIndonesian\nIndoBERT [\n30] 124M 96.43 96.42\nMultilingual\nmBERT [\n6] 167M 96.83 96.79\nXLM-RoBERTa [ 19] 278M 95.24 95.22\nmBERT [ 6] and XLM-RoBERTa [ 19] – where the latter two are larger in terms of\nthe number of parameters.\nMoreover, the Sundanese BERT model performed just slightly belo w the mBERT\nmodel while still outperforming the much larger XLM-RoBERTa. On the ot her\nhand, the Sundanese GPT-2 model could not even be on-par with the base line\nmethod of linear SVC. Overall, these results are in line with thei r respective papers’\nconclusions, whereby RoBERTa [ 9] can indeed exceed the results of BERT [ 6],\nwhereas GPT-2 [ 7] remains less eﬀective compared to BERT.\nLikewise, despite being smaller in the number of parameters and ove rall pre-\ntraining corpus size, both Sundanese BERT and RoBERTa were able to su rpass –\nor perform similarly to – the larger multilingual models. It should be noted that the\nmonolingual models were trained on a larger pre-training Sundanese corp us, whereas\nthe multilingual corpus used to pre-train the multilingual models contained a very\nsmall percentage of Sundanese texts. These results are hence paralle l with the ideas\nproposed in [ 11].\nFinally, the baseline linear SVC method with TF-IDF feature extr actor as pro-\nposed in [ 17] provided a relatively high baseline compared to Transformer model s of\nvery large sizes. We suspect that this ability is achievable due to t he sophisticated\npre-processing rules applied to the raw dataset prior to tokenizat ion and the high\ndimensionality of features after extracting through TF-IDF.\nAnalysis & Findings\nImpact of Pre-training Corpus Size\nFrom the results shown in\nTable 5 , it is apparent, yet unsurprising, that the per-\nformance of Transformer-based models is not always proportional to their number\nof parameters. Instead, as suggested by [ 11], the diﬀerence in performance between\nmonolingual and multilingual models depends on the size of the pre-trai ning corpus\nand the ability of their tokenizers to adapt to the target language. Theref ore, it is\nmuch more suitable to compare the diﬀerent models’ pre-training c orpus size over\ntheir respective number of parameters.\nLike the comparison method presented in [ 11], we compared the pre-training\ncorpus size of our Sundanese language models, mBERT [ 6], and XLM-RoBERTa\n[19], against their downstream performance, as shown in Figure 2 . The Sundanese\npre-training dataset discussed in the previous section, Data, contains about 89.8M\nwords. As for the multilingual models, only the Sundanese subsets of t heir respective\npre-training corpus were taken into account.\nWongso et al. Page 11 of 16\nFigure 2 Downstream performance of Sundanese and multilingual mode ls on tweet emotion\nclassiﬁcation in terms of their number of parameters. The ar ea of the points corresponds to their\npre-training corpus size (in million words).\nMoreover, as the mBERT model was trained on entire Wikipedia dumps , there is\nnot an exact number that represents the number of words present in th e Sundanese\nsubset used for the pre-training of mBERT. We resorted to ﬁnding an upper-bound\nestimate of 6.22M words through the statistics provided in Wikimedia [2], like the\nauthors of [ 11] did. On the other hand, there are 10M words present in the Sundanese\nsubset of the CC-100 dataset [ 20] used to pre-train XLM-RoBERTa [ 19] as readily\nshown in the original paper.\nThis shows that the pre-training corpus size of a language model signiﬁ cantly\naﬀects its ﬁnal performance on downstream tasks. Although the multilin gual models\nare much larger in terms of the number of parameters and were pre-traine d on\nsigniﬁcantly larger multilingual corpora like Wiki-100 and CC-100, they ma y not\nconsistently outperform smaller monolingual models. Factors such as th e varying\nability of diﬀerent tokenizers to adapt to a target language like Sundanes e should\nsimilarly be considered.\nSimilarity of Indonesian and Sundanese Corpus\nAs shown in\nTable 5 , the IndoBERT model [ 30] was somehow able to perform\njust slightly below our Sundanese BERT model, despite the former being trained\non mostly Indonesian data. This could mean a few things; either there are token\noverlaps between the Sundanese and Indonesian tokenizers or that the Indonesian\nlanguage is highly related to the Sundanese language on its own. Both possibi lities\nwere investigated.\nThe IndoBERT model [ 30] used the SentencePiece tokenizer [ 40] to encode their\npre-training corpus called Indo4B, collected from various Indonesian data. As none\n[2]https://meta.m.wikimedia.org/wiki/List_of_Wikipedias. Accessed on August 8, 2021.\nWongso et al. Page 12 of 16\nof our models used the same tokenizer, the closest would be Sundanese BERT’s\ntokenizer which was based on WordPiece [ 35]. Both tokenizers had about the same\nnumber of tokens, namely 30,521 and 30,522 tokens for the Indonesian and Sun-\ndanese BERT, respectively. The respective vocabularies were t hen compared and\noverlapping tokens were counted, to which we found only 12,935 overlapp ing to-\nkens out of roughly 30,000 tokens. Since there are only less than half overlap ping\ntokens out of the entire vocabulary of both tokenizers, this possibili ty seemed quite\nunlikely.\nOn the ﬂip side, it may just be that the Indonesian language is very muc h related\nto the Sundanese language by design. Although the measure of similarity be tween\nthese two languages isn’t trivial, it should be understood that the I ndonesian lan-\nguage was created with the inﬂuence of regional languages like Sundanese, wh ich\nis the third most spoken language in the nation [\n13]. Likewise, both languages\nstem from the same language family of Malayo-Sumbawan, a subgroup of Malayo-\nPolynesian languages [\n41]. Therefore, despite the lack of direct word overlaps as\nevident in the comparison of tokens, the two languages may exhibit an int rinsically\nsimilar linguistic structure which IndoBERT is able to model as we ll.\nWe suspect that the IndoBERT model [ 30] may hence be applicable as an alter-\nnative to the multilingual models for the case of regional languages in Indon esia,\nwhich are strongly related to the national language.\nLimitations and Bias\nPrevious studies [\n42, 43] have indicated social biases present in large language mod-\nels like BERT [ 6] and OpenAI GPT-3 [ 8]. This may include bias towards a certain\ngender, ethnicity, or beliefs. In this section, we aim to empiric ally test whether our\nSundanese models exhibit biased behavior towards a certain gender . Our Sundanese\nRoBERTa language model was tested for this purpose.\nAlthough the Sundanese language does not have gender-speciﬁc pronouns, u nlike\nEnglish and Chinese, daily informal texts found in social media web sites may exhibit\nstereotypical occupations and/or roles found in the local Indonesian cul ture, as\npointed out in [ 44]. This bias may be found in the pre-training corpus of our language\nmodels, especially noting that it does consist of common crawl data, i. e., public data\navailable on the internet. Hence, although our models are relatively pe rformant in\nterms of solving downstream tasks, they do carry over whatever intr insic biases are\nfound in the original data.\nInspired by [ 43], template prompts in the form of \"[NOUN] [VERB] [MASK].\",\nwere prepared in Sundanese. For instance, [VERB] is replaced by verbs like\nsaurang (is a/an), damel salaku (works as a/an), ngalakukeun (do/does), etc., while\n[NOUN] is replaced by gender-speciﬁc nouns and honoriﬁcs like lalaki (man), aw´ ew´ e\n(woman), bapak (mister), ibu (ma’am),saudara (male sibling), saudari (female sib-\nling), etc. As a control, [NOUN] is also replaced by gender-neutral nouns like urang\n´ eta(that person), pagawe ´ eta(that employee), kakak (older sibling), and anjeun\n(gender-neutral pronoun).\nThen, these prompts were passed to the model for it to predict the masked token\nand subsequently, the top 10 predictions for each prompt were collect ed. 24 prompts\nwere prepared for each gender category, yielding a sum of 240 predictions in total\nWongso et al. Page 13 of 16\nper gender. Table 6 shows the seven most common predictions for each gender\ncategory.\nTable 6 Seven most common predictions for each gender category made by the Sundanese RoBERTa\nmodel. English equivalents of the predicted Sundanese toke ns are also provided.\nGender Prediction Prediction (in English) Frequency\nMale\nbapak father 14\nlalaki man 10\naw´ ew´ e woman 7\nibu mother 7\nconto example 5\nsato animal 5\natlit athlete 5\nFemale\naw´ ew´ e woman 12\nlalaki man 7\npikaseurieun funny 7\nibu mother 7\nconto example 5\natlit athlete 4\nSMP secondary school 4\nNeutral\ndokter doctor 5\nconto example 5\nguru teacher 4\njalma creature 4\nprofesional professional 3\nIndonesia Indonesia 3\nurang person 3\nThe gender-neutral prompts seem to generally return gender-free p redictions.\nMoreover, they mostly returned reasonable predictions such as bapak and ibu for\nmale and female respectively, while the rest remained relatively neutral descriptions\nof a person. Interestingly, the same results may appear in either gen der category\ndespite being speciﬁed of a speciﬁc gender in the prompt.\nDiscussion and future directions\nWhile the monolingual Sundanese language models we have pre-trained are gener-\nally on-par with large multilingual models, it may be beneﬁcial to add e ven more\npre-training data to ensure a constant state-of-the-art performance in downstream\nSundanese tasks. This could mean adding more and richer data from various Sun-\ndanese sources such as online news outlets, social media, and formal Sun danese\ndocuments.\nMoreover, to encourage more work and establish a robust benchmark for the ﬁeld\nof Sundanese language modeling, there is a need to create a benchmark li ke GLUE\n(General Language Understanding Evaluation) [\n45], such that a more diverse set of\ntasks are taken into consideration when measuring the performance of p re-trained\nmodels. However, this might be among the more diﬃcult set of issues d ue to the\nscarcity of labeled Sundanese data. An alternative would be to translate c losely\nrelated benchmark datasets, like IndoNLU [ 30], for instance.\nLikewise, the same pre-training scheme proposed in the previous se ction, Sun-\ndanese Language Model Pre-Training , could similarly be applied to other regional\nlanguages in Indonesia if a suﬃciently large pre-training corpus has be en collected.\n[19] recommended a minimum of a few hundred MB of pre-training text dat a if we\nwere to pre-train a Transformer-based language model like BERT [ 6].\nWongso et al. Page 14 of 16\nConclusion\nWe have pre-trained three diﬀerent Sundanese, Transformer-base d language models,\nnamely GPT-2, BERT, and RoBERTa, using limited pre-training data. Af terward,\nwe evaluated these models by ﬁne-tuning them to a downstream task of emotion\nclassiﬁcation of Sundanese tweets and found that our RoBERTa model signi ﬁcantly\nimproved the results of larger multilingual models due to the disc repancy in pre-\ntraining corpus size, while our BERT model performed slightly bet ter than XLM-\nRoBERTa. An investigation of social biases present in our language model was al so\nconducted, to which the model seems to return neutral results.\nAbbreviations\nBART: Bidirectional and Auto-Regressive Transformers; BE RT: Bidirectional Encoder Representations from\nTransformers; ELMo: Embeddings from Language Models; GELU : Gaussian Error Linear Unit; GLUE: General\nLanguage Understanding Evaluation; GPT: Generative Pre-T raining; GPU: Graphical Processing Units; GRU: Gated\nRecurrent Unit; LSTM: Long Short-Term Memory; MBERT: Multi lingual BERT; MLM: Masked Language\nModeling; NSP: Next Sentence Prediction; OSCAR: Open Super- large Crawled Aggregated Corpus; RNN: Recurrent\nNeural Network; RoBERTa: Robustly Optimized BERT Pre-traini ng Approach; SVC: C-Support Vector Classiﬁer;\nTF-IDF: Term Frequency-Inverse Document Frequency; TPU: T ensor Processing Units; ULMFiT: Universal\nLanguage Model Fine-Tuning; XLM: Cross-lingual Language M odel Pre-training.\nAcknowledgements\nWe would like to thank Bina Nusantara University for facilita ting and supporting this entire research process.\nAuthors’ contributions\nWW contributed as the research principal in this work as well as the technical issues. HL contributed to technical\nissues. DS advised all processes for this work. Regarding th e manuscript, WW, HL, and DS wrote and revised the\nmanuscript. All authors read and approved the ﬁnal manuscri pt.\nAuthors’ information\nWilson Wongso is a third-year undergraduate Computer Scien ce student from Bina Nusantara University, Indonesia.\nHis research interests include natural language processing , especially in the domain of low-resource languages and\nIndonesia-related languages.\nHenry Lucky is a faculty member of Bina Nusantara University, I ndonesia. He is currently pursuing the M.S. degree\nin computer science at Bina Nusantara University, Indonesia . His research interest includes stock prediction and\nnatural language processing, especially natural language generation.\nDerwin Suhartono is faculty member of Bina Nusantara Univers ity, Indonesia. He got his PhD degree in computer\nscience from Universitas Indonesia in 2018. His research fel ds are natural language processing. Recently, he is\ncontinually doing research in argumentation mining and per sonality recognition. He actively involves in Indonesia\nAssociation of Computational Linguistics (INACL), a nation al scientifc association in Indonesia. He has his\nprofessional memberships in ACM, INSTICC, and IACT. He also ta kes role as reviewer in several international\nconferences and journals.\nFunding\nAll of this work is fully supported by Bina Nusantara Universi ty.\nAvailability of data and materials\nThe datasets used for this study are available on request to t he corresponding author.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare that they have no competing interests.\nAuthor details\nComputer Science Department, School of Computer Science, B ina Nusantara University, Jakarta, 11840, Indonesia.\nWongso et al. Page 15 of 16\nReferences\n1. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.:\nAttention is all you need. arXiv preprint arXiv:1706.03762 (2017)\n2. Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning r epresentations by back-propagating errors. nature\n323(6088), 533–536 (1986)\n3. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation 9(8), 1735–1780 (1997)\n4. Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., Bengio, Y.: Learning\nPhrase Representations using RNN Encoder-Decoder for Stati stical Machine Translation (2014). 1406.1078\n5. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.: Improving language understanding by generative\npre-training (2018)\n6. Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: BERT: Pr e-training of Deep Bidirectional Transformers for\nLanguage Understanding (2019). 1810.04805\n7. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutsk ever, I.: Language models are unsupervised\nmultitask learners. OpenAI blog 1(8), 9 (2019)\n8. Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., D hariwal, P., Neelakantan, A., Shyam, P., Sastry,\nG., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., He nighan, T., Child, R., Ramesh, A., Ziegler, D.M.,\nWu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M. , Gray, S., Chess, B., Clark, J., Berner, C.,\nMcCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Lan guage Models are Few-Shot Learners (2020).\n2005.14165\n9. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy , O., Lewis, M., Zettlemoyer, L., Stoyanov, V.:\nRoberta: A robustly optimized bert pretraining approach. a rXiv preprint arXiv:1907.11692 (2019)\n10. Ruder, S.: Why You Should Do NLP Beyond English. http://ruder.io/nlp-beyond-english (2020)\n11. Rust, P., Pfeiﬀer, J., Vuli´ c, I., Ruder, S., Gurevych, I .: How Good is Your Tokenizer? On the Monolingual\nPerformance of Multilingual Language Models (2021). 2012.15613\n12. Virtanen, A., Kanerva, J., Ilo, R., Luoma, J., Luotolaht i, J., Salakoski, T., Ginter, F., Pyysalo, S.: Multilingual\nis not enough: BERT for Finnish (2019). 1912.07076\n13. Badan Pusat Statistik: Kewarganegaraan, Suku Bangsa, a gama, dan Bahasa Sehari-hari Penduduk Indonesia:\nHasil Sensus Penduduk 2010. http://www.bps.go.id/website/pdf_publikasi/watermark%20_\nKewarganegaraan,%20Suku%20Bangsa,%20Agama%20dan%20B ahasa_281211.pdf\n14. Ethnologue: What are the top 200 most spoken languages? S IL International, Dallas, TX, USA (2021).\nhttps://www.ethnologue.com/guides/ethnologue200\n15. Ministry of Education, R. Culture, (Indonesia), T.: DAT A BAHASA DI INDONESIA.\nhttps://petabahasa.kemdikbud.go.id/databahasa.php\n16. Haerudin, D.: The role of parents in sundanese language pr eservation. In: Proceedings of the 1st International\nConference on Innovation in Education (ICoIE 2018), pp. 27– 32. Atlantis Press, ??? (2019/01).\ndoi:10.2991/icoie-18.2019.7. https://doi.org/10.2991/icoie-18.2019.7\n17. Putra, O.V., Wasmanson, F.M., Harmini, T., Utama, S.N.: Su ndanese twitter dataset for emotion\nclassiﬁcation. In: 2020 International Conference on Compu ter Engineering, Network, and Intelligent Multimedia\n(CENIM) (CENIM 2020), Online (2020)\n18. Ortiz Su´ arez, P.J., Sagot, B., Romary, L.: Asynchronou s pipelines for processing huge corpora on medium to\nlow resource infrastructures. Leibniz-Institut f¨ ur Deut sche Sprache, Mannheim (2019).\ndoi:10.14618/ids-pub-9021. http://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215\n19. Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., We nzek, G., Guzm´ an, F., Grave, E., Ott, M.,\nZettlemoyer, L., Stoyanov, V.: Unsupervised cross-lingua l representation learning at scale. In: Proceedings of\nthe 58th Annual Meeting of the Association for Computationa l Linguistics, pp. 8440–8451. Association for\nComputational Linguistics, Online (2020). doi: 10.18653/v1/2020.acl-main.747.\nhttps://aclanthology.org/2020.acl-main.747\n20. Wenzek, G., Lachaux, M.-A., Conneau, A., Chaudhary, V., Guzm´ an, F., Joulin, A., Grave, E.: CCNet:\nExtracting high quality monolingual datasets from web craw l data. In: Proceedings of the 12th Language\nResources and Evaluation Conference, pp. 4003–4012. Europ ean Language Resources Association, Marseille,\nFrance (2020). https://aclanthology.org/2020.lrec-1.4 94\n21. Raﬀel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Ma tena, M., Zhou, Y., Li, W., Liu, P.J.: Exploring the\nlimits of transfer learning with a uniﬁed text-to-text tran sformer. Journal of Machine Learning Research\n21(140), 1–67 (2020)\n22. LeCun, Y., Bengio, Y., Hinton, G.: Deep learning. nature 521(7553), 436–444 (2015)\n23. Howard, J., Ruder, S.: Universal language model ﬁne-tuni ng for text classiﬁcation. arXiv preprint\narXiv:1801.06146 (2018)\n24. Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., Zettlemoyer, L.: Deep contextualized\nword representations. In: Proc. of NAACL (2018)\n25. Cahyono, Y., Saprudin, S.: Analisis sentiment tweets be rbahasa sunda menggunakan naive bayes classiﬁer\ndengan seleksi feature chi squared statistic. Jurnal Infor matika Universitas Pamulang 4(3), 87–94 (2019)\n26. Sutedi, A., Kurniadi, D., Baswardono, W.: Sundanese lan guage level detection using rule-based classiﬁcation:\nCase studies on twitter\n27. Lample, G., Conneau, A.: Cross-lingual language model p retraining. arXiv preprint arXiv:1901.07291 (2019)\n28. Liu, Y., Gu, J., Goyal, N., Li, X., Edunov, S., Ghazvininej ad, M., Lewis, M., Zettlemoyer, L.: Multilingual\nDenoising Pre-training for Neural Machine Translation (202 0). 2001.08210\n29. Cahyawijaya, S., Winata, G.I., Wilie, B., Vincentio, K. , Li, X., Kuncoro, A., Ruder, S., Lim, Z.Y., Bahar, S.,\nKhodra, M.L., Purwarianti, A., Fung, P.: IndoNLG: Benchmark and Resources for Evaluating Indonesian\nNatural Language Generation (2021). 2104.08200\n30. Wilie, B., Vincentio, K., Winata, G.I., Cahyawijaya, S. , Li, X., Lim, Z.Y., Soleman, S., Mahendra, R., Fung, P.,\nBahar, S., Purwarianti, A.: IndoNLU: Benchmark and Resource s for Evaluating Indonesian Natural Language\nWongso et al. Page 16 of 16\nUnderstanding (2020). 2009.05387\n31. Nguyen, D.Q., Nguyen, A.T.: PhoBERT: Pre-trained languag e models for Vietnamese (2020). 2003.00744\n32. Lowphansirikul, L., Polpanumas, C., Jantrakulchai, N., Nutanong, S.: WangchanBERTa: Pretraining\ntransformer-based Thai Language Models (2021). 2101.09635\n33. Hendrycks, D., Gimpel, K.: Gaussian Error Linear Units (G ELUs) (2020). 1606.08415\n34. Sennrich, R., Haddow, B., Birch, A.: Neural machine transl ation of rare words with subword units. arXiv\npreprint arXiv:1508.07909 (2015)\n35. Schuster, M., Nakajima, K.: Japanese and korean voice sea rch. In: 2012 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pp. 5149 –5152 (2012). IEEE\n36. Loshchilov, I., Hutter, F.: Decoupled weight decay regul arization. arXiv preprint arXiv:1711.05101 (2017)\n37. Heek, J., Levskaya, A., Oliver, A., Ritter, M., Rondepier re, B., Steiner, A., van Zee, M.: Flax: A neural network\nlibrary and ecosystem for JAX (2020). https://github.com/google/flax\n38. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J. , Chanan, G., Killeen, T., Lin, Z., Gimelshein, N.,\nAntiga, L., et al.: Pytorch: An imperative style, high-perf ormance deep learning library. arXiv preprint\narXiv:1912.01703 (2019)\n39. Sun, C., Qiu, X., Xu, Y., Huang, X.: How to Fine-Tune BERT for Text Classiﬁcation? (2020). 1905.05583\n40. Kudo, T., Richardson, J.: SentencePiece: A simple and la nguage independent subword tokenizer and\ndetokenizer for Neural Text Processing (2018). 1808.06226\n41. Adelaar, A.: Malayo-sumbawan. Oceanic Linguistics 44(2), 357–388 (2005)\n42. Bhardwaj, R., Majumder, N., Poria, S.: Investigating Gen der Bias in BERT (2020). 2009.05021\n43. Kurita, K., Vyas, N., Pareek, A., Black, A.W., Tsvetkov, Y .: Measuring Bias in Contextualized Word\nRepresentations (2019). 1906.07337\n44. Mubarok, Y.: Representation of women in the sundanese pr overbs. IJASOS- International E-journal of Advances\nin Social Sciences, 205–205 (2017). doi: 10.18769/ijasos.309677\n45. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., Bowma n, S.R.: Glue: A multi-task benchmark and analysis\nplatform for natural language understanding. arXiv prepri nt arXiv:1804.07461 (2018)"
}