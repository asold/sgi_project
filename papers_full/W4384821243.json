{
  "title": "Evaluating Capabilities of Large Language Models: Performance of GPT4 on Surgical Knowledge Assessments",
  "url": "https://openalex.org/W4384821243",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4222928433",
      "name": "Brendin R Beaulieu-Jones",
      "affiliations": [
        "Harvard University",
        "Beth Israel Deaconess Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A3003507206",
      "name": "Sahaj Shah",
      "affiliations": [
        "Commonwealth Medical College"
      ]
    },
    {
      "id": "https://openalex.org/A3043262478",
      "name": "Margaret T. Berrigan",
      "affiliations": [
        "Beth Israel Deaconess Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2278473992",
      "name": "Jayson S. Marwaha",
      "affiliations": [
        "National Taiwan University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A4208253587",
      "name": "◯shuo-Lun Lai",
      "affiliations": [
        "National Taiwan University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A3191726181",
      "name": "Gabriel A Brat",
      "affiliations": [
        "Harvard University",
        "Beth Israel Deaconess Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A4222928433",
      "name": "Brendin R Beaulieu-Jones",
      "affiliations": [
        "Harvard University",
        "Beth Israel Deaconess Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A3003507206",
      "name": "Sahaj Shah",
      "affiliations": [
        "Commonwealth Medical College"
      ]
    },
    {
      "id": "https://openalex.org/A3043262478",
      "name": "Margaret T. Berrigan",
      "affiliations": [
        "Beth Israel Deaconess Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2278473992",
      "name": "Jayson S. Marwaha",
      "affiliations": [
        "National Taiwan University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A4208253587",
      "name": "◯shuo-Lun Lai",
      "affiliations": [
        "National Taiwan University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A3191726181",
      "name": "Gabriel A Brat",
      "affiliations": [
        "Beth Israel Deaconess Medical Center",
        "Harvard University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3127622810",
    "https://openalex.org/W2980542546",
    "https://openalex.org/W3024173558",
    "https://openalex.org/W2805089815",
    "https://openalex.org/W2994308387",
    "https://openalex.org/W2952726619",
    "https://openalex.org/W3004650426",
    "https://openalex.org/W3110784362",
    "https://openalex.org/W4306247339",
    "https://openalex.org/W2794518994",
    "https://openalex.org/W2971361125",
    "https://openalex.org/W2790209545",
    "https://openalex.org/W4288926427",
    "https://openalex.org/W2996280595",
    "https://openalex.org/W3215322027",
    "https://openalex.org/W3201437744",
    "https://openalex.org/W2944958482",
    "https://openalex.org/W3109246949",
    "https://openalex.org/W4308432230",
    "https://openalex.org/W4365601249",
    "https://openalex.org/W4322626024",
    "https://openalex.org/W4319460874",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4323920689",
    "https://openalex.org/W4360840406",
    "https://openalex.org/W4353016766",
    "https://openalex.org/W4322622443",
    "https://openalex.org/W4360938283",
    "https://openalex.org/W4321459182",
    "https://openalex.org/W4322500537",
    "https://openalex.org/W4323350039",
    "https://openalex.org/W4327946446",
    "https://openalex.org/W4322208207",
    "https://openalex.org/W4362521774",
    "https://openalex.org/W4321436564",
    "https://openalex.org/W4321351832",
    "https://openalex.org/W4323050332",
    "https://openalex.org/W4361284497",
    "https://openalex.org/W4324387439",
    "https://openalex.org/W4321435202",
    "https://openalex.org/W4324308091",
    "https://openalex.org/W2062321498",
    "https://openalex.org/W1978029960",
    "https://openalex.org/W4386045865"
  ],
  "abstract": "Abstract Background Artificial intelligence (AI) has the potential to dramatically alter healthcare by enhancing how we diagnosis and treat disease. One promising AI model is ChatGPT, a large general-purpose language model trained by OpenAI. The chat interface has shown robust, human-level performance on several professional and academic benchmarks. We sought to probe its performance and stability over time on surgical case questions. Methods We evaluated the performance of ChatGPT-4 on two surgical knowledge assessments: the Surgical Council on Resident Education (SCORE) and a second commonly used knowledge assessment, referred to as Data-B. Questions were entered in two formats: open-ended and multiple choice. ChatGPT output were assessed for accuracy and insights by surgeon evaluators. We categorized reasons for model errors and the stability of performance on repeat encounters. Results A total of 167 SCORE and 112 Data-B questions were presented to the ChatGPT interface. ChatGPT correctly answered 71% and 68% of multiple-choice SCORE and Data-B questions, respectively. For both open-ended and multiple-choice questions, approximately two-thirds of ChatGPT responses contained non-obvious insights. Common reasons for inaccurate responses included: inaccurate information in a complex question (n=16, 36.4%); inaccurate information in fact-based question (n=11, 25.0%); and accurate information with circumstantial discrepancy (n=6, 13.6%). Upon repeat query, the answer selected by ChatGPT varied for 36.4% of inaccurate questions; the response accuracy changed for 6/16 questions. Conclusion Consistent with prior findings, we demonstrate robust near or above human-level performance of ChatGPT within the surgical domain. Unique to this study, we demonstrate a substantial inconsistency in ChatGPT responses with repeat query. This finding warrants future consideration and presents an opportunity to further train these models to provide safe and consistent responses. Without mental and/or conceptual models, it is unclear whether language models such as ChatGPT would be able to safely assist clinicians in providing care.",
  "full_text": "Evaluating Capabilities of Large Language Models: Performance of GPT4 on Surgical \nKnowledge Assessments  \nBrendin R Beaulieu-Jones, MD MBA1,2; Sahaj Shah, BS3; Margaret T Berrigan, MD1; Jayson S \nMarwaha, MD MBI4; Shuo-Lun Lai, MD4; Gabriel A Brat, MD, FACS, MPH1-2 \n \n1Department of Surgery, Beth Israel Deaconess Medical Center, Boston, MA \n2Department of Biomedical Informatics, Harvard Medical School, Boston, MA \n3Geisinger Commonwealth School of Medicine, Scranton, PA \n4Division of Colorectal Surgery, National Taiwan University Hospital, Taipei, Taiwan \n \nDr. Beaulieu-Jones is supported by National Library of Medicine/NIH grant [T15LM007092]  \n \nDisclosure Information: Nothing to disclose.  \n \nCorresponding Author:  \nGabriel A Brat, MD, FACS, MPH \nDepartment of Surgery, Beth Israel Deaconess Medical Center \nDepartment of Biomedical Informatics, Harvard Medical School \n110 Francis Street, Suite 2G, Boston, MA 02215 \n \nKeywords: ChatGPT; artificial intelligence; language models; surgical education; surgery \n \nShort Title: Performance of ChatGPT-4 on Surgical Knowledge Assessments \n \n  \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 24, 2023. ; https://doi.org/10.1101/2023.07.16.23292743doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nAbstract \nBackground: Artificial intelligence (AI) has the potential to dramatically alter healthcare by \nenhancing how we diagnosis and treat disease. One promising AI model is ChatGPT, a large \ngeneral-purpose language model trained by OpenAI.  The chat interface has shown robust, \nhuman-level performance on several professional and academic benchmarks. We sought to probe \nits performance and stability over time on surgical case questions. \n \nMethods: We evaluated the performance of ChatGPT-4 on two surgical knowledge assessments: \nthe Surgical Council on Resident Education (SCORE) and a second commonly used knowledge \nassessment, referred to as Data-B. Questions were entered in two formats: open-ended and \nmultiple choice. ChatGPT output were assessed for accuracy and insights by surgeon evaluators. \nWe categorized reasons for model errors and the stability of performance on repeat encounters.  \n \nResults: A total of 167 SCORE and 112 Data-B questions were presented to the ChatGPT \ninterface. ChatGPT correctly answered 71% and 68% of multiple-choice SCORE and Data-B \nquestions, respectively. For both open-ended and multiple-choice questions, approximately two-\nthirds of ChatGPT responses contained non-obvious insights. Common reasons for inaccurate \nresponses included: inaccurate information in a complex question (n=16, 36.4%); inaccurate \ninformation in fact-based question (n=11, 25.0%); and accurate information with circumstantial \ndiscrepancy (n=6, 13.6%). Upon repeat query, the answer selected by ChatGPT varied for 36.4% \nof inaccurate questions; the response accuracy changed for 6/16 questions. \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 24, 2023. ; https://doi.org/10.1101/2023.07.16.23292743doi: medRxiv preprint \nConclusion:  Consistent with prior findings, we demonstrate robust near or above human-level \nperformance of ChatGPT within the surgical domain. Unique to this study, we demonstrate a \nsubstantial inconsistency in ChatGPT responses with repeat query. This finding warrants future \nconsideration and presents an opportunity to further train these models to provide safe and \nconsistent responses. Without mental and/or conceptual models, it is unclear whether language \nmodels such as ChatGPT would be able to safely assist clinicians in providing care. \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 24, 2023. ; https://doi.org/10.1101/2023.07.16.23292743doi: medRxiv preprint \nBackground \nArtificial intelligence (AI) models have the potential to dramatically alter healthcare by \nenhancing how we diagnosis and treat disease. These models could lead to increased efficiency, \nimproved accuracy and personalized patient care. Successful healthcare-related applications have \nbeen widely reported.1–10 Within surgery, machine learning approaches that include natural \nlanguage processing, computer vision, and reinforcement learning have each shown promise to \nadvance care.1,11–14 Still, despite the promise of AI to revolutionize healthcare, its use within the \nfield is markedly limited compared to other industries. The severe implications of errors and \nempathy concerns regarding the use of AI in healthcare have led to cautious adoption.7,11,15–18 \n \nOne promising recent model for use in healthcare is ChatGPT, a publicly-available, large \nlanguage model trained by OpenAI.19 Released in November 2022, ChatGPT received \nunprecedented attention,20 given its notable performance across a range of medical and non-\nmedical domains.21 ChatGPT has shown robust, human-level performance on several \nprofessional and academic benchmarks, including a simulated bar exam, the graduate record \nexamination (GRE), numerous Advanced Placement (AP) examinations, and the Advanced \nSommelier knowledge assessment.19 With regard to medical knowledge, an earlier version of \nChatGPT was shown to perform at or near the passing threshold of 60% accuracy on the United \nStates Medical Licensing Exam (USMLE).22,23 In addition, ChatGPT has demonstrated robust \nperformance on knowledge assessments in family medicine,24 neurosurgery,25 hepatology,26 and \na combination of all major medical specialties.27 Moreover, ChatGPT has shown promise as a \nclinical decision support tool in radiology,28 pathology,29 and orthodontics.30 ChatGPT has also \nperformed valuable clinical tasks,31–34 such as writing patient clinic letters, composing inpatient \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 24, 2023. ; https://doi.org/10.1101/2023.07.16.23292743doi: medRxiv preprint \ndischarge summaries, suggesting cancer screening, and conveying patient education.35 Lastly, \nseveral studies have highlighted the potential impact of ChatGPT on medical education and \nresearch, with roles ranging from supporting nursing education to advancing data analysis and \nstreamlining the writing of scientific publications.32,36–38 The emergence of ChatGPT has \nreignited interest in exploring AI applications in healthcare; however, it has also provoked \nnumerous concerns, regarding bias, reliability, privacy, and governance.21,26,32,36–41 \n \nIn the current study, we evaluate ChatGPT-4’s performance on two commonly used surgical \nknowledge assessments: The Surgical Council on Resident Education (SCORE) curriculum and \na second case-based question bank for general surgery residents and practicing surgeons – which \nis referred to as Data-B and not identified due to copyright restrictions. SCORE is an educational \nresource and self-assessment used by many US residents throughout residency training.42–45 \nData-B is principally designed for graduating surgical residents and fully-trained surgeons in \npreparation for the American Board of Surgery (ABS) Qualifying Exam (QE). These \nassessments were selected as their content represents the knowledge expected of surgical \nresidents and board-certified surgeons, respectively. As such, it was thought that Form-B, while \nbased on the same content area as SCORE, should include more higher-order management or \nmulti-step reasoning questions, and that we may observe differential ChatGPT performance. The \nperformance of ChatGPT on each of these assessments may provide important insights regarding \nChatGPT-4’s capabilities at this point in time. Perhaps more importantly, in addition to assessing \nperformance, this study investigates reasons for ChatGPT errors and assesses its performance on \nrepeat queries. This latter objective represents a significant contribution to our current \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 24, 2023. ; https://doi.org/10.1101/2023.07.16.23292743doi: medRxiv preprint \nunderstanding of large language models – and a critical domain for research for safe and \neffective use of AI in healthcare.  \n \nMethods  \nArtificial Intelligence \nChatGPT (Open AI, San Francisco, CA) is a publicly-available, subscription-based AI chatbot \nthat first launched in November 2022. It was initially derived from GPT-3 (Generative Pretrained \nTransformer) language models, which are pre-trained transformer models designed primarily to \ngenerate text via next word prediction. To improve performance for ChatGPT, initial GPT-3 \nmodels were further trained using a combination of supervised and reinforcement learning \ntechniques.50 In particular, ChatGPT was trained using Reinforcement Learning from Human \nFeedback (RLHF), in which a reward model is trained from human feedback. To create a reward \nmodel, a dataset of comparison data was created, which was comprised of two or more model \nresponses ranked by quality by a human AI trainer. This data could then be used to fine-tune the \nmodel using Proximal Policy Optimization.46 \n \nChatGPT-PLUS is the latest development from Open-AI and employs GPT-4, which is the \nfourth iteration of the GPT family of language models.19 Details regarding the architecture and \ndevelopment of GPT-4 are not publicly available. It is generally accepted that GPT-4 was trained \nin a similar fashion as GPT-3 via RLHF. While specific technical details are unknown, Open AI \nstates in their technical report that one of the main goals in developing GPT-4 was to improve \nthe language model’s ability to understand and generate natural text, particularly in complex and \nnuanced scenarios. The report highlights improved performance of GPT-4, relative to GPT-3.5. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 24, 2023. ; https://doi.org/10.1101/2023.07.16.23292743doi: medRxiv preprint \nFor example, GPT-4 passed a simulated bar exam with a score in the 90th percentile, whereas \nGPT-3.5 achieved a score in the 10th percentile of exam takers. GPT-4 was officially released on \nMarch 13, 2023 and is currently available via the ChatGPT Plus paid subscription.  \n \nInput Sources \nInput questions were derived from two commonly used surgical educational resources:   \n1. SCORE: The Surgical Council on Resident Education (SCORE) is a nonprofit \norganization established in 2004 by the principal organizations involved in surgical \neducation in the United States, including the American Board of Surgery (ABS) and the \nAssociation for Surgical Education (ASE). SCORE maintains a curriculum for surgical \ntrainees, which includes didactic educational content and more than 2400 multiple-choice \nquestions for self-assessment. A total of 175 self-assessment questions were obtained \nfrom the SCORE question bank. Access to the SCORE question bank was obtained \nthrough the research staff’s institutional access. SCORE was not part of the research team \nand did not participate in the study design and completion of research. Using existing \nfunctionality within SCORE, study questions were randomly selected from all topics, \nexcept systems-based practice; surgical professionalism and interpersonal communication \neducation; ethical issues in clinical surgery; biostatistics and evaluation of evidence; and \nquality improvement. Fellowship-level questions were not excluded from study inclusion. \nQuestions containing images were excluded from analysis. After exclusion, a total of 167 \nquestions from SCORE were included in the study analysis.  \n2. Data-B: Data-B is an educational resource for practicing surgeons and senior surgical \ntrainees, which includes case-based, multiple choice questions across a range of general \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 24, 2023. ; https://doi.org/10.1101/2023.07.16.23292743doi: medRxiv preprint \nsurgical domains, including endocrine, vascular, abdomen, alimentary tract, breast, head \nand neck, oncology, perioperative care, surgical critical care, and skin/soft issue. A total \nof 120 questions were randomly selected for inclusion in the study. Questions containing \nimages were excluded from analysis. After exclusion, 119 questions were included.  \n \nEncoding \nFor input into ChatGPT, all selected questions were formatted two ways: \n1. Open-ended (OE) prompting: Constructed by removing all answer choices and translating \nthe existing question into an open-ended phrase. Examples include: “What is the best \ninitial treatment for this patient?”; “For a patient with this diagnosis and risk factor, what \nis the most appropriate operative approach?”; or “What is the most appropriate initial \ndiagnostic test to determine the cause of this patient’s symptoms?” \n2. Multiple choice (MC) single answer without forced justification: Created by replicating \nthe original SCORE or Data-B question verbatim. Examples include: “After appropriate \ncardiac workup, which of the following surgeries should be performed?”; “Which of the \nfollowing laboratory values would most strongly suggest the presence of ischemic bowel \nin this patient?”; or “Which of the following options is the best next step in treatment?” \n \nOpen-ended prompts were deliberately varied to avoid systemic errors. For each entry, a new \nchat session was started in ChatGPT to avoid potential bias. To reiterate, all questions were \ninputted twice (once with open-ended prompting and once via the multiple choice format). \nPresenting questions to ChatGPT in two formats provided some insight regarding the capacity of \na predictive language model to generate accurate, domain-specific responses without prompting.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 24, 2023. ; https://doi.org/10.1101/2023.07.16.23292743doi: medRxiv preprint \n \nAssessment \nOutputs from ChatGPT-4 were assessed for Accuracy, Internal Concordance and Insight by two \nsurgical residents using the criteria outlined by Kung et al. in their related work on the \nperformance of ChatGPT on the USMLE exam.23 Response accuracy (i.e., correctness) was \nassessed based on the provided solution and explanation by SCORE and Data-B, respectively. \nInternal response concordance refers to the internal validity and consistency of ChatGPT’s \noutput—specifically whether the explanation affirms the answer and negates remaining choices \nwithout contradiction. Insight refers to text that is non-definitional, non-obvious and/or valid.23 \n \nEach reviewer adjudicated 100 SCORE questions and 70 Data-B questions, with 30% and 28% \noverlap, respectively. For overlapping questions, residents were blinded to each other’s \nassessment. Interrater agreement was evaluated by computing the Cohen kappa (k) statistic for \neach question type (Supplemental Table 1).  \n \nFor the combined set of 167 SCORE questions included in the study, the median performance for \nall human SCORE users was 65%, as reported in the SCORE dashboard. Reference data for \nData-B is not available, preventing an exact comparison between ChatGPT and surgeon users.  \n \nIn addition, we reviewed all inaccurate ChatGPT responses to multiple choice SCORE questions \nto determine and classify the reason for the incorrect output. The classification system for \ninaccurate responses was created by study personnel and designations were made by consensus. \nReasons for inaccurate responses included: inaccurate information in complex question, \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 24, 2023. ; https://doi.org/10.1101/2023.07.16.23292743doi: medRxiv preprint \ninaccurate information in fact-based question; accurate information, circumstantial discrepancy; \ninability to differentiate relative importance of information; imprecise application of detailed \ninformation; and imprecise application of general information. A description of each error type, \nas well as representative examples are shown in Table 1.  \n \nTo further assess the performance and reproducibility of GPT-4, all responses to SCORE \nquestions (MC format) that were initially deemed inaccurate were re-queried. Second, ChatGPT \nresponses were compared to the initial output to determine if the answer response changed and if \nit changed, whether the response was now accurate or if it remained inaccurate.  \n \nResults \nAccuracy of ChatGPT Responses \nA total of 167 SCORE and 112 Data-B questions were presented to ChatGPT. The accuracy of \nChatGPT responses for OE and MC SCORE and Data-B questions is presented in Figure 1. \nChatGPT correctly answered 71% and 68% of MC SCORE and Data-B questions, respectively. \nThe proportion of accurate responses for OE questions was lower than for MC, particularly for \nSCORE questions, which is largely due to an increase in responses that were deemed \nindeterminate by study adjudicators in the setting of the open-ended format.  \n \nInternal Response Concordance of ChatGPT Responses \nInternal Concordance was adjudicated by review of the entire ChatGPT response (Table 2). \nOverall internal response concordance was very high: 85.6% and 100% for OE SCORE and \nData-B questions, respectively, and 88.6% and 97.3% for MC SCORE and Data-B questions.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 24, 2023. ; https://doi.org/10.1101/2023.07.16.23292743doi: medRxiv preprint \n \nAmong OE SCORE questions, internal response concordance was also assessed by accuracy \nsubgroup (Figure 2). Concordance was nearly 100% (79/80) for accurate responses. Internally \ndiscordant responses were more frequently observed for inaccurate responses (33%, 31/75).  \n \nInsights within ChatGPT Responses \nFor both OE and MC questions, approximately two-thirds of ChatGPT responses contained \nnonobvious insights (Table 2). Insights were more frequently observed for OE questions \n(SCORE: 66.5% versus 63.5%; Data-B: 77.7% versus 62.7%).  \n \nClassification of Inaccurate ChatGPT Responses to MC SCORE Questions \nReasons for inaccurate responses are shown (Table 3). The most common reasons were: \ninaccurate information in a complex question (36.4%); inaccurate information in fact-based \nquestion (25.0%); and accurate information, circumstantial discrepancy (13.6%).  \n \nOutcome of Repeat Question for Initially Inaccurate ChatGPT Responses \nFor all inaccurate ChatGPT responses to MC SCORE questions, the exact MC SCORE question \nwas re-presented to the ChatGPT on a separate encounter, using a new chat. The accuracy of the \nresponse was assessed as prior. In total, the answer selected by ChatGPT varied between \niterations for 16 questions (36.4% of inaccurate questions). The response remained inaccurate in \n10/16 questions and was accurate on the second encounter for 6/16 questions. No change in the \nselected MC answer was observed in nearly two-thirds of cases (n=28, 63.6%).  \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 24, 2023. ; https://doi.org/10.1101/2023.07.16.23292743doi: medRxiv preprint \nDiscussion \nTo assess ChatGPT’s capabilities within the surgical domain, we assessed the performance of \nChatGPT-4 on two surgical knowledge self-assessments. Consistent with prior findings in other \ndomains, ChatGPT exhibited robust accuracy and internal concordance, near or above human-\nlevel performance. The study highlights the accuracy of ChatGPT within a highly specific and \nsophisticated field without specific training or fine-tuning in the domain. The findings also \nunderscore some of the current limitations of AI including variable performance on the same task \nand unpredictable gaps in the model’s capabilities. In addition, the non-tiered performance of \nChatGPT on SCORE and Data-B suggests a distinctiveness between human knowledge and/or \nlearning and the development of language models. Nonetheless, the robust performance of a \nlanguage model within the surgical domain – and potential to enhance its performance domain-\nspecific training (i.e., high-yield surgical literature) – highlights its potential value to support and \nadvance human tasks in clinical decision-making and healthcare. While human context and high-\nlevel conceptual models are needed for certain decisions and tasks within surgery, understanding \nthe performance large language models will direct their future development such that AI tools \nare complementarily positioned within healthcare, offloading extraneous cognitive demands.  \n \nForemost, within the surgical domain, ChatGPT demonstrated near or above human-level \nperformance, with an accuracy of 71% and 68% on MC SCORE and Data-B questions, \nrespectively. This is consistent with ChatGPT performance in other general and specific \nknowledge domains, including law, verbal reasoning, mathematics, and medicine.19,23 The \ncurrent findings are consistent with a study by Hopkins et al., in which ChatGPT was tested on \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 24, 2023. ; https://doi.org/10.1101/2023.07.16.23292743doi: medRxiv preprint \nand achieved near human-level performance on a subset of questions from the Congress of \nNeurological Surgeons (CNS) Self-Assessment Neurosurgery (SANS).25  \n \nThe current study utilizes two knowledge assessments, which are generally accepted to be tiered \nin difficulty, with SCORE principally designed for residents and Data-B targeted for senior \nresidents and board-certified attending general surgeons. This design provides additional insight \ninto the performance of ChatGPT relative to humans; we would anticipate that ChatGPT would \nperform superiorly on SCORE relative to Data-B. However, the near equivocal relative \nperformance of ChatGPT on SCORE and Data-B suggests that its capabilities do not parallel \nthose of surgical trainees.  Learners progressively attain greater layers of context and \nunderstanding to expand their knowledge. A predictive language processing model such as \nChatGPT does not improve in a similar manner, given the nature of its corpus of information and \nreinforcement-based training. It is an informal observation that SCORE questions often require \nmore precise delineation of similar answer choices (e.g., distal pancreatectomy with splenectomy \nversus distal pancreatectomy alone), and Data-B generally requires a broader knowledge set to \nanswer each question. The near equivocal performance suggests that a probabilistic algorithm \nlike ChatGPT can function at a high level in both tasks, but it also highlights that the mental and \nconceptual models that providers use to develop their expertise should not be attributed to these \nmodels.  Mental models have acknowledged limitations, but they allow physicians to think \nbroadly during clinical encounters where information is limited. Importantly, such differences \nmay lead to errors by the language model that experienced providers would consider basic or \nunlikely given the way we learn. Thus, it is still too early to assume language models can safely \nassist clinicians in providing care. Future research into how large language models perform, with \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 24, 2023. ; https://doi.org/10.1101/2023.07.16.23292743doi: medRxiv preprint \nspecific attention to end-points beyond accuracy, is needed to direct further development and \napplication of language models and related AI in surgery and healthcare.  \n \nTwo additional findings warrant consideration. First, our analysis highlighted the kind of errors \nthat ChatGPT makes on surgical knowledge questions. In 11 of 44 inaccurate responses (25%), \nthe incorrect response related to a straightforward, fact-based query (e.g., What is the second \nmost common location of blunt thoracic vascular injury after the aorta?). Second, we observed \ninconsistencies in ChatGPT responses. When erroneous responses were re-presented to the \nlanguage model interface, output varied in one-third of instances, and responses were different \nand incorrect (e.g. select another multiple choice response) in two-thirds. These two findings \nhighlight a substantial limitation of current predictive language models when the response \nchanges over several days. Future performance metrics should include a measure of consistency \nas well as initial capability. Fine-tuning to the specific domain may improve the confidence of \nthe model and subsequent consistency; this type of finding underscores the importance of \nimplementing AI tools in a complementary fashion in healthcare, given the high costs of errors. \n \nTo our knowledge, this is the first study testing the performance of ChatGPT on knowledge \nassessments over multiple instances. The extraordinary results of a general-purpose model like \nChatGPT highlight both the incredible opportunity that exists and the value of additional \ndomain-specific fine tuning and reinforcement learning. In particular, future research is needed \nto assess ChatGPT’s performance within clinical encounters, rather than standardized knowledge \nassessments. Large language models such as ChatGPT lack a conceptual model, and this is \nfundamentally different from how humans diagnose and treat, and may be a major limitation of \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 24, 2023. ; https://doi.org/10.1101/2023.07.16.23292743doi: medRxiv preprint \nChatGPT’s performance in clinical settings—as highlighted in a recent blog post by an \nemergency medicine physician who tested ChatGPT’s diagnostic capacity for a subset of recent \nclinical encounters.52 Without these mental or conceptual models, correct responses to \ndeterministic questions, like the questions within SCORE and Data-B, do not necessarily imply \nthat the model would be able to assist clinicians in providing care in its current form.  \n \nThe current study has notable limitations. First, a relatively small bank of questions was used, \nwhich may not accurately reflect the broader surgical knowledge domain. Second, the \nassessment of accuracy and internal concordance for the open-ended responses may be biased, \nbut we found significant inter-rater reliability to calm this concern. Third, and most importantly, \nit is possible that some of the questions and/or answers are available in some form online and \nmay allow the model to draw on previous answers. While the content is easily accessible online, \nthe specific questions are less likely to be available online as both SCORE and Data-B are not \nopen-source assessments. Finally, a metric of human performance on Data-B is not readily \navailable, though median performance is likely equivalent to SCORE, given the reported data on \nboth the American Board of Surgery In-Training and Qualifying Examinations. \n \nConclusion \nConsistent with prior findings, the current study demonstrates the robust performance of \nChatGPT within the surgical domain. Unique to this study, we demonstrate inconsistency in \nChatGPT responses and answer selections upon repeat query. This finding warrants future \nconsideration and demonstrates an opportunity for further research to develop tools for safe and \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 24, 2023. ; https://doi.org/10.1101/2023.07.16.23292743doi: medRxiv preprint \nreliable implementation in healthcare. Without mental models, it is unclear whether language \nmodels such as ChatGPT would be able to safely assist clinicians in providing care.   \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 24, 2023. ; https://doi.org/10.1101/2023.07.16.23292743doi: medRxiv preprint \nReferences \n1. Khalsa RK, Khashkhusha A, Zaidi S, Harky A, Bashir M. Artificial intelligence and cardiac \nsurgery during COVID-19 era. J Card Surg. 2021;36(5):1729-1733. doi:10.1111/JOCS.15417 \n2. Mehta N, Pandit A, Shukla S. Transforming healthcare with big data analytics and artificial \nintelligence: A systematic mapping study. J Biomed Inform. 2019;100. \ndoi:10.1016/J.JBI.2019.103311 \n3. Payrovnaziri SN, Chen Z, Rengifo-Moreno P, et al. Explainable artificial intelligence models \nusing real-world electronic health record data: a systematic scoping review. J Am Med \nInform Assoc JAMIA. 2020;27(7):1173-1185. doi:10.1093/JAMIA/OCAA053 \n4. Xiao C, Choi E, Sun J. Opportunities and challenges in developing deep learning models \nusing electronic health records data: a systematic review. J Am Med Inform Assoc JAMIA. \n2018;25(10):1419-1428. doi:10.1093/JAMIA/OCY068 \n5. Luh JY, Thompson RF, Lin S. Clinical Documentation and Patient Care Using Artificial \nIntelligence in Radiation Oncology. J Am Coll Radiol JACR. 2019;16(9 Pt B):1343-1346. \ndoi:10.1016/J.JACR.2019.05.044 \n6. Johnson SP, Wormer BA, Silvestrini R, Perdikis G, Drolet BC. Reducing Opioid Prescribing \nAfter Ambulatory Plastic Surgery With an Opioid-Restrictive Pain Protocol. Ann Plast Surg. \n2020;84(6S Suppl 5):S431-S436. doi:10.1097/SAP.0000000000002272 \n7. Makhni EC, Makhni S, Ramkumar PN. Artificial Intelligence for the Orthopaedic Surgeon: An \nOverview of Potential Benefits, Limitations, and Clinical Applications. J Am Acad Orthop \nSurg. 2021;29(6):235-243. doi:10.5435/JAAOS-D-20-00846 \n8. Hammouda N, Neyra JA. Can Artificial Intelligence Assist in Delivering Continuous Renal \nReplacement Therapy? Adv Chronic Kidney Dis. 2022;29(5):439-449. \ndoi:10.1053/J.ACKD.2022.08.001 \n9. McBee MP, Awan OA, Colucci AT, et al. Deep Learning in Radiology. Acad Radiol. \n2018;25(11):1472-1480. doi:10.1016/J.ACRA.2018.02.018 \n10. Rashidi HH, Tran NK, Betts EV, Howell LP, Green R. Artificial Intelligence and Machine \nLearning in Pathology: The Present Landscape of Supervised Methods. Acad Pathol. 2019;6. \ndoi:10.1177/2374289519873088 \n11. Hashimoto DA, Rosman G, Rus D, Meireles OR. Artificial Intelligence in Surgery: Promises \nand Perils. Ann Surg. 2018;268(1):70-76. doi:10.1097/SLA.0000000000002693 \n12. Mumtaz H, Saqib M, Ansar F, et al. The future of Cardiothoracic surgery in Artificial \nintelligence. Ann Med Surg 2012. 2022;80. doi:10.1016/J.AMSU.2022.104251 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 24, 2023. ; https://doi.org/10.1101/2023.07.16.23292743doi: medRxiv preprint \n13. Raffort J, Adam C, Carrier M, Lareyre F. Fundamentals in Artificial Intelligence for Vascular \nSurgeons. Ann Vasc Surg. 2020;65:254-260. doi:10.1016/J.AVSG.2019.11.037 \n14. Stumpo V, Staartjes VE, Regli L, Serra C. Machine Learning in Pituitary Surgery. Acta \nNeurochir Suppl. 2022;134:291-301. doi:10.1007/978-3-030-85292-4_33 \n15. Petch J, Di S, Nelson W. Opening the Black Box: The Promise and Limitations of Explainable \nMachine Learning in Cardiology. Can J Cardiol. 2022;38(2):204-213. \ndoi:10.1016/J.CJCA.2021.09.004 \n16. Jarrett D, Stride E, Vallis K, Gooding MJ. Applications and limitations of machine learning in \nradiation oncology. Br J Radiol. 2019;92(1100). doi:10.1259/BJR.20190001 \n17. Cheng JY, Abel JT, Balis UGJ, McClintock DS, Pantanowitz L. Challenges in the Development, \nDeployment, and Regulation of Artificial Intelligence in Anatomic Pathology. Am J Pathol. \n2021;191(10):1684-1692. doi:10.1016/J.AJPATH.2020.10.018 \n18. Sarno L, Neola D, Carbone L, et al. Use of artificial intelligence in obstetrics: not quite ready \nfor prime time. Am J Obstet Gynecol MFM. 2023;5(2). doi:10.1016/J.AJOGMF.2022.100792 \n19. OpenAI. GPT-4 Technical Report. Published online March 15, 2023. \n20. Zhang C, Zhang C, Li C, Qiao Y. One Small Step for Generative AI, One Giant Leap for AGI: A \nComplete Survey on ChatGPT in AIGC Era. ArXiv 1013140RG222478970883. Published \nonline April 4, 2023. \n21. Quick uptake of ChatGPT, and more - this week’s best science graphics. Nature. Published \nonline February 28, 2023. doi:10.1038/D41586-023-00603-2 \n22. Gilson A, Safranek CW, Huang T, et al. How Does ChatGPT Perform on the United States \nMedical Licensing Examination? The Implications of Large Language Models for Medical \nEducation and Knowledge Assessment. JMIR Med Educ. 2023;9. doi:10.2196/45312 \n23. Kung TH, Cheatham M, Medenilla A, et al. Performance of ChatGPT on USMLE: Potential for \nAI-assisted medical education using large language models. PLOS Digit Health. \n2023;2(2):e0000198. doi:10.1371/JOURNAL.PDIG.0000198 \n24. Morreel S, Mathysen D, Verhoeven V. Aye, AI! ChatGPT passes multiple-choice family \nmedicine exam. Med Teach. Published online 2023. doi:10.1080/0142159X.2023.2187684 \n25. Hopkins BS, Nguyen VN, Dallas J, et al. ChatGPT versus the neurosurgical written boards: a \ncomparative analysis of artificial intelligence/machine learning performance on \nneurosurgical board-style questions. J Neurosurg. Published online March 1, 2023:1-8. \ndoi:10.3171/2023.2.JNS23419 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 24, 2023. ; https://doi.org/10.1101/2023.07.16.23292743doi: medRxiv preprint \n26. Yeo YH, Samaan JS, Ng WH, et al. Assessing the performance of ChatGPT in answering \nquestions regarding cirrhosis and hepatocellular carcinoma. Clin Mol Hepatol. Published \nonline March 22, 2023. doi:10.3350/CMH.2023.0089 \n27. Johnson D, Goodman R, Patrinely J, et al. Assessing the Accuracy and Reliability of AI-\nGenerated Medical Responses: An Evaluation of the Chat-GPT Model. Res Sq. Published \nonline 2023. doi:10.21203/RS.3.RS-2566942/V1 \n28. Ismail A, Ghorashi NS, Javan R. New Horizons: The Potential Role of OpenAI’s ChatGPT in \nClinical Radiology. J Am Coll Radiol JACR. Published online March 2023. \ndoi:10.1016/J.JACR.2023.02.025 \n29. Sinha RK, Deb Roy A, Kumar N, Mondal H. Applicability of ChatGPT in Assisting to Solve \nHigher Order Problems in Pathology. Cureus. 2023;15(2). doi:10.7759/CUREUS.35237 \n30. Strunga M, Urban R, Surovková J, Thurzo A. Artificial Intelligence Systems Assisting in the \nAssessment of the Course and Retention of Orthodontic Treatment. Healthc Basel Switz. \n2023;11(5). doi:10.3390/HEALTHCARE11050683 \n31. Ali SR, Dobbs TD, Hutchings HA, Whitaker IS. Using ChatGPT to write patient clinic letters. \nLancet Digit Health. 2023;5(4). doi:10.1016/S2589-7500(23)00048-1 \n32. Sallam M. ChatGPT Utility in Healthcare Education, Research, and Practice: Systematic \nReview on the Promising Perspectives and Valid Concerns. Healthc Basel Switz. 2023;11(6). \ndoi:10.3390/HEALTHCARE11060887 \n33. Rao A, Pang M, Kim J, et al. Assessing the Utility of ChatGPT Throughout the Entire Clinical \nWorkflow. MedRxiv Prepr Serv Health Sci. Published online February 26, 2023. \ndoi:10.1101/2023.02.21.23285886 \n34. Haver HL, Ambinder EB, Bahl M, Oluyemi ET, Jeudy J, Yi PH. Appropriateness of Breast \nCancer Prevention and Screening Recommendations Provided by ChatGPT. Radiology. \nPublished online April 4, 2023:230424. doi:10.1148/RADIOL.230424 \n35. Hopkins AM, Logan JM, Kichenadasse G, Sorich MJ. Artificial intelligence chatbots will \nrevolutionize how cancer patients access information: ChatGPT represents a paradigm-shift. \nJNCI Cancer Spectr. 2023;7(2). doi:10.1093/JNCICS/PKAD010 \n36. Alkaissi H, McFarlane SI. Artificial Hallucinations in ChatGPT: Implications in Scientific \nWriting. Cureus. 2023;15(2). doi:10.7759/CUREUS.35179 \n37. Cascella M, Montomoli J, Bellini V, Bignami E. Evaluating the Feasibility of ChatGPT in \nHealthcare: An Analysis of Multiple Clinical and Research Scenarios. J Med Syst. 2023;47(1). \ndoi:10.1007/S10916-023-01925-4 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 24, 2023. ; https://doi.org/10.1101/2023.07.16.23292743doi: medRxiv preprint \n38. Thomas SP. Grappling with the Implications of ChatGPT for Researchers, Clinicians, and \nEducators. Issues Ment Health Nurs. 2023;44(3):141-142. \ndoi:10.1080/01612840.2023.2180982 \n39. Vaishya R, Misra A, Vaish A. ChatGPT: Is this version good for healthcare and research? \nDiabetes Metab Syndr. 2023;17(4):102744. doi:10.1016/J.DSX.2023.102744 \n40. Dahmen J, Kayaalp ME, Ollivier M, et al. Artificial intelligence bot ChatGPT in medical \nresearch: the potential game changer as a double-edged sword. Knee Surg Sports \nTraumatol Arthrosc Off J ESSKA. 2023;31(4). doi:10.1007/S00167-023-07355-6 \n41. Will ChatGPT transform healthcare? Nat Med. 2023;29(3). doi:10.1038/S41591-023-02289-\n5 \n42. American Board of Surgery. SCORE - Surgical Council on Resident Education. \nhttps://www.absurgery.org/default.jsp?scre_booklet. \n43. Bell RH. Surgical council on resident education: a new organization devoted to graduate \nsurgical education. J Am Coll Surg. 2007;204(3):341-346. \ndoi:10.1016/J.JAMCOLLSURG.2007.01.002 \n44. Klingensmith ME, Malangoni MA. SCORE provides residents with web-based curriculum for \ndeveloping key competencies. Bull Am Coll Surg. 2013;98(10):10-15. \n45. Moalem J, Edhayan E, Darosa DA, et al. Incorporating the SCORE curriculum and web site \ninto your residency. J Surg Educ. 2011;68(4):294-297. doi:10.1016/J.JSURG.2011.02.010 \n46. Gao L, Schulman J, Hilton J. Scaling Laws for Reward Model Overoptimization. Published \nonline October 19, 2022. \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 24, 2023. ; https://doi.org/10.1101/2023.07.16.23292743doi: medRxiv preprint \nFigure 1: Accuracy of ChatGPT Output for Open-Ended and Multiple-Choice Questions \n \n \n \nLegend: Surgical knowledge questions from SCORE and Data-B were presented to ChatGPT \nvia two formats: open-ended (OE; left sided and multiple-choice (MC; right side). ChatGPT’s \noutputs were assessed for accuracy by surgeon evaluators. A total of 167 SCORE and 112 Data-\nB questions were presented to the ChatGPT interface. ChatGPT correctly answered 71% and \n68% of multiple choice SCORE and Data-B questions, respectively.  \n \n \n \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 24, 2023. ; https://doi.org/10.1101/2023.07.16.23292743doi: medRxiv preprint \nFigure 2: Internal Concordance by Accuracy Subgroup among SCORE Questions \n \n \n \nLegend: SCORE questions were presented to ChatGPT via two formats: open-ended and \nmultiple-choice. ChatGPT’s outputs to open-ended SCORE questions were assessed for internal \nconcordance by accuracy subgroup. A total of 167 SCORE questions were presented to the \nChatGPT interface. Concordance was nearly 100% (79/80) for accurate responses. Internally \ndiscordant responses were more frequently observed for inaccurate responses (33%, 31/75).  \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 24, 2023. ; https://doi.org/10.1101/2023.07.16.23292743doi: medRxiv preprint \nTable 1: Classification of Error Type: Description and Examples \n \nError Type Description and Theoretical Example(s) of Error \nImprecise application of \ndetailed information \nDescription:  Answer selection was based on detailed clinical information, which \nwas applied imprecisely or inaccurately to the clinical context \n \nExample:  \n• Recommend medical management rather than surgery as first-line \ntreatment for specific diagnosis, which is accurate, unless symptoms are \nmedically-refractory, which is the case in the question  \nImprecise application of \ngeneral knowledge \nDescription: Answer selection was based on general knowledge, which was either \nincompletely accurate or out of scope given context of the question \n \nExample:  \n• Recommend against a secondary procedure in a child to avoid additional \nanesthesia and potential procedural complications  \nInability to differentiate relative \nimportance of information \nDescription: Answer selection was based on accurate information, but did not \ndelineate between more accurate options \n \nExample:  \n• Select a laboratory finding which is present in most patients with a \nspecific condition, when a more characteristic finding was intended  \nAccurate information; \ncircumstantial discrepancy \nDescription:  Response is based on accurate information, which is incorrect based \non question interpretation or other circumstantial factors that unlikely reflect \ncompetency of GPT \n \nExample:  \n• Select the cost-effective, first-line imaging, rather than the gold standard \nmechanism for diagnosis  \nInaccurate information in fact-\nbased question \nDescription: Response is based on inaccurate information in the context of a \nsingle-part, fact-based question \n \nExample:  \n• Incorrectly identify the second most common site of pathology \nInaccurate information in \ncomplex question \nDescription: Response is based on inaccurate information in the context of a \ncomplex clinical scenario or multi-part question \n \nExample:  \n• Inaccurate selection of most appropriate next step in patient with \nconstellation of symptoms and description of imaging \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 24, 2023. ; https://doi.org/10.1101/2023.07.16.23292743doi: medRxiv preprint \n Table 2: Accuracy, Internal Concordance and Nonobvious Insights of ChatGPT Responses  \n \n Accuracy, N (%) Internal Concordance Insights \nOpen-Ended Format    \n     SCORE 80 (47.9%) 143 (85.6%) 111 (66.5%) \n     Data-B 74 (66.1%) 112 (100%) 87 (77.7%) \n     Combined 154 (55.2%) 255 (91.4%) 198 (71.0%) \nMultiple Choice - Single Answer    \n     SCORE 119 (71.3%) 148 (88.6%) 106 (63.5%) \n     Data-B 76 (67.9%) 109 (97.3%) 69 (61.6%) \n     Combined 195 (69.9%) 257 (92.1%) 175 (62.7%) \n \nSCORE: Surgical Council on Resident Education;  \nData-B: refers to a second commonly used surgical knowledge assessment and question bank \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 24, 2023. ; https://doi.org/10.1101/2023.07.16.23292743doi: medRxiv preprint \nTable 3: Classification of Inaccurate ChatGPT Responses for SCORE Questions (N=44) \n \nClassification N (%) \nImprecise application of detailed information 3 (6.8) \nImprecise application of general knowledge  4 (9.1) \nInability to differentiate relative importance of information 4 (9.1) \nAccurate information; circumstantial discrepancy 6 (13.6) \nInaccurate information in fact-based question 11 (25.0) \nInaccurate information in complex question 16 (36.4) \n  \nTotal 44 (100%) \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 24, 2023. ; https://doi.org/10.1101/2023.07.16.23292743doi: medRxiv preprint \nTable 4:  Outcome of Repeat Question for 44 Initially Inaccurate Responses to SCORE  \n \nOutcome N (%) \nNo change in answer/response 28 (63.6%) \nChange in answer/response  \n     Inaccurate to inaccurate 10 (22.7%) \n     Inaccurate to accurate 6 (13.6%) \nTotal 44 (100%) \n \n \n \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 24, 2023. ; https://doi.org/10.1101/2023.07.16.23292743doi: medRxiv preprint \nSupplemental Material \nSupplemental Table 1: Interrater Agreement – Cohen kappa for OE and MC questions \n Open-Ended Questions Multiple Choice – Single Answer \n Cohen K N Cohen K N \nSCORE 0.720 30 1.0 30 \nData-B 0.681 20 1.0 20 \n \nSCORE: Surgical Council on Resident Education  \nData-B: refers to a second commonly used surgical knowledge assessment and question bank \n \nAdditional Supplemental Material \nAll input to the ChatGPT interface and associated output were recorded. Due to copyright laws, \nthis data is not presented in the current manuscript. However, pending requisite approval from \nthe respective organizations, this data may be shared upon reasonable request.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted July 24, 2023. ; https://doi.org/10.1101/2023.07.16.23292743doi: medRxiv preprint ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.45760321617126465
    },
    {
      "name": "Knowledge management",
      "score": 0.37760478258132935
    }
  ]
}