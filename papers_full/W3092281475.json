{
  "title": "On the importance of pre-training data volume for compact language models",
  "url": "https://openalex.org/W3092281475",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5003334803",
      "name": "Vincent Micheli",
      "affiliations": [
        "University of Geneva"
      ]
    },
    {
      "id": "https://openalex.org/A5064967815",
      "name": "Martin d’Hoffschmidt",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5076094010",
      "name": "François Fleuret",
      "affiliations": [
        "University of Geneva"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2969601108",
    "https://openalex.org/W3005700362",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3006185224",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3008851394",
    "https://openalex.org/W2948902769",
    "https://openalex.org/W2996159613",
    "https://openalex.org/W3017022649",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W3203309275",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3015982254",
    "https://openalex.org/W2970771982"
  ],
  "abstract": "Recent advances in language modeling have led to computationally intensive and resource-demanding state-of-the-art models. In an effort towards sustainable practices, we study the impact of pre-training data volume on compact language models. Multiple BERT-based models are trained on gradually increasing amounts of French text. Through fine-tuning on the French Question Answering Dataset (FQuAD), we observe that well-performing models are obtained with as little as 100 MB of text. In addition, we show that past critically low amounts of pre-training data, an intermediate pre-training step on the task-specific corpus does not yield substantial improvements.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 7853–7858,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n7853\nOn the importance of pre-training data volume for compact language\nmodels\nVincent Micheli\nIlluin Technology / EPFL\nvincent.micheli@illuin.tech\nMartin D’Hoffschmidt\nIlluin Technology\nmartin.dhoffschmidt@illuin.tech\nFranc ¸ois Fleuret\nUNIGE\nfrancois.fleuret@unige.ch\nAbstract\nRecent advances in language modeling have\nled to computationally intensive and resource-\ndemanding state-of-the-art models. In an ef-\nfort towards sustainable practices, we study\nthe impact of pre-training data volume on com-\npact language models. Multiple BERT-based\nmodels are trained on gradually increasing\namounts of French text. Through ﬁne-tuning\non the French Question Answering Dataset\n(FQuAD), we observe that well-performing\nmodels are obtained with as little as 100 MB\nof text. In addition, we show that past critically\nlow amounts of pre-training data, an interme-\ndiate pre-training step on the task-speciﬁc cor-\npus does not yield substantial improvements.\n1 Introduction\nOver the past year, pre-trained language mod-\nels have become the norm in Natural Language\nProcessing. These large-scale Transformer-based\n(Vaswani et al., 2017) networks considerably ad-\nvanced the state-of-the-art in language understand-\ning (Devlin et al., 2019) via a two-step process:\nself-supervised learning on a vast text corpus fol-\nlowed by ﬁne-tuning on a speciﬁc downstream task.\nFollowing these advances, the ongoing trend has\nbeen to build bigger models with an ever-increasing\namount of data (Liu et al., 2019; Raffel et al., 2020;\nRadford et al., 2019; Brown et al., 2020). How-\never, pre-training models with billions of param-\neters over hundreds of gigabytes of text requires\ntremendous computational resources that only a\nfew companies and institutions can afford. Besides,\nmany languages and speciﬁc corpora (e.g. legal,\nscientiﬁc) are currently under-resourced. Hence,\nour goal is to explore model architectures and data\nvolumes lowering the entry barrier to new research\nand practical applications.\nWe conduct experiments on French corpora in\norder to release the ﬁrst French compact language\nmodels and to illustrate the training process in an-\nother language than English. Furthermore, we con-\nsider the question answering task since compact\nmodels may ﬁnd their purpose in low-latency/fault-\ntolerant information retrieval systems.\n2 Problem statement\nWe intend to study the impact of pre-training data\nvolume when training compact bidirectional Trans-\nformers (Devlin et al., 2019). We assume a scarce\nresources setting, both in terms of data and com-\nputing power. Two key aspects are explored:\n• The amount of pre-training data required to\ntrain high-performing compact language mod-\nels.\n• The importance of corpus-speciﬁc MLM be-\nfore ﬁne-tuning.\nWe use the French part of the OSCAR corpora\n(Ortiz Suarez et al., 2019) for pre-training and the\nFQuAD dataset 1 (d’Hoffschmidt et al., 2020) for\nmachine reading comprehension ﬁne-tuning. More-\nover, the models under consideration are based on\nthe CamemBERT (Martin et al., 2020) language\nmodel.\n3 Related work\nA wealth of work has recently been released\n(Ganesh et al., 2020) on compressing Transformer-\nbased models (Vaswani et al., 2017; Devlin et al.,\n2019) through the pre-training of compact models\n(Turc et al., 2019), distillation (Hinton et al., 2015;\nJiao et al., 2019; Sun et al., 2020), pruning (Li et al.,\n2020; McCarley et al., 2019; Sanh et al., 2020; Fan\net al., 2020a) and quantization (Shen et al., 2019;\n1https://illuin-tech.github.io/\nFQuAD-explorer/\n7854\nFan et al., 2020b). Nevertheless, absolute perfor-\nmance is not the end goal of this study. Rather, we\ninvestigate the training process of compact models\nin the absence of larger ones to distillate or prune.\nFurthermore, Sanh et al. (2020) acknowledge the\ndifﬁculty of speeding up sparse models due to the\nabsence of specialized hardware. Therefore, from\nan inference speed standpoint, it is currently prefer-\nable to train compact models.\nLanguage models have been successfully pre-\ntrained on domain-speciﬁc corpora (Beltagy et al.,\n2019; Lee et al., 2019) and outperform their\ngeneral-purpose counterparts on targetted down-\nstream tasks. Still, training these models involved\nlarge datasets and computational resources out of\nreach for most.\nMultilingual models (Devlin et al., 2019; Lam-\nple and Conneau, 2019; Conneau et al., 2020) have\nbeen released to alleviate the need for language-\nspeciﬁc pre-training. While they offer competitive\nresults, they usually lag behind monolingual mod-\nels and require larger architectures.\nMartin et al. (2020) observed that large models\ndid not improve on evaluation tasks when increas-\ning the amount of pre-training data from 4 GB to\n138 GB. They left as future work to question the\nneed for large scale pre-training corpora with other\nmodel architectures and tasks.\n4 Datasets\n4.1 OSCAR\nOSCAR 2 (Ortiz Suarez et al., 2019) is a large-scale\nmultilingual open source collection of corpora ob-\ntained by language classiﬁcation and ﬁltering of\nthe Common Crawl corpus 3. The whole French\npart amounts to 138 GB of text and it has already\nbeen used to train French language models (Martin\net al., 2020). In this work, we only extract a sample\nof 4 GB of shufﬂed lines.\n4.2 FQuAD\nFQuAD (d’Hoffschmidt et al., 2020) is a recently\nintroduced open source French native reading com-\nprehension dataset. It consists of 60,000 questions\nand answers gathered on a set of 1,769 high-quality\nWikipedia articles. In many aspects, it is the French\nequivalent of SQuAD 1.1 (Rajpurkar et al., 2016).\nGiven a question and a paragraph, the task consists\n2https://oscar-corpus.com/\n3https://commoncrawl.org/about/\nModel Size Time\nCamemBERTSMALL 72 MB 157 ms\nCamemBERTBASE 440 MB 705 ms\nCamemBERTLARGE 1340 MB 2376 ms\nTable 1: Model size and inference time on an Intel\nXeon 2.30GHz Quad core CPU with batch size 1 and\nmax sequence length 384 tokens (average over 1000\nsamples).\nin extracting from the paragraph the span of text\nanswering the question.\nWe chose FQuAD as the ﬁne-tuning dataset be-\ncause it allows one to draw a direct parallel with\nits English counterpart (d’Hoffschmidt et al., 2020)\nand is one of the largest annotated French datasets.\nHowever, question answering is a notoriously dif-\nﬁcult task for compact models (McCarley et al.,\n2019). While distillation has shown to improve\ntheir results on the GLUE benchmark (Wang et al.,\n2018) substantially, machine reading comprehen-\nsion remains difﬁcult to speed-up without incurring\na signiﬁcant drop in accuracy.\n5 CamemBERT SMALL\nCamemBERT (Martin et al., 2020) is a multi-layer\nbidirectional Transformer (Vaswani et al., 2017)\nwith two architectures: base (12 layers, 768 hidden\ndimensions, 12 attention heads, 110M parameters)\nand large (24 layers, 1024 hidden dimensions, 16\nattention heads, 355M parameters). It is very sim-\nilar to RoBERTa (Liu et al., 2019). The main dif-\nferences are the use of whole-word masking and\nSentencePiece tokenization (Kudo and Richardson,\n2018) instead of subword-masking and byte-level\nByte-Pair encoding (Sennrich et al., 2016; Rad-\nford et al., 2019). RoBERTa itself improves upon\nBERT by aggregating several modiﬁcations on top\nof the original architecture such as removing the\nnext sentence prediction task, dynamic masking,\nand training with larger batches on more data.\nWe introduce CamemBERT SMALL 4, a\nCamemBERT-based language model with a small\narchitecture (12 layers, 256 hidden dimensions,\n4 attention heads, 17M parameters). The main\ndifference with the original CamemBERT lies in\nthe use of subword-masking. Indeed, the authors\nlater found out that whole-word masking had\n4The pre-trained models are made available in the Hugging\nFace collection: https://huggingface.co/illuin/\nlepetit.\n7855\nHyperparameter Pre-train Fine-tune\nTrain steps 200k 30k\nWarmup steps 10k 3k\nBatch size 128 32\nLearning rate 1e-4 1e-4\nAdam β1 0.9 0.9\nAdam β2 0.999 0.999\nWeight decay 0.01 0.0\nMax gradient norm 1.0 1.0\nDropout 0.1 0.1\nMask percent 15 n/a\nMax sequence length 512 384\nTable 2: Pre-training and ﬁne-tuning hyperparame-\nters. In the corpus-speciﬁc MLM step, we take the\nsame hyperparameters as in pre-training except that\nwe decrease the number of steps to 2.5k and drop the\nwarmup.\nat best a marginal impact on downstream task\nperformance.\nApart from inference speed and size considera-\ntions, two main factors explain this architectural\nchoice:\n• This is the same architecture as\nELECTRASMALL++ (Clark et al., 2020),\na recently released compact language model.\nEven though ELECTRA and CamemBERT\ndiffer in many regards (ELECTRA being\ntrained as a discriminator rather than a\ngenerator), prior experiments conducted by\nClark et al. (2020) give us an acceptable set\nof hyperparameters when pre-training and\nﬁne-tuning the model.\n• Turc et al. (2019)’s empirical results suggest\nthat depth should be prioritized over width\nwhen pre-training compact models.\nTable 1 shows that CamemBERTSMALL is much\nsmaller and faster than its larger siblings. In a\nplausible setup for question answering systems,\nit provides, respectively, a 4.5-fold and 15-fold\ninference speed-up compared to CamemBERTBASE\nand CamemBERTLARGE while being 6.2 and 18.8\ntimes smaller.\n6 Experiments\nSix overlapping subsets are built from the 4 GB OS-\nCAR sample. They are denoted as OSC10, OSC100,\nOSC500, OSC1000, OSC2000 and OSC4000 (the num-\nbers indicating the number of MB). We extract an\nadditional 10 MB sample from the corpus, which\nserves as a validation set for the self-supervised\npre-training task. On the other hand, FQuAD con-\nsists of a train/dev split of 50,741 and 5,668 ques-\ntion/context pairs.\nFor each OSCAR subset, we pre-train a\nCamemBERTSMALL model with the standard\nmasked language modeling (MLM) objective.\nThen we ﬁne-tune the pre-trained models on the\nquestion answering task with the same span pre-\ndiction method as BERT (Devlin et al., 2019). Be-\ntween those two steps, an optional MLM step over\nthe FQuAD train set is included.\nTable 2 shows the pre-training, intermediate\nMLM (if any) and ﬁne-tuning hyperparameters.\nFine-tuning being a brittle process (Dodge et al.,\n2020), ﬁne-tuning results are averaged over 3 seeds.\nThe experiments described were implemented\nusing Hugging Face’s Transformers library (Wolf\net al., 2019) and were conducted on an NVidia\nV100 16 GB.\n7 Analysis\nMartin et al. (2020) observed that complex down-\nstream tasks may require more pre-training steps.\nSince for each OSCAR subset the validation loss\nis still slowly decreasing after 200k steps, we as-\nsume that training longer might increase perfor-\nmance on the difﬁcult question answering task. On\nthe other hand, corpus-speciﬁc MLM ﬁne-tuning\nquickly converged for all models. Table 3 reports\nthe entirety of the results.\n7.1 How much data does one need to\npre-train a compact language model?\nAs we increase the amount of pre-training data,\nperplexity on the OSCAR dev set decreases in\nevery instance but one (OSC 4000). Nevertheless,\naside from OSC 10, discrepancies are small and\nthe models show almost identical learning curves.\nOSC10 is underperforming in terms of MLM per-\nplexity and question answering F1 score when\ncompared to larger subsets. However, past this\nsmallest dataset, pre-training data volume does\nnot exhibit any strong monotonic relationship with\ndownstream performance. The only OSCAR sub-\nset displaying a noticeable performance gap is\nOSC2000, with a +2.46 average F1 score increase\nover OSCAR100. For anchoring, a randomly ini-\ntialized CamemBERTSMALL model ”ﬁne-tuned” di-\nrectly on the FQuAD train set achieves an F1 score\n7856\nSubset Perplexities F1 score\nOSC10 45.20 / 43.34 58.18 (0.60)\nOSC100 14.22 / 11.91 68.50 (0.25)\nOSC500 12.75 / 10.58 69.50 (0.41)\nOSC1000 12.56 / 10.57 69.35 (0.64)\nOSC2000 12.45 / 10.41 70.96 (0.66)\nOSC4000 12.49 / 10.35 69.76 (0.61)\n(a) Without MLM ﬁne-tuning.\nSubset Perplexities F1 score\nOSC10 40.31 / 18.95 62.33 (0.58)\nOSC100 16.35 / 9.41 69.04 (0.16)\nOSC500 15.09 / 8.77 70.25 (0.43)\nOSC1000 14.74 / 8.83 69.84 (0.27)\nOSC2000 14.72 / 8.75 70.71 (0.08)\nOSC4000 14.90 / 8.68 69.84 (0.79)\n(b) With MLM ﬁne-tuning.\nTable 3: Dev OSCAR / FQuAD perplexities and FQuAD F1 score (average token overlap between predicted and\nground truth answers) for each pre-training subset.\nof only 17.76, i.e. 40 F1 points less than OSC 10.\nThis result indicates that even if a small amount\nof pre-training data is available, one should not\nneglect that step. Regarding larger architectures,\nCamemBERTBASE and CamemBERTLARGE mod-\nels from Martin et al. (2020) obtain an F1 score of\n88 and 92, respectively, after ﬁne-tuning.\nDue to computational constraints, we could not\ninvestigate smaller or larger datasets as well as\na prolonged pre-training phase. It could be the\ncase that for a 200k pre-training steps budget,\ndata volume is not the bottleneck. In fact, ad-\nditional training steps may be even more beneﬁ-\ncial for larger datasets. Nonetheless, a prelimi-\nnary experiment pushing the pre-training phase of\nCamemBERTSMALL on OSC2000 to 300k steps re-\nvealed that while the MLM loss decreased, the F1\nscore on the downstream task did not improve.\n7.2 Is corpus-speciﬁc MLM beneﬁcial?\nAgain, we observe a contrast between OSC10 and\nlarger subsets. OSC 10 is the only pre-training\ndataset signiﬁcantly improving on the downstream\ntask (+4.15 F1) and experiencing a decrease in per-\nplexity on both pre-training and ﬁne-tuning data\nwhen complemented with an intermediate MLM\nstep. However, this corpus-speciﬁc MLM step is\nnot truly intermediate since FQuAD contexts con-\ntain 10MB of raw text. This implies a 2-fold in-\ncrease in pre-training data rather than a speciﬁc\ndomain adaptation step. Therefore, we turn our\nfocus to larger subsets for the rest of this analysis.\nIn these cases, MLM ﬁne-tuning results in a\nnet FQuAD perplexity decrease at the cost of an\nOSCAR perplexity increase. Domain shift may be\nthe root cause of this trade-off. Indeed, as there\nexists a mismatch between pre-training and ﬁne-\ntuning sets, the language model has to adapt to the\nspeciﬁcity of descriptive paragraphs. In addition,\nperplexity is higher on the OSCAR dev set than\non the FQuAD one. This is most likely due to\nthe difﬁculty of predicting masked words in an\nheterogeneous web-crawled dataset compared to a\nset of high quality Wikipedia articles.\nFor every pre-training subset but one (OSC2000),\nMLM ﬁne-tuning induced a slight F1 score increase\non the downstream task. However, these gains are\nmarginal with at most a +0.75 average F1 score\nincrease in the case of OSC500. Additional exper-\niments are required to consolidate these ﬁndings,\nespecially on larger task-speciﬁc datasets such as\nscientiﬁc or legal corpora. In those instances, a\ngreater domain shift would probably legitimate an\nintermediate MLM ﬁne-tuning step.\n8 Conclusion\nWe investigated the importance of pre-training data\nvolume when training compact Transformer-based\nmodels. We made the observation that 100 MB of\nraw text are sufﬁcient to reach similar performance\nas with larger datasets on a question answering\ntask, and that corpus-speciﬁc self-supervised learn-\ning does not bring signiﬁcant improvements on\nthat particular problem. These preliminary results\npave the way for further experiments with other\nlanguage models, various architectures and new\ndownstream tasks.\nAcknowledgments\nWe gratefully thank Quentin Heinrich for his re-\nviewing and helpful discussions. We also thank\nIlluin Technology for its technical support and fund-\ning.\n7857\nReferences\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientiﬁc text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3615–\n3620, Hong Kong, China. Association for Computa-\ntional Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. Electra: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In International Conference on Learn-\ning Representations.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nACL, pages 8440–8451. Association for Computa-\ntional Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nMartin d’Hoffschmidt, Wacim Belblidia, Tom Brendl´e,\nQuentin Heinrich, and Maxime Vidal. 2020. Fquad:\nFrench question answering dataset.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah Smith.\n2020. Fine-tuning pretrained language models:\nWeight initializations, data orders, and early stop-\nping.\nAngela Fan, Edouard Grave, and Armand Joulin.\n2020a. Reducing transformer depth on demand with\nstructured dropout. In ICLR. OpenReview.net.\nAngela Fan, Pierre Stock, Benjamin Graham, Edouard\nGrave, Remi Gribonval, Herve Jegou, and Armand\nJoulin. 2020b. Training with quantization noise for\nextreme model compression.\nPrakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali\nKhan, Yin Yang, Deming Chen, Marianne Winslett,\nHassan Sajjad, and Preslav Nakov. 2020. Compress-\ning large-scale transformer-based models: A case\nstudy on bert.\nGeoffrey Hinton, Oriol Vinyals, and Jeffrey Dean.\n2015. Distilling the knowledge in a neural network.\nIn NIPS Deep Learning and Representation Learn-\ning Workshop.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2019. Tinybert: Distilling bert for natural language\nunderstanding.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nEMNLP (Demonstration), pages 66–71. Association\nfor Computational Linguistics.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2019. BioBERT: a pre-\ntrained biomedical language representation model\nfor biomedical text mining. Bioinformatics,\n36(4):1234–1240.\nZhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin,\nKurt Keutzer, Dan Klein, and Joseph E. Gonzalez.\n2020. Train large, then compress: Rethinking model\nsize for efﬁcient training and inference of transform-\ners.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Su ´arez, Yoann Dupont, Laurent Romary, ´Eric\nde la Clergerie, Djam ´e Seddah, and Beno ˆıt Sagot.\n2020. CamemBERT: a tasty French language model.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n7203–7219, Online. Association for Computational\nLinguistics.\nJ. S. McCarley, Rishav Chakravarti, and Avirup Sil.\n2019. Structured pruning of a bert-based question\nanswering model.\nPedro Javier Ortiz Suarez, Beno ˆıt Sagot, and Laurent\nRomary. 2019. Asynchronous pipelines for pro-\ncessing huge corpora on medium to low resource\ninfrastructures. Proceedings of the Workshop on\nChallenges in the Management of Large Corpora\n(CMLC-7) 2019. Cardiff, 22nd July 2019, pages\n9 – 16, Mannheim. Leibniz-Institut f ¨ur Deutsche\nSprache.\n7858\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nVictor Sanh, Thomas Wolf, and Alexander M. Rush.\n2020. Movement pruning: Adaptive sparsity by ﬁne-\ntuning.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei\nYao, Amir Gholami, Michael W. Mahoney, and Kurt\nKeutzer. 2019. Q-bert: Hessian based ultra low pre-\ncision quantization of bert. CoRR, abs/1909.05840.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. MobileBERT:\na compact task-agnostic BERT for resource-limited\ndevices. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2158–2170, Online. Association for Computa-\ntional Linguistics.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nThe impact of student initialization on knowledge\ndistillation. CoRR, abs/1908.08962.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning.",
  "topic": "Language model",
  "concepts": [
    {
      "name": "Language model",
      "score": 0.7447667717933655
    },
    {
      "name": "Computer science",
      "score": 0.7040956020355225
    },
    {
      "name": "Training set",
      "score": 0.7030284404754639
    },
    {
      "name": "Task (project management)",
      "score": 0.6666353940963745
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.6404285430908203
    },
    {
      "name": "Training (meteorology)",
      "score": 0.6105577945709229
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5257426500320435
    },
    {
      "name": "Natural language processing",
      "score": 0.5177318453788757
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.41947680711746216
    },
    {
      "name": "Machine learning",
      "score": 0.4103025794029236
    },
    {
      "name": "Engineering",
      "score": 0.09595087170600891
    },
    {
      "name": "Geography",
      "score": 0.07182040810585022
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Meteorology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I114457229",
      "name": "University of Geneva",
      "country": "CH"
    }
  ],
  "cited_by": 4
}