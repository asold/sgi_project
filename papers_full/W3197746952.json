{
  "title": "Memory and Knowledge Augmented Language Models for Inferring Salience in Long-Form Stories",
  "url": "https://openalex.org/W3197746952",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2715779443",
      "name": "David Wilmot",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102014928",
      "name": "Frank Keller",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2099813784",
    "https://openalex.org/W1991675220",
    "https://openalex.org/W4385574077",
    "https://openalex.org/W2890515900",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2036937632",
    "https://openalex.org/W2970168256",
    "https://openalex.org/W3034715004",
    "https://openalex.org/W2983301359",
    "https://openalex.org/W3174202502",
    "https://openalex.org/W1973499344",
    "https://openalex.org/W1941566443",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2492548828",
    "https://openalex.org/W2171733319",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2978962481",
    "https://openalex.org/W3081168214",
    "https://openalex.org/W3034514149",
    "https://openalex.org/W2006342812",
    "https://openalex.org/W3115285088",
    "https://openalex.org/W3155807546",
    "https://openalex.org/W53982325",
    "https://openalex.org/W2899371527",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W3104037392",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W2335559472",
    "https://openalex.org/W2325553602",
    "https://openalex.org/W1525331590",
    "https://openalex.org/W1574440611",
    "https://openalex.org/W2892456426",
    "https://openalex.org/W2995638926",
    "https://openalex.org/W3142421094",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W3145602566",
    "https://openalex.org/W2324296656",
    "https://openalex.org/W2000900121",
    "https://openalex.org/W3114665540",
    "https://openalex.org/W2312900102",
    "https://openalex.org/W2740470375",
    "https://openalex.org/W4256090461",
    "https://openalex.org/W2950130316",
    "https://openalex.org/W2989312920",
    "https://openalex.org/W2997103344",
    "https://openalex.org/W4323274322",
    "https://openalex.org/W2802209321",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3046169638",
    "https://openalex.org/W2745164258",
    "https://openalex.org/W1903562400",
    "https://openalex.org/W4287667694",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4287978516",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2112601319",
    "https://openalex.org/W3211582110",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W2045738181",
    "https://openalex.org/W2990928880",
    "https://openalex.org/W3159014618",
    "https://openalex.org/W3100292568"
  ],
  "abstract": "Measuring event salience is essential in the understanding of stories. This paper takes a recent unsupervised method for salience detection derived from Barthes Cardinal Functions and theories of surprise and applies it to longer narrative forms. We improve the standard transformer language model by incorporating an external knowledgebase (derived from Retrieval Augmented Generation) and adding a memory mechanism to enhance performance on longer works. We use a novel approach to derive salience annotation using chapter-aligned summaries from the Shmoop corpus for classic literary works. Our evaluation against this data demonstrates that our salience detection model improves performance over and above a non-knowledgebase and memory augmented language model, both of which are crucial to this improvement.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 851–865\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n851\nMemory and Knowledge Augmented Language Models for Inferring\nSalience in Long-Form Stories\nDavid Wilmot\nSchool of Informatics\nUniversity of Edinburgh\ndavid.wilmot@ed.ac.uk\nFrank Keller\nSchool of Informatics\nUniversity of Edinburgh\nkeller@inf.ed.ac.uk\nAbstract\nMeasuring event salience is essential in the un-\nderstanding of stories. This paper takes a re-\ncent unsupervised method for salience detec-\ntion derived from Barthes Cardinal Functions\nand theories of surprise and applies it to longer\nnarrative forms. We improve the standard\ntransformer language model by incorporating\nan external knowledgebase (derived from Re-\ntrieval Augmented Generation) and adding a\nmemory mechanism to enhance performance\non longer works. We use a novel approach\nto derive salience annotation using chapter-\naligned summaries from the Shmoop corpus\nfor classic literary works. Our evaluation\nagainst this data demonstrates that oursalience\ndetection model improves performance over\nand above a non-knowledgebase and memory\naugmented language model, both of which are\ncrucial to this improvement.\n1 Introduction\nForster (1985) compared a story to a wriggling\nworm of time that can be seen as a series of events\narranged in order (see also Abbott 2008) — dinner\ncomes after breakfast, night after day, nemesis fol-\nlows hubris. Not all events are of equal importance,\nand some far more salient than others. For exam-\nple, the beginning of Dickens’Great Expectations\n— Keep still, you little devil, or I’ll cut your throat!\n— is more salient to the story than events such as\nmy sister had a trenchant way of cutting our bread\nand butter for us. Salient events in storytelling are\nthose that drive the plot forward, change the state\nin the story world, as opposed to descriptive details\nor non-consequential activities. As such, detect-\ning salience is an essential part of understanding\nnarrative. Detecting salient events has important\ndownstream applications such as summarisation;\nsalient events are the core of plots and can aid sto-\nryline writing and story generation; they represent\nessential information and are relevant to question\nanswering.\nThis paper builds on the work of Otake et al.\n(2020), who use Barthes Cardinal Functions (BCF)\nfor unsupervised salience detection. We augment\nthis approach with a knowledgebase (KB) and\nmemory. Barthes Cardinal Functions (Barthes\nand Duisit, 1966) are hinge events that cannot be\ndeleted without altering the story; they are the deci-\nsion points between alternative consequential paths.\nBarthes and Duisit also deﬁne catalysers, which\nare inconsequential events such as the bread and\nbutter example, indices, which are descriptive, re-\nferring to a character or situation, and informants,\nwhich identify time and space. These latter types\ncan be seen as satellites around the nuclei, or ﬁll-\ning in gaps between cardinal functions. Hence to\nidentify BCF is to identify the main skeleton of the\nplot. We treat the BCF events as the salient events\nin a story. This scheme relates in narratology with\nChatman (1980) kernels and satellites model, as\nwell as with discourse theory in RST (Mann and\nThompson, 1988), which similarly has nuclei and\nsatellites and more loosely with SDRT (Asher and\nLascarides, 2005) with coordinating and subordi-\nnating relations. The key to the Otake et al. method\nis that it can be implemented using any LM (Lan-\nguage Model) on any text and does not require a\nlarge corpus of annotated training data.\nIn this paper, we extend the BCF concept by\nexploring new measures of salience derived from\nstructural manipulations: We infer swap salience,\nwhich is swapping rather than deleting an event\nwithin the BCF framework. Schmid (2003) dis-\ncusses how an event can be salient if a reader ex-\npects it, but it is unexpected to the character in the\nstory. The reader puts themselves into the char-\nacter’s shoes. Zillmann (1996) emphasises how\nsuspense is driven by anticipation and apprehen-\nsion on behalf of characters the reader cares about.\nBae and Young (2009) propose to use this knowl-\nedge disparity between the reader and the character\nto create more suspenseful plots and hence more\n852\nimportant events. We model knowledge salience as\nthe difference between an expert-informed reader\nversus a naive one by taking the difference between\nthe average log-likelihood of a base LM and an\nLM enriched with memory and a KB. We also\ntake inspiration from the model of Wilmot and\nKeller (2020), who compute suspense and surprise\nin short stories using vector states from a hierarchi-\ncal model; this follows from theoretical work by\nEly et al. (2015), and cognitive work from Li et al.\n(2019). We show how a vector salience measure\ncan be computed based on this approach.\nIn addition to exploring new salience measures,\nour work aims to overcome limitations of existing\nwork on salience modeling. Otake et al. (2020) only\nevaluate their model on a single type of narrative\n(Russian fairytales) and on a very small annotated\ndataset. We address this by using aligned sum-\nmaries from the Shmoop corpus (Chaudhury et al.,\n2019) to provide salience labels. This results in a\nlarge dataset of long works (novels and plays) anno-\ntated with silver-standard salience labels. A second\nlimitation of Otake et al. is that they use GPT-2\n(Radford et al., 2019) as their LM, which has a rel-\natively short context of a few hundred wordpieces.\nWhile this works for short stories, the context is\ntoo short to track lengthy novels or plays. Often a\ncharacter will disappear for a long period; for ex-\nample, Abel Magwitch in Great Expectations. Plots\nare often non-linear with recalls and ﬂash-forwards,\nand the same characters and places reoccur at inter-\nmittent points in the story. At any moment in the\nstory, the most relevant passages are not the most\nrecent but the previous actions of the characters,\nplaces, and situations involved.\nWe address this limitation by incorporating an\nepisodic knowledge retrieval mechanism (derived\nfrom RAG; Lewis et al. 2020b) and fuse this with\na short-term memory mechanism that can extend\nthe capabilities of a transformer LM. The intent is\nthat the memory will learn to recall the most rele-\nvant parts of the story, act as an implicit index into\nthese dimensions, and the KB will supplement this\nwith typical plot knowledge. This memory mech-\nanism is much more suitable than recent work on\nextended transformers for longer sequences, see\nTay et al. (2020) and Fournier et al. (2021) for\nthorough reviews. Characters, places, subplots ebb\nand ﬂow in long stories, so the most relevant in-\nformation may be hundreds of pages previous with\nmainly irrelevant information in-between, which\nsuits indexed episodic memory rather than a trans-\nformer that must ﬁlter out the mainly irrelevant\ndetails in-between. For example, Abel Magwitch in\nGreat Expectations is in the ﬁrst two chapters and\nthen reappears explicitly in Chapter 40.\nOur results show that integrating KB and mem-\nory components improves the overall performance\nof salience detection. Using a vector alternative\nto infer salience is a slight improvement over the\nLM. Other measures such as swap salience and\nknowledge salience perform worse than the main\nsalience measures but still show improvements over\nour baseline model.\n2 Related Work\nThe main architectural innovation is to use an ex-\nternal knowledgebase, based on RAG (Lewis et al.,\n2020b), and combine this seamlessly with a mem-\nory mechanism to improve the model’s predictive\nperformance. The main structure of this model\nis to use a question and document encoder, both\ntransformers, to learn and look up passages of text\nfrom a knowledgebase (based on DPR; Karpukhin\net al. 2020) and then fuse this knowledge into a\ntransformer encoder/decoder model such as BART\n(Lewis et al., 2020a) or T5 (Raffel et al., 2020).\nSimilar models including REALM (Guu et al.,\n2020), Hard EM (Min et al., 2019a), SpanSeqGen\n(Min et al., 2020), and Fusion-in-Decoder (Izac-\nard and Grave, 2021) have achieved state-of-the-\nart results in factual domains such as answering\nnatural language questions, trivia or games such\nas Jeopardy. In these domains, the key insight is\nthat ofﬂoading knowledge externally allows mod-\nels to perform better than much larger transformers\nthat need to encode all knowledge in their weights.\nThese methods that rely on retrieving raw text are\nalso competitive with those that have tried to in-\ncorporate structured information such as GraphRe-\ntriever (Min et al., 2019b) or PathRetriever (Asai\net al., 2020). We experiment both with a Wikipedia\nKB and Wikiplots, a KB of story plot summaries.\nThe motive for the latter is that these plot frag-\nments or vignettes act as a planning system (or\nschema; Schank and Abelson 1977) guiding expec-\ntations. Riedl and Sugandh (2008) used a similar\nconcept in a rule-based system. Sap et al. (2020)\nalso use a bag-like episodic memory mechanism for\ninference in stories without the more sophisticated\ntransformer encoders of the RAG model. After the\nexperimental work in this paper, a follow-up paper\n853\nby Shuster et al. (2021) on several RAG variants\nfound that the KB was able to reduce the amount\nof hallucination in generating dialogue. The KB\ngrounds the text generation in relevant facts re-\ntrieved from the KB. While the story domain is\ndifferent intuitively, the same effect is desirable;\ninferring salience should be grounded either in plot\nknowledge from Wikiplots or general knowledge\nfrom Wikipedia, and also the memory of the previ-\nous character actions and plot developments.\n3 Methods\n3.1 Model\nThe RAG model has been extended to incorporate\na memory module, see Figure 1. 1. Seen passages\nare added to the memory cache (conﬁgurable FIFO\nor LRU). The model retrievesnpassages, performs\na lookup in both the KB and memory and then\nreranks them together using the dot product score\nbetween the question and document encoder vec-\ntors. A signiﬁcant beneﬁt is that it naturally inte-\ngrates both a short-term and long-term KB retrieval\nmechanism with a relatively simple design while\nallowing a powerful pre-trained LM (BART Large;\nLewis et al. 2020a) and retrieval systems (DPR;\nKarpukhin et al. 2020) from RAG to be retained.\nFor comparison, we train abaseline model in which\nonly the question encoder from RAG is ﬁnetuned\nso that existing KBs can be used without becoming\nstale. We also compare to the mem model, where\nboth the question and document encoder are ﬁne-\ntuned, and only memory is used during inference.\nThe notation follows from the RAG paper and\nthe model derived from the RAG-Token model. As-\nsuming xis the original input or context text, and\nyis the target label for upcoming text, and za pas-\nsage of text from a retrieved document from the KB\nor memory, ta time index from the place of the pas-\nsage in the story, and θthe parameterisation of the\nmodel. The generation task, pθ(yt |x,z,y 1:t−1),\nis to predict yt by marginalising over the input, pre-\nviously generated word pieces and the retrieved\ndocument, this is deﬁned in (1). Each next token\nis marginalised over all the retrieved zdocuments.\nThe respective probability varies for each zat each\n1The code is provided via Github at https:\n//github.com/dwlmt/story-fragments/tree/\nemnlp2021-submission/\nstep for each retrieved passage.\nP(y|x) ≈\nN∏\nt\n∑\nz∈Z(p(·|x))\npµ(z|x)pθ(yt |x,z,y 1:t−1)\n(1)\nThe top z ∈Z, by default ﬁve, passages are re-\ntrieved by maximising the dot product, pµ(z |\nx) ∝exp(d(z)Tq(x)), where d is the document\nencoder, and q the question encoder, both pre-\ntrained from the DPR multiset model, resulting\nin a bi-encoder setup. Only q is ﬁnetuned in train-\ning. The text passages z, whether retrieved from\nthe KB or memory, are then concatenated onto the\noriginal xtext and fed through the BART large en-\ncoder/decoder model. The memory mechanism for\ntraining is a single pool of up to 128k vectors that\noperates as an LRU cache during training.\nThe principal training loss in (2) is simply the\nnegative log likelihood over the batch as per the\nstandard left-to-right decoder loss for BART. Be-\ncause the model marginalises the retrieved pas-\nsages, back-propagation through this loss also up-\ndates the question encoder to retrieve more relevant\npassages.\nLnll(y) =\n∑\nj\n−log p(yj |pj) (2)\n3.2 Training\nDatasets are read in an interleaved or round-robin\nfashion so that only one (x,y) pair from each story\nis in a batch. Batches are sliding windows of 12\nsentences for both x and y with a k of ﬁve pas-\nsages to retrieve. The combined context for the\nconcatenated encoder text is truncated to 512 word\npieces, and the max length for the decoder is 128.\nThe model is trained with a batch size of 32. RAG\nhas a delimiter separating retrieved text when con-\ncatenating for BART. We swap the order of RAGs\nconcatenation so that the context is ﬁrst and an-\nswer passages second to prevent truncation of the\ncontext text.\nTo allow the model to train on 12GB GPUs, we\nuse the zero optimisation memory saving features\nof DeepSpeed (Rasley et al., 2020), which also\nnecessitates using FP16, with gradient checkpoint-\ning for the model. Our training uses the base ver-\nsion of the RAG multiset encoders and the original\npre-trained BART Large. We ﬁnetune with Adam\n(Kingma and Ba, 2015) with a learning rate of 2−6.\n854\nDENSE SPACE\nKNOWLEDGEBASE\nMEMORY\nDOC ENCODER Context Text\n1\n3\nLM decode ...2\nRETRIEVER\n4\nGENERATOR\n5\n6\n7\n8\n1 Encode context using\nquestion encoder\n2 Lookup using max dot\nproduct\n3\nConcat retrieved docs\nand context\n4\nCalc doc scores, sort from\nKB and memory, then filter5\n6 Marginalise using doc\nscore and encoder context\n7Seq2Seq encoder/\ndecoder, i.e. BART\nAdd context to memory\ncache for future lookup8\nEncode context with\ndoc encoder\nFigure 1: Architecture of the memory RAG model: On the left-hand side are caches containing the permanent\nKB and transitory memory, which seen passages are added to. The Retriever encodes context text, looks up from\nboth KB and memory, and concatenates the retrieved text to the context text. The generator, the BART encoder-\ndecoder processes each passages concatenation, and marginalises over them to produce a distribution over output\nwordpieces.\n3.3 Datasets\nBooksCorpus (Zhu et al., 2015) provides a large\ncorpus of longer novel-length works and is used for\ntraining. However, BooksCorpus consists of free\nbooks scraped from Smashwords; these works are\nhighly slanted towards particular genres such as ro-\nmance and fantasy which are unlike the evaluated\ntask, which is mainly classic works. To supple-\nment BooksCorpus an additional training dataset\nfrom Gutenberg using the c-w/gutenberg library\nﬁltered to only English language ﬁctional works.\nAnother important area of longer-form storytelling\nis movies or dramatic works. So to improve diver-\nsity, the Movie Scripts datasets (Ramakrishna et al.,\n2017) is used. Multi-dataset models performed\nbetter on the validation set in training than single\ncorpus models, so only these are evaluated. The\ntraining set sizes are BooksCorpus circa 18k works,\nGutenberg 27k, and Movie Scripts 1.5k. We split\nsentences using Blingﬁre.\n3.4 Baselines\nThe primary baselines for salience prediction come\nfrom Otake et al. (2020). Random just randomly\nassigns a salience score to each sentence position.\nAscending assigns scores that increase per posi-\ntion. Descending, the reverse, assigns decreasing\nscores per position. The intuition behind these\nbenchmarks is that important information can be\nclustered at the beginning or end of a story or chap-\nter.\nOtake et al. use TF-IDF as another benchmark;\nwe use a BERT derived clustering summarisation\napproach (Miller, 2019). The method uses k-means\nto cluster BERT sentence vectors according to the\nnumber of desired sentences and then selecting the\nsentences closest to the centroids. Since salience\nscores are required, we adapt this method to output\nthe cosine distance from the centroid as a salience\nscore. We set the kso that there is one cluster for\nevery 10 sentences. One change from Miller is to\nuse the stsb-roberta-large sentence transformers\nmodel (Reimers and Gurevych, 2019), which has\nsentence embeddings that perform much better on\na range of semantic tasks than raw BERT.\n3.5 Inference\nSalience detection is based on the BCF method\n(Otake et al., 2020). We only use the sentence\ndeletion variant. Let Sbe the set of all sentences.\nThe salience is σ. For BCF this uses an event\nremoval function r and coherence evaluator c. c\nis the difference in coherence between when the\nsentence t is present and removed in (3) for the\nfollowing n sentences. Note that r can be used\nmore broadly as a structural manipulation function.\nIn this paper ris also used for swap function and a\nknowledge difference function, these are described\nlater.\nσ(St,S{1:n}) =c(S{1:n}) −c( ˜S{1:n}) (3)\nThe coherence (4) and (5) is the average log-\nlikelihood of the word pieces following sentences\nup to the maximum word pieces of the label, nor-\n855\n0 20 40 60 80 100\n−0.05\n0\n0.05\n0.1\n0.15\n0.2\nSentence\nSalience\nFigure 2: The Like-Sal of Moby Dick Chapter 1 . Stars are the Shmoop labels. Further interactive examples in\nsupplementary material.\nmalised by the length (6).\nc(S{1:n}) =Zlog P(S{t+1:n}|S{1:t−1},St)\n(4)\nc( ˜S{1:n}) =Zlog P(S{t+1:n}|S{1:t−1},r(St))\n(5)\nZ = 1\n|S{t+1:n}| (6)\nWe treat a sentence as an event. In inference, we\nuse a context of 12 sentences (truncated to 512\nword pieces) and up to 20 passages are retrieved\neither from the KB or memory. Otake et al. run\nsalience from each deleted sentence to the end of\nthe story, which is factorial complexity for the num-\nber of sentences. This is infeasible on novel-length\nworks, so our salience implementation is more lo-\ncalised and run over the next 128 LM wordpieces.\nAs well as BCF Salience, several other measures\nfor salience are explored. We experiment with\nknowledge salience, which measures the difference\nbetween salience with the RAG KB and Memory\nenabled versus with it disabled. Swap salience fol-\nlows the same structure as sentence deletion, but\nthe r function swaps the order of the sentences\nrather than deleting them, and so tests order de-\npendence as a form of salience. The sentiment\nis another relevant factor in whether something\nis salient; more emotional passages, either nega-\ntive or positive, might be more salient. We use\nV ADER sentiment (Hutto and Gilbert, 2014) as\nan adjustment factor for other salience measures\nsalience·(1.0 + abs(sentiment)) where senti-\nment is the absolute values of the sentiment in the\nrange 0.0−1.0. In addition, we follow Wilmot and\nKeller (2020) and deﬁne measures based on embed-\ndings: We deﬁne Eas the average of the word piece\nvectors from the BART encoder after marginalisa-\ntion. The ﬁrst measure is the cosine distance from\nsubsequent vectors, deﬁned by Wilmot and Keller\nas Ely surprise cos_dist(Et,Et−1). The second\nmeasure takes a vector distance rather than aver-\nage log-likelihood in the sentence deletion BCF\nmethod to create a version based on an embedding,\nnot LM decoding. The evaluated measures are:\n• Clus-Sal: The clustering baseline.\n• Like-Sal: The main BCF measure described.\n• No-Know-Sal: The same but with both the\nmemory and KB disabled as per Otake et al.\n• Like-Imp-Sal: Use sentiment to adjust the\nsalience.\n• Like-Clus-Sal: Combining the Like-Sal and\nClus-Sal measures via weighted addition:\nClus-Sal + 2∗Like-Sal.\n856\n• Like-Clus-Imp-Sal: Additionally adjusting\nfor the impact sentiment.\n• Know-Sal: The difference between average\nlog-likelihood of the LM with the KB and\nmemory on versus off, knowledge salience.\n• Swap-Sal: Use the same BCF approach but\nswaps rather than deletes a sentence to test\nstructural ordering.\n• Emb-Surp: The Ely surprise cosine distance\nmeasure.\n• Emb-Sal: Salience based on above embed-\nding distance not average log-likelihood.\nWe run the evaluation on three models: With the\nWikiplots dataset, with the Wikipedia dataset, and\nwith just mem enabled and additional ﬁnetuning of\nthe document encoder.\n4 Experiments\n4.1 Perplexity Model Improvements\nThe major innovation of the RAG derived model is\nincorporating the KB and memory mechanism into\nthe LM, and therefore, it needs to be tested what\nimpact it has as a general LM. Table 1 shows the\nbaseline model median perplexity with combina-\ntions of KB and memory access turned off.\nModel Perplexity ↓\nLM+Mem+KB 19.44\nLM+KB 19.37\nLM+KB(Wikipedia) 19.94\nLM+Mem 15.95\nLM 66.00\nLM+Scram(Mem+KB) 60.21\nTable 1: Median perplexity of the baseline model. Plus\nmeans that type of memory or KB is enabled. Scram\nmeans that random passages have been retrieved from\nthe KB and memory. Wikiplots KB unless Wikipedia\nis speciﬁed. All over the ﬁrst ﬁve stories in the dataset\nusing 20 retrieved passages.\nThe best model is the baseline with only the\nmemory, and the KB turned off. Both versions\nof the KB on their own and memory combined\nare slightly worse and around the same perplex-\nity. The crucial difference is that LM, the model\nwith neither, is far worse, and scrambling, which\nretrieve random passages, is only slightly better.\nOverall, these results validate that memory and KB\nare hugely improving the predictive power of the\nLM.\n4.2 ProppLearner\nFollowing on from the BCF paper (Otake et al.,\n2020), we evaluate the ProppLearner task derived\nfrom the Propp dataset (Finlayson, 2017), a richly\nannotated corpus of 15 Russian fairytales translated\ninto English. See Otake et al. for more rationale\nfor the link, but the Proppian functions with which\nthis corpus is annotated deﬁne stereotypically im-\nportant roles in the classic Russian fairytale. They\nrepresent the key events of a story’s plot, which\nshould therefore be salient. As per Otake et al.,\nthe results are reported using MAP (mean average\nprecision; Manning et al. 2008).\nModel MAP ↑\nRandom .213\nAscending .277\nDescending .185\nTF-IDF .279\nOtake Sal .280\nOtake Comb Sal .301\nClus-Sal .275\nLike-Sal .319\nLike-Imp-Sal .313\nKnow-Diff-Sal .309\nSwap-Sal .236\nEmb-Surp .247\nEmb-Sal .319\nTable 2: Compare Otake et al. baselines and models\nwith RAG equivalents.\nAll the RAG models are the baseline used with\ndifferent variants of the salience measures. The\nbest RAG models, see Table 2, measuresLike-Sal\nand Emb-Sal score slightly better than Otake et al.’s\nmodel. This validation is limited, though, as the\nPropp dataset is tiny, with only 15 stories of less\nthan 150 sentences and limited annotations. In the\nnext section, we extend this evaluation approach\nto a corpus of much longer works of classical lit-\nerature, using silver labels derived from a corpus\nof aligned summaries. This allows us to test both\nthe memory and KB mechanism adequately, as\nthese would be expected to be most advantageous\nfor longer works. This approach will enable us to\ntest whether our method scales beyond short texts\n857\nand adds robustness to the evaluation through the\nbreadth of the corpus and the challenging nature of\nthe text.\n4.3 Shmoop Automated Evaluation\nIdeally, to evaluate this thesis on longer works,\nthere would be a set of Gold standard annotations\nwith the salient sentences. Typically even short\nnovellas can be over 20K words, more normal nov-\nels longer than 50K words. More sweeping works\nsuch as Anna Karenina, Wuthering Heights, The\nFellowship of the Ring, or David Copperﬁeld can\nbe well over 100K words. Per-sentence annota-\ntions for longer works such as novels and plays\nare prohibitively expensive. This is especially true\nwhen multiple annotators are required to ensure\nhigh inter-annotator agreement. It would also not\nbe possible with insufﬁciently trained and lower\ncost crowdsourced workers. Reading a local pas-\nsage would not be enough as it is only possible to\njudge salience over the whole narrative, which can\nbe tens of thousands of words. This requires strong\ncomprehension and thus requires skilled annota-\ntors and is a daunting annotation task. Instead, this\npaper builds on a variant of an approach for event\nsalience in news articles (Liu et al., 2018; Jindal\net al., 2020). The method is to align expert-written\nsummaries with the full text, tagging sentences that\nalign with the summary as salient, thus turning the\nevaluation into a binary ranking problem. The intu-\nition is that the summary will mention only salient\nevents and themes.\nWe use the Shmoop corpus (Chaudhury et al.,\n2019), which contains classic works of literature,\nsuch as Moby Dick, but also plays such as A Mid-\nsummer Nights Dream, and short stories including\nThe Mask of the Red Death. The Shmoop corpus\nhas stories split into chapters with aligned sum-\nmaries. These bullet point summaries, if collo-\nquial in style, are professionally written as study\nguides for students. They are written with a deep\nunderstanding of the plots and the salient events\nin them, which can serve as a valid proxy for\nsalience. Conceptually they are also similar to the\nProppLearner evaluation, although without spe-\nciﬁc Proppian roles, which are unused anyway\nfor binary salience classiﬁcation. It also aligns\nwith the BCF concept, as if events from the sum-\nmary are removed, they would signiﬁcantly alter\nthe plot.2 Jindal et al. (2020) align summaries to\n2There are occasional exceptions, such as summary points\ntext by using BERT (Devlin et al., 2019) to match\nconstituent parts of events extracted from semantic\nrole labels (SRLs). However, in testing, this per-\nformed poorly. Unlike news, the story summaries\nare more loose descriptions of events, which the\nSRL method struggles with. We instead found us-\ning an S-Bert transformer (Reimers and Gurevych,\n2019) on the whole sentence worked much better\nin aligning summaries to the full text. The method\nis as follows:3\n1. Split aligned chapters into sentences, St for\nsummaries and Ft for the full text.\n2. Extract sentence embeddings using the Sen-\ntence Transformers model stsb-roberta-large\n(Reimers and Gurevych, 2019) , r(St) and\nr(Ft) .\n3. Calculate cosine similarity for all pairwise\nr(St) and r(Ft) for t±ρ, where the range is\nρ= 10.0% and the valid range for tis x∈X\nfor St, and y∈Y for Ft.\n4. Mark up to kas salient sentences for all sen-\ntence pairs in the alignment windows(x,y) =\ncos_sim(r(Stx ),r(Fty ) where:\n• k= 3\n• s(x,y) ≥µ, µ= 0.35\n• s(x,y) ≥argmaxx∈X,y∈Ys(x,y) −θ,\nwhere θ= 0.05\nPairs of summary and full-text sentences are\nmatched within a percentile range. The rationale\nis that matches are likely to occur in the full text\nin a roughly similar position to the summary. We\nallow up to three targets per summary sentence,\nas the summary sentences often compress infor-\nmation with multiple clauses and because some-\ntimes there are near identically suitable matches.\nThe advantage of this method is that it allows au-\ntomated evaluation of salience to scale to longer\nworks that test the memory and KB mechanism of\nthe model without excessive annotation cost. The\nsilver Shmoop annotations are on 226 titles, span-\nning 6,939 chapters with 214,617 silver standard\nlabels. Each chapter averages 148 sentences with\nan average of 31 labelled as salient using the crite-\nria speciﬁed. See Figure 2 for an example of the\nthat discuss themes of the overall work and not speciﬁc plot\nevents, but these are rare.\n3Full examples are referenced in the supplementary mate-\nrial.\n858\nMeasure MAP ↑ Rouge-L ↑ Recall K↑\nPlots Mem Pedia Plots Mem Pedia Plots Mem Pedia\nRandom .178 .178 .178 .250 .250 .250 .132 .132 .132\nAscending .152 .152 .152 .243 .243 .243 .163 .163 .163\nDescending .207 .207 .207 .180 .180 .180 .109 .109 .109\nClus-Sal .230 .230 .230 .296 .296 .296 .187 .187 .187\nNo-Know-Sal .246 .246 .246 .336 .336 .336 .205 .205 .205\nLike-Sal .294 .280 .288 .368 .356 .359 .254 .241 .243\nLike-Imp-Sal .291 .276 .287 .367 .352 .369 .253 .238 .251\nLike-Clus-Sal .291 .273 .287 .355 .339 .358 .245 .228 .243\nLike-Clus-Imp-Sal .289 .276 .285 .351 .336 .355 .240 .225 .251\nKnow-Diff-Sal .246 .242 .243 .301 .300 .306 .199 .194 .200\nSwap-Sal .256 .241 .252 .309 .294 .313 .210 .193 .210\nEmb-Surp .249 .196 .243 .311 .315 .315 .201 .245 .200\nEmb-Sal .312 .311 .309 .413 .371 .419 .271 .271 .272\nTable 3: Shmoop results from the silver label evaluations.\nShmoop labels plotted with the salience for a book\nchapter.\nAs for the ProppLearner data, we report MAP.\nWe also evaluate with ROUGE-L (Lin, 2004), com-\nparing the text by selecting the kmost salient sen-\ntences according to the measure where k is the\nnumber of salient sentences, and report recall at\nk. All measures are calculated by chapter, and we\ntake the mean across the dataset.\nThe results in Table 3 reveal several main themes.\nThe Clus-Sal baseline measure improves on all the\nother baselines but only by a comparatively small\nmargin with the best of each, by 0.03 compared\nwith the best MAP baseline, 0.04 with Rouge-L,\nand 0.02 with recall. The baseline is a centroid\nbased extractive summarisation model that uses a\npowerful transformer; the relatively small perfor-\nmance improvement increase shows that the task is\nchallenging.\nThe main Like-Sal measure shows an improve-\nment of around 0.05 over Clus-Sal, and 0.10–0.15\nover the baseline. This is a reasonable improve-\nment given the model is unsupervised. The No-\nKnow-Sal (without memory and KB) is about0.03–\n0.04 worse on MAP and recall, which indicates\nthat the RAG enhancements are helping improve\nsalience detection. The theoretical reason would\nbe that BCF detects shifts in state and the informed\nmodel with the KB and memory is more likely\nto predict more obvious events. So salient events\nare more likely to be signiﬁcant plot shifts. The\nbiggest ﬁnding is that salience based on the em-\nbedding, Emb-Sal is the strongest measure. This\nshows the merit of using the BART model more\nﬂexibly as a general-purpose sentence encoding\nmodel. The Emb-Surp measure is a slight improve-\nment on the baselines, indicating that it is mainly\nthe BCF method that causes an improvement in\nsalience detection, rather than a simple measure\nof how much the story changes from sentence to\nsentence.\nOne difference from the Otake et al. ﬁnding\nis that combining the Clus measures makes little\ndifference. Neither do the Imp measures that use\nabsolute sentiment score. While worth exploring\nfurther this is consistent with Wilmot and Keller\n(2020) ﬁndings when adjusting sentiment with in-\nferring surprise and suspense.\nOf the more esoteric measures, both Swap-Sal\nand Know-Sal improve on the baseline, although\nnot by much. The more interesting is Know-Diff-\nSal, which performs similarly to the Clus-Sal base-\nline. The measure as a proxy to exploit the differ-\nence between reader and character is quite crude.\nThere may be a more sophisticated way to develop\nthis idea by modelling character knowledge explic-\nitly.\nLargely speaking, there does not seem to be\nmuch of a difference between the different mem-\nory and KB conﬁgurations. With the best measure\nEmb-Sal, the results are nearly identical. With the\noriginal BCF measure Like-Sal and its variants,\n859\nboth the Wikiplots dataset (plot summaries from\nWikipedia) and the full Wikipedia dataset only re-\nsult in a tiny improvement. It might be expected\nthat a KB would improve performance for salience\nprediction, but recall that in the perplexity evalua-\ntion, memory-only performed better. The present\nresults also suggest that the memory mechanism\nis the main reason for the improvement over No-\nKnow-Sal.\nThe memory and KB access pattern of the model\nis highly non-linear and references the earlier men-\ntion of the same characters, places, or moods. One\nexample of this is from Great Expectations ﬁnal\nchapter, where Pip and Estella have their last meet-\ning. The passage most recalled is their early meet-\ning some 100K odd words earlier while walking in\nthe Garden where Estella plays a trick on Pip. The\nmemory focuses on the characters and their rela-\ntionship rather than many irrelevant details and sub-\nplots occurring in between. The episodic memory\ncan be thought of as acting as an index into crucial\nelements of the plot, which is essential for nar-\nrative comprehension (Zwaan, 1999; Zwaan et al.,\n1995). It justiﬁes the suitability of an episodic mem-\nory model for understanding longer-form narrative\ntexts.\n5 Conclusion\nThe main overall ﬁnding is that the BCF method\ncan infer salience over and above baselines with\nan improvement on much longer works. We ﬁnd\nthat augmenting an LM with memory and an ex-\nternal KB can improve the detection of salience\nand increase the predictive power of the LM on\nnarrative text. We also ﬁnd that a vector-based ver-\nsion of the concept can perform slightly better than\nusing the log-likelihood from an LM. Therefore,\nthis paper demonstrates that it is feasible to run an\nunsupervised method on novels from Dickens or\nplays by Shakespeare and achieve correlation with\nan automated silver label benchmark. Nevertheless,\nthe MAP results are around 0.3, and ROUGE-L is\n0.4, which leaves room for improvement.\nOne factor in the moderate increase could be that\nthe salience modelling is explicitly local over the\nlabel of the n next tokens. This is more a local\nview of salience as intended from the reader per-\nspective. The model may ﬂag up false leads that\nare locally important but not globally for the plot.\nIn contrast, the Shmoop is written with the knowl-\nedge of the whole story, and so will exclude them.\nA more reader orientated evaluation is for future\nwork. Although the Shmoop alignment is generally\nstrong, there are occasions where arguably mul-\ntiple sentences could be deemed the correct one,\nand the silver label is one and the salient peak the\nother. With this unsupervised approach, perfor-\nmance is likely to be underestimated as the labels\nare entirely independent. In contrast to much recent\nsupervised work, such as PEGASUS (Zhang et al.,\n2020) summarization Has additional human eval-\nuation on some datasets. system, use silver labels\ncreated with proxies such as ROUGE. The labels\nboth train the system and are evaluated on. Even\nwith a separate test set performance, the system is\nmore likely to replicate any noisy misalignments\nin the labelling process and overestimate perfor-\nmance.\nOn future work, if RAG can improve LM pre-\ndiction performance and help infer salience, then\nthe same models would seem to hold promise in\nimproving text generation, including story genera-\ntion.\nThe knowledge salience approach is a simple\nattempt to model the informed reader versus the\nnaive one. In narratology, the characters perspec-\ntive is crucial in for example eventfulness (Schmid,\n2003; Schmid et al., 2017); Lotman et al. (1977)\nnotion of characters crossing a forbidden seman-\ntic border; or suspense as per Zillmann (1996), or\nGerrig and Bernardo (1994) concept of the reader\nas problem solver. There is, therefore, rich poten-\ntial work in modelling character states, knowledge,\nintents and contrasting them with the readers’ ex-\npectations, and the norms of the narrative world in\ninferring concepts such as salience, suspense, and\nsurprise. Characters could be implicitly modelled\nusing a per entity memory model extending the\ncurrent RAG approach. Or take a more structured\napproach inspired by recent work such as Sims and\nBamman (2020) modelling literary character com-\nmunication, or story generation systems such as\nCAST (Peng et al., 2021) that model multiple char-\nacters goals or C2PO (Ammanabrolu et al., 2021)\nthat more explicitly models causal chain relations.\nAcknowledgments\nThe authors would like to thank the anonymous\nreviewers. We would also like to thank Shmoop for\npermission to use and publish from the summaries.\nWilmot’s work is funded by an EPSRC doctoral\ntraining award.\n860\nReferences\nH.P. Abbott. 2008. The Cambridge Introduction to Nar-\nrative. Cambridge Introductions to Literature. Cam-\nbridge University Press.\nPrithviraj Ammanabrolu, Wesley Cheung, William\nBroniec, and Mark O. Riedl. 2021. Automated story-\ntelling via causal, commonsense plot ordering. Pro-\nceedings of the AAAI Conference on Artiﬁcial Intel-\nligence, 35(7):5859–5867.\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\nRichard Socher, and Caiming Xiong. 2020. Learn-\ning to retrieve reasoning paths over wikipedia graph\nfor question answering. In International Conference\non Learning Representations.\nNicholas Asher and A. Lascarides. 2005. Logics of\nconversation. In Studies in natural language pro-\ncessing.\nByung-Chull Bae and R. Young. 2009. Suspense? sur-\nprise! or how to generate stories with surprise end-\nings by exploiting the disparity of knowledge be-\ntween a story’s reader and its characters. InICIDS.\nR. Barthes and Lionel Duisit. 1966. An introduction\nto the structural analysis of narrative. New Literary\nHistory, 6:237.\nSeymour Benjamin Chatman. 1980. Story and Dis-\ncourse: Narrative Structure in Fiction and Film .\nCornell paperbacks. Cornell University Press.\nAtef Chaudhury, Makarand Tapaswi, Seung Wook\nKim, and Sanja Fidler. 2019. The shmoop corpus:\nA dataset of stories with loosely aligned summaries.\nCoRR, abs/1912.13082.\nJ. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In\nNAACL-HLT.\nJ. Ely, Alexander Frankel, and Emir Kamenica. 2015.\nSuspense and surprise. Journal of Political Econ-\nomy, 123:215 – 260.\nMark A. Finlayson. 2017. Propplearner: Deeply anno-\ntating a corpus of russian folktales to enable the ma-\nchine learning of a russian formalist theory. Digit.\nScholarsh. Humanit., 32:284–300.\nEdward Morgan Forster. 1985. Aspects of the Novel ,\nvolume 19. Houghton Mifﬂin Harcourt.\nQuentin Fournier, Gaétan Marceau Caron, and Daniel\nAloise. 2021. A practical survey on faster and\nlighter transformers. CoRR, abs/2103.14636.\nR. Gerrig and A. Bernardo. 1994. Readers as problem-\nsolvers in the experience of suspense. Poetics,\n22:459–472.\nKelvin Guu, Kenton Lee, Z. Tung, Panupong Pasupat,\nand Ming-Wei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In ICML.\nC. Hutto and Eric Gilbert. 2014. Vader: A parsimo-\nnious rule-based model for sentiment analysis of so-\ncial media text. In ICWSM.\nGautier Izacard and E. Grave. 2021. Leveraging pas-\nsage retrieval with generative models for open do-\nmain question answering. In EACL.\nDisha Jindal, Daniel Deutsch, and Dan Roth. 2020.\nIs killed more signiﬁcant than ﬂed? a contextual\nmodel for salient event detection. In Proceedings of\nthe 28th International Conference on Computational\nLinguistics, pages 114–124, Barcelona, Spain (On-\nline). International Committee on Computational\nLinguistics.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 6769–\n6781, Online. Association for Computational Lin-\nguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020a. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020b.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. In Advances in Neural Infor-\nmation Processing Systems, volume 33, pages 9459–\n9474. Curran Associates, Inc.\nZhiwei Li, Neil R. Bramley, and T. Gureckis. 2019.\nThe critical moment is coming: Modeling the dy-\nnamics of suspense. In CogSci.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out , pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nZhengzhong Liu, Chenyan Xiong, Teruko Mitamura,\nand Eduard Hovy. 2018. Automatic event salience\nidentiﬁcation. In Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 1226–1236, Brussels, Belgium.\nAssociation for Computational Linguistics.\n861\nIu. M. Lotman, G. Lenhoff, and R. Vroon. 1977. The\nstructure of the artistic text.\nW. Mann and S. Thompson. 1988. Rhetorical structure\ntheory: Toward a functional theory of text organiza-\ntion. Text & Talk, 8:243 – 281.\nChristopher D. Manning, P. Raghavan, and Hinrich\nSchütze. 2008. Introduction to information retrieval:\nEvaluation in information retrieval.\nDerek Miller. 2019. Leveraging bert for extractive text\nsummarization on lectures. ArXiv, abs/1906.04165.\nSewon Min, Danqi Chen, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2019a. A discrete hard EM ap-\nproach for weakly supervised question answering.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 2851–\n2864, Hong Kong, China. Association for Computa-\ntional Linguistics.\nSewon Min, Danqi Chen, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2019b. Knowledge guided text re-\ntrieval and reading for open domain question answer-\ning. ArXiv, abs/1911.03868.\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2020. AmbigQA: Answering am-\nbiguous open-domain questions. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 5783–\n5797, Online. Association for Computational Lin-\nguistics.\nTakaki Otake, Sho Yokoi, Naoya Inoue, Ryo Takahashi,\nTatsuki Kuribayashi, and Kentaro Inui. 2020. Mod-\neling event salience in narratives via barthes’ car-\ndinal functions. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics ,\npages 1784–1794, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nXiangyu Peng, Siyan Li, Sarah Wiegreffe, and Mark\nRiedl. 2021. Inferring the reader: Guiding auto-\nmated story generation with commonsense reason-\ning. CoRR, abs/2105.01311.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam M. Shazeer, Adam Roberts,\nKatherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, W. Li, and Peter J. Liu. 2020. Explor-\ning the limits of transfer learning with a uniﬁed text-\nto-text transformer. J. Mach. Learn. Res., 21:140:1–\n140:67.\nAnil Ramakrishna, Victor R. Martinez, Nikos Malan-\ndrakis, K. Singla, and Shrikanth S. Narayanan. 2017.\nLinguistic analysis of differences in portrayal of\nmovie characters. In ACL.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,\nand Yuxiong He. 2020. Deepspeed: System opti-\nmizations enable training deep learning models with\nover 100 billion parameters. In Proceedings of the\n26th ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining , KDD ’20,\npage 3505–3506, New York, NY , USA. Association\nfor Computing Machinery.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nbert: Sentence embeddings using siamese bert-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing.\nAssociation for Computational Linguistics.\nMark O. Riedl and Neha Sugandh. 2008. Story plan-\nning with vignettes: Toward overcoming the content\nproduction bottleneck. In ICIDS.\nMaarten Sap, Eric Horvitz, Yejin Choi, Noah A. Smith,\nand James Pennebaker. 2020. Recollection versus\nimagination: Exploring human memory and cogni-\ntion via neural language models. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics , pages 1970–1978, On-\nline. Association for Computational Linguistics.\nR. Schank and R. Abelson. 1977. Scripts, plans, goals\nand understanding: an inquiry into human knowl-\nedge structures.\nWolf Schmid. 2003. Narrativity and eventfulness.\nWhat is narratology, pages 17–33.\nWolf Schmid, P. K. Hansen, John Pier, and Philippe\nRoussin. 2017. Eventfulness and repetitiveness:\nTwo aesthetics of storytelling.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmenta-\ntion reduces hallucination in conversation. CoRR,\nabs/2104.07567.\nMatthew Sims and David Bamman. 2020. Measuring\ninformation propagation in literary social networks.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 642–652, Online. Association for Computa-\ntional Linguistics.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald\nMetzler. 2020. Efﬁcient transformers: A survey.\nCoRR, abs/2009.06732.\nDavid Wilmot and Frank Keller. 2020. Modelling sus-\npense in short stories as uncertainty reduction over\nneural representation. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 1763–1788, Online. Association\nfor Computational Linguistics.\nJingqing Zhang, Y . Zhao, Mohammad Saleh, and Pe-\nter J. Liu. 2020. Pegasus: Pre-training with ex-\ntracted gap-sentences for abstractive summarization.\nIn ICML.\n862\nY . Zhu, Ryan Kiros, R. Zemel, R. Salakhutdinov, R. Ur-\ntasun, A. Torralba, and S. Fidler. 2015. Aligning\nbooks and movies: Towards story-like visual ex-\nplanations by watching movies and reading books.\n2015 IEEE International Conference on Computer\nVision (ICCV), pages 19–27.\nDolf Zillmann. 1996. The psychology of suspense\nin dramatic exposition. Suspense: Conceptual-\nizations, theoretical analyses, and empirical explo-\nrations, pages 199–231.\nRolf A. Zwaan. 1999. Five dimensions of narrative\ncomprehension: The event-indexing model.\nRolf A. Zwaan, M. C. Langston, and A. Graesser. 1995.\nThe construction of situation models in narrative\ncomprehension: An event-indexing model. Psycho-\nlogical Science, 6:292 – 297.\n863\nA Examples\nA.1 Interactive Plots\nWithin supplementary material are\nshmoop_iteractive_plots. This is a selection\nof interactive plots all.html from the Shmoop\nalignment of chapters. The plots show all the\nreported metrics that can be toggled using the\nlegend. Each plot has the full text of the chapter\nwith a sentence on each data point. The gold stars\nare the original Shmoop summary sentence, which\nsentence it aligns with, and the similarity score.\nAlso included with each chapter is a correlation\nheatmap plot showing the Spearman ρcorrelation\nbetween all the reported metrics.\nA.2 Retrieved Passages\nTo illustrate how the retrieval mechanism\nfunctions in supplementary material within\nshmoop_retrieved_passages are chapters from the\nend or near the end of Kim, Great Expectations\nand 20,000 Leagues Under the Sea . These json\nﬁles contain a list of the distinct passages for the\nchapter of each book. For each passage there is\na list of retrieved passages that were looked up\nfrom the KB and memory for each block, 10 per\npassage, using the baseline. Each passage has a\ndot product score and the marginalised probability.\nThe memory_id for a passage indicates the relative\nposition in the story. The main reason for inclusion\nis it shows the memory lookup is highly non-linear\nand the retrieved passages from earlier in the story\nare strongly related to the characters, places and\nthemes involved from anywhere in the story and\nnot the most recent chapters.\nA.3 Shmoop Alignment\nFor Shmoop alignment some examples are also\nfound in table 4 that illustrates a few different\ntypes of sentences and the matches against the full\ntext. In the supplementary materialﬁle within the\nshmoop_alignments folder are the alignment ﬁles\nfor 20,000 Leagues Under the Sea and Richard\nIII. Only two are included to demonsrate the align-\nment since Shmoop requires permission to use the\nsummaries for the dataset. The format of the ﬁle\nis:\n• Within the main alignment json ﬁles the main\njson element with the annoations is chapters\nwhich splits books or plays into sub-sections.\n• Within each element of chapters there is a\nsummary element.\n– summary is a list with each of the sum-\nmary sentences each one has a list of\nalignments that contains the index, text,\nand cosine similarity score of the full text\nsentences it is linked to 4.\n• Within each element of chapters there is a\nfull_text element.\n– full_text contains a list of the sen-\ntences with the full text. Each sentence\nhas a salient boolean attribute and a\nsalience_score attribute.\n– Not included in the submission but these\nare exported to separate per book json\nﬁles for running through the RAG model.\nFor running the Shmoop alignment from the\nGithub repository:\n• align_shmoop_summaries.py processes the\nseparate raw summaries and full text books\ninto a single jsonl ﬁle.\n• salience_event_processing.sh, a slurm script\nruns the Shmoop alignment and produce\nrunnable ﬁles for the model input. The ﬁle\nhas conﬁgurable parameters for changing the\nthresholds and the models used.\nB Environment and Reproducibility\nB.1 Setup\nThe environment can be setup either via the re-\nquirements.txt ﬁle with pip on the Anaconda envi-\nronment.yaml ﬁle, both in the Github repository.\nB.2 Training Datasets\nThe datasets are: BooksCorpus, Gutenberg, Script-\nCorpus, and Wikiplots (as a KB, and not for train-\ning).\nThe preprocessing for all datasets is the same:\n1. Sentence splitting using Blingﬁre.\n2. Stories are randomly shufﬂed according to a\nﬁxed seed.\n4The threshold in the ﬁle 0.3 for alignment. Within the\nevaluation script a tight ﬁlter is used on these alignments of\n0.35.\n864\n3. There is a 80/10/10 training/validation/test\nsplit but this is only used for early stopping\nin training since evaluation is on separate\ndatasets - ProppLearner and Shmoop.\nB.3 Training and Inference\n• Training\n– Conﬁg: The conﬁg ﬁles are in the train-\ning_conﬁg. The reported models are the\n12 sentence block variants without exper-\niment entmax or unlikelihood training\nwhich isn’t reported.\n– Models: The model ﬁles will be made\navailable via Github when the anonymity\nperiod ends. BART Large has 400M\nparameters, the question encoder has\n110M parameters and the doc encoder\nhas 110M parameters. In the baseline\nmodel all apart from the doc encoder is\nﬁnetuned, and all are ﬁnetuned with the\nmemory only models.\n– Policy: All models were trained with\nbatch size 20000 instance per epoch in\ntraining and 2000 for validation. The\nearly stopping policy is to stop training\nafter 3 epochs without improvement.\n– Time: For the baseline model training\ntook 9 days. Other models are compara-\nble.\n– Epochs: Baseline model training ran for\n11 epochs, again other models are simi-\nlar.\n– Validation Performance: The best vali-\ndation loss is 398 (sum of the log likeli-\nhood) from 694 on an untrained model.\n• Inference\n– Computation: Inference computation de-\npends on which salience measures are\nenabled. The main salience BCF method\nrequires two passes through the text.\nAdding in knowledge or swap salience\nadds another pass for each. This is be-\ncause the text must be passed through\nwith an without each structural change.\nWith all methods enabled for long works\nsuch as The Brothers Karamazov, Emma,\nMoby Dick , or Great Expectations all\n> 150K words inference time is typi-\ncally 4 −6 hours running on a single\nGPU. This is pretty reasonable given the\nlength of the works, and obviously much\nshorter novels and plays have proportion-\nally shorter inference time.\n– Memory: The base conﬁguration uses\n28GBof general purpose memory, this\nneeds to be increased to 64 is the full\nWikipedia KB with 23M passages is\nused.\nB.4 Evaluation\nWithin the Github repository the main evaluation\nscripts is salience_evaluation.sh. This script pro-\nduces a per chapter csv ﬁle with all the evaluation\nmetrics stats, and a single aggregated whole. It\nused evaluating output in jsonl format produced by\npredictor_batch.sh the main script to run salience\ninference with an existing model over a batch of\nstories. There is also a salience_plot.sh script for\nproducing the interactive charts for each evaluation\noutput.\nNote more documentation is needed for the env\nvariables to set but they are fairly self-explanatory\nin the slurm ﬁles. The main inference code is in the\nstory_fragments/predictors package. The conﬁg is\nlargely read through env variables in the python\nscript. These need to be documented further for a\nfull code release.\n865\nSummary Full Text Score\nThen, two huge columns\nof water shoot from it,\nknocking the crew down.\nThe electric light suddenly went out, and two\nenormous waterspouts crashed onto the deck\nof the frigate, racing like a torrent from stem\nto stern, toppling crewmen, breaking spare\nmasts and yardarms from their lashings.\n0.723\nHe grabs the extinguisher\ncap thing and tries to smother\nthe kid/grandpa ghost with it.\nIn the struggle, if that can be called a struggle\nin which the Ghost with no visible resistance\non its own part was undisturbed by any effort\nof its adversary, Scrooge observed that its light\nwas burning high and bright; and dimly\nconnecting that with its inﬂuence over him, he\nseized the extinguisher-cap, and by a sudden\naction pressed it down upon its head.\n0.600\nSara wants to say that she\nalready knows French, but\nshe doesn’t know how to say\nso and ends up giving Miss\nMinchin the impression that\nshe’s being difﬁcult and doesn’t\nwant to learn the language.\nMiss Minchin was a very severe and imposing\nperson, and she seemed so absolutely sure that\nSara knew nothing whatever of French that\nshe felt as if it would be almost\nrude to correct her.\n0.722\nEmerson still feels rough\nabout ruining the lecturer’s talk\nin the chapel.\nBut Mr. Emerson, contrite and unhappy,\nhurried away to apologize to the\nRev. Cuthbert Eager.\n0.486\nShe thinks Dinah should ﬁnd a\nnice man and settle down.\nAnd then you might get married to some decent\nman, and there’d be plenty ready to have you,\nif you’d only leave off that preaching, as is\nten times worse than anything your\nAunt Judith ever did.\n0.334\nAccording to him, the driftwood\nis dry and ideal for starting a ﬁre.\nIt is now dry and would\nburn like tinder. 0.630\nDetectives were sent to each port\nin England to see if the money might\nbe recovered.\nAs soon as the robbery was discovered, picked\ndetectives hastened off to Liverpool, Glasgow,\nHavre, Suez, Brindisi, New York, and other ports,\ninspired by the proffered reward of two thousand\npounds, and ﬁve per cent. on the sum that might\nbe recovered.\n0.618\nTable 4: Example showing the alignment of summary with full-text sentences, the score is cosine similarity. The\nexamples are all chosen because the previously used SRL event extraction fails with this approach and matches\nincorrect sentence and the examples show different strengthes of matches and types of sentence.",
  "topic": "Salience (neuroscience)",
  "concepts": [
    {
      "name": "Salience (neuroscience)",
      "score": 0.8915698528289795
    },
    {
      "name": "Surprise",
      "score": 0.8552592992782593
    },
    {
      "name": "Computer science",
      "score": 0.7586520910263062
    },
    {
      "name": "Narrative",
      "score": 0.6412099599838257
    },
    {
      "name": "Natural language processing",
      "score": 0.575130820274353
    },
    {
      "name": "Artificial intelligence",
      "score": 0.560664713382721
    },
    {
      "name": "Transformer",
      "score": 0.5015404224395752
    },
    {
      "name": "Annotation",
      "score": 0.44945070147514343
    },
    {
      "name": "Cognitive psychology",
      "score": 0.37815630435943604
    },
    {
      "name": "Linguistics",
      "score": 0.2045174539089203
    },
    {
      "name": "Psychology",
      "score": 0.1982276737689972
    },
    {
      "name": "Communication",
      "score": 0.11283084750175476
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": []
}