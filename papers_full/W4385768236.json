{
  "title": "Hierarchical Transformer for Scalable Graph Learning",
  "url": "https://openalex.org/W4385768236",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5101649969",
      "name": "Wenhao Zhu",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A5055005223",
      "name": "Tianyu Wen",
      "affiliations": [
        "Peking University",
        "Yuanpei University"
      ]
    },
    {
      "id": "https://openalex.org/A5088976879",
      "name": "Guojie Song",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A5100308734",
      "name": "Xiaojun Ma",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5100456543",
      "name": "Liang Wang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2153959628",
    "https://openalex.org/W3019011053",
    "https://openalex.org/W2900470550",
    "https://openalex.org/W2962711740",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W3196261868",
    "https://openalex.org/W2963695795",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W3101553402",
    "https://openalex.org/W3113177135",
    "https://openalex.org/W3169622372",
    "https://openalex.org/W4289389616",
    "https://openalex.org/W2790814121",
    "https://openalex.org/W4281706128",
    "https://openalex.org/W4221163422",
    "https://openalex.org/W4287123803",
    "https://openalex.org/W4304699884",
    "https://openalex.org/W2995345478",
    "https://openalex.org/W3095883070",
    "https://openalex.org/W4380993339",
    "https://openalex.org/W3100078588",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4294558607",
    "https://openalex.org/W3209214366",
    "https://openalex.org/W2963312446",
    "https://openalex.org/W2952029609",
    "https://openalex.org/W4287121301",
    "https://openalex.org/W2916106175",
    "https://openalex.org/W2961295589"
  ],
  "abstract": "Graph Transformer is gaining increasing attention in the field of machine learning and has demonstrated state-of-the-art performance on benchmarks for graph representation learning. However, as current implementations of Graph Transformer primarily focus on learning representations of small-scale graphs, the quadratic complexity of the global self-attention mechanism presents a challenge for full-batch training when applied to larger graphs. Additionally, conventional sampling-based methods fail to capture necessary high-level contextual information, resulting in a significant loss of performance. In this paper, we introduce the Hierarchical Scalable Graph Transformer (HSGT) as a solution to these challenges. HSGT successfully scales the Transformer architecture to node representation learning tasks on large-scale graphs, while maintaining high performance. By utilizing graph hierarchies constructed through coarsening techniques, HSGT efficiently updates and stores multi-scale information in node embeddings at different levels. Together with sampling-based training methods, HSGT effectively captures and aggregates multi-level information on the hierarchical graph using only Transformer blocks. Empirical evaluations demonstrate that HSGT achieves state-of-the-art performance on large-scale benchmarks with graphs containing millions of nodes with high efficiency.",
  "full_text": "Hierarchical Transformer for Scalable Graph Learning\nWenhao Zhu1 , Tianyu Wen2 , Guojie Song1 , Xiaojun Ma3 and Liang Wang4\n1National Key Laboratory of General Artificial Intelligence, School of Intelligence Science and\nTechnology, Peking University\n2Yuanpei College, Peking University\n3Microsoft\n4Alibaba Group\n{wenhaozhu, tianyuwen, gjsong}@pku.edu.cn, xiaojunma@microsoft.com, liangbo.wl@alibaba-inc.com\nAbstract\nGraph Transformer is gaining increasing attention\nin the field of machine learning and has demon-\nstrated state-of-the-art performance on benchmarks\nfor graph representation learning. However, as cur-\nrent implementations of Graph Transformer pri-\nmarily focus on learning representations of small-\nscale graphs, the quadratic complexity of the global\nself-attention mechanism presents a challenge for\nfull-batch training when applied to larger graphs.\nAdditionally, conventional sampling-based meth-\nods fail to capture necessary high-level contextual\ninformation, resulting in a significant loss of per-\nformance. In this paper, we introduce the Hier-\narchical Scalable Graph Transformer (HSGT) as\na solution to these challenges. HSGT success-\nfully scales the Transformer architecture to node\nrepresentation learning tasks on large-scale graphs,\nwhile maintaining high performance. By utilizing\ngraph hierarchies constructed through coarsening\ntechniques, HSGT efficiently updates and stores\nmulti-scale information in node embeddings at dif-\nferent levels. Together with sampling-based train-\ning methods, HSGT effectively captures and ag-\ngregates multi-level information on the hierarchical\ngraph using only Transformer blocks. Empirical\nevaluations demonstrate that HSGT achieves state-\nof-the-art performance on large-scale benchmarks\nwith graphs containing millions of nodes with high\nefficiency.\n1 Introduction\nTransformer [Vaswani et al., 2017] is now the prevalent uni-\nversal neural architecture in natural language processing and\ncomputer vision with its powerful, lowly inductive-biased\nself-attention mechanism. The great success of Transformer\nhas encouraged researchers to explore its adaptation to graph\nmachine learning on node-level and graph-level tasks [Ying\net al., 2021; Kreuzer et al., 2021; Chen et al., 2022]. While\nGNNs are known to suffer from inherent limitations in the\nPlease refer to https://arxiv.org/abs/2305.02866 for an extended\nversion of this paper.\nmessage-passing paradigm like over-smoothing and neighbor\nexplosion, the promising performance of these graph Trans-\nformer methods has encouraged researchers to expand the\nTransformer architecture to more scenarios.\nStill, challenges arise when scaling Transformer to large\ngraphs. In previous methods, self-attention calculates all pair-\nwise interactions in a graph, indicating that it has quadratic\ncomplexity to the total number of nodes. Thus, to per-\nform training on graphs of millions of nodes without sub-\nstantial modification to the Transformer architecture, one\nmust sample a properly sized subgraph at every batch so\nthat the computational graph can be fit into GPU memory.\nUsing sampling strategies like neighbor sampling in Graph-\nSAGE [Hamilton et al., 2017], we can build a simple scal-\nable graph Transformer model by directly applying existing\nmodels like Graphormer on the sampled subgraph. How-\never, there is an intrinsic weakness in this straightforward\ncombination of Transformer architecture and sampling-based\ntraining methods. It has been widely observed that high-\nlevel context information characterized by global receptive\nfield of self-attention module greatly contributes to Trans-\nformer’s outstanding performance [Vaswani et al., 2017;\nYing et al., 2021]. Considering that the entire input graph\nis usually far larger than every sampled subgraph, when the\nreceptive field of each node is restricted to the sampled local\ncontext, the model may ignore high-level context informa-\ntion, leading to possible performance loss. Meanwhile, if we\nadd globally sampled nodes to the sampled set to reduce con-\ntext locality, it is likely to introduce much redundant noise\nbecause most long-distance neighbors are irrelevant in large\ngraphs, which is further confirmed by our experiments.\nIn this paper, to alleviate the problem above and find\nthe real potentials of Transformer architecture on large-scale\ngraph learning tasks, we propose HSGT, a hierarchical scal-\nable graph Transformer framework. Our key insight is that,\nby building graph hierarchies with topological coarsening\nmethods, high-level context information can be efficiently\nstored and updated with a fused representation of high-level\nnode embeddings. As illustrated in Figure 1, through atten-\ntion interaction with nodes at higher hierarchical layers, the\nreceptive field of each node is expanded to a much higher\nscale, making it possible for HSGT to effectively capture\nhigh-level structural knowledge in the graph during sampling-\nbased training. Since the number of high-level nodes is\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n4702\n(a)                                    (b)     \nFigure 1: (a) During straightforward sampling-based training, recep-\ntive field of each node is restricted to the sampled context, leading\nto the loss of global context information. (b) In our method, through\nattention interaction with nodes at higher hierarchies, the receptive\nfield of each node is expanded to a higher scale (red shadows), mak-\ning it possible for the model to capture high-level knowledge.\nmarginal compared to the size of original graph, our approach\nis efficient and brings low extra computational cost. Be-\nsides, using adaptively aggregated representation to charac-\nterize high-level context information, our method is robust to\nrandom noise from long-range irrelevant neighbors.\nMore concretely, our proposed HSGT architecture utilizes\nthree types of Transformer blocks to support context trans-\nformations at different scales. At every hierarchical layer, the\nhorizontal blocks are first performed to exchange and trans-\nform information in the local context, then we use vertical\nblocks to aggregate representation in every substructure and\ncreate embeddings for nodes at the higher level. Eventu-\nally, readout blocks obtain final node representation by fusing\nmulti-level node embeddings. To achieve scalable training,\nwe have developed a hierarchical sampling method to sample\nmulti-level batches for training and inference, and utilized the\nhistorical embedding technique [Fey et al., 2021] to remove\ninter-batch dependencies and prune the computational graph.\nBeing completely Transformer-based, the resulting HSGT\narchitecture is highly scalable and generalizable, achieving\nstate-of-the-art results on a wide range of datasets from the\nstandard Cora [Sen et al., 2008] to ogbn-products [Chiang et\nal., 2019] with millions of nodes, outperforming the standard\nscalable GNN and Transformer baselines. We summarize our\nmain contributions as follows:\n• We propose HSGT, a new graph Transformer architec-\nture that efficiently generates high-quality representa-\ntions for graphs of varying sizes via effective usage of\nhierarchical structure and multi-level network design.\n• We develop the novel hierarchical sampling strategies\nand apply the historical embedding method, which al-\nlow HSGT to be trained efficiently large-scale graphs\nand gain high performance.\n• Extensive experiments show that HSGT achieves state-\nof-the-art performance against baseline methods on\nlarge-scale graph benchmarks with computational costs\nsimilar to the standard GraphSAGE method.\n2 Related Work\n2.1 Graph Transformers\nAlong with the recent surge of Transformer, many prior\nworks have attempted to bring Transformer architecture to the\ngraph domain, including GT [Dwivedi and Bresson, 2020 ],\nGROVER [Rong et al., 2020 ], Graphormer [Ying et al.,\n2021], SAN [Kreuzer et al., 2021], SAT [Chen et al., 2022],\nANS-GT [Zhang et al., 2022], GraphGPS [Ramp´aˇsek et al.,\n2022] and NodeFormer [Wu et al., 2022]. Graphormer [Ying\net al., 2021] proposes an enhanced Transformer with central-\nity, spatial and edge encodings, and achieves state-of-the-art\nperformance on many molecular graph representation learn-\ning benchmarks. SAN [Kreuzer et al., 2021 ] presents a\nlearned positional encoding that cooperates with full Lapla-\ncian spectrum to learn the position of each node in the graph.\nGophormer [Zhao et al., 2021] applies structural-enhanced\nTransformer to sampled ego-graphs to improve node clas-\nsification performance and scalability. SAT [Chen et al.,\n2022] studies the question of how to encode structural in-\nformation to Transformers and proposes the Structure-Aware-\nTransformer to generate position-aware information for graph\ndata. ANS-GT [Zhang et al., 2022] proposes an adaptive\nsampling strategy to effectively scale up graph Transformer\nto large graphs.\n2.2 Scalable Graph Learning\nModeling complex, real-world graphs with large scale require\nscalable graph neural models. On large graphs, message-\npassing GNNs mainly suffer from the neighbor explosion\nphenomenon, since the neighborhood dependency of nodes\ngrows exponentially as the model depth increases, which re-\nsults in the excessive expansion of computational graphs.\nSampling-based methods [Hamilton et al., 2017; Chen et\nal., 2018; Chen et al., 2017; Chiang et al., 2019; Zeng et\nal., 2019; Huang et al., 2018] generally solve this issue by\nrunning model on the sampled subgraph batches, and offline\npropagation methods [Wu et al., 2019; Klicperaet al., 2018;\nFrasca et al., 2020; Zhang et al., 2021] achieve fast training\nand inference by decoupling feature propagation from predic-\ntion as a pre-processing step. Notably, historical embedding\nmethods [Chen et al., 2017; Fey et al., 2021] store interme-\ndiate node embeddings from previous training iterations and\nuse them as approximations for accurate embeddings.\n3 Preliminaries\nIn this section we present some model backgrounds and ba-\nsic notations. Let G = (V, E) denote a graph, where V =\n{v1, v2, . . . , vn} is the node set that consists of n vertices\nand E ⊂ V × Vis the edge set. For node v ∈ V, let\nN(v) = {v′ : v′ ∈ V, (v, v′) ∈ E}denote the set of its\nneighbors. Let each node vi be associated with a feature\nvector xi ∈ RF where F is the hidden dimension, and let\nX = [x1, x2, . . . ,xn]⊤ ∈ Rn×F denote the feature matrix.\n3.1 Transformer\nStandard Transformer Layers. Transformer [Vaswani et\nal., 2017 ] is first proposed to model sequential text data\nwith consecutive Transformer layers, each of which mainly\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n4703\nconsists of a multi-head self-attention (MHA) module and a\nposition-wise feed-forward network (FFN) with residual con-\nnections. For queries Q ∈ Rnq×d, keys K ∈ Rnk×d and\nvalues V ∈ Rnk×d, the scaled dot-product attention module\ncan be defined as\nAttention(Q, K, V ) =softmax(A)V , A = QK⊤\n√\nd\n, (1)\nwhere nq, nk are number of elements in queries and keys, and\nd is the hidden dimension. After multi-head attention, the\nposition-wise feed-forward network and layer normalization\nare performed on the output.\nBiased Transformer Layers. A bias term can be added to\nthe attention weights A to represent pair-wise knowledge like\nrelative positional encodings in [Shaw et al., 2018]. Suppose\nwe have a bias matrix B ∈ Rnq×nk, the biased-MHA can be\nformulated by replacing the standard attention module with\nthe attention weight matrix computed by A = QK⊤\n√\nd + B.\n3.2 Graph Hierarchies\nFor graph G0 = (V 0, E0), graph coarsening aims to find\nG1 = (V 1, E1) that captures the essential substructures of\nG0 and is significantly smaller (|V 1| ≪ |V 0|,|E1| ≪ |E0|).\nWe assume graph coarsening is performed by coalescing\nnodes with a surjective mapping function ϕ : V 0 → V 1. Ev-\nery node v1\ni ∈ V 1 corresponds to a node cluster ϕ−1(v1\ni ) =\n{v0\nj ∈ V 0 : ϕ(v0\nj ) = v1\ni } in G0, and the edge set of\nG1 is defined as E1 = {(v1\ni , v1\nj ) : ∃v0\nr ∈ ϕ−1(v1\ni ), v0\ns ∈\nϕ−1(v1\nj ), such that (v0\nr, v0\ns) ∈ E0}. We also initialize node\nembeddings of G1 by x1\ni = Mean({x0\nj : v0\nj ∈ ϕ−1(v1\ni )})\nfor every node v1\ni in V 1. The coarsening ratio α at this step\nis defined as α = |V 1|\n|V 0|. By running the coarsening algorithm\nrecursively, a graph hierarchy{G0, G1, . . . , GH} can be con-\nstructed to summarize multi-level structures.\n4 Proposed Approach\nIn the following section, we will describe the motivation and\napproach behind the creation of our HSGT model, and sub-\nsequently provide a detailed description of the architecture of\nthe entire model. Figure 2 gives a high-level illustration of\nmodel architecture, and Algorithm 1 describes HSGT.\n4.1 Motivation for Utilizing Graph Hierarchies\nThe key difference between HSGT and previous graph Trans-\nformers is the utilization of graph hierarchical structure. Pre-\nvious methods for leveraging high-level context information\ncan only expand the receptive field and increase sample size,\nwhich leads to significant computational overhead and per-\nformance loss on node-level tasks. In contrast, HSGT utilizes\ngraph hierarchies to communicate high-level information via\nthe embedding of virtual nodes, resulting in several key ben-\nefits: (1) low computational cost: the number of high-level\nvirtual nodes is minimal and the updating process is com-\npleted via efficient sampling strategies and historical embed-\nding; (2) broad receptive field: the receptive field of a single\nnode includes any node related to its corresponding high-level\nAlgorithm 1 Overview of HSGT\nInput: Input graph G = ( V, E), with corresponding\nhierarchy {G0, G1, . . . , GH}, initial feature matrices\nX0, X1, . . . ,XH and hierarchical mappings ϕ1, . . . , ϕH,\nbatch size B.\nOutput: embedding hv for every node v.\n1: V H\nleft ← V H, where GH = (V H, EH).\n2: while V H\nleft is not empty do\n3: Sample V H\nB from V H\nleft with |V H\nB | = B.\n4: V H\nleft ← V H \\ V H\nB .\n5: for j in 1, 2, . . . , Hdo\n6: ˜V j−1\nB ← ϕ−1\nj (V j\nB).\n7: V j−1\nB ← NeighborSample( ˜V j−1\nB ). (Section 4.3)\n8: Hj−1 ← Xj−1[V j−1\nB ].\n9: end for\n10: for j in 0, 1, 2, . . . , Hdo\n11: Hj ← HorizontalBlock(Hj).\n12: if j < Hthen\n13: Hj+1 ← VerticalBlock(Hj, Hj+1).\n14: end if\n15: end for\n16: H0 ← ReadoutBlock(H0, H1, . . . ,HH).\n17: hv ← H0[v], ∀v ∈ V H\nB .\n18: end while\n19: return hv, ∀v ∈ V .\nnodes; (3) high flexibility: one can build the graph hierarchy\nusing any graph coarsening algorithm, and arbitrarily choose\nthe sampling strategy to control the structural information in-\nvolved in the learning process. In the following paragraphs,\nwe will elaborate on the design of the model and demonstrate\nits effectiveness through experiments.\n4.2 Model Architecture\nGraph Hierarchy Construction and Input\nTransformation\nFor input graph G0 = (V 0, E0) with feature matrix X,\nwe first use the chosen coarsening algorithm to produce the\ngraph hierarchy {G0, G1, . . . , GH} with initial feature ma-\ntrices X0, X1, . . . ,XH and corresponding hierarchical map-\npings ϕ1, . . . , ϕH. The number of hierarchical layers H\nand the coarsening ratios for each step α1, . . . , αH are pre-\ndefined as hyperparameters. The model imposes no limita-\ntions on the specific graph coarsening algorithm. In our im-\nplementation, we choose METIS [Karypis and Kumar, 1998]\nwhich is designed to partition a graph into mutually exclusive\ngroups and minimize the frequency of inter-links between\ndifferent groups. The chosen modern METIS algorithm is\nfast and highly scalable with time complexity approximately\nbounded by O(|E|), and only needs to compute once at the\npre-processing stage. In experiments, even partitioning of\nthe largest ogbn-products graph is finished within 5 minutes,\nbringing almost no overhead to the entire training process.\nWe also apply linear transformations and degree encod-\nings for initial features before they are fed into Transformer\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n4704\nInput Graph\n...\nOutputHorizontal \n   Block   \nHorizontal \n  Block   \n...\nReadout \nBlock\n...\n...\n...\n...\nLayer \nLayer\nLayer\nNode Embedding\n...\nVertical \n Block   \nFigure 2: A high-level illustration of the proposed HSGT model architecture. At every hierarchical layer, a horizontal block first exchanges\nand transforms information in each node’s local context, then a vertical block is performed to adaptively coalesce every substructure if a\nhigher layer exists. Finally, a readout block aggregates multi-level representations to calculate the final output.\nblocks. All feature vectors in X0, X1, . . . ,XH are projected\ninto Rd using a linear layer, whered is the hidden size. Addi-\ntionally, inspired by centrality encoding in[Ying et al., 2021],\nwe add learnable embedding vectors indexed by node degree\nto the transformed node feature at layer 0 to represent struc-\ntural information.\nHorizontal Block\nIn every horizontal block, we aim to horizontally aggregate\nand transform node representations in every node’s local con-\ntext using structural-enhanced Transformer layers. Suppose\nthe input graph is G = (V, E) with n nodes, the feature ma-\ntrix is H, and every node v has an individual local receptive\nfield R(v) ⊂ V which we will discuss later. Following [Ying\net al., 2021], to leverage graph structure into self-attention,\nwe choose to quantify the connectivity between nodes with\nthe distance of the shortest path (SPD), and use an attention\nbias term to represent the structural information. To reduce\nthe computational cost, we set a maximum SPD length D\nsuch that SPDs longer than D will not be computed. We also\nmask nodes not in R(v) out of the receptive field of v by set-\nting the corresponding bias term to −∞. Formally, the bias\nmatrix B ∈ Rn×n is defined as\nBi,j =\n\n\n\nbSPD(vi,vj), if vj ∈ R(vi), SPD(vi, vj) ≤ D,\n−∞, if vj /∈ R(vi),\n0, else.\nwhere Bi,j is the (i, j)-element of B, b0, b1, . . . , bD ∈ R are\na series of learnable scalars. Then the horizontal block is built\nby stacking the following Transfomer layers:\nH = Biased-Transformer(H, H, H, B). (2)\nCompared with GNNs, our method promotes a high-order\nknowledge aggregation in a broader context while leveraging\nthe complete structural information. In the case of full-batch\ntraining on small graphs, every node can have a global recep-\ntive field, i.e. R(v) =V, ∀v ∈ V . But during sampling-based\ntraining on huge graphs, the receptive field of each node is re-\nstricted to the current batch VB ⊂ V , and we empirically dis-\ncover that due to the unbalance and irregularity of sampling\nmethods, computing every pair-wise attention inVB will lead\nto significant performance drop on node-level tasks. In the\nmean time, intra-batch communication will be limited if we\nset the receptive field of each node to its local neighbors.\nTo balance the two aspects, we choose to form the recep-\ntive field R(v) of node v with its D-hop neighbors ND(v)\nand nodes individually randomly sampled from VB by prob-\nability p. Besides, during experiments we discover that by\nsharing parameters among horizontal blocks at different hier-\narchical layers, we can greatly reduce the number of model\nparameters while the model performance is not affected. This\nis probably due to the structural similarity between graphs at\ndifferent hierarchical levels and the strong expressive capac-\nity of Transformer layers. We will test the effectiveness of the\ntwo strategies in ablation studies.\nVertical Block\nThe vertical block focuses on aggregating node representa-\ntions produced by the previous horizontal block and gener-\nating embeddings for nodes at next hierarchical level. In\ncontrast to simple pooling functions like mean and sum, to\novercome their incapability of capturing important nodes and\nsubstructures, we reuse the attention mechanism to adaptively\nmerge vector embeddings. Suppose the vertical block aims to\ncalculate embeddings for nodes in Gi+1. For node v ∈ V i+1\nwith transformed initial featurexv, its representation hv after\nthe vertical aggregation is computed as\nNv = Stack({hs : s ∈ ϕ−1\ni+1(v)}), (3)\nhv = Transformer(xv, Nv, Nv), (4)\nwhere xv, hv are viewed as matrices inR1×d, and the vertical\nblock computes representation for every node in the input.\nThis aggregation scheme allows every fused representation\nhv to contain meaningful information on its corresponding\nlow-level substructure, helping the next horizontal block to\nachieve better high-level knowledge exchange.\nReadout Block\nAfter the horizontal blocks are performed on every hierar-\nchical level, we use the readout block to fuse representations\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n4705\nTarget Node\nNeighborhood Node\nNode out of the \nSampled Context\nGraph Partition\nFigure 3: An illustration of the proposed sampling method.\nand produce final node embeddings. This operation brings\nwell aggregated multi-level information to nodes in the orig-\ninal graph, expanding their receptive field to a much higher\nscale. Formally, let hr denote the embedding of node r at\nlevel l generated by the l-th horizontal block, then for a node\nv ∈ V 0, its final output embedding\nhv is calculated through\nattention with its corresponding high-level nodes:\nZv = [ht0 , ht1 , . . . ,htH ]⊤, (5)\nhv = Transformer(hv, Zv, Zv), (6)\nwhere t0 = v, tj = ϕj(tj−1), for j = 1, . . . , H.\n4.3 Training Strategy\nHierarchical Sampling\nWhen training on large-scale graphs, at every step, our model\ncan only be operated on a sampled batch{G0\nB, G1\nB, . . . , GH\nB }\nof the entire graph hierarchy {G0, G1, . . . , GH}, that Gi\nB is\na subgraph of Gi for every i = 0, 1, . . . , H. To keep ev-\nery substructure in low-level hierarchical layers complete,\nwe follow a top-to-bottom sampling approach. For batch\nsize b, we first randomly sample b nodes at V H as ˜V H\nB ,\nthen recursively sample nodes from layer H − 1 to 0 in\n˜V i−1\nB = S\nv∈˜V i\nB\nϕ−1\ni (v), i= H, H− 1, . . . ,1. Here we call\nnodes in {˜V 0\nB, ˜V 1\nB, . . . ,˜V H\nB } target nodes, because only rep-\nresentations of nodes in ˜V 0\nB will be used for supervised learn-\ning and {˜V 1\nB, . . . ,˜V H\nB } contains all high-level nodes they re-\nlate to. Meanwhile, to promote local context interaction in\nhorizontal blocks, we additionally sample a neighborhood\nset for every target node at all levels using neighbor sam-\npling. An illustration of this process is presented in Figure 3.\nWe construct the final sampled nodes set {V 0\nB, V1\nB, . . . , VH\nB }\nby adding the neighborhood sets to the target nodes set,\nand {G0\nB, G1\nB, . . . , GH\nB } are the corresponding induced sub-\ngraphs. The resulting sampling strategy allows the model to\noperate on complete hierarchical structures with local con-\ntext preserved, which is critical for the horizontal and vertical\nmodules to work well.\nHistorical Embeddings for High-level Nodes\nThe sampling scheme above could cause issues. For exam-\nple, for neighborhood node v ∈ V 1\nB \\ ˜V 1\nB, it is very likely that\nmost of its corresponding nodes at layer 0 will not appear in\nthe sampled nodes set V 0\nB, since we do not deliberately add\nϕ−1\n1 (v) to the sampled set as the target nodes do. Thus, it\nis not possible to directly get the representation of v by ag-\ngregating embeddings of nodes in ϕ−1\n1 (v) via vertical blocks\nwhen some of ϕ−1\n1 (v) do not exist in the sampled context.\nAnd manually adding those nodes to the data batch will lead\nto a massive expansion of the computational graph (almost\n10×). Nevertheless, if we skip the neighborhood sampling\nstep for high-level nodes, the inter-batch communication of\nstructural knowledge could be baffled, which contradicts with\nour initial goal.\nTo disentangle such multi-level dependencies, we utilize\nthe historical embedding method proposed in [Chen et al.,\n2017; Fey et al., 2021] to alleviate the neighbor explosion\nin GNNs. In our model, the historical embeddings act as an\noffline storage S of high-level nodes (above level 0), which\nis accessed and updated at every batch using push and pull\noperations. At every batch, the vertical blocks are only per-\nformed on the target nodes, and we push the newly aggregated\nembeddings for high-level target nodes to S. For horizontal\nblocks on high-level nodes, we approximate the embeddings\nof neighborhood nodes via pulling historical embeddings in\nS acquired in previous batches. Specifically, for horizontal\nblock i, its input H will be computed as\nH = Stack({hs : s ∈ V i\nB}), (7)\n≈ Stack({hs : s ∈ ˜V i\nB} ∪ {˜hs : s ∈ V i\nB \\ ˜V i\nB}), (8)\nwhere hs denotes accurate embedding of s calculated by the\nprevious horizontal block, and ˜hs denotes the historical em-\nbedding of s from previous batches. With historical embed-\ndings, we enable the inter-batch communication of high-level\ncontexts with low extra computational cost and high accu-\nracy bounded by theoretical results in [Chen et al., 2017;\nFey et al., 2021]. Our approach is the first implementation\nof the historical embedding method on graph Transformer\nmodels, and its effectiveness is further demonstrated by the\nablation studies.\n5 Experiments\nIn this section we first evaluate HSGT on different benchmark\ntasks, and then perform ablation studies, scalability tests, and\nparameter analysis.\n5.1 Node Classification Tasks\nDatasets. We conduct experiments on nine benchmark\ndatasets including four small-scale datasets (Cora, CiteSeer,\nPubMed [Sen et al., 2008; Yanget al., 2016], Amazon-Photo\n[Shchur et al., 2018 ]) and six large-scale datasets (ogbn-\narxiv, ogbn-proteins, ogbn-products [Hu et al., 2020], Reddit\n[Hamilton et al., 2017], Flickr, Yelp [Zeng et al., 2019]). We\nuse the predefined dataset split if possible, or we set a random\n1:1:8 train/valid/test split.\nBaselines and Settings. We compare HSGT against a wide\nrange of baseline scalable graph learning methods includ-\ning GCN [Kipf and Welling, 2016 ], GAT [Veliˇckovi´c et\nal., 2017 ], GIN [Xu et al., 2018], GraphSAGE [Hamilton\net al., 2017 ], Cluster-GCN [Chiang et al., 2019 ], Graph-\nSAINT [Zeng et al., 2019], GAS-GCN [Fey et al., 2021],\nSIGN [Frasca et al., 2020], GraphZoom [Deng et al., 2019]\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n4706\n#nodes 2.7K 3.3K\n19.7K 7.6K 169K 233K\n89.2K 716K 133K 2.4M\n#edges 5.2K 4.5K\n44.3K 119K 1.1M 11.6M\n450K 7.0M 40M 61.8M\nDataset COR\nA CITE SEER PUBMED AMAZON -PHOTO ogbn-arxiv RED\nDIT FLICKR YELP ogbn-proteins ogbn-products\nGCN 77.20±1.51 69.49±0.58 77.60±0.96 92.44±0.22 71.74±0.29* 91.01±0.29 51.86±0.10 32.14±0.66 72.51±0.35* 75.64±0.21*\nGAT 82.80±0.47 69.20±0.45 76.90±0.85 92.88±0.37 57.88±0.18 96.50±0.14 52.39±0.05 61.58±1.37 72.02±0.44* 79.45±0.59\nGIN 75.93±0.99 63.83±0.49 77.03±0.42 80.14±1.46 52.23±0.34 86.37±0.62 48.28±0.85 29.75±0.86 70.76±0.08 74.79±0.81\nGraphSAGE - - - - 71.49±0.27* 96.53±0.11 51.86±0.35 53.89±0.85 77.68±0.20* 78.50±0.14*\nCluster-GCN - - - - 69.76±0.49* 95.12±0.08 50.25±0.83 52.50±0.19 74.89±0.12 78.97±0.33*\nGraphSAINT - - - - 58.63±0.33 90.92±0.61 51.91±0.06 56.22±1.14 70.22±0.84 79.08±0.24*\nGAS-GCN 82.29±0.76* 71.18±0.97* 79.23±0.62* 90.53±1.40* 71.68* 95.45*\n54.00* 62.94* - 76.66*\nSIGN - - - - - 96.8\n±0.0* 51.4±0.1* 63.1±0.3* - 77.60±0.13*\nGraphZoom - - - - 71.18±0.18* 92.5* - - - 74.06\n±0.26*\nGraphormer 66.35±2.44 56.22±3.27 OOM OOM OOM OOM\nOOM OOM OOM OOM\nGraphormer-SAMPLE 75.14±1.31 61.46±1.90 75.45±0.98 92.76±0.59 70.43±0.20 93.05±0.22 51.93±0.21 60.01±0.45 72.34±0.51 79.10±0.12\nSAN 36.61±3.49 44.35±1.08 OOM OOM OOM OOM\nOOM OOM OOM OOM\nSAT 72.40±0.31 60.93±1.25 OOM OOM OOM OOM\nOOM OOM OOM OOM\nSAT-SAMPLE 74.55±1.24 61.58±0.87 76.70±0.74 91.35±0.42 68.20±0.46 93.37±0.32 50.48±0.34 60.32±0.65 70.62±0.85 77.64±0.20\nANS-GT 79.35±0.90 64.52±0.71 77.80±0.65 80.41±0.78 72.34±0.50 95.30±0.81 - -\n74.67±0.65 80.64±0.29\nHSGT 83.56±1.77 67.41±0.92 79.65±0.52 95.01±0.34 72.58±0.31 97.30±0.24 54.12±0.51 63.47±0.45 78.13±0.25 81.15±0.13\nTable 1: Results on node classification datasets. OOM stands for out of memory. * indicates results cited from the original papers and the\nOGB leaderboard.\nand graph Transformers including Graphormer [Ying et al.,\n2021], SAN [Kreuzer et al., 2021], SAT [Chen et al., 2022]\nand ANS-GT [Zhang et al., 2022]. For GCN, GAT and GIN,\nwe perform full-batch training on small-scale datasets and\nsampling-based training on large-scale datasets. By default,\nGraphormer, SAN and SAT require full-batch training, which\nis prohibited by GPU memory bound in most cases. We also\nadd the Graphormer-S AMPLE and SAT-SAMPLE baselines\nthat perform the model on subgraphs generated from neigh-\nborhood sampling, as mentioned in the introduction. For all\nexperiments, the overhead of preprocessing steps (including\nMETIS partition) does not exceed 5 minutes. The detailed\nsettings for baselines and HSGT are listed in the appendix.\nWe report the means and standard deviations of performances\non test set initialized by three different random seeds.\nResults. We present the node classification performances in\nTable 1, where the metric for OGBN -PROTEINS is roc-auc\nwhile the metric for other datasets is acc. On small-scale\ndatasets where graph Transformer baselines are mostly out-\nperformed by GNN methods, HSGT delivers competitive per-\nformance against the GNN baselines. While on large-scale\ndatasets, HSGT performs consistently better than all GNN\nbaselines, achieving state-of-the-art and showing that the\nTransformer architecture is well capable of handling large-\nscale node-level tasks. Overall, the results have also demon-\nstrated the outstanding generalizability of the HSGT method\non multi-scale graph data.\nIt can also be observed that Transformers generally per-\nform bad at node-level tasks on small graphs probably be-\ncause the global attention introduces much irrelevant noise.\nAnd the Graphormer-S AMPLE and SAT-SAMPLE baseline\nfail to produce satisfactory results on multi-level benchmarks\nsince a naive sampling-based approach can not capture the\nnecessary high-level contextual information. On the contrary,\non all datasets HSGT performs significantly better than the\nGraphormer-SAMPLE and SAT-SAMPLE baseline, showing\nthe effectiveness of our proposed graph hierarchical struc-\nture and training strategies. Notably, the performance gains\nof HSGT are greater on large-scale datasets than small-scale\nones, indicating that a large amount of data is crucial in opti-\nmizing the performance potentials of Transformer.\n5.2 Ablation Studies\nSettings. We design four HSGT variants to demonstrate\nthe benefits of vertical blocks, structural encodings, histori-\ncal embeddings and readout blocks, respectively, while other\nmodel settings stay unchanged. In Table 2, w/o vertical\nblocks: the vertical feature aggregation is performed with the\nsimple mean function, instead of a Transformer block. w/o\nstructural encodings: all SPD attention biases are removed.\nw/o historical embeddings: neighborhood nodes of high-level\nnodes are no longer sampled, then historical embeddings are\nno longer needed. w/o readout blocks: the multi-level read-\nout is performed by concatenating feature vectors at different\nlevels. w/o parameter sharing: all horizontal blocks and ver-\ntical blocks have an individual set of parameters. random\npartition: the coarsening process is performed via random\npartition, instead of METIS algorithm.\nFLI\nCKR YELP ogbn-products\nw/o vertical\nblocks 52.85 62.46\n78.01\nw/o structural encodings 49.11 59.78\n77.05\nw/o historical embeddings 52.04 61.79\n80.72\nw/o readout blocks 52.58 62.01\n80.34\nw/o parameter sharing 53.01 63.44\n81.13\nrandom partition 43.21 60.77\n75.23\nHSGT 53.02 63.47\n81.15\nTable 2: Results of ablation studies.\nResults. Table 2 summarizes the results of ablation stud-\nies. Most variants suffer from performance loss, showing that\nall tested modules are necessary to raise the performance of\nHSGT to its best level. If we replace Transformer layer in\nvertical and readout blocks with simple operations like mean,\nthen multi-scale information can not be adaptively fused. And\nthe model will not be able to recognize graph structure when\nwe remove the necessary structural encodings, which ex-\nplains the severe performance drop we witness. It can also\nbe observed that if we take out the neighborhood sampling\nof high-level nodes to avoid historical embeddings, the high-\nlevel context information exchange can be blocked, resulting\nin performance drop on large-scale datasets. When individual\nlearnable parameters are assigned to horizontal blocks at dif-\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n4707\nModel Settings Peak\nGPU Memory Usage #Parameters Inference Time Performance\nGraphSAGE\nl = 2,\nd = 64 1064MB 16.4K 48.6s 75.60\nl = 2,\nd = 128 1150MB 65.5K 53.7s 77.38\nl = 2,\nd = 256 1316MB 262.1K 70.2s 77.58\nl = 3,\nd = 128 1928MB 98.3K 103.3s 78.25\nGraphormer-\nSAMPLE l = 2,\nd = 128 1874MB 223.6K 90.6s 63.44\nl = 2,\nd = 256 2032MB 840.2K 93.5s 64.76\nHSGT\nl = 2,\nd = 64 1903MB 112.7K 97.4s 80.99\nl = 2,\nd = 128 2208MB 421.1K 99.8s 81.10\nl = 3,\nd = 64 3374MB 137.7K 108.2s 81.15\nTable 3: Results of scalability tests on ogbn-products dataset.\nferent levels, the model performance is almost not affected\nwhile the number of parameters could increase by at least\n1.5×. A random coarsening approach also causes the model\nperformance to drop dramatically because HSGT requires\nhigh-quality structural partitions calculated by the METIS al-\ngorithm to capture and aggregate high-level information.\n5.3 HSGT Efficiently Scales to Large Graphs\nSettings. To comprehensively examine HSGT’s scalabil-\nity to large-scale graphs, we perform tests on the largest\nogbn-products dataset that contains over 2.4 mil-\nlion nodes against the standard GraphSAGE method and\nGraphormer-SAMPLE above. To give a fair comparison, for\nGraphSAGE and Graphormer-SAMPLE we set the batch size\nto 400, and for HSGT we keep layer 0 nodes per batch to\naround 400. In Table 3, l stands for the number of layers\nfor GraphSAGE and Graphormer-SAMPLE , while number of\nTransformer layers at horizontal blocks for HSGT. d stands\nfor the hidden dimension for all models. Other model pa-\nrameters stay the same with experiments in Table 1. We use\nbuilt-in PyTorch CUDA tools to measure peak GPU memory\nusage during experiments. For the number of parameters, we\ncalculate the number of all learnable model parameters except\ninput and output projections.\nResults. In Table 3 we list the results. Traditionally, it is\nbelieved that the Transformer architecture tends to achieve\nhigh performance with high computational complexity and\nlots of parameters. However, experimental results show that\nthe proposed HSGT model can achieve outstanding perfor-\nmance with reasonable costs that are similar to the widely-\nused GraphSAGE method, showing that HSGT can be ef-\nficiently scaled to large graphs with normal computational\nresources. Even under the lightest setting l = 2, d= 64,\nHSGT is capable of delivering results higher than all GNN\nbaselines while keeping moderate GPU memory usage and\nparameter size, which can be attributed to the small hidden\nsize (64) and the parameter sharing among horizontal blocks.\nThe Graphormer-SAMPLE may cost fewer resources at light-\nweight configurations since HSGT has the additional horizon-\ntal and vertical blocks, but it is significantly outperformed by\nboth GraphSAGE and HSGT.\n5.4 Parameter Analysis\nCoarsening Ratios\nThe coarsening ratios α1, . . . , αH are used as parameters\nfor the METIS algorithm to generate initial graph hierarchy\n{G0, G1, . . . , GH}. Normally we set the number of addi-\ntional hierarchical layers H to 1 or 2, and coarsening ratios\nare picked from {0.2, 0.1, 0.05, 0.02, 0.01, 0.005}. Other set-\ntings stay the same with models in Table 1. Here we per-\nform experiments on dataset Flickr, Yelp and ogbn-products\nto study the influence of coarsening ratios, and we list the\nresults in Table 4. It can be observed that the best setting\nfor coarsening ratios and batch size could vary for different\ndatasets.\nCoarsening Ratios FLI\nCKR YELP ogbn-products\n{0.005} 52.71 61.98 81.15\n{0.002} 51.32 61.44\n80.55\n{0.1, 0.1} 47.59 62.95\nOOM\n{0.1, 0.2} 47.03 63.47 OOM\nTable 4: Results of coarsening ratio tests.\nIntra-batch Connectivity\nAt previous sections we have mentioned that in horizontal\nblocks, we construct the receptive field of each node with\nits D-hop neighbors and nodes randomly sampled from the\nsampled batch by probability p. Here we study the impact\nof p value with experiments and summarize the results in Ta-\nble 5, where other settings stay the same with those in Table\n1. From the results we can see that as p varies from 0 to 1,\nthe model performance generally increases until it reaches a\npeak and then decreases, which corresponds to our previous\nanalysis.\nIntra-batch Connecti\nvity FLI\nCKR YELP ogbn-products\np = 0.0 51.04 62.83\n80.45\np = 0.1 50.80 63.47 81.15\np =\n0.3 53.02 62.92 80.67\np =\n0.5 48.47 60.41\n80.14\np = 1.0 45.83 60.74\n78.65\nTable 5: Results of intra-batch connectivity tests.\n6 Conclusion\nIn this paper we propose HSGT, a Transformer-based neural\narchitecture for scalable graph learning, and our model has\nshown strong performance and generalizability on multi-scale\ngraph benchmarks with reasonable computational costs.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n4708\nAcknowledgements\nThis work was supported by the National Natural Science\nFoundation of China (Grant No. 62276006).\nReferences\n[Chen et al., 2017] Jianfei Chen, Jun Zhu, and Le Song.\nStochastic training of graph convolutional networks with\nvariance reduction. arXiv preprint arXiv:1710.10568,\n2017.\n[Chen et al., 2018] Jie Chen, Tengfei Ma, and Cao Xiao.\nFastgcn: fast learning with graph convolutional net-\nworks via importance sampling. arXiv preprint\narXiv:1801.10247, 2018.\n[Chen et al., 2022] Dexiong Chen, Leslie O’Bray, and\nKarsten Borgwardt. Structure-aware transformer for graph\nrepresentation learning. In International Conference on\nMachine Learning, pages 3469–3489. PMLR, 2022.\n[Chiang et al., 2019] Wei-Lin Chiang, Xuanqing Liu, Si Si,\nYang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn:\nAn efficient algorithm for training deep and large graph\nconvolutional networks. In Proceedings of the 25th ACM\nSIGKDD International Conference on Knowledge Discov-\nery & Data Mining, pages 257–266, 2019.\n[Deng et al., 2019] Chenhui Deng, Zhiqiang Zhao, Yongyu\nWang, Zhiru Zhang, and Zhuo Feng. Graphzoom: A\nmulti-level spectral approach for accurate and scalable\ngraph embedding. arXiv preprint arXiv:1910.02370,\n2019.\n[Dwivedi and Bresson, 2020] Vijay Prakash Dwivedi and\nXavier Bresson. A generalization of transformer networks\nto graphs. arXiv preprint arXiv:2012.09699, 2020.\n[Fey et al., 2021] Matthias Fey, Jan E Lenssen, Frank We-\nichert, and Jure Leskovec. Gnnautoscale: Scalable and ex-\npressive graph neural networks via historical embeddings.\narXiv preprint arXiv:2106.05609, 2021.\n[Frasca et al., 2020] Fabrizio Frasca, Emanuele Rossi, Da-\nvide Eynard, Ben Chamberlain, Michael Bronstein, and\nFederico Monti. Sign: Scalable inception graph neural\nnetworks. arXiv preprint arXiv:2004.11198, 2020.\n[Hamilton et al., 2017] Will Hamilton, Zhitao Ying, and Jure\nLeskovec. Inductive representation learning on large\ngraphs. Advances in neural information processing sys-\ntems, 30, 2017.\n[Hu et al., 2020] Weihua Hu, Matthias Fey, Marinka Zit-\nnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele\nCatasta, and Jure Leskovec. Open graph benchmark:\nDatasets for machine learning on graphs. arXiv preprint\narXiv:2005.00687, 2020.\n[Huang et al., 2018] Wenbing Huang, Tong Zhang,\nYu Rong, and Junzhou Huang. Adaptive sampling\ntowards fast graph representation learning. arXiv preprint\narXiv:1809.05343, 2018.\n[Karypis and Kumar, 1998] George Karypis and Vipin Ku-\nmar. A software package for partitioning unstructured\ngraphs, partitioning meshes, and computing fill-reducing\norderings of sparse matrices. University of Minnesota,\nDepartment of Computer Science and Engineering, Army\nHPC Research Center, Minneapolis, MN, 38:7–1, 1998.\n[Kipf and Welling, 2016] Thomas N Kipf and Max Welling.\nSemi-supervised classification with graph convolutional\nnetworks. arXiv preprint arXiv:1609.02907, 2016.\n[Klicpera et al., 2018] Johannes Klicpera, Aleksandar Bo-\njchevski, and Stephan G ¨unnemann. Predict then propa-\ngate: Graph neural networks meet personalized pagerank.\narXiv preprint arXiv:1810.05997, 2018.\n[Kreuzer et al., 2021] Devin Kreuzer, Dominique Beaini,\nWilliam L Hamilton, Vincent L ´etourneau, and Prudencio\nTossou. Rethinking graph transformers with spectral at-\ntention. arXiv preprint arXiv:2106.03893, 2021.\n[Ramp´aˇsek et al., 2022] Ladislav Ramp ´aˇsek, Mikhail\nGalkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy\nWolf, and Dominique Beaini. Recipe for a general,\npowerful, scalable graph transformer. arXiv preprint\narXiv:2205.12454, 2022.\n[Rong et al., 2020] Yu Rong, Yatao Bian, Tingyang Xu,\nWeiyang Xie, Ying Wei, Wenbing Huang, and Junzhou\nHuang. Self-supervised graph transformer on large-scale\nmolecular data. arXiv preprint arXiv:2007.02835, 2020.\n[Sen et al., 2008] Prithviraj Sen, Galileo Namata, Mustafa\nBilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.\nCollective classification in network data. AI magazine,\n29(3):93–93, 2008.\n[Shaw et al., 2018] Peter Shaw, Jakob Uszkoreit, and Ashish\nVaswani. Self-attention with relative position representa-\ntions. arXiv preprint arXiv:1803.02155, 2018.\n[Shchur et al., 2018] Oleksandr Shchur, Maximil-\nian Mumme, Aleksandar Bojchevski, and Stephan\nG¨unnemann. Pitfalls of graph neural network evaluation.\narXiv preprint arXiv:1811.05868, 2018.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In Advances in neural information processing sys-\ntems, pages 5998–6008, 2017.\n[Veliˇckovi´c et al., 2017] Petar Veliˇckovi´c, Guillem Cucurull,\nArantxa Casanova, Adriana Romero, Pietro Lio, and\nYoshua Bengio. Graph attention networks. arXiv preprint\narXiv:1710.10903, 2017.\n[Wu et al., 2019] Felix Wu, Amauri Souza, Tianyi Zhang,\nChristopher Fifty, Tao Yu, and Kilian Weinberger. Simpli-\nfying graph convolutional networks. In International con-\nference on machine learning, pages 6861–6871. PMLR,\n2019.\n[Wu et al., 2022] Qitian Wu, Wentao Zhao, Zenan Li, David\nWipf, and Junchi Yan. Nodeformer: A scalable graph\nstructure learning transformer for node classification.\nIn Advances in Neural Information Processing Systems,\n2022.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n4709\n[Xu et al., 2018] Keyulu Xu, Weihua Hu, Jure Leskovec, and\nStefanie Jegelka. How powerful are graph neural net-\nworks? arXiv preprint arXiv:1810.00826, 2018.\n[Yang et al., 2016] Zhilin Yang, William Cohen, and Ruslan\nSalakhudinov. Revisiting semi-supervised learning with\ngraph embeddings. In International conference on ma-\nchine learning, pages 40–48. PMLR, 2016.\n[Ying et al., 2021] Chengxuan Ying, Tianle Cai, Shengjie\nLuo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen,\nand Tie-Yan Liu. Do transformers really perform bad for\ngraph representation? arXiv preprint arXiv:2106.05234,\n2021.\n[Zeng et al., 2019] Hanqing Zeng, Hongkuan Zhou, Ajitesh\nSrivastava, Rajgopal Kannan, and Viktor Prasanna. Graph-\nsaint: Graph sampling based inductive learning method.\narXiv preprint arXiv:1907.04931, 2019.\n[Zhang et al., 2021] Wentao Zhang, Ziqi Yin, Zeang Sheng,\nWen Ouyang, Xiaosen Li, Yangyu Tao, Zhi Yang, and\nBin Cui. Graph attention multi-layer perceptron. arXiv\npreprint arXiv:2108.10097, 2021.\n[Zhang et al., 2022] Zaixi Zhang, Qi Liu, Qingyong Hu, and\nChee-Kong Lee. Hierarchical graph transformer with\nadaptive node sampling.arXiv preprint arXiv:2210.03930,\n2022.\n[Zhao et al., 2021] Jianan Zhao, Chaozhuo Li, Qianlong\nWen, Yiqi Wang, Yuming Liu, Hao Sun, Xing Xie, and\nYanfang Ye. Gophormer: Ego-graph transformer for node\nclassification. arXiv preprint arXiv:2110.13094, 2021.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n4710",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.732261598110199
    },
    {
      "name": "Scalability",
      "score": 0.6674800515174866
    },
    {
      "name": "Transformer",
      "score": 0.5561085939407349
    },
    {
      "name": "Theoretical computer science",
      "score": 0.5087314248085022
    },
    {
      "name": "Graph",
      "score": 0.4950629770755768
    },
    {
      "name": "Feature learning",
      "score": 0.4453818202018738
    },
    {
      "name": "Machine learning",
      "score": 0.40104806423187256
    },
    {
      "name": "Artificial intelligence",
      "score": 0.37903445959091187
    },
    {
      "name": "Engineering",
      "score": 0.10913580656051636
    },
    {
      "name": "Voltage",
      "score": 0.08112338185310364
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I25251306",
      "name": "Yuanpei University",
      "country": "TW"
    },
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210095624",
      "name": "Alibaba Group (United States)",
      "country": "US"
    }
  ],
  "cited_by": 7
}