{
  "title": "A survey of quantum language models",
  "url": "https://openalex.org/W2901762028",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A1980846011",
      "name": "Dawei Song",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1972955069",
      "name": "Peng Zhang",
      "affiliations": [
        "Tianjin University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2904531529",
      "name": "Xindian Ma",
      "affiliations": [
        "Tianjin University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1980846011",
      "name": "Dawei Song",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1972955069",
      "name": "Peng Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2904531529",
      "name": "Xindian Ma",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6678277124",
    "https://openalex.org/W6681698864",
    "https://openalex.org/W6680532216",
    "https://openalex.org/W2576410866",
    "https://openalex.org/W29665",
    "https://openalex.org/W2099345940",
    "https://openalex.org/W1590952807",
    "https://openalex.org/W2142819538",
    "https://openalex.org/W1787071328",
    "https://openalex.org/W2339896096",
    "https://openalex.org/W6703254000",
    "https://openalex.org/W2099558938",
    "https://openalex.org/W1556951561",
    "https://openalex.org/W1926201870",
    "https://openalex.org/W1602136775",
    "https://openalex.org/W2117925260",
    "https://openalex.org/W1502957213",
    "https://openalex.org/W2784311771",
    "https://openalex.org/W6641128210",
    "https://openalex.org/W6637409405",
    "https://openalex.org/W2489487449",
    "https://openalex.org/W2598684926",
    "https://openalex.org/W2612051204",
    "https://openalex.org/W2602856279",
    "https://openalex.org/W3104263599",
    "https://openalex.org/W6752082509",
    "https://openalex.org/W1964742985",
    "https://openalex.org/W2163382007",
    "https://openalex.org/W2070740689",
    "https://openalex.org/W2419175238",
    "https://openalex.org/W3102194711",
    "https://openalex.org/W2251818205",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W2167596506",
    "https://openalex.org/W193854645",
    "https://openalex.org/W2120735855",
    "https://openalex.org/W2160416736",
    "https://openalex.org/W1508567213",
    "https://openalex.org/W2136729221",
    "https://openalex.org/W2153439141",
    "https://openalex.org/W1852909287",
    "https://openalex.org/W1508593203",
    "https://openalex.org/W4211080241",
    "https://openalex.org/W2024932032",
    "https://openalex.org/W2137698233",
    "https://openalex.org/W1532325895",
    "https://openalex.org/W1525068081",
    "https://openalex.org/W2170608991",
    "https://openalex.org/W4240913316",
    "https://openalex.org/W2152909313",
    "https://openalex.org/W2068632118",
    "https://openalex.org/W4299295087",
    "https://openalex.org/W2767017507",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W4243333943",
    "https://openalex.org/W2165612380",
    "https://openalex.org/W4231510805",
    "https://openalex.org/W1689711448",
    "https://openalex.org/W1915024344",
    "https://openalex.org/W2964299903",
    "https://openalex.org/W2882319491",
    "https://openalex.org/W2147152072",
    "https://openalex.org/W4233135949",
    "https://openalex.org/W2336251867",
    "https://openalex.org/W2137852732",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W4234159136",
    "https://openalex.org/W1968731131",
    "https://openalex.org/W1985471546",
    "https://openalex.org/W2756668352",
    "https://openalex.org/W2087746031",
    "https://openalex.org/W2121227244"
  ],
  "abstract": "Language model is a fundamental research topic in areas related to natural language processing. In recent years, researchers have proposed quantum language models based on the probability theory of quantum mechanics. This paper aims to review the research motivation and the current progress of constructing various quantum language models. First, it reviews the research problems of classical language models. Then, it introduces some quantum language models in information retrieval and speech processing, as well as an end-to-end quantum language model based on neural network architecture. By analyzing the advantages and disadvantages of each quantum language model considered here, taking into account the essential connection between quantum mechanics and neural networks, we outline our vision for future research directions.",
  "full_text": "SCIENTIA SINICA Informationis\n中国科学: 信息科学 2018 年 第48 卷 第11 期: 1467{1486\nc⃝ 2018《中国科学》杂志社 www.scichina.com infocn.scichina.com\n社会媒体计算与自然语言处理专刊 . 评述\n量子语言模型研究综述\n张鹏1*, 马鑫典1, 宋大为2\n1. 天津大学计算机科学与技术学院, 天津 300350\n2. 北京理工大学计算机学院, 北京 100081\n* 通信作者. E-mail: pzhang@tju.edu.cn\n收稿日期: 2018–06–22; 接受日期: 2018–09–10; 网络出版日期: 2018–11–09\n国家重点研发计划 (批准号: 2017YFE0111900) 和国家自然科学基金 (批准号: U1636203, 61772363) 资助项目\n摘要 语言模型是自然语言处理相关领域研究工作的重要基础. 近年来, 人们基于量子力学概率理\n论提出量子语言模型. 本文旨在综述量子语言模型的研究动机和当前进展. 我们首先回顾语言模型\n的研究现状及存在的问题, 然后介绍信息检索领域和语音处理领域的量子语言模型, 以及我们所提\n出的应用于自动问答领域的端到端的量子语言模型. 通过分析各种量子语言模型的优缺点, 以及量\n子力学与神经网络的本质联系, 提出进一步的研究思路与未来愿景.\n关键词 语言模型, 量子语言模型, 神经网络, 信息检索, 量子力学\n1 语言模型的研究现状\n语言作为社会文明发展与传递的主要媒介之一, 对社会、政治、科技及文化等各方面产生深远且\n重要的影响. 现今时代, 随着计算机科学的飞速发展以及人工智能的长足进步 , 作为一个核心研究方\n向, 自然语言处理的重要性不言而喻. 自然语言处理的基本任务是利用计算机对自然语言 (即人类语\n言) 的内在规律进行建模, 从而进一步进行自然语言的生成与理解. 因此, 语言模型的研究进展是重中\n之重. 发展高效而鲁棒的语言模型有助于支持诸如搜索引擎、智能对话、在线推荐及电子商务等实际\n应用, 不仅具有重要的理论意义, 同样蕴含巨大的社会价值.\n语言模型作为自然语言处理的核心问题, 在取得了一系列发展的同时, 也暴露出诸多问题. 起初,\n科学家们根据语言学知识和领域知识, 人工编制一些文法规则, 设计出文法型语言模型[1,2]. 但是, 这\n种语言模型[2] 因为人工成本高且只针对特定领域, 不能处理大规模通用领域的文本. 于是, 基于统计\n不确定性的语言模型应运而生, 计算机通过估计统计语言模型的概率分布参数, 推断自然语言片段出\n现的可能性. 时至今日, 统计语言模型已经发展出很多具体的模型[3], 每种模型都有其基本假设, 这些\n基本假设在具体化语言模型的同时, 也限制了其在某些方面的建模能力, 例如建模词与词之间的依赖\n关系[3]、语义片段的潜在语义[4] 以及隐藏意图等方面.\n引用格式: 张鹏, 马鑫典, 宋大为. 量子语言模型研究综述. 中国科学: 信息科学, 2018, 48: 1467{1486, doi: 10.1360/N112018-00163\nZhang P, Ma X D, Song D W. A survey of quantum language models (in Chinese). Sci Sin Inform, 2018, 48: 1467{1486,\ndoi: 10.1360/N112018-00163\n张鹏等: 量子语言模型研究综述\n具体而言, 词袋模型 (bag of words) [5] (即一元语言模型) 假设将一篇文档或者一个词的序列看成\n是词的集合, 并且集合中的词是无序和独立的. 尽管词袋模型在一些应用任务(如 ad-hoc 信息检索[6])\n取得了不错的效果, 但是无法表示词与词之间的语义关联. 针对此问题, 多元语言模型 (n-gram) [7] 建\n模当前词与其前面 n− 1 个词的相关性, 通过考虑与当前词相邻近的词来估计语言模型的概率分布 ,\n但是这种方法增加了概率参数估计规模, 另外其更本质的局限性在于有些强语义关联的词汇在局域范\n围内并不是共现的, 基于局域物理位置相似性(proximity-based) 的方法不能很好地刻画潜在的或全局\n的语义关联性.\n面对词袋模型与多元语言模型的不足, 潜在语义索引模型 (latent semantic index, LSI) [8] 首先将\n每个词嵌入到一个由奇异值分解 (SVD) 得到的特征向量张成的子空间中, 并假设在这个 “潜在的” 子\n空间中, 语义关联的词项之间也是相似的(例如其向量内积所表示的余弦相似度较小). 通过这个假设,\nLSI 试图刻画多义词和近义词等全局语义关联. 类似地, 神经语言模型[9,10] 的方法也基于上述子空间\n嵌入假设 [11], 所不同的是其训练模型是神经网络模型. 潜在语义索引模型和神经语言模型都试图找\n到能够刻画词与词之间语义相似性的子空间, 但是语义相似性其实并不必然等同于实际应用所需要的\n语义性质, 例如在信息检索中, 相关性 (relevance) 就不必然等同于相似性 (similarity).\n为更好地建模语义关联, 信息检索领域的学者们提出利用 Markov 随机场 (Markov random ﬁeld,\nMRF) 改进语言模型[12]. 在估计文档的相关性时, MRF 模型按照查询词不同的组合 (比如单个词、 多\n个词组成的词组等) 分别算出文档的相关性分数, 然后计算总的相关性分数. 但是该模型在计算文档\n相关性分数时, 只是将不同的依赖 (或特征) 信息得到的分数做线性加权, 并不能有机地将不同语义依\n赖统一到一种原则性的语言模型框架之下[13].\n同样针对这一问题, Sordoni 等[13] 提出一种量子语言模型(quantum language model, QLM), 试图\n利用量子力学的密度矩阵建模文本序列(例如查询词和文档) 中词与词之间的依赖关系. 简单地说, 在\n量子语言模型中, 投影算子 (projector) 表示单个词或词的组合 (可以看作是基准的量子态), 密度矩阵\n可以用来测量各个可观测 (observed) 量子态 (例如查询词) 出现的概率, 查询和文档所对应的密度矩\n阵可以用极大似然估计方法求得, 然后利用两个密度矩阵的Von-Neumann 散度 (VN-divergence) 计算\n查询和文档的相关性. 实验表明在 ad-hoc 信息检索中, 相比一元语言模型和基于 MRF 的高阶语言模\n型, 量子语言模型取得显著的性能提升.\n尽管如此, 当前的量子语言模型研究工作仍然存在若干关键性的问题, 这些问题在一定程度上限\n制了量子语言模型的深入研究和广泛应用. 深入研究方面, 量子语言模型的局限之处在于: 首先, 虽\n然量子语言模型通过训练密度矩阵建模语义依赖[13], 但从实质上而言, 这种语义依赖只对应查询词组\n成的词组, 未能充分考虑全局依赖. 其次, 量子语言模型只是应用于 ad-hoc 信息检索任务中, 该检索\n任务非常简单: 用户给定查询词, 然后系统给出检索结果, 尚未拓展到文本生成和自动对话的典型任\n务[14] 上. 最后, 如何应对在大规模数据集下, 密度矩阵高效而快速的训练, 这些都是亟待研究的问题.\n2 量子语言模型的研究背景\n为了更好地了解量子语言模型的背景, 有必要从建模词与词之间的依赖关系、语义关联性入手,\n系统地回顾统计语言模型, 以及信息检索领域的语言模型. 因为量子语言模型是在信息检索领域首先\n提出的, 我们也得介绍量子力学在信息检索领域的发展历史.\n1468\n中国科学: 信息科学 第 48 卷 第 11 期\n2.1 统计语言模型\n统计语言模型[6] 通过表示语言基本单位 (例如词、词组、句子等, 这里统一叫做文本片段) 的分\n布函数, 力图描述语言的统计生成规则. 但是因为自然语言单词量大、 句法复杂, 很难计算句子的概率\n空间, 所以一般是根据某些假设条件, 将计算文本片段的的概率分解为边缘概率或者条件概率的乘积.\n例如在 n-gram 模型中, 采用了 Markov 假设, 认为每个单词只与其前面 n− 1 个词 (上下文) 相关, 参\n数 n 成为模型的阶数. 一阶语言模型 (又称为一元语言模型), 假设词与词之间是统计独立的, 随着阶\n数的增大, 模型的复杂性不断提高. n-gram 模型成功捕捉了自然语言局部依赖的性质 (例如上下文相\n邻词的依赖关系), 但却不能表示语言中存在的远程依赖 (例如句子结构、语义关系等).\n决策树语言模型[15,16] 的提出, 解决了 n-gram 模型的复杂性及冗余上下文依赖的问题. 具体而言,\n决策树语言模型通过设计一些问题或条件, 尽量保留与当前预测变量 (例如当前词) 关联的上下文, 去\n掉不关联的上下文. 随着决策树高度的增加, 每个节点的训练语料相对的就会减少, 导致精度的降低,\n产生所谓的数据碎化问题. 于是, 科学家们提出最大熵模型[17], 基本思想是将统计语言模型的估计问\n题看作是有约束的概率分布优化问题. 总体来说, 决策树语言模型和最大熵模型在解决 n-gram 模型\n上下文只用模型阶数约束的同时, 也存在约束条件设计麻烦以及时间复杂性高等问题. 这些问题在一\n定程度上, 限制了其在大规模文本处理任务 (例如信息检索) 中的广泛使用.\n2.2 经典信息检索\n信息检索 (information retrieval, IR) 是自然语言处理领域的一个重要研究方向, 其典型的应用是\n通用搜索引擎, 例如百度、Google 等[18]. 信息检索是对于用户的信息需求 (通常用查询词来表示), 找\n出与之相关的信息载体 (通常由文档来表示). 信息检索模型是在表示信息需求和信息载体的同时, 计\n算信息载体相对于信息需求的相关程度. 我们主要介绍基于概率统计的检索模型或语言模型在信息检\n索中的应用, 以及它们建模语义依赖的作用.\n在概率检索模型和统计语言模型之前, 科学家们已提出其他一些经典信息检索模型, 例如布尔模\n型和空间向量模型[19] 等. 布尔模型[20] 是最早的一种检索模型, 它将查询表示为布尔表达式, 然后输\n出符合布尔表达式的文档. 这种检索方式将相关性看成是一个二值属性 (即相关和不相关两种属性),\n虽然在一些图书管理系统中得到成功应用, 但对于通用搜索环境来说, 无法针对信息相关程度排序. 向\n量空间模型 (vector space model) [21] 的基本原理是将查询和文档分别表示成一个向量, 向量中每一个\n索引项的值可以是TF/IDF 值[22], 表示每个词的权重, 然后通过计算查询和文档之间向量的内积或者\n余弦相似度的方式, 求解相关性得分. 这种方法可以对文档进行排序, 并且改善了检索性能.\n概率检索模型 (probabilistic retrieval model) [23] 首次将概率统计的不确定性引入检索模型, 这是\n检索过程具有不确定性的体现. 通过一些基本假设 (例如词与词之间是独立的、索引项权重和文档相\n关性是二值的), 利用 Bayes 公式推导出文档与查询的相关性计算公式. 这种方法有概率统计的理论\n基础, 例如在符合基本假设的前提下, 证明概率检索模型可以提供最优的检索结果 [23]. 从实验角度,\n概率检索模型表现出优秀的检索性能, 例如其导出的 BM25 模型仍是现在搜索系统的一个很有效的\n特征[24].\n语言模型也是一种基于概率统计的检索模型 , 与概率检索模型的不同之处在于其概率生成的原\n则[25]: 概率检索模型假设查询 Q 生成相关文档集 D, 而语言模型假设文档集 D 生成查询 Q. 因此,\n语言模型的核心问题是计算语言模型生成查询的概率 (表示为 p(Q|D), 称为查询似然性). 通过 Bayes\n法则和词间独立性假设, p(Q|D) 就可以很容易的由文档语言模型的边缘概率的乘积求得. 语言模型形\n1469\n张鹏等: 量子语言模型研究综述\n式简单且有较好的理论基础, 在对文档做平滑 (smoothing) 之后, 其计算公式可以分解出 3 个信息检\n索典型的特征, 即词频 (term frequency, TF) 、 逆向文档频率(inverse document frequency, IDF) 以及文\n本长度 (document length, DL) [26].\n可以看出, 虽然上述基于统计的检索模型各有侧重, 但有一个共同的假设, 即不同词语之间是统\n计独立的, 无法建模词语之间的语义关联. 为更好地刻画语义关联 (特别是不同查询词之间的语义关\n联), 研究者们提出利用 Markov 随机场 (Markov random ﬁeld, MRF) 改进语言模型 [12]. 在估计文档\n的相关性时, MRF 模型按照查询词不同的组合 (比如单个词、多个词组成的词组等) 分别计算文档\n的相关性分数, 然后利用线性拟合方法将这些分数集成起来形成总的相关性分数 . 为了计算语义分\n布相似度, 建模语义关联, 研究者们提出翻译语言模型 [27], 通过融合多个训练集的翻译知识, 以提升\n领域翻译知识的权重来建模语义关联. 另外, 一些其他词聚类方法, 例如 Latent Dirichlet Allocation\n(LDA) [28] 和 Partially Observable Markov Decision Processes (POMDP) [29] 等, 也被用于改进语言模\n型. 但是上述工作在计算文档相关性分数时, 只是将不同的依赖信息 (或特征) 得到的分数做线性加\n权, 并不能有机地将不同的依赖信息统一到一种原则性的 (principled) 语言模型框架之下[13].\n2.3 量子信息检索\n首先, 需要澄清的是, 所谓量子信息检索指借助量子理论 (quantum theory, QT) 的数学方法、实\n验直觉, 以及类量子现象1)(例如宏观现象中体现出来的类量子规律) 来解决信息检索问题的研究工作.\n也就是说, 量子信息检索并不是说在量子计算机上运行的信息检索, 或者说物理上必须建模量子级别\n的微观粒子. 量子信息检索起初是希望经典信息检索的模型更加一般化, 从而建模一些非经典的概率\n现象或特征.\nvan Rijsbergen 在其文献 [30] 中开创性地提出将传统信息检索模型 (例如布尔模型、向量空间模\n型和概率检索模型等) 统一在 Hilbert 向量空间中的量子力学形式化框架中. 具体来说, 量子理论为信\n息检索基本元素 (例如查询, 文档和多媒体信息等) 提供了在 Hilbert 复数空间中的几何表示. 在 van\nRijsbergen 的启发下, 涌现出一些量子信息检索的工作.\n首先, 在信息检索领域中, 人们开始探索和建模宏观类量子现象. 受量子干涉现象的启发, 研究了\n认知干涉, 即用户的先期阅读经历是否会改变其对当前文本的相关性判断 [31], 并研究了查询词的次\n序效应 (order eﬀect), 以及相对应的量子干涉现象 [32]. Zuccon 等[33] 指出了信息检索 (IR) 中的文本\n排序场景和量子理论中的双缝实验解释之间的同构性, 并主张在测度文本相关性时考虑文本之间的干\n涉. Sordoni 等[34] 类比了量子力学中的双缝干涉实验, 将任意两个隐主题类比为双缝, 将主题词分布\n看作屏幕, 研究两个隐主题之间的干涉效应. 此外, 为了捕获搜索会话中的动态信息需求, 利用密度矩\n阵[35] 的演化过程建立了自适应量子语言模型, 研究 Session Search 中的查询词的不确定性. 在建模宏\n观类量子现象方面, Zhang 等[35∼37] 提出了光子极化实验在信息检索排序模型和查询扩展模型的对应\n关系. 文献 [38] 提出利用纯相关依赖关系建模后测量设置下的量子纠缠, 抽取一些依赖关系更强的词\n组作为量子基本事件. 并在量子语言模型中建模了量子纠缠这一宏观类量子现象.\n此外, 研究人员提出了若干基于 Hilbert 空间的信息检索模型和框架. 例如, 可以将用户的信息需\n求和信息对象表示为对应的子空间, 并融合不同维度的上下文信息 (例如, 文本、任务、用户或地点\n等). Piwowarski 等[39] 利用张量空间与状态向量空间构造量子信息检索方法, 随后 Frommholz[40] 基\n于信息需求的多元表示扩展了上述框架, 为各个表示定义合适的 Hilbert 子空间. Sordoni 等[13] 在量\n1) 在宏观层面, 我们用类量子现象表述, 这有区别于微观的量子现象.\n1470\n中国科学: 信息科学 第 48 卷 第 11 期\n子概率框架下扩展了传统语言模型, 并提出了量子语言模型.\n目前, 量子语言模型在信息检索主流研究中未能取得广泛应用, 原因有以下几点. 第一, 大多数同\n行认为量子力学主要针对微观世界, 与计算机的联系仅是量子计算. 但其实量子力学本身是一个数学\n框架, 已经应用于一些诸如社会学、经济学和认知科学等宏观领域[41,42], 并且其研究不依赖于量子计\n算. 第二, 在某种意义上说, 信息检索是一门实验科学, 而早期的量子信息检索模型在实验效果上未能\n表现出明显的优势. 第三, 量子语言模型提出来之后, 因为其密度矩阵计算成本较大, 加之不能利用监\n督信息, 所以它在很多任务上 (例如自动问答任务) 表现不佳. 这些原因都限制了量子信息检索和量子\n语言模型在主流研究中的影响力. 针对这些问题, 本文旨在澄清量子力学的数学框架及其在信息检索\n相关领域的研究动机, 并逐步叙述各种量子语言模型. 具体而言, 我们在本文第 3 节, 详细介绍量子力\n学公理假设, 及其与信息检索和自然语言处理任务的关系. 在第 4 节, 详细介绍 3 个不同领域的量子\n语言模型.\n3 量子力学公理及应用实例\n本节介绍量子力学的公理体系[43] 及其与语言建模的联系. 20 世纪30 年代数学家von Neumann [44]\n将量子理论进行公理化, 其概率测量是基于空间投影的测量方法, 这一形式化体系并不神秘, 主要是\n基于 Hilbert 空间下的线性代数和投影理论. 我们将介绍 4 个基本量子力学公理, 并且说明其在自然\n语言处理方面的应用实例.\n3.1 状态空间\n公理1 (量子叠加态) 假设一个量子比特有一个二维的状态空间, 用 |0⟩ = (1 ,0)T 和 |1⟩ = (0 ,1)T\n构成这个空间的标准正交基, 则状态空间中的任意状态可用叠加态表示, 如式 (1) 所示:\n|ϕ⟩ = α|0⟩ + β|1⟩, (1)\n其中, α和β是限定在实数域的概率振幅, |α|2 和|β|2 表示概率, 所以需要归一化, 即满足|α|2+|β|2 = 1.\n在信息检索中, 用户对文档的相关性判断 (relevance judgement) 可以用量子叠加态表示[12], |d⟩ =\nα|r⟩ + β|¬r⟩, 其中, |r⟩ 表示相关, |¬r⟩ 表示不相关, |α|2 表示文档相关的概率值, |β|2 表示文档不相关\n的概率值. 另外, 一个词的多种词义也可以用量子叠加态表示[13], 比如 “苹果” 这个词, 它既可以是水\n果, 也可以是手机. 故此, 我们使用一组完备的基向量 (|c1⟩,|c2⟩,..., |cm⟩) 表示一个词的多种词义, 如\n式 (2) 所示:\n|ϕ⟩ = α1|c1⟩ + α2|c2⟩ + · · · + αm|cm⟩. (2)\n根据向量表示方法, 词向量 (word embedding) 就可以看作是一种叠加态的表示.\n单个射线或向量 (量子叠加态) 对应纯态, 多个向量对应混合态. 混合态需要用密度矩阵表示. 在\n量子力学中, 一个系统的状态经常用 Hilbert 空间上的密度矩阵 ρ 表示. 密度矩阵需要满足两个条件:\n(1) 半正定; (2) 迹为 1, 即 tr(ρ) = 1. 根据密度矩阵空间特征分布, 可以分为混合态的密度矩阵与纯态\n的密度矩阵. 可以通过式 (3) 判断密度矩阵的状态为纯态或混合态. 如果密度矩阵 ρ 的平方的迹等于\n1, 则 ρ 是纯态, 小于 1 是混合态.\ntr(ρ2)\n\n\n\n= 1, pure state;\n<1, mixed state .\n(3)\n1471\n张鹏等: 量子语言模型研究综述\n2a\n1a\n2e\n1e\nψ\n图 1 (网络版彩图) 投影测量的二维几何表示\nFigure 1 (Color online) 2-dimensional geometric representation of projection measurement\n直观理解就是, 如果密度矩阵是纯态, 可以用一个向量的外积运算得到矩阵, 即 ρ = |ψ⟩⟨ψ|, 其中 |ψ⟩\n是一个叠加态. 如果密度矩阵是混合态, 可以用多个向量外积运算再加权求和得到的矩阵 . 即 ρ =\n∑\ni pi|ψi⟩⟨ψi|, 其中 ∑\ni pi = 1. 信息检索中, 用户在对文档的相关性判断时, 判断相关还是不相关, 是\n具有上下文性的, 需要借助量子测量得到. 通过量子测量, 量子的叠加状态会以一定的概率塌缩到文\n档相关和不相关这两种状态的其中一种. 词的含义也有上下文性, 一个给定的多义词通过量子测量会\n塌缩到一种具体的词义. 第 3.2 小节将具体介绍量子测量公理.\n3.2 量子测量\n量子力学中有很多重要的测量方法, 包括一般测量、 投影测量和POVM 测量等. 在量子语言模型\n建模过程中, 使用较多的一种测量方法是投影测量.\n公理2 (投影测量) 有一个可观测系统的状态, 由 Hilbert 空间向量 |ψ⟩ = ∑n\ni=1 ai|ei⟩ 表示, 用\nΠi = |ei⟩⟨ei| 表示本征空间上的一个测量算子. 测量状态 |ψ⟩ 时, 得到概率 P(ei|ψ):\nP(ei|ψ) = ⟨ψ|Πi|ψ⟩ = ⟨ei|ψ⟩2 = a2\ni . (4)\n测量后的量子系统状态为\n|ψ′⟩ = Πi|ψ⟩√\nP(ei|ψ)\n, (5)\n其中, ai 是概率振幅, a2\ni 是概率, 满足 ∑\ni a2\ni = 1, ⟨ei|ψ⟩ 表示向量的内积运算.\n为了能够形象的表示测量过程, 投影测量的二维几何表示如图 1 所示, |e1⟩ 和 |e2⟩ 对应两个基本\n量子事件, 用两个基向量表示. 量子系统状态 |ψ⟩ 是一个量子叠加态的表示, 即 |ψ⟩ = a1|e1⟩ + a2|e2⟩,\n朝两个不同的方向作投影, 可以分别得到概率 p(e1|ψ) 和 p(e2|ψ). p(e1|ψ) = a2\n1 表示该量子系统的状\n态塌缩到量子基本事件 e1 的概率, p(e2|ψ) = a2\n2 表示该量子系统状态塌缩到量子基本事件 e2 的概率.\n投影测量对应向量的内积计算, 可以用来表示余弦相似度的度量. 在信息检索中, 假设有一个检\n索的查询 q 和一篇文档 d, 用一组向量来表示 q 和 d, q 和 d 之间的相似度公式 (6) 如下:\ncos2(q,d) = |⟨q,d⟩|2 = p(q|d). (6)\n两个词向量的相似度也可以用余弦相似度来刻画.\n1472\n中国科学: 信息科学 第 48 卷 第 11 期\n\u0000\n2e\n1e\n2v\n1v\n'1v\nθ\n图 2 (网络版彩图) 酉演化二维空间示意图\nFigure 2 (Color online) Unitary evolution 2-dimensional spatial diagram\n3.3 酉演化\n一个封闭的量子系统的演化可以由一个酉变换来刻画, 即系统的状态 |ψ⟩ 是随时间变化的, 量子\n力学理论为这种量子系统的变化提供了一种方法, 即酉演化.\n公理3 (酉演化) 系统在时刻 t1 的状态 ψ 和系统在时刻 t2 的状态 |ψ′⟩, 可以通过一个 U 算子\n(即酉矩阵) 进行状态变化.\n|ψ′⟩ = U|ψ⟩. (7)\n定义一个二维的实数域酉矩阵: U = [\ncos\u0012 \u0000sin\u0012\nsin\u0012 cos\u0012\n], 一个二维空间状态, 用空间向量 |ψ⟩ = [ a1,a2]T 表\n示. 那么, 酉变化可以表示为\n|ψ′⟩ = U|ψ⟩ =\n\ncosθ −sinθ\nsinθ cosθ\n\n\n\na1\na2\n\n. (8)\n在量子语言建模过程中, 酉演化扮演一个重要的角色, 它通常用在连续的测量过程中. 假设有一\n个文本序列, S = ( w1,w2,...,w n), 序列 S 中的每一个词可以用一个叠加态的向量表示. 在语言模型\n的建模过程中, 需要计算在当前词序情况下, 出现下一个词的条件概率, 一直循环计算, 到句子结束,\n然后用这些概率值计算句子的困惑度 (perplexity), 这是一个 Markov 链的过程. Sordoni 等 [13] 在量\n子概率框架下提出的量子语言模型是计算单个词或词组的概率值, 不能够用来建模句子序列的条件概\n率. 为了建模句子的序列关系, 文献 [45] 用到了酉演化公理, 酉演化可以弥补量子测量建模句子序列的\n不足.\n为了便于直观理解, 基于基向量 |e1⟩ 和 |e2⟩ 建立一个二维的空间示意图, 如图 2 所示. “ 买了” 对\n应空间状态向量 |v1⟩, “ 书” 对应空间状态向量 |v2⟩. 假设我们的句子状态空间向量已经对 “张三” 这\n个词做了投影测量, 测量后的状态就落在了状态 |v1⟩ 上, 也就是 “买了” 对应的空间状态, 如果不进行\n空间酉变化, 就会从 |v1⟩ 的状态开始向 |v2⟩ 状态做投影测量, 这样就不能很好地建模句子的整体语义.\n对应语言模型上的理解就是, “ 书” 这个词出现的概率等于在 “买了” 这个词出现的条件下 “书” 出现\n的一元条件概率, “ 张三” 与 “书” 就是相对独立的, 无法表示在 “张三” 和 “买了” 出现的条件下 “书”\n出现的条件概率, 即二元条件概率. 因此需要一个酉演化, 让状态空间变化到 |v′\n1⟩ 之后再向 |v2⟩ 作\n投影.\n1473\n张鹏等: 量子语言模型研究综述\n3.4 复合系统\n公理4 一个复合系统的状态空间是由多个子系统的状态空间做张量积得到的, 若将多个子系统\n编号为 1 到 n, 子系统 i 的状态被置为 ψi, 则整个系统的总状态为 |ψ1⟩ ⊗ |ψ2⟩ ⊗ · · · ⊗ | ψn⟩, 即复合系\n统的状态.\n假设我们有两个子系统, 分别为 |0⟩ = [1,0]T 与 |1⟩ = [0,1]T, 张量积之后为\n|0⟩ ⊗ |1⟩ =\n\n1\n0\n\n⊗\n\n0\n1\n\n=\n\n\n1 × 0\n1 × 1\n0 × 0\n0 × 1\n\n\n=\n\n\n0\n1\n0\n0\n\n\n. (9)\n在量子语言建模的过程中, 这里用建模一个句子序列为例, S = ( w1,w2,...,w n), 把每一个单词\nwi 作为一个子系统, 用向量 |wi⟩ 表示, 那么多个子系统的张量积就是一个句子序列 S 对应的复合系\n统 |S⟩:\n|S⟩ = |w1⟩ ⊗ |w2⟩ ⊗ ... ⊗ |wn⟩.\n为了便于理解, 依然以句子“张三/买了/书” 为例. 每个单词用一个独热表示(one-hot), 分别是 “张三”\n对应 |w1⟩ = (1 ,0,0)T, “ 买了” 对应 |w2⟩ = (0 ,1,0)T, “ 书” 对应 |w3⟩ = (0 ,0,1)T, 那么这个句子的复合\n系统的状态表示为 |w1⟩ ⊗ |w2⟩ ⊗ |w3⟩, 每个向量 |wi⟩ 也可以用词向量来表示.\n4 量子语言模型的详细介绍\n基于上述量子力学公理体系, 本节详细介绍信息检索领域的量子语言模型、语音处理领域的量子\n语言模型和自动问答领域的量子语言模型. 信息检索领域的量子语言模型, 主要研究短文本与长文本\n的匹配问题, 即查询词与文档之间的匹配, 计算检索词与文档之间的匹配分数, 并根据匹配分数进行\n文档排序. 信息检索领域的量子语言模型[13] 主要使用了量子公理的 1 和 2, 分别从词的表示, 词与词\n依赖关系的表示, 以及文档的表示展开叙述, 然后介绍基于最大似然函数的密度矩阵优化, 以及文档\n和查询的匹配函数. 语音处理领域的量子语言模型, 主要研究使用量子测量的方法计算每个词在历史\n词出现条件下的条件概率, 然后进一步计算句子中所有词出现的联合概率, 从而去学习数据的分布情\n况. 语音处理领域的量子语言模型 [45] 使用了公理 2 和 3, 重点体现在量子公理 3, 即酉演化, 主要是\n建模句子序列, 计算词的条件概率与句子的联合概率. 然后, 介绍在自动问答领域的量子语言模型, 在\n该领域, 量子语言模型需要考虑问题与答案是否能够精确匹配, 研究的是短文本与短文之间的匹配问\n题, 需要获取更多的短文本语义信息. 在自动问答领域, 我们提出的基于神经网络的端到端量子语言模\n型[46] 介绍了句子的密度矩阵表示, 问答句对的联合表示和两种更新联合矩阵的方法.\n4.1 信息检索领域的量子语言模型\n为了更好地建模语义关联, Sordoni 等[13] 提出一种信息检索领域的量子语言模型, 这种语言模型\n利用量子概率的知识, 以及利用量子力学的密度矩阵建模文本序列 (例如查询词和文档) 中词与词之\n间的依赖关系, 这里体现了量子力学公理中的叠加态公理和测量公理. 简单地说, 在量子语言模型中,\n投影算子 (projector) 表示单个词或词的组合, 密度矩阵可以用来测量各个可观测的(observed) 量子态\n1474\n中国科学: 信息科学 第 48 卷 第 11 期\n(例如查询词) 出现的概率, 对应的密度矩阵 (查询与文档) 提出用极大似然估计的方法求得, 这里使用\n了一种 RρR[47] 的方法, 实质是一种 Expectation maximization (EM) 算法. 然后利用两个密度矩阵\n的 VN-divergence 计算查询和文档的相关性, 利用相关性的大小进行排序. 接下来, 将详细介绍该语言\n模型.\n4.1.1 单个词的表示\n在使用经典概率建模语言时, 样本空间是一个集合, 而使用量子概率建模语言时, 样本空间是一\n个 Hilbert 空间. 因此每个词的表示可以由其对应的 Hilbert 空间的空间向量做外积得到的投影算子\n表示. 这个过程可以理解为单个词的向量表示到量子事件空间的映射, 即一个词的空间向量映射成一\n个投影算子, 映射关系如下:\nmap(w) − →Πw, (10)\n这里的 w∈ V, V 是字典集合, 其中 Πw = |uw⟩⟨uw|.\n例如, 字典大小 N = 3, V = {量子, 语言, 模型}, 如果文档中 d= {语言, 模型}, 那么对应的投影\n算子为 {Π2 和 Π3}, 如果单个词的空间向量用 one-hot 表示, 那么,\nΠ2 =\n\n\n0 0 0\n0 1 0\n0 0 0\n\n, Π3 =\n\n\n0 0 0\n0 0 0\n0 0 1\n\n. (11)\nΠ2 和 Π3 同时是文档 d 对应在量子概率空间中的基本量子事件.\n4.1.2 词组中词与词之间的依赖关系表示\n为了表示词组中两个或更多词之间的联系, 可以用不同于单个词的投影算子来表示. 例如, K 是\n多个词组成的词组, 即 K = {w1,...,w n}, 对应投影算子的映射表示如下:\nmap(K) − →Kw1;:::;wn , K w1;:::;wn = |k⟩ ⟨k|, |k⟩ =\nK∑\ni=1\nαi|ui⟩. (12)\n关于系数 αi ∈ R, 为了对 |k⟩ 进行单位化, αi 的取值需要满足 ∑\ni αi2 = 1. 为了用实例说明词与\n词之间的依赖关系, 词表还用上面所述 V. 模拟词组 K1;2;3 = {量子, 语言, 模型} 这 3 个词之间的依\n赖关系, 投影算子 K1;2;3 = |K1;2;3⟩⟨K1;2;3|, 其中 |K1;2;3⟩ =\n√\n1/5|u1⟩ +\n√\n1/5|u2⟩ +\n√\n3/5|u3⟩, 矢量表\n示如图 3 所示, 图中的概率振幅 α1, α2 和 α3 就分别等于\n√\n1/5,\n√\n1/5 和\n√\n3/5. |u1⟩, |u2⟩ 和 |u3⟩ 分\n别对应单词 “量子”、“语言” 和 “模型” 的空间向量. 投影算子 K1;2;3 的矩阵表示如下:\nK1;2;3 =\n\n\n1\n5\n1\n5\n√\n3\n5\n1\n5\n1\n5\n√\n3\n5√\n3\n5\n√\n3\n5\n3\n5\n\n\n. (13)\n1475\n张鹏等: 量子语言模型研究综述\n1\n2\n3\n1u\n3u\n2u\n3 , 2 , 1K\nα α\nα\n图 3 (网络版彩图) 具有依赖关系的矢量 |K1;2;3⟩ 的向量空间表示\nFigure 3 (Color online) The dependency |K1;2;3⟩ is represented by vector space\n4.1.3 文档表示\n在传统语言模型中, 一篇文档由多个词的序列组成, 在量子语言模型中, 一篇文档用多个量子事\n件表示. 量子语言模型[13] 将一篇文档 Pd 看作是 M 个量子事件 (即 M 个单词或词组), 每个量子事\n件用一个投影算子表示, 该文档表示为\nPd = {Πi : i= 1,2,...,M }, (14)\n其中 Πi = |u⟩⟨u| 表示第 i个投影算子, |u⟩⟨u| 表示向量 |u⟩ 的外积, M 是文档 d中单词或词组的个数,\n|us⟩ 是维度为 N 的 Hermit 空间向量, N 也是字典大小.\n4.1.4 最大似然估计\n对于一篇由 M 个词或词组组成的文档 Pd = {Π1,..., ΠM }, 其中 Πi 是词或词组对应的空间向量\n映射得到的投影算子, 然后需要找到一种方法, 该方法能够学到一个代表这篇文档的密度矩阵 ρ. 在\n这项工作[13] 中, 使用的是最大似然估计, 因为它可以自然地看成是一个经典似然函数的量子泛化. 最\n大似然估计可以生成一个定义明确的密度矩阵 ρ. 在量子力学中, 计算单词和词组的概率可以通过\nGleason [48] 定理计算得到, 即\np(Πi; ρ) = tr( ρΠi), (15)\n其中 p(Πi; ρ) 表示密度矩阵 ρ 测量各个词或词组对应的量子态 (投影算子) 的概率. 把文档中所有词\n或词组对应的量子态通过测量得到的概率作连乘运算, 可以得到似然函数:\nLPd (ρ) =\nM∏\ni=1\ntr(ρΠi). (16)\n最大似然函数就可以表示为\nmaximize\u001a log LPd (ρ) =\nM∑\ni=1\nlog tr(ρΠi), (17)\n从而求得最优的密度矩阵 ρ.\n1476\n中国科学: 信息科学 第 48 卷 第 11 期\n4.1.5 匹配函数\nKullback Liebler (KL) 散度方法在计算不同的查询和文档表示方面具有灵活性, 这使得它对新框\n架中的候选评分函数很有吸引力. 经典的 KL 散度经过推广, 出现了量子相对熵 (即计算两个密度矩\n阵的 VN 散度). 对查询和文档分别优化出其密度矩阵ρq 和 ρd. Sordoni 等[13] 提出的匹配评分函数为\n− △ VN(ρq||ρd) = −tr(ρq(log ρq − log ρd))\nrank\n= tr( ρq log ρd).\n(18)\n实验表明在 ad-hoc 信息检索中, 相比一元语言模型和基于 MRF 的高阶语言模型, 量子语言模型\n取得显著的性能提升.\n4.2 语音处理领域的量子语言模型\n上述量子语言模型可以计算或匹配查询和文档的相关性, 但是无法建模词与词的序列关系. 不管\n是一段语音还是一个句子序列, 词与词之间是存在序列关系的. 语音处理领域的量子语言模型 [45] 使\n用了量子力学公理中的酉演化公理, 并通过连续的量子测量 (公理 2), 建模词与词的序列关系. 对于\n句子中所包含的词语, 不同的排列顺序可以表达不同的句子语义. 在量子语言模型中这种词语的顺序\n关系体现在不同的测量顺序上, 一组不同的测量顺序对应一组不同的语义表达, 文献 [45] 也是通过量\n子概率理论来构建语言模型, 该模型 [45] 与其他语言模型, 例如循环神经网络语言模型 [49] (recurrent\nneural network language model, RNNLM) 、长短期记忆网络[50] (long short-term memory, LSTM) 等相\n比, 在语言模型评价指标困惑度 (perplexity, PPL), 以及在自动语音识别评估设置中都取得了较好的\n效果.\n4.2.1 量子概率的相关知识\n• 一个可观测量 ρ (密度矩阵) 的投影测量输出是与可观测量的特征值 {λj} 对应的, 即 λj 为密度\n矩阵 ρ 的特征值.\n• 特征值 {λj} 的测量输出概率为 P(λj) = tr( ρΠ\u0015j ) = tr(Π \u0015j ρ), 其中 Π\u0015j 是观测量 ρ 特征值 λj 对\n应的投影算子. 需要注意的是 ΠT\n\u0015j = Π\u0015j , 同时满足 Π2\n\u0015j = Π\u0015j .\n• 系统 ρ 对应特征值 λj 测量之后的状态, 用密度矩阵 ρ′ 表示. 即\nρ′ = Π\u0015j ρΠ\u0015j\ntr(Π\u0015j ρΠ\u0015j ),\n其中, 分母 tr(Π\u0015j ρΠ\u0015j ) 是对密度矩阵的归一化计算.\n• 演化 (实数域), 通过一个酉矩阵 U 来刻画, U 满足性质 UTU = UUT = I, 其中 I 是一个单位矩\n阵. 在 t 时刻 (第 t 次测量之后), 关于系统的演化表示为\nρt+1 = UρtUT.\n关于量子概率理论的完整内容, 参见文献 [43].\n1477\n张鹏等: 量子语言模型研究综述\n4.2.2 建模过程\n假设有一个长度为 n 的句子序列, s = ( w1,w2,...,w n), 模型用到的词典大小为 N, 包含模型\n中用到的所有单词. 然后定义 N 个标准基向量 {ew : w ∈ { 1,2,...,N }} 对应每一个词. 为了计算\n每一个词 wi 出现的概率, 将每个单词对应的基向量外积运算, 作为语言模型的测量算子, 如: Π w =\neweT\nw. 该模型是按次序进行测量的 , 基于量子力学的测量理论 , 依次测量每个单词出现的概率 . 基\n本思想及计算步骤如算法 1. 把初始化测量得到的概率值与在循环过程中测量得到的所有条件概率\n算法 1 基于量子测量的语言建模\n1: 输入 密度矩阵 \u001a0 和酉演化矩阵 U;\n2: 输出 句子序列的联合概率 P (s|\u001ao; U);\n3: 初始化\n投影测量概率: P (w1; \u001a0; U) = tr( \u001a0Πw1 );\n投影后的状态: \u001a′\n1 =\n\u0005w1 \u001a0\u0005w1\ntr(\u0005w1\u001a0\u0005w1 ) ;\n演化后的状态: \u001a1 = U\u001a′\n1UT;\n4: 循环测量 (i = 2 ; : : : ; n )\n投影测量概率: P (wi|w1; : : : ; w i−1; \u001a0; U) = tr( \u001ai−1Πwi );\n投影后的状态: \u001a′\ni =\n\u0005wi \u001ai\u00001\u0005wi\ntr(\u0005wi\u001ai\u00001\u0005wi ) ;\n演化后的状态: \u001ai = U\u001a′\niUT;\n5: 结束\nP (s|\u001a0; U) = P (w1; \u001a0; U) ∏n\ni=1 P (wi|w1; : : : ; w i−1; \u001a0; U);\nP(wi|w1,...,w i−1; ρ0,U) 连乘, 就得到序列的联合概率P(s|ρ0,U), 其中, 密度矩阵 ρ0 和演化酉矩阵U\n都是模型的参数, 即需要得到的模型. 最后用一个公开评价函数 PPL 评价语言模型的好坏. 评价指标\n函数 PPL 见下式:\nPPL(ρ0,U) = exp\n(\n− 1\n|C|\n∑\ns∈C\nlog P(s|ρ0,U)\n)\n. (19)\nC 是语料集, |C| 是语料集中句子的个数, 该量子语言模型的目标是找到一组好的参数 (ρ0 和 U) 和最\n小化困惑度 PPL.\n4.2.3 辅助系统\n在多次测量的过程中, 为了解决不丢失全局语义信息的问题, 文献 [43] 通过构建一个虚构D 维的\nHilbert 空间 Hancilla = CD, 将 Hancilla 叫做辅助系统. 然后, 将原始的空间和虚构的空间进行张量积计\n算, 就得到了一个 DN 维的 Hilbert 空间 H2 = Hancilla ⊗ H = CDN . 在这个新的空间中, 投影算子可\n以重新定义为 Π(2)\nw2 = ID ⊗ Πw, 这里的 ID 是一个 D 维的单位矩阵. 这种设计将耦合系统的时间演化\n在两个纠缠系统之间建立了非平凡的关联, 使得单词的量子态表示的测量和演化保存了一些关于整个\n序列的信息, 并存储在了状态表示的辅助部分中. 然后, 通过时间演变将这些信息重新转换为单个词\n的量子状态表示, 从而产生 “记忆效应”, 将整个句子序列考虑在内, 进而扩展了 n-gram 语言模型方法\n背后的思想. 但是, 这种做法也导致了大量的参数需要学习.\n4.2.4 酉演化矩阵的构建\n系统的演化最重要的是构建演化矩阵, 构建酉矩阵的方法有多种. 第一种方法是使用一个固定的\n(DN)2 的实数域的酉矩阵计算[51], 但是这种方法并不能取得一个好的 “记忆” 效果. 第二种方法是对\n1478\n中国科学: 信息科学 第 48 卷 第 11 期\n于每个单词都设计一个酉演化的矩阵, 很显然这种方法将导致大量的参数需要学习和优化. 受 Markov\n模型[12] 的启发, 每一个单词可以表示为一个固定维长度的词向量表示, 因此可以针对单个词对应词\n向量的每一维都定义一个酉矩阵. 假设一个单词的词向量表示为 w→ (α1(w),...,α p(w)), p 表示词向\n量的维度. 这样就可以设置一组酉矩阵 U = ( U1,...,U p), 对于每一个单词, 我们就可以动态地构建新\n的酉矩阵表示:\nV(w) =\np∏\ni=1\nU\u000bi(w)\ni .\n即使这样, 单词的词向量表示的向量维度一般也比较大, 因此也存在很多的参数需要学习优化.\n4.3 自动问答领域的量子语言模型\n上述量子语言模型存在以下不足: 第一, 在量子语言模型[13] 模拟每个词语的时候, 使用的是独热\n向量 (one-hot vector) 表示的, 这种表示只能够表示该词的出现以及位置信息, 并不能有效地把全局的\n语义信息给建模出来; 第二, 表示单词用固定的密度矩阵表示, 该密度矩阵使用分析得到, 并不是通过\n训练优化得到, 它不能够通过端到端学习方法学习获得; 第三, 不能够将文本的表示、匹配, 以及排序\n结合起来, 而是分开进行的, 所以就不能联合优化, 从而限制了应用的推广. 为了建模全局语义表示,\n匹配以及排序过程结合起来进行优化, 提高量子语言模型的表现性能, 拓宽量子语言模型的应用. 同\n时针对以上量子语言模型的不足, 文献 [46] 提出结合卷积神经网络(convolution neural network, CNN)\n训练的量子语言模型框架[46] 用来做问答匹配任务. 该量子语言模型能够将需要更新的密度矩阵表示、\n匹配函数和训练过程统一优化, 并且在问答任务上该语言模型实验结果达到了state-of-the-art 的语言\n模型接近的效果.\n4.3.1 密度矩阵的句子表示\n端到端的类量子语言模型 [46] 也用一个密度矩阵来表示一个句子. 与以往不同的是借助词向量\n(word embeding) [52] 表示句子中的每一个单词, 这样可以建模全局的语义信息到密度矩阵中, 优于之\n前使用的 one-hot 表示. 假设一个句子有 N 个词, 每个词用 d 维的词向量来表示, 句子中的每个词用\n|vi⟩ 来表示, 其 |vi⟩ ∈ Rd×1. 根据密度矩阵的性质, 我们可以用式 (20):\nρ=\n∑\ni\npiΠi =\n∑\ni\nλi|vi⟩⟨vi|, (20)\n其中 ∑\ni pi = 1, ρ 是一个对称矩阵.\n这种表示如图 4, 该表示方法简单, 容易应用. 为了获得单位状态向量, 需要单位化词向量的分布\n式表示. 式 (20) 获得的密度矩阵可以表示一个句子, 这与量子系统状态相对应, 并且能够有效地表达\n单词与单词之间的依赖关系. 式 (20) 中的 pi 表示一个单词的位置信息. 在实验中有比较重要的意义.\n4.3.2 问句与答句的联合表示\n为了做文本匹配 (问答句对的匹配) 需要计算文本之间的相似性, 基于神经网络的端到端语言模\n型用迹内积的公式计算问答句对的相似性, 迹内积的编码形式对应密度矩阵的联合表示, 对应问句与\n答句的密度矩阵表示 ρq 和 ρd. 利用式 (21) 得到问句与答句的联合表示. 问句与答句之间的联合表示\n如图 5.\nMqa = ρqρa. (21)\n1479\n张鹏等: 量子语言模型研究综述\nSentence\nmatrix\nS\nDensity\nmatrix\nOuter\nproduct\n2v\n3v\nnv\n1v\nρ\n图 4 (网络版彩图) 单个句子的密度矩阵表示\nFigure 4 (Color online) Single sentence representation by density matrix\nJoint\nrepresentation\nJoint representation Matching\ntrace \ndiag\nSingle sentence representation\n→\nSoftmax\n \n \n \n \n \n  \n \nSentence\nmatrix\nDensity\nmatrix\nOuter\nproduct\nS ρ\nρρ\nρS\nM\nx\nx\n图 5 (网络版彩图) 前 3 层是句子的密度矩阵表示, 第 4 层是联合表示, 第 5 层是问答对匹配训练 softmax 输\n出层\nFigure 5 (Color online) The ﬁrst three layers are to obtain the single sentence representation, the fourth layer is to obtain\nthe joint representation of a QA pair, and the softmax layer is to match the QA pair.\n为了分析这种联合表示能够对应迹内积, 对密度矩阵做谱分解, 可以得到\nρqρa =\n∑\ni;j\nλiλj|vi⟩⟨vj||vj⟩⟨vj|\n=\n∑\ni;j\nλiλj⟨vi|vj⟩|vi⟩⟨vj|,\n(22)\n1480\n中国科学: 信息科学 第 48 卷 第 11 期\n其中 λi 是对应密度矩阵的特征值, |vi⟩ 对应特征值的特征向量. 在式 (22) 中, ⟨vi|vj⟩ 表示 |vi⟩ 和 ⟨vj|\n之间的相似度. 因为 ⟨vi|vj⟩ = tr(|vi⟩⟨vj|), 因此可以写出迹内积的公式:\ntr(ρqρa) =\n∑\ni;j\nλiλj⟨vi|vj⟩2. (23)\n联合矩阵对角线元素对应问句与答句的语义重叠, 可以计算问句与答句之间的匹配相似度. 因此\n联合矩阵采用式 (23) 这样一种迹内积的编码方式, 计算问句与答句之间语义相似性, 然后进行问题与\n答案的相似度匹配. 这种表示是一个更一般化的问题与答案对的特征表示方法, 是接近 VN 散度[13]\n的表示方法.\n4.3.3 学习如何匹配密度矩阵\n基于神经网络的量子语言模型提出了两个方法匹配问答对. 第一种方法是迹内积的编码方式. 计\n算密度矩阵 ρq 和 ρa 乘积的迹, 如式 (24) 所示:\nS(ρq,ρa) = tr( ρqρa). (24)\n这种迹内积的表示可以计算单词和句子之间相似度. 式 (24) 可以展开表示为\nxtrace = tr(ρqρa) = tr\n(∑\ni;j\nλiλj|ri⟩⟨rj|\n)\n, (25)\nxtrace 可以理解为语义的叠加, 在这里表示问题和答案对应密度矩阵的相似度. 另外, 由于联合矩阵的\n对角线元素对应了丰富的特征信息, 对角线元素用向量 ⃗ xdiag, ⃗ xdiag 中对应不同的对角线元素表示相\n似度测量的不同等级. 特征表示能够被定义为\n⃗ xfeat = [xtrace; ⃗ xdiag],\n对应神经网络的反向传播的损失函数为\nL= −\n∑\ni\nN[yi log h(⃗ xfeat) + (1 − yi) log 1 − h(⃗ xfeat)],\n其中 h(⃗ xfeat) 是 softmax 激活层之后的输出值, yi 是问答句对的标签 (label).\n另外一种方法是利用卷积神经网络的方法对联合表示做卷积操作, 这样就搭建起了一个神经网络\n结构, 然后做匹配训练, 这样一种结构能够达到一个很好的实验结果. 为了学习联合矩阵的抽象表示.\n采用了一个二维的卷积神经网络提取联合表示的主要特征, 这些特征表示文本之间的相似距离. 这种\n结构的表示, 如图 6 所示. 基于神经网络的量子语言模型结合词向量的有效表示能力构建密度矩阵 ,\n解决了密度矩阵的稀疏问题, 又结合了神经网络强有力的学习能力, 在 WIKI-QA 数据集[53] 上显著提\n升了量子语言模型的效果, 并在 TREC-QA 数据集[54] 上接近了 state-of-the-art 语言模型的效果. 但\n是, 上述端到端的量子语言模型, 未能说明量子力学与神经网络的本质联系, 还需要进一步的思考与\n研究.\n5 未来愿景\n鉴于神经网络建模语言模型是现在的研究热点, 量子力学的概率理论也可以建模语言模型, 那么\n我们可以想到, 如何构建量子力学、语言模型和神经网络三者之间的联系. 如图 7 所示的等价类图示,\n等价类的说法可能为时尚早, 但是未来工作可以朝这个方向进行努力. 一些想法如下:\n1481\n张鹏等: 量子语言模型研究综述\n \n \n \n  \n \nSentence\nmatrix\nDensity\nmatrix\nOuter\nproduct\nJoint\nrepresentation\nConvolution Pooling Softmax\nJoint representation Matching\nRaw-pooling\nCol-pooling\nConnect\nSingle sentence representation\nS\nS\nM\nρ\nρ\nρρ\n图 6 (网络版彩图) 句子表示和 QA 句对的联合表示, 以及 2 维卷积神经网络示意图\nFigure 6 (Color online) The single sentence representation and the joint representation, and the rest layers are to match\nthe QA pair by the similarity patterns learned by 2-dimensional-CNN\nLanguage \nModel\n Quantum \nMechanics\n Neural \nNetwork\nLanguage \nModel\nLanguage \nmodel\nQuantum QQ\nMechanics\n Quantum \nmechanics\nNeural NN\nNetworkNN\n Neural \nnetwork\n图 7 (网络版彩图) 语言建模、神经网络和量子力学的等价类图示\nFigure 7 (Color online) Language modeling, neural networks and equivalent class diagrams of quantum mechanics\n端到端的量子语言模型 [46] (NNQLM) 并没有解决神经网络与量子语言模型的内在联系, 因此有\n必要研究量子力学与神经网络之间的关系. 已有相关文献着重研究了量子力学与神经网络之间的本质\n关系[55∼59]. 其中发表在Science 的论文[55] 指出量子多体波函数与神经网络是有联系的, 文献 [56] 建\n立了量子力学领域与深度学习领域之间的基本联系, 利用这种联系来说明卷积网络的每一层信道所起\n的作用, 证明了深度卷积网络的实现与张量结构的量子多体波函数之间是存在等价性的, 文献 [57] 建\n立了张量结构的量子多体波函数与深度循环网络结构之间的联系 , 这种联系可以借助张量分解构建.\n文献 [58] 提出了一种基于算术电路的深层网络体系结构, 该体系结构建立了网络与分层张量分解之间\n的等价关系. 文献 [56∼58] 都有提到量子多体波函数的高维张量结构通过张量分解可以得到一个张量\n网络结构. 在文献 [56,57] 启发下, 我们提出一个新的由量子多体波函数启发的语言模型, 该模型采用\n张量积来建模词语之间的交互, 揭示了量子语言建模中使用卷积神经网络的必要性[59].\n1482\n中国科学: 信息科学 第 48 卷 第 11 期\n为了使得语言模型、神经网络以及量子力学三者在理论上建立一个如图7 的等价类关系, 我们可\n以在未来的研究工作中做以下尝试. 首先我们可以用量子多体波函数进行语言建模, 每个词的波函数\n作张量积形成量子多体波函数表示的复合系统, 进而表示一系列的词. 量子多体波函数是用高维张量\n表示的, 我们需要对它进行分解, 分解之后可以得到张量网络表示. 张量网络在数学和物理领域应用\n广泛, 可解释性强, 所以用张量网络结构来指导神经网络建模语言, 神经网络就是一个可解释的网络.\n在量子力学中, 复合系统的两个部分 (问句与答句) 的纠缠关系在自然语言处理的问答任务中是存在\n的. 因此可以尝试借助张量网络实现量子纠缠的语言建模, 进一步地, 基于量子语言模型的文本生成\n任务也将成为可能. 复数域词向量 [60] 的出现, 也启发我们在未来的工作中可以研究复数域的量子语\n言模型.\n6 总结\n本文从量子公理入手, 阐述了量子力学 4 个基本公理及其与语言建模的关系. 然后, 基于该公理\n体系, 详细介绍了 3 种量子语言模型, 分别是信息检索领域用于文本匹配的量子语言模型, 语音处理\n领域用于计算词语序列条件概率的量子语言模型, 以及在自动问答匹配任务中提出的端到端的量子语\n言模型. 为建立量子语言模型与神经网络之间的本质联系, 我们提出进一步的研究思路, 即利用量子\n多体波函数建模语言. 进而, 使得语言模型、 神经网络和量子力学三者能够逐步建立等价类关系, 旨在\n从基础理论和实际应用等各个方面, 更好地建模语言, 并使之应用于更多的自然语言处理任务.\n参考文献\n1 Minsky M. Semantic Information Processing. Cambridge: MIT Press, 1968. 440–441\n2 Schank R. Conceptual Information Processing. Amsterdam: Elsevier Science Inc, 1975. 5–21\n3 Bendersky M, Croft W B. Modeling higher-order term dependencies in information retrieval using query hypergraphs.\nIn: Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information\nRetrieval, Portland, 2012. 941–950\n4 Hofmann T. Probabilistic latent semantic indexing. In: Proceedings of the 22nd International ACM SIGIR Conference\non Research and Development in Information Retrieval, Berkeley, 1999. 50–57\n5 Harris Z S. Distributional structure. Word, 1954, 10: 146–162\n6 Zhai C X. Statistical language models for information retrieval. In: Proceedings of Human Language Technology\nConference of the North American Chapter of the Association of Computational Linguistics, New York, 2007. 1: 3–4\n7 Brown P F, Desouza P V, Mercer R L, et al. Class-based n-gram models of natural language. Comput Linguist, 1992,\n18: 467–479\n8 Deerwester S, Dumais S T, Furnas G W, et al. Indexing by latent semantic analysis. J Am Soc Inf Sci , 1990, 41:\n391–407\n9 Xu W, Rudnicky A. Can artiﬁcial neural networks learn language models? In: Procedings of the 6th International\nConference on Spoken Language Processing, 2000\n10 Bengio Y, Ducharme R, Vincent P, et al. A neural probabilistic language model. J Mach Learn Res, 2003, 3: 1137–1155\n11 Sun F, Guo J, Lan Y, et al. Sparse word embeddings using l1 regularized online learning. In: Proceedings of the 25th\nInternational Joint Conference on Artiﬁcial Intelligence, 2016. 2915–2921\n12 Metzler D, Croft W B. A Markov random ﬁeld model for term dependencies. In: Proceedings of the 28th Annual\nInternational ACM SIGIR Conference on Research and Development in Information Retrieval. Salvador, 2005. 472–479\n13 Sordoni A, Nie J, Bengio Y. Modeling term dependencies with quantum language models for IR. In: Proceedings of\nthe 36th International ACM SIGIR Conference on Research and Development in Information Retrieval, Dublin, 2013.\n653–662\n14 Robins D. Interactive information retrieval: context and basic notions. J Inform Sci, 2000, 3: 57–62\n1483\n张鹏等: 量子语言模型研究综述\n15 Magerman D M. Statistical decision-tree models for parsing. In: Proceedings of the 33rd Annual Meeting on Association\nfor Computational Linguistics, Cambridge, 1995. 276–283\n16 Bahl L R, Brown P F, de Souza P V, et al. A tree-based statistical language model for natural language speech\nrecognition. IEEE Trans Acoust Speech Signal Process , 1989, 37: 1001–1008\n17 Rosenfeld R, Carbonell J G, Rudnicky A, et al. Adaptive statistical language modeling: a maximum entropy approach.\nDissertation for Ph.D. Degree. Washington: Naval Research Laboratory, 2005\n18 Wang J C, Xiao R, Sun Z X, et al. Research progress of web information retrieval. Comput Res Develop, 2001, 2:\n187–193 [ 王继成, 萧嵘, 孙正兴, 等. Web 信息检索研究进展. 计算机研究与发展, 2001, 2: 187–193]\n19 Manning C D, Raghavan P, Sch¨ utze H. Introduction to Information Retrieval. Cambridge: Cambridge University,\n2008, 151: 5\n20 Salton G, Fox E A, Wu H. Extended Boolean information retrieval. Commun ACM , 1983, 26: 1022–1036\n21 Salton G, Wong A, Yang C S. A vector space model for automatic indexing. Commun ACM , 1975, 18: 613–620\n22 Robertson S. Understanding inverse document frequency: on theoretical arguments for IDF. J Documentation , 2004,\n60: 503–520\n23 Fuhr N. Probabilistic models in information retrieval. Comput J , 1992, 35: 243–255\n24 Robertson S, Zaragoza H. The probabilistic relevance framework: BM25 and beyond. J Found Trends Inf Ret, 2009,\n3: 333–389\n25 Laﬀerty J, Zhai C. Document language models, query models, and risk minimization for information retrieval. In:\nProceedings of the 24th Annual International ACM SIGIR Conference, Princeton, 2001. 111–119\n26 Zhai C, Laﬀerty J. A study of smoothing methods for language models applied to ad hoc information retrieval. In:\nProceedings of the 24th Annual International ACM SIGIR Conference, Princeton, 2001. 334–342\n27 Sennrich R. Perplexity minimization for translation model domain adaptation in statistical machine translation. In:\nProceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, Avignon,\n2012. 539–549\n28 Blei D M, Ng A Y, Jordan M I. Latent dirichlet allocation. J Mach Learn Res, 2003, 3: 993–1022\n29 Zhao Q, Tong L, Swami A, et al. Decentralized cognitive MAC for opportunistic spectrum access in ad hoc networks:\na POMDP framework. IEEE J Sel Areas Commun , 2007, 25: 589–600\n30 van Rijsbergen C J. The Geometry of Information Retrieval. Cambridge: Cambridge University Press, 2004. 15–20\n31 Zhang P, Song D W, Hou Y X, et al. Automata modeling for cognitive interference in users relevance judgment. In:\nProceedings of Symposium on Quantum Informatics for Cognitive, Social, and Semantic Processes, 2010. 125–133\n32 Wang B, Zhang P, Li J, et al. Exploration of quantum interference in document relevance judgement discrepancy.\nEntropy, 2016, 18: 144\n33 Zuccon G, Azzopardi L, van Rijsbergen K. The Quantum Probability Ranking Principle for Information Retrieval.\nBerlin: Springer, 2009. 232–240\n34 Sordoni A, He J, Nie J. Modeling latent topic interactions using quantum interference for information retrieval. In:\nProceedings of the 22nd CIKM, 2013. 1197–1200\n35 Zhang P, Li J, Wang B, et al. A quantum query expansion approach for session search. Entropy, 2016, 18: 146\n36 Zhang P, Song D W, Zhao X Z, et al. Investigating query-drift problem from a novel perspective of Photon polarization.\nBerlin: Springer, 2011, 6931: 332–336\n37 Zhao X, Zhang P, Song D, et al. A novel re-ranking approach inspired by quantum measurement. In: Proceedings of\nEuropean Conference on Information Retrieval. Berlin: Springer, 2011. 721–724\n38 Xie M J, Hou Y X, Zhang P, et al. Modeling quantum entanglements in quantum language models. In: Proceedings\nof the International Joint Conferences on Artiﬁcial Intelligence, 2015. 1362–1368\n39 Piwowarski B, Frommholz I, Lalmas M. What can quantum theory bring to information retrieval. In: Proceedings of\nthe 19th ACM International Conference on Information and Knowledge Management, 2010. 59–68\n40 Frommholz I, Larsen B, Piwowarski B, et al. Supporting poly representation in a quantum-inspired geometrical\nretrieval framework. In: Proceedings of the 3rd Symposium on Information Interaction in Context, 2010. 115–124\n41 Haven E, Khrennikov A. Quantum Social Science. Cambridge: Cambridge University Press, 2013\n42 Bruza P D, Wang Z, Busemeyer J R. Quantum cognition: a new theoretical approach to psychology. Trends Cogn Sci ,\n2015, 19: 383–393\n1484\n中国科学: 信息科学 第 48 卷 第 11 期\n43 Nielsen M A, Chuang I L. Quantum Computation and Quantum Information. Cambridge: Cambridge University\nPress, 2000 [ 赵千川, 译. 量子计算和量子信息 (一). 北京: 清华大学出版社, 2004. 44–88]\n44 von Neumann J. Mathematical Foundations of Quantum Mechanics. Princeton: Princeton University Press, 1996\n45 Basile I, Tamburini F. Towards quantum language models. In: Proceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing, 2017. 1840–1849\n46 Zhang P, Niu J B, Su Z, et al. End-to-End quantum-like language models with application to question answering. In:\nProceedings of the 32nd AAAI Conference on Artiﬁcial Intelligence, New Orleans, 2018\n47 Lvovsky A I. Iterative maximum-likelihood reconstruction in quantum homodyne tomography. J Opt B-Quantum\nSemiclass Opt , 2004, 6: S556–S559\n48 van Rijsbergen C J. The Geometry of Information Retrieval. Cambridge: Cambridge University Press, 2004. 39–40\n49 Shi Y Z, Zhang W Q, Cai M, et al. Variance regularization of RNNLM for speech recognition. In: Proceedings of\nIEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Florence, 2014. 4893–4897\n50 Greﬀ K, Srivastava R K, Koutn´ ık J, et al. LSTM: a search space odyssey. IEEE Trans neural Netw Learn Syst, 2017,\n28: 2222–2232\n51 Spengler C, Huber M, Hiesmayr B C. A composite parameterization of unitary groups, density matrices and subspaces.\nJ Phys A-Math Theor , 2010, 43: 385306\n52 Pennington J, Socher R, Manning C. Glove: global vectors for word representation. In: Proceedings of the 2014\nConference on Empirical Methods in Natural Language Processing (EMNLP). 2014. 1532–1543\n53 Yang Y, Yih W, Meek C. Wikiqa: a challenge dataset for open-domain question answering. In: Proceedings of the\n2015 Conference on Empirical Methods in Natural Language Processing, 2015. 2013–2018\n54 Wang M, Smith N A, Mitamura T. What is the Jeopardy model? A quasi-synchronous grammar for QA. In: Proceed-\nings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural\nLanguage Learning, 2007\n55 Carleo G, Troyer M. Solving the quantum many-body problem with artiﬁcial neural networks. Science, 2017, 355:\n602–606\n56 Levine Y, Yakira D, Cohen N, et al. Deep learning and quantum entanglement: fundamental connections with\nimplications to network design. In: Proceedings of the 6th International Conference on Learning Representations,\n2018\n57 Levine Y, Sharir O, Shashua A. Beneﬁts of depth for long-term memory of recurrent networks. In: Proceedings of the\n6th International Conference on Learning Representations, 2018\n58 Cohen N, Sharir O, Shashua A. On the expressive power of deep learning: a tensor analysis. In: Proceedings of\nConference on Learning Theory, 2016. 698–728\n59 Zhang P, Su Z, Zhang L P, et al. A quantum many-body wave function inspired language modeling approach. In:\nProceedings of the 27th ACM International Conference on Information and Knowledge Management, 2018\n60 Li Q C, Uprety S, Wang B Y, et al. Quantum-inspired complex word embedding. In: Proceedings of the 3rd Workshop\non Representation Learning for NLP, 2018\n1485\n张鹏等: 量子语言模型研究综述\nA survey of quantum language models\nPeng ZHANG 1*, Xindian MA 1 & Dawei SONG 2\n1. School of Computer Science and Technology, Tianjin University, Tianjin300350, China;\n2. School of Computer Science& Technology, Beijing Institute of Technology, Beijing100081, China\n* Corresponding author. E-mail: pzhang@tju.edu.cn\nAbstract Language model is a fundamental research topic in areas related to natural language processing. In\nrecent years, researchers have proposed quantum language models based on the probability theory of quantum\nmechanics. This paper aims to review the research motivation and the current progress of constructing various\nquantum language models. First, it reviews the research problems of classical language models. Then, it intro-\nduces some quantum language models in information retrieval and speech processing, as well as an end-to-end\nquantum language model based on neural network architecture. By analyzing the advantages and disadvantages\nof each quantum language model considered here, taking into account the essential connection between quantum\nmechanics and neural networks, we outline our vision for future research directions.\nKeywords language model, quantum language model, neural network, information retrieval, quantum mechan-\nics\nPeng ZHANGwas born in 1983. He\nreceived his Ph.D. degree from Robert\nGordon University, Aberdeen, United\nKingdom, in 2013. Currently, he is\nan associate professor in the School\nof Computer Science and Technology,\nTianjin University. His research inter-\nests include information retrieval, nat-\nural language processing, and quantum\nlanguage models.\nXindian MAwas born in 1995. He re-\nceived his Bachelor degree from Shanxi\nAgricultural University, Shanxi, China,\nin 2017. Currently, he is a postgraduate\nstudent at Tianjin University. His re-\nsearch interest focuses on quantum lan-\nguage models.\nDawei SONG was born in 1972. He\nreceived his Ph.D. degree from the Chi-\nnese University of Hong Kong. Cur-\nrently, he is a professor in the School\nof Computer Science and Technology,\nBeijing Institute of Technology. His\nresearch interests include information\nretrieval, natural language processing,\nand quantum cognition.\n1486",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6818918585777283
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4403197765350342
    },
    {
      "name": "Quantum information science",
      "score": 0.4402041435241699
    },
    {
      "name": "Language model",
      "score": 0.42790791392326355
    },
    {
      "name": "Cognitive science",
      "score": 0.42134788632392883
    },
    {
      "name": "Quantum",
      "score": 0.4006360173225403
    },
    {
      "name": "Quantum mechanics",
      "score": 0.14777472615242004
    },
    {
      "name": "Physics",
      "score": 0.11909204721450806
    },
    {
      "name": "Quantum entanglement",
      "score": 0.11069157719612122
    },
    {
      "name": "Psychology",
      "score": 0.1100720763206482
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I125839683",
      "name": "Beijing Institute of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I132369690",
      "name": "Tianjin University of Science and Technology",
      "country": "CN"
    }
  ],
  "cited_by": 2
}