{
  "title": "Heterogeneous Graph Transformer for Graph-to-Sequence Learning",
  "url": "https://openalex.org/W3035702572",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2136007140",
      "name": "Shaowei Yao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2159238107",
      "name": "Tianming Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101284925",
      "name": "Xiao-jun Wan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2922349260",
    "https://openalex.org/W2971187756",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2971087717",
    "https://openalex.org/W2796167946",
    "https://openalex.org/W2924961378",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W2890212927",
    "https://openalex.org/W2804057010",
    "https://openalex.org/W3207696368",
    "https://openalex.org/W2133459682",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2798749466",
    "https://openalex.org/W2600565200",
    "https://openalex.org/W2987178699",
    "https://openalex.org/W2963858333",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2951309718",
    "https://openalex.org/W2963374482",
    "https://openalex.org/W2963653811",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4286715520",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W2250342921",
    "https://openalex.org/W2998702685",
    "https://openalex.org/W2102605133",
    "https://openalex.org/W2964116568",
    "https://openalex.org/W2964035651",
    "https://openalex.org/W2136516698",
    "https://openalex.org/W2251823395",
    "https://openalex.org/W2964114465",
    "https://openalex.org/W2963212250"
  ],
  "abstract": "The graph-to-sequence (Graph2Seq) learning aims to transduce graph-structured representations to word sequences for text generation. Recent studies propose various models to encode graph structure. However, most previous works ignore the indirect relations between distance nodes, or treat indirect relations and direct relations in the same way. In this paper, we propose the Heterogeneous Graph Transformer to independently model the different relations in the individual subgraphs of the original graph, including direct relations, indirect relations and multiple possible relations between nodes. Experimental results show that our model strongly outperforms the state of the art on all four standard benchmarks of AMR-to-text generation and syntax-based neural machine translation.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7145–7154\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n7145\nHeterogeneous Graph Transformer for Graph-to-Sequence Learning\nShaowei Yao, Tianming Wang, Xiaojun Wan\nWangxuan Institute of Computer Technology, Peking University\nCenter for Data Science, Peking University\nThe MOE Key Laboratory of Computational Linguistics, Peking University\n{yaosw,wangtm,wanxiaojun}@pku.edu.cn\nAbstract\nThe graph-to-sequence (Graph2Seq) learning\naims to transduce graph-structured represen-\ntations to word sequences for text generation.\nRecent studies propose various models to en-\ncode graph structure. However, most previous\nworks ignore the indirect relations between dis-\ntance nodes, or treat indirect relations and di-\nrect relations in the same way. In this paper,\nwe propose the Heterogeneous Graph Trans-\nformer to independently model the different re-\nlations in the individual subgraphs of the origi-\nnal graph, including direct relations, indirect\nrelations and multiple possible relations be-\ntween nodes. Experimental results show that\nour model strongly outperforms the state of the\nart on all four standard benchmarks of AMR-\nto-text generation and syntax-based neural ma-\nchine translation.\n1 Introduction\nGraph-to-sequence (Graph2Seq) learning has at-\ntracted lots of attention in recent years. Many Nat-\nural Language Process (NLP) problems involve\nlearning from not only sequential data but also\nmore complex structured data, such as semantic\ngraphs. For example, AMR-to-text generation is\na task of generating text from Abstract Meaning\nRepresentation (AMR) graphs, where nodes denote\nsemantic concepts and edges refer to relations be-\ntween concepts (see Figure 1 (a)). In addition, it\nhas been shown that even if the sequential input can\nbe augmented by additional structural information,\nbringing beneﬁts for some tasks, such as semantic\nparsing (Pust et al., 2015; Guo and Lu, 2018) and\nmachine translation (Bastings et al., 2017). There-\nfore, Xu et al. (2018b) introduced the Graph2Seq\nproblems which aim to generate target sequence\nfrom graph-structured data.\nThe main challenge for Graph2Seq learning is\nto build a powerful encoder which is able to cap-\nture the inherent structure in the given graph and\nlearn good representations for generating the tar-\nget text. Early work relies on statistical methods\nor sequence-to-sequence (Seq2Seq) models where\ninput graphs are linearized (Lu et al., 2009; Song\net al., 2017; Konstas et al., 2017). Recent studies\npropose various models based on graph neural net-\nwork (GNN) to encode graphs (Xu et al., 2018b;\nBeck et al., 2018; Guo et al., 2019; Damonte and\nCohen, 2019; Ribeiro et al., 2019). However, these\napproaches only consider the relations between di-\nrectly connected nodes, ignore the indirect relations\nbetween distance nodes. Inspired by the success\nof Transformer (Vaswani et al., 2017) which can\nlearn the dependencies between all tokens without\nregard to their distance, the current state-of-the-\nart Graph2Seq models (Zhu et al., 2019; Cai and\nLam, 2020) are based on Transformer and learn\nthe relations between all nodes no matter they are\nconnected or not. These approaches use shortest\nrelation path between nodes to encode semantic re-\nlationships. However, they ignore the information\nof nodes in the relation path and encode the direct\nrelations and indirect relations without distinction.\nIt may disturb the information propagation process\nwhen aggregate information from direct neighbors.\nTo solve the issues above, we propose the Het-\nerogeneous Graph Transformer (HetGT) to encode\nthe graph, which independently model the different\nrelations in the individual subgraphs of the original\ngraph. HetGT is adapted from Transformer and\nit also employs an encoder-decoder architecture.\nFollowing Beck et al. (2018), we ﬁrst transform the\ninput into its corresponding Levi graph which is a\nheterogeneous graph (contains different types of\nedges). Then we split the transformed graph into\nmultiple subgraphs according to its heterogeneity,\nwhich corresponds to different representation sub-\nspaces of the graph. For updating the node repre-\nsentations, attention mechanisms are used for inde-\n7146\nFigure 1: (a) An example of AMR graph for the sentence of Here it is a country with the freedom of speech. (b) Its\ncorresponding extended Levi graph with three types of edges. (c) The architecture of HetGT encoder.\npendently aggregating information in different sub-\ngraphs. Finally, the representations of each node\nobtained in different subgraphs are concatenated\ntogether and a parameterized linear transformation\nis applied. In this way, HetGT could adaptively\nmodel the various relations in the graph indepen-\ndently, avoiding the information loss caused by\nmixing all of them. Moreover, we introduce the\njump connection in our model, which signiﬁcantly\nimproves the model performance.\nWe evaluate our model on four benchmark\ndatasets of two Graph2Seq tasks: the AMR-to-\ntext generation and the syntax-based Neural Ma-\nchine Translation (NMT). In terms of various eval-\nuation metrics, our model strongly outperforms\nthe state-of-the-art (SOTA) results on both two\ntasks. Particularly, in AMR-to-text generation, our\nmodel improves the BLEU scores of the SOTA by\nabout 2.2 points and 2.3 points on two benchmark\ndatasets (LDC2015E86 and LDC2017T10). In\nsyntax-based NMT, our model surpasses the SOTA\nby about 4.1 and 2.2 BLEU scores for English-\nGerman and English-Czech on News Commentary\nv11 datasets from the WMT16 translation task. Our\ncontributions can be summarized as follows:\n•We propose the Heterogeneous Graph Trans-\nformer (HetGT) which adaptively models the\nvarious relations in different representation\nsubgraphs.\n•We analyze the shortcomings of the residual\nconnection and introduce a better connectivity\nmethod around encoder layers.\n•Experimental results show that our model\nachieves new state-of-the-art performance on\nfour benchmark datasets of two Graph2Seq\ntasks.\n2 Neural Graph-to-Sequence Model\nIn this section, we will ﬁrst begin with a brief re-\nview of the Transformer which is the basis of our\nmodel. Then we will introduce the graph transfor-\nmation process. Finally, we will detail the whole\narchitecture of HetGT.\n2.1 Transformer\nThe Transformer employs an encoder-decoder ar-\nchitecture, consisting of stacked encoder and de-\ncoder layers. Encoder layers consist of two sublay-\ners: a self-attention mechanism and a position-wise\nfeed-forward network. Self-attention mechanism\nemploys h attention heads. Each attention head\noperates on an input sequence x = (x1,...,x n)\nof n elements where xi ∈Rdx, and computes a\nnew sequence z = (z1,...,z n) of the same length\nwhere z ∈Rdz . Finally, the results from all the\nattention heads are concatenated together and a pa-\nrameterized linear transformation is applied to get\nthe output of the self-attention sublayer. Each out-\nput element zi is computed as the weighted sum of\n7147\nFigure 2: An example of graph structure and its exten-\nsion to subword units.\nlinearly transformed input elements:\nzi =\nn∑\nj=1\nαij\n(\nxjWV )\n(1)\nwhere αij is weight coefﬁcient and computed by a\nsoftmax function:\nαij = softmax (eij) = exp eij∑n\nk=1 exp eik\n(2)\nAnd eij is computed using a compatibility function\nthat compares two input elements:\neij =\n(\nxiWQ)(\nxjWK)T\n√dz\n(3)\nScaled dot product was chosen for the compatibil-\nity function. WV ,WQ,WV ∈Rdx×dz are layer-\nspeciﬁc trainable parameter matrices. Meanwhile,\nthese parameter matrices are unique per attention\nhead.\n2.2 Input Graph Transformation\nFollowing Beck et al. (2018), we transform the\noriginal graph into the Levi graph. The transforma-\ntion equivalently turns edges into additional nodes\nso we can encode the original edge labels in the\nsame way as for nodes. We also add a reverse edge\nbetween each pair of connected nodes as well as\na self-loop edge for each node. These strategies\ncan make the model beneﬁt from the information\npropagation from different directions (See Figure 1\n(b)).\nIn order to alleviate the data sparsity problem in\nthe corpus, we further introduce the Byte Pair En-\ncoding (BPE) (Sennrich et al., 2016) into the Levi\nGraph. We split the original node into multiple sub-\nword nodes. Besides adding default connections,\nwe also add the reverse and self-loop edges among\nsubwords. For example, the word country in Fig-\nure 2 is segmented into co@@, un@@, try with\nthree types of edges between them. Finally, we\ntransform the AMR graph into the extended Levi\nGraph which can be seen as a heterogeneous graph,\nas it has different types of edges.\n2.3 Heterogeneous Graph Transformer\nOur model is also an encoder-decoder architecture,\nconsisting of stacked encoder and decoder layers.\nGiven a preprocessed extended Levi graph, we split\nthe extended Levi graph into multiple subgraphs\naccording to its heterogeneity. In each graph en-\ncoder block, the node representation in different\nsubgraphs is updated based on its neighbor nodes in\nthe current subgraph. Then all the representations\nof this node in different subgraphs will be com-\nbined to get its ﬁnal representation. In this way,\nthe model can attend to information from different\nrepresentation subgraphs and adaptively model the\nvarious relations. The learned representations of\nall nodes at the last block are fed to the sequence\ndecoder for sequence generation. The architecture\nof HetGT encoder is shown in Figure 1 (c). Due to\nthe limitation of space the decoder is omitted in the\nﬁgure. We will describe it in Section 2.3.2.\n2.3.1 Graph Encoder\nUnlike previous Transformer-based Graph2Seq\nmodels using relative position encoding to incorpo-\nrate structural information, we use a simpler way to\nencode the graph structure. As Transformer treats\nthe sentence as a fully-connected graph, we directly\nmask the non-neighbor nodes’ attention when up-\ndating each node’s representation. Speciﬁcally, we\nmask the attention αij for node j /∈Ni, where Ni\nis the set of neighbors of node iin the graph. So\ngiven the input sequence x= (x1,...,x n), the out-\nput representation of node idenoted as zi in each\nattention head is computed as follows:\nzi =\n∑\nj∈Ni\nαij\n(\nxjWV )\n(4)\nwhere αij represents the attention score of node j\nto iwhich is computed using scaled dot-product\nfunction as in Equation 2.\nWe also investigate another way to compute at-\ntention scores. We use the additive form of atten-\ntion instead of scaled dot-product attention, which\nis similar to graph attention network (Veli ˇckovi´c\net al., 2018). The additive form of attention shows\nbetter performance and trainability in some tasks\n(Chen et al., 2019). The attention coefﬁcient αij is\n7148\ncomputed as follows:\nαij = softmax (eij) = exp eij∑\nk∈Ni exp eik\neij = LeakyReLU\n(\naT [xiWV ; xjWV ]\n) (5)\nwhere a ∈R2dz is a weight vector. LeakyReLU\n(Girshick et al., 2014) is used as the activation func-\ntion.\n2.3.2 Heterogeneous Mechanism\nMotivated by the success of the multi-head mech-\nanism, we propose the heterogeneous mechanism.\nConsidering a sentence, multi-head attention al-\nlows the model to implicitly attend to information\nfrom different representation subspaces at differ-\nent positions. Correspondingly, our heterogeneous\nmechanism makes the model explicitly attend to\ninformation in different subgraphs, corresponding\nto different representation subspaces of the graph,\nwhich enhances the models’ encoding capability.\nAs stated above, the extended Levi graph is a het-\nerogeneous graph which contains different types\nof edges. For example, in Figure 1 (b), the edge\ntype vocabulary for the Levi graph of the AMR\ngraph is T ={default, reverse, self}. Speciﬁcally,\nwe ﬁrst group all edge types into a single one to\nget a homogeneous subgraph referred to connected\nsubgraph. The connected subgraph is actually an\nundirected graph which contains the complete con-\nnected information in the original graph. Then we\nsplit the input graph into multiple subgraphs accord-\ning to the edge types. Besides learning the directly\nconnected relations, we introduce afully-connected\nsubgraph to learn the implicit relationships between\nindirectly connected nodes. Finally, we get the set\nof subgraphs including M elements Gsub ={fully-\nconnected, connected, default, reverse}. For AMR\ngraph M = 4 (For NMT M = 6, we will detail\nit in section 3.1). Note that we do not have a sub-\ngraph only containing self edges. Instead, we add\nthe self-loop edges into all subgraphs. We think it\nis more helpful for information propagation than\nconstructing an independent self-connected sub-\ngraph. Now the output zin each encoder layer is\ncomputed as follows:\nz= FFN\n(\nconcat\n(\nzGsub\n1 ,...,z Gsub\nM\n)\nWO\n)\nzGsub\nm\ni =\n∑\nj∈NGsubm\ni\nαij\n(\nxjWV )\n,m ∈[1,M] (6)\nwhere WO ∈RMdz×dz is the parameter matrix.\nNGsub\nm\ni is the set of neighbors in the m-th subgraph\nFigure 3: Different layer aggregation methods: residual\n(left), jump (middle), dense (right).\nof node i. αij is computed as Equation 2 or Equa-\ntion 5. FFN is a feed-forward network which con-\nsists of two linear transformations with a ReLU\nactivation in between. We also employ the resid-\nual connections between sublayers as well as layer\nnormalization. Note that the heterogeneous mecha-\nnism is independent of the model architecture, so\nit can be applied to any other graph models which\nmay bring beneﬁts.\nFor decoder, we follow the standard implemen-\ntation of the sequential Transformer decoder to\ngenerate the text sequence. The decoder layers con-\nsist of three sublayers: self-attention followed by\nencoder-decoder attention, followed by a position-\nwise feed-forward layer.\n2.3.3 Layer Aggregation\nAs stated above, our model consists of stacked\nencoder layers. A better information propagation\nbetween encoder layers may bring better perfor-\nmance. Therefore, we investigate three different\nlayer aggregation methods, which are illustrated in\nFigure 3. When updating the representation of each\nnode at l-th layer, recent approaches aggregate the\nneighbors ﬁrst and then combine the aggregated re-\nsult with the node’s representation from(l−1)-th\nlayer. This strategy can be viewed as a form of a\nskip connection between different layers (Xu et al.,\n2018a):\nz(l)\nNi = AGGREGATE\n(\n{z(l−1)\nj ,∀j ∈Ni}\n)\nz(l)\ni = COMBINE\n(\nz(l)\nNi,z(l−1)\ni\n) (7)\nThe residual connection is another well-known skip\nconnection which uses the identity mapping as the\ncombine function to help signals propagate (He\net al., 2016). However, these skip connections\ncannot adaptively adjust the neighborhood size of\nthe ﬁnal-layer representation independently. If we\n”skip” a layer for z(l)\ni , all subsequent units such as\n7149\nz(l+j)\ni using this representation will be using this\nskip implicitly. Thus, to selectively aggregate the\noutputs of previous layers at the last, we introduce\nthe Jumping Knowledge architecture (Xu et al.,\n2018a) in our model. At the last layer L of the\nencoder, we combine all the outputs of previous\nencoder layers by concatenation to help the model\nselectively aggregate all of those intermediate rep-\nresentations.\nzﬁnal\ni = Concat\n(\nz(L)\ni ,...,z (1)\ni ,xi\n)\nWjump (8)\nwhere Wjump ∈ R(Ldz+dx)×dz . Furthermore, to\nbetter improve information propagation, dense con-\nnectivity can be introduced as well. With dense\nconnectivity, the nodes in l-th layer not only take\ninput from (l−1)-th layer but also draw informa-\ntion from all preceding layers:\nz(l)\ni = Concat\n(\nz(l−1)\ni ,...,z (1)\ni ,xi\n)\nW(l)\ndense (9)\nwhere W(l)\ndense ∈Rd(l)×dz . d(l) = dx + dz ×(l−1).\nDense connectivity are also introduced in previous\nresearches (Huang et al., 2017; Guo et al., 2019).\n3 Experiments\n3.1 Data and preprocessing\nWe build and test our model on two typical\nGraph2Seq learning tasks. One is AMR-to-text\ngeneration and the other is syntax-based NMT. Ta-\nble 1 presents the statistics of four datasets of the\ntwo tasks. For AMR-to-text generation, we use\ntwo standard benchmarks LDC2015E86 (AMR15)\nand LDC2017T10 (AMR17). These two datasets\ncontain 16K and 36K training instances, respec-\ntively, and share the development and test set. Each\ninstance contains a sentence and an AMR graph.\nIn the preprocessing steps, we apply entity sim-\npliﬁcation and anonymization in the same way as\nKonstas et al. (2017). Then we transform each\npreprocessed AMR graph into its extended Levi\ngraph as described in Section 2.2.\nFor the syntax-based NMT, we take syntac-\ntic trees of source texts as inputs. We evaluate\nour model on both English-German (En-De) and\nEnglish-Czech (En-Cs) News Commentary v11\ndatasets from the WMT16 translation task 1. Both\nsides are tokenized and split into subwords using\nBPE with 8000 merge operations. English text is\nparsed using SyntaxNet (Alberti et al., 2017). Then\n1http://www.statmt.org/wmt16/translation-task.html\nDataset Train Dev Test\nLDC2015E86 (AMR15) 16,833 1,368 1,371\nLDC2017T10 (AMR17) 36,521 1,368 1,371\nEnglish-Czech (En-Cs) 181,112 2,656 2,999\nEnglish-German (En-De) 226,822 2,169 2,999\nTable 1: The statistics of four datasets. The ﬁrst two\nare datasets in AMR-to-text generation subtask, the last\ntwo are datasets in syntax-based NMT subtask.\nwe transform the labeled dependency tree into the\nextended Levi graph as described in Section 2.2.\nUnlike AMR-to-text generation, in NMT task the\ninput sentence contains signiﬁcant sequential in-\nformation. This information is lost when treating\nthe sentence as a graph. Guo et al. (2019) consider\nthis information by adding sequential connections\nbetween each word node. In our model, we also\nadd forward and backward edges in the extended\nLevi graph. Thus, the edge types vocabulary for\nthe extended Levi graph of the dependency tree is\nT ={default, reverse, self, forward, backward }.\nSo the set of subgraphs for NMT is Gsub = {fully-\nconnected, connected, default, reverse, forward,\nbackward}. Note that we do not change the model\narchitecture in the NMT tasks. However, we still\nget good results, which indicates the effectiveness\nof our model on Graph2Seq tasks. Except for in-\ntroducing BPE into Levi graph, the above prepro-\ncessing steps are following Bastings et al. (2017).\nWe refer to them for further information on the\npreprocessing steps.\n3.2 Parameter Settings\nBoth our encoder and decoder have 6 layers with\n512-dimensional word embeddings and hidden\nstates. We employ 8 heads and dropout with a\nrate of 0.3. For optimization, we use Adam opti-\nmizer with β2 = 0.998 and set batch size to 4096\ntokens. Meanwhile, we increase learning rate lin-\nearly for the ﬁrst warmup steps, and decrease\nit thereafter proportionally to the inverse square\nroot of the step number. We set warmup steps\nto 8000. The similar learning rate schedule is\nadopted in (Vaswani et al., 2017). Our implementa-\ntion uses the openNMT library (Klein et al., 2017).\nWe train the models for 250K steps on a single\nGeForce GTX 1080 Ti GPU. Our code is available\nat https://github.com/QAQ-v/HetGT.\n7150\nModel LDC2015E86 (AMR15) LDC2017T10 (AMR17)\nBLEU CHRF++ METEOR BLEU CHRF++ METEOR\nGGNN2Seq (Beck et al., 2018) - - - 23.3 50.4 -\nGraphLSTM (Song et al., 2018) 23.3 - - - - -\nGCNSEQ (Damonte and Cohen, 2019)24.40 - 23.60 24.54 - 24.07\nDGCN (Guo et al., 2019) 25.9 - - 27.9 57.3 -\nG2S-GGNN (Ribeiro et al., 2019) 24.32 - 30.53 27.87 - 33.21\nTransformer-SA (Zhu et al., 2019) 29.66 63.00 35.45 31.54 63.84 36.02\nTransformer-CNN (Zhu et al., 2019)29.10 62.10 35.00 31.82 64.05 36.38\nGTransformer (Cai and Lam, 2020) 27.4 56.4 32.9 29.8 59.4 35.1\nGGNN2Seqensemble(Beck et al., 2018) - - - 27.5 53.5 -\nDGCNensemble(Guo et al., 2019) 28.2 - - 30.4 59.6 -\nTransformer 25.69 60.10 33.88 27.60 61.78 35.21\nHetGTdot-product(ours) 31.29 63.62 36.71 33.16 65.08 37.75\nHetGTadditive(ours) 31.84 63.81 36.89 34.10 65.60 38.10\nTable 2: Results for AMR-to-text generation on the test sets of AMR15 and AMR17.\nModel English-German English-Czech\nBLEU CHRF++ METEOR BLEU CHRF++ METEOR\nBiRNN+GCN (Bastings et al., 2017)16.1 - - 9.6 - -\nGGNN2Seq (Beck et al., 2018) 16.7 42.4 - 9.8 33.3 -\nDGCN (Guo et al., 2019) 19.0 44.1 - 12.1 37.1 -\nGTransformer (Cai and Lam, 2020) 21.3 47.9 - 14.1 41.1 -\nGGNN2Seqensenmble(Beck et al., 2018) 19.6 45.1 - 11.7 35.9 -\nDGCNensemble(Guo et al., 2019) 20.5 45.8 - 13.1 37.8 -\nTransformer 23.18 49.54 26.00 14.83 39.27 19.12\nHetGTdot-product(ours) 25.39 51.55 27.37 16.15 41.10 20.18\nHetGTadditive(ours) 25.44 51.27 27.26 16.29 41.14 20.35\nTable 3: Results for syntax-based NMT on the test sets of En-De and En-Cs.\n3.3 Metrics and Baselines\nFor performance evaluation, we use BLEU (Pa-\npineni et al., 2002), METEOR (Denkowski and\nLavie, 2014) and sentence-level CHRF++ (Popovi´c,\n2015) with default hyperparameter settings as eval-\nuation metrics. Meanwhile, we use the tools in\nNeubig et al. (2019) for the statistical signiﬁcance\ntests.\nOur baseline is the original Transformer 2. For\nAMR-to-text generation, Transformer takes lin-\nearized graphs as inputs. For syntax-based NMT,\nTransformer is trained on the preprocessed trans-\nlation dataset without syntactic information. We\nalso compare the performance of HetGT with pre-\nvious single/ensenmble approaches which can be\ngrouped into three categories: (1) Recurrent neu-\n2Parameters were chosen following the OpenNMT\nFAQ: http://opennmt.net/OpenNMT-py/FAQ.html#how-do-i-\nuse-the-transformer-model\nral network (RNN) based methods (GGNN2Seq,\nGraphLSTM); (2) Graph neural network (GNN)\nbased methods (GCNSEQ, DGCN, G2S-GGNN);\n(3) The Transformer based methods (Structural\nTransformer, GTransformer). The ensemble mod-\nels are denoted by subscripts in Table 2 and Table\n3.\n3.4 Results on AMR-to-text Generation\nTable 2 presents the results of our single model\nand previous single/ensemble models on the test\nsets of AMR15 and AMR17. We can see that\nour Transformer baseline outperforms most pre-\nvious single models, and our best single model\nHetGTadditive outperforms the Transformer baseline\nby a large margin (6.15 BLEU and 6.44 BLEU) on\nboth benchmarks. It demonstrates the importance\nof incorporating structural information. Mean-\nwhile, HetGTadditive gets an improvement of 2.18\nand 2.28 BLEU points over the latest SoTA results\n7151\n(Zhu et al., 2019) on AMR15 and AMR17, respec-\ntively. Previous models can capture the structural\ninformation but most of them ignore heterogeneous\ninformation. These results indicate that the hetero-\ngeneity in the graph carries lots of useful informa-\ntion for the downstream tasks, and our model can\nmake good use of it.\nFurthermore, our best single model still has bet-\nter results than previous ensemble models on both\ntwo datasets. Note that additive attention based\nmodel HetGTadditive is signiﬁcantly better that dot-\nproduct attention based model HetGTdot-product in\nAMR-to-text generation. It may be attributed to\nthat the additive attention has less parameters and\nis easier to train on the small dataset.\n3.5 Results on Syntax-based NMT\nTable 3 presents the results of our single model\nand previous single/ensemble models on the test\nsets for En-De and En-Cs language pairs. We can\nsee that our Transformer baseline already outper-\nforms all previous results even though some of\nthem are Transformer based. It shows the effective-\nness of Transformer for NMT tasks. Meanwhile,\neven without changing the model architecture for\nthe NMT tasks, our single model surpasses Trans-\nformer baseline by 2.26 and 1.46 BLEU points\non the En-De and En-Cs tasks, respectively, and\nour model surpasses previous best models by 4.14\nand 2.19 BLEU points. In syntax-based NMT\nwhere the dataset is larger than AMR-to-text gener-\nation, the HetGTdot-product gets comparable results\ncompared to the HetGT additive, and even outper-\nforms the HetGTadditive in terms of METEOR and\nCHRF++ on the language pair En-De. We think\non the larger datasets the HetGTdot-product will get\nbetter results than the HetGTadditive.\n4 Additional Experiments\n4.1 Effect of Layer Aggregation Method\nFirstly, we compare the performance of three layer-\naggregation methods discussed in Section 2.3.3.\nMethod HetGTdot-product HetGTadditive\nResidual 30.02 30.56\nJump 31.29 31.84\nDense 29.92 30.41\nTable 4: Results of different layer aggregation methods\non the test set of AMR15.\nBLEU METEOR\nFull Model 31.84 36.89\nw/ onlyfully-connectedsubgraph 25.69 33.88\nw/ onlyconnectedsubgraph 30.22 36.32\nw/ onlydefaultsubgraph 30.47 36.34\nw/ onlyreversesubgraph 29.76 35.97\nw/ofully-connectedsubgraph 30.84 36.35\nw/oconnectedsubgraph 31.62 36.75\nw/odefaultsubgraph 29.68 35.88\nw/oreversesubgraph 29.86 36.03\nw/o BPE 29.84 35.52\nTable 5: Ablation results on the AMR15 test set.\nThe results are shown in Table 4. We can see\nthe jump connection is the most effective method.\nHowever, the dense connection performs the worst.\nWe think the reason is that dense connection intro-\nduce lots of extra parameters which are harder to\nlearn.\n4.2 Effect of Subgraphs\nIn this section, we also use AMR15 as our bench-\nmark to investigate how each subgraph inﬂuences\nthe ﬁnal results of our best model HetGT additive.\nTable 5 shows the results of removing or only keep-\ning the speciﬁc subgraph. Only keeping the fully-\nconnected subgraph essentially is what the Trans-\nformer baseline does. It means the model does\nnot consider the inherent structural information in\ninputs. Obviously, it cannot get a good result. In ad-\ndition, only keeping the connected subgraph does\nnot perform well even it considers the structural in-\nformation. It demonstrates that the heterogeneous\ninformation in the graph is helpful for learning\nthe representation of the graph. When removing\nany subgraph, the performance of the model will\ndecrease. It demonstrates that each subgraph has\ncontributed to the ﬁnal results. At last, we remove\nBPE, and we get 29.84 BLEU score which is still\nbetter than previous SoTA that also uses BPE. Note\nthat when we remove the connected subgraph, the\nresults do not have statistically signiﬁcant changes\n(p = 0.293). We think the reason is that the left\nsubgraphs already contain the full information of\nthe original graph because the connected subgraph\nis obtained by grouping all edge types into a single\none. Except that, all the other results have statisti-\ncally signiﬁcant changes (p≤0.05).\n7152\n(p / possible-01 e.1 :polarity e.2 - e.2\n:ARG1 (w / work-01 e.3,4\n:ARG0 (i / i e.0)\n:location e.5 (h / home e.6))\n:ARG1-of (c / cause-01 e.8\n:ARG0 (s / shout-01 e.10\n:ARG0 (s2 / she e.9)\n:ARG2 e.11 i e.12)))\nREF: i can n’t do work at home , because she shouts at me .\nTransformer: i can n’t do work at home , because she shouts at me .\nHetGTadditive (ours): i can n’t do work at home , because she shouts at me\n.\n(s / say-01 e.1\n:ARG0 (h2 / he e.0)\n:ARG1 e.2 (a / agree-01 e.4\n:ARG0 h2 e.3\n:ARG1 e.5 (o / opine-01 e.9\n:ARG0 e.8 (p2 / person :wiki ”Liu Huaqing”\n:name (n / name :op1 ”Huaqing” e.6 :op2 ”Liu” e.7))\n:ARG1 e.10 (r / recommend-01 e.14\n:ARG1 (d / develop-02 e.16\n:ARG0 (a2 / and e.12\n:op1 (c4 / country :wiki ”Thailand”\n:name (n2 / name :op1 ”Thailand” e.11))\n:op2 (c5 / country :wiki ”China”\n:name (n3 / name :op1 ”China” e.13)))\n:ARG1 (a3 / and e.21\n:op1 (c6 / cooperate-01 e.23\n:ARG2 (e / economy e.20)\n:mod e.19 (f / form e.18\n:mod (v / various e.17)))\nop2 (c7 / cooperate-01 e.23\n:ARG2 (t2 / trade-01 e.22)\n:mod f))\n:degree (f2 / further e.15))))))\nREF: he said that he agreed with huaqing liu ’s opinion that thailand and\nchina should further develop various forms of economic and trade cooper-\nation .\nTransformer: he said huaqing liu agreed to agree with thailand and china\nshould further develop in various forms of economic cooperation and trade\ncooperation .\nHetGTadditive (ours): he said he agreed to huaqing liu ’s opinion that thai-\nland and china should further develop various forms of economic coopera-\ntion and trade cooperation .\nTable 6: Example outputs of different systems are com-\npared, including Transformer baseline and our HetGT.\n4.3 Case Study\nWe perform case studies for better understanding\nthe model performance. We compare the outputs of\nTransformer baseline and our HetGTadditive. The re-\nsults are presented in Table 6. In the ﬁrst simple ex-\nample, our Transformer baseline and HetGTadditive\ncan generate the target sequence without mistakes.\nIn the second example which is more complicated,\nthe Transformer baseline fails to identify the pos-\nsessor of “opinion” and the subject of “agreed”\nwhile our model successfully recognizes them.\nHowever, we ﬁnd the there is a common problem:\nthe sentences they generate all have some duplica-\ntion. We will explore this issue further in the future\nwork.\n5 Related Work\nEarly researches for Graph2Seq learning tasks are\nbased on statistical methods and neural seq2seq\nmodel. Lu et al. (2009) propose an NLG approach\nbuilt on top of tree conditional random ﬁelds to use\nthe tree-structured meaning representation. Song\net al. (2017) use synchronous node replacement\ngrammar to generate text. Konstas et al. (2017)\nlinearize the input graph and feed it to the seq2seq\nmodel for text-to-AMR parsing and AMR-to-text\ngeneration. However, linearizing AMR graphs into\nsequences may incurs in loss of information. Re-\ncent efforts consider to capture the structural in-\nformation in the encoder. Beck et al. (2018) em-\nploy Gated Graph Neural Networks (GGNN) as\nthe encoder and Song et al. (2018) propose the\ngraph-state LSTM to incorporate the graph struc-\nture. Their works belong to the family of recurrent\nneural network (RNN). In addition, there are some\nworks are build upon the GNN. Damonte and Co-\nhen (2019) propose stacking encoders including\nLSTM and GCN. Guo et al. (2019) introduce the\ndensely connected GCN to encode richer local and\nnon-local information for better graph representa-\ntion.\nRecent studies also extend Transformer to en-\ncode structure information. Shaw et al. (2018) pro-\npose the relation-aware self-attention which learns\nexplicit embeddings for pair-wise relationships be-\ntween input elements. Zhu et al. (2019) and Cai\nand Lam (2020) both extend the relation-aware self-\nattention to generate text from AMR graph. Our\nmodel is also based on Transformer. However, we\ndo not employ the relative position encoding to\nincorporate structural information. Instead, we di-\nrectly mask the non-neighbor nodes attention when\nupdating each nodes representation. Moreover, we\nintroduce the heterogeneous information and jump\nconnection to help model learn a better graph rep-\nresentation, bringing substantial gains in the model\nperformance.\n6 Conclusion\nIn this paper, we propose the Heterogeneous Graph\nTransformer (HetGT) for Graph2Seq learning. Our\nproposed heterogeneous mechanism can adaptively\nmodel the different representation subgraphs. Ex-\nperimental results show that HetGT strongly out-\nperforms the state of the art performances on four\nbenchmark datasets of AMR-to-text generation and\nsyntax-based neural machine translation tasks.\nThere are two directions for future works. One\nis to investigate how the other graph models can\nbeneﬁt from our proposed heterogeneous mecha-\n7153\nnism. On the other hand, we would also like to\ninvestigate how to make use of our proposed model\nto solve sequence-to-sequence tasks.\nAcknowledgments\nThis work was supported by National Natural Sci-\nence Foundation of China (61772036) and Key\nLaboratory of Science, Technology and Standard in\nPress Industry (Key Laboratory of Intelligent Press\nMedia Technology). We thank the anonymous re-\nviewers for their helpful comments. Xiaojun Wan\nis the corresponding author.\nReferences\nChris Alberti, Daniel Andor, Ivan Bogatyy, Michael\nCollins, Daniel Gillick, Lingpeng Kong, Terry K\nKoo, Ji Ma, Mark Omernick, Slav Petrov, Chayut\nThanapirom, Zora Tung, and David Weiss. 2017.\nSyntaxnet models for the conll 2017 shared task.\nArXiv, abs/1703.04929.\nJoost Bastings, Ivan Titov, Wilker Aziz, Diego\nMarcheggiani, and Khalil Sima’an. 2017. Graph\nconvolutional encoders for syntax-aware neural ma-\nchine translation. In Proceedings of the 2017 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1957–1967, Copenhagen, Den-\nmark. Association for Computational Linguistics.\nDaniel Beck, Gholamreza Haffari, and Trevor Cohn.\n2018. Graph-to-sequence learning using gated\ngraph neural networks. In Proceedings of the\n56th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers) ,\npages 273–283, Melbourne, Australia. Association\nfor Computational Linguistics.\nDeng Cai and Wai Lam. 2020. Graph transformer for\ngraph-to-sequence learning. In Proceedings of The\nThirty-Fourth AAAI Conference on Artiﬁcial Intelli-\ngence (AAAI).\nBenson Chen, Regina Barzilay, and Tommi Jaakkola.\n2019. Path-augmented graph transformer network.\narXiv preprint arXiv:1905.12712.\nMarco Damonte and Shay B. Cohen. 2019. Structural\nneural encoders for AMR-to-text generation. In Pro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 1 (Long and Short Papers) , pages 3649–3658,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nMichael Denkowski and Alon Lavie. 2014. Meteor uni-\nversal: Language speciﬁc translation evaluation for\nany target language. In Proceedings of the Ninth\nWorkshop on Statistical Machine Translation, pages\n376–380, Baltimore, Maryland, USA. Association\nfor Computational Linguistics.\nRoss Girshick, Jeff Donahue, Trevor Darrell, and Jiten-\ndra Malik. 2014. Rich feature hierarchies for accu-\nrate object detection and semantic segmentation. In\nProceedings of the IEEE conference on computer vi-\nsion and pattern recognition, pages 580–587.\nZhijiang Guo and Wei Lu. 2018. Better transition-\nbased AMR parsing with a reﬁned search space.\nIn Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 1712–1722, Brussels, Belgium. Association\nfor Computational Linguistics.\nZhijiang Guo, Yan Zhang, Zhiyang Teng, and Wei\nLu. 2019. Densely connected graph convolutional\nnetworks for graph-to-sequence learning. Transac-\ntions of the Association for Computational Linguis-\ntics, 7:297–312.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Identity mappings in deep residual net-\nworks. In European conference on computer vision,\npages 630–645. Springer.\nG. Huang, Z. Liu, L. v. d. Maaten, and K. Q. Wein-\nberger. 2017. Densely connected convolutional net-\nworks. In 2017 IEEE Conference on Computer Vi-\nsion and Pattern Recognition (CVPR) , pages 2261–\n2269.\nGuillaume Klein, Yoon Kim, Yuntian Deng, Jean Senel-\nlart, and Alexander M. Rush. 2017. OpenNMT:\nOpen-source toolkit for neural machine translation.\nIn Proc. ACL.\nIoannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin\nChoi, and Luke Zettlemoyer. 2017. Neural AMR:\nSequence-to-sequence models for parsing and gener-\nation. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 146–157, Vancouver,\nCanada. Association for Computational Linguistics.\nWei Lu, Hwee Tou Ng, and Wee Sun Lee. 2009. Nat-\nural language generation with tree conditional ran-\ndom ﬁelds. In Proceedings of the 2009 Conference\non Empirical Methods in Natural Language Pro-\ncessing, pages 400–409, Singapore. Association for\nComputational Linguistics.\nGraham Neubig, Zi-Yi Dou, Junjie Hu, Paul Michel,\nDanish Pruthi, Xinyi Wang, and John Wieting. 2019.\ncompare-mt: A tool for holistic comparison of lan-\nguage generation systems. CoRR, abs/1903.07926.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nMaja Popovi ´c. 2015. chrF: character n-gram f-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\n7154\npages 392–395, Lisbon, Portugal. Association for\nComputational Linguistics.\nMichael Pust, Ulf Hermjakob, Kevin Knight, Daniel\nMarcu, and Jonathan May. 2015. Parsing English\ninto abstract meaning representation using syntax-\nbased machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natu-\nral Language Processing , pages 1143–1154, Lis-\nbon, Portugal. Association for Computational Lin-\nguistics.\nLeonardo F. R. Ribeiro, Claire Gardent, and Iryna\nGurevych. 2019. Enhancing AMR-to-text genera-\ntion with dual graph representations. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 3181–3192, Hong\nKong, China. Association for Computational Lin-\nguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 464–468,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nLinfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo\nWang, and Daniel Gildea. 2017. AMR-to-text gener-\nation with synchronous node replacement grammar.\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n2: Short Papers) , pages 7–13, Vancouver, Canada.\nAssociation for Computational Linguistics.\nLinfeng Song, Yue Zhang, Zhiguo Wang, and Daniel\nGildea. 2018. A graph-to-sequence model for AMR-\nto-text generation. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1616–\n1626, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nPetar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Li `o, and Yoshua Bengio.\n2018. Graph Attention Networks. International\nConference on Learning Representations.\nKeyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro\nSonobe, Ken ichi Kawarabayashi, and Stefanie\nJegelka. 2018a. Representation learning on graphs\nwith jumping knowledge networks. In Proceedings\nof the 35th International Conference on Machine\nLearning.\nKun Xu, Lingfei Wu, Zhiguo Wang, Yansong Feng,\nMichael Witbrock, and Vadim Sheinin. 2018b.\nGraph2seq: Graph to sequence learning with\nattention-based neural networks. arXiv preprint\narXiv:1804.00823.\nJie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min\nZhang, and Guodong Zhou. 2019. Modeling graph\nstructure in transformer for better AMR-to-text gen-\neration. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n5458–5467, Hong Kong, China. Association for\nComputational Linguistics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6810628175735474
    },
    {
      "name": "Theoretical computer science",
      "score": 0.5125998854637146
    },
    {
      "name": "ENCODE",
      "score": 0.5077425241470337
    },
    {
      "name": "Transformer",
      "score": 0.5038544535636902
    },
    {
      "name": "Graph",
      "score": 0.4911348521709442
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3725259602069855
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    }
  ]
}