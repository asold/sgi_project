{
    "title": "FCN-Transformer Feature Fusion for Polyp Segmentation",
    "url": "https://openalex.org/W4287225564",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2183244547",
            "name": "Edward Sanderson",
            "affiliations": [
                "University of Central Lancashire"
            ]
        },
        {
            "id": "https://openalex.org/A1278365713",
            "name": "Bogdan J. Matuszewski",
            "affiliations": [
                "University of Central Lancashire"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3133432362",
        "https://openalex.org/W2008359794",
        "https://openalex.org/W2111147183",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W3200667602",
        "https://openalex.org/W3092344722",
        "https://openalex.org/W2927278850",
        "https://openalex.org/W3041409972",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3104061658",
        "https://openalex.org/W3081752372",
        "https://openalex.org/W2997286550",
        "https://openalex.org/W2999580839",
        "https://openalex.org/W2624917431",
        "https://openalex.org/W3177634011",
        "https://openalex.org/W2728714629",
        "https://openalex.org/W3196075503",
        "https://openalex.org/W4312694728",
        "https://openalex.org/W4214520160",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W4206841660",
        "https://openalex.org/W3162386519",
        "https://openalex.org/W6605946209",
        "https://openalex.org/W3134036841",
        "https://openalex.org/W4226068227",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W3175515048",
        "https://openalex.org/W4250482878",
        "https://openalex.org/W3204166336",
        "https://openalex.org/W2774320778",
        "https://openalex.org/W2884436604",
        "https://openalex.org/W4287324101",
        "https://openalex.org/W3211490618",
        "https://openalex.org/W3124994365",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W4376626248",
        "https://openalex.org/W3127751679",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W4300471784",
        "https://openalex.org/W4312965754",
        "https://openalex.org/W3204995672",
        "https://openalex.org/W4226351492",
        "https://openalex.org/W3142108902",
        "https://openalex.org/W4239072543",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W4287225564",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W4394643672"
    ],
    "abstract": null,
    "full_text": "FCN-Transformer Feature Fusion\nfor Polyp Segmentation\nEdward Sanderson(B) and Bogdan J. Matuszewski\nComputer Vision and Machine Learning (CVML) Group,\nUniversity of Central Lancashire, Preston, UK\n{esanderson4,bmatuszewski1}@uclan.ac.uk\nhttps://www.uclan.ac.uk/research/activity/cvml\nAbstract. Colonoscopy is widely recognised as the gold standard pro-\ncedure for the early detection of colorectal cancer (CRC). Segmentation\nis valuable for two signiﬁcant clinical applications, namely lesion detec-\ntion and classiﬁcation, providing means to improve accuracy and robust-\nness. The manual segmentation of polyps in colonoscopy images is time-\nconsuming. As a result, the use of deep learning (DL) for automation of\npolyp segmentation has become important. However, DL-based solutions\ncan be vulnerable to overﬁtting and the resulting inability to generalise\nto images captured by diﬀerent colonoscopes. Recent transformer-based\narchitectures for semantic segmentation both achieve higher performance\nand generalise better than alternatives, however typically predict a seg-\nmentation map of\nh\n4 × w\n4 spatial dimensions for a h × w input image. To\nthis end, we propose a new architecture for full-size segmentation which\nleverages the strengths of a transformer in extracting the most impor-\ntant features for segmentation in a primary branch, while compensating\nfor its limitations in full-size prediction with a secondary fully convolu-\ntional branch. The resulting features from both branches are then fused\nfor ﬁnal prediction of a h × w segmentation map. We demonstrate our\nmethod’s state-of-the-art performance with respect to the mDice, mIoU,\nmPrecision, and mRecall metrics, on both the Kvasir-SEG and CVC-\nClinicDB dataset benchmarks. Additionally, we train the model on each\nof these datasets and evaluate on the other to demonstrate its superior\ngeneralisation performance.\nCode available: https://github.com/CVML-UCLan/FCBFormer.\nKeywords: Polyp segmentation\n· Medical image processing · Deep\nlearning\n1 Introduction\nColorectal cancer (CRC) is a leading cause of cancer mortality worldwide; e.g.,\nin the United States, it is the third largest cause of cancer deaths, with 52,500\nCRC deaths predicted in 2022 [27]. In Europe, it is the second largest cause of\ncancer deaths, with 156,000 deaths in 27 EU countries reported in 2020 [7].\nc⃝ The Author(s) 2022\nG. Yang et al. (Eds.): MIUA 2022, LNCS 13413, pp. 892–907, 2022.\nhttps://doi.org/10.1007/978-3-031-12053-4\n_65\nFCN-Transformer Feature Fusion for Polyp Segmentation 893\nColon cancer survival rate depends strongly on an early detection. It is com-\nmonly accepted that most colorectal cancers evolve from adenomatous polyps\n[26]. Colonoscopy is the gold standard for colon screening as it can facilitate\ndetection and treatment during the same procedure, e.g., by using the resect-\nand-discard and diagnose-and-disregard approaches. However, colonoscopy has\nsome limitations; e.g., It has been reported that between 17%–28% of colon\npolyps are missed during colonoscopy screening procedures [18,20]. Importantly,\nit has been assessed that improvement of polyp detection rates by 1% reduces\nthe risk of CRC by approximately 3% [4]. It is therefore vital to improve polyp\ndetectability. Equally, correct classiﬁcation of detected polyps is limited by vari-\nability of polyp appearance and subjectivity of the assessment. Lesion detection\nand classiﬁcation are two tasks for which intelligent systems can play key roles\nin improving the eﬀectiveness of the CRC screening and robust segmentation\ntools are important in facilitating these tasks.\nTo improve on the segmentation of polyps in colonoscopy images, a range of\ndeep learning (DL) -based solutions [ 8,13,14,17,19,22,28,30,32,37] have been\nproposed. Such solutions are designed to automatically predict segmentation\nmaps for colonoscopy images, in order to provide assistance to clinicians perform-\ning colonoscopy procedures. These solutions have traditionally used fully con-\nvolutional networks (FCNs) [1,9,10,13–15,17,25,28,39]. However, transformer-\nbased architectures [24,32–34,36] have recently become popular for semantic seg-\nmentation and shown superior performance over FCN-based alternatives. This is\nlikely a result of the ability of transformers to eﬃciently extract features on the\nbasis of a global receptive ﬁeld from the ﬁrst layers of the model through global\nattention. This is especially true in generalisability tests, where a model is trained\non one dataset and evaluated on another dataset in order to test its robustness to\nimages from a somewhat diﬀerent distribution to that considered during training.\nSome studies have also combined FCNs and transformers/attention mechanisms\n[3,8,19,22,30,37] in order to combine their strengths in a single architecture\nfor medical image segmentation, however these hybrid architectures do not out-\nperform the highest performing FCN-based and transformer-based models in\nthis task, notably MSRF-Net [28] (FCN) and SSFormer [32] (transformer). One\nsigniﬁcant limitation of most the highlighted transformer-based architectures is\nhowever that the predicted segmentation maps of these models are typically of a\nlower resolution than the input images, i.e. are not full-size. This is due to these\nmodels operating on tokens which correspond to patches of the input image\nrather than pixels.\nIn this paper, we propose a new architecture for polyp segmentation in\ncolonoscopy images which combines FCNs and transformers to achieve state-\nof-the-art results. The architecture, named the Fully Convolutional Branch-\nTransFormer (FCBFormer) (Fig.1\na), uses two parallel branches which both start\nfrom a h × w input image: a fully convolutional branch (FCB) which returns\nfull-size (h × w) feature maps; and a transformer branch (TB) which returns\nreduced-size (h\n4 × w\n4 ) feature maps. The output tensors of TB are then upsam-\npled to full-size, concatenated with the output tensors of FCB along the channel\n894 E. Sanderson and B. J. Matuszewski\nFig. 1. The architectures of a) FCBFormer, b) the transformer branch (TB), c) the\nfully convolutional branch (FCB), d) the prediction head (PH), e) the improved local\nemphasis (LE) module, f) the residual block (RB).\ndimension, before a prediction head (PH) processes the concatenated tensors into\na full-size segmentation map for the input image. Through the use of the Ima-\ngeNet [5] pre-trained pyramid vision transformer v2 (PVTv2) [ 34] as an image\nencoder, we encourage the model to extract the most important features for\nsegmentation in TB. We then randomly initialise FCB to encourage extraction\nof the features required for processing outputs of TB into full-size segmentation\nmaps. TB largely follows the structure of the recent SSFormer [32] which predicts\nsegmentation maps of\nh\n4 × w\n4 spatial dimensions, and which achieved the current\nstate-of-the-art performance on polyp segmentation at reduced-size. However, we\nupdate the SSFormer architecture with a new progressive locality decoder (PLD)\nwhich features improved local emphasis (LE) and stepwise feature aggregation\n(SFA). FCB then takes the form of an advanced FCN architecture, composed\nof a modern variant of residual blocks (RBs) that include group normalisation\nFCN-Transformer Feature Fusion for Polyp Segmentation 895\n[35] layers, SiLU [12] activation functions, and convolutional layers, with a resid-\nual connection [11,29]; in addition to dense U-Net style skip connections [ 25].\nPH is then composed of RBs and a ﬁnal pixel-wise prediction layer which uses\nconvolution with 1×1 kernels. On this basis, we achieve state-of-the-art perfor-\nmance with respect to the mDice, mIoU, mPrecision, and mRecall metrics on the\nKvasir-SEG [16] and CVC-ClinicDB [ 2] datasets, and on generalisability tests\nwhere we train the model on one Kvasir-SEG and evaluate it on CVC-ClinicDB,\nand vice-versa.\nThe main novel contributions of this work are therefore:\n1. The introduction of a simple yet eﬀective approach for FCNs and transformers\nin a single architecture for dense prediction which, in contrast to previous\nwork on this, demonstrates advantages over these individual model types\nthrough state-of-the-art performance in polyp segmentation.\n2. The improvement of the progressive locality decoder (PLD) introduced with\nSSFormer [32] for decoding features extracted by a transformer encoder\nthrough residual blocks (RBs) composed of group normalisation [ 35], SiLU\nactivation functions [35], convolutional layers, and residual connections [11].\nThe rest of this paper is structured as follows: we ﬁrst deﬁne the design\nof FCBFormer and its components in Sect. 2; we then outline our experiments\nin terms of the implementation of methods, the means of evaluation, and our\nresults, in Sect. 3; and in Sect.4 we give our conclusion.\n2 FCBFormer\n2.1 Transformer Branch (TB)\nThe transformer branch (TB) (Fig.1b) is highly inﬂuenced by the current state-\nof-the-art architecture for reduced-size polyp segmentation, the SSFormer [ 32].\nOur implementation of SSFormer, as used in our experiments, is illustrated in\nFig.2. This architecture uses an ImageNet [5] pre-trained pyramid vision trans-\nformer v2 (PVTv2) [34] as an image encoder, which returns a feature pyramid\nwith 4 levels that is then taken as the input for the progressive locality decoder\n(PLD). In PLD, each level of the pyramid is processed individually by a local\nemphasis (LE) module, in order to address the weaknesses of transformer-based\nmodels in representing local features in the feature representation, before fusing\nthe locally emphasised levels of the feature pyramid through stepwise feature\naggregation (SFA). Finally, the fused multi-scale features are used to predict the\nsegmentation map for the input image.\nPLD takes the tensors returned by the encoder, with a number of channels\ndeﬁned by PVTv2, and changes the number of channels in the ﬁrst convolu-\ntional layer in each LE block to 64. Each subsequent layer, except channel-wise\nconcatenation and the prediction layer, then returns the same number of chan-\nnels (64).\n896 E. Sanderson and B. J. Matuszewski\nThe rest of this subsection will specify the design of TB in the proposed FCB-\nFormer and how this varies from this deﬁnition of SSFormer. The improvements\nresulting from our changes are then demonstrated in the experimental section of\nthis paper.\nFig. 2. The architecture of our implementation of SSFormer.\nTransformer Encoder. As in SSFormer, we used the PVTv2 [ 34] for the\nimage encoder in TB, pre-trained on ImageNet [5]. The variant of PVTv2 used\nis the B3 variant, which has 45.2M parameters. This model demonstrates excep-\ntional feature extraction capabilities for dense prediction owing to its pyramid\nfeature representation, contrasting with more traditional vision transformers\nwhich maintain the size of the spatial dimensions throughout the network, e.g.\n[6,24,31]. Additionally, the model embeds the position of patches through zero\npadding and overlapping patch embedding via strided convolution, as opposed\nto adding explicit position embeddings to tokens, and for eﬃciency uses linear\nspatial reduction attention. On this element we do not deviate from the design\nof SSFormer.\nImproved Progressive Locality Decoder (PLD+). We improve on the\nprogressive locality decoder (PLD) introduced with SSFormer using the archi-\ntecture shown in Fig.1b (PLD+), where we use residual blocks (RBs) (Fig.1f) to\nFCN-Transformer Feature Fusion for Polyp Segmentation 897\novercome identiﬁed limitations of the SSFormer’s LE and SFA. These RBs take\ninspiration from the components of modern convolutional neural networks which\nhave seen boosts in performance due to the incorporation of group normalisation\n[35], SiLU activation functions [12], and residual connections [11]. We identiﬁed\nSSFormer’s LE and SFA as being limited due to a lack of such modern elements,\nand a relatively low number of layers. As such, we modiﬁed these elements in\nFCBFormer to form the components of PLD+. The improvements resulting from\nthese changes are shown through ablation tests in the experimental section of\nthis paper.\nAs in SSFormer, the number of channels returned by the ﬁrst convolutional\nlayer in the LE blocks 64. Every subsequent layer, except channel-wise concate-\nnation, then returns the same number of channels (64).\n2.2 Fully Convolutional Branch (FCB)\nWe deﬁne the fully convolutional branch (FCB) (Fig. 1c) as a composition of\nresidual blocks (RBs), strided convolutional layers for downsampling, nearest\nneighbour interpolation for upsampling, and dense U-Net style skip connections.\nThis design allows for the extraction of highly fused multi-scale features at full-\nsize, which when fused with the important but coarse features extracted by the\ntransformer branch (TB) allows for inference of full-size segmentation maps in\nthe prediction head (PH).\nThrough the encoder of FCB, we increase the number of channels returned\nby each layer by a factor of 2 in the ﬁrst convolutional layer of the ﬁrst RB\nfollowing the second and fourth downsampling layers. Through the decoder of\nFCB, we then decrease the number of channels returned by each layer by a factor\nof 2 in the ﬁrst convolutional layer in the ﬁrst RB after the second and fourth\nupsampling layers.\n2.3 Prediction Head (PH)\nThe prediction head (PH) (Fig.1d) takes a full-size tensor resulted from concate-\nnating the up-sampled transformer branch (TB) output and the output from the\nfully convolutional branch (FCB). The PH predicts the segmentation map from\nimportant but coarse features extracted by TB by fusing them with the ﬁne-\ngrained features extracted by FCB. This approach for the combination of FCNs\nand transformers for dense prediction to the best of our knowledge has not been\nused before. As shown by our experiments, this approach is highly eﬀective in\npolyp segmentation and indicates that FCNs and transformers operating in par-\nallel prior to the fusion of features and pixel-wise prediction on the fused features\nis a powerful basis for dense prediction. Each layer of PH returns 64 channels,\nexcept the prediction layer which returns a single channel.\n898 E. Sanderson and B. J. Matuszewski\n3 Experiments\nTo evaluate the performance of FCBFormer in polyp segmentation, we con-\nsidered 2 popular open datasets, Kvasir-SEG [16]1 and CVC-ClinicDB [2]2,a n d\ntrained our models using the implementation detailed in Sect.3.1. These datasets\nprovide 1000/612 (Kvasir-SEG/CVC-ClinicDB) ground truth input-target pairs\nin total, with the samples in Kvasir-SEG varying in the size of the spatial dimen-\nsions while all samples in CVC-ClinicDB are of 288 × 384 spatial dimensions.\nAll images across both datasets contain polyps of varying morphology. These\ndatasets have been used extensively in the development of polyp segmentation\nmodels, and as such provide strong benchmarks for this assessment.\n3.1 Implementation Details\nWe trained FCBFormer to predict binary segmentation maps of h × w spa-\ntial dimensions for RGB images resized to h × w spatial dimensions, where we\nset h, w = 352 following the convention set by [ 8,32,37]. We used PyTorch,\nand due to the aliasing issues with resizing images in such frameworks which\nhave recently been brought to light [ 23], we used anti-aliasing in our resizing\nof the images. Both the images and segmentation maps were initially loaded\nin with a value range of [0 , 1]. We then used a random train/validation/test\nsplit of 80%/10%/10% following the convention set by [8,15,17,28,32], and ran-\ndomly augmented the training input-target pairs as they were loaded in during\neach epoch using: 1) a Gaussian blur with a 25 × 25 kernel with a standard\ndeviation uniformly sampled from [0 .001, 2]; 2) colour jitter with a brightness\nfactor uniformly sampled from [0 .6, 1.4], a contrast factor uniformly sampled\nfrom [0.5, 1.5], a saturation factor uniformly sampled from [0.75, 1.25], and a hue\nfactor uniformly sampled from [0.99, 1.01]; 3) horizontal and vertical ﬂips each\nwith a probability of 0.5; and 4) aﬃne transforms with rotations of an angle\nsampled uniformly from [− 180\n◦, 180◦], horizontal and vertical translations each\nof a magnitude sampled uniformly from [ − 44, 44], scaling of a magnitude sam-\npled uniformly from [0.5, 1.5] and shearing of an angle sampled uniformly from\n[− 22.5◦, 22◦]. Out of these augmentations, 1) and 2) were applied only to the\nimage, while the rest of the augmentations were applied consistently to both\nthe image and the corresponding segmentation map. Following augmentation,\nthe image RGB values were normalised to an interval of [ − 1, 1]. We note that\nperformance was achieved by resizing the segmentation maps used for train-\ning with bilinear interpolation without binarisation, however the values of the\nsegmentation maps in the validation and test sets were binarised after resizing.\nWe then trained FCBFormer on the training set for each considered polyp\nsegmentation dataset for 200 epochs using a batch size of 16 and the AdamW\noptimiser [21] with an initial learning rate of 1e− 4. The learning rate was then\nreduced by a factor of 2 when the performance (mDice) on the validation set\n1 Available: https://datasets.simula.no/kvasir-seg/.\n2 Available: https://polyp.grand-challenge.org/CVCClinicDB/.\nFCN-Transformer Feature Fusion for Polyp Segmentation 899\ndid not improve over 10 epochs until reaching a minimum of 1e − 6, and saved\nthe model after each epoch if the performance (mDice) on the validation set\nimproved. The loss function used was the sum of the binary cross entropy (BCE)\nloss and the Dice loss.\nFor comparison against alternative architectures, we also trained and eval-\nuated a selection of well-established and state-of-the-art examples, which also\npredict full-size segmentation maps, on the same basis as FCBFormer, includ-\ning: U-Net [ 25], ResUNet [ 38], ResUNet++ [ 17], PraNet [ 8], and MSRF-Net\n[28]. This did not include SSFormer, as an oﬃcial codebase has yet to be made\navailable and the model by itself does not predict full-size segmentation maps.\nHowever, we considered our own implementation of SSFormer in an ablation\nstudy presented at the end of this section. To ensure these models were trained\nand evaluated in a consistent manner while ensuring training and inference was\nconducted as the authors intended, we used the oﬃcial codebase\n3 provided for\neach, where possible 4 and modiﬁed this only to ensure that the models were\ntrained and evaluated using data of 352 × 352 spatial dimensions and that the\nsame train/validation/test splits were used.\nSome of the codebases for the existing models implement the respective model\nin TensorFlow/Keras, as opposed to PyTorch as is the case for FCBFormer. After\nobserving slight variation in the results returned by the implementations of the\nconsidered metrics in these frameworks for the same inputs, we took steps to\nensure a fair and balanced assessment. We therefore predicted the segmentation\nmaps for each assessment within each respective codebase, after training, and\nsaved the predictions. In a separate session using only Scikit-image, we then\nloaded in the targets for each assessment from source, resized to 352× 352 using\nbilinear interpolation, and binarised the result. The binary predictions were then\nloaded in, and we used the implementations of the metrics in Scikit-learn to\nobtain our results. Note that this was done for all models in each assessment.\n3.2 Evaluation\nWe present some example predictions for each model in Fig.3. From this, it can\nbe seen how FCBFormer predicts segmentation maps which are generally more\nconsistent with the target than the segmentation maps computed by the exist-\ning models, and which demonstrate robustness to challenging morphology, high-\nlighted by cases where the existing models are unable to represent the boundary\nwell. This particular strength in segmenting polyps for which the boundary is\nless apparent is likely a result of the successful combination of the strengths of\ntransformers and FCNs in FCBFormer, leading to the main structures of polyps\nbeing dealt with by the transformer branch (TB), while the fully convolutional\n3 ResUNet++ code available: https://github.com/DebeshJha/ResUNetPlusPlus.\nPraNet code available: https://github.com/DengPingFan/PraNet.\nMSRF-Net code available: https://github.com/NoviceMAn-prog/MSRF-Net.\n4 For U-Net and ResUNet, we used the implementations built into the ResUnet++\ncodebase (available: https://github.com/DebeshJha/ResUNetPlusPlus).\n900 E. Sanderson and B. J. Matuszewski\nbranch (FCB) serves to ensure a reliable full-size boundary around this main\nstructure. We demonstrate this in Fig.4, where we show the features extracted\nby TB and FCB, and the predictions, for examples from the Kvasir-SEG [ 16]\ntest set. The predictions are shown for the model with FCB, as deﬁned, as well\nas for the model without FCB, where we concatenate the output of TB channel-\nwise with a tensor of 0’s in place of the output of FCB. This reveals how the\nprediction head (PH) performs with and without the information provided by\nFCB, and in turn the role of FCB in assisting with the prediction. The most\napparent function is that FCB highlights the edges of polyps, as well as the\nedges of features that may cause occlusions of polyps, such as other objects in\nthe scene or the perimeter of the colonoscope view. This can then be seen to\nhelp provide a well-deﬁned boundary, particularly when a polyp is near or partly\noccluded by such features.\nFig. 3. Example inputs and targets from the Kvasir-SEG test set [16] and the predic-\ntions for FCBFormer and the considered existing architectures. FF is FCBFormer, PN\nis PraNet, MN is MSRF-Net, R++ is ResUNet++, RU is ResUNet, and UN is U-Net.\nEach model used for this was the variant trained on the Kvasir-SEG training set.\nPrimary Evaluation. For each dataset, we evaluated the performance of the\nmodels with respect to the mDice, mIoU, mPrecision, and mRecall metrics,\nwhere m indicates an average of the metric value over the test set. The results\nfrom these primary assessments are shown in Table 1, which show that FCB-\nFormer outperformed the existing models with respect to all metrics.\nWe note that for some of the previously proposed methods, we obtain worse\nresults than has been reported in the original papers, particularly MSRF-Net\n[28]. This is potentially due to some of the implementations being optimised for\nspatial dimensions of size 256 × 256, as opposed to 352 × 352 as has been used\nhere. This is supported by our retraining and evaluation of MSRF-Net [28] with\n256×256 input-targets, where we obtained similar results to those reported in the\noriginal paper. We therefore present the results originally reported by the authors\nof each model in Table 2. Despite the potential diﬀerences in the experimental\nFCN-Transformer Feature Fusion for Polyp Segmentation 901\nFig. 4. Visualisation of the features returned by TB and FCB (channel-wise average),\nand the with/without FCB predictions for examples from the Kvasir-SEG [16] test set.\nset up, it can be seen that FCBFormer consistently outperforms other models\nwith respect to the observed mDice, one of the most important metrics out of\nthose considered, and also outperforms other models with respect to mRecall\non the Kvasir-SEG dataset [16], and mPrecision on the CVC-ClinicDB dataset\n[2]. FCBFormer can also be seen to perform competitively with respect to the\nmIoU.\nTable 1. Results from our primary assessment.\nDataset Kvasir-SEG [16] CVC-ClinicDB [2]\nMetric mDice mIoU mPrec. mRec. mDice mIoU mPrec. mRec.\nU-Net [25] 0.7821 0.8141 0.7241 0.8450 0.8464 0.7730 0.8496 0.8796\nResUNet [38] 0.5133 0.3792 0.5937 0.5968 0.5221 0.4120 0.6151 0.5895\nResUNet++ [17] 0.8074 0.7231 0.8991 0.7874 0.5211 0.4126 0.5633 0.5693\nMSRF-Net [28] 0.8586 0.7906 0.8933 0.8774 0.9198 0.8729 0.9222 0.9308\nPraNet [8] 0.9011 0.8403 0.9034 0.9272 0.9358 0.8867 0.9370 0.93888\nFCBFormer (ours) 0.9385 0.8903 0.9459 0.9401 0.9469 0.9020 0.9525 0.9441\nGeneralisability Tests. We also performed generalisability tests following the\nconvention set by [28,32]. Using the same set of metrics, we evaluated the models\ntrained on the Kvasir-SEG/CVC-ClinicDB training set on predictions for the full\nCVC-ClinicDB/Kvasir-SEG dataset. Such tests reveal how models perform with\nrespect to a diﬀerent distribution to that considered during training.\n902 E. Sanderson and B. J. Matuszewski\nTable 2. Results originally reported for existing models. Note that U-Net and ResUNet\nwere not originally tested on polyp segmentation, and as such we present the results\nobtained by the authors of ResUNet++ [17] for these models. For ease of comparison,\nwe include the results we obtained for FCBFormer in our primary assessment.\nDataset Kvasir-SEG [16] CVC-ClinicDB [2]\nMetric mDice mIoU mPrec. mRec. mDice mIoU mPrec. mRec.\nU-Net [25] 0.7147 0.4334 0.9222 0.6306 0.6419 0.4711 0.6868 0.6756\nResUNet [38] 0.5144 0.4364 0.7292 0.5041 0.4510 0.4570 0.5614 0.5775\nResUNet++ [17] 0.8133 0.7927 0.7064 0.8774 0.7955 0.7962 0.8785 0.7022\nMSRF-Net [28] 0.9217 0.8914 0.9666 0.9198 0.9420 0.9043 0.9427 0.9567\nPraNet [8] 0.898 0.840 − − 0.899 0.849 − −\nFCBFormer (ours) 0.9385 0.8903 0.9459 0.9401 0.9469 0.9020 0.9525 0.9441\nThe results for the generalisability tests are given in Table 3, where it can\nbe seen that FCBFormer exhibits particular strength in dealing with images\nfrom a somewhat diﬀerent distribution to those used for training, signiﬁcantly\noutperforming the existing models with respect to most metrics. This is likely a\nresult of the same strengths highlighted in the discussion of Fig.3.\nTable 3. Results from our generalisability tests.\nTraining data Kvasir-SEG [16] CVC-ClinicDB [2]\nTest data CVC-ClinicDB [2] Kvasir-SEG [16]\nMetric mDice mIoU mPrec. mRec. mDice mIoU mPrec. mRec.\nU-Net [25] 0.5940 0.5081 0.6937 0.6184 0.5292 0.4036 0.4613 0.8481\nResUNet [38] 0.3359 0.2425 0.5048 0.3307 0.3344 0.2222 0.2618 0.8164\nResUNet++ [17] 0.5638 0.4750 0.7175 0.5908 0.3077 0.2048 0.3340 0.4778\nMSRF-Net [28] 0.6238 0.5419 0.6621 0.7051 0.7296 0.6415 0.8162 0.7421\nPraNet [8] 0.7912 0.7119 0.8152 0.8316 0.7950 0.7073 0.7687 0.9050\nFCBFormer (ours) 0.8735 0.8038 0.8995 0.8876 0.8848 0.8214 0.9354 0.8754\nAs in our primary assessment, we also present results reported elsewhere.\nSimilar generalisability tests were undertaken by the authors of MSRF-Net [28],\nleading to the results presented in Table 4. Again, we observe that FCBFormer\noutperforms other models with respect to most metrics.\nAblation Study. We also performed an ablation study, where we started from\nour implementation of SSFormer given in Fig. 2, since an oﬃcial codebase has\nyet to be made available, and stepped towards FCBFormer. We refer to our\nimplementation of SSFormer as SSFormer-I. This model was trained to predict\nsegmentation maps ofh\n4 × w\n4 spatial dimensions, and its performance in predicting\nFCN-Transformer Feature Fusion for Polyp Segmentation 903\nTable 4. Results from the generalisability tests conducted by the authors of MSRF-\nNet [28]. Note, ResUNet [38] was not included in these tests. For ease of comparison,\nwe include the results we obtained for FCBFormer in our generalisability tests.\nTraining data Kvasir-SEG [16] CVC-ClinicDB [2]\nTest data CVC-ClinicDB [2] Kvasir-SEG [16]\nMetric mDice mIoU mPrec. mRec. mDice mIoU mPrec. mRec.\nU-Net [25] 0.7172 0.6133 0.7986 0.7255 0.6222 0.4588 0.8133 0.5129\nResUNet++ [17] 0.5560 0.4542 0.6775 0.5795 0.5147 0.4082 0.7181 0.4860\nMSRF-Net [28] 0.7921 0.6498 0.7000 0.9001 0.7575 0.6337 0.8314 0.7197\nPraNet [8] 0.7225 0.6328 0.7888 0.7531 0.7293 0.6262 0.7623 0.8007\nFCBFormer (ours) 0.8735 0.8038 0.8995 0.8876 0.8848 0.8214 0.9354 0.8754\nfull-size segmentation maps was then assessed by upsampling the predictions to\nh×w using bilinear interpolation then binarisation. We then removed the original\nprediction layer and used the resulting architecture as the transformer branch\n(TB) in FCBFormer, to reveal the beneﬁts of our fully convolutional branch\n(FCB) and prediction head (PH) for full-size segmentation in isolation of the\nimproved progressive locality decoder (PLD+), and we refer to this model as\nSSFormer-I+FCB. The additional performance of FCBFormer over SSFormer-\nI+FCB then reveals the beneﬁts of PLD+. Note that SSFormer-I and SSFormer-\nI+FCB were both trained and evaluated on the same basis as FCBFormer and\nthe other considered existing state-of-the-art architectures.\nThe results from this ablation study are given in Tables5 and 6, which indi-\ncate that: 1) there are signiﬁcant beneﬁts of FCB, as demonstrated by SSFormer-\nI+FCB outperforming SSFormer-I with respect to most metrics; and 2) there\nare generally beneﬁts of PLD+, demonstrated by FCBFormer outperforming\nSSFormer-I+FCB on both experiments in the primary assessment and 1 out of\n2 of the generalisability tests, with respect to most metrics.\nTable 5. Results from the primary assessment in the ablation study. For ease of com-\nparison, we include the results we obtained for FCBFormer in our primary assessment.\nDataset Kvasir-SEG [16] CVC-ClinicDB [2]\nMetric mDice mIoU mPrec. mRec. mDice mIoU mPrec. mRec.\nSSFormer-I 0.9196 0.8616 0.9316 0.9226 0.9318 0.8777 0.9409 0.9295\nSSFormer-I+FCB 0.9337 0.8850 0.9330 0.9482 0.9410 0.8904 0.9556 0.9307\nFCBFormer 0.9385 0.8903 0.9459 0.9401 0.9469 0.9020 0.9525 0.9441\n904 E. Sanderson and B. J. Matuszewski\nTable 6. Results from the generalisability test in the ablation study. For ease of com-\nparison, we include the results we obtained for FCBFormer in our generalisability tests.\nTraining data Kvasir-SEG [16] CVC-ClinicDB [2]\nTest data CVC-ClinicDB [2] Kvasir-SEG [16]\nMetric mDice mIoU mPrec. mRec. mDice mIoU mPrec. mRec.\nSSFormer-I 0.8611 0.7813 0.8904 0.8702 0.8691 0.7986 0.9178 0.8631\nSSFormer-I+FCB 0.8754 0.8059 0.8935 0.8963 0.8704 0.7993 0.9280 0.8557\nFCBFormer 0.8735 0.8038 0.8995 0.8876 0.8848 0.8214 0.9354 0.8755\n4 Conclusion\nIn this paper, we introduced the FCBFormer, a novel architecture for the segmen-\ntation of polyps in colonoscopy images which successfully combines the strengths\nof transformers and fully convolutional networks (FCNs) in dense prediction.\nThrough our experiments, we demonstrated the models state-of-the-art perfor-\nmance in this task and how it outperforms existing models with respect to several\npopular metrics, and highlighted its particular strengths in generalisability and\nin dealing with polyps of challenging morphology. This work therefore repre-\nsents another advancement in the automated processing of colonoscopy images,\nwhich should aid in the necessary improvement of lesion detection rates and\nclassiﬁcation.\nAdditionally, this work has interesting implications for the understanding\nof neural network architectures for dense prediction. The method combines the\nstrengths of transformers and FCNs, by running a model of each type in par-\nallel and concatenating the outputs for processing by a prediction head (PH).\nTo the best of our knowledge, this method has not been used before, and its\nstrengths indicate that there is still a great deal to understand about these\ndiﬀerent architecture types and the basis on which they can be combined for\noptimal performance. Further work should therefore explore this in more depth,\nby evaluating variants of the model and performing further ablation studies. We\nwill also consider further investigation of dataset augmentation for this task,\nwhere we expect the random augmentation of segmentation masks to aid in\novercoming variability in the targets produced by diﬀerent annotators.\nAcknowledgements. This work was supported by the Science and Technology Facil-\nities Council grant number ST/S005404/1.\nDiscretionary time allocation on DiRAC Tursa HPC was also used for methods\ndevelopment.\nFCN-Transformer Feature Fusion for Polyp Segmentation 905\nReferences\n1. Ali, S., et al.: Deep learning for detection and segmentation of artefact and disease\ninstances in gastrointestinal endoscopy. Med. Image Anal. 70, 102002 (2021)\n2. Bernal, J., S´ anchez, F.J., Fern´andez-Esparrach, G., Gil, D., Rodr´ ıguez, C.,\nVilari˜no, F.: WM-DOVA maps for accurate polyp highlighting in colonoscopy:\nValidation vs. saliency maps from physicians. Comput. Med. Imaging Graph. 43,\n99–111 (2015)\n3. Chen, J., et al.: TransuNet: transformers make strong encoders for medical image\nsegmentation. arXiv preprint arXiv:2102.04306 (2021)\n4. Corley, D.A., et al.: Adenoma detection rate and risk of colorectal cancer and\ndeath. N. Engl. J. Med. 370(14), 1298–1306 (2014)\n5. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: a large-scale\nhierarchical image database. In: 2009 IEEE Conference on Computer Vision and\nPattern Recognition, pp. 248–255. IEEE (2009)\n6. Dosovitskiy, A., et al.: An image is worth 16 ×16 words: transformers for image\nrecognition at scale. In: ICLR (2021)\n7. Dyba, T., et al.: The European cancer burden in 2020: incidence and mortality\nestimates for 40 countries and 25 major cancers. Eur. J. Cancer 157, 308–347\n(2021)\n8. Fan, D.-P., et al.: PraNet: parallel reverse attention network for polyp segmenta-\ntion. In: Martel, A.L., et al. (eds.) MICCAI 2020. LNCS, vol. 12266, pp. 263–273.\nSpringer, Cham (2020). https://doi.org/10.1007/978-3-030-59725-2\n26\n9. Guo, Y.B., Matuszewski, B.: Giana polyp segmentation with fully convolutional\ndilation neural networks. In: Proceedings of the 14th International Joint Conference\non Computer Vision, Imaging and Computer Graphics Theory and Applications,\npp. 632–641. SCITEPRESS-Science and Technology Publications (2019)\n10. Guo, Y., Bernal, J., J Matuszewski, B.: Polyp segmentation with fully convolutional\ndeep neural networks-extended evaluation study. J. Imaging6(7), 69 (2020)\n11. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\npp. 770–778 (2016). https://doi.org/10.1109/CVPR.2016.90\n12. Hendrycks, D., Gimpel, K.: Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415 (2016)\n13. Huang, C.H., Wu, H.Y., Lin, Y.L.: HardNet-MSEG: a simple encoder-decoder\npolyp segmentation neural network that achieves over 0.9 mean dice and 86 fps.\narXiv preprint arXiv:2101.07172 (2021)\n14. Jha, D., et al.: Real-time polyp detection, localization and segmentation in\ncolonoscopy using deep learning. IEEE Access 9, 40496–40510 (2021)\n15. Jha, D., Riegler, M.A., Johansen, D., Halvorsen, P., Johansen, H.D.: Doubleu-net: a\ndeep convolutional neural network for medical image segmentation. In: 2020 IEEE\n33rd International Symposium on Computer-Based Medical Systems (CBMS), pp.\n558–564. IEEE (2020)\n16. Jha, D., et al.: Kvasir-SEG: a segmented polyp dataset. In: Ro, Y.M., et al. (eds.)\nMMM 2020. LNCS, vol. 11962, pp. 451–462. Springer, Cham (2020). https://doi.\norg/10.1007/978-3-030-37734-2\n37\n17. Jha, D., et al.: Resunet++: an advanced architecture for medical image segmenta-\ntion. In: 2019 IEEE International Symposium on Multimedia (ISM), pp. 225–2255.\nIEEE (2019)\n906 E. Sanderson and B. J. Matuszewski\n18. Kim, N.H., et al.: Miss rate of colorectal neoplastic polyps and risk factors for\nmissed polyps in consecutive colonoscopies. Intestinal Res. 15(3), 411 (2017)\n19. Kim, T., Lee, H., Kim, D.: UacaNet: Uncertainty augmented context attention for\npolyp segmentation. In: Proceedings of the 29th ACM International Conference on\nMultimedia, pp. 2167–2175 (2021)\n20. Lee, J., et al.: Risk factors of missed colorectal lesions after colonoscopy. Medicine\n96(27) (2017)\n21. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: International\nConference on Learning Representations (2018)\n22. Lou, A., Guan, S., Ko, H., Loew, M.H.: CaraNet: context axial reverse atten-\ntion network for segmentation of small medical objects. In: Medical Imaging 2022:\nImage Processing, vol. 12032, pp. 81–92. SPIE (2022)\n23. Parmar, G., Zhang, R., Zhu, J.Y.: On aliased resizing and surprising subtleties in\nGAN evaluation. In: CVPR (2022)\n24. Ranftl, R., Bochkovskiy, A., Koltun, V.: Vision transformers for dense prediction.\nIn: Proceedings of the IEEE/CVF International Conference on Computer Vision,\npp. 12179–12188 (2021)\n25. Ronneberger, O., Fischer, P., Brox, T.: U-net: convolutional networks for biomed-\nical image segmentation. In: Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F.\n(eds.) MICCAI 2015. LNCS, vol. 9351, pp. 234–241. Springer, Cham (2015).\nhttps://doi.org/10.1007/978-3-319-24574-4\n28\n26. Salmo, E., Haboubi, N.: Adenoma and malignant colorectal polyp: pathological\nconsiderations and clinical applications. Gastroenterology 7(1), 92–102 (2018)\n27. Siegel, R.L., Miller, K.D., Fuchs, H.E., Jemal, A.: Cancer statistics, 2022. CA\nCancer J. Clin. (2022)\n28. Srivastava, A., et al.: MSRF-net: a multi-scale residual fusion network for biomed-\nical image segmentation. IEEE J. Biomed. Health Inform. (2021)\n29. Srivastava, R.K., Greﬀ, K., Schmidhuber, J.: Highway networks. arXiv preprint\narXiv:1505.00387 (2015)\n30. Tomar, N.K., et al.: DDANet: dual decoder attention network for automatic polyp\nsegmentation. In: Del Bimbo, A., et al. (eds.) ICPR 2021. LNCS, vol. 12668, pp.\n307–314. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-68793-9 23\n31. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jegou, H.: Training\ndata-eﬃcient image transformers & distillation through attention. In: Meila, M.,\nZhang, T. (eds.) Proceedings of the 38th International Conference on Machine\nLearning. Proceedings of Machine Learning Research, vol. 139, pp. 10347–10357.\nPMLR, 18–24 July 2021. https://proceedings.mlr.press/v139/touvron21a.html\n32. Wang, J., Huang, Q., Tang, F., Meng, J., Su, J., Song, S.: Stepwise feature fusion:\nlocal guides global. arXiv preprint arXiv:2203.03635 (2022)\n33. Wang, W., et al.: Pyramid vision transformer: a versatile backbone for dense pre-\ndiction without convolutions. In: Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pp. 568–578 (2021)\n34. Wang, W., et al.: Pvtv 2: improved baselines with pyramid vision transformer.\nComput. Vis. Media 8(3), 1–10 (2022)\n35. Wu, Y., He, K.: Group normalization. In: Proceedings of the European Conference\non Computer Vision (ECCV), pp. 3–19 (2018)\n36. Xie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J.M., Luo, P.: SEG-\nFormer: simple and eﬃcient design for semantic segmentation with transformers.\nIn: Advances in Neural Information Processing Systems, vol. 34 (2021)\nFCN-Transformer Feature Fusion for Polyp Segmentation 907\n37. Zhang, Y., Liu, H., Hu, Q.: TransFuse: fusing transformers and CNNs for med-\nical image segmentation. In: de Bruijne, M., et al. (eds.) MICCAI 2021. LNCS,\nvol. 12901, pp. 14–24. Springer, Cham (2021).https://doi.org/10.1007/978-3-030-\n87193-2 2\n38. Zhang, Z., Liu, Q., Wang, Y.: Road extraction by deep residual U-net. IEEE Geosci.\nRemote Sens. Lett. 15(5), 749–753 (2018)\n39. Zhou, Z., Rahman Siddiquee, M.M., Tajbakhsh, N., Liang, J.: UNet++: a nested\nU-net architecture for medical image segmentation. In: Stoyanov, D., et al. (eds.)\nDLMIA/ML-CDS -2018. LNCS, vol. 11045, pp. 3–11. Springer, Cham (2018).\nhttps://doi.org/10.1007/978-3-030-00889-5 1\nOpen Access This chapter is licensed under the terms of the Creative Commons\nAttribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium\nor format, as long as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons license and indicate if changes were\nmade.\nThe images or other third party material in this chapter are included in the\nchapter’s Creative Commons license, unless indicated otherwise in a credit line to the\nmaterial. If material is not included in the chapter’s Creative Commons license and\nyour intended use is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright holder."
}