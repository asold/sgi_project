{
    "title": "Multimodal transformer augmented fusion for speech emotion recognition",
    "url": "https://openalex.org/W4377292302",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2105874898",
            "name": "Wang Yuanyuan",
            "affiliations": [
                "Xidian University"
            ]
        },
        {
            "id": "https://openalex.org/A2117761656",
            "name": "Yu Gu",
            "affiliations": [
                "Xidian University"
            ]
        },
        {
            "id": "https://openalex.org/A2100573994",
            "name": "Yifei Yin",
            "affiliations": [
                "Guangzhou Electronic Technology (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2142952821",
            "name": "Yingping Han",
            "affiliations": [
                "Xidian University"
            ]
        },
        {
            "id": "https://openalex.org/A2097096136",
            "name": "He Zhang",
            "affiliations": [
                "Northwest University"
            ]
        },
        {
            "id": "https://openalex.org/A2099309572",
            "name": "Shuang Wang",
            "affiliations": [
                "Xidian University"
            ]
        },
        {
            "id": "https://openalex.org/A2141670869",
            "name": "Chen-Yu Li",
            "affiliations": [
                "Xidian University"
            ]
        },
        {
            "id": "https://openalex.org/A2348045240",
            "name": "Dou Quan",
            "affiliations": [
                "Xidian University"
            ]
        },
        {
            "id": "https://openalex.org/A2105874898",
            "name": "Wang Yuanyuan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2117761656",
            "name": "Yu Gu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2100573994",
            "name": "Yifei Yin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2142952821",
            "name": "Yingping Han",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2097096136",
            "name": "He Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2099309572",
            "name": "Shuang Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2141670869",
            "name": "Chen-Yu Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2348045240",
            "name": "Dou Quan",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4221089191",
        "https://openalex.org/W2074788634",
        "https://openalex.org/W2146334809",
        "https://openalex.org/W6784497057",
        "https://openalex.org/W2523856713",
        "https://openalex.org/W6792098614",
        "https://openalex.org/W2963464104",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W3095251683",
        "https://openalex.org/W6801187997",
        "https://openalex.org/W6842630911",
        "https://openalex.org/W3174977508",
        "https://openalex.org/W3120680448",
        "https://openalex.org/W6784488644",
        "https://openalex.org/W6711763079",
        "https://openalex.org/W2191779130",
        "https://openalex.org/W6769949750",
        "https://openalex.org/W2109743529",
        "https://openalex.org/W2584561145",
        "https://openalex.org/W6741948945",
        "https://openalex.org/W6745947887",
        "https://openalex.org/W1133916940",
        "https://openalex.org/W6755541679",
        "https://openalex.org/W6761808342",
        "https://openalex.org/W2803098682",
        "https://openalex.org/W6767727736",
        "https://openalex.org/W7075385858",
        "https://openalex.org/W6784542593",
        "https://openalex.org/W2154091569",
        "https://openalex.org/W3160183718",
        "https://openalex.org/W4206192903",
        "https://openalex.org/W1520861770",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3197013730",
        "https://openalex.org/W6800561919",
        "https://openalex.org/W3139270985",
        "https://openalex.org/W4294225871",
        "https://openalex.org/W6805178650",
        "https://openalex.org/W3096723250",
        "https://openalex.org/W6767636311",
        "https://openalex.org/W3015964358",
        "https://openalex.org/W6770208711",
        "https://openalex.org/W3096164988",
        "https://openalex.org/W6741668545",
        "https://openalex.org/W2788776247",
        "https://openalex.org/W4200250510",
        "https://openalex.org/W2939224318",
        "https://openalex.org/W3015906487",
        "https://openalex.org/W2599674900",
        "https://openalex.org/W2972463723",
        "https://openalex.org/W2750779823",
        "https://openalex.org/W4200420051",
        "https://openalex.org/W1502957213",
        "https://openalex.org/W3107745088",
        "https://openalex.org/W1490960179",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2397083728",
        "https://openalex.org/W4238297360",
        "https://openalex.org/W4386441189",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3168391820"
    ],
    "abstract": "Speech emotion recognition is challenging due to the subjectivity and ambiguity of emotion. In recent years, multimodal methods for speech emotion recognition have achieved promising results. However, due to the heterogeneity of data from different modalities, effectively integrating different modal information remains a difficulty and breakthrough point of the research. Moreover, in view of the limitations of feature-level fusion and decision-level fusion methods, capturing fine-grained modal interactions has often been neglected in previous studies. We propose a method named multimodal transformer augmented fusion that uses a hybrid fusion strategy, combing feature-level fusion and model-level fusion methods, to perform fine-grained information interaction within and between modalities. A Model-fusion module composed of three Cross-Transformer Encoders is proposed to generate multimodal emotional representation for modal guidance and information fusion. Specifically, the multimodal features obtained by feature-level fusion and text features are used to enhance speech features. Our proposed method outperforms existing state-of-the-art approaches on the IEMOCAP and MELD dataset.",
    "full_text": "TYPE Original Research\nPUBLISHED /two.tnum/two.tnum May /two.tnum/zero.tnum/two.tnum/three.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/eight.tnum/one.tnum/five.tnum/nine.tnum/eight.tnum\nOPEN ACCESS\nEDITED BY\nChristian Wallraven,\nKorea University, Republic of Korea\nREVIEWED BY\nVittorio Cuculo,\nUniversity of Modena and Reggio Emilia, Italy\nErwei Yin,\nTianjin Artiﬁcial Intelligence Innovation Center\n(TAIIC), China\n*CORRESPONDENCE\nYu Gu\nguyu@xidian.edu.cn\nRECEIVED /zero.tnum/seven.tnum March /two.tnum/zero.tnum/two.tnum/three.tnum\nACCEPTED /two.tnum/eight.tnum April /two.tnum/zero.tnum/two.tnum/three.tnum\nPUBLISHED /two.tnum/two.tnum May /two.tnum/zero.tnum/two.tnum/three.tnum\nCITATION\nWang Y, Gu Y, Yin Y, Han Y, Zhang H, Wang S,\nLi C and Quan D (/two.tnum/zero.tnum/two.tnum/three.tnum) Multimodal transformer\naugmented fusion for speech emotion\nrecognition. Front. Neurorobot./one.tnum/seven.tnum:/one.tnum/one.tnum/eight.tnum/one.tnum/five.tnum/nine.tnum/eight.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/eight.tnum/one.tnum/five.tnum/nine.tnum/eight.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/three.tnum Wang, Gu, Yin, Han, Zhang, Wang, Li\nand Quan. This is an open-access article\ndistributed under the terms of the\nCreative\nCommons Attribution License (CC BY) . The use,\ndistribution or reproduction in other forums is\npermitted, provided the original author(s) and\nthe copyright owner(s) are credited and that\nthe original publication in this journal is cited, in\naccordance with accepted academic practice.\nNo use, distribution or reproduction is\npermitted which does not comply with these\nterms.\nMultimodal transformer\naugmented fusion for speech\nemotion recognition\nYuanyuan Wang/one.tnum, Yu Gu /one.tnum*, Yifei Yin /two.tnum, Yingping Han /one.tnum, He Zhang /three.tnum,\nShuang Wang/one.tnum, Chenyu Li /one.tnumand Dou Quan /one.tnum\n/one.tnumSchool of Artiﬁcial Intelligence, Xidian University, Xi’an, Chi na, /two.tnumGuangzhou Huya Technology Co., Ltd.,\nGuangzhou, China, /three.tnumSchool of Journalism and Communication, Northwest University, Xi’an , China\nSpeech emotion recognition is challenging due to the subjectivit y and ambiguity\nof emotion. In recent years, multimodal methods for speech emot ion recognition\nhave achieved promising results. However, due to the heteroge neity of data from\ndiﬀerent modalities, eﬀectively integrating diﬀerent modal information remains\na diﬃculty and breakthrough point of the research. Moreover, in view of the\nlimitations of feature-level fusion and decision-level fus ion methods, capturing\nﬁne-grained modal interactions has often been neglected in pr evious studies. We\npropose a method named multimodal transformer augmented fus ion that uses\na hybrid fusion strategy, combing feature-level fusion and mo del-level fusion\nmethods, to perform ﬁne-grained information interaction with in and between\nmodalities. A Model-fusion module composed of three Cross-T ransformer\nEncoders is proposed to generate multimodal emotional repres entation for modal\nguidance and information fusion. Speciﬁcally, the multimodal f eatures obtained\nby feature-level fusion and text features are used to enhance speech features.\nOur proposed method outperforms existing state-of-the-art approaches on the\nIEMOCAP and MELD dataset.\nKEYWORDS\nspeech emotion recognition, multimodal enhancement, hybridfusion, modal interaction,\ntransformer\n/one.tnum. Introduction\nSpeech emotion recognition (SER) is a branch of aﬀective computing that aims to\ndetermine a person’s emotional state from their speech ( Ayadi et al., 2011 ). With the\nincreasing demand for human-computer interaction and emotional interaction, research\ninto SER has practical signiﬁcance and broad application prospects. A SER system can be\nused in a vehicle system to determine the driver’s psychological state and ensure the safe\noperation of the vehicle in real-time (\nWani et al., 2021 ). In hospitals and other medical\nfacilities, SER system can help doctors analyze the patient’s emotions so as to enhance\ncommunication between doctors and patients and help doctors carry out disease diagnosis\n(\nTao and Tan, 2005 ). SER has also been applied in customer service centers, such as train\nstations to detect the emotional state of customers in real time, which can help customer\nservice personnel provide more eﬃcient and higher-quality services (\nSchuller, 2018 ). SER\nis also used to help children with autistic who may encounter diﬃculties in identifying and\nexpressing emotions, thereby improving their socioemotional communication skills (\nMarchi\net al., 2012). However, due to the complexity, subjectivity and ambiguity of emotions, it is still\na challenge to accurately recognize emotions from speech.\nFrontiers in Neurorobotics /zero.tnum/one.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/eight.tnum/one.tnum/five.tnum/nine.tnum/eight.tnum\nIn recent years, the introduction of multimodal methods\nfor SER has attracted the attention of researchers ( Sebe et al.,\n2005). Emotion is a form of multi-channel expression and people\ngenerally use multimodal information such as speech, text, and\nfacial expressions to express emotions (\nShimojo and Shams, 2001 ).\nMoreover, when noise occurs in one modality, the complementary\ninformation of diﬀerent modalities can increase the robustness of\nthe system. SER based on multimodal fusion information is can\ntherefore be expected to outperform SER based on speech only\n(\nWang et al., 2022 ).\nThis paper focuses on using the fusion of speech and text\nmodalities to improve the performance of SER. First, the text\nmodality provides the semantic content. The semantic information\nis rich and direct, but it is easily aﬀected by the speech recognition\ntask so as to contain ambiguity and bias (\nWu J. et al., 2021 ). The\nspeech modality provides information about the tone, speed, and\nvolume of the speech delivery. Its advantage is that it can help one\nperceive the speaker ’s emotions, but it is diﬃcult to obtain semantic\ninformation directly from speech. Second, text can be transcribed\nfrom speech, and the text features are part of the speech features.\nText and speech can complement each other well (\nAtmaja et al.,\n2022). In the event of ambiguity and bias in the text, the emotional\ninformation based on the speaker in the speech can be used as a\nreference. If it is diﬃcult to obtain semantic information from the\nspeech, the text can provide supplementary information.\nIn view of the limitations of feature-level fusion and decision-\nlevel fusion methods, capturing comprehensive and ﬁne-grained\nmodal interaction has often been neglected in previous studies.\nCompared with decision-level and feature-level fusion methods,\nmodel-level fusion can better use the advantages of deep neural\nnetworks, better integrate the features of diﬀerent modalities, and\nobtain more accurate emotional representations. In hybrid fusion,\nthe advantages of the diﬀerent fusion strategies can be combined\nto capture more ﬁne-grained information on intra-modal and\ninter-modal interaction.\nFurthermore, inspired by the attention mechanism, researchers\nhave proposed the transformer (\nVaswani et al., 2017 ), which\nhas achieved promising results in the ﬁeld of natural language\nprocessing (NLP). Transformer has an excellent ability in modeling\nlong-term dependencies in sequences. Although the original\nTransformer was proposed to solve machine translation problems\nin the ﬁeld of NLP , researchers are studying its adaptability to\nthe ﬁeld of speech signal processing. The multi-head self-attention\nmechanism can learn long-term time dependence; the multi-\nhead cross-attention mechanism can realize the fusion of diﬀerent\nmodal features from the model level, and generate intermediate\nmultimodal emotional representations from the common semantic\nfeature space, thereby improving the accuracy of SER.\nTherefore, we propose a method named multimodal\ntransformer augmented fusion (MTAF) that uses a hybrid fusion\nstrategy, combining feature-level fusion and model-level fusion\nmethods. The feature-level fusion method is used to fuse the speech\nand text features to obtain multimodal features. Self-Transformer\nEncoders are then used to model the long-term time dependence\nof diﬀerent modal features. A Model-fusion module is proposed\nto generate multimodal emotional intermediate representations\nfor modal guidance and information fusion by Cross-Transformer\nEncoders. Speciﬁcally, the multimodal features are used to enhance\nthe speech and text features. The enhanced text features are then\nused to further enhance the speech features. Finally, the enhanced\nspeech features are used for sentiment classiﬁcation. The superior\nperformance of MTAF over recent state-of-the-art methods is\ndemonstrated through a variety of experiments conducted on the\nIEMOCAP and MELD dataset.\n/two.tnum. Related work\nAlthough multimodal methods have achieved signiﬁcant\nsuccess in the ﬁeld of SER, there are great diﬀerences between\nmodalities, both in their relative independence and in their\nsynchronous or asynchronous information interaction. Hence,\neﬀectively integrating the information from diﬀerent modalities\nremains a diﬃculty and breakthrough point of the research (\nPoria\net al., 2017a ). In the ﬁeld of multimodal emotion recognition,\nresearchers have mainly sought to determine at what stage the\nmodel could perform the fusion of diﬀerent modal features.\nFusion methods can be divided into feature-level fusion (early\nfusion), decision-level fusion (late fusion), model-level fusion,\nand hybrid-level fusion. Feature-level fusion fuses the features of\nvarious modalities (such as visual features, text features, audio\nfeatures) into general feature vectors, and uses the combined\nfeatures for analysis. Wu et al. proposed a new deep learning\narchitecture Parallel Inception Convolutional Neural Network\n(PICNN). They performed convolution in parallel to process sEMG\nsignals from six channels and then used the concatenation method\nto combine features of diﬀerent scales directly before entering\nthem into the remainder of the common convolutional neural\nnetwork (\nWu J. et al., 2021 ; Wu et al., 2022 ). Joshi et al. combined\naudio, video, and text features by adding them and sent them to\nTransformer Encoder (\nJoshi et al., 2022 ). The advantage of feature-\nlevel fusion is that the low-level features of the data are used in the\nearly stage. More information from the original data is used, and\nthe task is completed based on the correlation between multimodal\nfeatures (\nLian et al., 2020 ). However, the features obtained by\nthis fusion method belong to diﬀerent modalities and may vary\ngreatly in many respects. The features must therefore be converted\nto the same format before the fusion process. Moreover, this\nfusion method lacks information interaction within the modality,\nand the high-dimensional feature set may be susceptible to data\nsparsity problems (\nLian et al., 2021 ), making the method prone to\nmodal information redundancy and leading to data overﬁtting. The\nadvantages of feature-level fusion are therefore limited.\nTo overcome this limitation, decision-level fusion uses\nunimodal decision values and fuses them by ensemble learning\n(\nChen and Zhao, 2020 ), tensor fusion ( Zadeh et al., 2017 ), or\nmultiplication layer fusion ( Mittal et al., 2020 ). In the decision-level\nor late fusion process, the features of each modality are examined\nand classiﬁed independently, and the results are fused into decision\nvectors to obtain the ﬁnal decision. The advantage of decision-level\nfusion is that, compared with feature-level fusion, the fusion of\ndecisions obtained from various modalities becomes easier, because\ndecisions generated by multiple modalities often have the same\ndata form (\nPoria et al., 2016 ). Another advantage of this fusion\nprocess is that each modality can use its most appropriate classiﬁer\nor model to learn its features (\nSun et al., 2021 ). However, due\nFrontiers in Neurorobotics /zero.tnum/two.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/eight.tnum/one.tnum/five.tnum/nine.tnum/eight.tnum\nto the use of diﬀerent classiﬁers or models in the analysis task,\nthe learning process in the decision-level fusion phase becomes\ncumbersome and time-consuming. Moreover, this method must\nsolve the problem of the inability to capture more ﬁne-grained\nmodal dynamics, without taking into account the interaction and\ncorrelation between diﬀerent modalities.\nModel-level fusion, in contrast, fuses intermediate\nrepresentations of diﬀerent modalities by using various models\nof deep learning, such as Long Short Term Memory (LSTM),\nattention, and transformer. In model-level fusion, previous\nwork has used kernel-based methods (\nNen et al., 2011 ) to fuse\nmultimodal features, and showing performance improvements.\nSubsequently, Charles achieved good results by combining the\nability of the graph model (\nSutton and McCallum, 2010 ) to\ncompactly model diverse data with the ability of classiﬁcation\nmethods to make predictions using a large number of input\nfeatures. Recently, more advanced methods use attention-based\nneural networks for model-level fusion.\nChen and Jin (2016)\nproposed a multi-modal conditional attention fusion method to\naccomplish a continuous multimodal emotion prediction task.\nTheir method can use the temporal information of video combined\nthe historical information and the diﬀerent levels of features of\ndiﬀerent modalities, and dynamically give diﬀerent weights to\nthe visual and auditory modalities input by LSTM at each time\nstep.\nPoria et al. (2017c) introduced an attention-based fusion\nmechanism called AT-Fusion that uses the attention score of\neach modality to fuse multimodal features. It ampliﬁes higher-\nquality and more informative modalities in the fusion process of\nmultimodal classiﬁcation, and has achieved promising results in\nemotion recognition.\nWang et al. (2021) proposed a multimodal\ntransformer with shared weights for SER. The proposed network\nshares cross-modal weights in each Transformer layer to learn\nthe correlation between multiple modalities. However, the\neﬀect of the model-level fusion method mainly depends on the\nfusion model used. This method lacks ﬁne-grained interactions\nwithin and between modalities, and cannot make full use of the\ncomplementary information between modalities.\nHybrid-level fusion (\nPoria et al., 2017a ) is a combination of\nthe ﬁrst three fusion methods and is therefore more complex.\nSebastian and Pierucci (2019) proposed a combination of early\nand late fusion techniques, using complementary information from\nspeech and text modalities.\nWu W. et al. (2021) proposed a\ndual-branch structure combining time synchronization and time\nasynchronous features for multimodal emotion recognition. A time\nsynchronous branch (TSB) captures the correlation between each\nword in each time step and its acoustic implementation, while\ntime asynchronous branch (TAB) integrates sentence embedding\nfrom context sentences.\nShen et al. (2020) designed a hierarchical\nrepresentation of audio at the word, phoneme and frame levels\nto form more emotionally relevant word-level acoustic features.\nXu et al. (2020) established a hierarchical granularity and feature\nmodel, which helps to capture more subtle clues and obtain a more\ncomplete representation from the original acoustic data. The hybrid\nfusion method varies depending on the combination of the diﬀerent\nfusion methods and is the best and most comprehensive fusion\nmethod at present. However, although the hybrid-level fusion\nmethod combines the advantages of diﬀerent fusion methods and\nmakes diﬀerent modalities interact well, it increases the complexity\nand training diﬃculty.\nMost of the methods above use a single fusion strategy or a\nsingle fusion model and lack ﬁne-grained modal interactions. In\ncontrast to the methods above, our proposed method uses a variety\nof fusion strategies and multi-level fusion models to capture ﬁne-\ngrained intra-modal and inter-modal information interactions,\nand achieve high recognition accuracy. This paper presents the\nmultimodal transformer augmented fusion (MTAF) method for\nemotion recognition, focusing on speech and text domains. The\nnovelty of the work lies in the combination of feature-level and\nmodel-level fusion methods and the introduction of a Model-fusion\nmodule to facilitate ﬁne-grained interactions between and within\nmodalities. We ﬁrst use feature-level fusion to perform early modal\ninteractions between the speech and text modalities. Then, we\nconstruct the three independent models using Self-Cransformer\nEncoders to capture the intra-modality dynamics. Finally, a Model-\nfusion module composed of three Cross-Transformer Encoders to\nperform late modal interactions. By using a joint model, Fine-\ngrained intermodal dynamic interactions are captured for the\nspeech and text modalities.\n/three.tnum. Proposed method\nAs shown in Figure 1, we propose a method that uses both\nspeech and text modalities for emotion recognition. The extracted\nlow-level features are fed in sequence to the transformer encoder.\nThe model consists of ﬁve parts: Speech, Text, Feature-fusion,\nModel-fusion modules, and a classiﬁcation layer.\n/three.tnum./one.tnum. Feature-level fusion\nThe input speech feature sequence of an utterance is\nrepresented as xa. The text feature sequence of an utterance is\nrepresented as xl.\nThe multimodal feature sequence of an utterance is as follows:\nxe = [xa; xl] (1)\nwhere [ ; ] is the concatenation operator.\n/three.tnum./two.tnum. Multimodal transformer\nWe ﬁrst map the speech, text, and multimodal features obtained\nin the previous step to the same dimension through a linear layer.\nThe features are then sent to the Self-Transformer Encoder to\ncapture the time dependence. Finally, the Model-fusion module,\nwhich is composed of three Cross-Transformer Encoders is used\nto generate multimodal emotional intermediate representations\nfor modal guidance and information fusion. Speciﬁcally, the\nmultimodal features are used to enhance the speech and text\nfeatures. The enhanced text features are then used to further\nenhance the speech features.\nFrontiers in Neurorobotics /zero.tnum/three.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/eight.tnum/one.tnum/five.tnum/nine.tnum/eight.tnum\nFIGURE /one.tnum\nArchitecture of the MTAF model.The core components of the Self-Transformer Encoder\nand the Cross-Transformer Encoder are a multihead self-\nattention mechanism and multihead cross-attention mechanism,\nrespectively. Each transformer encoder has m layers and n\nattention heads.\n/three.tnum./two.tnum./one.tnum. Scaled dot-product attention\nThe query Q, key K, and value V of the multi-head self-\nattention mechanism come from the same modality. However, for\nthe multi-head cross-attention mechanism, the source modality\nfeature is transformed to the pair of K and V while the target\nFrontiers in Neurorobotics /zero.tnum/four.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/eight.tnum/one.tnum/five.tnum/nine.tnum/eight.tnum\nmodality feature is transformed into Q. We compute the matrix of\noutputs as follows:\nAttention(Q, K, V) = softmax\n(\nQKT\n√\ndk\n)\nV (2)\nwhere dk is the dimension of Q.\n/three.tnum./two.tnum./two.tnum. Multihead attention\nMultihead Attention allows the model to focus on information\nfrom diﬀerent presentation subspaces in diﬀerent locations.\nMultihead (Q, K, V) = Concat\n(\nhead1, ..., headn\n)\nWO (3)\nwhere headi = Attention\n(\nQWQ\ni , KWK\ni , VWV\ni\n)\n(4)\nwhere n is the number of attention heads; WO, WQ\ni , WK\ni , and WV\ni\nare learned model parameters.\n/three.tnum./three.tnum. Classiﬁcation layer\nAfter the Model-fusion module, the ﬁnal multimodal emotional\nintermediate representation H is passed through a fully-connected\nnetwork and a softmax layer to predict the emotion class with the\ncross-entropy loss as the cost function:\n∼\ny = softmax(wH + b) (5)\nLoss = − 1\nN\nN∑\ni= 1\nyilog\n( ∼\nyi\n)\n(6)\nwhere yi is the true label,\n∼\nyi is the predicted probability distribution\nfrom the softmax layer, w and b are learned model parameters, and\nN is the total number of samples used in training.\n/four.tnum. Experiment set-up\n/four.tnum./one.tnum. Datasets\n/four.tnum./one.tnum./one.tnum. IEMOCAP\nThe IEMOCAP (\nBusso et al., 2008 ) contains approximately 12\nh of audiovisual data. We used the speech and text transcription\ndata which include 7,487 utterances conveying seven emotions:\nfrustration (1,849), neutral (1,708), anger (1,103), sadness (1,084),\nexcitement (1,041), happiness (595), and surprise (107). Excitement\nis incorporated into happiness. We randomly split the dataset into\na training (80%) and a test (20%) set.\n/four.tnum./one.tnum./two.tnum. MELD\nThe MELD (\nPoria et al., 2018 ) is a new multimodal dataset for\nemotion recognition. It consists of 13,708 utterances with seven\nemotions (anger, disgust, fear, joy, neutral, sadness, and surprise)\ntaken from 1,433 dialogues from the classic TV-series Friends. The\nwhole dataset is divided into training, validation, and test sets. In\nthis work, we only use the training and test sets.\n/four.tnum./two.tnum. Speech and text features\n/four.tnum./two.tnum./one.tnum. Speech features\nLibrosa (\nMcFee et al., 2015 ), a Python package, was used to\nextract utterance-level speech features. Features with a total of\n199 dimensions were extracted, including Mel-Frequency Ceptral\nCoeﬃcients (MFCC), chroma, pitch, zero-crossing rate, spectral\nand their statistical measures (HSDs) such as mean, standard\ndeviation, minimum, and maximum.\n/four.tnum./two.tnum./two.tnum. Text features\nThe transcripts in the IEMOCAP and MELD dataset were used\nto extract a 1,890-dimensional Term Frequency-Inverse Document\nFrequency (TFIDF) feature vector. TFIDF is a numerical statistic\nthat shows the correlation between a word and a document in a\ncollection or corpus (\nSahu, 2019).\n/four.tnum./three.tnum. Implementation details\nThrough a linear layer, we obtain 256-dimensional speech, text\nand multimodal features. We feed them into Self-Transformer\nEncoder, which has 2 Transformer Encoder layers and 4 multi-\nhead attention heads. Next, three newly generated 256-dimensional\nvectors are sent to the Model-fusion module, which is composed of\nthree Cross-Transformer Encoders. Q is the ﬁrst residual part of the\nCross-Transformer Encoder to perform deep interactions between\nmodalities. Cross-Transformer Encoder and Self-Transformer\nEncoder have the same number of layers and attention heads. After\nthe Model-fusion module, a 256-dimensional emotional feature\nvector is ﬁnally obtained for sentiment classiﬁcation.\nThe training procedure was implemented using PyTorch on a\nGTX3090. We used the Adam (\nKingma and Ba, 2014 ) optimizer,\nsetting the learning rate to 0.0001. The batch size was 200. To\nalleviate overﬁtting, we used the dropout method with a rate of 0.4.\nWe trained the model for at most 50,000 epochs until the accuracy\ndid not change. Weighted accuracy (WA) and unweighted accuracy\n(UA) were used as the evaluation metrics.\n/five.tnum. Experiment results\n/five.tnum./one.tnum. Comparison with state-of-the-art\napproaches\nTo verify the eﬀectiveness of our proposed method, we\ncompared our MTAF with the following thirteen state-of-the-\nart approaches, all of which use multiple modalities for emotion\nFrontiers in Neurorobotics /zero.tnum/five.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/eight.tnum/one.tnum/five.tnum/nine.tnum/eight.tnum\nrecognition. These methods can be divided into four groups\naccording to the fusion level.\n(1) Feature-level fusion: (a) Audio + Text_LSTM (\nSahu, 2019 )\ndirectly sends the concatenated features to the bidirectional LSTM\nnetwork. (b) COGMEN (\nJoshi et al., 2022 ) propose COntextualized\nGraph Neural Network based Multimodal Emotion recognitioN\n(COGMEN) system that leverages local information (inter/intra\ndependency between speakers) and global information (context).\n(2) Decision-level fusion: (a) In\nKumar et al. (2021), the audio\nand textual features were extracted separately using attention-\nbased Gated Recurrent Unit (GRU) and pre-trained Bidirectional\nEncoder Representations from Transformers (BERT), respectively.\nThen they were concatenated and used to predict the ﬁnal emotion\nclass. (b) In MDNN (\nZhou et al., 2018 ), the proposed framework\ntrain raw features by groups in local classiﬁers to avoid high\ndimensional. Then high-level features of each local classiﬁers are\nconcatenated as input of a global classiﬁer. (c) bcLSTM (\nPoria et al.,\n2017b) propose a LSTM network that takes as input the sequence\nof utterances in a video and extracts contextual unimodal and\nmultimodal features by modeling the dependencies.\n(3) Model-level fusion: (a)\nXu et al. (2019) utilized an attention\nnetwork to learn the alignment between speech and text. (b)\nMCSAN (\nSun et al., 2021 ) employed the parallel cross- and\nself attention modules to explicitly model both inter- and intra-\nmodal interactions of audio and text. (c) CAN (\nYoonhyung\net al., 2020 ) applied the attention weights of each modality\nto the other modality in a crossed way so that the CAN\ngathers the audio and text information from the same time steps\nbased on each modality. (d) CMA + Raw waveform (\nKrishna\nand Patil, 2020 ) applied Cross-modal attention to the output\nsequences from the audio encoder and text encoder, which helps\nin ﬁnding the interactive information between the audio and\ntext sequences and thus helps improve the performance. (e)\nCTNet (\nLian et al., 2021 ) proposed to use the transformer-\nbased structure to model intra-modal and cross-modal interactions\namong multimodal features.\n(4) Hybrid-level fusion: (a) Late Fusion-III (\nSebastian\nand Pierucci, 2019 ) employed various fusion techniques to\nprovide relevance to intermodality dynamics, while keeping\nthe separate models to capture the intra-modality dynamics.\n(b) HGFM (\nXu et al., 2020 ) took the output of frame-\nlevel structure as the input of utterance-level structure and\nextract the acoustic features of these two levels respectively\nfor eﬀective and complementary fusion. (c) STSER (\nChen\nand Zhao, 2020 ) applied a multi-scale fusion strategy,\nincluding feature fusion and ensemble learning to improve\nthe overall performance.\nAll the results are listed in\nTables 1, 2. On the IEMOCAP\ndataset, as we can see, our proposed method achieves 72.31% WA\nand 75.08% UA. Compared with other state-of-the-art approaches,\nthe WA of our method is 0.61 to 14.41% higher and the UA is 0.08\nto 26.38% higher. On the MELD dataset, our proposed method\nachieves 48.12% WA. A 5.82 to 14.12% improvement on other\napproaches. Although our method is superior to other algorithms,\nthe overall performance on the MELD dataset is not ideal. We\nspeculate that this is because, compared with the IEMOCAP\ndataset, the MELD dataset should be a relatively large dataset in\nthe ﬁeld of emotion recognition. Its data comes from the TV\nTABLE /one.tnum Model performance comparisons on the IEMOCAP dataset.\nModel WA (%) UA (%)\nAudio+Text_LSTM 64.20 —\nCOGMEN 68.2 —\nKumar et al. 71.70 75.00\nXu et al. 70.40 69.50\nMCSAN 61.20 56.00\nCAN 57.90 48.70\nCMA+Raw waveform — 72.82\nCTNet — 67.60\nLate Fusion-III 61.20 59.30\nSTSER 71.06 72.05\nMTAF 72.31 75.08\nThe bold values indicate the model with the best results, WA and UA wi th the highest\nresults, respectively.\nprogram Friends, which is not closer to real life than the data in\nIEMOCAP. Moreover, the data collection conditions are not as\nstandardized as those of IEMOCAP. The recognition accuracy for\nMELD is therefore not as high as IEMOCAP. But we do believe that\nMELD is a good platform to compare and validate our method,\nbecause compared with other models, the experimental results of\nour method improves a lot although the overall results are low.\nOn the surface it seems there is limited improvement in\nboth WA and UA, compared to Kumar’s work. Actually, our\nexperimental results were obtained by averaging the results of\n10 experiments. Each experiment is carried out under the same\nexperimental conditions, including datasets, model parameters,\ntraining and testing processes. The purpose is to reduce the\ninﬂuence of random errors and increase the reliability and stability\nof the results. The highest experimental results of WA was 73.18%\nand UA was 75.49% on the IEMOCAP dataset. Compared to\nKumar’s work, our method achieves 1.48% higher WA and 0.49%\nhigher UA. In addition, the training speed of our proposed model\nis very fast, so that it can process large amounts of data quickly. The\nmemory occupied by the model is also very small, which is achieved\nby optimizing and streamlining the model to minimize unnecessary\ncomputing and storage operations. We therefore believe that the\nexperimental results show the superiority of our method.\nBased on the above experimental results, we analyze the\nperformance enhancements of the model. (1) Though a\nhybrid fusion strategy, our model combines the advantages\nof feature-level and decision-level fusion methods to better\nintegrate the two modalities of speech and text. It uses the\ncomplementary information of the two modalities to better\ngenerate emotional representation. The improvement of the\naccuracy in the experimental results eﬀectively veriﬁes this point.\n(2) The multimodal Transformer Encoders achieve ﬁne-grained\ninter-modal and intra-modal interactions between speech and\ntext modalities well. The Model-fusion module composed of\nthree Cross-Transformer Encoders can generate multimodal\nemotional intermediate representations for modal guidance and\ninformation fusion.\nFrontiers in Neurorobotics /zero.tnum/six.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/eight.tnum/one.tnum/five.tnum/nine.tnum/eight.tnum\n/five.tnum./two.tnum. Confusion matrix of experiment\nFigure 2 shows the confusion matrices of models using speech\nonly and text only and the confusion matrix for our proposed\nmodel MTAF on the IEMOCAP dataset. The Speech-Only and\nTABLE /two.tnum Model performance comparisons on the MELD dataset.\nModel WA (%)\nMDNN 34.00\nbcLSTM 39.10\nHGFM 42.30\nMTAF 48.12\nThe bold values indicate the model with the best results, WA with th e highest\nresults, respectively.\nText-Only models have a Speech module and Text module,\nrespectively, but not a Model-fusion module.\nFor the two emotional categories of frustration and neutral\nfrom\nFigure 2, the recognition accuracy of the multimodal model is\nclose to those of the single-modality models. For other emotional\ncategories, the recognition accuracy of the multimodal model is\nmuch higher than those of the single-modality models. We also\nobserve a signiﬁcant improvement of 10–27% recognition accuracy\nfor the happiness category after combining speech and text for\nemotion recognition. Furthermore, the recognition accuracy of\nthe anger category is signiﬁcantly improved by 15–18%, and that\nof the sad category is signiﬁcantly improved by 9–14%. These\nexperiment results conﬁrm the eﬀectiveness of emotion recognition\nthat combines speech and text modalities. Multimodal methods\ncombine the advantages of diﬀerent modalities to obtain richer\nFIGURE /two.tnum\nConfusion matrices of each model on the IEMOCAP dataset. (A) Speech-only confusion matrix. (B) Text-only confusion matrix. (C) MTAF confusion\nmatrix.\nFrontiers in Neurorobotics /zero.tnum/seven.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/eight.tnum/one.tnum/five.tnum/nine.tnum/eight.tnum\nTABLE /three.tnum Ablation study of our proposed method.\nModel WA (%) UA (%)\nFeature fusion 67.15 68.30\nModel fusion 63.66 64.95\nWithout self-transformer encoder 70.09 73.48\nWithout cross-transformer encoder 69.65 72.24\nMTAF 72.31 75.08\nThe bold values indicate the model with the best results, WA and UA w ith the highest\nresults, respectively.\nemotional representation. Moreover, complementary information\nof diﬀerent modalities can also increase the robustness of the\nsystem when noise occurs in one modality.\nIt is notable that the anger, happiness, and neutral categories\nare misclassiﬁed as sadness with a relatively large probability.\nAdditionally, the happiness and sadness categories are also often\nclassiﬁed as neutral. 55% of interpersonal relationships rely on\nfacial expressions or body movements, 38% rely on speech, and\nonly 7% rely on text (\nMehrabian, 1971). Facial expressions therefore\ngive very important clues to human emotions ( Kim et al., 2017 ;\nDai et al., 2021 ; Lee et al., 2021 ). On the basis of speech and text\nmodalities, increasing facial expressions can improve recognition\naccuracy (\nYoon et al., 2019 ; Kumar et al., 2022 ). Thus, we infer\nthat humans express these emotions more in facial expressions\nthan in speech and semantic content. These are interesting ﬁndings\nrequiring more research and may lead to further improvement in\nthe recognition accuracy.\n/five.tnum./three.tnum. Ablation study\nA variety of ablation experiments were conducted on the\nIEMOCAP dataset to evaluate the fusion methods, transformer\nencoders, and model parameters in our proposed method.\nTables 3,\n4 present the results.\nTable 3 shows the ablation study results for : Feature Fusion,\nModel Fusion, Without Self-Transformer Encoder, and Without\nCross-Transformer Encoder. The Feature Fusion model includes\nthe Feature-fusion module in the middle of\nFigure 1, but not\nthe Model-fusion module. The Model Fusion model does not\ninclude the Feature-fusion module branch and uses only one Cross-\nTransformer Encoder for multimodal fusion. Then, the Without\nSelf-Transformer Encoder model directly sends extracted low-level\nspeech, text and concatenated multimodal features into the Model-\nfusion module. Finally, The Without Cross-Transformer Encoder\nmodel removes the Model-fusion module and concatenates the\noutputs of the three branches directly into a fully-connected layer.\nIn comparison to the Feature Fusion and Model Fusion\nmodels, our proposed model, MTAF, achieves 5.16 to 8.65%\nhigher WA and 6.78 to 10.13% higher UA. The experimental\nresults conﬁrm the eﬀectiveness of our proposed hybrid fusion\nstrategy. It combines the advantages of feature-level and model-\nlevel fusion methods, captures more ﬁne-grained intra-modal and\ninter-model interactions, and makes full use of the complementary\nTABLE /four.tnum Ablation study of transformer encoder.\nn m WA (%) UA (%)\n1 3 70.89 73.47\n2 4 72.31 75.08\n3 5 71.57 73.86\n5 8 70.95 73.64\n6 10 70.69 73.17\nThe bold values indicate the Transformer Encoder layers and multi-hea d attention heads with\nthe best results, WA and UA with the highest results, respectively.\ninformation of speech and text modalities to obtain richer\nemotional representation.\nWhen the Self-Transformer Encoder is removed, the\nmodel’s performance decreases by 2.22% in terms of WA\nand 1.60% in terms of UA. This ﬁnding highlights the\nimportance of using multi-head self-attention mechanisms\nfor intra-modal interaction to capture contextual time\ndependence. Furthermore, the model’s performance\ndecreases when the Cross-Transformer Encoder is removed,\nwhich indicates that the multi-head cross-attention\nmechanisms can integrate features of diﬀerent modalities for\ninformation exchange and generate multimodal emotional\nintermediate representations.\nTable 4 shows the results of using n Transformer Encoder layers\nand m multi-head attention heads. Through comparison of the\nresults, we found that n = 2, and m = 4 achieves the best results.\nThis ﬁnding shows that deep-level models are not suitable for SER\ntasks.\n/six.tnum. Conclusion\nWe propose a method named multimodal transformer\naugmented fusion that uses a hybrid fusion of both speech\nand text features, combining feature-level fusion and model-\nlevel fusion methods, to eﬀectively integrate diﬀerent\nmodal information. A Model-fusion module composed of\nthree Cross-Transformer Encoders is proposed to generate\nmultimodal emotional representation for modal guidance\nand information fusion. Speciﬁcally, the Transformer\nEncoders are used to perform ﬁne-grained dynamic intra-\nand inter-modality interactions. Moreover, experimental\nresults demonstrate the eﬀectiveness of our proposed method\non the IEMOCAP and MELD dataset. In future work, we\nwill try to add facial expressions for multimodal emotion\nrecognition and further improve the accuracy of speech\nemotion recognition.\nData availability statement\nThe original contributions presented in the study are included\nin the article/supplementary material, further inquiries can be\ndirected to the corresponding author.\nFrontiers in Neurorobotics /zero.tnum/eight.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/eight.tnum/one.tnum/five.tnum/nine.tnum/eight.tnum\nAuthor contributions\nYW: conceptualization and methodology. YG: validation and\nsupervision. All authors contributed to the article and approved the\nsubmitted version.\nFunding\nThis work was supported by the National Natural Science\nFoundation of China (Nos. 62271377 and 62201407), the Key\nResearch and Development Program of Shanxi (Program Nos.\n2023-YBGY-244, 2021ZDLGY0106, and 2022ZDLGY0112), the\nFundamental Research Funds for the Central Universities under\nGrant No. ZYTS23064, the National Key R&D Program of China\nunder Grant Nos. 2021ZD0110404 and SQ2021AAA010888, and\nthe Key Scientiﬁc Technological Innovation Research Project by\nMinistry of Education.\nConﬂict of interest\nYY was employed by the company Guangzhou Huya\nTechnology Co., Ltd.\nThe remaining authors declare that the research was\nconducted in the absence of any commercial or ﬁnancial\nrelationships that could be construed as a potential conﬂict\nof interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nAtmaja, B. T., Sasou, A., and Akagi, M. (2022). Survey on bimo dal speech emotion\nrecognition from acoustic and linguistic information fusio n. Speech Commun. 140,\n11–28. doi: 10.1016/j.specom.2022.03.002\nAyadi, M. E., Kamel, M. S., and Karray, F. (2011). Survey on spee ch emotion\nrecognition: features, classiﬁcation schemes, and databas es. Pattern Recogn. 44, 572–\n587. doi: 10.1016/j.patcog.2010.09.020\nBusso, C., Bulut, M., Lee, C.-C., Kazemzadeh, A., Mower, E., K im, S., et al. (2008).\nIEMOCAP: interactive emotional dyadic motion capture datab ase. Lang. Resour. Eval.\n42, 335–359. doi: 10.1007/s10579-008-9076-6\nChen, M., and Zhao, X. (2020). “A multi-scale fusion framework for bimodal speech\nemotion recognition, ” inInterspeech (Cary, NC), 374–378.\nChen, S., and Jin, Q. (2016). “Multi-modal conditional atten tion fusion for\ndimensional emotion prediction, ” in Proceedings of the 24th ACM International\nConference on Multimedia (New York, NY: ACM), 571–575.\nDai, W., Cahyawijaya, S., Liu, Z., and Fung, P. (2021). Multim odal end-to-end\nsparse model for emotion recognition. arXiv preprint arXiv:2103.09666 .\nJoshi, A., Bhat, A., and Jain, A. (2022). “Contextualized gnn based multimodal\nemotion recognition, ” in Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics : Human Language\nTechnologies (Stroudsburg, PA), 4148–4164.\nKim, H. R., Kim, S. J., and Lee, I. K. (2017). Building emotiona l machines:\nRecognizing image emotions through deep neural networks. IEEE Trans. Multimedia .\n20, 2980–2992. doi: 10.1109/TMM.2018.2827782\nKingma, D., and Ba, J. (2014). Adam: a method for stochastic o ptimization. Comput.\nSci.\nKrishna, D. N., and Patil, A. (2020). “Multimodal emotion reco gnition using cross-\nmodal attention and 1d convolutional neural networks, ” in Interspeech (Cary, NC),\n4243–4247.\nKumar, P., Kaushik, V., and Raman, B. (2021). “Towards the ex plainability of\nmultimodal speech emotion recognition, ” in InterSpeech (Cary, NC), 1748–1752.\nKumar, P., Malik, S., and Raman, B. (2022). Interpretable multim odal emotion\nrecognition using hybrid fusion of speech and image data. arXiv preprint\narXiv:2208.11868.\nLee, S., Han, D. K., and Ko, H. (2021). Multimodal emotion reco gnition fusion\nanalysis adapting bert with heterogeneous feature uniﬁcatio n. IEEE Access . 9, 94557–\n94572. doi: 10.1109/ACCESS.2021.3092735\nLian, Z., Liu, B., and Tao, J. (2021). “CTNet: conversationa l transformer network\nfor emotion recognition, ” in IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing (New York, NY: IEEE).\nLian, Z., Tao, J., Liu, B., Huang, J., Yang, Z., and Li, R. (202 0). “Context-\ndependent domain adversarial neural network for multimodal e motion recognition, ”\nin Interspeech (Cary, NC), 394–398.\nMarchi, E., Schuller, B., Batliner, A., Fridenzon, S., and Golan, O. (2012). “Emotion\nin the speech of children with autism spectrum conditions: Pros ody and everything\nelse, ” inProceedings 3rd Workshop on Child, Computer and Interaction (WOCCI 20 12).\nMcFee, B., Raﬀel, C., Liang, D., Ellis, D. P., McVicar, M., Batte nberg, E., et al. (2015).\n“librosa: Audio and music signal analysis in Python, ” in Proceedings of the 14th Python\nin Science Conference , 18–25.\nMehrabian, A. (1971). Silent Messages . Belmont, CA: Wadsworth Publishing\nCompany, Inc.\nMittal, T., Bhattacharya, U., Chandra, R., Bera, A., and Mano cha, D. (2020). “M3er:\nmultiplicative multimodal emotion recognition using facial, te xtual, and speech cues, ”\nin Proceedings of the AAAI Conference on Artiﬁcial Intelligence (Menlo Park, CA: AAAI\nPress), 1359–1367.\nNen, M., Alpayd, and Ethem, N. (2011). Multiple kernel learning algo rithms. J.\nMach. Learn. Res. 12, 2211–2268.\nPoria, S., Cambria, E., Bajpai, R., and Hussain, A. (2017a). A review of aﬀective\ncomputing: from unimodal analysis to multimodal fusion. Inform. Fusion 37, 98–125.\ndoi: 10.1016/j.inﬀus.2017.02.003\nPoria, S., Cambria, E., Hazarika, D., Majumder, N., Zadeh, A ., and Morency,\nL.-P. (2017b). “Context-dependent sentiment analysis in use r-generated videos, ” in\nProceedings of the 55th Annual Meeting of the Association for Computati onal Linguistics\n(Stroudsburg), 873–883.\nPoria, S., Cambria, E., Hazarika, D., Mazumder, N., Zadeh, A ., and Morency,\nL.-P. (2017c). “Multi-level multiple attentions for contextual multimodal sentiment\nanalysis, ” in2017 IEEE International Conference on Data Mining (ICDM) (Piscataway,\nNJ: IEEE), 1033–1038.\nPoria, S., Cambria, E., Howard, N., Huang, G., and Hussain, A.\n(2016). Fusing audio, visual and textual clues for sentiment analysis from\nmultimodal content. Neurocomputing 174, 50–59. doi: 10.1016/j.neucom.2015.\n01.095\nPoria, S., Hazarika, D., Majumder, N., Naik, G., Cambria, E., and Mihalcea,\nR. (2018). “MELD: a multimodal multi-party dataset for emotion recognition in\nconversations, ” in Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics (Stroudsburg).\nSahu, G. (2019). Multimodal speech emotion recognition and amb iguity resolution.\narXiv preprint arXiv:1904.06022 .\nSchuller, B. W. (2018). Speech emotion recognition two decades in a nutshell,\nbenchmarks, and ongoing trends. Commun. ACM 61, 90–99. doi: 10.1145/3129340\nSebastian, J., and Pierucci, P. (2019). “Fusion techniques f or utterance-level emotion\nrecognition combining speech and transcripts, ” in Interspeech (Cary, NC), 51–55.\nSebe, N., Cohen, I., Gevers, T., and Huang, T. S. (2005). “Multi modal approaches\nfor emotion recognition: a survey, ” in Proceedings of SPIE - The International Society\nfor Optical Engineering (Bellingham).\nFrontiers in Neurorobotics /zero.tnum/nine.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/eight.tnum/one.tnum/five.tnum/nine.tnum/eight.tnum\nShen, G., Lai, R., Chen, R., Zhang, Y., Zhang, K., Han, Q., et al. (2020). “Wise:\nword-level interaction-based multimodal fusion for speech em otion recognition, ” in\nInterspeech (Cary, NC), 369–373.\nShimojo, S., and Shams, L. (2001). Sensory modalities are not separate\nmodalities: plasticity and interactions. Curr. Opin. Neurobiol. 11, 505–509.\ndoi: 10.1016/S0959-4388(00)00241-5\nSun, L., Liu, B., Tao, J., and Lian, Z. (2021). “Multimodal cro ss-and self-attention\nnetwork for speech emotion recognition, ” in ICASSP 2021-2021 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP) (IEEE), 4275–4279.\nSutton, C., and McCallum, A. (2010). An introduction to condit ional random ﬁelds.\nFound. Trends Mach. Learn. 4, 267–373. doi: 10.1561/2200000013\nTao, J., and Tan, T. (2005). “Aﬀective computing: a review, ” i n Aﬀective Computing\nand Intelligent Interaction: First International Conference, ACII 20 05 (Beijing), 981–\n995.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). Attention is all you need. arXiv preprint arXiv:1706.03762 .\nWang, C., Ren, Y., Zhang, N., Cui, F., and Luo, S. (2022). Speec h emotion\nrecognition based on multi-feature and multi-lingual fusion. Multimedia Tools Appl.\n81, 4897–4907. doi: 10.1007/s11042-021-10553-4\nWang, Y., Shen, G., Xu, Y., Li, J., and Zhao, Z. (2021). “Learni ng mutual correlation\nin multimodal transformer for speech emotion recognition, ” i n Interspeech (Cary, NC),\n4518–4522.\nWani, T. M., Gunawan, T. S., Qadri, S., Kartiwi, M., and Ambika irajah, E. (2021).\nA comprehensive review of speech emotion recognition systems . IEEE Access . 9,\n47795–47814. doi: 10.1109/ACCESS.2021.3068045\nWu, J., Zhang, Y., Xie, L., Yan, Y., Zhang, X., Liu, S., et al. (2 022). A novel\nsilent speech recognition approach based on parallel inception conv olutional neural\nnetwork and mel frequency spectral coeﬃcient. Front. Neurorobot. 16, 971446.\ndoi: 10.3389/fnbot.2022.971446\nWu, J., Zhao, T., Zhang, Y., Xie, L., Yan, Y., and Yin, E. (2021 ). “Parallel-inception\ncnn approach for facial semg based silent speech recognition, ” i n 2021 43rd Annual\nInternational Conference of the IEEE Engineering in Medicine & Biology Society\n(EMBC), 554–557.\nWu, W., Zhang, C., and Woodland, P. C. (2021). “Emotion recogn ition by fusing\ntime synchronous and time asynchronous representations, ” i n ICASSP 2021-2021\nIEEE International Conference on Acoustics, Speech and Signal Processi ng (ICASSP)\n(Piscataway, NJ: IEEE), 6269–6273.\nXu, H., Zhang, H., Han, K., Wang, Y., Peng, Y., and Li, X. (2019 ). Learning\nalignment for multimodal emotion recognition from speech. arXiv preprint\narXiv:1909.05645.\nXu, Y., Xu, H., and Zou, J. (2020). “HGFM: a hierarchical grain ed and feature\nmodel for acoustic emotion recognition, ” in ICASSP 2020-2020 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP) (Piscataway, NJ: IEEE),\n6499–6503.\nYoon, S., Dey, S., Lee, H., and Jung, K. (2019). Attentive mod ality hopping\nmechanism for speech emotion recognition. arXiv preprint arXiv:1912.00846 .\nYoonhyung, L., Yoon, S., and Jung, K. (2020). “Multimodal spee ch emotion\nrecognition using cross attention with aligned audio and tex t, ” in Interspeech (Cary,\nNC), 2717–2721.\nZadeh, A., Chen, M., Poria, S., Cambria, E., and Morency, L. P . (2017). “Tensor\nfusion network for multimodal sentiment analysis, ” in Proceedings of the 2017\nConference on Empirical Methods in Natural Language Processing (Stroudsburg), 1103–\n1114.\nZhou, S., Jia, J., Wang, Q., Dong, Y., Yin, Y., and Lei, K. (201 8).\n“Inferring emotion from conversational voice data: a semi- supervised\nmulti-path generative neural network approach, ” in Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence (Menlo Park, CA:\nAAAI Press).\nFrontiers in Neurorobotics /one.tnum/zero.tnum frontiersin.org"
}