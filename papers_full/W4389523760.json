{
    "title": "Can We Edit Multimodal Large Language Models?",
    "url": "https://openalex.org/W4389523760",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2110418884",
            "name": "Siyuan Cheng",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A5086969601",
            "name": "Bozhong Tian",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2110561275",
            "name": "Qingbin Liu",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A1983188002",
            "name": "Xi Chen",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2136600212",
            "name": "Yongheng Wang",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2114954316",
            "name": "Huajun Chen",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2132377640",
            "name": "Ningyu Zhang",
            "affiliations": [
                "Zhejiang University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4367367040",
        "https://openalex.org/W4367628410",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W1889081078",
        "https://openalex.org/W4386080925",
        "https://openalex.org/W4362598605",
        "https://openalex.org/W4362515116",
        "https://openalex.org/W4385569933",
        "https://openalex.org/W4366850747",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W4318239201",
        "https://openalex.org/W4318718936",
        "https://openalex.org/W3036167779",
        "https://openalex.org/W4318142410",
        "https://openalex.org/W4298289240",
        "https://openalex.org/W4312933868",
        "https://openalex.org/W4377865177",
        "https://openalex.org/W4387356330",
        "https://openalex.org/W2947312908",
        "https://openalex.org/W4282980384",
        "https://openalex.org/W4366330503",
        "https://openalex.org/W4315881234",
        "https://openalex.org/W4378474098",
        "https://openalex.org/W4385262399",
        "https://openalex.org/W4375869762",
        "https://openalex.org/W4286897388",
        "https://openalex.org/W4385728911",
        "https://openalex.org/W4320561779",
        "https://openalex.org/W4311642023",
        "https://openalex.org/W4361194507",
        "https://openalex.org/W4287820586",
        "https://openalex.org/W2912924812",
        "https://openalex.org/W4385570086",
        "https://openalex.org/W4206118214",
        "https://openalex.org/W4226251122",
        "https://openalex.org/W3122241445",
        "https://openalex.org/W4306313145",
        "https://openalex.org/W4382132560",
        "https://openalex.org/W1933349210",
        "https://openalex.org/W4288329833",
        "https://openalex.org/W2560730294",
        "https://openalex.org/W4282963182",
        "https://openalex.org/W4304192692",
        "https://openalex.org/W4303443398",
        "https://openalex.org/W4385849354",
        "https://openalex.org/W4378465178",
        "https://openalex.org/W4386875188",
        "https://openalex.org/W4310998073",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W4285294723",
        "https://openalex.org/W4367860252",
        "https://openalex.org/W4324299623",
        "https://openalex.org/W4301194718",
        "https://openalex.org/W4386044411",
        "https://openalex.org/W4377864718",
        "https://openalex.org/W4386907423"
    ],
    "abstract": "In this paper, we focus on editing multimodal Large Language Models (LLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights.",
    "full_text": "Can We Edit Multimodal Large Language Models?\nSiyuan Chengâ™£râ™¥âˆ—, Bozhong Tianâ™£râˆ—, Qingbin Liuâ™¥, Xi Chenâ™¥â€ ,\nYongheng Wangâ™¦, Huajun Chenâ™£râ™ ,Ningyu Zhangâ™£râ€ \nâ™£Zhejiang Universityr Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph\nâ™ Donghai Laboratoryâ™¥Platform and Content Group, Tencentâ™¦Zhejiang Laboratory\n{sycheng,tbozhong,huajunsir,zhangningyu}@zju.edu.cn\n{qingbinliu,jasonxchen}@tencent.com,wangyh@zhejianglab.com\nAbstract\nIn this paper, we focus on editing Multimodal\nLarge Language Models (MLLMs). Compared\nto editing single-modal LLMs, multimodal\nmodel editing is more challenging, which de-\nmands a higher level of scrutiny and careful\nconsideration in the editing process. To fa-\ncilitate research in this area, we construct a\nnew benchmark, dubbedMMEdit, for editing\nmultimodal LLMs and establishing a suite of\ninnovative metrics for evaluation. We conduct\ncomprehensive experiments involving various\nmodel editing baselines and analyze the impact\nof editing different components for multimodal\nLLMs. Empirically, we notice that previous\nbaselines can implement editing multimodal\nLLMs to some extent, but the effect is still\nbarely satisfactory, indicating the potential dif-\nficulty of this task. We hope that our work can\nprovide the NLP community with insights1.\n1 Introduction\nWiththewidespreaddeploymentofLargeLanguage\nModels (LLMs) (Zhao et al., 2023), the necessity\nto maintain their knowledge accurate and current\nwithout incurring significant retraining costs is\nbecoming increasingly paramount (Sinitsin et al.,\n2020). Previousresearchhasintroducedknowledge\nediting methodologies designed to incrementally\ninfuse a language model with a new set of facts\n(Mitchelletal.,2022a;Hanetal.,2023;Hartvigsen\net al., 2022; Zhong et al., 2023; Gandikota et al.,\n2023; Yao et al., 2023).\nDifferent from single-modal model editing, the\ntask of editing multimodal LLMs presents consid-\nerablechallenges,giventheirinherentdiversityand\ncomplexity. Specifically, incorrect outputs from\nmultimodal models may stem from the synergistic\neffects of various modalities. Incorrect outputs\nâˆ—Equal contribution.\nâ€ Corresponding author.\n1Codeanddatasetareavailablein https://github.com/\nzjunlp/EasyEdit.\nBefore Editing After Editing\nA street barrier \nis on the ground.\nA ladder is\non the ground.\nA clock looks like a little bear.\nImage \nunderstanding \nerror\nThis picture \ndepicts\nA street barrier.\nUpdates\nFigure 1: Overview of themultimodal model editing\ntask. The editing target is to update the modelâ€™s under-\nstanding of the edited input (e.g., image or text), while\nensuring its interpretation of unrelated inputs remains\nas consistent as possible.\nmaystemnotjustfromLLMs, analogoustohuman\nerrorslikemisreadingormisrecognition(e.g.,color\nblindness affecting color identification in images).\nAs shown in Figure 1, before the editing, the model\nmisidentified the object as a â€œladderâ€ instead of\nthe correct â€œbarrierâ€, resulting in an erroneous pre-\ndiction. After the editing, the model accurately\nrecognized the â€œbarrierâ€. Note that the utility of\nmultimodal LLMs (Yin et al., 2023) is increas-\ning, yet there is a lack of corresponding dataset\nresources and benchmarks for editing multimodal\nlarge language models.\nTo facilitate research in this area, we take the\nfirst step to construct aMultimodal Model Editing\nbenchmark: dubbedasMMEdit,whichencompass\ntwo sub-tasks: Editing VQA and Editing Image\nCaptioning. Specifically, we follow single-modal\nmodel editing approaches (Mitchell et al., 2022a;\nCaoetal.,2021;Mitchelletal.,2022b)toconstruct\nthe datasets, which extends the previous evalua-\ntion principle, namelyReliability2, Locality3, and\nGenerality4, to multimodal settings.\nFor Reliability evaluation, we start with rigorous\ndata collection, gathering underperforming multi-\n2The metric used to measure the success of editing target.\n3It measures whether unrelated facts retain their outputs.\n4It measures the success of editing related knowledge.\nmodal model data to create a dedicated reliability\nediting dataset (Â§3.2.1). For Locality evaluation,\nwesplititintothetextualandmultimodallocalityto\nevaluate the stability of multimodal LLMs (Â§3.2.2).\nFor Generality evaluation, similar to Locality, we\ndivide it into textual and multimodal generality\nand utilize ChatGLM (Du et al., 2022), and Sta-\nble Diffusion (Rombach et al., 2022) to generate\nrephrased text as well as rephrased images for eval-\nuation (Â§3.2.3). We evaluate several knowledge\nediting approaches onMMEdit. Empirically, we\nnotice that current editing approaches are effective\nfor editing the textual model in the multimodal\nlanguage model but not as effective for editing the\nvision module. For example, in editing the lan-\nguage module of the BLIP-2 model, the reliability\nofMENDcanreach99.4%,butonlyattain65.2%if\nediting the vision module, indicating the potential\ndifficulty and opportunities of this task. In general,\nour primary contributions are as follows:\nâ€¢ Wetakethefirststeptoinvestigateeditingmul-\ntimodal LLMs, which extends model editing\nto multimodal settings.\nâ€¢ We proposeMMEdit, a new benchmark, to\nevaluate the reliability, locality, and generality\nof multimodal model editing approaches.\nâ€¢ We conduct experiments with various base-\nlines,demonstratingthatwhilecurrentmethod-\nologies can somewhat aid in multimodal edit-\ning, the outcomes still fall short of complete\nsatisfaction. We will make the code and\ndatasets publicly available for future research\npurposes.\n2 Related Work\n2.1 Multimodal Language Models\nMultimodal Learning (MML) (Xu et al., 2022a;\nYin et al., 2023) provides a holistic approach to\ncrafting AI models that can extract and correlate\ninformation from various data modalities. Due\nto its societal significance, MML has established\na foothold in the research community, solidify-\ning itself as a crucial field of study over the past\ndecade. Vision-language pre-training is one of\nthe important branches of MML, which aims to\nlearnmultimodalfoundationmodelswithimproved\nperformance on various vision and language tasks.\nVisionTransformer(ViT)(Dosovitskiyetal.,2021)\nis a seminal work that contributes an end-to-end\nsolution by applying the encoder of Transformers\nto images. CLIP (Radford et al., 2021) proposes\na method, which uses multimodal pre-training to\nconvert classification as a retrieval task that en-\nables the pre-trained models to tackle zero-shot\nrecognition. Recently, the advancement of LLMs,\nsuch as LLaMA (Touvron et al., 2023), BLOOM\n(Scao et al., 2022), and ChatGPT (OpenAI, 2022),\nhas been bolstered by scaled-up training data and\nincreased parameters, yielding significant recent\nsuccess. These models showcase impressive lan-\nguage understanding, generation, and knowledge\nreasoning capabilities, enhancing their ability to\ncomprehend natural language and generate high-\nquality, context-based text. The evolution of large\nlanguage models has spurred the widespread use\nof auto-regressive language models as decoders\nin vision-language tasks. Utilizing cross-modal\ntransfer, this approach enables knowledge sharing\nbetween language and multimodal domains (Gao\net al., 2023; Liu et al., 2023; Li et al., 2023a; Ye\netal.,2023;Zhuetal.,2023;Lietal.,2023b;Zhang\net al., 2023).\n2.2 Model Editing\nLLMs (Zhao et al., 2023) primarily derive knowl-\nedge from the training corpus. Yet, the quality of\nthe dataset is not always guaranteed, potentially\nintegrating harmful or incorrect information into\nthemodel(Hernandezetal.,2023). Onesolutionis\nretraining models with updated knowledge, though\nthis might be unfordable and difficult to implement.\nAlternatively, fine-tuning with a few updated facts\ncould be considered, but it risks over-fitting and\ncatastrophic forgetting (Zhai et al., 2023). To ad-\ndress these issues, (Sinitsin et al., 2020) proposes\nModel Editing, which aims to efficiently and ac-\ncurately alter the factual knowledge stored within\nmodels. This approach is applied in various do-\nmains(Maoetal.,2023;Onoeetal.,2023;Xuetal.,\n2022b; Wang et al., 2023a; Li et al., 2023c; Cheng\net al., 2023), with an increasing number of studies\ninvestigating the impact of editing (Ilharco et al.,\n2023; Gupta et al., 2023; Hase et al., 2023a; Cohen\net al., 2023; Wu et al., 2023; Wang et al., 2023b;\nGandikota et al., 2023; Li et al., 2023d; Hase et al.,\n2023b). Presently, there are three primary types\nof model editing approaches: 1) Meta-learning\nMethod, 2) Locate-Then-Edit Method, and 3) In-\nContext Knowledge Editing Method.\nBefore Editing\nAfter EditingImageEncoderQ-formerLargeLanguageModel(LLM)â€¦What are shown in the photo? \nQueries\nAbirdis flying onthe sky.\nA kite is flying in the sky.\nin onthe sky\nVisionEdit LanguageEdit\nVision Area\n1. A bird is a warm-blooded, feathered animal with wings, a beak, and lays eggs.2. A kite is a toy or a lightweight object that is flown in the air.3. â€¦\nEditToolBox\non the sky\nLanguage Area\nğ‘”!\nğ‘”\"âˆ†â€¦Multimodal\tLanguage\tModel\nFigure 2: Utilizing multimodal LLM (e.g., BLIP-2 OPT) as an example, we dissect the comprehensive multimodal\nLLM into two components (Vision module and Textual module). The modelâ€™s erroneous output could potentially\nstem from either or both of these modules. Drawing an analogy with human errors in â€œvisionâ€ and â€œspeechâ€, we\napply model editing methods to these two components, thereby changing the model to refine its output.\nMeta-learning Method. MEND (Mitchell et al.,\n2022a) and Knowledge Editor (KE) (Cao et al.,\n2021) propose approaches involving an external\neditor, capable of learning the optimal parameter\nset, ğœƒ, for knowledge updating, while concurrently\nimposing constraints to maintain model stability.\nCaliNET (Dong et al., 2022) and T-Patcher (Huang\net al., 2023), drawing inspiration from (Dai et al.,\n2022), introduce additional trainable parameters\ninto the feed-forward module of Pretrained Lan-\nguage Models. SERAC (Mitchell et al., 2022b)\nutilize an explicit memory to store edits and learns\nto reason over them to modulate the base modelâ€™s\npredictions as needed.\nLocate-Then-Edit Method. ROME (Meng et al.,\n2022a) proposes approaches that employ causal\nmediation analysis to identify the area for editing.\nROME discovers that memorized factual associa-\ntionscanbepinpointedtoaspecificlocationwithin\na GPT model. However, a notable limitation of\nROME is its ability only to edit one fact at a time.\nToaddressthis,Mengetal.(2022b)proposesanew\nmethod known as MEMIT, which is a successor to\nthe previous work ROME, which performs a rank-\none modification of the MLP weights of a single\nlayer to write a memory into the model directly.\nIn-Context Knowledge Editing Method. In-\nContext Learning (ICL) (Brown et al., 2020) signi-\nfies a training-free paradigm where knowledge is\nobtainedfromdemonstrationsdirectlyconcatenated\nwithin the input context. A novel editing paradigm\nhas recently emerged that capitalizes on the abil-\nity of LLMs to comprehend context (Zheng et al.,\n2023),therebyenablingtheperformanceofcontext-\nbased model editing, guiding the modelâ€™s genera-\ntion process, and offering an efficient, lightweight\napproach to model editing.\nModel editing methods to date largely cater to\nsingle-modalscenarios,leavingagapinmultimodal\nediting. To the best of our knowledge, we are the\nfirst to investigate multimodal model editing for\nLLMs and provide a new benchmark to facilitate\nresearch in this area.\n3 Editing Multimodal LLMs\nWe illustrate the proposed task of multimodal edit-\ninginFigure2. Wewillintroducethetaskdefinition\n(Â§3.1), dataset construction details in (Â§3.2), the\nmultimodal models (Â§3.3), and the baselines (Â§3.4)\nwe used in the experiments.\n3.1 Task Definition\nAssumingwehaveamultimodalLLM ğ‘“ parameter-\nized byğœƒ(consisting of two parts,ğ‘“ğ‘£ğ‘–ğ‘ ğ‘–ğ‘œğ‘› and ğ‘“ğ‘¡ğ‘’ğ‘¥ğ‘¡\nparameterized byğœƒğ‘£ğ‘–ğ‘ ğ‘–ğ‘œğ‘› and ğœƒğ‘¡ğ‘’ğ‘¥ğ‘¡) that map the\ninputğ‘–ğ‘’andğ‘¥ğ‘’tothepredictionto ğ‘¦ğ‘œ,where ğ‘–ğ‘’refer\nto the editing image input,ğ‘¥ğ‘’ refer to the editing\ntextpromptinputand ğ‘¦ğ‘œdenoteastheoriginoutput.\nWedenote Masasymbolicrepresentationforapar-\nEdit target In-scope Out-of-scopeEdit scope\nEdit Language\nEdit Vision\nHow many skyscrapers are there?\nWhat is the number of \nhigh-rise buildings in the city?\nWhen did prison \nbreak season 4 come out?\nWhat is the number of \nhigh-rise buildings in the city?\nHow many \nskyscrapers are there?\nWhen did prison \nbreak season 4 come out?\nFigure 3: Taking the text modality as an example,Edit\ntargetand its generalization pertain toin-scope, which\ninvolves querying the quantity of skyscrapers in a given\nimage, while theout-of-scoperefers to inquiries about\nthe publication date. In-scope inputs require editing,\nwhereas out-of-scope inputs remain unchanged.\nticular metric, with subscripts indicating specific\nmetrics and superscripts representing variations in\nedit data. We prepare the editing datasets stated\nin Â§3.2.1, which present asDedit. Inspired by Yao\net al. (2023), we introduce a series of multimodal\nmodel editing metrics.\nReliability. Editingreliabilityisneededtochange\nprediction from ğ‘¦ğ‘œ to ğ‘¦ğ‘’. Intuitively, what we\nneed is an updatedğœƒğ‘’ with ğ‘“(ğ‘–ğ‘’,ğ‘¥ğ‘’; ğœƒğ‘’)= ğ‘¦ğ‘’. To\nmeasure the reliability, we use the editing accuracy,\nas described by the following:\nMğ‘Ÿğ‘’ğ‘™ = E(ğ‘–ğ‘’,ğ‘¥ğ‘’,ğ‘¦ğ‘’ )âˆ¼Dedit\n\u0002\n1ğ‘“(ğ‘–ğ‘’,ğ‘¥ğ‘’;ğœƒğ‘’ (ğ‘–ğ‘’,ğ‘¥ğ‘’,ğ‘¦ğ‘’ ))=ğ‘¦ğ‘’\n\u0003\n(1)\nwhere ğœƒğ‘’ refers to the edited parameters.\nLocality. To maintain the modelâ€™s stability, min-\nimizing the unintended side effects of editing on\nthe modelâ€™s broader knowledge base is imperative.\nIn pursuit of this objective, we introduce two met-\nrics: Mğ‘‡ğ‘’ğ‘¥ğ‘¡\nğ‘™ğ‘œğ‘ (T-Locality)and Mğ¼ğ‘šğ‘”\nğ‘™ğ‘œğ‘ (M-Locality),\nboth of which are designed to preserve the modelâ€™s\nstability during the editing process. Given that the\nknowledge in the multimodal language model is in-\nheritedfromLLMs,safeguardingthisknowledgeis\nparamount. With this aim in mind, we set aside the\nmodelâ€™s visual discrimination module and instead\nemploy rudimentary question-and-answer datasets\nDloc-t aswestatedinÂ§3.2.2. Wedefinethequestion\nas ğ‘¥and the answer asğ‘¦, as below:\nMğ‘‡ğ‘’ğ‘¥ğ‘¡\nğ‘™ğ‘œğ‘ = E (ğ‘–ğ‘’,ğ‘¥ğ‘’,ğ‘¦ğ‘’)âˆ¼Dedit\n(ğ‘¥,ğ‘¦ )âˆ¼Dloc-t\n\u0002\n1ğ‘“(ğ‘¥;ğœƒğ‘’ (ğ‘–ğ‘’,ğ‘¥ğ‘’,ğ‘¦ğ‘’ ))= ğ‘“(ğ‘¥,ğœƒ)\n\u0003\n(2)\nTASK Train Test L-Locality M-Locality\nE-VQA 6,346 2,093 4,289 5,046\nE-IC 2,849 1,000 4,289 5,046\nTable 1: The statistic of datasets for the E-VQA and\nE-IC sub-tasks. L-Locality and M-Locality are the test\nsets for knowledge locality to evaluate the rest of the\nknowledge in multimodal models when successfully\nupdating specific facts.\nThe vision encoder serves a critical function\nin the multimodal language model, transforming\nimages into vector representations for co-encoding\nalongside natural language text. Consequently, we\nmust take into account the potential ramifications\nof any modifications to this module. We construct\nthe dataset denoted asDloc-v for testMğ¼ğ‘šğ‘”\nğ‘™ğ‘œğ‘ , and\ncalculate as delineated below:\nMğ¼ğ‘šğ‘”\nğ‘™ğ‘œğ‘ = E(ğ‘–ğ‘£,ğ‘¥ğ‘£,ğ‘¦ğ‘£ )âˆ¼Dloc-v\n\u0002\n1ğ‘“(ğ‘–ğ‘£,ğ‘¥ğ‘£;ğœƒğ‘’ )= ğ‘“(ğ‘–ğ‘£,ğ‘¥ğ‘£;ğœƒ)\n\u0003\n(3)\nwhere (ğ‘–ğ‘£,ğ‘¥ğ‘£,ğ‘¦ğ‘£)is the out-of-scope data, and\nğœƒğ‘’ denote the parameter updated by edit data\n(ğ‘–ğ‘’,ğ‘¥ğ‘’,ğ‘¦ğ‘’).\nGenerality. Throughout the editing process, it\nis not adequate to merely amend individual erro-\nneous inputs. The revised model should also retain\nthe capacity for generalization and consistently\nproduce congruent outputs for equivalent inputs\n(e.g., rephrased sentences), as shown in Figure\n3. While previous unimodal model editing tasks\nonly required consideration of the rephrased text,\nmultimodal scenarios necessitate the generaliza-\ntion of images as well. To address this, we in-\ntroduce two generalization considerations:Mğ‘‡ğ‘’ğ‘¥ğ‘¡\nğ‘”ğ‘’ğ‘›\n(T-Generality) andMğ¼ğ‘šğ‘”\nğ‘”ğ‘’ğ‘› (M-Generality), which\nare expressed as follows:\nMğ‘‡ğ‘’ğ‘¥ğ‘¡\nğ‘”ğ‘’ğ‘› = E(ğ‘¥ğ‘Ÿ )âˆ¼N(ğ‘¥ğ‘’ )\n\u0002\n1ğ‘“(ğ‘–ğ‘’,ğ‘¥ğ‘Ÿ ;ğœƒğ‘’ )= ğ‘“(ğ‘–ğ‘’,ğ‘¥ğ‘’;ğœƒğ‘’ )\n\u0003\n(4)\nMğ¼ğ‘šğ‘”\nğ‘”ğ‘’ğ‘› = E(ğ‘–ğ‘Ÿ )âˆ¼N(ğ‘–ğ‘’ )\n\u0002\n1ğ‘“(ğ‘–ğ‘Ÿ ,ğ‘¥ğ‘’;ğœƒğ‘’ )= ğ‘“(ğ‘–ğ‘’,ğ‘¥ğ‘’;ğœƒğ‘’ )\n\u0003\n(5)\nwhere ğ‘–ğ‘Ÿ presents the rephrased image,ğ‘¥ğ‘Ÿ refers\nto the rephrased text prompt, andN(ğ‘¥)denotes to\nin-scope objects ofğ‘¥.\n3.2 Datasets\nThe datasetMMEditwe constructed mainly con-\ntains two subtasks: Editing VQA (E-VQA) and\nEditing Image Captioning (E-IC).\nQuestion:\"What are you looking for?\"\nDiverse Text Generation\nPrompt Creation:\"Please rewrite the following question in a different way. Please do not include the original questionâ€¦ What are you looking for?\"\nChatGLM:\"What is it that you seek?\"\nPrompt Optimization\nCaption:\"A wood table holding an assortment of cooking utensils.\"\nDiverse Image Generation\nPrompt Creation:\"A wood table holding anâ€¦ Positive prompt: masterpiece, best qualityâ€¦ Negative prompt: EasyNegative, disfiguredâ€¦\"\nStableDiffusion:\nPrompt Optimization\nFigure 4: Generality dataset construction process.\n3.2.1 Reliability Dataset Construction\nTo benchmark our experiments, we selected two\ncommon multimodal tasks: Visual Question An-\nswering (VQA) (Antol et al., 2015) and Image\nCaptioning (Herdade et al., 2019). VQA is to de-\nvise algorithms that can not only comprehend the\nvisual content within an image, but also understand\nthe natural language used to inquire about that\nimage, and subsequently generate precise answers\nto those queries. Image Captioning is to devise\nalgorithms capable of comprehending the visual\ncontent of an image, subsequently generating a\ncoherent and precise description of the image in\nnatural language. In this study, we opt for BLIP-2\nOPT. Our foundational edit data originates from\nsuboptimalentriesacrosstwoevaldatasets,namely,\nVQAv2 (Goyal et al., 2017) and COCO Caption\n(Chen et al., 2015).\nBesides the foundational edit data, utilizing ad-\nditional data is crucial. This data not only aids the\nediting process but also validates the efficacy of\nthechanges,assessingmodeleditsforbothstability\nand generality.\n3.2.2 Locality Dataset Construction\nWe must deliberate on the effects of editing on\nthe language function within a multimodal model,\nanalogous to how we evaluate various cognitive\nregions of an individualâ€™s brain post-surgery.\nTextual Locality Dataset. To evaluate the sta-\nbility of the language model, we leverage the NQ\ndataset (Kwiatkowski et al., 2019), previously used\nin MEND, as a benchmark for the stability of the\nLLMcomponentwithinthemodel. Wespecifically\nuse the modelâ€™s output pre and post-editing to con-\nstruct a KL scatter plot, facilitating constraints on\nthe modelâ€™s edits. Additionally, we calculate the\nproportion of instances maintaining a top-1 status,\nfurther quantifying the modelâ€™s stability.\nMultiModalLocalityDataset. Similarly,itâ€™scru-\ncial to verify the impact of editing on the visual\nmodule. Hence,weutilizeastraightforwarddataset\nOK-VQA (Marino et al., 2019) in the realm of\nmultimodality, serving as a measure of the locality\nfor the multimodal visual module. Once again, we\nupdate the KL dispersion constraint using logits\nboth before and after the editing process.\n3.2.3 Generality Dataset Construction\nWe propose two forms of generality within a mul-\ntimodal model. The overall process of generality\ndataset construction is shown in Figure 4.\nTextual Generality Dataset. To be noted, LLMs\nexhibitrobustconversationalandpowerfulproblem-\nsolving capabilities, which enables us to formulate\ntaskinstructions,wherebywecaninstructthemodel\nto produce analogous text inputs. For the E-VQA\ntask, we utilize ChatGLM (Du et al., 2022; Zeng\net al., 2022) to generate similar queries. However,\nfor the E-IC task, due to the succinctness and rela-\ntive straightforwardness of the prompts, the quality\nof the modelâ€™s generated output is not satisfactory.\nTherefore, we employ a manually written template\nwith 20 prompts to replace the original ones ran-\ndomly.\nVisual Generality Dataset. The diffusion model\n(Hoetal.,2020)hasgarneredsignificantsuccessin\nthe realm of image generation in recent years. Sur-\npassingtheoriginalstate-of-the-artmodel: Genera-\ntiveAdversarialNetworks(GAN)models(Goodfel-\nlow et al., 2014). The diffusion model has excelled\ninnumerousimage-generationtasksandhasshown\ncommendable performance across various applica-\ntion domains. Stable Diffusion (Rombach et al.,\n2022) is a latent text-to-image diffusion model ca-\npable of generating photo-realistic images given\ntext input. We utilize Stable Diffusion 2.1 for gen-\nerating reinterpreted images. This dataset, drawing\nupon caption descriptions from the COCO dataset,\nis leveraged to evaluate the modelâ€™s capability for\nimage generalization.\n3.3 Multimodal Language Models\nBLIP-2 OPT. BLIP-2 (Li et al., 2023b) is a\ngeneric and efficient pre-training strategy that boot-\nstraps vision-language pre-training from off-the-\nshelf frozen pre-trained image encoders and frozen\nlarge language models. The model utilizes a\nlightweight Quering Transformer to bridge the gap\nbetween vision modality and text modality and\nachieves state-of-the-art performance on various\nvision-languagetasks. WeselecttheBLIP-2OPTas\nourbasiceditmodel,whichutilizestheViT-Linthe\nvision block, and select the unsupervised-trained\nOPT model for decoder-based LLM.\nMiniGPT-4. MiniGPT-4 (Zhu et al., 2023) is a\npotentvision-languagemodelakintoBLIP-2,lever-\naging a frozen visual encoder in tandem with the\nfrozen Vicuna (Chiang et al., 2023). Vicuna, built\nupon LLaMA, is reported to achieve 90% of Chat-\nGPTâ€™s performance based on GPT-4â€™s evaluation\ncriteria. MiniGPT-4 adds a single projection layer\nto align the encoded visual features with the Vi-\ncuna language model. And MiniGPT-4 employs\nthe same pre-trained vision component of BLIP-2\nthat consists of a Vit-G/14 from EVA-CLIP (Sun\net al., 2023) and a Q-Former.\n3.4 Baselines\nFinetune. Fine-tuning has emerged as a widely\nemployedstrategyforadaptingpre-trainedlanguage\nmodels to specific tasks or domains (Cortes et al.,\n2015). Inourexploration,wedelveintotwodistinct\nfine-tuning methodologies: one focusing on the\nlast layerof the language model. Take the BLIP-2\nOPT model as an example, we finetune the 31st\ndecoder layer of the OPT model. The other targets\nthe vision blockwithin the multimodal language\nmodel,specifically,wefinetunetheQ-formermodel\nto overfit the editing dataset.\nMEND. Model Editor Networks with Gradient\nDecomposition (Mitchell et al., 2022a) conducts\nefficientlocaleditstolanguagemodelswithasingle\ninput-output pair. Essentially, MEND learns to\ntransform the gradient of fine-tuned LLMs, which\nutilizes a low-rank decomposition of gradients.\nKnowledge Editor. KE (Cao et al., 2021) is a\nmethod that can edit wrong knowledge in language\nmodels without re-training the whole model. KE\nutilizes a hyper network (a bidirectional-LSTM)\nwith constrained optimization, which is used to\npredict the weight update during inference.\nSERAC. SERAC (Mitchell et al., 2022b) intro-\nduces a memory-based model editing approach,\nwhichleveragesanexplicitmemorysystemtocache\nedits. This memory is subsequently used to adjust\nthe output of the base model during inference. The\nsystem utilizes a small auxiliaryscope classifier\nalongside counterfactual model. The role of the\nscope classifier is to ascertain whether the input is\nwithin the ambit of the memory cache. Should the\ninput be found within this scope, it is combined\nwiththemostrelevantcacheitemandinputintothe\ncounterfactual model for prediction.\nIn-Context Knowledge Editing. In-Context\nKnowledge Editing (IKE) (Zheng et al., 2023)\nconstructs ğ‘˜demonstrationsğ¶ = {ğ‘1,...,ğ‘ ğ‘˜}, fol-\nlowing the approach outlined in Liu et al. (2022).\nThis method employs an unsupervised retriever\nbased on cosine similarity to fetch demonstra-\ntions from the training set prior to injecting fact\nğ‘“ = (ğ‘¥âˆ—,ğ‘¦âˆ—)into Language Models. The ğ‘¥âˆ— is\nthe prompt to probe the factual knowledge in mod-\nels (e.g.,The president of the US is), and\nğ‘¦âˆ— will be the editing targetJoe Biden. The\nranking of in-context demonstrations also hinges\non cosine similarity:ğ‘ğ‘œğ‘ (ğ‘1, ğ‘“)< ğ‘ğ‘œğ‘ (ğ‘2, ğ‘“)<\nÂ·Â·Â· < ğ‘ğ‘œğ‘ (ğ‘ğ‘˜, ğ‘“). where ğ‘1,...,ğ‘ ğ‘˜ are sequen-\ntially arranged in the context from left to right.\nDemonstrationsğ¶can be viewed as an externally\naugmented knowledge base, primarily designed\nto guide the generation within LMs. Its ultimate\nobjective is to maximizeP(ğ‘¦ |ğ‘¥, ğ‘“,ğ¶)when the\npromptğ‘¥falls within the editing scope of the target\npromptğ‘¥âˆ—.\n4 Experiments\n4.1 Results\nIn this part, we present a comparative analysis of\nmultiple editing methods onMMEdit. The results\nof these comparisons are displayed in Table 2. Af-\nter this, we delve into a tripartite evaluation of the\nexperimental results, including three aspects ofRe-\nliability,Locality, andGenerality. Furthermore,\nwe analyze Locality and Generality through text\nand visual modalities and provide several editing\ncases in Figure 6.\nReliability. From the results, all model editing\nmethods outperform the base methods in Reliabil-\nEditing VQA Editing Image Caption\nMethod Reliability â†‘ T-Generalityâ†‘ T-Localityâ†‘ M-Localityâ†‘ Reliabilityâ†‘ T-Generalityâ†‘ T-Localityâ†‘ M-Localityâ†‘\nBLIP-2 OPT Size: 3.8B\nBase Methods\nBase Model 0.00 0.00 100.0 100.0 0.00 0.00 100.0 100.0\nFT (vision block) 60.98 49.79 100.0 8.47 18.94 5.86 100.0 8.40\nFT (last layer) 58.66 49.22 21.67 3.06 16.60 3.50 24.96 7.12\nModel Editing\nKnowledge Editor 80.00 77.40 93.79 66.43 5.60 4.40 95.00 64.32\nIn-Context Editing 99.95 91.59 13.16 1.88 96.70 78.20 13.36 2.17\nSERAC 99.40 99.40 100.0 1.33 99.70 99.68 100.0 6.84\nMEND 99.40 98.80 99.94 96.65 96.11 95.82 94.54 70.84\nMiniGPT-4 Size: 7.3B\nBase Methods\nBase Model 0.00 0.00 100.0 100.0 0.00 0.00 100.0 100.0\nFT (vision block) 36.3 0.3 100.0 9.29 3.10 0.00 100.0 8.56\nFT (last layer) 0.10 0.00 72.60 15.75 0.00 0.00 53.50 12.68\nModel Editing\nKnowledge Editor 91.80 89.00 96.91 67.83 34.40 29.20 97.30 64.36\nIn-Context Editing 100.0 94.89 13.46 3.67 90.90 81.60 14.23 4.68\nSERAC 87.70 87.60 100.0 14.22 91.74 91.43 100.0 4.56\nMEND 98.80 98.60 98.23 81.08 96.55 96.08 98.41 75.25\nTable 2: Main results on theMMEdit. T-Locality, M-Localityrefer to the textual and multimodal stability.\nT-Generalityrepresents textual generality.Reliabilitydenotes the accuracy of successful editing.\nity. Particularly, IKE and SERAC, methodologies\nleveraging external memory for editing, exhibit\ncommendableperformanceinmultimodallanguage\nmodels. We observe that the fine-tuning method\ndemonstrates poorer performance than the model\nediting method. Note that merely fine-tuning the\nparameters of the LLM or the modal fusion block\ndoes not adequately capture the characteristics of\nthe multimodal data. We analyze the reasons as\n0.00.20.40.60.81.0\nKE\nIKE\nSERAC\nMEND\nM-Generality\nM-Locality\n0.00.20.40.60.81.0\nKE\nIKE\nSERAC\nMEND\nL-Generality\nL-Locality\nFigure 5: Generality of different editing methods.\nfollows: the data used for fine-tuning differs sig-\nnificantly from the original model, such as the\nQ-former and OPT model, which need to collab-\norate effectively. Simply fine-tuning one of these\nmodules may not capture the task-specific charac-\nteristics accurately. On the other hand, fine-tuning\nall modules incurs a significant resource overhead.\nMoreover, based on our experimental results, we\nobserve that fine-tuning can lead to substantial\nchanges in the original model, often resulting in\nthe loss of other knowledge, particularly evident in\nmultimodal datasets.\nLocality. Several traditional editing methods re-\nmain applicable in multimodal editing, proving\nvaluable for effectively modifying the knowledge\nwithin the model and rectifying its outputs. How-\never,IKEandSERAC,despitetheirsuperiorperfor-\nmance in Reliability, exhibit poor performance on\nthe M-Locality due to their lack of constraints on\nit, indicating that although these external memory-\nbased editing techniques undoubtedly succeed in\nfixing the outputs, their efficacy in stabilizing inter-\nnal knowledge within the models leaves room for\nimprovement. As for T-Locality, the majority of\nModel Editing methods obtain good performance,\nwith IKE once again falling short. The underlying\nreason is that the other three approaches impose\nconstraints on T-Locality, whereas IKE, as an In-\nContext Learning method, lacks a robust constraint\nmechanism, resulting in subpar performance.\nGenerality. We undertake a comparative explo-\nration of various methodsâ€™ text and image gener-\nalization capabilities with MiniGPT-4 in E-VQA.\nNote that KE tends to display a lesser degree of\nimagegeneralization,predominantlyduetoitsinher-\nent consideration of M-Locality during the training\nphase. On the other hand, the superior image gen-\neralization capability exhibited by memory-based\nmethods is achieved at the cost of compromising\nM-Locality, resulting in significantly lower levels\nof M-Locality. Through our evaluation of diverse\nediting methods, we recurrently identify that image\ngeneralization performance tends to be less robust\nthan text generalization.\nBoarding.What is the man doing? \nskateboarding.What is the man doing? Skateboarding.\nBeforeEditing\nAfterEditing\nCase of successful VQA editing (By SERAC)\nA photo getting on a bus that has bicycles on the rack.What are shown in the photo? \nA person getting on a bus that has bicycles on the rack.\nWhat are shown in the photo? \nBeforeEditing\nAfterEditing\nCase of successful Image Caption editing (By SERAC)\nWhat is the train number? \n18688.\nBeforeEditing\nAfterEditing\nCase of failure VQA editing(By IKE)\nWhat is the train number? \n17788.\nFigure 6: Cases of multimodal model editing.Top: The output before editing.Bottom: The output after editing.\nMEND KE FT\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Reliability\nEdit LLM\nEdit Vision\nMEND KE FT\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0M-Generality\nEdit Text\nEdit Vision\nMEND KE FT\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0T-Generality\nEdit LLM\nEdit Vision\nMEND KE FT\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0M-Locality\nEdit LLM\nEdit Vision\nFigure 7: Results of editing different components.\n4.2 Editing Different Component\nWefurtheranalyzethevariationsineditingdifferent\nregions of the multimodal model. In contrast to\neditingsingle-modalmodels,duetothecomplexity\nand diversity of multimodal models, we can try to\nedit more modules and analyze their impact on vi-\nsual and textual knowledge. The results are shown\nin Figure 7. For the BLIP-2 OPT model, we inves-\ntigate the distinctions in editing the Q-former and\nOPT on the VQA dataset. Regarding the MiniGPT-\n4 model, we mainly focus on the distinctions in\nediting the last few layers of thellama_proj and\nVicuna models. The selected editing approaches\nfor analysis are MEND, KE, and FT, which enable\nus to specify the editing area.\nThe results highlight that editing the vision mod-\nule is more challenging than editing the language\nmodule (also see the failure editing in Figure 6).\nWearguethatthisdifficultymaybeattributedtothe\nmodelâ€™s architecture. Editing the last layer of the\nLLM allows for direct modification of the output,\nwhile modifying the vision module only affects\nthe input to the LLM, resulting in relatively less\nimpact on the model. Concretely, various modal-\nities reside in distinct spaces, which implies that\nthe factual knowledge may be stored in separate\nparameters within the model. Considering that the\nLLMs possess a large number of parameters, this\naspect becomes even more critical for multimodal\nmodels. Thus editing the language model can lead\nto significant performance improvements. Notably,\nthe visual module in the model plays a crucial role\nin image comprehension, thereby suggesting that\nfuture work needs toconsider information from\ndifferent modalities simultaneously.\n5 Conclusion\nIn this paper, we introduce multimodal model edit-\ning, with a new benchmarkMMEdit. Empirically,\nwe analyze the effectiveness of various model edit-\ning baselines and explore their impact on different\ncomponents (e.g., visual and text).\nAcknowledgment\nWe would like to express gratitude to the anony-\nmous reviewers for their kind comments. This\nwork was supported by the National Natural Sci-\nence Foundation of China (No.62206246), Zhe-\njiang Provincial Natural Science Foundation of\nChina (No. LGG22F030011), Ningbo Natural\nScience Foundation (2021J190), Yongjiang Talent\nIntroduction Programme (2021A-156-G), Zhejiang\nProvincial Science and Technology Plan Project\n(2023C01120), CCF-Tencent Rhino-Bird Open Re-\nsearch Fund, and Information Technology Center\nand State Key Lab of CAD&CG, Zhejiang Univer-\nsity.\n6 Limitations\nModels. We only edit several basic multimodal\nLLMs,leavingmanyothersbehind. Besides,dueto\nthe resource limitation, the number of parameters\nforthemultimodalLLMsweeditisbelow10B,and\nwecannotaffordtoeditLLMswithalargernumber\nof parameters such as the 65B LLaMA Adapter V2\n(Gao et al., 2023).\nEfficient Vision Editing. In this paper, our anal-\nysis has been primarily focused on comparing the\nvaried effects of existing editing methods across\nmodules of different modalities. However, the re-\nsults are not satisfactory. Moving forward, our\nprimary objective is to explore how to efficiently\nand accurately edit information across other modal-\nities. Thisincludesinvestigatingtechniquessuchas\nco-editingbetweendifferentmodalitiesbypinpoint-\ning the knowledge within the multimodal model\nandidentifyingthecontentrequiringmodification.\nReferences\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaretMitchell,DhruvBatra,C.LawrenceZitnick,and\nDevi Parikh. 2015. VQA: visual question answering.\nIn2015 IEEE International Conference on Computer\nVision, ICCV 2015, Santiago, Chile, December 7-13,\n2015, pages 2425â€“2433. IEEE Computer Society.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemensWinter,ChristopherHesse,MarkChen,Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems\n33: Annual Conference on Neural Information Pro-\ncessingSystems2020,NeurIPS2020,December6-12,\n2020, virtual.\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2021.\nEditing factual knowledge in language models. In\nEMNLP.\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-\nishna Vedantam, Saurabh Gupta, Piotr DollÃ¡r, and\nC. Lawrence Zitnick. 2015. Microsoft COCO cap-\ntions: Data collection and evaluation server.CoRR,\nabs/1504.00325.\nSiyuan Cheng, Ningyu Zhang, Bozhong Tian, Xi Chen,\nQingbing Liu, and Huajun Chen. 2023. Editing\nlanguage model-based knowledge graph embeddings.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nRoi Cohen, Eden Biran, Ori Yoran, Amir Globerson,\nand Mor Geva. 2023. Evaluating the ripple effects\nof knowledge editing in language models.CoRR,\nabs/2307.12976.\nCorinna Cortes, Neil D. Lawrence, Daniel D. Lee,\nMasashiSugiyama,andRomanGarnett,editors.2015.\nAdvances in Neural Information Processing Systems\n28: Annual Conference on Neural Information Pro-\ncessing Systems 2015, December 7-12, 2015, Mon-\ntreal, Quebec, Canada.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao\nChang, and Furu Wei. 2022. Knowledge neurons in\npretrained transformers. InACL.\nQingxiu Dong, Damai Dai, Yifan Song, Jingjing Xu,\nZhifang Sui, and Lei Li. 2022. Calibrating factual\nknowledgeinpretrainedlanguagemodels. In EMNLP,\nFindings of EMNLP.\nAlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Un-\nterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, and\nNeil Houlsby. 2021. An image is worth 16x16 words:\nTransformers for image recognition at scale. In9th\nInternational Conference on Learning Representa-\ntions, ICLR 2021, Virtual Event, Austria, May 3-7,\n2021. OpenReview.net.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:\nGeneral language model pretraining with autoregres-\nsiveblankinfilling. In Proceedingsofthe60thAnnual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 320â€“335.\nRohit Gandikota, Joanna Materzynska, Jaden Fiotto-\nKaufman, and David Bau. 2023. Erasing concepts\nfrom diffusion models.CoRR, abs/2303.07345.\nPeng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, ShÄ³ie\nGeng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui\nHe, Xiangyu Yue, Hongsheng Li, and Yu Qiao. 2023.\nLlama-adapter V2: parameter-efficient visual instruc-\ntion model.CoRR, abs/2304.15010.\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBingXu,DavidWarde-Farley,SherjilOzair,AaronC.\nCourville, and Yoshua Bengio. 2014. Generative\nadversarial networks.CoRR, abs/1406.2661.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2017. Making the V in VQA\nmatter: Elevating the role of image understanding in\nvisual question answering. In2017 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR\n2017, Honolulu, HI, USA, July 21-26, 2017, pages\n6325â€“6334. IEEE Computer Society.\nAnshita Gupta, Debanjan Mondal, Akshay Krishna\nSheshadri, Wenlong Zhao, Xiang Lorraine Li, Sarah\nWiegreffe,andNiketTandon.2023. Editingcommon-\nsense knowledge in GPT.CoRR, abs/2305.14956.\nXiaoqi Han, Ru Li, Xiaoli Li, and Jeff Z. Pan. 2023. A\ndivide and conquer framework for knowledge editing.\nKnowledge-Based Systems, 279:110826.\nThomas Hartvigsen, Swami Sankaranarayanan, Hamid\nPalangi, Yoon Kim, and Marzyeh Ghassemi. 2022.\nAging with GRACE: lifelong model editing with\ndiscrete key-value adaptors.CoRR, abs/2211.11031.\nPeter Hase, Mohit Bansal, Been Kim, and Asma Ghan-\ndeharioun. 2023a. Does localization inform editing?\nsurprising differences in causality-based localization\nvs. knowledge editing in language models.CoRR,\nabs/2301.04213.\nPeter Hase, Mohit Bansal, Been Kim, and Asma Ghan-\ndeharioun. 2023b. Does localization inform editing?\nsurprising differences in causality-based localization\nvs. knowledge editing in language models.\nSimao Herdade, Armin Kappeler, Kofi Boakye, and\nJoao Soares. 2019. Image captioning: Transforming\nobjectsintowords. In AdvancesinNeuralInformation\nProcessingSystems32: AnnualConferenceonNeural\nInformation Processing Systems 2019, NeurIPS 2019,\nDecember8-14,2019,Vancouver,BC,Canada ,pages\n11135â€“11145.\nEvan Hernandez, Belinda Z. Li, and Jacob Andreas.\n2023. Inspecting and editing knowledge representa-\ntions in language models.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. De-\nnoising diffusion probabilistic models. InAdvances\nin Neural Information Processing Systems 33: An-\nnual Conference on Neural Information Processing\nSystems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual.\nZeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou,\nWenge Rong, and Zhang Xiong. 2023. Transformer-\npatcher: One mistake worth one neuron. CoRR,\nabs/2301.09785.\nGabriel Ilharco, Marco Tulio Ribeiro, Mitchell Worts-\nman, Ludwig Schmidt, Hannaneh Hajishirzi, and\nAli Farhadi. 2023. Editing models with task arith-\nmetic. InThe Eleventh International Conference on\nLearning Representations.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield,MichaelCollins,AnkurP.Parikh,ChrisAlberti,\nDanielleEpstein,IlliaPolosukhin,JacobDevlin,Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: a benchmark for question answering\nresearch. Trans. Assoc. Comput. Linguistics, 7:452â€“\n466.\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu. 2023a. Otter: A\nmulti-modalmodelwithin-contextinstructiontuning.\nCoRR, abs/2305.03726.\nJunnanLi, DongxuLi, SilvioSavarese, andStevenC.H.\nHoi. 2023b. BLIP-2: bootstrapping language-image\npre-training with frozen image encoders and large\nlanguage models.CoRR, abs/2301.12597.\nXiaopengLi,ShashaLi,ShezhengSong,JingYang,Jun\nMa, and Jie Yu. 2023c. Pmet: Precise model editing\nin a transformer.\nZhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang,\nXi Chen, and Huajun Chen. 2023d. Unveiling the pit-\nfalls of knowledge editing for large language models.\narXiv preprint arXiv:2310.02129.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023. Visual instruction tuning. CoRR,\nabs/2304.08485.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022. What\nmakes good in-context examples for GPT-3? In\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extraction\nand Integration for Deep Learning Architectures.\nAssociation for Computational Linguistics.\nShengyu Mao, Ningyu Zhang, Xiaohan Wang, Mengru\nWang, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei\nHuang, and Huajun Chen. 2023. Editing personality\nfor llms.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\nand Roozbeh Mottaghi. 2019. OK-VQA: A visual\nquestion answering benchmark requiring external\nknowledge. InIEEE Conference on Computer Vision\nand Pattern Recognition, CVPR 2019, Long Beach,\nCA, USA, June 16-20, 2019, pages 3195â€“3204. Com-\nputer Vision Foundation / IEEE.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022a. Locating and editing factual knowl-\nedge in GPT. InNeurIPS.\nKevin Meng, Arnab Sen Sharma, Alex Ando-\nnian, Yonatan Belinkov, and David Bau. 2022b.\nMass-editing memory in a transformer. CoRR,\nabs/2210.07229.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea\nFinn,andChristopherD.Manning.2022a. Fastmodel\nediting at scale. InICLR.\nEric Mitchell, Charles Lin, Antoine Bosselut, Christo-\npherD.Manning,andChelseaFinn.2022b. Memory-\nbased model editing at scale. InInternational Con-\nference on Machine Learning, ICML 2022, 17-23\nJuly 2022, Baltimore, Maryland, USA, volume 162\nofProceedingsofMachineLearningResearch ,pages\n15817â€“15831. PMLR.\nYasumasaOnoe,MichaelJ.Q.Zhang,ShankarPadman-\nabhan, Greg Durrett, and Eunsol Choi. 2023. Can\nlms learn new entities from descriptions? challenges\nin propagating injected knowledge. InProceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nACL 2023, Toronto, Canada, July 9-14, 2023, pages\n5469â€“5485. Association for Computational Linguis-\ntics.\nOpenAI. 2022. The blog used to introduce chatgpt.\nhttps://openai.com/blog/chatgpt.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchenKrueger,andIlyaSutskever.2021. Learning\ntransferable visual models from natural language\nsupervision. InProceedingsofthe38thInternational\nConferenceonMachineLearning,ICML2021,18-24\nJuly 2021, Virtual Event, volume 139 ofProceedings\nof Machine Learning Research, pages 8748â€“8763.\nPMLR.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and BjÃ¶rn Ommer. 2022. High-\nresolution image synthesis with latent diffusion mod-\nels. InIEEE/CVF Conference on Computer Vision\nand Pattern Recognition, CVPR 2022, New Orleans,\nLA, USA, June 18-24, 2022, pages 10674â€“10685.\nIEEE.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ilic, Daniel Hesslow, Roman\nCastagnÃ©, Alexandra Sasha Luccioni, FranÃ§ois Yvon,\nMatthias GallÃ©, Jonathan Tow, Alexander M. Rush,\nStellaBiderman,AlbertWebson,PawanSasankaAm-\nmanamanchi, Thomas Wang, BenoÃ®t Sagot, Niklas\nMuennighoff, Albert Villanova del Moral, Olatunji\nRuwase, Rachel Bawden, Stas Bekman, Angelina\nMcMillan-Major, Iz Beltagy, Huu Nguyen, Lucile\nSaulnier, Samson Tan, Pedro Ortiz Suarez, Victor\nSanh,HugoLaurenÃ§on,YacineJernite,JulienLaunay,\nMargaretMitchell,ColinRaffel,AaronGokaslan,Adi\nSimhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy,\nAnna Rogers, Ariel Kreisberg Nitzav, Canwen Xu,\nChenghao Mou, Chris Emezue, Christopher Klamm,\nColin Leong, Daniel van Strien, David Ifeoluwa Ade-\nlani, and et al. 2022. BLOOM: A 176b-parameter\nopen-access multilingual language model. CoRR,\nabs/2211.05100.\nAnton Sinitsin, Vsevolod Plokhotnyuk, Dmitry V.\nPyrkin, Sergei Popov, and Artem Babenko. 2020.\nEditable neural networks. InICLR.\nQuan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang,\nand Yue Cao. 2023. EVA-CLIP: improved training\ntechniques for CLIP at scale.CoRR, abs/2303.15389.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, TimothÃ©e Lacroix,\nBaptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal\nAzhar, AurÃ©lien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.CoRR,\nabs/2302.13971.\nJiaan Wang, Yunlong Liang, Zengkui Sun, Yuxuan Cao,\nand Jiarong Xu. 2023a. Cross-lingual knowledge\nediting in large language models.\nPeng Wang, Ningyu Zhang, Xin Xie, Yunzhi Yao,\nBozhong Tian, Mengru Wang, Zekun Xi, Siyuan\nCheng, Kangwei Liu, Guozhou Zheng, and Huajun\nChen. 2023b. Easyedit: An easy-to-use knowledge\nediting framework for large language models.CoRR,\nabs/2308.07269.\nSuhang Wu, Minlong Peng, Yue Chen, Jinsong Su, and\nMingming Sun. 2023. Eva-kellm: A new benchmark\nfor evaluating knowledge editing of llms.CoRR,\nabs/2308.09954.\nPeng Xu, Xiatian Zhu, and David A. Clifton. 2022a.\nMultimodal learning with transformers: A survey.\nCoRR, abs/2206.06488.\nYang Xu, Yutai Hou, and Wanxiang Che. 2022b. Lan-\nguageanisotropiccross-lingualmodelediting. ArXiv,\nabs/2205.12677.\nYunzhi Yao, Peng Wang, Bozhong Tian, Siyuan\nCheng, Zhoubo Li, Shumin Deng, Huajun Chen, and\nNingyu Zhang. 2023. Editing large language mod-\nels: Problems, methods, and opportunities.CoRR,\nabs/2305.13172.\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming\nYan, Yiyang Zhou, Junyang Wang, Anwen Hu,\nPengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong\nXu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang,\nand Fei Huang. 2023. mplug-owl: Modularization\nempowers large language models with multimodality.\nCoRR, abs/2304.14178.\nShukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing\nSun, Tong Xu, and Enhong Chen. 2023. A sur-\nvey on multimodal large language models.CoRR,\nabs/2306.13549.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An\nopen bilingual pre-trained model. arXiv preprint\narXiv:2210.02414.\nYuexiangZhai,ShengbangTong,XiaoLi,MuCai,Qing\nQu,YongJaeLee,andYiMa.2023. Investigatingthe\ncatastrophic forgetting in multimodal large language\nmodels. CoRR, abs/2309.10313.\nAo Zhang, Hao Fei, Yuan Yao, Wei Ji, Li Li, Zhiyuan\nLiu,andTat-SengChua.2023. Transfervisualprompt\ngenerator across llms.CoRR, abs/2305.01278.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Be-\nichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,\nChen Yang, Yushuo Chen, Zhipeng Chen, Jinhao\nJiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang\nLiu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.\n2023. A survey of large language models.CoRR,\nabs/2303.18223.\nCe Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong\nWu, Jingjing Xu, and Baobao Chang. 2023. Can we\neditfactualknowledgebyin-contextlearning? CoRR,\nabs/2305.12740.\nZexuan Zhong, Zhengxuan Wu, Christopher D. Man-\nning, Christopher Potts, and Danqi Chen. 2023.\nMquake: Assessing knowledge editing in lan-\nguage models via multi-hop questions. CoRR,\nabs/2305.14795.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny. 2023. Minigpt-4: Enhancing\nvision-language understanding with advanced large\nlanguage models.CoRR, abs/2304.10592.\nA Appendix\nOur code is available in the supplementary ma-\nterials for reproducibility.\nHyper-ParametersMaxIterEdit NumOptimizerLR\nğ·ğ¸âˆ’ğ‘‰ğ‘„ğ´\nğµğ¿ğ¼ğ‘ƒ2 40000 1 ASGD 1e-5\nğ·ğ¸âˆ’ğ¼ğ¶ğµğ¿ğ¼ğ‘ƒ2 40000 1 ASGD 1e-5\nğ·ğ¸âˆ’ğ‘‰ğ‘„ğ´\nğ‘€ğ‘–ğ‘›ğ‘–ğºğ‘ƒğ‘‡âˆ’4 40000 1 ASGD 1e-5\nğ·ğ¸âˆ’ğ¼ğ¶ğ‘€ğ‘–ğ‘›ğ‘–ğºğ‘ƒğ‘‡âˆ’4 40000 1 ASGD 1e-5\nTable 3: FT-vision hyper-parameters\nHyper-ParametersMaxIterEdit NumOptimizerLR\nğ·ğ¸âˆ’ğ‘‰ğ‘„ğ´\nğµğ¿ğ¼ğ‘ƒ2 20000 1 ASGD 1e-5\nğ·ğ¸âˆ’ğ¼ğ¶ğµğ¿ğ¼ğ‘ƒ2 20000 1 ASGD 1e-5\nğ·ğ¸âˆ’ğ‘‰ğ‘„ğ´\nğ‘€ğ‘–ğ‘›ğ‘–ğºğ‘ƒğ‘‡âˆ’4 20000 1 ASGD 1e-5\nğ·ğ¸âˆ’ğ¼ğ¶ğ‘€ğ‘–ğ‘›ğ‘–ğºğ‘ƒğ‘‡âˆ’4 20000 1 ASGD 1e-5\nTable 4: FT-last-layer hyper-parameters\nIn this section, we describe the implementation\nof our experiments in detail, including the training\nprocedures, backbone model, and hyperparameters\nfor each dataset.\nHyper-ParametersMaxIterEdit NumOptimizerLR\nğ·ğ¸âˆ’ğ‘‰ğ‘„ğ´\nğµğ¿ğ¼ğ‘ƒ2 20,000 1 Adam 1e-5\nğ·ğ¸âˆ’ğ¼ğ¶ğµğ¿ğ¼ğ‘ƒ2 20,000 1 Adam 1e-5\nğ·ğ¸âˆ’ğ‘‰ğ‘„ğ´\nğ‘€ğ‘–ğ‘›ğ‘–ğºğ‘ƒğ‘‡âˆ’4 25,000 1 AdamW 5e-4\nğ·ğ¸âˆ’ğ¼ğ¶ğ‘€ğ‘–ğ‘›ğ‘–ğºğ‘ƒğ‘‡âˆ’4 35,000 1 AdamW 5e-4\nTable 5: KE hyper-parameters\nHyper-ParametersMaxIterEdit NumOptimizerLR\nğ·ğ¸âˆ’ğ‘‰ğ‘„ğ´\nğµğ¿ğ¼ğ‘ƒ2 15,000 1 Adam 1e-5\nğ·ğ¸âˆ’ğ¼ğ¶ğµğ¿ğ¼ğ‘ƒ2 15,000 1 Adam 1e-5\nğ·ğ¸âˆ’ğ‘‰ğ‘„ğ´\nğ‘€ğ‘–ğ‘›ğ‘–ğºğ‘ƒğ‘‡âˆ’4 20,000 1 Adam 1e-5\nğ·ğ¸âˆ’ğ¼ğ¶ğ‘€ğ‘–ğ‘›ğ‘–ğºğ‘ƒğ‘‡âˆ’4 30,000 1 Adam 1e-5\nTable 6: SERAC hyper-parameters\nHyper-ParametersMaxIterEdit NumOptimizerLR\nğ·ğ¸âˆ’ğ‘‰ğ‘„ğ´\nğµğ¿ğ¼ğ‘ƒ2 20,000 1 Adam 1e-6\nğ·ğ¸âˆ’ğ¼ğ¶ğµğ¿ğ¼ğ‘ƒ2 20,000 1 Adam 1e-6\nğ·ğ¸âˆ’ğ‘‰ğ‘„ğ´\nğ‘€ğ‘–ğ‘›ğ‘–ğºğ‘ƒğ‘‡âˆ’4 20,000 1 Adam 1e-6\nğ·ğ¸âˆ’ğ¼ğ¶ğ‘€ğ‘–ğ‘›ğ‘–ğºğ‘ƒğ‘‡âˆ’4 20,000 1 Adam 1e-6\nTable 7: MEND hyper-parameters"
}