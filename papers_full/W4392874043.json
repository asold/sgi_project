{
  "title": "Welcome Your New AI Teammate: On Safety Analysis by Leashing Large Language Models",
  "url": "https://openalex.org/W4392874043",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2107621137",
      "name": "Ali Nouri",
      "affiliations": [
        "Volvo (Sweden)",
        "Volvo Cars (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2766693095",
      "name": "Beatriz Cabrero-Daniel",
      "affiliations": [
        "University of Gothenburg"
      ]
    },
    {
      "id": "https://openalex.org/A2137857975",
      "name": "Fredrik Törner",
      "affiliations": [
        "Volvo (Sweden)",
        "Volvo Cars (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A1060775355",
      "name": "Håkan Sivencrona",
      "affiliations": [
        "Zenuity (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2022892582",
      "name": "Christian Berger",
      "affiliations": [
        "University of Gothenburg"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4390482372",
    "https://openalex.org/W4320068749",
    "https://openalex.org/W4379598302",
    "https://openalex.org/W6800751262",
    "https://openalex.org/W4321649710",
    "https://openalex.org/W4323927216",
    "https://openalex.org/W4362655281",
    "https://openalex.org/W4386712343",
    "https://openalex.org/W2136451344",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W49973284"
  ],
  "abstract": "DevOps is a necessity in many industries, including the development of\\nAutonomous Vehicles. In those settings, there are iterative activities that\\nreduce the speed of SafetyOps cycles. One of these activities is \"Hazard\\nAnalysis &amp; Risk Assessment\" (HARA), which is an essential step to start the\\nsafety requirements specification. As a potential approach to increase the\\nspeed of this step in SafetyOps, we have delved into the capabilities of Large\\nLanguage Models (LLMs).\\n Our objective is to systematically assess their potential for application in\\nthe field of safety engineering. To that end, we propose a framework to support\\na higher degree of automation of HARA with LLMs. Despite our endeavors to\\nautomate as much of the process as possible, expert review remains crucial to\\nensure the validity and correctness of the analysis results, with necessary\\nmodifications made accordingly.\\n",
  "full_text": "Welcome Your New AI Teammate:\nOn Safety Analysis by Leashing Large Language\nModels\nAli Nouri\nVolvo Cars &\nChalmers University of Technology\nGothenburg, Sweden\nali.nouri@volvocars.com\nBeatriz Cabrero-Daniel\nUniversity of Gothenburg,\nDepartment of Computer Science\nGothenburg, Sweden\nbeatriz.cabrero-daniel@gu.se\nFredrik Törner\nVolvo Cars,\nGothenburg, Sweden\nfredrik.torner@volvocars.com\nHåkan Sivencrona\nZenseact\nGothenburg, Sweden\nhakan.sivencrona@zenseact.com\nChristian Berger\nUniversity of Gothenburg,\nDepartment of Computer Science\nGothenburg, Sweden\nchristian.berger@gu.se\nAbstract\nDevOps is a necessity in many industries, including the de-\nvelopment of Autonomous Vehicles. In those settings, there\nare iterative activities that reduce the speed of SafetyOps\ncycles. One of these activities is “Hazard Analysis & Risk\nAssessment” (HARA), which is an essential step to start the\nsafety requirements specification. As a potential approach to\nincrease the speed of this step in SafetyOps, we have delved\ninto the capabilities of Large Language Models (LLMs). Our\nobjective is to systematically assess their potential for ap-\nplication in the field of safety engineering. To that end, we\npropose a framework to support a higher degree of automa-\ntion of HARA with LLMs. Despite our endeavors to automate\nas much of the process as possible, expert review remains\ncrucial to ensure the validity and correctness of the analysis\nresults, with necessary modifications made accordingly.\nKeywords: Hazard Analysis Risk Assessment, Autonomous\nVehicles, DevOps, Safety, Large Language Model, Prompt\nEngineering, LLM, ChatGPT\nACM Reference Format:\nAli Nouri, Beatriz Cabrero-Daniel, Fredrik Törner, Håkan Siven-\ncrona, and Christian Berger. 2024. Welcome Your New AI Team-\nmate: On Safety Analysis by Leashing Large Language Models .\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for profit or commercial advantage and that\ncopies bear this notice and the full citation on the first page. Copyrights\nfor components of this work owned by others than the author(s) must\nbe honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific\npermission and/or a fee. Request permissions from permissions@acm.org.\nCAIN 2024, April 14–15, 2024, Lisbon, Portugal\n© 2024 Copyright held by the owner/author(s). Publication rights licensed\nto ACM.\nACM ISBN 979-8-4007-0591-5/24/04. . . $15.00\nhttps://doi.org/10.1145/3644815.3644953\nIn Conference on AI Engineering Software Engineering for AI (CAIN\n2024), April 14–15, 2024, Lisbon, Portugal. ACM, New York, NY, USA,\n6 pages. https://doi.org/10.1145/3644815.3644953\n1 Introduction\nThe safety analysis of Autonomous Driving (AD) functions\nis crucial for engineers to identify hazardous events, assess\ntheir risks, and determine their root causes. Ensuring the\nsafety of such functions often relies on iterative natural lan-\nguage (NL)-based activities, one of which is safety analysis.\nArtificial Intelligence (AI)-based tools capable of processing\nNL can be used to increase the efficiency and speed of these\nactivities. One of the most promising tools is Large Language\nModel (LLM).\nSafety analysis is, however, not trivial as it consists of vari-\nous activities such as identification of failure modes, and their\neffect in specific situations, which aim to mitigate or avoid\nunreasonable risks. Standards such as ISO 26262 [1] or ISO\n21448 [2], and regulations like UNECE R157 (ALKS) [3] often\npropose or mandate activities such as Hazard Analysis Risk\nAssessment (HARA) and System Theoretic Process Analysis\n(STPA) [4]. These guides are used when specifying the safety\nrequirements for mitigation or avoidance strategies.\nHARA is a well-known and usually required safety anal-\nysis for automotive functions, such as AD. The aim of this\nactivity is to identify the hazardous events, categorize them,\nand to specify safety goals to prevent or mitigate them. Safety\ngoals are top-level (i.e., vehicle-level) safety requirements\nthat are then used in other safety activities. Together, these\nactivities and intermediate results shed light on the safety\nof a function and the potential impact of a failure.\nSpecific Operational Design Domains (ODD) are a starting\npoint, where the AD function will be constrained to. The\nODD begins to gradually expand once pieces of evidence\nare gathered and safety argumentation is implemented. In\norder to continuously update and improve the AD function\narXiv:2403.09565v1  [cs.SE]  14 Mar 2024\nCAIN 2024, April 14–15, 2024, Lisbon, Portugal Nouri et al.\nand expand its ODD, the implementation of DevOps [5] is\ncommonplace. Moreover, due to unknown hazardous sce-\nnarios [2], DevOps serves as an approach to rapidly address\ntheir root causes as soon as they are identified. As it requires\nchanges in function, system, and software, the correspond-\ning safety activities shall also be addressed. This leads to\nthe introduction of SafetyOps [ 6], representing the safety\ndimension within the DevOps framework. Depending on\nthe updates in each iteration, SafetyOps may require modifi-\ncations or the redoing of certain safety activities, including\nHARA.\nHowever, the time needed for safety analysis and require-\nments engineering poses a significant challenge in achiev-\ning rapid SafetyOps [ 7]. This reinforces the need for new\nAI-based approaches that increase speed, reduce costs, and\nenable quicker responses to new events and incidents in the\nfield. Rule-based approaches have significantly contributed\nto repetitive and well-established tasks, such as testing new\nsoftware releases against legacy test cases. However, there\nare intellectual aspects in HARA such as identifying the\nconsequences of a hazard in a specific scenario or specify-\ning requirements, which cannot be automated by conven-\ntional tools. As mentioned before, LLMs are known for their\nstrength in NL-based tasks, from academic writing [ 8] to\nmedical education [9]. LLMs could therefore be considered a\npotential approach to address the limitations of conventional\ntools in automating safety analysis.\nCommunication with LLMs requires prompt engineering,\nwhose domain-dependencies such as for supporting safety-\nrelated activities are currently under-explored [ 8]. To the\nbest of the authors’ knowledge, no comprehensive and vali-\ndated guides exist so far to guide practitioners in integrating\nLLMs into their safety analysis activities. Additionally, the\nuse of these tools could soon be regulated by international\nregulations such as the AI Act [10]. Therefore, especially in\nsafety-critical systems, the use of such tools must be thor-\noughly reported and addressed, with attention to confidence\nin the software tool process.\n1.1 Background\nItem definition is the main input into HARA and includes\na high-level description of the function and its interaction\nwith the driver and environment. Hazard identification is\nthe first step in HARA. Then they are analyzed in various\nrelevant scenarios to identify the consequences of hazardous\nevents. If a hazardous event leads to any harm, the severity\nof this harm is classified with a value ranging from S0 to S3.\nFinally, there arises a need to specify a safety goal once the\nhazardous event is assigned an Automotive Safety Integrity\nLevel (ASIL) ranging from A to D.\n1.2 Research Goal & Research Questions\nThe goal of this research is (i) to identify effective prompt\npatterns, and (ii) design a pipeline of prompts resulting in an\nLLM-based tool for performing HARA. Achieving this would\nenable the creation of an initial ‘version zero’ of HARA. This\nversion can then be expanded, modified, or used for brain-\nstorming by development engineers, thereby accelerating\nthe development of the valid version of HARA. This led to\nthe following research question:\n“What are the steps in a prompting pipeline to propagate\nthe context needed to generate a HARA?”\nIn this paper, we present the designed pipeline of sub-tasks\n(i.e., prompts), serving as a guide for communicating with\nLLMs to achieve desired outcomes in analysis tasks such as\nHARA, and the lessons learnt during the design process.\nThe rest of the paper is organised as follows: Sec. 2 presents\nrelated research. Sec. 4 addresses our research question by\ndiscussing the pipeline of prompts and prompt engineer-\ning. Finally, Sec. 5 reports on threats to validity and Sec. 6\nsummarizes the findings and potential future studies.\n2 Related Work\nLLMs are trained on a wide range of data across various\ndomains and contexts, enabling them to generate text that\nis judged to be relevant for various tasks[ 11]. As prompt\nengineering plays an important role, there are some studies\nthat propose specific prompt patterns such as [ 12], which\nare discussed in detail in Sec. 4.1.\nHowever, some publications have reported the weaknesses\nof LLM such as hallucination [13], a phenomenon where the\nLLM generates text that does not align with reality. This\nhighlights the importance of identifying these weaknesses\nand their risks in each specific application.\nTo the best of the authors’ knowledge, there is no system-\natic or peer-reviewed study that has assessed the capability\nof LLMs in performing safety analysis, specifically focusing\non hazard analysis and risk assessment. While there have\nbeen studies on using LLMs in STPA for Autonomous Emer-\ngency Braking (AEB) [14], and Hazard Analysis for a water\ntank system [15], these studies do not focus on the LLM’s\ncapability to perform the analysis autonomously; instead,\nthey use it as a text generation assistant. In both studies, the\nuser continuously guided the LLM to perform the analysis\nand subsequently assessed the quality of the output. This\nposes a threat to the validity of the study, as guiding the LLM\ncan eventually lead to generating desirable text. The second\nthreat to validity arises from using mature functions, which\nthe model has most likely encountered during training.\n3 Experiment Design\nIn this study, we iteratively designed a pipeline for LLM-\nbased HARA that comprises a set of carefully engineered\nprompts and their communication. We followed the recom-\nmendations of Hevner et al.[16] and Roel J. Wieringa [17]\nin this study. The study is designed to be model agnostic,\nWelcome Your New AI Teammate: On Safety Analysis by Leashing Large Language Models CAIN 2024, April 14–15, 2024, Lisbon, Portugal\nalthough the experiment was conducted using GPT-4.0, de-\nveloped by OpenAI, accessed through an API.\nDuring the design cycles, the first author investigated\nthe problem. Subsequently, the prompts and their pipeline\nwere designed. The results were validated through internal\ndiscussions and review meetings with the other researchers\nin the team. During the initial phases, the study primarily\nfocused on a feasibility study, as the researchers in the team,\nwho had previously performed HARA, were aware of the\ntask’s difficulty and the intellectual capability required for it\nwhich was not expected from LLM.\nFor this experiment, we selected the fully unsupervised\n“collision avoidance by evasive maneuver” function (CAEM).\nThe publicly available documentation for this novel function\nis less extensive than that for more mature functions like\nAEB, thereby reducing the likelihood that its HARA results\nare present in the training data of the LLM model under test.\nWe began by performing HARA for a CAEM function\nand gradually investigated the causes of issues, clarifying\nthe question more thoroughly, which led to better results.\nAs a result, the detail of each task increased to the point\nwhere decomposition of the steps was necessary. Initially,\nwe provided the scenarios as is common in industry. Once\nthe model’s capability was demonstrated, we advanced a\nstep further and requested the scenario catalogue as well,\nproviding only the guidelines for creating scenarios.\nAs it is shown in Fig. 1, the LLM-based HARA is designed\nto be automated without an engineer’s intervention, where\nthe input is the item definition (i.e., the function descrip-\ntion), and the output is the HARA results. The HARA for\nthe full function can be completed in less than a day due to\nautomation, but it still requires review by human engineers.\n4 LLM-Based HARA\nDescribing a task to an LLM requires specific skills, known as\nprompt engineering. The crucial skills for our application are\ndetailed in Sec. 4.1. Performing HARA necessitates diverse\nexpertise for various subtasks, including scenario generation,\nidentifying the consequences of hazards and their severity,\nas well as requirements engineering. Furthermore, each ac-\ntivity requires an extensive process description, which, in\nmost cases, is potentially not publicly available and, as a\nconsequence, is not included in the training data of LLMs.\nMoreover, depending on the complexity of the function and\nits environment, this process could result in a large number\nof hazardous events and safety requirements. Considering\nthe token limitation constraint in LLMs, asking to perform\nHARA in the same prompt can lead to incomplete results.\nSo there is a need to break down HARA into subtasks and\nto design a pipeline to decompose and then integrate the\nresults as explained in Sec. 4.2 and presented in Fig. 1.\nWe aimed to design the prompts and the pipeline to receive\nthe function description of any automotive function and to\nprovide the HARA results without human intervention as\nshown in Fig. 1. In the design of the prompts and pipeline,\nwe considered the following restrictions (R1 to R4):\nR1: No input except the function description: Al-\nthough scenario descriptions and malfunctions can often\nbe extracted from company catalogs, this is not always the\ncase, and it can significantly affect the quality of HARA.\nTherefore, we designed the pipeline to be as independent\nas possible, limiting inputs solely to the item definition. For\ninstance, we chose not to request scenarios as input, enabling\nthe system to autonomously generate relevant scenarios.\nR2: No human intervention while HARA is on progress:\nThe analysis should be performed fully automated, without\nany intervention, allowing researchers or experts to review\nonly the final results. As the system’s capability to produce\nthe desired output might be compromised by human feed-\nback or alterations to the intermediate data.\nR3: No fine-tuning for specific function: The prompts\nshould only include the process to perform the analysis and\nnot be tuned for any specific item definition, ensuring us-\nability for any automotive safety-critical functions.\nR4: Automotive acceptable format: The results of the\nanalysis shall be presented in a format that is easily readable\nfor reviewers. For instance, HARA should be extracted into\na table, enabling reviewers to assess it systematically.\n4.1 Prompt Engineering\nTo identify effective communication strategies with LLMs for\nperforming the subtasks in HARA, we investigated prompt\nengineering. This was done to craft the prompts and their\npipeline effectively, aiming to improve the results in each\ndesign loop in comparison with the previous design loop.\nAfter testing various prompt patterns in several iterations,\nthe following patterns were found to be effective in achieving\nsuitable results:\nP1: Question Refinement Pattern[12] in forming the pre-\nliminary prompt: during the trial and error phase sometimes\nwe used sequences of prompts to see the feasibility of get-\nting the preferred results. When the preferred results were\nreached, the LLM was asked to provide the complete prompt\nneeded to achieve the correct result in another conversation.\nAlthough the prompts were changed afterward and tuned to\nour needs with more details added, this approach accelerated\nthe achievement of enhanced results.\nP2: Question Refinement Pattern[12] in bug fixing of the\nfinalized prompt: “Why did you make this mistake?” When\nLLM produced wrong inputs then the reason of the mistake\nwas asked and normally the reason was contributing to the\nimprovements of the new prompts and the same mistake\nwas not seen again. The main groups of mistakes were due\nto wrong assumptions by the LLM.\nP3: Breaking the tasks into sub-tasks: A well detailed and\nconcrete prompt, rather than a high-level one without any\ncontext, typically leads to better results. [12]. This leads to\nCAIN 2024, April 14–15, 2024, Lisbon, Portugal Nouri et al.\nFigure 1. LLM-based HARA, utilizing a pipeline of subtasks, each managed through a specific prompt. The item definition is\nimported (top-left), and the HARA results are exported (bottom-right). In the second row of the HARA table, the relationship\nof each column to the prompts is summarized.\nlonger prompts and given the limited number of tokens in\neach conversation, it is necessary to break down tasks into\nsub-tasks.\nP4: Reflection Pattern [12]: It is necessary to provide a ra-\ntionale for tasks such as severity classification, which assists\nreviewers in understanding the reasoning behind the selec-\ntion of severities. This pattern is like reflection pattern [12],\nleading to better judgements of LLM.\nThere are other patterns that could improve the results;\nhowever, their use would conflict with the established rules\nand restrictions of our study. For instance, the “Cognitive\nVerifier Pattern”[12] encourages the LLM to ask follow-up\nquestions for context clarification and to break down the\nquestion into sub-questions. However, this pattern necessi-\ntates user intervention during the pipeline’s intermediate\nsteps (a violation of R2) and demands additional inputs (a\nviolation of R1).\nEach prompt is constructed with the following structure:\n• Context: Part of the prompt where the context of the\ntask is described, i.e., function description, definitions,\nand process.\n• Task: In this second part the task itself is described.\nEach task needs inputs from previous sub-steps which\nare indicated by placeholders and they are replaced\nautomatically by the designed pipeline through the\nOpenAI API.\n• Template: At the end of the prompt the template of\nthe response is provided to indicate in which format\nthe output shall be produced which can be a table in\nComma-separated values (CSV) format.\n4.2 HARA Pipeline\nThe pipeline of the prompts is designed as it is shown in\nFig. 1 and implemented in Python. In each step, the relevant\ninputs are taken from the previous steps which are recorded\nand used in the next step.\nInput- Item Definition: The pipeline starts from top-\nleft corner of Fig. 1 which item definition of the function is\nimported as the only input.\nPrompt 1- Hazard Identification: The malfunction iden-\ntification process is explained at the beginning of the context,\nfollowed by a placeholder for the item definition, which will\nbe inserted. Then, “Your task: List the malfunctions of the\nfunction ... ” clarifies the task, and is followed by a table that\nserves as a template to be completed.\nPrompt 2- Geometry of Scenario: The core scenarios\nare created by incorporating various aspects and categories\nof roads, such as the number of lanes, road shape, slope, and\nWelcome Your New AI Teammate: On Safety Analysis by Leashing Large Language Models CAIN 2024, April 14–15, 2024, Lisbon, Portugal\nfeatures like bridges. Then the task is to create 20 diverse\nroad geometries followed by a template.\nPrompt 3- Scenario Expansion & Hazardous Event\nIdentification: The malfunction and core scenarios are com-\nbined and then each combination will be placed in the task\nof this prompt. As the context the item definition is provided.\nThe LLM is tasked with expanding the core scenario into\nmore detailed scenarios that could potentially lead to harm,\ntaking into account the provided malfunction. The LLM is\nprovided with a generic list of all possible objects and agents,\nalong with their relative trajectories.\nPrompt 4- Severity Identification: After outlining the\nprocess of severity identification, the LLM is asked to classify\nthe severity and provide a rationale for it.\nPrompt 5- Safety Goal Formulation: The item defini-\ntion and the process of safety requirements specification\nare provided as a context. Then the LLM is asked to specify\nsafety goals for severities higher than zero.\nPrompt 6- Cluster& Select 20 representative: The haz-\nardous events are grouped into four main categories, based\non the two primary guide-words (i.e., Omission and Com-\nmission) and two main categories of severity (i.e., S0/S1 and\nS2/S3). Then the LLM was asked to find the five most repre-\nsentative hazardous event in each category.\nOutput- HARA Table: The output is in the form of a table\n(in CSV format)1, as it is in industrial setup to be readable\nfor the engineers, which is shown in the table on the bottom\nof Fig. 1.\n5 Threats to Validity\nAs mentioned in Sec. 3, the evaluation has been conducted\nby researchers within the team to refine the pipeline and\nprompts. However, further evaluation by independent ex-\nperts external to the team, as part of the engineering cycle,\nis necessary and represents the next phase of this work.\n6 Conclusion & Future Work\nThis study focuses on the use of LLMs, more precisely the\nGPT-4.0 model, in safety analysis activities such as HARA\nin order to accelerate the iterative loops in SafetyOps. The\nprimary goal was to assess whether LLMs, through a sys-\ntematic process using carefully engineered prompts, could\ndeliver HARA results that meet the requirements of both\nindustry standards and experts, even if human oversight is\nstill required [10]. As a proof of concept, this study uses a\nnovel AD function that was not part of the training data of\nGPT-4.0, to avoid trivial or biased safety goals.\nInstead, all the contextual information was provided through\nthe prompts that make up the HARA pipeline and a number\nof experiments were conducted to ensure that the informa-\ntion was correctly propagated through it. To achieve this,\n1Providing part of the LLM-based HARA without human intervention,\nhttps://doi.org/10.5281/zenodo.10522786\nmultiple design iterations were conducted to improve the\nHARA prompting pipeline. The designed HARA pipeline,\ndescribed in Fig. 1, is able to provide an initial version of\na safety analysis without the need for human intervention.\nWhile these results demonstrate the potential of LLMs in\nconducting HARA, they also highlights areas for improve-\nment and the need for further development to fully meet\nindustry standards and expert expectations.\nAltogether, the results of this study show that GPT-4.0 is\nable to generate reasonable safety analysis that can be used\nas a “version zero” of the HARA. This version can then be ex-\npanded, modified, or used for brainstorming by development\nengineers, thereby accelerating the development of the valid\nversion of HARA. This points outs to the need for posterior\nrefinements by engineers, in line with the general recom-\nmendations for implementing human oversight strategies\nwhen using AI in safety contexts [10].\nFuture work could, therefore, focus on a more rigorous\nevaluation of the proposed pipeline by independent experts\nand on improvements of prompts and the pipeline to satisfy\nthe experts’ review comments.\nAcknowledgments\nThanks to Sweden’s Innovation Agency (Vinnova) for fund-\ning (Diarienummer: 2021-02585), and WASP, for supporting\nthis work.\nAI Ethics\nThe communication with LLMs and the evaluation of their\ncapabilities is at the core of this work. It is therefore crucial to\nhighlight the need to take AI ethics into consideration. The\nnature of LLMs, and their rapid adoption and change rate,\nmakes it difficult to completely explore their weaknesses and\nrisks. Regulations such as the AI Act [10], however, can guide\ntheir integration into systems or processes, as discussed in\nthis paper.\nSafety is also a source of concern for this LLM use case.\nFor this reason, the function under analysis is novel and\nnot yet commercialized. So the results of the analysis are\nonly used in this study and not in production. The proposed\npipeline and prompts are currently in the research phase and\nnot used in any engineering of production related projects.\nThe purpose, at this stage, is to conduct a feasibility study to\nassess whether further investigations are meaningful.\nDisclaimer\nThe views and opinions expressed are those of the authors\nand do not necessarily reflect the official policy or position\nof Volvo Cars.\nPlease keep in mind that HARA is a complex process that\ninvolves understanding both the system and the environ-\nment which it is used in. LLMs like GPT-4.0 are able to assist\nwith this process, as discussed in this paper, but they cannot\nCAIN 2024, April 14–15, 2024, Lisbon, Portugal Nouri et al.\nfully understand or model the system in the way a human ex-\npert would. Therefore, they shall be used cautiously and their\noutput must always be reviewed and validated by human\nexperts.\nReferences\n[1] “ISO 26262:2018 (all parts), Road Vehicles — Functional Safety, ” stan-\ndard, International Organization for Standardization, 2018.\n[2] “ISO 21448:2022, Road Vehicles — Safety of the Intended Functionality, ”\nstandard, International Organization for Standardization, 2022.\n[3] UNECE, “UN Regulation No. 157 - Automated Lane Keeping Systems\n(ALKS), ” 2021.\n[4] A. Nouri, C. Berger, and F. Törner, “On STPA for Distributed Develop-\nment of Safe Autonomous Driving: An Interview Study, ” inProceedings\nof the 49th EUROMICRO Conference on Software Engineering and Ad-\nvanced Applications (SEAA) , (DURRES, ALBANIA), Sep. 2023.\n[5] Google, “What is DevOps?, ” 2022.\n[6] U. Siddique, “SafetyOps, ”arXiv preprint arXiv:2008.04461 , 2020.\n[7] A. Nouri, C. Berger, and F. Törner, “An Industrial Experience Report\nabout Challenges from Continuous Monitoring, Improvement, and De-\nployment for Autonomous Driving Features, ” inEuromicro Conference\non Software Engineering and Advanced Applications , pp. 358–365, 2022.\n[8] L. Giray, “Prompt engineering with chatgpt: A guide for academic\nwriters, ”Annals of Biomedical Engineering (2023) .\n[9] A. Gilson, C. Safranek, T. Huang, V. Socrates, L. Chi, R. Taylor, and\nD. Chartash, “How well does chatgpt do when taking the medical\nlicensing exams? the implications of large language models for medical\neducation and knowledge assessment, ” 12 2022.\n[10] European Commission, Directorate-General for Communications Net-\nworks, Content and Technology,EUR-Lex - 52021PC0206 - EN - EUR-Lex .\nCNECT, 2021.\n[11] R. Bommasani et al. , “On the opportunities and risks of foundation\nmodels, ” 2022.\n[12] J. White, Q. Fu, S. Hays, M. Sandborn, C. Olea, H. Gilbert, A. Elnashar,\nJ. Spencer-Smith, and D. C. Schmidt, “A prompt pattern catalog to\nenhance prompt engineering with chatgpt, ” 2023.\n[13] O. Zheng, M. Abdel-Aty, D. Wang, Z. Wang, and S. Ding, “Chatgpt is on\nthe horizon: Could a large language model be suitable for intelligent\ntraffic safety research and applications?, ” 2023.\n[14] Y. Qi, X. Zhao, S. Khastgir, and X. Huang, “Safety analysis in the era\nof large language models: A case study of stpa using chatgpt, ” 2023.\n[15] S. Diemert and J. H. Weber, “Can large language models assist in hazard\nanalysis?, ” 2023.\n[16] A. R. Hevner, S. T. March, J. Park, and S. Ram, “Design science in\ninformation systems research, ”MIS Quarterly , vol. 28, no. 1, pp. 75–\n105, 2004.\n[17] R. Wieringa, Design Science Methodology for Information Systems and\nSoftware Engineering . 01 2014.\nReceived November 2023; revised January 2024; accepted January\n2024",
  "topic": "Correctness",
  "concepts": [
    {
      "name": "Correctness",
      "score": 0.8299461603164673
    },
    {
      "name": "Computer science",
      "score": 0.7111430168151855
    },
    {
      "name": "Risk analysis (engineering)",
      "score": 0.6432805061340332
    },
    {
      "name": "Process (computing)",
      "score": 0.6307635307312012
    },
    {
      "name": "Automation",
      "score": 0.6247349977493286
    },
    {
      "name": "DevOps",
      "score": 0.5457673668861389
    },
    {
      "name": "Hazard",
      "score": 0.49640852212905884
    },
    {
      "name": "Field (mathematics)",
      "score": 0.47869613766670227
    },
    {
      "name": "Software engineering",
      "score": 0.4548143148422241
    },
    {
      "name": "Unified Modeling Language",
      "score": 0.4372015595436096
    },
    {
      "name": "Engineering",
      "score": 0.2118937075138092
    },
    {
      "name": "Programming language",
      "score": 0.14207428693771362
    },
    {
      "name": "Software",
      "score": 0.11392742395401001
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Software deployment",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4387153628",
      "name": "Volvo Cars (Sweden)",
      "country": null
    },
    {
      "id": "https://openalex.org/I1340210623",
      "name": "Volvo (Sweden)",
      "country": "SE"
    },
    {
      "id": "https://openalex.org/I881427289",
      "name": "University of Gothenburg",
      "country": "SE"
    },
    {
      "id": "https://openalex.org/I4210086940",
      "name": "Zenuity (Sweden)",
      "country": "SE"
    }
  ]
}