{
    "title": "Pretrained Transformers for Text Ranking: BERT and Beyond",
    "url": "https://openalex.org/W3134665270",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2205106504",
            "name": "Andrew Yates",
            "affiliations": [
                "Max Planck Institute for Informatics"
            ]
        },
        {
            "id": "https://openalex.org/A2304906377",
            "name": "Rodrigo Nogueira",
            "affiliations": [
                "University of Waterloo"
            ]
        },
        {
            "id": "https://openalex.org/A2163619555",
            "name": "Jimmy Lin",
            "affiliations": [
                "University of Waterloo"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2971209824",
        "https://openalex.org/W2945127593",
        "https://openalex.org/W3023238803",
        "https://openalex.org/W2462891382",
        "https://openalex.org/W3104657626",
        "https://openalex.org/W3045033475",
        "https://openalex.org/W2136189984",
        "https://openalex.org/W3021397474",
        "https://openalex.org/W2951434086",
        "https://openalex.org/W3021779606",
        "https://openalex.org/W2940927814",
        "https://openalex.org/W3035183674",
        "https://openalex.org/W3100107515",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W3034212969",
        "https://openalex.org/W3012594078",
        "https://openalex.org/W2897754576",
        "https://openalex.org/W3004654429",
        "https://openalex.org/W3022373106",
        "https://openalex.org/W2982596739",
        "https://openalex.org/W3044812140",
        "https://openalex.org/W2260194779",
        "https://openalex.org/W3093955333",
        "https://openalex.org/W2982096936",
        "https://openalex.org/W2892181857",
        "https://openalex.org/W3092952717",
        "https://openalex.org/W2995289474",
        "https://openalex.org/W3064953855",
        "https://openalex.org/W2346933546",
        "https://openalex.org/W2909544278",
        "https://openalex.org/W3038572442",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3099446234",
        "https://openalex.org/W2611029872",
        "https://openalex.org/W3099384026",
        "https://openalex.org/W2755957574",
        "https://openalex.org/W2607074821",
        "https://openalex.org/W2937036051",
        "https://openalex.org/W2950133940",
        "https://openalex.org/W3102286003",
        "https://openalex.org/W3105107530",
        "https://openalex.org/W3092683697"
    ],
    "abstract": "The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query. Although the most common formulation of text ranking is search, instances of the task can also be found in many natural language processing applications. This tutorial, based on a forthcoming book, provides an overview of text ranking with neural network architectures known as transformers, of which BERT is the best-known example. The combination of transformers and self-supervised pretraining has, without exaggeration, revolutionized the fields of natural language processing (NLP), information retrieval (IR), and beyond. We provide a synthesis of existing work as a single point of entry for both researchers and practitioners. Our coverage is grouped into two categories: transformer models that perform reranking in multi-stage ranking architectures and learned dense representations that perform ranking directly. Two themes pervade our treatment: techniques for handling long documents and techniques for addressing the tradeoff between effectiveness (result quality) and efficiency (query latency). Although transformer architectures and pretraining techniques are recent innovations, many aspects of their application are well understood. Nevertheless, there remain many open research questions, and thus in addition to laying out the foundations of pretrained transformers for text ranking, we also attempt to prognosticate the future.",
    "full_text": null
}