{
  "title": "PTM-Mamba: a PTM-aware protein language model with bidirectional gated Mamba blocks",
  "url": "https://openalex.org/W4409340338",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5088818017",
      "name": "Peng Fei",
      "affiliations": [
        "Duke University"
      ]
    },
    {
      "id": "https://openalex.org/A5111046548",
      "name": "Chentong Wang",
      "affiliations": [
        "Westlake University"
      ]
    },
    {
      "id": "https://openalex.org/A5101443505",
      "name": "Tong Chen",
      "affiliations": [
        "Duke University"
      ]
    },
    {
      "id": "https://openalex.org/A5094043159",
      "name": "Benjamin Schussheim",
      "affiliations": [
        "Duke University"
      ]
    },
    {
      "id": "https://openalex.org/A5006944133",
      "name": "Sophia Vincoff",
      "affiliations": [
        "Duke University"
      ]
    },
    {
      "id": "https://openalex.org/A5016342562",
      "name": "Pranam Chatterjee",
      "affiliations": [
        "Duke University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3152110994",
    "https://openalex.org/W4367676419",
    "https://openalex.org/W4223515782",
    "https://openalex.org/W3109741298",
    "https://openalex.org/W4386034797",
    "https://openalex.org/W4327550249",
    "https://openalex.org/W3177500196",
    "https://openalex.org/W4318071656",
    "https://openalex.org/W4288066876",
    "https://openalex.org/W4387906121",
    "https://openalex.org/W4406728167",
    "https://openalex.org/W4387892965",
    "https://openalex.org/W4401277757",
    "https://openalex.org/W3112376646",
    "https://openalex.org/W4389326242",
    "https://openalex.org/W3213184710",
    "https://openalex.org/W2157658519",
    "https://openalex.org/W3138998566",
    "https://openalex.org/W4312090225",
    "https://openalex.org/W4387303685",
    "https://openalex.org/W2013743510",
    "https://openalex.org/W2799634000",
    "https://openalex.org/W4401039342",
    "https://openalex.org/W4205773061",
    "https://openalex.org/W4396809860",
    "https://openalex.org/W6675354045",
    "https://openalex.org/W6968038720",
    "https://openalex.org/W1993794768"
  ],
  "abstract": null,
  "full_text": "Nature Methods | Volume 22 | May 2025 | 945–949\n 945\nnature methods\nhttps://doi.org/10.1038/s41592-025-02656-9\nBrief Communication\nPTM-Mamba: a PTM-aware protein language \nmodel with bidirectional gated Mamba blocks\n \nFred Zhangzhi Peng1, Chentong Wang2, Tong Chen1, Benjamin Schussheim3, \nSophia Vincoff1 & Pranam Chatterjee    1,3,4 \nCurrent protein language models (LMs) accurately encode protein \nproperties but have yet to represent post-translational modifications \n(PTMs), which are crucial for proteomic diversity and influence protein \nstructure, function and interactions. T o address this gap, we develop \nPTM-Mamba, a PTM-aware protein LM that integrates PTM tokens using \nbidirectional Mamba blocks fused with ESM-2 protein LM embeddings \nvia a newly developed gating mechanism. PTM-Mamba uniquely models \nboth wild-type and PTM sequences, enabling downstream tasks such as \ndisease association and druggability prediction, PTM effect prediction on \nprotein–protein interactions and zero-shot PTM discovery. In total, our \nwork establishes PTM-Mamba as a foundational tool for PTM-aware protein \nmodeling and design.\nPTMs, such as phosphorylation, acetylation, ubiquitination and  \nglycosylation, vastly expand the functional diversity of eukaryotic pro-\nteomes, influencing essential processes like enzyme activity, protein \nturnover, signaling cascades and DNA repair1,2. Dysregulation of PTMs \noften leads to severe diseases, including cancer, neurodegeneration \nand aging2,3. For example, phosphorylation of STAT3 at specific resi-\ndues transforms it from a typical transcription factor into a driver of \ntumorigenesis and metastasis in various cancers4,5. Understanding and \nmodeling the unique sequence features of post-translationally modi-\nfied proteins is therefore crucial for advancing proteome-wide insights \nand therapeutic design. Protein LMs have emerged as transformative \ntools for encoding physicochemical and functional information in \nprotein sequences6. Models like ESM-2 and ProtT5 excel at sequence \nrepresentation, whereas autoregressive protein LMs like ProGen and \nProtGPT2 generate functional proteins7–10. From a therapeutic context, \nour generative language models, such as SaL T&PepPr, PepPrCLIP, \nPepMLM and moPPIt, have enabled the design of peptides that bind \nand degrade specific targets, including disordered proteins11–14. How-\never, existing protein LMs entirely exclude PTM residues from their \ntraining and inference pipelines 7–10, limiting their ability to model \nPTM-specific effects.\nWe hypothesized that combining ESM-2 embeddings with a spe-\ncialized framework for handling PTM tokens would enable accurate \nmodeling of both wild-type residues and PTMs. T o test this, we curated \na training dataset of 79,707 modified sequences, constructed from \n311,350 experimentally validated PTM records in Swiss-Prot 15. We \nspecifically mapped PTM annotations to their respective protein \nsequences, ensuring a diverse representation of PTM types (Supple -\nmentary Fig. 1) and sequence lengths (Supplementary Fig. 2).\nWe based our PTM protein LM on Mamba, a structured state-space \nmodel that offers computational efficiency and flexibility through a \nselective state-space architecture, which provides subquadratic time \nand memory complexity with sequence length16. Additionally, Mamba \nuses hardware-aware primitives, such as parallelized state transitions \nand convolutional projections, to accelerate computations without \naffecting scaling16. Although Mamba’s original design for autoregres-\nsive text generation limited its ability to capture full sequence seman-\ntics, we adapted it for bidirectional modeling by introducing forward \nand backward processing layers. The resulting bidirectional Mamba \nblock (Fig. 1a and code snippet below) processes the sequence in two \ndirections: a forward pass (left to right) and a backward pass (right to \nleft). Each pass independently generates hidden states through its \nrespective state-space layer, and the outputs are concatenated before \nbeing fused by a fully connected layer to generate a combined repre-\nsentation. Residual connections are applied to both the forward and \nbackward layers, and their contributions are averaged to retain both \nReceived: 27 February 2024\nAccepted: 5 March 2025\nPublished online: 10 April 2025\n Check for updates\n1Department of Biomedical Engineering, Duke University, Durham, NC, USA. 2School of Life Sciences, Westlake University, Hangzhou, China. 3Department \nof Computer Science, Duke University, Durham, NC, USA. 4Department of Biostatistics and Bioinformatics, Duke University, Durham, NC, USA.  \n e-mail: pranam.chatterjee@duke.edu\nNature Methods | Volume 22 | May 2025 | 945–949 946\nBrief Communication https://doi.org/10.1038/s41592-025-02656-9\nmodel7, in which wild-type amino acid tokens are passed into ESM-2-\n650M to retrieve its output embeddings and PTM tokens are converted \ninto <mask> tokens for ESM-2-650M input (Fig. 1a). Sequences are \nfinally fed into the embedding layer of PTM-Mamba, which naturally \nprocesses both wild-type and PTM tokens. T o join the ESM-2-650M \nand PTM-Mamba embeddings, we propose a new gating mechanism \nin which the two embeddings are concatenated and filtered via a \nsigmoid-activated linear gate to produce a final output representa -\ntion (Fig. 1a and code snippet below).\ndef gated_fuse(input_ids, esm_embedding): \n ptm_mamba_embedding = Embedding(input_ids) \n gate = Linear(torch.cat([hidden_states,  \nesm_embedding], dim=-1)).sigmoid() \n hidden_states = ptm_mamba_embedding * gate +  \nesm_embedding * (1 - gate) \n return hidden_states\nWe compared PTM-Mamba to a baseline PTM-Transformer  \nmodel and observed faster convergence on training accuracy (Sup -\nplementary Fig. 3), highlighting the comparative efficiency of the \nbidirectional Mamba blocks and gating mechanism. Beyond effi -\nciency, the primary objective of PTM-Mamba is to distinctly, yet rel -\nevantly, represent both unmodified and post-translationally modified \nsequences, capturing the critical biological functions and structural \nchanges induced by PTMs. T o assess this capability, we visualized \ndirectional contexts, ensuring comprehensive modeling of sequence \ndependencies for amino acids and PTMs.\ndef bidirectional_mamba(self, hidden_states): \n residual = None \n for f_layer, b_layer, h_fc in zip( \n   self.forward_layers, self.backward_layers, \nself.hidden_fc \n  ): \n   hidden_states_f, residual_f = f_layer( \n    hidden_states, residual, \n   )  \n   flip_residual = residual.flip([1]) if residual \nis not None else None \n   hidden_states_b, residual_b = b_layer( \n    hidden_states.flip([1]), flip_residual, \n   )  \n   hidden_states = h_fc( \n    torch.cat([hidden_states_f, hidden_states_  \nb.flip([1])], dim=-1) \n   )  \n   residual = 0.5 * (residual_f + residual_ \nb.flip([1]))\nT o preserve comprehension of regular amino acids, we trained our \nnew PTM-Mamba model as a head to the state-of-the-art ESM-2-650M \nPTM\nembedding\nWild-type sequence\nHALPYEIRPYI…\nMasked PTM sequence \nH<mask>LP<mask>EI<mask>PYI…\nPTM sequence\nHALP<Phosphotyrosine>EIRPYI…\nBackward\nMB\nGated embedding fusion\nConv\nSSM\nx\nσ\nσ\nLinear\nLinear +\nForward\nMB\nLinear\nBidirectional Mamba\nLinear\n+\nσ\nMamba block (MB)\nt-SNE 1\nt-SNE 1\nt-SNE 2\nt-SNE 2\nSequence embeddings\nToken embeddings\nb\nc\na\nESM-2\nembedding\n100\n50\n0\n–50\n–100\n–100 –50 –25 0 25 50 75 100\n16.0\n15.5\n15.0\n14.5\n14.0\n13.5\n13.0\n12.5\n1.0 1.5 2.0 2.5 3.0\nA\nT\nSG\nR H M\nI\nL\nV\nF\nY C\nW\nQKNED\nP\n<Phosphoserine>\n<Phosphothreonine>\n<Phosphotyrosine>\n<Omega-N-methylarginine>\n<S-palmitoyl cysteine>\n<S-diacylglycerol cysteine>\n<N-myristoyl glycine>\n<N\n5\n-methylglutamine>\n<S-geranylgeranyl cysteine>\n<O-linked (GaINAc...) threonine>\n<N-linked (GIcNAc...) asparagine>\n<4-carboxyglutamate>\n<4-hydroxyproline><N\n6\n-acetyllysine>\n<Pyrrolidone carboxylic acid>\n<N\n6\n-succinyllysine>\n<Asymmetric dimethylarginine>\n<N\n6\n-(pyridoxal phosphate)lysine>\n<O-(pantetheine 4’-phosphoryl)serine>\n<4-aspartylphosphate>\n<N\n6\n-carboxylysine>\n<Sulfotyrosine>\n<N-acetylmethionine>\n<N-acetylserine>\n<N-acetylalanine>\nWild-type sequence\nPTM sequence\nLinear\nFig. 1 | Architecture and embedding visualization of PTM-Mamba. a, Primitives \nof PTM-Mamba. Bottom left, given a sequence, with 80% probability, we perform \nstandard 15% token masking, and, with 20% probability, we mask all the PTM \ntokens and randomly mask 15% of wild-type tokens. The bidirectional Mamba \nblock in PTM-Mamba is built on top of the Mamba block (MB), which processes \nthe sequences in both the forward (forward Mamba block) and backward \n(backward Mamba block) orientation. The gated embedding fusion module \ninputs ESM-2 and PTM embeddings and fuses them in a gated manner via a \nsigmoid-activated linear layer. SSM, state-space model. b, t-SNE visualization of \nPTM-Mamba embeddings of select wild-type and corresponding PTM protein \nsequences. Orange lines connect the corresponding embeddings. c, t-SNE \nvisualization of labeled token embeddings. Conv, local 1D convolutional layer.\nNature Methods | Volume 22 | May 2025 | 945–949\n 947\nBrief Communication https://doi.org/10.1038/s41592-025-02656-9\n15\n10\n5\n0\n–5\na\nDisease association predictionDruggability prediction\n1.0\n0.8\n0.6\n0.4\n0.2\n0\n1.0\n0.8\n0.6\n0.4\n0.2\n0\nb\nc\nd Zero-shot PTM prediction\n1.0\n0.8\n0.6\n0.4\n0.2\n0\nQ09324 (N58)\nQ4L7X2 (C20)\nQ02261 (S89)\nQ32BW5 (K708)\nQ1GTW7 (K124)\nP47197 (T309)\nQ17QF0 (K304)\nQ3UYV9 (K204)\nAccuracy Precision Recall MCC AUPRC AUROC\nAccuracy Precision Recall MCCF1 AUPRC AUROC\nAccuracy Precision Recall MCCF1 AUPRC AUROC\nPTM-Mamba\nOneHot\nOneHot (+PTM)\nESM-2-650M\nESM-2-3B\nPTM-Transformer\nPTM-Mamba\nOneHot\nOneHot (+PTM)\nESM-2-650M\nESM-2-3B\nPTM-Transformer\nPTM-Mamba\nOneHot (+PTM)\nESM-2-650M\nPTM-SaProt\nESM-2-3B\nPTM-Transformer\nInduce Inhibit\nPrediction of PTM eﬀects on PPIs\nPTM\nPTM\n<4-carboxyglutamate>\n<S-geranylgeranyl cysteine>\n<4-aspartylphosphate>\n<N\n5\n-methylglutamine>\n<Asymmetric dimethylarginine>\n<4-hydroxyproline>\n<N-myristoyl glycine>\n<Omega-N-methylarginine>\n<O-linked (GalNAc...) threonine>\n<Sulfotyrosine>\n<O-(pantetheine 4’-phosphoryl)serine>\n<S-palmitoyl cysteine>\n<N\n6\n-succinyllysine>\n<N\n6\n-carboxylysine>\n<N-acetylserine>\n<N\n6\n-(pyridoxal phosphate)lysine>\n<S-diacylglycerol cysteine>\n<Phosphotyrosine>\n<N\n6\n-acetyllysine>\n<N-acetylmethionine>\n<N-acetylalanine>\n<Phosphothreonine>\n<Phosphoserine>\n<Pyrrolidone carboxylic acid>\n<N-linked (GlcNAc...) asparagine>\nFig. 2 | Performance evaluation of PTM-Mamba across diverse PTM-related \ntasks. a, Disease association prediction for PTM modified sequences, evaluated \nacross accuracy, precision, recall, F1, MCC, area under the precision–recall curve \n(AUPRC) and area under the receiver operating characteristic curve (AUROC). \nb, Druggability prediction of PTM modified sequences, evaluated across the \nsame metrics. c, Prediction of PTM effects on PPIs, using PTMint data to classify \nwhether a PTM induces or inhibits an interaction. All benchmarks in a–c were \nperformed with replicates (n = 5). d, Visualization of the predicted logits for \nzero-shot PTM discovery. Rows denote different amino acids in the format of \n‘Uniref-accession-id (amino-acid position)’ , and columns denote the logit value \nof the PTMs. Schematic in panel c created using BioRender.com.\nNature Methods | Volume 22 | May 2025 | 945–949 948\nBrief Communication https://doi.org/10.1038/s41592-025-02656-9\nPTM-Mamba embeddings using t -distributed stochastic neighbor \nembedding (t-SNE). The embeddings revealed a nuanced distinction \nbetween wild-type protein sequences and their PTM modified coun -\nterparts, with embeddings for each wild-type pair in close proxi mity \n(Fig. 1b). This suggests the ability of PTM-Mamba to capture the subtle \nyet notable effects of PTMs while maintaining the contextual integrity \nof the protein sequence. Additionally, token embeddings for PTM \nresidues showed class-specific organization, with spatial proxi mity \nobserved among tokens for phosphorylation and acetylation as exam-\nples (Fig. 1c). PTM residue tokens also exhibited greater spatial diver-\nsity than wild-type tokens, reflecting the model’s focus on encoding \nPTM-specific information (Fig. 1c).\nT o confirm that PTM-Mamba embeddings maintain strong  \nperformance on standard PTM prediction tasks, we evaluated them \non phosphorylation site prediction (Supplementary Fig. 4) and \nnon-histone acetylation site prediction (Supplementary Fig. 5). Using \ncurated datasets for both tasks, we conducted per-residue binary clas-\nsification and compared PTM-Mamba embeddings against baselines, \nincluding ESM-2-650M, ESM-2-3B, PTM-Transformer and baseline \none-hot embeddings. PTM-Mamba maintained comparable perfor -\nmance across all metrics, confirming that its embeddings retain gen-\neral applicability for PTM-related tasks. Notably, these tasks do not \nexplicitly represent PTM tokens, which aligns with the observation that \nPTM-Mamba is primarily optimized for use cases involving modified \nsequences, rather than wild-type-only benchmarks.\nWe next evaluated PTM-Mamba on three benchmarking tasks \nexplicitly leveraging PTM tokenization: disease association predic -\ntion, druggability prediction and the effects of PTMs on protein–pro-\ntein interactions (PPIs). For disease association prediction, we used \na dataset curated from the dbPTM database17 that links PTMs to con-\nditions such as cancer, neurodegenerative disorders and diabetes, \nwith annotations sourced from databases such as PhosphoSitePlus, \nActiveDriverDB and genome-wide association studies (GWAS) as well as \nmanual curation18,19. Druggability prediction assessed PTM sequences \nthat influence therapeutic targetability, focusing on how modifica -\ntions alter protein structure and accessibility of binding sites 17.  To \nevaluate the effects of PTMs on PPIs, we used the PTMint dataset, which \nannotates experimentally validated PTM-mediated regulatory roles, \nspecifically whether a PTM induces or inhibits a PPI 20. For all tasks, \nwild-type sequences were mapped to PTM-Mamba’s dataset, with \nresidues replaced by the corresponding PTMs for tokenization, while \nbaseline models, including one-hot embeddings and ESM-2 embed -\ndings, used wild-type sequences as input.\nFor disease association prediction, PTM-Mamba performs strongly \nversus baseline models, including ESM-2-650M and PTM-Transformer, \ndemonstrating its ability to capture PTM-specific effects essential \nfor identifying disease-associated proteins (Fig. 2a). Similarly, for \ndruggability prediction, PTM-Mamba achieved robust performance, \noften exceeding baselines across key metrics such as F1 score and Mat-\nthews correlation coefficient (MCC), highlighting its relevance for \ntherapeutic design (Fig. 2b). For the key PTM effect on the PPI task, \nPTM-Mamba achieved the highest metrics among all models, includ-\ning PTM-Transformer and PTM-SaProt, a novel baseline model that \nreplaces ESM-2 with state-of-the-art, structure-aware SaProt protein \nLM embeddings21, indicating that sequence-focused models may cap-\nture PTM effects more optimally (Fig. 2c). This benchmark showcases \nPTM-Mamba’s ability to model complex regulatory dynamics medi -\nated by PTMs, further highlighting its utility for biologically relevant \ndownstream applications.\nFinally, we explored PTM-Mamba’s utility for zero-shot PTM disco-\nvery, a task of great biological relevance. By analyzing model logits for \nmasked positions in wild-type sequences, PTM-Mamba accurately pre-\ndicted plausible PTMs for specific residues, such as <phosphoserine>  \nfor serine in UniProt sequence Q02261  and <S -diacylglycerol \ncysteine> for cysteine in UniProt sequence Q4L7X2 (Fig. 2d). This \ncapability offers PTM-Mamba as a tool for biologists to generate  \nnew insights into PTM biology without requiring additional training \nor labels.\nIn total, PTM-Mamba provides new opportunities for modeling \nand designing PTM-specific protein sequences, particularly via its abil-\nity to explicitly tokenize PTM modified proteoforms for applications \nranging from disease mechanism studies to therapeutic design with \nenhanced targeting specificity. For future work, we plan to address the \nlimited availability of experimentally validated PTM annotations by \naugmenting the training dataset using mass spectrometry-based PTM \ndatabases22. We also aim to explore structure prediction of PTM modi-\nfied sequences as a new task that can leverage PTM-Mamba’s embed-\ndings, alongside extending these embeddings to design PTM-specific \nbinders that selectively target modified protein states6,23,24. T ogether, \nby enabling PTM-aware modeling, PTM-Mamba has the potential \nto reshape proteome analysis and drive innovation in precision \ntherapeutics.\nOnline content\nAny methods, additional references, Nature Portfolio reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author contri-\nbutions and competing interests; and statements of data and code avail-\nability are available at https://doi.org/10.1038/s41592-025-02656-9.\nReferences\n1. Ramazi, S. & Zahiri, J. Posttranslational modifications in proteins: \nresources, tools and prediction methods. Database 2021, \nbaab012 (2021).\n2. Zhong, Q. et al. Protein posttranslational modifications in health \nand diseases: functions, regulatory mechanisms, and therapeutic \nimplications. MedComm 4, e261 (2023).\n3. Pan, S. & Chen, R. Pathological implication of protein post- \ntranslational modifications in cancer. Mol. Asp. Med. 86, 101097 \n(2022).\n4. Rébé, C., Végran, F., Berger, H. & Ghiringhelli, F. STAT3 activation: \na key factor in tumor immunoescape. JAKSTAT 2, e23010 (2013).\n5. Lin, W.-H. et al. STAT3 phosphorylation at Ser727 and Tyr705 \ndifferentially regulates the EMT–MET switch and cancer \nmetastasis. Oncogene 40, 791–805 (2021).\n6. Chen, T., Hong, L., Yudistyra, V., Vincoff, S. & Chatterjee, P. \nGenerative design of therapeutics that bind and modulate protein \nstates. Curr. Opin. Biomed. Eng. 28, 100496 (2023).\n7. Lin, Z. et al. Evolutionary-scale prediction of atomic-level protein \nstructure with a language model. Science 379, 1123–1130 (2023).\n8. Elnaggar, A. et al. ProtTrans: toward understanding the language \nof life through self-supervised learning. IEEE Trans. Pattern Anal. \nMach. Intell. 44, 7112–7127 (2022).\n9. Madani, A. et al. Large language models generate functional \nprotein sequences across diverse families. Nat. Biotechnol. 41, \n1099–1106 (2023).\n10. Ferruz, N., Schmidt, S. & Höcker, B. ProtGPT2 is a deep unsupervised \nlanguage model for protein design. Nat. Commun. 13, 4348 (2022).\n11. Brixi, G. et al. SaLT&PepPr is an interface-predicting language \nmodel for designing peptide-guided protein degraders. Commun. \nBiol. 6, 1081 (2023).\n12. Bhat, S. et al. De novo design of peptide binders to \nconformationally diverse targets with contrastive language \nmodeling. Sci. Adv. 11, 4 (2025).\n13. Chen, T. et al. PepMLM: target sequence-conditioned generation of \ntherapeutic peptide binders via span masked language modeling. \nPreprint at https://doi.org/10.48550/arXiv.2310.03842 (2023).\n14. Chen, T., Zhang, Y. & Chatterjee, P. moPPIt: generation of \nmotif-specific binders with protein language models. Preprint at \nbioRxiv https://doi.org/10.1101/2024.07.31.606098 (2024).\nNature Methods | Volume 22 | May 2025 | 945–949\n 949\nBrief Communication https://doi.org/10.1038/s41592-025-02656-9\n15. UniProt Consortium. UniProt: the universal protein knowledgebase \nin 2021. Nucleic Acids Res. 49, D480–D489 (2021).\n16. Gu, A. & Dao, T. Mamba: linear-time sequence modeling with \nselective state spaces. Preprint at https://doi.org/10.48550/\narXiv.2312.00752 (2023).\n17. Li, Z. et al. dbPTM in 2022: an updated database for exploring \nregulatory networks and functional associations of protein \npost-translational modifications. Nucleic Acids Res. 50,  \nD471–D479 (2022).\n18. Hornbeck, P. V. et al. PhosphoSitePlus: a comprehensive resource \nfor investigating the structure and function of experimentally \ndetermined post-translational modifications in man and mouse. \nNucleic Acids Res. 40, D261–D270 (2012).\n19. Krassowski, M. et al. ActiveDriverDB: interpreting genetic \nvariation in human and cancer genomes using post-translational \nmodification sites and signaling networks (2021 update). Front. \nCell Dev. Biol. 9, 626821 (2021).\n20. Hong, X. et al. PTMint database of experimentally verified PTM \nregulation on protein–protein interaction. Bioinformatics 39, \nbtac823 (2023).\n21. Su, J. et al. SaProt: protein language modeling with structure- \naware vocabulary. In Proc. 12th International Conference on \nLearning Representations (ICLR, 2024).\n22. Doll, S. & Burlingame, A. L. Mass spectrometry-based detection \nand assignment of protein posttranslational modifications.  \nACS Chem. Biol. 10, 63–71 (2015).\n23. Hattori, T. & Koide, S. Next-generation antibodies for \npost-translational modifications. Curr. Opin. Struct. Biol. 51, \n141–148 (2018).\n24. Hong, L. et al. Programmable protein stabilization with language \nmodel-derived peptide guides. Preprint at Res. Sq. https://doi.org/ \n10.21203/rs.3.rs-4670386/v1 (2024).\nPublisher’s note Springer Nature remains neutral with regard  \nto jurisdictional claims in published maps and institutional  \naffiliations.\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons licence, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article’s Creative Commons licence and your intended \nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\n© The Author(s) 2025\nNature Methods\nBrief Communication https://doi.org/10.1038/s41592-025-02656-9\nMethods\nData curation\nModel training data were curated from UniProt15. Specifically, 311,350 \nexperimentally validated PTM records were collected from Swiss-Prot, \nand the PTM annotations of their proteins were mapped to their respec-\ntive sequences to construct the new PTM sequences. The final dataset \nincludes a total of 79,707 PTM sequences. Data curation code can be \nfound at https://github.com/programmablebio/ptm-mamba/tree/\nmain/ptm_data_preprocessing.\nDatasets for the four benchmarking tasks were collected from \nthe following sources. Phosphorylation site data were obtained from \nthe corresponding ProteinBERT benchmark25, originally derived from \nPhospoSitePlus18 and filtered for sequences between 256 and 512 amino \nacids in length, yielding a training set of 15,588 sequences, a validation \nset of 1,707 sequences and a testing set of 3,106 sequences. Non-histone \nacetylation site prediction was performed equivalently as described in \nprior literature, using the non-histone acetylation collection dataset26. \nDruggability and disease association datasets were curated from the \ndbPTM database17. PPI data describing the effect of PTMs were curated \nfrom PTMint, which encompasses 2,477 nonredundant PTM sites in \n1,169 proteins affecting 2,371 protein–protein pairs20. In brief, wild-type \nsequences were mapped to corresponding entries in the PTM-Mamba \ndataset, and wild-type residues were replaced by the corresponding \nposition-specific PTMs for tokenization by specified models. For all other \nbaseline models trained with standard one-hot embeddings or ESM-2 \nembeddings, the corresponding wild-type sequence was used as input.\nTokenization\nIn our tokenization scheme, we use the standard set of amino acids \ntokens as described in ESM-2 (ref. 7 ). In addition to special tokens, \nthe 20 wild-type amino acids tokens are as follows: D, N, E, K, V, Y , A, \nQ, M, I, T, L, R, F , G, C, S, P, H, W. We introduce new PTM tokens, corres-\nponding to their unique specific UniProt annotations: <N-linked \n(GlcNAc…) asparagine>, <Pyrrolidone carboxylic acid>,  \n<Phosphoserine>, <Phosphothreonine>, <N-acetylalanine>,  \n<N-acetylmethionine>, <N6-acetyllysine>, <Phosphotyro-\nsine>, <S-diacylglycerol cysteine>, <N6-(pyridoxal phos-\nphate)lysine>, <N-acetylserine>, <N6-carboxylysine>, \n<N6-succinyllysine>, <S-palmitoyl cysteine>, \n<O-(pantetheine 4-phosphoryl)serine>, <Sulfotyrosine>, \n<O-linked (GalNAc…) threonine>, <Omega-N-methylarginine>, \n<N-myristoyl glycine>, <4-hydroxyproline>, <Asym -\nmetric dimethylarginine>, <N5-methylglutamine>, \n<4-aspartylphosphate>, <S-geranylgeranyl cysteine>,  \n<4-carboxyglutamate>. The top two most abundant PTM tokens  \nare <N-linked (GlcNAc…) asparagine> and <Phosphoserine>. \nThe full distribution of the PTM tokens is shown in Supplementary \nFig. 1, and the full PTM tokens are presented in Supplementary Table 1. \nThe wild-type amino acid tokens are then converted into embed dings \nby both ESM-2-650M and PTM-Mamba, while the PTM tokens are  \nonly processed by PTM-Mamba.\nPTM-Mamba training procedure\nPTM-Mamba was trained on an Nvidia 8xA100 DGX system with 640 GB \nof shared VRAM on an adjusted masked language modeling task, \nin which, rather than random 15% token masking, we bias masked \nto PTM residue tokens (Fig. 1d). Briefly, given a sequence with 80% \nprobability, we perform standard 15% token masking, and, with 20% \nprobability, we mask all the PTM tokens and randomly mask 15% of \nwild-type tokens. For training, we then consider a protein sequence \nwith masked residues, where the model aims to predict the original \ntokens at these residue positions. Let x i denote the original residue \ntoken at position i that has been masked, and let yi denote the residue \ntoken predicted by the model for this position. The loss function L \nfor masked language modeling can be defined as the negative log \nlikelihood of the correct tokens given their masked inputs, summed \nover all masked positions N:\nL =−\nN\n∑\ni=1\nlogP(xi|xmasked).\nP(xi|xmasked) represents the probability of predicting the correct  \noriginal token x i at the masked position, given the masked input \nsequence xmasked. PTM-Mamba was trained via the Adam optimizer with \nno weight decay. The final PTM-Mamba model has 24 layers with hidden \ndimensions of 768. It was trained for 16,765 steps (425 epochs) at a \nconstant learning rate of 0.0002 with a batch size of 256 and dynamic \nbatching. Training sequences were randomly cropped to a maximal \nlength of 1,024 or padded at the end to reach a length of 1,024. During \ntraining, we clustered the sequences by length and constructed  \nthe batches. The training batches were fed into the model, going from \nthe shortest to the longest sequences. PTM-Mamba was trained on an \nNvidia 8xA100 GPU DGX system with 640 GB of shared VRAM.\nBenchmark model training\nFor all the benchmark tasks, we leverage the embeddings from pre -\ntrained PTM-Mamba and ESM-2 models and fine-tune a classification \nhead on top of the embeddings. We extensively tuned the classification \nhead architectures as well as the training hyperparameters for the \nbenchmarks and have reported the optimal settings in Supplementary \nCodes 1–4 and Supplementary Table 2. For models trained on one-hot \nembeddings of wild-type input sequences, an nn.Embedding layer fol-\nlowed by a linear layer was used. All benchmark models were trained \non an Nvidia 8xA100 GPU DGX system with 640 GB of shared VRAM. \nFor robust performance comparison, we replicate each model (n = 5) \nand report the individual and average results. Models were evaluated \nusing accuracy, precision, recall, F 1 score, MCC, AUROC and AUPRC \nmetrics via scikit-learn27.\nReporting summary\nFurther information on research design is available in the Nature \nPortfolio Reporting Summary linked to this article.\nData availability\nAll data needed to evaluate the conclusions are presented in the paper, \ntables and Supplementary Information and are further available at \nhttps://doi.org/10.5281/zenodo.14794992 (ref. 28).\nCode availability\nPTM-Mamba, PTM-Transformer, PTM-SaProt, baseline model weights, \ntraining code and Python scripts for data preprocessing can be found \nat https://huggingface.co/ChatterjeeLab/PTM-Mamba and https://\ngithub.com/programmablebio/ptm-mamba.\nReferences\n25. Brandes, N., Ofer, D., Peleg, Y., Rappoport, N. & Linial, M. \nProteinBERT: a universal deep-learning model of protein \nsequence and function. Bioinformatics 38, 2102–2110 (2022).\n26. Meng, L. et al. TransPTM: a transformer-based model for \nnon-histone acetylation site prediction. Brief. Bioinform.  \n25, 3 (2024).\n27. Pedregosa, F. et al. Scikit-learn: machine learning in Python.  \nJ. Mach. Learn. Res. 12, 2825–2830 (2011).\n28. Peng, F. Z. et al. PTM-Mamba: a PTM-aware protein language \nmodel with bidirectional gated Mamba blocks. Zenodo  \nhttps://doi.org/10.5281/zenodo.14794992 (2025).\nAcknowledgements\nWe thank Mark III Systems for computing support. We further thank  \nY. Zhang and T. Chen for their insights related to the manuscript.  \nNature Methods\nBrief Communication https://doi.org/10.1038/s41592-025-02656-9\nWe thank L. Hong for rendering the PTM-Mamba logo. The work was \nsupported by a grant from the National Institute of General Medical \nSciences (award 1R35GM1555282-01) to the laboratory of P.C.\nAuthor contributions\nF.Z.P. designed and implemented PTM-Mamba architecture and \nconducted benchmarking analysis. F.Z.P. and C.W. developed and \ntrained PTM-SaProt. T.C., S.V. and B.S. conducted benchmarking \nanalysis. F.Z.P., S.V., T.C. and P.C. wrote and reviewed the \nmanuscript. P.C. conceived, designed, directed and supervised  \nthe study.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary information The online version contains supplementary \nmaterial available at https://doi.org/10.1038/s41592-025-02656-9.\nCorrespondence and requests for materials should be addressed to \nPranam Chatterjee.\nPeer review information Nature Methods thanks the anonymous \nreviewer(s) for their contribution to the peer review of this work. \nPrimary Handling Editor: Arunima Singh, in collaboration with the \nNature Methods team. Peer reviewer reports are available.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\n\n\n",
  "topic": "Druggability",
  "concepts": [
    {
      "name": "Druggability",
      "score": 0.6831943392753601
    },
    {
      "name": "Computer science",
      "score": 0.5957006812095642
    },
    {
      "name": "Computational biology",
      "score": 0.4706871807575226
    },
    {
      "name": "ENCODE",
      "score": 0.4441722631454468
    },
    {
      "name": "Function (biology)",
      "score": 0.4313991367816925
    },
    {
      "name": "Biology",
      "score": 0.2820650637149811
    },
    {
      "name": "Genetics",
      "score": 0.1406365931034088
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I170897317",
      "name": "Duke University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I3133055985",
      "name": "Westlake University",
      "country": "CN"
    }
  ],
  "cited_by": 16
}