{
  "title": "Improving speech recognition systems for the morphologically complex Malayalam language using subword tokens for language modeling",
  "url": "https://openalex.org/W4388347706",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2226309001",
      "name": "Kavya Manohar",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Jayan A R",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2228201541",
      "name": "Rajeev Rajan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2091746061",
    "https://openalex.org/W646660152",
    "https://openalex.org/W3081780578",
    "https://openalex.org/W3089901025",
    "https://openalex.org/W2953092638",
    "https://openalex.org/W3111869745",
    "https://openalex.org/W2714176837",
    "https://openalex.org/W2032942114",
    "https://openalex.org/W4224917952",
    "https://openalex.org/W6779307919",
    "https://openalex.org/W3185921853",
    "https://openalex.org/W2970653656",
    "https://openalex.org/W3201291635",
    "https://openalex.org/W2117621558",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W3173649224",
    "https://openalex.org/W2899879954",
    "https://openalex.org/W2527833599",
    "https://openalex.org/W3048135670",
    "https://openalex.org/W4360951492",
    "https://openalex.org/W1875324271",
    "https://openalex.org/W2806955548",
    "https://openalex.org/W4385571315",
    "https://openalex.org/W4294691598",
    "https://openalex.org/W2092860912",
    "https://openalex.org/W2069712814",
    "https://openalex.org/W2115084322",
    "https://openalex.org/W2404169761",
    "https://openalex.org/W4211116738",
    "https://openalex.org/W2046932483",
    "https://openalex.org/W2148154194",
    "https://openalex.org/W2103869314",
    "https://openalex.org/W2150769028",
    "https://openalex.org/W2402146185",
    "https://openalex.org/W2079623482",
    "https://openalex.org/W2514741789",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2122364000",
    "https://openalex.org/W2592584157",
    "https://openalex.org/W4210849719",
    "https://openalex.org/W58893626",
    "https://openalex.org/W3161539047",
    "https://openalex.org/W1579838312",
    "https://openalex.org/W3158607076"
  ],
  "abstract": "Abstract This article presents the research work on improving speech recognition systems for the morphologically complex Malayalam language using subword tokens for language modeling. The speech recognition system is built using a deep neural network–hidden Markov model (DNN-HMM)-based automatic speech recognition (ASR). We propose a novel method, syllable-byte pair encoding (S-BPE), that combines linguistically informed syllable tokenization with the data-driven tokenization method of byte pair encoding (BPE). The proposed method ensures words are always segmented at valid pronunciation boundaries. On a text corpus that has been divided into tokens using the proposed method, we construct statistical n-gram language models and assess the modeling effectiveness in terms of both information-theoretic and corpus linguistic metrics. A comparative study of the proposed method with other data-driven (BPE, Morfessor, and Unigram), linguistic (Syllable), and baseline (Word) tokenization algorithms is also presented. Pronunciation lexicons of subword tokenized units are built with pronunciation described as graphemes. We develop ASR systems employing the subword tokenized language models and pronunciation lexicons. The resulting ASR models are comprehensively evaluated to answer the research questions regarding the impact of subword tokenization algorithms on language modeling complexity and on ASR performance. Our study highlights the strong performance of the hybrid S-BPE tokens, achieving a notable 10.6% word error rate (WER), which represents a substantial 16.8% improvement over the baseline word-level ASR system. The ablation study has revealed that the performance of S-BPE segmentation, which initially underperformed compared to syllable tokens with lower amounts of textual data for language modeling, exhibited steady improvement with the increase in LM training data. The extensive ablation study indicates that there is a limited advantage in raising the n-gram order of the language model beyond $$n=3$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mrow> <mml:mi>n</mml:mi> <mml:mo>=</mml:mo> <mml:mn>3</mml:mn> </mml:mrow> </mml:math> . Such an increase results in considerable model size growth without significant improvements in WER. The implementation of the algorithm and all associated experiments are available under an open license, allowing for reproduction, adaptation, and reuse.",
  "full_text": "Manohar et al. \nEURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:47  \nhttps://doi.org/10.1186/s13636-023-00313-7\nEMPIRICAL RESEARCH Open Access\n© The Author(s) 2023. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http://creativecommons.org/licenses/by/4.0/.\nEURASIP Journal on Audio,\nSpeech, and Music Processing\nImproving speech recognition systems \nfor the morphologically complex Malayalam \nlanguage using subword tokens for language \nmodeling\nKavya Manohar1,2*  , Jayan A R1,3 and Rajeev Rajan1,2 \nAbstract \nThis article presents the research work on improving speech recognition systems for the morphologically complex \nMalayalam language using subword tokens for language modeling. The speech recognition system is built using \na deep neural network–hidden Markov model (DNN-HMM)-based automatic speech recognition (ASR). We pro-\npose a novel method, syllable-byte pair encoding (S-BPE), that combines linguistically informed syllable tokeniza-\ntion with the data-driven tokenization method of byte pair encoding (BPE). The proposed method ensures words \nare always segmented at valid pronunciation boundaries. On a text corpus that has been divided into tokens using \nthe proposed method, we construct statistical n-gram language models and assess the modeling effectiveness \nin terms of both information-theoretic and corpus linguistic metrics. A comparative study of the proposed method \nwith other data-driven (BPE, Morfessor, and Unigram), linguistic (Syllable), and baseline (Word) tokenization algo-\nrithms is also presented. Pronunciation lexicons of subword tokenized units are built with pronunciation described \nas graphemes. We develop ASR systems employing the subword tokenized language models and pronuncia-\ntion lexicons. The resulting ASR models are comprehensively evaluated to answer the research questions regard-\ning the impact of subword tokenization algorithms on language modeling complexity and on ASR performance. \nOur study highlights the strong performance of the hybrid S-BPE tokens, achieving a notable 10.6% word error rate \n(WER), which represents a substantial 16.8% improvement over the baseline word-level ASR system. The ablation \nstudy has revealed that the performance of S-BPE segmentation, which initially underperformed compared to syllable \ntokens with lower amounts of textual data for language modeling, exhibited steady improvement with the increase \nin LM training data. The extensive ablation study indicates that there is a limited advantage in raising the n-gram \norder of the language model beyond n = 3 . Such an increase results in considerable model size growth without sig-\nnificant improvements in WER. The implementation of the algorithm and all associated experiments are available \nunder an open license, allowing for reproduction, adaptation, and reuse.\nKeywords Subword tokens, Language modeling, Open vocabulary, Speech recognition, Morphological complexity, \nMalayalam language\n*Correspondence:\nKavya Manohar\nsakhi.kavya@gmail.com\nFull list of author information is available at the end of the article\nPage 2 of 25Manohar et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:47 \n1 Introduction\nAutomatic speech recognition (ASR) is the process of \nconverting speech acoustic signals into written text. This \ninvolves several stages, where the acoustic features are \nmapped to phonemes, which are the basic units of sound, \nand then reconstructed into meaningful words and sen -\ntences of the language under consideration.\nMorphologically complex low-resource languages pose \na significant challenge to speech recognition [1]. Mor -\nphologically complex languages exhibit productive word \nformation through agglutination, inflection, and com -\npounding, resulting in very long words with phonetic \nand orthographic changes at morpheme boundaries [2]. \nThe presence of such lengthy and morphologically com -\nplex words introduces difficulties in language modeling, \nwhich subsequently affects the word error rate (WER) in \nASR tasks. This study aims to examine how different sub-\nword tokenization algorithms affect the performance of \nASR models developed specifically for Malayalam.\n1.1  Morphological complexity of Malayalam language\nMalayalam is a morphologically complex language which \nhas seven nominal case forms (nominative, accusative, \ndative, sociative, locative, instrumental, and genitive), \ntwo nominal number forms (singular and plural) and \nthree gender forms (masculine, feminine, and neutral). \nThese forms are indicated as suffixes to the nouns. Verbs \nin Malayalam get inflected based on tense (present, past, \nand future), mood (imperative, compulsive, promissive, \noptative, abilitative, purposive, permissive, precative, \nirrealis, monitory, quotative, conditional, and satisfac -\ntive), voice (active and passive), and aspect (habitual, iter-\native, perfect) [3].\nThe inflecting suffix forms vary depending on the \nfinal phonemes of the root words. Words agglutinate to \nform new words depending on the context [4]. Table  1 \ngives examples of a few complex word formations in \nMalayalam. It has been demonstrated in the literature \nthat the Malayalam language exhibits a high level of \nmorphological complexity than many other Indian and \nEuropean languages in terms of type-token ratio and \ntype-token growth rate [5, 6].\nFigure  1 presents a comparison of the type-token \ngrowth rate of Malayalam with that of other Indian \nlanguages and English highlighting the notably higher \nrate at which new words (types) are encountered in the \nMalayalam corpus. This creates a large number of low-\nfrequency words and it is practically impossible to build \na pronunciation lexicon that covers all complex word \nforms. Additionally, it introduces the problem of data \nsparsity in language modeling [7].\n1.2  Deep neural network–hidden Markov Model \n(DNN‑HMM)‑based ASR architecture\nThe classical ASR decoder shown in Fig.  2 is composed \nof an acoustic model (AM), a language model (LM), and \na pronunciation lexicon (PL). The AM captures the rela -\ntionship between acoustic features and phonemes in the \nlanguage. The PL contains the phonemic representations \nof all words to be decoded by the ASR system. The LM \nestablishes the statistical relationship between words in \nthe language. Word-level statistical modeling of mor -\nphologically complex languages can not achieve the word \nsequence prediction capabilities of simple morphol -\nogy languages [8, 9]. Additionally, the finite-sized word \nvocabulary of a pronunciation lexicon does not cover \ncomplex word forms and loan words that appear in a \nreal-world setting. As a result, there is difficulty in recov -\nering words that are not in the lexicon. [7].\nThe out-of-vocabulary (OOV) rate is the proportion of \nwords in a given speech sample that are not present in \nthe vocabulary of the ASR lexicon. OOV words can not \nbe recognized by a word-based ASR decoder. A large \nnumber of OOV words and data sparsity are the natural \nconsequences of word-based language models in ASR \nfor morphologically complex languages [7]. Segmenting \nwords to appropriate subword tokens before process -\ning, and later reconstructing them to the whole words \nTable 1 Complex morphological word formation in Malayalam\nMalayalam word English translation Remark\nIn the box Nominal locative suffix to the word  (box).\nTo the child Nominal sociative suffix to the word  (child).\nBaby elephant Compound word formed by agglutination of nouns  \n(elephant) and  (baby)\nTo the baby elephants Nominal sociative suffix to the plural form of the compound word \n (baby elephant)\nDo not stay awake Negative imperative mood of the verb  (be awake)\nWill be singing Future tense iterative aspect of the verb  (to sing).\nPage 3 of 25\nManohar et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:47 \n \nis a viable approach to solve the issues of data sparsity \nand OOV rates. When subword tokens are used for lan -\nguage modeling, a dummy symbol is added to identify \nthe positions where the tokens can be glued together to \nform words [10]. When subwords replace words, the ASR \nvocabulary contains morphemes, syllables, or other char -\nacter sequences that together can be used to create an \nunlimited number of words [11, 12].\nAn alternate approach to developing ASR models is \nthe End to End (E2E) architecture, which is heavily data \nintensive. In the context of our research on the morpho -\nlogically complex Malayalam language, we encountered a \nsignificant challenge related to the limited availability of \nannotated speech corpora under open licenses, making \nit a low-resource language. With less than 75 h of anno -\ntated data, training the E2E ASR system from scratch \nproved to be less practical due to the poor accuracy \nachieved with limited training data [13]. For improved \nspeech recognition accuracy, DNN-HMM methods are \nbest suited for languages with limited annotated speech \n[14]. Also, when much more text data is available than \nspeech data, DNN-HMM models are the preferred \nchoice [15, 16] than the modern E2E approaches. Addi -\ntionally, DNN-HMM ASR models offer the advantage of \neasy integration into small hardware devices, enabling \nfast on-device speech recognition [14].\nFig. 1 Comparing the type-token growth rate of Indian languages in comparison to English. Reproduced from [5]\nFig. 2 Block schematic representation of a DNN-HMM ASR system\nPage 4 of 25Manohar et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:47 \n1.3  DNN‑HMM ASR with subword tokens\nThe ASR system illustrated in Fig.  3, which utilizes sub -\nword tokens, differs from the conventional ASR system \ndepicted in Fig. 2 in terms of how the LM and the PL are \nconstructed. In the subword token-based ASR, the LM is \ntrained on a text corpus that is subword tokenized, and \nthe PL consists of pronunciation descriptions for sub -\nwords instead of the conventional word-based entries. \nSubword ASR decoder requires an additional module to \nreconstruct words from the decoded subword units as \nshown in Fig. 3.\nThe reconstruction from subwords to words is facili -\ntated by adding a dummy marker symbol [7]. In the \nexperiments we perform in this work, we use the con -\ntinuity marker “+” to the right side of the subwords, \nto indicate another subword has to follow it. In this \napproach, reconstruction is straightforward, as the \nmarker indicates the positions for joining the following \nsubword. Table 2 illustrates the usage of continuity mark-\ners in our experiments.\nFor languages where space and punctuation marks act \nas delimiters between words, segmenting the raw text \nof the language into word tokens is pretty straightfor -\nward. However, to segment text to subword units, there \nare data-driven as well as linguistically informed algo -\nrithms [1, 7]. Morfessor [11, 17, 18], byte pair encoding \n(BPE) [19, 20] and Unigram [21] are a few data driven \nalgorithms in popular use. These algorithms do not \nensure that subword tokenization happens at valid pro -\nnunciation boundaries. This makes precise representa -\ntion of its pronunciation as a sequence of phonemes \nimpossible.\nFor example, if the word SOPHIA  is segmented \nas SOP+ HIA, the pronunciation can not be segmented \nin a valid way. Then what is viable is to represent the \nsubword tokens in the lexicon with their pronunciation \ndescribed as a grapheme sequence. Tables 3 and 4 indicate \nhow these entries would be represented in a phonemic \nand graphemic lexicon respectively. In this work, we use \ngraphemic lexicons where graphemes would be mapped \nto acoustic features during acoustic model training. Sub -\nword tokens in lexicon entries have the continuity marker \n“+\" indicating it will be followed by another subword seg-\nment to complete a word.\nIn this paper, we propose a novel hybrid subword \ntokenization algorithm, Syllable - byte pair encoding \n(S-BPE), combining linguistic syllabification rules with \nthe data-driven tokenization method of BPE.\nFig. 3 Block schematic representation of DNN-HMM ASR system, with subword-based language model and pronunciation lexicon\nTable 2 Subword tokenization illustrating the usage of \ncontinuity marker symbol ‘+’\nOriginal text\nSubword tokenized text\nTable 3 Phonemic Lexicon\nWord Pronunciation\nSOPHIA\nTable 4 Graphemic Lexicon\nWord/subword Pronunciation\nSOPHIA S O P H I A\nSOP+ S O P\nHIA H I A\nPage 5 of 25\nManohar et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:47 \n \n1.4  Research objectives and key contributions\nThe objectives of this paper are (i) to provide an over -\nview of existing subword tokenization algorithms in \nMalayalam and to propose a hybrid algorithm of sub -\nword tokenization named S-BPE; (ii) to perform an \nanalysis of the impact of different subword tokenization \nfor Malayalam ASR in terms of WER, OOV-WER 1, and \nmodel memory requirement; and (iii) to contribute to \nthe research on Malayalam ASR by publishing the source \ncodes of our experiments 2 under permissive licenses to \nreuse and reproduce the results. We design our experi -\nments to answer four related research questions (RQ).\n• RQ1: Which subword tokenization method offers the \nmost effective language modeling?\n• RQ2: Does the subword tokenization method that \nexhibits the best language modeling complexity also \nresult in the lowest WER for ASR tasks?\n• RQ3: How does subword tokenization impact ASR \nperformance concerning model memory require -\nments and OOV recovery rate in a morphologically \ncomplex language?\n• RQ4: How do the amount of audio and textual train -\ning data and the n-gram order of the language model \naffect the effectiveness of subword token-based ASR \nin a morphologically complex language?\nTo answer our research questions, we create subword \ntoken-based n-gram language models using six tokeniza -\ntion algorithms: Word, Morfessor, BPE, Unigram, Sylla -\nble, and S-BPE. We analyze the complexity of language \nmodeling using a modified perplexity metric (explained \nin detail in Section  2.2). These language models are then \nused to build DNN-HMM ASR models. We evaluate the \nperformance of these ASR models based on metrics such \nas WER, OOV-WER, and the memory requirements of \nthe models. The main contributions and practical impli -\ncations of this study can be summarized as follows. \n1 We introduce a novel subword tokenization algo -\nrithm, called S-BPE 3, specifically designed for the \nMalayalam language.\n2 We investigate the impact of different subword \ntokenization algorithms on the complexity of lan -\nguage modeling. Our analysis reveals that tokeni -\nzation algorithms resulting in a higher average \nnumber of tokens per word tend to receive a sig -\nnificant penalty in terms of the complexity evalua -\ntion metric known as surprisal per sentence (SPS). \nAs a result, language models based on syllable-\nlevel tokenization exhibit the highest complexity, \nwhile word-level tokenization leads to the lowest \ncomplexity.\n3 We also found that the reduction in language mod -\neling complexity as measured by SPS does not neces -\nsarily imply a reduction in WER for ASR in morpho -\nlogically complex languages.\n4 With the proposed S-BPE tokenization algorithm, \nwe could achieve state-of-the-art (SOTA) results for \nMalayalam ASR. The S-BPE model results in the best \nWER (10.6%) and best OOV-WER (24.8%), which \nis significantly better than the best baseline WER \n(27.4%) and OOV-WER (100%).\n5 Additionally, the S-BPE model demonstrates model \nmemory requirements comparable to other subword \ntokens, but considerably lower than the baseline \nword model. The significance of a smaller memory \nrequirement happens in small hardware scenarios. It \nenables the efficient deployment of ASR models on \nresource-constrained devices, facilitating seamless \non-device speech recognition.\n6 Through a rigorous ablation study by varying the \nn-gram order and the amount of textual data used for \nlanguage modeling and the amount of speech data \nused for acoustic modeling, we present the influence \nof these factors on ASR performance, providing valu-\nable insights for optimizing ASR systems in the con -\ntext of subword tokenization.\n7 We provide open licenses for all source codes and \nmodels related to subword tokenization 4 and the \nresulting ASR models. This allows for public evalua -\ntion and facilitates further research advancements in \nthe field.\nIn the following sections, we describe the related works, \npresent the proposed algorithm, describe the experimen -\ntal setup, and analyze the results which lead to answers to \nthe research questions posed in the earlier section.\n2  Related works\nIn this section, we review various studies related to \nsubword tokenization in morphologically complex lan -\nguages. We begin with different subword tokenization \nalgorithms and explore the possibility of employing them \nfor language modeling in the Malayalam language. We \nthen explore the use of subword token-based language \nmodeling on the DNN-HMM ASR task and discuss the \nSOTA status of Malayalam ASR.\n1 OOV-WER: The WER exclusively for OOV words\n2 ASR Training script: https:// gitlab. com/ kavya manoh ar/ ml- subwo rd- asr\n3 S-BPE Code Repo: https:// github. com/ kavya manoh ar/ subwo rd- syl- bpe- ml\n4 Segmentation Models: https:// gitlab. com/ kavya manoh ar/ ml-  subwo rd- \nsegme ntati on\nPage 6 of 25Manohar et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:47 \n2.1  Subword tokenization algorithms\nSubword token-based language modeling has been pro -\nposed for applications in speech recognition [7, 11, \n12, 22, 23], statistical machine translation [24], neural \nmachine translations [20, 21] and handwriting recogni -\ntion [25]. The choice of subword tokens used in language \nmodeling impacts the performance of the model on many \ndownstream tasks [26] including speech recognition [7].\nA language model estimates the likelihood of word or \nsubword sequences to form a valid sentence. To estimate \nthis likelihood, the raw text of the language has to be \ntokenized into words or subwords. This section explains \nvarious tokenization algorithms proposed in the litera -\nture. The suitability of tokenization algorithms depends \non the task (speech recognition, machine translation, \ntext prediction, etc.) under consideration. Tokenization \ntechniques that could be adapted for the Malayalam lan -\nguage are used in the ASR experiments performed in this \nresearch work.\nTo perform tokenization that aligns with the Malay -\nalam script, it is important to analyze the nature of the \ngrapheme inventory of Malayalam. The graphemes in \nMalayalam script are classified as: (i) vowels; (ii) vowel \nsigns; (iii) regular consonants; (iv) special consonants: \nanuswara, visarga, and chili ; and (v) multi-functional \ncharacter: virama [4]. Vowel graphemes occur only at \nword beginnings. Regular consonants inherently have \nthe vowel  present in them. Vowel sounds at positions \nother than word beginnings are represented by vowel \nsigns. Vowel signs modify the inherent vowel sound \nof the consonants. A consonant cluster, also known as \na conjunct, in Malayalam is a sequence of consonants \nseparated by virama  in between, where virama  kills the \ninherent vowel from the preceding consonant [4]. Chillus \nare special consonants that do not have inherent vowels \nassociated with them. The characteristics of other special \nconsonants and virama  are marked in Table  5. The four \ntypes of syllable structures possible in Malayalam are \nlisted in Table 6 [27]. The syllable tokenization will make \nuse of these linguistic rules.\nSeveral approaches for tokenizing Malayalam text \nto meaningful morpheme units incorporating linguis -\ntic knowledge are reported in the literature. But for the \nreasons listed below, none of these could be used for the \nlanguage modeling task required for ASR. For Malay -\nalam morphological tokenization, earlier studies have \nused probabilistic, rule-based suffix-stripping, machine \nlearning, and dictionary-based approaches [28–31]. The \nmost recent deep learning technique uses Romanised \nMalayalam text and requires annotated data for training \n[32]. However, none of these research offers an applica -\ntion program interface (API) that can be programmed \nto perform morphological tokenization for use in down -\nstream applications. The only tool with a programmable \ninterface that works with Malayalam script performs \nmorphological analysis 5 and not morphological tokeni -\nzation [3]. For example, we need the compound word \n to be tokenized as \n, while its morphological analysis returns \n<noun><plural>. Morphological analysis is not appro-\npriate for an ASR task, as we expect to piece together the \noriginal word from the morpheme tokens by concatena -\ntion. For the ASR task, we, therefore, do not rely on any \nknowledge-based morpheme tokenization in Malayalam.\nThe data-driven tokenization algorithms are designed \nonly based on word spellings and do not have access to \npronunciation information. It is therefore possible for \nthese algorithms to break a word sequence into units \nthat do not imply well-formed correspondence to pho -\nnetic units. Pronunciation-assisted subword modeling \n(PASM) is an algorithm proposed to solve this issue by \nusing a pronunciation dictionary as an aligner to deter -\nmine the positions for tokenization [23]. To perform this \ntask, PASM needs a pronunciation dictionary. PASM \nTable 5 Special consonants and Virama sign in Malayalam\nCharacter Properties\nAnuswara Represents /m/ at syllable ends\nVisarga Introduces aspirated glottal stop\nChillu Dead consonants with no inherent vowel\nVirama Kills Inherent vowel in conjuncts\nInserts schwa at word ends.\nTable 6 Syllable structure in Malayalam with examples\na  Beginning of word\n b End of word\nType Syllable structure Example\n1 <BoW>a + vowel\n<BoW> + vowel + special consonant\n2 Consonant\nConsonant + special consonant\nConsonant + vowel sign + special consonant\n3 Conjunct\nConjunct + special consonant\nConjunct + vowel sign + special consonant\n4 Consonant+ virama + <EoW>b\nConjunct + + virama +<EoW>\n5 Mlmorph: https:// pypi. org/ proje ct/ mlmor ph/\nPage 7 of 25\nManohar et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:47 \n \nimplemented in the English language was reported in \n[23] using the CMUDict pronunciation dictionary. But \nmany low-resource languages do not have a ready-to-use \npronunciation dictionary. In the comparative analysis \nperformed in this work, we use several tokenization algo -\nrithms that break the pronunciation flow. So to be fair in \nthe comparison, we use only graphemic lexicons and not \nphonemic ones. In this scenario, PASM for tokenization \nis not used.\nThe tokenization techniques already reported in the lit-\nerature and that could be adapted for the Malayalam lan -\nguage and employed in the experiments reported in this \nwork are described in the following subsections.\n2.1.1  Word tokens\nIn Malayalam, the technique of word tokenization is \nsimple. After removing punctuation, the raw text cor -\npus is divided up by spaces. Given a word of length \nM, it requires no more splitting, and hence time com -\nplexity of further tokenization of a word is a constant, \ndenoted by O(1).\n2.1.2  Morpheme tokens\nMorfessor is a language-independent, data-driven \nmethod of subword tokenization. The Morfessor base -\nline algorithm is based on the minimum description \nlength principle [17]. It is an unsupervised technique \nin which frequently occurring sub-strings in several \ndifferent word forms from the training text corpus are \nproposed as morphs (or morpheme-like units) and \nthe words are then represented as a concatenation of \nmorphs [11]. Its current version, Morfessor2.0, has \na Python interface that may be customized and it sup -\nports annotated training data as well [18]. The Morfes -\nsor algorithm does not guarantee either tokenization at \nappropriate pronunciation boundaries or tokenization \ninto meaningful units.\n2.1.3  BPE tokens\nBPE is a data-driven algorithm that determines the opti -\nmal set of subword tokens through an iterative process. \nIt was originally proposed as a data compression algo -\nrithm [19]. The BPE algorithm splits the training data \ninto characters and creates an initial vocabulary. During \nfurther iterations, the most frequent character bigrams \nare determined, merged into a single token, and added \nto the vocabulary. The process is continued until a \ndesired number of merge operations are performed. The \nfinal vocabulary size is the sum of the initial vocabulary \nand the number of merge operations, which is a hyper-\nparameter [20]. The time complexity in training the BPE \nmodel is O(Nm) , where N is the corpus length and m the \nnumber of merge operations [33].\nThe subword tokens in the learned vocabulary are later \nused to segment any text. BPE ensures that the most \ncommon words are represented in the pronunciation dic-\ntionary as a single token while the rare words are broken \ndown into two or more subword tokens [20]. BPE tokeni-\nzation algorithm available in subword-nmt Python \nlibrary is used in the experiments described in this work6. \nThe time complexity in tokenizing a word of length M \nusing BPE implementation in [20] is O(M2)7 [33].\n2.1.4  Unigram tokens\nSubword regularization with Unigram language model, \nhenceforth refrerred to as Unigram tokenization [21] is \na language-independent tokenization algorithm. It makes \nthe assumption that the probability of the occurrence of \nsubword tokens in the sentence is independent of one \nanother. The vocabulary of the desired size is built from \na heuristically large vocabulary by retaining only η% (say \nη = 80 ) of the subwords in each iteration and discarding \nthe rest. The top 80% of subwords are obtained by rank -\ning all subwords according to the likelihood reduction of \nremoving them from the vocabulary. The most probable \ntokenization of an input sentence is determined by the \nViterbi algorithm. The Unigram tokenization algorithm is \navailable in the open source Python library, sentence-\npiece8, which is used in the experiments performed in \nthis work.\n2.1.5  Syllable tokens\nOrthographic syllable-based tokenization of text was \nproposed by Kunchukuttan et  al. for statistical machine \ntranslation applications [24]. Splitting the tokens based \non vowels and adjacent consonants, named vowel seg -\nmentation, was proposed by Adiga et  al. and employed \nin the context of Sanskrit speech recognition [22]. These \ntwo methods segment text into syllable-like units at valid \npronunciation boundaries.\nA syllabification algorithm tailored for Malayalam script \nusing finite state transducers has been proposed in [34]. \nThe linguistic rules for syllable tokenization described \nin Table 6 have been computationally implemented as in \nAlgorithm  1 and made available in the Mlphon Python \nlibrary9. The algorithm analyzes the input text sequence \nand determines whether it falls into one of the four allow-\nable categories of syllable structures in Malayalam. If \nit falls into any of these categories, it inserts tags (  \n6 subword-nmt: https:// pypi. org/ proje ct/ subwo rd- nmt/\n7 Byte Pair Encoding and Data Structures https:// guill aume- be. github. io/ \n2021- 09- 16/ byte_ pair_ encod ing\n8 sentencepiece: https:// pypi. org/ proje ct/ sente ncepi ece/\n9 https:// pypi. org/ proje ct/ mlphon/\nPage 8 of 25Manohar et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:47 \nand ) at the appropriate positions to indicate the \nbeginning and end of syllables. For a more comprehen -\nsive understanding of this algorithm, please refer to [ 34]. \nThis results in variable-length subword tokens where each \nsegment is a syllable with valid pronunciation. Since this \nalgorithm is implemented as a deterministic and mini -\nmized finite state automaton, the time complexity for \nthe syllable tokenization process is O(M) , where M is the \nnumber of characters in a word [35].\nThe perplexity, PPL(S), of a sentence, is the inverse \nprobability normalized by the number of tokens, N. \nNormalization ensures, longer sentences are not heavily \npenalised [8].\n(3)PPL(S) = 1\nP(S)\n1\nN\nAlgorithm 1 FST-based Syllabification Algorithm\n2.2  Language modeling analysis\nTokenization can mitigate the effects of rich mor -\nphology by breaking down highly inflected words into \nsmaller components [ 9]. Statistical n-grams serve as \na simple and powerful tool to capture language mod -\neling information. The order of n-gram needed to cap -\nture this information depends largely on the properties \nof the tokens used. The tokenization algorithm deter -\nmines the properties (the total number of tokens in the \ntext, the number of characters within each segment, \nthe number of tokens in a word, and the frequency of \ntokens) of the subword tokenized language modeling \ncorpus.\nPerplexity can be interpreted as the weighted average \nbranching factor of a language. The branching factor of \na language is the number of possible next words that can \nfollow any word. Higher perplexity is positively corre -\nlated with difficulty in language modeling [ 36]. For a sen-\ntence, S, formed by a sequence of N tokens S = s1 ,s2 ...sN  , \nthe probability P(S) of the sentence is given by the follow-\ning formula applying the chain rule of probability [36].\nBased on the Markovian assumption of n-gram lan -\nguage modeling, the probability of each word depends \nonly on the previous n − 1 words [ 36]. This makes the \nsentence probability to be computed as\n(1)\nP(S) = P(s1 ,s2 , ...sN )\n= P(s1 )P(s2 |s1 )...P(sN |sN − 1 ,sN − 2 ...s1 )\n(2)P(S) =\nN∏\ni= 1\nP(si|si− 1 ,si− 2 ..si− (n− 1 ))\nApplying logarithm base 2 on the Eq. (3), we get\nSince the number of tokens, N, in a sentence is largely \ndetermined by the tokenization method, the perplex -\nity measure that is dependent on this parameter can not \nbe used to compare modeling complexity across differ -\nent tokenization algorithms [ 8]. The negative log-like -\nlihood, NLL(S), of the sentence probability distribution, \neffectively removes this dependency as described in the \nfollowing equation and is called the surprisal of that sen-\ntence [9].\nBased on Eq. (4), this can be rewritten as\nBy scaling down the NLL(S) by the number of characters, \nM in a sentence, we obtain the character level surprisal, \nNLL c(S) as in Eq. ( 7) [37]. The number of characters in a \nsentence is a parameter independent of the tokenization \nused. This metric can also be used to compare language \nmodeling complexity across tokenization algorithms.\nAlternatively, word level surprisal, NLLw (S) , where \nthe scale factor is the number of words, W in a sen -\ntence, can also be used for comparison across tokeniza -\ntion algorithms [ 7, 12].\n(4)log2 PPL (S) =− 1\nN log2 (P(S))\n(5)NLL (S) =− log2 (P(S))\n(6)NLL (S) = Nlog2 (PPL (S))\n(7)NLL c(S) = N\nM log2(PPL(S))\nPage 9 of 25\nManohar et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:47 \n \nThe metric surprisal per sentence (SPS) is a measure \nthat quantifies the average surprisal of a corpus by divid -\ning the total surprisal of the corpus by the number of sen-\ntences it contains. To calculate SPS, we follow these steps: \n1 Compute surprisal: First, calculate the surprisal for \neach sentence in the corpus. This involves using a \nlanguage model to estimate the probability of each \nword or subword unit in the sentence given the pre -\nceding context.\n2 Sum surprisal: Add up the surprisal values for all sen-\ntences in the corpus to obtain the total surprisal.\n3 Calculate SPS: Divide the total surprisal by the num -\nber of sentences k, to compute the SPS. \nFor language modeling complexity comparisons across \ntokenization algorithms, SPS computation using NLL(S) \nis employed in [9], NLL c is used in [37], and NLLw (S) is \nused in [7].\n2.3  Subword‑based ASR\nSubword-level modeling has proved to be instrumental in \naddressing the challenges posed by morphologically com-\nplex languages. Language modeling on subword tokens \nin ASR for morphologically complex languages has been \nextensively studied [11, 38, 39] in the framework of DNN-\nHMM ASR systems. Representation using subword units \nallowed for a more granular representation of words. \nPromising results have been achieved by applying Morfes-\nsor-based tokenization to DNN-HMM ASR systems, lead-\ning to reduced WER and improved OOV word recovery in \nlanguages such as Finnish, Arabic, Swedish, and English [7].\nThe introduction of subword units, such as morphemes \nor BPE tokens, has shown improved performance in cap -\nturing the morphological richness of Indian languages. An \nanalysis of data-driven subword tokenization algorithms \nin Tamil and Kannada revealed significant reductions in \nWER compared to baseline word-based ASR systems. Spe-\ncifically, the study conducted by Pilar et  al. [40] reported \nan absolute WER reduction of 6.24% for Tamil and 6.63% \nfor Kannada. Alternately, by manually identifying word \nclasses and creating lists of prefixes, infixes, and suffixes \nfor subwords, a subword grammar model was developed \nfor Tamil and Kannada. This approach achieved even \ngreater improvements in ASR performance, with a maxi -\nmum absolute WER reduction of 12.39% for Tamil and \n13.56% for Kannada [41]. These findings demonstrate the \n(8)NLLw (S) = N\nW log2(PPL(S))\n(9)SPS = 1\nk\nk∑\ni=1\nNLL(S i)\neffectiveness of subword-level modeling techniques in \nIndian languages, highlighting their potential for enhanc-\ning ASR systems by effectively capturing the morphologi-\ncal complexity present in the languages.\nOpen vocabulary speech recognition in the Malayalam \nlanguage has received limited exploration so far. To the \nbest of our knowledge, the only prior work in this area was \nreported by Manghat et al. [12]. They proposed a tokeniza-\ntion technique that ensures the inclusion of rare subwords \nin the vocabulary, specifically focusing on Malayalam-\nEnglish code-switched ASR. Their study marked the first \nattempt to employ subword-based language modeling for \nthis particular language pair. Unfortunately, as the algo -\nrithmic implementation is not publicly available, we were \nunable to incorporate it into our experiments for this study.\nThe current work presented in this paper builds upon \nthe initial findings presented in [42], which primarily \nfocused on investigating the impact of using syllables as \nsubword tokens in Malayalam ASR. While the previous \nstudy served as an exploration of a single algorithm, the \npresent work represents a significant advancement. In the \ncurrent study, we have expanded the acoustic modeling \ndataset size by five fold and conducted a thorough anal -\nysis of five different subword tokenization algorithms, \nincluding Morfessor, BPE, Unigram, Syllable, and S-BPE \nalong with detailed ablation studies. These enhance -\nments provide a more comprehensive understanding of \nsubword-based approaches for ASR in the context of the \nMalayalam language.\n2.3.1  Comparison with other reported works\nWhen comparing ASR models, it is reasonable when a \ncommon benchmark dataset is used. Most of the works on \nMalayalam ASR are evaluated either on private datasets or \nthe exact test data split is not published. Hence, we attempt \nto compare our results with the previous work [42] which \nwas tested on the same test dataset as in the current work.\nNotably, our SOTA results demonstrate remarkable \nimprovements in ASR performance. In the previous \nwork, the best WER achieved on a medium OOV test set \nwas 26%. However, through the advancements made in \nthe current study, we have achieved a significant reduc -\ntion in WER. Specifically, by utilizing the proposed \nS-BPE method, we have achieved a remarkable WER of \n10.6%. These results underscore the substantial improve -\nments in ASR performance that can be achieved through \nthe utilization of subword-based modeling techniques for \nthe Malayalam language.\n3  Proposed subword tokenization algorithm\nIn this section, we present the details of our proposed \nsubword tokenization algorithm. It is built on top of the \nsyllabification algorithm using finite state transducers \nPage 10 of 25Manohar et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:47 \n(FST) explained in Section  2.1.5. This hybrid algorithm \ncombines the data-driven approach of BPE with the lin -\nguistic information about syllables.\n3.1  S‑BPE algorithm\nS-BPE is a hybrid algorithm that takes into account sylla -\nbles as irreducible units in the same way that BPE takes into \naccount characters. The traditional BPE algorithm oper -\nates at the character level, where it progressively merges \nthe most frequent pairs of characters into a single subword \nunit [20]. However, in the context of S-BPE, the algorithm \noperates on syllables instead of individual characters. The \npseudocode for the S-BPE training algorithm is described \nin Algorithm  2. The S-BPE training begins by initializing \nthe vocabulary, V, with all individual syllables present in the \ntraining data. It then iteratively applies the following steps: \n1 Frequency calculation: The algorithm counts the fre -\nquencies of all pairs of adjacent syllables ( SL : The left \nsyllable and SR : The right syllable) in the training cor-\npus, C.\n2 Pair merging: It identifies the most frequent pair of \nsyllables and merges them into a new subword unit, \nSnew . This merged subword unit is then added to the \nvocabulary.\n3 Updating the corpus: The algorithm updates the cor -\npus by replacing occurrences of the merged pair with \nthe newly created subword unit, Snew.\n4 Repeat: The algorithm continues to iterate, recalcu -\nlating frequencies of pairs in the updated training \ncorpus, merging the most frequent pairs, and updat -\ning the vocabulary until a predefined number of \nmerges (k) is reached. The number of merge opera -\ntions is set to k = 10, 000  , in our experiments.\nAlgorithm 2 S-BPE Training Algorithm\nThe algorithmic implementation 10 has been adapted \nfrom the original BPE algorithm in subword-nmt Python \nlibrary, and made available under MIT License [20].\nThe syllabification operation, being implemented as \nan FST based regular expression [34], has a linear time \ncomplexity, O(N) , where N is the number of characters \nin the training data. The time complexity of BPE tain -\ning is documented as O(sm) in [33], where s represents \nthe length of the input string (measured in terms of the \nnumber of syllables), and m denotes the number of merge \noperations. The maximum possible number of syllables s \nis equal to N, the corpus length in number of characters. \nThus the BPE portion of the algorithm may potentially \nhave a complexity of O(Nm) . To summarize, S-BPE algo-\nrithm has an overall time complexity determined by the \ndominant factor in O(N) and O(Nm) , which is O(Nm).\nOnce the training part is completed, the S-BPE model \nis created with a model vocabulary. To segment words \nusing S-BPE, the algorithm compares the input text with \nthe learned vocabulary. First, the text is syllabified using \na specific syllabification algorithm tailored for the Malay-\nalam script (Algorithm 1). It has a linear time complexity, \nie., syllabifying a word of M characters is O(M) . Then, for \nevery instance of the syllable sequence SL,SR in the text, the \nalgorithm replaces it with a newly created subword sym -\nbol Snew . The replacements are performed in the order in \nwhich the symbols were learned and added to the vocabu-\nlary, as in the original BPE implementation in [20] and this \nprocess has a time complexity of O(M2)11. The overall time \ncomplexity of S-BPE-based syllabification is determined \nby the dominant factor which is O(M2) . On a comparative \nscale, the S-BPE algorithm has the same time complexity \nas that of BPE, both during training and during tokeniza -\ntion. However, tokenization is only a one-time process in \nthe training of ASR models discussed in this work.\nThe S-BPE algorithm ensures that the most common \nwords in the corpus are represented by a single symbol \nin the vocabulary. On the other hand, rare words are \nbroken down into two or more subword tokens, while \nmaintaining valid pronunciation for each segment. This \ncombined process of knowledge-based syllabification and \ndata-driven BPE allows for effective subword tokeniza -\ntion. While the syllabification algorithm is specifically \ndesigned for the Malayalam script, the S-BPE algo -\nrithm can be extended to other languages that can be \nsyllabified.\nIn summary, the S-BPE algorithm leverages both knowl-\nedge-based syllabification and data-driven BPE techniques. \nIt creates a vocabulary of frequent syllable sequences dur-\ning training and uses this vocabulary to segment words into \nsubwords during tokenization, ensuring effective represen-\ntation of both common and rare words in the language.\n10 https:// github. com/ kavya manoh ar/ subwo rd- syl- bpe- ml/ tree/ sbpe\n11 https:// guill aume- be. github. io/ 2021- 09- 16/ byte_ pair_ encod ing\nPage 11 of 25\nManohar et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:47 \n \n4  Experimental setup\nThis section presents the details of our experiments 12. \nWe begin with a description of the datasets followed \nby the structure of the DNN-HMM ASR system. We \nwill describe in detail about the tokenization of the text \ncorpus for language modeling, the creation of subword-\ntokenized pronunciation lexicons, and the process of \nacoustic modeling.\n4.1  Datasets\nObtaining an adequate training corpus for Malayalam \nASR is challenging due to the limited availability of \ncomprehensive speech datasets. Creating such datasets \ninvolves resource-intensive tasks, including recruiting \ndiverse speakers, establishing recording environments, \nand ensuring accurate transcription. While there is a vast \namount of multimedia content available online, we have \nnot attempted to use it for ASR training tasks mainly \nbecause of the difficulty in obtaining high-quality, aligned \nspeech and text segments from the online content. To \naddress this limitation, we leveraged existing publicly \navailable open licensed speech datasets in Malayalam, \nsuch as Indic TTS [43], Open SLR 63 [44], IMaSC [45], \nMSC [46], and IIITH [47]. Among these, MSC has the \nhighest number of speakers but exhibits an imbalance in \nthe number of utterances per speaker. In contrast, Open \nSLR 63 offers a more balanced distribution of utterances \namong its 44 speakers, allowing for multi-speaker testing. \nConsequently, we partitioned the Open SLR 63 dataset to \nfacilitate multi-speaker testing.\nEach audio recording in the dataset is paired with a \ncorresponding textual transcript written in the Malay -\nalam script. The recordings are provided as wav files, \nwith a sampling rate of either 16 or 48 kHz and 16-bit \nprecision for each sample. For consistency during acous -\ntic model training, the higher sampling rate of 48 kHz is \ndownsampled to 16 kHz.\nThe speech dataset content is predominantly non-\nconversational in nature, with one dataset [46] recorded \nin natural environments. By including diverse speech \nsamples from natural settings, we aim to enhance the \nrobustness and generalizability of our findings. We divide \nthe available speech into train and test datasets, ensur -\ning zero speaker overlap. The train datasets described in \nTable 7 are combined to get approximately 69 h of audio \nfor acoustic modeling. The ASR models are tested on a \nsubset of the multi-speaker Open SLR 63 [44] dataset.\nTo create the language model, we use the sentences \nfrom the speech transcripts and combine it with the \ncurated collection of text corpus published by SMC [48]. \nThe resulting text corpus contains 227,686 sentences, \n1,425,504 word types, and 364,170 unique word tokens.\n4.2  DNN‑HMM ASR system\nThe DNN-HMM ASR decoder consists of three modules \nas described in Fig.  2. The functions of these modules are \nlisted below: \n1 Acoustic model: It predicts the posterior likelihood \np(P|X) of phone states P = p0 ,p1 , ...pK  given the \nacoustic feature frames X = x0 ,x1 , ...,xN  trained with \ndeep neural networks based on the frame level align -\nment of audio and phoneme labels obtained from a \npreviously trained GMM-HMM acoustic model [49].\n2 Pronunciation lexicon: It maps words into a sequence of \nphonemes. The acoustic model training module would \nneed to look up the pronunciation lexicon to convert \nthe word-level transcripts into phoneme sequences.\n3 Language model: It predicts the conditional likeli -\nhood p(w i+1 |w0 ,w 1 , ...wi) of the next word w i+1 \ngiven the previous words.\nIn the ASR decoder, all these components are composed \ninto a weighted finite-state transducer framework [50] \nand the most likely word sequence is retrieved using \ngraph search methods. This word-based system would \nserve as the baseline for our experiments. In a subword \nASR system described in Fig. 3, the pronunciation lexicon \nand the language model are subword based.\nTable 7 Details of speech datasets used in our experiments\nCorpus #Speakers #Utterances Duration (hours) Environment Usage\nIndic TTS, IITM [43] 2 8601 14 Studio Training\nOpen SLR 63 - Train [44] 37 3346 5 Studio Training\nIMaSC [45] 8 34,473 49 Studio Training\nMSC [46] 75 1541 1 Natural Training\nIIITH [47] 1 1000 1 Studio Development\nOpen SLR 63 - Test [44] 7 679 1 Studio Testing\n12 The Kaldi Experimental Setup: https:// gitlab. com/ kavya manoh ar/ ml- \nsubwo rd- asr\nPage 12 of 25Manohar et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:47 \nThe creation of an acoustic model, a subword-tokenized \ntext corpus for subword language model training, and the \ncreation of subword-tokenized pronunciation lexicons \nare explained in the following subsections.\n4.3  Acoustic modeling\nAcoustic modeling in speech recognition begins with \nextracting relevant features from the raw audio signal. \nThis process involves dividing the audio into frames of a \nfixed size using overlapping windows. To ensure smooth \ntransitions at the frame borders and to avoid frequency \nartifacts, a Hamming window [51] is applied. In our \nexperiments, we use a window size of 25 ms with a 10 ms \noverlap with the previous frame.\nOnce the speech signal is windowed, a fast Fourier trans-\nform (FFT) is applied to convert the signal from the time \ndomain to the frequency domain. The resulting spectrum \nis then transformed logarithmically to obtain the log-\nmagnitude representation. To capture the spectral charac-\nteristics relevant to speech recognition, the energy within \nspecific Mel frequency ranges is computed. These energy \nvalues are typically represented as Mel frequency cepstral \ncoefficients (MFCC), which provide a compact represen -\ntation of the speech signal’s spectral content [52]. MFCCs \nare commonly used as features in acoustic modeling due to \ntheir effectiveness in capturing the phonetic information \nnecessary for speech recognition tasks [53].\nIn addition to MFCCs, the inclusion of i-vectors as \nfeatures in the acoustic model training process is cru -\ncial [54]. These i-vectors play a vital role in effectively \nmodeling and addressing speaker variability, resulting in \nimproved recognition accuracy, especially in scenarios \ninvolving multiple speakers or unknown speakers. By \ncapturing and incorporating speaker-specific informa -\ntion, i-vectors enable the system to adapt and account for \nindividual speaker characteristics, ultimately enhancing \nthe robustness of the acoustic model.\nThe training of the DNN-HMM model begins with the \ncreation of a traditional HMM acoustic model, followed \nby utilizing the HMM state labels for each frame to train \nthe time delay neural network (TDNN) acoustic model \n[55]. This two-step process facilitates the incorporation \nof both the conventional HMM framework and the pow -\nerful representation learning capabilities of the TDNN, \nresulting in an enhanced acoustic model for improved \nspeech recognition performance.\nAcoustic features used in TDNN training are (i) \n40-dimensional MFCCs extracted from frames of 25 ms \nlength and 10 ms shift and (ii) 100-dimensional i-vectors \n[56] computed from chunks of 150 consecutive frames. \nThree consecutive MFCC vectors ( 3 × 40 dimension) and \nthe i-vector corresponding to a chunk (100 dimension) are \nconcatenated, obtaining a 220-dimensional feature vector \nfor a frame [14].\nThere are 16 layers of TDNNs, each working with dif -\nferent temporal contexts. Each layer is a succession of \ntypical DNN operations, such as affine transforms, ReLU \nactivations, and batch normalizations. Layers 2 to 13 use \nfactored form of TDNN with the subsampled connection \nbetween layers. No subsampling is used in the remaining \nlayers. All other hidden layers of the TDNN are trained in \nparallel. A declining learning rate was used, with an initial \nαinitial = 0.0015 and a final αﬁnal = 0.00015  . This acous -\ntic model is trained simultaneously with two discrimina -\ntive training criteria, one based on cross-entropy loss and \nthe other based on maximum mutual information [57]. \nThe dimension of the output layer is determined auto -\nmatically, based on the number of tied phoneme states. \nThe model is trained for 5 epochs where every layer uses \nL2 regularization to avoid overfitting. To achieve opti -\nmal WER, we made parameter adjustments motivated by \nimprovements in WER on a development speech corpus, \naccounting for the interplay between the acoustic model, \npronunciation lexicon, and language model. The model is \ntrained on a single Nvidia Tesla T4 GPU.\n4.4  Creating subword tokenized text corpora\nSubword-based ASR, as shown in Fig.  3, is very much \nlike a word-based ASR system, except that (i) the lan -\nguage model represents the conditional probability of \nsubword sequences, instead of words and (ii) the pronun-\nciation lexicon is composed of subword tokens. The word \nboundary marker is chosen so that the predicted subword \ntokens can be easily concatenated to form words. We use \nthe tokenization algorithms described in Sections  2.1 \nand 3.1, and compare them with the baseline word-based \nASR to answer the research questions.\nData-driven and hybrid tokenization algorithms \nrequire a training corpus for learning the model param -\neters, which is then applied to the target text to obtain \na subword tokenized text corpus. Morfessor, BPE, and \nUnigram are data-driven tokenization algorithms while \nS-BPE is a hybrid one that additionally relies on linguistic \nknowledge. As the training corpus, we set aside a subset \nof the entire text corpus (7.5k sentences). \n1 Words are separated by spaces in the text corpus and \nare thus already segmented.\n2 Morfessor model is trained using the morfes -\nsor python library [18]. The training stops when \nthe decrease in the model cost of the last iteration \nis smaller than finish_threshold value of 0.005. \nThe trained model is applied to create the mor -\npheme-tokenized text corpus.\nPage 13 of 25\nManohar et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:47 \n \n3 BPE [20] learns the vocabulary from the train -\ning dataset. The initial vocabulary is formed by the \nMalayalam characters in the training dataset. The \nnumber of merge operations is set to 10,000. This \nresults in a BPE model which is used to obtain the \nBPE tokenized text corpus.\n4 Unigram [21] model is trained by the sentence \npiece library using the training dataset with a \nvocabulary of 15,000. The trained Unigram model is \nused to get the Unigram tokenized text corpus.\n5 Being a rule-based algorithm, the syllabifier requires \nno training. Algorithm 1 is directly applied to the text \ncorpus to obtain syllable tokenized corpus.\n6 The S-BPE model is trained using the Algorithm 2 so \nthat the vocabulary is learned. The initial vocabulary \nis formed by the Malayalam syllables present in the \ntraining dataset. The number of merge operations is \nset to 10,000. This results in a model which is used to \nobtain S-BPE tokenized text corpus.\nSamples of text tokenized using these methods are pre -\nsented in Table 8. These examples indicate how the num-\nber of tokens per sentence varies with the method of \ntokenization.\n4.5  Language modeling\nIn the experiments performed in this work, we report \nSPS computed on NLL (S) for measuring language mod -\neling complexity. Statistical n-gram language modeling is \nperformed on the subword tokenized text corpus. SRILM \ntoolkit is used for the training and evaluation of lan -\nguage models [58]. To avoid zero probability assignment \nto unseen word sequences, the probability weights are \nredistributed by a process known as smoothing. We use \nthe modified Kneser-Ney smoothing algorithm [59] to \ncreate n-gram language models of orders 2 to 6 for every \ntokenization algorithm. The models are trained to pre -\ndict the next segment based on the previous n-gram con -\ntext. The SRILM toolkit can evaluate the test dataset and \nreturn the log-likelihood values with respect to base 10 \nlogarithms and the perplexity. Surprisal values are com -\nputed by converting these values to base 2 logarithms.\n4.6  Creating subword tokenized lexicons\nThe graphemic lexicon describes the pronunciation using \nthe language’s native alphabets, or graphemes. Since BPE, \nUnigram, and Morfessor tokenization algorithms in our \nexperiments do not have access to pronunciation informa-\ntion, the tokenization can happen at locations that break the \npronunciation flow. So, it was decided to use a graphemic \npronunciation lexicon, instead of a phonemic one [7] for all \nthe tokenization algorithms to ensure fair comparison.\nFor the baseline ASR, the word pronunciation lexicon \nis prepared by using all the words in the text corpus with \nat least three occurrences. It is then expanded to include \nall the words in the training speech transcript. This word \nlexicon is referred to as PLword and has 79,947 entries. \nSubword lexicons are obtained by segmenting every word \nentry in PLword as per the tokenization algorithm under \nconsideration and choosing the list of unique tokens as \ndescribed in Algorithm  3. This involves the following \nsteps. \n1 Initialize an empty list to store the subword tokens.\n2 Iterate over each word in the input word lexicon.\n3 Tokenize the current word into subword units.\n4 Add these to the list of subword tokens.\n5 Repeat this process for all words in the input lexicon.\n6 Make the list of subword tokens unique by removing \nduplicates.\n7 Generate pronunciations for each unique subword \ntoken.\nAlgorithm 3 Subword Lexicon from Word Lexicon\nTable 8 Examples for different tokenization algorithms. Space \nis used as delimiter between tokens. Number of tokens per \nsentence is also tabulated\nMethod Example Segment \ncount\nWord 3\nMorfessor 6\nBPE 6\nUnigram 5\nSyllable 9\nS-BPE 4\nPage 14 of 25Manohar et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:47 \nThe number of entries in these lexicons is described \nin Table 9.\n4.7  Summary of experimental investigations\nThe acoustic models are built and combined with lan -\nguage models and pronunciation lexicons using the \nKaldi toolkit [60]. From six ways (word, morfessor, \nBPE, unigram, syllable, S-BPE) tokenized text corpus, \nwe construct language models with n-gram orders of \n2 to 6. The language modeling effectiveness is then \nmeasured using corpus level and information theo -\nretic metrics. Keeping the Kaldi-based TDNN acous -\ntic model fixed across tokenization algorithms, we use \nsubword tokenized lexicons and corresponding lan -\nguage models to create 30 (1 acoustic model × 6 sub -\nword tokenized lexicons × 5 n-gram orders) different \nASR decoders. These decoders are then tested on a \nmultispeaker test dataset described in Table 7 .\n5  Results\nIn this section, we present the findings from our experi -\nments. We first perform a corpus linguistic analysis on \nthe subword tokenized corpora. After that, we analyze \nthe language model. This is done in terms of the metric \nSPS, which roughly indicate the complexity, and the over-\nall difficulty that the model has in predicting sentences \n[8]. Finally, we analyze the ASR results. The WER, OOV-\nWER, lexicon size, and overall model size are used to \nmeasure this.\n5.1  Corpus linguistic analysis of the LM\nWords can be broken down into smaller pieces that are \nlikely to convey similar meanings in different contexts \nby segmenting them into subwords, which can lessen \nthe impact of rich morphology. We analyze the linguistic \nproperties of these tokens in this section.\n5.1.1  Linguistic validity of tokens\nThe tokens given by different methods, as exemplified \nin Table  8, do not necessarily comply with linguistic \ncorrectness. The word tokens are orthographically \nand phonetically valid linguistic units. The tokeniza -\ntion given by the Morfessor tool is not true morpheme \ntokens. The Morfessor tokens break the orthographic \nflow as in  being subword tokenized as \n. In the second segment, the vowel \nsign , occurs without a consonant preceding it, which \nis an invalid orthographic usage. Similar invalid ortho -\ngraphic usages can be observed in BPE and Unigram \ntokenization algorithms too.\nSyllable tokenization method, by its design, always \ngives orthographically valid subword units. The S-BPE \nmethod also gives orthographically valid subword units, \nwhich are longer than syllable tokens. But none of the \nmethods are capable of providing linguistically meaning -\nful subword tokens. However, unlike machine transla -\ntion applications, this is not an essential requirement for \nbuilding an ASR system.\n5.1.2  Mean length of tokens\nThe mean length of tokens is the average number of charac-\nters in a token and it depends on the tokenization algorithm. \nThe distribution of token lengths, in the form of box plots is \nshown in Fig. 4. It is the highest for words (8.3) as expected \nand the smallest is for syllables (2.2). The mean token length \nfor Morfessor, Unigram, BPE, and S-BPE tokenization algo-\nrithms are 3.9, 4.3, 4.5, and 4.8, respectively.\nA comparatively smaller box for syllables indicates \nthe length is distributed closely about the median value, \nwith very few outliers. However, for word tokenization, \nthe length of the box plot is larger, indicating the seg -\nment lengths vary widely.\n5.1.3  Token count per word and per sentence\nThe distribution of the number of tokens per word in \nthe test dataset is illustrated in Fig.  5. Word tokeniza -\ntion does not break down the words, resulting in a \nsingle bar graph. In BPE, Unigram, and S-BPE tokeni -\nzation algorithms, more than 50% of the words remain \nunsegmented, followed by words being tokenized into \ntwo subwords. In Morfessor tokenization, the distribu -\ntion shows more than half the words are tokenized into \ntwo, followed by words remaining unsegmented. The \npercentage of words that get segmented into more than \ntwo tokens is rare in all these methods. However, in syl -\nlable tokenization, about 28% and 24% of words get seg -\nmented into two and three subwords respectively. The \ntoken length per word is more broadly distributed in \nsyllable tokenization.\nOn analyzing the tokenization statistics over sen -\ntences, we get the values reported in Table  10. It \nTable 9 Lexicon Sizes of different tokenization algorithms\nSegmentation Lexicon size\nWord 79,947\nMorfessor 10,545\nBPE 9986\nUnigram 19,564\nSyllable 6279\nS-BPE 15,926\nPage 15 of 25\nManohar et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:47 \n \ndescribes the minimum, maximum, and mean number \nof tokens per sentence. Syllable tokenized sentences \ncontain on average 19.9 subword tokens, which is the \nhighest count of all. Sentences that contain a large \nnumber of tokens would need a longer n-gram lan -\nguage model context to guide the decoding [7 ]. We will \nanalyze its impact on ASR later in this section.\n5.2  Information theoretic analysis of the LM\nThe complexity of a language model is related to its dif -\nficulty in determining the next segment from the previ -\nous n-gram context. The higher order n-grams extract \nmore context for the occurrence of a segment and gen -\nerally reduce language modeling complexity and hence \nperplexity and surprisal. However, raising the n-gram \norder beyond a limit reintroduces the data sparsity prob -\nlem, resulting in unimproved perplexity and surprisal val-\nues [37]. Subword language models require higher-order \nn-grams to capture the context than word-based ones [40]. \nIn our experiments, we create language models of orders \nn=2 to 6 and analyze their complexity in terms of SPS.\nThe SPS values obtained in our experiments are shown \nin Table  11. For every tokenization method, with the \nincrease in n-gram order, the SPS reduces initially and \nthen stabilises. The best set of SPS values are obtained for \nthe word segment-based language model. Our investiga -\ntion demonstrates that tokenization algorithms yielding a \ngreater average number of tokens per sentence are asso -\nciated with a notable increase in the complexity evalua -\ntion metric SPS. Consequently, language models utilizing \nsyllable-level tokenization demonstrate the highest com -\nplexity, whereas word-level tokenization yields the lowest \ncomplexity. Syllable tokens of lower n-gram orders show \nhigher SPS values than all other tokenization algorithms.\nThe impact of subword token-based language mode -\nling on the ASR decoder needs to be evaluated in terms \nof its ability to recover OOV words and a correspond -\ning reduction in WER, which is attempted in the fol -\nlowing section. However, lowering the language model \ncomplexity does not always ensure an improvement in \nautomatic speech recognition accuracy [7 , 61].\n5.3  WER for each tokenization algorithm\nTo begin with, we present the ASR error rate which is \ncomputed as WER. It is based on the number of words \ninserted (I), deleted (D), and substituted (S) in the pre -\ndicted speech transcript when compared to the ground \ntruth transcript according to Eq. (10), where N repre -\nsents the total number of words in ground truth tran -\nscript [62].\nThe evaluation is performed on a multi-speaker studio \nrecorded dataset. About 14% of words in this test dataset \nare OOV words, which can not be recovered by word-\nbased ASR. According to [63], it has been shown that \nthe presence of an OOV word in the test set can result in \napproximately two errors during ASR decoding.\n(10)WER = (I+ D + S) × 100\n(N )\nFig. 4 Distribution of token length\nPage 16 of 25Manohar et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:47 \nFigure 6 presents the best set of WER obtained for dif -\nferent tokenization algorithms. To study the performance \nof subword-based ASR compared to the baseline word \nmodel on OOV recovery, we compute the WER, specifi -\ncally for OOV words. The OOV words in the test set are \ndetermined with respect to the word-level lexicon. To \nanalyze the extent of OOV-WER in subword token-based \nASR, we use the texterrors Python library[64]. Pro -\nviding the list of OOV words in the test set along with the \ntrue speech transcripts, this library computes the OOV-\nWER of the subword ASR model.\nThe baseline method, using words as the tokenization \nunits, achieves a WER of 27.4% but suffers from a high \nOOV-WER of 100.0%, indicating that it struggles with \nwords not present in the pronunciation lexicon. Among \nthe alternative tokenization algorithms, Morfessor \nachieves a WER of 12.8% and significantly reduces the \nOOV-WER to 26.6%. BPE and Unigram tokenization also \nshow competitive performance with WERs of 11.0% and \n11.9% respectively, but their OOV-WERs remain close to \nthat of Morfessor.\nSyllable tokenization, while having a relatively higher \nWER of 13.5%, manages to achieve a lower OOV-WER of \n24.8% compared to other methods. This is because syllables \nbeing the most granular of all tokenization algorithms, pro-\nvide more opportunities for partial matching with available \nFig. 5 Distribution of the number of tokens per word in the text corpus\nTable 10 Sentence length statistics in terms of the number of \ntokens per sentence\nTokenization Minimum Maximum Mean\nWord 5 14 6.4\nMorfessor 6 29 11.7\nBPE 5 26 8.5\nUnigram 5 29 10.1\nSyllable 8 49 19.9\nS-BPE 5 25 8.1\nPage 17 of 25\nManohar et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:47 \n \nlexical units, enabling better recovery of OOV words in \nASR systems. However, this same property also makes syl-\nlable token-based ASR less suitable for general words, as it \nrequires the decoder to recover a higher number of tokens \nper sentence, increasing the likelihood of errors.\nNotably, the proposed hybrid S-BPE tokenization \noutperforms all other methods with the lowest WER of \n10.6% and an OOV-WER of 24.8%, demonstrating its \neffectiveness in improving ASR performance. While \nboth S-BPE and syllable tokenization exhibit compa -\nrable OOV-WER, S-BPE holds the added advantage of \nsuperior performance on non-OOV words. This is due \nto its ability to strike a balance between granularity and \ncoverage. Overall, the results indicate that alternative \ntokenization algorithms offer improvements over the \nbaseline word tokenization in terms of both WER and \nOOV-WER, with S-BPE yielding the best performance in \nthis evaluation.\n6  Ablation studies\nIn the preceding section, we presented the optimal WER \nachieved for each tokenization algorithm, leveraging 69 \nh of speech data for acoustic modeling and 227,686 sen -\ntences of textual data for language modeling, employing \nan n-gram order of n = 6 . In this section, we investi -\ngate the influence of various tokenization algorithms by \naltering the n-gram order and adjusting the quantity of \nspeech and textual training data used in the experiments. \nOur aim is to gain deeper insights into how these factors \nimpact ASR performance and identify the most effec -\ntive combination of tokenization and n-gram order to \nTable 11 Language modeling complexity in terms of SPS. Lower SPS implies lower complexity\na  Morfessor\n b Unigram\n c Syllable\nn‑gram Word Morf.a BPE Uni.b Syl.c S‑BPE\nSPS\n2 45 88 82 108 157 78\n3 42 68 64 84 109 62\n4 42 63 61 79 93 60\n5 42 62 61 78 85 60\n6 42 62 61 79 82 60\nFig. 6 The best WER for each tokenization method and the corresponding OOV-WER\nPage 18 of 25Manohar et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:47 \noptimize the accuracy and memory efficiency of the ASR \nsystem.\n6.1  Impact of n‑gram order on WER for different \ntokenization algorithms\nIn this analysis, we investigate the impact of the \nn-gram order on the WER for different tokenization \nalgorithms utilized in subword token-based ASR. The \nresults are illustrated in Fig.  7. For ASR systems based \non word tokens, the n-gram order shows little or no \neffect on the WER. However, in other subword token-\nbased ASR models, we observe a significant reduc -\ntion in WER, by at least 4%, when the n-gram order \nincreases from n = 2 to n = 3 . Further increases in the \nn-gram order beyond n = 3 result in only marginal \nimprovements in WER.\nCompared to the baseline word token-based ASR, all \nsubword token-based ASR models perform better at all \nn-gram orders, except for the syllable token-based ASR \nat n = 2 . Particularly, the S-BPE token-based ASR out -\nperforms other tokens at corresponding n-gram orders.\nOverall, this analysis offers valuable insights into how \nthe n-gram order impacts WER for various tokenization \nalgorithms in subword token-based ASR. It highlights \nthe superiority of subword token-based approaches, \nespecially S-BPE tokenization, and underscores the \nimportance of choosing the right n-gram order to opti -\nmize ASR accuracy effectively.\n6.2  Impact of n‑gram Order on ASR model memory \nrequirement for different Tokenization algorithms\nThe order of the n-gram impacts the memory require -\nment of the ASR model. To study the model memory \nrequirement, we computed the size of the weighted \nFST graph (HCLG.fst) used for decoding. HCLG.\nfst is composed of four FSTs namely, H.fst, C.fst, \nL.fst, and G.fst. The H.fst and C.fst together \nform the acoustic model, L.fst the phonetic lexicon, \nand G.fst the grammar of the language model. Thus \nthe total memory includes the model size for both the \nacoustic model and the language model combined.\nThe bar plot in Fig.  8 illustrates the impact of n-gram \norders on the model memory requirement of dif -\nferent ASR models employing various tokenization \nalgorithms. The x -axis represents the tokenization algo -\nrithms and the y -axis shows the ASR model memory \nrequirement in Megabytes (MB).\nFrom the plot, we observe that the n-gram order sig -\nnificantly affects the memory requirement of ASR mod -\nels for most tokenization algorithms. As the n-gram \norder increases, the model memory requirement gen -\nerally tends to rise across all tokenization algorithms. \nFig. 7 WER comparison for different tokenization algorithms in ASR with varying n-gram order\nPage 19 of 25\nManohar et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:47 \n \nThis is expected as higher n-gram orders lead to larger \nlanguage models, which require more memory to store \nthe increased contextual information.\nNotably, the word token-based ASR (baseline) shows rel-\natively consistent memory requirements regardless of the \nn-gram order. However, other subword token-based ASR \nmodels, experience notable increases in memory require-\nment with higher n-gram orders. For n-gram orders ≤ 3, \nthe syllable tokenization method exhibits the lowest mem-\nory requirement while S-BPE tokenization requires the \nmost memory among the subword token-based models.\nBased on the discussions about the WER and the \nmodel memory requirement, we find that higher n-gram \norders, which lead to improved WER, come at the cost \nof increased model memory requirement. There exists a \ntrade-off between achieving better ASR accuracy through \nmore extensive context capture (higher n-gram order) \nand the computational resources needed to accommo -\ndate the larger language model in memory.\n6.3  Trade‑off between WER and model memory \nrequirement\nThrough our investigation, we have observed that \nsubword tokens of n-gram orders ≤ 3, exhibit a con -\nsiderably smaller WER compared to the correspond -\ning word-based models while having a lower model \nmemory requirement. For n-gram orders above 4, \nthe model size increases substantially without much \nimprovement in WER and hence is not recommended. \nA comparative analysis of the WER and model size is \npresented in Table 12, for n-gram = 3.\nIn the trade-off diagram shown in Fig.  9, the model \nsize of the word-based baseline ASR model does not \nchange significantly with the model size. However, the \nerror rate of the word-based baseline model is higher \nthan all subword-based models, except for the syllable \nbigram ASR. Although the syllable bigram ASR has the \nsmallest model size, its error rate is so high that it is not \npractical to use it.\nFig. 8 Size of ASR model (MB) for different tokenization algorithms in ASR with varying n-gram order\nTable 12 Comparing the WER and model size of each subword \ntokenization method, at n-gram = 3. The relative reduction with \nrespect to the baseline word model is also shown in percentage\nSegmentation WER (%) Model size (MB)\nWord (baseline) 27.4 123\nMorfessor 11.7 ↓ 57% 104 ↓ 15%\nBPE 13.7 ↓ 50% 90 ↓ 26%\nUnigram 12.6 ↓ 54% 108 ↓ 12%\nSyllable 14.7 ↓ 46% 94 ↓ 23%\nS-BPE 11.4 ↓ 58% 110 ↓ 11%\nPage 20 of 25Manohar et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:47 \nThe performance of all subword models is better than \nthe baseline, except for the bigram syllable language \nmodel. Syllables with the smallest mean length of tokens \namong all tokenization algorithms, require more n-gram \ncontext to make a reasonable prediction about the next \nsubword segment. But once it has enough context, the \npredictions become more reliable as exemplified in the \nWER reduction in Fig. 9. The subword token-based ASRs \nshow consistent improvement in WER with the increase \nin n-gram order. But the relative improvement dimin -\nishes with the n-gram order.\nResearchers and practitioners need to strike a balance \nbetween n-gram order selection, WER performance, \nand available computational resources to design effi -\ncient and effective ASR systems. This involves consider -\ning the desired ASR accuracy, memory constraints, and \ncomputational capabilities when making decisions about \nn-gram order and other language modeling parameters \nto optimize ASR performance.\n6.4  Impact of LM training data on WER for different \ntokenization algorithms\nTo investigate the influence of LM training data on the \nWER of the ASR model, we conducted experiments by \ncreating language models using varying amounts of tex -\ntual data, ranging from 12.5 to 100% of available 227,686 \nsentences utilizing an n-gram order of 3. The choice of \nn-gram was based on the fact that beyond n-gram=3, \nthere would be a significant increase in model memory \nrequirement without much improvement in WER. These \nlanguage models were then combined with acoustic \nmodels built using the entire available audio corpora of \nabout 69 h.\nThe plot in Fig.  10 depicts the relationship between \nWER and the percentage of available text corpora used \nfor LM training. As the percentage of LM training data \nincreases, there is a consistent reduction in WER for all \ntokenization algorithms. This demonstrates that more \nextensive LM training data leads to improved ASR accu -\nracy, regardless of the tokenization approach used.\nIn the analysis, we observe that each tokenization algo -\nrithm exhibits distinct WER performance across different \namounts of LM training data. Initially, when LM training \ndata usage is low, all subword tokenizations demonstrate \ncomparable performance. However, as the percentage of \nLM training data increases, the WER for these tokeni -\nzations starts to diverge. Morfessor, BPE, and Unigram \ntokenizations show competitive performance compared \nto the S-BPE tokenization, especially at lower levels of \nLM training data usage. However, with the increase in \nLM training data, the S-BPE tokenization consistently \noutperforms the others, showcasing the most robust \nWER reduction across all data sizes above 25%.\nHowever, as the amount of LM training data increases, \nsyllable tokenization, which initially showed competi -\ntiveness at lower data usage, gradually loses its competi -\ntive edge compared to other subword tokenizations. Its \nWER performance does not improve at the same rate as \nthe other subword tokenizations, making it less favorable \nwhen utilizing the full available training data. In contrast, \nthe word tokenization approach exhibits the highest \nWER among all algorithms, indicating its limitation in \ncapturing the complexities of the language, especially in \nmorphologically rich languages.\n6.5  Impact of AM training data on WER for different \ntokenization algorithms\nTo investigate the influence of audio training data on the \nWER of ASR model, we conducted experiments by cre -\nating acoustic models using varying amounts of speech \ndata, ranging from 4.5 to 69 h. These acoustic models \nwere then combined with language models built using \nthe entire available text corpora, utilizing an n-gram \norder of 3. The choice of n-gram was based on the fact \nthat beyond n-gram=3, there would be a significant \nincrease in model memory requirement without much \nimprovement in WER (Fig. 11).\nAs the amount of training data increases, all tokeni -\nzation algorithms show a clear reduction in WER. This \nFig. 9 Trade-off between ASR WER and Memory requirement. The \nn-gram order is indicated as labels within the circles\nPage 21 of 25\nManohar et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:47 \n \nfinding depicted in Fig.  11 indicates that more extensive \nand diverse training data leads to improved ASR accu -\nracy across all subword tokenization approaches.\nEvery subword tokenization algorithm maintains its \nrelative position in terms of WER at all levels of available \ntraining data. This is because the tokenization algorithms \ndirectly impact the LM modeling and not the acoustic \nmodeling and our LM model is constant thought out this \nexperimental investigation. Among the analyzed tokeni -\nzation algorithms, the S-BPE consistently exhibits the \nlowest WER.\n7  Analysis and discussions\nBased on our detailed experimental investigations on \ndeveloping subword-based DNN-HMM ASR models for \nMalayalam, we have reached answers to our research \nquestions posed in Section 1.4.\nRQ1: Subword and word token-based language models \nwere compared based on their complexity using the SPS \nmetric. The results showed that all subword tokeniza -\ntions resulted in a higher number of tokens per sentence \ncompared to word-based tokenization. Consequently, the \nsubword token-based language models exhibited higher \nvalues for SPS, indicating higher complexity compared \nto word token-based LMs. Syllable tokens demonstrated \nthe highest values, suggesting that their higher granu -\nlarity and token count adversely impacted the overall \ncomplexity of the language models. This shows the sub -\nword tokenization did not improve the LM modeling effi-\nciency when evaluated using the SPS metric.\nRQ2: The WER and LM complexity measured by the \nSPS metric were found to be uncorrelated. While sub -\nword tokenization did not significantly improve the \nintrinsic LM complexity metric, it indeed led to improved \nASR performance compared to word tokenization. This \nis because ASR involves additional complexities related \nto acoustics, pronunciation variations, and OOV words. \nThese factors influence WER independently of the LM \ncomplexity. Thus the subword tokenization method that \nexhibits the best language modeling complexity does not \nlead to the lowest WER for ASR tasks.\nRQ3: Subword token-based ASR models exhibit \nreduced WER and decreased model memory require -\nments, especially when the n-gram order is less than \nn = 4 when compared to the baseline word token-based \nASR. This finding suggests that subword tokenization can \nbe highly beneficial for ASR tasks, particularly in mor -\nphologically complex languages or datasets with a large \nvocabulary. The hybrid method of S-BPE tokenization \nproposed in this work exhibited the lowest WER over \ndiverse n-gram orders and AM and LM training data \nusage. Both S-BPE and Syllable token-based ASR could \nrecover many OOV words resulting in the lowest OOV-\nWER of 24.8%.\nFig. 10 WER vs. amount of textual data as a percentage of total available text corpora\nPage 22 of 25Manohar et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:47 \nRQ4: The n-gram order significantly influences WER and \nASR model size, with limited WER improvements beyond \nn-gram = 3, but higher orders increase model memory \nsignificantly. The optimal trade-off between WER and \nmemory requires careful consideration of n-gram selection. \nAdditionally, increasing the amount of AM training data \ngenerally leads to improved WER across all tokenization \nalgorithms. On the other hand, the impact of increased LM \ntraining data varies for different tokenization algorithms. \nSome subword tokenizations (Morfessor, BPE, Unigram, \nand S-BPE) benefit more from additional LM training data, \nwhile others (word and syllable) did not show significant \nimprovements. This underscores the importance of consid-\nering the interaction between tokenization algorithms and \nthe amount of training data, both for LM and AM, to opti-\nmize ASR accuracy effectively. In all these ablation studies, \nS-BPE stood out with the best WER.\n8  Conclusions\nThe presented study holds significant importance as it \nrepresents the first comprehensive investigation into \nimproving speech recognition systems for the morpho -\nlogically complex Malayalam language using subword \nlanguage modeling techniques. By exploring various \nsubword tokenization algorithms, we have conducted a \ndetailed analysis of statistical n-gram language models’ \nusage in the context of a hybrid ASR task.\nThe results of our study have demonstrated the excep -\ntional performance of the proposed hybrid S-BPE tokens, \nachieving a remarkable 10.6% WER, which represents a \n16.8% improvement over the baseline word-level ASR. \nWhile the linguistically informed syllable tokenization \napproach yielded a WER of 13.5%, it was unable to sur -\npass the WER performance of other data-driven tokeni -\nzation algorithms.\nThe comprehensive ablation study highlights that \nincreasing the n-gram order of the language model \nbeyond n = 3 offers little benefit, as it leads to sig -\nnificant model size growth without substantial WER \nimprovement. On the other hand, augmenting the \nacoustic model training data consistently enhances \nWER across all tokenization algorithms. However, for \ndata-driven tokenizations, increasing the LM train -\ning data proves especially beneficial, outperform -\ning word and syllable tokenizations in terms of WER \nimprovement.\nIn conclusion, the adoption of S-BPE subword \ntokens offers the advantage of reduced model memory \nrequirements. It enables the efficient deployment of \nASR models on memory-constrained devices, facilitat -\ning on-device speech recognition. Additionally, S-BPE \nand syllable subwords exhibit the lowest error rate for \nout-of-vocabulary words, effectively identifying more \nthan 75% of these words, with an OOV-WER of 24.8%. \nThe findings highlight the benefits of subword tokeni -\nzation, including decreased model memory demands \nand improved accuracy, thereby greatly benefiting lan -\nguages with complex morphology like Malayalam.\nFig. 11 WER vs. Amount of training data in hours\nPage 23 of 25\nManohar et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:47 \n \nBy addressing the challenges specific to Malayalam \nand offering valuable insights into subword tokeniza -\ntion techniques, our research makes a significant con -\ntribution to the field of speech recognition and lays the \nfoundation for further advancements in ASR systems \nfor morphologically rich languages.\nAbbreviations\nAM  Acoustic model\nAPI  Application program interface\nASR  Automatic speech recognition\nBPE  Byte pair encoding\nDNN  Deep neural network\nE2E  End to End\nFFT  Fast Fourier transform\nFST  Finite state transducers\nHMM  Hidden Markov model\nLM  Language model\nMB  Megabytes\nMFCC  Mel frequency cepstral coefficients\nOOV  Out of vocabulary\nPASM  Pronunciation-assisted subword modeling\nPL  Pronunciation lexicon\nRQ  Research questions\nS-BPE  Syllable - byte pair encoding\nSOTA  State of the art\nSPS  Surprisal per sentence\nTDNN  Time delay neural network\nWER  Word error rate\nAcknowledgements\nWe would like to acknowledge the publishers of all datasets and the authors of all \nopen-source toolkits and libraries that made this research possible. We also thank \nthe anonymous reviewers for helping shape this manuscript in its current form.\nAuthors’ contributions\nKavya Manohar is responsible for the experimental design, implementation \nand interpretation of the results. Kavya Manohar drafted the manuscript and \nA. R. Jayan and Rajeev Rajan revised it critically for intellectual content. All \nauthors read and approved the final manuscript.\nFunding\nNot applicable.\nAvailability of data and materials\nThe experiments in this work use open-licensed, publicly available speech and \ntext datasets described in Section 4.1.\n The source code for implementing the proposed subword tokenization algo-\nrithm is presented in this repos itory. All other subword tokenizations rely on \nopen-licensed libraries: Morfessor (Morfe ssor Python Library), BPE (Subwo rd \nNMTPython Library), Unigram (Sente nce Piece Python Library) and Malayalam \nSyllabifier (Mlphon Python Library).\n The acoustic model was created using Kaldi  speec h recog nitio n toolk it. Its \ntraining requires a GPU. Statistical language model was created using SRILM \ntoolkit. The acoustic model, the language model and the pronunciation \nlexicon are combined to form the ASR decoder using Kaldi. We have made the \nexperimental script available in this repos itory.\nDeclarations\nCompeting interests\nThe authors declare that they have no competing interests.\nAuthor details\n1 College of Engineering Trivandrum, Thiruvananthapuram, 695 016 Kerala, \nIndia. 2 APJ Abdul Kalam Technological University, Thiruvananthapuram, Kerala, \nIndia. 3 Government Engineering College, Thrissur, Kerala, India. \nReceived: 27 January 2023   Accepted: 12 October 2023\nReferences\n 1. L. Besacier, E. Barnard, A. Karpov, T. Schultz, Automatic Speech Recogni-\ntion for Under-resourced Languages: A Survey. Speech Commun. 56, \n85–100 (2014). https:// doi. org/ 10. 1016/j. specom. 2013. 07. 008\n 2. M. Baerman, D. Brown, G.G. Corbett, Understanding and measuring \nmorphological complexity (Oxford University Press, USA, 2015)\n 3. S. Thottingal, in Proceedings of the 2nd Workshop on Technologies for MT \nof Low Resource Languages. Finite State Transducer based Morphology \nanalysis for Malayalam Language (European Association for Machine \nTranslation, Dublin, 2019), pp. 1–5. https:// aclan tholo gy. org/ W19- 6801. \nAccessed 4 Sept 2023.\n 4. R.E. Asher, T.C. Kumari, Malayalam (Descriptive grammars) (Routledge, \nLondon and New York, 1997)\n 5. G.B. Kumar, K.N. Murthy, B. Chaudhuri, Statistical Analyses of Telugu \nText Corpora. IJDL. Int. J. Dravidian Linguist. 36(2), 71–99 (2007)\n 6. K. Manohar, A. Jayan, R. Rajan, in International Conference on Text, \nSpeech, and Dialogue. Quantitative Analysis of the Morphological Com-\nplexity of Malayalam Language (Springer, 2020), pp. 71–78. https:// doi.  \norg/ 10. 1007/ 978-3- 030- 58323-1_7\n 7. P . Smit, S. Virpioja, M. Kurimo, Advances in Subword-Based HMM-DNN \nSpeech Recognition Across Languages. Comput Speech Lang. 66, \n101158 (2021). https:// doi. org/ 10. 1016/j. csl. 2020. 101158\n 8. S.J. Mielke, R. Cotterell, K. Gorman, B. Roark, J. Eisner, in Proceedings of \nthe 57th Annual Meeting of the Association for Computational Linguistics. \nWhat Kind of Language Is Hard to Language-Model? (Association for \nComputational Linguistics, Florence, 2019), pp. 4975–4989. https:// doi.  \norg/ 10. 18653/ v1/ P19- 1491\n 9. H.H. Park, K.J. Zhang, C. Haley, K. Steimel, H. Liu, L. Schwartz, Morphol-\nogy Matters: A Multilingual Language Modeling Analysis. Trans. Assoc. \nComput. Linguist. 9 , 261–276 (2021). https:// doi. org/ 10. 1162/ tacl_a_ \n00365\n 10. P . Smit, S. Virpioja, M. Kurimo, in Proc. Interspeech 2017. Improved \nSubword Modeling for WFST-Based Speech Recognition (2017), pp. \n2551–2555. https:// doi. org/ 10. 21437/ Inter speech. 2017- 103\n 11. M. Creutz, T. Hirsimäki, M. Kurimo, A. Puurula, J. Pylkkönen, V. Siivola, M. \nVarjokallio, E. Arisoy, M. Saraçlar, A. Stolcke, Morph-Based Speech Rec-\nognition and Modeling of out-of-Vocabulary Words across Languages. \nACM Trans. Speech Lang. Process. 5 (1) (2007). https:// doi. org/ 10. 1145/ \n13223 91. 13223 94\n 12. S. Manghat, S. Manghat, T. Schultz, in ICASSP 2022 - 2022 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing (ICASSP). \nHybrid Sub-word Segmentation for Handling Long Tail in Morphologi-\ncally Rich Low Resource Languages (2022), pp. 6122–6126. https:// doi.  \norg/ 10. 1109/ ICASS P43922. 2022. 97466 52\n 13. H.S. Chadha, A. Gupta, P . Shah, N. Chhimwal, A. Dhuriya, R. Gaur, V. \nRaghavan, Vakyansh: Asr toolkit for low resource indic languages. arXiv \npreprint arXiv: 2203. 16512 (2022)\n 14. A.L. Georgescu, A. Pappalardo, H. Cucu, M. Blott, Performance vs. Hard-\nware Requirements in State-of-the-art Automatic Speech Recognition. \nEURASIP J. Audio Speech Music. Process. 2021(1), 1–30 (2021). https://  \ndoi. org/ 10. 1186/ s13636- 021- 00217-4\n 15. S.P . Bayerl, K. Riedhammer, in Text, Speech, and Dialogue, ed. by K. \nEkštein. A Comparison of Hybrid and End-to-End Models for Syllable \nRecognition (Springer International Publishing, Cham, 2019), pp. \n352–360. https:// doi. org/ 10. 1007/ 978-3- 030- 27947-9_ 30\n 16. A. Rouhe, A. Van Camp, M. Singh, H. Van Hamme, M. Kurimo, in Speech \nand Computer, ed. by A. Karpov, R. Potapova. An Equal Data Setting \nfor Attention-Based Encoder-Decoder and HMM/DNN Models: A Case \nStudy in Finnish ASR (Springer International Publishing, Cham, 2021), \npp. 602–613\n 17. M. Creutz, K. Lagus, in Proceedings of the ACL-02 Workshop on Mor -\nphological and Phonological Learning. Unsupervised Discovery of \nMorphemes (2002), pp. 21–30\n 18. S. Virpioja, P . Smit, S.A. Grönroos, M. Kurimo, et al., Morfessor 2.0: Python \nImplementation and Extensions for Morfessor Baseline. (Aalto Univer -\nsity, 2013), pp. 38. http:// urn. fi/ URN: ISBN: 978- 952- 60- 5501-5\nPage 24 of 25Manohar et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:47 \n 19. P . Gage, A New Algorithm for Data Compression. C Users J. 12(2), 23–38 \n(1994)\n 20. R. Sennrich, B. Haddow, A. Birch, in Proceedings of the 54th Annual \nMeeting of the Association for Computational Linguistics (Volume 1: \nLong Papers). Neural Machine Translation of Rare Words with Subword \nUnits (Association for Computational Linguistics, Berlin, 2016), pp. \n1715–1725. https:// doi. org/ 10. 18653/ v1/ P16- 1162\n 21. T. Kudo, in Proceedings of the 56th Annual Meeting of the Association for \nComputational Linguistics (Volume 1: Long Papers). Subword Regulari-\nzation: Improving Neural Network Translation Models with Multiple \nSubword Candidates (Association for Computational Linguistics, \nMelbourne, 2018), pp. 66–75. https:// doi. org/ 10. 18653/ v1/ P18- 1007\n 22. Adiga, Devaraja and Kumar, Rishabh and Krishna, Amrith and Jyothi, \nPreethi and Ramakrishnan, Ganesh and Goyal, Pawan, in Findings of the \nAssociation for Computational Linguistics: ACL-IJCNLP 2021. Automatic \nSpeech Recognition in Sanskrit: A New Speech Corpus and Modelling \nInsights (2021). https:// doi. org/ 10. 18653/ v1/ 2021. findi ngs- acl. 447\n 23. H. Xu, S. Ding, S. Watanabe, in ICASSP 2019 - 2019 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing (ICASSP). Improving End-to-\nend Speech Recognition with Pronunciation-assisted Sub-word Modeling \n(2019), pp. 7110–7114. https:// doi. org/ 10. 1109/ ICASSP . 2019. 86824 94\n 24. A. Kunchukuttan, P . Bhattacharyya, in Proceedings of the 2016 Conference \non Empirical Methods in Natural Language Processing. Orthographic Syl-\nlable as basic unit for SMT between Related Languages (Association for \nComputational Linguistics, Austin, 2016), pp. 1912–1917. https:// doi.  \norg/ 10. 18653/ v1/ D16- 1196\n 25. H. Singh, R.K. Sharma, V. Singh, Online Handwriting Recognition Sys-\ntems for Indic and non-Indic scripts: A Review. Artif. Intell. Rev. 54(2), \n1525–1579 (2021). https:// doi. org/ 10. 1007/ s10462- 020- 09886-7\n 26. C. Toraman, E.H. Yilmaz, F. Şahïnuç, O. Ozcelik, Impact of tokenization on \nlanguage models: An analysis for turkish. ACM Trans. Asian Low-Resour. \nLang. Inf. Process. 22(4) (2023). https:// doi. org/ 10. 1145/ 35787 07\n 27. V.R.P . Nair,  Introduction to \nLinguistics (MaluBen Publications, Thiruvananthapuram, 2016)\n 28. R. Rajeev, E. Sherly, in Proceedings of 20th Kerala Science Congress. A suffix \nStripping based Morph Analyser for Malayalam Language (Kerala State \nCouncil for Science, Technology and Environment, Thriruvananthapuram, \n2007), pp. 482–484\n 29. O. Rinju, R. Rajeev, P .R. Raj, E. Sherly, Morphological Analyzer for Malay-\nalam: Probabilistic Method vs Rule based Method. Int. J. Comput. Linguist. \nNat. Lang. Process. 2(10), 502–507 (2013)\n 30. P . Antony, K. Soman, Computational Morphology and Natural Language \nParsing for Indian Languages: A Literature Survey. Int. J. Sci. Eng. Res. 3, \n136-146 (2012)\n 31. V. Abeera, S. Aparna, R. Rekha, M. Anand Kumar, V. Dhanalakshmi, K. \nSoman, S. Rajendran, in International Conference on Data Engineering \nand Management. Morphological analyzer for Malayalam using Machine \nLearning (Springer, 2010), pp. 252–254\n 32. B. Premjith, K.P . Soman, M.A. Kumar, A Deep Learning Approach for \nMalayalam Morphological Analysis at Character Level. Procedia Comput. \nSci. 132, 47–54 (2018). https:// doi. org/ 10. 1016/j. procs. 2018. 05. 058\n 33. V. Zouhar, C. Meister, J. Gastaldi, L. Du, T. Vieira, M. Sachan, R. Cotterell, in \nFindings of the Association for Computational Linguistics: ACL 2023. A formal \nperspective on byte-pair encoding (Association for Computational Lin-\nguistics, Toronto, 2023), pp. 598–614. https:// doi. org/ 10. 18653/ v1/ 2023. \nfindi ngs- acl. 38\n 34. K. Manohar, A.R. Jayan, R. Rajan, Mlphon: A Multifunctional Grapheme-\nPhoneme Conversion Tool Using Finite State Transducers. IEEE Access 10, \n97555–97575 (2022). https:// doi. org/ 10. 1109/ ACCESS. 2022. 32044 03\n 35. G. Berry, R. Sethi, From regular expressions to deterministic automata. \nTheor. Comput. Sci. 48, 117–126 (1986)\n 36. D. Jurafsky, J.H. Martin, Speech and Language Processing: An Introduction \nto Natural Language Processing, Computational Linguistics, and Speech \nRecognition (Pearson, India, 2009)\n 37. A.I.E.D. Mousa, Sub-word based language modeling of morphologically \nrich languages for lvcsr. Ph.D. thesis, RWTH Aachen University (2014)\n 38. T. Hirsimäki, M. Creutz, V. Siivola, M. Kurimo, S. Virpioja, J. Pylkkönen, \nUnlimited Vocabulary Speech Recognition with Morph Language Models \nApplied to Finnish. Comput. Speech Lang. 20(4), 515–541 (2006). https:// \ndoi. org/ 10. 1016/j. csl. 2005. 07. 002\n 39. G. Choueiter, D. Povey, S. Chen, G. Zweig, in 2006 IEEE International Confer-\nence on Acoustics Speech and Signal Processing Proceedings. Morpheme-\nBased Language Modeling for Arabic LVCSR, vol. 1 (2006), pp. I–I. https:// \ndoi. org/ 10. 1109/ ICASSP . 2006. 16602 05\n 40. B. Pilar, et al., Subword Dictionary Learning and Segmentation Techniques \nfor Automatic Speech Recognition in Tamil and Kannada. arXiv preprint \narXiv: 2207. 13331 (2022)\n 41. B. Pilar, et al., Knowledge-driven subword grammar modeling for auto-\nmatic speech recognition in tamil and kannada. arXiv preprint arXiv: 2207. \n13333 (2022)\n 42. K. Manohar, A.R. Jayan, R. Rajan, in Proceedings of the Third International \nWorkshop on NLP Solutions for Under Resourced Languages (NSURL 2022) \nco-located with ICNLSP 2022. Syllable subword tokens for open vocabulary \nspeech recognition in Malayalam (Association for Computational Linguis-\ntics, Trento, 2022), pp. 1–7. https:// aclan tholo gy. org/ 2022. nsurl-1.1\n 43. A. Baby, A.L. Thomas, N. Nishanthi, T. Consortium, et al., in Proceedings \nof Text, Speech and Dialogue. Resources for Indian languages (Springer, \nCham, 2016)\n 44. F. He, S.H.C. Chu, O. Kjartansson, C. Rivera, A. Katanova, A. Gutkin, I. Demir-\nsahin, C. Johny, M. Jansche, S. Sarin, K. Pipatsrisawat, in Proceedings of The \n12th Language Resources and Evaluation Conference (LREC). Open-source \nMulti-speaker Speech Corpora for Building Gujarati, Kannada, Malay-\nalam, Marathi, Tamil and Telugu Speech Synthesis Systems (European \nLanguage Resources Association (ELRA), Marseille, 2020), pp. 6494–6503. \nhttps:// www. aclweb. org/ antho logy/ 2020. lrec-1. 800\n 45. D.P . Gopinath, V.V. Nair, et al., Imasc–icfoss malayalam speech corpus. \narXiv preprint arXiv: 2211. 12796 (2022)\n 46. K. Manohar. Releasing Malayalam speech corpus. (2020). https:// blog. \nsmc. org. in/ malay alam- speech- corpus/. Accessed 1 Sept 2023\n 47. K. Prahallad, E.N. Kumar, V. Keri, S. Rajendran, A.W. Black, in Thirteenth \nannual conference of the international speech communication association. \nThe IIIT-H Indic speech databases (ISCA, Portland, 2012)\n 48. S.M. Computing. Malayalam text corpora (Swathanthra Malayalam \nComputing, Kerala, 2020). https:// gitlab. com/ smc/ corpus . Retrieved \non Spetember 01, 2023\n 49. P . Żelasko, S. Feng, L. Moro Velázquez, A. Abavisani, S. Bhati, O. Scharen-\nborg, M. Hasegawa-Johnson, N. Dehak, Discovering Phonetic Inventories \nwith Crosslingual Automatic Speech Recognition. Comput. Speech Lang. \n74(C) (2022). https:// doi. org/ 10. 1016/j. csl. 2022. 101358\n 50. M. Mohri, F. Pereira, M. Riley, Weighted Finite-state Transducers in Speech \nRecognition. Comput. Speech Lang. 16(1), 69–88 (2002). https:// doi. org/ \n10. 1006/ csla. 2001. 0184\n 51. R.W. Hamming, Digital filters (Courier Corporation, USA, 1998)\n 52. S. Davis, P . Mermelstein, Comparison of parametric representations for \nmonosyllabic word recognition in continuously spoken sentences. IEEE \nTrans. Acoust. Speech Signal Process. 28(4), 357–366 (1980)\n 53. S.S. Stevens, J. Volkmann, E.B. Newman, A scale for the measurement of the \npsychological magnitude pitch. J. Acoust. Soc. Am. 8(3), 185–190 (1937)\n 54. N. Dehak, P .J. Kenny, R. Dehak, P . Dumouchel, P . Ouellet, Front-end factor \nanalysis for speaker verification. IEEE Trans. Audio Speech Lang. Process. \n19(4), 788–798 (2010)\n 55. V. Peddinti, D. Povey, S. Khudanpur, in Sixteenth annual conference of the \ninternational speech communication association. A time delay neural net-\nwork architecture for efficient modeling of long temporal contexts (ISCA, \nDresden, 2015)\n 56. G. Saon, H. Soltau, D. Nahamoo, M. Picheny, in 2013 IEEE Workshop on \nAutomatic Speech Recognition and Understanding. Speaker adaptation of \nneural network acoustic models using i-vectors (IEEE, Olomouc, 2013), \npp. 55–59\n 57. D. Povey, V. Peddinti, D. Galvez, P . Ghahremani, V. Manohar, X. Na, Y. Wang, \nS. Khudanpur, in Interspeech. Purely sequence-trained neural networks for \nasr based on lattice-free mmi. (2016), pp. 2751–2755\n 58. A. Stolcke, in Seventh international conference on spoken language process-\ning. SRILM-an extensible language modeling toolkit (ISCA, Denver, 2002)\n 59. R. Kneser, H. Ney, in 1995 International Conference on Acoustics, Speech, and \nSignal Processing. Improved backing-off for m-gram language modeling, \nvol. 1 (1995), pp. 181–184. https:// doi. org/ 10. 1109/ ICASSP . 1995. 479394\n 60. D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. \nHannemann, P . Motlicek, Y. Qian, P . Schwarz, et al., in IEEE 2011 workshop \non automatic speech recognition and understanding. The Kaldi speech \nrecognition toolkit (IEEE, Columbia, 2011), CONF\nPage 25 of 25\nManohar et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:47 \n \n 61. P . Kłosowski, Statistical analysis of orthographic and phonemic language \ncorpus for word-based and phoneme-based polish language modelling. \nEURASIP J. Audio Speech Music Process. 2017(1), 1–16 (2017)\n 62. J. Benesty, M.M. Sondhi, Y. Huang et al., Springer handbook of speech \nprocessing, vol. 1 (Springer, Berlin, 2008)\n 63. M. Bisani, H. Ney, in Proc. Interspeech 2005. Open vocabulary speech \nrecognition with flat hybrid models (2005), pp. 725–728. https:// doi. org/ \n10. 21437/ Inter speech. 2005- 11\n 64. R.A. Braun, S. Madikeri, P . Motlicek, in ICASSP 2021-2021 IEEE International \nConference on Acoustics, Speech and Signal Processing (ICASSP). A Com-\nparison of Methods for OOV-Word Recognition on a New Public Dataset \n(IEEE, 2021), pp. 5979–5983. https:// doi. org/ 10. 1109/ ICASS P39728. 2021. \n94151 24\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8417869210243225
    },
    {
      "name": "Language model",
      "score": 0.7396050095558167
    },
    {
      "name": "Lexical analysis",
      "score": 0.7230530977249146
    },
    {
      "name": "Syllable",
      "score": 0.6862379908561707
    },
    {
      "name": "Pronunciation",
      "score": 0.6825140118598938
    },
    {
      "name": "Speech recognition",
      "score": 0.6734358072280884
    },
    {
      "name": "Natural language processing",
      "score": 0.5717470049858093
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5700541138648987
    },
    {
      "name": "Hidden Markov model",
      "score": 0.5456507802009583
    },
    {
      "name": "Bigram",
      "score": 0.5121183395385742
    },
    {
      "name": "Word (group theory)",
      "score": 0.49841928482055664
    },
    {
      "name": "Text segmentation",
      "score": 0.4699842035770416
    },
    {
      "name": "Word error rate",
      "score": 0.4479057490825653
    },
    {
      "name": "Trigram",
      "score": 0.23629915714263916
    },
    {
      "name": "Segmentation",
      "score": 0.23169094324111938
    },
    {
      "name": "Linguistics",
      "score": 0.1645127832889557
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I158338959",
      "name": "University of Kerala",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I3129367976",
      "name": "A P J Abdul Kalam Technological University",
      "country": "IN"
    }
  ]
}