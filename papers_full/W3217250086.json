{
  "title": "Self-Supervised Pre-Training for Transformer-Based Person Re-Identification",
  "url": "https://openalex.org/W3217250086",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2100740392",
      "name": "Luo Hao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1238505444",
      "name": "Wang, Pichao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2072371361",
      "name": "Xu Yi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1973841088",
      "name": "Ding Feng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2366235665",
      "name": "Zhou Yanxin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2111321986",
      "name": "Wang Fan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A875780830",
      "name": "Li, Hao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2209117791",
      "name": "Jin Rong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2103012681",
    "https://openalex.org/W3195108980",
    "https://openalex.org/W3103165805",
    "https://openalex.org/W3160566314",
    "https://openalex.org/W2120587290",
    "https://openalex.org/W3105077954",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W3178838461",
    "https://openalex.org/W3109116076",
    "https://openalex.org/W3204619080",
    "https://openalex.org/W2995592331",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3212756788",
    "https://openalex.org/W2029538739",
    "https://openalex.org/W3175823695",
    "https://openalex.org/W3101821705",
    "https://openalex.org/W2967515867",
    "https://openalex.org/W2884366600",
    "https://openalex.org/W2204750386",
    "https://openalex.org/W2995852213",
    "https://openalex.org/W3035272520",
    "https://openalex.org/W3035186652",
    "https://openalex.org/W2156909104",
    "https://openalex.org/W3181858645",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2798381792",
    "https://openalex.org/W3196660197",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2953214814",
    "https://openalex.org/W3023371261",
    "https://openalex.org/W3143016713",
    "https://openalex.org/W3130138360",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W3034607353",
    "https://openalex.org/W3100506510",
    "https://openalex.org/W3009561768",
    "https://openalex.org/W3141972123",
    "https://openalex.org/W2942810103",
    "https://openalex.org/W3175445198",
    "https://openalex.org/W2945383715",
    "https://openalex.org/W3184225461",
    "https://openalex.org/W2104094955",
    "https://openalex.org/W1665214252"
  ],
  "abstract": "Transformer-based supervised pre-training achieves great performance in person re-identification (ReID). However, due to the domain gap between ImageNet and ReID datasets, it usually needs a larger pre-training dataset (e.g. ImageNet-21K) to boost the performance because of the strong data fitting ability of the transformer. To address this challenge, this work targets to mitigate the gap between the pre-training and ReID datasets from the perspective of data and model structure, respectively. We first investigate self-supervised learning (SSL) methods with Vision Transformer (ViT) pretrained on unlabelled person images (the LUPerson dataset), and empirically find it significantly surpasses ImageNet supervised pre-training models on ReID tasks. To further reduce the domain gap and accelerate the pre-training, the Catastrophic Forgetting Score (CFS) is proposed to evaluate the gap between pre-training and fine-tuning data. Based on CFS, a subset is selected via sampling relevant data close to the down-stream ReID data and filtering irrelevant data from the pre-training dataset. For the model structure, a ReID-specific module named IBN-based convolution stem (ICS) is proposed to bridge the domain gap by learning more invariant features. Extensive experiments have been conducted to fine-tune the pre-training models under supervised learning, unsupervised domain adaptation (UDA), and unsupervised learning (USL) settings. We successfully downscale the LUPerson dataset to 50% with no performance degradation. Finally, we achieve state-of-the-art performance on Market-1501 and MSMT17. For example, our ViT-S/16 achieves 91.3%/89.9%/89.6% mAP accuracy on Market1501 for supervised/UDA/USL ReID. Codes and models will be released to https://github.com/michuanhaohao/TransReID-SSL.",
  "full_text": "Self-Supervised Pre-Training for Transformer-Based Person Re-Identiﬁcation\nHao Luo, Pichao Wang, Yi Xu, Feng Ding, Yanxin Zhou, Fan Wang, Hao Li, Rong Jin\nAlibaba Group\nmichuan.lh@alibaba-inc.com\nAbstract\nTransformer-based supervised pre-training achieves\ngreat performance in person re-identiﬁcation (ReID). How-\never, due to the domain gap between ImageNet and ReID\ndatasets, it usually needs a larger pre-training dataset (e.g.\nImageNet-21K) to boost the performance because of the\nstrong data ﬁtting ability of the transformer. To address\nthis challenge, this work targets to mitigate the gap between\nthe pre-training and ReID datasets from the perspective of\ndata and model structure, respectively. We ﬁrst investigate\nself-supervised learning (SSL) methods with Vision Trans-\nformer (ViT) pretrained on unlabelled person images (the\nLUPerson dataset), and empirically ﬁnd it signiﬁcantly sur-\npasses ImageNet supervised pre-training models on ReID\ntasks. To further reduce the domain gap and accelerate\nthe pre-training, the Catastrophic Forgetting Score (CFS)\nis proposed to evaluate the gap between pre-training and\nﬁne-tuning data. Based on CFS, a subset is selected via\nsampling relevant data close to the down-stream ReID data\nand ﬁltering irrelevant data from the pre-training dataset.\nFor the model structure, a ReID-speciﬁc module named\nIBN-based convolution stem (ICS) is proposed to bridge\nthe domain gap by learning more invariant features. Ex-\ntensive experiments have been conducted to ﬁne-tune the\npre-training models under supervised learning, unsuper-\nvised domain adaptation (UDA), and unsupervised learn-\ning (USL) settings. We successfully downscale the LUPer-\nson dataset to 50% with no performance degradation. Fi-\nnally, we achieve state-of-the-art performance on Market-\n1501 and MSMT17. For example, our ViT-S/16 achieves\n91.3%/89.9%/89.6% mAP accuracy on Market1501 for su-\npervised/UDA/USL ReID. Codes and models will be re-\nleased to https://github.com/michuanhaohao/\nTransReID-SSL.\n1. Introduction\nTransformer-based methods [19,48] have attracted more\nand more attention and achieved great performance in per-\nson ReID. For example, the pure transformer-based method\nFigure 1. Performance of ViT-S on Market-1501 [50] and\nMSMT17 [41] in supervised, UDA and USL ReID. Our pre-\ntraining paradigm outperforms Baseline (supervised pre-training\non ImageNet + vanilla ViT) by a large margin.\nTransReID [19] achieves a signiﬁcant performance boost\nover state-of-the-art CNN-based methods. However, there\nexists a large domain gap between ImageNet and person\nReID datasets because 1) The image content of ImageNet\nand ReID datasets is very different [14]; 2) supervised pre-\ntraining on ImageNet is focused on category-level super-\nvision which reduces the rich visual information [3] while\nperson ReID prefers ﬁne-grained identity information. As a\nresult, Transformers need to be pre-trained on a larger-scale\ndataset ImageNet-21K [12] to avoid over-ﬁtting on the pre-\ntraining dataset. To bridge the gap between pre-training and\nﬁne-tuning datasets for better transformer-based ReID mod-\nels, this paper tackles the problem from the perspectives of\ndata and model structure, respectively.\nFrom the data view, we have seen a large-scale pre-\ntraining dataset named LUPerson being built by collect-\ning unlabeled person images [14], which has demonstrated\nthat CNN-based SSL pre-training on LUPerson dataset im-\nproves ReID performance compared with ImageNet-1k pre-\ntraininng. However, following the same paradigm and re-\nplacing the backbone with Vision Transformers (ViTs) [13]\nwould get poor performance due to the huge differences be-\ntween training CNNs and ViTs. It motivates us to explore an\neffective SSL pre-training paradigm for transformer-based\nperson ReID in the ﬁrst place. After thorough comparison\nbetween several Transformer-based self-supervised meth-\nods (e.g. MocoV3 [8], MoBY [43] and DINO [3]) with ViTs\non the LUPerson dataset, it is found that DINO signiﬁcantly\n1\narXiv:2111.12084v1  [cs.CV]  23 Nov 2021\noutperforms other SSL methods and supervised pre-training\non ImageNet, thus it is used as our following setup. Next,\nit is brought to our attention that, though with better perfor-\nmance, pre-training on LUPersonn needs a larger amount of\ncomputational resources due to the large amount of train-\ning data (3X of ImageNet-1K). Therefore, we propose to\nadopt conditional pre-training to speed-up the training pro-\ncess and further reduce the domain gap. As LUPerson is\ncollected from web videos, a portion of the images are of\nlow-quality or have great domain bias with the downstream\nReID datasets, so conditional ﬁltering shall be performed\nto downscale LUPerson (source domain) by selecting a rel-\nevant subset close to the downstream ReID datasets (target\ndomain). However, previous works on conditional ﬁltering\n[4,9,10,15,29] are mainly designed for close-set tasks by se-\nlecting data close to category labels or cluster centers of the\ntarget training data. Those methods will easily overﬁt the\nperson IDs instead of the true target domain if directly ap-\nplied to the open-set ReID task. We propose a metric named\nCatastrophic Forgetting Score (CFS) to evaluate the gap be-\ntween pre-training and downstream data, inspired by the\nconclusions in catastrophic forgetting problems [25,34,36].\nFor a pre-training image, the similarity between its features\nfrom the two proxy models (one is pre-trained on the source\ndataset and another is ﬁne-tuned on the target dataset) can\nrepresent the similarity between the source and target do-\nmain. In this way, a subset of images with higher CFS\nscores can be selected from the pre-training data to perform\nefﬁcient conditional pre-training. A preliminary theoretical\nanalysis is conducted to justify the effectiveness of CFS.\nFrom the perspective of model structure, some recent\nworks [8, 40, 42] have pointed out that, an important factor\nthat affects performance and stability of ViTs is thepatchify\nstem implemented by a stride-pp ×pconvolution (p= 16\nby default) on the input image. To address this problem,\nMocoV3 [8] froze the patch projection to train ViTs, while\nXiao et al . [42] and Wang et al . [40] proposed a convo-\nlution stem stacked by several convolution, Batch Normal-\nization (BN) [22], and ReLU [32] layers to increase opti-\nmization stability and improve performance. Inspired by\nthe success of integrating Instance Normalization (IN) and\nBN to learn domain-invariant representation in the ReID\ntask [11, 30, 33], we refer to IBN-Net [33] and improve the\nconvolution stem to the IBN-based convolution stem (ICS).\nICS inherits the stability of the convolution stem and also\nintroduces IBN to learn features with appearance invariance\n(e.g. viewpoint, pose and illumination invariance, etc.). ICS\nhas similar computational complexity to convolution stem,\nbut improves peak performance of ViTs signiﬁcantly in per-\nson ReID.\nEmbraced with the above two improvements based on\nDINO, we conduct experiments with supervised learning,\nUDA and USL settings on Market1501 and MSMT17. Our\npre-training paradigm helps ViTs achieve state-of-the-art\nperformance on these benchmarks. For instance, as shown\nin Figure 1, our ViT-S achieves 91.3%/89.9%/89.6% mAP\naccuracy on Market-1501 for supervised/UDA/USL ReID,\nwhich exceed the supervised pre-training setting on Ima-\ngeNet by a large margin. The pre-training cost on LUPerson\nis also reduced by 30% without any performance degrada-\ntion, through the proposed CFS-based conditional ﬁltering\nand downscaling the original LUPerson by 50%.\n2. Related Works\n2.1. Self-supervised Learning\nSelf-supervised learning (SSL) methods are proposed\nto learn discriminative features from large-scale unlabeled\ndata without any human-annotated labels [23]. One branch\nis developed from Momentum Contrast (MoCo) [6] by\ntreating a pair of augmentations of a sample as a posi-\ntive pair and all other samples as negative pairs. Since the\nnumber of negative samples greatly affects the ﬁnal perfor-\nmance, MoCo series [6, 8] require large batches or memory\nbanks. Among them, MoCoV3 [8] is a Transformer-speciﬁc\nversion. Fu et al. [14] have veriﬁed ResNet50 can be well\npre-trained by a modiﬁed MoCoV2 on human images in\nperson ReID. Many recent works have shown models can\nlearn feature representation without discriminating between\nimages. In this direction, Ge et al . [18] propose a new\nparadigm called BYOL, where the online network predicts\nthe representation of the target network on the same image\nunder a different augmented view. Large batch size is un-\nnecessary in BYOL since negative samples are not needed.\nMany variants have successfully improved BYOL in var-\nious ways. One of them is DINO [3], where a centering\nand sharpening of the momentum teacher outputs is used\nto avoid model collapse. DINO achieves state-of-the-art\nperformance with ViTs on both ImageNet classiﬁcation and\ndown-stream tasks. Xie et al. combines MoCo with BOYL\nto propose a Transformer-speciﬁc method called MoBY .\nGiven that there have been more SSL methods designed\nspeciﬁcally for Transformers, we will focus on several of\nthe state-of-the-art options in the following experiments,\ne.g. MoCo series, MoBY and DINO.\n2.2. Transformer-based ReID\nCNN-based methods have dominated the ReID commu-\nnity for many years. However, pure-transformer models are\ngradually becoming a popular choice. He et al. [19] are the\nﬁrst to successfully apply ViTs to ReID tasks by proposing\nTransReID which achieves state-of-the-art performance on\nboth person and vehicle ReID. Auto-Aligned Transformer\n(AAformer) [51] also uses ViT backbone with the addi-\ntional learnable vectors of “part tokens” to learn the part\nrepresentations and integrates the part alignment into the\n2\nself-attention. Other works try to use Transformer to ag-\ngregate features or information from CNN backbones. For\nexample, [27, 35, 48] integrate Transformer layers into the\nCNN backbone to aggregate hierarchical features and align\nlocal features. For video ReID, [28,49] exploit Transformer\nto aggregate appearance features, spatial features, and tem-\nporal features to learn a discriminative representation for a\nperson tracklet.\n2.3. Conditional Transfer Learning\nThere are a few works [4, 9, 10, 15, 29, 45] studying how\nto select relevant subsets from the pre-training dataset to im-\nprove the performance when transferred to target datasets.\n[10] use a feature extractor trained on the JFT300M [20] to\nselect the most similar source categories to the target cat-\negories in a greedy way. For each image from the target\ndomain, Ge et al. [15] search a certain number of images\nwith similar low-level characteristics from the source do-\nmain. Shuvam et al. [4] train the feature extractor on the\ntarget data and individually select source images that are\nclose to cluster centers in the target domain. Yan et al. [45]\npropose a Neural Data Server (NDS) to train expert models\non many subsets of the pre-training dataset. Source images\nused to train an expert with a good target task performance\nare assigned high importance scores. It is noted that condi-\ntional transfer learning has mainly been studied on close-set\ntasks such as image classiﬁcation, ﬁne-grained recognition,\nobject detection, etc. Therefore, these methods may be not\nsuitable for the open-set ReID task.\n3. Self-supervised Pre-training\nAs far as we know, there has been no literature studying\nthe SSL pre-training for transformer-based ReID. Therefore\nwe ﬁrst conduct an empirical study to gain better under-\nstanding on this problem. We investigate two backbones\n(CNN-based ResNet50 and Transformer-based ViT), four\nSSL methods (MoCoV2, MoCoV3, MoBY and DINO), and\ntwo pre-training datasets (ImageNet and LUPerson). Mo-\nCoV2 used here is a modiﬁed version proposed to adapt\nto person ReID on ResNet50 in [14], while the other three\nmethods, i.e. MoCoV3, MoBy and DINO, are transformer-\nspeciﬁc methods proposed on the ImageNet data.\n3.1. Supervised Fine-tuning\nThe baseline [19] we used is pre-trained on ImageNet.\nWe ﬁne-tune pre-trained models on two ReID benchmarks\nincluding Market-1501 (Market) and MSMT17, and the\npeak ﬁne-tuning performance of all models being compared\nare presented in Table 1. For convenience, a pre-trained\nmodel is marked in the form of Model+Method+Data.\nFor instance, R50+Supervised+IMG denotes the ResNet50\nmodel pre-trained on the ImageNet in the supervised man-\nPre-training Market MSMT17\nModels Methods Data mAP R1 mAP R1\nR50\nSupervised IMG 86.7 94.8 52.2 76.0\nMoCoV2 LUP 88.2 94.8 53.3 76.0\nMoCoV3 LUP 87.3 95.1 52.9 76.8\nDINO LUP 86.5 94.4 51.9 75.8\nViT-S/16\nSupervised IMG 85.0 93.8 53.5 75.2\nMoCoV2 IMG 63.6 72.1 19.6 36.1\nMoCoV3 IMG 81.7 92.1 46.6 70.3\nMoBY IMG 83.3 92.2 49.1 71.5\nDINO IMG 84.6 93.1 54.8 76.7\nViT-S/16\nMoCoV2 LUP 72.1 87.6 27.8 47.4\nMoCoV3 LUP 82.2 92.1 47.4 70.3\nMoBY LUP 84.0 92.9 50.0 73.2\nDINO LUP 90.3 95.4 64.2 83.4\nDINO LUP ∗ 89.6 95.1 62.3 82.6\nTable 1. Comparison of different pre-training models. We pre-\ntrained ResNet50 (R50) and ViT-S/16 on ImageNet-1K (IMG) and\nLUPerson (LUP) datasets. MoBY doesn’t provide training set-\ntings for ResNet50. To fairly compare with ImageNet, we ran-\ndomly sampled 1.28M images from LUPerson to build a subset\ndenoted as LUP∗.\nner, which is the standard pre-training paradigm in most\nprevious ReID methods.\nSome observations are made as follows. 1) MoCoV2\nperforms the best among all SSL methods for ResNet50,\nwhile it is much worse than the other three transformer-\nspeciﬁc methods with ViT, which means that it is nec-\nessary to explore speciﬁc methods for transformer-based\nmodels. 2) ViT is more sensitive to a proper pre-training\nthan ResNet50. For example, we can see that mAP using\nResNet50 ranges from 51.9% to 53.3% on MSMT17 with\ndifferent pre-training settings, while performance of ViT-\nS/16 differs more widely on MSMT17. 3) SSL methods\non LUP consistently achieve better performance comparing\nto SSL with ImageNet. Even when we restrict the number\nof training images of LUP to be the same as IMG, ViT-\nS/16+DINO+LUP∗ still surpasses ViT-S/16+DINO+ IMG\non both benchmarks, indicating that leveraging person im-\nages is a better choice to pre-train ReID models. 4) ViT-\nS/16+DINO+LUP achieves 64.2% mAP and 83.4% Rank-\n1 accuracy on the MSMT17, surpassing the baseline (ViT-\nS/16+Supervised+IMG) by 10.7% mAP and 8.2% Rank-1\naccuracy.\n3.2. Unsupervised Fine-tuning\nSince there has been no transformer-based baseline for\nunsupervised ReID, the state-of-the-art CNN-based frame-\nwork C-Contrast [11] is selected for the following experi-\nments. We reproduce C-Contrast with ResNet50 and ViT-\nS/16 on both USL ReID and UDA ReID 1. Based on ob-\n1https://github.com/alibaba/cluster- contrast-\nreid. All results are reproduced using the ofﬁcial code with the same\n3\nPre-training Market MSMT17\nModels Methods Data mAP R1 mAP R1\nR50 Sup IMG 82.6 93.0 33.1 63.3\nSSL LUP 84.0 93.4 31.4 58.8\nViT-S/16\nSup IMG 62.5 80.5 13.5 29.9\nSSL IMG 68.9(+6.4) 84.3(+3.8) 14.9(+1.4) 31.0(+1.1)\nSSL LUP 87.8(+25.3) 94.4(+13.9) 38.4(+24.9) 63.8(+33.9)\nTable 2. Model performance in USL ReID. Supervised pre-\ntraining and self-supervised pre-training are abbreviated as SUP\nand SSL, respectively.\nPre-training MS2MA MA2MS\nModels Methods Data mAP R1 mAP R1\nR50 Sup IMG 82.4 92.5 33.4 60.5\nSSL LUP 85.1 94.4 28.3 53.8\nViT-S/16\nSup IMG 68.5 85.4 13.6 29.5\nSSL IMG 79.7(+11.2) 90.5(+5.1) 21.8(+8.2) 41.6(+12.1)\nSSL LUP 88.5(+20.0) 95.0(+9.6) 43.9(+30.3) 67.7(+38.2)\nTable 3. Model performance in UDA ReID. MS2MA\nand MA2MS stands for MSMT17 →Market (MS2MA) and\nMarket→MSMT17 (MA2MS), respectively.\nservations made from Table 1, we choose MoCoV2 for\nResNet50 and DINO for ViT-S/16 in this section in Table 2.\nSSL pre-training doesn’t provide large gains compared with\nSup when applied with ResNet50, not to mention that per-\nformance drop is observed on MSMT17. This is consis-\ntent with the aforementioned conclusion that Transformer\nis more sensitive to the pre-training than CNN. In contrast,\nSSL pre-training on LUPerson improves the performance\nby a large margin for ViT-S/16. The UDA performance on\nMS2MA and MA2MS also yields the similar conclusion.\nConclusion. DINO is the most suitable SSL method\namong candidate methods for transformer-based ReID. Pre-\ntraining is more crucial for Transformers than CNN mod-\nels. Transformers pre-trained on LUPerson can signiﬁ-\ncantly improve the performance compared with ImageNet\npre-training, indicating that bridging the domain gap be-\ntween pre-training and ﬁne-tuning datasets for transformers\nis more beneﬁcial and worth doing.\n4. Conditional Pre-training\nThis section introduces the efﬁcient conditional pre-\ntraining where models are pre-trained on a subset closer\nto the target domain to speed up the pre-training process\nwhile maintaining or even improving the downstream ﬁne-\ntuning performance. Catastrophic Forgetting Score (CFS) is\nproposed to evaluate the similarity between the pre-training\ndata and the target domain. Theoretic analysis is provided\nto support the method.\nrandom seed and GPU machine to reduce randomness.\n4.1. Problem Deﬁnition\nGiven a target dataset Dt = ( Xt,Yt) where Xt =\n{x1\nt,x2\nt,x3\nt,...,x M\nt }with their ID labels Yt. We target to\nselect a subset D′\ns from a large-scale source/pre-training\ndataset Ds where Xs = {x1\ns,x2\ns,x3\ns,...,x N\ns }. The num-\nber of images in D′\ns is N′ < N. The efﬁcient conditional\npre-training is to pre-train models on D′\ns, which should re-\nduce the training cost of pre-training while maintaining or\neven improving performance on the target dataset. Some\nprevious works [4, 9, 29] have shown that the solution is to\nselect pre-training data close to the target domain, and we\nalso provide theoretical analysis in Appendix to further ver-\nify this. Since ReID is a open set problem with different IDs\nin training and testing set, the key problem is how to design\na metric to evaluate the ‘similarity’ between the pre-training\ndata xi\ns,i ∈[1,N] and the target dataset Dt (instead of the\nperson IDs in Dt).\n4.2. Catastrophic Forgetting Score\nAlgorithm 1 Our Proposed Conditional Filtering\n1: procedure FILTER (Ds,Dt)\n2: θs ←TRAIN (Ds) ⊿Source Proxy Model\n3: θt ←TRAIN (θs,Dt) ⊿Target Proxy Model\n4: for i←1 to N do ⊿xi\ns ∈Ds\n5: ci\ns ←CFS(xi\ns) ⊿Compute CFS\n6: cs ←SORT(c1\ns,c2\ns,...,c N\ns ) ⊿Get Score Set\n7: D′\ns ←TOP(Ds,N′,cs) ⊿Filter Source Dataset\n8: return D\n′\ns ⊿Return the Filtered Subset\nWe ﬁrst pre-train a modelθson the source datasetDs. θs\nis transferred into θt by ﬁne-tuning on the target dataset Dt.\nSince θs and θt only serve as proxy models to select data,\nthere is no need for them to achieve the best ReID perfor-\nmance, i.e. they can be lightweight models trained with less\nepochs. In this paper, Dt is the fusion of Market-1501 and\nMSMT17, so we only need to select one subset D′\ns sharing\nbetween different ReID datasets.\nMany previous works [25, 34, 36] have observed that\ncatastrophic forgetting in neural networks occurs during do-\nmain transferring, and the degree of forgetting is related\nto the gap between the source domain and the target do-\nmain. To evaluate the domain gap between the pre-training\ndata and the target domain, a simple metric named Catas-\ntrophic Forgetting Score (CFS) is proposed as below, which\ncomputes the representation similarity between θs(xi\ns) and\nθt(xi\ns) for the pre-training data xi\ns:\nci\ns = ⟨θs(xi\ns),θt(xi\ns)⟩\n||θs(xis)||||θt(xis)||. (1)\nThe greater the ci\ns, the smaller the degree of forgetting, i.e.\nthe smaller the domain gap. cs = {c1\ns,c2\ns,...,c N\ns }is sorted\n4\nin descending order and the top N′images are selected to\nget the subset D′\ns ∈Ds.\nThe advantage of CFS is that it does not compare xi\ns di-\nrectly with the images in the target domain. That is, un-\nlike previous methods designed for close-set tasks, it avoids\nscanning through all images from the target domain to com-\npute ci\ns, which is computationally costly.\n4.3. Theoretical Analysis of CFS\nIn this subsection, we give a theoretical analysis to show\nlarge CFS is a necessary condition for ﬁnding an image in\nthe source domain that is close to the target domain. To\nthis end, we ﬁrst rigorously deﬁne some terminologies as\nfollows, which will be used in the analysis.\nDeﬁnition 1. The feature representation θ is a function of\nx∈X that maps to Rd, i.e., θ(x) : X→ Rd.\nThe feature representation θis a function extracting fea-\ntures of an image. Without loss of generality, we use the\nnormalized representation function in our analysis, i.e.,\n˜θ:= θ\n∥θ∥, (2)\nwhere ∥·∥ is the norm.\nDeﬁnition 2. We say x and x′ are (ϵ,θ1,θ2)-similar, if\n∥˜θ1(x) −˜θ2(x′)∥ ≤ϵ, where ϵ ∈ (0,1) is a small con-\nstant, θ1 and θ2 are two representation functions, ˜θ1 and ˜θ2\nare deﬁned as (2). In addition, if θ1 = θ2 := θ, we use\n(ϵ,θ)-similar for simplicity.\nThe similarity of two images can be deﬁned as the\n“closeness” of their features. This is reasonable since the\nfeature is considered as the representation of an image.\nAssumption 1. If x ∈D and x′∈D′are close, then they\nare (ϵ/2,θ,θ ′)-similar, i.e., ∥˜θ(x) −˜θ′(x′)∥≤ ϵ/2, where\nθ : D →Rd and θ′ : D′ →Rd are two representation\nfunctions, ˜θand ˜θ′are deﬁned as (2).\nSince our inference focuses on downstream tasks, it is in-\nterested in considering the representation for the target do-\nmain (i.e., θt). The main goal is to ﬁnd an image in the\nsource domain that is (ϵ,θt)-similar to an image from the\ntarget domain. Mathematically, we aim to ﬁnd xi\ns ∈Ds to\nbe (ϵ,˜θt)-similar to xj\nt ∈Dt2, that is,\n∥˜θt(xi\ns) −˜θt(xj\nt)∥≤ ϵ. (3)\nThe following theorem shows that the large CFS is a neces-\nsary condition for ﬁnding such an image in the source do-\nmain.\n2Please note that xj\nt could not be a true image from target domain and\nit is for the proof use only. In general, the image xj\nt can be a virtual one\nthat follows the same distribution of the images of target domain.\nTheorem 1. For a given xi\ns ∈Ds, if there exists a xj\nt ∈\nDt that satisﬁes Assumption 1, then we have the following\nresult. If\nci\ns ≥1 −ϵ2/8, (4)\nthen (3) holds, where ci\ns is deﬁned in (1) and ϵ∈(0,1) is a\nsmall constant.\nProof. Let deﬁne the distance\nds,t := ∥˜θt(xi\ns) −˜θs(xi\ns)∥. (5)\nWe write the distance ∥˜θt(xi\ns) −˜θt(xj\nt)∥as\n∥˜θt(xi\ns) −˜θt(xj\nt)∥\n=∥˜θt(xi\ns) −˜θs(xi\ns) + ˜θs(xi\ns) −˜θt(xj\nt)∥\n≤ds,t + ∥˜θs(xi\ns) −˜θt(xj\nt)∥, (6)\nwhere the last inequality uses Triangle Inequality for norm.\nFurthermore, if xi\ns and xj\nt are close, then under Assump-\ntion 1 we have\n∥˜θs(xi\ns) −˜θt(xj\nt)∥≤ ϵ/2. (7)\nNext, we want to explain why the small distanceds,t can be\nused as the selection criterion, let consider two cases. When\nds,t ≥∥˜θs(xi\ns) −˜θt(xj\nt)∥, by (5) and (6) we have\n∥˜θt(xi\ns) −˜θt(xj\nt)∥≤ 2ds,t. (8)\nWhen ds,t ≤∥˜θs(xi\ns) −˜θt(xj\nt)∥, by (6) and (7), we have\n∥˜θt(xi\ns) −˜θt(xj\nt)∥≤ ϵ (9)\nThus, if the distance ds,t ≤ ϵ/2, then by (8) and (9) we\nknow xj\nt and xi\ns are (ϵ,˜θt)-similar, i.e., (3) holds.\nThe above condition of ds,t ≤ϵ/2 is equivalent to (4)\nsince d2\ns,t = ∥˜θt(xi\ns)−˜θs(xi\ns)∥2 = ∥˜θt(xi\ns)∥2 +∥˜θs(xi\ns)∥−\n2⟨˜θs(xi\ns),˜θt(xi\ns)⟩= 2 −2⟨˜θt(xi\ns),˜θs(xi\ns)⟩= 2 −2ci\ns. So\nthe greater ci\ns is, the smaller ds,t is.\nAlthough larger value of CFS is proved to be only a nec-\nessary condition for ﬁnding a source domain image close\nto the downstream task, Table 4 also empirically shows that\nthis metric is effective in practice.\n5. IBN-based Convolution Stem\nOur architecture design is based on two important ex-\nperiences. 1) The patchity stem implemented by a stride-\np p×p convolution (p = 16 by default) in the standard\nViT is the key reason of training instability [8]. Recent\nworks show the convolution stem [40, 42] stacked with sev-\neral convolution, BN and ReLU layers can signiﬁcantly\n5\n7x7 conv, 64\nBN, 64\nReLU\n3x3 conv, 64\nBN, 64\nReLU\n3x3 conv, 64\nBN, 64\nReLU\n16x16 conv, 384\n(c) Conv Stem\n7x7 conv, 64\nBN, 32\nReLU\n3x3 conv, 64\nReLU\n3x3 conv, 64\nReLU\n16x16 conv, 384\nIN, 32\nBN, 32IN, 32\nBN, 32\n(d) ICS\n1x1 conv, 64\nBN, 64\nReLU\n3x3 conv, 64\nBN, 64\nReLU\n1x1 conv, 256\nBN, 256\nReLU\n(a) ResNet Block\nReLU\n1x1 conv, 64\nReLU\n3x3 conv, 64\nReLU\n1x1 conv, 256\nBN, 256\nReLU\n(b) IBN-a Block\nReLU\nBN, 32IN, 32\nBN, 32IN, 32\nFigure 2. Comparison of different modules. We refer to the\nIBNNet-a block to design the IBN-based Convolution Stem (ICS).\n(a) ResNet block, (b) IBNNet-a block [33], (c) vanilla convolution\nstem [42], (d) our proposed ICS.\nimprove training stability and peak performance. 2) Data\nbias is a critical challenge for person ReID. By learning in-\nvariant representation, IBNNet-a block [33] has achieved a\nhuge success in many publications or academic challenges\n[11, 31].\nWhen applying ViTs in ReID tasks, a straightforward\nway is to introduce IBNNet-a block into the convolution\nstem. Following the original design of IBNNet-a, we pro-\npose IBN-based convolution stem (ICS) , as shown in Fig-\nure 2, by applying BN for half of the channels and IN for\nthe other half after shallow convolution layers, and applying\nonly BN layers after deep convolution layers. In this paper,\nwe choose to apply IN after the ﬁrst two convolution lay-\ners. Another variant worth consideration but may slightly\nreduce performance is to only apply IN after the ﬁrst con-\nvolution layer. The kernel sizes, channels and strides of the\nconvolution stem are kept the same as in [42]. Hereinafter,\nwe denote ViT with patchify stem, convolution stem, and\nICS as ViT, ViTC and ViTI, respectively\n6. Experiments\n6.1. Implementation Details\nDatasets. The dataset used for pre-training, LUPerson [14],\ncontains 4.18M unlabeled human images in total, collected\nfrom 50,534 online videos. To evaluate the performance on\nReID tasks, we conduct experiments on two popular bench-\nmarks, i.e. Market-1501 (Market) [50] and MSMT17 [41].\nThey contain 32,668 images of 1,501 identities and 126,441\nimages of 4,101 identities, respectively. Images in these two\ndatasets are resized to 256 ×128 during training and infer-\nence stages. Standard evaluation protocols are used with\nthe metrics of mean Average Precision (mAP) and Rank-1\naccuracy.\nPre-training. Unless otherwise speciﬁed, we follow the de-\nfault training settings of all self-supervised methods. Im-\nages are resized to 224 ×224 and 256 ×128 for ImageNet\nand LUPerson, respectively. For pre-training on LUPerson\nwith DINO, the model is trained on 8×V100 GPUs for 100\nepochs and a multi-crop strategy is applied to crop 8 images\nwith 96 ×48 resolution.\nSupervised ReID. The transformer-based baseline pro-\nposed in [19] is used as our baseline in this paper. That\nis to say, none of the overlapping patch embedding, jig-\nsaw patch module or side information embedding is in-\ncluded here. It is noticed that DINO models should be\nﬁne-tuned with a small learning rate and a longer warm-\nup process, so we recommend setting the learning rate to\nlr = 0.0004 ×batchsize\n64 and warm-up epochs to be 20. All\nother settings are same with the original paper.\nUSL/UDA ReID. We follow most of the default settings\nin [11]. USL ReID and UDA ReID share the same train-\ning settings in this paper, and the only difference is that the\nmodels for UDA ReID need to be ﬁrst pre-trained on source\ndatasets before training in an unsupervised manner on target\ndatasets. The maximum distance dbetween two samples is\nset to 0.6 and 0.7 for Market1501 and MSMT17, respec-\ntively. We use SGD optimizer to train ViTs for 50 epochs.\nThe initial learning rate is set to 3.5e-4, and is reduced 10\ntimes for every 20 epochs. Each mini-batch contains 256\nimages of 32 person IDs, i.e. each ID contains 8 images.\nThe rate of stochastic depth is set 0.3.\n6.2. Results of Conditional Pre-training\n88\n90\n92\n94\n96mAP Accuracy(%)\n88.7\n90.1 90.9 91.0 91.1 91.1 90.9 90.9 90.3\nMSMT17 Market\n20% 30% 40% 50% 60% 70% 80% 90% 100%\nData Percentage\n60\n62\n64\n66\n68\n70\n61.7\n64.5\n65.7 66.1 66.3 65.8 65.4 64.7 64.2\n94\n95\n96\n97\n98\n99Rank-1 Accuracy(%)\n94.8\n95.3 95.6 96.0 95.8 95.9 95.8 95.8 95.4\nMSMT17 Market\n20% 30% 40% 50% 60% 70% 80% 90% 100%\nData Percentage\n81\n82\n83\n84\n85\n86\n82.0\n83.8 84.2 84.6 84.7 84.3 84.3 84.0\n83.4\nFigure 3. Supervised ﬁne-tuning performance of our conditional\npre-training with different percentages of pre-training data. All\nresults are achieved by ViT-S. Left and right ﬁgures show mAP\nand Rank-1 accuracy, respectively.\nEffect of partial data. As shown in Figure 3, pre-training\nwith 30% data achieves comparable performance with the\nfull-data pre-training. It is surprising that 40% ∼60% pre-\ntraining data even improves the performance slightly( e.g.\n66.3% vs 64.2% mAP on MSMT17). Therefore, ﬁltering\nirrelevant data with CFS from the pre-training data is actu-\nally beneﬁcial instead of harmful to the down-stream per-\nformance. For a better trade-off between accuracy and pre-\ntraining cost, 50% of the pre-training data will be sampled\nin our following experiments.\nTime consumption of pre-training. Although our con-\nditional pre-training is two-stage, it is still more efﬁcient.\nStandard pre-training of a ViT-S on full LUPerson takes 107\n6\nPre-training Market MSMT17\nmAP R1 mAP R1\nFull (100%) 90.3 95.4 64.2 83.4\nRandom (50%) 89.9 95.2 62.9 82.6\nCluster (50%) [4] 90.0 95.4 63.8 83.3\nCFS (50%) 91.0 96.0 66.1 84.6\nTable 4. Comparison of different data selection strategies. Ran-\ndom sampling (Random) and selecting data close to cluster centers\n(Cluster) are compared.\nPre-training Market MSMT17\nmAP R1 mAP R1\nMocoV2+R50 [14] 88.2 94.8 53.3 76.0\n+CFS (50%) 89.4 95.5 56.8 78.8\nDINO+ViT-S 90.3 95.4 64.2 83.4\n+CFS (50%) 91.0 96.0 66.1 84.6\nTable 5. Conditional pre-training with different SSL pre-training\nmodels. We sample 50% pre-training data based on CFS.\nSupervised ReID USL ReID UDA ReID\nPre-training Market MSMT17 Market MSMT17 MS2MA MA2MS\nData Model mAP R1 mAP R1 mAP R1 mAP R1 mAP R1 mAP R1\nFull\n(100%)\nViT-S/16 90.3 95.4 64.2 83.4 87.8 94.4 38.4 63.8 88.5 95.0 43.9 67.7\nViTC-S/16 90.7 95.7 65.2 84.5 88.3 94.6 39.7 65.2 89.1 95.3 49.0 73.4\nViTI-S/16 91.1 95.9 66.8 85.5 89.3 94.8 48.8 74.4 89.6 95.6 55.0 77.9\n+0.8 +0.5 +2.6 +2.1 +1.5 +0.4 +10.4 +10.6 +1.1 +0.6 +11.1 +10.2\nCFS\n(50%)\nViT-S/16 91.0 96.0 66.1 84.6 88.2 94.2 40.9 66.4 89.4 95.4 47.4 70.8\nViTC-S/16 91.2 95.8 67.8 85.7 89.3 95.0 42.5 67.6 89.7 95.5 55.7 75.5\nViTI-S/16 91.3 96.2 68.1 86.1 89.6 95.3 50.6 75.0 89.9 95.5 57.8 79.5\n+0.3 +0.2 +2.0 +2.5 +1.4 +0.9 +9.7 +8.6 +0.5 +0.1 +10.4 +8.7\nTable 6. Comparison of patchify stem (ViT-S/16), convolution stem (ViTC-S/16) and the proposed ICS (ViTI-S/16). Gray numbers present\nperformance improvements from the proposed ICS.\nhours with 8×V100 GPUs. In our conditional pre-training,\ndata selection step takes 21 hours because the proxy mod-\nels only need to be pre-trained for 20 epochs. For the sec-\nond stage, pre-training the model on 50% of LUPerson only\ntakes 51 hours. In total we can still save about 30% of pre-\ntraining time with a slight performance improvement.\nSelection strategies. Different selection strategies are com-\npared in Table 4. Randomly sampling 50% data for pre-\ntraining does not help with the downnstream performance\nbecause it cannot reduce the noise in the original data.\nAlthough Cluster [4] is a state-of-the-art data selection\nmethod proposed for close-set tasks, it is not suitable for\nthe open-set person ReID problem. Our conditional pre-\ntraining based on CFS performs the best on all bench-\nmarks, even outperforming the full-data pre-training ver-\nsion, which shows its effectiveness. Some image examples\nselected by our method are presented in Appendix.\nDifferent Pre-training models. We evaluate the effective-\nness of our conditional pre-training with both ResNet50 and\nViT-S/16 backbone in Table 5. Consistent improvements\ncan be observed for these two different SSL paradigms,\nwhich demonstrates the universality of our method. Table 6\nalso provides more results of different ViT-S/16 models.\n6.3. Effectiveness of ICS\nTo evaluate the effectiveness of the proposed ICS, we\nperform a fair comparison between patchify stem, convo-\nlution stem and ICS with the ViT-S/16 backbone for both\nfull-data pre-training and the conditional pre-retraining in\nTable 6. Three settings ( e.g. supervised, USL and UDA\nReID) are all included for a comprehensive evaluation.\nWe can observe that ViT I-S/16 outperforms ViT-S/16\nand ViTC-S/16 in most cases. For instance, for the full-\ndata pre-training, ViT I-S/16 improves the mAP by 10.4%\nand 9.1% in USL ReID on MSMT compared with ViT-\nSP/16 and ViT C-S/16, respectively. For the conditional\npre-training, ICS can also consistently improve peak per-\nformance under supervised, UDA and USL settings. There-\nfore, ICS is proved to be an effective module for the ReID\ntask. An interesting phenomenon is that the performance\ngain of ICS decreases in the conditional pre-training case,\nbecause the domain bias between pre-training and down-\nstream datasets has been mitigated.\nModel Complexity and Computational Costs. We keep\nsame number of transformer blocks in Table 6 to clearly\ncompare different patch embeddings. The difference in\ncomputational complexity between ICS and convolution\nstem is negligible. [42] has shown that the convolution stem\nhas approximately the same complexity as a single trans-\nformer block, and adding the convolution stem while re-\nmoving one transformer block can maintain similar model\ncomplexity without affecting accuracy. To further verify\nthis point, extra experiments are conducted by removing\none transformer block in the CFS-based supervised setting (\nViTI-S/16 in “CFS” section of Table 6), we achieve 91.2%\nmAP and 96.1% rank-1 accuracy on Market, almost the\n7\nMarket MSMT17\nMethods Backbone mAP R1 mAP R1\nBOT [31] R50-IBN 88.2 95.0 57.8 80.7\nBOT∗ [31] R50-IBN 90.1 95.6 60.8 81.4\nMGN [39] R50 ↑384 87.5 95.1 63.7 85.1\nSCSN [7] R50 ↑384 88.5 95.7 58.5 83.8\nABDNet [5] R50 ↑384 88.3 95.6 60.8 82.3\nTransReID [19] ViT-B↑384 89.5 95.2 69.4 89.2\nMoCoV2∗ [14] MGN ↑384 91.0 96.4 65.7 85.5\nOurs∗ ViTI-S 91.3 96.2 68.1 86.1\nOurs∗ ViTI-S↑384 91.7 96.3 70.5 87.8\nOurs∗ ViTI-B↑384 93.2 96.7 75.0 89.5\nTable 7. Comparison to state-of-the-art methods in supervised\nReID. MGN is the improved version in fast-reid. ∗ means that\nbackbones are pre-trained on LUPerson. ↑384 represents that\nimages are resized to 384 ×128. From the perspective of\ncomputational complexity, ViTI-S and ViTI-B can be compared\nwith R50/R50-IBN and MGN, respectively. R50-IBN stands for\nResNet50-IBN-a.\nsame as the numbers in Table 6. Therefore, it is reasonable\nto expect that the performance of our other experiments will\nbe unchanged if we remove a transformer block to maintain\nthe complexity comparable to the vanilla ViT models.\nRemark. We have also tried the trick proposed in Mo-\ncoV3 [8] to freeze patchify stem. It can bring in slight per-\nformance increase but is still worse than convolution stem\nand ICS. More experiments are included in the Appendix to\nshow that ICS learns more invariant feature representations.\n6.4. Comparison to State-of-the-Art methods\nSupervised ReID. We compare to some of the state-\nof-the-art methods on supervised ReID in Table 7. Our\nViT-S↑384 outperforms MGN and ABDNet on both bench-\nmarks by a large margin. It is worth noting that Tran-\nsReID is pre-trained on ImageNet-21K and integrates ad-\nditional camera information and part features to achieve the\ncurrent performance. With self-supervised pre-training on\nLUPerson, our ViT-B↑384 obtains 93.2%/75.0% mAP and\n96.7%/89.5% rank-1 accuracy on Market1501/MSMT17\ndatasets, signiﬁcantly outperforming TransReID with no\nadditional modules. It is also observed that MGN bene-\nﬁts from the self-supervised pre-training on LUPerson via\nMocoV2, but is still inferior to our results.\nUSL ReID. Our methods are compared to MMCL [46],\nHCT [47], IICS [44], SPCL [17] and C-Contrast [11] in Ta-\nble 8, where the last two methods also adopt the pre-trained\nmodel on LUPerson. Our best results boost the mAP per-\nformance by 3.2% (89.6% vs 86.4%) and 10.8% (50.6% vs\n39.8%) on Market and MSMT17, respectively.\nUDA ReID. Some latest UDA-ReID methods are com-\npared in the Table 9. Among all existing state-of-the-\nart methods, C-Contrast achieves the best performance at\nMarket MSMT17\nMethods Backbone mAP R1 mAP R1\nMMCL [46] R50 45.5 80.3 11.2 35.4\nHCT [47] R50 56.4 80.0 - -\nIICS [44] R50 72.9 89.5 26.9 52.4\nSPCL∗ [17] R50 76.2 90.2 - -\nC-Contrast [11] R50 82.6 93.0 33.1 63.3\nC-Contrast∗ [11] R50 84.0 93.4 31.4 58.8\nC-Contrast∗ [11] R50-IBN 86.4 94.2 39.8 66.1\nOurs∗ ViT-S 88.2 94.2 40.9 66.4\nOurs∗ ViTI-S 89.6 95.3 50.6 75.0\nTable 8. Comparison to state-of-the-art methods in USL ReID. ∗\nmeans that backbones are pre-trained on LUPerson.\nMS2MA MA2MS\nMethods Backbone mAP R1 mAP R1\nDG-Net++ [52] R50 64.6 83.1 22.1 48.4\nMMT [16] R50 75.6 89.3 24.0 50.1\nSPCL [17] R50 77.5 89.7 26.8 53.7\nSPCL [17] R50-IBN 79.9 92.0 31.0 58.1\nC-Contrast [11] R50 82.4 92.5 33.4 60.5\nC-Contrast∗ [11] R50 85.1 94.4 28.3 53.8\nC-Contrast∗ [11] R50-IBN 86.9 94.6 42.6 69.1\nOurs∗ ViT-S 89.4 95.4 47.4 70.8\nOurs∗ ViTI-S 89.9 95.5 57.8 79.5\nTable 9. Comparison to state-of-the-art methods in UDA ReID. ∗\nmeans that backbones are pre-trained on LUPerson.\n86.9% and 42.6% mAP on MS2MA and MA2MS, respec-\ntively. Our method with both ViT-S and ViT I-S surpasses\nC-Contrast by a large margin. Especially, ViT I-S obtains\n89.9% mAP on MS2MA and 57.8% mAP on MA2MS,\nwhich are already comparable to many supervised methods.\n7. Conclusions and Discussions\nThis paper mitigate the domain gap between pre-training\nand ReID datasets for transformer-based person ReID from\naspects of data and model. After observing that the ex-\nisting pre-training paradigms of person ReID cannot per-\nform well for transformer-based backbones, we investigate\nSSL methods with ViTs on the LUPerson and ﬁnd DINO\nis the most suitable pre-training method for transformer-\nbased ReID. To further bridge the gap between pre-training\nand ReID datasets, we propose a conditional pre-training\nmethod based on CFS to select relevant pre-training data to\nthe target domain. The proposed method can speed up pre-\ntraining without performance drop. For the model structure,\na ReID-speciﬁc patch embedding called IBN-based convo-\nlution stem is proposed to improve the peak performance.\nWe believe the promising performance will inspire more\nwork to study the SSL pre-training for the transformer-\nbased models towards person ReID, i.e., a more suitable\n8\nViT variant, or a ReID-speciﬁc pre-training framework, etc.\nLimitations. The pre-trained models are only suitable\nfor the person ReID but cannot perform well on other less\nrelated tasks such as vehicle ReID, human parsing, or image\nclassiﬁcation, etc.\nReferences\n[1] Martin Anthony, Peter L Bartlett, and Peter L Bartlett. Neu-\nral network learning: Theoretical foundations , volume 9.\ncambridge university press Cambridge, 1999. 15\n[2] Shai Ben-David, John Blitzer, Koby Crammer, Alex\nKulesza, Fernando Pereira, and Jennifer Wortman Vaughan.\nA theory of learning from different domains.Machine learn-\ning, 79(1):151–175, 2010. 14, 15\n[3] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In\nICCV, 2021. 1, 2\n[4] Shuvam Chakraborty, Burak Uzkent, Kumar Ayush, Kumar\nTanmay, Evan Sheehan, and Stefano Ermon. Efﬁcient con-\nditional pre-training for transfer learning. arXiv preprint\narXiv:2011.10231, 2020. 2, 3, 4, 7\n[5] Tianlong Chen, Shaojin Ding, Jingyi Xie, Ye Yuan, Wuyang\nChen, Yang Yang, Zhou Ren, and Zhangyang Wang. Abd-\nnet: Attentive but diverse person re-identiﬁcation. In ICCV,\n2019. 8\n[6] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.\nImproved baselines with momentum contrastive learning.\narXiv preprint arXiv:2003.04297, 2020. 2\n[7] Xuesong Chen, Canmiao Fu, Yong Zhao, Feng Zheng,\nJingkuan Song, Rongrong Ji, and Yi Yang. Salience-guided\ncascaded suppression network for person re-identiﬁcation. In\nCVPR, pages 3300–3310, 2020. 8\n[8] Xinlei Chen, Saining Xie, and Kaiming He. An empiri-\ncal study of training self-supervised vision transformers. In\nICCV, pages 9640–9649, October 2021. 1, 2, 5, 8\n[9] C Coleman, C Yeh, S Mussmann, B Mirzasoleiman, P Bailis,\nP Liang, J Leskovec, and M Zaharia. Selection via proxy:\nEfﬁcient data selection for deep learning. In ICLR, 2020. 2,\n3, 4\n[10] Yin Cui, Yang Song, Chen Sun, Andrew Howard, and\nSerge Belongie. Large scale ﬁne-grained categorization and\ndomain-speciﬁc transfer learning. In CVPR, pages 4109–\n4118, 2018. 2, 3\n[11] Zuozhuo Dai, Guangyuan Wang, Weihao Yuan, Siyu Zhu,\nand Ping Tan. Cluster contrast for unsupervised person re-\nidentiﬁcation. arXiv preprint arXiv:2103.11568, 2021. 2, 3,\n6, 8\n[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, pages 248–255. Ieee, 2009. 1\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In ICLR, 2020. 1\n[14] Dengpan Fu, Dongdong Chen, Jianmin Bao, Hao Yang, Lu\nYuan, Lei Zhang, Houqiang Li, and Dong Chen. Unsu-\npervised pre-training for person re-identiﬁcation. In CVPR,\npages 14750–14759, 2021. 1, 2, 3, 6, 7, 8\n[15] Weifeng Ge and Yizhou Yu. Borrowing treasures from the\nwealthy: Deep transfer learning through selective joint ﬁne-\ntuning. In CVPR, pages 1086–1095, 2017. 2, 3\n[16] Yixiao Ge, Dapeng Chen, and Hongsheng Li. Mutual mean-\nteaching: Pseudo label reﬁnery for unsupervised domain\nadaptation on person re-identiﬁcation. In ICLR, 2020. 8\n[17] Yixiao Ge, Feng Zhu, Dapeng Chen, Rui Zhao, and Hong-\nsheng Li. Self-paced contrastive learning with hybrid mem-\nory for domain adaptive object re-id. In NeurIPS, 2020. 8\n[18] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin\nTallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch,\nBernardo Pires, Zhaohan Guo, Mohammad Azar, et al. Boot-\nstrap your own latent: A new approach to self-supervised\nlearning. In NIPS, 2020. 2\n[19] Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li,\nand Wei Jiang. Transreid: Transformer-based object re-\nidentiﬁcation. In ICCV, pages 15013–15022, October 2021.\n1, 2, 3, 6, 8\n[20] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-\ning the knowledge in a neural network. arXiv preprint\narXiv:1503.02531, 2015. 3\n[21] Wassily Hoeffding. Probability inequalities for sums of\nbounded random variables. Journal of the American Statis-\ntical Association, 58(301):13–30, 1963. 15\n[22] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. In ICML, pages 448–456. PMLR, 2015. 2\n[23] Longlong Jing and Yingli Tian. Self-supervised visual fea-\nture learning with deep neural networks: A survey. TPAMI,\n2020. 2\n[24] Daniel Kifer, Shai Ben-David, and Johannes Gehrke. De-\ntecting change in data streams. In VLDB, volume 4, pages\n180–191. Toronto, Canada, 2004. 15\n[25] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel\nVeness, Guillaume Desjardins, Andrei A Rusu, Kieran\nMilan, John Quan, Tiago Ramalho, Agnieszka Grabska-\nBarwinska, et al. Overcoming catastrophic forgetting in neu-\nral networks. PNAS, 114(13):3521–3526, 2017. 2, 4\n[26] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and\nGeoffrey Hinton. Similarity of neural network represen-\ntations revisited. In International Conference on Machine\nLearning, pages 3519–3529. PMLR, 2019. 11\n[27] Yulin Li, Jianfeng He, Tianzhu Zhang, Xiang Liu, Yongdong\nZhang, and Feng Wu. Diverse part discovery: Occluded per-\nson re-identiﬁcation with part-aware transformer. In CVPR,\npages 2898–2907, 2021. 3\n[28] Xuehu Liu, Pingping Zhang, Chenyang Yu, Huchuan Lu,\nXuesheng Qian, and Xiaoyun Yang. A video is worth three\nviews: Trigeminal transformers for video-based person re-\nidentiﬁcation. arXiv preprint arXiv:2104.01745, 2021. 3\n[29] Ziquan Liu, Yi Xu, Yuanhong Xu, Qi Qian, Hao Li, Antoni B\nChan, and Rong Jin. Improved ﬁne-tuning by leveraging pre-\ntraining data: Theory and practice. 2021. 2, 3, 4\n9\n[30] Hao Luo, Youzhi Gu, Xingyu Liao, Shenqi Lai, and Wei\nJiang. Bag of tricks and a strong baseline for deep person\nre-identiﬁcation. In CVPRW, pages 0–0, 2019. 2\n[31] Hao Luo, Wei Jiang, Youzhi Gu, Fuxu Liu, Xingyu Liao,\nShenqi Lai, and Jianyang Gu. A strong baseline and batch\nnormalization neck for deep person re-identiﬁcation. TMM,\n22(10):2597–2609, 2019. 6, 8\n[32] Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units\nimprove restricted boltzmann machines. In ICML, 2010. 2\n[33] Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two\nat once: Enhancing learning and generalization capacities\nvia ibn-net. In ECCV, pages 464–479, 2018. 2, 6\n[34] Vinay Venkatesh Ramasesh, Ethan Dyer, and Maithra\nRaghu. Anatomy of catastrophic forgetting: Hidden repre-\nsentations and task semantics. In ICLR, 2021. 2, 4\n[35] Fei Shen, Yi Xie, Jianqing Zhu, Xiaobin Zhu, and Huan-\nqiang Zeng. Git: Graph interactive transformer for vehicle\nre-identiﬁcation. arXiv preprint arXiv:2107.05475, 2021. 3\n[36] Brian Thompson, Jeremy Gwinnup, Huda Khayrallah, Kevin\nDuh, and Philipp Koehn. Overcoming catastrophic forgetting\nduring domain adaptation of neural machine translation. In\nNAACL, pages 2062–2068, 2019. 2, 4\n[37] VN Vapnik. On the uniform convergence of relative frequen-\ncies of events to their probabilities. Theory of Probability\nand its Applications, 16(2):264–281, 1971. 13\n[38] Vladimir Vapnik. The nature of statistical learning theory .\nSpringer science & business media, 2013. 13\n[39] Guanshuo Wang, Yufeng Yuan, Xiong Chen, Jiwei Li, and Xi\nZhou. Learning discriminative features with multiple granu-\nlarities for person re-identiﬁcation. In ACMMM, pages 274–\n282, 2018. 8\n[40] Pichao Wang, Xue Wang, Hao Luo, Jingkai Zhou, Zhipeng\nZhou, Fan Wang, Hao Li, and Rong Jin. Scaled relu\nmatters for training vision transformers. arXiv preprint\narXiv:2109.03810, 2021. 2, 5\n[41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.\nPerson transfer gan to bridge domain gap for person re-\nidentiﬁcation. In CVPR, pages 79–88, 2018. 1, 6\n[42] Tete Xiao, Piotr Dollar, Mannat Singh, Eric Mintun, Trevor\nDarrell, and Ross Girshick. Early convolutions help trans-\nformers see better. In NeurIPS, 2021. 2, 5, 6, 7\n[43] Zhenda Xie, Yutong Lin, Zhuliang Yao, Zheng Zhang, Qi\nDai, Yue Cao, and Han Hu. Self-supervised learning with\nswin transformers. arXiv preprint arXiv:2105.04553, 2021.\n1\n[44] Shiyu Xuan and Shiliang Zhang. Intra-inter camera similar-\nity for unsupervised person re-identiﬁcation. InCVPR, pages\n11926–11935, 2021. 8\n[45] Xi Yan, David Acuna, and Sanja Fidler. Neural data server:\nA large-scale search engine for transfer learning data. InPro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 3893–3902, 2020. 3\n[46] Hong-Xing Yu, Wei-Shi Zheng, Ancong Wu, Xiaowei Guo,\nShaogang Gong, and Jian-Huang Lai. Unsupervised person\nre-identiﬁcation by soft multilabel learning. In CVPR, pages\n2148–2157, 2019. 8\n[47] Kaiwei Zeng, Munan Ning, Yaohua Wang, and Yang Guo.\nHierarchical clustering with hard-batch triplet loss for person\nre-identiﬁcation. In CVPR, pages 13657–13665, 2020. 8\n[48] Guowen Zhang, Pingping Zhang, Jinqing Qi, and Huchuan\nLu. Hat: Hierarchical aggregation transformers for person\nre-identiﬁcation. arXiv preprint arXiv:2107.05946, 2021. 1,\n3\n[49] Tianyu Zhang, Longhui Wei, Lingxi Xie, Zijie Zhuang,\nYongfei Zhang, Bo Li, and Qi Tian. Spatiotemporal\ntransformer for video-based person re-identiﬁcation. arXiv\npreprint arXiv:2103.16469, 2021. 3\n[50] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-\ndong Wang, and Qi Tian. Scalable person re-identiﬁcation:\nA benchmark. In ICCV, pages 1116–1124, 2015. 1, 6\n[51] Kuan Zhu, Haiyun Guo, Shiliang Zhang, Yaowei Wang,\nGaopan Huang, Honglin Qiao, Jing Liu, Jinqiao Wang, and\nMing Tang. Aaformer: Auto-aligned transformer for person\nre-identiﬁcation. arXiv preprint arXiv:2104.00921, 2021. 2\n[52] Yang Zou, Xiaodong Yang, Zhiding Yu, BVK Vijaya Kumar,\nand Jan Kautz. Joint disentangling and adaptation for cross-\ndomain person re-identiﬁcation. In Computer Vision–ECCV\n2020: 16th European Conference, Glasgow, UK, August 23–\n28, 2020, Proceedings, Part II 16 , pages 87–104. Springer,\n2020. 8\n10\nA. Conditional Pre-training on ImageNet\nIn this section, our proposed conditional pre-training is\nfurther applied on ImageNet in Table 10. Since most of\nthe images in ImageNet are irrelevant to person images, it\nis difﬁcult to select enough data for the ReID task. There-\nfore, the pre-training on 50% data is inferior to the full pre-\ntraining version, but pre-training on CFS-selected data still\noutperforms the random sampling strategy, which shows the\neffectiveness of the CFS-based selection strategy.\nPre-training Market MSMT17\nmAP R1 mAP R1\nFull (100%) 84.6 93.1 54.8 76.7\nRandom (50%) 81.6 91.4 52.5 75.2\nCFS (50%) 83.8 92.8 53.8 76.2\nTable 10. Conditional Pre-training on ImageNet. The backbone\nis vanilla ViT-S.\nB. Image Examples with High CFS\n(a) Sampled data with high CFS\n(b) Filtered data with low CFS\n(c) Data in the target domain\nFigure 4. Some random examples of selected data with high CFS\nand ﬁltered data with low CFS. Images in the ﬁrst two rows are\nfrom the source domain (LUPerson). The last row shows images\nsampled from the target domain (Market and MSMT17). Images\nin each row have been rearranged for the best view.\nAs Figure 4 shows, the sampled images with higher CFS\nare more similar to the data in the target domain. In con-\ntrast, the ﬁltered images with lower CFS are of low-quality\nor have great domain gap with the target datasets. These\nﬁltered images cannot provide enough discriminative ID in-\nformation during pre-training. Therefore, our conditional\npre-training can effectively mitigate the domain gap be-\ntween the pre-training and target datasets by removing those\nirrelevant or harmful data.\nC. Analysis of Feature Representation\nTable 11 shows an experiment on Market-1501 to ana-\nlyze the feature invariance of pre-trained models. The orig-\ninal dataset is extended into six variations by applying six\ndifferent augmentations to each image, to simulate some\nimportant appearance variances in the ReID task (exam-\nples shown in Figure 5). Then, Centered Kernel Alignment\n(CKA) [26] scores can evaluate the similarity between the\nfeatures of the origin dataset and the features of the each\nsimulated dataset. The higher CKA score is, the features\nprovided by the pre-trained model show better invariance on\nthis type of augmentation. ViT I-S achieves the best CKA\nscores for all types of appearance variance, showing that\nICS is beneﬁcial for learning invariant features.\nContrastBrightness Cropping Flip ScalingOrigin\n Saturation\nFigure 5. Examples of generated images. Six different augmen-\ntations are included, i.e. brightness, contrast, saturation, cropping,\nﬂip, and scaling, which are all important for appearance variance\nin ReID. The ﬁrst two types of augmentation are related to color\ninformation and the last four are related to texture information.\nPre-train Brig Cont Sat Crop Flip Scale\nViTI-S 0.809 0.818 0.998 0.927 0.996 0.820\nViTC-S 0.727 0.691 0.997 0.919 0.996 0.794\nViT-S 0.702 0.664 0.997 0.877 0.996 0.728\nTable 11. Feature invariance of different patch embedding meth-\nods. The table compares the CKA scores of different pre-trained\nmodels for different augmentations. Brig, Cont, Sat, Crop, Flip,\nand Scale represent augmentation of brightness, contrast, satura-\ntion, cropping, ﬂip, and scaling, respectively.\nD. Improvements of LUPerson Pre-training\nPre-training Market MSMT17\nModels Data #Images mAP R1 mAP R1\nViT-S IMG-1K 1.28M 85.0 93.8 53.5 75.2\nLUP 4.18M 90.3 95.4 64.2 83.4\nViT-B IMG-21K 14.2M 86.8 94.7 61.0 81.8\nLUP 4.18M 92.5 96.5 70.1 87.3\nTable 12. SSL Pre-training on LUPerson can consistently im-\nprove the performance of supervised pre-training on ImageNet-1k\nor ImageNet-21k. Conditional pre-training or ICS is not included.\n11\nAlgorithm 2 Source codes of Patch Embedding for IBN-based Convolution Stem.\n1 class IBN(nn.Module):\n2 def __init__(self, planes):\n3 super(IBN, self).__init__()\n4 half1 = int(planes/2)\n5 self.half = half1\n6 half2 = planes - half1\n7 self.IN = nn.InstanceNorm2d(half1, affine=True)\n8 self.BN = nn.BatchNorm2d(half2)\n9\n10 def forward(self, x):\n11 split = torch.split(x, self.half, 1)\n12 out1 = self.IN(split[0].contiguous())\n13 out2 = self.BN(split[1].contiguous())\n14 out = torch.cat((out1, out2), 1)\n15 return out\n16\n17 class PatchEmbed(nn.Module):\n18 def __init__(self, img_size=224, patch_size=16, stride_size=16, in_chans=3, embed_dim=768,\nstem_conv=False):\n19 super().__init__()\n20 img_size = to_2tuple(img_size)\n21 patch_size = to_2tuple(patch_size)\n22 stride_size_tuple = to_2tuple(stride_size)\n23 self.num_x = (img_size[1] - patch_size[1]) // stride_size_tuple[1] + 1\n24 self.num_y = (img_size[0] - patch_size[0]) // stride_size_tuple[0] + 1\n25 print(’using stride: {}, and patch number is num_y{} * num_x{}’.format(stride_size, self.num_y,\nself.num_x))\n26 self.num_patches = self.num_x * self.num_y\n27 self.img_size = img_size\n28 self.patch_size = patch_size\n29\n30 self.stem_conv = stem_conv\n31 if self.stem_conv:\n32 hidden_dim = 64\n33 stem_stride = 2\n34 stride_size = patch_size = patch_size[0] // stem_stride\n35 self.conv = nn.Sequential(\n36 nn.Conv2d(in_chans, hidden_dim, kernel_size=7, stride=stem_stride, padding=3,bias=False\n),\n37 IBN(hidden_dim),\n38 nn.ReLU(inplace=True),\n39 nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=1,padding=1,bias=False),\n40 IBN(hidden_dim),\n41 nn.ReLU(inplace=True),\n42 nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=1,padding=1,bias=False),\n43 nn.BatchNorm2d(hidden_dim),\n44 nn.ReLU(inplace=True),\n45 )\n46 in_chans = hidden_dim\n47 self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride_size)\n48\n49 def forward(self, x):\n50 if self.stem_conv:\n51 x = self.conv(x)\n52 x = self.proj(x)\n53 x = x.flatten(2).transpose(1, 2)\n54 return x\n55\n56 # ICS for ViT-S in this paper\n57 # self.patch_embed = PatchEmbed(img_size=(256,128), patch_size=16, stride_size=16, in_chans=3 ,\nembed_dim=384, stem_conv=True)\n12\nE. Theoretical Analysis of Generalization Performance\nIn this section, we give a theoretical analysis of generalization performance relating source and target data. In particular,\nwe consider excess risk bound (ERB) as a measure of generalization performance, which is commonly used in the learning\ntheory literature [38]. The following informal theorem highlights the factors that inﬂuence the generalization of target task,\nwhich dramatically simplify Theorem 3.\nTheorem 2 (Informal Version of Theorem 3). Under the condition that the sample size of source data is larger than that of\ntarget data, then we have the following tow propositions to help improve generalization performance of target task:\n• use the examples from the source data that close to target data;\n• use a small learning rate to slightly update the backbone during the ﬁne-tuning process.\nIn the remaining of this section, we ﬁrst introduce the problem setting, and then present the formal theoretical result with\nits proof. To simplify the presentation of the proof, we include some supporting lemmas at the end of this section.\nE.1. Problem Setting\nTo give an insight of the proposed method from the view of theoretical side, we formalize the problem as follows. We\nconsider binary classiﬁcation for simplicity. A domain is deﬁned as a pair consisting of a distribution Don inputs Xand a\nlabeling function f : X→Y with Y:= {0,1}. The labeling function can have a fractional (expected) value when labeling\noccurs non-deterministically. We denote by ⟨Ds,fs⟩the source domain and ⟨Dt,ft⟩the target domain. A hypothesis is a\nfunction h∈H : X→Y , where His a hypothesis space on Xwith VC dimension [37] d. The loss on source domain that a\nhypothesis hdisagrees with a labeling function f is deﬁned as\nFs(h,f) := Ex∼Ds [|h(x) −f(x)|] , (10)\nwhere f can also be a hypothesis. Its empirical version is written as\nˆFs(h) := 1\n|Us|\n∑\nx∈Us\n|h(x) −f(x)|, (11)\nwhere Us is sampled from Ds and |Us|is the sample size. Similarly, for target domain we deﬁne\nFt(h,f) := Ex∼Dt [|h(x) −f(x)|] , (12)\nˆFt(h) := 1\n|Ut|\n∑\nx∈Ut\n|h(x) −f(x)|, (13)\nwhere Ut is sampled from Dt and |Ut|is the sample size.. Since the label function is deterministic for target task, the risk of\na hypothesis is deﬁned as\nFt(h) := Ex∼Dt [|h(x) −ft(x)|] , (14)\nand the empirical risk is\nˆFt(h) := 1\n|Ut|\n∑\nx∈Ut\n|h(x) −ft(x)|. (15)\nLet deﬁne\nh∗:= arg min\nh∈H\nFt(h),\n˜h∗, ˜f∗:= arg min\nh∈H,f∈H\nFs(h,f),\nF∗:=Ft(h∗) + Fs(˜h∗, ˜f∗).\n13\nThe excess risk is deﬁned by\nFt(h) −Ft(h∗), (16)\nwhich can be considered as a measure of generalization performance. Our goal is to minimize the excess risk.\nFor a hypothesis space H, we give the deﬁnitions of the symmetric difference hypothesis space H∆Hand its divergence\nfollowing by [2].\nDeﬁnition 3. [2] For a hypothesis space H, the symmetric difference hypothesis space H∆His the set of hypotheses\ng ∈H∆H⇔ g(x) = h(x) ⊕h′(x) forsome h,h′∈H, where ⊕is the XOR function. That is, every hypothesis g ∈H∆H\nis the set of disagreements between two hypotheses in H.\nDeﬁnition 4. [2] For a hypothesis space H, the H∆H-distance between two distributions D1 and D2 is deﬁned as\ndH∆H(D1,D2) = sup\nh,h′∈H\n|Prx∼D1 (h(x) ̸= h′(x)) −Prx∼D2 (h(x) ̸= h′(x))|.\nRemark. The empirical version of H∆H-distance, denoted by ˆdH∆H(Us,Ut), can be considered as the a measure of\nsimilarity between the sampled source data Us and the sampled target data Ut.\nE.2. Main Result and its Proof\nTheorem 3 (Formal Version). Let Us, Ut ﬁnite samples of sizes m, n(m ≫n) drawn i.i.d. according Ds, Dt respectively.\nˆdH∆H(Us,Ut) is the empirical H∆H-divergence between samples, then for any δ ∈(0,1), with probability at least 1 −δ,\nwe have\nFt(h) −Ft(h∗) ≤3\n2\nˆdH∆H(Us,Ut) + ˆFt(h,˜h∗) + Ft(h∗, ˜f∗) + Fs(˜h∗, ˜f∗)\n+\n√\nlog(8/δ)\n2n + 12\n√\n2dlog(2n) + log(8/δ)\nn . (17)\nRemark. We make several comments of the ERB in Theorem 3. First, the bound shows that the unlabeled empirical\nH∆H-divergence (i.e., ˆdH∆H(Us,Ut), the ﬁrst term of the left hand side of bound (17)) is important to improve the general-\nization performance (reduce the excess risk), indicating that when the used source data and target data are close, it will have\na good generalization performance in target task. Second, the bound needs the term ˆFt(h,˜h∗) is small for a hypothesis hand\nthe optimal hypothesis ˜h∗of source domain, which means that hshould be not too far from ˜h∗. This is consistent with the\npractice that usually a small learning rate is used to slightly update the backbone during the ﬁne-tuning process. Finally, the\nterm Fs(˜h∗, ˜f∗) in the bound tells us that the source error obtained during the pre-training process is also important to the\ntarget performance.\nProof of Theorem 3. By Lemma 1 we have\nFt(h) −Ft(h∗) ≤Ft(h,h∗)\n≤Fs(h,h∗) + |Ft(h,h∗) −Fs(h,h∗)|\n≤Fs(h,h∗) + sup\nh,h′∈H\n|Ft(h,h′) −Fs(h,h′)|\n=Fs(h,h∗) + sup\nh,h′∈H\n|Prx∼Dt (h(x) ̸= h′(x)) −Prx∼Ds (h(x) ̸= h′(x))|\n=Fs(h,h∗) + 1\n2dH∆H(Ds,Dt) (by Deﬁnition 3)\n≤Fs(h∗, ˜f∗) + Fs(h,˜h∗) + Fs(˜h∗, ˜f∗) + 1\n2dH∆H(Ds,Dt)\n(a)\n≤Fs(h∗, ˜f∗) + Fs(h,˜h∗) + Fs(˜h∗, ˜f∗) + 1\n2\nˆdH∆H(Us,Ut) + 4\n√\n2dlog(2n) + log(2/δ)\nn\n(b)\n≤Fs(h∗, ˜f∗) + ˆFs(h,˜h∗) +\n√\nlog(4/δ)\n2m + Fs(˜h∗, ˜f∗)\n+ 1\n2\nˆdH∆H(Us,Ut) + 4\n√\n2dlog(2n) + log(4/δ)\nn .\n14\nHere (a) uses Lemma 2 with the fact that since every g ∈H∆Hcan be represented as a linear threshold network of depth 2\nwith 2 hidden units, the VC dimension of H∆His at most 2d[1, 2]; (b) uses Hoeffding’s inequality in Lemma 3. Similarly,\nwe have\nFt(h) −Ft(h∗) ≤Ft(h∗, ˜f∗) + Ft(h,˜h∗) + Fs(˜h∗, ˜f∗) + 3\n2\nˆdH∆H(Us,Ut) + 12\n√\n2dlog(2n) + log(8/δ)\nn\n≤Ft(h∗, ˜f∗) + ˆFt(h,˜h∗) + Fs(˜h∗, ˜f∗) + 3\n2\nˆdH∆H(Us,Ut)\n+\n√\nlog(8/δ)\n2n + 12\n√\n2dlog(2n) + log(8/δ)\nn .\nE.3. Supporting Lemmas\nLemma 1. Deﬁne F(h,f) := Ex∼D[|h(x) −f(x)|]. For any h1,h2,h3 ∈H, we have F(h1,h2) = F(h2,h1) and\nF(h1,h2) ≤F(h1,h3) + F(h2,h3). (18)\nProof. By the deﬁnition of F(h,f), we know this is symmetric. On the other hand, we have\nF(h1,h2) =Ex∼D[|h1(x) −h2(x)|]\n≤Ex∼D[|h1(x) −h3(x)|+ |h2(x) −h3(x)|]\n≤Ex∼D[|h1(x) −h3(x)|] + Ex∼D[|h2(x) −h3(x)|]\n=F(h1,h3) + F(h2,h3).\nLemma 2. Let Hbe a hypothesis space on Xwith VC dimension d. D1 and D2 be two distribution on Xand U1, U2 ﬁnite\nsamples of sizes n1, n2 (n1 ≫n2) drawn i.i.d. according D1, D2 respectively. ˆdH∆H(U1,U2) is the empirical H-divergence\nbetween samples, then for any δ∈(0,1), with probability at least 1 −δ, we have\ndH∆H(D1,D2) ≤ˆdH∆H(U1,U2) + 4\n√\ndlog(2n2) + log(2/δ)\nn2\n. (19)\nProof. This lemma can be proved by a slight modiﬁcation of Theorem 3.4 of [24]. To this end, by setting ϵ =\n4\n√\ndlog(2n2)+log(2/δ)\nn2\nand ˜ϵ= 4\n√\ndlog(2n1)+log(2/δ)\nn1\n, then we know ϵ≤˜ϵ(since n1 ≫n2) and\n(2n1)dexp\n(\n−ϵ2n1\n16\n)\n≤(2n1)dexp\n(\n−˜ϵ2n1\n16\n)\n= δ/2,\n(2n2)dexp\n(\n−ϵ2n2\n16\n)\n= δ/2.\nWe then obtain (19) by using Theorem 3.4 of [24].\nLemma 3 (Hoeffding’s inequality [21]). Let X1,...,X n be i.i.d. random variables with bounded intervals: ai ≤Xi ≤bi,\ni= 1,...,n . Let ¯X = 1\nn\n∑n\ni=1 Xi, then we have\nPr\n(⏐⏐¯X−E[ ¯X]\n⏐⏐≥c\n)\n≤2 exp\n(\n− 2n2c2\n∑n\ni=1(bi −ai)2\n)\n.\n15",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7532821893692017
    },
    {
      "name": "Transformer",
      "score": 0.6209850907325745
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6189990639686584
    },
    {
      "name": "Machine learning",
      "score": 0.577805757522583
    },
    {
      "name": "Forgetting",
      "score": 0.4755396842956543
    },
    {
      "name": "Labeled data",
      "score": 0.45649731159210205
    },
    {
      "name": "Training set",
      "score": 0.4464277923107147
    },
    {
      "name": "Domain adaptation",
      "score": 0.4435079097747803
    },
    {
      "name": "Supervised learning",
      "score": 0.43612900376319885
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.33349156379699707
    },
    {
      "name": "Artificial neural network",
      "score": 0.21651163697242737
    },
    {
      "name": "Engineering",
      "score": 0.09054392576217651
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 40
}