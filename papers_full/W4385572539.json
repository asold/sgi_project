{
  "title": "Reanalyzing L2 Preposition Learning with Bayesian Mixed Effects and a Pretrained Language Model",
  "url": "https://openalex.org/W4385572539",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2560654518",
      "name": "Jakob Prange",
      "affiliations": [
        "Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A2782801930",
      "name": "Man Ho Ivy Wong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W4286895929",
    "https://openalex.org/W2022218664",
    "https://openalex.org/W380269942",
    "https://openalex.org/W4310820245",
    "https://openalex.org/W1988301765",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W366212185",
    "https://openalex.org/W2095881835",
    "https://openalex.org/W2531882892",
    "https://openalex.org/W3011954297",
    "https://openalex.org/W2004509085",
    "https://openalex.org/W2016188383",
    "https://openalex.org/W4311071470",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W3118784782",
    "https://openalex.org/W3129857645",
    "https://openalex.org/W2897613819",
    "https://openalex.org/W2966638880",
    "https://openalex.org/W2941666437",
    "https://openalex.org/W2739643355",
    "https://openalex.org/W207005892",
    "https://openalex.org/W2295951612",
    "https://openalex.org/W3161733629",
    "https://openalex.org/W4224311054",
    "https://openalex.org/W2069479944",
    "https://openalex.org/W3157088778",
    "https://openalex.org/W4287854517",
    "https://openalex.org/W4287888335",
    "https://openalex.org/W3195398731",
    "https://openalex.org/W4317838060",
    "https://openalex.org/W2250699162",
    "https://openalex.org/W2624054409",
    "https://openalex.org/W3093549722",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2887634819",
    "https://openalex.org/W2964262197",
    "https://openalex.org/W2914098476",
    "https://openalex.org/W1535520578"
  ],
  "abstract": "We use both Bayesian and neural models to dissect a data set of Chinese learnersâ€™ pre- and post-interventional responses to two tests measuring their understanding of English prepositions. The results mostly replicate previous findings from frequentist analyses and newly reveal crucial interactions between student ability, task type, and stimulus sentence. Given the sparsity of the data as well as high diversity among learners, the Bayesian method proves most useful; but we also see potential in using language model probabilities as predictors of grammaticality and learnability.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 12722â€“12736\nJuly 9-14, 2023 Â©2023 Association for Computational Linguistics\nReanalyzing L2 Preposition Learning with Bayesian Mixed Effects\nand a Pretrained Language Model\nJakob Prange\nHong Kong Polytechnic University\njakob.prange@polyu.edu.hk\nMan Ho Ivy Wong\nHong Kong Shue Yan University\nmhwong@hksyu.edu\nAbstract\nWe use both Bayesian and neural models to\ndissect a data set of Chinese learnersâ€™ pre- and\npost-interventional responses to two tests mea-\nsuring their understanding of English prepo-\nsitions. The results mostly replicate previous\nfindings from frequentist analyses and reveal\nnew and crucial interactions between student\nability, task type, and stimulus sentence. Given\nthe sparsity of the data as well as high diversity\namong learners, the Bayesian method proves\nmost useful; but we also see potential in using\nlanguage model probabilities as predictors of\ngrammaticality and learnability.1\n1 Introduction\nLearning a second or third language is hardâ€”not\nonly for NLP models but also for humans! Which\nlinguistic properties and external factors make it\nso difficult? And how can we improve instruction\nand testing to help learners accomplish their goals?\nHere we also ask a third question: How can we best\napply different computational models to such be-\nhavioral experimental data in order to get intuitive\nand detailed answers to the first two questions in a\npractical and efficient way? For example, we are in-\nterested in whether language model (LM) probabil-\nities might give a rough estimate of grammaticality\nand learning difficulty (table 1, right columns).\nThis work is in part a replication study of Wong\n(2022), who, in addressing these questions about\nnative Chinese speakersâ€™ learning of English prepo-\nsitions in context (see examples in table 1), mainly\nfocused on instructional intervention and found\ngenerally positive effects as well as differences\nbetween instruction types, in particular favoring\nconceptual over rule-based teaching. We pick up\nwhere Wong (2022) left off and search for more\nfine-grained patterns among studentsâ€™ individual\ndifferences, linguistic items, stimulus sentences,\n1Our experimental code is available at https://github.\ncom/jakpra/L2-Prepositions.\nand their grammaticality. Our main hypothesis\nis that the full story of the complex interactions\namong these factors can only be revealed by mod-\neling them holistically. Such a fine-grained holistic\nanalysis is well-aligned with Item Response Theory\n(IRT; Fischer, 1973; Lord, 1980). IRT allows us to\nformulate models in terms of predicting whether\nstudents provide the intended response to each test\nitem. We consider sparse Bayesian and dense neu-\nral versions of this framework. We can then inspect\nhow strongly each model input (the linguistic, ex-\nperimental, and student-specific factors mentioned\nabove, which are realized as random and fixed ef-\nfects for the Bayesian model and feature vectors for\nthe neural model) affects the outcome. As a repre-\nsentative of yet another modeling strategy, and also\nas an additional input to the IRT models, we ob-\ntain probability estimates for the target prepositions\nin each stimulus sentence from a pretrained trans-\nformer LM. These probabilities serve as a proxy for\ncontextual formulaicity, as learned distributionally\nfrom a large corpus.\nWhile the theoretical advantages of Bayesian\nover frequentist statistics, as well as the generally\nstrong performance of neuro-distributional mod-\nels, are often cited as justification for choosing\none particular modeling method, both replication\nstudies and side-by-side comparisons of such dras-\ntically different modeling strategies for linguistic\nanalysis remain rare (with notable exceptions, e.g.,\nMichaelov et al., 2023; Tack, 2021).\nWe contribute to this important development by\nâ€¢ designing (Â§4.1), fitting, and evaluating (Â§5.1)\na Bayesian mixed effects model on Wongâ€™s\n(2022) data (Â§3), considering more potential\nlinguistic and human factors in preposition\nlearning than previously and finding signifi-\ncant effects for several of them;\nâ€¢ training (Â§4.2) and evaluating an analogous\nmultilayer perceptron (MLP) model and com-\nparing it with the Bayesian model in terms of\n12722\nStudent\njudgment LM\n# Usage Stimuli Grammatical? pre post ptgt pctx\n1a H IR-Spat The bell hung over the babyâ€™s cradle and made him smile. âœ“ 80.65 92.59 4.15 54.66\nAs expected\n1b through âœ— 50.00 46.67 0.03 54.10\n2a H IR-Abst The tutors watched over the students during the oral presentation. âœ“ 80.00 96.77 96.70 67.19\n2b on âœ— 35.00 46.15 0.07 50.64\n3a C VR-Abst Tremendous fear fell over the town after the murder. âœ“ 71.43 91.67 18.58 65.61\n3b through âœ— 41.67 27.27 0.39 63.91\n4a C RS-Spat The painter reached over the paint can for a brush. âœ“ 41.18 63.33 0.05 49.50\nsign(âˆ†ptgt ) ???4b through âœ— 36.11 33.33 0.78 45.42\n5a C RS-Abst The lawyer jumped over a few pages of the contract. âœ“ 72.73 94.12 6.97 52.39\n5b to âœ— 42.11 34.78 20.27 51.66\n6a C VR-Abst Happiness diffused over the guests when they see the newly-weds. âœ“ 44.44 88.46 2.47 65.27\n6b on âœ— 80.00 35.48 3.19 62.82\n7a C VR-Spat The canvas stretched over a large hole in the road. âœ“ 44.12 70.37 17.99 51.66\nsign(âˆ†pctx) ???7b through âœ— 55.56 60.00 15.52 52.79\n8a C VR-Abst The tension swept over the school when the alarm rang. âœ“ 66.67 100.00 3.93 46.61\n8b onto âœ— 37.84 12.50 <0.01 46.63\n9a C RS-Abst The politicians skipped over sensitive topics during the debate. âœ“ 83.33 94.59 2.88 40.80\n9b to âœ— 60.98 35.00 0.17 42.06\nTable 1:Examples of stimulus sentences for grammatical ( âœ“) and ungrammatical ( âœ—) preposition use. In the\nStudent judgmentcolumns we show the percentage of students who judged the example as grammatical at the\npretest (including control group) and posttest (treatment groups only) in Wongâ€™s study. The LM columns show our\nprobed RoBERTa probabilities ptgt and pctx [in %], which are defined in Â§4.3 and discussed in Â§5.3.\nboth feature ablation and overall prediction ac-\ncuracy of the outcome, i.e., whether a student\nwill answer a test prompt correctly (Â§5.2);\nâ€¢ and probing a pretrained LM (Â§4.3 and Â§5) for\ncontextual probabilities of target prepositions\nin order to determine their correlationâ€”and\nthus, practical usefulnessâ€”with human lan-\nguage learning.\nThus, we aim to both better explain L2 prepo-\nsition learning and compare Bayesian, frequentist,\nand neural approaches to doing so.\n2 Background\n2.1 English Preposition Semantics\nPrepositions are among the most frequently used\nword classes in the English languageâ€”they make\nup between 6 and 10 % of all word tokens depend-\ning on text type and other factors (cf. Schneider\net al., 2018). This is because English does not\nhave a full-fledged morphological case system and\ninstead often expresses semantic roles via word\norder and lexical markers like prepositions. At\nthe same time, the inventory of preposition forms\nis relatively smallâ€”a closed set of largely gram-\nmaticalized function words covering a wide range\nof predictive, configurational, and other relational\nmeanings. The resulting many-to-many mapping\nbetween word forms and meanings is complex and\nwarrants nuanced linguistic annotation, analysis,\nand computational modeling in context (Oâ€™Hara\nand Wiebe, 2003; Hovy et al., 2010; Srikumar\nand Roth, 2013; Schneider et al., 2018; Kim et al.,\n2019b). Further, considerable cross-linguistic vari-\nation in the precise syntax-semantics interactions\nof prepositions and case has been shown to affect\nnot only machine translation (Hashemi and Hwa,\n2014; Weller et al., 2014; PopoviÂ´c, 2017), but also\nconstrual in human translation (Hwang et al., 2020;\nPeng et al., 2020; Prange and Schneider, 2021)\nandâ€”cruciallyâ€”learner writing (Littlemore and\nLow, 2006; Mueller, 2011; Gvarishvili, 2013; Kran-\nzlein et al., 2020).\n2.2 Cognitive and Concept-based Instruction\nCognitive linguistics (CogLx) maintains that many\naspects of natural language semantics are grounded\nin extra-linguistic cognition, even (or especially)\nwhen they do not directly arise from syntactic com-\nposition, or at the lexical level. For example, Brug-\nman (1988), Lakoff (1987), and Tyler and Evans\n(2003) argue that spatial prepositions evoke a net-\n12723\nwork of interrelated senses, ranging from more\nprototypical to extended and abstract ones. Incor-\nporating such conceptual connectedness into lan-\nguage instruction has shown some benefits (Tyler,\n2012; Boers and Demecheleer, 1998; Lam, 2009).\n2.3 Computational Modeling in SLA\nUntil recently, most studies in applied linguistics\nand second-language acquisition (SLA)â€”insofar\nas they are quantitativeâ€”have relied on null-\nhypothesis testing with frequentist statistical mea-\nsurements like analysis of variance (ANOV A)\n(Norouzian et al., 2018). This has the advantage\nthat it is generally unambiguous and interpretable\nwhat is being tested (because concrete and spe-\ncific hypotheses need to be formulated ahead of\ntime) and that conclusions are based directly on\ndata without any potentially confounding modeling\nmechanisms. At the same time, frequentist analy-\nses are relatively rigid, and thus run into efficiency,\nsparsity, and reliability issues as interactions of\ninterest grow more complex. Li and Lan (2022)\npropound a more widespread use of computational\nmodeling and AI in language learning and educa-\ntion research. A promising alternative exists in the\nform of Bayesian models (e.g., Murakami and Ellis,\n2022; Privitera et al., 2022; Guo and Ellis, 2021;\nNorouzian et al., 2018, 2019), which circumvent\nsparsity by sampling from latent distributions and\noffer intuitive measures of uncertainty â€œfor freeâ€ in\nform of the estimated distributionsâ€™ scale parame-\nters. They can also be made very efficient to train\nby utilizing stochastic variational inference (SVI).\nBayesian modeling for educational applications\ngoes hand-in-hand with Item Response Theory\n(IRT; Fischer, 1973; Lord, 1980), which posits that\nlearning outcomes depend on both student aptitude\nand test item difficulty. This addresses another\nlimitation of frequentist analysisâ€”the focus on ag-\ngregate test scoresâ€”by modeling each studentâ€™s\nresponse to each item individually. We loosely\nfollow this general paradigm with our model im-\nplementations, without committing to any specific\ntheoretical assumptions.\nWithin NLP, Bayesian and IRT-based ap-\nproaches have been used to evaluate both human\nannotators (Rehbein and Ruppenhofer, 2017; Pas-\nsonneau and Carpenter, 2014) and models (Kwako\net al., 2022; Sedoc and Ungar, 2020), to conduct\ntext analysis (Kornilova et al., 2022; Bamman et al.,\n2014; Wang et al., 2012), and natural language in-\nference (Gantt et al., 2020).\nMurakami and Ellis (2022) show that grammar\nlearning can be affected by contextual predictabil-\nity (or formulaicity). While they used a simple\nn-gram model, we account for this phenomenon\nmore broadly with a pretrained transformer LM.\n3 Original Study and Data\nWong (2022) measured studentsâ€™ pre- and post-\ninterventional understanding of the English prepo-\nsitions in, at, and over, particularly contrasting\nCogLx/schematics-based instruction with different\nflavors of rule-based methods. To this end, interme-\ndiate learners of English (all university students)\nwith first languages Mandarin or Cantonese took\ninitial English language tests (â€˜pretestâ€™) targeting\ndifferent usages of prepositions. They were then\ntaught with one of four methods (incl. one con-\ntrol group, who received instruction about definite\nand indefinite articles instead of prepositions), and\nsubsequently tested two more times. There were\ntwo different tests: a grammaticality judgment test\n(GJT) to measure effects on language processing\nand a picture elicitation test (PET) to measure ef-\nfects on production.\nWhile all preposition-focused training was found\nto enhance learnersâ€™ understanding of prepositions\ncompared to both the pretest and the control group,\nschematics-based mediation led to stronger learn-\ning results than any of the other methods, especially\nat the PET (fig. 1) and on spatial usages of preposi-\ntions (the interaction between instruction method\nand spatial usage is not shown in fig. 1 for brevity).\nThese latter findings in particular support our hy-\npothesis that in addition to external factors like\ntask type and instruction method, learning difficulty\nmay also be affected by inherent linguistic prop-\nerties of the prepositions and their usages (just as,\ne.g., Guo and Ellis (2021) show for distributional\nproperties of grammatical suffixes). In this work\nwe take a second look at Wongâ€™s data to directly\naddress this possibility for preposition learning.\n3.1 Data Summary\nWe conduct all of our computational analyses with\nWongâ€™s data (stimuli and behavioral results) but\nexpand on the original study by explicitly modeling\nas potential factors several additional dimensions,\nrelating to individual differences and interactions\namong stimuli, task types, and students (table 2,\nÂ§3.2 and Â§3.3). 71 students (after outlier filtering)\nparticipated in the study. There are a total of 48\n12724\ntest items (12 senses Ã— 4 contexts) and 22 fillers\nfor the GJT as well as 36 test items (12 senses\nÃ— 3 contexts) and 15 fillers for the PET. Outlier\nstudents and filler items are removed before any\nanalysis/model training, resulting in 17,644 data\npoints overall (GJT: 10,156; PET: 7,488).\n3.2 Stimulus Sentences\nIn the GJT (but not in the PET), students receive a\nlinguistic stimulus to evaluate for grammaticality\n(see examples in table 1). Intended-grammatical\nstimuli involve target prepositions used in a sen-\ntence context that evokes their intended sense or\nfunction (fxn), either literally/spatially or figura-\ntively/abstractly. For each intended-grammatical\nstimulus, there is an intended-ungrammatical stim-\nulus, consisting of the same sentence context but\nreplacing the target preposition with another that is\nmeant to fit the context less well.\n3.3 Categorical Features\nInstruction method. The main goal of Wongâ€™s\n(2022) study was to compare CogLx-based\nschematic mediation ( SM) with more tradi-\ntional rule-and-exemplar ( RM) and bare-bones\ncorrectness-based mediation (CM). SM, RM, and\nCM instruction focused on the same preposition\nforms and usages students were tested on.\nTime of test.Students were tested three times: Two\ndays before instructional intervention (PREtest, â—\nin fig. 1), two days after instruction (POSTtest, â—‹),\nand again 3 weeks later (DeLaYed posttest, â–·).\nPreposition form, function (fxn), and usage.The\ntest cues are made up of 6 pairs of preposition\nusages across three forms: â€˜ inâ€™ with the CON-\nTAINMENT (CTN) function; â€˜atâ€™ with the TARGET\n(TGT) and POINT (PNT) functions; and â€˜overâ€™ with\nthe HIGHER (HIR), ACROSS (CRS), and COVER\n(CVR) functions. Each usage pair consists of a\nspatial (e.g., â€˜in the boxâ€™) and a non-spatial cue\n(e.g., â€˜in loveâ€™) sharing the same schematization\n(in this case, CONTAINMENT ). The cues were se-\nlected based on the Principled Polysemy Frame-\nwork (Tyler and Evans, 2003), thereby ruling out\noverly fine-grained senses and allowing systematic\npresentation for instruction and testing.\nTest type.In the GJT, learners had to decide, for\neach stimulus sentence containing a preposition,\nwhether the whole sentence is â€œcorrectâ€ or â€œincor-\nrectâ€.2 We consider as a potential factor on the out-\ncome whether a stimulus is intended-grammatical\n(GJT-Y) or not (GJT-N). In the PET, learners were\nW22 Ours\nRandom Effects\nFeature Values\nInstruction SM, RM, CM, CTRL âœ“ âœ“\nTime PRE, POST, DLY âœ“ âœ“\nTest GJT, PET âœ“ âœ“\nUsage Spatial, Abstract âœ“ âœ“\nAnswer GJT-Y, GJT-N, PET âœ— âœ“\nForm-Fxn in-CTN, at-TGT âœ— âœ“\nat-PNT, over-HIR,\nover-CRS, over-CVR\nStudent s1, ..., s71 âœ— âœ“\nFixed Effects\nptgt â€”LM probability of target preposition âœ— âœ“\npctxâ€”Avg. LM prob. of non-tgt tokens in sent. âœ— âœ“\nTable 2:Features under consideration in Wong (2022)\n(W22) and our work.\nshown an illustration of a concrete scenario instan-\ntiating one of the cues and were asked to produce a\ndescriptive sentence containing a preposition. Re-\nsponses were counted as correct if they chose the\ntarget preposition.\nStudents. By adding local student identities to the\nmodel input (anonymized as, e.g.,s1, s23), we allow\nfine-grained degrees of freedom w.r.t. individual\ndifferences, as is suggested by IRT.\n4 Models\nOur main point of reference (or quasi-baseline) is\nWongâ€™s frequentist data analysis, which is summa-\nrized in Â§3. In this work, we newly consider the\nfollowing different modeling strategies: We train\na Bayesian logistic model(BLM, Â§4.1) as well\nas a small multilayer perceptron(MLP, Â§4.2) on\nthe same data. With the BLM we can define and\ninterpret the precise structure of how individual\nfeatures and their interactions affect the outcome.\nIn contrast, the MLP utilizes nonlinear activation\nfunctions and multiple iterations/layers of computa-\ntion, allowing it to pick up on complex interactions\namong input features without prior specification\nand thus to potentially achieve higher predictive\naccuracy, at the cost of interpretability. Both the\nBLM and MLP are implemented in Python and\nPyTorch, and are light-weight enough to be trained\nand run on a laptop CPU within several minutes for\ntraining and several seconds for inference. We also\nquery a pretrained neural language model(LM,\nnamely RoBERTa; Liu et al., 2019b) to obtain con-\ntextual probabilities for the stimulus sentences used\n2The testing prompt did not explicitly highlight or other-\nwise draw attention to the preposition in question.\n12725\nin the grammaticality judgment test and add those\nprobabilities to the BLM and MLPâ€™s inputs (Â§4.3).\n4.1 Bayesian Logistic Model\nWe model the posterior likelihood of a correct re-\nsponse (i.e., a given student providing the intended\nanswer to a given stimulus) as a logistic regression\nconditional on the aforementioned categorical vari-\nables. Concretely, responses are sampled from a\nBernoulli distribution with log-odds proportional\nto the weighted sum of the random and fixed ef-\nfects. As potential factors we consider the features\nlisted in Â§3.3 and table 2, as well as their mutual in-\nteractions. For the students feature, to keep model\nsize manageable, we only consider pairwise in-\nteractions with usage (spatial/abstract), form-fxn,\nand answer. Otherwise all n-wise interactions are\nincluded. The effectsâ€™ weight coefficients are sam-\npled from Normal distributions whose means and\nstandard deviations are fitted to the training data\nvia SVI with the AdamW optimizer, AutoNormal\nguide, and ELBO loss. We use standard-normal\npriors for means and flat half-normal priors for\nstandard deviations, meaning that, by default, pa-\nrameter estimates are pulled towards null-effects,\nand will only get more extreme if there is strong ev-\nidence for it. The model is implemented using the\nPyro-PPL/BRMP libraries (Bingham et al., 2018).\n4.2 Multilayer Perceptron\nWe train and test a multilayer perceptron (MLP)\nwith depth 3. We mirror the BLM setup by treating\nstudent response correctness as the output and op-\ntimization objective and the different feature sets\nas concatenated embedding vectors. Between hid-\nden layers we apply the GELU activation func-\ntion, and during training additionally dropout with\np =0.2 before activation. We also apply dropout\nwith p =0.1 to the input layer. We minimize binary\ncross-entropy loss using the AdamW optimizer. We\ntrain for up to 25 epochs but stop early if dev set ac-\ncuracy does not increase for 3 consecutive epochs.\n4.3 RoBERTa\nWe feed the GJT stimulus sentences to RoBERTa-\nbase (Liu et al., 2019b, accessed via Huggingface-\ntransformers). RoBERTa a pretrained neural LM\nbased on the transformer architecture (Vaswani\net al., 2017) and trained on English literary and\nWikipedia texts to optimize the masked-token and\nnext-sentence prediction objectives. For each sen-\ntence, we wish to obtain RoBERTaâ€™s posterior prob-\nability estimates for each observed word token\nwi âˆˆw0âˆ¶nâˆ’1, given w0âˆ¶nâˆ’1/{wi}, i.e., all other words\nin that sentence. Thus we run RoBERTa n times,\neach time i masking out wi in the input. From\nthese n sets of probabilities, we extract two mea-\nsurements of formulaicity we expect to be relevant\nto our modeling objective of student response cor-\nrectness:3 (a) ptgt , the contextual probability of the\ntarget or alternate preposition given all other words\nin the sentence and (b) pctx, the average contextual\nprobability of all words except the preposition. 4\nExamples are given in table 1. We standardize\nthese two variables to N(0, 1) and add them to the\nBLM (as fixed effects, both individually and with\ninteractions) and MLP (as scalar input features).\n5 Evaluation\nWe first analyze the BLMâ€™s learned latent coef-\nficients (Â§5.1). Then we compare different ver-\nsions of the BLM and MLP w.r.t. their ability to\npredict unseen student responses using their esti-\nmated weighting of linguistic and external features\nas well as LM probabilities (Â§5.2). Finally, we man-\nually inspect a small set of stimulus sentences with\nanomalous LM probabilities w.r.t. their intended\ngrammaticality and observed judgments (Â§5.3).\n5.1 Determining Relevant Input Features\nSetup. We fit BLMs on the entire data set (without\nreserving dev or eval splits). We run SVI for 1000\niterations with a sample size of 100 and a fixed\nrandom seed. We compute effect sizes (Cohenâ€™sd),\nand p-values based on 95%-confidence intervals\nof differences between estimated parameter values\n(Altman and Bland, 2011).\nReplication. As in Wong (2022), we use the fea-\ntures instruction, time, form-fxn, usage, and addi-\ntionally let the model learn individual coefficients\nfor each student. Separate models were trained\nfor GJT and PET. As shown in fig. 1, we mostly\nreplicate similar trends (differences between differ-\nences) as found previously, namely:\nâ€¢ Time: DLY â‰ˆPOST >PRE;\nâ€¢ Instruction: treatment >ctrl; SM >CM â‰ˆRM;\n3We also preliminarily experimented with inputting the\nentire LM hidden state of the last layer to the models but\ndid not find it to be helpful. Kauf et al. (2022) found that\nalignment with human judgments varies from layer to layer,\nwhich presents an interesting avenue for future work.\n4Note that the preposition token still has the potential to af-\nfect the other wordsâ€™ probabilities by occurring in their context\ncondition.\n12726\nâ€¢ and we generally see larger learning effects in\nthe PET than in the GJT.\nHowever, many effect sizes are amplifiedâ€”and\nthus p-values more significant-lookingâ€”in our\nmodel. A potential explanation for this could be\nthat the BLM models each individual item response\nwhereas ANOV A only considers overall %-correct.\nWe are thus comparing effects on all studentsâ€™ ac-\ncuracy at multiple test items in aggregate with ef-\nfects on each studentâ€™s accuracy at each test item\nseparately. It seems intuitive that the latter â€˜micro-\neffectsâ€™ are much greater on average than the for-\nmer â€˜macro-effectsâ€™, which are themselves effects\non the average performance metric. Another reason\ncould be that because the Bayesian effect sizes stem\nfrom simulated data points, they are only indirectly\nrelated to the real underlying data via SVI. The es-\ntimated distribution these samples are drawn from\nonly approximates the real data and thus the effect\nsize estimations may be over-confident. See Â§6.1\nfor a discussion of advantages and disadvantages.\nAlthough our model estimates spatial usages as\ngenerally more difficult than abstract ones, we do\nnot replicate Wongâ€™s finding of an interaction be-\ntween abstractness and instruction or time. Still,\nour Bayesian quasi-IRT approach allows us to find\nadditional interesting patterns that could not have\nbeen captured by a frequentist analysis 5 as they\ninvolve student-level and item-level interactions:\nAnswer type and individual differences. We\ntrained a single combined model on both GJT and\nPET data. As can be expected, in addition to the\noverall trends (fig. 1), we also find a strong effect\nfor expected answer type (fig. 2): the receptive task\nof accepting grammatical items (GJT-Y) is much\neasier than the productive task of choosing the right\npreposition when describing a picture (PET). Inter-\nestingly, ruling out ungrammatical items (GJT-N)\nis equally as difficult as the PET. In addition, out-\ncomes are affected by individual differences be-\ntween students, and student aptitude heavily de-\npends on answer type (fig. 3) as well as on prepo-\nsition form/function (fig. 5 in appendix A). There\nis some (negative) correlation between individual\naptitudes at GJT-N and GJT-Y and some (positive)\ncorrelation between GJT -N and PET. Still, both\ncorrelations are weak (R2 =0.23 and 0.20).\nIn sum, not only do receptive vs. productive task\n5Or only very tediously so.\n6Where W22 does not report Cohenâ€™s d, we show their\nreported partial-eta-squared Î·2p instead.\n0.6\n 0.4\n 0.2\n 0.0 0.2 0.4 0.6\nABSTRACT\nSPATIAL\nSM\nRM\nCM\nCTRL\nDLY\nPOST\nPRE\nGJT\n0.6\n 0.4\n 0.2\n 0.0 0.2 0.4 0.6\nABSTRACT\nSPATIAL\nSM\nRM\nCM\nCTRL\nDLY\nPOST\nPRE\nPET\nW22:   d=1.335, p<0.0001***\nOurs: d=12.838, p<0.0001***\nW22: d=0.172, p=0.357\nOurs: d=2.248, p<0.0001***\nïµDLY\nW22 (CTRL v all):  d=0.92, p=0.003***\nOurs (CTRL v CM): d=3.11, p<0.001***\nïµDLY\nW22: non-significant\nOurs: d=1.751, p<0.0001***\nW22: ðœ‚ð‘ƒ\n2=0.167, p=0.002***\nOurs:   d=0.491, p=0.0006***\nW22 (all):        ðœ‚ð‘ƒ\n2=0.48, p<0.0001***\nOurs (PRE v PST): d=15.4, p<0.0001***\nOurs (POST v DLY): d=0.228, p=0.109\nïµDLY\nW22 (CTRL v all):   d=1.49, p<0.001***\nOurs (CTRL v CM): d=6.92, p<0.001***\nâš«POST\nW22:                  p=0.040*\nOurs: d=3.851, p<0.0001***\nW22: ðœ‚ð‘ƒ\n2=0.697, p<0.001***\nOurs:   d=0.770, p<0.0001***\nFigure 1:Summary of our Bayesian effect estimations\n(marginal means and standard deviations over model\nparameters) for selected features. Coefficient values\n(x-axis) indicate the extent to which the feature value\n(y-axis) contributes to a correct (positive) or incorrect\n(negative) student response. On the right we compare\neffect sizes (Cohenâ€™s d) and statistical significance to\nWongâ€™s (2022) frequentist analysis.6\n0.04\n 0.02\n 0.00 0.02 0.04 0.06\nGJT-Y\nPET\nGJT-N\nd=0.108, p=0.444\nd=1.264, p<0.0001***\nFigure 2:Estimated effects for different answer types.\n1.0\n 0.5\n 0.0 0.5 1.0\nGJT-N\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\nGJT-Y\n1.0\n 0.0 0.5\nPET:\nFigure 3: Effect estimation means of individual stu-\ndents (points) in interaction with answer type (x=GJT-N,\ny=GJT-Y , color=PET). There is a weak negative correla-\ntion between being good at GJT-N and GJT-Y answers\n(blue line, R2=0.23) and a weak positive correlation\nbetween GJT-N and PET skills (R2=0.20).\n12727\ntypes vary in their overall difficulty (fig. 2), but\nthe wide spread in individual student differences\n(fig. 3) suggests that the skill sets required (letâ€™s\ncall them â€œsensitivityâ€ and â€œspecificityâ€) are some-\nwhat complementary to each other and tend to be\ndistributed unevenly among students. Each student\nhas a unique combination of them. We discuss this\nfurther in Â§6.2.\nLM probabilities. The model was trained on\nGJT data only. Recall from Â§3.3 that GJT test-\ning prompts did not explicate the target preposi-\ntion or even mention the word â€˜prepositionâ€™. All\nelse equal, it is thus conceivable that, despite the\npreposition-focused training, students evaluate the\nsentencesâ€™ grammaticality for reasons unrelated\nto the target preposition. However, we can with\nhigh probability rule out this option as our model\nestimates strong effects for numerous features di-\nrectly related to the preposition, namely: ptgt by\nitself (d=4.57; p<0.0001***); interaction ptgt :pctx\n(d=14.92; p<0.0001***);7 and spatial vs. abstract\nusage of each preposition form and function (fig. 1,\nfig. 6 in appendix A). Furthermore, due to the\nheavy interaction between LM probabilities and\ncategorical cue properties, 8 the singular random\neffect of spatial vs. abstract usage decreases when\nthe model considers the LM-based fixed effects\n(d=0.372; p=0.0093**) compared to when it does\nnot (d=0.491; p=0.0006***, fig. 1).\n5.2 Predicting Student Responses\nSetup. We train the BLM and MLP using a\ntraining:evaluation:development data split ratio of\n84:15:1, placing less weight on the dev set since\nit is only used to determine early-stopping during\nMLP training. Experiments are run 10 times with\nrandom data splits and model initializations.\nResults. As shown in table 3, both models eas-\nily outperform simple baselines, and the two mod-\nelsâ€™ overall accuracies are roughly on par (within\neach otherâ€™s stdevs) with a slight advantage for\nthe BLM. For predicting GJT outcomes only, the\naforementioned interaction between students and\nanswer types is most crucial, followed by infor-\nmation about the target preposition (BLM) and\ninstruction (MLP), respectively. The LM-based\nfeatures ptgt and pctx are useful for both models,\n7While pctx by itself is only very weakly correlated with\neither grammaticality or student response, it does become a\nuseful predictor in interaction with ptgt (cf. fig. 4 left).\n8Linguistic categories may to some extent be encoded in\nthe LMâ€™s distributed representations (Jawahar et al., 2019).\nGJT + PET GJT only\nBLM MLP BLM MLP\nUniform BL 49.7 Â±1.1 49.7 Â±1.2\nBLM prior BL 49.7 Â±2.1 48.2 Â±1.4\nMajority BL 64.2 Â±0.9 68.1 Â±0.7\nFull model 72.6 Â±1.1 71.5 Â±0.6 72.5 Â±0.8 71.3 Â±0.9\nâˆ’students âˆ’2.2 Â±0.6 âˆ’0.9 Â±0.7 âˆ’2.6 Â±0.9 âˆ’2.0 Â±0.8\nâˆ’answer âˆ’5.6 Â±0.8 âˆ’4.6 Â±0.6 âˆ’2.4 Â±0.8 âˆ’2.0 Â±0.8\nâˆ’fxn & usage âˆ’5.4 Â±1.0 âˆ’4.6 Â±1.0 âˆ’1.5 Â±0.4 âˆ’0.8 Â±1.3\nâˆ’instr & time âˆ’2.1 Â±0.9 âˆ’1.8 Â±0.9 âˆ’0.4 Â±0.7 âˆ’1.4 Â±0.9\nâˆ’ptgt & pctx n/a n/a âˆ’0.9 Â±0.9 âˆ’0.4 Â±0.9\nTable 3: Baselines (BL), BLM and MLP prediction\nperformance, and feature ablation (student response cor-\nrectness prediction accuracy in %). Means and standard\ndeviations over 10 random seeds, which affect not only\nmodel initialization but also data splitting and shuffling.\nBest full model results on each data split are underlined;\nhighest-impact features in each column are bolded.\nbut less so than the categorical ones. This is some-\nwhat unexpected based on their strong effect sizes\n(Â§5.1) and the overwhelmingly high performance\nof LMs on other tasks. A potential reason is the\ncontrast between the LM reflecting a gross aver-\nage of language useâ€”which indeed correlates with\ngrammaticality (R2 =0.48, fig. 4)â€”and the unrelia-\nbility of student judgments, especially at the pretest\nand in the control group (fig. 1 top). The lack of\nstimulus sentences (and thus LM probabilities) in\nthe PET further increases the importance of the\nanswer, form-function, and usage features in the\nGJT+PET condition. We also see a larger ablation\neffect of the instruction and time features, which\nis consistent with the larger interaction effect esti-\nmates for the PET (fig. 1 bottom).\n5.3 Qualitative Analysis of Stimuli\nWe take a closer look at individual stimuli in fig. 4.\nFrom the y-axis distribution in the center and right\npanels we can clearly see the learning development\namong students undergoing preposition-focused\ntraining. At the pretest (center), aggregate studentsâ€™\ngrammaticality judgment is less decisive (mostly\nvertically centered around 50% Â±â‰ˆ20pp. At the\nposttest (right), the spread is much more decisive,\nranging from almost 0% to 100%. At both points\nin time, there is a slight bias towards positive judg-\nment, i.e., students are generally more willing to\naccept ungrammatical stimuli as grammatical than\nto reject grammatical ones. In contrast, LM proba-\nbilities (x-axis) tend to err on the conservative side,\ni.e., the LM has higher recall on recognizing un-\ngrammatical items, whereas students have higher\n12728\n0 20 40 60 80 100\np_tgt\n0\n20\n40\n60\n80\n100p_ctx\n3b 3a\n1b1a\n6b6a\n2b\n2a\n8b8a4b\n4a\n9b9a\n5b5a\nanswer=Y\nanswer=N\n0 20 40 60 80 100\np_tgt\n0\n20\n40\n60\n80\n100% judged as Y @ pre\n3b\n3a\n1b\n1a6b\n6a\n2b\n2a\n8b\n8a\n4b\n4a\n9b\n9a\n5b\n5a\nanswer=Y\nanswer=N\n0 20 40 60 80 100\np_tgt\n0\n20\n40\n60\n80\n100% judged as Y @ post\n3b\n3a\n1b\n1a\n6b\n6a\n2b\n2a\n8b\n8a\n4b\n4a\n9b\n9a\n5b\n5a\nanswer=Y\nanswer=N\nFigure 4:Correlations of LM probabilities, student grammaticality judgment %, and intended answer (color/shape)\nfor individual stimuli (points). Left: ptgt (x) and pctx (y); Center: ptgt (x) and pretest judgment (y); Right: ptgt\n(x) and posttest judgment of non-control groups only (y). R2(answer, judge@post)=0.72; R2(ptgt , answer)=0.48;\nR2(ptgt , judge@post, blue line right)=0.41; R2(ptgt , pctx, blue line left)=0.30; R2(answer, judge@pre)=0.28; R2(ptgt ,\njudge@pre, blue line center)=0.22; R2(pctx, answer)=0.15; R2(pctx, judge@post)=0.13; R2(pctx, judge@pre)=0.04.\nData points/sentences listed in table 1 and discussed in Â§5.3 are labeled.\nrecall on recognizing grammatical items, each at\nthe cost of precision.9\nWe expect that intended-grammatical (âœ“) usages\ngenerally receive higher LM probabilities (âˆ†p) than\nintended-ungrammatical (âœ—) usages. This is the\ncase most of the time (for 41/48 stimulus pairs\ntotal), except for 7 cases, 6 of which involve the\npreposition â€˜overâ€™ as the target. We present these\nsentences in table 1, along with 3 examples where\nboth âˆ†pâ€™s are as expected.\nWhat makes ex. 4 â€“ 9 special? A potential ex-\nplanation is that the verb+preposition+object con-\nstructions in ex. 1 â€“ 3 seem to be more clearly\ndistinguishable as either grammatical or ungram-\nmatical than the rest. In contrast, the âœ— sentences in\nex. 4 â€“ 6 are not truly ungrammatical. The scenar-\nios they describe are unlikely but possible, and the\nunlikeliness mostly arises through the full-sentence\ncontext rather than the prepositional construction\nalone. In fact, each alternative preposition in 4b,\n5b, and 6b might in isolation be a more expected\ncollocation with the verb than â€˜overâ€™, which would\nexplain the ptgt trend. Ex. 7 â€“ 9 (both âœ— and âœ“)\ndescribe much more rare (i.e., unlikely as far as the\ndistributional LM is concerned) scenes, which may\nlead to the overall lower pctx values.10\n9Note that LM probabilities are not based on a binary gram-\nmaticality decision but on a selection decision over the entire\nvocabulary, and also that gradient linguistic judgments in gen-\neral cannot be said to only revolve around grammaticality (cf.\nLau et al., 2017). We could address this by looking at the\nratio between the probabilities for each pair, but that would in\nturn establish a dependency among stimuli within each pair\nwhich is not present in the human experimentâ€”each stimu-\nlus is presented in isolation, in randomized order. Thus, for\ntransparency, we stick with the plain probability and elaborate\nqualitatively on the expected behavior below.\n10A second tendency may lie in the concreteness and per-\nceived simplicity (both in terms of semantics and register) of\n6 Discussion\n6.1 Which model type is most appropriate?\nFor the purpose of our study, the Bayesian logis-\ntic model of student responses has clear advan-\ntages over both the previous frequentist analysis of\nscore aggregates (complexity of interactions, intu-\nitiveness; Â§5.1) and the neural response classifier\n(higher interpretability with roughly equal predic-\ntion accuracy; Â§5.2). However, while this obser-\nvation is in line with both our expectations and re-\ncent literature in SLA (e.g., Norouzian et al., 2018,\n2019), we still recommend testing model practica-\nbility on a case-by-case basis. For example, if much\nmore training data is available, a neural classifier\nis likely to outperform a sparse model at predic-\ntion accuracy. Whenever the BLM and ANOV A\nagree on a featureâ€™s significance (and they usuallyâ€”\nbut not alwaysâ€”do), the BLMâ€™s estimates are rel-\natively amplified (Â§5.1). This can be useful for\nidentifying potentially relevant effects and interac-\ntions, but should also be taken with a grain of salt\nas it sometimes may construe results too optimisti-\ncally. Where do these divergences come from?\nWe hesitate to make any strong statements about\nbroad philosophical differences between Bayesian\nand frequentist statistics in the abstract. Rather,\nwe suspect that it mostly comes down to practical\nconsiderations like framing model and data around\nindividual item responses vs. aggregate score, as\nwell as varying degrees of commitment to latent\nsampling and optimization. Item response predic-\ntion accuracy and ablation analyses give some in-\nthe preposition-governing verbs: â€˜ hang, watch, fallâ€™ are all\nfairly concrete, unambiguous, and colloquial, whereas â€˜reach,\ndiffuse, stretch, sweepâ€™ have more specialized meanings and\nare somewhat higher register.\n12729\nsight into how individual features affect modelsâ€™\nestimates of the outcome variable and is consistent\nwith statistical analyses (Â§5.2). This is particularly\nuseful for discriminative neural models such as our\nMLP classifier, and is, of course, common practice\nin NLP classification studies. However, it is also\nmuch more costly, less precise, and less reliable\nthan Bayesian and frequentist approaches.\n6.2 Implications for SLA\nOur analysis of answer types and student aptitudes\n(Â§5.1 and Â§5.2) confirms Wongâ€™s (2022) and othersâ€™\nfindings about differences between productive and\nreceptive knowledge. We support Wongâ€™s argument\nthat the type of assessment should align with both\ninstruction type and and intended learning outcome.\nWe further observe that even within the generally\nreceptive task of grammaticality judgment, the sub-\ntask of ruling out ungrammatical items (GJT -N)\nrequires higher specificity than accepting grammat-\nical ones (GJT-Y) and is thus more closely aligned\nwith productive tasks (e.g., PET). Interestingly, stu-\ndents who are better than average at productive tests\ntend to be slightly weaker than average at receptive\nones and vice versa. A potential future use case\nof explicitly modeling studentsâ€™ individual differ-\nences w.r.t. different task types and linguistic items\nis that educational applications can be tailored to\ntheir weaknesses, which is expected to increase\nlearning effectiveness and efficiency. 11 Outside\nof directly deploying learning technology to end\nusers, our findings can inform educators and SLA\nresearchers. For example, unexpected patterns in\nLM probabilities (Â§5.3) may point to suboptimally\ndesigned stimulus pairs. Thus, LM probing could\nbe a useful tool in cue selection and stimulus design\nof similar studies in the future.\n6.3 Implications for NLP\nIn this work, we primarily analyze human learner\nbehavior using different machine learning models,\nwhile in NLP-at-large it is much more common to\nanalyze machine learning models w.r.t. a human\nground truth. At the same time, our observations\nthat different senses and usages even of the same\npreposition form heavily affect human learnability\nare somewhat analogous to previous results in auto-\nmatic preposition disambiguation (varying model\nperformance for extended vs. lexicalized senses;\n11In practice, such a process should ideally be decentralized\nby training separate models for each student on the client side,\nto uphold privacy and other ethical standards.\nSchneider et al., 2018; Liu et al., 2019a). Liu et al.\nalso found that LM pretraining improves disam-\nbiguation performance, while Kim et al. (2019a)\ndrew attention to differences among various NLP\ntasks as â€˜instruction methodsâ€™. This is not to say\nthat current LM training practices are necessarily\nplausible models of human language learning and\nteaching, but even these high-level similarities in\nbehavioral patterns invite further investigation.\n7 Conclusion\nMuch quantitative research in many areas of lin-\nguistics, including SLA, has been relying on the\nfrequentist method for a long timeâ€”and for good\nreasons: It enables strong conclusions about clear\nhypotheses, closely following the observed data.\nHere we compared several alternative ap-\nproaches to estimating a multitude of potential ef-\nfects more holistically, namely via IRT-inspired\nBayesian sparse models of explicit interactions\namong facts, neural classifiers of student responses\nand feature ablation, as well as contextual proba-\nbilities of the experimental stimuli obtained from a\npretrained language model (Â§4).\nOverall, we were able to replicate previous fre-\nquentist findings regarding the difficulty of acquir-\ning the preposition system in English as a sec-\nond language and the benefits of concept-based\ninstruction (Â§5.1). Our computational analysis em-\nphasized the increased flexibility and occasionally\nstronger effect size estimates of IRT and Bayesian\nmodels, as well as their natural interpretability com-\npared to neural models with equal predictive power.\nWe also found novel interactions among task\nand subtask type, student individual differences,\npreposition cue and LM contextualization (Â§5), and\ndiscussed them in the broader contexts of both\nNLP and SLA, hoping to build bridges between\nthe two research communities (Â§6). As a final take-\naway for both fields, the differences between the\nLMâ€™s and studentsâ€™ overall tendencies to accept\nor reject stimuli (Â§5.3 and fig. 4 right) could po-\ntentially be exploited in both directions: The ag-\ngregate distributional grammatical knowledge of\nan LM could be used to teach students the most\naccepted usages of prepositions and other function\nwords across a large population of speakers (i.e.,\nimprove their specificity), while LMs could learn\nto be more creative and to utilize humansâ€™ intuitive\ncross-lingual meaning mappings by learning from\nsecond-language learner data.\n12730\nLimitations\nOur study and findings are limited to the spe-\ncific L1â€“L2 pair of Chinese (Mandarin and\nCantonese)â€“English. Further, the experimental set-\nting we draw our data from is highly controlled,\nwith carefully-chosen lexical items and carefully-\ndesigned (length- and distractor-matched) stimulus\nsentences. While this enables strong statistical con-\nclusions about the data itself, it poses a sparsity\nproblem for most state-of-the-art NLP models, as\ncan be seen even in the small and simple multi-\nlayer perceptron we test.\nWhile it would also be interesting to know\nwhether students respond differently to the same\ninstruction type or vice versa, the between-subjects\nexperimental design underlying our data does not\nallow such a measurement.\nWe inspect several model types representing a\nselection of extreme areas of a vast continuum of\ncomputational analysis methodologies. Naturally,\nthis means that we cannot go into a lot of depth\nregarding model engineering and detailed compari-\nson among similar implementations of each type.\nEthics Statement\nStudent identities are completely anonymized in\nour analyses and in the data we feed to our models.\nBy locally distinguishing individual students, we\ndo not wish to single out, over-interpret, or judge\nany individual studentâ€™s behavior or aptitude, but\nrather to fit the models to our data as best we can\nand also to control for spurious patterns that might\nhave been missed during initial outlier-filtering.\nAcknowledgments\nWe thank the anonymous reviewers for their in-\nsightful questions and feedback. This work\nhas been supported by Hong Kong PolyU grant\n1-YWBW, awarded to the first author, and grant\nEDB(LE)/P&R/EL/203 of the Hong Kong Stand-\ning Committee on Language Education and Re-\nsearch (SCOLAR), awarded to the second author.\nReferences\nDouglas G Altman and J Martin Bland. 2011. How to\nobtain the p value from a confidence interval. BMJ,\n343.\nDavid Bamman, Ted Underwood, and Noah A. Smith.\n2014. A Bayesian mixed effects model of literary\ncharacter. In Proc. of ACL, pages 370â€“379, Balti-\nmore, Maryland.\nEli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz\nObermeyer, Neeraj Pradhan, Theofanis Karaletsos,\nRohit Singh, Paul Szerlip, Paul Horsfall, and Noah D.\nGoodman. 2018. Pyro: Deep Universal Probabilis-\ntic Programming. Journal of Machine Learning Re-\nsearch.\nFrank Boers and Murielle Demecheleer. 1998. A cogni-\ntive semantic approach to teaching prepositions. ELT\nJournal, 52(3):197â€“204.\nClaudia Marlea Brugman. 1988. The story of over:\nPolysemy, semantics, and the structure of the lexicon.\nTaylor & Francis.\nGerhard H. Fischer. 1973. The linear logistic test model\nas an instrument in educational research. Acta Psy-\nchologica, 37(6):359â€“374.\nWilliam Gantt, Benjamin Kane, and Aaron Steven\nWhite. 2020. Natural language inference with mixed\neffects. In Proc. of *SEM, pages 81â€“87, Barcelona,\nSpain (Online).\nRundi Guo and Nick C. Ellis. 2021. Language usage\nand second language morphosyntax: Effects of avail-\nability, reliability, and formulaicity. Frontiers in Psy-\nchology, 12.\nZeinab Gvarishvili. 2013. Interference of L1 preposi-\ntional knowledge in acquiring of prepositional usage\nin English. Procedia-Social and Behavioral Sciences,\n70:1565â€“1573.\nHoma B Hashemi and Rebecca Hwa. 2014. A compari-\nson of MT errors and ESL errors. In Proc. of LREC,\npages 2696â€“2700.\nDirk Hovy, Stephen Tratz, and Eduard Hovy. 2010.\nWhatâ€™s in a preposition? Dimensions of sense disam-\nbiguation for an interesting word class. In Proc. of\nCOLING, pages 454â€“462, Beijing, China.\nJena D. Hwang, Hanwool Choe, Na-Rae Han, and\nNathan Schneider. 2020. K-SNACS: Annotat-\ning Korean adposition semantics. In Proc. of\nDMR@COLING, pages 53â€“66, Barcelona, Spain (on-\nline).\nGanesh Jawahar, BenoÃ®t Sagot, and DjamÃ© Seddah.\n2019. What does BERT learn about the structure\nof language? In Proc. of ACL, pages 3651â€“3657,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nCarina Kauf, Anna A. Ivanova, Giulia Rambelli, Em-\nmanuele Chersoni, Jingyuan S. She, Zawad Chowd-\nhury, Evelina Fedorenko, and Alessandro Lenci.\n2022. Event knowledge in large language models:\nthe gap between the impossible and the unlikely.\nPreprint arXiv:2212.01488.\n12731\nNajoung Kim, Roma Patel, Adam Poliak, Patrick Xia,\nAlex Wang, Tom McCoy, Ian Tenney, Alexis Ross,\nTal Linzen, Benjamin Van Durme, Samuel R. Bow-\nman, and Ellie Pavlick. 2019a. Probing what differ-\nent NLP tasks teach machines about function word\ncomprehension. In Proc. of *SEM, pages 235â€“249,\nMinneapolis, Minnesota, USA.\nNajoung Kim, Kyle Rawlins, Benjamin Van Durme,\nand Paul Smolensky. 2019b. Predicting the argu-\nmenthood of English prepositional phrases. In Proc.\nof AAAI, volume 33, pages 6578â€“6585.\nAnastassia Kornilova, Vladimir Eidelman, and Daniel\nDouglass. 2022. An Item Response Theory frame-\nwork for persuasion. In Findings of NAACL, pages\n77â€“86, Seattle, Washington, USA.\nMichael Kranzlein, Emma Manning, Siyao Peng, Shira\nWein, Aryaman Arora, and Nathan Schneider. 2020.\nPASTRIE: A corpus of prepositions annotated with\nsupersense tags in Reddit international English. In\nProc. of LAW@COLING, pages 105â€“116, Barcelona,\nSpain.\nAlexander Kwako, Yixin Wan, Jieyu Zhao, Kai-Wei\nChang, Li Cai, and Mark Hansen. 2022. Using Item\nResponse Theory to measure gender and racial bias of\na BERT-based automated English speech assessment\nsystem. In Proc. of BEA@NAACL-HLT, pages 1â€“7,\nSeattle, Washington, USA.\nGeorge Lakoff. 1987. Women, fire, and dangerous\nthings: What categories reveal about the mind.\nChicago: University of Chicago.\nYvonne Lam. 2009. Applying cognitive linguistics to\nteaching the Spanish prepositions por and para. Lan-\nguage Awareness, 18(1):2â€“18.\nJey Han Lau, Alexander Clark, and Shalom Lappin.\n2017. Grammaticality, acceptability, and probability:\nA probabilistic view of linguistic knowledge. Cogni-\ntive Science, 41(5):1202â€“1241.\nPing Li and Yu-Ju Lan. 2022. Digital language learn-\ning (DLL): Insights from behavior, cognition, and\nthe brain. Bilingualism: Language and Cognition ,\n25(3):361â€“378.\nJeannette Littlemore and Graham D Low. 2006. Fig-\nurative thinking and foreign language learning .\nSpringer.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proc. of NAACL-HLT , pages\n1073â€“1094, Minneapolis, Minnesota.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoBERTa: A robustly optimized BERT pretraining\napproach. Preprint arXiv:1907.11692.\nFrederic M Lord. 1980. Applications of item response\ntheory to practical testing problems. Routledge.\nJames A. Michaelov, Seana Coulson, and Benjamin K\nBergen. 2023. Can peanuts fall in love with distri-\nbutional semantics? In Proc. of CogSci. Preprint\narXiv:2301.08731.\nCharles M. Mueller. 2011. English learnersâ€™ knowledge\nof prepositions: Collocational knowledge or knowl-\nedge based on meaning? System, 39(4):480â€“490.\nAkira Murakami and Nick C Ellis. 2022. Effects of\navailability, contingency, and formulaicity on the ac-\ncuracy of English grammatical morphemes in second\nlanguage writing. Language Learning.\nReza Norouzian, Michael de Miranda, and Luke Plon-\nsky. 2018. The Bayesian revolution in second lan-\nguage research: An applied approach. Language\nLearning, 68(4):1032â€“1075.\nReza Norouzian, Michael de Miranda, and Luke Plon-\nsky. 2019. A Bayesian approach to measuring ev-\nidence in L2 research: An empirical investigation.\nThe Modern Language Journal, 103(1):248â€“261.\nTom Oâ€™Hara and Janyce Wiebe. 2003. Preposition se-\nmantic classification via Treebank and FrameNet. In\nProc. of CoNLL, pages 79â€“86, Edmonton, Canada.\nRebecca J. Passonneau and Bob Carpenter. 2014. The\nBenefits of a Model of Annotation. Transactions of\nthe ACL, 2:311â€“326.\nSiyao Peng, Yang Liu, Yilun Zhu, Austin Blodgett,\nYushi Zhao, and Nathan Schneider. 2020. A corpus\nof adpositional supersenses for Mandarin Chinese. In\nProc. of LREC, pages 5986â€“5994, Marseille, France.\nMaja PopoviÂ´c. 2017. Comparing language related is-\nsues for NMT and PBMT between German and En-\nglish. The Prague Bulletin of Mathematical Linguis-\ntics, 108(1):209.\nJakob Prange and Nathan Schneider. 2021. Draw mir a\nsheep: A supersense-based analysis of German case\nand adposition semantics. KÃ¼nstliche Intelligenz,\n35(3):291â€“306.\nAdam John Privitera, Mohammad Momenian, and Bren-\ndan Weekes. 2022. Graded bilingual effects on at-\ntentional network function in chinese high school\nstudents. Bilingualism: Language and Cognition ,\npage 1â€“11.\nInes Rehbein and Josef Ruppenhofer. 2017. Detecting\nannotation noise in automatically labelled data. In\nProc. of ACL, pages 1160â€“1170, Vancouver, Canada.\nNathan Schneider, Jena D. Hwang, Vivek Srikumar,\nJakob Prange, Austin Blodgett, Sarah R. Moeller,\nAviram Stern, Adi Bitan, and Omri Abend. 2018.\nComprehensive supersense disambiguation of En-\nglish prepositions and possessives. In Proc. of ACL,\npages 185â€“196, Melbourne, Australia.\n12732\nJoÃ£o Sedoc and Lyle Ungar. 2020. Item Response The-\nory for efficient human evaluation of chatbots. In\nProc. of Eval4NLP@EMNLP, pages 21â€“33, Online.\nVivek Srikumar and Dan Roth. 2013. Modeling seman-\ntic relations expressed by prepositions. Transactions\nof the ACL, 1:231â€“242.\nAnaÃ¯s Tack. 2021. Mark my words! On the automated\nprediction of lexical difficulty for foreign language\nreaders. Ph.D. thesis, KU Leuven.\nAndrea Tyler. 2012. Cognitive linguistics and second\nlanguage learning: Theoretical basics and experi-\nmental evidence. Routledge.\nAndrea Tyler and Vyvyan Evans. 2003. The seman-\ntics of English prepositions: Spatial scenes, embod-\nied meaning, and cognition. Cambridge University\nPress.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Åukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proc. of NeurIPS, pages 5998â€“6008,\nLong Beach, CA, USA.\nWilliam Yang Wang, Elijah Mayfield, Suresh Naidu,\nand Jeremiah Dittmar. 2012. Historical analysis of\nlegal opinions with a sparse mixed-effects latent vari-\nable model. In Proc. of ACL, pages 740â€“749, Jeju\nIsland, Korea.\nMarion Weller, Sabine Schulte im Walde, and Alexander\nFraser. 2014. Using noun class information to model\nselectional preferences for translating prepositions in\nSMT. In Proc. of AMTA, pages 275â€“287, Vancouver,\nCanada.\nMan Ho Ivy Wong. 2022. Fostering conceptual\nunderstanding through computer-based animated\nschematic diagrams and cue contrast. TESOL Quar-\nterly.\nA Effects of Preposition Cues\nIn the main text, for brevity, we omitted a detailed\nanalysis of the effects of specific combinations of\npreposition form, function, and usage on student\nperformance. Here we take a closer look at the six\ntypes of cues: in with the CONTAINMENT function,\nat with the TARGET and POINT functions, and over\nwith the HIGHER ,COVER , and CROSS functions.\nIn fig. 5, we see that there is a wide spread among\nstudents for each of the cue types, especially at the\nPET. The fact that these effects are estimated as\ninteractions in addition to the student-level inter-\ncepts suggests, again, that studentsâ€™ skill sets are\nunique, depending on the preposition cue, which is\nalso illustrated for 5 randomly chosen students.\nIn fig. 6, we see that the difficulty of these\nsix cues varies greatly, depending on both spatial/\nabstract use and task type. In fact, the difficulty\nranking is largely reversed between GJT and PET.\nAs a striking example of this, at-TARGET -Abstract\nand in-CONTAIN -Abstract are the easiest cues to\njudge correctly in the GJT but most difficult to pro-\nduce in the PET. There exceptions to this trend, too.\nE.g., at-POINT -Abstract is relatively difficult in\nboth GJT and PET. Another interesting observation\nis that, in the PET, both usages of over-HIGHER\nare much easier to produce than any other cue.\n0.6\n 0.4\n 0.2\n 0.0 0.2 0.4 0.6\nover-HIGHER\nover-CROSS\nover-COVER\nin-CONTAIN\nat-TARGET\nat-POINT\nGJT\n0.6\n 0.4\n 0.2\n 0.0 0.2 0.4 0.6\nover-HIGHER\nover-CROSS\nover-COVER\nin-CONTAIN\nat-TARGET\nat-POINT\nPET\nFigure 5:Spread among student effect means (x-axis)\nin interaction with preposition form/function. 5 ran-\ndomly chosen students are shown exemplarily (filled\nshapes; empty circles are outliers). Note that, while in\nour other figures the error bars denote standard devi-\nations over modelsâ€™ marginal parameter distributions,\nhere they describe the distribution over students of esti-\nmated mean interaction effects.\n12733\n0.50\n 0.25\n 0.00 0.25 0.50 0.75 1.00 1.25\nin-CONTAIN\nover-COVER\nat-TARGET\nover-CROSS\nover-HIGHER\nat-POINT\nABSTRACT\nSPATIAL\nat-TARGET:ABSTRACT\nin-CONTAIN:ABSTRACT\nover-CROSS:SPATIAL\nover-COVER:ABSTRACT\nat-POINT:SPATIAL\nover-COVER:SPATIAL\nover-HIGHER:ABSTRACT\nover-CROSS:ABSTRACT\nin-CONTAIN:SPATIAL\nover-HIGHER:SPATIAL\nat-TARGET:SPATIAL\nat-POINT:ABSTRACT\nGJT (sorted by GJT)\n0.50\n 0.25\n 0.00 0.25 0.50 0.75 1.00 1.25\nover-HIGHER\nover-COVER\nat-POINT\nover-CROSS\nat-TARGET\nin-CONTAIN\nSPATIAL\nABSTRACT\nover-HIGHER:SPATIAL\nover-HIGHER:ABSTRACT\nat-TARGET:SPATIAL\nover-COVER:SPATIAL\nat-POINT:SPATIAL\nover-COVER:ABSTRACT\nover-CROSS:ABSTRACT\nat-POINT:ABSTRACT\nin-CONTAIN:SPATIAL\nover-CROSS:SPATIAL\nin-CONTAIN:ABSTRACT\nat-TARGET:ABSTRACT\nGJT (sorted by PET)\n0.50\n 0.25\n 0.00 0.25 0.50 0.75 1.00 1.25\nin-CONTAIN\nover-COVER\nat-TARGET\nover-CROSS\nover-HIGHER\nat-POINT\nABSTRACT\nSPATIAL\nat-TARGET:ABSTRACT\nin-CONTAIN:ABSTRACT\nover-CROSS:SPATIAL\nover-COVER:ABSTRACT\nat-POINT:SPATIAL\nover-COVER:SPATIAL\nover-HIGHER:ABSTRACT\nover-CROSS:ABSTRACT\nin-CONTAIN:SPATIAL\nover-HIGHER:SPATIAL\nat-TARGET:SPATIAL\nat-POINT:ABSTRACT\nPET (sorted by GJT)\n0.50\n 0.25\n 0.00 0.25 0.50 0.75 1.00 1.25\nover-HIGHER\nover-COVER\nat-POINT\nover-CROSS\nat-TARGET\nin-CONTAIN\nSPATIAL\nABSTRACT\nover-HIGHER:SPATIAL\nover-HIGHER:ABSTRACT\nat-TARGET:SPATIAL\nover-COVER:SPATIAL\nat-POINT:SPATIAL\nover-COVER:ABSTRACT\nover-CROSS:ABSTRACT\nat-POINT:ABSTRACT\nin-CONTAIN:SPATIAL\nover-CROSS:SPATIAL\nin-CONTAIN:ABSTRACT\nat-TARGET:ABSTRACT\nPET (sorted by PET)\nFigure 6:Effect estimates for interactions between preposition form/fxns and spatial vs. abstract usage.\n12734\nACL 2023 Responsible NLP Checklist\nA For every submission:\nâ–¡\u0013 A1. Did you describe the limitations of your work?\nSection 6 Discussion and Conclusions; Limitations Statement\nâ–¡\u0017 A2. Did you discuss any potential risks of your work?\nWe conduct a small-scale research and replication study. We will release our experimental software\ncode, but do not deploy any end-user applications.\nâ–¡\u0013 A3. Do the abstract and introduction summarize the paperâ€™s main claims?\nSection 1 Introduction\nâ–¡\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB â–¡\u0013 Did you use or create scientiï¬c artifacts?\nWe used experimental data (stimuli and behavioral results) from Wong (2022). This is explained and\ndescribed in section 3 Original Study and Data.\nâ–¡\u0013 B1. Did you cite the creators of artifacts you used?\nsection 3 Original Study and Data and throughout the paper\nâ–¡\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nThe data are currently not publicly available. They were shared with us by the author of the original\nstudy, who is also a co-author on this paper.\nâ–¡ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciï¬ed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\nâ–¡\u0013 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiï¬es individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nsection 3 Original Study and Data; Ethics Statement\nâ–¡\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nsection 3 Original Study and Data\nâ–¡\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiï¬cant, while on small test sets they may not be.\nsection 3 Original Study and Data; section 5 Evaluation\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n12735\nC â–¡\u0013 Did you run computational experiments?\nsection 4 Models; section 5 Evaluation\nâ–¡\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nsection 4 Models\nâ–¡ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nNot applicable. We did not perform extensive hyperparameter search and are not proposing a\nstate-of-the-art model conï¬guration.\nâ–¡\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nsection 5 Evaluation\nâ–¡\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nsection 4 Models\nD â–¡\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nsection 3 Original Study and Data; section 5 Evaluation\nâ–¡\u0017 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nWe computationally replicate data analysis and outcome prediction of data that was collected by\nother researchers. We cite and discuss the relevant publication, which provides detailed information\nabout participants and procedures.\nâ–¡\u0017 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participantsâ€™ demographic\n(e.g., country of residence)?\nSee above.\nâ–¡\u0017 D3. Did you discuss whether and how consent was obtained from people whose data youâ€™re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nSee above.\nâ–¡\u0013 D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nSee above.\nâ–¡\u0013 D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nsection 3 Original Study and Data\n12736",
  "topic": "Grammaticality",
  "concepts": [
    {
      "name": "Grammaticality",
      "score": 0.83766770362854
    },
    {
      "name": "Computer science",
      "score": 0.751594066619873
    },
    {
      "name": "Learnability",
      "score": 0.6859825253486633
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6790978312492371
    },
    {
      "name": "Natural language processing",
      "score": 0.6315077543258667
    },
    {
      "name": "Bayesian probability",
      "score": 0.6080148220062256
    },
    {
      "name": "Sentence",
      "score": 0.585746705532074
    },
    {
      "name": "Frequentist inference",
      "score": 0.4899541437625885
    },
    {
      "name": "Replicate",
      "score": 0.4855217933654785
    },
    {
      "name": "Language model",
      "score": 0.44734641909599304
    },
    {
      "name": "Machine learning",
      "score": 0.4453471899032593
    },
    {
      "name": "Bayesian inference",
      "score": 0.33682894706726074
    },
    {
      "name": "Grammar",
      "score": 0.22243759036064148
    },
    {
      "name": "Linguistics",
      "score": 0.18434196710586548
    },
    {
      "name": "Statistics",
      "score": 0.13673889636993408
    },
    {
      "name": "Mathematics",
      "score": 0.11246615648269653
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}