{
  "title": "Empower Sequence Labeling with Task-Aware Neural Language Model",
  "url": "https://openalex.org/W2756381707",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2100063962",
      "name": "Liyuan Liu",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2223914299",
      "name": "Jingbo Shang",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2108009659",
      "name": "Xiang Ren",
      "affiliations": [
        "Southern California University for Professional Studies",
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2331775461",
      "name": "Frank Xu",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2096153351",
      "name": "Huan Gui",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2105091540",
      "name": "Jian Peng",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2103606203",
      "name": "Jiawei Han",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2100063962",
      "name": "Liyuan Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2223914299",
      "name": "Jingbo Shang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108009659",
      "name": "Xiang Ren",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2331775461",
      "name": "Frank Xu",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2096153351",
      "name": "Huan Gui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105091540",
      "name": "Jian Peng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103606203",
      "name": "Jiawei Han",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6685810106",
    "https://openalex.org/W6683738474",
    "https://openalex.org/W2758771122",
    "https://openalex.org/W2056451646",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W581956982",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W6697155078",
    "https://openalex.org/W2733116014",
    "https://openalex.org/W6726710110",
    "https://openalex.org/W2295030615",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2141099517",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2515406546",
    "https://openalex.org/W6691431627",
    "https://openalex.org/W2004763266",
    "https://openalex.org/W2609130030",
    "https://openalex.org/W6683257257",
    "https://openalex.org/W2516255829",
    "https://openalex.org/W4412260678",
    "https://openalex.org/W6678016260",
    "https://openalex.org/W2951299559",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W2963940534",
    "https://openalex.org/W2520117834",
    "https://openalex.org/W2137845336",
    "https://openalex.org/W2963563735",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2156515921",
    "https://openalex.org/W2610748790",
    "https://openalex.org/W2119035792",
    "https://openalex.org/W4394643672",
    "https://openalex.org/W2951941802",
    "https://openalex.org/W2045993505",
    "https://openalex.org/W2593560537",
    "https://openalex.org/W2950133940",
    "https://openalex.org/W2952264346",
    "https://openalex.org/W2950938254",
    "https://openalex.org/W2962950859",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2962902328",
    "https://openalex.org/W2963338481",
    "https://openalex.org/W1515847863",
    "https://openalex.org/W2952230511",
    "https://openalex.org/W2556468274",
    "https://openalex.org/W2963625095",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2158899491",
    "https://openalex.org/W2144578941",
    "https://openalex.org/W2963706742",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2296283641"
  ],
  "abstract": "Linguistic sequence labeling is a general approach encompassing a variety of problems, such as part-of-speech tagging and named entity recognition. Recent advances in neural networks (NNs) make it possible to build reliable models without handcrafted features. However, in many cases, it is hard to obtain sufficient annotations to train these models. In this study, we develop a neural framework to extract knowledge from raw texts and empower the sequence labeling task. Besides word-level knowledge contained in pre-trained word embeddings, character-aware neural language models are incorporated to extract character-level knowledge. Transfer learning techniques are further adopted to mediate different components and guide the language model towards the key knowledge. Comparing to previous methods, these task-specific knowledge allows us to adopt a more concise model and conduct more efficient training. Different from most transfer learning methods, the proposed framework does not rely on any additional supervision. It extracts knowledge from self-contained order information of training sequences. Extensive experiments on benchmark datasets demonstrate the effectiveness of leveraging character-level knowledge and the efficiency of co-training. For example, on the CoNLL03 NER task, model training completes in about 6 hours on a single GPU, reaching F_1 score of 91.71+/-0.10 without using any extra annotations.",
  "full_text": "Empower Sequence Labeling with\nTask-Aware Neural Language Model\nLiyuan Liu,† Jingbo Shang,† Xiang Ren,♯\nFrank F . Xu,‡ Huan Gui,♭ Jian Peng,† Jiawei Han†\n†University of Illinois at Urbana-Champaign,{ll2, shang7, jianpeng, hanj}@illinois.edu\n♯University of Southern California, xiangren@usc.edu\n‡Shanghai Jiao Tong University, frankxu@sjtu.edu.cn\n♭Facebook, huangui@fb.com\nAbstract\nLinguistic sequence labeling is a general approach encom-\npassing a variety of problems, such as part-of-speech tag-\nging and named entity recognition. Recent advances in neu-\nral networks (NNs) make it possible to build reliable mod-\nels without handcrafted features. However, in many cases, it\nis hard to obtain sufﬁcient annotations to train these mod-\nels. In this study, we develop a neural framework to extract\nknowledge from raw texts and empower the sequence label-\ning task. Besides word-level knowledge contained in pre-\ntrained word embeddings, character-aware neural language\nmodels are incorporated to extract character-level knowledge.\nTransfer learning techniques are further adopted to mediate\ndifferent components and guide the language model towards\nthe key knowledge. Comparing to previous methods, these\ntask-speciﬁc knowledge allows us to adopt a more concise\nmodel and conduct more efﬁcient training. Different from\nmost transfer learning methods, the proposed framework does\nnot rely on any additional supervision. It extracts knowledge\nfrom self-contained order information of training sequences.\nExtensive experiments on benchmark datasets demonstrate\nthe effectiveness of leveraging character-level knowledge and\nthe efﬁciency of co-training. For example, on the CoNLL03\nNER task, model training completes in about 6 hours on a\nsingle GPU, reaching F\n1 score of 91.71±0.10 without using\nany extra annotations.\nIntroduction\nLinguistic sequence labeling is a fundamental framework.\nIt has been applied to a variety of tasks including part-of-\nspeech (POS) tagging, noun phrase chunking and named en-\ntity recognition (NER) (Ma and Hovy 2016; Sha and Pereira\n2003). These tasks play a vital role in natural language\nunderstanding and fulﬁll lots of downstream applications,\nsuch as relation extraction, syntactic parsing, and entity link-\ning (Liu et al. 2017; Luo et al. 2015).\nTraditional methods employed machine learning models\nlike Hidden Markov Models (HMMs) and Conditional Ran-\ndom Fields (CRFs), and have achieved relatively high per-\nformance. However, these methods have a heavy reliance on\nhandcrafted features (e.g., whether a word is capitalized) and\nlanguage-speciﬁc resources (e.g., gazetteers). Therefore, it\nCopyright c⃝ 2018, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\ncould be difﬁcult to apply them to new tasks or shift to\nnew domains. To overcome this drawback, neural networks\n(NNs) have been proposed to automatically extract features\nduring model learning. Nevertheless, considering the over-\nwhelming number of parameters in NNs and the relatively\nsmall size of most sequence labeling corpus, annotations\nalone may not be sufﬁcient to train complicated models. So,\nguiding the learning process with extra knowledge could be\na wise choice.\nAccordingly, transfer learning and multi-task learning\nhave been proposed to incorporate such knowledge. For ex-\nample, NER can be improved by jointly conducting other re-\nlated tasks like entity linking or chunking (Luo et al. 2015;\nPeng and Dredze 2016). After all, these approaches would\nrequire additional supervision on related tasks, which might\nbe hard to get, or not even existent for low-resource lan-\nguages or special domains.\nAlternatively, abundant knowledge can be extracted from\nraw texts, and enhance a variety of tasks. Word embedding\ntechniques represent words in a continuous space (Mikolov\net al. 2013; Pennington, Socher, and Manning 2014) and\nretain the semantic relations among words. Consequently,\nintegrating these embeddings could be beneﬁcial to many\ntasks (Liu et al. 2017; Lample et al. 2016). Nonetheless,\nmost embedding methods take a word as a basic unit,\nthus only obtaining word-level knowledge, while character\nawareness is also crucial and highly valued in most state-of-\nthe-art NN models.\nOnly recently, character-level knowledge has been lever-\naged and empirically veriﬁed to be helpful in numerous se-\nquence labeling tasks (Peters et al. 2017; Rei 2017). Di-\nrectly adopting pre-trained language models, character-level\nknowledge can be integrated as context embeddings and\ndemonstrate its potential to achieve the state-of-the-art (Pe-\nters et al. 2017). However, the knowledge extracted through\npre-training is not task-speciﬁc, thus containing a large ir-\nrelevant portion. So, this approach would require a bigger\nmodel, external corpus and longer training. For example,\none of its language models was trained on 32 GPUs for more\nthan half a month, which is unrealistic in many situations.\nIn this paper, we propose an effective sequence labeling\nframework, LM-LSTM-CRF, which leverages both word-\nlevel and character-level knowledge in an efﬁcient way. For\ncharacter-level knowledge, we incorporate a neural language\nThe Thirty-Second AAAI Conference\non Artificial Intelligence (AAAI-18)\n5253\nP i e r r e ␣ V i n k e n ␣ , ␣ 6 1 ␣ y e a r s\nPierre\n␣␣\nPierre Vinken- , 61 years old\nPierre Vinken— , 61 Vinken-\nVinken , 61 years\ncharacter-level\nLSTMs\nsoft-max for\n language model\nword-level bi-LSTM\nCRF for Sequence Labeling\nhighway\nhighway\n  embedding\ncharacter-level lstm unit\nword-level lstm unit\nconcat unit\nc0, c1,0 c1,1 c1,2 c1,3 c1,4 c1,5 c1, c2,0 c2,1 c2,2 c2,3 c2,4 c2,5 c2, c3,0 c3, c4,0 c4,1 c4, c5,0 c5,1 c5,2 c5,3 c5,4 c5,\nx1 x2 x3 x4 x5\nc\nNNP\ny2\nNNP\ny1\n,\ny3\nCD\ny4\nNNS\ny5\nbackward-to-SL highway\nbackward-to-LM highway\nforward-to-SL highway\nforward-to-LM highway\nx\nFigure 1: LM-LSTM-CRF Neural Architecture\nmodel with the sequence labeling task and conduct multi-\ntask learning to guide the language model towards task-\nspeciﬁc key knowledge. Besides the potential of training\na better model, this strategy also poses a new challenge.\nBased on our experiments, when the tasks are discrepant,\nlanguage models could be harmful to sequence labeling in a\nna¨ıve co-training setting. For this reason, we employ high-\nway networks (Srivastava, Greff, and Schmidhuber 2015)\nto transform the output of character-level layers into dif-\nferent semantic spaces, thus mediating and unifying these\ntwo tasks. For word-level knowledge, we choose to ﬁne-tune\npre-trained word embeddings instead of co-training or pre-\ntraining the whole word-level layers, because the majority\nof parameters in word-level layers come from the embed-\nding layer and such co-training or pre-training cost lots of\ntime and resources.\nWe conduct experiments on the CoNLL 2003 NER task,\nthe CoNLL 2000 chunking task, as well as the WSJ portion\nof the Penn Treebank POS tagging task.LM-LSTM-CRF\nachieves a signiﬁcant improvement over the state-of-the-art.\nAlso, our co-training strategy allows us to capture more use-\nful knowledge with a smaller network, thus yielding much\nbetter efﬁciency without loss of effectiveness.\nLM-LSTM-CRF Framework\nThe neural architecture of our proposed framework, LM-\nLSTM-CRF, is visualized in Fig. 1. For a sentence with\nannotations y =( y1,...,y n), its word-level input is\nmarked as x =( x1,x2,...,x n), where xi is the i-\nth word; its character-level input is recorded as c =\n(c0, ,c1,1,c1,2,...,c 1, ,c2,1,...,c n, ), where ci,j is the j-th\ncharacter for wordwi and ci, is the space character afterwi.\nThese notations are also summarized in Table 1.\nNow, we ﬁrst discuss the multi-task learning strategy and\nthen introduce the architecture in a bottom-up fashion.\nx word-level input xi i-th word\nc character-level input ci,j j-th char inxi\nci, space after xi c0, space before x1\ny label sequence yi label of xi\nfi output of forward character-level LSTM atci,\nri output of backward character-level LSTM atci,\nfLi output of forward-to-LM highway unit\nrLi output of backward-to-LM highway unit\nfNi output of forward-to-SL highway unit\nrNi output of backward-to-SL highway unit\nvi input of word-level bi-LSTM atxi\nzi output of word-level bi-LSTM atxi\nTable 1: Notation Table.\nMulti-task Learning Strategy\nAs shown in Fig. 1, our language model and sequence la-\nbeling model share the same character-level layer, which\nﬁts the setting of multi-task learning and transfer learn-\ning. However, different from typical models of this set-\nting, our two tasks are not strongly related. This discor-\ndance makes our problem more challenging. E.g., although\na naive co-training setting, which directly uses the output\nfrom character-level layers, could be effective in several\nscenarios (Y ang, Salakhutdinov, and Cohen 2017), for our\ntwo tasks, it would hurt the performance. This phenomenon\nwould be further discussed in the experiment section.\nTo mediate these two tasks, we transform the output of\ncharacter-level layers into different semantic spaces for dif-\nferent objectives. This strategy allows character-level lay-\ners to focus on general feature extraction and lets the trans-\nform layers select task-speciﬁc features. Hence, our lan-\nguage model can provide related knowledge to the sequence\nlabeling, without forcing it to share the whole feature space.\n5254\nCharacter-level Layer\nCharacter-level neural language models are trained purely\non unannotated sequence data but can capture the underly-\ning style and structure. For example, it can mimic Shake-\nspeare’s writing and generate sentences of similar styles,\nor even master the grammar of programming languages\n(e.g., XML, L\nATEX, and C) and generate syntactically cor-\nrect codes (Karpathy 2015). Accordingly, we adopted the\ncharacter-level Long Short Term Memory (LSTM) networks\nto process character-level input. Aiming to capture lexical\nfeatures instead of remembering words’ spelling, we adjust\nthe prediction from the next character to the next word. As\nin Fig. 1, the character-level LSTM would only make pre-\ndictions for the next word at word boundaries (i.e., space\ncharacters or c\ni, ).\nFurthermore, we coupled two LSTM units to capture in-\nformation in both forward and backward directions. Al-\nthough it seems similar to the bi-LSTM unit, the outputs of\nthese two units are processed and aligned differently. Specif-\nically, we record the output of forward LSTM atc\ni, as fi,\nand the output of backward LSTM atci, as ri.\nHighway Layer\nIn computer vision, Convolutional Neural Networks (CNN)\nhas been proved to be an effective feature extractor, but its\noutput needs to be further transformed by fully-connected\nlayers to achieve the state-of-the-art. Bearing this in mind,\nit becomes natural to stack additional layers upon the ﬂat\ncharacter-level LSTMs. More speciﬁcally, we employ high-\nway units (Srivastava, Greff, and Schmidhuber 2015), which\nallow unimpeded information ﬂowing across several layers.\nTypically, highway layers conduct nonlinear transformation\nas m = H(n)= t ⊙ g(W\nHn +bH)+(1 −t)⊙ n , where\n⊙ is element-wise product, g(·) is a nonlinear transforma-\ntion such as ReLU in our experiments,t = σ(WTn +bT)is\ncalled transform gate and(1−t) is called carry gate.\nIn our ﬁnal architecture, there are four highway\nunits, named forward-to-LM, forward-to-SL,\nbackward-to-LM, and backward-to-SL. The ﬁrst\ntwo transfer fi into fLi and fNi, and the last two transfer\nri into rLi and rNi. fLi and rLi are used in the language\nmodel, whilefNi and rNi are used in the sequence labeling.\nWord-level Layer\nBi-LSTM is adopted as the word-level structure to capture\ninformation in both directions. As shown in Fig. 1, we con-\ncatenate f\nNi and rNi−1 with word embeddings and then\nfeed them into the bi-LSTM. Note that, in the backward\ncharacter-level LSTM, c\ni−1, is the space character before\nword xi, therefore, fNi would be aligned and concatenated\nwith rNi−1 instead of rNi. For example, in Fig. 1, the word\nembeddings of ‘Pierre’ will be concatenated with the output\nof theforward-to-SL over ‘... Pierre\n’ and the output of\nthe backward-to-SL over ‘... erreiP ’.\nAs to word-level knowledge, we chose to ﬁne-tune pre-\ntrained word embeddings, instead of co-training the whole\nword-level layer. This is because most parameters of our\nword-level model come from word embeddings, and ﬁne-\ntuning pre-trained word embeddings have been veriﬁed to be\neffective in leveraging word-level knowledge (Ma and Hovy\n2016). Besides, current word embedding methods can eas-\nily scale to the large corpus; pre-trained word embeddings\nare available in many languages and domains (Fernandez,\nY u, and Downey 2017). However, this strategy cannot be ap-\nplied to character-level layers, since the embedding layer of\ncharacter-level layers contains very few parameters. Based\non these considerations, we applied different strategies to\nleverage word-level knowledge from character-level.\nCRF for Sequence Labeling\nLabel dependencies are crucial for sequence labeling tasks.\nFor example, in NER task with BIOES annotation, it is\nnot only meaningless but illegal to annotate I-PER after\nB-ORG (i.e., mixing the person and the organization). There-\nfore, jointly decoding a chain of labels can ensure the result-\ning label sequence to be meaningful. Conditional random\nﬁeld (CRF) has been included in most state-of-the-art mod-\nels to capture such information and further avoid generat-\ning illegal annotations. Consequently, we build a CRF layer\nupon the word-level LSTM.\nFor training instance(x\ni,ci,yi), we suppose the output of\nword-level LSTM is Zi =( zi,1,zi,2,..., zi,n). CRF mod-\nels describe the probability of generating the whole label se-\nquence with regard to (x\ni,ci) or Z. That is, p(ˆy|xi,ci) or\np(ˆy|Z), where ˆy =(ˆy1,..., ˆyn)is a generic label sequence.\nSimilar to (Ma and Hovy 2016), we deﬁne this probability\nas follows.\np(ˆy|x\ni,ci)=\n∏n\nj=1 φ(ˆyj−1,ˆyj,zj)∑\ny′∈Y(Z)\n∏n\nj=1 φ(y′\nj−1,y′\nj,zj) (1)\nHere, Y(Z) is the set of all generic label sequences,\nφ(yj−1,yj,zj)=e x p ( Wyj−1,yj zi + byj−1,yj ), where\nWyj−1,yj and byj−1,yj are the weight and bias parameters\ncorresponding to the label pair(yj−1,yj).\nFor training, we minimize the following negative log-\nlikelihood.\nJCRF = −\n∑\ni\nlogp(yi|Zi) (2)\nAnd for testing or decoding, we want to ﬁnd the optimal\nsequence y∗ that maximizes the likelihood.\ny∗ =a r g m a x\ny∈Y(Z)\np(y|Z) (3)\nAlthough the denominator of Eq. 1 is complicated, we can\ncalculate Eqs. 2 and 3 efﬁciently by the Viterbi algorithm.\nNeural Language Model\nThe language model is a family of models describing the\ngeneration of sequences. In a neural language model, the\ngeneration probability of the sequence x =( x\n1,...,x n) in\nthe forward direction (i.e., from left to right) is deﬁned as\npf(x1,...,x n)=\nN∏\ni=1\npf(xi|x1,...,x i−1)\n5255\nDataset # of Sentences\nTrain Dev Test\nCoNLL03 NER 14,987 3,466 3,684\nCoNLL00 chunking 7,936 1,000 2,012\nWSJ 38,219 5,527 5,426\nTable 2: Dataset summary.\nwhere pf(xi|x1,...,x i−1) is computed by NN.\nIn this paper, our neural language model makes\npredictions for words but takes the character se-\nquence as input. Speciﬁcally, we would calcu-\nlate p\nf(xi|c0, ,...,c i−1,1,...,c i−1, ) instead of\npf(xi|x1,...,x i−1). This probability is assumed as\npf(xi|c0, ,...,c i−1, )= exp(wT\nxi fNi−1)∑\nˆxj\nexp(wT\nˆxj\nfNi−1)\nwhere wxi is the weight vector for predicting wordxi.I no r -\nder to extract knowledge in both directions, we also adopted\na reversed-order language model, which calculates the gen-\neration probability from right to left as\np\nr(x1,...,x n)=\nN∏\ni=1\npr(xi|ci+1, ,...,c n, )\nwhere pr(xi|ci+1, ,...,c n, )= exp(wT\nxi rNi)∑\nˆxj\nexp(wT\nˆxj\nrNi)\nThe following negative log likelihood is applied as the ob-\njective function of our language model.\nJLM = −\n∑\ni\nlogpf(xi)−\n∑\ni\nlogpr(xi) (4)\nJoint Model Learning\nBy combining Eqs. 2 and 4, we can write the joint objective\nfunction as\nJ = −\n∑\ni\n(\np(yi|Zi)+ λ\n(\nlogpf(xi)+log pr(xi)\n))\n(5)\nwhere λ is a weight parameter. In our experiments,λ is al-\nways set to1 without any tuning.\nIn order to train the neural network efﬁciently, stochas-\ntic optimization has been adopted. And at each iteration, we\nsample a batch of training instances and perform an update\naccording to the summand function of Eq. 5: p(y\ni|Zi)+\nλ\n(\nlogpf(xi)+log pr(xi)\n)\nExperiments\nHere, we evaluate LM-LSTM-CRF on three bench-\nmark datasets: the CoNLL 2003 NER dataset (Tjong\nKim Sang and De Meulder 2003), the CoNLL 2000 chunk-\ning dataset (Tjong Kim Sang and Buchholz 2000), and\nthe Wall Street Journal portion of Penn Treebank dataset\n(WSJ) (Marcus, Marcinkiewicz, and Santorini 1993).\n• CoNLL03 NER contains annotations for four entity\ntypes: PER, LOC, ORG, and MISC. It has been separated\ninto training, development and test sets.\nLayer Parameter POS NER chunking\ncharacter-level dimension 30embedding\ncharacter-level depth 1\nLSTM state size 300\nHighway depth 1\nword-level dimension 100embedding\nword-level depth 1\nbi-LSTM state size 300\nOptimization η0 0.015 0.01\nTable 3: Hyper-parameters ofLM-LSTM-CRF.\n• CoNLL00 chunkingdeﬁnes eleven syntactic chunk types\n(e.g., NP, VP) in addition toOther. It only includes train-\ning and test sets. Following previous works (Peters et al.\n2017), we sampled 1000 sentences from training set as a\nheld-out development set.\n• WSJ contains 25 sections and categorizes each word into\n45 POS tags. We adopt the standard split and use sections\n0-18 as training data, sections 19-21 as development data,\nand sections 22-24 as test data (Manning 2011).\nThe corpus statistics are summarized in Table 2. We re-\nport the accuracy for the WSJ dataset. And in the ﬁrst two\ndatasets, we adopt the ofﬁcial evaluation metric (micro-\naveraged F\n1), and use the BIOES scheme (Ratinov and Roth\n2009). Also, in all three datasets, rare words (i.e., frequency\nless than 5) are replaced by a special token (<UNK>).\nNetwork Training\nFor a fair comparison, we didn’t spend much time on tun-\ning parameters but borrow the initialization, optimization\nmethod, and all related hyper-parameter values (except the\nstate size of LSTM) from the previous work (Ma and Hovy\n2016). For the hidden state size of LSTM, we expand it from\n200 to 300, because introducing additional knowledge al-\nlows us to train a larger network. We will further discuss\nthis change later. Table 3 summarizes some important hyper-\nparameters. Since the CoNLL00 is similar to the CoNLL03\nNER dataset, we conduct experiments with the same param-\neters on both tasks.\nInitialization. We use GloV e 100-dimension pre-trained\nword embeddings released by Stanford\n1 and randomly ini-\ntialize the other parameters (Glorot and Bengio 2010; Joze-\nfowicz, Zaremba, and Sutskever 2015).\nOptimization. We employ mini-batch stochastic gradient\ndescent with momentum. The batch size, the momentum and\nthe learning rate are set to10, 0.9 and η\nt = η0\n1+ρt, where η0\nis the initial learning rate and ρ =0 .05 is the decay ratio.\nDropout is applied in our model, and its ratio is ﬁxed to0.5.\nTo increase stability, we use gradient clipping of5.0.\nNetwork Structure. The hyper-parameters of character-\nlevel LSTM are set to the same value of word-level bi-\nLSTM. We ﬁx the depth of highway layers as 1 to avoid\nan over-complicated model.\n1http://nlp.stanford.edu/projects/glove/\n5256\nNote that some baseline methods (e.g., (Chiu and Nichols\n2016; Peters et al. 2017)) incorporate the development set\nas a part of training. However, because we are using early\nstopping based on the evaluation on the development set,\nour model is trained purely on the training set.\nCompared Methods We consider three classes of base-\nline sequence labeling methods in our experiments.\n• Sequence Labeling Only. Without any additional su-\npervision or extra resources, LSTM-CRF (Lample et al.\n2016) and LSTM-CNN-CRF (Ma and Hovy 2016) are\nthe current state-of-art methods. We also list some top re-\nported performance on each dataset (Collobert et al. 2011;\nLuo et al. 2015; Chiu and Nichols 2016; Y ang, Salakhut-\ndinov, and Cohen 2017; Peters et al. 2017; Manning 2011;\nSøgaard and Goldberg 2016; Sun 2014).\n• Joint Model with Other Supervised Tasks. There are\nseveral attempts (Luo et al. 2015; Y ang, Salakhutdinov,\nand Cohen 2017) to enhance sequence labeling tasks\nby introducing additional annotations from other related\ntasks (e.g., enhance NER with entity linking labels).\n• Joint Model with Language Model: Language models\nhave been employed by some recent works to extract\nknowledge from raw text and thus enhancing sequence\nlabeling task. TagLM (Peters et al. 2017) leverages pre-\ntrained language models and shows the effectiveness with\nthe large external corpus, but the large model scale and\nlong training time make it hard to re-run this model. An-\nother work (Rei 2017) also incorporates the sequence la-\nbeling task with the language model.\nFor comparison, we tune the parameters of three most re-\nlated baselines (Ma and Hovy 2016; Lample et al. 2016;\nRei 2017)\n2. , and report the statics of the best working pa-\nrameter setting. Besides, we index these models by number,\nand summarize the results in Tables 4, 5 and 7.\nPerformance Comparison\nIn this section, we focus on the comparisons between\nLM-LSTM-CRF and pre vious state-of-the-arts, including\nboth effectiveness and efﬁciency. As demonstrated in Ta-\nbles 4, 5 and 7,LM-LSTM-CRF si gniﬁcantly outperforms\nall baselines without additional resources. Moreover, even\nfor those baselines with extra resources, LM-LSTM-CRF\nbeats most of them and is only slightly worse than TagLM\n(index 4) (Peters et al. 2017).\nTagLM (index 4) is equipped with both extra cor-\npoa (about 4000X larger than the CoNLL03 NER\ndataset) and a tremendous pre-trained forward language\nmodel ( 4096-8192-1024\n3) (Jozefowicz et al. 2016).\nDue to the expensive resources and time required by\n4096-8192-1024, even the authors of TagLM failed to\n2Implementations: https://github.com/xuezhemax/lasagnenlp\n(Ma et al. 2016), https://github.com/glample/tagger (Lample et\nal. 2016) and https://github.com/marekrei/sequence-labeler (Rei\n2017)\n34096-8192-1024 is composed of character-level CNN\nwith 4096 ﬁlters, 2 layers of stacked LSTMs with 8192 hidden\nunits each and a1024-dimension projection unit.\nExtra Resource Index & Model F1 score\nType V alue (±std)\ngazetteers 0) Collobert et al. 2011† reported 89.59\n1) Chiu et al. 2016† reported 91.62±0.33\nAIDA dataset 2) Luo et al. 2015 reported 91.20\nCoNLL 2000 / 3) Y ang et al. 2017† reported 91.26PTB-POS dataset\n1B Word dataset & 4) Peters et al. 2017†‡ reported 91.93±0.194096-8192-1024\n1B Word dataset 5) Peters et al. 2017†‡ reported 91.62±0.23\nNone\n6) Collobert et al. 2011† reported 88.67\n7) Luo et al. 2015 reported 89.90\n8) Chiu et al. 2016† reported 90.91±0.20\n9) Y ang et al. 2017† reported 91.20\n10) Peters et al. 2017† reported 90.87±0.13\n11) Peters et al. 2017†‡ reported 90.79±0.15\n12) Rei 2017†‡\nmean 87.38±0.36\nmax 87.94\nreported 86.26\n13) Lample et al. 2016†\nmean 90.76±0.08\nmax 91.14\nreported 90.94\n14) Ma et al. 2016†\nmean 91.37±0.17\nmax 91.67\nreported 91.21\n15) LM-LSTM-CRF †‡ mean 91.71±0.10\nmax 91.85\nTable 4: F1 score on the CoNLL03 NER dataset. We mark\nmodels adopting pre-trained word embedding as †, and\nrecord models which leverage language models as‡.\ntrain a backward language model of the same size, in-\nstead, chose a much smaller one ( LSTM-2048-5124).\nIt is worth noting that, when either extra corpus or\n4096-8192-1024 is absent, LM-LSTM-CRF shows\nsigniﬁcant improvements over TagLM (index 5, 10 and 11).\nAlso, LSTM-CNN-CRF outperforms LSTM-CRF in\nour experiments, which is different from (Reimers and\nGurevych 2017). During our experiments, we discover that,\nwhen trained on CPU, LSTM-CNN-CRF only reaches 90.83\nF\n1 score on the NER dataset, but gets 91.37 F1 score when\ntrained on GPU. We conjecture that this performance gap\nis due to the difference of runtime environments. Therefore,\nwe conduct all of our experiments on GPU. Additionally, we\ncan observe that, although co-trained with language model,\nresults of index 12 fails to outperform LSTM-CNN-CRF or\nLSTM-CRF. The reason of this phenomenon could be com-\nplicated and beyond the scope of this paper. However, it ver-\niﬁed the effectiveness of our method, and demonstrated the\ncontribution of outperforming these baselines.\nNER First of all, we have to point out that the results of in-\ndex 1, 4, 8, 10 and 11 are not directly comparable with others\nsince their ﬁnal models are trained on both training and de-\nvelopment set, while others are trained purely on the training\nset. As mentioned before,LM-LSTM-CRF outperforms all\nbaselines except TagLM (index 4). For a thorough com-\nparison, we also compare to its variants, TagLM (index 5),\nTagLM (index 10) and TagLM (index 11). Both index 10 and\n11 are trained on the CoNLL03 dataset alone, while index 11\nutilizes language model and index 10 doesn’t. Comparing F\n1\nscores of these two settings, we can ﬁnd that TagLM (index\n4LSTM-2048-512 is composed of a single-layer LSTM with\n2048 hidden units and a512-dimension projection unit.\n5257\nInd & Model Accuracy\nType V alue (±std)\n0) Collobert et al. 2011† reported 97.29\n16) Manning 2011 reported 97.28\n17) Søgaard 2011 reported 97.50\n18) Sun 2014 reported 97.36\n12) Rei 2017†‡\nmean 96.97±0.22\nmax 97.14\nreported 97.43\n13) Lample et al. 2016† mean±std 97.35±0.09\nmaximum 97.51\n14) Ma et al. 2016†\nmean±std 97.42±0.04\nmaximum 97.46\nreported 97.55\n15) LM-LSTM-CRF †‡ mean±std 97.53±0.03\nmaximum 97.59\nTable 5: Accuracy on the WSJ dataset. We mark models\nadopting pre-trained word embedding as†, and record mod-\nels which leverage language models as‡.\nModel CoNLL03 NER WSJ POS CoNLL00 Chunking\nh F1Score h Accuracy h F1Score\nLSTM-CRF 46 90.76 37 97.35 26 94.37\nLSTM-CNN-CRF 7 91.22 21 97.42 6 95.80\nLM-LSTM-CRF 6 91.71 16 97.53 5 95.96\nLSTM-CRF⋆ 4 91.19 8 97.44 2 95.82\nLSTM-CNN-CRF⋆ 3 90.98 7 96.98 2 95.51\nTable 6: Training statistics of TagLM (index 4 and 5) and\nLM-LSTM-CRF on the CoNLL03 NER dataset.\n11) even performs worse than TagLM (index 10) , which\nreveals that directly applying co-training might hurt the se-\nquence labeling performance. We will also discuss this chal-\nlenge later in the Highway Layers & Co-training section.\nBesides, changing the forward language model from\n4096-8192-1024 to LSTM-2048-512, TagLM (index\n5) gets a lower F\n1 score of 91.62 ±0.23. Comparing this\nscore to ours (91.71±0.10), one can verify that pre-trained\nlanguage model usually extracts a large portion of unrelated\nknowledge. Relieving such redundancy by guiding the lan-\nguage model with task-speciﬁc information, our model is\nable to conduct both effective and efﬁcient learning.\nPOS Tagging Similar to the NER task,LM-LSTM-CRF\noutperforms all baselines on the WSJ portion of the PTB\nPOS tagging task. Although the improvements over LSTM-\nCRF and CNN-LSTM-CRF are less obvious than those on\nthe CoNLL03 NER dataset, considering the fact that the\nPOS tagging task is believed to be easier than the NER task\nand current methods have achieved relatively high perfor-\nmance, this improvement could still be viewed as signiﬁcant.\nMoreover, it is worth noting that for both NER and POS tag-\nging tasks, LM-LSTM-CRF achieves not only higher F\n1\nscores, but also with smaller variances, which further veri-\nﬁes the superiority of our framework.\nChunking In the chunking task, LM-LSTM-CRF also\nachieves relatively high F1 scores, but with slightly higher\nvariances. Considering the fact that this corpus is much\nsmaller than the other two (only about 1/5 of WSJ or 1/2\nof CoNLL03 NER), we can expect more variance due to the\nExtra Resource Ind & Model F1 score\nType V alue (±std)\nPTB-POS 19) Hashimoto et al. 2016† reported 95.77\n20) Søgaard et al. 2016† reported 95.56\nCoNLL 2000 / 3)Y ang et al. 2017† reported 95.41PTB-POS dataset\n1B Word dataset 4) Peters et al. 2017†‡ reported 96.37±0.05\nNone\n21) Hashimoto et al. 2016† reported 95.02\n22) Søgaard et al. 2016† reported 95.28\n9) Y ang et al. 2017† reported 94.66\n12) Rei 2017†‡\nmean 94.24±0.11\nmax 94.33\nreported 93.88\n13) Lample et al. 2016† mean 94.37±0.07\nmaximum 94.49\n14) Ma et al. 2016† mean 95.80±0.13\nmaximum 95.93\n15) LM-LSTM-CRF †‡ mean 95.96±0.08\nmaximum 96.13\nTable 7: F1 score on the CoNLL00 chunking dataset. We\nmark models adopting pre-trained word embedding as†, and\nrecord models which leverage language models as‡.\nInd & Model F1score Module Time · Device\n15) LM-LSTM-CRF 91.71 total 6h ·GTX 1080\n5) Peters et al. 2017 91.62 LSTM-2048-512 320 h ·Telsa K40\nLSTM-2048-512 320 h ·Telsa K40\n4) Peters et al. 2017 91.93 4096-8192-1024 14112 h ·Telsa K40\nLSTM-2048-512 320 h ·Telsa K40\nTable 8: Training time and performance of LSTM-CRF,\nLSTM-CNN-CRF and LM-LSTM-CRF on three datasets.\nOur re-implementations are marked with⋆\nlack of training data. Still,LM-LSTM-CRF outperforms all\nbaselines without extra resources, and most of the baselines\ntrained with extra resources.\nEfﬁciency We implement LM-LSTM-CRF5 based on the\nPyTorch library6. Models has been trained on one GeForce\nGTX 1080 GPU, with training time recorded in Table 8.\nIn terms of efﬁciency, the language model component\nin LM-LSTM-CRF only introduces a small number of\nparameters in two highway units and a soft-max layer,\nwhich may not have a very large impact on the efﬁciency.\nTo control variables like infrastructures, we further re-\nimplemented both baselines, and report their performance\ntogether with original implementations. From the results,\nthese re-implementations achieve better efﬁciency compar-\ning to the original ones, but yield relative worse perfor-\nmance. Also, LM-LSTM-CRF achieves the best perfor-\nmance, and takes twice the training time of the most efﬁ-\ncient model, LSTM-CNN-CRF\n⋆. Empirically, considering\nthe difference among the implementations of these models,\nwe think these methods have roughly the same efﬁciency.\nBesides, we list the required time and resources for pre-\ntraining model index 4 and 5 on the NER task in Ta-\nble 6 (Jozefowicz et al. 2016). Comparing to these language\nmodels pre-trained on external corpus, our model has no\nsuch reliance on extensive corpus, and can achieve simi-\nlar performance with much more concise model and efﬁ-\ncient training. It veriﬁes that our LM-LSTM-CRF model\n5https://github.com/LiyuanLucasLiu/LM-LSTM-CRF\n6http://pytorch.org/\n5258\nModel State Size F1score±std Recall±std Precision±std\nLM-LSTM-CRF\n300 91.71±0.10 92.14±0.12 91.30±0.13\n200 91.63±0.23 92.07±0.22 91.19±0.30\n100 91.13±0.32 91.60±0.37 90.67±0.32\nLSTM-CRF\n300 90.76±0.08 90.82±0.08 90.69±0.08\n200 90.41±0.07 90.63±0.07 90.20±0.07\n100 90.74±0.22 91.08±0.50 90.42±0.17\nLSTM-CNN-CRF\n300 91.22±0.19 91.70±0.16 90.74±0.27\n200 91.37±0.17 91.08±0.53 90.58±0.11\n100 91.18±0.10 91.56±0.16 90.81±0.15\nTable 9: Effect of hidden state size of LSTM\ncan effectively leverage the language model to extract task-\nspeciﬁc knowledge to empower sequence labeling.\nAnalysis\nTo analyze the performance of LM-LSTM-CRF, we con-\nduct additional experiments on the CoNLL03 NER dataset.\nHidden State Size To explore the effect of model size,\nwe train our model with different hidden state sizes. For\ncomparison, we also apply the same hidden state sizes to\nLSTM-CRF and LSTM-CNN-CRF. From Table 9, one can\neasily observe that the F\n1 score of LM-LSTM-CRF k eeps\nincreasing when the hidden state size grows, while LSTM-\nCNN-CRF has a peak at state size200and LSTM-CRF has a\ndrop at state size200. This phenomenon further veriﬁed our\nintuition of employing the language model to extract knowl-\nedge and prevent overﬁtting.\nHighway Layers & Co-training To elucidate the effect\nof language model\n7 and highway units, we compare LM-\nLSTM-CRF with its two variants, LM-LSTM-CRF NL\nand LM-LSTM-CRF NH. The ﬁrst keeps highway units,\nbut optimizes JCRF alone; the second jointly optimizes\nJCRF and JLM, but without highway units. As shown\nin Table 10, LM-LSTM-CRF NH yields worse perfor-\nmance than LM-LSTM-CRF NL. This observation ac-\ncords with previous comparison between TagLM (index 10)\nand TagLM (index 11) on the CoNLL03 NER dataset. We\nconjecture that it is because the NER task and the language\nmodel is not strongly related to each other. In summary, our\nproposed co-training strategy is effective and introducing the\nhighway layers is necessary.\nRelated Work\nThere exist two threads of related work regarding the top-\nics in this paper, which are sequence labeling and how to\nimprove it with additional information.\nSequence Labeling. As one of the fundamental tasks in\nNLP , linguistic sequence labeling, including POS tagging,\nchunking, and NER, has been studied for years. Handcrafted\nfeatures were widely used in traditional methods like CRFs,\nHMMs, and maximum entropy classiﬁers (Lafferty, McCal-\nlum, and Pereira 2001; McCallum and Li 2003; Florian et\nal. 2003; Chieu and Ng 2002), but also make it hard to\napply them to new tasks or domains. Recently, getting rid\n7the perplexities of the forward language model on CoNLL03\nNER’s training / development / test sets are 52.87 / 55.03 / 50.22.\nState Size Model F1score±std Recall±std Precision±std\n300\nLM-LSTM-CRF 91.71±0.10 92.14±0.12 91.30±0.13\nLM-LSTM-CRF NL 91.43±0.09 91.85±0.18 91.01±0.19\nLM-LSTM-CRF NH 91.16±0.22 91.67±0.28 90.66±0.23\n200\nLM-LSTM-CRF 91.63±0.23 92.07±0.22 91.19±0.30\nLM-LSTM-CRF NL 91.44±0.10 91.95±0.16 90.94±0.16\nLM-LSTM-CRF NH 91.34±0.28 91.79±0.18 90.89±0.30\n100\nLM-LSTM-CRF 91.13±0.32 91.60±0.37 90.67±0.32\nLM-LSTM-CRF NL 91.17±0.11 91.72±0.14 90.61±0.21\nLM-LSTM-CRF NH 91.01±0.19 91.50±0.21 90.53±0.30\nTable 10: Effect of language model and highway\nof handcrafted features, there are attempts to build end-to-\nend systems for sequence labeling tasks, such as BiLSTM-\nCNN (Chiu and Nichols 2016), LSTM-CRF (Lample et\nal. 2016), and the current state-of-the-art method in NER\nand POS tagging tasks, LSTM-CNN-CRF (Ma and Hovy\n2016). These models all incorporate character-level struc-\nture, and report meaningful improvement over pure word-\nlevel model. Also, CRF layer has also been demonstrated to\nbe effective in capturing the dependency among labels. Our\nmodel is based on the success of LSTM-CRF model and is\nfurther modiﬁed to better capture the char-level information\nin a language model manner.\nLeveraging Additional Information. Integrating word-\nlevel and character-level knowledge has been proved to be\nhelpful to sequence labeling tasks. For example, word em-\nbeddings (Mikolov et al. 2013; Pennington, Socher, and\nManning 2014) can be utilized by co-training or pre-training\nstrategies (Liu et al. 2017; Lample et al. 2016). However,\nnone of these models utilizes the character-level knowledge.\nAlthough directly adopting character-level pre-trained lan-\nguage models could be helpful (Peters et al. 2017). Such pre-\ntrained knowledge is not task-speciﬁc and requires a larger\nneural network, external corpus, and longer training. Our\nmodel leverages both word-level and character-level knowl-\nedge through a co-training strategy, which leads to a concise,\neffective, and efﬁcient neural network. Besides, unlike other\nmulti-task learning methods, our model has no reliance on\nany extra annotation (Peters et al. 2017) or any knowledge\nbase (Shang et al. 2017). Instead, it extracts knowledge from\nthe self-contained order information.\nConclusion\nIn this paper, we proposed a sequence labeling framework,\nLM-LSTM-CRF, which effectively leverages the language\nmodel to extract character-level knowledge from the self-\ncontained order information. Highway layers are incorpo-\nrated to overcome the discordance issue of the naive co-\ntraining Beneﬁted from the effectively captured such task-\nspeciﬁc knowledge, we can build a much more concise\nmodel, thus yielding much better efﬁciency without loss of\neffectiveness (achieved the state-of-the-art on three bench-\nmark datasets) . In the future, we plan to further extract and\nincorporate knowledge from other “unsupervised” learning\nprinciples and empower more sequence labeling tasks.\nAcknowledgments\nWe thank Junliang Guo, Cheng Cheng and all reviewers for\ncomments on earlier drafts that led to substantial improve-\n5259\nments in the ﬁnal version. Research was sponsored in part\nby the U.S. Army Research Lab. under Cooperative Agree-\nment No. W911NF-09-2-0053 (NSCT A), National Science\nFoundation IIS 16-18481, IIS 17-04532, and IIS-17-41317,\ngrant 1U54GM114838 awarded by NIGMS through funds\nprovided by the trans-NIH Big Data to Knowledge (BD2K)\ninitiative (www.bd2k.nih.gov), and Google PhD Fellowship.\nThe views and conclusions contained in this document are\nthose of the author(s) and should not be interpreted as repre-\nsenting the ofﬁcial policies of the U.S. Army Research Lab-\noratory or the U.S. Government. The U.S. Government is au-\nthorized to reproduce and distribute reprints for Government\npurposes notwithstanding any copyright notation hereon.\nReferences\nChieu, H. L., and Ng, H. T. 2002. Named entity recognition:\nA maximum entropy approach using global information. In\nCOLING.\nChiu, J. P . C., and Nichols, E. 2016. Named entity recogni-\ntion with bidirectional lstm-cnns.TACL.\nCollobert, R.; Weston, J.; Bottou, L.; Karlen, M.;\nKavukcuoglu, K.; and Kuksa, P . P . 2011. Natural language\nprocessing (almost) from scratch.JMLR.\nFernandez, J.; Y u, Z.; and Downey, D. 2017. V ecshare: A\nframework for sharing word representation vectors.\nFlorian, R.; Ittycheriah, A.; Jing, H.; and Zhang, T. 2003.\nNamed entity recognition through classiﬁer combination. In\nCoNLL.\nGlorot, X., and Bengio, Y . 2010. Understanding the dif-\nﬁculty of training deep feedforward neural networks. In\nProceedings of the Thirteenth International Conference on\nArtiﬁcial Intelligence and Statistics.\nHashimoto, K.; Xiong, C.; Tsuruoka, Y .; and Socher, R.\n2016. A joint many-task model: Growing a neural network\nfor multiple nlp tasks.arXiv:1611.01587.\nJozefowicz, R.; Vinyals, O.; Schuster, M.; Shazeer, N.; and\nWu, Y . 2016. Exploring the limits of language modeling.\narXiv:1602.02410.\nJozefowicz, R.; Zaremba, W .; and Sutskever, I. 2015. An\nempirical exploration of recurrent network architectures. In\nICML.\nKarpathy, A. 2015. The unreasonable effectiveness of re-\ncurrent neural networks. http://karpathy.github.io/2015/05/\n21/rnn-effectiveness/. Accessed: 2017-08-22.\nLafferty, J. D.; McCallum, A.; and Pereira, F. 2001. Con-\nditional random ﬁelds: Probabilistic models for segmenting\nand labeling sequence data. InICML.\nLample, G.; Ballesteros, M.; Kawakami, K.; Subramanian,\nS.; and Dyer, C. 2016. Neural architectures for named entity\nrecognition. In NAACL-HLT.\nLiu, L.; Ren, X.; Zhu, Q.; Zhi, S.; Gui, H.; Ji, H.; and Han,\nJ. 2017. Heterogeneous Supervision for Relation Extraction:\nA Representation Learning Approach.Proc. EMNLP.\nLuo, G.; Huang, X.; Lin, C.-Y .; and Nie, Z. 2015. Joint\nnamed entity recognition and disambiguation. InEMNLP.\nMa, X., and Hovy, E. 2016. End-to-end sequence labeling\nvia bi-directional lstm-cnns-crf. InACL.\nManning, C. D. 2011. Part-of-speech tagging from 97% to\n100%: is it time for some linguistics? InInternational Con-\nference on Intelligent T ext Processing and Computational\nLinguistics. Springer.\nMarcus, M. P .; Marcinkiewicz, M. A.; and Santorini, B.\n1993. Building a large annotated corpus of english: The\npenn treebank. Computational linguistics.\nMcCallum, A., and Li, W . 2003. Early results for named\nentity recognition with conditional random ﬁelds, feature in-\nduction and web-enhanced lexicons. InCoNLL.\nMikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and\nDean, J. 2013. Distributed representations of words and\nphrases and their compositionality. InNIPS.\nPeng, N., and Dredze, M. 2016. Improving named entity\nrecognition for chinese social media with word segmenta-\ntion representation learning. InACL.\nPennington, J.; Socher, R.; and Manning, C. D. 2014. Glove:\nGlobal vectors for word representation. InEMNLP.\nPeters, M. E.; Ammar, W .; Bhagavatula, C.; and Power, R.\n2017. Semi-supervised sequence tagging with bidirectional\nlanguage models. arXiv:1705.00108.\nRatinov, L., and Roth, D. 2009. Design challenges and mis-\nconceptions in named entity recognition. InCoNLL.\nRei, M. 2017. Semi-supervised multitask learning for se-\nquence labeling. In ACL.\nReimers, N., and Gurevych, I. 2017. Reporting score\ndistributions makes a difference: Performance study of\nlstm-networks for sequence tagging. arXiv preprint\narXiv:1707.09861.\nSha, F., and Pereira, F. 2003. Shallow parsing with condi-\ntional random ﬁelds. InNAACL-HLT.\nShang, J.; Liu, J.; Jiang, M.; Ren, X.; V oss, C. R.; and Han, J.\n2017. Automated phrase mining from massive text corpora.\narXiv:1702.04457.\nSøgaard, A., and Goldberg, Y . 2016. Deep multi-task learn-\ning with low level tasks supervised at lower layers. InACL.\nSøgaard, A. 2011. Semisupervised condensed nearest neigh-\nbor for part-of-speech tagging. InNAACL-HLT.\nSrivastava, R. K.; Greff, K.; and Schmidhuber, J. 2015.\nHighway networks. arXiv:1505.00387.\nSun, X. 2014. Structure regularization for structured predic-\ntion. In NIPS.\nTjong Kim Sang, E. F., and Buchholz, S. 2000. Introduc-\ntion to the conll-2000 shared task: Chunking. In Learning\nlanguage in logic and CoNLL.\nTjong Kim Sang, E. F., and De Meulder, F. 2003. Introduc-\ntion to the conll-2003 shared task: Language-independent\nnamed entity recognition. In Natural language learning at\nNAACL-HLT.\nY ang, Z.; Salakhutdinov, R.; and Cohen, W . W . 2017. Trans-\nfer learning for sequence tagging with hierarchical recurrent\nnetworks. arXiv:1703.06345.\n5260",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8416756391525269
    },
    {
      "name": "Task (project management)",
      "score": 0.7365932464599609
    },
    {
      "name": "Sequence labeling",
      "score": 0.6790348887443542
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6630603075027466
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6114045977592468
    },
    {
      "name": "Natural language processing",
      "score": 0.598274827003479
    },
    {
      "name": "Character (mathematics)",
      "score": 0.5617283582687378
    },
    {
      "name": "Word (group theory)",
      "score": 0.5334508419036865
    },
    {
      "name": "Key (lock)",
      "score": 0.5159919261932373
    },
    {
      "name": "Sequence (biology)",
      "score": 0.5132789611816406
    },
    {
      "name": "Language model",
      "score": 0.5107071995735168
    },
    {
      "name": "Artificial neural network",
      "score": 0.5059258341789246
    },
    {
      "name": "Transfer of learning",
      "score": 0.47273388504981995
    },
    {
      "name": "Knowledge transfer",
      "score": 0.4727310240268707
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.41947704553604126
    },
    {
      "name": "Machine learning",
      "score": 0.40866780281066895
    },
    {
      "name": "Knowledge management",
      "score": 0.06866711378097534
    },
    {
      "name": "Linguistics",
      "score": 0.06375470757484436
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I157725225",
      "name": "University of Illinois Urbana-Champaign",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2800817003",
      "name": "Southern California University for Professional Studies",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I2252078561",
      "name": "Meta (Israel)",
      "country": "IL"
    }
  ]
}