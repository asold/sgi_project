{
    "title": "Automatic classification of ligneous leaf diseases via hierarchical vision transformer and transfer learning",
    "url": "https://openalex.org/W4390814098",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2110336981",
            "name": "Dian-Yuan Han",
            "affiliations": [
                "Weifang University"
            ]
        },
        {
            "id": "https://openalex.org/A2078739122",
            "name": "Chunhua GUO",
            "affiliations": [
                "Weifang University"
            ]
        },
        {
            "id": "https://openalex.org/A2110336981",
            "name": "Dian-Yuan Han",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2078739122",
            "name": "Chunhua GUO",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2899548089",
        "https://openalex.org/W3122351145",
        "https://openalex.org/W2999070562",
        "https://openalex.org/W6780226713",
        "https://openalex.org/W3211149346",
        "https://openalex.org/W3015562698",
        "https://openalex.org/W4289593677",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W4206618551",
        "https://openalex.org/W3198131804",
        "https://openalex.org/W6809912217",
        "https://openalex.org/W1990130820",
        "https://openalex.org/W2618530766",
        "https://openalex.org/W4304806827",
        "https://openalex.org/W2776705292",
        "https://openalex.org/W4281490755",
        "https://openalex.org/W6792155083",
        "https://openalex.org/W4224299147",
        "https://openalex.org/W6781794065",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W6756103980",
        "https://openalex.org/W4285745359",
        "https://openalex.org/W6807794776",
        "https://openalex.org/W2117539524",
        "https://openalex.org/W2548258044",
        "https://openalex.org/W6849271567",
        "https://openalex.org/W3034454251",
        "https://openalex.org/W6674914833",
        "https://openalex.org/W6795140394",
        "https://openalex.org/W4200252281",
        "https://openalex.org/W6966618176",
        "https://openalex.org/W3099570874",
        "https://openalex.org/W6810252205",
        "https://openalex.org/W4220976347",
        "https://openalex.org/W4281793257",
        "https://openalex.org/W6762226699",
        "https://openalex.org/W4200551431",
        "https://openalex.org/W2765407302",
        "https://openalex.org/W2097117768",
        "https://openalex.org/W2899968596",
        "https://openalex.org/W146900863",
        "https://openalex.org/W2187089797",
        "https://openalex.org/W4210434556",
        "https://openalex.org/W4220703736",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2992308087",
        "https://openalex.org/W4224238822",
        "https://openalex.org/W4316658481"
    ],
    "abstract": "Background Identification of leaf diseases plays an important role in the growing process of different types of plants. Current studies focusing on the detection and categorization of leaf diseases have achieved promising outcomes. However, there is still a need to enhance the performance of leaf disease categorization for practical applications within the field of Precision Agriculture. Methods To bridge this gap, this study presents a novel approach to classifying leaf diseases in ligneous plants by offering an improved vision transformer model. The proposed approach involves utilizing a multi-head attention module to effectively capture contextual information about the images and their classes. In addition, the multi-layer perceptron module has also been employed. To train the proposed deep model, a public dataset of leaf disease is exploited, which consists of 22 distinct kinds of images depicting ligneous leaf diseases. Furthermore, the strategy of transfer learning is employed to decrease the training duration of the proposed model. Results The experimental findings indicate that the presented approach for classifying ligneous leaf diseases can achieve an accuracy of 85.0% above. Discussion In summary, the proposed methodology has the potential to serve as a beneficial algorithm for automated detection of leaf diseases in ligneous plants.",
    "full_text": "Automatic classiﬁcation of\nligneous leaf diseases via\nhierarchical vision transformer\nand transfer learning\nDianyuan Han and ChunhuaGuo*\nMedia and Communications College of Weifang University, Weifang, Shandong, China\nBackground: Identiﬁcation of leaf diseases plays an important role in the growing\nprocess of different types of plants. Current studies focusing on the detection and\ncategorization of leaf diseases have achieved promising outcomes. However, there\nis still a need to enhance the performance of leaf disease categorization for\npractical applications within theﬁeld of Precision Agriculture.\nMethods: To bridge this gap, this study presents a novel approach to classifying\nleaf diseases in ligneous plants by offering an improved vision transformer model.\nThe proposed approach involves utilizing a multi-head attention module to\neffectively capture contextual information about the images and their classes.\nIn addition, the multi-layer perceptron module has also been employed. To train\nthe proposed deep model, a public dataset of leaf disease is exploited, which\nconsists of 22 distinct kinds of images depicting ligneous leaf diseases.\nFurthermore, the strategy of transfer learning is employed to decrease the\ntraining duration of the proposed model.\nResults: The experimental ﬁndings indicate that the presented approach for\nclassifying ligneous leaf diseases can achieve an accuracy of 85.0% above.\nDiscussion: In summary, the proposed methodology has the potential to serve as\na beneﬁcial algorithm for automated detection of leaf diseases in ligneous plants.\nKEYWORDS\nprecision agriculture, transformer, neural networks, machine vision, transfer learning\n1 Introduction\nThe occurrence of leaf diseases in plants holds signi ﬁcant relevance in the ﬁeld of plant\npathology. Severe leaf disease can have detrimental effects on plants, including leaf drying\nand hindered bud formation. It can weaken the health of the plant and worsen the\nsusceptibility to other diseases Kai et al. (2011) ; Bo et al. (2019) ; Xu et al. (2020) ; Wang et al.\n(2021; 2022). In addition, the occurrence of fruit leaf disease can lead to a decline in both\nFrontiers inPlant Science frontiersin.org01\nOPEN ACCESS\nEDITED BY\nJian Lian,\nShandong Management University, China\nREVIEWED BY\nWei Meng,\nBeijing Forestry University, China\nYuwan Gu,\nChangzhou University, China\nXiaoyang Liu,\nHuaiyin Institute of Technology, China\n*CORRESPONDENCE\nChunhua Guo\ngch58@163.com\nRECEIVED 27 October 2023\nACCEPTED 15 December 2023\nPUBLISHED 12 January 2024\nCITATION\nHan D andGuo C (2024) Automatic\nclassiﬁcation of ligneous leaf diseases\nvia hierarchical vision transformer\nand transfer learning.\nFront. Plant Sci.14:1328952.\ndoi: 10.3389/fpls.2023.1328952\nCOPYRIGHT\n© 2024 Han and Guo. This is an open-access\narticle distributed under the terms of the\nCreative Commons Attribution License (CC BY).\nThe use, distribution or reproduction in other\nforums is permitted, provided the original\nauthor(s) and the copyright owner(s) are\ncredited and that the original publication in\nthis journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted\nwhich does not comply with these terms.\nTYPE Original Research\nPUBLISHED 12 January 2024\nDOI 10.3389/fpls.2023.1328952\nthe quantity and quality of fruits, as well as increase the vulnerability\nof nearby plants to infection. Given the strong reliance of the\neconomy on agricultural productivity, the impact of the leaf disease\non the environment becomes particularly signi ﬁcant if preventive\nmeasures are not implemented in a timely manner. Therefore, the\nprompt identi ﬁcation of diseases affecting fruit leaves is crucial for\nhuman well-being Patil et al. (2017) ; Afzaal et al. (2021) ; Mahum\net al. (2022) . In general, the identi ﬁcation and categorization of leaf\ndiseases have predominantly depended on the human visual system,\nwhich is error prone, is time-consuming and labor-intensive.\nHence, the implementation o f automated leaf disease\nclassiﬁcation is imperative in the context of fruit production for\nmitigating both the production and economic losses Sneha and\nBagal (2019) ; JencyRubia and BabithaLincy (2021) ; Y et al. (2022) .\nIn recent decades, there has been a signi ﬁcant surge in the\nutilization of machine learning-based algorithms for addressing leaf\ndisease categorization problems. Numerous machine vision\nalgorithms have been proposed to classify illnesses affecting plant\nleaves. In the study conducted by Singh and Misra (2017) , the\nauthors proposed an image segmentation method for the automatic\nidentiﬁcation and classi ﬁcation of plant leaf diseases, speci ﬁcally\nfocusing on the minor leaf disease common in pine trees within the\nUnited States. The researchers investigated the utilization of several\nclassiﬁer algorithms for the purpose of identifying plant leaf disease.\nA system for automatic detection of plant disease using image\nprocessing techniques was proposed by the authors Mounika and\nBharathi (2020) . The approach was used for calculation of textural\ndata pertaining to illnesses affecting plant leaves. In their work,\nKulkarni and Sapariya (2021) proposed a method to automatically\ndetect and classify leaf illnesses, which encompasses many stages,\nincluding image gathering, image pre-processing, segmentation,\nand classi ﬁcation. In their study, Reddy et al. (2021) employed\nSupport Vector Machine (SVM) and Random Forest algorithms for\nthe purpose of detecting illnesses in leaves. This study compared\nassessment measures, such as Root Mean Square Error (RMSE),\nPeak Signal Noise Ratio (PSNR), for the diseaseaffected regions of\nthe leaves to assess their potential impact on agricultural output.\nIn recent years, deep learning has gained signi ﬁcant interest due\nto its remarkable achievements in many domains, such as natural\nlanguage processing (NLP) and machine vision. Consequently, there\nhave been additional advancements in the ﬁeld of plant leaf disease\ncategorization by the utilization of deep learning models. Liu et al.\n(2017) introduced a methodology for detecting apple leaf diseases\nutilizing deep convolutional neural networks (CNNs). The model\nreported in this study is capable of generating an ample number of\ndiseased images with a deep learning model, AlexNet. The study\nutilized a dataset including 13,689 images depicting various apple leaf\nillnesses. The CNN model developed in this research was trained to\naccurately classify four types of apple leaf diseases. In the study\nconducted by Anagnostis et al. (2020) , a resilient CNN model was\ndeveloped to address the timely identi ﬁcation of anthracnose, a\nprevalent fungal disease that affects numerous tree species globally.\nThis model was to use to classify images of plant leaves as either\ninfected or uninfected by anthracnose. The researchers acquired a\ndataset consisting of grayscale and RGB images. Then, they utilized a\nrapid Fourier transform to extract characteristics from the images.\nFinally, to implement the classi ﬁcation task, they employed a CNN\nmodel. To effectively identify olive leaf disease, Ksibi et al. (2022)\nproposed the utilization of ResNet50 and MobileNet models for\nimage feature extraction, emplo ying the technique of feature\nconcatenation. To train the deep learning models employed in this\ninvestigation, a dataset including 5,400 images of olive leaves was\nutilized. These images were acquired from an olive grove using an\nunmanned aerial vehicle (UAV) equipped with a camera. In their\nstudy, Devi et al. (2022) proposed a methodology for the prediction\nand classiﬁcation of corn leaf disease. The authors employed transfer\nlearning and the Alexnet model, leveraging the Adaptive Moment\nEstimation (ADAM) optimizer and the Stochastic Gradient Descent\nwith momentum (SGDM) mechanism. The model was trained and\nevaluated using a dataset consisting of 5,300 images, which were\ncategorized into four different types: healthy, blight, common rust,\nand gray leaf spot. Yao et al. (2022) conducted a study focusing on the\nidentiﬁcation of kiwifruit leaf disease. They developed a publicly\navailable dataset while using the YOLOX target detection algorithm\nto mitigate the in ﬂuence of environmental elements. The study of Yu\net al. (2022) introduced a method for ef ﬁciently detecting soybean\nillnesses. It leverages a residual attention network (RANet) model.\nThis study included the incorporation of three types of soybean leaf\nspot diseases, namely soybean brown leaf spot, soybean frog eye leaf\nspot, and soybean phyllosticta leaf spot, into the dataset. The OTSU\nalgorithm was utilized to pre-process the initial images for\neliminating the surrounding features. Additionally, the image\ndataset was augmented by the application of image enhancement\nalgorithms. Additionally, the residual attention layer was constructed\nby integrating attention processes into a ResNet18 model.\nThe majority of the preceding approaches in the ﬁeld of leaf\ndisease classi ﬁcation have predominantly employed CNN\narchitectures. Regrettably, the CNN-based models have\nlimitations due to the local receptive ﬁeld inside the convolutional\nmodules. This characteristic directs attention towards the\nsurrounding region in an image, perhaps overlooking the\nconnections between distant pixels. In contrast, the transformer is\nrenowned for its utilization of an attention mechanism to effectively\ncapture and represent the extensive inter-dependencies within the\ndata samples. The successful performance of transformer in NLP\ntasks has resulted in its integration and use in the ﬁeld of computer\nvision Liu et al. (2021) . For instance, the work conducted by Qian\net al. (2022) introduced a novel strategy for classifying maize leaf\ndiseases using a vision transformer-based method. The authors of\nthe study also gathered RGB ima ges from publicly available\ndatabases and experimental ﬁelds, classifying them into four\ndistinct categories: southern corn leaf blight, gray leaf spot,\nsouthern corn rust, and healthy s pecimens. Nevertheless, the\nvision transformer model proposed in this study might provide\nchallenges when used to high-resolution images due to the\nquadratic computational complexity of the self-attention\nmechanism in relation to image resolution. Furthermore, the\noriginal vision transformer necessitates a substantial allocation of\nmemory capacity and processing resources.\nTaking the aforementioned research into consideration, we\npropose a hierarchical vision transformerbased approach by\nemploying transfer learning strategy, for classifying leaf diseases of\nHan and Guo 10.3389/fpls.2023.1328952\nFrontiers inPlant Science frontiersin.org02\nligneous plants. The hierarchical design in the proposed vision\ntransformer yields notable reductions in computational resource\nrequirements and the number of weighting parameters for the\nvision transformer. Furthermore, this work utilizes the weighting\nfactors that were pre-trained on the dataset ImageNet Russakovsky\net al. (2014). To assess the effectiveness of the suggested methodology,\na subset of a publicly available dataset was utilized. This subset\ncomprises a total of 22 types of ligneous leaf images. Furthermore, a\nseries of comparative tests were carried out to evaluate the\nperformance of the suggested methodology as well as the state-of-\nthe-art methods. The experimental ﬁndings provide evidence that the\nsuggested methodology outperforms the state-of-the-art techniques\nin terms of accuracy, precision, recall, and, F1 score.\nIn general, the contributions of this study include:\n A leaf disease classi ﬁcation pipeline is proposed. The\nproposed model primarily consists of a hierarchical\nvision transformer.\n The presented vision transformer model comprises of two\nchannels, which are used to extract the features from the\noriginal leaf images and the edges in the corresponding\nimages, respectively.\n The experimental ﬁndings prove the superiority of the\nproposed methodology over the state-of-the-art algorithms.\nThe subsequent sections of this article are structured in the\nfollowing manner. Section 2 presents an elaborate exposition of the\nsuggested transformer concept. Section 3 provides a detailed\naccount of the experimental methodology employed in this study,\nas well as the subsequent ﬁndings and their analysis. Finally, The\nstudy concludes at Section 4.\n2 Methodology\n2.1 Dataset collection and pre-processing\nThe dataset utilized in this research is sourced from the publicly\naccessible plant dataset of AI Challenger 2018 Wu et al. (2017) ,\nwhich has a total of 10 plant specimens, each classi ﬁed into one of\n27 categories representing either leaf diseases or healthy conditions.\nIn a systematic manner, a total of 61 image classes have been\ncategorized into distinct groups based on species, pest species, and\nseverity levels. The objective of this work is to categorize diseases\naffecting ligneous fruit leaves. Therefore, only the leaves that were\naffected by diseases were selected from the dataset for the purposes\nof training and validation. In this study, a total of 22 categories of\nimages depicting leaf diseases were included in the dataset. These\ncategories encompassed both sick leaves and healthy leaves.\nAs seen in Figure 1 , the training set comprises 11,603 images,\nwhereas the testing set consists of 1,668 images. These images are\ncategorized into 22 distinct classes. Furthermore, the dataset\nincludes a collection of example images, as seen in Figure 1 .\nThese images encompass both healthy and sick leaves.\nIn this study, the utilization of transfer learning is employed to\nimprove the performance of the proposed approach, taking\ninspiration from the work of Chen et al. (2020) . To achieve this,\nthe proposed model is initially trained on the ImageNet dataset\nRussakovsky et al. (2014) , considering the relatively small size of the\npresented image dataset. In addition, the images are resized into a\nuniform dimension of 224×224 to minimize the computing\nresources needed during the training phase. Moreover, the\npresent study employs a set of data augmentation techniques to\nincrease the number of image samples, which can further enhance\nthe generalization of the proposed model and mitigate the risk of\nover-ﬁtting during the training process. These techniques include\nRandomFlip, Color Jitter, Cutmix Yun et al. (2019) , and Mixup\nZhang et al. (2017) .\n2.2 Overall framework\nThe proposed vision transformer model is provided in Figure 2,\nwhich is a typical two-channel swin vision transformer Liu et al.\n(2021) model, and there is no weighting parameter sharing between\nthese two channels.\nAs seen in Figure 3 , the input of the lower channel is achieved\nby the utilization of the Sobel operator Liu and Wang (2022) and\nthe continuous image fusion operation. The edge Sobel operator is\nemployed on the original image in order to provide input for the\nsuggested methodology. Initially, the gray-scale equivalent is\nderived from each original ima ge. Next, the original image\nundergoes convolution with the Sobel operators of size 3×3 in\nboth the horizontal and vertical axes. The speci ﬁc characteristics of\nthe horizontal and vertical Sobel operators, denoted as Gx and Gy\nrespectively, are outlined below in Equations 1 and 2.\nGx =\n+1 0 −1\n+2 0 −2\n+1 0 −1\n2\n66\n4\n3\n77\n5 /C2I, (1)\nG\ny =\n+1 +2 +1\n000\n−1 −2 −1\n2\n66\n4\n3\n77\n5 /C2I, (2)\nwhere the original image is taken as I, and let G\nx be equal to the\ntranspose of Gy. It is worth noting that the elements in the operators\nGx and Gy are differentiable. The starting values of the convolutional\nlayer, also known as the Sobel operator layer, are determined by the\nelements in the Gx and Gy operators. These values may be optimized\nby a back-propagation approach during the training phase of the\nproposed transformer. In addition to combining the output of these\ntwo channels through concatenation, the classi ﬁcation process\ninvolves the utilization of a softmax classi ﬁer, an average pooling\nlayer, and a fully-connected layer.\n2.2.1 Details of the backbone\nAs seen in Figure 2, the con ﬁguration of blocks in each channel\nand the size of tokens may be adjusted to accommodate diverse\nscales of machine vision applications. In accordance with the\npresent investigation, the quantity of blocks in each channel is\nHan and Guo 10.3389/fpls.2023.1328952\nFrontiers inPlant Science frontiersin.org03\nmultiplied by a factor of 2, 2, 6, and 2, respectively. Following the\ninput technique, the input image is initially partitioned into non-\noverlapping patches of size 4×4. Hence, the feature dimension of a\nsingle patch may be calculated as the product of its width, height,\nand number of color channels, resulting in a value of 48 (where 3\nrepresents the number of RGB channels). In a manner akin to the\nvision transformer proposed by Dosovitskiy et al. (2020) ,t h e\napproach involves treating each patch as a token, where the\nfeature representation of a token is obtained by concatenating the\npixel values inside the associated patch. Different from the original\nvision transformer, the proposed transformer model leverages the\nswin transformer block and the shift-window self-\nattention mechanism.\nIn the initial stage, a linear embedding layer is employed to\nproject the original feature into a dimension of arbitrary size (C=96\nin the context of this work). Next, a series of swin transformer\nFIGURE 1\nA collection of sample images depicting various types of leaf diseases. The leaves in the top row exhibit signs of good health. The leaves exhibiting\nsigns of illness are seen in the bottom row.\nFIGURE 2\nThe suggested model consists of a two-channel swin vision transformer, which exhibits a certain overall structure. The top channel of the proposed\nmodel receives an initial image as its input, while the lower channel gets the edge information of the original image as its input. It is worth noting\nthat the value of C, which is equal to 96, might vary depending on the architecture of the model.\nHan and Guo 10.3389/fpls.2023.1328952\nFrontiers inPlant Science frontiersin.org04\nblocks are utilized on the tokens, incorporating two distinct forms\nof self-attention modules. Furthermore, it should be noted that the\nnumber of tokens in the swin transformer blocks stays consistent\nwith the linear embedding unit, which is calculated as H\n4 /C2W\n4 .\nThe hierarchical representation is generated by the provided\nmodel through the utilization of patch merging modules, which\neffectively down-sample the feature resolutions by a factor of 2. The\nﬁrst merger module and feature modi ﬁcation are denoted as Stage 2,\nwhich are then repeated as Stage 3 and Stage 4. Furthermore, the\ndimensions of the output features progress from Stage 1 to Stage 4\nas\nH\n4 /C2H\n4 /C2C, H\n8 /C2H\n8 /C2C,  H\n16 /C2H\n16 /C2C,a n d H\n32 /C2H\n32 /C2C,\nrespectively. The hierarchical representation is primarily\ndistinguished between the swin vision transformer Liu et al.\n(2021) and the original vision transformer Dosovitskiy et al.\n(2020) by the inclusion of Stage 2, Stage 3, and Stage 3 together.\nThe given methodology does not include the utilization of any class\ntaken. In this approach, the output vector of dimensions N =\nH\n32 /C2W\n32 is generated by using global average pooling followed by a\nfully-connected layer. The linear classi ﬁer then takes into account\nthe ﬁrst C components of this output vector.\n2.2.2 Swin transformer block\nEach stage of the proposed model consists of the swin\ntransformer blocks. And each swin transformer block consists of\nconsecutive modules, as shown in Figure 4 . In this architecture,\nthere are two important modules W-MSA and SW-MSA, which\nrepresent the multi-head self-attention (MSA) with a standard\nwindow and the MSA with a shifted window, respectively.\nThe mathematical representation of the consecutive swing\ntransformer modules can be articulated in Equations 3 –6:\nbz\nl = W − MSA(LN(zl−1)) + zl−1, (3)\nzl = MLP(LN(bz l)) + bz , (4)\n^z l+1 = SW − MSA(LN(zl)) + zl, (5)\nzl+1 = MLP(LN(^z l+1)) + ^z l+1, (6)\nwhere the notation W-MSA refers to window-based MSA, MLP\nstands for multiple layer perception Tolstikhin et al. (2021) , SW-\nMSA represents shifted-window MSA, and LN signi ﬁes layer\nnormalization Ba et al. (2016) .\n2.2.3 Shifted window-based self-\nattention mechanism\nIn contrast to the initial vision transformer that heavily relies on\nglobal self-attention, which necessitates calculating the relationships\nbetween a token and all other tokens, the window-based MSA\nmodule employs a window of size M × M (with a default value of\nM=7) to restrict the extent of calculation. Hence, the computational\ncomplexity becomes more manageable with the incorporation of\nthe window-based self-attention mechanism, as opposed to the\nquadratic complexity of the vision transformer Dosovitskiy et al.\n(2020), which is dependent on the image resolution h × w (as shown\nin Equations 7 , 8).\nΩ(MSA)  =  4 hwC2+2 ( hw)2C, (7)\nΩ(W − MSA)  =  4 hwC2 +2 M2hwC, (8)\nwhere h and w denote the height and width of an image, C=96,\nand M=7 in the following settings.\nFurthermore, the SW-MSA strat egy is intended to enhance the\nencoding of global relationships among the pixels in multiple windows.\nThe use of the relationship across many windows may be maximized\nwith the introduction of SW-MSA. As seen inFigure 5, the partitioning\nmethod of the regular window is employed in layer l, where self-\nattention is computed within each window. In the subsequent layer,\ndenoted as l + 1, the partitioning of the window is adjusted both\nFIGURE 3\nThe formation of the input for the bottom channel of the proposed vision transformer model.\nHan and Guo 10.3389/fpls.2023.1328952\nFrontiers inPlant Science frontiersin.org05\nhorizontally and vertically, resulting in the creation of a greater number\nof distinct windows. Thus, the self-attention calculation in Layer l+1\ntraverses the initial windows in Layer l.\nIt should be noted that the loss function employed in the\nproposed model is a cross-entropy loss. This loss is computed by\ncomparing the ground truth category of the image with the\nclassiﬁcation output given by the suggested model, as seen in Figure 2.\n3 Experiments\n3.1 Implementation details\nThe tests were done employing four NVIDIA RTX 3080 GPUs, the\nPyTorch deep learning framework Paszke et al. (2019) version 2.0.1,\nand the Python programming language version 3.8.3. The backbone of\nthe suggested model consists of the Swin-T vision transformer, which is\nemployed for each channel. The di mensions of the input images are\nstandardized to 224×224. Further more, the suggested swin vision\ntransformer was initialized usi ng the pre-trained weighting\nparameters of ImageNet Russakovsky et al. (2014) .T y p i c a l l y ,t h e\nhyper-parameters employed in the experiments encompass the\nsubsequent elements, as shown in Table 1 To note that the\nexperiments by using the proposed approach were conducted in a\n10-fold cross-validation scheme. M eanwhile, the hyper-parameters\nwere determined by using a tr ial-and-error strategy.\nIn order to assess the effectiveness of the suggested model and\nthe comparison methodologies, the experiments contained several\nassessment measures, including accuracy, precision, recall, and F1\nscore (as shown in Equations 9 –12).\nFIGURE 5\nThe diagram depicting the SW-MSA mechanism employed in the proposed methodology. The red boxes are used to indicate the local window,\nwhich serves the purpose of constraining the scope of self-attention calculation.\nFIGURE 4\nThe detailed components inside the Swin Transformer model. The abbreviation LN is used to refer to layer normalization. The normal and shifted-windows\nmulti-head self-attention modules are denoted as W-MSA and SW-MSA, respectively. The acronym MLP stands for multiple-layer perception. .\nHan and Guo 10.3389/fpls.2023.1328952\nFrontiers inPlant Science frontiersin.org06\nAccuracy = (TP + TN)\n(TP + TN + FP + FN) , (9)\nPrecision = TP\nTP + FP , (10)\nRecall = TP\nTP + FN , (11)\nF1= 2 /C2Precision /C2Recall\nPrecision + Recall, (12)\nwhere TP, TN, FP, and FN denote number of true positive, true\nnegative, false positive, and false negative, respectively.\n3.2 Ablation study\nThe proposed vision transformer model incorporates two\ndistinct topologies for swin vision transformers. To evaluate the\nefﬁcacy of the introduced swin vision transformer, a series of\nablation experiments were conducted on a publicly available\ndataset. These experiments involved varying the settings of the\nintroduced models, which were used to replace the original settings\nof the proposed approach. The original approach consisted of a\nvision transformer Dosovitskiy et al. (2020) and the Sobel operator\nwith ﬁxed 3×3 values.\nAs seen in Figure 6, it is evident that the accuracy of the suggested\nmethodology surpasses that of the model utilizing the original vision\ntransformer or the ﬁxed Sobel operator. The transformer model\nunder consideration has demonstrated a performance improvement\nof 2.2% and 1.4% compared to the vision transformer version and the\nﬁxed Sobel operator version, respectively, when evaluated on a subset\ncomprising 25% of the utilized dataset. Furthermore, the transformer\nmodel under consideration has demonstrated a performance\nimprovement of 2.22% and 1.40% compared to the vision\ntransformer version and the ﬁxed Sobel operator version,\nrespectively, when evaluated o n 50% of the identical dataset.\nHence, the selected model was deemed suitable as the foundational\nframework for the subsequent investigations.\n3.3 Experimental results\nTo evaluate the performance of the proposed approach in a fair\nmanner, the comparison experiments were conducted between the\nstate-of-the-art methods, including, and ours on the same dataset as\nprovided in Table 2 .\nIn order to objectively assess the performance of the proposed\napproach, a series of comparative experiments were conducted.\nThese experiments involved benchmarking the proposed approach\nagainst several state-of-the-art methods, namely AlexNet\nKrizhevsky et al. (2012) , GoogleNet Szegedy et al. (2014) , VGG\nAbas et al. (2018) , ResNet101 Zhang (2021) ,E f ﬁcientNetB3 Singh\net al. (2022) , Inception V3 Jenipher and Radhika (2022) , MobileNet\nV2140 Elfatimi et al. (2022) , and vision transformer Dosovitskiy\net al. (2020) . In the experiments, these state-of-the-art methods\nadopted their original settings in the literature. To note that the\nformer seven state-of-the-art algorithms are CNN models. And the\nproposed approach was inspired by the work of the last model\nvision transformer. Meanwhile, the evaluation was carried out on\nthe dataset speci ﬁed in Table 2 .\nAs seen in Table 3 , the suggested strategy exhibits superior\naccuracy, precision, recall, and F1 score compared to existing state-\nof-the-art approaches. To provide speci ﬁc results, our method\ndemonstrates an increase in overall accuracy of 2.1% when\ncompared to MobileNet V2140. Additionally, our proposed\napproach exhibits improvements in Precision, Recall, and F1 score\nFIGURE 6\nAblation study with different settings with ratios (25% and 50%) of the training set in the publicly available dataset.\nHan and Guo 10.3389/fpls.2023.1328952\nFrontiers inPlant Science frontiersin.org07\nby 2.6%, 2.7%, and 2.7% respectively, when compared to MobileNet\nV2140. Furthermore, even when compared to the original vision\ntransformer, our approach showcases enhancements in accuracy,\nPrecision, Recall, and F1 score by 1.1%, 1.5%, 0.59%, and 1.1%\nrespectively. In summary, the suggested methodology demonstrates\nhigher performance compared to both CNN-based and vision\ntransformer-based algorithms. This provides evidence of the\nprospective capability of the proposed technique in feature extraction.\nIn order to assess the effectiveness of the suggested methodology\non various image categories within the leveraging dataset, we have\nincluded the accuracy-based confusion matrix (as seen in Figure 7 )\nfor the proposed technique. This matrix pertains to the 22\ncategories of leaf disease images inside the public dataset. The\nmajority of the categories have demonstrated encouraging\noutcomes. The leaf disease images that exhibit inadequate\nclassiﬁcation pertain to the plant species “Apple” and “Citrus.”\nThe category labeled as “Citrus healthy” can sometimes be mistaken\nwith the category known as “Citrus Greening June general. ” The\nattribution of the resemblance between various forms of leaf\ndiseases is warranted. Another challenging classi ﬁcation\nassignment involves distinguishing between “Apple_Scab general ”\nand “Apple_Scab serious. ” This phenomenon may be ascribed to\nthe existence of two distinct variants of an image falling under the\noverarching classi ﬁcation of “Apple_Scab.”\nIn addition, the T-distributed stochastic neighbor embedding (t-\nSNE) was implemented using the suggested methodology, as seen in\nFigure 8, van der Maaten and Hinton (2008) . It should be noted that\nt-SNE is a computational approach employed for the purpose of\nvisualizing the multidimensional feature space of the 22 categories of\nsick leaves in a two-dimensional (2D) format. Figure 8 presents a\nsummary of the t-SNE clustering outcomes for both the output\nproduced by the suggested technique and the ground truth. Figure 8\nexhibits a notable clustering pattern as classes 16 and 17 are closely\npacked together on the right side. It should be noted that the distinct\nattributes of these leaf images can only be ascribed to a limited\nnumber of locations that are outside the clusters.\n3.4 Discussion\nThe utilization of CNN models in deep learning has become\nprevalent. These models possess the capability to extract feature\nmaps from images. Furthermore, the effectiveness of feature\nextraction may be enhanced by employing a network structure\nwith increased depth. Nevertheless, the ef ﬁcacy of CNNs may be\nlimited due to the inherent constraint of the convolutional module,\nwhich primarily emphasizes the analysis of small receptive ﬁelds\ninside the images. This phenomenon rapidly results in the disregard\nof the interconnections among distant pixels within an image. In\naddition, the process of enhancing the performance of deeper\nconvolutional neural network models necessitates a greater\nallocation of processing resources.\nTABLE 2 Distribution of the images in the dataset of this study.\nClass Label Name No. of train-\ning images\nNo. of\ntesting\nimages\n1 Apple healthy 1,185 169\n2 Apple_Scab general 211 30\n3 Apple_Scab serious 152 22\n4 Apple Frogeye Spot 427 61\n5 Cedar Apple\nRust general\n142 20\n6 Cedar Apple\nRust serious\n40 6\n7 Cherry healthy 598 85\n8 Cherry_Powdery\nMildew general\n116 12\n9 Cherry_Powdery\nMildew serious\n110 18\n10 Grape healthy 294 42\n11 Grape Black Rot\nFungus general\n381 54\n12 Grape Black Rot\nFungus serious\n462 66\n13 Grape Black Measles\nFungus general\n503 74\n14 Grape Black Measles\nFungus serious\n419 59\n15 Grape Leaf Blight\nFungus general\n61 9\n16 Grape Leaf Blight\nFungus serious\n630 90\n17 Citrus healthy 367 52\n18 Citrus Greening\nJune general\n1,828 269\n19 Citrus Greening\nJune serious\n1,799 262\n20 Peach healthy 251 36\n21 Peach_Bacterial\nSpot general\n857 122\n22 Peach_Bacterial\nSpot serious\n770 110\n– Total 11,603 1,668\nTABLE 1 Hyper-parameters used in the experiments.\nItem Value\nBatch_size 8\noptimizer Adam\nlearning rate 1e-4\ndepth 12\nepochs 100\nHan and Guo 10.3389/fpls.2023.1328952\nFrontiers inPlant Science frontiersin.org08\nIn the context of leaf disease images, it is observed that the\naffected regions are frequently dispersed over the whole image,\nrather than being con ﬁned to a speci ﬁc localized location. This\ncharacteristic is exempli ﬁed in Figure 9 . Given the limitations of\nthe local receptive ﬁeld mechanism in addressing the speci ﬁc leaf\ndisease image, the mere addition of extra layers to the CNN\nmodels does not always ensure improved performance in image\nclassiﬁcation. This study presents the introduction of a vision\ntransformer-based model for image classi ﬁcation, which leverages\nthe relationships among distant pixels inside the images. The\nsuggested dual channel model employs the technique of MSA to\ncontinually extract the correlation between image patches. This\napproach effectively preserves the information that is\nadvantageous for classi ﬁcation purposes. In contrast to the\noriginal vision transformer model, the swin vision transformer\nmodel is capable of extracting valuable information from images\nwhile concurrently mitigating its computing resource\nrequirements. Nevertheless, this research endeavor is subject to\nmany constraints: The dataset utilized in the experiments suffers\nfrom unbalanced image samples, hence limiting the effectiveness\nTABLE 3 Comparison results between the state-of-the-arts and the proposed method.\nMethod Accuracy Precision Recall F1 score\nAlexNet 78.51 77.63 80.16 78.87\nGoogleNet 81.23 80.59 82.05 81.31\nVGG 82.35 82.19 82.94 82.56\nResNet101 83.18 82.56 83.29 82.92\nEfﬁcientNetB3 83.25 83.03 83.48 83.25\nInception V3 84.01 83.23 84.33 83.78\nMobileNet V2140 84.69 83.52 84.92 84.21\nVision Transformer 85.47 84.38 86.71 85.53\nOur method 86.43 85.73 87.22 86.47\nBold values denote the best performance.\nFIGURE 7\nThe confusion matrix of the proposed approach on the presented dataset.\nHan and Guo 10.3389/fpls.2023.1328952\nFrontiers inPlant Science frontiersin.org09\nof the presented method. Meanwhile, number of the image\nsamples contained in the leveraged dataset is still limited, which\nconstrains the accuracy of the proposed approach at relatively low\nlevel. In addition, there exists duplication between the edge\ninformation included in the lower channel of the proposed\nmodel and the upper channel.\n4 Conclusion\nThe present study introduces a novel network architecture for\nleaf disease image classi ﬁcation, utilizing a two-channel swin\ntransformer-based approach. The system consists of a dedicated\nchannel for the original image and an additional channel speci ﬁcally\nintended to capture the edges in the merged image. In addition, the\nSobel operator has been utilized to extract the edge information\nfrom the images of leaf diseases. The utilization of the two-channel\nswin vision transformer model has resulted in the attainment of\nimproved performance compared to the current state-of-the-art\nmethods. The ef ﬁcacy of the suggested model is demonstrated by\nexperimental ﬁndings conducted on the publically accessible\ndataset. The experimental results of the proposed approach have\nproved the superior performance of the proposed approach in leaf\ndisease classi ﬁcation. It can be concluded that the proposed\napproach could be a valuable algorithm for leaf classi ﬁcation and\nPrecision Agriculture.\nRecently, there has been encouraging performance\ndemonstrated by vision transformer-based models in challenges\nFIGURE 8\nThe outcome of performing t-SNE on outcome generated from the proposed approach (Top) and on the ground truth samples (Bottom).\nHan and Guo 10.3389/fpls.2023.1328952\nFrontiers inPlant Science frontiersin.org10\nrelated to multi-modal machine vision. Henceforth, we shall further\nexplore the intricacies of multi-model-based deep learning models\nin the context of leaf disease categorization and prediction. In\naddition, more samples need to be collected to eliminate the class\nimbalance issue in the dataset used in this study.\nData availability statement\nPublicly available datasets were analyzed in this study. The\ndatasets for this study can be found in the AI challenger 2018 at\nhttps://aistudio.baidu.com/datasetdetail/76075.\nAuthor contributions\nDH: Conceptualization, Data curation, Formal Analysis,\nInvestigation, Visualization, Writing – original draft. CG:\nFunding acquisition, Methodology, Project administration,\nSupervision, Validation, Writing – original draft.\nFunding\nThe author(s) declare that no ﬁnancial support was received for\nthe research, authorship, and/or publication of this article.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential con ﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their af ﬁliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nAbas, M. A. H., Ismail, N., Yassin, A. I. M., and Taib, M. N. (2018). Vgg16 for plant\nimage classiﬁcation with transfer learning and data augmentation. Int. J. Eng. Technol.7\n(4), 90 –94. doi: 10.14419/IJET.V7I4.11.20781\nAfzaal, H., Farooque, A. A., Schumann, A. W., Hussain, N., McKenzie-Gopsill, A.,\nEsau, T. J., et al. (2021). Detection of a potato disease (early blight) using arti ﬁcial\nintelligence. Remote. Sens.13, 411. doi: 10.3390/rs13030411\nAnagnostis, A., Asiminari, G., Papageorgiou, E. I., and Bochtis, D. D. (2020). A\nconvolutional neural networks based method for anthracnose infected walnut tree\nleaves identi ﬁcation. Appl. Sci.10 (2), 469. doi: 10.3390/app10020469\nBa, J., Kiros, J. R., and Hinton, G. E. (2016). Layer normalization. ArXiv. doi:\n10.48550/arXiv.1607.0645\nBo, G., Leilei, D., Wei, L., and Bo, L. (2019). Research on maize disease image\nrecognition method based on grabcut algorithms. J. Chin. Agric. Mechanization. doi:\n10.1155/2021/9110866\nChen, J., Chen, J., fu Zhang, D., Sun, Y., and Nanehkaran, Y. A. (2020). Using deep\ntransfer learning for image-based plant disease identi ﬁcation. Comput. Electron. Agric.\n173, 105393. doi: 10.1016/j.compag.2020.105393\nDevi, K. S. G., Balasubramanian, K., Senthilkumar, C., and Ramya, K. (2022).\nAccurate prediction and classi ﬁcation of corn leaf disease using adaptive moment\nestimation optimizer in deep learning networks. J. Electrical Eng. Technol.18, 637–649.\ndoi: 10.1007/s42835-022-01205-0\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T.,\net al. (2020). An image is worth 16x16 words: Transformers for image recognition at\nscale. ArXiv. doi: 10.48550/arXiv.2010.11929\nElfatimi, E., Eryigit, R., and Elfatimi, L. (2022). Beans leaf diseases classi ﬁcation using\nmobilenet models. IEEE Access10, 1 –1. doi: 10.1109/ACCESS.2022.3142817\nJencyRubia, J., and BabithaLincy, R. (2021). Detection of plant leaf diseases using\nrecent progress in deep learning-based identi ﬁcation techniques. J. Eng. Technol. Ind.\nAppl. 7, 29 –36. doi: 10.5935/jetia.v7i30.768\nFIGURE 9\nA sample image contains the diseased areas across the entire image.\nHan and Guo 10.3389/fpls.2023.1328952\nFrontiers inPlant Science frontiersin.org11\nJenipher, V. N., and Radhika, S. (2022). “An automated system for detecting rice crop\ndisease using cnn inception v3 transfer learning algorithm, ” in 2022 Second\nInternational Conference on Arti ﬁcial Intelligence and Smart Energy (ICAIS) .\n(Piscataway, NJ: IEEE) 88 –94.\nKai, S., Zhikun, L., Hang, S., and Chunhong, G. (2011). “A research of maize disease\nimage recognition of corn based on bp networks, ” in Third International Conference on\nMeasuring Technology & Mechatronics Automation. (Piscataway, NJ: IEEE).\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classi ﬁcation with\ndeep convolutional neural networks. Commun. ACM6084–90. doi: 10.1145/3065386\nKsibi, A., Ayadi, M., Sou ﬁene, B., Jamjoom, M. M., and Ullah, Z. (2022). Mobires-net:\nA hybrid deep learning model for detecting and classifying olive leaf diseases. Appl. Sci.\n12 (20), 10278. doi: 10.3390/app122010278\nKulkarni, R. R., and Sapariya, A. D. (2021). Detection of plant leaf diseases using machine\nlearning. 10th International Conference on Computing, Communication and Networking\nTechnologies, (Piscataway, NJ: IEEE). doi: 10.1109/ICCCNT45670.2019.8944556\nLiu, B., Zhang, Y., He, D., and Li, Y. (2017). Identi ﬁcation of apple leaf diseases based\non deep convolutional neural networks. Symmetry 10, 11. doi: 10.3390/sym10010011\nLiu, W., and Wang, L. (2022). Quantum image edge detection based on eight-\ndirection sobel operator for neqr. Quantum Inf. Process.21, 1–27. doi: 10.1007/s11128-\n022-03527-4\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., et al. (2021). “Swin transformer:\nHierarchical vision transformer using shifted windows, ” in 2021 IEEE/CVF\nInternational Conference on Computer Vision (ICCV). (Piscataway, NJ: IEEE). 9992 –\n10002.\nMahum, R., Munir, H., un-nisa Mughal, Z., Awais, M., Khan, F. S., Saqlain, M., et al.\n(2022). A novel framework for potato leaf disease detection using an ef ﬁcient deep\nlearning model. Hum. Ecol. Risk Assessment: Int. J. 29, 303 –326. doi: 10.1080/\n10807039.2022.2064814\nMounika, R., and Bharathi, P. S. (2020). Detection of plant leaf diseases using image\nprocessing. Agricultural Food Sci.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., et al. (2019).\nPytorch: An imperative style, high-performance deep learning library. ArXiv. doi:\n10.48550/arXiv.1912.01703\nPatil, P., Yaligar, N., and M, M. (2017). “Comparision of performance of classi ﬁers -\nsvm, rf and ann in potato blight disease detection using leaf images, ” in 2017 IEEE\nInternational Conference on Computational Intelligence and Computing Research\n(ICCIC). (Piscataway, NJ: IEEE). 1 –5.\nQian, X., Zhang, C., Chen, L., and Li, K. (2022). Deep learning-based identi ﬁcation of\nmaize leaf diseases is improved by an attention mechanism: Self-attention. Front. Plant\nSci. 13. doi: 10.3389/fpls.2022.864486\nReddy, P. C., Chandra, R. M. S., Vadiraj, P., Reddy, M. A., Mahesh, T. R., and\nMadhuri, G. S. (2021). “Detection of plant leaf-based diseases using machine learning\napproach,” in 2021 IEEE International Conference on Computation System and\nInformation Technology for Sustainable Solutions (CSITSS). (Piscataway, NJ: IEEE).\n1–4.\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., et al. (2014).\nImagenet large scale visual recognition challenge. Int. J. Comput. Vision115, 211 –252.\ndoi: 10.1007/s11263-015-0816-y\nSingh, V. B., and Misra, A. K. (2017). Detection of plant leaf diseases using image\nsegmentation and soft computing techniques. Inf. Process. Agric.4, 41–49. doi: 10.1016/\nj.inpa.2016.10.005\nSingh, R., Sharma, A., Anand, V., and Gupta, R. (2022). “Impact of ef ﬁcientnetb3 for\nstratiﬁcation of tomato leaves disease, ” in 2022 6th International Conference on\nElectronics, Communication and Aerospace Technology . (Piscataway, NJ: IEEE).\n1373–1378.\nSneha, H., and Bagal, S. (2019). Detection of plant leaf diseases using image\nprocessing. Int. J. Advance Res. Innovative Ideas Educ. 5, 168 –170. doi: 10.31838/\njcr.07.06.310\nSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S. E., Anguelov, D., et al. (2014).\n“Going deeper with convolutions, ” in 2015 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR). (Piscataway, NJ: IEEE). 1 –9.\nTolstikhin, I. O., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T.,\net al. (2021). “Mlp-mixer: An all-mlp architecture for vision, ” in Neural Information\nProcessing Systems. (Cambridge, USA: MIT Press).\nvan der Maaten, L., and Hinton, G. E. (2008). Visualizing data using t-sne. J. Mach.\nLearn. Res.9, 2579 –2605.\nWang, G., Wang, J., Yu, H., and Sui, Y. (2022). Research on identi ﬁcation of corn\ndisease occurrence degree based on improved resnext network. Int. J. Pattern\nrecognition Artif. Intell.36. doi: 10.1142/S0218001422500057\nWang, G., Yu, H., and Sui, Y. (2021). Research on maize disease recognition method\nbased on improved resnet50. Mobile Inf. Syst. doi: 10.1155/2021/9110866\nWu, J., Zheng, H., Zhao, B., Li, Y., Yan, B., Liang, R., et al. (2017). Ai challenger: A\nlarge-scale dataset for going deeper in image understanding. ArXiv. doi: 10.48550/\narXiv.1711.06475\nXu, Y., Hao, Q., Qiao, L., and Wang, S. (2020). “Study on classi ﬁcation of maize\ndisease image based on fast k-nearest neighbor support, ” in 2020 IEEE Intl Conf on\nDependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and\nComputing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and\nTechnology Congress (DASC/PiCom/CBDCom/CyberSciTech). (Piscataway, NJ: IEEE).\nY, V., Billakanti, N., Veeravalli, K. ,. N., A. D., R., and Kota, L. (2022). “Early\ndetection of casava plant leaf diseases using ef ﬁcientnet-b0,” in 2022 IEEE Delhi Section\nConference (DELCON). (Piscataway, NJ: IEEE).1 –5.\nYao, J., Wang, Y., Xiang, Y., Yang, J., Zhu, Y., Li, X., et al. (2022). Two-stage detection\nalgorithm for kiwifruit leaf diseases based on deep learning. Plants 11. doi: 10.3390/\nplants11060768\nYu, M., Ma, X., Guan, H., Liu, M., and Tao, Z. (2022). A recognition method of\nsoybean leaf diseases based on an improved deep learning model. Front. Plant Sci.13.\ndoi: 10.3389/fpls.2022.878834\nYun, S., Han, D., Oh, S. J., Chun, S., Choe, J., and Yoo, Y. J. (2019). “Cutmix:\nRegularization strategy to train strong classi ﬁers with localizable features, ” in 2019\nIEEE/CVF International Conference on Computer Vision (ICCV). (Piscataway, NJ:\nIEEE). 6022 –6031.\nZhang, Q. (2021). A novel resnet101 model based on dense dilated convolution for\nimage classi ﬁcation. SN Appl. Sci.4. doi: 10.1007/s42452-021-04897-7\nZhang, H., Cisse ́ , M., Dauphin, Y., and Lopez-Paz, D. (2017). mixup: Beyond\nempirical risk minimization. ArXiv. doi: 10.48550/arXiv.1710.09412\nHan and Guo 10.3389/fpls.2023.1328952\nFrontiers inPlant Science frontiersin.org12"
}