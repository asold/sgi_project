{
    "title": "Multi-Encoder Transformer for Korean Abstractive Text Summarization",
    "url": "https://openalex.org/W4378448549",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2229509862",
            "name": "Youhyun Shin",
            "affiliations": [
                "Incheon National University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963929190",
        "https://openalex.org/W2467173223",
        "https://openalex.org/W2964165364",
        "https://openalex.org/W6728979910",
        "https://openalex.org/W2741204321",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W2962965405",
        "https://openalex.org/W2962929176",
        "https://openalex.org/W6761205521",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2963385935",
        "https://openalex.org/W2606974598",
        "https://openalex.org/W2889518897",
        "https://openalex.org/W6737479944",
        "https://openalex.org/W2902385320",
        "https://openalex.org/W2963407669",
        "https://openalex.org/W3035629723",
        "https://openalex.org/W2970352405",
        "https://openalex.org/W3097306574",
        "https://openalex.org/W3160201895",
        "https://openalex.org/W3198871389",
        "https://openalex.org/W6761268247",
        "https://openalex.org/W2970326214",
        "https://openalex.org/W6682631176",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W3170092793",
        "https://openalex.org/W638387498",
        "https://openalex.org/W3191287074",
        "https://openalex.org/W2970419734",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W6769627184",
        "https://openalex.org/W2540831494",
        "https://openalex.org/W2924690340",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2189552882",
        "https://openalex.org/W2612675303",
        "https://openalex.org/W4288089799"
    ],
    "abstract": "In this paper, we propose a Korean abstractive text summarization approach that uses a multi -encoder transformer. Recently, in many natural language processing (NLP) tasks, the use of the pre-trained language models (PLMs) for transfer learning has achieved remarkable performance. In particular, transformer-based models such as Bidirectional Encoder Representations from Transformers (BERT) are used for pre-training and applied to downstream tasks, showing state-of-the-art performance including abstractive text summarization. However, existing text summarization models usually use one pre-trained model per model architecture, meaning that it becomes necessary to choose one PLM at a time. For PLMs applicable to Korean abstractive text summarization, there are publicly available BERT-based pre-trained Korean models that offer different advantages such as Multilingual BERT, KoBERT, HanBERT, and KorBERT. We assume that if these PLMs could be leveraged simultaneously, better performance would be obtained. We propose a model that uses multiple encoders which are capable of leveraging multiple pre-trained models to create an abstractive summary. We evaluate our method using three benchmark Korean abstractive summarization datasets, each named Law (AI-Hub), News (AI-Hub), and News (NIKL) datasets. Experimental results show that the proposed multi-encoder model variations outperform single -encoder models. We find the empirically best summarization model by determining the optimal input combination when leveraging multiple PLMs with the multi-encoder method.",
    "full_text": "Received 28 April 2023, accepted 16 May 2023, date of publication 18 May 2023, date of current version 24 May 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3277754\nMulti-Encoder Transformer for Korean\nAbstractive Text Summarization\nYOUHYUN SHIN\nDepartment of Computer Science and Engineering, Incheon National University, Incheon 22012, South Korea\ne-mail: yhshin@inu.ac.kr\nThis work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea Government\n(MSIT) under Grant 2021R1G1A1012766.\nABSTRACT In this paper, we propose a Korean abstractive text summarization approach that uses a\nmulti-encoder transformer. Recently, in many natural language processing (NLP) tasks, the use of the\npre-trained language models (PLMs) for transfer learning has achieved remarkable performance. In partic-\nular, transformer-based models such as Bidirectional Encoder Representations from Transformers (BERT)\nare used for pre-training and applied to downstream tasks, showing state-of-the-art performance including\nabstractive text summarization. However, existing text summarization models usually use one pre-trained\nmodel per model architecture, meaning that it becomes necessary to choose one PLM at a time. For\nPLMs applicable to Korean abstractive text summarization, there are publicly available BERT-based pre-\ntrained Korean models that offer different advantages such as Multilingual BERT, KoBERT, HanBERT, and\nKorBERT. We assume that if these PLMs could be leveraged simultaneously, better performance would\nbe obtained. We propose a model that uses multiple encoders which are capable of leveraging multiple\npre-trained models to create an abstractive summary. We evaluate our method using three benchmark Korean\nabstractive summarization datasets, each named Law (AI-Hub), News (AI-Hub), and News (NIKL) datasets.\nExperimental results show that the proposed multi-encoder model variations outperform single-encoder\nmodels. We find the empirically best summarization model by determining the optimal input combination\nwhen leveraging multiple PLMs with the multi-encoder method.\nINDEX TERMS Natural language processing, abstractive text summarization, bidirectional encoder repre-\nsentations from transformer, neural networks, natural language generation.\nI. INTRODUCTION\nText summarization refers to the task of creating a brief and\ncondensed summary of a document(s) containing multiple\nsentences, consisting of only the most important and relevant\ninformation [1]. In order to create a summary, two main\nmethodologies have been proposed. The first is an extractive\nsummarization method that extracts the sentences considered\nimportant among the sentences in the input. The second is\nan abstractive summarization method that may generate new\nsentences containing the core points of the input. This paper\nfocuses on an abstractive summarization method using a\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Arianna Dulizia\n.\ntransformer [2] model to generate a summary from a single\ndocument.\nAbstractive text summarization generates a concise sum-\nmary based on an understanding of input sentences using\na natural language generation technique. Accordingly, this\nmethod mainly relies on a sequence-to-sequence model\nthat uses an encoder-decoder architecture. The encoder part\nunderstands the entire input and the decoder part generates\na summary based on this understanding. Recent abstrac-\ntive summarization methods use a transformer architecture\n[3], [4], [5], [6], [7]. Consequently, the most effective\napproach to NLP tasks including abstractive summarization is\ngenerally to utilize a transformer-based pre-trained model and\nthen to fine-tune the model for the downstream task. Korean\n48768\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ VOLUME 11, 2023\nY. Shin: Multi-Encoder Transformer for Korean Abstractive Text Summarization\nabstractive text summarization that we are trying to solve can\nbe applied in the same way.\nOne of the most representative and influential pre-trained\nmodel is BERT [8], but it is a PLM trained only on English\ndata, making it suitable only for English downstream tasks.\nRelated to this, Multilingual BERT (M-BERT) 1 has been\nreleased for 104 languages other than English, including\nKorean. Because M-BERT is not a language-specific model,\nit usually performs well, but it does not always perform\nthe best for all Korean NLP tasks. Therefore, pre-trained\nmodels trained only with a Korean corpus have been pro-\nposed, with examples being KoBERT, 2 HanBERT34 and\nKorBERT.5 Compared to M-BERT, which is a multilingual\nmodel using WordPiece tokenizer, KoBERT, HanBERT and\nKorBERT are Korean-specific models that pre-train with cor-\npus in Korean employing different tokenization strategies.\nFor the tokenizer used by Korean-specific PLMs, KoBERT\nuses the SentencePiece tokenizer. HanBERT uses both an\ninternal tokenizer called Moran and the WordPiece tokenizer.\nMoran is a morphological analyzer that segments words into\nsyllable units without attaching POS tags. KorBERT uses\nthe ETRI morphological analyzer with POS tags. It is com-\nmon for Korean to use a morphological analyzer for text\npreprocessing because it is an agglutinative language [9].\nDue to the aforementioned differences, we assume that each\nmodel has specific own characteristics and that it would be\nbetter simultaneously to utilize various Korean PLMs when\nsummarizing text. Additionally, for the pre-training of each\nPLM, much time, effort, and resources are required, such\nas large-scale corpora and devices such as a GPU and TPU.\nHowever, existing summarization methods require the selec-\ntion of one pre-trained model when fine-tuning the model for\nthe summarization task.\nIn this paper, we propose a multi-encoder transformer\napproach for Korean abstract text summarization that utilizes\nmultiple pre-trained models that can take into account varied\ninfluences of them. Because abstractive summarization refers\nto the task of generating text based on how the encoder\nunderstands the input, the encoding ability can affect the\noverall quality of the summary. Instead of using only one pre-\ntrained model, the encoder encodes the input using multiple\npre-trained models to make a better summary. Methods using\nmultiple encoders commonly use a flat or parallel strategy\nto integrate the input values [10]. We propose a Korean\nabstractive summarization method that can take advantage of\nmultiple pre-trained models by combining the input in a flat\n(Figure 1) or parallel (Figure 2) manner. We show that sum-\nmaries generated by simultaneously encoding and leveraging\nmultiple pre-trained models outperform those created using\nonly one pre-trained model. The main contributions of this\nwork are as follows:\n1https://github.com/google-research/bert/blob/master/multilingual.md\n2https://github.com/SKTBrain/KoBERT\n3https://twoblockai.com/\n4https://github.com/monologg/HanBert-Transformers\n5https://aiopen.etri.re.kr/service_dataset.php\nFIGURE 1. Input Combination Strategy: Flat. Flat is a strategy of\nconcatenating values from multiple encoders before feeding them to the\ndecoder. A single decoder receives the concatenated representation and\nuses them to compute encoder-decoder attention (cross-attention) score.\nEach encoder #n represents one of several PLMs, such as M-BERT,\nKoBERT, HanBERT, and KorBERT.\n1) The proposed Korean abstractive summarization\napproach leverages the multi-encoder transformer\narchitecture to simultaneously utilize multiple Korean\nPLMs, that require a lot of resources for pre-training,\nin one model.\n2) The encoder representations using Korean PLMs\ntrained in different token units such as subwords as well\nas morphemes are combined and used in the decoders,\naiming to make input features that consider the unique\ncharacteristics of Korean as an agglutinative language.\n3) The proposed multi-encoder model has been demon-\nstrated to be both feasible and capable, as it takes\ninto account Korean-specific PLMs. The results\nof the experiments on a Korean judgment dataset\nand two news datasets showed that it outperforms\nboth the single-encoder model that only uses one\nKorean-specific PLM and the multi-encoder model that\ncombines the same PLM multiple times.\nII. RELATED WORK\nA. ABSTRACTIVE SUMMARIZATION\nAbstractive summarization aims to generate condensed sum-\nmaries from long documents. Abstractive summarization is\na text generation task that takes tokens X={x1, x2, . . . ,xn}\nin the source document as input and generates a sequence of\ndiscrete tokens as the target summary Y={y1, y2, . . . ,ym}.\nTokens xi and yj are each taken from the pre-built token\nvocabularies Vsrc and Vtgt , respectively. In general, the gen-\neration of the output summary is conditioned by the input\ntokens and is modeled with the following conditional proba-\nbility: P(Y |X) = P(y1, y2, . . . ,ym|x1, x2, . . . ,xn). Given that\nabstractive summarization is a sequence-to-sequence prob-\nlem that takes a sequence as input and outputs a sequence,\nthe summary is mainly generated using the encoder-decoder\nmethod. In earlier work [11], a neural encoder-decoder frame-\nwork was proposed for the first time. Since then, various\nVOLUME 11, 2023 48769\nY. Shin: Multi-Encoder Transformer for Korean Abstractive Text Summarization\nFIGURE 2. Input Combination Strategy: Parallel. For the parallel strategy,\nthere are multiple decoders equal in number to those multiple encoders\ncorrespond to, and the values that pass through the decoders are\nsummed. In more detail, after the calculated result of each encoder is\ntransmitted to the corresponding decoder, the cross-attention score is\ncalculated and the attention result from each decoder is summed.\nRNN-based methods have been proposed [12], [13], [14],\n[15], [16], [17], [18], [19].\nRecently, the transformer architecture was developed,\nshowing state-of-the-art performance such that most gener-\nation tasks including abstractive summarization are solved\nwith this architecture. It is also used as a fine-tuning method\nafter the pre-training of massive amounts of unlabeled text\nusing the transformer architecture. The encoder-decoder\ntransformer was also introduced and is mainly used for\nabstractive summarization [3], [4], [5], [6]. However, because\nthese models are intended utilized to perform adequately\nwith documents written in English, this paper proposes an\nabstractive summarization model that works well for Korean.\nWe use the encoder-decoder transformer with BERT-based\nPLMs which is especially applicable to Korean documents.\nB. PRE-TRAINED MODELS FOR KOREAN\nThere are several Korean pre-trained models that can be used\nto fine-tune a pre-trained transformer-based summarization\nmodel, such as M-BERT, KoBERT, HanBERT, and Kor-\nBERT. These are BERT-based language models that pre-train\nthe transformers encoder. M-BERT is a multi-lingual model\ntrained for 104 languages, including Korean. KoBERT, Han-\nBERT, and KorBERT are monolingual models trained by a\nBERT-based method on a Korean-only corpus. In addition,\nthere are Korean versions for GPT, BART, and T5. There\nare KoGPT,6 which pre-trains the transformer decoder, and\nKoBART7 and ET5, 8 which are sequence-to-sequence mod-\nels that pre-train using both the transformer encoder and\ndecoder. However, only BERT-based models have variants\n6https://github.com/kakaobrain/kogpt\n7https://github.com/SKT-AI/KoBART\n8https://aiopen.etri.re.kr/service_dataset.php\nthat use various corpora and tokenizers. In this paper, we pro-\npose a method that utilizes various BERT-based models,\nin this case M-BERT, KoBERT, HanBERT, and KorBERT.\nThe use of other PLMs is left for future work.\nBERT-based pre-trained models use different training\ndatasets and various tokenizers to build a vocabulary set.\nM-BERT uses the WordPiece tokenizer, KoBERT uses the\nSentencePiece tokenizer, and HanBERT uses their own\nmorphological analyzer called Moran and the WordPiece tok-\nenizer. KorBERT provides two versions of PLMs: subword-\nbased and morpheme-based types. We use a morpheme-based\nversion that contains more Korean characteristics and shows\nbetter performance. KorBERT uses a Korean morphological\nanalyzer with POS tags provided by an open API on AI Hub.. 9\nAccordingly, the vocabulary sizes of M-BERT, KoBERT,\nHanBERT, and KorBERT are 119,547 119,547 (including\nonly 1,187 Korean syllables and 3,271 Korean unicode\ntokens), 8,002, 54,000 and 30,349, respectively. In addition,\nM-BERT is trained using Wikipedia data in 104 languages,\nKoBERT is trained using 324M words in 25M Wikipedia\nsentences, HanBERT is trained using 70GB of general docu-\nments with 11.3 billion morphemes in 350 million sentences,\nand KorBERT is trained using 23GB of large-scale text, such\nas newspaper and encyclopedias, with 4.7 billion morphemes.\nIn this paper, we propose a method fine-tuned to the abstrac-\ntive summarization task using these different BERT-based\nPLMs, all of which are feasible for use with Korean, all at\nonce.\nC. MULTI-ENCODER TRANSFORMER\nThe multi-encoder transformer uses two or more encoders\nand more than one decoder, whereas the traditional\ntransformer architecture is generally based on a single\nencoder-decoder design. For the multi-encoder transformer,\nresearchers have focused on the different types of input values\n(sources) to be received and encoded and how to combine\nrepresentations generated from the multi-encoder and use it\nin the decoder. Multi-encoder approaches have mainly been\nstudied in relation to speech recognition [20 ], [21], [22],\nneural machine translation and automatic post-editing (APE),\nthe task of correcting errors in machine-translated texts [10],\n[23], [24], [25], [26], [27]. In the existing multi-encoder\nmethod, two encoders are mainly used. For example, one\nencoder takes the source as the input and while the second\ntype takes the context or a translated sentence as the input.\nIn previous work [10], four types of input combination\nstrategies for the multi-encoder transformer were introduced.\nThese were the flat, parallel, hierarchical, and serial types.\nIn this paper, we employ the flat and parallel methods. In the\nflat method, transformer usually consists of multiple encoders\nand a single decoder, and the combination is performed by\nconcatenating the values from multiple encoders and is used\nin a single decoder. In parallel method, multiple encoders\nand the same number of decoders are generally used, and\n9https://aiopen.etri.re.kr/\n48770 VOLUME 11, 2023\nY. Shin: Multi-Encoder Transformer for Korean Abstractive Text Summarization\nFIGURE 3. An overview of basic encoder-decoder transformer\narchitecture introduced in [2].\ninput combination is performed by summing the results of\neach encoder-decoder. These two methods are the most intu-\nitive combination methods, with the difference being how\nthey either combine the result values at the encoder or the\ndecoder. The hierarchical approach takes one more attention\nlayer to the resulting value of the parallel method, and the\nserial approach is a method that calculates the attention score\nby passing the values of each encoder sequentially to the\ndecoder. Because the goal of this paper is to demonstrate if\nperformance outcomes can be improved by using multiple\nPLMs in encoders, we attempt to verify this while focusing\non the two simple methods: flat and parallel.\nIII. BACKGROUND: TRANSFORMER\nIn this paper, we use the transformer architecture [2], which\nis a deep learning architecture that has recently shown the\nmost powerful performance in natural language processing.\nWe especially use the encoder-decoder transformer architec-\nture to perform abstractive summarization.\nA. TRANSFORMER ENCODER\nAs shown in Figure 3, the transformer encoder consists of\nmultiple encoder blocks which are indicated as N times\n(×N). The encoder creates input feature after an input text\nis fed to multiple encoder blocks. After receiving a text as\ninput, transforming it into an embedding matrix, and adding\npositional encoding, the input value is fed to the first encoder\nblock. In each encoder block, there are two sub-layers in the\norder of a multi-head attention (MHA) and a feed-forward\nnetwork.\nThe multi-head attention layer of the encoder calculates\nthe self-attention value for the input sentence with h (the\nnumber of heads) heads. The h attention matrices are created\nand concatenated. The output attention matrix value is fed\nto the next sub-layer, the feed-forward network. In the feed-\nforward network, the encoder representation is an output as\na result using two dense layers and an activation function.\nThis process is repeated N times to output the final encoder\nrepresentation which is the values from the last encoder\nblock. The output and input values of each sub-layer are\nconnected by add & norm layer to use residual connection and\nlayer normalization. The encoder representation of the final\nencoder block is used as an input to the decoder. In this paper,\nwe initialize the weight value of the encoders using PLMs\nand then fine-tuning, and our proposed model uses multiple\nencoders.\nB. TRANSFORMER DECODER\nThe decoder of the transformer also consists of multiple\ndecoder blocks. The input sentence of the decoder is also\ntransformed into an embedding matrix and added positional\nencoding information, and then it becomes an input of the\nfirst decoder block. One decoder block consists of three sub-\nlayers. It consists of masked multi-head attention, multi-\nhead attention, and feed-forward network in order. Residual\nconnection and layer normalization are applied to the input\nand output values of each sub-layer.\n1) MASKED MULTI-HEAD ATTENTION\nThe decoder takes the input and feed it to the masked\nmulti-head attention layer. The masked multi-head attention\nof the decoder is similar to the multi-head attention used in\nthe encoder, but there is a difference in the method of calcu-\nlating the query (Q), key (K ), and value (V ) matrices using\nself-attention. The self-attention of the encoder performs an\noperation on the words of the entire sentence to calculate\nthe attention value of each word. However, in the case of\nthe decoder, the self-attention value must be calculated by\nmasking words that the model has not yet generated. There-\nfore, masked multi-head attention allows the model to learn\naccurately by calculating the self-attention values using only\nthe words predicted in the previous step.\nMulti-head attention including masked multi-head atten-\ntion is the result of calculating and concatenating h queries,\nkey, and value matrices. In the case of calculating the l-th\nhead of the encoder, the query (Q l), key (K l), and value\n(Vl) matrices can be obtained by multiplying the input value\nmatrix of the multi-head attention layer by the weight matri-\nces W Q\nl , W K\nl , and W V\nl , respectively. As in eq. (1) where\nheadl = softmax(QlKT\nl /√\ndk ) and dk is the dimension of\nkey and query, after calculating h attention matrices, they\nare concatenated and multiplied by a new weight matrix W O\nto finally obtain an attention matrix M. Because h attention\nheads are concatenated, h times of the size of attention head\nis obtained, so the weight matrix is multiplied to reduce the\nresult to the original size of the attention head.\nM(Q, K, V ) = concat(head1, . . . ,headl, . . . ,headh)W O\n(1)\nVOLUME 11, 2023 48771\nY. Shin: Multi-Encoder Transformer for Korean Abstractive Text Summarization\nThe next sub-layer receives the obtained attention matrix as\nan input value. In decoder, the output value of the masked\nmulti-head attention layer is fed to the multi-head attention\nlayer as an input value.\n2) MULTI-HEAD ATTENTION\nThe multi-head attention layer, which is the next sub-layer of\nthe decoder, receives two input values. One is the output value\nof the previous sub-layer, and the other is the representation of\nthe encoder. In this multi-head attention layer, an interaction\ntakes place between the result of the encoder and the result\nof the decoder. This layer is also called the encoder-decoder\nattention layer. The representation of the encoder is denoted\nas R, and the attention matrix from the masked multi-head\nattention layer that is the previous sub-layer is denoted as M.\nFor the l-th head in the multi-head attention layer, a query\nmatrix Ql is generated by multiplying the attention matrix\nM by the weight matrix W Q\nl . The encoder representation R\nis multiplied by weight matrices W K\nl and W V\nl , respectively,\nto generate key and value matrices Kl and Vl. After obtaining\nthe attention matrix for h heads, the final attention matrix can\nbe obtained by concatenating them and multiplying them by\nthe weight matrix W O.\n3) FEED-FORWARD\nThe output attention matrix from the encoder-decoder atten-\ntion layer is an input to the feed-forward network, which is the\nnext sub-layer of the decoder. After repeating this operation N\ntimes, the decoder representation of the target text is returned.\nThe decoder representation of the target text goes through the\nlinear and softmax layers and finally outputs the predicted\nword.\nIV. PROPOSED METHOD\nA. KOREAN ABSTRACTIVE SUMMARIZER\nThe proposed Korean abstractive summarization method is a\ntransformer-based encoder-decoder model. We use our base\nmodel, as introduced earlier [3], [28], as the single encoder-\ndecoder method. We extend this model so that it becomes\napplicable to Korean and so that it allows multiple encoders.\nAt this time, the role of the encoder is to read all of the Korean\nsentences in a source document and create a representation by\ncomputing the self-attention score of its token embeddings.\nThe role of the decoder is to generate a target summary by\ncomputing the self-attention and cross-attention scores using\na vector from both the encoder and the decoder. We fine-tune\nour encoders using BERT-based Korean PLMs.\nWe denote a source document with n sentences as\nx = {sent1, sent2, . . . ,sentn}. Sentences tokenized using\ntokenizers of different PLMs are expressed as x′PLMi\ntok =\n{tokPLMi (sent1), tokPLMi (sent2), . . . ,tokPLMi (sentn)}. tokPLMi\n(sentj) is the result of tokenization after applying the tokenizer\nof PLMi to sentj, where PLMi represents one of the PLMs,\nin this case M-BERT, KoBERT, HanBERT, or KorBERT.\nThe final input sequence of the transformer is xPLMi\ntok =\n{[CLS], tokPLMi (sent1), [SEP], [CLS], tokPLMi (sent2),\n[SEP], . . . ,[CLS], tokPLMi (sentn), [SEP]}. Each sentence is\nidentified with special tokens [CLS] and [SEP], which start\nwith [CLS] and end with [SEP], as used in earlier work [3].\nAt this time, tokenization and encoding are applied using the\ndifferent pre-trained models. Each token in xPLMi\ntok is embed-\nded as token embeddings and is concatenated with segment\nembeddings and position embeddings. Finally, the concate-\nnated representation becomes the input value of the trans-\nformer encoder. Furthermore, to apply various pre-trained\nmodels, the multi-encoder transformer model is applied.\nB. MULTI-ENCODER\nWe employ a multi-encoder architecture to leverage mul-\ntiple pre-trained models for abstractive summarization. As\nthe Korean BERT-based PLMs, we select four representative\nmodels trained with different tokens: M-BERT, KoBERT,\nHanBERT, and KorBERT. Therefore, we encode the input\nsentences by applying various pre-trained models. Sentences\nare tokenized in different forms depending on the PLM used\nand encoded using this PLM. For example, as shown in\nTable 1, the word corresponding to the ‘\n’ (conductor\nin Korean) is tokenized differently for each PLM tokenizer.\nM-BERT, KoBERT, HanBERT, and KorBERT are tokenized\nas\n, and ‘\n /NNG’\n/XSN’’, respectively. The romanization results for each\ntoken are as follows: ‘ji ##hwi ##ja’, ‘_ji-hwi ja’, ‘ji-hwi-ja’,\nand ‘ji-hwi/NNG ja/XSN’. By using romanization, Korean\nlanguage named Hangul is expressed as it is pronounced\nso that we indicate that how the sentence is tokenized in\nphoneme units. We marked the syllable boundaries with\nhyphens to make them clear. We use the most widely used\nsystem of romanization for Korean, the revised romanization\nsystem (RR) 10 officially announced by the Ministry of Cul-\nture in 2000 and revised in 2014. The encoded value of each\npre-trained model PLMi is Enc(xPLMi\ntok ) and these values are\nfed to the decoder. The PLMs used in encoders are fine-tuned\nfor abstractive summarization through training. There are two\nways to combine the values from multiple encoders, depend-\ning on when to combine them. The first one uses a merging\nstep before feeding the values from multiple encoders to the\ndecoder, and the second one feeds the values from multiple\nencoders to the corresponding decoders and combines the\nvalues after passing them through each decoder’s attention\nlayer.\nC. DECODER\nAs a method of combining representations denoted as\nEnc(xPLMi\ntok ) from multiple transformer encoders, we use\nthe flat and parallel strategies introduced in [10]. The flat\ncombination strategy, as described in Figure 1, horizontally\nconcatenates all states from all encoders to create a sin-\ngle representation via equation (2), where K and V corre-\nspondingly stand for the transformer’s key and value. Ki\n10http://roman.cs.pusan.ac.kr/\n48772 VOLUME 11, 2023\nY. Shin: Multi-Encoder Transformer for Korean Abstractive Text Summarization\nTABLE 1. Tokenized result for an example original sentence (sent). The example sentence is a sentence written in Korean with the meaning of ‘‘This year\nmarks the 210th anniversary of the birth of Felix Mendelssohn, a German composer, conductor and pianist. ’’ . The sentences in parentheses are converted\nfrom Korean language into Latin script using romanization. The table shows the tokenized results of each PLM tokenizer and denoted astokPLMi (sent)\nwhere PLMi represents one of the PLMs, in this case M-BERT, KoBERT, HanBERT, or KorBERT. Each PLM is abbreviated as follows: M-BERT is M, KoBERT is\nKo, HanBERT is Han, and KorBERT is Kor.\nTABLE 2. Summarization datasets. The table shows the sizes of the training, validation, and test sets, and the average length (in words and sentences) of\ndocuments and summaries.\nrepresents the key value fed from each encoder to com-\npute the decoder’s cross-attention score and corresponds to\nEnc(xPLMi\ntok ) depending on the PLM used. In 1:n as in K1:n,\nn refers to the number of PLMs used. For example, if we\nencode the input document with M-BERT (M), KoBERT\n(Ko), HanBERT (Han), or KorBERT (Kor), these are cor-\nrespondingly expressed as Enc(xM\ntok ), Enc(xKo\ntok ), Enc(xHan\ntok ),\nand Enc(xKor\ntok ). Enc(xPLMi\ntok ) and these values correspond to\nK0 to K3. Accordingly, we concatenate these representations\nas concat[Enc(x M\ntok );Enc(xKo\ntok );Enc(xHan\ntok );Enc(xKor\ntok )] and feed\nthe outcome as the key Kflat and value Vflat of the decoder.\nKflat = Vflat = concat(K1:n) (2)\nAs in equation (3), where Q stands for the query that is the\nresult of the calculation of the masked self-attention score\nfor the decoder’s input sequence, the flat method uses the\nconcatenated representation as the key Kflat and value Vflat to\ncompute the encoder-decoder attention score of the decoder\nAflat (Q, K1:n, V1:n). At this point, we employ combined val-\nues from multiple encoders. In 1:n as in K1:n and V1:n, n refers\nto the number of PLMs used. Also, all layers of the decoder\nare tied together by residual connections with adjacent layers.\nAflat (Q, K1:n, V1:n) = A(Q, Kflat , Vflat ) (3)\nIn the parallel combination strategy, shown in Figure 2,\nmultiple decoders attend to each corresponding encoder\nEnc(xPLMi\ntok ) used as the key Ki and value Vi independently\nto compute the encoder-decoder attention score. Then, as in\nequation (4), the individual attention values A(Q, Ki, Vi) of\neach decoder are summed as Aparallel(Q, K1:n, V1:n). There is\na residual connection between the query Q and the summed\nattention values.\nAparallel(Q, K1:n, V1:n) =\nn∑\ni=1\n(A(Q, Ki, Vi)) (4)\nFinally, a summary is generated with an auto-regressive\nmethod using the summed value passed to the feed forward\nlayer.\nV. EXPERIMENT AND ANALYSIS\nA. DESCRIPTION OF DATASET\nWe evaluate our method using three Korean abstractive\nsummarization datasets, each named Law (AI-Hub), News\n(AI-Hub), and News (NIKL) datasets. Table 2 shows the\nstatistics of each data set.\nLaw (AI-Hub)is a legal text summarization dataset built\nby Bflysoft and published on AI-Hub. 11 The dataset is con-\ntinuously updated and version 1.1 was used. The dataset is\ncomposed of an average of 10 sentences, each consisting of\nan average of 418 words. Gold summary sentences are com-\nposed of one sentence of an average of 45 words. Originally,\n24,329 and 3,004 documents were provided for training and\ntesting, respectively. We split 24,329 documents into 22,000\n11https://aihub.or.kr/\nVOLUME 11, 2023 48773\nY. Shin: Multi-Encoder Transformer for Korean Abstractive Text Summarization\ndocuments for training and the remaining 2,329 documents\nfor validation.\nNews (AI-Hub) is a news summarization dataset, also\nbuilt by Bflysoft and published on AI-Hub. Because this\ndataset is continuously updated, the snapshot data set used\nin the 2020 Dacon competition 12 was used here. The dataset\nconsists of a total of 42,803 news articles. The average length\nof an article is 13 sentences and 214 words. Also, articles are\ncollected from 10 different news publishers. We divide the\ndataset into an 8:1:1 ratio. We use 34,243, 4,280, and 4,280\nfor training, validation, and test set, respectively.\nNews (NIKL)is a news summarization dataset published\nby the National Institute of Korean Language (NIKL). 13 This\ncorpus is composed of summarized sentences of articles,\nwhich were extracted from the news corpus of the National\nInstitute of Korean Language, consisting of 4,389 articles.\nOn average, a news article is composed of 10 sentences and\n313 words, while its summary is comprised of 3 sentences\nand 54 words. The dataset was split in a 8:1:1 ratio, with 3,511\ndocuments being used for training and 439 documents each\nbeing used for validation and testing.\nB. AUTOMATIC EVALUATION\nWe automatically evaluate the quality of the generated sum-\nmaries using ROUGE (Recall-Oriented Understudy for Gist-\ning Evaluation) [29], which is a representative metric used\nto evaluate generation tasks such as text summarization and\nmachine translation. Calculation by ROUGE-N are based\non the n-gram overlaps between a model-generated sum-\nmary and a human-made reference. ROUGE-L measures the\nlongest matching string using the longest common subse-\nquence (LCS) technique. The advantage of LCS is that it does\nnot require the consecutive matching of words like ROUGE-\nN, but rather measures the matching that occurs within a\nstring, allowing a more flexible performance comparison.\nWe use ROUGE-1, ROUGE-2, and ROUGE-L. We calculate\nROUGE scores using a Python code provided by the Dacon\nAI competition for Korean text summarization. 14\nC. IMPLEMENTATION DETAILS\nWe implement the transformer using PyTorch and train\nthe model with a total of 150,000 steps using NVIDIA\nV100 GPUs, with gradient accumulation performed every\nfive steps. For training, the encoders are fine-tuned using\nM-BERT, KoBERT, HanBERT, or KorBERT, and the decoder\nalso use PLMs to initialize the target embedding values.\nWe use a batch size of 32. The decoder applies dropout with\na probability of 0.2. We use the Adam optimizer [30], and as\nin earlier work [3], because the decoder must be trained more\nthan the encoder, the learning rates of the encoder and decoder\nare set differently. The learning rate of the encoder is set to\n0.002, and the learning rate of the decoder is 0.1. We choose\n12https://www.dacon.io/competitions/official/235673/data\n13https://corpus.korean.go.kr/\n14https://www.dacon.io/competitions/official/235673/talkboard/401911\nlearning rate warmup steps as 20,000. when training the\ndecoder using the HanBERT vocabulary, we use decoder\nwarmup steps as 30,000. The decoder generates a summary in\nan auto-regressive manner, and a beam search is applied with\na beam size of 5. Checkpoints are saved every 5,000 steps,\nand the top-5 checkpoints are selected through validation and\nare used for testing. The results are then reported.\nD. BASELINES\nIn an experiment, we compare the results with three baseline\napproaches. We use the LEAD-n baselines, which simply\nextracts the first to N-th sentences in an input document.\nWe report scores of LEAD-1, LEAD-2, and LEAD-3. Also,\nwe use the existing English model PreSumm [3] as our\nbaseline model by implementing it applicable to Korean.\nThis model uses BERT as a pre-trained model with a\nsingle-encoder transformer. In this paper, we use KoBERT,\nM-BERT, HanBERT, and KorBERT as pre-trained models\nfor single-encoder transformer and these models are used as\nour baselines. In addition, we compare the proposed model\nwith multiple encoder methods in which the same encoder is\nutilized multiple times for the same text, as seen in the Multi-\nEncoder Multi-Decoder (ME-MD) approach [31]. We com-\npared the performance of the proposed model with the case\nof using multiple encoders by combining the same PLM N\ntimes; for example, if M-BERT is used 4 times in the encoder,\nit is denoted as M4.\nE. KOREAN ABSTRACTIVE SUMMARIZATION\nAt this time, KorBERT is not used in the decoder part because\nthe morpheme tokens generated when using it are difficult to\nrestore. KorBERT creates some tokens in the original forms,\nsuch as a stem, while splitting the agglutination. There are\ntwo general methods of restoration for a generated summary\ndepending on how the morpheme analyzer tokenizes the mor-\npheme. To restore the tokenized result in morpheme units,\none way is simply to remove the separator token, such as\n#, as when restoring other tokens such as a subword. The\nsecond way is to make rules for restoring stemmed results.\nBoth HanBERT and KorBERT use a morphological analyzer\nas a tokenizer. As shown in Table 1, while the summary\ngenerated using the HanBERT decoder can be restored using\nthe former approach, the summary generated using the Kor-\nBERT decoder requires the latter method. Because the latter\napproach is a rule-based method, there may be cases that\ncannot be covered, and it is difficult to use it generally.\nConsequently, the method that uses KorBERT in the decoder\nis excluded in our experiment. Therefore, for the decoder,\nwe generate summaries using the vocabularies in M-BERT,\nKoBERT, or HanBERT.\n1) SUMMARIZATION RESULTS USING THE\nSINGLE-ENCODER TRANSFORMER\nThe values reported in the fourth to sixth results in Table 3\nare the experimental results for the single-encoder models.\n48774 VOLUME 11, 2023\nY. Shin: Multi-Encoder Transformer for Korean Abstractive Text Summarization\nTABLE 3. Average ROUGE F1 scores for single-encoder models and multi-encoder models. For the multi-encoder model, all four PLMs (M, Ko, Han, and\nKor) are used and combined in a flat or parallel manner.xPLM\ntok represents the PLMs used as a tokenizer in the encoders and used to pre-train the\nencoders. ytok denotes the PLM used as the target vocabulary in the decoder and used to initialize the target embedding value. Bold indicates the best\ncase for each group. Underline denotes the best case model among the other models in this table. The symbol† is used to indicate statistically significant\n(p < 0.05) results compared to a single encoder model using the same decoder, and‡ is used to indicate statistically significant (p< 0.05) results\ncompare to the highest value among single-encoder model results for each ROUGE score.\nA single-encoder model is a conventional encoder-decoder\ntransformer model. In this case, it is a model that uses only\none PLM for one encoder. We report the experimental results\nwith Korean PLMs applicable based on an earlier method [3].\nIn order to examine the effects of each Korean BERT-based\nPLM, we conduct an experiment on various single-encoder\nmodels that select and use only one PLM at a time for training\nand inference among several PLMs.\nThe best performing PLMs varied depending on each\ndataset. For the Law (AI-Hub) dataset, the experimental\nresults showed that using HanBERT for both the encoder\n(xPLM\ntok ) and decoder (y tok ) in a single-encoder model led to\nslightly better performance than other models for ROUGE-\n1 (R1) and ROUGE-L (RL). However, for ROUGE-2 (R2),\nusing M-BERT achieved the highest score. For the News\n(AI-Hub) dataset, the KoBERT-based model achieved the\nhighest scores for both R1 and R2, while the M-BERT-\nbased model achieved the highest score for RL. As for the\nNews (NIKL) dataset, the HanBERT-based model achieved\nthe highest score for R1, while the M-BERT-based model\nachieved the highest scores for R2 and RL. Therefore, if only\none PLM can be used, additional efforts, such as comparing\nthe performance experimentally to choose the most suitable\nsingle-encoder model, are necessary.\nCompared with LEAD-n, approaches using the single-\nencoder transformer outperform the simple baseline LEAD-\nn. Further, the results of LEAD-2 and LEAD-3 outperform\nthe score of LEAD-1, which can be inferred that the gold\nsummary contain information obtained from two or more\nsentences. In Tables 11 and 12, when the contents of the\narticle included in the human-made reference summary are\nunderlined, we examine that the gold summary is created\nusing two or more sentences. Therefore, it can be inferred\nthat a better summary can be generated when the information\nin multiple sentences of the document is comprehensively\nconsidered.\n2) SUMMARIZATION RESULTS USING MULTI-ENCODER\nMODELS\nWe conducted an experiment using the flat and parallel\nmethod as a strategy for combining encoders, to which var-\nious Korean PLMs were applied. The seventh to thirteenth\nvalues in Table 3 represent multi-encoder models using the\nsame encoders multiple times. However, the approaches of\nVOLUME 11, 2023 48775\nY. Shin: Multi-Encoder Transformer for Korean Abstractive Text Summarization\nTABLE 4. Experimental results of utilizing the same PLM multiple times in the baseline model. The PLM was connected using both flat or parallel\nmethods. Bold text indicates the best case for each group. Underlined text denotes the best case model among the other models in this table. The symbol\n‡ is used to indicate statistically significant (p< 0.05) results compared to the highest value among single-encoder model results for each ROUGE score.\nsimply utilizing the same encoder multiple times (Multi-\nEncoder Model w/ Same Encoders) did not always yield\nstatistically significant results. For example, in the News\n(AI-Hub) dataset, the multi-encoder approach with the same\nencoders did not achieve statistically significant better results\ncompared to the single-encoder model. Additionally, while\nthere was improvement in some of the metrics (R1, R2,\nor RL) for the News (NIKL) dataset, there was no statistically\nsignificant improvement in all three metrics at the same time.\nAs shown in Table 4, when the same encoders are used N\ntimes, generally higher performance can be achieved com-\npared to the single-encoder models. We assumed the case\nwhere the highest value was achieved for R1 as the best case\nand reported it as ‘Multi-Encoder Model w/ Optimal Num-\nber of Same Encoders’ in Table 3. However, after reaching\nthe maximum score, there is a tendency for performance to\ndecrease with the addition of more encoders. Therefore, while\nthere is potential for performance improvement by utilizing\nmore encoding information, it can be speculated that there\nare limitations to using the same encoder multiple times.\nAccordingly, we propose a multi-encoder model based on\ndiverse PLMs, which demonstrates statistically significant\nperformance improvement. In Table 3, we present the results\nof the proposed model (Multi-Encoder Model w/ Diverse\nEncoders), which incorporates four Korean PLMs, i.e., M,\nKo, Han, and Kor, in the encoders. This model is denoted\nas M,Ko,Han,Kor (ALL), and its performance is shown in\nthe fourteenth to twentieth rows of the table. It is observed\nthat the approach of utilizing diverse encoders consistently\nleads to a statistically significant improvement in R1 and\nRL scores, especially for KoBERT-based parallel models.\nAdditionally, when only some of the four PLMs are used\nin an optimal combination, referred to as the Multi-Encoder\nModel w/ Optimal Combination of Diverse Encoders, there\nis a significant improvement in all three metrics, namely R1,\nR2, and RL.\n3) SUMMARIZATION RESULTS ON BEST\nCOMBINATION MODEL\nWe explore variant methods of using diverse PLMs in the\nmulti-encoder model, including combinations of only some\nof the four PLMs. Through our investigation, we examine\nthe potential for performance improvement based on different\nPLM combinations. As shown in Tables 5 and 7, combining\nM-BERT as an additional encoder for HanBERT-based mod-\nels results in improved performance. When exploring dataset\nLaw (AI-Hub) and News (NIKL), as shown in Table 2, these\nare relatively small size datasets. For both datasets, methods\nbased on HanBERT or M-BERT show good performance.\n48776 VOLUME 11, 2023\nY. Shin: Multi-Encoder Transformer for Korean Abstractive Text Summarization\nTABLE 5. ROUGE F1 results on the Law (AI-Hub) test set. A model variant\nof the HanBERT-based model, which is the best performing model among\nsingle-encoder models. Based on the model, an additional PLM is used in\nthe encoder and the performance changes are observed.\nTABLE 6. ROUGE F1 results on the News (AI-Hub) test set. A model\nvariant of the KoBERT-based model, which is the best performing model\namong single-encoder models. Based on the model, an additional PLM is\nused in the encoder and the performance changes are observed.\nTABLE 7. ROUGE F1 results on the News (NIKL) test set. A model variant\nof the MBERT-based model, which is the best performing model among\nsingle-encoder models. Based on the model, an additional PLM is used in\nthe encoder and the performance changes are observed.\nThis is likely due to the fact that HanBERT and M-BERT are\nPLMs trained on large-scale data, which allows them to be\neasily fine-tuned for different tasks, even with small amounts\nof training data. Among these two models, HanBERT-based\nmodels showed slightly better performance, possibly because\nthe tokenization units are morphemes and subwords, allowing\nfor the use of the most suitable tokens for Korean.\nFor the Law (AI-Hub) dataset, combining additional\nencoders to HanBERT-based models always improves perfor-\nmance compared to single-encoder model using HanBERT\nonly. However, for the News (NIKL) dataset, statistically\nsignificant improvements are observed only when using the\ncombination of M-BERT (Han,M) and utilizing all four\nPLMs (ALL). This is likely due to the relatively small size\nof the News (NIKL) dataset, which makes it challenging to\nfine-tune the PLM effectively, especially KoBERT.\nAs shown in Table 6, The experimental results suggest that\nfor the News (NIKL) dataset, which has a sufficient amount\nof training data, using Korean-specific PLMs performs better\nthan using M-BERT, which can provide additional exter-\nnal knowledge. Among them, models based on KoBERT\nshowed the best performance among the single-encoder mod-\nels. At this time, both Kor and Han use a morpheme as a token\nunit and we can conjecture that better summary sentences\ncan be generated when we additionally utilize these Korean-\nspecific tokens. When employing the combination of three\nPLMs, i.e., when Ko, Han, and Kor are used in the encoders\n(Ko,Han,Kor) shows outstanding performance.\nWe are able to conclude that when we encode the input\nsentence with multiple tokens using multiple PLMs, it leads\nto a better understanding of the model. M and Ko are both use\nsubword as the token unit and Han, and Kor use morpheme\n& subword, and a morpheme as the token unit, respectively.\nBoth KoBERT and M-BERT utilize subword tokenization,\nand therefore serve similar functions. It can be viewed that\nM-BERT supplements the limited training dataset by provid-\ning additional knowledge. Our experimental results demon-\nstrate the best performance because the same sentence is\nunderstood in multiple token units and the approach then\ngenerates a summary based on this understanding.\nF. ABLATION STUDY\n1) EVALUATION USING CONTEXTUAL EMBEDDING\nWe measure the quality generated result summaries using\nanother metric, BERTScore 15 [32]. Compared with ROUGE\nscore that calculate the exact match of tokens, BERTScore\nmeasures the similarity of tokens using contextual embed-\ndings. BERTScore leverages BERT to obtain contextual\nembedding values for tokens in both reference summaries and\ncandidate sentences. The score of BERTScore is calculated\nby using the cosine similarity of the two sentences using\nthe obtained vectors. Accordingly, BERTScore allows us to\nevaluate the quality of generated sentences. In the case of\nKorean, the score is measured using M-BERT as it belongs\nto others language. In this case, the score is calculated using\nM-BERT before fine-tuning to our task.\nAs shown in Table 8, for the average F1-score\nof BERTScore, the model extended to multi-encoder\noutperforms the single-encoder model. For the Law\n(AI-Hub) dataset, the experimental results show that uti-\nlizing the multi-encoder approach with diverse encoders or\nusing the same encoder multiple times in a multi-encoder\nmodel both yield better performance than using a single\nencoder. Similarly, for the News (AI-Hub) dataset, perfor-\nmance improvement is only observed when using diverse\nencoders. However, for the News (NIKL) dataset, although\nthere was a slight improvement in performance for models\nusing diverse encoders, statistically significant improvement\n15https://github.com/Tiiiger/bert_score\nVOLUME 11, 2023 48777\nY. Shin: Multi-Encoder Transformer for Korean Abstractive Text Summarization\nTABLE 8. BERTScore results for single-encoder models and multi-encoder models. Precision (P), recall (R), and F1-score (F1) are reported. The best F1\nscore for each group is indicated in bold, while the best F1 score model among the other models in the table is underlined. The symbol† is used to\nindicate statistically significant (p< 0.05) results compared to a single-encoder model using the same decoder, and‡ is used to indicate statistically\nsignificant (p< 0.05) results compared to the highest F1 score value among single-encoder model results.\nwas not observed compared to the HanBERT-based single-\nencoder model, which showed the best performance among\nsingle-encoder models. The method calculating the vector\nsimilarity utilizing language model shows same tendency to\nthe metric that measures exact match.\n2) QUALITATIVE ANALYSIS ON UNKNOWN TOKENS\nIn general, Korean documents are composed primarily of\nHangul (Korean characters), Hanja (Chinese characters),\nalphabets (English characters), numbers, special charac-\nters, and punctuation. The Law dataset (AI-Hub) includes\nHangul (93.30%), Hanja (0.18%), alphabets (0.21%), num-\nbers (2.82%), special characters (0.18%), and punctuation\n(3.31%) as the most frequently occurring types of charac-\nters. Similarly, the News dataset (AI-Hub) consists mainly\nof Hangul (88.43%), with smaller percentages of Hanja\n(0.03%), alphabets (1.32%), numbers (4.33%), special char-\nacters (1.66%), and punctuation (4.23%) as the dominant\ncategories. Lastly, for the News dataset (NIKL), Hangul\n(88.46%) is also the most common type of character, followed\nby Hanja (0.19%), alphabets (0.90%), numbers (3.94%), spe-\ncial characters (1.85%), and punctuation (4.65%). We inves-\ntigate the impact of Out-of-V ocabulary (OOV) tokens, which\nwere not recognized by the tokenizers of PLMs due to their\nabsence in the vocabulary and are represented by [UNK]\ntokens.\nTable 9 demonstrates that M-BERT typically utilizes only\nup to 13% of its vocabulary tokens when summarizing docu-\nments primarily written in Korean. This is because it employs\nthe WordPiece tokenizer with only 1,187 Korean syllables.\nAny token that does not match within a given token will be\ntreated as [UNK]. As a result, among PLMs, M-BERT has\nthe highest number of [UNK] tokens. Nevertheless, M-BERT\nhas a distinctive characteristic in that it has a substantial\nportion of tokens for Chinese characters or special characters\nin its vocabulary set, as it has been trained on various lan-\nguages. Regarding KoBERT, it has a vocabulary size of 8,002,\nenabling it to utilize most of its tokens for text processing.\nAdditionally, since it employs the SentencePiece tokenizer,\n[UNK] tokens are generally infrequent. Nearly 99% of the\ntokens are utilized, but since combining initial consonants,\nvowels, and final consonants can create a total of 11,133\npossible Korean syllables, there are still some syllables that\nare not yet covered by the KoBERT vocabulary. Moreover,\nthe Law dataset (AI-Hub) contains a significant number of\nChinese characters, resulting in the utilization of only 86% of\nthe tokens. For HanBERT, it applies morpheme segmentation\nto split the text into morphemes and then applies the Word-\nPiece tokenizer, enabling it to include a wide range of diverse\nKorean tokens. However, some special characters and certain\nChinese characters are not included in the vocabulary, result-\ning in them being treated as [UNK]. Furthermore, KorBERT\nalso utilizes morphological analysis to include a large number\n48778 VOLUME 11, 2023\nY. Shin: Multi-Encoder Transformer for Korean Abstractive Text Summarization\nTABLE 9. Total number of vocabulary tokens (Total) and matched tokens (Match) and its ratio for each PLM, along with the number of unmatched tokens\n(OOV, Out-of-Vocabulary).\nTABLE 10. Top 4 frequently unmatched token (OOV) examples in the\ndatasets for each PLM.\nof Korean tokens. However, as it merges the part-of-speech\ntags into a single token, it has the drawback of potentially\nincluding errors from the morphological analyzer.\nTable 10 represents the top 4 tokens that are most com-\nmonly mapped to [UNK] when PLMs are used for tokeniza-\ntion across different datasets. Specifically, M-BERT tends to\nrank Korean characters at the top since it has a limited number\nof Korean characters in its vocabulary set. For KoBERT, the\nLaw (AI-Hub) dataset has high rankings for Chinese char-\nacters due to its small vocabulary set, whereas some Korean\nsyllables are highly ranked in the News (AI-Hub) dataset and\nspecial characters and Chinese characters are highly ranked in\nthe News (NIKL) dataset. With respect to HanBERT, the Law\n(AI-Hub) dataset shows high rankings for Chinese characters\nand special characters, while the other datasets rank special\ncharacters highly. Regarding KorBERT, we observe that it\nincludes mapping errors to incorrect morphological tags, and\nChinese characters or special characters are highly ranked in\nthe Law dataset, punctuation marks and special characters are\nhighly ranked in the News (AI-Hub) dataset, and punctuation\nmarks and Chinese characters are highly ranked in the News\n(NIKL) dataset.\nFigure 4 shows a word cloud based on the frequency\nof tokens mapped to [UNK]. It shows a similar tendency\nas Table 10, where M-BERT tends to map most Korean\ncharacters to [UNK]. KoBERT, on the other hand, generally\ntreats Chinese characters and infrequent Korean syllables as\nOOV words. HanBERT usually treats special characters and\nsome Chinese characters as [UNK], while KorBERT typically\nFIGURE 4. From left to right, the figure shows a word cloud for the [UNK]\ntoken of M-BERT, followed by KoBERT, then HanBERT, and finally\nKorBERT. The word cloud visualization in the figure represents Hangul\n(Korean characters) in blue, Hanja (Chinese characters) in red, alphabets\n(English characters) and numbers in gray, other special characters in\ngreen, and punctuations in purple.\nregards punctuation marks and Chinese or special characters\nas [UNK].\nIt is observed that the types of tokens included in PLMs\nvary, which could explain the differing performance of model\nvariants on three datasets, as shown in Table 3. Moreover,\nthe performance variation could also be due to the composi-\ntion of each dataset. For example, the Law (AI-Hub) dataset\nconsists mostly of Korean characters with fewer special char-\nacters, making it easier for HanBERT to cover most of the\ntokens. Among M, Ko, and HanBERT, HanBERT-based mod-\nels generally show the best performance on this dataset as\nit has the lowest number of unique OOV tokens. On the\nNews (AI-Hub) dataset, KoBERT’s tokenizer generates the\nfewest OOV tokens. This dataset has a lower proportion of\nChinese characters and some special characters, enabling\nKoBERT-based models to cover more tokens and obtain\nbetter performance than other PLM-based models. For the\nNews (NIKL) dataset, HanBERT results in the fewest OOV\ntokens. However, due to the limited amount of training data,\nit appears that HanBERT may not have been fully trained.\nVOLUME 11, 2023 48779\nY. Shin: Multi-Encoder Transformer for Korean Abstractive Text Summarization\nTABLE 11. Example 1 generated on News (AI-Hub). Results of the single-encoder models (baseline) and KoBERT-based multi-encoder variations (ours)\nare shown. The underlined texts in article are the contents that are included in the gold summary.\nNonetheless, it can handle most of the Korean characters,\npunctuation marks, and Chinese characters in the dataset,\nleading to overall decent performance for HanBERT-based\nmodels.\nTherefore, it can be concluded that PLM with a lower\nfrequency of OOV tokens generally performs better on a\nspecific dataset. In particular, models based on KoBERT that\nutilize the SentencePiece tokenizer usually have a high token\nutilization rate and show decent performance. Furthermore,\nthe vocabulary set of PLMs with fewer OOV tokens differs\nfor each dataset, which could explain why the best performing\nmodel varies depending on the dataset. However, identifying\nthe best model still requires manual effort, which is a limita-\ntion of this study and an area for future research. Additionally,\nwe highlight that multi-encoder model that leverages diverse\nPLMs can lead to improved performance, as each PLM has\nunique strengths in handling tokens.\n3) QUALITATIVE ANALYSIS OF GENERATED SUMMARIES\nTables 11 and 12 show example summaries generated by\nour model variations. Among the single-encoder models\nusing M, Ko, or Han for the encoder, Ko is quantita-\ntively performing the best on the News (AI-Hub) dataset.\nTo find the reason for this, we investigate the gener-\nated results. When we observe the examples, the sen-\ntences generated by the model using Ko are likely to\ncontain important information in the reference summary,\nsuch as ‘\n(Typoon Danas)’, ‘\n (Gwanju\nand Jeonnam)’, and ‘\n (agricultural land flood-\ning)’. However, there is still missing information that is not\nincluded. For the other two models, in Tables 11 and 12, many\nother details are included in the results. For example, the\nsingle-encoder model with M includes redundant informa-\ntion, such as ‘\n(part\nof the old house’s gate and stone wall collapsed)’.\nIn some cases, the generated sentences are incorrect.\nFor example, the generated summary using the single-\nencoder model with Han includes the incorrect informa-\ntion of ‘\n’(Beethoven’s\nromantic ‘A Midsummer Night’s Dream’)’ Also, when\nwe generate a summary using the multi-encoder model,\nthe accuracy of the generated sentence is higher because\ninformation not previously included in the single-encoder\nmodel with Ko is included at this time. For exam-\nple, in Table 5, for the multi-encoder model with\nKo, Han, and Kor, which shows the best performance,\nit even contains the same details as the reference, such\nas ‘\n(1,026ha of agricultural land in Yeosu, Gangjin, Haenam, and\nGoheung was flooded)’. Therefore, we find that the multi-\nencoder models additionally include important information\nthat is missed with a single encoder. We also observe that the\nmulti-encoder models generate sentences more accurately.\n48780 VOLUME 11, 2023\nY. Shin: Multi-Encoder Transformer for Korean Abstractive Text Summarization\nTABLE 12. Example 2 generated on News (AI-Hub).\nVI. CONCLUSION\nIn this paper, we propose a Korean abstractive summarization\nmodel using multiple PLMs with a multi-encoder approach.\nFor PLMs, much time and effort are required for pre-training.\nAlso, each different PLM has its own strengths. In particular,\nin the case of Korean, there are PLMs trained based on\ndifferent token units, such as subwords and morphemes. It is\nmore likely that we can achieve better performance if we\nuse them at the same time rather than choosing only one\nfor summarization. Our experiments on Korean judgment\nand two news datasets show that the proposed multi-encoder\nvariant models outperform both the traditional single-encoder\nmodels and the multi-encoder models. Moreover, we improve\nthe performance by using Korean-specific PLMs trained on\nonly a Korean corpus with various tokens. In future work,\nwe plan to investigate combination strategies for multiple\nPLMs in depth. Also, we plan to extend our model to a model\nthat utilizes not only BERT-based models, but also many\nother types of PLMs.\nACKNOWLEDGMENT\nThis work used Korean Language Model (KorBERT) pro-\nvided by ETRI (No.2013-2-00131, Development of Knowl-\nedge Evolutionary WiseQA Platform Technology for Human\nKnowledge Augmented Services).\nREFERENCES\n[1] E. Hovy, ‘‘Automated text summarization,’’ in The Oxford Handbook\nof Computational Linguistics . Oxford, U.K.: Oxford Univ. Press, 2005,\nch. 32, pp. 583–598, doi: 10.1093/oxfordhb/9780199276349.013.0032.\n[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., 2017, pp. 5998–6008.\n[3] Y . Liu and M. Lapata, ‘‘Text summarization with pretrained encoders,’’ in\nProc. Conf. Empirical Methods Natural Lang. Process. 9th Int. Joint Conf.\nNatural Lang. Process. (EMNLP-IJCNLP), 2019, pp. 3730–3740.\n[4] A. Sahu and S. G. Sanjeevi, ‘‘Better fine-tuning with extracted important\nsentences for abstractive summarization,’’ in Proc. Int. Conf. Commun.,\nControl Inf. Sci. (ICCISc), Jun. 2021, pp. 11328–11339.\n[5] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou,\nW. Li, and P. J. Liu, ‘‘Exploring the limits of transfer learning with a unified\ntext-to-text transformer,’’ J. Mach. Learn. Res., vol. 21, pp. 5485–5551,\nJan. 2020.\n[6] M. Lewis, Y . Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,\nV . Stoyanov, and L. Zettlemoyer, ‘‘BART: Denoising sequence-to-\nsequence pre-training for natural language generation, translation, and\ncomprehension,’’ inProc. 58th Annu. Meeting Assoc. Comput. Linguistics,\n2020, pp. 7871–7880.\n[7] W. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, ‘‘Unified pre-\ntraining for program understanding and generation,’’ in Proc. Conf. North\nAmer. Chapter Assoc. Comput. Linguistics, Human Lang. Technol., 2021,\npp. 13063–13075.\n[8] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ in Proc.\nNAACL-HLT, 2018, pp. 4171–4186.\n[9] J. J. Song, The Korean Language: Structure, Use and Context. Evanston,\nIL, USA: Routledge, 2006.\n[10] J. Libovický, J. Helcl, and D. Marecek, ‘‘Input combination strategies for\nmulti-source transformer decoder,’’ in Proc. 3rd Conf. Mach. Transl., Res.\nPapers, 2018, pp. 253–260.\nVOLUME 11, 2023 48781\nY. Shin: Multi-Encoder Transformer for Korean Abstractive Text Summarization\n[11] A. M. Rush, S. Chopra, and J. Weston, ‘‘A neural attention model for\nabstractive sentence summarization,’’ in Proc. Conf. Empirical Methods\nNatural Lang. Process., 2015, pp. 379–389.\n[12] S. Chopra, M. Auli, and A. M. Rush, ‘‘Abstractive sentence summa-\nrization with attentive recurrent neural networks,’’ in Proc. Conf. North\nAmer. Chapter Assoc. Comput. Linguistics, Human Lang. Technol., 2016,\npp. 93–98.\n[13] R. Nallapati, B. Zhou, C. Dos Santos, C. Gulcehre, and B. Xiang,\n‘‘Abstractive text summarization using sequence-to-sequence RNNs and\nbeyond,’’ in Proc. 20th SIGNLL Conf. Comput. Natural Lang. Learn.,\n2016, pp. 280–290.\n[14] Q. Chen, X. Zhu, Z. Ling, S. Wei, and H. Jiang, ‘‘Distraction-based neural\nnetworks for document summarization,’’ 2016, arXiv:1610.08462.\n[15] J. Gu, Z. Lu, H. Li, and V . O. K. Li, ‘‘Incorporating copying mechanism\nin sequence-to-sequence learning,’’ in Proc. 54th Annu. Meeting Assoc.\nComput. Linguistics, 2016, pp. 1631–1640.\n[16] A. See, P. J. Liu, and C. D. Manning, ‘‘Get to the point: Summarization with\npointer-generator networks,’’ in Proc. 55th Annu. Meeting Assoc. Comput.\nLinguistics, 2017, pp. 1073–1083.\n[17] A. Celikyilmaz, A. Bosselut, X. He, and Y . Choi, ‘‘Deep commu-\nnicating agents for abstractive summarization,’’ in Proc. Conf. North\nAmer. Chapter Assoc. Comput. Linguistics, Human Lang. Technol., 2018,\npp. 1662–1675.\n[18] R. Paulus, C. Xiong, and R. Socher, ‘‘A deep reinforced model for abstrac-\ntive summarization,’’ inProc. Int. Conf. Learn. Represent., 2018, pp. 1–12.\n[19] S. Gehrmann, Y . Deng, and A. Rush, ‘‘Bottom-up abstractive summariza-\ntion,’’ in Proc. Conf. Empirical Methods Natural Lang. Process., 2018,\npp. 4098–4109.\n[20] X. Zhou, E. Yilmaz, Y . Long, Y . Li, and H. Li, ‘‘Multi-encoder–decoder\ntransformer for code-switching speech recognition,’’ in Proc. Interspeech,\nOct. 2020, pp. 1042–1046.\n[21] T. Lohrenz, Z. Li, and T. Fingscheidt, ‘‘Multi-encoder learning and stream\nfusion for transformer-based end-to-end automatic speech recognition,’’\n2021, arXiv:2104.00120.\n[22] S. Dalmia, Y . Liu, S. Ronanki, and K. Kirchhoff, ‘‘Transformer-transducers\nfor code-switched speech recognition,’’ in Proc. IEEE Int. Conf. Acoust.,\nSpeech Signal Process. (ICASSP), Jun. 2021, pp. 5859–5863.\n[23] J. Libovický and J. Helcl, ‘‘Attention strategies for multi-source sequence-\nto-sequence learning,’’ in Proc. 55th Annu. Meeting Assoc. Comput. Lin-\nguistics, 2017, pp. 196–202.\n[24] J. Shin and J.-H. Lee, ‘‘Multi-encoder transformer network for automatic\npost-editing,’’ in Proc. 3rd Conf. Mach. Transl., Shared Task Papers, 2018,\npp. 840–845.\n[25] P. Littell, C.-K. Lo, S. Larkin, and D. Stewart, ‘‘Multi-source transformer\nfor Kazakh-Russian-English neural machine translation,’’ in Proc. 4th\nConf. Mach. Transl., 2019, pp. 267–274.\n[26] B. Li, H. Liu, Z. Wang, Y . Jiang, T. Xiao, J. Zhu, T. Liu, and C. Li,\n‘‘Does multi-encoder help? A case study on context-aware neural machine\ntranslation,’’ in Proc. 58th Annu. Meeting Assoc. Comput. Linguistics,\n2020, pp. 3512–3518.\n[27] W. Lee, J. Shin, and J.-H. Lee, ‘‘Transformer-based automatic post-editing\nmodel with joint encoder and multi-source attention of decoder,’’ in Proc.\n4th Conf. Mach. Transl., 2019, pp. 112–117.\n[28] Y . Liu, ‘‘Fine-tune BERT for extractive summarization,’’ 2019,\narXiv:1903.10318.\n[29] C.-Y . Lin, ‘‘ROUGE: A package for automatic evaluation of summaries,’’\nin Text Summarization Branches Out. Barcelona, Spain: Association for\nComputational Linguistics, 2004, pp. 74–81.\n[30] D. P. Kingma and J. Ba, ‘‘Adam: A method for stochastic optimization,’’\n2014, arXiv:1412.6980.\n[31] J. Zhang, Q. Liu, and J. Zhou, ‘‘ME-MD: An effective framework for neural\nmachine translation with multiple encoders and decoders,’’ in Proc. 26th\nInt. Joint Conf. Artif. Intell., Aug. 2017, pp. 3392–3398.\n[32] T. Zhang, V . Kishore, F. Wu, K. Q. Weinberger, and Y . Artzi, ‘‘BERTScore:\nEvaluating text generation with BERT,’’ in Proc. Int. Conf. Learn. Repre-\nsent., 2019, pp. 1–43.\nYOUHYUN SHIN received the B.S. degree in\ncomputer science education from Korea Univer-\nsity, in 2013, and the Ph.D. degree in computer\nscience and engineering from Seoul National Uni-\nversity, in 2019. Since 2020, she has been an Assis-\ntant Professor with the Department of Computer\nScience and Engineering, Incheon National Uni-\nversity. Her research interests include natural lan-\nguage processing, spoken language understanding,\nand technology driven learning.\n48782 VOLUME 11, 2023"
}