{
    "title": "Multi-Granularity Structural Knowledge Distillation for Language Model Compression",
    "url": "https://openalex.org/W4285269381",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A209039771",
            "name": "Chang Liu",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2585202595",
            "name": "Chongyang Tao",
            "affiliations": [
                "Microsoft (Finland)"
            ]
        },
        {
            "id": "https://openalex.org/A2950167766",
            "name": "Jiazhan Feng",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2098869281",
            "name": "Dongyan Zhao",
            "affiliations": [
                "Convergence",
                "Peking University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W3105966348",
        "https://openalex.org/W3173374050",
        "https://openalex.org/W2951299559",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W3101066076",
        "https://openalex.org/W2978670439",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2963140444",
        "https://openalex.org/W3095273266",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W3098576111",
        "https://openalex.org/W2970454332",
        "https://openalex.org/W2866343820",
        "https://openalex.org/W2995607862",
        "https://openalex.org/W1690739335",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2982242214",
        "https://openalex.org/W3008374555",
        "https://openalex.org/W3177265267",
        "https://openalex.org/W3038012435",
        "https://openalex.org/W3174544005",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3011411500",
        "https://openalex.org/W4287901267",
        "https://openalex.org/W3015609966",
        "https://openalex.org/W3198658868",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2948947170",
        "https://openalex.org/W2986015886",
        "https://openalex.org/W3199246732",
        "https://openalex.org/W131533222",
        "https://openalex.org/W3104033643",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2998183051",
        "https://openalex.org/W1599016936",
        "https://openalex.org/W3173482217",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W2975381464"
    ],
    "abstract": "Transferring the knowledge to a small model through distillation has raised great interest in recent years. Prevailing methods transfer the knowledge derived from mono-granularity language units (e.g., token-level or sample-level), which is not enough to represent the rich semantics of a text and may lose some vital knowledge. Besides, these methods form the knowledge as individual representations or their simple dependencies, neglecting abundant structural relations among intermediate representations. To overcome the problems, we present a novel knowledge distillation framework that gathers intermediate representations from multiple semantic granularities (e.g., tokens, spans and samples) and forms the knowledge as more sophisticated structural relations specified as the pair-wise interactions and the triplet-wise geometric angles based on multi-granularity representations. Moreover, we propose distilling the well-organized multi-granularity structural knowledge to the student hierarchically across layers. Experimental results on GLUE benchmark demonstrate that our method outperforms advanced distillation methods.",
    "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 1001 - 1011\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nMulti-Granularity Structural Knowledge Distillation for\nLanguage Model Compression\nChang Liu1,2, Chongyang Tao3∗, Jiazhan Feng1, Dongyan Zhao1,2,4,5∗\n1Wangxuan Institute of Computer Technology, Peking University\n2Center for Data Science, Peking University\n3Microsoft Corporation\n4Artificial Intelligence Institute of Peking University\n5State Key Laboratory of Media Convergence Production Technology and Systems\n{liuchang97,fengjiazhan,zhaody}@pku.edu.cn,\nchotao@microsoft.com\nAbstract\nTransferring the knowledge to a small model\nthrough distillation has raised great interest in\nrecent years. Prevailing methods transfer the\nknowledge derived from mono-granularity lan-\nguage units (e.g., token-level or sample-level),\nwhich is not enough to represent the rich seman-\ntics of a text and may lose some vital knowl-\nedge. Besides, these methods form the knowl-\nedge as individual representations or their sim-\nple dependencies, neglecting abundant struc-\ntural relations among intermediate representa-\ntions. To overcome the problems, we present\na novel knowledge distillation framework that\ngathers intermediate representations from mul-\ntiple semantic granularities (e.g., tokens, spans\nand samples) and forms the knowledge as more\nsophisticated structural relations specified as\nthe pair-wise interactions and the triplet-wise\ngeometric angles based on multi-granularity\nrepresentations. Moreover, we propose distill-\ning the well-organized multi-granularity struc-\ntural knowledge to the student hierarchically\nacross layers. Experimental results on GLUE\nbenchmark demonstrate that our method out-\nperforms advanced distillation methods.\n1 Introduction\nRecent years have witnessed a surge of pre-trained\nlanguage models (Devlin et al., 2019; Lewis et al.,\n2020; Clark et al., 2020; Brown et al., 2020). Build-\ning upon the transformer architecture (Vaswani\net al., 2017) and pre-trained on large-scale cor-\npora using self-supervised objectives, these PLMs\nhave achieved remarkable success in a wide range\nof natural language understanding and generation\ntasks. Despite their high performance, these PLMs\nusually suffer from high computation and memory\ncosts, which hinders them from being deployed\n∗Corresponding authors: Chongyang Tao and Dongyan\nZhao.\ninto resource-scarce scenarios, e.g., mobile phones\nand embedded devices.\nVarious attempts have been made to compress\nthe huge PLMs into small ones with minimum\nperformance degradation. As one of the main\napproaches, knowledge distillation (Hinton et al.,\n2015) utilizes a large and powerful teacher model\nto transfer the knowledge to a small student model.\nBased on the teacher-student framework, Jiao et al.\n(2020); Wang et al. (2020) distilled the token-level\nrepresentations and attention dependencies to the\nstudent, Sanh et al. (2019); Sun et al. (2019) taught\nthe student to mimic the output logits of the teacher,\nSun et al. (2020) enforced the student’s represen-\ntation to be closed to the teacher’s while pushing\nnegative samples to be far apart. Although proved\neffective, existing approaches have some flaws. For\none thing, these distillation methods only adopted\nthe representations of mono-granularity language\nunits (i.e., token-level or sample-level), while ne-\nglecting other granularity. For another, their distil-\nlation objectives either matched the corresponding\nrepresentations between the teacher and the stu-\ndent or aligned the attention dependencies, failing\nto capture more sophisticated structural relations\nbetween the representations.\nTo address these issues, in this paper we pro-\npose a novel knowledge distillation framework\nnamed Multi-Granularity Structural Knowledge\nDistillation (MGSKD) through answering the three\nresearch questions: (1) which granularity should\nthe knowledge be, (2) what form of knowledge\nis effective to transfer and (3) how to teach the\nstudent using the knowledge. For the “which” ques-\ntion, given that natural languages have multiple\nsemantic granularities, we consider the intermedi-\nate representations in three granularities: tokens,\nspans and samples. Specifically, we first take the\nsub-word tokens as the smallest granularity, then\n1001\nselect phrases and whole words as spans for they\nhold complete meanings, and finally treat the whole\ninput texts as samples. We use mean-pooling to\nobtain the representations of spans and samples\nbased on token representations. For the “ what”\nquestion, we propose to leverage the sophisticated\nstructural relations between the representations as\nthe knowledge. Concretely, instead of aligning the\ncorresponding representations of the teacher and\nthe student, we propose to form the knowledge as\nthe pair-wise interactions and the triplet-wise geo-\nmetric angels of a group of representations. For the\n“how” question, following the recent findings that\nthe bottom layers capture syntactic features while\nthe upper layers encode semantic features (Jawahar\net al., 2019), we conduct hierarchical distillation\nwhere the bottom layers of the student are taught\ntoken-level and span-level knowledge while the\nupper layers learn sample-level knowledge.\nWe conduct comprehensive experiments on\nstandard language understanding benchmark\nGLUE (Wang et al., 2018). Experimental results\ndemonstrate that our knowledge distillation frame-\nwork outperforms strong baselines methods. Sur-\nprisingly, MGSKD achieves comparable or better\nperformance than BERTbase on most of the tasks\non GLUE, while keeping much smaller and faster.\nOur contributions in this paper are three folds:\n• We are the first to leverage multi-granularity se-\nmantic representations in language (i.e., the repre-\nsentations of tokens, spans and samples) for knowl-\nedge distillation.\n• We propose to form the knowledge as sophisti-\ncated structural relations specified as the pair-wise\ninteractions and the triplet-wise geometric angles\nbased on multi-granularity representations.\n• We conduct comprehensive experiments on\nGLUE benchmark and MGSKD achieves superior\nresults over other knowledge distillation baselines.\n2 Related Work\nLanguage Model Compression. Pre-trained lan-\nguage models (Devlin et al., 2019; Clark et al.,\n2020; Brown et al., 2020) perform remarkably well\non various applications but at the cost of high com-\nputation and memory usage. To deploy these pow-\nerful models into resource-scarce scenarios, var-\nious attempts have been made to compress the\nlanguage models into small ones. Quantization\nmethods (Zafrir et al., 2019; Shen et al., 2020;\nZhang et al., 2020; Bai et al., 2021) convert the\nmodel parameters to lower precision. Pruning ap-\nproaches identify then remove unimportant individ-\nual weights or structures (Michel et al., 2019; Fan\net al., 2019; Gordon et al., 2020; Hou et al., 2020).\nWeight sharing techniques (Dehghani et al., 2018;\nLan et al., 2019) allow the model to reuse the trans-\nformer layer multiple times to reduce parameters.\nKnowledge Distillation. Knowledge distilla-\ntion (Hinton et al., 2015) is another major line of re-\nsearch to do model compression, which is the main\nconcentration in this paper. Hinton et al. (2015)\nfirst proposed to minimize the KL-divergence be-\ntween the predicted distributions of the teacher\nand the student. Sanh et al. (2019); Sun et al.\n(2019); Liang et al. (2020) adopted this objective\nto teach the student on masked language model-\ning or text classification tasks. Romero et al.\n(2014) proposed to directly match the feature acti-\nvations of the teacher and the student. Jiao et al.\n(2020) followed the idea and took the intermedi-\nate representations in each transformer layer of\nthe teacher as one of the knowledge to be trans-\nferred. Tian et al. (2019) proposed a contrastive\ndistillation framework where the teacher’s represen-\ntations were treated as positives to the correspond-\ning student’s representations. Sun et al. (2020);\nFu et al. (2021) customized this idea to language\nmodel compression and proved its effectiveness.\nResearchers also attempted to use the mutual rela-\ntions of representations as the knowledge to trans-\nfer. In the literature of image classification, Peng\net al. (2019); Tung and Mori (2019); Park et al.\n(2019) pointed out that the relations of the image\nrepresentations of the teacher should be preserved\nin the student’s feature space, and adopted a series\nof geometric measurements to model the sample\nrelations. For distilling transformer models, Park\net al. (2021) enforced the relations across tokens\nand layers between the teacher and the student to\nbe consistent. Jiao et al. (2020); Wang et al. (2020,\n2021) used the attention dependencies between to-\nkens to teach the student. In this paper, we propose\nto transfer the multi-granularity knowledge to the\nstudent. Different from previous works that only\nconsidered a single granularity of representations,\nwe jointly transfer the token-level, span-level and\nsample-level structural knowledge. And compared\nwith Shao and Chen (2021) which considered the\nmulti-granularity visual features in an image as the\nknowledge, our method works in a different modal-\nity, presents a different definition of granularity,\n1002\npolic-emanJackieChanactedasa inaseriesofactionmovies.\nJackie Chana policemana series ofactionmovies\nJackie Chan acted as a policeman in a series of action movies....\n...\nBatch Dimension Sample Granularity\nJackie Chan acted as a policeman in a series of action movies.\n...\n...\nToken GranularitySpan Granularity\nBatch Dimension\nBatch Inputs\nMulti-head Relations\n...\nBatch Inputs\n......\nTeacher StudentMulti-granularity RepresentationsStructural Knowledge\nDistill\nDistill\nMulti-head Relations\nJackie Chana policemana series ofactionmovies\npolic-emanJackieChanactedasa inaseriesofactionmovies.\nFigure 1: The overall framework of MGSKD.\nand prepares the multi-granularity knowledge as\nthe structural relations among representations.\n3 Method\nWe propose Multi-Granularity Structural\nKnowledge Distillation, a novel framework to\ndistill the knowledge from a large transformer\nlanguage model to a small one. Different from\nprevious works that transferred the knowledge\nderived from either token-level or sample-level\noutputs, we prepare the knowledge in three\nsemantic granularities: token-level, span-level\nand sample-level. Given some granularity of\nrepresentations of the teacher model, we form\nthe knowledge as the structural relations, i.e.,\nthe pair-wise interactions and the triplet-wise\ngeometric angles, between the representations. We\nthen distill the well-organized structural knowledge\nto the student hierarchically across layers, where\nthe token-level and the span-level knowledge\nare transferred to the bottom layers to provide\nmore syntactic guidance while the sample-level\nknowledge is transferred to the upper layers to\noffer more help of semantic understanding. The\nframework of MGSKD is illustrated in Figure 1.\n3.1 Multi-granularity Representation\nNatural languages have multiple granularities of\nconceptual units. In the context of pre-trained\ntransformers (Devlin et al., 2019), the basic unit is\nthe tokens produced by sub-word tokenizers (Wu\net al., 2016; Radford et al., 2019). Several consec-\nutive tokens become a text span, and the sample\nis comprised of all the tokens it contains. Exist-\ning knowledge distillation approaches (Jiao et al.,\n2020; Wang et al., 2020; Sun et al., 2020; Fu et al.,\n2021) focused on one granularity of representation,\nneglecting that texts are built upon language units\nfrom multiple granularities. Intuitively, incorporat-\ning multi-granularity representations in knowledge\ndistillation may provide more guidance since the\nstudent can be taught how to compose the semantic\nconcepts from small granularities to larger ones.\nTherefore, we propose to gather multi-granularity\nrepresentations for knowledge distillation. We con-\nstruct three granularities of representations: tokens,\nspans that hold complete meanings, and samples.\nToken Representation. The first granularity is\nthe sub-word token, which is the foundation of\nhigh-level granularity. Given an input text, a tok-\nenizer such as WordPiece (Wu et al., 2016) splits\nit into n tokens x = [t1, t2, . . . , tn]. The tokens\nare converted to a sequence of continuous repre-\nsentations E = [e1, e2, . . . ,en] ∈ Rn×d through\nthe embedding layer. For the sake of clarity, we\ntreat the embedding layer as the 0-th layer and\nset H0 = E. Then the token embeddings H0\nare passed to L stacked transformer layers. The\nl-th layer takes the output representations Hl−1\nof the previous layer as its input, and returns the\nupdated representations Hl using multi-head at-\ntention (MHA) and position-wise feed-forward net-\nwork (FFN). Herein, we obtainL+1 layers of token\n1003\nrepresentations {Hl}L\nl=0 where Hl ∈ Rn×d.\nSpan Representation. The second granularity is\nthe span, which is comprised of several consecu-\ntive tokens. Different from SpanBERT (Joshi et al.,\n2020) that randomly selects token spans whose start\npositions and lengths are sampled from some dis-\ntributions for masked language modeling, we pro-\npose to extract spans that have complete meanings.\nWidely adopted sub-word tokenizers in pre-trained\ntransformers split some of the English words into\nseveral sub-word tokens. We consider these whole\nwords consisting of multiple sub-word tokens, and\nphrases, as meaningful spans. Sub-word tokens for\nwhole words are easy to obtain using WordPiece\ntokenizer (Wu et al., 2016). While for phrase iden-\ntification, we train a classifier-based English chun-\nker on CoNLL-2000 corpus (Tjong Kim Sang and\nBuchholz, 2000) following the instructions1. We\nthen use the trained chunker to extract noun phrases\n(NP), verb phrases (VP), and prepositional phrases\n(PP). These identified phrases are tokenized by\nWordPiece tokenizer to obtain tokens. Herein, we\ncan obtain ns token spans xspan = [s1, s2, . . . , sns ],\nwhere si = [tj, tj+1, . . . , tj+nsi −1] denotes the i-\nth span that starts at the j-th token and contains nsi\ntokens. We then build span representations based\non token representations using mean pooling:\nˆhl\ni = Pool(Hl\nj:j+nsi\n), (1)\nwhere ˆhl\ni ∈ Rd is the representation of the i-th\nspan in layer l. We obtain L + 1layers of span\nrepresentations as { ˆHl}L\nl=0 where ˆHl ∈ Rns×d.\nSample Representation. The third granularity is\nthe input text sample itself. Based on token rep-\nresentations again, we use mean-pooling to aggre-\ngate all the token representations in a text sample\nto form sample representation:\n˜hl = Pool(Hl), (2)\nHerein, we get L + 1layers of sample representa-\ntions as {˜hl}L\nl=0 where ˜hl ∈ Rd.\n3.2 Structural Knowledge Extraction\nWith multi-granularity representations, we then\nneed to formulate the specific knowledge we aim\nto transfer from the teacher to the student. Con-\nsidering that an element holds its meaning only\nwhen it is put into a semantic space where it has\n1https://www.nltk.org/book/ch07.html\nvarious relations to other elements, we propose\nthat the knowledge is better specified as the struc-\ntural relations of the representations in a seman-\ntic space, instead of the individual representations\nthemselves. Therefore, instead of directly match-\ning each hidden representation between the teacher\nand the student, we propose to extract structural\nrelations from multi-granularity representations as\nthe knowledge to teach the student. We first project\nthe representations into multiple sub-spaces, then\nwe extract two types of structural knowledge: pair-\nwise interactions and triplet-wise geometric angles.\nMulti-head Modeling. A recent study by Wang\net al. (2021) pointed out that distilling knowl-\nedge with multiple relation heads helps the student\nlearn better. Therefore, before extracting struc-\ntural knowledge for intermediate representations,\nwe first project them into m sub-spaces, which\nwe call multi-head modeling. Specifically, given\na set of n representations R ∈ Rn×d, we linearly\nproject them into m sub-spaces whose dimensions\nare d/m. 2 We use R′ ∈ Rm×n×d/m to denote the\nmulti-head representations which are then used for\nextracting structural knowledge.\nPair-wise Interaction. Given two vectors\nri, rj ∈ Rd/m in a sub-space, we calculate their\ninteraction as their scaled dot product:\nφ(ri, rj) =\nri · r⊺\njp\nd/m\n. (3)\nHerein, we obtain the multi-head pair-wise inter-\naction features for each pair as P ∈ Rm×n×n,\nwhere Ph,i,j denotes the interaction between the\ni-th representation and the j-th representation in\nthe sub-space of the h-th relation head. Note that\nP can be considered as the unnormalized self-\nattention (Vaswani et al., 2017) scores for the given\nrepresentations, the difference lies in that in our\ncalculation the queries are identical to the keys.\nTriplet-wise Geometric Angle. Pair-wise inter-\naction features only consider two vectors at once,\nwhich is not enough to represent the complicated\nstructural relations between representations in the\nhigh-dimensional space. Therefore, we propose\nto model the high-order relations as the geometric\nangles for triplets of vectors. Specifically, given\n2For the student model, its representations are linearly\nprojected into intermediate states whose dimensions are the\nsame as the teacher model’s hidden dimensions, so that it can\nbe split into m sub-spaces as the teacher model.\n1004\na triplet of representations ri, rj, rk ∈ Rd/m, we\ncalculate their geometric angle as:\nψ(ri, rj, rk) =cos∠rirjrk = ⟨rij, rkj⟩\nrij = ri − rj\n∥ri − rj∥2\n, rkj = rk − rj\n∥rk − rj∥2\n. (4)\nWe can calculate the geometric angles for all the\ntriplets, and obtain T ∈ Rm×n×n×n where Th,i,j,k\nstands for the angle of ∠rirjrk in the sub-space\nof the h-th relation head. As the computation com-\nplexity increases cubically with n, such a calcu-\nlation is infeasible when the number of represen-\ntations is large. Hereby, we propose a two-stage\nselection strategy to sequentially select important\nrepresentations to form angles. Similar to Goyal\net al. (2020), we assume that the more attention a\nrepresentation receives from others, the more im-\nportant it is. Therefore, we first calculate the self-\nattention distributions A ∈ Rm×n×n by applying\nsoftmax function on the last dimension of P. Then\nfor the j-th representation, we calculate a global\nsalient score sj by summing up self-attention dis-\ntributions across all heads and all queries. Based\non the score, we pick the top-k1 salient representa-\ntions as vertices. Next, if the i-th representation is\nselected as vertex, we pick k2 representations with\nthe highest local salient score to form angles with\nthe vertex. We define the local salient score si,j\nas the attention posed by the i-th representation on\nthe j-th representation, The salient scores si and\nsi,j are calculated as follows:\nsj =\nmX\nh=1\nnX\ni=1\nAh,i,j, si,j =\nmX\nh=1\nAh,i,j. (5)\nTherefore, by sequentially selecting salient repre-\nsentations to form angles, we reduce the computa-\ntion complexity from O(mn3) to O(mk1k2\n2). By\nchoosing proper k2 and k2, we can facilitate the\ncomputation of triplet-wise geometric angles for\nany number of representations.\n3.3 Hierarchical Distillation\nWe utilize the structural knowledge extraction ap-\nproach described in Sec. 3.2 to prepare knowledge\nbased on three granularities of representations pre-\nsented in Sec. 3.1 for distillation. Based on the\nfindings that the bottom layers capture syntactic\nfeatures while the upper layers encode semantic\nfeatures (Jawahar et al., 2019), we propose to con-\nduct hierarchical distillation for the student where\ndifferent granularities of knowledge are transferred\nto different layers. For a teacher model with Lt\nlayers and a student model with Ls layers, we first\ndefine a layer mapping functiong(·) that maps each\nstudent layer to a teacher layer that it learns from.\nFollowing previous work (Jiao et al., 2020), we\nadopt the “uniform strategy” for g(·). Then we\ntransfer token-level and span-level knowledge to\nthe bottom-M layers of the student, while lever-\naging sample-level knowledge to teach its upper\nLs + 1− M layers.\nToken- and Span-level. Specifically, given the\ntoken-level and the span-level representations of\nthe teacher {Hl\nt, ˆHl\nt}Lt\nl=0, we use Eq. 3 and Eq. 4\nto calculate the pair-wise interactions and the\ntriplet-wise geometric angles among tokens and\nspans within a single sample as {Pl\nt , ˆPl\nt }Lt\nl=0 and\n{Tl\nt , ˆTl\nt }Lt\nl=0. Similarly, we can obtain the struc-\ntural relations of the students: {Pl\ns, ˆPl\ns}Ls\nl=0 and\n{Tl\ns, ˆTl\ns}Ls\nl=0. We then teach the student by mini-\nmizing the differences of the structural relations\namong their representations between the teacher\nand the student:\nLtoken =\nX\n0≤l<M\n(ℓ1(Pg(l)\nt , Pl\ns) +ℓ2(Tg(l)\nt , Tl\ns))\nLspan =\nX\n0≤l<M\n(ℓ1( ˆPg(l)\nt , ˆPl\ns) +ℓ2( ˆTg(l)\nt , ˆTl\ns)).\n(6)\nSample-level. Recall that we obtain {˜hl\nt}Lt\nl=0 and\n{˜hl\ns}Ls\nl=0 for the teacher and the student where\n˜hl\nt, ˜hl\ns ∈ Rd. Different from the structural knowl-\nedge of tokens and spans which is modeled within\na sample, the sample-level structural relations rely\non a group of sample representations. Although\nthe choice of samples may make a difference to\nthe overall performance, here we simply gather\nall the sample representations in a mini-batch to\ncalculate their structural relations as the sample-\nlevel knowledge. Specifically, we only focus on\nthe triplet-wise relations { ˜Tl\nt }Lt\nl=0 and { ˜Tl\ns}Ls\nl=0:\nLsample =\nX\nM≤l≤Ls\nℓ2( ˜Tg(l)\nt , ˜Tl\ns). (7)\nℓ1 and ℓ2 in Eq. 6 and Eq. 7 are loss functions\nthat measure the distance between the structural\nrelations of the teacher’s and the student’s repre-\nsentations. We empirically choose MSE for ℓ1 and\nHuber loss (δ = 1) for ℓ2.\n1005\nModel #Params Speedup SST-2 MRPC RTE STS-B MNLI-(m/mm) QNLI QQP CoLA\n(Acc) (F1) (Acc) (Spear) (Acc) (Acc) (Acc) (Mcc)\nBERTbase 109M ×1.0 92.8 90.3 65.3 88.4 84.6/84.4 91.3 91.2 56.8\nELECTRAbase 109M ×1.0 95.5 92.7 83.4 91.0 88.8/88.7 93.2 92.0 69.6\nDistilBERT 66M ×2.0 91.3 - 59.9 86.9 82.2/ - 89.2 88.5 51.3\nMiniLMv2 66M ×2.0 92.4 - 72.1 - 84.2/ - 90.8 91.1 52.5\nCKD 66M ×2.0 93.0 89.6 67.3 89.0 83.6/84.1 90.5 91.2 55.1\nStudentft 14M ×9.4 89.7 88.0 63.7 84.6 80.2/79.8 86.0 86.9 0.0\nStudent†\nMiniLMv2 14M ×9.4 92.9 90.3 67.1 88.7 83.7/83.4 89.5 90.9 43.5\nStudent†\nCKD 14M ×9.4 92.8 89.9 66.8 88.7 83.2/82.7 89.3 90.3 46.4\nStudent†\nMGSKD 14M ×9.4 93.7 90.7 67.9 89.2 84.7/84.3 89.6 91.6 44.8\nTable 1: Evaluation results on the dev set of GLUE benchmark. The results of the models with 66M parameters are\ntaken from published papers. Our results are averaged for 3 runs with different random seeds. The best results of\nthe student models are in-bold. † means the method is implemented with the same distillation setting as ours.\nOverall Objectives. The overall distillation ob-\njective for multi-granularity structural knowledge\ndistillation is:\nL1 = λ1Lsample + λ2Ltoken + λ3Lspan, (8)\nwhere λ1, λ2 and λ3 are weights of loss functions\nof different granularities.\nAfter this, we also teach the student to match the\nprediction distributions with the teacher’s for text\nclassification tasks:\nL2 = τ2DKL(zt/τ∥zs/τ), (9)\nwhere zt and zs are the predicted probability distri-\nbutions of the teacher and the student respectively,\nτ denotes the temperature.\n4 Experiments\n4.1 Datasets and Metrics\nWe conduct our experiments on the General Lan-\nguage Understanding Evaluation (GLUE) bench-\nmark (Wang et al., 2018). Sepcifically, there are 2\nsingle-sentence tasks: SST-2 (Socher et al., 2013),\nCoLA (Warstadt et al., 2019), 3 similarity and para-\nphrase tasks: MRPC (Dolan and Brockett, 2005),\nSTS-B (Cer et al., 2017), QQP (Chen et al., 2018),\nand 4 inference tasks: MNLI (Williams et al.,\n2018), QNLI (Rajpurkar et al., 2016), RTE (Ben-\ntivogli et al., 2009), WNLI (Levesque et al., 2012).\nFollowing previous work (Jiao et al., 2020; Wang\net al., 2021; Park et al., 2021), we evaluate our\nmethod on 8 datasets except WNLI. We report ac-\ncuracy on 5 datasets: SST-2, QQP, MNLI, QNLI\nand RTE. We report F1 score on MRPC, Matthews\ncorrelation coefficient on CoLA, and Spearman’s\nrank correlation coefficient on STS-B.\n4.2 Implementation Details\nWe focus on task-specific distillation. We follow\nJiao et al. (2020) to augment the training sets for\neach of the GLUE tasks using the code 3 they re-\nleased. We fine-tune ELECTRA base on the origi-\nnal training sets as the teacher model, and utilize\nTinyBERT-4-3124 which is distilled on general cor-\npora as the initialization of our student model. For\ntoken-level and span-level distillation, we use 64\nrelation heads for calculating pair-wise interactions,\nand 1 relation head for triplet-wise angles due to its\nhuge computation and memory costs. And we set\nk1 = k2 = 20for calculating angles. For sample-\nlevel distillation, we use 64 relation heads and set\nk1 and k2 as the batch size. We distill token-level\nand span-level knowledge to the bottom-2 layers\nof the student and distill sample-level knowledge\nto the other layers. For the structural distillation\nobjective, we set λ1 = 4, λ2 = λ3 = 1to maintain\ntheir gradient norms in the same order of magni-\ntude. We first distill the student model using Eq. 8\nfor 50 epochs on CoLA and 20 epochs on other\ntasks. The learning rate is 1e-5 and the batch size is\n32. Then we use Eq. 9 to distill the predictions for\nall tasks except STS-B since we empirically find\nthat directly fine-tuning after distillation using Eq.\n8 yields better performance for it. For QQP and\nCoLA, we adopt the original training set and distill\nthe student for 10 epochs while for other 5 tasks\nwe use the augmented training sets and distill the\nstudent for 3 epochs. We set τ as 1.0, the learning\n3https://github.com/huawei-noah/\nPretrained-Language-Model/blob/master/\nTinyBERT/data_augmentation.py\n4https://huggingface.co/huawei-noah/\nTinyBERT_General_4L_312D\n1006\nMethod SST-2 MNLI-(m/mm)\nMGSKDm=1 92.5 83.6/82.9\nMGSKDm=4 92.9 83.9/83.3\nMGSKDm=16 93.3 84.3/83.9\nMGSKDm=64 93.7 84.7/84.3\nMGSKDm=128 93.5 84.8/84.2\nTable 2: The impact of relation heads.\nrate as 1e-5, and the batch size as 32. We release\nour code to facilitate future research.5\n4.3 Comparison Methods\nMedium-sized Student Models. Most of the\nexisting knowledge distillation methods are con-\nducted on medium-sized student models which\nhave 6 transformer layers, 768 hidden neurons, 12\nattention heads, and overall 66M parameters. We\nadopt 3 of them as baselines: DistilBERT (Sanh\net al., 2019), MiniLMv2 (Wang et al., 2021) and\nCKD (Park et al., 2021). Notice that these mod-\nels adopted different distillation settings. Dis-\ntilBERT and MiniLMv2 were firstly under task-\nagnostic distillation then directly fine-tuned on\nGLUE, while CKD was under both task-agnostic\nand task-specific distillation. The corpora they\nadopted for task-agnostic distillation were also not\nexactly the same. Nevertheless, we list the results\nas they reported on GLUE dev set as baselines, and\nwe implement MiniLMv2 and CKD, two state-of-\nthe-art distillation methods under the same distilla-\ntion setting as ours for a fair comparison, which is\ndescribed in the next paragraph.\nSmall-sized Student Models. For fair compar-\nisons, we implement two state-of-the-art distilla-\ntion methods: MiniLMv2 (Wang et al., 2021),\nCKD (Park et al., 2021) under the same distilla-\ntion setting as ours. All these methods use the\nsame student model as ours which has 4 trans-\nformer layers, 312 hidden neurons, 12 attention\nheads and overall 14M parameters. We adopt the\nfine-tuned ELECTRAbase as the teacher, and con-\nduct task-specific distillation using the same distil-\nlation schedule and hyperparameters on the same\naugmented training sets as ours.\n4.4 Main Results\nWe first evaluate the effectiveness of our pro-\nposed distillation framework. The main results\nare shown in Table 1. We calculate #Params\n5https://github.com/LC97-pku/MGSKD\nMethod SST-2 MNLI-(m/mm)\nMGSKD 93.7 84.7/84.3\nMGSKD w/o token 93.0 84.1/83.7\nMGSKD w/o span 93.2 84.3/84.0\nMGSKD w/o sample 92.8 83.9/83.6\nMGSKD w tokenp 92.1 83.4/82.9\nMGSKD w tokent 91.7 82.8/82.6\nMGSKD w tokenp,t 92.5 83.7/83.2\nMGSKD w spanp 91.8 82.5/82.3\nMGSKD w spant 91.8 82.3/82.0\nMGSKD w spanp,t 92.2 83.0/82.7\nMGSKD w samplep 91.9 82.6/82.5\nMGSKD w samplet 92.9 83.9/83.5\nMGSKD w samplep,t 92.8 83.7/83.6\nTable 3: Ablation study of knowledge granularity. The\nsubscripts p and t denote pair-wise and triplet-wise rela-\ntions respectively.\nby summing up the number of parameters con-\ntained in the embedding layer and all the trans-\nformer layers. The speed-up ratios are directly\ntaken from previous works (Jiao et al., 2020; Wang\net al., 2021). It can be observed that under the\nsame distillation setting (models with † in Table 1),\nStudent†\nMGSKD outperforms strong baseline meth-\nods (i.e., Student†\nMiniLMv2 and Student†\nCKD) on 7 of\nthe 8 GLUE tasks. When compared with medium-\nsized models from the literature which have more\nparameters but under different distillation settings\n(e.g., CKD), our method can still beat them on the\nmajority of the 8 tasks. And surprisingly, with a\nstronger teacher model and data augmentation tech-\nnique, our method MGSKD enables a 14M student\ntransformer model to achieve comparable perfor-\nmance with BERTbase on most of the GLUE tasks,\nwhile keeping 9.4 times faster. Also, we observe\nthat although MGSKD performs well on most of\nthe GLUE tasks, it lags behind some baselines on\nCoLA, where the model is asked to judge the gram-\nmatical acceptability of a sentence. One reason\nmight be that CoLA requires the model to focus on\nsyntactic information while paying less attention to\nthe sample-level semantic meanings, thus reducing\nthe need for multi-granularity semantic knowledge\nthat we propose to transfer to the student.\n4.5 Discussions\nThe Impact of Relation Heads. Recall that\nwhen calculating the structural relations between\nrepresentations, we project them into m relation\nheads. We show how the number of relation heads\nimpacts the performance on SST-2 and MNLI. As\nshown in Table 2, the performance gets better as the\n1007\n8 12 16 20 24 28 32\nthe choice of k1, k2 (k1=k2)\n90.6\n90.8\n91.0\n91.2\n91.4\n91.6\n91.8\n SST-2\n81.6\n81.8\n82.0\n82.2\n82.4\n82.6\n82.8\nMNLI-m\n(a) Token-level\n8 12 16 20 24 28 32\nthe choice of k1, k2 (k1=k2)\n91.4\n91.6\n91.8\n92.0\n92.2\n92.4\n92.6\n92.8\nSST-2\n82.25\n82.50\n82.75\n83.00\n83.25\n83.50\n83.75\nMNLI-m (b) Sample-level\nFigure 2: The accuracy curve of different k1, k2 for calculating angles.\n0 1 2 3 4\nthe choice of boundary layer M\n92.8\n93.0\n93.2\n93.4\n93.6\nSST-2, hierarchical\nSST-2, all\n83.9\n84.0\n84.1\n84.2\n84.3\n84.4\n84.5\n84.6\n84.7\nMNLI-m, hierarchical\nMNLI-m, all\nFigure 3: The accuracy curve of differ-\nent choices of the boundary layer M.\nnumber of relation heads increases, since it eases\nthe trouble for the student to learn the structural\nrelations in the very high-dimensional vector space\nby providing fine-grained supervision in multiple\nrelatively low-dimensional spaces. We also find\nthat when m is large, continuing to increase m is\nnot worthwhile since the time and memory com-\nplexity increase linearly with m. Therefore we\nchoose m = 64in our setting.\nAblation Study of Knowledge Granularity.We\ntransfer the structural knowledge to the student\nin three granularities: token-level, span-level, and\nsample-level. We extract pair-wise and triplet-wise\nstructural relations for token- and span-level, while\nwe adopt triplet-wise relations for sample-level.\nTo verify the effectiveness of each granularity of\nknowledge and each form of structural relations,\nwe conduct ablation studies and present the results\nin Table 3. (1) We first remove each granularity\nof knowledge from the objectives of MGSKD indi-\nvidually.6 We can conclude that the sample-level\nknowledge is most crucial for the overall perfor-\nmance, the token-level knowledge provides mod-\nerate benefit, and the span-level knowledge con-\ntributes the least. We assume the reason why span-\nlevel knowledge distillation performs a little bit\nworse than token-level lies in that the average num-\nber of meaningful spans per sample on the 8 tasks\nis 7.19, which is 5.2 times fewer than the aver-\nage number of tokens. Nevertheless, distillation\nwith span-level knowledge still yields comparable\nperformance. Overall, the results prove that each\ngranularity of knowledge brings a positive effect\nto the model performance. (2) Then for each gran-\nularity, we study the effect of each form of struc-\ntural knowledge (i.e., pair-wise and triplet-wise\n6When the sample-level objective is removed, we use the\nremaining objectives for all the student layers instead of only\nthe bottom layers, as this setting yields better performance.\nrelations). In this stage, we distill each granularity\nof knowledge into all the student layers for a fair\ncomparison. It can be observed that for token-level\nand span-level knowledge, pair-wise relations are\nmore effective than triplet-wise relations, and the\nmodel performs better when jointly utilizing both.\nWhile for sample-level knowledge, we find that\nusing triplet-wise relations outperforms using pair-\nwise relations by a large margin. Moreover, jointly\nutilizing the sample-level pair-wise and triplet-wise\nrelations can’t further improve the model’s perfor-\nmance, therefore we only employ triplet-wise rela-\ntions as sample-level knowledge.\nThe Impact ofk1 and k2 for Calculating Angles.\nTo ease the computation and memory complexity,\nwe propose to sequentially select important repre-\nsentations to form angles, leading to the hyperpa-\nrameters k1 and k2. We test different choices of k1\nand k2 by adopting token-level and sample-level\ntriplet-wise relations to teach the student respec-\ntively. To reduce the search space, we simply set\nk1 = k2. We draw the accuracy curve for different\nchoices of k1, k2, as shown in Fig. 2. For token-\nlevel objectives, we find that increasing k1, k2 im-\nproves the accuracy when they are small and when\nk1, k2 ≥ 20, the curves begin to vibrate. Therefore\nwe choose k1 = k2 = 20 for token-level angle\ncalculation. While for the triplet-wise relations of\nsample-level features, we observe that the accuracy\nincreases monotonically with k1, k2. Therefore we\njust set k1, k2 as the batch size.\nThe Choice of the Boundary LayerM. We pro-\npose the hierarchical distillation strategy where we\ndistill the token- and span-level knowledge into the\nbottom-M layers of the student and transfer the\nsample-level knowledge to the upper layers. To\nverify the effectiveness as well as to find the best\nchoice of the boundary layer M, we conduct exper-\n1008\niments and show the results in Fig. 3. The dashed\nlines represent the setting dubbed as “all”, where\nwe distill token-, span- and sample-level knowl-\nedge into all the student layers. And the solid lines\ndenote our hierarchical distillation setting with dif-\nferent choices of the boundary layer M. When\nM = 0and M = 4, the student learns sample-level\nknowledge or token- and span-level knowledge for\nall layers. Without the help of other knowledge\ngranularities, the student yields relatively poor per-\nformance on both tasks. As M increases from 0 to\n4, we find the model’s performance curves surpass\nthe dashed lines, which verifies the effectiveness\nof our proposed hierarchical distillation strategy\nwhich transfers the knowledge to the proper posi-\ntions of the student. We find the model achieves\nthe highest accuracy when M = 2, i.e., the middle\nlayer, indicating that both the syntactic knowledge\ntransferred by token- and span-level features and\nthe semantic knowledge derived from sample-level\nfeatures are indispensable.\n5 Conclusion\nIn this paper, we propose a novel knowledge dis-\ntillation framework named MGSKD. We leverage\nintermediate representations of multi-granularity\nlanguage units (i.e., tokens, spans and samples),\nand form the knowledge as the sophisticated struc-\ntural relations between the representations rather\nthan the individual representations themselves. The\nwell-organized structural knowledge is then dis-\ntilled into the student hierarchically across layers.\nEvaluation results on GLUE benchmark verify the\neffectiveness of our method. In the future, we plan\nto explore more forms of structural knowledge.\nAcknowledgements\nWe would like to thank the anonymous re-\nviewers for their constructive comments. This\nwork was supported by the National Key Re-\nsearch and Development Program of China (No.\n2020AAA0106600).\nEthical Statement\nThis paper proposes a knowledge distillation frame-\nwork that leverages multi-granularity structural\nknowledge to compress a large and powerful lan-\nguage model into a small one with minimum perfor-\nmance degradation, which is beneficial to energy-\nefficient NLP applications. The research will not\npose ethical problems or negative social conse-\nquences. The datasets used in this paper are all\npublicly available and are widely adopted by re-\nsearchers as the general testbed for natural lan-\nguage understanding evaluation. The proposed\nmethod doesn’t introduce ethical/social bias or ag-\ngravate the potential bias in the data.\nReferences\nHaoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jin Jin,\nXin Jiang, Qun Liu, Michael Lyu, and Irwin King.\n2021. BinaryBERT: Pushing the limit of BERT quan-\ntization. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 4334–4348, Online. Association for Computa-\ntional Linguistics.\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. 2009. The fifth pascal recognizing\ntextual entailment challenge. In TAC.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14, Vancouver,\nCanada. Association for Computational Linguistics.\nZihan Chen, Hongbo Zhang, Xiaoji Zhang, and Leqi\nZhao. 2018. Quora question pairs. URL https://www.\nkaggle. com/c/quora-question-pairs.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than generators.\narXiv preprint arXiv:2003.10555.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Łukasz Kaiser. 2018. Universal\ntransformers. arXiv preprint arXiv:1807.03819.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\n1009\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nAngela Fan, Edouard Grave, and Armand Joulin. 2019.\nReducing transformer depth on demand with struc-\ntured dropout. arXiv preprint arXiv:1909.11556.\nHao Fu, Shaojun Zhou, Qihong Yang, Junjie Tang, Gui-\nquan Liu, Kaikui Liu, and Xiaolong Li. 2021. Lrc-\nbert: Latent-representation contrastive knowledge\ndistillation for natural language understanding. In\nProceedings of the AAAI Conference on Artificial\nIntelligence, volume 35, pages 12830–12838.\nMitchell Gordon, Kevin Duh, and Nicholas Andrews.\n2020. Compressing BERT: Studying the effects of\nweight pruning on transfer learning. In Proceedings\nof the 5th Workshop on Representation Learning for\nNLP, pages 143–155, Online. Association for Com-\nputational Linguistics.\nSaurabh Goyal, Anamitra Roy Choudhury, Saurabh\nRaje, Venkatesan Chakaravarthy, Yogish Sabharwal,\nand Ashish Verma. 2020. Power-bert: Accelerating\nbert inference via progressive word-vector elimina-\ntion. In International Conference on Machine Learn-\ning, pages 3690–3699. PMLR.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nLu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao\nChen, and Qun Liu. 2020. Dynabert: Dynamic bert\nwith adaptive width and depth. Advances in Neural\nInformation Processing Systems, 33:9782–9793.\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah.\n2019. What does BERT learn about the structure of\nlanguage? In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3651–3657, Florence, Italy. Association for\nComputational Linguistics.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao\nChen, Linlin Li, Fang Wang, and Qun Liu. 2020.\nTinyBERT: Distilling BERT for natural language un-\nderstanding. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020 , pages 4163–\n4174, Online. Association for Computational Lin-\nguistics.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Span-\nBERT: Improving pre-training by representing and\npredicting spans. Transactions of the Association for\nComputational Linguistics, 8:64–77.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nHector Levesque, Ernest Davis, and Leora Morgenstern.\n2012. The winograd schema challenge. In Thir-\nteenth international conference on the principles of\nknowledge representation and reasoning.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nKevin J Liang, Weituo Hao, Dinghan Shen, Yufan\nZhou, Weizhu Chen, Changyou Chen, and Lawrence\nCarin. 2020. Mixkd: Towards efficient distilla-\ntion of large-scale language models. arXiv preprint\narXiv:2011.00593.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? Advances\nin neural information processing systems, 32.\nGeondo Park, Gyeongman Kim, and Eunho Yang. 2021.\nDistilling linguistic context for language model com-\npression. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 364–378, Online and Punta Cana, Dominican\nRepublic. Association for Computational Linguistics.\nWonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho.\n2019. Relational knowledge distillation. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 3967–3976.\nBaoyun Peng, Xiao Jin, Jiaheng Liu, Dongsheng Li,\nYichao Wu, Yu Liu, Shunfeng Zhou, and Zhaoning\nZhang. 2019. Correlation congruence for knowledge\ndistillation. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages\n5007–5016.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nAdriana Romero, Nicolas Ballas, Samira Ebrahimi Ka-\nhou, Antoine Chassang, Carlo Gatta, and Yoshua\nBengio. 2014. Fitnets: Hints for thin deep nets.\narXiv preprint arXiv:1412.6550.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\n1010\nBaitan Shao and Ying Chen. 2021. Multi-granularity for\nknowledge distillation. Image and Vision Computing,\n115:104286.\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei\nYao, Amir Gholami, Michael W Mahoney, and Kurt\nKeutzer. 2020. Q-bert: Hessian based ultra low\nprecision quantization of bert. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 34, pages 8815–8821.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631–1642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for BERT model com-\npression. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4323–4332, Hong Kong, China. Association for Com-\nputational Linguistics.\nSiqi Sun, Zhe Gan, Yuwei Fang, Yu Cheng, Shuohang\nWang, and Jingjing Liu. 2020. Contrastive distil-\nlation on intermediate representations for language\nmodel compression. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 498–508, Online. Asso-\nciation for Computational Linguistics.\nYonglong Tian, Dilip Krishnan, and Phillip Isola. 2019.\nContrastive representation distillation. arXiv preprint\narXiv:1910.10699.\nErik F. Tjong Kim Sang and Sabine Buchholz. 2000. In-\ntroduction to the CoNLL-2000 shared task chunking.\nIn Fourth Conference on Computational Natural Lan-\nguage Learning and the Second Learning Language\nin Logic Workshop.\nFrederick Tung and Greg Mori. 2019. Similarity-\npreserving knowledge distillation. In Proceedings\nof the IEEE/CVF International Conference on Com-\nputer Vision, pages 1365–1374.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n353–355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nWenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong,\nand Furu Wei. 2021. MiniLMv2: Multi-head self-\nattention relation distillation for compressing pre-\ntrained transformers. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 2140–2151, Online. Association for Computa-\ntional Linguistics.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020. Minilm: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers. Advances in Neural In-\nformation Processing Systems, 33:5776–5788.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le,\nMohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.\n2016. Google’s neural machine translation system:\nBridging the gap between human and machine trans-\nlation. arXiv preprint arXiv:1609.08144.\nOfir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8bert: Quantized 8bit bert. In\n2019 Fifth Workshop on Energy Efficient Machine\nLearning and Cognitive Computing-NeurIPS Edition\n(EMC2-NIPS), pages 36–39. IEEE.\nWei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao\nChen, Xin Jiang, and Qun Liu. 2020. TernaryBERT:\nDistillation-aware ultra-low bit BERT. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n509–521, Online. Association for Computational Lin-\nguistics.\n1011"
}