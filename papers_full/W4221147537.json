{
  "title": "End-to-End Transformer Based Model for Image Captioning",
  "url": "https://openalex.org/W4221147537",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2098914013",
      "name": "Yiyu Wang",
      "affiliations": [
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2231905073",
      "name": "Jungang Xu",
      "affiliations": [
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2114162724",
      "name": "Yingfei Sun",
      "affiliations": [
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2098914013",
      "name": "Yiyu Wang",
      "affiliations": [
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2231905073",
      "name": "Jungang Xu",
      "affiliations": [
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2114162724",
      "name": "Yingfei Sun",
      "affiliations": [
        "University of Chinese Academy of Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2506483933",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W6771429369",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W2950626540",
    "https://openalex.org/W6666761814",
    "https://openalex.org/W6766818547",
    "https://openalex.org/W3113377113",
    "https://openalex.org/W6773428483",
    "https://openalex.org/W6753850902",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W6770800577",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W3124043039",
    "https://openalex.org/W6775772702",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2560313346",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W6639657675",
    "https://openalex.org/W6756667841",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W6754778999",
    "https://openalex.org/W3167939936",
    "https://openalex.org/W3035265375",
    "https://openalex.org/W3174377922",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2885013662",
    "https://openalex.org/W4287553517",
    "https://openalex.org/W3035497460",
    "https://openalex.org/W2904551248",
    "https://openalex.org/W1895577753",
    "https://openalex.org/W639708223",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4288329833",
    "https://openalex.org/W2990818246",
    "https://openalex.org/W3034655362",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2963084599",
    "https://openalex.org/W3035284526",
    "https://openalex.org/W4289542422",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2481240925",
    "https://openalex.org/W2986670728"
  ],
  "abstract": "CNN-LSTM based architectures have played an important role in image captioning, but limited by the training efficiency and expression ability, researchers began to explore the CNN-Transformer based models and achieved great success. Meanwhile, almost all recent works adopt Faster R-CNN as the backbone encoder to extract region-level features from given images. However, Faster R-CNN needs a pre-training on an additional dataset, which divides the image captioning task into two stages and limits its potential applications. In this paper, we build a pure Transformer-based model, which integrates image captioning into one stage and realizes end-to-end training. Firstly, we adopt SwinTransformer to replace Faster R-CNN as the backbone encoder to extract grid-level features from given images; Then, referring to Transformer, we build a refining encoder and a decoder. The refining encoder refines the grid features by capturing the intra-relationship between them, and the decoder decodes the refined features into captions word by word. Furthermore, in order to increase the interaction between multi-modal (vision and language) features to enhance the modeling capability, we calculate the mean pooling of grid features as the global feature, then introduce it into refining encoder to refine with grid features together, and add a pre-fusion process of refined global feature and generated words in decoder. To validate the effectiveness of our proposed model, we conduct experiments on MSCOCO dataset. The experimental results compared to existing published works demonstrate that our model achieves new state-of-the-art performances of 138.2% (single model) and 141.0% (ensemble of 4 models) CIDEr scores on 'Karpathy' offline test split and 136.0% (c5) and 138.3% (c40) CIDEr scores on the official online test server. Trained models and source code will be released.",
  "full_text": "End-to-End Transformer Based Model for Image Captioning\nYiyu Wang1, Jungang Xu2*, Yingfei Sun1\n1School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences\n2School of Computer Science and Technology, University of Chinese Academy of Sciences\nwangyiyu18@mails.ucas.ac.cn, xujg@ucas.ac.cn, yfsun@ucas.ac.cn\nAbstract\nCNN-LSTM based architectures have played an important\nrole in image captioning, but limited by the training efﬁ-\nciency and expression ability, researchers began to explore\nthe CNN-Transformer based models and achieved great suc-\ncess. Meanwhile, almost all recent works adopt Faster R-\nCNN as the backbone encoder to extract region-level fea-\ntures from given images. However, Faster R-CNN needs a\npre-training on an additional dataset, which divides the im-\nage captioning task into two stages and limits its potential ap-\nplications. In this paper, we build a pure Transformer-based\nmodel, which integrates image captioning into one stage and\nrealizes end-to-end training. Firstly, we adopt SwinTrans-\nformer to replace Faster R-CNN as the backbone encoder to\nextract grid-level features from given images; Then, referring\nto Transformer, we build a reﬁning encoder and a decoder.\nThe reﬁning encoder reﬁnes the grid features by capturing\nthe intra-relationship between them, and the decoder decodes\nthe reﬁned features into captions word by word. Furthermore,\nin order to increase the interaction between multi-modal (vi-\nsion and language) features to enhance the modeling capa-\nbility, we calculate the mean pooling of grid features as the\nglobal feature, then introduce it into reﬁning encoder to re-\nﬁne with grid features together, and add a pre-fusion process\nof reﬁned global feature and generated words in decoder. To\nvalidate the effectiveness of our proposed model, we conduct\nexperiments on MSCOCO dataset. The experimental results\ncompared to existing published works demonstrate that our\nmodel achieves new state-of-the-art performances of 138.2%\n(single model) and 141.0% (ensemble of 4 models) CIDEr\nscores on ‘Karpathy’ ofﬂine test split and 136.0% (c5) and\n138.3% (c40) CIDEr scores on the ofﬁcial online test server.\nTrained models and source code will be released.\nIntroduction\nImage captioning aims to automatically describe the vi-\nsual content of a given image with ﬂuent and credible sen-\ntences. It is a typical multi-modal learning task, which con-\nnects Computer Vision (CV) and Natural Language Pro-\ncessing (NLP). Inspired by the success of deep learning\nmethods in machine translation (Papineni et al. 2002; Cho\net al. 2014), almost all image captioning models adopt the\n*Corresponding Author\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nencoder-decoder framework with the visual attention mech-\nanism. The encoder encodes input images into ﬁx-length\nvector features, and the decoder decodes image features into\ndescriptions word by word (Vinyals et al. 2015; Xu et al.\n2015; Anderson et al. 2018; Huang et al. 2019; Pan et al.\n2020).\nInitially, researchers adopted a pre-trained Convolutional\nNeural Network (CNN) as an encoder to extract image grid-\nlevel features and Recurrent Neural Network (RNN) as a de-\ncoder (Vinyals et al. 2015; Xu et al. 2015). (Anderson et al.\n2018) ﬁrst adopted Faster R-CNN to extract region-level fea-\ntures. Due to its overwhelming advantage, most subsequent\nworks followed this pattern, and grid-level features extracted\nby CNN were discarded. Nevertheless, there are still some\ninherent defects in region-level features and encoder of ob-\nject detector: 1) region-level features may not cover the en-\ntire image, which results in the lack of ﬁne-grained informa-\ntion (Luo et al. 2021); 2) extracting region features is high\ntime consuming, and the object detector needs an extra Vi-\nsual Genome (Krishna et al. 2017) dataset for pre-training,\nwhich makes it difﬁcult to train image captioning model\nend-to-end from image pixels to descriptions, and also limits\npotential applications in the actual scene (Jiang et al. 2020).\nDecoder of LSTM (Hochreiter and Schmidhuber 1997)\nwith soft attention (Xu et al. 2015) mechanism has re-\nmained the common and dominant approach in the past few\nyears. However, the shortcomings of training efﬁciency and\nexpression ability of LSTM also limit the effect of rele-\nvant models. Inspired by the success of Multi-head Self-\nAttention (MSA) mechanism and Transformer architecture\nin NLP tasks (Vaswani et al. 2017), many researchers began\nto introduce MSA into decoder of LSTM (Huang et al. 2019;\nPan et al. 2020) or directly adopt Transformer architecture as\ndecoder (Cornia et al. 2020; Pan et al. 2020; Luo et al. 2021;\nJi et al. 2021) of image captioning models.\nEspecially, Transformer architecture gradually shows ex-\ntraordinary potential in CV tasks (Dosovitskiy et al. 2021;\nLiu et al. 2021) and multi-modal tasks (Lu et al. 2019; Zhu\nand Yang 2020; Radford et al. 2021), which provides a new\nchoice for encoding images into vector features. Different\nfrom Faster R-CNN, features extracted by a visual trans-\nformer are grid-level features, which have a higher comput-\ning efﬁciency and allows expediently exploring more effec-\ntive and complex designs for image captioning.\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n2585\nConsidering the disadvantage of pre-trained CNN and ob-\nject detector in encoder and limitations of LSTM in decoder,\nwe build a pure Transformer-based image captioning model\n(PureT) to integrate this task into one stage without pre-\ntraining process of object detection to achieve end-to-end\ntraining. In Encoder, we adopt Swin-Transformer (Liu et al.\n2021) to extract grid features from given images as the initial\nvector features and compute the average pooling of gird fea-\ntures as the initial image global feature. Then, we construct a\nreﬁning encoder similar to (Huang et al. 2019; Cornia et al.\n2020; Ji et al. 2021) by Shifted Window MSA (SW-MSA)\nfrom Swin-Transformer to reﬁne image initial grid features\nand global feature. The reﬁning encoder has a similar ar-\nchitecture with Transformer Encoder in machine translation\n(Vaswani et al. 2017) which can be regarded as an extension\nof Encoder of SwinTransformer for image captioning model.\nIn Decoder, we directly adopt Transformer Decoder in ma-\nchine translation (Vaswani et al. 2017) to generate captions.\nFurthermore, we pre-fuse the word embedding vector with\nthe image global feature from Encoder before the MSA of\nword embedding vector to increase the interaction of inter-\nmodel (image-to-words) features.\nWe validate our model on MSCOCO (Lin et al. 2014) of-\nﬂine “Karpathy” (Karpathy and Fei-Fei 2017) test split and\nofﬁcial online test server. The results demonstrate that our\nPureT achieves new state-of-the-art performance on both\nsingle model and ensemble of 4 models conﬁgurations: on\nofﬂine “Karpathy” test split, a single model and an ensem-\nble of 4 models achieve 138.2% and 140.8% CIDEr scores\nrespectively; on ofﬁcial online test server, an ensemble of 4\nmodels achieves 135.3% (c5) and 138.0% (c40) CIDEr.\nOur main contributions are summarized as follows:\n• We construct a pure Transformer-based (PureT) model\nfor image captioning, which integrates this task into one\nstage again without the pre-training process of object de-\ntector and provide a new simple and solid baseline of im-\nage captioning.\n• We add a pre-fusion process between the generated word\nembeddings and image global feature, which aims to\nincrease the interaction of inter-modal features and en-\nhance the reasoning ability from image to captions.\n• We conduct extensive experiments on the MSCOCO\ndataset, which demonstrate the effectiveness of our pro-\nposed model, and achieve a new state-of-the-art perfor-\nmance on both ‘Karpathy’ ofﬂine test split and ofﬁcial\nonline test server.\nRelated Work\nExisting works of image captioning can be divided into\nCNN-LSTM based models (Vinyals et al. 2015; Xu et al.\n2015; Anderson et al. 2018; Wang, Chen, and Hu 2019;\nHuang et al. 2019) and CNN-Transformer based models\n(Herdade et al. 2019; Li et al. 2019; Pan et al. 2020; Cornia\net al. 2020; Ji et al. 2021; Luo et al. 2021). Both adopted pre-\ntrained CNN or Faster R-CNN as the encoder to encode im-\nage into grid or region-level features, where the former mod-\nels adopted Long Short-Term Memory Network (LSTM)\n(Hochreiter and Schmidhuber 1997) as the decoder and the\nlatter models adopted Transformer (Vaswani et al. 2017) as\nthe decoder to generate description word by word.\nEarlier works used pre-trained CNN, e.g., VGG-16 (Si-\nmonyan and Zisserman 2015) and ResNet-101 (He et al.\n2016), as the encoder to encode image into grid-level fea-\ntures with ﬁxed-length, and then LSTM with attention mech-\nanism was applied among them to generate captions (Xu\net al. 2015; Rennie et al. 2017). (Anderson et al. 2018) ﬁrst\nintroduced Faster R-CNN (Ren et al. 2017) into image cap-\ntioning to extract the region-level features more in line with\nthe human visual habits, which has become a typical pattern\nto extract image features in subsequent works.\nAbove all models adopted LSTM as the decoder, which\nhave shortcomings in training efﬁciency and expression abil-\nity. Recently, researchers began to explore the application\nof transformer in image captioning. (Herdade et al. 2019)\nproposed the Object Relation Transformer to introduce the\nregion spatial information. (Pan et al. 2020) proposed the\nX-Linear attention block to capture the 2nd order interac-\ntions between the single- or multi-modal, and integrated it\ninto the Transformer encoder and decoder. (Cornia et al.\n2020) designed a mesh-like connectivity in decoder to ex-\nploit both low-level and high-level features from the en-\ncoder. (Luo et al. 2021) proposed a Dual-Level Collaborative\nTransformer (DLCT) to process both grid- and region-level\nfeatures for realizing the complementary advantages.\nDespite the outstanding performance of region-level fea-\ntures extracted by Faster R-CNN, the lack of ﬁne-grained\ninformation of region-level and the time cost of Faster R-\nCNN pre-training are unavoidable problems. Furthermore,\nextracting region-level features is time-consuming, so most\nmodels directly trained and evaluated on cached features in-\nstead of image, which makes it difﬁcult to train image cap-\ntioning model end-to-end from image to descriptions.\nModel\nThe overall architecture of our PureT model is shown in\nFigure 1. We adopt the widely used encoder-decoder frame-\nwork, in which the encoder consists of a backbone of Swin-\nTransformer and stacks of N reﬁning encoder blocks and\nthe decoder consists of stacks of N decoder blocks. The en-\ncoder is in charge of extracting grid features from the input\nimage and reﬁning them by capturing the intra-relationship\nbetween them. The decoder uses the reﬁned image grid fea-\ntures to generate the captions word by word by capturing the\ninter-relationship between word and image grid features.\nAttention Mechanism\nThe attention mechanism can be abstractly summarized as\nfollows:\nAttention(q;k;v ) = fsim(q;k)v (1)\nwhere fsim(·)is a function used to compute the similarity\nscores between some queries (q ) and keys (k). The output\nof attention mechanism is the weighted sum on values (v )\nbased on similarity scores.\nIn our model, Multi-head Self Attention (MSA) (Vaswani\net al. 2017) and its variants Window MSA / Shifted Window\n2586\nW-MSA / \nSW-MSA\nAdd & LN\nFeedForward\nAdd & LN\nN\nMSA\nAdd & LN\nFeedForward\nAdd & LN\nLinear\nAvgPool\n1 512\nImages\n384 384 3\nSwin\nTransformer\nLinear\nSoftmax\nN\n1 2 1tx x x \n23 tx x x\nindicates parameter sharing\ninput words:\noutput words:\n12 32 61 15\n12 112 52\ngrid features\nglobal feature\nl\ntX\nN\ngv\n,pl\ntX\nPre-Fusion Module\nN\ngv\nN\nGV\nGV\ngvRefining Encoder\nDecoder\nFigure 1: Overview of our proposed PureT model. We ﬁrst extract image grid features VG using SwinTransformer. vg is calcu-\nlated as the average pooling of VG. Then VG and vg are reﬁned into VN\nG and vN\ng through the Reﬁning Encoder composed of N\nblocks stacks and are fed into the Decoder to generate description word by word.\n(a) regular window partitioning scheme (b) shifted window partitioning scheme\nFigure 2: Illustration of regular window partitioning scheme\nand shifted window partitioning scheme adopted in reﬁning\nencoder. The size of input feature map isH×W = 12×12.\nMSA (W-MSA / SW-MSA) modules proposed by Swin-\nTransformer (Liu et al. 2021) are used, where MSA is\nadopted in the decoder to model the intra-relationship of\nword sequence and the inter-relationship between word and\ngrid features, and W-MSA / SW-MSA are adopted in the en-\ncoder to model intra-relationship of image grid features. The\nabove three attention modules use Softmax(·)as the simi-\nlarity scoring function, which can be formulated as follows:\nAttention(q;k;v ) = Softmax\n(qkT\n√dk\n)\nv (2)\nwhere dk is the dimension of k.\nMSA(Q;K;V ) = Concat(head1;:::;head h)\nheadi = Attention(Qi;Ki;V i);i = 1;2;:::;h (3)\nwhere his the number of heads. Qi;Ki and Vi are the i-th\nslice of Q;K and V respectively, which can be formulated\nas follows:\nF = Concat(F1;:::; Fi;:::; Fh) (4)\nwhere F ∈ RLF×DF and Fi ∈ RLF×\nDF\nh (F refers to\nQ;K and V), LF and DF are the length and dimension.\nIn the i-th head of MSA, each token of the query Qi cal-\nculates its similarity with all tokens of the key Ki, and per-\nforms the weighted sum on all tokens of the value Vi to\nobtain the corresponding output. Therefore, MSA can be re-\ngarded as a global attention mechanism.\nW-MSA and SW-MSA Aiming at the quadratic complex-\nity caused by the global computation of MSA, SwinTrans-\nformer proposed W-MSA and SW-MSA to compute self-\nattention within local windows (Liu et al. 2021). In this pa-\nper, both W-MSA and SW-MSA are used in the encoder, in\nwhich inputs of Q;K and V are all from image grid fea-\ntures, therefore they have the same length L= H×W and\ndimension D.\nCompared with MSA, W-MSA and SW-MSA ﬁrst parti-\ntion the inputs ofQ;K and V into several windows, and then\napply MSA separately in each window. Figure 2 illustrate\nthe regular window partitioning scheme and shifted window\npartitioning scheme of W-MSA and SW-MSA respectively.\nAdding SW-MSA after W-MSA aims to solve the lack of\nconnections across windows of W-MSA module to further\nimprove the modeling ability. W-MSA and SW-MSA can be\nformulated as follows:\n(S)W-MSA(Q;K;V ) = Merge(window1;:::;window w)\nwindowi = MSA(Qi\nW;Ki\nW;V i\nW);i = 1;2;:::;w (5)\nwhere w is the number of windows and Merge(·)is the\nreverse operation of regular/shifted window partitioning\nscheme. Qi\nW;Ki\nW and Vi\nW are the i-th window of Q;K and\nV respectively, which can be formulated as follows:\nF = Merge\n(\nF1\nW;:::; Fi\nW;:::; Fw\nW\n)\n(6)\nwhere F ∈RL×D and Fi\nW ∈R\nL\nw×D (F refers to Q;K\nand V).\n2587\nEncoder\nDifferent from most existing models, we ﬁrst employ Swin-\nTransformer (Liu et al. 2021) instead of pre-trained CNN\nor Faster R-CNN as the backbone encoder to extract a set\nof grid features VG = {v 1;v2;:::;v m}from given input\nimages as the initial visual features, where vi ∈RD, D is\nthe embedding dimension of each grid feature, and mis the\nnumber of grid features (m= 12 ×12 in this paper).\nAfter grid features VG are extracted, we refer to the stan-\ndard transformer encoder (Vaswani et al. 2017) to construct\na reﬁning encoder to enhance the grid features by capturing\nthe intra-relationship between them. Furthermore, inspired\nby (Ji et al. 2021), we calculate the mean pooling of grid\nfeatures vg = 1\nm\n∑m\ni=1 vi as the initial global feature and\nintroduce it into W-MSA and SW-MSA. Speciﬁcally, when\napplying MSA in each window, the global feature is added\ninto the keys k and values v as an extra token. Meanwhile,\nwe also reﬁne the global feature by using it as an extra query\nqtoken and applying MSA on all grid features.\nAs shown in Figure 1, the reﬁning encoder is composed\nof N blocks stacked in sequence (N = 3 in this paper), and\neach block consists of a W-MSA or SW-MSA module with\nfeedforward layer, in which W-MSA and SW-MSA are used\nalternately. The l-th block can be formulated as follows:\n^Vl\nG = LayerNorm\n(\nVl−1\nG + (S)W-MSA\n(\nWl\nQVl−1\nG ;\nWl\nK\n[\nVl−1\nG ; vl−1\ng\n]\ns;Wl\nV\n[\nVl−1\nG ; vl−1\ng\n]\ns\n))\n(7)\n^vl\ng = LayerNorm(vl−1\ng + MSA\n(\nWl\nQvl−1\ng ;\nWl\nK[Vl−1\nG ; vl−1\ng ]s;Wl\nV[Vl−1\nG ; vl−1\ng ]s\n))\n(8)\nVl\nG = LayerNorm\n(\n^Vl\nG + FeedForward(^Vl\nG)\n)\n(9)\nvl\ng = LayerNorm\n(\n^vl\ng + FeedForward(^vl\ng)\n)\n(10)\nwhere Vl−1\nG and vl−1\ng denote the output grid features and\nglobal feature of block l −1 respectively, and which are\nused as the input of block l, in which V0\nG = VG and\nv0\ng = vg, Wl\nQ;Ql\nK;Wl\nV ∈RD×D are learnt parameter ma-\ntrices; [Vl−1\nG ; vl−1\ng ]s ∈R(k+1)×D denotes the stack opera-\ntion of grid features and global feature and FeedForward(·)\nconsists of two linear layer with ReLU activation function\nin between, as formulated below:\nFeedForward (x) =W2 ReLU (W1x) (11)\nwhere W1 ∈R(4D)×D and W2 ∈RD×(4D) are the learnt\nparameter matrices of two linear layers respectively. Note\nthat the parameter of reﬁning process for grid features and\nglobal feature are reused. The output reﬁned grid features\nVN\nG and reﬁned global feature vN\ng of block N are fed into\nthe decoder as the input of visual content.\nDecoder\nThe decoder aims to generate the output caption word by\nword conditioned on the reﬁned global and grid features\nfrom the encoder. The interaction between multi-modal oc-\ncurs in this part. As shown in Figure 1, the decoder is com-\nposed of N blocks stacked in sequence (N = 3 in this pa-\nper), where each block can be divided into three modules:\n1) Pre-Fusion Module, which contains the pre-fusion pro-\ncess between previously generated words and reﬁned global\nfeature, which can be regarded as the ﬁrst inter-modal inter-\naction between natural language and visual content; 2) Lan-\nguage Masked MSA Module, which can be regarded as the\nintra-modal interaction within the generated words; 3) Cross\nMSA Module, which contains a MSA module with a Feed-\nForward layer, which can be regarded as the second inter-\nmodal interaction between visual content and natural lan-\nguage; 4) Word Generation Module, which contains a linear\nlayer with softmax function.\nPre-Fusion Module Most recent Transformer-based mod-\nels only use image region or grid features without global\nfeature, where the interaction between multi-modal features\nonly occurs in cross attention between generated word and\nvisual features before generating the next word. The lack of\ninteraction of global contextual information limits reasoning\ncapability to a certain extent. Therefore, we construct a pre-\nfusion module to fuse the reﬁned global feature vN\ng into the\ninput of each block of decoder, which can be regarded as the\nﬁrst time multi-modal interaction to capture global visual\ncontext information and can be formulated as follows:\nXp;l\n1:t−1 = Layer Norm\n(\nXl−1\n1:t−1+\nReLU\n(\nWf\n[\nXl−1\n1:t−1; vN\ng\n]))\n(12)\nwhere Xl−1\n1:t−1 ∈ R(t−1)×D denotes the output of block\nl −1 and is used as the input of block l at t-th timestep\n,\n[\nXl−1\n1:t−1; vg\n]\n∈ R(t−1)×2D indicates concatenation and\nWf ∈RD×2D is learnt parameters of a linear layer; the out-\nput Xp;l\n1:t−1 ∈R(t−1)×D is fed into the Language Masked\nMSA Module. Note that the initial input at the ﬁrst block\ncomes from the previously generated words:\nX0\n1:t−1 = Wex1:t−1 (13)\nwhere x1:t−1 are one-hot encodings of the generated words\nbefore t-th timestep, and We ∈RD×|\u0006|is the word embed-\nding matrix of the vocablulary \u0006.\nLanguage Masked MSA Module The module aims to\nmodel the intra-modal relationship (words-to-words) within\nXp;l\n1:t−1, which can be formulated as follows:\n~Xl\nt−1 = LayerNorm\n(\nXp;l\nt−1 + MSA\n(\nWm;l\nQ Xp;l\nt−1;\nWm;l\nK Xp;l\n1:t−1;Wm;l\nV Xp;l\n1:t−1\n))\n(14)\nwhere Wm;l\nQ ;Wm;l\nK ;Wm;l\nV ∈RD×D are learnt parameters,\nand Xp;l\nt−1 indicates the corresponding embedding vector of\nthe generated word at (t−1)-th timestep, which means that\neach word is only allowed to calculate attention map at its\nearlier generated words.\nCross MSA Module The module aims to model the inter-\nmodal relationship (words-to-vision) between ~Xl\n1:t−1 and\nVN\nG , which can be regarded as the second time multi-modal\ninteraction to capture local visual context information and\n2588\nModels Single Model Ensemble Model\nB-1 B-4 M R C S B-1 B-4 M R C S\nCNN-LSTM based models\nSCST - 34.2 26.7 55.7 114.0 - - 35.4 27.1 56.6 117.5 -\nRFNet 79.1 36.5 27.7 57.3 121.9 21.2 80.4 37.9 28.3 58.3 125.7 21.7\nUp-Down 79.8 36.3 27.7 56.9 120.1 21.4 - - - - - -\nGCN-LSTM 80.5 38.2 28.5 58.3 127.6 22.0 80.9 38.3 28.6 58.5 128.7 22.1\nAoANet 80.2 38.9 29.2 58.8 129.8 22.4 81.6 40.2 29.3 59.4 132.0 22.8\nX-LAN 80.8 39.5 29.5 59.2 132.0 23.4 81.6 40.3 29.8 59.6 133.7 23.6\nCNN-Transformer based models\nORT 80.5 38.6 28.7 58.4 128.3 22.6 - - - - - -\nX-Transformer 80.9 39.7 29.5 59.1 132.8 23.4 81.7 40.7 29.9 59.7 135.3 23.8\nM2 Transformer 80.8 39.1 29.2 58.6 131.2 22.6 82.0 40.5 29.7 59.5 134.5 23.5\nRSTNet 81.8 40.1 29.8 59.5 135.6 23.3 - - - - - -\nGET 81.5 39.5 29.3 58.9 131.6 22.8 82.1 40.6 29.8 59.6 135.1 23.8\nDLCT 81.4 39.8 29.5 59.1 133.8 23.0 82.2 40.8 29.9 59.8 137.5 23.3\nPureT 82.1 40.9 30.2 60.1 138.2 24.2 83.4 42.1 30.4 60.8 141.0 24.3\nTable 1: Ofﬂine evaluation results of our proposed model and other existing state-of-the-art models on MSCOCO “Karpathy”\ntest split, where B-N, M, R, C and S denote BLEU-N, METEOR, ROUGE-L, CIDEr and SPICE respectively.\ncan be formulated as follows:\n^Xl\nt−1 = LayerNorm\n(\n~Xl\nt−1 + MSA\n(\nWc;l\nQ ~Xl\nt−1;\nWc;l\nK VN\nG ;Wc;l\nV VN\nG\n))\n(15)\nXl\nt−1 = LayerNorm(^Xl\nt−1 + FeedForward(^Xl\nt−1)) (16)\nwhere Wc;l\nQ ;Wc;l\nK ;Wc;l\nV ∈ RD×D are learnt parameters,\n~Xl\nt−1 from the Language Masked MSA Module is fed into\nMSA as query, and reﬁned grid features VN\nG from the last\nblock of encoder are fed into MSA as keys and values.\nWord Generation Module Given the outputXN\n1:t−1 of the\nlast decoder block, the conditional distribution over the vo-\ncablary \u0006 is given by:\np(xt|x1:t−1) = Softmax(WxXN\nt−1) (17)\nwhere Wx ∈R|\u0006|×Dis learnt parameters.\nObjective Functions\nWe ﬁrst optimize our model by applying cross entropy (XE)\nloss as the objective function:\nLXE(\u0012) = −\nT∑\nt=1\nlog(p\u0012(y∗\nt|y∗\n1:t−1)) (18)\nwhere y∗\n1:T is the target ground truth sequence, and\u0012denotes\nthe parameters of our model. Then, we adopt self-critical se-\nquence training (SCST) strategy (Rennie et al. 2017) to opti-\nmize CIDEr (Vedantam, Zitnick, and Parikh 2015) metrics:\nLR(\u0012) = −Ey1:T∼p\u0012[r(y1:T)] (19)\nwhere r(·)is the score of CIDEr. The gradient of LR can be\napproximated as follows:\n∇\u0012LR(\u0012) ≈− (r(ys\n1:T) −r(^y1:T)) ∇\u0012log p\u0012(ys\n1:T) (20)\nwhere ys\n1:T is a sampled caption and r(^ys\n1:T) deﬁnes the\ngreedily decoded score obtained from the current model.\nExperiments\nDataset and Evaluation Metrics\nWe conduct experiments on the MSCOCO 2014 dataset (Lin\net al. 2014), which contains 123287 images (82783 for train-\ning and 40504 for validation), and each is annotated with 5\nreference captions. In this paper, we follow the “Karpathy”\nsplit (Karpathy and Fei-Fei 2017) to redivide the MSCOCO,\nwhere 113287 images for training, 5000 images for val-\nidation and 5000 images for ofﬂine evaluation. Besides,\nMSCOCO also provides 40775 images for online testing.\nFor the training process, we convert all training captions to\nlower case and drop the words occur less than 6 times, col-\nlect the rest 9487 words as our vocabulary \u0006.\nFor fair evaluation, we adopt ﬁve widely used metrics\nto evaluate the quality of generated captions, including\nBLEU (Papineni et al. 2002), METEOR (Lavie and Agar-\nwal 2007), ROUGE-L (Lin 2004), CIDEr (Vedantam, Zit-\nnick, and Parikh 2015), and SPICE (Anderson et al. 2016).\nExperimental Settings\nWe set the model embedding size D to 512, the number of\ntransformer heads to 8, the number of blocks N for both\nreﬁning encoder and decoder to 3. For the training process,\nwe ﬁrst train our model under XE loss LXE for 20 epochs,\nand set the batch size to 10 and warmup steps to 10,000; then\nwe train our model under LR for another 30 epochs with\nﬁxed learning rate of 5 ×10−6. We adopt Adam (Kingma\nand Ba 2015) optimizer in both above stages and the beam\nsize is set to 5 in validation and evaluation process.\nComparisons with State-of-The-Art Models\nOfﬂine Evaluation Table 1 reports the performances of\nsome existing state-of-the-art models and our proposed\nmodel on MSCOCO ofﬂine test split. The compared mod-\nels include: SCST (Rennie et al. 2017), RFNet (Jiang et al.\n2018), Up-Down (Anderson et al. 2018), GCN-LSTM (Yao\net al. 2018), AoANet (Huang et al. 2019) and X-LAN (Pan\n2589\nModels BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE-L CIDEr\nc5 c40 c5 c40 c5 c40 c5 c40 c5 c40 c5 c40 c5 c40\nSCST 78.1 93.7 61.9 86.0 47.0 75.9 35.2 64.5 27.0 35.5 56.3 70.7 114.7 116.7\nGCN-LSTM 80.8 95.2 65.5 89.3 50.8 80.3 38.7 69.7 28.5 37.6 58.5 73.4 125.3 126.5\nUp-Down 80.2 95.2 64.1 88.8 49.1 79.4 36.9 68.5 27.6 36.7 57.1 72.4 117.9 120.5\nSGAE 81.0 95.3 65.6 89.5 50.7 80.4 38.5 69.7 28.2 37.2 58.6 73.6 123.8 126.5\nAoANet 81.0 95.0 65.8 89.6 51.4 81.3 39.4 71.2 29.1 38.5 58.9 74.5 126.9 129.6\nX-Transformer 81.9 95.7 66.9 90.5 52.4 82.5 40.3 72.4 29.6 39.2 59.5 75.0 131.1 133.5\nM2 Transformer 81.6 96.0 66.4 90.8 51.8 82.7 39.7 72.8 29.4 39.0 59.2 74.8 129.3 132.1\nRSTNet 82.1 96.4 67.0 91.3 52.2 83.0 40.0 73.1 29.6 39.1 59.5 74.6 131.9 134.0\nGET 81.6 96.1 66.5 90.9 51.9 82.8 39.7 72.9 29.4 38.8 59.1 74.4 130.3 132.5\nDLCT 82.4 96.6 67.4 91.7 52.8 83.8 40.6 74.0 29.8 39.6 59.8 75.3 133.3 135.4\nPureT 82.8 96.5 68.1 91.8 53.6 83.9 41.4 74.1 30.1 39.9 60.4 75.9 136.0 138.3\nTable 2: Online evaluation results of our proposed model and other existing state-of-the-art models on MSCOCO.\net al. 2020); ORT (Herdade et al. 2019), X-Transformer (Pan\net al. 2020), M2 Transformer (Cornia et al. 2020), RST-\nNet(Zhang et al. 2021), GET (Ji et al. 2021) and DLCT (Luo\net al. 2021). We divide these models into CNN-LSTM based\nmodels and CNN-Transformer based models according to\nthe difference mothods adopted in decoder.\nFor fair comparisons, we report the results of a single\nmodel and ensemble of 4 models after SCST training. As\nshown in Table 1, both our single model and ensemble of\n4 models achieve best performances in all metrics. In the\ncase of single model, the CIDEr score of our model reaches\n138.2%, which achieves advancements of 2.6% and 4.4% to\nthe strong competitors RSTNet and DLCT. Meanwhile, our\nmodel achieves improvements of over 0.6% to RSTNet, and\nimprovements of over 1.0% to DLCT in terms of metrics\nBLEU-4, ROUGE-L and SPICE. In the case of ensemble\nmodel, our model also achieves the best performance, and\nadvances all other models by more than 1.0% over all met-\nrics except METEOR. In particular, the CIDEr score of our\nensemble model reaches 141.0%, which achieves advance-\nments of 3.5% and 5.9% to DLCT and GET.\nIn general, the signiﬁcant improvements of all metrics (es-\npecially CIDEr) demonstrate the advantage of our proposed\nmodel. In addition, compared to models that use region-level\nfeatures or both region and grid-level features, our model\nhas a relatively more balanced computational cost due to it\navoids the prediction of object regions coordinates. And our\nmodel can be trained end-to-end, which allows us to explore\nit in more actual scenes.\nOnline Evaluation As shown in Table 2, we also report\nthe performance with 5 reference captions (c5) and 40 refer-\nence captions (c40) of our model on the MSCOCO ofﬁcial\nonline test server. Compared to the other state-of-the-arts,\nour model achieves the best scores in all metrics except a\nslightly lower 0.1% in BLEU-1 (c40) than DLCT. Notably,\nthe scores of CIDEr (c5) and CIDEr (c40) of our model\nreach 136.0% and 138.3%, which achieve advancements of\n2.7% and 2.9% with respect to the best performer DLCT.\nAblation Study\nWe conduct several ablation studies to quantify the inﬂu-\nences of different modules in our model.\nGT1: a giraffe stands with several birds resting on it's neck\nGT2: a giraffe that has some birds perched on it\nGT3: this is a close up picture of a giraffe that is standing.\n        : a close up of a giraffe with a bird\nTransformer: a giraffe standing with a group of birds on it\nPureT: a close up of a giraffe with birds on its neck\nGT1: a man talking on a cell phone on a boat with a city in \n          the background\nGT2: a man is on a boat using his cell phone\nGT3: a man is standing by the water and talking on the phone\n        : a man talking on a cell phone next to the water\nTransformer: a man talking on a cell phone on a boat\nPureT: a man talking on a cell phone on a boat in the water\nGT1: a gray day at a park with a stone bench\nGT2: a sidewalk sitting near a green next to a body of water\nGT3: a tree that is sitting in the grass\n        : a bench on the side of a dirt road\nTransformer: a park bench sitting next to a body of water\nPureT: a stone bench sitting next to a tree in a park\nGT1: a person sits on top of a motorcycle with a stuffed toy\nGT2: a person riding a motorcycle with a stuffed animal on \n          the back\nGT3: a person on a motorcycle with a stuffed animal on back\n        : a man riding a motorcycle on a street\nTransformer: a man sitting on a motorcycle with a teddy bear\nPureT: a man riding a motorcycle with a stuffed animal on it\n2\n2\n2\n2\nFigure 3: Examples of captions generated by standard Trans-\nformer, M2 Transformer and our PureT with ground-truths.\nInﬂuence of W-MSA and SW-MSA For quantifying the\ninﬂuence of W-MSA and SW-MSA in our Reﬁning Encoder,\nwe ablate our model with different conﬁgurations of window\nsize ws and shift size ssas shown in Table 3. The number\nof reﬁning encoder and decoder blocks is set to 3. Note that\nthe input VG ∈Rm×D of Reﬁning Encoder has a size of\nm= 12 ×12 in this paper. The W-MSA and SW-MSA de-\ngenerate into MSA when ws= 12 and SW-MSA into W-\nMSA when ss= 0. It can be seen that the model with only\nMSA (ws= 12;ss = 0) performs better than the model with\nonly W-MSA (ws= 6;ss = 0) because W-MSA lacks con-\nnections across windows. However, the model combining\nW-MSA and SW-MSA (ws = 6;ss = 3) can improve the\nperformance of both models above in all metrics.\nInﬂuence of Pre-Fusion module To demonstrate the ef-\nfectiveness of the Pre-Fusion module in our Decoder, we\nremove the Pre-Fusion module from our PureT model and\n2590\ngroupa of zebras grazing in a\nfield with a rainbow in the sky <eos>\nFigure 4: Visualization of attention heatmap on image along caption generation process. For each generated word, we show the\nimage with different brigtness to represent the difference of attention weights.\nws ss B-1 B-4 M R C S\n12 0 82.0 40.3 29.9 59.9 137.5 23.8\n6 0 81.8 40.1 29.9 59.7 136.8 23.8\n6 3 82.1 40.9 30.2 60.1 138.2 24.2\nTable 3: Performance comparison of different conﬁguration\nof window size wsand shift size ss.\nModels B-1 B-4 M R C S\nTransformer 81.6 39.8 29.9 59.6 136.4 23.8\nTransformer\n+ p-f. 82.0 40.3 29.9 59.9 137.5 23.8\nPureT (w/o p-f.) 81.8 40.3 30.0 59.9 137.9 24.0\nPureT 82.1 40.9 30.2 60.1 138.2 24.2\nTable 4: Performance comparison with / without Pre-Fusion\nfor standard Transformer and our proposed PureT.\ncompare it with the full model as shown in rows 4 and 5 of\nTable 4. It can be seen that the Pre-Fusion module improves\nthe performance in all metrics. Furthermore, we construct\nthe standard Transformer (3 blocks of encoder/decoder) as\nthe baseline model, which reaches an excellent performance\nas shown in row 1 in Table 4. Then we extend the baseline\nmodel by adding the Pre-Fusion module (equivalent to the\nmodel in row 1 of Table 3), which also has a better perfor-\nmance in all metrics.\nInﬂuence of the number of stacked blocks We also con-\nduct several experiments to evaluate the inﬂuence of the\nnumber of the Reﬁning Encoder and Decoder blocks. As\nshown in Table 5, models with more than 2 blocks have a\nsigniﬁcant improvement (more than 2.0%) in CIDEr score\ncompare to the model with 1 block. Note that the model\nwith 4 blocks has a signiﬁcant advantage in BLEU scores\nto other models, but considering the increase of model pa-\nrameters and the sufﬁciently excellent performance of the\nmodel with 3 blocks, we set the number of blocks N to 3\nas the ﬁnal conﬁguration. Remarkably, the model with only\n1 block also has a better performance in comparison to ear-\nlier state-of-the-art works (e.g. RSTNet, GET and DLCT)\nin Table 1, which further indicates the effectiveness of our\nmodel.\nLayer B-1 B-4 M R C S\n1 81.8 40.2 29.7 59.5 135.8 23.5\n2 81.8 40.5 30.0 59.9 138.2 23.9\n3 82.1 40.9 30.2 60.1 138.2 24.2\n4 82.7 41.1 30.0 60.1 138.2 24.0\nTable 5: Performance comparison of different number of Re-\nﬁning Encoder and Decoder blocks.\nInﬂuence of different backbone To quantify the inﬂu-\nence of different features extracted by different backbone\nmodels, we adopt different image captioning models, as\nbaseline models and ablate them with different conﬁgura-\ntions of backbone models as shown in Table 6. The base-\nline models include: M2 Transformer (Cornia et al. 2020),\nX-Transformer (Pan et al. 2020) and standard Transformer\n(Vaswani et al. 2017). The backbone models include: Faster\nR-CNN (Ren et al. 2017) in conjunction with ResNet-101,\nwhich is adopted in (Anderson et al. 2018); Faster R-CNN in\nconjunction with ResNeXt-101, which is adopted in (Jiang\net al. 2020); ViT (Dosovitskiy et al. 2021) and SwinTrans-\nformer (Liu et al. 2021).\nAs we can see, grid features extracted by SwinTrans-\nformer can achieve signiﬁcant performance improvement\ncompared with region features extracted by ResNet-101 and\ngrid features extracted by ResNeXt-101 and ViT.\nIn terms of M2 Transformer and X-Transformer, the\nbackbone models of ResNet-101 and ResNeXt-101 have\nsimilar performance. The backbone model of SwinTrans-\nformer comprehensively improves scores of all metrics,\nwhich boosts the CIDEr score more than 3.7% inM2 Trans-\nformer especially. Note that the backbone model with N =\n3 has a better performance than N = 6 in X-Transformer,\nwhich indicates the superiority of SwinTransformer in im-\nage captioning and allows us to explore more tiny and ef-\nﬁcient models and apply it in more actual scenes. In terms\nof standard Transformer, the backbone model of SwinTrans-\nformer reaches an excellent performance and is even better\nthan M2 Transformer and X-Transformer in scores of ME-\nTEOR, CIDEr and SPICE. In terms of our PureT, the back-\nbone of SwinTransformer also achieves a better performance\nthan ResNeXt-101.\nIn general, in our extensive experiments, we ﬁnd that the\n2591\nBaseline Models Backbone Feat. Type Feat. Size N B-1 B-2 B-3 B-4 M R C S\nM2 Transformer\nResNet-101 Region (10−100) 3 y 80.8 - - 39.1 29.2 58.6 131.2 22.6\nResNeXt-101 Grid 7 ×7 3z 80.8 - - 38.9 29.1 58.5 131.7 22.6\nSwinTransformer Grid 12 ×12 3 81.8 66.8 52.6 40.5 29.6 59.9 135.4 23.3\nX-Transformer\nResNet-101 Region (10−100) 6 y 80.9 65.8 51.5 39.7 29.5 59.1 132.8 23.4\nResNeXt-101 Grid 7 ×7 6z 81.0 - - 39.7 29.4 58.9 132.5 23.1\nSwinTransformer Grid 12 ×12 6 81.4 66.3 52.0 39.9 29.5 59.5 133.7 23.4\nSwinTransformer Grid 12 ×12 3 81.9 66.7 52.3 40.1 29.6 59.6 134.8 23.4\nstandard\nTransformer\nResNet-101 Region (10−100) 3 80.0 64.9 50.5 38.7 29.0 58.6 130.1 22.9\nResNeXt-101 Grid 7 ×7 3z 81.2 - - 39.0 29.2 58.9 131.7 22.6\nResNeXt-101 Grid 12 ×12 3 80.8 65.8 51.4 39.4 29.4 59.2 132.8 23.2\nSwinTransformer Grid 12 ×12 3 81.6 66.5 52.0 39.8 29.9 59.6 136.4 23.8\nPureT ResNeXt-101 Grid 12 ×12 3 80.7 65.9 51.7 39.9 29.2 59.1 131.8 23.0\nViT Grid 12 ×12 3 81.6 66.6 52.3 40.3 29.7 59.5 135.2 23.6\nSwinTransformer Grid 12 ×12 3 82.1 67.3 52.0 40.9 30.2 60.1 138.2 24.2\nTable 6: Performance comparison of different conﬁguration of backbone models. ResNet-101 and ResNeXt-101 indicate Faster\nR-CNN in conjunction with them respectively. Region features extracted by ResNet-101 have adaptive size of 10 to 100. Grid\nfeatures extracted by ResNeXt-101 can be extracted in the size of 12 ×12 or 7 ×7 by average pooling as need. Grid features\n(SwinTransformer) are extracted in the size of 12 ×12. N denotes the number of encoder and decoder blocks, superscript †\nindicates that the results are from the respectively ofﬁcial paper and ‡indicates that the results are from (Luo et al. 2021), and\nother results come from our experiments.\nRef. Enc. B-1 B-4 M R C S\nw/o 81.5 39.5 29.3 59.2 134.3 23.0\nM2 81.9 40.2 29.6 59.7 135.9 23.7\nX 81.7 40.0 29.7 59.5 135.5 23.5\nPureT 82.1 40.9 30.2 60.1 138.2 24.2\nTable 7: Performance comparison of different Reﬁning En-\ncoder. w/o indicates deleting Reﬁning Encoder, M2 and X\nindicate replacing Reﬁning Encoder with encoders of M2\nTransformer and X-Transformer respectively.\nbackbone models of CNN (e.g. Faster RCNN in conjunction\nwith ResNet-101 or ResNeXt-101) are more suitable for us-\ning LSTM or Transformer with non-standard MSA (e.g. X-\nTransformer) as decoder, and the backbone of SwinTrans-\nformer is more suitable for using Transformer with standard\nMSA (e.g. M2 Transformer, standard Transformer and our\nPureT) as decoder. Therefore, we intend to explore a lighter\nand simpler Transformer-based model in our future work.\nInﬂuence of different Reﬁning Encoder To further quan-\ntify the inﬂuence of Reﬁning Encoder, we ablate the Reﬁn-\ning Encoder by different conﬁgurations as shown in Table 7.\nWe delete the Reﬁning Encoder to conﬁrm whether the Re-\nﬁning Encoder is a necessary module, and replace our pro-\nposed Reﬁning Encoder with encoders of M2 Transformer\nand X-Transformer to verify the advantages of our Reﬁn-\ning Encoder. As we can see, deleting Reﬁning Encoder can\nalso achieve good performance, which is better than most\nexisting SOTAs in Table 1. But our proposed Reﬁning En-\ncoder or other encoders bring signiﬁcant performance gain\nthan deleting Reﬁning Encoder, which denotes the impor-\ntance of Reﬁning Encoder. Our proposed Reﬁning Encoder\nbrings the maximum gain and achieves the best performance\nthan other, which denotes that the effectiveness and advan-\ntages of our proposed Reﬁning Encoder.\nVisualization Analysis\nFigure 3 proposes some example image captions generated\nby M2 Transformer (ofﬁcial model), standard Transformer\nand our PureT. Note that M2 Transformer adopts Faster R-\nCNN, standard Transformer and PureT adopt SwinTrans-\nformer as the encoder. Generally, our PureT is able to catch\nadditional ﬁne-grained information and generate more accu-\nrate and descriptive captions.\nTo qualitatively evaluate the effect of our PureT, we give\nthe visualization of attention heatmap on the image along\ncaption generation process in Figure 4. It can be observed\nthat our model can attend to correct areas when generating\nwords. When generating nominal words, such as “zebras”,\n“rainbow”, “ﬁeld” and “sky”, the attention heatmap is cor-\nrectly transformed into the body area of the corresponding\nobjects. In addition, our model focuses on the nearby areas\nof zebra heads when generating “grazing”, which correctly\ncaptures the semantic information and conﬁrms the advan-\ntages of our model.\nConclusion\nIn this paper, we propose a pure Transformer-based model,\nwhich adopts SwinTransformer as the backbone encoder and\ncan be trained end-to-end from image to descriptions eas-\nily. Furthermore, we construct a reﬁning encoder to reﬁne\nboth image grid features and global feature with the mutual\nguidance between them, which realizes the complementary\nadvantages between local and global attention. We also fuse\nthe reﬁned global feature with previously generated words in\nthe decoder to enhance the multi-modal interaction, which\nfurther improves the modeling capability. Experimental re-\nsults on MSCOCO dataset demonstrate that our proposed\nmodel achieves a new state-of-the-art performance.\n2592\nReferences\nAnderson, P.; Fernando, B.; Johnson, M.; and Gould, S.\n2016. SPICE: Semantic Propositional Image Caption Eval-\nuation. In Proceedings of the ECCV, 382–398.\nAnderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.;\nGould, S.; and Zhang, L. 2018. Bottom-Up and Top-Down\nAttention for Image Captioning and Visual Question An-\nswering. In Proceedings of the CVPR, 6077–6086.\nCho, K.; van Merrienboer, B.; G ¨ulc ¸ehre, C ¸ .; Bahdanau, D.;\nBougares, F.; Schwenk, H.; and Bengio, Y . 2014. Learn-\ning Phrase Representations using RNN Encoder-Decoder\nfor Statistical Machine Translation. In Proceedings of the\nEMNLP, 1724–1734.\nCornia, M.; Stefanini, M.; Baraldi, L.; and Cucchiara, R.\n2020. Meshed-Memory Transformer for Image Captioning.\nIn Proceedings of the CVPR, 10575–10584.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In Proceedings of the ICLR. OpenRe-\nview.net.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Resid-\nual Learning for Image Recognition. In Proceedings of the\nCVPR, 770–778.\nHerdade, S.; Kappeler, A.; Boakye, K.; and Soares, J. 2019.\nImage Captioning: Transforming Objects into Words. In\nProceedings of the NeurIPS, 11135–11145.\nHochreiter, S.; and Schmidhuber, J. 1997. Long Short-Term\nMemory. Neural Computation, 9(8): 1735–1780.\nHuang, L.; Wang, W.; Chen, J.; and Wei, X. 2019. Attention\non Attention for Image Captioning. In Proceedings of the\nICCV, 4633–4642.\nJi, J.; Luo, Y .; Sun, X.; Chen, F.; Luo, G.; Wu, Y .; Gao, Y .;\nand Ji, R. 2021. Improving Image Captioning by Leveraging\nIntra- and Inter-layer Global Representation in Transformer\nNetwork. In Proceedings of the AAAI, 1655–1663. AAAI\nPress.\nJiang, H.; Misra, I.; Rohrbach, M.; Learned-Miller, E. G.;\nand Chen, X. 2020. In Defense of Grid Features for Visual\nQuestion Answering. In Proceedings of the CVPR, 10264–\n10273. IEEE.\nJiang, W.; Ma, L.; Jiang, Y .; Liu, W.; and Zhang, T. 2018.\nRecurrent Fusion Network for Image Captioning. In Pro-\nceedings of the ECCV, 510–526.\nKarpathy, A.; and Fei-Fei, L. 2017. Deep Visual-Semantic\nAlignments for Generating Image Descriptions. IEEE\nTrans. Pattern Anal. Mach. Intell., 39(4): 664–676.\nKingma, D. P.; and Ba, J. 2015. Adam: A Method for\nStochastic Optimization. In Proceedings of the ICLR.\nKrishna, R.; Zhu, Y .; Groth, O.; Johnson, J.; Hata, K.;\nKravitz, J.; Chen, S.; Kalantidis, Y .; Li, L.; Shamma,\nD. A.; Bernstein, M. S.; and Fei-Fei, L. 2017. Visual\nGenome: Connecting Language and Vision Using Crowd-\nsourced Dense Image Annotations. IJCV, 123(1): 32–73.\nLavie, A.; and Agarwal, A. 2007. METEOR: An Automatic\nMetric for MT Evaluation with High Levels of Correlation\nwith Human Judgments. In Proceedings of the ACL, 228–\n231.\nLi, G.; Zhu, L.; Liu, P.; and Yang, Y . 2019. Entangled Trans-\nformer for Image Captioning. In Proceedings of the ICCV,\n8927–8936.\nLin, C.-Y . 2004. ROUGE: A Package for Automatic Evalua-\ntion of Summaries. In Proceedings of the ACL-04 workshop\non Text Summarization Branches Out.\nLin, T.; Maire, M.; Belongie, S. J.; Hays, J.; Perona, P.; Ra-\nmanan, D.; Doll ´ar, P.; and Zitnick, C. L. 2014. Microsoft\nCOCO: Common Objects in Context. In Proceedings of the\nECCV, 740–755.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin Transformer: Hierarchical Vi-\nsion Transformer using Shifted Windows. arXiv preprint\narXiv:2103.14030.\nLu, J.; Batra, D.; Parikh, D.; and Lee, S. 2019. ViL-\nBERT: Pretraining Task-Agnostic Visiolinguistic Represen-\ntations for Vision-and-Language Tasks. In Proceedings of\nthe NeurIPS, 13–23.\nLuo, Y .; Ji, J.; Sun, X.; Cao, L.; Wu, Y .; Huang, F.; Lin, C.;\nand Ji, R. 2021. Dual-level Collaborative Transformer for\nImage Captioning. In Proceedings of the AAAI, 2286–2293.\nAAAI Press.\nPan, Y .; Yao, T.; Li, Y .; and Mei, T. 2020. X-Linear Atten-\ntion Networks for Image Captioning. In Proceedings of the\nCVPR, 10968–10977.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W. 2002. Bleu:\na Method for Automatic Evaluation of Machine Translation.\nIn Proceedings of the ACL, 311–318.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\nKrueger, G.; and Sutskever, I. 2021. Learning Transfer-\nable Visual Models From Natural Language Supervision. In\nMeila, M.; and Zhang, T., eds., Proceedings of the ICML,\nvolume 139 of Proceedings of Machine Learning Research,\n8748–8763. PMLR.\nRen, S.; He, K.; Girshick, R. B.; and Sun, J. 2017. Faster\nR-CNN: Towards Real-Time Object Detection with Region\nProposal Networks. IEEE Trans. Pattern Anal. Mach. Intell.,\n39(6): 1137–1149.\nRennie, S. J.; Marcheret, E.; Mroueh, Y .; Ross, J.; and Goel,\nV . 2017. Self-Critical Sequence Training for Image Caption-\ning. In Proceedings of the CVPR, 1179–1195.\nSimonyan, K.; and Zisserman, A. 2015. Very Deep Convo-\nlutional Networks for Large-Scale Image Recognition. In\nBengio, Y .; and LeCun, Y ., eds.,Proceedings of the ICLR.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is All you Need. In Proceedings of the NIPS, 5998–\n6008.\nVedantam, R.; Zitnick, C. L.; and Parikh, D. 2015. CIDEr:\nConsensus-based image description evaluation. In Proceed-\nings of the CVPR, 4566–4575.\n2593\nVinyals, O.; Toshev, A.; Bengio, S.; and Erhan, D. 2015.\nShow and tell: A neural image caption generator. In Pro-\nceedings of the CVPR, 3156–3164.\nWang, W.; Chen, Z.; and Hu, H. 2019. Hierarchical Atten-\ntion Network for Image Captioning. In Proceedings of the\nAAAI, 8957–8964.\nXu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville, A. C.;\nSalakhutdinov, R.; Zemel, R. S.; and Bengio, Y . 2015. Show,\nAttend and Tell: Neural Image Caption Generation with Vi-\nsual Attention. In Proceedings of the ICML, 2048–2057.\nYao, T.; Pan, Y .; Li, Y .; and Mei, T. 2018. Exploring Visual\nRelationship for Image Captioning. In Proceedings of the\nECCV, 711–727.\nZhang, X.; Sun, X.; Luo, Y .; Ji, J.; Zhou, Y .; Wu, Y .; Huang,\nF.; and Ji, R. 2021. RSTNet: Captioning With Adaptive At-\ntention on Visual and Non-Visual Words. In Proceedings\nof the CVPR, 15465–15474. Computer Vision Foundation /\nIEEE.\nZhu, L.; and Yang, Y . 2020. ActBERT: Learning Global-\nLocal Video-Text Representations. In Proceedings of the\nCVPR, 8743–8752. IEEE.\n2594",
  "topic": "Closed captioning",
  "concepts": [
    {
      "name": "Closed captioning",
      "score": 0.905120849609375
    },
    {
      "name": "Computer science",
      "score": 0.8352280259132385
    },
    {
      "name": "Transformer",
      "score": 0.7423228025436401
    },
    {
      "name": "Encoder",
      "score": 0.6956743001937866
    },
    {
      "name": "Artificial intelligence",
      "score": 0.589491605758667
    },
    {
      "name": "Grid",
      "score": 0.49863767623901367
    },
    {
      "name": "Pooling",
      "score": 0.46083906292915344
    },
    {
      "name": "Language model",
      "score": 0.45350930094718933
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.36448901891708374
    },
    {
      "name": "Speech recognition",
      "score": 0.3616187870502472
    },
    {
      "name": "Image (mathematics)",
      "score": 0.22235232591629028
    },
    {
      "name": "Voltage",
      "score": 0.10415256023406982
    },
    {
      "name": "Engineering",
      "score": 0.07922109961509705
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    }
  ]
}