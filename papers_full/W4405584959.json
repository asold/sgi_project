{
  "title": "Retrieval Augmented Generation for 10 Large Language Models and its Generalizability in Assessing Medical Fitness",
  "url": "https://openalex.org/W4405584959",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2347612689",
      "name": "Yuhe Ke",
      "affiliations": [
        "Singapore General Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2326964429",
      "name": "Jin Liyuan",
      "affiliations": [
        "Duke-NUS Medical School"
      ]
    },
    {
      "id": "https://openalex.org/A4384585140",
      "name": "Kabilan Elangovan",
      "affiliations": [
        "Singapore Eye Research Institute",
        "Singapore National Eye Center"
      ]
    },
    {
      "id": "https://openalex.org/A2508072520",
      "name": "Hairil Rizal Abdullah",
      "affiliations": [
        "SingHealth"
      ]
    },
    {
      "id": "https://openalex.org/A2065359256",
      "name": "Nan Liu",
      "affiliations": [
        "Singapore General Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2620248450",
      "name": "Alex Sia",
      "affiliations": [
        "KK Women's and Children's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2438065910",
      "name": "Chai Rick Soh",
      "affiliations": [
        "Singapore General Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2794043957",
      "name": "Joshua Yi Min Tung",
      "affiliations": [
        "Singapore General Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2947319600",
      "name": "JASMINE ONG",
      "affiliations": [
        "Singapore General Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A1354848730",
      "name": "Vesela P. Kovacheva",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2619557440",
      "name": "Daniel Shu Wei Ting",
      "affiliations": [
        "Duke-NUS Medical School"
      ]
    },
    {
      "id": "https://openalex.org/A4211402138",
      "name": "Chang Fu-Kuo",
      "affiliations": [
        "Chang Gung Memorial Hospital",
        "Memorial Hospital of South Bend"
      ]
    },
    {
      "id": "https://openalex.org/A2712125072",
      "name": "Shao-Chun Wu",
      "affiliations": [
        "Memorial Hospital of South Bend",
        "Chang Gung Memorial Hospital"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4384700226",
    "https://openalex.org/W4385278400",
    "https://openalex.org/W1626788079",
    "https://openalex.org/W2400747978",
    "https://openalex.org/W4386482202",
    "https://openalex.org/W3167242683",
    "https://openalex.org/W2058432557",
    "https://openalex.org/W3206834005",
    "https://openalex.org/W3020807179",
    "https://openalex.org/W4367672504",
    "https://openalex.org/W4386867830",
    "https://openalex.org/W4392679398",
    "https://openalex.org/W2896334191",
    "https://openalex.org/W1493896010",
    "https://openalex.org/W4383711355",
    "https://openalex.org/W2140731588",
    "https://openalex.org/W3000603264"
  ],
  "abstract": "<title>Abstract</title> <bold>Purpose:</bold> Large Language Models (LLMs) offer potential for medical applications, but often lack the specialized knowledge needed for clinical tasks. Retrieval Augmented Generation (RAG) is a promising approach, allowing for the customization of LLMs with domain-specific knowledge, well-suited for healthcare. We focused on assessing the accuracy, consistency and safety of RAG models in determining a patient’s fitness for surgery and providing additional crucial preoperative instructions. <bold>Methods: </bold>We developed LLM-RAG models using 35 local and 23 international preoperative guidelines and tested them against human-generated responses, with a total of 3682 responses evaluated. Clinical documents were processed, stored, and retrieved using Llamaindex. Ten LLMs (GPT3.5, GPT4, GPT4-o, Llama2-7B, Llama2-13B, LLama2-70b, LLama3-8b, LLama3-70b, Gemini-1.5-Pro and Claude-3-Opus) were evaluated with 1) native model, 2) with local and 3) international preoperative guidelines. Fourteen clinical scenarios were assessed, focusing on 7 aspects of preoperative instructions. Established guidelines and expert physician judgment determined correct responses. Human-generated answers from senior attending anesthesiologists and junior doctors served as a comparison. Comparative analysis was conducted using Fisher’s exact test and agreement for inter-rater agreement within human and LLM responses. <bold>Results</bold>: The LLM-RAG model demonstrated good efficiency, generating answers within 20 seconds, with guideline retrieval taking less than 5 seconds. This performance is faster than the 10 minutes typically estimated by clinicians. Notably, the LLM-RAG model utilizing GPT4 achieved the highest accuracy in assessing fitness for surgery, surpassing human-generated responses (96.4% vs. 86.6%, p=0.016). The RAG models demonstrated generalizable performance, exhibiting similarly favorable outcomes with both international and local guidelines. Additionally, the GPT4 LLM-RAG model exhibited an absence of hallucinations and produced correct preoperative instructions that were comparable to those generated by clinicians. <bold>Conclusions:</bold> This study successfully implements LLM-RAG models for preoperative healthcare tasks, emphasizing the benefits of grounded knowledge, upgradability, and scalability for effective deployment in healthcare settings.",
  "full_text": null,
  "topic": "Generalizability theory",
  "concepts": [
    {
      "name": "Generalizability theory",
      "score": 0.9572669863700867
    },
    {
      "name": "Computer science",
      "score": 0.49019914865493774
    },
    {
      "name": "Psychology",
      "score": 0.3856053352355957
    },
    {
      "name": "Natural language processing",
      "score": 0.3706824779510498
    },
    {
      "name": "Econometrics",
      "score": 0.368779718875885
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34423935413360596
    },
    {
      "name": "Economics",
      "score": 0.1755312979221344
    },
    {
      "name": "Developmental psychology",
      "score": 0.06097489595413208
    }
  ]
}