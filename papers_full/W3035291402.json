{
  "title": "Learning Architectures from an Extended Search Space for Language Modeling",
  "url": "https://openalex.org/W3035291402",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2168056687",
      "name": "Li Yinqiao",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2126918207",
      "name": "Chi Hu",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2100256679",
      "name": "Yuhao Zhang",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A1988074067",
      "name": "Nuo Xu",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2150032407",
      "name": "Yufan Jiang",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A1983914940",
      "name": "Tong Xiao",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2496766346",
      "name": "Jingbo Zhu",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2163293007",
      "name": "Tongran Liu",
      "affiliations": [
        "Institute of Psychology, Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2104445069",
      "name": "Changliang Li",
      "affiliations": [
        "Kingsoft (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2556833785",
    "https://openalex.org/W2964515685",
    "https://openalex.org/W2952436057",
    "https://openalex.org/W2963186636",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W4300687381",
    "https://openalex.org/W2805097512",
    "https://openalex.org/W2964081807",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2742448943",
    "https://openalex.org/W2810075754",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W4299838440",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2113207845",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W2796265726",
    "https://openalex.org/W4300687870",
    "https://openalex.org/W2792376130",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2965658867",
    "https://openalex.org/W2817535134",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2804044248",
    "https://openalex.org/W3005996763",
    "https://openalex.org/W2963374479",
    "https://openalex.org/W2963137684",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2962832505",
    "https://openalex.org/W4295185264",
    "https://openalex.org/W2971628371",
    "https://openalex.org/W2912521296",
    "https://openalex.org/W2946558277",
    "https://openalex.org/W2111935653",
    "https://openalex.org/W2963970792",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2970463839",
    "https://openalex.org/W2970119519",
    "https://openalex.org/W2567070169",
    "https://openalex.org/W2962746461",
    "https://openalex.org/W2138784882",
    "https://openalex.org/W2963971244",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2962941447",
    "https://openalex.org/W2553303224",
    "https://openalex.org/W2553397501",
    "https://openalex.org/W2150355110",
    "https://openalex.org/W2963430354",
    "https://openalex.org/W2748513770",
    "https://openalex.org/W2951104886",
    "https://openalex.org/W2964331719",
    "https://openalex.org/W4297788867"
  ],
  "abstract": "Neural architecture search (NAS) has advanced significantly in recent years but most NAS systems restrict search to learning architectures of a recurrent or convolutional cell. In this paper, we extend the search space of NAS. In particular, we present a general approach to learn both intra-cell and inter-cell architectures (call it ESS). For a better search result, we design a joint learning method to perform intra-cell and inter-cell NAS simultaneously. We implement our model in a differentiable architecture search system. For recurrent neural language modeling, it outperforms a strong baseline significantly on the PTB and WikiText data, with a new state-of-the-art on PTB. Moreover, the learned architectures show good transferability to other systems. E.g., they improve state-of-the-art systems on the CoNLL and WNUT named entity recognition (NER) tasks and CoNLL chunking task, indicating a promising line of research on large-scale pre-learned architectures.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6629‚Äì6639\nJuly 5 - 10, 2020.c‚Éù2020 Association for Computational Linguistics\n6629\nLearning Architectures from an Extended Search Space\nfor Language Modeling\nYinqiao Li1, Chi Hu1, Yuhao Zhang1, Nuo Xu1, Yufan Jiang1,\nTong Xiao1,2‚àó, Jingbo Zhu1,2, Tongran Liu3, Changliang Li4\n1NLP Lab, Northeastern University, Shenyang, China\n2NiuTrans Research, Shenyang, China\n3CAS Key Laboratory of Behavioral Science, Institute of Psychology, CAS, Beijing, China\n4Kingsoft AI Lab, Beijing, China\nli.yin.qiao.2012@hotmail.com,\n{huchinlp,yoohao.zhang}@gmail.com,\n{xunuo0629,jiangyufan2018}@outlook.com,\n{xiaotong,zhujingbo}@mail.neu.edu.com,\nliutr@psych.ac.cn,lichangliang@kingsoft.com\nAbstract\nNeural architecture search (NAS) has ad-\nvanced signiÔ¨Åcantly in recent years but most\nNAS systems restrict search to learning archi-\ntectures of a recurrent or convolutional cell. In\nthis paper, we extend the search space of NAS.\nIn particular, we present a general approach\nto learn both intra-cell and inter-cell architec-\ntures (call it ESS). For a better search result,\nwe design a joint learning method to perform\nintra-cell and inter-cell NAS simultaneously.\nWe implement our model in a differentiable\narchitecture search system. For recurrent neu-\nral language modeling, it outperforms a strong\nbaseline signiÔ¨Åcantly on the PTB and Wiki-\nText data, with a new state-of-the-art on PTB.\nMoreover, the learned architectures show good\ntransferability to other systems. E.g., they im-\nprove state-of-the-art systems on the CoNLL\nand WNUT named entity recognition (NER)\ntasks and CoNLL chunking task, indicating a\npromising line of research on large-scale pre-\nlearned architectures.\n1 Introduction\nNeural models have shown remarkable perfor-\nmance improvements in a wide range of natural\nlanguage processing (NLP) tasks. Systems of this\nkind can broadly be characterized as following a\nneural network design: we model the problem via\na pre-deÔ¨Åned neural architecture, and the resulting\nnetwork is treated as a black-box family of func-\ntions for which we Ô¨Ånd parameters that can general-\nize well on test data. This paradigm leads to many\nsuccessful NLP systems based on well-designed\narchitectures. The earliest of these makes use of re-\ncurrent neural networks (RNNs) for representation\nlearning (Bahdanau et al., 2015; Wu et al., 2016),\n‚àóCorresponding author.\nwhereas recent systems have successfully incorpo-\nrated fully attentive models into language genera-\ntion and understanding (Vaswani et al., 2017).\nIn designing such models, careful engineering\nof the architecture plays a key role for the state-of-\nthe-art though it is in general extremely difÔ¨Åcult\nto Ô¨Ånd a good network structure. The next obvi-\nous step is toward automatic architecture design.\nA popular method to do this is neural architecture\nsearch (NAS). In NAS, the common practice is that\nwe Ô¨Årst deÔ¨Åne a search space of neural networks,\nand then Ô¨Ånd the most promising candidate in the\nspace by some criteria. Previous efforts to make\nNAS more accurate have focused on improving\nsearch and network evaluation algorithms. But the\nsearch space is still restricted to a particular scope\nof neural networks. For example, most NAS meth-\nods are applied to learn the topology in a recurrent\nor convolutional cell, but the connections between\ncells are still made in a heuristic manner as usual\n(Zoph and Le, 2017; Elsken et al., 2019).\nNote that the organization of these sub-networks\nremains important as to the nature of architecture\ndesign. For example, the Ô¨Årst-order connectivity\nof cells is essential to capture the recurrent dynam-\nics in RNNs. More recently, it has been found\nthat additional connections of RNN cells improve\nLSTM models by accessing longer history on lan-\nguage modeling tasks (Melis et al., 2019). Similar\nresults appear in Transformer systems. Dense con-\nnections of distant layers help in learning a deep\nTransformer encoder for machine translation (Shen\net al., 2018). A natural question that arises is: can\nwe learn the connectivity of sub-networks for better\narchitecture design?\nIn this paper, we address this issue by enlarging\nthe scope of NAS and learning connections among\n6630\nht‚àí1\nxt‚àí1\nxt\nht+1\nxt+1\nht\n(a) Connections in a cell\nht‚àí1\nxt‚àí1\nht\nxt\nht+1\nxt+1\nht‚àí3\nht‚àí2\n(b) Connections among cells\nFigure 1: Examples of intra and inter-cell architectures.\nsub-networks that are designed in either a hand-\ncrafted or automatic way (Figure 1). We call this\nthe Extended Search Space method for NAS (or\nESS for short). Here, we choose differentiable ar-\nchitecture search as the basis of this work because\nit is efÔ¨Åcient and gradient-friendly. We present a\ngeneral model of differentiable architecture search\nto handle arbitrary search space of NAS, which\noffers a uniÔ¨Åed framework of describing intra-cell\nNAS and inter-cell NAS. Also, we develop a joint\napproach to learning both high-level and low-level\nconnections simultaneously. This enables the inter-\naction between intra-cell NAS and inter-cell NAS,\nand thus the ability of learning the full architecture\nof a neural network.\nOur ESS method is simple for implementation.\nWe experiment with it in an RNN-based system for\nlanguage modeling. On the PTB and WikiText data,\nit outperforms a strong baseline signiÔ¨Åcantly by 4.5\nand 2.4 perplexity scores. Moreover, we test the\ntransferability of the learned architecture on other\ntasks. Again, it shows promising improvements on\nboth NER and chunking benchmarks, and yields\nnew state-of-the-art results on NER tasks. This\nindicates a promising line of research on large-\nscale pre-learned architectures. More interestingly,\nit is observed that the inter-cell NAS is helpful\nin modeling rare words. For example, it yields a\nbigger improvement on the rare entity recognition\ntask (WNUT) than that on the standard NER task\n(CoNLL).\n2 Related work\nNAS is a promising method toward AutoML (Hut-\nter et al., 2018), and has been recently applied\nto NLP tasks (So et al., 2019; Jiang et al., 2019;\nLi and Talwalkar, 2019). Several research teams\nhave investigated search strategies for NAS. The\nvery early approaches adopted evolutionary algo-\nrithms to model the problem (Angeline et al., 1994;\nStanley and Miikkulainen, 2002), while Bayesian\nand reinforcement learning methods made big pro-\ngresses in computer vision and NLP later (Bergstra\net al., 2013; Baker et al., 2017; Zoph and Le, 2017).\nMore recently, gradient-based methods were suc-\ncessfully applied to language modeling and image\nclassiÔ¨Åcation based on RNNs and CNNs (Liu et al.,\n2019a). In particular, differentiable architecture\nsearch has been of great interest to the commu-\nnity because of its efÔ¨Åciency and compatibility to\noff-the-shelf tools of gradient-based optimization.\nDespite of great success, previous studies re-\nstricted themselves to a small search space of neu-\nral networks. For example, most NAS systems\nwere designed to Ô¨Ånd an architecture of recurrent\nor convolutional cell, but the remaining parts of the\nnetwork are handcrafted (Zhong et al., 2018; Brock\net al., 2018; Elsken et al., 2019). For a larger search\nspace, Zoph et al. (2018) optimized the normal cell\n(i.e., the cell that preserves the dimensionality of\nthe input) and reduction cell (i.e., the cell that re-\nduces the spatial dimension) simultaneously and\nexplored a larger region of the space than the single-\ncell search. But it is still rare to see studies on the\nissue of search space though it is an important fac-\ntor to NAS. On the other hand, it has been proven\nthat the additional connections between cells help\nin RNN or Transformer-based models (He et al.,\n2016; Huang et al., 2017; Wang et al., 2018, 2019).\nThese results motivate us to take a step toward the\nautomatic design of inter-cell connections and thus\nsearch in a larger space of neural architectures.\n3 Inter-Cell and Intra-Cell NAS\nIn this work we use RNNs for description. We\nchoose RNNs because of their effectiveness at pre-\nserving past inputs for sequential data processing\ntasks. Note that although we will restrict ourselves\nto RNNs for our experiments, the method and dis-\ncussion here can be applied to other types of mod-\nels.\n6631\n3.1 Problem Statement\nFor a sequence of input vectors {x1,...,x T}, an\nRNN makes a cell on top of every input vector.\nThe RNN cell receives information from previous\ncells and input vectors. The output at time step tis\ndeÔ¨Åned to be:\nht = œÄ(ÀÜht‚àí1,ÀÜxt) (1)\nwhere œÄ(¬∑) is the function of the cell. ÀÜht‚àí1 is the\nrepresentation vector of previous cells, and ÀÜxt is\nthe representation vector of the inputs up to time\nstep t. More formally, we deÔ¨Åne ÀÜht‚àí1 and ÀÜxt as\nfunctions of cell states and model inputs, like this\nÀÜht‚àí1 = f(h[0,t‚àí1]; x[1,t‚àí1]) (2)\nÀÜxt = g(x[1,t]; h[0,t‚àí1]) (3)\nwhere h[0,t‚àí1] = {h0,...,h t‚àí1}and x[1,t‚àí1] =\n{x1,...,x t‚àí1}. f(¬∑) models the way that we pass\ninformation from previous cells to the next. Like-\nwise, g(¬∑) models the case of input vectors. These\nfunctions offer a general method to model connec-\ntions between cells. For example, one can obtain a\nvanilla recurrent model by settingÀÜht‚àí1 = ht‚àí1 and\nÀÜxt = xt, while more intra-cell connections can be\nconsidered if sophisticated functions are adopted\nfor f(¬∑) and g(¬∑).\nWhile previous work focuses on searching for\nthe desirable architecture design of œÄ(¬∑), we take\nf(¬∑) and g(¬∑) into account and describe a more\ngeneral case here. We separate two sub-problems\nout from NAS for conceptually cleaner description:\n‚Ä¢Intra-Cell NAS. It learns the architecture of\na cell (i.e., œÄ(¬∑)).\n‚Ä¢Inter-Cell NAS. It learns the way of connect-\ning the current cell with previous cells and\ninput vectors (i.e., f(¬∑) and g(¬∑)).\nIn the following, we describe the design and\nimplementation of our inter-cell and intra-cell NAS\nmethods.\n3.2 Differentiable Architecture Search\nFor search algorithms, we follow the method of\ndifferentiable architecture search (DARTS). It is\ngradient-based and runs orders of magnitude faster\nthan earlier methods (Zoph et al., 2018; Real et al.,\n2019). DARTS represents networks as a directed\nacyclic graph (DAG) and search for the appropri-\nate architecture on it. For a DAG, the edge oi,j(¬∑)\nF(Œ±,Œ≤)\n...\n...\nŒ±\nSŒ±\n...\n...\nŒ≤\nSŒ≤\nFigure 2: Formalizing intra and inter-cell NAS as learn-\ning function F(¬∑).\nbetween node pair (i,j) performs an operation to\ntransform the input (i.e., tail) to the output (i.e.,\nhead). Like Liu et al. (2019a)‚Äôs method and oth-\ners, we choose operations from a list of activation\nfunctions, e.g., sigmoid, identity and etc1. A node\nrepresents the intermediate states of the networks.\nFor node i, it weights vectors from all predecessor\nnodes (j <i) and simply sums over them. Let si\nbe the state of node i. We deÔ¨Åne si to be:\nsi =\n‚àë\nj<i\n‚àë\nk\nŒ∏i,j\nk ¬∑oi,j\nk (sj ¬∑Wj) (4)\nwhere Wj is the parameter matrix of the linear\ntransformation, and Œ∏i,j\nk is the weight indicating the\nimportance of oi,j\nk (¬∑). Here the subscript kmeans\nthe operation index. Œ∏i,j\nk is obtained by softmax\nnormalization over edges between nodes iand j:\nŒ∏i,j\nk = exp(wi,j\nk )/‚àë\nk‚Ä≤exp(wi,j\nk‚Ä≤). In this way, the\ninduction of discrete networks is reduced to learn-\ning continuous variables {Œ∏i,j\nk }at the end of the\nsearch process. This enables the use of efÔ¨Åcient\ngradient descent methods. Such a model encodes\nan exponentially large number of networks in a\ngraph, and the optimal architecture is generated by\nselecting the edges with the largest weights.\nThe common approach to DARTS constraints the\noutput of the generated network to be the last node\nthat averages the outputs of all preceding nodes.\nLet sn be the last node of the network. We have\nsn = 1\nn‚àí1\nn‚àí1‚àë\ni=1\nsi (5)\nGiven the input vectors, the network found by\nDARTS generates the result at the Ô¨Ånal node sn.\n1We also consider a special activation function ‚Äúdrop‚Äù that\nunlinks two nodes.\n6632\nht‚àí1\nxt\nyt\nÀÜxt\nÀÜht‚àí1\nht\nxt‚àí1 xt‚àí2 xt‚àí3\nht‚àí1\nht‚àí2\nht‚àí3\n...\ne1\ns1\ns2\ns3\nsnÀÜht‚àí1\nÀÜxt\nht\nIntra-cell\n... Avg\ne1\ne2\ne3\ns1\ns2\ns3\ns4 sn\nht‚àí1\nht‚àí2\nht‚àí3\n‚äô\nInter-cell\n...\nAvg\nxt\nFigure 3: An example of intra-cell and inter-cell NAS in RNN models.\nHere we present a method to Ô¨Åt this model into intra\nand inter-cell NAS. We re-formalize the function\nfor which we Ô¨Ånd good architectures as F(Œ±; Œ≤).\nŒ±and Œ≤ are two groups of the input vectors. We\ncreate DAGs on them individually. This gives us\ntwo DAGs with sŒ± and sŒ≤ as the last nodes. Then,\nwe make the Ô¨Ånal output by a Hadamard product\nof sŒ± and sŒ≤, like this,\nF(Œ±; Œ≤) = sŒ± ‚äôsŒ≤ (6)\nSee Figure 2 for the network of an example\nF(Œ±; Œ≤). This method transforms the NAS prob-\nlem into two learning tasks. The design of two\nseparate networks allows the model to group re-\nlated inputs together, rather than putting everything\ninto a ‚Äúmagic‚Äù system of NAS. For example, for\nthe inter-cell function f(¬∑), it is natural to learn the\npre-cell connection from h[0,t‚àí1], and learn the im-\npact of the model inputs from x[1,t‚àí1]. It is worth\nnoting that the Hadamard product of sŒ± and sŒ≤ is\ndoing something very similar to the gating mecha-\nnism which has been widely used in NLP (Dauphin\net al., 2017; Bradbury et al., 2017; Gehring et al.,\n2017). For example, one can learn sŒ≤ as a gate and\ncontrol how much sŒ± is used for Ô¨Ånal output. Table\n1 gives the design of Œ±and Œ≤for the functions used\nin this work.\nAnother note on F(Œ±; Œ≤). The grouping reduces\na big problem into two cheap tasks. It is particularly\nimportant for building affordable NAS systems be-\ncause computational cost increases exponentially\nas more input nodes are involved. Our method in-\nstead has a linear time complexity if we adopt a\nreasonable constraint on group size, leading to a\nFunction Œ± Œ≤\nœÄ(¬∑) {ÀÜht‚àí1,ÀÜxt} 1\nf(¬∑) h[0,t‚àí1] x[1,t‚àí1]\ng(¬∑) x[1,t] h[0,t‚àí1]\nTable 1: Œ±and Œ≤for different functions\npossibility of exploring a much larger space during\nthe architecture search process.\n3.3 The Intra-Cell Search Space\nThe search of intra-cell architectures is trivial.\nSince Œ≤ = 1 and sŒ≤ = 1 (see Table 1), we are\nbasically performing NAS on a single group of\ninput vectors ÀÜht‚àí1 and ÀÜxt. We follow Liu et al.\n(2019a)‚Äôs work and force the input of networks to\nbe a single layer network of ÀÜht‚àí1 and ÀÜxt. This can\nbe described as\ne1 = tanh(ÀÜht‚àí1 ¬∑W(h) + ÀÜxt ¬∑W(x)) (7)\nwhere W(h) and W(x) are parameters of the trans-\nformation, and tanh is the non-linear transforma-\ntion. e1 is the input node of the graph. See Figure\n3 for intra-cell NAS of an RNN models.\n3.4 The Inter-Cell Search Space\nTo learn ÀÜht‚àí1 and ÀÜxt, we can run the DARTS sys-\ntem as described above. However, Eqs. (2-3) de-\nÔ¨Åne a model with a varying number of parameters\nfor different time steps, in which our architecture\nsearch method is not straightforwardly applicable.\nApart from this, a long sequence of RNN cells\nmakes the search intractable.\n6633\nFunction JOINT LEARN (rounds, w, W)\n1: for iin range(1, rounds) do\n2: while intra-cell modelnot converged do\n3: Update intra-cell w(intra) and W\n4: while inter-cell modelnot converged do\n5: Update inter-cell w(inter) and W\n6: Derive architecturebased on w\n7: return architecture\nFigure 4: Joint search of intra-cell and inter-cell archi-\ntectures. w = edge weights, and W = model parame-\nters.\nFor a simpliÔ¨Åed model, we re-deÔ¨Åne f(¬∑) and\ng(¬∑) as:\nf(h[0,t‚àí1]; x[1,t‚àí1]) = f‚Ä≤(ht‚àí1; x[t‚àím,t‚àí1]) (8)\ng(x[1,t]; h[0,t‚àí1]) = g‚Ä≤(xt; h[t‚àím,t‚àí1]) (9)\nwhere mis a hyper-parameter that determines how\nmuch history is considered. Eq. (8) indicates a\nmodel that learns a network on x[t‚àím,t‚àí1] (i.e.,\nŒ≤ = x[t‚àím,t‚àí1]). Then, the output of the learned\nnetwork (i.e., sŒ≤) is used as a gate to control the\ninformation that we pass from the previous cell to\nthe current cell (i.e., Œ± = {ht‚àí1}). Likewise, Eq.\n(9) deÔ¨Ånes a gate on h[t‚àím,t‚àí1] and controls the\ninformation Ô¨Çow from xt to the current cell.\nLearning f‚Ä≤(¬∑) and g‚Ä≤(¬∑) Ô¨Åts our method well due\nto the Ô¨Åxed number of input vectors. Note thatf‚Ä≤(¬∑)\nhas minput vectors x[t‚àím,t‚àí1] for learning the gate\nnetwork. Unlike what we do in intra-cell NAS, we\ndo not concatenate them into a single input vector.\nInstead, we create a node for every input vector,\nthat is, the input vector ei = xt‚àíi links with node\nsi. We restrict si to only receive inputs from ei for\nbetter processing of each input. This can be seen\nas a pruned network for the model described in Eq.\n(4). See Figure 3 for an illustration of inter-cell\nNAS.\n4 Joint Learning for Architecture Search\nOur model is Ô¨Çexible. For architecture search, we\ncan run intra-cell NAS, or inter-cell NAS, or both\nof them as needed. However, we found that sim-\nply joining intra-cell and inter-cell architectures\nmight not be desirable because both methods were\nrestricted to a particular region of the search space,\nand the simple combination of them could not guar-\nantee the global optimum.\nThis necessitates the inclusion of interactions be-\ntween intra-cell and inter-cell architectures into the\nsearch process. Generally, the optimal inter-cell\narchitecture depends on the intra-cell architecture\nused in search, and vice versa. A simple method\nthat considers this issue is to learn two models in\na joint manner. Here, we design a joint search\nmethod to make use of the interaction between\nintra-cell NAS and inter-cell NAS. Figure 4 shows\nthe algorithm. It runs for a number of rounds. In\neach round, we Ô¨Årst learn an optimal intra-cell ar-\nchitecture by Ô¨Åxing the inter-cell architecture, and\nthen learn a new inter-cell architecture by Ô¨Åxing\nthe optimal intra-cell architecture that we Ô¨Ånd just\nnow.\nObviously, a single run of intra-cell (or inter-cell)\nNAS is a special case of our joint search method.\nFor example, one can turn off the inter-cell NAS\npart (lines 4-5 in Figure 4) and learn intra-cell archi-\ntectures solely. In a sense, the joint NAS method\nextends the search space of individual intra-cell\n(or inter-cell) NAS. Both intra-cell and inter-cell\nNAS shift to a new region of the parameter space\nin a new round. This implicitly explores a larger\nnumber of underlying models. As shown in our ex-\nperiments, joint NAS learns intra-cell architectures\nunlike those of the individual intra-cell NAS, which\nleads to better performance in language modeling\nand other tasks.\n5 Experiments\nWe experimented with our ESS method on Penn\nTreebank and WikiText language modeling tasks\nand applied the learned architecture to NER and\nchunking tasks to test its transferability.\n5.1 Experimental Setup\nFor language modeling task, the monolingual and\nevaluation data came from two sources.\n‚Ä¢Penn Treebank (PTB). We followed the stan-\ndard preprocessed version of PTB (Mikolov\net al., 2010). It consisted of 929k training\nwords, 73k validation words and 82k test\nwords. The vocabulary size was set to 10k.\n‚Ä¢WikiText-103 (WT-103). We also used\nWikiText-103 (Merity et al., 2017) data to\nsearch for a more universal architecture for\nNLP tasks. This dataset contained a larger\ntraining set of 103 million words and 0.2 mil-\nlion words in the validation and test sets.\n6634\nDataset Method Search Space Params Perplexity Search Cost\nintra-cell inter-cell valid test (GPU days)\nPTB\nAWD-LSTM (Merity et al., 2018c) - - 24M 61.2 58.8 -\nTransformer-XL (Dai et al., 2019) - - 24M 56.7 54.5 -\nMogriÔ¨Åer LSTM (Melis et al., 2019) - - 23M 51.4 50.1 -\nENAS (Pham et al., 2018) \u0013 - 24M 60.8 58.6 0.50\nRS (Li and Talwalkar, 2019) \u0013 - 23M 57.8 55.5 2\nDARTS‚Ä† \u0013 - 23M 55.2 53.0 0.25\nESS - \u0013 23M 54.1 52.3 0.5\nESS \u0013 \u0013 23M 47.9 45.6 0.5\nWT-103\nQRNN (Merity et al., 2018a) - - 151M 32.0 33.0 -\nHebbian + Cache (Rae et al., 2018) - - - 29.9 29.7 -\nTransformer-XL (Dai et al., 2019) - - 151M 23.1 24.0 -\nDARTS‚Ä† \u0013 - 151M 31.4 31.6 1\nESS \u0013 \u0013 156M 28.8 29.2 1.5\nTable 2: Comparison of language modeling methods on PTB and WikiText-103 tasks (lower perplexity is better).\n‚Ä†Obtained by training the corresponding architecture using our setup.\nNER and chunking tasks were also used to test\nthe transferability of the pre-learned architecture.\nWe transferred the intra and inter-cell networks\nlearned on WikiText-103 to the CoNLL-2003 (En-\nglish), the WNUT-2017 NER tasks and the CoNLL-\n2000 tasks. The CoNLL-2003 task focused on the\nnewswire text, while the WNUT-2017 contained a\nwider range of English text which is more difÔ¨Åcult\nto model.\nOur ESS method consisted of two components,\nincluding recurrent neural architecture search and\narchitecture evaluation. During the search process,\nwe ran our ESS method to search for the intra-cell\nand inter-cell architectures jointly. In the second\nstage, the learned architecture was trained and eval-\nuated on the test dataset.\nFor architecture search on language modeling\ntasks, we applied 5 activation functions as the can-\ndidate operations, including drop, identity, sigmoid,\ntanh and relu. On the PTB modeling task, 8 nodes\nwere equipped in the recurrent cell. For the inter-\ncell architecture, it received 3 input vectors from\nthe previous cells and consisted of the same number\nof the intermediate nodes. By default, we trained\nour ESS models for 50 rounds. We setbatch= 256\nand used 300 hidden units for the intra-cell model.\nThe learning rate was set as 3 √ó10‚àí3 for the intra-\ncell architecture and 1 √ó10‚àí3 for the inter-cell\narchitecture. The BPTT (Werbos, 1990) length was\n35. For the search process on WikiText-103, we\ndeveloped a more complex model to encode the\nrepresentation. There were 12 nodes in each cell\nand 5 nodes in the inter-cell networks. The batch\nsize was 128 and the number of hidden units was\n300 which was the same with that on the PTB task.\nWe set the intra-cell and inter-cell learning rate to\n1 √ó10‚àí3 and 1 √ó10‚àí4. A larger window size\n(= 70 ) for BPTT was applied for the WikiText-\n103. All experiments were run on a single NVIDIA\n1080Ti.\nAfter the search process, we trained the learned\narchitectures on the same data. To make it compa-\nrable with previous work, we copied the setup in\nMerity et al. (2018b). For PTB, the size of hidden\nlayers was set as 850 and the training epoch was\n3,000. While for the WikiText-103, we enlarged\nthe number of hidden units to 2,500 and trained the\nmodel for 30 epochs. Additionally, we transferred\nthe learned architecture to NER and chunking tasks\nwith the setting in Akbik et al. (2019). We only\nmodiÔ¨Åed the batch size to 24 and hidden size to\n512.\n5.2 Results\n5.2.1 Language Modeling tasks\nHere we report the perplexity scores, number of pa-\nrameters and search cost on the PTB and WikiText-\n103 datasets (Table 2). First of all, the joint ESS\nmethod improves the performance on language\nmodeling tasks signiÔ¨Åcantly. Moreover, it does\nnot introduce many parameters. Our ESS method\nachieves state-of-the-art result on the PTB task.\nIt outperforms the manually designed MogriÔ¨Åer-\nLSTM by 4.5 perplexity scores on the test set. On\n6635\n10/1 9/2 8/3 7/4 6/5 5/6 4/7 3/8 2/9 1/10\n59.5\n63.5\n67.5\nNumber of nodes (intra/inter)\nPerplexity NAS\nFigure 5: Perplexity on the validation data (PTB) vs.\nnumber of nodes in intra and inter-cell.\nthe WikiText task, it still yields a +2.4 perplexity\nscores improvement over the strong NAS baseline\n(DARTS) method. These results indicate that ESS\nis robust and can learn better architectures by en-\nlarging the scope of search space.\nAlso, we Ô¨Ånd that searching for the appropri-\nate connections among cells plays a more impor-\ntant role in improving the model performance. We\nobserve that the intra-cell NAS (DARTS) system\nunderperforms the inter-cell counterpart with the\nsame number of parameters. It is because the well-\ndesigned intra-cell architectures (e.g., MogriÔ¨Åer-\nLSTM) are actually competitive with the NAS\nstructures. However, the fragile connections among\ndifferent cells greatly restrict the representation\nspace. The additional inter-cell connections are\nable to encode much richer context.\nNevertheless, our ESS method does not defeat\nthe manual designed Transformer-XL model on the\nWikiText-103 dataset, even though ESS works bet-\nter than other RNN-based NAS methods. This is\npartially due to the better ability of Transformer-XL\nto capture the language representation. Note that\nRNNs are not good at modeling the long-distance\ndependence even if more history states are consid-\nered. It is a good try to apply ESS to Transformer\nbut this is out of the scope of this work.\n5.2.2 Sensitivity Analysis\nTo modulate the complexity of the intra and inter-\ncell, we study the system behaviors under different\nnumbers of intermediate nodes (Figure 5). Fix-\ning the number of model parameters, we compare\nthese systems under different numbers of the intra\nand inter-cell nodes. Due to the limited space, we\nshow the result on the PTB in the following sen-\nsitivity analysis. We observe that an appropriate\nchoice of node number (8 nodes for intra-cell and\n3 nodes for inter-cell) brings a consistent improve-\nment. More interestingly, we Ô¨Ånd that too many\nnodes for inter-cell architecture do not improve the\nmodel representation ability. This is reasonable\n0.5K 2K 3.5K 5K\n400\n550\n700\n850\n# of Training Steps\nPerplexity\njoint\nintra\n0.5K 2K 3.5K 5K\n0.00\n0.15\n0.30\n0.45\n0.60\n# of Training Steps\nMAD\nintra inter\nFigure 6: Perplexity on the validation data (PTB)\nand Mean Absolute Deviation (MAD) between edge\nweights and uniform distribution vs. number of train-\ning steps.\nWord Count ‚àÜloss Word Count ‚àÜloss\nmcmoran 11 -0.74 the 59421 -0.009\ncie. 9 -0.66 <unk > 53299 -0.004\nmall 13 -0.65 <eos > 49199 -0.010\nmissile 23 -0.55 N 37607 -0.008\nsiemens 12 -0.51 of 28427 -0.008\nbaldwin 9 -0.51 to 27430 -0.004\nnÔ¨Ç 21 -0.49 a 24755 -0.013\nprime-time 17 -0.47 in 21032 -0.015\nTable 3: Difference in word loss (normalized by word\ncounts) on validation data when searching intra and\ninter-cell jointly. The left column contains the words\nwith eight best improvements (larger absolute value of\n‚àÜloss) and right column presents the most frequent\nwords in the validation data.\nbecause more inter-cell nodes refer to considering\nmore history in our system. But for language mod-\neling, the current state is more likely to be relevant\nto most recent words. Too many inputs to the gate\nnetworks raise difÔ¨Åculties in modeling.\nWe observe that our ESS method leads to a\nmodel that is easier to train. The left part in Figure\n6 plots the validation perplexity at different training\nsteps. The loss curve of joint ESS signiÔ¨Åcantly goes\ndown as the training proceeds. More interestingly,\nour joint learning method makes the model achieve\na lower perplexity than the intra-cell NAS system.\nThis indicates better networks can be obtained in\nthe search process. Additionally, the convergence\ncan be observed from the right part in Figure 6.\nHere we apply Mean Absolute Deviation (MAD)\nto deÔ¨Åne the distance between edge weights and\ninitial uniform distribution. It is obvious that both\nthe intra and inter-cell architectures change little at\nthe Ô¨Ånal searching steps.\nIn order to Ô¨Ågure out the advantage of inter-cell\nconnections, we detail the model contribution on\neach word on the validation data. SpeciÔ¨Åcally, we\ncompute the difference in word loss function (i.e.,\n6636\nùë•!\n‚Ñé!\"# 0\n12 34\n5\n6\n87\n‚Ñé!\nidentityrelusigmoidrelusigmoidrelu\nidentity\nrelu\nidentityidentityrelureluidentityidentityrelu\nsigmoid\n(a)Anintra-cellarchitecturefoundbyusinginter-cell connections(b)Anintra-cell architecture foundwithoutusinginter-cell connections\nFigure 7: Comparison of intra-cell architectures found by using and not using additional inter-cell connections\nModels F1\nLSTM-CRF (Lample et al., 2016) 90.94\nLSTM-CRF + ELMo (Peters et al., 2018) 92.22\nLSTM-CRF + Flair (Akbik et al., 2019) 93.18\nGCDT + BERTLARGE (Liu et al., 2019b) 93.47\nCNN Large + ELMo (Baevski et al., 2019) 93.50\nDARTS + Flair (Jiang et al., 2019) 93.13\nI-DARTS + Flair (Jiang et al., 2019) 93.47\nESS 91.78\nESS + Flair 93.62\nTable 4: F1 scores on CoNLL-2003 NER task. Bi-\nLSTM\nlog perplexity) between methods with and without\ninter-cell NAS. The words with eight best improve-\nments are shown in the left column of Table 3. We\nobserve that the rare words in the training set ob-\ntain more signiÔ¨Åcant improvements. In contrast,\nthe most frequent words lead to very modest de-\ncrease in loss (right column of Table 3). This is\nbecause the connections between multiple cells en-\nable learning rare word representations from more\nhistories. While for common words, they can ob-\ntain this information from rich contexts. More in-\nputs from previous cells do not bring much useful\ninformation.\nAdditionally, we visualize the learned intra-\ncell architecture in Figure 7(a). The networks\nare jointly learned with the inter-cell architecture.\nCompared with the results of intra-cell NAS (Fig-\nure 7(b)), the learned network is more shallow.\nThe inter-cell architectures have deeper networks.\nThis in turn reduces the need for intra-cell capacity.\nThus a very deep intra-cell architecture might not\nbe necessary if we learn the whole model jointly.\n5.2.3 Transferring to Other Tasks\nAfter architecture search, we test the transferability\nof the learned architecture. In order to apply the\nmodel to other tasks, we directly use the architec-\nture searched on WikiText-103 and train the param-\nModels F1\nCross-BiLSTM-CNN (Aguilar et al., 2018) 45.55\nFlair (Akbik et al., 2019) 50.20\nDARTS + Flair‚Ä† 50.34\nESS 48.85\nESS + Flair 52.18\nTable 5: F1 scores on WNUT-2017 NER task.\n‚Ä†Obtained by training the corresponding architecture\nusing our setup.\nModels F1\nNCRF++ (Yang and Zhang, 2018) 95.06\nBiLSTM-CRF + IntNet (Xin et al., 2018) 95.29\nFlair (Akbik et al., 2019) 96.72\nGCDT + BERTLARGE (Liu et al., 2019b) 97.30\nDARTS + Flair‚Ä† 96.59\nESS 95.51\nESS + Flair 97.22\nTable 6: F1 scores on CoNLL-2000 chunking task.\n‚Ä†Obtained by training the corresponding architecture\nusing our setup.\neters with the in-domain data. In our experiments,\nwe adapt the model to CoNLL-2003, WNUT-2017\nNER tasks and CoNLL-2000 chunking task.\nFor the two NER tasks, it achieves new state-\nof-the-art F1 scores (Table 4 and Table 5). ELMo,\nFlair and BERTLARGE refer to the pre-trained lan-\nguage models. We apply these word embeddings\nto the learned architecture during model training\nprocess. For the chunking task, the learned archi-\ntecture also shows greater performance than other\nNAS methods (Table 6). Moreover, we Ô¨Ånd that our\npre-learned neural networks yield bigger improve-\nments on the WNUT-2017 task. The difference of\nthe two NER tasks lies in that the WNUT-2017 task\nis a long-tail emerging entities recognition task. It\nfocuses on identifying unusual, previously-unseen\nentities in the context of emerging discussions. As\nwe discuss in the previous part of the section, the\nadditional inter-cell NAS is good at learning the\nrepresentations of rare words. Therefore, it makes\n6637\nsense to have a bigger improvement on WNUT-\n2017.\n6 Conclusions\nWe have proposed the Extended Search Space\n(ESS) method of NAS. It learns intra-cell and\ninter-cell architectures simultaneously. Moreover,\nwe present a general model of differentiable ar-\nchitecture search to handle the arbitrary search\nspace. Meanwhile, the high-level and low-level\nsub-networks can be learned in a joint fashion. Ex-\nperiments on two language modeling tasks show\nthat ESS yields improvements of 4.5 and 2.4 per-\nplexity scores over a strong RNN-based baseline.\nMore interestingly, it is observed that transferring\nthe pre-learned architectures to other tasks also ob-\ntains a promising performance improvement.\nAcknowledgments\nThis work was supported in part by the National\nScience Foundation of China (Nos. 61876035\nand 61732005), the National Key R&D Program\nof China (No. 2019QY1801) and the Opening\nProject of Beijing Key Laboratory of Internet Cul-\nture and Digital Dissemination Research. The au-\nthors would like to thank anonymous reviewers for\ntheir comments.\nReferences\nGustavo Aguilar, Adrian Pastor L ¬¥opez-Monroy, Fabio\nGonz¬¥alez, and Thamar Solorio. 2018. Modeling\nnoisiness to recognize named entities using multi-\ntask neural networks on social media. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers) , pages 1401‚Äì1412, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nAlan Akbik, Tanja Bergmann, and Roland V ollgraf.\n2019. Pooled contextualized embeddings for named\nentity recognition. In NAACL 2019, 2019 An-\nnual Conference of the North American Chapter of\nthe Association for Computational Linguistics, page\n724‚Äì728.\nPeter J. Angeline, Gregory M. Saunders, and Jordan B.\nPollack. 1994. An evolutionary algorithm that con-\nstructs recurrent neural networks. IEEE Trans. Neu-\nral Networks, 5(1):54‚Äì65.\nAlexei Baevski, Sergey Edunov, Yinhan Liu, Luke\nZettlemoyer, and Michael Auli. 2019. Cloze-driven\npretraining of self-attention networks. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 5359‚Äì5368, Hong\nKong, China. Association for Computational Lin-\nguistics.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nBowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh\nRaskar. 2017. Designing neural network architec-\ntures using reinforcement learning. In 5th Inter-\nnational Conference on Learning Representations,\nICLR 2017, Toulon, France, April 24-26, 2017, Con-\nference Track Proceedings.\nJames Bergstra, Daniel Yamins, and David D. Cox.\n2013. Making a science of model search: Hyperpa-\nrameter optimization in hundreds of dimensions for\nvision architectures. In Proceedings of the 30th In-\nternational Conference on Machine Learning, ICML\n2013, Atlanta, GA, USA, 16-21 June 2013 , pages\n115‚Äì123.\nJames Bradbury, Stephen Merity, Caiming Xiong, and\nRichard Socher. 2017. Quasi-recurrent neural net-\nworks. In 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April\n24-26, 2017, Conference Track Proceedings.\nAndrew Brock, Theodore Lim, James M. Ritchie, and\nNick Weston. 2018. SMASH: one-shot model archi-\ntecture search through hypernetworks. In 6th Inter-\nnational Conference on Learning Representations,\nICLR 2018, Vancouver, BC, Canada, April 30 - May\n3, 2018, Conference Track Proceedings.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na Ô¨Åxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978‚Äì2988, Florence, Italy.\nAssociation for Computational Linguistics.\nYann N. Dauphin, Angela Fan, Michael Auli, and\nDavid Grangier. 2017. Language modeling with\ngated convolutional networks. In Proceedings of the\n34th International Conference on Machine Learning,\nICML 2017, Sydney, NSW, Australia, 6-11 August\n2017, pages 933‚Äì941.\nThomas Elsken, Jan Hendrik Metzen, and Frank Hut-\nter. 2019. EfÔ¨Åcient multi-objective neural architec-\nture search via lamarckian evolution. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nJonas Gehring, Michael Auli, David Grangier, De-\nnis Yarats, and Yann N. Dauphin. 2017. Convolu-\ntional sequence to sequence learning. In Proceed-\n6638\nings of the 34th International Conference on Ma-\nchine Learning, ICML 2017, Sydney, NSW, Australia,\n6-11 August 2017, pages 1243‚Äì1252.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In 2016 IEEE Conference on Computer Vi-\nsion and Pattern Recognition, CVPR 2016, Las Ve-\ngas, NV , USA, June 27-30, 2016, pages 770‚Äì778.\nGao Huang, Zhuang Liu, Laurens van der Maaten, and\nKilian Q. Weinberger. 2017. Densely connected con-\nvolutional networks. In 2017 IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR\n2017, Honolulu, HI, USA, July 21-26, 2017 , pages\n2261‚Äì2269.\nFrank Hutter, Lars Kotthoff, and Joaquin Vanschoren,\neditors. 2018. Automated Machine Learning: Meth-\nods, Systems, Challenges. Springer. In press, avail-\nable at http://automl.org/book.\nYufan Jiang, Chi Hu, Tong Xiao, Chunliang Zhang,\nand Jingbo Zhu. 2019. Improved differentiable ar-\nchitecture search for language modeling and named\nentity recognition. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 3583‚Äì3588, Hong Kong, China. As-\nsociation for Computational Linguistics.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn NAACL HLT 2016, The 2016 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, San Diego California, USA, June 12-17, 2016 ,\npages 260‚Äì270.\nLiam Li and Ameet Talwalkar. 2019. Random search\nand reproducibility for neural architecture search. In\nProceedings of the Thirty-Fifth Conference on Un-\ncertainty in ArtiÔ¨Åcial Intelligence, UAI 2019, Tel\nAviv, Israel, July 22-25, 2019, page 129.\nHanxiao Liu, Karen Simonyan, and Yiming Yang.\n2019a. DARTS: differentiable architecture search.\nIn 7th International Conference on Learning Repre-\nsentations, ICLR 2019, New Orleans, LA, USA, May\n6-9, 2019.\nYijin Liu, Fandong Meng, Jinchao Zhang, Jinan Xu,\nYufeng Chen, and Jie Zhou. 2019b. GCDT: A global\ncontext enhanced deep transition architecture for se-\nquence labeling. In Proceedings of the 57th Confer-\nence of the Association for Computational Linguis-\ntics, ACL 2019, Florence, Italy, July 28- August 2,\n2019, Volume 1: Long Papers, pages 2431‚Äì2441.\nG¬¥abor Melis, Tom¬¥aÀás KoÀácisk¬¥y, and Phil Blunsom. 2019.\nMogriÔ¨Åer lstm.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018a. An analysis of neural language mod-\neling at multiple scales. CoRR, abs/1803.08240.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018b. An analysis of neural language mod-\neling at multiple scales. CoRR, abs/1803.08240.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018c. Regularizing and optimizing LSTM\nlanguage models. In 6th International Conference\non Learning Representations, ICLR 2018, Vancou-\nver, BC, Canada, April 30 - May 3, 2018, Confer-\nence Track Proceedings.\nStephen Merity, Bryan McCann, and Richard Socher.\n2017. Revisiting activation regularization for lan-\nguage rnns.\nTomas Mikolov, Martin KaraÔ¨Å ¬¥at, Luk ¬¥as Burget, Jan\nCernock¬¥y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In IN-\nTERSPEECH 2010, 11th Annual Conference of the\nInternational Speech Communication Association,\nMakuhari, Chiba, Japan, September 26-30, 2010 ,\npages 1045‚Äì1048.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2018, New Or-\nleans, Louisiana, USA, June 1-6, 2018, Volume 1\n(Long Papers), pages 2227‚Äì2237.\nHieu Pham, Melody Y . Guan, Barret Zoph, Quoc V . Le,\nand Jeff Dean. 2018. EfÔ¨Åcient neural architecture\nsearch via parameter sharing. In Proceedings of the\n35th International Conference on Machine Learning,\nICML 2018, Stockholmsm¬®assan, Stockholm, Sweden,\nJuly 10-15, 2018, pages 4092‚Äì4101.\nJack W. Rae, Chris Dyer, Peter Dayan, and Timothy P.\nLillicrap. 2018. Fast parametric learning with acti-\nvation memorization. In Proceedings of the 35th In-\nternational Conference on Machine Learning, ICML\n2018, Stockholmsm ¬®assan, Stockholm, Sweden, July\n10-15, 2018, pages 4225‚Äì4234.\nEsteban Real, Alok Aggarwal, Yanping Huang, and\nQuoc V . Le. 2019. Regularized evolution for im-\nage classiÔ¨Åer architecture search. volume 33, page\n4780‚Äì4789. Association for the Advancement of Ar-\ntiÔ¨Åcial Intelligence (AAAI).\nYanyao Shen, Xu Tan, Di He, Tao Qin, and Tie-Yan\nLiu. 2018. Dense information Ô¨Çow for neural ma-\nchine translation. In Proceedings of the 2018 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2018, New Or-\nleans, Louisiana, USA, June 1-6, 2018, Volume 1\n(Long Papers), pages 1294‚Äì1303.\nDavid R. So, Quoc V . Le, and Chen Liang. 2019. The\nevolved transformer. In Proceedings of the 36th In-\nternational Conference on Machine Learning, ICML\n6639\n2019, 9-15 June 2019, Long Beach, California, USA,\npages 5877‚Äì5886.\nKenneth O. Stanley and Risto Miikkulainen. 2002.\nEvolving neural networks through augmenting\ntopologies. Evol. Comput., 10(2):99‚Äì127.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 6000‚Äì6010.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu,\nChangliang Li, Derek F. Wong, and Lidia S. Chao.\n2019. Learning deep transformer models for ma-\nchine translation. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1810‚Äì1822, Florence, Italy. Associa-\ntion for Computational Linguistics.\nQiang Wang, Fuxue Li, Tong Xiao, Yanyang Li, Yin-\nqiao Li, and Jingbo Zhu. 2018. Multi-layer repre-\nsentation fusion for neural machine translation. In\nProceedings of the 27th International Conference\non Computational Linguistics, COLING 2018, Santa\nFe, New Mexico, USA, August 20-26, 2018 , pages\n3015‚Äì3026.\nPaul J Werbos. 1990. Backpropagation through time:\nwhat it does and how to do it. Proceedings of the\nIEEE, 78(10):1550‚Äì1560.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google‚Äôs neural machine\ntranslation system: Bridging the gap between hu-\nman and machine translation. arXiv preprint\narXiv:1609.08144.\nYingwei Xin, Ethan Hart, Vibhuti Mahajan, and Jean-\nDavid Ruvini. 2018. Learning better internal struc-\nture of words for sequence labeling. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2584‚Äì2593,\nBrussels, Belgium. Association for Computational\nLinguistics.\nJie Yang and Yue Zhang. 2018. NCRF++: An open-\nsource neural sequence labeling toolkit. In Proceed-\nings of ACL 2018, System Demonstrations , pages\n74‚Äì79, Melbourne, Australia. Association for Com-\nputational Linguistics.\nZhao Zhong, Junjie Yan, Wei Wu, Jing Shao, and\nCheng-Lin Liu. 2018. Practical block-wise neural\nnetwork architecture generation. In 2018 IEEE Con-\nference on Computer Vision and Pattern Recogni-\ntion, CVPR 2018, Salt Lake City, UT, USA, June 18-\n22, 2018, pages 2423‚Äì2432.\nBarret Zoph and Quoc V . Le. 2017. Neural architec-\nture search with reinforcement learning. In 5th Inter-\nnational Conference on Learning Representations,\nICLR 2017, Toulon, France, April 24-26, 2017, Con-\nference Track Proceedings.\nBarret Zoph, Vijay Vasudevan, Jonathon Shlens, and\nQuoc V . Le. 2018. Learning transferable architec-\ntures for scalable image recognition. In 2018 IEEE\nConference on Computer Vision and Pattern Recog-\nnition, CVPR 2018, Salt Lake City, UT, USA, June\n18-22, 2018, pages 8697‚Äì8710.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8741488456726074
    },
    {
      "name": "Chunking (psychology)",
      "score": 0.7495907545089722
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5675305128097534
    },
    {
      "name": "Language model",
      "score": 0.5664689540863037
    },
    {
      "name": "Transferability",
      "score": 0.5299010872840881
    },
    {
      "name": "Architecture",
      "score": 0.5100184082984924
    },
    {
      "name": "Machine learning",
      "score": 0.43024110794067383
    },
    {
      "name": "Natural language processing",
      "score": 0.32013970613479614
    },
    {
      "name": "Logit",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I9224756",
      "name": "Northeastern University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210131870",
      "name": "Institute of Psychology, Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210108461",
      "name": "Kingsoft (China)",
      "country": "CN"
    }
  ]
}