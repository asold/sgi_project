{
    "title": "A Comparative Study of Using Pre-trained Language Models for Toxic Comment Classification",
    "url": "https://openalex.org/W3166550823",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2312694818",
            "name": "Zhi-xue Zhao",
            "affiliations": [
                "University of Sheffield"
            ]
        },
        {
            "id": "https://openalex.org/A2140318017",
            "name": "Ziqi Zhang",
            "affiliations": [
                "University of Sheffield"
            ]
        },
        {
            "id": "https://openalex.org/A2169642663",
            "name": "Frank Hopfgartner",
            "affiliations": [
                "University of Sheffield"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2613977835",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W2015717769",
        "https://openalex.org/W1619992285",
        "https://openalex.org/W2963625095",
        "https://openalex.org/W2927746189",
        "https://openalex.org/W2595653137",
        "https://openalex.org/W2740385731",
        "https://openalex.org/W2887782043",
        "https://openalex.org/W2785615365",
        "https://openalex.org/W2979860911",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W1832693441",
        "https://openalex.org/W80056832",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2242082950",
        "https://openalex.org/W2340954483",
        "https://openalex.org/W2963481894",
        "https://openalex.org/W2740168486",
        "https://openalex.org/W2954992865",
        "https://openalex.org/W2563826943",
        "https://openalex.org/W2963943967",
        "https://openalex.org/W2473555522",
        "https://openalex.org/W2963790884",
        "https://openalex.org/W3028677133",
        "https://openalex.org/W3103061166",
        "https://openalex.org/W2551706664",
        "https://openalex.org/W2913698966",
        "https://openalex.org/W4391156274",
        "https://openalex.org/W2979977993",
        "https://openalex.org/W2079735306"
    ],
    "abstract": "As user-generated contents thrive, so does the spread of toxic comment. Therefore, detecting toxic comment becomes an active research area, and it is often handled as a text classification task. As recent popular methods for text classification tasks, pre-trained language model-based methods are at the forefront of natural language processing, achieving state-of-the-art performance on various NLP tasks. However, there is a paucity in studies using such methods on toxic comment classification. In this work, we study how to best make use of pre-trained language model-based methods for toxic comment classification and the performances of different pre-trained language models on these tasks. Our results show that, Out of the three most popular language models, i.e. BERT, RoBERTa, and XLM, BERT and RoBERTa generally outperform XLM on toxic comment classification. We also prove that using a basic linear downstream structure outperforms complex ones such as CNN and BiLSTM. What is more, we find that further fine-tuning a pre-trained language model with light hyper-parameter settings brings improvements to the downstream toxic comment classification task, especially when the task has a relatively small dataset.",
    "full_text": null
}