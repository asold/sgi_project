{
  "title": "MAPLE: Mobile App Prediction Leveraging Large Language Model Embeddings",
  "url": "https://openalex.org/W4386874883",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4303033400",
      "name": "Khaokaew, Yonchanok",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2111440777",
      "name": "Xue Hao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225478810",
      "name": "Salim, Flora D.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4225494949",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W3030030185",
    "https://openalex.org/W3011222825",
    "https://openalex.org/W3216523475",
    "https://openalex.org/W2088394455",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2005567524",
    "https://openalex.org/W2934924962",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2984100107",
    "https://openalex.org/W3083166789",
    "https://openalex.org/W3196693533",
    "https://openalex.org/W4382203079",
    "https://openalex.org/W4309651822",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W4361760350",
    "https://openalex.org/W1485815177",
    "https://openalex.org/W2929944287",
    "https://openalex.org/W2952403926",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4303438998",
    "https://openalex.org/W2073601450",
    "https://openalex.org/W2996833046",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W4213235855",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W3159096087",
    "https://openalex.org/W3193326076",
    "https://openalex.org/W2120761625",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2032654855",
    "https://openalex.org/W4211263275",
    "https://openalex.org/W2102346651",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W2896682723",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W4306173949",
    "https://openalex.org/W2015218663",
    "https://openalex.org/W4292779060"
  ],
  "abstract": "In recent years, predicting mobile app usage has become increasingly important for areas like app recommendation, user behaviour analysis, and mobile resource management. Existing models, however, struggle with the heterogeneous nature of contextual data and the user cold start problem. This study introduces a novel prediction model, Mobile App Prediction Leveraging Large Language Model Embeddings (MAPLE), which employs Large Language Models (LLMs) and installed app similarity to overcome these challenges. MAPLE utilises the power of LLMs to process contextual data and discern intricate relationships within it effectively. Additionally, we explore the use of installed app similarity to address the cold start problem, facilitating the modelling of user preferences and habits, even for new users with limited historical data. In essence, our research presents MAPLE as a novel, potent, and practical approach to app usage prediction, making significant strides in resolving issues faced by existing models. MAPLE stands out as a comprehensive and effective solution, setting a new benchmark for more precise and personalised app usage predictions. In tests on two real-world datasets, MAPLE surpasses contemporary models in both standard and cold start scenarios. These outcomes validate MAPLE's capacity for precise app usage predictions and its resilience against the cold start problem. This enhanced performance stems from the model's proficiency in capturing complex temporal patterns and leveraging contextual information. As a result, MAPLE can potentially improve personalised mobile app usage predictions and user experiences markedly.",
  "full_text": "10\nMAPLE: Mobile App Prediction Leveraging Large Language Model\nEmbeddings\nYONCHANOK KHAOKAEW,School of Computer Science and Engineering, University of New South Wales,\nAustralia\nHAO XUE, School of Computer Science and Engineering, University of New South Wales, Australia\nFLORA D. SALIM, School of Computer Science and Engineering, University of New South Wales, Australia\nIn recent years, predicting mobile app usage has become increasingly important for areas like app recommendation, user\nbehaviour analysis, and mobile resource management. Existing models, however, struggle with the heterogeneous nature\nof contextual data and the user cold start problem. This study introduces a novel prediction model, Mobile App Prediction\nLeveraging Large Language Model Embeddings (MAPLE), which employs Large Language Models (LLMs) and installed\napp similarity to overcome these challenges. MAPLE utilises the power of LLMs to process contextual data and discern\nintricate relationships within it effectively. Additionally, we explore the use of installed app similarity to address the cold\nstart problem, facilitating the modelling of user preferences and habits, even for new users with limited historical data. In\nessence, our research presents MAPLE as a novel, potent, and practical approach to app usage prediction, making significant\nstrides in resolving issues faced by existing models. MAPLE stands out as a comprehensive and effective solution, setting\na new benchmark for more precise and personalised app usage predictions. In tests on two real-world datasets, MAPLE\nsurpasses contemporary models in both standard and cold start scenarios. These outcomes validate MAPLEâ€™s capacity for\nprecise app usage predictions and its resilience against the cold start problem. This enhanced performance stems from the\nmodelâ€™s proficiency in capturing complex temporal patterns and leveraging contextual information. As a result, MAPLE can\npotentially improve personalised mobile app usage predictions and user experiences markedly.\nCCS Concepts: â€¢ Information systemsâ†’Spatial-temporal systems; Data mining ; â€¢ Human-centered computing\nâ†’Empirical studies in ubiquitous and mobile computing; â€¢ Computing methodologiesâ†’Model development and\nanalysis.\nAdditional Key Words and Phrases: Mobile user behaviour modelling, App usage prediction, Large language model\nACM Reference Format:\nYonchanok Khaokaew, Hao Xue, and Flora D. Salim. 2024. MAPLE: Mobile App Prediction Leveraging Large Language\nModel Embeddings. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 8, 1, Article 10 (March 2024), 25 pages. https:\n//doi.org/10.1145/3643514\n1 INTRODUCTION\nAs reliance on smartphones increases, predicting app usage has become crucial. Smartphones are utilised for\nvarious tasks, including communication, entertainment, and work. Accurate predictions of app usage can enhance\nuser experience, conserve battery life, and improve network efficiency. Furthermore, app usage prediction can\nAuthorsâ€™ addresses: Yonchanok Khaokaew, y.khaokaew@unsw.edu.au, School of Computer Science and Engineering, University of New\nSouth Wales , Sydney, NSW, Australia, 2052; Hao Xue, hao.xue1@unsw.edu.au, School of Computer Science and Engineering, University of\nNew South Wales , Sydney, NSW, Australia, 2052; Flora D. Salim, flora.salim@unsw.edu.au, School of Computer Science and Engineering,\nUniversity of New South Wales , Sydney, NSW, Australia, 2052.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that\ncopies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.\nCopyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy\notherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from\npermissions@acm.org.\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n2474-9567/2024/3-ART10 $15.00\nhttps://doi.org/10.1145/3643514\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 10. Publication date: March 2024.\narXiv:2309.08648v4  [cs.CL]  1 Nov 2024\n10:2 â€¢ Khaokaew Y., et al.\naid app developers and marketers in better understanding user behaviour, allowing them to tailor their offerings\nmore effectively to their target audience.\nPrevious research has underscored the importance of contextual information in app usage predictions, en-\ncompassing aspects like time, location, and user history. Nevertheless, integrating this contextual data presents\nchallenges. Embedding such information requires extensive training, which can be time-consuming and resource-\nintensive. Additionally, incorporating this data into predictive models can be complicated, especially when dealing\nwith the cold start problem.\nRecent advancements in natural language processing have led to the development of Large Language Models\n(LLMs) based on the Transformer architecture. These models, such as GPT-3 [ 4], demonstrate remarkable\nproficiency in various natural language tasks, including language generation, translation, and sentiment analysis.\nTheir flexibility and adaptability make LLMs well-suited for diverse domains, including app usage prediction.\nThe Transformer architecture [32], foundational to many state-of-the-art LLMs, was initially designed for tasks\ninvolving sequences in natural language processing, making it particularly apt for app usage prediction.\nLLMs are especially effective in processing text data, positioning them as prime candidates for predicting app\nusage. Contextual information, such as location, can be textually represented (e.g., \"at a coffee shop\") or behaviours\n(e.g., \"frequently uses social media apps in the evening\"). Utilizing LLMs and the Transformer architecture enhances\nthe precision of app usage predictions. These models leverage features like self-attention and multi-head attention,\neffectively capturing app usage sequences in a manner similar to natural language processing. Beyond improving\nembedding training, LLMs address the cold start problem associated with new contexts or users. Traditional\nmodels frequently encounter difficulties when dealing with unfamiliar contexts. However, Large Language Models\n(LLMs), utilizing their vast pre-trained language abilities, are adept at generating pertinent text descriptions,\nthereby enhancing prediction accuracy. To address the issue of predicting for new users who lack historical\ndata, our approach utilises patterns observed in users with similar app usage. This strategy proficiently handles\ncold start scenarios, effectively circumventing the limitations typically associated with conventional embedding\ntraining methods.\nLeveraging insights from our experiments, we propose a novel app usage prediction model that capitalizes on\nthe strengths of LLMs and contextual data to tackle the user cold start issue for new users. This model employs\nLLMs to translate contextual information, such as time and location, into text format. It also integrates behaviour\ndata from users with a history of app usage, enhancing prediction accuracy. A key advantage of using LLMs is\nthe elimination of the need for initial training of the embedding layer. We utilise the pre-trained embeddings\nfrom LLMs, significantly reducing computational demands and training time.\nOur model, named Mobile App Prediction Leveraging Large Language Model Embeddings (MAPLE), primarily\nharnesses LLMs to process contextual data, yielding a more refined understanding of user behaviour. Additionally,\nit incorporates behaviour patterns from users with similar app usage to address the cold start problem in new\nusers, enhancing both accuracy and generalizability. By identifying and analyzing users with analogous app\nusage patterns, MAPLE can discern underlying behaviour trends, leading to more personalised and precise app\nusage predictions. In essence, our modelâ€™s contributions are threefold:\nâ€¢The proposed model leverages LLMs to process contextual information, which provides a more accurate\nand nuanced understanding of user behaviour.\nâ€¢The model addresses the user cold start problem for new users by incorporating the behaviour data of\nsimilar users with a history of app usage, improving the modelâ€™s accuracy and generalizability.\nâ€¢By utilizing pre-trained embeddings of LLMs, the proposed model significantly reduces computational\nrequirements and training time, making it more efficient and scalable.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 10. Publication date: March 2024.\nMAPLE: Mobile App Prediction Leveraging Large Language Model Embeddings â€¢ 10:3\nIn the following sections, we will delve deeper into the dataset and problem statement, present our language\nmodel-based methodology, evaluate the performance of our model using real-world datasets, and discuss the\nimplications of our findings for the future of app development and user experience optimization.\n2 MOTIVATION AND PROBLEM DEFINITION\n2.1 Motivation\nThe motivation for this research is rooted in addressing the significant challenges associated with contextual data\nheterogeneity and the cold start problem in app usage prediction. The diverse and complex nature of contextual\ndata hinders the development of accurate and robust models for predicting user behaviour. Furthermore, the\ncold start problem exacerbates these challenges, as providing accurate predictions for new users with limited\nhistorical data remains daunting.\nRecognizing the capabilities of Large Language Models (LLMs) and their foundational Transformer architecture,\nour research seeks to utilise their strengths in managing the complexity of contextual data. LLMs have shown\nexceptional performance in diverse natural language processing tasks, and recent studies indicate their effective-\nness in time series forecasting as well [37, 38]. Given their proficiency in handling and generating sequential data,\ncrucial for understanding app usage behaviour, we are motivated to investigate their use in app usage prediction.\nThis domain aligns with time series forecasting in terms of identifying sequential patterns. By deploying LLMs,\nwe aim to leverage their advanced processing power to unravel complex relationships within contextual data,\nthereby enhancing the accuracy of app usage predictions.\nFurthermore, we aim to explore the possibility of using the similarity of installed apps as a marker of comparable\napp usage behaviours. This approach could effectively address the cold start problem and facilitate the modelling\nof user preferences and habits, even for new users with limited historical data. By integrating the advantages of\nLarge Language Models (LLMs) with insights gained from installed app similarity, we seek to provide a thorough\nand effective method for predicting app usage. Our research stands to make a substantial contribution to the\nfield, presenting a novel, robust, and practical approach that tackles key challenges faced by current models in\napp usage prediction.\n2.1.1 Contextual Information Used in Modelling Mobile User Behaviour. Contextual information is pivotal in\nmodelling mobile user behaviour, and a range of contextual factors have been employed in previous research to\npredict user behaviour effectively. The types of contextual information frequently utilised in modelling mobile\nuser behaviour include the following.\nâ€¢Location: User location significantly influences behaviour predictions. Studies show that a userâ€™s current\nlocation, their location during app usage, and frequent visit spots offer behaviour insights. [6, 28, 35, 39].\nâ€¢Previous Applications: Prior studies have shown that previously used applications can be indicative of the\nuserâ€™s future application use [6, 13, 14, 28, 35, 41]. The extraction of previous application usage as a feature\nin prior works has demonstrated its usefulness in predicting mobile user behaviour.\nâ€¢App Categories: The userâ€™s applications category can provide insight into their interests and preferences.\nBy analyzing the types of applications a user frequently uses, such as social media, productivity, or\nentertainment, we can better understand their interests and predict their future behaviour [6, 39].\nâ€¢Temporal Information: The day of the week, hour of the day, and day of the month that a user used an\napplication can provide insight into their preferences and usage patterns. Prior works have shown that\nthe time of day when a user most frequently uses a specific application and their typical usage pattern\nthroughout the day can provide further information [2, 14, 28, 35, 41].\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 10. Publication date: March 2024.\n10:4 â€¢ Khaokaew Y., et al.\nâ€¢Anonymized ID: A unique identifier helps track individual usage patterns. Previous research suggests age\nand gender also illuminate user behaviour [28, 35, 41]. However, due to privacy and cold start problems, our\nmodel excludes anonymous IDs, as their absence in cold start scenarios would limit the modelâ€™s relevance.\nIn traditional methods of modelling mobile user behaviour, embedded layers represent contextual information\n1. Yet, these approaches face challenges with diverse and evolving contextual data. This is due to the static nature\nof embedded layers, which struggle to adapt to new or changing contexts. Moreover, for each new user or distinct\ncontext type, these layers require retraining, a process that can be both time-consuming and demanding in terms\nof resources.\nEmbedding\nEmbedding\nEmbeddingEmbedding\nUser ID POIApp typeApp usage\n0 1 0 -1 0.2 0.5 0.1 0.9 0.5 1 0.6 0 0.5 1 \nOutput\nUser ID POIApp typeApp usage\nContextual Prompts\nPretrained\nLLM\nOutput\nTarget Prompt\nThe Traditional Approaches The Proposed Approaches through LLM\nFig. 1. Conceptual illustrations of traditional approaches for using contextual information in app usage prediction problems\ncompared to the proposed model based on LLMs\nApplying a Large Language Model (LLM) presents a solution that addresses these limitations, offering a more\nexpansive and adaptable approach. Being a pre-trained model, the LLM can be fine-tuned for specific applications,\nlike predicting mobile user behaviour. It effectively processes various contextual information and seamlessly\nadjusts to new and evolving contexts. In contrast, traditional methods of modelling mobile user behaviour struggle\nto manage such a broad spectrum of contextual data and to make predictions based on it. Employing an LLM\nprovides a way to surmount these challenges, leading to a more comprehensive and efficacious method for\nmodelling mobile user behaviour.\nThe table 1 encapsulates the contextual information typically used in modelling mobile user behaviour,\nalongside potential prompts for extracting pertinent data from each context type. These prompts, derived from\nthe accessible contextual data, offer a targeted means for the LLM to discern relevant information and forecast\nmobile user behaviour. By formulating these prompts based on the available contextual information, the LLM is\nequipped to parse semantic details, thereby enhancing its accuracy in predicting mobile user behaviour.\nIn our proposed approach, LLMs will be utilised in conjunction with contextual information and prompts derived\nfrom this data. These prompts enable the LLM to discern relevant details from the given contextual information,\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 10. Publication date: March 2024.\nMAPLE: Mobile App Prediction Leveraging Large Language Model Embeddings â€¢ 10:5\nTable 1. The possible prompts that can be applied to each type of context information\nContextual Information Possible Prompts\nLocation \"The user is currently at [LOCATION]. \"\n\"The user visited [LOCATION] when using a specific app. \"\n\"The user frequently visits [LOCATION]. \"\n\"The user is close to [POINT OF INTEREST]. \"\nPrevious Applications \"The user has recently used [APP]. \"\n\"The user frequently uses [APP]. \"\n\"The app [APP] is used recently. \"\nApp Categories \"The user frequently uses apps in the [CATEGORY] category. \"\n\"The app [CATEGORY] is used prior to the prediction. \"\nTemporal Information \"It is currently [DAY OF WEEK]. \"\n\"It is [HOUR OF DAY]. \"\n\"It is [DAY OF MONTH]. \"\n\"The user frequently uses [APP] at [TIME OF DAY]. \"\n\"It is [TIME OF DAY ] on [WEEK DAY]. \"\nAnonymised ID \"The user [Anonymised ID]â€™s age is [AGE]. \"\n\"The user [Anonymised ID]â€™s gender is [GENDER]. \"\nleading to a more thorough and effective method for predicting mobile user behaviour. The synergistic use of\nLLMs with prompts based on contextual information represents an innovative strategy for modelling mobile\nuser behaviour. This technique has the potential to yield more precise predictions and deeper insights into user\nbehaviour. Our research is dedicated to examining this approach and its capacity to offer a more comprehensive\nand efficacious solution, addressing the limitations of traditional methods in modelling mobile user behaviour.\n2.1.2 Overcoming the User Cold Start Problem with Installed Application Analysis. The user cold start problem\nrefers to the difficulty in predicting a new userâ€™s behaviour due to the lack of available data. In the context of\nmobile user behaviour, this can make it challenging to accurately predict the applications that a new user is likely\nto use, especially when the userâ€™s identifier, which is commonly used as contextual information in modelling\nmobile user behaviour [28, 35, 41], is not available for the new user.\nTo address this issue, we aim to investigate the relationship between the set of installed applications and mobile\nuser behaviour, specifically app usage, in this section. By analyzing the installed applications, we hope to gain\ninsight into how they can indicate a userâ€™s interests and habits and how this information can be used to predict\ntheir future app usage. This analysis is crucial in overcoming the user cold start problem, as it provides a way\nto make initial predictions about a new userâ€™s behaviour, even when the userâ€™s identifier is unavailable. These\npredictions can then be refined and improved over time as more data about the user becomes available.\nFirst, weâ€™ll employ Jaccard Similarity to identify users with analogous app sets, aiming to understand how app\ncollections reflect interests and predict future usage. Using Principal Component Analysis (PCA) and t-Distributed\nNeighbor Embedding (t-SNE) [19, 31], weâ€™ll condense the one-hot encoded vectors of selected usersâ€™ app categories\nfor easier two-dimensional visualization of app usage trends.\nFigure 2 depicts our findings. The heatmap (Fig 2a) shows Jaccard Similarity scores among user pairs, ranging\nfrom 0 (dissimilar) to 1 (identical). Three user pairsâ€”U1 & U2, U3 & U4, and U5 & U6â€”demonstrate high similarity\nover 0.6, suggesting comparable app interests and behaviours. Notably, U3 & U4 share moderate similarity (around\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 10. Publication date: March 2024.\n10:6 â€¢ Khaokaew Y., et al.\n0.4) with U1 & U2, hinting at parallel behaviours. This analysis precedes a detailed visualization of these usersâ€™\napp usage patterns.\n(a) Jaccard Similarity Heatmap of Installed Apps Among\nTop Three Similar Users in Tsinghua App Usage Dataset\n(b) t-SNE Reduced 2D Visualization of App Usage Be-\nhaviour Among Users with Similar Installed App Sets\nFig. 2. Exploring App Usage Behaviour Similarity Among Users with Similar Installed App Sets\nThe scatter chart 2b analyzes usersâ€™ app usage with similar installed apps, identified through Jaccard Similarity.\nDisplayed in a two-dimensional space with axes representing the principal components from t-SNE, each point\nsignifies a userâ€™s app usage, colour-coded for distinction. Notably, clusters, especially the red circles for users U5,\nU6, U3, and U4, signify shared app behaviour due to similar app sets.\nFurthermore, a blue circle in the chart highlights four users, U1, U2, U3, and U4, who have similar apps and\nbehaviours, emphasizing the connection between the installed apps and user inclinations. This observation is\npivotal for forecasting future app usage. The chart expands upon the heatmapâ€™s analysis by illustrating the\nrelationships between apps and user similarities. These insights will guide the development of our predictive\nmodel for future app usage, concentrating on installed apps to address the user cold start problem. Next, we will\narticulate the problem formulation for our prediction model, integrating installed apps with contextual data as a\nfoundation for its development.\n2.2 Problem Definition\nIn mobile user behaviour modelling, predicting future app usage is essential. A userâ€™s app usage is influenced by\nvarious contextual factors such as app usage history, time, and location. To predict a userâ€™s next app, itâ€™s vital to\nintegrate these factors into the model.\nWe examine contextual information typesğ¶ = ğ´,ğ½,ğ‘‡,ğ¿ , where ğ´denotes app usage history, ğ½ app type history,\nğ‘‡ time, and ğ¿location. These types are converted into sentences ğ‘†ğ‘ = ğ‘†ğ´,ğ‘†ğ½,ğ‘†ğ‘‡,ğ‘†ğ¿ to represent a userâ€™s context\nfully. The goal is to predict the next app Pa user will use, through a machine learning model ğ‘§, formulated as:\nğ‘§(ğ‘†ğ‘)â†’P (1)\nHere, ğ‘§(Â·)is the function transforming contextual sentences ğ‘†ğ‘ into the next app prediction P. We employ a\nlarge language model (LLM) to address the shortcomings of traditional embedding models and capture the userâ€™s\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 10. Publication date: March 2024.\nMAPLE: Mobile App Prediction Leveraging Large Language Model Embeddings â€¢ 10:7\ncontext accurately. Weâ€™ll train and testğ‘§(Â·)on real-world data to evaluate its predictive accuracy with contextual\ninformation. Our ultimate aim is to create a model that leverages contextual data and installed apps to accurately\nforecast a userâ€™s app usage, offering insights into mobile user behaviour modelling and enhancing the fieldâ€™s\nprogress.\n3 DATASET\nThis section is dedicated to a thorough analysis of the datasets used in our study. We have employed two publicly\naccessible app usage datasets, chosen for their relevance to our research question. Our analysis includes both\nstatistical and characteristic examinations to gain a comprehensive understanding of the data. The statistical\nanalysis involves calculating essential descriptive statistics providing an overview of data distribution. In contrast,\nthe characteristic analysis offers a deeper exploration of the dataâ€™s features, trends, and patterns. This includes\nevaluating the overall characteristics and noting differences between the two datasets.\n3.1 Tsinghua App Usage Dataset\nThe first dataset, comprising app usage records, is sourced via Deep Packet Inspection (DPI) devices, a leading\nmobile network operator in China. This dataset records mobile usersâ€™ app usage, capturing both time and location\ndetails as users connect to the cellular network. The locations in the dataset reflect the accuracy of the cell towerâ€™s\nposition. Each record in the dataset contains an anonymized user ID, timestamps of HTTP requests or responses,\npacket lengths, visited domains, and the appâ€™s ID. The data was collected in Shanghai, a major metropolitan area\nin China. A summary of the datasetâ€™s statistics for Tsinghua App Usage is presented in Table 2.\nTable 2. Dataset Statistics for Tsinghua App Usage and LSApp\nMetric Tsinghua App Usage LSApp\n# users 870 292\n# unique app 2,000 87\n# sessions 102,422 76,247\n# locations 6,560 N/A\n# app usage records 2.4 M 600 K\nMean unique apps in each session 3.50 Â± 7.75 2.18 Â± 1.46\nMean used apps per location 24.96 N/A\n3.2 Large Dataset of Sequential Mobile App Usage (LSApp)\nLSApp [2], serving as a supplementary dataset to ISTAS [1, 14], similarly compiles sequential app usage events\nfrom users. In this study, 292 participants were enrolled and instructed to install the uSearch application on their\nsmartphones, operating it for a minimum of 24 hours. Participants were also asked to log their mobile searches\nin uSearch as they occurred promptly. Additionally, app usage statistics were gathered with the consent of the\nparticipants. The datasetâ€™s statistics for LSApp are detailed in Table. 2.\n3.3 Dataset Characteristics and Analysis\nAnalyzing the characteristics of the datasets used in a study is crucial in ensuring the validity and generalizability\nof the results. In this section, we analyse the characteristics of the Tsinghua App Usage and LSApp datasets\nmentioned in the previous section.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 10. Publication date: March 2024.\n10:8 â€¢ Khaokaew Y., et al.\nTable 3. Dataset and Available contextual information\nDataset Historical app usage Point of interest Temporal info. App type info.\nTsinghua App Usage dataset Ã— Ã— Ã— Ã—\nLSApp dataset Ã— Ã— Ã—\nTable 3 outlines the contextual data in each dataset. Both the Tsinghua App Usage and LSApp datasets include\nhistorical app usage, application types, temporal data, and user IDs. However, the Tsinghua App Usage dataset\noffers an extra layer of context with points of interest, potentially enhancing prediction model accuracy.\nOur proposed model employs a Large Language Model (LLM) adaptable to the data available, whether itâ€™s\nhistorical app usage, temporal information, or additional data like the points of interest in the Tsinghua App\nUsage dataset. This adaptability makes the model versatile across various datasets.\n(a) Temporal distribution of total mobile App usage number (1 hours resolution)\n(b) App type usage distribution of Tsinghua App Usage\nDataset\n (c) App type usage distribution of LSApp Dataset\nFig. 3. The usage of Apps at different time segments from different datasets.\nWe analyzed app usage over various time frames, as shown in Figure 3a. The x-axis represents time in 1-hour\nintervals from Sunday to Saturday, and the y-axis indicates the number of app usages in each interval. This\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 10. Publication date: March 2024.\nMAPLE: Mobile App Prediction Leveraging Large Language Model Embeddings â€¢ 10:9\ngraph compares app usage trends over a week, employing dual y-axes to contrast the Tsinghua App Usage and\nLSApp datasets. Both datasets exhibit similar patterns, with increased activity in the afternoons and evenings\nand reduced usage in early mornings and late nights. A noticeable difference is observed in the peak times: the\nTsinghua App Usage dataset shows an earlier peak than the LSApp dataset. Utilizing an LLM model can adeptly\naddress these temporal variations. Owing to its extensive pre-training, LLMs are skilled at identifying patterns\nacross diverse datasets. By processing context in sentence form, LLMs adeptly discern each datasetâ€™s unique app\nusage trends, thereby facilitating accurate predictions.\nWe also examined app type usage across various time segments (Night I, E-morning, Morning, Afternoon,\nEvening, Night II) from two datasets, as depicted in Fig. 3. The Tsinghua App Usage dataset predominantly uses\nsocial network apps, while the LSApp dataset primarily uses communication apps, with social networks as a\nclose second. Navigation apps rank in the top 6 in both, peaking in the mornings and afternoons. Coupled with\ninsights from Figure 3a, its explicit app usage varies by time. Thus, integrating app history and temporal details\ncan refine prediction accuracy. Recognizing this time-dependent app usage, itâ€™s vital to devise a model capturing\nthese nuances. Given its adeptness at handling intricate data dynamics, we believe a large language model can\naddress this. By fusing historical, temporal, and location data, our model aims to decipher context-driven app\nusage patterns better. The following sections will describe our proposed model and evaluate its performance\nusing real-world datasets.\n4 PROPOSED MODEL\n4.1 Model overview\nThis section introduces a proposed methodology to tackle the challenges outlined in Section 1. Our proposed\nmodel, named Mobile App Prediction Leveraging Large Language model Embeddings (MAPLE), depicted in\nFigure.4, is crafted to scrutinize mobile user behaviour and predict app usage accurately, utilizing the capabilities\nof a large language model. MAPLE leverages installed app data to discern new usersâ€™ interests, habits, and\npreferences, thus addressing the user cold start issue. The model is structured around two main components.\nTemplate based\nSentences generator\nApp usage data\nContextual data Two-stage App Prediction Model\nBased on LLMs\nContextual \nprompts\nPersonal prompts + Contextual prompts\nTop k Apps\nModel output\nFig. 4. Overall flowchart of the proposed model\nThe initial component, the Template-based Contextual Sentence Generation Module, processes app and\ncontextual information to generate sentences that encapsulate the userâ€™s present mobile usage context. These\ncontext-rich sentences are formulated using templates to integrate key information like time of day, location, and\napp type. Furthermore, we utilise the capabilities of Large Language Models in our distinctive Two-stage App\nPrediction Model. This approach unfolds sequentially: Initially, it focuses on predicting the broad category of the\napp. Subsequently, our model refines its prediction, narrowing down to the specific app a user will most likely\nuse next.\nIn essence, the proposed model harnesses the prowess of language models to adeptly handle contextual data\nheterogeneity and address the user cold start problem. Subsequent subsections will delve into more intricate\ndetails of each model component, showcasing our expert approach.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 10. Publication date: March 2024.\n10:10 â€¢ Khaokaew Y., et al.\nTable 4. The template for converting available contextual information to the written language used by the proposed prediction\nmodules.\nDescription Template Example\nInput\nHistorical app\ncategory usage\nThe apps { ğ‘1,ğ‘2,....,ğ‘ ğ‘¡ğ‘œğ‘ğ‘  âˆ’1 } are used\nprior to the prediction.\nThe apps Photo/Video, Communication, and\nUtilities are used prior to the prediction.\nPrediction Time On {ğ‘ğ‘¡ğ‘œğ‘ğ‘  } On Tuesday 02 PM\nPoint of interest The user is close to { ğ‘™ğ‘¡1 }, { ğ‘™ğ‘¡2 } and\n{ğ‘™ğ‘¡ğ‘œğ‘ğ‘  âˆ’1 }.\nThe user is close to service, shopping and restau-\nrants.\nHistorical app\nusage\nThe apps { ğ‘1,ğ‘2,....,ğ‘ ğ‘¡ğ‘œğ‘ğ‘  âˆ’1 } are used\nprior to the prediction.\nThe apps 1, 4, and 9 are used prior to the predic-\ntion.\nInstalled apps ğ‘1 : ğ‘1ğ‘1 ,ğ‘2ğ‘1\nğ‘2 : ğ‘1ğ‘2 ,ğ‘2ğ‘2 ..... .....\ntravel apps : 1,4,12\nutility apps : 2,7,16\nOutput 1ğ‘ ğ‘¡ stage result Based on the global information, the\nnext app will be a {ğ‘1ğ‘¡ğ‘œğ‘ğ‘  } app ({ğ‘1ğ‘¡ğ‘œğ‘ğ‘  }%),\n{ğ‘2ğ‘¡ğ‘œğ‘ğ‘  } app ({ ğ‘2ğ‘¡ğ‘œğ‘ğ‘  }%) or { ğ‘3ğ‘¡ğ‘œğ‘ğ‘  } app\n({ğ‘1ğ‘¡ğ‘œğ‘ğ‘  }%)\nBased on the global information, the next app\nwill be a communication app (70%), social app\n(20%) or travel app (10%)\n2ğ‘›ğ‘‘ stage result This user will use App ğ‘ğ‘¡ğ‘œğ‘ğ‘  . This user will use App 4.\n4.2 Template-based contextual sentence generation\nThe Template-based Contextual Sentence Generation Module stands as a pivotal element in our LLM-based\nproposed method. Its primary function is to transform available context features into informative and coherent\nsentences, which serve as inputs for the downstream prediction modules. This module is essential for crafting\ncontextual sentences that encapsulate the userâ€™s current mobile usage context, thus supplying vital contextual\ndata to the subsequent components of our model. Drawing inspiration from the work of Xue et al . [37], we\nhave devised a template-based strategy to create these contextual sentences. Our method encompasses two\ndistinct template types, each tailored for extracting specific information types: one for the App Type Prediction\nTraining Stage (ATP Training Stage) and another for the Next App Prediction Training Stage (NAP Training\nStage). Utilizing these templates, our model can produce coherent, context-relevant sentences that not only\naccurately foresee the next app type but also offer insights into the userâ€™s mobile usage patterns. Details of the\ntemplate for converting contextual information into textual format are illustrated in Table 4.\nTo construct contextual sentences for the ATP and NAP Training Stages, we utilise predefined templates\ntailored to our datasetâ€™s contextual data and adaptable for specific use cases. These stages incorporate historical\napp usage categories, the time of prediction, and, when pertinent, the userâ€™s point of interest. The historical\ncategory denotes previously engaged apps, while the prediction time signifies the moment of prediction. The\npoint of interest, linked to the userâ€™s location, is integrated as needed. These templates are instrumental in\ngenerating sentences that accurately reflect the userâ€™s current context, thereby facilitating precise app category\npredictions. In the NAP Training Stage, features such as historical app usage, installed apps (as discussed in\nSection 2.1.2), and context from the ATP stage are employed in sentence generation. These elements are crucial\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 10. Publication date: March 2024.\nMAPLE: Mobile App Prediction Leveraging Large Language Model Embeddings â€¢ 10:11\nfor understanding user behaviour and preferences in anticipating the next app the user will use. The forthcoming\nsections will expound upon the ATP and NAP Training Stages.\n4.3 Two-Stage LLM Training Module\nEx. Photo and Utility\napps are used before\nthe prediction.\nPretrained\nmodel\nTrained\nmodel\nContextual features (Multiple datasets)\nTemplate based\nSentences generator\nEx. The next app will be\na communication app\n(70%), social app (10%)\nor travel app (10%)\nApp usage\ncontext\n prompting\nTarget prompting\nEx. Apps 1 and 9 are used\nprior to the prediction..... \nPhoto app: 1,4,12...\nPhoto and Utility apps are\nused before the prediction.\nEx. This user will use\nApp 4\n1st Stage 2nd Stage \nFig. 5. Overall flowchart of the two-stage App Prediction Model\nOur MAPLE modelâ€™s core is an innovative two-stage training process utilizing Large Language Models (LLMs),\nwhich goes beyond traditional seq2seq applications. This bespoke method, comprising the App Type Prediction\nTraining Stage followed by the Next App Prediction Training Stage, is tailored to enhance the predictive precision\nof the next app a user will likely engage with.\n4.3.1 App Type Prediction Training Stage (ATP). The first stage, the ATP Training Stage, focuses on predicting\nthe type of the next app based on historical app categories and contextual features. Harnessing the capabilities\nof Large Language Models (LLMs), this stage incorporates previous app categories, time, and location data to\nmake predictions. Designed as a seq2seq model, a type of summarization task, this stage aims to generate target\nsentences based on the given contextual sentences (as shown in Figure 5). The mathematical representation of\nthe seq2seq model is denoted as ğ‘†ğ‘ğ‘¡ = ğ‘†ğ‘’ğ‘2ğ‘†ğ‘’ğ‘(ğ‘†ğ‘ğ‘ğ‘¡ ; ğœƒ), wherein ğ‘†ğ‘ğ‘ğ‘¡ denotes the contextual sentences for app\ntype prediction, and ğ‘†ğ‘ğ‘¡ represents the target sentence.\nFirstly, we transform contexts into sentences using predefined templates. This stage amplifies the modelâ€™s\nability to predict subsequent app types by drawing insights from historical app usage data across all available\ndatasets. We generate target sentences (ğ‘†ğ‘ğ‘¡) by calculating the probability of the next app category based on\nthe given historical app sequence. This probability is derived from analyzing diverse app-type sequences that\nprecede the target app type, as outlined in Algorithm 1. The resulting target sentences accurately capture usersâ€™\napp usage patterns, thereby enhancing the modelâ€™s predictive capabilities.\nIn sum, the ATP Training Stage enhances the modelâ€™s proficiency in predicting app types by leveraging historical\napp categories and contextual features. It emphasizes learning from various contextual feature sentences and\nanticipates further improvement in the subsequent training stage.\n4.3.2 Next App Prediction Training Stage (NTP). The NTP Training Stage constitutes a pivotal advancement in\nour proposed model, building upon the insights gleaned from the App Type Prediction Training Stage. This stage\naims to enhance the modelâ€™s predictive capabilities further, enabling it to forecast the precise next app while\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 10. Publication date: March 2024.\n10:12 â€¢ Khaokaew Y., et al.\nAlgorithm 1Generate the next application type prompting\nInput: Set of app type sequences ğº, number of datasets ğ·, ğ‘˜ top app type\nOutput: Next app categories prompting ğ‘†ğ‘ğ‘¡\n1: Initialize a dictionary final_dict to store the probabilities for each app type sequence;\n2: for each app type sequence ğ‘” (ğ´ğ‘1,ğ´ğ‘2,...,ğ´ğ‘ ğ‘›âˆ’1)in ğº do\n3: Init. dict. count_dict to store the counts of app categories;\n4: for each dataset ğ‘‘ in ğ· do\n5: for each user ğ‘¢in ğ‘‘ do\n6: Iterate through the app type sequences of ğ‘¢;\n7: if the sequence ğ‘”is found then\n8: Incr. the count of the next app type (ğ´ğ‘ğ‘›)in count_dict;\n9: Calculate the sum of all counts in count_dict as total_count;\n10: Init. a dict. prob_dict to store the prob. of app cats.;\n11: for each app category ğ‘ in count_dict do\n12: Calc. prob. ğ‘ƒ(ğ‘|ğ´1,..,ğ‘›)= ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡_ğ‘‘ğ‘–ğ‘ğ‘¡[ğ‘]/ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡;\n13: Store the probability in prob_dict[ğ‘];\n14: Sel. top ğ‘˜ app types with max. probs in p_dict & store in top_k_dict;\n15: Gen. next app type prompt from top ğ‘˜ types using template in Table 4;\n16: Add this generated sentence to final_dict with the key ğ‘”;\nreturn final_dict;\nconsidering broader contextual features. These encompass historical app types, individual app usage patterns,\nand the userâ€™s installed app repertoire.\nWe initiate this stage by leveraging the model trained during the ATP Training Stage. The seq2seq task\n(summarization) remains integral for predictions, and the model is enriched with a broader array of contextual\nfeatures. These features encompass the contextual sentences from the preceding stage and additional personal\ncontext elements, such as individual app usage history and the userâ€™s installed app set. This enhancement\nempowers the model to understand and adapt to usersâ€™ distinct habits and preferences. The seq2seq model for this\nstage is represented as: ğ‘†ğ‘›ğ‘ = ğ‘†ğ‘’ğ‘2ğ‘†ğ‘’ğ‘(ğ‘†ğ‘ğ‘›ğ‘ ; ğœƒ). Here, ğ‘†ğ‘ğ‘›ğ‘ represents the amalgamation of contextual sentences\nused to predict the next app. It incorporates sentences from the previous stage and supplementary personal\ncontext features, including the installed app set and historical app ID sequence. ğ‘†ğ‘›ğ‘ denotes the target sentence\n(2ğ‘›ğ‘‘ stage). This enriched context significantly contributes to predicting the next app, thus heightening predictive\naccuracy.\nThe NTP Training Stage, illustrated in Figure 5, merges installed app data with contextual information to\nformulate the next app prompts. Leveraging LLMs, this stage models complex relationships between installed\napps and contextual data, enabling highly accurate predictions of subsequent app usage. The prompts propose\nthe most suitable next app based on user history and context.\nThis stage essentially amalgamates personal app history, installed app set, historical app type usage, and\ncontextual information to yield exact predictions of the next app. This stage becomes pivotal in mobile app\ndevelopment by utilising LLM-based modelling and seq2seq architecture. The resultant predictions are highly\naccurate and tailored, potentially revolutionising the mobile app industry. Notably, both seq2seq modules in\nour model follow the \"pre-train and fine-tune\" approach using pre-trained weights from the HuggingFace\nModels repository1. This strategy, leveraging established linguistic patterns, significantly boosts prediction\n1https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 10. Publication date: March 2024.\nMAPLE: Mobile App Prediction Leveraging Large Language Model Embeddings â€¢ 10:13\naccuracy. Combining insights from both training stages, our model adeptly forecasts the next app for mobile\nusers. Leveraging an array of contextual features and pre-trained LLMs, the Next App Prediction Module is a\npivotal aspect of our approach. Our codes will be available for public access. Once ready, they can be found at the\nfollowing repository.2\n5 EVALUATION\nThis section aims to appraise our algorithmâ€™s efficacy utilizing authentic data derived from App usage. We\npresent the methodology implemented to undertake the evaluation in Section 5.1. Subsequently, in Section 5.2,\nwe comprehensively examine the algorithmâ€™s performance. The evaluation is centred on two key objectives: a\ncomparative analysis of the algorithmâ€™s performance relative to various baseline models and an exploration of\nthe impact of each module incorporated in our proposed model.\n5.1 Evaluation setup\n5.1.1 Data Pre-Processing. The Tsinghua App Usage dataset used for evaluation contains over 2.3 million mobile\napp usage logs from 871 users in one week, from April 19, 2016, to April 26, 2016. The LSApp dataset used for\nevaluation contains around 600,000 mobile app usage logs from 293 users, with an average duration of 15 days\nper user. We separated the sessions of the app usage logs using a 5-minute threshold, and sessions with more than\nfive thousand records were removed as noise. We removed sessions with over five thousand records, considered\nas noise due to their repetitive pattern, likely resulting from automated processes or system glitches, not user\nbehaviour. Users with less than ten records were also removed at this stage. In our evaluation, we will consider\nthe last fifteen applications used by a user as their historical app usage sequence. This sequence will provide the\nnecessary context for our model to make predictions based on the userâ€™s app usage patterns. We will evaluate\nour proposed model with two different settings (Fig. 6), as shown below:\nâ€¢Standard Setting: Following common practice in app prediction studies, we divided each userâ€™s data\nchronologically: 70% for training, 10% for validation, and 20% for testing. This approach trained, fine-tuned,\nand evaluated our modelâ€™s predictive precision\nâ€¢Cold Start Setting: We tested our modelâ€™s cold start proficiency by splitting users into two distinct groups:\n90% for training and 10% of unseen users for testing. This method evaluated the modelâ€™s performance with\nnew users, addressing the cold start problem. The process was reiterated across all users to validate its\neffectiveness consistently.\nUpon partitioning the dataset according to the specified settings, we subsequently employ a sliding window\nprocedure to generate the samples.\n5.1.2 Performance Metric. We assess the efficacy of our predictions through the utilization of the Accuracy@k\nand mean reciprocal rank (MRR) metrics [13, 14]. Accuracy@k (A@K), computed as the mean of hits@k across\nall test predictions, yields a value of 1 if the actual app is among the top k predictions and 0 otherwise.\nğ´@ğ‘˜ =\nÃ|ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡ |\nğ‘–=1 ğ›¼\n|ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡ | ,ğ›¼|=\n\u001a 1 : ğ‘¦ğ‘Ÿğ‘’ğ‘ğ‘™ âˆˆğ‘Œğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡\n0 : ğ‘¦ğ‘Ÿğ‘’ğ‘ğ‘™ âˆ‰ ğ‘Œğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡\n(2)\nwhere |ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡ |represents the number of test cases.\n2https://github.com/cruiseresearchgroup/MAPLE.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 10. Publication date: March 2024.\n10:14 â€¢ Khaokaew Y., et al.\nCH\ndataset\nLS\n dataset\nuser 1user 2\nuser nuser 1user 2\nuser n\ntraining testing\nCH datasetLS dataset\nuser 1\nuser n\nuser 1\nuser n\nuser 2\nuser nuser n\nuser 2... ...\ntraining testing\na) Standard setting b) Cold start setting \nFig. 6. Comparison of Data Division Strategies for Standard and Cold Start Settings\nMeanwhile, MRR determines the reciprocal of the position at which the first relevant document was retrieved.\nThe value of MRR is defined as follows, given the total number of items in the testing set (N):\nğ‘€ğ‘…ğ‘… =\n\u0010Ã|ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡ |\nğ‘–=1\n1\nğ‘Ÿğ‘ğ‘›ğ‘˜ğ‘–\n\u0011\n|ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡ | (3)\nWhere â€™ğ‘Ÿğ‘ğ‘›ğ‘˜ğ‘–â€™ represents the position of the correct result in the â€™i-thâ€™ prediction. Higher MRR values imply more\naccurate predictions. In our seq2seq model, we adapt these metrics to evaluate top k predictions by generating\nsentences until the top k predictions are collected.\n5.1.3 Baselines. We compare our algorithm design with the following baselines to illustrate our proposed modelâ€™s\nadvantages.\nVanilla model- These are basic statistical baselines commonly employed in numerous [30, 43] to serve as\na starting point for performance comparisons.\nâ€¢MFU (Most Frequently Used): refers to the app that was the most frequently used by each user.\nâ€¢MRU (Most Recently Used): refers to the app that was the most recently used by each user\nApp prediction model- These neural network baselines are tailored for app usage prediction, leveraging\ntechniques and architectures to capture app usage patterns and context, improving prediction accuracy\nand personalization.\nâ€¢AppUsage2Vec [41]: Deep learning model considering appsâ€™ combined influence, user characteristics, and\nusage context.\nâ€¢DeepApp [35]: Multitask learning model mapping time, location, and application to embeddings.\nâ€¢NeuSA [2]: LSTM-based model considering app sequences and temporal user behaviour.\nâ€¢DeepPattern [28]: Spatiotemporal and context-aware model adapted from DeepApp.\nâ€¢CoSEM [14]: Model combining app seq. with semantic info.\nâ€¢SA-GCN [39]: This model uses a Graph Convolutional Network [ 15] to generate dense embeddings of\napps, location, and time from an App usage graph, employing a \"meta-path-based objective\" for enriched\nrepresentations.3\n3Note that we use the same record count as other baselines for user vectors and employ location data from the public version, differing from\nthe version in their paper.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 10. Publication date: March 2024.\nMAPLE: Mobile App Prediction Leveraging Large Language Model Embeddings â€¢ 10:15\nTime series forecasting models4 - We also incorporate time series forecasting models in our experiments,\nas our historical app usage sequences resemble time series data. These models excel at detecting patterns\nin sequential data, serving as a pertinent baseline for our studyâ€™s comparison.\nâ€¢Transformer [32]: Encodes and decodes sequences with a self-attention mechanism.\nâ€¢FEDformer[42]: Combines Fourier analysis with Transformer for long-term forecasting.\nâ€¢Reformer [16]: Efficient Transformer model variant employing locality-sensitive hashing and reversible\nresidual layers.\nâ€¢DLinear [40]: Time series forecasting method combining a decomposition scheme with a linear model.\nâ€¢TimesNet [34]: Neural network architecture transforming 1D time series into 2D tensors.\nProposed model\nâ€¢MAPLE-ED: This is our proposed model that trains one model for each dataset.\nâ€¢MAPLE: This is our proposed model, which considers all components and is trained with the samples from\nboth datasets.\nNote that the experiment with DeepApp and DeepPattern are only reported for the Tsinghua App Usage\ndataset because the LSapp dataset does not contain the location information, an essential feature in the DeepApp\nand DeepPattern frameworks.\n5.2 Result Analysis\n5.2.1 Standard Setting. In this section, we compare the performance of our proposed model with the baseline\nmentioned in the previous section. We assess various models on the Tsinghua App Usage and LSapp datasets, as\ndepicted in table 5. This comparison sheds light on their efficacy in app prediction tasks. The table delineates the\nperformance of different models in predicting mobile app usage under standard conditions. MRR@K (M) and\nAccuracy@K (H) are performance metrics. Bold figures indicate the top performance, whereas underlined figures\ndenote the second best. The evaluation results demonstrate that our proposed models, MAPLE and MAPLE-ED,\nconsistently surpass other models across both Tsinghua App Usage and LSapp datasets in most metrics. This\nsuccess underscores the robustness of our approach in addressing the app prediction challenge.\nCompared to basic benchmarks like MFU and MRU, specialized app prediction models such as AppUsage2Vec,\nDeepApp, NeuSA, DeepPattern, CoSEM, and SA-GCN perform better but donâ€™t reach the levels of our proposed\nmodels. NeuSA, with its LSTM structure, particularly excels at capturing user behaviour. However, time series\nmodels like Transformer, FEDformer, Reformer, DLinear, and TimesNet, despite being adapted for app prediction,\nfall short due to their primary focus on time series forecasting, not app context specifics. This distinction in focus,\ncombined with the larger scale of the Tsinghua App Usage dataset, affects their performance, especially when\ncompared to models designed specifically for app prediction tasks.\nOur MAPLE and MAPLE-ED models excel in app prediction due to their use of a pre-trained LLM and cross-\ndataset user similarity. However, MAPLEâ€™s slightly weaker performance on the LSapp dataset may stem from\nits size discrepancy with the Tsinghua App Usage dataset and its incorporation of location data unavailable\nin LSapp. In essence, while MAPLE-ED stands out in the app prediction, emphasizing the power of LLMs and\nmulti-dataset synergy, model selection should be attuned to specific use cases and dataset nuances. Weâ€™ll next\nprobe our modelâ€™s mettle against the user cold-start challenge, laying the groundwork for deeper analysis in\nupcoming stages.\n5.2.2 User Cold Start Setting. In this section, we evaluate the performance of the models in the user cold start\nproblem. It is important to note that we only selected baselines that do not require user information in their\nmodels, as we could not access data from the new users. Table 6 presents the results for the Tsinghua App Usage\n4https://github.com/thuml/Time-Series-Library/\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 10. Publication date: March 2024.\n10:16 â€¢ Khaokaew Y., et al.\nTable 5. Performance Comparison in Standard Setting (*M=MRR@K, A=Accuracy@K)\nThe bold values represent the best performance, while the underlined values indicate the second-best performance.\nDataset Tsinghua App Usage LSapp\nModel/Metric A@1 A@3 A@5 M@3 M@5 A@1 A@3 A@5 M@3 M@5\nMFU 0.1972 0.4288 0.5384 0.2991 0.3241 0.2952 0.6258 0.7942 0.4378 0.4765\nMRU 0.000 0.5538 0.6536 0.2585 0.2817 0.0276 0.7850 0.8306 0.3974 0.4079\nAppusage2Vec 0.2909 0.4822 0.5781 0.3739 0.3958 0.6057 0.7858 0.8618 0.6848 0.7022\nNeuSA 0.4640 0.6562 0.7286 0.5492 0.5658 0.6832 0.8253 0.8830 0.7461 0.7593\nSA-GCN 0.0613 0.1882 0.2521 0.1183 0.1331 - - - - -\nDeepApp 0.2862 0.5931 0.7075 0.4210 0.4473 - - - - -\nDeepPattern 0.2848 0.5884 0.7016 0.4185 0.4444 - - - - -\nCoSEM 0.4163 0.6682 0.7499 0.5282 0.5469 0.4990 0.7466 0.8149 0.6083 0.6242\nTimesNet 0.0208 0.048 0.0614 0.0327 0.0358 0.4805 0.628 0.6897 0.5459 0.5600\nTransformer 0.0262 0.0534 0.0661 0.0383 0.0412 0.4978 0.653 0.7141 0.5659 0.5800\nFEDformer 0.0159 0.042 0.0553 0.0272 0.0303 0.4946 0.6374 0.6915 0.5585 0.5708\nDLinear 0.0072 0.037 0.0607 0.0202 0.0256 0.1611 0.3978 0.479 0.2637 0.2824\nReformer 0.0228 0.0503 0.0645 0.0346 0.0378 0.4920 0.6505 0.7074 0.5620 0.575\nMAPLE-ED 0.5173 0.7349 0.8070 0.6142 0.6308 0.7171 0.8670 0.9166 0.7836 0.7950\nMAPLE 0.5191 0.7385 0.8115 0.6169 0.6338 0.7157 0.8649 0.9150 0.7821 0.7936\nand LSapp datasets. It is evident that our proposed MAPLE model outperforms all other models in terms of\nAccuracy and Mean Reciprocal Rank metrics for both datasets, emphasizing its effectiveness in handling the cold\nstart problem.\nTable 6. Performance comparison Cold start setting (*M=MRR@K, A=Accuracy@K )\nDataset Tsinghua App Usage LSapp\nModel/Metric A@1 A@3 A@5 M@3 M@5 A@1 A@3 A@5 M@3 M@5\nMFU 0.1853 0.3906 0.4943 0.2752 0.2989 0.6098 0.7716 0.7377 0.4295 0.4667\nMRU 0.0000 0.6406 0.7226 0.3042 0.3234 0.0155 0.8625 0.8868 0.4341 0.4397\nTimesNet 0.0144 0.0433 0.0647 0.0277 0.0323 0.0022 0.0114 0.0246 0.0059 0.0089\nTransformer 0.0180 0.0461 0.0606 0.0308 0.0337 0.0028 0.0174 0.0702 0.0085 0.0201\nFEDformer 0.0100 0.0411 0.061 0.0231 0.0282 0.0024 0.0075 0.0231 0.0044 0.0081\nReformer 0.0224 0.0506 0.057 0.0349 0.0384 0.0034 0.0132 0.0330 0.0064 0.0119\nDLinear 0.0070 0.035 0.0586 0.0199 0.0245 0.1600 0.3968 0.4770 0.2617 0.2814\nCoSEM 0.3111 0.5597 0.6525 0.4204 0.4416 0.4523 0.7243 0.8104 0.5718 0.5918\nNeuSA 0.4433 0.6169 0.6812 0.5206 0.5353 0.6874 0.777 0.8135 0.7272 0.7355\nMAPLE w/o ins. app 0.5227 0.7405 0.8060 0.6201 0.6352 0.7645 0.8718 0.9061 0.8128 0.8207\nMAPLE 0.5228 0.7417 0.8128 0.6206 0.6369 0.7644 0.8848 0.9247 0.8181 0.8272\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 10. Publication date: March 2024.\nMAPLE: Mobile App Prediction Leveraging Large Language Model Embeddings â€¢ 10:17\nThe exceptional performance of our MAPLE model is attributed to its integration of usersâ€™ installed app datasets.\nMAPLE analyzes app installation patterns on mobile devices to deduce user behaviors and preferences. This\nfeature is particularly advantageous for new users lacking prior usage history. Our hypothesis was confirmed\nthrough a performance comparison between our model with installed app data (MAPLE) and without it (MAPLE\nw/o ins. app). Results indicated that although the modified model was effective, the complete versionâ€™s employment\nof installed app data markedly improved predictive accuracy.\nFurthermore, MAPLEâ€™s deployment of a pre-trained LLM enables it to adapt swiftly and more accurately to new\ndatasets, demonstrating its robustness in scenarios where typical LLMs may struggle. This is in contrast to time\nseries forecasting models that depend on a consistent data distribution across training and test setsâ€”a condition\nthat is rarely met with new user data. While models like NeuSA and CoSEM show potential by leveraging\nhistorical app usage patterns, they fall short of MAPLEâ€™s performance, which gains from a comprehensive\nunderstanding of user behaviour gleaned from historical usage and installed apps.\nIn summary, the results underscore the robustness and efficacy of our MAPLE model in tackling the user\ncold start problem, significantly surpassing other models, including established app prediction and time series\nforecasting approaches. These findings confirm that the MAPLE modelâ€™s nuanced ability to leverage data from\ninstalled apps, identify similar user profiles, and utilise a pre-trained LLM is well-equipped to manage the\ncomplexities associated with new user predictions effectively.\n/uni00000024/uni00000026/uni00000026/uni00000023/uni00000014/uni00000030/uni00000035/uni00000035/uni00000023/uni00000018/uni00000024/uni00000026/uni00000026/uni00000023/uni00000018\n/uni00000030/uni00000048/uni00000057/uni00000055/uni0000004c/uni00000046\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b/uni00000039/uni00000044/uni0000004f/uni00000058/uni00000048/uni00000037/uni00000018/uni00000010/uni00000056/uni00000050/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000005a/uni00000012/uni00000052/uni00000003/uni0000005a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni00000003\n/uni00000025/uni00000024/uni00000035/uni00000037/uni00000010/uni00000045/uni00000044/uni00000056/uni00000048/uni00000003\n/uni00000037/uni00000018/uni00000010/uni00000056/uni00000050/uni00000044/uni0000004f/uni0000004f/uni00000003\n/uni00000025/uni00000024/uni00000035/uni00000037/uni00000010/uni0000004f/uni00000044/uni00000055/uni0000004a/uni00000048/uni00000003\n/uni00000037/uni00000018/uni00000010/uni00000045/uni00000044/uni00000056/uni00000048/uni00000003\n/uni00000037/uni00000018/uni00000010/uni0000004f/uni00000044/uni00000055/uni0000004a/uni00000048/uni00000003\n(a) MAPLE with different pre-trained model\n/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b/uni0000001c/uni00000014/uni00000013/uni00000014/uni00000014/uni00000014/uni00000015/uni00000014/uni00000016/uni00000014/uni00000017\n/uni0000002f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b\n/uni00000013/uni00000011/uni00000019/uni00000013\n/uni00000013/uni00000011/uni00000019/uni00000018\n/uni00000013/uni00000011/uni0000001a/uni00000013\n/uni00000013/uni00000011/uni0000001a/uni00000018\n/uni00000013/uni00000011/uni0000001b/uni00000013\n/uni00000013/uni00000011/uni0000001b/uni00000018/uni00000039/uni00000044/uni0000004f/uni00000058/uni00000048\n/uni00000030/uni00000035/uni00000035/uni00000023/uni00000016\n/uni00000030/uni00000035/uni00000035/uni00000023/uni00000018\n/uni00000024/uni00000026/uni00000026/uni00000023/uni00000014\n/uni00000024/uni00000026/uni00000026/uni00000023/uni00000016\n/uni00000024/uni00000026/uni00000026/uni00000023/uni00000018 (b) MAPLE with varying historical app usage length\nFig. 7. Performance of MAPLE with different parameters\n5.2.3 Analysis of Model Parameters. In this section, we analyze the performance of the proposed model on\ndifferent parameters. Specifically, we investigate the effect of using different pre-trained language models and\nvarying the length of the historical app usage sequences.\nOur study assessed the efficacy of various renowned pre-trained language models, including T5 and BART, by\nintegrating them into our MAPLE framework and scrutinizing performance through pertinent metrics. As depicted\nin Figure 7a, performance diverges across the models. Notably, T5-large leads in Accuracy@1, Accuracy@5, and\nMRR@5 for both datasets.\nFurther examination reveals a clear positive correlation between the size of pre-trained models and the\nperformance of MAPLE. Both T5 and BART demonstrate improved results as we scale up from smaller to larger\nmodels, indicating the larger modelsâ€™ superior ability to interpret complex app usage patterns. Additionally,\nthe efficacy of pre-training strategies is evident. T5, with its denoising autoencoder objective and multi-task\ntraining approach, consistently outperforms BARTâ€™s sequence-to-sequence framework. This suggests that a\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 10. Publication date: March 2024.\n10:18 â€¢ Khaokaew Y., et al.\nmulti-tasking pre-training regimen might be more suitable for app usage prediction. The notable difference in\nperformance between the T5-small models, with and without pre-trained weights, also emphasizes the critical\nrole of knowledge transfer via pre-trained models in this domain. These findings solidly affirm the vital role\nof advanced pre-trained language models in boosting predictive accuracy for app usage within the MAPLE\nframework.\nIn evaluating our proposed model alongside various pre-trained language models, we also examined the\nimpact of varying historical app usage sequence lengths, from 3 to 14. The findings indicate that longer historical\nsequences correspond to improved model performance. Notably, metrics like MRR@5 and ACC@5 showed\nenhancement with extended sequences, illustrating the modelâ€™s increased proficiency in detecting app usage\npatterns. However, this trend levels off after approximately 11-12 sequences, hinting at an optimal sequence\nlength that balances comprehensive historical data with computational efficiency.\nIn conclusion, our analysis of the MAPLE model, focusing on the choice of pre-trained model and the length of\nhistorical sequences, reveals that employing models such as T5-large and fine-tuning the sequence length can\nsignificantly refine app usage predictions. This insight is instrumental in developing advanced prediction models\nthat more accurately capture app usage patterns and enhance the precision of forecasts.\nTable 7. Ablation study\nDataset Tsinghua App Usage LSapp\nModel/Metric A@1 A@3 A@5 M@3 M@5 A@1 A@3 A@5 M@3 M@5\nw/o 1ğ‘ ğ‘¡ stage training 0.3226 0.5598 0.6529 0.4275 0.4488 0.5326 0.8099 0.8922 0.6561 0.6750\nw/o App seq info 0.3146 0.5393 0.6392 0.4133 0.4361 0.4853 0.7693 0.8549 0.611 0.6306\nw/o installed App 0.5091 0.7221 0.7892 0.6042 0.6196 0.7146 0.8554 0.8996 0.7772 0.7874\nw/o optional contexts 0.5135 0.7314 0.8035 0.6105 0.6272 0.7145 0.8626 0.9126 0.7804 0.7920\nMAPLE 0.5191 0.7385 0.8115 0.6169 0.6338 0.7157 0.8649 0.9150 0.7821 0.7936\n5.2.4 Ablation Study. In this section, we present an ablation study to precisely delineate the individual contribu-\ntion of each component in the MAPLE model. We aim to understand each moduleâ€™s weightage in contributing to\nour modelâ€™s overall effectiveness. We tested the following model configurations:\nâ€¢MAPLE w/o 1ğ‘ ğ‘¡ stage training is our model trained solely on the second stage without leveraging the\napp-type prediction training.\nâ€¢MAPLE w/o App seq info is our proposed model without using historical app usage information.\nâ€¢MAPLE w/o installed Apps is our proposed model, which does not include the set of installed sentences.\nâ€¢MAPLE w/o optional context is a model that omits the optional context (location context - POI in our\ncases),\nTable 7 presents a detailed overview of performance metrics for various configurations of the MAPLE model\nacross two datasets: Tsinghua App Usage and LSapp. Each configuration corresponds to a version of the MAPLE\nmodel with a particular component excluded, aimed at isolating its individual impact.\nIn the \"w/o 1ğ‘ ğ‘¡ stage training\" configuration, the omission of the initial app-type prediction stage leads to\na decline in performance. For the Tsinghua App Usage dataset, the model yields an MRR@1 of 0.3226 and\nAccuracy@5 of 0.6529, both lower than the complete MAPLE model. Similarly, for the LSapp dataset, the metrics\nare 0.5326 and 0.8922, respectively, demonstrating a performance decrease. When excluding the historical app\nusage data, denoted as \"w/o App seq info, \" the model once again exhibits inferior performance, with an MRR@1\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 10. Publication date: March 2024.\nMAPLE: Mobile App Prediction Leveraging Large Language Model Embeddings â€¢ 10:19\nof 0.3146 and Accuracy@5 of 0.6392 for the Tsinghua App Usage dataset, highlighting the significance of this\ncomponent.\nThe interaction of our model with Point of Interest (POI) data in the \"w/o optional contexts\" setup provides\ncrucial insights. Omitting this context impacts the modelâ€™s effectiveness. For example, in the Tsinghua App\nUsage dataset without POI data, MRR@1 and Accuracy@5 are 0.5135 and 0.8035, respectively. In contrast, with\nthe full MAPLE model including all contexts, these metrics improve to 0.5191 and 0.8115. Notably, even in the\nLSapp dataset, which lacks explicit POI information, an improvement is seen. This suggests the model learns\nto identify app usage patterns indicative of POI-related behavior during training with Tsinghua App Usage\ndata. When it encounters similar patterns in LSapp, it uses these associations to improve predictions. However,\nthis approach has limitations. The model might sometimes misinterpret LSapp behaviors as influenced by POI,\ncausing occasional inaccuracies, as seen in the MAPLE-SEPâ€™s performance.\nIn conclusion, the MAPLE modelâ€™s ability to integrate various contexts and maintain performance despite\nmissing elements demonstrates its robustness. However, careful consideration is required to ensure optimal\naccuracy, particularly when interpreting patterns from diverse datasets.\nTable 8. Memory usage analysis\nModel/Metric # Parameters Model size (b) Inference time\n(s/samples)\nH@1\nTsinghua\nApp Usage\nH@1\nLSApp\nDeepPattern â‰ˆ11ğ‘€ â‰ˆ42ğ‘€ 0.1267ğ‘’âˆ’2 0.2848 -\nDeepApp â‰ˆ10.8ğ‘€ â‰ˆ42ğ‘€ 0.0947ğ‘’âˆ’2 0.2862 -\nAppusage2Vec â‰ˆ3.8ğ‘€ â‰ˆ40ğ‘€ 0.0829ğ‘’âˆ’2 0.2909 0.6057\nCoSEM â‰ˆ8.8ğ‘€ â‰ˆ35ğ‘€ 0.0790ğ‘’âˆ’2 0.4163 0.4990\nNeuSA â‰ˆ28ğ‘€ â‰ˆ167ğ‘€ 0.1197ğ‘’âˆ’2 0.4640 0.6832\nMAPLE (T5-small) â‰ˆ60ğ‘€ â‰ˆ480ğ‘€ 1.1270ğ‘’âˆ’2 0.5191 0.7157\n5.2.5 Limitation on resource requirements. The memory usage analysis of top-performing models in app usage\nprediction is captured in Table 8, highlighting not only the predictive accuracy as measured by Hit Rate at 1\n(H@1) but also the computational resources each model demands, a critical factor for real-world application. Our\nproposed MAPLE model, leveraging a smaller variant of the T5, demonstrates exceptional predictive performance\nwith the highest H@1 scores for both the Tsinghua App Usage and LSApp datasets. However, it is also the most\nresource-intensive in terms of the number of parameters, model size, and inference times.\nWhile MAPLEâ€™s considerable resource demands, evident in its inference time, may present challenges in\nsettings with stringent memory or processing speed requirements, this is offset by the evolving dynamics of\nLLMs and advancements in hardware efficiency, which increasingly support the use of more robust models. The\nversatility of LLMs like T5, which are capable of handling multiple tasks, argues in favor of their adoption, as\nthe overhead can be spread over a range of applications, improving the cost-effectiveness. The trend towards\nintegrating advanced LLMs into mobile technology is anticipated to accelerate, driven by ongoing improvements\nin model optimization and hardware performance. This integration is expected to enhance not just app usage\nprediction but also to equip devices with a broad array of intelligent functions, utilizing the comprehensive\npotential of LLMs. Employing multi-functional LLMs can rationalize the initial resource expenditure by delivering\na more adaptable and potent system on mobile platforms.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 10. Publication date: March 2024.\n10:20 â€¢ Khaokaew Y., et al.\nIn conclusion, the MAPLE modelâ€™s exceptional performance, despite its resource intensity, underscores the\nbenefits of LLMs in app usage prediction. Our ongoing research will focus on improving computational efficiency\nand examining the broader applications of LLMs, reinforcing our commitment to developing scalable and\nmultifunctional AI systems for mobile devices.\n6 DISCUSSION AND IMPLICATION\nOur model outshines traditional mobile app prediction methods using LLMs and contextual data to offer more\nprecise and tailored predictions. The ATP and NAP Modules, using personal app histories and installed apps, along\nwith context, lead to accurate predictions of a userâ€™s next app. This approach allows for a richer understanding of\nuser patterns, enabling finer predictions. By harnessing LLMs, our model expertly deciphers correlations between\napps and contextual factorsâ€”like location and timeâ€”boosting its predictive success. Our Two-Stage LLM Training\nModule fine-tunes these insights to mirror individual behaviours, enhancing user experiences for app developers\nand suggesting new horizons for app recommendations. Our approach adeptly addresses the user cold start issue\nand capitalizes on app data to elevate prediction personalization, showcasing the potential for wider applications\nand setting the stage for future innovation.\nOne important implication of our work is that it provides a framework for developing more effective and\npersonalised mobile app recommendation systems. By incorporating contextual information and personal app\nhistory data, our proposed model offers a more comprehensive approach to app recommendation that considers\nthe userâ€™s current context and the app usage patterns from multiple large datasets. This can lead to more engaging\nand enjoyable mobile user experiences, ultimately increasing user satisfaction and retention.\nAnother implication of our work is that it demonstrates the power of LLM-based models for handling complex,\nheterogeneous data. Our model combines contextual information with personal app history data, which can be\nhighly heterogeneous and difficult to model using traditional machine learning algorithms. By leveraging the\npower of LLM, we can model the complex relationships between the userâ€™s app usage patterns and contextual\ndata, resulting in more accurate and personalised app recommendations.\nOne potential extension of our proposed model is to modify the next app prompt to include more types of\ntarget variables, such as the expected time of usage for the next app (\"3\" seconds from now, this user will use\nthe \"Facebook\" app). Our model could offer users more comprehensive and personalised recommendations by\nincorporating additional variables into the next app prompt. This has important implications for many applications\nbeyond mobile next-app prediction, including time management and productivity. For example, our model could\nbe used to recommend apps that are most likely to be used during certain times of the day based on the userâ€™s\nhistorical app usage patterns. Furthermore, our proposed model has the potential to identify new apps that\nare highly relevant to the userâ€™s interests and usage patterns by leveraging both personal app history data and\ncontextual information. For example, if a user frequently uses apps related to health and fitness, our model could\nrecommend a new app in that category that matches the userâ€™s preferences. This has important implications for\napp developers, who could use our model to introduce their apps to users in a more personalised and targeted\nway.\nOverall, our proposed model offers a promising approach to mobile app usage prediction that combines the\nstrengths of LLM-based models with contextual information, personal app history data, and the set of installed\napplications. By incorporating these data types, our model offers a more accurate and personalised approach\nto app recommendation, improving the overall user experience for mobile users. We believe that our work has\nimportant implications for developing more effective and personalised recommendation systems across a wide\nrange of domains.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 10. Publication date: March 2024.\nMAPLE: Mobile App Prediction Leveraging Large Language Model Embeddings â€¢ 10:21\n7 RELATED WORKS\n7.1 App usage prediction\nOver the past decade, the surge in mobile users has elevated interest in app usage prediction. Huang et al. [12]\nutilised Bayesian methods, incorporating features such as location and mobile state, and achieved a 69% accuracy\non the Nokia MDC dataset [17]. Meanwhile, Shin et al. [27] introduced a naive Bayes-based model, employing\nvarious smartphone contextual data, which realized 78% accuracy. Emphasizing efficient feature usage, Liao\net al. [20] employed feature selection with a kNN classifier, crafting an app usage graph to attain 80% recall at\ntop-5 predictions. [36] explored the influence of social contexts by leveraging community behaviour similarities\nfor prediction improvement. Conversely, Parate et al.[25] demonstrated an 81% accuracy by focusing solely on\napp usage history, asserting that privacy-sensitive features are non-essential. Similarly, Natarajan et al . [24]\nunderscored the relevance of the last app used in a session, using a cluster-level Markov model and the iConRank\nalgorithm to delve into app transition behaviours.\nBaeza-Yates et al.[3] used the idea from the Word2vec [23] approach for extracting the app session feature from\nthe app usage record. This app session features will be used as input in a parallelized Tree Augmented Naive Bayes\nmodel (PTAN) with other contextual features. The contextual features and the app session features are related to\neach other in the app usage prediction, however, the PTAN model assumes these features to be independent.\nZhao et al. [41] proposed the Appusage2vec in 2019. Their work introduced an app-attention mechanism to\nmeasure the contribution of each app to the target app and also proposed the Dual Deep Neural Network model\nthat can learn the user vector from the app usage record. Their model can achieve the 84.47 % of Recall@5 on the\nlarge app usage dataset.\nIn addition, Aliannejadi et al. [1] introduced a framework for predicting target apps based on search intent\nand contextual features, leading to the creation of a universal mobile search system. In contrast, Chen et al. [6]\npresented the CAP model to forecast app usage across various locations and times by forming a relationship\nvector among app, location, and time. Wang et al.[33] addressed data sparsity in large datasets by creating a\nspatio-temporal app usage model, focusing on predicting app categories rather than specific apps. To enhance\napp prediction and address data scarcity in new locations, Fan et al.[8] devised a transfer model using data from\nboth app usage and check-in datasets. Finally, Kang et al. [13] developed a model for predicting app usage during\nspecific contexts like commuting, highlighting the importance of contextual information for accurate predictions\nwhen users are on the move.\nThese previous studies have demonstrated the effectiveness of using contextual information to enhance\nthe modelling performance of the app usage prediction model. The proposed models can solve the app usage\nprediction problem using a specific type of contextual feature and provide high performance. In some situations,\nhowever, these types of contextual features may not be available. Therefore, we develop our model that can be\napplied to different types of contextual information and address the contextual data heterogeneity problem.\n7.2 Large language model on other domains\nLarge language models (LLMs) have brought advancements across various fields, including natural language\nprocessing (NLP), healthcare, recommender systems, coding assistance, multimodal learning, and human mobility\nforecasting, demonstrating their flexibility and potential. In NLP, LLMs like BERT [7], GPT [26], and RoBERTa\n[21] have set benchmarks in tasks such as sentiment analysis, translation, and summarization. They have enabled\nfine-tuning for specific tasks, such as enhancing performances in text classification [11].\nThe healthcare sector has also embraced LLMs, utilizing them in medical information extraction and patient\ntriage. For example, BioBERT [18], a domain-specific LLM based on BERT, has been pre-trained on a large-scale\nbiomedical corpus and employed for tasks like named entity recognition, relation extraction, and question-\nanswering within the biomedical literature. Moreover, the recommender systems domain has also integrated\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 10. Publication date: March 2024.\n10:22 â€¢ Khaokaew Y., et al.\nLLMs to capture complex patterns in user behaviour and contextual information. For instance, BERT4Rec [29] is\nadept at capturing user behaviour in a session-based setting, surpassing traditional methods. LLMs have also\nbeen used to model user-item interactions and incorporate temporal dynamics, improving recommendation\npersonalization. In the realm of code generation, LLMs such as OpenAIâ€™s Codex [5, 10] and CodeBERT [9] have\nbeen valuable in generating code snippets, suggesting API usage, performing code completion tasks, and assisting\nin debugging.\nLLMs have been instrumental in multimodal learning, which involves fusing information from different\ndata modalities, such as text, images, and videos. Models like ViLBERT [ 22] have performed tasks like visual\nquestion answering and visual commonsense reasoning, showing LLMsâ€™ ability to reason across diverse data\ntypes. Temporal sequential pattern mining is another domain where LLMs have shown potential, as in the\nAuxMobLCast [38] pipeline that predicts Place-of-Interest customer flows, demonstrating their capability to\nmodel human behaviour.\nDespite LLMsâ€™ extensive applications, their use for app usage prediction remains underexplored. Given LLMsâ€™\nproven aptitude in domains like mobility forecasting, we assert that their ability to discern complex user behaviours\nand contexts can enhance app usage predictions. Consequently, we propose an innovative LLM-based method for\napp usage prediction, expanding LLM research and showcasing their potential to tackle challenges inherent to\nthis task.\n8 CONCLUSION\nIn conclusion, this research paper presents a novel approach for predicting app usage behaviour by leveraging\nlanguage foundation models (LLMs). Our model, MAPLE, outshines existing benchmarks in standard and cold\nstart scenarios, encompassing both time series forecasting models and various other app prediction models. The\nintegration of LLMs enables our model to adeptly adjust to different datasets through pre-trained models, taking\nadvantage of the data available in both datasets and the array of apps installed on usersâ€™ phones. Incorporating\ninstalled apps empowers our model to address the user cold start issue, as it identifies similar users through\ntheir app installation patterns, potentially reflective of akin habits and lifestyles. This key information is then\nleveraged to anticipate app usage behaviour for new users.\nOne of the main reasons for the success of our proposed model is the ability of LLMs to capture complex patterns\nand relationships within data. In the realm of app usage prediction, this translates to a deeper understanding of\nuser behaviour and context, resulting in more accurate forecasts. The related works discussed in this paper also\nunderscore the versatility and applicability of LLMs across diverse domains, such as human mobility forecasting\nand sentiment analysis. These explorations not only corroborate the efficacy of LLMs but also pave the way for\ntheir potential application in a broader range of predictive modelling scenarios, extending their relevance beyond\nthe scope of our current study.\nFuture research opportunities abound in enhancing the predictive prowess of our model. Investigating various\nLLM architectures alongside novel fine-tuning methodologies holds promise for boosting performance. Delving\ninto the integration of richer contextual datasets, including user demographics and broader environmental factors,\ncould unveil deeper influences on app utilization patterns. Additionally, the prospect of developing specialized,\nlightweight LLMs presents an enticing pathway to reduce computational demands while maintaining accuracy.\nThe adaptability of our model also paves the way for its application across diverse domains with sequential data,\npotentially expanding its utility and significance in user behaviour prediction.\nIn summary, this research paper has successfully demonstrated the potential of LLMs in app usage prediction\nand has opened up new opportunities for future research in this area. With its unique inclusion of the set of\ninstalled apps, the proposed MAPLE model has shown promising results in both regular and cold start settings,\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 10. Publication date: March 2024.\nMAPLE: Mobile App Prediction Leveraging Large Language Model Embeddings â€¢ 10:23\nhighlighting the value of leveraging LLMs in understanding and modelling human behaviour across various\ndomains.\nACKNOWLEDGMENTS\nThis research is supported by the Royal Thai Government, the UNSW RTP scholarship, and the ARC Centre of\nExcellence for Automated Decision-Making and Society (CE200100005). Additionally, this research was undertaken\nwith the assistance of resources and services from the National Computational Infrastructure (NCI), which is\nsupported by the Australian Government.\nREFERENCES\n[1] Mohammad Aliannejadi, Hamed Zamani, Fabio Crestani, and W. Bruce Croft. 2018. In Situ and Context-Aware Target Apps Selection\nfor Unified Mobile Search. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management (Torino,\nItaly) (CIKM â€™18) . Association for Computing Machinery, New York, NY, USA, 1383â€“1392. https://doi.org/10.1145/3269206.3271679\n[2] Mohammad Aliannejadi, Hamed Zamani, Fabio Crestani, and W Bruce Croft. 2021. Context-aware target apps selection and recommen-\ndation for enhancing personal mobile assistants. ACM Transactions on Information Systems (TOIS) 39, 3 (2021), 1â€“30.\n[3] Ricardo Baeza-Yates, Di Jiang, Fabrizio Silvestri, and Beverly Harrison. 2015. Predicting The Next App That You Are Going To Use. In\nProceedings of the Eighth ACM International Conference on Web Search and Data Mining (Shanghai, China) (WSDM â€™15) . Association for\nComputing Machinery, New York, NY, USA, 285â€“294. https://doi.org/10.1145/2684822.2685302\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam,\nGirish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33\n(2020), 1877â€“1901.\n[5] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott\nGray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski\nSuch, David W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol,\nIgor Babuschkin, Suchir Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford,\nMatthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,\nIlya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. ArXiv abs/2107.03374 (2021), â€“.\nhttps://api.semanticscholar.org/CorpusID:235755472\n[6] Xinlei Chen, Yu Wang, Jiayou He, Shijia Pan, Yong Li, and Pei Zhang. 2019. CAP: Context-Aware App Usage Prediction with\nHeterogeneous Graph Embedding. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 3, 1, Article 4 (March 2019), 25 pages.\nhttps://doi.org/10.1145/3314391\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers\nfor Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) .\nAssociation for Computational Linguistics, 4171â€“4186. https://doi.org/10.18653/V1/N19-1423\n[8] Yali Fan, Zhen Tu, Yong Li, Xiang Chen, Hui Gao, Lin Zhang, Li Su, and Depeng Jin. 2019. Personalized Context-Aware Collaborative\nOnline Activity Prediction. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 3, 4, Article 132 (Dec. 2019), 28 pages. https:\n//doi.org/10.1145/3369829\n[9] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming\nZhou. 2020. CodeBERT: A Pre-Trained Model for Programming and Natural Languages. In Findings of the Association for Computational\nLinguistics: EMNLP 2020, Online Event, 16-20 November 2020 (Findings of ACL, Vol. EMNLP 2020) . Association for Computational Linguistics,\n1536â€“1547. https://doi.org/10.18653/V1/2020.FINDINGS-EMNLP.139\n[10] James Finnie-Ansley, Paul Denny, Brett A. Becker, Andrew Luxton-Reilly, and James Prather. 2022. The Robots Are Coming: Exploring\nthe Implications of OpenAI Codex on Introductory Programming. In ACE â€™22: Australasian Computing Education Conference, Virtual\nEvent, Australia, February 14 - 18, 2022 . ACM, 10â€“19. https://doi.org/10.1145/3511861.3511863\n[11] Santiago GonzÃ¡lez-Carvajal and Eduardo C. Garrido-MerchÃ¡n. 2020. Comparing BERT against traditional machine learning text\nclassification. CoRR abs/2005.13012 (2020). arXiv:2005.13012 https://arxiv.org/abs/2005.13012\n[12] Ke Huang, Chunhui Zhang, Xiaoxiao Ma, and Guanling Chen. 2012. Predicting Mobile Application Usage Using Contextual Information.\nIn Proceedings of the 2012 ACM Conference on Ubiquitous Computing (Pittsburgh, Pennsylvania) (UbiComp â€™12). Association for Computing\nMachinery, New York, NY, USA, 1059â€“1065. https://doi.org/10.1145/2370216.2370442\n[13] Yufan Kang, Mohammad Saiedur Rahaman, Yongli Ren, Mark Sanderson, Ryen W White, and Flora D Salim. 2022. App usage on-the-move:\nContext-and commute-aware next app prediction. Pervasive and Mobile Computing 87 (2022), 101704.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 10. Publication date: March 2024.\n10:24 â€¢ Khaokaew Y., et al.\n[14] Yonchanok Khaokaew, Mohammad Saiedur Rahaman, Ryen W. White, and Flora D. Salim. 2021. CoSEM: Contextual and Semantic\nEmbedding for App Usage Prediction. In CIKM â€™21: The 30th ACM International Conference on Information and Knowledge Management,\nVirtual Event, Queensland, Australia, November 1 - 5, 2021 . ACM, 3137â€“3141. https://doi.org/10.1145/3459637.3482076\n[15] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with Graph Convolutional Networks. In 5th International\nConference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings . OpenReview.net.\nhttps://openreview.net/forum?id=SJU4ayYgl\n[16] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The Efficient Transformer. In8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net. https://openreview.net/forum?id=rkgNKkHtvB\n[17] Juha Laurila, Daniel Gatica-Perez, Imad Aad, Jan Blom, Olivier Bornet, T.-M.-T Do, Olivier Dousse, Julien Eberle, and Markus Miettinen.\n2012. The mobile data challenge: Big data for mobile computing research. Nokia Research Center.\n[18] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2020. BioBERT: a pre-trained\nbiomedical language representation model for biomedical text mining. Bioinformatics 36, 4 (2020), 1234â€“1240.\n[19] Youpeng Li, Xuyu Wang, and Lingling An. 2023. Hierarchical Clustering-based Personalized Federated Learning for Robust and Fair\nHuman Activity Recognition. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 7, 1 (2023), 1â€“38.\n[20] Zhung-Xun Liao, Shou-Chung Li, Wen-Chih Peng, Philip S. Yu, and Te-Chuan Liu. 2013. On the Feature Discovery for App Usage\nPrediction in Smartphones. In 2013 IEEE 13th International Conference on Data Mining, Dallas, TX, USA, December 7-10, 2013 . IEEE\nComputer Society, 1127â€“1132. https://doi.org/10.1109/ICDM.2013.130\n[21] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin\nStoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. CoRR abs/1907.11692 (2019). arXiv:1907.11692\nhttp://arxiv.org/abs/1907.11692\n[22] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for\nVision-and-Language Tasks. InAdvances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing\nSystems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada . 13â€“23. https://proceedings.neurips.cc/paper/2019/hash/\nc74d97b01eae257e44aa9d5bade97baf-Abstract.html\n[23] TomÃ¡s Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Distributed Representations of Words and\nPhrases and their Compositionality. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural\nInformation Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States . 3111â€“3119.\nhttps://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html\n[24] Nagarajan Natarajan, Donghyuk Shin, and Inderjit S. Dhillon. 2013. Which App Will You Use next? Collaborative Filtering with\nInteractional Context. In Proceedings of the 7th ACM Conference on Recommender Systems (Hong Kong, China) (RecSys â€™13) . Association\nfor Computing Machinery, New York, NY, USA, 201â€“208. https://doi.org/10.1145/2507157.2507186\n[25] Abhinav Parate, Matthias BÃ¶hmer, David Chu, Deepak Ganesan, and Benjamin M. Marlin. 2013. Practical Prediction and Prefetch\nfor Faster Access to Applications on Mobile Phones. In Proceedings of the 2013 ACM International Joint Conference on Pervasive and\nUbiquitous Computing (Zurich, Switzerland) (UbiComp â€™13) . Association for Computing Machinery, New York, NY, USA, 275â€“284.\nhttps://doi.org/10.1145/2493432.2493490\n[26] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised\nmultitask learners. OpenAI blog 1, 8 (2019), 9.\n[27] Choonsung Shin, Jin-Hyuk Hong, and Anind K. Dey. 2012. Understanding and Prediction of Mobile Application Usage for Smart\nPhones. In Proceedings of the 2012 ACM Conference on Ubiquitous Computing (Pittsburgh, Pennsylvania) (UbiComp â€™12) . Association for\nComputing Machinery, New York, NY, USA, 173â€“182. https://doi.org/10.1145/2370216.2370243\n[28] Basem Suleiman, Kevin Lu, Hong Wa Chan, and Muhammad Johan Alibasa. 2021. DeepPatterns: Predicting Mobile Apps Usage from\nSpatio-Temporal and Contextual Features. In Service-Oriented Computing - 19th International Conference, ICSOC 2021, Virtual Event,\nNovember 22-25, 2021, Proceedings (Lecture Notes in Computer Science, Vol. 13121) . Springer, 811â€“818. https://doi.org/10.1007/978-3-030-\n91431-8_58\n[29] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential Recommendation with\nBidirectional Encoder Representations from Transformer. In Proceedings of the 28th ACM International Conference on Information and\nKnowledge Management, CIKM 2019, Beijing, China, November 3-7, 2019 . Association for Computing Machinery, 1441â€“1450. https:\n//doi.org/10.1145/3357384.3357895\n[30] Yuan Tian, Ke Zhou, and Dan Pelleg. 2021. What and how long: Prediction of mobile app engagement. ACM Transactions on Information\nSystems (TOIS) 40, 1 (2021), 1â€“38.\n[31] Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing Data using t-SNE. Journal of Machine Learning Research 9, 86 (2008),\n2579â€“2605. http://jmlr.org/papers/v9/vandermaaten08a.html\n[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.\n2017. Attention is All you Need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information\nProcessing Systems 2017, December 4-9, 2017, Long Beach, CA, USA . 5998â€“6008. https://proceedings.neurips.cc/paper/2017/hash/\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 10. Publication date: March 2024.\nMAPLE: Mobile App Prediction Leveraging Large Language Model Embeddings â€¢ 10:25\n3f5ee243547dee91fbd053c1c4a845aa-Abstract.html\n[33] Huandong Wang, Yong Li, Sihan Zeng, Gang Wang, Pengyu Zhang, Pan Hui, and Depeng Jin. 2019. Modeling Spatio-Temporal App\nUsage for a Large User Population. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 3, 1, Article 27 (March 2019), 23 pages.\nhttps://doi.org/10.1145/3314414\n[34] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. 2023. TimesNet: Temporal 2D-Variation Modeling\nfor General Time Series Analysis. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May\n1-5, 2023 . OpenReview.net. https://openreview.net/pdf?id=ju_Uqw384Oq\n[35] Tong Xia, Yong Li, Jie Feng, Depeng Jin, Qing Zhang, Hengliang Luo, and Qingmin Liao. 2020. Deepapp: predicting personalized\nsmartphone app usage via context-aware multi-task learning. ACM Transactions on Intelligent Systems and Technology (TIST) 11, 6 (2020),\n1â€“12.\n[36] Ye Xu, Mu Lin, Hong Lu, Giuseppe Cardone, Nicholas Lane, Zhenyu Chen, Andrew Campbell, and Tanzeem Choudhury. 2013. Preference,\nContext and Communities: A Multi-Faceted Approach to Predicting Smartphone App Usage Patterns. In Proceedings of the 2013\nInternational Symposium on Wearable Computers (Zurich, Switzerland) (ISWC â€™13). Association for Computing Machinery, New York, NY,\nUSA, 69â€“76. https://doi.org/10.1145/2493988.2494333\n[37] Hao Xue, Flora D. Salim, Yongli Ren, and Charles L. A. Clarke. 2022. Translating Human Mobility Forecasting through Natural Language\nGeneration. In WSDM â€™22: The Fifteenth ACM International Conference on Web Search and Data Mining, Virtual Event / Tempe, AZ, USA,\nFebruary 21 - 25, 2022 . Association for Computing Machinery, 1224â€“1233. https://doi.org/10.1145/3488560.3498387\n[38] Hao Xue, Bhanu Prakash Voutharoja, and Flora D. Salim. 2022. Leveraging language foundation models for human mobility forecasting.\nIn Proceedings of the 30th International Conference on Advances in Geographic Information Systems, SIGSPATIAL 2022, Seattle, Washington,\nNovember 1-4, 2022 . Association for Computing Machinery, 90:1â€“90:9. https://doi.org/10.1145/3557915.3561026\n[39] Yue Yu, Tong Xia, Huandong Wang, Jie Feng, and Yong Li. 2020. Semantic-Aware Spatio-Temporal App Usage Representation\nvia Graph Convolutional Network. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 4, 3, Article 101 (sep 2020), 24 pages.\nhttps://doi.org/10.1145/3411817\n[40] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. 2023. Are Transformers Effective for Time Series Forecasting?. In Proceedings of the\nAAAI conference on artificial intelligence . AAAI Press, 11121â€“11128. https://doi.org/10.1609/AAAI.V37I9.26317\n[41] Sha Zhao, Zhiling Luo, Ziwen Jiang, Haiyan Wang, Feng Xu, Shijian Li, Jianwei Yin, and Gang Pan. 2019. AppUsage2Vec: Modeling\nSmartphone App Usage for Prediction. In 35th IEEE International Conference on Data Engineering, ICDE 2019, Macao, China, April 8-11,\n2019. IEEE, 1322â€“1333. https://doi.org/10.1109/ICDE.2019.00120\n[42] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. 2022. FEDformer: Frequency Enhanced Decomposed\nTransformer for Long-term Series Forecasting. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore,\nMaryland, USA (Proceedings of Machine Learning Research, Vol. 162) . PMLR, 27268â€“27286. https://proceedings.mlr.press/v162/zhou22g.\nhtml\n[43] Yifei Zhou, Shaoyong Li, and Yaping Liu. 2020. Graph-based Method for App Usage Prediction with Attributed Heterogeneous Network\nEmbedding. Future Internet 12, 3 (2020), 58.\nReceived 15 May 2023; revised 15 November 2023; accepted 11 January 2024\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 10. Publication date: March 2024.",
  "topic": "Maple",
  "concepts": [
    {
      "name": "Maple",
      "score": 0.800683319568634
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.7765701413154602
    },
    {
      "name": "Computer science",
      "score": 0.7497256994247437
    },
    {
      "name": "Process (computing)",
      "score": 0.5008625984191895
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.4807705283164978
    },
    {
      "name": "Mobile apps",
      "score": 0.4737818241119385
    },
    {
      "name": "Data science",
      "score": 0.4037700295448303
    },
    {
      "name": "Machine learning",
      "score": 0.3734504282474518
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36112359166145325
    },
    {
      "name": "World Wide Web",
      "score": 0.33812278509140015
    },
    {
      "name": "Humanâ€“computer interaction",
      "score": 0.3316190242767334
    },
    {
      "name": "Programming language",
      "score": 0.10006055235862732
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I31746571",
      "name": "UNSW Sydney",
      "country": "AU"
    }
  ]
}