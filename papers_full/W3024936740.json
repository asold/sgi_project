{
  "title": "Finding Experts in Transformer Models",
  "url": "https://openalex.org/W3024936740",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4221371087",
      "name": "Suau, Xavier",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227321588",
      "name": "Zappella, Luca",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222650835",
      "name": "Apostoloff, Nicholas",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2964074081",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2952984539",
    "https://openalex.org/W2970242004",
    "https://openalex.org/W2931212643",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2885396331",
    "https://openalex.org/W2557449848",
    "https://openalex.org/W2908854766",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2962776659",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2964222296",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2889056219",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2988660047",
    "https://openalex.org/W2891612330",
    "https://openalex.org/W2963610729",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963749936",
    "https://openalex.org/W2079182758",
    "https://openalex.org/W2184268318"
  ],
  "abstract": "In this work we study the presence of expert units in pre-trained Transformer Models (TM), and how they impact a model's performance. We define expert units to be neurons that are able to classify a concept with a given average precision, where a concept is represented by a binary set of sentences containing the concept (or not). Leveraging the OneSec dataset (Scarlini et al., 2019), we compile a dataset of 1641 concepts that allows diverse expert units in TM to be discovered. We show that expert units are important in several ways: (1) The presence of expert units is correlated ($r^2=0.833$) with the generalization power of TM, which allows ranking TM without requiring fine-tuning on suites of downstream tasks. We further propose an empirical method to decide how accurate such experts should be to evaluate generalization. (2) The overlap of top experts between concepts provides a sensible way to quantify concept co-learning, which can be used for explainability of unknown concepts. (3) We show how to self-condition off-the-shelf pre-trained language models to generate text with a given concept by forcing the top experts to be active, without requiring re-training the model or using additional parameters.",
  "full_text": "Finding Experts in Transformer Models\nXavier Suau Luca Zappella Nicholas Apostoloff\n{xsuaucuadros, lzappella, napostoloff}@apple.com\nApple\nAbstract\nIn this work we study the presence of expert units in\npre-trained Transformer Models (TMs), and how they\nimpact a model’s performance. We deﬁne expert units\nto be neurons that are able to classify a concept with a\ngiven average precision, where a concept is represented\nby a binary set of sentences containing the concept (or\nnot). Leveraging the OneSec dataset [32], we compile\na dataset of 1641 concepts that allows diverse expert\nunits in TMs to be discovered. We show that expert\nunits are important in several ways: (1) The presence\nof expert units is correlated (r2 = 0.833) with the gen-\neralization power of TMs, which allows ranking TMs\nwithout requiring ﬁne-tuning on suites of downstream\ntasks. We further propose an empirical method to de-\ncide how accurate such experts should be to evaluate\ngeneralization. (2) The overlap of top experts between\nconcepts provides a sensible way to quantify concept\nco-learning, which can be used for explainability of\nunknown concepts. (3) We show how to self-condition\noff-the-shelf pre-trained language models to generate\ntext with a given concept by forcing the top experts to\nbe active, without requiring re-training the model or\nusing additional parameters.\n1. Introduction\nNatural language processing (NLP) has evolved at\na fast pace during recent years. Large and powerful\nmodels based on the Transformer architecture [35] can\nnow be trained on large datasets, achieving impressive\nperformance on many NLP tasks, such as GLUE [36] or\nSQuAD [29, 28]. Training TMs is tedious due to both\nthe size of models and datasets, and requires resources\nthat are unavailable to many users. For example, in [25]\nIn this work we study the presence of expert units in pre-trained\nTransformer Models (TMs), and how they impact a model’s per-\nformance. We deﬁne expert units to be neurons that are able\nto classify a speciﬁc concept with a given average precision,\nhence being able to correctly classify a future ball’s passing\nor shooting angle. We ﬁnd that many multi-category TMs are\nspeciﬁcally trained to indicate the accuracy of a given concept\nwhen following the instructions of a pre-speciﬁed skill coach\nand/or observers. In this experiment we test this hypothesis,\nusing TMs that specialize in the shooting skills of throwing and\ncatching.\nFigure 1: Example of generated text by GPT2-L con-\nditioned on the WordNet concept football%1:04:00 1\nby forcing only its top 50 expert units (0.012% of\nthe 414720 units analyzed) as determined by our in-\nterpretation method detailed in Section 4.1. The begin-\nning of this paper’s abstract is given as context (gray).\nNeither the interpretation nor the conditioning require\nre-training, ﬁne-tuning or using additional parameters.\nNote the strong presence of concept football%1:04:00\nin the generated text, including words like ‘coach’ or\n‘shooting’. Even more interesting is how the the term\n‘TM’ appears in a sporting sense, and how TMs ‘spe-\ncialize’, taking the initial context also into account.\nthe model was trained on 512 GPUs. A recent trend is\nto pre-train these large models on diverse datasets and\nmake them available to the community [16, 9, 27, 19,\n38, 21, 31], so that end-users can leverage the powerful\nfeatures learnt to solve downstream tasks.\nHowever, it is not fully understood why these mod-\nels perform so well. Inspired by observations in neu-\nroscience stating that the human brain is a network of\nhierarchically organized specialized modules [13] and\nthat neurons become more selective as humans learn\n[22], we aim to ﬁnd specialized modules in TMs, the\narXiv:2005.07647v1  [cs.AI]  15 May 2020\nexpert units. We hypothesize that the presence of expert\nunits is related to the knowledge acquired by TMs and\nto their performance.\nPrevious work in image processing has shown that\nCNNs and GANs learn representations of speciﬁc ob-\njects at a ﬁlter level [ 3, 4] and by ﬁlter combinations\n[11], even if those objects were never explicitly labeled\nduring training. The key idea behind these works is\nto consider CNN feature maps as segmentation masks,\nwhich allows quantifying the coherence with a densely\nlabeled image by means of intersection over union\n(IOU). These works have also inspired our research,\nhowever there are fundamental differences in our work:\n(1) In NLP it is harder to deﬁne a concept with a single\nsentence, thus we propose to represent concepts with\nsets of positive and negative sentences as explained in\nSec. 3. We collect a total of 1641 concepts, leverag-\ning the OneSec dataset [32]. (2) We consider the most\nbasic units in TMs, the neurons, as expert unit candi-\ndates, which allows computing average precision AP\n(i.e. area under the precision-recall curve) to quantify\nhow a unit is able to disambiguate a concept . (3) Since\nsentences can be of arbitrary length, we maxpool the\nunit responses in time to be invariant to length. The pro-\nposed method to identify and rank experts is detailed\nin Sec. 4.1.\nIn Sec. 4.2 we deﬁne a metric called concept ex-\npertise, and we show that it is strongly correlated\n(r2 = 0.833) with the generalization power of TMs. As\na measure for model generalization we use the average\nperformance on diverse downstream tasks: all GLUE\ntasks + SQuAD v1.1/2.0. We propose an empirical\nmethod to compute the optimal expertise level that max-\nimizes the correlation between generalization and ex-\npertise. The obtained expertise (AP above 0.985) shows\nthat generalization is related to the presence of ex-\ntremely good and diverse experts. Our deﬁnition of ex-\npertise enables the ranking of TMs withoutﬁne-tuning\non large suites of downstream tasks (current practice),\nmitigating the need for hyper-parameter search and the\nproblem of downstream task bias [24]. Moreover, the\nproposed concept dataset can be easily enriched for\nﬁner model comparison.\nIn Sec. 5 we show that concepts with similar mean-\ning are co-learnt by a certain number of experts. We\ndeﬁne concept overlap to quantify co-learning, and we\nshow its utility for concept explainability.\nThe presence of experts is also exploited in Sec. 6\nto self-condition a pre-trained language model (LM)\nto generate text that contains a speciﬁc concept (see\nFig. 1 for an example). We base our approach on the\nproduct of experts formulation introduced by [14] and\nadapted to image generative models by [23] by training\nan external conditional expert. In addition to applying\nthe product of experts formulation to a new domain\n(NLP) and new architecture (TM), a notable difference\nis that we consider that conditional experts already\nexist in the pre-trained model. The results in Sec. 6.1\nshow that only a small number of experts is required to\ninduce a concept in the model output, supporting our\nhypothesis. Other works have tackled LM condition-\ning [17, 30, 6, 18], all based on learning disentangled\nconcepts during training. To the best of our knowl-\nedge, our work is the ﬁrst to condition an off-the-shelf\npre-trained LM without ﬁne-tuning, re-training or us-\ning additional parameters. Furthermore, our method is\nextremely simple to implement.\n2. Related work\nThe NLP community is experiencing a sharp in-\ncrease in interpretation methods. We focus on those\nexploring Transformer architectures, which are the key-\nstone for most of the recent top performing models.\nSaliency Some works focus on analyzing the self-\nattention layers in the Transformer blocks, visualizing\nsaliency [12] or studying how attention heads attend\nto different word families [ 7]. The analysis of atten-\ntion layers usually results in a word-word relationship,\nwhich can make it hard to extract model-wide con-\nclusions. Moreover, recent studies show that saliency\nbased methods may be invariant to the model or the\ndata [1] and can be easily manipulated [10].\nIntermediate discriminators Another trend is to\nprobe the model with a dataset representative of some\ndownstream task, either at a sentence level [2, 8] or at\na word level [33, 20]. The common practice is to train\na classiﬁer on top of selected intermediate features to\nassess their discriminative power. These approaches\ninspired our work, but rather than learning classiﬁers to\nsolve downstream tasks, we probe the TM’s responses\ndirectly with a large set of concepts unrelated to the\nﬁnal task. We propose treating the units of an already\ntrained TM as classiﬁers themselves.\nDisentangled learning Most methods tackling con-\ncept learning are based on training dedicated archi-\ntectures. Concepts such as syntax and semantics [ 6],\nmeaning and form [30], or sentiment and tense [17] can\nbe disentangled by capturing different intrinsic aspects\nof text. Although effective, these methods suffer from\nthe requirements of TM training. Our approach does\nnot require training or knowledge of the training proce-\ndure. It requires only a pre-trained model and a dataset\nof concepts, such as the dataset described in Sec. 3.\n3. SentenceConcepts: A dataset of concepts\nrepresented by sentences\nWe propose a data-driven approach to describe a\nconcept. We collect N+\nc positive sentences that contain\nconcept cand N−\nc negative sentences that do not con-\ntain concept c. Such ﬂexible deﬁnition allows diverse\ntypes of concepts to be represented. For example, one\ncan collect positive sentences containing a keyword\nwith a speciﬁc meaning, e.g. note as a reminder, or\nnote as a musical frequency. We construct our dataset\nleveraging the recently published OneSec dataset [32],\nwhich contains sentences with one keyword annotated\nwith a WordNet1 sense [26]. We consider 2 concept\ncategories:\n•Sense: Positive sentences contain a keyword with\na speciﬁc WordNet sense, whereas negative sen-\ntences do not contain the keyword.\n•Homograph: Positive sentences contain a key-\nword with a speciﬁc WordNet sense, whereas neg-\native sentences contain the same keyword with a\ndifferent meaning. Intuitively, homograph con-\ncepts are harder to disambiguate than sense con-\ncepts.\nIn total, the dataset contains 1344 sense and 297 ho-\nmograph concepts. The complete list of concepts and\ndetails on the annotations are provided in Appendix E.\nThe number of sentences collected is constrained by\navailability in the source dataset. We limit the data per\n1We adopt the WordNet sense key formulation, of the form\nlemma%A:BB:CC, clickable as web links across the paper.\nFigure 2: Schema of a Transformer block [ 35]. In\nthis work we analyze the units in the linear layers A,\nAproj, B and Bproj in each block (red dots), where D\nis the dimensionality of the embedding. For example,\nin GPT2-large (D= 1280 and 36 blocks) we analyze\n36 ·9D= 414720 units.\nconcept to N+\nc ,N−\nc ∈[100,1000], randomly sampling\nwhen more than 1000 sentences are available.\n4. Expert Units\n4.1. Finding expert units\nLet xi = [ xi,1,··· ,xi,T] ∈ RD×T be a sen-\ntence composed of an arbitrary number T of tokens\nxi,t ∈RD, with D being the dimensionality of the\ntoken embedding. A layer ℓ of a TM produces an\nintermediate representation zℓ\ni = [ zℓ\ni,1,··· ,zℓ\ni,T] ∈\nRDℓ×T, where typically Dℓ is a multiple of D. Let\n[u(ℓ,1)\ni ,··· ,u(ℓ,Dℓ)\ni ]⊤ = maxpool\n(\nzℓ\ni,axis = 1\n)\n∈\nRDℓ\nbe the intermediate representation max-pooled in\nthe temporal dimension, where each elementu(ℓ,k)\ni ∈R\nis the response of unit kin layer ℓto sentence i. For\nsimplicity the (ℓ,k) indexing is replaced bym= 1..M,\nwith M being the total number of units in the model.\nThe layers analyzed are shown in Fig. 2. Letum\nc ∈RNc\n(where Nc = N+\nc + N−\nc ) be the pooled response of\nunit mto the sentences that represent concept c, and let\nbc ∈ZNc\n2 be the binary labels for such sentences. We\ntreat a unit as a binary classiﬁer for the input sentences,\nand consider the whole network as a collection of binary\nclassiﬁers. By using um\nc ∈RNc as prediction scores\nfor bc, we can compute APm\nc = AP(um\nc ,bc) ∈[0,1]\nper unit m and per concept c, which allows ranking\nunits by expertise (or AP) on each concept. We expect\nthat, given the large search space, certain classiﬁers\nwill perform well on speciﬁc concepts: the expert units.\nFigure 3: Number of acquired concepts (γ = 0.95) per layer type for model GPT2-L (I, II) and RoBERTa-L (III,\nIV). Shallow layers acquire more sense concepts than deep layers, such effect being emphasized in GPT2-L. This\nbehavior is similar to the one observed by [4] for image GANs. We further observe that B layers acquire 3.5x more\nconcepts than A layers. Aproj and Bproj layers acquire very few concepts, suggesting that the expanding layers (A,\nB) in the Transformer block (Fig. 2) are more prone to learn concepts. Homograph concepts (II, IV) are spread\nacross the layers, few being detected in the ﬁrst Transformer blocks.\n4.2. Concept expertiseXγ\nLet AP⋆\nc = max\nm\n{\nAPm\nc }be the AP of the best expert\nfor concept c. Let γ be the acquisition threshold so\nthat a concept is considered as acquired in the model\nif ∃APm\nc ≥γ ∀m(or AP⋆\nc ≥γ). We deﬁne concept\nexpertise as the percentage of concepts acquired by the\nmodel:\nXγ = |{c s.t. AP ⋆\nc ≥γ}|\n|C| ∀c∈C. (1)\nFinding an optimalγ⋆ value The choice of γis im-\nportant to compute the concept expertise Xγ in Eq. (1).\nThe goal is to obtain an optimal γ⋆ that produces an\nexpertise representative of the generalization power of\nTMs. As a measure of generalization, we use the aver-\nage performance of each model on typical downstream\ntasks: the 10 datasets composing GLUE with their\ndifferent reported metrics [ 36] and SQuAD v1.1/2.0\n[29, 28]. The reported performance is presented in\nTable 5 in Appendix B.\nWe measure the squared Pearson’s correlation coef-\nﬁcient r2 between Xγ and generalization. The obtained\nγ⋆ tells the level of expertise required for a concept to\nbe considered as acquired. We then deﬁne the optimal\nvalue of γas\nγ⋆ = argmax\nγ\n( 1\nNtasks\n∑\ntask\nr2(Xγ,task)\n)\n, (2)\nwith γ ∈[0.5,1). To assess the robustness of γ⋆, the\ntasks are randomly split into reference and test sets,\nwith a ratio 60/40%. Next, we compute γ⋆ for each\nsubset, and we measure the RMSE between the ob-\ntained values on the reference and test set (10 random\nsplits). We treat the sense and homograph concepts\nindependently since they are fundamentally different.\nWe obtain a γ⋆ = 0.997 with a RMSE of 0.0004 for\nconcepts sense and γ⋆ = 0.985 with a RMSE of 0.0028\nfor concepts homograph. The low RMSE shows that\nthe value of γ⋆ generalizes well on disjoint sets of\ntasks. For simplicity, we express the optimal values\nas γ⋆ = {sense: 0.997,homograph: 0.985}, and we\ndeﬁne the combined expertise as\nXγ⋆ = 1\nNconcepts\n∑\ng∈(sense, homograph)\nNgXg\nγ=γ⋆[g]. (3)\nThe high values of γ⋆, together with the obtained r2 =\n0.833, suggest that the number of good and diverse\nexperts in the model is correlated with its generalization\npower (see Table 2 for full results).\n4.3. Results on expert units\nAll of the pre-trained models evaluated are obtained\nfrom the Huggingface Transformers repository [ 37],\nversion 2.1.1. More precisely, we analyze2: BERT-B/L\n[9], RoBERTa-S/L/Lm [20], DistilBERT [31], GPT2-\nS/M/L [27] and XLM [19]. The sentences are tokenized\nusing the default settings in the repository.\n4.3.1 Concept distribution results\nThe distribution of acquired concepts per layer type\nis shown in Fig. 3, for models GPT2-L (I, II) and\nRoBERTa-L (III, IV). A concept is considered acquired\nin a layer ℓif ∃AP(k,ℓ)\nc ≥γ. In this experiment, we\nuse γ = 0.95 for visualization purposes, γ⋆ being too\nrestrictive.\nWe observe that shallow layers in TMs accumulate\nmore concepts than deep layers. Within the Trans-\nformer blocks (see Fig. 2) in GPT2-L, B layers acquire\nabout 3.5x more concepts than A layers and more than\n10x than Aproj and Bproj layers. This suggests that the\nexpanding layers (A and B) in the Transformer block\nare better at learning concepts at a unit level. RoBERTa-\nL produces a similar distribution of concepts, with A\nand B layers accumulating most of the concepts. Com-\npared to GPT2-L, RoBERTa-L has a smaller drop in the\nnumber of concepts from shallow to deep layers. GPT2-\nL is a generative model composed of Transformer de-\ncoders, while RoBERTa is a stack of encoders. Our\nresults show that generative architectures in NLP tend\nto accumulate concepts early in the model. Such an\nobservation was reported by [4] in the image GAN do-\nmain, but to the best of our knowledge we report the\nﬁrst observation of this phenomenon in the NLP do-\nmain. Refer to Appendix A for results on other models.\n4.3.2 Expertise and generalization results\nConcept expertise The concept expertise obtained\nby the considered models is summarized in Table 1.\nRoBERTa-Lm is the model that achieves better com-\nbined expertise Xγ⋆ = 15.36%, followed by RoBERTa-\nL and GPT2-L, both with 12.92%. It is interesting to\nnote that RoBERTa-L doubles the concept expertise of\nBERT only by modifying the training procedure and\nthe data.\n2The corresponding names in the transform-\ners repository are: bert-base/large-cased,\nroberta-base/large/large-mnli,\ndistilbert-base-uncased, gpt2-∅/medium/large\nand xlm-mlm-en-2048.\nModel Model size Xsense\nγ=0.997 Xhomograph\nγ=0.985 Xγ⋆\nBERT-B 110M 1.04% (14) 5.72% (17) 1.89%\nBERT-L 330M 7.51% (101) 5.72% (17) 7.19%\nDistilbert 66M 3.65% (49) 5.72% (17) 4.02%\nGPT2-S 117M 1.79% (24) 1.35% (4) 1.71%\nGPT2-M 345M 3.65% (49) 3.03% (9) 3.53%\nGPT2-L 774M 15.03% (202) 3.37% (10) 12.92%\nRoBERTa-B 125M 1.71% (23) 3.70% (11) 2.07%\nRoBERTa-L 355M 14.66% (197) 5.05% (15) 12.92%\nRoBERTa-Lm 355M 17.86% (240) 4.04% (12) 15.36%\nXLM 667M 9.30% (125) 5.39% (16) 8.59%\nTable 1: Expertise for sense and homograph concepts,\nand combined expertise Xγ⋆ (3). In parenthesis the ac-\ntual number of concepts acquired. RoBERTa-Lm shows\nthe highest Xγ⋆ = 15.36%. The models analyzed ob-\ntain a low homograph expertise Xhomograph\nγ ≤5.72%\ncompared to sense concepts.\nWe observe that sense concepts are acquired better\nthan homograph concepts, as expected given the difﬁ-\ncult disambiguation of the latter. In Fig. 4.(I) we show\nthe histogram of AP⋆\nc for all concepts. Note how many\nhomograph concepts are not being detected, while al-\nmost all sense concepts are detected with AP⋆\nc >0.90.\nBuilding pre-trained models inherently able to disam-\nbiguate homograph concepts at unit level remains a\nchallenge, and we speculate that such knowledge will\nhelp the models generalize even better.\nIn Fig. 4.(II) we show the histogram of expert units\nthat acquire a sense concept at γ = 0.95, for model\nRoBERTa-L. We observe that most of the concepts\nhave less than 50 dedicated expert units (0.022%), with\na median of 7 experts (0.0032%) per concept. Taking\ninto account that 221184 units were analyzed for this\nmodel, we conclude that TMs dedicate very speciﬁc\ngroups of experts to different concepts. The results in\nSec. 6.1 how that these experts are causal.\nGeneralization In Table 2, we show that concept ex-\npertise Xγ⋆ in Eq. (3) is a robust measure of general-\nization of TMs, and can be used as a model evaluation\nmetric rather than ﬁne-tuning on downstream tasks.\nWe report the squared Pearson’s correlation coefﬁcient\nr2 ∈[0,1] of the linear regression between the per-\nformance of TMs on downstream tasks (see Sec. 4.2)\nversus Xγ⋆ . Only the models reporting results are used\nin the correlation analysis. For comparison, we report\nin column (T) the average correlation between perfor-\nFigure 4: (I) Histogram of the AP⋆\nc per concept for\nmodel RoBERTa-L. Most of sense concepts are de-\ntected with AP⋆\nc > 0.90, while homograph concepts\npresent a wide range of AP⋆\nc. The vertical lines corre-\nspond to the valuesγ⋆ found in Sec. 4.2. (II) Histogram\nof the number of expert units that acquire a sense con-\ncept at γ = 0.95. Most of the concepts have less than\n50 experts associated, a very low value (0.022%) com-\npared to the 221184 units analyzed for RoBERTa-L\nshowing that the number of experts per concept in a\nTM is very selective.\nmance on a task with all the other tasks, as well as the\ncorrelation of downstream performance with the model\nsize in column (S). We observe that Xγ⋆ is strongly\ncorrelated with the performance on downstream tasks\n(r2 >0.833), which is higher than the average correla-\ntion among tasks (0.826), the latter requiring evaluation\n(involving ﬁne-tuning) on all the tasks. The correlation\nwith Xγ⋆ is better than the correlation between tasks\nfor 12 of the 16 metrics used.\nWe further see that model size is not a metric that\ngeneralizes well, since it correlates strongly with the\nperfromance on some tasks (e.g. SQuAD), but weakly\nwith other tasks (e.g. QNLI or MRPC). For all of the\ntasks3 where the correlation with model size r2 <0.3,\nwe obtain a correlation with Xγ⋆ of r2 > 0.75, rein-\nforcing the claim that concept expertise Xγ⋆ is a good\nmeasure of generalization.\n3With the exception of STS-(p), which shows 0.0 correlation\nwith model size and a very poor correlation (0.49) with the other\ntasks too.\nModels used Task (S) (T) Xγ⋆\nBERT-B/L\nDistilbert\nRoBERTa-L\nXLM\nGLUE Score 0.361 0.871 0.892\nCoLA 0.572 0.821 0.874\nSST-2 0.554 0.849 0.864\nMRPC (acc) 0.163 0.714 0.753\nMRPC (F1) 0.162 0.916 0.855\nSTS-B (p) 0.000 0.490 0.401\nSTS-B (s) 0.115 0.903 0.840\nQQP (acc) 0.340 0.936 0.944\nQQP (F1) 0.397 0.834 0.859\nMNLI-m 0.603 0.833 0.771\nMNLI-mm 0.452 0.906 0.944\nQNLI 0.286 0.857 0.923\nRTE 0.332 0.861 0.873\nWNLI 0.314 0.751 0.619\nAX 0.426 0.914 0.956\nBERT-B/L\nDistilBERT\nRoBERTa-L\nSQuAD 1.1 (F1) 0.899 0.840 0.850\nSQuAD 2.0 (F1) 0.961 0.752 0.937\nAverage 0.408 0.826 0.833\nTable 2: Squared Pearson’s coefﬁcient(r2) of the linear\nregressions between the reported performance on vari-\nous tasks versus the model size (S), the average versus\nall the other tasks (T) and the combined concept exper-\ntise Xγ⋆ (Eq. (3)). Only those models reporting results\non each dataset are used (ﬁrst column). On average, the\ncorrelation using Xγ⋆ (0.833) is better than the average\ncorrelation among all tasks (0.826), the latter requires\nevaluation (involving ﬁne-tuning) on all the tasks.\n5. Concept overlapΩ\nRecent works in image processing have shown that\nCNN ﬁlters can represent multiple concepts [3, 4, 11].\nBased on this observation, we propose a method to\nexplore co-learning of concepts by TMs in the NLP do-\nmain. We ﬁrst set a threshold τc = percentile99(APm\nc )\nper concept, then we deﬁne sc = {1 if AP m\nc >\nτc else 0}∈ ZM\n2 as our binary concept representation.\nNote that sc has elements with value 1 for the top 1%\nunits classifying the concept. Let the overlap between\nconcepts qand vbe\nΩ(q,v) = ∥sq ∩sv∥1\n∥sq ∪sv∥1\n∈[0,1], (4)\nrepresenting the number of top units that classify both\nconcepts with high APm\nc .\n5.1. Concept overlap results\nIn Table 3, we show that concepts with related senses\npresent a high overlap of top experts, evidencing con-\ncept co-learning. More precisely, we show the 5 con-\ncepts vwith highest overlap Ω(q,v) (deﬁned in Eq. (4))\nQuery deﬁnition Concept Ω(q,v)\nA seat for one\nperson, with a support\nfor the back.\nchair%1:06:00 (query) 1.000\ntable%1:06:01 0.458\nbed%1:06:00 0.361\ncup%1:06:00 0.341\ntable%1:06:01 VS. table%1:14:00 0.336\nﬂoor%1:06:00 0.328\nThe position\nof professor.\nchair%1:04:00 (query) 1.000\nchair%1:04:00 VS. chair%1:06:00 0.575\nfellow%1:18:02 0.371\ndirector%1:18:03 0.297\nadministration%1:04:00 0.243\nmember%1:18:00 0.241\nTable 3: Top-5 concepts in terms of expert overlap\nΩ(q,v) (Eq. (4)) with a query concept for RoBERTa-L.\nThe overlap shows the amount of top experts shared by\ntwo concepts qand v. Even if the word representing the\nconcept is the same (chair), the concepts overlapping\nare different and relate to the actual WordNet deﬁnition\n(click each concept for WordNet link), showing that\nthe model takes the meaning into account. Concepts\nmarked with ‘VS.’ arehomograph concepts.\nwith a query concept q. The query concepts considered\nare represented by the same word (chair) but with dif-\nferent WordNet sense (ﬁrst row of Table 3). Observe\nhow the top-5 concepts are coherent with the deﬁnition\nof the query. The fact that the homograph concept that\ndisambiguates the query appears with high overlap is\nalso interesting. Concept co-learning can be used for\nexplainability: given a concept with unknown deﬁnition\n(e.g. failure cases in a dialogue system), the overlap-\nping concepts can help explain it. See Appendix C for\nmore results including t-SNE projections [34] of sc.\n6. Inducing concepts in pre-trained Language\nModels\nLanguage Models (LMs) are generative models\nable to generate text consistent with linguistic rules.\nMore formally, LMs learn the probability of a gen-\nerated sentence x [5] as p(x) = p(x1,...,x T) =∏T\nk=1 p(xk|x<k).\nA conditional generative model maximizes the joint\ndistribution p(x,y) = p(y|x)p(x), where xis the gen-\nerated sentence and y is a latent conditional variable\n(i.e. a speciﬁc concept in x). As proposed by [14], this\nequation can be interpreted as a product of experts. The\nsame interpretation was adopted by [23] for conditioned\nimage generation. We adapt Hinton’s and Nguyen’s in-\nterpretation to the case of conditioned text generation,\nwhere p(y|x) is an expert that determines the condition\nfor generation, and p(x) is an expert ensuring that the\ngenerated sequence lies within the manifold of sentence\ndistributions. Typically we do not sample jointly xand\ny, we rather deﬁne a condition y= cbeforehand (e.g.\nthe concept) and sample x. Thus one can write:\np(x|y= c) ∝p(y= c|x)p(x). (5)\nAs opposed to [ 23] that models p(y = c|x) with an\nexternal network, we hypothesize that the condition\nexpert p(y= c|x) already exists within the same model,\nand that the model is able to maximize p(x|y= c) by\ntrusting its internal condition expert. Such intuition\nis based on recent ﬁndings in neuroscience [ 22] that\nshow that the human brain increasingly trusts selective\ngroups of neurons as it learns. If we can identify the\nparts of the model that contribute to the condition expert\np(y= c|x), we could control the amount of concept c\nin the generated sentences. The quality of the experts\nfor a given concept will dictate the extent to which such\nconcept can be controlled during generation.\nAs explained in Sec. 4.1, APm\nc explains how well\nunit mis able to classify concept c. We consider those\nunits with high APm\nc as internal condition experts for\nconcept c, each accumulating evidence in p(y= c|x).\nTo control p(y= c|x) we force the top-K experts to be\nactive. The larger K, the more concept cwill be present\nin the output, provided that the expert has learnt the\nconcept. Too large K will result in illegible sentences,\nsince p(y= c|x) will dominate p(x) in Eq. (5).\nThe proposed forcing strategy is inspired by [ 4],\nwhere the authors compute the mean ﬁlter response\nconditioned to the presence of some object in a GAN\noutput image. We adapt their approach to the LM case:\n(1) we cannot quantify the presence of c, however the\noutput and the input of LMs are tightly related given\nthe sequential decoding, which allows the forcing value\nto be computed as the median active value when con-\ncept cis present in the input (not the output); (2) our\nquantiﬁcation strategy is by AP given a binary dataset,\nwhile [4] consider CNN ﬁlter responses as segmenta-\ntion masks to compute IOU with a labeled image.\nThe results in Sec. 6.1 conﬁrm our hypothesis that\nconditional experts exist in the model, and that the\nmodel leverages them to condition generation. Further\nK forced Once upon a time + Generated induced for concept bird%1:05:00 (warm-blooded egg-laying vertebrates)\n0 (0%) , I had a friend who used to teach high school English and he was like, \"Oh, all you have to do is just get out there\n40 (0.009%) , many of these treasures were worth hundreds of thousands of dollars.\\n But this isn’t the ﬁrst time that a horse has been\n60 (0.015%) , through a freak occurrence, an invasion of house sparrows, which so often reduces the black-browed this nation recreats through\n80 (0.019%) , our own ancestors rode about on chicken-like air wings.\\n But this wonder of the air has no such wings.\\n Taking down\n200 (0.048%) of year, birds chase each and watching. ﬂot racing form, bird, bird bird bird bird bird bird bird bird bird bird bird, Bird bird\nOnce upon a time + Generated induced for concept lead%1:07:02 (an advantage held by a competitor in a race)\n50 (0.012%) the left-hander would always start at the front in the ﬁrst two instances, but when Mauricio Gaponi rose to the podium,\nOnce upon a time + Generated induced for concept lead%1:27:00 (a soft heavy toxic malleable metallic element)\n100 (0.024%) a crust layer was applied to a partially fortiﬁed nickel base, thereby causing to zinc- and copper- ground element cob. The\noccurrence of those metal and chrome\nTable 4: Examples of generated sentences using GPT2-L with initial context Once upon a time, sorted by the\nnumber of top experts for different WordNet concepts. In parenthesis the percentage of experts forced out of the\ntotal 414720 units analyzed. For concept bird%1:05:00, the presence of the concept increases as we force more\nexperts, empirically proving the impact of expert forcing on p(y= c|x) in (5). The percentage of experts required\nis extremely low, saturating at 200 experts (0.048%) in this example, wherep(y= c|x) already dominates p(x) in\n(5). We also show generated sentences for concepts lead%1:27:00 and lead%1:07:02, showing the model’s ability\nto capture the meaning of the concept.\nexploration in the generative ﬁeld, although extremely\ninteresting, is out of scope for this work.\n6.1. Results on conditioned text generation\nExpert units can be used beyond model evaluation.\nIn this experiment, we use experts units for text gen-\neration. The objective is threefold: (1) to demonstrate\nthat ranking units by APm\nc is a suitable strategy to ﬁnd\nthe experts for concept c; (2) to prove expert units are\ncausal in the setting of LM, empirically showing that\nwe can control p(y = c|x) in Eq. (5); and (3) to show\nthat LMs can be conditioned without training or using\nadditional parameters. Table 4 illustrates the generation\nof sentences using GPT2-L while forcing the top-K ex-\nperts for WordNet concept bird%1:05:00, as explained\nin Sec. 6. The decoding strategy is by nucleus ﬁltering\nwith p = 0.9 [15]. We observe how the presence of\nthe concept gradually increases as we increase K, and\nthat it saturates at about 200 experts forced. This result\nempirically explains Eq. (5), showing that K controls\np(y = c|x) until saturation, when the effect of p(x)\n(generate plausible sentences) is not evident anymore.\nThe number of experts forced is extremely low, satu-\nrating at 200 experts (or 0.048% of the 414720 units\nanalyzed for GPT2-L), showing how LMs dedicate very\nselective units to speciﬁc concepts. This result draws\na parallelism between the behavior of TMs and that of\nhuman brains [22]. Extended results in Appendix D.\n7. Limitations of the method\nOn the data We have proposed a data-driven ap-\nproach, thus being limited to the presented data. The\nproposed dataset in Sec. 3 is weakly annotated, and\nthere are inconsistencies inherent in the source OneSec\ndataset. The more diverse and accurate the concept\ndataset, the better it will help evidence the generaliza-\ntion power of TMs.\nOn individual expert unitsWe have shown that in-\ndividual expert units can be interpreted as experts for\nspeciﬁc concepts, specially in the sense category. It\nis possible, but not yet explored, that more complex\nconcepts such as homograph, require a more complex\nexpert such as a set of units.\nOn the compute requirements Finding experts in\nTMs (Sec. 4.1) is an exhaustive task that implies some\nmemory and compute requirements. A forward pass\nover the dataset is required and is the most demanding\nstep. The proposed dataset in Sec. 3 consists of 1641\nconcepts with an average of 1.5Ksentences each, thus\n∼2.5M sentences. According to the benchmark in the\nTransformers repository, the average GPU inference\ntime for BERT-B for sentences of 128 tokens is 9ms,\nwhich translates into 6.15 hours for the whole dataset.\nOur method can be parallelized per concept, thus with\n8 GPUs the total time is reduced to 45 minutes. The\ncomputed APm\nc ∀m,c requires, in the GPT2-L case,\n414720 ×1641 ﬂoats ≈2.5GB plus overhead, such\nas unit naming. For comparison, a single evaluation\nof BERT-B on SQuAD v1.1 takes 24min. But several\nevaluations are required for hyper-parameter tuning\nand statistical signiﬁcance. Summing up, evaluating on\nSQuAD v1.1/2.0 plus all GLUE tasks is more demand-\ning than our proposed evaluation.\n8. Conclusions\nWe have deﬁned expert units in the context of TMs\nand proposed a method to identify and rank them with\nrespect to a speciﬁc concept. Our results show that\ngeneralization of TMs is related to the presence of both\ndiverse and high-performant experts. We also have\nshown how the top experts for different concepts can\nbe used to analyze concept co-learning, and how this\nco-learning can be used for concept explainability. In\naddition, we have proposed a method to condition the\noutput of language models by forcing its top experts\nidentiﬁed for a concept. Such conditioning is applied to\npre-trained models, without requiring re-training or us-\ning additional parameters, leveraging the actual model\nknowledge. A parallelism between the presence of ex-\nperts units in TMs and the presence of specialized ﬁlters\nin image processing CNNs/GANs has been suggested,\nas well as with the presence of specialized structures\nin the human brain. Our ﬁndings open new avenues\nof research, such as conditioning language models on\ntheir own knowledge or improving training leveraging\nthe presence of experts.\nReferences\n[1] J. Adebayo, J. Gilmer, M. Muelly, I. Goodfellow,\nM. Hardt, and B. Kim. Sanity checks for saliency\nmaps. NeurIPS, 2018.\n[2] Y . Adi, E. Kermany, Y . Belinkov, O. Lavi, and\nY . Goldberg. Fine-grained analysis of sen-\ntence embeddings using auxiliary prediction tasks.\nICLR, 2017.\n[3] D. Bau, B. Zhou, A. Khosla, A. Oliva, and A. Tor-\nralba. Network dissection: Quantifying inter-\npretability of deep visual representations. CVPR,\n2017.\n[4] D. Bau, J.-Y . Zhu, H. Strobelt, Z. Bolei, J. B.\nTenenbaum, W. T. Freeman, and A. Torralba. Gan\ndissection: Visualizing and understanding genera-\ntive adversarial networks. ICLR, 2019.\n[5] Y . Bengio, R. Ducharme, P. Vincent, and C. Jan-\nvin. A neural probabilistic language model. Jour-\nnal of Machine Learning Research, pages 1137–\n1155, 2003.\n[6] M. Chen, Q. Tang, S. Wiseman, and K. Gimpel. A\nmulti-task approach for disentangling syntax and\nsemantics in sentence representations. NAACL,\n2019.\n[7] K. Clark, U. Khandelwal, O. Levy, and C. D. Man-\nning. What does bert look at? an analysis of bert’s\nattention. ACL, 2019.\n[8] A. Conneau, G. Kruszewski, G. Lample, L. Bar-\nrault, and M. Baroni. What you can cram into a\nsingle vector: Probing sentence embeddings for\nlinguistic properties. ACL, 2018.\n[9] J. Devlin, M.-W. Chang, K. Lee, and\nK. Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language under-\nstanding. NAACL, 2018.\n[10] A.-K. Dombrowski, M. Alber, C. J. Anders,\nM. Ackermann, K.-R. Müller, and P. Kessel. Ex-\nplanations can be manipulated and geometry is to\nblame. NeurIPS, 2019.\n[11] R. Fong and A. Vedaldi. Net2vec: Quantifying\nand explaining how concepts are encoded by ﬁl-\nters in deep neural networks. CVPR, 2018.\n[12] R. Ghaeini, X. Fern, and P. Tadepalli. Interpreting\nrecurrent and attention-based neural models: a\ncase study on natural language inference.EMNLP,\n2018.\n[13] N. Hill and W. Schneider. Brain changes in the de-\nvelopment of expertise: Neuroanatomical and neu-\nrophysiological evidence about skill-based adap-\ntations. The Cambridge Handbook of Expertise\nand Expert Performance., 2006.\n[14] G. E. Hinton. Products of experts. ICANN, 1999.\n[15] A. Holtzman, J. Buys, M. Forbes, and Y . Choi.\nThe curious case of neural text degeneration.\narXiv preprint arXiv:1904.09751, 2019.\n[16] J. Howard and S. Ruder. Universal language\nmodel ﬁne-tuning for text classiﬁcation. ACL,\n2018.\n[17] Z. Hu, Z. Yang, X. Liang, R. Salakhutdinov, and\nE. P. Xing. Toward controlled generation of text.\nICML, 2017.\n[18] N. S. Keskar, B. McCann, L. Varshney, C. Xiong,\nand R. Socher. CTRL - A Conditional Trans-\nformer Language Model for Controllable Genera-\ntion. arXiv preprint, 2019.\n[19] G. Lample and A. Conneau. Cross-lingual\nlanguage model pretraining. arXiv preprint\narXiv:1901.07291, 2019.\n[20] N. F. Liu, M. Gardner, Y . Belinkov, M. E. Peters,\nand N. A. Smith. Linguistic knowledge and trans-\nferability of contextual representations. NAACL,\n2019.\n[21] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen,\nO. Levy, M. Lewis, L. Zettlemoyer, and V . Stoy-\nanov. Roberta: A robustly optimized bert pretrain-\ning approach. arXiv preprint arXiv:1907.11692,\n2019.\n[22] F. Najaﬁ, G. F. Elsayed, R. Cao, E. Pnevmatikakis,\nP. E. Latham, J. P. Cunningham, and A. K. Church-\nland. Excitatory and inhibitory subnetworks\nare equally selective during decision-making and\nemerge simultaneously during learning. Neuron,\n2019.\n[23] A. Nguyen, J. Yosinski, Y . Bengio, A. Dosovit-\nskiy, and J. Clune. Plug & play generative net-\nworks: Conditional iterative generation of images\nin latent space. CVPR, 2017.\n[24] T. Niven and H.-Y . Kao. Probing neural network\ncomprehension of natural language arguments.\nACL, 2019.\n[25] Nvidia. Megatron-lm.\nhttps://github.com/nvidia/megatron-lm, 2019.\n[26] Princeton University. Wordnet: A lexical database\nfor english. https://wordnet.princeton.edu.\n[27] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei,\nand I. Sutskever. Language models are unsuper-\nvised multitask learners. arXiv preprint, 2019.\n[28] P. Rajpurkar, R. Jia, and P. Liang. Know what you\ndon’t know: Unanswerable questions for squad.\nACL, 2018.\n[29] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang.\nSquad: 100,000+ questions for machine compre-\nhension of text. EMNLP, 2016.\n[30] A. Romanov, A. Rumshisky, A. Rogers, and\nD. Donahue. Adversarial decomposition of text\nrepresentation. NAACL, 2019.\n[31] V . Sanh, L. Debut, J. Chaumond, and T. Wolf. Dis-\ntilbert, a distilled version of bert: smaller, faster,\ncheaper and lighter. arXiv preprint, 2019.\n[32] B. Scarlini, T. Pasini, and R. Navigli. Just\n“onesec” for producing multilingual sense-\nannotated data. ACL, 2019.\n[33] I. Tenney, P. Xia, B. Chen, A. Wang, A. Poliak,\nR. T. McCoy, N. Kim, B. V . Durme, S. R. Bow-\nman, D. Das, and E. Pavlick. What do you learn\nfrom context? probing for sentence structure in\ncontextualized word representations. ICLR, 2019.\n[34] L. van der Maaten and G. Hinton. Visualizing\ndata using t-SNE. Journal of Machine Learning\nResearch, 2008.\n[35] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkor-\neit, L. Jones, A. N. Gomez, Łukasz Kaiser, and\nI. Polosukhin. Attention is all you need. NIPS,\n2017.\n[36] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy,\nand S. R. Bowman. GLUE: A multi-task bench-\nmark and analysis platform for natural language\nunderstanding. ICLR, 2019.\n[37] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. De-\nlangue, A. Moi, P. Cistac, T. Rault, R. Louf,\nM. Funtowicz, and J. Brew. Huggingface’s trans-\nformers: State-of-the-art natural language pro-\ncessing. ArXiv preprint, abs/1910.03771, 2019.\n[38] Z. Yang, Z. Dai, Y . Yang, J. Carbonell,\nR. Salakhutdinov, and Q. V . Le. Xlnet: Gen-\neralized autoregressive pretraining for language\nunderstanding. NeurIPS, 2019.\nAppendices\nA. Concept distribution for all models considered\nFigure 5: Concept distribution per layer at γ = 0.95 for model DistilBERT.\nFigure 6: Concept distribution per layer at γ = 0.95 for model BERT-B.\nFigure 7: Concept distribution per layer at γ = 0.95 for model BERT-L.\nFigure 8: Concept distribution per layer at γ = 0.95 for model GPT2-S.\nFigure 9: Concept distribution per layer at γ = 0.95 for model GPT2-M.\nFigure 10: Concept distribution per layer at γ = 0.95 for model RoBERTa-Lm.\nFigure 11: Concept distribution per layer at γ = 0.95 for model XLM.\nB. Performance of the considered models on downstream tasks\nModel BERT-B BERT-L Distilbert RoBERTa-L XLM\nModel size 110M 330M 66M 355M 667M\nGLUE Score 78.3 80.5 76.8 88.5 83.1\nCoLA 52.1 60.5 49.1 67.8 62.9\nSST-2 93.5 94.9 92.7 96.7 95.6\nMRPC (acc) 88.9 89.3 90.2 92.3 90.7\nMRPC (F1) 84.8 85.4 89.8 87.1\nSTS-B (p) 87.1 87.6 90.7 92.2 88.8\nSTS-B (s) 85.8 86.5 - 91.9 88.2\nQQP (acc) 71.2 72.1 - 74.3 73.2\nQQP (F1) 89.2 89.3 89.2 90.2 89.8\nMNLI-m 84.6 86.7 81.8 90.8 89.1\nMNLI-mm 83.4 85.9 - 90.2 88.5\nQNLI 90.5 92.7 90.2 98.9 94\nRTE 66.4 70.1 62.9 88.2 76\nWNLI 65.1 65.1 44.4 89 71.9\nAX 34.2 39.6 - 48.7 44.7\nSQuAD 1.1 (F1) 88.5 91.5 86.9 94.6 -\nSQuAD 2.0 (F1) 76.3 85.81 - 89.8 -\nTable 5: Performance of the considered models on various downstream tasks, as reported in the reference papers.\nNot all models report performance on all tasks.\nC. Concept co-learning extended results\nConcept Type Overlap WordNet deﬁnition\nchair%1:06:00 sense 1.000 a seat for one person, with a support for the back\ntable%1:06:01 sense 0.458 a piece of furniture having a smooth ﬂat top that is usually supported by one or\nmore vertical legs\nbed%1:06:00 sense 0.361 a piece of furniture that provides a place to sleep\ncup%1:06:00 sense 0.341 a small open container usually used for drinking; usually has a handle\ntable%1:06:01 VS. table%1:14:00 homograph 0.336 a piece of furniture having a smooth ﬂat top that is usually supported by one or\nmore vertical legs VS. a set of data arranged in rows and columns\nﬂoor%1:06:00 sense 0.328 the inside lower horizontal surface (as of a room, hallway, tent, or other structure)\nchair%1:04:00 sense 1.000 the position of professor\nchair%1:04:00 VS. chair%1:06:00 homograph 0.575 the position of professor VS. a seat for one person, with a support for the back\nfellow%1:18:02 sense 0.371 a friend who is frequently in the company of another\ndirector%1:18:03 sense 0.297 member of a board of directors\nadministration%1:04:00 sense 0.243 a method of tending to or managing the affairs of a some group of people (espe-\ncially the group’s business affairs)\nmember%1:18:00 sense 0.241 one of the persons who compose a social group (especially individuals who have\njoined and participate in a group organization)\nsuspension%1:28:00 sense 1.000 a time interval during which there is a temporary cessation of something\nsuspension%1:28:00 VS. suspension%1:27:00 homograph 0.522 a time interval during which there is a temporary cessation of something VS. a\nmixture in which ﬁne particles are suspended in a ﬂuid where they are supported\nby buoyancy\nrecovery%1:11:00 sense 0.398 return to an original state\nseason%1:28:02 sense 0.396 a period of the year marked by special events or activities in some ﬁeld\nprospect%1:26:00 sense 0.387 the possibility of future success\nattempt%1:04:00 sense 0.380 earnest and conscientious activity intended to do or accomplish something\nsuspension%1:27:00 sense 1.000 a mixture in which ﬁne particles are suspended in a ﬂuid where they are supported\nby buoyancy\nsolution%1:27:00 sense 0.492 a homogeneous mixture of two or more substances; frequently (but not necessar-\nily) a liquid solution\ndeposit%1:19:00 sense 0.438 the phenomenon of sediment or gravel accumulating\nmaterial%1:27:00 sense 0.432 the tangible substance that goes into the makeup of a physical object\npowder%1:27:00 sense 0.415 a solid substance in the form of tiny loose particles; a solid that has been pulver-\nized\ncrystal%1:27:00 sense 0.413 a solid formed by the solidiﬁcation of a chemical and having a highly regular\natomic structure\nphone%1:06:00 sense 1.000 electronic equipment that converts sound into electrical signals that can be trans-\nmitted over distances and then converts received signals back into sounds\nsubscriber%1:18:01 sense 0.423 someone who contracts to receive and pay for a service or a certain number of\nissues of a publication\ntalk%1:10:00 sense 0.344 an exchange of ideas via conversation\nneed%1:17:00 VS. need%1:26:00 homograph 0.328 anything that is necessary but lacking VS. a condition requiring relief\nuser%1:18:00 sense 0.321 a person who makes use of a thing; someone who uses or employs something\nmessage%1:10:01 sense 0.320 a communication (usually brief) that is written or spoken or signaled\nphone%1:10:00 sense 1.000 (phonetics) an individual sound unit of speech without concern as to whether or\nnot it is a phoneme of some language\nphone%1:10:00 VS. phone%1:06:00 homograph 0.412 (phonetics) an individual sound unit of speech without concern as to whether or\nnot it is a phoneme of some language VS. electronic equipment that converts\nsound into electrical signals that can be transmitted over distances and then con-\nverts received signals back into sounds\nletter%1:10:01 sense 0.362 the conventional characters of the alphabet used to represent speech\namerican%1:10:00 VS. american%1:18:00 homograph 0.335 the English language as used in the United States VS. a native or inhabitant of the\nUnited States\nword%1:10:00 sense 0.330 a unit of language that native speakers can identify\nform%1:10:00 sense 0.297 the phonological or orthographic sound or appearance of a word that can be used\nto describe or identify something\nTable 6: Top-5 concepts co-learnt with a querysense concept (represented by an overlap of 1.0) for model RoBERTa-\nL. Observe how the concepts that maximally overlap with each query are strongly related with the deﬁnition of the\nquery, even when the word representing the query is the same.\nConcept Type Overlap WordNet deﬁnition\nmarket%1:04:00 sense 1.000 the world of commercial activity where goods and services are bought and\nsold\neconomy%1:14:00 sense 0.388 the system of production and distribution and consumption\nmarket%1:14:00 sense 0.353 the customers for a particular product or service\ncapital%1:21:01 sense 0.349 assets available for use in the production of further assets\nlabor%1:14:00 sense 0.304 a social class comprising those who do manual labor or work for wages\nwealth%1:26:00 sense 0.267 the state of being rich and afﬂuent; having a plentiful supply of material\ngoods and money\nmarket%1:14:00 sense 1.000 the customers for a particular product or service\nmarket%1:04:00 sense 0.353 the world of commercial activity where goods and services are bought and\nsold\nindustry%1:14:00 sense 0.221 the people or companies engaged in a particular kind of commercial enter-\nprise\nbanking%1:04:00 sense 0.211 transacting business with a bank; depositing or withdrawing funds or re-\nquesting a loan etc.\nmarket%1:14:00 VS. market%1:04:00 homograph 0.208 the customers for a particular product or service VS. the world of commer-\ncial activity where goods and services are bought and sold\nleader%1:06:00 sense 0.198 a featured article of merchandise sold at a loss in order to draw customers\nTable 7: Example of sense concepts that have similar meanings (model RoBERTa-L). Both query concepts are\nrepresented by the word market, but they overlap strongly with the other query concept. Actually, the homograph\nconcept market%1:04:00 VS. market%1:14:00 achieves a low AP⋆\nc = 0.523.\nFigure 12: t-SNE projection of the concept representations sc proposed in Sec. 5. Zoom-in on concepts\nchair%1:04:00 and chair%1:06:00, whose meaning can easily be explained by their neighbors. The t-SNE\nprojection is an alternative view of the nearest neighbor results shown in Table 6. In orange homograph concepts.\nFigure 13: t-SNE projection of the concept representations sc proposed in Sec. 5. Zoom-in on con-\nceptsright%1:07:00 and right%1:15:00, whose meaning can easily be explained by their neighbors. In orange\nhomograph concepts.\nD. Conditioned generation extended results\nK forced WordNet concept AP ⋆\nc Context + Generated (conditioned to concept)\n60 elevator%1:06:00 0.9999 In a shocking ﬁnding, scientist discovered a herd of unicorns living in a remote, previously unex-\nplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that\nthe unicorns spoke perfect English. The two scientists were unable to solve a problem in their\nresearch when they started a great deal of unusual levitation and deceleration, which blew them\nup a few hundred feet and dropped them back to the ground.\n60 smoke%1:19:00 0.9999 In a shocking ﬁnding, scientist discovered a herd of unicorns living in a remote, previously unex-\nplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that\nthe unicorns spoke perfect English. The experiment in Alto Allegro was conducted in the sloping\nMan-of-War Mountain. This was a truly historic event! Researchers had to use three fresh, fresh\ninhalations to extract all of the smoke. The study has been approved by the Spanish government\n60 gold%1:21:00 0.9996 In a shocking ﬁnding, scientist discovered a herd of unicorns living in a remote, previously unex-\nplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that\nthe unicorns spoke perfect English. Our researcher found the magical ’Slab Silver’, which is one\nof the most beautiful forms of gold we have ever had our eyes on. It’s a beautiful shimmer that’s\ntruly exceptional,\" said Peter Kieper, the Executive Chairman of Canadian Gold Corporation in\nThe Vancouver Sun.\n60 frustration%1:12:00 0.9984 In a shocking ﬁnding, scientist discovered a herd of unicorns living in a remote, previously unex-\nplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that\nthe unicorns spoke perfect English. Even though we had spent a lot of time just to ﬁnd the path that\ncould lead to the species, we did not have success,\" has an Indian scientist, taking measurements\nfrom a lone unicorn on the walls of a remote mountain, wearing brightly red patches of clothing.\n60 retirement%1:26:00 0.9981 In a shocking ﬁnding, scientist discovered a herd of unicorns living in a remote, previously unex-\nplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that\nthe unicorns spoke perfect English. The longest lived of the bunch, 45 year old Count of Ivory\n(Count Monte) was found to be suffering from a brain tumour. Y et the Tibetan leviathan didn’t\nreceive the huge retirement pension provided by the CIA. He died peacefully at the age of 75 in\nApril in a spa\nTable 8: Extended results on successful conditioned generation. All the concepts shown have an AP⋆\nc ≥γ⋆. We\nborrow the context from the OpenAI GPT2 work [27]\n.\nK forced WordNet concept AP ⋆\nc Context + Generated (conditioned to concept)\n40 work%1:06:00 0.8508 Once upon a time, in an ancient palace at the heart of history, a was on. Magic, symbolism,\ndecadence and tragedy. Everything had come up, balancing with the architect’s.\\n\\nA madman’s\nmuseum. A thing of daub. Now, it’s hide and clay and mud and dirt\n70 work%1:06:00 0.8508 Once upon a time-spotted bench). Now I met my tools„„, work, work.<|endoftext|>Raw Products\nKretzer Top Tube Process\\n\\nPROTECT SHOP:\\n\\nDay 1: Screening on the work bench.\\n\\n\\n1.\nBeaksiewerk procedure - drill build\n100 work%1:06:00 0.8508 Once upon a time of WARD will i means to out out any.\\n:,. So! Work- WORK WORK WORK\nWORK W WORK WORK WORK WORK\\n WORK WORK\\n work work work\\n work\\n work work\nwork work work work work work work work work work. work work work work work work work work\nwork\n200 work%1:06:00 0.8508 Once upon a time of that done by... uses of such done object\\n\\n of.\\n 28, 37\\n WORK WORK\nWORK.... work article... delivery... ( bench work\\n call really work\\n out\\n work work work 40 work\nproduct if 5 40 work work 50\\n work work 35 means 34 twenty block 29 individual\nTable 9: Extended results on unsuccessful conditioned generation. The concept has AP⋆\nc ≪γ⋆, and we observe\nhow the model struggles to produce legible sentences.\n.\nE. Concept list\nThe sense concepts considered are listed in Tables 10, 11, 12, 13 and homograph concepts in Table 14. Concepts\nare sorted by the AP⋆\nc obtained by GPT2-L, to illustrate how concepts are acquired.\nNote that the meaning of the concept is important. For example, concept one%1:23:00 (the smallest whole\nnumber or a numeral representing this number, e.g. he has the one but will need a two and three to go with it\"; \"they\nhad lunch at one\") achieves a AP⋆\nc = 0.9885, while concept one%1:09:00 (a single person or thing, e.g. \"he is the\nbest one\"; \"this is the one I ordered\") only achieves AP⋆\nc = 0.8779.\nDetails on the annotationsEach sentence in the OneSec dataset [32] is annotated as in the following example:\n<instance docsrc=\"Indigenous architecture\" id=\"shelter.00002\">\n<answer instance=\"shelter.00002\" senseid=\"shelter%1:06:00::\" />\n<context>\nTypes There are three traditional types of igloos ,\nall of different sizes and used for different purposes.\nThe smallest were constructed as temporary\n<head>shelters</head>\n, usually only used for one or two nights .\n</context>\n</instance>\nThe senseid label is the one of the marked word (shelters in this example, between <head> and </head>).\nWe use the senseid as follows. The part before the % is called lemma, while the remaining numbers uniquely\nidentify the concept in WordNet. We parse all the sentences for a givensenseid to create the positive sentences\nof each concept, only keeping those senseid with more than 100 sentences. As explained in Sec. 3, the negative\nsentences of sense concepts are randomly selected from all the senseid with different lemma than the positive\nones, while the negative sentences of the homograph concepts are those with different senseid but same lemma.\nAP⋆\nc sense concept AP⋆\nc sense concept AP⋆\nc sense concept AP⋆\nc sense concept\n1.0000 inspiration%1:06:00 0.9992 sin%1:07:00 0.9983 cab%1:06:02 0.9976 memory%1:09:02\n1.0000 console%1:06:03 0.9991 sum%1:09:01 0.9983 text%1:10:03 0.9976 exception%1:09:00\n1.0000 connecticut%1:17:00 0.9991 companion%1:18:00 0.9983 crystal%1:27:00 0.9975 doctor%1:18:00\n1.0000 thanks%1:10:00 0.9990 worker%1:18:00 0.9982 backing%1:04:00 0.9975 plot%1:15:00\n1.0000 library%1:14:00 0.9990 mouth%1:08:01 0.9982 establishment%1:04:00 0.9975 estimate%1:09:00\n1.0000 ownership%1:21:00 0.9990 dollar%1:23:00 0.9982 eruption%1:11:00 0.9975 town%1:15:00\n0.9999 elevator%1:06:00 0.9990 journalist%1:18:00 0.9982 council%1:14:02 0.9975 shelter%1:06:00\n0.9999 reply%1:10:01 0.9990 illusion%1:09:01 0.9982 integrity%1:26:00 0.9975 profession%1:14:00\n0.9999 smell%1:09:02 0.9989 sign%1:10:05 0.9982 ﬁgure%1:10:00 0.9975 release%1:06:00\n0.9999 truck%1:06:00 0.9989 contract%1:10:00 0.9982 score%1:09:00 0.9975 fee%1:21:00\n0.9999 hangover%1:26:00 0.9989 concentration%1:07:02 0.9982 barrier%1:06:00 0.9975 message%1:10:00\n0.9999 skin%1:08:00 0.9989 behalf%1:04:00 0.9982 understanding%1:09:01 0.9975 defect%1:26:01\n0.9999 smoke%1:19:00 0.9989 frequency%1:28:00 0.9981 barrel%1:06:01 0.9975 recommendation%1:10:00\n0.9999 lobby%1:06:00 0.9989 ﬁt%1:26:01 0.9981 competition%1:24:01 0.9975 migration%1:04:00\n0.9998 log%1:27:01 0.9988 plane%1:06:01 0.9981 retirement%1:26:00 0.9974 ﬁshing%1:04:00\n0.9998 crack%1:17:00 0.9988 category%1:14:00 0.9981 prospect%1:26:00 0.9974 presence%1:26:00\n0.9998 exercise%1:04:00 0.9988 drink%1:13:04 0.9981 3d%1:10:00 0.9974 suit%1:06:00\n0.9998 inspiration%1:09:02 0.9988 injection%1:27:00 0.9981 attraction%1:19:00 0.9974 ﬁle%1:10:00\n0.9998 connecticut%1:15:00 0.9988 stranger%1:18:00 0.9981 envoy%1:18:00 0.9974 regulator%1:06:00\n0.9997 confusion%1:26:01 0.9988 liquor%1:13:00 0.9981 card%1:10:01 0.9974 evening%1:28:00\n0.9997 mess%1:26:00 0.9988 ofﬁce%1:06:00 0.9981 pocket%1:06:00 0.9974 practice%1:04:00\n0.9997 invasion%1:04:00 0.9988 personality%1:07:00 0.9981 suspension%1:27:00 0.9974 screen%1:06:06\n0.9997 subscriber%1:18:02 0.9988 chamber%1:06:00 0.9980 sort%1:09:00 0.9974 hall%1:06:05\n0.9997 oil%1:27:00 0.9987 ﬂood%1:19:00 0.9980 liquor%1:27:00 0.9973 contribution%1:04:02\n0.9997 oil%1:06:01 0.9987 emission%1:04:00 0.9980 vote%1:04:00 0.9973 friend%1:18:03\n0.9997 owner%1:18:00 0.9987 spring%1:28:00 0.9980 bonus%1:09:00 0.9973 average%1:24:00\n0.9997 gold%1:21:00 0.9987 addiction%1:26:00 0.9980 gathering%1:14:00 0.9973 pair%1:14:01\n0.9996 session%1:10:00 0.9987 substance%1:03:00 0.9980 loan%1:21:00 0.9973 counterpart%1:09:00\n0.9996 hell%1:15:01 0.9987 brazil%1:15:00 0.9979 tenant%1:18:00 0.9973 mg%1:23:00\n0.9996 ministry%1:06:00 0.9987 backing%1:06:00 0.9979 attitude%1:09:00 0.9973 vicar%1:18:02\n0.9995 poll%1:09:00 0.9987 bourbon%1:18:01 0.9979 height%1:07:00 0.9972 cloud%1:19:01\n0.9995 bird%1:05:00 0.9986 move%1:04:00 0.9979 identity%1:09:00 0.9972 jury%1:14:00\n0.9995 receptor%1:08:01 0.9986 sound%1:07:00 0.9978 copy%1:10:00 0.9972 deal%1:04:02\n0.9995 baby%1:18:00 0.9986 equity%1:21:01 0.9978 negotiation%1:04:00 0.9972 tissue%1:08:00\n0.9994 expression%1:07:00 0.9985 phone%1:06:00 0.9978 ritual%1:04:00 0.9972 arm%1:08:00\n0.9994 participant%1:18:00 0.9985 maintenance%1:04:00 0.9978 couple%1:14:00 0.9972 sense%1:09:05\n0.9994 indicator%1:10:00 0.9985 charge%1:04:01 0.9978 revolution%1:04:00 0.9972 contrast%1:24:00\n0.9994 budget%1:21:02 0.9985 farmer%1:18:00 0.9978 circle%1:25:00 0.9971 transmission%1:10:00\n0.9994 misconduct%1:04:01 0.9985 tower%1:06:00 0.9978 corner%1:15:02 0.9971 light%1:19:00\n0.9994 dog%1:05:00 0.9984 frustration%1:12:00 0.9978 operator%1:24:00 0.9971 scene%1:15:00\n0.9994 intensity%1:07:00 0.9984 depth%1:07:00 0.9978 negotiation%1:10:00 0.9971 behalf%1:07:00\n0.9993 boost%1:07:00 0.9984 prayer%1:04:00 0.9978 brain%1:08:00 0.9971 storm%1:19:00\n0.9993 execution%1:04:02 0.9984 surprise%1:12:00 0.9977 israel%1:15:00 0.9971 cash%1:21:00\n0.9993 message%1:10:01 0.9984 nightmare%1:26:00 0.9977 reaction%1:22:00 0.9971 portfolio%1:14:00\n0.9993 colon%1:08:00 0.9984 colleague%1:18:00 0.9977 overhaul%1:04:00 0.9971 attention%1:09:00\n0.9993 pursuit%1:04:00 0.9984 drinking%1:04:01 0.9977 mind%1:09:01 0.9971 mandate%1:10:00\n0.9993 speculation%1:10:03 0.9984 trafﬁc%1:14:00 0.9976 ceiling%1:06:00 0.9971 sentence%1:10:00\n0.9992 vicar%1:18:00 0.9983 wheel%1:06:00 0.9976 code%1:10:01 0.9971 trainer%1:18:00\n0.9992 skin%1:06:01 0.9983 cup%1:06:00 0.9976 identity%1:07:00 0.9971 discussion%1:10:02\n0.9992 campaign%1:11:00 0.9983 winner%1:18:00 0.9976 representative%1:18:02 0.9970 round%1:06:01\n0.9970 paper%1:27:00 0.9963 check%1:21:00 0.9956 sport%1:04:00 0.9950 subject%1:10:00\n0.9970 volume%1:23:00 0.9963 sin%1:04:00 0.9956 satisfaction%1:12:00 0.9950 retirement%1:04:00\n0.9970 row%1:14:00 0.9962 table%1:14:00 0.9956 chair%1:04:00 0.9950 population%1:14:00\n0.9970 mother%1:18:00 0.9962 subscriber%1:18:01 0.9955 tape%1:06:00 0.9950 cloud%1:17:00\n0.9969 sum%1:21:00 0.9962 powder%1:27:00 0.9955 earthquake%1:11:00 0.9950 pleasure%1:12:00\n0.9969 answer%1:10:01 0.9962 output%1:04:00 0.9955 duty%1:04:00 0.9950 representative%1:18:00\n0.9969 major%1:18:00 0.9962 scorer%1:18:00 0.9955 master%1:18:00 0.9949 personnel%1:14:00\n0.9969 pattern%1:09:00 0.9962 card%1:06:00 0.9955 variable%1:09:00 0.9949 survey%1:04:02\n0.9969 visit%1:04:02 0.9962 material%1:27:00 0.9955 deposit%1:19:00 0.9949 array%1:14:00\n0.9969 test%1:09:02 0.9962 radiation%1:19:00 0.9954 ﬂoor%1:06:00 0.9949 sport%1:04:01\n0.9969 cycle%1:28:00 0.9962 division%1:14:00 0.9954 intensity%1:07:03 0.9949 craft%1:04:00\n0.9968 bar%1:06:04 0.9962 growth%1:22:00 0.9954 comment%1:10:00 0.9949 plot%1:09:00\n0.9968 staff%1:14:01 0.9961 spot%1:15:01 0.9954 triumph%1:11:00 0.9949 draft%1:21:00\n0.9968 pound%1:23:01 0.9961 mood%1:12:00 0.9954 rest%1:24:00 0.9949 fate%1:11:00\n0.9968 isolation%1:26:00 0.9961 look%1:07:01 0.9954 drinking%1:04:00 0.9949 reason%1:16:00\n0.9968 effort%1:04:00 0.9960 unit%1:23:00 0.9954 rhythm%1:07:01 0.9948 diversity%1:07:02\n0.9968 wish%1:12:00 0.9960 delivery%1:04:04 0.9954 liability%1:26:01 0.9948 balloon%1:06:00\n0.9968 execution%1:04:00 0.9960 pilot%1:18:00 0.9953 aspect%1:09:00 0.9948 address%1:10:04\n0.9968 rhythm%1:10:01 0.9960 class%1:14:00 0.9953 ton%1:23:02 0.9948 nut%1:20:00\n0.9967 nightmare%1:09:00 0.9960 minute%1:28:00 0.9953 doctor%1:18:02 0.9947 mouse%1:05:00\n0.9967 resource%1:21:00 0.9960 vein%1:08:00 0.9953 variety%1:14:01 0.9947 mode%1:07:00\n0.9967 poster%1:10:00 0.9960 shop%1:06:00 0.9953 trade%1:04:05 0.9947 ground%1:17:00\n0.9967 couple%1:14:01 0.9960 executive%1:18:00 0.9953 rule%1:09:00 0.9947 complaint%1:26:00\n0.9967 joy%1:12:00 0.9959 welfare%1:04:00 0.9953 copy%1:06:00 0.9947 insight%1:09:02\n0.9967 neighbor%1:18:00 0.9959 mystery%1:09:00 0.9953 sound%1:09:00 0.9947 peak%1:23:00\n0.9967 sanction%1:10:00 0.9959 contractor%1:18:00 0.9953 contact%1:04:02 0.9946 replacement%1:04:00\n0.9966 note%1:10:00 0.9959 obligation%1:04:00 0.9953 bill%1:10:04 0.9946 adaptation%1:10:00\n0.9966 nickname%1:10:00 0.9959 claim%1:10:00 0.9953 option%1:21:00 0.9946 republican%1:18:01\n0.9966 pound%1:23:09 0.9959 dinner%1:14:00 0.9953 trial%1:04:00 0.9946 bed%1:06:00\n0.9966 stone%1:17:00 0.9959 violation%1:04:00 0.9952 envoy%1:18:01 0.9946 powder%1:27:01\n0.9966 bond%1:21:02 0.9959 employment%1:26:00 0.9952 week%1:28:00 0.9946 honor%1:10:00\n0.9966 appeal%1:07:00 0.9958 pleasure%1:09:00 0.9952 cleveland%1:15:00 0.9946 news%1:10:00\n0.9965 horn%1:06:06 0.9958 edition%1:10:02 0.9952 ﬂower%1:20:00 0.9946 help%1:04:00\n0.9965 arrival%1:04:00 0.9958 gender%1:10:00 0.9952 formation%1:14:00 0.9945 status%1:26:00\n0.9965 quantity%1:03:00 0.9958 duck%1:05:00 0.9952 category%1:09:02 0.9944 discovery%1:04:00\n0.9964 throat%1:08:00 0.9957 bronze%1:27:00 0.9952 bell%1:06:00 0.9944 tie%1:06:01\n0.9964 source%1:15:00 0.9957 triumph%1:12:00 0.9951 commerce%1:14:00 0.9944 commission%1:14:00\n0.9964 ministry%1:14:01 0.9957 wealth%1:26:00 0.9951 church%1:14:00 0.9944 pressure%1:19:00\n0.9964 grievance%1:10:01 0.9957 usa%1:15:00 0.9951 suspension%1:28:00 0.9944 path%1:04:00\n0.9964 ﬂexibility%1:07:02 0.9957 justice%1:07:00 0.9951 chart%1:10:00 0.9944 event%1:03:00\n0.9964 crisis%1:26:00 0.9957 woman%1:18:00 0.9951 sunday%1:28:00 0.9943 job%1:04:00\n0.9964 discipline%1:09:00 0.9957 market%1:14:00 0.9951 instruction%1:10:04 0.9943 swing%1:26:01\n0.9964 text%1:10:00 0.9957 plane%1:25:00 0.9951 partner%1:18:01 0.9943 intention%1:09:00\n0.9963 trick%1:04:05 0.9956 striker%1:18:02 0.9951 front%1:15:00 0.9943 dispute%1:10:00\n0.9963 evidence%1:09:00 0.9956 times%1:28:01 0.9951 budget%1:21:03 0.9943 cost%1:21:00\n0.9963 fear%1:12:00 0.9956 wind%1:19:00 0.9951 moon%1:17:01 0.9943 weight%1:07:00\n0.9963 violation%1:04:04 0.9956 representation%1:09:00 0.9951 shadow%1:26:01 0.9943 repression%1:26:00\n0.9963 favor%1:04:00 0.9956 chance%1:26:00 0.9950 sample%1:09:00 0.9942 surprise%1:11:00\n0.9963 play%1:10:01 0.9956 recovery%1:11:00 0.9950 phosphorus%1:27:00 0.9942 comfort%1:26:00\n0.9963 broadcaster%1:18:00 0.9956 ﬁre%1:11:00 0.9950 frequency%1:24:00 0.9942 appearance%1:07:00\nTable 10: List of sense concepts considered (1/4), sorted by the AP⋆\nc obtained by GPT2-L.\nAP⋆\nc sense concept AP⋆\nc sense concept AP⋆\nc sense concept AP⋆\nc sense concept\n0.9942 brand%1:10:00 0.9934 pier%1:06:00 0.9926 question%1:10:00 0.9917 interval%1:28:00\n0.9942 energy%1:19:00 0.9934 power%1:07:00 0.9926 current%1:19:01 0.9917 focus%1:09:00\n0.9941 opponent%1:18:02 0.9934 money%1:21:00 0.9926 china%1:15:00 0.9916 judge%1:18:00\n0.9941 sale%1:04:00 0.9934 delivery%1:11:00 0.9925 talk%1:10:00 0.9916 outlet%1:06:01\n0.9941 root%1:20:00 0.9934 commerce%1:04:00 0.9925 accumulation%1:22:00 0.9916 stage%1:28:00\n0.9941 scale%1:24:03 0.9934 battery%1:14:02 0.9925 trend%1:15:02 0.9916 piece%1:06:00\n0.9941 security%1:26:00 0.9934 left%1:15:00 0.9925 leader%1:18:00 0.9916 routine%1:10:01\n0.9941 bird%1:13:00 0.9933 entrance%1:06:00 0.9924 ﬁght%1:04:01 0.9915 relation%1:03:00\n0.9941 conference%1:14:00 0.9933 ﬁling%1:10:00 0.9924 head%1:08:00 0.9915 yield%1:04:00\n0.9940 capital%1:21:01 0.9933 lead%1:07:02 0.9924 progress%1:04:01 0.9915 movement%1:04:00\n0.9940 fall%1:28:00 0.9933 communication%1:10:01 0.9924 environment%1:26:00 0.9914 intervention%1:04:00\n0.9940 league%1:14:00 0.9932 dinner%1:13:00 0.9924 information%1:10:00 0.9914 conservative%1:18:01\n0.9940 russia%1:15:01 0.9932 friend%1:18:00 0.9924 danger%1:26:00 0.9914 distinction%1:09:00\n0.9940 hand%1:08:00 0.9932 impact%1:11:00 0.9923 hall%1:06:03 0.9913 range%1:07:00\n0.9940 panama%1:15:00 0.9932 step%1:04:02 0.9923 radio%1:10:00 0.9913 round%1:28:01\n0.9940 shock%1:04:01 0.9932 article%1:10:00 0.9923 master%1:18:04 0.9913 bonus%1:21:00\n0.9939 football%1:04:00 0.9932 obstacle%1:09:00 0.9923 advent%1:04:00 0.9913 blood%1:08:00\n0.9939 nickname%1:10:01 0.9932 shelter%1:06:01 0.9923 driver%1:18:00 0.9913 emphasis%1:26:00\n0.9939 voice%1:07:00 0.9932 race%1:11:01 0.9923 background%1:07:00 0.9913 earth%1:17:00\n0.9939 eye%1:08:00 0.9932 craft%1:06:00 0.9922 news%1:10:01 0.9913 ﬂow%1:11:00\n0.9939 youth%1:18:00 0.9931 personality%1:18:00 0.9922 community%1:14:00 0.9912 right%1:15:00\n0.9939 bell%1:06:02 0.9931 pet%1:05:00 0.9922 thought%1:09:01 0.9912 mistake%1:04:00\n0.9939 departure%1:04:00 0.9931 attendance%1:04:00 0.9922 dance%1:10:00 0.9912 actor%1:18:00\n0.9939 winner%1:18:01 0.9930 combination%1:14:00 0.9922 draft%1:19:00 0.9911 trainer%1:06:00\n0.9938 sideline%1:15:00 0.9930 account%1:10:00 0.9922 water%1:27:00 0.9911 past%1:28:00\n0.9938 equity%1:21:00 0.9930 investigation%1:09:00 0.9921 labor%1:14:00 0.9911 chance%1:19:00\n0.9938 machine%1:06:00 0.9930 door%1:06:00 0.9921 experiment%1:04:00 0.9911 shock%1:12:01\n0.9938 ﬁshing%1:04:01 0.9930 intervention%1:10:00 0.9921 reduction%1:04:00 0.9910 document%1:10:00\n0.9938 cancer%1:26:00 0.9930 morning%1:28:00 0.9921 color%1:07:00 0.9910 truck%1:06:01\n0.9937 wave%1:11:01 0.9930 generalization%1:09:01 0.9921 regulation%1:10:00 0.9910 election%1:04:01\n0.9937 cat%1:05:00 0.9930 strategy%1:09:00 0.9921 face%1:08:00 0.9910 favor%1:07:00\n0.9937 killer%1:18:00 0.9929 border%1:15:00 0.9921 agent%1:18:02 0.9910 sentence%1:04:00\n0.9937 spine%1:08:00 0.9929 violence%1:04:01 0.9920 margin%1:25:00 0.9909 crossing%1:17:00\n0.9937 factor%1:11:00 0.9929 name%1:10:00 0.9920 market%1:04:00 0.9909 suit%1:04:00\n0.9936 boost%1:04:00 0.9929 mouth%1:08:00 0.9920 dose%1:06:00 0.9909 infrastructure%1:06:01\n0.9936 metal%1:27:00 0.9929 bond%1:19:00 0.9920 computer%1:06:00 0.9909 length%1:07:00\n0.9936 side%1:15:02 0.9928 diet%1:13:00 0.9920 report%1:10:03 0.9909 action%1:04:02\n0.9936 debate%1:10:01 0.9928 memory%1:09:01 0.9919 client%1:18:00 0.9908 trip%1:26:00\n0.9936 today%1:28:00 0.9928 angle%1:25:00 0.9919 proposal%1:10:00 0.9908 plus%1:07:00\n0.9936 expenditure%1:21:00 0.9928 responsibility%1:04:00 0.9918 threat%1:26:00 0.9908 season%1:28:02\n0.9935 virtue%1:07:01 0.9927 solution%1:27:00 0.9918 football%1:06:00 0.9908 usa%1:14:00\n0.9935 weight%1:06:01 0.9927 compensation%1:21:00 0.9918 addition%1:06:00 0.9908 angle%1:09:00\n0.9935 chair%1:06:00 0.9927 risk%1:26:00 0.9918 ﬂower%1:20:02 0.9907 technology%1:04:00\n0.9935 medicine%1:09:00 0.9927 street%1:06:00 0.9918 camera%1:06:00 0.9907 show%1:04:00\n0.9935 assumption%1:10:00 0.9927 return%1:10:01 0.9918 language%1:10:00 0.9907 trip%1:04:00\n0.9935 dream%1:09:01 0.9927 analyst%1:18:00 0.9917 structure%1:06:00 0.9907 capability%1:07:00\n0.9935 foot%1:08:01 0.9927 russia%1:15:00 0.9917 defense%1:04:03 0.9907 advantage%1:07:00\n0.9935 brazil%1:13:00 0.9927 hunter%1:18:00 0.9917 examination%1:04:00 0.9907 application%1:04:02\n0.9935 nobility%1:14:00 0.9927 cut%1:10:00 0.9917 poster%1:18:00 0.9907 kid%1:18:00\n0.9934 disco%1:10:00 0.9926 tone%1:10:01 0.9917 beat%1:15:00 0.9906 planet%1:17:00\n0.9906 stake%1:21:02 0.9893 maintenance%1:21:00 0.9882 dispute%1:04:00 0.9868 economy%1:14:00\n0.9906 sample%1:09:01 0.9893 stance%1:07:00 0.9881 reference%1:10:02 0.9868 user%1:18:00\n0.9906 interference%1:10:00 0.9892 jury%1:14:01 0.9881 leadership%1:04:00 0.9868 goal%1:09:00\n0.9905 glass%1:27:00 0.9892 window%1:06:00 0.9881 record%1:10:03 0.9868 colleague%1:18:01\n0.9905 basket%1:06:00 0.9892 ﬂow%1:28:00 0.9881 english%1:10:00 0.9868 duck%1:23:00\n0.9904 top%1:15:01 0.9892 experience%1:09:01 0.9880 course%1:04:01 0.9867 sign%1:10:00\n0.9904 committee%1:14:00 0.9892 wheel%1:06:05 0.9880 patient%1:18:00 0.9867 limit%1:07:00\n0.9904 commission%1:21:00 0.9891 act%1:10:01 0.9880 temperature%1:07:00 0.9867 agent%1:17:00\n0.9904 director%1:18:00 0.9891 beneﬁt%1:21:00 0.9880 relative%1:18:00 0.9867 opinion%1:09:00\n0.9904 guest%1:18:00 0.9891 detail%1:09:00 0.9880 mind%1:09:00 0.9867 thanks%1:04:00\n0.9904 paint%1:06:00 0.9891 council%1:14:01 0.9879 match%1:06:00 0.9866 teaching%1:04:00\n0.9904 function%1:24:00 0.9891 compensation%1:22:00 0.9879 room%1:06:00 0.9866 distinction%1:26:00\n0.9903 situation%1:26:00 0.9891 administration%1:04:00 0.9879 birth%1:28:00 0.9866 increase%1:23:00\n0.9903 difference%1:07:00 0.9891 heel%1:06:00 0.9878 environment%1:15:00 0.9866 interest%1:09:00\n0.9903 appearance%1:11:00 0.9890 isolation%1:12:00 0.9878 operator%1:18:00 0.9866 court%1:14:00\n0.9903 club%1:14:01 0.9890 box%1:06:00 0.9878 space%1:25:00 0.9866 giant%1:05:00\n0.9903 banking%1:04:01 0.9890 game%1:04:00 0.9877 parish%1:15:00 0.9866 client%1:18:01\n0.9902 guy%1:18:00 0.9890 training%1:04:00 0.9877 ﬂoor%1:06:01 0.9866 barrier%1:09:00\n0.9902 stranger%1:18:01 0.9889 purpose%1:09:00 0.9877 log%1:10:01 0.9865 sunday%1:18:00\n0.9901 ﬁeld%1:15:00 0.9889 senate%1:14:00 0.9876 washington%1:15:01 0.9865 region%1:15:00\n0.9901 gas%1:26:00 0.9889 crystal%1:06:02 0.9876 right%1:07:00 0.9865 pet%1:18:00\n0.9901 face%1:07:03 0.9889 address%1:15:00 0.9876 meeting%1:14:00 0.9864 league%1:14:01\n0.9901 limit%1:28:00 0.9889 camera%1:06:01 0.9876 signal%1:10:00 0.9864 attempt%1:04:00\n0.9900 polyp%1:26:00 0.9888 ofﬁcial%1:18:01 0.9876 index%1:24:00 0.9864 grade%1:14:00\n0.9900 return%1:04:01 0.9888 baby%1:18:01 0.9876 basis%1:24:00 0.9864 challenge%1:26:00\n0.9899 challenge%1:10:00 0.9888 element%1:09:00 0.9875 attendance%1:28:00 0.9863 program%1:09:00\n0.9899 guy%1:06:01 0.9887 hell%1:11:00 0.9875 minister%1:18:00 0.9863 governor%1:18:00\n0.9899 care%1:04:01 0.9887 duty%1:04:02 0.9874 variation%1:11:01 0.9863 rank%1:14:00\n0.9899 seat%1:15:01 0.9887 signal%1:16:00 0.9874 conservative%1:18:00 0.9862 example%1:09:00\n0.9899 journalist%1:18:01 0.9887 insight%1:12:00 0.9873 canon%1:10:00 0.9862 session%1:28:00\n0.9899 bid%1:04:00 0.9887 performance%1:10:00 0.9873 candidate%1:18:01 0.9862 arrival%1:04:01\n0.9898 rank%1:26:00 0.9887 london%1:15:00 0.9873 project%1:04:00 0.9861 body%1:08:00\n0.9897 family%1:14:02 0.9886 space%1:03:00 0.9873 loss%1:21:01 0.9861 treatment%1:04:01\n0.9897 acceptance%1:09:00 0.9886 involvement%1:24:00 0.9872 story%1:10:03 0.9861 plan%1:09:00\n0.9897 protection%1:04:00 0.9886 rate%1:28:00 0.9872 instability%1:07:01 0.9860 decision%1:04:00\n0.9897 killer%1:26:00 0.9886 success%1:11:00 0.9872 home%1:06:00 0.9859 wish%1:10:00\n0.9897 union%1:14:01 0.9885 times%1:04:00 0.9872 youth%1:14:00 0.9859 art%1:06:00\n0.9896 personnel%1:14:01 0.9885 one%1:23:00 0.9871 lead%1:27:00 0.9859 france%1:15:00\n0.9896 advance%1:11:00 0.9885 cycle%1:14:00 0.9871 term%1:10:00 0.9859 move%1:04:01\n0.9896 bluff%1:17:00 0.9885 education%1:04:00 0.9870 investment%1:04:00 0.9858 chamber%1:08:00\n0.9895 bit%1:23:01 0.9885 posture%1:07:00 0.9870 border%1:25:00 0.9858 deal%1:10:00\n0.9895 code%1:10:00 0.9885 tea%1:13:00 0.9870 resident%1:18:00 0.9858 democracy%1:14:00\n0.9895 viewpoint%1:09:00 0.9885 birth%1:11:00 0.9870 israel%1:15:01 0.9858 amount%1:21:00\n0.9895 defense%1:04:00 0.9884 car%1:06:00 0.9870 inclusion%1:26:00 0.9858 heart%1:08:00\n0.9895 carnival%1:04:00 0.9884 advance%1:11:01 0.9869 solution%1:10:00 0.9857 liability%1:26:00\n0.9894 kingdom%1:26:00 0.9883 heel%1:08:00 0.9869 foundation%1:24:00 0.9857 par%1:23:00\n0.9894 selection%1:04:00 0.9883 bronze%1:06:00 0.9869 cause%1:11:00 0.9857 announcement%1:10:01\n0.9893 search%1:04:00 0.9883 future%1:28:00 0.9869 pupil%1:18:00 0.9857 female%1:05:00\n0.9893 noise%1:11:00 0.9882 administration%1:14:00 0.9869 drink%1:04:00 0.9856 shop%1:06:01\n0.9893 obstacle%1:06:00 0.9882 night%1:28:00 0.9869 industry%1:14:00 0.9856 companion%1:18:02\nTable 11: List of sense concepts considered (2/4), sorted by the AP⋆\nc obtained by GPT2-L.\nAP⋆\nc sense concept AP⋆\nc sense concept AP⋆\nc sense concept AP⋆\nc sense concept\n0.9856 cash%1:21:02 0.9835 obligation%1:26:00 0.9815 exposure%1:04:06 0.9787 interval%1:09:00\n0.9856 fellow%1:18:02 0.9835 partner%1:18:00 0.9814 machine%1:18:00 0.9786 help%1:18:00\n0.9856 extent%1:26:00 0.9834 input%1:10:00 0.9814 minute%1:28:01 0.9785 inﬂuence%1:07:00\n0.9855 blood%1:07:00 0.9834 force%1:19:00 0.9813 visit%1:14:00 0.9785 whole%1:09:00\n0.9855 decision%1:09:00 0.9833 cell%1:06:03 0.9813 treatment%1:04:00 0.9785 inﬂuence%1:04:00\n0.9854 balloon%1:06:01 0.9832 book%1:10:00 0.9810 candidate%1:18:00 0.9785 height%1:26:00\n0.9853 surgery%1:06:01 0.9832 assessment%1:09:00 0.9810 hand%1:18:00 0.9784 worker%1:18:01\n0.9853 start%1:11:00 0.9832 perimeter%1:25:00 0.9809 prayer%1:10:02 0.9782 wind%1:19:01\n0.9853 senate%1:14:01 0.9831 edition%1:14:00 0.9808 major%1:18:02 0.9782 series%1:14:00\n0.9852 nation%1:14:00 0.9831 band%1:14:00 0.9808 bank%1:17:01 0.9782 low%1:26:00\n0.9852 list%1:10:00 0.9830 authority%1:07:00 0.9808 circus%1:14:00 0.9781 restriction%1:09:00\n0.9852 programme%1:10:00 0.9830 damage%1:11:00 0.9806 pocket%1:25:00 0.9781 gas%1:27:00\n0.9851 congress%1:14:01 0.9829 letter%1:10:00 0.9805 minister%1:18:02 0.9780 gender%1:07:00\n0.9850 age%1:07:00 0.9828 role%1:04:00 0.9805 school%1:14:00 0.9780 application%1:10:00\n0.9850 heart%1:09:00 0.9827 hour%1:28:00 0.9805 force%1:07:01 0.9780 idea%1:09:01\n0.9849 change%1:11:00 0.9827 broadcaster%1:06:00 0.9805 sense%1:10:00 0.9780 back%1:08:00\n0.9849 tenant%1:18:02 0.9827 parish%1:14:00 0.9804 alarm%1:12:00 0.9778 kitty%1:21:01\n0.9848 march%1:28:00 0.9827 authority%1:18:01 0.9804 crisis%1:11:00 0.9777 loss%1:22:00\n0.9848 concern%1:09:00 0.9827 research%1:04:00 0.9803 trade%1:04:00 0.9777 investment%1:21:00\n0.9847 tablet%1:06:02 0.9826 comment%1:10:01 0.9803 result%1:19:00 0.9777 child%1:18:00\n0.9847 american%1:18:00 0.9826 factor%1:09:00 0.9803 woman%1:18:01 0.9777 number%1:07:00\n0.9847 intention%1:09:01 0.9826 matter%1:09:01 0.9802 dream%1:09:02 0.9775 bottom%1:15:00\n0.9846 philosophy%1:09:01 0.9826 way%1:07:01 0.9801 house%1:06:00 0.9774 decade%1:28:00\n0.9846 development%1:04:01 0.9825 involvement%1:04:00 0.9801 whole%1:03:00 0.9774 phone%1:10:00\n0.9845 duration%1:28:02 0.9825 step%1:23:00 0.9801 goal%1:15:00 0.9773 medicine%1:06:00\n0.9845 receptor%1:08:00 0.9825 institution%1:14:00 0.9801 principal%1:18:00 0.9773 season%1:28:00\n0.9845 proportion%1:24:00 0.9824 journal%1:10:01 0.9800 cab%1:06:01 0.9770 condition%1:26:00\n0.9845 man%1:18:00 0.9824 position%1:15:00 0.9800 form%1:10:00 0.9770 light%1:06:00\n0.9845 attempt%1:04:02 0.9824 care%1:09:00 0.9800 passion%1:12:00 0.9770 breast%1:08:00\n0.9844 interaction%1:04:00 0.9823 generation%1:14:01 0.9800 migration%1:14:00 0.9770 country%1:14:00\n0.9844 phase%1:28:00 0.9823 dog%1:18:01 0.9799 idea%1:09:00 0.9770 foot%1:23:00\n0.9844 level%1:07:00 0.9822 spring%1:06:00 0.9797 perspective%1:09:00 0.9770 score%1:10:00\n0.9844 outlet%1:06:02 0.9821 president%1:18:01 0.9797 region%1:08:00 0.9769 account%1:10:03\n0.9843 knot%1:06:00 0.9820 past%1:28:01 0.9797 need%1:26:00 0.9769 circumstance%1:26:01\n0.9843 campaign%1:04:02 0.9819 brand%1:09:00 0.9797 attention%1:04:01 0.9768 detail%1:24:00\n0.9843 career%1:04:00 0.9818 assumption%1:09:00 0.9796 patient%1:10:00 0.9767 question%1:10:01\n0.9843 advantage%1:23:00 0.9818 control%1:07:00 0.9794 protection%1:06:00 0.9767 release%1:04:01\n0.9843 word%1:10:00 0.9818 reaction%1:09:00 0.9794 history%1:28:00 0.9765 contract%1:10:01\n0.9841 thing%1:26:00 0.9818 power%1:19:00 0.9794 quantity%1:07:00 0.9765 executive%1:14:01\n0.9841 method%1:09:00 0.9818 bay%1:17:00 0.9793 mode%1:26:00 0.9765 version%1:10:01\n0.9841 party%1:14:01 0.9818 rise%1:11:00 0.9793 problem%1:26:00 0.9764 margin%1:07:00\n0.9839 lady%1:18:02 0.9818 review%1:09:00 0.9792 pressure%1:07:00 0.9763 area%1:15:01\n0.9839 place%1:15:00 0.9817 atmosphere%1:23:00 0.9792 cent%1:23:00 0.9763 disco%1:06:00\n0.9838 offer%1:10:01 0.9817 fear%1:12:01 0.9792 threat%1:10:00 0.9762 critic%1:18:01\n0.9838 characteristic%1:09:00 0.9817 player%1:18:01 0.9792 owner%1:18:02 0.9762 writer%1:18:00\n0.9838 understanding%1:10:00 0.9817 resource%1:07:00 0.9791 type%1:09:00 0.9761 america%1:15:00\n0.9836 issue%1:09:01 0.9816 evidence%1:10:01 0.9790 value%1:09:00 0.9760 altar%1:06:01\n0.9836 representation%1:06:00 0.9816 leaﬂet%1:20:00 0.9789 test%1:04:02 0.9760 circus%1:04:00\n0.9835 statement%1:10:00 0.9816 member%1:18:00 0.9789 capital%1:21:00 0.9760 ownership%1:04:00\n0.9835 territory%1:15:00 0.9816 pupil%1:08:00 0.9788 service%1:04:08 0.9759 noise%1:09:00\n0.9758 discovery%1:10:00 0.9711 use%1:04:00 0.9665 programme%1:10:05 0.9606 group%1:03:00\n0.9758 activity%1:04:00 0.9710 society%1:14:00 0.9665 vote%1:04:01 0.9606 situation%1:26:01\n0.9756 society%1:14:01 0.9708 habit%1:04:02 0.9664 course%1:14:00 0.9606 ﬁgure%1:08:00\n0.9754 talent%1:09:00 0.9708 language%1:10:01 0.9664 night%1:28:01 0.9606 ofﬁcial%1:18:00\n0.9754 war%1:04:00 0.9707 retention%1:04:00 0.9662 future%1:10:00 0.9604 stage%1:26:00\n0.9754 person%1:03:00 0.9707 circle%1:14:00 0.9661 length%1:07:01 0.9602 march%1:04:00\n0.9753 center%1:15:01 0.9706 treasury%1:21:00 0.9661 order%1:10:03 0.9602 volume%1:07:03\n0.9753 computer%1:18:00 0.9705 assertion%1:10:00 0.9661 earth%1:27:00 0.9601 church%1:06:00\n0.9753 will%1:09:00 0.9705 carnival%1:04:02 0.9661 life%1:26:01 0.9601 role%1:09:00\n0.9752 dose%1:23:00 0.9703 door%1:06:01 0.9660 arm%1:06:03 0.9600 source%1:10:00\n0.9751 phase%1:26:00 0.9701 term%1:28:00 0.9660 thought%1:09:00 0.9599 cent%1:21:00\n0.9750 stability%1:26:00 0.9699 proposal%1:10:02 0.9659 law%1:14:00 0.9596 seat%1:08:00\n0.9749 town%1:14:00 0.9699 condition%1:10:01 0.9656 state%1:15:01 0.9596 project%1:09:00\n0.9749 repression%1:22:00 0.9699 set%1:14:00 0.9656 aspect%1:07:02 0.9595 reduction%1:22:00\n0.9748 progress%1:04:00 0.9696 part%1:24:00 0.9656 system%1:06:00 0.9594 block%1:06:00\n0.9748 student%1:18:00 0.9694 money%1:21:02 0.9656 eye%1:09:00 0.9591 gold%1:07:00\n0.9747 publication%1:10:00 0.9694 hour%1:28:01 0.9654 perspective%1:07:00 0.9591 people%1:14:00\n0.9746 effect%1:19:00 0.9694 act%1:03:00 0.9651 banking%1:04:00 0.9590 piece%1:06:05\n0.9745 offer%1:10:00 0.9694 voice%1:10:00 0.9651 charge%1:10:00 0.9589 issue%1:10:00\n0.9744 table%1:06:01 0.9693 diet%1:14:00 0.9648 economy%1:09:01 0.9587 version%1:09:01\n0.9743 process%1:04:00 0.9693 hunter%1:18:01 0.9646 responsibility%1:26:00 0.9585 practice%1:04:02\n0.9742 bill%1:10:01 0.9692 fund%1:21:00 0.9646 variety%1:07:00 0.9583 reason%1:10:01\n0.9739 congress%1:14:00 0.9691 document%1:06:00 0.9644 temperature%1:09:00 0.9578 number%1:23:00\n0.9739 record%1:06:00 0.9690 end%1:15:00 0.9643 virtue%1:07:03 0.9577 president%1:18:04\n0.9738 justice%1:04:00 0.9690 head%1:05:00 0.9639 infrastructure%1:06:00 0.9574 philosophy%1:09:00\n0.9738 government%1:14:00 0.9689 period%1:28:00 0.9637 day%1:28:00 0.9572 interest%1:07:01\n0.9737 top%1:15:00 0.9687 background%1:09:00 0.9632 control%1:24:00 0.9569 washington%1:15:00\n0.9737 current%1:11:00 0.9686 handful%1:23:01 0.9631 glass%1:06:00 0.9566 fate%1:18:00\n0.9734 effort%1:04:01 0.9686 director%1:18:03 0.9629 cat%1:18:01 0.9565 support%1:04:04\n0.9734 union%1:15:00 0.9685 left%1:14:00 0.9629 horn%1:05:01 0.9564 chart%1:06:00\n0.9732 today%1:28:01 0.9685 driver%1:18:02 0.9627 discussion%1:10:00 0.9562 position%1:15:02\n0.9732 europe%1:17:00 0.9684 week%1:28:01 0.9627 mg%1:27:00 0.9560 metal%1:27:01\n0.9730 element%1:06:00 0.9684 violence%1:07:00 0.9626 pilot%1:18:01 0.9560 dollar%1:21:00\n0.9730 water%1:17:00 0.9679 company%1:14:01 0.9626 debate%1:10:00 0.9560 male%1:18:00\n0.9729 pair%1:23:00 0.9679 ﬁght%1:04:02 0.9623 strategy%1:09:01 0.9558 journal%1:10:00\n0.9728 perimeter%1:25:01 0.9677 ground%1:16:00 0.9623 stake%1:10:00 0.9558 london%1:18:00\n0.9727 box%1:06:02 0.9677 leadership%1:14:00 0.9623 interaction%1:19:00 0.9557 increase%1:11:00\n0.9725 path%1:06:00 0.9677 window%1:06:01 0.9623 month%1:28:01 0.9557 information%1:09:00\n0.9724 cup%1:23:01 0.9676 race%1:11:00 0.9622 unit%1:24:00 0.9557 fairness%1:07:00\n0.9722 conference%1:14:01 0.9674 3d%1:09:00 0.9621 success%1:04:00 0.9554 tie%1:26:01\n0.9721 estimate%1:04:01 0.9672 china%1:06:00 0.9621 range%1:15:01 0.9552 center%1:06:02\n0.9721 response%1:19:00 0.9672 level%1:26:01 0.9621 game%1:04:03 0.9552 trouble%1:09:00\n0.9720 examination%1:10:00 0.9671 security%1:21:01 0.9618 announcement%1:10:00 0.9552 governor%1:06:00\n0.9720 art%1:04:00 0.9670 status%1:26:01 0.9617 concern%1:12:01 0.9550 work%1:04:00\n0.9718 product%1:06:01 0.9670 technology%1:09:00 0.9614 list%1:07:00 0.9548 job%1:04:01\n0.9717 city%1:15:00 0.9670 difference%1:11:00 0.9614 pattern%1:04:00 0.9548 period%1:28:02\n0.9717 ﬁre%1:04:00 0.9669 ofﬁce%1:14:01 0.9612 need%1:17:00 0.9545 capability%1:26:00\n0.9716 wave%1:04:02 0.9669 opposite%1:10:00 0.9610 competition%1:11:00 0.9544 judge%1:18:01\n0.9714 ring%1:25:00 0.9666 english%1:18:00 0.9608 world%1:17:01 0.9543 actor%1:18:01\n0.9714 population%1:14:01 0.9666 pier%1:06:02 0.9606 bank%1:14:00 0.9542 bar%1:06:05\nTable 12: List of sense concepts considered (3/4), sorted by the AP⋆\nc obtained by GPT2-L.\nAP⋆\nc sense concept AP⋆\nc sense concept AP⋆\nc sense concept\n0.9541 word%1:10:03 0.9370 leader%1:06:00 0.9168 research%1:09:00\n0.9540 politician%1:18:01 0.9366 student%1:18:01 0.9167 cause%1:10:00\n0.9538 rate%1:21:00 0.9366 front%1:15:01 0.9162 farmer%1:18:01\n0.9536 operation%1:04:06 0.9362 circumstance%1:26:02 0.9159 cleveland%1:18:00\n0.9534 color%1:07:03 0.9355 community%1:21:00 0.9155 side%1:14:00\n0.9532 participant%1:18:01 0.9341 order%1:07:01 0.9153 show%1:10:03\n0.9530 value%1:07:00 0.9338 hope%1:12:01 0.9149 story%1:10:00\n0.9518 lady%1:18:01 0.9336 cost%1:07:00 0.9145 change%1:24:00\n0.9518 victim%1:18:00 0.9333 company%1:14:03 0.9116 cell%1:03:00\n0.9515 education%1:09:00 0.9333 statement%1:10:02 0.9109 extent%1:07:00\n0.9514 name%1:26:00 0.9323 series%1:10:01 0.9069 law%1:10:00\n0.9513 family%1:14:00 0.9312 air%1:27:00 0.9058 industry%1:04:00\n0.9506 line%1:14:03 0.9310 regard%1:09:01 0.9054 altar%1:06:00\n0.9505 paper%1:10:01 0.9305 france%1:18:00 0.9051 response%1:04:01\n0.9502 stone%1:06:00 0.9299 relation%1:04:01 0.9049 system%1:14:00\n0.9501 book%1:06:00 0.9293 time%1:11:00 0.9009 decade%1:23:00\n0.9498 moon%1:17:02 0.9292 performance%1:04:01 0.8987 career%1:04:01\n0.9498 view%1:09:02 0.9290 america%1:17:00 0.8977 government%1:04:00\n0.9495 opponent%1:18:00 0.9283 point%1:09:00 0.8976 history%1:10:00\n0.9495 movement%1:04:04 0.9282 policy%1:10:00 0.8951 standing%1:26:00\n0.9494 grade%1:26:00 0.9280 will%1:09:01 0.8950 process%1:09:00\n0.9493 damage%1:11:01 0.9276 selection%1:14:00 0.8939 day%1:28:03\n0.9493 club%1:14:00 0.9274 loan%1:10:00 0.8938 amount%1:07:00\n0.9484 sale%1:04:02 0.9271 case%1:26:00 0.8928 service%1:04:00\n0.9476 foundation%1:14:00 0.9268 structure%1:07:00 0.8926 theory%1:09:00\n0.9469 child%1:18:01 0.9263 meeting%1:14:01 0.8867 basis%1:09:00\n0.9467 ﬁle%1:14:00 0.9257 body%1:14:00 0.8861 city%1:15:01\n0.9466 month%1:28:00 0.9255 age%1:28:02 0.8790 people%1:14:01\n0.9464 commitment%1:07:01 0.9255 car%1:06:01 0.8787 high%1:07:00\n0.9461 constituent%1:18:00 0.9254 end%1:28:00 0.8779 one%1:09:00\n0.9457 diner%1:06:00 0.9252 class%1:14:03 0.8734 area%1:09:00\n0.9454 impact%1:19:00 0.9250 player%1:18:02 0.8732 house%1:14:01\n0.9453 example%1:09:02 0.9246 relative%1:05:00 0.8693 world%1:14:01\n0.9449 nature%1:07:02 0.9246 street%1:06:01 0.8668 state%1:03:00\n0.9447 event%1:26:00 0.9243 rule%1:09:01 0.8640 man%1:18:03\n0.9438 type%1:18:00 0.9226 activity%1:26:00 0.8637 standing%1:10:00\n0.9435 plan%1:09:01 0.9225 american%1:10:00 0.8582 outpost%1:14:00\n0.9425 member%1:24:00 0.9224 reference%1:10:03 0.8578 form%1:09:01\n0.9421 letter%1:10:01 0.9220 institution%1:06:00 0.8508 work%1:06:00\n0.9420 kid%1:27:00 0.9218 development%1:22:02 0.8481 use%1:07:00\n0.9416 addition%1:04:02 0.9217 part%1:06:00 0.8478 canon%1:18:00\n0.9414 result%1:10:00 0.9206 program%1:09:01 0.8242 year%1:28:02\n0.9413 claim%1:10:02 0.9206 method%1:04:00 0.8198 ringer%1:18:02\n0.9390 enterprise%1:04:00 0.9203 place%1:15:04 0.7439 time%1:28:05\n0.9387 fund%1:21:01 0.9197 investigation%1:04:00\n0.9386 adaptation%1:22:00 0.9193 thing%1:04:00\n0.9385 characteristic%1:07:00 0.9190 way%1:04:01\n0.9384 group%1:27:00 0.9189 country%1:15:00\n0.9380 resident%1:18:01 0.9180 effect%1:07:00\n0.9374 net%1:06:01 0.9168 material%1:10:00\nTable 13: List of sense concepts considered (4/4), sorted by the AP⋆\nc obtained by GPT2-L.\nAP⋆\nc homograph concept AP⋆\nc homograph concept AP⋆\nc homograph concept\n0.9984 killer%1:26:00 VS. killer%1:18:00 0.8459 phase%1:26:00 VS. phase%1:28:00 0.6803 violence%1:07:00 VS. violence%1:04:01\n0.9972 suspension%1:28:00 VS. suspension%1:27:00 0.8457 extent%1:07:00 VS. extent%1:26:00 0.6762 history%1:10:00 VS. history%1:28:00\n0.9956 margin%1:07:00 VS. margin%1:25:00 0.8404 plot%1:15:00 VS. plot%1:09:00 0.6751 ofﬁce%1:14:01 VS. ofﬁce%1:06:00\n0.9928 log%1:10:01 VS. log%1:27:01 0.8389 role%1:09:00 VS. role%1:04:00 0.6733 quantity%1:07:00 VS. quantity%1:03:00\n0.9911 round%1:28:01 VS. round%1:06:01 0.8323 area%1:09:00 VS. area%1:15:01 0.6721 use%1:07:00 VS. use%1:04:00\n0.9911 lead%1:27:00 VS. lead%1:07:02 0.8312 house%1:14:01 VS. house%1:06:00 0.6712 interest%1:07:01 VS. interest%1:09:00\n0.9882 relative%1:05:00 VS. relative%1:18:00 0.8295 arm%1:06:03 VS. arm%1:08:00 0.6707 session%1:28:00 VS. session%1:10:00\n0.9875 plane%1:25:00 VS. plane%1:06:01 0.8254 charge%1:10:00 VS. charge%1:04:01 0.6672 method%1:04:00 VS. method%1:09:00\n0.9871 stake%1:10:00 VS. stake%1:21:02 0.8192 amount%1:07:00 VS. amount%1:21:00 0.6660 bird%1:13:00 VS. bird%1:05:00\n0.9857 card%1:10:01 VS. card%1:06:00 0.8146 american%1:10:00 VS. american%1:18:00 0.6655 crisis%1:11:00 VS. crisis%1:26:00\n0.9840 patient%1:10:00 VS. patient%1:18:00 0.8143 times%1:04:00 VS. times%1:28:01 0.6640 march%1:04:00 VS. march%1:28:00\n0.9840 heart%1:08:00 VS. heart%1:09:00 0.8137 crystal%1:06:02 VS. crystal%1:27:00 0.6635 deal%1:10:00 VS. deal%1:04:02\n0.9815 duck%1:23:00 VS. duck%1:05:00 0.8118 commerce%1:14:00 VS. commerce%1:04:00 0.6605 attention%1:04:01 VS. attention%1:09:00\n0.9807 diet%1:14:00 VS. diet%1:13:00 0.8063 volume%1:07:03 VS. volume%1:23:00 0.6526 medicine%1:06:00 VS. medicine%1:09:00\n0.9794 bond%1:21:02 VS. bond%1:19:00 0.8055 visit%1:14:00 VS. visit%1:04:02 0.6520 campaign%1:04:02 VS. campaign%1:11:00\n0.9788 trainer%1:06:00 VS. trainer%1:18:00 0.8034 addition%1:04:02 VS. addition%1:06:00 0.6518 gas%1:27:00 VS. gas%1:26:00\n0.9780 china%1:06:00 VS. china%1:15:00 0.8030 cause%1:10:00 VS. cause%1:11:00 0.6485 machine%1:18:00 VS. machine%1:06:00\n0.9765 sum%1:09:01 VS. sum%1:21:00 0.8020 frequency%1:24:00 VS. frequency%1:28:00 0.6451 part%1:06:00 VS. part%1:24:00\n0.9764 governor%1:06:00 VS. governor%1:18:00 0.8018 nightmare%1:09:00 VS. nightmare%1:26:00 0.6422 representation%1:06:00 VS. representation%1:09:00\n0.9741 delivery%1:11:00 VS. delivery%1:04:04 0.8016 detail%1:24:00 VS. detail%1:09:00 0.6409 drink%1:04:00 VS. drink%1:13:04\n0.9674 region%1:08:00 VS. region%1:15:00 0.8007 chair%1:04:00 VS. chair%1:06:00 0.6381 security%1:21:01 VS. security%1:26:00\n0.9668 current%1:11:00 VS. current%1:19:01 0.8001 whole%1:03:00 VS. whole%1:09:00 0.6248 leadership%1:14:00 VS. leadership%1:04:00\n0.9618 spring%1:06:00 VS. spring%1:28:00 0.7994 increase%1:11:00 VS. increase%1:23:00 0.6228 obstacle%1:06:00 VS. obstacle%1:09:00\n0.9604 pupil%1:08:00 VS. pupil%1:18:00 0.7986 light%1:06:00 VS. light%1:19:00 0.6176 decade%1:23:00 VS. decade%1:28:00\n0.9567 mg%1:27:00 VS. mg%1:23:00 0.7961 fate%1:18:00 VS. fate%1:11:00 0.6083 reason%1:10:01 VS. reason%1:16:00\n0.9547 center%1:06:02 VS. center%1:15:01 0.7947 liquor%1:27:00 VS. liquor%1:13:00 0.6057 dispute%1:04:00 VS. dispute%1:10:00\n0.9547 space%1:25:00 VS. space%1:03:00 0.7928 reaction%1:09:00 VS. reaction%1:22:00 0.6044 maintenance%1:21:00 VS. maintenance%1:04:00\n0.9524 loan%1:10:00 VS. loan%1:21:00 0.7920 state%1:03:00 VS. state%1:15:01 0.6043 sound%1:09:00 VS. sound%1:07:00\n0.9487 activity%1:26:00 VS. activity%1:04:00 0.7882 agent%1:18:02 VS. agent%1:17:00 0.5968 parish%1:15:00 VS. parish%1:14:00\n0.9469 adaptation%1:22:00 VS. adaptation%1:10:00 0.7876 intervention%1:10:00 VS. intervention%1:04:00 0.5954 noise%1:09:00 VS. noise%1:11:00\n0.9399 response%1:04:01 VS. response%1:19:00 0.7862 america%1:17:00 VS. america%1:15:00 0.5947 aspect%1:07:02 VS. aspect%1:09:00\n0.9395 reduction%1:22:00 VS. reduction%1:04:00 0.7835 estimate%1:04:01 VS. estimate%1:09:00 0.5939 system%1:14:00 VS. system%1:06:00\n0.9385 value%1:07:00 VS. value%1:09:00 0.7810 voice%1:10:00 VS. voice%1:07:00 0.5935 concern%1:12:01 VS. concern%1:09:00\n0.9384 ministry%1:06:00 VS. ministry%1:14:01 0.7800 tie%1:26:01 VS. tie%1:06:01 0.5897 show%1:10:03 VS. show%1:04:00\n0.9371 craft%1:06:00 VS. craft%1:04:00 0.7785 executive%1:14:01 VS. executive%1:18:00 0.5874 chart%1:06:00 VS. chart%1:10:00\n0.9343 border%1:25:00 VS. border%1:15:00 0.7775 side%1:14:00 VS. side%1:15:02 0.5851 signal%1:16:00 VS. signal%1:10:00\n0.9330 interaction%1:19:00 VS. interaction%1:04:00 0.7774 trip%1:26:00 VS. trip%1:04:00 0.5846 dollar%1:21:00 VS. dollar%1:23:00\n0.9329 level%1:26:01 VS. level%1:07:00 0.7773 act%1:03:00 VS. act%1:10:01 0.5823 return%1:04:01 VS. return%1:10:01\n0.9319 solution%1:10:00 VS. solution%1:27:00 0.7755 change%1:24:00 VS. change%1:11:00 0.5800 book%1:06:00 VS. book%1:10:00\n0.9299 selection%1:14:00 VS. selection%1:04:00 0.7755 difference%1:11:00 VS. difference%1:07:00 0.5786 protection%1:06:00 VS. protection%1:04:00\n0.9296 path%1:06:00 VS. path%1:04:00 0.7737 cleveland%1:18:00 VS. cleveland%1:15:00 0.5778 pleasure%1:09:00 VS. pleasure%1:12:00\n0.9261 score%1:10:00 VS. score%1:09:00 0.7725 authority%1:18:01 VS. authority%1:07:00 0.5776 computer%1:18:00 VS. computer%1:06:00\n0.9248 right%1:15:00 VS. right%1:07:00 0.7718 process%1:09:00 VS. process%1:04:00 0.5768 youth%1:14:00 VS. youth%1:18:00\n0.9244 course%1:14:00 VS. course%1:04:01 0.7673 town%1:14:00 VS. town%1:15:00 0.5760 cup%1:23:01 VS. cup%1:06:00\n0.9190 interval%1:09:00 VS. interval%1:28:00 0.7668 commission%1:21:00 VS. commission%1:14:00 0.5747 copy%1:06:00 VS. copy%1:10:00\n0.9161 cycle%1:14:00 VS. cycle%1:28:00 0.7655 cloud%1:17:00 VS. cloud%1:19:01 0.5702 competition%1:11:00 VS. competition%1:24:01\n0.9157 angle%1:09:00 VS. angle%1:25:00 0.7646 resource%1:07:00 VS. resource%1:21:00 0.5684 dinner%1:14:00 VS. dinner%1:13:00\n0.9143 basis%1:09:00 VS. basis%1:24:00 0.7635 stage%1:26:00 VS. stage%1:28:00 0.5634 standing%1:10:00 VS. standing%1:26:00\n0.9109 ﬁle%1:14:00 VS. ﬁle%1:10:00 0.7634 one%1:09:00 VS. one%1:23:00 0.5612 cost%1:07:00 VS. cost%1:21:00\n0.9106 order%1:07:01 VS. order%1:10:03 0.7596 isolation%1:12:00 VS. isolation%1:26:00 0.5579 cent%1:21:00 VS. cent%1:23:00\n0.9098 obligation%1:26:00 VS. obligation%1:04:00 0.7572 term%1:28:00 VS. term%1:10:00 0.5573 thing%1:04:00 VS. thing%1:26:00\n0.9097 loss%1:22:00 VS. loss%1:21:01 0.7504 rank%1:26:00 VS. rank%1:14:00 0.5569 france%1:18:00 VS. france%1:15:00\n0.9071 record%1:06:00 VS. record%1:10:03 0.7494 wave%1:04:02 VS. wave%1:11:01 0.5564 ground%1:16:00 VS. ground%1:17:00\n0.9061 personality%1:18:00 VS. personality%1:07:00 0.7491 pair%1:23:00 VS. pair%1:14:01 0.5524 institution%1:06:00 VS. institution%1:14:00\n0.9050 horn%1:05:01 VS. horn%1:06:06 0.7475 cat%1:18:01 VS. cat%1:05:00 0.5482 range%1:15:01 VS. range%1:07:00\n0.9021 operator%1:18:00 VS. operator%1:24:00 0.7446 address%1:15:00 VS. address%1:10:04 0.5449 world%1:14:01 VS. world%1:17:01\n0.8994 examination%1:10:00 VS. examination%1:04:00 0.7440 heel%1:08:00 VS. heel%1:06:00 0.5312 ﬂow%1:28:00 VS. ﬂow%1:11:00\n0.8978 bank%1:14:00 VS. bank%1:17:01 0.7377 understanding%1:10:00 VS. understanding%1:09:01 0.5269 circus%1:04:00 VS. circus%1:14:00\n0.8964 repression%1:22:00 VS. repression%1:26:00 0.7376 care%1:09:00 VS. care%1:04:01 0.5264 inﬂuence%1:04:00 VS. inﬂuence%1:07:00\n0.8964 list%1:07:00 VS. list%1:10:00 0.7351 number%1:23:00 VS. number%1:07:00 0.5240 market%1:14:00 VS. market%1:04:00\n0.8955 3d%1:09:00 VS. 3d%1:10:00 0.7345 height%1:26:00 VS. height%1:07:00 0.5209 decision%1:09:00 VS. decision%1:04:00\n0.8954 suit%1:04:00 VS. suit%1:06:00 0.7319 goal%1:15:00 VS. goal%1:09:00 0.5145 broadcaster%1:06:00 VS. broadcaster%1:18:00\n0.8945 left%1:14:00 VS. left%1:15:00 0.7305 london%1:18:00 VS. london%1:15:00 0.5112 performance%1:04:01 VS. performance%1:10:00\n0.8939 cell%1:03:00 VS. cell%1:06:03 0.7293 dog%1:18:01 VS. dog%1:05:00 0.5100 stone%1:06:00 VS. stone%1:17:00\n0.8927 shock%1:04:01 VS. shock%1:12:01 0.7285 sunday%1:18:00 VS. sunday%1:28:00 0.5085 appearance%1:11:00 VS. appearance%1:07:00\n0.8925 name%1:26:00 VS. name%1:10:00 0.7281 discovery%1:10:00 VS. discovery%1:04:00 0.5070 research%1:09:00 VS. research%1:04:00\n0.8902 pocket%1:25:00 VS. pocket%1:06:00 0.7271 test%1:04:02 VS. test%1:09:02 0.4946 threat%1:10:00 VS. threat%1:26:00\n0.8894 future%1:10:00 VS. future%1:28:00 0.7270 guy%1:06:01 VS. guy%1:18:00 0.4884 wish%1:10:00 VS. wish%1:12:00\n0.8861 advantage%1:23:00 VS. advantage%1:07:00 0.7267 category%1:09:02 VS. category%1:14:00 0.4871 kid%1:27:00 VS. kid%1:18:00\n0.8846 chamber%1:08:00 VS. chamber%1:06:00 0.7262 information%1:09:00 VS. information%1:10:00 0.4854 challenge%1:10:00 VS. challenge%1:26:00\n0.8815 retirement%1:04:00 VS. retirement%1:26:00 0.7262 poster%1:18:00 VS. poster%1:10:00 0.4850 characteristic%1:07:00 VS. characteristic%1:09:00\n0.8802 phone%1:10:00 VS. phone%1:06:00 0.7247 step%1:23:00 VS. step%1:04:02 0.4800 usa%1:14:00 VS. usa%1:15:00\n0.8789 age%1:28:02 VS. age%1:07:00 0.7245 grade%1:26:00 VS. grade%1:14:00 0.4744 industry%1:04:00 VS. industry%1:14:00\n0.8788 table%1:06:01 VS. table%1:14:00 0.7236 triumph%1:12:00 VS. triumph%1:11:00 0.4732 face%1:07:03 VS. face%1:08:00\n0.8787 oil%1:06:01 VS. oil%1:27:00 0.7227 connecticut%1:17:00 VS. connecticut%1:15:00 0.4680 eye%1:09:00 VS. eye%1:08:00\n0.8762 power%1:19:00 VS. power%1:07:00 0.7207 variety%1:07:00 VS. variety%1:14:01 0.4643 prayer%1:10:02 VS. prayer%1:04:00\n0.8757 gold%1:07:00 VS. gold%1:21:00 0.7185 blood%1:07:00 VS. blood%1:08:00 0.4563 distinction%1:26:00 VS. distinction%1:09:00\n0.8755 seat%1:08:00 VS. seat%1:15:01 0.7163 involvement%1:24:00 VS. involvement%1:04:00 0.4468 document%1:06:00 VS. document%1:10:00\n0.8737 responsibility%1:26:00 VS. responsibility%1:04:00 0.7157 art%1:04:00 VS. art%1:06:00 0.4423 disco%1:06:00 VS. disco%1:10:00\n0.8727 bonus%1:21:00 VS. bonus%1:09:00 0.7151 favor%1:07:00 VS. favor%1:04:00 0.4179 investigation%1:04:00 VS. investigation%1:09:00\n0.8722 time%1:28:05 VS. time%1:11:00 0.7103 success%1:04:00 VS. success%1:11:00 0.4126 justice%1:04:00 VS. justice%1:07:00\n0.8720 administration%1:14:00 VS. administration%1:04:00 0.7063 canon%1:18:00 VS. canon%1:10:00 0.4088 inspiration%1:06:00 VS. inspiration%1:09:02\n0.8714 dose%1:23:00 VS. dose%1:06:00 0.7050 capability%1:26:00 VS. capability%1:07:00 0.3976 brand%1:09:00 VS. brand%1:10:00\n0.8691 pattern%1:04:00 VS. pattern%1:09:00 0.7044 evidence%1:10:01 VS. evidence%1:09:00 0.3917 assumption%1:09:00 VS. assumption%1:10:00\n0.8684 perspective%1:07:00 VS. perspective%1:09:00 0.7033 impact%1:19:00 VS. impact%1:11:00 0.3870 hell%1:11:00 VS. hell%1:15:01\n0.8670 brazil%1:13:00 VS. brazil%1:15:00 0.7006 factor%1:09:00 VS. factor%1:11:00 0.3861 country%1:15:00 VS. country%1:14:00\n0.8670 pet%1:18:00 VS. pet%1:05:00 0.6991 identity%1:09:00 VS. identity%1:07:00 0.3816 government%1:04:00 VS. government%1:14:00\n0.8665 need%1:17:00 VS. need%1:26:00 0.6983 glass%1:06:00 VS. glass%1:27:00 0.3808 help%1:18:00 VS. help%1:04:00\n0.8662 leader%1:06:00 VS. leader%1:18:00 0.6951 migration%1:14:00 VS. migration%1:04:00 0.3779 project%1:09:00 VS. project%1:04:00\n0.8642 temperature%1:09:00 VS. temperature%1:07:00 0.6935 control%1:24:00 VS. control%1:07:00 0.3775 insight%1:12:00 VS. insight%1:09:02\n0.8642 ﬁre%1:04:00 VS. ﬁre%1:11:00 0.6926 development%1:22:02 VS. development%1:04:01 0.3738 thanks%1:04:00 VS. thanks%1:10:00\n0.8617 rate%1:21:00 VS. rate%1:28:00 0.6922 issue%1:10:00 VS. issue%1:09:01 0.3728 negotiation%1:04:00 VS. negotiation%1:10:00\n0.8575 gender%1:07:00 VS. gender%1:10:00 0.6889 ownership%1:04:00 VS. ownership%1:21:00 0.3668 behalf%1:07:00 VS. behalf%1:04:00\n0.8562 material%1:10:00 VS. material%1:27:00 0.6887 limit%1:28:00 VS. limit%1:07:00 0.3621 backing%1:06:00 VS. backing%1:04:00\n0.8553 union%1:15:00 VS. union%1:14:01 0.6883 rhythm%1:07:01 VS. rhythm%1:10:01 0.3585 technology%1:09:00 VS. technology%1:04:00\n0.8512 relation%1:04:01 VS. relation%1:03:00 0.6867 water%1:17:00 VS. water%1:27:00 0.3484 sin%1:04:00 VS. sin%1:07:00\n0.8472 background%1:09:00 VS. background%1:07:00 0.6858 source%1:10:00 VS. source%1:15:00 0.2936 surprise%1:11:00 VS. surprise%1:12:00\n0.8469 circle%1:14:00 VS. circle%1:25:00 0.6852 boost%1:07:00 VS. boost%1:04:00\n0.8467 condition%1:10:01 VS. condition%1:26:00 0.6811 bronze%1:06:00 VS. bronze%1:27:00\n0.8467 paper%1:10:01 VS. paper%1:27:00 0.6807 draft%1:19:00 VS. draft%1:21:00\nTable 14: List of homograph concepts considered, sorted by the AP⋆\nc obtained by GPT2-L.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7740198373794556
    },
    {
      "name": "Transformer",
      "score": 0.6484348177909851
    },
    {
      "name": "Generalization",
      "score": 0.6035932302474976
    },
    {
      "name": "Machine learning",
      "score": 0.5959872603416443
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5814337730407715
    },
    {
      "name": "Binary number",
      "score": 0.4733426868915558
    },
    {
      "name": "Natural language processing",
      "score": 0.4486694931983948
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.43829068541526794
    },
    {
      "name": "Binary classification",
      "score": 0.43810051679611206
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.4378678798675537
    },
    {
      "name": "Programming language",
      "score": 0.11988908052444458
    },
    {
      "name": "Mathematics",
      "score": 0.10385039448738098
    },
    {
      "name": "Arithmetic",
      "score": 0.08116558194160461
    },
    {
      "name": "Engineering",
      "score": 0.07971367239952087
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Support vector machine",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 15
}