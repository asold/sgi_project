{
  "title": "GPTR: Gestalt-Perception Transformer for Diagram Object Detection",
  "url": "https://openalex.org/W4382240596",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5100602791",
      "name": "Xin Hu",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A5100384144",
      "name": "Lingling Zhang",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A5100361885",
      "name": "Jun Liu",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A5047282005",
      "name": "Jinfu Fan",
      "affiliations": [
        "Tongji University"
      ]
    },
    {
      "id": "https://openalex.org/A5100658703",
      "name": "Yang You",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A5065437706",
      "name": "Yaqiang Wu",
      "affiliations": [
        "Lenovo (China)",
        "Xi'an Jiaotong University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6681893956",
    "https://openalex.org/W3214167028",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W3106780750",
    "https://openalex.org/W3146251098",
    "https://openalex.org/W6605203061",
    "https://openalex.org/W3118694826",
    "https://openalex.org/W3210932363",
    "https://openalex.org/W6761108903",
    "https://openalex.org/W3123547918",
    "https://openalex.org/W3146175482",
    "https://openalex.org/W3199573137",
    "https://openalex.org/W3039437242",
    "https://openalex.org/W3200998783",
    "https://openalex.org/W6698632916",
    "https://openalex.org/W2746097825",
    "https://openalex.org/W6689029123",
    "https://openalex.org/W6742348326",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W6754632766",
    "https://openalex.org/W4226013992",
    "https://openalex.org/W6800709815",
    "https://openalex.org/W3178826664",
    "https://openalex.org/W1965372883",
    "https://openalex.org/W2950628590",
    "https://openalex.org/W6642652525",
    "https://openalex.org/W3111272232",
    "https://openalex.org/W3199093552",
    "https://openalex.org/W6752253332",
    "https://openalex.org/W3213428576",
    "https://openalex.org/W2787839673",
    "https://openalex.org/W2811124557",
    "https://openalex.org/W3171841353",
    "https://openalex.org/W4221138453",
    "https://openalex.org/W3141624005",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W126405584",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4250533120",
    "https://openalex.org/W2307512708",
    "https://openalex.org/W2951659295",
    "https://openalex.org/W4287394079",
    "https://openalex.org/W4287556569",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3019546484",
    "https://openalex.org/W2040553845",
    "https://openalex.org/W4287375617",
    "https://openalex.org/W4292692470",
    "https://openalex.org/W4235132993",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W4214627427",
    "https://openalex.org/W3175888430",
    "https://openalex.org/W3212609364",
    "https://openalex.org/W2962766617",
    "https://openalex.org/W2989604896",
    "https://openalex.org/W2988916019",
    "https://openalex.org/W3215000439",
    "https://openalex.org/W3176459575",
    "https://openalex.org/W1969294188",
    "https://openalex.org/W4287250326"
  ],
  "abstract": "Diagram object detection is the key basis of practical applications such as textbook question answering. Because the diagram mainly consists of simple lines and color blocks, its visual features are sparser than those of natural images. In addition, diagrams usually express diverse knowledge, in which there are many low-frequency object categories in diagrams. These lead to the fact that traditional data-driven detection model is not suitable for diagrams. In this work, we propose a gestalt-perception transformer model for diagram object detection, which is based on an encoder-decoder architecture. Gestalt perception contains a series of laws to explain human perception, that the human visual system tends to perceive patches in an image that are similar, close or connected without abrupt directional changes as a perceptual whole object. Inspired by these thoughts, we build a gestalt-perception graph in transformer encoder, which is composed of diagram patches as nodes and the relationships between patches as edges. This graph aims to group these patches into objects via laws of similarity, proximity, and smoothness implied in these edges, so that the meaningful objects can be effectively detected. The experimental results demonstrate that the proposed GPTR achieves the best results in the diagram object detection task. Our model also obtains comparable results over the competitors in natural image object detection.",
  "full_text": "GPTR: Gestalt-Perception Transformer for Diagram Object Detection\nXin Hu1,2, Lingling Zhang1,2\u0003, Jun Liu1,2, Jinfu Fan3, Yang You4, Yaqiang Wu2,5\n1 Shaanxi Provincial Key Laboratory of Big Data Knowledge Engineering,\nSchool of Computer Science and Technology, Xi’an Jiaotong University, China\n2 National Engineering Lab for Big Data Analytics, Xi’an Jiaotong University, China\n3 Department of Control Science and Engineering, Tongji University, Shanghai, China\n4 Department of Computer Science, National University of Singapore, Singapore\n5 Lenovo Research, Beijing, China\ndr.huxin711@foxmail.com, fzhanglling, liukeeng@xjtu.edu.cn, 1910648@tongji.edu.cn, youy@comp.nus.edu.sg,\nwuyqe@lenovo.com\nAbstract\nDiagram object detection is the key basis of practical appli-\ncations such as textbook question answering. Because the di-\nagram mainly consists of simple lines and color blocks, its\nvisual features are sparser than those of natural images. In ad-\ndition, diagrams usually express diverse knowledge, in which\nthere are many low-frequency object categories in diagrams.\nThese lead to the fact that traditional data-driven detection\nmodel is not suitable for diagrams. In this work, we propose a\ngestalt-perception transformer model for diagram object de-\ntection, which is based on an encoder-decoder architecture.\nGestalt perception contains a series of laws to explain hu-\nman perception, that the human visual system tends to per-\nceive patches in an image that are similar, close or connected\nwithout abrupt directional changes as a perceptual whole ob-\nject. Inspired by these thoughts, we build a gestalt-perception\ngraph in transformer encoder, which is composed of diagram\npatches as nodes and the relationships between patches as\nedges. This graph aims to group these patches into objects\nvia laws of similarity, proximity, and smoothness implied in\nthese edges, so that the meaningful objects can be effectively\ndetected. The experimental results demonstrate that the pro-\nposed GPTR achieves the best results in the diagram object\ndetection task. Our model also obtains comparable results\nover the competitors in natural image object detection.\n1 Introduction\nThe goal of object detection (Liu et al. 2020; Guo et al. 2021;\nDong et al. 2021) is to accurately locate and classify all ob-\njects in a given image, which is indeed dominated by various\ndeep neural networks (Pan et al. 2021; Wu et al. 2021; Zhong\net al. 2021; Chen et al. 2021b; Cao et al. 2021; Wang et al.\n2021). For this task, it is very important to understand the\ndetailed and implicit semantic information of images. It also\nhas great signiﬁcance in practical applications such as visual\nquestion answering (Yuan et al. 2021), cross-modal retrieval\n(Chen et al. 2021a; Diao et al. 2021), etc.\nMost existing detectors are designed for natural images of\nvariant objects, while the research on diagram object detec-\ntion is still blank. Diagram is a special kind of image, which\n\u0003Corresponding author.\nCopyright c\r 2023, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: Efﬁcient diagram object detectors can assist text-\nbook question answering. Q, A, and C represent question\ntext, candidate answer, and caption respectively.\nusually consists of simple lines and color blocks, and exists\nin many ﬁelds such as pedagogy and architecture (Hu et al.\n2021). Diagram object detection is a key step in many ap-\nplications as shown in Figure 1. On this basis, it plays an\nimportant role in smart education and so on. Taking text-\nbook question answering (Kembhavi et al. 2017; He et al.\n2021) as an example, given the diagram and question text,\ndiagram object detector outputs the locations and categories\nof objects in the diagram. And then, these objects interact\nwith the question text multimodally to facilitate the answer\nof the question. However, detectors for natural images can-\nnot be directly applied to diagram object detection. We adopt\nsome mainstream detectors to conduct experiments on nat-\nural images and diagrams, respectively. Taking the recent\nSAM-DETR (Zhang et al. 2022) model as an example, the\naverage precision of this model on natural images is as high\nas 39%, while the precision on diagrams drops to about 15%.\nSee Section 4 for more analyses of experimental results. The\nreason is that the diagram has two characteristics different\nfrom natural image. On the one hand, the visual features\nof diagram are sparser than those of natural image.As\nshown in Figure 2 (a), the frequency distribution histograms\nare drawn corresponding to the RGB values for all pixels\nfrom the diagrams in AI2D* dataset and the natural images\nin MSCOCO (Lin et al. 2014). We can see that the RGB\nvalue distribution of the natural image is more balanced than\nthat of the diagram, and the distribution of three RGB color\ncomponents in the diagram is extremely uneven, which is\nconcentrated around 255. This phenomenon illustrates the\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n899\nFigure 2: Comparative analyses of characteristics between diagram dataset AI2D* and natural image dataset MSCOCO.\npresence of a large amount of white in diagrams and the\nrest of the color information is scarce. White usually repre-\nsents the background, which contains almost useless infor-\nmation. Therefore, there are a large number of white back-\ngrounds in the diagrams, resulting in sparser visual features\nand fewer pixels occupied by foreground content compared\nwith the natural images. On the other hand, the ratio of\nlow-frequency object categories is larger in the diagram.\nIn Figure 2 (b), the orange line depicts the long-tail distri-\nbution of object category in diagram dataset AI2D*. For\nMSCOCO, there is little difference in the frequency of all\nobject categories. In summary, the existing detectors are not\nsuitable for the task of diagram object detection.\nHow can humans efﬁciently identify the objects? Ac-\ncording to the process of human perception (Wagemans\net al. 2012; Pomerantz, Sager, and Stoever 1977), the hu-\nman visual system tends to perceive patches in an image\nthat are similar, close or connected without abrupt direc-\ntional changes as a perceptual whole object. For example,\nin a jigsaw puzzle, humans consciously splice two patches\nwith similar colors and close positions into a whole, and the\nspliced object has a smooth and continuous contour. Gestalt\nperception contains a series of laws to explain human per-\nception, such as laws of similarity, proximity, closeness,\nsmoothness, symmetry and so on. The diagram is drawn\nby experts and the object recognition process conforms to\nthe gestalt perception theory (Wertheimer 1922; H¨orhan and\nEidenberger 2021; Desolneux, Moisan, and Morel 2004).\nAmong them, similarity, proximity and smoothness laws\nplay an important role in recognizing objects.\nInspired by this, we propose a Gestalt-Perception\nTRansformer model for diagram object detection (GPTR).\nGPTR is based on the transformer encoder-decoder archi-\ntecture, and the main module is the gestalt-perception graph\nnamed GPG that is constructed during encoding. Gestalt\nlaws are used as prior knowledge to guide the aggregation\nof diagram patches to form reasonable objects, without re-\nlying on large amounts of annotations. The way of dividing\nthe diagram into patches is the same as that of dividing the\nimage into patches in (Dosovitskiy et al. 2020). Speciﬁcally,\nGPG is composed of diagram patches as nodes and the rela-\ntionships between patches as edges. Node features in GPG\nare obtained by three gestalt-visual branches, namely color\nbranch, position branch and edge branch. Edge weights of\nthe graph are adaptively learned by the laws of color simi-\nlarity, position proximity, and contour smoothness. The de-\ncoder of GPTR decodes the object queries in parallel and\npredicts the ﬁnal location and classiﬁcation results. Our\nmain contributions are summarized as follows:\n\u000fAs far as we know, we put forward the diagram object\ndetection task for the ﬁrst time. Due to the problems of\nsparser visual features and more low-frequency objects\nof diagrams than those of natural images, we propose a\nnovel gestalt-perception model to complete this task. The\nmodel is based on transformer architecture, and it can\nsimulate the process of human visual perception to learn\nbetter features for diagram object detection.\n\u000fWe build a gestalt-perception graph, in which the adap-\ntive learning strategy of gestalt-visual branches simulates\nhumans to combine the diagram patches into more mean-\ningful objects in accordance with the gestalt laws. In ad-\ndition, we adopt the multi-scale attention mechanism to\nproduce better query initialization.\n\u000fWe conduct experiments on a diagram dataset AI2D* and\na benchmark MSCOCO of natural images to verify the\neffectiveness of GPTR. The experimental results show\nthat our model achieves the best results in the diagram\nobject detection task, and also obtains comparable results\nover the competitors in natural images.\n2 Related Work\nThis section mainly introduces DETR-series detection mod-\nels and the gestalt perception theory.\nDETR-Series Detection. DETR (Carion et al. 2020) is the\nﬁrst end-to-end transformer-based detection model, which\neffectively removes the need for many hand-designed com-\nponents. Subsequently, there are some improved models.\nDeform-DETR (Zhu et al. 2020) designs a deformable atten-\ntion module, which attends to a small set of sampling loca-\ntions for prominent key elements out of all the feature map\npixels. ConditionDETR (Meng et al. 2021) learns a condi-\ntional spatial query from decoder embedding, while DAB-\nDETR (Liu et al. 2021) presents a novel query formulation\nusing dynamic anchor boxes for DETR. SMCA-DETR con-\nstrains co-attention responses to be high near initially esti-\nmated bounding box locations. SAM-DETR interpretes its\ncross-attention as a matching and distillation process and\nsemantically aligns object queries with encoded image fea-\ntures to facilitate their matching. The above models adopt\n900\nFigure 3: The overview architecture of our proposed gestalt-perception transformer model (GPTR). At each transformer encoder\nlayer, we construct gestalt-perception graph to aggregate node features according to different laws. The whole process is stacked\nfor Llayers and the decoder layer is used to decode M object queries in parallel to predict the location and category of objects.\nconvolutional network, which cannot effectively represent\ndiagrams because of the sparse visual features.\nGestalt Perception Theory. According to the process of\nhuman perception (Pomerantz, Sager, and Stoever 1977),\ncharacterized by the laws of similarity, proximity, and conti-\nnuity, the human visual system tends to perceive objects that\nare similar, close or connected without abrupt directional\nchanges as a perceptual whole. For example, GLGOV (Yan\net al. 2018) is guided by the gestalt laws of perception for\nimage saliency detection with a bottom-up mechanism. In-\nspired by the gestalt laws of feature grouping, we propose a\ngestalt-perception model. Similarity, proximity and smooth-\nness laws are considered in our work.\n3 The GPTR Model\nThe overall architecture of GPTR is depicted in Figure 3.\nIt follows the encoder-decoder transformer and mainly con-\ntains three components: 1) the pre-projection module maps\nthe patch features from different visual branches to the same\ndimensional space to initialize GPTR model; 2) the trans-\nformer encoder is built by gestalt-perception graph, which\naims to model the relationships between the diagram patches\nand group these patches into objects via gestalt laws, so that\nthe meaningful features can be better processed by the detec-\ntor; 3) the transformer decoder transforms the object queries\nthat represented by learnable positional embeddings into an\noutput embedding and makes the ﬁnal prediction with a\nfeed-forward neural network (FFN). GPTR model is opti-\nmized with classiﬁcation loss and box regression loss that\nare same as DETR (Carion et al. 2020). These three compo-\nnents are detailed in the following subsections.\n3.1 Pre-Projection Module\nThe global features generated by only convolutional back-\nbone network can not effectively represent diagrams be-\ncause of the sparse visual features of diagrams. In order\nto make up for this limitation, we divide the diagram into\nlocal-level patches and let the GPTR model focus on the\ndetails of the diagram. Speciﬁcally, given an initial dia-\ngram d 2RH0\u0002W0\u00023, we reshape it into a set of patches\ndP = fdP\ni 2 R\nH0\np\nN \u0002 W0p\nN \u00023;i = 1;\u0001\u0001\u0001 ;Ngthat is same\nas (Dosovitskiy et al. 2020). (H0;W0) is the resolution of\ndiagram d and 3 means three color channels. N is the to-\ntal number of patches in a diagram. Then, the pre-projection\nmodule learns patch features by feedingdP into three differ-\nent gestalt-visual branches. In addition, this module adopts\nanother MLP layer to project the backbone feature of the\ndiagram into the d-dimension signed as FS.\nColor Branch (CB) maps per patch dP\ni into a 9-dim\ncolor feature fCB\ni , and all the features consist of the feature\nset FCB = ffCB\ni 2R1\u00029;i = 1;\u0001\u0001\u0001 ;Ng. Speciﬁcally,\nfCB\ni is concatenated by three central moments (Stricker and\nOrengo 1995) of each color channel. These three moments\nrepresent mean feature, variance feature, and skewness fea-\nture of color distribution, respectively.\nPosition Branch(PB) outputs a position feature fPB\ni for\neach patch, and FPB = ffPB\ni 2R1\u00024;i = 1;\u0001\u0001\u0001 ;Ng.\nfPB\ni is composed of the coordinates of the top left corner\n(xi0;yi0) and the bottom right corner (xi1;yi1) of patch dP\ni .\nEdge Branch(EB) represents each patch dP\ni as the pixel\nvalues of top, bottom, left, and right edges. We use Canny\nalgorithm (Canny 1986) converting 3-channel patch into 1-\nchannel contour map. Then, the pixel values of the four\nedges of each contour map are concatenated as the edge fea-\nture for per patch.FEB = ffEB\ni 2R1\u0002(2\u0002 W0\np\nN +2\u0002 H0p\nN );i =\n1;\u0001\u0001\u0001 ;Ngdenotes a set of edge features for each diagram.\nWe use FEB\n(t;b) and FEB\n(l;r) to distinguish the top and bottom\nedge features from the left and right edge features.\nIn order to facilitate the construction and updating of the\ngestalt-perception graph, the pre-projection module adopts\nthree kinds of MLP layers mapping low-dimensional visual\nfeatures into high-dimensional ones as shown in follows.dc,\ndp and de are mapping dimensions for color, position and\n901\nFigure 4: Example of color similarity, position proximity, and contour smoothness. A, B and C in (b) indicate three randomly\nsampled patches in the diagram, and the white curve refers to the contour existing in the patch. /okand /removerespectively indicate\nwhether the gestalt law is conformed to.\nedge features. jjdenotes the concatenating operator in (2).\n^FCB = MLPCB (FCB ); ^FCB 2RN\u0002dc;\n^FPB = MLPPB (FPB ); ^FPB 2RN\u0002dp;\n(1)\n^FEB = MLPEB(FEB\n(t;b))jjMLPEB(FEB\n(l;r));\n^FEB 2RN\u00024\u0002de:\n(2)\n3.2 Gestalt-Perception Graph in Encoder\nAccording to the process of human perception, the human\nvisual system tends to perceive similar, close, or connected\npatches as a perceptual whole object. As shown bythe color\nsimilarity and position proximityin Figure 4 (a), for the\ncactus patch and sun patch, because their color features are\nquite different, the two patches may not belong to the same\nobject according to the law of color similarity. The same\ngreen grass patch and cactus patch are consistent with color\nsimilarity, but they are far away in spatial and do not meet\nthe law of position proximity. Consequently, they can not be-\nlong to the same object. On the contrary, two different cac-\ntus patches that are both green and close to each other can\nbe easily recognized as the same object. Takingthe contour\nsmoothness in Figure 4 (b) as an example, according to the\nleft and right edges for each patch, the features of the left\nedge of patch B and the right edge of patch A are similar,\nwhich means that the contours in B and A can be connected\ninto a smooth curve, and they are more likely belong to the\nsame object. On the contrary, the features of the left edge of\npatch B and the right edge of patchC are quite different, that\nis, if B and C are spliced into one object, it does not meet the\nhuman perception of the contour smoothness law.\nThese gestalt laws, as a kind of priori knowledge, guide\nhuman to effectively identify the objects in the diagrams\nwithout relying on a large annotated dataset. Therefore,\ngestalt-perception based method can learn good representa-\ntions for low-frequency objects. Inspired by this, GPTR de-\nsigns a gestalt-perception graph (GPG) and it is composed\nof diagram patches as nodes and the relationships between\npatches as edges. GPG consists of three subgraphs, in other\nwords, similarity, proximity and smoothness are encoded by\nthe edges on subgraphs GCB , GPB and GEB, respectively.\nColor Similarity. GCB = (NCB ;ECB ) is a subgraph for\nmodeling color similarity between patches.NCB = ^FCB 2\nRN\u0002dc indicates N nodes, each node is a dc-dimension\ncolor feature for one patch. ECB \u0012N CB \u0002NCB repre-\nsents the color similarity between nodes. Speciﬁcally, given\ntwo node features ^FCB\ni and ^FCB\nj , the weight of ECB\nij is\ngiven by (3), where sim( \u0001) is a cosine similarity function\nand i;j = f1;\u0001\u0001\u0001 ;Ng.\nWCB\nij = sim( ^FCB\ni ; ^FCB\nj ): (3)\nPosition Proximity. In order to measure the proximity\nof spatial positions, GPB = (NPB ;EPB ) is formulated\nto learn the positional relation between two patches. Con-\ncretely, NPB indicates the nodes set with position feature\n^FPB 2RN\u0002dp, and EPB is denoted as position proxim-\nity between each pair of nodes in NPB . The weight of\nEPB\nij is shown in (4). The parameter \u000e is ﬁxed as 0.1 and\ni;j = f1;\u0001\u0001\u0001 ;Ng.\nWPB\nij = exp(\u0000\nq\nPdp\nt=1( ^FPB\nit \u0000^FPB\njt )2\n\u000e ): (4)\nContour Smoothness. The law of contour smoothness is\none of the gestalt laws that states humans perceive objects\nas continuous in a smooth pattern, which means that ob-\nject usually contains a smooth contour. In order to judge\nwhether two patches may belong to the same object, GEB\nis constructed to measure the feature consistency of the top,\nbottom, left and right edges between patches. Speciﬁcally,\nGEB is deﬁned as GEB = (NEB;EEB). NEB indicates the\nnodes with edge features ^FEB 2RN\u00024\u0002de and EEB deter-\nmines the possibility of splicing two patches. The weight of\nEEB is computed as follows, where i;j = f1;\u0001\u0001\u0001 ;Ngand\n^FEBb\ni represents the bottom edge feature of patch dP\ni .\n\u001b1 = sim( ^FEBb\ni ; ^FEBt\nj ); \u001b2 = sim( ^FEBt\ni ; ^FEBb\nj ); (5)\n\u001b3 = sim( ^FEBl\ni ; ^FEBr\nj ); \u001b4 = sim( ^FEBr\ni ; ^FEBl\nj ); (6)\nWEB\nij = maxf\u001b1;\u001b2;\u001b3;\u001b4g: (7)\nGPG Grouping with an Assignment Matrix.For aggre-\ngating patch features to obtain meaningful object features,\nwe denote a learned assignment matrix (Ying et al. 2018)\nat layer l as S(l) 2RNl\u0002Nl+1 , where Nl is the number of\nnodes at layer l. It provides a soft assignment of each node\n902\nat layer l to layer l+ 1. Taking GCB as an example, when\nl = 0, ^FCB(l\u00001) in (8) denotes the output of pre-projection\nmodule. The node feature ^FCB(l) at layer l is computed\nby (9). The node update method of GPB and GEB is sim-\nilar as that of GCB . GPG concatenates ^FCB(l), ^FPB (l) and\n^FEB(l), where \u000b, \f and \r are three learnt adaptive weight\ncoefﬁcients. Then, a self-attention layer (SA) is applied to\ngenerate the ﬁnal visual feature FV (l) as shown in (10).\n~FCB(l\u00001) = WCB(l\u00001) \u0002^FCB(l\u00001); (8)\n^FCB(l) = S(l)>\n\u0002~FCB(l\u00001); (9)\nFV (l) = SA(\u000b\u0002^FCB(l)jj\f\u0002^FPB (l)jj\r\u0002^FEB(l)): (10)\nGPTR updates the diagram feature of visual enhancement\nthrough cross-attention strategy (CA), then a self-attention\nlayer (SA) and a feed-forward layer (FFN) are stacked to\nform a transformer encoder layer. FS in (11) and (12) indi-\ncates the high-level semantic feature extracted from Convo-\nlution Neural Network (CNN).\nCA(FS;FV (l)) =softmax(FS;FV (l)>\n) \u0002FV (l); (11)\nF(l)\nENCODER = FFN(SA(CA(FS;FV (l)) +FS)): (12)\n3.3 Multi-Scale Visual-Enhanced Decoder\nThe decoder follows the standard architecture of Condition-\nDETR (Meng et al. 2021), transforming M embeddings\nusing multi-head self-attention and cross-attention mecha-\nnisms. Unlike ConditionDETR decoder, which receives zero\nset as initial queries, we consider the human visual percep-\ntion. When recognizing objects in diagrams, humans follow\nthe process of visual perception to identity variety of objects\nwith different scales. Inspired by this, our GPTR designs\na multi-scale attention mechanism named MSA, to acquire\nbetter initial query features.\nFirstly, the output of each layer of GCB , GPB and GEB\nis taken as the multi-scale visual feature. Taking GCB as\nan example, the multi-scale color feature is recorded as\nMSCB = [ ^FCB(1);\u0001\u0001\u0001 ; ^FCB(l);\u0001\u0001\u0001 ; ^FCB(L)]. The score\nof color feature SCORE CB is computed by a single-layer\nMLP as shown in (13), and top-M color feature ~MS\nCB\nis\nselected according to the score. ~MS\nPB\nand ~MS\nEB\nare ac-\nquired in the same way as ~MS\nCB\n. The ﬁnal selected multi-\nscale visual feature is written as MSV = ~MS\nCB\n+ ~MS\nPB\n+\n~MS\nEB\n. In (14), GPTR ﬁrst concatenates the MS V and the\noutput feature F(L)\nENCODER of the L-layer encoder, and\nthen obtains the enhanced features through the self-attention\nmechanism SA. [: M] indicates that the ﬁrst M features are\nselected as the initial query representation QUERY.\nSCORECB = softmax(MLP(jjL\nl=1 ^FCB(l))); (13)\nQUERY = SA(MSV jjF(L)\nENCODER)[: M]: (14)\nLoss Function. We follow DETR to ﬁnd an optimal bi-\npartite matching (Kuhn 1955) between the predicted and\nground-truth objects using the Hungarian algorithm, and\nthen form the loss function for optimizing GPTR model.\nFocal loss (Lin et al. 2017) is used for classiﬁcation and\nGIoU loss (Rezatoﬁghi et al. 2019) for box regression, both\nof which are the same as DETR.\n4 Experiments\n4.1 Datasets\nIn this work, we evalute the baselines and our GPTR model\nboth on the diagram and the natural image datasets.\nAI2D* is composed of diagrams in the original AI2D\ndataset (Kembhavi et al. 2016), and the topic is grade school\nscience. AI2D is mainly used to verify the question and an-\nswering task. We annotate it with more ﬁne-grained details,\nincluding the spatial coordinates and category labels of ob-\njects in per diagram. The novel AI2D* dataset contains total\n557 object categories and it is divided into a train set with\n1,634 diagrams and a test set with 404 diagrams.\nMSCOCO (Lin et al. 2014) is a large-scale object detec-\ntion dataset with 80 categories. It comprises 118,287 images\nfor training and 5,000 images for testing.\n4.2 Experimental Settings\nGPTR Implementation. Our architecture is almost the\nsame with the DETR-like architecture and contains the\nCNN backbone, transformer encoder and transformer de-\ncoder. The main difference is that we introduce the details\nof gestalt-perception graph in transformer encoder. For the\ngestalt visual preprocessing, we resize all images of two\ndatasets to 224\u0002224\u00023 and each image is divided into 196\npatches. The dimension of per patch feature is dc = dp =\nde =256. We set 50 and 100 object queries for AI2D* and\nMSCOCO datasets, respectively.\nTraining and Evaluation. The learning rate is initially\nset to 10\u00004 and the AdamW optimizer is used in GPTR.\nThe weight decay is set to be 10\u00004 and the dropout rate\nin transformer is 0.1. We use the standard COCO evalua-\ntion introduced in (Meng et al. 2021), and we also report the\naverage precision (AP), and the AP scores at 0.50 (AP50),\n0.75 (AP75) and for the small (APS), medium (APM), and\nlarge (APL) objects. For fair comparison, we adopt the same\nequipment and settings, such as the layer numberLfor trans-\nformer encoder and decoder, and the number of attention\nheads H inside the transformer’s attentions, to rerun all the\nbaseline models for several times, and then record the av-\nerage results. All the models are trained and evaluated on\nNVIDIA Tesla V100 GPU.\n4.3 Performance Comparison\nDiagram Object Detection. We conduct this experiment\non the AI2D* dataset and the proposed GPTR achieves the\nbest results compared with all the competitors. One can ﬁnd\nin Table 1 that DETR with 1,000 training epochs performs\nmuch worse than ConditionDETR with only 100 epochs.\nThe performance of our GPTR is 1.8% to 4.5% higher than\n903\nModels L H BS Epoch AP AP50 AP75 APS APM APL params\nCenterNet (Duan et al. 2019) / / 16 500 8.6 13.2 9.9 10.7 13.4 12.5 50.39M\nRetinaNet (Lin et al. 2017) / / 16 100 10.5 16.3 11.4 6.0 12.8 14.9 29.86M\nDETR (Carion et al. 2020) 4 4 16 1000 10.5 18.3 11.0 6.6 13.7 13.9 28.93M\nConditionDETR (Meng et al. 2021) 4 4 16 100 11.5 18.5 12.7 10.4 15.1 15.2 29.22M\nGPTR(Ours) 4 4 16 100 14.1 23.0 15.6 12.2 18.4 18.9 30.56M\nDeform-DETR (Zhu et al. 2020) 6 8 8 100 11.8 16.8 14.2 13.9 15.8 16.6 35.11M\nDAB-DETR (Liu et al. 2021) 6 8 8 100 10.8 17.1 12.0 14.9 14.1 14.6 41.55M\nSMCA-DETR (Gao et al. 2021) 6 8 8 300 13.8 21.7 15.4 10.5 18.1 18.4 39.66M\nSAM-DETR (Zhang et al. 2022) 6 8 8 200 14.6 21.7 16.6 10.9 19.0 18.5 47.08M\nAnchorDETR (Wang et al. 2022) 6 8 8 120 15.6 23.5 17.3 14.8 19.4 20.5 32.22M\nGPTR(Ours) 6 8 8 120 16.1 24.6 18.4 15.3 21.1 21.5 33.44M\nTable 1: The precision (%) comparison on challenging AI2D* dataset for diagram object detection. L, H, and BS represent\nthe layer number of transformer encoder-decoder, the number of attention heads and batchsize respectively.\nModels L H BS Epoch AP AP50 AP75 APS APM APL\nCenterNet (Duan et al. 2019) / / 32 100 20.1 39.6 16.6 7.5 22.8 29.1\nRetinaNet (Lin et al. 2017) / / 16 100 25.5 42.4 26.2 10.7 27.6 38.0\nDETR (Carion et al. 2020) 4 4 16 1000 30.2 49.7 30.6 10.1 31.4 47.2\nConditionDETR (Meng et al. 2021) 4 4 16 50 31.9 52.4 32.6 13.7 34.0 48.7\nGPTR(Ours) 4 4 16 50 32.1 52.4 33.2 13.8 34.7 48.8\nSMCA-DETR (Gao et al. 2021) 6 8 8 50 28.9 50.3 28.7 9.9 30.7 46.9\nSAM-DETR (Zhang et al. 2022) 6 8 8 50 39.0 60.5 40.8 19.7 42.5 58.0\nGPTR(Ours) 6 8 8 50 37.2 57.8 38.2 14.0 41.6 54.9\nSAM-DETR+GPG 6 8 8 50 39.3 61.5 41.2 19.5 43.3 58.9\nTable 2: The precision (%) comparison on benchmark MSCOCO dataset for natural image object detection.\nthat of ConditionDETR in all AP scores. Compared with\nSMCA-DETR and SAM-DETR, GPTR achieves better re-\nsults in all AP scores. Concretely, GPTR is 2.3% and 1.5%\nhigher in AP than SMCA-DETR and SAM-DETR, respec-\ntively. Especially for small objects, GPTR has a gain of 4.8%\nin APS than that of SMCA-DETR. In addition, the parame-\nters of SAM-DETR and SMCA-DETR are much more than\nour GPTR model, and the training time of GPTR is only\nless than half that of SMCA-DETR. Also, our GPTR out-\nperforms the recently proposed AnchorDETR model in all\nAP scores, especially 0.5% higher in AP.\nNatural Image Object Detection. Although GPTR\nmodel is especially proposed for the diagram object detec-\ntion, it can also be applied to the object detection in natu-\nral images. For natural images, the patches also meet three\ngestalt laws of color similarity, position proximity and con-\ntour smoothness. The performance of GPTR is veriﬁed on\nMSCOCO with natural images as shown in Table 2. One can\nﬁnd that our model achieves competitive results on this task.\nSpeciﬁcally, DETR works better than the anchor-free mod-\nels CenterNet and RetinaNet, but it converges more slowly.\nThe ConditionDETR model is built on the DETR model,\nwith higher AP scores and faster convergence. Compared\nwith ConditionDETR, our GPTR still achieves the best per-\nformance under the same experimental settings, and GPTR\nis 8.3% and 4.1% higher than SMCA-DETR in AP and\nAPS scores, respectively. In addition, the designed gestalt-\nperception graph (GPG) in transformer encoder can be ﬂex-\nModel AdaB VQ MSA AP APS APM APL\nGVB-CPE - - - 11.6 10.7 15.5 15.6\nGVB-CPEa X 11.9 11.6 16.0 16.5\nGVB-CPEb X X 12.7 11.7 16.9 18.0\nGPTR X X X 14.1 12.2 18.4 18.9\nTable 3: Ablation studies on the AI2D* dataset. “AdaB”\nindicates the adaptive combination of three gestalt-visual\nbranches. “VQ” represents the visual-guided query initial-\nization. “MSA” means adding multi-scale attention mecha-\nnism to GVB-CPEb to generate better query representations.\nibly added to SAM-DETR model, and the performance of\nSAM-DETR+GPG is improved in almost all the AP scores.\nDiagrams vs. Natural Images.From the experimental re-\nsults in Table 1 and Table 2, it can be seen that the GPTR out-\nperforms almost all the competitors in AP scores. In partic-\nular, compared with natural image object detection, GPTR\nimproves the performance of diagram object detection more\nsigniﬁcantly. In other words, the gestalt laws in GPTR are\nmore effective for the representation of diagrams. The rea-\nson is that the visual features of diagrams are sparse, and\nthere are many low-frequency object categories. As a kind of\nprior knowledge of human cognition, gestalt laws can effec-\ntively learn the visual features of diagrams without relying\non a large amount of labeled dataset, and alleviate the limi-\ntations of learning low-frequency object representations.\n904\nModel CB PB EB AP APS APM APL\n-ALL - - - 11.4 10.3 15.1 15.2\nGVB-C X 11.0 8.6 14.3 14.6\nGVB-P X 11.2 7.4 14.9 14.8\nGVB-E X 10.6 9.4 13.4 14.0\nGVB-CPE X X X 11.6 10.7 15.5 15.6\nTable 4: Ablation studies on the AI2D* dataset. “CB”, “PB”\nand “EB” represent color branch, position branch, and edge\nbranch respectively. “-All” refers the model that only uses\nCNN backbone for extracting diagram features.\n4.4 Ablation Studies\nThe performance of GPTR in diagram object detection is\nmainly improved in three aspects. They are gestalt-visual\nbranches in GPG module, visual-guided initialization for de-\ncoder queries, and multi-scale visual enhancement strategy.\nTo demonstrate the effectiveness of these aspects, we study\nthe ablation models and the differences between these ver-\nsions are shown in Table 3. 1) GVB-CPE represents the com-\nbination of three branches as visual features, and the com-\nbination mode is direct concatenate. 2) GVB-CPE a adopts\nadaptive learning method to combine the three branches on\nthe basis of GVB-CPE. 3) GVB-CPEb adopts visual-guided\ninitialization mechanism, and our GPTR model adds the\nmulti-scale mechanism to GVB-CPEb.\nThe experimental results are shown in Table 3. One can\nﬁnd that: 1) after using the adaptive method, GVB-CPE a\nhas signiﬁcantly improved the APS score compared with\nGVB-CPE model. 2) Compared with taking zero set as ini-\ntial queries in GVB-CPE a, the visual-guided initialization\nmechanism of GVB-CPEb has improved performance in all\nAP scores, especially 1.5% higher in APL. 5) Our GPTR,\nwhich adds multi-scale attention mechanism to GVB-CPEb,\nhas achieved the best results in all AP scores.\nSince three gestalt-visual branches are included in the\nGPTR model, we also analyze the effects of different vi-\nsual branches. As shown in Table 4, 1) GVB-C, GVB-P\nand GVB-E indicate that only the gestalt-visual branches of\ncolor, position and edge, respectively. 2) GVB-CPE is the\nsame model in Table 3. When three gestalt-visual branches\nare used separately, the performance of GVB-C, GVB-P and\nGVB-E is affected by the law-bias compared with -ALL\nmodel. 2) GVB-CPE has improved in most AP scores that\ncompared with all the ablation models.\n4.5 Qualitative Results\nVisualization of Detection Results.The detection results\nfor ConditionDETR and GPTR are shown in Figure 5. For\nthe ﬁrst case, when two orange are close to each other in a\ndiagram, ConditionDETR confuses them as a whole object,\nwhile GPTR accurately locates them respectively. For the\nsecond case, the ﬁshtail in the bottom right corner is com-\nposed of two polygons. ConditionDETR recognizes it as two\nindependent objects, while GPTR accurately recognizes it\nas a whole ﬁshtail. For the third case, the moon, earth and\nlight in this diagram are close in space, and the light as the\nbackground affects the recognition of the foreground objects\nFigure 5: Qualitative results of ConditionDETR and our\nGPTR. We use the red bounding boxes to highlight the dif-\nferences in the detection results between these two models.\nFigure 6: The AP score of low-frequency category for Con-\nditionDETR and our GPTR.\nmoon and earth by the ConditionDETR. On the contrary,\nGPTR effectively separates the foreground and background,\nand then accurately locate the foreground objects.\nLow-Frequency Objects in AI2D* Dataset. Figure 6\nshows the AP score of the ConditionDETR and GPTR mod-\nels on low-frequency objects, respectively. For the conve-\nnience of visualization, the abscissa represents several ob-\nject categories selected with a frequency of no more than 10\ntimes, and the ordinate indicates the AP score. It can be seen\nthat the performance of GPTR is better than that of the Con-\nditionDETR for low-frequency object categories. Especially\nfor some categories that only appear once, such asartichoke\nand cauliﬂower in the red box, the performance of GPTR is\nabout 20% higher than that of ConditionDETR.\n5 Conclusion\nIn this paper, we propose a gestalt-perception transformer\nmodel (GPTR) for the novel diagram object detection. The\ngestalt laws, as a kind of priori knowledge, guide human\nto identify the objects without relying on a large dataset.\nFor the sparse visual features and low-frequency objects of\ndiagrams, GPTR constructs a gestalt-perception graph and\nthese laws are encoded by the graph edges. During updating,\nthe designed adaptive learning strategy effectively combine\nthe laws of similarity, proximity and smoothness to group\nthe diagram patches to objects. In addition, we adopt the\nmulti-scale mechanism based on the visual features to pro-\nduce better queries. We have demonstrated the effectiveness\nof GPTR in diagram object detection by achieving signif-\nicant performance improvements. However, there are still\nlimitations in the application of gestalt laws in this work.\nFor example, GPTR only uses three laws. How to mine\nother laws for diagram representation, and how multiple\nlaws work together will be the future research works.\n905\nAcknowledgments\nThis work was supported by National Key Research and\nDevelopment Program of China (2020AAA0108800), Na-\ntional Natural Science Foundation of China (62137002,\n61937001, 62192781, 62176209, 62176207, 62106190, and\n62250009), Innovative Research Group of the National Nat-\nural Science Foundation of China (61721002), Innovation\nResearch Team of Ministry of Education (IRT\n17R86),\nConsulting research project of Chinese academy of engi-\nneering “The Online and Ofﬂine Mixed Educational Ser-\nvice System for ‘The Belt and Road’ Training in MOOC\nChina”, “LENOVO-XJTU” Intelligent Industry Joint Lab-\noratory Project, CCF-Lenovo Blue Ocean Research Fund,\nProject of China Knowledge Centre for Engineering Science\nand Technology, Foundation of Key National Defense Sci-\nence and Technology Laboratory (6142101210201).\nReferences\nCanny, J. 1986. A computational approach to edge detec-\ntion. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, (6): 679–698.\nCao, Y .; Wang, J.; Jin, Y .; Wu, T.; Chen, K.; Liu, Z.; and Lin,\nD. 2021. Few-Shot Object Detection via Association and\nDIscrimination. Advances in Neural Information Processing\nSystems, 34.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In Proceedings of the European Confer-\nence on Computer Vision, 213–229. Springer.\nChen, J.; Hu, H.; Wu, H.; Jiang, Y .; and Wang, C. 2021a.\nLearning the best pooling strategy for visual semantic em-\nbedding. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 15789–15798.\nChen, Y .; Li, Y .; Kong, T.; Qi, L.; Chu, R.; Li, L.; and Jia,\nJ. 2021b. Scale-aware automatic augmentation for object\ndetection. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 9563–9572.\nDesolneux, A.; Moisan, L.; and Morel, J.-M. 2004. Gestalt\ntheory and computer vision. In Seeing, Thinking and Know-\ning, 71–101. Springer.\nDiao, H.; Zhang, Y .; Ma, L.; and Lu, H. 2021. Similarity\nReasoning and Filtration for Image-Text Matching. In Pro-\nceedings of the AAAI Conference on Artiﬁcial Intelligence ,\nvolume 35, 1218–1226.\nDong, N.; Zhang, Y .; Ding, M.; and Lee, G. H. 2021. Bridg-\ning Non Co-occurrence with Unlabeled In-the-wild Data for\nIncremental Object Detection. Advances in Neural Informa-\ntion Processing Systems, 34.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An Image is Worth 16x16\nWords: Transformers for Image Recognition at Scale. In In-\nternational Conference on Learning Representations.\nDuan, K.; Bai, S.; Xie, L.; Qi, H.; Huang, Q.; and Tian, Q.\n2019. Centernet: Keypoint triplets for object detection. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, 6569–6578.\nGao, P.; Zheng, M.; Wang, X.; Dai, J.; and Li, H. 2021. Fast\nconvergence of detr with spatially modulated co-attention.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, 3621–3630.\nGuo, J.; Han, K.; Wang, Y .; Wu, H.; Chen, X.; Xu, C.; and\nXu, C. 2021. Distilling object detectors via decoupled fea-\ntures. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 2154–2164.\nHe, J.; Fu, X.; Long, Z.; Wang, S.; Liang, C.; and Lin, H.\n2021. Textbook Question Answering with Multi-type Ques-\ntion Learning and Contextualized Diagram Representation.\nIn International Conference on Artiﬁcial Neural Networks,\n86–98. Springer.\nH¨orhan, M.; and Eidenberger, H. 2021. Gestalt descriptions\nfor deep image understanding. Pattern Analysis and Appli-\ncations, 24(1): 89–107.\nHu, X.; Zhang, L.; Liu, J.; Zheng, Q.; and Zhou, J. 2021.\nFs-DSM: Few-Shot Diagram-Sentence Matching via Cross-\nModal Attention Graph Model.IEEE Transactions on Image\nProcessing, 30: 8102–8115.\nKembhavi, A.; Salvato, M.; Kolve, E.; Seo, M.; Hajishirzi,\nH.; and Farhadi, A. 2016. A diagram is worth a dozen im-\nages. In Proceedings of the European Conference on Com-\nputer Vision, 235–251. Springer.\nKembhavi, A.; Seo, M.; Schwenk, D.; Choi, J.; Farhadi, A.;\nand Hajishirzi, H. 2017. Are you smarter than a sixth grader?\ntextbook question answering for multimodal machine com-\nprehension. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern recognition, 4999–5007.\nKuhn, H. W. 1955. The Hungarian method for the assign-\nment problem. Naval Research Logistics Quarterly, 2(1-2):\n83–97.\nLin, T.-Y .; Goyal, P.; Girshick, R.; He, K.; and Dollar, P.\n2017. Focal loss for dense object detection. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, 2980–2988.\nLin, T.-Y .; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-\nmanan, D.; Doll ´ar, P.; and Zitnick, C. L. 2014. Microsoft\ncoco: Common objects in context. InProceedings of the Eu-\nropean Conference on Computer Vision, 740–755. Springer.\nLiu, L.; Ouyang, W.; Wang, X.; Fieguth, P.; Chen, J.; Liu,\nX.; and Pietik¨ainen, M. 2020. Deep learning for generic ob-\nject detection: A survey. International Journal of Computer\nVision, 128(2): 261–318.\nLiu, S.; Li, F.; Zhang, H.; Yang, X.; Qi, X.; Su, H.; Zhu, J.;\nand Zhang, L. 2021. DAB-DETR: Dynamic Anchor Boxes\nare Better Queries for DETR. In International Conference\non Learning Representations.\nMeng, D.; Chen, X.; Fan, Z.; Zeng, G.; Li, H.; Yuan, Y .;\nSun, L.; and Wang, J. 2021. Conditional detr for fast training\nconvergence. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, 3651–3660.\nPan, T.-Y .; Zhang, C.; Li, Y .; Hu, H.; Xuan, D.; Changpinyo,\nS.; Gong, B.; and Chao, W.-L. 2021. On model calibration\nfor long-tailed object detection and instance segmentation.\nAdvances in Neural Information Processing Systems, 34.\n906\nPomerantz, J. R.; Sager, L. C.; and Stoever, R. J. 1977. Per-\nception of wholes and of their component parts: some con-\nﬁgural superiority effects. Journal of Experimental Psychol-\nogy: Human Perception and Performance, 3(3): 422.\nRezatoﬁghi, H.; Tsoi, N.; Gwak, J.; Sadeghian, A.; Reid, I.;\nand Savarese, S. 2019. Generalized intersection over union:\nA metric and a loss for bounding box regression. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 658–666.\nStricker, M. A.; and Orengo, M. 1995. Similarity of color\nimages. In Storage and Retrieval for Image and Video\nDatabases III, volume 2420, 381–392. SPiE.\nWagemans, J.; Feldman, J.; Gepshtein, S.; Kimchi, R.;\nPomerantz, J. R.; Van der Helm, P. A.; and Van Leeuwen, C.\n2012. A century of Gestalt psychology in visual perception:\nII. Conceptual and theoretical foundations. Psychological\nBulletin, 138(6): 1218.\nWang, J.; Song, L.; Li, Z.; Sun, H.; Sun, J.; and Zheng, N.\n2021. End-to-end object detection with fully convolutional\nnetwork. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 15849–15858.\nWang, Y .; Zhang, X.; Yang, T.; and Sun, J. 2022. Anchor\ndetr: Query design for transformer-based detector. In Pro-\nceedings of the AAAI Conference on Artiﬁcial Intelligence ,\nvolume 36, 2567–2575.\nWertheimer, M. 1922. Untersuchungen zur Lehre von der\nGestalt. Psychologische Forschung, 1(1): 47–58.\nWu, A.; Zhao, S.; Deng, C.; and Liu, W. 2021. Generalized\nand Discriminative Few-Shot Object Detection via SVD-\nDictionary Enhancement. Advances in Neural Information\nProcessing Systems, 34.\nYan, Y .; Ren, J.; Sun, G.; Zhao, H.; Han, J.; Li, X.; Mar-\nshall, S.; and Zhan, J. 2018. Unsupervised image saliency\ndetection with Gestalt-laws guided optimization and visual\nattention based reﬁnement. Pattern Recognition, 79: 65–78.\nYing, Z.; You, J.; Morris, C.; Ren, X.; Hamilton, W.; and\nLeskovec, J. 2018. Hierarchical graph representation learn-\ning with differentiable pooling. Advances in Neural Infor-\nmation Processing Systems, 31.\nYuan, Y .; Wang, S.; Jiang, M.; and Chen, T. Y . 2021. Percep-\ntion matters: detecting perception failures of VQA models\nusing metamorphic testing. InProceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n16908–16917.\nZhang, G.; Luo, Z.; Yu, Y .; Cui, K.; and Lu, S. 2022. Accel-\nerating DETR Convergence via Semantic-Aligned Match-\ning. Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition.\nZhong, Y .; Wang, J.; Wang, L.; Peng, J.; Wang, Y .-X.; and\nZhang, L. 2021. DAP: Detection-Aware Pre-training with\nWeak Supervision. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, 4537–\n4546.\nZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J.\n2020. Deformable DETR: Deformable Transformers for\nEnd-to-End Object Detection. In International Conference\non Learning Representations.\n907",
  "topic": "Gestalt psychology",
  "concepts": [
    {
      "name": "Gestalt psychology",
      "score": 0.8819719552993774
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6170714497566223
    },
    {
      "name": "Computer science",
      "score": 0.578197717666626
    },
    {
      "name": "Encoder",
      "score": 0.558466911315918
    },
    {
      "name": "Perception",
      "score": 0.5495858192443848
    },
    {
      "name": "Computer vision",
      "score": 0.5187044739723206
    },
    {
      "name": "Visual perception",
      "score": 0.4439985156059265
    },
    {
      "name": "Transformer",
      "score": 0.42456504702568054
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4237849712371826
    },
    {
      "name": "Psychology",
      "score": 0.1379733681678772
    },
    {
      "name": "Engineering",
      "score": 0.10086226463317871
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I87445476",
      "name": "Xi'an Jiaotong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I116953780",
      "name": "Tongji University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I4210156165",
      "name": "Lenovo (China)",
      "country": "CN"
    }
  ],
  "cited_by": 7
}