{
  "title": "A Transformer-based Math Language Model for Handwritten Math Expression Recognition",
  "url": "https://openalex.org/W3190163316",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Ung, Huy Quang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2746477218",
      "name": "Nguyen Cuong Tuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2416899296",
      "name": "Nguyen Hung Tuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5094344000",
      "name": "Truong Thanh-Nghia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2302481699",
      "name": "Nakagawa Masaki",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3003915459",
    "https://openalex.org/W2623860192",
    "https://openalex.org/W3037932933",
    "https://openalex.org/W2806717845",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2144050596",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W3004196288",
    "https://openalex.org/W2481930807",
    "https://openalex.org/W2528252828",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W1502293651",
    "https://openalex.org/W3204737504",
    "https://openalex.org/W179875071",
    "https://openalex.org/W3002575754",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2031071334",
    "https://openalex.org/W2963088785"
  ],
  "abstract": "Handwritten mathematical expressions (HMEs) contain ambiguities in their interpretations, even for humans sometimes. Several math symbols are very similar in the writing style, such as dot and comma or 0, O, and o, which is a challenge for HME recognition systems to handle without using contextual information. To address this problem, this paper presents a Transformer-based Math Language Model (TMLM). Based on the self-attention mechanism, the high-level representation of an input token in a sequence of tokens is computed by how it is related to the previous tokens. Thus, TMLM can capture long dependencies and correlations among symbols and relations in a mathematical expression (ME). We trained the proposed language model using a corpus of approximately 70,000 LaTeX sequences provided in CROHME 2016. TMLM achieved the perplexity of 4.42, which outperformed the previous math language models, i.e., the N-gram and recurrent neural network-based language models. In addition, we combine TMLM into a stochastic context-free grammar-based HME recognition system using a weighting parameter to re-rank the top-10 best candidates. The expression rates on the testing sets of CROHME 2016 and CROHME 2019 were improved by 2.97 and 0.83 percentage points, respectively.",
  "full_text": "A Transformer-based Math Language Model for \nHandwritten Math Expression Recognition \nHuy Quang Ung [0000-0001-9238-8601], Cuong Tuan Nguyen [0000-0003-2556-9191],  \nHung Tuan Nguyen [0000-0003-4751-1302], Thanh-Nghia Truong [0000-0002-8635-8534],  \nand Masaki Nakagawa [0000-0001-7872-156X] \nTokyo University of Agriculture and Technology, Tokyo, Japan \nungquanghuy93@gmail.com \nfx4102@go.tuat.ac.jp \n{ntuanhung, thanhnghiadk}@gmail.com \nnakagawa@cc.tuat.ac.jp \nAbstract. Handwritten mathematical expressions (HMEs) contain ambiguities in \ntheir interpretations, even for humans sometimes. Several math symbols are very \nsimilar in the writing style, such as dot and comma or ‚Äú0‚Äù, ‚ÄúO‚Äù, and ‚Äúo‚Äù, which \nis a challenge for HME  recognition systems to handle without using contextual \ninformation. To address this problem, t his paper present s a Transformer-based \nMath Language Model (TMLM). Based on the self -attention mechanism, the \nhigh-level representation of an input token in a sequence of tokens is computed \nby how it is related to the previous tokens . Thus, TMLM can capture long de-\npendencies and correlations among symbols and relations in a mathematical ex-\npression (ME). We trained the proposed language model using a corpus of ap-\nproximately 70,000 LaTeX sequences  provided in CROHME 2016 . TMLM \nachieved the perplexity of 4.42, which outperformed the previous math language \nmodels, i.e., the N-gram and recurrent neural network-based language models. In \naddition, we combine TMLM into a stochastic context-free grammar-based HME \nrecognition system using a weighting parameter to re-rank the top-10 best candi-\ndates. The expression rates on the testing sets of CROHME 2016 and CROHME \n2019 were improved by 2.97 and 0.83 percentage points, respectively. \nKeywords: language model, mathematical expressions, handwritten, trans-\nformer, self-attention, recognition. \n1 Introduction \nNowadays, devices such as pen -based or touch -based tablets and electronic white-\nboards are becoming more and more popular for users as educational media. Learners \ncan use them to learn courses and do exercises. Especially, educational units can use \nthose devices to support their online learning platform in the context of the SARS-CoV-\n2 (COVID-19) widely spreading worldwide. These devices provide a user-friendly in-\nterface for learners to input handwritten mathematical  expressions, which is more nat-\nural and quicker than common editors such as Microsoft Equation Editor, MathType , \n2 \nand LaTeX. Due to the demands of real applications, the research on handwritten math-\nematical expression (HME) recognition has been conceived as an important role in doc-\nument analysis since the 1960s and very active during the last two decades. The perfor-\nmance of HME recognition systems ha s been significantly improved according to the \nseries of competitions on rec ognition of handwritten mathematical expressions \n(CROHME) [1]. \nHowever, there remain challenging problems in HME recognition. One problem is \nthat there are lots of ambiguities in the interpretation of HMEs. For instance, there exist \nmath symbols that are very similar in the writing style, such as ‚Äú0‚Äù, ‚Äúo‚Äù, and ‚ÄúO‚Äù or dot \nand comma. These ambiguities challenge HME recognition without utilizing contextual \ninformation. In addition, recognition systems without using predefined grammar rules \nsuch as the encoder-decoder model  [2, 3] might result in syntactically unacceptable \nmisrecognitions. One promising solution for th ese problems is to combine an HME \nrecognition system with a math language model. Employing language models for hand-\nwritten text recognition has shown effectiveness in previous research [4‚Äì6]. \nA mathematical expression (ME) has a 2D structure represented by several formats \nsuch as MathML, one -dimensional LaTeX sequences , and two-dimensional symbol \nlayout trees [7]. Almost all recent HME recognition systems output their predictions as \nthe LaTeX sequences since LaTeX is commonly used in real applications . Thus, we \nfocus on an ME language model for the LaTeX sequences in this paper. \nThere are some common challenges  in modeling MEs similar to natural language \nprocessing. First, there is a lack of corpora of MEs as MEs rarely appear in daily doc-\numents. Secondly, there are infinite combinations of symbols and spatial relationship s \nin MEs. Thirdly, there are long-term dependencies and correlations among symbols and \nrelations in an ME. For example, ‚Äú(‚Äù and ‚Äú)‚Äù are often used to contain a sub-expression, \nand if they contain a long sub -expression, it is challenging to learn the dependency \nbetween them. \nThere are several methods to modeling MEs. The statistical N-gram model was used \nin [8]. It assigns a probability for the n-th tokens given (n-1) previous tokens based on \nthe maximum likelihood  estimation. However, the N-gram model might not represent  \nthe long dependencies due to the limitation of the context length. Increasing this length \nmight lead to the problem of estimating a high-dimensional distribution, and it requires \na sufficient amount of training corpus. In practical application s, the trigram model is \nusually used, and the 5-gram model is more effective when the training data is suffi-\ncient. The recurrent neural network-based language model (RNNLM) proposed by [9] \nwas utilized in HME recognition systems [1, 2]. RNNLM predicts the n-th token given \n(n-1) previous tokens in previous time  steps. However, they still face the problem of \nthe long-term dependencies. \nIn recent years, the transformer-based network using a self-attention mechanism has \nachieved impressive results in natural language processing (NLP). In the language mod-\neling task, Al-Rfou et al.  [10] presented a  deep transformer-based language model  \n(TLM) for character-level modeling and showed the effectiveness against RNNLM. \nIn this paper, we present the first transformer-based math language model (TMLM). \nBased on the self-attention mechanism, the high-level representation of an input token \nin a sequence of tokens is computed by how it is related to the previous tokens so that \n3 \nTMLM can capture long dependencies  and correlations in MEs. Then, we propose a \nmethod to combine TMLM into a stochastic context-free grammar-based HME recog-\nnizer. In our experiments, we show that our TMLM outperforms the N-gram model and \nRNNLM in the task of modeling MEs. \nThe rest of the paper is organized as follows. Section 2 briefly  presents related re-\nsearch. Section 3 describes our method in detail. Section 4 presents our experiments for \nevaluating the proposed method. Finally, section 5 concludes our work and discusses \nfuture works. \n2 Related works \nLanguage models are  well-known as  generative models  and autoregressive models  \nsince they predict the next state of a variable given its previous states. In NLP, Radford \net al. [11, 12] proposed Generative Pre-Training models (GPT and GPT-2) with high \nachievements on NLP benchmarks  based on the vanilla  transformer-based network in \n[13]. Their models are trained by the casual language model ing loss, then fine -tuned \nfor multitask learning such as text classification, question answering, and similarity . \nDai et al. [14] presented a Transformer -XL for capturing  extended length of context \nusing a recurrent architecture for context segments. Transformer-XL can learn depend-\nency that is 80% longer than RNNs, 450% longer than TLM. The inference speed is \n1,800 times faster than TLM by caching and reusing previous computations.  XLNet \npresented by Yang et al. [15], is the first model utilizing bidirectional contexts for trans-\nformer-based language model s. This model significantly outperformed the conven-\ntional BERT model [16] in 20 tasks of NLP benchmarks. \nThere are several studies combining HME recognition systems with pre-trained lan-\nguage models. Wu et al. [8] combined their encoder-decoder HME recognizer with a \npre-trained 4-gram model to get the N best paths. Zhang et al. [2] utilized a Gated Re-\ncurrent Unit-based language model (GRULM) for their HME recognizer that is an en-\ncoder-decoder model with temporal attention. This attention is to help the decoder de-\ntermine the reliability of spatial attention and that of the language model per time step. \nThe language model s improved the expression rate by around  1 percentage point.  \nHence, the approach for combining language models into recognition systems is essen-\ntial to study. \nIn CROHME 2019  [1], the Samsung R&D team used a probabilistic context-free \ngrammar-based recognizer combined with  two bigram language models, i.e.,  a lan-\nguage sequence model and  a language model  for spatial relationships. Besides, the \n4 \nMyScript team used LSTM -based language models for their grammar-based recogni-\ntion system. \n3 Proposed method \nGiven a sequence of tokens ùëã = (ùë•1, ùë•2 ‚Ä¶ , ùë•ùëÅ), constructing a language model is to \nestimate the joint probability ùëÉ(ùëã), which is often auto -regressively factorized as \nùëÉ(ùëã) = ‚àè ùëÉ(ùë•ùë° |ùëã<ùë°)ùë°  where ùëã<ùë° = (ùë•1, ‚Ä¶ , ùë•ùë°‚àí1). According to this factorization, the \nproblem reduces to estimating each conditional factor ùëÉ(ùë•ùë°|ùëã<ùë°). In this paper, our pro-\nposed model with a self-attention mechanism encodes the context ùëã<ùë° to produce the \ncategorical probability of the token ùë•ùë°. \nIn this section, we first describe our proposed TMLM, which is mainly based on \n[10]. Then, we present a method for combining our model with an HME recognizer. \n3.1 Transformer-based math language model \nTMLM consists of three main parts:  an input embedding layer, a positional encoding \nlayer (PE), and a stack of transformer layers, as shown in Fig. 1. First, sequential input \ntokens {ùë•1, ùë•2, ‚Ä¶ , ùë•ùëÅ} are fed into the input embedding to embed the categories of dis-\ncrete tokens into a continuous space for better representation. Secondly, each embedded \nvector according to each input token is added by a PE vector to present  the token‚Äôs \nposition in the sequence. The detail of the PE is presented later in this section. Thirdly, \nthe ou tputs of the input embe dding and posit ional encoding are passed into stacked \ntransformer layers to learn high -level representation based on the self-attention mech-\nanism. Finally, the output of the top transformer layer is input to a softmax layer to \nobtain the categorical probability for the token ùë•ùë° given {ùë•1, ‚Ä¶ , ùë•ùë°‚àí1}. Although all in-\nput tokens are fed into our model at the same time, the model is restricted to attend only \n \nFig. 1. Overview of the proposed transformer-based language model  \nwith two transformer layers. \n‚Ä¶\nLeft-to-right masked multi-\nhead self-attention\nAdd & Norm\nTransformer layer\nTransformer layer\nFeedforward Neural Net\nAdd & Norm\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\nSoftmax\nInput embedding layer\nPositional encoding layer\n+\n5 \ntokens on the left side of ùë•ùë° to produce ùëÉ(ùë•ùë°|ùë•1, ‚Ä¶ , ùë•ùë°‚àí1) by a mask in the transformer \nlayer. \nThe architecture of the transformer layer is based on the decoder of the conventional \ntransformer-based model [16]. It consists of a masked multi -head self -attention \n(MMSA), layer normalization [17], and a feedforward neural network, as shown in Fig. \n1. In addition, residual connections are added for the model to learn better.  Here, we \npresent MMSA and PE, which play important roles in our model.  \nMasked multi-head self-attention. This layer receives the representation of input to-\nkens and ou tputs the higher representation for the tokens based on how each token is \nrelated to others. MMSA includes multiple attention functions, which allow the model \nto attend information from different representation subspace.  We firstly present a \nmasked single-head self-attention. \nA traditional attention function can be described as the mapping of a query and a set \nof key-value pairs to produce an output. Note that the query, the keys, and the values \nare all vectors. The output is a weighted sum of the values, where the weight assigned \nto each value is computed by a compatibility function of the query with the correspond-\ning key of the value. \nThe masked single-head self-attention function, called  scaled dot-product attention \n(SDPA), are also based on the queries (Q), the keys (K) of dimension ùëëùëò, and the values \n(V) of dimension ùëëùë£ as shown in Fig. 2 (a). We compute the dot products of the query \nwith all keys, then scale them by ‚àöùëëùëò. Next, we apply a mask to restrict the model to \nattend only the left side of the current predicted token. We then apply a softmax func-\ntion to obtain the weights on the values. The output of this attention function is formu-\nlated as follows: \n ùê¥ùë°ùë°(ùëÑ, ùêæ, ùëâ) = softmax (ùëÑùêæùëá\n‚àöùëëùëò\n) ùëâ                                       (1) \nSDPA is called a ‚Äúhead‚Äù in MMSA. The architecture of MMSA including h heads \nis shown in Fig. 2 (b). With multiple heads, we project the queries, keys, and values h \n  \n(a) (b) \nFig. 2. Illustration of scale dot-product attention and masked multi-head attention. \nMatMul\nScale\nSoftmax\nMatMul\nQ K V\nScale dot-product attention (SDPA)\nMask\n‚Ä¶\nh heads\nQ K V\nLinear Linear Linear\nScale dot-product attention\nConcat\nLinear\nMasked multi-head self-attention (MMSA)\n6 \ntimes with three different learnable linear projections. On each of these projected ver-\nsions of queries, keys, and values, we then perform SDPAs in parallel. Then, we con-\ncatenate their outputs and once again project to obtain the final output of MMSA. \nPositional encoding. Since tokens (ùë•1, ùë•2, ‚Ä¶ , ùë•ùëÅ) are input to our model at the same \ntime and there is no convolutional/recurrent layer,  the model cannot exploit the posi-\ntional information of tokens. It is a serious problem for the task of language modeling. \nTo address it, we utilize PE having the same dimensionality as the input embedded \nvector, ùëÖùëÅ√óùëëùëíùëöùëèùëíùëë  (ùëëùëíùëöùëèùëíùëë  is the dimension of the input embedded vector). Then, we \nadd PE to the input embedded vector  to provide the positional information  for our \nmodel. PE of the p-th token and the i-th dimension is computed by the sine and cosine \nfunction as follows: \n ùëÉùê∏(ùëù, ùëñ) = {\nsin ( ùëù\n10000ùëñ/ùëëùëíùëöùëèùëíùëë\n)          ùëñùëì ùëñ ùëñùë† ùëíùë£ùëíùëõ \ncos ( ùëù\n10000(ùëñ‚àí1)/ùëëùëíùëöùëèùëíùëë\n)      ùëúùë°‚Ñéùëíùëüùë§ùëñùë†ùëí\n                              (2) \n3.2 Combining language model with HME recognizer \nIn this study, we use a language model to sort the top-M best candidates outputted from \nthe stochastic context -free grammar -based HME recognizer. Given M candidates \n{ùëê1, ùëê2, ‚Ä¶ , ùëêùëÄ}  of LaTeX sequences  and their co rresponding scores , the combined \nscores are computed as follows: \n ùëÜùëêùëúùëüùëíùëêùëúùëöùëè(ùëêùëñ) = ùëÜùëêùëúùëüùëíùëüùëíùëêùëúùëî(ùëêùëñ ) + ùõº √ó ùëÜùëêùëúùëüùëíùêøùëÄ(ùëêùëñ )                           (3) \nwhere ùëÜùëêùëúùëüùëíùëüùëíùëêùëúùëî(ùëêùëñ ) and ùëÜùëêùëúùëüùëíùêøùëÄ (ùëêùëñ ) are the scores of the i-th candidate, ùëêùëñ, from the \nHME recognizer and the language model, respectively. ùëÜùëêùëúùëüùëíùëêùëúùëöùëè (ùëêùëñ) is the combined \nscore of ùëêùëñ. ùõº is a weighting parameter  to balance between recognition and language \nscores. Note that ùëÜùëêùëúùëüùëíùêøùëÄ(ùëêùëñ) is the sum of logarithms of conditional probabilities out-\nput from the language model and normalized by the length of the candidate, ùëêùëñ. For this \ncombination method, we refer to the HME recognizer producing ùëÜùëêùëúùëüùëíùëüùëíùëêùëúùëî(ùëêùëñ) based \non the sum of logarithm s of probability terms. The candidate having the highest com-\nbined score is the final recognition result. \n4 Experiments \nThis section presents evaluation s of our proposed TMLM using a corpus of LaTeX \nsequences. We also evaluate TMLM when combining with the HME recognizer pro-\nposed in [18] and present error analyses. \n7 \n4.1 Dataset \nWe uses a corpus of 68,862 LaTeX sequences provided in CROHME 2016  [19]. For \npreprocessing step s, w e first filtered invalid syntax  LaTeX sequences  and removed \nstyle-related characters such as ‚Äú\\mathrm‚Äù, ‚Äú\\textrm‚Äù, and so on. Then, we normalized \nthe LaTeX sequence s into the same format as the output of our HME recognizer. For \nexample, ‚Äú{a}^{2}‚Äù is normalized as ‚Äúa^{2}‚Äù. The corpus is partitioned into a training \nset, a validation set, and a testing set according to the ratio of 8:1:1.  The number of \nsymbols in the dictionary is 108 , including the padding  ‚Äú<pad>‚Äù and the end-of-se-\nquence symbol ‚Äú<eos>‚Äù. \n4.2 HME recognizer \nIn this section, we present the online HME recognizer [18] used in our exp eriments. \nThe recognizer  receives a sequence of point -based features extracted from an input \nHME and output s recognition result as a LaTeX sequence.  It consists of two  main \nstages: (1) A symbol-relation temporal classifier (SRTC) for segmenting and classify-\ning symbols and spatial relationships in an HME and (2) A symbol-level parser (SLP). \nWe denoted this HME recognizer as SRTC_SLP. \n \nFig. 3. Illustration for symbol-relation temporal classifier. \n \nFig. 4. Symbol-level parser. \n \nx 8 + i\nSup NoRel Right\nx 9\nRight Sup\nx ^ { 8 }\ni x ^ { 9 }x ^ { 8 }\nx ^ { 8 } + i x ^ { 9 }LaTeX output\nStructure \nanalysis\nSymbol-relation\noutput\nx ^ { 9 }\n8 \nSymbol-relation temporal classifier .  SRTC consists of three  stacked Bidirectional \nLong-Short Term Memory (BLSTM) layers and a Connectionist Temporal Classifica-\ntion (CTC) layer at the top , as shown in Fig. 3 . Its input is a sequence of point-based \nfeatures, including the representation of off-strokes (pen movements between strokes). \nHere, we  ideally assume that there are no delayed strokes  in the input HMEs.  The \nstacked multiple BLSTM layers encode bidirectional context from the input and learn \nhigh-level representation. Then, the CTC layer generates a sequence of symbols and \nspatial relationships. There are 7 types of spatial relationships: superscript, subscript, \nupper, lower, horizontal, inside, and no relation (denoted as ‚ÄúNoRel‚Äù). \nSymbol-level parser. Given the output of SRTC, SLP based on the Cocke‚ÄìYounger‚Äì\nKasami (CYK) algorithm [20] is applied to merge recognized symbols and spatial re-\nlationships along with predefined grammar rules , as shown in Fig. 4 . This bottom -up \nmethod considers many possible combinations of hypotheses at the intermediate levels. \nHence, it produces several candidates at the top of the combination tree even if the less \npromising candidates are pruned. Each candidate has a corresponding score computed \nbased on the classification probabilities of symbols and spatial relationships. \nTraining and testing. SRTC_SLP was trained on the CROHME 2016 and CROHME \n2019 training sets and tested on the CROHME 2016 and CROHME 2019 testing sets, \nrespectively. Without using a language model, i t achieved the expression rate of \n53.44% and 52.38% on the CROHME 2016 and CROHME 2019 testing set s, respec-\ntively. This expression rate is higher than that of the state-of-the-art TAP recognizer  \n(without using language mode ls and/or ensemble method s) [2] by 3.22 percentage \npoints in the expression rate on the CROHME 2016 testing set. \n \n4.3 Experimental settings \nIn this section, we present settings for training the proposed TMLM. We evaluate d \nTMLM with different numbers of transformer layers. The number of heads is fixed to \n4 heads. The dimension of each head is set to 16. The context length of TMLM is fixed \nto 256, which covers the maximum sequence length in our LaTeX corpus. The dimen-\nsion of vectors of input embedding and the dimen sion of hidden states are set to 256 \nand 512, respectively. The number of hidden nodes in the feedforward neural net layer \nis set to 1024. The dropout rate is set to 0.1. We applied an adaptive log-softmax func-\ntion proposed in [21]. Our model is trained by the AdamW optimizer [22] with a learn-\ning rate of 10‚àí5. The model is implemented based on the ‚ÄúHugging Faces‚Äù library [23]. \nFor combining language models with SRTC_SLP, we determined the parameter ùõº ‚àà\nùëÖ+ in Eq. 3 by applying the enumeration method on {0, 0.1, 0.2, ‚Ä¶ , 2.0}. The chosen ùõº \nparameters achieved the best expression rates on the CROHME 2014 testing set. \nTo evaluate language models, we utilized the perplexity measurement. Given a se-\nquence of tokens ùëã = (ùë•1, ùë•2, ‚Ä¶ , ùë•ùëÅ), the perplexity of ùëã is the exponentiated average \nnegative log-likelihood formulated as follows: \n9 \n ùëùùëùùëô(ùëã) = exp {‚àí 1\nùëÅ ‚àë log ùëù(ùë•ùë°|ùëã<ùë°)\nùëÅ\nùë°=1\n}                         (4) \nwhere ùëù(ùë•ùë° |ùëã<ùë°) is the conditional probability outputted from the language model. \n4.4 Evaluation \nIn this section, we compare the proposed language model with the previous methods. \nThen, we conduct experiments to compare the performance of those models when com-\nbined with the SRTC_SLP recognizer. \nComparisons with other language modeling methods . We compared our TMLM \nwith the traditional N-gram model and GRULM. We increased the context length N in \nthe N-gram model to 11 since TMLM can attend all the past contexts for a fair compar-\nison. For GRULM, we increased the number of GRU layers up to 3 layers for evaluating \nthe performance as well as comparing with TMLM in the condition of a similar number \nof trainable parameters. The dimension of an input embedded vector and hidden states \nin GRULM are set as the same as in TMLM.  \nTable 1 presents the perplexity of the models on the testing set  extracted from our \nLaTeX corpus, as mentioned in section 4.3. The results show that our proposed TMLM \nmodels outperform all N-gram models and all GRULMs, even using fewer trainable \nparameters. For the N-gram models, increasing the context length can improve the per-\nplexity, but it seems to converge when N reaches 11. GRULMs perform better than the \nN-gram models. Among GRULMs, the perplexity of GRULM_2L achieves the best, \nwhich implies that increasing the number of GRU layers is not adequate. On the other \nhand, TMLMs can learn better when increasing the number of transformer layers.  \nTable 1. Comparisons with other language modeling methods. \nModel #Layers in model #parameters Perplexity \n3-grams - - 9.603 \n5-grams - - 7.557 \n9-grams - - 6.550 \n11-grams - - 6.500 \nGRULM_1L 1 1.3M 6.050 \nGRULM _2L 2 2.8M 6.049 \nGRULM _3L 3 4.4M 6.377 \nOurs: TMLM_2L 2 2.7M 4.598 \nOurs: TMLM_5L 5 6.3M 4.509 \nOurs: TMLM_8L 8 10M 4.420 \n \n \n10 \nWith nearly the same number of trainable parameters, TMLM_2L performs signifi-\ncantly better than GRULM _2L. It implies that the architecture of TMLM is much more \neffective than the traditional GRULM on modeling MEs. \nEvaluation on combining language models into the HME recognizer. We combined \nthe SRTC_SLP recognizer with the language models that achieved the best perfor-\nmance in the previous experiment (i.e., 11 -grams, GRULM_2L, and TMLM_8L) . In \ndetail, the combined score in Eq. 3 is computed for the top-10 best candidates from \nSRTC_SLP. \nTable 2 presents the expression rates of the combined recognizers on the CROHME \n2016 and CROHME 2019 testing sets. The combination of SRTC_SLP and TMLM_8L \nachieves the best expression rates in both testing sets . TMLM_8L improves 2.97 and \n0.83 percentage points of the expression rate s on the CROHME 2016 and CROHME \n2019 testing set, respectively. The SRTC_SLP + 11-grams is better than SRTC_SLP + \nGRULM_2L in the  CROHME 2016 testing set , but that result is opposed in the \nCROHME 2019 testing set. \nTable 2 also presents the expression rates of  the state-of-the-art HME recognizers \nthat utilized math language models , i.e., TAP [2] and PAL_v2 [8]. Compared to those \nmodels, SRTC_SLP combined with our TMLM_8L yields the best expression rate on \nTable 2. Expression rates on combining the HME recognizers with language models. \nRecognition system Expression rate (%) \nCROHME 2016 CROHME 2019 \nSRTC_SLP 53.44 52.38 \nSRTC_SLP + 11-grams 56.15 52.54 \nSRTC_SLP + GRULM_2L 55.36 52.88 \n(Ours) SRTC_SLP + TMLM_8L 56.41 53.21 \n(Zhang et al. [2]) TAP 49.29 - \n(Zhang et al. [2]) TAP + GRUs 50.41 - \n(Wu et al. [8]) PAL-v2 49.00 - \n(Wu et al. [8]) PAL-v2 + 4-grams 49.35 - \nLM: language model \nTable 3. Percentages of corrected, miscorrected, and unchanged recognition results \nwhen combining the SRTC_SLP recognizer with language models. \nDataset Method Corrected \n(%) \nMiscorrected \n(%) \nUnchanged \n(%) \nCROHME \n2016 \n11-grams 4.01 1.31 94.68 \nGRULM_2L 2.96 1.05 95.99 \nTMLM_8L 4.62 1.66 93.72 \nCROHME \n2019 \n11-grams 1.83 1.67 96.50 \nGRULM_2L 1.92 1.42 96.66 \nTMLM_8L 2.50 1.67 95.83 \n \n \n11 \nthe CROHME 2016 testing set. Combining the language models only improved around \n1 percentage point in the case of TAP and PAL_v2  while TMLM_8L improves 2.97 \npercentage points. Here, we cannot conclude that our method for utilizing a math lan-\nguage model is better than their methods  since they utilized different type s of HME \nrecognizers as well as different LaTeX corpora to train their language models. We con-\nsider conducting more experiments on the combination method as a remaining work. \nTable 3 shows the recognition result s in more detail about the percentage of cor-\nrected cases, miscorrected cases, and unchanged cases  when combining SRTC_SLP \nwith three different language models on the CROHME 2016 and CROHME 2019 test-\ning sets. The percentages of the corrected cases by TMLM_8L are the highest compared \nto 11-grams and GRULM_2L on both testing sets . However, that of the miscorrected \ncases by TMLM_8L is the worst compared to others. GRULM_2L caused the  least \nmiscorrections compared to others, but it could not correct many cases. 11-grams and \nTMLM_8L have comparable percentages of miscorrected cases, but TMLM_8L cor-\nrected more cases than 11-grams did.  \n4.5 Error analysis \nIn this section, we present some samples which are corrected or miscorrected when \napplying TMLM_8L with the SRTC_SLP recognizer.  \nFig. 5(a) and Fig. 5(b) show two corrected cases. In Fig. 5(a), ‚Äúùõº‚Äù in the HME sample \nare recognized as ‚Äú2‚Äù without using TMLM_8L since it seems to be similarly written \nas ‚Äú2‚Äù. However, the  language score of the candidate with ‚Äúùõº‚Äù is significantly higher \nthan the one with ‚Äú2‚Äù.  Therefore, the recognizer combined TMLM_8L results in the \ncorrect prediction. TMLM_8L performs well since ‚Äú ùõº‚Äù seems more likely to appear \nnext to the trigonometry function (e.g., sine, cosine, and tangent) than a number. Simi-\nlarly, ‚Äú9‚Äù in the HME sample of Fig. 5(b) are correctly recognized by TMLM_8L. \n \n \n \nGroundtruth: 2 \\cos \\alpha   Groundtruth: x ^ { 9 } ‚Äì x ^ { 8 }  \nW/o LM: 2 \\cos 2    (-6.162)  W/o LM: x ^ { g } ‚Äì x ^ { 8 } (-2.309) \nW/ LM:   2 \\cos \\alpha (-3.144)  W/ LM:   x ^ { 9 } ‚Äì x ^ { 8 } (-1.361) \n(a) Corrected case (b) Corrected case \n \n \n \n \nGroundtruth: \\sqrt { \\beta } H   Groundtruth: m = 2 \\tan \\Delta \\pi  \nW/o LM: \\sqrt { \\beta } H (-3.893)  W/o LM: m = 2 \\tan \\Delta \\pi (-4.995) \nW/ LM:   \\sqrt { \\beta H } (-2.714)  W/ LM:   m = 2 \\tan \\alpha \\pi (-4.561) \n(c) Miscorrected case  (d) Miscorrected case \nFig. 5. Examples of corrected and miscorrected cases when combining the SRTC_SLP rec-\nognizer and TMLM_8L (LM: language model). Each case shows an HME image, its ground \ntruth, and its recognition candidates with/without TMLM_8L and their corresponding scores \nfrom TMLM_8L.  \n\n12 \nFig. 5(c) and Fig. 5(d) show two miscorrected cases. The case in Fig. 5(c) is miscor-\nrected since the language model score of the incorrect result is higher than t hat of the \ncorrect result. We can realize that the context, in this case, is not clear. The case in Fig. \n5(d) is miscorrected since ‚Äúùõº‚Äù seems more likely to appear next to the tangent symbol \nthan ‚ÄúŒî‚Äù. \nAccording to those examples, we can see that modeling ME s is still challenging \nsince the context in an ME is not clear and our corpus of MEs might not be enough to \nestimate the distribution of MEs. \n5 Conclusion and future works \nThis paper presented a transformer-based math language model (TMLM) for improving \nthe recognition rate of HME recognition systems. We showed that our TMLMs perform \nbetter than the traditional language models for MEs, i.e., the N-gram and GRULM. The \nbest perplexity achieved is 4. 42, resulted from TMLM_8L of 8 transformer layers. \nCombining TMLM_8L with the online HME recognizer in [18] improved the expres-\nsion rate by 2.97 and 0.83 percentage points  on the CROHME 2016 and CROHME \n2019 testing set, respectively. \nThere are several remaining works. Firstly, we should enrich the source of ME La-\nTeX by collecting open sources on the internet.  Secondly, we should modify our \nTMLM to exploit the bidirectional context in MEs. Thirdly, the method for jointly train-\ning an HME recognizer and a math language model should be studied for better opti-\nmization. \nAcknowledgement \nThis research is being partially supported by the grant-in-aid for scientific research (A) \n19H01117 and that for Early Career Research 21K17761. \nReferences  \n1. M. Mahdavi, R. Zanibbi, H. Mouchere, C. Viard-Gaudin, U. Garain: CROHME + TFD: \nCompetition on recognition of handwritten mathematical expressions and typeset \nformula detection. In: Proc. of Inter. Conf. on Doc. Anal. and Recognit. pp. 1533‚Äì1538 \n(2019). \n2. J. Zhang, J. Du, L. Dai: Track, Attend, and Parse (TAP): An End-to-End Framework for \nOnline Handwritten Mathematical Expression Recognition. IEEE Trans. Multimed. 21, \n221‚Äì233 (2019). \n3. Zhang, J., Du, J., Zhang, S., Liu, D., Hu, Y., Hu, J., Wei, S., Dai, L.: Watch, attend and \nparse: An end -to-end neural network based approach to  handwritten mathematical \nexpression recognition. Pattern Recognit. 71, 196 ‚Äì206 (2017). \nhttps://doi.org/10.1016/j.patcog.2017.06.017. \n4. Poznanski, A., Wolf, L.: CNN -N-Gram for HandwritingWord Recognition. In: \n13 \nProceedings of the IEEE Computer Society Conference on Computer Vision and Pattern \nRecognition. pp. 2305‚Äì2314 (2016). \n5. Zhu, B., Zhou, X.D., Liu, C.L., Nakagawa, M.: A robust model for on-line handwritten \njapanese text recognition. Int. J. Doc. Anal. Recognit. 13, 121 ‚Äì131 (2010). \nhttps://doi.org/10.1007/s10032-009-0111-y. \n6. Reeve Ingle, R., Fujii, Y., Deselaers, T., Baccash, J., Popat, A.C.: A scalable \nhandwritten text recognition system. In: Proceedings of the International Conference on \nDocument Analysis and Recognition, ICDAR. pp. 17‚Äì24 (2019). \n7. Zanibbi, R., Blostein, D.: Recognition and retrieval of mathematical expressions. Int. J. \nDoc. Anal. Recognit. 15, 331‚Äì357 (2012). \n8. Wu, J.W., Yin, F., Zhang, Y.M., Zhang, X.Y., Liu, C.L.: Handwritten Mathematical \nExpression Recognition via Paired Advers arial Learning. Int. J. Comput. Vis. 128, \n2386‚Äì2401 (2020). \n9. Mikolov, T., Karafi√°t, M., Burget, L., Cernocky, J.: Recurrent neural network based \nlanguage model. In: 11th Annual Conference of the International Speech \nCommunication Association. pp. 1045‚Äì1048 (2010). \n10. Al-Rfou, R., Choe, D., Constant, N., Guo, M., Jones, L.: Character -level language \nmodeling with deeper self -attention. In: 33rd AAAI Conference on Artificial \nIntelligence. pp. 3159‚Äì3166 (2019). \n11. Radford, A., Narasimhan, K., Salimans, T., Sutskever, L.: Improving Language \nUnderstanding by Generative Pre -Training. https://s3 -us west -\n2.amazonaws.com/openai-assets/research-covers/language-\nunsupervised/language_understanding_paper.pdf. (2018). \n12. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.: Language Models \nare Unsupervised Multitask Learners. https://d4mucfpksywv.cloudfront.net/better -\nlanguage-models/language_models_are_unsupervised_multitask_learners.pdf. (2019). \n13. Vaswani, A., Brain, G., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., \nKaiser, ≈Å., Polosukhin, I.: Attention Is All You Need. In: Advances in Neural \nInformation Processing Systems (2017). \n14. Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., Salakhutdinov, R.: Transformer -\nXL: Attentive Language Models Beyond a Fixed-Length Context. In: ACL 2019 - 57th \nAnnual Meeting of the Association for Computational Linguistics. pp. 2978 ‚Äì2988 \n(2019). \n15. Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., Le, Q. V: XLNet: \nGeneralized Autoregressive Pretraining for Language Understanding. In: Advances in \nNeural Information Processing Systems (2019). \n16. Devlin, J., Chang, M. -W., Lee, K., Toutanova, K.: BERT: Pre -training of Deep \nBidirectional Transformers for Language Understanding - ACL Anthology. In: \nProceedings of the 2019 Conference of the North American Chapter of the Association \nfor Computational Linguistics: Human Language Technologies. pp. 4171‚Äì4186 (2019). \n17. Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer Normalization. \nhttp://arxiv.org/abs/1607.06450. (2016). \n18. Nguyen, C.T., Truong, N. -T., Nguyen, H.T., Nakagawa, M.: Global Context for \nimproving recognition of Online Handwritten Mathematical Expressions. To appear in: \nProceedings of the International Conference on Document Analysi s and Recognition, \n14 \nICDAR (2021). \n19. Mouchere, H., Viard -Gaudin, C., Zanibbi, R., Garain, U.: ICFHR2016 CROHME: \nCompetition on Recognition of Online Handwritten Mathematical Expressions. In: 2016 \n15th International Conference on Frontiers in Handwriting Re cognition (ICFHR). pp. \n607‚Äì612 (2016). \n20. Cocke, J., T. Schwartz, J.: Programming Languages and Their Compilers: Preliminary \nNotes. Courant Institute of Mathematical Sciences, New York University (1970). \n21. Grave, E., Joulin, A., Cisse, M., Grangier, D.,  Jegou, H.: Efficient softmax \napproximation for GPUs√âdouard. In: 34th International Conference on Machine \nLearning. pp. 1302‚Äì1310 (2017). \n22. Loshchilov, I., Hutter, F.: Decoupled Weight Decay Regularization. In: International \nConference on Learning Representations (2019). \n23. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, \nT., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., \nPlu, J., Xu, C., Scao, T. Le, Gugger, S., Drame, M., L hoest, Q., Rush, A.M.: \nHuggingFace‚Äôs Transformers: State -of-the-art Natural Language Processing. \nhttp://arxiv.org/abs/1910.03771. (2019). \n ",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.8199377059936523
    },
    {
      "name": "Language model",
      "score": 0.7170900106430054
    },
    {
      "name": "Computer science",
      "score": 0.5804351568222046
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5218007564544678
    },
    {
      "name": "Natural language processing",
      "score": 0.5196998715400696
    },
    {
      "name": "Transformer",
      "score": 0.5125879645347595
    },
    {
      "name": "Security token",
      "score": 0.48184335231781006
    },
    {
      "name": "Expression (computer science)",
      "score": 0.43333059549331665
    },
    {
      "name": "Programming language",
      "score": 0.17384004592895508
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 2
}