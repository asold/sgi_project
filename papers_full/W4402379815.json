{
  "title": "PreparedLLM: effective <i>pre</i> -pretraining framework for domain-specific large language models",
  "url": "https://openalex.org/W4402379815",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2106825763",
      "name": "Zhou Chen",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2097408124",
      "name": "Ming Lin",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2143876973",
      "name": "Zimeng Wang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5092655363",
      "name": "Mingrun Zang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2109906601",
      "name": "Yuqi Bai",
      "affiliations": [
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3008110149",
    "https://openalex.org/W4390571745",
    "https://openalex.org/W3112689365",
    "https://openalex.org/W4384009677",
    "https://openalex.org/W4388047029",
    "https://openalex.org/W4392384758",
    "https://openalex.org/W4389524262",
    "https://openalex.org/W4387180269",
    "https://openalex.org/W4285294723",
    "https://openalex.org/W4381951819",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W4386113743",
    "https://openalex.org/W2134800885",
    "https://openalex.org/W3083410900",
    "https://openalex.org/W4390670134",
    "https://openalex.org/W3081464307",
    "https://openalex.org/W4387560543",
    "https://openalex.org/W4387561528",
    "https://openalex.org/W4386807359",
    "https://openalex.org/W4310638782",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W3177765786",
    "https://openalex.org/W4390215418",
    "https://openalex.org/W4393222761",
    "https://openalex.org/W4290705350",
    "https://openalex.org/W4390528775",
    "https://openalex.org/W3015453090",
    "https://openalex.org/W4390195781",
    "https://openalex.org/W2739533097",
    "https://openalex.org/W4387027586",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W4312239654",
    "https://openalex.org/W4287888698",
    "https://openalex.org/W2752630748",
    "https://openalex.org/W3127431615",
    "https://openalex.org/W4379468930",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2913323966",
    "https://openalex.org/W4321610465",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W4285819142",
    "https://openalex.org/W3099008231",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4327621204",
    "https://openalex.org/W4205523161",
    "https://openalex.org/W4361866125",
    "https://openalex.org/W4281763794",
    "https://openalex.org/W3164045210",
    "https://openalex.org/W6809982797",
    "https://openalex.org/W3169113923",
    "https://openalex.org/W4327993492",
    "https://openalex.org/W3089285634",
    "https://openalex.org/W4385766704",
    "https://openalex.org/W2963088995",
    "https://openalex.org/W3103187652",
    "https://openalex.org/W2774772126",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2591804103",
    "https://openalex.org/W2916009164"
  ],
  "abstract": "The direct application of large language models (LLMs) to specific domain tasks frequently encounters challenges due to the scarcity of domain data, variations in domain semantics, and the complexity of domain knowledge. Further pretraining of advanced foundational models on extensive domain-specific corpora can infuse these models with domain-specific knowledge and enhancing their ability of solving domain-specific tasks. However, the development of most domain-specific models focuses primarily on collecting large-scale domain data, often overlooking the crucial optimization of the pre-pretraining stage, which significantly impacts both model performance and training efficiency. This paper introduces PRE-PretrAining FRamEwork for Domain-specific Large Language Models (PreparedLLM), a framework designed to enhance the pre-pretraining stage for domain specialization of LLMs. PreparedLLM employs advanced techniques in data recipe, data cleaning, vocabulary expansion, and embedding initialization. These techniques are implemented to optimize both the composition and quality of the training data, and to enhance understanding of domain terminologies and concepts, as well as improve token embedding initialization. Utilizing the geoscience domain as a case study, this paper applies PreparedLLM for the domain specialization of the Llama, a widely recognized general-purpose LLM. Experimental results demonstrate that PreparedLLM enhances model convergence speed, training speed, inference speed, the text volume of the context window, and overall performance in domain specialization. The utilization of PreparedLLM in developing domain-specific LLMs has significantly increased performance while reducing both time and resource investment. The case study provides valuable insights into the development of domain-specific LLMs.",
  "full_text": null,
  "topic": "Domain (mathematical analysis)",
  "concepts": [
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.47466328740119934
    },
    {
      "name": "Computer science",
      "score": 0.4263029396533966
    },
    {
      "name": "Natural language processing",
      "score": 0.4097892642021179
    },
    {
      "name": "Psychology",
      "score": 0.3927006721496582
    },
    {
      "name": "Linguistics",
      "score": 0.34241265058517456
    },
    {
      "name": "Mathematics",
      "score": 0.22088325023651123
    },
    {
      "name": "Philosophy",
      "score": 0.14466211199760437
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    }
  ],
  "cited_by": 5
}