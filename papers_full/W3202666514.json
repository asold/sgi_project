{
  "title": "Layer-wise Pruning of Transformer Attention Heads for Efficient Language Modeling",
  "url": "https://openalex.org/W3202666514",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5064051041",
      "name": "Kyuhong Shim",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A5113822797",
      "name": "Iksoo Choi",
      "affiliations": [
        "SK Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5113491293",
      "name": "Wonyong Sung",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A5078440061",
      "name": "Jungwook Choi",
      "affiliations": [
        "Hanyang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3173787059",
    "https://openalex.org/W6762945437",
    "https://openalex.org/W6727099177",
    "https://openalex.org/W6768742326",
    "https://openalex.org/W6762287338",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W6765264507",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3159727696",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W3023489204",
    "https://openalex.org/W2963828549",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W3166574921",
    "https://openalex.org/W3197737164",
    "https://openalex.org/W3113057009",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2955227499",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W3098985395",
    "https://openalex.org/W2994721156"
  ],
  "abstract": "While Transformer-based models have shown impressive language modeling performance, the large computation cost is often prohibitive for practical use. Attention head pruning, which removes unnecessary attention heads in the multihead attention, is a promising technique to solve this problem. However, it does not evenly reduce the overall load because the heavy feedforward module is not affected by head pruning. In this paper, we apply layer-wise attention head pruning on All-attention Transformer so that the entire computation and the number of parameters can be reduced proportionally to the number of pruned heads. While the architecture has the potential to fully utilize head pruning, we propose three training methods that are especially helpful to minimize performance degradation and stabilize the pruning process. Our pruned model shows consistently lower perplexity within a comparable parameter size than Transformer-XL on WikiText-103 language modeling benchmark.",
  "full_text": "LAYER-WISE PRUNING OF TRANSFORMER ATTENTION HEADS\nFOR EFFICIENT LANGUAGE MODELING\nKyuhong Shim1 Iksoo Choi1 Wonyong Sung1 Jungwook Choi2\n1Seoul National University 2Hanyang University\n{skhu20, akacis, wysung}@snu.ac.kr, choij@hanyang.ac.kr\nABSTRACT\nWhile Transformer-based models have shown impressive\nlanguage modeling performance, the large computation\ncost is often prohibitive for practical use. Attention head\npruning, which removes unnecessary attention heads in\nthe multihead attention, is a promising technique to solve\nthis problem. However, it does not evenly reduce the\noverall load because the heavy feedforward module is not\naffected by head pruning. In this paper, we apply layer-\nwise attention head pruning on All-attention [1] Trans-\nformer so that the entire computation and the number of\nparameters can be reduced proportionally to the number\nof pruned heads. While the architecture has the poten-\ntial to fully utilize head pruning, we propose three train-\ning methods that are especially helpful to minimize per-\nformance degradation and stabilize the pruning process.\nOur pruned model shows consistently lower perplexity\nwithin a comparable parameter size than Transformer-\nXL on WikiText-103 language modeling benchmark.\nIndex Terms— pruning, transformer, multihead at-\ntention, language modeling\n1. INTRODUCTION\nTransformer-based neural networks [2] have been widely\nused for their great capability to capture long-range con-\ntextual relationships. Transformer models have achieved\nstate-of-the-art performance in various tasks using se-\nquential data, such as language modeling (LM) [3, 4] or\nlanguage representation learning [5, 6].\nTransformer layer consists of two sub-modules: a\nmultihead attention module (MHA) followed by a feed-\nforward module (FF). Both components behave similarly\nin that they transform representations, but the informa-\ntion they use is dearly distinct. MHA extracts features\nbased on the relationship between sequential inputs while\nFF transforms the feature irrespective of its relative loca-\ntion and value. In MHA, the connection between each\ninput is measured from multiple perspectives by dividing\nthe features into several attention heads. It has been re-\nported that each head focuses on different parts of the se-\nFig. 1. Relative computation and the number of parame-\nters reduced by attention head pruning. Transformer-XL\nand All-attention Transformer are compared.\nquence [7]. Concurrently, it has been also presented that\na considerable number of heads can be removed without\nperformance loss [8, 9, 10].\nDespite its excellent performance, the computational\ncost and the parameter size of Transformer are consider-\nably large. Attention head pruning is a promising method\nfor reducing both. This is a structured pruning approach,\nso the effect of pruning can be well reﬂected in practi-\ncal usage on modern devices, in contrast to unstructured\npruning. However, the beneﬁt only applies to MHA be-\ncause FF is not affected by the number of heads, while FF\noften takes approximately 2/3 of the parameters and half\nof the computations (depending on the sequence length\nand model conﬁguration). To further extend the ability to\ncompress Transformer models with attention head prun-\ning, we adopt the recently introduced All-attention [1]\nTransformer, which adds persistent memory blocks in-\nside MHA, instead of FF. We denote All-attention Trans-\nformer as All-att for simplicity.\nAll-att uniﬁes two sub-modules in the original Trans-\nformer and split almost every computation under the\nmultihead path, which is a desirable characteristic for\nattention head pruning. Figure 1 demonstrates the ad-\nvantage of the attention head pruning on All-att com-\npared to a Transformer-XL (TXL) [3], which is a widely\nadopted model for LM. For example, in 50% head spar-\nsity, TXL computes approximately 73% of full multiply-\narXiv:2110.03252v1  [cs.CL]  7 Oct 2021\naccumulate operations (MAC) and maintains 81% of the\nparameters, whereas All-att only requires 50% of the\nload and 50% of the parameters.\nIn pruning attention heads, we utilize a trainable\nmethod so the model can jointly learn which heads can\nbe pruned out while preserving the performance. Specif-\nically, we attach auxiliary gating parameters on each\nlayer, inspired by earlier works [8, 11]. Although All-att\nshows comparable performance to the original Trans-\nformer in LM, removing each attention head of All-att\nis directly connected to losing the information inside\npersistent memory which replaces the role of the FF. We\nidentify several difﬁculties in the pruning process; severe\ninstability at the initial stage, consistent increase of the\ntraining loss, overly sparse heads, and signiﬁcant perfor-\nmance drop of the pruned model. Therefore, we propose\nthree techniques that modify the pruning process to solve\nthese problems: (1) sparsity loss warm-up, (2) proper\ninitialization, and (3) attention output scaling.\nOur main contributions are summarized as follows.\nFirst, we adopt All-att to fully utilize the advantages of\nattention head pruning. Second, we propose advanced\ntraining techniques to minimize the damage to the per-\nformance of the pruned model and stabilize the pruning\nprocess. We demonstrate that our pruned All-att model\nshows consistently lower perplexity for word-level LM\nand lower bit-per-character for character-level LM, com-\npared to the original Transformer model of a comparable\nparameter size.\n2. RELATED WORK\nPruning on Transformer has been widely studied. Re-\nsearch on unstructured pruning [12, 13] shows that sev-\neral parameters can be removed without a signiﬁcant ef-\nfect on the ﬁnal performance. However, the unstructured\nnature is practically difﬁcult to take advantage of the ac-\ntual speedup without specialized hardware support [14].\nSeveral studies have focused on attention head re-\nmoval, which is a structured and GPU-friendly approach.\nThe most adopted method begins from a fully converged\npretrained model and prunes out attention heads during\nadditional training steps. For example, in [8], trainable\ngating parameters are attached to each head and regular-\nized with L0 loss. Other types of head pruning have also\nbeen proposed; without additional parameters, in [9], the\nsensitivity of each head to the loss is used as a proxy\nfor importance. A single-shot meta-pruner [15] is intro-\nduced in which a small convolutional neural network is\ntrained to select heads that contribute to maintaining the\nattention distribution. Because earlier studies on atten-\ntion head pruning do not compress FF, additional effort\nis needed to further reduce the computation and parame-\nters of FF for the original Transformer.\nFig. 2. All-attention Transformer with a head gating\nmodule. K and V indicates persistent vectors that replace\nthe role of the feedforward in original Transformer. The\nparts that disappear (dashed boxes) represent the saved\noperations of erased attention heads.\n3. ATTENTION HEAD PRUNING FOR LM\n3.1. All-Attention Transformer\nAll-att adds a set of trainable parameters, named as per-\nsistent vectors, instead of FF. These persistent vectors\nperform as an external key and value for Transformer but\ndo not depend on the input. Figure 2 illustrates an All-\natt layer. For simplicity, we omit the relative positional\nencoding and its projection in equations and the ﬁgure.\nWhen All-att architecture is used for LM, the mem-\nory caching algorithm of TXL is adopted. The hid-\nden representation computed for the previous sequence\nsegment is cached as memory and used as an exter-\nnal source of information. This memory mechanism\nenables much longer context, which is highly bene-\nﬁcial for LM. Consider a sequence of d-dimensional\nvector X={x1,x2,...xT }and a corresponding memory\nM={m1,m2,...mS}. The query (Q), key (K), and value\n(V) of i-th head is calculated as Qi = WQ\ni X, Ki =\nWK\ni [X,M], Vi = WV\ni [X,M]. The concatenation oper-\nator is noted as [ , ]. For the entire model, H heads are\nused per layer and Llayers are stacked.\nThe persistent vectors are realized as N trainable dh-\ndimensional vectors for each head, where dh=d/His the\nhead dimension. PK\ni ={pk\ni,1,...pk\ni,N }and PV\ni ={pv\ni,1,...pv\ni,N }\nrepresent the persistent key and value vectors of the i-th\nhead. Every query in the sequence treats PK\ni and PV\ni as\nextensions of K and V, respectively. The output of the\ni-th head is calculated as:\nhi = Softmax\n(\nQi[Ki,PK\ni ]T /\n√\ndh\n)\n[Vi,PV\ni ] (1)\nThe outputs from multiple attention heads are con-\ncatenated and projected to produce the ﬁnal result.\nO= WO[h1,h2,...hH] =\nH∑\ni=1\nWO\ni hi (2)\nBy setting the number of persistent vectors N same\nas the internal dimension of FF, the number of parameters\nof All-att becomes almost identical to that of the original\nTransformer (both MHA and FF).\n3.2. Head Pruning by Gating\nFor pruning, we attach a set of trainable head gating\nparameters Π={π1,π2,..πH}(πi ∈ R) to each layer.\nThe parameters pass through BinConcrete [16] func-\ntion and converted to stochastic discrete Bernoulli gate\nG={g1,g2,...gH}(gi ∈{0,1}). The ﬁnal projection is\nmodiﬁed as follows:\nO= sg\nH∑\ni=1\ngihiWO\ni = H∑H\ni=1 gi\nH∑\ni=1\ngihiWO\ni (3)\nTo avoid division by zero (if all gates are sampled to\n0), we clip sg to the maximum value of H. Because we\ncan easily absorb sg in the WO, the scaling sg does not\nrequire additional computation for the inference.\nIn addition to the default negative log-likelihood\n(nll) loss, we utilize additional L0 (sparsity) loss 1 to\nencourage higher sparsity explicitly. The overall loss\nis a weighted sum of both: Ltotal = Lnll + λLsparsity.\nThe weighting coefﬁcient λ controls the ﬁnal sparsity.\nWhen the i-th head is decided to be pruned ( gi = 0 ),\nwe remove the parameters corresponding to the head\n{WQ\ni ,WK\ni ,WV\ni ,PK\ni ,PV\ni ,WO\ni }. Concurrently, their cor-\nresponding computations are removed.\n3.3. Techniques for Head Pruning\nPruning begins from the converged model that is previ-\nously trained without an augmented gating mechanism;\ntherefore, the addition of attention head gating exces-\nsively changes the activation statistics and training dy-\nnamics. When this discrepancy is combined with the\nunique characteristics of All-att, we observe a signiﬁcant\nperformance drop and a severe instability of the prun-\ning process, particularly in the initial training phase. To\novercome the difﬁculties, we introduce three techniques\nto overcome the difﬁculties of pruning on All-att models.\nFirst, we linearly increase the sparsity loss coefﬁcient\nλfrom zero to the desired value. Gradual increase of λ\n1Please refer to [16] for L0 loss and BinConcrete function.\nTable 1. Effects of attention head pruning on the\nWikiText-103 test dataset. The number of parameters re-\nported does not include token embedding and output pro-\njection weight. The embedding and projection occupy\n19.6M parameters.\nλ Sparsity (%) #Params(w/o emb.) ppl\n0(base) 0 54.6M 23.24\n0.01 17.2 45.2M 23.45\n0.015 32.8 36.7M 24.07\n0.02 43.8 30.7M 24.85\nprevents the L0 loss to overly disturb the network adapt-\ning to the stochastic activation at the beginning of the\npruning process. Note that the L0 objective is a pow-\nerful pressure that can be always achieved by decreasing\nthe gating parameter values, which leads to the consistent\nincrease of the sparsity.\nSecond, we initialize gating parameters Π to a large\npositive value to bias the sampled stochastic gates to be\nopened (gi = 1) at the beginning. The zero initialization\nopens a gate with only 50% probability. In that case, up-\nper layers only receive abruptly reduced information and\nquickly loss the existing well-trained internal structure.\nWe initialize Π to 2, which takes about an 88% probabil-\nity of gate to be opened.\nThird, as expressed in Eq.(3), we scale the output in-\nversely proportional to the (1-sparsity). The scaling fac-\ntor sg compensates for the masked portion and maintains\nthe statistics after gating is applied. Recently, attention\nhead dropout [17, 18] has been introduced with similar\nscaling, but their scaling is used for regularization dur-\ning training. We found that this technique greatly stabi-\nlizes the training dynamics, especially after the training\nis stabilized by the above two methods. Without output\nscaling, we observe a consistent increase in the training\nloss.\n4. EXPERIMENTAL RESULTS\n4.1. Setup\n4.1.1. Datasets and Model Architecture\nWe evaluate the performance on WikiText-103 [19]\nword-level LM and Text8 [20] character-level LM bench-\nmarks. The pre-processing of datasets follows common\npractice [3]. The performance is reported in perplexity\n(ppl) for WikiText-103 and bit-per-character ( bpc) for\nText8. Lower is better for both ppl and bpc.\nThe baseline model is a variant of All-attention\nTransformer. The model adopts a pre-norm instead of a\npost-norm and omits the adaptive-span [4] mechanism.\nTable 2. Effects of attention head pruning on the Text8\ntest set. The embedding and projection occupy 0.6M pa-\nrameters.\nλ Sparsity (%) #Params(w/o emb.) bpc\n0(base) 0 40.9M 1.199\n0.01 15.6 34.5M 1.204\n0.015 27.1 29.8M 1.218\n0.02 37.5 25.6M 1.234\nTable 3. Ablation study of each proposed technique. The\nresults are evaluated on the WikiText-103 test dataset. λ\nis set to 0.02. \"vanilla\" means no technique is applied.\nAblation ∆ppl\n- λwarm-up (X,O,O) +0.12\n- proper gate initialization (O,X,O) +0.61\n- attention head output scaling (O,O,X) +1.25\n- all (vanilla) (X,X,X) +1.48\nThe conﬁguration of the transformer layer is as follows:\nthe hidden dimension d= 512, number of heads H = 8,\nand number of persistent vectors N = 2048. We stack\n16 layers for WikiText-103 and 12 layers for Text8.\n4.1.2. Training Details\nWe ﬁrst train the baseline model with full attention\nheads. We utilize the LAMB optimizer with a batch\nsize of 96 for WikiText-103 and 64 for Text8. The se-\nquence length T and memory length S are both set to\n192 for WikiText-103 and 512 for Text8. We apply linear\nwarm-up on learning rate for 4K iterations. The learning\nrate increases to 1e-2 and gradually decreased by cosine\nlearning rate scheduling to 1e-4. The training requires\n160K iterations to converge. We use a dropout rate of\n0.2 for attention matrices, 0.1 for embedding and hidden\nactivation.\nWe start pruning from the converged baseline. Prun-\ning follows identical training conﬁgurations except for\nthe learning rate, which increases to 1e-3 and gradually\ndecreased to 1e-4. The pruning requires additional 80K\niterations of training. As explained in Sec.3.3, we warm-\nup λfrom zero to the desired value for the ﬁrst 4K iter-\nations. After 16K iterations, we stop training the gat-\ning parameters, so that the training continues without\nrandomness for the remaining steps. Without this ﬁxa-\ntion, we observe that the sparsity continues to increase\nbecause of the inﬂuence of L0 loss becomes too large,\nwhich causes the network much difﬁcult to be ﬁne-tuned.\nWe exploreλ= {1.0,1.5,2.0}·10−2 to control the trade-\noff between the sparsity and performance.\nFig. 3. Perplexity of the pruned All-attention Trans-\nformer and TXL. The former shows a better parameter\nefﬁciency.\n4.2. Results on Language Modeling\nTables 1 and 2 show the results of attention head pruning\non two benchmarks. As expected, the number of param-\neters linearly decreases as sparsity increases on All-att\nmodels. We observe a clear trade-off between sparsity\nand performance for both datasets.\nTo compare with the original Transformer architec-\nture, we train TXL models with reduced dimensions un-\nder the same conﬁguration. Each TXL model utilizes the\nsame number of layers and heads, whereas the hidden\ndimension decreases from 512 by 32 in sequence. Both\nAll-att and TXL baselines achieve almost same perplex-\nity and the parameter size. Figure 3 shows that All-att\nmodels with attention head pruning achieve substantially\nbetter parameter efﬁciency than the TXL models. For ex-\nample, pruned All-att model with 43% sparsity (30.7M)\nachieves similar perplexity as TXL with only 25% spar-\nsity (47.9M).\nWe empirically show that the proposed three meth-\nods each contribute to the improvement. Table 3 com-\npares the effect of each technique by ablation. The most\ninﬂuential change is achieved by output scaling (+1.25),\nhowever, the other two also take a portion of the improve-\nment. All-att model without proposed techniques (de-\nnoted as \"vanilla\"), is expected to suffer from a similar\nlevel of performance degradation as TXL, which implies\nthat the potential of pruning efﬁciency on All-att cannot\nbe fully utilized without our techniques.\n5. CONCLUSION\nIn this paper, we introduced layer-wise attention head\npruning for All-attention Transformer models and pro-\nposed three techniques to reduce the performance degra-\ndation of the pruned model and stabilize the pruning pro-\ncess. Experiments on language modeling demonstrate\nthat the proposed method achieves a better performance\nthan traditional Transformer models with a comparable\nnumber of parameters.\n6. REFERENCES\n[1] Sainbayar Sukhbaatar, Edouard Grave, Guillaume\nLample, Herve Jegou, and Armand Joulin, “Aug-\nmenting self-attention with persistent memory,”\narXiv preprint arXiv:1907.01470, 2019.\n[2] Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin, “Attention is\nall you need,” in Advances in neural information\nprocessing systems, 2017, pp. 5998–6008.\n[3] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G\nCarbonell, Quoc Le, and Ruslan Salakhutdinov,\n“Transformer-xl: Attentive language models be-\nyond a ﬁxed-length context,” in Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, 2019, pp. 2978–2988.\n[4] Sainbayar Sukhbaatar, Édouard Grave, Piotr Bo-\njanowski, and Armand Joulin, “Adaptive attention\nspan in transformers,” in Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, 2019, pp. 331–335.\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova, “Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning,” in Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics, 2019, pp. 4171–4186.\n[6] Tom B Brown, Benjamin Mann, Nick Ryder,\nMelanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, et al., “Language models are few-\nshot learners,” arXiv preprint arXiv:2005.14165 ,\n2020.\n[7] Kevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D Manning, “What does bert look at?\nan analysis of bert’s attention,” in Proceedings of\nthe 2019 ACL Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , 2019,\npp. 276–286.\n[8] Elena V oita, David Talbot, Fedor Moiseev, Rico\nSennrich, and Ivan Titov, “Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned,” in Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, 2019, pp. 5797–5808.\n[9] Paul Michel, Omer Levy, and Graham Neubig, “Are\nsixteen heads really better than one?,” Advances\nin Neural Information Processing Systems, vol. 32,\npp. 14014–14024, 2019.\n[10] Yaru Hao, Li Dong, Furu Wei, and Ke Xu, “Self-\nattention attribution: Interpreting information inter-\nactions inside transformer,” in Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence , 2021,\nvol. 35, pp. 12963–12971.\n[11] Babak Ehteshami Bejnordi, Tijmen Blankevoort,\nand Max Welling, “Batch-shaping for learning con-\nditional channel gated networks,” in International\nConference on Learning Representations, 2019.\n[12] Demi Guo, Alexander M Rush, and Yoon Kim,\n“Parameter-efﬁcient transfer learning with diff\npruning,” arXiv preprint arXiv:2012.07463, 2020.\n[13] Victor Sanh, Thomas Wolf, and Alexander Rush,\n“Movement pruning: Adaptive sparsity by ﬁne-\ntuning,” Advances in Neural Information Process-\ning Systems, vol. 33, 2020.\n[14] Hanrui Wang, Zhekai Zhang, and Song Han, “Spat-\nten: Efﬁcient sparse attention architecture with cas-\ncade token and head pruning,” in 2021 IEEE In-\nternational Symposium on High-Performance Com-\nputer Architecture (HPCA) . IEEE, 2021, pp. 97–\n110.\n[15] Zhengyan Zhang, Fanchao Qi, Zhiyuan Liu, Qun\nLiu, and Maosong Sun, “Know what you\ndon’t need: Single-shot meta-pruning for attention\nheads,” AI Open, vol. 2, pp. 36–42, 2021.\n[16] Christos Louizos, Max Welling, and Diederik P\nKingma, “Learning sparse neural networks through\nl_0 regularization,” in International Conference on\nLearning Representations, 2018.\n[17] Wangchunshu Zhou, Tao Ge, Furu Wei, Ming\nZhou, and Ke Xu, “Scheduled drophead: A reg-\nularization method for transformer models,” in\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: Find-\nings, 2020, pp. 1971–1980.\n[18] Shucong Zhang, Erfan Loweimi, Peter Bell, and\nSteve Renals, “Stochastic attention head removal:\nA simple and effective method for improving trans-\nformer based asr models,” inINTERSPEECH 2021,\n2021.\n[19] Stephen Merity, Caiming Xiong, James Bradbury,\nand Richard Socher, “Pointer sentinel mixture mod-\nels,” arXiv preprint arXiv:1609.07843, 2016.\n[20] Matt Mahoney, “Large text compression bench-\nmark,” http://mattmahoney.net/dc/\ntextdata, 2009.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9069507718086243
    },
    {
      "name": "Language model",
      "score": 0.8071925640106201
    },
    {
      "name": "Transformer",
      "score": 0.7684438228607178
    },
    {
      "name": "Computer science",
      "score": 0.7633979916572571
    },
    {
      "name": "Computation",
      "score": 0.6851527690887451
    },
    {
      "name": "Pruning",
      "score": 0.5954560041427612
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.43660303950309753
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4323994815349579
    },
    {
      "name": "Machine learning",
      "score": 0.3518337309360504
    },
    {
      "name": "Algorithm",
      "score": 0.26249921321868896
    },
    {
      "name": "Engineering",
      "score": 0.07265442609786987
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Agronomy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I139264467",
      "name": "Seoul National University",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I10654025",
      "name": "SK Group (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4575257",
      "name": "Hanyang University",
      "country": "KR"
    }
  ]
}