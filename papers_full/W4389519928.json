{
  "title": "Towards Mitigating LLM Hallucination via Self Reflection",
  "url": "https://openalex.org/W4389519928",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4222246742",
      "name": "Ji, Ziwei",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4222272757",
      "name": "Yu, Tiezheng",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A760818427",
      "name": "Xu Yan",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3037325056",
      "name": "Lee， Na-Yeon",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A3162160406",
      "name": "Ishii, Etsuko",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": null,
      "name": "Fung, Pascale Ngan",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2891113091",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4381714872",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3100468923",
    "https://openalex.org/W2241687848",
    "https://openalex.org/W3047636089",
    "https://openalex.org/W3106969213",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W3212648282",
    "https://openalex.org/W3176793246",
    "https://openalex.org/W3175139183",
    "https://openalex.org/W3168090480",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W4297899309",
    "https://openalex.org/W2963866616",
    "https://openalex.org/W4319793767",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2977131552",
    "https://openalex.org/W4226375873",
    "https://openalex.org/W3128912454",
    "https://openalex.org/W4310926773",
    "https://openalex.org/W1981208470",
    "https://openalex.org/W4221154592",
    "https://openalex.org/W3031200717",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4385571050",
    "https://openalex.org/W4296870224",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4386566577",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W4303685607",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W3090073303",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W2963175042",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W3169283738",
    "https://openalex.org/W4389520749",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W4307123345",
    "https://openalex.org/W4301243929",
    "https://openalex.org/W3158616732",
    "https://openalex.org/W4302305884",
    "https://openalex.org/W3214498419",
    "https://openalex.org/W3020152921",
    "https://openalex.org/W4389519585",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W2963506049",
    "https://openalex.org/W2970986790",
    "https://openalex.org/W2252016937",
    "https://openalex.org/W4226101361",
    "https://openalex.org/W136732505",
    "https://openalex.org/W4362598378",
    "https://openalex.org/W3167021021",
    "https://openalex.org/W2913352150",
    "https://openalex.org/W2887528103",
    "https://openalex.org/W4309088836",
    "https://openalex.org/W4366327625",
    "https://openalex.org/W2970641574"
  ],
  "abstract": "Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks including question-answering (QA) tasks. However, the practical deployment still faces challenges, notably the issue of \"hallucination\", where models generate plausible-sounding but unfaithful or nonsensical information. This issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets. Our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. To tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation. Through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers. Consequently, we harness the interactivity and multitasking ability of LLMs and produce progressively more precise and accurate answers. Experimental results on both automatic and human evaluation demonstrate the superiority of our approach in hallucination reduction compared to baselines. © 2023 Association for Computational Linguistics.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1827–1843\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nTowards Mitigating Hallucination in Large Language Models\nvia Self-Reflection\nZiwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, Pascale Fung\nCenter for Artificial Intelligence Research (CAiRE)\nHong Kong University of Science and Technology\nzjiad@connect.ust.hk, pascale@ece.ust.hk\nAbstract\nLarge language models (LLMs) have shown\npromise for generative and knowledge-\nintensive tasks including question-answering\n(QA) tasks. However, the practical deployment\nstill faces challenges, notably the issue of\n\"hallucination\", where models generate\nplausible-sounding but unfaithful or non-\nsensical information. This issue becomes\nparticularly critical in the medical domain due\nto the uncommon professional concepts and\npotential social risks involved. This paper\nanalyses the phenomenon of hallucination\nin medical generative QA systems using\nwidely adopted LLMs and datasets. Our\ninvestigation centers on the identification\nand comprehension of common problematic\nanswers, with a specific emphasis on hallucina-\ntion. To tackle this challenge, we present an\ninteractive self-reflection methodology that\nincorporates knowledge acquisition and answer\ngeneration. Through this feedback process,\nour approach steadily enhances the factuality,\nconsistency, and entailment of the generated\nanswers. Consequently, we harness the\ninteractivity and multitasking ability of LLMs\nand produce progressively more precise and\naccurate answers. Experimental results on both\nautomatic and human evaluation demonstrate\nthe superiority of our approach in hallucination\nreduction compared to baselines.1\n1 Introduction\nLarge language models (LLMs) have shown\npromise for generative and knowledge-intensive\ntasks which require world or domain knowl-\nedge (Petroni et al., 2021). One representa-\ntive task is generative question-answering (GQA)\nwhich provides relevant information in response to\nqueries (Li et al., 2021a; Su et al., 2023; Nakano\net al., 2021) and can also serve to probe the capabil-\nities of language models (Wang et al., 2021). How-\n1https://github.com/ziweiji/Self_Reflection_\nMedical\nWhat causes Noonan syndrome?\nNoonan syndrome is caused by a mutation in the\nPTEN gene. This gene is responsible for regulating\ncell growth and division, and when it is mutated, it\ncan lead to the development of Noonan syndrome.\nNoonan syndrome is caused by mutations in one of\nseveral different genes, including PTPN11, SOS1,\nKRAS, NRAS, RAF1, BRAF, or MEK1. \nLLM\nReference\nFigure 1: An example of hallucinated Generative QA2.\nPTEN mutation is not typically associated with Noo-\nnan syndrome3. Thus, despite the plausible sounding\nanswer, it is a hallucinated one. We highlight the hallu-\ncinated parts in pink and underline them.\never, despite their potential and recent advance-\nments, these models face a concerning issue, “hal-\nlucination”, a phenomenon where the model gen-\nerates plausible-sounding but unfaithful or nonsen-\nsical information (Ji et al., 2023). In the medical\ndomain, the challenges posed by hallucination are\nparticularly critical, as inaccurate or misleading\ninformation can have severe consequences for pa-\ntient care (Puchert et al., 2023). Furthermore, un-\ncommon professional concepts complicate medical\nGQA task (Kaddari et al., 2020). For the GQA ex-\nample in Figure 1, PTEN (Phosphatase and tensin\nhomolog) mutation is not typically associated with\nNoonan syndrome, so the answer sounds plausible\nbut is hallucinated. Therefore, it is crucial to under-\nstand and mitigate these hallucinations to ensure\nthe reliability and safety of AI-enabled medical\nservices (Morley et al., 2020).\nParallel to retrieving external relevant knowl-\nedge (Lewis et al., 2020; Guu et al., 2020; Izacard\net al., 2022), some current works (Yu et al., 2023;\nWang et al., 2021; Roberts et al., 2020; Xu et al.,\n2directly generated by Alpaca-Lora.\n3According to https://www.chop.edu/\nconditions-diseases/noonan-syndrome and\nhttps://en.wikipedia.org/wiki/Noonan_syndrome.\n1827\n2022; Sun et al., 2023) explore leveraging the para-\nmetric knowledge in LLMs and tap into their po-\ntential for knowledge-intensive tasks. GQA works\nin other domains (Lin et al., 2021; Su et al., 2022)\nunderscore the importance of addressing halluci-\nnation and improving faithfulness. However, the\ncurrent understanding of the extent of hallucination\nin medical answers generated by LLMs remains\nunclear, and there is a need to explore the potential\nfor further improvement in this aspect.\nTo fill the gap, this study investigates halluci-\nnation in the context of medical GQA systems,\nparticularly in general LLMs like Vicuna (Chi-\nang et al., 2023), Alpaca-LoRA (Wang, 2023),\nChatGPT (OpenAI, 2023a), and medical LLMs\nlike MedAlpaca (Han et al., 2023), and Robin-\nmedical (Diao et al., 2023) on popular med-\nical datasets: PubMedQA (Jin et al., 2019),\nMedQuAD (Ben Abacha and Demner-Fushman,\n2019), MEDIQA2019 (Ben Abacha et al., 2019),\nLiveMedQA2017 (Ben Abacha et al., 2017),\nMASH-QA (Zhu et al., 2020). We evaluate the\nincidence of hallucinations, explore the potential\ncause, and propose a strategy to mitigate this issue.\nThrough a comprehensive analysis of the recent\nmodels, we hope to increase the understanding of\nhallucination in these systems and provide a road\nmap towards more reliable AI-assisted healthcare.\nCurrent research works (Yin et al., 2023; Burns\net al., 2022; Rajpurkar et al., 2018; Kadavath et al.,\n2022; Manakul et al., 2023) highlight a gap be-\ntween surface realization and inherent knowledge\nin NLG tasks. Models can realize they are gener-\nating something hallucinated in some way. To re-\nduce this gap and mitigate hallucinations in medical\nGQA, we devise an iterative, introspective process\nthat leverages the multi-turn interactivity and multi-\ntask ability of LLMs. Our self-reflective methodol-\nogy initiates the generation of pertinent background\nknowledge for a given question, followed by a fac-\ntuality evaluation. Upon detection of discrepancies,\nthe model is urged to self-correct, utilizing its in-\nherent reflective capacities to refine the knowledge.\nThis cyclical process is repeated until a satisfactory\nlevel of factuality is achieved. In the answering\nstage, we employ a similar generation-score-refine\nstrategy to ensure consistency between the gener-\nated answer and the background knowledge. Ad-\nditionally, an entailment evaluation is conducted\nbetween the answer and the question. If the gener-\nated answer fails to meet the standard, the process\nreturns to the initial stage, repeating the cycle. Our\nmethod fosters a dynamic interaction between the\nsystem and its knowledge, enhancing the model’s\nability to provide accurate, reliable, and factually\ngrounded responses in healthcare settings.\nThe experimental results of our method show-\ncase its effectiveness across LLMs with varying\nparameters, including 7B and 175B, on all five\ndatasets. This robust performance highlights the\ngeneralizability and scalability of our approach, fur-\nther validating its efficacy in the context of medical\nquestion-answering tasks. Our method explores ex-\ntracting and digging knowledge from LLMs, lever-\naging their full potential, and strives to approach\ntheir upper performance bound. Our method can\nalso combine other techniques such as utilizing\nexternal knowledge and more powerful LLMs in\nthe future to contribute to the development of ro-\nbust application systems. In summary, the major\ncontributions of this work are threefold:\n• We provide a comprehensive examination\nof the hallucination phenomenon in medical\nGQA systems, particularly those employing\nfive LLMs across five medical GQA datasets.\n• We propose an innovative self-reflection\nmethod to mitigate hallucination in LLMs.\nThe iterative feedback loop process generates,\nscores, and refines knowledge and answers\nuntil they reach satisfactory levels, enhancing\nthe accuracy and reliability of the answers.\n• Experimental results demonstrate the effec-\ntiveness, generalizability, and scalability of\nour method. This investigation sheds light\non the potential of LLMs as valuable tools\nfor medical GQA tasks, showcasing their abil-\nity to provide meaningful insights without ex-\nplicit training on specific datasets.\n2 Related Work\nMedical Question Answering Medical QA sys-\ntems have shown significant potential for enhanc-\ning information accessibility and understanding in\nthe healthcare field. These systems respond to vari-\nous question formats, including Yes/No (Tsatsaro-\nnis et al., 2015; Jin et al., 2019), multi-choice (Pal\net al., 2022; Li et al., 2021b; Welbl et al., 2018;\nBerant et al., 2014; Abacha et al., 2019; Morante\net al., 2012), extractive (Tsatsaronis et al., 2015;\nDhingra et al., 2018; Šuster and Daelemans, 2018;\nTang et al., 2020; Möller et al., 2020; Pampari\net al., 2018), and generative (Savery et al., 2020;\n1828\nCategory Question Generated Answer Golden Answer\nFact\nInconsistency\nIs Noonan syndrome\ninherited?\nNoonan syndrome isnot inherited. It is a ge-\nnetic mutation that occurs sporadically.\nNoonan syndromeis inherited in an autosomal\ndominant manner... Each child of a person with\nNoonan syndrome has a 50% chance to inherit\nthe condition...\nQuery\nInconsistency\nCardiac reoperations\nin octogenarians: Do\nthey really benefit?\nProper use ofVitamin C to stop smoking; Tak-\ning Vitamin C by mouth can help smokers quit...\nYes... sustainable recovery of the survivors of-\nfering the prospect of a highly independent and\nsatisfying life...\nTangentiality\nc-Kit-dependent\ngrowth of uveal\nmelanomacells: a\npotential therapeutic\ntarget?\nA muscle tumor formed in the anterior eyeball\nof the body, calleduveal melanoma, is a rare\ntumor that often leads to visual impairment ...\nthe addition ofGATA3can reduce the scale of\ncancer...\nYes ... c-kit is vastly expressed in uveal\nmelanoma, suggest that the c-kit molecular\npathway may be important in uveal melanoma\ngrowth...\nTable 1: The category of problematic answers with corresponding representative examples.\nMollá et al., 2016; Jin et al., 2019; Ben Abacha\nand Demner-Fushman, 2019; Ben Abacha et al.,\n2019, 2017). The introduction of pre-trained lan-\nguage models has further enhanced the capabilities\nof GQA systems, enabling them to generate fluent\nand meaningful responses to medical queries (Soni\nand Roberts, 2020; Liu et al., 2022; Savery et al.,\n2020; Alsentzer et al., 2019; Kaddari et al., 2020).\nHallucination in Generative Question Answer-\ning Faithful GQA, which aims to generate an-\nswers strictly grounded in the source text or valid\nexternal knowledge, has gained significant research\nattention (Nakano et al., 2021; Su et al., 2022,\n2023). The more faithful the answer is, the less\nhallucinated content it contains. Other terms like\nsemantic drift, factual correctness can also reflect\nhallucination level (Li et al., 2021a; Su et al., 2022).\nRationale-Enriched Answer Generator (REAG) (Li\net al., 2021a) add an extraction task to obtain an-\nswer rationale and generate answers with high con-\nfidence. Read-before-Generate (Su et al., 2022)\ncombines answer generation with machine reading\nto incorporate fine-grained, answer-related salient\ninformation. A benchmark (Lin et al., 2021) mea-\nsures the truthfulness of answers generated by lan-\nguage models across various domains.These stud-\nies underscore the importance of reducing halluci-\nnation, a key focus of our work.\nLarge Language Models The advent of LLMs,\nincluding GPT-3 (Brown et al., 2020), Chat-\nGPT (OpenAI, 2023a), LLaMA (Touvron et al.,\n2023), and GPT-4 (OpenAI, 2023b), has revolution-\nized natural language processing tasks, showcasing\ntheir impressive language capabilities in generat-\ning fluent, contextually relevant responses (Brown\net al., 2020; OpenAI, 2023a; Touvron et al., 2023;\nOpenAI, 2023b). In addition, emergent abili-\nties are revealed from these models, such as in-\ncontext learning (Min et al., 2022), zero-shot in-\nstruction (Ouyang et al., 2022; Wei et al., 2021),\nand chain-of-thought reasoning (Wei et al.). How-\never, their deployment in practical applications has\nalso surfaced challenges related to the control, bias,\nand reliability (Tamkin et al., 2021), where halluci-\nnation has recently become an increasingly visible\nissue (OpenAI, 2023a; Bang et al., 2023).\n3 Analysis of Hallucination\nIn this section, we directly ask LLMs medical ques-\ntions from five datasets leveraging their zero-shot\ncapability. We then comprehensively evaluate and\nanalyze the generated answers, with a focus on\nexamining the occurrence of hallucination.\n3.1 Models\nWe evaluate the generated answers from five LLMs,\nincluding three general LLMs and two LLMs fine-\ntuned in the medical domain. Vicuna (Chiang\net al., 2023) is trained by fine-tuning LLaMA on\nuser-shared conversations from ShareGPT.Alpaca-\nLoRA (Wang, 2023) employs Low-Rank Adapta-\ntion (LoRA) to replicate the results of Stanford’s\nAlpaca model. ChatGPT (OpenAI, 2023a) in-\nterprets prompts and provides comprehensive re-\nsponses using Reinforcement Learning from Hu-\nman Feedback (RLHF). MedAlpaca (Han et al.,\n2023) is built upon the frameworks of LLaMA and\nfine-tuned on instruction-tuning formatted medical\ndialogue and QA texts. Robin-medical (Diao et al.,\n2023) is fine-tuned LLaMA in the medical domain\nusing LMFlow.\n3.2 Dataset\nPubMedQA (Jin et al., 2019) is a biomedical\nQA dataset containing 1k expert-labeled instances\nwhich include questions derived from research\narticle titles, abstracts as the context, long an-\nswers from abstract conclusions, and concise\n1829\nyes/no/maybe answers. MedQuAD (Ben Abacha\nand Demner-Fushman, 2019) comprises 47,457\nQA pairs from National Institutes of Health web-\nsites and covers various medical topics including\ndiseases, medications, and diagnostic tests. We use\nthe medical QA dataset from MEDIQA2019 (Ben\nAbacha et al., 2019) challenge and consider an-\nswers with scores 3 and 4 as golden answers.\nLiveMedQA2017 (Ben Abacha et al., 2017) con-\ntains annotated medical QA pairs for question anal-\nysis and answering systems. MASH-QA (Zhu\net al., 2020) includes 34k QA pairs from the con-\nsumer health domain designed for Multiple Answer\nSpans Healthcare QA. Except for PubMedQA, an-\nswer annotation in these datasets involves manual\nextraction and copying from authentic web con-\ntent. While the answers are pertinent and verifiable,\nthere is room for improvement in terms of contex-\ntual coherence and question linkage. Please see\nAppendix A for details and an example.\n3.3 Evaluation Protocols\nTo evaluate the generation quality, we follow\nthe previous work (Su et al., 2022) utilizing\nGQA metrics: unigram F1 and ROUGE-L (Lin,\n2004). However, the widely used n-gram similar-\nity metrics often fail to discriminate the halluci-\nnated/incorrect answers and correlate weakly with\nhuman judgments (Lee et al., 2021; Zhou et al.,\n2021). We introduce Med-NLI (Medical Natural\nLanguage Inference) to assess the logical consis-\ntency/entailment of generated answers with the pro-\nvided context or the reference answer. We adopt\nSciFive (Phan et al., 2021), a T5 model pre-trained\non extensive biomedical corpora. Our evaluation\noccurs at two levels: Sample-level Med-NLI evalu-\nates whether each generated answer entails (1), is\nneutral (0), or contradicts (-1) the context or the\nreference answer. Sentence-level Med-NLI deter-\nmines the same but for each individual sentence\nwithin the generated response. We also use CTRL-\nEval (Ke et al., 2022), an unsupervised, reference-\nfree, and task-agnostic evaluation metric that as-\nsesses generation from various aspects by formulat-\ning each aspect into multiple text-infilling tasks. In\nour work, we specifically employ the consistency\naspect of this metric.\n3.4 Results and Discussion\nTable 2 shows the experimental results on auto-\nmatic metrics over the test sets from five datasets.\nError Analysis After analyzing 250 directly gen-\nerated examples from the five models, we classify\nproblematic answers into three categories: Fact\nInconsistency, Query Inconsistency, and Tangen-\ntiality. Please refer to Table 1 and Figure 2 for the\nrepresentative example and incidence for each cat-\negory and model. We consider the first two as the\nhallucination problem.\n1. Fact Inconsistency refers to answers that pro-\nvide information that is inconsistent or in conflict\nwith the fact. It arises when the model fails to\nappropriately recall relevant knowledge when re-\nsponding to the question. The example answer in\nTable 1 incorrectly states that Noonan syndrome is\nnot inherited while it is inherited in an autosomal\ndominant manner.\n2. Query Inconsistency refers to answers that are\nunrelated to the query or nonsensical. It occurs\nwhen the model neither responds to the question\nnor invokes relevant knowledge appropriately. The\nexample answer in Table 1 discusses the benefits of\nVitamin but does not mention cardiac reoperations.\n3. Tangentiality refers to answers that provide\ninformation related to the topic but do not directly\naddress the question. It occurs when the model\ndoes not further process mastered knowledge, such\nas inductive, deductive, and logical reasoning. The\nexample answer in Table 1 tangentially discusses\nuveal membrane but fails to mention the effect of\nc-Kit on uveal membrane.\nAddressing these challenges requires models to\nrecall factual knowledge, contextual understand-\ning, and reasoning abilities. Further exploration\nand development of these capabilities in LLMs are\nnecessary to improve the reliability and trustwor-\nthiness of generation systems.\nVicuna Alpaca-L ChatGPT MedAlpaca Robin-M0\n10\n20\n30\n40\n50\n60\n70Incidence (%)\nFact Inconsistency\nQuery Inconsistency\nT angentiality\n26\n44\n18\n44 481\n12\n20\n12\n12\n6\n2\nFigure 2: The incidence of each category of problematic\nanswers in each model.\nThe Effect of Fine-Tuning on Medical Domain\nLLMs fine-tuned on medical domain texts (Han\net al., 2023; Diao et al., 2023) have demonstrated\n1830\nModel MedNLI↑ CtrlEval↑ F1↑ R-L↑Spl Sent\nPubMedQA\nVicuna-7B .4684.5919 -1.95 15.51 12.06\nVicuna-7B_L .6380 .6326 -1.74 16.95 13.47\nAlpaca-Lora-7B.0940.1002 -3.25 9.15 11.09\nAlpaca-Lora-7B_L.4640 .4475 -1.85 13.69 13.42\nChatGPT .5850.4199 -2.09 18.17 13.48\nChatGPT_L .6824.6598 -1.73 23.45 16.54\nMedAlpaca-7B.2050.2912 -3.30 9.90 11.20\nMedAlpaca-7B_L.4720 .4545 -2.38 15.41 14.45\nRobin-medical-7B.2900.2900 -6.73 3.50 3.18\nMEDIQA2019\nVicuna-7B .8400.6330 -3.06 23.94 12.81\nVicuna-7B_L .8933.6868 -2.50 24.65 13.80\nAlpaca-Lora-7B.7226.6492 -2.48 5.96 4.83\nAlpaca-Lora-7B_L.8400 .6565 -2.37 10.93 8.35\nChatGPT .7467.5741 -2.77 20.02 11.35\nChatGPT_L .8067 .7180 -2.70 21.53 11.85\nMedAlpaca-7B.6333.5329 -3.08 8.06 6.95\nMedAlpaca-7B_L.7200 .5531 -2.84 11.14 9.04\nRobin-medical-7B.7200.7414 -5.12 1.96 2.30\nMASH-QA\nVicuna-7B .8103.6403 -2.46 14.75 9.82\nVicuna-7B_L .8381.7518 -2.06 20.69 13.47\nAlpaca-Lora-7B.7226.6492 -1.66 15.01 11.71\nAlpaca-Lora-7B_L.8363 .7812 -1.84 15.23 11.95\nChatGPT .7685.6425 -2.12 23.34 15.28\nChatGPT_L .7904 .7476 -2.14 23.47 15.92\nMedAlpaca-7B.5629.4705 -2.28 13.26 11.47\nMedAlpaca-7B_L.7445 .6983 -1.96 13.47 11.77\nRobin-medical-7B.0080.6378 -4.13 4.39 5.66\nMedQuAD\nVicuna-7B .8411.6564 -2.56 19.64 11.87\nVicuna-7B_L .8503.7355 -2.47 24.04 14.73\nAlpaca-Lora-7B.8104.7580 -2.29 11.86 9.59\nAlpaca-Lora-7B_L.8443 .7723 -2.26 14.34 11.25\nChatGPT .8000.6820 -2.75 25.59 16.01\nChatGPT_L .8317 .7597 -2.57 27.19 16.08\nMedAlpaca-7B.6634.5328 -2.80 12.19 10.61\nMedAlpaca-7B_L.8343 .7777 -2.60 12.19 10.96\nRobin-medical-7B.0775.5656 -3.78 4.88 5.96\nLiveMedQA2017\nVicuna-7B .5481.5212 -2.11 26.20 14.63\nVicuna-7B_L .6731 .5967 -2.00 26.75 15.21\nAlpaca-Lora-7B.3365.2460 -1.73 12.90 9.68\nAlpaca-Lora-7B_L.5962 .5090 -1.72 13.22 9.79\nChatGPT .6442.4879 -2.15 25.47 14.70\nChatGPT_L .8462.5627 -2.09 25.93 14.74\nMedAlpaca-7B.2019.1765 -2.18 10.79 8.87\nMedAlpaca-7B_L.3750 .2682 -2.17 13.82 10.47\nRobin-medical-7B.3077.6827 -5.25 2.40 2.58\nTable 2: Automatic evaluation results for LLMs with\nour interactive self-reflection loop (_L) and baselines.\n\"Spl\" and \"Sent\" mean sample and sentence level.\nimproved performance on certain types of ques-\ntions, such as multi-choice QA, benefiting from\nthe availability of abundant training data. How-\never, their performance in GQA tasks suffers from\nissues like unrelated content, grammar issues, un-\nwarranted templates, non-existent references, and\na lack of explanatory reasoning. As in Table 2,\nRobin-medical obtains the lowest F1 and Rouge-L\nscores. For example, given the question: “Who can\nFine Fact\nInconsistency\nQuery\nInconsistency\nT angentiality0\n1\n2\n3\n4\n5Google Ngram Frequency (%)\n1e 5\n4.37e-05\n3.13e-05\n9.38e-08\n1.17e-05\nFigure 3: The Google Ngrams Frequency of each cate-\ngory of problematic answers.\nget eczema?”, Robin-medical generates: “(A) All\n(B) 10% (C) 20% (D) 30%.\\n Output: A. ” The dis-\ncrepancy between MedAplpaca and Robin-medical\nindicates that instruction learning is more suitable\nfor LLMs than non-instruction tuning in our tasks.\nDue to Robin-medical’s relatively poor generation\nperformance, we exclude it from further experi-\nments.\nThe Effect of Frequency Considering the im-\npracticality of measuring frequency according to\nan LLM’s pre-train corpus, we use Google N-\ngrams4 as a proxy of the text distribution in the\nnatural world and pre-training corpora. We ran-\ndomly select 100 samples generated by the general\nmodels. We exact the keywords or topics of the\nquestions, which usually are disease names. We\ntake average frequencies between the years 1950-\n2019 (McKenna et al., 2023) of these keywords.\nAs in Figure 3, the problematic answers have lower\nfrequencies than fine ones. This suggests that low\nfrequency may be a potential cause of hallucina-\ntions, which requires more exploration to prove.\n4 Hallucination Mitigation Method\n4.1 Methodology\nTo address the issue of hallucination, we propose\nan iterative self-reflection process that leverages\nthe capabilities of LLMs in generating and refining\nresponses. Illustrated in Figure 4, our methodology\ncomprises three loops: Factual Knowledge Acquir-\ning Loop, Knowledge-Consistent Answering Loop,\nand Question-Entailment Answering Loop.\nFactual Knowledge Acquiring Loop First, the\nmodel generates background knowledge based on\n4The website https://books.google.com/ngrams\ncharts the frequencies of any set of search strings using a\nyearly count of n-grams found in printed sources.\n1831\nGenerated AnswerGenerated Answer\n× Non-Factual\n√ Factual\nAcquire Knowledge\nPrompt: Provide background\nknowledge to answer the given\nquestion: \"What causes\nNoonan syndrome?\"\nRefine Knowledge\nPrompt: The factuality score for the\nknowledge is -1.2 (less than\nTHRESHOLD_FACTUAL)\n, which means the knowledge is not strongly\nsupported by empirical evidence. Please\nrefine the knowledge to improve its factuality.\nTry Answering\nPrompt: Refer to the\nknowledge: \"{final_knowledge}\"\nand answer the question: \"What\ncauses Noonan syndrome?\" with\none paragraph.\n√ Consistent\n× Non-Consistent\nRefine Answer\nPrompt: The consistency score for the\nknowledge is -5.1 (less than\nTHRESHOLD_CONS), which means the\nalignment and consistency between response\nand knowledge are low. Please refine the\nresponse to improve its consistency.\n√ Entailed\n× Unentailed\nMedical Question\nWhat causes\nNoonan syndrome?\nLLM\n LLMScorer\n Scorer\n(2)(1)\nGenerated Answer\nNoonan syndrome is caused by\nmutations or changes in certain\ngenes....\nNoonan syndrome is a genetic\ndisorder that is caused by a mutation\nin a specific gene called the \"papillon\ngene.\"....\nGenerated Knowledge Final Answer\nNoonan syndrome\nis caused by a\ngenetic mutation on\nchromosome 12...\n(3)\nFigure 4: The overview of interactive self-reflection method, comprising (1) Factual Knowledge Acquiring Loop\n(yellow), (2) Knowledge-Consistent Answering Loop (green), and (3) Question-Entailment Answering Loop (black).\nthe provided question. This step capitalizes on\nthe inherent ability of LLMs to synthesize context-\nrelevant information, forming the foundation for\nthe subsequent scoring and refining stages.\nThen, a factuality evaluation of the generated\nknowledge is conducted with a customized and\nreference-free scorer. The factuality scorer is de-\nsigned through in-context instruction learning with\nthe following formula:\nFs(k|D, Q) =∑m\nt=1 logP(kt|k<t, T(D, Q)) (1)\nThe knowledge to be evaluated is k =\n{k1, k2, ..., km}. D is the few-shot demon-\nstrations that are annotated examples and Q is the\ngiven question. T(·) is the prompt template includ-\ning the definition of factuality and task description:\n“Based on Question, please generate the\nfactual knowledge. To do this, please\nconsider these factors: Verifiability,\nObjectivity, and Reliability of Source.\nNote that this evaluation should be\nbased on the best available medical\nknowledge.\\n\\nQuestion:...\\nKnowledge:...”\nIn-context instruction learning is verified in the\naspect of relevance, fluency, informativeness, etc.\non text generation tasks (Fu et al., 2023).\nIf the factuality score is lower than the thresh-\nold set in the evaluation phase, we instruct\nthe model self-reflect and refine the knowledge\nwith the following prompt: “ The factuality\nscore for the knowledge is XXX (less\nthan THRESHOLD_FACTUAL), which means the\nknowledge is not strongly supported by\nempirical evidence. Please refine the\nknowledge to improve its factuality.”\nThis generate-score-refine strategy is repeated\ninteractively until the generated knowledge reaches\nthe satisfactory factuality level. This iterative pro-\ncedure fosters dynamic and iterative interaction\nbetween the system and its generated knowledge.\nAnd ensures that the model progressively refines\nthe produced background knowledge, adhering it\nto established facts.\nKnowledge-Consistent Answering Loop Once\nthe generated knowledge attains the requisite qual-\nity, the model proceeds to generate an answer\nbased on the provided question and the final\nknowledge with the template: “ Refer to the\nknowledge: \"final_knowledge\" and answer\nthe question: XXX with one paragraph.” A\nconsistency evaluation of the generated answer is\nconducted with CTRLEval (introduced in § 3.3).\nIf the generated answer’s consistency score low-\ners the threshold, the model is prompted to intro-\nspect, self-correct, and revise the answer with “The\nconsistency score for the knowledge is XXX\n(less than THRESHOLD_CONS), which means\nthe alignment and consistency between\nresponse and knowledge are low. Please\nrefine the response to improve its\nconsistency.”\nThis generate-score-refine strategy is repeated\nuntil the generated answer reaches the consistency\nlevel. This iterative procedure ensures that the\nmodel progressively refines the produced answer\naligning with the vetted background knowledge,\nthus maintaining its integrity.\n1832\nModel Query-\nInconsistent↓ Tangentiality↓ Fact-\nInconsistent↓\nVicuna-7B 0.67% 6.04% 8.69%\nVicuna-7B_L 0.00% 2.00% 7.38%\nChatGPT 0.00% 18.00% 8.06%\nChatGPT_L 0.00% 17.33% 6.33%\nTable 3: Human evaluation results on PubMedQA.\nModel MedNLI↑ CtrlEval↑ F1↑ R-L↑Spl Sent\nVicuna-7B_L (ours).6380 .6326 -1.74 16.95 13.47\nw/o refinement .4520 .5799 -1.87 16.90 13.13\nw/o aspect .4940 .6276 -1.75 16.92 13.65\nw/o num .6320 .5915 -2.23 16.92 13.33\nChatGPT_L (ours).6824.6598 -1.73 23.45 16.54\nw/o refinement .5180 .5942 -1.86 19.60 15.25\nw/o aspect .5520 .6373 -1.87 19.34 15.46\nw/o num .6708 .5989 -1.79 21.25 15.97\nTable 4: Automatic evaluation results for Ablation Study\non PubMedQA.\nQuestion-Entailment Answering Loop After\nthe above two loops, we evaluate the generated an-\nswer’s entailment via sentence-BERT embedding\nsimilariy (Reimers and Gurevych, 2019)5 to ensure\nthe entailment and answerability. If the generated\nanswer does not meet the satisfactory entailment\nlevel, the process returns to the initial stage of the\nframework, and the entire cycle is repeated, iterat-\ning through the aforementioned stages.\n4.2 Experiments\n4.2.1 Evaluation\nIn addition to the automatic metrics described in\n§ 3.3, we conduct human evaluations using Ama-\nzon Mechanical Turk6 to further assess the quality\nof generated answers. The human evaluation for\nquestion-consistency and tangentiality is conducted\nat the sample level, where we ask annotators to cat-\negorize each answer as Query-Inconsistent, Tan-\ngential, or Entailed. “Query-Inconsistent” means\nthat the answer provides information unrelated to\nthe query or is non-sensical and meaningless. “Tan-\ngential” means that the answer provides informa-\ntion related to the question but doesn’t directly\naddress the question. “Entailed” means that the\nanswer directly addresses the question. The hu-\nman evaluation for fact-consistency is conducted\nat the sentence level, where we ask annotators to\ncategorize each sentence in the answer as Fact-\nInconsistent, Fact-Consistent, or Generic. “Fact-\nInconsistent” means that the answer sentence con-\n5https://huggingface.co/\nsentence-transformers/multi-qa-MiniLM-L6-cos-v1\n6https://www.mturk.com/\ntradicts or cannot be verified by the reference con-\ntexts or websites. “Fact-Consistent” means that the\nanswer sentence is supported by the given contexts\nor websites. “Generic” means that the sentence in\nthe answer has no statement to judge. Please see\nAppendix D for details.\n4.2.2 Results\nAutomatic Evaluation Table 2 presents the auto-\nmatic evaluation results for our self-reflection loop\napproach and the baselines that directly generate an-\nswers. We observe the superior performance of our\nmethod compared to the baselines, as evidenced\nby both classic overlap metrics and hallucination\nmetrics across all five datasets.\nOur method demonstrates a remarkable increase\nin MedNLI. For example, Alpaca-Lora-7B with\nself-reflection loop gains around three times larger\nthan baseline for Sample- and Sentence-level\nMedNLI on PubMedQA. The improvement in F1\nand Rouge-L scores, which are overlap-based met-\nrics, is relatively modest sometimes. This is pri-\nmarily due to the inherent reliance of these metrics\non the accuracy of the golden answers. As such,\neven though the generated answers exhibit high\nquality, they may differ from the golden answers,\nthus impacting the performance of these metrics.\nNotably, our method showcases its effectiveness\nacross language models with varying parameters,\nincluding 7B and 175B, across all five datasets.\nThis robust performance highlights the general-\nizability and scalability of our approach, further\nvalidating its efficacy in the context of medical\nquestion-answering tasks.\nHuman Evaluation The results in Table 3 demon-\nstrate that our method successfully reduces the per-\ncentage of query inconsistency, tangentiality, and\nfact inconsistency in both Vicuna and ChatGPT,\nwhich aligns with the findings from the automatic\nevaluation. The inter-annotator agreement, mea-\nsured using Krippendorff’s alpha (Krippendorff,\n2011), indicates high agreement among the anno-\ntators, with values exceeding 0.8 for question in-\nconsistency and tangentiality, and exceeding 0.7\nfor fact consistency. Please see Appendix D for\ndetailed results.\n4.3 Discussion\n4.3.1 Ablation Study\nTo assess the individual contributions of specific\ncomponents in our method, we conduct an ablation\nanalysis. The results in Table 4 demonstrate the\n1833\nQuestion Directly Generated Answer Generated Answer with Loop Golden Answer\nDose end-tidal car-bon dioxide mea-surement correlatewith arterial car-bon dioxide in ex-tremely low birthweight infants inthe first week oflife?\nMeasurement of end-tidal carbon dioxide (EtCO2)levels in extremely low birth weight infants in thefirst week of lifemay not be a reliable indicatorof arterial carbon dioxide levels. This is be-cause the relationship between EtCO2 and arte-rial CO2is not well establishedin preterm infants,and there may be significant variations in the accu-mulation and elimination of CO2 during the firstweek of life.\nThe evidence from the knowl-edge suggests that there isa positive correlationbetweenend-tidal carbon dioxide (ETCO2)and arterial carbon dioxide (ACO2)measurements in extremely low birthweight infants in the first week of life.This means that the two measurementsare likely to be reliable indicatorsofeach other...\nYes.There is good correlation andagreementbetween end-tidal CO(2)and arterial CO(2) in ELBW infants inthe EtCO(2) range 30-50 mmHg. End-tidal CO(2) monitoring can be helpfulin trending or for screening abnormalPaCO(2) values in ELBW infants infirst week of life.\nCharacterizationof the genderdimorphism afterinjury and hem-orrhagic shock:are hormonaldifferences re-sponsible?\nDear Nadine,Thank you for your letterdiscussing the potentialrole of hormones in the gender dimorphism ob-served in response to injury and hemorrhagicshock. While early research in this area is promis-ing, much more work remains to be done to fullyunderstand the mechanisms involved.In your experiments, you observed thatPAG-based GABAAergic signalingwas impairedin response to injury and hemorrhagic shockinadult male rats, which could contribute tothe development ofdepression- and anxiety-likebehavior...\nThe gender dimorphism of injuryandhemorrhagic shockis a com-plex topic. Research has shown thatmales and females may have differentresponses to injury and hemorrhagicshockin terms of blood pressure, organfunction, and hormone levels...\nThe independent protective effect of fe-male gender on multiple organ failureand nosocomial infection rates remainssignificant in both premenopausal andpostmenopausal women when com-pared with similarly aged men. Thisis contrary to previous experimentalstudies and the known physiologicsex hormone changes that occur aftermenopause in women. These resultssuggest that factors other than sex hor-mones may be responsible for gender-based differences after injury.\nTable 5: Example answers generated directly vs. with our self-reflection loop by Vicuna. We underline and bold\nthe parts relevant to the question or aligning with the golden answer; underline and italicize the parts unrelated to\nthe question or conflicting with the golden answer.\nperformance of different variations of our approach\nin terms of automatic hallucination metrics.\nEffect of Refinement To evaluate the impact of\nrefinement, we omit the scoring and refining stages\nand only conduct the generation stage. We acquire\nbackground knowledge based on the question and\nthen answer based on the knowledge. As in Table\n4, the answers generated by the loop without re-\nfinement attain lower MedNLI and CtrlEval, which\nmeans the refinement is helpful for fewer halluci-\nnations and higher consistency.\nEffect of Aspect Description To evaluate the im-\npact of providing explicit aspect descriptions for\nimprovement, we omit mentioning the specific as-\npect that requires refinement. Instead, we instruct\nthe model to engage in self-reflection by using a\nmore general instruction: “ Please refine the\nknowledge/response.” As in Table 4, the an-\nswers generated by the loop without aspect attain\nlower MedNLI and CtrlEval, which means the as-\npect description can lead to fewer hallucinations\nand higher consistency.\nEffect of Score Number To examine the influ-\nence of exact scores in the evaluation phase, we\nomit presenting exact values. Instead, we only\ndescribe the aspect that requires improvement\nin the instructions: “ The knowledge is not\nstrongly supported by empirical evidence.\nPlease refine the knowledge to improve\nits factuality. ” and “ The alignment\nand consistency between response and\nknowledge are low. Please refine the\nresponse to improve its consistency.” As\nin Table 4, the loop without score number attains\nlower MedNLI and CtrlEval than the full implemen-\ntation, indicating that the explicit score numbers\ncontribute to better refinement.\n4.3.2 Case Study\nThe examples in Table 5 demonstrate the effective-\nness of our method in addressing fact and query\ninconsistency. In the first line, the directly gener-\nated answer inaccurately states that EtCO2 levels\nmay not be a reliable indicator of arterial CO2 lev-\nels, while ours accurately indicates the positive\ncorrelation between these measures, aligning with\nthe golden answer and fact. In the second line, the\ndirectly generated answer is in an email format and\nmentions irrelevant information: It greets Nadine\nand mentions PAG-based GABAAergic signaling\nof adult male rats and depression- and anxiety-like\nbehavior. In contrast, our responses are more rele-\nvant and directly address the given question. Please\nsee Appendix E for more examples.\n5 Conclusion and Future Work\nHallucinations in generation tasks pose significant\nchallenges to AI’s accountability and trustworthi-\nness. We investigate this problem thoroughly and\nsystematically in the context of medical GQA in\ngeneral and domain-specific LLMs. To address this\nchallenge, we propose an iterative self-reflection\nmethod by adopting a generate-score-refine strat-\negy on background knowledge and answers. Our\n1834\nmethod is empirically proven effective, generaliz-\nable, and scalable in reducing hallucinations. In\nfuture work, we will investigate underlying causes\nof hallucination, examine this phenomenon in other\ngeneration tasks and extend our method to address\nchallenges associated with these tasks.\nLimitations\nWhile our methodology shows promise in miti-\ngating hallucination, it does not entirely eliminate\nthe possibility. The model might still generate un-\ngrounded information, especially in complex or\nambiguous scenarios. Currently, our method is\nstill in its early stages and not yet ready for direct\nreal-world deployment. It should be viewed as a\ncomplementary approach alongside other methods,\nsuch as retrieval, with the potential to contribute to\nmore robust application systems in the future.\nOur study primarily focuses on English medi-\ncal queries, limiting the generalizability to other\nlanguages, domains, and modalities. Further re-\nsearch is necessary to investigate the potential\nlanguage-specific challenges, domain adaptation\nchallenges, and multimodality fusion challenges.\nBy addressing these aspects, we can adapt our pro-\nposed methodology to different contexts, resulting\nin a more comprehensive understanding and ap-\nplication of our approach across diverse linguistic,\nmulti-domain, and multimodal settings.\nWhile this paper has addressed certain issues in\nthis domain, numerous challenges remain, such as\nempowering LLMs with high-level ability.\nEthics Statement\nWe used publicly available or synthetic datasets for\nour experiments, avoiding any potential harm to\nindividuals or groups. The data used in this study\nwere carefully selected and processed to ensure pri-\nvacy and confidentiality. No personally identifiable\ninformation is used, and all data were anonymized\nprior to analysis.\nIn considering the application of our research\nfindings, we acknowledge the potential risks and\nethical considerations associated with AI-assisted\nhealthcare. The hallucination issue, in particular,\ncan have significant implications for patient care\nand clinical decision-making. Our work is guided\nby the principle of \"do no harm\", aiming to enhance\nthe reliability and safety of medical QA systems\nrather than substitute for professional medical judg-\nment.\nAcknowledgement\nThe authors would like to thank Samuel Cahyawi-\njaya for the discussion and suggestions.\nThis work has been supported by the China\nNSFC Project (No. NSFC21EG14), SAAIR\nProject (No. Z1286), and HKJCCT21EG01\n(RG192).\nReferences\nAsma Ben Abacha, Chaitanya Shivade, and Dina\nDemner-Fushman. 2019. Overview of the mediqa\n2019 shared task on textual inference, question en-\ntailment and question answering. In Proceedings of\nthe 18th BioNLP Workshop and Shared Task, pages\n370–379.\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clini-\ncal bert embeddings. In Proceedings of the 2nd Clin-\nical Natural Language Processing Workshop, pages\n72–78.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, et al. 2023. A multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arXiv\npreprint arXiv:2302.04023.\nAsma Ben Abacha, Eugene Agichtein, Yuval Pinter,\nand Dina Demner-Fushman. 2017. Overview of the\nmedical question answering task at trec 2017 liveqa.\nIn TREC 2017.\nAsma Ben Abacha and Dina Demner-Fushman. 2019. A\nquestion-entailment approach to question answering.\nBMC Bioinform., 20(1):511:1–511:23.\nAsma Ben Abacha, Chaitanya Shivade, and Dina\nDemner-Fushman. 2019. Overview of the mediqa\n2019 shared task on textual inference, question en-\ntailment and question answering. In ACL-BioNLP\n2019.\nJonathan Berant, Vivek Srikumar, Pei-Chun Chen, Abby\nVander Linden, Brittany Harding, Brad Huang, Peter\nClark, and Christopher D Manning. 2014. Modeling\nbiological processes for reading comprehension. In\nProceedings of the 2014 conference on empirical\nmethods in natural language processing (EMNLP),\npages 1499–1510.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\n1835\nCollin Burns, Haotian Ye, Dan Klein, and Jacob Stein-\nhardt. 2022. Discovering latent knowledge in lan-\nguage models without supervision. In The Eleventh\nInternational Conference on Learning Representa-\ntions.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nBhuwan Dhingra, Danish Danish, and Dheeraj Ra-\njagopal. 2018. Simple and effective semi-supervised\nquestion answering. In Proceedings of the 2018 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 2 (Short Papers), pages\n582–587.\nShizhe Diao, Rui Pan, Hanze Dong, KaShun Shum,\nJipeng Zhang, Wei Xiong, and Tong Zhang. 2023.\nLmflow: An extensible toolkit for finetuning and\ninference of large foundation models.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire. arXiv\npreprint arXiv:2302.04166.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In International confer-\nence on machine learning, pages 3929–3938. PMLR.\nTianyu Han, Lisa C Adams, Jens-Michalis Papaioan-\nnou, Paul Grundmann, Tom Oberhauser, Alexander\nLöser, Daniel Truhn, and Keno K Bressem. 2023.\nMedalpaca–an open-source collection of medical\nconversational ai models and training data. arXiv\npreprint arXiv:2304.08247.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, 55(12):1–38.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William\nCohen, and Xinghua Lu. 2019. PubMedQA: A\ndataset for biomedical research question answering.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 2567–\n2577, Hong Kong, China. Association for Computa-\ntional Linguistics.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli\nTran-Johnson, et al. 2022. Language models\n(mostly) know what they know. arXiv preprint\narXiv:2207.05221.\nZakaria Kaddari, Youssef Mellah, Jamal Berrich, Toumi\nBouchentouf, and Mohammed G. Belkasmi. 2020.\nBiomedical question answering: A survey of meth-\nods and datasets. In 2020 Fourth International Con-\nference On Intelligent Computing in Data Sciences\n(ICDS), pages 1–8.\nPei Ke, Hao Zhou, Yankai Lin, Peng Li, Jie Zhou,\nXiaoyan Zhu, and Minlie Huang. 2022. Ctrleval:\nAn unsupervised reference-free metric for evaluat-\ning controlled text generation. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n2306–2319.\nKlaus Krippendorff. 2011. Computing krippendorff’s\nalpha-reliability.\nHwanhee Lee, Seunghyun Yoon, Franck Dernoncourt,\nDoo Soon Kim, Trung Bui, Joongbo Shin, and Ky-\nomin Jung. 2021. KPQA: A metric for generative\nquestion answering using keyphrase weights. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n2105–2115, Online. Association for Computational\nLinguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nChenliang Li, Bin Bi, Ming Yan, Wei Wang, and Song-\nfang Huang. 2021a. Addressing semantic drift in gen-\nerative question answering with auxiliary extraction.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 2: Short Papers), pages\n942–947.\nJing Li, Shangping Zhong, and Kaizhi Chen. 2021b.\nMlec-qa: A chinese multi-choice biomedical ques-\ntion answering dataset. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 8862–8874.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2021.\nTruthfulqa: Measuring how models mimic human\nfalsehoods. arXiv preprint arXiv:2109.07958.\n1836\nLang Liu, Junxiang Ren, Yuejiao Wu, Ruilin Song, Zhen\nCheng, and Sibo Wang. 2022. A pre-trained language\nmodel for medical question answering based on do-\nmain adaption. In Natural Language Processing and\nChinese Computing: 11th CCF International Confer-\nence, NLPCC 2022, Guilin, China, September 24–25,\n2022, Proceedings, Part II, pages 216–227. Springer.\nPotsawee Manakul, Adian Liusie, and Mark JF Gales.\n2023. Selfcheckgpt: Zero-resource black-box hal-\nlucination detection for generative large language\nmodels. arXiv preprint arXiv:2303.08896.\nNick McKenna, Tianyi Li, Liang Cheng, Moham-\nmad Javad Hosseini, Mark Johnson, and Mark Steed-\nman. 2023. Sources of hallucination by large lan-\nguage models on inference tasks. arXiv preprint\narXiv:2305.14552.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstra-\ntions: What makes in-context learning work? arXiv\npreprint arXiv:2202.12837.\nDiego Mollá, María Elena Santiago-Martínez, Abeed\nSarker, and Cécile Paris. 2016. A corpus for research\nin text processing for evidence based medicine. Lan-\nguage Resources and Evaluation, 50:705–727.\nTimo Möller, Anthony Reina, Raghavan Jayakumar, and\nMalte Pietsch. 2020. Covid-qa: A question answer-\ning dataset for covid-19. In Proceedings of the 1st\nWorkshop on NLP for COVID-19 at ACL 2020.\nRoser Morante, Martin Krallinger, Alfonso Valencia,\nand Walter Daelemans. 2012. Machine reading of\nbiomedical texts about alzheimers disease. In CLEF\n2012 Conference and Labs of the Evaluation Forum-\nQuestion Answering For Machine Reading Evalua-\ntion (QA4MRE), Rome/Forner, J.[edit.]; ea, pages\n1–14.\nJessica Morley, Caio CV Machado, Christopher Burr,\nJosh Cowls, Indra Joshi, Mariarosaria Taddeo, and\nLuciano Floridi. 2020. The ethics of ai in health\ncare: a mapping review. Social Science & Medicine,\n260:113172.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\net al. 2021. Webgpt: Browser-assisted question-\nanswering with human feedback. arXiv preprint\narXiv:2112.09332.\nOpenAI. 2023a. Chatgpt: Optimizing language models\nfor dialogue.\nOpenAI. 2023b. Gpt-4 technical report. ArXiv,\nabs/2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikan-\nnan Sankarasubbu. 2022. Medmcqa: A large-scale\nmulti-subject multi-choice dataset for medical do-\nmain question answering. In Conference on Health,\nInference, and Learning, pages 248–260. PMLR.\nAnusri Pampari, Preethi Raghavan, Jennifer Liang, and\nJian Peng. 2018. emrqa: A large corpus for question\nanswering on electronic medical records. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2357–2368.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\net al. 2021. Kilt: a benchmark for knowledge in-\ntensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523–2544.\nLong N Phan, James T Anibal, Hieu Tran, Shaurya\nChanana, Erol Bahadroglu, Alec Peltekian, and Gré-\ngoire Altan-Bonnet. 2021. Scifive: a text-to-text\ntransformer model for biomedical literature. arXiv\npreprint arXiv:2106.03598.\nPatrik Puchert, Poonam Poonam, Christian van\nOnzenoodt, and Timo Ropinski. 2023. Llmmaps–\na visual metaphor for stratified evaluation of large\nlanguage models. arXiv preprint arXiv:2304.00457.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for squad. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers). Association for\nComputational Linguistics.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 3982–3992.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426.\nMax Savery, Asma Ben Abacha, Soumya Gayen, and\nDina Demner-Fushman. 2020. Question-driven sum-\nmarization of answers to consumer health questions.\nScientific Data, 7(1):322.\nSarvesh Soni and Kirk Roberts. 2020. Evaluation\nof dataset selection for pre-training and fine-tuning\ntransformer language models for clinical question\nanswering. In Proceedings of the Twelfth Language\nResources and Evaluation Conference, pages 5532–\n5538.\n1837\nDan Su, Xiaoguang Li, Jindi Zhang, Lifeng Shang, Xin\nJiang, Qun Liu, and Pascale Fung. 2022. Read before\ngenerate! faithful long form question answering with\nmachine reading. In Findings of the Association for\nComputational Linguistics: ACL 2022 , pages 744–\n756.\nDan Su, Mostofa Patwary, Shrimai Prabhumoye, Peng\nXu, Ryan Prenger, Mohammad Shoeybi, Pascale\nFung, Anima Anandkumar, and Bryan Catanzaro.\n2023. Context generation improves open domain\nquestion answering. In Findings of the Association\nfor Computational Linguistics: EACL 2023 , pages\n793–808, Dubrovnik, Croatia. Association for Com-\nputational Linguistics.\nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and\nDenny Zhou. 2023. Recitation-augmented language\nmodels. In The Eleventh International Conference\non Learning Representations.\nSimon Šuster and Walter Daelemans. 2018. Clicr: A\ndataset of clinical case reports for machine reading\ncomprehension. In Proceedings of NAACL-HLT ,\npages 1551–1563.\nAlex Tamkin, Miles Brundage, Jack Clark, and Deep\nGanguli. 2021. Understanding the capabilities, limi-\ntations, and societal impact of large language models.\narXiv preprint arXiv:2102.02503.\nRaphael Tang, Rodrigo Nogueira, Edwin Zhang, Nikhil\nGupta, Phuong Cam, Kyunghyun Cho, and Jimmy\nLin. 2020. Rapidly bootstrapping a question an-\nswering dataset for covid-19. arXiv preprint\narXiv:2004.11339.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nGeorge Tsatsaronis, Georgios Balikas, Prodromos\nMalakasiotis, Ioannis Partalas, Matthias Zschunke,\nMichael R Alvers, Dirk Weissenborn, Anastasia\nKrithara, Sergios Petridis, Dimitris Polychronopou-\nlos, et al. 2015. An overview of the bioasq large-scale\nbiomedical semantic indexing and question answer-\ning competition. BMC bioinformatics, 16(1):1–28.\nCunxiang Wang, Pai Liu, and Yue Zhang. 2021. Can\ngenerative pre-trained language models serve as\nknowledge bases for closed-book qa? In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 3241–3251.\nEric J. Wang. 2023. Alpaca-lora.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed H Chi, Quoc V Le, Denny Zhou,\net al. Chain-of-thought prompting elicits reasoning\nin large language models. In Advances in Neural\nInformation Processing Systems.\nJohannes Welbl, Pontus Stenetorp, and Sebastian Riedel.\n2018. Constructing Datasets for Multi-hop Reading\nComprehension Across Documents. Transactions of\nthe Association for Computational Linguistics, 6:287–\n302.\nYan Xu, Etsuko Ishii, Samuel Cahyawijaya, Zihan\nLiu, Genta Indra Winata, Andrea Madotto, Dan Su,\nand Pascale Fung. 2022. Retrieval-free knowledge-\ngrounded dialogue response generation with adapters.\nIn Proceedings of the Second DialDoc Workshop on\nDocument-grounded Dialogue and Conversational\nQuestion Answering, pages 93–107.\nZhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu,\nXipeng Qiu, and Xuanjing Huang. 2023. Do large\nlanguage models know what they don’t know? In\nFindings of the Association for Computational Lin-\nguistics: ACL 2023 , pages 8653–8665, Toronto,\nCanada. Association for Computational Linguistics.\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. 2023. Generate\nrather than retrieve: Large language models are\nstrong context generators. In The Eleventh Inter-\nnational Conference on Learning Representations.\nHao Zhou, Minlie Huang, Yong Liu, Wei Chen,\nand Xiaoyan Zhu. 2021. Earl: Informative\nknowledge-grounded conversation generation with\nentity-agnostic representation learning. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 2383–2395.\nMing Zhu, Aman Ahuja, Da-Cheng Juan, Wei Wei, and\nChandan K. Reddy. 2020. Question answering with\nlong multiple-span answers. In Findings of the Asso-\nciation for Computational Linguistics: EMNLP 2020,\npages 3840–3849, Online. Association for Computa-\ntional Linguistics.\n1838\nA Dataset\nPubMedQA (Jin et al., 2019) is a biomedical QA\ndataset consisting of 1k expert labeled, 61.2k unla-\nbeled, and 211.3k artificially generated instances.\nIt includes questions derived from research article\ntitles, abstracts as the context, long answers from\nabstract conclusions, and concise yes/no/maybe\nanswers.\nMedQuAD (Ben Abacha and Demner-Fushman,\n2019) comprises 47,457 medical QA pairs derived\nfrom 12 National Institutes of Health (NIH) web-\nsites. It spans 37 question categories and covers\nvarious medical topics including diseases, medica-\ntions, and diagnostic tests.\nMEDIQA2019 (Ben Abacha et al., 2019) chal-\nlenge includes three tasks: Natural Language In-\nference (NLI), Recognizing Question Entailment\n(RQE), and QA in the medical domain. For this par-\nticular study, we focus on Task 3’s dataset, which\ncenters around medical question answering and\nconsider the golden answers with scores 3 and 4 as\nthe correct responses.\nLiveMedQA2017 (Ben Abacha et al., 2017) con-\ntains annotated medical QA pairs, aiding the devel-\nopment of question analysis and answering systems.\nThe test questions span 26 types within five main\ncategories, each question comprising one or more\nsub-questions, a focus, and a type. All reference\nanswers, curated from reliable sources and vetted\nby medical experts, include a URL and relevant\ncomments. A minimum of one reference answer is\nprovided for each test question.\nMASH-QA (Zhu et al., 2020) is a dataset from\nthe consumer health domain designed for Multi-\nple Answer Spans Healthcare QA. It includes 34k\nQA pairs where answers may be drawn from non-\nsequential sections of a lengthy document.\nIn these datasets, except PubMedQA, answer an-\nnotation is undertaken through a manually involved\nprocess of extracting and copying from authentic\nweb content. It is imperative to note that although\nthe annotated answers are indeed pertinent and ver-\nifiable, it has not undergone refinement, thereby\nimprovements are needed in harmonizing the con-\ntextual coherence and problem linkage. For an\nexample from LiveMedQA2017, given question:\n“Do Zolmitriptan 5mg tablets contain gluten?”, the\nanswer is “Zolmitriptan tablets are available as\n2.5 mg (yellow and functionally-scored) and 5 mg\n(pink, not scored) film coated tablets for oral ad-\nministration... ”\nB Implementation Details\nVicuna is an open-source foundation, equipped\nwith an enriched dataset and a scalable, user-\nfriendly infrastructure. It is trained by fine-tuning\nLLaMA (Touvron et al., 2023) on user-shared con-\nversations derived from ShareGPT. Vicuna gener-\nates more detailed and well-structured responses\ncompared to its predecessor, Alpaca, with a quality\nequivalent to ChatGPT. We use the code and check-\npoint from the official library7. We run generation\non two GeForce RTX 2080 GPUs with the follow-\ning settings: temperature 1.0, max new tokens 512,\nothers are default. The max number of the main\nloop, knowledge loop, and response loop is 3. The\nfactuality, consistency, and entailment threshold are\n-1.0, -5.0, and 0.8, respectively. The demo num-\nber for the factuality scorer is 1 for PubMedQA; 3\nfor MedQuAD, MEDIQA2019, MASH-QA, and\nLiveMedQA2017.\nAlpaca-LoRA replicates the results of Stanford’s\nAlpaca model, employing a technique known\nas Low-Rank Adaptation (LoRA). This instruct\nmodel, comparable in quality to GPT-3.5, is ca-\npable of operating under low resource conditions.\nRemarkably, even without hyperparameter tuning,\nAlpaca-LoRA generates outputs on par with the\noriginal Stanford Alpaca model. We use the code\nand checkpoint from the official library8. We run\ngeneration on two GeForce RTX 2080 GPUs with\nthe following settings: temperature 1.0, max new\ntokens 512, others are default. The max number\nof the main loop, knowledge loop, and response\nloop is 3. The factuality, consistency, and entail-\nment threshold are -1.0, -5.0, and 0.8, respectively.\nThe demo number for the factuality scorer is 1\nfor PubMedQA, MASH-QA, MEDIQA2019, and\nMedQuAD; 2 for LiveMedQA2017.\nChatGPT is designed to interpret prompts and\nfurnish comprehensive responses. It employs\nReinforcement Learning from Human Feedback\n(RLHF), similar to InstructGPT (Ouyang et al.,\n2022), albeit with minor differences in data col-\nlection methodology. The initial model is trained\nthrough supervised fine-tuning, where human AI\ntrainers conducted dialogues, simulating both the\nuser and AI assistant roles. We use the official API\nfrom 9 to generate answers. The max number of\n7https://github.com/lm-sys/FastChat\n8https://github.com/tloen/alpaca-lora\n9https://openai.com/blog/openai-api\n1839\nthe main loop and response loop is 3. The max\nnumber of the knowledge loop is 1 for MedQuAD\nand 3 for others. The factuality, consistency, and\nentailment threshold are -1.0, -5.0, and 0.8, respec-\ntively. The demo number for the factuality scorer\nis 1 for PubMedQA, MASH-QA, MEDIQA2019,\nand MedQuAD; 3 for LiveMedQA2017.\nMedAlpaca is a large language model meticu-\nlously fine-tuned for medical dialogue and QA\napplications upon the frameworks of Alpaca and\nAlpaca-LoRA. These models have been trained\nusing an array of medical texts, including medi-\ncal flashcards, wikis, and dialogue datasets. We\nuse the code and checkpoint from the official li-\nbrary10. We run generation on two GeForce RTX\n2080 GPUs with the following settings: tempera-\nture 1.0, max new tokens 512, others are default.\nThe max number of the main loop, knowledge loop,\nand response loop is 3. The factuality, consistency,\nand entailment threshold are -1.0, -5.0, and 0.8,\nrespectively. The demo number for the factual-\nity scorer is 1 for PubMedQA, MEDIQA2019,\nand LiveMedQA2017; 2 for MASH-QA, and\nMedQuAD.\nRobin-medical is a large language model fine-\ntuned in the medical domain via LMFlow, an exten-\nsible toolkit for finetuning and inference of large\nfoundation models. We use the code and check-\npoint from the official library11. We run generation\non two GeForce RTX 2080 GPUs with the follow-\ning settings: temperature 1.0, max new tokens 512,\nothers are default.\nC Factuality Scorer\nHere are demonstrations for our factuality\nscorer: Question: What are the risk\nfactors for heart disease?\\n Knowledge:\nRisk factors for heart disease can\nbe categorized into modifiable and\nnon-modifiable. Modifiable risk factors\ninclude high blood pressure, high\ncholesterol, smoking, unhealthy diet,\nphysical inactivity, obesity, and\nexcessive alcohol use. Non-modifiable\nrisk factors include age, gender, family\nhistory, and race or ethnicity.\\n\nQuestion: How does smoking affect lung\nhealth?\\n Knowledge: Smoking damages the\n10https://github.com/kbressem/medAlpaca\n11https://github.com/OptimalScale/LMFlow\nairways and small air sacs in your lungs,\nwhich can lead to a variety of lung\ndiseases including chronic bronchitis,\nemphysema, and lung cancer. It also\ndecreases your lung capacity and makes it\nharder for your lungs to defend against\ninfections and clear out mucus.\\n\nQuestion: Is it safe to take aspirin\nevery day?\\n Knowledge: For some people,\ntaking aspirin every day can help prevent\nheart attacks or strokes. However,\ndaily aspirin isn’t appropriate for\neveryone. It can cause side effects\nlike gastrointestinal bleeding and isn’t\nrecommended for people with certain\nhealth conditions or who take certain\nmedications. Always consult with a\nhealthcare professional before starting\nany new medication regimen.\\n\nD Human Evaluation\nWe conduct the human evaluation to assess our\nmethod’s performance in GQA for the ability to\nreduce hallucination. In detail, we randomly se-\nlect 50 question-answer pairs directly generated\nby Vicuna and 50 question-answer pairs generated\nwith our proposed loop. Each sample is evaluated\nby three different annotators to rule out potential\nbias. We specify that annotators must meet the fol-\nlowing qualifications: Their Human Intelligence\nTask (HIT) approval rates are greater than or equal\nto 95%, and the numbers of HITs approved are\ngreater than or equal to 5000. The annotators are\nlocated in Australia, Canada, the United Kingdom,\nand the United States. The evaluation cost for fact\ninconsistency is 0.3 US dollars per sentence, and\nfor question inconsistency and tangentiality, it is\n0.15 US dollars per answer. Figure 5 and 6 are\nthe user interfaces (UIs) on Amazon Mechanical\nTurk for human evaluation of fact-consistency and\nquestion-consistency and tangentiality, respectively.\nThe instructions, questions, and examples for anno-\ntators are shown.\nTo identify the Fact Inconsistency in generated\nanswers, we ask annotators to refer the context\nprovided in dataset. As a supplement, we re-\ntrieve the related paragraph from WikiMedical-\nTerms Dataset12 via sentence-BERT embedding\n12https://huggingface.co/datasets/gamino/wiki_\nmedical_terms\n1840\nFigure 5: The UI for human evaluation on fact-consistency.\nFigure 6: The UI for human evaluation on question-consistency and tangentiality.\n1841\nModel Query-Inconsistent↓ Tangentiality↓ Entailment↑\nVicuna-7B 0.67% 6.04% 93.29%Vicuna-7B_L 0.00% 2.00% 98.00%ChatGPT 0.00% 18.00% 82.00%ChatGPT_L 0.00% 17.33% 82.67%\nModel Fact-Inconsistent↓ Fact-Consistent Generic\nVicuna-7B 8.69% 78.07% 13.24%Vicuna-7B_L 7.38% 76.96% 15.66%ChatGPT 8.06% 65.62% 25.32%ChatGPT_L 6.33% 77.8% 15.87%\nTable 6: Human evaluation results on PubMedQA.\nsimilariy13. This dataset contains over 6,000 med-\nical terms and their Wikipedia text. In addition,\nwe ask annotators to search for references in Wiki-\nDoc14, a medical wiki encyclopedia used by the\ninternational community of healthcare profession-\nals.\nThe results in Table 6 demonstrate that our\nmethod successfully reduces the percentage of\nquery inconsistency, tangentially, and fact inconsis-\ntency in both Vicuna and ChatGPT. Furthermore,\nwe observe that ChatGPT tends to generate more\ngeneral sentences compared to Vicuna. Our ap-\nproach can keep generic responses at a relatively\nlow level while improving fact consistency.\nE Case Study\nTable 7 presents some example answers gener-\nated by ChatGPT, ChatGPT with our self-reflection\nloop, and GPT4. While ChatGPT provides a nega-\ntive answer, both ChatGPT with our self-reflection\nloop and GPT4 offer positive answers that align\nbetter with the golden answer. ChatGPT suggests\nthat there is no evidence in its available literature\nthat shape analysis can differentiate free-floating\ninternal carotid artery thrombus from atheroscle-\nrotic plaque in CTA. In contrast, the golden answer\nstates shape signature highlights the potential to dis-\ntinguish the two. ChatGPT with our self-reflection\nloop acknowledges the role of shape analysis in dif-\nferentiation and mentions works on machine learn-\ning algorithms and radiomic features that show\npromising results in classification. GPT4 also sug-\ngests shape analysis can potentially help differenti-\nate, but its subsequent content is unrelated to shape\nanalysis itself. These results demonstrate the effec-\ntiveness of our method and the capabilities of more\npowerful LLMs.\n13https://huggingface.co/\nsentence-transformers/all-MiniLM-L6-v2\n14https://www.wikidoc.org/index.php/Main_Page\n1842\nQuestion Can shape analysis differentiate free-floating internal carotid artery thrombus from atherosclerotic\nplaque in patients evaluated with CTA for stroke or transient ischemic attack?\nGolden Answer Yes.We identified five quantitative shape descriptors of carotid FFT. This shape \"signature\" shows\npotential for supplementing conventional lesion characterization in cases of suspected FFT.\nAnswer Generated\nby ChatGPT\nAs an AI language model, I don’t have access to the most recent studies database, but based on the\nliterature available to me so far, Icouldn’t find any evidencethat shape analysis can differentiate free-\nfloating internal carotid artery thrombus from atherosclerotic plaque in patients evaluated with CTA for\nstroke or transient ischemic attack. However,other imaging modalities such as MRI and ultrasound\nmay provide more information to differentiate between the two conditions. It is essential to consult\nwith a radiologist or a vascular specialist to make a definitive diagnosis and develop the appropriate\ntreatment plan.\nAnswer Generated\nby ChatGPT_L\nAccording to recent studies, shape analysiscan play a significant rolein differentiating free-floating\ninternal carotid artery thrombus from atherosclerotic plaque in patients evaluated with CTA for stroke\nor transient ischemic attack. Various machine learning algorithms and radiomic features have been\nused to analyze the shape of the thrombi and plaques in the internal carotid artery, leading to promising\nresults in terms of accurate classification. This differentiation is crucial as the treatment approaches for\nthrombus and plaque are different.\nAnswer Generated\nby GPT4\nShape analysiscan potentially helpdifferentiate a free-floating internal carotid artery thrombus from\nan atherosclerotic plaque in patients evaluated with computed tomography angiography (CTA) for\nstroke or transient ischemic attack. However, it is essential to note that shape analysis alone might not\nbe sufficient for a definitive diagnosis, and other imaging features and clinical information should be\nconsidered.\nA free-floating thrombus is an intraluminal filling defect that is not attached to the arterial wall, while\nan atherosclerotic plaque is a focal thickening of the arterial wall due to the presence of lipid-rich and\nfibrous tissue. Here are some shape analysis and imaging features that may help differentiate between\nthem:\n1. Shape and appearance: A free-floating thrombus usually appears as a well-defined, elongated, and\ntubular filling defect within the arterial lumen. In contrast, atherosclerotic plaques tend to be more\nirregularly shaped and may have calcifications.\n2. Location: Thrombi may occur at any location within the internal carotid artery, while atherosclerotic\nplaques are more commonly found at the carotid bifurcation, the proximal internal carotid artery, and\nthe origin of the external carotid artery.\n3. Attachment: A free-floating thrombus is not attached to the arterial wall and may show a \"bird’s\nbeak\" or \"meniscus\" sign at its proximal end, suggesting that it is freely floating within the lumen.\nAtherosclerotic plaques are typically attached to the arterial wall and may cause luminal narrowing or\nstenosis.\n4. Enhancement: In some cases, a free-floating thrombus may show minimal or no enhancement after\nthe administration of intravenous contrast, while atherosclerotic plaques typically enhance to varying\ndegrees.\nIt is important to recognize that these imaging features are not always definitive, and there may\nbe overlap between the appearance of a thrombus and an atherosclerotic plaque. A comprehensive\nevaluation that includes other imaging techniques (such as magnetic resonance imaging or ultrasound)\nand clinical information (such as patient history, symptoms, and risk factors) may be necessary for an\naccurate diagnosis. Additionally, further research and development of advanced imaging techniques\nand computational methods could improve the differentiation between these two entities in the future.\nTable 7: Example answers generated by ChatGPT, ChatGPT with our self-reflection loop, and GPT4. We\nunderline and bold the concluding parts aligning with the golden answer and underline and italicize the con-\ncluding parts conflicting with the golden answer.\n1843",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6226394772529602
    },
    {
      "name": "Comprehension",
      "score": 0.5072700381278992
    },
    {
      "name": "Reflection (computer programming)",
      "score": 0.47892579436302185
    },
    {
      "name": "Generative model",
      "score": 0.4775189459323883
    },
    {
      "name": "Interactivity",
      "score": 0.46306318044662476
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.4422130584716797
    },
    {
      "name": "Generative grammar",
      "score": 0.4279308021068573
    },
    {
      "name": "Process (computing)",
      "score": 0.42315536737442017
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3539282977581024
    },
    {
      "name": "Multimedia",
      "score": 0.1556091606616974
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    }
  ]
}