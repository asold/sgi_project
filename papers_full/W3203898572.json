{
  "title": "Deep learning approach towards accurate state of charge estimation for lithium-ion batteries using self-supervised transformer model",
  "url": "https://openalex.org/W3203898572",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2062736707",
      "name": "M. A. Hannan",
      "affiliations": [
        "Universiti Tenaga Nasional"
      ]
    },
    {
      "id": "https://openalex.org/A4209155700",
      "name": "D. N. T. How",
      "affiliations": [
        "Universiti Tenaga Nasional"
      ]
    },
    {
      "id": "https://openalex.org/A2759366225",
      "name": "M.S. Hossain Lipu",
      "affiliations": [
        "National University of Malaysia"
      ]
    },
    {
      "id": "https://openalex.org/A2166714095",
      "name": "M Mansor",
      "affiliations": [
        "Universiti Tenaga Nasional"
      ]
    },
    {
      "id": "https://openalex.org/A2490061549",
      "name": "Pin Jern Ker",
      "affiliations": [
        "Tenaga Nasional Berhad (Malaysia)",
        "Universiti Tenaga Nasional"
      ]
    },
    {
      "id": "https://openalex.org/A2424225920",
      "name": "Dong Zy",
      "affiliations": [
        "UNSW Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2437575132",
      "name": "K S M Sahari",
      "affiliations": [
        "Universiti Tenaga Nasional"
      ]
    },
    {
      "id": "https://openalex.org/A2545972232",
      "name": "S. K. Tiong",
      "affiliations": [
        "Universiti Tenaga Nasional",
        "Tenaga Nasional Berhad (Malaysia)"
      ]
    },
    {
      "id": "https://openalex.org/A2764479201",
      "name": "K M Muttaqi",
      "affiliations": [
        "University of Wollongong"
      ]
    },
    {
      "id": "https://openalex.org/A401077779",
      "name": "T.m. Indra Mahlia",
      "affiliations": [
        "Tenaga Nasional Berhad (Malaysia)",
        "Universiti Tenaga Nasional"
      ]
    },
    {
      "id": "https://openalex.org/A2250820617",
      "name": "F. Blaabjerg",
      "affiliations": [
        "Aalborg University"
      ]
    },
    {
      "id": "https://openalex.org/A2062736707",
      "name": "M. A. Hannan",
      "affiliations": [
        "Universiti Tenaga Nasional"
      ]
    },
    {
      "id": "https://openalex.org/A4209155700",
      "name": "D. N. T. How",
      "affiliations": [
        "Universiti Tenaga Nasional"
      ]
    },
    {
      "id": "https://openalex.org/A2759366225",
      "name": "M.S. Hossain Lipu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2166714095",
      "name": "M Mansor",
      "affiliations": [
        "Universiti Tenaga Nasional"
      ]
    },
    {
      "id": "https://openalex.org/A2490061549",
      "name": "Pin Jern Ker",
      "affiliations": [
        "Universiti Tenaga Nasional",
        "Tenaga Nasional Berhad (Malaysia)"
      ]
    },
    {
      "id": "https://openalex.org/A2424225920",
      "name": "Dong Zy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2437575132",
      "name": "K S M Sahari",
      "affiliations": [
        "Universiti Tenaga Nasional"
      ]
    },
    {
      "id": "https://openalex.org/A2545972232",
      "name": "S. K. Tiong",
      "affiliations": [
        "Universiti Tenaga Nasional",
        "Tenaga Nasional Berhad (Malaysia)"
      ]
    },
    {
      "id": "https://openalex.org/A2764479201",
      "name": "K M Muttaqi",
      "affiliations": [
        "University of Wollongong"
      ]
    },
    {
      "id": "https://openalex.org/A401077779",
      "name": "T.m. Indra Mahlia",
      "affiliations": [
        "Universiti Tenaga Nasional",
        "Tenaga Nasional Berhad (Malaysia)"
      ]
    },
    {
      "id": "https://openalex.org/A2250820617",
      "name": "F. Blaabjerg",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2806168267",
    "https://openalex.org/W2891238692",
    "https://openalex.org/W2751296269",
    "https://openalex.org/W2800572847",
    "https://openalex.org/W2796568833",
    "https://openalex.org/W3012309884",
    "https://openalex.org/W2973538758",
    "https://openalex.org/W2955355093",
    "https://openalex.org/W2087231251",
    "https://openalex.org/W2904167372",
    "https://openalex.org/W2567853363",
    "https://openalex.org/W2513919993",
    "https://openalex.org/W2986065461",
    "https://openalex.org/W2768033248",
    "https://openalex.org/W2021208097",
    "https://openalex.org/W3084393668",
    "https://openalex.org/W2337713278",
    "https://openalex.org/W3033613562",
    "https://openalex.org/W2589366957",
    "https://openalex.org/W1969506110",
    "https://openalex.org/W2920560441",
    "https://openalex.org/W2766784949",
    "https://openalex.org/W2802743995",
    "https://openalex.org/W2804457427",
    "https://openalex.org/W3087646440",
    "https://openalex.org/W2981358639",
    "https://openalex.org/W3122696643",
    "https://openalex.org/W2991075588",
    "https://openalex.org/W3035785571",
    "https://openalex.org/W3107692354",
    "https://openalex.org/W3137292479",
    "https://openalex.org/W2776458183",
    "https://openalex.org/W2885578090",
    "https://openalex.org/W2922306207",
    "https://openalex.org/W2959526377",
    "https://openalex.org/W3119781965",
    "https://openalex.org/W2940722387",
    "https://openalex.org/W2955325445",
    "https://openalex.org/W3102174132",
    "https://openalex.org/W3134772857",
    "https://openalex.org/W3172280330",
    "https://openalex.org/W3120705961",
    "https://openalex.org/W3128592650",
    "https://openalex.org/W3082760180",
    "https://openalex.org/W3088265803",
    "https://openalex.org/W3055270720",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3135891860",
    "https://openalex.org/W2972810968",
    "https://openalex.org/W2551393996",
    "https://openalex.org/W3012958665",
    "https://openalex.org/W3093838722",
    "https://openalex.org/W2900045578",
    "https://openalex.org/W3024490097",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3092146751",
    "https://openalex.org/W3045754018",
    "https://openalex.org/W3087971241",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3109365969",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2987741655",
    "https://openalex.org/W3094771832",
    "https://openalex.org/W2518108298",
    "https://openalex.org/W2557283755",
    "https://openalex.org/W2970803838",
    "https://openalex.org/W2741695306",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W3104527631",
    "https://openalex.org/W3091985885",
    "https://openalex.org/W2968917279",
    "https://openalex.org/W3198114109",
    "https://openalex.org/W4288265053",
    "https://openalex.org/W3096148550",
    "https://openalex.org/W3092172514",
    "https://openalex.org/W3116271753",
    "https://openalex.org/W3007476963",
    "https://openalex.org/W3004227146",
    "https://openalex.org/W3083891030",
    "https://openalex.org/W2964054038",
    "https://openalex.org/W3090448620",
    "https://openalex.org/W2972002785",
    "https://openalex.org/W3188872815"
  ],
  "abstract": null,
  "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2021) 11:19541  | https://doi.org/10.1038/s41598-021-98915-8\nwww.nature.com/scientificreports\nDeep learning approach \ntowards accurate state of charge \nestimation for lithium‑ion batteries \nusing self‑supervised transformer \nmodel\nM. A. Hannan1*, D. N. T. How1*, M. S. Hossain Lipu2, M. Mansor1, Pin Jern Ker3, Z. Y. Dong4, \nK. S. M. Sahari5, S. K. Tiong3, K. M. Muttaqi6, T. M. Indra Mahlia3,7 & F. Blaabjerg8\nAccurate state of charge (SOC) estimation of lithium‑ion (Li‑ion) batteries is crucial in prolonging \ncell lifespan and ensuring its safe operation for electric vehicle applications. In this article, we \npropose the deep learning‑based transformer model trained with self ‑supervised learning (SSL) for \nend‑to‑end SOC estimation without the requirements of feature engineering or adaptive filtering. \nWe demonstrate that with the SSL framework, the proposed deep learning transformer model \nachieves the lowest root‑mean‑square‑error (RMSE) of 0.90% and a mean‑absolute‑error (MAE) of \n0.44% at constant ambient temperature, and RMSE of 1.19% and a MAE of 0.7% at varying ambient \ntemperature. With SSL, the proposed model can be trained with as few as 5 epochs using only 20% \nof the total training data and still achieves less than 1.9% RMSE on the test data. Finally, we also \ndemonstrate that the learning weights during the SSL training can be transferred to a new Li‑ion cell \nwith different chemistry and still achieve on‑par performance compared to the models trained from \nscratch on the new cell.\nThe transportation and electricity production sectors account for more than 50% of total green-house gas \n emissions1 as both rely on fossil fuels as the energy source. Promising solutions include the electrification of the \ntransportation industry and the decarbonization of electrical  grids2,3. However, the mass adoption of electric \nvehicles and renewable energy remains low due to the high adoption cost which can be attributed to the Li-ion \n batteries4. A major challenge in Li-ion batteries research is the state of charge (SOC) estimation which signifies \nthe amount of charge left in a Li-ion battery  cell5. Accurate SOC estimation allows the Li-ion battery cells to be \nused to its maximum potential before disposal, resulting in tremendous cost savings in the manufacturing and \nadoption  costs6. Nevertheless, it is a notoriously hard to quantify SOC as it cannot be practically measured by \nsensors outside laboratory environment with existing sensor  technologies7.\nTwo most common approaches used in SOC estimation are the model-based and data-driven  approaches8. \nModel-based approach leverages on an in-depth understanding of domain knowledge such as the internal chemi-\ncal reaction in the cell, electrical properties of the components used to model them and complex mathemati-\ncal equations to model the  SOC9. Prominent model-based techniques include the Sliding Mode  Observer 10, \nLuenberger  Observer11, Kalman  filters12 Electrochemical  Model13, Equivalent Circuit  Model14, Electrochemical \nImpedance  Model15. While model-based approach can result in reliable and accurate models, it requires an \nOPEN\n1Department of Electrical Power Engineering, COE, Universiti Tenaga Nasional, 43000 Kajang, \nMalaysia. 2Department of Electrical, Electronic and Systems Engineering, Universiti Kebangsaan Malaysia, \n43600 Bangi, Malaysia. 3Institute of Sustainable Energy, Universiti Tenaga Nasional, 43000 Kajang, \nMalaysia. 4School of Electrical Engineering and Telecommunications, UNSW, Kensington, NSW 2033, \nAustralia. 5Department of Mechanical Engineering, COE, Universiti Tenaga Nasional, 43000 Kajang, \nMalaysia. 6School of Electrical, Computer and Telecommunications Engineering, University of Wollongong, \nWollongong, NSW 2522, Australia. 7Present address: Centre of Green Technology, Faculty of Engineering \nand Information Technology, University of Technology Sydney, Ultimo, NSW 2007, Australia. 8Department of \nEnergy Technology, Aalborg University, 9220 Aalborg, Denmark.  *email: hannan@uniten.edu.my; dickson@\nuniten.edu.my\n2\nVol:.(1234567890)Scientific Reports |        (2021) 11:19541  | https://doi.org/10.1038/s41598-021-98915-8\nwww.nature.com/scientificreports/\nextensive domain knowledge, rigorous feature engineering, and relatively long development  time16. Apart from \nthat, model-based approach also does not scale well across battery cells different chemistry. As a result, altera -\ntions in cell chemistry requires a re-development of the  model17. Additionally, model-based approach also does \nnot account for anomalies in cells such as manufacturing inconsistencies, unpredictable operating conditions, \ncell degradation, and so  forth18. Due to these shortcomings, more researchers are shifting their attention to \nusing the data-driven approach for SOC estimation. In this approach, the SOC is directly modeled from observ-\nable signals such as voltage, current and temperature of the Li-ion battery cell sampled over diverse operating \nconditions across different cell chemistry and  manufacturers19. There are various methods in data-driven SOC \nestimation such as artificial neural  network20, support vector  machine21, extreme learning  machine22 Gaussian \nprocess  regression23, wavelet neural  network24, nonlinear autoregressive with exogenous input neural  network25, \noptimized neural  network26 and fuzzy  logic27 to name a few. One method that has been gaining traction lately \nis the use of a data-driven technique known as deep learning (DL)28. DL has great potential for SOC estimation \ndue to its powerful capability to learn any function given the right data according to the universal approxima -\ntion  theorem29. In essence, DL can be used to directly approximate the relationship between the measurable cell \nsignals (voltage, current, temperature) and the SOC with no additional processing such as using adaptive  filters30. \nThis eliminates the needs of manual feature engineering which can take a considerable amount of time and \nexpert domain knowledge and still produce accurate SOC estimation  results31. Pioneering works by  authors32,33 \nintroduce long short-term memory (LSTM) and deep neural network DNN to directly estimate SOC from cell \nvoltage, current and temperature with no additional filters. The proposed model achieved lowest MAE of 2.17% \nover a varying ambient temperature dataset. Authors  in34,35 proposed the utilization of gated recurrent unit (GRU) \nmodel to directly estimate SOC over a wide ambient temperature range with a RMSE of 3.5% under untrained \ntemperature. There are also authors who proposed deep convolutional models such as  in36,37 with approximately \nless than 3% MAE on untrained data. Nonetheless, there are works on hybridizing convolutional and recurrent \nmodels such  as35,38 with under 2% RMSE on varying ambient temperature.\nHowever, there are several research gaps with the existing DL methods for SOC estimation. These research \ngaps are the primary motivations of the proposal in this article. Firstly, all the cited works uses the supervised \nlearning (SL) scheme to train the models which is known to require massive amount of data to  accomplish39. \nEven in the scenarios where adequate data is available the training time for DL models typically takes many \nhours or days to  complete32. Secondly, the models that are trained on one cell chemistry do not apply to other \ncell chemistry. Even though preliminary work indicates that transfer learning is  possible40, further tests are still \nrequired to verify its accuracy if it applies to more cells with differing chemistry. In most cases a model that is \ntrained on one Li-ion battery cell data does not generalize well across another cell and may require re-training \nof the model from scratch. Thirdly, most DL models use the recurrent DL architecture which may prove to \nwork well with sequence data such as the SOC but are hard and slow to  train41. Recurrent models also do not \nleverage on the parallel GPU computation that could significantly improve training  time25. Lastly, even though \nrecurrent architectures such as the LSTM or GRU can handle long sequences well, they are still susceptible to \nvanishing gradient for longer  sequences42. Due to these limitations, recurrent models are generally superseded by \nanother architecture known as the Transformer in many domains such as computer  vision43 and natural language \n processing44,45. In short, the primary motivations for the proposal in this article are (i) shortcomings of the SL \ntraining framework, (ii) Inadequate validation and testing of transfer learning capabilities in DL models, (iii) \nshortcomings of the DL recurrent architectures and (iv) Emergence and success of the Transformer DL model \nin other domains.\nIn this article, we introduce a new DL architecture for SOC estimation known as the Transformer. Apart from \nthat, we propose a training framework that leverages on self-supervised learning (SSL)39,46 and make it possible \nto train the Transformer on scarce amounts of Li-ion data in a short time and achieve higher estimation accuracy \ncompared to models trained with conventional fully-supervised method. With the proposed framework, we \ndemonstrate that the learned parameters from one cell can be transferred to another by fine-tuning the model \non little data with very short training time (approximately 30 min on a GPU). This proposed framework also \nincorporates various recent DL techniques such as using Ranger optimizer with learning rate finder, time series \ndata augmentation, and Log-Cosh loss function to boost the accuracy of the Transformer. Finally, we conclude \nthe study by comparing and validating the performance of our proposed model to other recent DL architectures \nfor SOC estimation. The key contributions of this study are as follows:\n• We introduce the transformer DL architecture for end-to-end SOC estimation with no feature engineering \nor adaptive filtering.\n• We propose the SSL training framework to train the proposed architecture in a very short amount of time \nand achieves improved estimation accuracy compared to conventional training framework.\n• The proposed model’s parameters are transferable to a different cell type and requires only five training epochs \nto achieve RMSE ≤ 2.5%.\n• The SSL training framework enables the proposed model to be re-trained with as few as 20% of the total \ntraining data and still achieve lower error compared to the models that do not use the SSL framework.\n• We evaluate and validate the performance of the proposed model to other state-of-the-art DL architectures \nfor SOC estimation.\nResults\nIn the first two subsections, we highlight the estimation accuracy of the proposed model trained at room tem -\nperature and varying ambient temperatures and compare the estimation robustness against other DL models. In \nthe subsequent sections, we study the influence of pre-training on the model and show that the pre-trained model \n3\nVol.:(0123456789)Scientific Reports |        (2021) 11:19541  | https://doi.org/10.1038/s41598-021-98915-8\nwww.nature.com/scientificreports/\noutperforms the non-pre-trained models in estimation accuracy and convergence time. Next, we highlight that \nthe learned weights of the pre-trained model can be transferred to perform estimation on a different cell with \ndifferent chemistry with short re-training or fine-tuning. In all experiments, the proposed Transformer model \nwas benchmarked against other widely used DL models for SOC estimation of various architecture types. The \nhyperparameter combinations of all comparison models were chosen to be as close as possible to the original \npublication. If that is infeasible, we adopted the hyperparameter configurations that is widely accepted or used \nand has proven to work well on many occasions. All common hyperparameters such as sliding window value, \ninput output units, learning rate, batch size, training epochs were held constant. All comparison models were \ntrained with the SL training framework and only the proposed Transformer model was trained with the SSL \ntraining framework under the same battery materials and EV drive cycle dataset.\nEstimation accuracy under constant ambient temperature.  In this section, we demonstrate the \nestimation accuracy and efficacy of the proposed model based on data sampled at room temperature. The error \nmetrics of all models are tabulated in Table 1, sorted in ascending order of the test error. We found that the pro-\nposed model performance on SOC estimation accuracy of the RMSE in terms of training, validation and testing \nare 1.1087%, 0.8661%, 0.9056% and the MAE are 0.3289%, 0.4059%, and 0.4459%, respectively. It is observed \nthat the proposed model outperforms all other models on the test dataset of RMSE and MAE tabulated in \nTable 1. For comparison, a baseline Transformer model that was trained using conventional training framework \nis included. Results indicate that the baseline model only scores abysmally compared to other models except the \nDNN, suggesting that the training framework plays a pivotal role in the robust performance of the proposed \nmodel. We observe that the recurrent models (GRU and LSTM) outperformed their convolutional and hybrid \ncounterpart, which is not surprising as the recurrent models are specifically designed to handle sequence data \nwell. Both the GRU and LSTM were configured to use single hidden layer with 100 neurons. Despite the com-\npromise in estimation accuracy, convolutional models may be advantageous in training complexity compared \nto the recurrent models, as they are relatively easier to train and could better utilize GPU parallelization. The \nResnet model is based on the Residual Network  architecture47 adapted to work sequential  data48. Inception Time \nis based on the implementation  in49 which has shown superior benchmark performance in sequential data. \nResCNN consist of convolutional neural network with Residual blocks as inputs. FCN consist of only convolu-\ntion operation with no pooling operations has shown promising performance  in50. GRU-FCN and LSTM-FCN \nare the hybrid models combining recurrent and convolutional models to obtain the advantages of both. The \nestimation plot on the test dataset consisting of US06, LS92 and UDDS drive cycle is shown in Fig. 1.\nEstimation accuracy under varying ambient temperatures. In this subsection, we compare the esti-\nmation accuracy of the proposed model to other widely used DL models of various architectures. Among all DL \narchitectures compared in the study, the proposed transformer model achieved the lowest RMSE of 1.1075%, \n1.3139% and 1.1914% and MAE of 0.4441%, 0.5680% and 0.6502% on the test drive cycles outperforming even \nthe recurrent models which has been widely used for SOC estimation as shown in Table 2. We also note that the \nconvolutional models such as the  Resnet40 and the Inception  Time51 also outperformed the conventional GRU \n41 and  LSTM52 model. The baseline Transformer model that is not trained with the proposed training frame-\nwork scores poorly along with the feedforward DNN. Figure 2 and Fig. 3 illustrate the SOC estimation plots of \nthe proposed model across all test drive cycles at above and below zero ◦C ambient temperature, respectively. \nTraditionally, SOC estimation under low ambient temperature settings are extremely challenging due to the dif-\nference in the dynamics of chemical reactions in the  cell53. However, observation in the estimated SOC by the \nproposed Transformer model shows promising results indicating its robustness in estimating SOC at extreme \ncold temperatures up to − 20 °C.\nTable 1.  SOC estimation accuracy comparison on various DL models on the train, validation, and test dataset \nat constant ambient temperature.\nName Arch. type\nTrain error (%) Valid. error (%) Test error (%)\nRMSE MAE RMSE MAE RMSE MAE\nProposed Transformer 1.1087 0.3289 0.8661 0.4059 0.9056 0.4459\nGRU Recurrent 1.5028 0.4461 0.9852 0.3624 1.0686 0.4877\nLSTM Recurrent 1.6951 0.5404 1.0699 0.4370 1.1381 0.5341\nResnet Convolutional 1.5523 0.5647 1.1395 0.5924 1.3349 0.7859\nResCNN Convolutional 1.6121 0.6134 1.1879 0.6548 1.3454 0.8031\nInceptionTime Convolutional 1.8177 0.7538 1.3079 0.7259 1.4404 0.8202\nGRU-FCN Hybrid 1.7138 0.5978 1.3405 0.6939 1.4477 0.8228\nFCN Convolutional 1.8635 0.7685 1.2912 0.6732 1.5555 0.9642\nLSTM-FCN Hybrid 1.7619 0.7611 1.4123 0.8218 1.7771 1.1954\nBaseline Transformer 3.8265 2.2195 2.4050 1.2735 3.6116 2.7453\nDNN Feedforward 11.0576 8.6442 10.4213 8.5476 10.1347 8.2181\n4\nVol:.(1234567890)Scientific Reports |        (2021) 11:19541  | https://doi.org/10.1038/s41598-021-98915-8\nwww.nature.com/scientificreports/\nInfluence of pre‑training. Size of training data. In this section, we investigate the influence of unsuper-\nvised pre-training on the amount of required data to train the proposed model with a low error rate. We divided \nthe experiment into three scenarios. In the first scenario, we pre-trained and re-trained/fine-tuned the model \nwith all (100%) available training data. The second and third scenarios were with 50% and 20% of the training \ndata, respectively. In all scenarios, we noted the training time and error metrics. All training in this section was \nonly performed for only 5 epochs to illustrate the short amount of training time required to achieve low error \nrates. There were three modes of training used in this setup namely the pre-training (PT), re-training (RT), \nfine-tuning (FT), and full training (T). In PT, the model was trained on unlabeled dataset with unsupervised \nlearning. In RT, the mode was trained on a labeled dataset with supervised learning. In FT, the all the weights of \nthe model were frozen except for the last layer and trained with supervised learning. In T, the model was trained \nfrom scratch with supervised learning.\nTable 3 shows the results obtained. We observe that when we pre-trained and re-trained the model on all \navailable data (row 1), the error metric is lower compared to the model that was not pre-trained (row 3). Both \ntook approximately the same duration of training time. We observe that in the event where we pre-trained and \nfine-tuned the model (row 2), even though the model only updates the weights of the final layer, it still scores \na respectable 2.10% test RMSE with a reduction of about 10 min training time compared to the previous two \nmodes. The effect of pre-training is even more pronounced in the second scenario where we only re-trained and \nfine-tuned the models on 20% of train data. At approximately the same amount of training time, the pre-trained \nmodel (row 7) scores lower on the non-pre-trained model (row 9). In this section we show that pre-training \nhelps in reducing the test error with approximately same amount of training time especially when there is very \nlittle training data.\nTransfer learning. In this section we investigate the role of unsupervised pre-training in helping the model \nto generalize its estimation capacity across different cell chemistry. We pre-trained the model on the LG 18650 \n LiNiMnCoO2 cell and tested the model’s estimation capacity on a Panasonic 18650  LiNiCoAlO2 cell, similar to \nthe cells used in some Tesla vehicles. Table 4 shows the performance of the proposed model with no changes in \nthe model architecture.\nFigure 1.  SOC estimation at room temperature. (a) LA92 drive cycle at 25 °C. (b) UDDS drive cycle at 25 °C. \n(c) US06 drive cycle at 25 °C.\nTable 2.  Cross-comparison of the SOC estimation accuracy on the train, validation, and test dataset at varying \nambient temperatures.\nName Arch. type\nTrain error (%) Valid. error (%) Test error (%)\nRMSE MAE RMSE MAE RMSE MAE\nProposed Transformer 1.1075 0.4441 1.3139 0.5680 1.1914 0.6502\nResnet Convolutional 1.2510 0.5077 1.5736 0.8058 1.3636 0.7771\nInceptionTime Convolutional 1.2112 0.4967 1.2573 0.5985 1.3792 0.8152\nGRU Recurrent 1.4183 0.3505 1.5989 0.4901 1.3856 0.5847\nLSTM Recurrent 1.5549 0.5681 1.5121 0.5961 1.4498 0.7300\nResCNN Convolutional 1.5803 0.7712 1.8668 0.9860 1.5860 0.9048\nGRU-FCN Hybrid 1.5903 0.7695 2.0070 0.9893 1.6215 0.9269\nFCN Convolutional 1.7358 0.8879 2.1282 1.1033 1.7808 1.0810\nLSTM-FCN Hybrid 1.9962 1.0561 2.2292 1.1804 1.9248 1.1552\nBaseline Transformer 3.7466 2.4466 4.4805 3.4827 3.6958 2.8129\nDNN Feedforward 8.9848 6.8349 8.1916 6.1570 7.7789 6.1567\n5\nVol.:(0123456789)Scientific Reports |        (2021) 11:19541  | https://doi.org/10.1038/s41598-021-98915-8\nwww.nature.com/scientificreports/\nWe divided the experiment into four scenarios. In the first scenario, we pre-trained and re-trained/fine-tuned \nthe model with all (100%) available training data. The second and third scenarios were with 50% and 20% of \nthe training data, respectively. In the fourth scenario we pre-trained and re-trained the model on the same cell \ntype with all available data shown in the last two rows of Table  4. Unsurprisingly, the best performing mode is \nwhen the model pre-trained and re-trained on the Panasonic cell and the worst performing model is when the \nmodel was pre-trained and re-trained on the LG cell. However, when the model was pre-trained on the LG cell \nand re-trained on the Panasonic cell, the test error rate is almost on par with the best performing model. This \nsuggests that the pre-training helps in downstream re-training despite the difference in cell type. In the scenario \nwhen the model was trained on less data (20% of the training data), we observe that without pre-training (row \n9) the model yields high test errors. In rows 7 and 8, we observe that the error rate is reduced by pre-training \nthe model, even on a different cell. This once again is evidence that pre-training contributes to minimizing the \ntest set error regardless of the cell type. Supplementary Fig. 2 illustrates the estimation of the worst performing \nmode. Despite being trained on a different cell type, the model still can capture the trend of the ground truth \nSOC value. With pre-training on the LG cell and re-training on the Panasonic cell, model can estimate the SOC \nmore accurately as shown in Fig. 4.\nIn this section, we showed that the weights of the model that is learned during the unsupervised pre-training \nphase can be reused in re-training or fine-tuning across different cell types. This opens the possibilities of transfer \nlearning which is extremely helpful especially when data and computational resource is scarce. On a side note, \nall re-training and fine-tuning in this and the previous section was only performed for 5 epochs to showcase the \nlearning capability of the model despite a small training epoch. Re-training and fine-tuning the model for more \nepochs will likely yield better performance.\nDiscussion\nIn this work, a Transformer-based SOC estimation model in combination with the SSL framework was developed \nto address the challenges on Li-ion cell data availability, transfer learning, training speed and model accuracy. \nWe show that the proposed model can achieve the lowest RMSE and MAE on the test set at various ambient \nFigure 2.  SOC estimation at above zero ambient temperatures. (a) LA92 drive cycle at 10 °C. (b) UDDS drive \ncycle at 10 °C. (c) US06 drive cycle at 10 °C. (d) LA92 drive cycle at 25 °C. (e) UDDS drive cycle at 25 °C. (f) \nUS06 drive cycle at 25 °C. (g) LA92 drive cycle at 40 °C. (h) UDDS drive cycle at 40 °C. (i) US06 drive cycle at \n40 °C.\n6\nVol:.(1234567890)Scientific Reports |        (2021) 11:19541  | https://doi.org/10.1038/s41598-021-98915-8\nwww.nature.com/scientificreports/\ntemperature settings. The proposed technique also enables the Transformer model to be trained in a relatively \nshort amount of time. The first contribution of this work is the introduction of a novel Transformer DL archi -\ntecture that is capable of accurately estimating the SOC of a Li-ion cell under constant and varying ambient \ntemperatures. Based on the provided dataset, the model can accurately estimate the SOC up to RMSE  ≤ 1.19% \nand MAE ≤ 0.65% (at varying ambient temperatures) and RMSE ≤ 0.9% and MAE ≤ 0.44% (at constant ambient \ntemperature) with no feature engineering or any type of filtering. This also shows that the transformer can self-\nlearn the model parameters and map the voltage, current and temperature input data directly to SOC.\nThe second contribution is the self-supervised learning (SSL) training scheme to effectively train the proposed \nmodel. Even though the conventional supervised learning (SL) scheme can train the proposed model up to a \nFigure 3.  SOC estimation at below zero ambient temperatures. (a) LA92 drive cycle at 0 °C. (b) UDDS drive \ncycle at 0 °C. (c) US06 drive cycle at 0 °C. (d) LA92 drive cycle at − 10 °C. (e) UDDS drive cycle at − 10 °C. (f) \nUS06 drive cycle at − 10 °C. (g) LA92 drive cycle at − 20 °C. (h) UDDS drive cycle at − 20 °C. (i) US06 drive \ncycle at − 20 °C.\nTable 3.  The influence of pre-training on the training time, training data amount, and cross-dataset \ngeneralization. *PT Pre-training, RT Re-training, FT Fine-tuning, T Training.\nMode* Train time (mins) Train data (%)\nTrain error (%) Valid. error (%) Test error (%)\nRMSE MAE RMSE MAE RMSE MAE\nPT + RT 33.1 100 1.4378 0.7229 1.6207 0.8074 1.4257 0.8384\nPT + FT 20.8 100 2.2281 1.4314 2.5211 1.4666 2.1042 1.4907\nT 32.0 100 1.6981 0.9980 1.8448 1.1185 1.6367 1.0675\nPT + RT 15.3 50 1.4318 0.6980 1.6749 0.9305 1.6578 1.0609\nPT + FT 10.0 50 2.4540 1.6110 2.8765 1.7509 2.3893 1.7097\nT 15.3 50 2.6117 1.7230 2.5167 1.5975 2.3953 1.7047\nPT + RT 6.7 20 1.3072 0.6778 2.7138 1.3145 1.8993 1.1944\nPT + FT 5.3 20 2.1391 1.4582 3.0693 1.8374 2.7173 1.9036\nT 6.8 20 2.4656 1.5721 2.8898 1.7170 2.5900 1.7833\n7\nVol.:(0123456789)Scientific Reports |        (2021) 11:19541  | https://doi.org/10.1038/s41598-021-98915-8\nwww.nature.com/scientificreports/\nreasonably low error (RMSE ≤ 1.63% at varying ambient temperatures), this work highlights that the SSL training \nframework is advantageous in further reduction of error rate (RMSE ≤ 1.42% at varying ambient temperatures) \nat approximately the same amount of training time. This suggests that the SSL framework proposed to train the \nTransformer contributes to lowering the RMSE.\nThe third contribution of this work is to demonstrate that the weights from the encoder layers of the Trans-\nformer learned using the unsupervised pre-training phase can be readily transferred to another cell type of a \nTable 4.  Cross dataset performance. *PT Pre-training, RT Re-training, FT Fine-tuning, T Training.\nMode* PT dataset RT/FT dataset Train data (%)\nTrain error (%) Valid. error (%) Test error (%)\nRMSE MAE RMSE MAE RMSE MAE\nPT + RT LG Panasonic 100 1.3128 0.7948 1.4245 0.8266 2.3558 1.5477\nPT + FT LG Panasonic 100 1.9400 1.2514 2.4220 1.4254 3.3119 2.3490\nT – Panasonic 100 1.2744 0.8086 1.4597 0.9165 2.7054 2.0383\nPT + RT LG Panasonic 50 1.1674 0.6602 1.5817 0.9911 3.0465 2.1906\nPT + FT LG Panasonic 50 2.0930 1.3643 2.9021 1.7583 3.8216 2.6578\nT – Panasonic 50 1.4121 0.9128 1.8295 1.1583 3.1793 2.3900\nPT + RT LG Panasonic 20 1.2377 0.7689 2.6921 2.0752 3.7735 2.8997\nPT + FT LG Panasonic 20 1.9597 1.2957 3.4970 2.4140 3.8079 2.6289\nT – Panasonic 20 1.5928 1.0598 3.1262 2.0098 4.3440 2.6932\nPT + RT LG LG 100 9.4881 8.6845 9.7792 8.9473 11.3084 10.4053\nPT + RT Panasonic Panasonic 100 1.0752 0.5757 1.3006 0.7778 2.3462 1.6941\nFigure 4.  Estimation plot on the test drive cycles of the Panasonic cell with various training combinations. (a) \nHWFET cycle at 0 °C. (b) US06 cycle at 0 °C. (c) HWFET cycle at 10 °C. (d) US06 cycle at 10 °C. (e) HWFET \ncycle at 25 °C. (f) US06 cycle at 25 °C.\n8\nVol:.(1234567890)Scientific Reports |        (2021) 11:19541  | https://doi.org/10.1038/s41598-021-98915-8\nwww.nature.com/scientificreports/\ndifferent chemistry. Additionally, with only five epochs of re-training, the model can achieve RMSE ≤ 2.5% on \nthe test set. Extending the training time further likely leads to further reduction in RMSE. However, this work \nshows that even with lightweight re-training, the weights transferred from pre-training significantly contribute \nto the short training time with significantly less data. This opens the possibility of adapting the Transformer \nmodel to other types of Li-ion cell with only a fraction of the training data.\nThe fourth contribution highlights the SSL training framework that enables the proposed model to be re-\ntrained with as few as 20% of the total training data and still achieve lower error compared to the models that do \nnot use the SSL framework. Despite the reduced amount of training data, the model still generalizes well across \ndifferent cell chemistry. This further accentuates the important role of unsupervised pretraining in allowing the \nmodel to be more data efficient.\nThe fifth contribution compares and validates the performance of the model against recent state-of-the-art \nDL models on SOC estimation. It is shown that the model clearly outperforms all other models in the RMSE \nand MAE metric on the test dataset. Given the efficacy of the model in SOC estimation accuracy, transfer learn-\ning capability, data-efficiency and training speed, the proposed Transformer model and framework is evidently \nadvantageous over other DL models.\nMethods\nDataset. In this study, we utilized raw data sampled from a brand-new cylindrical 18,650 LiNiMnCoO2 cell \nby LG which was made available by the McMaster University in Hamilton, Ontario,  Canada54. The specification \nof the cell is given in Supplementary Table 1. The data was collected by subjecting the battery cell to various EV \nstandard drive cycles such as UDDS, HWFET, LA92, US06 at varying ambient temperatures ranging from − 20 \nto 40 °C. In addition, to simulate the dynamics of driving conditions, the cell was also subjected to a random \nmix of the standard drive cycles. The division of train, validation and test dataset used in this study is specified \nin Supplementary Table 3. Figure  5a illustrates a sample plot of the UDDS drive cycle from the test dataset at \n− 20 °C ambient temperature. For DL models to work well, careful consideration is put in preprocessing the raw \ndata samples. Firstly, the raw data is normalized into a range of 0 to 1 using Eq. (1).\n(1)˜x = x − min(x)\nmax (x) − min(x)\nFigure 5.  Data preprocessing pipeline from raw values to positional encoding. (a) Raw data sample plot from \nthe UDDS drive cycle at − 20 °C ambient temperature. (b) Input-label pairs constructed by sliding a fix width \nwindow across the train, validation, and test dataset. (c) Dataset converted into positional encoding to be \ningested into Transformer.\n9\nVol.:(0123456789)Scientific Reports |        (2021) 11:19541  | https://doi.org/10.1038/s41598-021-98915-8\nwww.nature.com/scientificreports/\nNext, the raw data was divided into three separate sets namely train, validation, and test set. The train set was \nused to train the model, validation set to check the generalization of the model during training and the test set \nwas only used to evaluate the model at the end of training. The division of the train, valid and test set is tabulated \nin Supplementary Table 3.\nUsing supervised learning requires the dataset to be formatted into input-label pairs. In this study the input \nwas the normalized values of voltage, current, and temperature while the label corresponds to the SOC value. \nThe input-label pairs were constructed by running a sliding window across the time axis over the train, valida -\ntion, and test set as illustrated Fig. 5. Voltage, current and temperature values that resides in the green window \ncorresponds to the input and SOC value that resides in the purple window corresponds to the label. Note that \nthe width of the window, k was kept at k = 400 timesteps.\nHaving massive amounts of data is a crucial component in training the proposed model without which the \nmodel will fail to generalize  well55. Although the raw data already consists of hundreds of thousands of timesteps, \nit is still insufficient for the proposed model to work. Hence, the original dataset was augmented in various way \nby injecting random noise (µ = 0, σ = 0.33) onto the training and validation dataset. The types of noise injected \nincludes additive and multiplicative Gaussian noise on the magnitude and random frequency noise generated \nwith the wavelet decomposition method. Figure  5b shows a sample of the original and the augmented version \nof the plot.\nTransformer model architecture. The original Transformer proposed  in56 uses the encoder-decoder \narrangement in the architecture. The model proposed in this work only adopts the encoder portion and not the \ndecoder as detailed  in57 to work better with multivariate time-series data. Figure 6 illustrates the block diagrams \nof the proposed model depending on the training stage which will be detailed more in the next section. Observe \nthat in Stage 1 and Stage 2 there are several common blocks namely, input, x positional encoding, encoder stack, \nand linear layer. The input data to the model consists of the input vector of X = [Vk, Ik, Tk] representing the cell \nvoltage, current and temperature at timestep, k. To give the input data contextual information, the input vec-\ntor is then added to the positional encoding. There are various choices of positional encodings according  to58. \nHowever, in this work we use the learned positional encoding with the sine and cosine functions as shown in \nEqs. (2) and (3).\n(2)PE (pos,2i) = sin\n( pos\n10000 2i/dmodel\n)\nFigure 6.  Framework and architecture of the proposed transformer model.\n10\nVol:.(1234567890)Scientific Reports |        (2021) 11:19541  | https://doi.org/10.1038/s41598-021-98915-8\nwww.nature.com/scientificreports/\nwhere pos and i correspond to the position and dimension, respectively. The reasoning behind using both func-\ntions has been previously detailed  in56. Once tagged with the positional encodings, the input vector is passed \nthrough a series of encoder blocks. Core to the Transformer architecture is the multi-headed self-attention \n(MHSA) module inside the encoder block. The MHSA module applies self-attention to the input sequence with \nrespect to the output sequence. As shown in the figure, the input to the multi-headed self- attention module \nare the key, K, value, V and query, Q. In the MHSA block, it attempts to map the query to a set key-value pairs \nwith respect to an output to produce the attention matrix. The operation consists of a dot product of the query, \nQ with all keys and a division by dk and applying a softmax function over the result as given in Eq. (4) where dk \nis the dimension of the keys.\nInstead of performing the operation in Eq. (4) once to produce a single matrix, the operation can be repeated \nmultiple times in parallel and the resulting matrices can be concatenated into a larger matrix as show in Eq. (5).\nIn this work we use the number of parallel attention heads, h = 16. Each head uses dmodel = 128. Also used in \nthe encoder module is the residual connection and residual dropout, which was set at dropout probability, p = 0.1.\nThe remaining model hyperparameter values are in Supplementary Table 4. Since Transformer models pro-\ncesses the entire sequence and does not account for the order of the input, a positional encoder is required to \nadd contextual information. The positional encoder generates a unique encoding for each data point in the input \nvector and can generalize to longer sequences. Shown in Fig. 5c is the learned positional encoding generated for \nthe input dataset in this study which consists of voltage, current and temperature of the Li-ion cell. The x-axis \ncorresponds to the lag window in the input dataset which is used generate the dataset.\nTraining framework. Instantiating a DL model involves various stochastic processes. To ensure the repro-\nducibility and consistency of the results obtained, all experiments were conducted using a preset seed value. \nReferring to Fig. 6, model training was divided into two distinct phases, namely the unsupervised pre-training \nand downstream fine-tuning. In the unsupervised pre-training  stage59, unlabeled vectors of input sequence, X \nwas used to train the model. Part of each input sequence values were randomly set to 0 by performing element-\nwise multiplication with a binary mask, M. The corrupted input, X˜ was generated with the X˜ = M 0 X. The \nmodel was then required to reconstruct the masked input with a modified MSE loss function, as given in Eq. (6).\nwhere ˆx is the predicted input vector values and x is the un-corrupted input vector values. Note that the loss does \nnot require the model to reconstruct the entire input sequence but only elements in the mask, M. Upon comple-\ntion of the unsupervised pre-training phase, the weights of the model save were transferred for the downstream \nfine-tuning phase. In this phase, the model was re-trained on a labeled dataset with supervised learning. The loss \nfunction, used in this phase is the hyperbolic cosine (Log-cosh) loss function as given in Eq. (7 ).\nwhere y is the ground truth and yˆ is the predicted value by the model. The RMSE [Eq. ( 8)] and MAE [Eq. (9)] \nerror metric was used to evaluate all models.\nOne of the most important hyperparameter used in training DL models is the learning rate, α  (LR)60. To \nsearch for the optimal LR range of values, we employ the use of LR finder introduced  in61. The optimal LR \nfound with the LR finder is α = 1e3 as depicted in Supplementary Fig. 2. The LR value was used in conjunction \nwith the Ranger optimizer which is a synergistic combination of Rectified Adam (RAdam) 62 and Lookahead \n optimizer63. RAdam has been shown to stabilize the training at the start and Ranger stabilizes the convergence \nin the remaining  steps64. The Ranger optimizer is configured with momentum = 0.95, weight decay = 0.01 and \nepsilon of  1e−6. This combination has been shown to achieve state-of-the-art results on many  datasets65,66. As \nthe training approaches the end, the LR is decayed to a lower value to further facilitate convergence to a global \n minimum67. The LR is decayed for each batch as follows,\n(3)PE (pos,2i+1) = cos\n( pos\n10000 2i/dmodel\n)\n(4)Attention(Q,K,V ) = softmax\n(QK T\n√dk\n)\nV\n(5)MultiHead(Q ,K ,V ) = Concat(Attention1,Attention2 ...Attentionn)W O\n(6)L\n(\nx,ˆx\n)\n= 1\n|M |\n∑\n(t,i)\n∑\n∈M\n(ˆx(t,i) − x(t,i)\n)2\n(7)L\n(\ny,ˆy\n)\n=\n∑ n\ni=1 log\n(\ncosh\n(\nˆy − y\n))\n(8)RMSE =\n√\n1\nN\n∑ N\nk=1\n(\nSOCk − SOC∗\nk\n)2\n(9)MAE = 1\nN\n∑ N\nk=1\n(⏐⏐SOCk − SOC∗\nk\n⏐⏐)\n11\nVol.:(0123456789)Scientific Reports |        (2021) 11:19541  | https://doi.org/10.1038/s41598-021-98915-8\nwww.nature.com/scientificreports/\nη is the maximum and minimum LR values, and Tcurrent is the number of epochs since the last restart. Figure \nshows the LR values throughout the training. The training hyperparameter values of the proposed Transformer \nmodel is concisely summarized in Supplementary Table 5.\nImplementation. All models studied were trained on an Ubuntu 20.04.02 LTS Linux operating system \nwith Intel Core i7-4790 K CPU at 4.00 GHz clock frequency, 32 GB of RAM and a Nvidia GeForce RTX3090 \ngraphic processing unit. All DL models were built using the open source Pytorch 1.7.1 68 framework in tandem \nwith the TSAI  library69. The implementation of the proposed Transformer model and SSL training framework \nwas divided into several steps. In Step 1, two distinct datasets from the LG LiNiMnCoO2 cell (Supplementary \nTable 1) and Panasonic LiNoCoAlO2 cell (Supplementary Table 2) were downloaded. Both dataset consist of data \nsampled from respective cells over a diverse range of temperature and drive cycles to simulate dynamic operat-\ning conditions as elaborated in Sect. 3.1. The dataset was divided into train, validation and test sets as shown in \nSupplementary Table 3. Next the data was normalized into the appropriate range (0 to 1) and pre-processed with \nsliding window of lag, k = 400 timesteps (Fig. 5b). The sliding window lag value, n is arbitrarily selected to due \nto the limits in our computing resources. Given more computational resources, k can be made larger to allow \nthe model to consider more contextual information from the past. At this point the dataset was also augmented \nby injecting additive and multiplicative Gaussian noise. Finally, the dataset is transformed into the positional \nencoding form shown in Fig. 5c. This is the format of the data that is expected by the transformer model.\nIn Step 2, the Transformer was configured using the appropriate model hyperparameters as detailed in Sup-\nplementary Table 4. Careful attention is placed on the dropout hyperparameter value as it largely influences the \ndegree of overfitting on the dataset. We find that the settings of dropout in the feedforward layer to p  = 0.2 and \ndropout in the residual layer to p = 0.1 work well in our experiments.\nIn Step 3, the model is now ready to be trained. As illustrated, the model was trained in two distinct stages in \nthe order of unsupervised pretraining and then downstream retraining. In “Estimation accuracy under constant \nambient temperature” , the model was trained on the LG dataset and was evaluated on its estimation accuracy at \nfixed and variable ambient temperature settings. In “Estimation accuracy under varying ambient temperatures.”,  \nthe model was trained on the LG dataset and tested on its estimation accuracy on the Panasonic dataset. In this \nstep, the training hyperparameter was configured as detailed in Supplementary Table 5. Careful attention was \nplaced on setting the LR in both training phase as it largely determines the performance and convergence to \na global minimal. We rely extensively on the use of a LR finder algorithm which points us to setting the LR, \nα = 1 ×  10–3 for pretraining and α = 2 ×  10–4 for retraining.\nIn Step 4 the model was evaluated on the SOC estimation accuracy in “Estimation accuracy under constant \nambient temperature” and the influence of pretraining and SSL on the performance of the proposed model in \n“Estimation accuracy under varying ambient temperatures.” . The performance of the model was quantified with \nthe RMSE and MAE performance metrics. Finally, the performance of the model is compared to various widely \nused DL models on similar performance metrics.\nData availability\nThe data that support the findings of this study are available from the corresponding author upon reasonable \nrequest.\nCode availability\nThe software code and the examined cases that validated our method are available from the corresponding author \nupon reasonable request.\nReceived: 5 July 2021; Accepted: 6 September 2021\nReferences\n 1. US EPA, O. Sources of Greenhouse Gas Emissions. https:// www. epa. gov/ ghgem issio ns/ sourc es- green house- gas- emiss ions.\n 2. Luisa, M., Silvestre, D., Favuzza, S., Sanseverino, E. R. & Zizzo, G. How decarbonization, digitalization and decentralization are \nchanging key power infrastructures. Renew. Sustain. Energy Rev. 93, 483–498 (2018).\n 3. Teoh, T., Kunze, O., Teo, C. C. & Wong, Y . D. Decarbonisation of urban freight transport using electric vehicles and opportunity \ncharging. Sustain. 10, 3258 (2018).\n 4. Berckmans, G. et al.  Cost projection of state of the art lithium-ion batteries for electric vehicles up to 2030. Energies  10, 1–20 \n(2017).\n 5. Ilott, A. J., Mohammadi, M., Schauerman, C. M., Ganter, M. J. & Jerschow, A. Rechargeable lithium-ion cell state of charge and \ndefect detection by in-situ inside-out magnetic resonance imaging. Nat. Commun. 9, 1776 (2018).\n 6. Schmuch, R., Wagner, R., Hörpel, G., Placke, T. & Winter, M. Performance and cost of materials for lithium-based rechargeable \nautomotive batteries. Nat. Energy 3, 267–278 (2018).\n 7. Hannan, M. A. et al. Toward enhanced state of charge estimation of lithium-ion batteries using optimized machine learning \ntechniques. Sci. Rep. 10, 4687 (2020).\n 8. How, D. N. T., Hannan, M. A., Lipu, M. S. H. & Ker, P . J. State of charge estimation for lithium-ion batteries using model-based \nand data-driven methods: A review. IEEE Access 7, 136116–136136 (2019).\n 9. Shrivastava, P ., Soon, T. K., Idris, M. Y . I. B. & Mekhilef, S. Overview of model-based online state-of-charge estimation using \nKalman filter family for lithium-ion batteries. Renew. Sustain. Energy Rev. 113, 109233 (2019).\n 10. Fleischer, C., Waag, W ., Bai, Z. & Sauer, D. U. Self-learning state-of-available-power prediction for lithium-ion batteries in electrical \nvehicles. in IEEE Vehicle Power and Propulsion Conference, 370–375 (2012).\n(10)ηt = ηi\nmin + 1\n2\n(\nηi\nmax − ηi\nmin\n)(\n1 + cos\n( T current\nT i\nπ\n))\n12\nVol:.(1234567890)Scientific Reports |        (2021) 11:19541  | https://doi.org/10.1038/s41598-021-98915-8\nwww.nature.com/scientificreports/\n 11. Kim, W . Y ., Lee, P . Y ., Kim, J. & Kim, K. S. A nonlinear-model-based observer for a state-of-charge estimation of a lithium-ion \nbattery in electric vehicles. Energies 12, 1–20 (2019).\n 12. Ozcan, G. et al. Online state of charge estimation for lithium-ion batteries using Gaussian process regression. in IECON Proceed-\nings, 998–1003 (2016).\n 13. Zheng, L., Zhang, L., Zhu, J., Wang, G. & Jiang, J. Co-estimation of state-of-charge, capacity and resistance for lithium-ion batteries \nbased on a high-fidelity electrochemical model. Appl. Energy  180, 424–434 (2016).\n 14. Lai, X., Wang, S., Ma, S., Xie, J. & Zheng, Y . Parameter sensitivity analysis and simplification of equivalent circuit model for the \nstate of charge of lithium-ion batteries. Electrochim. Acta 330, 135239 (2020).\n 15. Vyroubal, P . & Kazda, T. Equivalent circuit model parameters extraction for lithium ion batteries using electrochemical impedance \nspectroscopy. J. Energy Storage 15, 23–31 (2018).\n 16. Han, X., Ouyang, M., Lu, L. & Li, J. Simplification of physics-based electrochemical model for lithium ion battery on electric vehicle. \nPart II: Pseudo-two-dimensional model simplification and state of charge estimation. J. Power Sources 278, 814–825 (2015).\n 17. Lipu, M. S. H. et al. Data-driven state of charge estimation of lithium-ion batteries: Algorithms, implementation factors, limitations \nand future trends. J. Clean. Prod. 277, 124110 (2020).\n 18. Liao, L. & Köttig, F . A hybrid framework combining data-driven and model-based methods for system remaining useful life \nprediction. Appl. Soft Comput. J. 44, 191–199 (2016).\n 19. Deng, Z. et al. Data-driven state of charge estimation for lithium-ion battery packs based on Gaussian process regression. Energy \n205, 118000 (2020).\n 20. Chen, J., Ouyang, Q., Xu, C. & Su, H. Neural network-based state of charge observer design for lithium-ion batteries. IEEE Trans. \nControl Syst. Technol. 26, 313–320 (2018).\n 21. Alvarez Anton, J. C., Garcia Nieto, P . J., Blanco Viejo, C. & Vilan Vilan, J. A. Support vector machines used to estimate the battery \nstate of charge. IEEE Trans. Power Electron. 28, 5919–5926 (2013).\n 22. Lipu, M. S. H. et al. Extreme learning machine model for state of charge estimation of lithium-ion battery using gravitational \nsearch algorithm. IEEE Trans. Ind. Appl. 55, 4225–4234 (2019).\n 23. Sahinoglu, G. O. et al. Battery state-of-charge estimation based on regular/recurrent gaussian process regression. IEEE Trans. Ind. \nElectron. 65, 4311–4321 (2018).\n 24. Cui, D. et al. A novel intelligent method for the state of charge estimation of lithium-ion batteries using a discrete wavelet transform-\nbased wavelet neural network. Energies 11, 995 (2018).\n 25. Lipu, M. S. H. et al. State of charge estimation for lithium-ion battery using recurrent NARX neural network model based lighting \nsearch algorithm. IEEE Access 6, 28150–28161 (2018).\n 26. Lipu, M. S. H. et al.  State of charge estimation in lithium-ion batteries: A neural network optimization approach. Electronics  9, \n1546 (2020).\n 27. Zheng, W . et al. State of charge estimation for power lithium-ion battery using a fuzzy logic sliding mode observer. Energies  12, \n2491 (2019).\n 28. Lipu, M. S. H. et al. Intelligent algorithms and control strategies for battery management system in electric vehicles: Progress, \nchallenges and future outlook. J. Clean. Prod. 292, 126044 (2021).\n 29. How, D. N. T. et al. State-of-charge estimation of li-ion battery in electric vehicles: A deep neural network approach. IEEE Trans. \nInd. Appl. 56, 5565–5574 (2020).\n 30. Hannan, M. A. et al. SOC estimation of li-ion batteries with learning rate-optimized deep fully convolutional network. IEEE Trans. \nPower Electron. 36, 7349–7353 (2021).\n 31. Hannan, M. A. et al. State-of-charge estimation of li-ion battery using gated recurrent unit with one-cycle learning rate policy. \nIEEE Trans. Ind. Appl. 57, 2964–2971 (2021).\n 32. Chemali, E., Kollmeyer, P . J., Preindl, M., Ahmed, R. & Emadi, A. Long short-term memory networks for accurate state-of-charge \nestimation of li-ion batteries. IEEE Trans. Ind. Electron. 65, 6730–6739 (2018).\n 33. Chemali, E., Kollmeyer, P . J., Preindl, M. & Emadi, A. State-of-charge estimation of Li-ion batteries using deep neural networks: \nA machine learning approach. J. Power Sources 400, 242–255 (2018).\n 34. Y ang, F ., Li, W ., Li, C. & Miao, Q. State-of-charge estimation of lithium-ion batteries based on gated recurrent neural network. \nEnergy 175, 66–75 (2019).\n 35. Huang, Z., Y ang, F ., Xu, F ., Song, X. & Tsui, K.-L. Convolutional gated recurrent unit-recurrent neural network for state-of-charge \nestimation of lithium-ion batteries. IEEE Access 7, 93139–93149 (2019).\n 36. Zhang, Z. et al. An improved bidirectional gated recurrent unit method for accurate state-of-charge estimation. IEEE Access 9, \n11252–11263 (2021).\n 37. Xiao, B., Liu, Y . & Xiao, B. Accurate state-of-charge estimation approach for lithium-ion batteries by gated recurrent unit with \nensemble optimizer. IEEE Access 7, 54192–54202 (2019).\n 38. Song, X., Y ang, F ., Wang, D. & Tsui, K. L. Combined CNN-LSTM network for state-of-charge estimation of lithium-ion batteries. \nIEEE Access 7, 88894–88902 (2019).\n 39. Holmberg, O. G. et al. Self-supervised retinal thickness prediction enables deep learning from unlabelled data to boost classifica-\ntion of diabetic retinopathy. Nat. Mach. Intell. 2, 719–726 (2020).\n 40. Bhattacharjee, A., Verma, A., Mishra, S. & Saha, T. K. Estimating state of charge for xEV batteries using 1D convolutional neural \nnetworks and transfer learning. IEEE Trans. Veh. Technol. 70, 3123–3135 (2021).\n 41. Ren, X., Liu, S., Yu, X. & Dong, X. A method for state-of-charge estimation of lithium-ion batteries based on PSO-LSTM. Energy \n234, 121236 (2021).\n 42. Li, S. et al. State-of-charge estimation of lithium-ion batteries in the battery degradation process based on recurrent neural network. \nEnergies 14, 306 (2021).\n 43. Bazi, Y ., Bashmal, L., Al Rahhal, M. M., Al Dayil, R. & Al Ajlan, N. Vision transformers for remote sensing image classification. \nRemote Sens. 13, 1–20 (2021).\n 44. Popel, M. et al. Transforming machine translation: A deep learning system reaches news translation quality comparable to human \nprofessionals. Nat. Commun. 11, 1–15 (2020).\n 45. Tetko, I. V ., Karpov, P ., Van Deursen, R. & Godin, G. State-of-the-art augmented NLP transformer models for direct and single-\nstep retrosynthesis. Nat. Commun. 11, 1–11 (2020).\n 46. Eun, D. et al. Deep-learning-based image quality enhancement of compressed sensing magnetic resonance imaging of vessel wall: \nComparison of self-supervised and unsupervised approaches. Sci. Rep. 10, 1–17 (2020).\n 47. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. in Proceedings of the IEEE Computer Society \nConference on Computer Vision and Pattern Recognition, 770–778 (2016).\n 48. Weimann, K. & Conrad, T. O. F . Transfer learning for ECG classification. Sci. Rep. 11, 1–12 (2021).\n 49. Ismail Fawaz, H. et al.  InceptionTime: Finding AlexNet for time series classification. Data Min. Knowl. Discov.  34, 1936–1962 \n(2020).\n 50. Wang, Z., Y an, W . & Oates, T. Time series classification from scratch with deep neural networks: A strong baseline. in International \nJoint Conference on Neural Networks 1578–1585 (2017).\n 51. Jiao, M., Wang, D. & Qiu, J. A GRU-RNN based momentum optimized algorithm for SOC estimation. J. Power Sources 459, 228051 \n(2020).\n13\nVol.:(0123456789)Scientific Reports |        (2021) 11:19541  | https://doi.org/10.1038/s41598-021-98915-8\nwww.nature.com/scientificreports/\n 52. Wei, M., Y e, M., Li, J. B., Wang, Q. & Xu, X. State of charge estimation of lithium-ion batteries using LSTM and NARX neural \nnetworks. IEEE Access 8, 189236–189245 (2020).\n 53. Wu, J., Li, T., Zhang, H., Lei, Y . & Zhou, G. Research on modeling and SOC estimation of lithium iron phosphate battery at low \ntemperature. Energy Procedia 1, 556–561 (2018).\n 54. Vidal, C. et al. Robust xEV battery state-of-charge estimator design using a feedforward deep neural network. SAE Tech. Pap.  \n(2020).\n 55. Goodfellow, I., Bengio, Y . & Courville, A. Deep Learning (MIT Press, 2020).\n 56. Vaswani, A. et al. Attention is all you need. Adv. Neural Inf. Process. Syst. 1, 5999–6009 (2017).\n 57. Zerveas, G., Jayaraman, S., Patel, D., Bhamidipaty, A. & Eickhoff, C. A Transformer-based Framework for Multivariate Time Series \nRepresentation Learning (Springer, 2020).\n 58. Gehring, J., Auli, M., Grangier, D., Y arats, D. & Dauphin, Y . N. Convolutional sequence to sequence learning. Int. Conf. Mach. \nLearn. 3, 2029–2042 (2017).\n 59. Caron, M., Bojanowski, P ., Mairal, J. & Joulin, A. Unsupervised pre-training of image features on non-curated data. Proc. IEEE \nInt. Conf. Comput. Vis. 2019, 2959–2968 (2019).\n 60. Li, Z., Lyu, K. & Arora, S. Reconciling Modern Deep Learning with Traditional Optimization Analyses: The Intrinsic Learning Rate \n(Springer, 2020).\n 61. Smith, L. N. Cyclical learning rates for training neural networks. IEEE Winter Conf. Appl. Comput. Vis. 1, 464–472 (2015).\n 62. Liu, L. et al. On the Variance of the Adaptive Learning Rate and Beyond (Springer, 2019).\n 63. Zhang, M. R., Lucas, J., Hinton, G. & Ba, J. Lookahead Optimizer: k steps forward, 1 step back. Adv. Neural Inf. Process. Syst. 32, \n1–10 (2019).\n 64. Valeri, J. A. et al. Sequence-to-function deep learning frameworks for engineered riboregulators. Nat. Commun. 11, 1–14 (2020).\n 65. Zhang, P ., Y ang, L. & Li, D. EfficientNet-B4-Ranger: A novel method for greenhouse cucumber disease recognition under natural \ncomplex environment. Comput. Electron. Agric. 176, 105652 (2020).\n 66. Bao, Y . et al. Named entity recognition in aircraft design field based on deep learning. in Lecture Notes in Computer Science (includ-\ning subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) vol. 12432 LNCS 333–340 (Springer, 2020).\n 67. Loshchilov, I. & Hutter, F . SGDR: Stochastic Gradient Descent with Warm Restarts. in International Conference on Learning Rep-\nresentations 1–6 (2016).\n 68. Paszke, A. et al. PyTorch: An imperative style, high-performance deep learning library. Adv. Neural Inf. Process. Syst. 32, 1–10 \n(2019).\n 69. Torres, J. F ., Hadjout, D., Sebaa, A., Martínez-Álvarez, F . & Troncoso, A. Deep learning for time series forecasting: A survey. Big \nData 9, 3–21 (2021).\nAcknowledgements\nThis work was supported by the LRGS project Grant Number 20190101LRGS from the Ministry of Higher \nEducation, Malaysia under Universiti Tenaga Nasional and in part by the UNITEN Bold Refresh Publication \nFund 2021 under project J5100D4103 and partial support by ARC Research Hub for Integrated Energy Storage \nSolutions, UNSW .\nAuthor contributions\nM.A.H. and D.N.T.H. designed the research. D.N.T.H., M.A.H., M.S.H.L., K.P .J. and M.M. conducted the machine \nlearning optimization and modelling, performed the experiments, analyzed the data and wrote the manuscript. \nZ.Y .D., K.S.M.S., S.K.T., K.M.M, T.M.I.M., and F .B. provided study oversight and edited the manuscript. All \nauthors discussed the results and commented on the manuscript.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nSupplementary Information The online version contains supplementary material available at https:// doi. org/ \n10. 1038/ s41598- 021- 98915-8.\nCorrespondence and requests for materials should be addressed to M.A.H. or D.N.T.H.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2021",
  "topic": "Mean squared error",
  "concepts": [
    {
      "name": "Mean squared error",
      "score": 0.8292111754417419
    },
    {
      "name": "Mean absolute error",
      "score": 0.7010279297828674
    },
    {
      "name": "Computer science",
      "score": 0.6233943104743958
    },
    {
      "name": "Transformer",
      "score": 0.5962026119232178
    },
    {
      "name": "State of charge",
      "score": 0.48533597588539124
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47482332587242126
    },
    {
      "name": "Deep learning",
      "score": 0.46928369998931885
    },
    {
      "name": "Machine learning",
      "score": 0.40291672945022583
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.34499603509902954
    },
    {
      "name": "Voltage",
      "score": 0.22329223155975342
    },
    {
      "name": "Electrical engineering",
      "score": 0.1919095814228058
    },
    {
      "name": "Mathematics",
      "score": 0.15734979510307312
    },
    {
      "name": "Statistics",
      "score": 0.1495647430419922
    },
    {
      "name": "Engineering",
      "score": 0.13658663630485535
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Battery (electricity)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I79156528",
      "name": "Universiti Tenaga Nasional",
      "country": "MY"
    },
    {
      "id": "https://openalex.org/I885383172",
      "name": "National University of Malaysia",
      "country": "MY"
    },
    {
      "id": "https://openalex.org/I874769580",
      "name": "Tenaga Nasional Berhad (Malaysia)",
      "country": "MY"
    },
    {
      "id": "https://openalex.org/I31746571",
      "name": "UNSW Sydney",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I204824540",
      "name": "University of Wollongong",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I891191580",
      "name": "Aalborg University",
      "country": "DK"
    }
  ]
}