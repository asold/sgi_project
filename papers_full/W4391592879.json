{
  "title": "Research on Financial Risk Intelligent Monitoring and Early Warning Model Based on LSTM, Transformer, and Deep Learning",
  "url": "https://openalex.org/W4391592879",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2617606151",
      "name": "Yun-an Song",
      "affiliations": [
        "Zhejiang Wanli University"
      ]
    },
    {
      "id": "https://openalex.org/A2106030441",
      "name": "Huaqing Du",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2293473479",
      "name": "Tianyu Piao",
      "affiliations": [
        "Changchun University"
      ]
    },
    {
      "id": "https://openalex.org/A2105374534",
      "name": "Hongyu Shi",
      "affiliations": [
        "Henan Normal University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4214483114",
    "https://openalex.org/W3160442817",
    "https://openalex.org/W4317725821",
    "https://openalex.org/W4362467178",
    "https://openalex.org/W4313831028",
    "https://openalex.org/W4318047387",
    "https://openalex.org/W4322765971",
    "https://openalex.org/W4386066081",
    "https://openalex.org/W2922897133",
    "https://openalex.org/W3208873264",
    "https://openalex.org/W4385236774",
    "https://openalex.org/W4376863141",
    "https://openalex.org/W4378078293",
    "https://openalex.org/W4323049450",
    "https://openalex.org/W4285252749",
    "https://openalex.org/W4316928279",
    "https://openalex.org/W4280611847",
    "https://openalex.org/W4313473147",
    "https://openalex.org/W3125493574",
    "https://openalex.org/W4384655811",
    "https://openalex.org/W4283700044",
    "https://openalex.org/W7066099882",
    "https://openalex.org/W4328109496",
    "https://openalex.org/W4367693552",
    "https://openalex.org/W4200534766",
    "https://openalex.org/W4317470214",
    "https://openalex.org/W3201719811",
    "https://openalex.org/W3136156967",
    "https://openalex.org/W4296617823",
    "https://openalex.org/W4281475950",
    "https://openalex.org/W3201364331",
    "https://openalex.org/W4294167179"
  ],
  "abstract": "As global financial markets continue to evolve and change, financial risk monitoring and early warning have become increasingly important. However, the complexity and diversity of financial markets have led to the emergence of multidimensional and multimodal data. Traditional risk monitoring methods face difficulties in handling such diverse data and adapting to the monitoring and early warning needs of emerging risk types. To address these issues, this article proposes a financial risk intelligent monitoring and early warning model that integrates deep learning to better cope with uncertainty and risk in the financial market. Firstly, the authors introduce an LSTM model in the initial approach, trained on historical financial market data, to capture long-term dependencies and trends in the data, enabling effective monitoring of financial risk. They also optimize the model architecture to improve its performance and prediction accuracy. Secondly, the authors further introduce a transformer model with self-attention mechanism to better handle sequential data.",
  "full_text": "DOI: 10.4018/JOEUC.337607\nJournal of Organizational and End User Computing\nVolume 36 • Issue 1 \nThis article published as an Open Access article distributed under the terms of the Creative Commons Attribution License\n(http://creativecommons.org/licenses/by/4.0/) which permits unrestricted use, distribution, and production in any medium,\nprovided the author of the original work and original publication source are properly credited.\n*Corresponding Author\n1\nResearch on Financial Risk Intelligent \nMonitoring and Early Warning Model Based \non LSTM, Transformer, and Deep Learning\nYunan Song, Business School, Zhejiang Wanli University, China\nHuaqing Du, International Business School, Hebei International Studies University, China\nTianyu Piao, School of Economics, Changchun University, China\nHongyu Shi, Business School, Henan Normal University, China*\nABSTRACT\nAs global financial markets continue to evolve and change, financial risk monitoring and early warning \nhave become increasingly important. However, the complexity and diversity of financial markets have \nled to the emergence of multidimensional and multimodal data. Traditional risk monitoring methods \nface difficulties in handling such diverse data and adapting to the monitoring and early warning \nneeds of emerging risk types. To address these issues, this article proposes a financial risk intelligent \nmonitoring and early warning model that integrates deep learning to better cope with uncertainty and \nrisk in the financial market. Firstly, the authors introduce an LSTM model in the initial approach, \ntrained on historical financial market data, to capture long-term dependencies and trends in the data, \nenabling effective monitoring of financial risk. They also optimize the model architecture to improve \nits performance and prediction accuracy. Secondly, the authors further introduce a transformer model \nwith self-attention mechanism to better handle sequential data.\nKEyWORDS\nDeep Learning, Early Warning, Financial Risk, LSTM, Monitoring, Transformer\n1. INTRODUCTION\nAs an integral component of a country’s economy, the financial market not only reflects the \nnation’s competitiveness but also carries significant responsibilities in the context of the country’s \nsocioeconomic mission. With the rapid development of the socioeconomic landscape, the complexity \nand diversity of financial markets have been on the rise, leading to the accumulation of vast volumes \nof financial data. This has also raised higher demands for financial information, making the efficient \nextraction, analysis, and prediction of financial data a pressing challenge in both academia and industry. \nTherefore, research into intelligent monitoring and early warning models for financial risks holds \nsubstantial practical value. The financial sector generates a plethora of structured and unstructured \nJournal of Organizational and End User Computing\nVolume 36 • Issue 1\n2\ndata, including market trading data, news reports, economic indicators, company financial reports, \namong other information sources. These data not only come in massive quantities but also typically \nexhibit highly dynamic and diverse characteristics, reflecting the intricacies and uncertainties of \nfinancial markets. Traditional time series analysis methods find widespread application in the field \nof finance, including autoregressive models (AR) (Kaur, Parmar & Singh, 2023), moving average \nmodels (MA) (Xu et al., 2023), autoregressive Moving Average Models (ARMA) (Rapoo, Chanza \n& Motlhwe, 2023), and autoregressive integrated moving average models (ARIMA) (Wang et al., \n2023a). Autoregressive (AR) models are advantageous for their simplicity, intuitiveness, and ease \nof understanding and implementation. They effectively capture the local patterns and trends in data, \noffering flexibility by adjusting the order to control model complexity. However, AR models, based \non the assumption of linear relationships, may struggle to capture nonlinear dynamics and complex \nrelationships. They are sensitive to initial values, require data stationarity, and may have limited \neffectiveness when dealing with non-stationary or complex data. Moving Average (MA) models, on \nthe other hand, excel at adapting to short-term fluctuations in data. By considering the moving average \nof past observations, they reduce the impact of noise and random fluctuations, resulting in a smoother \nand more stable model. MA models are particularly effective in handling seasonal and periodic time \nseries data. However, they have limitations in modeling trends and long-term dependencies, as they \nprimarily focus on short-term average effects and may not fully capture long-term trends in time series. \nAdditionally, MA models may perform poorly with long-term memory in noise, requiring a careful \nbalance and selection based on the data’s characteristics in practical applications. Autoregressive \nMoving Average (ARMA) models combine the strengths of both AR and MA components. They \ncapture long-term dependencies and trends (via the AR part) while effectively handling short-term \nfluctuations and noise (via the MA part). ARMA model parameter estimation is relatively intuitive, \nexhibiting strong adaptability to time series data of different natures. However, ARMA models \nhave limited capabilities in modeling nonlinearity and non-stationarity, requiring prior assurance \nof data stationarity. Additionally, parameter selection for the model may demand empirical and \ndomain knowledge. Careful consideration is necessary when balancing model complexity and fitting \nperformance, especially when dealing with high-order models to avoid overfitting. Autoregressive \nIntegrated Moving Average (ARIMA) models are widely used, decomposing time series data into \ntrend, seasonality, and residual components for predicting future trends. Although traditional methods \nperform well in certain situations, they often rely on strong domain knowledge and manual feature \nengineering. Their ability to handle nonlinear and non-stationary data is limited, and they typically \ndepend on statistical models and rule-based systems, posing constraints when dealing with large-\nscale, multimodal, and high-dimensional data.\nWith the rapid advancement of deep learning technologies, particularly the emergence of models \nsuch as Long Short-Term Memory (LSTM) (Alizamir et al., 2023) and Transformer (Korthikanti et \nal., 2023), we have the opportunity to leverage these advanced methods to gain a better understanding \nof financial markets, capture non-linear relationships, handle multidimensional data, and predict \npotential risk events(Cheng, van Dongen, & van der Aalst, 2019). This paper aims to delve into deep \nlearning-based models for intelligent monitoring and early warning of financial risks. We begin by \nintroducing the LSTM model and applying it to historical data from financial markets (Gupta et al., \n2022). Through training on financial time series data, this model can capture long-term dependencies \nand trends within the data. This is crucial for effective monitoring of financial risks since risk events \nin financial markets often exhibit temporal correlations. Subsequently, we optimize the architecture \nof the LSTM model to enhance its performance and prediction accuracy. Furthermore, we introduce \nthe Transformer model to complement the LSTM model, further improving prediction accuracy. \nThe Transformer model, with its self-attention mechanism (Hong, Zhang & Xu, 2023), possesses \nexceptional capabilities in modeling multidimensional time series data and adaptive feature extraction. \nIt efficiently handles multidimensional financial data, automatically captures complex market trends, \nand long-term dependencies, thus enabling more accurate risk prediction and real-time monitoring(Liu, \nJournal of Organizational and End User Computing\nVolume 36 • Issue 1\n3\nZeng, Cheng, Duan, & Cheng, 2021). Moreover, the Transformer model can also be utilized for \nanomaly detection and portfolio optimization, aiding in the identification of exceptional events and \noptimizing investment portfolios (Al Janabi, 2022). Compared to traditional statistical and linear \nmodels, the Transformer model excels at capturing non-linear relationships and complex market \ndynamics within the data, thereby enhancing the accuracy of financial risk prediction.\nThis study integrates deep learning methods such as LSTM and Transformer to harness their \nunique strengths in the field of financial risk monitoring. We will delve into how to construct a more \ncomprehensive and efficient financial risk monitoring and early warning system by combining the \noutputs of these deep learning models. This fusion approach will leverage LSTM’s ability to model \nlong-term dependencies in time series data and Transformer’s self-attention mechanism for capturing \nglobal correlations within sequences to better address the complexity and uncertainty of financial \nmarkets. We will also consider other factors such as market sentiment indicators, macroeconomic \ndata, and interrelationships among assets to enrich the input information of the model and enhance \nits sensitivity to financial risk (Mba & Mai, 2022). To validate the performance of the proposed \nfinancial risk monitoring and early warning model, we will extensively utilize large-scale financial \ntime series datasets for experiments. The experimental design will encompass multiple steps, including \ndata preprocessing, model training, and performance evaluation, to ensure the model’s effectiveness \nin real financial environments. We will employ various evaluation metrics such as accuracy, recall, \nF1 score, among others (Megdad, Abu-Naser & Abu-Nasser, 2022), to comprehensively assess the \nmodel and conduct in-depth comparisons with various methods to demonstrate its superiority and \npracticality. Through this research, we aim to provide a more comprehensive and flexible approach to \nfinancial risk monitoring and early warning, better equipped to handle market fluctuations and risks.\nThe contributions of this paper can be summarized in the following three aspects:\n(1)  Our research introduces Long Short-Term Memory (LSTM) networks into the field of financial \nrisk monitoring. This initiative not only adds innovation to our model but also expands the \nboundaries of traditional financial risk monitoring frameworks. Through the incorporation \nof LSTM, we are better equipped to capture long-term dependencies in time series data from \nfinancial markets. This aids in more accurately predicting various potential risk events, including \nmarket volatility and credit risks, thereby enhancing risk management in financial markets and \nsafeguarding financial stability.\n(2)  We introduce the Transformer model, emphasizing its sensitivity to non-linear relationships \nand complex dynamics in financial markets. Traditional statistical and linear models have \nlimitations in capturing the non-linear characteristics of financial markets. The Transformer \nmodel, with its self-attention mechanism, is better at capturing non-linear relationships in \nthe data, thereby increasing sensitivity to the diversity of financial market data. This enables \nfinancial risk monitoring models to more comprehensively understand the dynamic changes \nin financial markets.\n(3)  We introduce deep learning techniques, combining traditional statistical and machine learning \nmethods with deep learning. This integrated approach not only captures non-linear relationships \nin financial markets more effectively but also handles multidimensional data and long-term \ndependencies in time series. As a result, it improves the accuracy of financial risk prediction \nand enhances the sensitivity of monitoring models to complex market dynamics and uncertainty. \nThis approach brings a new paradigm to financial market risk management, providing financial \ninstitutions with more reliable and comprehensive tools to address risks.\nThe organizational structure of this paper is as follows: Firstly, in the introduction, we emphasize \nthe crucial role of financial markets in the national economy. Traditional methods such as AR, MA, \nARMA, and ARIMA are noted for their limitations in handling non-linear and non-stationary data. By \nintegrating deep learning models like LSTM and Transformer, we elucidate the paper’s objective—to \nJournal of Organizational and End User Computing\nVolume 36 • Issue 1\n4\nenhance the efficiency of financial risk monitoring through deep learning models. In the literature \nreview section, we review traditional financial risk monitoring methods including VaR, cointegration \nmodels, and rule-based systems, as well as the application of deep learning technologies such as \nCNN, RNN, GRU, and GAN. Given the challenges they face in dealing with dynamic markets and \nmulti-source data, we emphasize the key models employed—LSTM and Transformer—to improve \ncomprehensiveness and accuracy. Subsequently, in the methodology section, we provide a detailed \noverview of the comprehensive model based on LSTM, Transformer, and deep learning. This includes \nmodel architecture, data preprocessing strategies, and training methods. Additionally, we introduce \nconsiderations for model design, taking into account factors such as data quality, interpretability, and \ncomputational resources. In the experimental section, we construct a financial risk monitoring and \nwarning model using high-performance computing servers and the Python programming language, \nincorporating multiple datasets such as Fama-French, CRSP, Compustat, and World Bank. Through a \nsynthesis of metrics such as accuracy, precision, recall, and F1 score, we demonstrate the outstanding \nperformance of the model, particularly when combining LSTM and Transformer, leading to significant \nimprovements in accuracy and recall. Comparative analysis also indicates the competitiveness of \nour model in terms of parameters, inference time, and training time, showcasing efficiency and \nscalability. Furthermore, the discussion section delves into a thorough analysis of experimental \nresults, highlighting the model’s advantages in risk monitoring while addressing limitations and \nchallenges such as interpretability, data quality, and resource consumption. External factors, such as \nmacroeconomic changes and policy adjustments affecting risk, are also considered to enhance the \nstudy’s comprehensiveness. Finally, the conclusion section summarizes the main contributions of the \nresearch, emphasizing the potential value of the comprehensive model in financial risk monitoring. \nFuture research directions are outlined, including improving model efficiency, practical application in \nfinancial markets, and further investigation into interpretability. This organizational structure aims to \nprovide readers with a comprehensive understanding of our research, delving into the issues, solutions, \nand future prospects while considering the multifaceted factors in the field of financial risk monitoring.\n2. RELEVANT WORK\nFinancial risk monitoring and early warning have always been core tasks in the field of finance, \nand an excellent model can provide crucial decision support for financial institutions and investors. \nTraditional models for financial risk monitoring and early warning have made significant progress \nover the past few decades, primarily focused on the development of statistical and econometric \nmodels, as well as rule-based methods. For instance, (Behera et al., 2023) introduced the Value at \nRisk (VaR) model, a risk measurement method based on statistical approaches aimed at estimating \npotential losses in a portfolio or asset. It typically uses historical data and probability distributions to \ncalculate the maximum possible loss at a certain confidence level. By leveraging historical data and \nprobability distributions, the model is capable of quantifying various risk levels, providing investors \nwith decision-making references. However, due to its sensitivity to market assumptions, this may \nresult in inaccurate estimations of real market conditions and relative difficulties in handling extreme \nevents and dynamic market changes. (Gianfreda et al., 2023) proposed a cointegration model based on \ntime series analysis methods, used to study long-term relationships among multiple related variables. \nIts core concept is that although these variables may be non-stationary, there exists a stationary \nlinear combination among them, known as cointegration, indicating their long-term association. \nThis provides researchers with a more in-depth understanding of the dynamic relationships between \nvariables, particularly with significant applications in the fields of economics and finance. However, \nthe model’s identification of cointegration relationships requires thorough testing of the data and is \nsusceptible to factors such as sample period and data quality in empirical studies. Therefore, caution \nshould be exercised in its application, taking these limitations into account to ensure the reliability of \nthe results. Traditional financial institutions often use rule-based systems, as described in (Hassan et al., \nJournal of Organizational and End User Computing\nVolume 36 • Issue 1\n5\n2023), to monitor potential risk signals. These systems rely on manually defined rules and thresholds \nto detect situations such as abnormal transactions, credit defaults, or market volatility. However, these \nsystems often struggle to cope with complex market conditions and emerging risks. Traditional time \nseries analysis methods, such as the ARIMA (AutoRegressive Integrated Moving Average) model \nintroduced in (Mgammal, Al-Matari & Alruwaili, 2023), aim to capture trends, seasonality, and \nrandomness in time series data. It combines an AutoRegressive (AR) model to describe the correlation \nbetween the current value and past values and a Moving Average (MA) model to describe the \ncorrelation between the current value and white noise errors. It also includes a differencing operation \nto transform non-stationary time series into stationary ones for better application of the AR and MA \nmodels. This model finds widespread use in modeling and forecasting time series data in fields such \nas economics and finance. However, this model has relatively high data requirements, necessitating \na certain level of stationarity and recognizability of trends. Additionally, its treatment of seasonality \nis relatively simplified. In practical applications, researchers should be mindful of the characteristics \nof the data to ensure the accuracy and effectiveness of the model. (Chen, Huang & Liang, 2023) \ndiscusses GARCH (Generalized Autoregressive Conditional Heteroskedasticity), a statistical model \nused for modeling volatility in time series data, particularly suitable for the financial domain. This \nmodel allows volatility to change over time and predicts future volatility based on past observations. \nThis makes it crucial in risk management and asset pricing within the financial domain, particularly \nin modeling and forecasting financial market volatility. However, the limitation of GARCH models \nlies in their assumption that the conditional heteroscedasticity of volatility is stationary, potentially \noverlooking some nonlinear features. Predictive performance may be relatively limited in scenarios \ninvolving extreme events and fat-tail distributions. Therefore, it is essential to carefully consider the \nmodel’s assumptions and applicability in practical applications. Although traditional methods have \nplayed a crucial role in financial risk management, they often face limitations in adapting to non-\nlinear relationships and handling large-scale data, as well as challenges in real-time monitoring and \nearly warning. With the emergence of deep learning technologies, we have the opportunity to explore \nnew approaches, leveraging the capabilities of neural networks to capture complex relationships in \ndata and enhance the accuracy and efficiency of risk prediction.\nThe application of deep learning techniques in the field of financial risk monitoring has garnered \nwidespread attention and research. These models leverage deep neural network architectures to \nautomatically extract crucial features from large-scale financial data, enabling accurate monitoring \nand early warning of potential risks.For example, (Mousapour Mamoudan et al., 2023) introduces \na financial risk monitoring model based on Convolutional Neural Networks (CNNs). This model \nautonomously learns and extracts complex features from financial market data, such as stock price \ntrends and heatmaps, by employing multi-level feature extraction and pooling operations in the \nconvolutional layers. It can capture local patterns and trends in time series data, making it highly \neffective for monitoring short-term volatility. (Ashtiani & Raahmei, 2023) presents a Recurrent \nNeural Network (RNN) model designed for handling time series data such as stock prices, exchange \nrates, and interest rates. This model harnesses the sequential modeling capability of RNNs to better \ncapture temporal and complex dynamics in financial market data, making it perform well in long-\nterm risk monitoring and prediction. However, traditional RNN models suffer from the vanishing \ngradient problem. Therefore, more advanced variants like Gated Recurrent Units (GRUs) address \nthis issue effectively, offering fewer parameters and faster training speeds. (Hu, Chang & Yan, 2023) \nintroduces an innovative financial risk monitoring model that utilizes GRUs as its core structure to \nmodel historical market data, capturing dynamic market features. With its built-in gating mechanisms, \nit efficiently handles data at different time scales and captures both long-term and short-term \ndependencies, enabling the model to better identify and predict potential risk signals. Additionally, \nthis model offers faster training speeds, making it suitable for high-frequency trading and real-\ntime decision-making scenarios. (Vuletić, Prenzel & Cucuringu, 2023) proposes a model based on \nGenerative Adversarial Networks (GANs) for synthesizing financial data and improving data quality \nJournal of Organizational and End User Computing\nVolume 36 • Issue 1\n6\nto enhance the performance of monitoring models. By generating realistic synthetic data, GANs help \nexpand the training dataset, mitigate overfitting issues, and improve model generalization. This model \ncan monitor new data in real-time and provide alerts upon detecting abnormal behavior, facilitating \ntimely risk mitigation. (Fuchs & Horvath, 2023) introduces a financial risk monitoring model based \non Wasserstein Generative Adversarial Networks (WGANs). This model uses Wasserstein distance \nto enhance data quality and training stability, employing it to measure the distance between generated \ndata and real data for anomaly detection, thereby improving the accuracy of anomaly detection. These \napplications of deep learning in financial risk monitoring demonstrate the potential of neural networks \nin capturing complex relationships and enhancing the accuracy and efficiency of risk prediction.\nWhen it comes to financial risk monitoring, despite the significant achievements of deep \nlearning methods, they still face some challenges and issues. Firstly, the dynamism and complexity \nof financial markets may impose limitations on traditional deep learning models in capturing long-\nterm dependencies and nonlinear features in time-series data. This introduces our first model: the \nLSTM model, which excels at memorizing time-series data and can better handle the temporal and \ndynamic nature of the market. By introducing this model, we can more accurately model historical \nfinancial market data and predict future risk trends. On the other hand, financial markets involve a \nlarge amount of multisource data, including market trading data, news events, social media sentiment, \nand more. Traditional deep learning models may face challenges in integrating and jointly modeling \nmultisource data. Therefore, we introduce the second model: the Transformer model, renowned for \nits self-attention mechanism’s ability to effectively handle correlation learning and feature extraction \nbetween different data sources. By introducing this model, we can more comprehensively leverage \ninformation from multisource data, enhancing the comprehensiveness and accuracy of risk monitoring. \nMost importantly, while the introduction of deep learning methods can improve the efficiency and \nautomation of risk monitoring, it still requires a large amount of training data and computational \nresources. Additionally, robustness and generalization need to be considered to ensure reliability in \ndifferent market environments. The application of these models not only contributes to enhancing \nthe stability and efficiency of financial markets but also provides better tools for investors, financial \ninstitutions, and regulatory authorities to manage financial risks. However, given the complexity \nand risks of financial markets, the research and application of these models still require continuous \nimprovement and validation. We will continue to explore how to better integrate the strengths of \ndifferent deep learning models, enhance model robustness and practicality, to better address the \nchallenges and changes in financial markets.\n3. METHOD\nThe overall flowchart of the algorithm in this article is shown in Figure 1.\n3.1 LSTM Architecture\nLong Short-Term Memory (LSTM) is a variant of Recurrent Neural Network (RNN) (Lazcano, \nHerrera & Monge, 2023) widely employed for handling sequential data, particularly in domains \nsuch as natural language processing, time series analysis, and financial forecasting. This architecture, \ndistinguished by its unique structure, excels at effectively capturing long-term dependencies and \nretaining information within sequences, rendering it a potent tool for processing time series data. Its \nrole in intelligent financial risk monitoring and early warning is also significant. To begin with, one of \nthe core advantages of this network is its outstanding ability for time series modeling. Financial market \ndata often exhibits pronounced temporal correlations and long-term dependencies, as seen in stock \nprices, exchange rates, and interest rates (Adebayo, Akadiri & Rjoub, 2022). LSTM, equipped with its \ninternal gate units and memory cells, efficiently captures these long-term dependencies, resulting in \nmore precise predictions of future market trends and risk events. Secondly, this architecture is adept \nat addressing the nonlinear characteristics prevalent in financial markets. Financial market behavior \nJournal of Organizational and End User Computing\nVolume 36 • Issue 1\n7\nis influenced by various factors, and these factors often exhibit intricate nonlinear relationships that \ntraditional linear models struggle to encapsulate. Leveraging its multi-layer neural network structure \nand activation functions, LSTM can adeptly model the nonlinear features inherent in financial data, \nthereby enhancing the comprehension and prediction of market behavior (Ali et al., 2023).Furthermore, \nLSTM excels in handling sequence data, which is ubiquitous in financial datasets, encompassing \nhistorical prices, trading volumes, financial indicators, and more. It efficiently captures the correlations \nbetween different time points, facilitating a more comprehensive analysis of market conditions. It can \nalso be employed to establish sequence-to-sequence models, such as mapping a series of historical data \nto future predictions, a technique with extensive applications in financial risk early warning systems.\nThis architecture consists of multiple units, with each unit containing three gate units: the Forget \nGate, the Input Gate, and the Output Gate. These gate units regulate the flow of information through \na Sigmoid function and a dot product operation (Liu et al., 2023).The model diagram of the LSTM \nis shown in Figure 2.\nThe Forget Gate determines which information to discard from the cell state. Its output ranges \nfrom 0 to 1, where 0 means complete forgetting, and 1 means complete retention. The output of the \nForget Gate can be represented by the following formula:\nf W h x b\nt f t t f= ⋅ +−σ( [ , ] )1  \nwhere ft  is the output of the Forget Gate, W f  and b f  are the weights and bias of the Forget Gate, \nht-1  is the previous time step’s hidden state, and xt  is the input at the current time step.\nThe Input Gate determines how much information to add from the new input to the cell state. \nIt also uses a sigmoid function to decide which values to update. The Input Gate can be represented \nby the following formula:\nFigure 1. Overall algorithm flowchart\n\nJournal of Organizational and End User Computing\nVolume 36 • Issue 1\n8\ni W h x bt i t t i= ⋅ +−σ( [ , ] )1  \nC W h x bt C t t C\n~\ntanh( [ , ] )= ⋅ +−1  \nwhere it  is the output of the Input Gate, ct  is the new candidate cell state, W i ,WC , bi , and bC  are \nthe respective weights and biases.\nThe cell state is the core of LSTM and is updated through the Forget Gate and Input Gate. The \ncell state can be represented using the following formula:\nC f C i Ct t t t= ⋅ + ⋅−1\n~\nt  \nAmong them, C t  is the new cell state, and C t-1  is the cell state of the previous time step.\nThe output gate determines how much information is output from the cell state. It can be expressed \nby the following formula:\no W h x bt o t t o= ⋅ +−σ( [ , ] )1  \nh o Ct t t= ⋅ tanh( )  \nFigure 2. Model diagram of the LSTM\n\nJournal of Organizational and End User Computing\nVolume 36 • Issue 1\n9\nAmong them, ht  is the hidden state of the current time step, ot  is the output of the output gate, \nWo  and bo  are the weights and biases of the output gate.\n3.2 Transformer Architecture\nThe Transformer architecture is a deep learning model used for sequence-to-sequence tasks (Chen \net al., 2023). Its design goal is to overcome the computational bottleneck issues present in recurrent \nneural networks (RNNs) and handle longer sequence data. This architecture introduces self-attention \nmechanisms, allowing the model to compute representations for all positions in the sequence in parallel, \nwithout the need for sequential processing like RNNs. Therefore, it is more suitable for tasks such as \nfinancial risk monitoring and forecasting. The model diagram of the Transformer is shown in Figure 3.\nOne of the core ideas of the Transformer is the self-attention mechanism, which allows the model to \nlearn dependencies between each position in a sequence and other positions. The self-attention mechanism \ncomputes a weighted sum of representations, where each position is weighted based on its relationship \nwith other positions. The self-attention mechanism can be represented using the following formula:\nAttention Q K V soft QK\nd\nV\nT\nk\n( , , ) max( )=  \nwhere Q, K, and V are representations of Query, Key, and Value, and dk  is the dimension of the \nquery and key.\nFigure 3. Model diagram of the transformer\n\nJournal of Organizational and End User Computing\nVolume 36 • Issue 1\n10\nTo enhance the model’s representational capacity, the architecture introduces multi-head self-\nattention mechanism (Wang et al., 2023b). This is a core component of the architecture that allows \nthe model to simultaneously focus on information from different positions in the input sequence to \ncapture various types of relationships and dependencies. The introduction of this mechanism enables \nthe model to better handle sequence data, including natural language text, time series, and financial \ndata, among others. The multi-head attention mechanism can be represented by the following formula:\nMultiHead Q K V Concathead head head W\nn\nO( , , ) ( , ,..., )= 1 2  \nhead Attention QW K W V Wi i\nQ\ni\nK\ni\nV= ( , , )  \nwhere W i\nQ , W i\nK , and W i\nV  are the weight matrices for each head.\nPositional encoding is typically represented as a matrix with the same dimensions as the input \ndata. Its values vary based on position to provide a unique encoding for each position. Positional \nencoding can be expressed using the following formula:\nPE pos i pos\ni d el( , ) s in( / )\n/ mod2 10000\n2\n=  \nPE pos i pos\ni d el( , ) c os( / )\n/ mod2 1 10000\n2\n+ =  \nwhere “pos” is the position, “i” is the dimension, and “ dmode l ” is the model’s dimension.\nThe architecture consists of encoders and decoders, where encoders handle input sequences, \nand decoders generate output sequences. Each encoder and decoder layer includes multi-head self-\nattention and feedforward neural network layers (Dai, 2022). This can be represented using the \nfollowing formula:\nE x M S x x F x( ) ( ( ) ) ( )= + +  \nD y enc output M S y y M E D y enc output y F y( , _ ) ( ( ) ) ( ( , _ ) ) ( )= + + + +  \nHere, x represents the input to the encoder. In this process, the encoder first captures dependencies \nwithin the input sequence using the Multi Head Self-Attention mechanism. The result is then added to \nthe input x using residual connections, and finally processed through a Feed Forward neural network \nlayer. Y represents the input to the decoder, and enc_output is the output from the encoder. In the \ndecoder, a similar process occurs. It starts with capturing self-dependencies within the input sequence \nusing Multi Head Self-Attention, followed by capturing relationships between the input sequence and \nthe decoder input using the Multi Head Encoder-Decoder Attention mechanism. Finally, the data is \nprocessed through a Feed Forward neural network layer.\n3.3 Deep Learning Model\nDeep learning models, with multi-layer neural networks at their core, have the capability to \nautomatically extract high-level features and representations from data. In the financial domain, deep \nlearning models find extensive applications in risk monitoring and warning tasks, including credit \nrisk assessment, market risk analysis, and fraud detection, among others (Wang & Han, 2021). These \nmodels typically consist of multiple neural network layers, including input layers, hidden layers, and \noutput layers. Each hidden layer contains multiple neurons, which communicate information through \nweighted connections. The model diagram of the deep learning is shown in Figure 4.\nJournal of Organizational and End User Computing\nVolume 36 • Issue 1\n11\nThe training process of a deep learning model involves multiple steps, including data preparation, \nselection of loss functions, and the use of optimization algorithms. In deep learning, proper data \npreparation is a crucial first step. This involves steps such as data cleaning, standardization, and splitting \ninto training, validation, and test sets. Ensuring the quality and diversity of data is crucial for the \nperformance of deep learning models. The loss function is used to measure the difference between the \nmodel’s predictions and the actual values. During training, the goal is to minimize the loss function. \nCommon loss functions in the financial domain include Mean Squared Error and Cross-Entropy, with \nthe specific choice depending on the nature of the task. In the forward propagation stage, activation \nfunctions perform non-linear transformations on the output of neurons. Common activation functions \ninclude ReLU, Sigmoid, and Tanh. Choosing an appropriate activation function helps the model \nbetter learn complex data representations. Optimization algorithms are used to adjust the model’s \nweights to minimize the loss function. Common optimization algorithms include Stochastic Gradient \nDescent (SGD), Adam, and RMSprop. Selecting the right optimization algorithm is crucial for the \nconvergence speed and performance of the model. During the training process, forward propagation \ncalculates the model’s output, and then backpropagation computes the gradient of the loss function. \nFinally, the model’s weights are updated using gradient information. This process iterates multiple \ntimes until the model converges to a satisfactory state.\nForward propagation is the process in deep learning models used to compute the output by \npassing input data through the layers of the network, ultimately producing the model’s predictions \nor outputs. It can be represented using the following formula:\nz w x b\ni ij j ij\nn\n= ⋅ +=∑ ( )1  \na f zi i= ( )  \nIn this equation, zi  represents the weighted input of neuron i, x j  is the input data, w ij  is the \nweight, bi  is the bias, ai  is the activation function output of neuron i, and f() is the activation function.\nBackpropagation is the process in deep learning models used to update weights in order to \nminimize the loss function. It calculates gradients and updates weights in the direction of the gradient. \nIt can be represented using the following formula:\nFigure 4. The model diagram of the deep learning\n\nJournal of Organizational and End User Computing\nVolume 36 • Issue 1\n12\n∂\n∂\n= ∂\n∂\n⋅∂\n∂\n⋅ ∂\n∂\nL\nw\nL\na\na\nz\nz\nwij i\ni\ni\ni\nij\n \nwhere L is the loss function, and ¶\n¶\nL\nw ij\n represents the gradient of weight w ij .\nWe incorporate LSTM and Transformer architectures into the model for handling financial \nrisk monitoring tasks through deep learning. LSTM, a variant of Recurrent Neural Network \n(RNN), is specifically designed for processing and learning sequential data. Its core concept \nintroduces gated units, including forget gates, input gates, and output gates, to better capture long-\nterm dependencies in time series data. The input gate determines the information to be updated, the \nforget gate decides the information to be discarded, and the output gate determines the information \nto be outputted. Through these gating mechanisms, LSTM effectively addresses issues such as \ngradient vanishing and exploding, enabling it to capture long-term dependencies in time series \nmore effectively. In financial risk monitoring, LSTM finds extensive applications in modeling \ntime series data, such as stock prices, exchange rates, and interest rates. Its characteristics of \nlong-term memory enable better capturing of the complex dynamics in financial markets, aiding \nin improving the accuracy of future risk predictions. Transformer is an architecture based on the \nself-attention mechanism initially used for natural language processing tasks. Its core idea is \nto establish connections between different positions through self-attention, allowing the model \nto simultaneously consider all positions in the input sequence. The self-attention mechanism \nenables the model to dynamically focus on different parts of the input sequence without being \nconstrained by a fixed window size. Transformers consist of encoders for extracting features from \nthe input sequence and decoders for generating output sequences. In financial risk monitoring, \nthe nonlinear relationship modeling capability of Transformer makes it suitable for handling \ncomplex, nonlinear relationships in financial markets. It can more flexibly capture features in \nmarket data, including correlations and nonlinear dynamics between different assets, providing \na more comprehensive perspective for risk monitoring.\nThe combined deep learning models incorporating LSTM and Transformer architectures \nnot only demonstrate improved predictive performance in the field of intelligent financial risk \nmonitoring and early warning but also play a crucial role in the following aspects. Firstly, \nthese models are better equipped to handle the complexity of the financial market, including \nfactors such as market volatility, asset price changes, and investor sentiment, thereby enhancing \ntheir sensitivity to market risks and aiding in the early detection of potential issues. Secondly, \ndeep learning models exhibit superior generalization capabilities, enabling them to adapt to \ndifferent types of financial data and market conditions, thus extending their applicability across \nvarious financial domains, including stock markets, bond markets, and forex markets, among \nothers (Wang, Zhao & Qiu, 2022). Additionally, these models provide financial practitioners \nwith enhanced decision support, assisting them in risk management, portfolio optimization, \nand strategic planning. Most importantly, deep learning models offer a novel approach to \nfinancial risk monitoring, better equipped to address the uncertainty and complexity of financial \nmarkets, thereby providing new insights and opportunities for future research and practice in \nthe financial field.\nThe pseudocode of the algorithm in this paper is shown in Algorithm 1.\n4. EXPERIMENT\nThe experimental flow chart of this paper is shown in Figure 5.\nJournal of Organizational and End User Computing\nVolume 36 • Issue 1\n13\n4.1 Experimental Environment\n4.1.1 Hardware Environment\nThis experiment utilized a high-performance computing server, which provided excellent computational \nand storage capabilities to support research on financial risk monitoring and warning models. The \nserver was equipped with an Intel Xeon E5-2690 v4 @ 2.60GHz CPU, a high-performance multi-\ncore processor that offered powerful computational capabilities suitable for deep learning tasks. \nWith 512GB of RAM, it ensured ample memory resources for model training and data processing, \nAlgorithm 1. LSTM: Transformer training\nFigure 5. Experiment flow chart\n\nJournal of Organizational and End User Computing\nVolume 36 • Issue 1\n14\ncontributing to improved experimental efficiency. The server was equipped with eight Nvidia Tesla \nP100 16GB GPUs, which excelled in deep learning tasks, significantly accelerating model training \nand inference processes. These GPUs provided researchers with robust data processing capabilities, \nallowing the model to converge faster and make more accurate predictions in the realm of financial risk.\n4.1.2 Software Environment\nIn this research, we chose Python as the primary programming language and PyTorch as the deep \nlearning framework to explore efficient approaches to financial risk monitoring and warning models. \nLeveraging the powerful capabilities of deep learning, our aim was to enhance the performance \nand efficiency of financial risk monitoring and warning tasks. Taking full advantage of Python’s \nconvenience and flexibility, we swiftly constructed intelligent risk control models based on deep \nlearning. PyTorch, as our preferred deep learning framework, provided us with a rich set of tools and \nalgorithm libraries, greatly simplifying the process of model development and training. By utilizing \nPyTorch’s dynamic computation graph mechanism and built-in automatic differentiation functionality, \nwe were able to more easily build, optimize, and fine-tune models to achieve more precise financial \nrisk monitoring and warning results.\n4.2 Experimental Data\n4.2.1 Fama-French Three-Factor Dataset\nThis dataset is an essential resource for researching financial markets and asset pricing. The dataset \nis named after economists Eugene F. Fama and Kenneth R. French, who proposed the famous three-\nfactor model to explain the volatility of stock returns. The dataset originates from historical data of the \nU.S. stock market. It contains a vast amount of financial market indicators and stock data, spanning \nmultiple years, and even decades. The dataset includes monthly, quarterly, or yearly data, typically \nencompassing stock returns, market capitalization, price-to-book ratios, and more. These data allow \nresearchers to analyze the performance of stocks and portfolios under different time periods and market \nconditions. This data can be used to validate risk models, explore the behavior of financial markets, \nassess the risk and return of portfolios, and develop intelligent risk monitoring and warning systems.\n4.2.2 CRSP Dataset\nThis dataset is maintained and provided by the Center for Research in Security Prices (CRSP) at the \nUniversity of Chicago. It originates from the U.S. stock market, covering multiple exchanges, including \nthe New York Stock Exchange (NYSE), NASDAQ, and various types of financial assets. The dataset \ncontains multidimensional financial market data, including stock opening prices, closing prices, \nhigh prices, low prices, and other price-related information. It also records daily trading volumes, \ndividend payments, and stock splits, which help analyze trading activity and liquidity and make timely \nadjustments to stock prices and returns. Additionally, it includes historical data for various market \nindices, such as the S&P 500 index, which is used to study overall market performance. This dataset \nfinds wide applications in fields such as finance, asset pricing, portfolio management, and market \nbehavior research. It can be used for tasks such as stock price analysis, portfolio construction, and \nthe development of risk models.\n4.2.3 Compustat Dataset\nThe Compustat dataset is a global financial and accounting data resource provided by S&P Global \nMarket Intelligence. It includes rich financial and accounting information for both public and private \ncompanies from different countries and industries. This dataset comprises financial statements, \naccounting metrics, company information, and stock market data, covering various aspects of a \ncompany’s financial condition, operational performance, and market value. The Compustat dataset \nfinds extensive applications in fields such as finance, corporate analysis, investment decision-\nJournal of Organizational and End User Computing\nVolume 36 • Issue 1\n15\nmaking, supporting researchers and investment professionals in tasks such as company valuation, \nrisk assessment, and portfolio construction.\n4.2.4 World Bank Dataset\nThe World Bank Dataset is a comprehensive resource managed and maintained by the World Bank, \nwhich extensively collects and provides macroeconomic, social, environmental, and development \ndata from countries and regions worldwide. This dataset includes multidimensional information such \nas a country’s Gross Domestic Product (GDP), population statistics, education levels, poverty rates, \nenvironmental indicators, and offers a range of specialized thematic data in areas like infrastructure, \nagriculture, urban development, and more. It aims to support research, policymaking, and international \ndevelopment efforts. Researchers and policymakers can use the World Bank dataset to analyze global \ndevelopment trends, conduct socio-economic research, assess policies, and formulate international \ncooperation projects, thereby enhancing their ability to predict and assess financial risks effectively.\n4.3 Evaluation Metrics\nMultiple evaluation metrics are used in the study to comprehensively assess the performance of \nfinancial risk intelligence monitoring and warning models, ensuring that the models achieve the \nexpected performance levels across various aspects. These metrics include accuracy, precision, recall, \nand F1 score. By taking into account the results of these metrics in combination, it is possible to \nassess and compare the performance of different models more comprehensively, thereby providing \na more reliable basis for financial risk assessment.\n4.3.1 Accuracy\nAccuracy is used to evaluate the performance of a model in classification tasks. It measures the \nproportion of correctly classified samples by the model out of the total number of samples, typically \nexpressed as a percentage. Specifically, accuracy represents the model’s ability to correctly classify \nsamples into their respective categories. The formula for calculating accuracy is as follows:\nAccuracy TP TN\nTP FP FN TN\n= +\n+ + +\n \nIn this context, TP represents the number of risk events correctly identified as risk events, TN \nrepresents the number of normal cases correctly identified as normal cases, FP represents the number \nof normal cases incorrectly identified as risk events, and FN represents the number of risk events \nincorrectly identified as normal cases.\n4.3.2 Precision\nPrecision focuses on the accuracy of a model’s predictions for the positive class (e.g., risk events). \nPrecision represents the proportion of samples correctly predicted as the positive class out of all \nsamples predicted as the positive class by the model. Its formula for calculation is as follows:\nPr ecision TP\nTP FP\n=\n+\n \nPrecision’s higher values indicate greater accuracy in the model’s predictions for the positive \nclass. A high precision implies that the model rarely misclassifies negative cases as positive, making \nit crucial in applications like financial risk monitoring to avoid incorrect risk alerts.\nJournal of Organizational and End User Computing\nVolume 36 • Issue 1\n16\n4.3.3 Recall\nRecall measures the model’s ability to successfully identify the positive class (such as risk events). \nIt represents the proportion of samples that the model correctly predicts as the positive class out of \nall the actual positive class samples. The formula for calculating recall is as follows:\nRe call TP\nTP FN\n=\n+\n \nwhere FN represents the number of samples that the model incorrectly predicts as the negative class, \ni.e., the number of risk events incorrectly identified as normal cases.\nHigher values of recall indicate better performance by the model in capturing actual positive class \nsamples. A high recall means that the model can effectively discover potential risk events. However, \nin some cases, a high recall may come with a lower precision. Therefore, in practical applications, \nit is necessary to strike a balance between recall and precision to meet specific task requirements.\n4.3.4 F1-Score\nThe F1 score (F1-Score) is a comprehensive performance metric for classification models that \ncombines precision and recall. It is used to provide a holistic assessment of a model’s performance. \nThe F1 score aims to balance a model’s prediction accuracy for the positive class (precision) and its \nability to capture the positive class (recall), making it particularly useful for addressing class imbalance \nissues. The formula for calculating the F1 score is as follows:\nF Score Precision Recall\nPrecision Recall\n1 2/nobreakspace= ⋅ ⋅\n+\n \nHigher values of the F1 score indicate that the model performs better in balancing precision and \nrecall. When both precision and recall of the model are high, the F1 score will also be high, and vice \nversa. In applications like financial risk assessment, researchers may place greater emphasis on the \nF1 score because it provides a comprehensive performance evaluation by considering both model \nerrors and omissions.\n4.4 Experimental Comparison and Analysis\nIn the research of intelligent monitoring and warning models for financial risk, we utilized four crucial \nfinancial datasets: the Fama-French Three-Factor Dataset, CRSP Dataset, Compustat Dataset, and \nWorld Bank Dataset. These datasets contain extensive information on financial markets and companies, \ncovering areas such as stock markets, financial data, and macroeconomic indicators, providing us \nwith rich data resources. To evaluate the performance of our model in financial risk monitoring \ntasks, we employed four core evaluation metrics: Accuracy, Precision, Recall, and F1-Score. These \nmetrics have different strengths and can help us gain a comprehensive understanding of the model’s \nperformance, thereby providing valuable insights for financial risk management and decision-making.\nIn this section, first, we compared the performance of our approach with methods proposed by \nDu, Peng et al., Li, Xuetao et al., and others. We presented the results of the experimental comparisons \nand analyses using tables and visualizations. Additionally, we conducted comparative analyses of \ndifferent models in terms of parameter count, inference time, and training time.\nFrom the results in Table 1, we can analyze and compare the performance of different models on \nthe Fama-French Three-Factor Dataset and CRSP Dataset. First, on the Fama-French Three-Factor \nDataset, our model excels in all four metrics: Accuracy, Precision, Recall, and F1-Score, achieving \n94.37%, 93.21%, 91.65%, and 92.42%, respectively. Specifically, Li, Xuetao et al.’s method also \nJournal of Organizational and End User Computing\nVolume 36 • Issue 1\n17\nperforms well on this dataset, with high precision and recall. Compared to Du, Peng et al.’s method, our \napproach shows improvements of 5.16% in recall and 6.89% in F1-Score.Next, on the CRSP Dataset, \nour model also outperforms others, with the highest values in all metrics, including an accuracy of \n93.84% and an F1-Score of 92.91%. This indicates that our method exhibits excellent performance \non this dataset as well. Li, Xuetao et al.’s model also performs well on the CRSP Dataset, especially \nin terms of precision and F1-Score. Regin, R et al.’s model shows relatively lower performance on \nthis dataset, particularly in precision. By comparing the metrics on both datasets, it is evident that our \nmethod outperforms other models in terms of overall performance. Additionally, we have visualized \nthe results from Table 1 for comparison, as shown in the following Figure 6.\nTable 2 displays the performance of different experimental methods on the Compustat Dataset \nand World Bank Dataset. On the Compustat Dataset, our method achieves significantly better results \nin terms of accuracy, precision, recall, and F1-Score, with values of 95.13%, 93.42%, 91.49%, and \n92.44%, respectively, outperforming other methods. Compared to Li, Xuetao et al.’s method, our \napproach improves accuracy and F1-Score by 8.91% and 4.03%, respectively. Overall, our model \nexhibits remarkable performance improvements on the Compustat Dataset, demonstrating higher \nclassification accuracy and overall performance. On the World Bank Dataset, our method also \nperforms exceptionally well. It achieves an accuracy of 94.34%, which is 6.54% higher than the \naverage performance of other methods. Precision is 92.54%, surpassing the average performance of \nother methods. Recall is 92.31%, which is 5.36% higher than other methods, and the F1-Score reaches \nTable 1. Comparison of accuracy and other aspects between the method in this article and other methods under Fama-French \nthree-factor dataset and CRSP dataset\nFigure 6. Under the Fama-French three-factor dataset and CRSP dataset, the comparison of the accuracy of this method and \nother methods is visually displayed\n\nJournal of Organizational and End User Computing\nVolume 36 • Issue 1\n18\n92.42%, 4.01% higher than the average performance of other methods. These results indicate that our \nmodel demonstrates excellent stability and generalization across different financial datasets, further \nvalidating the reliability of our approach. Similarly, we have visualized the results from Table 2 for \ncomparison, as shown in Figure 7.\nFrom the data in Table 3, it can be observed that we conducted a comparative analysis of parameter \ncount, inference time, and training time between other methods and our method on four datasets. First, \non the Fama-French Three-Factor Dataset, our model has a relatively low parameter count (336.34M), \nindicating its efficiency in terms of model storage and computational resources, making it capable of \nrunning in resource-constrained environments. Compared to other methods, our model demonstrates \nexcellent performance in inference time, requiring only 213.58ms, showcasing its potential for real-time \nTable 2. Comparison of accuracy and other aspects between the method in this article and other methods under Compustat \nDataset and World Bank Dataset\nFigure 7. Under the Compustat Dataset and World Bank Dataset, the comparison between the accuracy and other aspects of this \nmethod and other methods is visually displayed\nTable 3. Under the four data sets, the method in this article is compared with other methods in terms of parameters and \nother indicators\n\nJournal of Organizational and End User Computing\nVolume 36 • Issue 1\n19\nmonitoring and warning. In terms of training time, our model is also competitive, completing the entire \ntraining process in just 147.24 seconds, which accelerates model iteration and optimization. On the \nCRSP Dataset, Compustat Dataset, and World Bank Dataset, our model also performs exceptionally \nwell. On these datasets, our model has a relatively low parameter count, and both inference time and \ntraining time are significantly shorter than other methods. Particularly on the CRSP Dataset, our \nmodel achieves the best training time of 137.91 seconds. These results indicate that our intelligent \nmonitoring and warning model for financial risk exhibits efficiency and scalability across different \nfinancial datasets. These advantages make our model well-suited for practical financial monitoring \nand warning tasks, especially when analyzing and applying it across multiple datasets. Similarly, we \nhave visualized the results from Table 3 for comparison, as shown in Figure 8.\nIn Table 4, we compared the performance metrics, including precision, recall, and F1-Score, of \ndifferent models on the Fama-French Three-Factor Dataset and CRSP Dataset. On the Fama-French \nThree-Factor Dataset, the baseline model has a precision of 76.82%, recall of 74.92%, and an F1-Score \nof 75.86%. With the addition of an LSTM layer, there is a slight improvement in performance, with \nprecision increasing to 81.73%, recall to 80.25%, and F1-Score to 80.98%. Further incorporating the \nTransformer, the performance is enhanced, with precision reaching 87.92%, recall at 86.92%, and \nan F1-Score of 87.42%. Finally, by combining LSTM and Transformer, the model achieves the best \nperformance on the Fama-French Three-Factor Dataset, with precision at 93.21%, recall at 91.65%, \nand an F1-Score of 92.42%.On the CRSP Dataset, a similar trend is observed. The baseline model \nperforms relatively poorly, with precision at 74.94%, recall at 72.42%, and an F1-Score of 73.66%. \nAdding LSTM and Transformer individually improves performance, but the best performance is still \nachieved when combining LSTM and Transformer, with precision at 94.62%, recall at 91.27%, and \nan F1-Score of 92.91%. These results indicate that on both datasets, the model that combines LSTM \nand Transformer achieves the best performance in financial risk monitoring, playing a crucial role \nFigure 8. Under four data sets, the comparison of parameters and other indicators between this method and other methods is \nvisually displayed\nTable 4. In ablation experiments on Fama-French Three-Factor Dataset and CRSP Dataset, precision, recall, and F1-score \nmetrics are selected for evaluation\n\nJournal of Organizational and End User Computing\nVolume 36 • Issue 1\n20\nin improving model accuracy and recall. Additionally, we have visualized the results from Table 4 \nfor comparison, as shown in Figure 9.\nIn Table 5, we conducted a comparative analysis of experimental results on the Compustat Dataset \nand World Bank Dataset. On the Compustat Dataset, the baseline model’s performance exhibited a \nprecision of 75.25%, recall of 73.42%, and an F1-Score of 74.32%. When we introduced an LSTM \nlayer, the performance significantly improved, with precision increasing to 82.93%, recall to 79.64%, \nand F1-Score to 81.25%. With the further addition of the Transformer, the performance improved \nonce again, with precision reaching 86.27%, recall at 83.35%, and an F1-Score of 84.78%. Finally, \nthe model that combined LSTM and Transformer achieved the best performance on the Compustat \nDataset, with precision at 93.42%, recall at 91.49%, and an F1-Score of 92.44%.On the World Bank \nDataset, a similar trend was observed. The baseline model’s performance was relatively low, with \nprecision at 72.83%, recall at 75.28%, and an F1-Score of 74.03%. Adding LSTM and Transformer \nindividually improved performance, but the best performance was still achieved when combining \nLSTM and Transformer, with precision at 92.54%, recall at 92.31%, and an F1-Score of 92.42%. \nThis series of results emphasizes the outstanding performance of the model that combines LSTM \nand Transformer in financial risk monitoring, significantly enhancing model accuracy and recall, \nespecially across different datasets. Finally, I have visualized the results from Table 5 for comparison, \nas shown in Figure 10.\nFigure 9. Visual comparison of precision, recall, and F1 score metrics in ablation experiments on Fama-French Three-Factor \nDataset and CRSP Dataset\nTable 5. In ablation experiments on Compustat Dataset and World Bank Dataset, precision, recall, and F1-score metrics are \nselected for evaluation\n\nJournal of Organizational and End User Computing\nVolume 36 • Issue 1\n21\n5. DISCUSSION\nOur research aims to explore a novel deep learning model that combines the strengths of LSTM and \nTransformer for financial risk monitoring and warning. We conducted extensive experiments on \nmultiple financial datasets to validate the exceptional performance of this model. The experimental \nresults demonstrate that our deep learning model excels in key metrics such as accuracy, precision, \nrecall, and F1-Score, affirming its significant advantages in the field of financial risk. However, \nit’s important to acknowledge some limitations in our study. Firstly, the datasets we used still have \nroom for improvement in terms of scale and diversity to better reflect the variability of real financial \nmarkets. Secondly, the model’s computational resource requirements are relatively high, which \nmay pose deployment challenges in practical scenarios. Further research is needed to optimize the \ncomputational efficiency of the model. Thirdly, the interpretability of deep learning models remains \na subject that requires deeper investigation to help financial practitioners better understand the \ndecision-making processes of the models. Additionally, our research did not consider the impact of \nexternal factors on risk, such as changes in the macroeconomic environment and policy adjustments. \nThis is an important direction for future research. Finally, model performance may vary in different \nfinancial markets or industries, necessitating further experimental validation.\nOn the other hand, our experimental results suggest that the deep learning model combining \nLSTM and Transformer may offer new insights for financial risk monitoring. Beyond the improvement \nin accuracy and recall, our model can detect potential risk signals at an earlier stage, facilitating \nproactive risk management strategies. Furthermore, our research underscores the importance of data \nquality in model performance, emphasizing the critical role of data preprocessing and cleansing in \nfinancial risk monitoring. Future research directions include exploring the applicability of this model \nin different financial domains and further enhancing model interpretability. Our study validates the \npotential value of the proposed hybrid model in financial data analysis, but challenges related to \ninterpretability and data quality need to be addressed further.\nIn the future, we will work on improving the efficiency of the model to accommodate larger-\nscale financial data and consider the impact of external factors. Additionally, we plan to apply the \nmodel to real financial markets to assess its practical utility in risk monitoring and decision-making. \nIn summary, our research highlights several key advantages of the model we proposed, including \nenhanced predictive accuracy and earlier risk detection, providing new insights and opportunities for \nfuture research and development in the field of financial risk management.\nFigure 10. Visual comparison of precision, recall, and F1 score metrics in ablation experiments on Compustat Dataset and World \nBank Dataset\n\nJournal of Organizational and End User Computing\nVolume 36 • Issue 1\n22\n6. CONCLUSION\nIn this study, we are committed to advancing innovation in the field of financial risk monitoring \nby introducing deep learning technologies, specifically Long Short-Term Memory (LSTM) and \nTransformer models. Our contributions are multifaceted. Firstly, the incorporation of the LSTM model \nsuccessfully extends traditional monitoring methods, enabling our model to more effectively capture \nthe long-term dependency relationships in time series data within financial markets. This is crucial for \npredicting risk events in financial markets, as traditional methods may struggle to accurately capture \nthese complex temporal dynamics. Secondly, the introduction of the Transformer model further \nemphasizes our focus on nonlinear relationships and complex dynamics in financial markets. Compared \nto traditional statistical and linear models, the Transformer model, with its self-attention mechanism, \nmore flexibly captures nonlinear relationships in the data, enhancing our model’s adaptability to \ncomplex market changes. Most importantly, we have innovatively blended deep learning techniques \nwith traditional statistical and machine learning approaches, creating a new methodological paradigm. \nThis fusion allows our model to comprehensively understand the dynamic changes in financial \nmarkets, improving predictive accuracy and market sensitivity.In summary, our research brings a \nfresh perspective and methodology to the field of financial risk monitoring. Compared to existing \nwork, our model possesses unique advantages in capturing nonlinear relationships, multidimensional \ndata, and long-term dependencies in time series. This innovation not only provides new directions for \ntheoretical research but also offers a critical tool for practical risk management in financial markets. \nIn the future, we aim to further optimize the efficiency of the model, expand its application to larger-\nscale financial datasets, and consider the impact of external factors to validate its utility in real-world \napplications. Through continued research, we anticipate bringing more innovation and opportunities \nto the field of financial risk management.\nJournal of Organizational and End User Computing\nVolume 36 • Issue 1\n23\nREFERENCES\nAdebayo, T. S., Akadiri, S. S., & Rjoub, H. (2022). On the relationship between economic policy uncertainty, \ngeopolitical risk and stock market returns in South Korea: A quantile causality analysis. Annals of Financial \nEconomics, 17(01), 2250008. doi:10.1142/S2010495222500087\nAl Janabi, M. A. (2022). Optimization algorithms and investment portfolio analytics with machine learning \ntechniques under time-varying liquidity constraints. Journal of Modelling in Management, 17(3), 864–895. \ndoi:10.1108/JM2-10-2020-0259\nAli, M., Khan, D. M., Alshanbari, H. M., & El-Bagoury, A. A. A. H. (2023). Prediction of complex stock \nmarket data using an improved hybrid emd-lstm model. Applied Sciences (Basel, Switzerland), 13(3), 1429. \ndoi:10.3390/app13031429\nAlizamir, M., Shiri, J., Fard, A. F., Kim, S., Gorgij, A. D., Heddam, S., & Singh, V. P. (2023). Improving the \naccuracy of daily solar radiation prediction by climatic data using an efficient hybrid deep learning model: Long \nshort-term memory (LSTM) network coupled with wavelet transform. Engineering Applications of Artificial \nIntelligence, 123, 106199. doi:10.1016/j.engappai.2023.106199\nAshtiani, M. N., & Raahmei, B. (2023). News-based intelligent prediction of financial markets using text \nmining and machine learning: A systematic literature review. Expert Systems with Applications, 217, 119509. \ndoi:10.1016/j.eswa.2023.119509\nBehera, J., Pasayat, A. K., Behera, H., & Kumar, P. (2023). Prediction based mean-value-at-risk portfolio \noptimization using machine learning regression algorithms for multi-national stock markets. Engineering \nApplications of Artificial Intelligence, 120, 105843. doi:10.1016/j.engappai.2023.105843\nChen, Q., Huang, Z., & Liang, F. (2023). Measuring systemic risk with high-frequency data: A realized GARCH \napproach. Finance Research Letters, 54, 103753. doi:10.1016/j.frl.2023.103753\nChen, X., Peng, H., Wang, D., Lu, H., & Hu, H. (2023). SeqTrack: Sequence to Sequence Learning for Visual \nObject Tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition \n(pp. 14572-14581). IEEE. doi:10.1109/CVPR52729.2023.01400\nCheng, L., van Dongen, B. F., & van der Aalst, W. M. (2019). Scalable discovery of hybrid process models \nin a cloud computing environment. IEEE Transactions on Services Computing, 13(2), 368–380. doi:10.1109/\nTSC.2019.2906203\nDai, W. (2022). Application of improved convolution neural network in financial forecasting. [JOEUC]. Journal \nof Organizational and End User Computing, 34(3), 1–16. doi:10.4018/JOEUC.289222\nFuchsF.HorvathB. (2023). A Hybrid Quantum Wasserstein GAN with Applications to Option Pricing. Available \nat SSRN 4514510. 10.2139/ssrn.4514510\nGianfreda, A., Maranzano, P., Parisio, L., & Pelagatti, M. (2023). Testing for integration and cointegration when \ntime series are observed with noise. Economic Modelling, 125, 106352. doi:10.1016/j.econmod.2023.106352\nHassan, S. G., Kieuvan, T. T., Liu, S., Garg, H., Hassan, M., & Iqbal, S. (2023). A Novel First-Order Fuzzy \nRules-Based Forecasting System Using Distance Measures Approach for Financial Market Forecasting. Journal \nof Mathematics, 2023, 2023. doi:10.1155/2023/8027664\nHong, J., Zhang, H., & Xu, X. (2023). Thermal fault prognosis of lithium-ion batteries in real-world electric \nvehicles using self-attention mechanism networks. Applied Thermal Engineering, 226, 120304. doi:10.1016/j.\napplthermaleng.2023.120304\nHu, J., Chang, Q., & Yan, S. (2023). A GRU-based hybrid global stock price index forecasting model with group \ndecision-making. International Journal on Computer Science and Engineering, 26(1), 12–19.\nKaur, J., Parmar, K. S., & Singh, S. (2023). Autoregressive models in environmental forecasting time series: \nA theoretical and application review. Environmental Science and Pollution Research International , 30(8), \n19617–19641. doi:10.1007/s11356-023-25148-9 PMID:36648728\nKorthikanti, V. A., Casper, J., Lym, S., McAfee, L., Andersch, M., Shoeybi, M., & Catanzaro, B. (2023). Reducing \nactivation recomputation in large transformer models. Proceedings of Machine Learning and Systems, 5.\nJournal of Organizational and End User Computing\nVolume 36 • Issue 1\n24\nLazcano, A., Herrera, P. J., & Monge, M. (2023). A Combined Model Based on Recurrent Neural Networks and \nGraph Convolutional Networks for Financial Time Series Forecasting. Mathematics, 11(1), 224. doi:10.3390/\nmath11010224\nLiu, C., Zeng, Q., Cheng, L., Duan, H., & Cheng, J. (2021). Measuring similarity for data-aware business \nprocesses. IEEE Transactions on Automation Science and Engineering , 19(2), 1070–1082. doi:10.1109/\nTASE.2021.3049772\nLiu, L., Cai, L., Zhang, C., Zhao, X., Gao, J., Wang, W., & Li, Q. (2023, July). Linrec: Linear attention mechanism \nfor long-term sequential recommender systems. In Proceedings of the 46th International ACM SIGIR Conference \non Research and Development in Information Retrieval (pp. 289-299). ACM. doi:10.1145/3539618.3591717\nMba, J. C., & Mai, M. M. (2022). A particle swarm optimization copula-based approach with application to \ncryptocurrency portfolio optimisation. Journal of Risk and Financial Management, 15(7), 285. doi:10.3390/\njrfm15070285\nMegdad, M. M., Abu-Naser, S. S., & Abu-Nasser, B. S. (2022). Fraudulent financial transactions detection \nusing machine learning.\nMgammal, M. H., Al-Matari, E. M., & Alruwaili, T. F. (2023). Value-added-tax rate increases: A comparative \nstudy using difference-in-difference with an ARIMA modeling approach. Humanities & Social Sciences \nCommunications, 10(1), 1–17. doi:10.1057/s41599-023-01608-y PMID:36969312\nMousapour Mamoudan, M., Ostadi, A., Pourkhodabakhsh, N., Fathollahi-Fard, A. M., & Soleimani, F. (2023). \nHybrid neural network-based metaheuristics for prediction of financial markets: A case study on global gold \nmarket. Journal of Computational Design and Engineering, 10(3), 1110–1125. doi:10.1093/jcde/qwad039\nRapoo, M. I., Chanza, M. M., & Motlhwe, G. (2023). Inflation Rate Modelling Through a Hybrid Model of \nSeasonal Autoregressive Moving Average and Multilayer Perceptron Neural Network. In Research Anthology \non Macroeconomics and the Achievement of Global Stability (pp. 551–567). IGI Global.\nVuletićM.PrenzelF.CucuringuM. (2023). Fin-gan: Forecasting and classifying financial time series via generative \nadversarial networks. Available at SSRN 4328302. 10.2139/ssrn.4328302\nWang, P., & Han, W. (2021). Construction of a New Financial E-Commerce Model for Small and Medium-Sized \nEnterprise Financing Based on Multiple Linear Logistic Regression. [JOEUC]. Journal of Organizational and \nEnd User Computing, 33(6), 1–18. doi:10.4018/JOEUC.286808\nWang, X., Kang, Y., Hyndman, R. J., & Li, F. (2023). Distributed ARIMA models for ultra-long time series. \nInternational Journal of Forecasting, 39(3), 1163–1184. doi:10.1016/j.ijforecast.2022.05.001\nWang, Y., Yang, G., Li, S., Li, Y., He, L., & Liu, D. (2023). Arrhythmia classification algorithm based on \nmulti-head self-attention mechanism. Biomedical Signal Processing and Control, 79, 104206. doi:10.1016/j.\nbspc.2022.104206\nWang, Z., Zhao, Q., & Qiu, L. (2022). Multi-Dimensional Factor Correlation, Multiple Interbank Network \nContagion, and Conditional VaR of Banks. Frontiers in Physics (Lausanne) , 10, 895603. doi:10.3389/\nfphy.2022.895603\nXu, X., Jin, X., Xiao, D., Ma, C., & Wong, S. C. (2023). A hybrid autoregressive fractionally integrated moving \naverage and nonlinear autoregressive neural network model for short-term traffic flow prediction. Journal of \nIntelligent Transport Systems, 27(1), 1–18. doi:10.1080/15472450.2021.1977639",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.632352888584137
    },
    {
      "name": "Transformer",
      "score": 0.6255223155021667
    },
    {
      "name": "Warning system",
      "score": 0.572053074836731
    },
    {
      "name": "Artificial intelligence",
      "score": 0.502990186214447
    },
    {
      "name": "Deep learning",
      "score": 0.4385489523410797
    },
    {
      "name": "Early warning system",
      "score": 0.4172613024711609
    },
    {
      "name": "Machine learning",
      "score": 0.3852798044681549
    },
    {
      "name": "Finance",
      "score": 0.3405027687549591
    },
    {
      "name": "Business",
      "score": 0.22195354104042053
    },
    {
      "name": "Engineering",
      "score": 0.11980593204498291
    },
    {
      "name": "Telecommunications",
      "score": 0.11227241158485413
    },
    {
      "name": "Electrical engineering",
      "score": 0.06923496723175049
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I49232843",
      "name": "Changchun University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I75955062",
      "name": "Henan Normal University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I1315279114",
      "name": "Zhejiang Wanli University",
      "country": "CN"
    }
  ]
}