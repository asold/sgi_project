{
  "title": "Performance of the Large Language Model ChatGPT on the National Nurse Examinations in Japan: Evaluation Study",
  "url": "https://openalex.org/W4378528433",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2095727881",
      "name": "Kazuya Taira",
      "affiliations": [
        "Kyoto University"
      ]
    },
    {
      "id": "https://openalex.org/A2142814391",
      "name": "Takahiro Itaya",
      "affiliations": [
        "Kyoto University",
        "Keio University"
      ]
    },
    {
      "id": "https://openalex.org/A4328092875",
      "name": "Ayame Hanada",
      "affiliations": [
        "Kyoto University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3090793558",
    "https://openalex.org/W2767381938",
    "https://openalex.org/W3017117984",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4319460874",
    "https://openalex.org/W4319663047",
    "https://openalex.org/W4321366933",
    "https://openalex.org/W4254034712",
    "https://openalex.org/W4319062614",
    "https://openalex.org/W4321018175",
    "https://openalex.org/W4289932535",
    "https://openalex.org/W4319452268"
  ],
  "abstract": "Background ChatGPT, a large language model, has shown good performance on physician certification examinations and medical consultations. However, its performance has not been examined in languages other than English or on nursing examinations. Objective We aimed to evaluate the performance of ChatGPT on the Japanese National Nurse Examinations. Methods We evaluated the percentages of correct answers provided by ChatGPT (GPT-3.5) for all questions on the Japanese National Nurse Examinations from 2019 to 2023, excluding inappropriate questions and those containing images. Inappropriate questions were pointed out by a third-party organization and announced by the government to be excluded from scoring. Specifically, these include “questions with inappropriate question difficulty” and “questions with errors in the questions or choices.” These examinations consist of 240 questions each year, divided into basic knowledge questions that test the basic issues of particular importance to nurses and general questions that test a wide range of specialized knowledge. Furthermore, the questions had 2 types of formats: simple-choice and situation-setup questions. Simple-choice questions are primarily knowledge-based and multiple-choice, whereas situation-setup questions entail the candidate reading a patient’s and family situation’s description, and selecting the nurse's action or patient's response. Hence, the questions were standardized using 2 types of prompts before requesting answers from ChatGPT. Chi-square tests were conducted to compare the percentage of correct answers for each year's examination format and specialty area related to the question. In addition, a Cochran-Armitage trend test was performed with the percentage of correct answers from 2019 to 2023. Results The 5-year average percentage of correct answers for ChatGPT was 75.1% (SD 3%) for basic knowledge questions and 64.5% (SD 5%) for general questions. The highest percentage of correct answers on the 2019 examination was 80% for basic knowledge questions and 71.2% for general questions. ChatGPT met the passing criteria for the 2019 Japanese National Nurse Examination and was close to passing the 2020-2023 examinations, with only a few more correct answers required to pass. ChatGPT had a lower percentage of correct answers in some areas, such as pharmacology, social welfare, related law and regulations, endocrinology/metabolism, and dermatology, and a higher percentage of correct answers in the areas of nutrition, pathology, hematology, ophthalmology, otolaryngology, dentistry and dental surgery, and nursing integration and practice. Conclusions ChatGPT only passed the 2019 Japanese National Nursing Examination during the most recent 5 years. Although it did not pass the examinations from other years, it performed very close to the passing level, even in those containing questions related to psychology, communication, and nursing.",
  "full_text": "Original Paper\nPerformance of the Large Language Model ChatGPT on the\nNational Nurse Examinations in Japan: Evaluation Study\nKazuya Taira1*, RN, PHN, PhD; Takahiro Itaya2,3*, RN, MPH, DrPH; Ayame Hanada1, RN, PHN, BHS\n1Department of Human Health Sciences, Graduate School of Medicine, Kyoto University, Kyoto, Japan\n2Department of Healthcare Epidemiology, Graduate School of Medicine and Public Health, Kyoto University, Kyoto, Japan\n3Department of Preventive Medicine and Public Health, School of Medicine, Keio University, Tokyo, Japan\n*these authors contributed equally\nCorresponding Author:\nKazuya Taira, RN, PHN, PhD\nDepartment of Human Health Sciences\nGraduate School of Medicine\nKyoto University\n53, Shogoinkawara-cho, Sakyo-ku\nKyoto, 606-8501\nJapan\nPhone: 81 0757513927\nEmail: taira.kazuya.5m@kyoto-u.ac.jp\nAbstract\nBackground: ChatGPT, a large language model, has shown good performance on physician certification examinations and\nmedical consultations. However, its performance has not been examined in languages other than English or on nursing examinations.\nObjective: We aimed to evaluate the performance of ChatGPT on the Japanese National Nurse Examinations.\nMethods: We evaluated the percentages of correct answers provided by ChatGPT (GPT-3.5) for all questions on the Japanese\nNational Nurse Examinations from 2019 to 2023, excluding inappropriate questions and those containing images. Inappropriate\nquestions were pointed out by a third-party organization and announced by the government to be excluded from scoring. Specifically,\nthese include “questions with inappropriate question difficulty” and “questions with errors in the questions or choices.” These\nexaminations consist of 240 questions each year, divided into basic knowledge questions that test the basic issues of particular\nimportance to nurses and general questions that test a wide range of specialized knowledge. Furthermore, the questions had 2\ntypes of formats: simple-choice and situation-setup questions. Simple-choice questions are primarily knowledge-based and\nmultiple-choice, whereas situation-setup questions entail the candidate reading a patient’s and family situation’s description, and\nselecting the nurse's action or patient's response. Hence, the questions were standardized using 2 types of prompts before requesting\nanswers from ChatGPT. Chi-square tests were conducted to compare the percentage of correct answers for each year's examination\nformat and specialty area related to the question. In addition, a Cochran-Armitage trend test was performed with the percentage\nof correct answers from 2019 to 2023.\nResults: The 5-year average percentage of correct answers for ChatGPT was 75.1% (SD 3%) for basic knowledge questions\nand 64.5% (SD 5%) for general questions. The highest percentage of correct answers on the 2019 examination was 80% for basic\nknowledge questions and 71.2% for general questions. ChatGPT met the passing criteria for the 2019 Japanese National Nurse\nExamination and was close to passing the 2020-2023 examinations, with only a few more correct answers required to pass.\nChatGPT had a lower percentage of correct answers in some areas, such as pharmacology, social welfare, related law and\nregulations, endocrinology/metabolism, and dermatology, and a higher percentage of correct answers in the areas of nutrition,\npathology, hematology, ophthalmology, otolaryngology, dentistry and dental surgery, and nursing integration and practice.\nConclusions: ChatGPT only passed the 2019 Japanese National Nursing Examination during the most recent 5 years. Although\nit did not pass the examinations from other years, it performed very close to the passing level, even in those containing questions\nrelated to psychology, communication, and nursing.\n(JMIR Nursing 2023;6:e47305) doi: 10.2196/47305\nJMIR Nursing 2023 | vol. 6 | e47305 | p. 1https://nursing.jmir.org/2023/1/e47305\n(page number not for citation purposes)\nTaira et alJMIR NURSING\nXSL•FO\nRenderX\nKEYWORDS\nChatGPT; artificial intelligence; natural language processing; registered nurses; National Nurse Examination; Japan\nIntroduction\nWhat is ChatGPT?\nChatGPT is a large language model developed by OpenAI [1].\nBased on the GPT architecture, it is capable of generating\nhigh-quality, human-like text in response to prompts. Pretrained\non a large corpus of text data, it has been fine-tuned for specific\nnatural language processing (NLP) tasks such as language\ngeneration and summarization. With several variants available,\nChatGPT—the largest one containing over 175 billion\nparameters [2]—is one of the largest deep learning models in\nexistence. Its potential applications include being used as a\nchatbot, language translation, text summarization, and content\ngeneration, making it a significant advancement in NLP.\nApplication of ChatGPT to Medical Fields\nArtificial intelligence (AI) applications have been used in the\nmedical field, including medical chatbots, and applications that\nanalyze and summarize electronic medical record systems,\nperform image diagnosis, analyze and organize the medical\nliterature, and perform patient monitoring [3,4]. Release of the\nhigh-quality chatbot ChatGPT has also attracted attention in the\nfield of medical education, as questions on the United States\nMedical Licensing Examination were reportedly answered with\n60% accuracy, which is the threshold for passing the\nexamination [5-7]. In addition, studies have evaluated the\nChatGPT’s responses to questions on counseling for the\ntreatment of infectious diseases [8] and prevention of\ncardiovascular diseases [9].\nDifferences Between Physician and Nurse Specialties\nAlthough physicians and nurses both play critical roles in the\nhealth care system, their specialties and responsibilities differ.\nPhysicians focus on diagnosing and treating illnesses, whereas\nnurses focus on providing direct patient care and support. Nurses\nmonitor patient health, administer medications, assist with\nactivities of daily living, and provide emotional support to\npatients and their families. Nurses also communicate with other\nhealth care professionals to ensure that patients receive the\nappropriate care. Therefore, their training and responsibilities\ngenerally focus more on patient care and communication than\non diagnosis and treatment.\nEvaluating the Performance of ChatGPT on the\nNational Nurse Examinations in Japan\nWhile passing the national examination does not guarantee the\nability to practice in a clinical setting, a different scenario arises\nwhen considering the performance on registered nurse licensing\nexaminations. These examinations feature questions that\nemphasize on patient emotions and communication, contrasting\nwith those found in physician licensing examinations. Notably,\nif excellent performance can be demonstrated in these nursing\nexaminations, it is likely to pave the way for a significant\nexpansion of AI applications in the medical field. However, the\nperformance of ChatGPT on nursing licensing examinations\nhas not yet been evaluated.\nWe aimed to evaluate the performance of ChatGPT on national\nexaminations for registered nurses in Japan.\nMethods\nInput Data Sets From the National Nurse Examinations\nin Japan\nThe data sets included questions and answers from the National\nNurse Examinations in Japan from 2019 to 2023 (Multimedia\nAppendix 1). These examinations are conducted annually and\ninclude 240 multiple-choice questions, in which candidates are\nrequired to select 1 or, in some cases, multiple correct answers\n(ie, all that apply) from several options. These examinations\nwere divided into morning and afternoon sessions, each\ncomprising 120 questions. The questions covered 32 areas,\nincluding basic nursing skills, adult nursing, gerontological\nnursing, pediatric nursing, pathology, anatomy, and physiology.\nThe Japanese National Nurse Examinations consist of 2 types\nof questions, basic knowledge and general questions, and all\n240 questions must be answered. The basic knowledge questions\nare based on basic issues of particular importance to nurses,\nsuch as fundamental knowledge and basic nursing skills, while\nthe general questions are based on the extensive knowledge of\neach nursing specialty, covering anatomy, physiology, and\ndisease. As inappropriate questions are excluded from scoring,\nthe criteria could change slightly; however, the passing criteria\nare 80% for basic knowledge questions and approximately 60%\nfor general questions. In addition, the situation-setup questions\nincluded among the general questions were worth 2 points,\nwhereas all other questions were worth 1 point. While the\nsimple-choice questions are mainly multiple-choice knowledge\nquestions, the situation-setup question requires the candidate\nto read a description of the situation of the patient and the\npatient's family, and then select the action to be taken by the\nnurse or the response to the patient.\nData Exclusion\nEach year, the Ministry of Health, Labor and Welfare (MHLW)\nof Japan, which certifies the qualification of registered nurses\nnationwide, reviews questions among conducted examinations,\nwhich cannot be answered with just 1 answer, or questions for\nwhich no correct answer exists, based on the MHLW’s own\nchecks and comments from a third-party organization—the\nJapan Nursing School Association. Then, the MHLW deems\nthese as “inappropriate questions” and removes them from the\nexaminations. The inappropriate questions were excluded from\nthis study. In addition, all questions were screened, and\nquestions containing visual assets, such as clinical images,\nmedical photography, and graphs, were removed because\nChatGPT (GPT-3.5) is an interactive language AI that does not\nsupport image recognition.\nPrompt Engineering\nBecause prompt engineering significantly affects generative\noutput, we standardized the input formats of the questions [10].\nQuestion and answer prompts were created optimally based on\nJMIR Nursing 2023 | vol. 6 | e47305 | p. 2https://nursing.jmir.org/2023/1/e47305\n(page number not for citation purposes)\nTaira et alJMIR NURSING\nXSL•FO\nRenderX\nthe Prompt-Engineering-Guide published on GitHub [11] to\nachieve conservative performance rather than simply achieving\nthe highest scores. As the National Nurse Examinations include\n2 types of questions, 2 prompts were created (Textbox 1).\nTextbox 1. Prompts for questions.\nPrompt 1: Simple-choice questions\nPlease answer the following questions briefly and by number.\nQuestion: <Questionnaire contents>\n1. <Option 1>\n2. <Option 2>\n3. <Option 3>\n4. <Option 4>\nPrompt 2: Situation-setup questions\nBased on the following situation setup, please answer the questions briefly and by number.\nSituation-setup: <Situation-setup contents>\nQuestion: <Questionnaire contents>\n1. <Option 1>\n2. <Option 2>\n3. <Option 3>\n4. <Option 4>\nData Analyses\nBased on the scoring criteria of the official nursing examination,\nthe percentage of correct answers provided by ChatGPT\n(GPT-3.5) was calculated separately for basic knowledge and\ngeneral questions. We calculated the percentage of correct\nanswers for each of the simple-choice questions (1 point, prompt\n1) and the situation-setup questions (2 points, prompt 2) and\nconducted a chi-square test to compare the percentage of correct\nanswers between the 2 prompts. Finally, the percentage of\ncorrect answers was calculated for each of the 32 subject areas,\nand areas with higher and lower percentages of correct answers\ncompared with the overall mean and 1 SD were extracted. All\nstatistical analyses were performed using R (version 3.6.2; R\nFoundation for Statistical Computing).\nEthics Approval\nThis study did not require ethics approval because we only\nanalyzed data from a published database.\nResults\nInput Data Statistics\nFive years of the National Nurse Examination data showed that\nthe largest number of inappropriate questions occurred in 2019,\nwith 10 questions having been excluded from the scoring and\n2 or 3 inappropriate questions in the other years. The number\nof questions with figures and tables ranged from 6 to 16. Thus,\nthe number of questions analyzed in this study was 214 of 240\nin the lowest year and 232 of 240 in the highest year (Table 1).\nTable 1. Questions included and excluded in the analysis from 2019 to 2023.\nTotal, nQuestions with chart (mean 10.2, SD\n3.4)a, n\nInappropriate questions (mean 4, SD\n3)a, n\nIncluded questions (mean 225.8, SD\n6.2), n\nYear\n24016102142019\n240832292020\n2401022282021\n240622322022\n2401132262023\na“Inappropriate questions” and “questions with chart” were excluded in the analysis.\nEvaluation Outcomes\nThe 5-year average percentage of correct answers provided by\nChatGPT was 75.1% (SD 3%) for basic knowledge questions\nand 64.5% (SD 5%) for general questions (Figure 1).\nThroughout the study period, the percentage of correct answers\nexceeded the passing criteria in 2019 for basic knowledge\nquestions (passing criterion: 80%) and in all years from 2019\nto 2023 for general questions (passing standard: approximately\n60%). The percentage of incorrect answers per question ID\nJMIR Nursing 2023 | vol. 6 | e47305 | p. 3https://nursing.jmir.org/2023/1/e47305\n(page number not for citation purposes)\nTaira et alJMIR NURSING\nXSL•FO\nRenderX\ntended to be higher in the morning and afternoon sessions for\nIDs 51-60, and in the afternoon session for IDs 91-120\n(Multimedia Appendix 2). IDs 51-60 included questions in the\nareas of pediatric and maternal nursing and IDs 91-120 included\nsituation-setup questions. Items with high percentages of\nincorrect answers included questions with complex situational\nsettings and a combination of questions requiring the selection\nof 2 correct answers from a set of choices (both of which must\nbe correct) and a situation-setup question. The percentage of\nincorrect answers for questions in which the options included\na combination of 2 items, such as combinations of words\nconnected by hyphens (1. A ———B, 2. C———D, 3.\nE———F, 4.G———H), were also high.\nComparing simple-choice questions (prompt 1) and\nsituation-setup questions (prompt 2), the average percentage of\ncorrect answers for prompt 1 was 66.3% (SD 3%) and 65.9%\n(SD 7%) for prompt 2. Differences in the proportions of correct\nanswers between prompts 1 and 2 were not observed throughout\nthe study period (Table 2). However, prompt 1 showed no\nsignificant change over time, while prompt 2 showed a gradual\ndownward trend over time (Figure 2).\nFigure 1. Percentages of correct scores provided by ChatGPT.\nTable 2. Percentages of correct answers by prompt type.\nP value (chi-square test)Correct answers, %Incorrect, nCorrect, nTotal, n\n.242019\n68.650109159Prompt 1\n78.2124355Prompt 2\n.942020\n71.249121170Prompt 1\n69.5184159Prompt 2\n.782021\n63.562108170Prompt 1\n60.3233558Prompt 2\n.782022\n64.262111173Prompt 1\n61.0233659Prompt 2\n.772023\n64.161109170Prompt 1\n60.7223456Prompt 2\nJMIR Nursing 2023 | vol. 6 | e47305 | p. 4https://nursing.jmir.org/2023/1/e47305\n(page number not for citation purposes)\nTaira et alJMIR NURSING\nXSL•FO\nRenderX\nFigure 2. Trends in the percentage of correct answers for prompts 1 and 2.\nOn comparing the percentages of correct answers for each\nsubject area among all questions included in the analysis, the\naverage percentage of correct answers for all areas was 65.9%\n(SD 10.5%; Figure 3). The subject areas with a mean value that\nis lower than 1 SD (55.4%) included pharmacology, social\nwelfare, related law and regulations, endocrinology/metabolism,\nand dermatology. The subject areas with a mean value that is\nhigher than 1 SD (76.4%) included nutrition, pathology,\nhematology, ophthalmology, otolaryngology, dentistry and\ndental surgery, and nursing integration and practice. ChatGPT\nalso performed well on dialogue-related questions, with no\nsignificant difference in the percentage of correct answers to\nnon–dialogue-related questions (P=.36; Multimedia Appendix\n3). A dialogue question is a question in which the options are\nsentences enclosed in brackets; in Japanese, the brackets are\nthe spoken words of a person.\nFigure 3. Percentages of correct answers by question area.\nJMIR Nursing 2023 | vol. 6 | e47305 | p. 5https://nursing.jmir.org/2023/1/e47305\n(page number not for citation purposes)\nTaira et alJMIR NURSING\nXSL•FO\nRenderX\nDiscussion\nPrincipal Results\nChatGPT met the passing criteria for only the 2019 Japanese\nNational Nurse Examination. Although it did not pass the\n2020-2023 examinations, it scored very close to the passing\ncriteria, with only a few more correct answers required to pass.\nVariations in the percentages of correct answers over the 5-year\nperiod were small, the probability of obtaining a high score by\nchance was low, and the performance of ChatGPT was stable.\nTherefore, although not significantly different, the possible\nreasons the percentage of correct responses tended to decrease\nwith each passing year from 2019 to 2023 include the following:\n(1) lack of up-to-date data (ChatGPT only studied records until\n2021) and (2) increased question complexity. Although GPT-3.5\nlearned data only up to 2021, it is crucial to highlight that\nChatGPT is able to answer first-time questions; in other words,\nit is not simply filling in holes using existing internet sources,\nas there was no sharp decrease in scores in the 2022 and 2023\nexaminations. The fact that the situation-setup questions were\nalso answered correctly without significant difference indicates\nthat ChatGPT seemed to do well on the questions dealing with\nthe human mind, such as those involving conversations with\npatients. Meanwhile, the possibility of losing track of relevant\nissues in complex situational settings and of having limitations\nsuch as difficulty recognizing certain expressions, including the\nfrequent use of hyphens and other expressions, were also shown.\nIf the current version of ChatGPT were used in nursing practice,\nit could be difficult to assess patients whose situations are\ncomplex, such as those requiring treatment for multiple diseases\nor those with socioeconomic problems. However, this is likely\nto depend on the amount of information that ChatGPT can store\nin its short-term memory, which would be resolved in the future\nmodels.\nStrengths and Limitations\nThis study used all questions from the 2019-2023 National\nNursing Licensing Examinations in Japan, and the results were\nhighly reliable for the performance assessment of the ChatGPT’s\nanswers with low variability. However, this study has some\nlimitations. First, questions with figures and tables were\nexcluded. Although GPT-3.5, which was used to measure the\nperformance in this study, was unable to judge figures and\ntables, Wang et al [12] reported that combining ChatGPT and\nimage judgment AI could interpret radiographs, and it is highly\nlikely that these questions will be supported in future ChatGPT\nupdates. Second, this study did not involve advanced prompt\nengineering or explanatory assistance for questions or answer\noptions. More detailed and complex prompt engineering—such\nas providing a question and several sample answers and then\nhaving the candidate answer them, rephrasing a question into\na sentence when it uses too many hyphens or other symbols, or\nallowing additional exchanges rather than 1 answer per\nquestion—could have resulted in a score above the passing\nstandard. We originally planned to validate ChatGPT’s\nperformance in line with the actual question format, and it is\nimportant to determine whether a simple question can be\nanswered correctly by ChatGPT. Third, it should be noted that\nChatGPT is like an advanced and sophisticated autocomplete\nsystem and may not inherently understand the meaning or\ncontent of the questions entered. The degree to which\nChatGPT’s expressions and responses deviate from those of\nhumans is a subject of debate; however, ChatGPT sometimes\nprovides completely false responses without prior warning.\nTherefore, it may be important to prompt ChatGPT not to answer\nif ChatGPT is not confident in its answer or to conduct multiple\ndialogues to clarify the ChatGPT's decision-making process.\nFinally, some of ChatGPT’s answers were misaligned between\nthe number of choices and the content of choices, and the\nnumber of digits in computational questions was not adjusted\nproperly. We have counted the number of questions that were\nmisaligned between the number of choices and the content of\nthe choices, and the number of questions included were 5 in\n2023, 6 in 2022,12 in 2021, 13 in 2020, and 6 in 2019. One\ncomputational question did not adjust its digits properly in 2019\nand 2020; however, this was not the case in the other years.\nAlthough there were slightly more in 2020 and 2021, there were\nno significant differences among other years, so the impact on\nthe overall results is expected to be limited.\nIn principle, judgments were made based on the content of\nchoices, and computational questions were judged as being\ncorrect if the formula and the results of the calculation were\ncorrect.\nComparison With Prior Work\nIn general, AI using a large language model is known to perform\nbetter in English than in other languages [13], although as with\nthe United States Medical Licensing Examination [5-7], a high\npercentage of correct responses for the Japanese National Nurse\nExaminations was observed. The National Nurse Examinations\ninclude emotion-based questions, such as those involving talking\nto patients, which could have been appropriately handled by\nChatGPT, as it reportedly has been acquiring a human-like\npsychological maturity [14]. A previous study pointed out that\naccess to medical databases was limited among the training data\nfor ChatGPT [8], and statistical data related to health, medical\ncare, and welfare in Japan may not have been acquired because\nthey are provided on interactive websites such as e-Stat [15] or\nin PDF format, thus potentially having influenced the accuracy\nrate of the ChatGPT’s responses.\nIn the future, if additional data in the areas of poor performance\nare acquired and tuned so that questions and options can be\nunderstood appropriately without prompt engineering or\nsupplementary human explanation, it is highly likely that the\npassing criteria will be exceeded in a stable manner. More\nadvanced tools, such as GPT-4 or Bard (developed by Google),\nsuperseding the capabilities of ChatGPT, continue to be released\nand are expected to be used in many clinical situations such as\ndiagnosis, explanation of treatments and drugs, and\ncommunication with patients. However, further research will\nbe needed on ethical issues such as the division of roles between\nhuman nurses and AI, decision-making responsibilities, and the\nrisks for patients when applied in clinical practice.\nConclusions\nChatGPT passed or performed very close to the passing level\non the Japanese National Nurse Examinations. With additional\nJMIR Nursing 2023 | vol. 6 | e47305 | p. 6https://nursing.jmir.org/2023/1/e47305\n(page number not for citation purposes)\nTaira et alJMIR NURSING\nXSL•FO\nRenderX\nlearning, prompt engineering, and tuning of ChatGPT, it will\nlikely exceed the passing criteria. ChatGPT has the potential to\nassist nurses with decisions based on data regarding the patient’s\nphysical condition, and to provide support for psychological\nissues.\nAcknowledgments\nThis work was supported by the Japan Society for the Promotion of Science KAKENHI, grants 22K21182 and 22K17549.\nAuthors' Contributions\nKT designed the methodology, carried out the formal analysis, and drafted the manuscript. TI conceptualized the study, designed\nthe methodology, acquired funding, and reviewed and edited the manuscript. AH curated and validated the data, and reviewed\nand edited the manuscript.\nConflicts of Interest\nNone declared.\nMultimedia Appendix 1\nData Sources for the 2019–2023 Japanese National Nurse Examination.\n[DOCX File , 61 KB-Multimedia Appendix 1]\nMultimedia Appendix 2\nHeatmap of correct and incorrect answers by question ID.\n[DOCX File , 167 KB-Multimedia Appendix 2]\nMultimedia Appendix 3\nComparison of percentage of correct answers between dialogue and non-dialogue questions.\n[DOCX File , 62 KB-Multimedia Appendix 3]\nReferences\n1. Introducing ChatGPT. OpenAI. URL: https://openai.com/blog/chatgpt/ [accessed 2022-11-30]\n2. Schneider C. Setting Up a Language Learning Environment in Microsoft Teams. SISAL 2020 Sep 1:263-270 [FREE Full\ntext] [doi: 10.37237/110312]\n3. Miller DD, Brown EW. Artificial intelligence in medical practice: the question to the answer? Am J Med 2018\nFeb;131(2):129-133 [FREE Full text] [doi: 10.1016/j.amjmed.2017.10.035] [Medline: 29126825]\n4. Vaishya R, Javaid M, Khan IH, Haleem A. Artificial intelligence (AI) applications for COVID-19 pandemic. Diabetes\nMetab Syndr 2020 Jul;14(4):337-339 [FREE Full text] [doi: 10.1016/j.dsx.2020.04.012] [Medline: 32305024]\n5. Kung TH, Cheatham M, Medenilla A, Sillos C, De Leon L, Elepaño C, et al. Performance of ChatGPT on USMLE: potential\nfor AI-assisted medical education using large language models. PLOS Digit Health 2023 Feb 9;2(2):e0000198 [FREE Full\ntext] [doi: 10.1371/journal.pdig.0000198] [Medline: 36812645]\n6. Gilson A, Safranek CW, Huang T, Socrates V, Chi L, Taylor RA, et al. How does ChatGPT perform on the United States\nMedical Licensing Examination? The implications of large language models for medical education and knowledge assessment.\nJMIR Med Educ 2023 Feb 08;9:e45312 [FREE Full text] [doi: 10.2196/45312] [Medline: 36753318]\n7. Mbakwe AB, Lourentzou I, Celi LA, Mechanic OJ, Dagan A. ChatGPT passing USMLE shines a spotlight on the flaws\nof medical education. PLOS Digit Health 2023 Feb 9;2(2):e0000205 [FREE Full text] [doi: 10.1371/journal.pdig.0000205]\n[Medline: 36812618]\n8. Howard A, Hope W, Gerada A. ChatGPT and antimicrobial advice: the end of the consulting infection doctor? The Lancet\nInfectious Diseases 2023 Apr;23(4):405-406 [doi: 10.1016/s1473-3099(23)00113-5]\n9. Chen Y, Zhao C, Yu Z, McKeown K, He H. On the relation between sensitivity and accuracy in in-context learning. arXiv\nPreprint posted online September 16, 2022. [doi: 10.5860/choice.189890]\n10. Sarraju A, Bruemmer D, Van Iterson E, Cho L, Rodriguez F, Laffin L. Appropriateness of cardiovascular disease prevention\nrecommendations obtained from a popular online chat-based artificial intelligence model. JAMA 2023 Mar\n14;329(10):842-844 [doi: 10.1001/jama.2023.1044] [Medline: 36735264]\n11. Saravia E. Prompt-Engineering-Guide. GitHub. URL: https://github.com/dair-ai/Prompt-Engineering-Guide [accessed\n2023-06-01]\n12. Wang S, Zhao Z, Ouyang X, Wang Q, Shen D. ChatCAD: interactive computer-aided diagnosis on medical image using\nlarge language models. arXiv Preprint posted online February 14, 2023. [doi: 10.48550/arXiv.2302.07257]\nJMIR Nursing 2023 | vol. 6 | e47305 | p. 7https://nursing.jmir.org/2023/1/e47305\n(page number not for citation purposes)\nTaira et alJMIR NURSING\nXSL•FO\nRenderX\n13. Hu J, Ruder S, Siddhant A, Neubig G, Firat O, Johnson M. XTREME: a massively multilingual multi-task benchmark for\nevaluating cross-lingual generalisation. In: Proceedings of the 37th International Conference on Machine Learning. 2020\nPresented at: 37th International Conference on Machine Learning; July 13-18, 2020; Virtual p. 4411-4421 [doi:\n10.1364/OL.464541]\n14. Kosinski M. Theory of mind may have spontaneously emerged in large language models. arXiv Preprint posted online\nFebruary 4, 2023. [doi: 10.48550/arXiv.2302.02083]\n15. Statistics of Japan. e-Stat. URL: https://www.e-stat.go.jp/en [accessed 2023-06-01]\nAbbreviations\nAI: artificial intelligence\nMHLW: Ministry of Health, Labor and Welfare\nNLP: natural language processing\nEdited by E Borycki; submitted 20.03.23; peer-reviewed by J Silver, H Namba, M Amith; comments to author 19.04.23; revised version\nreceived 20.05.23; accepted 27.05.23; published 27.06.23\nPlease cite as:\nTaira K, Itaya T, Hanada A\nPerformance of the Large Language Model ChatGPT on the National Nurse Examinations in Japan: Evaluation Study\nJMIR Nursing 2023;6:e47305\nURL: https://nursing.jmir.org/2023/1/e47305\ndoi: 10.2196/47305\nPMID: 37368470\n©Kazuya Taira, Takahiro Itaya, Ayame Hanada. Originally published in JMIR Nursing (https://nursing.jmir.org), 27.06.2023.\nThis is an open-access article distributed under the terms of the Creative Commons Attribution License\n(https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium,\nprovided the original work, first published in JMIR Nursing, is properly cited. The complete bibliographic information, a link to\nthe original publication on https://nursing.jmir.org/, as well as this copyright and license information must be included.\nJMIR Nursing 2023 | vol. 6 | e47305 | p. 8https://nursing.jmir.org/2023/1/e47305\n(page number not for citation purposes)\nTaira et alJMIR NURSING\nXSL•FO\nRenderX",
  "topic": "Multiple choice",
  "concepts": [
    {
      "name": "Multiple choice",
      "score": 0.7191554307937622
    },
    {
      "name": "Test (biology)",
      "score": 0.6985560059547424
    },
    {
      "name": "Certification",
      "score": 0.6750746369361877
    },
    {
      "name": "Specialty",
      "score": 0.6043407917022705
    },
    {
      "name": "Reading (process)",
      "score": 0.47714468836784363
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.43601080775260925
    },
    {
      "name": "Medicine",
      "score": 0.390584796667099
    },
    {
      "name": "Nursing",
      "score": 0.38777661323547363
    },
    {
      "name": "Psychology",
      "score": 0.3861689567565918
    },
    {
      "name": "Medical education",
      "score": 0.38053345680236816
    },
    {
      "name": "Family medicine",
      "score": 0.304718554019928
    },
    {
      "name": "Linguistics",
      "score": 0.15972411632537842
    },
    {
      "name": "Political science",
      "score": 0.09785211086273193
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I22299242",
      "name": "Kyoto University",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I203951103",
      "name": "Keio University",
      "country": "JP"
    }
  ]
}