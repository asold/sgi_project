{
  "title": "LCTR: On Awakening the Local Continuity of Transformer for Weakly Supervised Object Localization",
  "url": "https://openalex.org/W4200631217",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2100382403",
      "name": "Zhiwei Chen",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A2099118511",
      "name": "Changan Wang",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2584466352",
      "name": "Yabiao Wang",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2495913153",
      "name": "Guannan Jiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2799099420",
      "name": "Yunhang Shen",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2152449637",
      "name": "Ying Tai",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2097540737",
      "name": "Chengjie Wang",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A1506313323",
      "name": "Wei Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2651443321",
      "name": "Liujuan Cao",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A2100382403",
      "name": "Zhiwei Chen",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A2099118511",
      "name": "Changan Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2584466352",
      "name": "Yabiao Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2495913153",
      "name": "Guannan Jiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2799099420",
      "name": "Yunhang Shen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2152449637",
      "name": "Ying Tai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097540737",
      "name": "Chengjie Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1506313323",
      "name": "Wei Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2651443321",
      "name": "Liujuan Cao",
      "affiliations": [
        "Xiamen University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6778485988",
    "https://openalex.org/W2765793020",
    "https://openalex.org/W3207782762",
    "https://openalex.org/W2950557962",
    "https://openalex.org/W3142837074",
    "https://openalex.org/W3176171254",
    "https://openalex.org/W1677182931",
    "https://openalex.org/W2524608787",
    "https://openalex.org/W2606520630",
    "https://openalex.org/W6785652829",
    "https://openalex.org/W3042283600",
    "https://openalex.org/W3134189020",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W6638319203",
    "https://openalex.org/W3111272232",
    "https://openalex.org/W3187121874",
    "https://openalex.org/W2990371274",
    "https://openalex.org/W2944223741",
    "https://openalex.org/W3006847184",
    "https://openalex.org/W2798232928",
    "https://openalex.org/W2883554151",
    "https://openalex.org/W3048686909",
    "https://openalex.org/W2295107390",
    "https://openalex.org/W2751117407",
    "https://openalex.org/W3114896399",
    "https://openalex.org/W3188615767",
    "https://openalex.org/W3018757597",
    "https://openalex.org/W4310330865",
    "https://openalex.org/W3107169861",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W1797268635",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3034315787",
    "https://openalex.org/W2963045696",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W3106250896",
    "https://openalex.org/W3176774696",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3132890542",
    "https://openalex.org/W3110272085",
    "https://openalex.org/W3159833358",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3172752666",
    "https://openalex.org/W4287556569",
    "https://openalex.org/W2963795442",
    "https://openalex.org/W2964274719",
    "https://openalex.org/W3033210410",
    "https://openalex.org/W3121523901"
  ],
  "abstract": "Weakly supervised object localization (WSOL) aims to learn object localizer solely by using image-level labels. The convolution neural network (CNN) based techniques often result in highlighting the most discriminative part of objects while ignoring the entire object extent. Recently, the transformer architecture has been deployed to WSOL to capture the long-range feature dependencies with self-attention mechanism and multilayer perceptron structure. Nevertheless, transformers lack the locality inductive bias inherent to CNNs and therefore may deteriorate local feature details in WSOL. In this paper, we propose a novel framework built upon the transformer, termed LCTR (Local Continuity TRansformer), which targets at enhancing the local perception capability of global features among long-range feature dependencies. To this end, we propose a relational patch-attention module (RPAM), which considers cross-patch information on a global basis. We further design a cue digging module (CDM), which utilizes local features to guide the learning trend of the model for highlighting the weak local responses. Finally, comprehensive experiments are carried out on two widely used datasets, ie, CUB-200-2011 and ILSVRC, to verify the effectiveness of our method.",
  "full_text": "LCTR: On Awakening the Local Continuity of Transformer for\nWeakly Supervised Object Localization\nZhiwei Chen1, Changan Wang2, Yabiao Wang2, Guannan Jiang3,\nYunhang Shen2, Ying Tai2, Chengjie Wang2, Wei Zhang3, Liujuan Cao1*\n1Media Analytics and Computing Lab, Department of ArtiÔ¨Åcial Intelligence, School of Informatics,\nXiamen University, China. 2Tencent Youtu Lab, Shanghai, China.3CATL, China.\nzhiweichen.cn@gmail.com, {changanwang, caseywang}@tencent.com, jianggn@catl.com,\n{odysseyshen, yingtai, jasoncjwang}@tencent.com, zhangwei@catl.com, caoliujuan@xmu.edu.cn\nAbstract\nWeakly supervised object localization (WSOL) aims to learn\nobject localizer solely by using image-level labels. The con-\nvolution neural network (CNN) based techniques often result\nin highlighting the most discriminative part of objects while\nignoring the entire object extent. Recently, the transformer\narchitecture has been deployed to WSOL to capture the long-\nrange feature dependencies with self-attention mechanism\nand multilayer perceptron structure. Nevertheless, transform-\ners lack the locality inductive bias inherent to CNNs and\ntherefore may deteriorate local feature details in WSOL. In\nthis paper, we propose a novel framework built upon the\ntransformer, termed LCTR (Local Continuity TRansformer),\nwhich targets at enhancing the local perception capability\nof global features among long-range feature dependencies.\nTo this end, we propose a relational patch-attention mod-\nule (RPAM), which considers cross-patch information on a\nglobal basis. We further design a cue digging module (CDM),\nwhich utilizes local features to guide the learning trend of\nthe model for highlighting the weak local responses. Finally,\ncomprehensive experiments are carried out on two widely\nused datasets, i.e., CUB-200-2011 and ILSVRC, to verify the\neffectiveness of our method.\nIntroduction\nDeep learning based methods have achieved unprecedented\nsuccess in locating objects under a fully supervised set-\nting (Liu et al. 2016; Bochkovskiy, Wang, and Liao 2020;\nSun et al. 2021; Wang et al. 2021). However, these methods\nrely on a large number of bounding box annotations, which\nare expensive to acquire. Recently, the research on weakly\nsupervised object localization (WSOL) has gained a signiÔ¨Å-\ncant momentum (Zhou et al. 2016; Zhang et al. 2018a; Gao\net al. 2021) since it can learn object localizers using only\nimage-level labels.\nThe pioneering work (Zhou et al. 2016) aggregated fea-\ntures from classiÔ¨Åcation networks to generate class activa-\ntion maps (CAM) for object localization. Unfortunately, im-\nage classiÔ¨Åers tend to focus only on the most discriminative\nfeatures to achieve high classiÔ¨Åcation performance. There-\nfore, the spatial distribution of feature responses may only\n*Corresponding author.\nCopyright ¬© 2022, Association for the Advancement of ArtiÔ¨Åcial\nIntelligence (www.aaai.org). All rights reserved.\n(a) Image (b) CAM (c) TS -CAM (d) Ours(a) Image (b) CAM (c) TS-CAM (d) LCTR (Ours)\nFigure 1: Comparison of localization results on different\nmethods: (a) Original images. (b) CNN-based method tends\nto be dominated by the most discriminative region. (c)\nTransformer-based method maintains coarse long-range de-\npendencies while ignoring the local feature details (light yel-\nlow line). (d) The proposed LCTR not only considers Ô¨Åner\nlocal details but also retains global information. The pre-\ndicted bounding boxes are in red. Best viewed in color.\ncover the most discerning regions instead of the whole object\nrange, which limits localization accuracy with large mar-\ngins, as shown in Figure 1(b).\nTo address this critical problem, many CAM-based ap-\nproaches have been proposed, such as graph propaga-\ntion (Zhu et al. 2017), data augmentation (Kumar Singh and\nJae Lee 2017; Yun et al. 2019), adversarial erasing (Zhang\net al. 2018a; Choe and Shim 2019; Chen et al. 2021) and\nspatial relation activation (Xue et al. 2019; Zhang, Wei, and\nYang 2020; Guo et al. 2021). However, those approaches do\nalleviate the partial activation issue, but in a compromised\nmanner ‚Äî the essential philosophy behind it is Ô¨Årst obtain-\ning local features and then attempting to recover the non-\nsalient regions to get full object extent. In fact, the funda-\nmental root of this issue is determined by the intrinsic nature\nof convolution neural networks (CNNs). The CNN features\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n410\nwith the local receptive Ô¨Åeld only capture small-range fea-\nture dependencies.\nMore recently, the transformer architecture (Vaswani\net al. 2017) has been developed in the Ô¨Åeld of computer\nvision (Dosovitskiy et al. 2020; Wu et al. 2020; Yuan\net al. 2021; Touvron et al. 2021; Jiang, Chang, and Wang\n2021), which shows that pure transformers can be as ef-\nfective in feature extraction for image recognition as CNN-\nbased architectures. Notably, transformers with multi-head\nself-attention capture long-range dependencies, and retain\nmore detailed information without downsampling operators,\nwhich naturally handles the partial activation problem in\nWSOL. TS-CAM (Gao et al. 2021) proposed token seman-\ntic coupled attention map from transformer structure, which\ncaptured long-range feature dependency among pixels for\nWSOL. However, transformer-based methods lack the lo-\ncality inductive bias inherent to CNNs, ignoring the local\ninformation, which leads to weak local feature response on\nthe target object, as shown in Figure 1(c). Therefore, how\nto precisely mine local features in global representations for\nWSOL still remains an open problem.\nIn this paper, we propose a novel Local Continuity TRans-\nformer (LCTR) for discovering entire objects of interest\nvia end-to-end weakly supervised training. The key idea of\nLCTR is to rearrange local-continuous visual patterns with\nglobal-connective self-attention maps, thereby bringing lo-\ncality mechanism to transformer-based WSOL. To this end,\nwe Ô¨Årst propose a relational patch-attention module (RPAM)\nto construct a powerful patch relation map, which takes ad-\nvantage of the patch attention maps under the guidance of a\nglobal class-token attention map. The RPAM maintains the\ncross-patch information and models a global representation\nwith more local cues. Second, a cue digging module (CDM)\nis designed succinctly to induce the model to highlight the\nweak local features (e.g., blurred object boundaries) by a\nhide-and-seek manner under a local view. In the CDM, to\nreward the weak response parts, we propose to employ the\nerased strategy, and induce the learnable convolutional ker-\nnels to be weighted by the weak local features. To validate\nthe effectiveness of the proposed LCTR, we conduct a series\nof experiments on the challenging WSOL benchmarks.\nCollectively, our main contributions are summarized as:\n‚Ä¢ We propose a simple LCTR for WSOL, which greatly\nenhances the local perception capability of global self-\nattention maps among long-range feature dependencies.\n‚Ä¢ We design a relational patch-attention module (RPAM)\nby considering cross-patch information, which facilitates\nglobal representations.\n‚Ä¢ We introduce a cue digging module (CDM) that encodes\nweak local features by learnable kernels to highlight the\nlocal details of global representations.\n‚Ä¢ LCTR achieves new state-of-the-art performance on\nCUB-200-2011 and ILSVRC dataset with 79.2% and\n56.1% Top-1 localization accuracy, respectively.\nRelated Work\nCNN-based Methods for WSOL. WSOL aims to learn ob-\nject localizers with solely image-level supervision. There are\nmany state-of-the-art methods based on the CNN structure.\nA representative pipeline of CNN-based WSOL is to aggre-\ngate deep feature maps with a class-speciÔ¨Åc fully connected\nlayer to produce class attention maps (CAMs), from which\nÔ¨Ånal predicted bounding boxes are extracted (Zhou et al.\n2016). Later on, the last fully connected layer is dropped for\nsimplifying (Hwang and Kim 2016). Unfortunately, CAMs\ntend to be dominated by the most discriminative object part.\nTherefore, different extensions (Selvaraju et al. 2017; Chat-\ntopadhay et al. 2018; Xue et al. 2019; Zhang, Wei, and\nYang 2020) have been proposed to improve the generation\nprocess of localization maps in order to recover the non-\nsalient regions. HaS (Kumar Singh and Jae Lee 2017) and\nCutMix (Yun et al. 2019) adopted a random-erasing strat-\negy from input images to force the classiÔ¨Åcation networks\nto focus on relevant parts of objects. ACoL (Zhang et al.\n2018a) introduced two adversarial classiÔ¨Åcation classiÔ¨Åers\nto locate different object parts and discovered the comple-\nmentary regions belonging to the same objects or categories.\nADL (Choe and Shim 2019) further promoted the localiza-\ntion maps by applying dropout on multiple intermediate fea-\nture maps. Besides the erasing strategy, DANet (Xue et al.\n2019) used a divergent activation method to learn better lo-\ncalization maps. SPG (Zhang et al. 2018b) and I 2C (Zhang,\nWei, and Yang 2020) introduced the constraint of pixel-\nlevel correlations into the WSOL network. SPA (Pan et al.\n2021) leveraged structure information incorporated in con-\nvolutional features for WSOL. Some other methods (e.g.,\nGC-Net (Lu et al. 2020), PSOL (Zhang, Cao, and Wu 2020),\nSPOL (Wei et al. 2021) and SLT-Net (Guo et al. 2021))\ndivided WSOL into two independent sub-tasks, including\nclassiÔ¨Åcation and the class-agnostic localization.\nThese studies alleviate the problem by extending from lo-\ncal activations to global ones in an implicit way, which is\ndifÔ¨Åcult to balance the image classiÔ¨Åcation and the object lo-\ncalization. In fact, CNNs are prone to capture partial seman-\ntic features with local receptive Ô¨Åelds, which belongs to the\nprincipal problem of CNNs. The problem of how to explore\nglobal cues from local receptive Ô¨Åelds still exists. In this pa-\nper, we introduce a transformer-based structure, where the\nlocal-continuity and long-range feature dependencies can be\nsimultaneously activated.\nTransformer-based Methods for WSOL. The trans-\nformer model (Vaswani et al. 2017) is proposed to handle\nsequential data in the Ô¨Åeld of natural language processing.\nRecent studies also reveal its effectiveness for computer vi-\nsion tasks (Dosovitskiy et al. 2020; Beal et al. 2020; Carion\net al. 2020; Zheng et al. 2021; Hu et al. 2021). Since the\nlocal information extracted by the CNNs is deÔ¨Åcient, vari-\nous methods adopt the self-attention mechanism to capture\nthe long-range feature dependencies. ViT (Dosovitskiy et al.\n2020) applied the pure transformer directly to sequences of\nimage patches for exploring spatial correlation on the image\nclassiÔ¨Åcation task. DETR (Carion et al. 2020) employed a\ntransformer encoder-decoder architecture for the object de-\ntection task. As a pioneered work in WSOL, TS-CAM (Gao\net al. 2021) proposed a semantic coupling strategy based on\nDeit (Touvron et al. 2021) to fuse the patch tokens with the\nsemantic-agnostic attention map to achieve semantic-aware\n411\nùë¥ùë™ùë´ùë¥\nùíò!ùíâ\n‚®Ç\nInput\n‚Ä¶\nLinear Projection\nCls. Token\nPos.\nEmbedding\nùëµ+1\nùë´\nEmbedded Patches\nNorm\nMulti-Head\nAttention\nAdd\nNorm\nMLP\nAdd\nTransformer Encoder\nùë≥ !\nReshape\nùë∏\nùë≤\nùëΩ\nùíò!ùíâ ùë¥‚àó\nùíÑ\nReverse\n‚®Çconv\nfc\nfc\nGMPGAP\nAdd & Sigmoid\nùë™ ! ùíò ! ùíâ\nGAP\nClassifier\nùë¥‚àóùíì\nùíò!ùíâMean & Reshape\n‚®Ç: Element-wise multiplication\nGMP : Global Max Pooling\nGAP : Global Average Pooling\nconv\nRPAM\nCDM\nùë¥ùíáùíñùíîùíÜ\nùëÆ ! ùüè\nùëÆ ! ùüè\nùë≥ !\nImage\nùëøùë≥\nùëøùë™ùë´ùë¥\nùë¥ùíç\nùíâ\nùë®ùíç\n%\n‚®Ç\nùë¥ùíç\nùíë\nùëø)ùë≥ (Eq.6)\nùë¥ùíç\nùíÑ\nùëøùüè\nFigure 2: Overview of the proposed LCTR, which consists of vision transformer backbone for feature extraction, relational\npatch-attention module (RPAM) and cue digging module (CDM).\nlocalization results, which has inspired many scholars to\nstudy transformer in weakly supervised object localization.\nDespite of the progress, TS-CAM is relatively rough as\nit single-mindedly tries to get the long-range features but\nignores the local information. Compared with the existing\nmethods, our LCTR retains long-dependent features while\nmining for detailed feature cues based on the transformer\nstructure for WSOL.\nMethodology\nWe Ô¨Årst present the overview of the proposed LCTR, then\ngive a detailed description of RPAM and CDM, and Ô¨Ånally\nincorporate them with the transformer structure in a joint\noptimization framework, as shown in Figure 2.\nOverview\nIn accordance with the long-range info-preserving ability of\nthe transformer architecture, LCTR is designed to offer pre-\ncise localization maps for WSOL. We denote the input im-\nages as I = {(Ii;yi)}M‚àí1\ni=0 , where yi ‚àà{0;1;:::;C ‚àí1}\nindicates the label of the image Ii, M and Care the number\nof images and classes, respectively. We Ô¨Årst split Ii into N\nsame-sized patches xp ‚ààR1√óD, where D denotes the di-\nmension of each patch. We set N = w√óh, w = W=P and\nh= H=P, where P is the width/height of a patch,Hand W\ndenote image height and width. For simplicity, we omit the\nmini-batch dimension. A learnable class tokenxcls ‚ààR1√óD\nis embedded into the patches. These patches are Ô¨Çattened\nand linearly projected before being fed toLsequential trans-\nformer blocks, which can be formulated as:\nX1 = [xcls; F(x1\np); F(x2\np); ¬∑¬∑¬∑; F(xN\np )] + P; (1)\nwhere X1 denotes the input of the Ô¨Årst transformer block,\nP‚àà R(N+1)√óD is the position embedding and Fis a linear\nprojection. In particular, the proposed RPAM is employed in\neach transformer block to obtain a patch relation map Mr\n‚àó,\nwhich aggregates cross-patch information on a global basis.\nDenote XL ‚àà RN√óD as the output feature of the last\ntransformer block. We reshape XL ‚ààRD√ów√óh and apply\nthe proposed CDM for further highlighting weak local re-\nsponses. After that we obtain the feature map XCDM ‚àà\nRC√ów√óh. Finally, the XCDM are fed to a global average\npooling (GAP) layer (Lin, Chen, and Yan 2013) followed\nby a softmax layer to predict the classiÔ¨Åcation probability\np‚ààR1√óC. The loss function is deÔ¨Åned as\nL= ‚àílog p: (2)\nDuring testing, we extract the object map MCDM ‚àà\nRw√óh from XCDM according to the predicted class and ob-\ntain the Ô¨Ånal localization map by element-wise multiplica-\ntion, given as\nMfuse = MCDM ‚äóMr\n‚àó: (3)\nThe Mfuse is then resized to the same size as the original\nimages by linear interpolation. For a fair comparison, we\napply the same strategy detailed in CAM (Zhou et al. 2016)\nto produce the object bounding boxes.\nRelational Patch-Attention Module\nThe proposed relational patch-attention module (RPAM)\n(Figure 3) strengthens the global feature representation from\ntwo stages: First, we utilize the attention vectors of the class\ntoken in the transformer block to generate a global class-\ntoken attention map. To fully exploit the feature dependen-\ncies of the transformer structure, we then use all the attention\nvectors of the patches containing the correlation between lo-\ncal features to generate a patch relation map under the guid-\nance of the class-token attention map.\nIn the l-th transformer block, we hypothesize that the out-\nput feature map is Xl ‚ààR(N+1)√óD. The attention matrix\nAl ‚ààRS√ó(N+1)√ó(N+1) of multi-head self-attention module\nin the block is formulated as:\nAl = Softmax\n \nQl ¬∑K>\nlp\nD=S\n!\n; (4)\nwhere Ql and Kl denote the queries and keys projected by\nXl of self-attention operation in(l‚àí1)-th transformer block,\nrespectively. S represents the number of head and ‚ä§is a\ntranspose operator.\n412\nAt this point, we Ô¨Årst take the average operator to Al\nbased on S heads to obtain A‚Ä≤\nl ‚àà R(N+1)√ó(N+1). Then,\nthe class-token attention vector Mc\nl ‚àà R1√ó(N+1) is ex-\ntracted from A‚Ä≤\nl. The Mc\nl reveals how much each patch con-\ntributes to the object regions for image classiÔ¨Åcation. Un-\nfortunately, this map simply captures the global interactions\nof the class token to all patches, while ignoring the cross-\npatch correlations, which affects the modeling of local fea-\ntures. To remedy it, we take advantage of the patch attention\nmap Mp\nl ‚ààR(N+1)√óN in A‚Ä≤\nl to structure a patch relation\nvector Mr\nl under the guidance of Mc\nl . The Mp\nl learns the\ncorrelation between each patch but couldn‚Äôt tell which one\nis more important. Therefore, we weight each patch atten-\ntion map by multiplying Mp\nl by Mc\nl to obtain a new map\nMh\nl ‚ààR(N+1)√óN . Note that Mc\nl is reshaped ( R(N+1)√ó1)\nbefore the multiplication. After that, we squeeze the Ô¨Årst di-\nmension of Mh\nl to a vector ( Mr\nl ‚ààR1√óN ) by an average\noperation. The Ô¨Ånal patch relation map Mr\n‚àóis calculated by\nMr\n‚àó= Œìw√óh( 1\nL\nX\nl\nMr\nl ); (5)\nwhere Œìw√óh(¬∑) indicates the reshape operator which coverts\nthe vector (R1√óN ) to the map (Rw√óh).\nThe patch relation map Mr\n‚àóobtains the long-range de-\npendencies that depends on the class-token attention vector\nMc\nl . Aggregating cross-patch information from the patch at-\ntention maps, Mr\n‚àófacilitates better global representations of\nthe object without extra parameters in a simple way.\nCue Digging Module\nRPAM considers the cross-patch information by using the\nclass-token attention map from self-attention mechanism\nblock of the transformer structure, but it is vulnerable if the\ntransformer gets a poor class-token attention map. We thus\nfurther propose a cue digging module (CDM) to supply the\nlong-range features based on a hide-and-seek manner.\nInspired by erasing-based methods that remove the most\ndiscriminative parts of the target object to induce the model\nto cover the integral extent of the object, we erase the ob-\nject regions based on the global class-token attention map,\nleaving the weak response ones and the background. Then\nby weighting the learnable convolution kernels in the CDM\non the basis of them, we shift part of the attention to object\nregions with weak responses. With the help of weighted ker-\nnels, we can highlight the local details as a supplement to\nthe global representations.\nSpeciÔ¨Åcally, we convert the patch parts of class-token at-\ntention vectors to the map ÀúMc\n‚àó‚ààRw√óh, and apply it to the\nfeature map XL by spatial-wise multiplication after being\nreversed. Note that ÀúMc\n‚àóis calculated by ÀúMc\n‚àó= 1\nL\nP\nlMc\nl .\nThe feature map then passes through a convolutional layer\nto generate a new feature map ÀúXL ‚ààRD√ów√óh. Next, we\nscore the features into G parts corresponding to G learn-\nable convolution kernels. In particular, we apply two sep-\narate operators, the global average pooling and max pool-\ning, to ÀúXL. Then the feature maps are vectorized and sent to\na fully connected layer, respectively. Besides, we add them\n0.00 ‚àó\n ‚àó 0.67\n0.23 ‚àó\n0.36 ‚àó\n‚Ä¶‚Ä¶\n‚Ä¶‚Ä¶\n‚àó 0.00\n‚àó 0.00\n0.00\n0.00\n0.36\n‚Ä¶‚Ä¶\n0.67\n0.23\n0.00\nùë¥‚àó\nùíì\nFigure 3: The RPAM aggregates all patch attention maps\nbased on the scores (values) of the class-token attention map\nto learn local visual patterns.\ntogether and apply a sigmoid function. In this light, we ob-\ntain the scores as {Sg |g = 1 ;2;:::;G }. Finally, XL\npasses through a convolutional layer with weighted convo-\nlution kernels by the scores Sg for encouraging the model to\nlearn the object regions with weak responses, which can be\nformulated as\nXSGR = XL\nGX\ng=1\nSgWe\ng ; (6)\nwhere We\ng ‚ààRD√óC√ókw√ókh denotes the kernel weights of\nthe convolutional layer, which are initialized with kaiming\nuniform initialization (He et al. 2015). kw, kh represent the\nwidth and height of the kernels, respectively.\nThe convolution layer with the weighted kernels is ap-\nplied to the global feature map XL drawn from the trans-\nformer structure. Once the loss Lin Eq. 2 is optimized, the\nweighted convolution kernels become more sensitive to the\nfeatures (i.e., the weak response features of object regions)\nthat favor image classiÔ¨Åcation. In this way, the model pays\nmore attention to local cues and forms better global repre-\nsentations for the target object.\nExperiments\nExperimental Settings\nDatasets. We evaluate the proposed methods on two chal-\nlenging datasets, including CUB-200-2011 (Wah et al. 2011)\nand ILSVRC (Russakovsky et al. 2015). We only use image-\nlevel labels for training. CUB-200-2011 is a Ô¨Åne-grained\nbird dataset of 200 categories, which contains 5;994 images\nfor training and 5;794 for testing. ILSVRC has about 1:2\nmillion images in the training set and 50;000 images in the\nvalidation set, with a total of 1;000 different categories.\nEvaluation Metrics. Following previous methods (Zhou\net al. 2016; Russakovsky et al. 2015), we adopt the Top-\n1/Top-5 classiÔ¨Åcation accuracy (Top-1/Top-5 Cls.), Top-\n1/Top-5 localization accuracy (Top-1/Top-5Loc.) and local-\nization accuracy with known ground-truth class ( Gt-k.) as\nour evaluation metrics. SpeciÔ¨Åcally, Top-1/Top-5Cls. is cor-\nrect if the Top-1/Top-5 predicted category contains the cor-\nrect label. Gt-k. is correct when the intersection over union\n(IoU) between the ground-truth and the prediction is larger\n413\nMethods (Yr) Backbone Loc. Acc\nTop-1 Top-5 Gt-k.\nCAM (‚Äô16) GoogLeNet 41.1 50.7 55.1\nSPG (‚Äô18) GoogLeNet 46.7 57.2 -\nRCAM (‚Äô20) GoogLeNet 53.0 - 70.0\nDANet (‚Äô19) InceptionV3 49.5 60.5 67.0\nADL (‚Äô19) InceptionV3 53.0 - -\nPSOL (‚Äô20) InceptionV3 65.5 - -\nSPA (‚Äô21) InceptionV3 53.6 66.5 72.1\nSLT-Net(‚Äô21) InceptionV3 66.1 - 86.5\nCAM (‚Äô16) VGG16 44.2 52.2 56.0\nADL (‚Äô19) VGG16 52.4 - 75.4\nACoL (‚Äô18) VGG16 45.9 56.5 59.3\nSPG (‚Äô18) VGG16 48.9 57.2 58.9\nDANet (‚Äô19) VGG16 52.5 62.0 67.7\nMEIL (‚Äô20) VGG16 57.5 - 73.8\nPSOL (‚Äô20) VGG16 66.3 - -\nRCAM (‚Äô20) VGG16 59.0 - 76.3\nGC-Net (‚Äô20) VGG16 63.2 - -\nSPA (‚Äô21) VGG16 60.2 72.5 77.2\nSLT-Net(‚Äô21) VGG16 67.8 - 87.6\nTS-CAM (‚Äô21) Deit-S 71.3 83.8 87.7\nLCTR (Ours) Deit-S 79.2 89.9 92.4\nTable 1: Localization accuracy on the CUB-200-2011 test\nset.\nthan 0:5, and does not consider whether the predicted cate-\ngory is correct. Top-1/Top-5Loc. is correct when Top-1/Top-\n5 Cls. and Gt-k. are both correct.\nImplementation Details. We adopt the Deit (Touvron\net al. 2021) as the backbone network, which is pre-trained on\nILSVRC (Russakovsky et al. 2015). Particularly, we replace\nthe MLP head with our proposed CDM. Finally, a GAP layer\nand a softmax layer are added on the top of the convolutional\nlayers. The input images are randomly cropped to224√ó224\npixels after being resized to 256 √ó256 pixels. We adopt\nAdamW (Loshchilov and Hutter 2017) with\u000f=1e-8, \f1=0:9,\n\f2=0:99 and weight decay of 5e-4. On CUB-200-2011, we\nuse a batch size of 128 with a learning rate of 5e-5 to train\nthe model for 80 epochs. For ILSVRC, the training process\nlasts 14 epochs with a batch size of 256 and a learning rate\nof 5e-4. After meticulous experiments, we set G=4 in the\nCDM. All the experiments are performed with four Nvidia\nTesla V100 GPUs using the PyTorch toolbox.\nComparison with the State-of-the-Arts\nLocalization. We Ô¨Årst compare the proposed LCTR with the\nSOTAs on the localization accuracy on the CUB-200-2011\ntest set, as illustrated in Table 1. We observe that LCTR out-\nperforms the baseline (i.e., TS-CAM (Gao et al. 2021)) by\n7.9% in terms of Top-1 Loc., and is obviously superior to\nthese CNN-based methods. Besides, Table 2 illustrates the\nlocalization accuracy on the ILSVRC validation set. It re-\nports 0.4% performance improvement over the state-of-the-\nart SLT-Net (Guo et al. 2021).\nClassiÔ¨Åcation. Table 3 and Table 4 show the Top-1 and\nMethods (Yr) Backbone Loc. Acc\nTop-1 Top-5 Gt-k.\nCAM (‚Äô16) VGG16 38.9 48.5 -\nACoL (‚Äô18) VGG16 45.8 59.4 63.0\nCutMix (‚Äô19) VGG16 42.8 54.9 59.0\nADL (‚Äô19) VGG16 44.9 - -\nI2C (‚Äô20) VGG16 47.4 58.5 63.9\nMEIL (‚Äô20) VGG16 46.8 - -\nRCAM (‚Äô20) VGG16 44.6 - 60.7\nPSOL (‚Äô20) VGG16 50.9 60.9 64.0\nSPA (‚Äô21) VGG16 49.6 61.3 65.1\nSLT-Net(‚Äô21) VGG16 51.2 62.4 67.2\nCAM (‚Äô16) InceptionV3 46.3 58.2 62.7\nSPG (‚Äô18) InceptionV3 48.6 60.0 64.7\nADL (‚Äô19) InceptionV3 48.7 - -\nACoL (‚Äô18) GoogLeNet 46.7 57.4 -\nDANet (‚Äô19) GoogLeNet 47.5 58.3 -\nRCAM (‚Äô20) GoogLeNet 50.6 - 64.4\nMEIL (‚Äô20) InceptionV3 49.5 - -\nI2C (‚Äô20) InceptionV3 53.1 64.1 68.5\nGC-Net (‚Äô20) InceptionV3 49.1 58.1 -\nPSOL (‚Äô20) InceptionV3 54.8 63.3 65.2\nSPA (‚Äô21) InceptionV3 52.8 64.3 68.4\nSLT-Net(‚Äô21) InceptionV3 55.7 65.4 67.6\nTS-CAM (‚Äô21) Deit-S 53.4 64.3 67.6\nLCTR (Ours) Deit-S 56.1 65.8 68.7\nTable 2: Localization accuracy on the ILSVRC validation\nset.\nMethods (Yr) Backbone Cls. Acc\nTop-1 Top-5\nCAM (‚Äô16) GoogLeNet 73.8 91.5\nRCAM (‚Äô20) GoogLeNet 73.7 -\nDANet (‚Äô19) InceptionV3 71.2 90.6\nADL (‚Äô19) InceptionV3 74.6 -\nSLT-Net(‚Äô21) InceptionV3 76.4 -\nCAM (‚Äô16) VGG16 76.6 92.5\nACoL (‚Äô18) VGG16 71.9 -\nADL (‚Äô19) VGG16 65.3 -\nDANet (‚Äô19) VGG16 75.4 92.3\nSPG (‚Äô18) VGG16 75.5 92.1\nMEIL (‚Äô20) VGG16 74.8 -\nRCAM (‚Äô20) VGG16 75.0 -\nSLT-Net(‚Äô21) VGG16 76.6 -\nTS-CAM (‚Äô21) Deit-S 80.3 94.8\nLCTR (Ours) Deit-S 85.0 97.1\nTable 3: ClassiÔ¨Åcation accuracy on the CUB-200-2011 test\nset.\nTop-5 classiÔ¨Åcation accuracy on the CUB-200-2011 test\nset and ILSVRC validation set, respectively. For the Ô¨Åne-\ngrained recognition dataset CUB-200-2011, LCTR achieves\nremarkable performance of 85.0%/97.1% on Top1/Top-5\nAcc.. In addition, LCTR obtains comparable results with\nSLT-Net (Guo et al. 2021) on Top-1Acc. and surpasses other\n414\nImage CAM TS-CAM LCTR (Ours)\nCUB-200-2011\nImage CAM TS-CAM\nILSVRC\nLCTR (Ours)\nYellow_Throated_Vireo\nBaltimore_Oriole\nBank_Swallow\nBlack_Capped_Vireo\nCape_May_Warbler\nJay\nAmerican_Chameleon\nBox\t_Turtle\nColobus\nBroom\nFigure 4: Visual comparisons of localization results on different methods. 1st Column: Input images. 2nd Column: Results of\nCAM based on CNNs. 3rd Column: Results of TS-CAM based on Transformer. 4th Column: Results of our LCTR. Note that\nthe groundtruth bounding boxes are in red, the predictions are in green, and the IoU values (%) are shown in white text.\nmethods signiÔ¨Åcantly on the ILSVRC validation set. Note\nthat SLT-Net used a separated localization-classiÔ¨Åcation\nframework, it cannot retain the global information for the\nobjects in the individual classiÔ¨Åcation network. To sum up,\nthe proposed LCTR can greatly improve the quality of object\nlocalization while keeping high classiÔ¨Åcation performance.\nVisualization. For qualitative evaluation, Figure 4 visual-\nizes the Ô¨Ånal localization results of CAM (Zhou et al. 2016)\nbased on the CNNs, TS-CAM (Gao et al. 2021) based on the\ntransformer and our method on CUB200-2011 and ILSVRC\ndatasets. From the results, compared with the CAM, we con-\nsistently observe that our method can cover a more complete\nrange of object regions instead of focusing only on the most\ndiscriminative ones. In addition, we capture more localized\ncues than the TS-CAM method, resulting in more accurate\nlocalization. For example, the tail regions of the Bank Swal-\nlow and the Colobus are ignored by CAM and TS-CAM\nmethods, while our LCTR is able to aggregate more detailed\nfeatures of the target object, which enhances the local per-\nception capability of global features among long-range fea-\nture dependencies. Please refer to the supplementary mate-\nrials for more visualized localization results of our method.\nAblation Studies\nFirst, we visualize the localization maps with different set-\ntings in Figure 5. We observe that the RPAM strengthens\nMethods (Yr) Backbone Cls. Acc\nTop-1 Top-5\nCAM (‚Äô16) VGG16 68.8 88.6\nACoL (‚Äô18) VGG16 67.5 88.0\nI2C (‚Äô20) VGG16 69.4 89.3\nMEIL (‚Äô20) VGG16 70.3 -\nRCAM (‚Äô20) VGG16 68.7 -\nSLT-Net(‚Äô21) VGG16 72.4 -\nCAM (‚Äô16) InceptionV3 73.3 91.8\nSPG (‚Äô18) InceptionV3 69.7 90.1\nADL (‚Äô19) InceptionV3 72.8 -\nACoL (‚Äô18) GoogLeNet 71.0 88.2\nDANet (‚Äô19) GoogLeNet 63.5 91.4\nRCAM (‚Äô20) GoogLeNet 74.3 -\nMEIL (‚Äô20) InceptionV3 73.3 -\nI2C (‚Äô20) InceptionV3 73.3 91.6\nSLT-Net(‚Äô21) InceptionV3 78.1 -\nTS-CAM (‚Äô21) Deit-S 74.3 92.1\nLCTR (Ours) Deit-S 77.1 93.4\nTable 4: ClassiÔ¨Åcation accuracy on the ILSVRC validation\nset.\nthe global representations of the baseline (Gao et al. 2021),\ne.g., the tail-feature response of theAfrican chameleonis en-\n415\nAfrican_Chameleon\n(a) Image (b) Baseline (c) RPAM (d) CDM (e) RPAM+CDM\nBlack_Grouse\nFigure 5: Visualization of localization map with different\nsettings. (a) Input images. (b) The baseline obtains coarse\nlong-range dependencies. (c) The global representations are\nfacilitated when applying the RPAM. (d) The local cues are\nrewarded with the CDM. (e) The global perception capabil-\nity of the target object is fully exploited.\nApplied Mode Top-1 Loc. Gt-k. Top-1 Cls.\nGMP-fc 75.6 88.7 84.7\nGAP-fc 75.8 88.8 84.8\n(GMP+GAP)-fc 75.8 89.2 84.9\n(GMP-fc) + (GAP-fc) 76.0 90.0 85.0\nTable 5: The effect of different type of classiÔ¨Åer in the CMD\non CUB-200-2011 test set. GMP/GAP denotes the global\nmax/average pooling. fc is the fully connected layer.\nG Top-1 Loc. Gt-k. Top-1 Cls.\n2 75.2 88.7 84.7\n4 76.0 90.0 85.0\n8 74.3 87.8 84.4\n16 74.1 88.6 83.2\n32 74.0 88.4 82.9\nTable 6: The impact of the parameter G in the CDM on\nCUB-200-2011 test set.\nKernel Size (kw √ókh) Top-1 Loc. Gt-k. Top-1 Cls.\n1 √ó1 73.5 88.8 82.5\n3 √ó3 76.0 90.0 85.0\nTable 7: The impact of different kernel size in the CDM on\nCUB-200-2011 test set.\nhanced, as it considers more cross-patch information. When\nonly using CDM, we Ô¨Ånd that the local feature details are\nmined. For example, the abdominal features of the Black\ngrouse are further activated compared to the baseline. By\napplying both RPAM and CDM, the Ô¨Ånal localization map\n(Figure 5 (e)) highlights the full object extent.\nNext, we explore the concise design of the CDM. From\nthe results on Table 5, we can observe that the mode of us-\ning separate fcs with GAP and GMP reports the best perfor-\nmance. These results also verify that GAP and GMP work\ndifferently in the CDM. Then, we evaluate the accuracy un-\nder different parameters Gin the CDM, as shown in Table 6.\nMethods Dataset RPAM CDM Top-1 Top-1\nLoc. Cls.\nTS-CAM CUB 71.3 80.3\nTS-CAM* 73.1 81.6\nLCTR CUB\nX 74.0 81.6\nX 76.0 85.0\nX X 79.2 85.0\nTS-CAM ILSVRC 53.4 74.3\nTS-CAM* 53.0 74.0\nLCTR ILSVRC\nX 54.2 74.0\nX 55.1 77.1\nX X 56.1 77.1\nTable 8: Performance on both CUB-200-2011 test set and\nILSVRC validation set when using different conÔ¨Ågurations.\nNote that * indicates the re-implement method.\nFrom the experimental results, we observe that the best per-\nformance is achieved when G= 4. Setting a larger Gleads\nto a larger number of parameters and degrades accuracy,\nwhich we believe is caused by overÔ¨Åtting. Besides, we ex-\namine the impact of the weighted kernel size (i.e.,kw √ókh).\nResults shown in Table 7 indicate that a kernel size of3 √ó3\nyields better performance.\nLastly, we investigate the effect with different conÔ¨Ågura-\ntions on the accuracy, as reported in Table 8. On the CUB-\n200-2011 test set, we can see that RPAM increases the Top-1\nLoc. by 0.9% compared with the baseline TS-CAM method.\nNote that the lightweight RPAM is directly applied in the test\nphase, so the classiÔ¨Åcation performance remains unchanged.\nWhen applying the CDM to the network, we observe an\nimprovement in both classiÔ¨Åcation and localization perfor-\nmance. From this, we believe that the local cues captured\nby the CDM are important for both two tasks. The best lo-\ncalization/classiÔ¨Åcation accuracy can be achieved when em-\nploying both RPAM and CDM. Meanwhile, we conduct the\nsimilar experiments on the ILSVRC validation set, which\nalso validate the effectiveness of two modules, as shown in\nthe lower part of Table 8.\nConclusion\nIn this paper, we propose a novel Local Continuity TRans-\nformer, termed LCTR, for weakly supervised object local-\nization, which induces the model to learn the entire extent of\nthe object with more local cues. We Ô¨Årst design a relational\npatch-attention module (RPAM), considering cross-patch in-\nformation based on the multi-head self-attention mecha-\nnism, which gathers more local patch features for facili-\ntating the global representations. Moreover, we introduce a\ncue digging module (CDM), which employs a hide-and-seek\nmanner to wake up the weak local features for enhancing\nglobal representation learning. Extensive experiments show\nthe LCTR can successfully mine integral object regions and\noutperform the state-of-the-art localization methods.\n416\nAcknowledgments\nThis work is supported by the National Science Fund\nfor Distinguished Young Scholars (No.62025603), the Na-\ntional Natural Science Foundation of China (No.U1705262,\nNo.62072386, No.62072387, No.62072389, No.62002305,\nNo.61772443, No.61802324 and No.61702136), Guang-\ndong Basic and Applied Basic Research Foundation\n(No.2019B1515120049), the Natural Science Founda-\ntion of Fujian Province of China (No.2021J01002),\nand the Fundamental Research Funds for the cen-\ntral universities (No.20720200077, No.20720200090 and\nNo.20720200091).\nReferences\nBeal, J.; Kim, E.; Tzeng, E.; Park, D. H.; Zhai, A.; and\nKislyuk, D. 2020. Toward transformer-based object detec-\ntion. arXiv.\nBochkovskiy, A.; Wang, C.-Y .; and Liao, H.-Y . M. 2020.\nYolov4: Optimal speed and accuracy of object detection.\narXiv.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In ECCV.\nChattopadhay, A.; Sarkar, A.; Howlader, P.; and Balasub-\nramanian, V . N. 2018. Grad-cam++: Generalized gradient-\nbased visual explanations for deep convolutional networks.\nIn WACV.\nChen, Z.; Cao, L.; Shen, Y .; Lian, F.; Wu, Y .; and Ji, R. 2021.\nE2Net: Excitative-expansile learning for weakly supervised\nobject Localization. In ACMMM.\nChoe, J.; and Shim, H. 2019. Attention-based dropout layer\nfor weakly supervised object localization. In CVPR.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv.\nGao, W.; Wan, F.; Pan, X.; Peng, Z.; Tian, Q.; Han, Z.;\nZhou, B.; and Ye, Q. 2021. TS-CAM: Token semantic cou-\npled attention map for weakly supervised object localiza-\ntion. ICCV.\nGuo, G.; Han, J.; Wan, F.; and Zhang, D. 2021. Strengthen\nlearning tolerance for weakly supervised object localization.\nIn CVPR.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2015. Delving deep\ninto rectiÔ¨Åers: Surpassing human-level performance on ima-\ngenet classiÔ¨Åcation. In ICCV.\nHu, J.; Cao, L.; Lu, Y .; Zhang, S.; Wang, Y .; Li, K.; Huang,\nF.; Shao, L.; and Ji, R. 2021. ISTR: End-to-end instance\nsegmentation with transformers. arXiv.\nHwang, S.; and Kim, H.-E. 2016. Self-transfer learning for\nweakly supervised lesion localization. In MICCAI.\nJiang, Y .; Chang, S.; and Wang, Z. 2021. Transgan: Two\ntransformers can make one strong gan. arXiv.\nKumar Singh, K.; and Jae Lee, Y . 2017. Hide-and-seek:\nForcing a network to be meticulous for weakly-supervised\nobject and action localization. In ICCV.\nLin, M.; Chen, Q.; and Yan, S. 2013. Network in network.\narXiv.\nLiu, W.; Anguelov, D.; Erhan, D.; Szegedy, C.; Reed, S.;\nFu, C.-Y .; and Berg, A. C. 2016. Ssd: Single shot multibox\ndetector. In ECCV.\nLoshchilov, I.; and Hutter, F. 2017. Decoupled weight decay\nregularization. arXiv.\nLu, W.; Jia, X.; Xie, W.; Shen, L.; Zhou, Y .; and Duan, J.\n2020. Geometry constrained weakly supervised object lo-\ncalization. In ECCV.\nPan, X.; Gao, Y .; Lin, Z.; Tang, F.; Dong, W.; Yuan, H.;\nHuang, F.; and Xu, C. 2021. Unveiling the potential of struc-\nture preserving for weakly supervised object localization. In\nCVPR.\nRussakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.;\nMa, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.;\net al. 2015. Imagenet large scale visual recognition chal-\nlenge. In IJCV.\nSelvaraju, R. R.; Cogswell, M.; Das, A.; Vedantam, R.;\nParikh, D.; and Batra, D. 2017. Grad-cam: Visual expla-\nnations from deep networks via gradient-based localization.\nIn CVPR.\nSun, P.; Zhang, R.; Jiang, Y .; Kong, T.; Xu, C.; Zhan, W.;\nTomizuka, M.; Li, L.; Yuan, Z.; Wang, C.; et al. 2021. Sparse\nr-cnn: End-to-end object detection with learnable proposals.\nIn CVPR.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J¬¥egou, H. 2021. Training data-efÔ¨Åcient image trans-\nformers & distillation through attention. In ICML.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, ≈Å.; and Polosukhin, I. 2017. At-\ntention is all you need. In NIPS.\nWah, C.; Branson, S.; Welinder, P.; Perona, P.; and Belongie,\nS. 2011. The caltech-ucsd birds-200-2011 dataset. Compu-\ntation & Neural Systems Technical Report.\nWang, J.; Song, L.; Li, Z.; Sun, H.; Sun, J.; and Zheng, N.\n2021. End-to-end object detection with fully convolutional\nnetwork. In CVPR.\nWei, J.; Wang, Q.; Li, Z.; Wang, S.; Zhou, S. K.; and Cui, S.\n2021. Shallow feature matters for weakly supervised object\nlocalization. In CVPR.\nWu, B.; Xu, C.; Dai, X.; Wan, A.; Zhang, P.; Yan, Z.;\nTomizuka, M.; Gonzalez, J.; Keutzer, K.; and Vajda, P. 2020.\nVisual transformers: Token-based image representation and\nprocessing for computer vision. arXiv.\nXue, H.; Liu, C.; Wan, F.; Jiao, J.; Ji, X.; and Ye, Q. 2019.\nDanet: Divergent activation for weakly supervised object lo-\ncalization. In ICCV.\nYuan, L.; Chen, Y .; Wang, T.; Yu, W.; Shi, Y .; Jiang, Z.; Tay,\nF. E.; Feng, J.; and Yan, S. 2021. Tokens-to-token vit: Train-\ning vision transformers from scratch on imagenet. arXiv.\nYun, S.; Han, D.; Oh, S. J.; Chun, S.; Choe, J.; and Yoo, Y .\n2019. Cutmix: Regularization strategy to train strong classi-\nÔ¨Åers with localizable features. In ICCV.\n417\nZhang, C.-L.; Cao, Y .-H.; and Wu, J. 2020. Rethinking\nthe route towards weakly supervised object localization. In\nCVPR.\nZhang, X.; Wei, Y .; Feng, J.; Yang, Y .; and Huang, T. S.\n2018a. Adversarial complementary learning for weakly su-\npervised object localization. In CVPR.\nZhang, X.; Wei, Y .; Kang, G.; Yang, Y .; and Huang, T.\n2018b. Self-produced guidance for weakly-supervised ob-\nject localization. In ECCV.\nZhang, X.; Wei, Y .; and Yang, Y . 2020. Inter-image commu-\nnication for weakly supervised localization. In ECCV.\nZheng, S.; Lu, J.; Zhao, H.; Zhu, X.; Luo, Z.; Wang, Y .; Fu,\nY .; Feng, J.; Xiang, T.; Torr, P. H.; et al. 2021. Rethinking se-\nmantic segmentation from a sequence-to-sequence perspec-\ntive with transformers. In CVPR.\nZhou, B.; Khosla, A.; Lapedriza, A.; Oliva, A.; and Torralba,\nA. 2016. Learning deep features for discriminative localiza-\ntion. In CVPR.\nZhu, Y .; Zhou, Y .; Ye, Q.; Qiu, Q.; and Jiao, J. 2017. Soft\nproposal networks for weakly supervised object localization.\nIn ICCV.\n418",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7348549365997314
    },
    {
      "name": "Discriminative model",
      "score": 0.6894240379333496
    },
    {
      "name": "Locality",
      "score": 0.6441378593444824
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6345488429069519
    },
    {
      "name": "Transformer",
      "score": 0.6342325210571289
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5047820806503296
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4920944571495056
    },
    {
      "name": "Machine learning",
      "score": 0.36408329010009766
    },
    {
      "name": "Engineering",
      "score": 0.09826681017875671
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I191208505",
      "name": "Xiamen University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I2250653659",
      "name": "Tencent (China)",
      "country": "CN"
    }
  ]
}