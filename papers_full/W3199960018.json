{
  "title": "KroneckerBERT: Learning Kronecker Decomposition for Pre-trained Language Models via Knowledge Distillation",
  "url": "https://openalex.org/W3199960018",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4286256286",
      "name": "Tahaei, Marzieh S.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287077391",
      "name": "Charlaix, Ella",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4228029104",
      "name": "Nia, Vahid Partovi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2754134202",
      "name": "Ghodsi, Ali",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224409695",
      "name": "Rezagholizadeh, Mehdi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1724438581",
    "https://openalex.org/W3016263386",
    "https://openalex.org/W3002071578",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2976132230",
    "https://openalex.org/W2964299589",
    "https://openalex.org/W2754084392",
    "https://openalex.org/W2993845287",
    "https://openalex.org/W2073389244",
    "https://openalex.org/W3034408878",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W3099413717",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W3094085917",
    "https://openalex.org/W3101066076",
    "https://openalex.org/W2970454332",
    "https://openalex.org/W2294370754",
    "https://openalex.org/W1530239534",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W3156869386",
    "https://openalex.org/W3098873988",
    "https://openalex.org/W2932893307",
    "https://openalex.org/W2919207648",
    "https://openalex.org/W2964147297",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W3005444338",
    "https://openalex.org/W2963703075"
  ],
  "abstract": "The development of over-parameterized pre-trained language models has made a significant contribution toward the success of natural language processing. While over-parameterization of these models is the key to their generalization power, it makes them unsuitable for deployment on low-capacity devices. We push the limits of state-of-the-art Transformer-based pre-trained language model compression using Kronecker decomposition. We use this decomposition for compression of the embedding layer, all linear mappings in the multi-head attention, and the feed-forward network modules in the Transformer layer. We perform intermediate-layer knowledge distillation using the uncompressed model as the teacher to improve the performance of the compressed model. We present our KroneckerBERT, a compressed version of the BERT_BASE model obtained using this framework. We evaluate the performance of KroneckerBERT on well-known NLP benchmarks and show that for a high compression factor of 19 (5% of the size of the BERT_BASE model), our KroneckerBERT outperforms state-of-the-art compression methods on the GLUE. Our experiments indicate that the proposed model has promising out-of-distribution robustness and is superior to the state-of-the-art compression methods on SQuAD.",
  "full_text": "KroneckerBERT: Learning Kronecker Decomposition for Pre-trained\nLanguage Models via Knowledge Distillation\nMarzieh S. Tahaei\nNoah’s Ark Lab,\nHuawei Technologies Canada\nmarzieh.tahaei@huawei.com\nElla Charlaix\nNoah’s Ark Lab,\nHuawei Technologies Canada\ncharlaixe@gmail.com\nVahid Partovi Nia\nNoah’s Ark Lab\nHuawei Technologies Canada\nvahid.partovinia@huawei.com\nAli Ghodsi\nDepartment of Statistics and\nActuarial Science, University of Waterloo\nali.ghodsi@uwaterloo.com\nMehdi Rezagholizadeh\nNoah’s Ark Lab,\nHuawei Technologies Canada\nmehdi.rezagholizadeh@huawei.com\nAbstract\nThe development of over-parameterized pre-\ntrained language models has made a sig-\nniﬁcant contribution toward the success of\nnatural language processing. While over-\nparameterization of these models is the key to\ntheir generalization power, it makes them un-\nsuitable for deployment on low-capacity de-\nvices. We push the limits of state-of-the-\nart Transformer-based pre-trained language\nmodel compression using Kronecker decom-\nposition. We use this decomposition for com-\npression of the embedding layer, all linear\nmappings in the multi-head attention, and\nthe feed-forward network modules in the\nTransformer layer. We perform intermediate-\nlayer knowledge distillation using the uncom-\npressed model as the teacher to improve the\nperformance of the compressed model. We\npresent our KroneckerBERT, a compressed\nversion of the BERT BASE model obtained us-\ning this framework. We evaluate the per-\nformance of KroneckerBERT on well-known\nNLP benchmarks and show that for a high\ncompression factor of 19 (5% of the size of the\nBERTBASE model), our KroneckerBERT out-\nperforms state-of-the-art compression meth-\nods on the GLUE. Our experiments indicate\nthat the proposed model has promising out-\nof-distribution robustness and is superior to\nthe state-of-the-art compression methods on\nSQuAD.\n1 Introduction\nIn recent years, the emergence of Pre-trained Lan-\nguage Models(PLMs) has led to a signiﬁcant break-\nthrough in Natural Language Processing (NLP).\nThe introduction of Transformers and unsupervised\npre-training on enormous unlabeled data are the\ntwo main factors that contribute to this success.\nTransformer-based models (Devlin et al., 2018;\nRadford et al., 2019; Yang et al., 2019; Shoeybi\net al., 2019) are powerful yet highly over-\nparameterized. The enormous size of these models\ndoes not meet the constraints imposed by edge de-\nvices on memory, latency, and energy consumption.\nTherefore there has been a growing interest in de-\nveloping new methodologies and frameworks for\nthe compression of these large PLMs. Similar to\nother deep learning models, the main directions\nfor the compression of these models include low-\nbit quantization (Gong et al., 2014; Prato et al.,\n2019), network pruning (Han et al., 2015), matrix\ndecomposition (Yu et al., 2017; Lioutas et al., 2020)\nand Knowledge distillation (Hinton et al., 2015).\nThese methods are either used in isolation or in\ncombination to improve compression-performance\ntrade-off.\nRecent works (Sanh et al., 2019; Sun et al.,\n2019; Jiao et al., 2019; Sun et al., 2020b; Xu et al.,\n2020) have been quite successful in compressing\nTransformer-based PLMs to a certain degree; how-\never, extreme compression of these model (com-\npression factors >10) is still quite challenging. Sev-\narXiv:2109.06243v1  [cs.CL]  13 Sep 2021\neral works (Mao et al., 2020; Zhao et al., 2019,\n2021) that have tried to go beyond the compression\nfactor of 10, have done so at the expense of a sig-\nniﬁcant drop in performance. This work proposes\na novel framework that uses Kronecker decomposi-\ntion for extreme compression of Transformer-based\nPLMs and provides a very promising compression-\nperformance trade-off. Similar to other decompo-\nsition methods, Kronecker decomposition can be\nused to represent weight matrices in NNs to re-\nduce the model size and the computation overhead.\nUnlike the well-known SVD decomposition, Kro-\nnecker decomposition retains the rank of the matrix\nand hence provides a different expressiveness com-\npared to SVD.\nWe use Kronecker decomposition for the com-\npression of both Transformer layers and the em-\nbedding layer. For Transformer layers, the com-\npression is achieved by representing every weight\nmatrix both in the multi-head attention (MHA) and\nthe feed-forward neural network (FFN) as a Kro-\nnecker product of two smaller matrices. We also\npropose a Kronecker decomposition for compres-\nsion of the embedding layer. Previous works have\ntried different techniques to reduce the enormous\nmemory consumption of this layer (Khrulkov et al.,\n2019; Li et al., 2018). Our Kronecker decomposi-\ntion method can substantially reduce the amount of\nrequired memory while maintaining low computa-\ntion.\nUsing Kronecker decomposition for large com-\npression factors, which is the focus of this paper,\nleads to a reduction in the model expressiveness.\nTo address this issue, we propose to distill knowl-\nedge from the intermediate layers of the original\nuncompressed network to the Kronecker network\nduring training. We use this approach for compres-\nsion of the BERTBASE model. We show that for a\nlarge compression factor of 19×, KroneckerBERT\nprovides signiﬁcantly better performance than state-\nof-the-art compressed BERT models. While our\nevaluations in this work are limited to BERT, this\nframework can be easily used to compress other\nTransformer-based NLP models. The main contri-\nbutions of this paper are as follows:\n• Developing a framework for compression of\nthe embedding layer using the Kronecker de-\ncomposition.\n• Deploying the Kronecker decomposition for\nthe compression of Transformer modules.\n• Proposing a KD method to improve the per-\nFigure 1: An example of Kronecker product of two 2\nby 2 matrices\nformance of the Kronecker network.\n• Evaluating the proposed framework for com-\npression of BERTBASE model on well-known\nNLP benchmarks\n2 Related Work\nIn this section, we ﬁrst go through some of the\nmost related works for BERT compression in the\nliterature and then review the few works that have\nused Kronecker decomposition for compression of\nCNNs and RNNs.\n2.1 Pre-trained Language Model\nCompression\nIn recent years, many model compression methods\nhave been proposed to reduce the size of PLMs\nwhile maintaining their performance on different\ntasks. KD, which was ﬁrst introduced by (Bu-\nciluˇa et al., 2006) and then later generalized by\n(Hinton et al., 2015), is a popular compression\nmethod where a small student network is trained\nto mimic the behavior of a larger teacher network.\nRecently, using KD for the compression of PLMs\nhas gained a growing interest in the NLP commu-\nnity. BERT-PKD (Sun et al., 2019), uses KD to\ntransfer knowledge from the teacher’s intermediate\nlayers to the student in the ﬁne-tuning stage. Tiny-\nBERT (Jiao et al., 2019) uses a two-step distillation\nmethod applied both at the pre-training and at the\nﬁne-tuning stage. MobileBERT (Sun et al., 2020b)\nalso uses an intermediate-layer knowledge distilla-\ntion methodology, but the teacher and the student\nare designed by incorporating inverted-bottleneck.\nSeveral works combine different compression tech-\nniques such as knowledge distillation and prun-\ning with matrix factorization (Mao et al., 2020)\nor quantization (Kim and Awadalla, 2020). In\n(Mao et al., 2020), the authors present LadaBERT,\na lightweight model compression pipeline combin-\ning SVD-based matrix factorization with weight\npruning, as well as a knowledge distillation method-\nology as described in (Jiao et al., 2019).\nTo achieve higher compression factors, authors\nin (Zhao et al., 2019) use a layer-wise KD method\nto reduce the size of vocabulary (from the usual\n30k to 5k) and the width of the layers. Similarly,\nZhao et al. 2021 uses a mixed-vocabulary training\nmethod to train models with a smaller vocabulary.\n2.2 Kronecker Factorization\nKronecker products have previously been utilized\nfor the compression of CNNs and small RNNs.\nZhou and Wu 2015 was the ﬁrst work that uti-\nlized Kronecker decomposition for NN compres-\nsion. They used summation of multiple Kronecker\nproducts to replace weight matrices in the fully\nconnected and convolution layers in simple CNN\narchitectures like AlexNet. Thakker et al., 2020\nused Kronecker product for the compression of\nvery small language models for deployment on IoT\ndevices. To reduce the amount of performance\ndrop after compression, they propose a hybrid ap-\nproach where the weight matrix is decomposed\ninto an upper part and lower part. The upper part\nremains un-factorized, and only the lower part is\nfactorized using the Kronecker product. More re-\ncently, Thakker et al. 2020 tried to extend the pre-\nvious work to non-IoT applications. Inspired by\nrobust PCA, they add a sparse matrix to Kronecker\nproduct factorization and propose an algorithm for\nlearning these two matrices together.\nTo the best of our knowledge, this work is the\nﬁrst attempt to compress Transformer-based lan-\nguage models using Kronecker decomposition. Un-\nlike prior arts, this work uses a simple Kronecker\nproduct of two matrices for the representation of\nlinear layers and uses KD framework to improve\nthe performance of the compressed model.\n3 Methodology\nIn this section, we ﬁrst introduce the background\nof Kronecker decomposition. We then explain our\ncompression method in detail.\n3.1 Kronecker Product\nKronecker product is an operation that is applied\non two matrices resulting in a block matrix. Let\nA be a matrix ∈Rm1×n1 , and let B be a matrix\n∈Rm2×n2 , then the Kronecker product of A and\nB denoted by ⊗is a block matrix, where each\nblock (i, j) is obtained by multiplying the element\nLook-up table Look-up table\nsee see\n=\nConventional Proposed  \n(Kronecker Embedding)\nFigure 2: Illustration of our proposed method for the\ncompression of the embedding layer. Left: conven-\ntional embedding stored in a lookup table. Right: Our\nproposed compression method where the original em-\nbedding matrix is represented as a Kronecker product\nof a matrix and a row vector. The matrix is stored in a\nlookup table to minimize computation over head.\nAi,j by matrix B. Therefore, the of the resulting\nmatrix A ⊗B is ∈ Rm×n where m = m1m2\nand n = n1n2. Figure 1 illustrates the Kronecker\nproduct between two small matrices. See (Graham,\n2018) for more detailed information on Kronecker\nproduct.\n3.2 Kronecker factorization\nWe can use Kronecker products to represent the\nweight matrices in Neural Networks (NNs). When\ndecomposing a matrix W ∈ Rm×n, as A ⊗B,\nthere are different choices for the shapes of A and\nB. In fact the dimensions of A i.e m1 and n1 can\nbe any factor of m and n respectively. The dimen-\nsions of B will then be equal to m2 = m/m1 and\nn2 = n/n1. We can achieve different compres-\nsion factors by changing the shape of these two\nmatrices.\n3.2.1 Memory and computation reduction\nWhen representing W as A ⊗B, the number of\nelements is reduced from mn to m1n1 + m2n2.\nMoreover, using the Kronecker product to repre-\nsent linear layers can reduce the required compu-\ntation. A trivial way to multiply the Kronecker\nproduct A ⊗B with an input vector x, is to ﬁrst\nreconstruct W by obtaining A ⊗B and then multi-\nply the result with x, which is extremely inefﬁcient\nboth with respect to memory and computation. A\nmuch more efﬁcient way is to use Eq.1 which is\na well-known property of the Kronecker product\nthat allows obtaining (A ⊗B)x without explicit\nreconstruction, A ⊗B (Lutkepohl, 1997):\n(A ⊗B)x = V(BRn2×n1 (x)A⊤) (1)\nwhere A⊤is A transpose. Here, Vis an operation\nthat transforms a matrix to a vector by stacking\nits columns and Rn2×n1 (x) is an operation that\nconverts a vector x to a matrix of size n2 ×n1\nby dividing the vector to columns of size n2 and\nconcatenating the resulting columns together. The\nconsequence of performing multiplication in this\nway is that it reduces the number of FLOPs from\n(2m1m2 −1)n1n2 to:\nmin\n(\n(2n2 −1)m2n1 + (2n1 −1)m2m1,\n(2n1 −1)n2m1 + (2n2 −1)m2m1\n)\n(2)\nWe use this method for implementation of the Kro-\nnecker layers.\n3.3 Kronecker Embedding layer\nThe embedding layer in large language models is a\nvery large lookup table X ∈Rv×d, where v is the\nsize of the dictionary and d is the embedding di-\nmension. In order to compress X using Kronecker\nfactorization, the ﬁrst step is to deﬁne the shape of\nKronecker factors AE and BE. We deﬁne AE to\nbe a matrix of sizev×d\nn and BE to be a row vector\nof size n. There are two reasons for deﬁning BE\nas a row vector. 1) it allows disentangled embed-\nding of each word since every word has a unique\nrow in AE. 2) the embedding of each word can be\nobtained efﬁciently in O(d). More precisely, the\nembedding for the i’th word in the dictionary can\nbe obtained by the Kronecker product between AE\ni\nand BE:\nXi = AE\ni ⊗BE (3)\nwhereAE is stored as a lookup table. Note that\nsince AE\ni is of size1×d\nn and BE is of size1×n, the\ncomputation complexity of this operation is O(d).\nFigure 2 shows an illustration of the Kronecker\nembedding layer.\n3.4 Kronecker Transformer\nThe Transformer layer is composed of two main\ncomponents: MHA and FFN. We use Kronecker de-\ncomposition to compress both. In the Transformer\nblock, the self-attention mechanism is done by pro-\njecting the input into the Key, Query, and Value\nembeddings and obtaining the attention matrices\nthrough the following:\nO = QKT\n√dk\n(4)\nAttention(Q, K, V) =softmax(O)V\nwhere Q, K, and V are obtained by multiplying the\ninput by WQ, WK, WV respectively. In a multi-\nhead attention (MHA) module, there is a separate\nWQ\nl , WK\nl , and WV\nl matrix per attention head to\nallow for a richer representation of the data. In the\nimplementation usually, matrices from all heads\nare stacked together resulting in 3 matrices.\nW′k = concat(WK\n1 , .., WK\nl , ..., WK\nL ) (5)\nW′Q = concat(WQ\n1 , .., WQ\nl , ..., WQ\nL )\nW′V = concat(WV\n1 , .., WV\nl , ..., WV\nL )\nwhere L is the number of attention heads. Instead\nof decomposing the matrices of each head sepa-\nrately, we use Kronecker decomposition after con-\ncatenation:\nW′K = Ak ⊗BK (6)\nW′Q = AQ ⊗BQ\nW′V = AV ⊗BV\nBy choosing m2 to be smaller than the output di-\nmension of each attention head, matrix B in the\nKronecker decomposition is shared among all at-\ntention heads resulting in more compression. The\nresult of applying Eq.4 is then fed to a linear map-\nping (WO) to produce the MHA output. We use\nKronecker decomposition for compressing this lin-\near mapping as well the two weight matrices in the\nsubsequent FFN block:\nWO = AO ⊗BO (7)\nW1 = A1 ⊗B1 (8)\nW2 = A2 ⊗B2 (9)\n3.5 Knowledge distillation\nIn the following section, we describe how KD is\nused to improve the training of the KroneckerBERT\nmodel.\n3.5.1 Intermediate KD\nLet S be the student, and T be the teacher, then\nfor a batch of data (X, Y), we deﬁne fS\nl (X)\nandfT\nl (X) as the output of the lth layer for the\nstudent network and the teacher network respec-\ntively. The teacher here is the BERTBASE and the\nstudent is its corresponding KroneckerBERT that is\nEmbedding\nMulti-head attention\nAdd and Norm\nFeedforward Network\nAdd and Norm\nClassifier\nEmbedding\nMulti-head attention\nAdd and Norm\nFeedforward Network\nAdd and Norm\nClassifier\nFFN output\nAttention matrices\nEmbedding output\nLast block projected features\nConcat and project\n Concat\nMHA output\nFirst layer in FFN output\n \nTeacher Student \nPooling\nEnglish\nWikipedia\nTeacher\nStudentStudent\nKroenckerBERT\nTeacher\nKroenckerBERT\nInference on Edge\nTask dataset\n sample 5% of \nEnglish Wikipedia\nTwo stage KD training\nPooling\nPooling\nPooling\nFigure 3: Illustration of the proposed framework. Left: A diagram of the teacher BERT model and the student\nKronckerBERT. Right: The two stage KD methodology used to train KroneckerBERT.\nModel CMP WK,WQ,\nWV,WO\nW1,[W2]T WE\nn, m d\nBERTBASE 1× 768, 768 768, 3072 768\nn1, m1 n\nKroneckerBERT8 7.7× 384, 384 2, 8 8\nKroneckerBERT19 19.3× 48, 384 2, 16 12\nTable 1: Conﬁguration of the Kronecker layers for the\ntwo KroneckerBERT models used in this paper. CMP\nstands for compression factor.n and m are respectively\nthe input and output dimensions of the weight matrices\n(W ∈Rm×n). m1, n1 indicates the shape of the ﬁrst\nKronecker factor (A ∈Rm1×n1 ). For embedding layer\nwe only need to set the size of the row vector BE ∈\nR1×n.\nobtained by replacing the embedding layer and the\nlinear mappings in MHA and FFN modules with\nKronecker factors(see Sections 3.3 and 3.4 for de-\ntails). Note that like other decomposition methods,\nwhen we use Kronecker factorization to compress\nthe model, the number of layers and the dimen-\nsions of the input and output of each layer remain\nintact. Therefore, when performing intermediate\nlayer KD, we can directly obtain the difference\nin the output of a speciﬁc layer in the teacher and\nstudent networks without any projection. In the pro-\nposed framework, the intermediate KD from the\nteacher to student occurs at the embedding layer\noutput, attention matrices and FFN outputs:\nLEmbedding(X) =MSE\n(\nES(X), ET (X)\n)\n(10)\nLAttention(X) =\n∑\nl\nMSE\n(\nOS\nl (X), OT\nl (X)\n)\n(11)\nLFFN(X) =\n∑\nl\nMSE\n(\nHS\nl (X), HT\nl (X)\n)\n(12)\nwhere ES and ET are the output of the embedding\nlayer from the student and the teacher respectively.\nOS\nl and OT\nl are the attention matrices (Eq.4), HS\nl\nand HT\nl are the outputs of the FFN, of layer l in\nthe student and the teacher respectively.\nSo far we have been using MSE of the interme-\ndiate features for transferring knowledge from the\nteacher to the student. Therefore, each element in\nthe feature vector of the student and teacher is com-\npared independently of other elements. Inspired by\n(Sun et al., 2020a), and in order to have a richer\ncomparison between the student and the teacher\nnetworks, we also add a projection loss term. To\nobtain the projection loss we ﬁrst average pool\nthe FFN output and attention outputs of the last\nlayer (HL and AL) and concatenate them together\nto obtain a feature vector. We project the feature\nvector obtained from the teacher using a learnable\nweight matrix P ∈R2d×2d, where d is the hid-\nden dimension of the Transformer. We then obtain\nthe projection loss as the MSE between student’s\nfeatures and teacher’s projected features:\ngS(x) =concat\n[\npool(AS\nL), pool(HS\nL)\n]\n,(13)\ngT (x) =concat\n[\npool(AT\nL), pool(HT\nL ))\n]\n,(14)\nLProjection (x) =MSE(gS(x), PgT (x)).(15)\nOur ﬁnal loss is as follows:\nL(x, y) =\n∑\n(x,y)\nLEmbedding(x) +\nLAttention(x) +LFFN (x) +\nLProjection (x) +LLogits(x) +\nLCE (x, y). (16)\nNote that, unlike some other KD methods where\nthe motivation for this projection is to match the\ndimension of the student and teacher features, here\nwe use it to obtain a richer representation of the\nknowledge.\n3.5.2 KD at pre-training\nInspired by (Jiao et al., 2019) we use KD at the\npre-training stage to capture the general domain\nknowledge from the teacher. In pre-training dis-\ntillation, the teacher is the BERTBASE model that\nis pre-trained on BookCorpus (Zhu et al., 2015)\nand English Wikipedia. Intermediate layer KD is\nthen used to train the KroneckeBERT network in\nthe general domain. KD at pre-training improves\nthe initialization of the Kronecker model for the\ntask-speciﬁc KD stage. Similar to (Jiao et al., 2019)\nthe loss at pre-training stage only involves the in-\ntermediate layers LEmbedding(x)+ LAttention(x)+\nLFFN (x). Unlike (Jiao et al., 2019), we perform\npre-training distillation only on a small portion of\nthe dataset (5% of the English Wikipedia) for a few\nepochs (3 epochs) which makes our training far\nmore efﬁcient.\n3.6 Model Settings\nThe ﬁrst step of the proposed framework is to de-\nsign the Kronecker layers by deﬁning the shape of\nKronecker factors matrices A and B. To do this we\nneed to set the shape of one of these matrices and\nthe other one can be obtained accordingly. There-\nfore we only searched among different choices for\nm1 and n1 which are limited to the factors of the\noriginal weight matrix (m and n respectively). We\nused the same conﬁguration for all the matrices in\nthe MHA. Also For the FFN, we chose the conﬁg-\nuration for one layer, and for the other layer, the\ndimensions are swapped. For the embedding layer,\nsince BE is a row vector, we only need to choose\nn.\nThe shapes of the Kronecker factors are chosen\nto obtain the desired compression factor. However,\nthere may be multiple conﬁgurations to achieve\nthat, among which we chose the one that leads\nto minimum FLOPS according to Eq.2. Table 1\nsummarises the conﬁguration of Kronecker factor-\nization for the two compression factors used in this\nwork.\n3.7 Implementation details\nFor KD at the pre-training stage, the Kronecker-\nBERT model is initialized using the teacher (pre-\ntrained BERTBASE model). This means that for lay-\ners that are not compressed, the values are copied\nfrom the teacher to the student. For Kronecker\ndecomposed layers, the L2 norm approximation\n(Van Loan, 2000) is used to approximate Kronecker\nfactors (A and B) from the pre-trained BERTBASE\nmodel. In the pre-training stage, 5% of the English\nWikipedia was used for 3 epochs. The batch size in\npre-training was set to 64 and the learning rate was\nset e-3. After pre-training, the obtained Kronecker\nmodel is used to initialize the Kronecker layers in\nthe student model for task-speciﬁc ﬁne-tuning. The\nPrediction layer is initialized from the ﬁne-tuned\nBERTBASE teacher. For ﬁne-tuning on each task,\nwe optimize the hyper-parameters based on the per-\nformance of the model on the dev set. See appendix\nfor more details on the results on dev set and the\nselected hyperparamters.\n4 Experiments\nIn this section, we compare our KroneckerBERT\nwith the sate-of-the-art compression methods ap-\nplied to BERT on GLUE and SQuAD. We also\ninvestigate the effect of KD through ablation exper-\niments and investigate the robustness of our model\nto out-of-distribution samples. Moreover we show\nthe speedup results of deploying our model on edge\ndevices and justify our ﬁndings.\n4.1 Baselines\nAs for baselines we select two categories of com-\npression methods, those with compression factor\n<10 and those with compression factor >10. For\na fair comparison, we select models that have\nBERTBASE as the teacher. In the ﬁrst category,\nwe both have BERT PKD (Sun et al., 2019) with\nlow compression factor, and models with similar\nModel Params MNLI-(m/mm) SST-2 MRPC CoLA QQP QNLI RTE STS-B Avg\nBERTBASE 108.5M 83.9/83.4 93.4 87.9 52.8 71.1 90.9 67 85.2 79.5\nBERT4-PKD 52.2M 79.9/79.3 89.4 82.6 24.8 70.2 85.1 62.3 79.8 72.6\nTinyBERT 14.5M 82.5/81.8 92.6 86.4 44.1 71.3 87.7 66.6 80.4 77.0\nLadaBERT3 15M 82.1/81.8 89.9 - - 69.4 84.5 - - -\nKroneckerBERT8 14.3M 82.9/81.7 91.2 88.5 31.2 70.8 88.4 66.9 83.1 76.1\nSharedProject 5.6M 76.4/75.2 84.7 84.9 - - - - - -\nLadaBERT4 11M 75.8/76.1 84.0 - - 67.4 75.1 - - -\nKroneckerBERT19 5.7M 79.4/81.6 89.2 86.9 25.8 69.2 86.2 62.7 78.2 73.1\nTable 2: Results on the test set of GLUE ofﬁcial benchmark. The results for BERT, BERT 4-PKD and TinyBERT\nare taken from (Jiao et al., 2019). For all other baselines the results are taken from their associated papers. Shared-\nProject and LadaBERT refer to (Zhao et al., 2019) and (Mao et al., 2020) respectively. Note that TinyBERT\nperforms pre-training KD on the entire Wikipedia dataset and ﬁne-tuning on Augmented data whereas we only\nperform pre-training KD on 5% of the Wikipedia and we do not use data augmentation for ﬁne-tuning.\nModel MRPC →\nPA WS\nRTE →\nHANS\nSST-2 →\nIMDb\nBERTBASE 61.3 50.7 88.0\nTinyBERT 61.3 51.2 78.5\nKroneckerBERT8 61.4 52.7 81.0\nTable 3: The results of out of distribution experiment.\nFined-tuned models on MRPC, RTE and SST-2 are\nevaluated on PAWS, HANS and IMDb respectively.\nSQuAD1.1SQuAD2.0\nModel CompressEM F1 EM F1\nBERTBASE 1× 80.5 88 74.5 77.7\nBERT4-PKD 2.1× 70.1 79.5 60.8 64.6\nTinyBERT 7.5× 72.7 82.1 68.2 71.8\nKroneckerBERT8 7.6× 75.4 84.2 69.9 73.5\nKroneckerBERT19 19.3× 66.7 77.8 63.0 67.0\nTable 4: Results of the baselines and KroneckerBERT\non question SQuAD dev dataset. The results of the\nbaselines are taken from (Jiao et al., 2019).\ncompression factor as our KroneckerBERT8: Tiny-\nBERT (Jiao et al., 2019) and LadaBERT (Mao et al.,\n2020).\nNote that TinyBERT is not directly compara-\nble since they do KD at pre-training on the entire\nWikipedia dataset and also they do an extensive\naugmentation on GLUE for KD in the ﬁne-tuning\nstage (x20). We exclude MobileBERT (Sun et al.,\n2020b) since they use a redesign of BERT LARGE\nwith inverted-bottleneck as the teacher. For the\nsecond category, we compare our results with\nSharedProject (Zhao et al., 2019) and (Mao et al.,\n2020) with compression factor in the rage of 10-\n20. We exclude (Zhao et al., 2021) since they use\nBERTLARGE as the teacher.\n4.2 Results on GLUE benchmark\nWe evaluated the proposed framework on the Gen-\neral Language Understanding Evaluation (GLUE)\n(Wang et al., 2018) benchmark which consists\nof 9 natural language understanding tasks. We\nsubmitted the predictions of our proposed mod-\nels on the test data sets for different tasks\nto the ofﬁcial GLUE benchmark ( https://\ngluebenchmark.com/). Table 2 summa-\nrizes the results. The results are divided into\ntwo categories: in the upper part of the table\nKroneckerBERT8 are compared to several state-\nof-the-art KD methods that have a compression\nfactor less than 10. In the lower part of the table,\nwe compare the performance of KroneckerBERT19\nwith state-of-the-art models that have a compres-\nsion factor greater than 10. We can see that when\nthe compression factor is less than 10, Kronecker-\nBERT outperforms other baselines, except Tiny-\nBERT, on every task of GLUE. As for TinyBERT\nKroneckerBERT has a very similar performance\nin all the tasks except for CoLA. Note TinyBERT\nperforms pre-training KD on the entire Wikipedia\ndataset and uses an extensive data augmentation\n(20 times the original data) in the ﬁne-tuning stage\nto obtain these results. Moreover, the average per-\nformance of KroneckerBERT8 excluding CoLA is\n81.7 compared to 81.2 for TinyBERT. For higher\ncompression factors, KroneckerBERT outperforms\nall other baselines on all available results.\n4.3 Out of distribution robustness\nIt is shown that pre-trained Transformer-based\nlanguage models are robust to out-of-distribution\n(OOD) samples (Hendrycks et al., 2020). In this\nsection, we investigate how the proposed compres-\nsion method affects the OOD robustness of BERT\nby evaluating the ﬁned-tuned models on MRPC,\nRTE, and SST-2 on PAWS (Zhang et al., 2019),\nHANS (McCoy et al., 2019), and IMDb (Maas\net al., 2011) respectively. We compare OOD ro-\n \n \n \n \n \n \nText\nFigure 4: T-SNE visualization of the output of the\nmiddle Transformer layer of the ﬁne-tuned models\non SST-2 dev. Left: Fine-tuned BERT BASE, mid-\ndle: KroneckerBERT 8 ﬁne-tuned without KD, right:\nKroneckerBERT8 when trained using KD in two stages.\nThe colours indicate the positive and negative classes.\nPre-training Fine-tuning MNLI-m SST-2 MRPC\nNone No KD 66.0 81.3 68.3\nNone KD 80.7 86.2 70.3\nKD No KD 77.0 87.2 78.17\nKD KD 82.8 90.6 86.6\nTable 5: Ablation study of the effect of KD in the pre-\ntraining and ﬁne-tuning stages on the performance of\nthe model on GLUE dev. The teacher for KD at the\npre-training stage and at the ﬁne-tuning stage is the pre-\ntrained and the ﬁned-tuned BERTBASE respectively.\nbustness with the teacher, BERT BASE and Tiny-\nBERT. TinyBERT ﬁne-tuned checkpoint are ob-\ntained from their repsitory. Table 3 lists the results.\nWe can see the ﬁne-tuned KroneckerBERT8 mod-\nels on MRPC and RTE are robust to OOD since\nthere is a small increase in performance compared\nto BERTBASE. On IMDb, there is a signiﬁcant\ndrop in performance after compression, but our\nKroenckerBERT8 is still more robust than Tiny-\nBERT. In fact, KroencekrBERT8 outperforms Tiny-\nBERT on all the three OOD experiments.\n4.4 Results on SQuAD\nIn this section, we evaluate the performance of the\nproposed model on SQuAD datasets. SQuAD1.1\n(Rajpurkar et al., 2016) is a large-scale reading\ncomprehension that contain questions that have\nanswers in given context. SQuAD2.0 (Kudo\nand Richardson, 2018) also contains unanswer-\nable questions. Table 4 summarises the perfor-\nmance on dev set. For both SQuAD1.1 and\nSQuAD2.0, KroneckerBERT 8 can signiﬁcantly\noutperform other baselines. We have also listed\nthe performance of KroneckerBERT19. The results\nof baselines with higher compression factors on\nSQuAD were not available.\nModel FLOPS Inference Speedup\nSmartphone 1Smartphone 2CPU\nBERTBASE 22B 1 1 1\nKroneckerBERT8 5.5B 0.94 0.74 0.81\nKroneckerBERT19 1.4B 2.4 1.93 1.1\nTable 6: Number of FLOPS and inference speed up on\n2 smartphones and Intel CPU (Intel Xeon Gold6140)\nfor the proposed model with respect to BERTBASE.\n4.5 Ablation study on the effect of KD\nIn this section, we investigate the effect of KD\nin reducing the gap between the compressed Kro-\nneckerBERT model and the original BERT BASE\nnetwork. Table 5 summarises the results for\nKroneckerBERT8. Our proposed method uses KD\nin both the pre-training and the ﬁne-tuning stages.\nFor this ablation study, pre-training is either not\nperformed or is done using KD with the pre-trained\nBERTBASE as the teacher. We perform experiments\non 3 tasks from the GLUE benchmark with differ-\nent sizes of training data, namely MNLI-m, SST-\n2, and MRPC. We can see that without KD, Kro-\nnecker decomposition leads to a signiﬁcant drop in\nperformance. For MNLI-m we can see that KD at\nﬁne-tuning stages is more effective than in perform-\ning pre-training with KD. For SST-2 and MRPC,\nperforming pre-training with KD is more effective.\nFor all tasks, the highest performance is obtained\nwhen the two-stage KD is used(last row).\nWe also used t-SNE to visualize the output of\nthe FFN of the middle layer (layer 6) of the ﬁne-\ntuned KroneckerBERT8 with and without KD in\ncomparison with the ﬁne-tuned teacher, on SST-\n2 dev. Figure 4 shows the results. See how KD\nhelps the features of the middle layer to be more\nseparable with respect to the task compared to the\nno KD case.\n4.6 Inference speedup: Discussion and\nresults\nDecomposition methods like SVD, Tucker, etc. all\ntry to represent a given tensor as multiple signiﬁ-\ncantly smaller tensors and thus reduce the number\nof parameters. The same is also true for the number\nof operations involved in the forward pass: instead\nof one large matrix operation, we often have mul-\ntiple small matrix operations which can lead to a\nreduction in the number of FLOPS. However, this\nreduction in the number of FLOPS may not lead to\nlatency reduction since these smaller matrix oper-\nations have to be performed in serial. Kronecker\ndecomposition is no exception in this regard. In\norder to perform Kronecker product matrix mul-\ntiplication (A ⊗B)X we need to multiply 3 ma-\ntrices as in Eq.1. Multiplication of 3 matrices is\ndone through two matrix-matrix multiplication in\nserial. Serial nature of this operation limits utiliza-\ntion of parallel processing units on modern CPUs\nand GPUs. Therefore without modiﬁcation of the\nunderlying software/hardware, the reduction of the\nnumber of FLOPs may not lead to latency reduction\non high-performance devices.\nIn order to investigate how this reduction in\nthe number of FLOPS is translated to a reduc-\ntion in the latency, for high compression factors,\nwe serialized the Kronecker 19 using PyTorch’s\nTorchScript and deployed it on two smartphones\nas well as on a high-performance CPU (18 core\nIntel©Xeon©GOLD 6140). The speciﬁcation of\nocta-core processors for smartphone 1 and 2 is\n(4x2.4GHz, 4x1.8GHz) and (2x2.6 GHz,2x1.9GHz,\n4x1.8GHz) respectively. We measured the latency\nover 50 runs of a single input sequence of length\n128. The results are listed in Table 6. We can\nsee that for the high compression factor of 19, the\ncompressed model can have up to 2.4 speed up on\nedge devices. However, on the high-performance\nCPU with much higher memory and computation\ncapacity using KroneckerBERT does not lead to\nspeed-up which conﬁrms the above argument.\n5 Conclusion\nWe introduced a novel framework for compressing\nTransformer-based language models. The proposed\nmodel uses Kronecker decomposition for the com-\npression of the embedding layer and the linear map-\npings within the Transformer blocks. The proposed\nframework was used to compress the BERT BASE\nmodel. We used an efﬁcient two-stage KD method\nto train the compressed model. We show that the\nproposed framework can signiﬁcantly reduce the\nsize and the number of computations while out-\nperforming state-of-the-art for high compression\nfactors of at least 10×. The proposed methodol-\nogy can be applied to any other Transformer based\nlanguage model.\nAcknowledgements\nAuthors would like to thank Mahdi Zolnouri for\ndeploying the models on cellphones and obtaining\nlatency results and Aref Jaffari for preparing the\nOOD datasets. We also would like to thank Seyed\nAlireza Ghaffari and Eyyüb Sari for informative\ndiscussions throughout this project.\nReferences\nCristian Bucilu ˇa, Rich Caruana, and Alexandru\nNiculescu-Mizil. 2006. Model compression.\nIn Proceedings of the 12th ACM SIGKDD\ninternational conference on Knowledge discovery\nand data mining, pages 535–541.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nYunchao Gong, Liu Liu, Ming Yang, and Lubomir\nBourdev. 2014. Compressing deep convolutional\nnetworks using vector quantization. arXiv preprint\narXiv:1412.6115.\nAlexander Graham. 2018. Kronecker products and\nmatrix calculus with applications. Courier Dover\nPublications.\nSong Han, Huizi Mao, and William J Dally. 2015.\nDeep compression: Compressing deep neural net-\nworks with pruning, trained quantization and huff-\nman coding. arXiv preprint arXiv:1510.00149.\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam\nDziedzic, Rishabh Krishnan, and Dawn Song. 2020.\nPretrained transformers improve out-of-distribution\nrobustness. arXiv preprint arXiv:2004.06100.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2019. Tinybert: Distilling bert for natural language\nunderstanding. arXiv preprint arXiv:1909.10351.\nValentin Khrulkov, Oleksii Hrinchuk, Leyla Mir-\nvakhabova, and Ivan Oseledets. 2019. Tensorized\nembedding layers for efﬁcient model compression.\narXiv preprint arXiv:1901.10787.\nYoung Jin Kim and Hany Hassan Awadalla. 2020.\nFastformers: Highly efﬁcient transformer models\nfor natural language understanding. arXiv preprint\narXiv:2010.13382.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing.\narXiv preprint arXiv:1808.06226.\nZhongliang Li, Raymond Kulhanek, Shaojun Wang,\nYunxin Zhao, and Shuang Wu. 2018. Slim embed-\nding layers for recurrent neural language models. In\nProceedings of the AAAI Conference on Artiﬁcial\nIntelligence, volume 32.\nVasileios Lioutas, Ahmad Rashid, Krtin Kumar,\nMd Akmal Haidar, and Mehdi Rezagholizadeh.\n2020. Improving word embedding factorization for\ncompression using distilled nonlinear neural decom-\nposition. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing:\nFindings, pages 2774–2784.\nHelmut Lutkepohl. 1997. Handbook of matri-\nces. Computational statistics and Data analysis,\n2(25):243.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 142–150, Portland,\nOregon, USA. Association for Computational Lin-\nguistics.\nYihuan Mao, Yujing Wang, Chufan Wu, Chen Zhang,\nYang Wang, Yaming Yang, Quanlu Zhang, Yunhai\nTong, and Jing Bai. 2020. Ladabert: Lightweight\nadaptation of bert through hybrid model compres-\nsion. arXiv preprint arXiv:2004.04124.\nR Thomas McCoy, Ellie Pavlick, and Tal Linzen. 2019.\nRight for the wrong reasons: Diagnosing syntac-\ntic heuristics in natural language inference. arXiv\npreprint arXiv:1902.01007.\nGabriele Prato, Ella Charlaix, and Mehdi Reza-\ngholizadeh. 2019. Fully quantized trans-\nformer for machine translation. arXiv preprint\narXiv:1910.10485.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog, 1(8):9.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ Questions\nfor Machine Comprehension of Text. arXiv e-prints,\npage arXiv:1606.05250.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion pa-\nrameter language models using model parallelism.\narXiv preprint arXiv:1909.08053.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for bert model com-\npression. arXiv preprint arXiv:1908.09355.\nSiqi Sun, Zhe Gan, Yu Cheng, Yuwei Fang, Shuo-\nhang Wang, and Jingjing Liu. 2020a. Con-\ntrastive distillation on intermediate representations\nfor language model compression. arXiv preprint\narXiv:2009.14167.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020b. Mobilebert:\na compact task-agnostic bert for resource-limited de-\nvices. arXiv preprint arXiv:2004.02984.\nUrmish Thakker, Paul Whatamough, Matthew Mattina,\nand Jesse Beu. 2020. Compressing language mod-\nels using doped kronecker products. arXiv preprint\narXiv:2001.08896.\nCharles F Van Loan. 2000. The ubiquitous kronecker\nproduct. Journal of computational and applied\nmathematics, 123(1-2):85–100.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nCanwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei,\nand Ming Zhou. 2020. Bert-of-theseus: Compress-\ning bert by progressive module replacing.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nXiyu Yu, Tongliang Liu, Xinchao Wang, and Dacheng\nTao. 2017. On compressing deep models by low\nrank and sparse decomposition. In Proceedings\nof the IEEE Conference on Computer Vision and\nPattern Recognition, pages 7370–7379.\nYuan Zhang, Jason Baldridge, and Luheng He. 2019.\nPaws: Paraphrase adversaries from word scrambling.\narXiv preprint arXiv:1904.01130.\nSanqiang Zhao, Raghav Gupta, Yang Song, and Denny\nZhou. 2019. Extreme language model compres-\nsion with optimal subwords and shared projections.\narXiv preprint arXiv:1909.11687.\nSanqiang Zhao, Raghav Gupta, Yang Song, and Denny\nZhou. 2021. Extremely small bert models from\nmixed-vocabulary training. In Proceedings of the\n16th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Main\nV olume, pages 2753–2759.\nShuchang Zhou and Jia-Nan Wu. 2015. Compression\nof fully-connected layer in neural network by kro-\nnecker product. arXiv preprint arXiv:1507.05775.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE\ninternational conference on computer vision, pages\n19–27.",
  "topic": "Distillation",
  "concepts": [
    {
      "name": "Distillation",
      "score": 0.6205488443374634
    },
    {
      "name": "Decomposition",
      "score": 0.6084152460098267
    },
    {
      "name": "Kronecker delta",
      "score": 0.5845787525177002
    },
    {
      "name": "Computer science",
      "score": 0.5230706334114075
    },
    {
      "name": "Natural language processing",
      "score": 0.47064751386642456
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4587898254394531
    },
    {
      "name": "Machine learning",
      "score": 0.330214262008667
    },
    {
      "name": "Chemistry",
      "score": 0.17825043201446533
    },
    {
      "name": "Chromatography",
      "score": 0.12140804529190063
    },
    {
      "name": "Organic chemistry",
      "score": 0.06747099757194519
    },
    {
      "name": "Physics",
      "score": 0.0639641284942627
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}