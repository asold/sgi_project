{
  "title": "Exploring Corruption Robustness: Inductive Biases in Vision Transformers and MLP-Mixers",
  "url": "https://openalex.org/W3173494152",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5035829425",
      "name": "Katelyn Morrison",
      "affiliations": [
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A5075698118",
      "name": "Benjamin Gilby",
      "affiliations": [
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A5065858077",
      "name": "Colton Lipchak",
      "affiliations": [
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A5042360071",
      "name": "Adam Mattioli",
      "affiliations": [
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A5072882318",
      "name": "Adriana Kovashka",
      "affiliations": [
        "University of Pittsburgh"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963488527",
    "https://openalex.org/W2949650786",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3177096435",
    "https://openalex.org/W2989891204",
    "https://openalex.org/W3037492894",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3202088435",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W3125135622",
    "https://openalex.org/W2902617128",
    "https://openalex.org/W2950179405",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2963060032",
    "https://openalex.org/W2964661273",
    "https://openalex.org/W3145185940",
    "https://openalex.org/W3126994093",
    "https://openalex.org/W2787776114"
  ],
  "abstract": "Recently, vision transformers and MLP-based models have been developed in order to address some of the prevalent weaknesses in convolutional neural networks. Due to the novelty of transformers being used in this domain along with the self-attention mechanism, it remains unclear to what degree these architectures are robust to corruptions. Despite some works proposing that data augmentation remains essential for a model to be robust against corruptions, we propose to explore the impact that the architecture has on corruption robustness. We find that vision transformer architectures are inherently more robust to corruptions than the ResNet-50 and MLP-Mixers. We also find that vision transformers with 5 times fewer parameters than a ResNet-50 have more shape bias. Our code is available to reproduce.",
  "full_text": "Exploring Corruption Robustness: Inductive Biases in Vision Transformers\nand MLP-Mixers\nKatelyn Morrison * 1 Benjamin Gilby * 1 Colton Lipchak 1 Adam Mattioli 1 Adriana Kovashka1\nAbstract\nRecently, vision transformers and MLP-based\nmodels have been developed in order to address\nsome of the prevalent weaknesses in convolutional\nneural networks. Due to the novelty of transform-\ners being used in this domain along with the self-\nattention mechanism, it remains unclear to what\ndegree these architectures are robust to corrup-\ntions. Despite some works proposing that data\naugmentation remains essential for a model to be\nrobust against corruptions, we propose to explore\nthe impact that the architecture has on corruption\nrobustness. We ﬁnd that vision transformer archi-\ntectures are inherently more robust to corruptions\nthan the ResNet-50 and MLP-Mixers. We also\nﬁnd that vision transformers with 5 times fewer\nparameters than a ResNet-50 have more shape\nbias. Our code is available to reproduce.\n1. Introduction\nResearch indicates that humans tend to classify objects\nbased on shape rather than color or texture while convo-\nlutional neural networks are more biased towards texture\n(Ritter et al., 2017). Developing and deploying reliable,\naccurate computer vision models is integral to the success\nand trust of vision-based technologies such as self-driving\ncars or assistive technologies. Signiﬁcant errors in these\ntechnologies could be fatal, which is why it is important to\nunderstand the limitations of different models.\nIn the past decade, convolutional neural networks (CNNs)\nhave been the state-of-the-art for computer vision tasks\nsuch as image classiﬁcation. However, recent research has\nshown the limitations of CNNs in domain generalization\ntasks. Several recent works seeking to develop and train\n*Equal contribution 1Department of Computer Science, Uni-\nversity of Pittsburgh, Pittsburgh, United States. Correspon-\ndence to: Katelyn Morrison <kmorrison@pitt.edu>, Benjamin\nGilby <beg59@pitt.edu>, Colton Lipchak <cjl99@pitt.edu>,\nAdam Mattioli <afm45@pitt.edu>, Adriana Kovashka <ko-\nvashka@cs.pitt.edu>.\nCopyright 2021 by the author(s).\nmodels that can successfully achieve domain adaptation\nand/or domain generalization investigate the intricacies that\ncontribute to corruption robustness in a model (Mummadi\net al., 2021; Hermann & Kornblith, 2019; Geirhos et al.,\n2019; Brochu, 2019; Feinman & Lake, 2018). Recently,\nnovel architectures have achieved exceptional performance\non ImageNet and CIFAR baselines. While robustness of\nCNNs have been studied, it is vital to explore the robustness\nof these new architectures, including how well they perform\nwhen presented corrupted images.\nContributions. In this work, we investigate models with\nthree different types of architectures: CNNs, Vision Trans-\nformers, and MLP-Mixers. In total, we compare and con-\ntrast twenty different pre-trained models. Our ﬁndings re-\nveal how various pre-trained vision transformer architec-\ntures and MLP-Mixers make decisions (i.e., based on shape\nor texture) and how well they handle corruptions. Our con-\ntributions include the following:\n• Comparing corruption robustness and shape bias across\nstate-of-the-art vision transformer architectures and the\nMLP-Mixer.\n• Showing that vision transformers are inherently more\nrobust to common corruptions than CNNs and the\nMLP-Mixer.\n2. Related Works\nInvestigating inductive biases, such as shape bias and texture\nbias, and how these biases can improve the robustness of\na model have been extensively explored within CNNs. We\nwill highlight several advancements and contributions in the\npast few years ranging from data augmentation techniques\nto novel architectures designed to improve top-1 and top-5\naccuracy on image classiﬁcation tasks.\n2.1. Data Augmentation & Training Techniques\nGeirhos et al. (2019) conduct an empirical study to under-\nstand the inductive biases learned by CNNs trained on Ima-\ngeNet. After creating several augmented ImageNet data sets,\nthey show that CNNs are more texture-biased during ob-\nject recognition tasks while humans are more shape biased.\narXiv:2106.13122v2  [cs.CV]  3 Jul 2021\nExploring Corruption Robustness: Inductive Biases in Vision Transformers and MLP-Mixers\nThese results are contradicted soon after showing that CNNs\ncan learn shape bias as easily as texture bias (Hermann &\nKornblith, 2019). Hermann & Kornblith (2019) indicate\nthat the inductive biases that the CNN learns may be solely\ndependent on the data it sees instead of the architecture\nitself.\nA more recent empirical study investigates if shape bias\nand corruption robustness have a direct correlation (Mum-\nmadi et al., 2021). Mummadi et al. (2021) compares the\naccuracy and corruption robustness of CNNs trained on Im-\nageNet with standard images, standard and stylized images,\nand a combination of edge maps of ImageNet and standard\nimages. They show that the model trained on standard im-\nages and edge maps resulted in having the greatest shape\nbias. However, the network trained on standard and stylized\nimages performed the best on common corruptions. They\nconcluded that the stylized images caused increased shape\nbias, but corruption robustness was increased by the stylized\nimages, not the shape bias directly.\nAn alternative approach explains an algorithm for shape-\ntexture debiased learning by augmenting images in the train-\ning set with conﬂicting shapes and textures (Li et al., 2021).\nThis algorithm is still based on using CNNs, but their algo-\nrithm proves to achieve improvements on ImageNet-C and\nStylized-ImageNet among others. The augmentation in this\nalgorithm consists of using conﬂicting shape and texture\ninformation on the original image.\n2.2. Architectures for Better Accuracy\nDifferent convolutional neural network architectures have\nbeen modiﬁed and reconstructed to achieve a higher accu-\nracy on image classiﬁcation tasks. Most recently, transform-\ners have been modiﬁed and adapted for vision tasks such as\nimage classiﬁcation. We will only introduce the vision trans-\nformer architectures that we included in our experiments,\nbut there are several other variations of vision transformers\nin the literature.\nAn architecture called the Vision Transformer (ViT) uses\nlayers of multi-headed self attention and multi-layer per-\nceptrons (Dosovitskiy et al., 2021). They conduct image\nclassiﬁcation by splitting an image into a ﬁxed number of\npatches and embedding each image patch. This architecture\nachieves excellent results compared to CNNs on numerous\nbaselines. Bhojanapalli et al. (2021) investigate several dif-\nferent ViT and ResNet models to understand the robustness\nof the ViT models. They also show how the two archi-\ntectures perform when faced against different adversarial\nattacks such as PGD and FGSM. Overall, their results reveal\nthat the ViT is as least as robust to corruptions as ResNets\n(Bhojanapalli et al., 2021).\nA variation of the ViT vision transformer, called the Swin\nTransformer, calculates self-attention of a window of image\npatches to compute predictions for tasks such as image clas-\nsiﬁcation (Liu et al., 2021). The windows of image patches\nshift after calculating the self-attention of the previous win-\ndows. This shift results in a hierarchical feature map that\nprovides a better global representation of the image.\nTwo other variations of the vision transformer architectures\nare the Data-efﬁcient Image Transformers (DeiT) (Touvron\net al., 2020) and Class-Attention in Image Transformers\n(CaiT) (Touvron et al., 2021). DeiT uses a custom dis-\ntillation procedure and no convolutional layers and CaiT\nfeatures class-attention layers.\nA recent architecture, called MLP-Mixer, is designed to\nexclude convolutional and self-attention layers, and instead\nmix per-location features and spatial information through\ntwo MLP-based layers (Tolstikhin et al., 2021). This archi-\ntecture also incorporates signiﬁcant augmentation within the\npre-processing pipeline to increase accuracy of the model.\nTo our knowledge, no current research has been published\non how numerous different vision transformers compare\nto one another in terms of corruption robustness or shape\nbias. No previous research has explored how robust the\nMLP-Mixer is to corruption either.\n3. Method\nTo explore how robust vision transformer architectures and\nMLP-Mixers are to corruptions, we perform several experi-\nments consisting of four pre-trained CNNs, fourteen vision\ntransformers, and two MLP-Mixers.\n3.1. Pre-trained Models\nConvolutional Neural Networks. The convolutional\nneural networks we chose were inspired by the models used\nin Geirhos et al. (2019). Speciﬁcally, Geirhos et al. (2019)\nevaluated the shape bias of ResNet-50 (He et al., 2015),\nAlexNet (Krizhevsky et al., 2012), VGG-16 (Simonyan &\nZisserman, 2015), and GoogLeNet (Szegedy et al., 2014).\nWe evaluated the and corruption robustness of these mod-\nels to use as a baseline when determining how the vision\ntransformers and the MLP-Mixers perform.\nMLP-Mixers. We evaluated two different variations of\nthe MLP-Mixer architecture: the base and large varia-\ntions. These pre-trained models were provided by the timm\nlibrary (Wightman, 2019).\nVision Transformers. We evaluated a total of four state-\nof-the-art, competing vision transformers. Due to limited\nresources and ease of access, we choose to use the pre-\ntrained models provided by the timm library (Wight-\nman, 2019). Speciﬁcally from Wightman (2019), we in-\nExploring Corruption Robustness: Inductive Biases in Vision Transformers and MLP-Mixers\ncluded the Swin-T, the ViT, and the CaiT pre-trained models\nin our evaluation. Each of these architectures have multi-\nple pre-trained models available. We used four different\npre-trained Swin Transformers, two different pre-trained\nViT models, and two different pre-trained CaiT models. We\nobtained pre-trained DeiT models directly from Facebook\nResearch’s GitHub (Touvron et al., 2020). We used six\ndifferent pre-trained DeiT models.\n3.2. Data Sets\nImageNet-C. We evaluated all of our pre-trained mod-\nels on ImageNet-C to determine corruption robustness\n(Hendrycks & Dietterich, 2018). ImageNet-C is a bench-\nmark dataset used to assess how robust a model is to com-\nmon corruptions. This dataset consists of nineteen different\ncorruption types that are categorized within ﬁve general cor-\nruption categories (blur, weather, noise, digital, and extra)\nfor ﬁve different severity levels. The dataset is built off of\nthe ILSVRC 2012 validation set which has 1, 000 classes\nand ﬁfty validation images for each class totalling 50, 000\nvalidation images. In terms of ImageNet-C, each corruption\ntype (i.e., blur →motion blur) has 50, 000 images for each\nseverity level.\nTexture-Cue Conﬂict. We used the Texture-Cue Conﬂict\ndataset from Geirhos et al. (2019) to evaluate the shape bias\nof our models. The Texture-Cue Conﬂict dataset consists\nof images that have the shape of one class combined with\nthe texture of another. This results in conﬂicting shape and\ntexture in each image. Two labels are included to identify\nground-truth for both the shape and the texture of an image.\nThe dataset includes 16 classes and 80 images per class for\na total of 1280 images.\n3.3. Evaluation Metrics\nThe top-1 accuracy and top-1 error from each model is\nused to understand how robust the model is to different\ncorruptions. Since we are evaluating architectures that are\nsigniﬁcantly different than CNNs, we decided to deviate\naway from using the corruption error from AlexNet as a\nnormalization factor (Hendrycks & Dietterich, 2019) when\ncalculating the corruption error. Instead, we obtain the cor-\nruption error, CE, by sum the top-1 error for that corruption\nfrom severity 2 and severity 5 where f is the given model, s\nis the severity, and c is the corruption:\nCEf\nc = Ef\n2,c −Ef\n5,c\nTo calculate mean corruption error, mCE, we take the aver-\nage over all corruption errors calculated for a given model.\nTypically, mCE is calculated by averaging over corruption\nerrors from all severity levels, but we chose to only include\nthe corruption errors from severity 2 and severity 5 in our\nmCE calculation. We use these two severity levels to repre-\nsent an average of the overall mCE for a given model. Even\nthough our resulting mCE will not be directly comparable\nto previously published mCEs, it still provides enough evi-\ndence to draw conclusions about the models we evaluated.\nWe provide the top-1 accuracy on ILSVRC 2012 validation\nimages (Russakovsky et al., 2015) because this is the dataset\nused for creating ImageNet-C. This metric will help us\nunderstand how the model performs on a dataset without\ncorruptions.\nEach pre-trained model was also evaluated on the texture-\ncue conﬂict dataset from Geirhos et al. (2019) to calculate\nshape bias. The shape bias of a model is how much the\nmodel depends on shape when classifying images while\ntexture bias is how much the model depends on the texture.\nshape bias, as stated by Geirhos et al. (2019), is calculated\nby the following formula:\nSB = shapecorrect/(shapecorrect + texturecorrect)\n4. Results\nBy evaluating twenty different pre-trained models on a sub-\nset of ImageNet-C and on the Texture-Cue Conﬂict dataset,\nwe expose the robustness and inductive biases for each of\nthese models. Please refer to our appendix for more in depth\nresults from our experiments.\n4.1. Corruption Robustness\nWe evaluated every pre-trained model on ImageNet-C and\ncalculated the mCE to understand how each model per-\nformed against common corruptions in Table 1. We group\nthe pre-trained models by the type of architecture. For ex-\nample, the ResNet-50, AlexNet, GoogLeNet, and VGG16\nare all types of CNNs and are grouped together in Table 1.\nWhen referring to Table 1, a lower mCE is more favorable\nand a higher top-1 accuracy is more favorable.\nWe observe that MLP-Mixer models perform similarly to the\nCNNs when tested on ImageNet-C. All of the vision trans-\nformer models we evaluated achieved a signiﬁcantly better\nmCE than the MLP-Mixers and CNNs. One signiﬁcant ob-\nservation is that the tiny DeiT vision transformer with only\nﬁve million parameters achieves an mCE of 60.08% while\na ResNet50 with approximately ﬁve times the parameters\nhas an mCE of 65.54%. Overall, the model that achieved\nthe lowest mCE at 34.63% was the large Swin Transformer\nwith 197 million parameters. This model also performed the\nbest on the uncorrupted ILSVRC 2012 validation set with a\ntop-1 accuracy of 85.92%. We suspect the Swin transformer\nperformed the best out of all of the vision transformers be-\ncause of its shifting windows feature providing a global\nrepresentation of the image.\nExploring Corruption Robustness: Inductive Biases in Vision Transformers and MLP-Mixers\nTable 1.Evaluating Convolutional Neural Networks against Vi-\nsion Transformer Architectures and MLP-Mixers on ImageNet-C.\nmCE is calculated using only severity 2 and 5. Top-1 accuracy is\ncalculated for ILSVRC 2012 validation set.\nCONVOLUTIONAL NEURAL NETWORKS\nMODEL TOP -1(%) MCE(%) # PARAMS (M)\nRESNET-50 76.02 65 .54 26\nALEX NET 56.44 83 .18 61\nGOOG LENET 71.70 68 .82 7\nVGG-16 69.63 75 .10 138\nMLP-M IXERS\nMODEL TOP -1(%) MCE(%) # PARAMS (M)\nMLP-M IXER B 72.53 65 .54 60\nMLP-M IXER L 68.25 69 .65 208\nVISION TRANSFORMER ARCHITECTURES\nMODEL TOP -1(%) MCE(%) # PARAMS (M)\nVIT BASE 75.73 58 .55 86\nVIT LARGE 79.16 49 .02 304\nDEIT BASE 81.84 42 .30 86\nDEIT BASE -DIST . 83.16 41 .19 87\nDEIT SMALL 79.68 47 .79 22\nDEIT SMALL -DIST . 81.05 46 .25 22\nDEIT TINY 71.92 60.08 5\nDEIT TINY-DIST . 74.38 57 .45 6\nCAIT S24 83.28 40 .59 47\nCAIT XXS 24 78.38 49 .28 11\nSWIN -T TINY 80.85 50 .70 28\nSWIN -T SMALL 82.96 45 .51 50\nSWIN -T BASE 84.90 38 .52 88\nSWIN -T LARGE 85.92 34.63 197\n4.2. Shape Bias\nWe evaluated every pre-trained model on the Texture-Cue\nConﬂict dataset and calculated shape bias to understand\nwhether models were biased towards shape or texture when\nmaking decisions. When referring to Table 2, a higher shape\nbias is more favorable.\nWe observe that the MLP-Mixers and vision transform-\ners are more biased towards shape than CNNs, and many\nof the vision transformer models perform similarly to the\nMLP-Mixers. Notably, the tiny Data-efﬁcient image trans-\nformer (DeiT tiny) architecture with approximately ﬁve\ntimes fewer parameters than a ResNet-50 achieves a shape\nbias of 29.37% compared to 26.17% for the ResNet50. The\nbest performing vision transformer was the large ViT model\nwith a shape bias of 55.35% and 304 million parameters.\nTable 1 and Table 2 highlight a general inverse relationship\nbetween shape bias and mean corruption error. As a model\nis more robust to common corruptions (smaller mCE), its\nshape bias increases. We do not observe any relationship be-\nTable 2.Evaluating shape bias of Convolutional Neural Networks\nagainst Vision Transformer Architectures and MLP-Mixers on\nTexture-Cue Conﬂict dataset.\nCONVOLUTIONAL NEURAL NETWORKS\nMODEL SHAPE BIAS (%) # PARAMS (M)\nRESNET-50 26.17 26\nALEX NET 29.80 61\nGOOG LENET 28.52 7\nVGG-16 16.12 138\nMLP-M IXERS\nMODEL SHAPE BIAS (%) # PARAMS (M)\nMLP-M IXER BASE 36.90 60\nMLP-M IXER LARGE 38.64 208\nVISION TRANSFORMER ARCHITECTURES\nMODEL SHAPE BIAS (%) # PARAMS (M)\nVIT BASE 49.10 86\nVIT LARGE 55.35 304\nDEIT BASE 42.32 86\nDEIT BASE -DIST . 39.62 87\nDEIT SMALL 38.26 22\nDEIT SMALL -DIST . 36.65 22\nDEIT TINY 29.37 5\nDEIT TINY-DIST . 31.06 6\nCAIT S24 38.65 47\nCAIT XXS 24 34.24 11\nSWIN -T TINY 25.21 28\nSWIN -T SMALL 27.43 50\nSWIN -T BASE 36.39 88\nSWIN -T LARGE 40.20 197\ntween the shape bias or mCE and the number of parameters.\n5. Conclusion and Future Work\nWe compare several state-of-the-art vision transformers\nagainst CNNs and MLP-Mixers to better understand how\nthese different architectures handle corruptions and if they\nrely on shape or texture more when classifying images. As\nseen in the graph in our appendix, we generally observe that\nwhen a model has a strong bias towards shape, it is more\nrobust to common corruptions such as those in ImageNet-C.\nThis conclusion concurs with Geirhos et al. (2019).\nFuture directions include incorporating the rest of the sever-\nity levels from ImageNet-C to calculate the ﬁnal mean\ncorruption error for each model. It would also be bene-\nﬁcial to investigate different datasets such as ImageNet-A\n(Hendrycks et al., 2021), ImageNet-P (Hendrycks & Diet-\nterich, 2019), and ImageNet-R (Hendrycks et al., 2020) to\nnarrow down which speciﬁc components of these architec-\ntures are robust against all corruptions and perturbations.\nExploring Corruption Robustness: Inductive Biases in Vision Transformers and MLP-Mixers\nReferences\nBhojanapalli, S., Chakrabarti, A., Glasner, D., Li, D., Un-\nterthiner, T., and Veit, A. Understanding robustness of\ntransformers for image classiﬁcation, 2021.\nBrochu, F. Increasing shape bias in imagenet-trained net-\nworks using transfer learning and domain-adversarial\nmethods. CoRR, abs/1907.12892, 2019. URL http:\n//arxiv.org/abs/1907.12892.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,\nM., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby,\nN. An image is worth 16x16 words: Transformers for\nimage recognition at scale. In International Conference\non Learning Representations, 2021. URL https://\nopenreview.net/forum?id=YicbFdNTTy.\nFeinman, R. and Lake, B. M. Learning inductive biases with\nsimple neural networks. CoRR, abs/1802.02745, 2018.\nURL http://arxiv.org/abs/1802.02745.\nGeirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wich-\nmann, F. A., and Brendel, W. Imagenet-trained CNNs\nare biased towards texture; increasing shape bias im-\nproves accuracy and robustness. In International Confer-\nence on Learning Representations, 2019. URL https:\n//openreview.net/forum?id=Bygh9j09KX.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\nlearning for image recognition, 2015.\nHendrycks, D. and Dietterich, T. Benchmarking neural\nnetwork robustness to common corruptions and perturba-\ntions. In International Conference on Learning Represen-\ntations, 2019. URL https://openreview.net/\nforum?id=HJz6tiCqYm.\nHendrycks, D. and Dietterich, T. G. Benchmarking neural\nnetwork robustness to common corruptions and pertur-\nbations. CoRR, abs/1807.01697, 2018. URL http:\n//arxiv.org/abs/1807.01697.\nHendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F.,\nDorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M.,\nSong, D., Steinhardt, J., and Gilmer, J. The many faces\nof robustness: A critical analysis of out-of-distribution\ngeneralization. arXiv preprint arXiv:2006.16241, 2020.\nHendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and\nSong, D. Natural adversarial examples, 2021.\nHermann, K. L. and Kornblith, S. Exploring the origins\nand prevalence of texture bias in convolutional neural\nnetworks. CoRR, abs/1911.09071, 2019. URL http:\n//arxiv.org/abs/1911.09071.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Ima-\ngenet classiﬁcation with deep convolutional neural\nnetworks. In Pereira, F., Burges, C. J. C., Bottou,\nL., and Weinberger, K. Q. (eds.), Advances in Neural\nInformation Processing Systems, volume 25. Curran As-\nsociates, Inc., 2012. URL https://proceedings.\nneurips.cc/paper/2012/file/\nc399862d3b9d6b76c8436e924a68c45b-Paper.\npdf.\nLi, Y ., Yu, Q., Tan, M., Mei, J., Tang, P., Shen, W., Yuille,\nA., and cihang xie. Shape-texture debiased neural net-\nwork training. In International Conference on Learning\nRepresentations, 2021. URL https://openreview.\nnet/forum?id=Db4yerZTYkz.\nLiu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., Lin,\nS., and Guo, B. Swin transformer: Hierarchical vision\ntransformer using shifted windows, 2021.\nMummadi, C. K., Subramaniam, R., Hutmacher, R., Vi-\ntay, J., Fischer, V ., and Metzen, J. H. Does enhanced\nshape bias improve neural network robustness to common\ncorruptions? In International Conference on Learning\nRepresentations, 2021. URL https://openreview.\nnet/forum?id=yUxUNaj2Sl.\nRitter, S., Barrett, D. G., Santoro, A., and Botvinick, M. M.\nCognitive psychology for deep neural networks: A shape\nbias case study. In International conference on machine\nlearning, pp. 2940–2949. PMLR, 2017.\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,\nMa, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,\nM., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale\nVisual Recognition Challenge. International Journal of\nComputer Vision (IJCV), 115(3):211–252, 2015. doi:\n10.1007/s11263-015-0816-y.\nSimonyan, K. and Zisserman, A. Very deep convolutional\nnetworks for large-scale image recognition, 2015.\nSzegedy, C., Liu, W., Jia, Y ., Sermanet, P., Reed, S.,\nAnguelov, D., Erhan, D., Vanhoucke, V ., and Rabinovich,\nA. Going deeper with convolutions, 2014.\nTolstikhin, I., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai,\nX., Unterthiner, T., Yung, J., Keysers, D., Uszkoreit, J.,\nLucic, M., et al. Mlp-mixer: An all-mlp architecture for\nvision. arXiv preprint arXiv:2105.01601, 2021.\nTouvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles,\nA., and J ´egou, H. Training data-efﬁcient image trans-\nformers & distillation through attention. arXiv preprint\narXiv:2012.12877, 2020.\nTouvron, H., Cord, M., Sablayrolles, A., Synnaeve, G., and\nJ´egou, H. Going deeper with image transformers, 2021.\nExploring Corruption Robustness: Inductive Biases in Vision Transformers and MLP-Mixers\nWightman, R. Pytorch image models. https://github.\ncom/rwightman/pytorch-image-models,\n2019.\nExploring Corruption Robustness: Inductive Biases in Vision Transformers and MLP-Mixers\nFigure 1.Shape Bias & Number of Parameters by Pre-trained Model\nFigure 2.Mean Corruption Error & Number of Parameters by Pre-trained Model\nFigure 3.Relationship between mean Corruption Error and Shape Bias\nNote: Please visit our Notion to see our reported numbers for every subclass in ImageNet-C for severity 2 and severity 5.\nCamera ready paper will present these numbers in table/visualizations format in this appendix.",
  "topic": "Robustness (evolution)",
  "concepts": [
    {
      "name": "Robustness (evolution)",
      "score": 0.6104282736778259
    },
    {
      "name": "Transformer",
      "score": 0.5703523755073547
    },
    {
      "name": "Computer science",
      "score": 0.47605061531066895
    },
    {
      "name": "Inductive bias",
      "score": 0.43913114070892334
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3655657470226288
    },
    {
      "name": "Computer vision",
      "score": 0.323361337184906
    },
    {
      "name": "Electrical engineering",
      "score": 0.2941446900367737
    },
    {
      "name": "Engineering",
      "score": 0.23318281769752502
    },
    {
      "name": "Voltage",
      "score": 0.18426096439361572
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    },
    {
      "name": "Multi-task learning",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I170201317",
      "name": "University of Pittsburgh",
      "country": "US"
    }
  ]
}