{
  "title": "Categorizing Vaccine Confidence With a Transformer-Based Machine Learning Model: Analysis of Nuances of Vaccine Sentiment in Twitter Discourse",
  "url": "https://openalex.org/W3186243414",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5079331687",
      "name": "Per Egil Kummervold",
      "affiliations": [
        "Fundación para el Fomento de la Investigación Sanitaria y Biomédica de la Comunitat Valenciana"
      ]
    },
    {
      "id": "https://openalex.org/A5032272865",
      "name": "Sam Martin",
      "affiliations": [
        "London School of Hygiene & Tropical Medicine",
        "University College London",
        "University of Oxford",
        "Wellcome Centre for Ethics and Humanities"
      ]
    },
    {
      "id": "https://openalex.org/A5019469077",
      "name": "Sara Dada",
      "affiliations": [
        "London School of Hygiene & Tropical Medicine",
        "University College Dublin"
      ]
    },
    {
      "id": "https://openalex.org/A5079945535",
      "name": "Eliz Kilich",
      "affiliations": [
        "London School of Hygiene & Tropical Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A5004183471",
      "name": "Chermain Denny",
      "affiliations": [
        "Vrije Universiteit Amsterdam"
      ]
    },
    {
      "id": "https://openalex.org/A5054098675",
      "name": "Pauline Paterson",
      "affiliations": [
        "Health Protection Scotland",
        "London School of Hygiene & Tropical Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A5022662591",
      "name": "Heidi J. Larson",
      "affiliations": [
        "Chatham House",
        "Health Protection Scotland",
        "London School of Hygiene & Tropical Medicine",
        "Institute for Health Metrics and Evaluation",
        "University of Washington"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2899504823",
    "https://openalex.org/W2618892436",
    "https://openalex.org/W2888571776",
    "https://openalex.org/W2025330985",
    "https://openalex.org/W2963702064",
    "https://openalex.org/W2347127863",
    "https://openalex.org/W3048276999"
  ],
  "abstract": "Background Social media has become an established platform for individuals to discuss and debate various subjects, including vaccination. With growing conversations on the web and less than desired maternal vaccination uptake rates, these conversations could provide useful insights to inform future interventions. However, owing to the volume of web-based posts, manual annotation and analysis are difficult and time consuming. Automated processes for this type of analysis, such as natural language processing, have faced challenges in extracting complex stances such as attitudes toward vaccination from large amounts of text. Objective The aim of this study is to build upon recent advances in transposer-based machine learning methods and test whether transformer-based machine learning could be used as a tool to assess the stance expressed in social media posts toward vaccination during pregnancy. Methods A total of 16,604 tweets posted between November 1, 2018, and April 30, 2019, were selected using keyword searches related to maternal vaccination. After excluding irrelevant tweets, the remaining tweets were coded by 3 individual researchers into the categories Promotional, Discouraging, Ambiguous, and Neutral or No Stance. After creating a final data set of 2722 unique tweets, multiple machine learning techniques were trained on a part of this data set and then tested and compared with the human annotators. Results We found the accuracy of the machine learning techniques to be 81.8% (F score=0.78) compared with the agreed score among the 3 annotators. For comparison, the accuracies of the individual annotators compared with the final score were 83.3%, 77.9%, and 77.5%. Conclusions This study demonstrates that we are able to achieve close to the same accuracy in categorizing tweets using our machine learning models as could be expected from a single human coder. The potential to use this automated process, which is reliable and accurate, could free valuable time and resources for conducting this analysis, in addition to informing potentially effective and necessary interventions.",
  "full_text": "Original Paper\nCategorizing Vaccine Confidence With a Transformer-Based\nMachine Learning Model: Analysis of Nuances of Vaccine\nSentiment in Twitter Discourse\nPer E Kummervold1, PhD; Sam Martin2,3,4,5, LLB, MSc, PhD; Sara Dada4,6, MSc; Eliz Kilich4, BMBCh; Chermain\nDenny7, BSc; Pauline Paterson4,8, PhD; Heidi J Larson4,8,9,10, PhD\n1Vaccine Research Department, FISABIO-Public Health, Valencia, Spain\n2Centre for Clinical Vaccinology and Tropical Medicine, University of Oxford, Oxford, United Kingdom\n3Rapid Research Evaluation and Appraisal Lab, Departament of Targeted Intervention, University College London, London, United Kingdom\n4Faculty of Epidemiology and Population Health, London School of Hygiene & Tropical Medicine, London, United Kingdom\n5Ethox Centre, Nuffield Department of Population Health, Big Data Institute, University of Oxford, Oxford, United Kingdom\n6UCD Centre for Interdisciplinary Research, Education and Innovation in Health Systems, School of Nursing, Midwifery and Health Systems, University\nCollege Dublin, Dublin, Ireland\n7Faculty of Science, Vrije Universiteit Amsterdam, Amsterdam, Netherlands\n8NIHR Health Protection Research Unit, London, United Kingdom\n9Institute of Health Metrics and Evaluation, University of Washington, Seattle, WA, United States\n10Chatham House Centre on Global Health Security, The Royal Institute of International Affairs, London, United Kingdom\nCorresponding Author:\nPer E Kummervold, PhD\nVaccine Research Department\nFISABIO-Public Health\nAvda. de Catalunya, 21\nValencia, 46020\nSpain\nPhone: 34 41435795\nEmail: per@capia.no\nAbstract\nBackground: Social media has become an established platform for individuals to discuss and debate various subjects, including\nvaccination. With growing conversations on the web and less than desired maternal vaccination uptake rates, these conversations\ncould provide useful insights to inform future interventions. However, owing to the volume of web-based posts, manual annotation\nand analysis are difficult and time consuming. Automated processes for this type of analysis, such as natural language processing,\nhave faced challenges in extracting complex stances such as attitudes toward vaccination from large amounts of text.\nObjective: The aim of this study is to build upon recent advances in transposer-based machine learning methods and test whether\ntransformer-based machine learning could be used as a tool to assess the stance expressed in social media posts toward vaccination\nduring pregnancy.\nMethods: A total of 16,604 tweets posted between November 1, 2018, and April 30, 2019, were selected using keyword searches\nrelated to maternal vaccination. After excluding irrelevant tweets, the remaining tweets were coded by 3 individual researchers\ninto the categories Promotional, Discouraging, Ambiguous, and Neutral or No Stance. After creating a final data set of 2722\nunique tweets, multiple machine learning techniques were trained on a part of this data set and then tested and compared with\nthe human annotators.\nResults: We found the accuracy of the machine learning techniques to be 81.8% (F score=0.78) compared with the agreed score\namong the 3 annotators. For comparison, the accuracies of the individual annotators compared with the final score were 83.3%,\n77.9%, and 77.5%.\nConclusions: This study demonstrates that we are able to achieve close to the same accuracy in categorizing tweets using our\nmachine learning models as could be expected from a single human coder. The potential to use this automated process, which is\nJMIR Med Inform 2021 | vol. 9 | iss. 10 | e29584 | p. 1https://medinform.jmir.org/2021/10/e29584\n(page number not for citation purposes)\nKummervold et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nreliable and accurate, could free valuable time and resources for conducting this analysis, in addition to informing potentially\neffective and necessary interventions.\n(JMIR Med Inform 2021;9(10):e29584) doi: 10.2196/29584\nKEYWORDS\ncomputer science; information technology; public health; health humanities; vaccines; machine learning\nIntroduction\nBackground\nAlthough individuals have been found to share different\nthoughts, questions, and concerns about vaccines on social\nmedia [1], studies of the vaccine discourse on social media [2]\nindicate that concerns, and indeed the sharing of misinformation\nin particular, are amplified [3]. What is of concern is the number\nof imprecise and inaccurate articles available with regard to\nvaccinations.\nMultiple studies have been conducted to monitor vaccination\ndiscussions on social media [4-6]. Addressing misunderstandings\nand inaccuracies as early as possible is vital to making sound\nvaccine policies. However, there is currently insufficient\nresearch on how to effectively categorize the nuances in the\nperceptions of sentiment toward vaccines in the large volume\nof vaccine data shared daily on social media. Being able to\nmonitor and understand the spread and traction of\nmisinformation in social media on a larger global scale is key\nto mitigating the negative impact of such information.\nAlthough the data retrieved from social and news media might\nnot be representative of the entire population, they provide a\nsnapshot of discussions and thoughts, and the trends observed\nhere are still thought to be of vital importance to understanding\nemerging issues of concern as well as the link between\nmisinformation on news and social media platforms and the\neffect of this misinformation on vaccination confidence and\nuptake. To detect such trends, however, we need an in-depth\nunderstanding of the content of these messages. Although\nqualitative methods might provide this insight, the sheer volume\nof news and social media content makes it difficult to apply\nthese methods to conversations among entire populations over\ntime. Machine learning and natural language processing (NLP)\nhave the potential to handle huge amounts of information.\nHowever, concerns over accuracy, especially when dealing with\nthe complexity of the language used to express opinions about\nvaccines, have prevented these methods from being very\neffective.\nSentiment analysis in machine learning refers to the process of\nautomatically determining whether the author of a piece of text\nis in favor of, against, or neutral toward the subject of the\nstatement. This is slightly different from stance detection, which\ninvolves automatically determining the author’s attitude toward\na proposition or target [7]. Although sentiment analysis can\nlook only at the tone of a particular statement, stance detection\noften refers to a target outside of the particular statement.\nThe author of a tweet could express a positive attitude toward\nvaccination by expressing negativity toward people opposing\nvaccines (for instance, so-called antivaxxers). This double\nnegation would then be interpreted as positive or promotional.\nThis could be referred to as the author’s sentiment toward\nvaccination, but because sentiment is often used for referring\nto the sentiment of the statement, we find it less confusing to\nrefer to this as the author’s stance toward vaccination. This\ndistinction is particularly important when studying an issue as\ncomplex as vaccination because many texts often express strong\nopinions about vaccination without addressing vaccines directly.\nThe distinction can be illustrated using the examples presented\nin Table 1.\nHistorically, NLP has often focused on ordinary sentiment\nanalysis. This is technically a much easier task, but it is less\nuseful from a sociological point of view. In contrast to sentiment,\na person’s stance toward a target can be expressed using either\nnegative or positive language. People could, for instance, switch\nfrom opposing abortion to promoting prolife without changing\ntheir basic stance. In a sociological analysis, we would usually\nbe more interested in the stance that people have toward a topic\nor target than the sentiment expressed in a particular statement.\nTable 1. Difference between sentiment and stance.\nStance (target)Sentiment (subject)Text\nPositive (vaccines)Positive (vaccines)Vaccines save lives\nPositive (vaccines)Negative (antivaxxers)Antivaxxers kill people with their misinformation\nPositive (vaccines)Positive (physician’s knowledge)Trust your doctor’s knowledge regarding vaccines\nNegative (vaccines)Positive (antivaxxers)Antivaxxers tell the real truth about vaccines\nObjective\nThe aim of this study is to look more deeply into attitudes\ntoward maternal vaccination and how well the task of detecting\nstance in tweets can be accomplished by using multiple machine\nlearning methods. We attempt to quantify how accurately such\ntweets can be categorized by trained annotators and how this\ncompares with newer machine learning methods.\nJMIR Med Inform 2021 | vol. 9 | iss. 10 | e29584 | p. 2https://medinform.jmir.org/2021/10/e29584\n(page number not for citation purposes)\nKummervold et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nMethods\nOverview\nThis research collected 16,605 Twitter messages (tweets)\npublished over 6 months between November 1, 2018, and April\n30, 2019, from Meltwater [8], a media intelligence system. This\ndata set was collected and coded to complement a larger research\nstudy on sentiments and experiences around maternal\nvaccination across 15 countries (Australia, Brazil, Canada,\nFrance, Germany, India, Italy, Mexico, Panama, South Africa,\nSouth Korea, Spain, Taiwan, the United Kingdom, and the\nUnited States). Non-English tweets were translated into English\nusing Google Translate script (Alphabet Inc). Multimedia\nAppendix 1 includes the search queries used in this study. Before\nannotating, all usernames and links were replaced by a common\ntag. This served two purposes: it preserved anonymity, and it\nlimited potential bias based on the coder’s interpretation of the\nusername. The target of the analysis should be to decipher what\nthe text is actually telling the reader about the writer’s stance\ntoward vaccination.\nIn this study, maternal vaccination typically refers to the\nvaccines that are recommended by health authorities for\npregnant women.\nIndividual tweets were manually coded into stance categories\n(Textbox 1). Stance was categorized across four sentiments\ntoward maternal vaccines: Promotional (in favor of maternal\nvaccines), Ambiguous(uncertainty with mixed sentiment toward\nmaternal vaccines), Discouraging (against maternal vaccines),\nand No stance (statements or facts about maternal vaccines that\ndo not reveal the author’s stance). Although it can be argued\nthat some of the categories can be ordered, we treated them as\nnominal variables, not ordinal variables, in the analysis.\nTherefore, a tweet stating that pregnant women should take the\ntetanus vaccine but not the measles vaccine is considered a\npromotional post in favor of maternal vaccines because it\nencourages following the current health recommendations.\nTextbox 1. Stance categorized across four sentiments toward maternal vaccines.\nStance categories and their definitions\n• Promotional\n• Posts communicate public health benefits or safety of maternal vaccination.\n• Posts contain positive tones, supportive or encouraging toward maternal vaccination.\n• Ambiguous\n• Posts contain indecision and uncertainty on the risks or benefits of maternal vaccination, or they are ambiguous.\n• Posts contain disapproving and approving information.\n• Posts describe risks of not vaccinating during pregnancy.\n• Posts refute claims that maternal vaccines are dangerous.\n• Discouraging\n• Posts contain negative attitudes toward, or arguments against maternal vaccines.\n• Posts contain questions regarding effectiveness or safety or possibility of adverse reactions (eg, links to disability or autism).\n• Posts discourage the use of recommended maternal vaccines.\n• Neutral or no stance\n• Posts contain no elements of uncertainty or promotional or negative content. These are often not sentiments expressed on the web but rather\nstatements that are devoid of emotion. This category includes factual posts pointing to articles on maternal vaccines (eg, Study on effectiveness\nof maternal flu vaccine).\nCleaning the Data Set\nAfter the initial annotating, the data set was cleaned for\nduplicates and semiduplicates. Semiduplicates are tweets in\nwhich a few characters differ but the meaning is unchanged. A\ntypical example is a retweeted post with the RT: prefix. Another\nexample is a tweet suffixed (by a user or bot) with a few random\ncharacters to avoid being recognized (by Twitter detection\nalgorithms) as a mass posting. To detect semiduplicates, we\nused a nonnormalized Levenshtein distance of less than 30 for\ntweets with more than 130 characters. For shorter tweets, the\ndistance was scaled. The validity of the deduplication algorithm\nwas qualitatively evaluated by the annotators. We were aiming\nfor a greedy algorithm that identified too many semiduplicates\nrather than too few. Although this could slightly affect the size\nof the training set, it was considered to be of greater importance\nto prevent tweets that looked too similar from being included\nin both the training and test data sets. We have open sourced\nthe Python code that we developed for cleaning and removing\nduplicates and made it available in our web-based GitHub\nrepository [9].\nAs the stance of the tweet should be determined solely based\non the content of the text, we deidentified usernames and URLs.\nApart from cleaning usernames and URLs, the aim was to ensure\nthat the input to the machine learning algorithm was exactly the\nJMIR Med Inform 2021 | vol. 9 | iss. 10 | e29584 | p. 3https://medinform.jmir.org/2021/10/e29584\n(page number not for citation purposes)\nKummervold et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nsame as that presented to the human annotators. In this respect,\nthis served to both preserve anonymity and limit potential bias\narising from the annotator’s interpretation of the usernames.\nThis also helped fulfill ethical responsibilities as well as the\nEuropean Union’s General Data Protection Regulation\nguidelines. In all, 3 independent annotators screened all posts\nfor inclusion, excluding posts that were not about the vaccines\nadministered during pregnancy. The annotators also met to agree\non the final posts included in the analysis [10].\nDeduplication was conducted after the first round of annotating,\nand the annotators were then asked to recode any tweet for\nwhich they had provided inconsistent annotating. For example,\nthere were instances where the same coder coded identical\ntweets inconsistently. From the tweets that appeared only twice\nin the material, we calculated a self-agreement score both for\ninclude or exclude and for stance. This was done to illustrate\nsome of the potential challenges of manual annotating (Figure\n1).\nFigure 1. Screening and annotating procedure. DEV: development data set; I/E: include or exclude.\nBidirectional Encoder Representations From\nTransformers\nThe main model was based on the newest (May 2019) Whole\nWord Masking variant of Google’s Bidirectional Encoder\nRepresentations from Transformers (BERT) [11]. When\npublished in late 2018, the model demonstrated state-of-the-art\nresults on 11 NLP tasks, including the Stanford Question\nAnswering Dataset version 1.1 (The Stanford Natural Language\nProcessing Group) [12]. BERT is a bidirectional, contextual\nencoder built on a network architecture called Transformer,\nwhich is based solely on attention mechanisms [13]. The main\npart of the training can be performed on unsupervised and\nunlabeled text corpora such as Wikipedia, and the pretrained\nweights [14] are trained solely on this general corpus.\nWe trained the model on a domain-specific corpus to expose\nthe model to the vocabulary that is typically used in vaccination\nposts. We started creating domain-specific pretraining data by\ndownloading 5.9 million tweets acquired by keyword searches\nrelated to vaccine and vaccination (Multimedia Appendix 2).\nThe set was downloaded from Meltwater and preprocessed in\nthe same way as the maternal vaccine tweets (ie, deduplication\nand username or link anonymization). The BERT architecture\ndepends on carrying out unsupervised pretraining using a\ntechnique called Masked Language Modeling and Next Sentence\nPrediction. The latter method requires each text segment to have\nJMIR Med Inform 2021 | vol. 9 | iss. 10 | e29584 | p. 4https://medinform.jmir.org/2021/10/e29584\n(page number not for citation purposes)\nKummervold et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nat least two sentences. Therefore, we filtered out all tweets that\ndid not satisfy this criterion, reducing the data set from 1.6\nmillion to 1.1 million tweets. A later study by Liu et al [15]\npointed out that the Next Sentence Prediction task is not as\neffective as expected and that it might be beneficial to train the\nmodel only on the Masked Language Modeling task. As we\nhave such a large number of short texts, this would have\nextended our data set. We refer to this data set as the\nvaccine-tweet data set.\nWe tokenized the tweets using the BERT vocabulary and limited\nthe sequence length to 96 tokens. By limiting the sequence\nlength, we were able to increase the batch size, which is known\nto have a positive effect on performance. Figure 2 shows the\nsequence length of the downloaded tweets, showing that this\ntrimming would affect less than one in thousand tweets. Tweets\nlonger than 96 tokens were manually examined, confirming that\nthese were mainly repetitive sequences and that the trimming\ndid not affect the meaning (eg, a statement followed by strings\nof varying lengths of repeated characters such as ...... or ????).\nWe have further addressed discourse distribution, labeling, and\nword balance in the word corpus in the study by Martin et al\n[10].\nIn addition, we acquired a data set with a total of 201,133\nvaccine-related news articles from the Vaccine Confidence\nProject (London School of Hygiene & Tropical Medicine) media\narchive. The articles were collected by automated keyword\nsearches from several sources, including Google News,\nHealthMap, and Meltwater. It is an extensive collection of\nvaccine-related articles in English from both news media and\nblogs. The search criteria have been developed over the years,\nwhich is why they have varied slightly, but they are very similar\nto the list presented in Multimedia Appendix 2. We refer to this\ndata set as the vaccine news data set. We chose not to pretrain\nthe model on a maternal vaccine–specific data set because we\nwanted the encoder representations to also be used on other\nvaccine-related topics. All pretraining was carried out using a\nlearning rate of 2e-5, a batch size of 216, and a maximum\nsequence length of 96.\nThese domain-specific pretrained weights were the starting\npoints for the classification of the maternal vaccination tweets.\nThe manually classified maternal vaccination tweets were\npreprocessed in the same way as the tweets in the vaccine-tweet\ndata set and then divided into training, development, and test\ndata sets in the ratio 60:20:20 (N=1633:545:544).\nFigure 2. Number of tokens in each tweet (count per million tweets).\nFine-tuning\nThe pretraining of transposer models is a very slow process,\nbut when these pretrained weights are determined, the final\nfine-tuning step is fast. To our knowledge, the best way of\ncomparing the various pretrained models is by comparing their\nperformances after fine-tuning. Figure 3 shows that the\nfine-tuning did not improve performance after 15 epochs but\nthat there was considerable variance among the runs. For this\nreason, all pretrained models were evaluated using an average\nof 10 fine-tuned runs each at 15 epochs.\nJMIR Med Inform 2021 | vol. 9 | iss. 10 | e29584 | p. 5https://medinform.jmir.org/2021/10/e29584\n(page number not for citation purposes)\nKummervold et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nTo obtain a baseline score for comparative machine learning\nmodels, various traditional and well-established networks were\ntrained. The aim was to use well-established networks with\nknown performance against standardized data sets for sentiment\nand stance analysis. The benchmark architectures, the neural\nnetwork, and the long short-term memory (LSTM) networks\nwith and without Global Vectors for Word Representation\n(GloVe; The Stanford Natural Language Processing Group)\nword embeddings were all taken from Deep Learning With\nPython by Chollet [16].\nTo verify that the neural network was able to solve other neural\nnetwork tasks, we tested the network structures on one of the\nmost basic NLP tasks: predicting positive and negative\nsentiments in IMDb (Amazon, Inc) movie reviews [17].\nThe final domain-specific pretraining and fine-tuning were\ncarried out on a Cloud TPU (Tensor Processing Unit) v2-8 node\nwith 8 cores and 64 gibibytes memory and an estimated\nperformance of 180 teraflops (Google Cloud). Domain-specific\npretraining was carried out for 2 weeks, but, as shown in Figure\n3, there were no measurable improvements after a few days of\nrunning. Fine-tuning requires fewer computing resources and\nis usually completed in a few minutes on this platform.\nFigure 3. Data evaluation of pretraining accuracy.\nResults\nOverview\nIn total, 3 annotators individually coded 2722 tweets. Of these\n2722 tweets, 1559 (57.27%) were coded identically, with a\nFleiss agreement score of κ=0.56. After meeting and discussing\nthe tweets that they disagreed on, the annotators agreed on the\nannotating of all the remaining tweets. Although the annotators\nagreed on a final category for every tweet, they also reported\nthat 6.83% (186/2722) of tweets “could be open to\ninterpretation.” Comparing the final agreed annotating after the\ndiscussions with the annotators’ initial annotating, the accuracies\nof the individual annotators were 83.3%, 77.9%, and 77.5%.\nThe accuracy of the machine learning model was also calculated\nwith regard to the final agreed annotating.\nOne of the basic neural networks for NLP consists of two fully\nconnected layers. For our data set, this only provided an\naccuracy of 43.7%. Thus, the network was not able to obtain a\nbetter result than simply predicting the overrepresented task\nPromotional for all data points. Adding pretrained GloVe word\nembeddings to this structure resulted in a slightly better\nperformance, with a maximum accuracy of 55.5% on the test\ndata set. However, both approaches overtrained after just a few\nepochs with data sets of this size.\nTo evaluate the reason for this low accuracy, we tested the same\nnetwork on the IMDb data set, setting the same number of\ntraining examples (N=1633). In this case, the network achieved\nan accuracy of more than 80% even without the GloVe word\nembeddings, showing that the low accuracy was related to the\ndifficulty of the maternal vaccine categorization.\nJMIR Med Inform 2021 | vol. 9 | iss. 10 | e29584 | p. 6https://medinform.jmir.org/2021/10/e29584\n(page number not for citation purposes)\nKummervold et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nThe modern LSTM model is a recurrent neural network with a\nmemory module. This model architecture was considered\nstate-of-the-art a couple of years ago. We were able to obtain\nan accuracy of 63.1% here and can improve this to 65.5% by\nadding pretrained GloVe word embeddings.\nMain Research Target\nOur main research target was to investigate whether\nstate-of-the-art NLP models could be improved by using the\nBERT architecture. Using the original pretrained weights, we\nachieved an average accuracy of 76.7% when fine-tuning for\n15 epochs.\nStarting from the original weights, the model weights were\npretrained on the larger vaccine news data set for 1 million\nepochs. At various checkpoints (0 E, 250,000 E, 500,000 E,\n750,000 E, and 1,000,000 E), the model was forked and then\ntrained on the smaller and more specific vaccine-tweet data set.\nAt each of the checkpoints, the network was fine-tuned on the\nmanually labeled tweets for 15 epochs, and the average of 10\nruns is shown in Figure 4. Using pretraining on domain-specific\ncontent, the accuracy reached a peak of approximately 79%\nwhen training only on the vaccine news data set. However, by\ntraining first on the vaccine news data set for 250,000 epochs\nand then on the vaccine news data set for an additional 200,000\nepochs, we were able to obtain an accuracy of 81.8%. For a\ncomparison of accuracies, see Table 2.\nFigure 4. Average of pretraining accuracy.\nTable 2. Accuracy comparisons.\nF scoreAccuracy\n0.7393960.795861Coder average\n0.7962720.833211Coder 1: EK\n0.7103560.775165Coder 2: SCM\n0.7115590.779206Coder 3: SD and CD\n0.4366970.436697Neural network: no embeddings\n0.4578130.544954Neural network: GloVea word embeddings\n0.5499970.631193LSTMb: no embeddings\n0.5939420.655046LSTM+GloVe word embeddings\n0.7188780.766972dBERTc: default weights\n0.7758300.818349dBERT: domain-specific\naGloVe: Global Vectors for Word Representation.\nbLSTM: long short-term memory.\ncBERT: Bidirectional Encoder Representations from Transformers.\ndThe final accuracy scores for the Bidirectional Encoder Representations from Transformers–based models are based on selecting the best network\nfrom the results based on the results from the development data set. The reported numbers are from evaluating the training data set.\nJMIR Med Inform 2021 | vol. 9 | iss. 10 | e29584 | p. 7https://medinform.jmir.org/2021/10/e29584\n(page number not for citation purposes)\nKummervold et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nDiscussion\nPrincipal Findings\nThe categories chosen in this study underwent several revisions\nto ensure that they could be clearly understood. The annotators\nwere fluent English speakers with a postgraduate degree and\nseveral years of work experience in the field.\nSome of the nuances contained in the tweets meant that it was\ndifficult to categorize them as definitively one stance. Thus,\neven when the same coder was asked to code a nearly identical\ntweet at a later time, they chose the same code four out of five\ntimes. After being given a second opportunity to code all\nduplicate tweets that had inconsistencies, the annotators met\nand discussed the categories that they disagreed on. The average\nfinal accuracy was then 79.6%.\nIdeally, the correct annotating should be the annotating that an\naverage of a large number of experienced annotators would\nhave chosen. Limiting the number of annotators to 3 resulted\nin cases where all annotators by chance coded the same tweet\nidentically and erroneously and cases where none of the\nannotators chose the categories that a larger number of\nannotators would have chosen. It is therefore reasonable to\nassume that the accuracy of 79.6% might be slightly optimistic\nin terms of what can be expected from an average human coder,\neven one who has long experience in the area.\nThe task of annotating is challenging because it is open for\ninterpretation, which is a challenge that NLP also struggles with.\nOur tests showed that a simple neural network that had no\nproblem achieving an accuracy of more than 80% on an IMDb\nmovie review task was unable to predict anything better than\nthe most prevalent category when it was tested on maternal\nvaccination tweets.\nUnsurprisingly, LSTM networks perform better than ordinary\nneural networks. Pretrained embeddings help in all cases. Using\nGloVe word embeddings increases the accuracy. With the LSTM\nnetwork, we achieved an accuracy of 63.1% and improved this\nto 65.5% by adding pretrained GloVe word embeddings.\nHowever, LSTM networks still lag behind what could be\nconsidered human accuracy.\nIn contrast, transformer-based architectures perform significantly\nbetter. By using the pretrained openly available BERT weights,\nwe achieved an accuracy of 76.7%. This is approximately the\nsame level of accuracy achieved by the coder with the lowest\naccuracy in this study.\nDomain-specific pretraining also shows potential. Although\npretraining does require some computing power, it does not\nrequire manual annotating, which could entail high costs in\nterms of time and resources. It is also worth noting that we\ndeliberately trained the model only on general vaccine terms.\nWe did not perform optimizations specifically for the domain\nof maternal vaccines. The main reason for this is that we wanted\nweights that were transferable to other tasks in the field of\nvaccines.\nIn our setting, the best result of 81.8% accuracy was achieved\nafter initial training on news articles about vaccines and then\ntraining on vaccine-related tweets. This accuracy is better than\nthe average of the 3 annotators, even after the annotators had\ncarried out multiple annotatings of the same tweet and had been\ngiven the opportunity to recode any inconsistencies.\nIn our opinion, it is doubtful that any individual coder would\nachieve more than 90% accuracy on this task simply because\nit is difficult, even with a much larger number of annotators, to\nagree on an absolute categorization. There will always be tweets\nthat are open to interpretation, preventing a hard target of\nabsolute accuracy.\nLimitations\nWe used a limited data set, especially for the tweet data set\ncontaining only 1 million vaccine-related tweets. It is also\nreasonable to assume that pretraining on a larger data set of\nnon–vaccine-specific tweets could have a positive effect because\nthe language of tweets is quite different from that of other texts.\nEnlarging the data sets is an easy way of potentially increasing\nthe accuracy.\nAlthough they are accurate, transformer-based models are\ndemanding to run to analyze large amounts of text. This could\nbe a challenge when used for monitoring purposes. However,\nmost likely, this is a problem that will lessen in the future.\nAfter Google released BERT late in 2018, there have been\nmultiple general improvements made by Facebook, Microsoft,\nand Google to the model’s transformer-based architecture to\nimprove the base models [15,18,19]. These have not been\nimplemented in this study. There is currently significant research\nactivity in the field, and it is reasonable to assume that\nimplementing these improvements in the base model and\nrestarting the domain-specific pretraining checkpoints would\nlead to higher accuracy in our categorization.\nConclusions\nBeing able to categorize and understand the overall stance in\nsocial media conversations about vaccinations, especially in\nterms of identifying clusters of discouraging or ambiguous\nconversations, will make it easier to spot activities that may\nsignal vaccine hesitancy or a decline in vaccine confidence with\ngreater speed and more accuracy. To manually, and continually,\nmonitor these conversations in today’s information society is\nnear impossible. In that respect, it has always been obvious that\nNLP has huge potential because it can process an enormous\namount of textual information.\nHowever, so far, NLP has only been able to solve very easy\ntasks and is unable to handle the nuances in language related\nto complicated issues (eg, attitudes toward vaccination). The\nnew advances in transformer-based models indicate that they\nare about to become a useful tool in this area, opening up a new\narea for social research.\nWe have demonstrated that with a training data set of\napproximately 1600 tweets, we were able to obtain at least the\naccuracy that should be expected of a trained human coder in\ncategorizing the stance of maternal vaccination discussions on\nsocial media. Although there are benefits to increasing this\naccuracy even more, the main research challenge is to reduce\nthe number of training samples. So far, this has been an\nJMIR Med Inform 2021 | vol. 9 | iss. 10 | e29584 | p. 8https://medinform.jmir.org/2021/10/e29584\n(page number not for citation purposes)\nKummervold et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nunderprioritized area of research and an area where we should\nexpect advances in the future. The real benefit from the\ntechnology will first be apparent when we are able to do this\nkind of categorization with only a few initial examples. Being\nable to categorize text in large corpora gives us a new tool for\ntracking and ultimately understanding vaccine stance and\nsentiment.\nAcknowledgments\nPEK was funded by the European Commission for the call H2020-MSCA-IF-2017 and the funding scheme MSCA-IF-EF-ST for\nthe Vaccine Media Analytics project (grant agreement ID: 797876).\nHJL, PP, SM, SD, and EK were funded by a grant from GlaxoSmithKline to support research on maternal vaccination.\nThe funders had no role in the study design, data collection, analysis, interpretation, or writing of this paper. This research was\nsupported with Cloud TPUs (Tensor Processing Units) from Google’s TPU Research Cloud.\nConflicts of Interest\nHJL’s research group, the Vaccine Confidence Project (HJL, PP, SM, SD, and EK) received research funding from\nGlaxoSmithKline, Merck, and Johnson & Johnson. HJL served on the Merck Vaccines Strategic Advisory Board and received\nhonoraria for participating in GSK training sessions. None of the other authors have any conflicts of interest to declare.\nMultimedia Appendix 1\nMaternal vaccination keyword search.\n[DOCX File , 418 KB-Multimedia Appendix 1]\nMultimedia Appendix 2\nVaccination keyword search terms.\n[DOCX File , 7 KB-Multimedia Appendix 2]\nReferences\n1. Wilcox CR, Bottrell K, Paterson P, Schulz WS, Vandrevala T, Larson HJ, et al. Influenza and pertussis vaccination in\npregnancy: portrayal in online media articles and perceptions of pregnant women and healthcare professionals. Vaccine\n2018 Nov 29;36(50):7625-7631 [FREE Full text] [doi: 10.1016/j.vaccine.2018.10.092] [Medline: 30401620]\n2. Kang GJ, Ewing-Nelson SR, Mackey L, Schlitt JT, Marathe A, Abbas KM, et al. Semantic network analysis of vaccine\nsentiment in online social media. Vaccine 2017 Jun 22;35(29):3621-3638 [FREE Full text] [doi:\n10.1016/j.vaccine.2017.05.052] [Medline: 28554500]\n3. Broniatowski DA, Jamison AM, Qi S, AlKulaib L, Chen T, Benton A, et al. Weaponized health communication: Twitter\nbots and Russian trolls amplify the vaccine debate. Am J Public Health 2018 Oct;108(10):1378-1384. [doi:\n10.2105/AJPH.2018.304567] [Medline: 30138075]\n4. Salathé M, Khandelwal S. Assessing vaccination sentiments with online social media: implications for infectious disease\ndynamics and control. PLoS Comput Biol 2011 Oct;7(10):e1002199 [FREE Full text] [doi: 10.1371/journal.pcbi.1002199]\n[Medline: 22022249]\n5. Majumder N, Hazarika D, Gelbukh A, Cambria E, Poria S. Multimodal sentiment analysis using hierarchical fusion with\ncontext modeling. Knowl Based Syst 2018 Dec;161:124-133. [doi: 10.1016/j.knosys.2018.07.041]\n6. Smith MC, Dredze M, Quinn SC, Broniatowski DA. Monitoring real-time spatial public health discussions in the context\nof vaccine hesitancy. 2017. URL: http://ceur-ws.org/Vol-1996/paper2.pdf [accessed 2021-08-26]\n7. Mohammad S, Sobhani P, Kiritchenko S. Stance and Sentiment in Tweets. ACM Trans Internet Technol 2017 Jul\n14;17(3):1-23 [FREE Full text] [doi: 10.1145/3003433]\n8. Meltwater homepage. Meltwater. URL: https://www.meltwater.com/ [accessed 2021-08-26]\n9. VACMA GitHub repository. GitHub. URL: https://github.com/peregilk/VACMA-PUBLIC [accessed 2021-08-26]\n10. Martin S, Kilich E, Dada S, Kummervold PE, Denny C, Paterson P, et al. \"Vaccines for pregnant women…?! Absurd\" -\nmapping maternal vaccination discourse and stance on social media over six months. Vaccine 2020 Sep 29;38(42):6627-6637.\n[doi: 10.1016/j.vaccine.2020.07.072] [Medline: 32788136]\n11. Devlin J, Chang M, Lee K, Toutanova K. BERT: pre-training of Deep Bidirectional Transformers for Language\nUnderstanding. arXiv. Preprint posted online October 11, 2018 [FREE Full text]\n12. SQuAD2.0: The Stanford Question Answering Dataset. SQuAD. URL: https://rajpurkar.github.io/SQuAD-explorer/\n[accessed 2021-08-26]\n13. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez A, et al. Attention is all you need. arXiv. Preprint posted\nonline June 12, 2017 [FREE Full text]\nJMIR Med Inform 2021 | vol. 9 | iss. 10 | e29584 | p. 9https://medinform.jmir.org/2021/10/e29584\n(page number not for citation purposes)\nKummervold et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\n14. Tensorflow code and pre-trained models for BERT. GitHub. 2020. URL: https://github.com/google-research/bert [accessed\n2021-08-26]\n15. Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, et al. Roberta: a robustly optimized BERT pretraining approach. arXiv.\nPreprint posted online July 26, 2019 [FREE Full text]\n16. Chollet F. Deep Learning with Python, 1st Edition. Greenwich, CT, USA: Manning Publications Co; 2017.\n17. Dhande L, Patnaik G. Analyzing sentiment of movie review data using Naive Bayes neural classifier. Int J Emerg Trends\nTechnol Comput Sci 2014;3(4):1-8 [FREE Full text]\n18. He P, Liu X, Gao J, Chen W. DeBERTa: Decoding-enhanced BERT with Disentangled Attention. arXiv. Preprint posted\nonline June 5, 2020 [FREE Full text]\n19. Lan Z, Chen M, Goodman S, Gimpel K, Sharma P, Soricut R. ALBERT: a lite BERT for self-supervised learning of\nlanguage representations. arXiv. Preprint posted online September 26, 2019 [FREE Full text]\nAbbreviations\nBERT: Bidirectional Encoder Representations from Transformers\nGloVe: Global Vectors for Word Representation\nLSTM: long short-term memory\nNLP: natural language processing\nTPU: Tensor Processing Unit\nEdited by C Lovis; submitted 13.04.21; peer-reviewed by X Cheng, A Fernandes; comments to author 02.07.21; revised version\nreceived 15.07.21; accepted 19.07.21; published 08.10.21\nPlease cite as:\nKummervold PE, Martin S, Dada S, Kilich E, Denny C, Paterson P, Larson HJ\nCategorizing Vaccine Confidence With a Transformer-Based Machine Learning Model: Analysis of Nuances of Vaccine Sentiment in\nTwitter Discourse\nJMIR Med Inform 2021;9(10):e29584\nURL: https://medinform.jmir.org/2021/10/e29584\ndoi: 10.2196/29584\nPMID:\n©Per E Kummervold, Sam Martin, Sara Dada, Eliz Kilich, Chermain Denny, Pauline Paterson, Heidi J Larson. Originally\npublished in JMIR Medical Informatics (https://medinform.jmir.org), 08.10.2021. This is an open-access article distributed under\nthe terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted\nuse, distribution, and reproduction in any medium, provided the original work, first published in JMIR Medical Informatics, is\nproperly cited. The complete bibliographic information, a link to the original publication on https://medinform.jmir.org/, as well\nas this copyright and license information must be included.\nJMIR Med Inform 2021 | vol. 9 | iss. 10 | e29584 | p. 10https://medinform.jmir.org/2021/10/e29584\n(page number not for citation purposes)\nKummervold et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX",
  "topic": "Social media",
  "concepts": [
    {
      "name": "Social media",
      "score": 0.7054381370544434
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6685981750488281
    },
    {
      "name": "Computer science",
      "score": 0.6638892292976379
    },
    {
      "name": "Machine learning",
      "score": 0.6403389573097229
    },
    {
      "name": "Psychological intervention",
      "score": 0.49993181228637695
    },
    {
      "name": "Natural language processing",
      "score": 0.49631863832473755
    },
    {
      "name": "Sentiment analysis",
      "score": 0.4650989770889282
    },
    {
      "name": "Annotation",
      "score": 0.4361027479171753
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.42286789417266846
    },
    {
      "name": "Test set",
      "score": 0.4160309433937073
    },
    {
      "name": "Information retrieval",
      "score": 0.36251533031463623
    },
    {
      "name": "World Wide Web",
      "score": 0.3109440207481384
    },
    {
      "name": "Medicine",
      "score": 0.21073570847511292
    },
    {
      "name": "Nursing",
      "score": 0.0923464298248291
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}