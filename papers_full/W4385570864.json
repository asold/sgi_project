{
  "title": "KILM: Knowledge Injection into Encoder-Decoder Language Models",
  "url": "https://openalex.org/W4385570864",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A760818427",
      "name": "Xu Yan",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4280808593",
      "name": "Namazifar, Mahdi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4280808592",
      "name": "Hazarika, Devamanyu",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4281278687",
      "name": "Padmakumar, Aishwarya",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2102234800",
      "name": "Liu Yang",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4202058699",
      "name": "Hakkani-Tur, Dilek",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4285208819",
    "https://openalex.org/W2998696444",
    "https://openalex.org/W3197876970",
    "https://openalex.org/W2611458652",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W4287854593",
    "https://openalex.org/W2898875342",
    "https://openalex.org/W2997200074",
    "https://openalex.org/W2950902819",
    "https://openalex.org/W4225302335",
    "https://openalex.org/W4283794395",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W3091432621",
    "https://openalex.org/W11298561",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W4226279206",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W3203259592",
    "https://openalex.org/W3202673127",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3113639015",
    "https://openalex.org/W2594284271",
    "https://openalex.org/W2963691861",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2250770256",
    "https://openalex.org/W3176750236",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2949163130",
    "https://openalex.org/W3198455100",
    "https://openalex.org/W2995183464",
    "https://openalex.org/W3171434230",
    "https://openalex.org/W4224263125",
    "https://openalex.org/W2785611959",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3175604467",
    "https://openalex.org/W3104415840",
    "https://openalex.org/W3169841173",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4287888899",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3169283738",
    "https://openalex.org/W3182696977",
    "https://openalex.org/W4385573439",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W3102187933",
    "https://openalex.org/W2209138810",
    "https://openalex.org/W3040558716",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W2951048068",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W3104748221",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W4205179624",
    "https://openalex.org/W4206778668",
    "https://openalex.org/W3098266846"
  ],
  "abstract": "Large pre-trained language models (PLMs) have been shown to retain implicit knowledge within their parameters. To enhance this implicit knowledge, we propose Knowledge Injection into Language Models (KILM), a novel approach that injects entity-related knowledge into encoder-decoder PLMs, via a generative knowledge infilling objective through continued pre-training. This is done without architectural modifications to the PLMs or adding additional parameters. Experimental results over a suite of knowledge-intensive tasks spanning numerous datasets show that KILM enables models to retain more knowledge and hallucinate less while preserving their original performance on general NLU and NLG tasks. KILM also demonstrates improved zero-shot performances on tasks such as entity disambiguation, outperforming state-of-the-art models having 30x more parameters. © 2023 Association for Computational Linguistics.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 5013–5035\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nKILM: Knowledge Injection into Encoder-Decoder Language Models\nYan Xu1,2∗, Mahdi Namazifar1, Devamanyu Hazarika1, Aishwarya Padmakumar1,\nYang Liu1, Dilek Hakkani-Tür1\n1Amazon Alexa AI\n2Hong Kong University of Science and Technology\nyxucb@connect.ust.hk, mahdinam@amazon.com, dvhaz@amazon.com\npadmakua@amazon.com, yangliud@amazon.com, hakkanit@amazon.com\nAbstract\nLarge pre-trained language models (PLMs)\nhave been shown to retain implicit knowledge\nwithin their parameters. To enhance this im-\nplicit knowledge, we propose Knowledge In-\njection into Language Models (KILM), a novel\napproach that injects entity-related knowledge\ninto encoder-decoder PLMs, via a generative\nknowledge infilling objective through contin-\nued pre-training. This is done without architec-\ntural modifications to the PLMs or adding ad-\nditional parameters. Experimental results over\na suite of knowledge-intensive tasks spanning\nnumerous datasets show that KILM enables\nmodels to retain more knowledge and halluci-\nnate less while preserving their original perfor-\nmance on general NLU and NLG tasks. KILM\nalso demonstrates improved zero-shot perfor-\nmances on tasks such as entity disambiguation,\noutperforming state-of-the-art models having\n30x more parameters.1\n1 Introduction\nLarge pre-trained language models (PLMs) (Rad-\nford et al., 2019; Lewis et al., 2020a; Raffel\net al., 2020) have achieved great success across\nall NLP tasks. However, recent studies also reveal\nthat PLMs are susceptible to memorizing the pre-\ntraining corpora rather than capturing the knowl-\nedge within them (Niven and Kao, 2019; Talmor\net al., 2020; Yasunaga et al., 2022; Li et al., 2022).\nParticularly for generation tasks, PLMs are notori-\nous for hallucinating text that is factually incorrect\nor hard to verify (Logan et al., 2019; Sun et al.,\n2020; Lin et al., 2020; Longpre et al., 2021). To\naddress these issues, one approach is to retrieve\nrelevant knowledge and integrate it explicitly with\nPLMs (He et al., 2020; Liu et al., 2021b). Another\ndirection is incorporating the additional knowledge\nsources into the pre-training step (Zhang et al.,\n∗Work done in part while Yan was an intern at Amazon\nAlexa AI.\n1The code is available at https://github.com/alexa/kilm.\n2019; Xiong et al., 2019; Liu et al., 2022; Wang\net al., 2021b). While the former suffers from the is-\nsue of falling back on the models themselves with-\nout retrieved information (Krishna et al., 2021),\nknowledge-focused pre-training can be comple-\nmentary to those methods (Longpre et al., 2021)\nand shows its advantage on generalization.\nIn this paper, we propose an approach for inject-\ning knowledge into encoder-decoder PLMs, such\nas BART, as a continued pre-training process. We\nrefer to it as Knowledge Injection into Language\nModels (KILM). Instead of introducing additional\nparameters to PLMs or modifying the model ar-\nchitectures to incorporate additional knowledge,\nKILM infills knowledge sentences by adopting a\nnovel knowledge infilling objective that includes\na knowledge reconstruction step in addition to the\noriginal pre-training objectives of BART.\nThe aim of KILM is to teach PLMs additional\ncontent about concepts and entities that they en-\ncounter in a given context, so that the models are\nable to ground an entity mention with additional\ninformation and “describe” what that entity is (see\nFigure 1). It should be emphasized that in this pro-\ncess, the context is especially important for cases\nwhen an entity mention can refer to multiple en-\ntities, e.g., Titanic which can refer to the British\nship or to the 1997 movie. We utilize the short\ndescriptions of entities in Wikipedia which com-\nprise of entity definitions as the knowledge source\n(§3.1). Although there are existing works leverag-\ning similar knowledge for PLM enhancement, they\nignore the relationship among entities, contexts,\nand entity-centric knowledge, and restrict their ap-\nplications to NLU tasks. In contrast, we propose a\ndistinct structure (§3.2) to augment Wikipedia arti-\ncles with short descriptions of the entity mentions\nin the context, thus model this essential relation-\nship, so as to force PLMs to learn the correlation\namong entities and contexts, and differentiate be-\ntween the entities with similar surface forms during\n5013\nThe Joker is a comic book series published by DC Comics\nstarring the supervillain the Joker. It ran for nine issues from\nMay–June 1975, with a tenth previously unpublished ... \nThe Joker (comic book)\n{{Short description|Fictional character in the DC Universe}} \n{{Redirect|The Joker|other characters called Joker or ...\n...\nView source\nThe Joker is a comic book series published by DC Comics\nstarring the supervillain the <ent> Joker </ent><ent_desc>\n<mask> </ent_desc>. It ran ... \nThe Joker is a comic book series published by DC Comics\nstarring the supervillain the <ent> Joker </ent>\n<ent_desc> Joker (character) <sep> Fictional character\nthroughout the DC Universe </ent_desc>. It ran ...\nKnowledge\nInfilling\nKnowledge\nMasking\nMasked Knowledge\nReconstruction\nKILM\nPLM\nThe Joker is a supervillain appearing\nin American comic books published\nby DC Comics. The character was\ncreated by ...\nJoker (character)\nFigure 1: The illustration of the proposed KILM technique for injecting knowledge into PLMs. In the given\nexample, the mention, Joker, is linked to the page of Wikipedia entity Joker (character). While the figure only\nshows knowledge infilling, knowledge masking, and masked knowledge reconstruction steps, the proposed method\nis combined with the original pre-training objectives of PLMs for continued pre-training.\ncontinued pre-training. With recent work that high-\nlights the need for explicit grounding for PLMs to\ntruly understand text (Merrill et al., 2021), we posit\nthat KILM takes a step in that direction.\nThe proposed structure for knowledge infilling\nin KILM is further leveraged as a structured prompt\nin downstream tasks (see §4.2). We demonstrate\nbetter knowledge retention with KILM in zero-shot\nfor entity disambiguation and appositive generation\ntasks, showing the effectiveness of the proposed\nmethod. We also find that BART with KILM out-\nperforms BART on QA tasks and is less prone to\nhallucination on tasks such as knowledge-grounded\nresponse generation. As mentioned earlier, KILM\nrelies on continued pre-training of PLMs, which\npresents the possibility of catastrophic forgetting\nof original skills of the PLM. We mitigate this by\nretaining the original training objectives of BART\nduring the continued pre-training stage. We empir-\nically verify that our proposed objective does not\ndegrade the general language modeling ability of\nthe PLM, nor affect the fluency of these models\nfor natural language generation (NLG) tasks. Al-\nthough we focus on short descriptions of entities\nas the knowledge source for KILM, other forms of\nknowledge can also be used, which we leave for\nfuture exploration.\nWe summarize our contributions as follows:\n(1) We propose a novel approach, KILM, to lever-\nage Wikipedia annotations in pre-training of PLMs.\nWe inject knowledge into BART, solely through\ncontinued pre-training, with no change in the archi-\ntecture of the PLMs. KILM enables entity-based\nknowledge injection with knowledge in natural-\nlanguage form. KILM’s distinct structure also of-\nfers a direct way to probe the entity knowledge\nretained in pre-trained models.\n(2) We show that KILM enhances the performance\nof BART on knowledge-intensive tasks while main-\ntaining its original performance on other down-\nstream tasks. KILM demonstrates improved zero-\nshot performance on entity disambiguation task,\noutperforming state-of-the-art models having 30x\nmore parameters.\n2 Related Work\nKnowledge-Enhanced LMs To enhance PLMs’\nuse of knowledge, a number of work has at-\ntempted to augment them with external knowledge\nsources, such as knowledge graphs (KGs) (Yin\net al., 2022). Some recent work introduced ad-\nditional non-parametric memories into the mod-\nels (Zhang et al., 2019; Rosset et al., 2020) to obtain\nentity embeddings and modified the model struc-\ntures to accommodate extra information (Yamada\net al., 2020; Wang et al., 2021a,b), while others\nchanged the masking schema with the additional\ninformation (Sun et al., 2019; Wang et al., 2022), or\nconverted the external KGs into natural language\ntext as an additional pre-training corpus (Xiong\net al., 2019; Zhou et al., 2020; Liu et al., 2022;\nAgarwal et al., 2021; Li et al., 2022).\nModeling with Text Linking and Enrichment\nOur motivation bears similarity to text linking(Ya-\nsunaga et al., 2022; Deng et al., 2021; Arora\net al., 2022) during pre-training and text enrich-\nment (Elazar et al., 2022). Modeling the links\nbetween documents or metadata is motivated by\nthe fact that PLMs, pre-trained on plain text, are\nnot directly trained to capture inter-dependencies\nbetween documents. The similarity between the\nabove tasks and ours lies in the ways humans im-\nplicitly link information when reading or generat-\n5014\ning language. However, the former tasks are re-\nstricted to relationships within the text, while our\ngoal is to ground the concepts and entities to their\nrelated descriptions in encyclopedic resources.\nPre-training with Hypertext Besides PLMs that\nare pre-trained with natural language corpora,\nHTLM (Aghajanyan et al., 2021) directly pre-trains\nsimplified crawled HTML data based on BART\nmodels and CM3 (Aghajanyan et al., 2022) ex-\ntends HTLM into a multimodal setting with causal\nmasked language modeling. The target of HTLM\nand CM3 is to better leverage the enormous web-\nscraped data source for pre-training. In contrast,\nour work aims to leverage hypertext to explore how\nto inject extra knowledge into PLMs with a custom-\ndesigned structure to furnish advantages to PLMs\nin performing knowledge-intensive tasks.\n3 Methodology\nAlthough KILM is model-agnostic and could be\nused for any PLM (more on this in §5), in this\nwork, due to high computation costs, we focus on\napplying KILM to BART (Lewis et al., 2020a).\n3.1 Preliminaries\nWikipedia is a widely-used text corpus for LM\npre-training. It is often processed as a collection\nof individual articles in the form of flat natural\nlanguage text. However, due to the existence of\nhyperlinks in its text, Wikipedia is also a complex\nweb of connected Wikipedia topics, also known\nas Wikipedia entities. These hyperlinks build con-\nnections between different Wikipedia entities and\nestablish a rich source of information that is mostly\nignored in current pre-training approaches. More-\nover, most Wikipedia articles come with a short\ndescription of the entity (topic) discussed in the ar-\nticle. These short descriptions provide definitions\nfor Wikipedia entities. In this work, we take an ini-\ntial step towards using these additional information\nwithin Wikipedia articles and utilizing “short de-\nscriptions” of entities for continued pre-training of\nPLMs. Note that the proposed approach could be\nexpanded to other annotated text corpora.\n3.2 KILM: Knowledge Injection into\nLanguage Models\nWe propose KILM, which extends the text-infilling\nobjective to knowledge infilling objective through\ncontinued pre-training. KILM, as shown in Fig-\nure 1, consists of three steps: (1) knowledge in-\nfilling, (2) knowledge masking, and (3) masked\nknowledge reconstruction.\nKnowledge Infilling As mentioned in §3.1, in\nthis work, we mainly focus on injecting PLMs with\nhyperlinks and entity descriptions as the entity-\nrelated knowledge into PLMs. Specifically, we\nprocess Wikipedia data such that entity mentions\nin Wikipedia articles (which are annotated by hy-\nperlinks) are marked with a start-of-entity token\n<ent> and an end-of-entity token </ent>. Also,\neach entity mention is followed by an entity-related\nknowledge sentence marked with <ent_desc> and\n</ent_desc> as start- and end-of-description to-\nkens. The inserted knowledge component (high-\nlighted in blue in Figure 1) consists of the corre-\nsponding hyperlinked entity (which might be dif-\nferent from the entity’s surface form in the text)\nand the entity’s short description connected with\nthe <sep> token, where the short description is\nobtained from a lookup table extracted from the\nWikipedia dump. We denote this knowledge infill-\ning transformation as KNINFILL .\nKnowledge Masking The processed data is used\nfor the continued pre-training of a PLM. During\nthis step, we conduct knowledge masking trans-\nformation (denoted as KNMASK ) and the model\nis trained to reconstruct the whole inserted knowl-\nedge component from a single <mask> token with\nrespect to the context. More specifically, assum-\ning the ith token ti is a mention of an entity, the\nmasked input sequence X and the output sequence\nY can be denoted as:\nX ={t1,...,t i−1, <ent> ,ti, </ent>, <ent_desc> ,\n<mask> , </ent_desc> ,ti+1 ...,tN},\nY ={t1,...,t i−1, <ent> ,ti, </ent>, <ent_desc> ,\nk1, ..., kL , </ent_desc> ,ti+1 ...,tN},\nwhere tn represents the nth token of the original\ntarget sequence and kl represents the lth token in\nthe knowledge sequence of length L.\nMasked Knowledge Reconstruction The pa-\nrameters θof the PLM are optimized by a masked\nknowledge reconstruction loss:\nLkn = E\n( L∑\nl=1\n−log\n(\np\n(\nkl|t1:(i+l+2),X,θ\n))\n)\n.\nSince our goal is to inject entity-related knowl-\nedge without disrupting the function of the original\nBART as a general PLM, the masked knowledge\nreconstruction loss is combined with the original\n5015\nTask Knowledge\ntype\nTask\nadapation Input/Prompt Target\nEntity\nDisambigua-\ntion\nentity ✗\nContextD: The Big Blue River is ... Driftwood White,\n<ent>Wabash</ent><ent_desc><mask>\n</ent_desc>, and ...\nCandidateS1: Wabash River<sep>Tributary of the Ohio ...\nCandidateS2: Wabash, Indiana<sep>Wabash is a city in ...\nWabash River\nAppositive\nGeneration entity ✗\nThe game achieved the highest ... matchup between Larry\nBird and Spartans’ point guard<ent>Magic Johnson\n</ent><ent_desc><mask></ent_desc>.\na rivalry that lasted\nthroughout their\nprofessional careers\nIn-Context\nFew-Shot QA factoid ✗ Question: What jobs did Ben Franklin do? Answer: Diplomat\nQuestion: What did Ben Franklin invent? Answer:<mask> Lightning rod\nKGRG encyclopedia ✓\n<speaker2>Ross was an American painter and television host.\n<speaker1>That’s cool. What else?\n<speaker2>\nHe created the show\n\"The Joy of Painting\"\nTable 1: A summary of the knowledge-intensive tasks that are studied in this work. KGRG is short for Knowledge\nGrounded Response Generationtask. Examples of input and target formats are provided above along with the task\ninformation. The definitions of the knowledge types are discussed in the corresponding sections in §4.2.\ntext infilling objective of BART during continued\npre-training.2 At training time, the model is opti-\nmized by minimizing the reconstruction loss over\nthe whole target sequence instead of only the re-\ncovered masked spans. As a result, the training ob-\njectives force the model to learn to copy the tokens\nfrom the input sequences when the token is not a\nmask token during the pre-training process. This is\nto help the model recognize the inserted knowledge\ncomponents in the training sequences and ensure\nthe fluency of the PLM on NLG tasks. The weights\nof different objectives for loss are calculated based\non the proportion of the corresponding spans across\nthe entire sequence. We summarize the proposed\nKILM algorithm in Appendix B.\nThe advantages of leveraging this structure for\ntraining are two-fold. First, this structure builds\nan alignment between the entity-related knowledge\nand the corresponding mention in the paragraphs.\nSecond, the injected knowledge can be easily in-\nduced by probing the PLM with the structured\nprompts proposed for KILM (§4.2).\n4 Experiments\nWe start by exploring the performance of\nBART+KILM on knowledge-intensive tasks (§4.2).\nLater, we also demonstrate that KILM does not\ndegrade the original language modeling skills of\nBART in both NLU and NLG benchmarks (§4.3).\n2The comparison between the text infilling and sentence\npermutation objectives shows the advantage of the former\nobjective over the latter (Lewis et al., 2020a), so we only\npreserve the text infilling objective for KILM to simplify the\ncontinued pre-training task.\n4.1 Pre-training Details\nData To extract the short descriptions and the\nhyperlinks from Wikipedia articles, we process a\nWikipedia dump from scratch.3 We assign the first\nsentence of the Wikipedia page as the short descrip-\ntion if the “short description” attribute is missing\nin the raw data. We use the processed data by\nonly leveraging the paragraphs from the summary\nsections of Wikipedia as our primary training cor-\npus (denoted as primary setting), while we also\nexplore a data upscaling settingwhere we use the\nentire Wikipedia articles. We split the articles with\ndocument strides of 512 and consider one snippet\nas a data sample. We randomly select one entity\nfrom the paragraphs in each iteration for dynamic\nentity-centric knowledge injection.4 After data pre-\nprocessing, we obtain a collection of 5.70 million\ndata samples for the primary setting and 7.85 mil-\nlion data samples for the data upscaling setting\nfrom Wikipedia. We split the corpus into a training\nset and a validation set with around 10k samples,\nfor evaluation. In the following sections, KILM\nwithout a subscript indicates that it is conducted un-\nder the default primary setting, while KILM under\ndata upscaling setting will be denoted as KILMDU.\nFor pre-training in the primary setting, the model is\ncontinually trained for 7,000 steps, and for the data\nupscaling setting, the model is trained for 50,000\nsteps.5 Refer to Appendix C.1 for details.\n3The Wikipedia dump is downloaded from https://\ndumps.wikimedia.org/enwiki/.\n4We select different entities in each iteration.\n5Most of our results are based on KILM in the primary\nsetting, and due to the computational resource cost, only for a\n5016\nModels AIDA MSNBC AQUAINT ACE2004 CWEB WIKI Avg Parameters\nCM3-medium (Aghajanyan et al., 2022)‡ 78.0 80.1 75.4 81.4 68.5 76.2 76.6 2,700M\nCM3-large (Aghajanyan et al., 2022)‡ 80.1 80.8 77.7 82.8 72.4 80.2 79.0 13,000M\nBART-base 33.8 57.6 44.6 37.8 36.4 46.1 42.7 139M\nBART-base+Merge 28.2 43.3 27.1 19.5 27.3 39.9 30.9 139M\nBART-base+KILM (ours) 80.0 83.7 74.7 78.2 63.7 71.3 75.3 139M\nBART-large 34.4 58.8 42.3 38.9 36.9 46.5 43.0 406M\nBART-large+KILM (ours) 84.6 86.4 79.8 80.9 66.1 75.4 78.9 406M\nBART-large+KILMDU(ours) 86.2 87.8 84.3 83.7 68.4 79.9 81.7 406M\nTable 2: InKB Micro F1 on zero-shot entity disambiguation tasks with candidates from Le and Titov (2018). ‡The\nresults are from CM3 under the zero-shot setting.\nBaselines Besides the original BART, we also\nreport on another BART-base baseline that is con-\ntinue pre-trained on a merge of Wikipedia corpus\nand short descriptions for 7,000 steps (same num-\nber of steps as KILM) with only text infilling ob-\njective. The short descriptions are converted to\ngeneral text based on the format: “<Entity> is\n<Short Desc>” . This model is denoted as BART-\nbase+Merge. We demonstrate input and output\nformats of pre-training in Table C6. This baseline\nis introduced to separately evaluate the role of the\ndistinct structure that is introduced in this work, as\nwell as the additional training steps and data.\n4.2 Knowledge-Intensive Tasks\nFirst, we study the effectiveness of KILM on\nknowledge-intensive tasks (Petroni et al., 2019;\nRoberts et al., 2020; Petroni et al., 2021). As shown\nin Table 1, we evaluate BART+KILM on entity\ndisambiguation and appositive generation tasks,\nwhich have similar objectives to the continued\npre-training of KILM. We also evaluate if KILM\ncan contribute to downstream tasks where the pre-\ntraining objective of KILM is not fully aligned with\nthose of the downstream tasks. Specifically, We\ninclude question answering (QA) and knowledge\ngrounded response generation (KGRG) tasks.\nZero-shot Entity Disambiguation The entity\ndisambiguation task requires the model to link a\nmention qto the correct entity, given a contextD\nand several candidate entities. Without fine-tuning,\nwe evaluate BART+KILM by picking the candidate\nwith the lowest perplexity of generating short de-\nscriptions {Si}N\ni=1 using structured prompts among\nthe candidate entities{Ei}N\ni=1 in entity disambigua-\ntion datasets.6 It can be expressed as:\nsubset of knowledge intensive tasks we also report the results\nfor data upscaling setting too.\n6Note that the reference entities in this task come from\nWikipedia, hence we can use the associated entity description\nXi = KNMASK (KNINFILL (D,q, Si)) (1)\nEi∗\n= arg max\ni\n∑\nt\nlog p(si\nt|Xi,θ). (2)\nWe use the same datasets and candidate sets as\nthose in Le and Titov (2018). InKB micro-F1 re-\nsults are shown in Table 2, where CM3, a series\nof huge PLMs trained with multimodal hypertext\n(see §2), are tested in a zero-shot setting. We also\nincluded the performances of BART and BART-\nbase+Merge for reference.7 BART+KILM outper-\nforms CM3-large, which has over 30x more pa-\nrameters, for half of the datasets. BART+KILMDU\noutperforms CM3-large in four out of six datasets.\nCM3 as a PLM has an impressive performance on\nentity disambiguation task with no additional train-\ning, and this comparison shows that BART+KILM\ncan outperform CM3 with much less parameters.\nWe also present results comparing BART+KILM\nwith BLINK (Wu et al., 2020) in Table C1, where\nwe see that it performs competitively compared\nto BLINK (which is fine-tuned for entity disam-\nbiguation). Moreover, the large gap between the\nperformance of BART+KILM and BART+Merge\nshows that the proposed distinct structure (and not\nnecessarily the data) plays a key role in the perfor-\nmance of BART+KILM in this task.\nAppositive Generation Appositive generation\nis the task of adding background information for\nnamed entities in a sentence in the form of an ap-\npositive phrase. As shown in Table 1, we construct\nstructured prompts to probe PLMs without fine-\ntuning on ApposCorpus (Kementchedjhieva et al.,\n2020). We consider the generated texts recovered\nfrom the mask tokens in the short description field\nas the generated appositives.8\nfor each reference entity.\n7More details are included in Appendix C.3.\n8Since the pre-training corpus of BART includes\nWikipedia articles, BART can also recover appositives from\n5017\nModel News ORG News PER\nAp. Pref. NH. Ap. Pref. NH.\nBART-base 26.0 17.8 41.7 48.0 14.3 28.3\n+KILM 97.0 51.5 56.8 94.0 36.0 42.0\nModel Wiki ORG Wiki PER\nAp. Pref. NH. Ap. Pref. NH.\nBART-base 48.5 26.7 49.7 30.8 7.3 32.7\n+KILM 98.0 48.0 61.0 89.9 40.3 50.3\nTable 3: Human evaluation results on Appositive Gen-\neration in News and Wikipedia domains on org- and\nperson-type entities (see Appendix C.7). Ap., Pref., and\nNH. mean Is Appositive, Preference, and Not Halluci-\nnated. Numbers in bold are significantly better than\nthose from BART at p-value of 0.05 in a pairwise t-test.\nSince automatic metrics only assess the text over-\nlap based performance (Table C3 in Appendix C.4\nwith comparisons with SOTA), we conduct human\nevaluation for a more comprehensive evaluation\nfrom three aspects: Is Appositive(Ap.), Preference\n(Pref.), and Not Hallucinated (NH.). Ap. evalu-\nates whether the generation is an appositive or not,\nwhile Pref. evaluates the suitability of the generated\nappositives to the context. NH. evaluates whether\nthe model generates a hallucinated appositive or\nnot, verifying whether the generated appositive is\nfactually correct. Pairwise A/B testing is utilized to\ncompare the performances of BART before and af-\nter KILM (in the primary setting) on all four subsets\nof ApposCorpus. For each comparison, the same\ncontext and two options generated by models for\ncomparison are first randomly shuffled and then are\nshown to the annotators. Each comparison requires\nthree judgments. 50 data samples are randomly\nselected from each subset. More details of human\nevaluation are included in Appendix C.7. Table 3\nlists the human evaluation results in terms of the\nwinning rate (ties are counted as wins for both),\nwhere we observe that BART+KILM generates bet-\nter appositives and hallucinates less in all four sub-\nsets. These results indicate that BART+KILM pos-\nsesses more entity-related knowledge than BART.\nIn-Context Few-Shot QA The implicit knowl-\nedge embedded in the parameters can support\nlarge PLMs to obtain competitive results on open-\ndomain QA tasks without accessing external knowl-\nedge (Roberts et al., 2020; Radford et al., 2019;\nBrown et al., 2020). We conduct in-context few-\nshot experiments, in the primary setting of KILM,\nmask tokens without further task adaptation.\n0 1 2 5 8 10\n0\n5\n10\n15\n20EM\nTriviaQA\n0 1 2 5 8 10\nX-shot\nNatural Questions\nBART-base\nBAb+KILM\nBART-large\nBAl+KILM\nBAl+KILMDU\n0 1 2 5 8 10\nWeb Questions\nKALM-base\nKALM-large\nBAb+Merge\nFigure 2: Results on QA datasets with different\nshots. BART results are in blue, while the results of\nBART+KILM are in orange and green. We use dashed\nand solid lines to denote the base- and large-size mod-\nels, respectively. Also “BAb” and “BAl” correspond\nto BART-base and BART-large, respectively. KILMDU\nis KILM with data upscaling where entire Wikipedia\narticles are used instead of only their first paragraphs.\non TriviaQA (Joshi et al., 2017), Natural Questions\n(NQ) (Kwiatkowski et al., 2019), and Web Ques-\ntions (WQ) (Berant et al., 2013) datasets. Similar to\nthe settings of GPT-3 (Brown et al., 2020), we put\nseveral example QA pairs into the input sequences\nof both the encoder and decoder. The format of\nprompting is shown in Table 1, while the example\nQA pairs are retrieved with a TF-IDF retriever 9\nfrom the corresponding training set. The tokens\nrecovered from the mask tokens from the decoder\nwill be considered as the generated answers.\nWe illustrate learning trends with different\n“shots” in Figure 2 on all three datasets. Inter-\nestingly, BART+KILM mostly performs worse\nthan the original BART under the zero-shot setting.\nHowever, appending demonstrations into the con-\ntexts enables BART+KILM to outperform the orig-\ninal BART by a large margin. With the data upscal-\ning setting, KILMDU provides comparable (or even\nlarger) improvements to BART under the few-shot\nsetting while slightly improving the zero-shot per-\nformances of BART. Though far from perfect, these\nresults suggest that KILM significantly improves\nthe in-context learning ability of BART on all three\nQA datasets. KILM also enables BART to pack\nfactoid knowledge more effectively within its pa-\nrameters, which supports QA. BART-base+KILM\noutperforms BART-large under the in-context few-\nshot setting for the NQ and WQ datasets. The\nperformance of the baseline model, BART+Merge,\nshows a similar trend to BART+KILM with little\nadvantage on NQ and WQ datasets. This indicates\nthat pre-training with data in “<Entity> is <Short\n9The implementation is based on https://github.com/\nefficientqa/retrieval-based-baselines.\n5018\nModel Seen Test Unseen Test\nPPL R1 R2 PPL R1 R2\nSKT 52.0 19.3 6.8 81.4 16.1 4.2\nKAT-TSLF 14.4 21.7 7.6 15.8 20.7 7.2\nBART-base 17.1∗ 18.7 4.9 20.9∗ 17.5 4.0\n+Merge 21.4 19.3 5.2 26.8 18.0 4.2\n+KILM 21.5 19.3∗ 5.2 26.9 17.9 ∗ 4.2∗\nBART-large14.2∗ 20.6 5.8 18.7 18.5 4.3\n+KILM 18.9 20.8∗ 5.9 24.9 18.8∗ 4.5∗\nTable 4: WoW test set results. PPL denotes perplex-\nity, while R1/2 denotes ROUGE-1/2 metrics. While\nboth SKT (Kim et al., 2019) and KAT-TSLF (Liu et al.,\n2021a) use external knowledge as inputs, BART and\nBART+KILM are evaluated without knowledge to bet-\nter demonstrate the impact of KILM. ∗p <0.05 in a\npairwise t-test for comparison between ours and BART.\nModel Seen Test Unseen Test\nFlu. Info. NH. Flu. Info. NH.\nBART-base 59.7 64.0 48.4 65.8 70.3 46.6\n+KILM 66.7 63.0 60.3∗ 69.2 69.3 58.8†\nTable 5: Human evaluation results on WoW test sets\nwithout external knowledge inputs. Flu., Info., and\nNH. are Fluency, Informativeness and Not Hallucinated\nrespectively. ∗Model performs significantly better than\nthe baseline (p< 0.05); †Pairwise t-test (p< 0.07).\nDesc>” format is more suitable for QA tasks. Nev-\nertheless, the proposed distinct structure does not\nbring much obstacle to BART+KILM on QA tasks.\nKnowledge Grounded Response Generation\n(KGRG) The KGRG task requires topical and\nfactual knowledge (Petroni et al., 2021) for a chat-\nbot to make engaging conversations with users on\nvarious topics (Ghazvininejad et al., 2018). We fine-\ntune BART before and after KILM on the Wizard of\nWikipedia (WoW) (Dinan et al., 2018) dataset with-\nout using knowledge as input, to better study the im-\npact of the injected knowledge under a knowledge-\nunavailable setting. The generated responses are\nevaluated with PPL, ROUGE-1 and ROUGE-2\nmetrics. In Table 4, BART+KILM offers a con-\nsistent and significant advantage over BART on\nROUGE scores, whereas it underperforms BART\non PPL. The performance gap on PPL can be at-\ntributed to the fact that many of the responses in\nWoW contain hallucination (Dziri et al., 2022),\nwhich is somewhat mitigated by KILM. Compared\nto the strong baseline with external knowledge\ninputs, BART+KILM even performs comparably\nModel GLUE CNN XSUM\nAvg. R1 R1\nBART-base 83.3 42.79 40.83∗\n+KILM 83.8 42.86 40.76\nBART-large 87.1 44.14∗ 45.17\n+KILM 87.7 43.15 45.07\nTable 6: Results on the GLUE and summarization test\nsets. We report average score of Matthews correla-\ntion for CoLA and accuracy scores for other tasks in\nGLUE benchmark; and ROUGE-1 for summarization.\n∗pairwise t-test p< 0.05.\nwith SKT (Kim et al., 2019). Note that the perfor-\nmance of BART+Merge shows no difference from\nBART+KILM, which suggests that the introduced\ndistinct structure does not affect BART’s applica-\ntion of injected knowledge on WoW.\nWhile automatic metrics are important in KGRG\nevaluation, they do not always tell the whole\nstory (Hazarika et al., 2022), therefore we also con-\nduct human evaluation on WoW test sets from three\naspects, namely Fluency (Flu.), Informativeness\n(Info.), and Not Hallucinated (NH.). Flu. focuses\non whether the responses are fluent and consis-\ntent with respect to the conversation so far, while\nInfo. evaluates whether the responses contain veri-\nfiable factual information. The evaluation on NH.\nis only valid when a response is informative. The\nsettings of human evaluation are the same as those\nfor appositive generation (see Appendix C.7). The\nresults in Table 5 demonstrate that BART+KILM\nperforms comparably with BART in terms of flu-\nency and informativeness, while it tends to hallu-\ncinate less when generating factual information in\nthe responses, especially in unseen domains.\n4.3 General Tasks\nWe now evaluate the impact of KILM on models’\nperformance on general NLU and NLG tasks us-\ning the GLUE benchmark (Wang et al., 2018) and\nsummarization datasets, CNN/Dailymail (Hermann\net al., 2015) and XSUM (Narayan et al., 2018),\nby fine-tuning both BART and BART+KILM for\ncomparison. The summary of the results is shown\nin Table 6, and the detailed results shown in Ta-\nble C4 and Table C5. BART+KILM outperforms\nBART marginally on GLUE and the differences\nfor summarization datasets are small. These results\nsuggest that KILM preserves the performance of\nthe original BART on downstream NLU and NLG\ntasks, and even in some cases it improves it. They\n5019\nalso verify that KILM does not cause catastrophic\nforgetting of the original learnings in BART, thus\nmaking BART+KILM a reliable PLM.\n5 Discussions\nRoles of Introduced Special Tokens The intro-\nduced special tokens to mark beginning and end\nof entities (<ent>, </ent>) and entity descriptions\n(<ent_desc>, </ent_desc>) form a distinct struc-\nture in pre-training samples, which inserts entity-\ncentric knowledge into pre-training corpora, thus\ninjects knowledge in PLMs. We discuss the roles\nof these special tokens from the following aspects:\nEntity Knowledge Probing: This distinct struc-\nture in KILM provides a tool for probing the entity-\nrelated knowledge retained in PLMs. To demon-\nstrate this, we probe BART+KILM by prompting it\nto generate short descriptions for entities in valida-\ntion set10 of the pre-training corpus. The probing\nformat and the corresponding results are shown\nin Appendix A.1 and Table A1. BART+KILM\nachieve around 60 unigram F1 scores with no per-\nformance gap with the data samples from a subset\nof the training set. These results indicate that we\ncan easily recall the entity description knowledge\nin different contexts without sensitivity to prompt\ndesigns. It is shown that the proposed pre-training\nstructure is the main contributor of the improve-\nments on entity-related datasets, especially in zero-\nshot manner. By leveraging the introduced special\ntokens, the knowledge retained in PLMs can be\nmore efficiently leveraged on downstream tasks.\nStructured Prompt: The special tokens also pro-\nvide convenient knowledge probing for zero-shot\nentity-centric tasks, such as entity disambiguation\nand appositive generation (§4.2).\nAre New Special Tokens Needed? There are a\nfew reasons for introducing new special tokens in\nKILM for marking entities and their descriptions\ninstead of reusing existing tokens, such as commas\nor parentheses. First, many entities have commas\nand parentheses in their names, making the entity\ndescriptions indistinguishable from the contexts.\nFor instance, there are 378,093 entities in English\nWikipedia with a comma in their names, such as\nthe entity “Mars, Aurgazinsky District, Republic of\nBashkortostan”. Second, using commas or paren-\ntheses could break the fluency of the text. In a\ncontext like “The Baltic states [...] is used to group\n10The articles in validation set are not included in the pre-\ntraining process, whereas the involved entities mostly are.\nthree countries: Estonia, Latvia, and Lithuania”,\nadding a short description for the entity “Estonia”\nusing a comma would break the fluency of the sen-\ntence. Finally, using commas or parenthesis will\noverload their meanings, and during prompting of\nthe model for knowledge probing it will result in a\nlack of clarity for the model as to how the comma\nor parenthesis should be interpreted.\nIs KILM’s impact equal on different domains\nand tasks? Despite the above-mentioned gains,\nBART+KILM appears to be less knowledgeable\nthan BART on person-type entities, as manifested\nin the performance gap between organization- and\nperson-type entities in appositive generation (Ta-\nble 3). That may be due to the type of knowledge\ncontent injected by KILM. The entity knowledge\nrequired for generating appositives varies vastly\nfrom biographies to relationships with other peo-\nple. However, short descriptions in Wikipedia for\nperson-type entities focus mostly on their national-\nity and occupation. Also, many of them are simi-\nlar 11. This problem also affects the performance\nin Table A2 on G-RE datasets in LAMA bench-\nmark. More analyses are in Appendix A.2. We\nleave the study of enriching the knowledge content\nfor pre-training as future work.\nThe proposed pre-training structure shows its\nstrength in entity-related tasks. Nevertheless,\nKILM may downgrade to conventional knowledge-\naugmented pre-training (BART+Merge) when the\npre-training objective of KILM is not fully aligned\nwith those of the downstream tasks.\nPlacement of Knowledge Component An abla-\ntion study on the knowledge component placement\nin KILM is presented in Appendix A.3, where we\nshow that putting short descriptions right after en-\ntity mentions results in better performance com-\npared to placing them at the end of sentences.\nExtending KILM for Other PLM Architectures\nIn this paper, we choose BART as the default PLM;\nhowever, KILM can also be applied to other PLMs\nby adjusting their training objectives for knowledge\ninfilling. For decoder-only PLMs, such as GPT-2,\nthe knowledge component, i.e., short descriptions,\ncan be moved to the end of the target sequence (sim-\nilar to CM3) instead of being adjoined the surface\nform of the entity. As for encoder-only PLMs, such\nas BERT, contrastive training strategy introduced in\n11For example short descriptions for both Columbus Short\nand Drew Fullerare “American actor”\n5020\nLinkBERT (Yasunaga et al., 2022) is one option for\nthe training objective of KILM. Due to the substan-\ntial computational cost of training these models,\nwe leave these explorations for future works.\nJustifications on the additional cost during pre-\ntraining Injecting additional knowledge text into\npre-training corpora may introduce additional costs\nduring the pre-training process. While entity de-\nscriptions used in the paper are usually a one-\nsentence definition of an entity, the average length\nof short descriptions is 13.81 words. Considering\nthat we split the Wikipedia articles with document\nstrides of 512, the inserted tokens for short descrip-\ntions only take 2.6% of the length of the whole\nsequence, which does not bring much more train-\ning cost.\n6 Conclusion\nIn this paper, we propose a novel method, KILM,\nto inject entity-related knowledge into large PLMs\nthrough continued pre-training. Our approach en-\nhances the performance of the original PLMs on\nknowledge-intensive tasks, especially in zero- and\nfew-shot settings, while not causing catastrophic\nforgetting of the knowledge in the origianl PLMs.\nThe proposed distinct structure for entity knowl-\nedge shows its effectiveness on flexibly probing the\ninjected knowledge in different contexts.\nLimitations\nIn this paper, we propose a continued pre-training\nmethod to inject knowledge into large pre-trained\nlanguage models. There are eight V100 GPUs\ninvolved in each pre-training experiment and the\nwhole pre-training process takes 5 days for the\nbase-size model and 13 days for the large-size\nmodel, in primary settings. These numbers in\ndata upscaling settings are significantly greater (30\ndays for the large-size model). Despite its advan-\ntage in reducing resource need in inference time,\nKILM is both time-consuming and computationally\nresource-consuming during training time.\nSimilar to any model-based generation system,\nKILM could be prone to generating factually in-\ncorrect statements with regard to entities. These\nstatements might also be prone to be biased based\non ethnicity, race, and sexual orientation.\nReferences\nOshin Agarwal, Heming Ge, Siamak Shakeri, and Rami\nAl-Rfou. 2021. Knowledge graph based synthetic\ncorpus generation for knowledge-enhanced language\nmodel pre-training. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3554–3565.\nArmen Aghajanyan, Bernie Huang, Candace Ross,\nVladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro\nOkhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis,\net al. 2022. Cm3: A causal masked multi-\nmodal model of the internet. arXiv preprint\narXiv:2201.07520.\nArmen Aghajanyan, Dmytro Okhonko, Mike Lewis,\nMandar Joshi, Hu Xu, Gargi Ghosh, and Luke\nZettlemoyer. 2021. Htlm: Hyper-text pre-training\nand prompting of language models. arXiv preprint\narXiv:2107.06955.\nSimran Arora, Sen Wu, Enci Liu, and Christopher Ré.\n2022. Metadata shaping: A simple approach for\nknowledge-enhanced language models. In Findings\nof the Association for Computational Linguistics:\nACL 2022, pages 1733–1745.\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An\nautomatic metric for mt evaluation with improved\ncorrelation with human judgments. In Proceedings\nof the acl workshop on intrinsic and extrinsic\nevaluation measures for machine translation and/or\nsummarization, pages 65–72.\nBrigitte LM Bauer. 2017. Nominal apposition in\nIndo-European: Its forms and functions, and its\nevolution in Latin-Romance, volume 303. Walter\nde Gruyter GmbH & Co KG.\nJonathan Berant, Andrew Chou, Roy Frostig, and\nPercy Liang. 2013. Semantic parsing on freebase\nfrom question-answer pairs. In Proceedings of the\n2013 conference on empirical methods in natural\nlanguage processing, pages 1533–1544.\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim\nSturge, and Jamie Taylor. 2008. Freebase: a col-\nlaboratively created graph database for structuring\nhuman knowledge. In Proceedings of the 2008 ACM\nSIGMOD international conference on Management\nof data, pages 1247–1250.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nNicola De Cao, Gautier Izacard, Sebastian Riedel,\nand Fabio Petroni. 2020. Autoregressive entity re-\ntrieval. In International Conference on Learning\nRepresentations.\n5021\nXiang Deng, Yu Su, Alyssa Lees, You Wu, Cong Yu,\nand Huan Sun. 2021. Reasonbert: Pre-trained to rea-\nson with distant supervision. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 6112–6127.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, V olume1 (Long and Short Papers),\npages 4171–4186.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2018. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. In International Conference on Learning\nRepresentations.\nNouha Dziri, Sivan Milton, Mo Yu, Osmar R Zaiane,\nand Siva Reddy. 2022. On the origin of hallucina-\ntions in conversational models: Is it the datasets or\nthe models? In Proceedings of the 2022 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 5271–5285.\nYanai Elazar, Victoria Basmov*, Yoav Goldberg, and\nReut Tsarfaty. 2022. Text-based np enrichment.\nTransactions of the Association for Computational\nLinguistics, 10:764–784.\nHady Elsahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon Hare, Frederique Lafor-\nest, and Elena Simperl. 2018. T-rex: A large scale\nalignment of natural language with knowledge base\ntriples. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018).\nEvgeniy Gabrilovich, Michael Ringgaard, and Amarnag\nSubramanya. 2013. Facc1: Freebase annotation of\nclueweb corpora, version 1. Release date, pages 06–\n26.\nMarjan Ghazvininejad, Chris Brockett, Ming-Wei\nChang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, and\nMichel Galley. 2018. A knowledge-grounded neu-\nral conversation model. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 32.\nZhaochen Guo and Denilson Barbosa. 2018. Robust\nnamed entity disambiguation with random walks.\nSemantic Web, 9(4):459–479.\nDevamanyu Hazarika, Mahdi Namazifar, and Dilek\nHakkani-Tür. 2022. Attention biasing and con-\ntext augmentation for zero-shot control of encoder-\ndecoder transformers for natural language generation.\nIn Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 36, pages 10738–10748.\nBin He, Di Zhou, Jinghui Xiao, Xin Jiang, Qun Liu,\nNicholas Jing Yuan, and Tong Xu. 2020. Bert-\nmk: Integrating graph contextualized knowledge into\npre-trained language models. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, pages 2281–2290.\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. Advances in neural information\nprocessing systems, 28.\nJohannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino,\nHagen Fürstenau, Manfred Pinkal, Marc Spaniol,\nBilyana Taneva, Stefan Thater, and Gerhard Weikum.\n2011. Robust disambiguation of named entities\nin text. In Proceedings of the 2011 conference on\nempirical methods in natural language processing,\npages 782–792.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting\nof the Association for Computational Linguistics\n(V olume1: Long Papers), pages 1601–1611.\nJun Seok Kang, Robert Logan, Zewei Chu, Yang\nChen, Dheeru Dua, Kevin Gimpel, Sameer Singh,\nand Niranjan Balasubramanian. 2019. Pomo:\nGenerating entity-specific post-modifiers in con-\ntext. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, V olume1 (Long and Short Papers),\npages 826–838.\nJivat Kaur, Sumit Bhatia, Milan Aggarwal, Rachit\nBansal, and Balaji Krishnamurthy. 2022. Lm-core:\nLanguage models with contextually relevant exter-\nnal knowledge. In Findings of the Association for\nComputational Linguistics: NAACL 2022, pages\n750–769.\nYova Kementchedjhieva, Di Lu, and Joel Tetreault.\n2020. The apposcorpus: A new multilingual, multi-\ndomain dataset for factual appositive generation. In\nProceedings of the 28th International Conference on\nComputational Linguistics, pages 1989–2003.\nByeongchang Kim, Jaewoo Ahn, and Gunhee Kim.\n2019. Sequential latent knowledge selection for\nknowledge-grounded dialogue. In International\nConference on Learning Representations.\nKalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021.\nHurdles to progress in long-form question answer-\ning. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association\n5022\nfor Computational Linguistics: Human Language\nTechnologies, pages 4940–4957.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics, 7:453–\n466.\nPhong Le and Ivan Titov. 2018. Improving entity link-\ning by modeling latent relations between mentions.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (V olume\n1: Long Papers), pages 1595–1604.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020a.\nBart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and\ncomprehension. In Proceedings of the 58th Annual\nMeeting of the Association for Computational\nLinguistics, pages 7871–7880.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020b. Retrieval-augmented genera-\ntion for knowledge-intensive nlp tasks. Advances\nin Neural Information Processing Systems, 33:9459–\n9474.\nQuentin Lhoest, Albert Villanova del Moral, Yacine\nJernite, Abhishek Thakur, Patrick von Platen,\nSuraj Patil, Julien Chaumond, Mariama Drame,\nJulien Plu, Lewis Tunstall, Joe Davison, Mario\nŠaško, Gunjan Chhablani, Bhavitvya Malik, Si-\nmon Brandeis, Teven Le Scao, Victor Sanh, Can-\nwen Xu, Nicolas Patry, Angelina McMillan-Major,\nPhilipp Schmid, Sylvain Gugger, Clément De-\nlangue, Théo Matussière, Lysandre Debut, Stas\nBekman, Pierric Cistac, Thibault Goehringer, Vic-\ntor Mustar, François Lagunas, Alexander Rush,\nand Thomas Wolf. 2021. Datasets: A commu-\nnity library for natural language processing. In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 175–184, Online and Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nShuyang Li, Mukund Sridhar, Chandana Satya Prakash,\nJin Cao, Wael Hamza, and Julian McAuley. 2022.\nInstilling type knowledge in language models via\nmulti-task qa. In Findings of the Association for\nComputational Linguistics: NAACL 2022, pages\n594–603.\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen,\nPei Zhou, Chandra Bhagavatula, Yejin Choi, and\nXiang Ren. 2020. Commongen: A constrained\ntext generation challenge for generative common-\nsense reasoning. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1823–1840.\nLinlin Liu, Xin Li, Ruidan He, Lidong Bing, Shafiq\nJoty, and Luo Si. 2022. Enhancing multilingual lan-\nguage model with massive multilingual knowledge\ntriples. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 6878–6890.\nShilei Liu, Xiaofeng Zhao, Bochao Li, Feiliang Ren,\nLonghui Zhang, and Shujuan Yin. 2021a. A\nthree-stage learning framework for low-resource\nknowledge-grounded dialogue generation. In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pages\n2262–2272.\nYe Liu, Yao Wan, Lifang He, Hao Peng, and Philip S\nYu. 2021b. Kg-bart: Knowledge graph-augmented\nbart for generative commonsense reasoning. In\nProceedings of the AAAI Conference on Artificial\nIntelligence, volume 35, pages 6418–6425.\nRobert Logan, Nelson F Liu, Matthew E Peters,\nMatt Gardner, and Sameer Singh. 2019. Barack’s\nwife hillary: Using knowledge graphs for fact-\naware language modeling. In Proceedings of\nthe 57th Annual Meeting of the Association for\nComputational Linguistics, pages 5962–5971.\nShayne Longpre, Kartik Perisetla, Anthony Chen,\nNikhil Ramesh, Chris DuBois, and Sameer Singh.\n2021. Entity-based knowledge conflicts in ques-\ntion answering. In Proceedings of the 2021\nConference on Empirical Methods in Natural\nLanguage Processing, pages 7052–7063.\nWilliam Merrill, Yoav Goldberg, Roy Schwartz, and\nNoah A Smith. 2021. Provable limitations of acquir-\ning meaning from ungrounded form: What will future\nlanguage models understand? Transactions of the\nAssociation for Computational Linguistics, 9:1047–\n1060.\nLeora Morgenstern and Charles Ortiz. 2015. The wino-\ngrad schema challenge: Evaluating progress in com-\nmonsense reasoning. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 29,\npages 4024–4025.\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n2018. Don’t give me the details, just the sum-\nmary! topic-aware convolutional neural networks\nfor extreme summarization. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1797–1807.\nTimothy Niven and Hung-Yu Kao. 2019. Probing neu-\nral network comprehension of natural language argu-\nments. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics,\npages 4658–4664.\n5023\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\net al. 2021. Kilt: a benchmark for knowledge in-\ntensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523–2544.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as\nknowledge bases? In Proceedings of the\n2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2463–2473.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21:1–67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In Proceedings\nof the 2016 Conference on Empirical Methods in\nNatural Language Processing, pages 2383–2392.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426.\nCorby Rosset, Chenyan Xiong, Minh Phan, Xia Song,\nPaul Bennett, and Saurabh Tiwary. 2020. Knowledge-\naware language model pretraining. arXiv preprint\narXiv:2007.00655.\nRobyn Speer, Catherine Havasi, et al. 2012. Represent-\ning general relational knowledge in conceptnet 5. In\nLREC, volume 2012, pages 3679–86.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019. Ernie: Enhanced represen-\ntation through knowledge integration. arXiv preprint\narXiv:1904.09223.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao\nTian, Hua Wu, and Haifeng Wang. 2020. Ernie 2.0: A\ncontinual pre-training framework for language under-\nstanding. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 34, pages 8968–\n8975.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2020. olmpics-on what language\nmodel pre-training captures. Transactions of the\nAssociation for Computational Linguistics, 8:743–\n758.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n353–355.\nCunxiang Wang, Fuli Luo, Yanyang Li, Runxin Xu, Fei\nHuang, and Yue Zhang. 2022. On effectively learn-\ning of knowledge in continual pre-training. arXiv\npreprint arXiv:2204.07994.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,\nXuan-Jing Huang, Jianshu Ji, Guihong Cao, Daxin\nJiang, and Ming Zhou. 2021a. K-adapter: Infus-\ning knowledge into pre-trained models with adapters.\nIn Findings of the Association for Computational\nLinguistics: ACL-IJCNLP 2021, pages 1405–1418.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang.\n2021b. Kepler: A unified model for knowledge\nembedding and pre-trained language representation.\nTransactions of the Association for Computational\nLinguistics, 9:176–194.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nLedell Wu, Fabio Petroni, Martin Josifoski, Sebastian\nRiedel, and Luke Zettlemoyer. 2020. Scalable zero-\nshot entity linking with dense entity retrieval. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 6397–6407.\nWenhan Xiong, Jingfei Du, William Yang Wang, and\nVeselin Stoyanov. 2019. Pretrained encyclopedia:\nWeakly supervised knowledge-pretrained language\nmodel. In International Conference on Learning\nRepresentations.\nIkuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki\nTakeda, and Yuji Matsumoto. 2020. Luke: Deep\ncontextualized entity representations with entity-\naware self-attention. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6442–6454.\n5024\nMichihiro Yasunaga, Jure Leskovec, and Percy Liang.\n2022. Linkbert: Pretraining language models with\ndocument links. In Proceedings of the 60th Annual\nMeeting of the Association for Computational\nLinguistics (V olume1: Long Papers), pages 8003–\n8016.\nDa Yin, Li Dong, Hao Cheng, Xiaodong Liu, Kai-Wei\nChang, Furu Wei, and Jianfeng Gao. 2022. A survey\nof knowledge-intensive nlp with pre-trained language\nmodels. arXiv preprint arXiv:2202.08772.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. Ernie: Enhanced\nlanguage representation with informative entities.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n1441–1451.\nWangchunshu Zhou, Dong-Ho Lee, Ravi Kiran Sel-\nvam, Seyeon Lee, and Xiang Ren. 2020. Pre-training\ntext-to-text transformers for concept-centric com-\nmon sense. In International Conference on Learning\nRepresentations.\nA Analysis\nA.1 Entity Description Probing\nWe analyze the quality of the knowledge injection\nprocess by evaluating the model’s performance on\nentity description probing with structured prompts.\nThis task is aligned with our proposed pre-training\nobjective and reflects the effect of the continued\npre-training. This can be considered as a plug-and-\nplay process for knowledge induction by simply\ninserting the proposed distinct structure. We con-\nduct evaluation on the validation set and a subset\nof the training set with around 10k data samples of\nour pre-training corpus. The data samples in the\ntraining subset are randomly selected, whereas the\ndata samples in the validation set are not included\nin the training process. More specifically, the enti-\nties in the validation set may appear in the training\nset. However, the contexts of the entities in the\nparagraphs do not. We demonstrate the structured\nprompts for entity description probing as follows:\nInput/Prompt: The Joker is a\ncomic book series published by DC\nComics starring the supervillain\nthe <ent> Joker </ent><ent_desc>\n<mask> </ent_desc> .\nTarget: Joker (character) <sep>\nFictional character throughout\nthe DC Universe\nThe example illustrates the input sequence of\nthe encoder, while the prompt to the decoder is\nModel Train subset Valid\nEM F1 EM F1\nBART-base + KILM 37.75 58.08 37.60 58.48\nBART-large + KILM 42.58 61.96 42.84 62.69\nBART-large + KILM†\nEnd 38.64 57.97 38.59 57.71\nTable A1: Results of short description generation on a\nsubset of the training set and the validation set of the\npre-training corpus. †KILMEnd is a variant of KILM for\nablation study (Appendix A.3).\nModel G-RE T-REx C-Net SQuAD\nBERT-base 9.12 30.83 14.29 15.88\nERNIE 6.62 27.58 13.62 14.83\nLM-CORE 23.13 55.32 17.28 16.15\nKALM-base 3.27 25.96 8.61 6.64\nKALM-large 5.41 28.12 10.70 11.89\nBART-base 5.70 22.14 13.88 6.29\n+Merge 5.50 24.98 13.03 7.69\n+KILM 4.02 23.41 12.80 8.39\nBART-large 7.76 26.00 16.07 11.19\n+KILM 6.83 26.14 16.96 11.19\n+KILMDU 3.10 24.99 16.22 12.94\nTable A2: Accuracy on the LAMA benchmark. The best\nresults are marked with underline, while Bold indicates\nthe better result of comparison between BART before\nand after KILM. The results of previous models except\nBART are taken from (Zhang et al., 2019; Rosset et al.,\n2020; Kaur et al., 2022).\nthe same until the <ent_desc> token (marked with\nunderline). Similar to the decoder-only models,\nthe model is expected to continue generating en-\ntity descriptions following the prompt, until the\n</ent_desc> token is generated.\nThe generated entity descriptions are evaluated\nwith exact match (EM) and unigram F1 scores. As\nthe results are shown in Table A1, for KILM in the\nprimary setting, BART models with KILM achieve\naround 40 EM and 60 F1 scores. Interestingly,\nthere is a marginal performance gap between the\nseen and unseen validation sets. The results in-\ndicate our model not only embed the knowledge\nwith its parameters, but also can recall the injected\nknowledge under unseen contexts without much\nperformance loss.\nA.2 LAMA Knowledge Probing\nPetroni et al. (2019) proposed the LAMA bench-\nmark to provide an in-depth study of relational\n5025\nknowledge in PLMs by probing the answers to\n“fill-in-the-blank” cloze statements. Different types\nof relational knowledge are evaluated with state-\nments semi-manually constructed from different\nknowledge sources, including Google-RE (G-RE),\nT-REx (Elsahar et al., 2018), ConceptNet (C-\nNet) (Speer et al., 2012) and SQuAD (Rajpurkar\net al., 2016). We follow the original LAMA\nsettings, while only keeping the data samples\nwhose answer length is 1 after tokenization. The\nprobing input and output format of BART and\nBART+KILM is shown as followings:\nInput/Prompt: The Teatr Wielki\nis a <MASK> .\nTarget: theatre\nSimilar to entity description probing in Ap-\npendix A.1, “Input” and “Prompt” (with under-\nline) are inputs to BART encoder and decoder, re-\nspectively. The generation is considered to be cor-\nrect only if it is exactly the same with“Target”. We\npresent the probing results in Table A2. We also\ninclude the results of BERT (Devlin et al., 2019),\nBERT-based ERNIE (Zhang et al., 2019), BERT-\nbased LM-CORE (Kaur et al., 2022), and GPT-\n2-style KALM (Rosset et al., 2020) for reference.\nHowever, because of the differences on the tok-\nenization and pre-training process, different PLMs\nare not comparable on LAMA benchmark (Jiang\net al., 2020). Even though KILM does not inject re-\nlational knowledge into PLMs, we still observe im-\nprovements after KILM on all the datasets except\nG-RE. As it’s discussed in §5, the injected knowl-\nedge of person-type entities is not aligned with the\nknowledge required by G-RE, since the samples\nfrom G-RE are focused on date_of_birth and\nplace_of_birth relations in the person domain.\nUnder the data upscaling setting, KILM DU fur-\nther enhances the rational knowledge required for\nSQuAD, while LAMA performance is negatively\nimpacted for other datasets. The results indicate\nthat injecting the entity description knowledge also\nhelps models better understand the relationships\nbetween specific entities. Moreover, the results\nof KILMDU suggest that the injected knowledge\nhas closer relevance to the knowledge for SQuAD,\nwhereas far from that of G-RE and T-REx.\nA.3 Ablation Study\nWe conduct an ablation study on the knowledge\ncomponent position in KILM. We compare our\nmethod with KILM variant that moves the knowl-\nedge component (highlighted in blue in Figure 1)\nincluding <ent_desc> and </ent_desc> to the\nend of the target sequence. The variant of the target\nsequence in Figure 1 is as follows:\nThe Joker is a comic book series\npublished by DC Comics starring\nthe supervillain the <ent>\nJoker </ent> . It ran for\nnine ... </s></s> <ent_desc>\nJoker (character)<sep>Fictional\ncharacter throughout the DC\nUniverse </ent_desc>\nWe denote this KILM variant as KILMEnd. We\nevaluate these two models on entity description\nprobing and zero-shot entity disambiguation tasks.\nAs shown in Table A1 and Table C1, BART\nwith KILM consistently outperforms BART with\nKILMEnd on both tasks. Despite the performance\ngap, the advantage of KILM End is that KILMEnd\ncan also be applied to decoder-only models, such\nas GPT-2, for entity knowledge injection.\nA.4 Data Scaling Laws\nAs mentioned in §4.1, we conduct continued pre-\ntraining under two settings: the primary setting\nand the data upscaling setting. While the primary\nsetting only uses the paragraphs in Wikipedia sum-\nmary sections, the data upscaling setting extends\nthe training corpus to the whole Wikipedia corpus,\nwhich enlarges the training set by more than two\nmillion data samples and double the pre-training\ntime. To study the effect of data scaling, we com-\npare the performances of BART-large+KILM under\nprimary and data upscaling settings on knowledge-\nintensive tasks, including entity disambiguation,\nLAMA, and closed-book QA tasks. The evalua-\ntion on entity disambiguation tasks involves six\ndatasets and we only compare the average InKB F1\nscores, since during data scaling, the performances\nare consistently improved across all the datasets.\nIn Figure A1, we show the performance differ-\nence between BART-large+KILM ( or KILMDU)\nand the corresponding baseline models on entity\ndisambiguation, LAMA (in the first row) and QA\n(including three datasets under 0/5-shot in the\nsecond row) tasks. We also display the perfor-\nmance differences along with each bar, where\na positive number denote a better performance\nof BART+KILM. According to the comparison,\n5026\nEDGENRE EDBLINK LAMAGoogle RE\n LAMAT REx\n LAMAConceptNet LAMASQuAD\n10\n5\n0\n5\nDifference\n-0.1\n-4.5\n-0.93\n0.14 0.89 0.0\n2.7\n-0.8\n-4.66\n-1.01\n0.15\n1.75\nTriviaQA0 shot\n NQ0 shot\n WQ0 shot\n TriviaQA5 shot\n NQ5 shot\n WQ5 shot\n10\n0\n10\nDifference -3.3 -1.58 -1.73\n4.74 6.79\n11.13\n0.39\n-0.02\n0.64\n5.14 7.05\n10.59KILM KILMDU\nFigure A1: The performance difference between BART-large+KILM (or KILMDU) and the corresponding baseline\nmodels on entity disambiguation, LAMA and QA (TriviaQA, NQ, and WB) tasks. More specifically, the baseline\nmodels of entity disambiguation tasks are CM3-large and BLINK with GENRE and BLINK candidates, while\nthe baseline model of both LAMA and QA tasks is the original BART-large. We also display the performance\ndifferences along with each bar, where a positive number denotes a better performance of BART+KILM vs the\nbaseline.\nKILM in both settings shows little benefit for\nGoogle-RE and T-REx datasets in LAMA bench-\nmark and makes it harder for the model to recall\nthe relational knowledge in specific domains. On\nthe other hand, for the entitiy-based tasks, such\nas entity disambiguation, the injected knowledge\nthrough KILM equip BART with great zero-shot\nability, comparing to the strong baseline models,\nwhich we’ve discussed in §4.2. For QA tasks,\nBART+KILM in the primary setting performs\nworse than the original BART model in a zero-\nshot manner, however, BART+KILM in data up-\nscaling setting works comparably with the origi-\nnal BART in this case. Together all these compar-\nisons, we conclude that KILM, as a proposed novel\ntechnique for entity-related knowledge injection, is\nable to largely benefit the model in terms of zero-\nshot ability on entity-based knowledge-intensive\ntasks. However, even though we jointly pre-train\nthe model with the original text infilling objective\nof BART, catastrophic forgetting of some specific\nknowledge is unavoidable, especially in the data\nupscaling setting.\nA.5 Case Study\nSome selected data sample from ApposCorpus and\nWoW are shown in Table A3 and Table A4. For\nzero-shot appositive generation task, while the orig-\ninal BART-base model tends to generate apposi-\ntives with similar surface forms to the gold ones or\na piece of text that fit the context, it hallucinates\na lot. BART-base+KILM is more knowledgeable\non the actual meaning of the entities, however, it\nstill make mistakes in terms of the date and spe-\ncific occupation. For KGRG task with task-specific\ntraining, both models are able to generate fluent re-\nsponses. At the same time, BART+KILM tends to\nhallucinate less by including a bit less information\nin some cases.\nB KILM Algorithm\nWe denote the data transformations of the text\ninfilling and sentence permutation objectives for\nBART as TEXT MASK and SENT PERM . In the\noriginal pre-training process of BART, given a tar-\nget sequence with M tokens Y = {t1,t2,...,t M},\nand the corresponding corrupted input sequence\nX = {t′\n1,t′\n2,...,t ′\nN}with N tokens, the model, pa-\nrameterized by θ, is optimized by minimizing the\nreconstruction loss over the whole sequence Y:\nX = SENT PERM (TEXT MASK (Y)) (3)\nL= E(\nM∑\nm=1\n−log p(tm|t1:m−1,X,θ)). (4)\nFor the proposed KILM continued pre-training,\nthe original document, the selected entity, and\nthe corresponding injected knowledge are repre-\nsented as S = {t1,t2,...,t N}, E, and K =\n{k1,k2,...,k L}, respectively. The data transfor-\nmation procedure can be represented as\nY = KNINFILL (S,E, K), (5)\nX = KNMASK (Y). (6)\nThe final loss can be denoted as:\n5027\nAlgorithm 1: KILM Pre-training Process\nInput: Model Mθ, Number of Epochs T,\nWikipedia Corpus S, Knowledge\nCorpus K.\nfor i= 1to T do\nfor each Sj ∈S do\nSample one entity Ei\nj from Sj;\nRetrieve entity knowledge:\nK = LOOK UP(K,Ei\nj);\nConstruct training samples:\nYi\nj = KNINFILL (Sj,Ei\nj,K),\nXi\nj =\nTEXT MASK (KNMASK (Yi\nj));\nOptimize Mθ with Eq. 7.\nend\nend\n0.6\n0.7\n0.8\n0.9\n1.0Accuracy\nAIDA\n MSNBC\n0.6\n0.7\n0.8\n0.9\n1.0Accuracy\nAQUAINT\n ACE2004\nBLINK\nBAb+KILM\nBAl+KILM\n0 1 10 .1k.2k.3k.4k.5k 1k\nMin Freq.\n0.6\n0.7\n0.8\n0.9\n1.0Accuracy\nCWEB\n0 1 10 .1k.2k.3k.4k.5k 1k\nMin Freq.\nWIKI\n0.00\n0.25\n0.50\n0.75\n1.00\nRatio of Supports\nSupport\n0.00\n0.25\n0.50\n0.75\n1.00\nRatio of Supports\n0.00\n0.25\n0.50\n0.75\n1.00\nRatio of Supports\nFigure C1: Results of entity disambiguation tasks with\nthe top five candidates and different minimum frequen-\ncies at which the target entity is sampled during the\ncontinued pre-training. BAb and BAl denote BART mod-\nels in base and large sizes. The primary Y-axis shows\nthe performances of the models after KILM and the\nBLINK baseline on accuracy, while the Y-axis on the\nright shows the number of data samples that satisfy each\nsetting.\nL= (1−α−β)Lcopy + αLinfill + βLkn, (7)\nwhere αand βare calculated based on the propor-\ntion of the corresponding spans across the entire\nsequence. The resulting KILM algorithm for con-\ntinual pre-training is summarized in Algorithm 1.\nC Additional Details for Experiments\nC.1 Pre-training Settings\nWe initialize the model with the original BART\nweights and it is continually trained on eight V100\nGPUs with a batch size of 8,192. The models are\noptimized by the Adam optimizer with a linear\nscheduler and weight decay as 0.01. The peak\nlearning rate is 5e−5. Moreover, the maximum\ntext length of the sequences with a knowledge com-\nponent is set as 640. The mask probability and\nthe hyper-parameter λfor Poisson distribution are\nthe same as those of BART. The implementation is\nmainly based on HuggingFace Transformers (Wolf\net al., 2020) and Datasets (Lhoest et al., 2021) pack-\nages.\nIt is worth mentioning that more than 2.3 million\nentities with short descriptions are involved in the\npre-training, and, needless to say, the occurrence\nof entities in Wikipedia articles is not equally dis-\ntributed. For instance, while only 2,526 entities\nappear more than 1,000 times in the primary set-\nting, 40.5% of the entities only appear once in the\ntraining corpus.\nC.2 Pre-training Format\nWe use a piece of Wikipedia article to demonstrate\nthe input and output formats of the involved pre-\ntrained models involved in Table C6.\nC.3 Zero-shot Entity Disambiguation\nAs shown in §4.2, we include the performance\nof BART and BART+Merge for reference. Due\nto the lack of conventional methods for evaluat-\ning BART models on zero-shot entity disambigua-\ntion tasks, we are inspired by the entity disam-\nbiguation model BLINK (Wu et al., 2020). We\nevaluate BART and BART+Merge by selecting the\nlowest perplexity candidate that generates the cor-\nresponding Wikipedia summary/short description\nfrom a given context. In addition, we also use the\nsame datasets and the candidate sets as those in\nBLINK for more experiments. The InKB micro-F1\nresults are shown in Table C1, where BLINK is\nan entity linking model trained on TACKBP-2010\ndataset. BLINK outperforms BART+KILM in the\nprimary setting in all but one of the datasets, but\nBART+KILMDU in data upscaling setting largely\ncloses the performance gap between BLINK. It\nshould be noted that both BART+KILM is a gen-\neral PLM, while BLINK is not.\n5028\nModels AIDA MSNBC AQUAINT ACE2004 CWEB WIKI Avg #Params\nBLINK† 79.6 80.0 80.3 82.5 64.2 75.5 77.0 336M\nBART-base 18.3 30.8 8.7 20.3 23.7 20.5 20.4 139M\nBART-base+Merge 19.5 24.1 12.2 18.4 21.9 19.8 19.3 139M\nBART-base+KILM 75.1 69.3 67.8 77.4 57.4 62.2 68.2 139M\nBART-large 17.4 39.1 9.6 27.4 26.6 21.5 23.6 406M\nBART-large+KILM 80.1 75.2 71.0 82.4 60.0 66.5 72.5 406M\nBART-large+KILMDU 82.1 76.4 77.8 86.4 62.4 72.3 76.2 406M\nBART-large+KILM‡\nEnd 79.6 74.5 69.6 82.1 59.2 64.2 71.5 406M\nTable C1: InKB Micro F1 on zero-shot entity disambiguation tasks with BLINK candidates. †The results are taken\nfrom https://github.com/facebookresearch/BLINK and normalized over the whole dataset. ‡KILMEnd is a\nvariant of KILM for ablation study (Appendix A.3). #Params denotes the number of parameters of the models.\nModel TriviaQA NQ WQ\nFinetuned settings\nRAG (Open-domain) 68.0 44.5 45.5\nT5-base (Closed-Book) 29.1 25.9 27.9\nOne/Few-shot settings\nKALM-base 5.87 1.75 3.53\nBART-base 9.61 2.19 3.94\n+KILM 12.55 6.95 10.38\nKALM-large 11.68 4.34 6.56\nBART-large 15.74 3.80 9.25\n+KILM 16.42 7.83 12.65\nTable C2: Results on open-domain QA datasets. The\nbest results are marked in bold. The results of the previ-\nous models except BART are taken from (Lewis et al.,\n2020b; Roberts et al., 2020).\nEntity Frequency in Pre-training Data To\nstudy how the frequency of entities appearing in the\npre-training text affects the entity linking perfor-\nmance, Figure C1 also shows the results of experi-\nmenting with data samples with different minimum\nfrequencies of sampling the target entity during\nKILM pre-training in the primary setting. As the\nminimum frequency increases, the gap between\nBART+KILM and BLINK reduces.\nC.4 Appositive Generation\nWe conduct zero-shot probing on ApposCor-\npus (Kementchedjhieva et al., 2020). We display\nthe structured prompts of BART with KILM in\nTable 1. Following ApposCorpus, we use uni-\ngram F1 and METEOR (Banerjee and Lavie, 2005)\nfor evaluation. The results under constrained and\nnon-empty settings are listed in Table C3. Base-\nline results for Person-type entities in News do-\nmain come with the original ApposCorpus pa-\nper, while ApposCorpusconstrained denotes that the\nmodel is trained only with constrained data samples\nand ApposCorpusend2end denotes that the model\nis trained with all the data samples in a specific\ndomain. BART+KILM shows its advantage over\nBART for the Organization-type entities, while\nBART outperforms BART+KILM on all other en-\ntity types. However, as seen in Table 3, the dis-\ntinction in results between human evaluation and\nautomatic metrics demonstrate how the latter do\nnot capture important dimensions such as halluci-\nnations.\nC.5 In-Context Few-Shot QA\nIn Table C2, we list the QA results when provid-\ning one example QA pairs into the inputs (1-shot)\nto BART models with and without KILM. Align-\ning with the QA example in Table 1, the general\nevaluation format is as follows:\nQuestion: Example Q Answer: Example A\\n\nQuestion: Test Q Answer: <mask> .\nBesides BART, we also compare our perfor-\nmances with KALM (Rosset et al., 2020) under\nan 8-shot setting, for which the eight examples are\nhuman-written, and two finetuned models with sim-\nilar model sizes. Despite the performance gap with\nfinetuned models, BART+KILM shows a signifi-\ncant advantage over the original model and KALM\non all the datasets, especially for large-size models.\nThe 1-shot results of BART-base+KILM are even\nhigher than those of KALM-large, which has many\nmore trainable parameters.\nC.6 Fine-tuning Experiments\nFor fine-tuning experiments, including GLUE, sum-\nmarization, and KGRG tasks, we conduct each ex-\nperiment with random seeds 0, 42, and 852. The\n5029\nMethod News ORG News PER Wiki ORG Wiki PER\nF1 METEOR F1 METEOR F1 METEOR F1 METEOR\nConstrained setting\nApposCorpus†\nconstrained - - 19.6 7.9 - - - -\nApposCorpus†\nend2end - - 10.8 3.4 - - - -\nBART-base 8.4 2.4 12.1 5.6 5.2 1.9 9.2 4.3\n+KILM 17.6 8.1 9.7 3.7 9.7 4.4 8.8 3.7\nBART-large 10.8 4.7 15.9 8.3 8.1 3.9 13.1 7.2\n+KILM 18.0 7.8 13.9 5.9 9.5 4.5 9.9 4.4\nNon-empty setting\nBART-base 6.6 2.1 11.7 4.8 4.4 1.6 7.5 3.5\n+KILM 14.1 6.7 7.2 2.7 6.7 3.1 5.8 2.5\nBART-large 8.7 4.0 14.9 6.7 6.8 3.2 10.6 6.0\n+KILM 14.8 6.6 9.7 4.1 6.6 3.2 6.5 3.0\nTable C3: Results on zero-shot Appositive Generation under the constrained and non-empty settings. ORG and PER\nrepresent that the data samples are Person- and Organization-type entities. Bold results denote better performances\nof one over another with the same settings between BART and BART+KILM. †The results are taken from the\noriginal ApposCorpus paper, where ApposCorpusconstrained denotes that the model is trained only with constrained\ndata samples and ApposCorpusend2end denotes that the model is trained with all the data samples in a specific domain.\nThe result hightlighted with underline denotes that it outperforms both BART and BART+KILM.\nModel MNLI SST QQP QNLI STS-B RTE MRPC CoLA Avg\nm/mm Acc Acc Acc Acc Acc Acc Mcc -\nBART-base† 85.7/85.8 93.7 91.3 91.6 89.9 74.3 86.4 51.3 83.3\n+KILM 85.7/85.6 93.0 91.4 91.6 89.8 74.9 87.8 54.2 83.8\nBART-large† 90.0∗/90.0 96.4 92.2 94.8 91.7 ∗ 82.3 89.5 57.1 87.1\n+KILM 89.5/89.8 96.2 92.3∗ 94.7 91.3 87.0∗ 89.6 58.7 87.7\nTable C4: Results on the GLUE benchmark. We report accuracy for the first seven tasks, the Matthews correlation\nfor the CoLA dataset, and the average score (Avg) over all the tasks. ∗p< 0.05 with pairwise t-test.\nModel CNN Dailymail XSUM\nR1 R2 RL R1 R2 RL\nBART-base† 42.79 20.31 39.93 40.83∗ 18.18∗ 33.12∗\n+KILM 42.86 20.24 39.94 40.76 18.15 33.09\nBART-large† 44.14∗ 21.43∗ 41.24∗ 45.17 22.10 37.06\n+KILM 43.15 20.86 40.36 45.07 21.93 36.95\nTable C5: Results on summarization datasets, evaluating\nwith ROUGE metrics. †The results of the BART models\nare re-run with the original settings except maximum\nsequence length to be 1024. ∗p <0.05 with pairwise\nt-test.\nnumbers reported in Table 6, Table C4, Table C5\nand Table 4 above are the averages of the results\nwith three random seeds. The results of BART\nare re-run with the original settings except maxi-\nmum sequence length to be 1024 for summarization\ntasks. Pairwise t-tests are conducted to verify the\nsignificance level of the results of BART+KILM\nover the baseline model.\nC.7 Human Evaluation\nFor both appositive generation and KGRG task,\nwe conduct human evaluation for a comprehensive\nstudy. Pairwise A/B testing is utilized to compare\nthe performances of BART before and after KILM\n(in the primary setting). For each comparison, the\nsame context and two options generated by the\nmodels for comparison are first randomly shuffled\nand then are shown to the annotators. Both tasks\nevaluate the performances on whether the gener-\nations are hallucinated or not, named Not Hallu-\ncinated (NH.). We also include two more factors\nfor each task. For ApposCorpus, we also evaluate\nthe generated appositives from Is Appositive (Ap.)\nand Preference (Pref.), while we evaluate Fluency\n(Flu.) and Informativeness (Info.) for WoW. Be-\ncause the dialogue task feature, we only consider\nthe NH. factor when the generated response is in-\nformative for WoW task. Pairwise A/B testing is\nutilized to compare the performances of BART be-\n5030\nfore and after KILM on both ApposCorpus and\nWoW. Human evaluation is done among a group\nof experts fluent in English coming from countries\nacross Asia. For each comparison, the same context\nand the generations from both models for compari-\nson are shown to the annotators. The annotators are\nsupposed to choose among “generation A”, “gener-\nation B”, “both”, and “neither”. Especially for the\nfactor NH., the annotators are asked to search on\nthe Internet for hallucination validation. Each com-\nparison requires three judgments. We randomly\nsample 50 data samples from each subsets of Ap-\nposCorpus and 100 data samples from each WoW\ntest set. Finally, 600 annotations are collected in\ntotal for both tasks.\nD Datasets\nA number of datasets for downstream task evalua-\ntion are involved in this work:\nGLUE Benchmark GLUE benchmark is a col-\nlection of text classification datasets, which is\nwidely used to evaluate the language modeling abil-\nity of large PLMs. In this benchmark, nine datasets\nare involved, including binary QA and NLI tasks.\nIn this paper, we exclude WNLI (Morgenstern and\nOrtiz, 2015) task during evaluation because there\nare label conflicts in the dataset.12\nSummarization Datasets Text summarization\nis considered an essential NLG task, which re-\nquires the model to generate short summaries of\nlong texts. In this paper, we test our models on\ntwo summarization datasets, CNN/DailyMail and\nXSUM. Summaries in the CNN/DailyMail tend to\nbe more extractive, whereas XSUM contains highly\nabstractive summaries.\nEntity Disambiguation Datasets The entity dis-\nambiguation task is a subtask of entity linking.\nGiven an entity mention in the context, the model\nis expected to select the correct entity among a\nset of similar candidates. Following BLINK (Wu\net al., 2020) and GENRE (De Cao et al., 2020),\nwe test our models on six entity disambiguation\ndatasets, including AIDA-CoNLL dataset (Hof-\nfart et al., 2011), MSNBC, AQUAINT, ACE2004,\nWNED-CWEB (CWEB) (Gabrilovich et al., 2013)\nand WNED-WIKI (WIKI) (Guo and Barbosa,\n2018). We use the candidate sets from BLINK\n12https://gluebenchmark.com/faq\nand GENRE respectively, where those of GENRE\nare originally from Le and Titov (2018).\nApposCorpus Appositives are phrases that ap-\npear next to a named entity to provide background\ninformation (Bauer, 2017; Kang et al., 2019). They\nhelp the readers understand the semantics of the\nnamed entities in the context. ApposCorpus (Ke-\nmentchedjhieva et al., 2020) is constructed as the\nfirst end-to-end dataset for the appositive genera-\ntion task. The selected entities are Person and Or-\nganization entities from Wikipedia (Wiki)and News\narticles. Three types of appositives are included:\nconstrained, empty, and a third type denoted as non-\nempty in this paper. Constrained appositive sam-\nples leverage WikiData for appositive generation,\nwhile empty appositive samples do not require the\nmodel to generate any appositives and non-empty\nsamples require more general knowledge for the ap-\npositive generation. In this paper, since we do not\nconduct task-related training, we only evaluate our\nmodels on constrained and non-empty appositive\nsamples.\nOpen-domain Question Answering Datasets\nWe further evaluate our models on three open-\ndomain QA datasets to test the knowledge capacity:\nTriviaQA (Joshi et al., 2017), Natural Questions\n(NQ) (Kwiatkowski et al., 2019), and Web Ques-\ntions (WQ) (Berant et al., 2013). TriviaQA collects\nthe question-answer pairs from 14 trivia and quiz-\nleague websites, where web pages and Wikipedia\narticles are matched to each question. NQ is a\ndataset of questions from web queries that can be\nanswered with a span of Wikipedia articles. While\nNQ has two types of gold answers, we only eval-\nuate the generations with the short gold answers.\nWQ consists of questions constructed with web\nqueries and FreeBase (Bollacker et al., 2008)\nWizard of Wikipedia (WoW) dataset WoW is a\ncommon crowd-sourcing KGRG dataset that relies\non Wikipedia knowledge to augment the dialogue\nresponses when discussing various topics. Two\nspeakers are provided with an initial topic during\nthe data collection to start the conversation. There\nare two test sets, seen testand unseen testset, split\nfor evaluation, where the initial topics of the dia-\nlogue samples in seen test set appear in the training\nset and vice versa.\n5031\nModel Source Input/Output Format\nBART+KILM\n(ours)\nArticle with Entities:\nThe Joker is a comic book series published\nby [[DC Comics]] starring the supervillain\nthe [[Joker]]. It ran for nine issues from\nMay–June 1975 to Sep.–Oct. 1976.\nEntities & Short Descriptions:\nDC Comics, Inc.:American comic book\npublisher and the flagship unit of DC\nEntertainment, a subsidiary of Warner Bros.\nDiscovery.\nJoker (character):fictional character\nthroughout the DC Universe.\nSample 1\nInput:The Joker <mask>book series published by </ent>\nDC Comics </ent><ent_desc><mask></ent_desc>starring\nthe <mask>the Joker. It ran for nine issues from May–June\n1975 to Sep <mask>.\nOutput:The Joker is a comic book series published by DC\nComics</ent><ent_desc>DC Comics, Inc. <sep>\nAmerican comic book publisher and the flagship unit of DC\nEntertainment, a subsidiary of Warner Bros. Discovery.\n</ent_desc>. starring the supervillain the Joker It ran for\nnine issues from May–June 1975 to Sep.–Oct. 1976.\nSample 2\nInput:The Joker is a comic <mask>by DC Comics starring\n<mask>supervillain the <ent>Joker </ent><ent_desc>\n<mask></ent_desc>. It ran for nine issues from May <mask>\nSep. – Oct. 1976.\nOutput:The Joker is a comic book series published by DC\nComics starring the supervillain the <ent>Joker </ent>\n<ent_desc>Joker (character) <sep>fictional character\nthroughout the DC Universe </ent_desc>. It ran for nine\nissues from May–June 1975 to Sep.–Oct. 1976.\nBART+Merge\n(baseline)\nSample 1\nInput:The Joker <mask>book series published by DC\nComics starring the <mask>the Joker. It ran for nine issues\nfrom May–June 1975 to Sep <mask>.\nOutput:The Joker is a comic book series published by DC\nComics. starring the supervillain the Joker. It ran for nine\nissues from May–June 1975 to Sep.–Oct. 1976.\nSample 2\nInput:DC Comics, Inc. is American <mask>and the flag-\nship unit of DC <mask>, a subsidiary of <mask>Discovery.\nOutput:DC Comics, Inc. is American comic book publisher\nand the flagship unit of DC Entertainment, a subsidiary of\nWarner Bros. Discovery.\nSample 3\nInput:Joker <mask>fictional character <mask>Universe.\nOutput:Joker (character) is fictional character throughout\nthe DC Universe.\nOriginal BART\nInput:It ran for nine issues from May <mask>Sep. –\nOct. 1976. The Joker is a comic <mask>by DC Comics\nstarring <mask>supervillain the Joker.\nOutput:The Joker is a comic book series published by DC\nComics starring the supervillain the Joker. It ran for nine\nissues from May–June 1975 to Sep.–Oct. 1976.\nTable C6: Demonstrations of input and output formats of the pre-trained models involved in this work.\n“BART+KILM” denotes the models that are continued pre-trained with our proposed method; “BART+Merge”\ndenotes the situation when BART model is continued pre-trained on a merge of Wikipedia corpus and the entity\nshort descriptions; “BART” row shows the input and output formats of the original pre-training process of BART\nmodels.\n5032\nTarget Entity: Burma Socialist Programme Party\nContext:\nHe related the quality decline to the socialist\nsystem being practiced by Burma Socialist\nProgramme Party<appos> at the time. As\ncassette players became popular, people were\nselling their old gramophones to dealers.\nAppostive:\nGold BSPP\nBART-base PSP\nBART-base+KILM\nfounded in 1948 by Ne Win, was one of the\ntwo major political parties in Burma during\nthe 1980–1990s\nTarget Entity:Sultana Kamal\nContext:\nOn April 27 , 2013 an alliance of 68\nnon-governmental organisations working for\nwomen ’s rights held a rally in Dhaka to\nspeak out against the proposed subjugation\nof women. Sultana Kamal<appos> told\nreporters there: The people are non-communal\nand religious tolerant.\nAppostive:\nGold a human rights activist\nBART-base the head of the alliance\nBART-base+KILMBangladesh politician\nTable A3: Selected data samples from ApposCorpus, along with generated appositives from both BART-base and\nBART-base+KILM. <appos> marks the position where an appositive is needed to enhance the target entity. In the\nabove two tables, text in orange is not factual correct, while text in blue is not fully accurate.\nDialogue\nHistory:\nB: Skiers used to only use one pole or spear\ninstead of two. Have you every gone skiing?\nA: I haven’t ever gone skiing, but my kids\ndo and love it. Maybe one of these days I’ll\ntry it out. Do you ski?\nResponse:\nBART-base\nB: I do, but I don’t know if I can do it. I do\nknow thatskiing is the most popular sport\nin the world .\nBART-base+KILMB: I do, but I don’t ski much. I’m not a big\nfan of snow skiing.\nDialogue\nHistory:\nA: Nice, I walk my dog at least once a day.\nB: I like watching the dog on \"The Simpsons\".\nHe is funny along with the family and friends.\nA: I love The Simpsons.\nResponse:\nBART-base B: The Simpsons is a great show.It was\ncreated by Homer Simpson and Matt Groening.\nBART-base+KILMB: I love the Simpsons too.It was created by\nMatt Groening.\nTable A4: Selected data samples from WoW, along with generated responses from both BART-base and BART-\nbase+KILM. The highlighted generations in pink is annotated to contain verifiable information.\n5033\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□ A1. Did you describe the limitations of your work?\nLeft blank.\n□ A2. Did you discuss any potential risks of your work?\nLeft blank.\n□ A3. Do the abstract and introduction summarize the paper’s main claims?\nLeft blank.\n□ A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □ Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nLeft blank.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nLeft blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nLeft blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nLeft blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nLeft blank.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nLeft blank.\nC □ Did you run computational experiments?\nLeft blank.\n□ C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nLeft blank.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n5034\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nLeft blank.\n□ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nLeft blank.\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nLeft blank.\nD □ Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nLeft blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nLeft blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nLeft blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nLeft blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nLeft blank.\n5035",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.683130145072937
    },
    {
      "name": "Encoder",
      "score": 0.6448500156402588
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.5250394344329834
    },
    {
      "name": "Computational linguistics",
      "score": 0.4412342309951782
    },
    {
      "name": "Association (psychology)",
      "score": 0.4371100664138794
    },
    {
      "name": "Natural language processing",
      "score": 0.3835567533969879
    },
    {
      "name": "Artificial intelligence",
      "score": 0.32260727882385254
    },
    {
      "name": "Operating system",
      "score": 0.20126089453697205
    },
    {
      "name": "Philosophy",
      "score": 0.1039501428604126
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    }
  ]
}