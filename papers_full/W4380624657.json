{
  "title": "Transformer-based Text Classification on Unified Bangla Multi-class Emotion Corpus",
  "url": "https://openalex.org/W4380624657",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4202324743",
      "name": "Md Sakib Ullah Sourav",
      "affiliations": [
        "Shandong University of Finance and Economics"
      ]
    },
    {
      "id": "https://openalex.org/A2117771112",
      "name": "Huidong Wang",
      "affiliations": [
        "Shandong University of Finance and Economics"
      ]
    },
    {
      "id": "https://openalex.org/A2346969118",
      "name": "Mohammad Sultan Mahmud",
      "affiliations": [
        "Shenzhen University"
      ]
    },
    {
      "id": "https://openalex.org/A2105159432",
      "name": "Hua Zheng",
      "affiliations": [
        "Shenzhen University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3142165045",
    "https://openalex.org/W3128513378",
    "https://openalex.org/W2889149926",
    "https://openalex.org/W3215748564",
    "https://openalex.org/W2739499354",
    "https://openalex.org/W3125546619",
    "https://openalex.org/W3182055376",
    "https://openalex.org/W3155976349",
    "https://openalex.org/W2788420618",
    "https://openalex.org/W2019312772",
    "https://openalex.org/W2926337278",
    "https://openalex.org/W4362712080",
    "https://openalex.org/W2057409323",
    "https://openalex.org/W3089393278",
    "https://openalex.org/W4220866756",
    "https://openalex.org/W3133966466",
    "https://openalex.org/W2995515875",
    "https://openalex.org/W2747541555",
    "https://openalex.org/W1567406843",
    "https://openalex.org/W2988480041",
    "https://openalex.org/W3160518394",
    "https://openalex.org/W3127908888",
    "https://openalex.org/W2250243742",
    "https://openalex.org/W1972095489",
    "https://openalex.org/W4200559659",
    "https://openalex.org/W2986032230",
    "https://openalex.org/W2903101678",
    "https://openalex.org/W2984030454",
    "https://openalex.org/W2963223838",
    "https://openalex.org/W4296976275",
    "https://openalex.org/W2969138254",
    "https://openalex.org/W3118580427",
    "https://openalex.org/W2963647655",
    "https://openalex.org/W2752201871",
    "https://openalex.org/W2953739332",
    "https://openalex.org/W3128437760",
    "https://openalex.org/W3170705121"
  ],
  "abstract": "Abstract Due to its importance in studying people’s thoughts on various Web 2.0 services, emotion classification is a critical undertaking. Most existing research is focused on the English language , with little work on low-resource languages, e.g., Bangla. In recent years, sentiment analysis, particularly emotion classification in English, has received increasing attention, but little study has been done in the context of Bangla (one of the world’s most widely spoken languages). In this research, we propose a complete set of approaches for identifying and extracting emotions from Bangla texts. We provide a Bangla emotion classification for six classes, i.e., anger, disgust, fear, joy, sadness, and surprise, from Bangla words using transformer-based models, which exhibit phenomenal results in recent days, especially for high-resource languages. The Unified Bangla Multi-class Emotion Corpus (UBMEC) is used to assess the performance of our models. UBMEC is created by combining two previously released manually labelled datasets of Bangla comments on six emotion classes with fresh manually labelled Bangla comments created by us. The corpus dataset and code we used in this work are publicly available.",
  "full_text": "Transformer-based Text Classi\u0000cation on Uni\u0000ed\nBangla Multi-class Emotion Corpus\nMd Sakib Ullah Sourav \nShandong University of Finance and Economics\nHuidong Wang \nShandong University of Finance and Economics\nMohammad Sultan Mahmud  \n \nShenzhen University\nHua Zheng \nShenzhen University\nResearch Article\nKeywords: Bangla corpus, Bangla emotion analysis, Text classi\u0000cation, Multi-class emotion\nclassi\u0000cation, Natural language processing\nPosted Date: June 14th, 2023\nDOI: https://doi.org/10.21203/rs.3.rs-3051650/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: No competing interests reported.\nTransformer-based Text Classification on Unified\nBangla Multi-class Emotion Corpus\nMd Sakib Ullah Sourav1, Huidong Wang 1,\nMohammad Sultan Mahmud2*, Hua Zheng 2\n1School of Management Science and Engineering, Shandong University of\nFinance and Economics, Jinan, China.\n2*College of Computer Science and Software Engineering, Shenzhen\nUniversity, Shenzhen, 518060, China.\n*Corresponding author(s). E-mail(s):sultan@szu.edu.cn;\nContributing authors: sakibsourav@outlook.com; huidong.wang@ia.ac.cn;\nzhenghua2017@email.szu.edu.cn;\nAbstract\nDue to its importance in studying people’s thoughts on various Web 2.0 services,\nemotion classification is a critical undertaking. Most existing research is focused\non the English language, with little work on low-resource languages, e.g., Bangla.\nIn recent years, sentiment analysis, particularly emotion classification in English,\nhas received increasing attention, but little study has been done in the context\nof Bangla (one of the world’s most widely spoken languages). In this research,\nwe propose a complete set of approaches for identifying and extracting emotions\nfrom Bangla texts. We provide a Bangla emotion classification for six classes,\ni.e., anger, disgust, fear, joy, sadness, and surprise, from Bangla words using\ntransformer-based models, which exhibit phenomenal results in recent days,\nespecially for high-resource languages. The Unified Bangla Multi-class Emotion\nCorpus (UBMEC) is used to assess the performance of our models. UBMEC\nis created by combining two previously released manually labelled datasets of\nBangla comments on six emotion classes with fresh manually labelled Bangla\ncomments created by us. The corpus dataset and code we used in this work are\npublicly available.\nKeywords: Bangla corpus, Bangla emotion analysis, Text classification, Multi-class\nemotion classification, Natural language processing\n1\n1 Introduction\nWhile enough research has been done to identify emotions from visual and auditory\ndata, emotion recognition from textual data is still a new and active study topic [\n3].\nWeb 2.0 platforms, or social networks (SNs), such as WeChat, Twitter, YouTube,\nInstagram, and Facebook, have recently emerged as the most important platforms for\ncommunication [\n33], education [24], information exchange [32], and other purposes\n[1, 8, 9]. Users of SN connect, share their thoughts, feelings, and ideas, and participate\nin discussion groups. Text conversation, or more specifically, emotion classification\n(EC), is essential to comprehending people’s activities since the internet’s invisible\nnature has made it possible for a single user to engage in violent SN speech data [\n20].\nEC is a subset of sentiment analysis (SA). Text-based SA is usually classified\ninto two types: opinion-based and emotion-based. Text polarity, which divides text or\nsentences into positive, negative, or neutral feelings, is used to classify opinions [\n4]. EC\nis a technique for extracting fine-grained emotions from speech, voice, picture, or text\ndata [\n7]. Understanding the emotion or sentiment behind a particular activity or trend\nin online content is of significant value to businesses, consumers, corporate leaders,\ngovernments, and other interested parties [\n21] because an increasing number of people\non virtual platforms are producing online material at a rapid rate. In many human-\ncomputer interaction (HCI) systems where text is the major form of communication,\ntext classification is also crucial. The significant rise in SNs has caused EC to divert\nits attention to social media data analysis. Nowadays, it is common practice to employ\ncomputational linguistics, machine learning (ML), and deep learning (DL) to assess\nthe emotions or experiences indicated in user-written comments [\n28]. One of the most\ndifficult issues in natural language processing (NLP), a branch of artificial intelligence\n(AI) that requires a comprehension of natural language for many HCI applications\n[\n25], is classifying emotions in text.\n2\nAbout 228 million people speak Bangla as their mother tongue, and another 37\nmillion do so as a second language, making it the fifth most widely used native\nlanguage in the world\n1. Bangla data storage has increased dramatically online\nrecently due to the rise of Web 2.0 apps and related services, similar to other\nmajor languages. Unstructured textual formats such as reviews, opinions, suggestions,\nratings, comments, and feedback are examples of how this data is typically presented.\nDue to the supervised nature of classification approaches, more labelled data must be\nused for ML and DL model training to yield useful results. High-quality Bangla-labeled\ndata is nevertheless scarce in many sectors. Contrary to English and other Western\ndialects, which are acknowledged as rich dialects in terms of linguistics and technology,\nanalyzing these massive volumes of data using NLP to identify underlying sentiments\nor emotions is a challenging research topic for resource-constrained languages like\nBangla [\n11, 17]. The highly inflected elements of the Indo-Aryan language, such as its\n36 different noun forms, 24 different pronoun forms, and more than 160 varied verb\nforms, make the EC operation in Bangla exceptionally difficult.\nBNEmo [\n29] and BEmoC [16] are two Bangla emotion corpora that include 6327\nand 7000 tagged reviews, respectively, and are divided into six groups, i.e., anger,\ndisgust, fear, joy, sorrow, and surprise. However, the development of an automated\nemotion classifier for Bangla literature requires a comparatively larger amount of\nemotion corpus. As a consequence, the transformer model [\n2] is the basis of our\nproposed model’s inspiration as we strive to conduct the EC of Bangla phrases in\nthis study. The aforementioned paper presented a thorough analysis of transformer-\nbased models for emotion detection in texts using pre-trained Bidirectional Encoder\nRepresentations from Transformers (BERT) word embeddings. Recently, a range of\ndownstream NLP applications have shown the efficacy of transformer-based deep\n1https://www.ethnologue.com/language/ben/\n3\nneural network-based architectural models and modifications, especially for resource-\nrich languages (e.g., English). Bangla EC has been the subject of some previous\nstudies, so we want to evaluate it as effectively and credibly as possible.\nIn order to respond to the following two questions, this research study’s goal is:\n(1) Is it possible to determine the emotion expressed by a social network user in\nBangla using a transformer-based DL model? (2) Does the DL approach with BERT\nword embeddings work better than the ML-based approaches to EC for the Bangla\nlanguage? The use of a pre-trained word embedding model for EC of Bangla texts is\nexamined to solve the first study question. Unlike ML-based techniques, Multilingual\nBERT is a DL model based on pre-trained word embedding that is used to examine\nsemantic links between words. The DL models were compared to Bangla EC’s ML-\nbased approaches to answer the second question.\nThe main contributions of our research are as follows:\n• A new, larger, unified 6-class (i.e., anger, disgust, fear, joy, sadness, and surprise)\nEC dataset named Unified Bangla Multi-class Emotion Corpus (UBMEC) has\nbeen constructed for Bangla based on user reviews. It amalgamates two previously\npublished publicly available Bangla emotion corpus, BNEmo and BEmoC, as well\nas additional manually tagged corpus by us, resulting in an adequately developed\nBangla emotion corpus. It is gathered from various domains such as food, software,\nentertainment, politics, sports, and others.\n• A multilingual BERT model (m-BERT) for Bangla EC is being fine-tuned. This\nmodel is based on a BERT base with 12 layers, 768 hidden heads, and 110 M\nparameters and has been trained on 104 languages, including Bangla.\n• A set of baseline results from ML models, i.e., logistic regression (LR), naive Bayes\n(NB), and XGBoost, to create a benchmark for multi-class EC in Bangla.\nThe remainder of the paper is laid out as follows: The related work of EC is\ndiscussed in Section 2. The recommended technique is presented in Section 3. Section\n4\n4 examines the experimental findings and assessment criteria. The paper concludes in\nSection 5.\n2 Related Works\n2.1 Transformer models in text-based emotion classification\n(EC)\nResearchers put their efforts into building state-of-the-art transformer models that\ndetect emotions from a text in rich resource languages, mostly English. Huang\net al. [\n15] achieved F1 scores of 81.5 and 88.5 for the ”Friends” and ”Emotion-\nPush” datasets, respectively, while examining BERT’s emotion identification abilities.\nThere are four emotion classifications in both datasets, ”Friends” and ”Emotion-\nPush,” developed in [\n10]. Huang et al. [15] altered BERT and created two models,\nFriendsBERT and Chat-BERT, for the ”Friends” and ”Emotion-Push”, respectively,\nusing various pre-training tasks before the official training procedure. Using a modified\nBERT model, Huang et al. [\n14] achieved 0.7709 on four categories, i.e., angry, sad,\nhappy, and others, using Twitter data. Malte and Ratadiya [22] constructed the BERT\nmodel on three classes (i.e., overtly aggressive, covertly aggressive, and non-aggressive)\nto detect cyber abuse in English and Hindi texts, which received an F1 score of 0.6244\nand 0.6596, respectively. Using BERT-large on three datasets: ISEAR, SemEval,\nand Emobank, Park et al. [\n27] categorized emotions categorically and predicted the\nvalence-arousal dominance (VAD) scores of dimensional emotions. For categorically\nidentifying texts in the ISEAR and SemEval data, their model received micro F1\nvalues of 0.688 and 0.695, respectively, and VAD scores of 0.659, 0.327, and 0.287. To\nidentify emotions and propaganda, Vlad et al. [\n35] presented an ensemble of BERT,\nbi-LSTM, and capsule models that received a micro F1 score of 0.5868. Kazameini et\nal. [\n19] used BERT word embeddings with bagged SVM to predict author personality\nattributes. They were 59.03 percent accurate on average.\n5\nTo the best of our knowledge, there is only one earlier work on Bangla-based\ntransformer models. Das et al. [12] assessed three transformer models: m-BERT,\nBangla-BERT, and XLM-R on their own developed Bangla corpus BEmoC, along\nwith several ML and DL models. Among all the models in their work, they got the\nhighest weighted F1 score of 69.73%. Trinto et al. [\n34] performed emotion detection\nin six classes in their custom-made dataset containing 2,890 comments from different\ntypes of YouTube videos in Bangla, English, and Romanized Bangla. Using DL models\nsuch as LSTM and CNN, and ML models such as NB and SVM, the best model gives\na weighted F1 score of 53.54%.\n2.2 Datasets for text-based emotion classification (EC)\nThere are enormous publicly available large-scale English datasets on various emotion\nclasses from different domains. The EmotionLines dataset [\n10], consisting of two data\nsubsets (i.e., the Friends and Emotion-Push datasets), was released for the challenge;\nemotions were classified into four labels, i.e., neutral, joy, sadness, and anger, to\nclassify emotion from texts in English. Each subset includes 1,000 English dialogues.\nEmoBank [\n6] is another corpus of 10,000 English sentences balancing multiple\ngenres, annotated with dimensional emotion metadata in the VAD representation\nformat. EmoBank excels with a bi-perspectival and bi-representational design. ISEAR\n(International Survey on Emotion Antecedents and Reactions) is another widely\nrenowned dataset that contains 7,665 sentences. Over a period of many years during\nthe 1990s, a large group of psychologists all over the world collected data for the\nISEAR project [\n31]. Student respondents, both psychologists and non-psychologists,\nwere asked to report situations in which they had experienced all seven major\nemotions, i.e., joy, fear, anger, sadness, disgust, shame, and guilt. In each case, the\nquestions covered the way they had appraised the situation and how they reacted. The\nfinal dataset thus contained reports on seven emotions by close to 3,000 respondents in\n6\n37 countries on all five continents. SemEval-2017 Task 4 [30] is a dataset that contains\n1,250 texts obtained from Twitter, news headlines, Google News, and other sources\nthat were annotated for Ekman’s six basic emotions [\n13]. Mohammad and Marquez\n[23] developed a small dataset named WASSA-2017 Emotion Intensities (EmoInt)\nwith four emotion classes (fear, joy, sadness, and anger), which caught the attention\nof the NLP researchers.\nOn the other hand, only two publicly available datasets on Bangla emotion classes\nhave been published so far (Table\n1). Rahman and Seddiqui [29] presented a manually\nannotated Bangla emotion corpus named BNEmo, which incorporates a diversity of\nfine-grained emotion labels such as sadness, happiness, disgust, surprise, fear, and\nanger. They collected a large amount of raw text data from the user’s comments on\ntwo different Facebook groups (Ekattor TV and Airport Magistrates) and from the\npublic post of a popular blogger and activist. A total of 6,327 comments and reviews\nwere annotated in the six categories. In another recent work by Iqbal et al. [\n16] named\nBEmoC, they labelled a total of 7,000 Bangla texts into six basic emotion categories\nfrom various sources like Facebook and YouTube comments and posts, online blog\nposts, Bangla story books, Bangla novels, text conversations, and newspapers. We\nconsider EC to be as important for Bangla dialects as it is for any other dialect.\nHence, our focus is to build a BERT-based model that can classify Bangla emotions\nfrom texts.\nTable 1 Summary of existing Bangla emotion datasets.\nCorpus Publicly available Classes Total data (reviews)\nBEmoC Yes 6 7,000\nBNEmo Yes 6 6,327\nBangla Youtube Comments Yes 6 2,890\n7\n3 Methods\n3.1 Model and architecture\nIn this section, we present the implementation details of our experimental setup. We\nfine-tuned m-BERT (Fig.\n1), BERT-base-multilingual-uncased2, as this is one of the\nwell-structured backbones, especially for low-resource languages, and there is a lack\nof such robust monolingual Bangla models. It is also the only model that trained in\nBangla until we performed our experiment. The m-BERT is trained in more than\n100 languages and has the largest Wikipedia corpus. Embeddings from BERTbase\nhave 768 hidden units. The BERT configuration model takes a sequence of words\nor tokens at a maximum length of 512 and produces an encoded representation of\ndimensionality 768. When fine-tuned on the downstream sentence classification task,\nvery few changes are applied to the BERTbase configuration. In this architecture, only\nthe [CLS] (classification) token output provided by BERT is used. The [CLS] output\nis the output of the 12th transformer encoder with a dimensionality of 768. It is given\nas input to a fully connected neural network, and the Softmax activation function is\napplied to the neural network to classify the given sentence. Thus, BERT learns to\npredict whether a sentence can be classified as any of the six emotion classes.\nIn this work, we considered two publicly available Bangla corpora on six major\nemotion classes, named BNEmo [\n29] and BemoC [16]. We combined these two datasets\nwith external, manually annotated reviews. We took the reviews from the public\nYouTube comments on famous Bangla novelist and writer Humayun Ahmed’s dramas,\n“�কোথোও �কউ �নই ”, “বহুব্রী�হ”, “নক্ষ��র রোত ”, and comments from BBC Bangla’s various\nFacebook posts on contemporary issues, for instance, war, suicide, hatred, women’s\nabuse, and similar kinds.\n2https://github.com/google-research/BERT/blob/master/multilingual.md\n8\nFig. 1 BERT model for emotion classification in Bangla texts.\nIn the BNEmo dataset, the six emotion classes were named ”sad”, ”happy”,\n”disgust”, ”surprise”, ”fear”, and ”angry” . We changed ”sad” to ”sadness”, ”happy to\n”joy,” and ”angry” to ”anger” to keep all the labels similar to the BEmoC dataset.\nWe performed a few operations as part of the preprocessing step. For instance,\nstopwords are used to remove unnecessary words, fostering programs to deal with\ncrucial ones. Plus, we performed a duplicate drop function if any reviewer mistakenly\nadded to more than one of the above two datasets. After performing such cleaning\noperations, we finally got 6,125 out of 6,327 reviews from BNEmo and 6,838 out\nof 7,000 reviews from BEmoC (Fig.\n2). With our manually annotated reviews,\nwe constructed the Unified Bangla Multi-Class Emotion Corpus (UBMEC), which\naccumulates a total of 13,072 reviews. The data length of UBMEC is 2,68,298 words.\nThe rest of the data is shown in Table\n2.\nThe UBMEC dataset consists of 3,290 “joy”, 2,622 “sadness”, 2,422 “anger”, 2,049\n“disgust”, 1,348 “fear” and 1,341 “surprise” reviews (Fig.3).\n9\nFig. 2 Total number of reviews from different sources in development of UBMEC.\nTable 2 Description of UBMEC dataset.\nData length 268298 words\nData Class 6 (anger, disgust, fear, joy, sadness, and surprise)\nTotal Reviews 13,072\nSize on disk 1,100 KB\nTrain-test data split 9,150, 3,922\nMaximum words in a single review 218\nMinimum words in a single review 6\nAs mentioned by the authors in [18], emotions can be further categorized into\njust four classes: joy, sadness, fear/surprise, and anger/disgust. Because the semantic\nmeanings of a sentence expressing anger/disgust and fear/surprise are so similar,\nwe divided the dataset into 4 classes to observe the outcomes of the emotion\nclassification using our model. After splitting, we got 3,290 ”joy”, 2,622 ”sadness”,\n4,460 ”anger/disgust”, and 2,687 ”fear/surprise” reviews (Fig\n4).\nFrom Table 3, we can see sample annotated reviews in the UBMEC dataset for\neach of the six emotion classes in Bangla sentences.\n10\nTable 3 Sample annotated reviews in UBMEC dataset.\nEmotion Class Meaning Text Sample\nJoy When a writing expresses happy feelings derived\nfrom both new and familiar experiences, it is\nclassified as joyful [\n26].\nএরকম নোটক বোরবোর �দ�খও �বরক্ত লো�গ নো , এমন �লখক আর\n��তীয় একজন হ�ব নো , যোর নোম হুমোয় ু ন আহ�মদ\nSadness If a text communicates a response to loss, it falls\ninto the grief category, and feeling sad helps us\nconvince others that we need aid [\n26].\nস�ত�র জয় সবসময় হয় নোহ্ ! এমন হোজোরও বো�কর আ�� �ম�থ�র\nকোর�� চ�ল �য�তো হয় ! স�ত� �কোথোও �কউ �নই !\nAnger A letter is considered angry if it expresses unjust\ntreatment of us or others [26].\n�দশ যখন মোন ু ষরুপী শু�য়ো�ররো চোলোয়।\nDisgust When a text conveys a disappointed sensation\nabout anything bad or unpleasant that occurs\nphysically or psychologically, it falls into the\ndisgust category [\n26].\nজনগ��র ���টকোল �থং�কং এ�ব�ল�ট প্র�য়োজনীয়তোর তলো�নরও\n�ন�চ। আই ওয়োশ আর সোক�োজ�ম এরো �ব�শ উৎসোহী।\nFear When a literature depicts threats or dangers to\nour safety or existence, it expresses terror [\n26].\n�দ�শ �হন্দ ু মু স�লম�দর দোঙ্গো লোগো�নোর �চষ্টো চল��।\nSurprise A writing is considered surprising if it describes\nan unexpected or surprising incident that\noccurred unexpectedly [\n26].\nসু ইসোইড করো�ক জো��ফোই কর��ন ?!\n11\nFig. 3 Distribution of 6 classes in UBMEC Dataset.\nFig. 4 Distribution of 6 classes in UBMEC Dataset.\n3.2 Model training and fine-tuning\nThe entire EC is accomplished in two steps: the pre-training of the m-BERT language\nmodel took place in the first phase, while the second part involved fine-tuning the\noutermost classification layer. The Bangla Wikipedia has served as the primary source\nfor pre-training the m-BERT. The m-BERT model was tuned using the training\nset of the planned UBMEC dataset, which comprises labelled user evaluations. This\ntechnique has been used in particular to train the fully linked categorization layer.\nLanguage-specific Wikipedia content varies in amount; therefore, data is sampled\nusing an exponentially smoothed weighting (with a factor of 0.7). As a result, low-\nresource languages are more evenly sampled than high-resource languages like English.\nSimilarly, word counts are sampled to ensure that low-resource languages have an\n12\nadequate vocabulary. Categorical cross-entropy was used as the loss function during\ntraining. The lists of hyperparameters used for this research are shown in Table\n4.\nTable 4 m-BERT model\nhyper-parameters.\nHyper-parameter Value\nLearning rate 2e-5\nBatch size 64\nNumber of epochs 5\nAdam epsilon 1e-8\nGradient accumulation steps 1\nHidden size 768\nHidden layers 12\nMaximum sequence length 64\nParameters 110 M\n4 Results and discussions\n4.1 Result of machine learning models and m-BERT model\nWe compared the results of the m-BERT model with those of other ML algorithms.\nFirst, we see the results of logistic regression (LR), naive Bayes (NB), and XGBoost\non datasets BNEmo, BEmoC, and our proposed UBMEC dataset (Fig\n5). The train-\ntest split is the same in both the m-BERT model and the ML model. The results of\nthese three datasets using different ML models are given in detail in Table 5. From\nthe table, we get to see that among all ML models, LR gave the best result. In the\nBEmoC dataset, LR gives 47% accuracy. While the UBMEC dataset exhibits 44%\naccuracy through the LR method. Among all the ML models, it is evident that the\nthree datasets, along with our proposed one, perform consistently. This also validates\nthe correctness of the formation of the UBMEC dataset.\nML models have fewer trainable parameters on average than deep neural networks,\nwhich explains why they learn so rapidly. Instead of using semantic information, these\nclassifiers use the discriminative power of words with respect to their classes to create\n13\nFig. 5 Accuracy of different machine learning models on three emotion corpus.\nTable 5 Six emotion class results using machine learning\nand m-BERT models.\nDataset Models Accuracy Weighted F1-score\nBNEmo\nLR 0.41 0.45\nNB 0.40 0.47\nXGBoost 0.38 0.44\nBEmoC\nLR 0.47 0.47\nNB 0.44 0.48\nXGBoost 0.41 0.42\nUBMEC\nLR 0.44 0.45\nNB 0.39 0.44\nXGBoost 0.39 0.42\nm-BERT 0.61 0.71\nBest result is in bold.\nclass margins. Furthermore, LR outperforms all other ML algorithms since it not only\ndeduces maximum margin hyperplanes but also handles outliers substantially better\nthan other ML algorithms.\nOn the other hand, DL algorithms are substantially more capable of extracting\nhidden patterns than ML learning classifiers, not simply because they automate the\nfeature engineering process. Quite often, ML algorithms are generally found to be less\nsuccessful than DL algorithms in this regard.\n14\nTable 6 Four Emotion classes results using machine\nlearning and m-BERT models.\nDataset ML model Accuracy Weighted F1-score\nUBMEC\nLR 0.53 0.55\nNB 0.51 0.56\nXGBoost 0.49 0.53\nm-BERT 0.69 0.76\nBest result is in bold.\nTable 7 Model performance comparison.\nMethods Weighted F1 (%)\nWord2Vec + LSTM [34] 53.54\nXLM-R [12] 69.73\nm-BERT (proposed) 71.03\nBest result is in bold.\nIn view of getting a different insight, along with six classes of EC, we further\nperformed four classes of EC (i.e., joy, sadness, fear/surprise, and anger/disgust). For\nthis purpose, we split the UBMEC dataset into train-test data splits of 9,141 and\n3,918. This time, the length of the UBMEC dataset is 2,67,847 words. Total reviews\n(before cleaning, after cleaning) = (13,436, 13,059). Table\n6 shows the results for four\nclasses of EC results in different ML and m-BERT models. The performance metrics\nindicate the enhancement of results in classifying the four Bangla emotion classes.\nFrom Tables\n5 and 6, it is observed that the m-BERT model performs well\ncompared to the ML approaches. The highest weighted F1-score for the six Bangla\nemotion classification classes is 71%, and the highest F1-score for the other four classes\nis 76%. We limited the training to only seven epochs because the training of m-BERT\nmodels requires a substantial amount of time. It is obvious that the model can perform\neven better if we train it over a larger number of epochs. Additionally, there are other\npossible approaches. For example, [\n5] is being used to construct monolingual BERT\nmodels dedicated to the Bangla language by enthusiastic Bangla NLP researchers\naround the world. As a result, as a potential follow-up to this research, more advanced\n15\nBERT-based text classification models for Bangla EC can be created, which will surely\nenrich the field of Bangla NLP research.\n4.2 Performance comparison with previous models\nBy assessing the performance in F1-score, we compare the effectiveness of our m-BERT\nmodel with the prior techniques [\n12, 34]. A summary of the comparison is shown in\nTable 7. The findings demonstrate that the proposed m-BERT model performed better\nthan previous approaches for classifying six emotion classes in Bangla by obtaining\nthe highest F1-score (71.03%).\nAs previously indicated, research on applying BERT techniques to evaluate Bengali\nemotions is limited. In this domain few papers have been published, all of which\nmostly employed ML or DL classifiers on a small dataset with domain constraints.\nOn the other hand, our dataset comprises more user ratings than previous research\nand covers numerous genres with six categorization classes: anger, disgust, fear, joy,\nsadness, and surprise.\n5 Conclusions\nIt appears difficult to determine someone’s emotional state from their textual\ncommunication. Understanding the text’s emotions is crucial for human-computer\ninteraction (HCI). In this work, we employed multilingual-BERT (m-BERT) and pre-\ntrained BERT for emotion categorization in Bangla, a language with limited resources.\nBy comparing the outcomes between two earlier, smaller Bangla emotion datasets and\nour newly created unified dataset using ML and m-BERT classification models, we\nwere able to verify the high quality of our generated dataset. In contrast to traditional\nword-based ML methods, our analysis demonstrated that using the pre-trained BERT\nand m-BERT models and optimizing them for downstream Bangla emotion text\nclassification tasks produced higher weighted F1 scores and accuracy metrics.\n16\nDeclarations\n• Funding: None.\n• Conflict of interest/Competing interests: The authors declare that they have no\nconflicts of interest or competing interests.\n• Ethics approval: Not applicable.\n• Consent to participate: Not applicable.\n• Consent for publication: Not applicable\n• Availability of data and materials: The dataset we constructed\nin this paper is available at\nhttps://github.com/Sakibsourav019/\nUBMEC-Unified-Bangla-Multi-class-Emotion-Corpus-/blob/main/UBMEC%\n20Corpus_Sakib(updated).xlsx\n• Code availability: The source code is available at https://github.com/\nSakibsourav019/UBMEC-Unified-Bangla-Multi-class-Emotion-Corpus-\n• Authors’ contributions: M.S.U. Sourav: Conceptualization, Software, Methodology,\nWriting - original draft; H. Wang: Supervision, Methodology, Writing - review &\nediting; M.S. Mahmud: Supervision, Methodology, Writing - review & editing; H.\nZheng: Validation, Resources.\nReferences\n[1] Nisreen M. Abdulsalam and Marwan A Bakarman. Use of social media in food\nsafety in saudi arabia—a preliminary study.AIMS Public Health, 8:322 – 332,\n2021.\n[2] Francisca Adoma Acheampong, Henry Nunoo-Mensah, and Wenyu Chen.\nTransformer models for text-based emotion detection: a review of bert-based\napproaches. Artificial Intelligence Review, 54:5789 – 5829, 2021.\n17\n[3] Alex M.G. Almeida, Ricardo Cerri, Emerson Cabrera Paraiso, Rafael Gomes\nMantovani, and Sylvio Barbon Junior. Applying multi-label techniques in\nemotion identification of short texts.Neurocomputing, 320:35–46, 2018.\n[4] Nirmal Varghese Babu and E. Grace Mary Kanaga. Sentiment analysis in social\nmedia data for depression detection using artificial intelligence: A review.SN\nComputer Science, 3:1 – 20, 2022.\n[5] Abhik Bhattacharjee, Tahmid Hasan, Kazi Samin, M. Sohel Rahman, Anindya\nIqbal, and Rifat Shahriyar. Banglabert: Combating embedding barrier for low-\nresource language understanding.arXiv:2101.00204, 2021.\n[6] Sven Buechel and Udo Hahn. Readers vs. writers vs. texts: Coping with different\nperspectives of text understanding in emotion annotation. InProceedings of the\n11th Linguistic Annotation Workshop, 2017.\n[7] Aaron Frederick Bulagang, James Mountstephens, and Jason Teo. Multiclass\nemotion prediction using heart rate and virtual reality stimuli.Journal of Big\nData, 8:1–12, 2021.\n[8] Severina Cartwright, Hongfei Liu, and Chris Raddats. Strategic use of social\nmedia within business-to-business (b2b) marketing: A systematic literature\nreview. Industrial Marketing Management, 97:35–58, 2021.\n[9] Junhan Chen and Yuan Wang. Social media use for health purposes: Systematic\nreview. Journal of Medical Internet Research, 23, 2021.\n[10] Shengli Chen, Chao-Chun Hsu, Chuan-Chun Kuo, Ting-Hao ’Kenneth’ Huang,\nand Lun-Wei Ku. Emotionlines: An emotion corpus of multi-party conversations.\nArXiv, abs/1802.08379, 2018.\n18\n[11] Avishek Das, MD. Asif Iqbal, Omar Sharif, and Mohammed Moshiul Hoque.\nBemod: Development of bengali emotion dataset for classifying expressions of\nemotion in texts. InIntelligent Computing and Optimization, pages 1124–1136,\nCham, 2021. Springer International Publishing.\n[12] Avishek Das, Omar Sharif, Mohammed Moshiul Hoque, and Iqbal H. Sarker.\nEmotion classification in a resource constrained language using transformer-based\napproach. arXiv:2104.08613, 2021.\n[13] Paul Ekman, Wallace V. Friesen, M O’Sullivan, Anthony Chan, I Diacoyanni-\nTarlatzis, K Heider, Rainer Krause, William A. LeCompte, Thomas K. Pitcairn,\nand Pio Enrico Ricci-Bitti. Universals and cultural differences in the judgments\nof facial expressions of emotion. Journal of personality and social psychology,\n53:712–719, 1987.\n[14] Chenyang Huang, Amine Trabelsi, and Osmar R Zaiane. Ana at semeval-2019\ntask 3: Contextual emotion detection in conversations through hierarchical lstms\nand bert. In International Workshop on Semantic Evaluation, 2019.\n[15] Y.-H. Huang, S.-R. Lee, M.-Y. Ma, Y.-H. Chen, Y.-W. Yu, and Y.-S.\nChen. Emotionx-idea: Emotion bert–an affectional model for conversation.\narXiv:1908.06264, pages 1–6, 2019.\n[16] Md Asif Iqbal, Avishek Das, Omar Sharif, Mohammed Moshiul Hoque, and\nIqbal H. Sarker. Bemoc: A corpus for identifying emotion in bangla texts.SN\nComputer Science, 3:1–17, 2021.\n[17] Muntarin Islam, Shabbir Ahmed Shuvo, Musarrat Saberin Nipun, Rejwan Bin\nSulaiman, Jannatul Nayeem, Zubaer Haque, Md Mostak Shaikh, and\nMd Sakib Ullah Sourav. Efficient approach to using cnn-based pre-trained\n19\nmodels in bangla handwritten digit recognition. In Computational Vision\nand Bio-Inspired Computing, pages 697–716, Singapore, 2023. Springer Nature\nSingapore.\n[18] Rachael E. Jack, Oliver G.B. Garrod, and Philippe G. Schyns. Dynamic facial\nexpressions of emotion transmit an evolving hierarchy of signals over time.\nCurrent Biology, 24(2):187–192, 2014.\n[19] Amirmohammad Kazameini, Samin Fatehi, Yash Mehta, Sauleh Eetemadi, and\nE. Cambria. Personality trait detection using bagged svm over bert word\nembedding ensembles. ArXiv, abs/2010.01309, 2020.\n[20] Lal Khan, Ammar Amjad, Noman Ashraf, and Hsien-Tsung Chang. Multi-class\nsentiment analysis of urdu text using multilingual bert.Scientific Reports, 12(1):1\n– 17, 2022.\n[21] Alexander Ligthart, Cagatay Catal, and Bedir Tekinerdogan. Systematic reviews\nin sentiment analysis: a tertiary study.Artificial Intelligence Review, 54:4997 –\n5053, 2021.\n[22] Aditya Malte and Pratik Ratadiya. Multilingual cyber abuse detection using\nadvanced transformer architecture. InIEEE Region 10 Conference (TENCON),\npages 784–789, 2019.\n[23] Saif M. Mohammad and Felipe Bravo-Marquez. Wassa-2017 shared task on\nemotion intensity. ArXiv, 2017.\n[24] Gemma Nández and Ángel Borrego. Use of social networks for academic purposes:\na case study.The Electronic Library, 31:781–791, 2013.\n20\n[25] Emily Öhman. The validity of lexicon-based sentiment analysis in\ninterdisciplinary research. InNLP4DH, 2021.\n[26] Ekman P. Basic emotions. volume 98, pages 45–60, 1999.\n[27] Sungjoon Park, Jiseon Kim, Jaeyeol Jeon, Heeyoung Park, and Alice H. Oh.\nToward dimensional emotion detection from categorical emotion annotations.\nArXiv, abs/1911.02499, 2019.\n[28] Ishaani Priyadarshini and Chase Cotton. A novel lstm–cnn–grid search-based\ndeep neural network for sentiment analysis. The Journal of Supercomputing,\n77:13911 – 13932, 2021.\n[29] Md Ataur Rahman and Md Hanif Seddiqui. Banglaemotion: A benchmark\ndataset for bangla textual emotion analysis.Mendeley Data, 2020.\n[30] Sara Rosenthal, Noura Farra, and Preslav Nakov. Semeval-2017 task 4: Sentiment\nanalysis in twitter. InInternational Workshop on Semantic Evaluation, 2017.\n[31] Klaus R. Scherer and H G Wallbott. Evidence for universality and cultural\nvariation of differential emotion response patterning.Journal of personality and\nsocial psychology, 66:310–28, 1994.\n[32] Md Sakib Ullah Sourav, Xiaoyun Zhang, and Huidong Wang. Social media as\ninformation support in reducing covid – 19 depressions: Self-efficacy as mediator\nfor behavioral modeling. In 2021 11th International Conference on Intelligent\nControl and Information Processing (ICICIP), pages 82–87, 2021.\n[33] Christoph Stadtfeld, Károly Takács, and András Vörös. The emergence and\nstability of groups in social networks.Social Networks, 60:129–145, 2020.\n21\n[34] Nafis Irtiza Trinto and Mohammed Eunus Ali. Detecting multilabel sentiment\nand emotions from bangla youtube comments. International Conference on\nBangla Speech and Language Processing (ICBSLP), pages 1–6, 2018.\n[35] George-Alexandru Vlad, Mircea-Adrian Tanase, Cristian Onose, and Dumitru-\nClementin Cercel. Sentence-level propaganda detection in news articles with\ntransfer learning and bert-bilstm-capsule model. Proceedings of the Second\nWorkshop on Natural Language Processing for Internet Freedom: Censorship,\nDisinformation, and Propaganda, 2019.\n22",
  "topic": "Bengali",
  "concepts": [
    {
      "name": "Bengali",
      "score": 0.9860345125198364
    },
    {
      "name": "Computer science",
      "score": 0.6812496185302734
    },
    {
      "name": "Natural language processing",
      "score": 0.6326202154159546
    },
    {
      "name": "Sadness",
      "score": 0.6155831217765808
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5877635478973389
    },
    {
      "name": "Surprise",
      "score": 0.5450035929679871
    },
    {
      "name": "Anger",
      "score": 0.5358442664146423
    },
    {
      "name": "Emotion classification",
      "score": 0.5124531388282776
    },
    {
      "name": "Class (philosophy)",
      "score": 0.5064355134963989
    },
    {
      "name": "Sentiment analysis",
      "score": 0.4766205847263336
    },
    {
      "name": "Psychology",
      "score": 0.17800834774971008
    },
    {
      "name": "Communication",
      "score": 0.0717126727104187
    },
    {
      "name": "Psychiatry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I59483232",
      "name": "Shandong University of Finance and Economics",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I180726961",
      "name": "Shenzhen University",
      "country": "CN"
    }
  ]
}