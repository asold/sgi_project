{
  "title": "How fine can fine-tuning be? Learning efficient language models",
  "url": "https://openalex.org/W3023285645",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287264421",
      "name": "Radiya-Dixit, Evani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1837861352",
      "name": "Wang Xin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1902934009",
    "https://openalex.org/W2971246930",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2775461895",
    "https://openalex.org/W2964248489",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2805003733",
    "https://openalex.org/W2913190747",
    "https://openalex.org/W2791091755",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2913946806",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W131533222",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W4313908941",
    "https://openalex.org/W2396767181",
    "https://openalex.org/W2949650786",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2963828549",
    "https://openalex.org/W2912497767",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W2764043458",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2970277060",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2976132230",
    "https://openalex.org/W2943283198",
    "https://openalex.org/W2260663238",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W2907127169",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2771655537",
    "https://openalex.org/W3104033643"
  ],
  "abstract": "State-of-the-art performance on language understanding tasks is now achieved with increasingly large networks; the current record holder has billions of parameters. Given a language model pre-trained on massive unlabeled text corpora, only very light supervised fine-tuning is needed to learn a task: the number of fine-tuning steps is typically five orders of magnitude lower than the total parameter count. Does this mean that fine-tuning only introduces small differences from the pre-trained model in the parameter space? If so, can one avoid storing and computing an entire model for each task? In this work, we address these questions by using Bidirectional Encoder Representations from Transformers (BERT) as an example. As expected, we find that the fine-tuned models are close in parameter space to the pre-trained one, with the closeness varying from layer to layer. We show that it suffices to fine-tune only the most critical layers. Further, we find that there are surprisingly many good solutions in the set of sparsified versions of the pre-trained model. As a result, fine-tuning of huge language models can be achieved by simply setting a certain number of entries in certain layers of the pre-trained parameters to zero, saving both task-specific parameter storage and computational cost.",
  "full_text": "arXiv:2004.14129v1  [cs.CL]  24 Apr 2020\nHow ﬁne can ﬁne-tuning be? Learning efﬁcient language model s\nEvani Radiya-Dixit Xin W ang\nStanford University 1 Cerebras Systems 2\nAbstract\nState-of-the-art performance on language under-\nstanding tasks is now achieved with increasingly\nlarge networks; the current record holder has bil-\nlions of parameters. Given a language model\npre-trained on massive unlabeled text corpora,\nonly very light supervised ﬁne-tuning is needed\nto learn a task: the number of ﬁne-tuning steps is\ntypically ﬁve orders of magnitude lower than the\ntotal parameter count. Does this mean that ﬁne-\ntuning only introduces small differences from the\npre-trained model in the parameter space? If so,\ncan one avoid storing and computing an entire\nmodel for each task? In this work, we address\nthese questions by using Bidirectional Encoder\nRepresentations from Transformers (BER T) as\nan example. As expected, we ﬁnd that the ﬁne-\ntuned models are close in parameter space to the\npre-trained one, with the closeness varying from\nlayer to layer. W e show that it sufﬁces to ﬁne-\ntune only the most critical layers. Further, we\nﬁnd that there are surprisingly many good solu-\ntions in the set of sparsiﬁed versions of the pre-\ntrained model. As a result, ﬁne-tuning of huge\nlanguage models can be achieved by simply set-\nting a certain number of entries in certain lay-\ners of the pre-trained parameters to zero, saving\nboth task-speciﬁc parameter storage and compu-\ntational cost.\n1 Introduction\nModern deep neural networks operate in a regime\nwhere the generalization gap diminishes with growing\n1 W ork done during internship at Cerebras Systems (current\nemail: evanir@stanford.edu).\n2 Correspondence to XW ( poincare.disk@gmail.com).\nProceedings of the 23 rd International Conference on Artiﬁcial In-\ntelligence and Statistics (AIST A TS) 2020, Palermo, Italy. PMLR:\nV olume 108. Copyright 2020 by the author(s).\nmodel capacity, defying the classical bias-variance trade -\noff (Belkin et al., 2018). Increased model capacity al-\nways leads to better generalization, a trend highly promi-\nnent in the natural language understanding domain. From\nBER T (340M parameters, Devlin et al., 2018), to GPT -\n2 (1.5B parameters, Radford et al., 2019), and to Megatron-\nLM (8.3B parameters, Shoeybi et al., 2019), state-of-the-\nart performance in language comprehension tasks keeps\nimproving with larger model capacity.\nThese huge language models are ﬁrst pre-trained on large\ntext corpora. Pre-training is a learning procedure, often\nunsupervised, that yields a good common initialization for\nfurther supervised learning of various downstream tasks.\nThis further learning, called ﬁne-tuning , is an additional\noptimization of model parameters jointly with a very small\nnumber of extra task-speciﬁc parameters ( e.g. T able 1).\nThough larger models generalize better, they are more ex-\npensive computationally, and the costs grow with the num-\nber of tasks learned. The high computational cost is usually\naddressed by network compression techniques that produce\ncompact models for efﬁcient inference ( e.g. Zhao et al.,\n2019). T o reduce task-speciﬁc cost, transfer learning\nand continual learning methods are useful to maximize\nsharing of parameters across tasks ( e.g. Houlsby et al.,\n2019; Liu et al., 2019). In this work, we attempt to\nachieve these two desirable outcomes with a single ef-\nfort. W e use Bidirectional Encoder Representations from\nTransformers (BER T , Devlin et al., 2018) and the Gen-\neral Language Understanding Evaluation (GLUE) bench-\nmark (W ang et al., 2018) as a working example.\nOur intuition comes from the observation that the amount\nof ﬁne-tuning necessary to learn each task is very small\n(ﬁve orders of magnitude smaller than the dimensionality\nof the parameter space, T able 1). This is not surprising: the\nhigh quality of a pre-trained model should naturally lead to\nrather few iterations needed to ﬁne-tune it to perform spe-\nciﬁc tasks. But importantly, such light ﬁne-tuning might re -\nsult in ﬁne-tuned models hypothetically closer 3 to the pre-\ntrained one in parameter space. This suggests a potentially\nhigh degree of computational redundancy across tasks that\n3 This vague notion of closeness, viz. separation by few gra-\ndient update steps in the parameter space, will be made expli cit\nlater in the text.\nHow ﬁne can ﬁne-tuning be? Learning efﬁcient language model s\nθ1\nθ2\nθ3\n0 θ(1)\nθ(3) \nθ(2)\n˜θ = (˜θ1,˜θ2,˜θ3)\nL0-close \nSparsified \n(\nFigure 1: An illustration of L0-close and sparsiﬁca-\ntion constraints on a pre-trained parameter in a three-\ndimensional parameter space. Here ˜θ is the pre-trained\nparameters. Individual ﬁne-tuning procedures for differe nt\ntasks send the pre-trained parameters to distinct optimize d\nparameters in a close L1-neighborhood, e.g. θ(1), θ(2) and\nθ(3), of which each component, viz. θ1, θ2 or θ3, is sub-\nject to change. The L0-closeness constraint (red with its\nsaturation encoding closeness) forces optimization withi n\nthose parameter conﬁgurations that share a certain fractio n\nof components with ˜θ, i.e. having a small number of dif-\nferent components. The sparsiﬁcation constraint (blue wit h\nits saturation encoding density) further conﬁnes optimiza -\ntion to a discrete subset of L0-close parameters, where all\nchanged components have to be set to zero.\nmight be avoided at inference time.\nW e ﬁrst observe that the ﬁne-tuned and pre-trained pa-\nrameters are both L1-close and angular-close in parameter\nspace, consistent with the small number of ﬁne-tuning it-\nerations separating them. Despite this closeness in param-\neter space, the ﬁne-tuned parameters are not constrained\nto share any components with the pre-trained weights, and\nthus are equally expensive to store and to compute per iter-\nation. W e conjecture that there also exist good ﬁne-tuned\nparameters under efﬁcient computational constraints, eve n\nthough they might be more L1-distant or angular-distant.\nIn order to make ﬁne-tuned models share parameters with\nthe pre-trained models, we optimize parameters L0-close\nto pre-trained (Figure 1, red) by ﬁne-tuning only the most\nsensitive layers ( i.e. those most distant in parameter sub-\nspaces) of the network. Furthermore, we attempt to learn a\ntask by sparsifying the pre-trained weights (Figure 1, blue ).\nSurprisingly, our results reveal an abundance of good task-\nspeciﬁc parameter conﬁgurations within spariﬁed versions\nof pre-trained models: a speciﬁc task can be learned by\nsimply masking anywhere between 1% to 40% of the pre-\ntrained weights to zero.\nA major contribution of the present work is the demon-\nstration that ﬁne-tuning can be realized by sparsiﬁcation,\nwhich has favorable practical implications. By forcing ﬁne -\ntuned parameters to be L0-close to the pre-trained ones, one\nonly needs to store a small number of different weights per\ntask, in addition to the common pre-trained weights, sub-\nstantially saving parameter memory when there are many\ntasks to perform. By forcing ﬁne-tuned parameters to be\nsparse, one potentially saves both memory and compute,\nbecause each task only requires a binary mask on top of the\ncommon pre-trained parameters and sparse linear algebraic\noperations could be used instead of dense ones.\n2 Related work\nA large body of literature is concerned with sparsi-\nﬁcation of large networks for efﬁcient inference ( e.g.\nZhu and Gupta, 2017). Our search for L0-close ﬁne-tuning\nsolutions is motivated by the observation that sensitiviti es\nof the optimization objective to different layers in a net-\nwork are highly variable (Zhang et al., 2019). Zhou et al.\n(2019) trained sparse connectivity patterns over randomly\ninitialized network parameters, termed supermasks, sug-\ngesting that sparsiﬁcation plays a role similar and comple-\nmentary to gradient-based learning of the objective. This i s\nalso related to network architecture search (NAS).\nThe most similar study to ours is piggyback and its vari-\nants (Mallya et al., 2018; Mancini et al., 2019), where in a\nmulti-task visual object classiﬁcation scenario, the auth ors\ntrained task-speciﬁc binary masks on top of a shared set of\npre-trained parameters. In this work, we not only applied\nsimilar techniques to larger pre-trained language models,\nbut also studied the sparsity-accuracy tradeoff in a system -\natic way. Houlsby et al. (2019) added adapter modules to\npre-trained language models, achieving parameter sharing\nacross multiple tasks, but not reducing the computational\ncost of the resultant ﬁne-tuned networks. Also, note that\nrandomly generated high-dimensional masks can also sup-\nport multi-task learning, e.g. Cheung et al. (2019).\nT o impose parameter sparseness differentiably in com-\nbination with the L0-closeness constraint, instead of\nprincipled approaches to imposing L0-regularization\n(Louizos et al., 2017), we used the simpler straight-\nthrough estimator, much like binary quantization tech-\nniques (Courbariaux et al., 2015; Courbariaux and Bengio,\n2016); note that this is also used by Mallya et al. (2018)\nand Zhou et al. (2019).\n3 Methods\n3.1 Notations and model architecture\nConsider a pre-trained network Fθ : x ↦→F (x; θ) that\ntransforms input sequence x into a good representation. It\nEvani Radiya-Dixit, Xin W ang\nT able 1: T ask-speciﬁc model information of BERTBASE (parameter count 109M).\nGLUE T ask MNLI QQP QNLI SST -2 CoLA STS-B MRPC RTE\nAdditional parameter count 2,304 1 ,536 1 ,536 1 ,536 1 ,536 768 1 ,536 1 ,536\nFine-tuning iteration count 36,816 34 ,113 9 ,822 6 ,315 804 540 345 234\nis parameterized by θ, noted as subscript for convenience.\nThe ﬁne-tuning procedure to perform a task t ∈ T (T being\nthe set of tasks) can be described as a supervised training\nprocedure of model G(t)\nφ ◦ Fθ : x ↦→y on ﬁne-tuning set{\n(x(t)\ni , y(t)\ni )\n}\n. G(t)\nφ is a task-speciﬁc last layer unique to\ntask t and is parameterized by φ ; ◦ denotes function com-\nposition.\nIn the case of BER T , we have a stack of modules\nFθ = BERTθ ≜ Pθ L+1 ◦ Bθ L ◦ · · · ◦ Bθ 1 ◦ Eθ 0 (1)\n(θ ≜ [θl]L+1\n0 ),\namong which E is the embedding layer, P a ﬁnal pooling\nlayer and each B is a transformer block\nBϑ : x ↦→ LN(x + DO(WOGeLU(WI LN (x\n+DO(WDA(WQx, WK x, WV x)))))), (2)\nwhere ϑ ≜ [WQ, WK , WV , WD, WI , WO] collects all\nthe learnable parameter matrices in the block. WQ,\nWK , WV and WD are the query, key, value, and\ndense self-attention projection matrices, respectively.\nWI and WO are the intermediate and output feed-\nforward matrices, respectively. A(·, ·, ·) represents\nmulti-head scaled dot-product attention (V aswani et al.,\n2017), DO(·) dropout, LN (·) layer normalization, and\nGeLU(·) the Gaussian error linear unit activation func-\ntion (Hendrycks and Gimpel, 2016). W e experimented\nwith the BERTBASE model (Devlin et al., 2018), for which\nL = 12, with total parameter count of 109M 4 . See T able 1\nfor additional task-speciﬁc parameter counts, all 5 orders\nof magnitude smaller than the total parameter count. Opti-\nmization of them alone fails to ﬁne-tune (see Appendix A).\n3.2 GLUE benchmark\nThe GLUE benchmark is a collection of diverse natu-\nral language understanding tasks (W ang et al., 2018):\nCoLA (W arstadt et al., 2018), SST (Socher et al.,\n2013), MRPC (Dolan and Brockett, 2005),\nSTS (Cer et al., 2018), QQP (Shankar Iyer et al., 2017),\nMNLI (Williams et al., 2018), QNLI (Rajpurkar et al.,\n2016), and R TE (Dagan et al., 2006; Bar-Haim et al.,\n2006; Giampiccolo et al., 2007; Bentivogli et al., 2009).\nW e exclude the problematic WNLI set 5 (Levesque et al.,\n4 Pre-trained parameters obtained from\nhttps://github.com/google-research/bert.\n5 See (12) in https://gluebenchmark.com/faq.\n2012). W e ﬁne-tune on these tasks and report the evalu-\nation performances. F1 is reported for QQP and MRPC,\nMatthews correlation for CoLA, Pearson and Spearman\ncorrelation for STS-B, and accuracy for all other tasks.\n3.3 Constrained ﬁne-tuning procedures\nFor all ﬁne-tuning procedures, we use the exact hyperpa-\nrameters as described in the original paper (Devlin et al.,\n2018) unless speciﬁed otherwise, with additional con-\nstraints described as follows. No constraints are imposed\non task-speciﬁc last layers G(t)\nφ .\nL0-close ﬁne-tuning T o search for ﬁne-tuned solutions\nthat are L0-close to the pre-trained parameters, we selec-\ntively ﬁx the least sensitive parameter matrices at their pr e-\ntrained values and perform ﬁne-tuning optimization on a\nlower-dimensional parameter space.\nSparsiﬁcation as ﬁne-tuning In order to search for ﬁne-\ntuned networks that are both sparse and L0-close to the\npre-trained one, we reparameterize the model by a multi-\nplicative binary mask\nθ = ˜θ ⊙ µ , (3)\nwhere ˜θ is the pre-trained parameters, and µ ∈ { 0, 1}N the\nmask, N being the dimensionality of the parameter space,\nand ⊙ the Hadamard product.\nIf learning is purely through optimizing the mask µ\nwhile holding ˜θ constant, the mask is called a super-\nmask (Zhou et al., 2019). Since µ is discrete-valued and\nthus not differentiable, we reparameterize µ as\nµ = Bern(σ(ν )), (4)\nwhere Bern(p) denotes an element-wise independent\nBernoulli sampler with probability p, and σ(·) the sigmoid\nfunction, applied element-wise on ν ∈ RN , the continu-\nous mask parameter that is task-speciﬁc. W e treat gradient\nbackpropagation through µ as a straight-through estima-\ntor, similar to the techniques used in Mallya et al. (2018);\nZhou et al. (2019). Same ﬁne-tuning hyperparameters as\ndescribed in Devlin et al. (2018) were used except for the\nlearning rate (see Appendix B).\nControl over the ﬁnal sparsity 6 is exerted by initializa-\ntion of µ for ﬁne-tuning. W e initialize ν according to a\n6 Deﬁned as the fraction of zero components, equal to one\nminus density .\nHow ﬁne can ﬁne-tuning be? Learning efﬁcient language model s\n2 31\n2\n3\n4\n5\n6\n5 64 8 9 7 11 12 10 \nl\nWQ\nWK\nWV\nWD\nWI\nWO\nL1-distance \n2 31\n0.018 \n0.016 \n0.020 \n0.022 \n0.024 \n0.026 \n5 64 8 9 7 11 12 10 \nl\nWQ\nWK\nWV\nWD\nWI\nWO\nAngular distance \nFigure 2: L1- and angular distances in parameter sub-\nspaces between pre-trained and ﬁne-tuned weights. Shown\nare metrics across the 12 encoder stack layers for the self-\nattention projection matrices ( WQ, WK , WV and WD)\nand feed-forward matrices ( WI and WO). The results pre-\nsented here are for MNLI ﬁne-tuning, but similar patterns\nare observed across all GLUE tasks.\nsoft magnitude-based pruning mask: a fraction of small-\nmagnitude values are initialized to ν = −5 and the rest to\nν = 5. W e found that the initial sparsity directly controls\nthe ﬁnal sparsity (see Appendix C), allowing us to produce\nmasks with sparsity levels ranging from 1% to 89%.\n4 Experimental results\n4.1 Fine-tuned and pre-trained parameters are\nL1-close and angular-close\nW e observe that the original ﬁne-tuning procedures for\nGLUE tasks all take 102 to 104 parameter update steps (T a-\nble 1), negligible compared to the dimensionality of the pa-\nrameter space, viz. 108. Thus, we ﬁrst asked whether ﬁne-\ntuned parameters are indeed close to the pre-trained ones in\nparameter space. W e measured the L1-distances, i.e. L1-\nnorm of parameter difference, and angular distances (T a-\nble 2). Speciﬁcally, we inspect the weight matrices in all\nself-attention layers, of size 768 × 768 where 768 is the\nhidden state dimension. W e report the minimum and maxi-\nmum values across GLUE tasks: R TE showed the smallest\nvalues, and MNLI showed the largest values. Evidently,\nwe see a signiﬁcantly higher L1- and angular-closeness be-\ntween ﬁne-tuned and pre-trained parameters as compared\nto the expected distance between two independent random\ninitializations, or that between an initialization and the pre-\ntrained paremeters. This conﬁrms that, during the course\nof ﬁne-tuning, the very few model parameter updates tra-\nversed a very short distance in the parameter space. Com-\n0.0\n0.2\n0.4\n0.6\n0.8\nBaseline 0 20 30 10 40 60 70 80 90 50 \nTask performance \nGlobal sparsity (%) \nMNLI \nQQP \nQNLI \nSST-2 \nCoLA \nSTS-B \nMRPC \nRTE \nFigure 3: Performance of supermask ﬁne-tuned models\nacross GLUE tasks. W e show the mean of performance\nmetrics across 10 independent Bernoulli sampling proce-\ndures. Note the baseline performance for each task marked\nby the leftmost end of each curve.\nparing the parameter distance across GLUE tasks, we ﬁnd\nthat it scales with the number of ﬁne-tuning iterations (see\nAppendix D).\nFurther, we inspect the closeness in parameter subspaces\nfor each layer. W e found that, though all layers change\nvery little during ﬁne-tuning, there is nevertheless a high\ndegree of variability across different parameter matrices\n(Figure 2). Blocks deeper in the encoder stack are less L1-\nclose but more angular-close than shallower ones. In all\nself-attention modules, value and dense projection matric es\n(WV and WD) change considerably more than query and\nkey projection matrices ( WQ and WK ) during ﬁne-tuning.\n4.2 L0-close ﬁne-tuning\nInspired by the high degree of variability in each layer’s\nparameter change during ﬁne-tuning, we ask whether ef-\nfective ﬁne-tuning can be achieved by optimizing only a\nfraction of layers while having others ﬁxed at pre-trained\nvalues, resulting in ﬁne-tuned models L0-close in parame-\nter space.\nOur results suggest this is indeed feasible (T able 3). In-\nformed by different layers’ sensitivity to ﬁne-tuning, we\nperformed ﬁne-tuning experiments by progressively ex-\ncluding: (1) key projection layers in self-attention acros s\nall encoder stack layers, (2) the penultimate and ultimate\nencoder stacks, and (3) the word embedding layer. Each\nof these exclusions independently or all three combined\ndo not substantially degrade performance, while reducing\nthe number of parameters to ﬁne-tune by up to 40% (from\n109M to 66M).\n4.3 Sparsiﬁcation as ﬁne-tuning\nEncouraged by these results, we ask whether more aggres-\nsive constraints can be imposed on the ﬁne-tuning process\nEvani Radiya-Dixit, Xin W ang\nT able 2: Distance metrics between ﬁne-tuned and pre-trained parame ters, compared against the expected values between\ntwo independent random initializations, either uniformly or normally distributed from − 1√\nH to 1√\nH where H = 768is the\nhidden dimension, as well as those between the pre-trained a nd a random initialization. Statistics presented in the rig htmost\ncolumn are across all GLUE tasks.\nDistance metric Between uniform\ninitializations\nBetween normal\ninitializations\nBetween uniform\ninitialization\nand pre-trained\nBetween normal\ninitialization\nand pre-trained\nBetween ﬁne-tuned\nand pre-trained\n([min,max])\nL1-distance 20.0 ± 0.1 16 .7 ± 0.1 41 .9 ± 15.2 40 .9 ± 15.4 [0 .1,3.3]\nAngular distance 0.500 0 .500 0 .500 0 .500 [0 .001,0.027]\nT able 3: L0-close ﬁne-tuning results: layers excluded from ﬁne-tunin g, corresponding number of parameters remaining\nto ﬁne-tune, and the ﬁne-tuning performance on the MRPC task (F1 score); other GLUE tasks show similar patterns. W e\nreport the mean and standard deviation across 10 independen t runs.\nLayers excluded from ﬁne-tuning T ask-speciﬁc parameter st orage F1 score\nNone (baseline) 109M ﬂoat ( 100%) 89.4 ± 0.7\n(1) Key projection layers in self-attention 102M ﬂoat ( 94%) 89.1 ± 0.8\n(2) Deepest encoder stack layers 95M ﬂoat ( 87%) 88.8 ± 0.9\n(3) W ord embedding layer 86M ﬂoat ( 78%) 89.3 ± 0.8\n(1), (2), and (3) 66M ﬂoat ( 60%) 88.7 ± 0.9\n(1), (2), and (3) with 30% sparse ﬁne-tuning 66M binary ( 60%) 87.4 ± 2.2\n(1), (2), and (3) with 40% sparse ﬁne-tuning 66M binary ( 60%) 86.6 ± 2.2\n2 31\n7.5 \n5.0 \n10.0\n12.5\n15.0\n17.5\n5 64 8 9 7 11 12 10 \nl\nWQ\nWK\nWV\nWD\nWI\nWO\nSparsity (%) \nFigure 4: Supermask sparsity levels across layers. Shown\nis the low-sparsity MNLI supermask with a global spar-\nsity level of 12. 9%; similar patterns are observed across all\nGLUE tasks.\nto further reduce computational cost. Though L0-close\nﬁne-tuning obviates optimization of a substantial fractio n\nof parameters, avoiding full storage of all parameters for\neach ﬁne-tuned task, all operations still need to be per-\nformed at inference time. In order to reduce operations,\nwe seek to sparsify parameters. This amounts to a search\nover a binary mask in a high-dimensional parameter space.\nW e adopt supermask training (see Section 3) to this end.\nFigure 3 shows ﬁne-tuned model performance across\nGLUE tasks obtained by supermask training. Final spar-\nsity level of the supermask is controlled by its initializat ion\n(see Section 3 and Appendix C). W e note that there is lit-\ntle task performance degradation between 1% and 40% pa-\nrameter sparsity, very close to sparse networks produced by\niterative pruning (Zhu and Gupta, 2017) but underperfom-\ning it at high sparsity levels (see Appendix E). Layer-wise\nsparsity levels of supermasks also demonstrate system-\natic trends (Figure 4): (1) across GLUE tasks, WQ, WK\nand WI tend to be sparser than WV , WD and WO, and\n(2) shallower encoder stack layers are sparser than deeper\nones. Moreover, we show that supermask ﬁne-tuning of\nonly a fraction of sensitive layers could also achieve per-\nformance with little degradation from baseline (T able 3).\n4.4 Many good, sparse ﬁne-tuned supermasks exist,\nbut for pre-trained parameters only\nOne surprising ﬁnding of this study is the many occur-\nrences of good ﬁne-tuned parameters among the 2N con-\nﬁgurations in the set\n{\nθ : θ = ˜θ ⊙ µ\n⏐\n⏐\n⏐µ ∈ { 0, 1}N\n}\nviz.\nvertices of an N-dimensional hypercube, even though most\nof them are quite distant from the pre-trained parameters by\nL1-metric.\nFirst, there exist supermasks up to 40% sparse without re-\nmarkable performance degradation for all GLUE tasks, for\nsome tasks even sparser (Figure 3, right end). Second, for\nany task, below this maximum sparsity, we found good\nmasks at any sparsity level (Figure 3), which can be con-\ntrolled by initialization of the supermask (see Appendix C) .\nFinally, while it is natural that performance drops as the\nmask becomes extremely sparse (Figure 3, right end), it is\nrather counterintuitive that there exist good supermasks a t\nthe dense extreme (Figure 3, left end), since we observe\nthat the pre-trained model with only the task-speciﬁc last\nlayer ﬁne-tuned utterly fails to learn any task (Appendix A) .\nNoticeably, good supermasks selectively prune important\nweights of large magnitudes (Appendix F).\nT o understand this phenomenon better, we study the su-\nHow ﬁne can ﬁne-tuning be? Learning efﬁcient language model s\nT able 4: Low-sparsity supermask performance. W e report the sparsit y levels achieved when the supermasks were initial-\nized at 0% sparsity. For several tasks, ﬁne-tuning is achieved with le ss than 3% of pre-trained weights pruned. For the\nsupermask evaluation results, we include the mean and stand ard deviation of 10 Bernoulli samplings of a single run.\nGLUE T ask MNLI QQP QNLI SST -2 CoLA STS-B MRPC RTE\nBaseline 84.3/85.6 88 .5 91 .6 92 .7 55 .2 88 .5 90 .7 67 .1\nSupermask 81.5/82.9 87 .2 89 .8 91 .3 50 .8 88 .2 91 .3 68 .8\n± 0.1 ± 0.1 ± 0.1 ± 0.2 ± 0.8 ± 0.1 ± 0.4 ± 1.0\nFinal sparsity 12.9% 12 .6% 10 .3% 7 .4% 2 .9% 2 .2% 1 .3% 1 .0%\n0.5\n0.6\n0.7\n0.8\n0.9\n0.5 0.6 0.8 0.9 0.7\nLow-sparsity supermask performance \nBaseline performance \nCoLA \nRTE \nMNLI \nQQP \nSTS-B \nQNLI \nSST-2 \nMRPC \nFigure 5: Low-sparsity supermask performance, i.e. task\nperformance of super-masks initialized at 0% sparsity,\ncompared against baseline.\npermasks trained with all-dense initialization (Figure 5) .\nSurprisingly, these low-sparsity supermasks successfull y\nlearn to perform all the tasks without noticeable degra-\ndation from baseline. Essentially, complicated tasks like\nMNLI and QQP can be learned by clamping 12-13% of\nthe pre-trained weights to zero (see Appendix G for how\nmodel performance improves with sparsity), whereas for\nsimple tasks like MRPC and R TE, setting only 1-2% of the\npre-trained weight entries to zero sufﬁces to learn the task\n(T able 4). Fine-tuning can indeed be very ﬁne , suggesting\nrelatively frequent occurrences of good solutions within a\nsparse L0-neighborhood of the pre-trained parameters.\nMoreover, we ask whether such frequent occurrences of\ngood sparsiﬁed versions of parameters is a unique prop-\nerty of the pre-trained weights. In other words, can one\nalso obtain good supermasks on parameters that are not\npre-trained? T o answer this question, we perform super-\nmask ﬁne-tuning on pre-trained parameters with compo-\nnents shufﬂed (thusly norm-preserved). Performance de-\ngrades signiﬁcantly, for instance, for the MRPC task, from\nan F1 score of 91. 3 to 81. 2 with shufﬂed pre-trained param-\neters. It is clear that one cannot obtain any good masks by\ndoing so, suggesting that having high-performance sparsi-\nﬁed versions is unique to pre-trained parameters.\n4.5 T ask-uniqueness of ﬁne-tuned supermasks\nFinally, we ask whether the supermasks learned to per-\nform different tasks share commonalities. Speciﬁcally, we\nquantify the amount of overlapping zeros in learned super-\nmasks across different tasks (Figure 6). It seems the over-\nlaps are not substantially larger than what would have been\ncaused by chance, suggesting that, even though there seem\nto be many good supermasks for each task, these masks are\nlargely distinct from each other, each unique to the task it\nlearns.\n5 Discussion\nOne very puzzling fact about modern deep neural networks\nis that overparameterization helps both generalization an d\noptimization. On the one hand, given an effective network\narchitecture reﬂecting proper inductive biases, better ge n-\neralizing models are always larger models (Hestness et al.,\n2017). On the other hand, sheer increases in dimensionality\nof the parameter space seldom make stochastic gradient-\nbased optimization more difﬁcult: deeper and/or wider net-\nworks take just about the same, if not a lower number\nof training iterations to converge. For example, ResNet-\n18 (11.7M parameters) and ResNet-152 (60.2M parame-\nters) both train to converge, at similar convergence rates,\nin no more than 600K iterations on Imagenet (He et al.,\n2015). Thus, given adequate computing infrastructure, one\nalways trains the largest possible model in order to obtain\nthe best performance. This is perhaps most prominent in\nrecent pre-trained huge language models ( e.g. Devlin et al.,\n2018; Radford et al., 2019; Shoeybi et al., 2019) that have\nachieved state-of-the-art performance on language compre -\nhension tasks. Similarly, ﬁne-tuning larger pre-trained l an-\nguage models is just as easy, if not easier, than ﬁne-tuning\nsmaller ones. Fine-tuning steps are usually ﬁve orders\nof magnitude smaller than the dimensionality of the pa-\nrameter space (T able 1). A direct consequence of this is\nthat, in the parameter space, ﬁne-tuned networks do not\ndeviate substantially from the pre-trained one, which we\nquantitatively establish in this study. Analogous to the\ncontrast between the low generalization performance of\nsmall networks and the high compressibility of large net-\nworks in the case of ResNets ( e.g. Zhu and Gupta, 2017;\nFrankle and Carbin, 2018), we are faced with the high gen-\nEvani Radiya-Dixit, Xin W ang\nMNLI \nQQP SST-2 \nSupermask \nCoLA QNLI MRPC\nRTE STS-B \nCoLA \nRTE \nMNLI\nQQP \nSTS-B \nQNLI\nSST-2 \nMRPC 0.2 \n0.4 \n0.6 \n0.8 \n1.0 \nFraction of overlaping zeros \n0.19 0.18 0.21 0.23 0.38 0.38 0.45 1\n0.18 0.18 0.19 0.18 0.26 0.27 1 0.30\n0.18 0.16 0.16 0.15 0.14 1 0.14 0.13\n0.17 0.16 0.15 0.14 1 0.11 0.11 0.10\n0.17 0.16 0.13 1 0.06 0.05 0.03 0.03\n0.17 0.15 1 0.10 0.05 0.04 0.03 0.02\n0.16 1 0.13 0.10 0.05 0.04 0.02 0.01\n1 0.15 0.13 0.10 0.04 0.04 0.02 0.01\nMNLI \nQQP SST-2 \nRandom mask \nCoLA QNLI MRPC\nRTE STS-B \nCoLA \nRTE \nMNLI\nQQP \nSTS-B \nQNLI\nSST-2 \nMRPC 0.2 \n0.4 \n0.6 \n0.8 \n1.0 \nFraction of overlaping zeros \n0.15 0.14 0.12 0.09 0.04 0.03 0.02 1\n0.15 0.14 0.12 0.09 0.04 0.03 1 0.01\n0.15 0.14 0.12 0.10 0.04 1 0.02 0.01\n0.15 0.14 0.12 0.09 1 0.03 0.01 0.01\n0.15 0.14 0.12 1 0.04 0.03 0.02 0.01\n0.15 0.14 1 0.09 0.04 0.03 0.02 0.01\n0.15 1 0.12 0.09 0.04 0.03 0.02 0.01\n1 0.14 0.12 0.09 0.04 0.03 0.02 0.01\nFigure 6: Fractions of overlap of zero elements in super-\nmasks across GLUE tasks, compared to randomly gener-\nated masks. Each value in the grid shows the fraction of\npruned elements in one task (horizontal axis) that are also\npruned in the other (vertical axis). Here, we show low-\nsparsity supermasks (initialized at 0% sparsity) and com-\npare the masks in the value layer of the ﬁrst encoder, which\nis one of the most sparse layers in the entire model.\neralization performance of large language models and the\nlow level of dissimilarity before and after ﬁne-tuning. Jus t\nas network compression can generate compact models for\nefﬁcient inference, the abovementioned parameter close-\nness can also be taken advantage of to achieve efﬁcient\ncomputation, which we demonstrate in this work.\nW e show that, due to surprisingly frequent occurrences of\ngood parameter conﬁgurations in a close L0-neighborhood\nand in the set of sparsiﬁed large pre-trained language mod-\nels, two techniques are highly effective in producing ef-\nﬁcient ﬁne-tuned networks to perform speciﬁc language\nunderstanding tasks: (1) optimizing only the most sen-\nsitive layers and (2) learning to sparsify pre-trained pa-\nrameters as ﬁne-tuning. In contrast to commonly em-\nployed post-training sparsiﬁcation methods which always\nincur performance degradation, our procedure of sparsi-\nfying pre-trained networks (similar to Mallya et al., 2018;\nMancini et al., 2019) is by itself an optimization process\nthat learns speciﬁc tasks.\n6 Acknowledgements\nW e thank Soﬁa Samaniego de la Fuente for help with the\nexperiments. W e also wish to thank Robert S. Schreiber,\nUrs K ¨ oster, Jorge Albericio, Natalia S. V assilieva, and Ma r-\ncel Nassar for discussions and feedback on the manuscript.\nReferences\nBar-Haim, R., Dagan, I., Dolan, B., Ferro, L., Giampic-\ncolo, D., Magnini, B., and Szpektor, I. (2006). The\nSecond P ASCAL Recognising T extual Entailment Chal-\nlenge.\nBelkin, M., Hsu, D., Ma, S., and Mandal, S. (2018). Rec-\nonciling modern machine learning and the bias-variance\ntrade-off.\nBentivogli, L., Clark, P ., Dagan, I., and Giampiccolo, D.\n(2009). The Sixth P ASCAL Recognizing T extual Entail-\nment Challenge.\nCer, D., Diab, M., Agirre, E., Lopez-Gazpio, I., and Spe-\ncia, L. (2018). SemEval-2017 T ask 1: Semantic T extual\nSimilarity Multilingual and Crosslingual Focused Eval-\nuation. pages 1–14. Association for Computational Lin-\nguistics (ACL).\nCheung, B., T erekhov, A., Chen, Y ., Agrawal, P ., and Ol-\nshausen, B. (2019). Superposition of many models into\none.\nCourbariaux, M. and Bengio, Y . (2016). BinaryNet: Train-\ning Deep Neural Networks with W eights and Activations\nConstrained to +1 or -1. arXiv, page 9.\nCourbariaux, M., Bengio, Y ., and David, J.-P . (2015). Bina-\nryConnect: Training Deep Neural Networks with binary\nweights during propagations. Nips, pages 1–9.\nDagan, I., Glickman, O., and Magnini, B. (2006). The\nP ASCAL Recognising T extual Entailment Challenge. In\nLecture Notes in Computer Science (including subseries\nLecture Notes in Artiﬁcial Intelligence and Lecture Notes\nin Bioinformatics) , volume 3944 LNAI, pages 177–190.\nDevlin, J., Chang, M.-W ., Lee, K., and T outanova, K.\n(2018). BER T: Pre-training of Deep Bidirectional Trans-\nformers for Language Understanding.\nDolan, W . B. and Brockett, C. (2005). Automatically Con-\nstructing a Corpus of Sentential Paraphrases. Proceed-\nings of the Third International W orkshop on P araphras-\ning (IWP2005) , pages 9–16.\nFrankle, J. and Carbin, M. (2018). The Lottery Ticket Hy-\npothesis: Finding Small, Trainable Neural Networks.\nGiampiccolo, D., Magnini, B., Dagan, I., and Dolan, B.\n(2007). The Third P ASCAL Recognizing T extual En-\ntailment Challenge. T echnical report.\nHow ﬁne can ﬁne-tuning be? Learning efﬁcient language model s\nHe, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep\nResidual Learning for Image Recognition. Arxiv .Org,\n7(3):171–180.\nHendrycks, D. and Gimpel, K. (2016). Gaussian Error Lin-\near Units (GELUs).\nHestness, J., Narang, S., Ardalani, N., Diamos, G., Jun,\nH., Kianinejad, H., Patwary, M. M. A., Y ang, Y ., and\nZhou, Y . (2017). Deep Learning Scaling is Predictable,\nEmpirically.\nHoulsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B.,\nde Laroussilhe, Q., Gesmundo, A., Attariyan, M., and\nGelly, S. (2019). Parameter-Efﬁcient Transfer Learning\nfor NLP.\nLevesque, H. J., Davis, E., and Morgenstern, L. (2012).\nThe Winograd Schema Challenge. T echnical report.\nLiu, X., He, P ., Chen, W ., and Gao, J. (2019). Multi-\nT ask Deep Neural Networks for Natural Language Un-\nderstanding.\nLouizos, C., W elling, M., and Kingma, D. P . (2017). Learn-\ning Sparse Neural Networks through $L\n0$ Regulariza-\ntion.\nMallya, A., Davis, D., and Lazebnik, S. (2018). Piggyback:\nAdapting a Single Network to Multiple T asks by Learn-\ning to Mask W eights. In Lecture Notes in Computer Sci-\nence (including subseries Lecture Notes in Artiﬁcial In-\ntelligence and Lecture Notes in Bioinformatics) , volume\n11208 LNCS, pages 72–88.\nMancini, M., Ricci, E., Caputo, B., and Bul ` o, S. R. (2019).\nAdding new tasks to a single network with weight trans-\nformations using binary masks. In Lecture Notes in\nComputer Science (including subseries Lecture Notes in\nArtiﬁcial Intelligence and Lecture Notes in Bioinformat-\nics), volume 11130 LNCS, pages 180–189.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. (2019). Language Models are Unsupervised\nMultitask Learners.\nRajpurkar, P ., Zhang, J., Lopyrev, K., and Liang, P . (2016).\nSQuad: 100,000+ questions for machine comprehension\nof text. In EMNLP 2016 - Conference on Empirical\nMethods in Natural Language Processing, Proceedings ,\npages 2383–2392. Association for Computational Lin-\nguistics (ACL).\nShankar Iyer, Dandekar, N., and Csernai, K. (2017). First\nQuora Dataset Release: Question Pairs.\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P ., Casper,\nJ., and Catanzaro, B. (2019). Megatron-LM: Training\nMulti-Billion Parameter Language Models Using Model\nParallelism.\nSocher, R., Perelygin, A., Wu, J. Y ., Chuang, J., Manning,\nC. D., Ng, A. Y ., and Potts, C. (2013). Recursive deep\nmodels for semantic compositionality over a sentiment\ntreebank. In EMNLP 2013 - 2013 Conference on Empir-\nical Methods in Natural Language Processing, Proceed-\nings of the Conference , pages 1631–1642. Association\nfor Computational Linguistics.\nV aswani, A., Uszkoreit, J., Shazeer, N., Parmar, N., Uszko-\nreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polo-\nsukhin, I. (2017). Attention Is All Y ou Need. (Nips).\nW ang, A., Singh, A., Michael, J., Hill, F ., Levy, O., and\nBowman, S. R. (2018). GLUE: A Multi-T ask Bench-\nmark and Analysis Platform for Natural Language Un-\nderstanding.\nW arstadt, A., Singh, A., and Bowman, S. R. (2018). Neural\nNetwork Acceptability Judgments.\nWilliams, A., Nangia, N., and Bowman, S. (2018). A\nBroad-Coverage Challenge Corpus for Sentence Under-\nstanding through Inference. pages 1112–1122. Associa-\ntion for Computational Linguistics (ACL).\nZhang, C., Bengio, S., and Singer, Y . (2019). Are All Lay-\ners Created Equal?\nZhao, S., Gupta, R., Song, Y ., and Zhou, D. (2019). Ex-\ntreme Language Model Compression with Optimal Sub-\nwords and Shared Projections.\nZhou, H., Lan, J., Liu, R., and Y osinski, J. (2019). Decon-\nstructing Lottery Tickets: Zeros, Signs, and the Super-\nmask.\nZhu, M. and Gupta, S. (2017). T o prune, or not to prune:\nexploring the efﬁcacy of pruning for model compression.\nEvani Radiya-Dixit, Xin W ang\nAppendix A Optimization of task-speciﬁc\nlast layers alone fails to\nﬁne-tune\nOptimization of only task-speciﬁc layers does not lead to\nsuccessful ﬁne-tuning. For instance, for the MRPC task,\nfreezing parameter weights in the pre-trained model and\noptimizing the task-speciﬁc last layer alone yields a non-\nperforming model. Across 10 independent runs, the model\nconsistently predicts all 1’s for the paraphrase classiﬁca-\ntion task, yielding an F1 score of 81. 2. This is a signiﬁ-\ncant degradation compared to the baseline performance of\n89. 4±0. 7 across multiple runs (T able 3). Thus, it is critical\nto ﬁne-tune layers in the pre-trained model and not just the\ntask-speciﬁc layers alone.\nAppendix B Learning rate of supermask\ntraining\nSupermask training requires a much larger learning rate\ncompared to typical training (Zhang et al., 2019). While\na learning rate of 2 × 10− 5 is used for optimizing weights,\na learning rate of 2 × 10− 1 is used for optimizing masks.\nW e notice a degradation in performance at smaller learning\nrates for supermask training (T able 5). This pattern holds\ntrue across GLUE tasks.\nT able 5: MRPC low-sparsity supermask performance at\nlearning rates from 2 × 10− 5 and 2 × 10− 1.\nLearning-rate F1 score\n2 × 10− 1 91.3 ± 0.4\n2 × 10− 2 82.0 ± 0.2\n2 × 10− 3 0.0\n2 × 10− 4 0.0\n2 × 10− 5 0.0\nAppendix C Correlation between initial and\nﬁnal sparsities of supermasks\nThere is no straightforward control of the amount of\nweights pruned in previous reports of supermask train-\ning (Zhang et al., 2019; Mallya et al., 2018). W e ﬁnd that\nsetting the initial sparsity through a soft magnitude-base d\npruning mask controls the ﬁnal sparsity level, which we use\nto produce supermasks of varied sparsity levels. Figure 7\nshows this correlation between initial and ﬁnal sparsities\nof supermasks for different GLUE tasks. W e note that, at\nlower initial sparsity levels, the supermask is pushed to a\ngreater sparsity level, whereas at higher sparsity levels, the\nsupermask is pushed to a lower sparsity level. This pattern\nis similar across GLUE tasks but is most prominent in the\nMNLI task, scaling with the number of ﬁne-tuning steps\n(T able 1).\n0\n10 \n30 \n50 \n70 \n20 \n40 \n60 \n80 \n90 \n0 20 60 80 40 10 30 70 90 50 \nFinal Sparsity (%) \nInitialized sparsity (%) \nMNLI \nQQP \nQNLI \nSST-2 \nCoLA \nSTS-B \nMRPC \nRTE \nFigure 7: Initial versus ﬁnal sparsity levels of supermasks.\nAppendix D Correlation of parameter\ndistance with ﬁne-tuning steps\nIn order to understand how distance in parameter space in-\ncreases as a function of ﬁne-tuning steps, we study this\nrelationship across GLUE tasks. W e ﬁnd that parameter\ndistance scales with the number of ﬁne-tuning steps by a\npower law with exponent close to 0. 5 (Figure 8).\n-6.0\n-5.5\n-5.0\n-4.5\n-4.0\n6 7 9 10 8\nLogarithm of angular distance \nLogarithm of number of fine-tuning steps \nRTE \nMRPC \nSTS-B \nCoLA \nSST-2 \nQNLI \nQQP \nMNLI \nFigure 8: Correlation of parameter distance with the num-\nber of ﬁne-tuning iterations. Shown are angular distances.\nEach data point corresponds to a different GLUE task.\nAppendix E Fine-tuning with iterative\npruning\nW e also use iterative pruning (Zhu and Gupta, 2017) during\nﬁne-tuning to produce sparse models. Pruning is based on\nweight magnitudes in each layer and is performed periodi-\ncally during ﬁne-tuning with sparsity gradually increasin g\nfrom 0% to a ﬁnal level according to a cubic schedule.\nIterative pruning during ﬁne-tuning (Figure 9) outperform s\nsupermask training (Figure 3) at higher sparsity levels.\nWhile supermask training remains successful up to 40%\nsparsity, iterative pruning produces binary masks up to 50%\nHow ﬁne can ﬁne-tuning be? Learning efﬁcient language model s\n0.0\n0.2\n0.4\n0.6\n0.8\n0 20 30 10 40 60 70 80 90 50 \nTask performance \nGlobal sparsity (%) \nMNLI \nSST-2 \nCoLA \nSTS-B \nMRPC \nRTE \nFigure 9: Iterative pruning during ﬁne-tuning. W e plot\nthe evaluation performance at sparsity levels from 10% to\n90% across GLUE tasks. Note the baseline performance\nfor each task marked by the leftmost end of each curve ( 0%\nsparsity).\n-20 -15 -5 0 -10\nLogarithm of pruned weight magnitude  \nPruning mask \nSupermask \nRTE \n-20 -15 -5 0 -10\nLogarithm of pruned weight magnitude  \nPruning mask \nSupermask \nMNLI \nFigure 10: Pruned weight distributions, compared between\nsupermask and magnitude-based pruning. Shown for the\nR TE and MNLI ﬁne-tuning tasks.\nsparse and for some tasks even sparser without signiﬁcant\nperformance degradation. Though iterative pruning pro-\nduces sparse models, the ﬁne-tuned models do not share\nparameters–one still needs to store all parameters for each\ntask. Fine-tuned supermasks, on the other hand, store only\na binary mask of certain layers for each task, with all tasks\nsharing a same set of underlying pre-trained weights.\nAppendix F Fine-tuned supermasks are not\ntrivial\nHow does the learning of a supermask actually work? Does\na supermask simply learn to prune away the weights with\nsmallest magnitudes? Since pure magnitude-based pruning\nof pre-trained weights does not perform any task-speciﬁc\nlearning, we reason that the weight entries being set to zero\nby the supermask must have signiﬁcant values. Here, we\ninspect the magnitudes of the pre-trained weights zeroed\nby the supermasks (Figure 10, T able 6). These weights turn\nout to have remarkably higher magnitudes than the smallest\nentries, suggesting the learning of supermasks is not trivi al\nmagnitude-based pruning.\nAppendix G Learning curves of\nlow-sparsity supermask\nﬁne-tuning\nOur results suggest that supermask ﬁne-tuning, if initial-\nized at 0% sparsity, gradually increases sparsity during op-\ntimization, reaching a ﬁnal sparsity level that correlates\nwith the number of ﬁne-tuning steps (T able 4). For MNLI,\nthe GLUE task with the most ﬁne-tuning steps, the spar-\nsity level reaches 12. 9%. W e ask how prediction accuracy\ngrows with sparsity during ﬁne-tuning. As shown in Fig-\nure 11, like model performance, sparsity rapidly grows dur-\ning the initial phase of ﬁne-tuning. This makes model per-\nformance increase roughly linearly with sparsity.\n30 \n10 \n20 \n0\n0.65\n0.70\n0.75\n0.80\n65 7 9 10 11 12 13 8\nMNLI \nperformance \nNumber of fine-tuning \nitertions (K) \nSparsity (%) \nFigure 11: Learning curves of MNLI low-sparsity super-\nmask ﬁne-tuning.\nEvani Radiya-Dixit, Xin W ang\nT able 6: Comparison between weights pruned with low-sparsity super masks (initialized at 0% sparsity) and weights\npruned with magnitude-based pruning at the same ﬁnal sparsi ty. W e report the maximum and mean magnitude of the\npruned weights. The last row shows percentages of the overla p between the supermask and the magnitude-based pruning\nmask, i.e. the percentages of weights zeroed by the supermask that are a lso the smallest weights.\nGLUE task MNLI QQP QNLI SST -2 CoLA STS-B MRPC RTE\nPruned max 0.0093 0 .0093 0 .0075 0 .0059 0 .0022 0 .0018 0 .0009 0 .0007\nSupermask max 1.7 6 .4 2 .5 1 .7 1 .1 2 .8 1 .8 2 .8\nPruned mean 0.0033 0 .0032 0 .0026 0 .0020 0 .0008 0 .0006 0 .0003 0 .0002\nSupermask mean 0.032 0 .033 0 .033 0 .035 0 .037 0 .036 0 .038 0 .036\nOverlap 11.1% 10 .0% 6 .7% 3 .6% 0 .7% 0 .7% 0 .7% 0 .7%",
  "topic": "Fine-tuning",
  "concepts": [
    {
      "name": "Fine-tuning",
      "score": 0.8771491050720215
    },
    {
      "name": "Language model",
      "score": 0.8216674327850342
    },
    {
      "name": "Computer science",
      "score": 0.7770711183547974
    },
    {
      "name": "Task (project management)",
      "score": 0.6152607798576355
    },
    {
      "name": "Closeness",
      "score": 0.6010246872901917
    },
    {
      "name": "Parameter space",
      "score": 0.5519279837608337
    },
    {
      "name": "Transformer",
      "score": 0.5108880400657654
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5058967471122742
    },
    {
      "name": "Encoder",
      "score": 0.4759744703769684
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4311544597148895
    },
    {
      "name": "Layer (electronics)",
      "score": 0.4141024351119995
    },
    {
      "name": "Machine learning",
      "score": 0.34271955490112305
    },
    {
      "name": "Algorithm",
      "score": 0.3317023515701294
    },
    {
      "name": "Mathematics",
      "score": 0.11101946234703064
    },
    {
      "name": "Programming language",
      "score": 0.08126458525657654
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}