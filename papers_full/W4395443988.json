{
  "title": "Retrieval-Augmented Audio Deepfake Detection",
  "url": "https://openalex.org/W4395443988",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4284625580",
      "name": "Kang, Zuheng",
      "affiliations": [
        "Ping An (China)",
        "Shenzhen Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A4225768295",
      "name": "He, Yayun",
      "affiliations": [
        "Shenzhen Technology University",
        "Ping An (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2388092285",
      "name": "Zhao, Botao",
      "affiliations": [
        "Ping An (China)",
        "Shenzhen Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A4222750030",
      "name": "Qu, Xiaoyang",
      "affiliations": [
        "Ping An (China)",
        "Shenzhen Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A322498120",
      "name": "Peng, Junqing",
      "affiliations": [
        "Shenzhen Technology University",
        "Ping An (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2100842608",
      "name": "XIAO Jing",
      "affiliations": [
        "Ping An (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2386848996",
      "name": "Wang, Jianzong",
      "affiliations": [
        "Ping An (China)",
        "Shenzhen Technology University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3036601975",
    "https://openalex.org/W4389328740",
    "https://openalex.org/W3209984917",
    "https://openalex.org/W2148154194",
    "https://openalex.org/W4312743281",
    "https://openalex.org/W3170179936",
    "https://openalex.org/W4323022270",
    "https://openalex.org/W4385823150",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W3160325739",
    "https://openalex.org/W4226264925",
    "https://openalex.org/W3198329097",
    "https://openalex.org/W3212117663",
    "https://openalex.org/W3197358873",
    "https://openalex.org/W6602661299",
    "https://openalex.org/W4226346368",
    "https://openalex.org/W2936802426",
    "https://openalex.org/W2978329087",
    "https://openalex.org/W3197134965",
    "https://openalex.org/W3198486673"
  ],
  "abstract": "With recent advances in speech synthesis including text-to-speech (TTS) and\\nvoice conversion (VC) systems enabling the generation of ultra-realistic audio\\ndeepfakes, there is growing concern about their potential misuse. However, most\\ndeepfake (DF) detection methods rely solely on the fuzzy knowledge learned by a\\nsingle model, resulting in performance bottlenecks and transparency issues.\\nInspired by retrieval-augmented generation (RAG), we propose a\\nretrieval-augmented detection (RAD) framework that augments test samples with\\nsimilar retrieved samples for enhanced detection. We also extend the\\nmulti-fusion attentive classifier to integrate it with our proposed RAD\\nframework. Extensive experiments show the superior performance of the proposed\\nRAD framework over baseline methods, achieving state-of-the-art results on the\\nASVspoof 2021 DF set and competitive results on the 2019 and 2021 LA sets.\\nFurther sample analysis indicates that the retriever consistently retrieves\\nsamples mostly from the same speaker with acoustic characteristics highly\\nconsistent with the query audio, thereby improving detection performance.\\n",
  "full_text": "Retrieval-Augmented Audio Deepfake Detection\nZuheng Kangâˆ—\nPing An Technology (Shenzhen) Co.,\nLtd.\nShenzhen, China\nkangzuheng896@pingan.com.cn\nYayun Heâˆ—\nPing An Technology (Shenzhen) Co.,\nLtd.\nShenzhen, China\nheyayun097@pingan.com.cn\nBotao Zhao\nPing An Technology (Shenzhen) Co.,\nLtd.\nShenzhen, China\nzhaobotao204@pingan.com.cn\nXiaoyang Qu\nPing An Technology (Shenzhen) Co.,\nLtd.\nShenzhen, China\nquxiaoyang343@pingan.com.cn\nJunqing Peng\nPing An Technology (Shenzhen) Co.,\nLtd.\nShenzhen, China\npengjq@pingan.com.cn\nJing Xiao\nPing An Insurance (Group) Company\nof China\nShenzhen, China\nxiaojing661@pingan.com.cn\nJianzong Wangâ€ \nPing An Technology (Shenzhen) Co.,\nLtd.\nShenzhen, China\njzwang@188.com\nABSTRACT\nWith recent advances in speech synthesis including text-to-speech\n(TTS) and voice conversion (VC) systems enabling the generation of\nultra-realistic audio deepfakes, there is growing concern about their\npotential misuse. However, most deepfake (DF) detection methods\nrely solely on the fuzzy knowledge learned by a single model, result-\ning in performance bottlenecks and transparency issues. Inspired\nby retrieval-augmented generation (RAG), we propose a retrieval-\naugmented detection (RAD) framework that augments test samples\nwith similar retrieved samples for enhanced detection. We also\nextend the multi-fusion attentive classifier to integrate it with our\nproposed RAD framework. Extensive experiments show the supe-\nrior performance of the proposed RAD framework over baseline\nmethods, achieving state-of-the-art results on the ASVspoof 2021\nDF set and competitive results on the 2019 and 2021 LA sets. Further\nsample analysis indicates that the retriever consistently retrieves\nsamples mostly from the same speaker with acoustic character-\nistics highly consistent with the query audio, thereby improving\ndetection performance.\nCCS CONCEPTS\nâ€¢ Information systems â†’Clustering and classification .\nâˆ—Both authors contributed equally to this research\nâ€ Corresponding author: Jianzong Wang, jzwang@188.com\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nICMR â€™24, June 10â€“14, 2024, Phuket, Thailand\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0619-6/24/06. . . $15.00\nhttps://doi.org/10.1145/3652583.3658086\nKEYWORDS\naudio deepfake, deepfake detection, retrieval-augmented detection,\nretrieval-augmented generation, LLM, voice conversion, text-to-\nspeech\nACM Reference Format:\nZuheng Kang, Yayun He, Botao Zhao, Xiaoyang Qu, Junqing Peng, Jing Xiao,\nand Jianzong Wang. 2024. Retrieval-Augmented Audio Deepfake Detection.\nIn Proceedings of the 2024 International Conference on Multimedia Retrieval\n(ICMR â€™24), June 10â€“14, 2024, Phuket, Thailand. ACM, New York, NY, USA,\n9 pages. https://doi.org/10.1145/3652583.3658086\n1 INTRODUCTION\nRecent artificial intelligence (AI) techniques have enabled the gen-\neration of synthesized audio known as DeepFakes (DF) with in-\ncreasing degrees of fidelity to natural human speech. Sophisticated\nDF generation techniques, such as text-to-speech (TTS) and voice\nconversion (VC) to mimic the timbre, prosody, and intonation of\nthe speaker, can now generate audio perceptually indistinguishable\nfrom genuine recordings. However, the potential malicious use of\nsuch AI-synthesized speech has serious personal and societal im-\nplications, including disruption of automatic speaker verification\n(ASV) systems, propagating misinformation, and defaming repu-\ntations. As a result, the vulnerability of current audio-based com-\nmunication systems to synthesized speech poses a serious threat.\nTherefore, the development of effective DF detection techniques is\nurgently needed to address this emerging risk for everyone of us.\nRecent advances in artificial intelligence generation content\n(AIGC) techniques, such as DF generation, however, have made\ndetecting DFs increasingly challenging. In newly organized compe-\ntitions, such as the ASVspoof 2021 [27], ADD 2022 [28], 2023 [29],\neven the state-of-the-art (SOTA) DF detection systems perform\npoorly, with an equal error rate (EER) of over 10% [ 30], making\nthem impossible to detect properly, and unsuitable for commercial\ndeployment. This suggests that the rapid development of DF gener-\nation seems to have greatly outpaced the development of detection\narXiv:2404.13892v2  [cs.SD]  23 Apr 2024\nICMR â€™24, June 10â€“14, 2024, Phuket, Thailand Zuheng Kang et al.\ntechnologies â€“ the traditional detection methods are far from in-\nadequate for identifying current out-of-domain [3] DF-generated\nsamples. Building robust and reliable DF detection systems remains\na critical issue for the research community that has not yet been\nfully resolved.\nRecent decades have witnessed the emergence of various frame-\nworks for detecting DF audio. The predominant frameworks utilize\na pipeline consisting of a front-end feature extractor and a back-end\nclassifier (discussed in Â§2.1). Early works relied on hand-crafted\nfeatures for DF detection with some success, such as Mel-frequency\ncepstral coefficients (MFCC) [10], linear-frequency cepstral coef-\nficients (LFCC) [5, 17, 24], CQT [16, 32], and F0 sub-band feature\n[7]. However, these features exhibit limited performance due to the\nlimited dataset used to train the robust model. More recent SOTA\nframeworks have leveraged the capabilities of self-supervised model\nfeature extractors, such as wav2vec [1, 2, 18, 21, 25] and WavLM\n[2, 4, 9]. Moreover, Kawa et al. [13] utilize another powerful task-\nspecific feature in deepfake detection using the pre-trained Whisper\nmodel [19]. Theoretically, by training on a large number of labeled\nor unlabeled bonafide samples in very large datasets, these feature\nextractor models could be particularly sensitive to unseen DF arti-\nfacts. Some alternative frameworks replace hand-crafted features\nwith end-to-end trainable encoders. Notable examples include Jung\net al. â€™s advanced graph attention network architecture AASIST [26],\nand Huang et al. â€™s discriminative frequency-based improvements to\nSincNet [12], both of which achieve competitive performance. As\ndiscussed by Sun et al. [20], vocoders employed in regular speech\nsynthesis can introduce inconsistencies that reveal DFs. However,\nregardless of the methodology and architecture used, these frame-\nworks rely solely on a single model to accomplish this challenging\ntask, which may prove inadequate.\nDF detection should be a knowledge-intensive task that also\nrelies on vast external knowledge. To explain this, letâ€™s start with\na story. In the identification of antique artifacts, fakes are often\nfabricated so realistically that it is difficult to determine their au-\nthenticity. Senior experts usually conduct a meticulous comparative\nanalysis that includes material and textural attributes of many simi-\nlar artifacts. With so many subtle factors to consider, relying solely\non oneâ€™s limited intelligence is likely to result in poor judgment.\nSimilarly, relying only on a single model to detect deepfakes may\nbe too challenging to make mistakes.\nRetrieval augmented generation (RAG) [8, 15] methodology pro-\nvides a nice example for solving the problem of knowledge-intensive\ntasks (discussed in Â§2.2). The RAG resolved the limitation of the\nsingle model by combining a pre-trained large language model\n(LLM) with an information retrieval system over a large knowledge\ndatabase. Fundamentally, the RAG framework leverages significant\nsupplementary real-time updated additional interpretable knowl-\nedge (with rapidly changing, proprietary data sources) to augment\nthe limitations of a single modelâ€™s knowledge, enabling it to provide\na more reasoned answer. Complementary background information\nallows a single model to overcome its inherent knowledge gaps.\nSimilarly, the retrieval-augmented-based approach could provide\nthe same benefits for DF detection. When analyzing suspect au-\ndio, the model could query a retrieval system to find many similar\nreference audio segments. These retrieved results would provide\nadditional references to inform the deepfake detection, such as\ntypical artifacts for bonafide or synthetic spoofed examples. Then,\nthe deepfake detector could integrate these retrieval results and the\nsuspected audio into its decision process. This retrieval-augmented\napproach has several advantages. The model gains access to a much\nlarger knowledge base beyond what can be encoded only in its\nmodel parameters. The retrieved results also provide supporting\nevidence for decisions, improving model performance. Furthermore,\nthe system can update or modify its knowledge database for differ-\nent detection tasks, since the model has learned to characterize a\nlimited type of DF synthesizing methods. In summary, augmenting\ndeepfake detectors with conditional retrieval of external data is an\nattractive direction. Developing DF detection methodology with\na retrieval-augmented approach is worthy of further exploration.\nTo implement the above-mentioned issue, we made the following\ncontributions:\nâ€¢We proposed a retrieval augmented detection (RAD) frame-\nwork which innovatively retrieves similar samples with addi-\ntional knowledge, and incorporates them into the detection\nprocess for improved detection performance.\nâ€¢We extend the multi-fusion attentive classifier to integrate\nwith RAD.\nâ€¢Extensive Experiments show that our proposed method achieves\nstate-of-the-art results on the ASVspoof 2021 DF set and com-\npetitive results on the 2019 and 2021 LA sets, demonstrating\nthe effectiveness of the proposed retrieval-augmented ap-\nproaches.\n2 PRELIMINARY\n2.1 Traditional Frameworks\n(1) Pipeline Framework. The most common pipelines typically in-\nclude separate components for feature extraction, and classification\n(Figure 1-1). Specifically, the front-end feature extractor first con-\nverts the raw speech signalğ‘¥into a speech featuresğ‘¦. These speech\nfeatures are then passed to the back-end classifier, which analyzes\nthe speech features that make a bonafide vs. spoof decision ğ‘§. Such\narchitectures leverage the capabilities of efficient hand-crafted fea-\ntures or semantically rich self-supervised pre-trained features to\nobtain highly informative speech representations. The back-end\nclassifier then fully analyzes and mines these features, and makes\nthe final prediction.\n(2) End-to-End Framework. More advanced frameworks transform\nthe feature extraction into a trainable encoder, forming an end-\nto-end architecture (Figure 1-2). Differently, instead of the feature\nextractor, the raw speech ğ‘¥ is fed into a trainable encoder to pro-\nduce speech representations ğ‘¦. To further improve performance,\nsome approaches fuse multiple feature extractors or encoders, con-\ncatenating the resulting speech features and representations for\ndeeper training.\n2.2 Retrieval Augmented Generation\nPrior to introducing our proposed approach, it is instructive to\nfirst provide background knowledge on the retrieval augmented\ngeneration (RAG) framework [15] to facilitate comparison with our\nmethod. The RAG framework consists of three main stages:\nRetrieval-Augmented Audio Deepfake Detection ICMR â€™24, June 10â€“14, 2024, Phuket, Thailand\nFigure 1: The overview of traditional frameworks, and our\nproposed framework for audio deepfake detection. (1) shows\nthe pipeline framework. (2) shows the end-to-end framework.\n(ours) shows our proposed retrieval augmented-based detec-\ntion (RAD) framework.\n(1) Build Knowledge Retrieval Database. As shown in the stage 1\n(blue section) of Figure 3-RAG, the plain text and format-rich text\ndatabase ğ‘¥ is partitioned into smaller chunks {ğ‘¥ğ‘›}at ğ‘›th sample.\nThese text chunks are then embedded into dense vector represen-\ntations {ğ‘£ğ‘›}by a language model. In addition, each embedding\nğ‘£ğ‘› maintains an index that links it to its original text chunk ğ‘¥ğ‘›,\nallowing the retrieval of the original text content. Finally, these em-\nbeddings {ğ‘£ğ‘›}can be stored in a vector database Vthat facilitates\nefficient similarity search and retrieval.\n(2) Retrieve Knowledge. As shown in the stage 2 (red section) of\nFigure 3-RAG, the userâ€™s query text Ëœğ‘¥ğ‘ is embedded into a query\nembedding Ëœğ‘£ğ‘ using the same language model. This query embed-\nding is then leveraged to perform a similarity search across the\nvector database Vcontaining all the document chunk embeddings.\nThe top ğ¾ most similar embeddings {Ëœğ‘£ğ‘˜}related to its document\nchunks {Ëœğ‘¥ğ‘˜}are retrieved based on their semantic proximity to\nthe query embedding Ëœğ‘£ğ‘ in the vector space. These most relevant\nğ¾ document chunks {Ëœğ‘¥ğ‘˜}can be used as augmented contextual\ninformation to complement the original user query.\n(3) Get Results (Answer Generation). As shown in stage 3 (green\nsection) of Figure 3-RAG, the original user query Ëœğ‘¥ğ‘ is concate-\nnated with the retrieved document chunks {Ëœğ‘¥ğ‘˜}to construct an\nexpanded prompt ğ‘ with its function P. Where ğ‘ = P\u0000Ëœğ‘¥ğ‘,{Ëœğ‘¥ğ‘˜}\u0001.\nThis enriched prompt ğ‘, which contains both the initial query and\nrelevant contextual information, is subsequently input to a large\nlanguage model (LLM). The LLM analyzes the overall content and\nrelationships within ğ‘ to generate the final answer ğ‘§.\n3 METHODOLOGY\n3.1 Self-supervised Feature with WavLM\nThe overall framework of our proposed method is illustrated in Fig-\nure 1. Unlike traditional methods (described in Â§2.1), our proposed\nframework leverages the state-of-the-art WavLM [4] feature extrac-\ntor and incorporates an additional retrieval module after feature\nextraction to overcome performance bottlenecks. Specifically, we\nadopt a retrieval-augmented structure similar to RAG (described\nin Â§2.2), which retrieves a few similar features from the bonafide\nsamples and fuse them with the original test features before feeding\ninto the detection model. By incorporating retrieved features highly\nsimilar to the test sample, our model can make much more reliable\npredictions through joint analysis.\nIn the following section, we describe these modules in detail,\nincluding the WavLM feature extractor (described in Â§3.1), the\nretrieval augmented mechanism (described in Â§3.2), and the design\nof the detection model classifier (described in Â§3.3). Meanwhile,\nsince this framework is complex, we present a number of speed-\nup techniques (described in Â§3.4) to greatly reduce the space and\ntime complexity. To further improve performance, we also jointly\noptimize the WavLM feature extractor with the detection model in\nan end-to-end manner (described in Â§3.1).\nWavLM Feature Extraction. Recent advanced feature extractor WavLM\n[4] employs wav2vec 2.0 [ 1] as its backbone and is trained with\nlarger real multilingual, multi-channel unlabeled speech data for\nmuch better performance. WavLM utilizes a masked speech de-\nnoising and prediction framework that artificially adds noise and\noverlapping speech to clean input audio before masking certain time\nsegments, and the model should then predict the speech content\nof the original frames of these masked segments. This denoising\nprocess allows WavLM to learn robust representations that capture\nnot only a variety of speech features but also the acoustic envi-\nronment. In addition, WavLM performs excellently in a variety of\ndownstream speech tasks such as automatic speech recognition\n(ASR), automatic speaker verification (ASV), and text-to-speech\n(TTS) with minimal fine-tuning. This suggests that WavLM already\nunderstands and is familiar with many high-level speech charac-\nteristics of bonafide audio, which are particularly appropriate for\nunseen DF-synthesized audio, since it often contains features that\nare very different from bonafide audio.\nIn the proposed framework, the feature extraction component\nutilizes the complete set of latent features ğ‘¦ âˆˆRğ¿Ã—ğ‘‡Ã—ğ¹ from all\nlayers of the WavLM encoder transformer when processing an\ninput audio segment ğ‘¥. Where ğ¿is the number of WavLM encoder\ntransformer layers, ğ‘‡ is the number of frames, ğ¹ is the dimension\nof features (same as WavLM feature size). This enables the model\nto leverage speech information encompassing low-level acoustic\nfeatures as well as higher-level semantic abstractions extracted by\nthe deeper layers.\nWavLM Fine-Tuning. Since WavLM is trained only on bonafide\naudio during pre-training, it may not have exposure to spoofed\nsamples, which potentially leads to incorrect classifications. There-\nfore, we first fine-tune the entire WavLM feature extractor in an\nFigure 2: The baseline structure for fine-tuning.\nICMR â€™24, June 10â€“14, 2024, Phuket, Thailand Zuheng Kang et al.\nFigure 3: The overview of the RAG and RAD pipeline. Triangular edge rectangles represent vectors for retrieval databases.\nIn RAG, long rectangles represent document chunks. In RAD, long rectangles with/without an outline represent long/short\nfeatures, rounded edge rectangles represent audio segments.\nend-to-end manner without the RAD framework. That is, shown\nin Figure 2, the speech is encoded into short features by a train-\nable WavLM model Eand time-wise speedup method S, which is\nthen encoded into intermediate representations by an MFA mod-\nule. These representations are then classified as either bonafide\nor spoofed by a fully connected layer. By jointly optimizing the\nparameters, we obtain a fine-tuned WavLM model that can serve\nas an improved feature extractor in the subsequent RAD frame-\nwork. In the subsequent RAD inference phase, we only leverage\nthe fine-tuned WavLM and discard the back-end model.\n3.2 Retrieval Augmented Detection\nTo address the performance limitations imposed by the detection\nbottleneck, we propose the retrieval augmented detection (RAD)\nframework. Similar to the RAG framework, the proposed RAD\napproach consists of three main stages, but with some procedural\nmodifications compared to RAG.\n(1) Build Knowledge Retrieval Database. As shown in the stage 1\n(blue section) of Figure 3-RAD, the bonafide audio dataset ğ‘¥â€²is\nsegmented into smaller audio segments {ğ‘¥ğ‘›}at the ğ‘›th sample.\nThese audio segments can be encoded to latent long feature repre-\nsentations\nn\nğ‘¦â€²\nğ‘›,ğ‘™\no\nâˆˆRğ‘Ã—ğ¿Ã—ğ‘‡â€²Ã—ğ¹ by WavLM feature extractor E(Â·),\nwhere ğ‘‡â€²is the time dimension of long features,ğ‘ is the number of\naudio segments, and ğ‘™ indexes the encoder layer of WavLM. Subse-\nquently, two operations are performed on the long features\nn\nğ‘¦â€²\nğ‘›,ğ‘™\no\n:\n(a) The features are embedded into dense vector representations\b\nğ‘£ğ‘›,ğ‘™\n\t\nâˆˆRğ‘Ã—ğ¿Ã—ğ¹ via the mapping M(Â·)in Equation 1, where each\nğ‘£ğ‘›,ğ‘™ summarizes the time-wise features ğ‘¦â€²\nğ‘›,ğ‘™ by temporal averag-\ning to eliminate the time dimension ğ‘‡â€²; (b) The time dimension is\nshortened to form short feature\n\b\nğ‘¦ğ‘›,ğ‘™\n\t\nâˆˆRğ‘Ã—ğ¿Ã—ğ‘‡Ã—ğ¹ for improved\nefficiency by the functionS(Â·)(details in Â§3.4), whereğ‘‡ is the time\ndimension of short feature.\nğ‘£ğ‘›,ğ‘™ = M\n\u0010\nğ‘¦â€²\nğ‘›,ğ‘™\n\u0011\n= 1\nğ‘‡\nğ‘‡âˆ‘ï¸\nğ‘¡=1\nğ‘¦â€²\nğ‘›,ğ‘™\n\f\f\fğ‘¡\n. (1)\nImportantly, each embedding ğ‘£ğ‘›,ğ‘™ maintains an index linking to\nits original short feature ğ‘¦ğ‘›,ğ‘™, enabling the retrieval of the original\naudio segment ğ‘¥ğ‘› for the source content. Finally, the collection of\nembeddings\n\b\nğ‘£ğ‘›,ğ‘™\n\t\nare stored in ğ‘™ vector databases Vğ‘™ to enable\nefficient similarity search and retrieval.\n(2) Retrieve Knowledge. As shown in the stage 2 (red section) of\nFigure 3-RAD, a sample to be detected Ëœğ‘¥ğ‘ can be embedded into a\nquery embedding Ëœğ‘£ğ‘,ğ‘™ âˆˆRğ¿Ã—ğ¹ by function E, M, and can be con-\nverted to short features Ëœğ‘¦ğ‘,ğ‘™ âˆˆRğ¿Ã—ğ‘‡Ã—ğ¹ by function E, S. This query\nembedding Ëœğ‘£ğ‘,ğ‘™ is then utilized to perform a similarity search across\nvector databases â€“ for each layer ğ‘™, there is a corresponding vector\ndatabase Vğ‘™ to be searched. The top ğ¾ most similar embeddings\bËœğ‘£ğ‘˜,ğ‘™\n\t\nâˆˆRğ¾Ã—ğ¿Ã—ğ¹ are retrieved, along with their associated short\nfeatures\n\bËœğ‘¦ğ‘˜,ğ‘™\n\t\nâˆˆRğ¾Ã—ğ¿Ã—ğ‘‡Ã—ğ¹. These ğ¾ most relevant audio sam-\nples serve as references for detailed comparison with the sample\nto be detected. By analyzing the similarities and differences, the\nauthenticity of the tested samples can be better determined.\n(3) Get Results (Sample Detection). As shown in stage 3 (green sec-\ntion) of Figure 3-RAD, our proposed RAD framework requires the\ntraining of an additional detection model. This model accepts the\nsamples to be tested as well as the most relevant retrieved samples.\nSpecifically, the detection model is provided with the query short\nfeatures Ëœğ‘¦ğ‘,ğ‘™ and the top ğ¾similar short features\n\bËœğ‘¦ğ‘˜,ğ‘™\n\t\nto make the\nRetrieval-Augmented Audio Deepfake Detection ICMR â€™24, June 10â€“14, 2024, Phuket, Thailand\nComparison RAG RAD Full training / fine tuning for \ndetection\nKnowledge Updates\nUpdates directly  to the latest \nretrieved knowledge base, \nwithout the need for frequent re-\ntraining to obtain the latest \nknowledge.\nKnowledge is input implicitly. \nThe ability of previously \ntrained models for knowledge \nacquisition is \nyet to be verified .\nFull training or retraining is \nneeded\n for knowledge and data \nupdates.\nExternal Knowledge External knowledge can be \nacquired \nthrough retraining .\nData Processing Only large datasets of high quality \ncan improve performance.\nInterpretability\nThe results generated can be \ntraced back to specific data \nsources, offering \ngreater \ninterpretability  and traceability.\nOnly similar samples can be \ntraced, but it is \ndifficult to \ninterpret directly  how it relates \nto the sample being examined.\nWorks as a black box with very \nlow interpretability .\nComputational \nResources\nRelying only on one model's \njudgment.\nExternal Training Zero-shot learning , without \nexternal training.\nLatency Requirements Only the model inference process \nwill have\n low latency.\nHallucination/Detection \nError\nLess likely to hallucinate because \neach generated result is based \non retrieved evidence.\nCareful comparison and review \nof retrieved similar samples \nwith the tested samples \nmay \nreduce detection errors, but \nthis needs to be verified .\nRelying on only one model may \nproduce \nhigher detection errors .\nEthical and Privacy \nIssues\nContent in external databases \nmay \nhave ethical and privacy \nconcerns.\nInitially requires full training  for specific tasks with  high quality \ndata.\nUtilizing external resources, especially relevant or similar data \nfrom databases .\nMinimal data processing and manipulation is required.\nDatabase retrieval  techniques are required for each generated or \ndetection task, and  external data sources require regular \nmaintenance. These require additional computational resources.\nData retrieval process leads to higher latency.\nThe labels are fixed, credible, and free of ethical and privacy \nconcerns.\nFigure 4: Properties of RAG, RAD, and full training/fine-\ntuning for detection. Red text represents the focused atten-\ntion, and green cells represent ideas that should be verified\nin this paper.\nfinal decisionğ‘§. Importantly, this detection model not only evaluates\nrelevant samples, but also provides detailed comparisons with the\nmost similar real samples. This additional contextual information\nhelps to make more accurate judgments for DF detections.\nProperties similar to RAG in RAD. Despite their different applica-\ntions, with RAD optimized for detection and RAG for generation,\ngiven the similarities in structure and algorithms between RAG\nand RAD, it is likely that RAD also has the same advantages as\nRAG. Figure 4 provides a detailed summary of the key advantages\nand disadvantages of RAG, RAD, and full training / fine-tuning\napproaches. Although similarities and differences exist across these\nmethods, three critical questions emerge as follows:\nâ€¢Question 1: Does the RAD framework reduce detection errors?\nâ€¢Question 2: Does updating external knowledge for the RAD\nframework further improve detection performance?\nâ€¢Question 3: Can the retrieved audio samples be interpreted?\nResearch questions 1, 2 are verified in Â§4.4, and question 3 is\nverified in Â§4.5.\n3.3 Detection Model\nTo apply RAD to DF detection, we extend the Multi-Fusion Attentive\n(MFA) classifier [9], named RAD-MFA, which combines the raw\nquery input for detection and the retrieved similar bonafide samples\nto make comprehensive analysis for detections. Specifically, Figure\n5 illustrates the overall structure of our proposed detection model,\nand Figure 5 shows the MFA sub-modules in detail.\nMFA Module. The MFA Module in our framework handles the test\nfeature Ëœğ‘¦ğ‘ and the retrieved features\n\bËœğ‘¦ğ‘˜,ğ‘™\n\t\n. For conciseness, these\nfeatures are denoted by ğ‘¦in Figure 5. Specifically, the MFA module\nis implemented through the following steps:\nFigure 5: The structure of detection model architecture. âŠ•\ndenotes the concatenation. This process illustrates the 3rd get\nresults stage of Figure 3-RAD in detail.\n(1) The input feature ğ‘¦ âˆˆRğµÃ—ğ¿Ã—ğ‘‡Ã—ğ¹ is passed through ğ¿parallel\ntime-wise attentive statistic pooling (ASP) layers (denoted as\nASPğ‘‡(Â·)) to eliminate the time dimension. Here, ğµ denotes a\nvirtual dimension.\n(2) The last outputs are concatenated and passed through a fully\nconnected layer to transform the features to RğµÃ—ğ¿Ã—2ğ¹.\n(3) These outputs are then passed through a layer-wise ASP layer\n(denoted as ASPğ¿(Â·)) to form the intermediate representation\nğ‘Ÿ âˆˆRğµÃ—4ğ¹.\nExtended RAD-based MFA. The RAD-MFA is implemented through\nthe following steps:\n(1) The test feature Ëœğ‘¦ğ‘ âˆˆ R1Ã—ğ¿Ã—ğ‘‡Ã—ğ¹ and the retrieved features\bËœğ‘¦ğ‘˜,ğ‘™\n\t\nâˆˆRğ¾Ã—ğ¿Ã—ğ‘‡Ã—ğ¹ are sent to the same MFA module, creat-\ning intermediate representations ğ‘Ÿğ‘ âˆˆR1Ã—4ğ¹ and ğ‘Ÿğ‘˜ âˆˆR1Ã—4ğ¹\nrespectively.\n(2) These two representations are used to form ğ‘Ÿğ‘‘ âˆˆR1Ã—4ğ¹ by\ntaking their difference, ğ‘Ÿğ‘‘ = ğ‘Ÿğ‘˜ âˆ’ğ‘Ÿğ‘. We make a difference\nbetween two features with extremely similar timbre, which\nallows the discriminative model to pay more attention to other\ndifferential information, such as background noise.\n(3) This output is sent to a sample-wise ASP layer (denoted as\nASPğ¾(Â·)) to form the intermediate representation ğ‘Ÿğ‘’ âˆˆR1Ã—8ğ¹.\n(4) ğ‘Ÿğ‘’ is concatenated with ğ‘Ÿğ‘, and sent to a fully connected layer\nto make the final decision.\nThrough this scheme, the RAD-based detection model can take\ninto account numerous particularly similar bonafide samples and\nmake comprehensive judgments on their contents and distributions.\nThis enables the model to achieve more accurate detection results\nby accounting for many additional highly similar authentic cases.\n3.4 Performance Optimization\nIn order to speed up the process of training and testing, two ap-\nproaches are used for optimization.\nLocally Stored Features. To speed up the training and testing pro-\ncess, we pre-compute and cache the WavLM features of all audio\nICMR â€™24, June 10â€“14, 2024, Phuket, Thailand Zuheng Kang et al.\nsegments in the knowledge retrieval database. Specifically, each raw\naudio segment ğ‘¥ğ‘› is passed through the WavLM model to extract\nthe corresponding feature representation ğ‘¦ğ‘›,ğ‘™. The retrieved fea-\nture\n\b\nğ‘¦ğ‘›,ğ‘™\n\t\nare stored locally, and the indexes in the database only\nstore pointers to these pre-extracted feature paths. For training,\nthe entire pipeline operates on the pre-computed features rather\nthan raw audio, with the cached test features, retrieved database\nfeatures, and corresponding labels, making the training process\nextremely fast. For testing, this design allows efficient retrieval of\naudio contents without repetitively invoking the computationally\nexpensive WavLM feature extraction each time.\nTime-wise Speedup. The acoustic features extracted by WavLM\nhave a frame-to-frame hop size of 20 ms and a large number of\nlayers, resulting in an extremely large number of generated feature\nparameters that require a large amount of storage space. To solve\nthe problem, we propose a method S(Â·)to simplify features in the\ntime dimension.\nLet ğ‘¥ denote an audio segment sample, the feature extracted by\nWavLM is called latent long featuresğ‘¦â€²âˆˆRğ‘‡â€²Ã—ğ¹, which can then\nbe transformed into short features ğ‘¦ âˆˆRğ‘‡Ã—ğ¹ by the S. A speedup\nparameter ğœ is introduced, which partitions the long feature ğ‘¦â€²\nalong the time dimension as:\npartition \u0000ğ‘¦â€²\u0001 =\n\u0002\u0002\nğ‘¦â€²\n1,...,ğ‘¦ â€²\nğœ\n\u0003\n,\n\u0002\nğ‘¦â€²\nğœ+1,...,ğ‘¦ â€²\n2ğœ\n\u0003\n,...,\n\u0002\n...,ğ‘¦â€²\nğ‘‡â€²\n\u0003\u0003\n|                                               {z                                               }\nğ‘‡â€²/ğœ partitions\n. (2)\nThe speedup short feature ğ‘¦ğ‘¡ can then be derived by taking the\naverage along the partitioned time dimension as:\nğ‘¦ = S\u0000ğ‘¦â€²\u0001 =\n\u0002\nmean \u0000\u0002\n...,ğ‘¦â€²\nğœ\n\u0003\u0001 ,..., mean \u0000\u0002\n...,ğ‘¦â€²\nğ‘‡â€²\n\u0003\u0001\u0003\n. (3)\nApplying this speedup technique enables significant savings in\nstorage space. However, there is an additional question that needs\nto be experimentally verified:\nâ€¢Question 4: Does time-wise speedup method affect down-\nstream DF detection performance?\nThis question is validated in Â§4.4.\n4 EXPERIMENTS\nThe following section describes the datasets and assessment metrics\n(described in Â§4.1) used for all of the reported experimental work,\nas well as details of the reproducible implementation (described\nin Â§4.2). The experimental results (described in Â§4.3) will list the\nevaluation results compared to the existing SOTA. The ablation\nstudies (described in Â§4.4) will then focus on several experiments\nrelated to the Four Research Questions collected from Â§3.2, 3.4,\nand why our proposed RAD framework could be effective.\n4.1 Datasets and Metrics\nASVspoof 2019 LA Database. The ASVspoof 2019 [23] logical ac-\ncess (LA) dataset is comprised of bonafide and spoofed utterances\ngenerated using totally 19 different spoofing algorithms, including\nTTS, VC, and replay attacks. The dataset contains separate parti-\ntions for training, development, and evaluation. The training and\ndevelopment sets contain samples of 6 spoofing algorithms, while\nthe evaluation set contains samples from 2 algorithms seen during\ntraining as well as 11 unseen spoofing algorithms not present in the\ntraining data. The training set trains the model, the development\nset selects the best-performing model, and the evaluation set impar-\ntially evaluates the performance of the selected model. In addition,\nall bonafide samples will be used to build the retrieval database.\nThis experimental design aims to evaluate the generalization\nability of the DF detection system against unknown spoofing at-\ntacks. Furthermore, the dataset may not contain complete bonafide\nrecordings of all speakers that were impersonated in the spoof-\ning dataset. The lack of target speaker data may limit the ability\nof the DF detection system to perform accurate speaker charac-\nteristics comparisons. To mitigate this issue, additional bonafide\nsamples from impersonated speakers should be found to augment\nthe available knowledge database for the retrieval system.\nASVspoof 2021 LA Database. The LA and DF evaluation subsets\nfrom the ASVspoof 2021 [27] challenge present intentionally more\ndifficult spoofing detection tasks compared to the 2019 LA data,\nincluding more unseen attacks, and both encoding and transmis-\nsion distortions in the LA set, as well as the unseen coding and\ncompression artifacts in the DF set. According to the challenge\nguidelines, since no new training or development data was released\nfor the ASVspoof 2021 challenge, model training, and evaluation\nwere limited only to the ASVspoof 2019 database. The entire subset\nof ASVspoof 2021 LA and DF were used for model testing.\nVCTK Database. All bonafide samples of ASVSpoof are a very small\nsubset of the VCTK dataset. To mitigate the issue of insufficient\nbonafide samples per speaker, as the datasets used originate from\nthe VCTK dataset, we expand our database by using additional sam-\nples that exclude all samples already present in our experimental\ndata. This allows us to increase the number of bonafide samples\navailable for each speaker in the retrieval task.\nMetrics. We evaluated our proposed model on these datasets using\ntwo standard metrics: the minimum normalized tandem detection\ncost function (min t-DCF) [14] and pooled equal error rate (EER).\nThe min t-DCF measures the combined (tandem) performance of\nthe ASV systems, and the EER reflects the independent DF detection\ncapability.\n4.2 Implementation Details\nData Processing. The original audio recordings in the database are\nsegmented into clips of 4 seconds in length. Audio recordings over\nthe 4-second duration are truncated to 4 seconds. For audio record-\nings shorter than 4 seconds, the clips are padded to the 4-second\nlength by repeating the recording. No additional processing such\nas voice activity detection (VAD) is applied to the audio samples\nprior to segmentation. The audio segments are first encoded into\nshort features using a WavLM model, and are then shortened into\nmore compact short features through a time-wise speedup model\nwith parameter ğœ = 10.\nVector Database. In order to store and query embeddings from\nvector databases more conveniently, we created ğ¿ databases to\nstore the audio feature vectors extracted at each layer of the WavLM\nmodel. When performing a database retrieval, the query audio is\nconverted to query embedding, and the top 10 most similar WavLM\nshort features will be retrieved.\nRetrieval-Augmented Audio Deepfake Detection ICMR â€™24, June 10â€“14, 2024, Phuket, Thailand\nTable 1: Comparison with other anti-spoofing systems in the\nASVspoof 2019 LA evaluation set, reported in terms of pooled\nmin t-DCF and EER (%).\nSystem Configuration min t-DCF EER(%)\nHua et al. [11] DNN+ResNet 0.0481 1.64\nZhang et al. [31] FFT+SENet 0.0368 1.14\nDing et al. [6] SAMO 0.0356 1.08\nTak et al. [22] RawGAT-ST 0.0335 1.06\nJung et al. [26] AASIST 0.0275 0.83\nHuang et al. [12] DFSincNet 0.0176 0.52\nFan et al. [7] f0+Res2Net 0.0159 0.47\nGuo et al. [9] WavLM+MFA 0.0126 0.42\nOurs WavLM+RAD-MFA 0.0115 0.40\nModel Training. The front-end feature extractor utilized in this\nwork is WavLM. During fine-tuning of the front-end WavLM, the\nAdam optimizer is employed with a learning rate of 3e-6 and a batch\nsize of 4. For training the MFA, the batch size is changed to 32 and\nthe learning rate is 3e-5. All experiments were performed utilizing\ntwo NVIDIA GeForce RTX 3090 GPUs. Each model configuration\nis trained for approximately 30 epochs.\n4.3 Experimental Results\nTo demonstrate the superior performance of our proposed method\nover existing approaches, we compare our proposed method to\nrecent SOTA methods.\nResults on ASVspoof 2019 LA evaluation set. The experimental re-\nsults in Table 1 compare the performance of our proposed methods\nto existing approaches on the ASVspoof 2019 LA evaluation dataset.\nOur method achieves an EER of 0.40% and a min t-DCF of 0.0115\nwhich is the best reporting result, demonstrating the effectiveness\nand superiority of our proposed method. Notably, although Guo\net al. [9] utilizes a similar WavLM feature extractor and MFA net-\nwork, our proposed RAD framework improves its performance,\novercoming the limitations of single-model approaches.\nIn our analysis, the RAD framework first retrieves the most sim-\nilar audio samples, which are likely from the same speaker, and\nthen performs careful comparisons between these samples and the\ntest sample. For the detection model, it only needs to consider the\ndifferences between the two, rather than relying on fuzzy prior\nknowledge for detection. In contrast, our proposed method is more\nrobust. Specifically, by focusing on fine-grained differences rather\nthan generalized knowledge, our method can more accurately dis-\ntinguish more detailed information.\nResults on ASVspoof 2021 DF evaluation set. We further test our\nmodel on the ASVspoof 2021 LA and DF evaluation set, results are\nshown in Table 2. In the DF subset, our method achieves SOTA\nperformance on the DF subset with an EER of 2.38%. In the LA\nsubset, we obtain an EER of 4.89%, which is also quite a competitive\nperformance, but still better than the baseline system [9] without\nRAD. Further analysis and ablation studies are needed to a fully\nTable 2: Comparative results of our proposed method with\nother systems in the ASVspoof 2021 LA and DF evaluation\nset with pooled EER (%).\nSystem Configuration LA DF\nFan et al. [7] f0+Res2Net 3.61 â€“\nDoÃ±as et al. [18] wav2vec2+ASP 3.54 4.98\nWang et al. [25] wav2vec2+LGF 6.53 4.75\nTak et al. [21] wav2vec2+AASIST 0.82 2.85\nFan et al. [7] WavLM+MFA 5.08 2.56\nOurs WavLM+RAD-MFA 4.83 2.38\nTable 3: Ablation studies on ASVspoof 2021 DF dataset for\nthe effectiveness of each component with pooled EER (%). -L\nand -S: large and small. ft: fine-tuning. Just Difference is the\nğ‘Ÿğ‘’ (denoted in Figure 5, without ğ‘Ÿğ‘) directly connected to the\nfully connected layer for classification.\nAblation Configuration Pooled EER(%)\nFull Framework â€“ 2.38\nw/o RAD Baseline (Figure 2) 2.90\nw/o VCTK ASVspoof 2019 only 2.54\nw/o WavLM-L WavLM-S 9.15\nw/o ft WavLM-S 9.62\nWavLM-L 4.98\nVariation Structure Just Difference 2.49\ncharacterize the advantages of our proposed method on each com-\nponent.\n4.4 Ablation Study\nAblation Study on Different Components. The ablation study pre-\nsented in Table 3 summarizes the results obtained by evaluating\ndifferent configurations and components of the proposed system\non the ASVspoof 2021 DF subset. Specifically, the impact of the\nRAD framework, WavLM-L feature extractor, fine-tuning of the\nfeature extractor, incorporation of additional VCTK datasets for\ndata retrieval, and the structure of the detection network were ana-\nlyzed. The experiments were conducted using a time-wise speedup\nparameter ğœ = 10. System performance was assessed using the\npooled EER expressed as a percentage. The key observations are\nsummarized as follows (line-by-line explanation from Table 3):\n(1) The full system reaches the SOTA performance with a pooled\nEER of 2.38%.\n(2) Removing the proposed RAD framework for similar sample\nretrieval increases the pooled EER to 2.90%. This validates the\neffectiveness of the RAD framework, which answers the re-\nsearch Question 1. However, this result is slightly higher than\nthat of Guo et al. [9], which may be due to different parameter\nsettings and time-wise speedup operations.\nICMR â€™24, June 10â€“14, 2024, Phuket, Thailand Zuheng Kang et al.\nTable 4: Ablation study of the effect of different time-\nwise speedup parameter ğœ on DF detection performance of\nASVspoof 2021 DF dataset, using pooled EER (%).\nSpeedup (ğœ =) 5 10 20\nOriginal 4.68 4.98 5.45\nFine-tune 2.36 2.38 2.54\n(3) Excluding the supplementary VCTK dataset slightly increases\nthe pooled EER to 2.64%, indicating that updating the knowl-\nedge with additional related data could improve the detection\nperformance, which answers the research Question 2.\n(4) Replacing WavLM-L with WavLM-S significantly increases the\npooled EER to 9.15%, highlighting the importance of the feature\nextractor in the overall framework.\n(5) Without fine-tuning, the EER rises drastically to 9.62% and\n4.98% for WavLM-S and WavLM-L respectively. This observa-\ntion clearly highlights the positive influence of fine-tuning in\nenhancing DF detection performance, since fine-tuning com-\nbines spoofed data instead of using bonafide data alone, thereby\nimproving the discriminatory capability of DF samples.\n(6) We also tried the variation structure of removing the ğ‘Ÿğ‘ branch\nand directly connecting ğ‘Ÿğ‘’ (denoted in Figure 5) to the classifier\nslightly increases the pooled EER to 2.49%, suggesting that not\nonly the difference of the feature, but also the original feature\nplay the role for performance improvement.\nEffect of Time-wise Speedup Parameter. Table 4 examines whether\ntime-wise speedup affects the performance of DF detection. We\ntested the original and the fine-tuned WavLM-Large feature ex-\ntractor on the ASVspoof 2021 DF with ğœ is 5, 10, 20, reporting by\nthe pooled EER. It is difficult to test under ğœ < 5 due to very high\ncomputational costs and storage consumption, which need to be ad-\ndressed in future work. Before fine-tuning, the performance varies\ngreatly across ğœ: the smaller ğœ is, the better the performance, but\nthe much higher computational cost and storage consumption. Af-\nter fine-tuning, the gap narrows, suggesting that optimization can\nreduce the impact of time-wise speedup operation. Overall, the\ntime-wise speedup operation will affect the performance, but not\ntoo much, which answers the research Question 4. Taking these\nfactors into account, we finally chose ğœ = 10. However, we still\nneed better ways to reduce storage and computation, which is an\nopen problem that needs to be investigated.\n4.5 Sample Analysis\nThe retrieval samples shown in Figure 6 with clickable audio to\nhear, offer insights into the factors influencing successful detection\nof spoofing artifacts. This figure presents 4 test samples, comprising\n3 spoofed and 1 bonafide audio, along with the 2 to 3 most similar\nsamples. Extracts were taken from three layers (initial, middle, final)\nof a WavLM-L model. Despite quality defects in the spoofed test\nsamples, the system appeared to retrieve samples from the same\nspeaker identities. This suggests the system might rely strongly\non speaker-discriminative features, potentially providing an ap-\nproximate answer to the research Question 3. However, although\nFigure 6: Examples of retrieved samples ( click on the figure\nto hear the sound ).\ninitial layer retrievals corresponded to the same speakers, middle\nand final layer results may differ in speaker identity. Our analysis\nimplies that the shallower layers of the model may focus on timbral\nand quality-based features, whereas the deeper layers capture more\nabstract semantic information. Nevertheless, these explanations re-\nmain brief and qualitative, lacking rigorous argumentation, which\ncould be an interesting area for future exploration. In summary,\nthe retrieved samples have the greatest similarity at the feature\nlevel, providing insights for the successful detection of the spoof-\ning artifacts. After retrieval, a careful comparison of the retrieval\nresults with the test sample will be a key factor in the performance\nimprovement.\n5 CONCLUSIONS\nIn this work, we proposed a novel retrieval-augmented detection\n(RAD) framework that leverages retrieved samples to enhance deep-\nfake detection performance. We also extend the multi-fusion atten-\ntive classifier by integrating it with our proposed RAD framework.\nExtensive experiments demonstrate state-of-the-art results of the\nRAD framework on the ASVspoof 2021 DF dataset and competitive\nperformance on the 2019 and 2021 LA datasets. The consistent im-\nprovements achieved on multiple datasets highlight the potential\nof RAD as a new paradigm for DF detection. The ablation study re-\nveals RAD and retrieval-augmented generation (RAG) share similar\nproperties in improving detection performance. Additionally, the\nretrieved samples are usually from the same speaker, suggesting\npotential interpretability. In conclusion, this work opens promising\nresearch avenues into retrieval-based augmentation techniques to\nenhance performance for detection tasks. By breaking the reliance\non a single model, RAD provides a new perspective that utilizes\nmore available information to overcome performance limitations\nand advance DF detection techniques.\nACKNOWLEDGMENTS\nSupported by the Key Research and Development Program of Guang-\ndong Province (grant No. 2021B0101400003) and the Corresponding\nauthor is Jianzong Wang (jzwang@188.com).\nRetrieval-Augmented Audio Deepfake Detection ICMR â€™24, June 10â€“14, 2024, Phuket, Thailand\nREFERENCES\n[1] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020.\nwav2vec 2.0: A framework for self-supervised learning of speech representations.\nAdvances In Neural Information Processing Systems 33 (2020), 12449â€“12460.\n[2] Zexin Cai and Ming Li. 2024. Integrating frame-level boundary detection and\ndeepfake detection for locating manipulated regions in partially spoofed audio\nforgery attacks. Computer Speech & Language 85 (2024), 101597.\n[3] Jinggang Chen, Junjie Li, Xiaoyang Qu, Jianzong Wang, Jiguang Wan, and Jing\nXiao. 2024. GAIA: Delving into Gradient-based Attribution Abnormality for\nOut-of-distribution Detection. Advances in Neural Information Processing Systems\n(NIPS) 36 (2024).\n[4] Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen,\nJinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou,\nShuo Ren, Yanmin Qian, Yao Qian, Micheal Zeng, and Furu Wei. 2021. WavLM:\nLarge-Scale Self-Supervised Pre-Training for Full Stack Speech Processing. IEEE\nJournal of Selected Topics in Signal Processing 16 (2021), 1505â€“1518.\n[5] Steven Davis and Paul Mermelstein. 1980. Comparison of parametric representa-\ntions for monosyllabic word recognition in continuously spoken sentences. IEEE\nTransactions on Acoustics, Speech, And Signal Processing 28, 4 (1980), 357â€“366.\n[6] Sivan Ding, You Zhang, and Zhiyao Duan. 2022. SAMO: Speaker Attractor Multi-\nCenter One-Class Learning For Voice Anti-Spoofing. International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) (2022), 1â€“5.\n[7] Cunhang Fan, Jun Xue, Jianhua Tao, Jiangyan Yi, Chenglong Wang, Chengshi\nZheng, and Zhao Lv. 2024. Spatial reconstructed local attention Res2Net with F0\nsubband for fake speech detection. Neural Networks (2024), 106320.\n[8] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai,\nJiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. 2023. Retrieval-\nAugmented Generation for Large Language Models: A Survey.\n[9] Yinlin Guo, Haofan Huang, Xi Chen, He Zhao, and Yuehai Wang. 2024. Audio\nDeepfake Detection With Self-Supervised Wavlm And Multi-Fusion Attentive\nClassifier. In International Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 12702â€“12706.\n[10] Ameer Hamza, Abdul Rehman Rehman Javed, Farkhund Iqbal, Natalia Kryvinska,\nAhmad S Almadhor, Zunera Jalil, and Rouba Borghol. 2022. Deepfake audio\ndetection via MFCC features using machine learning. IEEE Access 10 (2022),\n134018â€“134028.\n[11] Guang Hua, A. Teoh, and Haijian Zhang. 2021. Towards End-to-End Synthetic\nSpeech Detection. IEEE Signal Processing Letters 28 (2021), 1265â€“1269.\n[12] Bingyuan Huang, Sanshuai Cui, Jiwu Huang, and Xiangui Kang. 2023. Discrimi-\nnative Frequency Information Learning for End-to-End Speech Anti-Spoofing.\nIEEE Signal Processing Letters 30 (2023), 185â€“189.\n[13] Piotr Kawa, Marcin Plata, Michal Czuba, Piotr Szymaâ€™nski, and Piotr Syga. 2023.\nImproved DeepFake Detection Using Whisper Features. International Speech\nCommunication Association (Interspeech) abs/2306.01428 (2023).\n[14] Tomi H. Kinnunen, Kong-Aik Lee, HÃ©ctor Delgado, Nicholas W. D. Evans, Massi-\nmiliano Todisco, Md. Sahidullah, Junichi Yamagishi, and Douglas A. Reynolds.\n2018. t-DCF: a Detection Cost Function for the Tandem Assessment of Spoofing\nCountermeasures and Automatic Speaker Verification. (2018).\n[15] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nNaman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel,\net al. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks.\nAdvances in Neural Information Processing Systems 33 (2020), 9459â€“9474.\n[16] Xu Li, Xixin Wu, Hui Lu, Xunying Liu, and Helen Meng. 2021. Channel-wise\ngated res2net: Towards robust detection of synthetic speech attacks.International\nSpeech Communication Association (Interspeech) (2021).\n[17] Anwei Luo, Enlei Li, Yongliang Liu, Xiangui Kang, and Z Jane Wang. 2021. A\ncapsule network based approach for detection of audio spoofing attacks. In\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 6359â€“6363.\n[18] Juan M. Martâ€™in-Donas and Aitor Ãlvarez. 2022. The Vicomtech Audio Deepfake\nDetection System Based on Wav2vec2 for the 2022 ADD Challenge. International\nConference on Acoustics, Speech and Signal Processing (ICASSP) (2022), 9241â€“9245.\n[19] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and\nIlya Sutskever. 2023. Robust speech recognition via large-scale weak supervision.\n(2023), 28492â€“28518.\n[20] Chengzhe Sun, Shan Jia, Shuwei Hou, and Siwei Lyu. 2023. AI-Synthesized\nVoice Detection Using Neural Vocoder Artifacts. 2023 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition Workshops (CVPRW) (2023), 904â€“912.\n[21] Hemlata Tak, Massimiliano Todisco, Xin Wang, Jee weon Jung, Junichi Yamagishi,\nand Nicholas W. D. Evans. 2022. Automatic speaker verification spoofing and\ndeepfake detection using wav2vec 2.0 and data augmentation. Speaker Odyssey\nWorkshop (2022).\n[22] Hemlata Tak, Jee weon Jung, Jose Patino, Madhu R. Kamble, Massimiliano Todisco,\nand Nicholas W. D. Evans. 2021. End-to-End Spectro-Temporal Graph Atten-\ntion Networks for Speaker Verification Anti-Spoofing and Speech Deepfake\nDetection. ASVspoof 2021 Workshop-Automatic Speaker Verification and Spoofing\nCoutermeasures Challenge (2021).\n[23] Massimiliano Todisco, Xin Wang, Ville Vestman, Md. Sahidullah, HÃ©ctor Delgado,\nAndreas Nautsch, Junichi Yamagishi, Nicholas W. D. Evans, Tomi H. Kinnunen,\nand Kong-Aik Lee. 2019. ASVspoof 2019: Future Horizons in Spoofed and Fake\nAudio Detection. InInternational Speech Communication Association (Interspeech) .\n[24] Xin Wang and Junich Yamagishi. 2021. A comparative study on recent neural\nspoofing countermeasures for synthetic speech detection. International Speech\nCommunication Association (Interspeech) (2021).\n[25] Xin Wang and Junichi Yamagishi. 2022. Investigating self-supervised front ends\nfor speech spoofing countermeasures. The Speaker and Language Recognition\nWorkshop abs/2111.07725 (2022).\n[26] Jee weon Jung, Hee-Soo Heo, Hemlata Tak, Hye jin Shim, Joon Son Chung, Bong-\nJin Lee, Ha jin Yu, and Nicholas W. D. Evans. 2021. AASIST: Audio Anti-Spoofing\nUsing Integrated Spectro-Temporal Graph Attention Networks. International\nConference on Acoustics, Speech and Signal Processing (ICASSP) (2021), 6367â€“6371.\n[27] Junichi Yamagishi, Xin Wang, Massimiliano Todisco, Md Sahidullah, Jose Patino,\nAndreas Nautsch, Xuechen Liu, Kong Aik Lee, Tomi Kinnunen, Nicholas Evans,\net al. 2021. ASVspoof 2021: accelerating progress in spoofed and deepfake speech\ndetection. In ASVspoof 2021 Workshop-Automatic Speaker Verification and Spoofing\nCoutermeasures Challenge .\n[28] Jiangyan Yi, Ruibo Fu, Jianhua Tao, Shuai Nie, Haoxin Ma, Chenglong Wang,\nTao Wang, Zhengkun Tian, Ye Bai, Cunhang Fan, et al. 2022. ADD 2022: the first\naudio deep synthesis detection challenge. InInternational Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) . IEEE, 9216â€“9220.\n[29] Jiangyan Yi, Jianhua Tao, Ruibo Fu, Xinrui Yan, Chenglong Wang, Tao Wang,\nChu Yuan Zhang, Xiaohui Zhang, Yan Zhao, Yong Ren, Leling Xu, Jun Zhou, Hao\nGu, Zhengqi Wen, Shan Liang, Zheng Lian, Shuai Nie, and Haizhou Li. 2023. ADD\n2023: the Second Audio Deepfake Detection Challenge. ArXiv abs/2305.13774\n(2023).\n[30] Jiangyan Yi, Chenglong Wang, Jianhua Tao, Xiaohui Zhang, Chu Yuan Zhang,\nand Yan Zhao. 2023. Audio Deepfake Detection: A Survey. ArXiv abs/2308.14970\n(2023).\n[31] Yuxiang Zhang, Wenchao Wang, and Pengyuan Zhang. 2021. The Effect of\nSilence and Dual-Band Fusion in Anti-Spoofing System. In International Speech\nCommunication Association (Interspeech) .\n[32] Pedram Abdzadeh Ziabary and Hadi Veisi. 2021. A countermeasure based on cqt\nspectrogram for deepfake speech detection. In 2021 7th International Conference\non Signal Processing and Intelligent Systems (ICSPIS) . IEEE, 1â€“5.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8397307395935059
    },
    {
      "name": "Classifier (UML)",
      "score": 0.5898739099502563
    },
    {
      "name": "Speech recognition",
      "score": 0.5353171825408936
    },
    {
      "name": "Detector",
      "score": 0.46996232867240906
    },
    {
      "name": "Voice activity detection",
      "score": 0.44538983702659607
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.436404824256897
    },
    {
      "name": "Transparency (behavior)",
      "score": 0.419176310300827
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3974646329879761
    },
    {
      "name": "Speech processing",
      "score": 0.2413201630115509
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}