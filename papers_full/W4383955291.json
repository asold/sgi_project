{
    "title": "Transformer Architecture-Based Transfer Learning for Politeness Prediction in Conversation",
    "url": "https://openalex.org/W4383955291",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5037044938",
            "name": "Shakir Khan",
            "affiliations": [
                "Chandigarh University",
                "Imam Mohammad ibn Saud Islamic University"
            ]
        },
        {
            "id": "https://openalex.org/A5102786804",
            "name": "Mohd Fazil",
            "affiliations": [
                "University of Limerick"
            ]
        },
        {
            "id": "https://openalex.org/A5026263106",
            "name": "Agbotiname Lucky Imoize",
            "affiliations": [
                "University of Lagos"
            ]
        },
        {
            "id": "https://openalex.org/A5108920104",
            "name": "Bayan Alabduallah",
            "affiliations": [
                "Princess Nourah bint Abdulrahman University"
            ]
        },
        {
            "id": "https://openalex.org/A5070935387",
            "name": "Bader M. Albahlal",
            "affiliations": [
                "Imam Mohammad ibn Saud Islamic University"
            ]
        },
        {
            "id": "https://openalex.org/A5043087403",
            "name": "Saad Abdullah Alajlan",
            "affiliations": [
                "Imam Mohammad ibn Saud Islamic University"
            ]
        },
        {
            "id": "https://openalex.org/A5006008961",
            "name": "Abrar Almjally",
            "affiliations": [
                "Imam Mohammad ibn Saud Islamic University"
            ]
        },
        {
            "id": "https://openalex.org/A5049694218",
            "name": "Tamanna Siddiqui",
            "affiliations": [
                "Aligarh Muslim University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4229451007",
        "https://openalex.org/W4200585446",
        "https://openalex.org/W3210354869",
        "https://openalex.org/W2981183580",
        "https://openalex.org/W3205285765",
        "https://openalex.org/W6838148842",
        "https://openalex.org/W4205415902",
        "https://openalex.org/W2157163421",
        "https://openalex.org/W3034319502",
        "https://openalex.org/W2962753250",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2963797754",
        "https://openalex.org/W2964104642",
        "https://openalex.org/W4226170304",
        "https://openalex.org/W4231533450",
        "https://openalex.org/W6751230656",
        "https://openalex.org/W2994200508",
        "https://openalex.org/W4206242530",
        "https://openalex.org/W2963955897",
        "https://openalex.org/W2970356971",
        "https://openalex.org/W2963240572",
        "https://openalex.org/W3093603380",
        "https://openalex.org/W3198674704",
        "https://openalex.org/W6810593127",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3209409148",
        "https://openalex.org/W2470673105",
        "https://openalex.org/W3198943295",
        "https://openalex.org/W3185909895",
        "https://openalex.org/W4281251145",
        "https://openalex.org/W4226002520"
    ],
    "abstract": "Politeness is an essential part of a conversation. Like verbal communication, politeness in textual conversation and social media posts is also stimulating. Therefore, the automatic detection of politeness is a significant and relevant problem. The existing literature generally employs classical machine learning-based models like naive Bayes and Support Vector-based trained models for politeness prediction. This paper exploits the state-of-the-art (SOTA) transformer architecture and transfer learning for respectability prediction. The proposed model employs the strengths of context-incorporating large language models, a feed-forward neural network, and an attention mechanism for representation learning of natural language requests. The trained representation is further classified using a softmax function into polite, impolite, and neutral classes. We evaluate the presented model employing two SOTA pre-trained large language models on two benchmark datasets. Our model outperformed the two SOTA and six baseline models, including two domain-specific transformer-based models using both the BERT and RoBERTa language models. The ablation investigation shows that the exclusion of the feed-forward layer displays the highest impact on the presented model. The analysis reveals the batch size and optimization algorithms as effective parameters affecting the model performance.",
    "full_text": null
}