{
  "title": "Wikipedia and large language models: perfect pairing or perfect storm?",
  "url": "https://openalex.org/W4375867222",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2104202426",
      "name": "Paul A. Thomas",
      "affiliations": [
        "University of Kansas"
      ]
    },
    {
      "id": "https://openalex.org/A2104202426",
      "name": "Paul A. Thomas",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4353015365",
    "https://openalex.org/W4319653860",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4360620450",
    "https://openalex.org/W1972644898",
    "https://openalex.org/W4360796975",
    "https://openalex.org/W4221163727",
    "https://openalex.org/W3197759495",
    "https://openalex.org/W2134392575",
    "https://openalex.org/W4320495408",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W1994081067",
    "https://openalex.org/W4322494770",
    "https://openalex.org/W4321106177",
    "https://openalex.org/W3084063681",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4321249382"
  ],
  "abstract": "Purpose The purpose of this paper is to explore the potential benefits and challenges of using large language models (LLMs) like ChatGPT to edit Wikipedia. Design/methodology/approach The first portion of this paper provides background about Wikipedia and LLMs, explicating briefly how each works. The paper's second section then explores both the ways that LLMs can be used to make Wikipedia a stronger site and the challenges that these technologies pose to Wikipedia editors. The paper's final section explores the implications for information professionals. Findings This paper argues that LLMs can be used to proofread Wikipedia articles, outline potential articles and generate usable Wikitext. The pitfalls include the technology's potential to generate text that is plagiarized or violates copyright, its tendency to produce “original research” and its tendency to generate incorrect or biased information. Originality/value While there has been limited discussion among Wikipedia editors about the use of LLMs when editing the site, hardly any scholarship has been given to how these models can impact Wikipedia's development and quality. This paper thus aims to fill this gap in knowledge by examining both the potential benefits and pitfalls of using LLMs on Wikipedia.",
  "full_text": "Wikipedia and Large Language Models: \nPerfect Pairing or Perfect Storm? \n \nBy Paul A. Thomas* \nORCID: 0000-0002-5596-7951 \n \n \nScheduled to be published as: Thomas, Paul A. \"Wikipedia and Large Language Models: \nPerfect Pairing or Perfect Storm?\" Library Hi Tech News, 2023. \nhttp://doi.org/10.1108/LHTN-03-2023-0056. \n \n \nAbstract \nPurpose: The purpose of this paper is to explore the potential benefits and challenges of using \nlarge language models (LLMs) like ChatGPT to edit Wikipedia.  \nApproach: The first portion of this paper provides background about Wikipedia and LLMs, \nexplicating briefly how each works. The paper's second section then explores both the ways \nthat LLMs can be used to make Wikipedia a stronger site and the challenges that these \ntechnologies pose to Wikipedia editors. The paper's final section explores the implications \nfor information professionals. \nFindings: The paper argues that LLMs can be used to proofread Wikipedia articles, outline \npotential articles, and generate usable Wikitext. The pitfalls include the technology's \npotential to generate text that is plagiarized or violates copyright, its tendency to produce \n\"original research,\" and its tendency to generate incorrect or biased information. \nOriginality: While there has been limited discussion among Wikipedia editors about the use of \nLLMs when editing the site, hardly any scholarship has been given to how these models can \nimpact Wikipedia's development and quality. This paper thus aims to fill this gap in \nknowledge by examining both the potential benefits and pitfalls of using LLMs on \nWikipedia. \n \nKeywords \nWikipedia, large language models, ChatGPT, artificial intelligence, AI  \n                                                 \n*  Paul A. Thomas is a library specialist at the University of Kansas (paulthomas@ku.edu). He holds a Doctor of \nPhilosophy (Ph.D.) degree from the School of Library and Information Management, Emporia State University.  \nIntroduction \nLate 2022 saw an explosion of interest in \"large language models\" (LLMs)—a \nspecialized term that refers to artificial intelligence (AI) models that have been trained on large \ncorpora of text and, via neural network-powered deep learning, can generate coherent text in \nresponse to user queries (Lund and Wang, 2023). As Sun and Hoelscher (2023) note, these \nmodels are revolutionary given their ability to \"process vast amounts of information and generate \nlarge amounts of coherent and informative text in real time, based on simple, even short, vague, \nor ambiguous prompts\" (p. 1). Of the many LLMs that now exist, arguably the most-discussed \nmodels include OpenAI's ChatGPT (OpenAI, 2023), Google's BERT (Devlin et al., 2019), \nMeta's LLaMA (Touvron et al., 2023), and HuggingFace et al.'s BLOOM (Gibney, 2022). \nUnderstandably, LLMs have been both heralded and criticized by academics and \ninformation professionals for their disruptive potential (Taecharungroj, 2023), but hardly any \nscholarship has been given to how these models can impact the development and quality of \nWikipedia, the free encyclopedia that anyone can edit. As tools that can produce paragraphs of \ntext in only a few seconds, LLMs could be used to rapidly expand Wikipedia; at the same time, \nhowever, there are also many pitfalls to their use. For librarians and academics interested in both \nusing and studying Wikipedia, it is thus imperative to consider the benefits and dangers of LLMs \nvis-à-vis Wikipedia. This paper aims to do just that. The first portion of this paper provides \nbackground about Wikipedia and LLMs, explicating briefly how each works. The paper's second \nsection then explores both the ways that LLMs can be used to make Wikipedia a stronger site \nand the challenges that these technologies pose to Wikipedia editors. The paper's final section \nexplores the implications for information professionals. \nBackground: Wikipedia and Large Language Models \n\"Wikipedia\" is the name of a free encyclopedia that allows anyone with internet access \nthe ability to change (or \"edit\") its contents. Officially launched in 2001, Wikipedia was initially \nlambasted by critics who argued that because anyone could edit the site, its content was almost \ncertain to be unreliable. However, these negative evaluations were, for the most part, unfounded: \nIn 2005, the journal Nature concluded that Wikipedia is just as reliable as Encyclopædia \nBritannica (Giles, 2005). Further studies by The Guardian, the Journal of Clinical Oncology, PC \nPro, the Canadian Library Association, and Library Journal have all found that Wikipedia is \nrelatively reliable (Wolchover, 2011). As of February 2023, Wikipedia is 7th most-visited \nwebsite in the world, according to SimilarWeb (2023), receiving around 315.1 million visits a \nday (https://w.wiki/6UJt\n). In terms of size, Wikipedia is a leviathan, comprising over 6.63 \nmillion articles (https://w.wiki/6UK4), which have been collaboratively written by 45.23 million \nregistered editors and millions more unregistered users (https://w.wiki/6UK7). Today, thanks in \nlarge part to its visibility and its comprehensiveness, Wikipedia is—whether people care to admit \nit or not—often the go-to destination when individuals need to locate information.  \nOn the other hand, the term \"large language model\" (often abbreviated \"LLM\") refers to a \ncomplex language model that uses neural networks to analyze and reproduce human language. \nTo do this, LLMs are first \"trained\" on corpora of millions—and, in some cases, billions—of \nwords. The models then use deep learning to identify linguistic patterns in the training data; by \nprocessing and analyzing this data, LLMs can consequently generate human-like responses to \ntext-based inquiries (Roose, 2023). Although LLMs have been around for some time now, it was \nonly near the end of the beginning of the 2020s that they truly captured the zeitgeist. Much of \nthis interest was engendered by the free release of OpenAI's impressive ChatGPT in late 2022 \n(Atlas, 2023), which in turn resulted in an increasing number of academics beginning to question \nthe wider impact of this technology (Taecharungroj, 2023). While some scholars have argued \nthat LLMs will lead to increased productivity, others have argued that the technology raises \nserious ethical issues (Dwivedi, et al., 2023). \nWikipedia and Large Language Models: Uses and Issues \nAs LLMs have grown in popularity, there has been limited but growing discussion among \nWikipedia editors as to whether LLMs should or should not be used when editing the site. As \nwith all things, the answer to this question is complicated, with the technology having the \npotential to both \"massively improve [and] disrupt Wikipedia\", as Wikipedia itself notes \n(https://w.wiki/6WXw). This section will thus examine the ways that LLMs can be used to \nimprove Wikipedia and the issues that such use entails. \n \nPotential Uses \nProofreading \nAs AI models that have been trained on billions of words, LLMs are remarkably effective \nediting tools (Atlas, 2023). In particular, LLMs like ChatGPT have proven themselves capable of \nidentifying grammatical mistakes, misspellings/typos, and syntactical errors (Atlas, 2023; Sun \nand Hoelscher, 2023). In addition, many of these models are also capable of suggesting changes \nto text that will enhance clarity or how phrases can be rewritten to ensure tonal consistency (Sun \nand Hoelscher, 2023). Because Wikipedia has, in the past, been criticized for the quality of its \nprose (Rosenzweig, 2006) and the readability of certain articles (Lucassen et al., 2012), LLMs \ncould come to serve as a sort of grammatical panacea in this regard (Liu et al., 2021) \n \nArticle Outlining \nLLMs can also be used to generate article outlines (Atlas, 2023). For a practical \ndemonstration consider the following: Upon being prompted to \"provide an outline in the style of \nWikipedia\" for an article on the \"Abgig Obelisk\" (a topic for which I created an article without \nthe assistance of an LLM on March 10, 2023), ChatGPT generated the following: \n• Introduction \n• Description \no Physical features of the Abgig Obelisk \no Materials used in the construction of the Abgig Obelisk \no Inscriptions on the Abgig Obelisk \n• History \no Origins of the Abgig Obelisk \no Early history of the Abgig Obelisk \no Rediscovery and preservation of the Abgig Obelisk \n• Significance \no Cultural significance of the Abgig Obelisk \no Religious significance of the Abgig Obelisk \no Political significance of the Abgig Obelisk \n• Controversies \no Disputes over ownership of the Abgig Obelisk \no Cultural and religious conflicts surrounding the Abgig Obelisk \no Vandalism and damage to the Abgig Obelisk \n• See also \n• References \n   \n \n \nWith a few exceptions, this outline is remarkably similar to the way in which I laid out my article \n(https://en.wikipedia.org/wiki/Abgig_obelisk). For editors who struggle with how best to \norganize an article, LLMs can thus be a major boon. \n \nWikitext Generation \nFor many Wikipedia neophytes, one of the most confusing aspects of the site can be \nWikitext, or the markup language used to code text, templates, and tables. Luckily, many LLMs \nwere trained on data that at least partially included Wikitext, meaning that these models can \noften help editors parse and generate usable markup language; in fact, Wikipedia user JPxG \n(2023) has documented the ways in which ChatGPT, after being presented with a sample of \nWikitext, can quickly generate code that, were it implemented, would create and rotate tables, \nformat text, and generate userboxes, among other things. \n \nPotential Pitfalls \nPlagiarism and Copyright Violation \nWhen adding content to Wikipedia, editors must ensure that their textual contributions do \nnot violate copyright and that they are properly sourced. The text generated by LLMs, however, \noften lacks citations, and because these models are trained to generate output on a probabilistic \nbasis, they can potentially \"create\" content that either closely paraphrases or reproduces in full \nnon-free text (Eliot, 2023). As such, editors who add LLM-generated text to Wikipedia without \nfirst checking to make sure that text is properly sourced and adequately paraphrased run the risk \nof being accused of plagiarism and/or copyright infringement. \n \nGeneration of \"Original Research\" \nAnother problem with LLMs is that, when asked to generate text on a given subject, they \nwill often produce responses that either lack reliable sources (Zhong, 2023) or which distill down \na variety of sources to reach a novel conclusion that has not been published. The issue here is \nthat Wikipedia is epistemologically citational and thus forbids the inclusion of both \"original \nresearch\" (i.e., \"facts, allegations, and ideas … for which no reliable, published sources exist\") \nand the \"synthesis of published material\" (i.e., the combination of \"material from multiple \nsources to reach or imply a conclusion not explicitly stated by any source\") \n(https://w.wiki/6WXg). Wikipedia's prohibition of original research exists to ensure that all the \ncontent on the site has already been vetted by experts. LLM-generated text, however, often \nincludes assertions that have not been vetted, meaning that this text is often intrinsically unfit for \ninclusion in Wikipedia. \n \nGeneration of Incorrect and Biased Information \nThe final issue with using LLMs to generate Wikipedia content is that these models are \noften known to output blatantly wrong or patently biased information—and often, they do so in \nlanguage that is confident and assertive, which can easily fool an incredulous user (Borji, 2023; \nJi et al., 2023; Marr, 2023). What is more, when asked to provide references on a given topic, \nsome LLMs will respond with a list of sources that might look legitimate but which are actually \nnon-existent (Gravel et al., 2023). These sorts of erroneous outputs—which are colloquially \nknown as \"hallucinations\" (Azamfirei et al., 2023; Borji, 2023; Marr, 2023)—in and of \nthemselves pose a considerable risk for a world that is increasingly marred by disinformation, \nsystemic bias, and \"fake news\", but the situation is even graver when these outputs are \nconsidered in the context of Wikipedia: After all, the site is extremely popular and is used by \nmillions of people every day. If hallucinations—in the form of either erroneous \"facts\" or \nfabricated references—are uncritically added to the site, they will almost certainly be \ndisseminated the world over, resulting in the widespread circulation of misinformation (cf. \nThomas, 2021). This issue is made even more pressing given how fast LLMs can produce text, \nwhich means that Wikipedia could easily be swamped by a deluge of erroneous or biased LLM-\ngenerated content. \nImplications for Information Professionals \nGiven that Wikipedia and LLMs are, in their own ways, resources that facilitate the \ndissemination of information, the overlap of these two topics is arguably of great interest to the \ninformation profession. Thanks to their technological capabilities, LLMs can pair nicely with \nWikipedia, helping to make the site stronger and more readable, but uncritical widespread use of \nLLMs on Wikipedia could be something of the perfect storm, resulting in the addition of text that \nplagiarizes, violates copyright, lacks acceptable sourcing, or contains misinformation. It is a \ncomplex situation with far-reaching ramifications, but luckily, for information professionals who \nrecognize the significance of the issue, there are several ways to mitigate any damages while \nclarifying the benefits: First, information professionals should increasingly discuss the benefits \nand pitfalls of using LLMs on Wikipedia, both amongst themselves and with the community \nmembers that they regularly help. Second, information professionals are also encouraged to \nmove to the \"front lines\" (so to speak) by actively becoming Wikipedia editors. By moving from \ntalk to action, information professionals can actually demonstrate appropriate LLM use, critically \nanalyze content that may have been created using LLMs, and ultimately ensure that LLM-\ngenerated misinformation is not broadcast the world over. \n \n  \nReferences \nAtlas, S. (2023), ChatGPT for higher education and professional development: A \nguide to conversational AI, University of Rhode Island DigitalCommons@URI, South \nKingstown, RI. \nAzamfirei, R., Kudchadkar, S.R. and Fackler, J. (2023), \"Large language models and the perils \nof their hallucinations\", Critical Care, Vol. 27 No. 1, pp. 1-2. \nBorji, A. (2023), \"A categorical archive of ChatGPT failures\", arXiv, doi: \n10.48550/arXiv.2302.03494. \nDevlin, J., Chang, M-W., Lee, K. and Toutanova, K. (2019). \"BERT: Pre-training of deep \nbidirectional transformers for language understanding\", arXiv, \ndoi:10.48550/arXiv.1810.04805. \nDwivedi, Y. K., Kshetri, N., Hughes, L., Slade, E. L., Jeyaraj, A., Kar, A. K., ... and Wright, R. \n(2023), \"So what if ChatGPT wrote it?' Multidisciplinary perspectives on opportunities, \nchallenges and implications of generative conversational AI for research, practice and \npolicy\", International Journal of Information Management, Vol. 71, \ndoi:10.1016/j.ijinfomgt.2023.102642. \nEliot, L. (2023), \" Legal doomsday for generative AI ChatGPT if caught plagiarizing or \ninfringing, warns AI ethics and AI law\", Forbes, available at: \nhttps://www.forbes.com/sites/lanceeliot/2023/02/26/legal-doomsday-for-generative-ai-\nchatgpt-if-caught-plagiarizing-or-infringing-warns-ai-ethics-and-ai-\nlaw/?sh=5a5d8dff122b (accessed 29 March 2023). \nGibney, E. (2022), \"Open-source language AI challenges big tech’s models\", Nature, available \nat: https://www.nature.com/articles/d41586-022-01705-z (accessed 29 March 2023) \nGiles, J. (2005), \"Internet encyclopaedias go head to head\", Nature, Vol. 438 No. 7070, pp. 900-\n901, doi:10.1038/438900a. \nGravel, J., D’Amours-Gravel, M., Osmanlliu, E. (2023), \"Learning to fake it: Limited responses \nand fabricated references provided by ChatGPT for medical questions\", medRxiv, \ndoi:10.1101/2023.03.16.23286914. \nJi, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., ... & Fung, P. (2023), \"Survey of hallucination \nin natural language generation\", ACM Computing Surveys, Vol. 55 No. 12, 1-38. \nJPxG (2023), \"User:JPxG/LLM demonstration\", Wikipedia, available at \nhttps://en.wikipedia.org/w/index.php?title=User:JPxG/LLM_demonstration&oldid=1135\n546141 (accessed 29 March 2023). \nLiu, Y., Medlar, A. and Glowacka, D. (2021), \"Can language models identify Wikipedia articles \nwith readability and style issues?\", in Proceedings of the 2021 ACM SIGIR International \nConference on Theory of Information Retrieval, Association for Computing Machinery, \nNew York City, pp. 113-117. \nLucassen, T., Dijkstra, R., Schraagen, J.M. (2012), \"Readability of Wikipedia\", First Monday, \nVol. 17 No. 9, doi:10.5210/fm.v0i0.3916. \nLund, B. and Wang, T. (2023), \"Chatting about ChatGPT: How may AI and GPT impact \nacademia and libraries?\", Library Hi Tech News, doi:10.1108/LHTN-01-2023-0009. \nMarr, B. (2023), \"ChatGPT: What are hallucinations and why are they a problem for AI \nsystems\", available at https://bernardmarr.com/chatgpt-what-are-hallucinations-and-why-\nare-they-a-problem-for-ai-systems/ (accessed 22 March 2023). \nOpenAI (2023), \"GPT-4 technical report\", arXiv, doi:10.48550/arXiv.2303.08774. \nRosenzweig, R. (2006), \"Can history be open source? Wikipedia and the future of the past\", The \nJournal of American History, Vol. 93 No. 1, 117-146. \nRoose, K. (2023), \"How does ChatGPT really work?\", The New York Times, available at \nhttps://www.nytimes.com/2023/03/28/technology/ai-chatbots-chatgpt-bing-bard-llm.html  \n(accessed 29 March 2023). \nSimilarWeb (2023), \"Top Websites Ranking\", available at https://www.similarweb.com/top-\nwebsites/ (accessed 29 March 2023). \nSun, G.H. and Hoelscher, S.H. (2023), \"The ChatGPT storm and what faculty can do\", Nurse \nEducator, doi:10.1097/NNE.0000000000001390. \nTaecharungroj V. (2023), \"'What can ChatGPT do?' Analyzing early reactions to the innovative \nAI chatbot on Twitter\", Big Data and Cognitive Computing, Vol. 7 No. 1, doi: \n10.3390/bdcc7010035. \nThomas, P.A. (2021), \"Reverting hegemonic ideology: Research librarians and information \nprofessionals as 'critical editors' of Wikipedia\", College & Research Libraries, Vol. 82 \nNo. 4, pp. 567-583, doi:10.5860/crl.82.4.567. \nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M-A., Lacroix, T., Rozière, B., \nGoyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E. and Lample G. \n(2023),\"LLaMA: Open and Efficient Foundation Language Models\", arXiv, \ndoi:10.48550/arXiv.2302.13971. \nWolchover, N. (2011), \"How Accurate Is Wikipedia?\", Live Science, available at \nhttps://www.livescience.com/32950-how-accurate-is-wikipedia.html (accessed 22 March \n2023). \nZhong, Q., Tan, X., Du, R., Liu, J., Liao, L., Wang, C., Sun, R., Tang, Z., Ren, J., Mebrahtu, C., \nZeng, F. (2023), \"Is ChatGPT A reliable source for writing review articles in catalysis \nresearch? A case study on CO2 hydrogenation to higher alcohols\", Preprints, \ndoi:10.20944/preprints202302.0292.v1. ",
  "topic": "Scholarship",
  "concepts": [
    {
      "name": "Scholarship",
      "score": 0.6111675500869751
    },
    {
      "name": "Value (mathematics)",
      "score": 0.5628101825714111
    },
    {
      "name": "USable",
      "score": 0.5586503148078918
    },
    {
      "name": "Computer science",
      "score": 0.5439635515213013
    },
    {
      "name": "Originality",
      "score": 0.5339170098304749
    },
    {
      "name": "Section (typography)",
      "score": 0.4491429924964905
    },
    {
      "name": "World Wide Web",
      "score": 0.3258671760559082
    },
    {
      "name": "Sociology",
      "score": 0.260911762714386
    },
    {
      "name": "Political science",
      "score": 0.21486800909042358
    },
    {
      "name": "Social science",
      "score": 0.16642212867736816
    },
    {
      "name": "Law",
      "score": 0.14199504256248474
    },
    {
      "name": "Qualitative research",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Machine learning",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I93131583",
      "name": "Emporia State University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I146416000",
      "name": "University of Kansas",
      "country": "US"
    }
  ]
}