{
  "title": "ReasoningLM: Enabling Structural Subgraph Reasoning in Pre-trained Language Models for Question Answering over Knowledge Graph",
  "url": "https://openalex.org/W4389518745",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5069058824",
      "name": "Jinhao Jiang",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5100722039",
      "name": "Kun Zhou",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5037145565",
      "name": "Wayne Xin Zhao",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5046576694",
      "name": "Yaliang Li",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5025631695",
      "name": "Ji-Rong Wen",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6813608150",
    "https://openalex.org/W4389520779",
    "https://openalex.org/W2964120615",
    "https://openalex.org/W2971155257",
    "https://openalex.org/W3113177135",
    "https://openalex.org/W3034862985",
    "https://openalex.org/W4225580830",
    "https://openalex.org/W2963448850",
    "https://openalex.org/W4288286281",
    "https://openalex.org/W4310744116",
    "https://openalex.org/W3116847845",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W3211394146",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W2890961898",
    "https://openalex.org/W2251079237",
    "https://openalex.org/W2755637027",
    "https://openalex.org/W4387559293",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4287210309",
    "https://openalex.org/W4293140814",
    "https://openalex.org/W4285147034",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3172335055",
    "https://openalex.org/W4229061408",
    "https://openalex.org/W4285172793",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4385572953"
  ],
  "abstract": "Question Answering over Knowledge Graph (KGQA) aims to seek answer entities for the natural language question from a large-scale Knowledge Graph (KG). To better perform reasoning on KG, recent work typically adopts a pre-trained language model (PLM) to model the question, and a graph neural network (GNN) based module to perform multi-hop reasoning on the KG. Despite the effectiveness, due to the divergence in model architecture, the PLM and GNN are not closely integrated, limiting the knowledge sharing and fine-grained feature interactions. To solve it, we aim to simplify the above two-module approach, and develop a more capable PLM that can directly support subgraph reasoning for KGQA, namely ReasoningLM. In our approach, we propose a subgraph-aware self-attention mechanism to imitate the GNN for performing structured reasoning, and also adopt an adaptation tuning strategy to adapt the model parameters with 20,000 subgraphs with synthesized questions. After adaptation, the PLM can be parameter-efficient fine-tuned on downstream tasks. Experiments show that ReasoningLM surpasses state-of-the-art models by a large margin, even with fewer updated parameters and less training data. Our codes and data are publicly available at https://github.com/RUCAIBox/ReasoningLM.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3721‚Äì3735\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nReasoningLM: Enabling Structural Subgraph Reasoning in Pre-trained\nLanguage Models for Question Answering over Knowledge Graph\nJinhao Jiang1,3, Kun Zhou2,3, Wayne Xin Zhao1,3‚àó, Yaliang Li4, Ji-Rong Wen1,2,3\n1Gaoling School of Artificial Intelligence, Renmin University of China.\n2School of Information, Renmin University of China.\n3Beijing Key Laboratory of Big Data Management and Analysis Methods.\n4Alibaba Group.\n{jiangjinhao,jrwen}@ruc.edu.cn,francis_kun_zhou@163.com\nyaliang.li@alibaba-inc.com, batmanfly@gmail.com\nAbstract\nQuestion Answering over Knowledge Graph\n(KGQA) aims to seek answer entities for\nthe natural language question from a large-\nscale Knowledge Graph (KG). To better per-\nform reasoning on KG, recent work typically\nadopts a pre-trained language model (PLM) to\nmodel the question, and a graph neural net-\nwork (GNN) based module to perform multi-\nhop reasoning on the KG. Despite the effective-\nness, due to the divergence in model architec-\nture, the PLM and GNN are not closely inte-\ngrated, limiting the knowledge sharing and fine-\ngrained feature interactions. To solve it, we aim\nto simplify the above two-module approach,\nand develop a more capable PLM that can di-\nrectly support subgraph reasoning for KGQA,\nnamely ReasoningLM. In our approach, we\npropose a subgraph-aware self-attention mech-\nanism to imitate the GNN for performing struc-\ntured reasoning, and also adopt an adaptation\ntuning strategy to adapt the model parame-\nters with 20,000 subgraphs with synthesized\nquestions. After adaptation, the PLM can be\nparameter-efficient fine-tuned on downstream\ntasks. Experiments show that ReasoningLM\nsurpasses state-of-the-art models by a large\nmargin, even with fewer updated parameters\nand less training data. Our codes and data\nare publicly available at https://github.com/\nRUCAIBox/ReasoningLM.\n1 Introduction\nQuestion answering over knowledge graph\n(KGQA) (Sun et al., 2018; He et al., 2021) has gar-\nnered significant attention in recent years, which\naims to find answers for the natural language ques-\ntions based on knowledge graphs (KGs), e.g., Free-\nbase (Bollacker et al., 2008) and Wikidata (Tanon\net al., 2016). Since a massive amount of world\nknowledge has been formatted into a structured\nform (e.g., a triple ‚ü®head, relation, tail‚ü©) in the\n‚àó Corresponding author.\nKG, we can develop KGQA methods by leveraging\nstructural semantics of KG to more accurately infer\nthe answer entities to complex factual questions.\nStarting from the topic entities mentioned in the\nquestion, a typical KGQA approach (Sun et al.,\n2018) is to perform the multi-hop reasoning along\nwith relations on the KG, until finding a path that\ncan reach the answer entities. To develop this ap-\nproach, existing methods (Sun et al., 2018; He et al.,\n2021; Shi et al., 2021) mostly incorporate a text\nencoder to produce the representation of the given\nquestion, and a reasoning module to perform multi-\nhop reasoning on the KG using the question rep-\nresentation. Typically, recent work (Jiang et al.,\n2022b,a) adopts the pre-trained language mod-\nels (PLM) (e.g., BERT (Devlin et al., 2019)) and\ngraph neural networks (GNN) (e.g., GAT (Velick-\novic et al., 2017)) to implement the text encoder\nand reasoning module respectively, which can bet-\nter understand the semantic information in the ques-\ntion and structured knowledge from the KG, im-\nproving the final performance\nDespite the effectiveness, there are two major\nshortcomings with the aforementioned approach\nthat combines a PLM encoder and a GNN reasoner.\nFirst, due to different model architectures, PLM\nand GNN are often integrated in a loose way (e.g.,\nrelevance score sharing), which largely limits the\nknowledge sharing and fine-grained interaction be-\ntween the question and KG (a subgraph spanned\nby related entities). Second, the GNN based rea-\nsoner performs reasoning mainly based on sub-\ngraph structure, which lack rich semantic knowl-\nedge as that in PLMs, making the reasoning results\nlikely to be less effective, especially for complex\nquestions. In addition to the two shortcomings,\nsuch a approach also requires a complicated im-\nplementation in practice, since it involves in two\ndifferent modules.\nTo address these issues, we aim to simplify the\nabove two-module approach and develop a more\n3721\ncapable PLM that can directly support structural\nsubgraph reasoning for KGQA. Our approach is\ninspired by the finding that the Transformer archi-\ntecture consisting of stacked self-attention mod-\nules can be explained as a fully-connected graph\nencoder (Dwivedi and Bresson, 2020) in a mathe-\nmatically equivalent way. Therefore, Transformer\nessentially has the potential to effectively model\ngraph data and further performs graph reasoning,\nwhich has been shown in existing work (Ying et al.,\n2021). While, these attempts based on graph Trans-\nformers either neglect the modeling in text seman-\ntics, or cannot capture fine-grained semantic inter-\naction between question and KG subgraph, making\nthem infeasible for KGQA tasks.\nTo this end, in this paper, we propose a subgraph\nreasoning enhanced PLM, calledReasoningLM, en-\nabling both effective question understanding and\nKG reasoning in a unified approach. As the major\ncontribution, we propose a subgraph-aware self-\nattention mechanism, which can imitate the GNN\nto model the entities and their relations via attend-\ning to neighboring nodes on the KG. Further, such a\nstructural attention mechanism has been integrated\ninto a constrained masking framework, to jointly\nmodel question attention, KG to question attention,\nand KG attention. In this way, we can not only\nperform the knowledge interaction and sharing be-\ntween the question and KG, but also leverage PLM\nto perform structural reasoning as GNN. However,\nsince the PLM is originally trained by learning\nfrom general-purpose natural language text, it is\nnecessary to adapt it to the special input format and\nattention mechanism. Thus, we propose an adapta-\ntion tuning strategy that utilizes 20,000 subgraphs\nwith synthesized questions to adapt the parameters\nof the PLM. After adaptation, the PLM has been\nwell adapted for subgraph reasoning, hence it can\nbe fine-tuned on different downstream KGQA tasks\nin a parameter-efficient manner, achieving better\nperformance with only a few parameters trained.\nTo evaluate the effectiveness of our approach,\nwe conduct extensive experiments on three KGQA\ndatasets. Experimental results demonstrate that our\nproposed approach can surpass existing state-of-\nthe-art models by a large margin, even with fewer\nupdated parameters and less training data.\nOur contributions can be summarized as follows:\n‚Ä¢We enable the PLM to simultaneously model\nquestion understanding, deep interaction between\nquestion and subgraph, and reasoning over sub-\ngraph by leveraging an adapted subgraph-aware\nself-attention mechanism.\n‚Ä¢We propose an automatic data construction\nmethod for the KGQA task format using LLMs to\nsupport the adaptation of PLM to the special input\nformat and attention mechanism.\n2 Related Work\nQuestion Answering over Knowledge Graph.\nMulti-hop KGQA aims to find answer entities\nthat are multiple hops away from the topic enti-\nties in a large-scale KG. Existing work (Sun et al.,\n2018) typically first retrieves a question-relevant\nsubgraph from the KG to reduce the search space\nand then performs multi-hop reasoning to find the\nanswer entities. Several methods (Sun et al., 2018;\nHe et al., 2021) have been developed to facilitate\nthe answer reasoning over the KG. These methods\ntypically consist of a question encoder to repre-\nsent the question, and a reason module to perform\nmulti-hop reasoning over the KG using the question\nrepresentation. Early work (Sun et al., 2018, 2019)\nuses a simple LSTM to encode the question and a\nGNN to model the reasoning over KG. However, a\nsingular representation of the entire question cre-\nates confusion for the GNN regarding the specific\nrelation that should be attended to at each step. To\naddress this concern, subsequent work (He et al.,\n2021) attempts to decompose the semantics of the\nquestion and send the corresponding representation\nto the GNN module at each step. With the develop-\nment of PLMs, recent work (Shi et al., 2021; Jiang\net al., 2022b) proposes to enhance the question un-\nderstanding by using PLM as the question encoder.\nThey use PLM to compute the semantic similarity\nbetween the question and relations at each step and\nuse a simpler GNN to propagate similarity infor-\nmation and update entity scores over KG.\nPLM for Knowledge Graph Reasoning. Besides\nKGQA, PLM has also been used for other knowl-\nedge graph reasoning tasks, such as commonsense\nreasoning (Yasunaga et al., 2021) or predicting\nmissing facts (Zamini et al., 2022). There are\nmainly two methods for PLM to leverage the KG.\nSeveral studies (Yasunaga et al., 2021) attempt to\nfuse the representation of the KG into the PLM,\nwhich is modeled by the GNN. It can make the\nPLM aware of the KG to some extent through the\nmodeled representation. However, it‚Äôs not easy to\nbridge the gap between the PLM and GNN given\n3722\nthe different model architecture and initialized pa-\nrameters, leading to a sufficient understanding of\nKG for PLM. In contrast, a more direct way is\nto linearize the KG as a sequence and input it to\nPLMs (Xie et al., 2022; Saxena et al., 2022). In\nthis way, the PLM can directly utilize the KG to\nperform reasoning. Despite its simplicity, such a\nway neglects the structure of KG, which is an im-\nportant feature. In contrast, we propose to model\nthe question and KG in a single PLM, while captur-\ning the structure information with subgraph-aware\nself-attention mechanism.\n3 Preliminary\nIn this section, we present the notations utilized\nthroughout the paper, followed by a formal defini-\ntion of the KGQA task.\nKnowledge Graph (KG). A knowledge graph is\ncommonly composed of a collection of triples, ex-\npressed as G= {‚ü®e, r, e‚Ä≤‚ü©|e, e‚Ä≤‚ààE, r‚ààR}, where\nEand Rdenote the entity set and relation set, re-\nspectively. A triple ‚ü®e, r, e‚Ä≤‚ü©describes the fact that\na relation r exists between head entity e and tail\nentity e‚Ä≤. Furthermore, we introduce entity neigh-\nborhood to denote both incoming and outgoing\ntriples for an entitye, denoted asNe = {‚ü®e, r, e‚Ä≤‚ü©‚àà\nG}‚à™{‚ü® e‚Ä≤, r, e‚ü©‚ààG} . In this way, we can simplify\nthe definition of the neighborhood triples for an\nentity e as Ne = {‚ü®e, r, e‚Ä≤‚ü©‚ààG} .\nQuestion Answering over Knowledge Graph\n(KGQA). Given a natural language question q and\na KG G, the task of KGQA aims to find answer en-\ntitie(s), denoted as Aq ‚ààE, to the question on the\nKG. Following previous work (Sun et al., 2018),\nwe assume that the entities mentioned in the ques-\ntion have already been linked with entities on KG,\ncalled topic entities, denoted as Tq ‚äÇE . In this\nwork, we focus on solving the KGQA task where\nthe answer entities are multiple hops away from\nthe topic entities over the KG. Considering the\ntrade-off between efficiency and accuracy, we fol-\nlow existing work (Sun et al., 2018) that solves this\ntask using a retrieval-then-reasoning framework.\nIn the two-stage framework, given a question q\nand topic entities Tq, the retrieval stage aims to re-\ntrieve a small question-relevant subgraph Gq from\nthe large-scale input KG G, while the reasoning\nstage finds answer entities Aq by reasoning over\nthe retrieved subgraph Gq.\n4 Approach\nIn this section, we present our proposed approach,\ni.e., ReasoningLM, which enables both effective\nquestion understanding and KG reasoning in a sin-\ngle PLM for better solving the KGQA task.\n4.1 Overview\nExisting KGQA methods (Sun et al., 2018) typi-\ncally adopt a PLM to encode the question into latent\nrepresentation, and a GNN to perform the reason-\ning over KG guided by the representation. How-\never, due to the architecture divergence, it is hard\nto closely integrate the PLM and GNN for knowl-\nedge sharing and feature interaction. Inspired by\nrecent work (Dwivedi and Bresson, 2020) that re-\nveals the similarity between GNN and Transformer\narchitecture of PLMs, we take a different perspec-\ntive to simplify the above two-module approach\nand develop a more capable PLM that can support\nstructural subgraph reasoning for KGQA.\nOur basic idea is to implement a GNN within\nthe PLM, to bridge the knowledge gap for question\nunderstanding and KG reasoning. To achieve it, we\npropose the ReasoningLM, enabling both effective\nquestion understanding and KG reasoning in a uni-\nfied approach. Concretely, we first adapt the PLM\nto implement the functionality of GNN to aggre-\ngate the information from the neighboring entities\nand relations with subgraph-aware self-attention\nmechanism (Section 4.2). Then, we adopt adap-\ntation tuning to optimize the PLM parameters to\nbetter adapt it to the special input format and atten-\ntion mechanism (Section 4.3). Finally, we apply\nthe adapted PLM to solve the KGQA task with\nparameter-efficient fine-tuning (Section 4.4).\nThe overview of ReasoningLM is shown in Fig-\nure 1. In our model, as the PLM can understand\nthe question and perform reasoning in KG in a uni-\nfied approach, it can freely share and interact with\nthe knowledge from both for improving the KGQA\ntask. Besides, such a way also enables the PLM to\nfully participate in the KG reasoning process (in-\nstead of GNN solely), which can make full use of\nits rich knowledge and strong reasoning capacity.\n4.2 Adapting PLM for Subgraph Reasoning\nIn this section, we discuss how to adapt PLM\nfor subgraph reasoning. Next, we introduce the\nsubgraph data serialization and then present the\nsubgraph-aware self-attention mechanism to unify\nquestion understanding and subgraph reasoning\n3723\nTransformer Layer with\nSubgraph-aware Self-Attention\nùê∂ùêøùëÜ ùëû! ùëû\"‚Ä¶ ùëí# ùëí! ùëí$ ùëí% ùëí&ùëü! ùëü$ ùëü&ùëü%ùëÜùê∏ùëÉ\nInput\nEmbeddings\nTransformer\nEncoder\nOutput\nHidden States\nLinear Layer\nùê∂ùêøùëÜ ùëû! ùëû\"‚Ä¶ ùëí# ùëí! ùëí$ ùëí% ùëí&ùëü! ùëü$ ùëü&ùëü%ùëÜùê∏ùëÉ\nMasked Attention visible\ninvisible\nPredicted\nEntity Scores\n(x N)\n{what, is, the, name, of, justin, bieber, brother}Question ùëí#: Justin \nBieber ùëü!:sibling ùëí!: Jaxon\nBieber ‚Ä¶\n{ Jaxon\nBieber ,Justin\nBieber , ,sibling gender, ,sibling femaleJazmyn\nBieber ,male, }gender ,Subgraph\n0.0 0.7 0.1 0.2 0.0 !\"#\n!!\n!\"\n‚Ä¶\n#$%\n!\"#!! !\"‚Ä¶ #$% ## $! $$ #$#! $% $&#% #&\n$!\n$$\n#$\n#!\n$%\n$&\n#%\n#&\n##\nA\n D\nB\n C\nSubgraph\nJustin \nBieber\nMale\nsibling\nsibling\nJaxon \nBieber\nJazmyn \nBieber\nFemale\ngender\ngender\nFigure 1: The illustration of performing answer entity reasoning over a subgraph according to the question using\nReasoningLM with our proposed subgraph serialization and subgraph-aware self-attention.\nwithin a single PLM, respectively.\n4.2.1 BFS-based Subgraph Serialization\nFor KGQA task, the input data typically consists\nof the natural language question q and its relevant\nsubgraph Gq retrieved from the KG. To enable the\nPLM to capture the structured information from the\nsubgraph and perform reasoning on it, we propose\na breadth-first search (BFS) based subgraph serial-\nization to convert the subgraph into a sequence.\nConcretely, starting from a topic entity e0 ‚ààTq,\nwe perform the BFS to visit all triples on Gq. It\nfirst visits all the triples whose head entity is the\ntopic entity, e.g., ‚ü®e0, r1, e1‚ü©, and then moves on to\nthe triples whose head entities have been visited\nbefore, and so forth. Based on the order in the\nBFS process, we concatenate all the visited triples\nas a long sequence. To reduce the node sequence\nlength, we only concatenate the entities or relations\nfor the first time that they are visited, hence the\nfinal serialized subgraph consists of all the visited\nentities and relations, denoted as:\nSGq = {e0, r1, e1, r2, e2, ..., rm, em}, (1)\nwhere m is the total number of triples in SGq .\nIn this way, we can preserve the structure infor-\nmation of the subgraph within a relatively short\nsequence (with length as the count of entities and\nrelations in SGq ), which can be varied according to\nthe context length of language models.\n4.2.2 Subgraph-aware Self-Attention\nBased on the serialized subgraph, we leverage the\nsubgraph-aware self-attention mechanism to sup-\nport graph reasoning within the PLM, to propa-\ngate relevance information along with the relation\nedges on KG. We first initialize the embeddings of\nthe serialized subgraph and the question, and then\nperform self-attention with constrained masks to\naggregate their representations.\nEmbeddings Mapping. Since PLMs do not have\nthe mapping embeddings for the entities and rela-\ntions within the serialized subgraph, we need to\ninitialize their embeddings before feeding them\ninto the PLM. To embed them into the semantic\nspace of the PLM, we first tokenize each entity\nor relation into subwords1 using the tokenizer of\nthe PLM, and then sum their embeddings into a\nsingle embedding to represent it. Finally, we can\nobtain the embedding matrix of the whole serial-\nized subgraph, denoted as NGq ‚ààRl√ód, where d\nis the embedding dimension and l is the length of\nthe input sequence. Next, we concatenate it with\nthe token embedding matrix Nq of the question\nq after tokenization, to compose the input token\nembedding matrix of the PLM:\nN = [Nq; NGq ] = [nq1 , ¬∑¬∑¬∑ , nqn; ne0 , ¬∑¬∑¬∑ , nem]\nBased on it, we also add the position embeddings\nas NE = N + E to obtain the input embedding\nmatrix of the PLM.\nSelf-Attention with Constrained Masks. After\nobtaining the input embedding matrix NE, we\nfeed it into the multi-layer Transformer encoder\n1PLMs mostly use Byte-Pair Encoding tokenizer, which\nmay segment an entity or relation name into several subwords.\n3724\nof the PLM, to perform reasoning over the sub-\ngraph based on the given question. To enable the\nfine-grained semantic interaction between the ques-\ntion and the associated subgraph, we propose a\nconstrained mask mechanism on the self-attention\nlayers for controlling the attention interaction, in-\ncluding four kinds of attention modes:\n‚Ä¢Full question attention. The representations\nof each token of question can attend to the other\ntokens of question (part A in Figure 1).\n‚Ä¢Full subgraph ‚Üíquestion attention. The rep-\nresentations of all the entities and relations can\nattend to the question representations, hence we\ncan perform reasoning on the KG based on the\nquestion (part B in Figure 1).\n‚Ä¢Structural subgraph attention. In the serialized\nsubgraph, an entity can aggregate the information\nfrom its one-hop neighboring entities and relations\n(in a triple), similar to the updated way of node\nrepresentations in GNN. Further, a relation can\naggregate the information from its head and tail\nentities in the triple, as it establishes a link between\nthe two entities (part C in Figure 1).\n‚Ä¢Except for the above ways, other information\nflows are forbidden in the self-attention layer. In\naddition to the constraint on the subgraph struc-\nture, we also prevent the question attending to the\nsubgraph, which can avoid the question representa-\ntions to be influenced by the irrelevant information\nin the subgraph (part D in Figure 1).\nTo achieve them, we design the constrained self-\nattention mask M ‚ààRl√ól, where the value in the\ni-th raw and j-th column denotes whether the i-th\ntoken can attend to the j-th one (0) or not (-inf),\ndenoted as:\nMij =\nÔ£±\nÔ£¥Ô£≤\nÔ£¥Ô£≥\n0 xi ‚ààSGq and xj ‚ààq,\n0 xi, xj ‚ààSGq and aij = 1,\n-INF others,\n(2)\nwhere xi and xj represent the tokens (i.e., relations,\nentities or words) in the i-th and j-th positions of\nthe input, and aij = 1indicates that xi and xj are\nadjacent (within a KG triple). Then, we utilize\nthe mask matrix to compute the constrained self-\nattention on the multi-layer Transformer encoder\nof the PLM as follows:\nAttn(Q, K, V ) =softmax(A + M)V , (3)\nwhere A ‚ààRl√ól is the original attention matrix,\nand Q, K, V ‚ààRl√ód are the input representation\nmatrices of the self-attention layer. In this way,\nonly the self-attention values between invisible to-\nkens would be zero (after softmax activation on\n-INF values), avoiding the PLM to aggregate repre-\nsentations from them.\n4.3 Adaptation Tuning\nTo help the PLM well adapt into the special in-\nput format and attention mechanism, we adopt the\nadaptation tuning to adapt the parameters of the\nPLM. We first collect the tuning dataset based on\nsampled subgraphs and synthesized questions, and\nthen utilize the answer entity prediction task for\ntraining.\n4.3.1 Tuning Data Construction\nTo enable the PLM to understand the question and\nperform reasoning on the KG, we construct an adap-\ntation dataset in an automatic way to tune its pa-\nrameters. The dataset consists of 20,000 synthe-\nsized questions with relevant subgraphs extracted\nfrom the KG, and the answer entities. Next, we\nintroduce the process of subgraph extraction and\nquestion synthesis.\nSubgraph Extraction. We extract the subgraphs\nfrom Wikidata2, a general-domain KG with natural\nlanguage descriptions of the entities and relations.\nWe consider to extract a set of subgraphs centering\naround popular entities, to better adapt the PLM for\nunderstanding commonly-used knowledge. Thus,\nwe use the popular entities in Wikidata5M (Wang\net al., 2021) as our seed topic entity set following\nKQA Pro (Cao et al., 2022), then randomly sample\nthe answer and the subgraph. Starting from the\ntopic entity, we perform a random walk over the\nKG, to sample a reasoning path with no more than\n4 hops, whose ending entity is regarded as the an-\nswer entity. Then, we randomly extract the entities\nand relations around the topic entity to compose\nthe subgraph, where we guarantee that the entities\nand relations from the reasoning path are also in-\ncluded. Such a way is easy to conduct and can\nautomatically extract multiple subgraphs with the\nanswer entities.\nQuestion Synthesis. Based on the sampled rea-\nsoning path and answer entities, we also adopt an\nautomatic way to synthesize the questions. Here,\nwe propose two approaches, i.e., rule-based synthe-\nsis and LLM-based synthesis. For the rule-based\n2https://www.wikidata.org/\n3725\none, we first hand-craft several general templates,\nand then utilize them to convert the topic entity\nand the relations on the reasoning path into a natu-\nral language question. However, such a way leads\nto a poor diversity of questions, and also needs\nhuman efforts. Recently, as large language mod-\nels (e.g., ChatGPT) have shown a powerful gener-\nation capability (Brown et al., 2020; Zhao et al.,\n2023) and have been used to generate label-free\ndatasets (Chen et al., 2023), we seek help from\nthem to produce more high-quality questions. Con-\ncretely, we write a prompt to guide ChatGPT, a pop-\nular LLM, to generate the corresponding question\nfor the answer entity based on the reasoning path\nand the topic entity. In this way, we cost approxi-\nmately 15 dollars, and obtain 20,000 questions with\ndiverse formats and fluent expression. The detail\nprompt we used is shown in Appendix D.\n4.3.2 Answer Entity Prediction\nGiven the synthesized question, extracted sub-\ngraphs and the answer entities, we feed them into\nour ReasoningLM, and tune the model parame-\nters via the answer entity prediction task. It is a\nmulti-classification task to predict which entity in\nthe subgraph is the answer entity of the question.\nConcretely, through the multi-layer Transformer\nencoder with constrained self-attention, we can\nobtain the hidden state H ‚àà Rl√ód of the input\nsequence at the last layer. Then, we add a linear\nprediction layer with the softmax activation func-\ntion to transform the hidden state into the answer\nscores of all entities:\ns = softmax(Linear(H)), (4)\nwhere s ‚àà Rl. Then, we minimize the KL di-\nvergence between the predicted and ground-truth\nanswer scores as:\nLat = DKL(s, s‚ãÜ), (5)\nwhere s‚ãÜ is the ground-truth answer scores of all\nentities, where an entity is 1 if it is a labeled answer\nentity. Note that we only compute the loss for the\nentities, as the relations and question words are not\nable to be the answer.\n4.4 Efficient Fine-tuning\nAfter adaptation tuning, the PLM has been well\nadapted to performing reasoning over the general-\ndomain subgraphs. Therefore, we further perform\nparameter-efficiently fine-tuning on the PLM, to\napply it to the subgraph retrieval and answer reason-\ning subtasks respectively, where we only tune the\nparameters in the adapters (Houlsby et al., 2019)\nbut freeze other parameters.\nSubgraph Retrieval. We follow Zhang et al.\n(2022) to fine-tune our model on the subgraph re-\ntrieval subtask, where we optimize our model to\nlearn to predict the similarity between the question\nand relevant relations. During inference, starting\nfrom the topic entities, the model iteratively mea-\nsures the semantic relevance between the question\nand neighboring relations, and adds proper ones\nand their corresponding triples into the subgraph,\nto extract a question-relevant subgraph.\nAnswer Reasoning. Based on the retrieved sub-\ngraph, we also utilize the answer entity prediction\ntask in Section 4.3.2 to fine-tune our ReasoningLM,\nto learn to accurately find the answer entities of the\ngiven question from the subgraph. During infer-\nence, we select the highest scoring entity predicted\nby our approach as the answer entity.\n5 Experiments\n5.1 Experimental Setup\nAdaptation Tuning Corpus. Our adaptation tun-\ning corpus is collected from Wikidata (Tanon\net al., 2016), a general domain knowledge\ngraph. We download the English Wikidata\nDumps (2018/12/31) from the official site, and\nextract 2,000 entities from Wikidata5M (Wang\net al., 2021) as seed topic entities. Finally, we\nconstruct 20,000 samples for adaptation tuning and\nsplit 1,000 samples as the validation set for select-\ning the best checkpoint.\nDatasets. Following existing work on KGQA (He\net al., 2021), we conduct experiments on three pop-\nular datasets to evaluate our proposed approach,\nincluding WebQuestionsSP (WebQSP)(Yih et al.,\n2015), Complex WebQuestions 1.1 (CWQ) (Talmor\nand Berant, 2018), and MetaQA (MQA) (Zhang\net al., 2018). Table 5 shows the statistics of the\nthree datasets. We give a detailed description of\neach dataset in Appendix A\nEvaluation Protocol. We follow existing work\nthat treats the reasoning as a ranking task for evalu-\nation (Sun et al., 2018). For each question, we rank\nthe answer score of all candidate entities and then\nassess the correctness of the top-1 answer using the\n3726\nTable 1: Performance comparison of different methods for KGQA (Hits@1 and F1 in percent). We copy the results\nof LLMs from Jiang et al. (2023) and the results of the other baselines from Jiang et al. (2022b). Bold and underline\nfonts denote the best and the second-best methods, respectively. ‚ÄúFPT‚Äù and ‚ÄúPET‚Äù denote the full-parameter tuning\nand parameter-efficient tuning, respectively. ‚ÄúRule-SYN‚Äù and ‚ÄúLLM-SYN‚Äù refer to synthesize the questions using\nrule-based and LLM-based strategies, respectively.\nModels Updated\nParams\nWebQSP CWQ MQA-1H MQA-2H MQA-3H\nHits@1 F1 Hits@1 F1 Hits@1 Hits@1 Hits@1\nKV-Mem (Miller et al., 2016a) - 46.7 34.5 18.4 15.7 - - -\nGraftNet (Sun et al., 2018) 0.5M 66.4 60.4 36.8 32.7 82.5 - -\nPullNet (Sun et al., 2019) - 68.1 - 45.9 - - - -\nEmbedKGQA (Saxena et al., 2020) 125M 66.6 - - - 92.0 40.7 34.6\nNSM (He et al., 2021) 3M 68.7 62.8 47.6 42.4 94.8 97.0 91.0\nTransferNet (Shi et al., 2021) 111M 71.4 - 48.6 - 96.5 97.5 90.1\nSR+NSM+E2E (Zhang et al., 2022) 130M 69.5 64.1 49.3 46.3 - - -\nUniKGQA (Jiang et al., 2022b) 12M 75.1 70.2 50.7 48.0 97.1 98.2 92.6\nDavinci-003 (Ouyang et al., 2022) - 48.3 - - - 52.1 25.3 42.5\nChatGPT - 61.2 - - - 61.9 31.0 43.2\nStructGPT (Jiang et al., 2023) - 72.6 - - - 94.2 93.9 80.2\nReasoningLM (FPT, LLM-SYN) 1M 78.5 71.0 69.0 64.0 96.5 98.3 92.7\nw FPT, Rule-SYN 1M 78.0 70.5 62.8 55.4 96.1 96.9 91.0\nw PET, LLM-SYN 1M 76.7 69.1 68.3 62.4 95.7 97.0 90.9\nTable 2: Statistics of the experiment datasets.\nType Task KG Train Dev Test\nKGQA\nWebQSP Freebase 2,848 250 1,639\nCWQ Freebase 27,639 3,519 3,531\nMQA-1H OMDb 161 9,992 9,947\nMQA-2H OMDb 210 14,872 14,872\nMQA-3H OMDb 150 14,274 14,274\nHits@1 metric. Given that a question may have\nmultiple answers, we also adopt the F1 metric.\nBaselines. We consider the following three\ntypes of baseline methods for performance com-\nparison: (1) non PLM-based methods: KV-\nMem (Miller et al., 2016b), GraphtNet (Sun et al.,\n2018), PullNet (Sun et al., 2019), NSM (He\net al., 2021); (2) PLM-based methods: Embed-\nKGQA (Saxena et al., 2020), TransferNet (Shi\net al., 2021), SR+NSM+E2E (Zhang et al., 2022),\nUniKGQA (Jiang et al., 2022b); (3) LLM-based\nmethods: Davinci-003 (Ouyang et al., 2022), Chat-\nGPT, StructGPT (Jiang et al., 2023). We give a de-\ntailed description of each baseline in Appendix B.\n5.2 Implementation Details\nIn our experiment, we use RoBERTa-base as our\nbase PLM. During adaptation tuning, we optimize\nparameters with the AdamW optimizer, where the\nbatch size is 40 and the learning rate is 1e-4. We\nselect the best checkpoint of adaptation tuning ac-\ncording to the evaluation of the constructed vali-\ndation set. After adaptation tuning, we apply the\nReasoningLM to downstream KGQA tasks with\nparameter-efficient fine-tuning. We add the extra re-\ntrieval and reasoning adapter to the ReasoningLM\nfor subgraph retrieval and answering reasoning re-\nspectively, and only update the reasoning adapter\nwhile freezing other parameters.\nFor the retrieval stage, we follow the pipeline\nof Zhang et al. (2022) to fine-tune our Reason-\ningLM and then perform subgraph retrieval. we\ncollect question-relation pairs based on the shortest\nrelation paths between topic entities and answer\nentities, and then use these pairs to fine-tune the\nmodel to compute the similarity between the ques-\ntion and relations. We directly compute the similar-\nity between the question and relations at each hop\nand freeze other parameters except for the adapter\nmodule. we optimize parameters with the AdamW\noptimizer, where the batch size is 10 and the learn-\ning rate is 5e-5 for all datasets. Then, we leverage\nthe model to retrieve the subgraph. Specifically,\nstarting from the topic entities, the model itera-\ntively measures the semantic relevance between\nthe question and neighboring relations, and adds\ntop-k relations and their corresponding triples into\nthe subgraph. We set the k as 15 for WebQSP and\nCWQ, and 3 for MetaQA.\nFor the reasoning stage, we fine-tune Reason-\ningLM on the retrieved subgraph to perform answer\nreasoning. Similarly, we only update the adapter\nmodel and freeze other parameters. We optimize\n3727\nparameters with the AdamW optimizer, where the\nbatch size is 4 for MetaQA, 60 for WebQSP, and\n300 for CWQ while the learning rate is 1e-4 for all\ndatasets.\n5.3 Main Results\nWe show the results of all five data in Table 1.\nFirst, directly using LLMs (e.g., Davinci-003 and\nChatGPT) can achieve performance comparable\nto part of the supervised learning baselines on the\nWebQSP dataset. However, LLMs perform not\nwell on the more complex multi-hop datasets, such\nas MQA-2H and MQA-3H. It demonstrates that\nLLMs are still hard for effectively solving KGQA\ntasks relying solely on the LLM. Despite the incor-\nporation of external KG can enhance LLMs ( i.e.,\nStructGPT), they still have a performance gap com-\npared to the strong supervised learning models.\nSecondly, PLM-based methods (e.g., Transfer-\nNet, SR+NSM+E2E, and UnikGQA) can achieve\na consistently better performance compared with\nmethods not using PLM ( e.g., GraftNet, PullNet,\nand NSM).And UniKGQA achieves a further per-\nformance improvement on all datasets, benefiting\nfrom its unified architecture to learn the essential\ncapability of question-relation semantic matching\nfor both retrieval and reasoning stages.\nFinally, our ReasoningLM is substantially better\nthan all other competitive baselines in all datasets\nwith only updating a few parameters (only 1M),\nachieving a 4.5% improvement of Hits@1 on We-\nbQSP and 36.1% improvement of Hits@1 on more\ndifficult CWQ compared to the best baseline. Un-\nlike the other baselines, our approach develops a\nsubgraph reasoning enhanced PLM to model the\nquestion and subgraph seamlessly. We can utilize\nthe pre-trained knowledge within PLM, while en-\nabling the reasoning process over the subgraph can\ndirectly attend to the question. With further adap-\ntation tuning, our model can be applied to down-\nstream tasks with parameter-efficient fine-tuning.\nThese results demonstrate the effectiveness and ef-\nficiency of our ReasoningLM model.\nIn our approach, we update the full parame-\nters of our model (FPT) during adaptation tun-\ning with LLM synthesis data (LLM-SYN). Ac-\ntually, we can accomplish adaptation tuning at a\nsmaller cost by updating fewer parameters or us-\ning cheaper constructed data. Here, we study it\nby proposing two variants of our ReasoningLM:\n(1) w FPT, Rule-SYN that updates the full pa-\nTable 3: Ablation study of our the subgraph-aware self-\nattention mechanism (SA) and adaptation tuning (AT).\nModels\nWebQSP CWQ\nHits@1 F1 Hits@1 F1\nReasoningLM 78.5 70.1 69.0 64.0\nw/o SA 68.5 63.2 40.5 38.2\nw/o AT 67.5 60.4 55.2 43.3\nTable 4: Performance of implementing ReasoningLM\nwith different PLMs on CWQ.\nCWQ RoBERTa\n(base)\nRoBERTa\n(large)\nDeBERTa\n(base)\nBERT\n(base)\nHits@1 69.0 70.0 68.1 67.4\nF1 64.0 65.2 63.1 63.0\nrameters with rule-based constructed data, (2)\nw PET, LLM-SYN that updates an added adapter\nwhile freezing the model parameters with LLM\nsynthesis data. We can see that although both vari-\nants show varying degrees of performance decline,\nthey can still achieve better results compared to\nthe existing baselines on WebQSP and CWQ. For\nMetaQA, the two variants only achieve comparable\nperformance to existing baselines. A possible rea-\nson is that limited data makes it difficult to adapt\nthe model to a specific domain.\n5.4 Further Analysis\nAblation Study. Our approach contains two im-\nportant technical contributions, the first is the\nsubgraph-aware self-attention mechanism (SA),\nwhich imitates the GNN reasoning over the graph,\nand the second is the adaptation tuning (AT), which\nenhances the reasoning capability over KG. Here,\nwe conduct the ablation study to verify their effec-\ntiveness by removing each one individually. We ex-\nperiment with two variants as: (1)w/o AT removing\nthe adaptation tuning procedure, (2) w/o SA remov-\ning the subgraph-aware self-attention mechanism\nin self-attention mechnism. We show the results of\nthe ablation study in Table 3. All these variants un-\nderperform the complete ReasoningLM, which in-\ndicates that the two strategies are both important for\ndeveloping a subgraph reasoning enhanced PLM\nfor KGQA. Besides, we also analyze combining\nother reasoning models (e.g., NSM and UniKGQA)\nwith our retrieval sugraphs in Appendix C.\nVariants with Different PLMs. Since our pro-\nposed adaptation method does not change the orig-\n3728\n/uni00000013/uni00000011/uni00000018/uni0000002e/uni00000014/uni0000002e/uni00000014/uni00000011/uni00000018/uni0000002e/uni00000015/uni0000002e/uni00000015/uni00000011/uni00000018/uni0000002e/uni00000016/uni0000002e\n/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000029/uni0000004c/uni00000051/uni00000048/uni00000010/uni00000057/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000036/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056\n/uni00000015/uni00000018\n/uni00000016/uni00000018\n/uni00000017/uni00000018\n/uni00000018/uni00000018\n/uni00000019/uni00000018\n/uni0000001a/uni00000013/uni0000002b/uni0000004c/uni00000057/uni00000056/uni00000023/uni00000014/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000035/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni0000004c/uni00000051/uni0000004a/uni0000002f/uni00000030\n/uni00000038/uni00000051/uni0000004c/uni0000002e/uni0000002a/uni00000034/uni00000024\n/uni00000031/uni00000036/uni00000030\nFigure 2: The Hits@1 scores of our ReasoningLM on\nWebQSP and CWQ after adaptation tuning with a vari-\nous number of samples (Left). And the Hits@1 score\nof our ReasoningLM compared with two strong base-\nlines (i.e., NSM and UniKGQA) on CWQ when fine-\ntuning with various numbers of samples (Right)\ninal model architecture, it can be applied to other\ndifferent PLMs. To explore the performance with\ndifferent PLMs, we conduct experiments with other\nthree PLMs (i.e., RoBERTa-large, DeBERTa, and\nBERT). We show the results in Table 4. It is ob-\nserved that the utilization of DeBERTa-base and\nBERT-base also yields performance comparable\nto RoBERTa-base. This suggests that our adapta-\ntion method is agnostic to the PLMs used. At the\nsame time, a larger RoBERTa-large can achieve\nfurther performance improvement compared with\nRoBERTa-base. It indicates the potential of our\nmethod to be applied to larger PLMs. Limited\nby computational resources, we would apply our\nmethod to larger PLMs in future work.\nAdaptation Tuning Efficiency.Although the adap-\ntation tuning strategy is important to enhance the\nreasoning capability of ReasoningLM, too many\ntuning samples require significant construction and\ntuning costs. Here, we investigate the downstream\nperformance of ReasoningLM on WebQSP and\nCWQ w.r.t.varying numbers of adaptation tuning\nsamples. As shown in Figure 2, we can see that\nthe ReasoningLM can reach a competitive perfor-\nmance compared with the best baseline UniKGQA\nafter adaptation tuning with a few samples ( i.e.,\n5K). It shows that our approach does not require\ntoo much data to complete a successful adaptation\ntuning. Simultaneously, we can observe that as the\nnumber of tuning data increases, our model‚Äôs per-\nformance will improve even further and eventually\nreach a stable state. It indicates that we only need a\nfew tuning examples to achieve a trade-off between\nthe tuning costs and downstream performance.\nFine-tuning Efficiency. As our ReasoningLM\nmodel has become familiar with the multi-hop rea-\nsoning over the subgraph after adaptation tuning,\nit can be easily applied to specific downstream\nKGQA tasks with fewer labeled samples, which\nis meaningful for sparse data scenarios. To ex-\nplore it, we compare the final performance changes\nof our ReasoningLM with two strong baselines\nUniKGQA and NSM w.r.t. the increasing of fine-\ntuning samples with the same retrieval model. We\nconduct experiments using CWQ, which is more\nchallenging and has a larger training set. The re-\nsults are presented on the right of Figure 2. Reason-\ningLM can obtain consistent performance improve-\nments compared with other two baselines under var-\nious numbers of tine-tuning samples. It indicates\nthat our ReasoningLM has a better understanding\nof the answer reasoning over the KG.\n6 Conclusion\nIn this work, we proposed a subgraph reasoning\nenhanced PLM to support question understand-\ning and KG reasoning in a single PLM, namely\nReasoningLM. In our approach, we first adopted\na BFS-based subgraph serialization to enable the\nPLM to capture the structured information and then\nproposed a subgraph-aware self-attention mecha-\nnism to support graph reasoning within the PLM\nbased on the serialized subgraph. In order to adapt\nthe PLM to the special input format and attention\nmechanism, we further utilized an adaptation tun-\ning strategy with a cheap data construction cost\nof 15 dollars by using ChatGPT in an automatic\nway. Finally, we applied the ReasoningLM to solve\ndownstream KGQA tasks with parameter-efficient\nfine-tuning. Experimental results have shown that\nour approach can significantly improve the perfor-\nmance compared to existing strong baselines by\nupdating only 1M parameters.\n7 Limitations\nIn our approach, we propose a subgraph reasoning\nenhanced PLM by adapting existing PLM with-\nout modifying its original architecture. Therefore,\nthe input is usually limited ( e.g., 512) for most\nof PLMs, causing our model unable to process\nthe arbitrary size of the retrieved subgraph. We\ncan relieve it by using the relative position embed-\nding or a better retrieval model to obtain the proper\nsize of the subgraph. In addition, although we\nconduct experiments on multiple KGQA datasets,\nthere is a lack of evaluation on other KG reasoning\n3729\ntasks, such as commonsense question answering\nand knowledge graph completion. They can be\ntransformed into the same input format, which can\nbe potentially solved with our method. From Ta-\nble 4, it can be seen that increasing the model pa-\nrameters will lead to further performance improve-\nment. However, due to limited computational re-\nsources, we did not conduct experiments on larger\nPLMs (e.g., more than 1 Billion parameters).\nAcknowledgments\nThis work was partially supported by Beijing Natu-\nral Science Foundation under Grant No. 4222027,\nNational Natural Science Foundation of China un-\nder Grant No. 62222215, and the Outstanding Inno-\nvative Talents Cultivation Funded Programs 2022\nof Renmin University of China. This work was\nalso supported by Alibaba Group through Alibaba\nInnovative Research Program. Xin Zhao is the cor-\nresponding author.\nReferences\nKurt D. Bollacker, Colin Evans, Praveen K. Paritosh,\nTim Sturge, and Jamie Taylor. 2008. Freebase: a\ncollaboratively created graph database for structuring\nhuman knowledge. In Proceedings of the ACM SIG-\nMOD International Conference on Management of\nData, SIGMOD 2008, Vancouver, BC, Canada, June\n10-12, 2008, pages 1247‚Äì1250. ACM.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nShulin Cao, Jiaxin Shi, Liangming Pan, Lunyiu Nie,\nYutong Xiang, Lei Hou, Juanzi Li, Bin He, and Han-\nwang Zhang. 2022. KQA pro: A dataset with explicit\ncompositional programs for complex question an-\nswering over knowledge base. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2022, Dublin, Ireland, May 22-27, 2022, pages 6101‚Äì\n6119. Association for Computational Linguistics.\nZhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han,\nWei Jin, Haiyang Zhang, Hui Liu, and Jiliang Tang.\n2023. Label-free node classification on graphs\nwith large language models (llms). arXiv preprint\narXiv:2310.04668.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171‚Äì4186. Association for Computational\nLinguistics.\nVijay Prakash Dwivedi and Xavier Bresson. 2020. A\ngeneralization of transformer networks to graphs.\nCoRR, abs/2012.09699.\nGaole He, Yunshi Lan, Jing Jiang, Wayne Xin Zhao, and\nJi-Rong Wen. 2021. Improving multi-hop knowledge\nbase question answering by learning intermediate\nsupervision signals. In WSDM ‚Äô21, The Fourteenth\nACM International Conference on Web Search and\nData Mining, Virtual Event, Israel, March 8-12, 2021,\npages 553‚Äì561. ACM.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. In Pro-\nceedings of the 36th International Conference on Ma-\nchine Learning, ICML 2019, 9-15 June 2019, Long\nBeach, California, USA, volume 97 of Proceedings\nof Machine Learning Research , pages 2790‚Äì2799.\nPMLR.\nDrew A. Hudson and Christopher D. Manning. 2019.\nLearning by abstraction: The neural state machine.\nIn Advances in Neural Information Processing Sys-\ntems 32: Annual Conference on Neural Information\nProcessing Systems 2019, NeurIPS 2019, December\n8-14, 2019, Vancouver, BC, Canada , pages 5901‚Äì\n5914.\nJinhao Jiang, Kun Zhou, Zican Dong, Keming Ye,\nWayne Xin Zhao, and Ji-Rong Wen. 2023. Structgpt:\nA general framework for large language model to\nreason over structured data. CoRR, abs/2305.09645.\nJinhao Jiang, Kun Zhou, Ji-Rong Wen, and Xin Zhao.\n2022a. $great truths are always simple: $ A rather\nsimple knowledge encoder for enhancing the com-\nmonsense reasoning capacity of pre-trained models.\nIn Findings of the Association for Computational Lin-\nguistics: NAACL 2022, Seattle, WA, United States,\nJuly 10-15, 2022, pages 1730‚Äì1741. Association for\nComputational Linguistics.\nJinhao Jiang, Kun Zhou, Wayne Xin Zhao, and Ji-Rong\nWen. 2022b. Unikgqa: Unified retrieval and reason-\ning for solving multi-hop question answering over\nknowledge graph. CoRR, abs/2212.00959.\n3730\nAlexander H. Miller, Adam Fisch, Jesse Dodge, Amir-\nHossein Karimi, Antoine Bordes, and Jason Weston.\n2016a. Key-value memory networks for directly read-\ning documents. In Proceedings of the 2016 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, EMNLP 2016, Austin, Texas, USA, Novem-\nber 1-4, 2016, pages 1400‚Äì1409. The Association for\nComputational Linguistics.\nAlexander H. Miller, Adam Fisch, Jesse Dodge, Amir-\nHossein Karimi, Antoine Bordes, and Jason Weston.\n2016b. Key-value memory networks for directly\nreading documents. In Proceedings of the 2016\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2016, Austin, Texas, USA,\nNovember 1-4, 2016, pages 1400‚Äì1409. The Associ-\nation for Computational Linguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\nder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\n2022. Training language models to follow instruc-\ntions with human feedback. In NeurIPS.\nApoorv Saxena, Adrian Kochsiek, and Rainer Gemulla.\n2022. Sequence-to-sequence knowledge graph com-\npletion and question answering. In Proceedings of\nthe 60th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers), ACL\n2022, Dublin, Ireland, May 22-27, 2022, pages 2814‚Äì\n2828. Association for Computational Linguistics.\nApoorv Saxena, Aditay Tripathi, and Partha P. Taluk-\ndar. 2020. Improving multi-hop question answering\nover knowledge graphs using knowledge base embed-\ndings. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, ACL\n2020, Online, July 5-10, 2020 , pages 4498‚Äì4507.\nAssociation for Computational Linguistics.\nJiaxin Shi, Shulin Cao, Lei Hou, Juanzi Li, and Han-\nwang Zhang. 2021. Transfernet: An effective and\ntransparent framework for multi-hop question an-\nswering over relation graph. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2021, Virtual Event\n/ Punta Cana, Dominican Republic, 7-11 November,\n2021, pages 4149‚Äì4158. Association for Computa-\ntional Linguistics.\nHaitian Sun, Tania Bedrax-Weiss, and William W. Co-\nhen. 2019. Pullnet: Open domain question answering\nwith iterative retrieval on knowledge bases and text.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019, pages 2380‚Äì2390.\nAssociation for Computational Linguistics.\nHaitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn\nMazaitis, Ruslan Salakhutdinov, and William W. Co-\nhen. 2018. Open domain question answering using\nearly fusion of knowledge bases and text. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, Brussels, Belgium,\nOctober 31 - November 4, 2018, pages 4231‚Äì4242.\nAssociation for Computational Linguistics.\nAlon Talmor and Jonathan Berant. 2018. The web as\na knowledge-base for answering complex questions.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2018, New Orleans, Louisiana, USA,\nJune 1-6, 2018, Volume 1 (Long Papers), pages 641‚Äì\n651. Association for Computational Linguistics.\nThomas Pellissier Tanon, Denny Vrandecic, Sebastian\nSchaffert, Thomas Steiner, and Lydia Pintscher. 2016.\nFrom freebase to wikidata: The great migration. In\nProceedings of the 25th International Conference on\nWorld Wide Web, WWW 2016, Montreal, Canada,\nApril 11 - 15, 2016, pages 1419‚Äì1428. ACM.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford al-\npaca: An instruction-following llama model. https:\n//github.com/tatsu-lab/stanford_alpaca.\nPetar Velickovic, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Lio‚Äô, and Yoshua Ben-\ngio. 2017. Graph attention networks. ArXiv,\nabs/1710.10903.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021.\nKEPLER: A unified model for knowledge embed-\nding and pre-trained language representation. Trans.\nAssoc. Comput. Linguistics, 9:176‚Äì194.\nTianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong,\nTorsten Scholak, Michihiro Yasunaga, Chien-Sheng\nWu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Vic-\ntor Zhong, Bailin Wang, Chengzu Li, Connor Boyle,\nAnsong Ni, Ziyu Yao, Dragomir Radev, Caiming\nXiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,\nLuke Zettlemoyer, and Tao Yu. 2022. Unifiedskg:\nUnifying and multi-tasking structured knowledge\ngrounding with text-to-text language models. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, EMNLP 2022,\nAbu Dhabi, United Arab Emirates, December 7-11,\n2022, pages 602‚Äì631. Association for Computational\nLinguistics.\nMichihiro Yasunaga, Hongyu Ren, Antoine Bosselut,\nPercy Liang, and Jure Leskovec. 2021. QA-GNN:\nreasoning with language models and knowledge\ngraphs for question answering. In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2021,\nOnline, June 6-11, 2021, pages 535‚Äì546. Association\nfor Computational Linguistics.\nWen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jian-\nfeng Gao. 2015. Semantic parsing via staged query\n3731\ngraph generation: Question answering with knowl-\nedge base. In Proceedings of the 53rd Annual Meet-\ning of the Association for Computational Linguistics\nand the 7th International Joint Conference on Natu-\nral Language Processing of the Asian Federation of\nNatural Language Processing, ACL 2015, July 26-31,\n2015, Beijing, China, Volume 1: Long Papers, pages\n1321‚Äì1331. The Association for Computer Linguis-\ntics.\nChengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin\nZheng, Guolin Ke, Di He, Yanming Shen, and Tie-\nYan Liu. 2021. Do transformers really perform badly\nfor graph representation? In Advances in Neural\nInformation Processing Systems 34: Annual Confer-\nence on Neural Information Processing Systems 2021,\nNeurIPS 2021, December 6-14, 2021, virtual, pages\n28877‚Äì28888.\nMohamad Zamini, Hassan Reza, and Minou Rabiei.\n2022. A review of knowledge graph completion. Inf.,\n13(8):396.\nJing Zhang, Xiaokang Zhang, Jifan Yu, Jian Tang, Jie\nTang, Cuiping Li, and Hong Chen. 2022. Subgraph\nretrieval enhanced model for multi-hop knowledge\nbase question answering. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), ACL 2022,\nDublin, Ireland, May 22-27, 2022, pages 5773‚Äì5784.\nAssociation for Computational Linguistics.\nYuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexan-\nder J. Smola, and Le Song. 2018. Variational reason-\ning for question answering with knowledge graph. In\nProceedings of the Thirty-Second AAAI Conference\non Artificial Intelligence, (AAAI-18), the 30th inno-\nvative Applications of Artificial Intelligence (IAAI-\n18), and the 8th AAAI Symposium on Educational\nAdvances in Artificial Intelligence (EAAI-18), New\nOrleans, Louisiana, USA, February 2-7, 2018, pages\n6069‚Äì6076. AAAI Press.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\nPeiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A\nsurvey of large language models. CoRR.\n3732\nA Datasets\nFollowing existing work on KGQA (He et al.,\n2021), we conduct experiments on three popular\ndatasets to evaluate our proposed approach, includ-\ning WebQuestionsSP (WebQSP)(Yih et al., 2015),\nComplex WebQuestions 1.1 (CWQ) (Talmor and\nBerant, 2018), and MetaQA (MQA) (Zhang et al.,\n2018).\n‚Ä¢MetaQA (Zhang et al., 2018) comprises over\n400,000 questions in the movie domain, with an-\nswer entities located up to three hops away from\nthe topic entities. Based on the number of hops, the\ndataset is divided into three sub-datasets: MetaQA-\n1hop, MetaQA-2hop, and MetaQA-3hop. Existing\nwork has demonstrated that the training data for\nMetaQA is more than sufficient (He et al., 2021),\nhence all the comparison methods in our exper-\niments can achieve very high performance. We\nrandomly sample just one training case for each\nquestion template from the original training set, to\nform a one-shot training dataset following existing\nwork (He et al., 2021; Jiang et al., 2022b). In this\nway, the numbers of training samples for MetaQA-\n1hop, MetaQA-2hop, and MetaQA-3hop are 161,\n210, and 150, respectively.\n‚Ä¢ WebQuestionsSP (WebQSP) (Yih et al.,\n2015) consists of 4,737 questions. The answer\nentities are within a maximum of 2 hops from the\ntopic entity on the Freebase (Bollacker et al., 2008)\nKG. We adopt the train/valid/test splits from Graft-\nNet (Sun et al., 2018) for consistency.\n‚Ä¢Complex WebQuestions 1.1 (CWQ) (Talmor\nand Berant, 2018) is constructed based on WebQSP,\nwhich is more challenging. It complicates WebQSP\nby extending the question entities or adding con-\nstraints to restrict the answers. The answer entities\nare within a maximum of 4 hops from the topic\nentity on the Freebase (Bollacker et al., 2008) KG.\nExisting work (He et al., 2021) has demonstrated\nthat the training data for MetaQA is more than\nsufficient. To better reflect the reasoning capability\nof baselines and our method, we extract one sample\nfor each question template and conduct the one-\nshot experiment on all three MetaQA sub-datasets\nfollowing existing work (He et al., 2021). Table 5\nshows the statistics of the three datasets.\nB Baselines\nWe consider the following three types of base-\nline methods for performance comparison: (1)\nnon PLM-based methods: KV-Mem (Miller et al.,\nTable 5: Statistics of the experiment datasets.\nType Task KG Train Dev Test\nKGQA\nWebQSP Freebase 2,848 250 1,639\nCWQ Freebase 27,639 3,519 3,531\nMQA-1H OMDb 161 9,992 9,947\nMQA-2H OMDb 210 14,872 14,872\nMQA-3H OMDb 150 14,274 14,274\n2016b), GraphtNet (Sun et al., 2018), Pull-\nNet (Sun et al., 2019), NSM (He et al., 2021);\n(2) PLM-based methods: EmbedKGQA (Sax-\nena et al., 2020), TransferNet (Shi et al.,\n2021), SR+NSM+E2E (Zhang et al., 2022),\nUniKGQA (Jiang et al., 2022b); (3) LLM-based\nmethods: Davinci-003 (Ouyang et al., 2022), Chat-\nGPT, StructGPT (Jiang et al., 2023). We give a\ndetailed description of each baseline:\n‚Ä¢KV-Mem (Miller et al., 2016b) employs a key-\nvalue memory table to store KG facts and facilitates\nmulti-hop reasoning through iterative read opera-\ntions on the memory.\n‚Ä¢GraftNet (Sun et al., 2018) first utilize a\nheuristic method to retrieve the question-relevant\nsubgraph and text sentences from the Knowledge\nGraph (KG) and Wikipedia, respectively. Subse-\nquently, it employs a graph neural network to con-\nduct multi-hop reasoning on a heterogeneous graph\nconstructed from the subgraph and text sentences.\n‚Ä¢PullNet (Sun et al., 2019) trains a graph re-\ntrieval model instead of the heuristic way in Graft-\nNet for the retrieval task, and then conducts multi-\nhop reasoning with GraftNet.\n‚Ä¢NSM (He et al., 2021) first conducts retrieval\nfollowing GraftNet and then adapts the neural state\nmachine (Hudson and Manning, 2019) used in vi-\nsual reasoning for multi-hop reasoning on the KG.\nIt consists of a question understanding module\nbased on LSTM and a graph reasoning module\nwith the adapted neural state machine, which is a\ngraph neural network in essence.\n‚Ä¢EmbedKGQA (Saxena et al., 2020) trans-\nforms the multi-hop reasoning process of Graft-\nNet into a link prediction task. This is achieved\nby comparing pre-trained entity embeddings with\nquestion representations derived from a Pre-trained\nLanguage Model (PLM).\n‚Ä¢TransferNet (Shi et al., 2021) first conducts\nretrieval following GraftNet and then performs the\nmulti-hop reasoning on a KG or a text-formed rela-\ntion graph in a transparent framework. It consists\nof a PLM for question encoding and a graph neural\n3733\nnetwork for updating the relevance scores between\nentities and the question.\n‚Ä¢ SR+NSM+E2E (Zhang et al., 2022) first\nlearns a PLM-based relation path retriever to con-\nduct effective retrieval and then leverages NSM rea-\nsoner to perform multi-hop reasoning. The whole\nprocess is optimized in an end-to-end way.\n‚Ä¢UniKGQA (Jiang et al., 2022b) is a unified\nmodel architecture based on PLMs for both re-\ntrieval and reasoning stages. It consists of PLM\nfor computing the semantic similarity between the\nquestion and relation and a simple graph neural\nnetwork for propagating the matching information.\n‚Ä¢Davinci-003 (Ouyang et al., 2022) and Chat-\nGPT are both large language models developed by\nOpenAI. We can use their provided APIs to access\nthem and solve KGQA tasks.\n‚Ä¢StructGPT (Jiang et al., 2023) is a general\nframework for improving the zero-shot reason-\ning ability of LLMs over structured data, such as\nKnowledge Graph. It use an invoking-linearization-\ngeneration procedure that leverages LLMs to read\nand perform reasoning based on the interface of\nstructured data.\nC Ablation Study of Retrieval Subgraphs\nWe conduct experiments on two strong base-\nlines (NSM and UniKGQA) with our retrieval sub-\ngraphs to explore the effect of the retrieval stage.\nWe show the results in Table 6. We can see that\nthe two baselines achieve consistent performance\nimprovement with our retrieved subgraphs (63.43%\nHits@1 of UniKGQA w Ours v.s. 50.7% Hits@1\nof UniKGQA and 61.9% Hits@1 of NSM w Ours\nv.s. 47.6% Hits@1 of NSM). It indicates that our\nmodel can achieve a better retrieval compared to ex-\nisting retrieval methods. Although enhanced with\nour retrieval methods, the performance of the two\nbaselines still have a great gap with our Reason-\ningLM. This demonstrates the effectiveness of our\nReasoningLM to perform multi-hop reasoning over\nthe subgraph.\nD Prompt for ChatGPT\nInspired by existing work (Taori et al., 2023), we\nshow the prompt of generating questions used by\nChatGPT in Table 7.\n3734\nTable 6: Performance of ReasoningLM and two strong baselines (i.e., NSM and UniKGQA) on CWQ based on our\nretrieval subgraphs represented by ‚Äúw Ours‚Äù.\nCWQ ReasoningLM NSM NSM w Ours UniKGQA UniKGQA w Ours\nHits@1 69.0 47.6 61.9 50.7 63.43\nF1 64.0 42.4 50.1 48.0 57.65\nHere are the guidelines for formulating a question based on the given factual triples, object of the question,\nand the answer:\n1. A factual triple consists of a head entity, a tail entity, and their relationship, representing a real-world fact.\nFor example, (Gino Finizio, sex or gender, male) indicates that Gino Finizio is male.\n2. Your question should pertain to the provided entity based on the factual background, and the answer\nprovided should align with the provided answer.\n3. The question should include all of the given information as constraints, except for the provided answer, to\nensure that all provided information is fully considered in deriving the answer.\n4. Utilize as many different entities and relations as possible in the question to promote variety.\n5. Questions should generally be one to two sentences and require no added content aside from the question\nitself.\n6. Use subordinate clauses to link multiple triples in the question, excluding intervening entities when pos-\nsible. For example, the knowledge ‚Äú(Elevator Action, platform, Commodore 64), (Commodore 64, Giphy\nusername, commodore)‚Äù can be conveyed as ‚ÄúWhat is the Giphy username for the platform of Elevator Ac-\ntion?‚Äù\nWe provide a set of 4 examples that you can reference:\nExample 1:\nGiven the factual background: (Euler-Lagrange equation, discoverer or inventor, Leonhard Euler), (Leonhard\nEuler, student, Mikhail Golovin). Please generate a question about the ‚ÄúEuler-Lagrange equation‚Äù and the\nanswer to the question should be ‚ÄúMikhail Golovin‚Äù.\nThe question is: Who is the student that coined the Euler-Lagrange equation?\nExample 2:\nGiven the factual background: (Spokane, population, 208,916), (Spokane, point in time, 2007). Please generate\na question about ‚ÄúSpokane‚Äù and the answer to the question should be ‚Äú208,916‚Äù.\nThe question is: In 2007, what is the population of Spokane?\nExample 3:\nGiven the factual background: (Kristen Stewart, place of birth, Los Angeles), (Los Angeles, capital of, Los\nAngeles County). Please generate a question about ‚ÄúKristen Stewart‚Äù and the answer to the question should\nbe ‚ÄúLos Angeles‚Äù.\nThe question is: What is the birth city of Kristen Stewart, which has the county seat of Los Angeles County?\nExample 4:\nGiven the factual background: (Lampedusa Airport, country, Italy), (Italy, capital, Rome), (Italy, start time, 2\nJune 1946). Please generate a question about ‚ÄúLampedusa Airport‚Äù and the answer to the question should be\n‚ÄúRome‚Äù.\nThe question is: Starting in 1946, what was the capital of the country to which Lampedusa Airport belonged?\nTable 7: Instruction of generating questions in our adaptation-tuning strategy.\n3735",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7761785984039307
    },
    {
      "name": "Question answering",
      "score": 0.7327771186828613
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.5965203642845154
    },
    {
      "name": "Knowledge graph",
      "score": 0.5680783987045288
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4967518448829651
    },
    {
      "name": "Graph",
      "score": 0.4936600625514984
    },
    {
      "name": "Language model",
      "score": 0.492615282535553
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.43548229336738586
    },
    {
      "name": "Natural language processing",
      "score": 0.32530534267425537
    },
    {
      "name": "Machine learning",
      "score": 0.3223216235637665
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3014747202396393
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210096250",
      "name": "Beijing Institute of Big Data Research",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I78988378",
      "name": "Renmin University of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210095624",
      "name": "Alibaba Group (United States)",
      "country": "US"
    }
  ],
  "cited_by": 16
}