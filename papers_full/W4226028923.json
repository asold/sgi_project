{
  "title": "When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism",
  "url": "https://openalex.org/W4226028923",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2115323275",
      "name": "Guangting Wang",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2102282813",
      "name": "Yucheng Zhao",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2223267499",
      "name": "Chuanxin Tang",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2124221993",
      "name": "Chong Luo",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2134447754",
      "name": "Wenjun Zeng",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2223267499",
      "name": "Chuanxin Tang",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2124221993",
      "name": "Chong Luo",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2134447754",
      "name": "Wenjun Zeng",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2922008600",
    "https://openalex.org/W6676297131",
    "https://openalex.org/W6746975906",
    "https://openalex.org/W6762215367",
    "https://openalex.org/W2902930830",
    "https://openalex.org/W2809048190",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2901751978",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6746034047",
    "https://openalex.org/W2769427605",
    "https://openalex.org/W6777239595",
    "https://openalex.org/W2507296351",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3034885317",
    "https://openalex.org/W2990152177",
    "https://openalex.org/W4394645241",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W4287324101",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W4287119852",
    "https://openalex.org/W2964080601",
    "https://openalex.org/W3181925591",
    "https://openalex.org/W3040304705",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W4287203089",
    "https://openalex.org/W2963589041",
    "https://openalex.org/W4394668313",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W2982619380",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W4287073339",
    "https://openalex.org/W2963844898",
    "https://openalex.org/W3183804933",
    "https://openalex.org/W2108598243"
  ],
  "abstract": "Attention mechanism has been widely believed as the key to success of vision transformers (ViTs), since it provides a flexible and powerful way to model spatial relationships. However, is the attention mechanism truly an indispensable part of ViT? Can it be replaced by some other alternatives? To demystify the role of attention mechanism, we simplify it into an extremely simple case: ZERO FLOP and ZERO parameter. Concretely, we revisit the shift operation. It does not contain any parameter or arithmetic calculation. The only operation is to exchange a small portion of the channels between neighboring features. Based on this simple operation, we construct a new backbone network, namely ShiftViT, where the attention layers in ViT are substituted by shift operations. Surprisingly, ShiftViT works quite well in several mainstream tasks, e.g., classification, detection, and segmentation. The performance is on par with or even better than the strong baseline Swin Transformer. These results suggest that the attention mechanism might not be the vital factor that makes ViT successful. It can be even replaced by a zero-parameter operation. We should pay more attentions to the remaining parts of ViT in the future work. Code is available at github.com/microsoft/SPACH.",
  "full_text": "When Shift Operation Meets Vision Transformer:\nAn Extremely Simple Alternative to Attention Mechanism\nGuangting Wang1*, Yucheng Zhao1*, Chuanxin Tang2*, Chong Luo2 , Wenjun Zeng2\n1 University of Science and Technology of China 2 Microsoft Research Asia\n{flylight, lnc}@mail.ustc.edu.cn, {chutan, cluo}@microsoft.com, zengw2011@hotmail.com\nAbstract\nAttention mechanism has been widely believed as the key\nto success of vision transformers (ViTs), since it provides\na flexible and powerful way to model spatial relationships.\nHowever, is the attention mechanism truly an indispensable\npart of ViT? Can it be replaced by some other alternatives?\nTo demystify the role of attention mechanism, we simplify\nit into an extremely simple case: ZERO FLOP and ZERO\nparameter. Concretely, we revisit the shift operation. It does\nnot contain any parameter or arithmetic calculation. The only\noperation is to exchange a small portion of the channels\nbetween neighboring features. Based on this simple opera-\ntion, we construct a new backbone network, namely ShiftViT,\nwhere the attention layers in ViT are substituted by shift op-\nerations. Surprisingly, ShiftViT works quite well in several\nmainstream tasks, e.g., classification, detection, and segmen-\ntation. The performance is on par with or even better than\nthe strong baseline Swin Transformer. These results suggest\nthat the attention mechanism might not be the vital factor that\nmakes ViT successful. It can be even replaced by a zero-\nparameter operation. We should pay more attentions to the\nremaining parts of ViT in the future work. Code is available\nat github.com/microsoft/SPACH.\nIntroduction\nDesigning backbone networks plays a fundamental role\nin computer vision. Since the revolutionary progress of\nAlexNet (Krizhevsky, Sutskever, and Hinton 2012), convo-\nlution neural networks (CNNs) have dominated this area\nfor nearly 10 years. However, the recently developed Vision\nTransformers (ViTs) have shown potential to challenge this\nthrone. The advantage of ViT was first demonstrated in im-\nage classification task (Dosovitskiy et al. 2020), where the\nViT backbone outperforms its CNN counterparts by a re-\nmarkable margin. Thanks to the promising results, the flour-\nish of ViT variants rapidly broadcasts to many other com-\nputer vision tasks, such as object detection, semantic seg-\nmentation, and action recognition.\nDespite the impressive performances of recent ViT vari-\nants, it is still not yet clear what makes ViT good for vi-\n*These authors contributed equally.\n†This work was done during the internship of Guangting and\nYucheng at MSRA\nCopyright © 2022, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nAttention\n Feed-forward\nnetwork\nShift\n Feed-forward\nnetwork\n(a) Standard attention building block.\n(b) Our shift building block.\nHeight\nWidth\nChannel\nShift\nFigure 1: An illustration of our shift building block. We pro-\npose to replace the attention layer with a simple shift oper-\nation in vision transformers. It spatially shifts a small por-\ntion of the channels along four directions, and the rest of the\nchannels remain unchanged.\nsual recognition tasks. Some conventional wisdom leans to\ncredit the success to the attention mechanism, since it pro-\nvides a flexible and powerful way to model spatial relation-\nships. Concretely, the attention mechanism leverages a self-\nattention matrix to aggregate features from arbitrary loca-\ntions. Compared with the convolution operation in CNN, it\nhas two significant strengths. First, this mechanism opens a\npossibility to simultaneously capture both short- and long-\nranged dependencies, and get rid of the local restriction of\nthe convolution. Second, the interaction between two spatial\nlocations dynamically depends on their own features, rather\nthan a fixed convolutional kernel. Due to such good proper-\nties, some pieces of work believe it is the attention mecha-\nnism that facilitates the powerful expressive ability of ViTs.\nHowever, are these two advantages truly the key to suc-\ncess? The answer is probably NOT. Some existing work\nproves that, even without these properties, the ViT variants\ncan still work well. For the first one, the fully-global depen-\ndencies may not be inevitable. More and more ViTs intro-\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n2423\nduce a local attention mechanism to restrict their attention\nscope within a small local region, e.g., Swin Transformer\n(Liu et al. 2021b) and Local ViT (Li et al. 2021). The exper-\niments show that the performance does not drop due to the\nlocal restriction. Besides, another line of research investi-\ngates the necessity of the dynamic aggregation. MLP-Mixer\n(Tolstikhin et al. 2021) proposes to substitute the attention\nlayer with a linear projection layer, where the linear weights\nare not dynamically generated. In this case, it can still reach\na leading performance on the ImageNet dataset.\nNow that both global and dynamic properties might not be\ncrucial for the ViT framework, what is the essential reason\nfor the success of ViT? To figure it out, we further simplify\nthe attention layer into an extremely simple case:NO global\nscope, NO dynamics, and even NO parameter and NO arith-\nmetic calculation. We desire to know whether ViT can retain\nthe good performance under this extreme case.\nConceptually, this zero-parameter alternative must rely on\nthe handcrafted rule to model spatial relationships. In this\nwork, we revisit the shift operation, which we believe is one\nof the simplest spatial modeling module. As depicted in Fig-\nure 1, the standard ViT building block consists of two parts:\nthe attention layer and the feed-forward network (FFN). We\nreplace the former attention layer with a shift operation,\nwhile keeping the latter FFN part untouched. Given an input\nfeature, the proposed building block will first shift a small\nportion of the channels along four spatial directions, namely\nleft, right, top, and down. As such, the information of neigh-\nboring features is explicitly mingled by the shifted channels.\nThen, the subsequent FFN performs channel-wise mixing to\nfurther fuse the information from neighbors.\nBased on this shift building block, we construct a ViT-like\nbackbone network, namely ShiftViT. Surprisingly, this back-\nbone can also work well for the mainstream visual recog-\nnition tasks. The performance is on par with or even bet-\nter than the strong Swin Transformer baseline. Concretely,\nwithin the same computational budgets as Swin-T model,\nour ShiftViT achieves a top-1 classification accuracy of\n81.7% (against Swin-T’s 81.3%) on ImageNet dataset. For\nthe dense prediction task, it attains a mean average precision\n(mAP) score of 45.7% (against Swin-T’s 43.7%) on COCO\ndetection dataset, and a mean IoU (mIoU) score of 46.3%\n(against Swin-T’s 44.5%) on ADE20k segmentation dataset.\nSince the shift operation is already the simplest spatial\nmodelling module, the excellent performance must come\nfrom the remaining components, e.g., the linear layers and\nthe activation function in FFN. These components are less\nstudied in existing work, because they look trivial. However,\nto further demystify the reasons why ViT works, we argue\nthat we should pay more attentions to these components, in-\nstead of just focusing on the attention mechanism. We hope\nour work can shed a new light on the ViT research. As a\nsummary, the contributions of this work are two folds:\n• We present a ViT-like backbone, where the vanilla atten-\ntion layer is replaced by an extremely simple shift op-\neration. The proposed model can achieve an even better\nperformance than Swin Transformer.\n• We analyze the reasons behind the success of ViTs. It\nhints that the attention mechanism might not be the vital\nfactor that makes ViT work. We should take the remain-\ning components seriously in the future study of ViTs.\nRelated Work\nAttention and Vision Transformers\nTransformer architecture (Vaswani et al. 2017) is first in-\ntroduced in the area of natural language processing (NLP).\nIt solely adopts attention mechanism to build the connec-\ntions between different language tokens. Thanks to the great\nperformance, Transformers have rapidly dominated the NLP\narea and become the de factostandard.\nInspired by the successful application in NLP, attention\nmechanism has also received increasing interests from the\ncomputer vision community. The early explorations can be\nroughly divided into two categories. On the one hand, some\nliterature considers attention as a plug-and-play module,\nwhich can be seamlessly integrated into the existing CNN ar-\nchitectures. The representative work includes non-local net-\nwork (Wang et al. 2018), relation network (Hu et al. 2018),\nand CCNet (Huang et al. 2019). On the other hand, some\npieces of work aim to substitute all convolution operations\nwith the attention mechanism, such as local relation network\n(Hu et al. 2019) and self-attention network (Zhao, Jia, and\nKoltun 2020).\nAlthough these two kinds of work have shown promis-\ning results, they are still built on the CNN architecture. ViT\n(Dosovitskiy et al. 2020) is the pioneering work that lever-\nages a pure transformer architecture for visual recognition\ntasks. Thanks to its impressive performance, the community\nrecently bursts out a rising wave of research on vision trans-\nformers. Along this line of research, the main focus is to\nimprove the attention mechanism, so that it can satisfy the\nintrinsic properties of visual signals. For example, MSViT\n(Fan et al. 2021) builds hierarchical attention layers to obtain\nmulti-scale features. Swin Transformers (Liu et al. 2021b)\nintroduces a locality constrain into its attention mechanism.\nThe related efforts also include pyramid attention (Wang\net al. 2021), local-global attention (Li et al. 2021), cross at-\ntention (Chen, Fan, and Panda 2021), to name a few.\nUnlike the particular interests in attention mechanism, the\nremaining components of ViT are less studies. DeiT (Tou-\nvron et al. 2020) has setup a standard training pipeline for vi-\nsion transformers. Most follow-up work inherits its setting,\nand only make some modifications on the attention mech-\nanism. Our work also follows this paradigm. However, the\ngoal of this work is not to complex the design of attention.\nOn the contrary, we aim to show that the attention mecha-\nnism might not be the critical part of making ViTs work. It\ncan be even replaced by an extremely simple shift operation.\nWe hope these results can inspire researchers to rethink the\nrole of attention mechanism.\nMLP Variants\nOur work is related to the recent multi-layer-perceptron\n(MLP) variants. Specifically, MLP variants propose to ex-\ntract image features through a pure MLP-like architecture.\nThey also jump out of the attention-based framework in\n2424\nInput \nimage\nPatch Partition\nLinear embedding\nShift\nblock\nPatch Merging\nShift\nblock\nPatch Merging\nShift\nblock\nPatch Merging\nShift\nblock\nStage 1 Stage 2 Stage 3 Stage 4\nShift\nLN\nMLP\n(a) overall architecture (b) shift block\nFigure 2: (a) The overall architecture of our ShiftViT. We follow Swin Transformer (Liu et al. 2021b) to build hierarchical\nrepresentations. (b) The detail design of a shift block. We only use a simple shift operation to model spatial relationships.\nViT. For example, instead of using the self-attention ma-\ntrix, MLP-Mixer (Tolstikhin et al. 2021) introduces a token-\nmixing MLP to directly connect all spatial locations. It elim-\ninates the dynamic property of ViT, but without losing accu-\nracy. The follow-up work investigates more MLP designs,\nlike the spatial gating unit (Liu et al. 2021a) or cyclic con-\nnection (Chen et al. 2021).\nOur ShiftViT can be also categorized into the pure MLP\narchitecture, where the shift operation is viewed as a spe-\ncial token-mixing layer. Compared with the existing MLP\nwork, our shift operation is even much simpler, since it con-\ntains no parameter and no FLOP. Moreover, the vanilla MLP\nvariants fail to handle variable input size because of the fixed\nlinear weights. Our shift operation overcomes this obstacle\nand therefore make the backbone feasible for more vision\ntasks like object detection and semantic segmentation.\nShift Operation\nShift operation is not new in computer vision. As early as\nin 2017, it was proposed to be an efficient alternative to the\nspatial convolution operation (Wu et al. 2018). Concretely, it\nuses a sandwich-like architecture, two1×1 convolutions and\na shift operation, to approximate a K × K convolution. In\nthe follow-up work, the shift operation is further extended\ninto different variants, such as active shift (Jeon and Kim\n2018), sparse shift (Chen et al. 2019) and partial shift (Lin,\nGan, and Han 2019).\nIn this work, we adopt the partial shift operation (Lin,\nGan, and Han 2019). It is notable that the goal of this work\nis not to present a novel operation. Instead of that, we inte-\ngrate the existing shift operation with the popular ViT to ver-\nify the effectiveness of attention mechanism. The similar vi-\nsion are shared with the concurrent work ShiftMLP (Yu et al.\n2021) and AS-MLP (Lian et al. 2021), but the design details\nare quite different. Their building blocks are more complex,\nwhich involve some auxiliary layers like pre-transformation\nand post-transformation.\nShift Operation Meets Vision Transformer\nArchitecture Overview\nFor a fair comparison, we follow the architecture of Swin\nTransformer (Liu et al. 2021b). The architecture overview is\nillustrated in Figure 2 (a). Specifically, given an input image\nof shape H × W × 3, it first splits the images into non-\noverlapping patches. The patch size is 4 × 4 pixels. There-\nfore, the output of patch partition is is H\n4 × W\n4 tokens, where\neach token has a channel size of 48.\nThe modules followed by can be divided into 4 stages.\nEach stage contains two parts: embedding generation and\nstacked shift blocks. For the embedding generation of the\nfirst stage, a linear projection layer is used to map each token\ninto an embedding of channel size C. For the rest stages, we\nmerge neighbouring patches through the convolution with a\nkernel size of 2 × 2. After patch merging, the spatial size of\nthe output is half down-sampled, while channel size is twice\nthe input, i.e., from C to 2C.\nThe stacked shift block is built by some repeated basic\nunits. The detail design of each shift block is shown in Fig-\nure 2 (b). It composes of a shift operation, a layer normaliza-\ntion and a MLP network. This design is almost the same as\nthe standard transformer block. The only difference is that\nwe use a shift operation rather than a attention layer. For\neach stage, the number of shift blocks can be various, which\nis denoted asN1, N2, N3, N4 respectively. In our implemen-\ntation, we carefully choose the value ofNi so that the overall\nmodel share a similar number of parameters with the base-\nline Swin Transformer model.\nShift Block\nThe detail architecture of our shift block is depicted in\nFigure 2 (b). Specifically, this block consists of three\nsequentially-stacked components: shift operation, layer nor-\nmalization and MLP network.\nShift operation has been well studied in CNNs. It can have\nmany design choices, such as active shift (Jeon and Kim\n2018) and sparse shift (Chen et al. 2019). In this work, we\nfollow the partial shift operation in TSM (Lin, Gan, and Han\n2019). The illustration is presented in Figure 1 (b). Given an\n2425\ninput tensor, a small portion of channels will be shifted along\n4 spatial directions, namely left, right, top, and down, while\nthe remaining channels keep unchanged. After shifting, the\nout-of-scope pixels are simply dropped and the vacant pixels\nare zero padded. In this work, the shift step is set to 1 pixel.\nFormally, we assume that the input feature z is of shape\nH × W × C, where C is the number of channels, H and W\nare spatial height and width, respectively. The output feature\nˆz has the same shape as input. It can be written as:\nˆz[0 :H, 1 :W,0 :γC] ←z[0 :H, 0 :W −1, 0 :γC]\nˆz[0 :H, 0 :W −1, γC: 2γC] ←z[0 :H, 1 :W, γC: 2γC]\nˆz[0 :H −1, 0 :W,2γC : 3γC] ←z[1 :H, 0 :W,2γC : 3γC]\nˆz[1 :H, 0 :W,3γC : 4γC] ←z[0 :H −1, 0 :W,3γC : 4γC]\nˆz[0 :H, 0 :W,4γC : C] ←z[0 :H, 0 :W,4γC : C]\nwhere γ is a ratio factor to control how many percentages of\nchannels will be shifted. In most experiments, the value ofγ\nis set to 1\n12 .\nIt is notable that shift operation does not hold any param-\neter or arithmetic calculation. The only implementation is\nmemory copying. Therefore, shift operation is highly effi-\ncient and it is very easy to implement. The pseudo code is\npresented in Algorithms 1. Compared with the self-attention\nmechanism, shift operation is clean, neat, and more friendly\nto deep learning inference library like TensorRT.\nThe rest of the shift block is the same as the standard\nbuilding block of ViT. The MLP network has two linear lay-\ners. The first one increases the channel of the input feature\nto a higher dimension, e.g., from C to τC . Then the sec-\nond linear layer projects the high-dimensional feature into\nthe original channel size ofC. Between these two layers, we\nadopt GELU as the non-linear activation function.\nArchitecture Variants\nFor a fair comparison with the baseline Swin Transformer,\nwe also build multiple models with various number of pa-\nrameters and computational complexity. Specifically, we in-\ntroduce Shift-T(iny), Shift-S(mall), Shift-B(ase) variants 1,\nwhich is corresponded to Swin-T, Swin-S and Swin-B, re-\nspectively. Shift-T is the smallest one, which shares a simi-\nlar size with Swin-T and ResNet-50. Another two variants,\nShift-S and Shift-B, are roughly 2× and 4× more complex\nthan ShiftViT-T. The detail configurations of basic embed-\nding channels C and number of blocks {Ni} are presented\nas following:\n• Shift-T: C = 96,{Ni} = {6, 8, 18, 6}, γ = 1/12\n• Shift-S: C = 96,{Ni} = {10, 18, 36, 10}, γ = 1/12\n• Shift-B: C = 128,{Ni} = {10, 18, 36, 10}, γ = 1/16\nBeside the model size, we also have a closer look at the\nmodel depth. In our proposed model, nearly all parameters\nare concentrated in the MLP part. Therefore, we can control\nthe expand ratio of MLPτ to obtain a deeper network depth.\nIf not specified, the expand ratio τ is set to 2. We have an\nablation analysis to show that the deeper model achieve a\nbetter performance.\n1For simplification, we ignore the suffix of “ViT” and use Shift-\nT to denote ShiftViT-T in this work.\nAlgorithm 1: Pytorch-like pseudo code of shift\n1 def shift(x, gamma=1/12):\n2 # x is a tensor with a shape of\n3 # [Batch, Channel, Height, Width]\n4 B, C, H, W = x.shape\n5 g = int(gamma * C)\n6 out = zeros_like(x)\n7 # spatially shift\n8 out[:,0*g:1*g,:,:-1]= x[:,0*g:1*g,:,1:]\n9 out[:,1*g:2*g,:,1:]= x[:,1*g:2*g,:,:-1]\n10 out[:,2*g:3*g,:-1,:]= x[:,2*g:3*g,1:,:]\n11 out[:,3*g:4*g,1:,:]= x[:,3*g:4*g,:-1,:]\n12 # remaining channels\n13 out[:,4*g:,:,:]= x[:,4*g:,:,:]\n14 return out\nExperiments\nImplementation Details\nWe conduct experiments on three mainstream visual recog-\nnition benchmarks: image classification on ImageNet-1k\ndataset (Deng et al. 2009), object detection on COCO dataset\n(Lin et al. 2014) and semantic segmentation on ADE20k\ndataset (Zhou et al. 2019).\nFor image classification task, we exactly follow the pro-\ntocol as in Swin Transformer (Liu et al. 2021b). An average\npooling layer and a linear classification layer are appended\nafter the backbone network. All the parameters are randomly\ninitialized and trained for 300 epochs with an AdamW op-\ntimizer. The learning rate starts from 0.001 and gradually\ndecay to 0 with a cosine schedule. We include all data aug-\nmentations and regularization tricks as in Swin Transformer\n(Liu et al. 2021b). The batch size is set to 1024.\nFor object detection task, there exists many off-the-shelf\ndetection frameworks, such as Faster R-CNN, Mask R-CNN\nand RetinaNet. For a fair comparison with other methods,\nwe follow the common practice of using Mask R-CNN and\nCascade Mask R-CNN. In such detection frameworks, the\nbackbone is our proposed Shift network, while the rest of\ncomponents like FPN and detection head remain the same.\nWe initialize the backbone with pretrained weights of the\nImageNet-1k classifier. The training duration lasts for 12\nepochs (denoted as 1× schedule) or 36 epochs (denoted\nas 3× schedule). The optimizer is AdamW, with an initial\nlearning rate 0.0001. The batch size is 16. During train-\ning period, we utilize the multi-scale training trick, i.e., the\nshorter side of the input image is resized into a range from\n480 pixels to 800 pixels. We report the mean average preci-\nsion (mAP) metrics on the validation set of COCO dataset.\nFor semantic segmentation task, we evaluate our method\non ADE20K dataset, which contains 20K images for training\nand 2K images for validation. In these experiments, the base\nsegmentation framework is UperNet. The model is trained\non the training set of ADE20K and the evaluation metric is\nthe mean IoU (mIoU) score on the validation set. Similar to\nthe setting of object detection, our Shift backbones are also\npretrained on ImageNet-1k. The rest of settings are same as\nSwin-Transformer. The training batch size is 16 and we train\nthe model for 160k iterations. For the comparison with the\nstate-of-the-arts, we adopt the multi-scale testing strategy.\n2426\nModel Param\n(M)\nImageNet COCO ADE20k\nFLOPs Speed Top-1 Mask R-CNN 1× Mask R-CNN 3× UpperNet\n(G) (FPS) Acc.(%) APb APm APb APm mIoU\nResNet-50 26 4.1 676 76.1 38.0 34.4 41.0 37.1 -\nSwin-T 29 4.5 356 81.3 43.7 39.5 46.0 41.6 44.5\nShift-T/light 20 3.0 790 79.4 41.3 38.0 43.2 39.2 42.6\nShift-T 29 4.5 396 81.7 (+0.4) 45.4 (+1.7) 40.9 (+1.4) 47.1 (+1.1) 42.3 (+0.7) 46.3 (+1.8)\nSwin-S 50 8.7 217 83.0 46.4 41.7 48.5 43.3 47.6\nShift-S/light 34 5.7 457 81.6 44.8 40.4 46.0 41.1 45.4\nShift-S 50 8.8 215 82.8 (-0.2) 47.2 (+0.8) 42.2 (+0.5) 48.6 (+0.1) 43.4 (+0.1) 47.8 (+0.2)\nSwin-B 88 15.4 158 83.5 46.9 42.1 48.7 43.4 48.1\nShift-B/light 60 10.2 312 82.3 45.7 41.0 46.0 41.2 45.8\nShift-B 89 15.6 154 83.3 (-0.2) 47.7 (+0.8) 42.7 (+0.6) 48.0 (-0.7) 42.8 (-0.6) 47.9 (-0.2)\nTable 1: Comparison with the baseline Swin Transformer on three mainstream tasks: image classification, object detection and\nsemantic segmentation. The suffix /light denotes the lightweight version of our ShiftViT, where we only replace attention\nlayers with the shift operation and keep remaining parts unchanged. The throughput speed is evaluated on a single NVidia\nGTX1080-Ti GPU.\nModel Input # Params FLOPs Top-1\nresolution (M) (B) Acc. (%)\nCNN-based\nRegNetY-4G 2242 21 4.0 80.0\nRegNetY-8G 2242 39 8.0 81.7\nRegNetY-16G 2242 84 16.0 82.9\nEfficientNet-B4 3802 19 4.2 82.9\nEfficientNet-B5 4562 30 9.9 83.6\nEfficientNet-B6 5282 43 19.0 84.0\nViT-based and MLP-based\nDeiT-S 2242 22 4.6 79.8\nDeiT-B 2242 86 17.5 81.8\nPVT-S 2242 25 3.8 79.8\nPVT-L 2242 61 9.8 81.7\nSwin-T 2242 29 4.5 81.3\nSwin-S 2242 50 8.7 83.0\nSwin-B 2242 88 15.4 83.5\nMLP-Mixer-B/16 2242 79 - 76.4\ngMLP-S 2242 20 4.5 79.4\ngMLP-B 2242 73 15.8 81.6\nS2-MLP-D 2242 71 14.0 80.0\nS2-MLP-W 2242 51 10.5 80.7\nAS-MLP-T 2242 28 4.4 81.3\nAS-MLP-S 2242 50 8.5 83.1\nAS-MLP-B 2242 88 15.2 83.3\nOurs\nShift-T 2242 28 4.4 81.7\nSfhit-S 2242 50 8.5 82.8\nSfhit-B 2242 88 15.2 83.3\nTable 2: Comparison with state-of-the-art methods on the\nImageNet-1k classification task.\nComparison with Baseline\nThe goal of this work is to demystify the role of attention\nmechanism and explore whether it can be replaced by an\nextremely simple shift operation. Concretely, our proposed\nbackbones are based on the architecture of Swin Trans-\nformer, which is one of the most representative ViT vari-\nants. We therefore consider Swin Transformer as the base-\nline model, and compare our ShiftViT to it.\nFor an apple-to-apple comparison, we first build a\nlightweight version of ShiftViT. It is nearly the same as the\nSwin Transformer counterpart, except that the attention lay-\ners are substituted by the shift operations. We denote this\nbackbone with a suffix/light, because replacing attention\nwith shift will lead to a reduction in parameters and FLOPs.\nThe experimental results are presented in Table 1. We ex-\nhaustively compare all variants in three different sizes. The\nresults show that the shift operation is weaker than the atten-\ntion mechanism, because it does not contain any learnable\nparameter or arithmetic calculation. For example, the Shift-\nT/light model has only 20M parameters and 3.0 FLOPs,\nwhich are nearly 33% less than the Swin-T model. There-\nfore, there is no wonder that its performance is marginally\nworse than the baseline. Despite the relative gap to the base-\nline, it is worth noting that the absolute accuracy of the\nlightweight ShiftViT is not bad. Compared with the typi-\ncal ResNet-50 backbone, Shift-T/light is more powerful and\nmore efficient.\nTo remedy the complexity gap between shift operation\nand attention mechanism, we can adopt more building\nblocks in ShiftViT to make sure it has a similar number\nof parameters with the Swin baseline. In such fair com-\nparisons, our models achieve even better results than Swin-\nTransformer. For the small-size models, our Shift-T back-\nbone attains an mAP score of 45.4% on COCO and an mIoU\nscore of 46.3% on ADE20k, which outperform the Swin-T\nbackbone by a remarkable margin. For the large-size mod-\n2427\nels, ShiftViT seems to be saturated. But the performance is\nstill on par with the Swin baseline.\nAlthough the shift operation is weaker than the atten-\ntion mechanism in spatial modelling, its simple architec-\nture allows the network to grow deeper. As such, the weak-\nness of the shift operation is greatly alleviated. Within\nthe same computational budget, the overall performance of\nShiftViT is comparable to the attention-based Swin Trans-\nformer. These experiments prove that the attention mecha-\nnism might not be necessary for ViTs. Even an extremely\nsimple operation can achieve the similar results.\nComparison with State-of-the-Art\nTo further demonstrate the effectiveness, we compare\nShiftViT backbones with existing state-of-the-art methods.\nFor image classification task on ImageNet-1k, our proposed\nmodels are compared to three different types of models,\nnamely CNN, ViT and MLP. The results are detailed in Ta-\nble 2. Overall, our method can achieve a comparable perfor-\nmance with the state-of-the-arts. For ViT-based and MLP-\nbased methods, the best performances are around 83.5% top-\n1 accuracy, while our model achieves an accuracy of 83.3%.\nFor CNN-based methods, our model is slightly worse than\nEfficientNet series, but the comparison is not fully fair be-\ncause EfficientNet takes a larger input size.\nAnother interesting thing is the comparison with two con-\ncurrent work S 2-MLP (Yu et al. 2021) and AS-MLP (Lian\net al. 2021). These two pieces of work share the similar idea\non shift operation , but they introduce some auxiliary mod-\nules into the building block, e.g., the pre- and post-projection\nlayers. In Table 2, our performances are slightly better than\nthese two work. It justifies our design choice that build-\ning backbone solely with a simple shift operation is good\nenough.\nBeside the classification task, the similar performance\ntread can be also observed in the object detection task and\nsemantic segmentation task. It is notable that some ViT-\nbased and MLP-based methods cannot be easily extended to\nsuch dense prediction tasks, because the high-resolution in-\nputs yield unaffordable computational burdens. Our method\ndoes not suffer from this obstacle thanks to the high effi-\nciency of shift operation. As shown in Table 3 and Table 4,\nthe advantages of our ShiftViT backbones are clear. Shift-\nT attains an mAP score of 47.1 on object detection and an\nmIoU score of 47.8 on semantic segmentation, which out-\nperform other methods by a considerable margin.\nAblation Analysis\nIn this section, we aim to explore what factors contribute\nto the good performance of ShiftViT. We first analyze the\nimpact of two hyper-parameters in ShiftViT. Then, we dive\ninto the training scheme of ViT series.\nExpand ratio of MLP The previous experiments have\njustified our design principle, i.e., a great model depth can\nremedy the weakness of each building block. Generally,\nthere exists a trade-off between the model depth and the\ncomplexity of building blocks. With a fixed computational\nBackbone Params (M) FLOPs (G) APb APm\nMask R-CNN 3×\nRes-50 44 260 41.0 37.1\nPVT-S 44 245 43.0 39.9\nAS-MLP-T 48 260 46.0 41.5\nSwin-T 48 264 46.0 41.6\nShift-T 48 265 47.1 42.3\nRes-101 63 336 42.8 38.5\nPVT-M 64 302 44.2 40.5\nAS-MLP-S 69 346 47.8 42.9\nSwin-S 69 354 48.5 43.3\nShift-S 70 350 48.6 43.4\nCascade Mask R-CNN 3×\nRes-50 82 739 46.3 40.1\nAS-MLP-T 86 745 50.1 43.5\nSwin-T 86 739 50.4 43.7\nShift-T 86 743 50.3 43.4\nResX-101 101 819 48.1 41.6\nAS-MLP-S 107 824 51.1 44.2\nSwin-S 107 838 51.8 44.7\nShift-S 107 827 50.9 44.0\nTable 3: Comparison with state-of-the-art methods on the\nCOCO object detection task. Following the common prac-\ntice, we couple the backbones with two detection frame-\nworks, namely Mask R-CNN and Cascade Mask R-CNN.\nMethod Backbone Params FLOPs val\n(M) (G) mIoU\nDANet ResNet-101 69 1119 45.2\nDNL ResNet-101 69 1249 46.0\nDeepLabV3 ResNet-101 63 1021 44.1\nOCRNet ResNet-101 89 1381 44.9\nDeepLabV3 ResNeSt-101 66 1051 46.9\nDeepLabV3 ResNeSt-200 88 1381 48.4\nOCRNet HRNet-w64 71 664 45.7\nUperNet ResNet-101 89 1029 44.9\nUperNet Swin-T 60 945 45.8\nUperNet AS-MLP-T 60 937 46.5\nUperNet Shift-T 60 942 47.8\nUperNet Swin-S 81 1038 49.5\nUperNet AS-MLP-S 81 1024 49.2\nUperNet Shift-S 81 1029 49.6\nUperNet Swin-B 121 1188 49.7\nUperNet AS-MLP-B 121 1166 49.5\nUperNet Shift-B 121 1174 49.2\nTable 4: Comparison with state-of-the-art methods on the\nADE20k semantic segmentation task. We report the mIoU\nmetrics on the validation set.\n2428\nExpand Depth ImgNet COCO ADE20k\nRatio Acc. (%) APb APm mIoU\nSwin 48 81.3 43.7 39.5 44.5\n4 57 81.3 44.0 39.8 44.4\n3 75 81.5 44.4 40.2 45.5\n2 114 81.7 45.4 40.9 46.3\n1 225 81.8 45.2 40.6 47.3\nTable 5: Ablation analysis on the expand ratio of MLP. The\nfirst row shows the Swin-T baseline. All entries share the\nsame number of parameters and FLOPs.\nSGD ReLU BN 90ep ImageNet\n↓ ↓ ↓ ↓ Top-1 Acc.\nAdamW GELU LN 300ep (%)\n76.4\n✓ 77.9\n✓ ✓ 78.5\n✓ ✓ ✓ 78.4\n✓ ✓ ✓ ✓ 81.7\nTable 6: Ablation analysis on the typical configurations of\nCNNs and ViTs. We gradually transfer the training configu-\nration from the CNN’s setting to the ViT’s setting, and inves-\ntigate how these factors influence the model performances.\nbudget, a lightweight building block can enjoy a deeper net-\nwork architecture.\nTo further investigate this trade-off, we present some\nShiftViT models with different depths. For ShiftViT, most\nparameters exist in the MLP part. We can change the ex-\npand ratio of MLP τ to control the model depth. As shown\nin Table 5, we choose Shift-T as our baseline model. We\nexplore the expand ratio τ within a range from 1 to 4. It is\nworth noting that the parameters and FLOPs for different\nentries are almost the same. From Table 5, we can observe\na trend that a deeper model results in a better performance.\nWhen the depth of ShiftViT increases to 225, it outperforms\nthe 57-layer counterpart by 0.5%, 1.2% and 2.9% absolute\ngains on classification, detection and segmentation, respec-\ntively. This trend supports our conjecture that a powerful-\nand-heavy module, like attention, may not be the optimal\nchoice for backbone. We hope it can help the future work to\nrethink such trade-off when designing backbones.\nPercentage of shifted channels The shift operation has\nonly one hyper-parameter, namely the percentage of shifted\nchannels. By default, it is set to 33%. In this section, we ex-\nplore some other settings. Specifically, we set the percent-\nage of shifted channels to 20%, 25%, 33% and 50%, re-\nspectively. The results are presented in Figure 3. It shows\nthat the final performance is not very sensitive to this hyper-\nparameter. Shifting 25% of channels only results in 0.3%\nabsolute loss compared to the best setting. Within the rea-\nsonable range (from 25% to 50%), all the settings achieve a\nbetter accuracy than the Swin-T baseline.\n20% 25% 33% 50%\nPercentage of shifted channels\n81.0\n81.2\n81.4\n81.6\n81.8T op-1 acc. (%)\nFigure 3: Ablation analysis on the percentage of shifted\nchannels. We plot the top-1 classification accuracy on\nImageNet-1k. The red line indicates Swin-T baseline.\nViT-style training scheme Shift operation has been well\nstudied in CNNs. However, the previous work does not show\nthe impressive performance as ours. Shift-ResNet-50 (Wu\net al. 2018) only achieve an accuracy of 75.6% on ImageNet,\nwhich is far behind our 81.7% accuracy. This gap raise a\nnatural concern about what makes good for our ShiftViT.\nWe suspect the reason might lie in the ViT-style training\nscheme. Specifically, most existing ViT variants follow the\nsetting as in DeiT (Touvron et al. 2020), which is quite dif-\nferent from the standard pipeline of training CNNs. For ex-\nample, ViT-style scheme adopts AdamW optimizer and the\ntraining duration lasts for 300 epochs on ImageNet. As a\ncomparison, CNN-style scheme prefers SGD optimizer and\nthe training schedule is usually 90 epochs only. Since our\nmodel inherit the ViT-style training scheme, it is interesting\nto see how such differences affect the performance.\nDue to the resource limitation, we cannot fully align all\nsettings between ViT-style and CNN-style. Therefore, we\npick four important factors that we believe can bring some\ninsights, i.e. optimizer, activation function, normalization\nlayer and training schedule. From Table 6, we can observe\nthat such factors can significantly influence the accuracy, es-\npecially the training schedule. These results shows that the\ngood performance of ShiftViT is partly brought by the ViT-\nstyle training scheme. Similarly, the success of ViT may be\nalso related to its special training scheme. We should take it\nseriously in the future study of ViTs.\nConclusion\nIn this work, we move a small step toward demystifying the\nessential reason why ViT works. The experiments show that\nthe attention mechanism might not be the vital factor for\nthe success of ViT. We can even use an extremely simple\nshift operation to replace the attention layer. The proposed\nbackbone, namely ShiftViT, can work as well as the Swin\nTransformer baseline. Since the shift operation is already the\nsimplest spatial modelling module, we argue that the good\nperformance must come from the remaining components of\nViT, e.g., the FFN and the training scheme. In future work,\nwe plan to have more analysis on such factors and investi-\ngate more ViT variants.\n2429\nReferences\nChen, C.-F.; Fan, Q.; and Panda, R. 2021. Crossvit: Cross-\nattention multi-scale vision transformer for image classifica-\ntion. arXiv preprint arXiv:2103.14899.\nChen, S.; Xie, E.; Ge, C.; Liang, D.; and Luo, P. 2021. Cy-\nclemlp: A mlp-like architecture for dense prediction. arXiv\npreprint arXiv:2107.10224.\nChen, W.; Xie, D.; Zhang, Y .; and Pu, S. 2019. All you\nneed is a few shifts: Designing efficient convolutional neural\nnetworks for image classification. In CVPR, 7241–7250.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, 248–255. Ieee.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth\n16x16 words: Transformers for image recognition at scale.\nIn ICLR.\nFan, H.; Xiong, B.; Mangalam, K.; Li, Y .; Yan, Z.; Malik, J.;\nand Feichtenhofer, C. 2021. Multiscale vision transformers.\narXiv preprint arXiv:2104.11227.\nHu, H.; Gu, J.; Zhang, Z.; Dai, J.; and Wei, Y . 2018. Relation\nnetworks for object detection. In CVPR, 3588–3597.\nHu, H.; Zhang, Z.; Xie, Z.; and Lin, S. 2019. Local relation\nnetworks for image recognition. In ICCV, 3464–3473.\nHuang, Z.; Wang, X.; Huang, L.; Huang, C.; Wei, Y .; and\nLiu, W. 2019. Ccnet: Criss-cross attention for semantic seg-\nmentation. In ICCV, 603–612.\nJeon, Y .; and Kim, J. 2018. Constructing Fast Network\nthrough Deconstruction of Convolution.NeurIPS, 31: 5951–\n5961.\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Im-\nagenet classification with deep convolutional neural net-\nworks. NeurIPS, 25: 1097–1105.\nLi, J.; Yan, Y .; Liao, S.; Yang, X.; and Shao, L. 2021. Local-\nto-Global Self-Attention in Vision Transformers. arXiv\npreprint arXiv:2107.04735.\nLian, D.; Yu, Z.; Sun, X.; and Gao, S. 2021. AS-MLP: An\nAxial Shifted MLP Architecture for Vision. arXiv preprint\narXiv:2107.08391.\nLin, J.; Gan, C.; and Han, S. 2019. Tsm: Temporal shift\nmodule for efficient video understanding. In ICCV, 7083–\n7093.\nLin, T.-Y .; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-\nmanan, D.; Doll ´ar, P.; and Zitnick, C. L. 2014. Microsoft\ncoco: Common objects in context. In ECCV, 740–755.\nSpringer.\nLiu, H.; Dai, Z.; So, D. R.; and Le, Q. V . 2021a. Pay Atten-\ntion to MLPs. arXiv preprint arXiv:2105.08050.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin, S.;\nand Guo, B. 2021b. Swin transformer: Hierarchical vision\ntransformer using shifted windows. In ICCV, 10012–10022.\nTolstikhin, I.; Houlsby, N.; Kolesnikov, A.; Beyer, L.; Zhai,\nX.; Unterthiner, T.; Yung, J.; Keysers, D.; Uszkoreit, J.; Lu-\ncic, M.; et al. 2021. Mlp-mixer: An all-mlp architecture for\nvision. arXiv preprint arXiv:2105.01601.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J´egou, H. 2020. Training data-efficient image trans-\nformers and distillation through attention. arXiv preprint\narXiv:2012.12877.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In NeurIPS, 5998–6008.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2021. Pyramid vision trans-\nformer: A versatile backbone for dense prediction without\nconvolutions. arXiv preprint arXiv:2102.12122.\nWang, X.; Girshick, R.; Gupta, A.; and He, K. 2018. Non-\nlocal neural networks. In CVPR, 7794–7803.\nWu, B.; Wan, A.; Yue, X.; Jin, P.; Zhao, S.; Golmant, N.;\nGholaminejad, A.; Gonzalez, J.; and Keutzer, K. 2018. Shift:\nA zero flop, zero parameter alternative to spatial convolu-\ntions. In CVPR, 9127–9135.\nYu, T.; Li, X.; Cai, Y .; Sun, M.; and Li, P. 2021. S 2-MLP:\nSpatial-Shift MLP Architecture for Vision. arXiv preprint\narXiv:2106.07477.\nZhao, H.; Jia, J.; and Koltun, V . 2020. Exploring self-\nattention for image recognition. In CVPR, 10076–10085.\nZhou, B.; Zhao, H.; Puig, X.; Xiao, T.; Fidler, S.; Barriuso,\nA.; and Torralba, A. 2019. Semantic understanding of scenes\nthrough the ade20k dataset. IJCV, 127(3): 302–321.\n2430",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7359451055526733
    },
    {
      "name": "Transformer",
      "score": 0.6619324684143066
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.5660869479179382
    },
    {
      "name": "Mechanism (biology)",
      "score": 0.46753594279289246
    },
    {
      "name": "Construct (python library)",
      "score": 0.41139212250709534
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4112722873687744
    },
    {
      "name": "Algorithm",
      "score": 0.3476911187171936
    },
    {
      "name": "Electrical engineering",
      "score": 0.1316622793674469
    },
    {
      "name": "Programming language",
      "score": 0.12705713510513306
    },
    {
      "name": "Engineering",
      "score": 0.11854422092437744
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210113369",
      "name": "Microsoft Research Asia (China)",
      "country": "CN"
    }
  ],
  "cited_by": 70
}