{
    "title": "Information extraction from historical well records using a large language model",
    "url": "https://openalex.org/W4405910028",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2098637498",
            "name": "Zhiwei Ma",
            "affiliations": [
                "Los Alamos National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2885465492",
            "name": "Javier E. Santos",
            "affiliations": [
                "Los Alamos National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2516597783",
            "name": "Greg Lackey",
            "affiliations": [
                "National Energy Technology Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2342232499",
            "name": "Hari Viswanathan",
            "affiliations": [
                "Los Alamos National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2948558563",
            "name": "Daniel O'Malley",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2098637498",
            "name": "Zhiwei Ma",
            "affiliations": [
                "Los Alamos National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2885465492",
            "name": "Javier E. Santos",
            "affiliations": [
                "Los Alamos National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2516597783",
            "name": "Greg Lackey",
            "affiliations": [
                "National Energy Technology Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2342232499",
            "name": "Hari Viswanathan",
            "affiliations": [
                "Los Alamos National Laboratory"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4297259270",
        "https://openalex.org/W4321221756",
        "https://openalex.org/W4381433870",
        "https://openalex.org/W3155415452",
        "https://openalex.org/W2565019508",
        "https://openalex.org/W6854692045",
        "https://openalex.org/W4385287322",
        "https://openalex.org/W4297236903",
        "https://openalex.org/W3012166879",
        "https://openalex.org/W4388409489",
        "https://openalex.org/W4296342943",
        "https://openalex.org/W4310398257",
        "https://openalex.org/W4404699068",
        "https://openalex.org/W3208677614",
        "https://openalex.org/W4391935887",
        "https://openalex.org/W4390692489",
        "https://openalex.org/W4404781802",
        "https://openalex.org/W3198685994",
        "https://openalex.org/W4387131402",
        "https://openalex.org/W4389523706",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W3199400376",
        "https://openalex.org/W3160638507",
        "https://openalex.org/W4402997213",
        "https://openalex.org/W4367188881",
        "https://openalex.org/W2990138404"
    ],
    "abstract": null,
    "full_text": "Information extraction from \nhistorical well records using a large \nlanguage model\nZhiwei Ma1, Javier E. Santos1, Greg Lackey2, Hari Viswanathan1 & Daniel O’Malley1\nTo reduce environmental risks and impacts from orphaned wells (abandoned oil and gas wells), it \nis essential to first locate and then plug these wells. Manual reading and digitizing of information \nfrom historical documents is not feasible, given the large number of wells. Here, we propose a new \ncomputational approach for rapidly and cost-effectively characterizing these wells. Specifically, we \nleverage the advanced capabilities of large language models (LLMs) to extract vital information \nincluding well location and depth from historical records of orphaned wells. In this paper, we present \nan information extraction workflow based on open-source Llama 2 models and test it on a dataset \nof 160 well documents. The developed workflow achieves an overall accuracy of 100%, accounting \nfor both text conversion and LLM analysis when applied to clean, PDF-based reports. However, it \nstruggles with unstructured image-based well records, where accuracy drops to 70%. The workflow \nprovides significant benefits over manual human digitization, because it reduces labor and increases \nautomation. Additionally, more detailed prompting leads to improved information extraction, and \nLLMs with more parameters typically perform better. Given that a vast amount of geoscientific \ninformation is locked up in old documents, this work demonstrates that recent breakthroughs in LLMs \nallow us to access and utilize this information more effectively.\nIn the oil and gas industry, orphaned wells are defined as a class of unplugged wells whose owner/operator is \nunknown. Thus, other than agencies from the government, no one is responsible for the well-plugging operations \nand site restoration processes 1. While some orphaned wells are well-documented with detailed information, \nsuch as name, location, and drilling details, many others lack important information and are referred to as \nundocumented orphaned wells. Based on a recent report from the U.S. Geological Survey (USGS), there are only \n117,672 documented orphaned oil and gas wells in the 27 states in the U.S2. On the other hand, the Interstate Oil \nand Gas Compact Commission (IOGCC) reported that there are between 310,000 to 800,000 undocumented \norphan wells in the 32 states of the U.S. that produce the most oil and gas, as of 2020 3. However, it is believed \nthat the actual number of undocumented orphan wells is much larger. Orphaned wells often present numerous \nenvironmental and health risks, including emitting methane, releasing hazardous air pollutants, creating a risk \nof explosion, leaking continent into underground water 4–6. For example, according to a technical report from \nU.S. Environmental Protection Agency (EPA) and a recent study4,5, in the U.S., the methane emissions from all \nabandoned oil and gas wells amounted to about 3% of those from natural gas and petroleum systems. However, \nthe “documented” orphaned wells that are covered by the Bipartisan Infrastructure Law (BIL) only emit \napproximately 3% to 6% of total U.S. methane from all abandoned oil and gas wells. Therefore, it is necessary \nto find vital information on the orphaned wells such as well locations and depths for subsequent treatments to \nmitigate these environmental risks.\nOil and gas regulatory agencies in the U.S. maintain regulatory records (e.g., permitting documents) for \nwells under their jurisdiction that often contain valuable information about the location and the construction \nof wells. These historical records are often decades old and exist in a variety of formats that sometimes include \ndigital PDFs but are usually scanned images or paper copies. The current practice for extracting information \nfrom historical documents related to orphaned wells involves hiring individuals to review and enter the data \ninto a computer. This manual process requires some domain knowledge to accurately interpret the documents \nand correct errors, which are frequently compounded by the presence of stamps and various information \nformats (e.g., 45°25’28.56” and 56.358599 degrees, when dealing with unit of latitude). Given the high number \nof orphaned wells, it is neither practical nor realistic to manually extract and digitize this information from \nhistorical well documents. That is because the manual extraction process is labor-intensive and time-consuming. \n1Earth & Environmental Sciences Division, Los Alamos National Laboratory, Los Alamos, NM 87544, USA. \n2Geological and Environmental Systems Directorate, National Energy Technology Laboratory, Pittsburgh, PA \n15236, USA. email: m.ma@lanl.gov\nOPEN\nScientific Reports |        (2024) 14:31702 1| https://doi.org/10.1038/s41598-024-81846-5\nwww.nature.com/scientificreports\n\nTherefore, it is crucial to develop an automatic information extraction workflow to analyze those historical well \ndocuments, facilitating the rapid and precise identification of the wells’ location and depth information. To deal \nwith this challenge, we developed an information extraction workflow combining text conversion techniques, \n(e.g., Optical Character Recognition or OCR) and large language models (LLMs). Specifically, OCR technology \nis used to convert different types of well documents such as PDFs and scanned images, into machine-encoded \ntexts, which are editable and searchable data 7,8. Next, we employed publicly available pre-trained LLMs to \nperform the well information extraction, during which, the converted texts are used as inputs for a properly-\ndesigned prompt. This developed workflow is based on the strong capabilities of LLMs.\nLLMs are often referred to as pre-trained language models based on vast amounts of data 9,10. Recently, \nartificial intelligence and machine learning have rapidly advanced and been widely adopted in the geoscience \nand subsurface flow fields for various applications. These include well control/production optimization \nin oil/gas applications 11,12, reconstruction of complex spatial fields for geospatial analysis 13, for upscaling \ngeomechanical properties 14, for geological CO 2 storage modeling 15,16, for rapid forecasting and history \nmatching in unconventional reservoirs 17, and for inference of random medium properties 18. As one type of \nartificial intelligence model, LLMs can be described as extensive, pre-trained statistical language models that \nutilize neural networks 19. The development and advances in LLMs are very fast. These developments include \nthe introduction of new models and increased model parameter sizes, along with incorporating domain \ninformation for fine-tuned LLMs. New fine-tuned versions of base models are released many times per day, and \nnew base models such as Llama, Mistral, and Mixtral are also released frequently. Currently, many LLMs are \nreferred to transformer-based neural language models. These models typically possess billions of parameters \nand are trained using an extremely large dataset19. Due to their emergent ability and generalizability20, LLMs are \ncapable of generating text, understanding natural language, translating, summarizing content, and performing \nsentiment analysis, among other capabilities. Examples of applications of LLMs can be found in the following \ncategories: translation21, sentiment analysis 22, question and answering 23, code generation 24, summarization25, \nand chatbots26. In the field of hydrology and earth science, a brief overview of opportunities, prospects, and \nconcerns using ChatGPT was provided 27. The research topic addressed in this work pertains to the question-\nanswering category. In other words, we pose specific questions to the LLMs based on well records and anticipate \nthat the LLMs will generate the desired answers, after analyzing the provided text. Our objective is to leverage \nLLMs’ capability for processing text as an alternative approach to overcome challenges associated with the \nmanual extraction of well information from historical documents, as highlighted above.\nIn this work, we mainly focused on Llama 2 family of Large Language Models. Llama 2 is an updated version \nof Llama 128 and it was trained on a mix of data that are publicly available29. In addition, there is a 40% increase \nin the pre-training corpus, with the model’s the context length being doubled when compared with Llama 1. \nMeta’s release of Llama 2 family consists of several pre-trained Llama 2 models, ranging from 7 billion to 70 \nbillion parameters, along with their corresponding fine-tuned LLMs for dialogue use cases. Training Llama 2 \nmodels is not trivial, as they require advanced graphics processing unit (GPU) clusters. To train these models, \nMeta used two clusters equipped with NVIDIA A100 GPUs. It took about 3,311,616 GPU hours to train these \nmodels and with 539 tCO 2eq generated. According to Touvron et al. 29, Llama 2-chat models, in general, have \na better performance than some open-source models on a series of safety and helpfulness benchmark tests. In \naddition to that, the authors also claimed that Llama 2 models achieve performance comparable to some closed-\nsource models in their human evaluations. Because of their superior performance and open-source nature, \nLlama 2 models were used for analysis in this work.\nIn order to interact with LLMs and receive responses, it is common to use prompts 9. A typical prompt \nconsists of three elements: instruction, context, and input text 20. As a new field of study, the goal of prompt \nengineering to improve LLMs performance for a given task by creating and refining prompt contents20. Recently, \nvarious prompting approaches have been developed to improve the reasoning capability of LLMs30. One of these \nexamples is the chain-of-thought strategy proposed by Wei et al. 31, in which the LLMs are asked to provide a \nseries of intermediate reasoning steps and to improve the final performances for complex reasoning tasks 30,31. \nIn this work, we optimized prompt contents including the approach of chain-of-thought in order to improve the \nperformance of well information extraction tasks.\nThe contribution of this work can be summarized as follows: First, we developed a new LLM-based workflow \nfor well information extraction. To the best of our knowledge, the use of LLMs to extract critical information \nrelevant to managing orphaned oil and gas wells has not been widely reported in the literature. Therefore, this \nwork could serve as an example for information identification tasks for other researchers and the research \ncommunities. Second, we conducted a detailed analysis of the impact of prompts, model sizes, and the chain-\nof-thought strategy on the information extraction performance. Third, the developed workflow can be easily \ndeployed and we believe that employing this workflow can potentially accelerate information digitization from \nhistorical well documents.\nThe rest of this paper is organized as follows: we will first introduce our detailed methodology related to the \nworkflow of information extraction, historical well records, text conversion, the theory of LLMs, Llama 2, and \nperformance evaluations in Section 2. We will present the extraction results including various treatments that \nare incorporated in this work in Section  3, which is followed by a brief discussion of the potential impacts of \nthis study, challenges, and the corresponding opportunities in Section 4. Finally, we will summarize the major \nfindings and provide the potential future works in Section 5.\nMaterials and methods\nIn this section, we provide a detailed description of the proposed information extraction workflow, historical \nwell records, large language models used for information extractions, and performance evaluation method.\nScientific Reports |        (2024) 14:31702 2| https://doi.org/10.1038/s41598-024-81846-5\nwww.nature.com/scientificreports/\nOverview of the information extraction workflow via large language models (LLMs)\nThe proposed workflow for information extraction from orphaned well historical records is presented in Figure 1. \nAs shown in this figure, the first step involves converting historical documents into text via text conversion \napproaches such as optical character recognition (OCR). Next, the converted texts are subjected to LLMs. Here, \nwe integrate the texts into some predefined prompt templates to form the final question prompts.\nAfter running the LLMs with the complete prompt, an answer in text format can be generated, as shown in \nFigure 2. The answer can be examined, and if the result is satisfactory, the information extraction task for this \nhistorical document is completed. Otherwise, we may need to refine the prompt or switch to different LLMs to \nachieve the desired outputs.\nIn this work, we used a standard for-loop to automatically extract the information of interest from the 160 \nwell documents. It is worth noting that we used quantized Llama 2 models in this work to reduce the memory \nusage. Specifically, for example, for Llama 70B model, we utilized the Llama-2-70B-chat-GPTQ32 obtained from the \nHugging face33 due to the reduced size, instead of its the standard version. For the Llama-2-70B-chat-GPTQ, the \nGPTQ algorithm34, was employed to quantize Llama 2 models within AutoGPTQ library.\nCurrently, we have access to only 160 documents from Colorado and Pennsylvania. This small dataset \nallowed us to quickly validate our approach. Once a larger dataset becomes available, the developed framework \nis expected to scale easily for information extraction. We acknowledge that this sample size, particularly for \nPennsylvania, may not be sufficient for a highly reliable statistical evaluation. However, this dataset reflects the \ncurrent limitations of available data for our study. We are actively working to obtain additional well documents \nfrom other states, and once these data are available, our workflow can be readily applied to extract information \non a larger scale. Our scope of this study is to propose a novel process for information extraction from well \nrecords by leveraging the capabilities of LLMs and to test this concept.\nThe following subsections will cover the detailed methodologies for each major step in the information \nextraction workflow.\nFig. 1. The proposed workflow for well information extraction via LLM.\n \nScientific Reports |        (2024) 14:31702 3| https://doi.org/10.1038/s41598-024-81846-5\nwww.nature.com/scientificreports/\nHistorical well records\nIn this study, we analyzed two types of well records: well drilling completion reports from Colorado and well \nrecord reports from Pennsylvania, as illustrated in Figure 3. These types of well records are commonly utilized \nby oil and gas regulatory agencies to document the construction history of wells. However, it is worth noting \nthat each jurisdiction tracks well information with their own records that have unique formats. The multitude \nFig. 3. Examples of well records used in this study (some sensitive information was blocked).\n \nFig. 2. An illustration of model inputs and outputs for LLM. Note that we aim to show the structure of the \nmodel’s input and output. One has to provide specific well record texts to the model input section, and the \nLLM would generate the corresponding detailed output in terms of well location and depth.\n \nScientific Reports |        (2024) 14:31702 4| https://doi.org/10.1038/s41598-024-81846-5\nwww.nature.com/scientificreports/\nof oil and gas jurisdictions in the U.S. and the differences between the records they use increases the practical \nchallenge of digitizing well information into a unified platform for characterizing orphaned wells.\nThe preliminary dataset assembled for this study includes 150 well drilling completion reports from Colorado \nand 10 well records from Pennsylvania for demonstration purposes. The well records presented in Figure \n3 contain a wealth of information, such as the operator’s name, address, and phone number; the American \nPetroleum Institute number (a unique identifier assigned to each oil and gas well), name and location of the well; \nthe spud date; the depth; and details on casing, liner, and cement. Although well records contain an abundance \nof information, the location and depth data are crucial for well remediation and will be extracted using LLMs in \nthis work. That is because depth information provides a better understanding of the casing depth.\nAs shown in Figure 3, we can see that well drilling completion reports from Colorado are clean. On the other \nhand, the well records from Pennsylvania contain many hand-written words and stamps. For example, in the \ntop left corner, there are three hand-written words: “Standard Survey Report” . There is a stamp on the mid-right \nside of this record, which shows “RECEIVED AUG 25 2016” . In addition, the middle part of the document is \nsomewhat blurred with grey shadow. All these hand-written words, marks, and stamps increase the challenge \nof information extraction using LLMs. That is because the LLMs employed in this work require texts as input; \ntherefore we must utilize text conversion technologies (e.g., OCR) to convert the image-based well records into \ntext.\nText conversion\nPlain text was acquired from the Colorado and Pennsylvania well records using two approaches selected based \non the original format of the document: 1) PDF-to-text conversion and 2) optical character recognition (OCR). \nColorado well records were stored in text-based PDF format, which enabled a direct extraction of embedded text \nusing the open-source tool pdftotext35. Pennsylvania well records were stored as scanned image files, which have \nno embedded text. Consequently, Google’s Enterprise OCR, made available through their Document AI API36, \nwas used to convert text in the Pennsylvania records into a machine-readable format.\nFigure 4 displays a portion of the text information extracted from the two examples shown in Figure  3. \nWhen compared with the original documents in Figure  3, the quality of plain text conversion is acceptable, \nas the information presented in Figure 4 matches that in the original documents. For example, the converted \ninformation of well locations (latitude and longitude) agrees with that in the two documents in Figure 3. Another \nobservation is that the formatting of converted information is not the same. The structure of the PDF-converted-\ntext in Figure 4a preserves the alignment and structure of the original document as in Figure  3a. However, the \nOCR converted text in Figure  4b does not maintain a similar table-style structure to that in the well record \nshown in Figure  3b. Instead, the words within one single line in the image are divided into multiple rows in \nthe OCR-processed text. The lack of correct structure in OCR-converted text poses a significant challenge for \ninformation extraction using LLMs as it requires LLMs to have advanced understanding capability to analyze \nthe overall text. To improve performance, more advanced computer vision approaches should be applied for text \nconversion, which is beyond the scope of this paper. Once we have converted the text information, the next step \nis to feed it into a pre-designed prompt for LLM for extracting well location and depth.\nFig. 4. Part of the texts extracted from the two well records shown in Figure 3. Some sensitive information was \nblocked.\n \nScientific Reports |        (2024) 14:31702 5| https://doi.org/10.1038/s41598-024-81846-5\nwww.nature.com/scientificreports/\nLarge language models (LLMs)\nLLMs are machine learning models trained on vast amounts of data. This training enables them to comprehend \nand produce text that closely resembles human writing. The sheer scale of these models, coupled with the large \namounts of data they are trained on (on the order of trillions of tokens), allows them to learn complex patterns \nand relationships within the text. As the training progresses, these models develop abilities to perform a variety \nof tasks. For example, they can accurately answer queries, summarize vast amounts of information, and generate \nnew text that is both coherent and contextually sound.\nLlama 229, developed by Meta AI, is a large language model that has attracted attention from the research \ncommunity for its capabilities. It follows a structure similar to GPT (Generative Pretrained Transformer) 37, \nwhich relies on stacked attention layers to process and generate text. These layers work by focusing on different \nparts of the input text to determine what is important and what is not. This mechanism enables Llama 2 to \nprocess and generate text effectively, understanding the context and nuances of the input text.\nAvailable in different sizes, from smaller versions like Llama 2 7B to the largest, Llama 2 70B, these variants \ndiffer in their processing power and the depth of understanding they can provide. Larger models, while \nrequiring more computational resources, can deliver more accurate interpretations of data. Llama 2 operates \nunder an open-weights regime, meaning the model weights are accessible to the public, but the specific data \nused for training these models is not disclosed. Llama 2 comes in two main versions: Foundational and Chat. \nThe Foundational model is a general-purpose tool for text completion, while the Chat model has been further \nrefined with techniques like supervised fine-tuning and Reinforcement Learning from Human Feedback (RLHF) \nto enhance its abilities to be a useful assistant. In our work, we focused on the Chat-type models, which are \noptimized for tasks requiring in-depth analysis and information extraction.\nAs mentioned previously, prompt engineering is a crucial aspect of working with chat models. It involves \ncrafting the input text (or prompt) to guide the model in generating a desired output. Through effective \nprompt engineering, users can steer the focus of LLMs and improve the quality of the extracted information. \nMoreover, LLMs like Llama 2 can perform zero-shot learning, which allows the model to make predictions or \ngenerate responses in tasks it has not explicitly been trained on. Contrarily, few-shot learning for LLMs refers \nto the process of a model learning from a small number of examples. For a specific task, we can provide a \nfew demonstrative examples in the prompt to enhance the performance through few-shot learning. Another \nuseful concept for LLM prompting is the chain-of-thought approach31. This involves the model breaking down \na problem into smaller, manageable parts, similar to how humans approach complex problems. This method \ncan enhance the model’s ability to understand and solve intricate tasks, making it a very useful approach for \nanalyzing and extracting data from extensive and complex records. In this work, we mainly tested zero-short \nlearning and chain-of-thought methods.\nLLMs performance evaluation\nAlthough additional information is available from the well documents, in this work, we focused only on the \nlocation (latitude and longitude) and depth (true vertical depth) of each well. We employed one metric to assess \nthe performance of our information extraction process. The metric is the accuracy based on offset or AOS, \nwhich is defined as:\n \nAOS = NOS\nNT\n× 100% (1)\nwhere AOS denotes the accuracy; NOS represents the number of entries that are within the offset threshold of \nthe true value; NT  represents the total number of entries. In an ideal case, AOS would be 100% if the workflow \ngenerates results that are accurate, which may not always be the case in reality. In this paper, we calculated \nAOS  for location and depth information extraction only rather than latitude, longitude, and depth. That is \nbecause the location can be represented as latitude and longitude. The location offset is calculated based on \ngeographical distance, also known as geodetic distance, which is the shortest arc length between two locations \nalong the Earth’s surface, using GeographicLib package38.\nThe rationale behind this metric is that it is also acceptable if a certain LLM generates a close approximation to \nthe true value. For example, if the extracted location, in terms of latitude and longitude, is within 10 meters of \nthe true location, we treat the result as correct. Similarly, if the extracted depth is within a range of 10 feet ( ∼3 \nmeters), we would accept this extraction result. From a practical point of view, an offset of 10 meters for the well \nlocation is considered acceptable because field operators can easily locate the well based on the extracted well \nlocation information.\nResults\nIn this section, we demonstrate the capabilities of LLMs for information extraction using Llama 2 models. \nWe begin by comparing the performance of Llama 2 70B model with various prompts. Next, we illustrate \nthe performance differences among Llama 2 7B, 13B, and 70B models using the optimal prompt. Finally, we \nshowcase the effectiveness of implementing the chain-of-thought strategy with Llama 2 70B model and compare \nits performance with other LLMs, again using the selected optimal prompt.\nScientific Reports |        (2024) 14:31702 6| https://doi.org/10.1038/s41598-024-81846-5\nwww.nature.com/scientificreports/\nPrompt engineering for Llama 2 70B\nBefore applying any LLM for information extraction, we must formulate and select the optimum prompts for the \nQuestion-Answering task. This can be achieved through prompt engineering, which involves designing proper \nprompts to achieve desired outcomes from LLMs39,40.\nTo design the best-performing prompt, we used a trial-and-error approach with iterative refinements. During \nthis process, domain knowledge of the oil and gas industry and geosciences was applied to meet our specific data \nextraction requirements. For example, we specified that longitude and latitude should be in decimal format to \nensure compatibility with future processing. As shown in Table 1, we designed a total of four prompts, from \nPrompt 1 to Prompt 4, to locate wells and identify their depths from the documents. To design these prompts, \nwe began by manually reviewing a few representative documents to obtain a preliminary understanding of their \ncontents and structures. Based on our domain knowledge and specific requirements, we first created a simple \nand straightforward Prompt 1. We then added additional constraints and guidance to subsequent prompts to \nenhance their focus on information extraction. For example, we directed the LLMs to extract true vertical depth \nas the depth information rather than measured depth (since both types are present in the documents) and \nenforced that longitude be negative given the wells’ geographic locations. Once these prompts were designed, \nthey were subjected to information extraction testing via LLMs, as shown in Figure 1.\nTable 1 provides detailed information on four proposed prompts including prompt index, prompt content, \nand the corresponding explanation. Prompt 1 is the simplest one by just instructing LLMs to extract well \ninformation in terms of latitude, longitude, and depth information and to report in a JSON format. If users \nlack detailed information about the documents, the simplest prompt can be used directly without extensive \ndomain knowledge. On the other hand, Prompt 4 is the most comprehensive one, ensuring that: (1) reported \nlatitudes and longitudes are drilled latitudes and longitudes, and use decimal degrees as the unit; (2) longitudes \nare negative, given the well’s location in the U.S.; (3) only true vertical depth is exported as depth information, \ndespite that other depth information, e.g., measured depth, are available; (4) the true vertical depth is a positive \nnumber. By combining the proposed prompt with converted text through a text extraction process, a complete \nquestion was created for LLMs, which was then subjected to LLMs to perform information extraction. It is \nimportant to note that once the questions are formulated using the prompts, they can be directly utilized across \nvarious LLMs without any further adjustments.\nAfter running Llama 2 70B model with a question, an output as shown in Figure 5 can be obtained. Figure \n5 represents the output from Llama 2 70B model for the drilling completion report in Colorado (in Figure \n3a) using Prompt 1. As expected, the output is in the format of a JSON file within the Python environment. \nFig. 5. An example of information extraction output using Llama 2 70B.\n \nPrompt \nindex Prompt Explanation\nPrompt 1\nExtract the location using latitude and longitude, and well depth of the well described in this well completion \nreport. Output only the latitude, longitude, and depth in JSON format as numbers, not strings, in a clean version. \nOnly output the JSON and nothing else. Here is the OCR’ d contents of the well completion report:\nThis prompt directs the LLM to extract the well’s \nlocation (latitude and longitude) and depth from \nthe well completion report, and to output the \nnumbers in JSON format. This is the simplest \nprompt for this task.\nPrompt 2\nExtract the drilled latitude (in degrees), longitude (in degrees), and true vertical depth (TVD) of the well described in \nthis well completion report. Output only the latitude, longitude, and depth in JSON format as numbers, not strings, \nin a clean version. Only output the JSON and nothing else. Here is the OCR’ d contents of the well completion \nreport:\nProvide more detailed instructions for reporting \nthe well’s location using decimal degrees, and \noutputting the well depth specifically in terms of \nTrue Vertical Depth (TVD).\nPrompt 3\nExtract the drilled latitude (in degrees), longitude (in degrees), and true vertical depth (TVD) of the well described \nin this well completion report. Do not report depth in terms of Measured Depth (MD). Keep in mind that this text \nis extracted using optical character recognition (OCR), so the format may be jumbled. This well is in the western \nhemisphere, so the longitude should be negative. Output only the latitude, longitude, and depth in JSON format as \nnumbers, not strings, in a clean version. Only output the JSON and nothing else. Here is the OCR’ d contents of the \nwell completion report:\nAdditional instructions are provided to ensure \nthe LLM not report measured depth, and the \nlongitude should be negative given the location of \nthe well of interest.\nPrompt 4\nExtract the drilled latitude (in degrees), longitude (in degrees), and true vertical depth (TVD) (not footage at surface \nand not plug back total depth) of the well described in this well completion report. Do not report depth in terms of \nMeasured Depth (MD). Keep in mind that this text is extracted using optical character recognition (OCR), so the \nformat may be jumbled. This well is in the western hemisphere, so the longitude should be negative. In addition, the \ntrue vertical depth cannot be negative. Output only the latitude, longitude, and depth in JSON format as numbers, \nnot strings, in a clean version. Only output the JSON and nothing else. Here is the OCR’ d contents of the well \ncompletion report:\nEnsure the LLM not using footage at surface \nand plug back total depth as well as depth \ninformation. Also, ensure that well depth \ninformation from the well completion report is \nnot negative. If a negative value is found, it should \nbe corrected to the corresponding positive value. \nThis is the most complicated prompt for this task.\nTable 1. Different prompts for information extraction. The italicized texts in the second column highlights the \ndifferences between the current prompt and the previous one.\n \nScientific Reports |        (2024) 14:31702 7| https://doi.org/10.1038/s41598-024-81846-5\nwww.nature.com/scientificreports/\nSpecifically, the output for this example contains the names “latitude” , “longitude” , and “depth” , along with their \ncorresponding numbers as instructed by the prompt. In this example case, the latitude, longitude, and depth \nare “40.197079” , “-104.575949” , and “6893” ft, respectively. The extracted information exactly matches the true \nvalues, demonstrating the good performance of Llama 2 70B model with Prompt 1. It should be noted that \nalthough JSON-style outcomes are generated by LLMs after analyzing the text-based well records, the current \nworkflow does not have the capability to save the outputs directly as local JSON files. Therefore, an additional \npost-processing step is required to save the information extraction outputs as local files.\nLet’s now examine the final extraction performance for the four prompts across the 160 well records in \nColorado and Pennsylvania. Given the fact that the well records from the U.S. are not in the same format, we \nhere evaluate the performance of LLMs for information extraction separately. Again as introduced in Section 2, \nwe used AOS as the metric for performance evaluation. We reiterate that we used 10 meters and ∼3 meters (10 \nft) as the thresholds for computing well location and depth offset, respectively. For the 150 Colorado drilling well \nrecord documents, the information extraction results obtained by Llama 2 70B with four different prompts are \npresented in Table 2.\nClearly, excellent location extraction performances were obtained using Llama 2 70B model for Colorado \ncases. As shown in this table, the values of AOS for location reach 100% despite the varying contents of the \nprompt. This observation reveals that the locations of all 150 wells were correctly extracted from the well drilling \ncompletion reports in Colorado using the LLM and proposed prompts. In terms of well depth extraction, we find \nthat Llama 2 70B model with Prompt 1 (the simplest prompt) yielded the lowest accuracy. Specifically, the value of \nAOS were only 48%, indicating that the locations of only 72 documents were correctly identified. In other words, \nLlama 2 70B model, when using Prompt 1, encountered difficulty in extracting information from the remaining \n78 documents. Despite utilizing an offset of 10 ft for computing AOS, the result here shows that the identified \ndepth, using the LLM with Prompt 1, deviates by more than 10 ft from the true value for those 78 documents. \nFor example, in one drilling well completion report, the actual well depth is 6806 ft, however, the extracted \nvalue is 17529 ft, which is about 10723 ft away from the true value. This observation illustrates that for this case, \nextracting well depth information was much more challenging than extracting well location information using \nPrompt 1. Except Prompt 1, Llama 2 70B with the remaining prompts resulted in very reliable depth extraction \nperformance with 100% accuracy. This result demonstrates that more detailed prompts (Prompts 2 to 4) enable \nmore reliable information extraction compared to a simple prompt (Prompt 1).\nTable 3 compares well information extraction results for the 10 Pennsylvania well records using Llama 2 70B \nwith these predefined four prompts in Table 1. The major difference between the Pennsylvania and the Colorado \ncase studies is that Llama 2 70B provided inferior results for Pennsylvania cases. Clearly, none of the prompts \nresulted in completely correct extraction for the 10 Pennsylvania well records. For the location, Llama 2 70B \nmodel with the best-performing prompt (i.e., Prompt 4) resulted in an accuracy of 70%. This corresponded to \n7 correctly extracted well locations. For the depth information extraction task, a value of 90% was achieved for \nAOS from all four prompts, indicating that we obtained correct depth information for 9 out of 10 documents. \nTwo possible reasons may explain this performance. First, the original image document contains some errors. \nFor example, one Pennsylvania record shows a longitude of “77.670522” , which should be the negative value \n“-77.670522” . As a result, OCR also missed the “-” sign in the converted text. Despite adding, “This well is in \nthe western hemisphere, so the longitude should be negative, ” in Prompts 3 and 4, Llama 2 70B may still have \ndifficulty correcting this error. The second reason relates to unit conversion. Some Pennsylvania well records use \ndegrees, minutes, and seconds for latitude and longitude, which differs from our request for decimal degrees. \nLlama 2 70B model used in this work did not perform the unit conversion completely correctly. To verify our \nhypothesis, we conducted another test by: (1) manually correcting any errors/issues in the OCR-converted texts \nfor the Pennsylvania well records, given the small number of such records, and (2) re-running the information \nPrompt index Location Depth\nPrompt 1 60% 90%\nPrompt 2 60% 90%\nPrompt 3 60% 90%\nPrompt 4 70% 90%\nTable 3. Information extraction results using Llama 2 70B model with different prompts for Pennsylvania well \nrecords.\n \nPrompt index Location Depth\nPrompt 1 100% 48%\nPrompt 2 100% 100%\nPrompt 3 100% 100%\nPrompt 4 100% 100%\nTable 2. Information extraction results using Llama 2 70B model with different prompts for Colorado well \ncompletion reports.\n \nScientific Reports |        (2024) 14:31702 8| https://doi.org/10.1038/s41598-024-81846-5\nwww.nature.com/scientificreports/\nextraction workflow with Llama 2 7B and Prompt 4. As expected, we achieved 100% accuracy in the location \nextraction task, with all 10 correct location extractions from the Pennsylvania well records. This demonstrates \nthat LLM performance can significantly improve when OCR-converted texts are more reliable. For the depth \ninformation extraction task, we observed that high accuracy was achieved with all four prompts. Specifically, \nthe AOS of all Llama 2 70B model runs reached 90%, which is very close to the accuracy obtained from the \nColorado completion reports. A detailed examination of this failed record shows that it had a depth of “0” in the \ntrue vertical depth field, but Llama 2 models used the measured depth of “381” instead.\nOn the other hand, we observed that the extraction accuracy for locations increased with the complexity \nof prompts. For AOS, Llama 2 70B led to 70% accuracy using Prompt 4, compared to only 60% accuracy for \nthe remaining three prompts for the location information extraction task. The results in this case reveal that \nmore complicated prompts result in better extraction performance. Based on the information extraction results \nin Tables 2–3, we see that a more detailed prompt often leads to better information extraction results. In the \nfollowing sections of this paper, Prompt 4 was used for the investigation.\nComparison of different Llama 2 models\nIn this test, we used the best prompt via zero-shot learning from the previous section to test the extraction \nperformance of the three Llama 2 models, i.e., 7B, 13B, and 70B. Here, Prompt 4 was employed within Llama \n2 7B and 13B to extract well location and depth information from the 160 well records. Subsequently, we \ncompared these extraction results with those from Llama 2 70B model. Tables 4-5 show the comparison results \nfor Colorado and Pennsylvania cases, respectively. These two comparisons reveal that, in general, as the size \n(model parameters) of Llama 2 increases, better performance is achieved, though some deviations from the \ntrend are observed, as shown in Table 4.\nFor the 150 drilling completion reports in Colorado, Llama 2 7B model achieved an accuracy of 82.67% \naccuracy in terms of AOS for depth, which is lower than the 97.33% accuracy achieved by Llama 2 13B model. \nAs expected, neither of the smaller models can surpass the 70B model in depth extraction. However, surprisingly, \nwe find that for the location extraction task, the 7B model yielded a slightly better result when compared with \nthe 13B model, contrary to expectations. For example, the Llama 13B model only achieved 66% for AOS. \nInterestingly, the Llama 7B slightly outperformed it, achieving a higher accuracy rate 77.33% for AOS. For the \n10 Pennsylvania well records, we observed a consistent pattern: the larger model yielded better information \nextraction results. This conclusion is supported by data on both location and depth. For example, for the location \ninformation extraction, Llama 2 7B, 13B, and 70B models resulted in accuracy values of 30%, 70%, and 70%, \nrespectively. For the depth case, Llama 2 models with 70B and 13B parameters significantly outperformed the \n7B model. The results presented here demonstrate that larger LLMs are generally more effective for information \nextraction tasks. It is recommended that users opt for larger models if they have sufficiently powerful hardware \nsupport, as more advanced hardware is required to run larger LLMs.\nImpact of the chain-of-thought on the performance of Llama 2 70B\nAs presented in the previous sections, Llama 2, regardless of the prompt used or the size employed, was difficult \nto extract completely correct information from the 10 Pennsylvania well records. In this work, we also explored \nthe possibility of enhancing extraction performance by incorporating the chain-of-thought approach with Llama \n2 70B, using Prompt 4. Given that the text extraction performance from the Colorado well drilling completion \nreports is reliable, we focused solely on the 10 Pennsylvania reports. We implemented the chain-of-thought \napproach to Prompt 4 in Table 1 by adding the following words to the end of the prompt: Please explain your \ndetailed steps to get the numbers . By incorporating this strategy, Llama 2 70B model generates the following \noutput, as presented in Figure 6.\nUnlike those without the chain-of-thought strategy, the texts generated by Llama 2 here exhibited more \ndetailed “thinking” steps for extracting the numbers for location and depth. As shown in the texts, Llama 2 first \nPrompt index Location Depth\n7B 30% 40%\n13B 70% 90%\n70B 70% 90%\nTable 5. Information extraction results using three Llama 2 models with Prompt 4 for Pennsylvania well \nrecords.\n \nPrompt index Location Depth\n7B 77.33% 82.67%\n13B 66% 97.33%\n70B 100% 100%\nTable 4. Information extraction results using three Llama 2 models with Prompt 4 for Colorado well \ncompletion report.\n \nScientific Reports |        (2024) 14:31702 9| https://doi.org/10.1038/s41598-024-81846-5\nwww.nature.com/scientificreports/\nidentified the location in terms of latitude and longitude of 41°1’56.9” , −79°27’14” , respectively. The location \nnumbers in degrees, minutes, and seconds accurately matched the true values in the original well record. \nHowever, as instructed by the prompt, the extracted location should be in decimal degrees, instead of degrees, \nminutes, and seconds. Therefore, another implicit task for the LLM is to convert the numbers from degrees, \nminutes, and seconds to decimal degrees, which needs a certain degree of mathematical skill. As shown in its \noutput, Llama 2 70B directly converted the latitude of 41°1’56.9” to 41.02733333333334. However, the correct \nconversion should result in 41.032472. Despite the two values being very close, a slight difference remained.\nThe corresponding information extraction results with incorporating the chain-of-thought strategy are \npresented in Table 6. It is interesting to observe that, with the chain-of-thought employed, Llama 2 70B model \nresulted in the same or even worse extraction performance than that achieved without using the chain-of-thought \nstrategy. Specifically, Llama 2 70B with two strategies resulted in the same level of accuracy for depth extraction. \nFor location extraction, Llama 2 70B with the chain-of-thought strategy yielded 60% in terms of AOS, which is \nlower than the 70% by Llama 2 70B without chain-of-thought strategy. The comparison presented here reveals \nthat the chain-of-thought strategy offered limited improvement for location and depth information extractions \nfor Llama 2 70B. It is anticipated that if a post-processing procedure is applied to convert the location units from \ndegrees, minutes, and seconds to decimal degrees, the results could be potentially improved. However, this is \nbeyond the current scope of this paper. In addition, combining LLMs with external tools through the strategy of \nfunction calling may be one potential solution to this precise mathematical problem.\nComparison with other LLMs\nIn this work, we also tested the information extraction workflow using three additional models, including \nMixtral 8×7B, Llama 3.1 70B, and Llama 3.1 405B. For the Mixtral 8 ×7B model, the results are presented in \nTable 7. Interestingly, the Mixtral 8×7B did not yield better information extraction results compared to Llama \n2 70B model used in this study.\nScenario Location Depth\nWith Chain-of-thought 60% 90%\nWithout Chain-of-thought 70% 90%\nTable 6. Comparison of information extraction results for Pennsylvania well records using Llama 2 70B Model \nwith and without implementing of chain-of-thought strategy.\n \nFig. 6. An example of LLM output for one Pennsylvania well record with implementing chain-of-thought. The \ntrue latitude, longitude, and depth are 41◦1’56.9” , -79◦27’14” , and 6038 ft, respectively.\n \nScientific Reports |        (2024) 14:31702 10| https://doi.org/10.1038/s41598-024-81846-5\nwww.nature.com/scientificreports/\nOn the other hand, the Llama 3.1 models were just released in July 2024. We applied the SambaNova Platform \n(https://sambanova.ai/) to implement the full Llama 3.1 70B and 405B models. Here, we used the Llama 3.1 \nmodels to extract information for the 10 Pennsylvania well records only. Our results showed an improved \nperformance when Llama 3.1 models were used. Specifically, the values of AOS for location and depth reached \n80% and 90% with the Llama 3.1 70B model, and 90% and 100% with the Llama 3.1 405B model. The Llama \n3.1 405B model extracted all correct depth information, including the case with a depth of “0” . It made only \none error in location extraction out of 10 documents in terms of AOS. This reveals that the Llama 3.1 405B \nmodel has stronger mathematical capabilities for converting units of latitude and longitude. This additional \nresult demonstrates the potential of more powerful and new models to achieve higher accuracy.\nDiscussions\nIn this section, we briefly discuss the potential impacts of the developed workflow, the current challenges, and \npotential opportunities for applying LLMs to tasks such as information extraction. This discussion is based \non our results from extracting well location and depth data using 160 documents. In addition, given that the \ndevelopment of LLMs is progressing rapidly, it is possible that some of the information summarized here may \nnot accurately reflect the latest advancements in LLMs.\nPotential impacts\nThe developed LLM-based information extraction framework has great potential to accelerate the document \ndigitization process. We expect that this workflow can save significant time and reduce costs for large-scale \ndocument information extraction tasks. Since the LLM-based workflow can operate continuously and in parallel, \nthe efficiency of information extraction could be improved. Although the workflow was developed for orphaned \nwell characterization task, it has a wide range of potential applications, as similar large-scale data extraction \nchallenges exist across various fields.\nIt is worth noting that the information extraction framework may not achieve 100% accuracy in locating \nwells and identifying depth information from documents, as seen, for example, in the Pennsylvania case study. \nHigh accuracy is likely necessary for the practical application of this framework in some real-world scenarios, as \ninaccuracies could result in significant operational costs. Therefore, enhancing the accuracy of this framework as \nmuch as possible is recommended. In practice, this methodology can be combined with other techniques to more \nreliably identify well locations for orphaned well applications. Examples include remote sensing technologies, \nsuch as aero-magnetometers and fixed-wing drones equipped with magnetometers, as reported by O’Malley et \nal.41.\nEnhance text conversion quality from historical documents\nAs introduced previously, the current information extraction tasks require that the original historical documents \nbe converted to texts before feeding into LLMs. This is because the LLMs employed in this work are designed to \nprocess textual inputs. Thus, the tasks heavily depend on the accuracy of the text conversion process used (e.g., \nOCR). However, even the best text conversion techniques still struggle to achieve 100% accurate text conversions \nfrom documents such as PDFs and images. To deal with this challenge, it is recommended to further advance \ntext extraction techniques to improve the accuracy and quality. Integrating computer vision techniques or \nmachine learning algorithms could be a potential area. An alternative path to improving text extraction quality \nis to utilize large multi-modal models that can extract textual information from the images directly. This is a \npromising direction for future research.\nImprove the capabilities of LLMs\nThe technology of LLMs advances rapidly in terms of new models, increased parameter sizes, and capabilities42. \nHowever, given that numerous LLMs are available from both the private and public domains, exploring other \nLLMs is necessary to get a better result. In this paper, we mainly focused on testing Llama 2 models. As discussed \nin Section 3, Llama 2, despite the use of various prompts, changes in model parameter sizes, and the incorporation \nof the chain-of-thought strategy, could not achieve precisely correct information extraction from historical well \ndocuments. Therefore, it is worth testing other LLMs for the same task.\nMany commercial LLM-based tools are available for document processing and information extraction, \nincluding Amazon Textract, OpenAI’s GPT-4, and Google Document AI. For instance, we used Google \nDocument AI’s Enterprise OCR for the text conversion task in this study. Generally, these commercial models \nor tools deliver better performance than some open-source models, likely due to their larger model sizes. \nHowever, commercial LLMs have certain limitations compared to smaller open-source models. First, due to \ntheir commercial nature, users must pay for access, which can increase costs. Second, proprietary models like \nGPT-4 come with potential data security concerns. For example, some cloud-based tools require documents to \nbe uploaded to the cloud for processing. These requirements may limit their applicability in certain industrial \nState name Location Depth\nColorado 99.33% 97.33%\nPennsylvania 30% 50%\nTable 7. Information extraction results using Mixtral 8×7B model with Prompt 4 for Colorado and \nPennsylvania well reports.\n \nScientific Reports |        (2024) 14:31702 11| https://doi.org/10.1038/s41598-024-81846-5\nwww.nature.com/scientificreports/\nand governmental contexts. Smaller open-source models, such as Llama 2 70B, can offer a balanced solution, \nproviding cost-effective and more secure options for various tasks. Specifically, once downloaded, they can be \nused directly on appropriate hardware without incurring additional costs related to utilization of the models. \nAdditionally, they can operate offline and locally, without the need to upload data to a cloud environment. Given \nthe rapid development and advancement of LLMs, it is expected that more advanced open-source LLMs will \nbecome available to the public.\nAnother opportunity lies in fine-tuning the pre-trained LLMs for specific tasks. In this work, we focused \nsolely on the zero-shot learning strategy, without performing any fine-tuning. However, fine-tuning LLMs could \npotentially be a better option, if feasible. Currently, we are investigating the improvement of fine-tuned LLMs \nfor information extraction and will report their findings on performance in a future publication. Furthermore, \nit is also possible to incorporate large multi-modal models for information extraction. Specifically, these models \ncan directly take images or PDFs as inputs, eliminating the need for text conversion using OCR techniques. \nAlthough not employed in this study, it is also advisable to implement some post-processing procedures to \nenhance the information extraction performance.\nOvercome the challenges of extreme hardware requirements\nIn order to use these LLMs offline, we must meet the hardware requirements, especially regarding GPUs due to \nthe extremely large size of the LLMs. For example, according to the Hugging Face data repository, the total size \nof the standard version of Llama 2 70B-chat-hf is approximately 280 GB. Additionally, Hugging Face suggests \nusing 4 × NVIDIA A100 GPUs for the deployment of Llama 2 70B models. While using the pre-trained LLMs as \npresented in this paper does not demand extensive computational resources, it still requires higher-end GPUs to \nrun. In the information extraction task, we utilized an NVIDIA RTX A6000 GPU with 48 GB of memory. Even \nwith this GPU, we encountered difficulties loading the full standard version of Llama 2 70B model. This was the \nreason for using the quantized Llama 2 models in this study. Specifically, when we applied 4-bit quantization \nto the Llama-2-70B-chat-GPTQ model, the GPU memory usage was approximately 42 GB according to \nStoelinga43, which fit within the available memory of an NVIDIA RTX A6000. Therefore, the extreme hardware \nrequirements may hinder the wide applications of LLMs. One way to address this challenge is through using \nmore advanced GPUs. Given the recent rapid advancements in GPU technology, the situation should continue \nto improve. Additionally, the development of smaller LLMs could also be a viable solution. Another alternative \nis to use commercial LLMs that are only available through an API if costs and data security are not concerns.\nConcluding remarks\nIn this work, we presented an LLM-based workflow to extract vital information from well records for orphaned \nwell management, including the well’s location and depth. Extracting data from historic records is currently \na time-consuming and costly process. The information contained in well records is critically important for \nsuccessful plugging operations to reduce environmental impacts such as methane leakage from wellbores. To \ndemonstrate the capability of information analysis workflow, we primarily focused on Llama 2 models, which are \npublicly available. To facilitate information extraction, we developed multiple prompts, varying the instructions \nfrom the simplest to the most complex one. Different variants of Llama 2 were also evaluated, including the 7B, \n13B, and 70B models. Additionally, we also employed the chain-of-thought approach in an attempt to enhance \nperformance. We tested the developed workflow using a dataset of 160 well records. Although this number is \nquite small, the goal of this paper is to prove the concept of this method. We emphasize that these forms are used \nonly for validation of the approach, not for training the models. The development of an information extraction \nframework capable of handling much larger datasets of well documented information is an ongoing project.\nSeveral major conclusions can be drawn from the results. First, the content of the prompt impacts the final \nextraction results, even when an identical LLM is used. In this work, we found that Llama 2 70B model with \nPrompt 4 led to the best performance. The general trend is that the information extraction performance improves \nwith the complexity of the prompt instructions. Therefore, it is recommended to optimize prompt content before \nusing LLMs. Second, the size of the model is an important parameter that influences the result. With Llama 2 \nmodels, better performance was often obtained when a larger model was used. For example, Llama 2 70B model \noutperformed the smaller models, including the 7B and 13B variants. Third, although Llama 2 models achieved \n100% accuracy for the Colorado reports, they still had difficulties in correctly extracting information from some \nPennsylvania well reports. For example, Llama 2 70B extracted the correct location information in the units of \ndegrees, minutes, and seconds after incorporating a chain-of-thought strategy, but it did not accurately convert \nit into decimal degrees as instructed.\nWhile the developed workflow achieved good performance, especially for the PDF-based documents, \nopportunities for further improvement still remain. These include: (1) improving the quality of text conversion \nfrom historical documents, since the current workflow relies heavily on that; (2) fine-tuning the pre-trained \nLLMs for this specific task using a smaller dataset; (3) executing these information extraction tasks on higher-\nend hardware to enhance the results; (4) utilizing large multi-modal models that can directly process PDFs and \nimages, thereby eliminating the need for text extraction; and (5) utilization of LLM function calling techniques \nto aid the LLM with routine tasks like converting. These techniques could automate significant portions of the \nextraction workflow, accelerating the plugging of abandoned wells and enabling large-scale data collection for \nresearch purposes.\nOpen research section\nThe well documents analyzed in this manuscript are publicly available. Specifically, Colorado records were \nacquired from the Colorado Energy and Carbon Management Commission’s online system (COGIS): ECMC \nScientific Reports |        (2024) 14:31702 12| https://doi.org/10.1038/s41598-024-81846-5\nwww.nature.com/scientificreports/\nData (state.co.us), and Pennsylvania records were acquired from the Pennsylvania Geological Survey’s EDWIN \nonline tool: Home - EDWIN Subscriptions (pa.gov). We have permission to use these well records for the \nanalysis in this paper.\nReceived: 21 May 2024; Accepted: 29 November 2024\nReferences\n 1. Boutot, J., Peltz, A. S., McVay, R. & Kang, M. Documented orphaned oil and gas wells across the United States. Environmental \nScience & Technology. 56, 14228–14236 (2022).\n 2. Merrill, M. D., Grove, C. A., Gianoutsos, N. J. & Freeman, P . A. Analysis of the United States documented unplugged orphaned oil \nand gas well dataset. Technical Report from US Geological Survey. (2023).\n 3. IOGCC. Idle and orphan oil and gas wells: State and provincial regulatory strategies 2021. Technical Report from Interstate Oil and \nGas Compact Commission (IOGCC) (2021).\n 4. EPA. Inventory of U.S. greenhouse gas emissions and sinks: 1990-2020. Technical Report from United States Environmental \nProtection Agency (EPA) (2022).\n 5. Kang, M. et al. Environmental risks and opportunities of orphaned oil and gas wells in the United States. Environmental Research \nLetters. 18, 074012. https://doi.org/10.1088/1748-9326/acdae7 (2023).\n 6. Raimi, D., Krupnick, A. J., Shah, J.-S. & Thompson, A. Decommissioning orphaned and abandoned oil and gas wells: New estimates \nand cost drivers. Environ. Sci. Technol. 55, 10224–10230. https://doi.org/10.1021/acs.est.1c02234 (2021).\n 7. Eikvil, L. Optical character recognition. citeseer. ist. psu. edu/142042. html 26 (1993).\n 8. Chaudhuri, A. et al. Optical character recognition systems. (Springer, 2017).\n 9. Chang, Y . et al. A survey on evaluation of large language models.  ACM Transactions on Intelligent Systems and Technology. (2023).\n 10. Topsakal, O. & Akinci, T. C. Creating large language model applications utilizing langchain: A primer on developing llm apps fast. \nInternational Conference on Applied Engineering and Natural Sciences. 1, 1050–1056 (2023).\n 11. Ma, Z., Kim, Y . D., Volkov, O. & Durlofsky, L. J. Optimization of subsurface flow operations using a dynamic proxy strategy. \nMathematical Geosciences. 54, 1261–1287. https://doi.org/10.1007/s11004-022-10020-2 (2022).  \n 12. Ma, Z. & Leung, J. Y . Design of warm solvent injection processes for heterogeneous heavy oil reservoirs: A hybrid workflow of \nmulti-objective optimization and proxy models. Journal of Petroleum Science and Engineering. 191, 107186.  h t t p s : / / d o i . o r g / 1 0 . 1 0 1 \n6 / j . p e t r o l . 2 0 2 0 . 1 0 7 1 8 6     (2020).\n 13. Santos, J. E. et al. Development of the senseiver for efficient field reconstruction from sparse observations. Nature Machine \nIntelligence. 5, 1317–1325 (2023).\n 14. Zhang, B., Ma, Z., Zheng, D., Chalaturnyk, R. J. & Boisvert, J. Upscaling shear strength of heterogeneous oil sands with interbedded \nshales using artificial neural network. SPE Journal. 28, 737–753 (2023).\n 15. Y an, B., Harp, D. R., Chen, B. & Pawar, R. J. Improving deep learning performance for predicting large-scale geological co 2 \nsequestration modeling through feature coarsening. Scientific Reports. 12, 20667 (2022).\n 16. Ma, Z., Guo, Q., Viswanathan, H., Pawar, R. & Chen, B., Deep Learning Assisted History Matching and Forecasting: Applied to the \nIllinois Basin—Decatur Project (IBDP). Available at SSRN: https://ssrn.com/abstract=5019810 or  h t t p : / / d x . d o i . o r g / 1 0 . 2 1 3 9 / s s r n . 5 \n0 1 9 8 1 0     (2024).\n 17. Srinivasan, S. et al. A machine learning framework for rapid forecasting and history matching in unconventional reservoirs. \nScientific Reports. 11, 21730. https://doi.org/10.1038/s41598-021-01023-w (2021).\n 18. Gao, K. & Modrak, R. T. Machine learning inference of random medium properties. IEEE Transactions on Geoscience and Remote \nSensing. 62, 1–13. https://doi.org/10.1109/TGRS.2024.3367541 (2024).\n 19. Minaee, S. et al. Large language models: A survey. arXiv preprint arXiv:2402.06196 ( 2024).\n 20. Pan, S. et al. Unifying large language models and knowledge graphs: A roadmap. IEEE Transactions on Knowledge and Data \nEngineering (2024).\n 21. Koshkin, R., Sudoh, K. & Nakamura, S. Transllama: Llm-based simultaneous translation system. arXiv:2402.04636 (2024).\n 22. Sun, X. et al. Sentiment analysis through llm negotiations. arXiv:2311.01876 (2023).\n 23. Zhuang, Y ., Yu, Y ., Wang, K., Sun, H. & Zhang, C. Toolqa: A dataset for llm question answering with external tools. In Oh, A. et al. \n(eds.) Advances in Neural Information Processing Systems., vol. 36, 50117–50143 (Curran Associates, Inc., 2023).\n 24. Wang, Y ., Wang, W ., Joty, S. & Hoi, S. C.  H. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code \nunderstanding and generation. arXiv:2109.00859 (2021). 2109.00859.\n 25. Shekhar, S. et al. Towards optimizing the costs of llm usage. arXiv:2402.01742 ( 2024).\n 26. Tan, T. F . et al. Fine-tuning large language model (llm) artificial intelligence chatbots in ophthalmology and llm-based evaluation \nusing GPT-4. arXiv:2402.10083 (2024).\n 27. Foroumandi, E. et al. ChatGPT in hydrology and earth sciences: Opportunities, prospects, and concerns. Water Resources Research.  \n59, e2023WR036288. https://doi.org/10.1029/2023WR036288 (2023).\n 28. Touvron, H. et al. Llama: Open and efficient foundation language models. arXiv:2302.13971 (2023).\n 29. Touvron, H. et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).\n 30. Huang, J. et al. Large language models can self-improve. arXiv preprint arXiv:2210.11610 (2022).\n 31. Wei, J. et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing \nSystems. 35, 24824–24837 (2022).\n 32. TheBloke. Llama 2 70b chat - gptq (2024).\n 33. Wolf, T. et al. Huggingface’s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771 (2019).\n 34. Frantar, E., Ashkboos, S., Hoefler, T. & Alistarh, D. GPTQ: Accurate post-training quantization for generative pre-trained \ntransformers. arXiv preprint arXiv:2210.17323 (2022).\n 35. Pdftotext. pdftotext - portable document format (pdf) to text converter (version 3.00). software available at  h t t p s : / / l i n u x . d i e . n e t / m \na n / 1 / p d ft   o t e x t     (2024).\n 36. Google. Google Document AI. online tool available https://cloud.google.com/document-ai?hl=en (2024).\n 37. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I. et al. Improving language understanding by generative pre-training. \nOpenAI (2018).\n 38. Karney, C. Geographiclib.  online at ( 2015).\n 39. Liu, V . & Chilton, L. B. Design guidelines for prompt engineering text-to-image generative models. In Proceedings of the 2022 CHI \nConference on Human Factors in Computing Systems., 1–23 (2022).\n 40. Reynolds, L. & McDonell, K. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended \nAbstracts of the 2021 CHI Conference on Human Factors in Computing Systems, 1–7 (2021).\n 41. O’Malley, D. et al. Unlocking solutions: Innovative approaches to identifying and mitigating the environmental impacts of \nundocumented orphan wells in the united states. Environmental Science & Technology. 58, 44.  h t t p s : / / d o i . o r g / 1 0 . 1 0 2 1 / a c s . e s t . 4 c 0 2 \n0 6 9     (2024).\nScientific Reports |        (2024) 14:31702 13| https://doi.org/10.1038/s41598-024-81846-5\nwww.nature.com/scientificreports/\n 42. Birhane, A., Kasirzadeh, A., Leslie, D. & Wachter, S. Science in the age of large language models. Nature Reviews Physics. 1–4 \n(2023).\n 43. Substratus, S. Calculating gpu memory for large language models (2023).\nAcknowledgements\nThis work is supported by the U.S. Department of Energy’s Undocumented Orphan Well program through the \nCATALOG consortium (catalog.energy.gov). LA-UR number “LA-UR-24-23837” .\nAuthor contributions\nZhiwei Ma contributed to Methodology, Formal Analysis, Investigation, Writing and Reviewing the Original \nDraft, and Visualization; Javier E. Santos Contributed to Editing and Reviewing the Original Draft; Greg Lackey \nContributed to Data Collection and Editing and Reviewing the Original Draft; Hari Viswanathan Contributed \nto Reviewing the Original Draft, Project Supervision, and Funding Acquisition; Daniel O’Malley Contributed \nto Data Collection, Methodology, Result Discussion, Reviewing the Original Draft, Project Supervision, and \nFunding Acquisition.\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to Z.M.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2024 \nScientific Reports |        (2024) 14:31702 14| https://doi.org/10.1038/s41598-024-81846-5\nwww.nature.com/scientificreports/"
}