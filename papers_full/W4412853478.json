{
    "title": "Multi-model assurance analysis showing large language models are highly vulnerable to adversarial hallucination attacks during clinical decision support",
    "url": "https://openalex.org/W4412853478",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2682811430",
            "name": "Mahmud Omar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2809724045",
            "name": "Vera Sorin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2142851274",
            "name": "Jeremy D. Collins",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2013158180",
            "name": "David Reich",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1968102139",
            "name": "Robert Freeman",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2990165486",
            "name": "Nicholas Gavin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4212339766",
            "name": "Alexander Charney",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2183157241",
            "name": "Lisa Stump",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1966051105",
            "name": "Nicola Luigi Bragazzi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2170298636",
            "name": "GIRISH N. NADKARNI",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1945837108",
            "name": "Eyal Klang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4384561707",
        "https://openalex.org/W4387500346",
        "https://openalex.org/W4394009806",
        "https://openalex.org/W4410543053",
        "https://openalex.org/W3134818558",
        "https://openalex.org/W4409210035",
        "https://openalex.org/W4353015365",
        "https://openalex.org/W4386459994",
        "https://openalex.org/W4405326640",
        "https://openalex.org/W4399803256",
        "https://openalex.org/W4384071683",
        "https://openalex.org/W2906826669",
        "https://openalex.org/W4405173238",
        "https://openalex.org/W4388585881",
        "https://openalex.org/W4398203672",
        "https://openalex.org/W4401542090",
        "https://openalex.org/W6810242208",
        "https://openalex.org/W4403970945",
        "https://openalex.org/W4389108554",
        "https://openalex.org/W4400688542",
        "https://openalex.org/W4309674289",
        "https://openalex.org/W4413328432"
    ],
    "abstract": "LLMs are highly susceptible to adversarial hallucination attacks, frequently generating false clinical details that pose risks when used without safeguards. While prompt engineering reduces errors, it does not eliminate them.",
    "full_text": null
}