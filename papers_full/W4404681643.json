{
  "title": "Advancements and Applications of Large Language Models in Natural Language Processing: A Comprehensive Review",
  "url": "https://openalex.org/W4404681643",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4383525039",
      "name": "Mengchao Ren",
      "affiliations": [
        "University of California, Irvine"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4287777801",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2784121710",
    "https://openalex.org/W4387723579",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W3197782487",
    "https://openalex.org/W4400702838",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W4396788662",
    "https://openalex.org/W4399428753",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W4288089799"
  ],
  "abstract": "Abstract. Large language models (LLMs) have revolutionized the field of natural language processing (NLP), demonstrating remarkable capabilities in understanding, generating, and manipulating human language. This comprehensive review explores the development, applications, optimizations, and challenges of LLMs. This paper begin by tracing the evolution of these models and their foundational architectures, such as the Transformer, GPT, and BERT. We then delve into the applications of LLMs in natural language understanding tasks, including sentiment analysis, named entity recognition, question answering, and text summarization, highlighting real-world use cases. Next, we examine the role of LLMs in natural language generation, covering areas such as content creation, language translation, personalized recommendations, and automated responses. We further discuss LLM applications in other NLP tasks like text style transfer, text correction, and language model pre-training. Subsequently, we explore techniques for optimizing and improving LLMs, including model compression, explainability, robustness, and security. Finally, we address the challenges posed by the significant computational requirements, sample inefficiency, and ethical considerations surrounding LLMs. We conclude by discussing potential future research directions, such as efficient architectures, few-shot learning, bias mitigation, and privacy-preserving techniques, which will shape the ongoing development and responsible deployment of LLMs in NLP.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6470745801925659
    },
    {
      "name": "Natural language processing",
      "score": 0.5086954236030579
    },
    {
      "name": "Linguistics",
      "score": 0.373405784368515
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3357527256011963
    },
    {
      "name": "Philosophy",
      "score": 0.060977280139923096
    }
  ]
}