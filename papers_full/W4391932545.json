{
  "title": "Large Language Models for Mental Health Applications: Systematic Review (Preprint)",
  "url": "https://openalex.org/W4391932545",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2097093968",
      "name": "Zhijun Guo",
      "affiliations": [
        "University College London",
        "Farr Institute"
      ]
    },
    {
      "id": "https://openalex.org/A4207927760",
      "name": "Alvina  Lai",
      "affiliations": [
        "Farr Institute",
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A2165986366",
      "name": "Johan H Thygesen",
      "affiliations": [
        "Farr Institute",
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A2112187065",
      "name": "Joseph Farrington",
      "affiliations": [
        "Farr Institute",
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A4292334021",
      "name": "Thomas Keen",
      "affiliations": [
        "Farr Institute",
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A2123425403",
      "name": "Kezhi Li",
      "affiliations": [
        "University College London",
        "Farr Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4361218589",
    "https://openalex.org/W1969577117",
    "https://openalex.org/W3013038708",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W4389523980",
    "https://openalex.org/W2623779865",
    "https://openalex.org/W4390202391",
    "https://openalex.org/W4324373918",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4379164629",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4232356144",
    "https://openalex.org/W4380737876",
    "https://openalex.org/W3089862415",
    "https://openalex.org/W2005501262",
    "https://openalex.org/W4224942983",
    "https://openalex.org/W4294975644",
    "https://openalex.org/W3096405390",
    "https://openalex.org/W3176528293",
    "https://openalex.org/W4206478453",
    "https://openalex.org/W4210455994",
    "https://openalex.org/W3176169354",
    "https://openalex.org/W3211311205",
    "https://openalex.org/W3203366686",
    "https://openalex.org/W2586790131",
    "https://openalex.org/W2891575196",
    "https://openalex.org/W2803193013",
    "https://openalex.org/W175750906",
    "https://openalex.org/W4292388466",
    "https://openalex.org/W4309229269",
    "https://openalex.org/W4214873408",
    "https://openalex.org/W4366317365",
    "https://openalex.org/W3173209146",
    "https://openalex.org/W3173498569",
    "https://openalex.org/W4382394716",
    "https://openalex.org/W4376491640",
    "https://openalex.org/W4366591012",
    "https://openalex.org/W4386537453",
    "https://openalex.org/W4384470461",
    "https://openalex.org/W4384926409",
    "https://openalex.org/W4385564356",
    "https://openalex.org/W4378470708",
    "https://openalex.org/W4372052843",
    "https://openalex.org/W3174116563",
    "https://openalex.org/W4385572601",
    "https://openalex.org/W4366333677",
    "https://openalex.org/W4385459268",
    "https://openalex.org/W4382918229",
    "https://openalex.org/W3013568653",
    "https://openalex.org/W4225327430"
  ],
  "abstract": "<sec> <title>BACKGROUND</title> Large language models (LLMs) are advanced artificial neural networks trained on extensive datasets to accurately understand and generate natural language. While they have received much attention and demonstrated potential in digital health, their application in mental health, particularly in clinical settings, has generated considerable debate. </sec> <sec> <title>OBJECTIVE</title> This systematic review aims to critically assess the use of LLMs in mental health, specifically focusing on their applicability and efficacy in early screening, digital interventions, and clinical settings. By systematically collating and assessing the evidence from current studies, our work analyzes models, methodologies, data sources, and outcomes, thereby highlighting the potential of LLMs in mental health, the challenges they present, and the prospects for their clinical use. </sec> <sec> <title>METHODS</title> Adhering to the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines, this review searched 5 open-access databases: MEDLINE (accessed by PubMed), IEEE Xplore, Scopus, JMIR, and ACM Digital Library. Keywords used were (&lt;i&gt;mental health&lt;/i&gt; OR &lt;i&gt;mental illness&lt;/i&gt; OR &lt;i&gt;mental disorder&lt;/i&gt; OR &lt;i&gt;psychiatry&lt;/i&gt;) AND (&lt;i&gt;large language models&lt;/i&gt;). This study included articles published between January 1, 2017, and April 30, 2024, and excluded articles published in languages other than English. </sec> <sec> <title>RESULTS</title> In total, 40 articles were evaluated, including 15 (38%) articles on mental health conditions and suicidal ideation detection through text analysis, 7 (18%) on the use of LLMs as mental health conversational agents, and 18 (45%) on other applications and evaluations of LLMs in mental health. LLMs show good effectiveness in detecting mental health issues and providing accessible, destigmatized eHealth services. However, assessments also indicate that the current risks associated with clinical use might surpass their benefits. These risks include inconsistencies in generated text; the production of hallucinations; and the absence of a comprehensive, benchmarked ethical framework. </sec> <sec> <title>CONCLUSIONS</title> This systematic review examines the clinical applications of LLMs in mental health, highlighting their potential and inherent risks. The study identifies several issues: the lack of multilingual datasets annotated by experts, concerns regarding the accuracy and reliability of generated content, challenges in interpretability due to the “black box” nature of LLMs, and ongoing ethical dilemmas. These ethical concerns include the absence of a clear, benchmarked ethical framework; data privacy issues; and the potential for overreliance on LLMs by both physicians and patients, which could compromise traditional medical practices. As a result, LLMs should not be considered substitutes for professional mental health services. However, the rapid development of LLMs underscores their potential as valuable clinical aids, emphasizing the need for continued research and development in this area. </sec> <sec> <title>CLINICALTRIAL</title> PROSPERO CRD42024508617; https://www.crd.york.ac.uk/prospero/display_record.php?RecordID=508617 </sec>",
  "full_text": "JMIR Preprints Guo et al\nLarge Language Model for Mental Health: A\nSystematic Review\n Zhijun Guo, Alvina Lai, Johan Thygesen, Joseph Farrington, Thomas Keen, Kezhi\nLi\nSubmitted to: JMIR Mental Health\non: February 18, 2024\nDisclaimer: © The authors. All rights reserved. This is a privileged document currently under peer-review/community\nreview. Authors have provided JMIR Publications with an exclusive license to publish this preprint on it's website for\nreview purposes only. While the final peer-reviewed paper may be licensed under a CC BY license on publication, at this\nstage authors and publisher expressively prohibit redistribution of this draft paper other than for review purposes.\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nTable of Contents\nOriginal Manuscript ....................................................................................................................................................................... 5\nSupplementary Files ..................................................................................................................................................................... 50\nFigures ......................................................................................................................................................................................... 51\nFigure 1 ...................................................................................................................................................................................... 52\nFigure 2 ...................................................................................................................................................................................... 53\nFigure 3 ...................................................................................................................................................................................... 54\nMultimedia Appendixes ................................................................................................................................................................. 55\nMultimedia Appendix 1 .................................................................................................................................................................. 56\nMultimedia Appendix 2 .................................................................................................................................................................. 56\nMultimedia Appendix 3 .................................................................................................................................................................. 56\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nLarge Language Model for Mental Health: A Systematic Review\nZhijun Guo1; Alvina Lai1; Johan Thygesen1; Joseph Farrington1; Thomas Keen1, 2; Kezhi Li1\n1Institute of Health Informatics University College, London London GB\n2GOS Institute of Child Health, University College London London GB\nCorresponding Author:\nKezhi Li\nInstitute of Health Informatics University College, London\nInstitute of Health Informatics University College London 222 Euston Road London United Kingdom\nLondon\nGB\nAbstract\nBackground: Large language models (LLMs) have received much attention and show their potential in digital health, while their\napplication in mental health is subject to ongoing debate. This systematic review aims to summarize and characterize the use of\nLLMs in mental health by investigating the strengths and limitations of the latest work in LLMs and discusses the challenges and\nopportunities for early screening, digital interventions, and other clinical applications in mental health.\nObjective: This systematic review aims to summarize how LLMs are used in mental health. We focus on the models, data\nsources, methodologies, and main outcomes in existing work, in order to assess the applicability of LLMs to early screening,\ndigital interventions, and other clinical applications.\nMethods: Adhering to the PRISMA guidelines, this review searched three open-access databases: PubMed, DBLP Computer\nScience Bibliography (DBLP), and IEEE Xplore (IEEE). Keywords used were: (mental health OR mental illness OR mental\ndisorder OR psychology OR depression OR anxiety) AND (large language models OR LLMs OR GPT OR ChatGPT OR BERT\nOR Transformer OR LaMDA OR PaLM OR Claude). We included articles published between January 1, 2017, and September\n1, 2023, and excluded non-English articles.\nResults: In total, 32 articles were evaluated, including mental health analysis using social media datasets (n=13), LLMs usage\nfor mental health chatbots (n=10), and other applications of LLMs in mental health (n=9). LLMs exhibit substantial effectiveness\nin classifying and detecting mental health issues and offer more efficient and personalized healthcare to improve\ntelepsychological services. However, assessments also indicate that the current risks associated with the clinical use might\nsurpass their benefits. These risks include inconsistencies in generated text, the production of hallucinatory content, and the\nabsence of a comprehensive ethical framework.\nConclusions: This systematic review examines the clinical applications of LLMs in mental health, highlighting their potential\nand their inherent risks. The study identifies significant concerns, including inherent biases in training data, ethical dilemmas,\nchallenges in interpreting the 'black box' nature of LLMs, and concerns about the accuracy and reliability of the content they\nproduce. Consequently, LLMs should not be considered substitutes for professional mental health services. Despite these\nchallenges, the rapid advancement of LLMs may highlight their potential as new clinical tools, emphasizing the need for\ncontinued research and development in this field.\n(JMIR Preprints 18/02/2024:57400)\nDOI: https://doi.org/10.2196/preprints.57400\nPreprint Settings\n1) Would you like to publish your submitted manuscript as preprint?\nPlease make my preprint PDF available to anyone at any time (recommended).\nPlease make my preprint PDF available only to logged-in users; I understand that my title and abstract will remain visible to all users.\nOnly make the preprint title and abstract visible.\nNo, I do not wish to publish my submitted manuscript as a preprint.\n2) If accepted for publication in a JMIR journal, would you like the PDF to be visible to the public?\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nYes, please make my accepted manuscript PDF available to anyone at any time (Recommended). \nYes, but please make my accepted manuscript PDF available only to logged-in users; I understand that the title and abstract will remain visible to all users (see Important note, above). I also understand that if I later pay to participate in <a href=\"https://jmir.zendesk.com/hc/en-us/articles/360008899632-What-is-the-PubMed-Now-ahead-of-print-option-when-I-pay-the-APF-\" target=\"_blank\">JMIR’s PubMed Now! service</a> service, my accepted manuscript PDF will automatically be made openly available.\nYes, but only make the title and abstract visible (see Important note, above). I understand that if I later pay to participate in  <a href=\"https://jmir.zendesk.com/hc/en-us/articles/360008899632-What-is-the-PubMed-Now-ahead-of-print-option-when-I-pay-the-APF-\" target=\"_blank\">JMIR’s PubMed Now! service</a> service, my accepted manuscript PDF will automatically be made openly available.\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nOriginal Manuscript\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nLarge Language Model for Mental Health: A Systematic\nReview\n______________________________________________________________________________\n____________\n______________________________________________________________________________\n____________\nZhijun Guo 1 ; Alvina Lai 1 ; Johan Hilge Thygesen 1 ; Joseph Farrington 1 ; Thomas Keen 1 , 2;\nKezhi Li 1\n______________________________________________________________________________\n____________\n1 Institute of Health Informatics, University College London\n2 GOS Institute of Child Health, University College London\nCorresponding author:\nKezhi Li\nInstitute of Health Informatics\nUniversity College London\n222 Euston Road\nLondon\nUnited Kingdom\nPhone:  +44 7859 995590\nEmail: ken.li@ucl.ac.uk\n______________________________________________________________________________\n____________\nAbstract\nBackground  \nLarge l anguage m odels ( LLMs) h ave r eceived m uch a ttention a nd s how t heir\npotential i n d igital h ealth, w hile t heir a pplication i n m ental h ealth i s s ubject\nt o  o n g o i n g  d e b a t e .  T h i s  s y s t e m a t i c  r e v i e w  a i m s  t o  s u m m a r i z e  a n d\ncharacterize the use of LLMs in mental health by investigating the strengths\nand limitations of the latest work in LLMs and discusses the challenges and\no p p o r t u n i t i e s  f o r  e a r l y  s c r e e n i n g ,  d i g i t a l  i n t e r v e n t i o n s ,  a n d  o t h e r  c l i n i c a l\napplications in mental health. \nObjective:\nT h i s  s y s t e m a t i c  r e v i e w  a i m s  t o  s u m m a r i z e  h o w  L L M s  a r e  u s e d  i n  m e n t a l\nh e a l t h .  W e  f o c u s  o n  t h e  m o d e l s ,  d a t a  s o u r c e s ,  m e t h o d o l o g i e s ,  a n d  m a i n\no u t c o m e s  i n  e x i s t i n g  w o r k ,  i n  o r d e r  t o  a s s e s s  t h e  a p p l i c a b i l i t y  o f  L L M s  t o\nearly screening, digital interventions, and other clinical applications.\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nMethods:\nAdhering  to  the  PRISMA  guidelines,  this  review  searched  three  open-access  databases:\nPubMed, DBLP Computer Science Bibliography (DBLP), and IEEE Xplore (IEEE). Keywords used\nwere: (mental health OR mental illness OR mental disorder OR psychology OR depression OR\nanxiety) AND (large language models OR LLMs OR GPT OR ChatGPT OR BERT OR Transformer\nOR LaMDA OR PaLM OR Claude). We included articles published between January 1, 2017, and\nSeptember 1, 2023, and excluded non-English articles. \nResults:\nIn  total,  32  articles  were  evaluated,  including  mental  health  analysis  using  social  media\ndatasets (n=13), LLMs usage for mental health chatbots (n=10), and other applications of LLMs\nin  mental  health  (n=9).  LLMs  exhibit  substantial  effectiveness  in  classifying  and  detecting\nmental  health  issues  and  offer  more  efficient  and  personalized  healthcare  to  improve\ntelepsychological services. However , assessments also indicate that the current risks associated\nwith  the  clinical  use  might  surpass  their  benefits.  These  risks  include  inconsistencies  in\ngenerated text, the production of hallucinatory content, and the absence of a comprehensive\nethical framework.\nConclusions:\nThis systematic review examines the clinical applications of LLMs in mental health, highlighting\ntheir potential and their inherent risks. The study identifies significant concerns, including\ninherent biases in training data, ethical dilemmas, challenges in interpreting the 'black box'\nnature of LLMs, and concerns about the accuracy and reliability of the content they produce.\nConsequently,  LLMs  should  not  be  considered  substitutes  for  professional  mental  health\nservices. Despite these challenges, the rapid advancement of LLMs may highlight their potential\nas new clinical tools, emphasizing the need for continued research and development in this\nfield.\nKeywords\nLarge language models; Mental health; Digital healthcare; ChatGPT; BERT\n__________________________________________________________________________________________\n1. Introduction and Background\n1.1Mental Health\nMental health, a critical component of overall well-being, is at the forefront of global health\nchallenges  1. In 2019, an estimated 970 million individuals worldwide suffered from mental\nillness, accounting for 12.5% of the global population 2. Anxiety and depression are among the\nmost  prevalent  psychological  conditions,  affecting  301  million  and  280  million  individuals\nrespectively 2. However, they often go undetected or untreated, and the resources allocated to\nthe diagnosis and treatment of mental illness are far less than the negative impact it has on\nsociety  3. Focusing specifically on the UK, the scale of the mental health crisis is significant.\nFigures show that one in six individuals in England reported experiencing a common mental\nhealth problem in a given week  4.  The COVID-19 pandemic has further intensified existing\nmental health challenges globally. Specifically, the World Health Organization (WHO) reported a\n26% rise in anxiety disorders and a 28% increase in major depression disorders within just\none year of the pandemic 5. This escalating crisis underscores the urgent need for innovative\napproaches to mental health. The negative effects of poor mental health are far-reaching, with\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nover 90% of people who die by suicide annually being diagnosed with a mental illness 6. These\nstatistics point to the need to raise awareness of mental health in society and to take proactive\nearly intervention and preventive measures.\nMental  illness  treatment  encompasses  a  range  of  modalities  including  medication,\npsychotherapy, support groups, hospitalization, and complementary & alternative medicine  7.\nHowever,  societal  stigma  attached  to  mental  illnesses  often  deters  people  from  seeking\nappropriate care  8. Many individuals with mental illness are afraid to discuss their condition\nwith others or seek help from a professional psychologist  9. The COVID-19 crisis and global\npandemics have highlighted the importance of digital tools such as telemedicine and apps to\ndeliver care in times of need  10, accelerating a paradigm shift in healthcare.  In this evolving\nlandscape, LLMs offer new possibilities for mental health care delivery and support.\nRecent  technological  advancements  have  revealed  some  unique  advantages  of LLMs  in  the\nmental health area. These models, capable of processing and generating text similar to human\ncommunication, offer a non-judgmental space for individuals to share concerns and receive\nsupport 11. LLMs also contribute to psychoeducation, enhance therapeutic methods, and may\nprovide timely interventions, particularly when traditional mental health resources are limited\nor unavailable 12. Recent data indicates that 23% of people with mental illness report having\nto wait more than 12 weeks to begin treatment, and 43% of adults with mental illness report\nthat  the  long  wait  for  treatment  has  exacerbated  their  conditions  13.  In  this  landscape  of\nlimited healthcare availability, LLMs present a feasible solution for providing timely access to\nmental health services. For instance, a range of mental health chatbots, developed by using\nlanguage  models,  have  been  gaining  recognition,  such  as  Woebot  14 and  Wysa  15.  Both\nchatbots follow the principles of Cognitive Behavioural Therapy. These platforms are designed\nto provide users with self-help tools to help them cope with mental health issues such as stress,\nanxiety, and depression 16.\nMeanwhile,  the  application  of  LLMs  in  the  mental  health  sector  presents  several  risks,\nespecially  concerning  vulnerable  groups.  Challenges  such  as  inconsistencies  in  the  content\ngenerated and the production of 'hallucinatory' content which may mislead or harm users 17.\nGiven these concerns, a thorough and rigorous assessment of LLMs' responsible and effective\nuse in healthcare is essential. The following section will further examine the workings of LLMs,\nand their potential mental health applications, and critically evaluate the opportunities and\nchallenges they introduce.\n1.2Large Language Models\nLLMs represent  significant  advancements in  machine learning  (ML), characterized  by their\nability to understand and generate human-like text with  high accuracy. Distinguished from\ntraditional language models by their scale, LLMs often contain billions of parameters 18. This\nbreakthrough is largely due to the Transformer architecture, a deep neural network structure\nthat employs a 'self-attention' mechanism, developed by Vaswani et al. in 2017. This allows\nLLMs to process information in parallel rather than sequentially, greatly enhancing speed and\ncontextual  understanding  19.  Notable  examples  of  such  state-of-the-art  LLMs  include\nGenerative Pre-trained Transformers (GPT), and Bidirectional Encoder Representations from\nTransformers (BERT), among others (Fig.1). \nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nFig1. Comparative analysis of large language models by parameter size and developer entity. The bar chart\nrepresents the number of parameters in billions for various language models by date of publication, with the\noldest models at the top. The legend is color-coded by the development entity. Data was summarized with the\nlatest models up to September 2023, with data for parameters and developers from GPT to LLaMA adapted from\nthe work of  Thirunavukarasu AJ et al 20.\nLLMs are designed to learn the fundamental statistical patterns of language 21. Initially, these\nmodels are the basis for fine-tuning task-specific models rather than training those models\nfrom scratch. This fine-tuning process involves adjusting a pre-trained model to a specific task\nby further training it on a smaller, task-specific dataset 22. However, with the advent of even\nlarger and more complex models, this fine-tuning step is often unnecessary for a wide range of\ntasks. These advanced LLMs are capable of understanding and executing tasks specified in\nnatural language prompts. A 'prompt' is a natural language text that describes a task that the AI\nshould  perform  23.  While  highly  effective,  one  must  be  cautious  of  'hallucinations'  –  a\nphenomenon where these models confidently generate incorrect or irrelevant outputs 24. This\ncan be particularly challenging in scenarios requiring high accuracy, such as healthcare and\nmedical applications 25-262728.\nThe  existing  literature  includes  a  review  of  the  application  of  ML  and  natural  learning\nprocessing (NLP) in mental health 29, as well as an analysis of LLMs in medicine  20. Studies\nhave demonstrated NLP's efficacy in performing statistical tasks, such as text categorization\nand sentiment analysis  29. Despite these findings, a systematic review of the use of state-of-\nthe-art LLMs specifically for mental health has yet to be conducted. Furthermore, there is a lack\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nof in-depth discussion on the ethical challenges unique to the application of LLMs in various\nmental health contexts. This study aims to fill these gaps by providing a comprehensive review\nof the application of LLMs in mental health, examining the relevant ethical considerations, and\nassessing their ability as tools for early screening of mental health conditions and support in\ntherapeutic interventions. \n2. Methods\nThis  systematic  review  followed  the Preferred  Reporting  Items  for  Systematic  Review  and\nMeta-analysis (PRISMA)  guidelines 30. The protocol was registered on PROSPERO under the\nID: CRD42024508617.\n2.1Inclusion and Exclusion Criteria\nThree major databases were investigated in this systematic review: PubMed, DBLP , and IEEE.\nThese  databases  were  chosen  because  of  their  open  search  capabilities.  The  criteria  for\nselecting  articles  were  as  follows:  We  limited  our  search  to  English-language  publications,\nfocusing on articles published between January 1, 2017, and September 1, 2023. The search\nwas  conducted  from  July  1  to  September  1,  2023,  primarily  targeting  the  titles,  abstracts,\nmodels used, data sources, methodology, and main outcomes of the articles. This timeframe\nwas chosen considering the significant developments in the field of LLMs in 2017, marked\nnotably  by  the  introduction  of  the  Transformer  architecture,  which  has  greatly  influenced\nacademic and public interest in this area.  \nIn this review, the original research articles and available full-text papers have been carefully\nselected  aiming  to  focus  on  the  application  of  LLMs  in  mental  health.  Due  to  the  limited\nliterature specifically addressing the mental health applications of LLMs, we included review\narticles  to  ensure  a  comprehensive  perspective.  Our  selection  criteria  focused  on  direct\napplications, expert evaluations, and ethical considerations related to the use of LLMs in mental\nhealth contexts, with the goal of providing a thorough analysis of this rapidly developing field.\n2.2Search Strategies\nThe  mental  health-related  terms  were  combined  with  LLM  descriptors  using  Boolean\noperators. The search query was: ((mental health OR mental illness OR mental disorder OR\npsychology  OR  depression  OR  anxiety)  AND  (large  language  models  OR  LLMs  OR  GPT  OR\nChatGPT  OR  Bert  OR  Transformer  OR  LaMDA  OR  PaLM  OR  Claude)).  For  the  articles  that\nmatched our search criteria, a meticulous and iterative assessment has been conducted by two\nindependent reviewers (ZG, KL) to ensure each article fell within the scope of LLMs in mental\nhealth (Multimedia Appendix 1). This involved the removal of duplicates followed by a detailed\nmanual evaluation of each article to confirm adherence to our predefined inclusion criteria,\nensuring a comprehensive and focused review.\n2.3 Information Extraction \nWe examined the application scenarios, model architecture, data sources, methodologies used,\nand main outcomes from selected studies on LLMs in mental health. Firstly, we categorized\neach study to clarify their primary goals and applications, providing an overview of how LLMs\nare being utilized in the mental health field. This categorization helps in understanding the\ndiverse ways in which these models are applied. Following that the main model architecture of\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nLLMs used was summarized. Secondly, we conducted a thorough examination of data sources.\nOur analysis covered both public and private datasets used in these studies. Noted that some\nreview  articles  lacked  detail  on  dataset  content,  we  focused  on  providing  comprehensive\ninformation on public datasets, including their origins and sample sizes. After that, various\nmethods employed across different scenarios are investigated. They include data collection\nstrategies  and  analytical  methodologies.  We  examined  their  comparative  structures  and\nstatistical techniques to provide a clear understanding of how these methods are applied in\npractice. Finally, the main outcome of each study was addressed. We documented significant\nresults, aligning them with relevant performance metrics and evaluation criteria, and providing\nquantitative data where applicable to underscore these findings. This allowed us to highlight\nthe efficacy and impact of LLMs in mental health, providing quantitative data where applicable\nto underscore these findings.\n3. Results\n3.1Strategy and Screening Process\nThe PRISMA diagram of the systematic screening process can be seen in Figure. 2.  Our initial\nsearch across three academic databases: PubMed, DBLP , and IEEE yielded 205 papers: 107\nfrom PubMed, 17 from DBLP , and 81 from IEEE. After duplication, 199 unique papers were\nretained.  Subsequent  screening  is  based  on  predefined  inclusion  and  exclusion  criteria,\nnarrowing down the selection to 32 papers included in this review. The reasons for the full-text\nexclusion of 56 papers can be found in Multimedia Appendix 2.\nFig 2. PRISMA flow of selection process.\nPapers were classified into three main categories: mental health analysis using social media\n(n=13), LLMs in mental health chatbots (n=10), and the other applications of LLMs in mental\nhealth (n=9). Figure 3 highlights the significant increase in related publications over the last\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\ntwo years, indicating the emerging nature of LLMs in mental health. \n \nFig 3. Number of articles after keyword search grouped by the year of publication and application field. \nThe black line indicates the total number of articles in each year.\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nTable 1. Summary of the 13 selected articles from the literature on LLMs in mental health  analysis\nusing social media datasets.\nRef. Cases Models Data Sources Methodology Used Main Outcomes\n(Baird et al., 2022)\n31\nAnalysis  of\ntelehealth\ntopics  using\nLLMs\nBERT Twitter (10,689\ntweets)\nData from Twitter at the intersection of\ntelehealth  and  mental  health  was\nprocessed  with  BERT,  optimized  for\nboth pre and during-pandemic periods,\nfollowed  by  human  evaluation  and\nrefinement.\nA  fourfold  increase  in  tweets\nabout telehealth for mental health\nor  substance  abuse  during  the\npandemic  was  revealed,  with  a\nnotable  shift  towards  mental\nhealth topics in discussions about\nfunding and support.\n(Senn et al., 2022)\n32\nDepression\nclassification\nby  using\nLLMs\nRoBERTa 33;\nDistilBERT 34;\ntheir ensemble\ncombinations\nDistress Analysis\nInterview Corpus -\nWizard of Oz 35\nThe  paper  compared  three  BERT\nvariants  (BERT,  RoBERTa,  and\nDistilBERT)  and  four  ensembles  of\nBERT  variants  for  depression\nclassification  using  transcripts  of  12\nclinical interview questions.\nEnsemble  models  of  BERT\nvariants  outperformed  individual\nBERT  models  in  depression\nclassification,  with  the  ensemble\nmodels Ens 2 and Ens achieving\nthe highest mean F1 score of 0.62.\n(Xu  et  al.,  2023)\n36\nMental  health\nprediction  by\nusing LLMs\nAlpaca; Alpaca-\nLoRA; FLAN-\nT5; GPT-3.5/4.\nDreaddit dataset\n(posts from Reddit\nin domains: abuse,\nsocial, anxiety,\nPTSD, and\nfinancial)\nThis  paper  evaluated  several  LLMs\nusing various setups such as zero-shot\nprompting,  few-shot  prompting,  and\ninstruction  fine-tuning  for  mental\nhealth prediction tasks based on online\ntext data.\nInstruction  fine-tuning\nsignificantly  enhanced  LLM\nperformance  for  all  tasks,  with\nMental-Alpaca  and  Mental-\nFLAN-T5  models  surpassing\nGPT-3.5  and  GPT-4  designs;\nethical  concerns  and  limitations\nwere highlighted.\n(Lamichhane et al.,\n2023) 37\nMental  health\nclassification\nby  using\nLLMs\nChatGPT 3.5-\nturbo\nStress Detection\nDataset 38;\nDepression\nDetection Dataset\n39; Suicidality\nDetection 40\nChatGPT  was  employed  to  perform\nmental  health  classification,\nspecifically stress detection, depression\ndetection,  and  suicidality  detection\nusing  labeled  datasets  from  social\nmedia posts.\nChatGPT  showed  potential  for\nmental  health  classifications,\nachieving F1 scores of 0.73, 0.86,\nand  0.37  for  stress  detection,\ndepression  detection,  and\nsuicidality  detection  respectively,\noutperforming a baseline model.\n(Hassan  et  al.,\n2023) 41\nEvaluation  of\nChatGPT  in\nmental health \nChatGPT;\nGPT-3\n11 benchmark\ndatasets, including\nT-SID 42 and\nCAMS 43\nThe paper employed various prompting\nstrategies for ChatGPT, enhanced with\nchain-of-thought  and  emotional  cues,\ncoupled  with  human  evaluations  on\nexplanation quality.\nChatGPT  had  shown  promise  in\nmental  health  care  with  certain\nlimitations,  benefited  from\nemotional  cue  prompts,  and\nsurpassed  GPT-3  in  explanation\nquality.\n(Vajre et al., 2021)\n44\nMental  health\nclassification\nby  using\nLLMs\nPsychBERT Twitter hashtags\nand Subreddit (6\ndomains: anxiety,\nmental health,\nsuicide, etc)\nThe paper developed a taxonomy based\non  HiTOP,  implemented  a  two-stage\nframework  for  mental  health  text\nidentification  and  behavior  detection,\nand  incorporated  interpretability\nThe  introduced  framework,\nincluding the PsychBERT model,\nsurpassed  existing  methods  in\nmental health  behavior detection\nfrom social  media, proving both\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\ncomponents. effective and interpretable.\n(El-Ramly  et  al.,\n2021) 45\nMental  health\nprediction  by\nusing LLMs\nARABERT;\nMARBERT 46\nCairoDep v1.0\n(7,000 depressed\nand non-depressed\nposts in Arabic)\nBERT models, specifically ARABERT\nand  MARBERT,  were  trained  and\ntested.  Data  collection  encompassed\ncrowdsourcing,  Arabic  forums,  and\ntranslated  English  datasets,  using\nPython.\nBoth ARABERT and MARBERT\nmodels  achieved  high  accuracy,\nprecision,  recall,  and  F1-score\nvalues  in  detecting  depression,\nsurpassing  traditional  lexicon-\nbased and ML approaches.\n(Bajaj et al., 2021)\n47\nMental  health\nclassification\nby  using\nLLMs\nTransformer-\nbased\nclassification\nmodels\nSubreddits (50,242\nsamples)\nA framework consisting  of four parts\nwas proposed to analyze Reddit users\npotentially  affected  by  the  pandemic.\nTransformer-based  classification\nmodels  were  applied  to  this  data,\nfocusing  particularly  on  the  March-\nMay period.\n6.4%  of  the  user  base  was\nmentally  healthy  before  the\npandemic.  An  observable\nrelationship  was  found  between\nthe  onset  of  depression  and\nCOVID-19,  with  a  significant\nnumber  of  users  starting  to  post\nabout  their  struggles  during  the\ninitial stages of the pandemic.\n(Vishwakarma  et\nal., 2021) 48\nSpeech\nemotion\nrecognition  by\nusing LLMs\nBERT;\nMultilayer\nperceptron;\nConvolutional\nneural network\n(CNN);\nGenerative\nadversarial\nnetworks\n(GANs)\nISEAR 49;\nEmotion dataset\n50; RAVDESS 51;\nTESSl 52;\nEmo-DB 53\nThe  paper  developed  a  personalized\nmulti-modal  architecture  integrating\ntext,  speech,  and  facial  expressions,\nusing GANs for human-like interaction\nand lip-synced post-emotion analysis.\nThe multi-modal system surpassed\nexisting  single-mode  emotion\ndetection  models  in  predicting\ncumulative  emotional  status  and\noffering  timely  support  through\nenhanced  human-like  GAN-\ngenerated responses.\n(William  et  al.,\n2022) 54\nMental  health\nprediction  by\nusing LLMs\nBERT Reddit's\n\"rsuicidewatch\"\nand\n\"rcasualconversati\non\" sub-forum\n(3,412 data);\nTwitter (from 1st\nJan 2015 to 20th\nSep 2020)\nUsing BERT combined with extractive\nsummarization, data was pre-processed\nand  benchmarked  against  other  text\nclassification techniques for depression\ndetection,  considering  metrics  like\naccuracy and F1-score.\nUsing  BERT  with  extractive\nsummarization  enhanced\ndepression  detection  on  social\nmedia, outperforming base BERT\nand  BiLSTM  models,  though\nXLNet  remained  superior  in\ndetection effectiveness.\n(Kaseb  et  al.,\n2022) 55\nMental  health\nprediction  by\nusing LLMs\nTransformer-\nbased pre-trained\nlanguage models\nTwitter (1,200\nnon-depressed\ntweets and 800\ndepressed tweets)\n56;\nKaggle (1,350k\nVarious  pre-trained  language  models,\nincluding  BERT  and  RoBERTa  33,\nwere trained on a depression detection\ndataset, with RoBERTa then applied to\npseudo-label  datasets  related  to\nCOVID-19  and  vaccinations  for\nThe RoBERTa model achieved a\n78.85%  F1-score  and  revealed,\nthrough  pseudo-labeling,  an\nincrease  in  depression  levels  on\ntweets  during  the  pandemic,\nhighlighting  the  impact  of\n9\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\ntweets) 57;\nKaggle (3,000k\ntweets) 58\ndepression insights. COVID-19 and vaccinations.\n(Zeberga  et  al.,\n2022) 59\nMental  health\nprediction  by\nusing LLMs\nBERT;\nBi-LSTM\nReddit (95,000\nposts);\nTwitter (100,000\ntweets)\nA framework was designed integrating\nBERT,  Bi-LSTM,  word2vec,  and\nknowledge  distillation  for  depression\nand  anxiety  detection  from  social\nmedia.\nThe developed model successfully\ndetected  depression  and  anxiety-\nrelated posts with a 98% accuracy\nrate.\n(Heinz  et  al.,\n2023) 60\nEvaluation  of\nAI  models  in\nmental  health\nassessment\nGPT-3 59 distinct clinical\nvignettes\nThe  paper  employed  a  generative  AI\nmodel, tested it with clinical vignettes,\nand utilized balanced accuracy (BAC),\ngeneralized  linear  mixed-effects\nmodels,  and  odds  ratios  (ORs)  to\nanalyze  domain  knowledge  and\ndemographic bias.\nThe AI model displayed variable\ndiagnostic performance with high\nBAC  for  certain  psychiatric\ndisorders and low BAC for others,\nunderscoring the need for caution\nand  further  development  before\ndeployment  in  critical  healthcare\nsettings.\nTable 2. Summary of the 10 selected articles from the literature on LLMs in mental health chatbot.\nRef. Cases Models Data Sources Methodology Used Main Outcomes\n(Wei  et  al.,  2023)\n61\nDesigned\nmental chatbot\nusing LLMs to\ninvestigate  the\ndesign  factors\nof prompts\nGPT-3 User Self-Reported\n(48 participants)\nUsing  an  online  study  with  48\nparticipants,  four  chatbot  prompt\ndesigns  were  assessed  for  dialogue\nflow  and  user  perception. to  correct\nthese harmful behaviors in AI chatbots.\nChatbots  captured  79%  of  desired\ninformation,  with  prompt  design\nsignificantly  affecting  conversation\nquality and data collection.\n(Lin  et  al.,  2023)\n62\nDesigned\nmodels  using\nLLMs to\ncorrect\nharmful\nbehaviors  in\nAI chatbots\nGPT-3.5 Conservations\nbetween chatbots and\na hypothetical user\nThe  SafeguardGPT  framework  was\nintroduced,  incorporating  four  AI\nagents,  and  its  effectiveness  was\ndemonstrated via a social conversation\nsimulation.\nSafeguardGPT effectively detected and\ncorrected  harmful  chatbot  behaviors,\nbut  challenges  in  evaluation  and\nalignment with human values persisted.\n(Kumar  et  al.,\n2022) 63\nDesigned\nmental chatbot\nfor  managing\nmood  using\nLLMs \nGPT-3 Survey responses\nfrom Amazon\nMechanical Turk\n(945 participants)\nThe  paper  centered  on  the  GPT-3\nchatbot's  prompt  design  dimensions,\nspecifically  identity,  intent,  and\nbehavior, and applied both quantitative\nand  qualitative  analyses  of  user\ninteractions and perceptions.\nUsers found the chatbot was helpful in\nmany  scenarios,  but  raised  concerns\nabout repetitiveness and privacy, with\ncertain  prompt  designs  showing\npromise  around  problem-solving  and\ncognitive behavioral therapy.\n(Lai  et  al.,  2023)\n64\nDesigned\nmental chatbot\nPanGu;\nWenZhong\nPsyQA dataset (5000\nsamples) 65\nThe paper introduced the creation and\nassessment  of  the  Psy-LLM\nThe  Psy-LLM  framework  effectively\ngenerated  coherent  and  relevant\n10\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nin\npsychological\nconsultation\nsettings  using\nLLMs\nModel framework.  It  merged  pre-trained\nLLMs with expert inputs and articles,\nanalyzed data  via  word and sentence\nmetrics, and measured outcomes using\nboth  intrinsic  metrics  and  human\nfeedback  on  effectiveness  and\napplicability.\nanswers  to  psychological  questions,\nheld  the  potential  to  enhance  mental\nhealth support using AI, and improved\noverall societal well-being.\n(Crasto  et  al.,\n2021) 66\nDesigned\nmental chatbot\nwith LLMs for\nstudent  mental\nhealth  support\non  an  online\nplatform\nDialoGPT Counselchat\n(includes tags of\nillness);\nquestion answers\nfrom 100 college\nstudents\nRecognized  mental  health\nquestionnaires  (Patient  Health\nQuestionnaire-9  &  WHO-5)  were\ncompleted.  The  DialoGPT  fine-tuned\nwith Counselchat data, was employed\nfor  chatbot  interaction.  Micro-\ninterventions were suggested based on\nidentified issues, and a student survey\nwas administered.\nThe  DialoGPT  model,  demonstrating\nhigher perplexity and preferred by 63%\nof college  participants for its human-\nlike  and  empathetic  responses,  was\nchosen as the most suitable system for\naddressing  student  mental  health\nissues.\n(Chen et al., 2023)\n67 \nDesigned\nmental  health\nchatbots  using\nLLMs  and\nassessed  their\nviability  in\npsychiatric\noutpatient\nscenarios\nChatGPT Human evaluation\nresults (14 patients\nand 11 psychiatrists)\nUsing  a  human-centered  design,  the\npaper collaborated with psychiatrists to\nharness  ChatGPT  for  simulating\npsychiatrist-patient  interactions,\ninfluenced  by  real-world  scenarios,\nand  evaluated  through  interactions\nwith professionals and patients.\nThe paper revealed a ChatGPT-backed\nevaluation  framework  that  validated\nthe  effectiveness  of  chatbots  in\npsychiatric  contexts  based  on\nevaluations with real psychiatrists and\npatients.\n(Cabrera  et  al.,\n2023) 68\nA  review  of\nthe  mental\nchatbot  for\nbioethical\ndilemmas \nChatGPT\n4.0\n33 scientific\nabstracts; \n13 media\nScientific  literature  and  media  news\nwere  systematically  reviewed  using\npredefined  criteria  on  the  Web  of\nScience  and  Microsoft  Bing  search\nengines,  focusing  on  the  relationship\nbetween chatbots and mental health.\nBioethical dilemmas about chatbots in\nmental  health  were  systematically\nidentified and classified into four major\nareas,  with  a  call  for  tailored\ndevelopment and ethical regulation.\n(Fournier-Tombs\net al., 2023) 69\nA  review  of\nmental\nchatbots  to\ninform  safe\nand\nappropriate\nfuture\ndevelopments\nin  the  use  of\nchatbots  in\nGPT-3 Literature,  policies,\nand\nrecommendations\nfrom  the  European\nUnion, UNESCO and\nWHO\nThe  paper  employed  a  conceptual\nanalysis of conversational chatbots in\nmedicine,  likely  informed  by  a\nliterature  review,  focusing  on  their\nethical implications, and proposed an\nintegrated  framework  connecting  AI\nand medical ethics.\nA  framework  was  proposed  to\nunderstand  the  impacts  of\nconversational chatbots on patients and\nthe broader medical community, with\nan emphasis on aligning AI ethics with\ntraditional medical ethics, in hopes of\nguiding  future  development  and\nregulations  in  a  safe  and  relevant\nmanner.\n11\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nhealthcare\n(Jo et al., 2023) 70 Designed\nmental  health\nchatbot  using\nLLMs  to\nprovide\nsupport  for\nsocially\nisolated\nindividuals  in\npublic  health\ninterventions\nHyperCLO\nVA\n34 audio-recorded\ninterviews;\nobservational notes;\ncodebook (10 parent\nand 24 child codes)\nFocus  group  workshop  sessions  with\n14 CareCall users were observed, and\ninterviews  with  20  people  spanning\nthree  stakeholder  groups  (CareCall\nusers,  teleoperators,  and  developers)\nwere  conducted,  to  understand  the\nbenefits,  challenges,  and  perspectives\nof using LLM-based chatbots in public\nhealth.\nInsights  into  the  holistic  support\nprovided  by  CareCall  to  mitigate\nloneliness,  offload  public  health\nworkloads, and challenges LLM-driven\nchatbots pose for public and personal\nhealth  were  highlighted,  and\nconsiderations  for  the  design  and\ndeployment of such chatbots in public\nhealth interventions were addressed.\n(Webster  et  al.,\n2023) 71\nA  review  of\nmental\nchatbot:\nexamining  the\ncapabilities,\nconcerns,  and\nethical\nconsiderations\nChatGPT;\nBard; \nMed-PaLM\nMultiMedQA;\nHealthSearchQA\nDatabase;\nPaLM Training\nCorpu;\nresearch papers\nThe paper investigated the capabilities,\nlimitations,  and  potential  risks  of  AI\nchatbots  like  Med-PaLM  in  the\nmedical  field  by  exploring  their\ntraining,  accuracy  in  medical  exams,\nrepresentation of doubt, and potential\nfor misdiagnosis.\nConcerns  were  raised  about  the\npotential  inaccuracy  of  AI  chatbot\ndiagnoses  and  the  importance  of\nmedical governance, informed consent,\nand the  collaboration of AI scientists\nwith clinicians. OpenAI, the maker of\nChatGPT,  warned  against  using  their\nmodel for critical medical decisions.\nTable 3. Summary of the 9 selected articles from the literature on other applications of LLMs in\nmental health.\nRef. Cases Model Data Sources Methodology Used Main Outcomes\n(Salah et al.,\n2023) 72\nEvaluation  of\nChatGPT  in\nmental health\nChatGPT Literature review;\nresearch papers;\nonline textual data\nThe  study  offered  a  thorough\noverview of ChatGPT's application in\nsocial psychology research, discussed\nethical,  theoretical,  and\nmethodological  challenges,  and\nunderscored  the  importance  of  a\ntheoretical framework that integrates\nGenerative  AI  with  current  social\npsychology theories.\nThe  study  found  that  ChatGPT  could\ntransform  social  psychology  research\nthrough data  analysis and modeling of\nsocial interactions, but researchers must\naddress associated challenges and follow\nguidelines  for  ethical  and  responsible\nuse,  including  bias  management,  data\nvalidation,  and  adherence  to  privacy\nstandards.\n(Farhat et al.,\n2023) 73\nEvaluation  of\nChatGPT  in\nmental health\nChatGPT Responses\ngenerated by\nChatGPT\nThe  study  evaluated  ChatGPT's\neffectiveness in mental health support\nby analyzing its responses and cross-\nquestioning,  particularly  focusing  on\nissues  related  to  anxiety  and\ndepression  and  its  suggestions\nregarding medications.\nChatGPT  displayed  significant\ninconsistencies and low reliability when\nproviding  mental  health  support  for\nanxiety and depression, underlining the\nnecessity  of  validation  by  medical\nprofessionals and cautious use in mental\nhealth contexts.\n(Woodnutt et al., Evaluation  of ChatGPT Responses The study input basic text commands The study found that OpenAI's ChatGPT\n12\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\n2023) 74 ChatGPT  in\nmental health\ngenerated by\nChatGPT\ninto  ChatGPT  regarding  a  fictitious\nperson  with self-harming  tendencies,\nand  the  output  was  assessed  for\nquality,  accuracy,  errors,  ethical\nconcerns,  and  potential  harms  using\nthe  authors'  clinical  expertise  and\ncurrent care guidelines.\ngenerated outputs with significant errors\nand ethical issues, presenting a risk of\npotential harm if used in mental health\ncare without expert oversight; AI's use\ncould  decrease  the  quality  of  care\nprovided by nurses and affect aspects of\nrecovery  tied  to  personal  relationships\nand social interactions.\n(Elyoseph et al.,\n2023) 75\nEvaluation  of\nChatGPT  in\nmental health\nChatGPT Responses\ngenerated by\nChatGPT\nThe  Levels  of Emotional  Awareness\nScale  (LEAS)  was  used  to  measure\nChatGPT's emotional awareness (EA)\nby analyzing its responses to twenty\nscenarios. This performance was then\ncompared  with  two  evaluations\nconducted  on  ChatGPT  in  January\nand  February  2023  using  different\nversions of the model.\nChatGPT  showcased  a  superior\nperformance  on  the  LEAS  scales\ncompared to the general population. The\nAI's scores were particularly high in the\nsecond  evaluation,  indicating  potential\nimprovement over time, which suggests\nits capability to generate appropriate EA\nresponses  and  its  utility  in  clinical\napplications.\n(Bhattacharyya et\nal., 2023) 76\nEvaluation  of\nChatGPT  in\nmental health\nChatGPT N/A The  paper  focused  on  ChatGPT's\napplication  in  mental  health,  its\nethical  considerations,  its  interaction\ndynamics, and its synergy with other\ndigital health tools.\nChatGPT  was  highlighted  for  its\npotential in mental health, its ability to\nwork alongside other tools, and the need\nfor  ethical  caution  due  to  potential\ninaccuracies.\n(Qiu et al., 2023)\n77\nSolutions  to\nmental  health\ndata problems\nSMILE ESCon 78;\nAugESC 79;\nPsyQA 65\nUtilizing  the  SMILE  approach,  the\npaper expanded single-turn dialogues\ninto  multi-turn  ones  using  ChatGPT\nand  validated  this  through  an\nexploratory  study  and  contrastive\nanalysis.\nThe  SMILE  method  effectively\nproduced a comprehensive, real-life-like\nmulti-turn  mental  health  support\nconversation  dataset,  with  utterance\nlengths  aligning  with  genuine\ncounseling sessions.\n(Perlis et al.,\n2023) 80\nEvaluation  of\nChatGPT  in\nmental health\nChatGP;\nGPT-4\n10 antidepressant\nprescribing\nvignettes\nChatGPT-4  was  given  10\nantidepressant prescribing vignettes in\na  randomized  order,  results  were\nregenerated five times for consistency,\nand  model  outcomes  were  then\njuxtaposed  with  expert  clinician\nconsensus.\nGPT-4  included  at  least  one  optimal\nmedication choice in 76% of vignettes\nbut  also  presented  less  optimal  or\ncontraindicated choices in 48% of them.\n(Elyoseph et al.,\n2023) 81\nEvaluation  of\nChatGPT  in\nmental health\nChatGPT A hypothetical\ntext vignette\nChatGPT's evaluations of the vignette\nwere  contrasted  with  mental  health\nprofessional norms reported by Levi-\nBelz and Gamliel using two-sample t-\ntests.\nChatGPT  consistently  underestimated\nthe risk of suicide attempts compared to\nmental  health professionals, suggesting\nit may not provide accurate suicide risk\nassessments.\n(Egli et al., 2023) Evaluation  of ChatGP; N/A A descriptive analysis of how chatbot The  paper  delved  into  the  potential\n13\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\n82 ChatGPT  in\nmental health\nGPT-4 technologies, like ChatGPT and GPT-\n4, operate and explores their potential\napplications,  strengths,  and\nweaknesses in clinical microbiology.\napplications  of  LLMs  in  clinical\nmicrobiology,  focusing  on  their\nfunctionalities, quality-control measures,\nand  potential  biases  in  their  training\ndata.\n14\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\n3.2 Mental Health Analysis Using Social Media Datasets\nEarly intervention and screening are crucial in mitigating the global burden\no f  m e n t a l  h e a l t h  i s s u e s  8 3.  W e  e x a m i n e d  L L M s '  p e r f o r m a n c e  i n  p r e d i c t i n g\nmental h ealth c onditions a nd c ategorizing m ental h ealth i ssues t hrough t ext\na n a l y s i s .  F o r  i n s t a n c e ,  b y  i n t e g r a t i n g  e x t r a c t i v e  s u m m a r i z a t i o n  B E R T  c a n\ni m p r o v e  d e p r e s s i o n  d e t e c t i o n  o n  s o c i a l  m e d i a  p l a t f o r m s  l i k e  R e d d i t  a n d\nT w i t t e r  b y  o u t p e r f o r m i n g  t h e  b a s e  B E R T  m o d e l  5 4.  A R A B E R T  ( F 1 - S c o r e :\n96.92%) and MARBERT (F1-Score: 96.07%) also excelled in Arabic contexts,\ns u r p a s s i n g  t r a d i t i o n a l  M L  t e c h n i q u e s  i n  d e t e c t i n g  d e p r e s s i o n  4 5.\nA d d i t i o n a l l y ,  t h e  R o B E R T a  m o d e l ,  a  f i n e - t u n e d  t r a n s f o r m e r - b a s e d  m o d e l ,\ne f f e c t i v e l y  t r a c k e d  t h e  r i s e  i n  d e p r e s s i o n  l e v e l s  o n  T w i t t e r  d u r i n g  t h e\nC O V I D - 1 9  p a n d e m i c ,  o u t p e r f o r m i n g  L S T M  ( F 1 - S c o r e :  7 8 . 8 5 %  v s .  7 1 . 2 1 % )\n5 5.  M e n t a l - A l p a c a  a n d  M e n t a l - F L A N - T 5 ,  i m p r o v e d  L L M s '  p e r f o r m a n c e  w i t h\nf e w e r  p r o m p t s  f o r  m e n t a l  h e a l t h  t a s k s ,  m e a n w h i l e  h i g h l i g h t i n g  l i m i t a t i o n s\nsuch as racial and gender bias issues 36 .\nC h a t G P T  c a n  a l s o  d e t e c t  m e n t a l  h e a l t h  i s s u e s  t h a t  n o t a b l y  s u r p a s s e d\nbaseline m odels t hrough t ext a nalysis, w ith F 1 s cores o f 0 .73, 0 .86, a nd 0 .37\nin s tress d etection, d epression d etection, a nd s uicide d etection, r espectively\n3 7.  M o r e o v e r ,  H a s s a n  e t  a l . ' s  r e s e a r c h  o n  C h a t G P T  s h o w e d  i t s  s t r o n g\nc o n t e x t u a l  l e a r n i n g  c a p a b i l i t i e s  i n  m e n t a l  h e a l t h  a n a l y s i s  b u t  a l s o  r e v e a l e d\ns h o r t c o m i n g s  i n  c o m p a r i s o n  t o  a d v a n c e d  t a s k - s p e c i f i c  m e t h o d s  4 1.  L L M s ,\np a r t i c u l a r l y  B E R T ,  h a v e  b e e n  i n s t r u m e n t a l  i n  a n a l y z i n g  c o m p l e x  m e n t a l\nh e a l t h  t e x t  d i a l o g u e s .  A  s t u d y  a n a l y z i n g  T w i t t e r  d a t a  i n d i c a t e d  a  f o u r f o l d\nincrease in telemedicine discussions related to mental health and substance\nabuse d uring t he p andemic, h ighlighting a n i ncreased n eed f or i nterventions\n3 1.  E n s e m b l e  m o d e l s  c o m b i n i n g  v a r i o u s  B E R T  v a r i a n t s ,  s u c h  a s  B E R T ,\nRoBERTa, a nd D istilBERT, d emonstrated e nhanced p erformance i n c lassifying\nd e p r e s s i o n  3 2.  C o n t i n u o u s  a d v a n c e m e n t s  a r e  n o t a b l e  i n  t h i s  a r e a .  F o r\ninstance, P sychBERT h as e stablished b enchmarks i n d etecting m ental h ealth\nb e h a v i o r s  o n  s o c i a l  m e d i a ,  p r o v i d i n g  h i g h  i n t e r p r e t a b i l i t y  f o r  b o t h  b i n a r y\nand multi-class classification 44 .\n3.3 LLMs in mental health chatbot\nThe use of a mental health chatbot employing LLMs shows promise for early\nintervention i n m ental i llnesses 71 . T hese r emote i ntervention t ools n ot o nly\nc a n  e n c o u r a g e  p e o p l e  w h o  w e r e  r e l u c t a n t  t o  s e e k  h e a l t h - r e l a t e d  a d v i c e\n62 t o m ake i nteractions, b ut t hey c an a lso h elp s upport o verburdened h ealth\n15\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nsystems 69 . Multiple studies have shown that LLMs are effective in creating\nc h a t b o t s  f o r  m e n t a l  h e a l t h  u s e .  C r a s t o  e t  a l .  o b s e r v e d  t h a t  6 3  o u t  o f  1 0 0\nc o l l e g e  s t u d e n t s  s u r v e y e d  s h o w e d  a  p r e f e r e n c e  f o r  r e s p o n s e s  f r o m\nDialoG PT, a variant of the GP T architecture, over those from LSTM and RNN\nm o d e l s  6 6.  T h e  p a r t i c i p a n t s  n o t e d  t h a t  D i a l o G P T - g e n e r a t e d  r e s p o n s e s\na p p e a r e d  m o r e  h u m a n - l i k e  a n d  r e s o n a n t .  H o w e v e r ,  i t  w a s  n o t e d  t h a t  t h e\nresponse q uality f rom G PT m odels w as v ariable. W ei e t a l. i dentified t hat t he\nf o r m a t  a n d  p e r s o n a l i t y  m o d i f i e r s  i n  p r o m p t  d e s i g n  s i g n i f i c a n t l y  i n f l u e n c e\nt h e  c h a t b o t s '  c a p a b i l i t y  i n  s l o t  f i l l i n g  a n d  c o n v e r s a t i o n a l  s t y l e s\n6 1.  T h i s  f i n d i n g  w a s  f u r t h e r  s u p p o r t e d  b y  Ku m a r  e t  a l .’ s  r e s e a r c h  w i t h  9 4 5\np a r t i c i p a n t s ,  w h i c h  r e v e a l e d  t h a t  p r o m p t  d e s i g n  m a r k e d l y  a f f e c t s\np e r c e p t i o n s  o f  c h a t b o t  r i s k ,  t r u s t w o r t h i n e s s ,  e x p e r t i s e ,  a n d  w i l l i n g n e s s  t o\ninteract 63 .\nT h e  C O V I D - 1 9  p a n d e m i c  h a s  s i g n i f i c a n t l y  i n c r e a s e d  t h e  d e m a n d  f o r\np s y c h o l o g i c a l  c o u n s e l i n g .  T h e  P s y - L L M  f r a m e w o r k  b y  L a i  e t  a l . ,  w h i c h\ni n t e g r a t e s  p r e - t r a i n e d  L L M s  w i t h  p r o f e s s i o n a l  Q & A  f r o m  p s y c h o l o g i s t s  a n d\np s y c h o l o g i c a l  a r t i c l e s ,  h a s  b e e n  e v a l u a t e d  f o r  i t s  a b i l i t y  t o  g e n e r a t e\nc o h e r e n t ,  r e l e v a n t  r e s p o n s e s  t o  p s y c h o l o g i c a l  i n q u i r i e s  6 4.  T h e  c r i t e r i a  f o r\nthese evaluations included usefulness, fluency, relevance, and logic, proving\nthe Psy-LLM to be an effective front-end tool for healthcare professionals to\ne n h a n c e  h e a l t h c a r e  e f f i c i e n c y  6 4.  H o w e v e r ,  t h e  d e p l o y m e n t  o f  t h i s  s i n g l e\nc a s e  d o e s  n o t  a l l e v i a t e  t h e  s a f e t y  c o n c e r n s  o f  L L M s  i n  m e n t a l  h e a l t h .\nWebster h ighlighted t he l imitations o f m edical L LMs l ike M ed-PaLM i n d irect\np a t i e n t  c a r e  d u e  t o  l i m i t e d  h e a l t h c a r e  d a t a  d e s p i t e  h i g h  s c o r e s  i n  m e d i c a l\ne x a m i n a t i o n s  7 1.  C o n s e q u e n t l y ,  e s t a b l i s h i n g  a  m e d i c a l  g o v e r n a n c e\nframework focused on informed consent is crucial. \nTo a ddress e thical c hallenges, C abrera e t a l. e xplored b ioethical d ilemmas i n\nc h a t b o t  u s e  i n  m e n t a l  h e a l t h ,  i n c l u d i n g  i s s u e s  o f  p r i v a c y ,  t r a n s p a r e n c y ,\na c c o u n t a b i l i t y ,  c o n f l i c t s  o f  i n t e r e s t ,  c u l t u r a l  s e n s i t i v i t y ,  a n d  b i a s e s ,  e t c\n6 8.  F o u r n i e r - T o m b s  e t  a l .  e m p h a s i z e d  t h e  e x p a n d e d  r o l e  o f  v i r t u a l  c a r e\nduring and following the COVID-19 pandemic for a range of health services,\ni n c l u d i n g  p r o v i d i n g  C O V I D - 1 9  i n f o r m a t i o n ,  p e r s o n a l i z e d  h e a l t h  a d v i c e ,  a n d\nprescription a ssistance 69 . H owever, t his p henomenon e xacerbated t he i ssue\nof b oundaries b etween A I a pplications a nd t he t raditional r ole o f h ealthcare\np r o f e s s i o n a l s .  T o  m i t i g a t e  t h e s e  c o n c e r n s ,  i n c l u d i n g  b i a s e s  r e l a t e d  t o\ng e n d e r ,  r a c e ,  a n d  d a t a  f r o m  v u l n e r a b l e  p o p u l a t i o n s ,  a n  e t h i c a l  f r a m e w o r k\nwa s  d i s c u s s e d  t o  g u i d e  c h a t b o t  d e p l o y m e n t  b a s e d  o n  t h e  c o r e  p r i n c i p l e s  o f\nmedical e thics, i ncluding b eneficence, n onmaleficence, a utonomy, a nd j ustice\n69 .\n16\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\n3.4 Exploring other applications of LLMs in mental health\nC h a t G P T  h a s  g a i n e d  a t t e n t i o n  f o r  i t s  a d v a n c e d  l a n g u a g e  g e n e r a t i o n\nc a p a b i l i t i e s ,  a t t r a c t i n g  t h e  i n t e r e s t  o f  m a n y  r e s e a r c h e r s  a n d  p r a c t i t i o n e r s\n8 2 d u e  t o  i t s  u n p a r a l l e l e d  a b i l i t y  t o  g e n e r a t e  h u m a n - l i k e  t e x t  a n d  a n a l y z e\nl a r g e  a m o u n t s  o f  t e x t u a l  d a t a  7 2.  C h a t G P T  h a s  b e e n  u s e d  t o  a d d r e s s  t h e\ns c a r c i t y  o f  c o m p r e h e n s i v e  d a t a  o n  p r o b l e m s  i n  m e n t a l  h e a l t h .  T h e  S M I L E\na p p r o a c h  u s e s  C h a t G P T  t o  e x p a n d  s i n g l e  r o u n d s  o f  d i a l o g u e  i n t o  m o r e\nex t e n s ive ,  m u l t i - t u r n  c o nve r s a t i o n s  a n d  e n a b l e s  t h e  c re a t i o n  o f  l a r g e - s c a l e ,\nd i v e r s e ,  a n d  c l o s e - t o - r e a l - l i f e  m e n t a l  h e a l t h  s u p p o r t  c o n v e r s a t i o n  c o r p o r a\n7 7.  H o w e v e r,  t h i s  a p p r o a c h  h a s  n o t  b e e n  t h o r o u g h l y  a n d  r e l i a b l y  e v a l u a t e d\ny e t ,  a n d  t h e  v i r t u a l  d i a l o g u e  s y s t e m  e x h i b i t s  f r e q u e n t  a n t h r o p o m o r p h i c\nbehavior 77 . In the context of social psychology, ChatGPT has demonstrated\np o t e n t i a l  i n  a n a l y z i n g  c o m p l e x  h u m a n  b e h a v i o r s  a n d  s i m u l a t i n g  s o c i a l\ni n t e r a c t i o n s ,  a s  d i s c u s s e d  by  S a l a h  e t  a l .  T h e s e  i nve s t i g a t i o n s  i n t o  C h a t G P T\nhave shown mixed results. Elyoseph et al. highlighted ChatGPT’s proficiency\ni n  E A ,  n o t i n g  t h a t  C h a t G P T ' s  p e r f o r m a n c e  i n  g e n e r a t i n g  a p p r o p r i a t e  E A\nr e s p o n s e s  w a s  i m p r e s s i v e ,  a c h i e v i n g  n e a r l y  t h e  h i g h e s t  p o s s i b l e  s c o r e  ( 9 8\nout of 100) on a subsequent assessment using the LEAS 75 .\nD e s p i t e  t h i s ,  t h e  m o d e l  d e m o n s t r a t e d  l i m i t a t i o n s  i n  a c c u r a t e l y  e v a l u a t i n g\ns u i c i d e  r i s k ,  f a l l i n g  s h o r t  o f  t h e  a s s e s s m e n t s  m a d e  b y  m e n t a l  h e a l t h\nprofessionals 81 . I n p sychopharmacology, w hile C hatGPT c orrectly i dentified\nat l east o ne o f t he o ptimal m edications i n 7 6% o f c ases, i t a lso r ecommended\ncon train dicated dru g s in 48% o f vig ne ttes 80 , g ivin g  the c aveat th at it does\nnot operate independently in medication management. Furthermore, Farhat\ne t  a l .  o b s e r v e d  i n c o n s i s t e n c i e s  i n  C h a t G P T ' s  m e n t a l  h e a l t h  s u p p o r t ,  w i t h\nv a r y i n g  p r o m p t s  l e a d i n g  t o  d i f f e r e n t  o u t p u t s  7 3.  S i m i l a r l y,  Wo o d n u t t  e t  a l .\nh i g h l i g h t e d  e t h i c a l  c o n c e r n s  r e g a rd i n g  t h e  u s e  o f  C h a t G P T  i n  m e n t a l  h e a l t h\nc a r e  s t r a t e g i e s ,  p a r t i c u l a r l y  p o i n t i n g  o u t  p o t e n t i a l  i n a c c u r a c i e s  a n d  e t h i c a l\nd i l e m m a s  i n  h a n d l i n g  s e n s i t i v e  t o p i c s  7 4.  T h e s e  f i n d i n g s  u n d e r s c o r e  t h e\nv i t a l  i m p o r t a n c e  o f  m e t i c u l o u s  s u p e r v i s i o n  by  h e a l t h c a re  p ro fe s s i o n a l s  a n d\ni n s t i t u t i o n s  i n  t h e  a p p l i c a t i o n  o f  L L M s  l i k e  C h a t G P T  w i t h i n  h e a l t h c a r e\nenvironments.\n3.5 Strengths and limitations of using LLMs in mental health\nB a s e d  o n  t h e  w o r k s  o f  l i t e r a t u r e  t h e  s t r e n g t h s  a n d  w e a k n e s s e s  o f  a p p l y i n g\nthe LLMs in mental health are summarized in Table 4. \n17\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nLLMs h ave d emonstrated t heir p otential t o c lassify a nd p redict m ental h ealth\np r o b l e m s  t h r o u g h  t e x t u a l  a n a l y s i s  3 2,3 7,3 9,4 4,4 5,4 7,5 5,5 9.   L L M s  h a v e\ns h o w n  t h e  p o t e n t i a l  f o r  t h e  e a r l y  d e t e c t i o n  o f  m e n t a l  i l l n e s s  u s i n g  s o c i a l\nm e d i a  3 7,  t h e r e b y  p o s s i b l y  e n h a n c i n g  t r e a t m e n t  o u t c o m e s  a n d  a l l e v i a t i n g\nh e a l t h c a r e  b u r d e n s .  I n  t h e  c o n t e x t  o f  t h e  g l o b a l  m e n t a l  h e a l t h  c r i s i s ,  L L M s\np r e s e n t  a  p r o m i s i n g  d i g i t a l  i n t e r v e n t i o n  s t r a t e g y  t h a t  c o u l d  m i t i g a t e  t h e\ne f f e c t s  o f  t h e  c u r r e n t  s h o r t a g e  o f  h e a l t h c a r e  r e s o u r c e s ,  p a r t i c u l a r l y  d u r i n g\nc r i s e s  s u c h  a s  C O V I D - 1 9  6 4.  T h e  L L M  i s  a d e p t  a t  p r e d i c t i n g  t h e  e m o t i o n a l\ntrends of crowds from social media texts, which can be used to increase the\ns o c i a l  awa re n e s s  o f  s p e c i f i c  g ro u p s  a t  p a r t i c u l a r  t i m e s  a n d  e n c o u ra g e  e a r ly\ni n t e r v e n t i o n  i n  a r e a s  w i t h  l i m i t e d  m e d i c a l  r e s o u r c e s .  A d d i t i o n a l l y ,  L L M s\nexcel i n u ser i nteraction, o ffering a nonymity a nd r educing s tigma, w hich m ay\nen c o u ra g e m ore  p e op l e su f fer in g  fro m  m e n ta l il ln e ss  to ac t ive ly pa r ti ci p ate\nin their treatment 66 . The ability of LLMs to personalize interactions caters\nto the diverse interests of different user groups thus enhancing engagement\n70 . Meanwhile, it is crucial to recognize that mental health chatbots are not\ns u b s t i t u t e s  f o r  p r o f e s s i o n a l  p s y c h o l o g i c a l  s e r v i c e s .  D e s p i t e  r e s e a r c h\nrevealing t he p otential o f L LM, i t h as n ot y et b een r ecognized f or c linical u se\nd u e  t o  t e c h n o l o g i c a l  r i s k s  a n d  e t h i c a l  i s s u e s  t h a t  r e m a i n  t o  b e  t h o r o u g h l y\naddressed.\nT h e  a p p l i c a t i o n  o f  L L M s  i n  m e n t a l  h e a l t h ,  p a r t i c u l a r l y  t h o s e  f i n e - t u n e d  f o r\ns p e c i f i c  i n s t r u c t i o n s  l i k e  C h a t G P T ,  r e v e a l s  n o t a b l e  l i m i t a t i o n s .  T h e\ne f f e c t i v e n e s s  o f  i n s t r u c t i o n  f i n e - t u n e d  L L M s  h e a v i l y  d e p e n d s  o n  t h e\nspecificity of user-generated prompts. In these models, inadequate prompts\nc a n  d i s r u p t  d i a l o g u e  f l o w  a n d  r e d u c e  c o n v e r s a t i o n  e f f e c t i v e n e s s\n68 , leading to inconsistent dialogue quality due to users' unfamiliarity with\np ro m p t  e n g in e e r i n g . D ev i a t i o n s  in  t ra i n i n g  d a t a  fo r  L L M s  in ev i t a b ly  l e a d  to\ni s s u e s  l i k e  h a l l u c i n a t i o n s  a n d  i n s t a b i l i t y ,  w h i c h  a r e  c h a l l e n g i n g  t o  r e c t i f y\n7 6.  A n o t h e r  c r i t i c a l  c o n c e r n  i s  t h e  ' b l a c k  b ox '  n a t u r e  o f  L L M s ,  w h i c h  r a i s e s\ni n t e r p r e t a b i l i t y  i s s u e s ,  p a r t i c u l a r l y  i n  c l i n i c a l  a n d  m e n t a l  h e a l t h  s e t t i n g s\n7 3.  T h i s  l a c k  o f  i n t e r p r e t a b i l i t y  c o m p l i c a t e s  t h e  a p p l i c a t i o n  o f  L L M s  i n\nm e n t a l  h e a l t h ,  w h e r e  t r u s t w o r t h i n e s s  a n d  c l a r i t y  a r e  i m p o r t a n t .  W h e n  w e\ntalk about neural networks as black boxes, we know what they were trained\nwith, how they were trained, what the weights are, etc. However, with many\nn e w  L L M s  l i k e  G P T  3 . 5 / 4 ,  r e s e a r c h e r s  a n d  p r a c t i t i o n e r s  o f t e n  a c c e s s  t h e\nm o d e l s  v i a  w e b  i n t e r f a c e s  o r  A P I s  w i t h o u t  c o m p l e t e  k n o w l e d g e  o f  t h e\ntraining data, methods, and model updates. This situation not only presents\nt h e  t r a d i t i o n a l  c h a l l e n g e s  a s s o c i a t e d  w i t h  n e u r a l  n e t w o r k s  b u t  a l s o  h a s  a l l\nthese additional problems that come from the \"hidden\" model. \n18\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nE t h i c a l  c o n c e r n  i s  a n o t h e r  s i g n i f i c a n t  c h a l l e n g e  a s s o c i a t e d  w i t h  a p p l y i n g\nL L M s  i n  m e n t a l  h e a l t h .  D e b a t e s  a r e  e m e r g i n g  a r o u n d  i s s u e s  l i k e  d i g i t a l\np e r s o n h o o d ,  i n f o r m e d  c o n s e n t ,  t h e  r i s k  o f  m a n i p u l a t i o n ,  a n d  t h e\na p p r o p r i a t e n e s s  o f  A I  i n  m i m i c k i n g  h u m a n  i n t e r a c t i o n s  8 1.  A d d i t i o n a l l y ,\nh u m a n  r i g h t s  r i s k s  s u c h  a s  d i s c r i m i n a t i o n  a r e  p i v o t a l  c o n c e r n s .  T h e r e  i s  a\np o s s i b i l i t y  t h a t  L L M s  c o u l d  g e n e r a t e  i n c o n s i s t e n t  r e c o m m e n d a t i o n s  o r\nd i s p l a y  i n c r e a s e d  e r r o r  r a t e s  i n f l u e n c e d  b y  p a t i e n t  d e m o g r a p h i c s  s u c h  a s\ng e n d e r ,  e t h n i c i t y ,  r a c e ,  a n d  r e l i g i o n ,  t h e r e b y  p e r p e t u a t i n g  h a r m f u l\ns t e r e o t y p e s  o r  b i a s e d  p e r s p e c t i v e s  6 9.  A c c o m p a n y i n g  t h e s e  i s s u e s  a r e\nw o r r i e s  a b o u t  d a t a  s e c u r i t y  a n d  v i o l a t i o n s  o f  u s e r  p r i v a c y  a n d  a u t o n o m y\n62 . T herefore, t here i s a n eed t o s trengthen t he r egulation a nd r eview o f t he\nLLM as an adjunctive tool in the mental health field, which is a key focus for\nfuture exploration and action.\n19\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nTable 4: Summary of main strengths, limitations, and suggestions of LLMs in mental health from\nthe selected articles.\nCATEGORY STRENGTH LIMITATION SUGGESTION\nMENTAL\nHEALTH\nANALYSIS\n LLMs have shown great potential in\nmental  health  categorization  tasks\nand  mental  health  analysis,\nespecially  in  stress  and  depression\ndetection 37,41,47,48.\n LLMs can show better performance\nand  accuracy  by  integrating  with\nother models 32,36,44,55,59.\n LLM  can  measure  emerging  trends\nrelated to mental health at scale. This\ncan  inform  public  health  strategies\nand resource allocation 47.\n LLMs  display  diminished  efficacy  in\nnon-English settings and struggle with\nidentifying  complex  psychological\nconditions 37,47,60.\n Assessment  relies  heavily  on  dataset\nannotations, and there are limitations to\nthe  use  of  sentiment  and  emotion\nlexicons due to annotation bias, limited\nvocabulary, and the evolving nature of\nonline language 37,60.\n Most of the studies used limited prompt\nsettings and relied on the first response\nof the LLMs as a prediction. Different\nprompts  or  variations  may  produce\nmore optimized or varied results 37.\n Pay more attention to social media\nin  other  countries  where  other\nlanguages are dominant and build\ndatabases  specifically  for\ndepression  screening  and  training\nof LLMs 31.\n The  current  dataset for stress and\ndepression  testing  can  be  re-\nannotated  by  multiple\npsychologists to minimize bias 37.\n Future experiments should broaden\ndatasets,  models,  and  prompt\ndesigns,  and  refine  evaluations  to\nimprove  categorization  accuracy\nthrough varied prompt settings and\nresponse  analysis  across  iterations\n36,37.\nMENTAL\nHEALTH\nCHATBOT\n The  LLMs  show  potential  as  a\nmental  health  intervention  tool  and\nthe ability to generate coherent and\nrelevant  answers  to  psychological\nquestions 64.\n LLMs-driven  mental  chatbots  can\nreduce  the  burden  of  healthcare  to\nsome extent, and the online dialogue\napproach  offers  the  possibility  of\ntelemedicine 70.\n LLMs-driven  mental  chatbots  can\nhelp  users  reduce  loneliness  and\nemotional  burden  and  reduce  the\nstigma associated with mental health\nas well as prejudice 62,70.\n Over-reliance on prompts results in the\nquality and relevance of the generated\nresponse  being  directly  related to the\naccuracy of the input prompts, leading\nto inaccurate responses if the prompts\nare inadequate 68.\n Inappropriate  responses  persist  in  the\npublic health context, in part because\nLLMs  rely  on  inherently  biased\ntraining data 62,70.\n LLM-driven  chatbot  output  is\ninfluenced  by  user  biases  and\nconversational styles, which can impact\nits  message  slot-filling  performance\n61,68.\n Refine  prompts  by  integrating\nsample  dialogues  for  incremental\nlearning  to  sharpen  questioning\nabilities and broaden the scope of\nprompt dimensions 61,62.\n Customising LLMs-driven chatbots\nto target specific groups of people\nand expanding the dataset used to\ntrain LLMs through a wide  range\nof  sources  representing  more\ndiverse  demographics,\nperspectives, and scenarios 70.\n Advanced  techniques  like  multi-\nagent  reinforcement  learning\nenable chatbots to adapt slot-filling\nstrategies  to  conversation  context,\nmoving beyond static training data\n62.\n The  LLMs  (ChatGPT)  outperform\nhumans  in  the  assessment  of  EA,\nrecognizing and describing emotions\n LLMs  may  generate  'hallucinatory'\ncontent, presenting inconsistencies and\ndifficulties in grasping the nuances of\n Detailed documentation of training\ndatasets,  shared  model\narchitectures,  and  third-party\n20\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nOTHER\nAPPLICATION\nS OF LLMS IN\nMENTAL\nHEALTH\nin specific scenarios 70.\n The LLMs can be invoked to build a\ncorpus  of  mental  health  support\ndialogues  using  its  inclusive\nlinguistic scalability from single-turn\nto multi-turn 77.\n LLMs improve the accuracy of data\nanalysis  and  the  fluency  of\ninteractions 72.\nsocial language 73,7680.\n LLMs  that  mimic  human\ncommunication prompt critical ethical\ndiscussions on digital identity, consent,\nmanipulation  risks,  and  the  potential\nfor  deceptive  simulations  when  users\nare unaware, they're engaging with AI\n73,82.\n LLMs are not yet precise  enough for\nstandalone  clinical  application  in\nmental health diagnostics and therapy\nand lack interpretability 76.\naudits;  outlining  logical\nrelationships  and  facts  with\nknowledge graphs 84.\n Rigorous  risk  assessments  and\nenhanced transparency are essential\nfor  user  interactions  with  AI;\nmoreover, establishing channels for\nuser  feedback  on  AI  and\ndeveloping  stringent ethical  codes\nand  industry  standards  is\nimperative\n22,36,62,70,73,74,76,81.\n To build and refine a large database\nof  LLMs  specifically  for  mental\nhealth training  77, to improve the\nperformance of LLMs in empathy\n74,  psychopharmacology  80,  etc.,\nand  to  develop  their  potential  as\nsupportive tools 73.\n21\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\n4. Discussion\n4.1Principal findings\nIn the context of the wider prominence of LLMs in the literature 72,76,82, our research\nsupports the assertion that interest in LLMs is growing in the field of mental health.\nFigure 3 shows the increasing trends in mental health studies using LLMs, while we note\nthat our search for 2023 ended on 1st September so it only includes part of the year’s\ndata. Key areas of interest identified by our work are mental health chatbots and the use\nof  LLMs  on  social  media  datasets  for  primary  mental  health  screenings.  This  likely\nreflects  the  promise  of  these  two  contexts  of  use  as  opportunities  for  LLM-driven\nstrategies that can be scaled at a low cost to improve mental healthcare provision.  This\nmay be particularly relevant in scenarios where the existing capacity to provide care is\nlimited. We also note that none of the existing work discovered classes as the strong\nstandard of clinical evaluation evidence required to support live clinical use.\n4.2 Limitations of the Selected Articles \nBeyond  a  lack  of  high-quality  clinical  evidence,  much  of  the  work  discovered  falls\noutside the peer-reviewed literature. As detailed in Tables 1, 2, and 3 listing the relevant\narticles, 11 of the 32 articles were from arXiv without peer review. These include 4\narticles  on  mental  health  analysis  using  social  media  datasets,  6  on  mental  health\nchatbots, and 1 on other applications of LLMs in mental health.  Articles from arXiv were\nmarked in  the  tables.  The  scarcity of peer-reviewed  literature  in  the  public  domain\npresents a significant challenge for new researchers in the field. \nThroughout the literature review, several further research limitations were identified. A\nkey concern is the age bias in social media data used for depression and mental health\nscreening. Users of social media skew towards younger demographics, leading to the\nunderrepresentation  of  older  age  groups.  This  skew  is  further  compounded  by  the\nfrequent absence of crucial metadata such as age and gender , adding to the problem of\nrepresentation in datasets 47. Language barriers also shape the current development of\nthe field. While tools like ChatGPT show proficiency in English, their accuracy in other\nlanguages can still be lacking  76. The resultant focus on English-centric social media\nplatforms  can  overlook  insights  from  non-English-speaking  regions. Future  research\ncould gain from incorporating data from platforms prevalent in these areas to provide a\nmore holistic view  31. Another limitation is the diversity of LLMs used was low.  Most\narticles in our review focused on variants of BERT and ChatGPT . Therefore, this review\nprovides  only  a  limited  picture  of  the  variability  we  might  expect  in  applicability\nbetween different LLMs. Another limitation is the rapid evolution of LLMs.  For example,\nstudies using GPT-3.5-turbo do not incorporate advancements in subsequent models\n22\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nlike  GPT-4,  making  it  hard  to  just  find  the  applicability  to  subsequent  models  37.\nAdditionally,  the  common  practice  of  binary  analysis  in  these  studies  risks\noversimplifying  complex  psychological  conditions by categorizing  social media posts\nmerely  as  'depressed'  or  'non-depressed'  55.  In  the  case  of  complex  mental  health\nconditions,  assessment  is  often  more  subjective,  relying  on  expert  judgment  or\nassessment of people’s behavior (rather than just text or voice fragments).\n4.3 Opportunities and Future Work\nImplementing  technologies  involving  LLMs  within  the  healthcare  provision  of  real\npatients  demands  thorough  and  multi-faceted  evaluations.  It  is  imperative  for  both\nindustry  and  researchers  to  not  let  rollout  exceed  proportional  requirements  for\nevidence  on  safety  and  efficacy.  At  the  level  of  the  service  provider ,  this  includes\nproviding explicit warnings to the public to discourage mistaking LLM functionality for\nclinical reliability. For example, the latest version of ChatGPT-4 introduces the ability to\nprocess and interpret image inputs within conversational contexts, leading OpenAI to\nissue  an  official  warning  that  ChatGPT-4  is  not  approved  for  analyzing  specialized\nmedical images, such as CT scans 85.\nA key challenge to address in LLM research is the tendency to produce incoherent text\nor hallucinations. This is a reasonable cause of concern in considering LLM’s role in\nfuture mental healthcare  Future efforts could focus on training LLMs specifically for\nmental health applications, using datasets with expert labeling to reduce bias and create\nspecialized mental health lexicons. The creation of specialized datasets could leverage\nthe customizable nature of LLMs, fostering the development of models that cater to the\ndistinct  needs  of  varied  demographic  groups.  For  example,  in  contrast  to  models\ndesigned  for  healthcare  professionals  to  assist  in  tasks  like  data  documentation,\nsymptom analysis, medication management, and postoperative care, LLMs for patient\ninteraction might be trained with an emphasis on empathy and comfortable dialogue.\nData  privacy  is  another  significant  area  of  concern.  Many  LLMs,  like  ChatGPT  and\nClaude, involve sending data to third-party servers opening up the risk of data leakage.\nOne potential solution lies in locally hosting open-source models or deploying LLMs in\nprivate clouds, enabling enhanced control over data storage and access 86.\nThe lack of interpretability of LLM decision-making is another important area for new\nwork  on  healthcare  applications.  Future  research  should  examine  the  models'\narchitecture,  training,  and  inferential  processes  for  clearer  understanding. Detailed\ndocumentation  of  training  datasets,  sharing  of  model  architectures,  and  third-party\naudits  would  ideally  form  part  of  this  undertaking.  Investigating  techniques  like\nattention  mechanisms  and  modular  architectures  could  illuminate  aspects  of  neural\n23\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nnetwork processing. The implementation of knowledge graphs might help in outlining\nlogical relationships and facts 84.\nAnother area for research is reducing LLMs' dependence on user inputs, by optimizing\nprompt  design.  Future  developers  can  explore  advanced  reinforcement  learning\napproaches,  such  as  multi-agent  reinforcement  learning,  which  enhance  LLMs'\ncapabilities  to  learn  from  interactions  and  advance  their  understanding  of  natural\nlanguage  62. In  the  realm  of  mental  health,  LLMs'  abilities  for  sentiment  analysis,\npersonalized  responses,  and  empathy  are  especially  important.  Using  randomized\nfactorial experiments could deepen understanding of prompt design  63. Additionally,\nexpanding  the  scope  of  experimental  testing  in  prompt  engineering,  including  the\nstrategic use of minimal learning with psychologically validated dialogue examples, can\nrefine  the  models'  questioning  acumen. Further  exploration  is  required  to  see  how\nvarious  parameters  influence  prompt  effectiveness,  alongside  experimental\ninvestigations into zero-shot, one-shot, and few-shot prompting 41. These investigations\nare expected to reveal how different prompt designs might impact LLM output in terms\nof accuracy, reliability, and ultimately applicability.\nPrior  to  deployment  in  the  mental  health  sector ,  it  is  also  imperative  for  medical\nprofessionals to rigorously evaluate these models to prevent any intervention that might\ncause harm.  Developing ethical guidelines is a priority for future research.  In clinical\nsettings,  combining  LLMs  with  physician  oversight  could  enhance  effectiveness. For\nexample,  while  ChatGPT  has  demonstrated  initial  competence  in  recommending\nmedication, it is not appropriate to be used independent of clinician scrutiny. However , if\nviewed instead as a decision-making aid, it could save the physician's time and increase\nefficiency. Future  evaluation  schemes  might  include  combined  impact  and  expert\nassessments to investigate criteria such as reliability, security, fairness, resistance to\nmisuse, interpretability, adherence to social norms, robustness, performance, linguistic\naccuracy, and cognitive competence  87. These measures are fundamental for creating\nethical frameworks suitable for mental health care.\n4.4Conclusion\nThis review thoroughly examines LLMs in mental health, covering social media data\nanalysis, chatbots, and their evaluation and solutions. Despite their potential, challenges\nlike training data bias, model accuracy, and ethical concerns persist. As data quality and\nethical guidelines improve, LLMs are expected to become more integral and important\nas they provide an alternative solution to mental health, this global healthcare issue.\n4.5Contributors\nZG  and  KL  contributed  to  the  conception  and  design  of  the  study.  ZG  and  KL  also\ncontributed to the development of the search strategy. Database search outputs were\n24\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nscreened by ZG, and data were extracted by ZG. An assessment of the risk of bias of the\nincluded  studies  was  performed  by  ZG  and  KL.  ZG  completed  the  literature  review,\ncollated the data, performed the data analysis, interpreted the results, and wrote the\nfirst  draft  of  the  manuscript.  KL,  AL,  JHT ,  JF,  and  TK  reviewed  the  manuscript  and\nprovided multiple rounds of guidance in the writing of the manuscript. All authors read\nand approved the final version of the manuscript.\n4.6Acknowledgements \nThis  work  was  funded  by  the  UKRI  Centre  for  Doctoral  Training  in  AI-enabled\nhealthcare systems (grant EP/S021612/1). The funders were not involved in the study\ndesign, data collection, analysis, publication decisions, or manuscript writing. The views\nexpressed in the text are those of the authors and not those of the funder.\n4.7Conflicts of Interest\nThe authors declare no conflict of interest.\n4.8Data sharing statement\nThe authors ensure that all pertinent data have been incorporated within the article\nand/or its supplementary materials. For access to the research data, interested parties\nmay  contact  the  corresponding  author ,  Kezhi  Li  (ken.li@ucl.ac.uk),  subject  to  a\nreasonable request.\n4.9Abbreviations\nBAC: balanced accuracy\nBERT: Bidirectional Encoder Representations from Transformers\nCNN: Convolutional Neural Network\nDBLP: DBLP Computer Science Bibliography\nEA: emotional awareness\nGANs: General Adversarial Networks\nGPT: Generative Pre-trained Transformer\nIEEE: IEEE Xplore\nLEAS: Levels of Emotional Awareness Scale\nLLM: large language model\nML: machine learning\nNLP: natural language processing\nOR: odds ratio\nPRISMA: Preferred Reporting Items for Systematic Review and Meta-analysis\nWHO: World Health Organization\n25\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\n5. Multimedia Appendix 1\nSupplementary material 1: Risk of bias assessment\nStudy Selection\nBias\nPerformance\nBias\nDetection\nBias\nAttrition\nBias\nReporting\nBias\nOverall\nRisk of\nBias\n1 Leveraging  Large  Language  Models  to\nPower Chatbots for Collecting User Self-\nReported Data\nLow Low Low Low Low Low\n2 LLM-Empowered  Chatbots  for\nPsychiatrist  and  Patient  Simulation:\nApplication and Evaluation \nLow Low Low Low Low Low\n3 Ethical  Dilemmas,  Mental  Health,\nArtificial  Intelligence,  and  LLM-Based\nChatbots\nLow Low Low Low Low Low\n4 A  Medical  Ethics  Framework  for\nConversational Artificial Intelligence\nModerate Low Low Low Low Low\n5 Understanding  the  Benefits  and\nChallenges of Deploying Conversational\nAI Leveraging Large Language Models\nfor Public Health Intervention\nModerate Low Low Low Low Low\n6 Towards  Healthy  AI:  Large  Language\nModels Need Therapists Too\nLow Low Low Low Low Low\n7 Exploring  The  Design  of  Prompts  for\nApplying  GPT-3  Based  Chatbots:  A\nMental  Wellbeing  Case  Study  on\nMechanical Turk\nLow Low Low Low Low Low\n8 Psy-LLM:  Scaling  Up  Global  Mental\nHealth Psychological Services with AI-\nBased Large Language Models\nLow Low Low Low Low Low\n26\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\n9 Carebot: A Mental Health Chatbot Moderate Low Low Low Low Low\n1\n0\n1. May The Force of  Text Data Analysis\nBe with You: Unleashing the Power of\nGenerative  AI  for  Social  Psychology\nResearch\nUnclear Unclear Unclear Low Low Low\n11 Chatgpt  as  a  Complementary  Mental\nHealth Resource: A Boon or A Bane\nUnclear Unclear Unclear Low Low Low\n1\n2\nTesting Domain Knowledge and Risk of\nBias of A Large-Scale General Artificial\nIntelligence Model in Mental Health\nModerate Low Low Low Low Low\n1\n3\nCould  Artificial  Intelligence  Write\nMental Health Nursing Care Plans?\nModerate Low Low Low Low Low\n1\n4\nChatGPT  Outperforms  Humans  in\nEmotional Awareness Evaluations\nLow Low Low Low Low Low\n1\n5\nChatGPT  and  Its  Application  in  The\nField of Mental Health\nModerate Low Low Low Low Low\n1\n6\nSMILE:  Single-Turn  to  Multi-Turn\nInclusive  Language  Expansion  Via\nChatGPT for Mental Health Support\nModerate Low Low Low Low Low\n1\n7\nResearch Letter: Application of GPT-4 to\nSelect  Next-Step  Antidepressant\nTreatment in Major Depression\nModerate Low Low Low Low Low\n1\n8\nBeyond Human Expertise: The Promise\nand Limitations of ChatGPT in Suicide\nRisk Assessment\nLow Low Low Low Low Low\n1\n9\nChatGPT,  GPT-4,  and  Other  Large\nLanguage  Models  –  The  Next\nRevolution for Clinical Microbiology?\nUnclear Unclear Unclear Low Low Low\n2\n0\nMedical AI Chatbots: Are They Safe to\nTalk to Patients?\nUnclear Unclear Unclear Low Low Low\n2\n1\nConsumer Perceptions of Telehealth for\nMental  Health  or  Substance  Abuse:  A\nTwitter-Based Topic Modeling Analysis\nModerate Low Low Low Low Low\n2\n2\nEnsembles  of  BERT  for  Depression\nClassification\nModerate Low Low Low Low Low\n2\n3\nMental-LLM:  Leveraging  Large\nLanguage  Models  for  Mental  Health\nPrediction Via Online Text Data\nModerate Low Low Low Low Low\n2 Evaluation of ChatGPT for NLP-Based Moderate Low Low Low Low Low\n27\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\n4 Mental Health Applications\n2\n5\nTowards  Interpretable  Mental  Health\nAnalysis with ChatGPT\nModerate Low Low Low Low Low\n2\n6\nPsychbert:  A Mental  Health  Language\nModel for Social Media Mental Health\nBehavioral Analysis\nLow Low Low Low Low Low\n2\n7\nCairodep:  Detecting  Depression  in\nArabic Posts Using BERT Transformers  \nLow Low Low Low Low Low\n2\n8\nMental Health Analysis During COVID-\n19: A Comparison Before and During the\nPandemic\nModerate Low Low Low Low Low\n2\n9\nAn Emotionally Aware Friend: Moving\nTowards Artificial General Intelligence\nLow Low Low Low Low Low\n3\n0\nLeveraging  BERT  With  Extractive\nSummarization for Depression Detection\non Social Media\nModerate Low Low Low Low Low\n3\n1\nAnalysis on Tweets Towards COVID-19\nPandemic: An Application of Text-Based\nDepression Detection\nModerate Low Low Low Low Low\n3\n2\nA  Novel  Text  Mining  Approach  for\nMental  Health  Prediction  Using  Bi-\nLSTM and BERT Model\nLow Low Low Low Low Low\nTable S1: Risk of bias assessment\n6. Multimedia Appendix 2\nSupplementary material 2: List of studies excluded at full-text screening stage\nTitle Year Author Exclusion reason\n1 Global  Mental  Health  Services  and  the  Impact  of  Artificial\nIntelligence-Powered Large Language Models\n2023 Alastair  C.  van\nHeerden\nReview  paper,  too\nshort\n2 Safety  Profile  of Methylphenidate  Under Long-Term Treatment in\nAdult ADHD Patients - Results of the COMPAS Study\n2020 Bernhard Kis Not about LLMs\n3 Mental Health and Discrimination among Migrants from Africa: An\nItalian Cross-Sectional Study\n2022 Gianluca V oglino Not about LLMs\n4 Chat-GPT: Opportunities and Challenges in Child Mental Healthcare 2023 Nazish Imran Review  article,  too\nshort\n5 The  Emergent  Role  of  Artificial  Intelligence,  Natural  Learning\nProcessing, and Large  Language  Models in Higher Education  and\nResearch\n2023 Tariq Alqahtani Not  about  mental\nhealth\n6 Mental  Health  and  Adherence  to  Mediterranean  Diet  among\nUniversity Students: An Italian Cross-Sectional Study\n2021 Giuseppina Lo Moro Not  about  mental\nhealth\n28\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\n7 Exploring Cyberaggression and Mental Health Consequences among\nAdults: An Italian Nationwide Cross-Sectional Study\n2023 Giuseppina Lo Moro Not  about  mental\nhealth\n8 Can  Natural  Language  Processing  Models  Extract  and  Classify\nInstances of Interpersonal Violence in Mental Healthcare Electronic\nRecords: An Applied Evaluative Study\n2022 Riley Botelle Not about LLMs \n9 Effects  of  Covid-19  Lockdown  on  Mental  Health  and  Sleep\nDisturbances in Italy\n2020 Maria  Rosaria\nGualano\nNot about LLMs\n1\n0\nThe Culture of Health in Early Care and Education: Workers’ Wages,\nHealth, And Job Characteristics\n2019 Jennifer J. Otten Not about LLMs\n11 ChatGPT  on  ECT:  Can  Large  Language  Models  Support\nPsychoeducation?\n2023 Robert M Lundin Review  article,  too\nshort\n1\n2\nEffectiveness of Guided and Unguided Online Alcohol Help: A Real-\nLife Study\n2022 Ans V angrunderbeek Not about LLMs\n1\n3\nListening  to  Mental  Health  Crisis  Needs  at  Scale:  Using  Natural\nLanguage Processing to Understand and Evaluate a Mental Health\nCrisis Text Messaging Service\n2021 Zhaolu Liu Not about LLMs\n1\n4\nEmotional Eating and Depression During the Pandemic: QuarantEat,\nan Italian Nationwide Survey\n2022 Giuseppina  Lo  Moro\nM.D.\nNot about LLMs\n1\n5\nTechnology Enhanced Health and Social Care for Vulnerable People\nDuring the COVID-19 Outbreak\n2021 Evangelia  D\nRomanopoulou\nNot about LLMs\n1\n6\nA CNN-Transformer Hybrid Approach for Decoding Visual Neural\nActivity into Text\n2022 Jiang Zhang Not  about  mental\nhealth\n1\n7\nMultimodal  Automatic  Coding  of Client  Behavior in Motivational\nInterviewing\n2020 Leili Tavabi Not about LLMs\n1\n8\nThe  Effectiveness  of  Psychological  Interventions  Alone,  or  in\nCombination with Phosphodiesterase-5 Inhibitors, for the Treatment\nof Erectile Dysfunction: A Systematic Review\n2021 Sandrine Atallah Not about LLMs\n1\n9\nTreatment of Pain in Cancer: Towards Personalised Medicine 2018 Marieke H J van den\nBeuken-van\nEverdingen\nNot about LLMs\n2\n0\nAutomatic  Depression  Severity  Assessment  with  Deep  Learning\nUsing Parameter-Efficient Tuning\n2023 Clinton Lau Not about LLMs\n2\n1\nEnabling Early Health Care Intervention by Detecting Depression in\nUsers of Web-Based Forums using Language Models: Longitudinal\nAnalysis and Evaluation\n2022 David Owen Duplicate\n2\n2\nAuthentic Engagement: A Conceptual Model for Welcoming Diverse\nand Challenging  Consumer and Survivor  Views  in Mental  Health\nResearch, Policy, and Practice\n2019 Indigo Daya BBus Not about LLMs\n2\n3\nThe Impact of COVID-19 on Mental Health in Medical Students: A\nCross-Sectional Survey Study in Italy\n2022 Sara Carletto Not about LLMs\n2 How do you feel? Using natural language processing to automatically 2021 Michael J. Tanana Not about LLMs\n29\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\n4 rate emotion in psychotherapy\n2\n5\nDepression  Risk  Prediction  for  Chinese  Microblogs  via  Deep-\nLearning Methods: Content Analysis\n2020 Xiaofeng Wang Duplicate\n2\n6\nDepression,  Suicidal  Ideation  and  Perceived  Stress  in  Italian\nHumanities Students: A Cross-Sectional Study\n2020 Fabrizio Bert Not about LLMs\n2\n7\nTransfer  Learning  for  Risk  Classification  of  Social  Media  Posts:\nModel Evaluation Study\n2019 Derek Howard Duplicate\n2\n8\nAI Assisted Attention Mechanism for Hybrid Neural Model to Assess\nOnline Attitudes About COVID-19\n2022 Harnain Kour Not  about  mental\nhealth\n2\n9\nBehavioural,  Emotional  and  Rhythm-Related  Disturbances  in\nToddlers: Preliminary Findings from a Community-Based Study in\nKerala, India\n2021 Preeti Jacob Not about LLMs\n3\n0\nMMASleepNet:  A  Multimodal  Attention  Network  Based  on\nElectrophysiological Signals for Automatic Sleep Staging\n2022 Yubo Zheng Not about LLMs\n3\n1\nDevelopment  of  Internet  Suicide  Message  Identification  and  the\nMonitoring-Tracking-Rescuing Model in Taiwan\n2023 En-Liang Wu Not about LLMs\n3\n2\nLGCCT: A Light Gated and Crossed Complementation Transformer\nfor Multimodal Speech Emotion Recognition\n2022 Feng Liu Not  about  mental\nhealth\n3\n3\nNursing  Education  in  the  Age  of  Artificial  Intelligence  Powered\nChatbots (AI-Chatbots): Are We Ready Yet?\n2023 Wilson Tam Not  about  mental\nhealth\n3\n4\nNeural  Mediation  of  Greed  Personality  Trait  on  Economic  Risk-\nTaking\n2019 Weiwei Li Not about LLMs\n3\n5\nData-driven Depression Detection System for Textual Data on Twitter\nUsing Deep Learning\n2022 Mushrifah Hasan Duplicate\n3\n6\nStress Identification in Online Social Networks 2022 Ashok Kumar Duplicate\n3\n7\nStress  Detection  from  Social  Media  Articles:  New  Dataset\nBenchmark and Analytical Study\n2022 Aryan Rastogi Duplicate\n3\n8\nA Radical Approach to Depression Detection 2022 Xue Lei Not about LLMs\n3\n9\nDoing Well-Being: Self-Reported Activities Are Related to Subjective\nWell-Being\n2022 August Håkan Nilsson Not about LLMs\n4\n0\nIncreased Online Aggression During COVID-19 Lockdowns: Two-\nStage  Study  of  Deep  Text  Mining  and  Difference-in-Differences\nAnalysis\n2022 Jerome Tze-Hou Hsu Duplicate\n4\n1\nSurveilling COVID-19 Emotional Contagion on Twitter by Sentiment\nAnalysis\n2020 Cristina Crocamo Duplicate\n4\n2\nEfficacy Of  A Group  Psychoeducation  Treatment  in  Binge  Eating\nDisorder: An Open-Label Study\n2022 Silvia Liquori Not about LLMs\n4\n3\nNetwork  Sentiment  Analysis  of  College  Students  in  Different\nEpidemic Stages Based on Text Clustering\n2022 Zhenghuai Song Duplicate\n30\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\n4\n4\nRvm-Gsm: Classification of Oct Images of Genitourinary Syndrome\nof  Menopause  Based  on  Integrated  Model  of  Local-Global\nInformation Pattern\n2023 Kaiwen Song Not  about  mental\nhealth\n4\n5\nMonitoring The Impact of Covid-19 Pandemic on Mental Health: A\nPublic Health Challenge? Reflection On Italian Data\n2021 Maria  Rosaria\nGualano\nReview  article,  too\nshort\n4\n6\nAnalysis  of  sentiment  changes  in  online  messages  of  depression\npatients  before  and  during  the  COVID-19  epidemic  based  on\nBERT+BiLSTM\n2022 Chaohui Guo Duplicate\n4\n7\nEffectiveness  of  A Brief  Dialectical  Behavior  Therapy  Intensive-\nOutpatient Community Health Program\n2022 Craig A Warlick Not  about  mental\nhealth\n4\n8\nThe Auto Segmentation for Cardiac Structures Using a Dual-Input\nDeep Learning Network Based on Vision Saliency and Transformer\n2022 Jing Wang Not  about  mental\nhealth\n4\n9\nExploring  the  Possible  Health  Consequences  of  Job  Insecurity:  A\nPilot Study Among Young Workers\n202 Fabrizio Bert Not about LLMs\n5\n0\nSentiment Analysis of Insomnia-Related Tweets via A Combination\nof  Transformers  Using  Dempster-Shafer  Theory:  Pre-  and  Peri-\nCOVID-19 Pandemic Retrospective Study\n2022 Arash Maghsoudi Duplicate\n5\n1\nOpioid  Death  Projections  with  Ai-Based  Forecasts  Using  Social\nMedia Language\n2023 Matthew Matero Not  about  mental\nhealth\n5\n2\nPredicting  Generalized  Anxiety  Disorder  from  Impromptu  Speech\nTranscripts  Using  Context-Aware  Transformer-Based  Neural\nNetworks: Model Evaluation Study\n2023 Bazen Gashaw Teferra Duplicate\n5\n3\nMultimodal  Treatment  Efficacy  Differs  in  Dependence  of  Core\nSymptom Profiles in Adult Attention-Deficit/Hyperactivity Disorder:\nAn Analysis of the Randomized Controlled Compas Trial\n2022 Benjamin\nSelaskowski\nNot about LLMs\n5\n4\nExamining  the  Psychometric  Properties  of  the  Integrative  Hope\nScale's  English  Translation  in  A  Mixed-Diagnostic  Community\nHealth Sample\n2022 Craig A Warlick Not about LLMs\n5\n5\nSocial  Media  for  Psychological  Support  of  Patients  with  Chronic\nNon-Infectious Diseases: A Systematic Review\n2023 Fabrizio Bert Not about LLMs\n5\n6\nSystematic  Review  and  Meta-Analysis  of  the  Effects  of  Group\nPainting Therapy on the Negative Emotions of Depressed Adolescent\nPatients\n2021 Zhaoxia Yuan Not about LLMs\nTable S2: Studies excluded after full text screening. LLMs=large language models\n7. Multimedia Appendix 3\nSupplementary material 3: PRISMA Checklist\nSection and \nTopic \nItem\n#\nChecklist item \nLocation where \nitem is reported \nTITLE \n31\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nSection and \nTopic \nItem\n#\nChecklist item \nLocation where \nitem is reported \nTitle 1 Identify the report as a systematic review. Title Page- Pg 1\nABSTRACT \nAbstract 2 See the PRISMA 2020 for Abstracts checklist. Abstract- Pg 1-2\nINTRODUCTION \nRationale 3 Describe the rationale for the review in the context of existing knowledge. Introduction- Pg \n2-5\nObjectives 4 Provide an explicit statement of the objective(s) or question(s) the review addresses. Introduction- Pg 5\nMETHODS \nEligibility \ncriteria \n5 Specify the inclusion and exclusion criteria for the review and how studies were grouped for the \nsyntheses.\nMethods- Pg 5\nInformation \nsources \n6 Specify all databases, registers, websites, organisations, reference lists and other sources searched \nor consulted to identify studies. Specify the date when each source was last searched or consulted.\nMethods- Pg 6\nSearch \nstrategy\n7 Present the full search strategies for all databases, registers and websites, including any filters and \nlimits used.\nMethods- Pg 5\nSelection \nprocess\n8 Specify the methods used to decide whether a study met the inclusion criteria of the review, \nincluding how many reviewers screened each record and each report retrieved, whether they \nworked independently, and if applicable, details of automation tools used in the process.\nMethods- Pg 5\nData \ncollection \nprocess \n9 Specify the methods used to collect data from reports, including how many reviewers collected data\nfrom each report, whether they worked independently, any processes for obtaining or confirming \ndata from study investigators, and if applicable, details of automation tools used in the process.\nMethods- Pg 5-6\nData items 10a List and define all outcomes for which data were sought. Specify whether all results that were \ncompatible with each outcome domain in each study were sought (e.g. for all measures, time points,\nanalyses), and if not, the methods used to decide which results to collect.\nMethods- Pg 5-6\n10b List and define all other variables for which data were sought (e.g. participant and intervention \ncharacteristics, funding sources). Describe any assumptions made about any missing or unclear \ninformation.\nMethods- Pg 6\nStudy risk of \nbias \nassessment\n11 Specify the methods used to assess risk of bias in the included studies, including details of the \ntool(s) used, how many reviewers assessed each study and whether they worked independently, and\nif applicable, details of automation tools used in the process.\nMethods- Pg 5-6; \nMultimedia \nAppendix 1\nEffect \nmeasures \n12 Specify for each outcome the effect measure(s) (e.g. risk ratio, mean difference) used in the \nsynthesis or presentation of results.\nMethods- Pg 6\nSynthesis \nmethods\n13a Describe the processes used to decide which studies were eligible for each synthesis (e.g. tabulating\nthe study intervention characteristics and comparing against the planned groups for each synthesis \n(item #5)).\nMethods- Pg 5-6\n13b Describe any methods required to prepare the data for presentation or synthesis, such as handling of Methods- Pg 5-6\n32\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nSection and \nTopic \nItem\n#\nChecklist item \nLocation where \nitem is reported \nmissing summary statistics, or data conversions.\n13c Describe any methods used to tabulate or visually display results of individual studies and \nsyntheses.\nMethods- Pg 5-6\n13d Describe any methods used to synthesize results and provide a rationale for the choice(s). If meta-\nanalysis was performed, describe the model(s), method(s) to identify the presence and extent of \nstatistical heterogeneity, and software package(s) used.\nMethods- Pg 5-6\n13e Describe any methods used to explore possible causes of heterogeneity among study results (e.g. \nsubgroup analysis, meta-regression).\nMethods- Pg 5-6\n13f Describe any sensitivity analyses conducted to assess robustness of the synthesized results.\nReporting \nbias \nassessment\n14 Describe any methods used to assess risk of bias due to missing results in a synthesis (arising from \nreporting biases).\nMethods- Pg 5-6\nCertainty \nassessment\n15 Describe any methods used to assess certainty (or confidence) in the body of evidence for an \noutcome.\nMethods- Pg 5-6\nRESULTS \nStudy \nselection \n16a Describe the results of the search and selection process, from the number of records identified in the\nsearch to the number of studies included in the review, ideally using a flow diagram.\nResults- Pg 6\n16b Cite studies that might appear to meet the inclusion criteria, but which were excluded, and explain \nwhy they were excluded.\nResults- Pg 6; \nMultimedia \nAppendix 2\nStudy \ncharacteristic\ns \n17 Cite each included study and present its characteristics. Results- Pg 9-12 \n(Table1-3)\nRisk of bias \nin studies \n18 Present assessments of risk of bias for each included study. Multimedia \nAppendix 1\nResults of \nindividual \nstudies \n19 For all outcomes, present, for each study: (a) summary statistics for each group (where appropriate) \nand (b) an effect estimate and its precision (e.g. confidence/credible interval), ideally using \nstructured tables or plots.\nResults- Pg 9-12 \n(Table1-3)\nResults of \nsyntheses\n20a For each synthesis, briefly summarise the characteristics and risk of bias among contributing \nstudies.\nResults – Pg 9-12,\n15-16\n20b Present results of all statistical syntheses conducted. If meta-analysis was done, present for each the\nsummary estimate and its precision (e.g. confidence/credible interval) and measures of statistical \nheterogeneity. If comparing groups, describe the direction of the effect.\nResults - Pg 13-16\n20c Present results of all investigations of possible causes of heterogeneity among study results. Results - Pg 13-16\n20d Present results of all sensitivity analyses conducted to assess the robustness of the synthesized \n33\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nSection and \nTopic \nItem\n#\nChecklist item \nLocation where \nitem is reported \nresults.\nReporting \nbiases\n21 Present assessments of risk of bias due to missing results (arising from reporting biases) for each \nsynthesis assessed.\nMethods - Pg 15-\n16; Multimedia \nAppendix 1\nCertainty of \nevidence \n22 Present assessments of certainty (or confidence) in the body of evidence for each outcome assessed. Results - Pg 9-16\nDISCUSSION \nDiscussion 23a Provide a general interpretation of the results in the context of other evidence. Discussion- Pg \n17-20\n23b Discuss any limitations of the evidence included in the review. Methods- Pg 15-\n16; Discussion- \nPg 17\n23c Discuss any limitations of the review processes used. Discussion- Pg \n18-19\n23d Discuss implications of the results for practice, policy, and future research. Discussion- Pg \n19-20\nOTHER INFORMATION\nRegistration \nand protocol\n24a Provide registration information for the review, including register name and registration number, or\nstate that the review was not registered.\nMethods- Pg 5\n24b Indicate where the review protocol can be accessed, or state that a protocol was not prepared. Methods- Pg 5\n24c Describe and explain any amendments to information provided at registration or in the protocol. Methods- Pg 5\nSupport 25 Describe sources of financial or non-financial support for the review, and the role of the funders or \nsponsors in the review.\nAcknowledgment-\nPg 21\nCompeting \ninterests\n26 Declare any competing interests of review authors. Conflicts of \nInterest- Pg 21\nAvailability \nof data, code \nand other \nmaterials\n27 Report which of the following are publicly available and where they can be found: template data \ncollection forms; data extracted from included studies; data used for all analyses; analytic code; any\nother materials used in the review.\nData sharing \nstatement- Pg 21\nTable S3: PRISMA Checklist \n34\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\n8. Reference\n1. World  Health  Organization.  Mental  health:  strengthening  our  response\n[Internet].  2022.  Available  from:  https://www.who.int/news-room/fact-\nsheets/detail/mental-health-strengthening-our-response.\n2. World  Health  Organization.  Mental  disorders  [Internet].  2022.  Available\nfrom:  https://www.who.int/news-room/fact-sheets/detail/mental-\ndisorders/?gclid=CjwKCAiApaarBhB7EiwA\nYiMwqi4yDXDAAvPftDkV_3GkuAV2lJxAyFdFWvHbEomzPBKgVpCpqupx_RoC\n2_IQAvD_BwE.\n3. Zhang W, Yang C, Cao Z, Li Z, Zhuo L, Tan Y, et al. Detecting individuals with\nsevere  mental  illness  using  artificial  intelligence  applied  to  magnetic\nresonance  imaging.  eBioMedicine.  2023  Apr;  90.  Available  from:\nhttps://www.thelancet.com/journals/ebiom/article/PIIS2352-\n3964(23)00106-8/fulltext\n4. McManus S, Bebbington P, Jenkins R, Brugha T. Mental health and wellbeing\nin  England:  Adult  psychiatric  morbidity  survey  2014.  Leeds:  NHS  Digital;\n2016. \n5. World Health Organization. Mental health and COVID-19: Early evidence of\nthe  pandemic’s  impact:  Scientific  brief,  2  March  2022  [Internet].  2022.\nAvailable  from:  https://www.who.int/publications/i/item/WHO-2019-\nnCoV-Sci_Brief-Mental_health-2022.1.\n6. Bertolote JM, Fleischmann A. Suicide and psychiatric diagnosis: a worldwide\nperspective. World Psychiatry. 2002 Oct;1(3):181-185.\n7. Mental Health America. Mental health treatments [Internet]. 2023. Available\nfrom: https://mhanational.org/mental-health-treatments.\n8. Rüsch  N,  Angermeyer  MC,  Corrigan  PW.  Mental  illness  stigma:  concepts,\nconsequences,  and  initiatives  to  reduce  stigma.  Eur  Psychiatry.  2005\nDec;20(8):529-539. DOI: 10.1016/j.eurpsy.2005.04.004.\n9. Corrigan PW, Watson AC. Understanding the impact of stigma on people with\nmental illness. World Psychiatry. 2002 Feb;1(1):16-20.\n10. Torous J, Myrick KJ, Rauseo-Ricupero N, Firth J. Digital mental health and\nCOVID-19:  Using  technology  today  to  accelerate  the  curve  on  access  and\nquality tomorrow. JMIR Ment Health. 2020;7(3):e18848.\n35\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\n11. Kasneci E, Sessler K, Küchemann S, Bannert M, Dementieva D, Fischer F, et al.\nChatGPT for good? On opportunities and challenges of large language models\nfor education. Learn Individ Differ. 2023;103:102274. ISSN 1041-6080. DOI:\nhttps://doi.org/10.1016/j.lindif.2023.102274. \n12. Yang K, Ji S, Zhang T, Xie Q, Kuang Z, Ananiadou S. Towards interpretable\nmental  health  analysis  with  large  language  models.  arXiv.  2023.\nhttps://doi.org/10.48550/arXiv.2304.03347. [Preprint]\n13. The  Guardian.  NHS  mental  health  patients  wait  times  [Internet].  2022.\nAvailable  from:  https://www.theguardian.com/society/2022/oct/10/nhs-\nmental-health-patients-wait-times.\n14. Fitzpatrick KK, Darcy A, Vierhile M. Delivering cognitive behavior therapy to\nyoung  adults  with  symptoms  of  depression  and  anxiety  using  a  fully\nautomated  conversational  agent  (Woebot):  a  randomized  controlled  trial.\nJMIR  Ment  Health.  2017;4(2):e19.  Available  from:\nhttps://mental.jmir.org/2017/2/e19/.\n15. Wysa—Everyday  Mental  Health.  n.d.  Wysa  -  Everyday  mental  health\n[Internet]. Available from: https://www.wysa.com/.\n16. Elyoseph  Z,  Refoua  E,  Asraf  K,  Lvovsky  M,  Shimoni  Y,  Hadar-Shoval  D.\nCapacity  of  generative  AI  to  interpret  human  emotions  from  visual  and\ntextual data: Pilot evaluation study. JMIR Ment Health. 2024;11:e54369. DOI:\n10.2196/54369. PMID: 38319707.\n17. Harrer S. Attention is not all you need: the complicated case of ethically using\nlarge  language  models  in  healthcare  and  medicine.  eBioMedicine.  2023\nApr;90. Available from: https://www.thelancet.com/journals/ebiom/article/\nPIIS2352-3964(23)00077-4/fulltext.\n18. OpenAI.  Better  language  models  [Internet].  2023.  Available  from:\nhttps://openai.com/research/better-language-models.\n19. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser L,\nPolosukhin I. Attention is all you need. 2017. Available from: https://doi.org/\n10.48550/arXiv.1706.03762. [Preprint].\n20. Thirunavukarasu AJ, Ting DSJ, Elangovan K, Gutierrez L, Tan TF, Ting DSW.\nLarge  language  models  in  medicine.  Nat  Med.  2023;29(8):Article  8.  DOI:\n10.1038/s41591-023-02448-8.\n36\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\n21. Priest  M.  Large  Language  Models  explained  [Internet].  Boost.ai.  2023.\nAvailable from: https://boost.ai/blog/llms-large-language-models.\n22. Kumar M. Understanding large language models and fine-tuning for business\nscenarios: a simple guide [Internet]. Medium. 2023. Available from: https://\nmedium.com/@careerInAI/understanding-large-language-models-and-fine-\ntuning-for-business-scenarios-a-simple-guide-42f44cb687f0.\n23. Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I. Language models are\nunsupervised  multitask  learners  [Internet].  2019.  Available  from:\nhttps://d4mucfpksywv.cloudfront.net/better-language-models/language-\nmodels.pdf.\n24. Bender  EM,  Koller  A.  Climbing  towards  NLU:  on  meaning,  form,  and\nunderstanding in the age of data. In Proceedings of the 58th Annual Meeting\nof  the  Association  for  Computational  Linguistics.  2020;5185-5198.\nhttps://doi.org/10.18653/v1/2020.acl-main.463.\n25. Lee J, Yoon W, Kim S, Kim D, Kim S, So CH, Kang J. BioBERT: A pre-trained\nbiomedical  language  representation  model  for  biomedical  text  mining.\nBioinformatics.  2019;36(4):1234-1240.  DOI:\n10.1093/bioinformatics/btz682.\n26. Huang K, Altosaar J, Ranganath R. Clinicalbert: Modeling clinical notes and\npredicting  hospital  readmission.  arXiv.  2020.  Available  from:\nhttps://arxiv.org/abs/1904.05342. [Preprint].\n27. Zhang K, Liu X, Shen J, Li Z, Sang Y, Wu X, Zha Y, et al. Clinically applicable AI\nsystem for accurate diagnosis, quantitative measurements, and prognosis of\nCOVID-19 pneumonia using computed tomography. Cell. 2020;182(5):1360.\nDOI: 10.1016/j.cell.2020.08.029.\n28. Trengove M, Vandersluis R, Goetz L. Response to \"Attention is not all you\nneed:  the  complicated  case  of  ethically  using  large  language  models  in\nhealthcare and medicine\". eBioMedicine. 2023 Jul;93. Available from: https://\nwww.thelancet.com/journals/ebiom/article/PIIS2352-3964(23)00236-0/\nfulltext\n29. Le Glaz A, Haralambous Y, Kim-Dufor D, Lenca P, Billot R, Ryan TC, Marsh J,\nDeVylder J, Walter M, Berrouiguet S, Lemey C. Machine learning and natural\nlanguage processing in mental health: systematic review. J Med Internet Res.\n37\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\n2021;23(5):e15708.  DOI:  10.2196/15708.  PMID:  33944788.  PMCID:\nPMC8132982.\n30. Moher  D.  Preferred  reporting  items  for  systematic  reviews  and  meta-\nanalyses:  The  PRISMA  statement.  Ann  Intern  Med.  2009;151(4):264.  DOI:\n10.7326/0003-4819-151-4-200908180-00135.\n31. Baird A, Xia Y, Cheng Y. Consumer perceptions of telehealth for mental health\nor substance abuse: A Twitter-based topic modeling analysis. JAMIA Open.\n2022;5(2). DOI: 10.1093/jamiaopen/ooac028.\n32. Senn  S,  Tlachac  ML,  Flores  R,  Rundensteiner  E.  Ensembles  of  BERT  for\ndepression  classification.  In:  Annual  International  Conference  of  the  IEEE\nEngineering in Medicine and Biology Society. 2022;4691-4694. DOI: https://\ndoi.org/10.1109/EMBC48229.2022.9871120.\n33. Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, Levy O, Lewis M, Zettlemoyer L,\nStoyanov  V.  RoBERTa:  a  robustly  optimized  BERT  pretraining  approach.\narXiv.  2019.  Available  from:  https://doi.org/10.48550/arXiv.1907.11692.\n[Preprint].\n34. Sanh V, Debut L, Chaumond J, Wolf T. DistilBERT, a distilled version of BERT:\nsmaller,  faster,  cheaper  and  lighter.  arXiv.  2020.  Available  from:\nhttps://doi.org/10.48550/arXiv.1910.01108. [Preprint].\n35. Gratch  J, Artstein  R, Lucas  G,  Stratou  G,  Scherer  S,  Nazarian  A,  et  al.  The\ndistress  analysis  interview  corpus  of  human  and  computer  interviews.\nProceedings of the 9th International Conference on Language Resources and\nEvaluation; 2014.\n36. Xu X, Yao B, Dong Y, Gabriel S, Yu H, Hendler J, et al. Mental-LLM: leveraging\nlarge language models for mental health [Internet]. arXiv.org. 2023. Available\nfrom: https://arxiv.org/pdf/2307.14385.pdf. [Preprint].\n37. Lamichhane  B.  Evaluation  of  ChatGPT  for  NLP-based  mental  health\napplications  [Internet].  arXiv.org.  2023.  Available  from:\nhttps://arxiv.org/abs/2303.15727. [Preprint].\n38. Chiang  G,  Stepanyan  A.  Insight  stress  analysis  [Internet].  2020.  Available\nfrom: https://github.com/gillian850413/Insight-Stress-Analysis.\n39. Inna  P.  Identifying  depression  [Internet].  2018.  Available  from:\nhttps://github.com/Inusette/Identifying-depression.\n38\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\n40. Alambo A. Suicide risk assessment using reddit [Internet]. 2022. Available\nfrom: https://github.com/AmanuelF/Suicide-Risk-Assessment-using-Reddit.\n41. Hassan  T.  Towards  robust  and  interpretable  practical  applications  of\nautomatic  mental  state  analysis  using  a  dynamic  and  hybrid  facial  action\nestimation  approach.  [Year  not  provided];[Preprint].  DOI:  10.20378/irb-\n48641.\n42. Ji S, Li X, Huang Z, Cambria E. Suicidal ideation and mental disorder detection\nwith  attentive  relation  networks.  Neural  Comput  Appl.  2022;34:10309-\n10319.\n43. Garg M, Saxena C, Saha S, Krishnan V, Joshi R, Mago V. CAMS: an annotated\ncorpus  for  causal  analysis  of  mental  health  issues  in  social  media  posts.\nProceedings  of  the  Thirteenth  Language  Resources  and  Evaluation\nConference. 2022. pp. 6387–6396.\n44. Vajre V, Naylor M, Kamath U, Shehu A. Psychbert: A mental health language\nmodel  for  social  media  mental  health  behavioral  analysis.  In:  2021  IEEE\nInternational Conference on Bioinformatics and Biomedicine (BIBM). 2021.\nDOI: 10.1109/BIBM52615.2021.9669469.\n45. El-Ramly M, Abu-Elyazid H, Mo’men Y, Alshaer G, et al. CairoDep: Detecting\ndepression  in  Arabic  posts  using  BERT  transformers.  2021  Tenth\nInternational Conference on Intelligent Computing and Information Systems\n(ICICIS).  2021;  Cairo,  Egypt.  pp.  207-212.  DOI:\n10.1109/ICICIS52592.2021.9694178.\n46. Abdul-Mageed M, Elmadany A, Nagoudi EMB. ARBERT & MARBERT: Deep\nbidirectional  transformers  for  Arabic.  Proceedings  of  the  59th  Annual\nMeeting  of  the  Association  for  Computational  Linguistics  and  the  11th\nInternational Joint Conference on Natural Language Processing (Volume 1:\nLong  Papers).  2021.  pp.  7088-7105.  DOI:\nhttps://doi.org/10.18653/v1/2021.acl-long.551.\n47. Bajaj GS, Yadav H, Sahdev HS, Sah S, Kaur P. Mental health analysis during\nCOVID-19: A comparison before and during the pandemic. 2021 IEEE 4th\nInternational  Conference  on  Computing,  Power  and  Communication\nTechnologies  (GUCON).  2021;  Kuala  Lumpur,  Malaysia.  pp.  1-7.  DOI:\n10.1109/GUCON50781.2021.9573763.\n39\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\n48. Vishwakarma A, Sawant S, Sawant P, Shankarmani R. An emotionally aware\nfriend:  moving  towards  artificial  general  intelligence.  2021  Third\nInternational Conference on Inventive Research in Computing Applications\n(ICIRCA).  2021;  Coimbatore,  India.  pp.  1094–1100.  DOI:\n10.1109/ICIRCA51532.2021.9544616.\n49. Abdel Razek M, Frasson C. Text-based intelligent learning emotion system. J\nIntell Learn Syst Appl. 2017;9:17-20.\n50. Saravia E, Liu HT, Huang YH, Wu J, Chen YS. CARER: Contextualized affect\nrepresentations for emotion recognition. EMNLP. 2018.\n51. Livingstone SR, Russo FA. The Ryerson Audio-Visual Database of Emotional\nSpeech and Song (RAVDESS): a dynamic multimodal set of facial and vocal\nexpressions in North American English. PLoS ONE. 2018;13(5):e0196391.\n52. Pichora-Fuller MK, Dupuis K. Toronto emotional speech set (TESS). Scholars\nPortal  Dataverse.  2020.  Available  from:\nhttps://doi.org/10.5683/SP2/E8H2MF.\n53. Burkhardt F, Paeschke A, Rolfes M, Sendlmeier WF, Weiss B. A database of\nGerman  emotional  speech.  Ninth  European  Conference  on  Speech\nCommunication and Technology. 2005.\n54. William  D,  Achmad  S,  Suhartono  D,  Gema  AP.  Leveraging  BERT  with\nextractive  summarization  for  depression  detection  on  social  media.  2022\nInternational Seminar on Intelligent Technology and Its Applications (ISITIA).\n2022;  Surabaya,  Indonesia.  pp.  63-68.  DOI:\n10.1109/ISITIA56226.2022.9855370.\n55. Kaseb A, Galal O, Elreedy D. Analysis on tweets towards COVID-19 pandemic:\nAn application of text-based depression detection. 2022 4th Novel Intelligent\nand  Leading  Emerging  Sciences  Conference  (NILES).  2022.  DOI:\n10.1109/NILES56402.2022.9942363.\n56. Wang S, Rupty LK, Mohona MH, Alagammai A, Omar M, Qabeel M. Depression\ndetection using twitter data. 2019.\n57. Shannak Y, Shurrab S. US COVID tweets. 2020.\n58. Database HD. Vaccine tweets. 2021.\n40\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\n59. Zeberga K, Attique M, Shah B, Ali F, Jembre YZ, Chung TS. A novel text mining\napproach  for  mental  health  prediction  using  Bi-LSTM  and  BERT  model.\nComput  Intell  Neurosci.  2022;2022:7893775.  DOI:\nhttps://doi.org/10.1155/2022/7893775.\n60. Heinz MV, Bhattacharya S, Trudeau B, Quist R, Song SH, Lee CM, Jacobson NC.\nTesting domain knowledge and risk of bias of a large-scale general artificial\nintelligence  model  in  mental  health.  Digit  Health.\n2023;9:20552076231170499.  DOI:\nhttps://doi.org/10.1177/20552076231170499.\n61. Wei J, Kim S, Jung H, Kim YH. Leveraging large language models to power\nchatbots  for  collecting  user  self-reported  data  [Internet].  arXiv.org.  2023.\nAvailable from: https://arxiv.org/abs/2301.05843. [Preprint].\n62. Lin  B,  Bouneffouf  D,  Cecchi  G,  Varshney  KR.  Towards  healthy  AI:  Large\nlanguage  models  need therapists  too  [Internet]. arXiv.org. 2023.  Available\nfrom: https://arxiv.org/abs/2304.00416. [Preprint].\n63. Kumar H, Musabirov I, Shi J, Lauzon A, Choy KK, Gross O, Kulzhabayeva D,\nWilliams  JJ.  Exploring  the  design  of  prompts  for  applying  GPT-3  based\nchatbots:  A  mental  wellbeing  case  study  on  mechanical  turk  [Internet].\narXiv.org.  2022.  Available  from:  https://arxiv.org/abs/2209.11344.\n[Preprint].\n64. Lai T, Shi Y, Du Z, Wu J, Fu K, Dou Y, Wang Z. Psy-LLM: Scaling up global\nmental health  psychological services with AI-based large language models\n[Internet].  arXiv.org.  2023.  Available  from:\nhttps://arxiv.org/abs/2307.11991. [Preprint].\n65. Sun  H,  Lin  Z,  Zheng  C,  Liu  S,  Huang  M.  PsyQA:  A  Chinese  dataset  for\ngenerating long counseling text for mental health support. Findings of the\nAssociation for Computational Linguistics: ACL-IJCNLP 2021. Association for\nComputational Linguistics. 2021. pp. 1489-1500.\n66. Crasto R, Dias L, Miranda D, Kayande D. CareBot: A mental health chatbot.\n2021 2nd International Conference for Emerging Technology (INCET). 2021.\nDOI: 10.1109/incet51464.2021.9456326.\n67. Chen S, Wu M, Zhu KQ, Lan K, Zhang Z, Cui L. LLM-empowered chatbots for\npsychiatrist  and  patient  simulation:  application  and  evaluation.  arXiv.org.\n2023. Available from: https://arxiv.org/abs/2305.13614. [Preprint].\n41\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\n68. Cabrera J, Loyola MS, Magaña I, Rojas R. Ethical dilemmas, mental health,\nartificial  intelligence,  and  LLM-based  chatbots.  Bioinformatics  and\nBiomedical Engineering. IWBBIO 2023. Lecture Notes in Computer Science.\nVol 13920. Cham: Springer; 2023. DOI: https://doi.org/10.1007/978-3-031-\n34960-7_22. \n69. Fournier-Tombs E, McHardy J. A medical ethics framework for conversational\nartificial intelligence. J Med Internet Res. 2023;25. DOI: 10.2196/43068.\n70. Jo E, Epstein DA, Jung H, Kim YH. Understanding the benefits and challenges\nof deploying conversational AI leveraging large language models for public\nhealth  intervention.  Proceedings  of  the  2023  CHI  Conference  on  Human\nFactors in Computing Systems (CHI '23). New York, NY, USA: Association for\nComputing  Machinery;  2023.  Article  18,  pages  1–16.  DOI:\nhttps://doi.org/10.1145/3544548.3581503. \n71. Webster P. Medical AI chatbots: Are they safe to talk to patients? Nature Med.\n2023;[Preprint]. DOI: 10.1038/s41591-023-02535-w. \n72. Salah M, Al Halbusi H, Abdelfattah F. May the force of text data analysis be\nwith  you:  Unleashing  the  power  of  generative  AI  for  social  psychology\nresearch.  Comput  Hum  Behav:  Artif  Humans.  2023;1(2):100006.  DOI:\n10.1016/j.chbah.2023.100006. \n73. Farhat F. ChatGPT as a complementary mental health resource: a boon or a\nbane.  Ann  Biomed  Eng.  2023  Jul  21.  DOI:  10.1007/s10439-023-03326-7.\nEpub ahead of print. PMID: 37477707.\n74. Woodnutt S, Allen C, Snowden J, Flynn M, Hall S, Libberton P, Purvis F. Could\nartificial intelligence write mental health nursing care plans? J Psychiatr Ment\nHealth  Nurs.  2023  Aug  4.  DOI:  10.1111/jpm.12965.  Epub  ahead  of  print.\nPMID: 37538021. \n75. Elyoseph  Z,  Hadar-Shoval  D,  Asraf  K,  Lvovsky  M.  ChatGPT  outperforms\nhumans in emotional awareness evaluations. Front Psychol. 2023;14. DOI:\n10.3389/fpsyg.2023.1199058.\n76. Bhattacharyya R, Chakraborty K, Neogi R. ChatGPT and its application in the\nfield  of  mental  health.  J  SAARC  Psychiatr  Fed.  2023;1(1):6.  DOI:\n10.4103/jspf.jspf_9_23.\n42\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\n77. Qiu H, He H, Zhang S, Li A, Lan Z. Smile: single-turn to multi-turn inclusive\nlanguage expansion via CHATGPT for mental health support. arXiv.org. 2023.\nAvailable from: https://arxiv.org/abs/2305.00450. [Preprint].\n78. Liu S, Zheng C, Demasi O, Sabour S, Li Y, Yu Z, Jiang Y, Huang M. Towards\nemotional  support  dialog  systems.  In:  Proceedings  of  the  59th  Annual\nMeeting  of  the  Association  for  Computational  Linguistics  and  the  11th\nInternational Joint Conference on Natural Language Processing (Volume 1:\nLong  Papers).  2021;3469-3483.  https://doi.org/10.18653/v1/2021.acl-\nlong.269.\n79. Zheng C, Sabour S, Wen J, Zhang Z, Huang M. AugESC: dialogue augmentation\nwith large language models for emotional support conversation. arXiv. 2023.\nAvailable from: https://doi.org/10.48550/arXiv.2202.13047. [Preprint].\n80. Perlis  RH.  Research  letter:  application  of  GPT-4  to  select  next-step\nantidepressant  treatment  in  major  depression.  [Year  not  provided].  DOI:\n10.1101/2023.04.14.23288595. [Preprint].\n81. Elyoseph  Z,  Levkovich  I.  Beyond  human  expertise:  the  promise  and\nlimitations  of  ChatGPT  in  suicide  risk  assessment.  Front  Psychiatry.\n2023;14:1213141. DOI: https://doi.org/10.3389/fpsyt.2023.1213141.\n82. Egli A. ChatGPT, GPT-4, and other large language models: The next revolution\nfor  clinical  microbiology?  Clin  Infect  Dis.  2023;77(9):1322-1328.  DOI:\nhttps://doi.org/10.1093/cid/ciad407.\n83. Colizzi M, Lasalvia A, Ruggeri M. Prevention and early intervention in youth\nmental health: is it time for a multidisciplinary and trans-diagnostic model\nfor  care?  Int  J  Ment  Health  Syst.  2020;14:23.\nhttps://doi.org/10.1186/s13033-020-00356-9\n84. The Black Box Problem: opaque inner workings of large language models.\nPrompt  Engineering.  2023  Oct  23.  Available  from:\nhttps://promptengineering.org/the-black-box-problem-opaque-inner-\nworkings-of-large-language-models/.\n85. OpenAI Help Center. Image inputs for ChatGPT - FAQ. [No publication date\navailable].  Available  at:  https://help.openai.com/en/articles/8400551-\nimage-inputs-for-chatgpt-faq.\n43\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\n86. Hinkle M. LLMs and data privacy: navigating the new frontiers of AI. The New\nStack.  2023.  Available from:  https://thenewstack.io/llms-and-data-privacy-\nnavigating-the-new-frontiers-of-ai.\n87. Emaminejad N, Akhavian R. Trustworthy AI and robotics: implications for the\nAEC industry. Automation in Construction. 2022;139:104298. Available from:\nhttps://doi.org/10.1016/j.autcon.2022.104298.\n44\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nSupplementary Files\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nFigures\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nComparative analysis of large language models by parameter size and developer entity.\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nPRISMA flow of selection process.\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nNumber of articles after keyword search grouped by the year of publication and application field.\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nMultimedia Appendixes\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n\nJMIR Preprints Guo et al\nRisk of bias assessment.\nURL: http://asset.jmir.pub/assets/9f883b502d85e8e296e2013a852c0cf5.pdf\nList of studies excluded at full-text screening stage.\nURL: http://asset.jmir.pub/assets/b4ecdde2b52b5343e75a1b540de7c6b6.pdf\nPRISMA Checklist.\nURL: http://asset.jmir.pub/assets/68a692b050ce59bb258b2f5e879c89a7.pdf\nPowered by TCPDF (www.tcpdf.org)\nhttps://preprints.jmir.org/preprint/57400 [unpublished, non-peer-reviewed preprint]\n",
  "topic": "Mental health",
  "concepts": [
    {
      "name": "Mental health",
      "score": 0.7305430173873901
    },
    {
      "name": "Psychological intervention",
      "score": 0.6765149235725403
    },
    {
      "name": "Psychology",
      "score": 0.36581748723983765
    },
    {
      "name": "Medicine",
      "score": 0.36027759313583374
    },
    {
      "name": "Psychiatry",
      "score": 0.3394883871078491
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I45129253",
      "name": "University College London",
      "country": "GB"
    }
  ]
}