{
  "title": "AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models",
  "url": "https://openalex.org/W3174730533",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2163935963",
      "name": "Yichun Yin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095817051",
      "name": "Cheng Chen",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2151842933",
      "name": "Lifeng Shang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097242334",
      "name": "Xin Jiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1975751340",
      "name": "Xiao Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2109590494",
      "name": "Qun Liu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2964259004",
    "https://openalex.org/W2963137684",
    "https://openalex.org/W3101731278",
    "https://openalex.org/W1690739335",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3158342816",
    "https://openalex.org/W2970557265",
    "https://openalex.org/W3034560159",
    "https://openalex.org/W2912521296",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W2782417188",
    "https://openalex.org/W3099547366",
    "https://openalex.org/W3184738308",
    "https://openalex.org/W3202316938",
    "https://openalex.org/W2970454332",
    "https://openalex.org/W4288256350",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2975429091",
    "https://openalex.org/W3015609966",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3098576111",
    "https://openalex.org/W3015042199",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3104216863",
    "https://openalex.org/W2964299589",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W2964118293",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3125341084",
    "https://openalex.org/W2748513770",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W3035251378",
    "https://openalex.org/W3035281298",
    "https://openalex.org/W3109946440",
    "https://openalex.org/W4297813615",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3034971973",
    "https://openalex.org/W3095851346",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W4292779060"
  ],
  "abstract": "Yichun Yin, Cheng Chen, Lifeng Shang, Xin Jiang, Xiao Chen, Qun Liu. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 5146‚Äì5157\nAugust 1‚Äì6, 2021. ¬©2021 Association for Computational Linguistics\n5146\nAutoTinyBERT: Automatic Hyper-parameter Optimization\nfor EfÔ¨Åcient Pre-trained Language Models\nYichun Yin1, Cheng Chen2*, Lifeng Shang1, Xin Jiang1, Xiao Chen1, Qun Liu1\n1Huawei Noah‚Äôs Ark Lab\n2Department of Computer Science and Technology, Tsinghua University\n{yinyichun,shang.lifeng,jiang.xin,chen.xiao2,qun.liu}@huawei.com\nc-chen19@mails.tsinghua.edu.cn\nAbstract\nPre-trained language models (PLMs) have\nachieved great success in natural language pro-\ncessing. Most of PLMs follow the default set-\nting of architecture hyper-parameters (e.g., the\nhidden dimension is a quarter of the intermedi-\nate dimension in feed-forward sub-networks) in\nBERT (Devlin et al., 2019). Few studies have\nbeen conducted to explore the design of archi-\ntecture hyper-parameters in BERT, especially\nfor the more efÔ¨Åcient PLMs with tiny sizes,\nwhich are essential for practical deployment\non resource-constrained devices. In this pa-\nper, we adopt the one-shot Neural Architecture\nSearch (NAS) to automatically search architec-\nture hyper-parameters. SpeciÔ¨Åcally, we care-\nfully design the techniques of one-shot learn-\ning and the search space to provide an adaptive\nand efÔ¨Åcient development way of tiny PLMs\nfor various latency constraints. We name our\nmethod AutoTinyBERT1 and evaluate its ef-\nfectiveness on the GLUE and SQuAD bench-\nmarks. The extensive experiments show that\nour method outperforms both the SOTA search-\nbased baseline (NAS-BERT) and the SOTA\ndistillation-based methods (such as DistilBERT,\nTinyBERT, MiniLM and MobileBERT). In ad-\ndition, based on the obtained architectures, we\npropose a more efÔ¨Åcient development method\nthat is even faster than the development of a\nsingle PLM.\n1 Introduction\nPre-trained language models, such as BERT (De-\nvlin et al., 2019), RoBERTa (Liu et al., 2019) and\nXLNet (Yang et al., 2019), have become prevalent\nin natural language processing. To improve model\nperformance, most PLMs (e.g. ELECTRA (Clark\net al., 2019) and GPT-2/3 (Radford et al., 2019;\n*Contribution during internship at Noah‚Äôs Ark Lab.\n1Our code implementation and pre-trained models are\navailable at https://github.com/huawei-noah/\nPretrained-Language-Model .\n5 10 15 20 25 30\nSpeedup (compared to BERT-base)\n68\n70\n72\n74\n76\n78GLUE score (%)\nAutoTinyBERT(Ours)\nBERT\nPF\nNAS-BERT\nFigure 1: Inference speedup vs. GLUE scores. Under\nthe same speedup constraint, our method outperforms\nboth the default hyper-parameter setting of BERT (De-\nvlin et al., 2019), PF (Turc et al., 2019)) and NAS-\nBERT (Xu et al., 2021). More details are in the Sec-\ntion 4.2.\nBrown et al., 2020)) follow the default rule of\nhyper-parameter setting2 in BERT to scale up their\nmodel sizes. Due to its simplicity, this rule has\nbeen widely used and can help large PLMs obtain\npromising results (Brown et al., 2020).\nIn many industrial scenarios, we need to deploy\nPLMs on resource-constrained devices, such as\nsmartphones and servers with limited computation\npower. Due to the expensive computation and slow\ninference speed, it is usually difÔ¨Åcult to deploy\nPLMs such as BERT (12/24 layers, 110M/340M\nparameters) and GPT-2 (48 layers, 1.5B parame-\nters) at their original scales. Therefore, there is an\nurgent need to develop PLMs with smaller sizes\nwhich have lower computation cost and inference\nlatency. In this work, we focus on a speciÔ¨Åc type of\nefÔ¨Åcient PLMs, which we deÔ¨Åne to have inference\ntime less than 1/4 of BERT-base.3\n2The default rule is dm = dq|k|v = 1/4df , which means\nthe dimension of hidden vector dm is equal to the dimen-\nsions of query/key/value vector dq|k|v and a quarter of the\nintermediate size df in feed-forward networks.\n3We empirically Ô¨Ånd that being at least 4x faster is a basic\nrequirement in practical deployment environment.\n5147\nAlthough, there have been quite a few work\nusing knowledge distillation to build small\nPLMs (Sanh et al., 2019; Jiao et al., 2020b; Sun\net al., 2019, 2020), all of them focus on the ap-\nplication of distillation techniques (Hinton et al.,\n2015; Romero et al., 2014) and do not study the\neffect of architecture hyper-parameter settings on\nmodel performance. Recently, neural architecture\nsearch and hyper-parameter optimization (Tan and\nLe, 2019; Han et al., 2020) have been widely ex-\nplored in machine learning, mostly in computer\nvision, and have been proven to Ô¨Ånd better designs\nthan heuristic ones. Inspired by this research, one\nproblem that naturally arises is can we Ô¨Ånd better\nsettings of hyper-parameters4 for efÔ¨Åcient PLMs?\nIn this paper, we argue that the conventional\nhyper-parameter setting is not best for efÔ¨Åcient\nPLMs (as shown in Figure 1) and introduce a\nmethod to automatically search for the optimal\nhyper-parameters for speciÔ¨Åc latency constraints.\nPre-training efÔ¨Åcient PLMs is inevitably resource-\nconsuming (Turc et al., 2019). Therefore, it is infea-\nsible to directly evaluate millions of architectures.\nTo tackle this challenge, we introduce the one-shot\nNeural Architecture Search (NAS) (Brock et al.,\n2018; Cai et al., 2018; Yu et al., 2020) to perform\nthe automatic hyper-parameter optimization on ef-\nÔ¨Åcient PLMs, named as AutoTinyBERT. SpeciÔ¨Å-\ncally, we Ô¨Årst use the one-shot learning to obtain\na big SuperPLM, which can act as proxies for all\npotential sub-architectures. Proxy means that when\nevaluating an architecture, we only need to extract\nthe corresponding sub-model from the SuperPLM,\ninstead of training the model from scratch. Super-\nPLM helps avoid the time-consuming pre-training\nprocess and makes the search process efÔ¨Åcient. To\nmake SuperPLM more effective, we propose prac-\ntical techniques including the head sub-matrix ex-\ntraction and efÔ¨Åcient batch-wise training, and par-\nticularly limit the search space to the models with\nidentical layer structure. Furthermore, by using\nSuperPLM, we leverage search algorithm (Xie and\nYuille, 2017; Wang et al., 2020a) to Ô¨Ånd hyper-\nparameters for various latency constraints.\nIn the experiments, in addition to the pre-training\nsetting (Devlin et al., 2019), we also consider the\nsetting of task-agnostic BERT distillation (Sun\net al., 2020) that pre-trains with the loss of knowl-\nedge distillation, to build efÔ¨Åcient PLMs. Exten-\n4We abbreviate the phrase architecture hyper-parameter\nas hyper-parameter in the paper.\nsive results show that in pre-training setting, Au-\ntoTinyBERT not only consistently outperforms the\nBERT with conventional hyper-parameters under\ndifferent latency constraints, but also outperforms\nNAS-BERT based on neural architecture search. In\ntask-agnostic BERT distillation, AutoTinyBERT\noutperforms a series of existing SOTA methods of\nDistilBERT, TinyBERT and MobileBERT.\nOur contributions are three-fold: (1) we explore\nthe problem of how to design hyper-parameters for\nefÔ¨Åcient PLMs and introduce an effective and ef-\nÔ¨Åcient method: AutoTinyBERT; (2) we conduct\nextensive experiments in both scenarios of pre-\ntraining and knowledge distillation, and the results\nshow our method consistently outperforms base-\nlines under different latency constraints; (3) we\nsummarize a fast rule and it develops an AutoTiny-\nBERT for a speciÔ¨Åc constraint with even about 50%\nof the training time of a conventional PLM.\n2 Preliminary\nBefore presenting our method, we Ô¨Årst provide\nsome details about the Transformer layer (Vaswani\net al., 2017) to introduce the conventional hyper-\nparameter setting. Transformer layer includes two\nsub-structures: the multi-head attention (MHA)\nand the feed-forward network (FFN).\nFor clarity, we show the MHA as a decompos-\nable structure, where the MHA includes h indi-\nvidual and parallel self-attention modules (called\nheads). The output of MHA is obtained by sum-\nming the output of all heads. SpeciÔ¨Åcally, each\nhead is represented by four main matrices Wq\ni ‚àà\nRdm√ódq/h, Wk\ni ‚ààRdm√ódk/h, Wv\ni ‚ààRdm√ódv/h\nand Wo\ni ‚ààRdv/h√ódo\n, and takes the hidden states5\nH ‚ààRl√ódm\nof the previous layer as input. The\noutput of MHA is given by the following formulas:\nQi,Ki,Vi = HW q\ni ,HW k\ni ,HW v\ni\nATTN(Qi,Ki,Vi) = softmax(QiKiT\n‚àö\ndq|k/h\n)Vi\nHi = ATTN(Qi,Ki,Vi)Wo\ni\nMHA(H) =\nh‚àë\ni=1\nHi,\n(1)\nwhere Qi ‚àà Rl√ódq/h, Ki ‚àà Rl√ódk/h, Vi ‚àà\nRl√ódv/h are obtained by the linear transformations\nof Wq\ni , Wk\ni , Wv\ni respectively. ATTN(¬∑) is the\n5We omitted the batch size for simplicity.\n5148\nHuawei Confidential63\nAttention\nAdd&Norm\nAdd&Norm\nMulti-Head Attention Feed-Forward Networks\n√óLTransformer Layers\n‚ë†One-shot learning for SuperPLM.\nEvaluator\nPerf(¬∑ )|Lat(¬∑ )\nEvolver\nMut(¬∑ )\nProxy Models\nArchitectures\nPerformance\n& Efficiency\nFurther \nTraining\nùëæùíó\nSuperPLM \nTraining\n‚ë°Search the optimal architecture.\n‚ë¢Further training sub-models.\nMax input dim.\nMax output dim. \nSampled output dim.\nSampled input dim.\nSub-matrix (width-wise) extraction.\nùëæùíå ùëæùíí\nùëæùíê\nùëæùüè\nùëæùüê\nùú∂ùê®ùê©ùê≠\nùëæùíí|ùíå|ùíó|ùíê|1|2\nFigure 2: Overview of AutoTinyBERT. We Ô¨Årst train an effective SuperPLM with one-shot learning, where the\nobjectives of pre-training or task-agnostic BERT distillation are used. Then, given a speciÔ¨Åc latency constraint,\nwe perform an evolutionary algorithm on the SuperPLM to search optimal architectures. Finally, we extract the\ncorresponding sub-models based on the optimal architectures and further train these models.\nscaled dot-product attention operation. Then out-\nput of each head is transformed to Hi ‚ààRl√ódo\nby\nWo\ni . Finally, outputs of all heads are summed as\nthe output of MHA. In addition, residual connec-\ntion and layer normalization are added on top of\nMHA to get the Ô¨Ånal output:\nHMHA = LayerNorm(H + MHA(H)). (2)\nIn the conventional setting of the hyper-parameters\nin BERT, all dimensions of matrices are the same\nas the dimension of the hidden vector, namely,\ndq|k|v|o=dm. In fact, there are only two require-\nments of dq=dk and do=dm that must be satisÔ¨Åed\nbecause of the dot-product attention operation in\nMHA and the residual connection.\nTransformer layer also contains an FFN that is\nstacked on the MHA, that is:\nHFFN = max(0,HMHAW1 +b1)W2 +b2, (3)\nwhere W1 ‚ààRdm√ódf\n, W2 ‚ààRdf √ódm\n, b1 ‚ààRdf\nand b2 ‚ààRdm\n. Similarly, there are modules of\nresidual connection and layer normalization on\ntop of FFN. In the original Transformer, df=4dm\nis assumed. Thus, we conclude that the conven-\ntional hyper-parameter setting follows the rule of\n{dq|k|v|o=dm, df=4dm}.\n3 Methodology\n3.1 Problem Statement\nGiven a constraint of inference time, our goal is to\nÔ¨Ånd an optimal conÔ¨Åguration of architecture hyper-\nparameters Œ±opt built with which PLM can achieve\nthe best performances on downstream tasks. This\noptimization problem is formulated as:\nŒ±opt = arg max\nŒ±‚ààA\nPerf(Œ±,Œ∏‚àó\nŒ±),\ns.t. Œ∏‚àó\nŒ± = arg min\nŒ∏\nLŒ±(Œ∏), Lat(Œ±) ‚â§T,\n(4)\nwhere T is a speciÔ¨Åc time constraint, Arefers to\nthe set of all possible architectures (i.e., combi-\nnation of hyper-parameters), Lat(¬∑) is a latency\nevaluator, LŒ±(¬∑) denotes the loss function of PLMs\nwith the hyper-parameter Œ±, and Œ∏ is the corre-\nsponding model parameters. We aim to search an\noptimal architecture for efÔ¨Åcient PLM (Lat(Œ±) <\n1/4 √óLat(BERTbase)).\n3.2 Overview\nA straightforward way to get the optimal archi-\ntecture is to enumerate all possible architectures.\nHowever, it is infeasible because each trial involves\na time-consuming pre-training process. Therefore,\nwe introduce one-shot NAS to search Œ±opt, as\nshown in the Figure 2. The proposed method in-\ncludes three stages: (1) the one-shot learning to\nobtain SuperPLM that can be used as the proxy for\n5149\nHuawei Confidential63\n(a)\n(b)\nùëæùëû|ùëò|ùë£√ó\n√ó\n√ó\n=\nùë∏|ùë≤|ùëΩ\nùëØ\n=\n=\n(c)\nFigure 3: MHA sub-matrix extraction. (a) means that\nthe original matrix operation where we take four heads\nand three hidden vectors as an example. White boxes\nrefer to the un-extracted parameters. (b) means that we\nextract heads while keeping the dimension per head. (c)\nmeans that we extract parameters from each head while\nkeeping the head number as the original matrix.\nvarious architectures; (2) the search process for the\noptimal hyper-parameters; (3) the further training\nwith the optimal architectures and corresponding\nsub-models. In the following sections, we Ô¨Årst in-\ntroduce the search space, which is the basis for\nthe one-shot learning and search process. Then we\npresent the three stages respectively.\n3.3 Search Space\nFrom the Section 2, we know that the conven-\ntional hyper-parameter setting is: {dq|k|v|o=dm,\ndf=4dm}, which is widely-used in PLMs. The\narchitecture of a PLM is parameterized as: Œ± =\n{lt,dm,dq,dk,dv,df,do}, which is subjected to\nthe constraints {dq = dk,do = dm}. Let lt\ndenote the layer number and d‚àó refer to differ-\nent dimensions in the Transformer layer. We de-\nnote the search space of lt and d‚àó as Alt and\nAd‚àó respectively. The overall search space is:\nA= Alt √óAdm|o √óAdq|k √óAdv √óAdf .\nIn this work, we only consider the case of identi-\ncal structure for each Transformer layer, instead of\nthe non-identical Transformer (Wang et al., 2020a)\nor other heterogeneous modules (Xu et al., 2021)\n(such as convolution units). It has two advan-\ntages: (1) it reduces an exponential search space\nof O(‚àè\n‚àó\n|Ad‚àó||Alt |) to a linear search space of\nO(‚àè\n‚àó\n|Ad‚àó||Alt |), greatly reducing the number of\npossible architectures in SuperPLM training and\nthe exploration space in the search process. It leads\nto a more efÔ¨Åcient search process. (2) An identical\nand homogeneous structure is in fact more friendly\nto hardware and software frameworks, e.g., Hug-\nging Face Transformer (Wolf et al., 2020). With a\nAlgorithm 1 Batch-wise training for SuperPLM\nInput: All possible candidates A; Training thread\n(GPU) number N; Large-scale unsupervised\ndataset D; Training epochs E. Sample times\nM per batch. SuperPLM parameters (Œ∏).\nOutput: Trained SuperPLM (Œ∏)\n1: for t= 1‚ÜíEdo\n2: for batchin Ddo\n3: Divide batchinto N subbatches\n4: Distribute sub batchesto N threads\n5: Clear the gradients\n6: for m= 1‚ÜíM do\n7: Sample N sub-models from A\n8: Distribute sub-models to threads\n9: Calculate gradients in each thread\n10: end for\n11: Update the Œ∏with the average gradients\n12: end for\n13: end for\nfew changes, we can use the original code to use\nAutoTinyBERT, as shown in Appendix A.\n3.4 One-shot Learning for SuperPLM\nWe employ the one-shot learning (Brock et al.,\n2018; Yu et al., 2020) to obtain a SuperPLM whose\nsub-models can act as the proxy for PLMs trained\nfrom scratch. The conÔ¨Ågurations of SuperPLM in\nthis work are lt=8, dm|q|k|v|o=768, and df=3072.\nIn each step of the one-shot learning, we train\nseveral sub-models randomly sampled from Su-\nperPLM to make their performance close to the\nmodels trained from scratch. Although the sam-\npling/search space has been reduced to linear com-\nplexity, there are still more than 10M possible sub-\nstructures in SuperPLM (the details are shown in\nthe Appendix B). Therefore, we introduce an ef-\nfective batch-wise training method to cover the\nsub-models as much as possible. SpeciÔ¨Åcally, in\nparallel training, we Ô¨Årst divide each batch into mul-\ntiple sub-batches and distribute them to different\nthreads as parallel training data. Then, we sample\nseveral sub-models on each thread for training and\nmerge the gradients of all threads to update the\nSuperPLM parameters. We illustrate the training\nprocess in the Algorithm 1.\nGiven a speciÔ¨Åc hyper-parameter setting Œ± =\n{lt,dm,dq,dk,dv,df,do}, we get a sub-model\nfrom SuperPLM by the depth-wise and width-\nwise extraction. SpeciÔ¨Åcally, we Ô¨Årst perform the\ndepth-wise extraction that extracts the Ô¨Årst lt Trans-\n5150\nModel SpeedupSQuAD SST-2 MNLI MRPC CoLA QNLI QQP STS-B RTEScore Avg.\nAutoTinyBERT-S1 7.2√ó 83.3 89.4 79.4 85.5 42.4 87.3 88.8 87.5 66.3 78.3 78.9\nBERT-S1 7.1√ó 81.5 88.9 78.4 81.3 35.8 86.4 88.2 86.7 66.4 76.5 77.1\nPF-4L512D‚Ä° (Turc et al., 2019)7.1√ó 81.7 89.4 78.5 82.8 35.2 87.0 88.6 87.4 65.7 76.8 77.4\nPF-2L768D‚Ä° (Turc et al., 2019)7.0√ó 71.1 88.8 76.5 79.6 26.7 84.9 88.1 86.6 67.1 74.8 74.4\nAutoTinyBERT-S2 15.7√ó 78.1 88.2 76.8 82.8 35.5 85.4 87.8 86.5 68.2 76.4 76.6\nBERT-S2 14.8√ó 77.6 87.5 76.5 79.6 32.8 84.4 87.0 86.6 66.4 75.1 75.4\nNAS-BERT10‚Ä†(Xu et al., 2021)12.7√ó - 88.6 76.0 81.5 27.8 86.3 88.4 84.3 68.7 75.2 -\nPF-2L512D‚Ä° (Turc et al., 2019)12.8√ó 69.2 87.1 74.7 76.9 23.2 84.4 87.0 86.0 64.9 73.0 72.6\nPF-6L256D‚Ä° (Turc et al., 2019)13.3√ó 77.0 87.6 76.4 80.3 33.2 85.7 86.7 86.0 64.9 75.1 75.3\nAutoTinyBERT-S3 20.2√ó 75.8 86.8 76.4 80.4 33.2 85.0 87.6 86.7 66.4 75.3 75.4\nBERT-S3 20.1√ó 73.7 86.4 75.0 81.3 31.2 84.0 87.1 85.8 63.8 74.3 74.3\nAutoTinyBERT-S4 31.0√ó 71.9 86.5 74.2 81.9 17.6 84.6 86.5 85.9 66.7 73.0 72.9\nBERT-S4 31.3√ó 69.5 85.5 73.9 76.9 15.9 83.9 85.9 85.3 61.0 71.0 70.9\nNAS-BERT5‚Ä†(Xu et al., 2021)32.0√ó - 84.9 74.2 80.0 19.6 83.9 85.7 82.8 67.0 72.3 -\nPF-6L128D‚Ä° (Turc et al., 2019)28.2√ó 63.6 84.6 72.3 78.6 0 83.3 83.8 84.5 65.7 69.1 68.5\nTable 1: Comparison between AutoTinyBERT and baselines in pre-training setting. The results are evaluated on\nthe dev set of GLUE benchmark and SQuADv1.1. We use the metric of Matthews correlation for CoLA, F1 for\nSQuADv1.1, Pearson-Spearman correlation for STS-B, and accuracy for other tasks. We report the average score\nexcluding SQuAD (Score) in addition to the average score of all tasks (Avg.). The speedup is in terms of the\nBERTbase inference speed and evaluated on a single CPU with a single input of 128 length. PF-xLyD, the x and y\nrefer to the layer number and hidden dimension respectively. ‚Ä†denotes that the results are taken from (Xu et al.,\n2021) and ‚Ä°denotes that the results are obtained by Ô¨Åne-tuning the released models.\nformer layers from SuperPLM, and then perform\nthe width-wise extraction that extracts bottom-left\nsub-matrices from original matrices. For MHA, we\napply two strategies illustrated in Figure 3 : (1)\nkeep the dimension of each head same as Super-\nPLM, and extract some of the heads; (2) keep the\nhead number same as SuperPLM, and extract sub-\ndimensions from each head. The Ô¨Årst strategy is\nthe standard one and we use it for pre-training and\nthe second strategy is used for task-agnostic distil-\nlation because that attention-based distillation (Jiao\net al., 2020b) requires the student model to have\nthe same head number as the teacher model.\n3.5 Search Process\nIn the search process, we adopt an evolutionary\nalgorithm (Xie and Yuille, 2017; Jiao et al., 2020a),\nwhere Evolver and Evaluator interact with each\nother to evolve better architectures. Our search\nprocess is efÔ¨Åcient, as shown in the Section 4.4.\nSpeciÔ¨Åcally, Evolver Ô¨Årstly samples a generation\nof architectures from A. Then Evaluator extracts\nthe corresponding sub-models from SuperPLM and\nranks them based on their performance on tasks of\nSQuAD and MNLI. The architectures with the high\nperformance are chosen as the winning architec-\ntures and Evolver performs the mutation Mut(¬∑)\noperation on the winning ones to produce a new\ngeneration of architectures. This process is con-\nducted repeatedly. Finally, we choose several ar-\nchitectures with the best performance for further\ntraining. We use Lat(¬∑) to predict the latency of\nthe candidates to Ô¨Ålter out the candidates that do\nnot meet the latency constraint. Lat(¬∑) is built with\nthe method by Wang et al. (2020a), which Ô¨Årst sam-\nples about 10k architectures from Aand collects\ntheir inference time on target devices, and then uses\na feed-forward network to Ô¨Åt the data. For more\ndetails of evolutionary algorithm, please refer to\nAppendix C. Note that we can use different meth-\nods in search process, such as random search and\nmore advanced search, which is left as future work.\n3.6 Further Training\nThe search process produces top several architec-\ntures, with which we extract these corresponding\nsub-models from SuperPLM and continue training\nthem using the pre-training or KD objectives.\n4 Experiment\n4.1 Experimental Setup\nDataset and Fine tuning. We conduct the experi-\nments on the GLUE benchmark (Wang et al., 2018)\nand SQuADv1.1 (Rajpurkar et al., 2016). For\nGLUE, we set the batch size to 32, choose the learn-\ning rate from {1e-5, 2e-5, 3e-5 }and choose the\nepoch number from {4, 5, 10}. For SQuADv1.1,\nwe set the batch size to 16, the learning rate to 3e-\n5 and the epoch number to 4. The details for all\ndatasets are displayed in Appendix D.\nAutoTinyBERT. Both the one-shot and further\n5151\nModel SpeedupSQuAD SST-2 MNLI MRPC¬∂ CoLA QNLI QQP¬∂ STS-B RTEScore Avg.\nDev results on GLUE and dev result on SQuAD\nAutoTinyBERT-KD-S1 4.6√ó 87.6 91.4 82.3 88.5 47.3 89.7 89.9 89.0 71.1 81.2 81.9\nBERT-KD-S1 4.9√ó 86.2 89.7 81.1 87.9 41.8 87.3 88.4 88.4 68.2 79.1 79.9\nMobileBERTTiny‚Ä°(Sun et al., 2020)3.6*√ó 88.6 91.6 82.0 86.7 - - - - - - -\nAutoTinyBERT-KD-S2 9.0√ó 84.6 88.8 79.4 87.3 32.2 88.0 87.7 88.0 68.9 77.5 78.3\nBERT-KD-S2 9.8√ó 82.5 87.8 77.9 86.5 31.5 86.9 87.6 87.4 66.4 76.5 77.2\nMiniLM-4L312D‚Ä† (Wang et al., 2020b)9.8√ó 82.1 87.3 78.3 83.6 26.3 87.1 87.3 86.3 62.4 74.8 75.6\nTinyBERT-4L312D‚Ä†¬ß(Jiao et al., 2020b)9.8√ó 81.0 87.8 76.9 77.9 22.9 86.0 87.7 83.3 58.8 72.7 73.6\nAutoTinyBERT-KD-S3 10.7√ó 83.3 88.3 78.2 85.8 29.1 87.4 87.4 86.7 66.4 76.2 77.0\nBERT-KD-S3 11.7√ó 81.6 86.5 76.8 82.5 27.6 85.6 86.5 86.2 64.9 74.6 75.4\nAutoTinyBERT-KD-S4 17.0√ó 78.7 86.8 76.0 81.4 20.4 85.5 86.9 86.0 64.9 73.5 74.1\nBERT-KD-S4 17.0√ó 77.4 85.7 75.4 80.3 18.9 85.0 85.9 84.7 63.1 72.4 72.9\nTest results on GLUE and dev result on SQuAD\nAutoTinyBERT-KD-S1 4.6√ó 87.6 90.6 81.2 88.9 44.7 87.4 70.5 85.1 64.8 76.7 77.9\nBERT-3L-PKD‚Ä° (Sun et al., 2019)4.1√ó - 87.5 76.7 80.7 - 84.7 68.1 - 58.2 - -\nDistilBERT-4L‚Ä° (Sanh et al., 2019)3.0√ó 81.2 91.4 78.9 82.4 32.8 85.2 68.5 76.1 54.1 71.2 72.3\nTinyBERT-4L516D‚Ä†¬ß(Jiao et al., 2020b)4.9√ó 84.6 88.2 80.0 86.3 27.9 85.6 69.1 83.0 61.5 72.7 74.0\nMiniLM-4L516D‚Ä† (Wang et al., 2020b)4.9√ó 85.5 90.0 80.2 87.2 39.1 86.5 70.0 83.4 63.7 75.0 76.2\nMobileBERTTiny‚Ä°(Sun et al., 2020)3.6*√ó 88.6 91.7 81.5 87.9 46.7 89.5 68.9 80.1 65.1 76.4 77.8\nTable 2: Comparison between AutoTinyBERT and baselines based on knowledge distillation. ‚Ä°denotes that\nthe results are taken from (Sun et al., 2020) and ‚Ä†means the models trained using the released code or the re-\nimplemented code with ELECTRAbase as the teacher model. ¬∂means these tasks use accuracy for dev set and F1\nfor test set respectively. ¬ßdenotes the task-agnostic TinyBERT without task-speciÔ¨Åc distillation. * means that the\nspeedup is different from the (Sun et al., 2020), because it is evaluated on a Pixel phone and not on server CPUs. -\nmeans that the results are missing in the original paper. Other information refer to the Table 1.\ntraining use BooksCorpus (Zhu et al., 2015) and\nEnglish Wikipedia as training data. The settings for\none-shot training are: peak learning rate of 1e-5,\nwarmup rate of 0.1, batch size of 256 and 5 running\nepochs. Further training follows the same setting\nas the one-shot training except for the warmup rate\nof 0. In the batch-wise training algorithm 1, the\nthread number N is set to 16, the sample times\nM per batch is set to 3, and epoch number E is\nset to 5. We train the SuperPLM with an archi-\ntecture of {lt=8, dm|q|k|v|o=768, df=3072}. In the\nsearch process, Evolver performs 4 iterations with a\npopulation size of 25 and it chooses top three archi-\ntectures for further training. For more details of the\nsampling/search space and evolutionary algorithm,\nplease refer to Appendix B and C.\nWe train AutoTinyBERT in both ways of pre-\ntraining (Devlin et al., 2019) and task-agnostic\nBERT distillation (Sun et al., 2020). For task-\nagnostic distillation, we follow the Ô¨Årst stage of\nTinyBERT (Jiao et al., 2020b) except that only the\nlast-layer loss is used, and ELECTRAbase (Clark\net al., 2019) is used as the teacher model.\nBaselines. For the pre-training baselines, we in-\nclude PF ( Pre-training + Fine-tuning, proposed\nby Turc et al. (2019)), BERT-S* (BERT under\nseveral hyper-parameter conÔ¨Ågurations), and NAS-\nBERT (Xu et al., 2021). Both PF and BERT-\nS* follow the conventional setting rule of hyper-\nparameters. BERT-S* uses the training setting:\npeak learning rate of 1e-5, warmup rate of 0.1,\nbatch size of 256 and 10 running epochs. NAS-\nBERT searches the architecture built on the non-\nidentical layer and heterogeneous modules. For\nthe distillation baselines, we compare some typical\nmethods, including DistilBERT, BERT-PKD, Tiny-\nBERT, MiniLM, and MobileBERT. The Ô¨Årst four\nmethods use the conventional architectures. Mo-\nbileBERT is equipped with a bottleneck structure\nand a carefully designed balance between MHA\nand FFN. We also consider BERT-KD-S*, which\nuse the same training setting of BERT-S*, except\nfor the loss of knowledge distillation. BERT-KD-\nS* also uses ELECTRAbase as the teacher model.\n4.2 Results and Analysis\nThe experiment is conducted under different la-\ntency constraints that are from 4 √óto 30√ófaster\nthan the inference of BERTbase. The results of pre-\ntraining and task-agnostic distillation are shown in\nthe Table 1 and Table 2 respectively.\nWe observe that in the settings of the pre-training\nand knowledge distillation, the performance gap of\ndifferent models with similar inference time is ob-\nvious, which shows the necessity of architecture op-\ntimization for efÔ¨Åcient PLMs. In the Table 1, some\nobservations are: (1) the architecture optimization\nmethods of AutoTinyBERT and NAS-BERT out-\nperform both BERT and PF that use the default\n5152\n40 50 60 70 80\nOne-shot Acc.(%) \n (a) Baseline(HAT)\n50\n55\n60\n65\n70\n75\n80\n85\nStand-Alone Acc.(%)\n40 50 60 70 80\nOne-shot Acc.(%) \n (b) ILS\n50\n55\n60\n65\n70\n75\n80\n85\nStand-Alone Acc.(%)\n40 50 60 70 80\nOne-shot Acc.(%)\n (c) ILS + SME\n50\n55\n60\n65\n70\n75\n80\n85\nStand-Alone Acc.(%)\n50 55 60 65 70 75 80 85\nOne-shot Acc.(%)\n (d) Our (ILS + SME + EBL)\n50\n55\n60\n65\n70\n75\n80\n85\nStand-Alone Acc.(%)\nFigure 4: Ablation study of one-shot SuperPLM learning. Acc. means the average score on SQuAD and MNLI.\nThe dashed line represents the function of y=x.\nTraining SQuAD MNLIPairwise\nMethod F1(%) Acc.(%)Acc.(%)\nStand-alone 71.2 76.3 100\nBaseline (HAT) 50.1 72.7 90.0\nILS 52.9 73.4 91.6\nILS + SME 59.5 74.1 94.2\nILS + SME + EBL (Ours)70.5 74.4 96.7\nTable 3: Ablation Study of SuperPLM. ILS, SME and\nEBL mean that the identical layer structure, MHA sub-\nmatrix extraction and effective batch-wise training.\narchitecture hyper-parameters; (2) our method out-\nperforms NAS-BERT that is built with the non-\nidentical layer and heterogeneous modules, which\nshows that the proposed method is effective for the\narchitecture search of efÔ¨Åcient PLMs. In the Ta-\nble 2, we observe that: (1) our method consistently\noutperforms the conventional structure in all the\nspeedup constraints; (2) our method outperforms\nthe classical distillation methods (e.g., BERT-PKD,\nDistilBERT, TinyBERT, and MiniLM) that use the\nconventional architecture. Moreover, AutoTiny-\nBERT achieves comparable results with Mobile-\nBERT, and its inference speed is 1.5√ófaster.\n4.3 Ablation Study of One-shot Learning\nWe demonstrate the effectiveness of one-shot learn-\ning by comparing the performance of one-shot\nmodel and stand-alone trained model on the given\narchitectures. We choose 16 architectures and their\ncorresponding PF models6 as the evaluation bench-\nmark. The pairwise accuracy is used as a metric to\nindicate the ranking correction between the archi-\ntectures under one-shot training and the ones under\nstand-alone full training (Luo et al., 2019) and its\nformula is described in Appendix E.\nWe do the ablation study to analyze the effect\nof proposed identical layer structure (ILS), MHA\nsub-matrix extraction (SME) and effective batch-\nwise learning (EBL) on SuperPLM learning. More-\n6The Ô¨Årst 16 models https://github.com/\ngoogle-research/bert from 2L128D to 8L768D.\nVersionBERT AutoTinyBERT Speedup\nPre-training\nS1 4-512-2048-8-5125-564-1054-8-5127.1/7.2√ó\nS2 4-320-1280-5-3204-396-624-6-38414.8/15.7√ó\nS3 4-256-1024-4-2564-432-384-4-25620.1/20.2√ó\nS4 4-192-768-3-1923-320-608-4-25628.4/27.2√ó\nTask-agnostic BERT Distillation\nKD-S1 4-512-2048-12-5165-564-1024-12-5284.9/4.6√ó\nKD-S2‚Ä†4-312-1200-12-3125-324-600-12-3249.8/9.0√ó\nKD-S3 4-264-1056-12-2645-280-512-12-27611.7/10.7√ó\nKD-S4 4-192-768-12-1924-256-480-12-19217.0/17.0√ó\nTable 4: BERT and AutoTinyBERT architectures un-\nder the different speedup constraints. The architecture\nis formatted as ‚Äúlt-dm|o-df -h-dq|k|v‚Äù. We assume that\ndq|k = dv in the experiment for the training and search\nefÔ¨Åciency. ‚Ä†means that we use the structure of Tiny-\nBERT and do not strictly follow the conventional rule.\nover, we introduce HAT (Wang et al., 2020a), as a\nbaseline of one-shot learning. HAT focuses on the\nsearch space of non-identical layer structures. The\nresults are displayed in Table 3 and Figure 4.\nIt can be seen from the Ô¨Ågure that compared with\nstand-alone trained models, the HAT baseline has\na signiÔ¨Åcant performance gap, especially in small\nsizes. Both ILS and SME beneÔ¨Åt the one-shot learn-\ning for large and medium-sized models. When\nfurther combined with EBL, SuperPLM can ob-\ntain similar or even better results than stand-alone\ntrained models of small sizes and perform close to\nstand-alone trained models of big sizes. The results\nof the table show that: (1) the proposed techniques\nhave positive effects on SuperPLM learning, and\nEBL brings a signiÔ¨Åcant improvement on a chal-\nlenging task of SQuAD; (2) SuperPLM achieves a\nhigh pairwise accuracy of 96.7% which indicates\nthat the proposed SuperPLM can be a good proxy\nmodel for the search process; (3) the performance\nof SuperPLM is still a little worse than the stand-\nalone trained model and we need to do the further\ntraining to boost the performance.\n5153\nMethod SpeedupSearch CostTraining CostAvg.(GPU Hours)(GPU Hours)\nBERT-S5 9.9√ó 0 580 76.1\nAutoTinyBERT-S510.8√ó 150 870 77.9\nAutoTinyBERT-Fast-S510.3√ó 12 290 77.6\nTable 5: Computation cost of different methods. AutoTiny-\nBERT and AutoTinyBERT-Fast have 100 and 8 architectures\n(√ó1.5 V100 GPU hour) respectively to be tested in the search\nprocess. AutoTinyBERT performs further 5 epochs ( √ó58\nV100 GPU hours) training for top three architectures, BERT\nis trained from scratch with 10 epochs, and AutoTinyBERT-\nFast does the further training for one architecture. We give\nmore information including the model architectures and de-\ntailed scores of all tasks in the Appendix F.\n0 10 20 30 40 50 60 70 80\nTraining step (10k)\n1\n2\n3\n4\n5\n6\n7Pre-training loss\n350k steps 700k steps\nAutoTinyBERT-S5\nAutoTinyBERT-S5-TFS\nFigure 5: Learning curves of AutoTinyBERT and\nthe stand-alone trained model. TFS means the\nmodel trained from scratch. AutoTinyBERT can\nsave 50% training time compared with the model\ntrained from scratch.\n4.4 Fast Development of EfÔ¨Åcient PLM\nIn this section, we explore an effective setting rule\nof hyper-parameters based on the obtained architec-\ntures and also discuss the computation cost of the\ndevelopment of efÔ¨Åcient PLM. The conventional\nand new architectures are displayed in Table 4. We\nobserve that AutoTinyBERT follows an obvious\nrule (except the S3 model) in the speedup con-\nstraints that are from 4√óto 30√ó. The rule is sum-\nmarized as: {1.6dm ‚â§df ‚â§1.9dm, 0.7dm ‚â§\ndq|k|v ‚â§1.0dm}.\nWith the above rule, we propose a faster way to\nbuild efÔ¨Åcient PLM, denoted as AutoTinyBERT-\nFast. SpeciÔ¨Åcally, we Ô¨Årst obtain the candidates by\nthe rule, and then select Œ±opt from the candidates.\nWe observe the fact that the candidates of the same\nlayer number seem to have similar shapes and we\nassume that they have similar performance. There-\nfore, we only need to test one architecture at each\nlayer number and choose the best one as Œ±opt.\nTo demonstrate the effectiveness of the pro-\nposed method, we evaluate these methods at a\nnew speedup constraint of about 10 √óunder the\npre-training setting. The results are shown in Ta-\nble 5. We Ô¨Ånd AutoTinyBERT is efÔ¨Åcient and its\ndevelopment time is twice that of the conventional\nmethod (BERT) and the result is improved by about\n1.8%. AutoTinyBERT-Fast achieves a competitive\nscore of 77.6 by only about 50% of BERT training\ntime. In addition to the proposed search method\nand fast building rule, one reason for the high efÔ¨Å-\nciency of AutoTinyBERT is that the initialization\nof SuperPLM helps the model to achieve 2 √óthe\nconvergence speedup, as illustrated in Figure 5.\n5 Related Work\nEfÔ¨Åcient PLMs with Tiny sizes. There are two\nwidely-used methods for building efÔ¨Åcient PLMs:\npre-training and model compression. Knowledge\ndistillation (KD) (Hinton et al., 2015; Romero\net al., 2014) is the most widely studied technique\nin PLM compression, which uses a teacher-student\nframework. The typical distillation studies include\nDistilBERT (Sanh et al., 2019), BERT-PKD (Sun\net al., 2019), MiniLM (Wang et al., 2020b), Mobile-\nBERT (Sun et al., 2020), MiniBERT (Tsai et al.,\n2019) and ETD (Chen et al., 2021). In addition to\nKD, the techniques of pruning (Han et al., 2016;\nHou et al., 2020), quantization (Shen et al., 2020;\nZhang et al., 2020; Wang et al., 2020c) and pa-\nrameter sharing (Lan et al., 2019) introduced for\nPLM compression. Our method is orthogonal to\nthe building method of efÔ¨Åcient PLM and is trained\nunder the settings of pre-training and task-agnostic\nBERT distillation, which can be used by direct Ô¨Åne-\ntuning.\nNAS for NLP.NAS is extensively studied in com-\nputer vision (Tan and Le, 2019; Tan et al., 2020),\nbut relatively little studied in the natural language\nprocessing. Evolved Transformer (So et al., 2019)\nand HAT (Wang et al., 2020a) search architec-\nture for Transformer-based neural machine transla-\ntion. For BERT distillation, AdaBERT (Chen et al.,\n2020) focuses on searching the architecture in the\nÔ¨Åne-tuning stage and relies on data augmentation\nto improve its performance. schuBERT (Khetan\nand Karnin, 2020) obtains the optimal structures\nof PLM by a pruning method. A work similar to\nours is NAS-BERT (Xu et al., 2021). It proposes\nsome techniques to tackle the challenging exponen-\ntial search space of non-identical layer structure\nand heterogeneous modules. Our method adopts a\nlinear search space and introduces several practical\ntechniques for SuperPLM training. Moreover, our\nmethod is efÔ¨Åcient in terms of computation cost\nand the obtained PLMs are easy to use.\n5154\n6 Conclusion\nWe propose an effective and efÔ¨Åcient method Au-\ntoTinyBERT to search for the optimal architecture\nhyper-parameters of efÔ¨Åcient PLMs. We evaluate\nthe proposed method in the scenarios of both the\npre-training and task-agnostic BERT distillation.\nThe extensive experiments show that AutoTiny-\nBERT can consistently outperform the baselines\nunder different latency constraints. Furthermore,\nwe develop a fast development rule for efÔ¨Åcient\nPLMs which can build an AutoTinyBERT model\neven with less training time of a conventional one.\nAcknowledgments\nWe thank all the anonymous reviewers for their\nvaluable comments. We thank MindSpore7 for the\npartial support of this work, which is a new deep\nlearning computing framework.\nReferences\nAndrew Brock, Theo Lim, JM Ritchie, and Nick Weston.\n2018. Smash: One-shot model architecture search\nthrough hypernetworks. In ICLR.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. NeurIPS.\nHan Cai, Ligeng Zhu, and Song Han. 2018. Proxyless-\nnas: Direct neural architecture search on target task\nand hardware. In ICLR.\nCheng Chen, Yichun Yin, Lifeng Shang, Zhi Wang,\nXin Jiang, Xiao Chen, and Qun Liu. 2021. Extract\nthen distill: EfÔ¨Åcient and effective task-agnostic bert\ndistillation.\nDaoyuan Chen, Yaliang Li, Minghui Qiu, Zhen Wang,\nBofang Li, Bolin Ding, Hongbo Deng, Jun Huang,\nWei Lin, and Jingren Zhou. 2020. Adabert: Task-\nadaptive bert compression with differentiable neural\narchitecture search. In IJCAI.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2019. Electra: Pre-training\ntext encoders as discriminators rather than generators.\nIn ICLR.\nJ. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In\nNAACL-HLT.\n7MindSpore. https://www.mindspore.cn/\nKai Han, Yunhe Wang, Qiulin Zhang, Wei Zhang, Chun-\njing Xu, and Tong Zhang. 2020. Model rubik‚Äôs cube:\nTwisting resolution, depth and width for tinynets.\nNeurIPS.\nSong Han, Huizi Mao, and William J Dally. 2016. Deep\ncompression: Compressing deep neural networks\nwith pruning, trained quantization and huffman cod-\ning. ICLR.\nGeoffrey E. Hinton, Oriol Vinyals, and J. Dean. 2015.\nDistilling the knowledge in a neural network. ArXiv,\nabs/1503.02531.\nLu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao\nChen, and Qun Liu. 2020. Dynabert: Dynamic bert\nwith adaptive width and depth. NeurIPS, 33.\nXiaoqi Jiao, Huating Chang, Yichun Yin, Lifeng Shang,\nXin Jiang, Xiao Chen, Linlin Li, Fang Wang, and\nQun Liu. 2020a. Improving task-agnostic bert dis-\ntillation with layer mapping search. arXiv preprint\narXiv:2012.06153.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao\nChen, Linlin Li, Fang Wang, and Qun Liu. 2020b.\nTinybert: Distilling bert for natural language under-\nstanding. In EMNLP: Findings.\nAshish Khetan and Zohar Karnin. 2020. schubert: Opti-\nmizing elements of bert. In ACL.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learning\nof language representations. In ICLR.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nRenqian Luo, Tao Qin, and Enhong Chen. 2019.\nBalanced one-shot neural architecture optimization.\narXiv preprint arXiv:1909.10815.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In EMNLP.\nAdriana Romero, Nicolas Ballas, Samira Ebrahimi Ka-\nhou, Antoine Chassang, Carlo Gatta, and Yoshua\nBengio. 2014. Fitnets: Hints for thin deep nets.\narXiv preprint arXiv:1412.6550.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\n5155\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei\nYao, Amir Gholami, Michael W Mahoney, and Kurt\nKeutzer. 2020. Q-bert: Hessian based ultra low pre-\ncision quantization of bert. In AAAI.\nDavid So, Quoc Le, and Chen Liang. 2019. The evolved\ntransformer. In ICML.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for bert model com-\npression. In EMNLP-IJCNLP.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. Mobilebert: a\ncompact task-agnostic bert for resource-limited de-\nvices. In ACL.\nMingxing Tan and Quoc Le. 2019. EfÔ¨Åcientnet: Re-\nthinking model scaling for convolutional neural net-\nworks. In ICML.\nMingxing Tan, Ruoming Pang, and Quoc V Le. 2020.\nEfÔ¨Åcientdet: Scalable and efÔ¨Åcient object detection.\nIn CVPR.\nHenry Tsai, Jason Riesa, Melvin Johnson, Naveen Ari-\nvazhagan, Xin Li, and Amelia Archer. 2019. Small\nand practical bert models for sequence labeling. In\nEMNLP-IJCNLP.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nOn the importance of pre-training compact models.\narXiv preprint arXiv:1908.08962.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NeurIPS.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In EMNLP Workshop\nBlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP.\nHanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai,\nLigeng Zhu, Chuang Gan, and Song Han. 2020a. Hat:\nHardware-aware transformers for efÔ¨Åcient natural\nlanguage processing. In ACL.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020b. Minilm: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers.\nZiheng Wang, Jeremy Wohlwend, and Tao Lei. 2020c.\nStructured pruning of large language models. In\nEMNLP.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R¬¥emi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nEMNLP: System Demonstrations.\nLingxi Xie and Alan Yuille. 2017. Genetic cnn. In\nICCV.\nJin Xu, Xu Tan, Renqian Luo, Kaitao Song, Li Jian,\nTao Qin, and Tie-Yan Liu. 2021. Task-agnostic and\nadaptive-size bert compression. In openreview.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. In NeurIPS.\nJiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Ben-\nder, Pieter-Jan Kindermans, Mingxing Tan, Thomas\nHuang, Xiaodan Song, Ruoming Pang, and Quoc Le.\n2020. Bignas: Scaling up neural architecture search\nwith big single-stage models. In ECCV.\nWei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao\nChen, Xin Jiang, and Qun Liu. 2020. Ternarybert:\nDistillation-aware ultra-low bit bert. In EMNLP.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In ICCV.\n5156\nA Code ModiÔ¨Åcations for\nAutoTinyBERT.\nWe modify the original code 8 to load AutoTiny-\nBERT model and present the details of code mod-\niÔ¨Åcations in the Figure B.1. We assume that\ndq/k = dv, and more complicated setting is that dv\ncan be different with dq/k, we can do correspond-\ning changes based on the given modiÔ¨Åcations.\nB Search Space of Architecture\nHyper-parameters.\nWe has trained two SuperPLMs with a architec-\nture of {lt=8, dm/q/k/v=768, df=3072}to cover\nthe two scenarios of building efÔ¨Åcient PLMs (pre-\ntraining and task-agnostic BERT distillation). The\nsampling space in the SuperPLM training is the\nsame as the search space in the search process, as\nshown in the Table B.1. It can be inferred from\nthe table that the search spaces of the pre-training\nsetting and the knowledge distillation setting are\nabout 46M and 10M, respectively.\nVariables Search Space\nSuperPLM in Pre-training\nlt [1,2,3,4,5,6,7,8]\ndm/o [128,132,...,4k,...,764,768]\ndf [128,132,...,4k,...,3068,3072]\nh [1,2,...,k,...,11,12]\ndq/k/v 64h\nSuperPLM in Knowledge Distillation\nlt [1,2,3,4,5,6,7,8]\ndm/o [128,132,...,4k,...,764,768]\ndf [128,132,...,4k,...,3068,3072]\nh [12]\ndq/k/v [180,192,...,12k,...,756,768]\nTable B.1: The search space for architecture hyper-\nparameters. We assume that dq|k = dv in the exper-\niment for the training and search efÔ¨Åciency.\nC Evolutionary Algorithm.\nWe give a detailed description of evolutionary al-\ngorithm in Algorithm 2.\nD Hyper-parameters for Fine-Tuning.\nFine-tuning hyper-parameters of GLUE benchmark\nand SQuAD are displayed in Table D.1. AutoTiny-\n8https://github.com/huggingface/\ntransformers\nBERT and baselines follow the same settings.\nTasks Batch Learning Epochssize rate\nSQuAD 16 3e-5 4\nSST-2 32 2e-5 4\nMNLI 32 3e-5 4\nMRPC 32 2e-5 10\nCoLA 32 1e-5 10\nQNLI 32 2e-5 10\nQQP 32 2e-5 5\nSTS-B 32 3e-5 10\nRTE 32 2e-5 10\nTable D.1: Hyper-parameters used for Ô¨Åne-tuning on\nGLUE benchmark and SQuAD.\nE Pairwise Accuracy.\nWe denote a set of architectures {Œ±1,Œ±2,...,Œ± n}\nas Aeva and evaluate SuperPLM on this set. The\npairwise accuracy is formulated as bellow:\n‚àë\nŒ±1‚ààAeva,Œ±2 ‚ààAeva 1f(Œ±1)‚â•f(Œ±2)1s(Œ±1)‚â•s(Œ±2)\n‚àë\nŒ±1‚ààAeva,Œ±2 ‚ààAeva 1 ,\n(5)\nwhere 1 is the 0-1 indicator function, f(‚àó) and s(‚àó)\nrefer to the performance of one-shot model and\nstand-alone trained model respectively.\nF More details for Fast Development of\nefÔ¨Åcient PLM.\nWe present the detailed results and architecture\nhyper-parameters for fast development of efÔ¨Åcient\nPLM in Table F.1.\n5157\nclass BertSelfAttention(nn.Module):\ndef __init__(self, config):\n### Before modifications:\nself.attention_head_size = int(config.hidden_size /\nconfig.num_attention_heads)\n### After modifications:\ntry:\nqkv_size = config.qkv_size\nexcept:\nqkv_size = config.hidden_size\nself.attention_head_size = int(qkv_size / config.num_attention_heads)\nclass BertSelfOutput(nn.Module):\ndef __init__(self, config):\n### Before modifications:\nself.dense = nn.Linear(config.hidden_size, config.hidden_size)\n### After modifications:\ntry:\nqkv_size = config.qkv_size\nexcept:\nqkv_size = config.hidden_size\nself.dense = nn.Linear(qkv_size, config.hidden_size)\nFigure B.1: Code ModiÔ¨Åcations to load AutoTinyBERT.\nAlgorithm 2 The Evolutionary Algorithm\n1: Input: the number of generations T = 4, the number of archtectures Œ±s in each generation S = 25,\nthe mutation Mut(‚àó) probability pm = 1/2, the exploration probability pe = 1/2.\n2: Sample Ô¨Årst generation G1 from A, and Evoluator produces its performance V1.\n3: for t= 2,3 ¬∑¬∑¬∑ ,T do\n4: Gt ‚Üê{}\n5: while |Gt|<S do\n6: Sample one architecture: Œ±with a Russian roulette process on Gt‚àí1 and Vt‚àí1.\n7: With probability pm, do Mut(‚àó) for Œ±.\n8: With probability pe, sample a new architecture from A.\n9: Append the newly generated architectures into Gt.\n10: end while\n11: Evaluator obtains Vt for Gt.\n12: end for\n13: Output: Output the Œ±opt with best performance in the above process.\nModel SpeedupSQuAD SST-2 MNLI MRPC CoLA QNLI QQP STS-B RTEScore\nBERT-S54‚àí384‚àí1536‚àí6‚àí384 9.3√ó 78.5 86.1 76.8 83.1 35.5 84.6 87.5 86.9 65.7 76.0\nAutoTinyBERT-S55‚àí450‚àí636‚àí6‚àí384 10.8√ó 79.7 89.1 78.3 84.6 39.0 85.9 88.2 87.4 68.7 77.8\nAutoTinyBERT-Fast-S55‚àí432‚àí720‚àí6‚àí384 10.3√ó 80.0 88.2 77.9 84.6 37.7 86.1 88.0 87.3 68.7 77.6\nTable F.1: Detailed results for fast development of efÔ¨Åcient PLM.",
  "topic": "Chen",
  "concepts": [
    {
      "name": "Chen",
      "score": 0.9221041202545166
    },
    {
      "name": "Computer science",
      "score": 0.6480851173400879
    },
    {
      "name": "Computational linguistics",
      "score": 0.6103924512863159
    },
    {
      "name": "Natural language processing",
      "score": 0.5539054274559021
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.4956263303756714
    },
    {
      "name": "Association (psychology)",
      "score": 0.4862164556980133
    },
    {
      "name": "Joint (building)",
      "score": 0.48233482241630554
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4813874065876007
    },
    {
      "name": "Linguistics",
      "score": 0.3565586805343628
    },
    {
      "name": "Philosophy",
      "score": 0.14410531520843506
    },
    {
      "name": "Engineering",
      "score": 0.11035868525505066
    },
    {
      "name": "Epistemology",
      "score": 0.06094461679458618
    },
    {
      "name": "Physics",
      "score": 0.05664828419685364
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210116209",
      "name": "The Ark",
      "country": "IE"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    }
  ]
}