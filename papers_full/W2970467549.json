{
    "title": "Open Domain Web Keyphrase Extraction Beyond Language Modeling",
    "url": "https://openalex.org/W2970467549",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A2222088932",
            "name": "Lee Xiong",
            "affiliations": [
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2109976295",
            "name": "Chuan Hu",
            "affiliations": [
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2226924701",
            "name": "Chenyan Xiong",
            "affiliations": [
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2112344567",
            "name": "Daniel Campos",
            "affiliations": [
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2971209968",
            "name": "Arnold Overwijk",
            "affiliations": [
                "Microsoft (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2167329753",
        "https://openalex.org/W32253530",
        "https://openalex.org/W2163659824",
        "https://openalex.org/W2963531963",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2049119796",
        "https://openalex.org/W1525595230",
        "https://openalex.org/W2030903088",
        "https://openalex.org/W2963275829",
        "https://openalex.org/W3102654612",
        "https://openalex.org/W2045181608",
        "https://openalex.org/W2895865835",
        "https://openalex.org/W2962903510",
        "https://openalex.org/W2148212498",
        "https://openalex.org/W2114874863",
        "https://openalex.org/W2145049651",
        "https://openalex.org/W2156577800",
        "https://openalex.org/W2146769536",
        "https://openalex.org/W2091895997",
        "https://openalex.org/W2060772621",
        "https://openalex.org/W2890515900",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2031302834",
        "https://openalex.org/W1567365482",
        "https://openalex.org/W1983873791",
        "https://openalex.org/W2115584760",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2963265326",
        "https://openalex.org/W2888766462",
        "https://openalex.org/W4385245566"
    ],
    "abstract": "Lee Xiong, Chuan Hu, Chenyan Xiong, Daniel Campos, Arnold Overwijk. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",
    "full_text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing, pages 5175‚Äì5184,\nHong Kong, China, November 3‚Äì7, 2019.c‚Éù2019 Association for Computational Linguistics\n5175\nOpen Domain Web Keyphrase Extraction\nBeyond Language Modeling\nLee Xiong Chuan Hu Chenyan Xiong Daniel Campos Arnold Overwijk\nMicrosoft AI and Research\nRedmond, W A 98052, USA\n{lexion, chuah, cxiong, dacamp, arnoldo}@microsoft.com\nAbstract\nThis paper studies keyphrase extraction in\nreal-world scenarios where documents are\nfrom diverse domains and have variant con-\ntent quality. We curate and release OpenKP, a\nlarge scale open domain keyphrase extraction\ndataset with near one hundred thousand web\ndocuments and expert keyphrase annotations.\nTo handle the variations of domain and con-\ntent quality, we develop BLING-KPE, a neu-\nral keyphrase extraction model that goes be-\nyond language understanding using visual pre-\nsentations of documents and weak supervision\nfrom search queries. Experimental results on\nOpenKP conÔ¨Årm the effectiveness of BLING-\nKPE and the contributions of its neural archi-\ntecture, visual features, and search log weak\nsupervision. Zero-shot evaluations on DUC-\n2001 demonstrate the improved generalization\nability of learning from the open domain data\ncompared to a speciÔ¨Åc domain.\n1 Introduction\nAutomatically extracting keyphrases that are\nsalient to the document meanings is an essential\nstep to semantic document understanding. An\neffective keyphrase extraction (KPE) system can\nbeneÔ¨Åt a wide range of natural language process-\ning and information retrieval tasks (Turney, 2001;\nHasan and Ng, 2014). Recent neural methods\nformulate the task as a document-to-keyphrase\nsequence-to-sequence task. These neural KPE\nmodels have shown promising results compared to\nprevious systems (Chen et al., 2018; Meng et al.,\n2017; Ye and Wang, 2018).\nNoticeably, the recent progress in neural KPE\nis mostly observed in documents originating from\nthe scientiÔ¨Åc domain (Meng et al., 2017; Augen-\nstein et al., 2017). Perhaps because the scientiÔ¨Åc\ndomain has sufÔ¨Åcient training data for these neu-\nral methods: Authors are in the practice of as-\nsigning keyphrases to their publications. In real-\nworld scenarios, most potential applications of\nKPE deal with diverse documents originating from\nsparse sources that are rather different from scien-\ntiÔ¨Åc papers. They often include a much diverse\ndocument structure and reside in various domains\nwhose contents target much wider audiences than\nscientists. It is unclear how well the neural meth-\nods trained in the scientiÔ¨Åc domain generalize to\nother domains and in real-world scenarios.\nThis paper focuses on the task of open domain\nweb keyphrase extraction, which targets KPE for\nweb documents without any restriction of the\ndomain, quality, nor content of the documents.\nWe curate and release a large scale open do-\nmain KPE dataset, OpenKP, which includes about\none hundred thousand web documents with ex-\npert keyphrase annotations.1 The web documents\nare randomly sampled from the English fraction\nof a large web corpus and reÔ¨Çect the characteris-\ntics of typical web pages, with large variation in\ntheir domains and content qualities. To the best of\nour knowledge, this will be the Ô¨Årst publicly avail-\nable open domain manually annotated keyphrase\nextraction dataset at this scale.\nThis paper develops BLING-KPE, Beyond\nLanguage UnderstandING KeyPhrase Extraction,\nthat tackles the challenges of KPE in docu-\nments from variant domains and content qualities.\nBLING-KPE uses a convolutional transformer ar-\nchitecture to model the language properties in the\ndocument, while also goes beyond by introduc-\ning the visual representation of the document and\nweak supervision from search user clicks.\nThe visual presentations of the document, in-\ncluding the location, size, font, and HTML struc-\nture of each text piece in the document, are inte-\ngrated as visual features to the word embeddings\n1The dataset, resources, and future updates are available\nat aka.ms/BLING.\n5176\nin BLING-KPE. BLING-KPE learns to model the\nvisual representations together with the document\nlanguage in its network.\nThe weak supervision from search clicks is for-\nmulated as a pre-training task: Query Prediction.\nIt trains the model to predict which phrase in the\ndocument has been used as a ‚Äúclick query‚Äù, a\nquery that a user issued to search and click on the\ndocument. The click queries on a document re-\nÔ¨Çect the user‚Äôs perceptions of the relatedness and\nimportance when searching the document and can\nbe considered as pseudo keyphrases. Pre-training\non this weak supervision brings in training signals\navailable at scale in commercial search systems.\nOur experiments on OpenKP demonstrate the\neffectiveness of BLING-KPE. It outperforms stan-\ndard KPE baselines, recent neural approaches and\na highly optimized commercial KPE system by\nlarge margins. Ablation studies show the contri-\nbutions of the neural architecture, visual features,\nand search weak supervision to BLING-KPE; re-\nmoving any of them signiÔ¨Åcantly reduces its accu-\nracy.\nAnother advantage of learning from real-world\nopen domain documents is improved generaliza-\ntion ability. We conduct zero-shot evaluations on\nthe DUC-2001 news KPE datasets (Wan and Xiao,\n2008b), where neural KPE systems are evaluated\nwithout seeing any labels from their news articles.\nBLING-KPE trained on OpenKP is the only neu-\nral method that outperforms traditional non-neural\nKPE methods, while neural KPE systems trained\non the scientiÔ¨Åc documents do not generalize well\nto the news domain due to the domain differences.\n2 Related Work\nThe classic keyphrase extraction systems typically\ninclude two components: candidate keyphrase\nextraction and keyphrase importance estima-\ntion (Hasan and Ng, 2014). The candidate\nkeyphrases are often extracted by heuristic rules,\nfor example, Ô¨Ånding phrases following certain\nPOS tag sequences (Wan and Xiao, 2008b; Liu\net al., 2009a; Mihalcea and Tarau, 2004), pre-\ndeÔ¨Åned lexical patterns (Nguyen and Phan, 2009;\nMedelyan et al., 2009), or using entities as candi-\ndate phrases (Grineva et al., 2009).\nThe importance of the candidate keyphrases\ncan be estimated by unsupervised or supervised\nmethods. The unsupervised methods leverage\nthe graph structures between phrases in the doc-\nument (Mihalcea and Tarau, 2004; Wan and Xiao,\n2008a,b), and topic information from topic mod-\neling (Grineva et al., 2009; Liu et al., 2009b,\n2010). The supervised keyphrase selection meth-\nods formulate a classiÔ¨Åcation or ranking task and\ncombine features from phrase frequencies (Wit-\nten et al., 2005), document structures (Chen et al.,\n2005; Yih et al., 2006), and external resources\nsuch as Wikipedia (Medelyan et al., 2009) and\nquery log (Yih et al., 2006).\nRecently, neural techniques have been applied\nto keyphrase tasks. Meng et al. formulate a\nseq2seq learning task that learns to extract and\ngenerate the keyphrase sequence from the docu-\nment sequence; they incorporate a copy mecha-\nnism to the seq2seq RNN to extract phrases in\nthe generation process (CopyRNN) (Meng et al.,\n2017). Improving this seq2seq setup has been the\nfocus of recent research, for example, adding di-\nverse constraints to reduce the duplication of pro-\nduced keyphrases (Yuan et al., 2018; Chen et al.,\n2018), bringing auxiliary tasks to reduce the needs\nof training data (Ye and Wang, 2018), and adding\ntitle information to improve model accuracy (Chen\net al., 2019).\nThe recent neural KPE methods have shown\nstrong performances on the scientiÔ¨Åc domain,\nwhere large scale training data is available from\nthe author assigned keyphrases on papers (Meng\net al., 2017). Such speciÔ¨Åc domain training data\nlimits the model generalization ability. Chen et\nal. show the seq2seq keyphrase generation mod-\nels trained on scientiÔ¨Åc papers do not generalize\nwell to another domain (Chen et al., 2018).\nIn general, previous research Ô¨Ånds automatic\nkeyphrase extraction a challenging task: its state-\nof-the-art accuracy is much lower than other lan-\nguage processing tasks, while supervised meth-\nods do not necessarily outperform simple unsuper-\nvised ones. Hasan and Ng (2014) pointed out po-\ntential ways to improve automatic keyphrase ex-\ntraction, including better incorporation of back-\nground knowledge , better handling long docu-\nments, and better evaluation schemes. BLING-\nKPE aims to address these challenges by incorpo-\nrating pre-training as a form ofbackground knowl-\nedge, visual information to improve long docu-\nment modeling, and OpenKP as a large scale open\ndomain evaluation benchmark.\n5177\n3 Open Domain Keyphrase Benchmark\nThis section describes the curation of OpenKP\nand its notable characteristics.\n3.1 Data Curation\nDocuments in OpenKP include about seventy\nthousand web pages sampled from the index of\nBing search engine. 2 The sampling is conducted\non the pool of pages seen by United State users\nbetween Nov 2018 and Feb 2019.\nThere is no restriction on the domain or type of\ndocuments. They can be content-oriented pages\nlike news articles, multi-media pages from video\nsites, or indexing pages with many hyperlinks.\nOpenKP is designed to reÔ¨Çect the diverse prop-\nerties of web documents in the internet.\nKeyphrase Labels are generated by our ex-\npert annotators. For each document, they exam-\nine the rendered web page and manually label 1-3\nkeyphrases following these deÔ¨Ånitions:\n‚Ä¢Salience: A keyphrase captures the essential\nmeaning of the page with no ambiguity.\n‚Ä¢Extraction: The keyphrase has to appear in\nthe document.\n‚Ä¢Fine-Grained: The keyphrase cannot be gen-\neral topics, such as ‚ÄúSports‚Äù and ‚ÄúPolitics‚Äù.\n‚Ä¢Correct & Succinct: The keyphrase has to\nform a correct English noun phrase, while\nalso cannot be clauses or sentences.\nWe use the extraction setting to ensure label-\ning consistency and to increase annotation speed,\nwhich is around 42 pages per hour.\nExpert Agreements. Our annotation experts\nare trained employees dedicated to providing high-\nquality annotations on web documents. We fol-\nlow standard practice in generating annotations for\nproduction systems, which included regular touch-\npoints to understand the confusion, as well as up-\ndates on the judgment guidelines to resolve ambi-\nguities.\nTo study the task difÔ¨Åculty, we had Ô¨Åve judges\neach annotate the same 50 random URLs. We\nmeasure the pairwise agreements between experts\nat different depths by Exact Match on the whole\nkeyphrase, as well as the overlap between select\nkeyphrases‚Äô unigrams. The agreement between\njudges is listed in Table 1.\n2A new OpenKP version with 150K documents is avail-\nable at msmarco.org.\nTable 1: The agreements between pairs of expert\njudges at different annotation depth. Exact and\nUnigram show the percentage of judge agreement\non exact keyphrases and overlapped unigrams.\nJudge Depth Exact Match Unigram Match\nKeyphrase@1 64.74% 64.74%\nKeyphrase@2 48.30% 63.12%\nKeyphrase@3 43.51% 57.66%\nThe results conÔ¨Årm that open domain keyphrase\nextraction is not an easy task. When measur-\ning agreement for the top 3 keyphrases, our ex-\npert judges completely agree on about 43% of\nkeyphrase pairs. Compared to the previous small\nscale annotations, for example, on DUC-2001‚Äôs\nnews articles (Wan and Xiao, 2008b), annotating\nweb pages with diverse contents and domains are\nharder.\nWe manually examined these annotations and\nfound two sources of disagreement: Chunking\nVariances and KP Choices.\nChunking Variances we deÔ¨Åne as two judges\npick different boundaries of the same concept. For\nexample, one judge may select ‚ÄúProtein Synthe-\nsis‚Äù as the keyphrase, and others may select ‚ÄúPro-\ntein‚Äù and ‚ÄúSynthesis‚Äù as two separate keyphrases.\nWe found Chunking Variances consist of about\n20% of disagreements. As shown in Table 1, the\njudge agreements is substantially higher on Uni-\ngram overlaps than on Exact matches, indicating\nthat they may select chunks that overlap with each\nother but not exactly the same.\nKP Choices we deÔ¨Åne as two judges pick dif-\nferent keyphrases. The judges agree mostly (64%)\non the Ô¨Årst entered keyphrase, as shown in Table 1.\nThe variations on the second and third keyphrases\nare larger. However, we found the variations are\nmore about which keyphrases they choose to en-\nter, not about whether a phrase is a keyphrase\nor not. The variations on judge labels mostly re-\nÔ¨Çect the missing positives inOpenKP; most of the\nkeyphrases annotated by judges are correct. We\ncan reduce the missing positives by a deeper an-\nnotation, i.e. ten keyphrases per document, or by\nlabeling all candidate phrases with classiÔ¨Åcation\nlabels (Liu et al., 2018). However, that will sig-\nniÔ¨Åcantly reduce the number of total documents in\nOpenKP, as each document costs much more to\nannotate. We chose the current design choice of\nOpenKP to favor a larger amount of training la-\nbels, which, in our experience, is more effective in\n5178\n3.7%\n3.4%\n2.3%\n1.9%\n1.5%\n1.5%\n1.5%\n1.2%\n1.2%\n1.1%\n1.1%\n1.0%\n0.9%\n0.8%\n0.7%\nhealthcare\nautomobiles\nfood\nmusic\nmovies\nvideo games\nreal estate\ntravel\nprogramming\ncareers\neducation\ne-commerce\nU.S. politics\ntv series\nlaw\nOpen KP Document Topics (Top 15)  \nFigure 1: The most popular topics in OpenKP .\nTable 2: Statistics of OpenKP used in our experi-\nments. The new version on MSMARCO include\n150K documents.\nStatistics Mean STD\nDoc Length 900.4 1494.4\n# of KPs per Doc 1.8 0.8\nKeyphrase Length 2.0 0.96\nDoc V ocabulary Size 1.5M n.a.\nKP V ocabulary Size 62K n.a.\n# of Documents 68K n.a.\n# of Unique KPs 99.6K n.a.\ntraining deep neural models.\n3.2 Data Characteristics\nTable 2 lists the statistics of OpenKP. The docu-\nment length is the length of the text parsed from\nthe HTML of the web page, using a production\nHTML parser. The parsed texts will be released\nwith the dataset. These statistics reÔ¨Çect the large\nvariations in the document contents; their length\nvaries a lot and share little common keyphrases,\nas shown by a large number of unique keyphrases.\nWe also leverage a production classiÔ¨Åer to clas-\nsify OpenKP documents into 5K predeÔ¨Åned do-\nmains. The top 15 most popular classes and their\ndistributions are shown in Figure 1. As expected,\nthese documents have a large variation in their\ntopic domains. The most popular domain, ‚Äúhealth-\ncare‚Äù, only covers 3.7% documents; the tenth\nmost popular topic only covers 1% of documents.\nMoreover, the top 15 classes make up less than\n25% of the entire dataset which showcases what a\ndomain diverse dataset OpenKP is.\n4 Keyphrase Extraction Model\nThis section describes the architecture, visual fea-\ntures, and weak supervision of BLING-KPE.\nùë§1 ‚Ä¶\nELMO \n‚Ä¶\nùë§2 ùë§3 ùë§ùëõ\n‚Ñé1 ‚Ñé2 ‚Ñé3 ‚Ñéùëõ\nùëù1 ùëù2 ùëù3 ùëùùëõ\nùë£1 ùë£2 ùë£3 ùë£ùëõ\nùëê1\n1 ùëê2\n1 ùëê3\n1 ‚Ä¶\nVisual Feature \nPositional Encoding\nùëê1\n2 ùëê2\n2 ‚Ä¶ ‚Ä¶\n1-Gram CNN 2-Gram CNN\nBi-Directional \nTransformer \nBi-Directional \nTransformer ‚Ä¶ shared weight\nFeed Forward Feed Forward ‚Ä¶ shared weight\nùë†1\n1 ùë†2\n1 ùë†3\n1 ‚Ä¶ ùë†1\n2 ùë†2\n2 ‚Ä¶ ‚Ä¶\nUp to N-Gram CNN\nUp to N-Gram Prediction\nFigure 2: The BLING-KPE model architecture\n4.1 Network Architecture\nAs shown in Figure 2, BLING-KPE is a keyphrase\nextraction model. It takes the word sequence of\nthe document, d = {w1, ...wi, ...wn}, and assigns\nkeyphrase scores to its n-gram: f(wi:i+k, d). This\nprocess includes two main components: Hybrid\nWord Embedding and Convolutional Transformer.\nHybrid Word Embedding. BLING-KPE rep-\nresents each word by its ELMo embedding, posi-\ntion embedding, and visual features.\nThe ELMo embedding brings the local contex-\ntual information:\n‚Éóhi = ELMo(wi). (1)\nThe standard pre-trained ELMo is used (Peters\net al., 2018).\nThe position embedding models the location\nthe word in the document content. It uses the\nstandard sinusoidal position embedding (Vaswani\net al., 2017):\n‚Éóposi(2p) =sin(i/100002p/P ), (2)\n‚Éóposi(2p + 1) =cos(i/100002p/P ). (3)\nThe p-th dimension of the position embedding is a\nfunction of its position (i) and dimension (p).\nThe visual features represent the visual presen-\ntation of each word. We denote the visual feature\nas ‚Éó vi and will describe its details in ¬ß4.2.\nThe hybrid word embedding is the concatena-\ntion of the three:\n‚Éó wi = ‚Éóhi ‚å¢ ‚Éóposi ‚å¢ ‚Éó vi. (4)\n5179\nConvolutional Transformer. BLING-KPE\nuses a convolutional transformer architecture to\nmodel n-grams and their interactions.\nIt Ô¨Årst composes the hybrid word embeddings to\nn-gram embeddings using CNNs. The embedding\nof i-th k-gram is calculated as\n‚Éó gk\ni = CNNk( ‚Éó wi:i+k), (5)\nwhere k is the length of the n-gram, 1 ‚â§k ‚â§K.\nK is the maximum length of allowed candidate n-\ngrams. Each k-gram has its own set of convolution\nÔ¨Ålters CNNk with window size k and stride 1.\nIt then models the interactions between k-grams\nusing Transformer (Vaswani et al., 2017).\n‚Éótk\ni = Transformeri( ‚ÉóGk), (6)\n‚ÉóGk = ‚Éó gk\n1 ‚å¢ ... ‚å¢ ‚Éó gk\ni ... ‚å¢ ‚Éó gk\nn‚àík+1. (7)\nThe sequence ‚ÉóGk is the concatenations of all k-\ngram embeddings. The Transformer models the\nself-attentions between k-grams and fuses them to\nglobal contextualized embeddings.\nThe Transformer is convolutional on all length\nk of n-grams; the same parameters are used model\nthe interactions between n-grams at each length, to\nreduce the parameter size. The intuition is that the\ninteractions between bi-grams and that between\ntri-grams are not signiÔ¨Åcantly different.\nThe Ô¨Ånal score of an n-gram is calculated by\na feedforward layer upon the Transformer. Like\nthe Transformer, the same feedforward layer is ap-\nplied (convolutional) on all n-grams.\nf(wi:i+k, d) =softmaxi,k(sk\ni ), (8)\nsk\ni = Feedforward(‚Éótk\ni ). (9)\nThe softmax is taken over all possible n-grams at\neach position i and each length k. The model de-\ncides the span location and length jointly.\nLearning. The whole model is trained as a clas-\nsiÔ¨Åcation problem using cross-entropy loss:\nl = Cross-Entropy(yk\ni , f(wi:i+k, d)), (10)\nwhere yk\ni is the label of whether the phrase wi:i+k\nis a keyphrase of the document.\n4.2 Visual Features\nWe extract four groups of visual features for each\nword in the document.\n‚Ä¢Size features include the height and width of\nthe text block a word appears in.\nTable 3: Visual Features. All features are extracted\nat per word level and the parent block level (the\nparent node of the word in the HTML DOM tree).\nName Dimension\nFont Size 1√ó2\nText Block Size 2√ó2\nLocation in Rendered Page 2√ó2\nIs Bold Font 1√ó2\nAppear In Inline 1√ó2\nAppear In Block 1√ó2\nAppear In DOM Tree Leaf 1√ó2\n‚Ä¢Location features include the 2-d location of\nthe word in the rendered web page.\n‚Ä¢Font feature includes the font size and\nwhether the word is in Bold.\n‚Ä¢DOM features include whether the word ap-\npears in ‚Äúinline‚Äù or ‚Äúblock‚Äù HTML tags, also\nwhether it is in a leaf node of the DOM tree.\nThe full feature set is listed in Table 3. We double\nthe features by including the same features from\nthe word‚Äôs parent block in the DOM tree. The vi-\nsual features are included in the OpenKP releases.\n4.3 Weak Supervisions from Search\nAn application of keyphrases is information re-\ntrieval. The extracted keyphrases are expected\nto capture the main topic of the document,\nthus can provide high quality document index-\ning terms (Gutwin et al., 1999) or new semantic\nranking features (Xiong et al., 2018). Reversely,\nuser clicks bring the user‚Äôs perception of the doc-\nument during the search and provide a large num-\nber of feedback signals for document understand-\ning (Croft et al., 2010).\nBLING-KPE leverages the user feedback sig-\nnals as weak supervision, in the task of Query\nPrediction. Given the document d, BLING-\nKPE learns to predict its click queries Q =\n{q1, ..., qm}.\nThis pre-training step uses the same cross en-\ntropy loss:\nlpre = Cross-Entropy(y‚Ä≤\ni, f(qi, d)), (11)\nwhere y‚Ä≤\ni indicates whether the query qi is a click\nquery and also appears as an n-gram in the docu-\nment d. The Query Prediction labels exist at scale\nin commercial search logs and provide a large\nnumber of pre-training signals.\n5180\nTable 4 : Statistics of Query Prediction Dataset.\nThe data is from a sample of Bing search log in\non week.\nStatistics Mean STD\nDoc Length 1211.8 1872.6\n# of Query per Doc 1.32 0.76\nQuery Length 2.4 1.07\nDoc V ocabulary Size 15M n.a\nQuery V ocabulary Size 383K n.a\n# of Documents 1.6M n.a.\n# of Unique Queries 1.5M n.a.\n5 Experimental Methodology\nDatasets used in our experiments include\nOpenKP, as described in ¬ß3, Query Prediction,\nand DUC-2001 (Wan and Xiao, 2008b).\nThe Query Prediction data is sampled from the\nBing search log with navigational and offensive\nqueries Ô¨Åltered out. We keep only the click queries\nthat are included as an n-gram in the document\nto be consistent with OpenKP‚Äôs extractive setting.\nThe statistics of the sample is listed in Table 4.\nDUC-2001 is the KPE extraction dataset on\nDUC news articles (Wan and Xiao, 2008b). It\nincludes 309 news articles and on average 8\nkeyphrase per article.\nWe use random 80%-20% train-test splits on\nOpenKP and Query Prediction. On OpenKP,\nBLING-KPE is Ô¨Årst pre-trained on Query Predic-\ntion and then further trained on its manual labels.\nThere is no overlap between the documents in\nQuery Prediction and OpenKP.\nDUC-2001 uses the zero-shot evaluation setting\nfrom prior research (Meng et al., 2017; Chen et al.,\n2018); no labels in DUC-2001 are used to train nor\nvalidate the neural models. It tests neural models‚Äô\ngeneralization ability from the training domain to\na different testing domain.\nEvaluation Metrics. OpenKP and Query Pre-\ndiction use Precision and Recall@{1, 3, 5}.\nDUC-2001 uses F1@10, the same as prior re-\nsearch (Meng et al., 2017; Chen et al., 2018).\nStatistically, signiÔ¨Åcant improvements are eval-\nuated by permutation test with p<0.05 on OpenKP\nand Query Prediction. The baselines on DUC-\n2001 reuse scores from previous results; the sta-\ntistical signiÔ¨Åcant test is not applicable as per doc-\nument results are not shared.\nBaselines. OpenKP and Query Prediction ex-\nperiments compare BLING-KPE with: traditional\nKPE methods, production systems, and a neural\nTable 5: Parameters to learn in BLING-KPE.\nComponent Dimensions\nELMo pre-trained and frozen\nPosition Embedding 256\nVisual Feature 18\nN-gram CNN 512 Ô¨Ålter, 1-5 window size (ngram)\nTransformer 8 head, 512 hidden dimension\nFeedforward 512-relu-512-relu-1\nbaseline.\nTraditional KPE baselines include the follows.\n‚Ä¢TFIDF is the unsupervised frequency based\nKPE system. The IDF scores are calculated\non the corresponding corpus.\n‚Ä¢TextRank is the popular graph-based unsu-\npervised KPE model (Mihalcea and Tarau,\n2004). Our in-house implementation is used.\n‚Ä¢LeToR is the feature-based KPE model. It\nuse LambdaMart (Burges, 2010) and stan-\ndard KPE features, i.e. those in KEA (Witten\net al., 2005).\nThe production baselines include two versions.\n‚Ä¢PROD is our current feature-based produc-\ntion KPE system. It uses many carefully en-\ngineered features and LambdaMart.\n‚Ä¢PROD (Body) is the same system but only\nuses the body text, i.e. the title is not used.\nAll these unsupervised and feature-based meth-\nods use the same keyphrase candidate selection\nsystem with PROD.\nThe neural baseline is CopyRNN (Meng et al.,\n2017). We use their open-source implementation\nand focus on the OpenKP dataset which is publicly\navailable.\nImplementation Details. Table 5 lists BLING-\nKPE parameters. The training uses Adam opti-\nmizer, learning rate 0.3 with logarithmic decreas-\ning to 0.001, batch size 16, and 0.2 dropout proba-\nbility in n-gram CNN, Transformer and feedfor-\nward layers. Learning takes about 2.5 hours (2\nepochs) to converge on OpenKPE and about 13\nhours (3 epochs) on Query Prediction, based on\nvalidation loss. In BLING-KPE, the maximum\ndocument length is 256 and documents are zero-\npadded or truncated to this length. Baselines use\nthe original documents, except CopyRNN which\nworks better with 256. The maximum n-gram\nlength is set to Ô¨Åve (K=5).\n5181\nTable 6: Keyphrase Extraction Accuracy. Bold marks statistically signiÔ¨Åcant improvements over all\nbaselines.\nOpenKP Query Prediction\nMethod P@1 R@1 P@3 R@3 P@5 R@5 P@1 R@1 P@3 R@3 P@5 R@5\nTFIDF 0.283 0.150 0.184 0.284 0.137 0.347 0.403 0.332 0.204 0.491 0.133 0.526\nTextRank 0.077 0.041 0.062 0.098 0.055 0.142 0.132 0.111 0.089 0.218 0.073 0.295\nLeToR 0.301 0.158 0.173 0.268 0.127 0.324 0.328 0.271 0.169 0.406 0.119 0.471\nPROD 0.353 0.188 0.195 0.299 0.131 0.331 0.376 0.308 0.197 0.468 0.129 0.505\nPROD (Body) 0.214 0.094 0.130 0.196 0.094 0.234 0.353 0.287 0.191 0.454 0.125 0.492\nCopyRNN 0.288 0.174 0.185 0.331 0.141 0.413 ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì\nBLING-KPE 0.404 0.220 0.248 0.390 0.188 0.481 0.540 0.449 0.275 0.654 0.188 0.729\nTable 7: Performance of BLING-KPE ablations. Italic marks statistically signiÔ¨Åcant worse performances\nthan Full Model.\nOpenKP Query Prediction\nMethod P@1 R@1 P@3 R@3 P@5 R@5 P@1 R@1 P@3 R@3 P@5 R@5\nNo ELMo 0.270 0.145 0.172 0.271 0.132 0.347 0.323 0.274 0.189 0.450 0.136 0.527\nNo Transformer 0.389 0.211 0.247 0.385 0.189 0.481 0.489 0.407 0.258 0.618 0.178 0.698\nNo Position 0.394 0.213 0.247 0.386 0.187 0.475 0.543 0.452 0.281 0.666 0.191 0.742\nNo Visual 0.370 0.201 0.230 0.362 0.176 0.450 0.492 0.409 0.258 0.615 0.178 0.695\nNo Pretraining 0.369 0.198 0.236 0.367 0.181 0.460 ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì\nFull Model 0.404 0.220 0.248 0.390 0.188 0.481 0.540 0.449 0.275 0.654 0.188 0.729\n6 Evaluation Results\nThree experiments are conducted to evaluate the\naccuracy of BLING-KPE, the source of its effec-\ntiveness, and its generalization ability.\n6.1 Overall Accuracy\nThe overall extraction accuracy on OpenKP and\nQuery Prediction is shown in Table 6.\nTFIDF works well on both tasks. Frequency-\nbased methods are often strong baselines in doc-\nument representation tasks. LeToR performs bet-\nter than its frequency feature TFIDF in OpenKP\nbut worse on Query Prediction. Supervised meth-\nods are not necessarily stronger than unsupervised\nones in KPE (Hasan and Ng, 2014). TextRank\ndoes not work well in our dataset; its word graph\nis likely misguided by the noisy contents.\nPROD, our feature-based production system,\noutperforms all other baselines by large margins\non OpenKP. It is expected as it is highly optimized\nwith a lot of engineering efforts. Nonetheless,\nadapting a complex feature-based system to a new\ntask/domain requires extra engineering work; di-\nrectly applying it to the Query Prediction task does\nnot work well. The feature-based Production sys-\ntem also needs the title information; PROD (Body)\nperforms much worse than PROD.\nCopyRNN performs relatively well on OpenKP,\nespecially on later keyphases. The main challenge\nfor CopyRNN is the low-quality and highly vari-\nant contents on the web. Real-world web pages are\nnot cohesive nor well-written articles but include\nvarious structures such as lists, media captions,\nand text fragments. Modeling them as a word\nsequence is not ideal. The other differences are\nnot as signiÔ¨Åcant: The vocabulary size and train-\ning data size on Query Prediction are similar to\nCopyRNN‚Äôs KP 40K dataset; CopyRNN performs\nbetter on keyphrase extraction than generation in\nKP20k (Meng et al., 2017).\nBLING-KPE outperforms all other methods by\nlarge margins. The improvements are robust and\nsigniÔ¨Åcant on both tasks, both metrics, and on all\ndepths. It achieves 0.404 P@1 on OpenKP and re-\ncovers 72% of clicked queries at depth 5 on Query\nPrediction. The sources of this effectiveness is\nstudied in the next experiment.\n6.2 Ablation Study\nTable 7 shows ablation results on BLING-KPE‚Äôs\nvariations. Each variation removes a component\nand keeps all others unchanged.\nELMo Embedding. We Ô¨Årst verify the effec-\ntiveness of using ELMo embedding by replacing\nELMo with the WordPiece token embedding (Wu\net al., 2016). The accuracy of this variation is\nmuch lower than the accuracy of the full model\nand others. The result is shown in the Ô¨Årst row of\nTable 7. The context-aware word embedding is a\nnecessary component of BLING-KPE.\nNetwork Architecture. The second part of\n5182\nTable 8: Performance on DUC-2001. Neural mod-\nels are evaluated directly on DUC-2001 without\nÔ¨Åne-tuning on DUC labels. Results better than\nTFIDF are marked Bold.\nMethod F1@10 Method F1@10\nTFIDF 0.270 TopicRank 0.154\nTextRank 0.097 KeyCluster 0.140\nSingleRank 0.256 ExpandRank 0.269\nTrained by ScientiÔ¨Åc Papers\nCopyRNN 0.164 CorrRNN 0.173\nBLING-KPE (No Visual, No Pretraining) 0.267\nTrained by Open Domain Documents\nBLING-KPE (No Visual, No Pretraining) 0.282\nTable 7 studies the contribution of Transformer\nand position embedding. Transformer contributes\nsigniÔ¨Åcantly to Query Prediction; with a lot of\ntraining data, the self-attention layers capture the\nglobal contexts between n-grams. But on OpenKP,\nits effectiveness is mostly observed on the Ô¨Årst po-\nsition. The position embedding barely helps, since\nreal-world web pages are often not one text se-\nquence.\nBeyond Language Understanding. As shown\nin the second part of Table 7, both visual features\nand search pretraining contribute signiÔ¨Åcantly to\nBLING-KPE‚Äôs effectiveness. Without either of\nthem, the accuracy drops signiÔ¨Åcantly. Visual fea-\ntures even help on Query Prediction, though users\nissued the click queries and clicked on the docu-\nments before seeing its full page.\nThe crucial role of ELMo embeddings conÔ¨Årm\nthe beneÔ¨Åts of bringing background knowledge\nand general language understanding, in the format\nof pre-trained contextual embedding, in keyphrase\nextraction. The importance of visual features and\nsearch weak supervisions conÔ¨Årms the beneÔ¨Åts of\ngoing beyond language understanding in modeling\nreal-world web documents.\n6.3 Generalization Ability\nThis experiment studies the generalization ability\nof BLING-KPE using the zero-shot evaluation on\nDUC-2001 (Meng et al., 2017; Chen et al., 2018).\nFor fair comparisons, we use KP20K or OpenKP,\nthe two public datasets, to train the No Visual &\nNo Pretraining version of BLING-KPE, and eval-\nuate on DUC-2001 directly. No labels in DUC are\nused to Ô¨Åne-tune the neural models.\nTo adjust to DUC‚Äôs larger number of\nkeyphrases, we apply the trained BLING-\nKPE on the 256-length chunks of DUC articles\nFigure 3: An example of visual feature contribu-\ntions in BLING-KPE. The big green blocks are\nthe keyphrases extracted with visual features. The\nsmall read blocks are those extracted without vi-\nsual information.\nand merge the extracted keyphrases using simple\nheuristics:\n‚Ä¢Weighted Sum: Scores of the same keyphrase\nfrom different chunks are summed with\nweights 0.9p. P is the index of the chunk.\n‚Ä¢Deduplication: A keyphrase is discarded if it\nis a sub-string of a top 1/4 ranked keyphrase.\nThe results are shown in Table 8.BLING-KPE,\nwhen trained with OpenKP, is the only neu-\nral method that outperforms TFIDF in this zero-\nshot evaluation. It outperforms previous neu-\nral methods by more than 60%, and itself when\ntrained on KP20k, conÔ¨Årming the strong gener-\nalization ability of BLING-KPE and the training\nwith OpenKP.\n6.4 Discussion\nOur manual case studies found many interesting\nexamples that illustrate the advantage of modeling\ndocuments with visual information.\nFor example, in Figure 3, the page is anno-\ntated with ‚ÄúBostitch 651S5‚Äù, the product name,\nand ‚ÄúStapler‚Äù, the product type. Their salience\nis highlighted by larger and bold fonts, which are\npicked up by BLING-KPE. However, without the\nvisual information, the product ontology names\nare extracted as keyphrases: they are meaningful\nconcepts, correlated with the page content, and po-\nsitioned at the beginning of the document‚Äîonly\nthat they appear less important in the web page by\ndesign.\n5183\n7 Conclusion\nThis paper curates OpenKP, the Ô¨Årst public large\nscale open domain keyphrase extraction bench-\nmark to facilitate future research keyphrase extrac-\ntion research in real-world scenarios. It also de-\nvelops BLING-KPE, which leverages visual rep-\nresentation and search-based weak supervision to\nmodel real-world documents with variant con-\ntents, appearances, and diverse domains.\nOur experiments demonstrate the robust im-\nprovements of BLING-KPE compared to previous\napproaches. Our studies showcase how BLING-\nKPE‚Äôs language understanding, visual features\nand search weak supervision jointly deliver this ef-\nfective performance, as well as its generalization\nability to an unseen domain in zero-shot setting.\nIn the future, we plan to extend OpenKP with\nmore annotated documents and connect it with\ndownstream applications.\nReferences\nIsabelle Augenstein, Mrinal Das, Sebastian Riedel,\nLakshmi Vikraman, and Andrew McCallum. 2017.\nSemEval 2017 task 10: ScienceIE - extracting\nkeyphrases and relations from scientiÔ¨Åc publica-\ntions. In SemEval@ACL.\nChris J.C. Burges. 2010. From RankNet to Lamb-\ndaRank to LambdaMART: An overview. MSR-TR-\n2010-82.\nJun Chen, Xiaoming Zhang, Yu Wu, Zhao Yan, and\nZhoujun Li. 2018. Keyphrase generation with corre-\nlation constraints. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2018, pages 4057‚Äì4066.\nMo Chen, Jian-Tao Sun, Hua-Jun Zeng, and Kwok-Yan\nLam. 2005. A practical system of keyphrase extrac-\ntion for web pages. In Proceedings of the 14th ACM\ninternational conference on Information and knowl-\nedge management, pages 277‚Äì278. ACM.\nWang Chen, Yifan Gao, Jiani Zhang, Irwin King, and\nMichael R. Lyu. 2019. Title-guided encoding for\nkeyphrase generation. In The Thirty-Third AAAI\nConference on ArtiÔ¨Åcial Intelligence 2019 , pages\n6268‚Äì6275.\nW Bruce Croft, Donald Metzler, and Trevor Strohman.\n2010. Search Engines: Information Retrieval in\nPractice. Addison-Wesley Reading.\nMaria P. Grineva, Maxim N. Grinev, and Dmitry Li-\nzorkin. 2009. Extracting key terms from noisy\nand multitheme documents. In Proceedings of the\n18th International Conference on World Wide Web,\nWWW 2009, pages 661‚Äì670.\nCarl Gutwin, Gordon W. Paynter, Ian H. Wit-\nten, Craig G. Nevill-Manning, and Eibe Frank.\n1999. Improving browsing in digital libraries with\nkeyphrase indexes. Decision Support Systems ,\n27:81‚Äì104.\nKazi Saidul Hasan and Vincent Ng. 2014. Automatic\nkeyphrase extraction: A survey of the state of the\nart. In Proceedings of the 52nd Annual Meeting of\nthe Association for Computational Linguistics, ACL\n2014, pages 1262‚Äì1273.\nFeifan Liu, Deana Pennell, Fei Liu, and Yang Liu.\n2009a. Unsupervised approaches for automatic key-\nword extraction using meeting transcripts. In Hu-\nman Language Technologies: Conference of the\nNorth American Chapter of the Association of Com-\nputational Linguistics, Proceedings, HLT-NAACL\n2009, pages 620‚Äì628.\nZhengzhong Liu, Chenyan Xiong, Teruko Mitamura,\nand Eduard Hovy. 2018. Automatic event salience\nidentiÔ¨Åcation. In Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language\nProcessing, EMNLP 2018s, pages 1226‚Äì1236.\nZhiyuan Liu, Wenyi Huang, Yabin Zheng, and\nMaosong Sun. 2010. Automatic keyphrase extrac-\ntion via topic decomposition. In Proceedings of the\n2010 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2010 , pages 366‚Äì\n376.\nZhiyuan Liu, Peng Li, Yabin Zheng, and Maosong\nSun. 2009b. Clustering to Ô¨Ånd exemplar terms for\nkeyphrase extraction. In Proceedings of the 2009\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2009, pages 257‚Äì266.\nOlena Medelyan, Eibe Frank, and Ian H. Witten.\n2009. Human-competitive tagging using automatic\nkeyphrase extraction. In Proceedings of the 2009\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2009, pages 1318‚Äì1327.\nRui Meng, Sanqiang Zhao, Shuguang Han, Daqing\nHe, Peter Brusilovsky, and Yu Chi. 2017. Deep\nkeyphrase generation. In Proceedings of the 55th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, ACL 2017, pages 582‚Äì592.\nRada Mihalcea and Paul Tarau. 2004. TextRank:\nBringing order into text. In Proceedings of the 2004\nConference on Empirical Methods in Natural Lan-\nguage Processing , EMNLP 2004, pages 404‚Äì411.\nChau Q. Nguyen and Tuoi T. Phan. 2009. An ontology-\nbased approach for key phrase extraction. In ACL\n2009, Proceedings of the 47th Annual Meeting of the\nAssociation for Computational Linguistics and the\n4th International Joint Conference on Natural Lan-\nguage Processing of the AFNLP , ACL/IJCNLP 2009,\npages 181‚Äì184.\n5184\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2018, pages 2227‚Äì2237.\nPeter D. Turney. 2001. Mining the web for synonyms:\nPMI-IR versus LSA on TOEFL. In Machine Learn-\ning: EMCL 2001, 12th European Conference on\nMachine Learning, 2001, Proceedings , pages 491‚Äì\n502.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems, NIPS 2017 , pages\n5998‚Äì6008.\nXiaojun Wan and Jianguo Xiao. 2008a. Col-\nlabRank: Towards a collaborative approach to\nsingle-document keyphrase extraction. In 22nd In-\nternational Conference on Computational Linguis-\ntics, Proceedings of the Conference, COLING 2008,\npages 969‚Äì976.\nXiaojun Wan and Jianguo Xiao. 2008b. Single\ndocument keyphrase extraction using neighborhood\nknowledge. In Proceedings of the Twenty-Third\nAAAI Conference on ArtiÔ¨Åcial Intelligence, AAAI\n2008, pages 855‚Äì860.\nIan H Witten, Gordon W Paynter, Eibe Frank, Carl\nGutwin, and Craig G Nevill-Manning. 2005. KEA:\nPractical automated keyphrase extraction. In Design\nand Usability of Digital Libraries: Case Studies in\nthe Asia PaciÔ¨Åc, pages 129‚Äì152. IGI Global.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin\nJohnson, Xiaobing Liu, Lukasz Kaiser, Stephan\nGouws, Yoshikiyo Kato, Taku Kudo, Hideto\nKazawa, Keith Stevens, George Kurian, Nishant\nPatil, Wei Wang, Cliff Young, Jason Smith, Jason\nRiesa, Alex Rudnick, Oriol Vinyals, Greg Corrado,\nMacduff Hughes, and Jeffrey Dean. 2016. Google‚Äôs\nneural machine translation system: Bridging the gap\nbetween human and machine translation. CoRR,\nabs/1609.08144.\nChenyan Xiong, Zhengzhong Liu, Jamie Callan, and\nTie-Yan Liu. 2018. Towards better text understand-\ning and retrieval through kernel entity salience mod-\neling. In The 41st International ACM SIGIR Con-\nference on Research & Development in Information\nRetrieval, SIGIR 2018, pages 575‚Äì584.\nHai Ye and Lu Wang. 2018. Semi-supervised learning\nfor neural keyphrase generation. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 4142‚Äì4153.\nWen-tau Yih, Joshua Goodman, and Vitor R. Carvalho.\n2006. Finding advertising keywords on web pages.\nIn Proceedings of the 15th international conference\non World Wide Web, WWW 2006, pages 213‚Äì222.\nXingdi Yuan, Tong Wang, Rui Meng, Khushboo\nThaker, Daqing He, and Adam Trischler. 2018.\nGenerating diverse numbers of diverse keyphrases.\nCoRR, abs/1810.05241."
}