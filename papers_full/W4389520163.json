{
  "title": "Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization",
  "url": "https://openalex.org/W4389520163",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2607956288",
      "name": "Chenhui Shen",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2103479347",
      "name": "Liying Cheng",
      "affiliations": [
        "Zhejiang Lab"
      ]
    },
    {
      "id": "https://openalex.org/A2952241938",
      "name": "Xuan Phi Nguyen",
      "affiliations": [
        "Zhejiang Lab"
      ]
    },
    {
      "id": "https://openalex.org/A2110349951",
      "name": "Yang You",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2160800796",
      "name": "Lidong Bing",
      "affiliations": [
        "Zhejiang Lab"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4285171765",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4297900071",
    "https://openalex.org/W2890419630",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2962974924",
    "https://openalex.org/W4389519239",
    "https://openalex.org/W4318903783",
    "https://openalex.org/W3176266728",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4387428151",
    "https://openalex.org/W4389519254",
    "https://openalex.org/W4239181501",
    "https://openalex.org/W4385571232",
    "https://openalex.org/W2896807716",
    "https://openalex.org/W3209219303",
    "https://openalex.org/W2905279751",
    "https://openalex.org/W2218641061",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3101845158",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W4292402161",
    "https://openalex.org/W2963204221",
    "https://openalex.org/W4313854414",
    "https://openalex.org/W4389519863",
    "https://openalex.org/W4361230777",
    "https://openalex.org/W4385565156",
    "https://openalex.org/W4287854442",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4206637810",
    "https://openalex.org/W4368755400",
    "https://openalex.org/W3176456866",
    "https://openalex.org/W4309591663",
    "https://openalex.org/W4389520264",
    "https://openalex.org/W3035620455",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3105484636",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4389520463",
    "https://openalex.org/W4385573994",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W2973379954",
    "https://openalex.org/W4362679631",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W3111372071",
    "https://openalex.org/W2950670227",
    "https://openalex.org/W4385573460",
    "https://openalex.org/W1520857482",
    "https://openalex.org/W3170432046",
    "https://openalex.org/W2963227052",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W105852416",
    "https://openalex.org/W2963241389",
    "https://openalex.org/W2963045354"
  ],
  "abstract": "With the recent undeniable advancement in reasoning abilities in large language models (LLMs) like ChatGPT and GPT-4, there is a growing trend for using LLMs on various tasks. One area where LLMs can be employed is as an alternative evaluation metric for complex generative tasks, which generally demands expensive human judges to complement the traditional automatic metrics for various evaluation dimensions such as fluency and consistency. In this work, we conduct extensive analysis to investigate the stability and reliability of LLMs as automatic evaluators for abstractive summarization. We found that while ChatGPT and GPT-4 outperform the commonly used automatic metrics, they are not ready as human replacements due to significant limitations. That is, LLM evaluators rate each candidate system inconsistently and are dimension-dependent. They also struggle to compare candidates with close performance and become more unreliable with higher-quality summaries by obtaining a lower correlation with humans. In other words, with better abstractive summarization systems being introduced at a fast pace, LLMs may result in misleading and unreliable evaluations.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 4215–4233\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nLarge Language Models are Not Yet Human-Level Evaluators for\nAbstractive Summarization\nChenhui Shen∗1,2 Liying Cheng 1,3 Xuan-Phi Nguyen 1,3 Yang You2 Lidong Bing††1,3\n1DAMO Academy, Alibaba Group, Singapore 2National University of Singapore\n3Hupan Lab, 310023, Hangzhou, China\n{chenhui.shen, liying.cheng, x.nguyen, l.bing}@alibaba-inc.com\nyouy@comp.nus.edu.sg\nAbstract\nWith the recent undeniable advancement in\nreasoning abilities in large language models\n(LLMs) like ChatGPT and GPT-4, there is a\ngrowing trend for using LLMs on various tasks.\nOne area where LLMs can be employed is as\nan alternative evaluation metric for complex\ngenerative tasks, which generally demands ex-\npensive human judges to complement the tradi-\ntional automatic metrics for various evaluation\ndimensions such as fluency and consistency. In\nthis work, we conduct extensive analysis to in-\nvestigate the stability and reliability of LLMs as\nautomatic evaluators for abstractive summariza-\ntion. We found that while ChatGPT and GPT-4\noutperform the commonly used automatic met-\nrics, they are not ready as human replacements\ndue to significant limitations. That is, LLM\nevaluators rate each candidate system incon-\nsistently and are dimension-dependent. They\nalso struggle to compare candidates with close\nperformance and become more unreliable with\nhigher-quality summaries by obtaining a lower\ncorrelation with humans. In other words, with\nbetter abstractive summarization systems being\nintroduced at a fast pace, LLMs may result in\nmisleading and unreliable evaluations.1\n1 Introduction\nThe desire for inexpensive and fast automatic met-\nrics has never stopped growing. In certain tasks\nlike extractive summarization, where full source\nsentences are selected to appear in the summaries,\nsimple n-gram overlap metrics against the “gold”\nsummaries like ROUGE (Lin, 2004) or BLEU (Pa-\npineni et al., 2002) may work well because the\ncorrect answer space is narrow. However, for\nmore open tasks like abstractive summarization,\nthere are countless equally good summaries and the\n∗Chenhui is under the Joint PhD Program between Al-\nibaba and National University of Singapore.\n††Corresponding author.\n1Our code and data are fully released at https://github\n.com/DAMO-NLP-SG/LLM_summeval.\n“gold” summaries become less important. Although\nmany neural-based metrics such as BERTScore\nand BARTScore (Zhang et al., 2020b; Yuan et al.,\n2021), are advocated as more human-aligned, the\nevaluation criteria are also becoming increasingly\ncomplex. As a result, abstractive summarization\nmay not be sufficiently evaluated with automatic\nmetrics (Owczarzak et al., 2012; Nenkova, 2006),\nand often require extensive human evaluations as\ncomplements(Yang et al., 2023; Welbl et al., 2021).\nHowever, human evaluations often come with hefty\ncosts and slow iteration cycles, while also being\ndifficult to reproduce and standardize due to small\nsample sizes and potential human biases (Shen\net al., 2022b; Liu et al., 2022).\nRecent large language models (LLMs) like Chat-\nGPT and GPT-4 (OpenAI, 2023) have demon-\nstrated outstanding capabilities in language com-\nprehension and reasoning. This leads to a growing\ntrend of employing LLMs as evaluators for com-\nplex language generation tasks by prompting them\nwith carefully and elaborately crafted instructions\n(Chiang and Lee, 2023; Gao et al., 2023; Wang\net al., 2023a; Wu et al., 2023; Luo et al., 2023; Liu\net al., 2023). Despite the preliminary success sug-\ngested by such works, it is still inconclusive as to\nwhat degree of confidence we can trust the evalu-\nation results produced by LLMs across different\ndimensions, despite their supposedly high average\ncorrelation with humans. It is also unclear if certain\nLLM-based metrics are more reliable than others,\nor if their reliability and fairness vary for different\ncandidate systems.\nIn this work, we conduct extensive analysis to\nassess whether LLM evaluators can reliably re-\nplace human judges. Specifically, we incorporate\ntwo common human evaluation approaches with\nLLM evaluators, namely Likert-scale scoring (He\net al., 2022; Shen et al., 2022b; Zhang et al., 2020a)\nand head-to-head (H2H) comparisons (Shen et al.,\n2022a; Li et al., 2020; Liu and Lapata, 2019). For\n4215\nLikert-scale scoring, we explore direct reason-then-\nscore (RTS) generation and a multiple-choice ques-\ntion (MCQ) method. The former instructs the LLM\nto provide reasoning before giving a score, while\nthe latter simply prompts it to choose a specific\nscore with a pre-determined description as the rea-\nson. For the Head-to-Head (H2H) comparison, we\nprompt LLM for a preference over the summaries\nfrom two compared candidate systems.\nOur experiments show that LLM evaluators,\nwith RTS and MCQ, outperform existing auto-\nmatic metrics (Lin, 2004; Yuan et al., 2021). How-\never, they are not ready to be reliable alternatives\nfor human evaluation yet. Specifically, ( i) LLM\nevaluators struggle to distinguish candidates with\nclose performances (§ 4.2.1). ( ii) LLM evalua-\ntors are candidate-dependent, meaning they do\nnot exhibit highly consistent degrees of human\nalignment for different candidates (§ 4.2.3). Thus,\nthey may unfairly favor or disfavor an evaluated\ncandidate. ( iii) LLM evaluators are dimension-\ndependent, meaning they have varying degrees of\nevaluation capabilities for different dimensions like\ncoherence and fluency (§ 4.2.3). (iv) Lastly, as the\nquality of summaries improves with better candi-\ndates, LLM evaluators become unreliably less cor-\nrelated with human judgments, according to our\nnewly proposed meta-correlation metric (§ 4.2.4).\nWhile we still call for a better automatic metric,\nin the meantime, we suggest a temporary solution\nin § 5 for abstractive summarization practitioners to\nuse LLMs more reliably. Specifically, we advocate\ncalculating the correlation between RTS and MCQ\nas a preliminary indicator of the reliability of the\nLLM for certain dimensions. If RTS and MCQ do\nnot generally agree with each other, then further\nhuman evaluations are required.\n2 Related Work\nSummarization The summarization task in-\nvolves generating a summary that contains concise\nand important (i.e., salient) contents of the origi-\nnal input article (Nenkova and McKeown, 2012).\nThis task has been handled with 2 different ap-\nproaches: extractive and abstractive. Unlike extrac-\ntive summarization systems that directly extract\nsalient phrases or sentences from the input article\n(Ernst et al., 2022; Chen et al., 2021; Zhou et al.,\n2018; Dong et al., 2018), abstractive summariza-\ntion systems are expected to generate summaries\nusing their own words and apply sentence fusion\nor paraphrasing techniques (Shen et al., 2023; Liu\net al., 2022; Xiao et al., 2022; Lewis et al., 2020;\nZhang et al., 2020a; Ziegler et al., 2019; Bing\net al., 2015; Xu and Durrett, 2021). As such, ab-\nstractive summarization poses significantly more\nchallenges for automatic and human evaluation\npipelines (Saha et al., 2022; Pagnoni et al., 2021),\nbecause it is increasingly insufficient to use the\nprovided “gold” summary as ground truth.\nHuman Evaluation Human evaluation can be\nconducted with different approaches. Some work\n(He et al., 2022; Shen et al., 2022b; Zhang et al.,\n2020a; Cheng et al., 2020; Gao et al., 2019; Liu\net al., 2018; Li et al., 2017; Kry´sci´nski et al., 2018)\nemploy a Likert scale to evaluate the summaries on\ndiscrete ranges, such as from 1 to 5. Meanwhile,\nmany others suggest comparison approaches by\nasking human annotators to select the best sum-\nmary out of 2 or more generated summaries from\ndifferent systems (Shen et al., 2022a; Li et al., 2020;\nLiu and Lapata, 2019; Fan et al., 2018; Fabbri et al.,\n2019). Following this, we test LLM-based evalu-\nators using both approaches with human-friendly\ninstruction prompts.\nAutomatic Evaluation ROUGE (Lin, 2004) has\nbeen a common lexical overlap metric to evalu-\nate summarization systems. Apparently, R OUGE\nis not sufficient for abstractive summarization, be-\ncause the “gold” labels it relies on cannot com-\nprehensively account for the complexity and vari-\nability of this task. In addition, the common us-\nage of sentence fusion techniques and novel words\nfor abstractive summarization may make ROUGE\neven less reliable. Zhang et al. (2020b) propose\nthe neural-based BERTScore, which leverages the\nBERT word embeddings to compute the semantic\nsimilarity among tokens. Yuan et al. (2021) later\nintroduce BARTScore, which uses BART (Lewis\net al., 2020) to compute the probability of a sum-\nmary given its input article. Nonetheless, these\nmetrics may not reflect all of the complicated eval-\nuation dimensions required for abstractive summa-\nrization mentioned earlier, nor do they have suffi-\nciently high correlations with humans.\nLLM-based Evaluation There are many concur-\nrent works that demonstrate the potential of LLMs\nto conduct complex human tasks (Chiang and Lee,\n2023; Gao et al., 2023; Wang et al., 2023a; Wu\net al., 2023; Luo et al., 2023; Liu et al., 2023; Cheng\net al., 2023). The key advantage of instruction-\n4216\ntuned LLMs, like ChatGPT or GPT-4 (Ouyang\net al., 2022; OpenAI, 2023), is that we can ex-\nplicitly describe in natural language what our eval-\nuation criteria and dimensions are and how to score\nthe summaries, similar to how we would explain\nsuch tasks to a human expert. Chiang and Lee\n(2023) use LLMs for open-ended story evaluations,\nwhile Luo et al. (2023) apply ChatGPT specifi-\ncally for evaluating the consistency of summaries.\nWu et al. (2023) formulate LLMs as diverse role-\nplayers to evaluate summaries from the perspec-\ntives of different personas. Wang et al. (2023a) and\nLiu et al. (2023) also explore the LLM’s evalua-\ntion potential in various dimensions for the natural\nlanguage generation task. Our work differs from\nthe above works in that besides investigating the\nLLMs’ capability using different approaches across\nvarious dimensions for abstractive summarization,\nwe further focus on the reliability of LLM across\nevaluated systems and dimensions.\n3 LLM as a Zero-Shot Evaluator\nWe investigate an LLM’s evaluation capabilities in\nthe dimensions of coherence, consistency, fluency,\nand relevance respectively, as defined by Fabbri\net al. (2021) (see Appendix A). Following com-\nmon human evaluation approaches, we propose\ntwo methods for Likert-scale scoring, namely the\nreason-then-score method and the multiple-choice\nquestion method, as well as one method for head-\nto-head comparisons. We describe each method in\n§ 3.1 using the relevance dimension as an example\n(see more prompts and details in Appendix B). We\nfurther experiment with alternative phrasings for\ndifferent methods in Appendix C.\nBesides exploring different evaluation methods,\nthe stability of LLM-based evaluations across dif-\nferent summarization systems is equally important.\nIdeally, a stable LLM evaluator should perform\nequally well regardless of the evaluated systems,\nwith a close (if not identical) degree of alignment\nwith human judgments. In § 3.2, we propose a\nmeta-correlation metric and explain how it can\ngauge the extent to which LLM evaluators’ per-\nformances depend on the evaluated systems, which\nindicates how stable and reliable they may be with\nevaluating any future candidate systems.\n3.1 Summary Evaluation Methods\nReason-then-Score (RTS) Given the success of\nchain-of-thought prompting (Kojima et al., 2022;\nScore the following Summary given the corresponding Arti-\ncle with respect to relevance from one to five, where one in-\ndicates “irrelevance”, and five indicates “perfect relevance”.\nNote that relevance measures the Summary’s selection of\nimportant content from the Article, whether the Summary\ngrasps the main message of the Article without being over-\nwhelmed by unnecessary or less significant details.\nArticle: {article}\nSummary: {summary}\nProvide your reason in one sentence, then give a final score:\nTable 1: Example prompt for the RTS method on the\nrelevance dimension. Texts in {blue} represent the arti-\ncle and the corresponding summary to be evaluated.\nChoose an option from A to E in order to score the follow-\ning Summary given the corresponding Article with respect\nto relevance from one to five, where one indicates “irrel-\nevance”, and five indicates “perfect relevance”. Note that\nrelevance measures the Summary’s selection of important\ncontent from the Article, whether the Summary grasps the\nmain message of the Article without being overwhelmed by\nunnecessary or less significant details.\nArticle: {article}\nSummary: {summary}\nA: The Summary is totally irrelevant to the Article. Score:\nOne.\nB: The majority of the Summary is irrelevant to the Article.\nScore: Two.\nC: Some information in the Summary is relevant to the\nArticle whereas some are not. Score: Three.\nD: The majority of the Summary is relevant to the Article.\nScore: Four.\nE: All information included in the Summary is relevant to\nthe Article. Score: Five.\nYour Answer (enter 1 letter from A to E):\nTable 2: Example prompt for the MCQ method on the\nrelevance dimension. Texts in {blue} represent the arti-\ncle and the corresponding summary to be evaluated.\nWei et al., 2022), an intuitive method is to ask\nthe LLM to evaluate a specific dimension by first\ngenerating the reasoning and then a corresponding\nscore. Since the SummEval dataset (Fabbri et al.,\n2021) contains human scores on a Likert scale of 1\nto 5, we also ask the LLM to score the summaries\nin the same range, as shown in Table 1.\nMCQ Scoring (MCQ) Nevertheless, previous\nworks find that the reasoning generated by the\nLLM does not always make sense (Lyu et al., 2023;\n4217\nChoose a more relevant summary from Summary #1 and\nSummary #2 with respect to the corresponding Article by\nchoosing an option from A, B, or C. Note that relevance\nmeasures the summary’s selection of important content from\nthe Article, whether the summary grasps the main message\nof the Article without being overwhelmed by unnecessary\nor less significant details.\nArticle: {article}\nSummary #1: {summary from model A}\nSummary #2: {summary from model B}\nA: Summary #1 is more relevant.\nB: Summary #2 is more relevant.\nC: Both Summary #1 and Summary #2 are equally relevant.\nYour choice (enter 1 letter from A to C):\nTable 3: Example prompt for the H2H method on the\nrelevance dimension. Text in {blue}: the specific article,\nand the corresponding summaries generated by a pair\nof compared models.\nWang et al., 2023b; Gao et al., 2022). To avoid the\nmisguidance of wrongly generated reasoning, we\nexplore a more constrained MCQ method for the\nLikert-scale scoring. As shown in Table 2, instead\nof allowing the LLM to freely generate its thoughts,\nwe dictate specific reasoning for each score.\nHead-to-Head Comparison (H2H) Lastly,\nsome concurrent works also observe that ChatGPT\ncan act as an effective ranking model (Ma et al.,\n2023a,b). We thus explore the head-to-head\ncomparison approach for LLM-based evaluations.\nAs shown in Table 3, we present 2 summaries\n(Summary #1 and #2) generated by different\nsummarization systems on the same input article,\nthen prompt the LLM to select the better summary,\nor to indicate a tie. Moreover, to avoid potential\nbiases that arise from the summary IDs, we\nconduct each evaluation twice, presenting the same\nsummary as either #1 or #2 respectively.\n3.2 Stability of LLM Evaluators\nTo ensure fairness across all evaluated systems,\nwe argue that it is crucial for LLMs to produce\nstable evaluations. That is, regardless of evaluated\nsystems, the LLMs should maintain a consistent\ndegree of alignment with human judgments. We\ninvestigate such stability in two ways.\nFirst, We categorize the summaries based on\ntheir originating summarization systems, and then\nexamine the correlation between the LLM and\nhuman evaluations for each system. Ideally, if\nan LLM is stable across systems, it should pro-\nduce evaluations that are similarly correlated to\nhuman evaluations. Otherwise, if the correlations\ndiffer significantly across different candidates, then\nwe may conclude that the LLM’s evaluations are\nsystem-dependent.\nSecond, we define a meta-correlation metric\nto quantify the extent to which the LLM’s perfor-\nmance is affected by the quality of the evaluated\nsystems. Specifically, we use the average human\nscore for each candidate as an indicator of its sum-\nmarization quality (Qi), as shown in Equation (1):\nQi = 1\nN\nN∑\nj=1\nfhuman(gi,j) (1)\nwhere fhuman(·) indicates the human evaluation,\ngi,j represents the jth summary generated by the\nith candidate system. Each candidate’s quality is\ncalculated as an average ofNgenerated summaries\n(N = 100for all systems). Next, we use the corre-\nlation Pi between LLM scores and human scores as\nan indicator of the LLM’s evaluation performance\nfor the ith candidate, as follows:\nPi = ρ([fLLM(gi,1),...,f LLM(gi,N )],\n[fhuman(gi,1),...,f human(gi,N )]) (2)\nwhere ρdenotes the correlation metric (i.e., Spear-\nman correlation, Pearson correlation, or Kendall’s\nTau2), and fLLM(·) indicates the LLM’s evaluation\nfor each summary gi,j. Finally, we calculate the\nmeta-correlation3 M on a total of kcandidates as:\nM = ρ([Q1,...,Q k],[P1,...,P k]) (3)\nIdeally, an LLM should work well regardless of\nthe quality of the evaluated systems, which means\nthat M should be close to zero. On the other hand,\na significant M would indicate an undesirable rela-\ntionship between the LLM’s evaluation capability\nand the quality of the evaluated systems, suggest-\ning that the LLM evaluation is not stable, such that\nit may not evaluate each candidate system fairly\nusing the same standards.\n2We use the scipy.stats.kendalltau package, which imple-\nments the tau-b variant that accounts for ties.\n3We use the same correlation metric for both Equation2 and\n3. For instance, if Pi is obtained using Spearman correlation,\nthen M is also calculated using Spearman correlation.\n4218\n4 Experiments\n4.1 Setups\nWe use the ChatGPT “gpt-3.5-turbo-0301”\nsnapshot (§ 4.2) for all three methods. By using a\nfixed snapshot, we ensure all evaluations are con-\nducted with the same LLM model. In addition,\nwe evaluate with the GPT-4 “gpt-4-0314” snap-\nshot (§ 4.3) using the best evaluation method de-\ntermined by ChatGPT to check for any potential\nimprovement. Given that ChatGPT and GPT-4 are\namongst the top performing LLMs, we use their\nperformance to estimate the potential of LLMs as\nreliable evaluators. Additional results using three\ndifferent-sized Llama 2 models (Touvron et al.,\n2023) are reported in Appendix D, which all per-\nforms worse. Similar to Luo et al. (2023) and Wu\net al. (2023), we set the temperature to 0 and reset\nthe dialogue history for each evaluation instance.\nDataset We use the SummEval benchmark\ndataset (Fabbri et al., 2021). This dataset contains\nexpert human annotations for coherence, consis-\ntency, fluency, and relevance on the generation re-\nsults from 12 abstractive systems (see details in\nAppendix table 21) on the CNN/DM dataset (Her-\nmann et al., 2015). Each evaluated system gen-\nerates summaries for the same 100 news articles,\nand each summary is scored by 3 expert annotators\nfrom 1 to 5. The annotations achieve with a high\nkappa coefficient of 0.713 (Fabbri et al., 2021). We\nfurther calculate the annotations’ standard devia-\ntions across each evaluated system in Appendix\nTable 20. Given a step size of 1, the standard devi-\nations are considered very small, thus suggesting\nthat this dataset has a high level of human agree-\nment. Following Chiang and Lee (2023), Chhun\net al. (2022), and Guan and Huang (2020), we use\nthe average human scores as the reference scores.\nBaselines We use R OUGE (Lin, 2004) F1\nscores for R OUGE -1, R OUGE -2, and R OUGE -L,\nBERTScore (Zhang et al., 2020b), BARTScore,\nBARTScore-CNN, and BARTScore-CNN-PARA\n(Yuan et al., 2021) as baseline metrics. The\nlast two metrics use BART models fine-tuned on\nCNN/DM’s training data, and are especially strong.\nPrompts We conduct evaluation following our\nprompt formats given in Table 1, 2, and 3. Follow-\ning Fabbri et al. (2021), we re-use the definitions\nof the evaluation dimensions: (i) Coherence - the\ncollective quality of all sentences, (ii) Consistency\n- the factual alignment between the summary and\nthe summarized source, (iii) Fluency - the quality\nof individual sentences, and ( iv) Relevance - the\nselection of important content from the source.\nMeasurements To compare all evaluation meth-\nods on equal ground with human evaluation, we\nuse four different measurements. First, we count\nthe number of correct preferences(#CP), which is\nthe number of times each automatic metric has the\nsame preference as the average human scores do\nover a set of compared system pairs (§ 4.2.1). This\ncan help measure the alignment of evaluation meth-\nods with humans at a granular level. To determine\nthe preferred system by a particular metric, we as-\nsign a system 1 point if its generated summary is\nevaluated as better than that of the other system\naccording to the metric, or assign both systems 0.5\nfor a tie. Then, we aggregate the different scores\nfor the compared systems for all 100 test inputs,\nand the system with a higher score is considered\nthe preferred system by that metric (see Appendix\nTable 22 for details).\nNext, we also use the Pearson correlation (Co-\nhen et al., 2009), Spearman correlation (Spearman,\n1987), and Kendall’s Tau (Kendall, 1938) to mea-\nsure the relationship between the scores of auto-\nmatic evaluators and humans (§ 4.2.2, 4.2.3, 4.2.4).\nWhile the Pearson score measures linear relation-\nships, the other two measure the ordinal relation-\nship that may be non-linear. Moreover, Kendall’s\nTau is less sensitive than Spearman correlation to\noutliers due to its paired counting of concordant\nand discordant pairs.\n4.2 ChatGPT Evaluator\nIn this section, we examine the ChatGPT evaluator\nacross many aspects, ranging from human correla-\ntion and stability across different systems.\n4.2.1 Correct Preferences\nThe ultimate goal of evaluation is to determine if\none candidate system is better than the other in a\ncompared pair. The number of correct preferences\n(#CP) metric normalizes all evaluation methods\ninto determining whether an evaluator can, as a\nhuman expert would, pick the same better system\nor determine a tie. We conduct such analysis with\ndifferent pairs of summarization systems on the\nsame input articles. Due to the limited budget for\nAPI calls, we only evaluate H2H on a challenge set,\nconsisting of 11 candidate pairs with the closest\n4219\nCoherence Consistency Fluency Relevance\nMetrics Spear. Pear. Kend. Spear. Pear. Kend. Spear. Pear. Kend. Spear. Pear. Kend.\nROUGE-1* 0.193 0.202 0.136 0.155 0.186 0.121 0.075 0.153 0.058 0.323 0.361 0.231\nROUGE-2* 0.145 0.146 0.101 0.137 0.156 0.107 0.053 0.095 0.041 0.255 0.262 0.181\nROUGE-L* 0.148 0.158 0.105 0.133 0.167 0.103 0.078 0.146 0.060 0.306 0.340 0.219\nBERTScore* 0.375 0.383 0.265 0.163 0.182 0.127 0.167 0.229 0.130 0.396 0.414 0.285\nBARTScore* 0.381 0.391 0.275 0.271 0.265 0.212 0.168 0.187 0.131 0.381 0.391 0.276\nBARTScore-CNN* 0.461 0.480 0.332 0.389 0.413 0.305 0.310 0.378 0.241 0.425 0.450 0.309\nBARTScore-CNN-PARA* 0.4550.455 0.328 0.413 0.459 0.324 0.368 0.417 0.286 0.414 0.440 0.299\nChatGPT-RTS 0.388 0.399 0.312 0.423 0.532 0.378 0.285 0.302 0.240 0.448 0.463 0.357\nChatGPT-MCQ 0.424 0.416 0.350 0.343 0.487 0.320 0.343 0.431 0.305 0.384 0.395 0.329\nGPT-4-RTS 0.427 0.461 0.361 0.556 0.618 0.522 0.498 0.600 0.452 0.448 0.428 0.373\nTable 4: Spearman (Spear.) correlations, Pearson (Pear.) correlations, and Kendall’s Tau (Kend.) between various\nmetrics and human scores for a total of 1,200 summaries. *: results derived from Wang et al. (2023a). Bolded: best\nresults. Underlined: second best results. Values in light gray color are insignificant (p-value ≥0.05).\nMetrics Coh Con Flu Rel Avg\nRandom 3.67/22 3.67/22 3.67/22 3.67/22 3.67/22\nROUGE-1 3/40 5/46 3/46 4/47 3.75/44.75\nROUGE-2 2/38 7/48 3/45 4/46 4.00/44.25\nROUGE-L 2/31 5/37 4/39 6/41 4.25/37.00\nBERTScore 4/48 5/44 4/44 6/46 4.75/45.50\nBARTScore 8/46 7/48 5/45 6/46 6.50/46.25\nBARTScore-CNN9/53 5/53 5/54 4/53 5.75/53.25\nBARTScore-CNN-PARA9/49 8/54 6/52 5/51 7.00/51.50\nChatGPT-RTS 6/ 54 6/56 9 /62 7 /62 7.00/58.50\nChatGPT-MCQ 5/ 54 7/56 8/60 7/58 6.75/57.00\nChatGPT-H2H 8/- 7/- 7/- 4/- 6.50/-\nGPT-4-RTS 5/55 7/53 8/60 7/56 6.75/56.00\nTable 5: Number of correct preferences (#CP) on the\n11-pair challenge set (in black) and the 66-pair full set\n(in brown). Random: for each pair, there are three pos-\nsibilities (two possibilities for one model being better,\none possibility for a tie) so the random #CP is one-third\nof the total compared pairs.\naverage performances according to human scores.\nHowever, for RTS, MCQ, and other baselines, we\ncan easily calculate the #CP for all 66 possible\npairs (see Appendix E).\nTable 5 reports the #CP for both the standard 66-\npair full set (in brown) and the 11-pair challenge\nset (in black). As shown for the larger standard set,\nRTS unanimously obtains the largest #CP across\nall dimensions, with an average of 58.5 out of 66\ncandidate pairs (i.e. 88.6% accuracy).\nDespite the high overall accuracy, weaknesses\nof such evaluators are revealed as we dive into their\nperformances in the 11-pair challenge set (black\nscores of Table 5), where the evaluated candidates\nare close matches. Specifically, BARTScore-CNN-\nPara performs better than RTS in coherence and\nconsistency, possibly because it is fine-tuned with\nsame-domain summarization data. For fluency and\nrelevance, ChatGPT-RTS still performs best among\nall evaluators. Nonetheless, its average accuracy\ndrops significantly to 63.6% (7 out of 11), which\nindicates LLM evaluators struggle to differentiate\nthe closely matched candidate systems. In other\nwords, LLM evaluators may only reliably compare\ncandidates with a relatively large performance gap.\n4.2.2 Correlations with Human\nTable 4 reports that Spearman, Pearson correla-\ntions, and Kendall’s Tau between scores of multiple\nautomatic evaluators and humans with a total of\n1200 summaries from all systems, across the four\nevaluation dimensions. As shown, ChatGPT RTS\nand MCQ demonstrate stronger correlations with\nhumans than many automatic evaluators, such as\nROUGE and BARTScore, with up to 0.2 gains in\nfluency. While RTS achieves higher correlations in\nthe dimensions of consistency and relevance, MCQ\nhas relatively strong correlations in the dimensions\nof coherence and fluency. Meanwhile, the special-\nized BARTScore-CNN family also shows competi-\ntive performance in coherence, most likely due to\nthe fine-tuning process with CNN/DM.\n4.2.3 Per-candidate Correlations\nNext, we break down the human correlation of\nChatGPT-RTS for each candidate system and mea-\nsure the statistical spread for the correlations across\nall systems (see raw results in Appendix table 23).\nIdeally, a stable evaluator should exhibit the same\nhuman correlation across candidates and dimen-\nsions, and display flattened boxes in a line.\nHowever, as illustrated in Figure 1, the spread\nof correlations for different candidates is particu-\nlarly wide, with up to 0.5 correlation difference in\nconsistency. This means that the RTS evaluator\nexhibits a significantly varying degree of alignment\nwith human judgment for different candidates. In\nother words, ChatGPT-RTS iscandidate-dependent\n4220\nCoherence Consistency Fluency Relevance\nSpear. Pear. Kend. Spear. Pear. Kend. Spear. Pear. Kend. Spear. Pear. Kend.\nROUGE-1 0.0 -0.032 -0.091 -0.490 -0.527 -0.303 -0.420 -0.518 -0.273 -0.420 -0.387 -0.273\nROUGE-2 0.559 0.508 0.485 -0.259 -0.480 -0.152 -0.217 -0.438 -0.152 -0.084 -0.120 -0.121\nROUGE-L 0.231 0.251 0.121 -0.510 -0.522 -0.303 -0.168 -0.412 -0.152 -0.224 -0.266 -0.121\nBERTScore -0.413 -0.403 -0.212 -0.580 -0.869 -0.424 -0.455 -0.663 -0.303 -0.685 -0.756 -0.515\nBARTScore -0.916 -0.747 -0.788 -0.266 -0.504 -0.121 0.154 0.123 0.182 -0.769 -0.837 -0.606\nBARTScore-CNN -0.748 -0.800 -0.636 -0.671 -0.913 -0.515 -0.510 -0.604 -0.485 -0.825 -0.852 -0.667\nBARTScore-CNN-PARA -0.720-0.858 -0.606 -0.685 -0.888-0.576 -0.294 -0.522 -0.212 -0.853-0.880 -0.727\nChatGPT-RTS -0.042 -0.072 -0.121 -0.811 -0.751 -0.636 -0.748 -0.728 -0.606 -0.559 -0.473 -0.394\nChatGPT-MCQ -0.175 -0.11 -0.182 -0.818 -0.411 -0.636 -0.622 -0.484 -0.394 -0.350 -0.622 -0.212\nGPT-4-RTS -0.531 -0.678 -0.424 -0.600 -0.103 -0.236 -0.839 -0.520 -0.515 -0.958 -0.880 -0.848\nTable 6: Meta-correlation for various evaluation methods. Bolded: most negative meta-correlation. Underlined:\nsecond most negative meta-correlation. Values in light gray color are insignificant (p-value≥0.05).\nCoh Con Flu Rel\n−0.2\n0\n0.2\n0.4\n0.6\nSpearman Pearson Kendall\nFigure 1: Spread of per-candidate correlations with\nhuman scores for ChatGPT-RTS evaluations.\nCoh Con Flu Rel\n−1\n−0.5\n0\nRTS MCQ\nFigure 2: Meta-correlation (Kendall’s Tau) for RTS and\nMCQ. Shaded: statistically significant with p <0.05.\nand one should not expect such LLM evaluators to\nhave the same level of human alignment on a new\nsummarization system. Similar trends can also be\nobserved for MCQ (see Appendix table 24).\nIn addition, the medians across the four dimen-\nsions are also different. This indicates that the\nChatGPT is also dimension-dependent and unsta-\nble. Given such varying performances across dif-\nferent dimensions, ChatGPT may not behave well\nwith a newly introduced evaluation criterion.\n4.2.4 Summary Quality vs Human Alignment\nUsing our proposed meta-correlation measurement\nin § 3.2, we analyze the relationship between sum-\nmary quality and human correlation of LLM evalu-\nators. We illustrate the meta-correlation in terms of\n3.5 4 4.5 5\n0\n0.2\n0.4\nAverage Human Scores (Qi)\nCorrelations (Pi) RTS\nMCQ\nFigure 3: Relationship between per-model correlations\n(Kendall’s Tau) and human scores on consistency.\nKendall’s Tau for both RTS and MCQ in Figure 2.\nAs shown, both RTS and MCQ exhibit strong neg-\native meta-correlation for consistency and fluency.\nThis suggests that ChatGPT becomes less human-\naligned with improving qualities of the evaluated\nsystems.\nTo illustrate this phenomenon further, we scatter\nthe paired coordinates of the summarization system\nquality (Qi, Equation (1)) and ChatGPT’s evalua-\ntion performance (Pi, Equation (2)) in Figure 3. As\nshown, while the LLM evaluator is better human-\ncorrelated with lower-quality candidates (< 3.5), it\nis less reliable when dealing with high-quality can-\ndidates (> 4.7) with much lower and inconsistent\ncorrelations.\nWe compare the meta-correlation for all evalu-\nation metrics in Table 6. We can see that while\nthe ROUGE metrics exhibit no significantly nega-\ntive meta-correlation, the neural metrics all display\nsignificant meta-correlation in certain dimensions.\nOne highly likely reason for this behavior is due\nto the varying biases inherent to the neural mod-\nels, which would explain why ROUGE as a simple\nn-gram overlap metric doesn’t exhibit significant\nnegative meta-correlations. Interestingly, ROUGE -\n2 even shows a strong positive meta-correlation\non coherence (which is plausible, because bi-gram\n4221\nCoh Con Flu Rel\n2\n3\n4\n5\n2.52\n4.06\n2.59\n3.683.68\n4.59\n3.66\n4.29\n3.35\n4.55 4.61\n3.72\nAverage Scores\nRTS MCQ Human\nFigure 4: The average ChatGPT RTS and MCQ scores\nand human scores across dimensions.\noverlap performance may be more accurate as can-\ndidates produce more coherent texts).\nBoth the BARTScore variants and LLMs\ndemonstrate the most negative meta-correlations.\nChatGPT-RTS has the most negative meta-\ncorrelation in the dimensions of consistency and\nfluency, indicating that it may be the least reliable\nto evaluate high-quality systems on these dimen-\nsions. On the other hand, the BARTScore family\nmay be unreliable in comparing systems with high\nqualities of coherence, consistency, and relevance.\nSo far, the observations discussed in § 4.2.3 and\n§ 4.2.4 collectively suggest that LLM evaluators\nmay not be a reliable standalone metric for chal-\nlenging scenarios, and further human evaluation is\nrequired for conclusive decisions.\n4.2.5 RTS and MCQ Scores\nLastly, we delve into the detailed scores generated\nby ChatGPT with either the RTS or MCQ method.\nSince both methods score the summaries in the\nsame range of human scores of 1 to 5 (Fabbri et al.,\n2021), we can show a direct comparison of the aver-\nage RTS and MCQ scores with human scores in Fig-\nure 4 (see more details in Appendix F). As shown,\nthe RTS scores are much lower than the human\nscores across all dimensions, while MCQ scores\nare consistently higher and better match the hu-\nman scores (except for relevance). In other words,\nwhile RTS is best aligned with humans according\nto § 4.2.1 and § 4.2.2, we cannot replace the human\nscores with RTS scores in absolute terms.\nThe discrepancy may be attributed to the un-\nfaithful reasoning generated by LLMs (Lyu et al.,\n2023; Wang et al., 2023b; Gao et al., 2022). Our\nfurther investigation suggests that ChatGPT-RTS\ngenerates false or unrelated-to-dimension reason-\ning. Thus, it is possible that the much lower scores\ncould be caused by ChatGPT penalizing the sum-\nCoh Con Flu Rel\n0\n0.2\n0.4\n0.6\n0.8\nSpearman Pearson Kendall\nFigure 5: GPT-4’s (RTS) spread of per-candidate corre-\nlations.\nmaries according to false premises (more examples\nin Appendix G). For instance, RTS may penalize\nthe summary’s repetitiveness in the consistency di-\nmension or suppress fluency ratings for missing\nimportant details.4 On the other hand, the MCQ\ncounterpart gives higher overall scores, most likely\nbecause the confined set of pre-defined reasons\nprevents such unrelated penalization, though not\nleading to better human alignment.\n4.3 GPT-4 Evaluator\nA natural question to ask is whether such afore-\nmentioned limitations are resolved with an stronger\nLLM. In this section, we conduct similar analyses\non GPT-4 (OpenAI, 2023) with the RTS method.\nWe present the GPT-4 results in the last rows of\nTable 4 and 5. The results suggest that a stronger\nLLM does not necessarily translate to a stronger\nLLM evaluator, although Table 4 does show that\nGPT-4 outperforms ChatGPT in terms of human\ncorrelation consistently across most dimensions.\nUnfortunately, GPT-4 still suffers from the same\nlimitations as ChatGPT. It appears to be both\ncandidate-dependent and dimension-dependent, as\ndemonstrated by the large spreads with varying\nmedian values across dimensions in Figure 5 and\nthe significantly negative meta-correlations out of\n3 dimensions (Table 6). However, GPT-4 is less\ndimension-dependant as compared to ChatGPT, as\nthe medians in the box plots in Figure 5 are more\naligned than those in Figure 1.\nIn addition, there is a notable enhancement in\nthe meta-correlation for consistency, which we at-\ntribute to a significant reduction in reported halluci-\nnations with GPT-4 (OpenAI, 2023). It is possible\nthat with much more instruction training to avoid\nhallucinations, GPT-4 is much better aligned with\nhumans to detect inconsistencies (i.e. hallucina-\n4Fabbri et al. (2021) observe similar issues with crowd-\nsourced non-expert annotators.\n4222\nCoherence Consistency Fluency Relevance\nSpear. Pear. Kend. Spear. Pear. Kend. Spear. Pear. Kend. Spear. Pear. Kend.\nρ(Ri,PRTSi ) 0.343 0.198 0.091 0.685 0.506 0.576 0.727 0.797 0.545 0.168 0.128 0.000\nρ(Ri,PMCQi ) 0.657 0.625 0.394 0.685 0.616 0.515 0.322 0.573 0.212 0.091 -0.106 0.000\nTable 7: Correlations between RTS-MCQ Ri and RTS-Human (PRTS\ni ) and MCQ-Human (PMCQ\ni ). High values\nsuggest Ri can be a reliability indicator for RTS and MCQ. Light gray values are insignificant (p ≥0.05).\ntions) in summaries.\nNevertheless, GPT-4 exhibits a much worse neg-\native meta-correlation in the relevance dimension,\nwhich, interestingly, seems to reflect the challenges\nof maintaining both “truthfulness” and “informa-\ntiveness” (Ouyang et al., 2022). This is because\na model could be easily made more truthful if al-\nlowed to provide less relevant information (for in-\nstance, by refusing to answer the users’ questions).\nIt is possible that with reduced capability in the\ninformativeness dimension, the model is less capa-\nble of differentiating the nuances of less relevant\nsummaries when the summary quality is generally\nhigh. Nevertheless, we leave it to future work to\ndetermine whether GPT-4’s more negative meta-\ncorrelation in the relevance dimension could be\nrelated to its stronger performance in consistency.\nWe provide more details on the GPT-4 evaluator in\nAppendix H.\n5 A Temporary Efficient Framework\nDespite the aforementioned limitations, it may be\nhard to resist the temptation of using LLM evalu-\nators given their superiority over other automatic\nmetrics. In such a case, one should be able to tell\nwhen LLM evaluators are more likely to be unre-\nliable and employ further human evaluation when\nnecessary. To this end, we suggest combining the\nRTS and MCQ scores as a cost-efficient framework.\nSpecifically, we calculate the correlation between\nRTS and MCQ scores for the ith candidate system\nas a reliability indicator:\nRi = ρ([fRTS(gi,1),...,f RTS(gi,N )],\n[fMCQ(gi,1),...,f MCQ(gi,N )]) (4)\nThen, we can loosely infer that up to a reliability\ntolerance r ∈(0,1), the LLM evaluators (either\nRTS or MCQ) are reliable if Ri > r. In other\nwords, given a candidate i, if RTS and MCQ agree\nwith each other up to a certain degree of tolerance\nr, we may assume the evaluator is reliable enough\nto avoid invoking further human evaluation.\nTo validate this theory, we measure the cor-\nrelations ρ(Ri,PRTS\ni ) or ρ(Ri,PMCQ\ni ), where\nPRTS/MCQ\ni is the performance of either method\nas defined in Equation (2). Given significantly\nlarge positive values of either ρ(Ri,PRTS\ni ) or\nρ(Ri,PMCQ\ni ), we can then conclude that Ri can\nbe used as a reliable indicator for the performance\nof the corresponding method.\nAs shown in Table 7, Ri demonstrates a signif-\nicant correlation with PRTS\ni on both the consis-\ntency and fluency dimensions, and with PMCQ\ni on\nthe coherence and consistency dimensions. This\nmeans that if RTS and MCQ generally agree with\neach other on the candidate’s performance on a\nparticular dimension with high ρ(Ri,PRTS\ni ) (or\nρ(Ri,PMCQ\ni )), RTS (or MCQ) is more likely to\nbe human-aligned. Meanwhile, if RTS disagrees\nwith MCQ (Ri <r), further human evaluators are\nrequired to provide a conclusive evaluation. We\nprovide Ri values for ChatGPT on each evaluated\nsystem in Appendix Table 29.\n6 Conclusion\nWe explore the potential of using LLMs with dif-\nferent prompting techniques as metrics for abstrac-\ntive summarization systems. Our extensive anal-\nysis suggests that while LLMs like ChatGPT per-\nform better than commonly used automatic met-\nrics across different summarization systems and\ndimensions, they are still not ready to replace hu-\nman evaluators because they are candidate- and\ndimension-dependent, and they do not align well\nwith human when comparing high-quality candi-\ndates. Nonetheless, if an LLM evaluator is to be\nused, we suggest combining multiple evaluation\nmethods as a preliminary indicator to determine\nwhether the metric is likely to be unreliable and\nwhether further human evaluation is required.\nLimitations\nPotential Human Bias. We benchmark the LLM\nevaluation results against the average of three hu-\nman expert scores. Naturally, it is possible that\nthese scores may exhibit potential biases of the\nhuman experts. Nevertheless, we wish to explore\n4223\nwhether LLM evaluators are aligned with human\nexperts, and may naturally exhibit the same bias\nas a human would. In other words, we examine\nwhether we can reliably replace human annotators\nwith LLMs, instead of seeking a “perfect” solution\nthat has absolutely zero bias.\nDataset Size. Given the constraints of the small\nsize of the human-annotated SummEval dataset,\nwe could only evaluate 100 summaries generated\nfor each summarization system, with a total of 12\nabstractive summarization systems. Since we have\nobserved a significant correlation of LLM evalua-\ntions with humans for the consolidated 1200 sum-\nmaries across all systems, it is possible that with\na larger evaluation number, the per-system corre-\nlation could also be improved. In addition, given\nonly 12 evaluated systems, our meta-correlation\nmay still be subject to sample biases. We leave\nmore investigations for the future once there are\nlarger annotated datasets.\nPrompt tuning. Designing better prompt for\nLLMs are also ongoing research. Although it is\npossible that LLMs may act as better evaluators\nwith better prompts, prompt tuning is not our fo-\ncus. We seek to highlight the limitations of the\ninvestigated LLMs and have demonstrated that lim-\nitations such as negative meta-correlation are also\nfound with a few other alternative prompts (see\nAppendix C).\nAvailability of Commercialized LLM We note\nthat the “gpt-3.5-turbo-0301” snapshot is cur-\nrently taken down5 by OpenAI and replaced with a\nnewer snapshot, “gpt-3.5-turbo-0613”. This is also\none disadvantage of using out-of-the-box commer-\ncialized LLM for summarization evaluations, as\nthe exact checkpoints may not be stably available.\nAs a result, future models may not be fairly com-\npared against previously evaluated models using a\ndifferent LLM checkpoint. Nevertheless, our paper\nonly seeks to investigate the potential of LLM as an\nout-of-the-box evaluator, and the OpenAI models\nare currently one of the strongest. Eventually, we\nwish to raise awareness of some of the significant\nlimitations found with these LLMs, which need to\nbe resolved before LLMs can be used as direct re-\nplacements for human evaluations. In addition, we\nalso note that the cost of evaluating only 100 sum-\n5We release all LLM generations involved in our experi-\nments in https://github.com/DAMO-NLP-SG/LLM_summev\nalas JSON files.\nmaries for each system is relatively low (around\n2 USD per system using ChatGPT). Since LLMs\nalso conduct evaluations much faster than humans\n(around 2 minutes for LLMs versus 10 hours for\nhuman for 100 summaries), it may not pose signifi-\ncant barriers if one was to re-evaluate all compared\nsystems on a single LLM.\nLimited Use of the Temporary Solution Unfor-\ntunately, our temporary efficient framework doesn’t\napply to the relevance dimension, where theRi has\nno significant correlation with the performances of\neither RTS or MCQ. Moreover, the rvalue may be\ndataset-dependent, and it is hard to decide where\nto draw this line. We leave for future work of de-\nveloping better methods to gauge the reliability of\nLLM evaluations.\nAcknowledgements\nYang You is being sponsored by NUS startup\ngrant (Presidential Young Professorship), Singa-\npore MOE Tier-1 grant, ByteDance grant, ARCTIC\ngrant, SMI grant and Alibaba grant.\nReferences\nLidong Bing, Piji Li, Yi Liao, Wai Lam, Weiwei Guo,\nand Rebecca Passonneau. 2015. Abstractive multi-\ndocument summarization via phrase selection and\nmerging. In Proceedings of ACL-IJCNLP.\nMoye Chen, Wei Li, Jiachen Liu, Xinyan Xiao, Hua\nWu, and Haifeng Wang. 2021. Sgsum: Transform-\ning multi-document summarization into sub-graph\nselection. In Proceedings of EMNLP.\nLiying Cheng, Xingxuan Li, and Lidong Bing. 2023.\nIs gpt-4 a good data analyst? arXiv preprint\narXiv:2305.15038.\nLiying Cheng, Dekun Wu, Lidong Bing, Yan Zhang,\nZhanming Jie, Wei Lu, and Luo Si. 2020. Ent-desc:\nEntity description generation by exploring knowl-\nedge graph. In Proceedings of EMNLP.\nCyril Chhun, Pierre Colombo, Fabian M. Suchanek,\nand Chloé Clavel. 2022. Of human criteria and au-\ntomatic metrics: A benchmark of the evaluation of\nstory generation. In Proceedings of Coling.\nCheng-Han Chiang and Hung-yi Lee. 2023. Can large\nlanguage models be an alternative to human evalua-\ntions? In Proceedings of ACL.\nIsrael Cohen, Yiteng Huang, Jingdong Chen, Jacob Ben-\nesty, Jacob Benesty, Jingdong Chen, Yiteng Huang,\nand Israel Cohen. 2009. Pearson correlation coeffi-\ncient. Noise reduction in speech processing.\n4224\nYue Dong, Yikang Shen, Eric Crawford, Herke van\nHoof, and Jackie Chi Kit Cheung. 2018. Banditsum:\nExtractive summarization as a contextual bandit. In\nProceedings of EMNLP.\nOri Ernst, Avi Caciularu, Ori Shapira, Ramakanth Pa-\nsunuru, Mohit Bansal, Jacob Goldberger, and Ido\nDagan. 2022. Proposition-level clustering for multi-\ndocument summarization. In Proceedings of NAACL.\nAlexander Fabbri, Irene Li, Tianwei She, Suyi Li, and\nDragomir Radev. 2019. Multi-news: A large-scale\nmulti-document summarization dataset and abstrac-\ntive hierarchical model. In Proceedings of ACL.\nAlexander R Fabbri, Wojciech Kry´sci´nski, Bryan Mc-\nCann, Caiming Xiong, Richard Socher, and Dragomir\nRadev. 2021. Summeval: Re-evaluating summariza-\ntion evaluation. TACL.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018.\nHierarchical neural story generation. In Proceedings\nof ACL.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2022. Pal: Program-aided language\nmodels. In Proceedings of ICML.\nMingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Ship-\ning Yang, and Xiaojun Wan. 2023. Human-like sum-\nmarization evaluation with chatgpt. arXiv preprint\narXiv:2304.02554.\nShen Gao, Xiuying Chen, Piji Li, Zhaochun Ren, Li-\ndong Bing, Dongyan Zhao, and Rui Yan. 2019. Ab-\nstractive text summarization by incorporating reader\ncomments. In Proceedings of AAAI.\nJian Guan and Minlie Huang. 2020. UNION: An Un-\nreferenced Metric for Evaluating Open-ended Story\nGeneration. In Proceedings of EMNLP.\nJunxian He, Wojciech Kryscinski, Bryan McCann,\nNazneen Rajani, and Caiming Xiong. 2022. CTRL-\nsum: Towards generic controllable text summariza-\ntion. In Proceedings of EMNLP.\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. NeurIPS.\nMG Kendall. 1938. A new measure of rank correlation.\nBiometrika.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. NeurIPS.\nWojciech Kry´sci´nski, Romain Paulus, Caiming Xiong,\nand Richard Socher. 2018. Improving abstraction in\ntext summarization. In Proceedings of EMNLP.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020. Bart:\nDenoising sequence-to-sequence pre-training for nat-\nural language generation, translation, and compre-\nhension. In Proceedings of ACL.\nPiji Li, Wai Lam, Lidong Bing, and Zihao Wang. 2017.\nDeep recurrent generative decoder for abstractive text\nsummarization. In Proceedings of EMNLP.\nWei Li, Xinyan Xiao, Jiachen Liu, Hua Wu, Haifeng\nWang, and Junping Du. 2020. Leveraging graph to\nimprove abstractive multi-document summarization.\nIn Proceedings of ACL.\nChin-Yew Lin. 2004. ROUGE: A package for automatic\nevaluation of summaries. In Text Summarization\nBranches Out.\nPeter J Liu, Mohammad Saleh, Etienne Pot, Ben\nGoodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\nShazeer. 2018. Generating wikipedia by summariz-\ning long sequences. In ICLR.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. G-eval:\nNlg evaluation using gpt-4 with better human align-\nment. arXiv 2303.16634.\nYang Liu and Mirella Lapata. 2019. Hierarchical trans-\nformers for multi-document summarization. In Pro-\nceedings of ACL.\nYixin Liu, Ansong Ni, Linyong Nan, Budhaditya Deb,\nChenguang Zhu, Ahmed H Awadallah, and Dragomir\nRadev. 2022. Leveraging locality in abstractive text\nsummarization. In Proceedings of EMNLP.\nZheheng Luo, Qianqian Xie, and Sophia Ananiadou.\n2023. Chatgpt as a factual inconsistency evaluator\nfor abstractive text summarization. arXiv preprint\narXiv:2303.15621.\nQing Lyu, Shreya Havaldar, Adam Stein, Li Zhang,\nDelip Rao, Eric Wong, Marianna Apidianaki, and\nChris Callison-Burch. 2023. Faithful chain-of-\nthought reasoning. arXiv preprint arXiv:2301.13379.\nXueguang Ma, Xinyu Zhang, Ronak Pradeep, and\nJimmy Lin. 2023a. Zero-shot listwise document\nreranking with a large language model. arXiv\npreprint arXiv:2305.02156.\nYubo Ma, Yixin Cao, YongChing Hong, and Aixin Sun.\n2023b. Large language model is not a good few-shot\ninformation extractor, but a good reranker for hard\nsamples! arXiv preprint arXiv:2303.08559.\nAni Nenkova. 2006. Summarization evaluation for text\nand speech: issues and approaches. In Ninth Interna-\ntional Conference on Spoken Language Processing.\nAni Nenkova and Kathleen McKeown. 2012. A survey\nof text summarization techniques. Mining text data.\nOpenAI. 2023. Gpt-4 technical report. arXiv preprint.\n4225\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. NeurIPS.\nKarolina Owczarzak, John Conroy, Hoa Trang Dang,\nand Ani Nenkova. 2012. An assessment of the accu-\nracy of automatic evaluation in summarization. In\nProceedings of workshop on evaluation metrics and\nsystem comparison for automatic summarization.\nArtidoro Pagnoni, Vidhisha Balachandran, and Yulia\nTsvetkov. 2021. Understanding factuality in abstrac-\ntive summarization with frank: A benchmark for\nfactuality metrics. In Proceedings of NAACL-HLT.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of ACL.\nSwarnadeep Saha, Shiyue Zhang, Peter Hase, and Mo-\nhit Bansal. 2022. Summarization programs: Inter-\npretable abstractive summarization with neural mod-\nular trees. In ICLR.\nChenhui Shen, Liying Cheng, Lidong Bing, Yang You,\nand Luo Si. 2022a. Sentbs: Sentence-level beam\nsearch for controllable summarization. EMNLP.\nChenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Li-\ndong Bing, and Yang You. 2023. A hierarchi-\ncal encoding-decoding scheme for abstractive multi-\ndocument summarization. In Findings of EMNLP.\nChenhui Shen, Liying Cheng, Ran Zhou, Lidong Bing,\nYang You, and Luo Si. 2022b. Mred: A meta-\nreview dataset for structure-controllable text genera-\ntion. Findings of ACL.\nC Spearman. 1987. The proof and measurement of\nassociation between two things. American Journal\nof Psychology.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023a. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2023b. Self-consistency\nimproves chain of thought reasoning in language\nmodels. In ICLR.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed H Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. NeurIPS.\nJohannes Welbl, Amelia Glaese, Jonathan Uesato,\nSumanth Dathathri, John Mellor, Lisa Anne Hen-\ndricks, Kirsty Anderson, Pushmeet Kohli, Ben Cop-\npin, and Po-Sen Huang. 2021. Challenges in detoxi-\nfying language models. In Findings of EMNLP.\nNing Wu, Ming Gong, Linjun Shou, Shining Liang,\nand Daxin Jiang. 2023. Large language models are\ndiverse role-players for summarization evaluation.\narXiv preprint arXiv:2303.15078.\nWen Xiao, Iz Beltagy, Giuseppe Carenini, and Arman\nCohan. 2022. Primera: Pyramid-based masked sen-\ntence pre-training for multi-document summarization.\nIn Proceedings of ACL.\nJiacheng Xu and Greg Durrett. 2021. Dissecting gen-\neration modes for abstractive summarization models\nvia ablation and attribution. In Proceedings of ACL-\nIJCNLP.\nKexin Yang, Dayiheng Liu, Wenqiang Lei, Baosong\nYang, Mingfeng Xue, Boxing Chen, and Jun Xie.\n2023. Tailor: A soft-prompt-based approach to\nattribute-based controlled text generation. In Pro-\nceedings of ACL.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\nBartscore: Evaluating generated text as text genera-\ntion. NeurIPS.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter Liu. 2020a. Pegasus: Pre-training with extracted\ngap-sentences for abstractive summarization. In Pro-\nceedings of ICML.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-\nberger, and Yoav Artzi. 2020b. Bertscore: Evaluating\ntext generation with bert. In ICLR.\nQingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang,\nMing Zhou, and Tiejun Zhao. 2018. Neural docu-\nment summarization by jointly learning to score and\nselect sentences. In Proceedings of ACL.\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B\nBrown, Alec Radford, Dario Amodei, Paul Chris-\ntiano, and Geoffrey Irving. 2019. Fine-tuning lan-\nguage models from human preferences. arXiv\npreprint arXiv:1909.08593.\n4226\nScore the following Summary given the corresponding Ar-\nticle with respect to consistency from one to five, where\none indicates “inconsistency” and five indicates “perfect\nconsistency”. Note that consistency measures the factual\nalignment between the Summary and the Article, whether\nthe Summary is faithful to the Article without introducing\ncontradictions or misleading representations.\nArticle: {article}\nSummary: {summary}\nProvide your reason in one sentence, then give a final score:\nTable 8: Example prompt for the RTS method on the\nconsistency dimension. Text in {blue}: the specific\narticle and the corresponding summary to be evaluated.\nScore the following Summary given the corresponding Arti-\ncle with respect to fluency from one to five, where one indi-\ncates “disfluency” and five indicates “perfect fluency”. Note\nthat fluency measures the quality of individual sentences in\nthe Summary, whether the Summary is well-written, gram-\nmatically correct, and readable on the sentence level.\nArticle: {article}\nSummary: {summary}\nProvide your reason in one sentence, then give a final score:\nTable 9: Example prompt for the RTS method on the\nfluency dimension. Text in {cyan}: the specific article\nand the corresponding summary to be evaluated.\nScore the following Summary given the corresponding Ar-\nticle with respect to coherence from one to five, where one\nindicates “incoherence” and five indicates “perfect coher-\nence”. Note that coherence measures the collective quality\nof the Summary, whether the Summary presents informa-\ntion that flows smoothly and avoids abrupt transitions or\ndisjoint statements.\nArticle: {article}\nSummary: {summary}\nProvide your reason in one sentence, then give a final score:\nTable 10: Example prompt for the RTS method on\nthe coherence dimension. Text in {cyan}: the specific\narticle and the corresponding summary to be evaluated.\nA Evaluation Dimensions\nFabbri et al. (2021) has defined 4 evaluation dimen-\nsions as follows:\n1. Coherence: The collective quality of all\nsentences. The summary should be well-\nChoose an option from A to E in order to score the follow-\ning Summary given the corresponding Article with respect\nto consistency from one to five, where one indicates “in-\nconsistency” and five indicates “perfect consistency”. Note\nthat consistency measures the factual alignment between the\nSummary and the Article, whether the Summary is faithful\nto the Article without introducing contradictions or mislead-\ning representations.\nArticle: {article}\nSummary: {summary}\nA: The Summary is totally inconsistent with the Article.\nScore: One.\nB: The majority of the Summary is inconsistent with the\nArticle. Score: Two.\nC: Some information in the Summary is consistent with the\nArticle whereas some are not. Score: Three.\nD: The majority of the Summary is consistent with the\nArticle. Score: Four.\nE: All information included in the Summary is consistent\nwith the Article. Score: Five.\nYour Answer (enter 1 letter from A to E):\nTable 11: Example prompt for the MCQ method on\nthe consistency dimension. Text in {cyan}: the specific\narticle and the corresponding summary to be evaluated.\nstructured and well-organized. The summary\nshould not just be a heap of related informa-\ntion but should build from sentence to sen-\ntence to a coherent body of information about\na topic.\n2. Consistency: The factual alignment between\nthe summary and the summarized source. A\nfactually consistent summary contains only\nstatements that are entailed by the source doc-\nument.\n3. Fluency: The quality of individual sentences.\nSentences in the summary should have no for-\nmatting problems, capitalization errors or ob-\nviously ungrammatical sentences (e.g., frag-\nments, missing components) that make the\ntext difficult to read.\n4. Relevance: Selection of important content\nfrom the source. The summary should include\nonly important information from the source\ndocument.\nWe follow the above definitions for designing\nChatGPT’s evaluation prompts.\n4227\nChoose an option from A to E in order to score the follow-\ning Summary given the corresponding Article with respect\nto fluency from one to five, where one indicates \"disflu-\nency\" and five indicates \"perfect fluency\". Note that fluency\nmeasures the quality of individual sentences in the Sum-\nmary, whether the Summary is well-written, grammatically\ncorrect, and readable on the sentence level.\nArticle: {article}\nSummary: {summary}\nA: The Summary is totally disfluent. Score: One.\nB: The majority of the Summary is disfluent. Score: Two.\nC: Some sentences in the Summary are fluent whereas some\nare not. Score: Three.\nD: The majority of the Summary is fluent. Score: Four\nE: All sentences in the Summary are fluent. Score: Five.\nYour Answer (enter 1 letter from A to E):\nTable 12: Example prompt for the MCQ method on the\nfluency dimension. Text in {cyan}: the specific article\nand the corresponding summary to be evaluated.\nChoose an option from A to E in order to score the following\nSummary given the corresponding Article with respect to co-\nherence from one to five, where one indicates \"incoherence\"\nand five indicates \"perfect coherence\". Note that coherence\nmeasures the collective quality of the Summary, whether\nthe Summary presents information that flows smoothly and\navoids abrupt transitions or disjoint statements.\nArticle: {article}\nSummary: {summary}\nA: The Summary is completely incoherent. Score: One.\nB: The Summary is mostly incoherent. Score: Two.\nC: The Summary is somewhat coherent. Score: Three.\nD: The Summary is mostly coherent. Score: Four.\nE: The Summary is completely coherent. Score: Five.\nYour Answer (enter 1 letter from A to E):\nTable 13: Example prompt for the MCQ method on\nthe coherence dimension. Text in {cyan}: the specific\narticle and the corresponding summary to be evaluated.\nB Prompt Details and Design\nWe show the RTS prompts for relevance, consis-\ntency, fluency, and coherence in Table 1, Table 8,\nTable 9, and Table 10 respectively.\nWe show the MCQ prompts for relevance, con-\nsistency, fluency, and coherence in Table 2, Table\n11, Table 12, and Table 13 respectively.\nWe show the H2H prompts for relevance, consis-\ntency, fluency, and coherence in Table 3, Table 14,\nTable 15, and Table 16 respectively.\nChoose a more consistent summary from Summary #1 and\nSummary #2 with respect to the corresponding Article by\nchoosing an option from A, B, or C. Note that consistency\nmeasures the factual alignment between the summary and\nthe Article, whether the summary is faithful to the Article\nwithout introducing contradictions or misleading represen-\ntations.\nArticle: {article}\nSummary #1: {summary from model A}\nSummary #2: {summary from model B}\nA: Summary #1 is more consistent.\nB: Summary #2 is more consistent.\nC: Both Summary #1 and Summary #2 are equally consis-\ntent.\nYour choice (enter 1 letter from A to C):\nTable 14: Example prompt for the H2H method on the\nconsistency dimension. Text in {cyan}: the specific\narticle, and the corresponding summaries generated by\na pair of compared models.\nChoose a more fluent summary from Summary #1 and Sum-\nmary #2 with respect to the corresponding Article by choos-\ning an option from A, B, or C. Note that fluency measures\nthe quality of individual sentences in the summary, whether\nthe summary is well-written, grammatically correct, and\nreadable on the sentence level.\nArticle: {article}\nSummary #1: {summary from model A}\nSummary #2: {summary from model B}\nA: Summary #1 is more fluent.\nB: Summary #2 is more fluent.\nC: Both Summary #1 and Summary #2 are equally fluent.\nYour choice (enter 1 letter from A to C):\nTable 15: Example prompt for the H2H method on the\nfluency dimension. Text in {cyan}: the specific article,\nand the corresponding summaries generated by a pair\nof compared models.\nTo determine the exact definitions used in our\nprompts for each dimension, we re-use the first\nsentence from Fabbri et al. (2021)’s definition. We\nthen prompt the LLM to provide a definition for\nthe evaluated dimension, such as “define the word\nrelevance in the context of summarization”, then\nextract the key phrases generated that we believe to\nfit the definitions of Fabbri et al. (2021) to make up\nthe full definition. We believe this approach may\n4228\nChoose a more coherent summary from Summary #1 and\nSummary #2 with respect to the corresponding Article by\nchoosing an option from A, B, or C. Note that coherence\nmeasures the collective quality of the summary, whether\nthe summary presents information that flows smoothly and\navoids abrupt transitions or disjoint statements.\nArticle: {article}\nSummary #1: {summary from model A}\nSummary #2: {summary from model B}\nA: Summary #1 is more coherent.\nB: Summary #2 is more coherent.\nC: Both Summary #1 and Summary #2 are equally coherent.\nYour choice (enter 1 letter from A to C):\nTable 16: Example prompt for the H2H method on\nthe coherence dimension. Text in {cyan}: the specific\narticle, and the corresponding summaries generated by\na pair of compared models.\nCoh Con Flu Rel Avg\nChatGPT-RTS2 5 6 7 6 6.00\nChatGPT-MCQ2 4 7 6 4 5.25\nChatGPT-StarEval 7 7 7 6 6.75\nTable 17: Total correct pairs for alternative prompts.\nCoh: coherence; Con: consistency; Flu: fluency; Rel:\nrelevance.\nhelp the LLM to better evaluate the summaries\naccording to definitions partially generated in its\nown language. Nevertheless, we didn’t invest ex-\ntensive efforts in prompt designs as this is not our\nkey focus. We also demonstrate that our prompts\nhave better evaluation results than two alternative\nprompts in Appendix C.\nC Alternative prompts\nWe also use ChatGPT to evaluate with the exact\nprompts from Wang et al. (2023a). We name these\nprompts “StarEval” since they prompt the LLM\nto give one to five stars for the summary. In ad-\ndition, we use ChatGPT to evaluate with alterna-\ntive prompts for RTS and MCQ by using the full\ndefinition as shown in Appendix A instead of sup-\nplementing the definition with ChatGPT-generated\nphrases. We name these two prompts RTS2 and\nMCQ2 respectively.\nWe show the results of these alternative prompts\nin Table 17 and Table 18.\nD Llama 2 Results\nWe report the results of using three different sizes\nof Llama 2 models as LLM evaluators in Table 19.\nAs shown, while the smallest model (7B) exhibits\nvery low correlations with human scores (and only\nsignificant on the consistency and relevance dimen-\nsions), the larger models (13B and 70B) demon-\nstrate significant correlations with human scores\non the full dataset level. However, even the best-\nperforming 70B model fails to outperform the hu-\nman correlation of BARTScore, and is completely\noverwhelmed by the results of ChatGPT and GPT-\n4. This suggests that the open-sourced Llama 2\nmodels are not suitable to be used as zero-shot\nevaluators. Moreover, all Llama 2 models exhibit\nsignificant meta-correlations for at least one dimen-\nsion.\n4229\nCoherence Consistency Fluency Relevance\nSpear. Pear. Kend. Spear. Pear. Kend. Spear. Pear. Kend. Spear. Pear. Kend.\nRTS2 Human-Corr 0.339 0.338 0.285 0.393 0.497 0.350 0.290 0.280 0.252 0.440 0.441 0.351\nMeta-Corr -0.559 -0.476 -0.424 -0.748 -0.843 -0.576 -0.825 -0.823 -0.636 -0.385 -0.506 -0.212\nMCQ2 Human-Corr 0.430 0.423 0.355 0.327 0.483 0.306 0.258 0.396 0.229 0.240 0.258 0.206\nMeta-Corr -0.308 -0.275 -0.212 -0.545 -0.446 -0.364 -0.811 -0.615 -0.636 -0.217 -0.707 -0.182\nStarEvalHuman-Corr 0.418 0.417 0.341 0.297 0.421 0.264 0.246 0.323 0.217 0.393 0.405 0.323\nMeta-Corr -0.084 -0.101 0.000 -0.664 -0.684 -0.545 -0.441 -0.460 -0.212 -0.497 -0.579 -0.303\nTable 18: Results of using alternative prompt with ChatGPT. Light gray values are insignificant (p-value≥0.05).\nHuman-Corr reports the overall correlation of ChatGPT scores with human scores. Meta-corr shows the meta-\ncorrelation.\nCoherence Consistency Fluency Relevance\nSpear. Pear. Kend. Spear. Pear. Kend. Spear. Pear. Kend. Spear. Pear. Kend.\n7B Human-Corr -0.001 -0.000 -0.001 0.114 0.130 0.104 0.043 0.038 0.039 0.067 0.064 0.057\nMeta-Corr 0.245 -0.258 0.182 -0.524 -0.720 -0.424 -0.042 0.463 -0.03 0.238 0.169 0.212\n13B Human-Corr 0.153 0.165 0.123 0.180 0.209 0.162 0.187 0.179 0.167 0.234 0.266 0.192\nMeta-Corr -0.287 -0.425 -0.182 -0.580 -0.495 -0.424 -0.455 -0.233 -0.303 -0.049 -0.406 0.000\n70B Human-Corr 0.234 0.254 0.186 0.357 0.395 0.319 0.155 0.161 0.134 0.248 0.285 0.200\nMeta-Corr -0.420 -0.407 -0.273 -0.811 -0.690 -0.667 -0.322 -0.319 -0.182 -0.238 -0.123 -0.182\nTable 19: Results of Llama 2 models of 7B, 13B, and 70B RTS correlations. Light gray values are insignificant\n(p-value ≥0.05). Human-Corr reports the overall correlation of LLM scores with human scores. Meta-corr shows\nthe meta-correlation.\nID Coh Con Flu Rel\nM8 0.58 0.09 0.13 0.51\nM9 0.65 0.15 0.34 0.54\nM10 0.58 0.22 0.20 0.54\nM11 0.59 0.34 0.40 0.49\nM12 0.65 0.03 0.14 0.54\nM13 0.62 0.07 0.14 0.51\nM14 0.65 0.09 0.18 0.54\nM15 0.60 0.04 0.13 0.53\nM17 0.58 0.05 0.08 0.52\nM20 0.48 0.29 0.24 0.49\nM22 0.58 0.02 0.14 0.54\nM23 0.58 0.05 0.12 0.54\nTable 20: The standard deviation of human annotations\nacross different summarization systems and evaluation\ndimensions.\nE Challenging Pairs\nTo count the total correct pairs, we only evaluate\nthe challenging pairs, which consist of summariza-\ntion systems of consecutive performances accord-\ning to average human scores across all dimensions.\nThus, each pair contains 2 summarization systems\nwith the smallest difference in terms of average\nperformance.\nFor instance, as shown in Table 21, M22 has\nthe best average human score of 4.57, followed by\nM23 of 4.55, then M17 of 4.52. We thus compare\nmodel pairs of “M22-M23” and “M23-M17”. The\nfull challenge set is shown in Table 22.\nFor RTS, MCQ, and all other baseline metrics,\nID Model Name Coh Con Flu Rel Avg\nM22 BART 4.18 4.94 4.90 4.25 4.57\nM23 Pegasus (C4) 4.16 4.91 4.88 4.26 4.55\nM17 T5 4.00 4.93 4.93 4.23 4.52\nM12 Unified-ext-abs 3.60 4.96 4.85 3.85 4.32\nM13 ROUGESal 3.44 4.82 4.86 3.83 4.24\nM15 Closed book decoder 3.35 4.95 4.80 3.67 4.19\nM14 Multi-task (Ent + QG) 3.20 4.90 4.74 3.63 4.12\nM8 Pointer Generator 3.29 4.65 4.79 3.55 4.07\nM9 Fast-abs-rl 2.38 4.67 4.50 3.52 3.77\nM10 Bottom-Up 2.73 4.25 4.42 3.38 3.70\nM20 GPT-2 (zero-shot) 3.63 3.40 3.97 3.30 3.58\nM11 Improve-abs 2.28 3.27 3.65 3.15 3.09\nTable 21: The average human evaluation scores of vari-\nous abstractive summarization models reported by Fab-\nbri et al. (2021). We calculate the average (Avg) score\nof the reported coherence (Coh), consistency (Con), flu-\nency (Flu), and relevance (Rel) scores. Rows are sorted\naccording to the Avg column values in descending or-\nder.\nwe simply need to compare the evaluated values\nacross all systems, and each metric only needs to\nevaluate a total of 1200 summaries. However, for\nH2H, we need to evaluate a total of 6,600 summary\npairs for the full standard set, and each pair needs\nto be evaluated twice with different summary posi-\ntions (see § 3.1), resulting in a total of 13,200 LLM\nevaluations. Due to a limited budget, we thus only\ncompare a challenge set of 11 pairs, reducing the\ntotal required LLM evaluations to 2,200.\n4230\nCoherence Consistency Fluency Relevance\nModel A Model B LLM Human LLM Human LLM Human LLM Human\nM22 M23 65.5 53.5 ✓ 55.75 52.5 ✓ 61 49.5 × 58.75 49.5 ×\nM23 M17 48.25 52.5 × 47 49 ✓ 49 45.5 ✓ 45 52 ×\nM17 M12 44 66.5 × 43.25 48.5 ✓ 40.5 54.5 × 49.25 72.5 ×\nM12 M13 58 51 ✓ 56.5 54.5 ✓ 56.75 50 × 58 45 ×\nM13 M15 45.5 49.5 ✓ 52 46 × 48.75 52 × 51.25 60.5 ✓\nM15 M14 57 54 ✓ 55 53.5 ✓ 56.5 52 ✓ 54 57 ✓\nM14 M8 49.25 44 ✓ 50 54.5 × 47 46.5 ✓ 49.25 53.5 ×\nM8 M9 77.5 82 ✓ 78.5 53 ✓ 80.5 63.5 ✓ 76 54 ✓\nM9 M10 45 36 ✓ 41.5 58 × 46 44.5 ✓ 41.5 56 ×\nM10 M20 58.25 24 × 61.75 64 ✓ 63.75 61.5 ✓ 61.5 54.5 ✓\nM20 M11 56.5 82 ✓ 50 53 × 51 58.5 ✓ 50 53 ×\n#CP 8 7 7 4\nTable 22: #CP calculation for the ChatGPT-H2H metric of Model A over Model B. The numerical values in the\nmiddle section columns are aggregated scores for Model A. We omit the value for Model B, which is simply “100 -\naggregated scores for Model A”. We use “✓” to indicate both LLM and humans prefer the same model, and “×”\notherwise. The model pairs are sorted in descending order according to the average human scores for each model.\nCoherence Consistency Fluency Relevance\nID Spear. Pear. Kend. Spear. Pear. Kend. Spear. Pear. Kend. Spear. Pear. Kend.\nM8 0.420 0.383 0.323 0.229 0.273 0.209 0.274 0.245 0.236 0.519 0.509 0.438\nM9 0.174 0.243 0.142 0.119 0.265 0.103 0.256 0.258 0.219 0.254 0.307 0.200\nM10 0.365 0.415 0.292 0.305 0.452 0.251 0.258 0.288 0.223 0.367 0.378 0.284\nM11 0.378 0.420 0.320 0.404 0.488 0.335 0.227 0.288 0.182 0.501 0.511 0.394\nM12 0.208 0.237 0.160 0.087 0.020 0.082 0.086 0.107 0.071 0.438 0.451 0.354\nM13 0.455 0.473 0.359 0.178 0.037 0.167 0.063 0.151 0.055 0.403 0.403 0.329\nM14 0.433 0.467 0.355 0.114 0.187 0.105 -0.007 0.055 -0.005 0.421 0.435 0.336\nM15 0.372 0.385 0.279 0.189 0.316 0.177 0.100 0.087 0.085 0.252 0.288 0.193\nM17 0.291 0.320 0.233 -0.086 -0.061 -0.084 0.017 0.046 0.015 0.204 0.199 0.175\nM20 0.382 0.394 0.310 0.528 0.501 0.430 0.367 0.349 0.307 0.292 0.278 0.237\nM22 0.215 0.293 0.162 -0.072 -0.052 -0.070 -0.114 -0.131 -0.101 0.201 0.300 0.172\nM23 0.392 0.427 0.320 0.003 0.305 0.002 -0.078 -0.022 -0.069 0.223 0.324 0.184\nTable 23: Spearman (Spear.) correlations, Pearson (Pear.) correlations, and Kendall’s Tau (Kend.) between\nChatGPT-RTSand human scores on the 100 summaries for each model. Values in light gray color are insignificant\n(p-value ≥0.05).\nCoherence Consistency Fluency Relevance\nID Spear. Pear. Kend. Spear. Pear. Kend. Spear. Pear. Kend. Spear. Pear. Kend.\nM8 0.289 0.310 0.236 0.235 0.362 0.226 0.348 0.348 0.321 0.349 0.419 0.302\nM9 0.170 0.170 0.148 0.211 0.321 0.200 0.198 0.299 0.174 0.318 0.356 0.276\nM10 0.352 0.314 0.293 0.138 0.273 0.125 0.155 0.210 0.138 0.420 0.427 0.362\nM11 0.285 0.312 0.250 0.380 0.370 0.317 0.200 0.218 0.163 0.397 0.428 0.334\nM12 0.306 0.304 0.258 -0.025 -0.059 -0.024 0.256 0.306 0.239 0.283 0.287 0.246\nM13 0.425 0.425 0.351 0.471 0.312 0.457 0.199 0.214 0.186 0.435 0.402 0.375\nM14 0.490 0.472 0.422 0.112 0.157 0.110 0.084 0.143 0.078 0.326 0.329 0.284\nM15 0.317 0.298 0.250 0.061 0.513 0.060 0.055 0.025 0.050 0.433 0.420 0.378\nM17 0.250 0.255 0.215 -0.106 -0.081 -0.105 -0.011 0.024 -0.011 0.293 0.285 0.260\nM20 0.463 0.450 0.381 0.455 0.442 0.371 0.494 0.450 0.404 0.326 0.334 0.264\nM22 0.211 0.173 0.182 -0.096 -0.080 -0.095 -0.092 -0.056 -0.087 0.352 0.371 0.313\nM23 0.218 0.209 0.189 0.107 0.448 0.104 -0.275 -0.269 -0.261 0.147 0.187 0.129\nTable 24: Spearman (Spear.) correlations, Pearson (Pear.) correlations, and Kendall’s Tau (Kend.) between\nChatGPT-MCQ and human scores on the 100 summaries for each model. Values in light gray color are insignificant\n(p-value ≥0.05).\nF Average ChatGPT scores\nWe present the average ChatGPT evaluation scores\nfor each model across all dimensions in Table 25.\nGenerally, the same trend holds for the individual\nsystems, that ChatGPT score systems much more\nconservatively with RTS, and becomes more opti-\nmistic with MCQ.\n4231\nCoherence Consistency Fluency Relevance\nID Chat-RTS Chat-MCQ GPT-4 human Chat-RTS Chat-MCQ GPT-4 human Chat-RTS Chat-MCQ GPT-4 human Chat-RTS Chat-MCQ GPT-4 human\nM8 2.43 3.70 4.56 3.29 4.21 4.70 4.77 4.65 2.51 3.76 4.61 4.79 3.56 4.33 4.71 3.55M9 1.80 3.51 4.41 2.38 3.93 4.69 4.94 4.67 2.16 3.45 4.15 4.50 3.51 4.33 4.82 3.52M10 2.05 3.49 4.05 2.73 3.77 4.52 4.60 4.25 2.23 3.43 3.92 4.42 3.45 4.25 4.62 3.38M11 1.70 2.63 2.93 2.28 2.36 4.11 3.72 3.27 1.71 2.66 2.77 3.65 2.92 4.06 3.87 3.15M12 2.75 3.92 4.75 3.60 4.46 4.75 5.00 4.96 2.59 3.84 4.72 4.85 3.89 4.40 4.95 3.85M13 2.91 3.99 4.76 3.44 4.40 4.73 4.89 4.82 2.69 3.93 4.69 4.86 3.90 4.41 4.85 3.83M14 2.38 3.87 4.54 3.20 4.25 4.70 4.99 4.90 2.42 3.83 4.53 4.74 3.67 4.32 4.84 3.63M15 2.65 3.81 4.58 3.35 4.33 4.75 4.90 4.95 2.60 3.84 4.61 4.80 3.78 4.32 4.82 3.67M17 3.05 4.11 4.89 4.00 4.84 4.87 4.97 4.93 3.29 4.08 4.79 4.93 4.30 4.43 4.97 4.23M20 2.00 2.95 2.99 3.63 2.66 3.57 3.51 3.40 2.22 3.02 3.05 3.97 2.73 3.78 3.47 3.30M22 3.54 4.09 4.92 4.18 4.83 4.82 4.97 4.94 3.57 4.06 4.94 4.90 4.41 4.43 4.95 4.25M23 3.00 4.14 4.88 4.16 4.74 4.84 4.94 4.91 3.08 4.05 4.76 4.88 4.10 4.45 4.96 4.26\navg 2.52 3.68 4.35 3.35 4.06 4.59 4.68 4.55 2.59 3.66 4.29 4.61 3.68 4.29 4.65 3.72\nTable 25: Average scores for ChatGPT using RTS (Chat-RTS) and MCQ (Chat-MCQ), GPT-4 using RTS (GPT-4),\nas well as the human scores.\nDimension Generation\nConsistencyThe summary is inconsistent with the article\nas itomits important detailsandrepeats a\nphrase multiple times, resulting in a score of\n1.\nConsistencyThe summary includes some relevant informa-\ntion butlacks coherenceandomits important\ndetails, resulting in a score of 2.5 out of 5 for\nconsistency.\nFluency The summary is well-written and grammat-\nically correct, butlacks important details\nfrom the article such as the potential link\nbetween antibiotics and weight gain, and\nthe warnings about over-prescription of an-\ntibiotics and the development of superbugs.\nScore: 3/5.\nFluency The summarylacks coherenceanddoes not\nprovide a clear understanding of the article,\nresulting in a score of 1 for disfluency.\nTable 26: Examples of wrong reasons generated during\nRTS by ChatGPT that do not correspond to the evalu-\nated dimension. Bolded: reasons that don’t match the\nevaluated dimension.\nG Incorrect Reasons in RTS\nWe illustrate some of the observed incorrect rea-\nsons generated with the RTS method by ChatGPT\nthat do not correspond to the evaluated dimension\nin Table 26. We further provide incorrect reasons\ngenerated by GPT-4 in Table 27.\nWe haven’t compiled the exact number of mis-\nmatched reasons for either LLM, but we do ob-\nserve a non-trivial fraction of the evaluations con-\ntaining unrelated-to-dimension reasoning during\npreliminary verification. For instance, on the con-\nsistency dimension, ChatGPT-RTS has 42% eval-\nuations containing dimension-irrelevant reasoning\nfor the M11 (avg. score of 2.36) model, and 10%\nfor the M17 model (avg. score of 4.84).\nDimension Generation\nCoherenceThe summary is somewhat coherent butin-\ncludes incorrect information, such as stat-\ning that Merson restarted his row with\nBurnley instead of Andros Townsend, and\ndoes not flow smoothly;therefore, I would\ngive it a score of 3.\nConsistencyThe summary is consistent with the article as\nit mentions the helicopter airlifting patients to\nthe hospital, but itleaves out important de-\ntails such as the number of people involved\nand their injuries.Final score: 3.\nConsistencyThe summary accurately reflects the main\npoints of the article,although it could have\nmentioned Falcao’s unhappiness in France.\nFinal score: 4.\nFluency The summary is coherent and grammatically\ncorrect, but itdoes not provide enough infor-\nmation about the incident. Final score: 3.\nFluency The summary isonly one sentence and do\nnot provide enough information about the\narticle’s content.Final score: 1.\nTable 27: Examples of wrong reasons generated during\nRTS by GPT-4 that don’t correspond to the evaluated\ndimension. Bolded: reasons that don’t match the evalu-\nated dimension.\nH GPT-4 Evaluator\nWe also look into the reasoning of GPT-4 and dis-\ncover that it makes the same mistakes as ChatGPT\nby penalizing the summary for reasons unrelated\nto the evaluated dimension (see Table 27).\nAnother major difference is that GPT-4 tends\nto give overly generous scores. In one exception-\nally extreme case, GPT-4 gives full scores for all\ngenerations by M12 in terms of consistency. Ta-\nble 25 also shows the much higher average scores\ngiven by GPT-4 across all dimensions than those\nof ChatGPT-RTS.\n4232\nCoherence Consistency Fluency Relevance\nID Spear. Pear. Kend. Spear. Pear. Kend. Spear. Pear. Kend. Spear. Pear. Kend.\nM8 0.429 0.446 0.362 0.449 0.597 0.433 0.412 0.409 0.385 0.425 0.489 0.365\nM9 0.288 0.346 0.243 0.349 0.350 0.331 0.462 0.465 0.407 0.413 0.490 0.358\nM10 0.495 0.480 0.403 0.518 0.683 0.469 0.406 0.579 0.360 0.510 0.537 0.429\nM11 0.584 0.572 0.474 0.529 0.536 0.439 0.448 0.436 0.358 0.500 0.519 0.405\nM12 0.245 0.360 0.209 - - - 0.387 0.506 0.372 0.169 0.106 0.149\nM13 0.271 0.271 0.230 0.535 0.444 0.518 0.093 0.337 0.089 0.237 0.337 0.206\nM14 0.263 0.312 0.222 -0.040 -0.026 -0.040 0.311 0.432 0.291 0.277 0.315 0.241\nM15 0.403 0.418 0.342 0.158 0.504 0.155 0.240 0.242 0.225 0.307 0.402 0.267\nM17 0.285 0.240 0.246 0.390 0.748 0.385 0.091 0.131 0.088 0.210 0.253 0.185\nM20 0.427 0.418 0.334 0.378 0.362 0.313 0.482 0.468 0.402 0.479 0.491 0.391\nM22 0.026 0.004 0.023 0.346 0.608 0.343 0.248 0.141 0.242 0.143 0.084 0.126\nM23 0.320 0.330 0.283 0.360 0.168 0.354 0.267 0.136 0.251 0.005 0.003 0.005\nTable 28: Spearman (Spear.) correlations, Pearson (Pear.) correlations, and Kendall’s Tau (Kend.) between GPT-4\nRTS and human scores on the 100 summaries for each model. Values in light gray color are insignificant (p-value\n≥0.05). Note that for the consistency of M12, correlations cannot be calculated because GPT-4 gives 5 scores to all\nexamples.\nCoherence Consistency Fluency Relevance\nID Spear. Pear. Kend. Spear. Pear. Kend. Spear. Pear. Kend. Spear. Pear. Kend.\nM8 0.464 0.455 0.407 0.557 0.613 0.526 0.391 0.367 0.341 0.517 0.514 0.471\nM9 0.436 0.415 0.416 0.215 0.342 0.195 0.578 0.571 0.517 0.450 0.485 0.412\nM10 0.374 0.345 0.340 0.300 0.297 0.270 0.631 0.585 0.551 0.506 0.507 0.459\nM11 0.326 0.428 0.307 0.437 0.421 0.393 0.354 0.509 0.303 0.467 0.486 0.419\nM12 0.500 0.469 0.444 0.446 0.486 0.424 0.273 0.320 0.241 0.588 0.591 0.542\nM13 0.462 0.450 0.408 0.470 0.510 0.445 0.325 0.345 0.282 0.515 0.498 0.475\nM14 0.524 0.460 0.463 0.325 0.293 0.307 0.216 0.257 0.194 0.501 0.476 0.455\nM15 0.534 0.499 0.470 0.266 0.326 0.251 0.383 0.369 0.337 0.584 0.550 0.535\nM17 0.312 0.294 0.274 -0.016 -0.008 -0.015 0.251 0.261 0.225 0.530 0.479 0.504\nM20 0.636 0.623 0.566 0.733 0.664 0.644 0.566 0.556 0.492 0.570 0.565 0.496\nM22 0.314 0.338 0.276 0.245 0.151 0.241 0.371 0.341 0.330 0.471 0.412 0.451\nM23 0.380 0.386 0.337 0.158 0.496 0.153 0.296 0.275 0.259 0.450 0.505 0.421\nTable 29: Ri, the reliability indicator calculated by the Spearman (Spear.) correlations, Pearson (Pear.) correlations,\nand Kendall’s Tau (Kend.) between ChatGPT-RTS and ChatGPT-MCQ. Values in light gray color are insignificant\n(p-value ≥0.05).\n4233",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.8089396953582764
    },
    {
      "name": "Computer science",
      "score": 0.6564662456512451
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.5798911452293396
    },
    {
      "name": "Fluency",
      "score": 0.5752569437026978
    },
    {
      "name": "Pace",
      "score": 0.5601093173027039
    },
    {
      "name": "Metric (unit)",
      "score": 0.5334449410438538
    },
    {
      "name": "Reliability (semiconductor)",
      "score": 0.4914926290512085
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.44894927740097046
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43593788146972656
    },
    {
      "name": "Natural language processing",
      "score": 0.38374078273773193
    },
    {
      "name": "Machine learning",
      "score": 0.3481943607330322
    },
    {
      "name": "Cognitive psychology",
      "score": 0.33042457699775696
    },
    {
      "name": "Psychology",
      "score": 0.2369897961616516
    },
    {
      "name": "Mathematics education",
      "score": 0.13432824611663818
    },
    {
      "name": "Operations management",
      "score": 0.08302649855613708
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ]
}