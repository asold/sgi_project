{
  "title": "Retrieval-Augmented Chain-of-Thought in Semi-structured Domains",
  "url": "https://openalex.org/W4389519273",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5039688324",
      "name": "Vaibhav Mavi",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A5062813195",
      "name": "Abulhair Saparov",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A5100351992",
      "name": "Chen Zhao",
      "affiliations": [
        "New York University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385573970",
    "https://openalex.org/W4386185625",
    "https://openalex.org/W3099950029",
    "https://openalex.org/W4382202595",
    "https://openalex.org/W4361866125",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4287019748",
    "https://openalex.org/W4300427681",
    "https://openalex.org/W4385611862",
    "https://openalex.org/W4391876619",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W3117696238",
    "https://openalex.org/W4205508242",
    "https://openalex.org/W4386504863",
    "https://openalex.org/W4382998379",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W4385573414",
    "https://openalex.org/W4385570645",
    "https://openalex.org/W4381855801",
    "https://openalex.org/W4385571421",
    "https://openalex.org/W4385573607",
    "https://openalex.org/W4307073337",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4378465439",
    "https://openalex.org/W4320558759",
    "https://openalex.org/W4318586794",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4321009772",
    "https://openalex.org/W4287779512",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W4386942555",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3175097602"
  ],
  "abstract": "Applying existing question answering (QA) systems to specialized domains like law and finance presents challenges that necessitate domain expertise. Although large language models (LLMs) have shown impressive language comprehension and in-context learning capabilities, their inability to handle very long inputs/contexts is well known. Tasks specific to these domains need significant background knowledge, leading to contexts that can often exceed the maximum length that existing LLMs can process. This study explores leveraging the semi-structured nature of legal and financial data to efficiently retrieve relevant context, enabling the use of LLMs for domain-specialized QA. The resulting system outperforms contemporary models and also provides useful explanations for the answers, encouraging the integration of LLMs into legal and financial NLP systems for future research.",
  "full_text": "Proceedings of the Natural Legal Language Processing Workshop 2023, pages 178–191\nDecember 7, 2023 ©2023 Association for Computational Linguistics\nRetrieval-Augmented Chain-of-Thought in Semi-structured Domains\nVaibhav Mavi\nNew York University\nvm2241@nyu.edu\nAbulhair Saparov\nNew York University\nas17582@nyu.edu\nChen Zhao\nNew York University\ncz1285@nyu.edu\nAbstract\nApplying existing question answering (QA)\nsystems to specialized domains like law and\nfinance presents challenges that necessitate do-\nmain expertise. Although large language mod-\nels (LLMs) have shown impressive language\ncomprehension and in-context learning capa-\nbilities, their inability to handle very long in-\nputs/contexts is well known. Tasks specific\nto these domains need significant background\nknowledge, leading to contexts that can often\nexceed the maximum length that existing LLMs\ncan process. This study explores leveraging the\nsemi-structured nature of legal and financial\ndata to efficiently retrieve relevant context, en-\nabling the use of LLMs for domain-specialized\nQA. The resulting system outperforms contem-\nporary models and also provides useful expla-\nnations for the answers, encouraging the inte-\ngration of LLMs into legal and financial NLP\nsystems for future research.\n1 Introduction\nBuilding NLP systems for answering questions in\nthe legal and financial domains could save time\nand resources, ensure compliance, and enhance\nthe overall accuracy and effectiveness of legal and\nfinancial operations (Nay et al., 2023; Yang et al.,\n2023). Applying QA systems to such domains\nposes unique challenges. These domains feature\ncomplex jargon, nuanced phrasing, and contextual\ndependencies that require specialized knowledge\nand expertise (Katz et al., 2023; Wu et al., 2023a).\nA system tailored to these domains should be able\nto efficiently process and analyze large volumes of\nlegal, financial, or regulatory documents, extracting\nrelevant insights and answering targeted queries.\nLarge language models (LLMs) have shown im-\npressive performance on several NLP tasks (Zhao\net al., 2023). de Padua et al. (2023) show that\nLLMs trained on large amounts of data are able to\nobtain the necessary domain knowledge through in-\ncontext learning (ICL) (Brown et al., 2020). How-\never, a major limitation of LLMs is the limit on the\ninput size. There are many attempts to address this\nlimitation (Press et al., 2022; Haviv et al., 2022;\nZhu et al., 2023) and multiple transformer mod-\nels are able to handle longer contexts (OpenAI,\n2023; Rozière et al., 2023; Dai et al., 2019; Sun\net al., 2023b). However, (Liu et al., 2023) show\nthat model performance on certain parts of the in-\nput decreases with input size. Further, the cost and\nlatency of LLMs increases with the input size.\nThe context required for legal and financial ques-\ntions is often large and may not fit within the token\nlimit, requiring more efficient retrieval. Financial\nand legal documents are often semi-structured. For\nexample, Figure 1 shows a section from the US\nInternal Revenue Code. The text is organized into\nsubsections, paragraphs and bullet points, which\nwe leverage for better information retrieval. Fur-\nther, financial reports often contain quantities in\ntabular format. We exploit these structures in a\nprompting approach that incorporates retrieval to\nworkaround the context token limit.\nWe evaluate the proposed method on two\ndatasets: FinQA (Chen et al., 2021) and SARA\n(Holzenberger et al., 2020). These datasets feature\ncomplex questions which require multiple steps of\nreasoning and arithmetic computations, which is\nchallenging for language systems. We adopt chain-\nof-thought (CoT) prompting (Wei et al., 2023) for\ngenerating the answers since it is well suited for\nperforming reasoning in a step-by-step manner. A\nchain of thought is a coherent sequence of reason-\ning steps that lead to the correct answer in a step-\nby-step manner. Providing examples of question-\nanswer pairs along with their CoTs prepended to\nthe test question causes GPT-3 to likewise output\na CoT along with the answer for the test question,\nand improve its overall reasoning accuracy. CoT\nprompting is especially useful for complex tasks\nwhich require multiple steps of reasoning over the\ngiven input.\n178\nFigure 1: An example of a statute from US Internal Revenue Code (left) and the subsection name assigned to each\nsentence after parsing as described in section 4.1.1 (right).\nThe results demonstrate that this simple and effi-\ncient approach outperforms state-of-the-art models\nin these domains. Training LLMs on financial and\nlegal data may not be feasible as they may contain\nsensitive information. The use of ICL circumvents\nthe problem and avoids expensive and tedious pro-\ncess of data collection and training. This makes the\nproposed approach a practical solution in scenar-\nios where labeled data is limited or expensive to\nobtain. Additionally, CoT prompting offers the ad-\nvantage of generating explanations and facilitating\ninterpretability in critical domains where it is a key\nobstacle for the adoption of AI systems (Danilevsky\net al., 2020). However, a major drawback of the\napproach is that it is task-specific. In particular,\nthe retrieval relies on the structure within in the\ndata and needs to be adapted to data from different\nsources1.\nWe hope our work fosters research on coupling\nLLMs with retrieval in domains such as finance and\nlaw, where the ability to extract insights and answer\nquestions about vast amounts of domain-specific\ndata has many practical applications.\n2 Related work\nPrevious work has proposed training specialized\nLLMs for financial and legal domains (Wu et al.,\n2023a; Huang et al., 2023; Nguyen, 2023; Yang\net al., 2023). However, doing so requires a large\namount of data, compute and cost.\nSun et al. (2023a) evaluate GPT-2 on FinQA\n(Chen et al., 2021). Blair-Stanek et al. (2023) eval-\nuate GPT-3 with different prompting techniques on\nSARA where the context includes all the sections\nfrom the statutes. Since the input size of GPT-3\nis limited, the prompts only included a subset of\nsections, which may not contain the required infor-\nmation. Further, fewer in-context examples were\nused for CoT as compared to few-shot learning. Li\net al. (2023) and Wu et al. 2023b observe better\n1The code for this work is publicly available at\ngithub.com/vaibhavg152/Retrieval-Augmented-\nChain-of-Thought-in-Semi-structured-Domains\nperformance with more in-context examples.\nNay et al. (2023) test various GPT models with\nICL to answer multiple choice questions over tax\nlaws. A retriever augmented setting is tested where\na dense passage retriver, GTR (Ni et al., 2021),\nretrieves the top 4 relevant sections to the questions.\nSince entire sections are passed to the LLMs, the\ntext has to be truncated.\nThis study extends past work by complementing\nLLMs with a retriever that extracts the relevant text\nfrom within the statutes, allowing for larger con-\ntexts and more in-context examples in the prompt.\n3 Data\nWe use two datasets containing questions that in-\nvolve multi-step logical and arithmetic reasoning\nfrom the legal and financial domains respectively.\n3.1 SARA\nStAtutory Reasoning Assessment dataset (SARA)\n(Holzenberger et al., 2020) is designed to evaluate\nstatutory reasoning over a set of sections extracted\nfrom the US Internal Revenue Code (IRC). For\neach of the subsections contained in the selected\nsections, there are two hand-written case scenar-\nios. Correctly solving these cases requires multiple\nsteps of arithmetic as well as logical reasoning. For\ninstance, some cases require computing the amount\nof tax owed according to a given section, only if the\nsection applies to the given case. Thus, this dataset\nserves as a challenging task for an AI system, re-\nquiring domain expertise and reasoning abilities.\n3.2 FinQA\nFinQA (Chen et al., 2021) is a financial QA dataset.\nIt comprises of 8,281 examples where each ques-\ntion is accompanied by a financial report, contain-\ning text as well as a table. The report contains\nthe necessary information to correctly answer the\nquestion. FinQA poses many challenges for a QA\nsystem. The questions require retrieval, arithmetic\nand logical reasoning simultaneously over tables\n179\nAlice is married under section 7703 \nfor 2017. Taxable income for the year \n2017 is $103272.\nAlice has to pay $24543 in taxes for \n2017 under section 1(a)(iii). True or \nFalse?\nRetriever\n§63. Taxable income defined\n(a) In general\nExcept as provided in subsection \n(b) Individuals who do not \nitemize...\nIn the case of an individual who ...\n    (1) the standard deduction, and\n    (2) the deduction for \npersonal  ...\n§1. Tax imposed\n(a) Married individuals filing ....\nThere is hereby imposed on the ....\na tax determined in accordance ....\n    (iii) $20,165, plus 31% of .... \n    \nQuestion\nAlice is married under section ....\nAlice and her spouse have to pay ....\nTrue or False?\nAnswer\n§151.Allowance of deduct...\n    (d) Exemption amount\n        (3) Phaseout\n            (B) Applicable \npercentage\n            For purposes of ... \nQuestion\nAlice's income in 2015 is ...\nTrue or False?\nAnswer\nSection 151(d)(3)(B) states ...\nTrue\nFor the following questions,...\n§151. Allowance of deductions...\n    (d) Exemption amount\n    For purposes of this section-\n        ...\nQuestion\nAlice's income in 2015 is ...\nTrue or False?\nAnswer\nSection 151(d)(3)(B) states that ...\nTrue\n...\n...\n§1. Tax imposed\n(a) Married individuals filing ....\n    (iii) $20,165, plus 31% of .... \n    \nQuestion\nAlice is married under section ....\nTrue or False?\nAnswer\nLLM\nQuestion\nContext\nIn-context\n examples\nCoT prompt\nFigure 2: An overview of the proposed system on a sample input from SARA. The retriever extracts the relevant\ninformation from the context and combines it with the question. In-context examples are appended with the retrieval\noutput to construct a prompt which is used for querying LLMs to generate an answer along the chain-of-thought.\nand text. The questions also require an understand-\ning of financial jargon. Finally, multiple reasoning\nsteps are required to derive the answer.\n4 Methodology\nFigure 2 shows an overview of our proposed ap-\nproach, which consists of two main components:\nretrieval and answering. The retrieval step involves\nfiltering paragraphs from text and rows from ta-\nbles that are relevant to the question. The retrieved\ninformation is then passed to the answering model.\n4.1 Retrieval\nRetrieval is essential for fully leveraging ICL and\nCoT reasoning abilities of LLMs. It can help to\nprevent the required context from exceeding the\ntoken limit, while also allowing the prompt to in-\nclude enough in-context examples along with their\nCoT explanations. It can also help to reduce the\ntime and cost of inference.\nWe propose to leverage the structure present in\nthe data to retrieve the relevant context from the\nlegal statutes and financial reports. This structure\nis specific to the data source and the retriever needs\nto be designed accordingly. In our analysis, we\nexplore datasets with two different sources: SARA,\nwhere a template-based algorithm can be used for\neffective retrieval; and FinQA, where a more so-\nphisticated pre-trained retrieval model is required.\n4.1.1 SARA\nAs shown in Figure 1, the statutes in SARA are\norganized in a hierarchical structure with sections,\nsub-sections, paragraphs, and bullets. This hier-\narchical structure offers valuable information for\nefficient and accurate retrieval.\nFigure 3 in the Appendix shows an example of\na question from SARA. The questions contain ref-\nerences to the specific sub-sections they pertain to.\nFirstly, a simple regular expression-based extrac-\ntor scans the question text to identify the relevant\nsection name.\nNext, a rule-based statute parser extracts the\nmentioned sub-section. The parser reads each sen-\ntence in the given statutes and assigns it to the most\nspecific sub-section to which the sentence belongs.\nFigure 1 shows an example of a parsed statute sec-\ntion. We explore three retrieval strategies:\n1. mentioned-only: The retriever returns all\nthe sentences that are assigned to sub-sections\ncontaining the queried sub-section as a pre-\nfix. For Figure 1, a query for sub-section\n7703(a)(1) will result in sentences assigned\nto s7703, s7703(a) and s7703(a)(1).\n2. entire-section: Retriever returns the en-\ntire sub-section. In Figure 1, a query for sub-\nsection 7703(a)(1) will result in sentences\nassigned to s7703, s7703(a), s7703(a)(1), as\nwell as s7703(a)(2).\n3. references: Retriever returns sub-sections\nmentioned in the question along with those that\nare referenced in these retrieved subsections2.\n4.1.2 FinQA\nThe absence of a hierarchical structure in FinQA\nreports makes it impractical to adopt a rule-based\napproach for retrieval. Chen et al. (2021) convert\nthe tables into text and then use BERT for retrieving\nrelevant sentences from the report.\n2It is intuitive to consider an approach that recursively\nretrieves text from sections mentioned in the sections retrieved\nin the previous step. However, this recursive approach proves\nto be impractical as it generates excessively large contexts.\n180\nHowever, using templates to convert tables into\ntext leads to very long contexts. These templates\ncan also introduce grammatical and logical errors,\nleading to a loss in the performance of the answer-\ning module. Thus, we use a tabular format during\nthe answering step in order to exploit the structure\n(see Figure 5 in Appendix).\nWe also evaluate the system with gold retrieved\nsentences (GPT3-Gold, LLaMA2-70B-Gold).\n4.2 Answering\nIn this study, we test GPT-3 (text-davinci-003)\n(Brown et al., 2020) and LLaMA-2 (Touvron et al.,\n2023) to answer the queries. We experiment with\ndifferent prompting techniques, namely zero-shot,\nfew-shot and chain-of-thought prompting. CoT\nprompting has been shown to improve the ICL abil-\nities of sufficiently large LLMs (Zhao et al., 2023)\nand is especially useful for tasks that require multi-\nple steps of reasoning.\nIn the zero-shot setting, the model is given the\nretrieved context and the question and is expected\nto output just the answer without any explanation.\nIn the few-shot setting, we further include in-\ncontext examples of question-answer pairs (8 ex-\namples for SARA and 12 examples for FinQA3).\nIn the CoT setting, we use the same in-context\nexamples as used for the few-shot setting but each\nexample also includes a CoT explanation. These\nexplanations are manually written for each exam-\nple. The model is expected to generate the answer\nalong with the CoT explanations for the test cases.\nFor all questions in a dataset, we use the same\nprompt containing the same in-context examples\nwhich are selected using prompt tuning as de-\nscribed in Appendix section A.1.\nFigures 4 and 5 in the Appendix show the CoT\nprompts used for SARA and FinQA respectively.\n5 Experimental setup\n5.1 Evaluation\nFor SARA, the task is formulated as an entailment\ntask and is evaluated as a binary classification task.\nFor FinQA, Chen et al. (2021) proposeprogram\naccuracy where the model is expected to generate\na ‘program’ along with the answer. A program is\na sequence of mathematical operations that leads\nto the final answer. The evaluation thus compares\n3Tabular data in FinQA leads to shorter retrieved context\nand allows more examples per prompt.\nModel name Accuracy\nMajority baseline [11] 50.0 ± 8.22\nFeed-forward [11] 54.0 ± 8.20\nLegal-BERT [11] 49.0 ± 8.22\nBERT [12] 59.0 ± 8.09\nGPT-3 (0-shot) [3] 71.0 ± 7.46\nGPT-3 (CoT) [3] 57.0 ± 8.14\nGPT-3 (dynamic) [3] 60.0 ± 8.06\nGPT-3 + Ret 81.6 ± 4.22\nLLaMA2-7B + Ret 53.5 ± 5.43\nLLaMA2-7B_chat + Ret 54.4 ± 5.43\nLLaMA2-13B + Ret 57.5 ± 5.39\nLLaMA2-13B_chat + Ret 66.7 ± 5.13\nLLaMA2-70B + Ret 71.1 ± 4.94\nTable 1: Comparison of proposed system’s performance\non SARA with the existing baselines. The top section\nshows non-LLM based methods. The middle section\nshows the evaluation results from Blair-Stanek et al.\n(2023). The bottom section shows the results of our\nproposed system with ‘Ret’ representing the proposed\nretrieval. Results are shown with the 90% confidence\ninterval.\nthe output program with the gold standard program\nand checks if the two evaluate to the same answer.\nWe also measure the answer accuracyby ig-\nnoring errors only in units, prefix, suffix, precision\ndigits or rounding errors.\n5.2 Comparison with existing methods\nTables 1 and 2 show results on SARA and FinQA\nrespectively4. Descriptions of the baselines are\nprovided in Appendix Section B.\nOn SARA, both GPT-3 and LLaMA2-70B sur-\npass the existing methods by a significant mar-\ngin. We also observe the expected trend of the\nperformance improving with the increase in the\nmodel size, with GPT-3 (175B) performing signifi-\ncantly better with LLaMA-2 models (Kaplan et al.,\n2020)5.\nOn the other hand, the performance on FinQA\nwith GPT-3 is comparable with baselines in terms\nof program accuracy but lags behind in answer\naccuracy. We believe this behavior is due to arith-\nmetic errors made by LLMs (Qian et al., 2023), re-\nsulting in cases with correct programs but incorrect\nanswers. Our approach with LLaMA2-13B/70B\nand GPT-3 outperforms general crowd workers\n4For testing on FinQA, we randomly sample 200 examples\nfrom the public test set due to the high cost of LLM queries.\n5Although Touvron et al. (2023) report similar perfor-\nmance of LLaMA-2 to GPT-3 on benchmark datasets, the\ntask addressed here is domain-specific and requires more com-\nplex mathematical and logical reasoning than the benchmarks\nthey use for evaluation.\n181\nModel Program acc Answer acc\nLongformer [2] 21.90±2.01 20.48±1.96\nELASTIC [35] 57.54±2.40 62.16±2.36\nDyRRen [17] 61.29±2.37 63.30±2.34\nTabT5 [1] 68.00±2.27 70.79±2.21\nAPOLLO [28] 65.60±2.31 67.99±2.27\nFinQANet-BERT [6] 58.86±2.39 61.24±2.37\nGPT-3-BERT 68.00±5.43 52.50±5.81\nLLaMA2-7B-BERT 25.50±5.07 16.50±4.32\nLLaMA2-7B_chat-BERT 34.50±5.53 14.50±4.10\nLLaMA2-13B-BERT 52.50±5.81 33.00±5.47\nLLaMA2-13B_chat-BERT 50.50±5.82 26.50±5.13\nLLaMA2-70B-BERT 60.50±5.69 51.00±5.81\nHuman non-expert [6] 48.17±2.43 50.68±2.43\nHuman expert [6] 87.49±1.61 91.16±1.38\nFinQANet-Gold [6] 68.76±2.25 70.00±2.23\nGPT-3-Gold 72.50±5.19 56.50±5.77\nLLaMA2-70B-Gold 63.00±5.62 54.50±5.79\nTable 2: Comparison with state of the art and baselines\nmethods on FinQA. Results are presented with a 90%\nconfidence interval.\nRetrieval strategy Accuracy\nentire-section 52.50 ± 12.99\nreferences 75.00 ± 11.26\nmentioned-only 77.50 ± 10.86\nTable 3: Comparison of the three retrieval strategies\nused with GPT-3 on SARA validation set.\nwho lack domain expertise in finance, whereas it\nfalls short compared to financial experts.\nThe bottom section of Table 2 highlights the\neffectiveness of GPT-3 over FinQANet (Chen et al.,\n2021) when provided with the gold retrieved results.\nHowever, LLaMA-2 shows sub-par performance.\n5.3 Ablation studies\nComparison of prompting techniques:Table 4 in\nthe Appendix shows the evaluation results for zero-\nshot, few-shot and CoT prompting. CoT prompting\nleads to significantly better results across all mod-\nels.\nComparing retrieval strategies:As outlined in\nsection 4, we test three different retrieval strategies\nfor SARA. Table 3 reveals thatmentioned-only\nand references perform significantly better than\nentire-section. The questions in SARA are\ndesigned in a way where additional context apart\nfrom the mentioned sub-sections is not required.\nThe difference in accuracy indicates the benefit\nof more targeted retrieval for model performance,\nsince over-retrieval may dilute the signal provided\nby more directly relevant context.\nCase analysisWe perform manual qualitative in-\nspection of the generated CoT explanations and\nreport the analysis in Appendix section D.2.\n6 Discussion\nThis study aims to utilize LLMs for challenging\ndomain-specific QA tasks by using ICL along\nwith retrieval techniques that leverage the semi-\nstructured nature of financial and legal data. The\nproposed approach is simple and performs well\ncompared to existing systems. It exploits ICL\nwhich avoids the costly and time-consuming pro-\ncesses of data collection and training. Since the\nproposed system produces a chain-of-thought with\neach output, it is easily interpretable and errors can\nbe identified and rectified by human supervision\n(Danilevsky et al., 2020).\nWe hope this work will encourage researchers\nto delve deeper into the analysis and develop-\nment of LLM-integrated NLP systems and retrieval-\naugmented LLMs.\n7 Limitations\nThe retrieval algorithms in our study are specifi-\ncally tailored to each dataset. Despite good reason-\ning abilities, the evaluation reveals that arithmetic\nerrors are common. Further, inference with LLMs\ncan be costly with latency higher than traditional\napproaches, making it sub-optimal for handling\nlarge volumes of data efficiently.\nThese limitations point to interesting future di-\nrections such as using arithmetic tools as plug-\nins (Schick et al., 2023) for better performance\nand more generalizable retrieval algorithms. Fur-\nther, several domain-specific LLMs can be tested\n(Huang et al., 2023; Yang et al., 2023; Wu et al.,\n2023a).\n8 Acknowledgements\nWe sincerely thank the anonymous reviewers for\ntheir valuable feedback. For the experiments with\nLLaMA2, we thank the NYU IT High Performance\nComputing resources, services, and staff expertise.\nReferences\nEwa Andrejczuk, Julian Martin Eisenschlos, Francesco\nPiccinno, Syrine Krichene, and Yasemin Altun. 2022.\nTable-to-text generation and pre-training with tabt5.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer.\nAndrew Blair-Stanek, Nils Holzenberger, and Ben-\njamin Van Durme. 2023. Can gpt-3 perform statutory\nreasoning?\n182\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nIlias Chalkidis, Manos Fergadiotis, Prodromos Malaka-\nsiotis, Nikolaos Aletras, and Ion Androutsopoulos.\n2020. LEGAL-BERT: The muppets straight out of\nlaw school. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020 , pages 2898–\n2904, Online. Association for Computational Lin-\nguistics.\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena\nShah, Iana Borova, Dylan Langdon, Reema Moussa,\nMatt Beane, Ting-Hao Huang, Bryan Routledge, and\nWilliam Yang Wang. 2021. FinQA: A dataset of nu-\nmerical reasoning over financial data. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 3697–3711, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na fixed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2978–2988, Florence, Italy. Asso-\nciation for Computational Linguistics.\nMarina Danilevsky, Kun Qian, Ranit Aharonov, Yan-\nnis Katsis, Ban Kawas, and Prithviraj Sen. 2020. A\nsurvey of the state of explainable AI for natural lan-\nguage processing. In Proceedings of the 1st Confer-\nence of the Asia-Pacific Chapter of the Association\nfor Computational Linguistics and the 10th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing, pages 447–459, Suzhou, China. Association\nfor Computational Linguistics.\nRaul Salles de Padua, Imran Qureshi, and Mustafa U.\nKarakaplan. 2023. Gpt-3 models are few-shot finan-\ncial reasoners.\nAdi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer\nLevy. 2022. Transformer language models without\npositional encodings still learn positional informa-\ntion. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2022, pages 1382–1390,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nNils Holzenberger, Andrew Blair-Stanek, and Ben-\njamin Van Durme. 2020. A dataset for statutory\nreasoning in tax law entailment and question answer-\ning.\nNils Holzenberger and Benjamin Van Durme. 2021.\nFactoring statutory reasoning as language understand-\ning challenges. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 2742–2758, Online. Association for\nComputational Linguistics.\nQuzhe Huang, Mingxu Tao, Zhenwei An, Chen Zhang,\nCong Jiang, Zhibin Chen, Zirui Wu, and Yansong\nFeng. 2023. Lawyer llama technical report.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models.\nDaniel Martin Katz, Dirk Hartung, Lauritz Gerlach,\nAbhik Jana, and Michael J. Bommarito II au2. 2023.\nNatural language processing in the legal domain.\nMukai Li, Shansan Gong, Jiangtao Feng, Yiheng Xu,\nJun Zhang, Zhiyong Wu, and Lingpeng Kong. 2023.\nIn-context learning with many demonstration exam-\nples.\nXiao Li, Yin Zhu, Sichen Liu, Jiangzhou Ju, Yuzhong\nQu, and Gong Cheng. 2022. Dyrren: A dynamic\nretriever-reranker-generator model for numerical rea-\nsoning over tabular and textual data.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2023. Lost in the middle: How language\nmodels use long contexts.\nJohn J. Nay, David Karamardian, Sarah B. Lawsky,\nWenting Tao, Meghana Bhat, Raghav Jain,\nAaron Travis Lee, Jonathan H. Choi, and Jungo\nKasai. 2023. Large language models as tax attorneys:\nA case study in legal capabilities emergence.\nHa-Thanh Nguyen. 2023. A brief report on lawgpt 1.0:\nA virtual legal assistant based on gpt-3.\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-\ntavo Hernández Ábrego, Ji Ma, Vincent Y . Zhao,\nYi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei\nYang. 2021. Large dual encoders are generalizable\nretrievers.\nRodrigo Nogueira and Kyunghyun Cho. 2020. Passage\nre-ranking with bert.\nOpenAI. 2023. Gpt-4 technical report. ArXiv,\nabs/2303.08774.\nOfir Press, Noah A. Smith, and Mike Lewis. 2022. Train\nshort, test long: Attention with linear biases enables\ninput length extrapolation.\n183\nJing Qian, Hong Wang, Zekun Li, Shiyang Li, and\nXifeng Yan. 2023. Limitations of language models in\narithmetic and symbolic induction. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 9285–9298, Toronto, Canada. Association for\nComputational Linguistics.\nBaptiste Rozière, Jonas Gehring, Fabian Gloeckle,\nSten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi\nAdi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom\nKozhevnikov, Ivan Evtimov, Joanna Bitton, Manish\nBhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wen-\nhan Xiong, Alexandre Défossez, Jade Copet, Faisal\nAzhar, Hugo Touvron, Louis Martin, Nicolas Usunier,\nThomas Scialom, and Gabriel Synnaeve. 2023. Code\nllama: Open foundation models for code.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\nJiashuo Sun, Hang Zhang, Chen Lin, Yeyun Gong, Jian\nGuo, and Nan Duan. 2023a. Apollo: An optimized\ntraining approach for long-form numerical reasoning.\nYutao Sun, Li Dong, Barun Patra, Shuming Ma, Shao-\nhan Huang, Alon Benhaim, Vishrav Chaudhary, Xia\nSong, and Furu Wei. 2023b. A length-extrapolatable\ntransformer. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), pages 14590–14604,\nToronto, Canada. Association for Computational Lin-\nguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023. Llama 2: Open foundation and fine-\ntuned chat models.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023. Chain-of-thought prompting elic-\nits reasoning in large language models.\nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski,\nMark Dredze, Sebastian Gehrmann, Prabhanjan Kam-\nbadur, David Rosenberg, and Gideon Mann. 2023a.\nBloomberggpt: A large language model for finance.\nZhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Ling-\npeng Kong. 2023b. Self-adaptive in-context learn-\ning: An information compression perspective for in-\ncontext example selection and ordering.\nHongyang Yang, Xiao-Yang Liu, and Christina Dan\nWang. 2023. Fingpt: Open-source financial large\nlanguage models.\nJiaxin Zhang and Yashar Moshfeghi. 2022. Elastic: Nu-\nmerical reasoning with adaptive symbolic compiler.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\nPeiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A\nsurvey of large language models.\nDawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wen-\nhao Wu, Furu Wei, and Sujian Li. 2023. Pose: Effi-\ncient context window extension of llms via positional\nskip-wise training.\nA Appendix\nA.1 Prompt tuning\nWe iteratively refine the prompt using the validation\nsets of 40 samples for each dataset, with the aim of\nfinding a prompt that encompasses a diverse range\nof cases while avoiding an overabundance of trivial\nor similar examples.\nB Baselines\nOn SARA, we evaluate our system against the fol-\nlowing baselines:\n• Majority baseline:A trivial baseline that pre-\ndicts the majority class for all the samples.\n• Feed-forward: The feed-forward networks\nevaluated by Holzenberger et al. (2020).\n• Legal-BERT:A BERT model trained specifi-\ncally on legal domain (Chalkidis et al., 2020)\nand adapted for SARA by Holzenberger et al.\n(2020).\n• BERT:A BERT model adopted for SARA by\nHolzenberger and Van Durme (2021)\n• GPT-3 (0-shot): GPT-3 evaluated with a\n0-shot prompt and without retrieval (Blair-\nStanek et al., 2023).\n184\n• GPT-3 (CoT):GPT-3 evaluated with a CoT\nprompt and without retrieval (Blair-Stanek\net al., 2023).\n• GPT-3 (dynamic): GPT-3 evaluated with\na dynamic few-shot prompt and without re-\ntrieval(Blair-Stanek et al., 2023). The prompt\nincludes different in-context examples for dif-\nferent questions.\nOn FinQA, we compare our system with the fol-\nlowing baselines in Table 2:\n• Pre-trained Longformer:Longformer (Belt-\nagy et al., 2020) is a model designed to take\nlong input documents in one step. The model\ncan be seen as a representative of one-step\napproaches.\n• FinQANet: (Chen et al., 2021) use an LSTM\ndecoder with attention for implementing the\nprogram generator and different models of\nBERT for retrieval.\n• ELASTIC: (Zhang and Moshfeghi, 2022) use\nan adaptive symbolic compiler to generate the\nprogram.\n• DyRRen: (Li et al., 2022) employ dynamic\nreranking of retrieved facts in every step.\n• TabT5: (Andrejczuk et al., 2022) use a T5\nmodel pre-trained on the Wikipedia tables.\n• APOLLO: (Sun et al., 2023a) The retriever\nis based on the sequence-pair classification\nfollowing (Nogueira and Cho, 2020). The pro-\ngram generator leverages a BERT encoder and\nan LSTM decoder with attention mechanism\nalong with consistency-based reinforcement\nlearning.\nFigure 3: A case in SARA for section 7703.\nModel Zero-shot Few-shot CoT\nLLaMA2-7B 50.4±5.4 58.8±5.3 53.5±5.4\nLLaMA2-7B_chat 42.1±5.3 51.8±5.4 54.4±5.4\nLLaMA2-13B 43.9±5.4 53.1±5.4 57.5±5.4\nLLaMA2-13B_chat 60.1±5.3 53.9±5.4 66.7±5.1\nLLaMA2-70B 49.6±5.4 67.5±5.1 71.1±4.9\nGPT-3 64.9±5.2 74.6±4.7 81.6±4.2\nTable 4: Comparison of different prompting techniques\nused on SARA. The prompts used are as described in\nsection 4.2 and shown in Appendix section E. Note that\nthe results shown here are from our proposed retrieval-\naugmented method, and not the same as the baselines\nshown in Table 1, which come from Blair-Stanek et al.\n(2023).\nCorrect CoT Incorrect CoT\nCorrect ans 23 8\nIncorrect ans 3 6\nTable 5: Results of the manual analysis performed on\nthe validation set using GPT-3.\nC Data examples\nFigure 3 shows a question from SARA.\nD Ablation studies\nD.1 Zero-shot, few-shot and CoT prompting\nTable 4 shows the performance of different models\nwith different prompting techniques.\nD.2 Case analysis\nSARA We conducted a manual analysis of the\nmodel’s output on the validation set. Table 5\npresents the results of this analysis on SARA, indi-\ncating the number of examples where both the an-\nswer and the chain-of-thought reasoning provided\nby the model were correct, both were incorrect and\ncases where one of them was incorrect. We found\nthat in 58.5% of the examples, the model accurately\npredicted both the output and the reasoning. For\nthe remaining cases, we categorized the errors into\nfour distinct categories, shown in Table 6.\nReasoning Error type # of cases\nArithmetic errors 8\nLogical errors 6\nContext too long 2\nRetrieval error 1\nTable 6: Error analysis on the validation set of SARA.\n185\nError type # of cases\nArithmetic error 3\nLogical error 2\nAnnotation error 2\nRetrieval error 3\nTable 7: Error analysis of the incorrect examples on 40\nsamples from FinQA.\nFinQA On the constructed validation set compris-\ning 40 samples, we observe that 30 samples have\ncorrect answers as well as corresponding programs.\nFor the remaining 10 samples, we manually clas-\nsify the errors into different categories, as shown\nin Table 7.\nE Prompts\nFigures 4 and 5 show the prompts used for SARA\nand FinQA respectively.\n186\n187\n188\nFigure 4: Chain-of-thought prompt for SARA for a sample. The complete prompt contains 8 in-context examples\nwith CoT explanations followed by the question that the model is supposed to answer. The in-context examples\nand explanations remain the same for all questions in the dataset. The text highlighted in yellow are the CoT\nexplanations that we hand-crafted, while the test question is shown in blue.\n189\n190\nFigure 5: Chain-of-thought prompt for FinQA for a sample. The complete prompt contains 12 in-context examples\nwith CoT explanations followed by the question that the model is supposed to answer. The in-context examples\nand explanations remain the same for all questions in the dataset. The text highlighted in yellow are the CoT\nexplanations that we hand-crafted, while the test question is shown in blue.\n191",
  "topic": "Context (archaeology)",
  "concepts": [
    {
      "name": "Context (archaeology)",
      "score": 0.6786741614341736
    },
    {
      "name": "Comprehension",
      "score": 0.635854184627533
    },
    {
      "name": "Computer science",
      "score": 0.6092692017555237
    },
    {
      "name": "Process (computing)",
      "score": 0.5992439985275269
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5841602087020874
    },
    {
      "name": "Data science",
      "score": 0.380575954914093
    },
    {
      "name": "Natural language processing",
      "score": 0.36669936776161194
    },
    {
      "name": "Knowledge management",
      "score": 0.35098904371261597
    },
    {
      "name": "Artificial intelligence",
      "score": 0.32192596793174744
    },
    {
      "name": "History",
      "score": 0.08214443922042847
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    }
  ]
}