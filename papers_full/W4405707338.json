{
  "title": "Harnessing multimodal approaches for depression detection using large language models and facial expressions",
  "url": "https://openalex.org/W4405707338",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2966142081",
      "name": "Misha Sadeghi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097991123",
      "name": "Robert Richer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2054711334",
      "name": "Bernhard Egger",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5000485873",
      "name": "Lena Schindler‐Gmelch",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4296306591",
      "name": "Lydia Helene Rupp",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2613542317",
      "name": "Farnaz Rahimi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A712971403",
      "name": "Matthias Berking",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1265058821",
      "name": "Bjoern M. Eskofier",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2612565440",
    "https://openalex.org/W2766194567",
    "https://openalex.org/W2048533792",
    "https://openalex.org/W4247665917",
    "https://openalex.org/W2809356716",
    "https://openalex.org/W2766165088",
    "https://openalex.org/W2797568851",
    "https://openalex.org/W2889097229",
    "https://openalex.org/W2887345170",
    "https://openalex.org/W2927590866",
    "https://openalex.org/W2927148761",
    "https://openalex.org/W2946396904",
    "https://openalex.org/W4366378733",
    "https://openalex.org/W4389520112",
    "https://openalex.org/W2914765137",
    "https://openalex.org/W4285211483",
    "https://openalex.org/W2132322340",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W2252180568",
    "https://openalex.org/W2109636054",
    "https://openalex.org/W2981677410",
    "https://openalex.org/W2767030624",
    "https://openalex.org/W2531271152",
    "https://openalex.org/W2530305026",
    "https://openalex.org/W2889056793",
    "https://openalex.org/W2962762902",
    "https://openalex.org/W2981590713",
    "https://openalex.org/W2982183006",
    "https://openalex.org/W4224924082",
    "https://openalex.org/W2981660166",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2981501230",
    "https://openalex.org/W4388674301",
    "https://openalex.org/W2395639500",
    "https://openalex.org/W2085662862",
    "https://openalex.org/W4311000453",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4285240641",
    "https://openalex.org/W6675354045",
    "https://openalex.org/W3152626170",
    "https://openalex.org/W6910740736",
    "https://openalex.org/W1588539311",
    "https://openalex.org/W2885905848",
    "https://openalex.org/W2098597588",
    "https://openalex.org/W1769933788",
    "https://openalex.org/W2131774270",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W4313443450",
    "https://openalex.org/W2529925562",
    "https://openalex.org/W2765540322",
    "https://openalex.org/W2767043218",
    "https://openalex.org/W2803017656",
    "https://openalex.org/W2960672642",
    "https://openalex.org/W2940060706",
    "https://openalex.org/W2250553926",
    "https://openalex.org/W2402700",
    "https://openalex.org/W2580767461",
    "https://openalex.org/W4392963607",
    "https://openalex.org/W4307820296",
    "https://openalex.org/W4405334247",
    "https://openalex.org/W3200946425",
    "https://openalex.org/W3161724753",
    "https://openalex.org/W2972853497",
    "https://openalex.org/W2346454595",
    "https://openalex.org/W2766122029",
    "https://openalex.org/W2751214333",
    "https://openalex.org/W4384916583",
    "https://openalex.org/W3098431772",
    "https://openalex.org/W3101267588"
  ],
  "abstract": "Abstract Detecting depression is a critical component of mental health diagnosis, and accurate assessment is essential for effective treatment. This study introduces a novel, fully automated approach to predicting depression severity using the E-DAIC dataset. We employ Large Language Models (LLMs) to extract depression-related indicators from interview transcripts, utilizing the Patient Health Questionnaire-8 (PHQ-8) score to train the prediction model. Additionally, facial data extracted from video frames is integrated with textual data to create a multimodal model for depression severity prediction. We evaluate three approaches: text-based features, facial features, and a combination of both. Our findings show the best results are achieved by enhancing text data with speech quality assessment, with a mean absolute error of 2.85 and root mean square error of 4.02. This study underscores the potential of automated depression detection, showing text-only models as robust and effective while paving the way for multimodal analysis.",
  "full_text": "npj |mental health research Article\nhttps://doi.org/10.1038/s44184-024-00112-8\nHarnessing multimodal approaches for\ndepression detection using large\nlanguage models and facial expressions\nCheck for updates\nMisha Sadeghi1 , Robert Richer1,B e r n h a r dE g g e r2, Lena Schindler-Gmelch3, Lydia Helene Rupp3,\nFarnaz Rahimi1, Matthias Berking3 & Bjoern M. Eskoﬁer1,4\nDetecting depression is a critical component of mental health diagnosis, and accurate assessment is\nessential for effective treatment. This study introduces a novel, fully automated approach to predicting\ndepression severity using the E-DAIC dataset. We employ Large Language Models (LLMs) to extract\ndepression-related indicators from interview transcripts, utilizing the Patient Health Questionnaire-8\n(PHQ-8) score to train the prediction model. Additionally, facial data extracted from video frames is\nintegrated with textual data to create a multimodal model for depression severity prediction. We\nevaluate three approaches: text-based features, facial features, and a combination of both. Our\nﬁndings show the best results are achieved by enhancing text data with speech quality assessment,\nwith a mean absolute error of 2.85 and root mean square error of 4.02. This study underscores the\npotential of automated depression detection, showing text-only models as robust and effective while\npaving the way for multimodal analysis.\nDepression, often termed a silent epidemic, impacts approximately 300\nmillion individuals globally, profoundly affecting their thoughts, behaviors,\nemotions, and overall well-being\n1. It is a major public health challenge, with\nan estimated 5% of adults suffering from this condition2,3. Depression does\nnot discriminate; it can affect anyone, regardless of background. People who\nhave experienced abuse, severe losses, or other stressful events are more\nsusceptible to developing depression. The consequences of untreated\ndepression can be severe, leading to impaired functioning in daily life,\nstrained relationships, and in the worst cases, suicide\n2. Despite the avail-\nability of effective psychotherapeutic and psychopharmacological treat-\nments, many individuals do not receiveadequate support. For instance, over\n75% of people in low- and middle-income countries lack access to the care\nthey need. Barriers to effective treatment include insufﬁcient investment in\nmental health services, a lack of trained healthcare providers, and the social\nstigma associated with mental disorders\n2,4. As a consequence, a signiﬁcant\nnumber of individuals affected by depression may never receive adequate\ndiagnoses and therefore, treatment\n2. Diagnosing depression primarily relies\non clinical interviews and questionnaires such as the Patient Health Ques-\ntionnaire (PHQ)\n5. This process can be time-consuming and susceptible to\nconfounding inﬂuences such as recall or rater biases, making false-positive\nor false-negative results a possibility6. One of the major hurdles in depres-\nsion treatment is the subjective nature of assessment, which can result in\ninconsistent evaluations and potentially inaccurate diagnoses\n6. Thus, it is of\nparamount importance to improve the understanding, diagnosis, and\ntreatment of depression to allow more effective and accessible clinical care.\nRecent advancements in artiﬁcial intelligence (AI) have opened up new\npossibilities for tackling complex health challenges such as depression.\nAmong these innovations, large language models (LLMs) like GPT-4o\n7 have\nshowcased impressive capabilities inunderstanding and generating natural\nlanguage. By leveraging these models, we can extract subtle linguistic and\nbehavioral features indicative of depression from multimodal data sources,\nsuch as text, audio, and video, providing more objective and reliable\nassessments that overcome the drawbacks of conventional assessment\napproaches. Looking to the future, the potential applications of LLMs in\nmental health care extend beyond detection. Imagine an AI assistant capable\nof automatically and continuously monitoring an individual’s mental health\nby analyzing their written, verbal, or video interaction with clinical practi-\nt i o n e r su p o nt h ep a t i e n t’s consent. Such an assistant could provide early\nwarnings if signs of depression are detected, prompting individuals to seek\nprofessional help sooner and increasing the chances of successful\n1Machine Learning and Data Analytics Lab (MaD Lab), Department Artiﬁcial Intelligence in Biomedical Engineering (AIBE), Friedrich-Alexander-Universität\nErlangen-Nürnberg (FAU), Erlangen, 91052, Germany.2Chair of Visual Computing (LGDV), Department of Computer Science, Friedrich-Alexander-Universität\nErlangen-Nürnberg (FAU), Erlangen, 91058, Germany.3Chair of Clinical Psychology and Psychotherapy (KliPs), Friedrich-Alexander-Universität Erlangen-Nürn-\nberg (FAU), Erlangen, 91052, Germany.4Translational Digital Health Group, Institute of AI for Health, Helmholtz Zentrum München - German Research Center for\nEnvironmental Health, Neuherberg, 85764, Germany. e-mail: misha.sadeghi@fau.de\nnpj Mental Health Research|            (2024) 3:66 1\n1234567890():,;\n1234567890():,;\nintervention. Moreover, such AI systems could offer personalized recom-\nmendations for self-help and psychotherapeutic/psychopharmacological\ntreatment options. An AI assistant could suggest activities to foster beha-\nvioral activation, provide cognitive r estructuring exercises, facilitate\nimproved interpersonal relationships,and offer practical problem-solving\nstrategies. By delivering such interventions through an accessible and per-\nsonalized platform, AI could empower individuals to take proactive steps in\nmanaging their mental health, thereby complementing traditional ther-\napeutic approaches.\nIn recent years, several studies have explored the use of AI and mul-\ntimodal approaches for depression detection, laying a solid foundation for\nfuture advancements. Research on social media-based depression detection\nh a sb e e np a r t i c u l a r l yp r o l iﬁc. Deshpande et al.\n8 utilized emotion AI and\nsentiment analysis to detect depression on Twitter9 by analyzing a curated\nlist of depression-related words. Yazdavar et al.10 developed a semi-\nsupervised model that incorporated word usage patterns and topical pre-\nferences to identify clinical depression symptoms from Twitter posts.\nSimilarly, Trotzek et al.\n11 employed machine learning models, including\nConvolutional Neural Networks (CNNs) on word embeddings, to detect\nearly depression symptoms from socialmedia messages. Expanding to other\nplatforms, Islam et al.\n12 analyzed user comments on Facebook13 using\ndecision trees for emotional and linguistic features and support vector\nmachines (SVMs) for temporal analysis. Orabi et al.14 proposed detecting\ndepression from Twitter data through optimized word embeddings and\ndeep learning models, including CNNs and recurrent neural networks\n(RNNs). Cacheda et al.\n15 focused on early depression detection methods\nusing social media data, emphasizinglinguistic and behavioral analysis.\nTadesse et al.16 utilized Natural Language Processing (NLP) and machine\nlearning to detect depression in Reddit posts17, relying on lexicons of words\ncommonly used by individuals with depression. Burdisso et al.18 introduced\na text classiﬁer for early risk detection tasks such as depression detection\nusing incremental classiﬁcation and explainable AI. Moreover, Guo et al.19\ndemonstrated improved depression detection capabilities in resource-\nconstrained settings by leveraging the strengths of pre-trained language\nmodels and topic modeling techniques. Pérez et al.20 developed a semantic\npipeline to estimate depression severity from social media, using a multi-\nclass classiﬁcation approach to differentiate severity levels. Their method,\nwhich incorporates clinical symptoms from the BDI-II questionnaire21,\nachieves state-of-the-art results on Reddit benchmark17.A d d i t i o n a l l y ,\nNguyen et al.22 enhanced depression detection by grounding their models in\nthe PHQ-923 questionnaire’s symptoms, improving out-of-domain gen-\neralization on social media datasets. By integrating these clinically relevant\nconstraints, they enhanced model interpretability and maintained compe-\ntitive performance compared to standard BERT-based\n24 approaches. This\ndemonstrates the potential of symptom-based modeling to improve AI\napplications in mental health diagnosis.\nBesides social media, an invaluable dataset for AI research in depres-\nsion detection is the distress analysis interview corpus Wizard-of-Oz\n(DAIC-WOZ)\n25,26. It comprises audiovisual recordings of 142 participants\ninteracting with a human-controlled virtual agent, designed to diagnose\npsychological distress conditions such as anxiety, depression, and PTSD.\nThe extended DAIC-WOZ dataset (E-DAIC) represents an expansion of\nthe original DAIC-WOZ dataset (DAIC), featuring a larger cohort of 275\nparticipants who underwent semi-clinical interviews with a virtual inter-\nviewer. The 20-minute interview sessions are converted into written tran-\nscripts and supplemented with annotations of acoustic and visual cues. The\ndataset ensures diverse representation and includes data from both human-\ncontrolled and autonomous AI interviews, along with clinical annotations\nlike PTSD Checklist Civilian Version (PCL-C) and Patient Health\nQuestionnaire-8 (PHQ-8) scores\n25–27. Several studies have utilized text,\naudio, video, or multimodal approaches to detect depression using the\nDAIC or E-DAIC datasets. For instance, Gong et al.28 developed a topic\nmodeling approach that facilitated context-aware analysis of lengthy\ninterviews in the DAIC dataset by extracting relevant topics. Williamson\net al.\n29 discovered that analyzing an avatar’s speech patterns can be a\npowerful way to identify depression, highlighting how the condition affects\ncommunication.\nOther studies have integrated multiple modalities for enhanced\ndepression detection. Nasir et al.30 explored multimodal features for\ndepression classiﬁcation, and found that i-vector (identity vector) modeling,\na feature extraction technique, performed exceptionally well in audio ana-\nlysis. Al Hanai et al.31 used long short-term memory (LSTM) networks to\ndetect depression using both audio and text modalities. Stepanov et al.32 took\na multimodal approach, fusing speech,language, and visual cues to predict\ndepression severity, as measured by PHQ-8 scores, using the DAIC dataset.\nFan et al.\n33 utilized a multi-scale temporal dilated CNN for depression\ndetection, incorporating both text and audio features. Yin et al.34 predicted\ndepression severity using a multimodal approach with bidirectional LSTM\nnetworks. Shen et al.35 identiﬁed two key predictors of depression severity in\nthe DAIC dataset: spectral features extracted from audio recordings and\nbehavioral cues extracted from interview transcripts, both of which proved\nto be strong indicators of depression severity. Prabhu et al.36 developed a\nmultimodal depression detection system combining facial expressions\n(CNN-LSTM), text (LSTM), and audio (LSTM) features, leveraging transfer\nlearning and ensemble techniques for enhanced performance. The E-DAIC\ndataset27 h a sb e e nu t i l i z e di nv a r i o u ss t u d i e sa sw e l l .F o ri n s t a n c e ,M a k i u c h i\net al.37 proposed a multimodal approach for depression detection, com-\nbining text, audio, and facial features using deep learning techniques,\nincluding VGG-16\n38 for speech, BERT for language, and ResNet-5039 for\nvisual features. Ray et al.40 introduced a multi-level attention-based network\nfor predicting depression severity, highlighting the importance of textual\ninformation.\nAs i g n iﬁcant limitation of prior research on text-based depression\ndetection using the DAIC or E-DAIC dataset is the reliance on manual\ntext preprocessing, feature extraction, and topic identiﬁcation. This\nlabor-intensive approach underscores the need for more automated and\ncomprehensive methods. The current paper addresses these limitations\nby presenting a novel, fully automated approach built on the E-DAIC\ndataset, which leverages state-of-the-art LLMs to automate the extrac-\ntion of depression-relevant features from interview transcripts. This\nautomation enables the efﬁcient identiﬁcation of language features\npertinent to depression, eliminating the need for manual processing. In\nour previous work\n41, we demonstrated the power of LLMs in uncovering\nvaluable insights from textual data, leveraging the E-DAIC dataset to\nexplore the potential of automated feature extraction. Our study\nrevealed that LLMs can enhance the efﬁciency and accuracy of depres-\nsion screening and diagnosis, by automating the analysis process and\nminimizing the need for manual review. Building upon this foundation,\nthe current paper presents a novel approach that reﬁnes and expands our\nprevious work. We introduce alternative prompts to improve the\nextraction of depression-related features from interview transcripts\nusing state-of-the-art LLMs. This reﬁnement enables more accurate and\nnuanced feature extraction, enhancing the automated analysis process.\nWe then use these extracted features to build a machine-learning model\nthat predicts PHQ-8 scores of individuals as a measure of depression\nseverity. Furthermore, we explore the incorporation of visual cues\nextracted from video frames, including facial expressions, eye gaze, and\nhead pose, using bidirectional LSTM networks. By fusing both textual\nand visual modalities, we construct a multimodal model for predicting\ndepression symptoms. Our approach enables a comprehensive evalua-\ntion of the effectiveness of each modality, allowing us to identify which\ndata modality is more effective in detecting depression symptoms. We\nevaluate the performance of each model using standard metrics to\ncompare their effectiveness and determine the relative strengths of\ntextual and visual features in detecting depression symptoms. Figure1\nprovides a summary of our proposed approach.\nThe structure of the paper is outlined as follows: Section“Methods”\nprovides a detailed description of the proposed method, while Section\n“Results and Discussion ” presents and discusses the experimental\nﬁndings.\nhttps://doi.org/10.1038/s44184-024-00112-8 Article\nnpj Mental Health Research|            (2024) 3:66 2\nMethods\nIn this section, we describe our proposed method for depression detection.\nFirst, we detail the dataset used. Then, we consider the automated depres-\nsion assessment based on textual data, including the feature extraction and\ntraining process. Following this, we explain a speech quality assessment that\nwe suggest to potentially improve prediction accuracy. In Section“Visual\nfeatures for automated depression assessment”,w eo u t l i n eo u rm e t h o df o r\ndepression detection based on visual features. Finally, in Section“Multi-\nmodal features for automated depression assessment”, we present the\nproposed multimodal approach, which combines both textual and visual\nfeatures for depression detection.\nDataset description\nExpanding upon DAIC, the E-DAIC dataset offers a collection of semi-\nclinical interviews facilitated by Ellie, a virtual interviewer, with accom-\npanying transcripts and annotations of acoustic and visual cues. The virtual\ni n t e r v i e w e rc a nb ec o n t r o l l e de i t h e rb yah u m a ni naW i z a r d - o f - O zs e t t i n g\nor autonomously by AI, allowing for realistic simulation of clinical inter-\nviews. The dataset consists of 275 interview sessions, featuring a participant\npool of 170 males and 105 females, which are then divided into three subsets:\na train set of 163 instances, a development set of 56 instances, and a test set of\n56 instances. The test set is solely constituted from the data collected by the\nautonomous AI. The dataset was carefully curated to ensure diverse speaker\nrepresentation, with deliberate attention paid to age and gender distribution,\nresulting in a dataset reﬂecting the broader population’s diversity. Details\nregarding the size of each partition and speaker distribution over the par-\ntitions are given in Table1. The provided visual features have been extracted\nusing the OpenFace software\n42, and acoustic features have been extracted\nusing the openSMILE tool43. The dataset also includes automatic tran-\nscription of the interactions using Google Cloud’s speech recognition ser-\nvice, participants’audioﬁles, as well as PTSD and PHQ-8 scores. The PTSD\nscore ranges from 0 to 85, while the PHQ-8 score ranges from 0 to 24, with\nhigher scores indicating greater depression severity25–27.I nt h ep r o v i d e d\ndevelopment set, PHQ-8 scores range from 0 to 20, with PTSD severity\nscores ranging from 17 to 72. The train set exhibits PHQ-8 scores ranging\nfrom 0 to 23, with PTSD severity scores spanning from 17 to 85. Finally, in\nthe test set, PHQ-8 scores range from 0 to 22, while PTSD severity scores\nrange from 19 to 77. Figure3 shows the distribution of PHQ-8 scores in each\nset of the dataset. As observed in the plots, higher PHQ-8 scores are rare, and\nthe mean score in each set is below 10.This uneven distribution poses a\nchallenge for machine learning modelstrained on this data. Since the model\nis exposed to fewer high PHQ-8 scores during training, it struggles to\naccurately predict the higher scores due to insufﬁcient high-score examples.\nIt is important to note that the E-DAIC dataset used in this study\ncontains no protected health information (PHI). The dataset curators\nremoved all identifying details, such as names, dates, and locations, from the\naudio recordings and transcriptions. Additionally, the facial features in the\ndataset are not detailed enough to identify individuals. The dataset is pub-\nlicly available, and interested researchers can apply to receive access at\nhttps://dcapswoz.ict.usc.edu/. Research investigators planning to use similar\nmethods on other datasets should beaware that they may encounter PHI\nand take necessary measures to ensure compliance with relevant\nregulations.\nTextual features for automated depression assessment\nThis section explores textual featuresfor automated depression assessment.\nBased on the pipeline proposed in our previous work41,w ee x t e n do u r\napproach to improve the assessment of depression from interview tran-\nscripts. The proposed pipeline is depicted in Fig.2. We begin by converting\ni n t e r v i e wa u d i or e c o r d i n g si n t ot e x tusing automatic speech recognition,\nfollowed by the application of LLMs to transform the transcripts and extract\nfeatures pertinent to depression. Finally, we train the model using PHQ-8\nscores as the target variable. Through iterative performance evaluations and\nprompt engineering, we reﬁne the prompts utilized in the pipeline. Each\ncomponent of the pipeline will be thoroughly explained in the following\nsections.\nThe E-DAIC dataset includes automated transcripts generated by\nspeech-to-text systems, which are often incomplete and inaccurate, result-\ning in the loss of crucial context and details. Through inspection of several\ntranscripts and by listening to the corresponding interview audios, we\nobserved that signiﬁcant questions and responses are sometimes reduced to\nsimple ‘yes’or ‘no’answers, with the original question missing. Occasion-\nally, the question is included without the corresponding answer. Further-\nmore, the conversationalﬂow is frequently disrupted, with key phrases\nfragmented or essential background information missing. To address these\nlimitations, we utilized OpenAI’sW h i s p e r\n44 automatic speech recognition\nsystem to generate high-quality transcripts from raw interview recordings.\nThe Whisper‘large’model stands out due to its unique robustness prop-\nerties and superior performance across various datasets. While it achieves a\nword error rate (WER) of 2.7 on the LibriSpeech test-clean dataset\n45,i t sz e r o -\nshot capabilities allow it to outperform all benchmarked LibriSpeech models\nby signiﬁcant margins on other datasets. Even the smallest zero-shot\nWhisper model is competitive with the best-supervised models when\nevaluated outside of the LibriSpeech test-clean framework. The best zero-\nshot Whisper models not only closely match human accuracy and\nFig. 1 | Graphical abstract of the proposed framework.(1) E-DAIC dataset including transcripts and video recordings25. (2) Feature extraction with Large Language Models\n(LLMs) and OpenFace42. (3) Model training for PHQ-8 score prediction and performance evaluation.\nTable 1 | Number of participants and duration of the interviews\nincluded in the E-DAIC dataset25\nPartition # Participants Duration [h:min:s]\nTrain 163 43:30:20\nDevelopment 56 14:47:31\nTest 56 14:52:42\nAll 275 73:10:33\nhttps://doi.org/10.1038/s44184-024-00112-8 Article\nnpj Mental Health Research|            (2024) 3:66 3\nrobustness but also deliver a 55.2% average relative error reduction on\ndiverse speech recognition datasets44. By employing the Whisper‘large’\nmodel and reviewing several transcripts, we observed an improvement in\nquality, with more context and information retained in the Whisper-\ngenerated transcripts. This analysis builds on our previous study, where we\nproposed this method to enhance the quality of E-DAIC dataset\ntranscripts\n41.\nDespite Whisper’s advantages, our examination of several transcripts\nrevealed a few issues. During this inspection, we occasionally found that\nsome answers to questions were missing. Upon reviewing the interview\naudio, we determined that this missing information was primarily due to\nlow audio quality, which rendered certain parts unrecognizable. Addition-\nally, we observed instances of word duplication in Whisper-generated\ntranscripts, such as repeated phrases like“That’sn o tt r u e .T h a t’sn o tt r u e .\nThat’sn o tt r u e . . .”. Similar duplication issues have also been noted by other\nresearchers\n46. To remain consistent with our automated approach, we opted\nnot to manually correct these duplications. Instead, we relied on LLMs in the\nsubsequent transcript transformation step to help address these challenges\nby extracting the most signiﬁcant topics related to depression.\nTo prepare the transcripts for depression detection, we reﬁne them to\nmake them clearer and more concise using a Clean-up Prompt:\"This inter-\nview involves a conversation with someone. Could you modify it by removing\nquestions that don’t have an answer? Keep in mind that responses such as‘yes’\nand ‘no’ are also acceptable.” As mentioned earlier, during our screening of\nWhisper-generated transcripts, we noticed some questions were stated\nwithout a corresponding answer. To address this, we developed this Clean-up\nPrompt as an automated solution. Our aim was to investigate whether\nremoving questions that lacked answers could improve the performance of\nour proposed depression detection model, as questions without answers\nmight confuse LLMs during the transcript transformation step. Next, we use\nthree speciﬁc prompts to extract crucial information related to depression\nfrom the revised transcripts. These prompts help us identify key points and\nsummarize relevant information. The three prompts used in our analysis are:\n Prompt 1 (derived from our previous study\n41): “Your task is to read the\nfollowing text which is an interview with a person and to summarize the\nkey points that might be related to the depression of the person.”\n Prompt 2 (reﬁned from our previous study\n41): “Your task is to read the\nfollowing text which is an interview with a person and to summarize the\nkey points that might be related to the depression of the person. Please be\nconcise and write your response from theﬁrst-person perspective, as if\nyou are the interviewee narrating about your own experiences.”\n Prompt 3 (newly designed):“Could you provide a summary of the main\npoints concerning the mental health of the interviewee from the\ninterview?”\nWe applied these prompts to the revised transcripts using GPT-3.5-\nTurbo-0125, a state-of-the-art LLM developed by OpenAI\n47.T h r o u g h\nprompt engineering, we iteratively designed and reﬁned the prompts. We\nalso utilized the GPT model to suggest alternative prompts, and by evalu-\nating the whole pipeline and validating the model’s predictions using error\nFig. 2 | Overview of the proposed framework for depression detection based on\ntextual data.(1) Data preprocessing using Whisper’s automatic speech\nrecognition44. (2) Transcript transformation via GPT-3.5-Turbo-012547. (3) Feature\nextraction by DepRoBERTa49 and an LLM-driven question-based method. (4) PHQ-\nbased depression severity prediction: model training, PHQ-8 score forecasting,\nperformance evaluation, and prompt engineering.\nhttps://doi.org/10.1038/s44184-024-00112-8 Article\nnpj Mental Health Research|            (2024) 3:66 4\nmetrics, speciﬁcally mean absolute error (MAE) on the test set, we selected\nthe three most effective prompts that are presented in this paper. Addi-\ntionally, we conducted a separate experiment where we bypassed the revi-\nsion step and directly applied the three mentioned prompts to the original\nWhisper-generated transcripts, without applying the Clean-up Prompt.\nThis allowed us to compare the performance of the prompts on both reﬁned\nand raw transcripts.\nFollowing the transformation and summarization of the interview\ntranscripts, we utilize a pre-trained language model to examine the pro-\ncessed transcripts, as outlined in our previous research\n41. Speciﬁcally, we\nemploy a ﬁne-tuned RoBERTa 48 language model, known as\nDepRoBERTa49, which is speciﬁcally designed for depression detection. This\nmodel is built upon RoBERTa-large48 and was pre-trained on Reddit17 posts\nrelated to depression. The‘deproberta-large-depression’model has shown\nexceptional performance in detectingdepression levels in English social\nmedia posts49. Notably, DepRoBERTa emerged as the top solution in the\nShared Task on Detecting Signs of Depression from Social Media Text at\nLT-EDI-ACL2022 and is available on the Hugging Face model hub\n50.T h e\nmodel can detect three different levels of depression:‘not depression’,\n‘moderate’,a n d‘severe’based on text data49.\nTo tailor the model to our dataset, we conductedﬁne-tuning of the\nmodel with a low learning rate of 5 × 10−6.A st h eD e p R o B E R T am o d e lw a s\ninitially trained on a dataset with three classes, we categorized the trans-\nformed transcripts into three labelsbased on their corresponding PHQ-8\nscores to match the original model’s training data. The standard PHQ-8\nscore categories for depression diagnosis include mild symptoms (5–9),\nmoderate symptoms (10–14), moderately severe symptoms (15–19), and\nsevere symptoms (20–24), with scores below 5 indicating no depression\n5.\nGiven the imbalanced distribution ofPHQ-8 scores in the E-DAIC dataset,\nas illustrated in Fig.3, particularly the scarcity of instances in the moderately\nsevere and severe groups, we devised a simpliﬁed categorization scheme to\naddress this imbalance. Speciﬁcally, scores of 14 or higher were categorized\nas ‘severe’, scores between 7 and 13 (inclusive) were labeled as‘moderate’,\nand scores lower than 7 were designated as‘not depression’. This scheme\nwas designed to ensure theﬁne-tuned DepRoBERTa model had a sufﬁcient\nnumber of instances in each category, thereby improving the model’sa b i l i t y\nto learn effectively from the data. This categorization was not intended for\nclinical diagnosis but was crucial for balancing the dataset and avoiding the\nissue of the model being skewed by the scarcity of severe depression\ninstances in the original E-DAIC dataset. Subsequently, we trained the\nmodel on the labeled data, evaluating its performance on the development\nset and implementing early stopping to prevent overﬁtting. This mechanism\nmonitored the development loss at the end of each epoch. Speciﬁcally, we\ntracked the best development loss observed, which was computed using the\ncross-entropy loss function, and terminated the training if the development\nloss did not improve for three consecutive epochs. Followingﬁne-tuning, we\nused the ﬁne-tuned DepRoBERTa model to perform inference on the\ntransformed transcripts. Each transformed transcript was provided as input\nto the model, which then produced an output representing the probabilities\nof the text belonging to each of the three depression classes:‘not depression’,\n‘moderate’,a n d‘severe’. These probabilities, ranging between 0 and 1, served\nas features extracted from the model. For example, when the text“Ia m\nf e e l i n gv e r yw e l l .” was input into the model, the resulting output might be an\narray such as [0.966, 0.026, 0.008], corresponding to the probabilities for‘not\ndepression’, ‘moderate’,a n d‘severe’classes, respectively. This output indi-\ncates a high probability of‘not depression’and very low probabilities for the\nother two classes.\nTo further enrich our feature vector beyond what the DepRoBERTa\nmodel generated, we explored an additional approach to extract relevant\ninformation from the transcripts. Our aim was to develop a more nuanced\nand targeted set of features tailored to the unique characteristics of the\ninterviews. To achieve this, we used the GPT-3.5-Turbo-0125 model. We\nprovided the model with a selection of sample interviews from the dataset\nand tasked it with designing questions that could differentiate responses\nfrom individuals with and without depression. The model generated a set of\nquestions based on the provided transcripts. We then used these questions\nto extract features based on each interview transcript. The 11 questions listed\nbelow were crafted to probe various aspects of depression, including emo-\ntional and physical well-being, mood changes, sleep disturbances, con-\ncentration difﬁculties, and past diagnoses.\n1. Have you felt emotionally and physically well lately?\n2. Have you noticed signiﬁcant changes in your mood, such as feeling\npersistently sad, empty, or hopeless?\n3. Have you experienced difﬁculties with your sleep, such as trouble\nfalling asleep, staying asleep, or waking up too early?\n4. Are you ﬁnding it challenging to concentrate on tasks or make\ndecisions?\n5. Have you lost interest or pleasure in activities you used to enjoy?\n6. Have you ever been diagnosed with depression or experienced pro-\nlonged periods of feeling down or hopeless in the past?\n7. Have you ever been diagnosed with PTSD (Post-Traumatic Stress\nDisorder)?\n8. Have you been experiencing anyﬁnancial problems recently?\n9. Do youﬁnd it challenging to socialize and prefer solitary activities,\nindicating introverted tendencies?\n10. Have you had thoughts of death or suicide, or have you made any\nsuicide attempts?\n11. Have you ever served in the military?\nNext, we crafted a custom prompt and posed the questions to the GPT\nmodel, asking it to respond with one of the following answers:‘YES’, ‘NO’,\n‘To Some Extent’,o r ‘Not Mentioned’. The prompt was formulated as\nfollows: “Can you answer these questions from this text, which is an\ninterview with a person, only with‘YES’or ‘NO’or ‘To Some Extent’?I ft h e\nquestion or corresponding answer is not found, answer‘Not Mentioned’.”\nThe model’s responses were then converted into a numerical feature vector,\nwhere‘YES’was mapped to 1,‘NO’to 0, and‘To Some Extent’to 0.5. In cases\nwhere the model responded with‘Not Mentioned’, we initially assigned a\nvalue of NaN (Not a Number) and then substituted the mean value of the\nrespective question within each of the train, development, and test sets\nseparately.\nIn the subsequent step, we trained a support vector regression\n(SVR) machine learning model, using the PHQ-8 scores as the outcome\nvariable. The model leverages the extracted features to predict PHQ-8\nscores for each interview transcript. From the two feature extraction\nsteps described above, we obtained a 14-dimensional feature vector for\neach interview: 3 features are derived from the ﬁne-tuned\nFig. 3 | PHQ-8 score distribution in the E-DAIC25 dataset. The box plots show the\nmedian (horizontal line within each box), the interquartile range (IQR; represented\nby the edges of the box), and whiskers extending to 1.5× IQR from the box edges. The\ny-axis shows PHQ-8 scores ranging from 0 to 24.\nhttps://doi.org/10.1038/s44184-024-00112-8 Article\nnpj Mental Health Research|            (2024) 3:66 5\nDepRoBERTa model while 11 features are from the LLM-driven\nquestion-based feature extraction method. The SVR model uses a\nlinear kernel, with the hyperparameter C = 1 . 0 .W et r a i n e dt h eS V R\nmodel exclusively on the train set and evaluated its performance on the\nuntouched development and test sets, ensuring direct comparability\nwith existing studies that use the same dataset conﬁguration.\nAdditionally, to evaluate the impact of transcript transformation on\ndepression detection, we conducted an additional experiment where we\nbypassed the transformation step entirely. In this analysis, we extracted\nfeatures directly from the raw interview transcripts generated by Whisper\nusing theﬁne-tuned DepRoBERTa model. We assessed two conﬁgurations:\nﬁrst, using DepRoBERTa features in conjunction with features derived from\nthe question-based method, and second, using only the DepRoBERTa\nfeatures without question-based features. In both cases, the extracted fea-\ntures were used to train the SVR model for predicting the PHQ-8 scores. The\ngoal was to determine whether using raw transcripts without transforma-\ntion could achieve comparable performance.\nTo further validate the robustness of our model and ensure its gen-\neralizability, we also performed a nested cross-validation analysis on the\ncombined train and development sets.This additional evaluation involved\nan outerﬁvefold cross-validation loop, coupled with an innerﬁvefold cross-\nvalidation loop for hyperparameter tuning. Speciﬁcally, the inner loop\nutilized GridSearchCV from the Scikit-learn library\n51 to optimize para-\nmeters such as the kernel type,regularization parameter (C), and Kernel\ncoefﬁcient (gamma) for non-linear kernels. By leveraging GridSearchCV,\nwe explored different hyperparameter combinations to identify the best\nmodel conﬁguration. This nested cross-validation approach allowed us to\nassess the model’s performance more rigorously by leveraging multiple folds\nwithin the training data for both training and validation, while still keeping\nthe test set untouched forﬁnal evaluation. The results of these evaluations\nare detailed in Section“Outcomes of Depression evaluation using tex-\ntual data”.\nThe E-DAIC dataset poses a signiﬁcant challenge due to the varying\naudio quality of the interview recordings. Background noise, inconsistent\nloudness levels, and other imperfections can degrade the accuracy of\nautomated speech recognition (ASR) systems, resulting in incomplete or\ninaccurate transcripts and undermining the reliability of subsequent ana-\nlyses. Even with advanced ASR systems like Whisper, which were shown to\noutperform conventional methods, limitations remain. To tackle this issue,\nwe introduce an approach that aims to enhance the accuracy of our method.\nFor automatic speech quality assessment, we used the Python Package\nNISQA\n52,53, which yields multidimensional audio quality predictions,\nincluding overall speech quality as well as quality dimensions such as noi-\nsiness, coloration, discontinuity, and loudness52. To ensure the reliability of\nour analysis, we applied the NISQA tool to all interview audios in the train,\ndevelopment, and test sets. We set a threshold for acceptable speech quality\nat 2.5, speciﬁcally for the overall speech quality score provided by NISQA,\nwhich ranges from 1 to 5\n54. This decision was made based on empirical\nobservations and testing of the dataset. We found that an overall speech\nquality score of 2.5 or higher allowed us to include a sufﬁcient number of\ninterviews while still maintaining a standard for acceptable audioﬁdelity.\nThe overall speech quality scores ranged as follows: in the train set, the scores\nranged from a minimum of 1.34 to a maximum of 3.99; in the development\nset, from a minimum of 1.15 to a maximum of 4.09; and in the test set, from a\nminimum of 1.23 to a maximum of 3.48. After the quality check, 49\ninterviews from the train set (30.1%), 21 from the development set (37.5%),\nand 31 from the test set (55.4%) failed to meet the quality threshold. The\nresults based solely on data that match the speech quality requirement are\nreported in the“Results” section.\nVisual features for automated depression assessment\nAs previously mentioned, the E-DAIC dataset comprises audiovisual\nrecordings of semi-clinical interviews. Even though the publicly available\nversion of the E-DAIC dataset does not contain the original videoﬁles, it\nprovides visual features per video frame. The visual features that were\nextracted using the OpenFace software\n42 can be categorized into the fol-\nlowing groups:\n Action Units (AU): A subset of 18 AUs, along with their presence and\nintensity. The Facial Action Coding System (FACS)55 is a system to\ntaxonomize facial expressions byc o d i n gt h em o v e m e n t so ff a c i a l\nmuscle groups into AUs. For instance, the activation of AU6 corre-\nsponds to the raising of the cheeks.\n Head Pose: The three-dimensional position of the head relative to the\ncamera, as well as rotational data encompassing roll (rotation around\nthe head’s front-to-back axis), pitch (rotation around the head’ss i d e -\nto-side axis), and yaw (rotation around the head’s vertical axis)\n56,57.\n Eye-Gaze: The angle of the left and right eye gaze in radians58.\nThe E-DAIC dataset comprises a range of facial features, totaling 49,\nincluding head pose, eye gaze, and AUs. For each AU, OpenFace yields a\nvariable indicating the presence of an AU in the respective video frame (0—\nnot present, 1— present; denoted by the sufﬁx ‘_c’) as well as an intensity\nvariable providing a continuous output between 1 and 5 (denoted by the\nsufﬁx ‘_r’). Speciﬁcally, the E-DAIC dataset includes the following AUs:\nAU1, AU2, AU4, AU5, AU6, AU7, AU9, AU10, AU12, AU14, AU15,\nAU17, AU20, AU23, AU25, AU26, AU28, and AU45. For more informa-\ntion regarding the description of each AU, refer to refs.59,60.\nThe proposed architecture for predicting PHQ-8 scores from video\ndata is depicted in Fig.4. The E-DAIC dataset provides pre-extracted fea-\ntures from OpenFace\n42, including head pose, eye gaze, and AUs for each\nvideo frame, totalingn = 49 features (6 head pose features, 8 eye gaze fea-\ntures, 17 AU intensities, and 18 AU occurrences). These pre-extracted\nfeatures serve as inputs to the model. As shown in Fig.4, the model leverages\na bidirectional LSTM network architecture61 to capture temporal depen-\ndencies in sequential data62. The architecture consists of three layers of\nbidirectional LSTM units with 64 hidden units each. Additionally,\nFig. 4 | Overview of the proposed framework for depression detection using visual data.(1) Visual feature extraction module using OpenFace42. (2) Bidirectional sequence\nanalysis module with Bi-LSTM layers. (3) Attention-based prediction module for PHQ-8 score estimation.\nhttps://doi.org/10.1038/s44184-024-00112-8 Article\nnpj Mental Health Research|            (2024) 3:66 6\nOpenFace outputs an extraction conﬁdence score per video frame, ranging\nfrom 0 to 1. To mitigate the impact of noisy or incomplete data on the model\nperformance, we excluded video frames with a conﬁdence score below 0.90\nfrom further analysis. To accommodatevariable-length video frames, we\napplied padding with zero to equalize all frames to the same length. To\nprevent overﬁtting during training, we applied a dropout rate of 0.3 as a\nregularization technique. Furthermore, we incorporated an attention\nmechanism consisting of a single attention layer to dynamically weigh the\nimportance of each input feature based on its relevance to the predic-\ntion task.\nWe trained the proposed LSTM model exclusively on the train set,\nexploring various feature combinations as input, such as head pose features,\neye gaze features, AU intensities, and AU occurrences. This also included\ncombinations of two, three, or all feature types, such as AU intensities with\nhead pose and eye gaze. Using the Adam optimizer with a learning rate of\n0.1, the model was trained over 20 epochs to minimize mean squared error\n(MSE) loss. For evaluation, the development and test sets were left untou-\nched, ensuring that performance assessments via root mean square error\n(RMSE) and MAE metrics allow for direct comparability with existing\nstudies employing the same dataset conﬁguration.\nMultimodal features for automated depression assessment\nTo harness the complementary strengths of visual and textual features, we\nconducted a third experiment in which we combined our two previous\nprediction pipelines (Sections“Dataset description” and“Textual features for\nautomated depression assessments”). Speciﬁcally, we fused the outputs of the\nDepRoBERTa model, the features extracted by the LLM-driven question-\nb a s e dm e t h o d ,a n dt h ev i s u a lf e a t u r e sextracted by the LSTM model, thereby\ncreating a uniﬁed feature space that captures both nonverbal behavioral\npatterns and linguistic cues potentiallyindicative of depression. The proposed\nmultimodal framework is depicted in Fig.5.A m o n gt h eL S T Mm o d e l s\ntrained on different combinations of visual features as mentioned in the\nprevious section, we identiﬁed the optimal model for feature extraction. This\nmodel was then used to extract 128-dimensional representations for each data\nsample. To reduce the dimensionality of these representations, we applied\nprincipal componentanalysis (PCA) with aﬁxed number of 10 output\ncomponents. The resulting PCA-transformed LSTM features were then\nmerged with the textual features. Following dimensionality reduction, we\nemployed a feature selection technique using SelectKBest from the Scikit-\nlearn library\n51 with the F-regression score function to identify the top 10\nfeatures most strongly associated with PHQ-8 scores. The combined feature\nset, paired with PHQ-8 scores as the target variable, served as input to train an\nSVR model with a radial basis function (RBF) kernel. To optimize the SVR\nhyperparameters, we utilized GridSearchCV\n51 with ﬁvefold cross-validation,\nusing only the training set while reserving the development and test sets for\nﬁnal evaluation. This approach allowed us to keep the development and test\nsets untouched, ensuring an unbiased evaluation of our model’s performance\nand facilitating a direct comparison with other studies. Speciﬁcally, we\nexplored the following parameters:\n Regularization parameter (C): 0.1, 1, 10, and 100\n Kernel coefﬁcient (gamma): ‘scale’, ‘auto’,0 . 1 ,1 ,a n d1 0\n Epsilon parameter (epsilon): 0.1, 0.2, 0.5, and 1\nFinally, we evaluated the performance of the SVR model using RMSE\nand MAE metrics on both the development and test sets.\nMoreover, to assess the resilience and adaptability of our multimodal\nmodel, we performed a nested cross-validation analysis on the merged train\nand development sets. This approach involved an externalﬁvefold cross-\nvalidation loop for assessment, paired with an internalﬁvefold loop focused\non hyperparameter optimization. Inside the internal loop, we leveraged\nGridSearchCV\n51 to ﬁne-tune key hyperparameters, including the regular-\nization parameter, kernel coefﬁcient, and epsilon for the SVR model. Fur-\nthermore, we applied PCA and SelectKBest to streamline the feature set\nprior to training.\nResults and discussion\nOutcomes of depression evaluation using textual data\nThe prediction results of our differentapproaches for depression assessment\nbased on the E-DAIC interview transcripts are listed in Table2.T oe n s u r e\ncomparability with existing methods, we included results from previous\nstudies using the DAIC and E-DAIC datasets, many of which participated in\nthe 2016, 2017, or 2019 AVEC challenges27. We focused on studies that used\nthe PHQ-8 score as the target variable. To highlight one of our main goals—\nproviding a fully automated processing pipeline— we distinguished between\nfully automated approaches and those requiring manual processing.\nTherefore, the table’s ﬁnal column speciﬁes whether each study underwent\nautomated processing of transcripts and extraction of relevant features.\nAll proposed methods employed transcript transformation using\nGPT-3.5-Turbo-0125 and aﬁne-tuned DepRoBERTa model combined\nwith our question-based feature extraction method. We experimented with\nvarious prompts (1, 2, and 3) applied to both the original Whisper-\ngenerated transcripts and revised transcripts using the Clean-up prompt.\nEach prompt is described in the Section“Textual features for automated\ndepression assessment”. In the proposed methods Pr1 +Revised,\nPr2 +Revised, and Pr3+Revised, we used revised transcripts and applied\nprompts 1, 2, and 3, respectively. In the proposed methods Pr1+Whisper,\nPr2 +Whisper, and Pr3 +Whisper, we used the original Whisper-\nFig. 5 | Overview of the multimodal depression detection framework.(1) Feature\nextraction and fusion, combining 3 DepRoBERTa49 textual features, 11 question-\nbased features, and 128 visual features, followed by PCA and fusion into a 24-\ndimensional representation. (2) Depression severity prediction using an SVR model\nto estimate PHQ-8 scores with performance evaluation.\nhttps://doi.org/10.1038/s44184-024-00112-8 Article\nnpj Mental Health Research|            (2024) 3:66 7\ngenerated transcripts and applied prompts 1, 2, and 3, respectively. Addi-\ntionally, Pr3+Whisper+AudioQual integrated speech quality assessment\ninto the Pr3+Whisper pipeline, analyzing only interviews with acceptable\nspeech quality. This speech quality assessment was performed on all train,\ndevelopment, and test sets, and only interviews of acceptable speech quality\nwere utilized for further analysis.\nOur results demonstrated that Pr3+Whisper achieved the best per-\nformance among methods without speech quality assessment, with an MAE\no f3 . 1 7a n dR M S Eo f4 . 5 1o nt h ed e v e l o p m e n ts e t ,a n da nM A Eo f4 . 2 2a n d\nRMSE of 5.07 on the test set. However, the best overall results were obtained\nusing Pr3+Whisper+AudioQual, with an MAE of 2.85 and RMSE of 4.02\non the development set, and an MAE of 3.86 and RMSE of 4.66 on the test\nset. As shown in Table2,G o n ge ta l .\n28 achieved better results on the\ndevelopment set with an MAE of 2.77 and RMSE of 3.54, though their\nmethod involved substantial manual processing, including cleaning tran-\nscripts, extracting topics, and creating interview questions, which compli-\ncates direct comparison. Additionally, Ray et al.\n40 and Williamson et al.29\nachieved better RMSE on the development set compared to Pr3+Whisper,\nhowever, their approaches also included manual processing at various\nstages.\nOn the test set, Gong et al.28, Ray et al.40, and Fang et al.63 reported\nsuperior results compared to Pr3+Whisper, with MAEs of 3.96, 4.02, and\n3.61, and RMSEs of 4.99, 4.73, and 4.76, respectively. Ray et al.40 manually\ncleaned the transcripts, although the extent of this cleaning was not fully\ndetailed. Fang et al.\n63 did not specify whether their results were based on the\ntest or development set; we assumed they used the test set. They manually\nsegregated segments where the participant was speaking from the rest of the\ninterview and standardized oral expressions by expanding abbreviations\nwhile preserving tone markers such as‘umm’ or ‘hmm’. Due to these\nmanual processing steps and their useof the DAIC dataset, which differs in\nparticipant numbers and PHQ-8 score distribution from the E-DAIC\ndataset, direct comparisons are challenging.\nWe selected Pr3+Whisper as a baseline for integrating speech quality\nassessment because it achieved the best performance compared to other\nmethods. The resulting model, Pr3 +Whisper+AudioQual, out-\nperformed all previous studies on thetest set, including those involving\nmanual transcript processing. Notably, only Gong et al.\n28 surpassed our\nresults on the development set, and Fang et al.63 achieved better MAE on the\ntest set. However, both approaches involved manual processing and used\nthe DAIC dataset, making direct comparisons challenging, as\nmentioned above.\nIn contrast to previous studies, our approach stands out for its auto-\nmated pipeline, eliminating the need for manual processing and transcript\ncleaning. This distinction is crucial,as manual interventions can introduce\nvariability and bias, compromising the model’s generalizability. By lever-\naging Pr3 +Whisper and integrating speech quality assessment, we\nachieved superior performance on the test set without relying on manual\nprocessing. This automated approachnot only streamlines the process but\nalso ensures consistency and reproducibility. Our results demonstrate the\nimportance of high-quality input data as low-quality audio can compromise\nthe model’s performance, leading to inaccurate judgments. Consequently,\nrigorous quality assessments are essential to ensure reliable predictions,\nparticularly for individuals with high PHQ-8 scores who may otherwise be\nmisclassiﬁed as having low scores, resulting in potential missed depression\ndiagnoses.\nIn an additional experiment where we bypassed the transcript\ntransformation step, the results demonstrated a substantial decline in\nmodel performance. When both DepRoBERTa and question-based\nfeatures were used, we observed an MAE of 4.31 and RMSE of 5.58 on\nthe development set, and an MAE of 4.91 and RMSE of 6.22 on the test\nset. Without the question-based features, the performance worsened\nfurther, with an MAE of 4.69 and RMSE of 5.96 on the development\nset, and an MAE of 5.55 and RMSE of 7.11 on the test set. In contrast,\nour best-performing model without speech quality assessment\nTable 2 | Performance comparison of PHQ-8 score prediction models using textual features from DAIC (or E-DAIC) dataset\nMethod Dataset MAE (dev a) RMSE (dev) MAE (test) RMSE (test) Auto Proc\nWilliamson et al.29 DAIC 3.34 4.46 –– No\nGong et al.28 DAIC 2.77 3.54 3.96 4.99 No\nYang et al.65,b DAIC 3.52 4.52 –– No\nStepanov et al.32 DAIC –– 4.88 5.83 No\nRay et al.40 E-DAIC – 4.37 4.02 4.73 No\nOureshi et al.77 DAIC 3.78 –– – No\nNiu et al.78 DAIC 3.73 4.80 –– No\nFang et al.63 DAIC –– 3.61 4.76 No\nRohanian et al.79 DAIC –– 4.98 6.05 Yes\nMakiuchi et al.37 E-DAIC –– 4.22 6.88 Yes\nAl Hanai et al.31 DAIC 5.18 6.38 –– Yes\nQureshi et al.69 DAIC 3.74 4.80 –– Yes\nSadeghi et al.41 E-DAIC 3.65 5.27 4.26 5.36 Yes\nPr1 + Revised E-DAIC 4.15 5.28 4.73 6.02 Yes\nPr2 + Revised E-DAIC 3.87 5.15 4.50 5.61 Yes\nPr3 + Revised E-DAIC 3.79 4.93 4.36 5.42 Yes\nPr1 + Whisper E-DAIC 3.99 5.26 4.65 5.95 Yes\nPr2 + Whisper E-DAIC 3.90 5.10 5.00 6.23 Yes\nPr3 + Whisper E-DAIC 3.17 4.51 4.22 5.07 Yes\nPr3 + Whisper + AudioQual E-DAIC 2.85 4.02 3.86 4.66 Yes\nThe best-performing results of this study and previous results that outperformed our results are highlighted in bold.‘–’indicates that the respective metrics were not reported in the publication.“Auto Proc”\nindicates whether each processing step was performed automatically (Yes) or involved manual processing for at least one step (No).\naDevelopment.\nbThis study reported separate results for males and females, which we averaged for comparison.\nhttps://doi.org/10.1038/s44184-024-00112-8 Article\nnpj Mental Health Research|            (2024) 3:66 8\n(Pr3 + Whisper with transcript transformation) achieved sig-\nniﬁcantly lower error rates. These results indicate that transcript\ntransformation using the GPT model signi ﬁcantly enhances the\nextraction of depression-related features, thereby improving the\nDepRoBERTa model’s accuracy. The marked decline in performance\nwhen using raw transcripts highlights the crucial role of this trans-\nformation step in optimizing feature extraction and achieving higher\nprediction accuracy.\nTo further assess the robustness of our best-performing model without\nspeech quality assessment, we applied the nested cross-validation procedure\ndescribed in Section“Methods” to the Pr3+Whisper method. This analysis\ncombined the train and development sets (219 samples) while leaving the\ntest set untouched forﬁnal evaluation. The nested cross-validation yielded a\nmean MAE of 3.39 and a mean RMSE of 4.50 on the development set. The\nﬁnal model selected through this process (C = 10,epsilon=0 . 1 ,gamma=\n0.1,kernel=R B F )a c h i e v e da nM A Eo f4 . 5 2a n da nR M S Eo f5 . 4 7o nt h et e s t\nset. In comparison, the original evaluation of the Pr3+Whisper method\nwithout nested cross-validation showed better performance on the test set.\nThis indicates that while nested cross-validation provided a more rigorous\napproach with additional hyperparameter tuning, it did not necessarily\nenhance the model’s generalization on unseen test data. The original\nPr3 +Whisper approach appeared to maintain a better balance between the\ndevelopment and test set performance.Furthermore, it is worth noting that\nthe results from the nested cross-validation analysis cannot be directly\ncompared with other studies, as those studies exclusively trained on the train\nset and evaluated on untouched development and test sets, which is more\naligned with the original Pr3+Whisper evaluation strategy.\nOutcomes of depression evaluation using visual data\nTable 3 presents a comparison of our proposed method with previous\nstudies that utilized visual features from the DAIC or E-DAIC datasets to\npredict PHQ-8 scores. Notably, we achieved the best results regarding the\nMAE on the test set by combining AU intensities, head pose, and eye gaze\nfeatures, which are reported as LSTM-AU+pose+gaze in the table. This\ncombination includes a total of 31 features (6 head pose features, 8 eye gaze\nfeatures, and 17 AU intensities), resulting in an MAE of 4.22 and RMSE of\n4.98, outperforming other feature combinations, such as using only AU\nintensities. Although Fang et al.\n63 reported a lower MAE of 4.12, as men-\ntioned earlier, a direct comparison is challenging due to differences in\ndatasets and evaluation sets. On the development set, our study yielded an\nMAE of 4.74 and an RMSE of 5.66. Notable exceptions to our results are\nseveral studies that achieved better scores. Yang et al. (2016) achieved an\nMAE of 3.19 and RMSE of 4.29\n64. Yang et al. obtained an RMSE of 5.4065.\nAdditionally, Sun et al.66,S o n ge ta l .67,a n dD ue ta l .68 achieved MAEs of 4.60,\n4.37, and 4.61, respectively. However, these studies, which utilized the DAIC\ndataset, are not directly comparable to our study due to the differences in\ndatasets.\nOutcomes of depression evaluation using multimodal data\nTable 4 illustrates the results of our multimodal method alongside previous\nstudies that have considered text andvideo-based features for predicting\nPHQ-8 scores on the DAIC or E-DAIC datasets. As shown in the table, we\nconducted two analyses. Theﬁrst analysis is based on the best-performing\ntext-based model without incorporating speech quality assessment. The\nsecond analysis includes speech quality assessment and is based on the most\nsuccessful model in this regard. We refer to these methods as LSTM-\nAU +pose +gaze +Pr3 +Whisper and LSTM-AU +pose +gaze +\nPr3 +Whisper+AudioQual, respectively. Using the LSTM-AU +\npose +gaze +Pr3 +Whisper method, we achieved an MAE of 3.31 and an\nRMSE of 4.65 on the development set, and an MAE of 4.16 and an RMSE of\n4.99 on the test set. With the LSTM-AU +pose +gaze +Pr3 +\nWhisper+AudioQual method, the MAE was 3.01 on the development set\nand 3.76 on the test set, while the RMSE was 4.18 on the development set and\n4.53 on the test set. These results outperform our video-only approach\n(LSTM-AU+pose +gaze) on both the development and test sets.\nHowever, compared to the text-based methods, the multimodal models\nshow worse performance on the development set but achieve slightly better\nerror metrics on the test set. Notably, we compare the LSTM-\nAU +pose +gaze +Pr3 +Whisper model with the Pr3 +Whisper\nmethod and the LSTM-AU+pose +gaze +Pr3 +Whisper+AudioQual\nmodel with the Pr3+Whisper+AudioQual method. This ensures a fair\ncomparison, as the text-based components are consistent within each pair of\nmethods.\nThe exploration of both video and text modalities simultaneously has\nb e e nr e l a t i v e l yl i m i t e di np r e v i o u ss t u d i e s .A ss h o w ni nT a b l e4, Ray et al.\n40\nTable 4 | Performance comparison of PHQ-8 score prediction\nmodels using textual and visual features from the DAIC (or E-\nDAIC) dataset\nMethod Dataset MAE\n(deva)\nRMSE\n(dev)\nMAE\n(test)\nRMSE\n(test)\nRay et al.40 E-DAIC – 4.64 ––\nQureshi et al.69 DAIC –– 3.65 5.11\nFang et al.63 DAIC –– 3.36 4.48\nLSTM-AU+pose\n+gaze+\nPr3+Whisper E-DAIC 3.31 4.65 4.16 4.99\nLSTM-AU+pose\n+gaze+\nPr3+Whisper\n+AudioQual\nE-DAIC 3.01 4.18 3.76 4.53\nThe best-performing results of this study and previous results that outperformed our results are\nhighlighted in bold.‘–’indicates that the respective metrics were not reported in the publication.\naDevelopment.\nTable 3 | Performance comparison of PHQ-8 score prediction\nmodels using visual features from DAIC (or E-DAIC) dataset\nMethod Dataset MAE\n(deva)\nRMSE\n(dev)\nMAE\n(test)\nRMSE\n(test)\nNasir et al.30 DAIC 6.48 7.86 ––\nValstar et al.80 DAIC 5.88 7.13 6.12 6.97\nYang et al.642 DAIC 3.19 4.29 ––\nWilliamson et al.29 DAIC 5.33 6.45 ––\nYang et al.65,b DAIC 4.75 5.40 ––\nSun et al.66 DAIC 4.60 5.90 ––\nDang et al.81 DAIC 5.33 6.67 ––\nRingeval et al.82 DAIC –– 6.12 6.97\nStepanov et al.32 DAIC –– 5.36 6.72\nSong et al.67 DAIC 4.37 5.84 ––\nRingeval et al.27 E-DAIC – 7.02 – 10.00\nDu et al.68 DAIC 4.61 5.78 ––\nMakiuchi et al.37 E-DAIC – 5.74 ––\nRay et al.40 E-DAIC – 5.70 ––\nQureshi et al.69 DAIC –– 5.06 6.53\nGupta et al.83 DAIC –– 5.30 6.26\nFang et al.63 DAIC –– 4.12 5.44\nLSTM-\nAU + pose + gaze\nE-DAIC 4.74 5.66 4.22 4.98\nThe best-performing results of this study and previous results that outperformed our results are\nhighlighted in bold.‘–’indicates that the respective metrics were not reported in the publication.\naDevelopment.\nbThis study reported separate results for males and females, which we averaged for comparison.\nhttps://doi.org/10.1038/s44184-024-00112-8 Article\nnpj Mental Health Research|            (2024) 3:66 9\nonly reported an RMSE of 4.64 on the development set of the E-DAIC\ndataset without providing additional error metrics. In addition, Qureshi\net al.69 achieved an MAE of 3.65 and RMSE of 5.11 on the test set of the\nDAIC dataset, while Fang et al.63 reported an MAE of 3.36 and RMSE of\n4.48. However, due to the variability in datasets, comparison with these two\nmentioned studies becomes challenging. Similarly, Qureshi et al.69 also\nobserved that integrating text and video modalities did not necessarily lead\nto the best results and found that relying solely on the text modality yielded\nsuperior results.\nIn an effort to rigorously validate the robustness of our best-performing\nmodel, excluding speech quality assessment, we conducted nested cross-\nvalidation on the LSTM-AU+pose+gaze+Pr3+Whisper approach. The\nnested cross-validation process resulted in a mean MAE of 3.42 and a mean\nRMSE of 4.54 on the development set. Theﬁnal model selected from this\nprocedure (C =1 0 ,epsilon=1 ,gamma =0 . 1 ,kernel= R B F )a c h i e v e da n\nMAE of 4.22 and an RMSE of 5.08 on the test set. Notably, the original\nevaluation of the LSTM-AU +pose +gaze +Pr3 +Whisper method\nwithout nested cross-validation demonstrated superior performance on the\ntest set. This difference suggests thatwhile nested cross-validation provides\nthorough hyperparameter optimization, it may lead to slight overﬁtting,\nreducing generalization to the test set. The original model, with a consistent\ntrain-test split, likely better captured the dataset’s structure, leading to more\nstable test performance.\nTo further illustrate the performance of our multimodal approach,\nFig.6 presents the distribution of MAE scores across the train, development,\nand test sets using the LSTM-AU +pose +gaze +Pr3 +Whisper+\nAudioQual method. The plot reveals that the median MAE is highest for the\ntest set, followed by the train set, and then the development set. This indi-\ncates that while the model generalizes reasonably well, it faces slightly greater\nchallenges when applied to the test set. The train set exhibits a narrower\nrange of MAE scores, indicating more stable performance during training.\nIn contrast, the wider distributions in the development and test sets suggest\nthat the model experiences greater variability in its predictions on new data.\nSince errors in the train set are not substantially lower than in the other sets,\nit indicates that the model is not overﬁtting. This balance suggests that the\nmodel maintains good generalizationwithout being overly optimized for the\ntraining data. Additionally, the broader range and presence of outliers in the\ndevelopment and test sets imply that certain data points are more chal-\nlenging for the model to predict accurately. A major factor contributing to\nthis variability is the distributiono fP H Q - 8s c o r e sw i t h i nt h ed a t a s e t ,a s\ns h o w ni nF i g .3. High PHQ-8 scores are relatively rare, leading to an\nimbalance across the sets. This scarcity of samples with severe depression\nscores makes it more challenging for the model to predict higher PHQ-8\nscores accurately, thereby increasing error variability. Differences in input\nfeature quality and the inherent complexity of certain samples contribute to\nprediction errors. Additionally, the scarcity of high-score instances hinders\nthe model’s ability to generalize, highlighting the need for strategies to\naddress data imbalances.\nAfter assessing the model’s performance across different sets, it was\nalso essential to identify which features had the most inﬂ\nuence on the\npredictions. To this end, we performed a SHapley Additive exPlanations\n(SHAP)70 analysis to highlight the most impactful features. As shown in\nFig. 7, this analysis illustrates the relative importance of both text-based\nand visual features in predicting PHQ-8 scores, focusing on the top 10\nFig. 7 | SHAP analysis of the top 10 features in the\nmultimodal approach.‘Q’labels indicate questions\nextracted via the LLM-driven method.‘Not\ndepression,’‘Moderate depression,’and ‘Severe\ndepression’are DepRoBERTa-derived indicators.\n‘LSTM extracted 1’is theﬁrst component of a 128-\ndimensional feature vector from the visual modality\nusing the LSTM model.\nFig. 6 | Mean absolute error (MAE) score distribution in the train, development,\nand test sets for the proposed multimodal approach.The box plots display the\nmedian (horizontal line within each box), the interquartile range (IQR; the bounds of\nthe box), and whiskers extending to 1.5× IQR from the box edges. Each point\nrepresents the MAE for an individual sample, providing a detailed view of the\nsample-level variability within each dataset. They-axis reﬂects the MAE values,\nwhere lower scores indicate better performance.\nhttps://doi.org/10.1038/s44184-024-00112-8 Article\nnpj Mental Health Research|            (2024) 3:66 10\nselected features for the multimodal model. The results indicate that the\nmost inﬂuential features are‘Not depression’and ‘Severe depression,’both\nextracted by the DepRoBERTa model. As seen in the plot, higher values of\nthe ‘Not depression’feature are associated with lower predicted PHQ-8\nscores, whereas higher values of the‘Severe depression’ and ‘Moderate\ndepression’features result in higher predicted scores. This suggests that\nthe model correctly interprets stronger indicators of depression as leading\nto higher severity scores. Among the text-based features derived from our\nLLM-driven question-based method, the feature‘Q10,’ related to the\nquestion on suicidal thoughts, is particularly impactful. The SHAP ana-\nlysis shows that when the value of this feature increases, the predicted\nPHQ-8 score also rises, indicating its strong association with higher\ndepression severity. Additionally, the features labeled‘Q1,’‘Q2,’and so on,\ncorrespond to the responses to each respective question in our feature\nextraction process, with‘Q1’derived from theﬁrst question,‘Q2’from the\nsecond, and so forth. Notably, the only feature extracted from the visual\nmodality (‘LSTM extracted 1’) appears at the bottom of the list, suggesting\nthat visual features have a much lower impact on the model’s predictions\ncompared to textual features. This observation aligns with the earlier\nfeature selection process, where 9 out of the top 10 features were text-\nbased, with only one derived from visual data. Theseﬁndings underscore\nthat, within our multimodal framework, text-based features are far more\ninﬂuential in predicting depression severity, while visual features con-\ntribute to a lesser extent.\nToward more effective multimodal depression detection: limita-\ntions, insights, and future directions\nAs discussed in the previous section,our multimodal models demonstrate\nbetter performance on the test set compared to text-only approaches, while\ntext-only models perform better on the development set. Additionally, the\nmultimodal models outperform the video-only approach on both the\ndevelopment and test sets. To better understand this pattern, it is insightful\nto examine the performance of the video-only approach. As shown in Table\n3, the video-only model achieves better error metrics on the test set com-\npared to the development set. In contrast, Table2 reveals that for all text-\nbased models, the error metrics are consistently better on the development\nset than on the test set. This suggests that integrating text and video enables\nthe multimodal model to achieve better performance on the test set than the\ntext-only model, although the improvements are marginal. One possible\nexplanation for the performance difference between the development and\ntest sets across different data types may be related to the collection process of\nthe E-DAIC dataset. The test set consists exclusively of interviews conducted\nby an autonomous AI interviewer, whereas the development set includes a\nmix of interviews controlled by both a human (Wizard-of-Oz) and the AI\n25.\nThis distinction could introduce a distribution shift, affecting how the\nmodels perform across the two sets. The presence of a human interviewer in\nthe development set may result in richer and more engaging interview\ntranscripts, thereby enabling text-based models to perform better on the\ndevelopment set. Conversely, the autonomous AI in the test set might lead to\nless expressive or less detailed responses, diminishing the effectiveness of\ntext-based features.\nDespite the multimodal approach showing better performance on the\ntest set compared to text-only models, the improvements remain modest.\nFeature importance analysis highlights that text data is a crucial component\nin the model’s predictive power. Text data, especially from sources like social\nmedia posts, therapy transcripts, and personal journals, often contains\nexplicit and detailed information about emotional states and thought pro-\ncesses. Symptoms of depression, such as hopelessness, worthlessness, and\nself-deprecating thoughts, are often directly articulated in language, pro-\nviding clear indicators for detection models\n71–73. However, integrating text\nand video data in a multimodal approach introduces additional complex-\nities. Aligning and combining information from different modalities\nrequires sophisticated techniques for temporal synchronization, feature\nscaling, and data fusion, which may not always be optimal. This integration\ncan introduce noise andredundancy, where conﬂicting information from\none modality can adversely affect overall model performance. These chal-\nlenges help explain why text-based models often outperform video-based\nand multimodal models in our study. In contrast, video data presents\nadditional challenges\n74. Non-verbal cues, such as facial expressions and body\nlanguage, may vary greatly among individuals and situations, making them\ndifﬁcult to interpret accurately. Moreover, facial expressions might not\nalways reﬂect true emotional states; for instance, someone could smile while\ndiscussing distressing experiences. Extracting meaningful features from\nvideo involves complex tasks such as facial expression analysis, gesture\nrecognition, and emotional state detection, which can be error-prone due to\nvariations in lighting, camera angles, and individual differences in\nexpressiveness\n74,75. Additionally, the robustness of video-based models can\nbe compromised by the noisy and variable nature of video data, which may\ncontain irrelevant or redundant information. The strengths of LLMs lie in\ntheir ability to identify the most relevant parts of interview transcripts\nrelated to depression or mental health. However, such methods have yet to\nbe effectively applied to video data. One suggestion for future work is to\nleverage state-of-the-art LLMs toﬁrst identify the most critical segments of\nan interview related to depression from text data. Subsequently, these key\nsegments could be mapped to the corresponding video frames, allowing the\nanalysis to focus only on speciﬁc video portions. This targeted approach\nc o u l dr e d u c en o i s ea n di m p r o v et h eeffectiveness of multimodal models.\nIn addition to these challenges, a notable limitation of the E-DAIC\ndataset is the relatively small number of samples with high PHQ-8 scores.\nThis means that when the model is trained on such limited high-score\nsamples, its performance on the test set, especially with high PHQ-8 score\nsamples not sufﬁciently represented during training, can be suboptimal. To\nmaintain comparability with previous studies, we intentionally did not alter\nthe dataset structure using techniqueslike oversampling or undersampling.\nFuture research could beneﬁt from collecting more balanced datasets that\ninclude a representative number of high PHQ-8 score samples, allowing\nmodels to be trained and evaluated more effectively across all levels of\ndepression severity. Such balanced datasets could help improve model\nperformance and generalizability byproviding a clearer understanding of\nvarying depressive symptoms. To address these limitations, we are con-\nducting a randomized-controlled trial\n76 within the Collaborative Research\nCenter (CRC 1483)“EmpkinS” (Empatho-Kinesthetic Sensor Technology\n— Sensor Techniques and Data Analysis Methods for Empatho-Kinesthetic\nModeling and Condition Monitoring). This trial aims to establish a com-\nprehensive dataset with balanced samples representing various levels of\ndepressive symptoms: none, mild, moderate, and severe. The dataset will\ncomprise extensive video, audio, and biosignal recordings, including elec-\ntromyography (EMG) to measure muscle activity, electrocardiography\n(ECG) to monitor heart activity, and respiratory signals (RSP) to track\nbreathing patterns. Our objective is to analyze these multimodal data\nstreams to better understand the links between body language, physical\nbehavior, and depressive symptoms. The insights gained from this research\ncould contribute to the development of more accurate depression detection\nmodels, ultimately supporting moreeffective and personalized mental\nhealth interventions.\nBeyond addressing the technical challenges and opportunities in\nmultimodal depression detection, it isv i t a lt oc o n s i d e rt h eb r o a d e ri m p l i -\ncations of integrating AI technologiesinto healthcare. Our study illustrates\nthe potential of AI tools in detecting depression through a publicly available,\nanonymized dataset. Theﬁndings emphasize the capability of LLMs and\nvisual cues in identifying depressivesymptoms. However, it is important to\nrecognize the limitations of these tools and approach their integration into\nclinical practice carefully. While AI can enhance screening processes and\nsupport healthcare professionals, it is not meant to replace human judg-\nment. Thus, considering the ethical implications and potential biases of\nincorporating AI technology into healthcare is essential.\nData availability\nThe dataset used in this study is available upon request athttps://dcapswoz.\nict.usc.edu/.\nhttps://doi.org/10.1038/s44184-024-00112-8 Article\nnpj Mental Health Research|            (2024) 3:66 11\nReceived: 22 June 2024; Accepted: 13 December 2024;\nReferences\n1. Institute of Health Metrics and Evaluation. Global Health Data\nExchange (GHDx).https://vizhub.healthdata.org/gbd-results/.\nAccessed on 3 June 2024.\n2. World Health Organization. Depression. https://www.who.int/news-\nroom/fact-sheets/detail/depression. Accessed on 3 June 2024.\n3. Woody, C., Ferrari, A., Siskind, D., Whiteford, H. & Harris, M. A\nsystematic review and meta-regression of the prevalence and\nincidence of perinatal depression.J. Affect. Disord.219,8 6–92 (2017).\n4. Evans-Lacko, S. et al. Socio-economic variations in the mental health\ntreatment gap for people with anxiety, mood, and substance use\ndisorders: results from the who world mental health (wmh) surveys.\nPsychol. Med.48, 1560–1571 (2018).\n5. Kroenke, K. et al. The phq-8 as a measure of current depression in the\ngeneral population.J. Affect. Disord.114, 163–173 (2009).\n6. American Psychiatric Association. Diagnostic and Statistical Manual\nof Mental Disorders(American Psychiatric Publishing, Washington,\nDC, 2013), 5th edn.\n7. OpenAI. Hello, GPT-4. https://openai.com/index/hello-gpt-4o.\nAccessed on 3 June 2024.\n8. Deshpande, M. & Rao, V. Depression detection using emotion artiﬁcial\nintelligence. In2017 International Conference on Intelligent\nSustainable Systems (ICISS), 858–862 (IEEE, 2017).\n9. X Corp. X (formerly Twitter)https://x.com/ (2024). Accessed on 4 June\n2024.\n10. Yazdavar, A. H. et al. Semi-supervised approach to monitoring clinical\ndepressive symptoms in social media. InProceedings of the 2017\nIEEE/ACM International Conference on Advances in Social Networks\nAnalysis and Mining 2017, 1191–1198 (2017).\n11. Trotzek, M., Koitka, S. & Friedrich, C. M. Utilizing neural networks and\nlinguistic metadata for early detection of depression indications in text\nsequences. IEEE Trans. Knowl. Data Eng.32, 588–601 (2018).\n12. Islam, M. R. et al. Depression detection from social network data using\nmachine learning techniques.Health Inf. Sci. Syst.6,1 –12 (2018).\n13. Meta Platforms, Inc. Facebookhttps://www.facebook.com/ (2024).\nAccessed on 4 June 2024.\n14. Orabi, A. H., Buddhitha, P., Orabi, M. H. & Inkpen, D. Deep learning for\ndepression detection of Twitter users. InProceedings of the Fifth\nWorkshop on Computational Linguistics and Clinical Psychology:\nFrom Keyboard to Clinic,8 8–97 (2018).\n15. Cacheda, F., Fernandez, D., Novoa, F. J. & Carneiro, V. Early detection\nof depression: social network analysis and random forest techniques.\nJ. Med. Internet Res.21, e12554 (2019).\n16. Tadesse, M. M., Lin, H., Xu, B. & Yang, L. Detection of depression-\nrelated posts in Reddit social media forum.IEEE Access7,\n44883–44893 (2019).\n17. Reddit Inc. Reddithttps://www.reddit.com/ (2024). Accessed on 4\nJune 2024.\n18. Burdisso, S. G., Errecalde, M. & Montes-y Gómez, M. A text\nclassiﬁcation framework for simple and effective early depression\ndetection over social media streams.Expert Syst. Appl.133, 182–197\n(2019).\n19. Guo, Y. et al. A prompt-based topic-modeling method for depression\ndetection on low-resource data.IEEE Transactions on Computational\nSocial Systems(2023).\n20. Pérez, A., Warikoo, N., Wang, K., Parapar, J. & Gurevych, I. Semantic\nsimilarity models for depression severity estimation. InProceedings of\nthe 2023 Conference on Empirical Methods in Natural Language\nProcessing, 16104–16118 (Association for Computational\nLinguistics, Singapore, 2023).\n21. Beck, A. T., Steer, R. A. & Brown, G. K.Beck Depression Inventory:\nBDI-II: Manual(Psychological Corporation, New York, 1996).\n22. Nguyen, T., Yates, A., Zirikly, A., Desmet, B. & Cohan, A. Improving the\ngeneralizability of depression detection by leveraging clinical\nquestionnaires. InProceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers),\n8446–8459 (Association for Computational Linguistics, Dublin,\nIreland, 2022).\n23. Kroenke, K., Spitzer, R. L. & Williams, J. B. The phq-9: validity of a brief\ndepression severity measure.J. Gen. Intern. Med.16, 606–613 (2001).\n24. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. Bert: pre-training of\ndeep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805 (2018).\n25. Gratch, J. et al. The distress analysis interview corpus of human and\ncomputer interviews. InLREC, 3123–3128 (Reykjavik, 2014).\n26. DeVault, D. et al. Simsensei kiosk: A virtual human interviewer for\nhealthcare decision support. InProceedings of the 2014 International\nConference on Autonomous Agents and Multi-Agent Systems,\n1061–1068 (2014).\n27. Ringeval, F. et al. Avec 2019 workshop and challenge: state-of-mind,\ndetecting depression with AI, and cross-cultural affect recognition. In\nProceedings of the 9th International on Audio/visual Emotion\nChallenge and Workshop,3 –12 (2019).\n28. Gong, Y. & Poellabauer, C. Topic modeling based multi-modal\ndepression detection. InProceedings of the 7th Annual Workshop on\nAudio/Visual Emotion Challenge,6 9–76 (2017).\n29. Williamson, J. R. et al. Detecting depression using vocal, facial and\nsemantic communication cues. InProceedings of the 6th International\nWorkshop on Audio/Visual Emotion Challenge,1 1–18 (2016).\n30. Nasir, M., Jati, A., Shivakumar, P. G., Nallan Chakravarthula, S. &\nGeorgiou, P. Multimodal and multiresolution depression detection\nfrom speech and facial landmark features. InProceedings of the 6th\ninternational workshop on audio/visual emotion challenge,4 3–50\n(2016).\n31. Al Hanai, T., Ghassemi, M. M. & Glass, J. R. Detecting depression with\naudio/text sequence modeling of interviews. InInterspeech,\n1716–1720 (2018).\n32. Stepanov, E. A. et al. Depression severity estimation from multiple\nmodalities. In 2018 ieee 20th International Conference on e-Health\nNetworking, Applications and Services (healthcom),1\n–6 (IEEE,\n2018).\n33. Fan, W., He, Z., Xing, X., Cai, B. & Lu, W. Multi-modality depression\ndetection via multi-scale temporal dilated cnns. InProceedings of the\n9th International on Audio/Visual Emotion Challenge and Workshop,\n73–80 (2019).\n34. Yin, S., Liang, C., Ding, H. & Wang, S. A multi-modal hierarchical\nrecurrent neural network for depression detection. InProceedings of\nthe 9th International on Audio/Visual Emotion Challenge and\nWorkshop,6 5–71 (2019).\n35. Shen, Y., Yang, H. & Lin, L. Automatic depression detection: An\nemotional audio-textual corpus and a gru/bilstm-based model. In\nICASSP 2022-2022 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), 6247–6251 (IEEE, 2022).\n36. Prabhu, S., Mittal, H., Varagani, R., Jha, S. & Singh, S. Harnessing\nemotions for depression detection.Pattern Analysis and Applications\n1–11 (2022).\n37. Rodrigues Makiuchi, M., Warnita, T., Uto, K. & Shinoda, K. Multimodal\nfusion of bert-cnn and gated cnn representations for depression\ndetection. InProceedings of the 9th International on Audio/Visual\nEmotion Challenge and Workshop,5 5–63 (2019).\n38. Simonyan, K. & Zisserman, A. Very deep convolutional networks for\nlarge-scale image recognition. 3rd International Conference on\nLearning Representations (ICLR 2015) (Computational and\nBiologicalLearning Society, 2015).\n39. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image\nrecognition. InProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 770–778 (2016).\nhttps://doi.org/10.1038/s44184-024-00112-8 Article\nnpj Mental Health Research|            (2024) 3:66 12\n40. Ray, A., Kumar, S., Reddy, R., Mukherjee, P. & Garg, R. Multi-level\nattention network using text, audio and video for depression\nprediction. InProceedings of the 9th International on Audio/Visual\nEmotion Challenge and Workshop,8 1–88 (2019).\n41. Sadeghi, M. et al. Exploring the capabilities of a language model-only\napproach for depression detection in text data. In2023 IEEE EMBS\nInternational Conference on Biomedical and Health Informatics (BHI),\n1–5 (IEEE, 2023).\n42. Baltru š aitis, T., Robinson, P. & Morency, L.-P. Openface: an open\nsource facial behavior analysis toolkit. In2016 IEEE Winter\nConference on Applications of Computer Vision (WACV),1 –10 (IEEE,\n2016).\n43. Eyben, F., Wöllmer, M. & Schuller, B. Opensmile: the Munich versatile\nand fast open-source audio feature extractor. InProceedings of the\n18th ACM International Conference on Multimedia, 1459–1462 (2010).\n44. Radford, A. et al. Robust speech recognition via large-scale weak\nsupervision. InInternational Conference on Machine Learning,\n28492–28518 (PMLR, 2023).\n45. Panayotov, V., Chen, G., Povey, D. & Khudanpur, S. Librispeech: an\nasr corpus based on public domain audio books. In2015 IEEE\nInternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 5206–5210 (IEEE, 2015).\n46. Gerganov, G. Issues with word duplication.Github https://github.\ncom/ggerganov/whisper.cpp/issues/896. Accessed on 30 October\n2024.\n47. OpenAI. Gpt-3.5 model documentation.https://platform.openai.com/\ndocs/models/gpt-3-5. Accessed on 1 April 2024.\n48. Liu, Y. et al. Roberta: A robustly optimized Bert pretraining approach.\nPreprint atarXiv https://doi.org/10.48550/arXiv.1907.11692 (2019).\n49. Po świata, R. & Perełkiewicz, M. Opi@ lt-edi-acl2022: Detecting signs\nof depression from social media text using Roberta pre-trained\nlanguage models. InProceedings of the Second Workshop on\nLanguage Technology for Equality, Diversity and Inclusion, 276–282\n(2022).\n50. depRoberta-large depression. Hugging face.https://huggingface.co/\nrafalposwiata/deproberta-large-depression. Accessed on 1 April\n2024.\n51. Pedregosa, F. et al. Scikit-learn: machine learning in Python.J. Mach.\nLearn. Res.12, 2825–2830 (2011).\n52. Mittag, G., Naderi, B., Chehadi, A. & Möller, S. Nisqa: A deep cnn-self-\nattention model for multidimensional speech quality prediction with\ncrowdsourced datasets. Preprint atarXiv https://doi.org/10.48550/\narXiv.2104.09494 (2021).\n53. Mittag, G.Nisqa. https://github.com/gabrielmittag/NISQA. Accessed\non 3 June 2024.\n54. Mittag, G. & Möller, S. Deep learning based assessment of synthetic\nspeech naturalness. Preprint atarXiv https://doi.org/10.48550/arXiv.\n2104.11673 (2021).\n55. Ekman, P. & Friesen, W. V. Facial action coding system. Environmental\nPsychology & NonverbalBehavior (APA PsycTests, 1978).\n56. Zadeh, A., Chong Lim, Y., Baltrusaitis, T. & Morency, L.-P.\nConvolutional experts constrained local model for 3d facial landmark\ndetection. InProceedings of the IEEE International Conference on\nComputer Vision Workshops, 2519–2528 (2017).\n57. Baltrusaitis, T., Robinson, P. & Morency, L.-P. Constrained local\nneural ﬁelds for robust facial landmark detection in the wild. In\nProceedings of the IEEE International Conference on Computer Vision\nWorkshops, 354–361 (2013).\n58. Wood, E. et al. Rendering of eyes for eye-shape registration and gaze\nestimation. InProceedings of the IEEE International Conference on\nComputer Vision, 3756–3764 (2015).\n59. iMotions. Facial action coding system (facs) (2024).https://imotions.\ncom/blog/learning/research-fundamentals/facial-action-coding-\nsystem/. Accessed on 17 June 2024.\n60. Baltrusaitis, T. et al.Openface 2.2.0: A Facial Behavior Analysis\nToolkit. https://github.com/TadasBaltrusaitis/OpenFace. Accessed\non 10 May 2024.\n61. Schuster, M. & Paliwal, K. K. Bidirectional recurrent neural networks.\nIEEE Trans. Signal Process.45, 2673–2681 (1997).\n62. Hochreiter, S. & Schmidhuber, J. Long short-term memory.Neural\nComput. 9, 1735–1780 (1997).\n63. Fang, M., Peng, S., Liang, Y., Hung, C.-C. & Liu, S. A multimodal fusion\nmodel with multi-level attention mechanism for depression detection.\nBiomed. Signal Process. Control82, 104561 (2023).\n64. Yang, L. et al. Decision tree based depression classiﬁcation from\naudio video and language information. InProceedings of the 6th\ninternational workshop on audio/visual emotion challenge,8 9–96\n(2016).\n65. Yang, L. et al. Multimodal measurement of depression using deep\nlearning models. InProceedings of the 7th Annual Workshop on\nAudio/Visual Emotion Challenge,5 3–59 (2017).\n66. Sun, B. et al. A random forest regression method with selected-text\nfeature for depression assessment. InProceedings of the 7th Annual\nWorkshop on Audio/Visual Emotion Challenge,6 1–68 (2017).\n67. Song, S., Shen, L. & Valstar, M. Human behaviour-based automatic\ndepression analysis using hand-crafted statistics and deep learned\nspectral features. In2018 13th IEEE International Conference on\nAutomatic Face & Gesture Recognition (FG 2018), 158–165 (IEEE,\n2018).\n68. Du, Z., Li, W., Huang, D. & Wang, Y. Encoding visual behaviors with\nattentive temporal convolution for depression prediction. In2019 14th\nIEEE International Conference on Automatic Face & Gesture\nRecognition (FG 2019),1 –7 (IEEE, 2019).\n69. Qureshi, S. A., Hasanuzzaman, M., Saha, S. & Dias, G. The verbal and\nnon verbal signals of depression–combining acoustics, text and\nvisuals for estimating depression level. Preprint atarXiv https://doi.\norg/10.48550/arXiv.1904.07656 (2019).\n70. Lundberg, S. M. & Lee, S.-I. A uniﬁed approach to interpreting model\npredictions. InAdvances in Neural Information Processing Systems,\nvol. 30, 4765–4774 (Curran Associates, Inc., 2017).\n71. Coppersmith, G., Dredze, M. & Harman, C. Quantifying mental health\nsignals in Twitter. InProceedings of the Workshop on Computational\nLinguistics and Clinical Psychology: From Linguistic Signal to Clinical\nReality,5 1–60 (2014).\n72. De Choudhury, M., Gamon, M., Counts, S. & Horvitz, E. Predicting\ndepression via social media. InProceedings of the International AAAI\nConference on Web and Social Media, vol. 7, 128–137 (2013).\n73. Gkotsis, G. et al. Characterisation of mental health conditions in social\nmedia using informed deep learning.Sci. Rep.7,1 –11 (2017).\n74. Gimeno-Gómez, D., Bucur, A.-M., Cosma, A., Martínez-Hinarejos, C.-\nD. & Rosso, P. Reading between the frames: Multi-modal depression\ndetection in videos from non-verbal cues. InEuropean Conference on\nInformation Retrieval, 191–209 (Springer, 2024).\n75. Yadav, U., Sharma, A. K. & Patil, D. Review of automated depression\ndetection: social posts, audio and video, open challenges and future\ndirection. Concurr. Comput.35, e7407 (2023).\n76. Keinert, M. et al. Facing depression: Evaluating the efﬁcacy of the\nEmpkinS-EKSpression reappraisal training augmented with facial\nexpressions - protocol of a randomized controlled trial.BMC\nPsychiatry 24, 896 (2024).\n77. Oureshi, S. A., Dias, G., Saha, S. & Hasanuzzaman, M. Gender-aware\nestimation of depression severity level in a multimodal setting. In2021\nInternational Joint Conference on Neural Networks (IJCNN),1 –8 (IEEE,\n2021).\n78. Niu, M., Chen, K., Chen, Q. & Yang, L. Hcag: A hierarchical context-\naware graph attention model for depression detection. InICASSP\n2021-2021 IEEE international conference on acoustics, speech and\nsignal processing (ICASSP), 4235–4239 (IEEE, 2021).\nhttps://doi.org/10.1038/s44184-024-00112-8 Article\nnpj Mental Health Research|            (2024) 3:66 13\n79. Rohanian, M., Hough, J., Purver, M. et al. Detecting depression with\nword-level multimodal fusion. InInterspeech, 1443–1447 (2019).\n80. Valstar, M. et al. Avec 2016: Depression, mood, and emotion\nrecognition workshop and challenge. InProceedings of the 6th\nInternational Workshop on Audio/Visual Emotion Challenge,3 –10\n(2016).\n81. Dang, T. et al. Investigating word affect features and fusion of\nprobabilistic predictions incorporating uncertainty in avec 2017. In\nProceedings of the 7th Annual Workshop on Audio/Visual Emotion\nChallenge,2 7–35 (2017).\n82. Ringeval, F. et al. Avec 2017: Real-life depression, and affect\nrecognition workshop and challenge. InProceedings of the 7th Annual\nWorkshop on Audio/Visual Emotion Challenge,3 –9 (2017).\n83. Kumar Gupta, R. & Sinha, R. An investigation on the audio-video data\nbased estimation of emotion regulation difﬁculties and their association\nwith mental disorders.IEEE Access11, 74324–74336 (2023).\nAcknowledgements\nThis work was funded by the Deutsche Forschungsgemeinschaft (DFG,\nGerman Research Foundation)— SFB 1483— Project-ID 442419336, EmpkinS.\nAuthor contributions\nM.S. led the study design, conducted the data analysis, and drafted the\npaper andﬁgures. R.R. contributed to the study design, assisted with data\nanalysis, and co-authored the paper. L.S.G., L.H.R., and F.R. helped in\nwriting speciﬁc sections of the paper. B.M.E. and B.E. provided technical\nsupervision, while M.B. offered psychological supervision. All authors\nthoroughly reviewed and approved theﬁnal paper.\nFunding\nOpen Access funding enabled and organized by Projekt DEAL.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s44184-024-00112-8\nCorrespondenceand requests for materials should be addressed to\nMisha Sadeghi.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long\nas you give appropriate credit to the original author(s) and the source,\nprovide a link to the Creative Commons licence, and indicate if changes\nwere made. The images or other third party material in this article are\nincluded in the article’s Creative Commons licence, unless indicated\notherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons licence and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to\nobtain permission directly from the copyright holder. To view a copy of this\nlicence, visithttp://creativecommons.org/licenses/by/4.0/\n.\n© The Author(s) 2024\nhttps://doi.org/10.1038/s44184-024-00112-8 Article\nnpj Mental Health Research|            (2024) 3:66 14",
  "topic": "Depression (economics)",
  "concepts": [
    {
      "name": "Depression (economics)",
      "score": 0.6848978996276855
    },
    {
      "name": "Facial expression",
      "score": 0.6625431776046753
    },
    {
      "name": "Computer science",
      "score": 0.6160156726837158
    },
    {
      "name": "Mean squared error",
      "score": 0.5653587579727173
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5221798419952393
    },
    {
      "name": "Patient Health Questionnaire",
      "score": 0.47721177339553833
    },
    {
      "name": "Machine learning",
      "score": 0.4297364354133606
    },
    {
      "name": "Natural language processing",
      "score": 0.413308322429657
    },
    {
      "name": "Psychology",
      "score": 0.2740917205810547
    },
    {
      "name": "Anxiety",
      "score": 0.26284462213516235
    },
    {
      "name": "Statistics",
      "score": 0.23661112785339355
    },
    {
      "name": "Depressive symptoms",
      "score": 0.17605739831924438
    },
    {
      "name": "Psychiatry",
      "score": 0.14915058016777039
    },
    {
      "name": "Mathematics",
      "score": 0.10660392045974731
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Macroeconomics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I181369854",
      "name": "Friedrich-Alexander-Universität Erlangen-Nürnberg",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I3018134672",
      "name": "Helmholtz Zentrum München",
      "country": "DE"
    }
  ],
  "cited_by": 18
}