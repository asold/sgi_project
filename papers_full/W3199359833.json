{
    "title": "Efficient Domain Adaptation of Language Models via Adaptive Tokenization",
    "url": "https://openalex.org/W3199359833",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2902365525",
            "name": "Vin Sachidananda",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A2109560042",
            "name": "Jason Kessler",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A2124327469",
            "name": "Yi-An Lai",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3099008231",
        "https://openalex.org/W4299567010",
        "https://openalex.org/W2963809228",
        "https://openalex.org/W2963355221",
        "https://openalex.org/W3099178230",
        "https://openalex.org/W3015453090",
        "https://openalex.org/W2971292190",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W3103671331",
        "https://openalex.org/W2108240007",
        "https://openalex.org/W2937845937",
        "https://openalex.org/W2963716420",
        "https://openalex.org/W3092115807",
        "https://openalex.org/W2963718112",
        "https://openalex.org/W3046375318",
        "https://openalex.org/W3045492832",
        "https://openalex.org/W4297971002",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W3098649723",
        "https://openalex.org/W2082303464",
        "https://openalex.org/W2279376656",
        "https://openalex.org/W4294554825",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2963639288",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2945808907",
        "https://openalex.org/W2955041501",
        "https://openalex.org/W2113459411",
        "https://openalex.org/W3105601216",
        "https://openalex.org/W2970217403",
        "https://openalex.org/W3030236966",
        "https://openalex.org/W3119077874",
        "https://openalex.org/W2971008823",
        "https://openalex.org/W2808556605",
        "https://openalex.org/W3104330316",
        "https://openalex.org/W2121879602",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2038411619",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W2119595472"
    ],
    "abstract": "Contextual embedding-based language models trained on large data sets, such as BERT and RoBERTa, provide strong performance across a wide range of tasks and are ubiquitous in modern NLP. It has been observed that fine-tuning these models on tasks involving data from domains different from that on which they were pretrained can lead to suboptimal performance. Recent work has explored approaches to adapt pretrained language models to new domains by incorporating additional pretraining on domain-specific corpora and task data. We propose an alternative approach for transferring pretrained language models to new domains by adapting their tokenizers. We show that domain-specific subword sequences can be determined efficiently directly from divergences in the conditional token distributions of the base and domain-specific corpora. In datasets from four disparate domains, we find adaptive tokenization on a pretrained RoBERTa model provides greater than 85% of the performance benefits of domain specific pretraining. Our approach produces smaller models and less training and inference time than other approaches using tokenizer augmentation. Although using adaptive tokenization incurs a 6% increase in model parameters (due to the introduction of 10k new domain-specific tokens), our approach, using 64 CPUs, is >72x faster than further pretraining the language model on domain-specific corpora on 8 TPUs.",
    "full_text": "Proceedings of the 2nd Workshop on Simple and Efﬁcient Natural Language Processing, pages 155–165\nNovember 10, 2021. ©201 Association for Computational Linguistics\n155\nEfﬁcient Domain Adaptation of Language Models via Adaptive\nTokenization\nVin Sachidananda∗\nStanford University\nvsachi@stanford.edu\nJason S. Kessler\nAmazon\njasokess@amazon.com\nYi-An Lai\nAWS AI HLT\nyianl@amazon.com\nAbstract\nContextual embedding-based language mod-\nels trained on large data sets, such as BERT\nand RoBERTa, provide strong performance\nacross a wide range of tasks and are ubiq-\nuitous in modern NLP. It has been observed\nthat ﬁne-tuning these models on tasks involv-\ning data from domains different from that on\nwhich they were pretrained can lead to subop-\ntimal performance. Recent work has explored\napproaches to adapt pretrained language mod-\nels to new domains by incorporating additional\npretraining using domain-speciﬁc corpora and\ntask data. We propose an alternative approach\nfor transferring pretrained language models\nto new domains by adapting their tokeniz-\ners. We show that domain-speciﬁc subword se-\nquences can be efﬁciently determined directly\nfrom divergences in the conditional token dis-\ntributions of the base and domain-speciﬁc cor-\npora. In datasets from four disparate domains,\nwe ﬁnd adaptive tokenization on a pretrained\nRoBERTa model provides>97% of the perfor-\nmance beneﬁts of domain speciﬁc pretraining.\nOur approach produces smaller models and\nless training and inference time than other ap-\nproaches using tokenizer augmentation. While\nadaptive tokenization incurs a 6% increase in\nmodel parameters in our experimentation, due\nto the introduction of 10k new domain-speciﬁc\ntokens, our approach, using 64 vCPUs, is 72x\nfaster than further pretraining the language\nmodel on domain-speciﬁc corpora on 8 TPUs.\n1 Introduction\nPretrained language models (PLMs) trained on\nlarge “base” corpora, oftentimes >100GB of un-\ncompressed text Liu et al. (2019); Brown et al.\n(2020), are used in many NLP tasks. These models\nﬁrst learn contextual representations in an unsuper-\nvised manner by minimizing a masked language\nmodeling objective over a base corpus. This stage\nof unsupervised language model training is referred\n∗ Work done during an internship at Amazon.\nto as \"pretraining\". Subsequently, for supervised\nclassiﬁcation tasks, the output head of this pre-\ntrained model is swapped for a lightweight classi-\nﬁer and trained further on a classiﬁcation objective\nover labeled data, referred to as “ﬁne-tuning”.\nRecent work has examined the transferability of\nPLMs Gururangan et al. (2020) and their contex-\ntual representations to domains differing from their\nbase corpora. On text classiﬁcation tasks from four\ndifferent domains, it was shown that continuing to\npretrain RoBERTa’s contextual embeddings on ad-\nditional domain (DAPT) and/or task-speciﬁc data\n(TAPT) resulted in performance gains over only\nﬁne-tuning a baseline RoBERTa model. These per-\nformance gains, however, were inferior to each\ntask’s start-of-the-art metrics which were largely\nbased on training versions of RoBERTa, or other\nLMs, from scratch on a large sample of in-domain\ndata.\nThese performance gains come at substantial ﬁ-\nnancial, time, and environmental costs in the form\nof increased computation, with pretraining an LM\nfrom scratch being the most expensive, using ad-\nditional pretraining in the middle, and only ﬁne-\nturning an off-the-shelf model the most economi-\ncal.\nOne observed advantage Gu et al. (2020) that\npretraining from scratch on in-domain data has\nover continual pretraining is that the tokenizer’s\nvocabulary captures domain-speciﬁc terms. This al-\nlows semantics of those terms to be directly learned\nin their ﬁxed embeddings, and relieves the lan-\nguage model from having to encode these seman-\ntics through the contextual embeddings of these\ndomain-speciﬁc term’s subwords. Recent work\nZhang et al. (2020); Poerner et al. (2020) has shown\nadding whole words common to the target domain\nbut absent from a PLM’s tokenizer improves perfor-\nmance on single tasks. In this work, we show that\naugmenting an PLM with statistically derived sub-\nword tokens selected for domain association with\n156\nsimple embedding initializations and no further\npretraining provide an effective means of adapt-\ning a PLM across tasks and domains. In contrast,\nboth Zhang et al. (2020) and Poerner et al. (2020)\nadd inefﬁciencies by respectively requiring further\nmasked language model (MLM) pretraining and\ndoubling the resources needed for inference.\nIn this paper, we efﬁciently adapt a PLM by\nsimply augmenting its vocabulary with domain-\nspeciﬁc token sequences. We ﬁnd that this adap-\ntation, which requires no further pretraining, ri-\nvals the accuracy of domain and task-adapted pre-\ntraining approaches proposed in Gururangan et al.\n(2020) but requires only a small fraction of the\ncompute cost.\n2 Related work\nGururangan et al. (2020) describes two comple-\nmentary methods using a task’s training data or a\nseparate unlabeled domain-speciﬁc corpus to fur-\nther pretrain an LM, denoted as Task-Adaptive Pre-\ntraining (TAPT) and Domain-Adaptive Pretraining\n(DAPT) respectively. This paper shows the value of\nemploying additional in-domain data in pretraining\non four domains relative to only ﬁne-tuning a PLM.\nOur approach is directly comparable to DAPT, as\nwe only use in-domain corpora for adaptation.\nZhang et al. (2020) augment RoBERTa’s vocab-\nulary with in-domain OOV whole words. The most\nfrequently occurring whole words are added un-\ntil the OOV rate drops to 5% on the task corpus.\nThey randomly initialize weights and pretrain a\nmodel. This improves performance on TechQA\nand AskUbuntu. Tai et al. (2020) also augmented\nBERT with tokens selected by frequency (12k OOV\nwordpieces were used) and pretrained a modiﬁed\nversion of BERT which allowed for only new to-\nken’s embeddings to be modiﬁed while the original\nembeddings remained ﬁxed. They found that using\nmore than 12k augmented tokens didn’t improve\ntheir biomed NER and relation extraction perfor-\nmance, and that, once augmented, performance\nimproved with more pretraining (4-24 hours were\nstudied.)\nPoerner et al. (2020) augment BERT’s vocabu-\nlary with all in-domain OOV whole words, adding\n31K tokens to bert-base-cased’s 29K wordpieces.\nThey trained a word2vec model on an in-domain\ncorpus and ﬁt a linear transformation to project\nthe word embeddings into the model’s input em-\nbedding space. No further pretraining is done, but\nduring ﬁnetuning, the original tokenizer and the\nadapted tokenizer are both used. For inference, the\nﬁnetuned model is run with both the original tok-\nenizer and the adapted tokenizer and the outputs\nare averaged. Their F1 score outperforms BERT\non all eight biomedical NER tasks studied. The\napproach has the disadvantage of increasing the\nparameter size of bert-base-cased by 2.2x due to\nthe embeddings of added tokens and doubles the\nresources needed for inference.\nHofmann et al. (2021) demonstrates how Word-\npiece tokenization does not capture the semantics\nof derivationally complex words as well as an ap-\nproach using a modiﬁed version of Wordpiece de-\nsigned to produce subword segmentations consist-\ning of linguistic preﬁxes, sufﬁxes and afﬁxes Hof-\nmann et al. (2020). This subword tokenizer outper-\nformed WordPiece in determining words’ polarity\nor their source domains. Experiments were con-\nducted on novel embedding tokens in BERT via\napproaches including a projection-based method\nand mean pooling (both similar to §3.3).\nTraining language models from scratch in the\ndomain of interest has been shown to provide im-\nproved in-domain performance when compared to\nout-of-domain PLMs Huang et al. (2019). In ad-\ndition to Gururangan et al. (2020), prior work has\nshown the effectiveness of continued pretraining\nfor domain adaptation of PLMs Alsentzer et al.\n(2019); Chakrabarty et al. (2019); Lee et al. (2019).\nFor the task of Aspect-Target Sentiment Classi-\nﬁcation, Rietzler et al. (2020) uses both DAPT\nand task-speciﬁc ﬁne-tuning in order to adapt lan-\nguage models representations. Identifying domain-\ncharacteristic words is a well-studied problem, and\nmany metrics have been proposed for this task\nthrough comparing the distributions of tokens in\ncontrasting corpora Rayson et al. (1997); Monroe\net al. (2008); Kessler (2017). Muthukrishnan et al.\n(2008) used the pointwise KL-divergence to distin-\nguish informativeness of key phrase candidates in\na domain corpus relative to a background.\n3 Adaptive tokenization of contextual\nembeddings\nWe deﬁne adaptive tokenization (AT) as the pro-\ncess of augmenting a PLM’s tokenizer and ﬁxed\nsubword embeddings with new entries taken from\na novel corpus. AT consists of two goals which\nmust be achieved for domain adaptation. First, se-\nlection of domain-speciﬁc tokens, with which to\n157\naugment a pretrained tokenizer, from an in-domain\ncorpus must be determined. Second, an appropriate\ninitialization in the input space of the contextual\nembedding models needs to be determined for ad-\nditions to the tokenizer vocabulary. In this section,\nwe detail approaches for each of these linked tasks.\n3.1 Tokenizer vocabulary augmentation\nIn this section, we detail approaches for identify-\ning domain-speciﬁc token sequences to be added\nduring tokenizer augmentation. Common tokeniza-\ntion schemes such as Byte Pair Encoding Sennrich\net al. (2016) and WordPiece Schuster and Nakajima\n(2012); Wu et al. (2016) are greedy algorithms and,\nas a result, merge subwords into individual tokens\nif such a sequence occurs with high relative fre-\nquency. When adapting a tokenizer our goal is\nto identify subword sequences which occur with\nhigh relative frequency in a domain speciﬁc corpus\ncompared to the pretraining corpus. In Table 1,\nwe provide the corpora for each domain in which\nexperimentation is conducted. Next, we show how\nto operationalize this framework to ﬁnd domain-\nspeciﬁc token sequences.\n3.2 Identifying domain-speciﬁc token\nsequences\nIn this section, we detail our approach for selection\nof token sequences which are both difﬁcult to rep-\nresent in a base tokenizer and have large disparities\nin occurrence between domain-speciﬁc and base\ncorpora. Conceptually, we would like to add new\ntokens to the source tokenizer which are sequences\nof existing tokens and, in the in-domain corpus, are\nextensions of existing token sequences.\n(I) Computing Empirical Token Sequence Dis-\ntributions We ﬁrst compute counts of sequences\nof [1,λ] subword tokens ( s) in each corpus C,\nnamely the source corpus for RoBERTa ( S) and\nthe in-domain corpus which is the target of our\nadaptation (D). The source language model’s tok-\nenizer (namely Roberta-base) is used as the source\nof subword tokens. The counts of each subtoken\nsequences are represented as Cs, where C is the\ncorpus and ss is the subword sequence. If sdoes\nnot appear in C, Cs = 0. We only retain sequences\noccurring at least φ= 20times in one corpus. The\nmaximum subword token sequence length (λ) is 10.\nWe limit subtoken sequences to word boundaries\nas detected through whitespace tokenization.\nNext, we predict how “phrase-like” a sequence\nof tokens Cs is, using a probability PC(s). Deﬁne\nPC(s) =Cs\nCt\nwhere tis ﬁrst |s|−1 subtoken sequence ofs. These\nprobabilities should be thought of as the surprise\nof the sequence sin the corpus being counted and\nare indicative of the how phrase-like sis.\nAs an example, consider a hypothetical corpus\nconsisting of documents written about classical mu-\nsic. Roberta-base’s tokenizer splits “oboe” into the\nsubtokens ⟨ob,oe⟩. In this classical music corpus,\nthe portion of tokens following “ob” which are “oe”\n(composing in the word “oboe”) is surely much\nhigher than in a general base corpus where other\nwords staring with the “ob” subtoken like “obama”\n(tokenized as ⟨ob,ama⟩) are much more frequent\nand “oboe” much less.\n(II) Domain shift scoring of Token Sequence\nDistributions with Conditional KL Divergence\nIn order to characterize these differences in proba-\nbilities, we use the pointwise KL-divergence. Let-\nting p and q be probabilities, the pointwise KL-\ndivergence is deﬁned as:\nDKL(p∥q)) =plog p\nq\nLet the sequence relevance score R(s) be de-\nﬁned as\nR(s) =DKL(PD(s) ∥PS(s)).\nR(s) indicates how much the phrase-like proba-\nbility of sequence s in the in-domain corpus D\n(PD(s)) diverges from the baseline phrase-like\nprobability of sin the base corpus S.\n(III) Selection of Token Sequences for Tok-\nenizer Augmentation For all experiments, we add\nthe η= 10Ksequences with the largest R, sorted\nirrespective of sequence length, to the domain-\naugmented tokenizer.\nThis introduces of 7.68M parameters (embed-\nding size 768 ×10K new tokens), a 6% increase\nover Roberta-base’s 125M.1\n3.3 Initialization approaches for AT\nIn this section, we provide two approaches to im-\npute contextual embedding input representations\nfor tokens added in §3.1.\nSubword-based initialization In this common ini-\ntialization Casanueva et al. (2020); Vuli ´c et al.\n1github.com/pytorch/fairseq/tree/master/examples/roberta\n158\nAlgorithm 1 Selection of Domain-Speciﬁc Token Sequences for Tokenizer Augmentation\nRequire: Base Tokenizer Tok, Base LM LMbase, Base and Domain Unigram Dists. Ubase,Udomain,\nBase and Domain Seq. Dists. Tbase= {},Tdomain= {} Min. Seq. Frequency Fmin,# Aug. to make N,\nMax Aug. Length L,Augmentations = []\n(I) Computing Empirical Token Sequence Distributions\nfor word, count (w,count) in Ubase do ⊿Do the same for Domain Corpus\nSeq[t0,t1,...,t n] :=Tok(w)\nfor i in [1,n] do\nTbase[Seq[: i]] +=count\nend for\nend for\nTdomain.values() /= sum(Udomain.values()) ⊿Normalize Sequence Distributions\nTbase.values() /= sum(Ubase.values())\n(II) Domain shift scoring of Token Seq. Dists. with Conditional KL Divergence\nScoreDKL = {}\nfor Seq in Tbase\n⋂Tdomain do\nScoreDKL [Seq] :=Tdomain[Seq] ∗log Tdomain[Seq]\nTbase[Seq]\nend for\n(III) Selection of Token Sequences for Augmentation\nSortDescending(ScoreDKL )\nfor Seq in ScoreDKL do\nif Len(Augmentations) =N then\nbreak\nend if\nif Len(Seq) <L AND Tdomain >Fmin AND Tbase >Fmin then\nAugmentations.append(Seq)\nend if\nend for\nreturn Augmentations\n(2020); Hofmann et al. (2021), additions to the tok-\nenizer are embedded as the mean of their Roberta-\nbase ﬁxed subword embeddings. In cases where all\na novel word’s subwords are unrelated to its spe-\nciﬁc, in-domain meaning, this initialization may\ncause unwanted model drift in ﬁne-tuning for unre-\nlated tokens with similar ﬁxed embeddings.\nAlgorithm 2 Projection-Based Initialization of\nAugmented Tokens\nRequire: LM Input Embeddings Cs,Base and\nDomain Learned Input Embeddings Xs,Xt,\nand Embedding Size d.\n(I) Learn Mapping ˆM: Cs →Xs with SGD:\nˆM= arg minM∈Rd×d ∥MXs −Cs∥F\n(II) Get Inits. for Aug. Tokens using ˆM:\nCt = ˆMXt\nreturn Ct\nProjection-based initialization To mitigate pos-\nsible issues with averaging subword embeddings,\nwe also consider projections between static token\nembeddings to the input space of contextual em-\nbeddings, similar to Poerner et al. (2020).\nTo summarize this approach, our goal is to learn\na mapping between the input token embeddings in\nRoBERTa,Cbase, and word2vec token embeddings\nlearned independently on the base 2 and domain\nspeciﬁc corpora, Xbase,Xdomain. The tokens in\nCbase include the original RoBERTa tokens while\nthose in Xbase and Xdomain include both the orig-\ninal RoBERTa tokens and the augmented tokens\nfound using adaptive tokenization detailed in §3.2.\nFirst, a mapping M, parametrized as a single layer\nfully connected network, from Xbase to Cbase is\nlearned which minimizes distances, on the origi-\nnal set of tokens in RoBERTa. The goal of this\nmapping is to learn a function which can translate\n2See §5.4 for how the RoBERTa source corpora is approx-\nimated to form our base corpus.\n159\nDomain Pretrain Corpus [# Tokens] Task Task Type Train (Lab.) Dev. Test Classes\nBioMed 1.8M papers from S2ORC [5.1B] ChemProt relation classiﬁcation 4169 2427 3469 13\nRCT abstract sent. roles 18040 30212 30135 5\nCS 580K papers from S2ORC [2.1B] ACL-ARC citation intent 1688 114 139 6\nSciERC relation classiﬁcation 3219 455 974 7\nNews 11.9M articles [6.7B] HyperPartisan partisanship 515 65 65 2\nReviews 24.75M Amazon reviews [2.1B] IMDB review sentiment 20000 5000 25000 2\nTable 1: Speciﬁcations of the various target task and pretraining datasets to replicate experiments in Gururangan\net al. (2020). Due to the restrictions on accessible papers in S2ORC, we are using versions of BioMed and CS\nwhich are approximately 33% and 74% smaller than were used in Gururangan et al. (2020). Sources: S2ORC\nLo et al. (2020), News Zellers et al. (2019), Amazon reviews He and McAuley (2016), CHEMPROT Kringelum\net al. (2016), RCT Dernoncourt and Lee (2017), ACL-ARC Jurgens et al. (2018), SCIERC Luan et al. (2018),\nHYPERPARTISAN Kiesel et al. (2019), and IMDB Maas et al. (2011).\nword2vec token embeddings to the input space of\nRoBERTa. Then, the learned mapping M is ap-\nplied to Xdomain in order to obtain initializations\nin the input space of RoBERTa for the augmented\ntokens found using the approach in §3.2. The op-\nerations involved in this approach are detailed in\nAlgorithm 2.\n4 Experimentation\nIn this section, we perform evaluation of our adap-\ntation approach on six natural language process-\ning tasks in four domains, BioMedical, Computer\nScience, News, and Reviews, following the eval-\nuations in Gururangan et al. (2020). Due to re-\nsource constraints, we perform experimentation on\nall datasets in Gururangan et al. (2020) excluding\nthe Helpfulness dataset from the reviews domain\nand the Hyperpartisan dataset in the news domain.\nEach of the excluded datasets contain greater than\n100K training examples, resulting in greater than\n12 hours of time required for ﬁnetuning on 8 Tesla\nV100 GPUs for a single seed.\nApproaches Roberta-base, a commonly used PLM\nwith high performance, is used as a baseline on\nwhich supervised ﬁnetuning is performed sepa-\nrately for each dataset. Additionally, we compare\nAT to the DAPT method from Gururangan et al.\n(2020). As we do not make use of task speciﬁc\ndata (i.e., the training data used in ﬁne-tuning), AT\nis comparable to DAPT in terms of the data uti-\nlized. We focus on using large, in-domain data sets\nwhich are commonly used in further pretraining\n(rather than variably sized task-data) since their\nsize both allows for reliable extraction of charac-\nteristic subtoken sequences to use in tokenizer aug-\nmentation. Adaptive tokenization for task-speciﬁc\ndata is future work.\nClassiﬁcation Architecture We use the same clas-\nsiﬁcation architecture as in Gururangan et al.\n(2020), originally proposed in Devlin et al. (2019),\nin which the ﬁnal layer’s [CLS] token representa-\ntion is passed to a task-speciﬁc feed forward layer\nfor prediction. All hyperaparameters used in ex-\nperimentation are equivalent to either the \"mini\",\n\"small\", or \"big\" hyperparameter sets from Guru-\nrangan et al. (2020).\nResults We ﬁnd that adaptive tokenization im-\nproves performance when compared to the base-\nline RoBERTa model in all four of the domains\non which experimentation is performed. AT pro-\nvides 97% of the aggregate relative improvement\nattained by DAPT respectively over Roberta-base\nwhile providing an order of magnitude efﬁciency\ngain detailed in Table 3. We do not see a signiﬁcant\ndifference in the performance of AT models based\non the Mean or Proj initialization schemes. Given\nthat Mean initialization required half the time as\nProj, we recommend its use over Proj.\n5 Discussion\n5.1 Resource Efﬁciency in LM Adaptation\nCurrent approaches for training and adapting LMs\nhave resulted in negative environmental impact\nand high computational resource budgets for re-\nsearchers. PLMs incur signiﬁcant compute time\nduring pretraining, typically requiring numerous\ndays of training on ≥8 GPUs or TPUs Liu et al.\n(2019); Devlin et al. (2019); Gururangan et al.\n(2020). In Table 3, we provide a runtime com-\nparison between continued pretraining and AT. We\nﬁnd that AT provides a 72x speedup compared to\nDAPT and does not require a GPU or TPU to run.\nThe most resource-intensive portion of this proce-\ndure involves indexing the corpora and conducting\n160\nDomain Task RoBERTa DAPT TAPT DAPT + TAPT AT (Mean) AT (Proj) State-of-the-art (in 2020)\nBioMed∗ ChemProt 81.9 1.0 84.20.2 82.60.4 84.40.4 83.60.4 83.10.3 84.6\nRCT 87.2 0.1 87.60.1 87.70.1 87.80.1 87.50.4 87.60.3 92.9\nCS∗ ACL-ARC 63.0 5.8 75.42.5 67.41.8 75.63.8 70.12.0 68.91.6 71.0\nSciERC 77.3 1.9 80.81.5 79.31.5 81.31.8 81.40.4 81.21.2 81.8\nNews HyperPartisan 86.6 0.9 88.25.9 90.45.2 90.06.6 93.14.2 91.65.5 94.8\nReviews IMDB 95.0 0.2 95.40.1 95.50.1 95.60.1 95.40.1 95.50.1 96.2\nTable 2: Results of different adaptive pretraining methods compared to the baseline RoBERTa. AT with mean\nsubword and projective initializations are denoted as AT (Mean) and AT (Proj) respectively. Stddevs are from 5\nseeds. Results for DAPT, TAPT, DAPT+TAPT, and state-of-the-arts are quoted from Gururangan et al. (2020). The\nhighest non-state-of-the-art result is bolded, since the state-of-the-art functions as a performance ceiling, leverag-\ning both domain-speciﬁc pretraining and an adapted tokenizer. The best of the three approaches which utilize only\nsource and domain domain data before ﬁne-tuning (i.e., DAPT and AT) is underlined. *Due to restrictions on ac-\ncessible papers in S2ORC, The BioMed and CS pretraining corpora used were respectively 33% and 74% smaller\nthan the versions in Gururangan et al. (2020). Note that state-of-the-art numbers are current at the time of Gururan-\ngan et al. (2020), and are from the following works: ChemProt: S2ORC-BERT Lo et al. (2020), RCT: Sequential\nSentence Classiﬁcation Cohan et al. (2019), ACL-ARC: SciBert Beltagy et al. (2019), SciERC: S2ORC-BERT Lo\net al. (2020), HyperPartisan: Longformer Beltagy et al. (2020), IMDB: XLNet Large Yang et al. (2019).\nMethod Hardware Specs. Runtime [h:m:s]\nDAPT 8x TPU V-3 94 hours\nAT (Mean) 64x vCPUs 1:17:35\nAT (Projection) 64x vCPUs 4:54:58\nTable 3: Runtime and hardware speciﬁcations for AT\ncompared to DAPT. The vast majority of the time is\nspent reading the corpus and creating token distribu-\ntions. Runtimes are based on the CS 8.1B token corpus.\nThe DAPT runtime is mentioned in Github Issue 16 in\nGururangan et al. (2020) and the AT runtimes are lin-\nearly extrapolated (an overestimate) from our observed\nruntime on the open version of CS, a 2.1B token corpus.\nWe needed to perform this extrapolation since the full\nCS corpus which was used to benchmark Gururangan\net al. (2020) is unavailable in S2ORC. “64x vCPUs” in-\ndicate the equivalent of an AWS ml.m5.16xlarge EC2\ninstance was used to determine which subtoken se-\nquences to use for vocabulary augmentation and com-\npute their embeddings. The times reported for AT\n(Mean) and AT (Projection) where from a single run,\nwith precomputed base corpus token counts and embed-\ndings.\nsubtoken sequence counts.\nIn addition to time and resources, the environ-\nmental impact of pretraining BERT with a single\nset of hyperparameters incurs a carbon footprint\nof approximately 1.5K pounds of CO2 emissions,\nmore than the average monthly emissions of an indi-\nvidual Strubell et al. (2019). Continued pretraining,\nwhich has a similar resource budget to BERT, exac-\nerbates this problem Schwartz et al. (2019). Lastly,\nwe ﬁnd that the cloud computing costs associated\nwith continual pretraining for both a single domain\nand set of hyperparameters are $750 compared\nto around $4.77 (using a ml.m5.16xlarge EC2 in-\nstance for 1:17) for AT on cloud computing plat-\nforms when using non-preemptible instances. High\ncosts associated with the training of NLP models\nhas led to inequity in the research community in\nfavor of industry labs with large research budgets\nStrubell et al. (2019).\n5.2 Augmented Token Sequences selected in\neach domain\nIn Table 4, we provide examples of augmented vo-\ncabulary selected by our adaptive tokenization al-\ngorithm for each of the four domains used in exper-\nimentation. In each domain, the augmented tokens\nidentiﬁed by AT correspond to domain-speciﬁc lan-\nguage. For instance, augmented tokens in the Re-\nviews domain token sequences often contain con-\ntractions such as “I’ve” and “it’s”, which are fre-\nquently used in informal language. In the News\ndomain, augmented tokens include ﬁnancial terms\nsuch as “NYSE” and “Nasdaq” along with media\noutlets such as “Reuters” and “Getty”. Many of\nthe augmented tokens in the Computer Science\ndomain are mathematical and computing terms\nsuch as “Theorem”, “Lemma”, “Segmentation”,\nand “Gaussian”. Lastly, augmented tokens in the\nBioMedical domain are largely concerned with bi-\nological mechanisms and medical procedures such\nas “phosphorylation”, “assays”, and “transfect”.\n5.3 Future directions\nWhile we have evaluated this approach on Roberta-\nbase, it can be used on any PLM which uses sub-\nword tokenization. It would be interesting future\n161\nBioMed CS News Reviews\n[inc, ub, ated] → incubated [The, orem] → Theorem [t, uesday] → tuesday [it, ’s] → it’s\n[trans, fect] → transfect [L, em, ma] → Lemma [ob, ama] → obama [that, ’s] → that’s\n[ph, osph, ory] → phosphory [vert, ices] → vertices [re, uters] → reuters [sh, oes] → shoes\n[mi, R] → miR [E, q] → Eq [iph, one] → iphone [doesn, ’t] → doesn’t\n[st, aining] → staining [cl, ust, ering] → clustering [ny, se] → nyse [didn, ’t] → didn’t\n[ap, opt, osis] → apoptosis [H, ence] → Hence [get, ty] → getty [can, ’t] → can’t\n[G, FP] → GFP [Seg, mentation] → Segmentation [inst, agram] → instagram [I, ’ve] → I’ve\n[pl, asm] → plasm [class, iﬁer] → classiﬁer [bre, xit] → brexit [b, ought] → bought\n[ass, ays] → assays [Ga, ussian] → Gaussian [nas, daq] → nasdaq [you, ’ll] → you’ll\n[ph, osph, ory, lation] → phosphorylation [p, olyn] → polyn [ce, o] → ceo [kind, le] → kindle\nTable 4: Samples of token sequences with large JSD between base and domain corpora sequence distributions; all\nof these sequences were added during AT to the Roberta-Base tokenizer.\nwork to see if the performance gain will hold on\nlarger PLMs with richer vocabularies or on smaller\nPLMs. One may speculate the beneﬁt of AT is due\nto encoding non-compositional subword tokens in\nthe input embedding space. And furthermore, this\nlifts some of the responsibility for encoding their\nsemantics from the LM’s interior weights. Since\nthese non-compositional tokens are characteristic\nto the domain corpus, their representations may be\nimportant to the end task and and need to be learned\nor improved during ﬁne-tuning. If this is the case,\nthen perhaps models with fewer interior weights\nbeneﬁt more from AT since the connection between\nthe non-compositional tokens would be built into\nthe input, allowing interior weights to better learn\nthe semantics of novel non-compositional tokens\nand opposed to also having to learn the component\ntokens’ connection.\nWhile this work tests AT on an English language\nPLM, it can hypothetically be applied to any PLM\nregardless of its source language(s). Exploring how\nAT can work with additional pretraining on domain\ndata is clear future work. Tai et al. (2020) show\nthat specialized further pretraining on domain data\non using a model augmented with domain charac-\nteristic whole word tokens results in an improved\nperformance/pretraining time curve. It would also\nbe fruitful to explore how that curve changes when\nusing more efﬁcient pretraining techniques such as\nin Clark et al. (2020).\nWhile we compared different novel token se-\nquence embedding techniques, we did not study\ndifferent ways of identifying subtoken sequences\nto add. Comparing AT to approaches such adding\nwhole word tokens Tai et al. (2020) would conﬁrm\nour hypothesis that phrase-like token sequences are\nuseful.\nExperimenting with the number of subtoken se-\nquences added to the tokenizer ( η ﬁxed at 10K)\nmay also be worthwhile. While Tai et al. (2020)\nfound 12Ktokens additions optimal, Poerner et al.\n(2020) added 310K tokens. Seeing the trade-off\nbetween added tokens and performance would be\nuseful, as each additional parameter increases the\nmodel size.\nOur approach requires new tokens to appear φ\ntimes in both the source and domain corpora. While\nthis was necessary in order to produce source-\ncorpus word embeddings in Proj, it does not al-\nlow for domain-exclusive subtoken sequences to\nbe added to the tokenizer. Abandoning this require-\nment for Mean may lead to a better set of token\naugmentations.\nWe can also experiment with other subtoken can-\ndidate selection techniques. For example, Schwartz\net al. (2013) used pointwise mutual information\n(PMI) to determine how phrase-like candidates\nword sequences were. PMI is the log ratio of\nthe probability of a phrase vs. the product of\nthe probability of its component unigrams. While\nour approach considers the probability of a subto-\nken given a preceding sequence, it, unlike PMI,\ndoes not consider the probability of that following\nsubtoken in isolation. This may lead to domain-\nspeciﬁc subtokens sneaking into augmented token\nsequences, such as the contraction tokens added to\nthe reviews Reviews tokenizer in Table 4.\n5.4 Implementation details\nThe code is in preparation for release.\nThe hyperparameter search used was\nROBERTA_CLASSIFIER_MINI from Gururan-\ngan et al. (2020) from their codebase https://\ngithub.com/allenai/dont-stop-pretraining.\nToken counts for RoBERTa-base were estimated\nusing English Wikipedia 20200501.en and an\nopen source book corpus from https://storage.\ngoogleapis.com/huggingface-nlp/datasets/\nbookcorpus/bookcorpus.tar.bz2. Word2vec\nembeddings were computed with Gensim (Rehurek\n162\nand Sojka, 2011), using the following parameters:\nWord2Vec(..., size=768, window=5,\nmin_count=100, epochs=2,\nsample=1e-5)\n6 Conclusion\nIn this paper, we introduced adaptive tokenization\n(AT) a method for efﬁciently adapting pretrained\nlanguage models utilizing subword tokenization\nto new domains. AT augments a PLM’s tokeniza-\ntion vocabulary to include domain-speciﬁc token\nsequences. We provide two approaches for ini-\ntializing augmented tokens: mean subword and\nprojections from static subword embeddings. AT\nrequires no further language model pretraining on\ndomain-speciﬁc corpora, resulting in a 38x speedup\nover pretraining on the corpora without specialized\nhardware. Across four domains, AT provides >97%\nof the performance improvement of further pre-\ntraining on domain-speciﬁc data over Roberta-base.\nThis initial work suggests that adapting the sub-\nword tokenization scheme of PLMs is an effective\nmeans of transferring models to new domains. Fu-\nture work entails hybrid approaches using both AT\nand small amounts of LM pretraining, alternative\nmetrics for augmented token selection, improved\ninitialization of augmented token representations,\nand the use of task data.\nAcknowledgements\nWe thank Yi Zhang, William Headden, Max Harper,\nChandni Singh, Anuj Ahluwalia, Sushant Sagar,\nJay Patel, Sachin Hulyalkar, and the anonymous\nreviewers for their valuable feedback.\nEthics statement\nAs mentioned in §5, pretrained language models\nincur signiﬁcant costs with respect to time, compu-\ntational resources and environmental impact. Con-\ntinued domain speciﬁc pretraining, which has a\nsimilar resource budget to BERT, exacerbates this\nproblem Schwartz et al. (2019). In this work, we\nprovide approaches for adapting pretrained lan-\nguage models to new domains with an approach,\nAdaptive Tokenization, which seeks to minimize\ncosts associated with continued domain speciﬁc\npretraining. It should be noted that we do not de-\ncrease the resource and environmental associated\nwith pretraining, only the costs for domain adaptive\npretraining which are nevertheless sizable (e.g. 32\nTPU days for DAPT).\nAdditionally, we ﬁnd that the cloud computing\ncosts associated with continued domain speciﬁc\npretraining on a single domain and set of hyperpa-\nrameters are around $750 compared to around $5\nfor AT on a cloud computing platform. High costs\nassociated with the training of NLP models has led\nto inequity in the research community in favor of\nindustry labs with large research budgets Strubell\net al. (2019), a problem we seek to ameliorate.\nThis work does not address the high resource\ncost in ﬁne-tuning PLMs. Risks associated with\nthis paper are that this work may encourage the use\nof PLMs in more settings, such as domains with\nsmall amounts of data, and introduce potentially\nharmful inductive biases which have been found in\nmany commonly used PLMs.\nWe include statistics about the data sets used in\nTable 1, these data sets were introduced in Guru-\nrangan et al. (2020) and open source.\n163\nReferences\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clini-\ncal BERT embeddings. In Proceedings of the 2nd\nClinical Natural Language Processing Workshop,\npages 72–78, Minneapolis, Minnesota, USA. Asso-\nciation for Computational Linguistics.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientiﬁc text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3615–\n3620, Hong Kong, China. Association for Computa-\ntional Linguistics.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv:2004.05150.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners.\nIñigo Casanueva, Tadas Tem ˇcinas, Daniela Gerz,\nMatthew Henderson, and Ivan Vuli´c. 2020. Efﬁcient\nintent detection with dual sentence encoders. In Pro-\nceedings of the 2nd Workshop on Natural Language\nProcessing for Conversational AI, pages 38–45, On-\nline. Association for Computational Linguistics.\nTuhin Chakrabarty, Christopher Hidey, and Kathy\nMcKeown. 2019. IMHO ﬁne-tuning improves claim\ndetection. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n558–563, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In ICLR.\nArman Cohan, Iz Beltagy, Daniel King, Bhavana Dalvi,\nand Dan Weld. 2019. Pretrained language models\nfor sequential sentence classiﬁcation. In EMNLP.\nFranck Dernoncourt and Ji Young Lee. 2017. PubMed\n200k RCT: a dataset for sequential sentence clas-\nsiﬁcation in medical abstracts. In Proceedings of\nthe Eighth International Joint Conference on Natu-\nral Language Processing (Volume 2: Short Papers),\npages 308–313, Taipei, Taiwan. Asian Federation of\nNatural Language Processing.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas,\nNaoto Usuyama, Xiaodong Liu, Tristan Naumann,\nJianfeng Gao, and Hoifung Poon. 2020. Domain-\nspeciﬁc language model pretraining for biomedical\nnatural language processing.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n8342–8360, Online. Association for Computational\nLinguistics.\nRuining He and Julian McAuley. 2016. Ups and downs:\nModeling the visual evolution of fashion trends with\none-class collaborative ﬁltering. In Proceedings of\nthe 25th International Conference on World Wide\nWeb, WWW ’16, page 507–517, Republic and Can-\nton of Geneva, CHE. International World Wide Web\nConferences Steering Committee.\nValentin Hofmann, Janet Pierrehumbert, and Hinrich\nSchütze. 2020. DagoBERT: Generating derivational\nmorphology with a pretrained language model. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 3848–3861, Online. Association for Computa-\ntional Linguistics.\nValentin Hofmann, Janet B. Pierrehumbert, and Hin-\nrich Schütze. 2021. Superbizarre is not superb: Im-\nproving bert’s interpretations of complex words with\nderivational morphology.\nKexin Huang, Jaan Altosaar, and Rajesh Ranganath.\n2019. Clinicalbert: Modeling clinical notes and pre-\ndicting hospital readmission. arXiv:1904.05342.\nDavid Jurgens, Srijan Kumar, Raine Hoover, Dan Mc-\nFarland, and Dan Jurafsky. 2018. Measuring the evo-\nlution of a scientiﬁc ﬁeld through citation frames.\nTransactions of the Association for Computational\nLinguistics, 6:391–406.\nJason Kessler. 2017. Scattertext: a browser-based tool\nfor visualizing how corpora differ. In Proceedings\nof ACL 2017, System Demonstrations, pages 85–90,\nVancouver, Canada. Association for Computational\nLinguistics.\n164\nJohannes Kiesel, Maria Mestre, Rishabh Shukla, Em-\nmanuel Vincent, Payam Adineh, David Corney,\nBenno Stein, and Martin Potthast. 2019. SemEval-\n2019 task 4: Hyperpartisan news detection. In\nProceedings of the 13th International Workshop on\nSemantic Evaluation, pages 829–839, Minneapo-\nlis, Minnesota, USA. Association for Computational\nLinguistics.\nJens Kringelum, Sonny Kim Kjaerulff, Søren Brunak,\nOle Lund, Tudor I. Oprea, and Olivier Taboureau.\n2016. ChemProt-3.0: a global chemical biology dis-\neases mapping. Database, 2016. Bav123.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2019. BioBERT: a pre-\ntrained biomedical language representation model\nfor biomedical text mining. Bioinformatics,\n36(4):1234–1240.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-\nney, and Daniel Weld. 2020. S2ORC: The semantic\nscholar open research corpus. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 4969–4983, Online. As-\nsociation for Computational Linguistics.\nYi Luan, Luheng He, Mari Ostendorf, and Hannaneh\nHajishirzi. 2018. Multi-task identiﬁcation of enti-\nties, relations, and coreference for scientiﬁc knowl-\nedge graph construction. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 3219–3232, Brussels, Bel-\ngium. Association for Computational Linguistics.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analy-\nsis. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 142–150, Port-\nland, Oregon, USA. Association for Computational\nLinguistics.\nB. L. Monroe, Michael Colaresi, and K. Quinn. 2008.\nFightin’ words: Lexical feature selection and evalu-\nation for identifying the content of political conﬂict.\nPolitical Analysis, 16:372–403.\nPradeep Muthukrishnan, Joshua Gerrish, and\nDragomir R. Radev. 2008. Detecting multiple\nfacets of an event using graph-based unsupervised\nmethods. In Proceedings of the 22nd International\nConference on Computational Linguistics (Coling\n2008), pages 609–616, Manchester, UK. Coling\n2008 Organizing Committee.\nNina Poerner, Ulli Waltinger, and Hinrich Schütze.\n2020. Inexpensive domain adaptation of pretrained\nlanguage models: Case studies on biomedical NER\nand covid-19 QA. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1482–1490, Online. Association for Computational\nLinguistics.\nPaul Rayson, G. Leech, and Mary Hodges. 1997. So-\ncial differentiation in the use of english vocabulary:\nsome analyses of the conversational component of\nthe british national corpus. International Journal of\nCorpus Linguistics, 2:133–152.\nRadim Rehurek and Petr Sojka. 2011. Gensim–python\nframework for vector space modelling. NLP Centre,\nFaculty of Informatics, Masaryk University, Brno,\nCzech Republic, 3(2).\nAlexander Rietzler, Sebastian Stabinger, Paul Opitz,\nand Stefan Engl. 2020. Adapt or get left behind:\nDomain adaptation through BERT language model\nﬁnetuning for aspect-target sentiment classiﬁcation.\nIn Proceedings of the 12th Language Resources\nand Evaluation Conference, pages 4933–4941, Mar-\nseille, France. European Language Resources Asso-\nciation.\nMike Schuster and Kaisuke Nakajima. 2012. Japanese\nand korean voice search. In International Confer-\nence on Acoustics, Speech and Signal Processing,\npages 5149–5152.\nH. Andrew Schwartz, Johannes C. Eichstaedt, Mar-\ngaret L. Kern, Lukasz Dziurzynski, Stephanie M.\nRamones, Megha Agrawal, Achal Shah, Michal\nKosinski, David Stillwell, Martin E. P. Seligman,\nand Lyle H. Ungar. 2013. Personality, gender, and\nage in the language of social media: The open-\nvocabulary approach. PLOS ONE, 8(9):1–16.\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren\nEtzioni. 2019. Green AI. CoRR, abs/1907.10597.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 3645–3650, Florence, Italy.\nAssociation for Computational Linguistics.\nWen Tai, H. T. Kung, Xin Dong, Marcus Comiter,\nand Chang-Fu Kuo. 2020. exBERT: Extending\npre-trained models with domain-speciﬁc vocabulary\nunder constrained training resources. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2020, pages 1433–1439, Online. Associa-\ntion for Computational Linguistics.\n165\nIvan Vuli´c, E. Ponti, Robert Litschko, Goran Glavas,\nand Anna Korhonen. 2020. Probing pretrained\nlanguage models for lexical semantics. ArXiv,\nabs/2010.05731.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation. CoRR, abs/1609.08144.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. CoRR, abs/1906.08237.\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. CoRR, abs/1905.12616.\nRong Zhang, Revanth Gangi Reddy, Md Arafat Sul-\ntan, Vittorio Castelli, Anthony Ferritto, Radu Flo-\nrian, Efsun Sarioglu Kayi, Salim Roukos, Avi Sil,\nand Todd Ward. 2020. Multi-stage pre-training for\nlow-resource domain adaptation. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 5461–\n5468, Online. Association for Computational Lin-\nguistics."
}