{
    "title": "Vision-Language Transformer for Interpretable Pathology Visual Question Answering",
    "url": "https://openalex.org/W4225844297",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2963399842",
            "name": "Usman Naseem",
            "affiliations": [
                "University of Sydney"
            ]
        },
        {
            "id": "https://openalex.org/A2155621213",
            "name": "Matloob Khushi",
            "affiliations": [
                "University of Sydney"
            ]
        },
        {
            "id": "https://openalex.org/A2098955200",
            "name": "Jinman Kim",
            "affiliations": [
                "University of Sydney"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2943370629",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W4213019189",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3175170601",
        "https://openalex.org/W3199422761",
        "https://openalex.org/W3179963059",
        "https://openalex.org/W1832693441",
        "https://openalex.org/W6754089394",
        "https://openalex.org/W6766253171",
        "https://openalex.org/W6754139713",
        "https://openalex.org/W6754175302",
        "https://openalex.org/W6784715303",
        "https://openalex.org/W2976365868",
        "https://openalex.org/W3094950914",
        "https://openalex.org/W3011651912",
        "https://openalex.org/W6765968251",
        "https://openalex.org/W6753792695",
        "https://openalex.org/W6785642512",
        "https://openalex.org/W2963383024",
        "https://openalex.org/W2963954913",
        "https://openalex.org/W6752083267",
        "https://openalex.org/W2963150162",
        "https://openalex.org/W2747623286",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W6637373629",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W6640212811",
        "https://openalex.org/W6736057607",
        "https://openalex.org/W2136655611",
        "https://openalex.org/W6766904570",
        "https://openalex.org/W2970231061",
        "https://openalex.org/W6767279747",
        "https://openalex.org/W6767211374",
        "https://openalex.org/W2964242047",
        "https://openalex.org/W6775041699",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W3090449556",
        "https://openalex.org/W6784910178",
        "https://openalex.org/W3165058054",
        "https://openalex.org/W2616247523",
        "https://openalex.org/W6678914141"
    ],
    "abstract": "Pathology visual question answering (PathVQA) attempts to answer a medical question posed by pathology images. Despite its great potential in healthcare, it is not widely adopted because it requires interactions on both the image (vision) and question (language) to generate an answer. Existing methods focused on treating vision and language features independently, which were unable to capture the high and low-level interactions that are required for VQA. Further, these methods failed to offer capabilities to interpret the retrieved answers, which are obscure to humans where the models' interpretability to justify the retrieved answers has remained largely unexplored. Motivated by these limitations, we introduce a vision-language transformer that embeds vision (images) and language (questions) features for an interpretable PathVQA. We present an interpretable transformer-based Path-VQA (TraP-VQA), where we embed transformers' encoder layers with vision and language features extracted using pre-trained CNN and domain-specific language model (LM), respectively. A decoder layer is then embedded to upsample the encoded features for the final prediction for PathVQA. Our experiments showed that our TraP-VQA outperformed the state-of-the-art comparative methods with public PathVQA dataset. Our experiments validated the robustness of our model on another medical VQA dataset, and the ablation study demonstrated the capability of our integrated transformer-based vision-language model for PathVQA. Finally, we present the visualization results of both text and images, which explain the reason for a retrieved answer in PathVQA.",
    "full_text": "This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. \nCitation information: DOI 10.1109/JBHI.2022.3163751,\nIEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. XX, NO. XX, XXXX 2021 1\nVision-Language Transformer for Interpretable\nPathology Visual Question Answering\nUsman Naseem,Student Member, IEEE, Matloob Khushi, and Jinman Kim,Member, IEEE\nAbstract— Pathology visual question answering\n(PathVQA) attempts to answer a medical question posed\nby pathology images. Despite its great potential in\nhealthcare, the technology is still in its early stages.\nIt is not widely adopted because it requires both high\nand low-level interactions on both the image (vision)\nand question (language) to generate an answer. Existing\nmethods focused on treating vision and language features\nindependently, which were unable to capture these high\nand low-level interactions. Further, these methods failed\nto offer capabilities to interpret the retrieved answers,\nwhich are obscure to humans. Despite this, models’\ninterpretability to justify the retrieved answers has\nremained largely unexplored. Interpretability has become\nimportant to engender users’ trust in the retrieved answer\nby providing insight into the model prediction. Motivated by\nthese limitations, this paper introduces a vision-language\ntransformer that embeds vision (images) and language\n(questions) features for an interpretable PathVQA. We\npresent an interpretable tra\nnsformer-based P ath-VQA\n(TraP-VQA), where we embed transformers’ encoder\nlayers with vision and language features extracted using\npre-trained CNN and domain-specific language model\n(LM), respectively. A decoder layer is then embedded to\nupsample the encoded features for the final prediction\nfor PathVQA. Our experiments showed that our TraP-\nVQA outperformed the state-of-the-art comparative\nmethods with the public PathVQA dataset. Furthermore,\nour additional experiments validated the robustness\nof our model on another medical VQA dataset, and\nthe ablation study demonstrated the capability of our\nintegrated transformer-based vision-language model for\nPathVQA and the robustness of our model on another\nmedical VQA dataset. Finally, we conclude by discussing\nthe interpretability of each component of Trap-VQA by\npresenting the visualization results of both text and\nimages, which explains the reason for a retrieved answer\nin the PathVQA.\nIndex Terms— Pathology Images, Interpretability, Visual\nQuestion Answering, Vision-Language\nI. INTRODUCTION\nP\nATHOLOGY examines the causes and effects of diseases\nor injuries and involves diagnosing conditions through\nspecimens surgically removed from the body, such as organs,\nAuthors would like to acknowledge contribution to this research from\nthe Australian Research Council (ARC) grants.\nU. Naseem, M. Khushi, J. Kim are with the School of Computer\nScience, The University of Sydney, Australia. MK is with University\nof Suffolk, Ipswich, UK (e-mail: {usman.naseem,matloob.khushi, jin-\nman.kim}@sydney.edu.au).\nFig. 1. Examples of a PathVQA where questions are asked in two\ntypes: (right) closed-ended questions where the answers are restricted\nto “Y es” or “No” and, (left) open-ended questions where the answers are\nin free-form text such as keyword.\ntissues, bodily fluids, and in some cases, the whole body 1.\nThese specimens are routinely captured using images, as\nexemplified in Fig. 1. Pathology visual question answering\n(PathVQA) can answer many questions, including necrosis,\ninflammation, and cancer diagnosis. Despite rapid advances\nin the use of machine learning for medical image analysis,\ne.g., such as X-rays, MRIs, and CT scans, with results often\nsurpassing or on-par with clinicians, there has been a paucity\nof research for pathology images [1]. This is partly due to the\ncomplexity in combining the pathology imaging data with the\naccompanying clinical question and the answer.\nIn VQA, given an image accompanied by a relevant ques-\ntion, an algorithm deals with both the image (vision) and the\nquestion (language) and predicts an answer to the question\n(see Fig. 1). VQA is a challenging task as it demands an in-\ndepth understanding and high-level interactions on both the\ninput image and the question of both structural language and\nnon-structural image to generate the answer. Because of this\nvision-language (VL) analysis property, which is commonly\nused in medical applications, e.g., decision support systems\nand for medical training, VQA is generating growing interest\nin the medical imaging community. Interpretability is crucial\nto producing convincing answers for the system’s reliability\nand efficiency of medical visual question-answer (MedVQA).\nTo mitigate the limitations of modeling long dependen-\ncies and support parallel processing of sequence in recurrent\nneural networks (RNNs), Vaswani et al. [2] proposed an\nencoder-decoder based architecture built on multi-head self-\nattention and feed-forward neural networks referred to as\ntransformer for machine translation tasks. It is the state-of-\nthe-art (SOTA) method in various natural language processing\n1https://www.mcgill.ca/pathology/about/definition\nCopyright © 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or \nfuture media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or \nredistribution to servers or lists, or reuse of any copyrighted component of this work in other works.\n2 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. XX, NO. XX, XXXX 2021\n(NLP) tasks [3] and models such as Bidirectional Encoder\nRepresentations from Transformers (BERT) [4], Generative\nPre-trained Transformer (GPT) [5] and others [6]–[8] have\nbuilt upon this architecture. Inspired by the success of NLP\nand the strong representation capabilities of transformers, it\nhas attracted tremendous interest and proved its dominance\nover convolutional networks (CNNs) [9] in computer vision\n(CV) [3], including medical imaging, to leverage the self-\nattention mechanisms to fuse information across the whole\nimage, considering the image’s low-level features. However,\nthere is a paucity of research that combines both the encoder-\ndecoder layers of the transformer in ‘vision-language’ tasks,\nsuch as for MedVQA, which can complement the benefit of\nleveraging the transformer’s architecture.\nExisting approaches [10]–[18], though improved the accu-\nracy but failed to capture high and low-level interactions from\nboth image and text that are important to retrieve a correct\nanswer. Furthermore, these methods did not leverage both\nencoder-decoder layers of a transformer to fuse both image\nand text features for a MedVQA task. These methods did not\nprovide users with appropriate interpretations of the retrieved\nanswer. To provide interpretability, it is important to capture\nthe interaction of both image and text features and understand\nquestion-answer pairs. To develop an interpretable PathVQA\nmodel, we address the following challenges: (i) capture both\nhigh and low-level representation of question-answer pair, and\n(ii) interpretability is as important as accuracy for MedVQA,\nwhich is unfortunately often neglected.\nIn this paper, we present a novel and interpretable vision-\nlanguage transformer, which leverages both the encoder and\nthe decoder layers of a transformer to embed vision and\nlanguage features for the MedVQA (PathVQA in our case)\ntask. We propose TraP-VQA – a transformer-based pathology\nvisual question answering method where we embed low-level\nimage features with domain-specific contextual information\nderived from the questions, which are then used to answer\nthe question. TraP-VQA uses the strength of CNNs to extract\nimage features at low-level, domain-specific language model\n(LM) to extract domain-specific contextual information, and\ntransformer to capture global dependencies at high-level. Ex-\ntensive experiments show the advantage of our model against\nother methods and establish the new SOTA results on a popular\nbenchmark PathVQA dataset.\nThe remainder of the paper is structured as follows: A sum-\nmary of the related work is provided in section II. Section III\npresents the methodology of the proposed model. Experiments\ndetails and the results are then presented in section IV. Finally,\nsection V concludes the study.\nII. R ELATED WORK\nA. Medical Visual Question Answering (MedVQA)\nExisting works in MedVQA [10]–[14], [19], especially\nmethods used in the ImageCLEF challenges [11], [20], [21]\non MedVQA tend to adapt the advance methods used in\ngeneral-domain VQA such as Multi-modal Compact Bilinear\n(MCB) [22], Stacked Attention Networks (SAN) [23], Bilin-\near Attention Networks (BAN) [24], Multi-modal factorized\nbilinear (MFB) [25] and Multimodal Factorized High-order\n(MFH) [26]. Typically, pre-trained models such as ResNet [27]\nor VGGNet [28] are used to extract image features, recurrent\nneural networks (RNNs), such as long short-term memory\n(LSTM) [29] and gated recurrent unit (GRU) [30], word\nembeddings, and Bidirectional Encoder Representations from\nTransformers (BERT) [4] are adopted for extracting text-based\nfeatures. In first edition of ImageCLEF challenge 2, Peng et\nal. [13] used ResNet-152 and LSTM for extracting image and\ntext-based features, respectively and adopted MFH for LV\nfeatures concatenation. Zhou et al. [14] adopted Inception-\nResnet-v2 and BiLSTM to model features from both image\nand text, respectively, and fused the encoded questions with\nthe image features to predict the answers. Abacha et al. [11]\nemployed pre-trained VGG-16 and LSTM for extracting image\nand text features and later used SAN to combine the question\nand image features. In the second edition of ImageCLEF\nchallenge3, the best model [12] adopted BERT and pre-trained\nVGG-16 for text and image features, respectively, and used\nMFB for fusing the VL features. In the third edition of\nImageCLEF challenge 4, the best method [15] detected the\nquestion type by dividing questions into two types, i.e., open-\nended and close-ended type and transformed the VQA task\ninto a simplifier multi-task image classification problem.\nHowever, approaches tested on general-domain VQA for\nMedVQA undergo data scarcity and lack of multilevel reason-\ning ability due to discrepancies between medical and general\ndomains. To overcome the issue of limited data, Nguyen et\nal. [16] presented the Mixture of Enhanced Visual Features\n(MEVF) component, which utilizes the Model-Agnostic Meta-\nLearning (MAML) [31] and the Convolutional Denoising\nAuto-Encoder (CDAE) [32] to solve the data limitation by\ninitializing the model weights for image feature extraction.\nTo enable VQA models to learn reasoning skills due to the\ndisparity of questions, Zhan et al. [17] proposed Question-\nConditioned Reasoning (QCR) and Type Conditioned Reason-\ning (TCR) modules and applied the MEVF image backbone.\nMost recently, to adapt the model to a different form of output,\nRen and Zhou proposed CGMVQA [18] - a novel classifica-\ntion and generative model for MedVQA which integrated both\na classifier and a generator and adopted the multi-head self-\nattention method of a transformer. The CGMVQA model was\ntested on VQA-Med-2019 and outperformed the winner of the\nVQA-Med-2019 challenge. However, these methods are not\ninterpretable. In contrast to the previous studies, our approach\nis designed to generate vision and language interpretations in\nthe context of PathVQA. To the best of our knowledge, it is\nthe first work that fuses both encoder and decoder layers to\nfuse vision and language features and provide interpretation\nfor retrieved answers.\nB. Transformers in Vision-Language Tasks\nRecently, researchers from the VL community have also\nadopted transformers [2], e.g., for video captioning, visual\n2https://www.imageclef.org/2018/VQA-Med\n3https://www.imageclef.org/2019/medical/vqa/\n4https://www.imageclef.org/2020/medical/vqa\nCopyright © 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future \nmedia, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or \nredistribution to servers or lists, or reuse of any copyrighted component of this work in other works.\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final \npublication. Citation information: DOI 10.1109/JBHI.2022.3163751, \nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS AND JOURNALS (FEBRUARY 2017) 3\ncommonsense reasoning (VCR), and VQA. Some of the ex-\namples include Vision, and Language BERT (ViLBERT) [33],\nand Learning Cross Modality Encoder Representations from\nTransformers (LXMERT) [34]; both of these models used two-\nstream BERT with a VL co-attention component to model\nthe vision and language inputs. ViLBERT is fine-tuned on\nvarious downstream tasks, including image-to-text retrieval,\nreferring expression, and VQA, whereas a pre-trained model\nof LXMERT, is fine-tuned on only Visual Reasoning for\nReal (NLVR) and VQA. Other related works such as Visu-\nalBERT [35] and VL-BERT [36] used a single stream of a\ntransformer to model visual and image-text relation for tasks\nlike VQA and VCR. Due to BERTs’ tremendous success\nand popularity, researchers focused only on the encoder part\nof the transformer in all of the studies mentioned above,\nleaving the decoder layer unutilized. Conversely, we present\na unified method using both the encoder and decoder layers\nof a transformer and fully leverage the benefit of a complete\ntransformer architecture.\nIII. M ETHOD\nProblem Definition: First, we define our problem formally,\ngiven a pathology image I and a relevant question Q; the goal\nof the PathVQA task is to predict the answer ˆA. Mathemati-\ncally, it can be formulated as:\nˆA = f(I, Q, θ) (1)\nwhere theta denotes the model parameters, and f is the\nanswer prediction function.\nOverview of the Architecture: TraP-VQA consists of four\nmain components as shown in Fig. 2: (1) question feature ex-\ntraction using domain-specific LM (BioELMO) with BiLSTM\nto capture contextual information; (2) image feature extraction\nusing ResNet with CNN to capture low-level features; (3)\nTransformer Encoder used to fuse the extracted image (vision)\nand question (language) features, and to model high-level\nglobal features and, (4) Transformer Decoder to upsample the\nencoded features for final prediction.\nA. Question Feature Extraction\nBioELMO with BiLSTM: BioELMo [37] is trained on\n10M PubMed biomedical abstracts text and has the same\nnetwork structure as ELMo.\nELMo is a task-specific concatenation of these features\nlearned from Bi-LM, where all layers are flattened to a single\nvector (equation 2).\nELMo task\nn = E(Mn; Θtask) = γtask\nLX\nj=0\nstask\nj hLM\nh,j (2)\nwhere stask are softmax normalised weights for concate-\nnation of several layer representations and γtask is a hyper\nparameter for representation optimisation and scaling.\nWe used pre-trained BioELMo to extract the contextual\nfeatures of the given questions Q, as given by equation. (3).\nBioELMo largely outperforms ELMo and previous SOTA\nmethods in a variety of biomedical text mining tasks.\nFig. 2. Overall architecture of interpretable TraP-VQA.\nXQ = BioELMo (Q) (3)\nUsing BioELMo, a 1,024 dimensional vector XQ, is fed to a\nBiLSTM layer to model the information from both directions\nand outputs a hidden representation hi at a given time i, by\nconcatenating the hidden representations from both forward− →hi and backward ← −hi LSTM (equation 4).\nhi = [− →hi ∥ ← −hi] (4)\nwhere ∥ indicate the concatenate operator, XQ ∈ Rl∗d, l is\nthe question length, d is the vector size for each word. XQ is\npadded to match the maximum questions length lmax to Xpad\nQ\nbefore forwarding to BiLSTM layer ( equation 5).\nXl\nQ = BiLST M(Xpad\nQ ); (5)\nwhere, Xpad\nQ ∈ Rlmax∗d, Xl\nQ ∈ Rlmax∗512; Xl\nQ will go\nthrough a denser layer, a positional encoding, and a dropout\nlayer and outputs a matrix of question features represented by\nXf\nQ.\nwhere Xf\nQ ∈ Rlmax∗512. To extract the question features,\nwe experimented with various general and domain-specific LM\nCopyright © 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future \nmedia, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or \nredistribution to servers or lists, or reuse of any copyrighted component of this work in other works.\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final \npublication. Citation information: DOI 10.1109/JBHI.2022.3163751, IEEE Journal of Biomedical and Health Informatics\n4 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. XX, NO. XX, XXXX 2021\nsuch as ELMo, BERT, BioBERT, and BLUEBERT. BioELMO\nperformed best (see Table III) as compared to others.\nB. Image Feature Extraction\nResNet with CNN: We extracted image features using\npre-trained VGG19, InceptNet, DenseNet, and ResNets and\nidentified that pre-trained ResNet50 performed best compared\nto others (see Table III). We reshaped an image I to match\nthe shape of ResNet50 (224, 224, 3). Since we did not need\nResNet50 to act as a classifier but rather as a feature extractor,\nwe dropped the last three fully connected layers, retaining only\nthe output of the last average pooling layer as image features\nXI (equation 6).\nXI = ResNet50(I ) (6)\nXI is fed to another 2-D CNN layer of kernel size 3, the\nactivation function of ReLu, and forwarded to a dense layer\nto shrink the channel, reshaping and flattening is to maintain\nas much information as possible and outputs a matrix of\nimage features represented by Xl\nI, as given by equation (7).\nThis structure retains as much information as possible while\nmatching the first dimension of the image feature matrix Xl\nI\nto the first dimension of the question feature matrix Xf\nQ.\nXl\nI = Convolution2D (ResNet50(XI); (7)\nwhere; Xl\nI ∈ R7∗7∗512, Xl\nI ∈ Rlmax∗512\nC. Transformer\nTransformer comprises of an encoder(s)-decoder(s) struc-\nture. Each encoder layer is comprised of a multi-head self-\nattention and a feed-forward neural network. Like the encoder,\nthe decoder has three sublayers, two of which are similar\nto the encoder (multi-head self-attention and feed-forward),\nwhile the third sublayer carries out multi-head attention on\nthe encoder’s outputs. The input vector is first transformed\ninto three different vectors: the value vector v, the key vector\nk and the query vector q. Vectors derived from different inputs\nare then combined together into 3 different matrices, namely,\nV, K, and Q (equation 8).\nAtt(Q, K, V) = softmax( Q.KT\n√dk\n).V (8)\nwhere Q ∈ RLxd, K ∈ RLxdd, V ∈ RLxd, L is the\nsequence length and d is the feature depth.\nThe query is passed to the component, which searches for\nthe most similar key and returns the value related to that key.\nTwo matrix multiplications, as well as a softmax function, help\nto speed up the process. softmax(Q.K T ) generates a proba-\nbility distribution with peaks at locations that are positioned\nby the keys for the relating query. This serves as a pseudo-\nmask, and by matrix multiplying it with V , we can get the\ncentralized values that the network should pay attention to in\nthe first place.\n1) Transformer Encoder:We used only the transformer en-\ncoder layer to fuse the image (vision) and question (language)\nfeatures extracted in previous steps. In the original transformer\nencoder, the input (V , Q, K) to the encoder is a sequence of\nwords that we modified and replaced with image and question\nfeatures.\nAt the first encoder layer, the image feature matrix Xl\nI, is\nused as V , and the question feature matrix Xf\nQ is forwarded\nas an input of both Q and K with positional encoding. At the\nsecond encoder layer, we again used Xl\nI as input V and the\noutput of the first encoder layer is forwarded to Q and V of\nthe second encoder. Here the input V , Q, and V are processed\nin the same way as in the original transformer encoder layer.\n2) Transformer Decoder: We used the same transformer\ndecoder layer as the original transformer to upsample the\nencoded features. Here, again we used two decoder layers for\nthe final prediction.\nAt first, a one-hot vector of < start > token will go\nthrough a trainable embedding layer, and positional encoding\nis fed to the decoder layers. The softmax function will give a\nprobability distribution of each one-hot vector. The decoder\nwill take the one-hot vector with the highest probability\nand append the corresponding vocabulary to the answer. The\ndecoding process continues until the decoder generates the\none-hot vector of the < end > token. Here the working\nmechanism of decoder layers is the same as in the original\ntransformer decoder layers.\nIV. EXPERIMENTS AND RESULTS\nA. Datasets\nWe experimented with data sets from PathVQA [38]. The\nquestion design was inspired by the examination of the Amer-\nican Board of Pathology (ABP), with the aim to capture\nmedical questions that are a part of a certified pathologist\ntesting in the US. The PathVQA contains 4,998 images and\n32,799 QA pairs. The images with captions are extracted\nfrom textbooks and online digital libraries. The question are\ndivided into seven categories: what (40.9%), where (4.0%),\nwhen (0.9%), whose (0.6%), how(3.0%), how much/how many\n(0.9%), and, yes/no (49.8%). The first six categories’ questions\nare open-ended, including 16,465 in total and accounting for\n50.2% of all questions. The last category is the close-ended\n(yes or no) question. We used the standard training, validation\nand test set provided in [38] to evaluate our model.\nB. Experimental Settings\nWe trained our model using Adam [39] optimizer with a\nlearning rate of 0.0001 and with a batch size of 64 for 20\nepochs. We used the grid-search optimization technique to tune\nthe optimal parameters. We tested with different transformer\nlayers (see Table ?? in appendix) and used accuracy as an\nevaluation metric.\nC. Baseline Methods\nWe compared our results with the following baselines.\nCopyright © 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future \nmedia, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or \nredistribution to servers or lists, or reuse of any copyrighted component of this work in other works.\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final \npublication. Citation information: DOI 10.1109/JBHI.2022.3163751, \nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS AND JOURNALS (FEBRUARY 2017) 5\nGeneral VQA Methods:\nBAN [24]: embeds image and textual features using a Gated\nRecurrent Unit and a Faster R-CNN network and learns\nbilinear attention distributions employing bilinear attention\nnetworks (BAN) and approximates the bilinear interaction\nbetween question and image embeddings using low-rank ap-\nproximations.\nMCB [22]: A CNN encodes the image, while an LSTM\nnetwork encodes the questions and answers. An attention\nmechanism is used to infer the answer using a multimodal\ncompact bilinear (MCB) pooling method.\nSAN [23]: With CNN and LSTM, the stacked attention\nnetwork (SAN) locates image regions that are useful to answer\nthe question. It queries the image several times to narrow the\nregion to be observed.\nMFB [25]: embeds images and questions/answers using CNN\nand LSTM and uses Multi-Modal Factorized Bilinear (MFB)\npooling to fuse question and image features.\nMEVF [16]: extracts image and language features using CNN\nand LSTM and uses a mixture of enhanced visual features\n(MEVF) with SAN and BAN to fuse visual and language\nfeatures.\nOther Methods:\nVision language: We used SOTA vision language-based mod-\nels such as LXMERT [34], VisualBERT [35] and UniTER [40]\nto fuse image and the language features extracted using CNN\nand LSTM.\nCMSSL [41]: a SOTA approach for PathVQ, which detects\nand ignores noisy self-supervised examples from pretraining\nto learn robust visual and textual features.\nD. Results\nTable I presents the results of TraP-VQA and the baselines\nTraP-VQA achieved the best performance and outperformed\nall the baselines. Compared to the following closed method\n(UniTER), our method achieves 64.82% accuracy, an absolute\nincrease of 4.49% for the overall task and 37.72% accuracy\nfor open-ended question type, which is an absolute increase\nof 2.39% compared to second-best (LXMERT). Furthermore,\n93.57% accuracy is an absolute increase of 5.87% compared\nto the second-best method (UniTER) for closed-ended ques-\ntion types. We observed that general VQA methods such\nas MFB, SAN, MCB, and BAN perform poorly compared\nto transformer-based methods. Although MEVF+BAN and\nMEVF+SAN perform better than the base BAN and SAN\nmethods, performance is less than that of transformer-based\nmethods.\nOur results showed that our TraP-VQA consistently out-\nperformed all the baselines. These results demonstrate the\neffectiveness of the transformers in capturing global rela-\ntionships, as evident from the baselines failing to capture\nthem; the attention maps generated by the scaled dot-product\nattention module in a transformer highlight the image region\nresponsible for each generated text token. This performance\nimprovement can be thought to be due to the presented\nframework’s ability to encode low-level visual features using\na convolutional neural network (ResNet50) and a domain-\nspecific language model (BioELMo) for text representation\nTABLE I\nCOMPARISON OF PROPOSED METHOD V /S THE BASELINES .\nModel Overall Open-ended Close-ended\nMFB [25] 39.85 20.15 53.77\nSAN [23] 42.43 23.40 59.40\nMCB [22] 57.04 29.03 57.60\nBAN [24] 55.10 33.50 68.20\nMEVF +SAN [16] 57.10 25.87 86.90\nMEVF +BAN [16] 57.90 26.75 87.50\nLXMERT [34] 60.00 35.33 83.00\nVisualBERT [35] 60.08 33.03 86.99\nUniTER [40] 60.33 33.79 87.70\nLXMERT+CMSSL [41] 60.10 34.50 87.10\nBAN+CMSSL [41] 58.40 33.50 87.20\nOurs 64.82 37.72 93.57\ntrained on relevant corpora, as well as its ability to leverage\nthe powerful transformer capability in modeling the global\nrelationship.\nE. Ablation Study\n1) Effects of using text only features with transformer:Fol-\nlowing [41], we fused text only features with a transformer to\nanalyze the impact on the performance from the absence of the\nimages. Table II shows the results of using text-only features\nextracted from different LMs with a transformer. We observed\na 10.72% drop in performance (from 64.82% to 54.10%) in\nthe overall task. For open-ended question types, a 16.44%\ndrop in performance was observed (from 37.72% to 21.28%),\nwhereas, in closed-ended question types, a minor drop (1.92%)\nis observed when we used BioELMO to extract text features.\nPerformance dropped in all other cases when we used text-only\nfeatures (ranging from 9.98% to 11.25%) overall task, 16.44%\nto 19.70% in open-ended question types, and 0.72% to 3.65%\nin closed-ended question types. Although the accuracy drops\nwhen using only the text features obtained by BioELMo, the\nperformance is better than the overall accuracy of 2 baselines\n(MFB and SAN) when compared to using the full model\n(TraP-VQA). We attribute these due to the fact that most\nquestions do not require visual content to answer questions.\nThis drop-in accuracy shows the importance of using both VL\nfeatures in our model.\n2) Effect of different VL models with transformer: We re-\nplaced different pre-trained VL models to extract features and\nfused them to the transformer layer. Table III shows the results\nfrom fusing the transformer with different models to extract\nthe image and question features. The optimal combination was\nfound to be the use of ResNet50 and BioELMo in all tasks.\nThe performance varies from 51.79% to 64.82%, 9.57% to\nTABLE II\nCOMPARISON OF FUSING ONLY LANGUAGE FEATURES (TEXT ONLY )\nWITH TRANSFORMER .\nModel Overall Open-ended Close-ended\nBioELMo 54.10 21.28 91.65\nBioBERT 47.53 11.83 78.38\nBLUEBERT 46.79 11.05 74.74\nBERT 46.92 12.22 76.78\nCopyright © 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future \nmedia, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or \nredistribution to servers or lists, or reuse of any copyrighted component of this work in other works.\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final \npublication. Citation information: DOI 10.1109/JBHI.2022.3163751, IEEE Journal of Biomedical and Health Informatics\n6 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. XX, NO. XX, XXXX 2021\nTABLE III\nCOMPARISON OF FUSING DIFFERENT PRE -TRAINED CNN S AND LMS USED FOR VL FEATURE EXTRACTION WITH TRANSFORMER .\nImage\\Question Overall Open-ended Close-ended\nBioELMo BioBERT BLUEBERT BERT BioELMo BioBERT BLUEBERT BERT BioELMo BioBERT BLUEBERT BERT\nResNet50 64.82 58.78 56.91 56.90 37.72 31.53 30.24 30.72 93.57 79.10 81.76 80.43\nInception 51.79 49.03 47.67 48.36 9.57 12.06 12.63 10.59 92.14 83.56 81.42 82.64\nDenseNet 57.71 54.54 52.93 51.78 23.63 21.59 21.38 20.98 93.26 80.94 82.51 82.03\nVGG19 58.64 56.74 56.52 55.64 33.69 26.48 27.70 27.70 92.82 79.85 80.63 77.91\n37.72% and 92.14% to 0 93.57% in accuracy for overall,\nopen and closed-ended tasks respectively when BioELMO is\nfused with different pre-trained CNN models to a transformer.\nWe observed that in all cases, ResNet50’s performance was\nmore prominent compared to others. In addition, it had the\nhighest consistency among the different LMs. Further, for\nquestion features, BioELMo outperformed all other methods\nin extracting question features compared to SOTA BERT and\nbiomedical versions of BERT on all tasks. This is expected, as\nBioELMo is proven to be a better-fixed feature extractor and\nalso better at clustering similar information than BERT-based\nmodels for extraction question features [37].\nF . Robustness to other MedVQA\nTo evaluate the robustness of our method, we designed\nexperiments using other MedVQA datasets. We note that there\nwas no other public pathology VQA dataset in our thorough in-\nvestigation. Hence, we used SLAKE [42], a MedVQA dataset\nof radiology images. This dataset is different because it ensures\nthe diversity of modalities (e.g., CT, MRI, and X-Ray), covered\nbody parts (e.g., head, neck, and chest), and question types\n(e.g., vision-only, knowledge-based, and bilingual). SLAKE\nis a comprehensive MedVQA dataset with semantic labels\nand a structured medical knowledge base annotated by expert\nphysicians (see Fig. ?? in an appendix for examples). Fig. ??\nin the appendix presents the detailed results, showing that\nour method outperformed the baselines, including the SOTA\nmethod used in [42] on other MedVQA datasets due to its\nability to capture global relationships between image-question\npairs. These results validate the robustness of our method on\nMedVQA tasks.\nG. TraP-VQA Interpretation\nUsing existing state-of-the-art interpretable tools such\nas Gradient-weighted Class Activation Mapping (Grad-\nCAM) [43] and SHapley Additive exPlanations (SHAP) [44],\nwe perform a qualitative evaluation for visual, textual, and\ntransformer attention interpretations on the PathVQA dataset.\nIn addition, we illustrate qualitative examples of multi-modal\ninterpretations.\n1) Qualitative Evaluation for Textual Interpretation:To eval-\nuate the use of BioELMO compared to other LMs, we\nperformed interpretable qualitative analysis. Fig. 3 illustrates\nthe visual representation of different textual features extracted\nusing Word2Vec, BioELMo, BERT, BioBERT, and BLUE-\nBERT used in our model. We used K-means [45] to cluster the\ntextual embedding into the two-dimensional feature space. It is\nclear to observe from Fig. 3 that BioELMo embeddings show\nseparable distributions compared to other LMs (Fig. 3). This\ndemonstration visually shows the efficacy of using BioELMo\nembeddings in our model. In Table II, we further show the\nimportance of textual features in our model and demonstrated\nthat BioELMo performed better compared to other language\nfeatures.\n2) Qualitative Evaluation for transformers’ attention layer In-\nterpretation: To quantify the proposed transformer as a fusion\nlayer, we performed an interpretable qualitative analysis. In\nFig. 4, we show a pathology image (left) with a relevant ques-\ntion (top) and SHAP values at the bottom of the image. High\nvisual scores (attention weights) are shown in blue, whereas\nlow visual scores are shown in red. The visual scores are taken\nas values from attention weights from the last decoder layer\nof the transformer. We observe that in closed-ended question\ntypes, TraP-VQA assigned high visual scores to words like\n‘pemphigus’ and ‘gland.’ These words are directly associated\nwith their pathology image counterpart; for example, top left\nshows the image of dilation and hypertrophy in pemphigus\nvulgaris and gland acini for the bottom left image. Similarly,\nin the open-ended question types, the visualization shows that\nour model gives more weight to words like ‘Where’ (top\nright), ‘What,’ and ‘present’ (bottom right) according to our\nintuition and low visual scores determiners and prepositions.\nThis visualization explains the reason behind the retrieved\nanswer and how TraP-VQA assigns more weight to important\nwords relevant to pathology images.\nFig. 3. Interpretation (Visualization) of textual features obtained using different language models.\nCopyright © 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future \nmedia, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or \nredistribution to servers or lists, or reuse of any copyrighted component of this work in other works.\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final \npublication. Citation information: DOI 10.1109/JBHI.2022.3163751,\nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS AND JOURNALS (FEBRUARY 2017) 7\nFig. 4. Visual scores of words in transformers attention. We demonstrate the attention weight of the last transformer layer as the raw visual scores.\nBlue (high) and red (low) represent high and low visual scores.\nFig. 5. Trap-VQA Visualization using SHAP . The red color represents high scores, whereas blue shows low scores.\n3) Qualitative Evaluation for Trap-VQA Interpretation: In\nFig. 5 and Fig. 6, we show outputs of visual and textual\ninterpretations for the unseen classes on PathVQA. Fig. 5\nis an example of visual interpretations obtained using SHAP.\nSHAP is a game-theoretic method for explaining any machine\nlearning model’s output. It uses the traditional Shapley values\nfrom game theory and associated extensions to connect opti-\nmal credit allocation with local explanations and final output\nvisualization. In Fig. 5, we present a pathology image at the\ntop with SHAP values and a relevant question at the bottom,\nalso with SHAP values. High visual scores are indicated by\nred, whereas low visual scores are shown by blue. From\nthe visual scores at the bottom of the pathology images in\nFig. 5 (left), we see that TraP-VQA gives more weight (red)\nto relevant words such as ‘coronary’ for the question ‘What\ndoes this image show?’ and ‘right coronary artery’ region is\nappropriately highlighted in red pixels (boxes) in the image\n(top left). However, in the close-ended question ( ‘Is the\nnormal gland acini associated with chronic alcohol use?’)\nshown in Fig. 5 (right), a high visual score is assigned to\nthe question word ‘Is’ and predicts a correct answer ‘no\n.’This interpretation of TraP-VQA is aligned with the attention\nweight visualization shown in Fig. 4 and explains the reason\nfor the retrieved answer.\nTo evaluate the use of ResNet in TraP-VQA compared\nto other pre-trained CNNs, we performed an interpretable\nqualitative analysis. Fig. 6 visualizes the extracted image\nfeatures (column 2 to column 5) using Grad-Cam. We can see\nthat different models emphasized different parts of the image,\nand ResNet (column 2) best associated the region of interest\nwhen compared to other CNN-based models. We attribute this\nto ResNet having a deeper (50 layers) model compared to\nInceptNet, DenseNet, and VGG19. This is consistent with\nResNet having the best feature extraction performance as in\nTable III.\nFurthermore, Fig. 6 presents the randomly selected ex-\namples where TraP-VQA correctly predicted the answers.\nA pathology image and a relevant question are shown in\nthe left (column 1), and the visualization of different CNN\nmodels and predicted answering using different models are\nshown in the right columns 2 to 5. TraP-VQA focuses on the\nregion of interest corresponding to the attributed label, whereas\nother models failed in some predictions. Correct answers are\nindicated in green, while incorrect answers are shown in red.\nV. CONCLUSION\nThis paper presented a TraP-VQA method that embeds the\nimage and question features, coupled with domain-specific\ncontextual information, via a transformer for PathVQA. We\nused Grad-Cam and SHAP to interpret our retrieved answers\nvisually to indicate which area of the image contributed to\nthe predicted answer. We show that using ResNet in our\nCopyright © 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future \nmedia, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or \nredistribution to servers or lists, or reuse of any copyrighted component of this work in other works.\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final \npublication. Citation information: DOI 10.1109/JBHI.2022.3163751, IEEE Journal of Biomedical and Health Informatics\n8 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. XX, NO. XX, XXXX 2021\nFig. 6. Qualitative results of TraP-VQA: Original pathology images and their associated relevant questions are shown in the left column 1, whereas,\nvisualization of different CNN models using Grad-Cam and predicted answering using different models are shown in the right column 2 to column\n5. Correct answers are indicated ingreen, while incorrect answers are shown inred\nmodel focuses on the region of interest. In contrast, other\nmodels sometimes focus on the wrong part of the image. For\ntextual interpretations, we visually show that text embeddings\nobtained using domain-specific language models have clear\nseparable distributions compared to other language models\ntested. In addition, visualization of the transformers’ attention\nshowed proposed model assigns more weight to the relevant\nwords and explains the reason for the retrieved answer. Empir-\nical evaluation of the popular benchmark dataset of PathVQA\ndemonstrated that our method achieved superior performance\nrelative to state-of-the-art comparative models and ensured\nadequate evidence to interpret the retrieved answers.\nREFERENCES\n[1] Muhammad Khalid Khan Niazi, Anil V Parwani, and Metin N Gurcan.\nDigital pathology and artificial intelligence. The lancet oncology,\n20(5):e253–e261, 2019.\n[2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, and booktitle=Advances in neural information\nprocessing systems pages=5998–6008 year=2017 Kaiser, Lukasz and\nPolosukhin, Illia. Attention is all you need.\n[3] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo,\nZhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, et al. A\nsurvey on visual transformer. arXiv preprint arXiv:2012.12556, 2020.\n[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBert: Pre-training of deep bidirectional transformers for language un-\nderstanding. arXiv preprint arXiv:1810.04805, 2018.\n[5] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,\nand Ilya Sutskever. Language models are unsupervised multitask\nlearners. 2018.\n[6] Usman Naseem, Imran Razzak, Shah Khalid Khan, and Mukesh Prasad.\nA comprehensive survey on word representation models: From classical\nto state-of-the-art word representation language models. Transactions on\nAsian and Low-Resource Language Information Processing, 20(5):1–35,\n2021.\n[7] Usman Naseem, Matloob Khushi, Vinay Reddy, Sakthivel Rajendran,\nImran Razzak, and Jinman Kim. Bioalbert: A simple and effective pre-\ntrained language model for biomedical named entity recognition. In 2021\nInternational Joint Conference on Neural Networks (IJCNN), pages 1–7.\nIEEE, 2021.\n[8] Usman Naseem, Adam G Dunn, Matloob Khushi, and Jinman Kim.\nCopyright © 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, \nincluding reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, \nor reuse of any copyrighted component of this work in other works.\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final \npublication. Citation information: DOI 10.1109/JBHI.2022.3163751, \nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS AND JOURNALS (FEBRUARY 2017) 9\nBenchmarking for biomedical natural language processing tasks with a\ndomain specific albert. arXiv preprint arXiv:2107.04374, 2021.\n[9] Yoon Kim. Convolutional neural networks for sentence classification.\narXiv preprint arXiv:1408.5882, 2014.\n[10] Asma Ben Abacha, Soumya Gayen, Jason J Lau, Sivaramakrishnan\nRajaraman, and Dina Demner-Fushman. Nlm at imageclef 2018 visual\nquestion answering in the medical domain. In CLEF (Working Notes) ,\n2018.\n[11] Asma Ben Abacha, Sadid A Hasan, Vivek V Datla, Joey Liu, Dina\nDemner-Fushman, and Henning M ¨uller. Vqa-med: Overview of the\nmedical visual question answering task at imageclef 2019. In CLEF\n(Working Notes), 2019.\n[12] Xin Yan, Lin Li, Chulin Xie, Jun Xiao, and Lin Gu. Zhejiang university\nat imageclef 2019 visual question answering in the medical domain. In\nCLEF (Working Notes), 2019.\n[13] Yalei Peng, Feifan Liu, and Max P Rosen. Umass at imageclef medical\nvisual question answering (med-vqa) 2018 task. In CLEF (Working\nNotes), 2018.\n[14] Yangyang Zhou, Xin Kang, and Fuji Ren. Employing inception-resnet-\nv2 and bi-lstm for medical domain visual question answering. In CLEF\n(Working Notes), 2018.\n[15] Zhibin Liao, Qi Wu, Chunhua Shen, Anton van den Hengel, and Johan\nVerjans. Aiml at vqa-med 2020: Knowledge inference via a skeleton-\nbased sentence mapping approach for medical domain visual question\nanswering. CLEF, 2020.\n[16] Binh D Nguyen, Thanh-Toan Do, Binh X Nguyen, Tuong Do, Erman\nTjiputra, and Quang D Tran. Overcoming data limitation in medical\nvisual question answering. In International Conference on Medical\nImage Computing and Computer-Assisted Intervention, pages 522–530.\nSpringer, 2019.\n[17] Li-Ming Zhan, Bo Liu, Lu Fan, Jiaxin Chen, and Xiao-Ming Wu. Medi-\ncal visual question answering via conditional reasoning. In Proceedings\nof the 28th ACM International Conference on Multimedia, pages 2345–\n2354, 2020.\n[18] Fuji Ren and Yangyang Zhou. Cgmvqa: a new classification and\ngenerative model for medical visual question answering. IEEE Access,\n8:50626–50636, 2020.\n[19] Lei Shi, Feifan Liu, and Max P Rosen. Deep multimodal learning for\nmedical visual question answering. In CLEF (Working Notes), 2019.\n[20] Sadid A Hasan, Yuan Ling, Oladimeji Farri, Joey Liu, Henning M ¨uller,\nand Matthew P Lungren. Overview of imageclef 2018 medical domain\nvisual question answering task. In CLEF (Working Notes), 2018.\n[21] Asma Ben Abacha, Vivek V Datla, Sadid A Hasan, Dina Demner-\nFushman, and Henning M ¨uller. Overview of the vqa-med task at\nimageclef 2020: Visual question answering and generation in the medical\ndomain. CLEF 2020 Working Notes, pages 22–25, 2020.\n[22] Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor\nDarrell, and Marcus Rohrbach. Multimodal compact bilinear pooling\nfor visual question answering and visual grounding. arXiv preprint\narXiv:1606.01847, 2016.\n[23] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola.\nStacked attention networks for image question answering. In Proceed-\nings of the IEEE conference on computer vision and pattern recognition,\npages 21–29, 2016.\n[24] Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. Bilinear attention\nnetworks. arXiv preprint arXiv:1805.07932, 2018.\n[25] Zhou Yu, Jun Yu, Jianping Fan, and Dacheng Tao. Multi-modal\nfactorized bilinear pooling with co-attention learning for visual question\nanswering. In Proceedings of the IEEE international conference on\ncomputer vision, pages 1821–1830, 2017.\n[26] Zhou Yu, Jun Yu, Chenchao Xiang, Jianping Fan, and Dacheng Tao.\nBeyond bilinear: Generalized multimodal factorized high-order pooling\nfor visual question answering. IEEE transactions on neural networks\nand learning systems, 29(12):5947–5959, 2018.\n[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep\nresidual learning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 770–778,\n2016.\n[28] Karen Simonyan and Andrew Zisserman. Very deep convolu-\ntional networks for large-scale image recognition. arXiv preprint\narXiv:1409.1556, 2014.\n[29] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory.\nNeural Computation, 9(8):1735–1780, 1997.\n[30] Junyoung Chung, C ¸ aglar G¨ulc ¸ehre, KyungHyun Cho, and Yoshua Ben-\ngio. Empirical evaluation of gated recurrent neural networks on sequence\nmodeling. CoRR, abs/1412.3555, 2014.\n[31] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic\nmeta-learning for fast adaptation of deep networks. In International\nConference on Machine Learning, pages 1126–1135. PMLR, 2017.\n[32] Jonathan Masci, Ueli Meier, Dan Cires ¸an, and J ¨urgen Schmidhuber.\nStacked convolutional auto-encoders for hierarchical feature extraction.\nIn International conference on artificial neural networks, pages 52–59.\nSpringer, 2011.\n[33] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretrain-\ning task-agnostic visiolinguistic representations for vision-and-language\ntasks. arXiv preprint arXiv:1908.02265, 2019.\n[34] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder\nrepresentations from transformers. arXiv preprint arXiv:1908.07490,\n2019.\n[35] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei\nChang. Visualbert: A simple and performant baseline for vision and\nlanguage. arXiv preprint arXiv:1908.03557, 2019.\n[36] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng\nDai. Vl-bert: Pre-training of generic visual-linguistic representations.\narXiv preprint arXiv:1908.08530, 2019.\n[37] Qiao Jin, Bhuwan Dhingra, William W. Cohen, and Xinghua Lu. Probing\nbiomedical embeddings from language models, 2019.\n[38] Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie.\nPathvqa: 30000+ questions for medical visual question answering. arXiv\npreprint arXiv:2003.10286, 2020.\n[39] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980, 2014.\n[40] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed,\nZhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text\nrepresentation learning. In European conference on computer vision,\npages 104–120. Springer, 2020.\n[41] Xuehai He, Zhuo Cai, Wenlan Wei, Yichen Zhang, Luntian Mou, Eric\nXing, and Pengtao Xie. Pathological visual question answering. arXiv\npreprint arXiv:2010.12435, 2020.\n[42] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu.\nSlake: A semantically-labeled knowledge-enhanced dataset for medical\nvisual question answering. In 2021 IEEE 18th International Symposium\non Biomedical Imaging (ISBI), pages 1650–1654. IEEE, 2021.\n[43] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakr-\nishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual\nexplanations from deep networks via gradient-based localization. In\nProceedings of the IEEE international conference on computer vision,\npages 618–626, 2017.\n[44] Scott M Lundberg and Su-In Lee. A unified approach to interpreting\nmodel predictions. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach,\nR. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in\nNeural Information Processing Systems 30, pages 4765–4774. Curran\nAssociates, Inc., 2017.\n[45] James MacQueen et al. Some methods for classification and analysis of\nmultivariate observations. In Proceedings of the fifth Berkeley symposium\non mathematical statistics and probability , volume 1, pages 281–297.\nOakland, CA, USA, 1967.\nCopyright © 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or \nfuture media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for \nresale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final \npublication. Citation information: DOI 10.1109/JBHI.2022.3163751, IEEE Journal of Biomedical and Health Informatics"
}