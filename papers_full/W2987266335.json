{
  "title": "Syntax-Infused Transformer and BERT models for Machine Translation and Natural Language Understanding",
  "url": "https://openalex.org/W2987266335",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4227046523",
      "name": "Sundararaman, Dhanasekar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2525333815",
      "name": "Subramanian, Vivek",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2183268562",
      "name": "Wang Guoyin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222526195",
      "name": "Si, Shijing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288513539",
      "name": "Shen, Dinghan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2042012318",
      "name": "Wang Dong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2619478227",
      "name": "Carin Lawrence",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963913268",
    "https://openalex.org/W2410082850",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2773734397",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2789541106",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W2743555600",
    "https://openalex.org/W2889607424",
    "https://openalex.org/W2798727047",
    "https://openalex.org/W2964302946",
    "https://openalex.org/W2963653811",
    "https://openalex.org/W2971191050"
  ],
  "abstract": "Attention-based models have shown significant improvement over traditional algorithms in several NLP tasks. The Transformer, for instance, is an illustrative example that generates abstract representations of tokens inputted to an encoder based on their relationships to all tokens in a sequence. Recent studies have shown that although such models are capable of learning syntactic features purely by seeing examples, explicitly feeding this information to deep learning models can significantly enhance their performance. Leveraging syntactic information like part of speech (POS) may be particularly beneficial in limited training data settings for complex models such as the Transformer. We show that the syntax-infused Transformer with multiple features achieves an improvement of 0.7 BLEU when trained on the full WMT 14 English to German translation dataset and a maximum improvement of 1.99 BLEU points when trained on a fraction of the dataset. In addition, we find that the incorporation of syntax into BERT fine-tuning outperforms baseline on a number of downstream tasks from the GLUE benchmark.",
  "full_text": "Syntax-Infused Transformer and BERT models for\nMachine Translation and Natural Language Understanding\nDhanasekar Sundararaman, Vivek Subramanian, Guoyin Wang, Shijing Si,\nDinghan Shen, Dong Wang, Lawrence Carin\nDepartment of Electrical & Computer Engineering, Duke University\nDurham, NC 27708\n{dhanasekar.sundararaman, vivek.subramanian, guoyin.wang, shijing.si, dinghan.shen, dong.wang363, lcarin}@duke.edu\nAbstract\nAttention-based models have shown signiﬁcant im-\nprovement over traditional algorithms in several NLP\ntasks. The Transformer, for instance, is an illustrative\nexample that generates abstract representations of to-\nkens inputted to an encoder based on their relation-\nships to all tokens in a sequence. Recent studies have\nshown that although such models are capable of learn-\ning syntactic features purely by seeing examples, ex-\nplicitly feeding this information to deep learning mod-\nels can signiﬁcantly enhance their performance. Lever-\naging syntactic information like part of speech (POS)\nmay be particularly beneﬁcial in limited training data\nsettings for complex models such as the Transformer.\nWe show that the syntax-infused Transformer with mul-\ntiple features achieves an improvement of 0.7 BLEU\nwhen trained on the full WMT ’14 English to Ger-\nman translation dataset and a maximum improvement\nof 1.99 BLEU points when trained on a fraction of the\ndataset. In addition, we ﬁnd that the incorporation of\nsyntax into BERT ﬁne-tuning outperforms baseline on\na number of downstream tasks from the GLUE bench-\nmark.\nIntroduction\nAttention-based deep learning models for natural language\nprocessing (NLP) have shown promise for a variety of ma-\nchine translation and natural language understanding tasks.\nFor word-level, sequence-to-sequence tasks such as transla-\ntion, paraphrasing, and text summarization, attention-based\nmodels allow a single token ( e.g., a word or subword) in a\nsequence to be represented as a combination of all tokens in\nthe sequence (Luong, Pham, and Manning, 2015). The dis-\ntributed context allows attention-based models to infer rich\nrepresentations for tokens, leading to more robust perfor-\nmance. One such model is the Transformer, which features\na multi-headed self- and cross-attention mechanism that al-\nlows many different representations to be learned for a given\ntoken in parallel (Vaswani et al., 2017). The encoder and\ndecoder arms each contain several identical subunits that\nare chained together to learn embeddings for tokens in the\nsource and target vocabularies.\nThough the Transformer works well across a variety of\ndifferent language pairs, such as (English, German) and (En-\nglish, French), it consists of a large number of parameters\nand relies on a signiﬁcant amount of data and extensive\ntraining to accurately pick up on syntactic and semantic rela-\ntionships. Previous studies have shown that an NLP model’s\nperformance improves with the ability to learn underlying\ngrammatical structure of a sentence (Kuncoro et al., 2018;\nLinzen, Dupoux, and Goldberg, 2016). In addition, it has\nbeen shown that simultaneously training models for machine\ntranslation, part of speech (POS) tagging, and named entity\nrecognition provides a slight improvement over baseline on\neach task for small datasets (Niehues and Cho, 2017). In-\nspired by these previous efforts, we propose to utilize the\nsyntactic features that are inherent in natural language se-\nquences, to enhance the performance of the Transformer\nmodel.\nWe suggest a modiﬁcation to the embeddings fed into the\nTransformer architecture, that allows tokens inputted into\nthe encoder to attend to not only other tokens but also syn-\ntactic features including POS, case, and subword position.\nThese features are identiﬁed using a separate model (for\nPOS) or are directly speciﬁed (for case and subword posi-\ntion) and are appended to the one-hot vector encoding for\neach token. Embeddings for the tokens and their features are\nlearned jointly during the Transformer training process. As\nthe embeddings are passed through the layers of the Trans-\nformer, the representation for each token is synthesized us-\ning a combination of word and syntactic features.\nWe evaluate the proposed model on English to German\n(EN-DE) translation on the WMT ’14 dataset. For the EN-\nDE translation task, we utilize multiple syntactic features\nincluding POS, case and subword tags that denote the rel-\native position of subwords within a word (Sennrich and\nHaddow, 2016). Like POS, case is a categorical feature,\nwhich can allow the model to distinguish common words\nfrom important ones. Subword tags can help bring cohe-\nsion among subwords of a complex word (say, “amalgama-\ntion”) so that their identity as a unit is not compromised by\ntokenization. We prove that the incorporation of these fea-\ntures improves the translation performance in the EN-DE\ntask with a number of different experiments. We show that\nthe BLEU score improvements of the feature-rich syntax-\ninfused Transformer uniformly outperforms the baseline\nTransformer as a function of the training data size. Exam-\nining the attention weights learned by the proposed model\nfurther justiﬁes the effectiveness of incorporating syntactic\narXiv:1911.06156v1  [cs.CL]  10 Nov 2019\nfeatures.\nWe also experiment with this modiﬁcation of embeddings\non the BERTBASE model on a number of General Language\nUnderstanding Evaluation (GLUE) benchmarks and observe\nconsiderable improvement in performance on multiple tasks.\nWith the addition of POS embeddings, the BERT BASE + POS\nmodel outperforms BERT BASE on 4 out of 8 downstream\ntasks.\nTo summarize, our main contributions are as follows:\n1. We propose a modiﬁcation to the trainable embeddings of\nthe Transformer model, incorporating explicit syntax in-\nformation, and demonstrate superior performance on EN-\nDE machine translation task.\n2. We modify pretrained BERT BASE embeddings by feeding\nin syntax information and ﬁnd that the performance of\nBERTBASE + POS outperforms BERTBASE on a number of\nGLUE benchmark tasks.\nBackground\nBaseline Transformer\nThe Transformer consists of encoder and decoder modules,\neach containing several subunits that act sequentially to gen-\nerate abstract representations for words in the source and\ntarget sequences (Vaswani et al., 2017). As a preprocessing\nstep, each word is ﬁrst divided into subwords of length less\nthan or equal to that of the original word (Sennrich, Had-\ndow, and Birch, 2015). These subwords are shared between\nthe source and target vocabularies.\nFor all m∈{1,2, ..., M}, where M is the length of the\nsource sequence, the encoder embedding layer ﬁrst converts\nsubwords xm into embeddings em:\nem = Exm (1)\nwhere E ∈ RD×N is a trainable matrix with column m\nconstituting the embedding for subword m, N is the total\nnumber of subwords in the shared vocabulary, and xm ∈\n{0,1}N : ∑\ni xmi = 1 is a one-hot vector corresponding\nto subword m. These embeddings are passed sequentially\nthrough six encoder subunits. Each of these subunits fea-\ntures a self-attention mechanism, that allows subwords in\nthe input sequence to be represented as a combination of all\nsubwords in the sequence. Attention is accomplished using\nthree sets of weights: the key, query, and value matrices (K,\nQ, and V, respectively). The key and query matrices interact\nto score each subword in relation to other subwords, and the\nvalue matrix gives the weights to which the score is applied\nto generate output embedding of a given subword. Stated\nmathematically,\nK = HWK\nQ = HWQ\nV = HWV\nA = softmax\n(QK⊤\n√ρ\n)\nV\n(2)\nwhere H = [ h1 h2 ··· hM ]⊤ ∈ RM×D are the D-\ndimensional embeddings for a sequence of M subwords in-\ndexed by m; WK, WQ, and WV all ∈ RD×P are the\nFigure 1: Formation of attention matrices ( K, Q, and V)\nwith syntactic information. The left column shows the word\nembedding matrix; the embedding matrices for the various\nfeatures are shown on top. Embeddings for the chosen fea-\ntures are either concatenated or summed together (denoted\nby ⊕) and ﬁnally, concatenated to the word embeddings.\nMatrix multiplication with learned weights results in K, Q,\nand V. The attention matrices are double shaded to indicate\nthe mix of word and syntax information.\nprojection matrices for keys, queries, and values, respec-\ntively; ρ is a scaling constant (here, taken to be P) and\nA ∈RM×P is the attention-weighted representation of each\nsubword. Note that these are subunit-speciﬁc – a separate\nattention-weighted representation is generated by each sub-\nunit and passed on to the next. Moreover, for the ﬁrst layer,\nhm := em.\nThe ﬁnal subunit then passes its information to the de-\ncoder, that also consists of six identical subunits that behave\nsimilarly to those of the encoder. One key difference be-\ntween the encoder and decoder is that the decoder not only\nfeatures self-attention but also cross-attention; thus, when\ngenerating new words, the decoder pays attention to the en-\ntire input sequence as well as to previously decoded words.\nBERT\nWhile the Transformer is able to generate rich represen-\ntations of words in a sequence by utilizing attention, its\ndecoder arm restricts it to be task-speciﬁc. The word em-\nbeddings learned by the Transformer encoder, however, can\nbe ﬁne-tuned to perform a number of different downstream\ntasks. Bidirectional encoder representations of Transform-\ners (BERT) is an extension of the Transformer model that\nallows for such ﬁne-tuning. The BERT model is essentially\na Transformer encoder (with number of layers l, embedding\ndimension D, and number of attention heads α) which is\npre-trained using two methods: masked language modeling\n(MLM) and next-sentence prediction (NSP). Subsequently,\na softmax layer is added, allowing the model to perform var-\nious tasks such as classiﬁcation, sequence labeling, question\nanswering, and language inference. According to (Devlin et\nal., 2018), BERT signiﬁcantly outperforms previous state-\nof-the-art models on the eleven NLP tasks in the GLUE\nbenchmark (Wang et al., 2018).\nModel\nSyntax-infused Transformer\nSyntax is an essential feature of grammar that facilitates\ngeneration of coherent sentences. For instance, POS dictates\nhow words relate to one another (e.g., verbs represent the ac-\ntions of nouns, adjectives describe nouns, etc.). Studies have\nshown that when trained for a sufﬁciently large number of\nsteps, NLP models can potentially learn underlying patterns\nabout text like syntax and semantics, but this knowledge is\nimperfect (Jawahar et al., 2019). However, works such as\n(Kuncoro et al., 2018; Linzen, Dupoux, and Goldberg, 2016)\nshow that NLP models that acquire even a weak understand-\ning of syntactic structure through training demonstrate im-\nproved performance relative to baseline. Hence, we hypoth-\nesize that explicit prior knowledge of syntactic information\ncan beneﬁt NLP models in a variety of tasks.\nTo aid the Transformer in more rapidly acquiring and uti-\nlizing syntactic information for better translation, we (i) em-\nploy a pretrained model1 to tag words in the source sequence\nwith their POS, (ii) identify the case of each word, and (iii)\nidentify the position of each subword relative to other sub-\nwords that are part of the same word (subword tagging). We\nthen append trainable syntax embedding vectors to the to-\nken embeddings, resulting in a combined representation of\nsyntactic and semantic elements.\nSpeciﬁcally, each word in the source sequence is ﬁrst as-\nsociated with its POS label according to syntactic structure.\nAfter breaking up words into their corresponding subwords\n(interchangeably denoted as tokens), we assign each sub-\nword the POS label of the word from which it originated. For\nexample, if the wordsunshine is broken up into subwords\nsun, sh, and ine, each subword would be assigned the\nPOS NOUN. The POS embeddings are then extracted from\na trainable embedding matrix using a look-up table, in a\nmanner similar to that of the subword embeddings (see Fig-\nure 1). The POS embeddings fP\nm of each subword (indexed\nby m) are then concatenated with the subword embeddings\nem ∈RD−d to create a combined embedding where dis the\ndimension of the feature embedding.\nIn a similar manner, we incorporate case and subword\nposition features. For case, we use a binary element zc\nm ∈\n{0,1}to look up a feature embedding fc\nm for each sub-\nword, depending on whether the original word is capital-\nized. For subword position, we use a categorical element\nzs\nm ∈ {B,M,E,O }to identify a feature embedding fs\nm\nfor each subword depending on whether the subword is at\nthe beginning (B), middle ( M), or end ( E) of the word; if\nthe subword comprises the full word, it is given a tag of O.\nThese are then added onto the POS embedding. Mathemati-\ncally, in the input stage, hm becomes:\n[e⊤\nm f⊤\nm]⊤= h′\nm ∈RD\nwhere fm = fP\nm ⊕fc\nm ⊕fs\nm ∈Rd is the learned embed-\nding for the syntactic features of subwordmin the sequence\n1https://spacy.io/\nFigure 2: The BERT BASE + POS model. Token embeddings\nare combined with trainable POS embeddings and fed into\nthe BERT encoder. The ﬁnal embedding of the [CLS] to-\nken is fed into a softmax classifer for downstream classiﬁ-\ncation tasks. The model is illustrated as taking in a pair of\nsequences but single sequence classiﬁcation is also possible.\nof M subwords and ⊕denotes either the concatenation or\nsummation operation.\nWe conjecture that our syntax-infused Transformer model\ncan boost translation performance by injecting grammatical\nrelationships, without having to learn them from examples.\nSyntax-infused BERT\nAdding syntactic features to the BERT model is a natural\nextension of the above modiﬁcation to the Transformer. As\nmentioned above, embeddings trained by BERT can be uti-\nlized for a variety of downstream tasks. We hypothesize that\ninfusing BERT with syntactic features is beneﬁcial in many\nof these tasks, especially those involving semantic structure.\nMany of the datasets on which we evaluate our modiﬁed\nBERT model are low-resource (as few as 2.5k sentences)\nrelative to those on which we evaluate the syntax-infused\nTransformer; hence, we choose to utilize only POS as a syn-\ntactic feature for BERT. We consider two approaches for\ncombining POS features with the pre-trained embeddings in\nBERT, a model we denote as BERTBASE + POS: (1) addition of\nthe trainable POS embedding vector of dimension d= Dto\nthe token embedding and (2) concatenation of the POS em-\nbedding with the token embedding. To make a fair compar-\nison with BERTBASE, the input dimension Dof the encoder\nmust match that of BERTBASE (D= 768). Thus, if option 2\nis used, the concatenated embedding must be passed through\na trainable afﬁne transformation with weight matrix of size\n(D+d)×D. While this option provides a more robust way\nto merge POS and word embeddings, it requires learning\na large matrix, which is problematic for downstream tasks\nwith very little training data. Hence, to facilitate training for\nthese tasks and to standardize the comparison across differ-\nent downstream tasks, we choose to use the ﬁrst approach.\nTherefore, for a given token, its input representation is con-\nstructed by summing the corresponding BERT token embed-\ndings with POS embeddings (see Figure 2).\nMathematically, the input tokens h′\nm ∈RD are given by\nh′\nm = em + fP\nm, where em is the BERT token embedding\nand fP\nm is the POS embedding for token m. For single se-\nquence tasks, m = 1,2,...,M , where M is the number\nof tokens in the sequence; while for paired sequence tasks,\nm = 1,2,...,M 1 + M2, where M1 and M2 are the num-\nber of tokens in each sequence. As is standard with BERT,\nfor downstream classiﬁcation tasks, the ﬁnal embedded rep-\nresentation ˆ yCLS of the ﬁrst token (denoted as [CLS]) is\npassed through a softmax classifer to generate a label.\nDatasets and Experimental Details\nFor translation, we consider WMT ’14 EN-DE dataset.\nThe WMT ’14 dataset consists of 4.5M training sentences.\nValidation is performed on newstest2013 (3000 sentences)\nand testing is on the newstest2014 dataset (2737 sentences,\n(Zhang, Titov, and Sennrich, 2019)). Parsers that infer syn-\ntax from EN sentences are typically trained on a greater\nnumber and variety of sentences and are therefore more ro-\nbust than parsers for other languages. Since one of the key\nfeatures of our models is to incorporate POS features into the\nsource sequence, we translate from EN to DE. While incor-\nporating all linguistic features described above is generally\nbeneﬁcial to NLP models, adding features may compromise\nthe model by restricting the number of dimensions allocated\nto word embeddings, which still the play the primary role.\nWe consider this tradeoff in greater detail below.\nMachine translation\nWe train both the baseline and syntax-infused Transformer\nfor 100,000 steps. All hyperparameter settings of the base-\nline Transformer, including embedding dimensions of the\nencoder and decoder, match those of (Vaswani et al., 2017).\nWe train the syntax-infused Transformer model using 512-\ndimensional embedding vectors. In the encoder, D = 492\ndimensions are allocated for word embeddings whiled= 20\nfor feature embeddings (chosen by hyperparameter tuning).\nIn the decoder, all 512 dimensions are used for word embed-\ndings (since we are interested only in decoding words, not\nword-POS pairs).\nThe model architecture consists of six encoder and six\ndecoder layers, with eight heads for multi-headed atten-\ntion. Parameters are initialized with Glorot (Glorot and Ben-\ngio, 2010). We use a dropout rate of 0.1 and batch size of\n4096. We utilize the Adam optimizer to train the model with\nβ1 = 0.9 and β2 = 0.998; gradients are accumulated for\ntwo batches before updating parameters. A label-smoothing\nfactor of 0.1 is employed.\nThe context and size of the EN-DE translation dataset is\nquite different compared that of the datasets on which POS\ntagging methods are typically trained, implying that the POS\ntagging model may not generalize well. Hence, we include\nnot only POS but also case and subword tag features. The\ntraining procedure is identical to that of (Vaswani et al.,\n2017) except that, for the syntax-infused Transformer, the\ndimension d of features fm is chosen to be 20 by doing a\ngrid search over the range of 8 to 64.\nData Number of Baseline Syntax-infused\nFraction Sentences Transformer Transformer\n1% 45k 1.10 1.67\n5% 225k 8.51 10.50\n10% 450k 16.28 17.28\n25% 1.1M 22.72 23.24\n50% 2.25M 25.41 25.74\n100% 4.5M 28.94 29.64\nTable 1: BLEU scores for different proportions of the data\nfor baseline Transformer vs syntax-infused Transformer for\nthe EN-DE task on newstest2014.\nNatural language understanding\nThe General Language Understanding Evaluation (GLUE)\nbenchmark (Wang et al., 2018) is a collection of differ-\nent natural language understanding tasks evaluated on eight\ndatasets: Multi-Genre Natural Language Inference (MNLI),\nQuora Question Pairs (QQP), Question Natural Language\nInference (QNLI), Stanford Sentiment Treebank (SST-2),\nThe Corpus of Linguistic Acceptability (CoLA), The Se-\nmantic Textual Similarity Benchmark (STS-B), Microsoft\nResearch Paraphrase Corpus (MRPC), and Recognizing\nTextual Entailment (RTE). For a summary of these datasets,\nsee (Devlin et al., 2018). We use POS as the syntactic feature\nfor BERT for these tasks. Aside from the learning rate, we\nuse identical hyperparameter settings to ﬁne-tune both the\nBERTBASE and BERTBASE + POS models for each task. This\nincludes a batch size of 32 and 3 epochs of training for all\ntasks. For each model, we also choose a task-speciﬁc learn-\ning rate among the values {5,4,3,2}×10−5, which is stan-\ndard for BERTBASE.\nExperimental Results\nMachine translation\nWe evaluate the impact of infusing syntax into the baseline\nTransformer for the EN-DE translation task. We add three\nfeatures namely POS, subword tags, and case to aid Trans-\nformer model learn underlying patterns about the sentences.\nWith more than one feature, there are multiple ways to\nincorporate feature embeddings into the word embeddings.\nFor a fair comparison to the Transformer baseline, we use\na total of 512 dimensions for representing both the word\nembeddings as well as feature embeddings. One important\ntradeoff is that as the dimensionality of the syntax informa-\ntion increases, the dimensionality for actual word embed-\ndings decreases. Since POS, case, and subword tags have\nonly a limited number of values they can take, dedicating a\nhigh dimensionality for each feature proves detrimental (ex-\nperimentally found). We ﬁnd that the total feature dimension\nfor which the gain in BLEU score is maximized is 20 (found\nthrough grid search). This means that (1) each feature em-\nbedding dimension can be allocated to 20 and summed to-\ngether or (2) the feature embeddings can be concatenated to\neach other such that their total dimensionality is 20. There-\nfore, in order to efﬁciently learn the feature embeddings\nSystem MNLI QQP QNLI SST-2 CoLA STS-B MRPC RTE Average\n392k 363k 108k 67k 8.5k 5.7k 3.5k 2.5k -\nPre-OpenAI SOTA 80.6/80.1 66.1 82.3 93.2 35.0 81.0 86.0 61.7 74.0\nBiLSTM+ELMo+Attn 76.4/76.1 64.8 79.8 90.4 36.0 73.3 84.9 56.8 71.0\nOpenAI GPT 82.1/81.4 70.3 87.4 91.3 45.4 80.0 82.3 56.0 75.1\nBERTBASE 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6\nBERTBASE + POS 84.4/83.3 71.4 90.4 93.9 52.9 85.5 88.8 66.9 79.7\nTable 2: GLUE test results scored using the GLUE evaluation server. The number below each task denotes the number of\ntraining examples. The scores in bold denote the tasks for which BERTBASE + POS outperforms BERTBASE.\nInstead\n,\nB\nwell\ne\nspent\nyears\nescort\ning\nhis\nfather\nto\nover\ncro\nw\nded\nclinic\ns\nand\nhospital\ns\n,\ngetting\nwhatever\ntreatment\nthey\ncould\nget\n.\nStattdessen\nverbracht\ne\nB\nwell\ne\nJahre\ndamit\n,\nseinen\nVater\nzu\nüber\nfüll\nten\nKlinik\nen\nund\nKrankenhäuser\nn\nzu\nbegleiten\n,\num\njede\nBehandlung\nzu\nerhalten\n.\n</s>\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n(a) Baseline (EN-DE)\nInstead\n,\nB\nwell\ne\nspent\nyears\nescort\ning\nhis\nfather\nto\nover\ncro\nw\nded\nclinic\ns\nand\nhospital\ns\n,\ngetting\nwhatever\ntreatment\nthey\ncould\nget\n.\nStattdessen\nverbracht\ne\nB\nwell\ne\nJahre\ndamit\n,\nseinen\nVater\nzu\nüber\nfüll\nten\nKlinik\nen\nund\nKrankenhäuser\nn\nzu\nbegleiten\n,\num\njede\nBehandlung\nzu\nbekommen\n,\ndie\nsie\nbekommen\nkonnten\n.\n</s>\n0.1\n0.2\n0.3\n0.4 (b) Syntax-infused (EN-DE)\nFigure 3: Comparison of attention for example sentences\ntranslated by baseline and POS Transformer models (ob-\ntained from the last layer). Rows depict the attention score\nfor a given target subword to each of the subwords in the\nsource sequence. In syntax-infused models for EN-DE trans-\nlation, we ﬁnd that attention is more widely distributed\nacross subwords. For instance, the subword “Vater” (the\nGerman word for “father”) attends mostly to the nearby sub-\nwords “his” and “father” in the base model while “Vater”\nalso attends to the more distant words “Bwelle” (a person)\nand “escorting” in the syntax-infused model. This suggests\nthat the syntax-infused model is able to better connect dis-\nparate parts of a sentence to aid translation. Note that the\nnumber of rows in the baseline and syntax-infused Trans-\nformer are different because each produces different predic-\ntions.\nwhile also not sacriﬁcing the word embedding dimension-\nality, we ﬁnd that summing the embeddings for all three dif-\nferent features of d = 20and concatenating the sum to the\nword embeddings of D = 492gives the maximum perfor-\nmance on translation. We also ﬁnd that incorporation of a\ncombination of two features among {POS, case, subword\ntags}does not perform as well as having all the three fea-\ntures.\nIn Table 1, we vary the proportion of data used for\ntraining and observe the performance of both the baseline\nand syntax-infused Transformer. The syntax-infused model\nmarkedly outperforms the baseline model, offering an im-\nprovement of 0.57, 1.99, 1, 0.52, 0.33, and 0.7 points, re-\nspectively, for 1, 5, 10, 25, 50, and 100% of the data. It is\nnotable that the syntax-infused model translates the best rel-\native to the baseline when only a fraction of the dataset is\nused for training. Speciﬁcally, the maximum improvement\nis 1.99 BLEU points when only 10% of the training data\nis used. This shows that explicit syntax information is most\nhelpful under limited training data conditions. As shown in\nFigure 3(a)-(b), the syntax-infused model is better able to\ncapture connections between tokens that are far apart yet se-\nmantically related, resulting in improved translation perfor-\nmance. In addition, Table 3 shows a set of sample German\npredictions made by the baseline and syntax-infused Trans-\nformer.\nNatural language understanding\nThe results obtained for the BERT BASE + POS model on\nthe GLUE benchmark test set are presented in Table 2.\nBERTBASE + POS outperforms BERTBASE on 4 out of the 8\ntasks. The improvements range from marginal to signiﬁ-\ncant, with a maximum improvement of 0.8 points of the\nPOS model over BERT BASE on CoLA. Fittingly, CoLA is\na task which assesses the linguistic structure of a sentence,\nwhich is explictly informed by POS embeddings. Moreover,\nBERTBASE + POS outperforms BERT BASE on tasks that are\nconcerned with evaluating semantic relatedness. For exam-\nples of predictions made on the RTE dataset, see Table 4.\nRelated Works\nPrevious work has sought to improve the self-attention mod-\nule to aid NLP models. For instance, (Yang et al., 2018)\nintroduced a Gaussian bias to model locality, to enhance\nmodel ability to capture local context while also maintaining\nthe long-range dependency. Instead of absolute positional\nembeddings, (Shaw, Uszkoreit, and Vaswani, 2018) exper-\nimented with relative positional embeddings or distance be-\ntween sequences and found that it led to a drastic improve-\nment in performance.\nAdding linguistic structure to models like the Transformer\ncan be thought of as a way of improving the attention mech-\nanism. The POS and subword tags act as a form of rela-\ntive positional embedding by enforcing the sentence struc-\nture. (Li et al., 2018) encourages different attention heads\nto learn about different information like position and rep-\nresentation by introducing a disagreement regularization. In\norder to model the local dependency between words more\nefﬁciently, (Im and Cho, 2017) introduced distance between\nwords and incorporated that into the self-attention.\nReference Baseline Transformer Syntax-infused Transformer\nParken in Frankfurt k ¨onnte bald\nempﬁndlich teurer werden .\nDas Personal war sehr freundlich\nund hilfsbereit .\nParken in Frankfurt k ¨onnte bald\nsp¨urbar teurer sein .\nDie zur ¨uckgerufenen Modelle wur-\nden zwischen dem 1. August und 10.\nSeptember hergestellt .\nZwischen August 1 und September\n10.\nDie zur ¨uckgerufenen Modelle wur-\nden zwischen dem 1. August und 10.\nSeptember gebaut\nStattdessen verbrachte Bwelle Jahre\ndamit , seinen Vater in ¨uberf¨ullte\nKliniken und Hospit¨aler zu begleiten\n, um dort die Behandlung zu bekom-\nmen , die sie zu bieten hatten .\nStattdessen verbrachte Bwelle Jahre\ndamit , seinen Vater mit ¨uber f¨ullten\nKliniken und Krankenh¨qusern zu be-\nherbergen .\nStattdessen verbrachte Bwelle Jahre\ndamit , seinen Vater zu ¨uberf¨ullten\nKliniken und Krankenh¨ausern zu be-\ngleiten , um jede Behandlung zu\nbekommen , die sie bekommen kon-\nnten .\nPatek kann gegen sein Urteil noch\nBerufung ein legen .\nPatek kann noch seinen Satz an rufen\n.\nPatek mag sein Urteil noch Berufung\nein legen .\nTable 3: Translation examples of baseline Transformer vs. syntax-infused Transformer on the EN-DE dataset. The text high-\nlighted in blue represents words correctly predicted by the syntax-infused model but not by the baseline Transformer.\nSentence 1 Sentence 2 True label\nThe Qin (from which the name China is derived) established\nthe approximate boundaries and basic administrative system\nthat all subsequent dynasties were to follow .\nQin Shi Huang was the ﬁrst Chi-\nnese Emperor .\nNot entailment\nIn Nigeria, by far the most populous country in sub-Saharan\nAfrica, over 2.7 million people are infected with HIV .\n2.7 percent of the people infected\nwith HIV live in Africa .\nNot entailment\nTable 4: Examples of randomly chosen sentences from the RTE dataset (for evaluation of entailment between pairs of sentences)\nthat were misclassiﬁed by BERTBASE and correctly classiﬁed by BERTBASE + POS.\nPrevious literature also has sought to incorporate syntax\ninto deep learning NLP models. (Bastings et al., 2017) used\nsyntax dependency tree information on a bidirectional RNN\non translation systems by modeling the trees using Graph\nConvolutional Networks (GCNs) (Kipf and Welling, 2016).\nModeling source label syntax information has helped signif-\nicantly in the Chinese-English translation (Li et al., 2017)\nby linearizing parse trees to obtain drastic performance im-\nprovements. Adding a syntax-based distance constraint on\nthe attention module, to generate a more semantic context\nvector, has proven to work for translation systems in the\nChinese-English as well as English-German tasks.\nThese works afﬁrm that adding syntax information can\nhelp the NLP models to translate better from one language\nto another and also achieve better performance measures.\nConclusions\nWe have augmented the Transformer network with syn-\ntax information for machine translation. The syntax-infused\nTransformer improvements were highest when a subset of\nthe training data is used. We then distinguish the syntax-\ninfused and baseline Transformer models by providing an\ninterpretation of attention visualization. Additionally, we\nﬁnd that the syntax-infused BERT model performs better\nthan baseline on a number of GLUE downstream tasks.\nIt is an open question whether the efﬁciency of these so-\nphisticated models can further be improved by creating an\narchitecture that is enabled to model the language structure\nmore inherently than using end to end models. Future work\nmay extend toward this direction.\nReferences\nBastings, J.; Titov, I.; Aziz, W.; Marcheggiani, D.; and\nSima’an, K. 2017. Graph convolutional encoders for\nsyntax-aware neural machine translation. arXiv preprint\narXiv:1704.04675.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K.\n2018. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. arXiv preprint\narXiv:1810.04805.\nGlorot, X., and Bengio, Y . 2010. Understanding the dif-\nﬁculty of training deep feedforward neural networks. In\nProceedings of the thirteenth international conference on\nartiﬁcial intelligence and statistics, 249–256.\nIm, J., and Cho, S. 2017. Distance-based self-attention\nnetwork for natural language inference. arXiv preprint\narXiv:1712.02047.\nJawahar, G.; Sagot, B.; Seddah, D.; Unicomb, S.; I ˜niguez,\nG.; Karsai, M.; L ´eo, Y .; Karsai, M.; Sarraute, C.; Fleury,\n´E.; et al. 2019. What does bert learn about the structure\nof language? In 57th Annual Meeting of the Association\nfor Computational Linguistics (ACL), Florence, Italy.\nKipf, T. N., and Welling, M. 2016. Semi-supervised classiﬁ-\ncation with graph convolutional networks. arXiv preprint\narXiv:1609.02907.\nKuncoro, A.; Dyer, C.; Hale, J.; Yogatama, D.; Clark, S.; and\nBlunsom, P. 2018. Lstms can learn syntax-sensitive de-\npendencies well, but modeling structure makes them bet-\nter. In Proceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1: Long\nPapers), 1426–1436.\nLi, J.; Xiong, D.; Tu, Z.; Zhu, M.; Zhang, M.; and Zhou,\nG. 2017. Modeling source syntax for neural machine\ntranslation. arXiv preprint arXiv:1705.01020.\nLi, J.; Tu, Z.; Yang, B.; Lyu, M. R.; and Zhang, T. 2018.\nMulti-head attention with disagreement regularization.\narXiv preprint arXiv:1810.10183.\nLinzen, T.; Dupoux, E.; and Goldberg, Y . 2016. Assessing\nthe ability of lstms to learn syntax-sensitive dependencies.\nTransactions of the Association for Computational Lin-\nguistics 4:521–535.\nLuong, M.-T.; Pham, H.; and Manning, C. D. 2015. Effec-\ntive approaches to attention-based neural machine trans-\nlation. arXiv preprint arXiv:1508.04025.\nNiehues, J., and Cho, E. 2017. Exploiting linguistic re-\nsources for neural machine translation using multi-task\nlearning. arXiv preprint arXiv:1708.00993.\nSennrich, R., and Haddow, B. 2016. Linguistic input fea-\ntures improve neural machine translation. arXiv preprint\narXiv:1606.02892.\nSennrich, R.; Haddow, B.; and Birch, A. 2015. Neural ma-\nchine translation of rare words with subword units. arXiv\npreprint arXiv:1508.07909.\nShaw, P.; Uszkoreit, J.; and Vaswani, A. 2018. Self-attention\nwith relative position representations. arXiv preprint\narXiv:1803.02155.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998–6008.\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and\nBowman, S. R. 2018. Glue: A multi-task benchmark\nand analysis platform for natural language understanding.\narXiv preprint arXiv:1804.07461.\nYang, B.; Tu, Z.; Wong, D. F.; Meng, F.; Chao, L. S.; and\nZhang, T. 2018. Modeling localness for self-attention\nnetworks. arXiv preprint arXiv:1810.10182.\nZhang, B.; Titov, I.; and Sennrich, R. 2019. Improving deep\ntransformer with depth-scaled initialization and merged\nattention. arXiv preprint arXiv:1908.11365.",
  "topic": "Machine translation",
  "concepts": [
    {
      "name": "Machine translation",
      "score": 0.678180456161499
    },
    {
      "name": "Computer science",
      "score": 0.6769678592681885
    },
    {
      "name": "Transformer",
      "score": 0.6545506119728088
    },
    {
      "name": "Syntax",
      "score": 0.6399353742599487
    },
    {
      "name": "Natural language processing",
      "score": 0.4534703195095062
    },
    {
      "name": "Programming language",
      "score": 0.4252777099609375
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38987427949905396
    },
    {
      "name": "Linguistics",
      "score": 0.33033132553100586
    },
    {
      "name": "Engineering",
      "score": 0.11638793349266052
    },
    {
      "name": "Philosophy",
      "score": 0.09210661053657532
    },
    {
      "name": "Electrical engineering",
      "score": 0.04930415749549866
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": []
}