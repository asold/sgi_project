{
    "title": "SINA-BERT: A pre-trained Language Model for Analysis of Medical Texts in Persian",
    "url": "https://openalex.org/W3152494620",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4288964218",
            "name": "Taghizadeh, Nasrin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4288541553",
            "name": "Doostmohammadi, Ehsan",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Seifossadat, Elham",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222023159",
            "name": "Rabiee, Hamid R.",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Tahaei, Maedeh S.",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3017604292",
        "https://openalex.org/W2911866413",
        "https://openalex.org/W1992034625",
        "https://openalex.org/W2412542244",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2493916176",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2087227067",
        "https://openalex.org/W2415691382",
        "https://openalex.org/W2805133648",
        "https://openalex.org/W3030156796",
        "https://openalex.org/W2973154071",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2944400536",
        "https://openalex.org/W3204526376",
        "https://openalex.org/W3035390927"
    ],
    "abstract": "We have released Sina-BERT, a language model pre-trained on BERT (Devlin et al., 2018) to address the lack of a high-quality Persian language model in the medical domain. SINA-BERT utilizes pre-training on a large-scale corpus of medical contents including formal and informal texts collected from a variety of online resources in order to improve the performance on health-care related tasks. We employ SINA-BERT to complete following representative tasks: categorization of medical questions, medical sentiment analysis, and medical question retrieval. For each task, we have developed Persian annotated data sets for training and evaluation and learnt a representation for the data of each task especially complex and long medical questions. With the same architecture being used across tasks, SINA-BERT outperforms BERT-based models that were previously made available in the Persian language.",
    "full_text": null
}