{
  "title": "Persistence Initialization: a novel adaptation of the Transformer architecture for time series forecasting",
  "url": "https://openalex.org/W4386253606",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5031392374",
      "name": "Espen Haugsdal",
      "affiliations": [
        "Norwegian University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2105657449",
      "name": "Erlend Aune",
      "affiliations": [
        "BI Norwegian Business School",
        "Norwegian University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2096481777",
      "name": "Massimiliano Ruocco",
      "affiliations": [
        "Norwegian University of Science and Technology",
        "SINTEF",
        "SINTEF Digital"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3177828909",
    "https://openalex.org/W3177318507",
    "https://openalex.org/W2100495367",
    "https://openalex.org/W6828894009",
    "https://openalex.org/W6600421821",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W2963507686",
    "https://openalex.org/W2945680505",
    "https://openalex.org/W2962752580",
    "https://openalex.org/W2613328025",
    "https://openalex.org/W2963917928",
    "https://openalex.org/W2016210396",
    "https://openalex.org/W2048665112",
    "https://openalex.org/W2808800115"
  ],
  "abstract": "Abstract Time series forecasting is an important problem, with many real world applications. Transformer models have been successfully applied to natural language processing tasks, but have received relatively little attention for time series forecasting. Motivated by the differences between classification tasks and forecasting, we propose PI-Transformer , an adaptation of the Transformer architecture designed for time series forecasting, consisting of three parts: First, we propose a novel initialization method called Persistence Initialization , with the goal of increasing training stability of forecasting models by ensuring that the initial outputs of an untrained model are identical to the outputs of a simple baseline model. Second, we use ReZero normalization instead of Layer Normalization, in order to further tackle issues related to training stability. Third, we use Rotary positional encodings to provide a better inductive bias for forecasting. Multiple ablation studies show that the PI-Transformer is more accurate, learns faster, and scales better than regular Transformer models. Finally, PI-Transformer achieves competitive performance on the challenging M4 dataset, both when compared to the current state of the art, and to recently proposed Transformer models for time series forecasting.",
  "full_text": "Applied Intelligence (2023) 53:26781–26796\nhttps://doi.org/10.1007/s10489-023-04927-4\nPersistence Initialization: a novel adaptation of the Transformer\narchitecture for time series forecasting\nEspen Haugsdal 1 · Erlend Aune 1,2 · Massimiliano Ruocco 1,3\nAccepted: 29 July 2023 / Published online: 29 August 2023\n© The Author(s) 2023\nAbstract\nTime series forecasting is an important problem, with many real world applications. Transformer models have been successfully\napplied to natural language processing tasks, but have received relatively little attention for time series forecasting. Motivated\nby the differences between classiﬁcation tasks and forecasting, we propose PI-Transformer, an adaptation of the Transformer\narchitecture designed for time series forecasting, consisting of three parts: First, we propose a novel initialization method called\nPersistence Initialization, with the goal of increasing training stability of forecasting models by ensuring that the initial outputs\nof an untrained model are identical to the outputs of a simple baseline model. Second, we use ReZero normalization instead of\nLayer Normalization, in order to further tackle issues related to training stability. Third, we use Rotary positional encodings\nto provide a better inductive bias for forecasting. Multiple ablation studies show that the PI-Transformer is more accurate,\nlearns faster, and scales better than regular Transformer models. Finally, PI-Transformer achieves competitive performance\non the challenging M4 dataset, both when compared to the current state of the art, and to recently proposed Transformer\nmodels for time series forecasting.\nKeywords Transformer · Time series forecasting · M4 competition · Deep neural networks\n1 Introduction\nThe ability to forecast the future is a valuable tool across\na wide range of applications, such as ﬁnance, energy, and\nindustry. Forecasting allows for better decision-making in\nthe present, and even small improvements in accuracy can\noften provide great beneﬁts.\nThe Transformer [ 1] has recently become the domi-\nnant method for most Natural Language Processing (NLP)\ntasks [2, 3]. It has also been successfully applied to a diverse\nset of challenging problems outside of NLP , such as protein\nB Espen Haugsdal\nespen.haugsdal@ntnu.no\nErlend Aune\nerlend.aune@ntnu.no\nMassimiliano Ruocco\nmassimiliano.ruocco@ntnu.no\n1 Norwegian University of Science and Technology,\nTrondheim, Norway\n2 BI Norwegian Business School, Oslo, Norway\n3 Sintef Digital, Trondheim, Norway\nfolding [ 4] and Reinforcement Learning [ 5]. However, rela-\ntively little attention has been given to the use of Transformers\nfor time series forecasting. Most prior work in this direction\nhas focused on the addressing the computational limitations\nof the Transformer, by proposing computationally efﬁcient\nalternatives to regular attention [ 6–8]. In contrast, this work\nis primarily focused on improving the forecasting accuracy\nof the Transformer on time series with shorter forecasting\nhorizons, by addressing differences between time series data\nand text data.\nTime series forecasting and natural language modeling\nmight at ﬁrst glance appear to be highly similar; they both\nform ordered sequences, and forecasting the next step of a\ntime series can be seen as analogous to predicting the next\ntoken in a language modeling task.\nHowever, there are also important differences between\ntime series data and text data. First, time series forecasting is\na continuous regression problem, while language modeling\nis a discrete classiﬁcation problem. Consequently, in order\nto use the Transformer to forecast, the ﬁnal softmax activa-\ntion layer must be removed. We argue that this makes the\nmodel more sensitive to how the weights are initialized, as\nthe initial forecasts will now be proportional to the weights\n123\n26782 E. Haugsdal et al.\nof the ﬁnal linear layer. Conversely, models including a ﬁnal\nsoftmax layer are likely to be more robust to weight ini-\ntialization. A random initialization of such a model likely\nhas an approximately uniform output distribution, which is\narguably a good starting point for a reasonably balanced clas-\nsiﬁcation task. Second, time series data often do not have any\nparticular semantics associated with the beginning or con-\nclusion of a sequence. In other words, for time series data,\nthere is in general no reason to assume any meaning from\nthe fact that the sequence started or ended at some particular\npoint in time. As a consequence, time series sequences can\noften be subdivided into smaller sub-sequences, a technique\nwhich is commonly referred to as windowing. In contrast, for\ntext sequences, both the start and the end of a sequence has\nsemantic meaning, because the start and end of the sequence\nsigniﬁes the bounds of a connected body of text.\nWe propose three modiﬁcations of the Transformer archi-\ntecture, directly motivated by these differences. First, we\npropose an adaptation called Persistence Initialization (PI),\nwhich aims to improve the Transformer’s ability to forecast.\nIt has long been known that initialization is an important com-\nponent in the process of training deep neural networks [ 9–11].\nPersistence Initialization works by implicitly initializing the\nmodel in such a way that the initial forecasts (before training)\nbecome equal to the forecasts of a persistence model. The per-\nsistence model, also known as a random walk method [ 12],\nis deﬁned by letting the forecast ˆx\nt +1 be equal to the previous\nvalue xt . In order to implement PI, we add two components:\na residual skip connection, and a scalar multiplicative gating\nparameter γ. The residual skip connection has the effect of\nadding the value at time t (i.e. x\nt ) to the forecast value for\ntime t +1 (i.e. ˆxt +1). The scalar multiplicative gating param-\neter γ is initialized to 0, and is multiplied with the outputs\nof the Transformer. As a consequence of this combination,\nonly the skip connection contributes to the initial forecasts,\nwhich means that any complex model can be effectively ini-\ntialized as a persistence model, regardless of the values of\nthe randomly initialized parameters within the model.\nOur second proposed adaption attempts to further improve\ntraining stability by replacing the commonly used Layer\nNormalization [ 13] layer with ReZero normalization [ 14].\nReZero is a technique designed to improve the training sta-\nbility of deep networks, and was proposed as an alternative\nto normalization layers such as Layer Norm and Batch norm.\nNote that while the implementation of Persistence Initializa-\ntion is almost identical to that of ReZero, these techniques\nare intended to solve different problems. The goal of ReZero\nnormalization is to control the magnitude of gradients in deep\nnetworks, while The goal of Persistence Initialization is to\nimprove the Transformer’s forecasting accuracy by providing\nan inductive bias towards models with a signiﬁcant autore-\ngressive component.\nOur third proposed adaptation is related to the difference\nin the semantics of the time series sequences, compared to\nnatural language sequences. Instead of using the absolute\nsinusoidal encoding [1], we propose to use the relative Rotary\nEncoding [ 15], which has been shown to outperform the\nsinusoidal encoding in some NLP tasks [ 15]. In the context\nof time series, we argue that a relative positional encod-\ning provides a better inductive bias for forecasting. Time\nseries sequences are often “windowed”, which means that\nthe absolute position within the window has no semantic sig-\nniﬁcance. Consequently, absolute positional encodings are\nill-suited for forecasting, as they put undue emphasis on\nan arbitrary location in the sequence. In contrast, a rela-\ntive encoding emphasizes the position of the outputs, i.e. the\nforecasts, which should result in a better inductive bias for\nforecasting.\nIn summary, our contributions are:\n1. We propose Persistence Initialization, a novel and general\nadaptation autoregressive for time series forecasting with\nneural networks. This adaptation initializes the model\nsuch that it starts off as a persistence model, which pro-\nvides a good starting point for further learning.\n2. We propose the PI-Transformer architecture, a Trans-\nformer architecture with three main modiﬁcations: Per-\nsistence Initialization, ReZero normalization, and Rotary\npositional encodings. We perform two ablation studies\nto verify the importance of each modiﬁcation. The ﬁrst\nablation study compares the effects of the components of\nPersistence Initialization, and the second compares the\neffect of positional encoding and normalization layers.\nBoth studies show that the proposed modiﬁcations are\nnecessary for good forecasting performance.\n3. We evaluate PI-Transformer on the challenging M4 fore-\ncasting dataset, and show that PI-Transformer achieves\ncompetitive accuracy, outperforming the winner of the\noriginal M4 competition. Furthermore, PI-Transformer is\nhighly accurate without the need for a large ensemble of\nmodels, in contrast to other state-of-the-art methods on\nthe M4 dataset. To the best of our knowledge, this is the\nﬁrst time a Transformer model has been successfully used\nto forecast the complete M4 dataset to a high degree of\naccuracy. We also compare PI-Transformer with recent\nexisting Transformer architectures for time series fore-\ncasting, and show that PI-Transformer outperforms these\nby a large margin on the M4-Hourly dataset.\nIn order to ensure reproducibility, all the code related to\nour work is publicly available\n1. The rest of the paper is orga-\n1 A public implementation is available at https://github.com/EspenHa/\npersistence_initialization.\n123\nPersistence Initialization: a novel adaptation... 26783\nnized as follows: Section 2 provides some background on the\nTransformer, Section 4 describes our proposed adaptation,\nSection 3 reviews existing related work, Section 5 describes\nthe experiments, and Section 6 provides analysis and dis-\ncussion of the results. Finally, Section 7 concludes with a\nsummary.\n2 Background\n2.1 Decoder-only transformer\nA decoder-only Transformer [ 1] consists of blocks of causal\nself-attention layers and feedforward layers, both followed\nby a residual skip connection and Layer-Normalization [ 13].\nThe model can be deﬁned recursively by letting Xi be the\noutput of the i th block, as follows:\nXi (Xi −1) = FFi (SAi (Xi −1)) (1)\nSAi (X ) = LayerNorm(X + SelfAttentioni (X )) (2)\nFFi (X ) = LayerNorm(X + FeedForwardi (X )), (3)\nwhere Xi is a matrix of shape L ×dmodel, with L representing\nthe “sequence” or “time” dimension and dmodel representing\nthe feature dimension. X0 is the base case of the recursion,\nand represents the initial input to the model. The number\nof blocks N is a hyperparameter which determines the ﬁnal\noutput of the model, X\nN .\nIn order to deﬁne self-attention, we must ﬁrst deﬁne\nmulti-head attention. Multi-head attention combines multiple\nattention heads, by giving each head a separate set of learn-\nable weights, ensuring that each head can perform a different\noperation. Self-attention is then deﬁned as a special case of\nmulti-head attention where the keys, queries, and values are\nall equal:\nSelfAttention(x ) = MHA(X , X , X ) (4)\nMHA(Q, K , V ) = Concat(head\n1, ...,headh )WO (5)\nhead j = Attention\n(\nQW (j )\nQ , KW (j )\nK , VW (j )\nV\n)\n(6)\nAttention(Q, K , V ) = softmax\n( QK T\n√dhead\n+ M\n)\nV , (7)\nwhere h is the number of attention heads, and dhead =\ndmodel/h. The learnable weight matrices W (j )\nQ , W (j )\nK , and\nW (j )\nV are of shape dmodel × dhead. WO is a learnable weight\nmatrix of shape dmodel ×dmodel, and has the effect of mixing\nthe outputs of each head. M is an upper triangular mask-\ning matrix, which ensures that the model does not attend to\n“future” time steps.\nThe feed-forward layer is performed point-wise, i.e. it only\nconsiders information in the current time step, like a 1-D\nconvolution. It is deﬁned as two afﬁne transformations with\na ReLU non-linearity in between:\nFeedForward(X ) = ReLU(XW 1 + b1)W2 + b2, (8)\nwhere W1 and W2 are learnable weight matrices of shape\ndmodel × dff and dff × dmodel, and b1 and b2 are learnable\nbias vectors of shape dff and dmodel.\n2.2 Positional encoding\nThe Transformer cannot distinguish elements of a sequence\nbased on their ordering, because the attention operation is\npermutation invariant. Consequently, it is necessary to pro-\nvide explicit positional information to the model, which is\nthe purpose of the positional encoding.\n2.2.1 Absolute positional encoding\nThe sinusoidal positional encoding is one of the most com-\nmonly used absolute positional encodings. The encoding is\napplied by adding it to the inputs of the model, and can be\nimplemented by creating a matrix E of size L ×d\nmodel, where\nL is the sequence length. Each row contains dmodel/2 pairs\nof sine and cosine functions with varying wavelengths, with\neach pair sharing the same wavelength. The wavelength is\nincreased geometrically for each pair, which can be written\nas follows:\nE\ni ,2 j = sin\n( i\nK j /dmodel\n)\n(9)\nEi ,2 j +1 = cos\n( i\nK j /dmodel\n)\n(10)\nwhere i ∈[ 1, L] is the position in the sequence, and j ∈\n[1,dmodel/2] is the index of the feature dimension. The value\nof K determines what the largest wavelength will be, and is\ncommonly set to 10000.\n2.2.2 Rotary encoding\nThe Rotary Encoding [ 15] is a relative positional encod-\ning, which was introduced as an alternative to the absolute\npositional encoding and other previously proposed relative\npositional encodings. It encodes relative positional informa-\ntion in the angle between the key and query vectors. This\nis different to most other positional encodings, which are\ntypically additive. The encoding function f is derived by\nconsidering a relation between q\nm , a query vector at position\nm, and kn , a key vector at position n. The dot product of\nthe encoded vectors should be equal to the output of some\nfunction g, which only depends on the original vectors and\n123\n26784 E. Haugsdal et al.\nthe relative distance between their positions:\nf (qm ,m) · f (kn ,n) = g(qm , kn , m − n) (11)\nWe will consider only case of 2D vectors q and k.( T h e\ngeneral case of an arbitrarily sized vector is more cumber-\nsome to state, but is a straightforward generalization of the\n2D case.) The desired relation can be achieved by the fol-\nlowing encoding:\nf (x,k) = xe\nik θ , (12)\nwhere the 2D vector x is considered as a number in the com-\nplex plane, and θ is a real non-zero constant. This encoding\nsatisﬁes the desired relation with the following function:\ng(qm ,kn ,m − n) = Re[qm k∗\nn ei (m−n)θ ], (13)\nwhere Re [·] is the real part of the complex number and k∗\nn is\nthe complex conjugate of kn .\n2.3 Normalization\nTraining deep neural networks can be a challenging, due to\nissues such as the vanishing gradient problem. Normalization\nlayers [ 13, 16] have been proposed to speed up the training\nprocess, and to make training more robust to random weight\ninitialization. The Transformer architecture also includes\nnormalization layers, speciﬁcally Layer Normalization [ 13].\nThere are mainly two alternatives for the location of the nor-\nmalization layer within the architecture; the ﬁrst is post-layer\nnormalization, and the second is pre-layer normalization.\nThe original Transformer [ 1] used post-layer normalization,\nhowever pre-layer normalization has been found by some to\nlead to more effective training [ 17]. The two orderings can\nbe formalized as follows. Let Sublayer (·) refer to either the\nmulti-head attention or the feedforward layers of the Trans-\nformer. Then we have:\nPostLN(x ) = LayerNorm(x + Sublayer(x )) (14)\nPreLN(x ) = x + Sublayer(LayerNorm(x )) (15)\nReZero [ 14] is an alternative to Layer Normalization for\ntraining deep networks. Instead of calculating statistics and\nusing these to normalize the data, it simply uses a multiplica-\ntive gating parameter α, which is initially set to 0:\nReZero(x ) = x + α · Sublayer(x ) (16)\nThe same α parameter is used within a single Transformer\nblock, both for the multi-head attention layer and for the\nfeedforward layer.\n3 Related work\n3.1 Transformers for time series forecasting\nFollowing the introduction of the Transformer [ 1] in 2017,\nresearchers also started to use Transformers for time series\ntasks. However, the topic of using Transformers for time\nseries tasks have received relatively little research attention,\ncompared to the use of Transformers for other kinds of data.\nThe main focus of the work on Transformers for time\nseries has been on the quadratic computational complex-\nity of the attention operation. In the context of NLP , there\nare numerous works attempting to address this issue, see\nfor instance the recent survey by Tay et al. [ 18]. We will\nnot attempt to summarize this line of work here, but instead\nfocus on works that speciﬁcally target time series forecasting\nproblems.\nThe ﬁrst work in the direction of efﬁcient Transformers for\ntime series was by Li et al. [ 6], who introduced the LogSparse\nTransformer. The architecture improves the efﬁciency of the\nattention operation by removing queries that are far away\nfrom the current time step, by exponentially increasing the\nspace between consecutive queries, resulting in a complexity\nof O(N log N ) instead of O(N\n2). Moreover, a causal con-\nvolution layer was added before the attention layer, to allow\nthe model to easily discover similarities between ranges of\ntime series points.\nThe Informer [ 7] is a Transformer designed for the task of\nLong Sequence Time-Series Forecasting, which was deﬁned\nby the authors as forecasting horizons of size 48 or longer.\nThe authors propose an efﬁcient attention operation, which\nﬁrst approximates the query-key similarity and then selects\nthe most important queries, resulting in O(N log N ) com-\nputational complexity. The model uses an encoder-decoder\narchitecture that produces forecasts for the entire horizon\nin a single evaluation. Compared to the LogSparse Trans-\nformer, this leads to improved speed when forecasting long\nhorizons, as the model does not need to iteratively generate\nforecasts.\nThe Autoformer [ 8] is another Transformer designed for\nLong Sequence Time-Series Forecasting, and similarly to the\nInformer, it also has an encoder-decoder architecture which\nforecasts the entire horizon in a single step. It proposes a\ndifferent modiﬁcation of the attention operation, replacing\nthe dot-product attention mechanism with an auto-correlation\nbased mechanism, again with O(N log N ) complexity. Like\nthe Informer, it can evaluate long horizons quickly and efﬁ-\nciently, and moreover, the authors show improved accuracy\ncompared to the Informer on several datasets.\nIn contrast to these works, we are primarily interested in\nimproving the forecasting accuracy of the Transformer, and\ndo not attempt to improve the computational complexity of\nthe model.\n123\nPersistence Initialization: a novel adaptation... 26785\n3.2 M4 competition methods\nTime series forecasting is an increasingly relevant area of\nresearch. However, in our opinion, there has not been a\nclear consensus within the Machine Learning community on\nhow to benchmark forecasting models. Earlier work, such as\nthe LogSparse Transformer by Li et al. [ 6], frequently used\nthe trafﬁc\n2 and electricity3 datasets. However, these datasets\nhave some issues that make them less suitable as a bench-\nmark dataset. There are at least three different train/test split\npoints used for each dataset, which makes comparing perfor-\nmance across different splits difﬁcult [ 19]. Moreover, these\ndatasets contain missing data, which complicates the train-\ning and evaluation setup. The trafﬁc dataset also has missing\ndata during some public holidays, but lacks documentation\nregarding which speciﬁc dates have been removed [ 19].\nWe suggest that the M4 dataset should be used as a\nstandard benchmark dataset for research into time series fore-\ncasting. The M4 dataset was introduced in the M4 forecasting\ncompetition [20], the fourth competition in a series of highly\ninﬂuential forecasting competitions, known as the Makri-\ndakis competitions. The dataset contains 100,000 time series;\ncompared to previously used datasets in Machine Learning\nforecasting studies, this is a very large dataset. These time\nseries were collected from a wide range of domains, and\nexhibit a wide range of behaviors. The organizers of the\ncompetition provided forecasts of several well-known base-\nline methods, including various naïve methods, exponential\nsmoothing methods, and the well-known ARIMA method.\nThese baseline methods are able to capture linear relation-\nships well, and are often difﬁcult to beat in many real world\nproblems which have strong auto-correlation. However, on\nthe M4 dataset, the best methods outperform these baselines\nsubstantially, which indicates the necessity of being able to\ncapture non-linear dynamics to achieve a high level of accu-\nracy on this dataset.\nThe M4 dataset has several desirable properties com-\npared to previously used forecasting datasets. It is quality\ncontrolled, and there are no missing data. The evaluation\nprocedures are clearly deﬁned, and there is no confusion\nregarding train/test split points. Furthermore, the M4 dataset\nis large, which arguably is a precondition for Deep Learning\nmethods to be successful. (The small size of the previous\nM3 competition dataset is believed to be a major reason for\nwhy neural networks did not perform well in that competi-\ntion [ 21].)\nThe M4 competition included 61 methods in total. These\nmethods used a wide variety of techniques, the majority of\nwhich were not Deep Learning techniques. However, to the\n2 https://archive.ics.uci.edu/dataset/204/pems+sf\n3 https://archive.ics.uci.edu/dataset/321/electricityloaddiagrams2011\n2014\nsurprise of some, the winning method relied heavily on Deep\nLearning. This method was developed by Smyl [ 22], and\nwas a hybrid model combining a recurrent neural network\nwith multiple exponential smoothing models. The parame-\nters of the exponential smoothing model were learned for\na single time series, while the neural network parameters\nwere shared across time series. The recurrent neural network\nwas a dilated LSTM [ 23] with attention [ 24] and resid-\nual connections [ 25]. Moreover, several such hybrid models\nwere combined in ensembles in order to improve forecasting\naccuracy.\nAfter the competition was ﬁnished, Oreshkin et al. pro-\nposed a new method which outperformed even the winner of\nthe competition, called N-BEATS [19]. The authors wanted\nto show that a deep neural network could perform well on\nthe M4 dataset, without the need for classical time series\nforecasting techniques, such as those used in Smyl’s hybrid\nmodel. N-BEA TS consists of blocks of feed-forward net-\nworks, which are combined such that each block provides\na partial forecast, and these are added together to produce\nthe ﬁnal forecast. Furthermore, instead of using a regular\nresidual skip connection, the partial forecasts are subtracted\nfrom the input of the next block, which the authors call\ndouble residual stacking . The ﬁnal model is an ensemble\nof 180 such networks, which are trained with 18 different\nconﬁgurations in order to ensure sufﬁcient diversity in the\nensemble.\nNone of the works on Transformers for time series from\nthe previous section evaluate their method on the complete\nM4 dataset. While Li et al. [ 6] evaluate their LogSparse\nTransformer on the hourly portion of the M4 dataset, the\nauthors do not report their performance in the metric used in\nthe competition (OW A), making it difﬁcult to compare their\naccuracy to the original M4 contestants.\nTo the best of our knowledge, our proposed method is\nthe ﬁrst to achieve competitive results on the complete M4\ndataset using a Transformer model. Moreover, to the best of\nour knowledge, our method is also the ﬁrst to achieve com-\npetitive results using a single neural network model, instead\nof using ensembles of Deep Learning models.\n4 Method\n4.1 The time series forecasting task\nA time series is deﬁned to be a sequence of fully observ-\nable measurements x =[ x\n1,..., xT ]∈ RT , where xt\nis the observation at time t , and T is the length of the\nseries. The goal of forecasting is to predict y given x, where\ny =[ xT +1,..., xT +H ]∈ RH , and H is the forecasting\nhorizon.\n123\n26786 E. Haugsdal et al.\n4.2 The PI-transformer\nWe will now introduce the PI-Transformer architecture. The\narchitecture can be divided into four parts: normalization,\nlinear projections, a decoder-only Transformer with Rotary\npositional encodings and ReZero normalization, and Persis-\ntence Initialization. A diagram of the complete architecture\nis shown in Fig. 1.\n4.2.1 Normalization\nThe ﬁrst step of our method is the normalization step. We\nﬁrst divide by the mean μ\nH of the H most recent values of\nSelfAttention\nNormalization\nfunction\nLinear\nProjection\nReZero\nTransformer\nFeedForward\nReZero\nPersisitence\nInitialization Multiplicative\nGating\nResidual Skip\nConnection\nInverse\nnormalization\nfunction\nLinear\nProjection\nFig. 1 The proposed adaptation consists of a skip connection and\na scalar multiplicative gating mechanism initialized to 0. The initial\nmodel becomes the naïve persistence model, i.e. the model that predicts\nˆxt +1 = xt\nx, and then perform a log transform:\nz = f (x) = ln x\nμH\n, (17)\nBy using only the H most recent values, instead of the entire\nsequence, we can better capture the trend of the series close to\nthe forecasting window. To produce an output in the original\ndata space, we perform the inverse transformation:\nf −1(z) = μH · ez (18)\nDuring training, gradients are back-propagated through the\ninverse transformation. From this point on, we will focus on\nhow to forecast the value of zt +1, as this can be converted\nto a forecast in the original data space by using the inverse\nnormalization function: ˆxt +1 = f −1(ˆzt +1).\n4.2.2 Transformer model\nOur method generates forecasts autoregressively by using\na decoder-only Transformer architecture, similar to genera-\ntive language models in NLP [ 3]. The architecture consists\nof blocks of causal self-attention layers and by feedforward\nlayers. In order to improve the stability of training the Trans-\nformer for forecasting, we connect the layers using residual\nskip connections with ReZero gating [ 14].\nIn NLP , embedding layers are commonly used to trans-\nform text tokens into feature vectors with size d\nmodel.W e\ninstead need to transform a univariate time series into a\nsequence of feature vectors, which we do with a linear pro-\njection. In other words, the initial input to the Transformer,\nX\n0, is deﬁned as follows:\nX0 = zWin , (19)\nwhere Win ∈ R1×dmodel is a learnable weight matrix and z is\nthe normalized input vector. Now, if we let Xi be the output\nof the i -th Transformer block, the rest of the model can be\ndeﬁned recursively:\nXi (Xi −1) = FFi (SAi (Xi −1)) (20)\nSAi (X ) = X + αi · SelfAttentioni (X ) (21)\nFFi (X ) = X + αi · FeedForwardi (X ), (22)\nwhere αi is the learnable ReZero scalar parameter shared\nbetween the self-attention and feedforward layer within each\nblock.\nThe feedforward layer is deﬁned as in the original Trans-\nformer, i.e. ( 8). However, for the self-attention layer, we\nreplace the standard absolute sinusoidal positional encod-\ning with a relative positional encoding, more speciﬁcally\n123\nPersistence Initialization: a novel adaptation... 26787\nthe Rotary encoding [ 15]. The effect of the Rotary encod-\ning is to multiply each key and query with a rotation matrix,\nwhich causes positional information to be encoded in the\nangle between the vectors. The use of the Rotary encoding is\nmotivated by the fact that there the absolute position within\na time series window has no semantic signiﬁcance, in con-\ntrast to text data, where the start of the sequence often has\na semantic meaning (for instance as the start of a document\nor a sentence). Instead of adding the positional encoding to\nthe input features of the model, as is done with a standard\nsinusoidal encoding, the Rotary encoding is implemented by\nmodifying the deﬁnition of self-attention:\nAttention(Q, K , V ) = softmax\n( ˜Q\nr ˜K T\nr√ dqk\n+ M\n)\nV , (23)\nwhere ˜Qr and ˜Kr represents the Q and K matrices with\nRotary positional encoding applied.\nFinally, we perform a linear projection on the output of\nthe ﬁnal Transformer block in order to go back to a univariate\nsequence. We deﬁne the T (z), the “Transformer function”,\nto be the value after this projection:\nT (z) = X\nN Wout, (24)\nwhere X N is the output of the N th block, and Wout ∈\nRdmodel×1 is a learnable weight matrix.\n4.2.3 Persistence initialization\nPersistence Initialization (PI) is a technique to implicitly\ninitialize an autoregressive neural network for forecasting,\nin order to improve training stability and forecasting per-\nformance. Speciﬁcally, the neural network is initialized to\nbecome a persistence model, which is a model that sim-\nply uses the last known value at time t as the forecast for\nt + 1, i.e. ˆz\nt +1 = zt . One way of combining this persis-\ntence forecast with the forecast from the Transformer model\ndeﬁned above, would be to add the outputs of both models:\nˆzt +1 = zt + T (z). However, this combined model will not\nbecome a persistence model at initialization, as the initial\noutputs will depend on the randomly initialized weights of\nnetwork. In order to ensure that the neural network does not\ncontribute to the initial forecasts, we introduce a new zero-\ninitialized gating parameter γ:\nˆz\nt +1 = zt + γ · T (z) (25)\nThis results in a combined architecture with the property that\nthe forecasts produced by the initial model are exactly equal\nto the persistence forecast. However, the combined archi-\ntecture is still able to improve upon this simple forecast by\nchanging the value of γ through learning.\n5 Experimental settings\n5.1 Dataset\nPublic datasets have played an important role for the develop-\nment of both deep learning methods and forecasting methods.\nThere are clear beneﬁts to having a publicly available high\nquality dataset, of which the most important is that it allows\nresearchers to measure progress in a standardized way. How-\never, Machine Learning research focusing on time series\nforecasting has lacked a commonly agreed up benchmark\ndataset. We propose to use the M4 dataset [ 20] for this\npurpose.\nThe M4 dataset was introduced in the fourth Makridakis\ncompetition, held in 2018. The previous Makridakis com-\npetitions have been very inﬂuential for the development of\nforecasting methods, and are well regarded in the forecasting\ncommunity. The M4 dataset consists of 100,000 time series\nfrom various domains, divided into six sub-sets based on their\nsampling frequency. Each frequency has a corresponding\nforecasting horizon H : Yearly ( H = 6), Quarterly ( H = 8),\nMonthly ( H = 18), Weekly ( H = 13), Daily ( H = 14), and\nHourly ( H = 48). Table 1 contains some descriptive statis-\ntics for each frequency. We consider each data frequency\nas an independent learning problem, and consequently train\nseparate models for each data frequency.\n5.2 Metrics\nPerformance in on the M4 dataset is measured a metric called\nOverall Weighted Average (OW A) [ 20]. OW A is a combi-\nnation metric, which combines the Mean Absolute Scaled\nError (MASE) and symmetric Mean Absolute Percentage\nError (sMAPE) metrics. MASE and sMAPE are both scale-\nindependent error metrics which are commonly used in the\ntime series forecasting literature [ 26]. The purpose of these\nmetrics is to enable comparisons of forecasting accuracy\nacross time series data with varying scales. In order to com-\nbine sMAPE and MASE metric into the single OW A metric,\nthese metric scores are scaled by corresponding metric scores\nfrom a baseline model. In the M4 competition, the base-\nline model was the Naïve2 model, which is a persistence\nmodel that is seasonally adjusted by multiplicative decom-\nposition [ 20, 27]. After scaling the metrics, their values are\ncombined by taking the average of the two, as follows:\nOWA = 1\n2\n[ sMAPE\nsMAPENa¨ıve2\n+ MASE\nMASENa¨ıve2\n]\n(26)\n5.2.1 MASE\nMASE is a scaled version of Mean Absolute Error (MAE).\nThe scaling factor for MASE is the MAE of a baseline model\n123\n26788 E. Haugsdal et al.\nTable 1 Descriptive statistics\nfor each frequency in the M4\ndataset\nYearly Quarterly Monthly Weekly Daily Hourly\nNumber of time series 23,000 24,000 48,000 359 4227 414\nForecasting horizon, H 6 8 18 13 14 48\nSeasonality, S 1 4 12 1 1 24\nMinimum length 19 24 60 93 107 748\n25% percentile length 26 70 100 392 337 748\n50% percentile length 35 96 220 947 2954 1008\n75% percentile length 46 123 324 1616 4211 1008\nMaximum length 841 874 2812 2610 9933 1008\non the training set. The baseline model used in the M4 com-\npetition is the seasonal naïve model, which always predicts\nthe value S steps in the past (e.g. 24 for all hourly time\nseries, 12 for all monthly time series, etc.). Let x be the\ntraining portion of the series, y the true continuation of the\nseries, and ˆy be the forecast. Then MASE can be deﬁned as\nfollows:\nMASE = 1\nN\nN∑\ni =1\n1\nH\n∑ H\nj =1|y(i )\nj −ˆy(i )\nj |\n1\nT (i )−S\n∑ T (i )\nj =S+1|x (i )\nj − x (i )\nj −S |\n, (27)\nwhere N is the number of time series, T (i ) is the length of\nthe time series, H is the forecasting horizon, and S is the\nseasonality. The superscript (i ) (as in x (i )) denotes the time\nseries with index i , with 1 ≤ i ≤ N .\n5.2.2 sMAPE\nsMAPE calculates the symmetric percentage difference\nbetween the forecast and the actual values. sMAPE scales\nthe absolute error at each time step by the average between\nthe forecast and ground truth at that time step, and can\nbe deﬁned as follows, using the previously introduced\nnotation:\nsMAPE = 100 · 1\nN\nN∑\ni =1\n1\nH\nH∑\nj =1\n|y(i )\nj −ˆy(i )\nj |\n(\n|y(i )\nj |+|ˆ y(i )\nj |\n)\n/2\n(28)\n5.3 Training\nWe used a sliding window approach to train our models. In\nother words, instead of full length time series, ﬁxed length\nsub-sequences (windows) were used to train. This was done\nto avoid the computational issues related to attention over\nlong sequences. The size of the sliding window was deﬁned\nto be nH , where H is the forecasting horizon and n is a\nhyperparameter determining the size of the window relative\nthe forecasting horizon.\nDuring training, teacher forcing was used to produce H\npredictions in parallel. Consequently, it is necessary to sam-\nple sub-sequences of length L = nH + H to train the model,\nwhere the ﬁrst nH elements are the sliding window inputs\nand the ﬁnal H elements are targets. To construct a training\nmini-batch, we ﬁrst a sampled a time series i with uniform\nprobability, and then sampled from the sub-sequences within\nthat time series with (conditional) uniform probability.\nWe created our validation set by combining all the right-\nmost sub-sequences of length L. However, in order to have\na greater number of sub-sequences available in the training\nset, we excluded the rightmost sub-sequences belonging to\nthe shortest sequences of the dataset. Using set notation, the\nprocedure can be described as follows: Create an index set\nI ={ i |∀\ni ,1≤i ≤N T (i ) ≥ P25 }, where T (i ) is the length\nof time series i , and P25 is equal to the 25th percentile in the\ndistribution of time series lengths. Then the validation set is\nXval = ⋃\ni X(i )\nval ={ x (i )\nT (i )−L<t ≤T (i ) | i ∈ I}, where the\nxa≤t ≤b notation indicates the sub-sequence of x starting at a\nand ending at b, i.e.: [xa , xa+1,..., xb−1, xb]. The training\nset is then created by enumerating all possible sub-sequences\nwithout overlapping targets in the validation set: Xtrain =⋃\ni X(i )\ntrain = ⋃\ni {x (i )\nj ≤t <j +L |∀ j ,1≤j ≤T (i )−L−H II (i )}, where\nII is the indicator function for I. See Fig. 2 for a graphical\nrepresentation.\nFig. 2 The validation set was created by taking the rightmost sub-\nsequence of length L = nH + H . The training set was created by\nenumerating all sub-sequences that do not overlap with the forecasting\nhorizon (i.e. the ﬁnal H time steps) of the validation set sub-sequence\n123\nPersistence Initialization: a novel adaptation... 26789\n5.4 Hyperparameters\nWe performed manual hyperparameter tuning with the goal\nof ﬁnding a general setting which could work well across\nall data frequencies of the M4 dataset. However, most of the\ntuning focused on the Monthly frequency. We were largely\nsuccessful in ﬁnding a general setting for all data frequencies,\nexcept for the value of n, which determines the size of the\ninput window relative to the forecasting horizon.\nFor the Yearly, Quarterly, Monthly, and Daily frequencies\nwe set n = 3; while for the Weekly and Hourly frequencies\nwe set n = 4. A value of n = 3 resulted in to poor perfor-\nmance on the Weekly and Hourly frequencies, likely due to\nseasonal patterns that are only included with window sizes\ncorresponding to n = 4. In the case of Weekly, n = 4 corre-\nsponds to 52 weeks, which indicates the presence of yearly\nseasonality. For the Hourly frequency, n = 4 corresponds\nto 192 hours, which is approximately 8 days, indicating a\nweekly seasonality.\nThe remaining hyperparameters were set to be identical\nfor all data frequencies. The model has 4 layers, 4 attention\nheads, d\nmodel = 512, and dff = 2048. We use the Lamb [ 28]\noptimizer with default hyperparameters, bias correction, and\ngradient clipping for gradients with norms greater than 10.\nOur loss function is deﬁned to be identical to the MASE\nmetric (27). We deﬁne a training epoch to consist of 128 mini-\nbatches of size 1024. As our stopping criterion, we use early\nstopping with a patience value of 8, such that training was\nstopped after 8 epochs without improvement in the validation\nloss.\n5.5 Ablation studies\nIn order to better understand the effects of the various com-\nponents of our models, we perform two ablation studies. To\nreduce the complexity of these studies, we focus exclusively\non the monthly portion of M4 dataset, which contains 48%\nof the series in the M4 dataset.\nThe ﬁrst ablation study focuses on the effects of Persis-\ntence Initialization, and the second ablation study focuses\non the effect of the positional encoding and the normaliza-\ntion layer. Moreover, in both studies we are also interested\nin the effect of the size of the Transformer model, and\npossible interactions between architectural components and\nmodel size. For this reason we also we vary the model\nsize by setting the hyperparameter d\nmodel to values in the\nset {32,64,128,256,512}, with the feedforward size dff set\nto 4 · dmodel.\nIn order to ensure fair comparisons, we perform 9 repeated\nexperiments for each model setting, such that each repeated\nexperiment has different weight initialization and data sam-\npling. This minimizes the effect of randomness due to weight\ninitialization and data sampling, and ensures that we are not\ncherry-picking the best performing models after the fact.\n5.5.1 First ablation study\nThe ﬁrst ablation study investigates the effects of the skip\nconnection and the multiplicative gating. We compare archi-\ntectures with neither skip connections nor multiplicative\ngating ( 29), architectures with a skip connection but with-\nout multiplicative gating ( 30), and architectures with both\na skip connection and multiplicative gating, i.e. Persistence\nInitialization ( 31):\nˆz\nt +1 = T (z) (29)\nˆzt +1 = zt + T (z) (30)\nˆzt +1 = zt + γ · T (z) (31)\n5.5.2 Second ablation study\nThe second ablation study compares the effect of the posi-\ntional encoding and the normalization layers. We compare\ntwo positional encodings: the original sinusoidal encod-\ning [ 1], and the Rotary encoding [ 15], both of which\nare described in Section 2.2. We compare three kinds of\nnormalization: ReZero, post-activation Layer Norm, and pre-\nactivation Layer Norm, as described in Section 2.3.T h i s\nresults in a total of six combinations of architecture settings\nfor the second ablation study.\n5.6 M4 comparison\nIn our second experiment we want to measure the perfor-\nmance of our PI-Transformer on the complete M4 dataset, in\norder to compare it to other state-of-the-art methods that have\nbeen evaluated on the complete M4 dataset. In particular, we\ncompare against the top 10 methods of the M4 competition.\nWe also compare against two versions of the N-BEATS [19]\nmethod, which was developed after the conclusion of the\ncompetition.\nAll the top performing methods on the M4 dataset are\nensemble methods, and for this reason we are mainly inter-\nested in two issues: forecasting performance and ensemble\nsize. This presents an obvious difﬁculty, as an ensemble\nmodel might have better forecasting accuracy compared to\na single model, which would mean that neither model is\nstrictly superior. To address this issue, we measure both\nthe performance of a single PI-Transformer model, and the\nperformance of an ensemble of PI-Transformers. First, we\nperform 9 repeated experiments for each subset of the M4\ndataset. As in the previous ablation studies, each repeated\nexperiment has different weight initialization and data sam-\npling. We will consider the model with the median OW A\n123\n26790 E. Haugsdal et al.\nscore within each data subset to estimate the expected per-\nformance of a single PI-Transformer on that subset. The total\nOW A score is computed by concatenating the predictions of\nthese median-score models from each data frequency into a\nsingle prediction on the complete dataset. Second, in order\nto estimate the effect of using a PI-Transformer in an ensem-\nble, we compute the mean of the 9 predictions for each data\nsubset. These mean predictions are then concatenated into a\nsingle prediction for the complete M4 dataset.\n5.7 Comparison to other transformer models for\ntime series\nFor the sake of completeness, we perform a ﬁnal experi-\nment where we compare the performance of our architecture\nagainst three Transformer models which have been applied\nto time series forecasting: LogSparse Transformer [ 6],\nInformer [ 7], and Autoformer [ 8].\nIn this comparison, we only use data from the Hourly sub-\nset of the M4 dataset, for two reasons. First, the Informer and\nAutoformer architectures were designed to deal with very\nlong forecasting horizons. The Hourly sub-set of the M4 for\nhas the longest forecasting horizon ( H = 48), so it is the part\nof the M4 dataset that most resembles the problems these\narchitectures were designed for. Second, in the case of the\nLogSparse Transformer, the authors report the performance\nof their model on the Hourly sub-set of the M4 dataset. This\nallows us to refer to the authors’ own reported performance,\ninstead of re-implementing the architecture.\nSimilarly to previous experiments, we perform repeated\nexperiments and report the median score. However, we only\nperform 5 repeats in this comparison instead of 9, as was\ndone previously. (For the LogSparse Transformer, we use\nthe authors’ own reported performance, which was not the\nmedian of 5 repeated experiments.)\nWe used publicly available code\n4,5 to implement the\nInformer and Autoformer. However, we found training the\nAutoformer to be challenging, as the loss values were gen-\nerally high throughout training. This was especially the case\nas the number of parameters increased, and for this reason\nwe decided to only consider the relatively small setting of\nd\nmodel = 32. Similarly to the previous experiments, we set\ndff = 4 · dmodel. For the Autoformer and the Informer we\nused 2 encoder layers and 2 decoder layers, and for the Trans-\nformer we used 4 layers, as before. This setting results in a\nsimilar number of parameters for the three models. We use\na context window of length H for the decoder of both the\nInformer and Autoformer.\nWe also found the previously used strategy of early stop-\nping on the validation loss to be unreliable when training\n4 https://github.com/zhouhaoyi/Informer2020\n5 https://github.com/thuml/Autoformer\nthe Informer and Autoformer, as the validation loss would\noften have much larger variance than in the previous experi-\nments. (We believe this difference comes from the one-shot\nforecasting approach taken by both methods. In contrast, our\nautoregressive model uses teacher forcing during training,\nleading to more stable loss values, as the model only has to\nperform 1-step predictions.) To provide a more fair compar-\nison, we instead allocate a ﬁxed amount of computation to\neach method by setting a limit of 100 epochs, and then select-\ning the weights with the lowest validation loss to compute\nthe ﬁnal test score.\nThe authors of LogSparse Transformer measured per-\nformance on M4-Hourly using a 0.5-quantile loss. To be\ncomparable, we also report the 0.5-quantile loss, which can\nbe deﬁned as follows:\nR\n0.5 =\n∑ N\ni =1\n∑ H\nj =1|y(i )\nj −ˆy(i )\nj |\n∑ N\ni =1\n∑ H\nj =1|y(i )\nj |\n(32)\n6 Results and discussion\n6.1 First ablation study\nThe ﬁrst ablation study focuses on the components of Per-\nsistence Initialization. We compare models using Persistence\nInitialization (PI) to two different ablation settings: models\nlacking both skip connection and multiplicative gating, and\nmodels with a skip connection but no multiplicative gating.\nAdditional details regarding this experiment can be found in\nSection 5.5.\nFigure 3 shows box plots of OW A test scores the three\nsettings, and Fig. 4 shows the training and validation loss\ncurves of the three settings. To give additional context about\nthe accuracy of the settings relative to other methods, the\nbox plots in Fig. 3 also includes striped lines representing\nthe ﬁrst and second place entries in the M4 competition. As a\nshorthand we will refer to the three settings by their ordering\nin the plots; i.e. setting 1 refers to models lacking both skip\nconnections and gating, setting 2 refers to the models with\nskip connections but no gating, and setting 3 refers to models\nwith Persistence Initialization.\nThe box plots in Fig. 3 shows that each setting has a\ndifferent relationship between the size of the model and\nforecasting accuracy. The two ablation settings do not show\nimproved accuracy as model size is increased. For setting 1,\nthe smallest model performs worse than the largest model.\nIn setting 2 all the model sizes perform at a similar level.\nOnly models in setting 3 (i.e. with PI) improve in accuracy\nwhen model size is increased. Moreover, the largest model\nsize has a lower median OW A model than the winner of the\n123\nPersistence Initialization: a novel adaptation... 26791\nFig. 3 Box plots of OW A test\nscores from the ﬁrst ablation\nstudy. Each box represents 9\nrepeated runs. The striped lines\ncorrespond to the ﬁrst and\nsecond place entries in the M4\ncompetition\nM4 competition, which shows that Transformer models with\nPersistence Initialization are able to achieve a high level of\naccuracy.\nLooking at the loss curves for in Fig. 4, we see several\nindications as to why models with PI achieve better accuracy.\nCompared to the curves of the two ablation settings, the loss\ncurves of setting 3 (i.e. PI) are shifted both down and to the\nleft. In other words, models with PI start at a lower loss and\nend at a lower loss, and do so in fewer iterations. It is not\nsurprising that these models start at lower initial loss values,\nas PI is an initialization technique, designed to improve the\ninitial forecasts of a forecasting model. It might be more\nsurprising that these models also achieve lower ﬁnal loss\nvalues, as the Transformer is known to be a very powerful\narchitecture. One might expect that a Transformer without\nPI would be able to achieve the same level of accuracy by\nquickly learning to select the previous time step in one of its\nattention heads. However, the fact that the models without\nPI train for more iterations, only to achieve worse results,\nsuggests that learning this simple mapping is in fact non-\ntrivial.\nIn conclusion, this ablation study clearly shows that Per-\nsistence Initialization has a large effect on the training\nprocess, improving both performance and training stability.\nBoth the residual skip connection and the multiplicative gat-\ning parameter are necessary to see these effects.\nFig. 4 V alidation and training\nlosses from the ﬁrst ablation\nstudy. Each line represents the\nmean loss over 9 repeated runs,\nwith the shaded area\nrepresenting the standard\ndeviation. Note that this plot\ncontains a form of survival bias,\nas training is stopped once the\nvalidation loss ﬂattens or\nincreases\n123\n26792 E. Haugsdal et al.\nFig. 5 Box plots of OW A test\nscores from the second ablation\nstudy. Each box represents 9\nrepeated runs. The striped lines\ncorrespond to the ﬁrst and\nsecond place entries in the M4\ncompetition. Note that some\nboxes are located entirely\noutside the bounds of the plot\n6.2 Second ablation study\nThe second ablation study compares the effect of the posi-\ntional encodings and normalization layers. We compare two\nkinds of positional encodings and three kinds of normaliza-\ntion layers. The two positional encodings are the standard\nsinusoidal positional encoding and the Rotary encoding.\nThe three kinds of normalization layers are post-activation\nLayer Normalization, pre-activation Layer Normalization,\nand ReZero normalization. Additional details regarding this\nexperiment can be found in Section 5.5.\nFigure 5 shows box plots of test scores for the six ablation\nsettings. As in the previous experiment, we include striped\nlines representing the ﬁrst and second place entries in the\nM4 competition to give additional context about the level of\naccuracy.\nBy inspecting the box plots we immediately see that\nthe Rotary encoding outperforms the sinusoidal encoding\nin every setting of the experiment. This suggests that the\nRotary encoding is better suited for time series tasks than the\nsinusoidal encoding. Furthermore, the models with ReZero\nnormalization show improved performance compared to the\nother two options, indicating that ReZero might be the better\nchoice of normalization function for time series tasks.\n6.3 M4 dataset performance\nThis experiment compares the PI-Transformer to other state-\nof-the-art methods on the complete M4 dataset. We are\nmainly interested in two issues: forecasting performance and\nensemble size. Table 2 compares our method to the top 10\nmethods of the M4 competition. We also include two ver-\nsions of the N-BEATS [19] method, which was proposed after\nthe conclusion of the competition. See Section 5.6 for more\ndetails regarding this experiment.\nThe size of an ensemble of models is an important aspect of\npractical usefulness in real world settings. This is especially\ntrue when the ensemble members are themselves complex\nmodels, such as Deep Learning models. The current top per-\nforming method on the M4 dataset is the N-BEA TS method\nby Oreshkin et al. [ 19], which consists of an ensemble of\n180 feed-forward neural networks. To ensure diversity in the\nensemble, the authors used three different loss functions and\nsix different window sizes, resulting in a total of 18 different\nmodel conﬁgurations. The ﬁnal ensemble was then formed\nby using 18 copies of 10 identical model conﬁgurations,\nresulting in 180 models in total. The authors also reported\nthe performance of a smaller ensemble which only used one\ncopy of each model conﬁguration, which we have called N-\nBEATS-18 in Table 2. The top performing method of the M4\ncompetition was also an ensemble. The winner of the compe-\ntition, Smyl [ 22], used a complex strategy which combined\nmodels at multiple conceptual levels. First, at the level of\nensembles of models, the method combines forecasts from\n6-9 independent training runs. Second, each of these train-\ning runs consisted of multiple models which were trained\non subsets of the dataset. Finally, the forecasts of each such\nmodel were produced by taking the average of the predictions\nproduced by the models in the ﬁnal 4-5 training epochs.\nBoth of these methods are arguably highly complex, but\nthey also substantially improved forecasting performance\ncompared simpler methods. The signiﬁcance of the improve-\nments in accuracy can be most easily be seen by inspecting\nthe total OW A of the methods from rank 2 to until rank\n6. The median difference between these consecutive ranks\nis 0.001 OW A, and the difference between the best and\nthe worst method in this group is 0.010 OW A. In contrast,\nthe difference between the rank 1 method (i.e. the winner,\nSmyl) and the rank 2 method is 0.017 OW A. This obser-\nvation led the organizers of the competition to characterize\n123\nPersistence Initialization: a novel adaptation... 26793\nTable 2 OW A test scores for each frequency of the M4 dataset. Bold\nfont is used to indicate the best model, an underline indicates second\nbest. N is the number of time series, and H is the forecasting horizon.\nN-BEA TS-18 is a version of the N-BEA TS model with 18 models in\nits ensemble instead of 180. We report the OW A score for this model\nbased on Fig. 3 from Oreshkin et al. [ 19]\nNumber Yearly Quarterly Monthly Weekly Daily Hourly Total\nof N =23000 N =24000 N =48000 N =359 N =4227 N =414\nmodels H =6 H =8 H =18 H =13 H =14 H =48\nM4 Rank 10 7 0.824 0.883 0.899 0.939 0.990 0.485 0.869\nM4 Rank 9 2 0.836 0.878 0.881 0.782 1.002 0.410 0.865\nM4 Rank 8 1 0.788 0.898 0.905 0.968 0.996 1.012 0.861\nM4 Rank 7 2 to 3 0.801 0.908 0.882 0.957 1.060 0.653 0.860\nM4 Rank 6 4 0.806 0.853 0.876 0.751 0.984 0.663 0.848\nM4 Rank 5 4 0.802 0.855 0.868 0.897\n0.977 0.674 0.843\nM4 Rank 4 24 0.813 0.859 0.854 0.795 0.996 0.474 0.842\nM4 Rank 3 4 to 15 0.820 0.855 0.867 0.766 0.806 0.444 0.841\nM4 Rank 2 9 0.799 0.847 0.858 0.796 1.019 0.484 0.838\nM4 Rank 1 6 to 9 0.778 0.847 0.836 0.851 1.046 0.440 0.821\nN-BEA TS-18\n1 1 8 ---- - - 0 . 8 0 2\nN-BEA TS1 180 0.758 0.800 0.819 - - - 0.795\nPI-Transformer2 1 0.777 0.852 0.833 0.733 0.987 0.431 0.815\nPI-Transformer3 9 0.769 0.836 0.813 0.697 0.987 0.397 0.800\n1 Authors do not report OW A scores for columns containing “-”\n2 Median OW A scores from 9 repeated experiments\n3 Mean ensemble of the predictions of the 9 repeated experiments\nthe difference between ranks 6 to 2 as “miniscule”, while\nthe difference between ranks 2 and 1 was characterized as\n“considerable” [29]. A similar argument can be made for the\ndifference between N-BEA TS and the rank 1 method, which\nis even greater: 0.026 OW A.\nIn this experiment, we trained 9 PI-Transformer mod-\nels for each frequency of the M4 dataset. We measure the\nexpected OW A of a single PI-Transformer by the median\nOW A of the 9 models. We measure the OW A of an ensem-\nble of PI-Transformer models by using the mean of the 9\npredictions, which is arguably a more fair comparison to the\nother methods of Table 2, as most of these are in fact also\nensembles.\nAs can be seen from Table 2, the median-OW A PI-\nTransformer achieved a score between the winner of the M4\ncompetition and the N-BEA TS method. The difference to the\nwinner of the competition is 0.006 OW A, which is relatively\nsmall, as we have discussed above. However, we would argue\nthat the most important advantage of our method is that it is\neasier to use for Machine Learning practitioners.\nThe mean-ensemble PI-Transformer achieves a score\nbetween N-BEA TS-18 and the full N-BEA TS model. As\nbefore, the differences in OW A are small: 0.002 to 0.005\nOW A. Considering that N-BEA TS consists of an ensemble of\n180 models, we believe that our approach represents a favor-\nable trade-off between forecasting accuracy and ensemble\nsize in this case.\n6.4 Comparison with other transformer models for\ntime series\nIn this experiment, we compare our proposed Transformer\narchitecture against three other Transformer models recently\nproposed for time series forecasting: the LogSparse Trans-\nformer [ 6], the Informer [ 7], and the Autoformer [ 8]. The\ndetails regarding the experimental setup for this experiment\ncan be found in Section 5.7.\nTable 3 shows the results of the comparison. As can be\nseen from the table, our method outperforms the others, both\nin terms of OW A and in terms of R0.5.\nTable 3 Comparison of Transformer models on M4-Hourly. We include\nthe R0.5 score to be comparable with the LogSparse Transformer, as the\nauthors do not report the OW A score\nOWA R0.5\nLogSparse Transformer1 - 0.067\nInformer2 0.670 0.056\nAutoformer2 1.033 0.078\nPI-Transformer2 0.525 0.046\nFor this experiment, we only used small models with dmodel = 32.\n1 Score taken from [ 6]\n2 Median scores of 5 repeated experiments\n123\n26794 E. Haugsdal et al.\nThis shows that our method is able to achieve better fore-\ncasting accuracy compared to recently proposed Transformer\nmethods for time series. However, we would emphasize\nthat these architectures are inherently different, and were\ndesigned for different purposes. The PI-Transformer is a\ndecoder-only architecture, while the Informer and Auto-\nformer are encoder-decoder architectures. We have attempted\nto make the comparison between these models as fair as pos-\nsible by keeping the number of parameters in these models\napproximately equal. This required using two encoder lay-\ners and two decoder layers for the Informer and Autoformer,\ncompared to the four decoder layers of our PI-Transformer.\nHowever, this results in models with different depth; a better\ncomparison might be to use a depth of 4 for both the encoder\nlayers and decoder layers, regardless of the total parameter\ncount. Moreover, our model is an autoregressive architecture\nwhich needs to perform several model evaluations whenever\nthe forecasting horizon is greater than 1, in contrast to the\nAutoformer and Informer which produce a full horizon of\nforecasts in a single evaluation. Consequently, these models\nare likely much faster to evaluate.\n6.5 Interpretations of persistence initialization\nIt is perhaps surprising that adding a single parameter, as\nPersistence Initialization does, can have such a big impact\non forecasting accuracy. Persistence Initialization is a rel-\natively simple change, and the Transformer is a powerful\nmodel. One might expect that it would be able to “discover”\nthis pattern by itself, by using the attention mechanism to\nselect the previous time step in one of its attention heads.\nIn this section we will discuss some interpretations of what\nPersistence Initialization is doing.\nOne interpretation, which is the one suggested by our\nnaming choice, is that the model is initialized to become a per-\nsistence model. During training the model changes from the\nnaïve persistence model to become a more complex model.\nIn other words, Persistence Initialization can be seen as a\nkind of implicit weight initialization.\nA related interpretation is that Persistence Initialization is\na re-parametrization of the forecasting problem. Instead of\ndirectly forecasting the values of the time series, the model\nmust instead predict the difference to the previous time step.\nFurthermore, the way the model is initialized corresponds to a\nprior belief that these differences are zero. This interpretation\nis somewhat related to the concept of differencing, which is a\ntechnique commonly used in statistical forecasting methods\nto make time series more stationary. However, Persistence\nInitialization is not the same as differencing, as only the out-\nputs of the PI-Transformer are (implicitly) differenced, and\nnot the inputs.\nA third interpretation is that we are combining two models\nin a way that is somewhat similar to boosting. In boosting,\na sequence of models are trained iteratively to predict the\nresidual errors of the previous models. We combine the naïve\npersistence model and a Transformer, such that the Trans-\nformer predicts the residuals of the persistence model. The\npersistence model has a ﬁxed weight of 1, and the weight of\nthe Transformer is the gating parameter γ.\n7 Conclusion\nIn this work, we have presented Persistence Initialization,\na novel and general adaptation for autoregressive time\nseries forecasting with neural networks. Furthermore, we\nintroduced PI-Transformer, a Transformer model based on\nPersistence Initialization, Rotary positional encodings, and\nReZero normalization. We perform two ablation studies, and\nshow that the PI-Transformer learns faster, is more accurate,\nand scales better than Transformer models without our pro-\nposed modiﬁcations. Moreover, we measure the performance\nof our proposed PI-Transformer model on the complete M4\ndataset, and ﬁnd that it is able to achieve a high level of\nforecasting accuracy, similar to other state-of-the-art meth-\nods. Our method outperforms the original winner of the\nM4 competition, and achieves a comparable level of accu-\nracy to N-BEA TS, which is an ensemble of 180 deep neural\nnetworks, using a signiﬁcantly smaller ensemble of only 9\nPI-Transformers.\nAcknowledgements This research was carried out with the support\nof the ML4ITS project (312062), funded by the Norwegian Research\nCouncil (NFR)\nFunding Open access funding provided by NTNU Norwegian Univer-\nsity of Science and Technology (incl St. Olavs Hospital - Trondheim\nUniversity Hospital).\nData and Code Availability Source code and raw data (i.e. model\npredictions) are available at https://github.com/EspenHa/persistence_\ninitialization\nDeclarations\nConﬂicts of Interest The authors have no conﬂicts of interest to declare\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing, adap-\ntation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indi-\ncate if changes were made. The images or other third party material\nin this article are included in the article’s Creative Commons licence,\nunless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your\nintended use is not permitted by statutory regulation or exceeds the\npermitted use, you will need to obtain permission directly from the copy-\nright holder. To view a copy of this licence, visit http://creativecomm\nons.org/licenses/by/4.0/.\n123\nPersistence Initialization: a novel adaptation... 26795\nReferences\n1. V aswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez\nAN, Kaiser L, Polosukhin I (2017) Attention is all you need. In:\nAdvances in Neural Information Processing Systems, pp. 5998–\n6008\n2. Devlin J, Chang M-W, Lee K, Toutanova K (2019) BERT: Pre-\ntraining of Deep Bidirectional Transformers for Language Under-\nstanding. In: Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguis-\ntics: Human Language Technologies, V olume 1 (Long and Short\nPapers), pp. 4171–4186. https://aclanthology.org/N19-1423\n3. Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P ,\nNeelakantan A, Shyam P , Sastry G, Askell A et al (2020) Lan-\nguage models are few-shot learners. Adv Neural Inf Process Syst\n33:1877–1901\n4. Jumper J, Evans R, Pritzel A, Green T, Figurnov M, Ron-\nneberger O, Tunyasuvunakool K, Bates R, Žídek A, Potapenko\nA et al (2021) Highly accurate protein structure prediction with\nAlphaFold. Nature 596(7873):583–589\n5. Chen L, Lu K, Rajeswaran A, Lee K, Grover A, Laskin M, Abbeel\nP , Srinivas A, Mordatch I (2021) Decision Transformer: Reinforce-\nment learning via sequence modeling. Adv Neural Inf Process Syst\n34:5084–15097\n6. Li S, Jin X, Xuan Y , Zhou X, Chen W, Wang Y -X, Yan X (2019)\nEnhancing the locality and breaking the memory bottleneck of\ntransformer on time series forecasting. Adv Neural Inf Process\nSyst 32:5243–5253\n7. Zhou H, Zhang S, Peng J, Zhang S, Li J, Xiong H, Zhang W (2021)\nInformer: Beyond efﬁcient Transformer for Long Sequence Time-\nSeries Forecasting. In: Proceedings of AAAI\n8. Wu H, Xu J, Wang J, Long M (2021) Autoformer: Decomposition\ntransformers with auto-correlation for long-term series forecasting.\nAdv Neural Inf Process Syst 34:22419–22430\n9. Hinton GE, Salakhutdinov RR (2006) Reducing the dimensionality\nof data with neural networks. Science 313(5786):504–507\n10. Glorot X, Bengio Y (2010) Understanding the difﬁculty of train-\ning deep feed-forward neural networks. In: Proceedings of the\nThirteenth International Conference on Artiﬁcial Intelligence and\nStatistics, pp. 249–256. JMLR Workshop and Conference Proceed-\nings\n11. Goodfellow I, Bengio Y , Courville A (2016) Deep Learning\n12. Hyndman RJ, Athanasopoulos G (2018) Forecasting: Principles\nand Practice\n13. Ba JL, Kiros JR, Hinton GE (2016) Layer normalization.\narXiv:1607.06450\n14. Bachlechner T, Majumder BP , Mao H, Cottrell G, McAuley J\n(2021) Rezero is all you need: Fast convergence at large depth.\nIn: Uncertainty in Artiﬁcial Intelligence, pp. 1352–1361. PMLR\n15. Su J, Lu Y , Pan S, Murtadha A, Wen B, Liu Y (2021)\nRoformer: Enhanced transformer with rotary position embedding.\narXiv:2104.09864\n16. Ioffe S, Szegedy C (2015) Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift. In: Interna-\ntional Conference on Machine Learning, pp. 448–456. PMLR\n17. Baevski A, Auli M (2019) Adaptive Input Representations for Neu-\nral Language Modeling. In: International Conference on Learning\nRepresentations. https://openreview.net/forum?id=ByxZX20qFQ\n18. Tay Y , Dehghani M, Bahri D, Metzler D (2022) Efﬁcient trans-\nformers: A survey. ACM Comput Surv 55(6):1–28\n19. Oreshkin BN, Carpov D, Chapados N, Bengio Y (2020) N-BEA TS:\nNeural basis expansion analysis for interpretable time series fore-\ncasting. In: International Conference on Learning Representations.\nhttps://openreview.net/forum?id=r1ecqn4YwB\n20. Makridakis S, Spiliotis E, Assimakopoulos V (2020) The M4 Com-\npetition: 100,000 time series and 61 forecasting methods. Int J\nForecasting 36(1):54–74\n21. Hyndman RJ (2020) A brief history of forecasting competitions.\nInt J Forecasting 36(1):7–14\n22. Smyl S (2020) A hybrid method of exponential smoothing and\nrecurrent neural networks for time series forecasting. Int J Fore-\ncasting 36(1):75–85\n23. Chang S, Zhang Y , Han W, Y u M, Guo X, Tan W, Cui X, Witbrock\nM, Hasegawa-Johnson MA, Huang TS (2017) Dilated recurrent\nneural networks. Adv Neural Inf Process Syst 30\n24. Qin Y , Song D, Cheng H, Cheng W, Jiang G, Cottrell GW (2017) A\ndual-stage attention-based recurrent neural network for time series\nprediction. In: Proceedings of the 26th International Joint Confer-\nence on Artiﬁcial Intelligence, pp. 2627–2633\n25. Kim J, El-Khamy M, Lee J (2017) Residual LSTM: Design of a\nDeep Recurrent Architecture for Distant Speech Recognition. In:\nProc. Inter-speech 2017, pp. 1591–1595. https://doi.org/10.21437/\nInterspeech.2017-477\n26. Hyndman RJ, Koehler AB (2006) Another look at measures of\nforecast accuracy. Int J Forecasting 22(4):679–688\n27. Makridakis S, Hibon M (2000) The M3-competition: Results. Con-\nclusions and Implications. Intl J Forecasting 16(4):451–476\n28. Y ou Y , Li J, Reddi S, Hseu J, Kumar S, Bhojanapalli S, Song X,\nDemmel J, Keutzer K, Hsieh C-J (2020) Large Batch Optimization\nfor Deep Learning: Training BERT in 76 minutes. In: International\nConference on Learning Representations. https://openreview.net/\nforum?id=Syx4wnEtvH\n29. Makridakis S, Spiliotis E, Assimakopoulos V (2018) The M4\ncompetition: Results, ﬁndings, conclusion and way forward. Int\nJ Forecasting 34(4):802–808\nPublisher’s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional afﬁliations.\nEspen Haugsdal is currently pur-\nsuing a PhD in Computer Science\nand Artiﬁcial Intelligence at the\nNorwegian University of Science\nand Technology (NTNU). He com-\npleted his Master’s degree in Com-\nputer Science at NTNU in 2021.\nHis current research is focused on\nthe use of Transformer models for\ntime series tasks.\n123\n26796 E. Haugsdal et al.\nErlend Aune is a quant researcher\nat Abelee, and Adjunct Associate\nProfessor of Data Science at NTNU\nand BI. He has a PhD in statistics\nfrom NTNU. Research interests\ninclude: creative use of AI and\ndata, machine learning for time\nseries modeling, innovation using\nAI.\nMassimiliano Ruocco is presently a\nSenior Researcher at SINTEF Dig-\nital. He received his PhD at the\nNorwegian University of Science\nand Technology (NTNU). Currently,\nhe also holds the position of Adjunct\nAssociate Professor at NTNU in the\nData and Artiﬁcial Intelligence\nGroup within the Computer Science\nDepartment. His research interests\nencompass a wide spectrum, focus-\ning on Machine Learning and Deep\nNeural Networks with application\nacross various domains, includ-\ning Telecommunication, Health-\ncare, Transport, and Aviation.\n123",
  "topic": "Initialization",
  "concepts": [
    {
      "name": "Initialization",
      "score": 0.8868356943130493
    },
    {
      "name": "Computer science",
      "score": 0.8235070109367371
    },
    {
      "name": "Transformer",
      "score": 0.6851212978363037
    },
    {
      "name": "Normalization (sociology)",
      "score": 0.5098215937614441
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4374406933784485
    },
    {
      "name": "Time series",
      "score": 0.4190611243247986
    },
    {
      "name": "Machine learning",
      "score": 0.40485483407974243
    },
    {
      "name": "Voltage",
      "score": 0.1858246922492981
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Anthropology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    }
  ]
}