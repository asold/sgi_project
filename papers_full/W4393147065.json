{
  "title": "Sparsity-Guided Holistic Explanation for LLMs with Interpretable Inference-Time Intervention",
  "url": "https://openalex.org/W4393147065",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5085962901",
      "name": "Zhen Tan",
      "affiliations": [
        "Arizona State University"
      ]
    },
    {
      "id": "https://openalex.org/A5103073431",
      "name": "Tianlong Chen",
      "affiliations": [
        "University of North Carolina at Chapel Hill"
      ]
    },
    {
      "id": "https://openalex.org/A5100389535",
      "name": "Zhenyu Zhang",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    },
    {
      "id": "https://openalex.org/A5100338946",
      "name": "Huan Liu",
      "affiliations": [
        "Arizona State University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4281965619",
    "https://openalex.org/W3126996274",
    "https://openalex.org/W6770699648",
    "https://openalex.org/W6779450612",
    "https://openalex.org/W2119144962",
    "https://openalex.org/W2125389748",
    "https://openalex.org/W2737121650",
    "https://openalex.org/W6779243538",
    "https://openalex.org/W4221165032",
    "https://openalex.org/W6677103964",
    "https://openalex.org/W4386071642",
    "https://openalex.org/W4385574275",
    "https://openalex.org/W2618851150",
    "https://openalex.org/W2772984056",
    "https://openalex.org/W6851326247",
    "https://openalex.org/W6695661434",
    "https://openalex.org/W3116103312",
    "https://openalex.org/W2769063188",
    "https://openalex.org/W6808394947",
    "https://openalex.org/W2949202705",
    "https://openalex.org/W4382491061",
    "https://openalex.org/W6787302050",
    "https://openalex.org/W3170790933",
    "https://openalex.org/W6726973340",
    "https://openalex.org/W4308244910",
    "https://openalex.org/W3173813266",
    "https://openalex.org/W4288347855",
    "https://openalex.org/W3128098768",
    "https://openalex.org/W3174057701",
    "https://openalex.org/W4385573119",
    "https://openalex.org/W2520760693",
    "https://openalex.org/W4206634569",
    "https://openalex.org/W2962862931",
    "https://openalex.org/W4388926523",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3035602609",
    "https://openalex.org/W4381586827",
    "https://openalex.org/W2516809705",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4365600667",
    "https://openalex.org/W4389524416",
    "https://openalex.org/W4387165318",
    "https://openalex.org/W4388650587",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W2915589364",
    "https://openalex.org/W4323323809",
    "https://openalex.org/W2963791246",
    "https://openalex.org/W4379928343",
    "https://openalex.org/W4306295121",
    "https://openalex.org/W4388584666",
    "https://openalex.org/W2964027067",
    "https://openalex.org/W4387994806",
    "https://openalex.org/W2963363373",
    "https://openalex.org/W4297813615",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2950784811",
    "https://openalex.org/W4385568196",
    "https://openalex.org/W2990844796",
    "https://openalex.org/W3031696893",
    "https://openalex.org/W4310002146"
  ],
  "abstract": "Large Language Models (LLMs) have achieved unprecedented breakthroughs in various natural language processing domains. However, the enigmatic ``black-box'' nature of LLMs remains a significant challenge for interpretability, hampering transparent and accountable applications. While past approaches, such as attention visualization, pivotal subnetwork extraction, and concept-based analyses, offer some insight, they often focus on either local or global explanations within a single dimension, occasionally falling short in providing comprehensive clarity. In response, we propose a novel methodology anchored in sparsity-guided techniques, aiming to provide a holistic interpretation of LLMs. Our framework, termed SparseCBM, innovatively integrates sparsity to elucidate three intertwined layers of interpretation: input, subnetwork, and concept levels. In addition, the newly introduced dimension of interpretable inference-time intervention facilitates dynamic adjustments to the model during deployment. Through rigorous empirical evaluations on real-world datasets, we demonstrate that SparseCBM delivers a profound understanding of LLM behaviors, setting it apart in both interpreting and ameliorating model inaccuracies. Codes are provided in supplements.",
  "full_text": "Sparsity-Guided Holistic Explanation for LLMs with\nInterpretable Inference-Time Intervention\nZhen Tan1, Tianlong Chen2, Zhenyu Zhang3, Huan Liu1\n1Arizona State University\n2University of North Carolina at Chapel Hill\n3University of Texas at Austin\n{ztan36,huanliu}@asu.edu, tianlong@cs.unc.edu, zhenyu.zhang@utexas.edu\nAbstract\nLarge Language Models (LLMs) have achieved unprece-\ndented breakthroughs in various natural language process-\ning domains. However, the enigmatic “black-box” nature of\nLLMs remains a significant challenge for interpretability,\nhampering transparent and accountable applications. While\npast approaches, such as attention visualization, pivotal sub-\nnetwork extraction, and concept-based analyses, offer some\ninsight, they often focus on either local or global explana-\ntions within a single dimension, occasionally falling short in\nproviding comprehensive clarity. In response, we propose a\nnovel methodology anchored in sparsity-guided techniques,\naiming to provide a holistic interpretation of LLMs. Our\nframework, termed SparseCBM, innovatively integrates spar-\nsity to elucidate three intertwined layers of interpretation: in-\nput, subnetwork, and concept levels. In addition, the newly\nintroduced dimension of interpretable inference-time inter-\nvention facilitates dynamic adjustments to the model during\ndeployment. Through rigorous empirical evaluations on real-\nworld datasets, we demonstrate that SparseCBM delivers a\nprofound understanding of LLM behaviors, setting it apart in\nboth interpreting and ameliorating model inaccuracies. Codes\nare provided in supplements.\nIntroduction\nThe advent of Large Language Models (LLMs) has captured\nthe intricacies of language patterns with striking finesse, ri-\nvaling, and at times, surpassing human performance (Zhou\net al. 2022; OpenAI 2023). However, their laudable suc-\ncess story is shadowed by a pressing concern: a distinct lack\nof transparency and interpretability. As LLMs burgeon in\ncomplexity and scale, the elucidation of their internal mech-\nanisms and decision-making processes has become a daunt-\ning challenge. The opaque “black-box” characteristics of\nthese models obfuscate the transformation process from in-\nput data to generated output, presenting a formidable bar-\nrier to trust, debugging, and optimal utilization of these po-\ntent computational tools. Consequently, advancing the in-\nterpretability of LLMs has emerged as a crucial frontier in\nmachine learning and natural language processing research,\naiming to reconcile the dichotomy between superior model\nperformance and comprehensive usability.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: The illustration includes: (a) Attention visual-\nization provides a localized, attention-driven explanation.\nWhile insightful, this might be less decipherable or intuitive\nfor users outside the realm of computer science. (b) CBMs\ndeliver a broader, concept-level understanding, resonating\nnaturally with human cognition. However, they sometimes\nmiss out on the nuanced, granular insights of the LLM’s\nworkings. (c) SparseCBMs outline a holistic decision path-\nway for each input, seamlessly progressing from tokens, via\npertinent subnetworks and concepts, to the final task label.\nThis approach marries the strengths of both local and global\nexplanations, addressing their respective shortcomings.\nThe spectrum of interpretability solutions for language\nmodels can be broadly bifurcated into two categories.❶ Ini-\ntial approaches predominantly leverage local explanations,\nemploying techniques such as visualization of attention\nweights (Galassi, Lippi, and Torroni 2020), probing of fea-\nture representations (Mishra, Sturm, and Dixon 2017; Lund-\nberg and Lee 2017), and utilization of counterfactuals (Wu\net al. 2021; Ross, Marasovi ´c, and Peters 2021), among oth-\ners. These methods focus on providing explanations at gran-\nular levels, such as individual tokens, instances, neurons, or\nsubnetworks, as exemplified in Figure 1 (a). While these\nlow-level explanations offer a degree of reliability, they of-\nten sacrifice readability and intuitiveness (Losch, Fritz, and\nSchiele 2019), thereby constraining their practical applica-\nbility. ❷ More recently, researchers have tended to global\nexplanations, such as concept-based analyses that inherently\nresonate with human cognition (Wang et al. 2023a; Abraham\net al. 2022). For instance, one recent work (Tan et al. 2023)\nincorporates Concept Bottleneck Models (CBMs) (Koh et al.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21619\n2020) into pretrained language models, leading to an im-\npressive “interpretability-utility” Pareto front. Figure 1 (b)\nexemplifies this for sentiment analysis tasks, where human-\nintelligible concepts like “Food”, “Ambiance”, and “Ser-\nvice” correspond to neurons in the concept bottleneck layer.\nThe final decision layer is designed as a linear function\nof these concepts, rendering the decision rules easily un-\nderstandable. However, these methods excessively focus on\nglobal explanations. The underlying reasoning between\nraw input and concepts remains unclear.\nTo address these limitations, our work champions aholis-\ntic interpretation of LLM predictions. We unveil Spar-\nseCBM, an evolved CBM variant that melds the complemen-\ntary “strengths” of local and global explanations, thereby\naddressing the individual “weaknesses” of each. This con-\nfluence is born from rigorous sparsity-guided refinement de-\nsigned specifically for LLMs, as depicted in Figure 1 (c).\nConcretely, SparseCBM iteratively prunes the LLM back-\nbone guided by a joint objective of optimizing for both con-\ncept and task labels until the desired sparsity level is accom-\nplished. This exercise distills the LLM into distinct yet in-\nterconnected subnetworks, each corresponding to a prede-\nfined concept. As such, SparseCBM provides a comprehen-\nsive and intelligible decision-making pathway for each input\ntext, tracing from tokens through subnetworks and concepts,\nultimately leading to the final task label.\nAnother unique feature is that, SparseCBMs allow inter-\npretable inference-time intervention(Koh et al. 2020; Li\net al. 2023a). The inherent sparsity-driven structure of Spar-\nseCBM allows it to adjust its internal parameters dynami-\ncally, based on the context of the input. In practical terms,\nthis means that, during inference, SparseCBM can identify\npotential areas of ambiguity or misconception, and proac-\ntively modify its internal decision-making routes without a\nfull-scale retraining. This “on-the-fly” adaptability not only\nenhances prediction accuracy but also offers users a window\ninto how the model adjusts its reasoning in real time. By\nmaking these modifications both accessible and understand-\nable, SparseCBM bridges the common chasm between in-\nterpretability and agility for LLMs. This real-time decision\npathway modification, stands as a beacon for fostering trust\nand facilitating more nuanced human-model interactions. In\nsummary, SparseCBM carries the following advantages:\n• Empirical Validation\n: Our experiments reveal that Spar-\nseCBM enables interpretability at the token, subnetwork,\nand concept levels, creating a synergy that surpasses the\nmere aggregation of these elements.\n• Superior Performance\n: SparseCBM demonstrates state-\nof-the-art performance on conventional benchmarks,\nboth in terms of concept and task label predictions.\n• Metacognitive Inference-Time Intervention: Compared\nto vanilla CBMs, SparseCBM exhibits a unique ca-\npability for efficient and interpretable inference-time\nintervention. By subtly modulating internal sparsity,\nSparseCBM learns to sidestep known pitfalls. This\nproperty bolsters user trust in SparseCBMs and, by\nextension, LLMs.\nRelated Work\nInterpreting Language Models\nResearch on the interpretability of language models has been\nrobust, with previous work focusing on visualization of hid-\nden states and attention weights in transformer-based mod-\nels (Vig 2019; Galassi, Lippi, and Torroni 2020). These tech-\nniques, while valuable, often provided granular insights that\nwere not easily interpretable at a high level. Feature im-\nportance methods like LIME (Ribeiro, Singh, and Guestrin\n2016) and SHAP (Lundberg and Lee 2017) provided valu-\nable insights into how each input feature contributes to the\nprediction, but still fail to offer a global understanding of the\nmodel behavior, and often lack intuitiveness and readability.\nThe advent of concept-based interpretability has marked\na significant development, offering more global, high-level\nexplanations (Koh et al. 2020; Abraham et al. 2022; Wang\net al. 2023a). Concept Bottleneck Models (CBMs) (Koh\net al. 2020; Oikarinen et al. 2023) which incorporate a con-\ncept layer into the model, have gained traction recently (Tan\net al. 2023). CBMs are trained with task labels and concept\nlabels either independently, sequentially, or jointly. This de-\nsign enables inference-time debugging by calibrating the ac-\ntivations of concepts. Yet, current CBMs are deficient in\ntheir ability to offer granular interpretations, and inference-\ntime interventions remain incapable of altering the language\nmodel backbone, leading to recurrent errors. On the other\nhand, the interpretability of LLMs remains a less explored\narea. Although some progress has been made, such as guid-\ning LLMs to generate explanations for their predictions us-\ning finely tuned prompts (Li et al. 2022), the reliability of\nthese explanations remains questionable. In summary, a reli-\nable method facilitating holistic insights into model behavior\nis still wanting. In response, our work advances this field by\nintroducing SparseCBM, a holistic interpretation framework\nfor LLMs that tackles both local and global interpretations,\nthus enhancing the usability and trustworthiness of LLMs.\nSparsity Mining for Language Models\nSparsity-driven techniques, often associated with model\npruning, form an energetic subset of research primarily in\nthe pursuit of model compression. At their core, these meth-\nods focus on the elimination of less influential neurons while\nretaining the more critical ones, thereby sustaining optimal\nmodel performance (LeCun, Denker, and Solla 1990a; Han,\nMao, and Dally 2016; Han et al. 2015; LeCun, Denker,\nand Solla 1990b; Liu et al. 2017; He, Zhang, and Sun\n2017; Zhou, Alvarez, and Porikli 2016). Contemporary re-\nsearch has shed light on the heightened robustness of pruned\nmodels against adversarial conditions, such as overfitting\nand distribution shifts. Typical pruning methods for lan-\nguage models encompass structured pruning (Michel, Levy,\nand Neubig 2019), fine-grained structured pruning (Lagu-\nnas et al. 2021), and unstructured pruning (Gale, Elsen, and\nHooker 2019). In brief, unstructured pruning removes in-\ndividual weights in a network, leading to a sparse matrix,\nstructured pruning eliminates entire structures like neurons\nor layers for a dense model, while fine-grained structured\npruning prunes smaller structures like channels or weight\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21620\nvectors, offering a balance between the previous two. We\ndirect the readers to the benchmark (Liu et al. 2023) for a\ncomprehensive overview. In our case, we focus on unstruc-\ntured pruning for its effectiveness and better interpretability.\nRecently, studies have underscored the interpretability af-\nforded by sparse networks (Subramanian et al. 2018). For in-\nstance, Meister et al. (2021) delve into the interpretability of\nsparse attention mechanisms in language models, Liu et al.\n(2022) incorporate sparse contrastive learning in an ancillary\nsparse coding layer to facilitate word-level interpretability,\nand Oikarinen et al. (2023) demonstrate that a sparsity con-\nstraint on the final linear predictor enhances concept-level\ninterpretation of CBMs. Despite their effectiveness, these\nframeworks restrict sparsity to a handful of layers, leading\nto unidimensional interpretability that falls short of the de-\nsired comprehensiveness. In contrast, our proposed frame-\nwork, SparseCBM, imposes sparsity across the entire LLM\nbackbone, enabling holistic interpretation at the token, sub-\nnetwork, and concept levels.\nMethodology\nPreliminary: Concept Bottleneck Models for\nLanguage Models\nProblem Setup. In this study, we aim to interpret\nthe predictions of fine-tuned Large Language Models\n(LLMs) in text classification tasks. Given a dataset D =\n{(x(i), y(i), c(i))N\ni=1}, we consider an LLM fθ that encodes\nan input text x ∈ RD into a latent representation z ∈ RE,\nand a linear classifier gϕ that maps z into the task label y.\nIncorporate Concept Bottlenecks for Large Language\nModels. Our architecture mainly follows Tan et al. (2023).\nInstead of modifying LLM encoders, which could signifi-\ncantly affect the quality of the learned text representation,\nwe introduce a linear layer with sigmoid activation pψ. This\nlayer projects the learned latent representation z ∈ RE into\nthe concept space c ∈ RK, resulting in a pathway rep-\nresented as x → z → c → y. Here, we allow multi-\nclass concepts for more flexible interpretation. For conve-\nnience, we represent CBM-incorporated LLMs as LLM-\nCBMs (e.g., BERT-CBM). LLM-CBMs are trained with two\nobjectives: (1) align concept prediction ˆc = pψ(fθ(x)) to\nx’s ground-truth concept labelsc, and (2) align label predic-\ntion ˆy = gϕ(pψ(fθ(x))) to ground-truth task labels y. We\nmainly experiment with our framework optimized through\nthe joint training strategy for its significantly better perfor-\nmance, as also demonstrated in Tan et al. (2023). Jointly\ntraining LLM with the concept and task labels entails learn-\ning the concept encoder and label predictor via a weighted\nsum, Ljoint, of the two objectives:\nθ∗, ψ∗, ϕ∗ = arg min\nθ,ψ,ϕ\nLjoint(x, c, y)\n= arg min\nθ,ψ,ϕ\n[LCE (gϕ(pψ(fθ(x), y)\n+ γLCE (pψ(fθ(x)), c)].\n(1)\nIt’s worth noting that the LLM-CBMs trained jointly are sen-\nsitive to the loss weight γ. We set the default value for γ as\n5.0 for its better performance (Tan et al. 2023). Despite the\npromising progress having been made, present LLM-CBMs\ntypically train all concepts concurrently, leading to inter-\ntwined parameters for concept prediction, making the pro-\ncess less transparent and hampering targeted intervention.\nSparseCBMs\nTo address the aforementioned issue, the goal of this paper is\nto provide a holistic and intelligible decision-making path-\nway for each input text, tracing from tokens through sub-\nnetworks and concepts, ultimately leading to the final task\nlabel. To this end, we introduce SparseCBM, a pioneering\nframework capable of unraveling the intricate LLM archi-\ntectures into a number of concept-specific subnetworks. Our\napproach not only outperforms conventional CBMs in con-\ncept and task label prediction performance but also proffers\nenhanced interpretation concerning neuron activations, for\ninstance, illuminating which weights inside the LLM back-\nbone play pivotal roles in learning specific concepts.\nOur framework starts with decomposing the joint opti-\nmization defined in Eq. (1) according to each concept ck,\nwhich is formulated as follows:\nθ∗, ψ∗, ϕ∗ = {(θ∗\nk)K\nk=1}, {(ψ∗\nk)K\nk=1}, {(ϕ∗\nk)K\nk=1}\n= arg min\nθ,ψ,ϕ\nKX\nk=1\nLjoint(x, ck, y).\n= arg min\nθ,ψ,ϕ\nKX\nk=1\n[LCE (gϕk (pψk (fθ(x), y)\n+ γLCE (pψk (fθ(x)), ck)],\n(2)\nwhere ϕk, ψk are the weights of the kth parameter of the\nprojector and classifier, and θk is the subnetwork specific\nfor the concept ck, which is explained later. Since both of\nthem are comprised of a single linear layer (with or with-\nout the activation function), the involved parameters for ck\ncan be directly indexed from these models and are self-\ninterpretable (Koh et al. 2020; Tan et al. 2023).\nThe remaining task is to excavate concept-specific subnet-\nworks for each concept from the vast architecture of Large\nLanguage Models (LLMs). The guiding intuition behind this\nstrategy is to perceive the prediction of concept labels as in-\ndividual classification tasks, ones that should not strain the\nentirety of pretrained LLMs given their colossal reserves of\nknowledge encapsulated in multi-million to multi-billion pa-\nrameters. We propose an unstructured pruning of the LLM\nbackbone for each concept classification task, such that dis-\ntinct pruned subnetworks are accountable for different con-\ncepts while preserving prediction performance.\nHolistic and Intelligible Decision-making Pathways.\nWe leverage unstructured pruning strategies to carve out\nconcept-specific subnetworks within the LLM backbones.\nThe noteworthy edge of such unstructured pruning strategies\nlies in their ability to engender weight masks in accordance\nwith the weight importance. Such masks naturally can offer\nan immediate and clear interpretation. Concretely, we intro-\nduce a 0/1 weight mask Mk for each corresponding subnet-\nwork. Consequently, the weights of each subnetwork can be\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21621\nrepresented as θMk = Mk ⊙θ∗, representing the Hadamard\n(element-wise) product between the LLM weights θ∗ ∈ RL\nand the weight mask Mk ∈ {0,1}L for the concept ck.\nWith well-optimized {(M)K\nk=1}, during inference, the\ndecision-making pathway can be represented as:\nˆy =\nKX\nk=1\nϕ∗\nk·σ(ψ∗\nk ·fθMk\n(x)) =\nKX\nk=1\nϕ∗\nk·σ(ψ∗\nk ·fM⊙\nk θ∗(x)),\n(3)\nwhere σ(·) is the sigmoid activation function of the projec-\ntor. This decision-making pathway defined in Eq. (3) fac-\ntorizes the parameters of the SparseCBM, and can be opti-\nmized through one backward pass of the discomposed joint\nloss defined in Eq. (2) with θ∗\nk = θMk . Importantly, we\nposit that such decision-making pathways can deliver holis-\ntic explanations for the model’s predictions. For instance, by\nscrutinizing the weights in the classifier gϕ and the concept\nactivation post the σ function, we can get a concept-level\nexplanation regarding the importance of different concepts.\nAlso, visualizing each subnetwork mask Mk will furnish\na subnetwork-level comprehension of neuron behavior and\nits importance in acquiring a specific concept and forming\npredictions. Additionally, the study of the gradient of input\ntokens in masked concept-specific subnetworks can provide\nmore accurate token-concept mapping. Notably, our exper-\niments demonstrate that SparseCBMs, in addition to pro-\nviding multi-dimensional interpretations, can match or even\nsurpass their dense counterparts in performance on both con-\ncept and task label prediction. Another unique feature of\nSparseCBMs lies in that, the weight masks {(Mk)K\nk=1} en-\ngendered by unstructured pruning facilitates the process of\nefficient and interpretable Sparsity-based Inference-time In-\ntervention, which is expounded later.\nConcept-Induced Sparsity Mining. Next, we elaborate\non how to compute those sparsity masks, given an optimized\nLLM backbone. A second-order unstructured pruning (Has-\nsibi and Stork 1992; Kurtic et al. 2022) for LLMs has been\nincorporated. Initially, the joint lossL (we omit the subscript\njoint for brevity in subsequent equations) can be expanded\nat the weights of subnetwork θMk via Taylor expansion:\nL(θMk ) ≃ L(θ∗) + (θMk − θ∗)⊤∇L(θ∗)\n+ 1\n2(θMk − θ∗)⊤HL(θ∗)(θMk − θ∗),\n(4)\nwhere HL(θ∗) stands for the Hessian matrix of the decom-\nposed joint loss at θ∗. Since θ∗ is well-optimized, we as-\nsume ∇L(θ∗) ≈ 0 as the common practice (Hassibi and\nStork 1992; Kurtic et al. 2022). Then, the change in loss af-\nter pruning can be represented as:\n∆L(∆θ) = L(θMk ) − L(θ∗) ≃ 1\n2∆θ⊤HL∆θ, (5)\nwhere, ∆θ = θMk − θ∗ signifies the change in LLM\nweights, that is, pruned parameters. Given a target sparsity\ns ∈ [0, 1), we seek the minimum loss change incurred by\npruning. In our case, the default sparsity is designed as:\ns ≥ 1 − 1\nK , implying each subnetwork contains a maximum\nof 1\nK parameters in the dense counterpart. Ideally, we desire\nseparate parameters in the LLM backbone to ensure optimal\ninterpretability. Then, the problem of computing the sparsity\nmasks can be formulated as a constrained optimization task:\nmin\n∆θ\n1\n2∆θ⊤HL(θ∗)∆θ,\ns.t. e⊤\nb ∆θ + θb = 0, ∀b ∈ Q,\n(6)\nwhere eb denotes the bth canonical basis vector of the block\nof weights Q to be pruned. This optimization can be solved\nby approximating the Hessian atθ∗ via the dampened empir-\nical Fisher information matrix (Hassibi and Stork 1992; Kur-\ntic et al. 2022). Hence, we can derive the optimized concept-\nspecific masks {(Mk)K\nk=1}. More details are in Appendix C.\nSparsity-based Inference-time intervention. Spar-\nseCBMs also exhibit the capability to allow inference-time\nconcept intervention (a trait inherited from CBMs), thus en-\nabling more comprehensive and user-friendly interactions.\nSparseCBMs allow modulation of the inferred concept\nactivations: ˆa = σ(pϕ(fθ(x))). There are two straightfor-\nward strategies for undertaking such intervention. The first\noption is the oracle intervention (Koh et al. 2020), where\nhuman experts manually calibrate the concept activations\nˆa and feed them into the classifier. Despite its apparent\nsimplicity, oracle intervention directly operates on concept\nactivations and, therefore, cannot fix the flawed mapping\nlearned by the LLM backbone. As a consequence, the\nmodel will replicate the same error when presented with the\nsame input. Meanwhile, another strategy involves further\nfine-tuning the LLM backbone on the test data. However,\nthis approach is not only inefficient but also has a high risk\nof leading to significant overfitting on the test data. Those\nlimitations present a barrier to the practical implementation\nof CBMs in high-stakes or time-sensitive applications.\nAs a remedy, we further propose a sparsity-based inter-\nvention that is self-interpretable and congruent with Spar-\nseCBMs. It helps LLMs to learn from each erroneously\npredicted concept during inference time, while preserving\noverall performance. The core idea is to subtly modify the\nconcept-specific masks for the LLM backbone when a mis-\npredicted concept is detected. Specifically, parameters of the\nLLM backbone fθ, projector pψ, and the classifier gϕ are\nfrozen, while the concept-specific masks{(Mk)K\nk=1} is kept\ntrainable. During the test phase, if a concept predictionˆck for\nan input text x is incorrect, we acquire the gradient Gk(x)\nfor the corresponding subnetwork fθMk\n, and modulate the\nlearned mask Mk accordingly.\nInspired by Evci et al. (2020); Sun et al. (2023), we de-\nfine the saliency scores for LLM parameters by the l2-norm\nof the product of the gradient of the mask and the parame-\nter weights: S = ∥Gk(x) · θ∗∥. Subsequently, we perform\nthe following two operations based on the saliency scores:\n(1) Drop a proportion of r unpruned weights with the low-\nest saliency scores:arg minr·|θ|\nm Sm, ∀m ∈ |θMk |. (2) Grow\na proportion of r pruned weights with the highest saliency\nscores: arg maxr·|θ|\nm Sm, ∀m ∈ |θ \\ θMk |. Here m refers to\nthe parameter index of the LLM backbone. By dropping and\ngrowing an equal number of parameters, the overall spar-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21622\nDataset CEBaB (5-way classification)\nTrain/Dev/Test 1755 / 1673 / 1685\nConcept\nConcept Negative Positive Unknown\nFood 1693 (33%) 2087 (41%) 1333 (26%)\nAmbiance 787 (15%) 994 (20%) 3332 (65%)\nService 1249 (24%) 1397 (27%) 2467 (49%)\nNoise 645 (13%) 442 (9%) 4026 (78%)\nDataset IMDB-C (2-way classification)\nTrain/Dev/Test 100 / 50 / 50\nConcept\nConcept Negative Positive Unknown\nActing 76 (38%) 66 (33%) 58 (29%)\nStoryline 80 (40%) 77 (38%) 43 (22%)\nEmotion 74 (37%) 73 (36%) 53 (27%)\nCinematography 118 (59%) 43 (22%) 39 (19%)\nTable 1: Statistics of experimented datasets and concepts.\nsity s of the LLM backbone remains unchanged. This mask-\nlevel intervention is further optimized through the decom-\nposed joint loss Ljoint defined in Eq. (2). Note that r is set\nas a relatively small value (e.g., 0.01) to compel the model to\nretain the overall performance while learning from the mis-\ntake. Our experiments validate that the proposed sparsity-\nbased intervention can effectively enhance inference-time\naccuracy without necessitating training of the entire LLM\nbackbone. Also, the intervened parameters provide insight\ninto the parameters that contributed to each misprediction.\nExperiments\nExperimental Setup\nDatasets. Our experiments are conducted on two widely-\nused real-world datasets: CEBaB (Abraham et al. 2022)\nand IMDB-C (Tan et al. 2023). Each of them is a text-\nclassification dataset comprised of human-annotated con-\ncept and task labels. Their statistics are presented in Table 1.\nLLM backbones. In this research, we primarily consider\ntwo widely-recognized, open-source lineages of pretrained\nLLMs: the BERT-family models (Devlin et al. 2018; Liu\net al. 2019; Sanh et al. 2019) and OPT-family models (Zhang\net al. 2022). Specially, we also include directly prompt-\ning GPT4 (OpenAI 2023) as a baseline to let it generate\nconcept and task labels for given texts. Even though be-\ning proprietary, GPT4 is widely regarded as the most capa-\nble LLM currently, so we choose it as the baseline back-\nbone. For better performance, we obtain the representa-\ntions of the input texts by pooling the embedding of all to-\nkens. Reported scores are the averages of three independent\nruns. Our work is based on general text classification im-\nplementations. The PyTorch Implementation is available at\nhttps://github.com/Zhen-Tan-dmml/SparseCBM.git.\nInterpretability\nUtility v.s. Interpretability. Table 2 presents the perfor-\nmance of the concept and task label prediction:\nBackbone Acc. / F1 CEBaB IMDB-C\nConcept Task Concept Task\nGPT4 Prompt 75.9/71.5 51.3/45.9 64.5/61.5 71.4/68.7\nDistilBERT\nStandard - 70.3/80.4 - 77.1/73.8\nCBM 81.1/83.5 63.9/76.5 67.5/63.8 76.5/69.8\nSparseCBM 82.0/84.0 64.7/77.1 68.4/64.3 76.9/71.4\nBERT\nStandard - 67.9/79.8 - 78.3/72.1\nCBM 83.2/85.3 66.9/78.1 68.2/62.8 77.3/70.4\nSparseCBM 83.5/85.6 66.9/79.1 69.8/65.276.5/71.6\nRoBERTa\nStandard - 71.8/81.3 - 82.2/77.3\nCBM 82.6/84.9 70.1/81.3 69.9/68.9 81.4/79.3\nSparseCBM 82.8/85.5 70.3/81.4 70.2/69.7 81.5/79.9\nOPT-125M\nStandard - 70.8/81.4 - 84.3/80.0\nCBM 85.4/87.3 68.9/79.7 68.7/66.5 81.8/78.2\nSparseCBM 86.2/88.0 68.9/79.8 70.0/67.4 82.6/79.9\nOPT-350M\nStandard - 71.6/82.6 - 86.4/83.5\nCBM 87.8/89.4 69.9/80.7 72.6/70.5 84.5/82.4\nSparseCBM 87.3/88.7 68.2/79.8 73.3/71.1 85.0/82.5\nOPT-1.3B\nStandard - 74.7/83.9 - 88.4/83.7\nCBM 90.0/91.5 73.6/82.1 76.8/74.6 85.7/83.3\nSparseCBM 89.9/91.6 73.8/82.676.4/74.7 86.6/83.9\nTable 2: Comparisons of task accuracy and interpretability\nusing CEBaB and IMDB-C datasets with BERT-family and\nOPT-family models as the backbones. Metrics for both task\nand concept labels are Accuracy/Macro F1 in %. A score in\nbold indicate that the SparseCBM under the current setting\noutperforms its dense CBM counterpart.\n• Multidimensional Interpretability:SparseCBMs stand\nout by offering multidimensional interpretability without\ncompromising task prediction performance. In compar-\nison with standard LLMs (which are fine-tuned exclu-\nsively with task labels), SparseCBMs grant concept-level\ninterpretability with only a slight dip in task prediction\naccuracy. Impressively, SparseCBMs can match or even\noutperform their dense CBM counterparts while provid-\ning multifaceted explanations that extend beyond mere\nconcepts. This underlines the potency of SparseCBMs in\nstriking a balance between interpretability and utility.\n• Scalability with Larger LLM Backbones:The utiliza-\ntion of larger LLMs within SparseCBMs leads to supe-\nrior interpretability-utility Pareto fronts. This observation\nvalidates our guiding hypothesis that predicting concept\nlabels should not strain the entirety of pretrained LLMs\nas they are individual classification tasks. Larger LLMs,\nbeing repositories of more knowledge through increased\nparameters, facilitate easier pruning.\n• Limitations of Direct Prompting: When directly\nprompting LLMs, such as GPT4 (without fine-tuning on\nthe target datasets), to predict concept and task labels, the\nresulting performance is noticeably inferior. This high-\nlights the necessity of learning concepts and task labels\nin target domains. Additionally, since LLMs’ task pre-\ndictions are autoregressively generated and do not rely\nentirely on the generated concepts, doubts arise regard-\ning the reliability of concept-level explanations.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21623\nExplainable Prediction Pathways. The centerpiece of\nthis paper revolves around providing a transparent and in-\nterpretive decision-making pathway for each input text vec-\ntor x = [t 1, t2, ··· , td, ··· , tD], where td, ∀d ∈ D de-\nnotes the tokens in the input text. SparseCBMs, at inference\ntime, unravel the following layers of understanding across\nthe decision-making trajectory:\n1. Subnetwork-Level Explanation:Identification of spe-\ncific neurons within the LLM backbones responsible for\ncorresponding concepts. This insight is achieved by vi-\nsualizing individual binary subnetwork masks Mk.\n2. Token-Level Explanation:Detection of the tokens in-\nstrumental in shaping a particular concept. This analysis\nis carried out by evaluating the gradient of each subnet-\nwork mask with respect to individual tokens ∥Gk(td)∥.\n3. Concept-Level Explanation: Understanding the pre-\ndicted concept labels ˆck and their contribution to the final\nprediction. This is captured by computing the dot product\nbetween each predicted concept activation and the corre-\nsponding weight of the linear predictor: ϕk · ˆak.\nA schematic representation of the decision-making process\nfor a representative example is provided in Figure 2, with\n“Neg Concept” denoting negative concept values. Addi-\ntional real-world examples are delineated in Appendix D.\nSeveral interesting findings can be drawn from those results:\n• Neural Responsibility Across Concepts:Various con-\ncepts necessitate differing proportions of neurons in the\nLLM backbone for learning. This resonates with our\nambition to demystify the “black-box” LLM backbones\nby partitioning them into distinct subnetworks, each ac-\ncountable for an individual concept. Intriguingly, over-\nlaps exist among subnetworks, reflecting that strict dis-\nentanglement constraints were not imposed on the back-\nbone parameters. This opens avenues for future research\ninto entirely concept-wise disentangled LLM backbones.\n• Holistic Decision Pathway: The SparseCBM frame-\nwork successfully crafts a comprehensive decision-\nmaking path that navigates from tokens, through subnet-\nworks and concepts, culminating in the final task label.\nThis rich interpretability paves the way for unique in-\nsights into practical applications. For instance, although\nFigure 2: The illustration of a decision pathway of a toy ex-\nample from the SparseCBM framework with BERT as the\nbackbone. The binary weight masks for each concept is rep-\nresented as a heatmap.\nconcepts like “Food” and “Ambiance” may carry identi-\ncal positive values, the “Food” concept may wield greater\ninfluence on the final task label. Additionally, careful ex-\namination of parameter masks can shed light on the root\ncauses behind mispredicted concepts, enabling effective\nand interpretable interventions. We explore this topic fur-\nther in the subsequent section.\nInference-time Intervention\nSparseCBMs distinguish themselves by enabling sparsity-\nbased inference-time intervention, compared to vannila\nCBMs. This innovative feature creates a pathway for more\nrefined, user-centric interactions by subtly adjusting the\nmasks without the need for direct retraining of the LLM\nbackbone. The significance of this intervention approach lies\nin its application to real-world scenarios where users often\nfind it easier to articulate broad concepts (e.g., food quality)\nrather than precise sentiment scores or categorical labels.\nExperimental Evaluation. To methodically evaluate this\nintervention strategy, extensive experiments were conducted\non the CEBaB dataset, employing DistilBERT as the repre-\nsentative LLM backbone. The insights gleaned from these\nexperiments apply consistently to other LLMs as well. Fig-\nure 3 provides a detailed comparison between concept and\ntask label predictions using SparseCBMs against a baseline,\nwhere a vanilla DistilBERT is independently trained to clas-\nsify concept or task labels. These baseline scores serve as a\ntheoretical upper bound for prediction accuracy, providing\na reliable and illustrative benchmark. This analytical explo-\nration not only validates the proposed sparsity-based inter-\nvention’s efficacy in enhancing inference-time accuracy for\nboth concept and task predictions but also reveals its ele-\ngance in execution. With minimal alterations to the underly-\ning model structure, remarkable improvements are achieved.\nEven for a relatively small model, DistilBERT, the optimal\nadjustment proportion is found to be a mere 1%, translating\nto modifications in only 2% of the backbone parameters.\nRobustness and Adaptability. These results shed light\non the broader applicability and resilience of sparsity-based\nintervention across various contexts and domains. The ca-\npacity to implement such nuanced adjustments without the\nresource-intensive process of retraining the entire model\n10 3\n 10 2\n 10 1\nThe proportion (r) of the weights.\n84\n85\n86\n87\n88\n89\n90Concept T est F1 (%).\nDistilBERT NI SI\n(a) Concept Prediction\n10 3\n 10 2\n 10 1\nThe proportion (r) of the weights.\n77.0\n77.5\n78.0\n78.5\n79.0\n79.5\n80.0\n80.5T ask T est F1 (%).\nDistilBERT NI SI (b) Task Prediction\nFigure 3: The results of Test-time Intervention. “NI” denotes\n“no intervention”, “SI” denotes “Sparsity-based Interven-\ntion”. (a) and (b) represent the results for concept and task\nlabel prediction respectively. The x-axis indicates the pro-\nportion (r) of the weights to perform the intervention.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21624\nFigure 4: Illustration of the explainable prediction for a real-\nworld example from the IMDB-C dataset using OPT-350m\nas the backbone. The brown boxes with dash lines indi-\ncate the test-time intervention on corresponding concepts by\nmodulating the corresponding mask.M2 and M′\n2 denote the\nparameter masks for the second concept, “Acting”, before\nand after the intervention, respectively. We visualizeM′\n2 af-\nter seeing all test samples.\nmarks a substantial advancement toward more agile, re-\nsponsive machine learning systems. This adaptability res-\nonates with the growing demand for models that can quickly\nadapt to ever-changing requirements without compromising\non performance or interpretability.\nCase Study and Insights.To provide an in-depth illustra-\ntion, a case study depicting the sparsity-based intervention\nprocess is presented in Figure 4. This visualization eluci-\ndates how the predicted label for the concept “Acting” can be\ntransformed from incorrect “ -” to correct “+”, subsequently\nrefining the final task label. But the insights run deeper:\nby visualizing the parameter masks before (M 2) and after\n(M′\n2) the intervention, we expose the neural mechanics be-\nhind the misprediction and the corrective strategy at the neu-\nron level. This ability to not only correct but also interpret\nthe underlying reasons for prediction errors enhances the\noverall trustworthiness and usability of the model. In con-\njunction with the experimental findings, this case study am-\nplifies our understanding of the potential for sparsity-based\ninterventions, not merely as a method for model fine-tuning,\nbut as a principled approach towards more transparent and\nadaptable AI systems.\nImplication. The integration of sparsity-based inference-\ntime intervention within SparseCBMs represents a conflu-\nence of accuracy, flexibility, and interpretability. Through\ncareful experimentation and insightful case studies, this\nwork lays the groundwork for models that respond dynam-\nically to the needs of users, augmenting human-machine\ncollaboration in complex decision-making processes. It is\na promising step towards building AI models that are not\nonly more effective but also more aligned with the human-\ncentered objectives and ethical considerations of modern\nmachine learning applications.\n0.00 0.05\n75.0\n77.5\n80.0\n82.5\n85.0\n87.5\n90.0\n92.5\n0.65 0.70 0.75 0.80 0.85 0.90 0.95\nSparsity of LLM backbone.\nMacro F1 (%).\nDistilBERT BERT RoBERT a OPT-125m OPT-350m OPT-1.3b\nFigure 5: The performance of SparseCBMs across varying\nLLM backbones in relation to the target sparsity s on the\nCEBaB dataset. Solid lines delineate scores for concept label\npredictions. Dashed lines capture those for task label predic-\ntions. Notably, larger LLM backbones are adept at handling\nincreased sparsity without compromising on prediction effi-\ncacy. Nonetheless, excessive pruning invariably impinges on\nthe performance across all LLM backbones.\nSensitivity Analysis on the Sparsitys\nIn Figure 5, we study the effect of target Large Language\nModel (LLM) sparsity on concept and task prediction per-\nformance across various LLM sizes. The results reveal an\ninteresting trend: larger LLMs tend to have a higher optimal\nsparsity level compared to smaller ones. This is attributed\nto the greater knowledge repository and higher redundancy\npresent in larger LLMs, allowing for more extensive prun-\ning without significant performance loss. However, a deli-\ncate balance must be struck. While larger LLMs can accom-\nmodate more pruning, overdoing it may harm performance.\nIdentifying this balance remains an intriguing avenue for fu-\nture research, as well as investigating how different pruning\nstrategies interact with various tasks and data distributions.\nConclusion\nIn this study, we introduced Sparse Concept Bottleneck\nModels (SparseCBMs), a novel method integrating the in-\nterpretability of Concept Bottleneck Models (CBMs) with\nthe efficiency of unstructured pruning. By exploiting the\nproperties of second-order pruning, we constructed concept-\nspecific sparse subnetworks in a Large Language Model\n(LLM) backbone, thereby providing multidimensional in-\nterpretability while retaining model performance. Addition-\nally, we proposed a sparsity-driven inference-time interven-\ntion mechanism that improves accuracy at inference time,\nwithout the need for expensive fine-tuning LLMs. This inter-\nvention mechanism effectively identifies the parameters that\ncontribute to each misprediction, enhancing interpretabil-\nity further. Through rigorous experiments, we demonstrated\nthat SparseCBMs match the performance of full LLMs\nwhile offering the added benefits of increased interpretabil-\nity. Our work underscores the potential of sparsity in LLMs,\npaving the way for further exploration of this intersection.\nWe envisage future investigations to refine the use of struc-\ntured sparsity, such as group or block sparsity, to further en-\nhance model transparency and efficiency.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21625\nEthical Statement\nThis research explores methods to enhance the interpretabil-\nity and reliability of large language models (LLMs) through\nthe proposed Sparse Concept Bottleneck Models (Spar-\nseCBMs). While the development and application of such\ntechnology have benefits, including improved model under-\nstanding, and more efficient use of computational resources,\nseveral considerations arise that warrant discussion.\nTransparency and Explainability: Though our work aims\nto make models more interpretable, the actual understanding\nof these models can still be quite complex and may be be-\nyond the reach of the general public. Furthermore, the opac-\nity of these models can potentially be exploited, reinforcing\nthe need for ongoing work in model transparency.\nRobustness: As indicated in (Tan et al. 2023; Wang et al.\n2023b), the proposed framework is sensitive to the noisy\nconcept and target task labels, requesting future work in\nmodel robustness. Potential direction include selective learn-\ning (Li et al. 2023b,c), knowledge editting (Wang et al.\n2023d), to name a few.\nEfficiency: It is worthnoteing that, even though the\ninference-time intervention is highly efficient, SparseCBM\nrequire more training time due to the cocnept-specific prun-\ning. Potential way to enhance the training efficiency is\nto share part of the sparsity among concepts, as studied\nin (Wang et al. 2020; Chen et al. 2021).\nLabel Reliance: SparseCBMs, along with other CBM\nvariants, necessitate the annotation of concepts. To reduce\nthis burden, several approaches are promising. These in-\nclude leveraging other LLMs for automated annotation, as\ndiscussed in (Tan et al. 2023; Wang et al. 2023c), employ-\ning data-efficient learning techniques (Tan et al. 2022), and\nexploring the acquisition of implicit concepts through dic-\ntionary learning methods (Wang et al. 2022).\nMisuse: Advanced AI models like LLMs can be repur-\nposed for harmful uses, including disinformation campaigns\nor privacy infringement (Jiang et al. 2023; Chen and Shu\n2023). It’s crucial to implement strong ethical guidelines and\nsecurity measures to prevent misuse.\nAutomation and Employment: The advancements in AI\nand machine learning could lead to increased automation\nand potential job displacement. We must consider the soci-\netal implications of this technology and work towards strate-\ngies to manage potential employment shifts.\nData Bias: If the training data contains biases, LLMs may\namplify these biases and result in unfair outcomes. We need\nto continue to develop methods to mitigate these biases in\nAI systems and promote fair and equitable AI use.\nIn conducting this research, we adhered to OpenAI’s use\ncase policy and are committed to furthering responsible and\nethical AI development. As AI technology advances, contin-\nuous dialogue on these topics will be needed to manage the\npotential impacts and ensure the technology is used for the\nbetterment of all.\nAcknowledgments\nThis work is supported by the National Science Foundation\n(NSF) under grants IIS-2229461.\nReferences\nAbraham, E. D.; D’Oosterlinck, K.; Feder, A.; Gat, Y .;\nGeiger, A.; Potts, C.; Reichart, R.; and Wu, Z. 2022. CE-\nBaB: Estimating the causal effects of real-world concepts\non NLP model behavior. NeurIPS, 35: 17582–17596.\nChen, C.; and Shu, K. 2023. Combating misinforma-\ntion in the age of llms: Opportunities and challenges.\narXiv:2311.05656.\nChen, T.; Zhang, Z.; Liu, S.; Chang, S.; and Wang, Z. 2021.\nLong live the lottery: The existence of winning tickets in\nlifelong learning. In ICLR.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv:1810.04805.\nEvci, U.; Gale, T.; Menick, J.; Castro, P. S.; and Elsen, E.\n2020. Rigging the lottery: Making all tickets winners. In\nICML, 2943–2952.\nGalassi, A.; Lippi, M.; and Torroni, P. 2020. Attention in\nnatural language processing. IEEE transactions on neural\nnetworks and learning systems, 32(10): 4291–4308.\nGale, T.; Elsen, E.; and Hooker, S. 2019. The state of spar-\nsity in deep neural networks. arXiv:1902.09574.\nHan, S.; Mao, H.; and Dally, W. J. 2016. Deep compression:\nCompressing deep neural networks with pruning, trained\nquantization and huffman coding. In ICLR.\nHan, S.; Pool, J.; Tran, J.; and Dally, W. 2015. Learning both\nWeights and Connections for Efficient Neural Network. In\nNeurIPS, 1135–1143.\nHassibi, B.; and Stork, D. 1992. Second order derivatives\nfor network pruning: Optimal brain surgeon. NeurIPS, 5.\nHe, Y .; Zhang, X.; and Sun, J. 2017. Channel pruning for\naccelerating very deep neural networks. In Proceedings of\nICCV.\nJiang, B.; Tan, Z.; Nirmal, A.; and Liu, H. 2023. Disinforma-\ntion Detection: An Evolving Challenge in the Age of LLMs.\narXiv:2309.15847.\nKoh, P. W.; Nguyen, T.; Tang, Y . S.; Mussmann, S.; Pierson,\nE.; Kim, B.; and Liang, P. 2020. Concept bottleneck models.\nIn ICML, 5338–5348.\nKurtic, E.; Campos, D.; Nguyen, T.; Frantar, E.; Kurtz, M.;\nFineran, B.; Goin, M.; and Alistarh, D. 2022. The Opti-\nmal BERT Surgeon: Scalable and Accurate Second-Order\nPruning for Large Language Models. In Proceedings of\nthe 2022 Conference on Empirical Methods in Natural Lan-\nguage Processing, 4163–4181.\nLagunas, F.; Charlaix, E.; Sanh, V .; and Rush, A. M. 2021.\nBlock pruning for faster transformers. arXiv:2109.04838.\nLeCun, Y .; Denker, J. S.; and Solla, S. A. 1990a. Optimal\nbrain damage. In NeurIPS, 598–605.\nLeCun, Y .; Denker, J. S.; and Solla, S. A. 1990b. Optimal\nBrain Damage. In Touretzky, D. S., ed., NeurIPS, 598–605.\nMorgan-Kaufmann.\nLi, K.; Patel, O.; Vi´egas, F.; Pfister, H.; and Wattenberg, M.\n2023a. Inference-Time Intervention: Eliciting Truthful An-\nswers from a Language Model. arXiv:2306.03341.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21626\nLi, S.; Chen, J.; Shen, Y .; Chen, Z.; Zhang, X.; Li, Z.; Wang,\nH.; Qian, J.; Peng, B.; Mao, Y .; et al. 2022. Explanations\nfrom large language models make small reasoners better.\narXiv:2210.06726.\nLi, Y .; Han, H.; Shan, S.; and Chen, X. 2023b. DISC: Learn-\ning from Noisy Labels via Dynamic Instance-Specific Se-\nlection and Correction. In Proceedings of the IEEE/CVF\nConference on CVPR, 24070–24079.\nLi, Y .; Tan, Z.; Shu, K.; Cao, Z.; Kong, Y .; and Liu, H.\n2023c. CSGNN: Conquering Noisy Node labels via Dy-\nnamic Class-wise Selection. arXiv:2311.11473.\nLiu, J.; Lin, Y .; Jiang, L.; Liu, J.; Wen, Z.; and Peng, X.\n2022. Improve Interpretability of Neural Networks via\nSparse Contrastive Coding. In Findings of the Association\nfor Computational Linguistics: EMNLP 2022, 460–470.\nLiu, S.; Chen, T.; Zhang, Z.; Chen, X.; Huang, T.; Jaiswal,\nA.; and Wang, Z. 2023. Sparsity May Cry: Let Us Fail (Cur-\nrent) Sparse Neural Networks Together! arXiv:2303.02141.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. Roberta: A robustly optimized bert pretraining ap-\nproach. arXiv:1907.11692.\nLiu, Z.; Li, J.; Shen, Z.; Huang, G.; Yan, S.; and Zhang,\nC. 2017. Learning efficient convolutional networks through\nnetwork slimming. In Proceedings of ICCV, 2736–2744.\nLosch, M.; Fritz, M.; and Schiele, B. 2019. Interpretability\nbeyond classification output: Semantic bottleneck networks.\narXiv:1907.10882.\nLundberg, S. M.; and Lee, S.-I. 2017. A unified approach to\ninterpreting model predictions. NeurIPS, 30.\nMeister, C.; Lazov, S.; Augenstein, I.; and Cotterell, R.\n2021. Is Sparse Attention more Interpretable? In Proceed-\nings of the 59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International Joint Con-\nference on Natural Language Processing (Volume 2: Short\nPapers), 122–129.\nMichel, P.; Levy, O.; and Neubig, G. 2019. Are sixteen heads\nreally better than one? NeurIPS, 32.\nMishra, S.; Sturm, B. L.; and Dixon, S. 2017. Local in-\nterpretable model-agnostic explanations for music content\nanalysis. In ISMIR, volume 53, 537–543.\nOikarinen, T.; Das, S.; Nguyen, L. M.; and Weng, T.-W.\n2023. Label-free Concept Bottleneck Models. In ICLR.\nOpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.\nRibeiro, M. T.; Singh, S.; and Guestrin, C. 2016. ” Why\nshould i trust you?” Explaining the predictions of any classi-\nfier. In Proceedings of the 22nd ACM SIGKDD Conference,\n1135–1144.\nRoss, A.; Marasovi´c, A.; and Peters, M. E. 2021. Explaining\nNLP Models via Minimal Contrastive Editing (MiCE). In\nFindings of the Association for Computational Linguistics:\nACL-IJCNLP 2021, 3840–3852.\nSanh, V .; Debut, L.; Chaumond, J.; and Wolf, T. 2019. Dis-\ntilBERT, a distilled version of BERT: smaller, faster, cheaper\nand lighter. arXiv:1910.01108.\nSubramanian, A.; Pruthi, D.; Jhamtani, H.; Berg-\nKirkpatrick, T.; and Hovy, E. 2018. Spine: Sparse\ninterpretable neural embeddings. In Proceedings of the\nAAAI conference on artificial intelligence, volume 32.\nSun, M.; Liu, Z.; Bair, A.; and Kolter, Z. 2023. A Simple\nand Effective Pruning Approach for Large Language Mod-\nels. arXiv:2306.11695.\nTan, Z.; Cheng, L.; Wang, S.; Bo, Y .; Li, J.; and Liu, H.\n2023. Interpreting Pretrained Language Models via Concept\nBottlenecks. arXiv:2311.05014.\nTan, Z.; Ding, K.; Guo, R.; and Liu, H. 2022. Graph few-shot\nclass-incremental learning. In Proceedings of the Fifteenth\nACM International Conference on WSDM, 987–996.\nVig, J. 2019. A Multiscale Visualization of Attention in\nthe Transformer Model. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics:\nSystem Demonstrations, 37–42.\nWang, H.; Hong, Z.; Zhang, D.; and Wang, H. 2023a.\nIntepreting & Improving Pretrained Language Mod-\nels: A Probabilistic Conceptual Approach. Openre-\nview:id=kwF1ZfHf0W.\nWang, P.; Fan, Z.; Chen, T.; and Wang, Z. 2022. Neural\nimplicit dictionary learning via mixture-of-expert training.\nIn ICML, 22613–22624.\nWang, S.; Tan, Z.; Guo, R.; and Li, J. 2023b. Noise-Robust\nFine-Tuning of Pretrained Language Models via External\nGuidance. arXiv:2311.01108.\nWang, S.; Tan, Z.; Liu, H.; and Li, J. 2023c. Contrastive\nMeta-Learning for Few-shot Node Classification. In Pro-\nceedings of the 29th ACM SIGKDD Conference, 2386–2397.\nWang, S.; Zhu, Y .; Liu, H.; Zheng, Z.; Chen, C.; et al. 2023d.\nKnowledge Editing for Large Language Models: A Survey.\narXiv:2310.16218.\nWang, Z.; Jian, T.; Chowdhury, K.; Wang, Y .; Dy, J.; and\nIoannidis, S. 2020. Learn-prune-share for lifelong learn-\ning. In 2020 IEEE International Conference on Data Mining\n(ICDM), 641–650. IEEE.\nWu, T.; Ribeiro, M. T.; Heer, J.; and Weld, D. 2021.\nPolyjuice: Generating Counterfactuals for Explaining, Eval-\nuating, and Improving Models. In Joint Conference of the\n59th Annual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Conference on\nNatural Language Processing (ACL-IJCNLP 2021).\nZhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;\nChen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V .; et al.\n2022. Opt: Open pre-trained transformer language models.\narXiv:2205.01068.\nZhou, H.; Alvarez, J. M.; and Porikli, F. 2016. Less is more:\nTowards compact cnns. In ECCV, 662–677. Springer.\nZhou, Y .; Muresanu, A. I.; Han, Z.; Paster, K.; Pitis, S.;\nChan, H.; and Ba, J. 2022. Large Language Models are\nHuman-Level Prompt Engineers. In ICLR.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21627",
  "topic": "Inference",
  "concepts": [
    {
      "name": "Inference",
      "score": 0.7354726791381836
    },
    {
      "name": "Intervention (counseling)",
      "score": 0.5967817902565002
    },
    {
      "name": "Psychology",
      "score": 0.423002690076828
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3431072235107422
    },
    {
      "name": "Computer science",
      "score": 0.2989358901977539
    },
    {
      "name": "Psychiatry",
      "score": 0.1033831238746643
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I55732556",
      "name": "Arizona State University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I114027177",
      "name": "University of North Carolina at Chapel Hill",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I86519309",
      "name": "The University of Texas at Austin",
      "country": "US"
    }
  ],
  "cited_by": 10
}