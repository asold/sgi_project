{
  "title": "Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators",
  "url": "https://openalex.org/W4389524463",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1908610349",
      "name": "Chen Liang",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2164798556",
      "name": "Deng, Yang",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A4221439924",
      "name": "Bian, Yatao",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2355365909",
      "name": "Qin, Zeyu",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4224469617",
      "name": "Wu, Bingzhe",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A3005059581",
      "name": "Chua, Tat-Seng",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2087743882",
      "name": "Wong, Kam-Fai",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3153947101",
    "https://openalex.org/W6631349028",
    "https://openalex.org/W2963961878",
    "https://openalex.org/W4384641500",
    "https://openalex.org/W4389519598",
    "https://openalex.org/W3186138538",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4281679115",
    "https://openalex.org/W4385572598",
    "https://openalex.org/W4285239949",
    "https://openalex.org/W4385571659",
    "https://openalex.org/W4322718421",
    "https://openalex.org/W4385570355",
    "https://openalex.org/W3106234277",
    "https://openalex.org/W3206704670",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4389518954",
    "https://openalex.org/W2898875342",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W4224947967",
    "https://openalex.org/W4375870261",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W3099911888",
    "https://openalex.org/W2982596739",
    "https://openalex.org/W4366735603",
    "https://openalex.org/W4310560155",
    "https://openalex.org/W4386721614",
    "https://openalex.org/W3155807546",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W3170432046",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4389518784",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W3170180819",
    "https://openalex.org/W2804363453",
    "https://openalex.org/W3099977667",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W4389520185",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W3217305727",
    "https://openalex.org/W4385573484",
    "https://openalex.org/W3034188538",
    "https://openalex.org/W4378509182",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3169283738",
    "https://openalex.org/W2030346400",
    "https://openalex.org/W4327810286",
    "https://openalex.org/W3034383590"
  ],
  "abstract": "Large language models (LLMs) outperform information retrieval techniques for downstream knowledge-intensive tasks when being prompted to generate world knowledge. Yet, community concerns abound regarding the factuality and potential implications of using this uncensored knowledge. In light of this, we introduce CONNER, a COmpreheNsive kNowledge Evaluation fRamework, designed to systematically and automatically evaluate generated knowledge from six important perspectives - Factuality, Relevance, Coherence, Informativeness, Helpfulness and Validity. We conduct an extensive empirical analysis of the generated knowledge from three different types of LLMs on two widely-studied knowledge-intensive tasks, i.e., open-domain question answering and knowledge-grounded dialogue. Surprisingly, our study reveals that the factuality of generated knowledge, even if lower, does not significantly hinder downstream tasks. Instead, the relevance and coherence of the outputs are more important than small factual mistakes. Further, we show how to use CONNER to improve knowledge-intensive tasks by designing two strategies: Prompt Engineering and Knowledge Selection. Our evaluation code and LLM-generated knowledge with human annotations will be released to facilitate future research. © 2023 Association for Computational Linguistics.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6325–6341\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nBeyond Factuality: A Comprehensive Evaluation of\nLarge Language Models as Knowledge Generators\nLiang Chen1 †, Yang Deng3, Yatao Bian2‡, Zeyu Qin4,\nBingzhe Wu2, Tat-Seng Chua3, Kam-Fai Wong1‡\n1The Chinese University of Hong Kong, 2Tencent AI Lab\n3National University of Singapore, 4The Hong Kong University of Science and Technology\nlchen@se.cuhk.edu.hk\nAbstract\nLarge language models (LLMs) outperform\ninformation retrieval techniques for down-\nstream knowledge-intensive tasks when being\nprompted to generate world knowledge. Yet,\ncommunity concerns abound regarding the fac-\ntuality and potential implications of using this\nuncensored knowledge. In light of this, we in-\ntroduce CONNER, a COmpreheNsive kNowledge\nEvaluation fRamework, designed to system-\natically and automatically evaluate generated\nknowledge from six important perspectives –\nFactuality, Relevance, Coherence, Informative-\nness, Helpfulness and Validity. We conduct an\nextensive empirical analysis of the generated\nknowledge from three different types of LLMs\non two widely-studied knowledge-intensive\ntasks, i.e., open-domain question answering\nand knowledge-grounded dialogue. Surpris-\ningly, our study reveals that the factuality of\ngenerated knowledge, even if lower, does not\nsignificantly hinder downstream tasks. Instead,\nthe relevance and coherence of the outputs are\nmore important than small factual mistakes.\nFurther, we show how to use CONNER to im-\nprove knowledge-intensive tasks by designing\ntwo strategies: Prompt Engineering and Knowl-\nedge Selection. Our evaluation code and LLM-\ngenerated knowledge with human annotations\nwill be released1 to facilitate future research.\n1 Introduction\nThe exceptional success of large language models\n(LLMs) like ChatGPT and GPT4 (Ouyang et al.,\n2022; OpenAI, 2023) has fueled a growing interest\nin substituting traditional models with LLMs to\nattain superior performance across various NLP\ntasks (Liu et al., 2023b; Jagerman et al., 2023;\nWang et al., 2023). In open-domain question an-\nswering (QA) and knowledge-grounded dialogue,\n†Partial work was done in his Tencent AI Lab internship.\n‡Corresponding author.\n1https://github.com/ChanLiang/CONNER\nFigure 1: The CONNER Framework: Intrinsic evaluations\nprobe the internal properties of acquired knowledge,\nwhile extrinsic evaluations assess its downstream im-\npacts. This framework applies universally to two-stage\nprocesses in knowledge-intensive tasks.\nLLMs have demonstrated superior performance\nthan information retrieval (IR) models (Karpukhin\net al., 2020) when it comes to generating world\nknowledge (Yu et al., 2023; Liu et al., 2022) for the\ndownstream tasks. However, the knowledge gener-\nated may contain inherent issues, such as false state-\nments or off-topic information. Therefore, the lack\nof extensive evaluation of this knowledge raises\nconcerns about its use in downstream tasks.\nTo this end, four lines of research emerge.\nFirstly, human evaluations are conducted to as-\nsess the generated knowledge from diverse per-\nspectives (Li et al., 2022; Yu et al., 2023; Liu et al.,\n2023a). However, their time-consuming nature\nand subjectivity often encounter issues of scalabil-\nity and reproducibility. Secondly, datasets have\nbeen constructed to evaluate open-domain gener-\nation with the aid of references (Honovich et al.,\n2021; Glover et al., 2022a; Lee et al., 2023; Li\net al., 2023). These methods, while more objec-\ntive, are limited by their dependence on human-\nlabelled references, impacting their real-world ap-\nplicability and generalizability to dynamically gen-\nerated content. Thirdly, self-evaluation methods\n(Kadavath et al., 2022b; Manakul et al., 2023) esti-\n6325\nEvaluation Taxonomy Definition\nIntrinsic\nFactuality whether the information in the knowledge can be verified by external evidence.\nRelevance whether the knowledge is relevant to the user query.\nCoherence whether the knowledge is coherent at the sentence and paragraph levels.\nInformativeness whether the knowledge is new or unexpected against the model’s existing knowledge.\nExtrinsic Helpfulness whether the knowledge can improve the downstream tasks.\nValidity whether the results of downstream tasks using the knowledge are factually accurate.\nTable 1: Taxonomy of evaluation metrics of acquired knowledge.\nmate a model’s uncertainty in its generated content.\nDespite simplicity, they lack interpretability and\nare less effective for long-form answers. Lastly,\ncontemporary studies (Pan et al., 2023; Min et al.,\n2023) apply fact-checking principles to spot factual\ninaccuracies. However, these evaluation methods\nmainly assess a single aspect of the intrinsic quality\nof generated knowledge, overlooking other facets\nand their extrinsic impact on downstream tasks,\nthereby limiting a comprehensive understanding of\nLLM-generated content.\nIn light of these limitations, we propose CONNER,\na COmpreheNsive kNowledge Evaluation fRame-\nwork, as illustrated in Figure 1. CONNER is designed\nto be a reference-free framework that can system-\natically and automatically evaluate the generated\nknowledge from six fine-grained perspectives, in-\ncluding diverse intrinsic evaluation of its internal\nproperties, as well as uniform extrinsic evaluation\nof its impact on specific downstream tasks. The\ntaxonomy of evaluation metrics is presented in Ta-\nble 1. Based on CONNER, we conduct empirical eval-\nuations on three different types of LLMs, including\nLLaMA (Wei et al., 2022) (a base LLM), FLAN-T5\n(Wei et al., 2022) (an instruction-tuned LLM), Chat-\nGPT (Ouyang et al., 2022) (a commercial LLM\ntrained with human feedbacks). We evaluate them\non two widely-studied knowledge-intensive tasks:\nopen-domain QA (Kwiatkowski et al., 2019) and\nknowledge-grounded dialogue (Dinan et al., 2018).\nOur detailed investigations yield several valu-\nable insights about the LLM-generated knowledge:\n1) LLM-generated knowledge surpasses retrieved\nknowledge in most evaluation perspectives, while\nit actually suffers from the factuality issue as ex-\npected. Notably, the factuality of downstream tasks\nis found to be less affected by this issue, when\ncompared to the impact of lower relevancy and\ncoherency observed in the retrieved knowledge (§\n4.3). 2) Several critical factors are identified to in-\nfluence the factuality of the generated knowledge,\nsuch as their frequency and length, while few-shot\nin-context learning and larger size of models do not\nnecessarily guarantee higher quality and reliability\n(§4.4). 3) In addition to assessing and analyzing\nthe generated knowledge from different LLMs, the\nevaluation outcome of CONNER can be exploited to\nenhance knowledge generation and further improve\nthe performance of downstream tasks (§5).\nOur main contributions are as follows:\n• We conduct the first empirical analysis focusing\non both intrinsic quality and extrinsic reliability\nof the generated knowledge from LLMs.\n• We propose CONNER, a COmpreheNsive kNowl-\nedge Evaluation fRamework that enables the au-\ntomatic evaluation of LLMs as knowledge gener-\nators from diverse perspectives, eliminating the\nneed for human-labelled references.\n• The extensive evaluation and analysis yield pro-\nfound insights and valuable practical experience\nfor leveraging LLMs as knowledge generators.\n• We collect a new set of multi-perspective hu-\nman judgments of LLM-generated knowledge\nfor two knowledge-intensive generation datasets.\nWe demonstrate that CONNER aligns well with hu-\nman judgments. The human annotations will be\nreleased to facilitate future research.\n2 Related Work\nKnowledge-intensive tasks rely heavily on ac-\ncess to external knowledge sources, such as open-\ndomain dialogue and QA (Dinan et al., 2018;\nKwiatkowski et al., 2019; Petroni et al., 2021). The\nmain-streamed methods (Karpukhin et al., 2020;\nLewis et al., 2020; Izacard and Grave, 2021) typi-\ncally employ IR techniques to first retrieve the rele-\nvant knowledge from Wikipedia and then produce\nthe answer or response conditioned on the knowl-\nedge. Nowadays, with the powerful capabilities of\nLLMs (OpenAI, 2023; Kadavath et al., 2022a), a\nnew trending approach is to leverage LLMs to di-\nrectly generate the relevant knowledge for a given\nquery and then apply the model-generated knowl-\nedge to complete the downstream tasks (Liu et al.,\n2022; Li et al., 2022; Yu et al., 2023). Despite out-\nperforming retrieval-based methods, these knowl-\n6326\nedge generation techniques lack rigorous evalua-\ntion of their quality and reliability, which may con-\ntain misleading or even plausible false information,\ne.g., hallucination and factual inconsistency.\nThese issues are prevalent across various NLP\ntasks (Ji et al., 2023). However, most studies target\nspecific downstream tasks, such as text summa-\nrization (Maynez et al., 2020; Wang et al., 2020;\nKryscinski et al., 2020a; Pagnoni et al., 2021), di-\nalogue generation (Dziri et al., 2022; Chen et al.,\n2023; Xue et al., 2023; Deng et al., 2023), and\nfact verification (Thorne et al., 2018; Wadden et al.,\n2020; Schuster et al., 2021; Pan et al., 2023). These\ntasks are designed to examine consistency either\nbetween the input and output or between the input\nand a human-labeled reference,e.g., the source doc-\nument and its summary, the grounded knowledge\nand the generated response, or a human-written\nclaim and pre-annotated references.\nThe success of LLMs and generative search en-\ngines (Zhao et al., 2023; Zhu et al., 2023) have\nbrought hallucinations in LLM outputs (Rawte\net al., 2023; Zhang et al., 2023) into focus. Re-\nsearch typically falls into four categories. (Lee\net al., 2023; Li et al., 2023) aim to assess the fac-\ntuality of open-domain generation automatically\nusing specially designed datasets, but their reliance\non references may limit real-world applicability.\nAnother stream of work (Li et al., 2022; Yu et al.,\n2023; Liu et al., 2023a) uses human evaluation to\nmeasure output quality, which is difficult to scale.\nA third approach (Kadavath et al., 2022b; Manakul\net al., 2023) detects hallucinations by examining\nthe model’s uncertainty or confidence, which can\nbe inaccurate for long answers. Lastly, recent stud-\nies (Peng et al., 2023; Min et al., 2023) apply fact-\nchecking principles to spot factual inaccuracies.\nDifferent from previous studies, we propose a\ncomprehensive framework for evaluating knowl-\nedge generated by LLMs. Our goal is to automati-\ncally test the intrinsic quality and extrinsic impact\nof generated information in knowledge-intensive\ntasks, without requiring knowledge labelling or hu-\nman involvement. Through extensive testing with\nthis framework, we aim to deepen and broaden our\nunderstanding of LLM-generated knowledge and\nprovide valuable insights for future research.\n3 The Evaluation Framework\nWe introduce CONNER, a comprehensive and inno-\nvative framework, specifically designed for the rig-\norous evaluation of the quality and dependability\nof knowledge used in knowledge-intensive tasks.\nCONNER is rooted in in-depth error analysis, paving\nthe way for the construction of an evaluation tax-\nonomy, which integrates six unique perspectives\ninto two coherent categories, as delineated in Ta-\nble 1. Capitalizing on the advantages of unsuper-\nvised metrics, our framework eliminates the need\nfor human-labeled reference knowledge and stan-\ndardizes scores within an intuitive range of [0,1],\nsimplifying comparison and interpretation.\nThe subsequent subsections provide a detailed\nexamination of the framework’s design, commenc-\ning with the formulation of knowledge-intensive\ntasks and the identification of associated error pat-\nterns. These insights direct the design of our met-\nrics. Through comprehensive intrinsic and extrinsic\nevaluations, we aim to gain a holistic understanding\nof the LLMs-generated knowledge.\n3.1 Tasks Formulation\nFormally, we define the knowledge-intensive task\nas follows: given a user query q, the goal is to\nproduce an answer with access to knowledge re-\nsources as illustrated in Figure. 1. Specifically,\nthe system first obtains the relevant knowledge k\nthat can help answer the query qfrom knowledge\nresources K, then the reader generates an answer a\nusing the acquired knowledge k. Specifically, the\nknowledge resource Kcan be either a knowledge\nbase for knowledge retrieval or language models\nfor knowledge generation. Detailed formulations\nof these two settings are presented in Appendix A.\n3.2 From Error Patterns to Metrics Design\nTo identify common errors by LLMs in knowledge-\nintensive tasks and create a more targeted evalua-\ntion framework, we used thematic analysis (Braun\nand Clarke, 2012). We began by extracting and\nconsolidating patterns from subtle errors in knowl-\nedge and answers in responses from LLaMA to\n160 samples from NQ (Kwiatkowski et al., 2019)\nand WoW (Dinan et al., 2018) datasets. To ensure\nthe breadth of the error spectrum was adequately\nrepresented, we further substantiated these patterns\nusing additional questions from NQ and WoW. As\na result, we discerned four primary error categories\nin knowledge generation and two in answer genera-\ntion. In response, we devised four intrinsic metrics\nfor knowledge evaluation and two extrinsic metrics\nfor answer evaluation, as outlined in Table 1.\n6327\n3.3 Intrinsic Evaluation\nIntrinsic evaluation refers to the assessment of the\nacquired knowledge based on its internal properties\nand performance, without considering its impact\non downstream tasks or applications. In specific,\nwe implement four model-based metrics for evalu-\nating the acquired knowledge in terms of factuality,\nrelevance, informativeness, and coherence.\nFactuality The core of factuality assessment is\nvalidating the acquired knowledge by external ev-\nidence 2. Given an acquired knowledge k =\n{s1,...,s m}composed of m sentences, we can\nuse a dense retrieval model (Santhanam et al., 2021)\nor search engine API to recall the li most relevant\nevidence Ei = {ei,1,...,e i,li}for each sentencesi\nfrom the expert knowledge base or the internet. Af-\nter collecting all the evidence E = {E1,...,E m},\nthe factuality score is computed as follows:\nSfact(k,E) = min\ni=1..m\nf(si,Ei)\n= min\ni=1..m\nmax\nj=1..li\nNLI(si,ei,j) (1)\nwhere f(·) is a function to compute sentence-level\nfactuality, NLI(·) is a natural language inference\nmodel processing a premise-hypothesis pair to out-\nput a R3 vector, indicating whether a hypothesis\n(si) is entailed by, neutral to or refuted by the\ngiven premise ( ei,j). Following these computa-\ntions, sentence-level results are aggregated along\nthe entailment dimension using one of three oper-\nations: min, mean, or max to match the desired\nerror tolerance level. In this instance, we exem-\nplify the process using min. Finally, we obtain\na three-dimensional factuality score Sfact(k,E).\nFrom each dimension of this vector, we can de-\nrive three fine-grained scores. We denote those\nscores as factual-consistent, non-verified,\nand factual-inconsistent, respectively.\nThis strategy seeks to address the shortcomings\nof traditional factuality metrics (Wang et al., 2020;\nHonovich et al., 2021; Glover et al., 2022a; Lee\net al., 2023) that mainly depend on consistency\nwith human-annotated references. These metrics\noften fail in emerging knowledge generation sce-\nnarios (Table 10), as they struggle with model-\ngenerated content beyond reference knowledge\nscope and face difficulties when references are un-\navailable in real-world applications. Our method\nof evidence collection and results aggregation ef-\nfectively tackles these issues.\n2We empirically demonstrate ground-truth knowledge is\ndispensable for the factuality evaluation in Appendix B.\nRelevance To assess the relevance between a\ngiven query qand the acquired knowledge k, we\ncompute the relevance score as follows:\nSrel(k,q) = Matching(k,q) (2)\nThe Matching(·) function denotes a fine-grained\nmatching model specifically designed for assessing\nthe relevance between the query and knowledge.\nIn our study, we employ the BERT ranking model\n(Nogueira et al., 2019) for this purpose.\nThis methodology addresses the limitations that\narise when traditional relevance metrics are applied\nwithin knowledge generation scenarios. Traditional\nrelevance metrics (Karpukhin et al., 2020; Shuster\net al., 2021; Komeili et al., 2021), which typically\nrely on word overlap or similarity with human-\nwritten references, face two significant challenges.\nFirst, these traditional metrics do not correspond\nwell with scenarios where LLMs serve as genera-\ntive search engines, as evidenced by the unsatisfac-\ntory results in Table 10. Second, the reliance on\nreference knowledge constitutes a substantial chal-\nlenge, especially when such references are scarce\nor absent in real-world applications. Contrarily,\nour BERT ranking model, trained on manually an-\nnotated Bing search data, excels at comparing the\nrelevance of different knowledge to a given query.\nCoherence As the acquired knowledge is typi-\ncally long-form texts composed of multiple sen-\ntences, we propose to measure sentence-level co-\nhesion and paragraph-level coherence: the for-\nmer measures the cohesion of individual sentences,\nand the latter measures the coherence between\nsentences. The sentence-level cohesion score\nScoh_sent(k) is computed as follows:\nScoh_sent(k) = 1\nm\n∑ m\ni=1\n1/PPL(si) (3)\nwhere PPL(·) is computed by a GPT-based model\n(Radford et al., 2019; Black et al., 2021), measuring\nthe perplexity for each sentence.\nOn the other hand, the paragraph-level coherence\nscore is determined by the normalized score of\na discourse coherence model (Jwalapuram et al.,\n2021), denoted as Scoh_para(k):\nScoh_para(k) = Scorerpara(s1,...,s m) (4)\nBy considering both sentence-level cohesion and\nparagraph-level coherence, we gain insights into\nthe overall coherence of the acquired knowledge.\n6328\nInformativeness To assess the informativeness\nof the procured knowledge—defined as the degree\nto which the knowledge is novel or unexpected in\nrelation to the model’s existing knowledge about\nthe query—we calculate the informativeness score\nof the acquired knowledge kgiven qas follows:\nSinfo(k,q) = 1−exp\n(\n1\nM\nM∑\nt=1\nln Pθ(kt|k1:t−1,q)\n)\n(5)\nAssuming the unbiased benchmark model θencap-\nsulates world knowledge from general pretraining\ndata, we thus select the GPT-2 series models.\nTo grasp the expected behaviour of this metric,\nconsider a simple query: \"What is the capital of\nthe United States?\"The knowledge acquired here\nis \"Washington\". In this situation, the model’s aver-\nage probability of generating \"Washington\" is high,\nas it already knows this fact. Consequently, our\ninformativeness score for this knowledge would be\nlow. Conversely, if the acquired knowledge was\n\"Chicago\", the model’s probability of generating it\nwould be low. This knowledge is surprising com-\npared to its existing knowledge, resulting in a high\ninformativeness score. On the other hand, for a\ntough query where the model is clueless, any pro-\nvided knowledge would score high on informative-\nness due to the model’s low output probabilities.\n3.4 Extrinsic Evaluation\nExtrinsic evaluation, in contrast to intrinsic evalu-\nation, focuses on uniformly assessing the perfor-\nmance of the acquired knowledge within the con-\ntext of different downstream tasks. Specifically,\nwe measure how well the acquired knowledge con-\ntributes to the downstream task on two types of\nmetrics (helpfulness and validity). Extrinsic evalua-\ntion provides a more comprehensive understanding\nof the practical value of the acquired knowledge.\nHelpfulness Given a query and answer pair ( q,\na), we assess to what extent the acquired knowl-\nedge kcan help answer the query. As we assume\nno pre-annotated ground-truth knowledge, we use\nirrelevant knowledge as the baseline. Specifically,\nwe randomly sampled uknowledge {k−\n1 ,··· ,k−\nu }\nto reduce the variance of baseline estimation. Then\nthe helpfulness score is computed as follows:\nShelp(q,a,k,k −\n1 ,··· ,k−\nu)\n= max(0,1 − L(q,k,a )\n1\nu\n∑u\ni=1 L(q,k−\ni ,a))\n= max(0,1 − log P(a|q,k)\n1\nu\n∑u\ni=1 log P(a|q,k−\ni ))\n(6)\nwhere L(q,k,a ) and L(q,k−\ni ,a) are cross entropy\nlosses of answer generation using k and k−\ni re-\nspectively. Ideally, the generated knowledge k\ncan provide enough information and reduce the\nL(q,k,a ) to zero, and then the helpfulness score\nequals one. The worst case is the generated\nknowledge is no better than random knowledge\n(L(q,k,a ) ≥ 1\nu\n∑u\ni=1 L(q,k−\ni ,a)), and the help-\nfulness score is naturally zero.\nValidity To measure how the reliability of the ac-\nquired knowledge affects the factuality of the gener-\nated answer aon downstream tasks, we define the\nvalidity metric for two types of downstream tasks:\nspan-based answers (e.g., open-domain QA) and\nopen-ended answers (e.g., knowledge-grounded di-\nalogue). As for span-based answers, the generated\nanswers cannot form a complete sentence for fac-\ntuality measurement. To this end, we concatenate\n(q,a∗) as the premise and (q,a) as the hypothesis\nfor deriving the factual-consistent score of the\nNLI(·) model as the validity score:\nSval(q,a∗,a) = NLIfact((q,a),(q,a∗)) (7)\nwhere a∗ denotes the ground-truth answer for\ndownstream tasks and theNLI(·) model is the same\nas that of Eq. (1).\nWe demonstrate this measure outperforms tra-\nditional metrics like Exact Match and F1 score as\nshown in Table 10, which rely on literal matches,\nand often yield low recall. For instance, an entity\npair like ’PRC’ and ’China’ would receive a zero\nscore due to their differing literal presentations.\nAs for open-ended answers, we collect l evi-\ndence E = {e1,...,e l}and adjust Eq. (1) to be:\nSval(a,E) =f(a,E) = max\ni=1..l\nNLIfact(a,ei) (8)\n4 Evaluation\nIn this section, we will first validate our proposed\nmetrics, and then leverage them to comprehen-\nsively evaluate three different types of LLMs across\ntwo knowledge-intensive tasks, followed by an in-\ndepth analysis of the results.\n4.1 Metrics Efficacy Validation\nTo validate the effectiveness of our proposed met-\nrics, we conducted manual evaluations and com-\npared the results with baseline metrics. Specifi-\ncally, we developed specific annotation guidelines\nfor each metric, detailed in Appendix J, and per-\nformed manual annotations accordingly. These\n6329\nModel Setting Factuality Relevance Coherence Inform. Helpful. ValidityFact-cons. Non-verif. Fact-incon. Coh-sent. Coh-para.\nDPR Supervised 97.78% 2.23% 0.00% 0.7514 0.0301 0.7194 0.8965 0.1236 36.86%\nFLAN-T5 58.40% 27.80% 13.80% 0.6848 0.1249 0.7776 0.6727 0.0000 32.47%\nLLAMA Zero-shot 94.20% 4.80% 1.00% 0.7316 0.1183 0.8240 0.7572 0.2191 42.00%\nCHATGPT 83.63% 13.6% 2.77% 0.8491 0.0909 0.9033 0.7330 0.1461 43.35%\nFLAN-T5 20.75% 62.40% 25.40% 0.6787 0.0416 0.8110 0.6899 0.0000 34.65%\nLLAMA Few-shot 89.00% 9.20% 1.80% 0.6966 0.0776 0.8550 0.8545 0.2528 40.49%\nCHATGPT 86.07% 10.97% 2.96% 0.9205 0.0653 0.8837 0.7700 0.1966 42.36%\nTable 2: Automatic evaluation results of different LLMs in the Natural Question test set. Underlined and Bold\nresults denote the best results among each setting and among all settings, respectively.\nModel Setting Factuality Relevance Coherence Inform. Helpful. ValidityFact-cons. Non-verif. Fact-incon. Coh-sent. Coh-para.\nDPR Supervised 91.96% 5.18% 2.87% 0.0907 0.0223 0.6569 0.9357 0.0000 61.52%\nFLAN-T5 77.90% 17.28% 4.82% 0.3776 0.1203 0.8331 0.7239 0.0904 56.97%\nLLAMA Zero-shot 89.46% 8.89% 1.65% 0.5041 0.0548 0.8389 0.7889 0.1178 63.50%\nCHATGPT 88.51% 10.38% 1.11% 0.5283 0.1028 0.9250 0.7448 0.1023 59.76%\nFLAN-T5 76.50% 17.20% 6.30% 0.4463 0.1523 0.7988 0.6983 0.0934 57.18%\nLLAMA Few-shot 85.07% 12.05% 2.88% 0.3930 0.1088 0.7947 0.7855 0.1132 63.79%\nCHATGPT 85.75% 12.01% 2.24% 0.4618 0.0979 0.8632 0.7922 0.1164 60.27%\nTable 3: Automatic evaluation results of different LLMs in the Wizard of Wikipedia test set.\nannotations allowed us to calculate the correlation\nbetween each metric and human evaluations. Sub-\nsequently, we compared these correlations with\nbaseline metrics (Table 10). Our metrics demon-\nstrated a strong correlation with human evaluations,\nsignificantly outperforming the baseline metrics.\nDetails are presented in Chapter 6 and Appendix J.\n4.2 Experimental Setups\nBaselines Compared with a popular retrieval-\nbased model, DPR (Karpukhin et al., 2020), we\nevaluate knowledge generation with the three differ-\nent types of LLMs, including FLAN-T5 (Wei et al.,\n2022), LLaMA (Touvron et al., 2023), and Chat-\nGPT (Ouyang et al., 2022). By default, we report\nthe results with the largest size of each LLM and\nadopt greedy decoding in our experiments for re-\nproducibility. Details are presented in Appendix C.\nDatasets We evaluate the generated knowledge\non two widely-studied benchmark datasets, includ-\ning 1) Natural Questions (NQ) (Kwiatkowski\net al., 2019), an open-domain QA dataset; and 2)\nWizard of Wikipedia (WoW) (Dinan et al., 2018)\na knowledge-grounded dialogue dataset. During\nexperiments, we randomly sample 500 examples\nfrom the NQ and WoW test sets respectively for\nevaluation. Details are presented in Appendix D.\nImplementation Details All the adopted models\nin CONNER are introduced in Appendix E.\nEvaluation Setting Following (Yu et al., 2023),\nwe evaluate the knowledge generation of LLMs\nunder both zero-shot and few-shot settings. Af-\nter the knowledge acquisition, we perform QA or\ndialogue generation under the few-shot setting to\nfurther investigate the impact of different knowl-\nedge acquisition methods on downstream tasks.\n1) Zero-shot Evaluation: We test with varied\nprompts and report peak performance. A prompt\ncould be “Generate Wikipedia knowledge for the\nquery. {query}”. Prompts tried are in Appendix F.\n2) Few-shot Evaluation: We construct the\nprompt with krandomly chosen samples from the\ntraining set. The example templates used for knowl-\nedge generation are listed in Appendix F and G.\n4.3 Overall Evaluation\nTable 2 and Table 3 summarize the evaluation re-\nsults of DPR and three LLM-based knowledge\ngenerators on NQ and WoW datasets, respectively.\nThere are several notable observations as follows:\nGenerated knowledge exceeds retrieved knowl-\nedge in most evaluation perspectives, except the\nfactuality and informativeness. In both NQ and\nWoW scenarios, LLMs show remarkable capabil-\nities in generating highly relevant and coherent\nknowledge. Moreover, the knowledge generated\nby LLMs also proves to be more beneficial for\ndownstream tasks, regarding both helpfulness and\nvalidity. These results highlight the significant ad-\n6330\nModel Query Acquired Knowledge Answer\nLLaMA Who wrote the song\nthe glory of love?\n\"The Glory of Love\" is a song written by Billy Hill, recorded by\nAmerican singer Benny Goodman in 1936. The song has been covered\nby many other artists, including a version by American singer Peggy\nLee with Bing Crosby and the Johnny Mann Singers in 1959.\nBilly Hill✓\nFLAN-T5 Who wrote the song\nthe glory of love?\nThe Glory of Love is a song written by the American composer and\nlyricist Irving Berlin.\nIrving Berlin✗\nDPR Rain sleet or snow\nthat contains a high\nconcentration of\nacids is called?\n... Ice pellets are a form of precipitation consisting of small, translucent\nballs of ice. This form of precipitation is also referred to as \"\"sleet\"\"\nby the United States National Weather Service. (In British English\n\"\"sleet\"\" refers to a mixture of rain and snow)...\nicy rain✗\nTable 4: Factuality of acquired knowledge may not influence the validity of the answer. Red words represent factual\nerrors in critical information, while blue words represent factual errors in non-critical information.\nModel Extrinsic Instrinsic\nFact. Rel. Coh-sent. Coh-para. Info.\nDPR helpful. 0.10 0.24† 0.07 -0.03 -0.14 †\nvalidity 0.04 0.19† 0.04 -0.06 -0.09\nLLM S helpful. 0.14 -0.05 0.10 -0.09 -0.05\nvalidity 0.15† -0.02 0.07 -0.03 -0.03\nTable 5: The Somers’ correlation between intrinsic and\nextrinsic metrics on NQ. Scores with p-value <0.05\nare marked with †. Bold results denote the most corre-\nlated intrinsic metric to the concerned extrinsic metric.\nThe breakdowns of all correlations are in Appendix H.\nModel Extrinsic Instrinsic\nFact. Rel. Coh-sent. Coh-para. Info.\nDPR helpful. 0.01 0.27† 0.10† -0.03 -0.14 †\nvalidity -0.01 -0.06 0.13† -0.12† -0.13†\nLLM S helpful. 0.06 0.05 0.10 0.00 -0.16\nvalidity 0.24† 0.09 0.05 -0.02 -0.07\nTable 6: The Somers’ correlation between intrinsic and\nextrinsic metrics on WoW.\nvantages of utilizing LLMs as knowledge genera-\ntors in terms of knowledge quality and applicability,\nrendering them a valuable knowledge resource for\nvarious knowledge-intensive applications.\nDespite obtaining lower factuality than retrieved\nknowledge, generated knowledge contributes\nmore to the factuality of downstream tasks (i.e.,\nhigher validity). To investigate the underlying\nreason, we analyze the correlation between differ-\nent intrinsic metrics and extrinsic metrics on two\ntasks. As shown in Tables 5 and 6, the perfor-\nmance of downstream tasks is indeed hindered by\nthe issue of factuality in the generated knowledge\nfrom LLMs. However, for retrieval models ( e.g.,\nDPR), limitations may arise from the relevance\nand coherence of the retrieved knowledge, while\nits high factuality fails to ensure the performance\nof downstream tasks. We present a case study in Ta-\nble 4, which intuitively shows that the presence of\nfactual errors in non-critical information has mini-\nmal impact on downstream tasks, while it is highly\nimpossible to derive the correct answer from the\nirrelevant retrieved knowledge. While LLaMA and\nChatGPT generate knowledge with slightly lower\nfactuality than DPR, it is shown to be adequate for\ndownstream tasks. At this point, the relevance of\nthe acquired knowledge is more critical. Hence,\nrelying solely on the factuality of the knowledge\nitself is an unreliable means of assessing its impact\non the factuality of downstream tasks. Motivated\nby this finding, we investigate approaches to guid-\ning the generated knowledge selection with the\nmulti-perspective evaluation outcome of CONNER\nfor improving the downstream performance in §5.\nDPR falls short of retrieving relevant and help-\nful knowledge for knowledge-grounded dia-\nlogues. As the DPR model is finetuned on QA\ndatasets to match a question to Wikipedia knowl-\nedge, the DPR model struggles to match dialogue\nutterances with the necessary knowledge. Also,\nthe candidate Wikipedia passages in DPR (100 to-\nkens) are much longer than the knowledge needed\nin WoW, containing much redundant information.\nThis reveals the shortcomings of supervised dense\nretrieval models, such as limited transferability and\nbeing constrained by knowledge bases.\nFew-shot in-context learning for LLMs gener-\nally harms the factuality of generated knowl-\nedge. We observe that the length of knowledge\ngenerated by few-shot ICL is generally longer than\nthat of zero-shot prompting since the ground-truth\nknowledge for demonstrations is relatively long.\nConsequently, LLM is more error-prone (see the\nanalysis of long-form generation in §4.4). This in-\ndicates that few-shot ICL is not always better than\nzero-shot ICL in knowledge generation, and the\nselected demonstrations attach great importance.\n6331\n4 5 6 7 8 9\nPageview of Wikipedia knowledge (10^)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Avg probability\n(a) Analysis of Long-tail Knowledge\nFactual-con.\nNon-verified\nFactual-incon.\n1 2 3 4 5\n# of sentences in generated knowledge\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n(b) Analysis of Long-form Generation\nFigure 2: The impact of knowledge frequency and\nlength on the factuality of the generated knowledge.\nInspired by this, we investigate approaches to guid-\ning the few-shot demonstration selection with the\nevaluation outcome of CONNER for improving the\nperformance of few-shot ICL in §5.\nFLAN-T5 fails to be a qualified knowledge gen-\nerator since its generated knowledge is poorly\nfactual and rarely helpfulto downstream tasks.\nAlthough FLAN-T5 (11B) significantly surpasses\nmany models of the same scale through instruction\ntuning on numerous tasks, it falls short of being\na qualified knowledge generator. As shown in Ta-\nble 4, such a low factuality leads to frequent oc-\ncurrences of factual errors in critical information,\nthereby harming downstream tasks. To this end,\nwe study the scaling of performance w.r.t different\nperspectives by varying the model size in §4.4.\n4.4 Further Analysis\nWe further analyze how different factors affect the\nquality and reliability of the generated knowledge\nand discuss our findings below.\nLong-tail Knowledge We investigate the impact\nof the knowledge frequency on the factuality perfor-\nmance of LLaMA on the WoW dataset. Each data\nentry in WoW comprises a topic, query, knowledge,\nand answer. The topic indicates the corresponding\nWikipedia page linked to the knowledge. We as-\nsess this knowledge’s frequency using Wikipedia\npageviews from 2015 to 20213. This enables us to\ndifferentiate between common and long-tail knowl-\nedge in WoW. Our findings reveal that LLaMA\nexhibits lower reliability when it is expected to gen-\nerate rare/long-tail knowledge compared to com-\nmon knowledge, as depicted in Figure 2(a).\nLong-form Generation We investigate the im-\npact of generation length on the factuality of the\ngenerated knowledge. Specifically, we consider\nknowledge over 40 tokens and take sentences as\nevaluation units aligned with factuality evaluation.\n3https://wikimedia.org/api/rest_v1\nFact.\nRel.\nCoh.\nInfo.\nHelp.\nVal.\nFact.\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n(a) Model scale of FLAN-T5\n65B\n33B\n7B\nFact.\nRel.\nCoh.\nInfo.\nHelp.\nVal.\nFact.\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n(b) Model scale of LLaMA\n11B\n3B\n0.8B\nFigure 3: Performance on NQ with different sizes of\nFLAN-T5 and LLaMA as the knowledge generator\n(Help. and Val. scores are linearly scaled).\nFigure 2(b) displays the factuality performance\nbased on the number of sentences in the gener-\nated knowledge. The results show that LLaMA\nexhibits higher error rates when generating long-\nform knowledge. Therefore, prompting the LLMs\nto generate the required knowledge in a concise\nrather than lengthy manner can benefit factuality.\nImpact of Model Size Figures 3 depicts the per-\nformance scaling with the model size, including\nLLaMA-65B/33B/7B and FLAN-T5-11B/3B/780M.\nThe results are reported on the NQ dataset using\nzero-shot prompting. We observe that larger mod-\nels do not necessarily outperform smaller models in\nterms of intrinsic evaluation (particularly when pa-\nrameter magnitudes are similar). However, larger\nmodels consistently outperform smaller models in\nterms of extrinsic evaluation(helpfulness and valid-\nity). Detailed tables are presented in Appendix I.\n5 Two Use Cases of CONNER\nTo explore how our framework can guide the future\ndesign of utilizing LLMs as a knowledge generator,\nwe design two strategies to employ CONNER as a\nmeasurement for guiding the Prompt Engineering\nand Knowledge Selection for knowledge-intensive\ntasks. We define the overall quality of knowledge\nkgiven the query qas follows:\nQknow(q,k) =γ⊺ ·Sintr γ ∈R4\nSintr = [Sfact,Srel,Scoh_para,Sinfo]⊺ (9)\nwhere Qknow is the linear combination of four in-\nstinct metrics Sintr and γis the coefficient vector.\nPrompt Engineering We show how to use\nCONNER to improve knowledge generation by per-\nforming prompt engineering for few-shot ICL. We\nrandom sample a small set of msamples from the\ntraining set, then use Qknow(q,k) as the scoring\nfunction to select the top n samples to compose\nthe few-shot prompt. As shown in Table 7, the\n6332\nModel Fact. Rel. Coh. Info.\nChatGPT 85.8% 0.462 0.863 0.792\nChatGPTselectprompt 87.7% 0.503 0.899 0.775\nTable 7: CONNER-guided demonstration selection im-\nproves the intrinsic quality of generated knowledge.\nModel Helpfulness Validity\nChatGPT 0.1461 43.45%\nChatGPTselectknowledge 0.2090 44.28%\nTable 8: CONNER-guided knowledge selection improves\nextrinsic (downstream) performance.\nknowledge generated by CONNER-enhanced few-\nshot prompting outperforms that with random\ndemonstrations on 3 out of 4 perspectives, under\nthe setting of m= 30and n= 8.\nKnowledge Selection We employ CONNER to im-\nprove downstream tasks by selecting high-quality\ngenerated knowledge. Specifically, we generate r\ndifferent knowledge H= {˜k1,..., ˜kr}from LLMs\nwith top- p sampling, then select the generated\nknowledge for the downstream task, according to\nk= argmax˜k∈HQknow(q,˜k). As shown in Table 8,\nwe achieve a relative improvement of 43.15% in\nhelpfulness on ChatGPT with p= 0.9 and r= 5.\n6 Human Evaluation\nWe conducted a human evaluation by randomly se-\nlecting 400 samples from the NQ and WoW test\nsets. Our three annotators provided ratings for the\nintrinsic and extrinsic metrics for the four mod-\nels. Additionally, for FLAN-T5 and LLaMA, we\nannotated the specific locations of factual errors\nin the generated knowledge, aiming to facilitate\nfuture research on fine-grained fallacy detection.\nDetailed annotation instructions and the statistics\nof our labelled data can be found in Appendix J.1.\nTo evaluate how well CONNER matches human\nevaluation of knowledge and compares with several\nbaseline metrics, we measure the Somers’ D cor-\nrelation (Somers, 1962) between the human rating\n0, 1, 2 of the knowledge quality and corresponding\nmetric scores. Table 9 and Table 10 illustrate the re-\nsults of four models on the NQ dataset. We observe\nthat: (1) CONNER yields consistently good correla-\ntions with human evaluation w.r.t different eval-\nuation perspectives (except for informativeness),\nwhich indicates that the quality of knowledge can\nbe more effectively evaluated with CONNER. The\ninconsistency between informativeness and human\njudgment is attributed to the differences in model\nMetric DPR FLAN-T5 LLaMA ChatGPT\nFactuality 0.65† 0.66† 0.66† 0.63†\nRelevance 0.69† 0.37† 0.55† 0.54†\nCoherence 0.53† 0.58† 0.44† 0.49†\nInformative 0.30† 0.17 0.35 0.32†\nHelpfulness 0.75† 0.45† 0.81† 0.69†\nValidity 0.83† 0.73† 0.85† 0.82†\nTable 9: Somer’s D correlation of metrics with the\nhuman annotation on NQ (The results on WoW are\npresented in Appendix J.2). Correlation scores with\np-value< 0.05 are marked with †.\nMetric DPR FLAN-T5 LLaMA ChatGPT\nFactuality 0.65† 0.66† 0.66† 0.63†\nHE -0.24 0.15 -0.03 0.29†\nNLI 0.23 0.47† 0.27† 0.38†\nNLI-Multitask 0.18† 0.51† 0.26† 0.32†\nNLI-Decompose. 0.23† 0.47† 0.27† 0.38†\nRelevance 0.69† 0.37† 0.55† 0.54†\nF1 0.45† 0.21 0.41† 0.47†\nValidity 0.83† 0.73† 0.85† 0.82†\nEM 0.59† 0.51† 0.54† 0.61†\nF1 0.74† 0.67† 0.76† 0.77†\nTable 10: Comparing CONNER with reference-reliant\nbaseline metrics on the NQ dataset. Details of baseline\nmetrics are presented in Appendix J.3.\nknowledge and human knowledge. (2)CONNER met-\nrics consistently outperform all other reference-\nreliant metrics, indicating the effectiveness of our\nframework in the knowledge evaluation scenarios.\n7 Conclusion\nIn this work, we introduce CONNER, a comprehen-\nsive evaluation framework designed to automati-\ncally assess both the intrinsic quality and extrinsic\nreliability of the knowledge generated by LLMs.\nNotably, CONNER is reference-free but demonstrates\na better correlation with human judgement com-\npared with previous reference-reliant metrics.\nThrough extensive evaluation and in-depth analy-\nsis, we identify several key factors affecting the fac-\ntuality of generated knowledge. We find although\nthe generated knowledge is less factual than the\nretrieved knowledge, it remarkably enhances the\nfactuality of downstream tasks over the retrieved\nones. Furthermore, we propose two approaches\nto improve knowledge generation and downstream\ntask performance with the guidance of CONNER. We\nbelieve our framework and findings will facilitate\nthe future research of trustworthy AIGC.\n6333\nLimitations\nIn this section, we discuss the limitations in this\nwork from three perspectives.\nFirstly, the knowledge we evaluate primarily re-\nlies on information sourced from Wikipedia. This\nchoice is driven by two considerations: (1) Large\nlanguage models (LLMs) are trained on diverse\ncorpora, which may include undisclosed domain-\nspecific or task-specific data. To ensure fairness\nin our evaluations and enable meaningful compar-\nisons, we focus on the common data sources that\nall models have learned from, with Wikipedia be-\ning a prevalent pre-training corpus for different\nLLMs. (2) Wikipedia is renowned for its high-\nquality knowledge, providing us with authoritative\nevidence to validate the generated knowledge. Ad-\nditionally, leveraging such authoritative evidence\nenhances the interpretability of our factual judg-\nments. In future work, we aim to expand our\nevaluations to include a broader range of world\nknowledge, thus further enhancing the scope and\ngeneralizability of our findings.\nSecondly, while our work primarily aims to pro-\npose a general framework that can be applied to\nany language, our evaluation framework presents\npotential generalization challenges for non-English\nlanguages. This is due to its reliance on several\ncommon NLP components, a limitation echoed\nacross many NLP methodologies. Encouragingly,\nthe development of model variants in other lan-\nguages, such as Chinese (Hu et al., 2020; Xie et al.,\n2023; Huang et al., 2017), indicates the potential\nfor broader applications. Nonetheless, the reality\nremains that for very low-resource languages with-\nout existing NLP models, these components may\nneed to be developed from scratch. This issue rep-\nresents a challenge that the community needs to\naddress in the future.\nA third limitation is that our assessment of\nfactuality is limited to sentence-level granularity.\nThrough analysis and manual annotation, we have\nidentified that large language models (LLMs) tend\nto exhibit errors at a more detailed level, particu-\nlarly concerning numbers, time, and the generation\nof misleading or fabricated concepts (e.g., key char-\nacters, identities, and locations), particularly within\nparallel structures. To address this limitation, fu-\nture research will concentrate on developing more\nfine-grained methods for detecting hallucinations\nand assessing factual accuracy. To facilitate such\nresearch, we have annotated a specific subset of\ndata that targets fine-grained factual errors.\nDespite these limitations, we believe our work\nserves as a significant catalyst for the automated\nevaluation of knowledge generated by large lan-\nguage models, contributing positively to the ad-\nvancement of more trustworthy AI systems.\nAcknowledgements\nWe extend our sincerest gratitude to Professor Jing\nMa, whose insightful discussions and suggestions\non factuality evaluation have significantly inspired\nour design. We are particularly grateful to our\nthree anonymous reviewers, whose thorough and\nmeticulous reviews have considerably improved\nthe quality of our work. Their constructive discus-\nsions and insights have undoubtedly enhanced our\nrevisions. This research work is partially supported\nby CUHK under Project No. 3230377 (Ref. No.\nKPF23GW20).\nReferences\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and\nStella Biderman. 2021. GPT-Neo: Large scale autore-\ngressive language modeling with Mesh-Tensorflow.\nVirginia Braun and Victoria Clarke. 2012. Thematic\nanalysis., pages 57–71.\nLiang Chen, Hongru Wang, Yang Deng, Wai Chung\nKwan, Zezhong Wang, and Kam-Fai Wong. 2023.\nTowards robust personalized dialogue generation via\norder-insensitive representation regularization. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2023, pages 7337–7345, Toronto,\nCanada. Association for Computational Linguistics.\nYang Deng, Wenqiang Lei, Minlie Huang, and Tat-Seng\nChua. 2023. Goal awareness for conversational AI:\nproactivity, non-collaborativity, and beyond. In Pro-\nceedings of the 61st Annual Meeting of the Associ-\nation for Computational Linguistics: Tutorial Ab-\nstracts, ACL 2023, Toronto, Canada, July 9-14, 2023,\npages 1–10. Association for Computational Linguis-\ntics.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2018. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. arXiv preprint arXiv:1811.01241.\nNouha Dziri, Ehsan Kamalloo, Sivan Milton, Os-\nmar Zaiane, Mo Yu, Edoardo M. Ponti, and Siva\nReddy. 2022. Faithdial: A faithful benchmark for\ninformation-seeking dialogue.\nJohn Glover, Federico Fancellu, Vasudevan Jagan-\nnathan, Matthew R. Gormley, and Thomas Schaaf.\n2022a. Revisiting text decomposition methods for\n6334\nnli-based factuality scoring of summaries. CoRR,\nabs/2211.16853.\nJohn Glover, Federico Fancellu, Vasudevan Jagan-\nnathan, Matthew R. Gormley, and Thomas Schaaf.\n2022b. Revisiting text decomposition methods for\nnli-based factuality scoring of summaries.\nOr Honovich, Leshem Choshen, Roee Aharoni, Ella\nNeeman, Idan Szpektor, and Omri Abend. 2021.\nq2: Evaluating factual consistency in knowledge-\ngrounded dialogues via question generation and ques-\ntion answering.\nHai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra\nKübler, and Lawrence Moss. 2020. OCNLI: Orig-\ninal Chinese Natural Language Inference. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2020, pages 3512–3526, Online. Association\nfor Computational Linguistics.\nGuimin Huang, Min Tan, Sirui Huang, Ruyu Mo, and\nYa Zhou. 2017. A discourse coherence model for\nanalyzing chinese students’ essay. In 2017 Interna-\ntional Conference on Progress in Informatics and\nComputing (PIC), pages 430–434.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open\ndomain question answering. In EACL 2021, pages\n874–880.\nRolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui\nWang, and Michael Bendersky. 2023. Query expan-\nsion by prompting large language models. CoRR,\nabs/2305.03653.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput.\nSurv., 55(12).\nPrathyusha Jwalapuram, Shafiq R. Joty, and Xiang\nLin. 2021. Rethinking self-supervision objectives\nfor generalizable coherence modeling. CoRR,\nabs/2110.07198.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield Dodds, Nova DasSarma, Eli\nTran-Johnson, et al. 2022a. Language models\n(mostly) know what they know. arXiv preprint\narXiv:2207.05221.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli\nTran-Johnson, Scott Johnston, Sheer El Showk, Andy\nJones, Nelson Elhage, Tristan Hume, Anna Chen,\nYuntao Bai, Sam Bowman, Stanislav Fort, Deep\nGanguli, Danny Hernandez, Josh Jacobson, Jack-\nson Kernion, Shauna Kravec, Liane Lovitt, Ka-\nmal Ndousse, Catherine Olsson, Sam Ringer, Dario\nAmodei, Tom Brown, Jack Clark, Nicholas Joseph,\nBen Mann, Sam McCandlish, Chris Olah, and Jared\nKaplan. 2022b. Language models (mostly) know\nwhat they know. CoRR, abs/2207.05221.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781.\nMojtaba Komeili, Kurt Shuster, and Jason Weston. 2021.\nInternet-augmented dialogue generation.\nWojciech Kryscinski, Bryan McCann, Caiming Xiong,\nand Richard Socher. 2020a. Evaluating the factual\nconsistency of abstractive text summarization. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9332–9346, Online. Association for Computa-\ntional Linguistics.\nWojciech Kryscinski, Bryan McCann, Caiming Xiong,\nand Richard Socher. 2020b. Evaluating the factual\nconsistency of abstractive text summarization. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2020, pages 9332–9346.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:452–466.\nNayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pas-\ncale Fung, Mohammad Shoeybi, and Bryan Catan-\nzaro. 2023. Factuality enhanced language models for\nopen-ended text generation.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\nTim Rocktäschel, Sebastian Riedel, and Douwe\nKiela. 2020. Retrieval-augmented generation for\nknowledge-intensive NLP tasks. In Advances in Neu-\nral Information Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2020.\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun\nNie, and Ji-Rong Wen. 2023. Halueval: A large-\nscale hallucination evaluation benchmark for large\nlanguage models.\nYanyang Li, Jianqiao Zhao, Michael R. Lyu, and Li-\nwei Wang. 2022. Eliciting knowledge from large\npre-trained models for unsupervised knowledge-\ngrounded conversation. In Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2022, pages 10551–\n10564.\n6335\nNelson F. Liu, Tianyi Zhang, and Percy Liang. 2023a.\nEvaluating verifiability in generative search engines.\nYixin Liu, Alexander R. Fabbri, Pengfei Liu, Dragomir\nRadev, and Arman Cohan. 2023b. On learning to\nsummarize with large language models as references.\nCoRR, abs/2305.14239.\nZihan Liu, Mostofa Patwary, Ryan Prenger, Shrimai\nPrabhumoye, Wei Ping, Mohammad Shoeybi, and\nBryan Catanzaro. 2022. Multi-stage prompting for\nknowledgeable dialogue generation.\nPotsawee Manakul, Adian Liusie, and Mark J. F. Gales.\n2023. Selfcheckgpt: Zero-resource black-box hal-\nlucination detection for generative large language\nmodels. CoRR, abs/2303.08896.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan T. McDonald. 2020. On faithfulness and factu-\nality in abstractive summarization. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics, ACL 2020, pages 1906–1919.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\nLewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\nFactscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation.\nRodrigo Frassetto Nogueira, Wei Yang, Kyunghyun\nCho, and Jimmy Lin. 2019. Multi-stage document\nranking with BERT. CoRR, abs/1910.14424.\nOpenAI. 2023. GPT-4 technical report. CoRR,\nabs/2303.08774.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nArtidoro Pagnoni, Vidhisha Balachandran, and Yulia\nTsvetkov. 2021. Understanding factuality in abstrac-\ntive summarization with FRANK: A benchmark for\nfactuality metrics. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 4812–4829, Online. As-\nsociation for Computational Linguistics.\nLiangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan\nLuu, William Yang Wang, Min-Yen Kan, and Preslav\nNakov. 2023. Fact-checking complex claims with\nprogram-guided reasoning.\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng,\nYujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou\nYu, Weizhu Chen, and Jianfeng Gao. 2023. Check\nyour facts and try again: Improving large language\nmodels with external knowledge and automated feed-\nback.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\net al. 2021. Kilt: a benchmark for knowledge in-\ntensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523–2544.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text.\nVipula Rawte, Amit Sheth, and Amitava Das. 2023. A\nsurvey of hallucination in large foundation models.\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon,\nChristopher Potts, and Matei Zaharia. 2021. Col-\nbertv2: Effective and efficient retrieval via\nlightweight late interaction. CoRR, abs/2112.01488.\nTal Schuster, Adam Fisch, and Regina Barzilay. 2021.\nGet your vitamin C! robust fact verification with\ncontrastive evidence. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 624–643, Online. As-\nsociation for Computational Linguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021, pages 3784–3803, Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nRobert H Somers. 1962. A new asymmetric measure of\nassociation for ordinal variables. American sociolog-\nical review, pages 799–811.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction\nand VERification. In Proceedings of the 2018\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nPapers), pages 809–819, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\n6336\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu\nWang, Madeleine van Zuylen, Arman Cohan, and\nHannaneh Hajishirzi. 2020. Fact or fiction: Verifying\nscientific claims. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 7534–7550, Online. As-\nsociation for Computational Linguistics.\nAlex Wang, Kyunghyun Cho, and Mike Lewis. 2020.\nAsking and answering questions to evaluate the fac-\ntual consistency of summaries. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5008–5020, Online. Asso-\nciation for Computational Linguistics.\nShuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang,\nFei Wu, Tianwei Zhang, Jiwei Li, and Guoyin Wang.\n2023. GPT-NER: named entity recognition via large\nlanguage models. CoRR, abs/2304.10428.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2022. Finetuned\nlanguage models are zero-shot learners.\nXiaohui Xie, Qian Dong, Bingning Wang, Feiyang Lv,\nTing Yao, Weinan Gan, Zhijing Wu, Xiangsheng Li,\nHaitao Li, Yiqun Liu, and Jin Ma. 2023. T2ranking:\nA large-scale chinese benchmark for passage ranking.\nBoyang Xue, Weichao Wang, Hongru Wang, Fei Mi,\nRui Wang, Yasheng Wang, Lifeng Shang, Xin Jiang,\nQun Liu, and Kam-Fai Wong. 2023. Improving fac-\ntual consistency for knowledge-grounded dialogue\nsystems via knowledge enhancement and alignment.\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. 2023. Generate\nrather than retrieve: Large language models are\nstrong context generators.\nYue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,\nTingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,\nYulong Chen, Longyue Wang, Anh Tuan Luu, Wei\nBi, Freda Shi, and Shuming Shi. 2023. Siren’s song\nin the ai ocean: A survey on hallucination in large\nlanguage models.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\nPeiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A\nsurvey of large language models.\nYutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan\nLiu, Wenhan Liu, Chenlong Deng, Zhicheng Dou,\nand Ji-Rong Wen. 2023. Large language models for\ninformation retrieval: A survey.\n0 2 4 6 8 10\nNum of retrieved evidence\n0\n10\n20\n30\n40\n50\n60\n70Prob of factual-consistent (%)\n w reference knowledge\nwo reference knowledge\nFigure 4: The influence of reference knowledge (e.g.,\nthe annotated Wiki. document in WoW dataset) in fac-\ntuality evaluation weakens as the amount of retrieved\nevidence increases.\nAppendix\nA Details of Problem Formulation\nWe provide a formulation of the two-step process\nfor knowledge-intensive tasks, as illustrated in\nFig.1. Formally, the knowledge-intensive gener-\nation problem can be formulated as the following\nchain rule:\nP(a|q,K) =\n∑\nk\nP(k|q,K)P(a|q,k) (10)\nwhere P(k|q,K) is the knowledge acquisition pro-\ncess and P(a|q,k) = ∏N\nt=1 P(at|a1:t−1,q,k ) is\nthe autoregressive answer generation process of the\nreader model based on the acquired knowledge.\nRetrieval-based knowledge acquisition methods\nuse a retrieval model to retrieve the most rel-\nevant knowledge from the knowledge resource\nK= {d1,d2,...,d K}composed of Kdocuments:\nP(k= di|q,K) = esim(q,di)\n∑K\nj=1 esim(q,dj ) (11)\nwhere sim(·) function is used to measure the sim-\nilarity, e.g., cosine similarity, between the query\nand the knowledge document.\nGeneration-based knowledge acquisition meth-\nods prompt a large language model to directly gen-\nerate the required knowledge:\nP(k|q,K) =\n∏ M\nt=1\nPK(kt|k1:t−1,q, prompt) (12)\nwhere prompt denotes the zero-shot or few-shot\nprompt and the LLM is regarded as the knowledge\nresource Kand PKstands for the distribution in-\nduced by the LLM.\n6337\nDataset Prompts Best\nNQ\nTopic: {topic}\\n Query: {query}\\n Related wikipedia knowledge:\nTopic: {topic}\\n Generate a background document from Wikipedia to answer the given question.\\n {query}\\n\nTopic: {topic}\\n Generate a Wikipedia knowledge to answer the given question.\\n Question: {query}\\n Wikipedia knowledge:\nTopic: {topic}\\n Generate a Wikipedia to answer the given question.\\n Question: {query}\\n Wikipedia: ✓\nWOW\nTopic: {topic}\\n Query: {utterance}\\n Related Wikipedia knowledge:\nTopic: {topic}\\n Generate a background document from Wikipedia to reply to the utterance.\\n {utterance}\\n\nTopic: {topic}\\n Generate a Wikipedia knowledge to answer the given question.\\n Utterance: {utterance}\\n Wikipedia knowledge:\nTopic: {topic}\\n Generate a Wikipedia to answer the given question.\\n Question: {utterance}\\n Wikipedia: ✓\nTable 11: List of human prompts we tried for zero-shot knowledge generation, evaluated on the validation set of\nNQ, WoW. {} represents placeholder, and ’utterance’ denotes the last utterance of the dialogue partner. We use✓to\ndenote the prompt achieving the best performance.\nDataset Prompts Best\nNQ\nTopic: {topic}\\n Query: {query}\\n Related Wikipedia knowledge: {knowledge} ✓\nTopic: {topic}\\n Query: {query}\\n Knowledge: {knowledge}\nTopic: {topic}\\n Query: {query}\\n Document: {knowledge}\nTopic: {topic}\\n Generate a background document from Wikipedia to answer the given question.\\n {query}\\n {knowledge}\nTopic: {topic}\\n Generate a Wikipedia to answer the given question.\\n Question: {query}\\n Wikipedia: {knowledge}\nWOW\nTopic: {topic}\\n Query: {utterance}\\n Related Wikipedia knowledge: {knowledge} ✓\nTopic: {topic}\\n Query: {utterance}\\n Knowledge: {knowledge}\nTopic: {topic}\\n Query: {utterance}\\n Document: {knowledge}\nTopic: {topic}\\n Generate a background document from Wikipedia to reply to the utterance.\\n {utterance}\\n {knowledge}\nTopic: {topic}\\n Generate a Wikipedia to answer the given question.\\n Question: {utterance}\\n Wikipedia: {knowledge}\nTable 12: List of example templates we tried for few-shot knowledge generation.\nDataset Prompts Best\nNQ\nTopic: {topic}\\n Passage: {knowledge}\\n Query: {query}\\n Answer: {answer} ✓Topic: {topic}\\n Read the passage and answer the question below:\\n Passage: {knowledge}\\n Question: {query}\\n Answer: {answer}Topic: {topic}\\n Using the knowledge from the passage to answer the question below:\\n Passage: {knowledge}\\n Question: {query}\\n Answer: {answer}\nWOW\nTopic: {topic}\\n Passage: {knowledge}\\n Speaker 1: {utterance}\\n Speaker 2: {response} ✓Topic: {topic}\\n Knowledge: {knowledge}\\n Speaker 1: {utterance}\\n Speaker 2: {response}Topic: {topic}\\n Grounding document: {knowledge}\\n Speaker 1: {utterance}\\n Speaker 2: {response}Passage: {knowledge}\\n Query: {utterance}\\n Answer: {response}Topic: {topic}\\n Using the knowledge from the passage, complete the dialogue below: {knowledge}\\n Speaker 1: {utterance}\\n Speaker 2: {response}\nTable 13: List of example templates we tried for few-shot answer generation.\nB Analysis of Reference Knowledge\nWe investigated the importance of reference knowl-\nedge in evaluating the factuality of generated\nknowledge. Specifically, we conducted FLAN-\nT5 experiments on the WoW dataset using a zero-\nshot approach. Two sets of experiments were per-\nformed: one included reference knowledge in the\nretrieved evidence pool, while the other did not.\nFigure 4 illustrates our findings, indicating that the\ngroup with reference knowledge exhibits a clear\nadvantage when the number of retrieved evidence\nis limited. However, as the number of retrieved\nevidence increases, the performance of both groups\nconverges. These results suggest that reference\nknowledge is dispensable, particularly when a sig-\nnificant amount of evidence is available. When\nthe number of retrieved evidence surpasses ten, the\nimpact of reference knowledge becomes negligible.\nWe hope this will provide valuable insights for fu-\nture designs of factuality assessment for generated\nknowledge.\nC Details of Baselines\nDPR (Karpukhin et al., 2020) is a supervised dense\nretrieval model trained on several QA datasets (in-\ncluding NQ) to retrieve the most relevant Wikipedia\npassages given a query.\nFLAN-T5 (Wei et al., 2022) is an enhanced ver-\nsion of T5 (Raffel et al., 2020) that is instruction-\nfinetuned in 1.8k NLP datasets to acquire the gen-\neralization ability to unseen tasks.\nLLaMA (Touvron et al., 2023) is an open-source\nfoundation language model trained on publicly\navailable datasets and shows competitive perfor-\nmance with the best models, including GPT-3\n(175B) and PaLM-540B.\nChatGPT is a sibling model to InstructGPT\n(Ouyang et al., 2022) that is trained to follow in-\nstructions in a prompt and provide a detailed re-\nsponse. We adopt text-davinci-003 version for\n6338\nevaluation.\nD Details of Datasets\nNatural Questions (NQ) (Kwiatkowski et al.,\n2019) is an open-domain QA dataset, where the\nquestions are mined from real Google search\nqueries. The corresponding ground truth knowl-\nedge and the answers to the questions are para-\ngraphs and short spans in the Wikipedia pages.\nWizard of Wikipedia (WoW) (Dinan et al., 2018)\nis a knowledge-grounded dialogue dataset de-\nsigned for information-seeking scenarios, where\none speaker introduces knowledge related to a topic\nto the other speaker by grounding his/her responses\nin a specific sentence from a Wikipedia page.\nE Implementation Details\nAll the metrics we designed are model-based met-\nrics, utilizing solely off-the-shelf models. We\npresent the models used in Table 14.\nF Prompts for Knowledge Generation\nF.1 Zero-shot Prompts\nIn our experiments, we observed that zero-shot\nprompting was highly unstable. Therefore, we con-\nduct experiments using multiple human prompts\nand select the most effective ones for the WoW and\nNQ datasets. The human prompts we evaluate are\nlisted in Table 11.\nF.2 Few-shot Prompts\nIn the few-shot setting, our prompt is constructed\nusing k randomly chosen examples from the train-\ning set:\nprompt= (example1\\n ...examplek\\n exampletest)\nThe example templates utilized for knowledge gen-\neration are provided in Table 12. Please note that\nexampletest differs from examplei as it does not\ncontain {knowledge}in the placeholder.\nG Prompts for Answer Generation\nWe adopt few-shot prompting on the LLaMA\nmodel in answer generation and the example tem-\nplates used for answer generation are provided in\nTable 13.\nH Detailed Correlations between\nIntrinsic and Extrinsic Metrics\nWe listed the detailed correlations between intrinsic\nand extrinsic metrics for LLaMA, FLAN-T5, and\nChatGPT on the NQ dataset in Table 15.\nI Table of Model Size Impact\nWe list the specific numerical values of perfor-\nmance scaling with the model size in Table 16,\nincluding LLaMA-65B/33B/7B and FLAN-T5-\n11B/3B/780M.\nJ Details of Human Evaluation\nJ.1 Human Annotation\nWe conducted a human evaluation with 400 sam-\nples from the NQ and WoW test set. Among these,\n320 samples were from the zero-shot setting in\nthe NQ dataset, involving all four models, while 80\nsamples were from the few-shot setting in the WoW\ndataset, involving one model (ChatGPT). Three ex-\npert annotators, who were familiar with the tasks,\nwere employed to rate the acquired knowledge and\ngenerated answers based on four intrinsic perspec-\ntives and two extrinsic perspectives. Each perspec-\ntive was scored on a scale of 0, 1, or 2, representing\nunacceptable, acceptable, and excellent, respec-\ntively. The average kappa value of the annotation is\n0.612 on 20% cross-annotation data. The detailed\ninstructions for the human annotation can be found\nin Table 17.\nNote for factuality assessment, the reliable evi-\ndence for the generated knowledge kis acquired\nby the following process: For each sentence in k,\nwe use it as the query to search Google, and regard\nthe top1 Wikipedia webpage as a reliable knowl-\nedge source to verify the factuality of this sentence.\nAnother point worth noting is that for the evalua-\ntion of validity in WoW, we reused the factuality\nevaluation process since the responses in WoW are\nopen-ended.\nJ.2 Human Evaluation Results on WoW\nBased on the provided annotations, we assessed the\ncorrelation between ChatGPT’s automatic metrics\nand human judgement on the WoW dataset. The\nresults are presented in Table 18.\nJ.3 Baseline Metrics\nWe compared it with three reference-reliant metrics\nin knowledge evaluation. Their definitions and\ncalculation methods are as follows:\n6339\nMetric Model Link\nFactuality NLI-RoBERTa-large https://huggingface.co/sentence-transformers/nli-roberta-large\nColBERTv2 https://github.com/stanford-futuredata/ColBERT\nRelevance BERT-ranking-large https://github.com/nyu-dl/dl4marco-bert\nCoherence GPT-neo-2.7B https://huggingface.co/EleutherAI/gpt-neo-2.7B\nCoherence-Momentum https://huggingface.co/aisingapore/coherence-momentum\nInformativeness GPT-neo-2.7B https://huggingface.co/EleutherAI/gpt-neo-2.7B\nHelpfulness LLaMA-65B https://github.com/facebookresearch/llama/tree/main\nValidity NLI-RoBERTa-large https://huggingface.co/sentence-transformers/nli-roberta-large\nColBERTv2 https://github.com/stanford-futuredata/ColBERT\nTable 14: List of all models that we use in designing our framework.\nModel Extrinsic Instrinsic\nFact. Rel. Coh-sent. Coh-para. Info.\nFLAN-T5 helpful. 0.15 †-0.21† 0.20† -0.21† 0.02\nvalidity 0.23 †-0.16† 0.14† -0.10† 0.07\nLLAMA helpful. 0.03 0.05 0.06 -0.09 † -0.01\nvalidity 0.09 † 0.07 0.05 -0.06 -0.03\nCHATGPT helpful. 0.16 † 0.03 0.08 0.02 -0.04 †\nvalidity 0.22 †0.13† 0.02† 0.09† 0.03\nTable 15: The Somers’ correlation between intrinsic\nand extrinsic metrics in zero-shot setting on NQ. Corre-\nlation scores with p-value< 0.05 are marked with †.\nModel Size Fact. Rel. Coh. Info Help. Val.\nLLaMA\n65B 0.942 0.732 0.824 0.757 0.219 0.420\n33B 0.656 0.633 0.734 0.608 0.203 0.402\n7B 0.773 0.626 0.805 0.662 0.154 0.375\nFLAN-T5\n11B 0.584 0.685 0.778 0.673 -0.146 0.325\n3B 0.657 0.663 0.816 0.708 -0.155 0.324\n780M 0.506 0.729 0.793 0.729 -0.162 0.252\nTable 16: Performance on NQ with varying sizes of\nFLAN-T5 and LLaMA as knowledge generators. The\nmax(0, .) operation in Eq.6 has been excluded to empha-\nsize the sequential relationship among different sizes\nof FLAN-T5. Bold and Underlined results represent\nthe best and second-best performances for each model,\nrespectively.\nHallucinated NE Ratio (HE) (Lee et al., 2023)\nproposed a NE-based metric that is designed with\nan intuition that a model is hallucinating (making\nfactual errors) if it generates an NE that does not\nappear in the reference knowledge source. The\nNE-based metric can be calculated as: HNE =\n|HALLU NE|/|ALLNE|where ALLNE is the set of\nall the NEs detected in the LM generation, and\nHALLU NE is a subset of NEAll that does not appear\nin the reference Wikipedia knowledge. Note that\nevaluating NEER requires the existence of refer-\nence knowledge. We adopt −HNE when comput-\ning the correlation with human judgement.\nEntailment Ratio (ER) (Lee et al., 2023) also in-\ntroduces an NLI-based approach to assess factual\nknowledge by measuring its entailing relationship\nwith ground-truth/reference knowledge. The en-\ntailment ratio is computed as follows: EntailR=\n|ENTAIL gen|/|ALLgen|,where ALLgen is a set of\nall generated knowledge, and ENTAIL gen is the set\nof generated knowledge that can be entailed by the\nNLI model. Specifically, we use the entailment\nprobability of each example as its ER score.\nF1 of knowledge (F1) (Liu et al., 2022) employs a\nunigram F1 score to evaluate the quality of gener-\nated knowledge. This metric measures the overlap\nbetween the generated knowledge and the reference\nknowledge by evaluating word-level matches. By\nassessing the degree of agreement, the F1 metric\nprovides an estimation of the knowledge quality,\nspecifically from a relevance perspective.\nNLI-weak-supervised (Kryscinski et al., 2020b)\ntrain a classification model on constructed data\nto perform consistency checking on (document,\nsentence) pairs. We chose the factCC version as\nour baseline.\nNLI-decompose-claim (Glover et al., 2022b)\nfound that in general, sentence-level decomposi-\ntion is preferable for the hypothesis side of the\nNLI input. So we also decompose the generated\nknowledge into sentences and then aggregate the\nsentence-level scores to produce a document-level\nscore.\nNLI-multitask fine-tunes the DeBERTa-v3-large\nmodel on FEVER and two NLI datasets.\nExact Match (EM) (Rajpurkar et al., 2016) use Ex-\nact Match to measure the percentage of predictions\nthat match its ground truth answers exactly.\n6340\nDimension Value Description\n2 All sentences inkare factually accurate and the information in them can be verified with reliable evidence.\nFactuality 1 kcontains at least one sentence with non-verified information, while others are factually accurate.\n0 kcontains at least one sentence with at least one factual error that is inconsistent with reliable knowledge.\n2 kis highly relevant to the topic and query/utterance.\nRelevance 1 kis relevant to the topic but less relevant to the query/utterance.\n0 kis irrelevant to both the topic and query/utterance.\n2 kis very coherent and fluent (do not consider the truncation at the end due to the maximum generation length).\nCoherence 1 khas some minor incoherence or lack of fluency,e.g., phrase or sentence repetition, but it does not affect understanding.\n0 khas significant coherence and fluency issues that are hard to understand.\n2 kcontains informative content that you don’t know before.\nInformative 1 kcontains limited or trivial information against your knowledge.\n0 kfails to provide any meaningful information.\n2 kdirectly provides or contains the correct answer.\nHelpfulness 1 kindirectly help in generating the correct answer.\n0 kdoes not contain any useful information for the correct answer.\n2 The answer generated based onkis correct.\nValidity 1 The correctness of the generated answer cannot be determined.\n0 The answer generated based onkis completely incorrect.\nTable 17: Annotation guideline of LLM generated knowledge.\nModel Fact. Rel. Coh. Info. Help. Val.\nChatGPT 0.71 0.57 0.52 0.40 0.79 0.54\nTable 18: Somer’s D correlation of metrics with the\nhuman annotation on WoW ). p-valuefor all results are\n< 0.05. We report the maximum for coherence.\n6341",
  "topic": "Helpfulness",
  "concepts": [
    {
      "name": "Helpfulness",
      "score": 0.8201338648796082
    },
    {
      "name": "Relevance (law)",
      "score": 0.7395714521408081
    },
    {
      "name": "Computer science",
      "score": 0.7333780527114868
    },
    {
      "name": "Coherence (philosophical gambling strategy)",
      "score": 0.6337096095085144
    },
    {
      "name": "Domain knowledge",
      "score": 0.5560271739959717
    },
    {
      "name": "Natural language processing",
      "score": 0.3831586241722107
    },
    {
      "name": "Knowledge management",
      "score": 0.3808857798576355
    },
    {
      "name": "Information retrieval",
      "score": 0.3390616774559021
    },
    {
      "name": "Psychology",
      "score": 0.1306459903717041
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ]
}